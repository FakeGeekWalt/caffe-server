./build/tools/caffe: /home/zkx/anaconda2/lib/libtiff.so.5: no version information available (required by /home/zkx/env/opencv/lib/libopencv_highgui.so.2.4)
I0522 21:55:35.914993 35003 upgrade_proto.cpp:1084] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': models/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/solver.prototxt
I0522 21:55:35.915302 35003 upgrade_proto.cpp:1091] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W0522 21:55:35.915313 35003 upgrade_proto.cpp:1093] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I0522 21:55:35.915454 35003 caffe.cpp:204] Using GPUs 0, 1, 2, 3
I0522 21:55:35.939368 35003 caffe.cpp:209] GPU 0: TITAN Xp
I0522 21:55:35.940050 35003 caffe.cpp:209] GPU 1: TITAN Xp
I0522 21:55:35.940666 35003 caffe.cpp:209] GPU 2: TITAN Xp
I0522 21:55:35.941295 35003 caffe.cpp:209] GPU 3: TITAN Xp
I0522 21:55:38.154410 35003 solver.cpp:45] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 500000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx"
solver_mode: GPU
device_id: 0
net: "models/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/train.prototxt"
train_state {
  level: 0
  stage: ""
}
snapshot_after_train: true
stepvalue: 150000
stepvalue: 300000
iter_size: 1
type: "SGD"
I0522 21:55:38.154603 35003 solver.cpp:102] Creating training net from net file: models/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/train.prototxt
I0522 21:55:38.156389 35003 net.cpp:51] Initializing net from parameters: 
name: "2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_train"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  image_data_param {
    source: "/home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/image_list/train_list/shot_Oriental_Age_Lan_DHUA_PAKJ_Indon_Migrant_celeb_label.txt"
    batch_size: 64
    shuffle: true
    root_folder: "/home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/fc_0.35_112x96/"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_1"
  type: "PReLU"
  bottom: "conv1_1"
  top: "conv1_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "conv1_3"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_3"
  type: "PReLU"
  bottom: "conv1_3"
  top: "conv1_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res1_3"
  type: "Eltwise"
  bottom: "conv1_1"
  bottom: "conv1_3"
  top: "res1_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res1_3_reduce"
  type: "Convolution"
  bottom: "res1_3"
  top: "res1_3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "res1_3_reduce"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "res1_3"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1"
  type: "PReLU"
  bottom: "conv2_1"
  top: "conv2_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res1_3_p"
  type: "Eltwise"
  bottom: "pool1"
  bottom: "conv2_1"
  top: "res1_3_p"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_3"
  type: "Convolution"
  bottom: "res1_3_p"
  top: "conv2_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_3"
  type: "PReLU"
  bottom: "conv2_3"
  top: "conv2_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res2_3"
  type: "Eltwise"
  bottom: "res1_3_p"
  bottom: "conv2_3"
  top: "res2_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_5"
  type: "Convolution"
  bottom: "res2_3"
  top: "conv2_5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_5"
  type: "PReLU"
  bottom: "conv2_5"
  top: "conv2_5"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res2_5"
  type: "Eltwise"
  bottom: "res2_3"
  bottom: "conv2_5"
  top: "res2_5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res2_5_reduce"
  type: "Convolution"
  bottom: "res2_5"
  top: "res2_5_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2_5_reduce"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "res2_5"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1"
  type: "PReLU"
  bottom: "conv3_1"
  top: "conv3_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res2_5_p"
  type: "Eltwise"
  bottom: "pool2"
  bottom: "conv3_1"
  top: "res2_5_p"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "res2_5_p"
  top: "conv3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_3"
  type: "PReLU"
  bottom: "conv3_3"
  top: "conv3_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res3_3"
  type: "Eltwise"
  bottom: "res2_5_p"
  bottom: "conv3_3"
  top: "res3_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_5"
  type: "Convolution"
  bottom: "res3_3"
  top: "conv3_5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_5"
  type: "PReLU"
  bottom: "conv3_5"
  top: "conv3_5"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res3_5"
  type: "Eltwise"
  bottom: "res3_3"
  bottom: "conv3_5"
  top: "res3_5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res3_5_reduce"
  type: "Convolution"
  bottom: "res3_5"
  top: "res3_5_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 144
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3_5_reduce"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "res3_5"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 144
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1"
  type: "PReLU"
  bottom: "conv4_1"
  top: "conv4_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res3_5_p"
  type: "Eltwise"
  bottom: "pool3"
  bottom: "conv4_1"
  top: "res3_5_p"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "res3_5_p"
  top: "conv4_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 144
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_3"
  type: "PReLU"
  bottom: "conv4_3"
  top: "conv4_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res4_3"
  type: "Eltwise"
  bottom: "res3_5_p"
  bottom: "conv4_3"
  top: "res4_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "fc5"
  type: "InnerProduct"
  bottom: "res4_3"
  top: "fc5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc5_bn"
  type: "BatchNorm"
  bottom: "fc5"
  top: "fc5"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "norm1"
  type: "Normalize"
  bottom: "fc5"
  top: "norm1"
}
layer {
  name: "fc-6_l2"
  type: "InnerProduct"
  bottom: "norm1"
  top: "fc-6_l2"
  param {
    lr_mult: 1
  }
  inner_product_param {
    num_output: 21331
    bias_term: false
    weight_filler {
      type: "xavier"
    }
    normalize: true
  }
}
layer {
  name: "fc-6_margin"
  type: "LabelSpecificAdd"
  bottom: "fc-6_l2"
  bottom: "label"
  top: "fc-6_margin"
  label_specific_add_param {
    bias: -0.35
  }
}
layer {
  name: "fc-6_margin_scale"
  type: "Scale"
  bottom: "fc-6_margin"
  top: "fc-6_margin_scale"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      type: "constant"
      value: 30
    }
  }
}
layer {
  name: "softmax_loss"
  type: "SoftmaxWithLoss"
  bottom: "fc-6_margin_scale"
  bottom: "label"
  top: "softmax_loss"
}
I0522 21:55:38.156694 35003 layer_factory.hpp:77] Creating layer data
I0522 21:55:38.156743 35003 net.cpp:84] Creating Layer data
I0522 21:55:38.156749 35003 net.cpp:380] data -> data
I0522 21:55:38.156775 35003 net.cpp:380] data -> label
I0522 21:55:38.156793 35003 image_data_layer.cpp:38] Opening file /home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/image_list/train_list/shot_Oriental_Age_Lan_DHUA_PAKJ_Indon_Migrant_celeb_label.txt
I0522 21:55:41.440763 35003 image_data_layer.cpp:53] Shuffling data
I0522 21:55:41.674480 35003 image_data_layer.cpp:63] A total of 1134127 images.
I0522 21:55:41.676734 35003 image_data_layer.cpp:90] output data size: 64,3,112,96
I0522 21:55:42.494333 35003 net.cpp:122] Setting up data
I0522 21:55:42.494403 35003 net.cpp:129] Top shape: 64 3 112 96 (2064384)
I0522 21:55:42.494411 35003 net.cpp:129] Top shape: 64 (64)
I0522 21:55:42.494415 35003 net.cpp:137] Memory required for data: 8257792
I0522 21:55:42.494426 35003 layer_factory.hpp:77] Creating layer label_data_1_split
I0522 21:55:42.494458 35003 net.cpp:84] Creating Layer label_data_1_split
I0522 21:55:42.494467 35003 net.cpp:406] label_data_1_split <- label
I0522 21:55:42.494482 35003 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0522 21:55:42.494495 35003 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0522 21:55:42.494632 35003 net.cpp:122] Setting up label_data_1_split
I0522 21:55:42.494642 35003 net.cpp:129] Top shape: 64 (64)
I0522 21:55:42.494647 35003 net.cpp:129] Top shape: 64 (64)
I0522 21:55:42.494650 35003 net.cpp:137] Memory required for data: 8258304
I0522 21:55:42.494654 35003 layer_factory.hpp:77] Creating layer conv1_1
I0522 21:55:42.494683 35003 net.cpp:84] Creating Layer conv1_1
I0522 21:55:42.494688 35003 net.cpp:406] conv1_1 <- data
I0522 21:55:42.494729 35003 net.cpp:380] conv1_1 -> conv1_1
I0522 21:55:44.205071 35003 net.cpp:122] Setting up conv1_1
I0522 21:55:44.205106 35003 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:44.205111 35003 net.cpp:137] Memory required for data: 24773376
I0522 21:55:44.205137 35003 layer_factory.hpp:77] Creating layer relu1_1
I0522 21:55:44.205152 35003 net.cpp:84] Creating Layer relu1_1
I0522 21:55:44.205159 35003 net.cpp:406] relu1_1 <- conv1_1
I0522 21:55:44.205168 35003 net.cpp:367] relu1_1 -> conv1_1 (in-place)
I0522 21:55:44.206573 35003 net.cpp:122] Setting up relu1_1
I0522 21:55:44.206591 35003 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:44.206595 35003 net.cpp:137] Memory required for data: 41288448
I0522 21:55:44.206605 35003 layer_factory.hpp:77] Creating layer conv1_1_relu1_1_0_split
I0522 21:55:44.206615 35003 net.cpp:84] Creating Layer conv1_1_relu1_1_0_split
I0522 21:55:44.206619 35003 net.cpp:406] conv1_1_relu1_1_0_split <- conv1_1
I0522 21:55:44.206625 35003 net.cpp:380] conv1_1_relu1_1_0_split -> conv1_1_relu1_1_0_split_0
I0522 21:55:44.206634 35003 net.cpp:380] conv1_1_relu1_1_0_split -> conv1_1_relu1_1_0_split_1
I0522 21:55:44.206670 35003 net.cpp:122] Setting up conv1_1_relu1_1_0_split
I0522 21:55:44.206677 35003 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:44.206681 35003 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:44.206686 35003 net.cpp:137] Memory required for data: 74318592
I0522 21:55:44.206688 35003 layer_factory.hpp:77] Creating layer conv1_3
I0522 21:55:44.206709 35003 net.cpp:84] Creating Layer conv1_3
I0522 21:55:44.206714 35003 net.cpp:406] conv1_3 <- conv1_1_relu1_1_0_split_0
I0522 21:55:44.206720 35003 net.cpp:380] conv1_3 -> conv1_3
I0522 21:55:44.217211 35003 net.cpp:122] Setting up conv1_3
I0522 21:55:44.217229 35003 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:44.217233 35003 net.cpp:137] Memory required for data: 90833664
I0522 21:55:44.217244 35003 layer_factory.hpp:77] Creating layer relu1_3
I0522 21:55:44.217253 35003 net.cpp:84] Creating Layer relu1_3
I0522 21:55:44.217258 35003 net.cpp:406] relu1_3 <- conv1_3
I0522 21:55:44.217264 35003 net.cpp:367] relu1_3 -> conv1_3 (in-place)
I0522 21:55:44.217407 35003 net.cpp:122] Setting up relu1_3
I0522 21:55:44.217417 35003 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:44.217420 35003 net.cpp:137] Memory required for data: 107348736
I0522 21:55:44.217425 35003 layer_factory.hpp:77] Creating layer res1_3
I0522 21:55:44.217433 35003 net.cpp:84] Creating Layer res1_3
I0522 21:55:44.217437 35003 net.cpp:406] res1_3 <- conv1_1_relu1_1_0_split_1
I0522 21:55:44.217442 35003 net.cpp:406] res1_3 <- conv1_3
I0522 21:55:44.217449 35003 net.cpp:380] res1_3 -> res1_3
I0522 21:55:44.217495 35003 net.cpp:122] Setting up res1_3
I0522 21:55:44.217504 35003 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:44.217509 35003 net.cpp:137] Memory required for data: 123863808
I0522 21:55:44.217512 35003 layer_factory.hpp:77] Creating layer res1_3_res1_3_0_split
I0522 21:55:44.217519 35003 net.cpp:84] Creating Layer res1_3_res1_3_0_split
I0522 21:55:44.217525 35003 net.cpp:406] res1_3_res1_3_0_split <- res1_3
I0522 21:55:44.217530 35003 net.cpp:380] res1_3_res1_3_0_split -> res1_3_res1_3_0_split_0
I0522 21:55:44.217538 35003 net.cpp:380] res1_3_res1_3_0_split -> res1_3_res1_3_0_split_1
I0522 21:55:44.217571 35003 net.cpp:122] Setting up res1_3_res1_3_0_split
I0522 21:55:44.217577 35003 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:44.217581 35003 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:44.217584 35003 net.cpp:137] Memory required for data: 156893952
I0522 21:55:44.217587 35003 layer_factory.hpp:77] Creating layer res1_3_reduce
I0522 21:55:44.217599 35003 net.cpp:84] Creating Layer res1_3_reduce
I0522 21:55:44.217604 35003 net.cpp:406] res1_3_reduce <- res1_3_res1_3_0_split_0
I0522 21:55:44.217609 35003 net.cpp:380] res1_3_reduce -> res1_3_reduce
I0522 21:55:44.228075 35003 net.cpp:122] Setting up res1_3_reduce
I0522 21:55:44.228088 35003 net.cpp:129] Top shape: 64 48 56 48 (8257536)
I0522 21:55:44.228092 35003 net.cpp:137] Memory required for data: 189924096
I0522 21:55:44.228102 35003 layer_factory.hpp:77] Creating layer pool1
I0522 21:55:44.228113 35003 net.cpp:84] Creating Layer pool1
I0522 21:55:44.228118 35003 net.cpp:406] pool1 <- res1_3_reduce
I0522 21:55:44.228124 35003 net.cpp:380] pool1 -> pool1
I0522 21:55:44.228188 35003 net.cpp:122] Setting up pool1
I0522 21:55:44.228196 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.228199 35003 net.cpp:137] Memory required for data: 198181632
I0522 21:55:44.228204 35003 layer_factory.hpp:77] Creating layer conv2_1
I0522 21:55:44.228212 35003 net.cpp:84] Creating Layer conv2_1
I0522 21:55:44.228216 35003 net.cpp:406] conv2_1 <- res1_3_res1_3_0_split_1
I0522 21:55:44.228222 35003 net.cpp:380] conv2_1 -> conv2_1
I0522 21:55:44.236462 35003 net.cpp:122] Setting up conv2_1
I0522 21:55:44.236479 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.236483 35003 net.cpp:137] Memory required for data: 206439168
I0522 21:55:44.236495 35003 layer_factory.hpp:77] Creating layer relu2_1
I0522 21:55:44.236505 35003 net.cpp:84] Creating Layer relu2_1
I0522 21:55:44.236511 35003 net.cpp:406] relu2_1 <- conv2_1
I0522 21:55:44.236517 35003 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I0522 21:55:44.236632 35003 net.cpp:122] Setting up relu2_1
I0522 21:55:44.236640 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.236644 35003 net.cpp:137] Memory required for data: 214696704
I0522 21:55:44.236649 35003 layer_factory.hpp:77] Creating layer res1_3_p
I0522 21:55:44.236656 35003 net.cpp:84] Creating Layer res1_3_p
I0522 21:55:44.236660 35003 net.cpp:406] res1_3_p <- pool1
I0522 21:55:44.236665 35003 net.cpp:406] res1_3_p <- conv2_1
I0522 21:55:44.236670 35003 net.cpp:380] res1_3_p -> res1_3_p
I0522 21:55:44.236696 35003 net.cpp:122] Setting up res1_3_p
I0522 21:55:44.236702 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.236706 35003 net.cpp:137] Memory required for data: 222954240
I0522 21:55:44.236709 35003 layer_factory.hpp:77] Creating layer res1_3_p_res1_3_p_0_split
I0522 21:55:44.236714 35003 net.cpp:84] Creating Layer res1_3_p_res1_3_p_0_split
I0522 21:55:44.236718 35003 net.cpp:406] res1_3_p_res1_3_p_0_split <- res1_3_p
I0522 21:55:44.236723 35003 net.cpp:380] res1_3_p_res1_3_p_0_split -> res1_3_p_res1_3_p_0_split_0
I0522 21:55:44.236729 35003 net.cpp:380] res1_3_p_res1_3_p_0_split -> res1_3_p_res1_3_p_0_split_1
I0522 21:55:44.236760 35003 net.cpp:122] Setting up res1_3_p_res1_3_p_0_split
I0522 21:55:44.236766 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.236771 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.236774 35003 net.cpp:137] Memory required for data: 239469312
I0522 21:55:44.236791 35003 layer_factory.hpp:77] Creating layer conv2_3
I0522 21:55:44.236801 35003 net.cpp:84] Creating Layer conv2_3
I0522 21:55:44.236805 35003 net.cpp:406] conv2_3 <- res1_3_p_res1_3_p_0_split_0
I0522 21:55:44.236812 35003 net.cpp:380] conv2_3 -> conv2_3
I0522 21:55:44.248386 35003 net.cpp:122] Setting up conv2_3
I0522 21:55:44.248402 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.248406 35003 net.cpp:137] Memory required for data: 247726848
I0522 21:55:44.248414 35003 layer_factory.hpp:77] Creating layer relu2_3
I0522 21:55:44.248426 35003 net.cpp:84] Creating Layer relu2_3
I0522 21:55:44.248431 35003 net.cpp:406] relu2_3 <- conv2_3
I0522 21:55:44.248436 35003 net.cpp:367] relu2_3 -> conv2_3 (in-place)
I0522 21:55:44.248554 35003 net.cpp:122] Setting up relu2_3
I0522 21:55:44.248564 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.248566 35003 net.cpp:137] Memory required for data: 255984384
I0522 21:55:44.248574 35003 layer_factory.hpp:77] Creating layer res2_3
I0522 21:55:44.248580 35003 net.cpp:84] Creating Layer res2_3
I0522 21:55:44.248586 35003 net.cpp:406] res2_3 <- res1_3_p_res1_3_p_0_split_1
I0522 21:55:44.248590 35003 net.cpp:406] res2_3 <- conv2_3
I0522 21:55:44.248596 35003 net.cpp:380] res2_3 -> res2_3
I0522 21:55:44.248621 35003 net.cpp:122] Setting up res2_3
I0522 21:55:44.248628 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.248631 35003 net.cpp:137] Memory required for data: 264241920
I0522 21:55:44.248636 35003 layer_factory.hpp:77] Creating layer res2_3_res2_3_0_split
I0522 21:55:44.248641 35003 net.cpp:84] Creating Layer res2_3_res2_3_0_split
I0522 21:55:44.248646 35003 net.cpp:406] res2_3_res2_3_0_split <- res2_3
I0522 21:55:44.248651 35003 net.cpp:380] res2_3_res2_3_0_split -> res2_3_res2_3_0_split_0
I0522 21:55:44.248658 35003 net.cpp:380] res2_3_res2_3_0_split -> res2_3_res2_3_0_split_1
I0522 21:55:44.248698 35003 net.cpp:122] Setting up res2_3_res2_3_0_split
I0522 21:55:44.248704 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.248708 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.248711 35003 net.cpp:137] Memory required for data: 280756992
I0522 21:55:44.248714 35003 layer_factory.hpp:77] Creating layer conv2_5
I0522 21:55:44.248724 35003 net.cpp:84] Creating Layer conv2_5
I0522 21:55:44.248729 35003 net.cpp:406] conv2_5 <- res2_3_res2_3_0_split_0
I0522 21:55:44.248734 35003 net.cpp:380] conv2_5 -> conv2_5
I0522 21:55:44.252511 35003 net.cpp:122] Setting up conv2_5
I0522 21:55:44.252526 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.252532 35003 net.cpp:137] Memory required for data: 289014528
I0522 21:55:44.252540 35003 layer_factory.hpp:77] Creating layer relu2_5
I0522 21:55:44.252549 35003 net.cpp:84] Creating Layer relu2_5
I0522 21:55:44.252555 35003 net.cpp:406] relu2_5 <- conv2_5
I0522 21:55:44.252560 35003 net.cpp:367] relu2_5 -> conv2_5 (in-place)
I0522 21:55:44.252676 35003 net.cpp:122] Setting up relu2_5
I0522 21:55:44.252683 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.252687 35003 net.cpp:137] Memory required for data: 297272064
I0522 21:55:44.252696 35003 layer_factory.hpp:77] Creating layer res2_5
I0522 21:55:44.252703 35003 net.cpp:84] Creating Layer res2_5
I0522 21:55:44.252707 35003 net.cpp:406] res2_5 <- res2_3_res2_3_0_split_1
I0522 21:55:44.252712 35003 net.cpp:406] res2_5 <- conv2_5
I0522 21:55:44.252717 35003 net.cpp:380] res2_5 -> res2_5
I0522 21:55:44.252739 35003 net.cpp:122] Setting up res2_5
I0522 21:55:44.252746 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.252750 35003 net.cpp:137] Memory required for data: 305529600
I0522 21:55:44.252753 35003 layer_factory.hpp:77] Creating layer res2_5_res2_5_0_split
I0522 21:55:44.252758 35003 net.cpp:84] Creating Layer res2_5_res2_5_0_split
I0522 21:55:44.252761 35003 net.cpp:406] res2_5_res2_5_0_split <- res2_5
I0522 21:55:44.252766 35003 net.cpp:380] res2_5_res2_5_0_split -> res2_5_res2_5_0_split_0
I0522 21:55:44.252786 35003 net.cpp:380] res2_5_res2_5_0_split -> res2_5_res2_5_0_split_1
I0522 21:55:44.252820 35003 net.cpp:122] Setting up res2_5_res2_5_0_split
I0522 21:55:44.252826 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.252831 35003 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:44.252833 35003 net.cpp:137] Memory required for data: 322044672
I0522 21:55:44.252837 35003 layer_factory.hpp:77] Creating layer res2_5_reduce
I0522 21:55:44.252846 35003 net.cpp:84] Creating Layer res2_5_reduce
I0522 21:55:44.252851 35003 net.cpp:406] res2_5_reduce <- res2_5_res2_5_0_split_0
I0522 21:55:44.252857 35003 net.cpp:380] res2_5_reduce -> res2_5_reduce
I0522 21:55:44.259984 35003 net.cpp:122] Setting up res2_5_reduce
I0522 21:55:44.260001 35003 net.cpp:129] Top shape: 64 72 28 24 (3096576)
I0522 21:55:44.260006 35003 net.cpp:137] Memory required for data: 334430976
I0522 21:55:44.260015 35003 layer_factory.hpp:77] Creating layer pool2
I0522 21:55:44.260026 35003 net.cpp:84] Creating Layer pool2
I0522 21:55:44.260032 35003 net.cpp:406] pool2 <- res2_5_reduce
I0522 21:55:44.260038 35003 net.cpp:380] pool2 -> pool2
I0522 21:55:44.260078 35003 net.cpp:122] Setting up pool2
I0522 21:55:44.260087 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.260089 35003 net.cpp:137] Memory required for data: 337527552
I0522 21:55:44.260092 35003 layer_factory.hpp:77] Creating layer conv3_1
I0522 21:55:44.260102 35003 net.cpp:84] Creating Layer conv3_1
I0522 21:55:44.260107 35003 net.cpp:406] conv3_1 <- res2_5_res2_5_0_split_1
I0522 21:55:44.260113 35003 net.cpp:380] conv3_1 -> conv3_1
I0522 21:55:44.264422 35003 net.cpp:122] Setting up conv3_1
I0522 21:55:44.264439 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.264443 35003 net.cpp:137] Memory required for data: 340624128
I0522 21:55:44.264451 35003 layer_factory.hpp:77] Creating layer relu3_1
I0522 21:55:44.264463 35003 net.cpp:84] Creating Layer relu3_1
I0522 21:55:44.264469 35003 net.cpp:406] relu3_1 <- conv3_1
I0522 21:55:44.264475 35003 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I0522 21:55:44.264578 35003 net.cpp:122] Setting up relu3_1
I0522 21:55:44.264586 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.264590 35003 net.cpp:137] Memory required for data: 343720704
I0522 21:55:44.264600 35003 layer_factory.hpp:77] Creating layer res2_5_p
I0522 21:55:44.264607 35003 net.cpp:84] Creating Layer res2_5_p
I0522 21:55:44.264611 35003 net.cpp:406] res2_5_p <- pool2
I0522 21:55:44.264616 35003 net.cpp:406] res2_5_p <- conv3_1
I0522 21:55:44.264621 35003 net.cpp:380] res2_5_p -> res2_5_p
I0522 21:55:44.264645 35003 net.cpp:122] Setting up res2_5_p
I0522 21:55:44.264652 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.264655 35003 net.cpp:137] Memory required for data: 346817280
I0522 21:55:44.264658 35003 layer_factory.hpp:77] Creating layer res2_5_p_res2_5_p_0_split
I0522 21:55:44.264664 35003 net.cpp:84] Creating Layer res2_5_p_res2_5_p_0_split
I0522 21:55:44.264668 35003 net.cpp:406] res2_5_p_res2_5_p_0_split <- res2_5_p
I0522 21:55:44.264673 35003 net.cpp:380] res2_5_p_res2_5_p_0_split -> res2_5_p_res2_5_p_0_split_0
I0522 21:55:44.264680 35003 net.cpp:380] res2_5_p_res2_5_p_0_split -> res2_5_p_res2_5_p_0_split_1
I0522 21:55:44.264710 35003 net.cpp:122] Setting up res2_5_p_res2_5_p_0_split
I0522 21:55:44.264716 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.264721 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.264724 35003 net.cpp:137] Memory required for data: 353010432
I0522 21:55:44.264727 35003 layer_factory.hpp:77] Creating layer conv3_3
I0522 21:55:44.264736 35003 net.cpp:84] Creating Layer conv3_3
I0522 21:55:44.264740 35003 net.cpp:406] conv3_3 <- res2_5_p_res2_5_p_0_split_0
I0522 21:55:44.264746 35003 net.cpp:380] conv3_3 -> conv3_3
I0522 21:55:44.269202 35003 net.cpp:122] Setting up conv3_3
I0522 21:55:44.269218 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.269222 35003 net.cpp:137] Memory required for data: 356107008
I0522 21:55:44.269243 35003 layer_factory.hpp:77] Creating layer relu3_3
I0522 21:55:44.269253 35003 net.cpp:84] Creating Layer relu3_3
I0522 21:55:44.269258 35003 net.cpp:406] relu3_3 <- conv3_3
I0522 21:55:44.269264 35003 net.cpp:367] relu3_3 -> conv3_3 (in-place)
I0522 21:55:44.269378 35003 net.cpp:122] Setting up relu3_3
I0522 21:55:44.269387 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.269389 35003 net.cpp:137] Memory required for data: 359203584
I0522 21:55:44.269398 35003 layer_factory.hpp:77] Creating layer res3_3
I0522 21:55:44.269407 35003 net.cpp:84] Creating Layer res3_3
I0522 21:55:44.269412 35003 net.cpp:406] res3_3 <- res2_5_p_res2_5_p_0_split_1
I0522 21:55:44.269417 35003 net.cpp:406] res3_3 <- conv3_3
I0522 21:55:44.269423 35003 net.cpp:380] res3_3 -> res3_3
I0522 21:55:44.269451 35003 net.cpp:122] Setting up res3_3
I0522 21:55:44.269459 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.269462 35003 net.cpp:137] Memory required for data: 362300160
I0522 21:55:44.269465 35003 layer_factory.hpp:77] Creating layer res3_3_res3_3_0_split
I0522 21:55:44.269475 35003 net.cpp:84] Creating Layer res3_3_res3_3_0_split
I0522 21:55:44.269480 35003 net.cpp:406] res3_3_res3_3_0_split <- res3_3
I0522 21:55:44.269486 35003 net.cpp:380] res3_3_res3_3_0_split -> res3_3_res3_3_0_split_0
I0522 21:55:44.269493 35003 net.cpp:380] res3_3_res3_3_0_split -> res3_3_res3_3_0_split_1
I0522 21:55:44.269524 35003 net.cpp:122] Setting up res3_3_res3_3_0_split
I0522 21:55:44.269531 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.269536 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.269538 35003 net.cpp:137] Memory required for data: 368493312
I0522 21:55:44.269542 35003 layer_factory.hpp:77] Creating layer conv3_5
I0522 21:55:44.269554 35003 net.cpp:84] Creating Layer conv3_5
I0522 21:55:44.269559 35003 net.cpp:406] conv3_5 <- res3_3_res3_3_0_split_0
I0522 21:55:44.269567 35003 net.cpp:380] conv3_5 -> conv3_5
I0522 21:55:44.276234 35003 net.cpp:122] Setting up conv3_5
I0522 21:55:44.276252 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.276257 35003 net.cpp:137] Memory required for data: 371589888
I0522 21:55:44.276266 35003 layer_factory.hpp:77] Creating layer relu3_5
I0522 21:55:44.276278 35003 net.cpp:84] Creating Layer relu3_5
I0522 21:55:44.276283 35003 net.cpp:406] relu3_5 <- conv3_5
I0522 21:55:44.276289 35003 net.cpp:367] relu3_5 -> conv3_5 (in-place)
I0522 21:55:44.276401 35003 net.cpp:122] Setting up relu3_5
I0522 21:55:44.276409 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.276412 35003 net.cpp:137] Memory required for data: 374686464
I0522 21:55:44.276418 35003 layer_factory.hpp:77] Creating layer res3_5
I0522 21:55:44.276425 35003 net.cpp:84] Creating Layer res3_5
I0522 21:55:44.276429 35003 net.cpp:406] res3_5 <- res3_3_res3_3_0_split_1
I0522 21:55:44.276433 35003 net.cpp:406] res3_5 <- conv3_5
I0522 21:55:44.276439 35003 net.cpp:380] res3_5 -> res3_5
I0522 21:55:44.276464 35003 net.cpp:122] Setting up res3_5
I0522 21:55:44.276471 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.276474 35003 net.cpp:137] Memory required for data: 377783040
I0522 21:55:44.276477 35003 layer_factory.hpp:77] Creating layer res3_5_res3_5_0_split
I0522 21:55:44.276484 35003 net.cpp:84] Creating Layer res3_5_res3_5_0_split
I0522 21:55:44.276489 35003 net.cpp:406] res3_5_res3_5_0_split <- res3_5
I0522 21:55:44.276494 35003 net.cpp:380] res3_5_res3_5_0_split -> res3_5_res3_5_0_split_0
I0522 21:55:44.276499 35003 net.cpp:380] res3_5_res3_5_0_split -> res3_5_res3_5_0_split_1
I0522 21:55:44.276531 35003 net.cpp:122] Setting up res3_5_res3_5_0_split
I0522 21:55:44.276537 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.276542 35003 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:44.276545 35003 net.cpp:137] Memory required for data: 383976192
I0522 21:55:44.276547 35003 layer_factory.hpp:77] Creating layer res3_5_reduce
I0522 21:55:44.276557 35003 net.cpp:84] Creating Layer res3_5_reduce
I0522 21:55:44.276576 35003 net.cpp:406] res3_5_reduce <- res3_5_res3_5_0_split_0
I0522 21:55:44.276583 35003 net.cpp:380] res3_5_reduce -> res3_5_reduce
I0522 21:55:44.285702 35003 net.cpp:122] Setting up res3_5_reduce
I0522 21:55:44.285719 35003 net.cpp:129] Top shape: 64 144 14 12 (1548288)
I0522 21:55:44.285724 35003 net.cpp:137] Memory required for data: 390169344
I0522 21:55:44.285733 35003 layer_factory.hpp:77] Creating layer pool3
I0522 21:55:44.285743 35003 net.cpp:84] Creating Layer pool3
I0522 21:55:44.285748 35003 net.cpp:406] pool3 <- res3_5_reduce
I0522 21:55:44.285753 35003 net.cpp:380] pool3 -> pool3
I0522 21:55:44.285796 35003 net.cpp:122] Setting up pool3
I0522 21:55:44.285804 35003 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:44.285807 35003 net.cpp:137] Memory required for data: 391717632
I0522 21:55:44.285812 35003 layer_factory.hpp:77] Creating layer conv4_1
I0522 21:55:44.285825 35003 net.cpp:84] Creating Layer conv4_1
I0522 21:55:44.285830 35003 net.cpp:406] conv4_1 <- res3_5_res3_5_0_split_1
I0522 21:55:44.285837 35003 net.cpp:380] conv4_1 -> conv4_1
I0522 21:55:44.298847 35003 net.cpp:122] Setting up conv4_1
I0522 21:55:44.298866 35003 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:44.298869 35003 net.cpp:137] Memory required for data: 393265920
I0522 21:55:44.298877 35003 layer_factory.hpp:77] Creating layer relu4_1
I0522 21:55:44.298885 35003 net.cpp:84] Creating Layer relu4_1
I0522 21:55:44.298892 35003 net.cpp:406] relu4_1 <- conv4_1
I0522 21:55:44.298897 35003 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I0522 21:55:44.298996 35003 net.cpp:122] Setting up relu4_1
I0522 21:55:44.299005 35003 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:44.299008 35003 net.cpp:137] Memory required for data: 394814208
I0522 21:55:44.299023 35003 layer_factory.hpp:77] Creating layer res3_5_p
I0522 21:55:44.299031 35003 net.cpp:84] Creating Layer res3_5_p
I0522 21:55:44.299036 35003 net.cpp:406] res3_5_p <- pool3
I0522 21:55:44.299041 35003 net.cpp:406] res3_5_p <- conv4_1
I0522 21:55:44.299047 35003 net.cpp:380] res3_5_p -> res3_5_p
I0522 21:55:44.299075 35003 net.cpp:122] Setting up res3_5_p
I0522 21:55:44.299082 35003 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:44.299085 35003 net.cpp:137] Memory required for data: 396362496
I0522 21:55:44.299089 35003 layer_factory.hpp:77] Creating layer res3_5_p_res3_5_p_0_split
I0522 21:55:44.299096 35003 net.cpp:84] Creating Layer res3_5_p_res3_5_p_0_split
I0522 21:55:44.299101 35003 net.cpp:406] res3_5_p_res3_5_p_0_split <- res3_5_p
I0522 21:55:44.299108 35003 net.cpp:380] res3_5_p_res3_5_p_0_split -> res3_5_p_res3_5_p_0_split_0
I0522 21:55:44.299115 35003 net.cpp:380] res3_5_p_res3_5_p_0_split -> res3_5_p_res3_5_p_0_split_1
I0522 21:55:44.299150 35003 net.cpp:122] Setting up res3_5_p_res3_5_p_0_split
I0522 21:55:44.299157 35003 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:44.299161 35003 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:44.299165 35003 net.cpp:137] Memory required for data: 399459072
I0522 21:55:44.299170 35003 layer_factory.hpp:77] Creating layer conv4_3
I0522 21:55:44.299185 35003 net.cpp:84] Creating Layer conv4_3
I0522 21:55:44.299190 35003 net.cpp:406] conv4_3 <- res3_5_p_res3_5_p_0_split_0
I0522 21:55:44.299198 35003 net.cpp:380] conv4_3 -> conv4_3
I0522 21:55:44.312296 35003 net.cpp:122] Setting up conv4_3
I0522 21:55:44.312312 35003 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:44.312317 35003 net.cpp:137] Memory required for data: 401007360
I0522 21:55:44.312325 35003 layer_factory.hpp:77] Creating layer relu4_3
I0522 21:55:44.312337 35003 net.cpp:84] Creating Layer relu4_3
I0522 21:55:44.312343 35003 net.cpp:406] relu4_3 <- conv4_3
I0522 21:55:44.312350 35003 net.cpp:367] relu4_3 -> conv4_3 (in-place)
I0522 21:55:44.312460 35003 net.cpp:122] Setting up relu4_3
I0522 21:55:44.312469 35003 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:44.312471 35003 net.cpp:137] Memory required for data: 402555648
I0522 21:55:44.312477 35003 layer_factory.hpp:77] Creating layer res4_3
I0522 21:55:44.312499 35003 net.cpp:84] Creating Layer res4_3
I0522 21:55:44.312505 35003 net.cpp:406] res4_3 <- res3_5_p_res3_5_p_0_split_1
I0522 21:55:44.312510 35003 net.cpp:406] res4_3 <- conv4_3
I0522 21:55:44.312515 35003 net.cpp:380] res4_3 -> res4_3
I0522 21:55:44.312542 35003 net.cpp:122] Setting up res4_3
I0522 21:55:44.312549 35003 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:44.312553 35003 net.cpp:137] Memory required for data: 404103936
I0522 21:55:44.312558 35003 layer_factory.hpp:77] Creating layer fc5
I0522 21:55:44.312567 35003 net.cpp:84] Creating Layer fc5
I0522 21:55:44.312572 35003 net.cpp:406] fc5 <- res4_3
I0522 21:55:44.312578 35003 net.cpp:380] fc5 -> fc5
I0522 21:55:44.325120 35003 net.cpp:122] Setting up fc5
I0522 21:55:44.325139 35003 net.cpp:129] Top shape: 64 256 (16384)
I0522 21:55:44.325142 35003 net.cpp:137] Memory required for data: 404169472
I0522 21:55:44.325150 35003 layer_factory.hpp:77] Creating layer fc5_bn
I0522 21:55:44.325165 35003 net.cpp:84] Creating Layer fc5_bn
I0522 21:55:44.325170 35003 net.cpp:406] fc5_bn <- fc5
I0522 21:55:44.325176 35003 net.cpp:367] fc5_bn -> fc5 (in-place)
I0522 21:55:44.325357 35003 net.cpp:122] Setting up fc5_bn
I0522 21:55:44.325366 35003 net.cpp:129] Top shape: 64 256 (16384)
I0522 21:55:44.325369 35003 net.cpp:137] Memory required for data: 404235008
I0522 21:55:44.325377 35003 layer_factory.hpp:77] Creating layer norm1
I0522 21:55:44.325388 35003 net.cpp:84] Creating Layer norm1
I0522 21:55:44.325393 35003 net.cpp:406] norm1 <- fc5
I0522 21:55:44.325399 35003 net.cpp:380] norm1 -> norm1
I0522 21:55:44.325456 35003 net.cpp:122] Setting up norm1
I0522 21:55:44.325464 35003 net.cpp:129] Top shape: 64 256 1 1 (16384)
I0522 21:55:44.325466 35003 net.cpp:137] Memory required for data: 404300544
I0522 21:55:44.325470 35003 layer_factory.hpp:77] Creating layer fc-6_l2
I0522 21:55:44.325479 35003 net.cpp:84] Creating Layer fc-6_l2
I0522 21:55:44.325484 35003 net.cpp:406] fc-6_l2 <- norm1
I0522 21:55:44.325489 35003 net.cpp:380] fc-6_l2 -> fc-6_l2
I0522 21:55:44.368712 35003 net.cpp:122] Setting up fc-6_l2
I0522 21:55:44.368743 35003 net.cpp:129] Top shape: 64 21331 (1365184)
I0522 21:55:44.368748 35003 net.cpp:137] Memory required for data: 409761280
I0522 21:55:44.368757 35003 layer_factory.hpp:77] Creating layer fc-6_margin
I0522 21:55:44.368780 35003 net.cpp:84] Creating Layer fc-6_margin
I0522 21:55:44.368788 35003 net.cpp:406] fc-6_margin <- fc-6_l2
I0522 21:55:44.368794 35003 net.cpp:406] fc-6_margin <- label_data_1_split_0
I0522 21:55:44.368800 35003 net.cpp:380] fc-6_margin -> fc-6_margin
I0522 21:55:44.368840 35003 net.cpp:122] Setting up fc-6_margin
I0522 21:55:44.368849 35003 net.cpp:129] Top shape: 64 21331 (1365184)
I0522 21:55:44.368854 35003 net.cpp:137] Memory required for data: 415222016
I0522 21:55:44.368856 35003 layer_factory.hpp:77] Creating layer fc-6_margin_scale
I0522 21:55:44.368871 35003 net.cpp:84] Creating Layer fc-6_margin_scale
I0522 21:55:44.368876 35003 net.cpp:406] fc-6_margin_scale <- fc-6_margin
I0522 21:55:44.368883 35003 net.cpp:380] fc-6_margin_scale -> fc-6_margin_scale
I0522 21:55:44.369022 35003 net.cpp:122] Setting up fc-6_margin_scale
I0522 21:55:44.369031 35003 net.cpp:129] Top shape: 64 21331 (1365184)
I0522 21:55:44.369035 35003 net.cpp:137] Memory required for data: 420682752
I0522 21:55:44.369040 35003 layer_factory.hpp:77] Creating layer softmax_loss
I0522 21:55:44.369046 35003 net.cpp:84] Creating Layer softmax_loss
I0522 21:55:44.369050 35003 net.cpp:406] softmax_loss <- fc-6_margin_scale
I0522 21:55:44.369055 35003 net.cpp:406] softmax_loss <- label_data_1_split_1
I0522 21:55:44.369061 35003 net.cpp:380] softmax_loss -> softmax_loss
I0522 21:55:44.369072 35003 layer_factory.hpp:77] Creating layer softmax_loss
I0522 21:55:44.373597 35003 net.cpp:122] Setting up softmax_loss
I0522 21:55:44.373617 35003 net.cpp:129] Top shape: (1)
I0522 21:55:44.373622 35003 net.cpp:132]     with loss weight 1
I0522 21:55:44.373643 35003 net.cpp:137] Memory required for data: 420682756
I0522 21:55:44.373648 35003 net.cpp:198] softmax_loss needs backward computation.
I0522 21:55:44.373673 35003 net.cpp:198] fc-6_margin_scale needs backward computation.
I0522 21:55:44.373678 35003 net.cpp:198] fc-6_margin needs backward computation.
I0522 21:55:44.373683 35003 net.cpp:198] fc-6_l2 needs backward computation.
I0522 21:55:44.373687 35003 net.cpp:198] norm1 needs backward computation.
I0522 21:55:44.373690 35003 net.cpp:198] fc5_bn needs backward computation.
I0522 21:55:44.373693 35003 net.cpp:198] fc5 needs backward computation.
I0522 21:55:44.373697 35003 net.cpp:198] res4_3 needs backward computation.
I0522 21:55:44.373702 35003 net.cpp:198] relu4_3 needs backward computation.
I0522 21:55:44.373705 35003 net.cpp:198] conv4_3 needs backward computation.
I0522 21:55:44.373709 35003 net.cpp:198] res3_5_p_res3_5_p_0_split needs backward computation.
I0522 21:55:44.373713 35003 net.cpp:198] res3_5_p needs backward computation.
I0522 21:55:44.373718 35003 net.cpp:198] relu4_1 needs backward computation.
I0522 21:55:44.373721 35003 net.cpp:198] conv4_1 needs backward computation.
I0522 21:55:44.373725 35003 net.cpp:198] pool3 needs backward computation.
I0522 21:55:44.373729 35003 net.cpp:198] res3_5_reduce needs backward computation.
I0522 21:55:44.373733 35003 net.cpp:198] res3_5_res3_5_0_split needs backward computation.
I0522 21:55:44.373737 35003 net.cpp:198] res3_5 needs backward computation.
I0522 21:55:44.373741 35003 net.cpp:198] relu3_5 needs backward computation.
I0522 21:55:44.373744 35003 net.cpp:198] conv3_5 needs backward computation.
I0522 21:55:44.373749 35003 net.cpp:198] res3_3_res3_3_0_split needs backward computation.
I0522 21:55:44.373752 35003 net.cpp:198] res3_3 needs backward computation.
I0522 21:55:44.373757 35003 net.cpp:198] relu3_3 needs backward computation.
I0522 21:55:44.373787 35003 net.cpp:198] conv3_3 needs backward computation.
I0522 21:55:44.373791 35003 net.cpp:198] res2_5_p_res2_5_p_0_split needs backward computation.
I0522 21:55:44.373795 35003 net.cpp:198] res2_5_p needs backward computation.
I0522 21:55:44.373800 35003 net.cpp:198] relu3_1 needs backward computation.
I0522 21:55:44.373803 35003 net.cpp:198] conv3_1 needs backward computation.
I0522 21:55:44.373807 35003 net.cpp:198] pool2 needs backward computation.
I0522 21:55:44.373811 35003 net.cpp:198] res2_5_reduce needs backward computation.
I0522 21:55:44.373816 35003 net.cpp:198] res2_5_res2_5_0_split needs backward computation.
I0522 21:55:44.373819 35003 net.cpp:198] res2_5 needs backward computation.
I0522 21:55:44.373824 35003 net.cpp:198] relu2_5 needs backward computation.
I0522 21:55:44.373828 35003 net.cpp:198] conv2_5 needs backward computation.
I0522 21:55:44.373832 35003 net.cpp:198] res2_3_res2_3_0_split needs backward computation.
I0522 21:55:44.373836 35003 net.cpp:198] res2_3 needs backward computation.
I0522 21:55:44.373841 35003 net.cpp:198] relu2_3 needs backward computation.
I0522 21:55:44.373844 35003 net.cpp:198] conv2_3 needs backward computation.
I0522 21:55:44.373848 35003 net.cpp:198] res1_3_p_res1_3_p_0_split needs backward computation.
I0522 21:55:44.373852 35003 net.cpp:198] res1_3_p needs backward computation.
I0522 21:55:44.373857 35003 net.cpp:198] relu2_1 needs backward computation.
I0522 21:55:44.373860 35003 net.cpp:198] conv2_1 needs backward computation.
I0522 21:55:44.373864 35003 net.cpp:198] pool1 needs backward computation.
I0522 21:55:44.373868 35003 net.cpp:198] res1_3_reduce needs backward computation.
I0522 21:55:44.373872 35003 net.cpp:198] res1_3_res1_3_0_split needs backward computation.
I0522 21:55:44.373875 35003 net.cpp:198] res1_3 needs backward computation.
I0522 21:55:44.373880 35003 net.cpp:198] relu1_3 needs backward computation.
I0522 21:55:44.373884 35003 net.cpp:198] conv1_3 needs backward computation.
I0522 21:55:44.373888 35003 net.cpp:198] conv1_1_relu1_1_0_split needs backward computation.
I0522 21:55:44.373891 35003 net.cpp:198] relu1_1 needs backward computation.
I0522 21:55:44.373894 35003 net.cpp:198] conv1_1 needs backward computation.
I0522 21:55:44.373899 35003 net.cpp:200] label_data_1_split does not need backward computation.
I0522 21:55:44.373910 35003 net.cpp:200] data does not need backward computation.
I0522 21:55:44.373914 35003 net.cpp:242] This network produces output softmax_loss
I0522 21:55:44.373952 35003 net.cpp:255] Network initialization done.
I0522 21:55:44.374130 35003 solver.cpp:57] Solver scaffolding done.
I0522 21:55:44.375612 35003 caffe.cpp:235] Resuming from ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_75265.solverstate
I0522 21:55:48.920449 35003 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_75265.caffemodel
I0522 21:55:48.920493 35003 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:55:48.930153 35003 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:55:48.954862 35003 caffe.cpp:239] Starting Optimization
I0522 21:55:53.229491 35143 image_data_layer.cpp:38] Opening file /home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/image_list/train_list/shot_Oriental_Age_Lan_DHUA_PAKJ_Indon_Migrant_celeb_label.txt
I0522 21:55:53.249698 35144 image_data_layer.cpp:38] Opening file /home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/image_list/train_list/shot_Oriental_Age_Lan_DHUA_PAKJ_Indon_Migrant_celeb_label.txt
I0522 21:55:53.963866 35145 image_data_layer.cpp:38] Opening file /home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/image_list/train_list/shot_Oriental_Age_Lan_DHUA_PAKJ_Indon_Migrant_celeb_label.txt
I0522 21:56:02.131795 35144 image_data_layer.cpp:53] Shuffling data
I0522 21:56:02.142545 35143 image_data_layer.cpp:53] Shuffling data
I0522 21:56:02.172191 35145 image_data_layer.cpp:53] Shuffling data
I0522 21:56:02.430057 35143 image_data_layer.cpp:63] A total of 1134127 images.
I0522 21:56:02.434330 35143 image_data_layer.cpp:90] output data size: 64,3,112,96
I0522 21:56:02.454027 35145 image_data_layer.cpp:63] A total of 1134127 images.
I0522 21:56:02.457250 35144 image_data_layer.cpp:63] A total of 1134127 images.
I0522 21:56:02.467808 35145 image_data_layer.cpp:90] output data size: 64,3,112,96
I0522 21:56:02.471258 35144 image_data_layer.cpp:90] output data size: 64,3,112,96
I0522 21:56:12.450990 35143 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_75265.caffemodel
I0522 21:56:12.451092 35143 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:56:12.460752 35143 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:56:13.547852 35145 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_75265.caffemodel
I0522 21:56:13.547852 35144 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_75265.caffemodel
I0522 21:56:13.547904 35145 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:56:13.547909 35144 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:56:13.558305 35145 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:56:13.558333 35144 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:56:13.920537 35003 solver.cpp:293] Solving 2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_train
I0522 21:56:13.920564 35003 solver.cpp:294] Learning Rate Policy: multistep
I0522 21:56:15.514837 35003 solver.cpp:239] Iteration 75270 (47752.2 iter/s, 1.57626s/10 iters), loss = 8.12114
I0522 21:56:15.514891 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.12114 (* 1 = 8.12114 loss)
I0522 21:56:16.249285 35003 sgd_solver.cpp:112] Iteration 75270, lr = 0.01
I0522 21:56:18.199537 35003 solver.cpp:239] Iteration 75280 (3.72505 iter/s, 2.68453s/10 iters), loss = 7.37423
I0522 21:56:18.199586 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37423 (* 1 = 7.37423 loss)
I0522 21:56:18.212436 35003 sgd_solver.cpp:112] Iteration 75280, lr = 0.01
I0522 21:56:21.324714 35003 solver.cpp:239] Iteration 75290 (3.20001 iter/s, 3.12499s/10 iters), loss = 8.53106
I0522 21:56:21.324764 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.53106 (* 1 = 8.53106 loss)
I0522 21:56:22.058878 35003 sgd_solver.cpp:112] Iteration 75290, lr = 0.01
I0522 21:56:25.629600 35003 solver.cpp:239] Iteration 75300 (2.32307 iter/s, 4.30465s/10 iters), loss = 8.39949
I0522 21:56:25.629653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.39949 (* 1 = 8.39949 loss)
I0522 21:56:26.351064 35003 sgd_solver.cpp:112] Iteration 75300, lr = 0.01
I0522 21:56:29.144420 35003 solver.cpp:239] Iteration 75310 (2.84526 iter/s, 3.51462s/10 iters), loss = 7.82025
I0522 21:56:29.144466 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82025 (* 1 = 7.82025 loss)
I0522 21:56:29.492830 35003 sgd_solver.cpp:112] Iteration 75310, lr = 0.01
I0522 21:56:32.205704 35003 solver.cpp:239] Iteration 75320 (3.2668 iter/s, 3.0611s/10 iters), loss = 8.61728
I0522 21:56:32.205745 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.61728 (* 1 = 8.61728 loss)
I0522 21:56:32.218618 35003 sgd_solver.cpp:112] Iteration 75320, lr = 0.01
I0522 21:56:35.962831 35003 solver.cpp:239] Iteration 75330 (2.66175 iter/s, 3.75692s/10 iters), loss = 7.78474
I0522 21:56:35.962898 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78474 (* 1 = 7.78474 loss)
I0522 21:56:35.972004 35003 sgd_solver.cpp:112] Iteration 75330, lr = 0.01
I0522 21:56:38.737104 35003 solver.cpp:239] Iteration 75340 (3.60479 iter/s, 2.77409s/10 iters), loss = 6.16476
I0522 21:56:38.737157 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16476 (* 1 = 6.16476 loss)
I0522 21:56:39.042009 35003 sgd_solver.cpp:112] Iteration 75340, lr = 0.01
I0522 21:56:40.751150 35003 solver.cpp:239] Iteration 75350 (4.9655 iter/s, 2.01389s/10 iters), loss = 6.73498
I0522 21:56:40.751235 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73498 (* 1 = 6.73498 loss)
I0522 21:56:40.764411 35003 sgd_solver.cpp:112] Iteration 75350, lr = 0.01
I0522 21:56:45.189390 35003 solver.cpp:239] Iteration 75360 (2.25328 iter/s, 4.43797s/10 iters), loss = 7.83644
I0522 21:56:45.189611 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83644 (* 1 = 7.83644 loss)
I0522 21:56:45.203637 35003 sgd_solver.cpp:112] Iteration 75360, lr = 0.01
I0522 21:56:49.585355 35003 solver.cpp:239] Iteration 75370 (2.27502 iter/s, 4.39557s/10 iters), loss = 7.51401
I0522 21:56:49.585418 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51401 (* 1 = 7.51401 loss)
I0522 21:56:49.741058 35003 sgd_solver.cpp:112] Iteration 75370, lr = 0.01
I0522 21:56:53.881222 35003 solver.cpp:239] Iteration 75380 (2.32796 iter/s, 4.29561s/10 iters), loss = 7.14111
I0522 21:56:53.881271 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14111 (* 1 = 7.14111 loss)
I0522 21:56:53.894515 35003 sgd_solver.cpp:112] Iteration 75380, lr = 0.01
I0522 21:56:57.797096 35003 solver.cpp:239] Iteration 75390 (2.55384 iter/s, 3.91567s/10 iters), loss = 8.3763
I0522 21:56:57.797163 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.3763 (* 1 = 8.3763 loss)
I0522 21:56:58.382220 35003 sgd_solver.cpp:112] Iteration 75390, lr = 0.01
I0522 21:57:02.288066 35003 solver.cpp:239] Iteration 75400 (2.2268 iter/s, 4.49074s/10 iters), loss = 6.94813
I0522 21:57:02.288117 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94813 (* 1 = 6.94813 loss)
I0522 21:57:03.010499 35003 sgd_solver.cpp:112] Iteration 75400, lr = 0.01
I0522 21:57:04.858718 35003 solver.cpp:239] Iteration 75410 (3.89032 iter/s, 2.57048s/10 iters), loss = 7.3742
I0522 21:57:04.858760 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3742 (* 1 = 7.3742 loss)
I0522 21:57:04.872355 35003 sgd_solver.cpp:112] Iteration 75410, lr = 0.01
I0522 21:57:06.946240 35003 solver.cpp:239] Iteration 75420 (4.79071 iter/s, 2.08737s/10 iters), loss = 8.08622
I0522 21:57:06.946283 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08622 (* 1 = 8.08622 loss)
I0522 21:57:06.964524 35003 sgd_solver.cpp:112] Iteration 75420, lr = 0.01
I0522 21:57:11.508522 35003 solver.cpp:239] Iteration 75430 (2.19201 iter/s, 4.56202s/10 iters), loss = 7.80984
I0522 21:57:11.508574 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80984 (* 1 = 7.80984 loss)
I0522 21:57:11.511761 35003 sgd_solver.cpp:112] Iteration 75430, lr = 0.01
I0522 21:57:13.941506 35003 solver.cpp:239] Iteration 75440 (4.11046 iter/s, 2.43282s/10 iters), loss = 8.13264
I0522 21:57:13.941557 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.13264 (* 1 = 8.13264 loss)
I0522 21:57:13.950776 35003 sgd_solver.cpp:112] Iteration 75440, lr = 0.01
I0522 21:57:16.801518 35003 solver.cpp:239] Iteration 75450 (3.49672 iter/s, 2.85982s/10 iters), loss = 8.25065
I0522 21:57:16.801810 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.25065 (* 1 = 8.25065 loss)
I0522 21:57:16.813104 35003 sgd_solver.cpp:112] Iteration 75450, lr = 0.01
I0522 21:57:21.115883 35003 solver.cpp:239] Iteration 75460 (2.31809 iter/s, 4.31391s/10 iters), loss = 7.54298
I0522 21:57:21.115942 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54298 (* 1 = 7.54298 loss)
I0522 21:57:21.170542 35003 sgd_solver.cpp:112] Iteration 75460, lr = 0.01
I0522 21:57:23.984740 35003 solver.cpp:239] Iteration 75470 (3.48593 iter/s, 2.86868s/10 iters), loss = 6.75786
I0522 21:57:23.984784 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75786 (* 1 = 6.75786 loss)
I0522 21:57:23.993333 35003 sgd_solver.cpp:112] Iteration 75470, lr = 0.01
I0522 21:57:26.331809 35003 solver.cpp:239] Iteration 75480 (4.26094 iter/s, 2.3469s/10 iters), loss = 6.75514
I0522 21:57:26.331856 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75514 (* 1 = 6.75514 loss)
I0522 21:57:27.040832 35003 sgd_solver.cpp:112] Iteration 75480, lr = 0.01
I0522 21:57:32.198777 35003 solver.cpp:239] Iteration 75490 (1.70454 iter/s, 5.86668s/10 iters), loss = 8.11804
I0522 21:57:32.198839 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.11804 (* 1 = 8.11804 loss)
I0522 21:57:32.211796 35003 sgd_solver.cpp:112] Iteration 75490, lr = 0.01
I0522 21:57:36.615897 35003 solver.cpp:239] Iteration 75500 (2.26405 iter/s, 4.41686s/10 iters), loss = 6.0804
I0522 21:57:36.615947 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0804 (* 1 = 6.0804 loss)
I0522 21:57:37.331341 35003 sgd_solver.cpp:112] Iteration 75500, lr = 0.01
I0522 21:57:42.509413 35003 solver.cpp:239] Iteration 75510 (1.69687 iter/s, 5.89322s/10 iters), loss = 7.09014
I0522 21:57:42.509456 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09014 (* 1 = 7.09014 loss)
I0522 21:57:42.513535 35003 sgd_solver.cpp:112] Iteration 75510, lr = 0.01
I0522 21:57:44.826099 35003 solver.cpp:239] Iteration 75520 (4.31679 iter/s, 2.31654s/10 iters), loss = 7.69897
I0522 21:57:44.826146 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69897 (* 1 = 7.69897 loss)
I0522 21:57:45.519593 35003 sgd_solver.cpp:112] Iteration 75520, lr = 0.01
I0522 21:57:49.198622 35003 solver.cpp:239] Iteration 75530 (2.28713 iter/s, 4.37228s/10 iters), loss = 7.29333
I0522 21:57:49.198889 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29333 (* 1 = 7.29333 loss)
I0522 21:57:49.919631 35003 sgd_solver.cpp:112] Iteration 75530, lr = 0.01
I0522 21:57:53.890247 35003 solver.cpp:239] Iteration 75540 (2.13166 iter/s, 4.69117s/10 iters), loss = 7.4401
I0522 21:57:53.890295 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4401 (* 1 = 7.4401 loss)
I0522 21:57:54.600766 35003 sgd_solver.cpp:112] Iteration 75540, lr = 0.01
I0522 21:57:58.055450 35003 solver.cpp:239] Iteration 75550 (2.40098 iter/s, 4.16497s/10 iters), loss = 7.75655
I0522 21:57:58.055500 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75655 (* 1 = 7.75655 loss)
I0522 21:57:58.796084 35003 sgd_solver.cpp:112] Iteration 75550, lr = 0.01
I0522 21:58:03.167150 35003 solver.cpp:239] Iteration 75560 (1.9564 iter/s, 5.11143s/10 iters), loss = 7.702
I0522 21:58:03.167207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.702 (* 1 = 7.702 loss)
I0522 21:58:03.908064 35003 sgd_solver.cpp:112] Iteration 75560, lr = 0.01
I0522 21:58:06.015625 35003 solver.cpp:239] Iteration 75570 (3.51089 iter/s, 2.84828s/10 iters), loss = 7.94229
I0522 21:58:06.015662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94229 (* 1 = 7.94229 loss)
I0522 21:58:06.028458 35003 sgd_solver.cpp:112] Iteration 75570, lr = 0.01
I0522 21:58:08.110831 35003 solver.cpp:239] Iteration 75580 (4.7731 iter/s, 2.09507s/10 iters), loss = 6.70839
I0522 21:58:08.110873 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70839 (* 1 = 6.70839 loss)
I0522 21:58:08.123814 35003 sgd_solver.cpp:112] Iteration 75580, lr = 0.01
I0522 21:58:10.429627 35003 solver.cpp:239] Iteration 75590 (4.31286 iter/s, 2.31865s/10 iters), loss = 6.50255
I0522 21:58:10.429679 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50255 (* 1 = 6.50255 loss)
I0522 21:58:10.437579 35003 sgd_solver.cpp:112] Iteration 75590, lr = 0.01
I0522 21:58:12.500577 35003 solver.cpp:239] Iteration 75600 (4.82903 iter/s, 2.07081s/10 iters), loss = 8.04655
I0522 21:58:12.500615 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04655 (* 1 = 8.04655 loss)
I0522 21:58:13.202157 35003 sgd_solver.cpp:112] Iteration 75600, lr = 0.01
I0522 21:58:17.614385 35003 solver.cpp:239] Iteration 75610 (1.95558 iter/s, 5.11356s/10 iters), loss = 7.86855
I0522 21:58:17.614430 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86855 (* 1 = 7.86855 loss)
I0522 21:58:17.628046 35003 sgd_solver.cpp:112] Iteration 75610, lr = 0.01
I0522 21:58:21.106640 35003 solver.cpp:239] Iteration 75620 (2.86364 iter/s, 3.49206s/10 iters), loss = 6.60552
I0522 21:58:21.106845 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60552 (* 1 = 6.60552 loss)
I0522 21:58:21.119369 35003 sgd_solver.cpp:112] Iteration 75620, lr = 0.01
I0522 21:58:23.304356 35003 solver.cpp:239] Iteration 75630 (4.55079 iter/s, 2.19742s/10 iters), loss = 8.01317
I0522 21:58:23.304410 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01317 (* 1 = 8.01317 loss)
I0522 21:58:23.312284 35003 sgd_solver.cpp:112] Iteration 75630, lr = 0.01
I0522 21:58:26.944216 35003 solver.cpp:239] Iteration 75640 (2.74751 iter/s, 3.63966s/10 iters), loss = 6.77609
I0522 21:58:26.944260 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77609 (* 1 = 6.77609 loss)
I0522 21:58:26.949892 35003 sgd_solver.cpp:112] Iteration 75640, lr = 0.01
I0522 21:58:30.266371 35003 solver.cpp:239] Iteration 75650 (3.01026 iter/s, 3.32197s/10 iters), loss = 7.4873
I0522 21:58:30.266415 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4873 (* 1 = 7.4873 loss)
I0522 21:58:30.271926 35003 sgd_solver.cpp:112] Iteration 75650, lr = 0.01
I0522 21:58:33.074371 35003 solver.cpp:239] Iteration 75660 (3.5615 iter/s, 2.80781s/10 iters), loss = 7.39374
I0522 21:58:33.074432 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39374 (* 1 = 7.39374 loss)
I0522 21:58:33.092636 35003 sgd_solver.cpp:112] Iteration 75660, lr = 0.01
I0522 21:58:36.567981 35003 solver.cpp:239] Iteration 75670 (2.86254 iter/s, 3.4934s/10 iters), loss = 7.13698
I0522 21:58:36.568034 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13698 (* 1 = 7.13698 loss)
I0522 21:58:36.590920 35003 sgd_solver.cpp:112] Iteration 75670, lr = 0.01
I0522 21:58:41.320518 35003 solver.cpp:239] Iteration 75680 (2.10425 iter/s, 4.75229s/10 iters), loss = 6.61063
I0522 21:58:41.320580 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61063 (* 1 = 6.61063 loss)
I0522 21:58:42.028743 35003 sgd_solver.cpp:112] Iteration 75680, lr = 0.01
I0522 21:58:44.098733 35003 solver.cpp:239] Iteration 75690 (3.59972 iter/s, 2.778s/10 iters), loss = 7.51291
I0522 21:58:44.098807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51291 (* 1 = 7.51291 loss)
I0522 21:58:44.111915 35003 sgd_solver.cpp:112] Iteration 75690, lr = 0.01
I0522 21:58:46.364491 35003 solver.cpp:239] Iteration 75700 (4.4139 iter/s, 2.26557s/10 iters), loss = 7.43706
I0522 21:58:46.364562 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43706 (* 1 = 7.43706 loss)
I0522 21:58:47.080165 35003 sgd_solver.cpp:112] Iteration 75700, lr = 0.01
I0522 21:58:49.991282 35003 solver.cpp:239] Iteration 75710 (2.75743 iter/s, 3.62657s/10 iters), loss = 7.591
I0522 21:58:49.991333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.591 (* 1 = 7.591 loss)
I0522 21:58:50.004048 35003 sgd_solver.cpp:112] Iteration 75710, lr = 0.01
I0522 21:58:52.918123 35003 solver.cpp:239] Iteration 75720 (3.41688 iter/s, 2.92665s/10 iters), loss = 7.91387
I0522 21:58:52.918360 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91387 (* 1 = 7.91387 loss)
I0522 21:58:52.921700 35003 sgd_solver.cpp:112] Iteration 75720, lr = 0.01
I0522 21:58:56.461293 35003 solver.cpp:239] Iteration 75730 (2.82265 iter/s, 3.54277s/10 iters), loss = 6.42559
I0522 21:58:56.461354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42559 (* 1 = 6.42559 loss)
I0522 21:58:56.557463 35003 sgd_solver.cpp:112] Iteration 75730, lr = 0.01
I0522 21:58:59.527096 35003 solver.cpp:239] Iteration 75740 (3.26198 iter/s, 3.06563s/10 iters), loss = 5.45303
I0522 21:58:59.527144 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.45303 (* 1 = 5.45303 loss)
I0522 21:58:59.535349 35003 sgd_solver.cpp:112] Iteration 75740, lr = 0.01
I0522 21:59:03.077230 35003 solver.cpp:239] Iteration 75750 (2.81695 iter/s, 3.54994s/10 iters), loss = 8.88609
I0522 21:59:03.077270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.88609 (* 1 = 8.88609 loss)
I0522 21:59:03.084409 35003 sgd_solver.cpp:112] Iteration 75750, lr = 0.01
I0522 21:59:08.721215 35003 solver.cpp:239] Iteration 75760 (1.77189 iter/s, 5.6437s/10 iters), loss = 8.49133
I0522 21:59:08.721261 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.49133 (* 1 = 8.49133 loss)
I0522 21:59:09.436252 35003 sgd_solver.cpp:112] Iteration 75760, lr = 0.01
I0522 21:59:13.609444 35003 solver.cpp:239] Iteration 75770 (2.04583 iter/s, 4.88799s/10 iters), loss = 6.78842
I0522 21:59:13.609486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78842 (* 1 = 6.78842 loss)
I0522 21:59:14.311601 35003 sgd_solver.cpp:112] Iteration 75770, lr = 0.01
I0522 21:59:17.135370 35003 solver.cpp:239] Iteration 75780 (2.83629 iter/s, 3.52573s/10 iters), loss = 6.68262
I0522 21:59:17.135422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68262 (* 1 = 6.68262 loss)
I0522 21:59:17.799574 35003 sgd_solver.cpp:112] Iteration 75780, lr = 0.01
I0522 21:59:20.529422 35003 solver.cpp:239] Iteration 75790 (2.9465 iter/s, 3.39386s/10 iters), loss = 6.69787
I0522 21:59:20.529460 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69787 (* 1 = 6.69787 loss)
I0522 21:59:20.542351 35003 sgd_solver.cpp:112] Iteration 75790, lr = 0.01
I0522 21:59:24.220887 35003 solver.cpp:239] Iteration 75800 (2.7091 iter/s, 3.69126s/10 iters), loss = 6.75384
I0522 21:59:24.221168 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75384 (* 1 = 6.75384 loss)
I0522 21:59:24.948876 35003 sgd_solver.cpp:112] Iteration 75800, lr = 0.01
I0522 21:59:29.345929 35003 solver.cpp:239] Iteration 75810 (1.95138 iter/s, 5.12458s/10 iters), loss = 7.68714
I0522 21:59:29.345965 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68714 (* 1 = 7.68714 loss)
I0522 21:59:29.359369 35003 sgd_solver.cpp:112] Iteration 75810, lr = 0.01
I0522 21:59:33.687031 35003 solver.cpp:239] Iteration 75820 (2.30368 iter/s, 4.34088s/10 iters), loss = 7.22992
I0522 21:59:33.687077 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22992 (* 1 = 7.22992 loss)
I0522 21:59:33.826484 35003 sgd_solver.cpp:112] Iteration 75820, lr = 0.01
I0522 21:59:37.355473 35003 solver.cpp:239] Iteration 75830 (2.7261 iter/s, 3.66824s/10 iters), loss = 7.847
I0522 21:59:37.355537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.847 (* 1 = 7.847 loss)
I0522 21:59:37.376859 35003 sgd_solver.cpp:112] Iteration 75830, lr = 0.01
I0522 21:59:39.410245 35003 solver.cpp:239] Iteration 75840 (4.8671 iter/s, 2.05461s/10 iters), loss = 6.90488
I0522 21:59:39.410292 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90488 (* 1 = 6.90488 loss)
I0522 21:59:39.414640 35003 sgd_solver.cpp:112] Iteration 75840, lr = 0.01
I0522 21:59:43.070274 35003 solver.cpp:239] Iteration 75850 (2.73238 iter/s, 3.65982s/10 iters), loss = 8.43795
I0522 21:59:43.070329 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.43795 (* 1 = 8.43795 loss)
I0522 21:59:43.791457 35003 sgd_solver.cpp:112] Iteration 75850, lr = 0.01
I0522 21:59:47.395236 35003 solver.cpp:239] Iteration 75860 (2.31229 iter/s, 4.32472s/10 iters), loss = 7.54504
I0522 21:59:47.395282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54504 (* 1 = 7.54504 loss)
I0522 21:59:47.408394 35003 sgd_solver.cpp:112] Iteration 75860, lr = 0.01
I0522 21:59:50.915287 35003 solver.cpp:239] Iteration 75870 (2.84104 iter/s, 3.51984s/10 iters), loss = 7.83743
I0522 21:59:50.915356 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83743 (* 1 = 7.83743 loss)
I0522 21:59:51.634747 35003 sgd_solver.cpp:112] Iteration 75870, lr = 0.01
I0522 21:59:54.348954 35003 solver.cpp:239] Iteration 75880 (2.91252 iter/s, 3.43346s/10 iters), loss = 7.86692
I0522 21:59:54.349189 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86692 (* 1 = 7.86692 loss)
I0522 21:59:54.532558 35003 sgd_solver.cpp:112] Iteration 75880, lr = 0.01
I0522 21:59:58.818543 35003 solver.cpp:239] Iteration 75890 (2.23754 iter/s, 4.4692s/10 iters), loss = 8.03683
I0522 21:59:58.818594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03683 (* 1 = 8.03683 loss)
I0522 21:59:58.831301 35003 sgd_solver.cpp:112] Iteration 75890, lr = 0.01
I0522 22:00:01.898605 35003 solver.cpp:239] Iteration 75900 (3.24688 iter/s, 3.07988s/10 iters), loss = 7.88424
I0522 22:00:01.898663 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88424 (* 1 = 7.88424 loss)
I0522 22:00:01.917021 35003 sgd_solver.cpp:112] Iteration 75900, lr = 0.01
I0522 22:00:05.647918 35003 solver.cpp:239] Iteration 75910 (2.66731 iter/s, 3.74909s/10 iters), loss = 7.44482
I0522 22:00:05.647975 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44482 (* 1 = 7.44482 loss)
I0522 22:00:06.334508 35003 sgd_solver.cpp:112] Iteration 75910, lr = 0.01
I0522 22:00:09.664834 35003 solver.cpp:239] Iteration 75920 (2.48962 iter/s, 4.01667s/10 iters), loss = 7.80268
I0522 22:00:09.664882 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80268 (* 1 = 7.80268 loss)
I0522 22:00:09.750279 35003 sgd_solver.cpp:112] Iteration 75920, lr = 0.01
I0522 22:00:13.944672 35003 solver.cpp:239] Iteration 75930 (2.33666 iter/s, 4.27962s/10 iters), loss = 7.25051
I0522 22:00:13.944722 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25051 (* 1 = 7.25051 loss)
I0522 22:00:14.668098 35003 sgd_solver.cpp:112] Iteration 75930, lr = 0.01
I0522 22:00:17.209062 35003 solver.cpp:239] Iteration 75940 (3.06354 iter/s, 3.2642s/10 iters), loss = 8.39614
I0522 22:00:17.209117 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.39614 (* 1 = 8.39614 loss)
I0522 22:00:17.217211 35003 sgd_solver.cpp:112] Iteration 75940, lr = 0.01
I0522 22:00:20.107870 35003 solver.cpp:239] Iteration 75950 (3.44991 iter/s, 2.89863s/10 iters), loss = 7.41417
I0522 22:00:20.107915 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41417 (* 1 = 7.41417 loss)
I0522 22:00:20.121162 35003 sgd_solver.cpp:112] Iteration 75950, lr = 0.01
I0522 22:00:23.040822 35003 solver.cpp:239] Iteration 75960 (3.40973 iter/s, 2.93279s/10 iters), loss = 7.63065
I0522 22:00:23.040868 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63065 (* 1 = 7.63065 loss)
I0522 22:00:23.782363 35003 sgd_solver.cpp:112] Iteration 75960, lr = 0.01
I0522 22:00:27.376902 35003 solver.cpp:239] Iteration 75970 (2.30635 iter/s, 4.33586s/10 iters), loss = 7.66299
I0522 22:00:27.377189 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66299 (* 1 = 7.66299 loss)
I0522 22:00:27.386466 35003 sgd_solver.cpp:112] Iteration 75970, lr = 0.01
I0522 22:00:30.955755 35003 solver.cpp:239] Iteration 75980 (2.79451 iter/s, 3.57844s/10 iters), loss = 8.99975
I0522 22:00:30.955797 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.99975 (* 1 = 8.99975 loss)
I0522 22:00:30.969168 35003 sgd_solver.cpp:112] Iteration 75980, lr = 0.01
I0522 22:00:35.470031 35003 solver.cpp:239] Iteration 75990 (2.21531 iter/s, 4.51405s/10 iters), loss = 7.38651
I0522 22:00:35.470082 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38651 (* 1 = 7.38651 loss)
I0522 22:00:36.198762 35003 sgd_solver.cpp:112] Iteration 75990, lr = 0.01
I0522 22:00:39.061143 35003 solver.cpp:239] Iteration 76000 (2.78481 iter/s, 3.59091s/10 iters), loss = 7.66609
I0522 22:00:39.061208 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66609 (* 1 = 7.66609 loss)
I0522 22:00:39.074183 35003 sgd_solver.cpp:112] Iteration 76000, lr = 0.01
I0522 22:00:42.658850 35003 solver.cpp:239] Iteration 76010 (2.77972 iter/s, 3.59749s/10 iters), loss = 7.44915
I0522 22:00:42.658895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44915 (* 1 = 7.44915 loss)
I0522 22:00:42.677152 35003 sgd_solver.cpp:112] Iteration 76010, lr = 0.01
I0522 22:00:45.094393 35003 solver.cpp:239] Iteration 76020 (4.10611 iter/s, 2.43539s/10 iters), loss = 7.77825
I0522 22:00:45.094436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77825 (* 1 = 7.77825 loss)
I0522 22:00:45.809828 35003 sgd_solver.cpp:112] Iteration 76020, lr = 0.01
I0522 22:00:49.843250 35003 solver.cpp:239] Iteration 76030 (2.10588 iter/s, 4.74862s/10 iters), loss = 6.86544
I0522 22:00:49.843292 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86544 (* 1 = 6.86544 loss)
I0522 22:00:49.856256 35003 sgd_solver.cpp:112] Iteration 76030, lr = 0.01
I0522 22:00:52.708295 35003 solver.cpp:239] Iteration 76040 (3.49054 iter/s, 2.86489s/10 iters), loss = 6.74714
I0522 22:00:52.708333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74714 (* 1 = 6.74714 loss)
I0522 22:00:52.709924 35003 sgd_solver.cpp:112] Iteration 76040, lr = 0.01
I0522 22:00:55.996546 35003 solver.cpp:239] Iteration 76050 (3.04129 iter/s, 3.28807s/10 iters), loss = 6.59362
I0522 22:00:55.996594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59362 (* 1 = 6.59362 loss)
I0522 22:00:56.007550 35003 sgd_solver.cpp:112] Iteration 76050, lr = 0.01
I0522 22:00:58.011211 35003 solver.cpp:239] Iteration 76060 (4.96396 iter/s, 2.01452s/10 iters), loss = 6.54229
I0522 22:00:58.011420 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54229 (* 1 = 6.54229 loss)
I0522 22:00:58.752622 35003 sgd_solver.cpp:112] Iteration 76060, lr = 0.01
I0522 22:01:01.023474 35003 solver.cpp:239] Iteration 76070 (3.32014 iter/s, 3.01192s/10 iters), loss = 7.49576
I0522 22:01:01.023550 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49576 (* 1 = 7.49576 loss)
I0522 22:01:01.330881 35003 sgd_solver.cpp:112] Iteration 76070, lr = 0.01
I0522 22:01:05.699077 35003 solver.cpp:239] Iteration 76080 (2.13888 iter/s, 4.67534s/10 iters), loss = 6.78576
I0522 22:01:05.699120 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78576 (* 1 = 6.78576 loss)
I0522 22:01:05.712507 35003 sgd_solver.cpp:112] Iteration 76080, lr = 0.01
I0522 22:01:08.482318 35003 solver.cpp:239] Iteration 76090 (3.59314 iter/s, 2.78308s/10 iters), loss = 7.02921
I0522 22:01:08.482373 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02921 (* 1 = 7.02921 loss)
I0522 22:01:09.180462 35003 sgd_solver.cpp:112] Iteration 76090, lr = 0.01
I0522 22:01:12.308548 35003 solver.cpp:239] Iteration 76100 (2.61369 iter/s, 3.82602s/10 iters), loss = 7.26653
I0522 22:01:12.308601 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26653 (* 1 = 7.26653 loss)
I0522 22:01:13.030833 35003 sgd_solver.cpp:112] Iteration 76100, lr = 0.01
I0522 22:01:16.494024 35003 solver.cpp:239] Iteration 76110 (2.38934 iter/s, 4.18525s/10 iters), loss = 6.28754
I0522 22:01:16.494081 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28754 (* 1 = 6.28754 loss)
I0522 22:01:17.228615 35003 sgd_solver.cpp:112] Iteration 76110, lr = 0.01
I0522 22:01:19.774435 35003 solver.cpp:239] Iteration 76120 (3.04859 iter/s, 3.28021s/10 iters), loss = 8.37933
I0522 22:01:19.774499 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.37933 (* 1 = 8.37933 loss)
I0522 22:01:20.495663 35003 sgd_solver.cpp:112] Iteration 76120, lr = 0.01
I0522 22:01:24.147821 35003 solver.cpp:239] Iteration 76130 (2.28668 iter/s, 4.37314s/10 iters), loss = 7.88783
I0522 22:01:24.147879 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88783 (* 1 = 7.88783 loss)
I0522 22:01:24.152993 35003 sgd_solver.cpp:112] Iteration 76130, lr = 0.01
I0522 22:01:26.644526 35003 solver.cpp:239] Iteration 76140 (4.00561 iter/s, 2.4965s/10 iters), loss = 6.82074
I0522 22:01:26.644613 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82074 (* 1 = 6.82074 loss)
I0522 22:01:27.359381 35003 sgd_solver.cpp:112] Iteration 76140, lr = 0.01
I0522 22:01:30.724323 35003 solver.cpp:239] Iteration 76150 (2.45125 iter/s, 4.07955s/10 iters), loss = 7.37003
I0522 22:01:30.724509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37003 (* 1 = 7.37003 loss)
I0522 22:01:30.733660 35003 sgd_solver.cpp:112] Iteration 76150, lr = 0.01
I0522 22:01:33.719990 35003 solver.cpp:239] Iteration 76160 (3.3385 iter/s, 2.99536s/10 iters), loss = 6.48686
I0522 22:01:33.720043 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48686 (* 1 = 6.48686 loss)
I0522 22:01:34.375911 35003 sgd_solver.cpp:112] Iteration 76160, lr = 0.01
I0522 22:01:37.191457 35003 solver.cpp:239] Iteration 76170 (2.88079 iter/s, 3.47127s/10 iters), loss = 6.31775
I0522 22:01:37.191511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31775 (* 1 = 6.31775 loss)
I0522 22:01:37.203987 35003 sgd_solver.cpp:112] Iteration 76170, lr = 0.01
I0522 22:01:40.838668 35003 solver.cpp:239] Iteration 76180 (2.74201 iter/s, 3.64696s/10 iters), loss = 9.3645
I0522 22:01:40.838755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 9.3645 (* 1 = 9.3645 loss)
I0522 22:01:41.560133 35003 sgd_solver.cpp:112] Iteration 76180, lr = 0.01
I0522 22:01:45.094043 35003 solver.cpp:239] Iteration 76190 (2.35011 iter/s, 4.25512s/10 iters), loss = 8.37526
I0522 22:01:45.094084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.37526 (* 1 = 8.37526 loss)
I0522 22:01:45.103860 35003 sgd_solver.cpp:112] Iteration 76190, lr = 0.01
I0522 22:01:48.967912 35003 solver.cpp:239] Iteration 76200 (2.58153 iter/s, 3.87367s/10 iters), loss = 7.42959
I0522 22:01:48.967962 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42959 (* 1 = 7.42959 loss)
I0522 22:01:48.975967 35003 sgd_solver.cpp:112] Iteration 76200, lr = 0.01
I0522 22:01:52.609331 35003 solver.cpp:239] Iteration 76210 (2.74634 iter/s, 3.64121s/10 iters), loss = 6.57421
I0522 22:01:52.609381 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57421 (* 1 = 6.57421 loss)
I0522 22:01:52.614780 35003 sgd_solver.cpp:112] Iteration 76210, lr = 0.01
I0522 22:01:55.194144 35003 solver.cpp:239] Iteration 76220 (3.869 iter/s, 2.58464s/10 iters), loss = 8.13663
I0522 22:01:55.194206 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.13663 (* 1 = 8.13663 loss)
I0522 22:01:55.935119 35003 sgd_solver.cpp:112] Iteration 76220, lr = 0.01
I0522 22:01:59.405658 35003 solver.cpp:239] Iteration 76230 (2.37458 iter/s, 4.21127s/10 iters), loss = 7.42266
I0522 22:01:59.405709 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42266 (* 1 = 7.42266 loss)
I0522 22:01:59.418665 35003 sgd_solver.cpp:112] Iteration 76230, lr = 0.01
I0522 22:02:01.822584 35003 solver.cpp:239] Iteration 76240 (4.13787 iter/s, 2.4167s/10 iters), loss = 7.7274
I0522 22:02:01.822871 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7274 (* 1 = 7.7274 loss)
I0522 22:02:02.502017 35003 sgd_solver.cpp:112] Iteration 76240, lr = 0.01
I0522 22:02:05.886610 35003 solver.cpp:239] Iteration 76250 (2.46089 iter/s, 4.06358s/10 iters), loss = 7.04145
I0522 22:02:05.886649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04145 (* 1 = 7.04145 loss)
I0522 22:02:05.904551 35003 sgd_solver.cpp:112] Iteration 76250, lr = 0.01
I0522 22:02:10.343250 35003 solver.cpp:239] Iteration 76260 (2.24396 iter/s, 4.4564s/10 iters), loss = 7.43471
I0522 22:02:10.343303 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43471 (* 1 = 7.43471 loss)
I0522 22:02:10.347482 35003 sgd_solver.cpp:112] Iteration 76260, lr = 0.01
I0522 22:02:12.870688 35003 solver.cpp:239] Iteration 76270 (3.95682 iter/s, 2.52728s/10 iters), loss = 6.12553
I0522 22:02:12.870751 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12553 (* 1 = 6.12553 loss)
I0522 22:02:13.586127 35003 sgd_solver.cpp:112] Iteration 76270, lr = 0.01
I0522 22:02:16.460093 35003 solver.cpp:239] Iteration 76280 (2.78614 iter/s, 3.58919s/10 iters), loss = 7.44052
I0522 22:02:16.460139 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44052 (* 1 = 7.44052 loss)
I0522 22:02:16.478659 35003 sgd_solver.cpp:112] Iteration 76280, lr = 0.01
I0522 22:02:19.583703 35003 solver.cpp:239] Iteration 76290 (3.20164 iter/s, 3.1234s/10 iters), loss = 7.63045
I0522 22:02:19.583802 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63045 (* 1 = 7.63045 loss)
I0522 22:02:19.604938 35003 sgd_solver.cpp:112] Iteration 76290, lr = 0.01
I0522 22:02:22.514832 35003 solver.cpp:239] Iteration 76300 (3.4119 iter/s, 2.93092s/10 iters), loss = 6.90745
I0522 22:02:22.514889 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90745 (* 1 = 6.90745 loss)
I0522 22:02:22.533915 35003 sgd_solver.cpp:112] Iteration 76300, lr = 0.01
I0522 22:02:27.366183 35003 solver.cpp:239] Iteration 76310 (2.06139 iter/s, 4.85109s/10 iters), loss = 7.78748
I0522 22:02:27.366243 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78748 (* 1 = 7.78748 loss)
I0522 22:02:27.378599 35003 sgd_solver.cpp:112] Iteration 76310, lr = 0.01
I0522 22:02:29.468933 35003 solver.cpp:239] Iteration 76320 (4.75603 iter/s, 2.10259s/10 iters), loss = 7.9197
I0522 22:02:29.468972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9197 (* 1 = 7.9197 loss)
I0522 22:02:29.474673 35003 sgd_solver.cpp:112] Iteration 76320, lr = 0.01
I0522 22:02:33.143152 35003 solver.cpp:239] Iteration 76330 (2.72181 iter/s, 3.67403s/10 iters), loss = 7.77134
I0522 22:02:33.143402 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77134 (* 1 = 7.77134 loss)
I0522 22:02:33.722380 35003 sgd_solver.cpp:112] Iteration 76330, lr = 0.01
I0522 22:02:36.254292 35003 solver.cpp:239] Iteration 76340 (3.21463 iter/s, 3.11078s/10 iters), loss = 8.00106
I0522 22:02:36.254348 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00106 (* 1 = 8.00106 loss)
I0522 22:02:36.267457 35003 sgd_solver.cpp:112] Iteration 76340, lr = 0.01
I0522 22:02:38.307646 35003 solver.cpp:239] Iteration 76350 (4.87043 iter/s, 2.05321s/10 iters), loss = 7.76369
I0522 22:02:38.307699 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76369 (* 1 = 7.76369 loss)
I0522 22:02:39.049129 35003 sgd_solver.cpp:112] Iteration 76350, lr = 0.01
I0522 22:02:42.896728 35003 solver.cpp:239] Iteration 76360 (2.1792 iter/s, 4.58884s/10 iters), loss = 7.95547
I0522 22:02:42.896785 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95547 (* 1 = 7.95547 loss)
I0522 22:02:43.612422 35003 sgd_solver.cpp:112] Iteration 76360, lr = 0.01
I0522 22:02:46.514647 35003 solver.cpp:239] Iteration 76370 (2.76418 iter/s, 3.61772s/10 iters), loss = 8.15862
I0522 22:02:46.514708 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.15862 (* 1 = 8.15862 loss)
I0522 22:02:46.528724 35003 sgd_solver.cpp:112] Iteration 76370, lr = 0.01
I0522 22:02:50.865255 35003 solver.cpp:239] Iteration 76380 (2.29865 iter/s, 4.35038s/10 iters), loss = 7.53757
I0522 22:02:50.865295 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53757 (* 1 = 7.53757 loss)
I0522 22:02:50.873780 35003 sgd_solver.cpp:112] Iteration 76380, lr = 0.01
I0522 22:02:53.753420 35003 solver.cpp:239] Iteration 76390 (3.46261 iter/s, 2.888s/10 iters), loss = 5.56764
I0522 22:02:53.753484 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.56764 (* 1 = 5.56764 loss)
I0522 22:02:54.312711 35003 sgd_solver.cpp:112] Iteration 76390, lr = 0.01
I0522 22:02:57.443941 35003 solver.cpp:239] Iteration 76400 (2.7098 iter/s, 3.6903s/10 iters), loss = 6.87764
I0522 22:02:57.443986 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87764 (* 1 = 6.87764 loss)
I0522 22:02:57.471043 35003 sgd_solver.cpp:112] Iteration 76400, lr = 0.01
I0522 22:03:00.246657 35003 solver.cpp:239] Iteration 76410 (3.56818 iter/s, 2.80255s/10 iters), loss = 7.96761
I0522 22:03:00.246736 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96761 (* 1 = 7.96761 loss)
I0522 22:03:00.254961 35003 sgd_solver.cpp:112] Iteration 76410, lr = 0.01
I0522 22:03:03.903935 35003 solver.cpp:239] Iteration 76420 (2.73445 iter/s, 3.65704s/10 iters), loss = 7.90739
I0522 22:03:03.904312 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90739 (* 1 = 7.90739 loss)
I0522 22:03:03.911146 35003 sgd_solver.cpp:112] Iteration 76420, lr = 0.01
I0522 22:03:07.506575 35003 solver.cpp:239] Iteration 76430 (2.77612 iter/s, 3.60214s/10 iters), loss = 7.18614
I0522 22:03:07.506625 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18614 (* 1 = 7.18614 loss)
I0522 22:03:07.519572 35003 sgd_solver.cpp:112] Iteration 76430, lr = 0.01
I0522 22:03:09.371119 35003 solver.cpp:239] Iteration 76440 (5.36364 iter/s, 1.8644s/10 iters), loss = 7.01673
I0522 22:03:09.371197 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01673 (* 1 = 7.01673 loss)
I0522 22:03:09.399653 35003 sgd_solver.cpp:112] Iteration 76440, lr = 0.01
I0522 22:03:13.644430 35003 solver.cpp:239] Iteration 76450 (2.34025 iter/s, 4.27305s/10 iters), loss = 8.25756
I0522 22:03:13.644482 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.25756 (* 1 = 8.25756 loss)
I0522 22:03:14.386131 35003 sgd_solver.cpp:112] Iteration 76450, lr = 0.01
I0522 22:03:17.248991 35003 solver.cpp:239] Iteration 76460 (2.77442 iter/s, 3.60436s/10 iters), loss = 7.41165
I0522 22:03:17.249039 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41165 (* 1 = 7.41165 loss)
I0522 22:03:17.262748 35003 sgd_solver.cpp:112] Iteration 76460, lr = 0.01
I0522 22:03:21.593727 35003 solver.cpp:239] Iteration 76470 (2.30176 iter/s, 4.34451s/10 iters), loss = 6.89103
I0522 22:03:21.593771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89103 (* 1 = 6.89103 loss)
I0522 22:03:21.601084 35003 sgd_solver.cpp:112] Iteration 76470, lr = 0.01
I0522 22:03:24.503374 35003 solver.cpp:239] Iteration 76480 (3.43704 iter/s, 2.90948s/10 iters), loss = 6.76239
I0522 22:03:24.503432 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76239 (* 1 = 6.76239 loss)
I0522 22:03:25.223117 35003 sgd_solver.cpp:112] Iteration 76480, lr = 0.01
I0522 22:03:29.633306 35003 solver.cpp:239] Iteration 76490 (1.94946 iter/s, 5.12963s/10 iters), loss = 6.01434
I0522 22:03:29.633359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01434 (* 1 = 6.01434 loss)
I0522 22:03:29.652561 35003 sgd_solver.cpp:112] Iteration 76490, lr = 0.01
I0522 22:03:32.533783 35003 solver.cpp:239] Iteration 76500 (3.44792 iter/s, 2.9003s/10 iters), loss = 7.85819
I0522 22:03:32.533828 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85819 (* 1 = 7.85819 loss)
I0522 22:03:33.261801 35003 sgd_solver.cpp:112] Iteration 76500, lr = 0.01
I0522 22:03:36.835211 35003 solver.cpp:239] Iteration 76510 (2.32493 iter/s, 4.3012s/10 iters), loss = 7.51355
I0522 22:03:36.835507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51355 (* 1 = 7.51355 loss)
I0522 22:03:36.853978 35003 sgd_solver.cpp:112] Iteration 76510, lr = 0.01
I0522 22:03:41.724062 35003 solver.cpp:239] Iteration 76520 (2.04567 iter/s, 4.88838s/10 iters), loss = 6.68604
I0522 22:03:41.724103 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68604 (* 1 = 6.68604 loss)
I0522 22:03:41.798887 35003 sgd_solver.cpp:112] Iteration 76520, lr = 0.01
I0522 22:03:44.618327 35003 solver.cpp:239] Iteration 76530 (3.4553 iter/s, 2.8941s/10 iters), loss = 8.12801
I0522 22:03:44.618367 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.12801 (* 1 = 8.12801 loss)
I0522 22:03:45.080078 35003 sgd_solver.cpp:112] Iteration 76530, lr = 0.01
I0522 22:03:47.928463 35003 solver.cpp:239] Iteration 76540 (3.02119 iter/s, 3.30995s/10 iters), loss = 7.30078
I0522 22:03:47.928508 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30078 (* 1 = 7.30078 loss)
I0522 22:03:47.939858 35003 sgd_solver.cpp:112] Iteration 76540, lr = 0.01
I0522 22:03:50.618140 35003 solver.cpp:239] Iteration 76550 (3.71815 iter/s, 2.68951s/10 iters), loss = 7.63831
I0522 22:03:50.618188 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63831 (* 1 = 7.63831 loss)
I0522 22:03:50.622560 35003 sgd_solver.cpp:112] Iteration 76550, lr = 0.01
I0522 22:03:53.455899 35003 solver.cpp:239] Iteration 76560 (3.52412 iter/s, 2.83759s/10 iters), loss = 7.16555
I0522 22:03:53.455942 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16555 (* 1 = 7.16555 loss)
I0522 22:03:53.460165 35003 sgd_solver.cpp:112] Iteration 76560, lr = 0.01
I0522 22:03:56.547706 35003 solver.cpp:239] Iteration 76570 (3.23454 iter/s, 3.09163s/10 iters), loss = 7.08802
I0522 22:03:56.547755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08802 (* 1 = 7.08802 loss)
I0522 22:03:56.560657 35003 sgd_solver.cpp:112] Iteration 76570, lr = 0.01
I0522 22:04:00.888463 35003 solver.cpp:239] Iteration 76580 (2.30387 iter/s, 4.34052s/10 iters), loss = 8.34751
I0522 22:04:00.888543 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.34751 (* 1 = 8.34751 loss)
I0522 22:04:00.901660 35003 sgd_solver.cpp:112] Iteration 76580, lr = 0.01
I0522 22:04:05.216104 35003 solver.cpp:239] Iteration 76590 (2.31086 iter/s, 4.32739s/10 iters), loss = 7.81636
I0522 22:04:05.216147 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81636 (* 1 = 7.81636 loss)
I0522 22:04:05.225394 35003 sgd_solver.cpp:112] Iteration 76590, lr = 0.01
I0522 22:04:08.672037 35003 solver.cpp:239] Iteration 76600 (2.89373 iter/s, 3.45574s/10 iters), loss = 7.9595
I0522 22:04:08.672258 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9595 (* 1 = 7.9595 loss)
I0522 22:04:09.405570 35003 sgd_solver.cpp:112] Iteration 76600, lr = 0.01
I0522 22:04:12.124049 35003 solver.cpp:239] Iteration 76610 (2.89717 iter/s, 3.45165s/10 iters), loss = 7.39583
I0522 22:04:12.124085 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39583 (* 1 = 7.39583 loss)
I0522 22:04:12.137377 35003 sgd_solver.cpp:112] Iteration 76610, lr = 0.01
I0522 22:04:16.035949 35003 solver.cpp:239] Iteration 76620 (2.55644 iter/s, 3.91169s/10 iters), loss = 6.82395
I0522 22:04:16.035991 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82395 (* 1 = 6.82395 loss)
I0522 22:04:16.049154 35003 sgd_solver.cpp:112] Iteration 76620, lr = 0.01
I0522 22:04:20.181242 35003 solver.cpp:239] Iteration 76630 (2.4125 iter/s, 4.14508s/10 iters), loss = 7.94423
I0522 22:04:20.181300 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94423 (* 1 = 7.94423 loss)
I0522 22:04:20.922139 35003 sgd_solver.cpp:112] Iteration 76630, lr = 0.01
I0522 22:04:24.491852 35003 solver.cpp:239] Iteration 76640 (2.31998 iter/s, 4.31038s/10 iters), loss = 6.56815
I0522 22:04:24.491899 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56815 (* 1 = 6.56815 loss)
I0522 22:04:25.193816 35003 sgd_solver.cpp:112] Iteration 76640, lr = 0.01
I0522 22:04:28.074048 35003 solver.cpp:239] Iteration 76650 (2.79174 iter/s, 3.582s/10 iters), loss = 7.25017
I0522 22:04:28.074093 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25017 (* 1 = 7.25017 loss)
I0522 22:04:28.128362 35003 sgd_solver.cpp:112] Iteration 76650, lr = 0.01
I0522 22:04:31.812280 35003 solver.cpp:239] Iteration 76660 (2.67521 iter/s, 3.73803s/10 iters), loss = 7.51204
I0522 22:04:31.812320 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51204 (* 1 = 7.51204 loss)
I0522 22:04:32.076567 35003 sgd_solver.cpp:112] Iteration 76660, lr = 0.01
I0522 22:04:34.710789 35003 solver.cpp:239] Iteration 76670 (3.45025 iter/s, 2.89834s/10 iters), loss = 7.41354
I0522 22:04:34.710841 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41354 (* 1 = 7.41354 loss)
I0522 22:04:35.451859 35003 sgd_solver.cpp:112] Iteration 76670, lr = 0.01
I0522 22:04:39.504040 35003 solver.cpp:239] Iteration 76680 (2.08638 iter/s, 4.793s/10 iters), loss = 8.20965
I0522 22:04:39.504302 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.20965 (* 1 = 8.20965 loss)
I0522 22:04:39.511011 35003 sgd_solver.cpp:112] Iteration 76680, lr = 0.01
I0522 22:04:42.336275 35003 solver.cpp:239] Iteration 76690 (3.53125 iter/s, 2.83186s/10 iters), loss = 7.46393
I0522 22:04:42.336318 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46393 (* 1 = 7.46393 loss)
I0522 22:04:42.349287 35003 sgd_solver.cpp:112] Iteration 76690, lr = 0.01
I0522 22:04:48.204526 35003 solver.cpp:239] Iteration 76700 (1.70417 iter/s, 5.86797s/10 iters), loss = 7.18393
I0522 22:04:48.204569 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18393 (* 1 = 7.18393 loss)
I0522 22:04:48.217022 35003 sgd_solver.cpp:112] Iteration 76700, lr = 0.01
I0522 22:04:50.910042 35003 solver.cpp:239] Iteration 76710 (3.69638 iter/s, 2.70535s/10 iters), loss = 7.01477
I0522 22:04:50.910084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01477 (* 1 = 7.01477 loss)
I0522 22:04:50.915510 35003 sgd_solver.cpp:112] Iteration 76710, lr = 0.01
I0522 22:04:53.560999 35003 solver.cpp:239] Iteration 76720 (3.77245 iter/s, 2.6508s/10 iters), loss = 6.32378
I0522 22:04:53.561059 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32378 (* 1 = 6.32378 loss)
I0522 22:04:54.275909 35003 sgd_solver.cpp:112] Iteration 76720, lr = 0.01
I0522 22:04:56.756958 35003 solver.cpp:239] Iteration 76730 (3.12914 iter/s, 3.19577s/10 iters), loss = 6.65389
I0522 22:04:56.756996 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65389 (* 1 = 6.65389 loss)
I0522 22:04:56.758632 35003 sgd_solver.cpp:112] Iteration 76730, lr = 0.01
I0522 22:05:00.346467 35003 solver.cpp:239] Iteration 76740 (2.78606 iter/s, 3.5893s/10 iters), loss = 6.98162
I0522 22:05:00.346549 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98162 (* 1 = 6.98162 loss)
I0522 22:05:01.087327 35003 sgd_solver.cpp:112] Iteration 76740, lr = 0.01
I0522 22:05:05.303576 35003 solver.cpp:239] Iteration 76750 (2.01742 iter/s, 4.95683s/10 iters), loss = 7.83812
I0522 22:05:05.303637 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83812 (* 1 = 7.83812 loss)
I0522 22:05:05.312445 35003 sgd_solver.cpp:112] Iteration 76750, lr = 0.01
I0522 22:05:07.328322 35003 solver.cpp:239] Iteration 76760 (4.93925 iter/s, 2.0246s/10 iters), loss = 6.86963
I0522 22:05:07.328364 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86963 (* 1 = 6.86963 loss)
I0522 22:05:07.333640 35003 sgd_solver.cpp:112] Iteration 76760, lr = 0.01
I0522 22:05:10.730126 35003 solver.cpp:239] Iteration 76770 (2.93978 iter/s, 3.40162s/10 iters), loss = 7.4715
I0522 22:05:10.730367 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4715 (* 1 = 7.4715 loss)
I0522 22:05:10.743260 35003 sgd_solver.cpp:112] Iteration 76770, lr = 0.01
I0522 22:05:12.816390 35003 solver.cpp:239] Iteration 76780 (4.79401 iter/s, 2.08594s/10 iters), loss = 7.89998
I0522 22:05:12.816431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89998 (* 1 = 7.89998 loss)
I0522 22:05:13.503726 35003 sgd_solver.cpp:112] Iteration 76780, lr = 0.01
I0522 22:05:17.778738 35003 solver.cpp:239] Iteration 76790 (2.01528 iter/s, 4.9621s/10 iters), loss = 7.01258
I0522 22:05:17.778795 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01258 (* 1 = 7.01258 loss)
I0522 22:05:18.494146 35003 sgd_solver.cpp:112] Iteration 76790, lr = 0.01
I0522 22:05:22.921160 35003 solver.cpp:239] Iteration 76800 (1.94471 iter/s, 5.14215s/10 iters), loss = 8.28818
I0522 22:05:22.921205 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.28818 (* 1 = 8.28818 loss)
I0522 22:05:22.956244 35003 sgd_solver.cpp:112] Iteration 76800, lr = 0.01
I0522 22:05:26.580871 35003 solver.cpp:239] Iteration 76810 (2.73261 iter/s, 3.65951s/10 iters), loss = 7.18857
I0522 22:05:26.580914 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18857 (* 1 = 7.18857 loss)
I0522 22:05:26.596848 35003 sgd_solver.cpp:112] Iteration 76810, lr = 0.01
I0522 22:05:29.441464 35003 solver.cpp:239] Iteration 76820 (3.49598 iter/s, 2.86043s/10 iters), loss = 7.18766
I0522 22:05:29.441514 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18766 (* 1 = 7.18766 loss)
I0522 22:05:29.449283 35003 sgd_solver.cpp:112] Iteration 76820, lr = 0.01
I0522 22:05:33.065050 35003 solver.cpp:239] Iteration 76830 (2.75985 iter/s, 3.62339s/10 iters), loss = 7.64097
I0522 22:05:33.065099 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64097 (* 1 = 7.64097 loss)
I0522 22:05:33.184098 35003 sgd_solver.cpp:112] Iteration 76830, lr = 0.01
I0522 22:05:36.208631 35003 solver.cpp:239] Iteration 76840 (3.18127 iter/s, 3.1434s/10 iters), loss = 8.33279
I0522 22:05:36.208680 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.33279 (* 1 = 8.33279 loss)
I0522 22:05:36.219389 35003 sgd_solver.cpp:112] Iteration 76840, lr = 0.01
I0522 22:05:39.118283 35003 solver.cpp:239] Iteration 76850 (3.43705 iter/s, 2.90948s/10 iters), loss = 8.19043
I0522 22:05:39.118335 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.19043 (* 1 = 8.19043 loss)
I0522 22:05:39.838343 35003 sgd_solver.cpp:112] Iteration 76850, lr = 0.01
I0522 22:05:41.974969 35003 solver.cpp:239] Iteration 76860 (3.50078 iter/s, 2.8565s/10 iters), loss = 7.14299
I0522 22:05:41.975201 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14299 (* 1 = 7.14299 loss)
I0522 22:05:42.712153 35003 sgd_solver.cpp:112] Iteration 76860, lr = 0.01
I0522 22:05:45.349889 35003 solver.cpp:239] Iteration 76870 (2.96336 iter/s, 3.37455s/10 iters), loss = 7.27867
I0522 22:05:45.349927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27867 (* 1 = 7.27867 loss)
I0522 22:05:45.362751 35003 sgd_solver.cpp:112] Iteration 76870, lr = 0.01
I0522 22:05:48.252529 35003 solver.cpp:239] Iteration 76880 (3.44534 iter/s, 2.90247s/10 iters), loss = 7.10909
I0522 22:05:48.252571 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10909 (* 1 = 7.10909 loss)
I0522 22:05:48.266268 35003 sgd_solver.cpp:112] Iteration 76880, lr = 0.01
I0522 22:05:49.528008 35003 solver.cpp:239] Iteration 76890 (7.84083 iter/s, 1.27538s/10 iters), loss = 7.50982
I0522 22:05:49.528050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50982 (* 1 = 7.50982 loss)
I0522 22:05:49.541062 35003 sgd_solver.cpp:112] Iteration 76890, lr = 0.01
I0522 22:05:53.056854 35003 solver.cpp:239] Iteration 76900 (2.83398 iter/s, 3.52861s/10 iters), loss = 6.81779
I0522 22:05:53.056939 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81779 (* 1 = 6.81779 loss)
I0522 22:05:53.147264 35003 sgd_solver.cpp:112] Iteration 76900, lr = 0.01
I0522 22:05:55.899304 35003 solver.cpp:239] Iteration 76910 (3.51835 iter/s, 2.84225s/10 iters), loss = 6.60221
I0522 22:05:55.899344 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60221 (* 1 = 6.60221 loss)
I0522 22:05:55.903668 35003 sgd_solver.cpp:112] Iteration 76910, lr = 0.01
I0522 22:05:59.464346 35003 solver.cpp:239] Iteration 76920 (2.80517 iter/s, 3.56485s/10 iters), loss = 6.94563
I0522 22:05:59.464413 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94563 (* 1 = 6.94563 loss)
I0522 22:05:59.471068 35003 sgd_solver.cpp:112] Iteration 76920, lr = 0.01
I0522 22:06:01.559866 35003 solver.cpp:239] Iteration 76930 (4.77245 iter/s, 2.09536s/10 iters), loss = 8.04806
I0522 22:06:01.559906 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04806 (* 1 = 8.04806 loss)
I0522 22:06:01.572446 35003 sgd_solver.cpp:112] Iteration 76930, lr = 0.01
I0522 22:06:05.249112 35003 solver.cpp:239] Iteration 76940 (2.71073 iter/s, 3.68905s/10 iters), loss = 7.35346
I0522 22:06:05.249174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35346 (* 1 = 7.35346 loss)
I0522 22:06:05.255522 35003 sgd_solver.cpp:112] Iteration 76940, lr = 0.01
I0522 22:06:08.195397 35003 solver.cpp:239] Iteration 76950 (3.39435 iter/s, 2.94607s/10 iters), loss = 7.46005
I0522 22:06:08.195540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46005 (* 1 = 7.46005 loss)
I0522 22:06:08.199854 35003 sgd_solver.cpp:112] Iteration 76950, lr = 0.01
I0522 22:06:11.952107 35003 solver.cpp:239] Iteration 76960 (2.6621 iter/s, 3.75643s/10 iters), loss = 7.70384
I0522 22:06:11.952157 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70384 (* 1 = 7.70384 loss)
I0522 22:06:11.959712 35003 sgd_solver.cpp:112] Iteration 76960, lr = 0.01
I0522 22:06:14.691270 35003 solver.cpp:239] Iteration 76970 (3.65097 iter/s, 2.739s/10 iters), loss = 7.90148
I0522 22:06:14.691478 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90148 (* 1 = 7.90148 loss)
I0522 22:06:14.703060 35003 sgd_solver.cpp:112] Iteration 76970, lr = 0.01
I0522 22:06:17.626372 35003 solver.cpp:239] Iteration 76980 (3.40743 iter/s, 2.93476s/10 iters), loss = 6.82374
I0522 22:06:17.626435 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82374 (* 1 = 6.82374 loss)
I0522 22:06:17.649541 35003 sgd_solver.cpp:112] Iteration 76980, lr = 0.01
I0522 22:06:21.071986 35003 solver.cpp:239] Iteration 76990 (2.90241 iter/s, 3.44541s/10 iters), loss = 8.4343
I0522 22:06:21.072062 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.4343 (* 1 = 8.4343 loss)
I0522 22:06:21.810081 35003 sgd_solver.cpp:112] Iteration 76990, lr = 0.01
I0522 22:06:25.414269 35003 solver.cpp:239] Iteration 77000 (2.30307 iter/s, 4.34203s/10 iters), loss = 8.40929
I0522 22:06:25.414321 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.40929 (* 1 = 8.40929 loss)
I0522 22:06:25.427028 35003 sgd_solver.cpp:112] Iteration 77000, lr = 0.01
I0522 22:06:28.088891 35003 solver.cpp:239] Iteration 77010 (3.73908 iter/s, 2.67446s/10 iters), loss = 7.47
I0522 22:06:28.088940 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47 (* 1 = 7.47 loss)
I0522 22:06:28.102562 35003 sgd_solver.cpp:112] Iteration 77010, lr = 0.01
I0522 22:06:31.371136 35003 solver.cpp:239] Iteration 77020 (3.04687 iter/s, 3.28205s/10 iters), loss = 7.51997
I0522 22:06:31.371192 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51997 (* 1 = 7.51997 loss)
I0522 22:06:31.377789 35003 sgd_solver.cpp:112] Iteration 77020, lr = 0.01
I0522 22:06:34.780252 35003 solver.cpp:239] Iteration 77030 (2.93348 iter/s, 3.40892s/10 iters), loss = 7.76163
I0522 22:06:34.780289 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76163 (* 1 = 7.76163 loss)
I0522 22:06:34.793454 35003 sgd_solver.cpp:112] Iteration 77030, lr = 0.01
I0522 22:06:38.944726 35003 solver.cpp:239] Iteration 77040 (2.40139 iter/s, 4.16425s/10 iters), loss = 7.32716
I0522 22:06:38.944778 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32716 (* 1 = 7.32716 loss)
I0522 22:06:38.953059 35003 sgd_solver.cpp:112] Iteration 77040, lr = 0.01
I0522 22:06:41.664844 35003 solver.cpp:239] Iteration 77050 (3.67654 iter/s, 2.71995s/10 iters), loss = 6.02261
I0522 22:06:41.664887 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02261 (* 1 = 6.02261 loss)
I0522 22:06:41.819351 35003 sgd_solver.cpp:112] Iteration 77050, lr = 0.01
I0522 22:06:45.622908 35003 solver.cpp:239] Iteration 77060 (2.52662 iter/s, 3.95786s/10 iters), loss = 6.4954
I0522 22:06:45.623133 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4954 (* 1 = 6.4954 loss)
I0522 22:06:45.636683 35003 sgd_solver.cpp:112] Iteration 77060, lr = 0.01
I0522 22:06:47.689656 35003 solver.cpp:239] Iteration 77070 (4.83929 iter/s, 2.06642s/10 iters), loss = 7.1033
I0522 22:06:47.689744 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1033 (* 1 = 7.1033 loss)
I0522 22:06:47.702916 35003 sgd_solver.cpp:112] Iteration 77070, lr = 0.01
I0522 22:06:50.481176 35003 solver.cpp:239] Iteration 77080 (3.58253 iter/s, 2.79132s/10 iters), loss = 7.35596
I0522 22:06:50.481215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35596 (* 1 = 7.35596 loss)
I0522 22:06:50.493522 35003 sgd_solver.cpp:112] Iteration 77080, lr = 0.01
I0522 22:06:51.837393 35003 solver.cpp:239] Iteration 77090 (7.37404 iter/s, 1.35611s/10 iters), loss = 7.94188
I0522 22:06:51.837435 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94188 (* 1 = 7.94188 loss)
I0522 22:06:52.175132 35003 sgd_solver.cpp:112] Iteration 77090, lr = 0.01
I0522 22:06:55.680328 35003 solver.cpp:239] Iteration 77100 (2.60232 iter/s, 3.84273s/10 iters), loss = 6.49095
I0522 22:06:55.680387 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49095 (* 1 = 6.49095 loss)
I0522 22:06:55.703441 35003 sgd_solver.cpp:112] Iteration 77100, lr = 0.01
I0522 22:06:57.747290 35003 solver.cpp:239] Iteration 77110 (4.83837 iter/s, 2.06681s/10 iters), loss = 7.587
I0522 22:06:57.747331 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.587 (* 1 = 7.587 loss)
I0522 22:06:57.753476 35003 sgd_solver.cpp:112] Iteration 77110, lr = 0.01
I0522 22:07:01.304927 35003 solver.cpp:239] Iteration 77120 (2.811 iter/s, 3.55745s/10 iters), loss = 6.66013
I0522 22:07:01.304986 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66013 (* 1 = 6.66013 loss)
I0522 22:07:02.045498 35003 sgd_solver.cpp:112] Iteration 77120, lr = 0.01
I0522 22:07:06.473342 35003 solver.cpp:239] Iteration 77130 (1.93493 iter/s, 5.16816s/10 iters), loss = 7.56325
I0522 22:07:06.473384 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56325 (* 1 = 7.56325 loss)
I0522 22:07:07.169091 35003 sgd_solver.cpp:112] Iteration 77130, lr = 0.01
I0522 22:07:12.114585 35003 solver.cpp:239] Iteration 77140 (1.77274 iter/s, 5.64098s/10 iters), loss = 7.5787
I0522 22:07:12.114627 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5787 (* 1 = 7.5787 loss)
I0522 22:07:12.123708 35003 sgd_solver.cpp:112] Iteration 77140, lr = 0.01
I0522 22:07:14.204612 35003 solver.cpp:239] Iteration 77150 (4.78496 iter/s, 2.08988s/10 iters), loss = 6.78955
I0522 22:07:14.204664 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78955 (* 1 = 6.78955 loss)
I0522 22:07:14.897600 35003 sgd_solver.cpp:112] Iteration 77150, lr = 0.01
I0522 22:07:20.133178 35003 solver.cpp:239] Iteration 77160 (1.68683 iter/s, 5.92827s/10 iters), loss = 7.83378
I0522 22:07:20.133343 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83378 (* 1 = 7.83378 loss)
I0522 22:07:20.147665 35003 sgd_solver.cpp:112] Iteration 77160, lr = 0.01
I0522 22:07:25.141189 35003 solver.cpp:239] Iteration 77170 (1.99695 iter/s, 5.00764s/10 iters), loss = 7.06276
I0522 22:07:25.141244 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06276 (* 1 = 7.06276 loss)
I0522 22:07:25.851058 35003 sgd_solver.cpp:112] Iteration 77170, lr = 0.01
I0522 22:07:28.757750 35003 solver.cpp:239] Iteration 77180 (2.76522 iter/s, 3.61635s/10 iters), loss = 7.88975
I0522 22:07:28.757794 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88975 (* 1 = 7.88975 loss)
I0522 22:07:29.403425 35003 sgd_solver.cpp:112] Iteration 77180, lr = 0.01
I0522 22:07:33.579362 35003 solver.cpp:239] Iteration 77190 (2.0741 iter/s, 4.82137s/10 iters), loss = 7.3995
I0522 22:07:33.579403 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3995 (* 1 = 7.3995 loss)
I0522 22:07:33.591979 35003 sgd_solver.cpp:112] Iteration 77190, lr = 0.01
I0522 22:07:36.856076 35003 solver.cpp:239] Iteration 77200 (3.052 iter/s, 3.27654s/10 iters), loss = 7.85991
I0522 22:07:36.856117 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85991 (* 1 = 7.85991 loss)
I0522 22:07:36.875272 35003 sgd_solver.cpp:112] Iteration 77200, lr = 0.01
I0522 22:07:40.372062 35003 solver.cpp:239] Iteration 77210 (2.84431 iter/s, 3.51579s/10 iters), loss = 7.39621
I0522 22:07:40.372109 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39621 (* 1 = 7.39621 loss)
I0522 22:07:40.385174 35003 sgd_solver.cpp:112] Iteration 77210, lr = 0.01
I0522 22:07:43.138768 35003 solver.cpp:239] Iteration 77220 (3.61462 iter/s, 2.76654s/10 iters), loss = 6.61793
I0522 22:07:43.138811 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61793 (* 1 = 6.61793 loss)
I0522 22:07:43.876296 35003 sgd_solver.cpp:112] Iteration 77220, lr = 0.01
I0522 22:07:46.893652 35003 solver.cpp:239] Iteration 77230 (2.66334 iter/s, 3.75468s/10 iters), loss = 6.55145
I0522 22:07:46.893699 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55145 (* 1 = 6.55145 loss)
I0522 22:07:47.594540 35003 sgd_solver.cpp:112] Iteration 77230, lr = 0.01
I0522 22:07:51.974292 35003 solver.cpp:239] Iteration 77240 (1.96835 iter/s, 5.08039s/10 iters), loss = 7.92219
I0522 22:07:51.974555 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.92219 (* 1 = 7.92219 loss)
I0522 22:07:51.986780 35003 sgd_solver.cpp:112] Iteration 77240, lr = 0.01
I0522 22:07:54.828447 35003 solver.cpp:239] Iteration 77250 (3.50412 iter/s, 2.85378s/10 iters), loss = 8.95498
I0522 22:07:54.828491 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.95498 (* 1 = 8.95498 loss)
I0522 22:07:54.841480 35003 sgd_solver.cpp:112] Iteration 77250, lr = 0.01
I0522 22:07:57.360256 35003 solver.cpp:239] Iteration 77260 (3.94999 iter/s, 2.53165s/10 iters), loss = 7.17056
I0522 22:07:57.360318 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17056 (* 1 = 7.17056 loss)
I0522 22:07:57.986500 35003 sgd_solver.cpp:112] Iteration 77260, lr = 0.01
I0522 22:08:02.246001 35003 solver.cpp:239] Iteration 77270 (2.04688 iter/s, 4.88549s/10 iters), loss = 7.96899
I0522 22:08:02.246050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96899 (* 1 = 7.96899 loss)
I0522 22:08:02.260325 35003 sgd_solver.cpp:112] Iteration 77270, lr = 0.01
I0522 22:08:05.130028 35003 solver.cpp:239] Iteration 77280 (3.46758 iter/s, 2.88386s/10 iters), loss = 7.18443
I0522 22:08:05.130087 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18443 (* 1 = 7.18443 loss)
I0522 22:08:05.845927 35003 sgd_solver.cpp:112] Iteration 77280, lr = 0.01
I0522 22:08:09.031455 35003 solver.cpp:239] Iteration 77290 (2.5633 iter/s, 3.90122s/10 iters), loss = 6.20801
I0522 22:08:09.031498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20801 (* 1 = 6.20801 loss)
I0522 22:08:09.441316 35003 sgd_solver.cpp:112] Iteration 77290, lr = 0.01
I0522 22:08:14.621400 35003 solver.cpp:239] Iteration 77300 (1.78901 iter/s, 5.58968s/10 iters), loss = 6.57492
I0522 22:08:14.621436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57492 (* 1 = 6.57492 loss)
I0522 22:08:14.627430 35003 sgd_solver.cpp:112] Iteration 77300, lr = 0.01
I0522 22:08:18.799389 35003 solver.cpp:239] Iteration 77310 (2.39362 iter/s, 4.17778s/10 iters), loss = 6.56413
I0522 22:08:18.799444 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56413 (* 1 = 6.56413 loss)
I0522 22:08:19.508188 35003 sgd_solver.cpp:112] Iteration 77310, lr = 0.01
I0522 22:08:23.409036 35003 solver.cpp:239] Iteration 77320 (2.16948 iter/s, 4.6094s/10 iters), loss = 6.22612
I0522 22:08:23.409332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22612 (* 1 = 6.22612 loss)
I0522 22:08:23.415145 35003 sgd_solver.cpp:112] Iteration 77320, lr = 0.01
I0522 22:08:25.515092 35003 solver.cpp:239] Iteration 77330 (4.74903 iter/s, 2.10569s/10 iters), loss = 7.06095
I0522 22:08:25.515146 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06095 (* 1 = 7.06095 loss)
I0522 22:08:25.536242 35003 sgd_solver.cpp:112] Iteration 77330, lr = 0.01
I0522 22:08:30.714483 35003 solver.cpp:239] Iteration 77340 (1.92341 iter/s, 5.19911s/10 iters), loss = 7.49373
I0522 22:08:30.714551 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49373 (* 1 = 7.49373 loss)
I0522 22:08:30.722483 35003 sgd_solver.cpp:112] Iteration 77340, lr = 0.01
I0522 22:08:34.760416 35003 solver.cpp:239] Iteration 77350 (2.47177 iter/s, 4.04569s/10 iters), loss = 6.93165
I0522 22:08:34.760468 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93165 (* 1 = 6.93165 loss)
I0522 22:08:35.462316 35003 sgd_solver.cpp:112] Iteration 77350, lr = 0.01
I0522 22:08:40.257863 35003 solver.cpp:239] Iteration 77360 (1.81912 iter/s, 5.49717s/10 iters), loss = 7.06955
I0522 22:08:40.257910 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06955 (* 1 = 7.06955 loss)
I0522 22:08:40.263782 35003 sgd_solver.cpp:112] Iteration 77360, lr = 0.01
I0522 22:08:43.175462 35003 solver.cpp:239] Iteration 77370 (3.42768 iter/s, 2.91742s/10 iters), loss = 7.97962
I0522 22:08:43.175518 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97962 (* 1 = 7.97962 loss)
I0522 22:08:43.849104 35003 sgd_solver.cpp:112] Iteration 77370, lr = 0.01
I0522 22:08:46.630131 35003 solver.cpp:239] Iteration 77380 (2.89482 iter/s, 3.45444s/10 iters), loss = 7.6305
I0522 22:08:46.630198 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6305 (* 1 = 7.6305 loss)
I0522 22:08:46.635500 35003 sgd_solver.cpp:112] Iteration 77380, lr = 0.01
I0522 22:08:48.670516 35003 solver.cpp:239] Iteration 77390 (4.90143 iter/s, 2.04022s/10 iters), loss = 7.02771
I0522 22:08:48.670580 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02771 (* 1 = 7.02771 loss)
I0522 22:08:49.398949 35003 sgd_solver.cpp:112] Iteration 77390, lr = 0.01
I0522 22:08:52.235430 35003 solver.cpp:239] Iteration 77400 (2.80529 iter/s, 3.5647s/10 iters), loss = 7.74859
I0522 22:08:52.235493 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74859 (* 1 = 7.74859 loss)
I0522 22:08:52.253489 35003 sgd_solver.cpp:112] Iteration 77400, lr = 0.01
I0522 22:08:56.455162 35003 solver.cpp:239] Iteration 77410 (2.36995 iter/s, 4.21949s/10 iters), loss = 6.50281
I0522 22:08:56.455412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50281 (* 1 = 6.50281 loss)
I0522 22:08:56.463317 35003 sgd_solver.cpp:112] Iteration 77410, lr = 0.01
I0522 22:08:59.945698 35003 solver.cpp:239] Iteration 77420 (2.86519 iter/s, 3.49017s/10 iters), loss = 8.11955
I0522 22:08:59.945749 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.11955 (* 1 = 8.11955 loss)
I0522 22:08:59.958844 35003 sgd_solver.cpp:112] Iteration 77420, lr = 0.01
I0522 22:09:02.833266 35003 solver.cpp:239] Iteration 77430 (3.46333 iter/s, 2.88739s/10 iters), loss = 7.75215
I0522 22:09:02.833305 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75215 (* 1 = 7.75215 loss)
I0522 22:09:02.838593 35003 sgd_solver.cpp:112] Iteration 77430, lr = 0.01
I0522 22:09:06.440089 35003 solver.cpp:239] Iteration 77440 (2.77267 iter/s, 3.60663s/10 iters), loss = 6.88748
I0522 22:09:06.440135 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88748 (* 1 = 6.88748 loss)
I0522 22:09:07.161226 35003 sgd_solver.cpp:112] Iteration 77440, lr = 0.01
I0522 22:09:10.080742 35003 solver.cpp:239] Iteration 77450 (2.74691 iter/s, 3.64046s/10 iters), loss = 7.75742
I0522 22:09:10.080792 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75742 (* 1 = 7.75742 loss)
I0522 22:09:10.795982 35003 sgd_solver.cpp:112] Iteration 77450, lr = 0.01
I0522 22:09:14.393364 35003 solver.cpp:239] Iteration 77460 (2.31891 iter/s, 4.31237s/10 iters), loss = 6.97852
I0522 22:09:14.393414 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97852 (* 1 = 6.97852 loss)
I0522 22:09:14.409917 35003 sgd_solver.cpp:112] Iteration 77460, lr = 0.01
I0522 22:09:19.401042 35003 solver.cpp:239] Iteration 77470 (1.99704 iter/s, 5.00742s/10 iters), loss = 7.27271
I0522 22:09:19.401090 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27271 (* 1 = 7.27271 loss)
I0522 22:09:20.123090 35003 sgd_solver.cpp:112] Iteration 77470, lr = 0.01
I0522 22:09:22.997377 35003 solver.cpp:239] Iteration 77480 (2.78076 iter/s, 3.59613s/10 iters), loss = 7.99242
I0522 22:09:22.997418 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99242 (* 1 = 7.99242 loss)
I0522 22:09:23.010435 35003 sgd_solver.cpp:112] Iteration 77480, lr = 0.01
I0522 22:09:27.175961 35003 solver.cpp:239] Iteration 77490 (2.39328 iter/s, 4.17837s/10 iters), loss = 7.73495
I0522 22:09:27.176146 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73495 (* 1 = 7.73495 loss)
I0522 22:09:27.810837 35003 sgd_solver.cpp:112] Iteration 77490, lr = 0.01
I0522 22:09:30.709036 35003 solver.cpp:239] Iteration 77500 (2.83066 iter/s, 3.53274s/10 iters), loss = 7.87256
I0522 22:09:30.709084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87256 (* 1 = 7.87256 loss)
I0522 22:09:31.443413 35003 sgd_solver.cpp:112] Iteration 77500, lr = 0.01
I0522 22:09:34.138670 35003 solver.cpp:239] Iteration 77510 (2.91593 iter/s, 3.42944s/10 iters), loss = 7.60805
I0522 22:09:34.138756 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60805 (* 1 = 7.60805 loss)
I0522 22:09:34.162236 35003 sgd_solver.cpp:112] Iteration 77510, lr = 0.01
I0522 22:09:37.867331 35003 solver.cpp:239] Iteration 77520 (2.6821 iter/s, 3.72842s/10 iters), loss = 7.44794
I0522 22:09:37.867396 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44794 (* 1 = 7.44794 loss)
I0522 22:09:38.301340 35003 sgd_solver.cpp:112] Iteration 77520, lr = 0.01
I0522 22:09:42.018451 35003 solver.cpp:239] Iteration 77530 (2.40913 iter/s, 4.15088s/10 iters), loss = 7.57732
I0522 22:09:42.018503 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57732 (* 1 = 7.57732 loss)
I0522 22:09:42.759379 35003 sgd_solver.cpp:112] Iteration 77530, lr = 0.01
I0522 22:09:46.222188 35003 solver.cpp:239] Iteration 77540 (2.37897 iter/s, 4.20351s/10 iters), loss = 7.21504
I0522 22:09:46.222254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21504 (* 1 = 7.21504 loss)
I0522 22:09:46.243472 35003 sgd_solver.cpp:112] Iteration 77540, lr = 0.01
I0522 22:09:49.126864 35003 solver.cpp:239] Iteration 77550 (3.44295 iter/s, 2.90448s/10 iters), loss = 6.81294
I0522 22:09:49.126910 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81294 (* 1 = 6.81294 loss)
I0522 22:09:49.131376 35003 sgd_solver.cpp:112] Iteration 77550, lr = 0.01
I0522 22:09:51.387421 35003 solver.cpp:239] Iteration 77560 (4.42398 iter/s, 2.26041s/10 iters), loss = 7.81376
I0522 22:09:51.387464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81376 (* 1 = 7.81376 loss)
I0522 22:09:52.103189 35003 sgd_solver.cpp:112] Iteration 77560, lr = 0.01
I0522 22:09:54.123289 35003 solver.cpp:239] Iteration 77570 (3.65536 iter/s, 2.73571s/10 iters), loss = 7.82767
I0522 22:09:54.123338 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82767 (* 1 = 7.82767 loss)
I0522 22:09:54.136312 35003 sgd_solver.cpp:112] Iteration 77570, lr = 0.01
I0522 22:09:58.428949 35003 solver.cpp:239] Iteration 77580 (2.32265 iter/s, 4.30543s/10 iters), loss = 8.21043
I0522 22:09:58.429203 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.21043 (* 1 = 8.21043 loss)
I0522 22:09:58.457512 35003 sgd_solver.cpp:112] Iteration 77580, lr = 0.01
I0522 22:10:02.745570 35003 solver.cpp:239] Iteration 77590 (2.31686 iter/s, 4.31618s/10 iters), loss = 7.28295
I0522 22:10:02.745622 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28295 (* 1 = 7.28295 loss)
I0522 22:10:02.761118 35003 sgd_solver.cpp:112] Iteration 77590, lr = 0.01
I0522 22:10:06.364722 35003 solver.cpp:239] Iteration 77600 (2.76323 iter/s, 3.61895s/10 iters), loss = 7.81516
I0522 22:10:06.364759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81516 (* 1 = 7.81516 loss)
I0522 22:10:06.373833 35003 sgd_solver.cpp:112] Iteration 77600, lr = 0.01
I0522 22:10:08.515916 35003 solver.cpp:239] Iteration 77610 (4.64887 iter/s, 2.15106s/10 iters), loss = 6.5513
I0522 22:10:08.515980 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5513 (* 1 = 6.5513 loss)
I0522 22:10:08.524488 35003 sgd_solver.cpp:112] Iteration 77610, lr = 0.01
I0522 22:10:11.495648 35003 solver.cpp:239] Iteration 77620 (3.35622 iter/s, 2.97954s/10 iters), loss = 7.38456
I0522 22:10:11.495699 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38456 (* 1 = 7.38456 loss)
I0522 22:10:12.236802 35003 sgd_solver.cpp:112] Iteration 77620, lr = 0.01
I0522 22:10:15.723947 35003 solver.cpp:239] Iteration 77630 (2.36516 iter/s, 4.22805s/10 iters), loss = 7.36267
I0522 22:10:15.724014 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36267 (* 1 = 7.36267 loss)
I0522 22:10:15.871709 35003 sgd_solver.cpp:112] Iteration 77630, lr = 0.01
I0522 22:10:19.089107 35003 solver.cpp:239] Iteration 77640 (2.97181 iter/s, 3.36496s/10 iters), loss = 9.08267
I0522 22:10:19.089145 35003 solver.cpp:258]     Train net output #0: softmax_loss = 9.08267 (* 1 = 9.08267 loss)
I0522 22:10:19.105456 35003 sgd_solver.cpp:112] Iteration 77640, lr = 0.01
I0522 22:10:24.157088 35003 solver.cpp:239] Iteration 77650 (1.97327 iter/s, 5.06773s/10 iters), loss = 6.25091
I0522 22:10:24.157137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25091 (* 1 = 6.25091 loss)
I0522 22:10:24.166260 35003 sgd_solver.cpp:112] Iteration 77650, lr = 0.01
I0522 22:10:27.086071 35003 solver.cpp:239] Iteration 77660 (3.41437 iter/s, 2.9288s/10 iters), loss = 8.42933
I0522 22:10:27.086122 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.42933 (* 1 = 8.42933 loss)
I0522 22:10:27.800705 35003 sgd_solver.cpp:112] Iteration 77660, lr = 0.01
I0522 22:10:30.504040 35003 solver.cpp:239] Iteration 77670 (2.92588 iter/s, 3.41778s/10 iters), loss = 7.98835
I0522 22:10:30.504266 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98835 (* 1 = 7.98835 loss)
I0522 22:10:31.244925 35003 sgd_solver.cpp:112] Iteration 77670, lr = 0.01
I0522 22:10:34.880741 35003 solver.cpp:239] Iteration 77680 (2.28503 iter/s, 4.37631s/10 iters), loss = 7.28835
I0522 22:10:34.880792 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28835 (* 1 = 7.28835 loss)
I0522 22:10:34.893851 35003 sgd_solver.cpp:112] Iteration 77680, lr = 0.01
I0522 22:10:40.039024 35003 solver.cpp:239] Iteration 77690 (1.93873 iter/s, 5.15802s/10 iters), loss = 8.21446
I0522 22:10:40.039081 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.21446 (* 1 = 8.21446 loss)
I0522 22:10:40.047021 35003 sgd_solver.cpp:112] Iteration 77690, lr = 0.01
I0522 22:10:42.977911 35003 solver.cpp:239] Iteration 77700 (3.40286 iter/s, 2.9387s/10 iters), loss = 8.50052
I0522 22:10:42.977973 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.50052 (* 1 = 8.50052 loss)
I0522 22:10:43.692489 35003 sgd_solver.cpp:112] Iteration 77700, lr = 0.01
I0522 22:10:47.458525 35003 solver.cpp:239] Iteration 77710 (2.23196 iter/s, 4.48038s/10 iters), loss = 7.00198
I0522 22:10:47.458570 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00198 (* 1 = 7.00198 loss)
I0522 22:10:48.133303 35003 sgd_solver.cpp:112] Iteration 77710, lr = 0.01
I0522 22:10:51.725746 35003 solver.cpp:239] Iteration 77720 (2.34357 iter/s, 4.267s/10 iters), loss = 8.20909
I0522 22:10:51.725795 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.20909 (* 1 = 8.20909 loss)
I0522 22:10:52.460839 35003 sgd_solver.cpp:112] Iteration 77720, lr = 0.01
I0522 22:10:56.079707 35003 solver.cpp:239] Iteration 77730 (2.29688 iter/s, 4.35373s/10 iters), loss = 7.43528
I0522 22:10:56.079751 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43528 (* 1 = 7.43528 loss)
I0522 22:10:56.091130 35003 sgd_solver.cpp:112] Iteration 77730, lr = 0.01
I0522 22:10:58.858273 35003 solver.cpp:239] Iteration 77740 (3.5992 iter/s, 2.7784s/10 iters), loss = 6.15921
I0522 22:10:58.858325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15921 (* 1 = 6.15921 loss)
I0522 22:10:58.865710 35003 sgd_solver.cpp:112] Iteration 77740, lr = 0.01
I0522 22:11:02.379683 35003 solver.cpp:239] Iteration 77750 (2.83993 iter/s, 3.52121s/10 iters), loss = 7.38516
I0522 22:11:02.379932 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38516 (* 1 = 7.38516 loss)
I0522 22:11:02.393779 35003 sgd_solver.cpp:112] Iteration 77750, lr = 0.01
I0522 22:11:04.524494 35003 solver.cpp:239] Iteration 77760 (4.66315 iter/s, 2.14447s/10 iters), loss = 6.84066
I0522 22:11:04.524538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84066 (* 1 = 6.84066 loss)
I0522 22:11:05.233813 35003 sgd_solver.cpp:112] Iteration 77760, lr = 0.01
I0522 22:11:08.446847 35003 solver.cpp:239] Iteration 77770 (2.54963 iter/s, 3.92214s/10 iters), loss = 6.55013
I0522 22:11:08.446897 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55013 (* 1 = 6.55013 loss)
I0522 22:11:08.455968 35003 sgd_solver.cpp:112] Iteration 77770, lr = 0.01
I0522 22:11:11.673488 35003 solver.cpp:239] Iteration 77780 (3.09938 iter/s, 3.22645s/10 iters), loss = 6.98261
I0522 22:11:11.673547 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98261 (* 1 = 6.98261 loss)
I0522 22:11:12.377625 35003 sgd_solver.cpp:112] Iteration 77780, lr = 0.01
I0522 22:11:15.085165 35003 solver.cpp:239] Iteration 77790 (2.9313 iter/s, 3.41146s/10 iters), loss = 7.02525
I0522 22:11:15.085238 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02525 (* 1 = 7.02525 loss)
I0522 22:11:15.091358 35003 sgd_solver.cpp:112] Iteration 77790, lr = 0.01
I0522 22:11:18.483911 35003 solver.cpp:239] Iteration 77800 (2.94244 iter/s, 3.39854s/10 iters), loss = 8.65935
I0522 22:11:18.483963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.65935 (* 1 = 8.65935 loss)
I0522 22:11:18.491595 35003 sgd_solver.cpp:112] Iteration 77800, lr = 0.01
I0522 22:11:21.739245 35003 solver.cpp:239] Iteration 77810 (3.07206 iter/s, 3.25515s/10 iters), loss = 6.73213
I0522 22:11:21.739292 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73213 (* 1 = 6.73213 loss)
I0522 22:11:21.746851 35003 sgd_solver.cpp:112] Iteration 77810, lr = 0.01
I0522 22:11:24.619218 35003 solver.cpp:239] Iteration 77820 (3.47246 iter/s, 2.8798s/10 iters), loss = 6.73706
I0522 22:11:24.619266 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73706 (* 1 = 6.73706 loss)
I0522 22:11:25.353806 35003 sgd_solver.cpp:112] Iteration 77820, lr = 0.01
I0522 22:11:28.765837 35003 solver.cpp:239] Iteration 77830 (2.41173 iter/s, 4.14639s/10 iters), loss = 7.691
I0522 22:11:28.765892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.691 (* 1 = 7.691 loss)
I0522 22:11:28.778524 35003 sgd_solver.cpp:112] Iteration 77830, lr = 0.01
I0522 22:11:32.370635 35003 solver.cpp:239] Iteration 77840 (2.77425 iter/s, 3.60457s/10 iters), loss = 7.09367
I0522 22:11:32.370730 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09367 (* 1 = 7.09367 loss)
I0522 22:11:33.070168 35003 sgd_solver.cpp:112] Iteration 77840, lr = 0.01
I0522 22:11:36.400275 35003 solver.cpp:239] Iteration 77850 (2.48177 iter/s, 4.02939s/10 iters), loss = 7.41046
I0522 22:11:36.400321 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41046 (* 1 = 7.41046 loss)
I0522 22:11:36.413311 35003 sgd_solver.cpp:112] Iteration 77850, lr = 0.01
I0522 22:11:40.306898 35003 solver.cpp:239] Iteration 77860 (2.55989 iter/s, 3.90641s/10 iters), loss = 7.06561
I0522 22:11:40.306955 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06561 (* 1 = 7.06561 loss)
I0522 22:11:41.042032 35003 sgd_solver.cpp:112] Iteration 77860, lr = 0.01
I0522 22:11:44.675151 35003 solver.cpp:239] Iteration 77870 (2.28937 iter/s, 4.36802s/10 iters), loss = 8.4343
I0522 22:11:44.675217 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.4343 (* 1 = 8.4343 loss)
I0522 22:11:44.688041 35003 sgd_solver.cpp:112] Iteration 77870, lr = 0.01
I0522 22:11:46.743120 35003 solver.cpp:239] Iteration 77880 (4.83603 iter/s, 2.06781s/10 iters), loss = 7.63489
I0522 22:11:46.743166 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63489 (* 1 = 7.63489 loss)
I0522 22:11:46.772030 35003 sgd_solver.cpp:112] Iteration 77880, lr = 0.01
I0522 22:11:50.752909 35003 solver.cpp:239] Iteration 77890 (2.49403 iter/s, 4.00957s/10 iters), loss = 7.96179
I0522 22:11:50.752962 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96179 (* 1 = 7.96179 loss)
I0522 22:11:51.460047 35003 sgd_solver.cpp:112] Iteration 77890, lr = 0.01
I0522 22:11:54.534243 35003 solver.cpp:239] Iteration 77900 (2.64472 iter/s, 3.78112s/10 iters), loss = 6.42197
I0522 22:11:54.534296 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42197 (* 1 = 6.42197 loss)
I0522 22:11:54.877207 35003 sgd_solver.cpp:112] Iteration 77900, lr = 0.01
I0522 22:11:58.419173 35003 solver.cpp:239] Iteration 77910 (2.5742 iter/s, 3.8847s/10 iters), loss = 7.97279
I0522 22:11:58.419234 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97279 (* 1 = 7.97279 loss)
I0522 22:11:58.426980 35003 sgd_solver.cpp:112] Iteration 77910, lr = 0.01
I0522 22:12:02.045928 35003 solver.cpp:239] Iteration 77920 (2.75745 iter/s, 3.62655s/10 iters), loss = 6.18642
I0522 22:12:02.045979 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18642 (* 1 = 6.18642 loss)
I0522 22:12:02.650120 35003 sgd_solver.cpp:112] Iteration 77920, lr = 0.01
I0522 22:12:06.010969 35003 solver.cpp:239] Iteration 77930 (2.52219 iter/s, 3.96481s/10 iters), loss = 7.03314
I0522 22:12:06.011181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03314 (* 1 = 7.03314 loss)
I0522 22:12:06.014191 35003 sgd_solver.cpp:112] Iteration 77930, lr = 0.01
I0522 22:12:08.083027 35003 solver.cpp:239] Iteration 77940 (4.82698 iter/s, 2.07169s/10 iters), loss = 7.31523
I0522 22:12:08.083070 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31523 (* 1 = 7.31523 loss)
I0522 22:12:08.091079 35003 sgd_solver.cpp:112] Iteration 77940, lr = 0.01
I0522 22:12:11.695050 35003 solver.cpp:239] Iteration 77950 (2.76868 iter/s, 3.61183s/10 iters), loss = 7.79427
I0522 22:12:11.695098 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79427 (* 1 = 7.79427 loss)
I0522 22:12:11.709085 35003 sgd_solver.cpp:112] Iteration 77950, lr = 0.01
I0522 22:12:16.124828 35003 solver.cpp:239] Iteration 77960 (2.25757 iter/s, 4.42954s/10 iters), loss = 7.98121
I0522 22:12:16.124900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98121 (* 1 = 7.98121 loss)
I0522 22:12:16.834046 35003 sgd_solver.cpp:112] Iteration 77960, lr = 0.01
I0522 22:12:22.725349 35003 solver.cpp:239] Iteration 77970 (1.51511 iter/s, 6.60019s/10 iters), loss = 7.27991
I0522 22:12:22.725401 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27991 (* 1 = 7.27991 loss)
I0522 22:12:22.733345 35003 sgd_solver.cpp:112] Iteration 77970, lr = 0.01
I0522 22:12:27.856485 35003 solver.cpp:239] Iteration 77980 (1.949 iter/s, 5.13084s/10 iters), loss = 7.87861
I0522 22:12:27.856570 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87861 (* 1 = 7.87861 loss)
I0522 22:12:28.564752 35003 sgd_solver.cpp:112] Iteration 77980, lr = 0.01
I0522 22:12:32.138285 35003 solver.cpp:239] Iteration 77990 (2.33561 iter/s, 4.28154s/10 iters), loss = 7.4214
I0522 22:12:32.138339 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4214 (* 1 = 7.4214 loss)
I0522 22:12:32.151443 35003 sgd_solver.cpp:112] Iteration 77990, lr = 0.01
I0522 22:12:34.156368 35003 solver.cpp:239] Iteration 78000 (4.95554 iter/s, 2.01794s/10 iters), loss = 6.85198
I0522 22:12:34.156404 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85198 (* 1 = 6.85198 loss)
I0522 22:12:34.164477 35003 sgd_solver.cpp:112] Iteration 78000, lr = 0.01
I0522 22:12:36.869726 35003 solver.cpp:239] Iteration 78010 (3.68569 iter/s, 2.7132s/10 iters), loss = 6.81796
I0522 22:12:36.869951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81796 (* 1 = 6.81796 loss)
I0522 22:12:36.882064 35003 sgd_solver.cpp:112] Iteration 78010, lr = 0.01
I0522 22:12:39.748546 35003 solver.cpp:239] Iteration 78020 (3.47927 iter/s, 2.87416s/10 iters), loss = 6.8195
I0522 22:12:39.748657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8195 (* 1 = 6.8195 loss)
I0522 22:12:40.464009 35003 sgd_solver.cpp:112] Iteration 78020, lr = 0.01
I0522 22:12:44.808707 35003 solver.cpp:239] Iteration 78030 (1.97634 iter/s, 5.05985s/10 iters), loss = 6.24121
I0522 22:12:44.808749 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24121 (* 1 = 6.24121 loss)
I0522 22:12:44.821435 35003 sgd_solver.cpp:112] Iteration 78030, lr = 0.01
I0522 22:12:49.030905 35003 solver.cpp:239] Iteration 78040 (2.36856 iter/s, 4.22197s/10 iters), loss = 7.57814
I0522 22:12:49.030969 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57814 (* 1 = 7.57814 loss)
I0522 22:12:49.041646 35003 sgd_solver.cpp:112] Iteration 78040, lr = 0.01
I0522 22:12:53.116181 35003 solver.cpp:239] Iteration 78050 (2.44796 iter/s, 4.08504s/10 iters), loss = 7.22369
I0522 22:12:53.116226 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22369 (* 1 = 7.22369 loss)
I0522 22:12:53.122992 35003 sgd_solver.cpp:112] Iteration 78050, lr = 0.01
I0522 22:12:56.028028 35003 solver.cpp:239] Iteration 78060 (3.43445 iter/s, 2.91168s/10 iters), loss = 6.6526
I0522 22:12:56.028080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6526 (* 1 = 6.6526 loss)
I0522 22:12:56.047755 35003 sgd_solver.cpp:112] Iteration 78060, lr = 0.01
I0522 22:12:58.103237 35003 solver.cpp:239] Iteration 78070 (4.81913 iter/s, 2.07506s/10 iters), loss = 9.1628
I0522 22:12:58.103294 35003 solver.cpp:258]     Train net output #0: softmax_loss = 9.1628 (* 1 = 9.1628 loss)
I0522 22:12:58.747330 35003 sgd_solver.cpp:112] Iteration 78070, lr = 0.01
I0522 22:13:03.169463 35003 solver.cpp:239] Iteration 78080 (1.97396 iter/s, 5.06596s/10 iters), loss = 6.4776
I0522 22:13:03.169519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4776 (* 1 = 6.4776 loss)
I0522 22:13:03.182416 35003 sgd_solver.cpp:112] Iteration 78080, lr = 0.01
I0522 22:13:06.089947 35003 solver.cpp:239] Iteration 78090 (3.4243 iter/s, 2.92031s/10 iters), loss = 8.09537
I0522 22:13:06.089987 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.09537 (* 1 = 8.09537 loss)
I0522 22:13:06.824898 35003 sgd_solver.cpp:112] Iteration 78090, lr = 0.01
I0522 22:13:11.065353 35003 solver.cpp:239] Iteration 78100 (2.00999 iter/s, 4.97516s/10 iters), loss = 7.54661
I0522 22:13:11.065594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54661 (* 1 = 7.54661 loss)
I0522 22:13:11.078507 35003 sgd_solver.cpp:112] Iteration 78100, lr = 0.01
I0522 22:13:14.176735 35003 solver.cpp:239] Iteration 78110 (3.21435 iter/s, 3.11104s/10 iters), loss = 7.22403
I0522 22:13:14.176776 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22403 (* 1 = 7.22403 loss)
I0522 22:13:14.181874 35003 sgd_solver.cpp:112] Iteration 78110, lr = 0.01
I0522 22:13:17.805830 35003 solver.cpp:239] Iteration 78120 (2.75568 iter/s, 3.62887s/10 iters), loss = 7.79188
I0522 22:13:17.805932 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79188 (* 1 = 7.79188 loss)
I0522 22:13:17.808540 35003 sgd_solver.cpp:112] Iteration 78120, lr = 0.01
I0522 22:13:22.230075 35003 solver.cpp:239] Iteration 78130 (2.26042 iter/s, 4.42396s/10 iters), loss = 7.6603
I0522 22:13:22.230150 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6603 (* 1 = 7.6603 loss)
I0522 22:13:22.243630 35003 sgd_solver.cpp:112] Iteration 78130, lr = 0.01
I0522 22:13:27.010068 35003 solver.cpp:239] Iteration 78140 (2.09218 iter/s, 4.77971s/10 iters), loss = 7.13332
I0522 22:13:27.010113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13332 (* 1 = 7.13332 loss)
I0522 22:13:27.023819 35003 sgd_solver.cpp:112] Iteration 78140, lr = 0.01
I0522 22:13:31.274827 35003 solver.cpp:239] Iteration 78150 (2.34492 iter/s, 4.26453s/10 iters), loss = 7.39678
I0522 22:13:31.274879 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39678 (* 1 = 7.39678 loss)
I0522 22:13:31.287616 35003 sgd_solver.cpp:112] Iteration 78150, lr = 0.01
I0522 22:13:35.605581 35003 solver.cpp:239] Iteration 78160 (2.30919 iter/s, 4.33052s/10 iters), loss = 6.7392
I0522 22:13:35.605628 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7392 (* 1 = 6.7392 loss)
I0522 22:13:35.614070 35003 sgd_solver.cpp:112] Iteration 78160, lr = 0.01
I0522 22:13:37.943289 35003 solver.cpp:239] Iteration 78170 (4.27801 iter/s, 2.33753s/10 iters), loss = 6.79739
I0522 22:13:37.943348 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79739 (* 1 = 6.79739 loss)
I0522 22:13:37.950500 35003 sgd_solver.cpp:112] Iteration 78170, lr = 0.01
I0522 22:13:42.198773 35003 solver.cpp:239] Iteration 78180 (2.35004 iter/s, 4.25525s/10 iters), loss = 7.17314
I0522 22:13:42.199020 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17314 (* 1 = 7.17314 loss)
I0522 22:13:42.216542 35003 sgd_solver.cpp:112] Iteration 78180, lr = 0.01
I0522 22:13:45.317744 35003 solver.cpp:239] Iteration 78190 (3.21108 iter/s, 3.11422s/10 iters), loss = 7.25343
I0522 22:13:45.317797 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25343 (* 1 = 7.25343 loss)
I0522 22:13:45.319111 35003 sgd_solver.cpp:112] Iteration 78190, lr = 0.01
I0522 22:13:48.063817 35003 solver.cpp:239] Iteration 78200 (3.64178 iter/s, 2.74591s/10 iters), loss = 7.94298
I0522 22:13:48.063858 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94298 (* 1 = 7.94298 loss)
I0522 22:13:48.069033 35003 sgd_solver.cpp:112] Iteration 78200, lr = 0.01
I0522 22:13:52.422008 35003 solver.cpp:239] Iteration 78210 (2.29465 iter/s, 4.35796s/10 iters), loss = 8.71383
I0522 22:13:52.422070 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.71383 (* 1 = 8.71383 loss)
I0522 22:13:52.426309 35003 sgd_solver.cpp:112] Iteration 78210, lr = 0.01
I0522 22:13:56.061483 35003 solver.cpp:239] Iteration 78220 (2.74781 iter/s, 3.63926s/10 iters), loss = 7.56467
I0522 22:13:56.061524 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56467 (* 1 = 7.56467 loss)
I0522 22:13:56.068318 35003 sgd_solver.cpp:112] Iteration 78220, lr = 0.01
I0522 22:13:59.647509 35003 solver.cpp:239] Iteration 78230 (2.78875 iter/s, 3.58584s/10 iters), loss = 8.07761
I0522 22:13:59.647547 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.07761 (* 1 = 8.07761 loss)
I0522 22:13:59.758286 35003 sgd_solver.cpp:112] Iteration 78230, lr = 0.01
I0522 22:14:01.825527 35003 solver.cpp:239] Iteration 78240 (4.60084 iter/s, 2.17352s/10 iters), loss = 7.80733
I0522 22:14:01.825580 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80733 (* 1 = 7.80733 loss)
I0522 22:14:01.847477 35003 sgd_solver.cpp:112] Iteration 78240, lr = 0.01
I0522 22:14:04.603942 35003 solver.cpp:239] Iteration 78250 (3.5994 iter/s, 2.77824s/10 iters), loss = 7.48256
I0522 22:14:04.603993 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48256 (* 1 = 7.48256 loss)
I0522 22:14:04.610036 35003 sgd_solver.cpp:112] Iteration 78250, lr = 0.01
I0522 22:14:07.347091 35003 solver.cpp:239] Iteration 78260 (3.64568 iter/s, 2.74297s/10 iters), loss = 7.04775
I0522 22:14:07.347156 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04775 (* 1 = 7.04775 loss)
I0522 22:14:08.076596 35003 sgd_solver.cpp:112] Iteration 78260, lr = 0.01
I0522 22:14:11.689970 35003 solver.cpp:239] Iteration 78270 (2.30275 iter/s, 4.34264s/10 iters), loss = 7.53895
I0522 22:14:11.690032 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53895 (* 1 = 7.53895 loss)
I0522 22:14:12.422962 35003 sgd_solver.cpp:112] Iteration 78270, lr = 0.01
I0522 22:14:16.609313 35003 solver.cpp:239] Iteration 78280 (2.0329 iter/s, 4.91907s/10 iters), loss = 7.00277
I0522 22:14:16.609380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00277 (* 1 = 7.00277 loss)
I0522 22:14:16.622120 35003 sgd_solver.cpp:112] Iteration 78280, lr = 0.01
I0522 22:14:20.184420 35003 solver.cpp:239] Iteration 78290 (2.79729 iter/s, 3.57489s/10 iters), loss = 7.53183
I0522 22:14:20.184468 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53183 (* 1 = 7.53183 loss)
I0522 22:14:20.192433 35003 sgd_solver.cpp:112] Iteration 78290, lr = 0.01
I0522 22:14:24.391253 35003 solver.cpp:239] Iteration 78300 (2.37723 iter/s, 4.20657s/10 iters), loss = 7.84253
I0522 22:14:24.391332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84253 (* 1 = 7.84253 loss)
I0522 22:14:24.393877 35003 sgd_solver.cpp:112] Iteration 78300, lr = 0.01
I0522 22:14:29.151126 35003 solver.cpp:239] Iteration 78310 (2.10103 iter/s, 4.75958s/10 iters), loss = 7.74616
I0522 22:14:29.151182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74616 (* 1 = 7.74616 loss)
I0522 22:14:29.173542 35003 sgd_solver.cpp:112] Iteration 78310, lr = 0.01
I0522 22:14:31.980922 35003 solver.cpp:239] Iteration 78320 (3.53404 iter/s, 2.82962s/10 iters), loss = 7.63652
I0522 22:14:31.980965 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63652 (* 1 = 7.63652 loss)
I0522 22:14:32.075634 35003 sgd_solver.cpp:112] Iteration 78320, lr = 0.01
I0522 22:14:36.220731 35003 solver.cpp:239] Iteration 78330 (2.35873 iter/s, 4.23957s/10 iters), loss = 8.06815
I0522 22:14:36.220796 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.06815 (* 1 = 8.06815 loss)
I0522 22:14:36.962033 35003 sgd_solver.cpp:112] Iteration 78330, lr = 0.01
I0522 22:14:40.407949 35003 solver.cpp:239] Iteration 78340 (2.38836 iter/s, 4.18697s/10 iters), loss = 8.443
I0522 22:14:40.408002 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.443 (* 1 = 8.443 loss)
I0522 22:14:40.420998 35003 sgd_solver.cpp:112] Iteration 78340, lr = 0.01
I0522 22:14:43.407702 35003 solver.cpp:239] Iteration 78350 (3.3338 iter/s, 2.99958s/10 iters), loss = 6.25692
I0522 22:14:43.407829 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25692 (* 1 = 6.25692 loss)
I0522 22:14:43.413061 35003 sgd_solver.cpp:112] Iteration 78350, lr = 0.01
I0522 22:14:48.746306 35003 solver.cpp:239] Iteration 78360 (1.87327 iter/s, 5.33826s/10 iters), loss = 7.00874
I0522 22:14:48.746356 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00874 (* 1 = 7.00874 loss)
I0522 22:14:48.756920 35003 sgd_solver.cpp:112] Iteration 78360, lr = 0.01
I0522 22:14:50.664615 35003 solver.cpp:239] Iteration 78370 (5.21337 iter/s, 1.91814s/10 iters), loss = 7.5838
I0522 22:14:50.664678 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5838 (* 1 = 7.5838 loss)
I0522 22:14:50.676378 35003 sgd_solver.cpp:112] Iteration 78370, lr = 0.01
I0522 22:14:54.347546 35003 solver.cpp:239] Iteration 78380 (2.71539 iter/s, 3.68271s/10 iters), loss = 7.01528
I0522 22:14:54.347606 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01528 (* 1 = 7.01528 loss)
I0522 22:14:55.042176 35003 sgd_solver.cpp:112] Iteration 78380, lr = 0.01
I0522 22:14:57.912508 35003 solver.cpp:239] Iteration 78390 (2.80524 iter/s, 3.56475s/10 iters), loss = 6.7796
I0522 22:14:57.912554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7796 (* 1 = 6.7796 loss)
I0522 22:14:58.236835 35003 sgd_solver.cpp:112] Iteration 78390, lr = 0.01
I0522 22:15:02.764503 35003 solver.cpp:239] Iteration 78400 (2.06111 iter/s, 4.85175s/10 iters), loss = 8.01008
I0522 22:15:02.764549 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01008 (* 1 = 8.01008 loss)
I0522 22:15:03.505272 35003 sgd_solver.cpp:112] Iteration 78400, lr = 0.01
I0522 22:15:06.865414 35003 solver.cpp:239] Iteration 78410 (2.43861 iter/s, 4.10069s/10 iters), loss = 6.9375
I0522 22:15:06.865463 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9375 (* 1 = 6.9375 loss)
I0522 22:15:07.580245 35003 sgd_solver.cpp:112] Iteration 78410, lr = 0.01
I0522 22:15:11.420799 35003 solver.cpp:239] Iteration 78420 (2.19532 iter/s, 4.55514s/10 iters), loss = 7.26222
I0522 22:15:11.420857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26222 (* 1 = 7.26222 loss)
I0522 22:15:12.129973 35003 sgd_solver.cpp:112] Iteration 78420, lr = 0.01
I0522 22:15:14.893486 35003 solver.cpp:239] Iteration 78430 (2.87978 iter/s, 3.47248s/10 iters), loss = 6.6847
I0522 22:15:14.893745 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6847 (* 1 = 6.6847 loss)
I0522 22:15:15.634443 35003 sgd_solver.cpp:112] Iteration 78430, lr = 0.01
I0522 22:15:19.816022 35003 solver.cpp:239] Iteration 78440 (2.03166 iter/s, 4.92209s/10 iters), loss = 7.78852
I0522 22:15:19.816064 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78852 (* 1 = 7.78852 loss)
I0522 22:15:19.829704 35003 sgd_solver.cpp:112] Iteration 78440, lr = 0.01
I0522 22:15:23.190585 35003 solver.cpp:239] Iteration 78450 (2.96351 iter/s, 3.37438s/10 iters), loss = 8.51917
I0522 22:15:23.190637 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.51917 (* 1 = 8.51917 loss)
I0522 22:15:23.893169 35003 sgd_solver.cpp:112] Iteration 78450, lr = 0.01
I0522 22:15:27.482666 35003 solver.cpp:239] Iteration 78460 (2.32999 iter/s, 4.29186s/10 iters), loss = 8.05681
I0522 22:15:27.482741 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05681 (* 1 = 8.05681 loss)
I0522 22:15:28.223987 35003 sgd_solver.cpp:112] Iteration 78460, lr = 0.01
I0522 22:15:31.294068 35003 solver.cpp:239] Iteration 78470 (2.62387 iter/s, 3.81116s/10 iters), loss = 8.10278
I0522 22:15:31.294128 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.10278 (* 1 = 8.10278 loss)
I0522 22:15:31.298627 35003 sgd_solver.cpp:112] Iteration 78470, lr = 0.01
I0522 22:15:34.223922 35003 solver.cpp:239] Iteration 78480 (3.41337 iter/s, 2.92965s/10 iters), loss = 7.82427
I0522 22:15:34.223978 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82427 (* 1 = 7.82427 loss)
I0522 22:15:34.230425 35003 sgd_solver.cpp:112] Iteration 78480, lr = 0.01
I0522 22:15:37.156603 35003 solver.cpp:239] Iteration 78490 (3.41005 iter/s, 2.9325s/10 iters), loss = 7.17101
I0522 22:15:37.156644 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17101 (* 1 = 7.17101 loss)
I0522 22:15:37.176255 35003 sgd_solver.cpp:112] Iteration 78490, lr = 0.01
I0522 22:15:39.918442 35003 solver.cpp:239] Iteration 78500 (3.62099 iter/s, 2.76168s/10 iters), loss = 7.9873
I0522 22:15:39.918488 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9873 (* 1 = 7.9873 loss)
I0522 22:15:39.930315 35003 sgd_solver.cpp:112] Iteration 78500, lr = 0.01
I0522 22:15:42.024327 35003 solver.cpp:239] Iteration 78510 (4.74896 iter/s, 2.10572s/10 iters), loss = 7.5026
I0522 22:15:42.024364 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5026 (* 1 = 7.5026 loss)
I0522 22:15:42.706527 35003 sgd_solver.cpp:112] Iteration 78510, lr = 0.01
I0522 22:15:46.105947 35003 solver.cpp:239] Iteration 78520 (2.45013 iter/s, 4.08141s/10 iters), loss = 7.36154
I0522 22:15:46.106189 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36154 (* 1 = 7.36154 loss)
I0522 22:15:46.109398 35003 sgd_solver.cpp:112] Iteration 78520, lr = 0.01
I0522 22:15:49.629779 35003 solver.cpp:239] Iteration 78530 (2.83811 iter/s, 3.52347s/10 iters), loss = 6.46821
I0522 22:15:49.629822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46821 (* 1 = 6.46821 loss)
I0522 22:15:49.636145 35003 sgd_solver.cpp:112] Iteration 78530, lr = 0.01
I0522 22:15:52.395371 35003 solver.cpp:239] Iteration 78540 (3.61607 iter/s, 2.76543s/10 iters), loss = 6.78823
I0522 22:15:52.395416 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78823 (* 1 = 6.78823 loss)
I0522 22:15:52.408167 35003 sgd_solver.cpp:112] Iteration 78540, lr = 0.01
I0522 22:15:54.738155 35003 solver.cpp:239] Iteration 78550 (4.26869 iter/s, 2.34264s/10 iters), loss = 6.94439
I0522 22:15:54.738189 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94439 (* 1 = 6.94439 loss)
I0522 22:15:54.763268 35003 sgd_solver.cpp:112] Iteration 78550, lr = 0.01
I0522 22:15:57.472443 35003 solver.cpp:239] Iteration 78560 (3.65746 iter/s, 2.73414s/10 iters), loss = 7.52859
I0522 22:15:57.472482 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52859 (* 1 = 7.52859 loss)
I0522 22:15:57.485774 35003 sgd_solver.cpp:112] Iteration 78560, lr = 0.01
I0522 22:16:01.375247 35003 solver.cpp:239] Iteration 78570 (2.5624 iter/s, 3.90259s/10 iters), loss = 7.97528
I0522 22:16:01.375311 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97528 (* 1 = 7.97528 loss)
I0522 22:16:01.442523 35003 sgd_solver.cpp:112] Iteration 78570, lr = 0.01
I0522 22:16:05.099611 35003 solver.cpp:239] Iteration 78580 (2.68518 iter/s, 3.72415s/10 iters), loss = 7.23545
I0522 22:16:05.099659 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23545 (* 1 = 7.23545 loss)
I0522 22:16:05.834786 35003 sgd_solver.cpp:112] Iteration 78580, lr = 0.01
I0522 22:16:09.195982 35003 solver.cpp:239] Iteration 78590 (2.44132 iter/s, 4.09615s/10 iters), loss = 7.29106
I0522 22:16:09.196038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29106 (* 1 = 7.29106 loss)
I0522 22:16:09.208940 35003 sgd_solver.cpp:112] Iteration 78590, lr = 0.01
I0522 22:16:11.657414 35003 solver.cpp:239] Iteration 78600 (4.06294 iter/s, 2.46127s/10 iters), loss = 7.21918
I0522 22:16:11.657455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21918 (* 1 = 7.21918 loss)
I0522 22:16:11.668612 35003 sgd_solver.cpp:112] Iteration 78600, lr = 0.01
I0522 22:16:15.299803 35003 solver.cpp:239] Iteration 78610 (2.7456 iter/s, 3.64219s/10 iters), loss = 7.34773
I0522 22:16:15.299844 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34773 (* 1 = 7.34773 loss)
I0522 22:16:15.318063 35003 sgd_solver.cpp:112] Iteration 78610, lr = 0.01
I0522 22:16:17.741148 35003 solver.cpp:239] Iteration 78620 (4.09635 iter/s, 2.4412s/10 iters), loss = 7.95305
I0522 22:16:17.741354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95305 (* 1 = 7.95305 loss)
I0522 22:16:17.754554 35003 sgd_solver.cpp:112] Iteration 78620, lr = 0.01
I0522 22:16:21.371353 35003 solver.cpp:239] Iteration 78630 (2.75494 iter/s, 3.62985s/10 iters), loss = 8.08363
I0522 22:16:21.371400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08363 (* 1 = 8.08363 loss)
I0522 22:16:21.384650 35003 sgd_solver.cpp:112] Iteration 78630, lr = 0.01
I0522 22:16:23.472678 35003 solver.cpp:239] Iteration 78640 (4.75922 iter/s, 2.10118s/10 iters), loss = 6.81608
I0522 22:16:23.472717 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81608 (* 1 = 6.81608 loss)
I0522 22:16:24.113281 35003 sgd_solver.cpp:112] Iteration 78640, lr = 0.01
I0522 22:16:27.082327 35003 solver.cpp:239] Iteration 78650 (2.7705 iter/s, 3.60946s/10 iters), loss = 7.75648
I0522 22:16:27.082370 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75648 (* 1 = 7.75648 loss)
I0522 22:16:27.107805 35003 sgd_solver.cpp:112] Iteration 78650, lr = 0.01
I0522 22:16:29.215018 35003 solver.cpp:239] Iteration 78660 (4.68921 iter/s, 2.13255s/10 iters), loss = 7.46458
I0522 22:16:29.215056 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46458 (* 1 = 7.46458 loss)
I0522 22:16:29.923990 35003 sgd_solver.cpp:112] Iteration 78660, lr = 0.01
I0522 22:16:32.692006 35003 solver.cpp:239] Iteration 78670 (2.8762 iter/s, 3.47681s/10 iters), loss = 8.2611
I0522 22:16:32.692050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.2611 (* 1 = 8.2611 loss)
I0522 22:16:32.712225 35003 sgd_solver.cpp:112] Iteration 78670, lr = 0.01
I0522 22:16:35.536145 35003 solver.cpp:239] Iteration 78680 (3.51621 iter/s, 2.84397s/10 iters), loss = 8.02177
I0522 22:16:35.536191 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02177 (* 1 = 8.02177 loss)
I0522 22:16:35.545704 35003 sgd_solver.cpp:112] Iteration 78680, lr = 0.01
I0522 22:16:38.254840 35003 solver.cpp:239] Iteration 78690 (3.67846 iter/s, 2.71853s/10 iters), loss = 7.80332
I0522 22:16:38.254884 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80332 (* 1 = 7.80332 loss)
I0522 22:16:38.260442 35003 sgd_solver.cpp:112] Iteration 78690, lr = 0.01
I0522 22:16:42.298105 35003 solver.cpp:239] Iteration 78700 (2.47338 iter/s, 4.04305s/10 iters), loss = 7.46326
I0522 22:16:42.298154 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46326 (* 1 = 7.46326 loss)
I0522 22:16:42.705163 35003 sgd_solver.cpp:112] Iteration 78700, lr = 0.01
I0522 22:16:45.406883 35003 solver.cpp:239] Iteration 78710 (3.2169 iter/s, 3.10858s/10 iters), loss = 8.18277
I0522 22:16:45.406939 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.18277 (* 1 = 8.18277 loss)
I0522 22:16:45.420419 35003 sgd_solver.cpp:112] Iteration 78710, lr = 0.01
I0522 22:16:47.454952 35003 solver.cpp:239] Iteration 78720 (4.88299 iter/s, 2.04793s/10 iters), loss = 7.70621
I0522 22:16:47.454998 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70621 (* 1 = 7.70621 loss)
I0522 22:16:47.464053 35003 sgd_solver.cpp:112] Iteration 78720, lr = 0.01
I0522 22:16:52.433470 35003 solver.cpp:239] Iteration 78730 (2.00873 iter/s, 4.97827s/10 iters), loss = 7.19133
I0522 22:16:52.433643 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19133 (* 1 = 7.19133 loss)
I0522 22:16:52.446771 35003 sgd_solver.cpp:112] Iteration 78730, lr = 0.01
I0522 22:16:55.732697 35003 solver.cpp:239] Iteration 78740 (3.0313 iter/s, 3.29892s/10 iters), loss = 6.83867
I0522 22:16:55.732735 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83867 (* 1 = 6.83867 loss)
I0522 22:16:55.750926 35003 sgd_solver.cpp:112] Iteration 78740, lr = 0.01
I0522 22:17:00.241048 35003 solver.cpp:239] Iteration 78750 (2.21822 iter/s, 4.50812s/10 iters), loss = 7.74825
I0522 22:17:00.241093 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74825 (* 1 = 7.74825 loss)
I0522 22:17:00.255916 35003 sgd_solver.cpp:112] Iteration 78750, lr = 0.01
I0522 22:17:03.509519 35003 solver.cpp:239] Iteration 78760 (3.05971 iter/s, 3.26829s/10 iters), loss = 7.9459
I0522 22:17:03.509572 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9459 (* 1 = 7.9459 loss)
I0522 22:17:04.212131 35003 sgd_solver.cpp:112] Iteration 78760, lr = 0.01
I0522 22:17:06.993811 35003 solver.cpp:239] Iteration 78770 (2.87019 iter/s, 3.48409s/10 iters), loss = 8.48535
I0522 22:17:06.993858 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.48535 (* 1 = 8.48535 loss)
I0522 22:17:07.003298 35003 sgd_solver.cpp:112] Iteration 78770, lr = 0.01
I0522 22:17:09.643856 35003 solver.cpp:239] Iteration 78780 (3.77377 iter/s, 2.64987s/10 iters), loss = 7.14295
I0522 22:17:09.643927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14295 (* 1 = 7.14295 loss)
I0522 22:17:10.345389 35003 sgd_solver.cpp:112] Iteration 78780, lr = 0.01
I0522 22:17:13.156249 35003 solver.cpp:239] Iteration 78790 (2.84724 iter/s, 3.51218s/10 iters), loss = 5.96803
I0522 22:17:13.156288 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96803 (* 1 = 5.96803 loss)
I0522 22:17:13.169402 35003 sgd_solver.cpp:112] Iteration 78790, lr = 0.01
I0522 22:17:15.748214 35003 solver.cpp:239] Iteration 78800 (3.8583 iter/s, 2.59181s/10 iters), loss = 5.94685
I0522 22:17:15.748256 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.94685 (* 1 = 5.94685 loss)
I0522 22:17:16.470311 35003 sgd_solver.cpp:112] Iteration 78800, lr = 0.01
I0522 22:17:20.021502 35003 solver.cpp:239] Iteration 78810 (2.34024 iter/s, 4.27306s/10 iters), loss = 6.00218
I0522 22:17:20.021559 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00218 (* 1 = 6.00218 loss)
I0522 22:17:20.717216 35003 sgd_solver.cpp:112] Iteration 78810, lr = 0.01
I0522 22:17:23.389056 35003 solver.cpp:239] Iteration 78820 (2.96968 iter/s, 3.36736s/10 iters), loss = 7.55816
I0522 22:17:23.389257 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55816 (* 1 = 7.55816 loss)
I0522 22:17:23.957062 35003 sgd_solver.cpp:112] Iteration 78820, lr = 0.01
I0522 22:17:27.093513 35003 solver.cpp:239] Iteration 78830 (2.6997 iter/s, 3.70412s/10 iters), loss = 7.08102
I0522 22:17:27.093562 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08102 (* 1 = 7.08102 loss)
I0522 22:17:27.099781 35003 sgd_solver.cpp:112] Iteration 78830, lr = 0.01
I0522 22:17:31.474169 35003 solver.cpp:239] Iteration 78840 (2.28289 iter/s, 4.38041s/10 iters), loss = 7.05798
I0522 22:17:31.474242 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05798 (* 1 = 7.05798 loss)
I0522 22:17:31.487000 35003 sgd_solver.cpp:112] Iteration 78840, lr = 0.01
I0522 22:17:34.682940 35003 solver.cpp:239] Iteration 78850 (3.11666 iter/s, 3.20856s/10 iters), loss = 6.37951
I0522 22:17:34.682991 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37951 (* 1 = 6.37951 loss)
I0522 22:17:35.378401 35003 sgd_solver.cpp:112] Iteration 78850, lr = 0.01
I0522 22:17:39.591642 35003 solver.cpp:239] Iteration 78860 (2.0373 iter/s, 4.90846s/10 iters), loss = 7.1328
I0522 22:17:39.591686 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1328 (* 1 = 7.1328 loss)
I0522 22:17:39.610435 35003 sgd_solver.cpp:112] Iteration 78860, lr = 0.01
I0522 22:17:44.517616 35003 solver.cpp:239] Iteration 78870 (2.03017 iter/s, 4.9257s/10 iters), loss = 8.70349
I0522 22:17:44.517694 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.70349 (* 1 = 8.70349 loss)
I0522 22:17:45.199949 35003 sgd_solver.cpp:112] Iteration 78870, lr = 0.01
I0522 22:17:47.321535 35003 solver.cpp:239] Iteration 78880 (3.56672 iter/s, 2.8037s/10 iters), loss = 7.13944
I0522 22:17:47.321588 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13944 (* 1 = 7.13944 loss)
I0522 22:17:47.327688 35003 sgd_solver.cpp:112] Iteration 78880, lr = 0.01
I0522 22:17:50.842646 35003 solver.cpp:239] Iteration 78890 (2.84018 iter/s, 3.52091s/10 iters), loss = 8.46178
I0522 22:17:50.842726 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.46178 (* 1 = 8.46178 loss)
I0522 22:17:51.558130 35003 sgd_solver.cpp:112] Iteration 78890, lr = 0.01
I0522 22:17:53.724105 35003 solver.cpp:239] Iteration 78900 (3.4707 iter/s, 2.88126s/10 iters), loss = 6.85181
I0522 22:17:53.724257 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85181 (* 1 = 6.85181 loss)
I0522 22:17:53.728956 35003 sgd_solver.cpp:112] Iteration 78900, lr = 0.01
I0522 22:17:55.988730 35003 solver.cpp:239] Iteration 78910 (4.41625 iter/s, 2.26436s/10 iters), loss = 7.33595
I0522 22:17:55.988798 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33595 (* 1 = 7.33595 loss)
I0522 22:17:56.642946 35003 sgd_solver.cpp:112] Iteration 78910, lr = 0.01
I0522 22:18:00.707078 35003 solver.cpp:239] Iteration 78920 (2.1195 iter/s, 4.71809s/10 iters), loss = 7.57581
I0522 22:18:00.707130 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57581 (* 1 = 7.57581 loss)
I0522 22:18:00.726258 35003 sgd_solver.cpp:112] Iteration 78920, lr = 0.01
I0522 22:18:03.496789 35003 solver.cpp:239] Iteration 78930 (3.58482 iter/s, 2.78954s/10 iters), loss = 7.31901
I0522 22:18:03.496840 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31901 (* 1 = 7.31901 loss)
I0522 22:18:03.505955 35003 sgd_solver.cpp:112] Iteration 78930, lr = 0.01
I0522 22:18:05.555157 35003 solver.cpp:239] Iteration 78940 (4.85856 iter/s, 2.05822s/10 iters), loss = 7.46805
I0522 22:18:05.555207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46805 (* 1 = 7.46805 loss)
I0522 22:18:05.568624 35003 sgd_solver.cpp:112] Iteration 78940, lr = 0.01
I0522 22:18:08.536165 35003 solver.cpp:239] Iteration 78950 (3.35479 iter/s, 2.98082s/10 iters), loss = 6.74035
I0522 22:18:08.536221 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74035 (* 1 = 6.74035 loss)
I0522 22:18:08.549044 35003 sgd_solver.cpp:112] Iteration 78950, lr = 0.01
I0522 22:18:13.469439 35003 solver.cpp:239] Iteration 78960 (2.02716 iter/s, 4.933s/10 iters), loss = 7.53224
I0522 22:18:13.469498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53224 (* 1 = 7.53224 loss)
I0522 22:18:14.210366 35003 sgd_solver.cpp:112] Iteration 78960, lr = 0.01
I0522 22:18:17.959414 35003 solver.cpp:239] Iteration 78970 (2.2273 iter/s, 4.48973s/10 iters), loss = 8.28747
I0522 22:18:17.959462 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.28747 (* 1 = 8.28747 loss)
I0522 22:18:18.655287 35003 sgd_solver.cpp:112] Iteration 78970, lr = 0.01
I0522 22:18:21.124740 35003 solver.cpp:239] Iteration 78980 (3.15942 iter/s, 3.16514s/10 iters), loss = 6.87898
I0522 22:18:21.124781 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87898 (* 1 = 6.87898 loss)
I0522 22:18:21.138252 35003 sgd_solver.cpp:112] Iteration 78980, lr = 0.01
I0522 22:18:24.621526 35003 solver.cpp:239] Iteration 78990 (2.85993 iter/s, 3.49659s/10 iters), loss = 7.2849
I0522 22:18:24.621834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2849 (* 1 = 7.2849 loss)
I0522 22:18:24.634598 35003 sgd_solver.cpp:112] Iteration 78990, lr = 0.01
I0522 22:18:27.488751 35003 solver.cpp:239] Iteration 79000 (3.48819 iter/s, 2.86682s/10 iters), loss = 7.04765
I0522 22:18:27.488822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04765 (* 1 = 7.04765 loss)
I0522 22:18:27.492395 35003 sgd_solver.cpp:112] Iteration 79000, lr = 0.01
I0522 22:18:30.330462 35003 solver.cpp:239] Iteration 79010 (3.51925 iter/s, 2.84151s/10 iters), loss = 6.76729
I0522 22:18:30.330514 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76729 (* 1 = 6.76729 loss)
I0522 22:18:30.341269 35003 sgd_solver.cpp:112] Iteration 79010, lr = 0.01
I0522 22:18:34.695487 35003 solver.cpp:239] Iteration 79020 (2.29107 iter/s, 4.36477s/10 iters), loss = 7.59258
I0522 22:18:34.695554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59258 (* 1 = 7.59258 loss)
I0522 22:18:35.170655 35003 sgd_solver.cpp:112] Iteration 79020, lr = 0.01
I0522 22:18:38.725710 35003 solver.cpp:239] Iteration 79030 (2.4814 iter/s, 4.02999s/10 iters), loss = 7.1555
I0522 22:18:38.725755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1555 (* 1 = 7.1555 loss)
I0522 22:18:39.033447 35003 sgd_solver.cpp:112] Iteration 79030, lr = 0.01
I0522 22:18:43.961292 35003 solver.cpp:239] Iteration 79040 (1.9101 iter/s, 5.23532s/10 iters), loss = 6.97764
I0522 22:18:43.961340 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97764 (* 1 = 6.97764 loss)
I0522 22:18:44.059581 35003 sgd_solver.cpp:112] Iteration 79040, lr = 0.01
I0522 22:18:46.180909 35003 solver.cpp:239] Iteration 79050 (4.50559 iter/s, 2.21947s/10 iters), loss = 7.52348
I0522 22:18:46.180960 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52348 (* 1 = 7.52348 loss)
I0522 22:18:46.908879 35003 sgd_solver.cpp:112] Iteration 79050, lr = 0.01
I0522 22:18:48.918951 35003 solver.cpp:239] Iteration 79060 (3.65246 iter/s, 2.73788s/10 iters), loss = 6.5137
I0522 22:18:48.918995 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5137 (* 1 = 6.5137 loss)
I0522 22:18:49.536715 35003 sgd_solver.cpp:112] Iteration 79060, lr = 0.01
I0522 22:18:51.957856 35003 solver.cpp:239] Iteration 79070 (3.29085 iter/s, 3.03873s/10 iters), loss = 7.2393
I0522 22:18:51.957916 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2393 (* 1 = 7.2393 loss)
I0522 22:18:51.973281 35003 sgd_solver.cpp:112] Iteration 79070, lr = 0.01
I0522 22:18:54.011726 35003 solver.cpp:239] Iteration 79080 (4.86923 iter/s, 2.05371s/10 iters), loss = 7.69268
I0522 22:18:54.011770 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69268 (* 1 = 7.69268 loss)
I0522 22:18:54.018427 35003 sgd_solver.cpp:112] Iteration 79080, lr = 0.01
I0522 22:18:58.359467 35003 solver.cpp:239] Iteration 79090 (2.30017 iter/s, 4.34751s/10 iters), loss = 7.66251
I0522 22:18:58.359745 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66251 (* 1 = 7.66251 loss)
I0522 22:18:58.366153 35003 sgd_solver.cpp:112] Iteration 79090, lr = 0.01
I0522 22:19:02.382194 35003 solver.cpp:239] Iteration 79100 (2.48615 iter/s, 4.02229s/10 iters), loss = 7.4381
I0522 22:19:02.382241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4381 (* 1 = 7.4381 loss)
I0522 22:19:02.538173 35003 sgd_solver.cpp:112] Iteration 79100, lr = 0.01
I0522 22:19:06.800232 35003 solver.cpp:239] Iteration 79110 (2.26357 iter/s, 4.41781s/10 iters), loss = 7.81799
I0522 22:19:06.800282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81799 (* 1 = 7.81799 loss)
I0522 22:19:06.803330 35003 sgd_solver.cpp:112] Iteration 79110, lr = 0.01
I0522 22:19:09.659188 35003 solver.cpp:239] Iteration 79120 (3.498 iter/s, 2.85878s/10 iters), loss = 6.90413
I0522 22:19:09.659236 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90413 (* 1 = 6.90413 loss)
I0522 22:19:10.400766 35003 sgd_solver.cpp:112] Iteration 79120, lr = 0.01
I0522 22:19:12.471395 35003 solver.cpp:239] Iteration 79130 (3.55615 iter/s, 2.81203s/10 iters), loss = 5.81767
I0522 22:19:12.471459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.81767 (* 1 = 5.81767 loss)
I0522 22:19:12.477005 35003 sgd_solver.cpp:112] Iteration 79130, lr = 0.01
I0522 22:19:16.689115 35003 solver.cpp:239] Iteration 79140 (2.37108 iter/s, 4.21748s/10 iters), loss = 7.53228
I0522 22:19:16.689167 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53228 (* 1 = 7.53228 loss)
I0522 22:19:16.695899 35003 sgd_solver.cpp:112] Iteration 79140, lr = 0.01
I0522 22:19:20.203547 35003 solver.cpp:239] Iteration 79150 (2.84558 iter/s, 3.51422s/10 iters), loss = 8.23674
I0522 22:19:20.203586 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.23674 (* 1 = 8.23674 loss)
I0522 22:19:20.860812 35003 sgd_solver.cpp:112] Iteration 79150, lr = 0.01
I0522 22:19:24.441819 35003 solver.cpp:239] Iteration 79160 (2.35958 iter/s, 4.23805s/10 iters), loss = 7.3012
I0522 22:19:24.441869 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3012 (* 1 = 7.3012 loss)
I0522 22:19:24.452375 35003 sgd_solver.cpp:112] Iteration 79160, lr = 0.01
I0522 22:19:28.846489 35003 solver.cpp:239] Iteration 79170 (2.27044 iter/s, 4.40444s/10 iters), loss = 6.77084
I0522 22:19:28.846658 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77084 (* 1 = 6.77084 loss)
I0522 22:19:29.445036 35003 sgd_solver.cpp:112] Iteration 79170, lr = 0.01
I0522 22:19:31.565737 35003 solver.cpp:239] Iteration 79180 (3.67787 iter/s, 2.71897s/10 iters), loss = 7.31363
I0522 22:19:31.565776 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31363 (* 1 = 7.31363 loss)
I0522 22:19:32.307265 35003 sgd_solver.cpp:112] Iteration 79180, lr = 0.01
I0522 22:19:35.738739 35003 solver.cpp:239] Iteration 79190 (2.39649 iter/s, 4.17277s/10 iters), loss = 6.76018
I0522 22:19:35.738806 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76018 (* 1 = 6.76018 loss)
I0522 22:19:36.473887 35003 sgd_solver.cpp:112] Iteration 79190, lr = 0.01
I0522 22:19:38.149176 35003 solver.cpp:239] Iteration 79200 (4.14892 iter/s, 2.41026s/10 iters), loss = 5.82416
I0522 22:19:38.149224 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.82416 (* 1 = 5.82416 loss)
I0522 22:19:38.165781 35003 sgd_solver.cpp:112] Iteration 79200, lr = 0.01
I0522 22:19:40.878461 35003 solver.cpp:239] Iteration 79210 (3.66418 iter/s, 2.72912s/10 iters), loss = 7.78222
I0522 22:19:40.878502 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78222 (* 1 = 7.78222 loss)
I0522 22:19:40.902897 35003 sgd_solver.cpp:112] Iteration 79210, lr = 0.01
I0522 22:19:44.545951 35003 solver.cpp:239] Iteration 79220 (2.72681 iter/s, 3.66729s/10 iters), loss = 6.24484
I0522 22:19:44.546000 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24484 (* 1 = 6.24484 loss)
I0522 22:19:45.254328 35003 sgd_solver.cpp:112] Iteration 79220, lr = 0.01
I0522 22:19:48.245231 35003 solver.cpp:239] Iteration 79230 (2.70338 iter/s, 3.69908s/10 iters), loss = 7.56168
I0522 22:19:48.245287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56168 (* 1 = 7.56168 loss)
I0522 22:19:48.264119 35003 sgd_solver.cpp:112] Iteration 79230, lr = 0.01
I0522 22:19:51.738824 35003 solver.cpp:239] Iteration 79240 (2.86255 iter/s, 3.49339s/10 iters), loss = 8.19268
I0522 22:19:51.738876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.19268 (* 1 = 8.19268 loss)
I0522 22:19:51.756677 35003 sgd_solver.cpp:112] Iteration 79240, lr = 0.01
I0522 22:19:55.990530 35003 solver.cpp:239] Iteration 79250 (2.35214 iter/s, 4.25145s/10 iters), loss = 6.14033
I0522 22:19:55.990586 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14033 (* 1 = 6.14033 loss)
I0522 22:19:56.711764 35003 sgd_solver.cpp:112] Iteration 79250, lr = 0.01
I0522 22:19:59.547677 35003 solver.cpp:239] Iteration 79260 (2.8114 iter/s, 3.55694s/10 iters), loss = 6.74458
I0522 22:19:59.547936 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74458 (* 1 = 6.74458 loss)
I0522 22:19:59.557240 35003 sgd_solver.cpp:112] Iteration 79260, lr = 0.01
I0522 22:20:04.037358 35003 solver.cpp:239] Iteration 79270 (2.22755 iter/s, 4.48925s/10 iters), loss = 8.36931
I0522 22:20:04.037412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.36931 (* 1 = 8.36931 loss)
I0522 22:20:04.775048 35003 sgd_solver.cpp:112] Iteration 79270, lr = 0.01
I0522 22:20:09.290405 35003 solver.cpp:239] Iteration 79280 (1.90375 iter/s, 5.25278s/10 iters), loss = 7.13613
I0522 22:20:09.290452 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13613 (* 1 = 7.13613 loss)
I0522 22:20:09.294589 35003 sgd_solver.cpp:112] Iteration 79280, lr = 0.01
I0522 22:20:13.707993 35003 solver.cpp:239] Iteration 79290 (2.2638 iter/s, 4.41736s/10 iters), loss = 7.66557
I0522 22:20:13.708055 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66557 (* 1 = 7.66557 loss)
I0522 22:20:13.730535 35003 sgd_solver.cpp:112] Iteration 79290, lr = 0.01
I0522 22:20:15.820883 35003 solver.cpp:239] Iteration 79300 (4.73319 iter/s, 2.11274s/10 iters), loss = 7.29558
I0522 22:20:15.820922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29558 (* 1 = 7.29558 loss)
I0522 22:20:15.844406 35003 sgd_solver.cpp:112] Iteration 79300, lr = 0.01
I0522 22:20:20.160333 35003 solver.cpp:239] Iteration 79310 (2.30455 iter/s, 4.33923s/10 iters), loss = 8.82887
I0522 22:20:20.160369 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.82887 (* 1 = 8.82887 loss)
I0522 22:20:20.771534 35003 sgd_solver.cpp:112] Iteration 79310, lr = 0.01
I0522 22:20:27.264001 35003 solver.cpp:239] Iteration 79320 (1.40779 iter/s, 7.10333s/10 iters), loss = 7.56996
I0522 22:20:27.264050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56996 (* 1 = 7.56996 loss)
I0522 22:20:27.277652 35003 sgd_solver.cpp:112] Iteration 79320, lr = 0.01
I0522 22:20:31.631608 35003 solver.cpp:239] Iteration 79330 (2.2897 iter/s, 4.36738s/10 iters), loss = 7.63493
I0522 22:20:31.631800 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63493 (* 1 = 7.63493 loss)
I0522 22:20:31.644309 35003 sgd_solver.cpp:112] Iteration 79330, lr = 0.01
I0522 22:20:34.198216 35003 solver.cpp:239] Iteration 79340 (3.89665 iter/s, 2.56631s/10 iters), loss = 7.10203
I0522 22:20:34.198274 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10203 (* 1 = 7.10203 loss)
I0522 22:20:34.204468 35003 sgd_solver.cpp:112] Iteration 79340, lr = 0.01
I0522 22:20:37.019723 35003 solver.cpp:239] Iteration 79350 (3.54444 iter/s, 2.82132s/10 iters), loss = 6.42558
I0522 22:20:37.019778 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42558 (* 1 = 6.42558 loss)
I0522 22:20:37.048533 35003 sgd_solver.cpp:112] Iteration 79350, lr = 0.01
I0522 22:20:39.776927 35003 solver.cpp:239] Iteration 79360 (3.62709 iter/s, 2.75703s/10 iters), loss = 6.32177
I0522 22:20:39.776968 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32177 (* 1 = 6.32177 loss)
I0522 22:20:39.790944 35003 sgd_solver.cpp:112] Iteration 79360, lr = 0.01
I0522 22:20:43.339751 35003 solver.cpp:239] Iteration 79370 (2.80692 iter/s, 3.56263s/10 iters), loss = 7.5864
I0522 22:20:43.339799 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5864 (* 1 = 7.5864 loss)
I0522 22:20:43.347223 35003 sgd_solver.cpp:112] Iteration 79370, lr = 0.01
I0522 22:20:47.421394 35003 solver.cpp:239] Iteration 79380 (2.45012 iter/s, 4.08143s/10 iters), loss = 8.8666
I0522 22:20:47.421442 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.8666 (* 1 = 8.8666 loss)
I0522 22:20:47.434492 35003 sgd_solver.cpp:112] Iteration 79380, lr = 0.01
I0522 22:20:50.141793 35003 solver.cpp:239] Iteration 79390 (3.67616 iter/s, 2.72023s/10 iters), loss = 6.66817
I0522 22:20:50.141839 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66817 (* 1 = 6.66817 loss)
I0522 22:20:50.154871 35003 sgd_solver.cpp:112] Iteration 79390, lr = 0.01
I0522 22:20:53.763381 35003 solver.cpp:239] Iteration 79400 (2.76137 iter/s, 3.6214s/10 iters), loss = 7.16458
I0522 22:20:53.763417 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16458 (* 1 = 7.16458 loss)
I0522 22:20:53.776752 35003 sgd_solver.cpp:112] Iteration 79400, lr = 0.01
I0522 22:20:56.559916 35003 solver.cpp:239] Iteration 79410 (3.57606 iter/s, 2.79638s/10 iters), loss = 8.08604
I0522 22:20:56.559957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08604 (* 1 = 8.08604 loss)
I0522 22:20:56.572710 35003 sgd_solver.cpp:112] Iteration 79410, lr = 0.01
I0522 22:21:00.596151 35003 solver.cpp:239] Iteration 79420 (2.47768 iter/s, 4.03603s/10 iters), loss = 7.84206
I0522 22:21:00.596190 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84206 (* 1 = 7.84206 loss)
I0522 22:21:01.298821 35003 sgd_solver.cpp:112] Iteration 79420, lr = 0.01
I0522 22:21:03.868698 35003 solver.cpp:239] Iteration 79430 (3.0559 iter/s, 3.27236s/10 iters), loss = 7.48968
I0522 22:21:03.868897 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48968 (* 1 = 7.48968 loss)
I0522 22:21:03.875535 35003 sgd_solver.cpp:112] Iteration 79430, lr = 0.01
I0522 22:21:06.673388 35003 solver.cpp:239] Iteration 79440 (3.56586 iter/s, 2.80438s/10 iters), loss = 6.28292
I0522 22:21:06.673429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28292 (* 1 = 6.28292 loss)
I0522 22:21:06.686805 35003 sgd_solver.cpp:112] Iteration 79440, lr = 0.01
I0522 22:21:10.035172 35003 solver.cpp:239] Iteration 79450 (2.97478 iter/s, 3.3616s/10 iters), loss = 7.20233
I0522 22:21:10.035238 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20233 (* 1 = 7.20233 loss)
I0522 22:21:10.086004 35003 sgd_solver.cpp:112] Iteration 79450, lr = 0.01
I0522 22:21:12.977035 35003 solver.cpp:239] Iteration 79460 (3.39944 iter/s, 2.94166s/10 iters), loss = 7.66207
I0522 22:21:12.977092 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66207 (* 1 = 7.66207 loss)
I0522 22:21:13.717922 35003 sgd_solver.cpp:112] Iteration 79460, lr = 0.01
I0522 22:21:18.165488 35003 solver.cpp:239] Iteration 79470 (1.92746 iter/s, 5.18818s/10 iters), loss = 7.71635
I0522 22:21:18.165539 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71635 (* 1 = 7.71635 loss)
I0522 22:21:18.178634 35003 sgd_solver.cpp:112] Iteration 79470, lr = 0.01
I0522 22:21:20.324492 35003 solver.cpp:239] Iteration 79480 (4.63208 iter/s, 2.15886s/10 iters), loss = 7.76825
I0522 22:21:20.324540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76825 (* 1 = 7.76825 loss)
I0522 22:21:20.341063 35003 sgd_solver.cpp:112] Iteration 79480, lr = 0.01
I0522 22:21:22.383940 35003 solver.cpp:239] Iteration 79490 (4.85602 iter/s, 2.0593s/10 iters), loss = 6.54607
I0522 22:21:22.384001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54607 (* 1 = 6.54607 loss)
I0522 22:21:22.397729 35003 sgd_solver.cpp:112] Iteration 79490, lr = 0.01
I0522 22:21:23.729311 35003 solver.cpp:239] Iteration 79500 (7.43356 iter/s, 1.34525s/10 iters), loss = 6.38774
I0522 22:21:23.729354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38774 (* 1 = 6.38774 loss)
I0522 22:21:24.451319 35003 sgd_solver.cpp:112] Iteration 79500, lr = 0.01
I0522 22:21:26.608906 35003 solver.cpp:239] Iteration 79510 (3.47293 iter/s, 2.87942s/10 iters), loss = 7.94927
I0522 22:21:26.608968 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94927 (* 1 = 7.94927 loss)
I0522 22:21:27.325486 35003 sgd_solver.cpp:112] Iteration 79510, lr = 0.01
I0522 22:21:30.301671 35003 solver.cpp:239] Iteration 79520 (2.70816 iter/s, 3.69255s/10 iters), loss = 8.39308
I0522 22:21:30.301724 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.39308 (* 1 = 8.39308 loss)
I0522 22:21:30.314839 35003 sgd_solver.cpp:112] Iteration 79520, lr = 0.01
I0522 22:21:32.414129 35003 solver.cpp:239] Iteration 79530 (4.73416 iter/s, 2.11231s/10 iters), loss = 7.41201
I0522 22:21:32.414182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41201 (* 1 = 7.41201 loss)
I0522 22:21:33.123064 35003 sgd_solver.cpp:112] Iteration 79530, lr = 0.01
I0522 22:21:38.262961 35003 solver.cpp:239] Iteration 79540 (1.70983 iter/s, 5.84855s/10 iters), loss = 7.43144
I0522 22:21:38.263209 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43144 (* 1 = 7.43144 loss)
I0522 22:21:38.276139 35003 sgd_solver.cpp:112] Iteration 79540, lr = 0.01
I0522 22:21:42.950404 35003 solver.cpp:239] Iteration 79550 (2.13356 iter/s, 4.68701s/10 iters), loss = 6.85255
I0522 22:21:42.950459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85255 (* 1 = 6.85255 loss)
I0522 22:21:42.971662 35003 sgd_solver.cpp:112] Iteration 79550, lr = 0.01
I0522 22:21:47.438938 35003 solver.cpp:239] Iteration 79560 (2.22802 iter/s, 4.4883s/10 iters), loss = 7.75514
I0522 22:21:47.438976 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75514 (* 1 = 7.75514 loss)
I0522 22:21:48.161350 35003 sgd_solver.cpp:112] Iteration 79560, lr = 0.01
I0522 22:21:51.828661 35003 solver.cpp:239] Iteration 79570 (2.27816 iter/s, 4.3895s/10 iters), loss = 7.49717
I0522 22:21:51.828727 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49717 (* 1 = 7.49717 loss)
I0522 22:21:51.834959 35003 sgd_solver.cpp:112] Iteration 79570, lr = 0.01
I0522 22:21:55.276396 35003 solver.cpp:239] Iteration 79580 (2.90063 iter/s, 3.44753s/10 iters), loss = 7.29503
I0522 22:21:55.276456 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29503 (* 1 = 7.29503 loss)
I0522 22:21:55.984869 35003 sgd_solver.cpp:112] Iteration 79580, lr = 0.01
I0522 22:21:59.578656 35003 solver.cpp:239] Iteration 79590 (2.32449 iter/s, 4.30202s/10 iters), loss = 6.84744
I0522 22:21:59.578735 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84744 (* 1 = 6.84744 loss)
I0522 22:22:00.289647 35003 sgd_solver.cpp:112] Iteration 79590, lr = 0.01
I0522 22:22:03.175191 35003 solver.cpp:239] Iteration 79600 (2.78063 iter/s, 3.59631s/10 iters), loss = 7.40415
I0522 22:22:03.175248 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40415 (* 1 = 7.40415 loss)
I0522 22:22:03.853238 35003 sgd_solver.cpp:112] Iteration 79600, lr = 0.01
I0522 22:22:06.752359 35003 solver.cpp:239] Iteration 79610 (2.79567 iter/s, 3.57696s/10 iters), loss = 7.2735
I0522 22:22:06.752418 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2735 (* 1 = 7.2735 loss)
I0522 22:22:07.480088 35003 sgd_solver.cpp:112] Iteration 79610, lr = 0.01
I0522 22:22:11.741369 35003 solver.cpp:239] Iteration 79620 (2.00451 iter/s, 4.98874s/10 iters), loss = 7.3247
I0522 22:22:11.741611 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3247 (* 1 = 7.3247 loss)
I0522 22:22:11.747134 35003 sgd_solver.cpp:112] Iteration 79620, lr = 0.01
I0522 22:22:13.809973 35003 solver.cpp:239] Iteration 79630 (4.83496 iter/s, 2.06827s/10 iters), loss = 7.58195
I0522 22:22:13.810058 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58195 (* 1 = 7.58195 loss)
I0522 22:22:13.817595 35003 sgd_solver.cpp:112] Iteration 79630, lr = 0.01
I0522 22:22:16.855602 35003 solver.cpp:239] Iteration 79640 (3.28362 iter/s, 3.04542s/10 iters), loss = 7.00816
I0522 22:22:16.855639 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00816 (* 1 = 7.00816 loss)
I0522 22:22:17.596222 35003 sgd_solver.cpp:112] Iteration 79640, lr = 0.01
I0522 22:22:21.048851 35003 solver.cpp:239] Iteration 79650 (2.3849 iter/s, 4.19304s/10 iters), loss = 7.18457
I0522 22:22:21.048892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18457 (* 1 = 7.18457 loss)
I0522 22:22:21.058077 35003 sgd_solver.cpp:112] Iteration 79650, lr = 0.01
I0522 22:22:23.737109 35003 solver.cpp:239] Iteration 79660 (3.7201 iter/s, 2.6881s/10 iters), loss = 7.11925
I0522 22:22:23.737159 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11925 (* 1 = 7.11925 loss)
I0522 22:22:23.748237 35003 sgd_solver.cpp:112] Iteration 79660, lr = 0.01
I0522 22:22:27.565526 35003 solver.cpp:239] Iteration 79670 (2.61219 iter/s, 3.8282s/10 iters), loss = 6.63926
I0522 22:22:27.565585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63926 (* 1 = 6.63926 loss)
I0522 22:22:27.578352 35003 sgd_solver.cpp:112] Iteration 79670, lr = 0.01
I0522 22:22:32.914445 35003 solver.cpp:239] Iteration 79680 (1.86963 iter/s, 5.34865s/10 iters), loss = 6.75079
I0522 22:22:32.914495 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75079 (* 1 = 6.75079 loss)
I0522 22:22:32.938925 35003 sgd_solver.cpp:112] Iteration 79680, lr = 0.01
I0522 22:22:37.196983 35003 solver.cpp:239] Iteration 79690 (2.33519 iter/s, 4.28231s/10 iters), loss = 6.97354
I0522 22:22:37.197038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97354 (* 1 = 6.97354 loss)
I0522 22:22:37.201453 35003 sgd_solver.cpp:112] Iteration 79690, lr = 0.01
I0522 22:22:40.038812 35003 solver.cpp:239] Iteration 79700 (3.51907 iter/s, 2.84166s/10 iters), loss = 7.90593
I0522 22:22:40.038857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90593 (* 1 = 7.90593 loss)
I0522 22:22:40.058141 35003 sgd_solver.cpp:112] Iteration 79700, lr = 0.01
I0522 22:22:44.496263 35003 solver.cpp:239] Iteration 79710 (2.24355 iter/s, 4.45722s/10 iters), loss = 8.37826
I0522 22:22:44.496517 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.37826 (* 1 = 8.37826 loss)
I0522 22:22:44.508199 35003 sgd_solver.cpp:112] Iteration 79710, lr = 0.01
I0522 22:22:46.924468 35003 solver.cpp:239] Iteration 79720 (4.11884 iter/s, 2.42787s/10 iters), loss = 7.63766
I0522 22:22:46.924509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63766 (* 1 = 7.63766 loss)
I0522 22:22:46.927295 35003 sgd_solver.cpp:112] Iteration 79720, lr = 0.01
I0522 22:22:51.597581 35003 solver.cpp:239] Iteration 79730 (2.14001 iter/s, 4.67287s/10 iters), loss = 8.66718
I0522 22:22:51.597620 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.66718 (* 1 = 8.66718 loss)
I0522 22:22:51.610695 35003 sgd_solver.cpp:112] Iteration 79730, lr = 0.01
I0522 22:22:55.137181 35003 solver.cpp:239] Iteration 79740 (2.82533 iter/s, 3.53941s/10 iters), loss = 6.94639
I0522 22:22:55.137224 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94639 (* 1 = 6.94639 loss)
I0522 22:22:55.179368 35003 sgd_solver.cpp:112] Iteration 79740, lr = 0.01
I0522 22:22:58.804366 35003 solver.cpp:239] Iteration 79750 (2.72703 iter/s, 3.66699s/10 iters), loss = 8.24085
I0522 22:22:58.804405 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.24085 (* 1 = 8.24085 loss)
I0522 22:22:58.818251 35003 sgd_solver.cpp:112] Iteration 79750, lr = 0.01
I0522 22:23:03.856923 35003 solver.cpp:239] Iteration 79760 (1.9793 iter/s, 5.0523s/10 iters), loss = 7.23355
I0522 22:23:03.856976 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23355 (* 1 = 7.23355 loss)
I0522 22:23:04.565124 35003 sgd_solver.cpp:112] Iteration 79760, lr = 0.01
I0522 22:23:06.732020 35003 solver.cpp:239] Iteration 79770 (3.47836 iter/s, 2.87492s/10 iters), loss = 7.03199
I0522 22:23:06.732064 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03199 (* 1 = 7.03199 loss)
I0522 22:23:06.736599 35003 sgd_solver.cpp:112] Iteration 79770, lr = 0.01
I0522 22:23:08.648638 35003 solver.cpp:239] Iteration 79780 (5.21787 iter/s, 1.91649s/10 iters), loss = 7.3466
I0522 22:23:08.648679 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3466 (* 1 = 7.3466 loss)
I0522 22:23:08.661464 35003 sgd_solver.cpp:112] Iteration 79780, lr = 0.01
I0522 22:23:10.221505 35003 solver.cpp:239] Iteration 79790 (6.35826 iter/s, 1.57276s/10 iters), loss = 7.66687
I0522 22:23:10.221554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66687 (* 1 = 7.66687 loss)
I0522 22:23:10.766398 35003 sgd_solver.cpp:112] Iteration 79790, lr = 0.01
I0522 22:23:16.443675 35003 solver.cpp:239] Iteration 79800 (1.60724 iter/s, 6.22186s/10 iters), loss = 6.89005
I0522 22:23:16.443842 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89005 (* 1 = 6.89005 loss)
I0522 22:23:17.147656 35003 sgd_solver.cpp:112] Iteration 79800, lr = 0.01
I0522 22:23:19.253474 35003 solver.cpp:239] Iteration 79810 (3.55935 iter/s, 2.8095s/10 iters), loss = 7.70296
I0522 22:23:19.253535 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70296 (* 1 = 7.70296 loss)
I0522 22:23:19.262521 35003 sgd_solver.cpp:112] Iteration 79810, lr = 0.01
I0522 22:23:21.377959 35003 solver.cpp:239] Iteration 79820 (4.70737 iter/s, 2.12433s/10 iters), loss = 6.90285
I0522 22:23:21.378021 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90285 (* 1 = 6.90285 loss)
I0522 22:23:21.385839 35003 sgd_solver.cpp:112] Iteration 79820, lr = 0.01
I0522 22:23:23.502918 35003 solver.cpp:239] Iteration 79830 (4.70633 iter/s, 2.1248s/10 iters), loss = 6.938
I0522 22:23:23.502962 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.938 (* 1 = 6.938 loss)
I0522 22:23:23.506255 35003 sgd_solver.cpp:112] Iteration 79830, lr = 0.01
I0522 22:23:27.143362 35003 solver.cpp:239] Iteration 79840 (2.74707 iter/s, 3.64024s/10 iters), loss = 6.93447
I0522 22:23:27.143404 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93447 (* 1 = 6.93447 loss)
I0522 22:23:27.151739 35003 sgd_solver.cpp:112] Iteration 79840, lr = 0.01
I0522 22:23:29.452172 35003 solver.cpp:239] Iteration 79850 (4.3315 iter/s, 2.30867s/10 iters), loss = 7.15165
I0522 22:23:29.452214 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15165 (* 1 = 7.15165 loss)
I0522 22:23:29.463646 35003 sgd_solver.cpp:112] Iteration 79850, lr = 0.01
I0522 22:23:34.256537 35003 solver.cpp:239] Iteration 79860 (2.08154 iter/s, 4.80413s/10 iters), loss = 8.29162
I0522 22:23:34.256572 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.29162 (* 1 = 8.29162 loss)
I0522 22:23:34.284497 35003 sgd_solver.cpp:112] Iteration 79860, lr = 0.01
I0522 22:23:37.843897 35003 solver.cpp:239] Iteration 79870 (2.78772 iter/s, 3.58716s/10 iters), loss = 6.7809
I0522 22:23:37.843960 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7809 (* 1 = 6.7809 loss)
I0522 22:23:37.956378 35003 sgd_solver.cpp:112] Iteration 79870, lr = 0.01
I0522 22:23:42.260119 35003 solver.cpp:239] Iteration 79880 (2.2645 iter/s, 4.41598s/10 iters), loss = 7.01342
I0522 22:23:42.260155 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01342 (* 1 = 7.01342 loss)
I0522 22:23:42.287737 35003 sgd_solver.cpp:112] Iteration 79880, lr = 0.01
I0522 22:23:46.280637 35003 solver.cpp:239] Iteration 79890 (2.48737 iter/s, 4.02031s/10 iters), loss = 7.31685
I0522 22:23:46.280681 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31685 (* 1 = 7.31685 loss)
I0522 22:23:46.291818 35003 sgd_solver.cpp:112] Iteration 79890, lr = 0.01
I0522 22:23:49.391930 35003 solver.cpp:239] Iteration 79900 (3.21428 iter/s, 3.11112s/10 iters), loss = 7.09346
I0522 22:23:49.392210 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09346 (* 1 = 7.09346 loss)
I0522 22:23:49.399333 35003 sgd_solver.cpp:112] Iteration 79900, lr = 0.01
I0522 22:23:53.874590 35003 solver.cpp:239] Iteration 79910 (2.23105 iter/s, 4.48219s/10 iters), loss = 7.29717
I0522 22:23:53.874655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29717 (* 1 = 7.29717 loss)
I0522 22:23:53.887130 35003 sgd_solver.cpp:112] Iteration 79910, lr = 0.01
I0522 22:23:56.305303 35003 solver.cpp:239] Iteration 79920 (4.11431 iter/s, 2.43054s/10 iters), loss = 6.89138
I0522 22:23:56.305351 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89138 (* 1 = 6.89138 loss)
I0522 22:23:56.323568 35003 sgd_solver.cpp:112] Iteration 79920, lr = 0.01
I0522 22:23:58.408469 35003 solver.cpp:239] Iteration 79930 (4.75508 iter/s, 2.10302s/10 iters), loss = 6.97198
I0522 22:23:58.408511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97198 (* 1 = 6.97198 loss)
I0522 22:23:59.110301 35003 sgd_solver.cpp:112] Iteration 79930, lr = 0.01
I0522 22:24:01.269609 35003 solver.cpp:239] Iteration 79940 (3.49531 iter/s, 2.86097s/10 iters), loss = 7.60002
I0522 22:24:01.269655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60002 (* 1 = 7.60002 loss)
I0522 22:24:02.010452 35003 sgd_solver.cpp:112] Iteration 79940, lr = 0.01
I0522 22:24:06.151041 35003 solver.cpp:239] Iteration 79950 (2.04868 iter/s, 4.88119s/10 iters), loss = 7.89916
I0522 22:24:06.151082 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89916 (* 1 = 7.89916 loss)
I0522 22:24:06.164551 35003 sgd_solver.cpp:112] Iteration 79950, lr = 0.01
I0522 22:24:11.160357 35003 solver.cpp:239] Iteration 79960 (1.99638 iter/s, 5.00907s/10 iters), loss = 7.38548
I0522 22:24:11.160403 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38548 (* 1 = 7.38548 loss)
I0522 22:24:11.167337 35003 sgd_solver.cpp:112] Iteration 79960, lr = 0.01
I0522 22:24:15.507536 35003 solver.cpp:239] Iteration 79970 (2.30047 iter/s, 4.34694s/10 iters), loss = 7.59325
I0522 22:24:15.507611 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59325 (* 1 = 7.59325 loss)
I0522 22:24:16.221017 35003 sgd_solver.cpp:112] Iteration 79970, lr = 0.01
I0522 22:24:18.509733 35003 solver.cpp:239] Iteration 79980 (3.33599 iter/s, 2.99761s/10 iters), loss = 6.84594
I0522 22:24:18.509786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84594 (* 1 = 6.84594 loss)
I0522 22:24:19.219893 35003 sgd_solver.cpp:112] Iteration 79980, lr = 0.01
I0522 22:24:21.976897 35003 solver.cpp:239] Iteration 79990 (2.88438 iter/s, 3.46695s/10 iters), loss = 8.18794
I0522 22:24:21.977046 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.18794 (* 1 = 8.18794 loss)
I0522 22:24:22.711932 35003 sgd_solver.cpp:112] Iteration 79990, lr = 0.01
I0522 22:24:25.498658 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_80000.caffemodel
I0522 22:24:25.807147 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_80000.solverstate
I0522 22:24:25.968036 35003 solver.cpp:239] Iteration 80000 (2.50575 iter/s, 3.99082s/10 iters), loss = 7.87011
I0522 22:24:25.968084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87011 (* 1 = 7.87011 loss)
I0522 22:24:25.980937 35003 sgd_solver.cpp:112] Iteration 80000, lr = 0.01
I0522 22:24:29.553856 35003 solver.cpp:239] Iteration 80010 (2.78892 iter/s, 3.58562s/10 iters), loss = 8.58499
I0522 22:24:29.553902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.58499 (* 1 = 8.58499 loss)
I0522 22:24:29.581393 35003 sgd_solver.cpp:112] Iteration 80010, lr = 0.01
I0522 22:24:33.380779 35003 solver.cpp:239] Iteration 80020 (2.61321 iter/s, 3.82672s/10 iters), loss = 7.37475
I0522 22:24:33.380831 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37475 (* 1 = 7.37475 loss)
I0522 22:24:33.387231 35003 sgd_solver.cpp:112] Iteration 80020, lr = 0.01
I0522 22:24:36.282032 35003 solver.cpp:239] Iteration 80030 (3.44699 iter/s, 2.90108s/10 iters), loss = 7.11071
I0522 22:24:36.282071 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11071 (* 1 = 7.11071 loss)
I0522 22:24:36.306766 35003 sgd_solver.cpp:112] Iteration 80030, lr = 0.01
I0522 22:24:40.712049 35003 solver.cpp:239] Iteration 80040 (2.25745 iter/s, 4.42979s/10 iters), loss = 8.62638
I0522 22:24:40.712110 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.62638 (* 1 = 8.62638 loss)
I0522 22:24:41.424754 35003 sgd_solver.cpp:112] Iteration 80040, lr = 0.01
I0522 22:24:43.489612 35003 solver.cpp:239] Iteration 80050 (3.60051 iter/s, 2.77738s/10 iters), loss = 7.7479
I0522 22:24:43.489652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7479 (* 1 = 7.7479 loss)
I0522 22:24:43.500912 35003 sgd_solver.cpp:112] Iteration 80050, lr = 0.01
I0522 22:24:47.175066 35003 solver.cpp:239] Iteration 80060 (2.71351 iter/s, 3.68526s/10 iters), loss = 8.93704
I0522 22:24:47.175107 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.93704 (* 1 = 8.93704 loss)
I0522 22:24:47.191133 35003 sgd_solver.cpp:112] Iteration 80060, lr = 0.01
I0522 22:24:49.407744 35003 solver.cpp:239] Iteration 80070 (4.47921 iter/s, 2.23254s/10 iters), loss = 7.50809
I0522 22:24:49.407796 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50809 (* 1 = 7.50809 loss)
I0522 22:24:49.525677 35003 sgd_solver.cpp:112] Iteration 80070, lr = 0.01
I0522 22:24:52.822939 35003 solver.cpp:239] Iteration 80080 (2.92826 iter/s, 3.415s/10 iters), loss = 6.93603
I0522 22:24:52.823221 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93603 (* 1 = 6.93603 loss)
I0522 22:24:52.828338 35003 sgd_solver.cpp:112] Iteration 80080, lr = 0.01
I0522 22:24:57.127588 35003 solver.cpp:239] Iteration 80090 (2.32331 iter/s, 4.30421s/10 iters), loss = 6.48719
I0522 22:24:57.127668 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48719 (* 1 = 6.48719 loss)
I0522 22:24:57.146389 35003 sgd_solver.cpp:112] Iteration 80090, lr = 0.01
I0522 22:25:00.781317 35003 solver.cpp:239] Iteration 80100 (2.7371 iter/s, 3.65351s/10 iters), loss = 7.59548
I0522 22:25:00.781381 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59548 (* 1 = 7.59548 loss)
I0522 22:25:00.926400 35003 sgd_solver.cpp:112] Iteration 80100, lr = 0.01
I0522 22:25:04.531896 35003 solver.cpp:239] Iteration 80110 (2.66641 iter/s, 3.75036s/10 iters), loss = 6.2143
I0522 22:25:04.531949 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2143 (* 1 = 6.2143 loss)
I0522 22:25:05.271055 35003 sgd_solver.cpp:112] Iteration 80110, lr = 0.01
I0522 22:25:10.114542 35003 solver.cpp:239] Iteration 80120 (1.79136 iter/s, 5.58236s/10 iters), loss = 7.04389
I0522 22:25:10.114598 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04389 (* 1 = 7.04389 loss)
I0522 22:25:10.120628 35003 sgd_solver.cpp:112] Iteration 80120, lr = 0.01
I0522 22:25:12.462339 35003 solver.cpp:239] Iteration 80130 (4.25959 iter/s, 2.34764s/10 iters), loss = 7.0084
I0522 22:25:12.462378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0084 (* 1 = 7.0084 loss)
I0522 22:25:12.475785 35003 sgd_solver.cpp:112] Iteration 80130, lr = 0.01
I0522 22:25:16.097818 35003 solver.cpp:239] Iteration 80140 (2.75085 iter/s, 3.63524s/10 iters), loss = 6.73466
I0522 22:25:16.097877 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73466 (* 1 = 6.73466 loss)
I0522 22:25:16.812830 35003 sgd_solver.cpp:112] Iteration 80140, lr = 0.01
I0522 22:25:19.017792 35003 solver.cpp:239] Iteration 80150 (3.4249 iter/s, 2.91979s/10 iters), loss = 7.80465
I0522 22:25:19.017841 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80465 (* 1 = 7.80465 loss)
I0522 22:25:19.693567 35003 sgd_solver.cpp:112] Iteration 80150, lr = 0.01
I0522 22:25:23.172632 35003 solver.cpp:239] Iteration 80160 (2.40696 iter/s, 4.15462s/10 iters), loss = 7.60592
I0522 22:25:23.172766 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60592 (* 1 = 7.60592 loss)
I0522 22:25:23.185672 35003 sgd_solver.cpp:112] Iteration 80160, lr = 0.01
I0522 22:25:25.570497 35003 solver.cpp:239] Iteration 80170 (4.1708 iter/s, 2.39762s/10 iters), loss = 7.70637
I0522 22:25:25.570544 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70637 (* 1 = 7.70637 loss)
I0522 22:25:25.598896 35003 sgd_solver.cpp:112] Iteration 80170, lr = 0.01
I0522 22:25:28.922660 35003 solver.cpp:239] Iteration 80180 (2.98331 iter/s, 3.35198s/10 iters), loss = 7.16857
I0522 22:25:28.922711 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16857 (* 1 = 7.16857 loss)
I0522 22:25:28.935868 35003 sgd_solver.cpp:112] Iteration 80180, lr = 0.01
I0522 22:25:32.004385 35003 solver.cpp:239] Iteration 80190 (3.24511 iter/s, 3.08156s/10 iters), loss = 7.22435
I0522 22:25:32.004426 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22435 (* 1 = 7.22435 loss)
I0522 22:25:32.017551 35003 sgd_solver.cpp:112] Iteration 80190, lr = 0.01
I0522 22:25:37.104799 35003 solver.cpp:239] Iteration 80200 (1.96072 iter/s, 5.10016s/10 iters), loss = 8.7272
I0522 22:25:37.104842 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.7272 (* 1 = 8.7272 loss)
I0522 22:25:37.115764 35003 sgd_solver.cpp:112] Iteration 80200, lr = 0.01
I0522 22:25:39.220863 35003 solver.cpp:239] Iteration 80210 (4.72606 iter/s, 2.11593s/10 iters), loss = 7.54061
I0522 22:25:39.220912 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54061 (* 1 = 7.54061 loss)
I0522 22:25:39.882526 35003 sgd_solver.cpp:112] Iteration 80210, lr = 0.01
I0522 22:25:43.386469 35003 solver.cpp:239] Iteration 80220 (2.40074 iter/s, 4.16539s/10 iters), loss = 7.2455
I0522 22:25:43.386517 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2455 (* 1 = 7.2455 loss)
I0522 22:25:43.405141 35003 sgd_solver.cpp:112] Iteration 80220, lr = 0.01
I0522 22:25:47.669304 35003 solver.cpp:239] Iteration 80230 (2.33502 iter/s, 4.28261s/10 iters), loss = 6.8479
I0522 22:25:47.669348 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8479 (* 1 = 6.8479 loss)
I0522 22:25:47.674804 35003 sgd_solver.cpp:112] Iteration 80230, lr = 0.01
I0522 22:25:51.354212 35003 solver.cpp:239] Iteration 80240 (2.71392 iter/s, 3.68471s/10 iters), loss = 7.71472
I0522 22:25:51.354269 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71472 (* 1 = 7.71472 loss)
I0522 22:25:51.363570 35003 sgd_solver.cpp:112] Iteration 80240, lr = 0.01
I0522 22:25:54.616693 35003 solver.cpp:239] Iteration 80250 (3.06533 iter/s, 3.26229s/10 iters), loss = 6.8508
I0522 22:25:54.616829 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8508 (* 1 = 6.8508 loss)
I0522 22:25:54.629878 35003 sgd_solver.cpp:112] Iteration 80250, lr = 0.01
I0522 22:25:58.061432 35003 solver.cpp:239] Iteration 80260 (2.90321 iter/s, 3.44447s/10 iters), loss = 7.59026
I0522 22:25:58.061482 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59026 (* 1 = 7.59026 loss)
I0522 22:25:58.081912 35003 sgd_solver.cpp:112] Iteration 80260, lr = 0.01
I0522 22:26:01.514602 35003 solver.cpp:239] Iteration 80270 (2.89606 iter/s, 3.45297s/10 iters), loss = 8.3796
I0522 22:26:01.514649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.3796 (* 1 = 8.3796 loss)
I0522 22:26:01.521958 35003 sgd_solver.cpp:112] Iteration 80270, lr = 0.01
I0522 22:26:05.023535 35003 solver.cpp:239] Iteration 80280 (2.85003 iter/s, 3.50873s/10 iters), loss = 7.68809
I0522 22:26:05.023583 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68809 (* 1 = 7.68809 loss)
I0522 22:26:05.037168 35003 sgd_solver.cpp:112] Iteration 80280, lr = 0.01
I0522 22:26:09.420581 35003 solver.cpp:239] Iteration 80290 (2.27437 iter/s, 4.39681s/10 iters), loss = 7.3874
I0522 22:26:09.420624 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3874 (* 1 = 7.3874 loss)
I0522 22:26:10.129432 35003 sgd_solver.cpp:112] Iteration 80290, lr = 0.01
I0522 22:26:12.913918 35003 solver.cpp:239] Iteration 80300 (2.86275 iter/s, 3.49314s/10 iters), loss = 7.99745
I0522 22:26:12.913965 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99745 (* 1 = 7.99745 loss)
I0522 22:26:12.919612 35003 sgd_solver.cpp:112] Iteration 80300, lr = 0.01
I0522 22:26:16.858809 35003 solver.cpp:239] Iteration 80310 (2.53506 iter/s, 3.94468s/10 iters), loss = 8.15022
I0522 22:26:16.858868 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.15022 (* 1 = 8.15022 loss)
I0522 22:26:16.872891 35003 sgd_solver.cpp:112] Iteration 80310, lr = 0.01
I0522 22:26:20.424479 35003 solver.cpp:239] Iteration 80320 (2.80469 iter/s, 3.56545s/10 iters), loss = 7.20321
I0522 22:26:20.424531 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20321 (* 1 = 7.20321 loss)
I0522 22:26:20.938534 35003 sgd_solver.cpp:112] Iteration 80320, lr = 0.01
I0522 22:26:23.337966 35003 solver.cpp:239] Iteration 80330 (3.43252 iter/s, 2.91331s/10 iters), loss = 7.56564
I0522 22:26:23.338014 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56564 (* 1 = 7.56564 loss)
I0522 22:26:23.343677 35003 sgd_solver.cpp:112] Iteration 80330, lr = 0.01
I0522 22:26:26.191377 35003 solver.cpp:239] Iteration 80340 (3.50478 iter/s, 2.85324s/10 iters), loss = 7.94544
I0522 22:26:26.191660 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94544 (* 1 = 7.94544 loss)
I0522 22:26:26.199297 35003 sgd_solver.cpp:112] Iteration 80340, lr = 0.01
I0522 22:26:30.035188 35003 solver.cpp:239] Iteration 80350 (2.60186 iter/s, 3.84341s/10 iters), loss = 6.46022
I0522 22:26:30.035230 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46022 (* 1 = 6.46022 loss)
I0522 22:26:30.142129 35003 sgd_solver.cpp:112] Iteration 80350, lr = 0.01
I0522 22:26:33.760794 35003 solver.cpp:239] Iteration 80360 (2.68427 iter/s, 3.72541s/10 iters), loss = 7.09465
I0522 22:26:33.760843 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09465 (* 1 = 7.09465 loss)
I0522 22:26:33.770200 35003 sgd_solver.cpp:112] Iteration 80360, lr = 0.01
I0522 22:26:37.189113 35003 solver.cpp:239] Iteration 80370 (2.91704 iter/s, 3.42813s/10 iters), loss = 7.74728
I0522 22:26:37.189147 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74728 (* 1 = 7.74728 loss)
I0522 22:26:37.201884 35003 sgd_solver.cpp:112] Iteration 80370, lr = 0.01
I0522 22:26:40.109028 35003 solver.cpp:239] Iteration 80380 (3.42494 iter/s, 2.91976s/10 iters), loss = 7.90677
I0522 22:26:40.109076 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90677 (* 1 = 7.90677 loss)
I0522 22:26:40.467147 35003 sgd_solver.cpp:112] Iteration 80380, lr = 0.01
I0522 22:26:42.209583 35003 solver.cpp:239] Iteration 80390 (4.76096 iter/s, 2.10042s/10 iters), loss = 7.02801
I0522 22:26:42.209620 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02801 (* 1 = 7.02801 loss)
I0522 22:26:42.773561 35003 sgd_solver.cpp:112] Iteration 80390, lr = 0.01
I0522 22:26:45.113374 35003 solver.cpp:239] Iteration 80400 (3.44396 iter/s, 2.90363s/10 iters), loss = 7.45564
I0522 22:26:45.113417 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45564 (* 1 = 7.45564 loss)
I0522 22:26:45.118283 35003 sgd_solver.cpp:112] Iteration 80400, lr = 0.01
I0522 22:26:48.638550 35003 solver.cpp:239] Iteration 80410 (2.83689 iter/s, 3.52499s/10 iters), loss = 8.46096
I0522 22:26:48.638605 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.46096 (* 1 = 8.46096 loss)
I0522 22:26:48.646747 35003 sgd_solver.cpp:112] Iteration 80410, lr = 0.01
I0522 22:26:52.976296 35003 solver.cpp:239] Iteration 80420 (2.30547 iter/s, 4.3375s/10 iters), loss = 6.59426
I0522 22:26:52.976346 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59426 (* 1 = 6.59426 loss)
I0522 22:26:52.978487 35003 sgd_solver.cpp:112] Iteration 80420, lr = 0.01
I0522 22:26:55.975574 35003 solver.cpp:239] Iteration 80430 (3.33434 iter/s, 2.9991s/10 iters), loss = 7.95427
I0522 22:26:55.975620 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95427 (* 1 = 7.95427 loss)
I0522 22:26:55.977043 35003 sgd_solver.cpp:112] Iteration 80430, lr = 0.01
I0522 22:26:56.931821 35003 solver.cpp:239] Iteration 80440 (10.4588 iter/s, 0.956134s/10 iters), loss = 6.25841
I0522 22:26:56.931998 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25841 (* 1 = 6.25841 loss)
I0522 22:26:56.936545 35003 sgd_solver.cpp:112] Iteration 80440, lr = 0.01
I0522 22:26:58.890777 35003 solver.cpp:239] Iteration 80450 (5.10541 iter/s, 1.95871s/10 iters), loss = 7.77697
I0522 22:26:58.890825 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77697 (* 1 = 7.77697 loss)
I0522 22:26:58.904070 35003 sgd_solver.cpp:112] Iteration 80450, lr = 0.01
I0522 22:27:02.434767 35003 solver.cpp:239] Iteration 80460 (2.82184 iter/s, 3.54378s/10 iters), loss = 7.29949
I0522 22:27:02.434828 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29949 (* 1 = 7.29949 loss)
I0522 22:27:02.446599 35003 sgd_solver.cpp:112] Iteration 80460, lr = 0.01
I0522 22:27:05.554080 35003 solver.cpp:239] Iteration 80470 (3.20604 iter/s, 3.11911s/10 iters), loss = 7.54628
I0522 22:27:05.554128 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54628 (* 1 = 7.54628 loss)
I0522 22:27:05.567109 35003 sgd_solver.cpp:112] Iteration 80470, lr = 0.01
I0522 22:27:10.054497 35003 solver.cpp:239] Iteration 80480 (2.22213 iter/s, 4.50018s/10 iters), loss = 7.90025
I0522 22:27:10.054541 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90025 (* 1 = 7.90025 loss)
I0522 22:27:10.062214 35003 sgd_solver.cpp:112] Iteration 80480, lr = 0.01
I0522 22:27:13.678995 35003 solver.cpp:239] Iteration 80490 (2.75915 iter/s, 3.6243s/10 iters), loss = 7.41385
I0522 22:27:13.679036 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41385 (* 1 = 7.41385 loss)
I0522 22:27:13.682173 35003 sgd_solver.cpp:112] Iteration 80490, lr = 0.01
I0522 22:27:17.165168 35003 solver.cpp:239] Iteration 80500 (2.86863 iter/s, 3.48598s/10 iters), loss = 8.03462
I0522 22:27:17.165212 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03462 (* 1 = 8.03462 loss)
I0522 22:27:17.170753 35003 sgd_solver.cpp:112] Iteration 80500, lr = 0.01
I0522 22:27:18.673010 35003 solver.cpp:239] Iteration 80510 (6.63254 iter/s, 1.50772s/10 iters), loss = 7.3989
I0522 22:27:18.673065 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3989 (* 1 = 7.3989 loss)
I0522 22:27:18.683004 35003 sgd_solver.cpp:112] Iteration 80510, lr = 0.01
I0522 22:27:21.753001 35003 solver.cpp:239] Iteration 80520 (3.24696 iter/s, 3.0798s/10 iters), loss = 6.63201
I0522 22:27:21.753072 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63201 (* 1 = 6.63201 loss)
I0522 22:27:22.188515 35003 sgd_solver.cpp:112] Iteration 80520, lr = 0.01
I0522 22:27:26.382071 35003 solver.cpp:239] Iteration 80530 (2.16038 iter/s, 4.62881s/10 iters), loss = 6.40631
I0522 22:27:26.382129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40631 (* 1 = 6.40631 loss)
I0522 22:27:27.091116 35003 sgd_solver.cpp:112] Iteration 80530, lr = 0.01
I0522 22:27:30.633210 35003 solver.cpp:239] Iteration 80540 (2.35244 iter/s, 4.25091s/10 iters), loss = 7.9047
I0522 22:27:30.633256 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9047 (* 1 = 7.9047 loss)
I0522 22:27:30.646301 35003 sgd_solver.cpp:112] Iteration 80540, lr = 0.01
I0522 22:27:35.029716 35003 solver.cpp:239] Iteration 80550 (2.27465 iter/s, 4.39628s/10 iters), loss = 7.43252
I0522 22:27:35.029767 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43252 (* 1 = 7.43252 loss)
I0522 22:27:35.739938 35003 sgd_solver.cpp:112] Iteration 80550, lr = 0.01
I0522 22:27:38.753993 35003 solver.cpp:239] Iteration 80560 (2.68524 iter/s, 3.72406s/10 iters), loss = 8.40282
I0522 22:27:38.754077 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.40282 (* 1 = 8.40282 loss)
I0522 22:27:38.772161 35003 sgd_solver.cpp:112] Iteration 80560, lr = 0.01
I0522 22:27:43.047641 35003 solver.cpp:239] Iteration 80570 (2.32916 iter/s, 4.29339s/10 iters), loss = 8.06451
I0522 22:27:43.047690 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.06451 (* 1 = 8.06451 loss)
I0522 22:27:43.061069 35003 sgd_solver.cpp:112] Iteration 80570, lr = 0.01
I0522 22:27:45.442791 35003 solver.cpp:239] Iteration 80580 (4.17537 iter/s, 2.395s/10 iters), loss = 6.44515
I0522 22:27:45.442857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44515 (* 1 = 6.44515 loss)
I0522 22:27:45.454977 35003 sgd_solver.cpp:112] Iteration 80580, lr = 0.01
I0522 22:27:50.416249 35003 solver.cpp:239] Iteration 80590 (2.01078 iter/s, 4.97319s/10 iters), loss = 7.11252
I0522 22:27:50.416301 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11252 (* 1 = 7.11252 loss)
I0522 22:27:50.877182 35003 sgd_solver.cpp:112] Iteration 80590, lr = 0.01
I0522 22:27:54.768532 35003 solver.cpp:239] Iteration 80600 (2.29776 iter/s, 4.35206s/10 iters), loss = 7.74817
I0522 22:27:54.768577 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74817 (* 1 = 7.74817 loss)
I0522 22:27:55.509091 35003 sgd_solver.cpp:112] Iteration 80600, lr = 0.01
I0522 22:27:57.609117 35003 solver.cpp:239] Iteration 80610 (3.52061 iter/s, 2.84042s/10 iters), loss = 7.48727
I0522 22:27:57.609325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48727 (* 1 = 7.48727 loss)
I0522 22:27:58.187275 35003 sgd_solver.cpp:112] Iteration 80610, lr = 0.01
I0522 22:28:01.984499 35003 solver.cpp:239] Iteration 80620 (2.28572 iter/s, 4.375s/10 iters), loss = 7.3094
I0522 22:28:01.984544 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3094 (* 1 = 7.3094 loss)
I0522 22:28:02.722007 35003 sgd_solver.cpp:112] Iteration 80620, lr = 0.01
I0522 22:28:06.448982 35003 solver.cpp:239] Iteration 80630 (2.24002 iter/s, 4.46425s/10 iters), loss = 8.38844
I0522 22:28:06.449021 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.38844 (* 1 = 8.38844 loss)
I0522 22:28:06.663384 35003 sgd_solver.cpp:112] Iteration 80630, lr = 0.01
I0522 22:28:10.647790 35003 solver.cpp:239] Iteration 80640 (2.38175 iter/s, 4.19859s/10 iters), loss = 8.23117
I0522 22:28:10.647837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.23117 (* 1 = 8.23117 loss)
I0522 22:28:10.653100 35003 sgd_solver.cpp:112] Iteration 80640, lr = 0.01
I0522 22:28:15.151640 35003 solver.cpp:239] Iteration 80650 (2.22045 iter/s, 4.5036s/10 iters), loss = 8.02878
I0522 22:28:15.151700 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02878 (* 1 = 8.02878 loss)
I0522 22:28:15.154575 35003 sgd_solver.cpp:112] Iteration 80650, lr = 0.01
I0522 22:28:17.081074 35003 solver.cpp:239] Iteration 80660 (5.18332 iter/s, 1.92927s/10 iters), loss = 7.25082
I0522 22:28:17.081115 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25082 (* 1 = 7.25082 loss)
I0522 22:28:17.777035 35003 sgd_solver.cpp:112] Iteration 80660, lr = 0.01
I0522 22:28:22.014211 35003 solver.cpp:239] Iteration 80670 (2.02721 iter/s, 4.93289s/10 iters), loss = 7.80194
I0522 22:28:22.014281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80194 (* 1 = 7.80194 loss)
I0522 22:28:22.729460 35003 sgd_solver.cpp:112] Iteration 80670, lr = 0.01
I0522 22:28:26.398831 35003 solver.cpp:239] Iteration 80680 (2.28083 iter/s, 4.38437s/10 iters), loss = 7.12759
I0522 22:28:26.398874 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12759 (* 1 = 7.12759 loss)
I0522 22:28:27.106436 35003 sgd_solver.cpp:112] Iteration 80680, lr = 0.01
I0522 22:28:29.189725 35003 solver.cpp:239] Iteration 80690 (3.58329 iter/s, 2.79073s/10 iters), loss = 6.25344
I0522 22:28:29.189854 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25344 (* 1 = 6.25344 loss)
I0522 22:28:29.889664 35003 sgd_solver.cpp:112] Iteration 80690, lr = 0.01
I0522 22:28:34.294757 35003 solver.cpp:239] Iteration 80700 (1.95898 iter/s, 5.10469s/10 iters), loss = 7.63559
I0522 22:28:34.294808 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63559 (* 1 = 7.63559 loss)
I0522 22:28:35.008152 35003 sgd_solver.cpp:112] Iteration 80700, lr = 0.01
I0522 22:28:36.997695 35003 solver.cpp:239] Iteration 80710 (3.69991 iter/s, 2.70277s/10 iters), loss = 7.89522
I0522 22:28:36.997735 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89522 (* 1 = 7.89522 loss)
I0522 22:28:37.010869 35003 sgd_solver.cpp:112] Iteration 80710, lr = 0.01
I0522 22:28:39.598054 35003 solver.cpp:239] Iteration 80720 (3.84584 iter/s, 2.60021s/10 iters), loss = 7.4116
I0522 22:28:39.598091 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4116 (* 1 = 7.4116 loss)
I0522 22:28:39.611037 35003 sgd_solver.cpp:112] Iteration 80720, lr = 0.01
I0522 22:28:42.473646 35003 solver.cpp:239] Iteration 80730 (3.47775 iter/s, 2.87542s/10 iters), loss = 7.08095
I0522 22:28:42.473703 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08095 (* 1 = 7.08095 loss)
I0522 22:28:42.486896 35003 sgd_solver.cpp:112] Iteration 80730, lr = 0.01
I0522 22:28:45.329449 35003 solver.cpp:239] Iteration 80740 (3.50187 iter/s, 2.85562s/10 iters), loss = 6.81489
I0522 22:28:45.329491 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81489 (* 1 = 6.81489 loss)
I0522 22:28:45.342079 35003 sgd_solver.cpp:112] Iteration 80740, lr = 0.01
I0522 22:28:49.253455 35003 solver.cpp:239] Iteration 80750 (2.54855 iter/s, 3.9238s/10 iters), loss = 7.82524
I0522 22:28:49.253502 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82524 (* 1 = 7.82524 loss)
I0522 22:28:49.266932 35003 sgd_solver.cpp:112] Iteration 80750, lr = 0.01
I0522 22:28:52.003939 35003 solver.cpp:239] Iteration 80760 (3.63595 iter/s, 2.75031s/10 iters), loss = 7.1299
I0522 22:28:52.003983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1299 (* 1 = 7.1299 loss)
I0522 22:28:52.635777 35003 sgd_solver.cpp:112] Iteration 80760, lr = 0.01
I0522 22:28:55.805850 35003 solver.cpp:239] Iteration 80770 (2.6304 iter/s, 3.80171s/10 iters), loss = 7.69157
I0522 22:28:55.805893 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69157 (* 1 = 7.69157 loss)
I0522 22:28:56.495887 35003 sgd_solver.cpp:112] Iteration 80770, lr = 0.01
I0522 22:29:00.606535 35003 solver.cpp:239] Iteration 80780 (2.08314 iter/s, 4.80045s/10 iters), loss = 7.15439
I0522 22:29:00.606784 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15439 (* 1 = 7.15439 loss)
I0522 22:29:01.335422 35003 sgd_solver.cpp:112] Iteration 80780, lr = 0.01
I0522 22:29:04.839565 35003 solver.cpp:239] Iteration 80790 (2.3626 iter/s, 4.23263s/10 iters), loss = 6.16141
I0522 22:29:04.839612 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16141 (* 1 = 6.16141 loss)
I0522 22:29:05.473266 35003 sgd_solver.cpp:112] Iteration 80790, lr = 0.01
I0522 22:29:09.286890 35003 solver.cpp:239] Iteration 80800 (2.24866 iter/s, 4.4471s/10 iters), loss = 6.96303
I0522 22:29:09.286926 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96303 (* 1 = 6.96303 loss)
I0522 22:29:09.291936 35003 sgd_solver.cpp:112] Iteration 80800, lr = 0.01
I0522 22:29:13.522831 35003 solver.cpp:239] Iteration 80810 (2.36087 iter/s, 4.23572s/10 iters), loss = 7.83897
I0522 22:29:13.522874 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83897 (* 1 = 7.83897 loss)
I0522 22:29:13.526839 35003 sgd_solver.cpp:112] Iteration 80810, lr = 0.01
I0522 22:29:16.309976 35003 solver.cpp:239] Iteration 80820 (3.58812 iter/s, 2.78698s/10 iters), loss = 6.76387
I0522 22:29:16.310014 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76387 (* 1 = 6.76387 loss)
I0522 22:29:16.317708 35003 sgd_solver.cpp:112] Iteration 80820, lr = 0.01
I0522 22:29:20.084914 35003 solver.cpp:239] Iteration 80830 (2.64919 iter/s, 3.77474s/10 iters), loss = 7.3758
I0522 22:29:20.084961 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3758 (* 1 = 7.3758 loss)
I0522 22:29:20.096616 35003 sgd_solver.cpp:112] Iteration 80830, lr = 0.01
I0522 22:29:22.815796 35003 solver.cpp:239] Iteration 80840 (3.66204 iter/s, 2.73072s/10 iters), loss = 6.83015
I0522 22:29:22.815834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83015 (* 1 = 6.83015 loss)
I0522 22:29:22.977576 35003 sgd_solver.cpp:112] Iteration 80840, lr = 0.01
I0522 22:29:28.047293 35003 solver.cpp:239] Iteration 80850 (1.91159 iter/s, 5.23125s/10 iters), loss = 7.93136
I0522 22:29:28.047355 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93136 (* 1 = 7.93136 loss)
I0522 22:29:28.276408 35003 sgd_solver.cpp:112] Iteration 80850, lr = 0.01
I0522 22:29:31.510242 35003 solver.cpp:239] Iteration 80860 (2.88789 iter/s, 3.46274s/10 iters), loss = 7.04213
I0522 22:29:31.510576 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04213 (* 1 = 7.04213 loss)
I0522 22:29:31.516314 35003 sgd_solver.cpp:112] Iteration 80860, lr = 0.01
I0522 22:29:35.246511 35003 solver.cpp:239] Iteration 80870 (2.6768 iter/s, 3.73581s/10 iters), loss = 7.4719
I0522 22:29:35.246551 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4719 (* 1 = 7.4719 loss)
I0522 22:29:35.252076 35003 sgd_solver.cpp:112] Iteration 80870, lr = 0.01
I0522 22:29:38.095384 35003 solver.cpp:239] Iteration 80880 (3.51036 iter/s, 2.84871s/10 iters), loss = 6.47923
I0522 22:29:38.095432 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47923 (* 1 = 6.47923 loss)
I0522 22:29:38.099042 35003 sgd_solver.cpp:112] Iteration 80880, lr = 0.01
I0522 22:29:42.478683 35003 solver.cpp:239] Iteration 80890 (2.28151 iter/s, 4.38306s/10 iters), loss = 7.80176
I0522 22:29:42.478765 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80176 (* 1 = 7.80176 loss)
I0522 22:29:42.570963 35003 sgd_solver.cpp:112] Iteration 80890, lr = 0.01
I0522 22:29:45.895342 35003 solver.cpp:239] Iteration 80900 (2.92703 iter/s, 3.41643s/10 iters), loss = 6.81438
I0522 22:29:45.895401 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81438 (* 1 = 6.81438 loss)
I0522 22:29:45.913259 35003 sgd_solver.cpp:112] Iteration 80900, lr = 0.01
I0522 22:29:48.527346 35003 solver.cpp:239] Iteration 80910 (3.79963 iter/s, 2.63183s/10 iters), loss = 6.81822
I0522 22:29:48.527400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81822 (* 1 = 6.81822 loss)
I0522 22:29:49.255141 35003 sgd_solver.cpp:112] Iteration 80910, lr = 0.01
I0522 22:29:51.352262 35003 solver.cpp:239] Iteration 80920 (3.54016 iter/s, 2.82473s/10 iters), loss = 7.66571
I0522 22:29:51.352325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66571 (* 1 = 7.66571 loss)
I0522 22:29:51.364544 35003 sgd_solver.cpp:112] Iteration 80920, lr = 0.01
I0522 22:29:53.463901 35003 solver.cpp:239] Iteration 80930 (4.736 iter/s, 2.11149s/10 iters), loss = 7.3498
I0522 22:29:53.463948 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3498 (* 1 = 7.3498 loss)
I0522 22:29:54.179170 35003 sgd_solver.cpp:112] Iteration 80930, lr = 0.01
I0522 22:29:57.586784 35003 solver.cpp:239] Iteration 80940 (2.42563 iter/s, 4.12264s/10 iters), loss = 8.51675
I0522 22:29:57.586841 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.51675 (* 1 = 8.51675 loss)
I0522 22:29:58.282256 35003 sgd_solver.cpp:112] Iteration 80940, lr = 0.01
I0522 22:30:01.299980 35003 solver.cpp:239] Iteration 80950 (2.69326 iter/s, 3.71297s/10 iters), loss = 6.90638
I0522 22:30:01.300024 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90638 (* 1 = 6.90638 loss)
I0522 22:30:01.312892 35003 sgd_solver.cpp:112] Iteration 80950, lr = 0.01
I0522 22:30:02.664265 35003 solver.cpp:239] Iteration 80960 (7.33048 iter/s, 1.36417s/10 iters), loss = 7.45702
I0522 22:30:02.664518 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45702 (* 1 = 7.45702 loss)
I0522 22:30:03.405093 35003 sgd_solver.cpp:112] Iteration 80960, lr = 0.01
I0522 22:30:05.728266 35003 solver.cpp:239] Iteration 80970 (3.26407 iter/s, 3.06366s/10 iters), loss = 6.6528
I0522 22:30:05.728301 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6528 (* 1 = 6.6528 loss)
I0522 22:30:05.758049 35003 sgd_solver.cpp:112] Iteration 80970, lr = 0.01
I0522 22:30:10.076823 35003 solver.cpp:239] Iteration 80980 (2.29973 iter/s, 4.34835s/10 iters), loss = 6.93018
I0522 22:30:10.076865 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93018 (* 1 = 6.93018 loss)
I0522 22:30:10.085469 35003 sgd_solver.cpp:112] Iteration 80980, lr = 0.01
I0522 22:30:12.169788 35003 solver.cpp:239] Iteration 80990 (4.77822 iter/s, 2.09283s/10 iters), loss = 8.01469
I0522 22:30:12.169843 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01469 (* 1 = 8.01469 loss)
I0522 22:30:12.190582 35003 sgd_solver.cpp:112] Iteration 80990, lr = 0.01
I0522 22:30:15.854809 35003 solver.cpp:239] Iteration 81000 (2.71384 iter/s, 3.68481s/10 iters), loss = 6.89262
I0522 22:30:15.854864 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89262 (* 1 = 6.89262 loss)
I0522 22:30:15.868300 35003 sgd_solver.cpp:112] Iteration 81000, lr = 0.01
I0522 22:30:18.864655 35003 solver.cpp:239] Iteration 81010 (3.32263 iter/s, 3.00967s/10 iters), loss = 7.70979
I0522 22:30:18.864711 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70979 (* 1 = 7.70979 loss)
I0522 22:30:19.605940 35003 sgd_solver.cpp:112] Iteration 81010, lr = 0.01
I0522 22:30:22.249866 35003 solver.cpp:239] Iteration 81020 (2.9542 iter/s, 3.38501s/10 iters), loss = 8.01431
I0522 22:30:22.249923 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01431 (* 1 = 8.01431 loss)
I0522 22:30:22.262751 35003 sgd_solver.cpp:112] Iteration 81020, lr = 0.01
I0522 22:30:25.820451 35003 solver.cpp:239] Iteration 81030 (2.80082 iter/s, 3.57038s/10 iters), loss = 7.27829
I0522 22:30:25.820503 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27829 (* 1 = 7.27829 loss)
I0522 22:30:26.440810 35003 sgd_solver.cpp:112] Iteration 81030, lr = 0.01
I0522 22:30:30.083617 35003 solver.cpp:239] Iteration 81040 (2.3458 iter/s, 4.26294s/10 iters), loss = 8.4939
I0522 22:30:30.083665 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.4939 (* 1 = 8.4939 loss)
I0522 22:30:30.091948 35003 sgd_solver.cpp:112] Iteration 81040, lr = 0.01
I0522 22:30:33.566196 35003 solver.cpp:239] Iteration 81050 (2.87159 iter/s, 3.48239s/10 iters), loss = 7.06251
I0522 22:30:33.566442 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06251 (* 1 = 7.06251 loss)
I0522 22:30:33.583184 35003 sgd_solver.cpp:112] Iteration 81050, lr = 0.01
I0522 22:30:37.297363 35003 solver.cpp:239] Iteration 81060 (2.68193 iter/s, 3.72865s/10 iters), loss = 6.16577
I0522 22:30:37.297402 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16577 (* 1 = 6.16577 loss)
I0522 22:30:37.304841 35003 sgd_solver.cpp:112] Iteration 81060, lr = 0.01
I0522 22:30:40.227743 35003 solver.cpp:239] Iteration 81070 (3.41273 iter/s, 2.93021s/10 iters), loss = 7.34199
I0522 22:30:40.227799 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34199 (* 1 = 7.34199 loss)
I0522 22:30:40.242358 35003 sgd_solver.cpp:112] Iteration 81070, lr = 0.01
I0522 22:30:42.943517 35003 solver.cpp:239] Iteration 81080 (3.68242 iter/s, 2.7156s/10 iters), loss = 7.78186
I0522 22:30:42.943576 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78186 (* 1 = 7.78186 loss)
I0522 22:30:42.953986 35003 sgd_solver.cpp:112] Iteration 81080, lr = 0.01
I0522 22:30:46.586760 35003 solver.cpp:239] Iteration 81090 (2.74497 iter/s, 3.64303s/10 iters), loss = 8.19347
I0522 22:30:46.586814 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.19347 (* 1 = 8.19347 loss)
I0522 22:30:47.302179 35003 sgd_solver.cpp:112] Iteration 81090, lr = 0.01
I0522 22:30:50.098459 35003 solver.cpp:239] Iteration 81100 (2.84779 iter/s, 3.5115s/10 iters), loss = 6.4578
I0522 22:30:50.098510 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4578 (* 1 = 6.4578 loss)
I0522 22:30:50.111568 35003 sgd_solver.cpp:112] Iteration 81100, lr = 0.01
I0522 22:30:51.424480 35003 solver.cpp:239] Iteration 81110 (7.54201 iter/s, 1.32591s/10 iters), loss = 7.37209
I0522 22:30:51.424525 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37209 (* 1 = 7.37209 loss)
I0522 22:30:51.437680 35003 sgd_solver.cpp:112] Iteration 81110, lr = 0.01
I0522 22:30:54.150099 35003 solver.cpp:239] Iteration 81120 (3.66912 iter/s, 2.72545s/10 iters), loss = 6.90204
I0522 22:30:54.150153 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90204 (* 1 = 6.90204 loss)
I0522 22:30:54.162896 35003 sgd_solver.cpp:112] Iteration 81120, lr = 0.01
I0522 22:30:57.890406 35003 solver.cpp:239] Iteration 81130 (2.67373 iter/s, 3.74009s/10 iters), loss = 7.26135
I0522 22:30:57.890462 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26135 (* 1 = 7.26135 loss)
I0522 22:30:57.903578 35003 sgd_solver.cpp:112] Iteration 81130, lr = 0.01
I0522 22:31:01.007589 35003 solver.cpp:239] Iteration 81140 (3.20821 iter/s, 3.117s/10 iters), loss = 7.18265
I0522 22:31:01.007638 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18265 (* 1 = 7.18265 loss)
I0522 22:31:01.465248 35003 sgd_solver.cpp:112] Iteration 81140, lr = 0.01
I0522 22:31:04.949160 35003 solver.cpp:239] Iteration 81150 (2.5372 iter/s, 3.94136s/10 iters), loss = 7.143
I0522 22:31:04.949446 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.143 (* 1 = 7.143 loss)
I0522 22:31:04.959079 35003 sgd_solver.cpp:112] Iteration 81150, lr = 0.01
I0522 22:31:08.612712 35003 solver.cpp:239] Iteration 81160 (2.72989 iter/s, 3.66315s/10 iters), loss = 7.35369
I0522 22:31:08.612766 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35369 (* 1 = 7.35369 loss)
I0522 22:31:09.017424 35003 sgd_solver.cpp:112] Iteration 81160, lr = 0.01
I0522 22:31:11.595643 35003 solver.cpp:239] Iteration 81170 (3.35261 iter/s, 2.98275s/10 iters), loss = 8.26434
I0522 22:31:11.595685 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.26434 (* 1 = 8.26434 loss)
I0522 22:31:12.330698 35003 sgd_solver.cpp:112] Iteration 81170, lr = 0.01
I0522 22:31:14.926187 35003 solver.cpp:239] Iteration 81180 (3.00267 iter/s, 3.33037s/10 iters), loss = 6.17132
I0522 22:31:14.926226 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17132 (* 1 = 6.17132 loss)
I0522 22:31:14.939465 35003 sgd_solver.cpp:112] Iteration 81180, lr = 0.01
I0522 22:31:19.408367 35003 solver.cpp:239] Iteration 81190 (2.23118 iter/s, 4.48194s/10 iters), loss = 7.68056
I0522 22:31:19.408424 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68056 (* 1 = 7.68056 loss)
I0522 22:31:20.149242 35003 sgd_solver.cpp:112] Iteration 81190, lr = 0.01
I0522 22:31:22.003178 35003 solver.cpp:239] Iteration 81200 (3.8541 iter/s, 2.59464s/10 iters), loss = 7.80986
I0522 22:31:22.003223 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80986 (* 1 = 7.80986 loss)
I0522 22:31:22.015763 35003 sgd_solver.cpp:112] Iteration 81200, lr = 0.01
I0522 22:31:24.966413 35003 solver.cpp:239] Iteration 81210 (3.37491 iter/s, 2.96305s/10 iters), loss = 6.71759
I0522 22:31:24.966464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71759 (* 1 = 6.71759 loss)
I0522 22:31:24.978241 35003 sgd_solver.cpp:112] Iteration 81210, lr = 0.01
I0522 22:31:30.339870 35003 solver.cpp:239] Iteration 81220 (1.86109 iter/s, 5.37318s/10 iters), loss = 8.22522
I0522 22:31:30.339921 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.22522 (* 1 = 8.22522 loss)
I0522 22:31:30.460963 35003 sgd_solver.cpp:112] Iteration 81220, lr = 0.01
I0522 22:31:32.599047 35003 solver.cpp:239] Iteration 81230 (4.42668 iter/s, 2.25903s/10 iters), loss = 7.99841
I0522 22:31:32.599097 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99841 (* 1 = 7.99841 loss)
I0522 22:31:33.333402 35003 sgd_solver.cpp:112] Iteration 81230, lr = 0.01
I0522 22:31:34.757210 35003 solver.cpp:239] Iteration 81240 (4.63388 iter/s, 2.15802s/10 iters), loss = 8.47433
I0522 22:31:34.757264 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.47433 (* 1 = 8.47433 loss)
I0522 22:31:34.770581 35003 sgd_solver.cpp:112] Iteration 81240, lr = 0.01
I0522 22:31:37.140211 35003 solver.cpp:239] Iteration 81250 (4.19667 iter/s, 2.38284s/10 iters), loss = 7.67455
I0522 22:31:37.140499 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67455 (* 1 = 7.67455 loss)
I0522 22:31:37.152003 35003 sgd_solver.cpp:112] Iteration 81250, lr = 0.01
I0522 22:31:40.007351 35003 solver.cpp:239] Iteration 81260 (3.48827 iter/s, 2.86675s/10 iters), loss = 7.24228
I0522 22:31:40.007398 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24228 (* 1 = 7.24228 loss)
I0522 22:31:40.724823 35003 sgd_solver.cpp:112] Iteration 81260, lr = 0.01
I0522 22:31:43.417497 35003 solver.cpp:239] Iteration 81270 (2.93259 iter/s, 3.40996s/10 iters), loss = 6.02408
I0522 22:31:43.417546 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02408 (* 1 = 6.02408 loss)
I0522 22:31:43.421170 35003 sgd_solver.cpp:112] Iteration 81270, lr = 0.01
I0522 22:31:46.828228 35003 solver.cpp:239] Iteration 81280 (2.93209 iter/s, 3.41054s/10 iters), loss = 5.90975
I0522 22:31:46.828274 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.90975 (* 1 = 5.90975 loss)
I0522 22:31:46.841974 35003 sgd_solver.cpp:112] Iteration 81280, lr = 0.01
I0522 22:31:50.345916 35003 solver.cpp:239] Iteration 81290 (2.84294 iter/s, 3.51749s/10 iters), loss = 7.70484
I0522 22:31:50.345976 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70484 (* 1 = 7.70484 loss)
I0522 22:31:50.355504 35003 sgd_solver.cpp:112] Iteration 81290, lr = 0.01
I0522 22:31:53.846424 35003 solver.cpp:239] Iteration 81300 (2.85689 iter/s, 3.50031s/10 iters), loss = 6.19257
I0522 22:31:53.846464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19257 (* 1 = 6.19257 loss)
I0522 22:31:54.568320 35003 sgd_solver.cpp:112] Iteration 81300, lr = 0.01
I0522 22:31:58.365332 35003 solver.cpp:239] Iteration 81310 (2.21304 iter/s, 4.51867s/10 iters), loss = 6.54442
I0522 22:31:58.365380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54442 (* 1 = 6.54442 loss)
I0522 22:31:58.379024 35003 sgd_solver.cpp:112] Iteration 81310, lr = 0.01
I0522 22:32:02.003947 35003 solver.cpp:239] Iteration 81320 (2.74845 iter/s, 3.63842s/10 iters), loss = 7.48642
I0522 22:32:02.003999 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48642 (* 1 = 7.48642 loss)
I0522 22:32:02.745385 35003 sgd_solver.cpp:112] Iteration 81320, lr = 0.01
I0522 22:32:06.422348 35003 solver.cpp:239] Iteration 81330 (2.26338 iter/s, 4.41816s/10 iters), loss = 6.19429
I0522 22:32:06.422422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19429 (* 1 = 6.19429 loss)
I0522 22:32:06.488509 35003 sgd_solver.cpp:112] Iteration 81330, lr = 0.01
I0522 22:32:10.920145 35003 solver.cpp:239] Iteration 81340 (2.22344 iter/s, 4.49754s/10 iters), loss = 6.61391
I0522 22:32:10.920375 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61391 (* 1 = 6.61391 loss)
I0522 22:32:10.930115 35003 sgd_solver.cpp:112] Iteration 81340, lr = 0.01
I0522 22:32:13.778419 35003 solver.cpp:239] Iteration 81350 (3.49902 iter/s, 2.85794s/10 iters), loss = 7.39027
I0522 22:32:13.778473 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39027 (* 1 = 7.39027 loss)
I0522 22:32:13.790084 35003 sgd_solver.cpp:112] Iteration 81350, lr = 0.01
I0522 22:32:16.639519 35003 solver.cpp:239] Iteration 81360 (3.49537 iter/s, 2.86093s/10 iters), loss = 6.36221
I0522 22:32:16.639554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36221 (* 1 = 6.36221 loss)
I0522 22:32:16.653039 35003 sgd_solver.cpp:112] Iteration 81360, lr = 0.01
I0522 22:32:21.028842 35003 solver.cpp:239] Iteration 81370 (2.27838 iter/s, 4.38909s/10 iters), loss = 7.6377
I0522 22:32:21.028904 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6377 (* 1 = 7.6377 loss)
I0522 22:32:21.033746 35003 sgd_solver.cpp:112] Iteration 81370, lr = 0.01
I0522 22:32:25.229207 35003 solver.cpp:239] Iteration 81380 (2.38088 iter/s, 4.20013s/10 iters), loss = 7.22596
I0522 22:32:25.229254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22596 (* 1 = 7.22596 loss)
I0522 22:32:25.233078 35003 sgd_solver.cpp:112] Iteration 81380, lr = 0.01
I0522 22:32:27.749958 35003 solver.cpp:239] Iteration 81390 (3.96733 iter/s, 2.52058s/10 iters), loss = 8.44509
I0522 22:32:27.750003 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.44509 (* 1 = 8.44509 loss)
I0522 22:32:28.483686 35003 sgd_solver.cpp:112] Iteration 81390, lr = 0.01
I0522 22:32:31.556068 35003 solver.cpp:239] Iteration 81400 (2.6275 iter/s, 3.8059s/10 iters), loss = 7.49412
I0522 22:32:31.556126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49412 (* 1 = 7.49412 loss)
I0522 22:32:32.238090 35003 sgd_solver.cpp:112] Iteration 81400, lr = 0.01
I0522 22:32:34.329291 35003 solver.cpp:239] Iteration 81410 (3.60614 iter/s, 2.77305s/10 iters), loss = 7.7636
I0522 22:32:34.329340 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7636 (* 1 = 7.7636 loss)
I0522 22:32:34.344406 35003 sgd_solver.cpp:112] Iteration 81410, lr = 0.01
I0522 22:32:36.518105 35003 solver.cpp:239] Iteration 81420 (4.56898 iter/s, 2.18867s/10 iters), loss = 7.74308
I0522 22:32:36.518144 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74308 (* 1 = 7.74308 loss)
I0522 22:32:36.521894 35003 sgd_solver.cpp:112] Iteration 81420, lr = 0.01
I0522 22:32:39.465167 35003 solver.cpp:239] Iteration 81430 (3.3934 iter/s, 2.94689s/10 iters), loss = 7.17904
I0522 22:32:39.465219 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17904 (* 1 = 7.17904 loss)
I0522 22:32:39.484225 35003 sgd_solver.cpp:112] Iteration 81430, lr = 0.01
I0522 22:32:43.240895 35003 solver.cpp:239] Iteration 81440 (2.64864 iter/s, 3.77552s/10 iters), loss = 7.37601
I0522 22:32:43.241106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37601 (* 1 = 7.37601 loss)
I0522 22:32:43.248906 35003 sgd_solver.cpp:112] Iteration 81440, lr = 0.01
I0522 22:32:46.835553 35003 solver.cpp:239] Iteration 81450 (2.78218 iter/s, 3.59431s/10 iters), loss = 7.61189
I0522 22:32:46.835592 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61189 (* 1 = 7.61189 loss)
I0522 22:32:46.858036 35003 sgd_solver.cpp:112] Iteration 81450, lr = 0.01
I0522 22:32:49.603508 35003 solver.cpp:239] Iteration 81460 (3.61297 iter/s, 2.7678s/10 iters), loss = 7.61403
I0522 22:32:49.603545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61403 (* 1 = 7.61403 loss)
I0522 22:32:49.614424 35003 sgd_solver.cpp:112] Iteration 81460, lr = 0.01
I0522 22:32:51.847774 35003 solver.cpp:239] Iteration 81470 (4.45608 iter/s, 2.24412s/10 iters), loss = 7.25701
I0522 22:32:51.847813 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25701 (* 1 = 7.25701 loss)
I0522 22:32:51.855661 35003 sgd_solver.cpp:112] Iteration 81470, lr = 0.01
I0522 22:32:54.729426 35003 solver.cpp:239] Iteration 81480 (3.47042 iter/s, 2.88149s/10 iters), loss = 7.33087
I0522 22:32:54.729465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33087 (* 1 = 7.33087 loss)
I0522 22:32:54.742674 35003 sgd_solver.cpp:112] Iteration 81480, lr = 0.01
I0522 22:32:58.973124 35003 solver.cpp:239] Iteration 81490 (2.35655 iter/s, 4.24349s/10 iters), loss = 6.8443
I0522 22:32:58.973165 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8443 (* 1 = 6.8443 loss)
I0522 22:32:58.986562 35003 sgd_solver.cpp:112] Iteration 81490, lr = 0.01
I0522 22:33:01.815421 35003 solver.cpp:239] Iteration 81500 (3.51849 iter/s, 2.84213s/10 iters), loss = 8.16158
I0522 22:33:01.815466 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16158 (* 1 = 8.16158 loss)
I0522 22:33:01.828557 35003 sgd_solver.cpp:112] Iteration 81500, lr = 0.01
I0522 22:33:04.661281 35003 solver.cpp:239] Iteration 81510 (3.51408 iter/s, 2.84569s/10 iters), loss = 6.93599
I0522 22:33:04.661321 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93599 (* 1 = 6.93599 loss)
I0522 22:33:04.674859 35003 sgd_solver.cpp:112] Iteration 81510, lr = 0.01
I0522 22:33:08.178869 35003 solver.cpp:239] Iteration 81520 (2.84301 iter/s, 3.5174s/10 iters), loss = 7.74119
I0522 22:33:08.178921 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74119 (* 1 = 7.74119 loss)
I0522 22:33:08.182693 35003 sgd_solver.cpp:112] Iteration 81520, lr = 0.01
I0522 22:33:12.504218 35003 solver.cpp:239] Iteration 81530 (2.31207 iter/s, 4.32513s/10 iters), loss = 7.24024
I0522 22:33:12.504261 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24024 (* 1 = 7.24024 loss)
I0522 22:33:12.517285 35003 sgd_solver.cpp:112] Iteration 81530, lr = 0.01
I0522 22:33:16.743120 35003 solver.cpp:239] Iteration 81540 (2.35923 iter/s, 4.23868s/10 iters), loss = 7.77918
I0522 22:33:16.743458 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77918 (* 1 = 7.77918 loss)
I0522 22:33:16.748450 35003 sgd_solver.cpp:112] Iteration 81540, lr = 0.01
I0522 22:33:20.221902 35003 solver.cpp:239] Iteration 81550 (2.87494 iter/s, 3.47833s/10 iters), loss = 6.65419
I0522 22:33:20.221946 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65419 (* 1 = 6.65419 loss)
I0522 22:33:20.241029 35003 sgd_solver.cpp:112] Iteration 81550, lr = 0.01
I0522 22:33:23.093449 35003 solver.cpp:239] Iteration 81560 (3.48265 iter/s, 2.87137s/10 iters), loss = 6.71146
I0522 22:33:23.093519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71146 (* 1 = 6.71146 loss)
I0522 22:33:23.106524 35003 sgd_solver.cpp:112] Iteration 81560, lr = 0.01
I0522 22:33:27.939514 35003 solver.cpp:239] Iteration 81570 (2.06366 iter/s, 4.84577s/10 iters), loss = 7.49755
I0522 22:33:27.939574 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49755 (* 1 = 7.49755 loss)
I0522 22:33:28.122866 35003 sgd_solver.cpp:112] Iteration 81570, lr = 0.01
I0522 22:33:30.532045 35003 solver.cpp:239] Iteration 81580 (3.85748 iter/s, 2.59236s/10 iters), loss = 6.9415
I0522 22:33:30.532088 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9415 (* 1 = 6.9415 loss)
I0522 22:33:30.555948 35003 sgd_solver.cpp:112] Iteration 81580, lr = 0.01
I0522 22:33:34.785936 35003 solver.cpp:239] Iteration 81590 (2.35091 iter/s, 4.25367s/10 iters), loss = 8.1744
I0522 22:33:34.785989 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.1744 (* 1 = 8.1744 loss)
I0522 22:33:35.282320 35003 sgd_solver.cpp:112] Iteration 81590, lr = 0.01
I0522 22:33:38.969913 35003 solver.cpp:239] Iteration 81600 (2.3902 iter/s, 4.18375s/10 iters), loss = 7.77921
I0522 22:33:38.969969 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77921 (* 1 = 7.77921 loss)
I0522 22:33:38.975623 35003 sgd_solver.cpp:112] Iteration 81600, lr = 0.01
I0522 22:33:42.189713 35003 solver.cpp:239] Iteration 81610 (3.10599 iter/s, 3.21959s/10 iters), loss = 7.77556
I0522 22:33:42.189769 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77556 (* 1 = 7.77556 loss)
I0522 22:33:42.196372 35003 sgd_solver.cpp:112] Iteration 81610, lr = 0.01
I0522 22:33:46.517199 35003 solver.cpp:239] Iteration 81620 (2.31093 iter/s, 4.32726s/10 iters), loss = 6.74508
I0522 22:33:46.517241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74508 (* 1 = 6.74508 loss)
I0522 22:33:47.226202 35003 sgd_solver.cpp:112] Iteration 81620, lr = 0.01
I0522 22:33:50.199569 35003 solver.cpp:239] Iteration 81630 (2.71579 iter/s, 3.68217s/10 iters), loss = 6.05399
I0522 22:33:50.199623 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05399 (* 1 = 6.05399 loss)
I0522 22:33:50.379850 35003 sgd_solver.cpp:112] Iteration 81630, lr = 0.01
I0522 22:33:53.412075 35003 solver.cpp:239] Iteration 81640 (3.11302 iter/s, 3.21232s/10 iters), loss = 7.3978
I0522 22:33:53.412122 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3978 (* 1 = 7.3978 loss)
I0522 22:33:53.418462 35003 sgd_solver.cpp:112] Iteration 81640, lr = 0.01
I0522 22:33:55.516898 35003 solver.cpp:239] Iteration 81650 (4.75131 iter/s, 2.10468s/10 iters), loss = 6.81432
I0522 22:33:55.516950 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81432 (* 1 = 6.81432 loss)
I0522 22:33:56.153630 35003 sgd_solver.cpp:112] Iteration 81650, lr = 0.01
I0522 22:33:59.198793 35003 solver.cpp:239] Iteration 81660 (2.71615 iter/s, 3.68168s/10 iters), loss = 8.5943
I0522 22:33:59.198837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.5943 (* 1 = 8.5943 loss)
I0522 22:33:59.207602 35003 sgd_solver.cpp:112] Iteration 81660, lr = 0.01
I0522 22:34:02.745872 35003 solver.cpp:239] Iteration 81670 (2.81937 iter/s, 3.54689s/10 iters), loss = 7.59749
I0522 22:34:02.745923 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59749 (* 1 = 7.59749 loss)
I0522 22:34:02.754334 35003 sgd_solver.cpp:112] Iteration 81670, lr = 0.01
I0522 22:34:04.818001 35003 solver.cpp:239] Iteration 81680 (4.8263 iter/s, 2.07198s/10 iters), loss = 7.49252
I0522 22:34:04.818064 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49252 (* 1 = 7.49252 loss)
I0522 22:34:04.836251 35003 sgd_solver.cpp:112] Iteration 81680, lr = 0.01
I0522 22:34:08.775146 35003 solver.cpp:239] Iteration 81690 (2.52722 iter/s, 3.95692s/10 iters), loss = 7.84625
I0522 22:34:08.775194 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84625 (* 1 = 7.84625 loss)
I0522 22:34:08.783630 35003 sgd_solver.cpp:112] Iteration 81690, lr = 0.01
I0522 22:34:12.276067 35003 solver.cpp:239] Iteration 81700 (2.85655 iter/s, 3.50072s/10 iters), loss = 8.41941
I0522 22:34:12.276139 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.41941 (* 1 = 8.41941 loss)
I0522 22:34:13.007603 35003 sgd_solver.cpp:112] Iteration 81700, lr = 0.01
I0522 22:34:15.903862 35003 solver.cpp:239] Iteration 81710 (2.75666 iter/s, 3.62758s/10 iters), loss = 7.7801
I0522 22:34:15.903899 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7801 (* 1 = 7.7801 loss)
I0522 22:34:15.916929 35003 sgd_solver.cpp:112] Iteration 81710, lr = 0.01
I0522 22:34:18.768445 35003 solver.cpp:239] Iteration 81720 (3.49113 iter/s, 2.86441s/10 iters), loss = 8.03451
I0522 22:34:18.768754 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03451 (* 1 = 8.03451 loss)
I0522 22:34:19.170931 35003 sgd_solver.cpp:112] Iteration 81720, lr = 0.01
I0522 22:34:24.452469 35003 solver.cpp:239] Iteration 81730 (1.75947 iter/s, 5.68352s/10 iters), loss = 8.11886
I0522 22:34:24.452524 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.11886 (* 1 = 8.11886 loss)
I0522 22:34:24.476603 35003 sgd_solver.cpp:112] Iteration 81730, lr = 0.01
I0522 22:34:28.213948 35003 solver.cpp:239] Iteration 81740 (2.65868 iter/s, 3.76127s/10 iters), loss = 7.21111
I0522 22:34:28.214001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21111 (* 1 = 7.21111 loss)
I0522 22:34:28.221240 35003 sgd_solver.cpp:112] Iteration 81740, lr = 0.01
I0522 22:34:31.772815 35003 solver.cpp:239] Iteration 81750 (2.81004 iter/s, 3.55866s/10 iters), loss = 7.2126
I0522 22:34:31.772859 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2126 (* 1 = 7.2126 loss)
I0522 22:34:31.786727 35003 sgd_solver.cpp:112] Iteration 81750, lr = 0.01
I0522 22:34:34.587710 35003 solver.cpp:239] Iteration 81760 (3.55274 iter/s, 2.81473s/10 iters), loss = 7.32914
I0522 22:34:34.587760 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32914 (* 1 = 7.32914 loss)
I0522 22:34:34.598337 35003 sgd_solver.cpp:112] Iteration 81760, lr = 0.01
I0522 22:34:36.663425 35003 solver.cpp:239] Iteration 81770 (4.81794 iter/s, 2.07558s/10 iters), loss = 7.51693
I0522 22:34:36.663468 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51693 (* 1 = 7.51693 loss)
I0522 22:34:36.676684 35003 sgd_solver.cpp:112] Iteration 81770, lr = 0.01
I0522 22:34:40.241833 35003 solver.cpp:239] Iteration 81780 (2.79469 iter/s, 3.57822s/10 iters), loss = 7.63422
I0522 22:34:40.241883 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63422 (* 1 = 7.63422 loss)
I0522 22:34:40.914525 35003 sgd_solver.cpp:112] Iteration 81780, lr = 0.01
I0522 22:34:43.657001 35003 solver.cpp:239] Iteration 81790 (2.92828 iter/s, 3.41498s/10 iters), loss = 7.67437
I0522 22:34:43.657039 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67437 (* 1 = 7.67437 loss)
I0522 22:34:43.661669 35003 sgd_solver.cpp:112] Iteration 81790, lr = 0.01
I0522 22:34:47.295528 35003 solver.cpp:239] Iteration 81800 (2.74852 iter/s, 3.63833s/10 iters), loss = 7.01594
I0522 22:34:47.295579 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01594 (* 1 = 7.01594 loss)
I0522 22:34:47.300726 35003 sgd_solver.cpp:112] Iteration 81800, lr = 0.01
I0522 22:34:50.109158 35003 solver.cpp:239] Iteration 81810 (3.55435 iter/s, 2.81345s/10 iters), loss = 7.18507
I0522 22:34:50.109400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18507 (* 1 = 7.18507 loss)
I0522 22:34:50.303953 35003 sgd_solver.cpp:112] Iteration 81810, lr = 0.01
I0522 22:34:55.448173 35003 solver.cpp:239] Iteration 81820 (1.87316 iter/s, 5.33856s/10 iters), loss = 6.71872
I0522 22:34:55.448217 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71872 (* 1 = 6.71872 loss)
I0522 22:34:55.454504 35003 sgd_solver.cpp:112] Iteration 81820, lr = 0.01
I0522 22:34:59.065629 35003 solver.cpp:239] Iteration 81830 (2.76453 iter/s, 3.61725s/10 iters), loss = 7.81851
I0522 22:34:59.065680 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81851 (* 1 = 7.81851 loss)
I0522 22:34:59.071396 35003 sgd_solver.cpp:112] Iteration 81830, lr = 0.01
I0522 22:35:03.483497 35003 solver.cpp:239] Iteration 81840 (2.26365 iter/s, 4.41764s/10 iters), loss = 7.10873
I0522 22:35:03.483553 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10873 (* 1 = 7.10873 loss)
I0522 22:35:03.697052 35003 sgd_solver.cpp:112] Iteration 81840, lr = 0.01
I0522 22:35:05.042343 35003 solver.cpp:239] Iteration 81850 (6.41553 iter/s, 1.55872s/10 iters), loss = 7.30554
I0522 22:35:05.042383 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30554 (* 1 = 7.30554 loss)
I0522 22:35:05.054414 35003 sgd_solver.cpp:112] Iteration 81850, lr = 0.01
I0522 22:35:05.888379 35003 solver.cpp:239] Iteration 81860 (11.8211 iter/s, 0.845947s/10 iters), loss = 7.48041
I0522 22:35:05.888443 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48041 (* 1 = 7.48041 loss)
I0522 22:35:05.895417 35003 sgd_solver.cpp:112] Iteration 81860, lr = 0.01
I0522 22:35:07.199926 35003 solver.cpp:239] Iteration 81870 (7.6253 iter/s, 1.31142s/10 iters), loss = 7.75161
I0522 22:35:07.199972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75161 (* 1 = 7.75161 loss)
I0522 22:35:07.213915 35003 sgd_solver.cpp:112] Iteration 81870, lr = 0.01
I0522 22:35:10.712733 35003 solver.cpp:239] Iteration 81880 (2.84689 iter/s, 3.51261s/10 iters), loss = 4.708
I0522 22:35:10.712786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.708 (* 1 = 4.708 loss)
I0522 22:35:10.724973 35003 sgd_solver.cpp:112] Iteration 81880, lr = 0.01
I0522 22:35:12.833058 35003 solver.cpp:239] Iteration 81890 (4.71661 iter/s, 2.12017s/10 iters), loss = 7.50519
I0522 22:35:12.833114 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50519 (* 1 = 7.50519 loss)
I0522 22:35:13.529155 35003 sgd_solver.cpp:112] Iteration 81890, lr = 0.01
I0522 22:35:15.566588 35003 solver.cpp:239] Iteration 81900 (3.65851 iter/s, 2.73335s/10 iters), loss = 7.62342
I0522 22:35:15.566634 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62342 (* 1 = 7.62342 loss)
I0522 22:35:15.589459 35003 sgd_solver.cpp:112] Iteration 81900, lr = 0.01
I0522 22:35:19.142067 35003 solver.cpp:239] Iteration 81910 (2.79698 iter/s, 3.57528s/10 iters), loss = 8.16525
I0522 22:35:19.142105 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16525 (* 1 = 8.16525 loss)
I0522 22:35:19.148617 35003 sgd_solver.cpp:112] Iteration 81910, lr = 0.01
I0522 22:35:23.388044 35003 solver.cpp:239] Iteration 81920 (2.35529 iter/s, 4.24576s/10 iters), loss = 6.68115
I0522 22:35:23.388170 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68115 (* 1 = 6.68115 loss)
I0522 22:35:23.420769 35003 sgd_solver.cpp:112] Iteration 81920, lr = 0.01
I0522 22:35:27.754843 35003 solver.cpp:239] Iteration 81930 (2.29017 iter/s, 4.36649s/10 iters), loss = 6.9098
I0522 22:35:27.754892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9098 (* 1 = 6.9098 loss)
I0522 22:35:27.759431 35003 sgd_solver.cpp:112] Iteration 81930, lr = 0.01
I0522 22:35:32.762257 35003 solver.cpp:239] Iteration 81940 (1.99714 iter/s, 5.00716s/10 iters), loss = 7.38019
I0522 22:35:32.762308 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38019 (* 1 = 7.38019 loss)
I0522 22:35:32.775905 35003 sgd_solver.cpp:112] Iteration 81940, lr = 0.01
I0522 22:35:35.660245 35003 solver.cpp:239] Iteration 81950 (3.45087 iter/s, 2.89782s/10 iters), loss = 8.08684
I0522 22:35:35.660285 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08684 (* 1 = 8.08684 loss)
I0522 22:35:35.667227 35003 sgd_solver.cpp:112] Iteration 81950, lr = 0.01
I0522 22:35:39.300369 35003 solver.cpp:239] Iteration 81960 (2.74731 iter/s, 3.63993s/10 iters), loss = 6.93501
I0522 22:35:39.300411 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93501 (* 1 = 6.93501 loss)
I0522 22:35:39.312695 35003 sgd_solver.cpp:112] Iteration 81960, lr = 0.01
I0522 22:35:43.309763 35003 solver.cpp:239] Iteration 81970 (2.49428 iter/s, 4.00918s/10 iters), loss = 6.9845
I0522 22:35:43.309806 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9845 (* 1 = 6.9845 loss)
I0522 22:35:43.313390 35003 sgd_solver.cpp:112] Iteration 81970, lr = 0.01
I0522 22:35:47.724609 35003 solver.cpp:239] Iteration 81980 (2.26523 iter/s, 4.41457s/10 iters), loss = 7.72767
I0522 22:35:47.724671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72767 (* 1 = 7.72767 loss)
I0522 22:35:47.741158 35003 sgd_solver.cpp:112] Iteration 81980, lr = 0.01
I0522 22:35:50.286224 35003 solver.cpp:239] Iteration 81990 (3.90404 iter/s, 2.56145s/10 iters), loss = 7.13926
I0522 22:35:50.286278 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13926 (* 1 = 7.13926 loss)
I0522 22:35:50.641798 35003 sgd_solver.cpp:112] Iteration 81990, lr = 0.01
I0522 22:35:54.529781 35003 solver.cpp:239] Iteration 82000 (2.35664 iter/s, 4.24333s/10 iters), loss = 7.62231
I0522 22:35:54.530091 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62231 (* 1 = 7.62231 loss)
I0522 22:35:54.541827 35003 sgd_solver.cpp:112] Iteration 82000, lr = 0.01
I0522 22:35:58.130738 35003 solver.cpp:239] Iteration 82010 (2.77737 iter/s, 3.60053s/10 iters), loss = 7.86758
I0522 22:35:58.130784 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86758 (* 1 = 7.86758 loss)
I0522 22:35:58.313807 35003 sgd_solver.cpp:112] Iteration 82010, lr = 0.01
I0522 22:35:59.877243 35003 solver.cpp:239] Iteration 82020 (5.72614 iter/s, 1.74638s/10 iters), loss = 8.03452
I0522 22:35:59.877295 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03452 (* 1 = 8.03452 loss)
I0522 22:36:00.205081 35003 sgd_solver.cpp:112] Iteration 82020, lr = 0.01
I0522 22:36:03.781988 35003 solver.cpp:239] Iteration 82030 (2.56113 iter/s, 3.90453s/10 iters), loss = 7.0071
I0522 22:36:03.782042 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0071 (* 1 = 7.0071 loss)
I0522 22:36:04.496809 35003 sgd_solver.cpp:112] Iteration 82030, lr = 0.01
I0522 22:36:08.081063 35003 solver.cpp:239] Iteration 82040 (2.32621 iter/s, 4.29885s/10 iters), loss = 7.20526
I0522 22:36:08.081111 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20526 (* 1 = 7.20526 loss)
I0522 22:36:08.092516 35003 sgd_solver.cpp:112] Iteration 82040, lr = 0.01
I0522 22:36:10.900887 35003 solver.cpp:239] Iteration 82050 (3.54654 iter/s, 2.81965s/10 iters), loss = 7.61187
I0522 22:36:10.900930 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61187 (* 1 = 7.61187 loss)
I0522 22:36:10.913532 35003 sgd_solver.cpp:112] Iteration 82050, lr = 0.01
I0522 22:36:12.673266 35003 solver.cpp:239] Iteration 82060 (5.65636 iter/s, 1.76792s/10 iters), loss = 7.83874
I0522 22:36:12.673310 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83874 (* 1 = 7.83874 loss)
I0522 22:36:13.408830 35003 sgd_solver.cpp:112] Iteration 82060, lr = 0.01
I0522 22:36:16.641077 35003 solver.cpp:239] Iteration 82070 (2.52042 iter/s, 3.9676s/10 iters), loss = 7.43839
I0522 22:36:16.641135 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43839 (* 1 = 7.43839 loss)
I0522 22:36:16.728528 35003 sgd_solver.cpp:112] Iteration 82070, lr = 0.01
I0522 22:36:19.531251 35003 solver.cpp:239] Iteration 82080 (3.46021 iter/s, 2.89s/10 iters), loss = 7.26619
I0522 22:36:19.531289 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26619 (* 1 = 7.26619 loss)
I0522 22:36:19.654716 35003 sgd_solver.cpp:112] Iteration 82080, lr = 0.01
I0522 22:36:24.082651 35003 solver.cpp:239] Iteration 82090 (2.19723 iter/s, 4.55118s/10 iters), loss = 6.79388
I0522 22:36:24.082707 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79388 (* 1 = 6.79388 loss)
I0522 22:36:24.089177 35003 sgd_solver.cpp:112] Iteration 82090, lr = 0.01
I0522 22:36:26.010020 35003 solver.cpp:239] Iteration 82100 (5.18878 iter/s, 1.92723s/10 iters), loss = 7.24083
I0522 22:36:26.010259 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24083 (* 1 = 7.24083 loss)
I0522 22:36:26.016006 35003 sgd_solver.cpp:112] Iteration 82100, lr = 0.01
I0522 22:36:28.927830 35003 solver.cpp:239] Iteration 82110 (3.42762 iter/s, 2.91747s/10 iters), loss = 7.87808
I0522 22:36:28.927894 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87808 (* 1 = 7.87808 loss)
I0522 22:36:29.623560 35003 sgd_solver.cpp:112] Iteration 82110, lr = 0.01
I0522 22:36:32.991413 35003 solver.cpp:239] Iteration 82120 (2.46102 iter/s, 4.06335s/10 iters), loss = 6.84075
I0522 22:36:32.991461 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84075 (* 1 = 6.84075 loss)
I0522 22:36:33.699651 35003 sgd_solver.cpp:112] Iteration 82120, lr = 0.01
I0522 22:36:37.394006 35003 solver.cpp:239] Iteration 82130 (2.27151 iter/s, 4.40236s/10 iters), loss = 5.98238
I0522 22:36:37.394053 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98238 (* 1 = 5.98238 loss)
I0522 22:36:37.407174 35003 sgd_solver.cpp:112] Iteration 82130, lr = 0.01
I0522 22:36:40.644508 35003 solver.cpp:239] Iteration 82140 (3.07662 iter/s, 3.25032s/10 iters), loss = 7.52086
I0522 22:36:40.644556 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52086 (* 1 = 7.52086 loss)
I0522 22:36:40.651582 35003 sgd_solver.cpp:112] Iteration 82140, lr = 0.01
I0522 22:36:45.608290 35003 solver.cpp:239] Iteration 82150 (2.01469 iter/s, 4.96353s/10 iters), loss = 6.97704
I0522 22:36:45.608330 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97704 (* 1 = 6.97704 loss)
I0522 22:36:45.614100 35003 sgd_solver.cpp:112] Iteration 82150, lr = 0.01
I0522 22:36:49.266582 35003 solver.cpp:239] Iteration 82160 (2.73367 iter/s, 3.65809s/10 iters), loss = 7.76525
I0522 22:36:49.266633 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76525 (* 1 = 7.76525 loss)
I0522 22:36:49.885197 35003 sgd_solver.cpp:112] Iteration 82160, lr = 0.01
I0522 22:36:53.013651 35003 solver.cpp:239] Iteration 82170 (2.6689 iter/s, 3.74686s/10 iters), loss = 7.85381
I0522 22:36:53.013696 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85381 (* 1 = 7.85381 loss)
I0522 22:36:53.034725 35003 sgd_solver.cpp:112] Iteration 82170, lr = 0.01
I0522 22:36:57.450409 35003 solver.cpp:239] Iteration 82180 (2.25401 iter/s, 4.43653s/10 iters), loss = 7.68744
I0522 22:36:57.450635 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68744 (* 1 = 7.68744 loss)
I0522 22:36:57.456362 35003 sgd_solver.cpp:112] Iteration 82180, lr = 0.01
I0522 22:37:01.011508 35003 solver.cpp:239] Iteration 82190 (2.80838 iter/s, 3.56077s/10 iters), loss = 7.50428
I0522 22:37:01.011554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50428 (* 1 = 7.50428 loss)
I0522 22:37:01.706454 35003 sgd_solver.cpp:112] Iteration 82190, lr = 0.01
I0522 22:37:05.478765 35003 solver.cpp:239] Iteration 82200 (2.23863 iter/s, 4.46703s/10 iters), loss = 6.95642
I0522 22:37:05.478804 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95642 (* 1 = 6.95642 loss)
I0522 22:37:05.492214 35003 sgd_solver.cpp:112] Iteration 82200, lr = 0.01
I0522 22:37:08.317489 35003 solver.cpp:239] Iteration 82210 (3.52291 iter/s, 2.83856s/10 iters), loss = 8.70781
I0522 22:37:08.317531 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.70781 (* 1 = 8.70781 loss)
I0522 22:37:08.322532 35003 sgd_solver.cpp:112] Iteration 82210, lr = 0.01
I0522 22:37:14.203387 35003 solver.cpp:239] Iteration 82220 (1.69906 iter/s, 5.88561s/10 iters), loss = 8.0673
I0522 22:37:14.203438 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0673 (* 1 = 8.0673 loss)
I0522 22:37:14.229173 35003 sgd_solver.cpp:112] Iteration 82220, lr = 0.01
I0522 22:37:17.211074 35003 solver.cpp:239] Iteration 82230 (3.32501 iter/s, 3.00751s/10 iters), loss = 7.13489
I0522 22:37:17.211118 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13489 (* 1 = 7.13489 loss)
I0522 22:37:17.229449 35003 sgd_solver.cpp:112] Iteration 82230, lr = 0.01
I0522 22:37:20.981652 35003 solver.cpp:239] Iteration 82240 (2.65226 iter/s, 3.77037s/10 iters), loss = 8.63048
I0522 22:37:20.981696 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.63048 (* 1 = 8.63048 loss)
I0522 22:37:21.328346 35003 sgd_solver.cpp:112] Iteration 82240, lr = 0.01
I0522 22:37:24.943828 35003 solver.cpp:239] Iteration 82250 (2.524 iter/s, 3.96197s/10 iters), loss = 7.11691
I0522 22:37:24.943873 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11691 (* 1 = 7.11691 loss)
I0522 22:37:25.662454 35003 sgd_solver.cpp:112] Iteration 82250, lr = 0.01
I0522 22:37:29.352766 35003 solver.cpp:239] Iteration 82260 (2.26824 iter/s, 4.40871s/10 iters), loss = 8.00977
I0522 22:37:29.353108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00977 (* 1 = 8.00977 loss)
I0522 22:37:29.730067 35003 sgd_solver.cpp:112] Iteration 82260, lr = 0.01
I0522 22:37:34.634390 35003 solver.cpp:239] Iteration 82270 (1.89355 iter/s, 5.2811s/10 iters), loss = 7.88051
I0522 22:37:34.634446 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88051 (* 1 = 7.88051 loss)
I0522 22:37:34.640257 35003 sgd_solver.cpp:112] Iteration 82270, lr = 0.01
I0522 22:37:38.562551 35003 solver.cpp:239] Iteration 82280 (2.54586 iter/s, 3.92794s/10 iters), loss = 8.47919
I0522 22:37:38.562593 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.47919 (* 1 = 8.47919 loss)
I0522 22:37:38.576215 35003 sgd_solver.cpp:112] Iteration 82280, lr = 0.01
I0522 22:37:42.491806 35003 solver.cpp:239] Iteration 82290 (2.54514 iter/s, 3.92905s/10 iters), loss = 6.922
I0522 22:37:42.491858 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.922 (* 1 = 6.922 loss)
I0522 22:37:42.504487 35003 sgd_solver.cpp:112] Iteration 82290, lr = 0.01
I0522 22:37:44.990417 35003 solver.cpp:239] Iteration 82300 (4.00247 iter/s, 2.49846s/10 iters), loss = 6.15349
I0522 22:37:44.990458 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15349 (* 1 = 6.15349 loss)
I0522 22:37:45.724313 35003 sgd_solver.cpp:112] Iteration 82300, lr = 0.01
I0522 22:37:49.199681 35003 solver.cpp:239] Iteration 82310 (2.37583 iter/s, 4.20905s/10 iters), loss = 7.15403
I0522 22:37:49.199734 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15403 (* 1 = 7.15403 loss)
I0522 22:37:49.212707 35003 sgd_solver.cpp:112] Iteration 82310, lr = 0.01
I0522 22:37:53.634160 35003 solver.cpp:239] Iteration 82320 (2.25517 iter/s, 4.43425s/10 iters), loss = 8.1472
I0522 22:37:53.634207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.1472 (* 1 = 8.1472 loss)
I0522 22:37:53.647653 35003 sgd_solver.cpp:112] Iteration 82320, lr = 0.01
I0522 22:37:56.497684 35003 solver.cpp:239] Iteration 82330 (3.49242 iter/s, 2.86334s/10 iters), loss = 7.3549
I0522 22:37:56.497752 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3549 (* 1 = 7.3549 loss)
I0522 22:37:56.498380 35003 sgd_solver.cpp:112] Iteration 82330, lr = 0.01
I0522 22:38:00.033936 35003 solver.cpp:239] Iteration 82340 (2.82803 iter/s, 3.53603s/10 iters), loss = 6.69719
I0522 22:38:00.034148 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69719 (* 1 = 6.69719 loss)
I0522 22:38:00.046912 35003 sgd_solver.cpp:112] Iteration 82340, lr = 0.01
I0522 22:38:03.590832 35003 solver.cpp:239] Iteration 82350 (2.8117 iter/s, 3.55656s/10 iters), loss = 7.21986
I0522 22:38:03.590888 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21986 (* 1 = 7.21986 loss)
I0522 22:38:04.194283 35003 sgd_solver.cpp:112] Iteration 82350, lr = 0.01
I0522 22:38:07.715955 35003 solver.cpp:239] Iteration 82360 (2.4243 iter/s, 4.1249s/10 iters), loss = 7.89977
I0522 22:38:07.715999 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89977 (* 1 = 7.89977 loss)
I0522 22:38:08.176597 35003 sgd_solver.cpp:112] Iteration 82360, lr = 0.01
I0522 22:38:10.359246 35003 solver.cpp:239] Iteration 82370 (3.7834 iter/s, 2.64313s/10 iters), loss = 7.14585
I0522 22:38:10.359292 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14585 (* 1 = 7.14585 loss)
I0522 22:38:10.369185 35003 sgd_solver.cpp:112] Iteration 82370, lr = 0.01
I0522 22:38:12.928972 35003 solver.cpp:239] Iteration 82380 (3.8917 iter/s, 2.56957s/10 iters), loss = 6.97087
I0522 22:38:12.929019 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97087 (* 1 = 6.97087 loss)
I0522 22:38:13.656545 35003 sgd_solver.cpp:112] Iteration 82380, lr = 0.01
I0522 22:38:16.366833 35003 solver.cpp:239] Iteration 82390 (2.90896 iter/s, 3.43766s/10 iters), loss = 7.81863
I0522 22:38:16.366888 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81863 (* 1 = 7.81863 loss)
I0522 22:38:16.561803 35003 sgd_solver.cpp:112] Iteration 82390, lr = 0.01
I0522 22:38:20.472275 35003 solver.cpp:239] Iteration 82400 (2.43593 iter/s, 4.1052s/10 iters), loss = 8.36857
I0522 22:38:20.472316 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.36857 (* 1 = 8.36857 loss)
I0522 22:38:20.485788 35003 sgd_solver.cpp:112] Iteration 82400, lr = 0.01
I0522 22:38:23.201275 35003 solver.cpp:239] Iteration 82410 (3.66455 iter/s, 2.72885s/10 iters), loss = 7.03085
I0522 22:38:23.201326 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03085 (* 1 = 7.03085 loss)
I0522 22:38:23.916389 35003 sgd_solver.cpp:112] Iteration 82410, lr = 0.01
I0522 22:38:25.457720 35003 solver.cpp:239] Iteration 82420 (4.4321 iter/s, 2.25627s/10 iters), loss = 7.13609
I0522 22:38:25.457764 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13609 (* 1 = 7.13609 loss)
I0522 22:38:25.468894 35003 sgd_solver.cpp:112] Iteration 82420, lr = 0.01
I0522 22:38:30.071756 35003 solver.cpp:239] Iteration 82430 (2.16741 iter/s, 4.6138s/10 iters), loss = 6.90824
I0522 22:38:30.072062 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90824 (* 1 = 6.90824 loss)
I0522 22:38:30.803936 35003 sgd_solver.cpp:112] Iteration 82430, lr = 0.01
I0522 22:38:33.598508 35003 solver.cpp:239] Iteration 82440 (2.83581 iter/s, 3.52633s/10 iters), loss = 7.51304
I0522 22:38:33.598564 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51304 (* 1 = 7.51304 loss)
I0522 22:38:33.623476 35003 sgd_solver.cpp:112] Iteration 82440, lr = 0.01
I0522 22:38:36.651223 35003 solver.cpp:239] Iteration 82450 (3.27597 iter/s, 3.05253s/10 iters), loss = 8.32217
I0522 22:38:36.651271 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.32217 (* 1 = 8.32217 loss)
I0522 22:38:36.660591 35003 sgd_solver.cpp:112] Iteration 82450, lr = 0.01
I0522 22:38:41.722410 35003 solver.cpp:239] Iteration 82460 (1.97267 iter/s, 5.06927s/10 iters), loss = 7.70638
I0522 22:38:41.722457 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70638 (* 1 = 7.70638 loss)
I0522 22:38:42.461165 35003 sgd_solver.cpp:112] Iteration 82460, lr = 0.01
I0522 22:38:45.848073 35003 solver.cpp:239] Iteration 82470 (2.42399 iter/s, 4.12544s/10 iters), loss = 7.3374
I0522 22:38:45.848135 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3374 (* 1 = 7.3374 loss)
I0522 22:38:45.857126 35003 sgd_solver.cpp:112] Iteration 82470, lr = 0.01
I0522 22:38:50.841003 35003 solver.cpp:239] Iteration 82480 (2.00294 iter/s, 4.99266s/10 iters), loss = 6.77554
I0522 22:38:50.841050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77554 (* 1 = 6.77554 loss)
I0522 22:38:50.863564 35003 sgd_solver.cpp:112] Iteration 82480, lr = 0.01
I0522 22:38:54.462386 35003 solver.cpp:239] Iteration 82490 (2.76153 iter/s, 3.62118s/10 iters), loss = 6.21003
I0522 22:38:54.462437 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21003 (* 1 = 6.21003 loss)
I0522 22:38:54.480880 35003 sgd_solver.cpp:112] Iteration 82490, lr = 0.01
I0522 22:38:57.345062 35003 solver.cpp:239] Iteration 82500 (3.46921 iter/s, 2.8825s/10 iters), loss = 7.90978
I0522 22:38:57.345111 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90978 (* 1 = 7.90978 loss)
I0522 22:38:57.358781 35003 sgd_solver.cpp:112] Iteration 82500, lr = 0.01
I0522 22:39:00.298140 35003 solver.cpp:239] Iteration 82510 (3.3865 iter/s, 2.9529s/10 iters), loss = 6.89762
I0522 22:39:00.298501 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89762 (* 1 = 6.89762 loss)
I0522 22:39:00.306188 35003 sgd_solver.cpp:112] Iteration 82510, lr = 0.01
I0522 22:39:03.889869 35003 solver.cpp:239] Iteration 82520 (2.78456 iter/s, 3.59123s/10 iters), loss = 7.93854
I0522 22:39:03.889951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93854 (* 1 = 7.93854 loss)
I0522 22:39:03.902547 35003 sgd_solver.cpp:112] Iteration 82520, lr = 0.01
I0522 22:39:06.094241 35003 solver.cpp:239] Iteration 82530 (4.546 iter/s, 2.19974s/10 iters), loss = 7.68856
I0522 22:39:06.094293 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68856 (* 1 = 7.68856 loss)
I0522 22:39:06.821677 35003 sgd_solver.cpp:112] Iteration 82530, lr = 0.01
I0522 22:39:08.369609 35003 solver.cpp:239] Iteration 82540 (4.39518 iter/s, 2.27522s/10 iters), loss = 8.758
I0522 22:39:08.369655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.758 (* 1 = 8.758 loss)
I0522 22:39:08.383028 35003 sgd_solver.cpp:112] Iteration 82540, lr = 0.01
I0522 22:39:12.718601 35003 solver.cpp:239] Iteration 82550 (2.2995 iter/s, 4.34877s/10 iters), loss = 7.25465
I0522 22:39:12.718641 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25465 (* 1 = 7.25465 loss)
I0522 22:39:12.731540 35003 sgd_solver.cpp:112] Iteration 82550, lr = 0.01
I0522 22:39:15.778496 35003 solver.cpp:239] Iteration 82560 (3.26827 iter/s, 3.05972s/10 iters), loss = 7.46888
I0522 22:39:15.778544 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46888 (* 1 = 7.46888 loss)
I0522 22:39:15.791769 35003 sgd_solver.cpp:112] Iteration 82560, lr = 0.01
I0522 22:39:18.749064 35003 solver.cpp:239] Iteration 82570 (3.36655 iter/s, 2.9704s/10 iters), loss = 6.74366
I0522 22:39:18.749105 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74366 (* 1 = 6.74366 loss)
I0522 22:39:19.484102 35003 sgd_solver.cpp:112] Iteration 82570, lr = 0.01
I0522 22:39:23.898948 35003 solver.cpp:239] Iteration 82580 (1.94189 iter/s, 5.14963s/10 iters), loss = 7.02961
I0522 22:39:23.898994 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02961 (* 1 = 7.02961 loss)
I0522 22:39:23.911190 35003 sgd_solver.cpp:112] Iteration 82580, lr = 0.01
I0522 22:39:27.441642 35003 solver.cpp:239] Iteration 82590 (2.82287 iter/s, 3.5425s/10 iters), loss = 6.51418
I0522 22:39:27.441689 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51418 (* 1 = 6.51418 loss)
I0522 22:39:28.149775 35003 sgd_solver.cpp:112] Iteration 82590, lr = 0.01
I0522 22:39:30.896023 35003 solver.cpp:239] Iteration 82600 (2.89504 iter/s, 3.45419s/10 iters), loss = 8.34795
I0522 22:39:30.896257 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.34795 (* 1 = 8.34795 loss)
I0522 22:39:30.908983 35003 sgd_solver.cpp:112] Iteration 82600, lr = 0.01
I0522 22:39:33.744760 35003 solver.cpp:239] Iteration 82610 (3.51072 iter/s, 2.84841s/10 iters), loss = 6.37233
I0522 22:39:33.744802 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37233 (* 1 = 6.37233 loss)
I0522 22:39:33.748808 35003 sgd_solver.cpp:112] Iteration 82610, lr = 0.01
I0522 22:39:36.442009 35003 solver.cpp:239] Iteration 82620 (3.7077 iter/s, 2.69709s/10 iters), loss = 8.1069
I0522 22:39:36.442046 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.1069 (* 1 = 8.1069 loss)
I0522 22:39:36.455027 35003 sgd_solver.cpp:112] Iteration 82620, lr = 0.01
I0522 22:39:39.889035 35003 solver.cpp:239] Iteration 82630 (2.90122 iter/s, 3.44683s/10 iters), loss = 7.54397
I0522 22:39:39.889111 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54397 (* 1 = 7.54397 loss)
I0522 22:39:39.901870 35003 sgd_solver.cpp:112] Iteration 82630, lr = 0.01
I0522 22:39:43.340030 35003 solver.cpp:239] Iteration 82640 (2.8979 iter/s, 3.45078s/10 iters), loss = 7.72469
I0522 22:39:43.340080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72469 (* 1 = 7.72469 loss)
I0522 22:39:43.353065 35003 sgd_solver.cpp:112] Iteration 82640, lr = 0.01
I0522 22:39:46.905401 35003 solver.cpp:239] Iteration 82650 (2.80492 iter/s, 3.56517s/10 iters), loss = 6.47662
I0522 22:39:46.905457 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47662 (* 1 = 6.47662 loss)
I0522 22:39:46.911923 35003 sgd_solver.cpp:112] Iteration 82650, lr = 0.01
I0522 22:39:50.308851 35003 solver.cpp:239] Iteration 82660 (2.94049 iter/s, 3.4008s/10 iters), loss = 7.88927
I0522 22:39:50.308905 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88927 (* 1 = 7.88927 loss)
I0522 22:39:51.025779 35003 sgd_solver.cpp:112] Iteration 82660, lr = 0.01
I0522 22:39:54.791375 35003 solver.cpp:239] Iteration 82670 (2.23101 iter/s, 4.48228s/10 iters), loss = 7.64633
I0522 22:39:54.791424 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64633 (* 1 = 7.64633 loss)
I0522 22:39:54.838038 35003 sgd_solver.cpp:112] Iteration 82670, lr = 0.01
I0522 22:39:59.114943 35003 solver.cpp:239] Iteration 82680 (2.31303 iter/s, 4.32334s/10 iters), loss = 7.7298
I0522 22:39:59.114992 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7298 (* 1 = 7.7298 loss)
I0522 22:39:59.803148 35003 sgd_solver.cpp:112] Iteration 82680, lr = 0.01
I0522 22:40:01.826481 35003 solver.cpp:239] Iteration 82690 (3.68817 iter/s, 2.71137s/10 iters), loss = 6.3725
I0522 22:40:01.826752 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3725 (* 1 = 6.3725 loss)
I0522 22:40:01.855474 35003 sgd_solver.cpp:112] Iteration 82690, lr = 0.01
I0522 22:40:05.410688 35003 solver.cpp:239] Iteration 82700 (2.79032 iter/s, 3.58381s/10 iters), loss = 8.1432
I0522 22:40:05.410748 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.1432 (* 1 = 8.1432 loss)
I0522 22:40:05.723796 35003 sgd_solver.cpp:112] Iteration 82700, lr = 0.01
I0522 22:40:09.284891 35003 solver.cpp:239] Iteration 82710 (2.58133 iter/s, 3.87398s/10 iters), loss = 7.27673
I0522 22:40:09.284943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27673 (* 1 = 7.27673 loss)
I0522 22:40:10.018909 35003 sgd_solver.cpp:112] Iteration 82710, lr = 0.01
I0522 22:40:12.861438 35003 solver.cpp:239] Iteration 82720 (2.79616 iter/s, 3.57633s/10 iters), loss = 7.89959
I0522 22:40:12.861491 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89959 (* 1 = 7.89959 loss)
I0522 22:40:13.582559 35003 sgd_solver.cpp:112] Iteration 82720, lr = 0.01
I0522 22:40:16.372259 35003 solver.cpp:239] Iteration 82730 (2.84853 iter/s, 3.51058s/10 iters), loss = 6.60945
I0522 22:40:16.372323 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60945 (* 1 = 6.60945 loss)
I0522 22:40:17.106633 35003 sgd_solver.cpp:112] Iteration 82730, lr = 0.01
I0522 22:40:19.950968 35003 solver.cpp:239] Iteration 82740 (2.79447 iter/s, 3.5785s/10 iters), loss = 7.12553
I0522 22:40:19.951015 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12553 (* 1 = 7.12553 loss)
I0522 22:40:20.671939 35003 sgd_solver.cpp:112] Iteration 82740, lr = 0.01
I0522 22:40:23.403617 35003 solver.cpp:239] Iteration 82750 (2.89648 iter/s, 3.45246s/10 iters), loss = 7.28026
I0522 22:40:23.403663 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28026 (* 1 = 7.28026 loss)
I0522 22:40:23.539384 35003 sgd_solver.cpp:112] Iteration 82750, lr = 0.01
I0522 22:40:27.144837 35003 solver.cpp:239] Iteration 82760 (2.67307 iter/s, 3.74102s/10 iters), loss = 8.17393
I0522 22:40:27.144891 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17393 (* 1 = 8.17393 loss)
I0522 22:40:27.155511 35003 sgd_solver.cpp:112] Iteration 82760, lr = 0.01
I0522 22:40:29.103240 35003 solver.cpp:239] Iteration 82770 (5.10656 iter/s, 1.95826s/10 iters), loss = 8.02284
I0522 22:40:29.103286 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02284 (* 1 = 8.02284 loss)
I0522 22:40:29.109828 35003 sgd_solver.cpp:112] Iteration 82770, lr = 0.01
I0522 22:40:32.032313 35003 solver.cpp:239] Iteration 82780 (3.41425 iter/s, 2.9289s/10 iters), loss = 6.79852
I0522 22:40:32.032503 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79852 (* 1 = 6.79852 loss)
I0522 22:40:32.772822 35003 sgd_solver.cpp:112] Iteration 82780, lr = 0.01
I0522 22:40:36.497795 35003 solver.cpp:239] Iteration 82790 (2.23958 iter/s, 4.46512s/10 iters), loss = 7.42011
I0522 22:40:36.497838 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42011 (* 1 = 7.42011 loss)
I0522 22:40:36.507812 35003 sgd_solver.cpp:112] Iteration 82790, lr = 0.01
I0522 22:40:40.424433 35003 solver.cpp:239] Iteration 82800 (2.54684 iter/s, 3.92644s/10 iters), loss = 6.51774
I0522 22:40:40.424474 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51774 (* 1 = 6.51774 loss)
I0522 22:40:40.449661 35003 sgd_solver.cpp:112] Iteration 82800, lr = 0.01
I0522 22:40:42.879825 35003 solver.cpp:239] Iteration 82810 (4.07291 iter/s, 2.45524s/10 iters), loss = 6.19203
I0522 22:40:42.879869 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19203 (* 1 = 6.19203 loss)
I0522 22:40:42.884651 35003 sgd_solver.cpp:112] Iteration 82810, lr = 0.01
I0522 22:40:45.733078 35003 solver.cpp:239] Iteration 82820 (3.50498 iter/s, 2.85308s/10 iters), loss = 8.04677
I0522 22:40:45.733121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04677 (* 1 = 8.04677 loss)
I0522 22:40:45.736999 35003 sgd_solver.cpp:112] Iteration 82820, lr = 0.01
I0522 22:40:49.249590 35003 solver.cpp:239] Iteration 82830 (2.84388 iter/s, 3.51632s/10 iters), loss = 9.0657
I0522 22:40:49.249639 35003 solver.cpp:258]     Train net output #0: softmax_loss = 9.0657 (* 1 = 9.0657 loss)
I0522 22:40:49.990890 35003 sgd_solver.cpp:112] Iteration 82830, lr = 0.01
I0522 22:40:53.150728 35003 solver.cpp:239] Iteration 82840 (2.56351 iter/s, 3.9009s/10 iters), loss = 8.69375
I0522 22:40:53.150777 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.69375 (* 1 = 8.69375 loss)
I0522 22:40:53.892180 35003 sgd_solver.cpp:112] Iteration 82840, lr = 0.01
I0522 22:40:57.372937 35003 solver.cpp:239] Iteration 82850 (2.36856 iter/s, 4.22198s/10 iters), loss = 7.44405
I0522 22:40:57.372984 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44405 (* 1 = 7.44405 loss)
I0522 22:40:57.386234 35003 sgd_solver.cpp:112] Iteration 82850, lr = 0.01
I0522 22:41:00.807190 35003 solver.cpp:239] Iteration 82860 (2.912 iter/s, 3.43406s/10 iters), loss = 7.64032
I0522 22:41:00.807240 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64032 (* 1 = 7.64032 loss)
I0522 22:41:00.879806 35003 sgd_solver.cpp:112] Iteration 82860, lr = 0.01
I0522 22:41:04.983376 35003 solver.cpp:239] Iteration 82870 (2.39466 iter/s, 4.17596s/10 iters), loss = 7.2517
I0522 22:41:04.983614 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2517 (* 1 = 7.2517 loss)
I0522 22:41:04.996853 35003 sgd_solver.cpp:112] Iteration 82870, lr = 0.01
I0522 22:41:08.545435 35003 solver.cpp:239] Iteration 82880 (2.80766 iter/s, 3.56168s/10 iters), loss = 7.31898
I0522 22:41:08.545511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31898 (* 1 = 7.31898 loss)
I0522 22:41:09.260447 35003 sgd_solver.cpp:112] Iteration 82880, lr = 0.01
I0522 22:41:13.604774 35003 solver.cpp:239] Iteration 82890 (1.97665 iter/s, 5.05906s/10 iters), loss = 7.16444
I0522 22:41:13.604815 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16444 (* 1 = 7.16444 loss)
I0522 22:41:13.644919 35003 sgd_solver.cpp:112] Iteration 82890, lr = 0.01
I0522 22:41:16.471297 35003 solver.cpp:239] Iteration 82900 (3.48876 iter/s, 2.86635s/10 iters), loss = 6.64108
I0522 22:41:16.471344 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64108 (* 1 = 6.64108 loss)
I0522 22:41:16.475805 35003 sgd_solver.cpp:112] Iteration 82900, lr = 0.01
I0522 22:41:20.014919 35003 solver.cpp:239] Iteration 82910 (2.82213 iter/s, 3.54342s/10 iters), loss = 6.70945
I0522 22:41:20.014987 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70945 (* 1 = 6.70945 loss)
I0522 22:41:20.703438 35003 sgd_solver.cpp:112] Iteration 82910, lr = 0.01
I0522 22:41:24.969583 35003 solver.cpp:239] Iteration 82920 (2.01841 iter/s, 4.95439s/10 iters), loss = 6.12896
I0522 22:41:24.969637 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12896 (* 1 = 6.12896 loss)
I0522 22:41:24.976307 35003 sgd_solver.cpp:112] Iteration 82920, lr = 0.01
I0522 22:41:28.788796 35003 solver.cpp:239] Iteration 82930 (2.61849 iter/s, 3.81899s/10 iters), loss = 6.85224
I0522 22:41:28.788846 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85224 (* 1 = 6.85224 loss)
I0522 22:41:29.427139 35003 sgd_solver.cpp:112] Iteration 82930, lr = 0.01
I0522 22:41:32.373812 35003 solver.cpp:239] Iteration 82940 (2.78955 iter/s, 3.58481s/10 iters), loss = 7.97963
I0522 22:41:32.373860 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97963 (* 1 = 7.97963 loss)
I0522 22:41:32.383821 35003 sgd_solver.cpp:112] Iteration 82940, lr = 0.01
I0522 22:41:35.367416 35003 solver.cpp:239] Iteration 82950 (3.34066 iter/s, 2.99342s/10 iters), loss = 7.11345
I0522 22:41:35.367563 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11345 (* 1 = 7.11345 loss)
I0522 22:41:35.391444 35003 sgd_solver.cpp:112] Iteration 82950, lr = 0.01
I0522 22:41:36.674682 35003 solver.cpp:239] Iteration 82960 (7.65076 iter/s, 1.30706s/10 iters), loss = 7.40632
I0522 22:41:36.674748 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40632 (* 1 = 7.40632 loss)
I0522 22:41:36.684435 35003 sgd_solver.cpp:112] Iteration 82960, lr = 0.01
I0522 22:41:40.960544 35003 solver.cpp:239] Iteration 82970 (2.33339 iter/s, 4.28561s/10 iters), loss = 7.27015
I0522 22:41:40.960599 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27015 (* 1 = 7.27015 loss)
I0522 22:41:40.962847 35003 sgd_solver.cpp:112] Iteration 82970, lr = 0.01
I0522 22:41:44.127068 35003 solver.cpp:239] Iteration 82980 (3.15823 iter/s, 3.16633s/10 iters), loss = 7.88864
I0522 22:41:44.127115 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88864 (* 1 = 7.88864 loss)
I0522 22:41:44.130347 35003 sgd_solver.cpp:112] Iteration 82980, lr = 0.01
I0522 22:41:46.994597 35003 solver.cpp:239] Iteration 82990 (3.48753 iter/s, 2.86736s/10 iters), loss = 7.43054
I0522 22:41:46.994642 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43054 (* 1 = 7.43054 loss)
I0522 22:41:46.998342 35003 sgd_solver.cpp:112] Iteration 82990, lr = 0.01
I0522 22:41:50.553603 35003 solver.cpp:239] Iteration 83000 (2.80993 iter/s, 3.55881s/10 iters), loss = 7.10718
I0522 22:41:50.553654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10718 (* 1 = 7.10718 loss)
I0522 22:41:51.268862 35003 sgd_solver.cpp:112] Iteration 83000, lr = 0.01
I0522 22:41:54.868618 35003 solver.cpp:239] Iteration 83010 (2.31762 iter/s, 4.31476s/10 iters), loss = 7.61408
I0522 22:41:54.868660 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61408 (* 1 = 7.61408 loss)
I0522 22:41:54.890686 35003 sgd_solver.cpp:112] Iteration 83010, lr = 0.01
I0522 22:41:59.519569 35003 solver.cpp:239] Iteration 83020 (2.1502 iter/s, 4.65072s/10 iters), loss = 6.61145
I0522 22:41:59.519614 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61145 (* 1 = 6.61145 loss)
I0522 22:41:59.736426 35003 sgd_solver.cpp:112] Iteration 83020, lr = 0.01
I0522 22:42:04.846940 35003 solver.cpp:239] Iteration 83030 (1.87719 iter/s, 5.32711s/10 iters), loss = 7.1896
I0522 22:42:04.846990 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1896 (* 1 = 7.1896 loss)
I0522 22:42:05.449095 35003 sgd_solver.cpp:112] Iteration 83030, lr = 0.01
I0522 22:42:07.549870 35003 solver.cpp:239] Iteration 83040 (3.69992 iter/s, 2.70276s/10 iters), loss = 7.21173
I0522 22:42:07.549927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21173 (* 1 = 7.21173 loss)
I0522 22:42:08.290900 35003 sgd_solver.cpp:112] Iteration 83040, lr = 0.01
I0522 22:42:12.028159 35003 solver.cpp:239] Iteration 83050 (2.23311 iter/s, 4.47805s/10 iters), loss = 8.37482
I0522 22:42:12.028209 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.37482 (* 1 = 8.37482 loss)
I0522 22:42:12.756096 35003 sgd_solver.cpp:112] Iteration 83050, lr = 0.01
I0522 22:42:16.782649 35003 solver.cpp:239] Iteration 83060 (2.10339 iter/s, 4.75424s/10 iters), loss = 7.50469
I0522 22:42:16.782724 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50469 (* 1 = 7.50469 loss)
I0522 22:42:16.787468 35003 sgd_solver.cpp:112] Iteration 83060, lr = 0.01
I0522 22:42:20.126312 35003 solver.cpp:239] Iteration 83070 (2.99092 iter/s, 3.34345s/10 iters), loss = 6.97215
I0522 22:42:20.126354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97215 (* 1 = 6.97215 loss)
I0522 22:42:20.140254 35003 sgd_solver.cpp:112] Iteration 83070, lr = 0.01
I0522 22:42:22.681401 35003 solver.cpp:239] Iteration 83080 (3.91399 iter/s, 2.55494s/10 iters), loss = 7.73703
I0522 22:42:22.681443 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73703 (* 1 = 7.73703 loss)
I0522 22:42:22.699754 35003 sgd_solver.cpp:112] Iteration 83080, lr = 0.01
I0522 22:42:26.267101 35003 solver.cpp:239] Iteration 83090 (2.78901 iter/s, 3.58551s/10 iters), loss = 7.46837
I0522 22:42:26.267144 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46837 (* 1 = 7.46837 loss)
I0522 22:42:26.280254 35003 sgd_solver.cpp:112] Iteration 83090, lr = 0.01
I0522 22:42:28.429975 35003 solver.cpp:239] Iteration 83100 (4.62377 iter/s, 2.16274s/10 iters), loss = 7.73626
I0522 22:42:28.430023 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73626 (* 1 = 7.73626 loss)
I0522 22:42:29.124987 35003 sgd_solver.cpp:112] Iteration 83100, lr = 0.01
I0522 22:42:33.469949 35003 solver.cpp:239] Iteration 83110 (1.98424 iter/s, 5.03972s/10 iters), loss = 6.98234
I0522 22:42:33.469993 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98234 (* 1 = 6.98234 loss)
I0522 22:42:34.172166 35003 sgd_solver.cpp:112] Iteration 83110, lr = 0.01
I0522 22:42:38.884366 35003 solver.cpp:239] Iteration 83120 (1.84701 iter/s, 5.41414s/10 iters), loss = 7.06203
I0522 22:42:38.884544 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06203 (* 1 = 7.06203 loss)
I0522 22:42:39.592145 35003 sgd_solver.cpp:112] Iteration 83120, lr = 0.01
I0522 22:42:44.766408 35003 solver.cpp:239] Iteration 83130 (1.70021 iter/s, 5.88164s/10 iters), loss = 7.44988
I0522 22:42:44.766446 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44988 (* 1 = 7.44988 loss)
I0522 22:42:44.779551 35003 sgd_solver.cpp:112] Iteration 83130, lr = 0.01
I0522 22:42:47.495059 35003 solver.cpp:239] Iteration 83140 (3.66504 iter/s, 2.72848s/10 iters), loss = 6.98508
I0522 22:42:47.495120 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98508 (* 1 = 6.98508 loss)
I0522 22:42:47.507777 35003 sgd_solver.cpp:112] Iteration 83140, lr = 0.01
I0522 22:42:50.933028 35003 solver.cpp:239] Iteration 83150 (2.90886 iter/s, 3.43777s/10 iters), loss = 7.55344
I0522 22:42:50.933084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55344 (* 1 = 7.55344 loss)
I0522 22:42:50.955880 35003 sgd_solver.cpp:112] Iteration 83150, lr = 0.01
I0522 22:42:52.354966 35003 solver.cpp:239] Iteration 83160 (7.03328 iter/s, 1.42181s/10 iters), loss = 8.52537
I0522 22:42:52.355032 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.52537 (* 1 = 8.52537 loss)
I0522 22:42:52.356556 35003 sgd_solver.cpp:112] Iteration 83160, lr = 0.01
I0522 22:42:55.550400 35003 solver.cpp:239] Iteration 83170 (3.12966 iter/s, 3.19523s/10 iters), loss = 7.77101
I0522 22:42:55.550446 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77101 (* 1 = 7.77101 loss)
I0522 22:42:56.285053 35003 sgd_solver.cpp:112] Iteration 83170, lr = 0.01
I0522 22:42:59.509964 35003 solver.cpp:239] Iteration 83180 (2.52567 iter/s, 3.95935s/10 iters), loss = 8.93999
I0522 22:42:59.510007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.93999 (* 1 = 8.93999 loss)
I0522 22:42:59.523352 35003 sgd_solver.cpp:112] Iteration 83180, lr = 0.01
I0522 22:43:03.369065 35003 solver.cpp:239] Iteration 83190 (2.59141 iter/s, 3.8589s/10 iters), loss = 8.01268
I0522 22:43:03.369113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01268 (* 1 = 8.01268 loss)
I0522 22:43:04.103839 35003 sgd_solver.cpp:112] Iteration 83190, lr = 0.01
I0522 22:43:07.030798 35003 solver.cpp:239] Iteration 83200 (2.7311 iter/s, 3.66153s/10 iters), loss = 7.96656
I0522 22:43:07.030843 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96656 (* 1 = 7.96656 loss)
I0522 22:43:07.771248 35003 sgd_solver.cpp:112] Iteration 83200, lr = 0.01
I0522 22:43:11.690927 35003 solver.cpp:239] Iteration 83210 (2.14598 iter/s, 4.65988s/10 iters), loss = 6.66506
I0522 22:43:11.691210 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66506 (* 1 = 6.66506 loss)
I0522 22:43:11.695405 35003 sgd_solver.cpp:112] Iteration 83210, lr = 0.01
I0522 22:43:14.864409 35003 solver.cpp:239] Iteration 83220 (3.15151 iter/s, 3.17308s/10 iters), loss = 6.39595
I0522 22:43:14.864454 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39595 (* 1 = 6.39595 loss)
I0522 22:43:14.877528 35003 sgd_solver.cpp:112] Iteration 83220, lr = 0.01
I0522 22:43:18.934470 35003 solver.cpp:239] Iteration 83230 (2.45711 iter/s, 4.06982s/10 iters), loss = 7.68248
I0522 22:43:18.934535 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68248 (* 1 = 7.68248 loss)
I0522 22:43:18.947505 35003 sgd_solver.cpp:112] Iteration 83230, lr = 0.01
I0522 22:43:23.441557 35003 solver.cpp:239] Iteration 83240 (2.21885 iter/s, 4.50684s/10 iters), loss = 8.3583
I0522 22:43:23.441604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.3583 (* 1 = 8.3583 loss)
I0522 22:43:24.162834 35003 sgd_solver.cpp:112] Iteration 83240, lr = 0.01
I0522 22:43:27.625509 35003 solver.cpp:239] Iteration 83250 (2.39022 iter/s, 4.18372s/10 iters), loss = 7.63223
I0522 22:43:27.625560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63223 (* 1 = 7.63223 loss)
I0522 22:43:27.634096 35003 sgd_solver.cpp:112] Iteration 83250, lr = 0.01
I0522 22:43:30.494614 35003 solver.cpp:239] Iteration 83260 (3.48562 iter/s, 2.86893s/10 iters), loss = 8.20707
I0522 22:43:30.494673 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.20707 (* 1 = 8.20707 loss)
I0522 22:43:30.928840 35003 sgd_solver.cpp:112] Iteration 83260, lr = 0.01
I0522 22:43:36.637694 35003 solver.cpp:239] Iteration 83270 (1.62793 iter/s, 6.14278s/10 iters), loss = 6.54743
I0522 22:43:36.637743 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54743 (* 1 = 6.54743 loss)
I0522 22:43:37.340668 35003 sgd_solver.cpp:112] Iteration 83270, lr = 0.01
I0522 22:43:39.603129 35003 solver.cpp:239] Iteration 83280 (3.37238 iter/s, 2.96526s/10 iters), loss = 8.25854
I0522 22:43:39.603174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.25854 (* 1 = 8.25854 loss)
I0522 22:43:40.324787 35003 sgd_solver.cpp:112] Iteration 83280, lr = 0.01
I0522 22:43:43.082137 35003 solver.cpp:239] Iteration 83290 (2.87454 iter/s, 3.47882s/10 iters), loss = 7.89353
I0522 22:43:43.082348 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89353 (* 1 = 7.89353 loss)
I0522 22:43:43.100917 35003 sgd_solver.cpp:112] Iteration 83290, lr = 0.01
I0522 22:43:46.710559 35003 solver.cpp:239] Iteration 83300 (2.75627 iter/s, 3.62809s/10 iters), loss = 8.49323
I0522 22:43:46.710606 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.49323 (* 1 = 8.49323 loss)
I0522 22:43:46.723597 35003 sgd_solver.cpp:112] Iteration 83300, lr = 0.01
I0522 22:43:49.441282 35003 solver.cpp:239] Iteration 83310 (3.66227 iter/s, 2.73055s/10 iters), loss = 7.46699
I0522 22:43:49.441329 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46699 (* 1 = 7.46699 loss)
I0522 22:43:49.454035 35003 sgd_solver.cpp:112] Iteration 83310, lr = 0.01
I0522 22:43:53.467188 35003 solver.cpp:239] Iteration 83320 (2.48404 iter/s, 4.0257s/10 iters), loss = 7.78393
I0522 22:43:53.467239 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78393 (* 1 = 7.78393 loss)
I0522 22:43:54.201946 35003 sgd_solver.cpp:112] Iteration 83320, lr = 0.01
I0522 22:43:57.104130 35003 solver.cpp:239] Iteration 83330 (2.74971 iter/s, 3.63674s/10 iters), loss = 7.65684
I0522 22:43:57.104177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65684 (* 1 = 7.65684 loss)
I0522 22:43:57.178037 35003 sgd_solver.cpp:112] Iteration 83330, lr = 0.01
I0522 22:44:00.790249 35003 solver.cpp:239] Iteration 83340 (2.71306 iter/s, 3.68587s/10 iters), loss = 7.9378
I0522 22:44:00.790311 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9378 (* 1 = 7.9378 loss)
I0522 22:44:00.794689 35003 sgd_solver.cpp:112] Iteration 83340, lr = 0.01
I0522 22:44:03.611292 35003 solver.cpp:239] Iteration 83350 (3.54502 iter/s, 2.82086s/10 iters), loss = 6.73358
I0522 22:44:03.611340 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73358 (* 1 = 6.73358 loss)
I0522 22:44:03.617938 35003 sgd_solver.cpp:112] Iteration 83350, lr = 0.01
I0522 22:44:05.740802 35003 solver.cpp:239] Iteration 83360 (4.69624 iter/s, 2.12936s/10 iters), loss = 7.12474
I0522 22:44:05.740872 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12474 (* 1 = 7.12474 loss)
I0522 22:44:06.318434 35003 sgd_solver.cpp:112] Iteration 83360, lr = 0.01
I0522 22:44:09.068060 35003 solver.cpp:239] Iteration 83370 (3.00567 iter/s, 3.32705s/10 iters), loss = 6.82683
I0522 22:44:09.068104 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82683 (* 1 = 6.82683 loss)
I0522 22:44:09.080685 35003 sgd_solver.cpp:112] Iteration 83370, lr = 0.01
I0522 22:44:11.680727 35003 solver.cpp:239] Iteration 83380 (3.82774 iter/s, 2.6125s/10 iters), loss = 7.36406
I0522 22:44:11.680776 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36406 (* 1 = 7.36406 loss)
I0522 22:44:12.409179 35003 sgd_solver.cpp:112] Iteration 83380, lr = 0.01
I0522 22:44:15.548007 35003 solver.cpp:239] Iteration 83390 (2.58594 iter/s, 3.86707s/10 iters), loss = 8.99133
I0522 22:44:15.548146 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.99133 (* 1 = 8.99133 loss)
I0522 22:44:15.560832 35003 sgd_solver.cpp:112] Iteration 83390, lr = 0.01
I0522 22:44:18.752305 35003 solver.cpp:239] Iteration 83400 (3.12107 iter/s, 3.20403s/10 iters), loss = 8.03251
I0522 22:44:18.752362 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03251 (* 1 = 8.03251 loss)
I0522 22:44:19.396045 35003 sgd_solver.cpp:112] Iteration 83400, lr = 0.01
I0522 22:44:22.291031 35003 solver.cpp:239] Iteration 83410 (2.82604 iter/s, 3.53852s/10 iters), loss = 7.91521
I0522 22:44:22.291075 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91521 (* 1 = 7.91521 loss)
I0522 22:44:22.311249 35003 sgd_solver.cpp:112] Iteration 83410, lr = 0.01
I0522 22:44:25.756595 35003 solver.cpp:239] Iteration 83420 (2.88569 iter/s, 3.46538s/10 iters), loss = 8.25781
I0522 22:44:25.756649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.25781 (* 1 = 8.25781 loss)
I0522 22:44:25.769351 35003 sgd_solver.cpp:112] Iteration 83420, lr = 0.01
I0522 22:44:29.468940 35003 solver.cpp:239] Iteration 83430 (2.69387 iter/s, 3.71214s/10 iters), loss = 6.58086
I0522 22:44:29.468983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58086 (* 1 = 6.58086 loss)
I0522 22:44:29.487493 35003 sgd_solver.cpp:112] Iteration 83430, lr = 0.01
I0522 22:44:33.058200 35003 solver.cpp:239] Iteration 83440 (2.78624 iter/s, 3.58906s/10 iters), loss = 7.95502
I0522 22:44:33.058251 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95502 (* 1 = 7.95502 loss)
I0522 22:44:33.063524 35003 sgd_solver.cpp:112] Iteration 83440, lr = 0.01
I0522 22:44:35.737151 35003 solver.cpp:239] Iteration 83450 (3.73305 iter/s, 2.67877s/10 iters), loss = 7.84997
I0522 22:44:35.737217 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84997 (* 1 = 7.84997 loss)
I0522 22:44:36.160894 35003 sgd_solver.cpp:112] Iteration 83450, lr = 0.01
I0522 22:44:38.081701 35003 solver.cpp:239] Iteration 83460 (4.26551 iter/s, 2.34438s/10 iters), loss = 6.47897
I0522 22:44:38.081743 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47897 (* 1 = 6.47897 loss)
I0522 22:44:38.087338 35003 sgd_solver.cpp:112] Iteration 83460, lr = 0.01
I0522 22:44:43.826186 35003 solver.cpp:239] Iteration 83470 (1.74088 iter/s, 5.74421s/10 iters), loss = 6.88214
I0522 22:44:43.826231 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88214 (* 1 = 6.88214 loss)
I0522 22:44:43.844271 35003 sgd_solver.cpp:112] Iteration 83470, lr = 0.01
I0522 22:44:46.626018 35003 solver.cpp:239] Iteration 83480 (3.57185 iter/s, 2.79967s/10 iters), loss = 8.50305
I0522 22:44:46.626338 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.50305 (* 1 = 8.50305 loss)
I0522 22:44:46.634193 35003 sgd_solver.cpp:112] Iteration 83480, lr = 0.01
I0522 22:44:50.659970 35003 solver.cpp:239] Iteration 83490 (2.47923 iter/s, 4.03351s/10 iters), loss = 8.03363
I0522 22:44:50.660013 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03363 (* 1 = 8.03363 loss)
I0522 22:44:50.673117 35003 sgd_solver.cpp:112] Iteration 83490, lr = 0.01
I0522 22:44:52.044903 35003 solver.cpp:239] Iteration 83500 (7.22113 iter/s, 1.38482s/10 iters), loss = 6.80288
I0522 22:44:52.044950 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80288 (* 1 = 6.80288 loss)
I0522 22:44:52.056493 35003 sgd_solver.cpp:112] Iteration 83500, lr = 0.01
I0522 22:44:54.183341 35003 solver.cpp:239] Iteration 83510 (4.67662 iter/s, 2.1383s/10 iters), loss = 6.05272
I0522 22:44:54.183389 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05272 (* 1 = 6.05272 loss)
I0522 22:44:54.918331 35003 sgd_solver.cpp:112] Iteration 83510, lr = 0.01
I0522 22:44:57.288749 35003 solver.cpp:239] Iteration 83520 (3.22039 iter/s, 3.10521s/10 iters), loss = 6.74737
I0522 22:44:57.288810 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74737 (* 1 = 6.74737 loss)
I0522 22:44:58.016679 35003 sgd_solver.cpp:112] Iteration 83520, lr = 0.01
I0522 22:45:00.884941 35003 solver.cpp:239] Iteration 83530 (2.78088 iter/s, 3.59598s/10 iters), loss = 8.45728
I0522 22:45:00.884995 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.45728 (* 1 = 8.45728 loss)
I0522 22:45:00.896554 35003 sgd_solver.cpp:112] Iteration 83530, lr = 0.01
I0522 22:45:03.729166 35003 solver.cpp:239] Iteration 83540 (3.52149 iter/s, 2.83971s/10 iters), loss = 7.29362
I0522 22:45:03.729216 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29362 (* 1 = 7.29362 loss)
I0522 22:45:03.745836 35003 sgd_solver.cpp:112] Iteration 83540, lr = 0.01
I0522 22:45:07.351410 35003 solver.cpp:239] Iteration 83550 (2.76088 iter/s, 3.62203s/10 iters), loss = 6.27496
I0522 22:45:07.351459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27496 (* 1 = 6.27496 loss)
I0522 22:45:07.364811 35003 sgd_solver.cpp:112] Iteration 83550, lr = 0.01
I0522 22:45:10.126952 35003 solver.cpp:239] Iteration 83560 (3.60312 iter/s, 2.77537s/10 iters), loss = 6.30701
I0522 22:45:10.126998 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30701 (* 1 = 6.30701 loss)
I0522 22:45:10.842110 35003 sgd_solver.cpp:112] Iteration 83560, lr = 0.01
I0522 22:45:12.191608 35003 solver.cpp:239] Iteration 83570 (4.84375 iter/s, 2.06452s/10 iters), loss = 6.76736
I0522 22:45:12.191655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76736 (* 1 = 6.76736 loss)
I0522 22:45:12.932355 35003 sgd_solver.cpp:112] Iteration 83570, lr = 0.01
I0522 22:45:15.645743 35003 solver.cpp:239] Iteration 83580 (2.89524 iter/s, 3.45394s/10 iters), loss = 7.6085
I0522 22:45:15.645788 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6085 (* 1 = 7.6085 loss)
I0522 22:45:15.659665 35003 sgd_solver.cpp:112] Iteration 83580, lr = 0.01
I0522 22:45:19.183645 35003 solver.cpp:239] Iteration 83590 (2.82669 iter/s, 3.5377s/10 iters), loss = 8.37564
I0522 22:45:19.183964 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.37564 (* 1 = 8.37564 loss)
I0522 22:45:19.190281 35003 sgd_solver.cpp:112] Iteration 83590, lr = 0.01
I0522 22:45:23.608649 35003 solver.cpp:239] Iteration 83600 (2.26013 iter/s, 4.42452s/10 iters), loss = 7.61672
I0522 22:45:23.608706 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61672 (* 1 = 7.61672 loss)
I0522 22:45:24.342978 35003 sgd_solver.cpp:112] Iteration 83600, lr = 0.01
I0522 22:45:26.860759 35003 solver.cpp:239] Iteration 83610 (3.07512 iter/s, 3.25191s/10 iters), loss = 7.38936
I0522 22:45:26.860824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38936 (* 1 = 7.38936 loss)
I0522 22:45:26.866839 35003 sgd_solver.cpp:112] Iteration 83610, lr = 0.01
I0522 22:45:30.288581 35003 solver.cpp:239] Iteration 83620 (2.91749 iter/s, 3.42761s/10 iters), loss = 7.04721
I0522 22:45:30.288632 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04721 (* 1 = 7.04721 loss)
I0522 22:45:30.296303 35003 sgd_solver.cpp:112] Iteration 83620, lr = 0.01
I0522 22:45:33.834692 35003 solver.cpp:239] Iteration 83630 (2.82015 iter/s, 3.54591s/10 iters), loss = 7.36166
I0522 22:45:33.834774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36166 (* 1 = 7.36166 loss)
I0522 22:45:34.499600 35003 sgd_solver.cpp:112] Iteration 83630, lr = 0.01
I0522 22:45:37.575951 35003 solver.cpp:239] Iteration 83640 (2.67306 iter/s, 3.74103s/10 iters), loss = 7.96966
I0522 22:45:37.576001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96966 (* 1 = 7.96966 loss)
I0522 22:45:37.584018 35003 sgd_solver.cpp:112] Iteration 83640, lr = 0.01
I0522 22:45:40.969676 35003 solver.cpp:239] Iteration 83650 (2.94679 iter/s, 3.39352s/10 iters), loss = 7.95077
I0522 22:45:40.969736 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95077 (* 1 = 7.95077 loss)
I0522 22:45:40.990010 35003 sgd_solver.cpp:112] Iteration 83650, lr = 0.01
I0522 22:45:43.091266 35003 solver.cpp:239] Iteration 83660 (4.71382 iter/s, 2.12142s/10 iters), loss = 8.88139
I0522 22:45:43.091328 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.88139 (* 1 = 8.88139 loss)
I0522 22:45:43.095228 35003 sgd_solver.cpp:112] Iteration 83660, lr = 0.01
I0522 22:45:46.716176 35003 solver.cpp:239] Iteration 83670 (2.75887 iter/s, 3.62467s/10 iters), loss = 7.16068
I0522 22:45:46.716228 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16068 (* 1 = 7.16068 loss)
I0522 22:45:46.722781 35003 sgd_solver.cpp:112] Iteration 83670, lr = 0.01
I0522 22:45:49.575529 35003 solver.cpp:239] Iteration 83680 (3.49751 iter/s, 2.85918s/10 iters), loss = 7.01168
I0522 22:45:49.575753 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01168 (* 1 = 7.01168 loss)
I0522 22:45:49.580706 35003 sgd_solver.cpp:112] Iteration 83680, lr = 0.01
I0522 22:45:52.990886 35003 solver.cpp:239] Iteration 83690 (2.92824 iter/s, 3.41502s/10 iters), loss = 7.82899
I0522 22:45:52.990937 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82899 (* 1 = 7.82899 loss)
I0522 22:45:52.999130 35003 sgd_solver.cpp:112] Iteration 83690, lr = 0.01
I0522 22:45:55.086949 35003 solver.cpp:239] Iteration 83700 (4.77119 iter/s, 2.09592s/10 iters), loss = 7.03851
I0522 22:45:55.087028 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03851 (* 1 = 7.03851 loss)
I0522 22:45:55.171020 35003 sgd_solver.cpp:112] Iteration 83700, lr = 0.01
I0522 22:45:57.977299 35003 solver.cpp:239] Iteration 83710 (3.46003 iter/s, 2.89015s/10 iters), loss = 8.09293
I0522 22:45:57.977340 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.09293 (* 1 = 8.09293 loss)
I0522 22:45:57.990909 35003 sgd_solver.cpp:112] Iteration 83710, lr = 0.01
I0522 22:46:02.276255 35003 solver.cpp:239] Iteration 83720 (2.32627 iter/s, 4.29873s/10 iters), loss = 7.76837
I0522 22:46:02.276312 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76837 (* 1 = 7.76837 loss)
I0522 22:46:03.013919 35003 sgd_solver.cpp:112] Iteration 83720, lr = 0.01
I0522 22:46:05.769573 35003 solver.cpp:239] Iteration 83730 (2.86278 iter/s, 3.49311s/10 iters), loss = 6.69412
I0522 22:46:05.769618 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69412 (* 1 = 6.69412 loss)
I0522 22:46:05.781589 35003 sgd_solver.cpp:112] Iteration 83730, lr = 0.01
I0522 22:46:08.025877 35003 solver.cpp:239] Iteration 83740 (4.43232 iter/s, 2.25615s/10 iters), loss = 8.50151
I0522 22:46:08.025925 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.50151 (* 1 = 8.50151 loss)
I0522 22:46:08.034431 35003 sgd_solver.cpp:112] Iteration 83740, lr = 0.01
I0522 22:46:12.801862 35003 solver.cpp:239] Iteration 83750 (2.09392 iter/s, 4.77574s/10 iters), loss = 7.34996
I0522 22:46:12.801906 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34996 (* 1 = 7.34996 loss)
I0522 22:46:12.808972 35003 sgd_solver.cpp:112] Iteration 83750, lr = 0.01
I0522 22:46:15.635586 35003 solver.cpp:239] Iteration 83760 (3.52915 iter/s, 2.83354s/10 iters), loss = 6.93778
I0522 22:46:15.635632 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93778 (* 1 = 6.93778 loss)
I0522 22:46:15.644377 35003 sgd_solver.cpp:112] Iteration 83760, lr = 0.01
I0522 22:46:19.308084 35003 solver.cpp:239] Iteration 83770 (2.72309 iter/s, 3.67229s/10 iters), loss = 7.7089
I0522 22:46:19.308132 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7089 (* 1 = 7.7089 loss)
I0522 22:46:19.755908 35003 sgd_solver.cpp:112] Iteration 83770, lr = 0.01
I0522 22:46:22.369837 35003 solver.cpp:239] Iteration 83780 (3.26629 iter/s, 3.06157s/10 iters), loss = 7.63699
I0522 22:46:22.369889 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63699 (* 1 = 7.63699 loss)
I0522 22:46:22.371059 35003 sgd_solver.cpp:112] Iteration 83780, lr = 0.01
I0522 22:46:26.715128 35003 solver.cpp:239] Iteration 83790 (2.30146 iter/s, 4.34506s/10 iters), loss = 7.2003
I0522 22:46:26.715168 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2003 (* 1 = 7.2003 loss)
I0522 22:46:27.403748 35003 sgd_solver.cpp:112] Iteration 83790, lr = 0.01
I0522 22:46:30.371505 35003 solver.cpp:239] Iteration 83800 (2.7351 iter/s, 3.65617s/10 iters), loss = 6.97412
I0522 22:46:30.371562 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97412 (* 1 = 6.97412 loss)
I0522 22:46:30.395746 35003 sgd_solver.cpp:112] Iteration 83800, lr = 0.01
I0522 22:46:33.279055 35003 solver.cpp:239] Iteration 83810 (3.43954 iter/s, 2.90736s/10 iters), loss = 7.58277
I0522 22:46:33.279121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58277 (* 1 = 7.58277 loss)
I0522 22:46:33.770227 35003 sgd_solver.cpp:112] Iteration 83810, lr = 0.01
I0522 22:46:36.592115 35003 solver.cpp:239] Iteration 83820 (3.01854 iter/s, 3.31286s/10 iters), loss = 8.3302
I0522 22:46:36.592175 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.3302 (* 1 = 8.3302 loss)
I0522 22:46:36.598285 35003 sgd_solver.cpp:112] Iteration 83820, lr = 0.01
I0522 22:46:38.709213 35003 solver.cpp:239] Iteration 83830 (4.72378 iter/s, 2.11695s/10 iters), loss = 7.1124
I0522 22:46:38.709254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1124 (* 1 = 7.1124 loss)
I0522 22:46:38.716572 35003 sgd_solver.cpp:112] Iteration 83830, lr = 0.01
I0522 22:46:42.121033 35003 solver.cpp:239] Iteration 83840 (2.93114 iter/s, 3.41164s/10 iters), loss = 6.86709
I0522 22:46:42.121075 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86709 (* 1 = 6.86709 loss)
I0522 22:46:42.129189 35003 sgd_solver.cpp:112] Iteration 83840, lr = 0.01
I0522 22:46:45.783915 35003 solver.cpp:239] Iteration 83850 (2.73024 iter/s, 3.66269s/10 iters), loss = 6.80044
I0522 22:46:45.783970 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80044 (* 1 = 6.80044 loss)
I0522 22:46:46.492225 35003 sgd_solver.cpp:112] Iteration 83850, lr = 0.01
I0522 22:46:49.013793 35003 solver.cpp:239] Iteration 83860 (3.09628 iter/s, 3.22968s/10 iters), loss = 7.04989
I0522 22:46:49.013854 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04989 (* 1 = 7.04989 loss)
I0522 22:46:49.748306 35003 sgd_solver.cpp:112] Iteration 83860, lr = 0.01
I0522 22:46:52.552973 35003 solver.cpp:239] Iteration 83870 (2.82568 iter/s, 3.53897s/10 iters), loss = 8.02344
I0522 22:46:52.553180 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02344 (* 1 = 8.02344 loss)
I0522 22:46:53.275799 35003 sgd_solver.cpp:112] Iteration 83870, lr = 0.01
I0522 22:46:56.086318 35003 solver.cpp:239] Iteration 83880 (2.83047 iter/s, 3.53299s/10 iters), loss = 7.47309
I0522 22:46:56.086369 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47309 (* 1 = 7.47309 loss)
I0522 22:46:56.103652 35003 sgd_solver.cpp:112] Iteration 83880, lr = 0.01
I0522 22:46:58.549073 35003 solver.cpp:239] Iteration 83890 (4.06074 iter/s, 2.4626s/10 iters), loss = 8.45571
I0522 22:46:58.549113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.45571 (* 1 = 8.45571 loss)
I0522 22:46:58.561669 35003 sgd_solver.cpp:112] Iteration 83890, lr = 0.01
I0522 22:47:01.892747 35003 solver.cpp:239] Iteration 83900 (2.99089 iter/s, 3.34349s/10 iters), loss = 6.13002
I0522 22:47:01.892788 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13002 (* 1 = 6.13002 loss)
I0522 22:47:02.575610 35003 sgd_solver.cpp:112] Iteration 83900, lr = 0.01
I0522 22:47:06.677639 35003 solver.cpp:239] Iteration 83910 (2.09002 iter/s, 4.78465s/10 iters), loss = 6.69873
I0522 22:47:06.677691 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69873 (* 1 = 6.69873 loss)
I0522 22:47:07.398784 35003 sgd_solver.cpp:112] Iteration 83910, lr = 0.01
I0522 22:47:11.595265 35003 solver.cpp:239] Iteration 83920 (2.03362 iter/s, 4.91735s/10 iters), loss = 7.17362
I0522 22:47:11.595322 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17362 (* 1 = 7.17362 loss)
I0522 22:47:11.603204 35003 sgd_solver.cpp:112] Iteration 83920, lr = 0.01
I0522 22:47:15.230774 35003 solver.cpp:239] Iteration 83930 (2.75081 iter/s, 3.6353s/10 iters), loss = 7.94217
I0522 22:47:15.230818 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94217 (* 1 = 7.94217 loss)
I0522 22:47:15.243242 35003 sgd_solver.cpp:112] Iteration 83930, lr = 0.01
I0522 22:47:19.039023 35003 solver.cpp:239] Iteration 83940 (2.62602 iter/s, 3.80805s/10 iters), loss = 7.37476
I0522 22:47:19.039067 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37476 (* 1 = 7.37476 loss)
I0522 22:47:19.066759 35003 sgd_solver.cpp:112] Iteration 83940, lr = 0.01
I0522 22:47:21.916622 35003 solver.cpp:239] Iteration 83950 (3.47532 iter/s, 2.87743s/10 iters), loss = 7.28736
I0522 22:47:21.916677 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28736 (* 1 = 7.28736 loss)
I0522 22:47:22.611976 35003 sgd_solver.cpp:112] Iteration 83950, lr = 0.01
I0522 22:47:26.158130 35003 solver.cpp:239] Iteration 83960 (2.35778 iter/s, 4.24128s/10 iters), loss = 7.60019
I0522 22:47:26.158179 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60019 (* 1 = 7.60019 loss)
I0522 22:47:26.170980 35003 sgd_solver.cpp:112] Iteration 83960, lr = 0.01
I0522 22:47:28.914784 35003 solver.cpp:239] Iteration 83970 (3.62781 iter/s, 2.75649s/10 iters), loss = 8.2376
I0522 22:47:28.914824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.2376 (* 1 = 8.2376 loss)
I0522 22:47:28.932760 35003 sgd_solver.cpp:112] Iteration 83970, lr = 0.01
I0522 22:47:31.009214 35003 solver.cpp:239] Iteration 83980 (4.77487 iter/s, 2.0943s/10 iters), loss = 7.79371
I0522 22:47:31.009268 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79371 (* 1 = 7.79371 loss)
I0522 22:47:31.022027 35003 sgd_solver.cpp:112] Iteration 83980, lr = 0.01
I0522 22:47:34.564551 35003 solver.cpp:239] Iteration 83990 (2.81283 iter/s, 3.55513s/10 iters), loss = 7.46206
I0522 22:47:34.564608 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46206 (* 1 = 7.46206 loss)
I0522 22:47:35.286658 35003 sgd_solver.cpp:112] Iteration 83990, lr = 0.01
I0522 22:47:38.055104 35003 solver.cpp:239] Iteration 84000 (2.86504 iter/s, 3.49036s/10 iters), loss = 6.39081
I0522 22:47:38.055141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39081 (* 1 = 6.39081 loss)
I0522 22:47:38.068315 35003 sgd_solver.cpp:112] Iteration 84000, lr = 0.01
I0522 22:47:41.478574 35003 solver.cpp:239] Iteration 84010 (2.92117 iter/s, 3.42329s/10 iters), loss = 7.59684
I0522 22:47:41.478619 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59684 (* 1 = 7.59684 loss)
I0522 22:47:41.492264 35003 sgd_solver.cpp:112] Iteration 84010, lr = 0.01
I0522 22:47:45.113826 35003 solver.cpp:239] Iteration 84020 (2.75099 iter/s, 3.63505s/10 iters), loss = 7.73429
I0522 22:47:45.113870 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73429 (* 1 = 7.73429 loss)
I0522 22:47:45.119956 35003 sgd_solver.cpp:112] Iteration 84020, lr = 0.01
I0522 22:47:47.911540 35003 solver.cpp:239] Iteration 84030 (3.57456 iter/s, 2.79754s/10 iters), loss = 7.26919
I0522 22:47:47.911600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26919 (* 1 = 7.26919 loss)
I0522 22:47:48.652364 35003 sgd_solver.cpp:112] Iteration 84030, lr = 0.01
I0522 22:47:52.286521 35003 solver.cpp:239] Iteration 84040 (2.28585 iter/s, 4.37474s/10 iters), loss = 7.43848
I0522 22:47:52.286572 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43848 (* 1 = 7.43848 loss)
I0522 22:47:52.290307 35003 sgd_solver.cpp:112] Iteration 84040, lr = 0.01
I0522 22:47:57.982954 35003 solver.cpp:239] Iteration 84050 (1.75557 iter/s, 5.69615s/10 iters), loss = 7.05472
I0522 22:47:57.983288 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05472 (* 1 = 7.05472 loss)
I0522 22:47:58.145058 35003 sgd_solver.cpp:112] Iteration 84050, lr = 0.01
I0522 22:48:01.755777 35003 solver.cpp:239] Iteration 84060 (2.65085 iter/s, 3.77237s/10 iters), loss = 6.79096
I0522 22:48:01.755822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79096 (* 1 = 6.79096 loss)
I0522 22:48:02.496891 35003 sgd_solver.cpp:112] Iteration 84060, lr = 0.01
I0522 22:48:06.162139 35003 solver.cpp:239] Iteration 84070 (2.26957 iter/s, 4.40612s/10 iters), loss = 7.44235
I0522 22:48:06.162206 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44235 (* 1 = 7.44235 loss)
I0522 22:48:06.902990 35003 sgd_solver.cpp:112] Iteration 84070, lr = 0.01
I0522 22:48:09.309012 35003 solver.cpp:239] Iteration 84080 (3.17796 iter/s, 3.14667s/10 iters), loss = 7.61525
I0522 22:48:09.309069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61525 (* 1 = 7.61525 loss)
I0522 22:48:09.321844 35003 sgd_solver.cpp:112] Iteration 84080, lr = 0.01
I0522 22:48:12.249893 35003 solver.cpp:239] Iteration 84090 (3.40055 iter/s, 2.9407s/10 iters), loss = 7.23649
I0522 22:48:12.249943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23649 (* 1 = 7.23649 loss)
I0522 22:48:12.252720 35003 sgd_solver.cpp:112] Iteration 84090, lr = 0.01
I0522 22:48:15.086395 35003 solver.cpp:239] Iteration 84100 (3.52569 iter/s, 2.83632s/10 iters), loss = 8.17482
I0522 22:48:15.086447 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17482 (* 1 = 8.17482 loss)
I0522 22:48:15.099018 35003 sgd_solver.cpp:112] Iteration 84100, lr = 0.01
I0522 22:48:20.264827 35003 solver.cpp:239] Iteration 84110 (1.93118 iter/s, 5.17817s/10 iters), loss = 7.37295
I0522 22:48:20.264884 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37295 (* 1 = 7.37295 loss)
I0522 22:48:20.688231 35003 sgd_solver.cpp:112] Iteration 84110, lr = 0.01
I0522 22:48:24.424116 35003 solver.cpp:239] Iteration 84120 (2.40439 iter/s, 4.15906s/10 iters), loss = 7.64071
I0522 22:48:24.424161 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64071 (* 1 = 7.64071 loss)
I0522 22:48:24.431190 35003 sgd_solver.cpp:112] Iteration 84120, lr = 0.01
I0522 22:48:26.496955 35003 solver.cpp:239] Iteration 84130 (4.82465 iter/s, 2.07269s/10 iters), loss = 8.61262
I0522 22:48:26.497006 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.61262 (* 1 = 8.61262 loss)
I0522 22:48:26.507377 35003 sgd_solver.cpp:112] Iteration 84130, lr = 0.01
I0522 22:48:30.146224 35003 solver.cpp:239] Iteration 84140 (2.74044 iter/s, 3.64905s/10 iters), loss = 7.87457
I0522 22:48:30.146525 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87457 (* 1 = 7.87457 loss)
I0522 22:48:30.830735 35003 sgd_solver.cpp:112] Iteration 84140, lr = 0.01
I0522 22:48:35.180840 35003 solver.cpp:239] Iteration 84150 (1.98644 iter/s, 5.03414s/10 iters), loss = 8.17637
I0522 22:48:35.180891 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17637 (* 1 = 8.17637 loss)
I0522 22:48:35.203456 35003 sgd_solver.cpp:112] Iteration 84150, lr = 0.01
I0522 22:48:38.359845 35003 solver.cpp:239] Iteration 84160 (3.14582 iter/s, 3.17882s/10 iters), loss = 7.22037
I0522 22:48:38.359899 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22037 (* 1 = 7.22037 loss)
I0522 22:48:38.366928 35003 sgd_solver.cpp:112] Iteration 84160, lr = 0.01
I0522 22:48:42.817755 35003 solver.cpp:239] Iteration 84170 (2.24333 iter/s, 4.45766s/10 iters), loss = 7.94476
I0522 22:48:42.817812 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94476 (* 1 = 7.94476 loss)
I0522 22:48:42.823026 35003 sgd_solver.cpp:112] Iteration 84170, lr = 0.01
I0522 22:48:45.461786 35003 solver.cpp:239] Iteration 84180 (3.78235 iter/s, 2.64386s/10 iters), loss = 6.76705
I0522 22:48:45.461839 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76705 (* 1 = 6.76705 loss)
I0522 22:48:45.480159 35003 sgd_solver.cpp:112] Iteration 84180, lr = 0.01
I0522 22:48:49.308478 35003 solver.cpp:239] Iteration 84190 (2.59978 iter/s, 3.84649s/10 iters), loss = 7.59397
I0522 22:48:49.308519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59397 (* 1 = 7.59397 loss)
I0522 22:48:49.335765 35003 sgd_solver.cpp:112] Iteration 84190, lr = 0.01
I0522 22:48:53.716624 35003 solver.cpp:239] Iteration 84200 (2.26864 iter/s, 4.40792s/10 iters), loss = 6.96518
I0522 22:48:53.716675 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96518 (* 1 = 6.96518 loss)
I0522 22:48:53.733047 35003 sgd_solver.cpp:112] Iteration 84200, lr = 0.01
I0522 22:48:57.985311 35003 solver.cpp:239] Iteration 84210 (2.34276 iter/s, 4.26846s/10 iters), loss = 7.22128
I0522 22:48:57.985359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22128 (* 1 = 7.22128 loss)
I0522 22:48:57.992691 35003 sgd_solver.cpp:112] Iteration 84210, lr = 0.01
I0522 22:49:01.472509 35003 solver.cpp:239] Iteration 84220 (2.86779 iter/s, 3.48701s/10 iters), loss = 7.60444
I0522 22:49:01.472704 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60444 (* 1 = 7.60444 loss)
I0522 22:49:01.484616 35003 sgd_solver.cpp:112] Iteration 84220, lr = 0.01
I0522 22:49:05.775028 35003 solver.cpp:239] Iteration 84230 (2.32441 iter/s, 4.30217s/10 iters), loss = 7.45164
I0522 22:49:05.775076 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45164 (* 1 = 7.45164 loss)
I0522 22:49:05.792649 35003 sgd_solver.cpp:112] Iteration 84230, lr = 0.01
I0522 22:49:08.660387 35003 solver.cpp:239] Iteration 84240 (3.46599 iter/s, 2.88518s/10 iters), loss = 6.42312
I0522 22:49:08.660444 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42312 (* 1 = 6.42312 loss)
I0522 22:49:08.682287 35003 sgd_solver.cpp:112] Iteration 84240, lr = 0.01
I0522 22:49:12.068593 35003 solver.cpp:239] Iteration 84250 (2.93427 iter/s, 3.40801s/10 iters), loss = 7.83602
I0522 22:49:12.068634 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83602 (* 1 = 7.83602 loss)
I0522 22:49:12.074815 35003 sgd_solver.cpp:112] Iteration 84250, lr = 0.01
I0522 22:49:14.928817 35003 solver.cpp:239] Iteration 84260 (3.49643 iter/s, 2.86006s/10 iters), loss = 7.09051
I0522 22:49:14.928864 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09051 (* 1 = 7.09051 loss)
I0522 22:49:14.938765 35003 sgd_solver.cpp:112] Iteration 84260, lr = 0.01
I0522 22:49:18.499125 35003 solver.cpp:239] Iteration 84270 (2.80103 iter/s, 3.57011s/10 iters), loss = 7.85446
I0522 22:49:18.499181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85446 (* 1 = 7.85446 loss)
I0522 22:49:18.506916 35003 sgd_solver.cpp:112] Iteration 84270, lr = 0.01
I0522 22:49:21.570673 35003 solver.cpp:239] Iteration 84280 (3.25588 iter/s, 3.07136s/10 iters), loss = 7.6454
I0522 22:49:21.570755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6454 (* 1 = 7.6454 loss)
I0522 22:49:21.986323 35003 sgd_solver.cpp:112] Iteration 84280, lr = 0.01
I0522 22:49:25.490552 35003 solver.cpp:239] Iteration 84290 (2.55126 iter/s, 3.91963s/10 iters), loss = 7.35186
I0522 22:49:25.490608 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35186 (* 1 = 7.35186 loss)
I0522 22:49:25.493975 35003 sgd_solver.cpp:112] Iteration 84290, lr = 0.01
I0522 22:49:29.529811 35003 solver.cpp:239] Iteration 84300 (2.47584 iter/s, 4.03903s/10 iters), loss = 7.80251
I0522 22:49:29.529861 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80251 (* 1 = 7.80251 loss)
I0522 22:49:29.548696 35003 sgd_solver.cpp:112] Iteration 84300, lr = 0.01
I0522 22:49:33.187711 35003 solver.cpp:239] Iteration 84310 (2.73396 iter/s, 3.65769s/10 iters), loss = 7.04218
I0522 22:49:33.187883 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04218 (* 1 = 7.04218 loss)
I0522 22:49:33.220023 35003 sgd_solver.cpp:112] Iteration 84310, lr = 0.01
I0522 22:49:36.765730 35003 solver.cpp:239] Iteration 84320 (2.79509 iter/s, 3.5777s/10 iters), loss = 6.63436
I0522 22:49:36.765771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63436 (* 1 = 6.63436 loss)
I0522 22:49:36.777590 35003 sgd_solver.cpp:112] Iteration 84320, lr = 0.01
I0522 22:49:39.967438 35003 solver.cpp:239] Iteration 84330 (3.12351 iter/s, 3.20153s/10 iters), loss = 5.54741
I0522 22:49:39.967475 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.54741 (* 1 = 5.54741 loss)
I0522 22:49:39.980051 35003 sgd_solver.cpp:112] Iteration 84330, lr = 0.01
I0522 22:49:42.045418 35003 solver.cpp:239] Iteration 84340 (4.81267 iter/s, 2.07785s/10 iters), loss = 8.20632
I0522 22:49:42.045459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.20632 (* 1 = 8.20632 loss)
I0522 22:49:42.058198 35003 sgd_solver.cpp:112] Iteration 84340, lr = 0.01
I0522 22:49:45.231540 35003 solver.cpp:239] Iteration 84350 (3.13879 iter/s, 3.18594s/10 iters), loss = 6.36973
I0522 22:49:45.231588 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36973 (* 1 = 6.36973 loss)
I0522 22:49:45.234755 35003 sgd_solver.cpp:112] Iteration 84350, lr = 0.01
I0522 22:49:49.421216 35003 solver.cpp:239] Iteration 84360 (2.38695 iter/s, 4.18945s/10 iters), loss = 8.09235
I0522 22:49:49.421268 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.09235 (* 1 = 8.09235 loss)
I0522 22:49:50.118263 35003 sgd_solver.cpp:112] Iteration 84360, lr = 0.01
I0522 22:49:52.921603 35003 solver.cpp:239] Iteration 84370 (2.85699 iter/s, 3.50019s/10 iters), loss = 7.15823
I0522 22:49:52.921654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15823 (* 1 = 7.15823 loss)
I0522 22:49:52.932502 35003 sgd_solver.cpp:112] Iteration 84370, lr = 0.01
I0522 22:49:56.390617 35003 solver.cpp:239] Iteration 84380 (2.88283 iter/s, 3.46882s/10 iters), loss = 7.20817
I0522 22:49:56.390674 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20817 (* 1 = 7.20817 loss)
I0522 22:49:57.049186 35003 sgd_solver.cpp:112] Iteration 84380, lr = 0.01
I0522 22:49:59.766264 35003 solver.cpp:239] Iteration 84390 (2.96257 iter/s, 3.37544s/10 iters), loss = 8.14771
I0522 22:49:59.766333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.14771 (* 1 = 8.14771 loss)
I0522 22:49:59.779485 35003 sgd_solver.cpp:112] Iteration 84390, lr = 0.01
I0522 22:50:04.120527 35003 solver.cpp:239] Iteration 84400 (2.29674 iter/s, 4.354s/10 iters), loss = 6.39807
I0522 22:50:04.120808 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39807 (* 1 = 6.39807 loss)
I0522 22:50:04.168534 35003 sgd_solver.cpp:112] Iteration 84400, lr = 0.01
I0522 22:50:06.996517 35003 solver.cpp:239] Iteration 84410 (3.47752 iter/s, 2.87561s/10 iters), loss = 6.97437
I0522 22:50:06.996568 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97437 (* 1 = 6.97437 loss)
I0522 22:50:07.008188 35003 sgd_solver.cpp:112] Iteration 84410, lr = 0.01
I0522 22:50:09.942111 35003 solver.cpp:239] Iteration 84420 (3.3951 iter/s, 2.94542s/10 iters), loss = 7.86153
I0522 22:50:09.942158 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86153 (* 1 = 7.86153 loss)
I0522 22:50:10.669873 35003 sgd_solver.cpp:112] Iteration 84420, lr = 0.01
I0522 22:50:13.488055 35003 solver.cpp:239] Iteration 84430 (2.82028 iter/s, 3.54574s/10 iters), loss = 7.27003
I0522 22:50:13.488101 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27003 (* 1 = 7.27003 loss)
I0522 22:50:13.491127 35003 sgd_solver.cpp:112] Iteration 84430, lr = 0.01
I0522 22:50:17.935367 35003 solver.cpp:239] Iteration 84440 (2.24867 iter/s, 4.44707s/10 iters), loss = 7.328
I0522 22:50:17.935417 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.328 (* 1 = 7.328 loss)
I0522 22:50:17.948940 35003 sgd_solver.cpp:112] Iteration 84440, lr = 0.01
I0522 22:50:22.227398 35003 solver.cpp:239] Iteration 84450 (2.33002 iter/s, 4.29181s/10 iters), loss = 6.71612
I0522 22:50:22.227447 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71612 (* 1 = 6.71612 loss)
I0522 22:50:22.962209 35003 sgd_solver.cpp:112] Iteration 84450, lr = 0.01
I0522 22:50:25.397264 35003 solver.cpp:239] Iteration 84460 (3.15489 iter/s, 3.16968s/10 iters), loss = 8.74572
I0522 22:50:25.397323 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.74572 (* 1 = 8.74572 loss)
I0522 22:50:25.992178 35003 sgd_solver.cpp:112] Iteration 84460, lr = 0.01
I0522 22:50:28.766371 35003 solver.cpp:239] Iteration 84470 (2.96832 iter/s, 3.36891s/10 iters), loss = 7.93898
I0522 22:50:28.766412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93898 (* 1 = 7.93898 loss)
I0522 22:50:28.779685 35003 sgd_solver.cpp:112] Iteration 84470, lr = 0.01
I0522 22:50:33.748653 35003 solver.cpp:239] Iteration 84480 (2.00721 iter/s, 4.98204s/10 iters), loss = 7.79815
I0522 22:50:33.748692 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79815 (* 1 = 7.79815 loss)
I0522 22:50:33.771802 35003 sgd_solver.cpp:112] Iteration 84480, lr = 0.01
I0522 22:50:36.607708 35003 solver.cpp:239] Iteration 84490 (3.49786 iter/s, 2.85889s/10 iters), loss = 6.74652
I0522 22:50:36.607902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74652 (* 1 = 6.74652 loss)
I0522 22:50:36.644762 35003 sgd_solver.cpp:112] Iteration 84490, lr = 0.01
I0522 22:50:39.441942 35003 solver.cpp:239] Iteration 84500 (3.52866 iter/s, 2.83394s/10 iters), loss = 7.58571
I0522 22:50:39.441996 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58571 (* 1 = 7.58571 loss)
I0522 22:50:39.464190 35003 sgd_solver.cpp:112] Iteration 84500, lr = 0.01
I0522 22:50:43.647150 35003 solver.cpp:239] Iteration 84510 (2.37814 iter/s, 4.20498s/10 iters), loss = 8.2823
I0522 22:50:43.647204 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.2823 (* 1 = 8.2823 loss)
I0522 22:50:43.674701 35003 sgd_solver.cpp:112] Iteration 84510, lr = 0.01
I0522 22:50:47.968947 35003 solver.cpp:239] Iteration 84520 (2.31398 iter/s, 4.32156s/10 iters), loss = 7.56111
I0522 22:50:47.968992 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56111 (* 1 = 7.56111 loss)
I0522 22:50:47.975040 35003 sgd_solver.cpp:112] Iteration 84520, lr = 0.01
I0522 22:50:52.296231 35003 solver.cpp:239] Iteration 84530 (2.31104 iter/s, 4.32706s/10 iters), loss = 6.88014
I0522 22:50:52.296270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88014 (* 1 = 6.88014 loss)
I0522 22:50:52.305622 35003 sgd_solver.cpp:112] Iteration 84530, lr = 0.01
I0522 22:50:55.188402 35003 solver.cpp:239] Iteration 84540 (3.4578 iter/s, 2.89201s/10 iters), loss = 6.66898
I0522 22:50:55.188441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66898 (* 1 = 6.66898 loss)
I0522 22:50:55.193725 35003 sgd_solver.cpp:112] Iteration 84540, lr = 0.01
I0522 22:50:57.976944 35003 solver.cpp:239] Iteration 84550 (3.58631 iter/s, 2.78838s/10 iters), loss = 7.61582
I0522 22:50:57.976984 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61582 (* 1 = 7.61582 loss)
I0522 22:50:57.983212 35003 sgd_solver.cpp:112] Iteration 84550, lr = 0.01
I0522 22:51:00.856688 35003 solver.cpp:239] Iteration 84560 (3.47275 iter/s, 2.87956s/10 iters), loss = 7.44479
I0522 22:51:00.856740 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44479 (* 1 = 7.44479 loss)
I0522 22:51:00.870434 35003 sgd_solver.cpp:112] Iteration 84560, lr = 0.01
I0522 22:51:03.335685 35003 solver.cpp:239] Iteration 84570 (4.03414 iter/s, 2.47884s/10 iters), loss = 6.43099
I0522 22:51:03.335736 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43099 (* 1 = 6.43099 loss)
I0522 22:51:04.070705 35003 sgd_solver.cpp:112] Iteration 84570, lr = 0.01
I0522 22:51:08.501322 35003 solver.cpp:239] Iteration 84580 (1.93597 iter/s, 5.16538s/10 iters), loss = 7.26449
I0522 22:51:08.501602 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26449 (* 1 = 7.26449 loss)
I0522 22:51:09.236244 35003 sgd_solver.cpp:112] Iteration 84580, lr = 0.01
I0522 22:51:12.176512 35003 solver.cpp:239] Iteration 84590 (2.72125 iter/s, 3.67478s/10 iters), loss = 7.96069
I0522 22:51:12.176558 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96069 (* 1 = 7.96069 loss)
I0522 22:51:12.545269 35003 sgd_solver.cpp:112] Iteration 84590, lr = 0.01
I0522 22:51:15.373952 35003 solver.cpp:239] Iteration 84600 (3.12768 iter/s, 3.19726s/10 iters), loss = 7.98123
I0522 22:51:15.374003 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98123 (* 1 = 7.98123 loss)
I0522 22:51:15.383455 35003 sgd_solver.cpp:112] Iteration 84600, lr = 0.01
I0522 22:51:18.130033 35003 solver.cpp:239] Iteration 84610 (3.62856 iter/s, 2.75591s/10 iters), loss = 7.68111
I0522 22:51:18.130080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68111 (* 1 = 7.68111 loss)
I0522 22:51:18.143507 35003 sgd_solver.cpp:112] Iteration 84610, lr = 0.01
I0522 22:51:22.379849 35003 solver.cpp:239] Iteration 84620 (2.35317 iter/s, 4.24959s/10 iters), loss = 6.54176
I0522 22:51:22.379904 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54176 (* 1 = 6.54176 loss)
I0522 22:51:22.392416 35003 sgd_solver.cpp:112] Iteration 84620, lr = 0.01
I0522 22:51:26.050566 35003 solver.cpp:239] Iteration 84630 (2.72443 iter/s, 3.67049s/10 iters), loss = 7.9085
I0522 22:51:26.050616 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9085 (* 1 = 7.9085 loss)
I0522 22:51:26.076081 35003 sgd_solver.cpp:112] Iteration 84630, lr = 0.01
I0522 22:51:31.305634 35003 solver.cpp:239] Iteration 84640 (1.90302 iter/s, 5.2548s/10 iters), loss = 8.48685
I0522 22:51:31.305694 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.48685 (* 1 = 8.48685 loss)
I0522 22:51:32.046398 35003 sgd_solver.cpp:112] Iteration 84640, lr = 0.01
I0522 22:51:36.261168 35003 solver.cpp:239] Iteration 84650 (2.01806 iter/s, 4.95526s/10 iters), loss = 7.67504
I0522 22:51:36.261229 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67504 (* 1 = 7.67504 loss)
I0522 22:51:36.969467 35003 sgd_solver.cpp:112] Iteration 84650, lr = 0.01
I0522 22:51:39.560053 35003 solver.cpp:239] Iteration 84660 (3.03151 iter/s, 3.29869s/10 iters), loss = 7.78866
I0522 22:51:39.560298 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78866 (* 1 = 7.78866 loss)
I0522 22:51:40.274680 35003 sgd_solver.cpp:112] Iteration 84660, lr = 0.01
I0522 22:51:43.834682 35003 solver.cpp:239] Iteration 84670 (2.3396 iter/s, 4.27424s/10 iters), loss = 7.41269
I0522 22:51:43.834758 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41269 (* 1 = 7.41269 loss)
I0522 22:51:43.847553 35003 sgd_solver.cpp:112] Iteration 84670, lr = 0.01
I0522 22:51:47.982813 35003 solver.cpp:239] Iteration 84680 (2.41087 iter/s, 4.14788s/10 iters), loss = 7.21523
I0522 22:51:47.982877 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21523 (* 1 = 7.21523 loss)
I0522 22:51:48.027725 35003 sgd_solver.cpp:112] Iteration 84680, lr = 0.01
I0522 22:51:51.592034 35003 solver.cpp:239] Iteration 84690 (2.77084 iter/s, 3.60901s/10 iters), loss = 7.33572
I0522 22:51:51.592075 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33572 (* 1 = 7.33572 loss)
I0522 22:51:51.597869 35003 sgd_solver.cpp:112] Iteration 84690, lr = 0.01
I0522 22:51:55.604429 35003 solver.cpp:239] Iteration 84700 (2.49241 iter/s, 4.01219s/10 iters), loss = 6.66227
I0522 22:51:55.604487 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66227 (* 1 = 6.66227 loss)
I0522 22:51:56.057610 35003 sgd_solver.cpp:112] Iteration 84700, lr = 0.01
I0522 22:51:59.394124 35003 solver.cpp:239] Iteration 84710 (2.63889 iter/s, 3.78948s/10 iters), loss = 7.49884
I0522 22:51:59.394173 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49884 (* 1 = 7.49884 loss)
I0522 22:51:59.647814 35003 sgd_solver.cpp:112] Iteration 84710, lr = 0.01
I0522 22:52:02.534657 35003 solver.cpp:239] Iteration 84720 (3.18435 iter/s, 3.14035s/10 iters), loss = 7.7838
I0522 22:52:02.534710 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7838 (* 1 = 7.7838 loss)
I0522 22:52:02.539526 35003 sgd_solver.cpp:112] Iteration 84720, lr = 0.01
I0522 22:52:05.439514 35003 solver.cpp:239] Iteration 84730 (3.44272 iter/s, 2.90468s/10 iters), loss = 7.10116
I0522 22:52:05.439558 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10116 (* 1 = 7.10116 loss)
I0522 22:52:05.452178 35003 sgd_solver.cpp:112] Iteration 84730, lr = 0.01
I0522 22:52:08.257427 35003 solver.cpp:239] Iteration 84740 (3.54893 iter/s, 2.81775s/10 iters), loss = 6.81847
I0522 22:52:08.257467 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81847 (* 1 = 6.81847 loss)
I0522 22:52:08.278931 35003 sgd_solver.cpp:112] Iteration 84740, lr = 0.01
I0522 22:52:11.631686 35003 solver.cpp:239] Iteration 84750 (2.96377 iter/s, 3.37408s/10 iters), loss = 7.46038
I0522 22:52:11.631990 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46038 (* 1 = 7.46038 loss)
I0522 22:52:11.637266 35003 sgd_solver.cpp:112] Iteration 84750, lr = 0.01
I0522 22:52:15.261175 35003 solver.cpp:239] Iteration 84760 (2.75553 iter/s, 3.62907s/10 iters), loss = 7.05841
I0522 22:52:15.261224 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05841 (* 1 = 7.05841 loss)
I0522 22:52:15.266762 35003 sgd_solver.cpp:112] Iteration 84760, lr = 0.01
I0522 22:52:18.918509 35003 solver.cpp:239] Iteration 84770 (2.7344 iter/s, 3.65712s/10 iters), loss = 7.51977
I0522 22:52:18.918565 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51977 (* 1 = 7.51977 loss)
I0522 22:52:18.925253 35003 sgd_solver.cpp:112] Iteration 84770, lr = 0.01
I0522 22:52:22.507448 35003 solver.cpp:239] Iteration 84780 (2.7865 iter/s, 3.58873s/10 iters), loss = 8.77576
I0522 22:52:22.507496 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.77576 (* 1 = 8.77576 loss)
I0522 22:52:23.197087 35003 sgd_solver.cpp:112] Iteration 84780, lr = 0.01
I0522 22:52:27.320770 35003 solver.cpp:239] Iteration 84790 (2.07767 iter/s, 4.81307s/10 iters), loss = 7.72237
I0522 22:52:27.320817 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72237 (* 1 = 7.72237 loss)
I0522 22:52:27.333753 35003 sgd_solver.cpp:112] Iteration 84790, lr = 0.01
I0522 22:52:30.242632 35003 solver.cpp:239] Iteration 84800 (3.42268 iter/s, 2.92169s/10 iters), loss = 7.7241
I0522 22:52:30.242684 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7241 (* 1 = 7.7241 loss)
I0522 22:52:30.957331 35003 sgd_solver.cpp:112] Iteration 84800, lr = 0.01
I0522 22:52:33.640645 35003 solver.cpp:239] Iteration 84810 (2.94306 iter/s, 3.39782s/10 iters), loss = 7.83439
I0522 22:52:33.640691 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83439 (* 1 = 7.83439 loss)
I0522 22:52:33.653920 35003 sgd_solver.cpp:112] Iteration 84810, lr = 0.01
I0522 22:52:37.317358 35003 solver.cpp:239] Iteration 84820 (2.71998 iter/s, 3.6765s/10 iters), loss = 5.92605
I0522 22:52:37.317818 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92605 (* 1 = 5.92605 loss)
I0522 22:52:37.320922 35003 sgd_solver.cpp:112] Iteration 84820, lr = 0.01
I0522 22:52:40.903111 35003 solver.cpp:239] Iteration 84830 (2.7893 iter/s, 3.58512s/10 iters), loss = 7.39232
I0522 22:52:40.903156 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39232 (* 1 = 7.39232 loss)
I0522 22:52:40.916255 35003 sgd_solver.cpp:112] Iteration 84830, lr = 0.01
I0522 22:52:44.394170 35003 solver.cpp:239] Iteration 84840 (2.86462 iter/s, 3.49087s/10 iters), loss = 6.79089
I0522 22:52:44.394467 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79089 (* 1 = 6.79089 loss)
I0522 22:52:44.399760 35003 sgd_solver.cpp:112] Iteration 84840, lr = 0.01
I0522 22:52:48.619544 35003 solver.cpp:239] Iteration 84850 (2.36691 iter/s, 4.22492s/10 iters), loss = 7.20602
I0522 22:52:48.619619 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20602 (* 1 = 7.20602 loss)
I0522 22:52:48.632084 35003 sgd_solver.cpp:112] Iteration 84850, lr = 0.01
I0522 22:52:52.287209 35003 solver.cpp:239] Iteration 84860 (2.7267 iter/s, 3.66744s/10 iters), loss = 8.23212
I0522 22:52:52.287246 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.23212 (* 1 = 8.23212 loss)
I0522 22:52:52.291949 35003 sgd_solver.cpp:112] Iteration 84860, lr = 0.01
I0522 22:52:55.652194 35003 solver.cpp:239] Iteration 84870 (2.97196 iter/s, 3.36479s/10 iters), loss = 7.6321
I0522 22:52:55.652240 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6321 (* 1 = 7.6321 loss)
I0522 22:52:56.354655 35003 sgd_solver.cpp:112] Iteration 84870, lr = 0.01
I0522 22:53:00.316927 35003 solver.cpp:239] Iteration 84880 (2.14386 iter/s, 4.66448s/10 iters), loss = 7.30807
I0522 22:53:00.316995 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30807 (* 1 = 7.30807 loss)
I0522 22:53:01.051107 35003 sgd_solver.cpp:112] Iteration 84880, lr = 0.01
I0522 22:53:03.385290 35003 solver.cpp:239] Iteration 84890 (3.25929 iter/s, 3.06815s/10 iters), loss = 8.30138
I0522 22:53:03.385336 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.30138 (* 1 = 8.30138 loss)
I0522 22:53:04.061996 35003 sgd_solver.cpp:112] Iteration 84890, lr = 0.01
I0522 22:53:07.401428 35003 solver.cpp:239] Iteration 84900 (2.49009 iter/s, 4.01592s/10 iters), loss = 6.98786
I0522 22:53:07.401469 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98786 (* 1 = 6.98786 loss)
I0522 22:53:07.771241 35003 sgd_solver.cpp:112] Iteration 84900, lr = 0.01
I0522 22:53:11.503677 35003 solver.cpp:239] Iteration 84910 (2.43781 iter/s, 4.10204s/10 iters), loss = 7.04932
I0522 22:53:11.503718 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04932 (* 1 = 7.04932 loss)
I0522 22:53:11.520897 35003 sgd_solver.cpp:112] Iteration 84910, lr = 0.01
I0522 22:53:15.203455 35003 solver.cpp:239] Iteration 84920 (2.70301 iter/s, 3.69958s/10 iters), loss = 7.89441
I0522 22:53:15.203737 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89441 (* 1 = 7.89441 loss)
I0522 22:53:15.211047 35003 sgd_solver.cpp:112] Iteration 84920, lr = 0.01
I0522 22:53:18.071386 35003 solver.cpp:239] Iteration 84930 (3.48728 iter/s, 2.86757s/10 iters), loss = 7.52104
I0522 22:53:18.071434 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52104 (* 1 = 7.52104 loss)
I0522 22:53:18.344419 35003 sgd_solver.cpp:112] Iteration 84930, lr = 0.01
I0522 22:53:21.193725 35003 solver.cpp:239] Iteration 84940 (3.20291 iter/s, 3.12216s/10 iters), loss = 6.57789
I0522 22:53:21.193771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57789 (* 1 = 6.57789 loss)
I0522 22:53:21.208477 35003 sgd_solver.cpp:112] Iteration 84940, lr = 0.01
I0522 22:53:25.514178 35003 solver.cpp:239] Iteration 84950 (2.31469 iter/s, 4.32023s/10 iters), loss = 8.47473
I0522 22:53:25.514230 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.47473 (* 1 = 8.47473 loss)
I0522 22:53:26.252877 35003 sgd_solver.cpp:112] Iteration 84950, lr = 0.01
I0522 22:53:29.642549 35003 solver.cpp:239] Iteration 84960 (2.42239 iter/s, 4.12815s/10 iters), loss = 8.00014
I0522 22:53:29.642594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00014 (* 1 = 8.00014 loss)
I0522 22:53:30.337359 35003 sgd_solver.cpp:112] Iteration 84960, lr = 0.01
I0522 22:53:33.194859 35003 solver.cpp:239] Iteration 84970 (2.81523 iter/s, 3.55211s/10 iters), loss = 7.7889
I0522 22:53:33.194913 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7889 (* 1 = 7.7889 loss)
I0522 22:53:33.919991 35003 sgd_solver.cpp:112] Iteration 84970, lr = 0.01
I0522 22:53:39.000259 35003 solver.cpp:239] Iteration 84980 (1.72262 iter/s, 5.80512s/10 iters), loss = 7.70171
I0522 22:53:39.000308 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70171 (* 1 = 7.70171 loss)
I0522 22:53:39.006258 35003 sgd_solver.cpp:112] Iteration 84980, lr = 0.01
I0522 22:53:42.739217 35003 solver.cpp:239] Iteration 84990 (2.67469 iter/s, 3.73875s/10 iters), loss = 6.1018
I0522 22:53:42.739269 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1018 (* 1 = 6.1018 loss)
I0522 22:53:43.473748 35003 sgd_solver.cpp:112] Iteration 84990, lr = 0.01
I0522 22:53:45.042742 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_85000.caffemodel
I0522 22:53:45.431318 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_85000.solverstate
I0522 22:53:45.636479 35003 solver.cpp:239] Iteration 85000 (3.45175 iter/s, 2.89708s/10 iters), loss = 6.67909
I0522 22:53:45.636554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67909 (* 1 = 6.67909 loss)
I0522 22:53:46.363991 35003 sgd_solver.cpp:112] Iteration 85000, lr = 0.01
I0522 22:53:50.002568 35003 solver.cpp:239] Iteration 85010 (2.29051 iter/s, 4.36584s/10 iters), loss = 7.23452
I0522 22:53:50.002624 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23452 (* 1 = 7.23452 loss)
I0522 22:53:50.083056 35003 sgd_solver.cpp:112] Iteration 85010, lr = 0.01
I0522 22:53:53.471107 35003 solver.cpp:239] Iteration 85020 (2.88323 iter/s, 3.46833s/10 iters), loss = 6.48592
I0522 22:53:53.471168 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48592 (* 1 = 6.48592 loss)
I0522 22:53:53.482772 35003 sgd_solver.cpp:112] Iteration 85020, lr = 0.01
I0522 22:53:58.350860 35003 solver.cpp:239] Iteration 85030 (2.04939 iter/s, 4.8795s/10 iters), loss = 7.04065
I0522 22:53:58.350900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04065 (* 1 = 7.04065 loss)
I0522 22:53:58.364025 35003 sgd_solver.cpp:112] Iteration 85030, lr = 0.01
I0522 22:54:01.178738 35003 solver.cpp:239] Iteration 85040 (3.53642 iter/s, 2.82771s/10 iters), loss = 7.36656
I0522 22:54:01.178786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36656 (* 1 = 7.36656 loss)
I0522 22:54:01.893916 35003 sgd_solver.cpp:112] Iteration 85040, lr = 0.01
I0522 22:54:04.694877 35003 solver.cpp:239] Iteration 85050 (2.84419 iter/s, 3.51594s/10 iters), loss = 7.86349
I0522 22:54:04.694927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86349 (* 1 = 7.86349 loss)
I0522 22:54:04.722127 35003 sgd_solver.cpp:112] Iteration 85050, lr = 0.01
I0522 22:54:07.646996 35003 solver.cpp:239] Iteration 85060 (3.38761 iter/s, 2.95194s/10 iters), loss = 6.76749
I0522 22:54:07.647045 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76749 (* 1 = 6.76749 loss)
I0522 22:54:08.182368 35003 sgd_solver.cpp:112] Iteration 85060, lr = 0.01
I0522 22:54:11.860504 35003 solver.cpp:239] Iteration 85070 (2.37347 iter/s, 4.21325s/10 iters), loss = 7.27285
I0522 22:54:11.860569 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27285 (* 1 = 7.27285 loss)
I0522 22:54:12.413942 35003 sgd_solver.cpp:112] Iteration 85070, lr = 0.01
I0522 22:54:14.239346 35003 solver.cpp:239] Iteration 85080 (4.20399 iter/s, 2.37869s/10 iters), loss = 7.80634
I0522 22:54:14.239385 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80634 (* 1 = 7.80634 loss)
I0522 22:54:14.247280 35003 sgd_solver.cpp:112] Iteration 85080, lr = 0.01
I0522 22:54:17.902309 35003 solver.cpp:239] Iteration 85090 (2.73017 iter/s, 3.66277s/10 iters), loss = 8.11086
I0522 22:54:17.902609 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.11086 (* 1 = 8.11086 loss)
I0522 22:54:17.926664 35003 sgd_solver.cpp:112] Iteration 85090, lr = 0.01
I0522 22:54:19.278307 35003 solver.cpp:239] Iteration 85100 (7.26921 iter/s, 1.37567s/10 iters), loss = 6.94907
I0522 22:54:19.278355 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94907 (* 1 = 6.94907 loss)
I0522 22:54:19.950321 35003 sgd_solver.cpp:112] Iteration 85100, lr = 0.01
I0522 22:54:23.837206 35003 solver.cpp:239] Iteration 85110 (2.19363 iter/s, 4.55866s/10 iters), loss = 7.35959
I0522 22:54:23.837262 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35959 (* 1 = 7.35959 loss)
I0522 22:54:23.925773 35003 sgd_solver.cpp:112] Iteration 85110, lr = 0.01
I0522 22:54:27.414829 35003 solver.cpp:239] Iteration 85120 (2.79532 iter/s, 3.57741s/10 iters), loss = 7.8856
I0522 22:54:27.414875 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8856 (* 1 = 7.8856 loss)
I0522 22:54:27.422307 35003 sgd_solver.cpp:112] Iteration 85120, lr = 0.01
I0522 22:54:30.232269 35003 solver.cpp:239] Iteration 85130 (3.54954 iter/s, 2.81726s/10 iters), loss = 7.71084
I0522 22:54:30.232326 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71084 (* 1 = 7.71084 loss)
I0522 22:54:30.567997 35003 sgd_solver.cpp:112] Iteration 85130, lr = 0.01
I0522 22:54:32.801676 35003 solver.cpp:239] Iteration 85140 (3.89221 iter/s, 2.56924s/10 iters), loss = 6.92122
I0522 22:54:32.801722 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92122 (* 1 = 6.92122 loss)
I0522 22:54:33.536897 35003 sgd_solver.cpp:112] Iteration 85140, lr = 0.01
I0522 22:54:35.610532 35003 solver.cpp:239] Iteration 85150 (3.56038 iter/s, 2.80869s/10 iters), loss = 8.17654
I0522 22:54:35.610571 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17654 (* 1 = 8.17654 loss)
I0522 22:54:35.614363 35003 sgd_solver.cpp:112] Iteration 85150, lr = 0.01
I0522 22:54:39.164577 35003 solver.cpp:239] Iteration 85160 (2.81385 iter/s, 3.55385s/10 iters), loss = 8.33654
I0522 22:54:39.164636 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.33654 (* 1 = 8.33654 loss)
I0522 22:54:39.859922 35003 sgd_solver.cpp:112] Iteration 85160, lr = 0.01
I0522 22:54:44.243245 35003 solver.cpp:239] Iteration 85170 (1.96913 iter/s, 5.0784s/10 iters), loss = 6.92581
I0522 22:54:44.243311 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92581 (* 1 = 6.92581 loss)
I0522 22:54:44.250591 35003 sgd_solver.cpp:112] Iteration 85170, lr = 0.01
I0522 22:54:48.337769 35003 solver.cpp:239] Iteration 85180 (2.44243 iter/s, 4.09428s/10 iters), loss = 6.71947
I0522 22:54:48.338043 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71947 (* 1 = 6.71947 loss)
I0522 22:54:49.078253 35003 sgd_solver.cpp:112] Iteration 85180, lr = 0.01
I0522 22:54:51.831591 35003 solver.cpp:239] Iteration 85190 (2.86251 iter/s, 3.49344s/10 iters), loss = 7.88709
I0522 22:54:51.831635 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88709 (* 1 = 7.88709 loss)
I0522 22:54:51.845031 35003 sgd_solver.cpp:112] Iteration 85190, lr = 0.01
I0522 22:54:53.894060 35003 solver.cpp:239] Iteration 85200 (4.84891 iter/s, 2.06232s/10 iters), loss = 7.30703
I0522 22:54:53.894109 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30703 (* 1 = 7.30703 loss)
I0522 22:54:53.907281 35003 sgd_solver.cpp:112] Iteration 85200, lr = 0.01
I0522 22:54:57.590384 35003 solver.cpp:239] Iteration 85210 (2.70554 iter/s, 3.69611s/10 iters), loss = 7.6439
I0522 22:54:57.590431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6439 (* 1 = 7.6439 loss)
I0522 22:54:57.615928 35003 sgd_solver.cpp:112] Iteration 85210, lr = 0.01
I0522 22:55:01.919858 35003 solver.cpp:239] Iteration 85220 (2.30987 iter/s, 4.32925s/10 iters), loss = 7.54406
I0522 22:55:01.919909 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54406 (* 1 = 7.54406 loss)
I0522 22:55:01.925596 35003 sgd_solver.cpp:112] Iteration 85220, lr = 0.01
I0522 22:55:04.048209 35003 solver.cpp:239] Iteration 85230 (4.69883 iter/s, 2.12819s/10 iters), loss = 6.17597
I0522 22:55:04.048259 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17597 (* 1 = 6.17597 loss)
I0522 22:55:04.055488 35003 sgd_solver.cpp:112] Iteration 85230, lr = 0.01
I0522 22:55:05.796581 35003 solver.cpp:239] Iteration 85240 (5.72011 iter/s, 1.74822s/10 iters), loss = 6.87159
I0522 22:55:05.796627 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87159 (* 1 = 6.87159 loss)
I0522 22:55:05.810535 35003 sgd_solver.cpp:112] Iteration 85240, lr = 0.01
I0522 22:55:08.662590 35003 solver.cpp:239] Iteration 85250 (3.48938 iter/s, 2.86584s/10 iters), loss = 7.08762
I0522 22:55:08.662637 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08762 (* 1 = 7.08762 loss)
I0522 22:55:09.377355 35003 sgd_solver.cpp:112] Iteration 85250, lr = 0.01
I0522 22:55:12.301045 35003 solver.cpp:239] Iteration 85260 (2.74857 iter/s, 3.63826s/10 iters), loss = 7.83089
I0522 22:55:12.301095 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83089 (* 1 = 7.83089 loss)
I0522 22:55:12.306243 35003 sgd_solver.cpp:112] Iteration 85260, lr = 0.01
I0522 22:55:15.043403 35003 solver.cpp:239] Iteration 85270 (3.64672 iter/s, 2.74219s/10 iters), loss = 7.64277
I0522 22:55:15.043442 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64277 (* 1 = 7.64277 loss)
I0522 22:55:15.056171 35003 sgd_solver.cpp:112] Iteration 85270, lr = 0.01
I0522 22:55:17.872490 35003 solver.cpp:239] Iteration 85280 (3.53491 iter/s, 2.82892s/10 iters), loss = 7.58484
I0522 22:55:17.872548 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58484 (* 1 = 7.58484 loss)
I0522 22:55:17.878973 35003 sgd_solver.cpp:112] Iteration 85280, lr = 0.01
I0522 22:55:22.281630 35003 solver.cpp:239] Iteration 85290 (2.26814 iter/s, 4.4089s/10 iters), loss = 6.89454
I0522 22:55:22.281884 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89454 (* 1 = 6.89454 loss)
I0522 22:55:22.306016 35003 sgd_solver.cpp:112] Iteration 85290, lr = 0.01
I0522 22:55:24.915268 35003 solver.cpp:239] Iteration 85300 (3.79753 iter/s, 2.63329s/10 iters), loss = 7.18217
I0522 22:55:24.915326 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18217 (* 1 = 7.18217 loss)
I0522 22:55:25.089212 35003 sgd_solver.cpp:112] Iteration 85300, lr = 0.01
I0522 22:55:28.581027 35003 solver.cpp:239] Iteration 85310 (2.7281 iter/s, 3.66555s/10 iters), loss = 7.78611
I0522 22:55:28.581066 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78611 (* 1 = 7.78611 loss)
I0522 22:55:28.594115 35003 sgd_solver.cpp:112] Iteration 85310, lr = 0.01
I0522 22:55:32.234290 35003 solver.cpp:239] Iteration 85320 (2.73743 iter/s, 3.65307s/10 iters), loss = 6.71117
I0522 22:55:32.234410 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71117 (* 1 = 6.71117 loss)
I0522 22:55:32.237658 35003 sgd_solver.cpp:112] Iteration 85320, lr = 0.01
I0522 22:55:35.038287 35003 solver.cpp:239] Iteration 85330 (3.56663 iter/s, 2.80376s/10 iters), loss = 6.76566
I0522 22:55:35.038324 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76566 (* 1 = 6.76566 loss)
I0522 22:55:35.050781 35003 sgd_solver.cpp:112] Iteration 85330, lr = 0.01
I0522 22:55:37.708576 35003 solver.cpp:239] Iteration 85340 (3.74517 iter/s, 2.6701s/10 iters), loss = 7.77735
I0522 22:55:37.708632 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77735 (* 1 = 7.77735 loss)
I0522 22:55:37.715854 35003 sgd_solver.cpp:112] Iteration 85340, lr = 0.01
I0522 22:55:41.431200 35003 solver.cpp:239] Iteration 85350 (2.68643 iter/s, 3.72241s/10 iters), loss = 7.15367
I0522 22:55:41.431241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15367 (* 1 = 7.15367 loss)
I0522 22:55:41.435950 35003 sgd_solver.cpp:112] Iteration 85350, lr = 0.01
I0522 22:55:43.520133 35003 solver.cpp:239] Iteration 85360 (4.78744 iter/s, 2.0888s/10 iters), loss = 7.81639
I0522 22:55:43.520182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81639 (* 1 = 7.81639 loss)
I0522 22:55:43.527878 35003 sgd_solver.cpp:112] Iteration 85360, lr = 0.01
I0522 22:55:48.414968 35003 solver.cpp:239] Iteration 85370 (2.04307 iter/s, 4.89459s/10 iters), loss = 6.97309
I0522 22:55:48.415009 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97309 (* 1 = 6.97309 loss)
I0522 22:55:48.427858 35003 sgd_solver.cpp:112] Iteration 85370, lr = 0.01
I0522 22:55:51.340628 35003 solver.cpp:239] Iteration 85380 (3.41823 iter/s, 2.92549s/10 iters), loss = 7.40288
I0522 22:55:51.340675 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40288 (* 1 = 7.40288 loss)
I0522 22:55:52.049861 35003 sgd_solver.cpp:112] Iteration 85380, lr = 0.01
I0522 22:55:56.119277 35003 solver.cpp:239] Iteration 85390 (2.09275 iter/s, 4.77841s/10 iters), loss = 7.65289
I0522 22:55:56.119555 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65289 (* 1 = 7.65289 loss)
I0522 22:55:56.132313 35003 sgd_solver.cpp:112] Iteration 85390, lr = 0.01
I0522 22:55:59.720230 35003 solver.cpp:239] Iteration 85400 (2.77736 iter/s, 3.60055s/10 iters), loss = 6.96665
I0522 22:55:59.720284 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96665 (* 1 = 6.96665 loss)
I0522 22:55:59.794570 35003 sgd_solver.cpp:112] Iteration 85400, lr = 0.01
I0522 22:56:03.442679 35003 solver.cpp:239] Iteration 85410 (2.68655 iter/s, 3.72225s/10 iters), loss = 6.46365
I0522 22:56:03.442745 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46365 (* 1 = 6.46365 loss)
I0522 22:56:03.455613 35003 sgd_solver.cpp:112] Iteration 85410, lr = 0.01
I0522 22:56:04.863790 35003 solver.cpp:239] Iteration 85420 (7.03741 iter/s, 1.42098s/10 iters), loss = 7.1563
I0522 22:56:04.863836 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1563 (* 1 = 7.1563 loss)
I0522 22:56:05.592532 35003 sgd_solver.cpp:112] Iteration 85420, lr = 0.01
I0522 22:56:07.663537 35003 solver.cpp:239] Iteration 85430 (3.57196 iter/s, 2.79958s/10 iters), loss = 7.06174
I0522 22:56:07.663584 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06174 (* 1 = 7.06174 loss)
I0522 22:56:07.677000 35003 sgd_solver.cpp:112] Iteration 85430, lr = 0.01
I0522 22:56:10.537744 35003 solver.cpp:239] Iteration 85440 (3.47944 iter/s, 2.87403s/10 iters), loss = 8.22296
I0522 22:56:10.537786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.22296 (* 1 = 8.22296 loss)
I0522 22:56:10.550979 35003 sgd_solver.cpp:112] Iteration 85440, lr = 0.01
I0522 22:56:13.878748 35003 solver.cpp:239] Iteration 85450 (2.99328 iter/s, 3.34082s/10 iters), loss = 6.71747
I0522 22:56:13.878805 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71747 (* 1 = 6.71747 loss)
I0522 22:56:14.612543 35003 sgd_solver.cpp:112] Iteration 85450, lr = 0.01
I0522 22:56:16.783118 35003 solver.cpp:239] Iteration 85460 (3.44331 iter/s, 2.90418s/10 iters), loss = 6.85871
I0522 22:56:16.783160 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85871 (* 1 = 6.85871 loss)
I0522 22:56:16.810266 35003 sgd_solver.cpp:112] Iteration 85460, lr = 0.01
I0522 22:56:20.329566 35003 solver.cpp:239] Iteration 85470 (2.8199 iter/s, 3.54623s/10 iters), loss = 7.99598
I0522 22:56:20.329604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99598 (* 1 = 7.99598 loss)
I0522 22:56:20.337443 35003 sgd_solver.cpp:112] Iteration 85470, lr = 0.01
I0522 22:56:23.269915 35003 solver.cpp:239] Iteration 85480 (3.40116 iter/s, 2.94017s/10 iters), loss = 7.15252
I0522 22:56:23.269982 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15252 (* 1 = 7.15252 loss)
I0522 22:56:23.985054 35003 sgd_solver.cpp:112] Iteration 85480, lr = 0.01
I0522 22:56:27.554867 35003 solver.cpp:239] Iteration 85490 (2.33388 iter/s, 4.28471s/10 iters), loss = 5.88151
I0522 22:56:27.555016 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.88151 (* 1 = 5.88151 loss)
I0522 22:56:27.568796 35003 sgd_solver.cpp:112] Iteration 85490, lr = 0.01
I0522 22:56:31.024408 35003 solver.cpp:239] Iteration 85500 (2.88247 iter/s, 3.46925s/10 iters), loss = 7.59387
I0522 22:56:31.024464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59387 (* 1 = 7.59387 loss)
I0522 22:56:31.032470 35003 sgd_solver.cpp:112] Iteration 85500, lr = 0.01
I0522 22:56:34.574723 35003 solver.cpp:239] Iteration 85510 (2.81683 iter/s, 3.55009s/10 iters), loss = 7.69389
I0522 22:56:34.574772 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69389 (* 1 = 7.69389 loss)
I0522 22:56:34.585777 35003 sgd_solver.cpp:112] Iteration 85510, lr = 0.01
I0522 22:56:37.465126 35003 solver.cpp:239] Iteration 85520 (3.45995 iter/s, 2.89022s/10 iters), loss = 6.95483
I0522 22:56:37.465205 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95483 (* 1 = 6.95483 loss)
I0522 22:56:37.472910 35003 sgd_solver.cpp:112] Iteration 85520, lr = 0.01
I0522 22:56:39.984892 35003 solver.cpp:239] Iteration 85530 (3.96892 iter/s, 2.51958s/10 iters), loss = 6.8825
I0522 22:56:39.984935 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8825 (* 1 = 6.8825 loss)
I0522 22:56:39.998293 35003 sgd_solver.cpp:112] Iteration 85530, lr = 0.01
I0522 22:56:44.419981 35003 solver.cpp:239] Iteration 85540 (2.25486 iter/s, 4.43486s/10 iters), loss = 8.98022
I0522 22:56:44.420018 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.98022 (* 1 = 8.98022 loss)
I0522 22:56:45.122198 35003 sgd_solver.cpp:112] Iteration 85540, lr = 0.01
I0522 22:56:48.761240 35003 solver.cpp:239] Iteration 85550 (2.30359 iter/s, 4.34105s/10 iters), loss = 6.96965
I0522 22:56:48.761277 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96965 (* 1 = 6.96965 loss)
I0522 22:56:48.777674 35003 sgd_solver.cpp:112] Iteration 85550, lr = 0.01
I0522 22:56:52.138556 35003 solver.cpp:239] Iteration 85560 (2.96109 iter/s, 3.37713s/10 iters), loss = 7.02151
I0522 22:56:52.138600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02151 (* 1 = 7.02151 loss)
I0522 22:56:52.153439 35003 sgd_solver.cpp:112] Iteration 85560, lr = 0.01
I0522 22:56:55.898654 35003 solver.cpp:239] Iteration 85570 (2.65966 iter/s, 3.75988s/10 iters), loss = 7.77527
I0522 22:56:55.898746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77527 (* 1 = 7.77527 loss)
I0522 22:56:56.529271 35003 sgd_solver.cpp:112] Iteration 85570, lr = 0.01
I0522 22:56:59.480763 35003 solver.cpp:239] Iteration 85580 (2.79184 iter/s, 3.58187s/10 iters), loss = 6.90187
I0522 22:56:59.480999 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90187 (* 1 = 6.90187 loss)
I0522 22:56:59.493857 35003 sgd_solver.cpp:112] Iteration 85580, lr = 0.01
I0522 22:57:04.814882 35003 solver.cpp:239] Iteration 85590 (1.87488 iter/s, 5.33367s/10 iters), loss = 7.60974
I0522 22:57:04.814939 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60974 (* 1 = 7.60974 loss)
I0522 22:57:04.827105 35003 sgd_solver.cpp:112] Iteration 85590, lr = 0.01
I0522 22:57:08.841492 35003 solver.cpp:239] Iteration 85600 (2.48361 iter/s, 4.02639s/10 iters), loss = 6.68297
I0522 22:57:08.841537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68297 (* 1 = 6.68297 loss)
I0522 22:57:08.866977 35003 sgd_solver.cpp:112] Iteration 85600, lr = 0.01
I0522 22:57:10.961122 35003 solver.cpp:239] Iteration 85610 (4.72333 iter/s, 2.11715s/10 iters), loss = 6.63343
I0522 22:57:10.961169 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63343 (* 1 = 6.63343 loss)
I0522 22:57:10.988852 35003 sgd_solver.cpp:112] Iteration 85610, lr = 0.01
I0522 22:57:12.998473 35003 solver.cpp:239] Iteration 85620 (4.90867 iter/s, 2.03721s/10 iters), loss = 7.21022
I0522 22:57:12.998513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21022 (* 1 = 7.21022 loss)
I0522 22:57:13.007896 35003 sgd_solver.cpp:112] Iteration 85620, lr = 0.01
I0522 22:57:15.945103 35003 solver.cpp:239] Iteration 85630 (3.3939 iter/s, 2.94646s/10 iters), loss = 7.20709
I0522 22:57:15.945153 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20709 (* 1 = 7.20709 loss)
I0522 22:57:16.139523 35003 sgd_solver.cpp:112] Iteration 85630, lr = 0.01
I0522 22:57:19.874591 35003 solver.cpp:239] Iteration 85640 (2.545 iter/s, 3.92927s/10 iters), loss = 8.08595
I0522 22:57:19.874651 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08595 (* 1 = 8.08595 loss)
I0522 22:57:19.887675 35003 sgd_solver.cpp:112] Iteration 85640, lr = 0.01
I0522 22:57:23.531170 35003 solver.cpp:239] Iteration 85650 (2.73496 iter/s, 3.65637s/10 iters), loss = 7.06285
I0522 22:57:23.531219 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06285 (* 1 = 7.06285 loss)
I0522 22:57:23.538818 35003 sgd_solver.cpp:112] Iteration 85650, lr = 0.01
I0522 22:57:26.914574 35003 solver.cpp:239] Iteration 85660 (2.95577 iter/s, 3.38322s/10 iters), loss = 6.94158
I0522 22:57:26.914615 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94158 (* 1 = 6.94158 loss)
I0522 22:57:26.921598 35003 sgd_solver.cpp:112] Iteration 85660, lr = 0.01
I0522 22:57:29.013278 35003 solver.cpp:239] Iteration 85670 (4.76515 iter/s, 2.09857s/10 iters), loss = 7.5854
I0522 22:57:29.013329 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5854 (* 1 = 7.5854 loss)
I0522 22:57:29.059121 35003 sgd_solver.cpp:112] Iteration 85670, lr = 0.01
I0522 22:57:33.307538 35003 solver.cpp:239] Iteration 85680 (2.32881 iter/s, 4.29403s/10 iters), loss = 7.40822
I0522 22:57:33.307839 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40822 (* 1 = 7.40822 loss)
I0522 22:57:33.316154 35003 sgd_solver.cpp:112] Iteration 85680, lr = 0.01
I0522 22:57:35.376286 35003 solver.cpp:239] Iteration 85690 (4.83469 iter/s, 2.06838s/10 iters), loss = 8.36529
I0522 22:57:35.376353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.36529 (* 1 = 8.36529 loss)
I0522 22:57:35.389035 35003 sgd_solver.cpp:112] Iteration 85690, lr = 0.01
I0522 22:57:38.235666 35003 solver.cpp:239] Iteration 85700 (3.49748 iter/s, 2.8592s/10 iters), loss = 7.46905
I0522 22:57:38.235710 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46905 (* 1 = 7.46905 loss)
I0522 22:57:38.241830 35003 sgd_solver.cpp:112] Iteration 85700, lr = 0.01
I0522 22:57:41.102531 35003 solver.cpp:239] Iteration 85710 (3.48833 iter/s, 2.8667s/10 iters), loss = 6.71893
I0522 22:57:41.102572 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71893 (* 1 = 6.71893 loss)
I0522 22:57:41.115797 35003 sgd_solver.cpp:112] Iteration 85710, lr = 0.01
I0522 22:57:44.649119 35003 solver.cpp:239] Iteration 85720 (2.81976 iter/s, 3.5464s/10 iters), loss = 6.99342
I0522 22:57:44.649163 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99342 (* 1 = 6.99342 loss)
I0522 22:57:44.668272 35003 sgd_solver.cpp:112] Iteration 85720, lr = 0.01
I0522 22:57:47.583369 35003 solver.cpp:239] Iteration 85730 (3.40823 iter/s, 2.93408s/10 iters), loss = 6.81503
I0522 22:57:47.583428 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81503 (* 1 = 6.81503 loss)
I0522 22:57:47.589366 35003 sgd_solver.cpp:112] Iteration 85730, lr = 0.01
I0522 22:57:51.045506 35003 solver.cpp:239] Iteration 85740 (2.88856 iter/s, 3.46194s/10 iters), loss = 7.81041
I0522 22:57:51.045547 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81041 (* 1 = 7.81041 loss)
I0522 22:57:51.058509 35003 sgd_solver.cpp:112] Iteration 85740, lr = 0.01
I0522 22:57:54.403401 35003 solver.cpp:239] Iteration 85750 (2.97822 iter/s, 3.35771s/10 iters), loss = 8.03659
I0522 22:57:54.403455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03659 (* 1 = 8.03659 loss)
I0522 22:57:54.408696 35003 sgd_solver.cpp:112] Iteration 85750, lr = 0.01
I0522 22:57:57.348073 35003 solver.cpp:239] Iteration 85760 (3.39617 iter/s, 2.94449s/10 iters), loss = 6.72405
I0522 22:57:57.348114 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72405 (* 1 = 6.72405 loss)
I0522 22:57:57.365633 35003 sgd_solver.cpp:112] Iteration 85760, lr = 0.01
I0522 22:57:58.947285 35003 solver.cpp:239] Iteration 85770 (6.25353 iter/s, 1.5991s/10 iters), loss = 7.99154
I0522 22:57:58.947332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99154 (* 1 = 7.99154 loss)
I0522 22:57:58.976529 35003 sgd_solver.cpp:112] Iteration 85770, lr = 0.01
I0522 22:58:02.535523 35003 solver.cpp:239] Iteration 85780 (2.78704 iter/s, 3.58804s/10 iters), loss = 6.3148
I0522 22:58:02.535578 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3148 (* 1 = 6.3148 loss)
I0522 22:58:02.542202 35003 sgd_solver.cpp:112] Iteration 85780, lr = 0.01
I0522 22:58:06.558106 35003 solver.cpp:239] Iteration 85790 (2.48611 iter/s, 4.02235s/10 iters), loss = 7.6639
I0522 22:58:06.558317 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6639 (* 1 = 7.6639 loss)
I0522 22:58:07.141017 35003 sgd_solver.cpp:112] Iteration 85790, lr = 0.01
I0522 22:58:08.870225 35003 solver.cpp:239] Iteration 85800 (4.32561 iter/s, 2.31181s/10 iters), loss = 8.1
I0522 22:58:08.870291 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.1 (* 1 = 8.1 loss)
I0522 22:58:09.571383 35003 sgd_solver.cpp:112] Iteration 85800, lr = 0.01
I0522 22:58:11.752317 35003 solver.cpp:239] Iteration 85810 (3.46992 iter/s, 2.88191s/10 iters), loss = 6.18326
I0522 22:58:11.752358 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18326 (* 1 = 6.18326 loss)
I0522 22:58:11.765702 35003 sgd_solver.cpp:112] Iteration 85810, lr = 0.01
I0522 22:58:16.014888 35003 solver.cpp:239] Iteration 85820 (2.34613 iter/s, 4.26234s/10 iters), loss = 6.56896
I0522 22:58:16.014933 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56896 (* 1 = 6.56896 loss)
I0522 22:58:16.032517 35003 sgd_solver.cpp:112] Iteration 85820, lr = 0.01
I0522 22:58:18.631225 35003 solver.cpp:239] Iteration 85830 (3.82238 iter/s, 2.61617s/10 iters), loss = 5.60062
I0522 22:58:18.631286 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.60062 (* 1 = 5.60062 loss)
I0522 22:58:18.653857 35003 sgd_solver.cpp:112] Iteration 85830, lr = 0.01
I0522 22:58:22.529541 35003 solver.cpp:239] Iteration 85840 (2.56535 iter/s, 3.8981s/10 iters), loss = 7.1217
I0522 22:58:22.529583 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1217 (* 1 = 7.1217 loss)
I0522 22:58:22.537845 35003 sgd_solver.cpp:112] Iteration 85840, lr = 0.01
I0522 22:58:26.156203 35003 solver.cpp:239] Iteration 85850 (2.75751 iter/s, 3.62646s/10 iters), loss = 8.57608
I0522 22:58:26.156258 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.57608 (* 1 = 8.57608 loss)
I0522 22:58:26.633608 35003 sgd_solver.cpp:112] Iteration 85850, lr = 0.01
I0522 22:58:30.077505 35003 solver.cpp:239] Iteration 85860 (2.55031 iter/s, 3.92109s/10 iters), loss = 7.98617
I0522 22:58:30.077548 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98617 (* 1 = 7.98617 loss)
I0522 22:58:30.786098 35003 sgd_solver.cpp:112] Iteration 85860, lr = 0.01
I0522 22:58:32.973206 35003 solver.cpp:239] Iteration 85870 (3.45359 iter/s, 2.89553s/10 iters), loss = 7.56308
I0522 22:58:32.973245 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56308 (* 1 = 7.56308 loss)
I0522 22:58:32.979225 35003 sgd_solver.cpp:112] Iteration 85870, lr = 0.01
I0522 22:58:34.620509 35003 solver.cpp:239] Iteration 85880 (6.071 iter/s, 1.64717s/10 iters), loss = 7.57572
I0522 22:58:34.620555 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57572 (* 1 = 7.57572 loss)
I0522 22:58:34.634235 35003 sgd_solver.cpp:112] Iteration 85880, lr = 0.01
I0522 22:58:39.407912 35003 solver.cpp:239] Iteration 85890 (2.08892 iter/s, 4.78716s/10 iters), loss = 7.70407
I0522 22:58:39.408192 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70407 (* 1 = 7.70407 loss)
I0522 22:58:39.415401 35003 sgd_solver.cpp:112] Iteration 85890, lr = 0.01
I0522 22:58:43.433846 35003 solver.cpp:239] Iteration 85900 (2.48416 iter/s, 4.02551s/10 iters), loss = 8.13689
I0522 22:58:43.433905 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.13689 (* 1 = 8.13689 loss)
I0522 22:58:43.438763 35003 sgd_solver.cpp:112] Iteration 85900, lr = 0.01
I0522 22:58:48.543063 35003 solver.cpp:239] Iteration 85910 (1.95735 iter/s, 5.10895s/10 iters), loss = 6.98567
I0522 22:58:48.543118 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98567 (* 1 = 6.98567 loss)
I0522 22:58:48.556012 35003 sgd_solver.cpp:112] Iteration 85910, lr = 0.01
I0522 22:58:51.322803 35003 solver.cpp:239] Iteration 85920 (3.59768 iter/s, 2.77957s/10 iters), loss = 7.73395
I0522 22:58:51.322849 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73395 (* 1 = 7.73395 loss)
I0522 22:58:52.057535 35003 sgd_solver.cpp:112] Iteration 85920, lr = 0.01
I0522 22:58:53.374606 35003 solver.cpp:239] Iteration 85930 (4.87409 iter/s, 2.05166s/10 iters), loss = 6.71061
I0522 22:58:53.374652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71061 (* 1 = 6.71061 loss)
I0522 22:58:53.387203 35003 sgd_solver.cpp:112] Iteration 85930, lr = 0.01
I0522 22:58:55.367282 35003 solver.cpp:239] Iteration 85940 (5.01872 iter/s, 1.99254s/10 iters), loss = 6.88327
I0522 22:58:55.367321 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88327 (* 1 = 6.88327 loss)
I0522 22:58:55.371665 35003 sgd_solver.cpp:112] Iteration 85940, lr = 0.01
I0522 22:58:58.250274 35003 solver.cpp:239] Iteration 85950 (3.46881 iter/s, 2.88283s/10 iters), loss = 7.058
I0522 22:58:58.250322 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.058 (* 1 = 7.058 loss)
I0522 22:58:58.256603 35003 sgd_solver.cpp:112] Iteration 85950, lr = 0.01
I0522 22:59:02.621516 35003 solver.cpp:239] Iteration 85960 (2.2878 iter/s, 4.37102s/10 iters), loss = 7.50202
I0522 22:59:02.621567 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50202 (* 1 = 7.50202 loss)
I0522 22:59:02.707589 35003 sgd_solver.cpp:112] Iteration 85960, lr = 0.01
I0522 22:59:06.930371 35003 solver.cpp:239] Iteration 85970 (2.32093 iter/s, 4.30861s/10 iters), loss = 6.26031
I0522 22:59:06.930433 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26031 (* 1 = 6.26031 loss)
I0522 22:59:06.966897 35003 sgd_solver.cpp:112] Iteration 85970, lr = 0.01
I0522 22:59:10.647212 35003 solver.cpp:239] Iteration 85980 (2.69061 iter/s, 3.71663s/10 iters), loss = 7.15622
I0522 22:59:10.647430 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15622 (* 1 = 7.15622 loss)
I0522 22:59:11.387694 35003 sgd_solver.cpp:112] Iteration 85980, lr = 0.01
I0522 22:59:14.210074 35003 solver.cpp:239] Iteration 85990 (2.807 iter/s, 3.56252s/10 iters), loss = 6.99492
I0522 22:59:14.210117 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99492 (* 1 = 6.99492 loss)
I0522 22:59:14.231302 35003 sgd_solver.cpp:112] Iteration 85990, lr = 0.01
I0522 22:59:17.486196 35003 solver.cpp:239] Iteration 86000 (3.05256 iter/s, 3.27594s/10 iters), loss = 7.31941
I0522 22:59:17.486249 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31941 (* 1 = 7.31941 loss)
I0522 22:59:18.097082 35003 sgd_solver.cpp:112] Iteration 86000, lr = 0.01
I0522 22:59:21.079972 35003 solver.cpp:239] Iteration 86010 (2.78275 iter/s, 3.59357s/10 iters), loss = 6.9619
I0522 22:59:21.080018 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9619 (* 1 = 6.9619 loss)
I0522 22:59:21.087224 35003 sgd_solver.cpp:112] Iteration 86010, lr = 0.01
I0522 22:59:23.092840 35003 solver.cpp:239] Iteration 86020 (4.96838 iter/s, 2.01273s/10 iters), loss = 6.56432
I0522 22:59:23.092880 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56432 (* 1 = 6.56432 loss)
I0522 22:59:23.100795 35003 sgd_solver.cpp:112] Iteration 86020, lr = 0.01
I0522 22:59:26.089598 35003 solver.cpp:239] Iteration 86030 (3.33713 iter/s, 2.99659s/10 iters), loss = 6.9095
I0522 22:59:26.089649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9095 (* 1 = 6.9095 loss)
I0522 22:59:26.549835 35003 sgd_solver.cpp:112] Iteration 86030, lr = 0.01
I0522 22:59:30.924460 35003 solver.cpp:239] Iteration 86040 (2.06842 iter/s, 4.83462s/10 iters), loss = 7.4101
I0522 22:59:30.924506 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4101 (* 1 = 7.4101 loss)
I0522 22:59:30.937507 35003 sgd_solver.cpp:112] Iteration 86040, lr = 0.01
I0522 22:59:34.553278 35003 solver.cpp:239] Iteration 86050 (2.75587 iter/s, 3.62862s/10 iters), loss = 6.96532
I0522 22:59:34.553316 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96532 (* 1 = 6.96532 loss)
I0522 22:59:34.560295 35003 sgd_solver.cpp:112] Iteration 86050, lr = 0.01
I0522 22:59:37.256935 35003 solver.cpp:239] Iteration 86060 (3.69892 iter/s, 2.70349s/10 iters), loss = 8.15069
I0522 22:59:37.256983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.15069 (* 1 = 8.15069 loss)
I0522 22:59:37.285890 35003 sgd_solver.cpp:112] Iteration 86060, lr = 0.01
I0522 22:59:40.893471 35003 solver.cpp:239] Iteration 86070 (2.75002 iter/s, 3.63633s/10 iters), loss = 7.09074
I0522 22:59:40.893772 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09074 (* 1 = 7.09074 loss)
I0522 22:59:40.902582 35003 sgd_solver.cpp:112] Iteration 86070, lr = 0.01
I0522 22:59:45.790767 35003 solver.cpp:239] Iteration 86080 (2.04214 iter/s, 4.89682s/10 iters), loss = 7.78182
I0522 22:59:45.790815 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78182 (* 1 = 7.78182 loss)
I0522 22:59:46.488420 35003 sgd_solver.cpp:112] Iteration 86080, lr = 0.01
I0522 22:59:50.076556 35003 solver.cpp:239] Iteration 86090 (2.33341 iter/s, 4.28557s/10 iters), loss = 8.4688
I0522 22:59:50.076604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.4688 (* 1 = 8.4688 loss)
I0522 22:59:50.090088 35003 sgd_solver.cpp:112] Iteration 86090, lr = 0.01
I0522 22:59:53.074872 35003 solver.cpp:239] Iteration 86100 (3.3354 iter/s, 2.99814s/10 iters), loss = 7.33976
I0522 22:59:53.074919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33976 (* 1 = 7.33976 loss)
I0522 22:59:53.078130 35003 sgd_solver.cpp:112] Iteration 86100, lr = 0.01
I0522 22:59:55.738979 35003 solver.cpp:239] Iteration 86110 (3.75383 iter/s, 2.66394s/10 iters), loss = 7.3726
I0522 22:59:55.739027 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3726 (* 1 = 7.3726 loss)
I0522 22:59:55.752699 35003 sgd_solver.cpp:112] Iteration 86110, lr = 0.01
I0522 22:59:58.463263 35003 solver.cpp:239] Iteration 86120 (3.67091 iter/s, 2.72412s/10 iters), loss = 7.03581
I0522 22:59:58.463305 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03581 (* 1 = 7.03581 loss)
I0522 22:59:58.472208 35003 sgd_solver.cpp:112] Iteration 86120, lr = 0.01
I0522 23:00:02.016201 35003 solver.cpp:239] Iteration 86130 (2.81472 iter/s, 3.55275s/10 iters), loss = 7.09719
I0522 23:00:02.016238 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09719 (* 1 = 7.09719 loss)
I0522 23:00:02.028821 35003 sgd_solver.cpp:112] Iteration 86130, lr = 0.01
I0522 23:00:04.720890 35003 solver.cpp:239] Iteration 86140 (3.69749 iter/s, 2.70454s/10 iters), loss = 7.27175
I0522 23:00:04.720940 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27175 (* 1 = 7.27175 loss)
I0522 23:00:04.733964 35003 sgd_solver.cpp:112] Iteration 86140, lr = 0.01
I0522 23:00:06.909335 35003 solver.cpp:239] Iteration 86150 (4.56979 iter/s, 2.18828s/10 iters), loss = 7.30071
I0522 23:00:06.909401 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30071 (* 1 = 7.30071 loss)
I0522 23:00:06.918251 35003 sgd_solver.cpp:112] Iteration 86150, lr = 0.01
I0522 23:00:10.576992 35003 solver.cpp:239] Iteration 86160 (2.72673 iter/s, 3.6674s/10 iters), loss = 7.58027
I0522 23:00:10.577039 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58027 (* 1 = 7.58027 loss)
I0522 23:00:10.582263 35003 sgd_solver.cpp:112] Iteration 86160, lr = 0.01
I0522 23:00:14.280706 35003 solver.cpp:239] Iteration 86170 (2.70014 iter/s, 3.70352s/10 iters), loss = 7.85949
I0522 23:00:14.281018 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85949 (* 1 = 7.85949 loss)
I0522 23:00:15.019279 35003 sgd_solver.cpp:112] Iteration 86170, lr = 0.01
I0522 23:00:18.712236 35003 solver.cpp:239] Iteration 86180 (2.25679 iter/s, 4.43106s/10 iters), loss = 6.69208
I0522 23:00:18.712294 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69208 (* 1 = 6.69208 loss)
I0522 23:00:18.719877 35003 sgd_solver.cpp:112] Iteration 86180, lr = 0.01
I0522 23:00:22.978648 35003 solver.cpp:239] Iteration 86190 (2.34402 iter/s, 4.26618s/10 iters), loss = 8.04825
I0522 23:00:22.978710 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04825 (* 1 = 8.04825 loss)
I0522 23:00:23.002511 35003 sgd_solver.cpp:112] Iteration 86190, lr = 0.01
I0522 23:00:28.040839 35003 solver.cpp:239] Iteration 86200 (1.97553 iter/s, 5.06193s/10 iters), loss = 7.06509
I0522 23:00:28.040886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06509 (* 1 = 7.06509 loss)
I0522 23:00:28.043362 35003 sgd_solver.cpp:112] Iteration 86200, lr = 0.01
I0522 23:00:32.230384 35003 solver.cpp:239] Iteration 86210 (2.38705 iter/s, 4.18926s/10 iters), loss = 7.44927
I0522 23:00:32.230437 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44927 (* 1 = 7.44927 loss)
I0522 23:00:32.242698 35003 sgd_solver.cpp:112] Iteration 86210, lr = 0.01
I0522 23:00:36.558615 35003 solver.cpp:239] Iteration 86220 (2.31053 iter/s, 4.328s/10 iters), loss = 5.84469
I0522 23:00:36.558653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84469 (* 1 = 5.84469 loss)
I0522 23:00:36.572988 35003 sgd_solver.cpp:112] Iteration 86220, lr = 0.01
I0522 23:00:39.406049 35003 solver.cpp:239] Iteration 86230 (3.51214 iter/s, 2.84727s/10 iters), loss = 7.3844
I0522 23:00:39.406113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3844 (* 1 = 7.3844 loss)
I0522 23:00:39.429354 35003 sgd_solver.cpp:112] Iteration 86230, lr = 0.01
I0522 23:00:44.574303 35003 solver.cpp:239] Iteration 86240 (1.93501 iter/s, 5.16794s/10 iters), loss = 6.78478
I0522 23:00:44.574492 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78478 (* 1 = 6.78478 loss)
I0522 23:00:44.578723 35003 sgd_solver.cpp:112] Iteration 86240, lr = 0.01
I0522 23:00:48.230434 35003 solver.cpp:239] Iteration 86250 (2.73539 iter/s, 3.65578s/10 iters), loss = 6.91565
I0522 23:00:48.230486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91565 (* 1 = 6.91565 loss)
I0522 23:00:48.654471 35003 sgd_solver.cpp:112] Iteration 86250, lr = 0.01
I0522 23:00:52.163517 35003 solver.cpp:239] Iteration 86260 (2.54268 iter/s, 3.93287s/10 iters), loss = 8.28237
I0522 23:00:52.163571 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.28237 (* 1 = 8.28237 loss)
I0522 23:00:52.170845 35003 sgd_solver.cpp:112] Iteration 86260, lr = 0.01
I0522 23:00:55.737649 35003 solver.cpp:239] Iteration 86270 (2.79804 iter/s, 3.57393s/10 iters), loss = 8.04527
I0522 23:00:55.737694 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04527 (* 1 = 8.04527 loss)
I0522 23:00:55.754720 35003 sgd_solver.cpp:112] Iteration 86270, lr = 0.01
I0522 23:00:59.025658 35003 solver.cpp:239] Iteration 86280 (3.04154 iter/s, 3.28781s/10 iters), loss = 7.17102
I0522 23:00:59.025707 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17102 (* 1 = 7.17102 loss)
I0522 23:00:59.031744 35003 sgd_solver.cpp:112] Iteration 86280, lr = 0.01
I0522 23:01:01.995306 35003 solver.cpp:239] Iteration 86290 (3.36762 iter/s, 2.96945s/10 iters), loss = 7.13349
I0522 23:01:01.995352 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13349 (* 1 = 7.13349 loss)
I0522 23:01:02.623555 35003 sgd_solver.cpp:112] Iteration 86290, lr = 0.01
I0522 23:01:04.682741 35003 solver.cpp:239] Iteration 86300 (3.72125 iter/s, 2.68727s/10 iters), loss = 7.15972
I0522 23:01:04.682780 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15972 (* 1 = 7.15972 loss)
I0522 23:01:04.704684 35003 sgd_solver.cpp:112] Iteration 86300, lr = 0.01
I0522 23:01:07.450320 35003 solver.cpp:239] Iteration 86310 (3.61349 iter/s, 2.76741s/10 iters), loss = 6.67626
I0522 23:01:07.450377 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67626 (* 1 = 6.67626 loss)
I0522 23:01:07.532928 35003 sgd_solver.cpp:112] Iteration 86310, lr = 0.01
I0522 23:01:10.790014 35003 solver.cpp:239] Iteration 86320 (2.99446 iter/s, 3.3395s/10 iters), loss = 7.43053
I0522 23:01:10.790057 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43053 (* 1 = 7.43053 loss)
I0522 23:01:10.798910 35003 sgd_solver.cpp:112] Iteration 86320, lr = 0.01
I0522 23:01:13.883486 35003 solver.cpp:239] Iteration 86330 (3.2328 iter/s, 3.09329s/10 iters), loss = 6.65445
I0522 23:01:13.883528 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65445 (* 1 = 6.65445 loss)
I0522 23:01:13.894388 35003 sgd_solver.cpp:112] Iteration 86330, lr = 0.01
I0522 23:01:18.238237 35003 solver.cpp:239] Iteration 86340 (2.29646 iter/s, 4.35453s/10 iters), loss = 7.98077
I0522 23:01:18.238487 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98077 (* 1 = 7.98077 loss)
I0522 23:01:18.251852 35003 sgd_solver.cpp:112] Iteration 86340, lr = 0.01
I0522 23:01:21.084213 35003 solver.cpp:239] Iteration 86350 (3.51419 iter/s, 2.84561s/10 iters), loss = 7.25158
I0522 23:01:21.084262 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25158 (* 1 = 7.25158 loss)
I0522 23:01:21.096473 35003 sgd_solver.cpp:112] Iteration 86350, lr = 0.01
I0522 23:01:26.735610 35003 solver.cpp:239] Iteration 86360 (1.76956 iter/s, 5.65111s/10 iters), loss = 6.25953
I0522 23:01:26.735651 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25953 (* 1 = 6.25953 loss)
I0522 23:01:26.748407 35003 sgd_solver.cpp:112] Iteration 86360, lr = 0.01
I0522 23:01:28.817669 35003 solver.cpp:239] Iteration 86370 (4.80325 iter/s, 2.08192s/10 iters), loss = 7.27249
I0522 23:01:28.817718 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27249 (* 1 = 7.27249 loss)
I0522 23:01:28.820755 35003 sgd_solver.cpp:112] Iteration 86370, lr = 0.01
I0522 23:01:32.310056 35003 solver.cpp:239] Iteration 86380 (2.86353 iter/s, 3.49219s/10 iters), loss = 8.20769
I0522 23:01:32.310114 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.20769 (* 1 = 8.20769 loss)
I0522 23:01:32.322880 35003 sgd_solver.cpp:112] Iteration 86380, lr = 0.01
I0522 23:01:35.188100 35003 solver.cpp:239] Iteration 86390 (3.47479 iter/s, 2.87787s/10 iters), loss = 7.21068
I0522 23:01:35.188145 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21068 (* 1 = 7.21068 loss)
I0522 23:01:35.858705 35003 sgd_solver.cpp:112] Iteration 86390, lr = 0.01
I0522 23:01:39.070338 35003 solver.cpp:239] Iteration 86400 (2.576 iter/s, 3.88199s/10 iters), loss = 7.95608
I0522 23:01:39.070392 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95608 (* 1 = 7.95608 loss)
I0522 23:01:39.084398 35003 sgd_solver.cpp:112] Iteration 86400, lr = 0.01
I0522 23:01:41.172564 35003 solver.cpp:239] Iteration 86410 (4.75721 iter/s, 2.10207s/10 iters), loss = 9.65731
I0522 23:01:41.172642 35003 solver.cpp:258]     Train net output #0: softmax_loss = 9.65731 (* 1 = 9.65731 loss)
I0522 23:01:41.874179 35003 sgd_solver.cpp:112] Iteration 86410, lr = 0.01
I0522 23:01:44.802520 35003 solver.cpp:239] Iteration 86420 (2.75503 iter/s, 3.62973s/10 iters), loss = 6.82596
I0522 23:01:44.802562 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82596 (* 1 = 6.82596 loss)
I0522 23:01:45.543470 35003 sgd_solver.cpp:112] Iteration 86420, lr = 0.01
I0522 23:01:49.866137 35003 solver.cpp:239] Iteration 86430 (1.97497 iter/s, 5.06337s/10 iters), loss = 6.79794
I0522 23:01:49.866415 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79794 (* 1 = 6.79794 loss)
I0522 23:01:49.879484 35003 sgd_solver.cpp:112] Iteration 86430, lr = 0.01
I0522 23:01:54.179553 35003 solver.cpp:239] Iteration 86440 (2.31858 iter/s, 4.31299s/10 iters), loss = 7.81728
I0522 23:01:54.179590 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81728 (* 1 = 7.81728 loss)
I0522 23:01:54.198050 35003 sgd_solver.cpp:112] Iteration 86440, lr = 0.01
I0522 23:01:57.053047 35003 solver.cpp:239] Iteration 86450 (3.48559 iter/s, 2.86895s/10 iters), loss = 7.28725
I0522 23:01:57.053098 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28725 (* 1 = 7.28725 loss)
I0522 23:01:57.058007 35003 sgd_solver.cpp:112] Iteration 86450, lr = 0.01
I0522 23:01:59.757344 35003 solver.cpp:239] Iteration 86460 (3.69808 iter/s, 2.70411s/10 iters), loss = 8.04762
I0522 23:01:59.757391 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04762 (* 1 = 8.04762 loss)
I0522 23:01:59.761588 35003 sgd_solver.cpp:112] Iteration 86460, lr = 0.01
I0522 23:02:02.464395 35003 solver.cpp:239] Iteration 86470 (3.69431 iter/s, 2.70687s/10 iters), loss = 7.13893
I0522 23:02:02.464474 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13893 (* 1 = 7.13893 loss)
I0522 23:02:02.488243 35003 sgd_solver.cpp:112] Iteration 86470, lr = 0.01
I0522 23:02:04.551496 35003 solver.cpp:239] Iteration 86480 (4.79172 iter/s, 2.08693s/10 iters), loss = 6.68581
I0522 23:02:04.551548 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68581 (* 1 = 6.68581 loss)
I0522 23:02:04.563161 35003 sgd_solver.cpp:112] Iteration 86480, lr = 0.01
I0522 23:02:07.407627 35003 solver.cpp:239] Iteration 86490 (3.50145 iter/s, 2.85596s/10 iters), loss = 7.58489
I0522 23:02:07.407678 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58489 (* 1 = 7.58489 loss)
I0522 23:02:07.413362 35003 sgd_solver.cpp:112] Iteration 86490, lr = 0.01
I0522 23:02:09.555694 35003 solver.cpp:239] Iteration 86500 (4.65565 iter/s, 2.14793s/10 iters), loss = 7.7536
I0522 23:02:09.555744 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7536 (* 1 = 7.7536 loss)
I0522 23:02:10.293633 35003 sgd_solver.cpp:112] Iteration 86500, lr = 0.01
I0522 23:02:13.153998 35003 solver.cpp:239] Iteration 86510 (2.77925 iter/s, 3.5981s/10 iters), loss = 6.63005
I0522 23:02:13.154062 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63005 (* 1 = 6.63005 loss)
I0522 23:02:13.862272 35003 sgd_solver.cpp:112] Iteration 86510, lr = 0.01
I0522 23:02:17.410650 35003 solver.cpp:239] Iteration 86520 (2.3494 iter/s, 4.25641s/10 iters), loss = 7.50568
I0522 23:02:17.410713 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50568 (* 1 = 7.50568 loss)
I0522 23:02:17.995424 35003 sgd_solver.cpp:112] Iteration 86520, lr = 0.01
I0522 23:02:19.939378 35003 solver.cpp:239] Iteration 86530 (3.95481 iter/s, 2.52857s/10 iters), loss = 7.86475
I0522 23:02:19.939579 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86475 (* 1 = 7.86475 loss)
I0522 23:02:19.952016 35003 sgd_solver.cpp:112] Iteration 86530, lr = 0.01
I0522 23:02:23.957664 35003 solver.cpp:239] Iteration 86540 (2.48884 iter/s, 4.01793s/10 iters), loss = 6.21715
I0522 23:02:23.957708 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21715 (* 1 = 6.21715 loss)
I0522 23:02:24.698110 35003 sgd_solver.cpp:112] Iteration 86540, lr = 0.01
I0522 23:02:28.830562 35003 solver.cpp:239] Iteration 86550 (2.05227 iter/s, 4.87264s/10 iters), loss = 8.26193
I0522 23:02:28.830622 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.26193 (* 1 = 8.26193 loss)
I0522 23:02:28.962098 35003 sgd_solver.cpp:112] Iteration 86550, lr = 0.01
I0522 23:02:32.759064 35003 solver.cpp:239] Iteration 86560 (2.54564 iter/s, 3.92829s/10 iters), loss = 6.23468
I0522 23:02:32.759104 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23468 (* 1 = 6.23468 loss)
I0522 23:02:32.929499 35003 sgd_solver.cpp:112] Iteration 86560, lr = 0.01
I0522 23:02:36.185017 35003 solver.cpp:239] Iteration 86570 (2.91905 iter/s, 3.42577s/10 iters), loss = 7.85596
I0522 23:02:36.185075 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85596 (* 1 = 7.85596 loss)
I0522 23:02:36.198170 35003 sgd_solver.cpp:112] Iteration 86570, lr = 0.01
I0522 23:02:39.755751 35003 solver.cpp:239] Iteration 86580 (2.80071 iter/s, 3.57052s/10 iters), loss = 7.87201
I0522 23:02:39.755811 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87201 (* 1 = 7.87201 loss)
I0522 23:02:39.760205 35003 sgd_solver.cpp:112] Iteration 86580, lr = 0.01
I0522 23:02:42.653115 35003 solver.cpp:239] Iteration 86590 (3.45163 iter/s, 2.89718s/10 iters), loss = 7.58514
I0522 23:02:42.653157 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58514 (* 1 = 7.58514 loss)
I0522 23:02:43.367835 35003 sgd_solver.cpp:112] Iteration 86590, lr = 0.01
I0522 23:02:47.370128 35003 solver.cpp:239] Iteration 86600 (2.12009 iter/s, 4.71678s/10 iters), loss = 6.58692
I0522 23:02:47.370172 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58692 (* 1 = 6.58692 loss)
I0522 23:02:47.382264 35003 sgd_solver.cpp:112] Iteration 86600, lr = 0.01
I0522 23:02:50.163998 35003 solver.cpp:239] Iteration 86610 (3.57947 iter/s, 2.79371s/10 iters), loss = 7.35628
I0522 23:02:50.164297 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35628 (* 1 = 7.35628 loss)
I0522 23:02:50.175199 35003 sgd_solver.cpp:112] Iteration 86610, lr = 0.01
I0522 23:02:53.533529 35003 solver.cpp:239] Iteration 86620 (2.96813 iter/s, 3.36912s/10 iters), loss = 7.23118
I0522 23:02:53.533565 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23118 (* 1 = 7.23118 loss)
I0522 23:02:53.546813 35003 sgd_solver.cpp:112] Iteration 86620, lr = 0.01
I0522 23:02:56.232599 35003 solver.cpp:239] Iteration 86630 (3.70519 iter/s, 2.69892s/10 iters), loss = 7.07663
I0522 23:02:56.232635 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07663 (* 1 = 7.07663 loss)
I0522 23:02:56.276310 35003 sgd_solver.cpp:112] Iteration 86630, lr = 0.01
I0522 23:02:59.944494 35003 solver.cpp:239] Iteration 86640 (2.69418 iter/s, 3.7117s/10 iters), loss = 5.91866
I0522 23:02:59.944540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91866 (* 1 = 5.91866 loss)
I0522 23:02:59.949487 35003 sgd_solver.cpp:112] Iteration 86640, lr = 0.01
I0522 23:03:03.560988 35003 solver.cpp:239] Iteration 86650 (2.76526 iter/s, 3.6163s/10 iters), loss = 6.52337
I0522 23:03:03.561035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52337 (* 1 = 6.52337 loss)
I0522 23:03:03.578997 35003 sgd_solver.cpp:112] Iteration 86650, lr = 0.01
I0522 23:03:07.247337 35003 solver.cpp:239] Iteration 86660 (2.71286 iter/s, 3.68615s/10 iters), loss = 7.02605
I0522 23:03:07.247378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02605 (* 1 = 7.02605 loss)
I0522 23:03:07.968858 35003 sgd_solver.cpp:112] Iteration 86660, lr = 0.01
I0522 23:03:11.533910 35003 solver.cpp:239] Iteration 86670 (2.33298 iter/s, 4.28636s/10 iters), loss = 7.72148
I0522 23:03:11.533951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72148 (* 1 = 7.72148 loss)
I0522 23:03:11.555673 35003 sgd_solver.cpp:112] Iteration 86670, lr = 0.01
I0522 23:03:15.556699 35003 solver.cpp:239] Iteration 86680 (2.48596 iter/s, 4.02258s/10 iters), loss = 7.39444
I0522 23:03:15.556746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39444 (* 1 = 7.39444 loss)
I0522 23:03:15.570204 35003 sgd_solver.cpp:112] Iteration 86680, lr = 0.01
I0522 23:03:18.770278 35003 solver.cpp:239] Iteration 86690 (3.11198 iter/s, 3.21339s/10 iters), loss = 6.75351
I0522 23:03:18.770344 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75351 (* 1 = 6.75351 loss)
I0522 23:03:18.778015 35003 sgd_solver.cpp:112] Iteration 86690, lr = 0.01
I0522 23:03:21.649296 35003 solver.cpp:239] Iteration 86700 (3.47363 iter/s, 2.87883s/10 iters), loss = 6.82252
I0522 23:03:21.649511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82252 (* 1 = 6.82252 loss)
I0522 23:03:21.662514 35003 sgd_solver.cpp:112] Iteration 86700, lr = 0.01
I0522 23:03:25.193020 35003 solver.cpp:239] Iteration 86710 (2.82218 iter/s, 3.54336s/10 iters), loss = 7.69286
I0522 23:03:25.193069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69286 (* 1 = 7.69286 loss)
I0522 23:03:25.205631 35003 sgd_solver.cpp:112] Iteration 86710, lr = 0.01
I0522 23:03:28.905534 35003 solver.cpp:239] Iteration 86720 (2.69374 iter/s, 3.71231s/10 iters), loss = 7.38751
I0522 23:03:28.905580 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38751 (* 1 = 7.38751 loss)
I0522 23:03:29.516762 35003 sgd_solver.cpp:112] Iteration 86720, lr = 0.01
I0522 23:03:32.287860 35003 solver.cpp:239] Iteration 86730 (2.95671 iter/s, 3.38214s/10 iters), loss = 7.48333
I0522 23:03:32.287907 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48333 (* 1 = 7.48333 loss)
I0522 23:03:32.464247 35003 sgd_solver.cpp:112] Iteration 86730, lr = 0.01
I0522 23:03:34.558168 35003 solver.cpp:239] Iteration 86740 (4.40497 iter/s, 2.27016s/10 iters), loss = 7.91782
I0522 23:03:34.558220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91782 (* 1 = 7.91782 loss)
I0522 23:03:34.577723 35003 sgd_solver.cpp:112] Iteration 86740, lr = 0.01
I0522 23:03:37.347908 35003 solver.cpp:239] Iteration 86750 (3.58479 iter/s, 2.78957s/10 iters), loss = 7.27883
I0522 23:03:37.347955 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27883 (* 1 = 7.27883 loss)
I0522 23:03:37.361177 35003 sgd_solver.cpp:112] Iteration 86750, lr = 0.01
I0522 23:03:41.039221 35003 solver.cpp:239] Iteration 86760 (2.70922 iter/s, 3.6911s/10 iters), loss = 7.0044
I0522 23:03:41.039276 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0044 (* 1 = 7.0044 loss)
I0522 23:03:41.214114 35003 sgd_solver.cpp:112] Iteration 86760, lr = 0.01
I0522 23:03:45.008108 35003 solver.cpp:239] Iteration 86770 (2.51973 iter/s, 3.96867s/10 iters), loss = 8.13385
I0522 23:03:45.008148 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.13385 (* 1 = 8.13385 loss)
I0522 23:03:45.021692 35003 sgd_solver.cpp:112] Iteration 86770, lr = 0.01
I0522 23:03:48.587211 35003 solver.cpp:239] Iteration 86780 (2.79415 iter/s, 3.57891s/10 iters), loss = 7.66146
I0522 23:03:48.587261 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66146 (* 1 = 7.66146 loss)
I0522 23:03:48.608856 35003 sgd_solver.cpp:112] Iteration 86780, lr = 0.01
I0522 23:03:51.461519 35003 solver.cpp:239] Iteration 86790 (3.47931 iter/s, 2.87413s/10 iters), loss = 7.99087
I0522 23:03:51.461561 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99087 (* 1 = 7.99087 loss)
I0522 23:03:51.476089 35003 sgd_solver.cpp:112] Iteration 86790, lr = 0.01
I0522 23:03:55.639004 35003 solver.cpp:239] Iteration 86800 (2.3939 iter/s, 4.17728s/10 iters), loss = 7.34752
I0522 23:03:55.639124 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34752 (* 1 = 7.34752 loss)
I0522 23:03:55.662839 35003 sgd_solver.cpp:112] Iteration 86800, lr = 0.01
I0522 23:03:57.788617 35003 solver.cpp:239] Iteration 86810 (4.65289 iter/s, 2.1492s/10 iters), loss = 7.55555
I0522 23:03:57.788672 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55555 (* 1 = 7.55555 loss)
I0522 23:03:58.364316 35003 sgd_solver.cpp:112] Iteration 86810, lr = 0.01
I0522 23:04:03.335372 35003 solver.cpp:239] Iteration 86820 (1.80295 iter/s, 5.54648s/10 iters), loss = 7.29908
I0522 23:04:03.335415 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29908 (* 1 = 7.29908 loss)
I0522 23:04:03.347440 35003 sgd_solver.cpp:112] Iteration 86820, lr = 0.01
I0522 23:04:05.468050 35003 solver.cpp:239] Iteration 86830 (4.68924 iter/s, 2.13254s/10 iters), loss = 7.06795
I0522 23:04:05.468089 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06795 (* 1 = 7.06795 loss)
I0522 23:04:05.475173 35003 sgd_solver.cpp:112] Iteration 86830, lr = 0.01
I0522 23:04:08.219467 35003 solver.cpp:239] Iteration 86840 (3.6347 iter/s, 2.75126s/10 iters), loss = 7.72774
I0522 23:04:08.219509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72774 (* 1 = 7.72774 loss)
I0522 23:04:08.232487 35003 sgd_solver.cpp:112] Iteration 86840, lr = 0.01
I0522 23:04:11.130131 35003 solver.cpp:239] Iteration 86850 (3.43583 iter/s, 2.9105s/10 iters), loss = 8.52474
I0522 23:04:11.130175 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.52474 (* 1 = 8.52474 loss)
I0522 23:04:11.135509 35003 sgd_solver.cpp:112] Iteration 86850, lr = 0.01
I0522 23:04:13.844175 35003 solver.cpp:239] Iteration 86860 (3.68475 iter/s, 2.71388s/10 iters), loss = 7.16512
I0522 23:04:13.844224 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16512 (* 1 = 7.16512 loss)
I0522 23:04:13.852324 35003 sgd_solver.cpp:112] Iteration 86860, lr = 0.01
I0522 23:04:18.146996 35003 solver.cpp:239] Iteration 86870 (2.32418 iter/s, 4.30259s/10 iters), loss = 5.8654
I0522 23:04:18.147053 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8654 (* 1 = 5.8654 loss)
I0522 23:04:18.881458 35003 sgd_solver.cpp:112] Iteration 86870, lr = 0.01
I0522 23:04:21.722923 35003 solver.cpp:239] Iteration 86880 (2.79665 iter/s, 3.57571s/10 iters), loss = 8.22136
I0522 23:04:21.722983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.22136 (* 1 = 8.22136 loss)
I0522 23:04:22.457268 35003 sgd_solver.cpp:112] Iteration 86880, lr = 0.01
I0522 23:04:23.891279 35003 solver.cpp:239] Iteration 86890 (4.61212 iter/s, 2.1682s/10 iters), loss = 7.7061
I0522 23:04:23.891333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7061 (* 1 = 7.7061 loss)
I0522 23:04:23.904464 35003 sgd_solver.cpp:112] Iteration 86890, lr = 0.01
I0522 23:04:27.333895 35003 solver.cpp:239] Iteration 86900 (2.90494 iter/s, 3.44242s/10 iters), loss = 7.26628
I0522 23:04:27.334177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26628 (* 1 = 7.26628 loss)
I0522 23:04:27.345336 35003 sgd_solver.cpp:112] Iteration 86900, lr = 0.01
I0522 23:04:30.850054 35003 solver.cpp:239] Iteration 86910 (2.84433 iter/s, 3.51576s/10 iters), loss = 6.51501
I0522 23:04:30.850098 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51501 (* 1 = 6.51501 loss)
I0522 23:04:31.531036 35003 sgd_solver.cpp:112] Iteration 86910, lr = 0.01
I0522 23:04:34.303617 35003 solver.cpp:239] Iteration 86920 (2.89573 iter/s, 3.45337s/10 iters), loss = 6.70437
I0522 23:04:34.303659 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70437 (* 1 = 6.70437 loss)
I0522 23:04:34.321552 35003 sgd_solver.cpp:112] Iteration 86920, lr = 0.01
I0522 23:04:39.343551 35003 solver.cpp:239] Iteration 86930 (1.98425 iter/s, 5.03968s/10 iters), loss = 6.77775
I0522 23:04:39.343592 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77775 (* 1 = 6.77775 loss)
I0522 23:04:39.365067 35003 sgd_solver.cpp:112] Iteration 86930, lr = 0.01
I0522 23:04:42.138741 35003 solver.cpp:239] Iteration 86940 (3.5778 iter/s, 2.79502s/10 iters), loss = 7.01276
I0522 23:04:42.138814 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01276 (* 1 = 7.01276 loss)
I0522 23:04:42.152258 35003 sgd_solver.cpp:112] Iteration 86940, lr = 0.01
I0522 23:04:45.908905 35003 solver.cpp:239] Iteration 86950 (2.65257 iter/s, 3.76992s/10 iters), loss = 7.01683
I0522 23:04:45.908948 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01683 (* 1 = 7.01683 loss)
I0522 23:04:45.912735 35003 sgd_solver.cpp:112] Iteration 86950, lr = 0.01
I0522 23:04:49.677605 35003 solver.cpp:239] Iteration 86960 (2.65358 iter/s, 3.7685s/10 iters), loss = 8.45847
I0522 23:04:49.677646 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.45847 (* 1 = 8.45847 loss)
I0522 23:04:49.685148 35003 sgd_solver.cpp:112] Iteration 86960, lr = 0.01
I0522 23:04:52.398957 35003 solver.cpp:239] Iteration 86970 (3.67485 iter/s, 2.7212s/10 iters), loss = 7.59757
I0522 23:04:52.399001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59757 (* 1 = 7.59757 loss)
I0522 23:04:52.412545 35003 sgd_solver.cpp:112] Iteration 86970, lr = 0.01
I0522 23:04:54.943559 35003 solver.cpp:239] Iteration 86980 (3.93014 iter/s, 2.54444s/10 iters), loss = 7.18899
I0522 23:04:54.943621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18899 (* 1 = 7.18899 loss)
I0522 23:04:54.956468 35003 sgd_solver.cpp:112] Iteration 86980, lr = 0.01
I0522 23:04:58.692245 35003 solver.cpp:239] Iteration 86990 (2.66776 iter/s, 3.74846s/10 iters), loss = 6.97083
I0522 23:04:58.692536 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97083 (* 1 = 6.97083 loss)
I0522 23:04:59.422588 35003 sgd_solver.cpp:112] Iteration 86990, lr = 0.01
I0522 23:05:03.619973 35003 solver.cpp:239] Iteration 87000 (2.02953 iter/s, 4.92725s/10 iters), loss = 7.40312
I0522 23:05:03.620023 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40312 (* 1 = 7.40312 loss)
I0522 23:05:03.627758 35003 sgd_solver.cpp:112] Iteration 87000, lr = 0.01
I0522 23:05:06.386726 35003 solver.cpp:239] Iteration 87010 (3.61456 iter/s, 2.76659s/10 iters), loss = 7.01742
I0522 23:05:06.386765 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01742 (* 1 = 7.01742 loss)
I0522 23:05:06.394826 35003 sgd_solver.cpp:112] Iteration 87010, lr = 0.01
I0522 23:05:09.332273 35003 solver.cpp:239] Iteration 87020 (3.39516 iter/s, 2.94537s/10 iters), loss = 7.73089
I0522 23:05:09.332339 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73089 (* 1 = 7.73089 loss)
I0522 23:05:10.066577 35003 sgd_solver.cpp:112] Iteration 87020, lr = 0.01
I0522 23:05:12.007655 35003 solver.cpp:239] Iteration 87030 (3.73803 iter/s, 2.67521s/10 iters), loss = 7.56358
I0522 23:05:12.007699 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56358 (* 1 = 7.56358 loss)
I0522 23:05:12.029281 35003 sgd_solver.cpp:112] Iteration 87030, lr = 0.01
I0522 23:05:13.821748 35003 solver.cpp:239] Iteration 87040 (5.51278 iter/s, 1.81397s/10 iters), loss = 8.53719
I0522 23:05:13.821799 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.53719 (* 1 = 8.53719 loss)
I0522 23:05:14.530326 35003 sgd_solver.cpp:112] Iteration 87040, lr = 0.01
I0522 23:05:16.333817 35003 solver.cpp:239] Iteration 87050 (3.98105 iter/s, 2.5119s/10 iters), loss = 7.55231
I0522 23:05:16.333884 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55231 (* 1 = 7.55231 loss)
I0522 23:05:17.071782 35003 sgd_solver.cpp:112] Iteration 87050, lr = 0.01
I0522 23:05:20.719724 35003 solver.cpp:239] Iteration 87060 (2.28016 iter/s, 4.38566s/10 iters), loss = 6.2325
I0522 23:05:20.719779 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2325 (* 1 = 6.2325 loss)
I0522 23:05:21.460587 35003 sgd_solver.cpp:112] Iteration 87060, lr = 0.01
I0522 23:05:25.126170 35003 solver.cpp:239] Iteration 87070 (2.26953 iter/s, 4.40621s/10 iters), loss = 6.66065
I0522 23:05:25.126211 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66065 (* 1 = 6.66065 loss)
I0522 23:05:25.155594 35003 sgd_solver.cpp:112] Iteration 87070, lr = 0.01
I0522 23:05:28.824998 35003 solver.cpp:239] Iteration 87080 (2.7037 iter/s, 3.69863s/10 iters), loss = 7.18213
I0522 23:05:28.825239 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18213 (* 1 = 7.18213 loss)
I0522 23:05:29.188590 35003 sgd_solver.cpp:112] Iteration 87080, lr = 0.01
I0522 23:05:32.844020 35003 solver.cpp:239] Iteration 87090 (2.4884 iter/s, 4.01865s/10 iters), loss = 8.14846
I0522 23:05:32.844063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.14846 (* 1 = 8.14846 loss)
I0522 23:05:32.852310 35003 sgd_solver.cpp:112] Iteration 87090, lr = 0.01
I0522 23:05:36.992969 35003 solver.cpp:239] Iteration 87100 (2.41038 iter/s, 4.14873s/10 iters), loss = 7.2546
I0522 23:05:36.993022 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2546 (* 1 = 7.2546 loss)
I0522 23:05:37.006543 35003 sgd_solver.cpp:112] Iteration 87100, lr = 0.01
I0522 23:05:39.798840 35003 solver.cpp:239] Iteration 87110 (3.56418 iter/s, 2.80569s/10 iters), loss = 7.22487
I0522 23:05:39.798882 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22487 (* 1 = 7.22487 loss)
I0522 23:05:39.821660 35003 sgd_solver.cpp:112] Iteration 87110, lr = 0.01
I0522 23:05:44.271114 35003 solver.cpp:239] Iteration 87120 (2.23611 iter/s, 4.47205s/10 iters), loss = 7.52935
I0522 23:05:44.271158 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52935 (* 1 = 7.52935 loss)
I0522 23:05:44.297062 35003 sgd_solver.cpp:112] Iteration 87120, lr = 0.01
I0522 23:05:46.385627 35003 solver.cpp:239] Iteration 87130 (4.72955 iter/s, 2.11437s/10 iters), loss = 7.28401
I0522 23:05:46.385689 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28401 (* 1 = 7.28401 loss)
I0522 23:05:46.390905 35003 sgd_solver.cpp:112] Iteration 87130, lr = 0.01
I0522 23:05:51.074815 35003 solver.cpp:239] Iteration 87140 (2.13269 iter/s, 4.68892s/10 iters), loss = 7.39142
I0522 23:05:51.074873 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39142 (* 1 = 7.39142 loss)
I0522 23:05:51.081244 35003 sgd_solver.cpp:112] Iteration 87140, lr = 0.01
I0522 23:05:53.872884 35003 solver.cpp:239] Iteration 87150 (3.57413 iter/s, 2.79789s/10 iters), loss = 7.3564
I0522 23:05:53.872943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3564 (* 1 = 7.3564 loss)
I0522 23:05:54.568079 35003 sgd_solver.cpp:112] Iteration 87150, lr = 0.01
I0522 23:05:58.252933 35003 solver.cpp:239] Iteration 87160 (2.28323 iter/s, 4.37975s/10 iters), loss = 7.13979
I0522 23:05:58.253026 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13979 (* 1 = 7.13979 loss)
I0522 23:05:58.255676 35003 sgd_solver.cpp:112] Iteration 87160, lr = 0.01
I0522 23:06:01.309623 35003 solver.cpp:239] Iteration 87170 (3.27176 iter/s, 3.05646s/10 iters), loss = 8.32215
I0522 23:06:01.309888 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.32215 (* 1 = 8.32215 loss)
I0522 23:06:01.317163 35003 sgd_solver.cpp:112] Iteration 87170, lr = 0.01
I0522 23:06:05.004225 35003 solver.cpp:239] Iteration 87180 (2.70694 iter/s, 3.69421s/10 iters), loss = 7.81208
I0522 23:06:05.004273 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81208 (* 1 = 7.81208 loss)
I0522 23:06:05.713289 35003 sgd_solver.cpp:112] Iteration 87180, lr = 0.01
I0522 23:06:07.843262 35003 solver.cpp:239] Iteration 87190 (3.52254 iter/s, 2.83886s/10 iters), loss = 6.76455
I0522 23:06:07.843312 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76455 (* 1 = 6.76455 loss)
I0522 23:06:08.578310 35003 sgd_solver.cpp:112] Iteration 87190, lr = 0.01
I0522 23:06:10.670858 35003 solver.cpp:239] Iteration 87200 (3.53678 iter/s, 2.82743s/10 iters), loss = 7.40013
I0522 23:06:10.670897 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40013 (* 1 = 7.40013 loss)
I0522 23:06:10.683625 35003 sgd_solver.cpp:112] Iteration 87200, lr = 0.01
I0522 23:06:13.156345 35003 solver.cpp:239] Iteration 87210 (4.0236 iter/s, 2.48534s/10 iters), loss = 7.75953
I0522 23:06:13.156383 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75953 (* 1 = 7.75953 loss)
I0522 23:06:13.169435 35003 sgd_solver.cpp:112] Iteration 87210, lr = 0.01
I0522 23:06:16.728042 35003 solver.cpp:239] Iteration 87220 (2.79993 iter/s, 3.57151s/10 iters), loss = 7.97436
I0522 23:06:16.728080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97436 (* 1 = 7.97436 loss)
I0522 23:06:16.741545 35003 sgd_solver.cpp:112] Iteration 87220, lr = 0.01
I0522 23:06:20.413422 35003 solver.cpp:239] Iteration 87230 (2.71358 iter/s, 3.68517s/10 iters), loss = 7.37531
I0522 23:06:20.413477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37531 (* 1 = 7.37531 loss)
I0522 23:06:20.426712 35003 sgd_solver.cpp:112] Iteration 87230, lr = 0.01
I0522 23:06:23.250788 35003 solver.cpp:239] Iteration 87240 (3.52462 iter/s, 2.83718s/10 iters), loss = 7.99951
I0522 23:06:23.250838 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99951 (* 1 = 7.99951 loss)
I0522 23:06:23.268707 35003 sgd_solver.cpp:112] Iteration 87240, lr = 0.01
I0522 23:06:27.036048 35003 solver.cpp:239] Iteration 87250 (2.64197 iter/s, 3.78506s/10 iters), loss = 9.10712
I0522 23:06:27.036093 35003 solver.cpp:258]     Train net output #0: softmax_loss = 9.10712 (* 1 = 9.10712 loss)
I0522 23:06:27.041870 35003 sgd_solver.cpp:112] Iteration 87250, lr = 0.01
I0522 23:06:30.447139 35003 solver.cpp:239] Iteration 87260 (2.93177 iter/s, 3.41091s/10 iters), loss = 6.61422
I0522 23:06:30.447186 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61422 (* 1 = 6.61422 loss)
I0522 23:06:30.487877 35003 sgd_solver.cpp:112] Iteration 87260, lr = 0.01
I0522 23:06:33.333024 35003 solver.cpp:239] Iteration 87270 (3.46535 iter/s, 2.88571s/10 iters), loss = 7.56571
I0522 23:06:33.333304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56571 (* 1 = 7.56571 loss)
I0522 23:06:33.351194 35003 sgd_solver.cpp:112] Iteration 87270, lr = 0.01
I0522 23:06:37.680014 35003 solver.cpp:239] Iteration 87280 (2.30067 iter/s, 4.34655s/10 iters), loss = 8.40982
I0522 23:06:37.680063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.40982 (* 1 = 8.40982 loss)
I0522 23:06:37.685072 35003 sgd_solver.cpp:112] Iteration 87280, lr = 0.01
I0522 23:06:40.529016 35003 solver.cpp:239] Iteration 87290 (3.51051 iter/s, 2.84859s/10 iters), loss = 7.39101
I0522 23:06:40.529059 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39101 (* 1 = 7.39101 loss)
I0522 23:06:40.553838 35003 sgd_solver.cpp:112] Iteration 87290, lr = 0.01
I0522 23:06:44.019155 35003 solver.cpp:239] Iteration 87300 (2.86537 iter/s, 3.48995s/10 iters), loss = 7.08202
I0522 23:06:44.019198 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08202 (* 1 = 7.08202 loss)
I0522 23:06:44.032038 35003 sgd_solver.cpp:112] Iteration 87300, lr = 0.01
I0522 23:06:47.687237 35003 solver.cpp:239] Iteration 87310 (2.72637 iter/s, 3.66789s/10 iters), loss = 7.48037
I0522 23:06:47.687279 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48037 (* 1 = 7.48037 loss)
I0522 23:06:48.383203 35003 sgd_solver.cpp:112] Iteration 87310, lr = 0.01
I0522 23:06:53.469466 35003 solver.cpp:239] Iteration 87320 (1.72952 iter/s, 5.78194s/10 iters), loss = 6.56146
I0522 23:06:53.469523 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56146 (* 1 = 6.56146 loss)
I0522 23:06:54.128327 35003 sgd_solver.cpp:112] Iteration 87320, lr = 0.01
I0522 23:06:56.991564 35003 solver.cpp:239] Iteration 87330 (2.83938 iter/s, 3.52189s/10 iters), loss = 6.57788
I0522 23:06:56.991616 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57788 (* 1 = 6.57788 loss)
I0522 23:06:56.998708 35003 sgd_solver.cpp:112] Iteration 87330, lr = 0.01
I0522 23:07:00.989281 35003 solver.cpp:239] Iteration 87340 (2.50156 iter/s, 3.9975s/10 iters), loss = 7.38294
I0522 23:07:00.989325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38294 (* 1 = 7.38294 loss)
I0522 23:07:01.001649 35003 sgd_solver.cpp:112] Iteration 87340, lr = 0.01
I0522 23:07:03.068042 35003 solver.cpp:239] Iteration 87350 (4.81087 iter/s, 2.07862s/10 iters), loss = 7.92535
I0522 23:07:03.068084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.92535 (* 1 = 7.92535 loss)
I0522 23:07:03.235405 35003 sgd_solver.cpp:112] Iteration 87350, lr = 0.01
I0522 23:07:07.763922 35003 solver.cpp:239] Iteration 87360 (2.12964 iter/s, 4.69564s/10 iters), loss = 7.90344
I0522 23:07:07.764168 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90344 (* 1 = 7.90344 loss)
I0522 23:07:08.437813 35003 sgd_solver.cpp:112] Iteration 87360, lr = 0.01
I0522 23:07:12.499802 35003 solver.cpp:239] Iteration 87370 (2.11173 iter/s, 4.73546s/10 iters), loss = 7.31232
I0522 23:07:12.499857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31232 (* 1 = 7.31232 loss)
I0522 23:07:12.504560 35003 sgd_solver.cpp:112] Iteration 87370, lr = 0.01
I0522 23:07:15.400702 35003 solver.cpp:239] Iteration 87380 (3.44742 iter/s, 2.90072s/10 iters), loss = 7.50106
I0522 23:07:15.400753 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50106 (* 1 = 7.50106 loss)
I0522 23:07:15.906365 35003 sgd_solver.cpp:112] Iteration 87380, lr = 0.01
I0522 23:07:17.202510 35003 solver.cpp:239] Iteration 87390 (5.55039 iter/s, 1.80167s/10 iters), loss = 8.67585
I0522 23:07:17.202554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.67585 (* 1 = 8.67585 loss)
I0522 23:07:17.224946 35003 sgd_solver.cpp:112] Iteration 87390, lr = 0.01
I0522 23:07:21.267313 35003 solver.cpp:239] Iteration 87400 (2.46027 iter/s, 4.06459s/10 iters), loss = 8.18002
I0522 23:07:21.267365 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.18002 (* 1 = 8.18002 loss)
I0522 23:07:21.274418 35003 sgd_solver.cpp:112] Iteration 87400, lr = 0.01
I0522 23:07:24.865726 35003 solver.cpp:239] Iteration 87410 (2.77916 iter/s, 3.5982s/10 iters), loss = 7.0771
I0522 23:07:24.865766 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0771 (* 1 = 7.0771 loss)
I0522 23:07:24.884708 35003 sgd_solver.cpp:112] Iteration 87410, lr = 0.01
I0522 23:07:27.940502 35003 solver.cpp:239] Iteration 87420 (3.25245 iter/s, 3.0746s/10 iters), loss = 7.95292
I0522 23:07:27.940546 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95292 (* 1 = 7.95292 loss)
I0522 23:07:27.950770 35003 sgd_solver.cpp:112] Iteration 87420, lr = 0.01
I0522 23:07:30.765936 35003 solver.cpp:239] Iteration 87430 (3.53948 iter/s, 2.82527s/10 iters), loss = 7.88119
I0522 23:07:30.765975 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88119 (* 1 = 7.88119 loss)
I0522 23:07:30.779873 35003 sgd_solver.cpp:112] Iteration 87430, lr = 0.01
I0522 23:07:33.814131 35003 solver.cpp:239] Iteration 87440 (3.28081 iter/s, 3.04802s/10 iters), loss = 7.2501
I0522 23:07:33.814179 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2501 (* 1 = 7.2501 loss)
I0522 23:07:33.825804 35003 sgd_solver.cpp:112] Iteration 87440, lr = 0.01
I0522 23:07:35.979570 35003 solver.cpp:239] Iteration 87450 (4.61832 iter/s, 2.16529s/10 iters), loss = 7.38659
I0522 23:07:35.979615 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38659 (* 1 = 7.38659 loss)
I0522 23:07:35.992411 35003 sgd_solver.cpp:112] Iteration 87450, lr = 0.01
I0522 23:07:40.436161 35003 solver.cpp:239] Iteration 87460 (2.24398 iter/s, 4.45637s/10 iters), loss = 6.98818
I0522 23:07:40.436298 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98818 (* 1 = 6.98818 loss)
I0522 23:07:40.449399 35003 sgd_solver.cpp:112] Iteration 87460, lr = 0.01
I0522 23:07:43.929013 35003 solver.cpp:239] Iteration 87470 (2.86322 iter/s, 3.49257s/10 iters), loss = 7.34577
I0522 23:07:43.929070 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34577 (* 1 = 7.34577 loss)
I0522 23:07:43.940948 35003 sgd_solver.cpp:112] Iteration 87470, lr = 0.01
I0522 23:07:45.013635 35003 solver.cpp:239] Iteration 87480 (9.22074 iter/s, 1.08451s/10 iters), loss = 7.27243
I0522 23:07:45.013685 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27243 (* 1 = 7.27243 loss)
I0522 23:07:45.709431 35003 sgd_solver.cpp:112] Iteration 87480, lr = 0.01
I0522 23:07:50.090370 35003 solver.cpp:239] Iteration 87490 (1.96988 iter/s, 5.07646s/10 iters), loss = 6.66711
I0522 23:07:50.090453 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66711 (* 1 = 6.66711 loss)
I0522 23:07:50.095890 35003 sgd_solver.cpp:112] Iteration 87490, lr = 0.01
I0522 23:07:53.669952 35003 solver.cpp:239] Iteration 87500 (2.7938 iter/s, 3.57936s/10 iters), loss = 7.96508
I0522 23:07:53.669996 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96508 (* 1 = 7.96508 loss)
I0522 23:07:54.385363 35003 sgd_solver.cpp:112] Iteration 87500, lr = 0.01
I0522 23:07:57.316833 35003 solver.cpp:239] Iteration 87510 (2.74222 iter/s, 3.64668s/10 iters), loss = 6.38296
I0522 23:07:57.316895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38296 (* 1 = 6.38296 loss)
I0522 23:07:58.032274 35003 sgd_solver.cpp:112] Iteration 87510, lr = 0.01
I0522 23:08:01.792114 35003 solver.cpp:239] Iteration 87520 (2.23462 iter/s, 4.47504s/10 iters), loss = 8.09774
I0522 23:08:01.792153 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.09774 (* 1 = 8.09774 loss)
I0522 23:08:01.797245 35003 sgd_solver.cpp:112] Iteration 87520, lr = 0.01
I0522 23:08:03.875638 35003 solver.cpp:239] Iteration 87530 (4.79988 iter/s, 2.08339s/10 iters), loss = 6.75074
I0522 23:08:03.875679 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75074 (* 1 = 6.75074 loss)
I0522 23:08:03.916462 35003 sgd_solver.cpp:112] Iteration 87530, lr = 0.01
I0522 23:08:06.757467 35003 solver.cpp:239] Iteration 87540 (3.47021 iter/s, 2.88167s/10 iters), loss = 7.44479
I0522 23:08:06.757509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44479 (* 1 = 7.44479 loss)
I0522 23:08:06.766933 35003 sgd_solver.cpp:112] Iteration 87540, lr = 0.01
I0522 23:08:10.479624 35003 solver.cpp:239] Iteration 87550 (2.68676 iter/s, 3.72195s/10 iters), loss = 6.84536
I0522 23:08:10.479928 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84536 (* 1 = 6.84536 loss)
I0522 23:08:11.194485 35003 sgd_solver.cpp:112] Iteration 87550, lr = 0.01
I0522 23:08:15.149674 35003 solver.cpp:239] Iteration 87560 (2.14152 iter/s, 4.66959s/10 iters), loss = 6.9854
I0522 23:08:15.149720 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9854 (* 1 = 6.9854 loss)
I0522 23:08:15.153512 35003 sgd_solver.cpp:112] Iteration 87560, lr = 0.01
I0522 23:08:17.892760 35003 solver.cpp:239] Iteration 87570 (3.64575 iter/s, 2.74292s/10 iters), loss = 7.22983
I0522 23:08:17.892810 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22983 (* 1 = 7.22983 loss)
I0522 23:08:18.627832 35003 sgd_solver.cpp:112] Iteration 87570, lr = 0.01
I0522 23:08:22.256006 35003 solver.cpp:239] Iteration 87580 (2.292 iter/s, 4.363s/10 iters), loss = 8.03528
I0522 23:08:22.256079 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03528 (* 1 = 8.03528 loss)
I0522 23:08:22.996268 35003 sgd_solver.cpp:112] Iteration 87580, lr = 0.01
I0522 23:08:25.616523 35003 solver.cpp:239] Iteration 87590 (2.97592 iter/s, 3.36031s/10 iters), loss = 7.12784
I0522 23:08:25.616560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12784 (* 1 = 7.12784 loss)
I0522 23:08:25.629719 35003 sgd_solver.cpp:112] Iteration 87590, lr = 0.01
I0522 23:08:28.772943 35003 solver.cpp:239] Iteration 87600 (3.16833 iter/s, 3.15623s/10 iters), loss = 7.73154
I0522 23:08:28.773007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73154 (* 1 = 7.73154 loss)
I0522 23:08:28.785627 35003 sgd_solver.cpp:112] Iteration 87600, lr = 0.01
I0522 23:08:33.009990 35003 solver.cpp:239] Iteration 87610 (2.36026 iter/s, 4.23682s/10 iters), loss = 7.81102
I0522 23:08:33.010026 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81102 (* 1 = 7.81102 loss)
I0522 23:08:33.718998 35003 sgd_solver.cpp:112] Iteration 87610, lr = 0.01
I0522 23:08:38.124467 35003 solver.cpp:239] Iteration 87620 (1.95533 iter/s, 5.11422s/10 iters), loss = 7.87691
I0522 23:08:38.124505 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87691 (* 1 = 7.87691 loss)
I0522 23:08:38.128290 35003 sgd_solver.cpp:112] Iteration 87620, lr = 0.01
I0522 23:08:40.215811 35003 solver.cpp:239] Iteration 87630 (4.78192 iter/s, 2.09121s/10 iters), loss = 8.22147
I0522 23:08:40.215852 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.22147 (* 1 = 8.22147 loss)
I0522 23:08:40.836680 35003 sgd_solver.cpp:112] Iteration 87630, lr = 0.01
I0522 23:08:43.601886 35003 solver.cpp:239] Iteration 87640 (2.95344 iter/s, 3.38588s/10 iters), loss = 7.88516
I0522 23:08:43.601948 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88516 (* 1 = 7.88516 loss)
I0522 23:08:43.605204 35003 sgd_solver.cpp:112] Iteration 87640, lr = 0.01
I0522 23:08:48.000183 35003 solver.cpp:239] Iteration 87650 (2.27374 iter/s, 4.39805s/10 iters), loss = 8.41117
I0522 23:08:48.000232 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.41117 (* 1 = 8.41117 loss)
I0522 23:08:48.652045 35003 sgd_solver.cpp:112] Iteration 87650, lr = 0.01
I0522 23:08:52.220158 35003 solver.cpp:239] Iteration 87660 (2.36981 iter/s, 4.21975s/10 iters), loss = 6.11701
I0522 23:08:52.220208 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11701 (* 1 = 6.11701 loss)
I0522 23:08:52.233337 35003 sgd_solver.cpp:112] Iteration 87660, lr = 0.01
I0522 23:08:54.028046 35003 solver.cpp:239] Iteration 87670 (5.53175 iter/s, 1.80775s/10 iters), loss = 7.22403
I0522 23:08:54.028100 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22403 (* 1 = 7.22403 loss)
I0522 23:08:54.768782 35003 sgd_solver.cpp:112] Iteration 87670, lr = 0.01
I0522 23:08:58.184902 35003 solver.cpp:239] Iteration 87680 (2.40579 iter/s, 4.15663s/10 iters), loss = 7.07484
I0522 23:08:58.184940 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07484 (* 1 = 7.07484 loss)
I0522 23:08:58.190776 35003 sgd_solver.cpp:112] Iteration 87680, lr = 0.01
I0522 23:09:01.812589 35003 solver.cpp:239] Iteration 87690 (2.75673 iter/s, 3.62749s/10 iters), loss = 6.62872
I0522 23:09:01.812647 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62872 (* 1 = 6.62872 loss)
I0522 23:09:02.553325 35003 sgd_solver.cpp:112] Iteration 87690, lr = 0.01
I0522 23:09:06.644179 35003 solver.cpp:239] Iteration 87700 (2.06983 iter/s, 4.83132s/10 iters), loss = 6.40328
I0522 23:09:06.644234 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40328 (* 1 = 6.40328 loss)
I0522 23:09:06.651907 35003 sgd_solver.cpp:112] Iteration 87700, lr = 0.01
I0522 23:09:11.040647 35003 solver.cpp:239] Iteration 87710 (2.27467 iter/s, 4.39624s/10 iters), loss = 6.90613
I0522 23:09:11.040921 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90613 (* 1 = 6.90613 loss)
I0522 23:09:11.054033 35003 sgd_solver.cpp:112] Iteration 87710, lr = 0.01
I0522 23:09:15.526588 35003 solver.cpp:239] Iteration 87720 (2.2294 iter/s, 4.4855s/10 iters), loss = 6.63394
I0522 23:09:15.526646 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63394 (* 1 = 6.63394 loss)
I0522 23:09:15.539564 35003 sgd_solver.cpp:112] Iteration 87720, lr = 0.01
I0522 23:09:17.526652 35003 solver.cpp:239] Iteration 87730 (5.00019 iter/s, 1.99992s/10 iters), loss = 7.25214
I0522 23:09:17.526712 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25214 (* 1 = 7.25214 loss)
I0522 23:09:17.536381 35003 sgd_solver.cpp:112] Iteration 87730, lr = 0.01
I0522 23:09:21.033955 35003 solver.cpp:239] Iteration 87740 (2.85135 iter/s, 3.50711s/10 iters), loss = 7.1804
I0522 23:09:21.033994 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1804 (* 1 = 7.1804 loss)
I0522 23:09:21.047026 35003 sgd_solver.cpp:112] Iteration 87740, lr = 0.01
I0522 23:09:23.033646 35003 solver.cpp:239] Iteration 87750 (5.0011 iter/s, 1.99956s/10 iters), loss = 7.49302
I0522 23:09:23.033694 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49302 (* 1 = 7.49302 loss)
I0522 23:09:23.047313 35003 sgd_solver.cpp:112] Iteration 87750, lr = 0.01
I0522 23:09:26.878950 35003 solver.cpp:239] Iteration 87760 (2.60072 iter/s, 3.84509s/10 iters), loss = 7.9171
I0522 23:09:26.879003 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9171 (* 1 = 7.9171 loss)
I0522 23:09:26.887495 35003 sgd_solver.cpp:112] Iteration 87760, lr = 0.01
I0522 23:09:28.877780 35003 solver.cpp:239] Iteration 87770 (5.00328 iter/s, 1.99869s/10 iters), loss = 7.8582
I0522 23:09:28.877820 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8582 (* 1 = 7.8582 loss)
I0522 23:09:28.891597 35003 sgd_solver.cpp:112] Iteration 87770, lr = 0.01
I0522 23:09:32.202599 35003 solver.cpp:239] Iteration 87780 (3.00785 iter/s, 3.32463s/10 iters), loss = 7.56704
I0522 23:09:32.202649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56704 (* 1 = 7.56704 loss)
I0522 23:09:32.230180 35003 sgd_solver.cpp:112] Iteration 87780, lr = 0.01
I0522 23:09:36.449995 35003 solver.cpp:239] Iteration 87790 (2.35451 iter/s, 4.24717s/10 iters), loss = 6.97509
I0522 23:09:36.450047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97509 (* 1 = 6.97509 loss)
I0522 23:09:36.473191 35003 sgd_solver.cpp:112] Iteration 87790, lr = 0.01
I0522 23:09:39.365505 35003 solver.cpp:239] Iteration 87800 (3.43013 iter/s, 2.91534s/10 iters), loss = 7.22177
I0522 23:09:39.365547 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22177 (* 1 = 7.22177 loss)
I0522 23:09:40.090131 35003 sgd_solver.cpp:112] Iteration 87800, lr = 0.01
I0522 23:09:42.567992 35003 solver.cpp:239] Iteration 87810 (3.12275 iter/s, 3.2023s/10 iters), loss = 6.68148
I0522 23:09:42.568308 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68148 (* 1 = 6.68148 loss)
I0522 23:09:42.582042 35003 sgd_solver.cpp:112] Iteration 87810, lr = 0.01
I0522 23:09:46.576014 35003 solver.cpp:239] Iteration 87820 (2.49528 iter/s, 4.00756s/10 iters), loss = 7.12768
I0522 23:09:46.576066 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12768 (* 1 = 7.12768 loss)
I0522 23:09:47.297107 35003 sgd_solver.cpp:112] Iteration 87820, lr = 0.01
I0522 23:09:51.699203 35003 solver.cpp:239] Iteration 87830 (1.95201 iter/s, 5.12292s/10 iters), loss = 6.51819
I0522 23:09:51.699255 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51819 (* 1 = 6.51819 loss)
I0522 23:09:52.436197 35003 sgd_solver.cpp:112] Iteration 87830, lr = 0.01
I0522 23:09:54.562383 35003 solver.cpp:239] Iteration 87840 (3.49284 iter/s, 2.863s/10 iters), loss = 7.5146
I0522 23:09:54.562463 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5146 (* 1 = 7.5146 loss)
I0522 23:09:55.302764 35003 sgd_solver.cpp:112] Iteration 87840, lr = 0.01
I0522 23:09:58.926287 35003 solver.cpp:239] Iteration 87850 (2.29166 iter/s, 4.36365s/10 iters), loss = 6.88721
I0522 23:09:58.926332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88721 (* 1 = 6.88721 loss)
I0522 23:09:58.939574 35003 sgd_solver.cpp:112] Iteration 87850, lr = 0.01
I0522 23:10:01.878026 35003 solver.cpp:239] Iteration 87860 (3.38803 iter/s, 2.95157s/10 iters), loss = 7.03923
I0522 23:10:01.878070 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03923 (* 1 = 7.03923 loss)
I0522 23:10:01.886929 35003 sgd_solver.cpp:112] Iteration 87860, lr = 0.01
I0522 23:10:04.577810 35003 solver.cpp:239] Iteration 87870 (3.70422 iter/s, 2.69962s/10 iters), loss = 8.04918
I0522 23:10:04.577854 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04918 (* 1 = 8.04918 loss)
I0522 23:10:04.581099 35003 sgd_solver.cpp:112] Iteration 87870, lr = 0.01
I0522 23:10:06.684392 35003 solver.cpp:239] Iteration 87880 (4.74736 iter/s, 2.10643s/10 iters), loss = 6.99601
I0522 23:10:06.684445 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99601 (* 1 = 6.99601 loss)
I0522 23:10:06.704942 35003 sgd_solver.cpp:112] Iteration 87880, lr = 0.01
I0522 23:10:10.552722 35003 solver.cpp:239] Iteration 87890 (2.58524 iter/s, 3.86812s/10 iters), loss = 7.55853
I0522 23:10:10.552767 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55853 (* 1 = 7.55853 loss)
I0522 23:10:10.565667 35003 sgd_solver.cpp:112] Iteration 87890, lr = 0.01
I0522 23:10:14.279906 35003 solver.cpp:239] Iteration 87900 (2.68314 iter/s, 3.72698s/10 iters), loss = 7.24118
I0522 23:10:14.280200 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24118 (* 1 = 7.24118 loss)
I0522 23:10:14.940917 35003 sgd_solver.cpp:112] Iteration 87900, lr = 0.01
I0522 23:10:17.785204 35003 solver.cpp:239] Iteration 87910 (2.85316 iter/s, 3.50489s/10 iters), loss = 6.57816
I0522 23:10:17.785244 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57816 (* 1 = 6.57816 loss)
I0522 23:10:18.507086 35003 sgd_solver.cpp:112] Iteration 87910, lr = 0.01
I0522 23:10:23.170789 35003 solver.cpp:239] Iteration 87920 (1.8569 iter/s, 5.38532s/10 iters), loss = 6.96546
I0522 23:10:23.170846 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96546 (* 1 = 6.96546 loss)
I0522 23:10:23.175613 35003 sgd_solver.cpp:112] Iteration 87920, lr = 0.01
I0522 23:10:26.748332 35003 solver.cpp:239] Iteration 87930 (2.79537 iter/s, 3.57734s/10 iters), loss = 7.70046
I0522 23:10:26.748374 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70046 (* 1 = 7.70046 loss)
I0522 23:10:27.332950 35003 sgd_solver.cpp:112] Iteration 87930, lr = 0.01
I0522 23:10:30.142163 35003 solver.cpp:239] Iteration 87940 (2.94668 iter/s, 3.39365s/10 iters), loss = 8.73591
I0522 23:10:30.142207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.73591 (* 1 = 8.73591 loss)
I0522 23:10:30.323577 35003 sgd_solver.cpp:112] Iteration 87940, lr = 0.01
I0522 23:10:34.711915 35003 solver.cpp:239] Iteration 87950 (2.18841 iter/s, 4.56952s/10 iters), loss = 7.1657
I0522 23:10:34.711962 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1657 (* 1 = 7.1657 loss)
I0522 23:10:34.719951 35003 sgd_solver.cpp:112] Iteration 87950, lr = 0.01
I0522 23:10:37.584558 35003 solver.cpp:239] Iteration 87960 (3.48132 iter/s, 2.87247s/10 iters), loss = 6.52639
I0522 23:10:37.584600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52639 (* 1 = 6.52639 loss)
I0522 23:10:38.221696 35003 sgd_solver.cpp:112] Iteration 87960, lr = 0.01
I0522 23:10:41.179846 35003 solver.cpp:239] Iteration 87970 (2.78157 iter/s, 3.59509s/10 iters), loss = 6.55441
I0522 23:10:41.179883 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55441 (* 1 = 6.55441 loss)
I0522 23:10:41.192565 35003 sgd_solver.cpp:112] Iteration 87970, lr = 0.01
I0522 23:10:43.834924 35003 solver.cpp:239] Iteration 87980 (3.76659 iter/s, 2.65492s/10 iters), loss = 7.36132
I0522 23:10:43.834967 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36132 (* 1 = 7.36132 loss)
I0522 23:10:43.843148 35003 sgd_solver.cpp:112] Iteration 87980, lr = 0.01
I0522 23:10:45.940304 35003 solver.cpp:239] Iteration 87990 (4.75008 iter/s, 2.10523s/10 iters), loss = 7.35474
I0522 23:10:45.940551 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35474 (* 1 = 7.35474 loss)
I0522 23:10:45.968185 35003 sgd_solver.cpp:112] Iteration 87990, lr = 0.01
I0522 23:10:48.642379 35003 solver.cpp:239] Iteration 88000 (3.70135 iter/s, 2.70172s/10 iters), loss = 7.83282
I0522 23:10:48.642431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83282 (* 1 = 7.83282 loss)
I0522 23:10:48.655946 35003 sgd_solver.cpp:112] Iteration 88000, lr = 0.01
I0522 23:10:51.508196 35003 solver.cpp:239] Iteration 88010 (3.48962 iter/s, 2.86565s/10 iters), loss = 7.68815
I0522 23:10:51.508240 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68815 (* 1 = 7.68815 loss)
I0522 23:10:51.516299 35003 sgd_solver.cpp:112] Iteration 88010, lr = 0.01
I0522 23:10:54.174723 35003 solver.cpp:239] Iteration 88020 (3.75044 iter/s, 2.66635s/10 iters), loss = 6.67081
I0522 23:10:54.174758 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67081 (* 1 = 6.67081 loss)
I0522 23:10:54.187413 35003 sgd_solver.cpp:112] Iteration 88020, lr = 0.01
I0522 23:10:57.068413 35003 solver.cpp:239] Iteration 88030 (3.45599 iter/s, 2.89353s/10 iters), loss = 7.60092
I0522 23:10:57.068456 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60092 (* 1 = 7.60092 loss)
I0522 23:10:57.081324 35003 sgd_solver.cpp:112] Iteration 88030, lr = 0.01
I0522 23:11:01.380367 35003 solver.cpp:239] Iteration 88040 (2.31926 iter/s, 4.31173s/10 iters), loss = 7.67639
I0522 23:11:01.380419 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67639 (* 1 = 7.67639 loss)
I0522 23:11:01.393966 35003 sgd_solver.cpp:112] Iteration 88040, lr = 0.01
I0522 23:11:05.569334 35003 solver.cpp:239] Iteration 88050 (2.38735 iter/s, 4.18874s/10 iters), loss = 6.92474
I0522 23:11:05.569384 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92474 (* 1 = 6.92474 loss)
I0522 23:11:06.264657 35003 sgd_solver.cpp:112] Iteration 88050, lr = 0.01
I0522 23:11:10.730226 35003 solver.cpp:239] Iteration 88060 (1.93775 iter/s, 5.16062s/10 iters), loss = 8.14607
I0522 23:11:10.730285 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.14607 (* 1 = 8.14607 loss)
I0522 23:11:11.470939 35003 sgd_solver.cpp:112] Iteration 88060, lr = 0.01
I0522 23:11:16.319524 35003 solver.cpp:239] Iteration 88070 (1.78922 iter/s, 5.58902s/10 iters), loss = 8.33094
I0522 23:11:16.319803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.33094 (* 1 = 8.33094 loss)
I0522 23:11:16.323709 35003 sgd_solver.cpp:112] Iteration 88070, lr = 0.01
I0522 23:11:21.547237 35003 solver.cpp:239] Iteration 88080 (1.91305 iter/s, 5.22725s/10 iters), loss = 8.22727
I0522 23:11:21.547276 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.22727 (* 1 = 8.22727 loss)
I0522 23:11:21.560171 35003 sgd_solver.cpp:112] Iteration 88080, lr = 0.01
I0522 23:11:25.287551 35003 solver.cpp:239] Iteration 88090 (2.67371 iter/s, 3.74011s/10 iters), loss = 7.81921
I0522 23:11:25.287600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81921 (* 1 = 7.81921 loss)
I0522 23:11:25.996310 35003 sgd_solver.cpp:112] Iteration 88090, lr = 0.01
I0522 23:11:28.079102 35003 solver.cpp:239] Iteration 88100 (3.58245 iter/s, 2.79138s/10 iters), loss = 6.5375
I0522 23:11:28.079138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5375 (* 1 = 6.5375 loss)
I0522 23:11:28.097543 35003 sgd_solver.cpp:112] Iteration 88100, lr = 0.01
I0522 23:11:29.951814 35003 solver.cpp:239] Iteration 88110 (5.3402 iter/s, 1.87259s/10 iters), loss = 6.76454
I0522 23:11:29.951865 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76454 (* 1 = 6.76454 loss)
I0522 23:11:30.465240 35003 sgd_solver.cpp:112] Iteration 88110, lr = 0.01
I0522 23:11:33.998288 35003 solver.cpp:239] Iteration 88120 (2.47142 iter/s, 4.04626s/10 iters), loss = 7.10982
I0522 23:11:33.998332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10982 (* 1 = 7.10982 loss)
I0522 23:11:34.006305 35003 sgd_solver.cpp:112] Iteration 88120, lr = 0.01
I0522 23:11:37.326153 35003 solver.cpp:239] Iteration 88130 (3.0051 iter/s, 3.32768s/10 iters), loss = 7.20483
I0522 23:11:37.326203 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20483 (* 1 = 7.20483 loss)
I0522 23:11:37.348212 35003 sgd_solver.cpp:112] Iteration 88130, lr = 0.01
I0522 23:11:39.419301 35003 solver.cpp:239] Iteration 88140 (4.77782 iter/s, 2.09301s/10 iters), loss = 7.47344
I0522 23:11:39.419342 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47344 (* 1 = 7.47344 loss)
I0522 23:11:39.459872 35003 sgd_solver.cpp:112] Iteration 88140, lr = 0.01
I0522 23:11:42.554412 35003 solver.cpp:239] Iteration 88150 (3.18985 iter/s, 3.13494s/10 iters), loss = 8.42929
I0522 23:11:42.554455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.42929 (* 1 = 8.42929 loss)
I0522 23:11:43.296090 35003 sgd_solver.cpp:112] Iteration 88150, lr = 0.01
I0522 23:11:46.072543 35003 solver.cpp:239] Iteration 88160 (2.84257 iter/s, 3.51794s/10 iters), loss = 6.90763
I0522 23:11:46.072592 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90763 (* 1 = 6.90763 loss)
I0522 23:11:46.080139 35003 sgd_solver.cpp:112] Iteration 88160, lr = 0.01
I0522 23:11:49.573354 35003 solver.cpp:239] Iteration 88170 (2.85665 iter/s, 3.50061s/10 iters), loss = 7.67733
I0522 23:11:49.573616 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67733 (* 1 = 7.67733 loss)
I0522 23:11:49.580137 35003 sgd_solver.cpp:112] Iteration 88170, lr = 0.01
I0522 23:11:52.994621 35003 solver.cpp:239] Iteration 88180 (2.92322 iter/s, 3.42089s/10 iters), loss = 7.83735
I0522 23:11:52.994673 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83735 (* 1 = 7.83735 loss)
I0522 23:11:53.007771 35003 sgd_solver.cpp:112] Iteration 88180, lr = 0.01
I0522 23:11:56.619616 35003 solver.cpp:239] Iteration 88190 (2.75878 iter/s, 3.62479s/10 iters), loss = 6.96911
I0522 23:11:56.619678 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96911 (* 1 = 6.96911 loss)
I0522 23:11:56.626366 35003 sgd_solver.cpp:112] Iteration 88190, lr = 0.01
I0522 23:11:59.707603 35003 solver.cpp:239] Iteration 88200 (3.23856 iter/s, 3.08779s/10 iters), loss = 7.07156
I0522 23:11:59.707645 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07156 (* 1 = 7.07156 loss)
I0522 23:11:59.715343 35003 sgd_solver.cpp:112] Iteration 88200, lr = 0.01
I0522 23:12:05.650491 35003 solver.cpp:239] Iteration 88210 (1.68276 iter/s, 5.94261s/10 iters), loss = 6.71436
I0522 23:12:05.650532 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71436 (* 1 = 6.71436 loss)
I0522 23:12:05.663120 35003 sgd_solver.cpp:112] Iteration 88210, lr = 0.01
I0522 23:12:08.516764 35003 solver.cpp:239] Iteration 88220 (3.48906 iter/s, 2.8661s/10 iters), loss = 7.35822
I0522 23:12:08.516826 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35822 (* 1 = 7.35822 loss)
I0522 23:12:08.517482 35003 sgd_solver.cpp:112] Iteration 88220, lr = 0.01
I0522 23:12:11.698740 35003 solver.cpp:239] Iteration 88230 (3.14289 iter/s, 3.18178s/10 iters), loss = 6.89226
I0522 23:12:11.698782 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89226 (* 1 = 6.89226 loss)
I0522 23:12:12.062594 35003 sgd_solver.cpp:112] Iteration 88230, lr = 0.01
I0522 23:12:15.518990 35003 solver.cpp:239] Iteration 88240 (2.61777 iter/s, 3.82004s/10 iters), loss = 7.68465
I0522 23:12:15.519032 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68465 (* 1 = 7.68465 loss)
I0522 23:12:15.532198 35003 sgd_solver.cpp:112] Iteration 88240, lr = 0.01
I0522 23:12:18.904659 35003 solver.cpp:239] Iteration 88250 (2.95378 iter/s, 3.38549s/10 iters), loss = 8.35865
I0522 23:12:18.904696 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.35865 (* 1 = 8.35865 loss)
I0522 23:12:19.060686 35003 sgd_solver.cpp:112] Iteration 88250, lr = 0.01
I0522 23:12:23.479640 35003 solver.cpp:239] Iteration 88260 (2.18591 iter/s, 4.57475s/10 iters), loss = 7.87118
I0522 23:12:23.479804 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87118 (* 1 = 7.87118 loss)
I0522 23:12:23.491701 35003 sgd_solver.cpp:112] Iteration 88260, lr = 0.01
I0522 23:12:26.218763 35003 solver.cpp:239] Iteration 88270 (3.65119 iter/s, 2.73884s/10 iters), loss = 7.3086
I0522 23:12:26.218833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3086 (* 1 = 7.3086 loss)
I0522 23:12:26.220764 35003 sgd_solver.cpp:112] Iteration 88270, lr = 0.01
I0522 23:12:30.131368 35003 solver.cpp:239] Iteration 88280 (2.55599 iter/s, 3.91238s/10 iters), loss = 7.57333
I0522 23:12:30.131412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57333 (* 1 = 7.57333 loss)
I0522 23:12:30.144202 35003 sgd_solver.cpp:112] Iteration 88280, lr = 0.01
I0522 23:12:35.148871 35003 solver.cpp:239] Iteration 88290 (1.99312 iter/s, 5.01725s/10 iters), loss = 6.66164
I0522 23:12:35.148924 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66164 (* 1 = 6.66164 loss)
I0522 23:12:35.889684 35003 sgd_solver.cpp:112] Iteration 88290, lr = 0.01
I0522 23:12:40.304134 35003 solver.cpp:239] Iteration 88300 (1.93986 iter/s, 5.155s/10 iters), loss = 7.39701
I0522 23:12:40.304188 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39701 (* 1 = 7.39701 loss)
I0522 23:12:40.378770 35003 sgd_solver.cpp:112] Iteration 88300, lr = 0.01
I0522 23:12:44.563414 35003 solver.cpp:239] Iteration 88310 (2.34794 iter/s, 4.25905s/10 iters), loss = 7.54723
I0522 23:12:44.563473 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54723 (* 1 = 7.54723 loss)
I0522 23:12:44.575793 35003 sgd_solver.cpp:112] Iteration 88310, lr = 0.01
I0522 23:12:48.177495 35003 solver.cpp:239] Iteration 88320 (2.76712 iter/s, 3.61386s/10 iters), loss = 6.6827
I0522 23:12:48.177565 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6827 (* 1 = 6.6827 loss)
I0522 23:12:48.892822 35003 sgd_solver.cpp:112] Iteration 88320, lr = 0.01
I0522 23:12:51.791491 35003 solver.cpp:239] Iteration 88330 (2.76718 iter/s, 3.61378s/10 iters), loss = 6.1969
I0522 23:12:51.791540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1969 (* 1 = 6.1969 loss)
I0522 23:12:52.533125 35003 sgd_solver.cpp:112] Iteration 88330, lr = 0.01
I0522 23:12:54.552779 35003 solver.cpp:239] Iteration 88340 (3.62171 iter/s, 2.76112s/10 iters), loss = 7.88824
I0522 23:12:54.553020 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88824 (* 1 = 7.88824 loss)
I0522 23:12:54.561946 35003 sgd_solver.cpp:112] Iteration 88340, lr = 0.01
I0522 23:12:57.734964 35003 solver.cpp:239] Iteration 88350 (3.14285 iter/s, 3.18182s/10 iters), loss = 8.5601
I0522 23:12:57.735023 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.5601 (* 1 = 8.5601 loss)
I0522 23:12:57.738340 35003 sgd_solver.cpp:112] Iteration 88350, lr = 0.01
I0522 23:13:02.018211 35003 solver.cpp:239] Iteration 88360 (2.3348 iter/s, 4.28302s/10 iters), loss = 7.05863
I0522 23:13:02.018257 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05863 (* 1 = 7.05863 loss)
I0522 23:13:02.023295 35003 sgd_solver.cpp:112] Iteration 88360, lr = 0.01
I0522 23:13:05.635648 35003 solver.cpp:239] Iteration 88370 (2.76454 iter/s, 3.61724s/10 iters), loss = 6.90598
I0522 23:13:05.635699 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90598 (* 1 = 6.90598 loss)
I0522 23:13:05.649298 35003 sgd_solver.cpp:112] Iteration 88370, lr = 0.01
I0522 23:13:09.446396 35003 solver.cpp:239] Iteration 88380 (2.6243 iter/s, 3.81054s/10 iters), loss = 7.58492
I0522 23:13:09.446439 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58492 (* 1 = 7.58492 loss)
I0522 23:13:09.459255 35003 sgd_solver.cpp:112] Iteration 88380, lr = 0.01
I0522 23:13:12.939493 35003 solver.cpp:239] Iteration 88390 (2.86294 iter/s, 3.49291s/10 iters), loss = 7.8799
I0522 23:13:12.939538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8799 (* 1 = 7.8799 loss)
I0522 23:13:12.953203 35003 sgd_solver.cpp:112] Iteration 88390, lr = 0.01
I0522 23:13:15.740440 35003 solver.cpp:239] Iteration 88400 (3.57043 iter/s, 2.80078s/10 iters), loss = 7.67504
I0522 23:13:15.740499 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67504 (* 1 = 7.67504 loss)
I0522 23:13:15.864953 35003 sgd_solver.cpp:112] Iteration 88400, lr = 0.01
I0522 23:13:19.099615 35003 solver.cpp:239] Iteration 88410 (2.9771 iter/s, 3.35897s/10 iters), loss = 7.47212
I0522 23:13:19.099670 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47212 (* 1 = 7.47212 loss)
I0522 23:13:19.105573 35003 sgd_solver.cpp:112] Iteration 88410, lr = 0.01
I0522 23:13:23.723707 35003 solver.cpp:239] Iteration 88420 (2.1627 iter/s, 4.62385s/10 iters), loss = 7.01981
I0522 23:13:23.723757 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01981 (* 1 = 7.01981 loss)
I0522 23:13:24.458720 35003 sgd_solver.cpp:112] Iteration 88420, lr = 0.01
I0522 23:13:28.154067 35003 solver.cpp:239] Iteration 88430 (2.25727 iter/s, 4.43013s/10 iters), loss = 8.57359
I0522 23:13:28.154335 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.57359 (* 1 = 8.57359 loss)
I0522 23:13:28.163028 35003 sgd_solver.cpp:112] Iteration 88430, lr = 0.01
I0522 23:13:31.798569 35003 solver.cpp:239] Iteration 88440 (2.74415 iter/s, 3.64412s/10 iters), loss = 6.91062
I0522 23:13:31.798616 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91062 (* 1 = 6.91062 loss)
I0522 23:13:32.487094 35003 sgd_solver.cpp:112] Iteration 88440, lr = 0.01
I0522 23:13:36.125248 35003 solver.cpp:239] Iteration 88450 (2.31136 iter/s, 4.32645s/10 iters), loss = 7.29563
I0522 23:13:36.125301 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29563 (* 1 = 7.29563 loss)
I0522 23:13:36.821279 35003 sgd_solver.cpp:112] Iteration 88450, lr = 0.01
I0522 23:13:39.631042 35003 solver.cpp:239] Iteration 88460 (2.85258 iter/s, 3.5056s/10 iters), loss = 7.28132
I0522 23:13:39.631093 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28132 (* 1 = 7.28132 loss)
I0522 23:13:39.640612 35003 sgd_solver.cpp:112] Iteration 88460, lr = 0.01
I0522 23:13:43.371548 35003 solver.cpp:239] Iteration 88470 (2.67358 iter/s, 3.7403s/10 iters), loss = 7.36216
I0522 23:13:43.371595 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36216 (* 1 = 7.36216 loss)
I0522 23:13:44.035152 35003 sgd_solver.cpp:112] Iteration 88470, lr = 0.01
I0522 23:13:46.141278 35003 solver.cpp:239] Iteration 88480 (3.61068 iter/s, 2.76956s/10 iters), loss = 7.93384
I0522 23:13:46.141332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93384 (* 1 = 7.93384 loss)
I0522 23:13:46.154364 35003 sgd_solver.cpp:112] Iteration 88480, lr = 0.01
I0522 23:13:50.005915 35003 solver.cpp:239] Iteration 88490 (2.58771 iter/s, 3.86442s/10 iters), loss = 6.37914
I0522 23:13:50.005966 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37914 (* 1 = 6.37914 loss)
I0522 23:13:50.013109 35003 sgd_solver.cpp:112] Iteration 88490, lr = 0.01
I0522 23:13:52.199694 35003 solver.cpp:239] Iteration 88500 (4.55866 iter/s, 2.19363s/10 iters), loss = 6.80046
I0522 23:13:52.199748 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80046 (* 1 = 6.80046 loss)
I0522 23:13:52.207994 35003 sgd_solver.cpp:112] Iteration 88500, lr = 0.01
I0522 23:13:55.571581 35003 solver.cpp:239] Iteration 88510 (2.96587 iter/s, 3.3717s/10 iters), loss = 7.94569
I0522 23:13:55.571643 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94569 (* 1 = 7.94569 loss)
I0522 23:13:55.580965 35003 sgd_solver.cpp:112] Iteration 88510, lr = 0.01
I0522 23:13:58.398756 35003 solver.cpp:239] Iteration 88520 (3.53732 iter/s, 2.827s/10 iters), loss = 6.8877
I0522 23:13:58.399016 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8877 (* 1 = 6.8877 loss)
I0522 23:13:58.420359 35003 sgd_solver.cpp:112] Iteration 88520, lr = 0.01
I0522 23:14:02.674852 35003 solver.cpp:239] Iteration 88530 (2.33881 iter/s, 4.27568s/10 iters), loss = 6.28065
I0522 23:14:02.674901 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28065 (* 1 = 6.28065 loss)
I0522 23:14:02.690549 35003 sgd_solver.cpp:112] Iteration 88530, lr = 0.01
I0522 23:14:04.794891 35003 solver.cpp:239] Iteration 88540 (4.71721 iter/s, 2.11989s/10 iters), loss = 6.60205
I0522 23:14:04.794948 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60205 (* 1 = 6.60205 loss)
I0522 23:14:05.535785 35003 sgd_solver.cpp:112] Iteration 88540, lr = 0.01
I0522 23:14:10.454375 35003 solver.cpp:239] Iteration 88550 (1.76703 iter/s, 5.6592s/10 iters), loss = 6.54005
I0522 23:14:10.454419 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54005 (* 1 = 6.54005 loss)
I0522 23:14:10.464134 35003 sgd_solver.cpp:112] Iteration 88550, lr = 0.01
I0522 23:14:14.737468 35003 solver.cpp:239] Iteration 88560 (2.33488 iter/s, 4.28287s/10 iters), loss = 7.48779
I0522 23:14:14.737512 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48779 (* 1 = 7.48779 loss)
I0522 23:14:14.747273 35003 sgd_solver.cpp:112] Iteration 88560, lr = 0.01
I0522 23:14:18.252161 35003 solver.cpp:239] Iteration 88570 (2.84536 iter/s, 3.5145s/10 iters), loss = 8.00059
I0522 23:14:18.252212 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00059 (* 1 = 8.00059 loss)
I0522 23:14:18.276448 35003 sgd_solver.cpp:112] Iteration 88570, lr = 0.01
I0522 23:14:21.721307 35003 solver.cpp:239] Iteration 88580 (2.88272 iter/s, 3.46895s/10 iters), loss = 7.86794
I0522 23:14:21.721344 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86794 (* 1 = 7.86794 loss)
I0522 23:14:21.734143 35003 sgd_solver.cpp:112] Iteration 88580, lr = 0.01
I0522 23:14:24.602218 35003 solver.cpp:239] Iteration 88590 (3.47132 iter/s, 2.88075s/10 iters), loss = 7.36816
I0522 23:14:24.602258 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36816 (* 1 = 7.36816 loss)
I0522 23:14:24.608402 35003 sgd_solver.cpp:112] Iteration 88590, lr = 0.01
I0522 23:14:28.169553 35003 solver.cpp:239] Iteration 88600 (2.80337 iter/s, 3.56714s/10 iters), loss = 7.3573
I0522 23:14:28.169592 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3573 (* 1 = 7.3573 loss)
I0522 23:14:28.182679 35003 sgd_solver.cpp:112] Iteration 88600, lr = 0.01
I0522 23:14:31.755466 35003 solver.cpp:239] Iteration 88610 (2.78884 iter/s, 3.58572s/10 iters), loss = 7.85473
I0522 23:14:31.755653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85473 (* 1 = 7.85473 loss)
I0522 23:14:31.760093 35003 sgd_solver.cpp:112] Iteration 88610, lr = 0.01
I0522 23:14:35.152261 35003 solver.cpp:239] Iteration 88620 (2.94423 iter/s, 3.39648s/10 iters), loss = 7.59393
I0522 23:14:35.152300 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59393 (* 1 = 7.59393 loss)
I0522 23:14:35.164970 35003 sgd_solver.cpp:112] Iteration 88620, lr = 0.01
I0522 23:14:37.221393 35003 solver.cpp:239] Iteration 88630 (4.83327 iter/s, 2.06899s/10 iters), loss = 6.90956
I0522 23:14:37.221441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90956 (* 1 = 6.90956 loss)
I0522 23:14:37.225981 35003 sgd_solver.cpp:112] Iteration 88630, lr = 0.01
I0522 23:14:40.055995 35003 solver.cpp:239] Iteration 88640 (3.52808 iter/s, 2.83441s/10 iters), loss = 7.36266
I0522 23:14:40.056035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36266 (* 1 = 7.36266 loss)
I0522 23:14:40.082963 35003 sgd_solver.cpp:112] Iteration 88640, lr = 0.01
I0522 23:14:43.594753 35003 solver.cpp:239] Iteration 88650 (2.826 iter/s, 3.53857s/10 iters), loss = 7.22826
I0522 23:14:43.594795 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22826 (* 1 = 7.22826 loss)
I0522 23:14:43.598630 35003 sgd_solver.cpp:112] Iteration 88650, lr = 0.01
I0522 23:14:46.540721 35003 solver.cpp:239] Iteration 88660 (3.39467 iter/s, 2.94579s/10 iters), loss = 7.74974
I0522 23:14:46.540776 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74974 (* 1 = 7.74974 loss)
I0522 23:14:47.249948 35003 sgd_solver.cpp:112] Iteration 88660, lr = 0.01
I0522 23:14:50.224915 35003 solver.cpp:239] Iteration 88670 (2.71445 iter/s, 3.68398s/10 iters), loss = 6.19589
I0522 23:14:50.224972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19589 (* 1 = 6.19589 loss)
I0522 23:14:50.943644 35003 sgd_solver.cpp:112] Iteration 88670, lr = 0.01
I0522 23:14:54.437013 35003 solver.cpp:239] Iteration 88680 (2.37425 iter/s, 4.21186s/10 iters), loss = 6.83978
I0522 23:14:54.437068 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83978 (* 1 = 6.83978 loss)
I0522 23:14:55.153122 35003 sgd_solver.cpp:112] Iteration 88680, lr = 0.01
I0522 23:15:00.296995 35003 solver.cpp:239] Iteration 88690 (1.70658 iter/s, 5.85969s/10 iters), loss = 7.62902
I0522 23:15:00.297045 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62902 (* 1 = 7.62902 loss)
I0522 23:15:00.299906 35003 sgd_solver.cpp:112] Iteration 88690, lr = 0.01
I0522 23:15:02.457845 35003 solver.cpp:239] Iteration 88700 (4.62832 iter/s, 2.16061s/10 iters), loss = 7.46785
I0522 23:15:02.458019 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46785 (* 1 = 7.46785 loss)
I0522 23:15:03.153888 35003 sgd_solver.cpp:112] Iteration 88700, lr = 0.01
I0522 23:15:07.564805 35003 solver.cpp:239] Iteration 88710 (1.95826 iter/s, 5.10658s/10 iters), loss = 7.34728
I0522 23:15:07.564843 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34728 (* 1 = 7.34728 loss)
I0522 23:15:07.577657 35003 sgd_solver.cpp:112] Iteration 88710, lr = 0.01
I0522 23:15:10.128839 35003 solver.cpp:239] Iteration 88720 (3.90033 iter/s, 2.56388s/10 iters), loss = 6.2558
I0522 23:15:10.128885 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2558 (* 1 = 6.2558 loss)
I0522 23:15:10.134615 35003 sgd_solver.cpp:112] Iteration 88720, lr = 0.01
I0522 23:15:12.985251 35003 solver.cpp:239] Iteration 88730 (3.50111 iter/s, 2.85624s/10 iters), loss = 7.94589
I0522 23:15:12.985301 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94589 (* 1 = 7.94589 loss)
I0522 23:15:12.992605 35003 sgd_solver.cpp:112] Iteration 88730, lr = 0.01
I0522 23:15:17.378213 35003 solver.cpp:239] Iteration 88740 (2.27649 iter/s, 4.39274s/10 iters), loss = 7.29549
I0522 23:15:17.378253 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29549 (* 1 = 7.29549 loss)
I0522 23:15:17.488775 35003 sgd_solver.cpp:112] Iteration 88740, lr = 0.01
I0522 23:15:20.228853 35003 solver.cpp:239] Iteration 88750 (3.50818 iter/s, 2.85048s/10 iters), loss = 7.50148
I0522 23:15:20.228895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50148 (* 1 = 7.50148 loss)
I0522 23:15:20.236546 35003 sgd_solver.cpp:112] Iteration 88750, lr = 0.01
I0522 23:15:25.131620 35003 solver.cpp:239] Iteration 88760 (2.03977 iter/s, 4.90251s/10 iters), loss = 7.48027
I0522 23:15:25.131666 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48027 (* 1 = 7.48027 loss)
I0522 23:15:25.145125 35003 sgd_solver.cpp:112] Iteration 88760, lr = 0.01
I0522 23:15:27.839558 35003 solver.cpp:239] Iteration 88770 (3.69307 iter/s, 2.70777s/10 iters), loss = 6.34987
I0522 23:15:27.839601 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34987 (* 1 = 6.34987 loss)
I0522 23:15:27.964200 35003 sgd_solver.cpp:112] Iteration 88770, lr = 0.01
I0522 23:15:32.106672 35003 solver.cpp:239] Iteration 88780 (2.34362 iter/s, 4.2669s/10 iters), loss = 6.9257
I0522 23:15:32.106755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9257 (* 1 = 6.9257 loss)
I0522 23:15:32.831362 35003 sgd_solver.cpp:112] Iteration 88780, lr = 0.01
I0522 23:15:36.840427 35003 solver.cpp:239] Iteration 88790 (2.11261 iter/s, 4.73347s/10 iters), loss = 7.3407
I0522 23:15:36.840487 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3407 (* 1 = 7.3407 loss)
I0522 23:15:36.855294 35003 sgd_solver.cpp:112] Iteration 88790, lr = 0.01
I0522 23:15:40.549523 35003 solver.cpp:239] Iteration 88800 (2.69623 iter/s, 3.70889s/10 iters), loss = 7.66798
I0522 23:15:40.549563 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66798 (* 1 = 7.66798 loss)
I0522 23:15:40.562608 35003 sgd_solver.cpp:112] Iteration 88800, lr = 0.01
I0522 23:15:44.846666 35003 solver.cpp:239] Iteration 88810 (2.32725 iter/s, 4.29692s/10 iters), loss = 7.8951
I0522 23:15:44.846740 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8951 (* 1 = 7.8951 loss)
I0522 23:15:44.849377 35003 sgd_solver.cpp:112] Iteration 88810, lr = 0.01
I0522 23:15:49.246402 35003 solver.cpp:239] Iteration 88820 (2.273 iter/s, 4.39948s/10 iters), loss = 5.8983
I0522 23:15:49.246456 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8983 (* 1 = 5.8983 loss)
I0522 23:15:49.946591 35003 sgd_solver.cpp:112] Iteration 88820, lr = 0.01
I0522 23:15:52.807015 35003 solver.cpp:239] Iteration 88830 (2.80867 iter/s, 3.56041s/10 iters), loss = 6.77138
I0522 23:15:52.807054 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77138 (* 1 = 6.77138 loss)
I0522 23:15:53.528029 35003 sgd_solver.cpp:112] Iteration 88830, lr = 0.01
I0522 23:15:57.167227 35003 solver.cpp:239] Iteration 88840 (2.29358 iter/s, 4.35999s/10 iters), loss = 7.9118
I0522 23:15:57.167280 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9118 (* 1 = 7.9118 loss)
I0522 23:15:57.895529 35003 sgd_solver.cpp:112] Iteration 88840, lr = 0.01
I0522 23:16:01.214325 35003 solver.cpp:239] Iteration 88850 (2.47104 iter/s, 4.04688s/10 iters), loss = 8.4335
I0522 23:16:01.214375 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.4335 (* 1 = 8.4335 loss)
I0522 23:16:01.219252 35003 sgd_solver.cpp:112] Iteration 88850, lr = 0.01
I0522 23:16:04.484378 35003 solver.cpp:239] Iteration 88860 (3.05822 iter/s, 3.26987s/10 iters), loss = 7.21032
I0522 23:16:04.484589 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21032 (* 1 = 7.21032 loss)
I0522 23:16:04.497954 35003 sgd_solver.cpp:112] Iteration 88860, lr = 0.01
I0522 23:16:08.571656 35003 solver.cpp:239] Iteration 88870 (2.44682 iter/s, 4.08693s/10 iters), loss = 7.372
I0522 23:16:08.571691 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.372 (* 1 = 7.372 loss)
I0522 23:16:08.585007 35003 sgd_solver.cpp:112] Iteration 88870, lr = 0.01
I0522 23:16:12.178812 35003 solver.cpp:239] Iteration 88880 (2.77242 iter/s, 3.60696s/10 iters), loss = 6.46651
I0522 23:16:12.178871 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46651 (* 1 = 6.46651 loss)
I0522 23:16:12.191735 35003 sgd_solver.cpp:112] Iteration 88880, lr = 0.01
I0522 23:16:14.957782 35003 solver.cpp:239] Iteration 88890 (3.59869 iter/s, 2.77879s/10 iters), loss = 7.12852
I0522 23:16:14.957834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12852 (* 1 = 7.12852 loss)
I0522 23:16:14.974545 35003 sgd_solver.cpp:112] Iteration 88890, lr = 0.01
I0522 23:16:16.576959 35003 solver.cpp:239] Iteration 88900 (6.17646 iter/s, 1.61905s/10 iters), loss = 7.19639
I0522 23:16:16.576999 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19639 (* 1 = 7.19639 loss)
I0522 23:16:16.583360 35003 sgd_solver.cpp:112] Iteration 88900, lr = 0.01
I0522 23:16:17.496755 35003 solver.cpp:239] Iteration 88910 (10.873 iter/s, 0.91971s/10 iters), loss = 7.63003
I0522 23:16:17.496798 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63003 (* 1 = 7.63003 loss)
I0522 23:16:17.503590 35003 sgd_solver.cpp:112] Iteration 88910, lr = 0.01
I0522 23:16:19.346796 35003 solver.cpp:239] Iteration 88920 (5.40566 iter/s, 1.84991s/10 iters), loss = 6.75817
I0522 23:16:19.346848 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75817 (* 1 = 6.75817 loss)
I0522 23:16:19.354522 35003 sgd_solver.cpp:112] Iteration 88920, lr = 0.01
I0522 23:16:21.761304 35003 solver.cpp:239] Iteration 88930 (4.1419 iter/s, 2.41435s/10 iters), loss = 7.62662
I0522 23:16:21.761355 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62662 (* 1 = 7.62662 loss)
I0522 23:16:22.234381 35003 sgd_solver.cpp:112] Iteration 88930, lr = 0.01
I0522 23:16:25.091411 35003 solver.cpp:239] Iteration 88940 (3.00308 iter/s, 3.32992s/10 iters), loss = 7.4192
I0522 23:16:25.091459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4192 (* 1 = 7.4192 loss)
I0522 23:16:25.780195 35003 sgd_solver.cpp:112] Iteration 88940, lr = 0.01
I0522 23:16:30.161891 35003 solver.cpp:239] Iteration 88950 (1.9723 iter/s, 5.07022s/10 iters), loss = 8.13304
I0522 23:16:30.161937 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.13304 (* 1 = 8.13304 loss)
I0522 23:16:30.877115 35003 sgd_solver.cpp:112] Iteration 88950, lr = 0.01
I0522 23:16:33.014376 35003 solver.cpp:239] Iteration 88960 (3.50592 iter/s, 2.85232s/10 iters), loss = 7.4261
I0522 23:16:33.014421 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4261 (* 1 = 7.4261 loss)
I0522 23:16:33.024338 35003 sgd_solver.cpp:112] Iteration 88960, lr = 0.01
I0522 23:16:37.193511 35003 solver.cpp:239] Iteration 88970 (2.39297 iter/s, 4.17891s/10 iters), loss = 7.63604
I0522 23:16:37.193801 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63604 (* 1 = 7.63604 loss)
I0522 23:16:37.201014 35003 sgd_solver.cpp:112] Iteration 88970, lr = 0.01
I0522 23:16:40.002684 35003 solver.cpp:239] Iteration 88980 (3.56025 iter/s, 2.80879s/10 iters), loss = 7.8828
I0522 23:16:40.002768 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8828 (* 1 = 7.8828 loss)
I0522 23:16:40.737118 35003 sgd_solver.cpp:112] Iteration 88980, lr = 0.01
I0522 23:16:43.630815 35003 solver.cpp:239] Iteration 88990 (2.75642 iter/s, 3.62789s/10 iters), loss = 6.72076
I0522 23:16:43.630877 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72076 (* 1 = 6.72076 loss)
I0522 23:16:44.343698 35003 sgd_solver.cpp:112] Iteration 88990, lr = 0.01
I0522 23:16:47.458179 35003 solver.cpp:239] Iteration 89000 (2.61291 iter/s, 3.82715s/10 iters), loss = 7.60371
I0522 23:16:47.458215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60371 (* 1 = 7.60371 loss)
I0522 23:16:47.476850 35003 sgd_solver.cpp:112] Iteration 89000, lr = 0.01
I0522 23:16:50.842356 35003 solver.cpp:239] Iteration 89010 (2.95509 iter/s, 3.38399s/10 iters), loss = 5.96894
I0522 23:16:50.842417 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96894 (* 1 = 5.96894 loss)
I0522 23:16:51.505164 35003 sgd_solver.cpp:112] Iteration 89010, lr = 0.01
I0522 23:16:56.496575 35003 solver.cpp:239] Iteration 89020 (1.76868 iter/s, 5.65393s/10 iters), loss = 6.70515
I0522 23:16:56.496630 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70515 (* 1 = 6.70515 loss)
I0522 23:16:56.586194 35003 sgd_solver.cpp:112] Iteration 89020, lr = 0.01
I0522 23:16:58.802485 35003 solver.cpp:239] Iteration 89030 (4.33697 iter/s, 2.30576s/10 iters), loss = 6.96019
I0522 23:16:58.802531 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96019 (* 1 = 6.96019 loss)
I0522 23:16:58.816035 35003 sgd_solver.cpp:112] Iteration 89030, lr = 0.01
I0522 23:17:01.667299 35003 solver.cpp:239] Iteration 89040 (3.49083 iter/s, 2.86465s/10 iters), loss = 7.61941
I0522 23:17:01.667346 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61941 (* 1 = 7.61941 loss)
I0522 23:17:02.337286 35003 sgd_solver.cpp:112] Iteration 89040, lr = 0.01
I0522 23:17:05.374877 35003 solver.cpp:239] Iteration 89050 (2.69733 iter/s, 3.70737s/10 iters), loss = 8.11927
I0522 23:17:05.374941 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.11927 (* 1 = 8.11927 loss)
I0522 23:17:06.109385 35003 sgd_solver.cpp:112] Iteration 89050, lr = 0.01
I0522 23:17:09.725462 35003 solver.cpp:239] Iteration 89060 (2.29867 iter/s, 4.35035s/10 iters), loss = 7.97019
I0522 23:17:09.725700 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97019 (* 1 = 7.97019 loss)
I0522 23:17:09.733222 35003 sgd_solver.cpp:112] Iteration 89060, lr = 0.01
I0522 23:17:14.020777 35003 solver.cpp:239] Iteration 89070 (2.32833 iter/s, 4.29492s/10 iters), loss = 6.92823
I0522 23:17:14.020820 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92823 (* 1 = 6.92823 loss)
I0522 23:17:14.033974 35003 sgd_solver.cpp:112] Iteration 89070, lr = 0.01
I0522 23:17:17.607030 35003 solver.cpp:239] Iteration 89080 (2.78859 iter/s, 3.58604s/10 iters), loss = 8.60174
I0522 23:17:17.607092 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.60174 (* 1 = 8.60174 loss)
I0522 23:17:18.326480 35003 sgd_solver.cpp:112] Iteration 89080, lr = 0.01
I0522 23:17:21.750728 35003 solver.cpp:239] Iteration 89090 (2.41345 iter/s, 4.14344s/10 iters), loss = 7.30915
I0522 23:17:21.750777 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30915 (* 1 = 7.30915 loss)
I0522 23:17:21.763617 35003 sgd_solver.cpp:112] Iteration 89090, lr = 0.01
I0522 23:17:25.261324 35003 solver.cpp:239] Iteration 89100 (2.84868 iter/s, 3.51039s/10 iters), loss = 7.60798
I0522 23:17:25.261391 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60798 (* 1 = 7.60798 loss)
I0522 23:17:25.267025 35003 sgd_solver.cpp:112] Iteration 89100, lr = 0.01
I0522 23:17:28.773401 35003 solver.cpp:239] Iteration 89110 (2.84749 iter/s, 3.51187s/10 iters), loss = 7.79427
I0522 23:17:28.773454 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79427 (* 1 = 7.79427 loss)
I0522 23:17:29.507709 35003 sgd_solver.cpp:112] Iteration 89110, lr = 0.01
I0522 23:17:32.195165 35003 solver.cpp:239] Iteration 89120 (2.92264 iter/s, 3.42156s/10 iters), loss = 7.34139
I0522 23:17:32.195220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34139 (* 1 = 7.34139 loss)
I0522 23:17:32.220425 35003 sgd_solver.cpp:112] Iteration 89120, lr = 0.01
I0522 23:17:35.181311 35003 solver.cpp:239] Iteration 89130 (3.349 iter/s, 2.98596s/10 iters), loss = 7.01488
I0522 23:17:35.181370 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01488 (* 1 = 7.01488 loss)
I0522 23:17:35.194706 35003 sgd_solver.cpp:112] Iteration 89130, lr = 0.01
I0522 23:17:38.795717 35003 solver.cpp:239] Iteration 89140 (2.76687 iter/s, 3.61419s/10 iters), loss = 8.17265
I0522 23:17:38.795761 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17265 (* 1 = 8.17265 loss)
I0522 23:17:38.808048 35003 sgd_solver.cpp:112] Iteration 89140, lr = 0.01
I0522 23:17:41.709867 35003 solver.cpp:239] Iteration 89150 (3.43687 iter/s, 2.90963s/10 iters), loss = 8.13462
I0522 23:17:41.710101 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.13462 (* 1 = 8.13462 loss)
I0522 23:17:42.450814 35003 sgd_solver.cpp:112] Iteration 89150, lr = 0.01
I0522 23:17:44.511535 35003 solver.cpp:239] Iteration 89160 (3.56973 iter/s, 2.80134s/10 iters), loss = 7.63065
I0522 23:17:44.511580 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63065 (* 1 = 7.63065 loss)
I0522 23:17:44.524997 35003 sgd_solver.cpp:112] Iteration 89160, lr = 0.01
I0522 23:17:47.855370 35003 solver.cpp:239] Iteration 89170 (2.99074 iter/s, 3.34365s/10 iters), loss = 7.73118
I0522 23:17:47.855414 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73118 (* 1 = 7.73118 loss)
I0522 23:17:47.862041 35003 sgd_solver.cpp:112] Iteration 89170, lr = 0.01
I0522 23:17:49.957480 35003 solver.cpp:239] Iteration 89180 (4.75744 iter/s, 2.10197s/10 iters), loss = 7.41366
I0522 23:17:49.957521 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41366 (* 1 = 7.41366 loss)
I0522 23:17:49.976007 35003 sgd_solver.cpp:112] Iteration 89180, lr = 0.01
I0522 23:17:52.005074 35003 solver.cpp:239] Iteration 89190 (4.8841 iter/s, 2.04746s/10 iters), loss = 7.38799
I0522 23:17:52.005113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38799 (* 1 = 7.38799 loss)
I0522 23:17:52.017590 35003 sgd_solver.cpp:112] Iteration 89190, lr = 0.01
I0522 23:17:54.864362 35003 solver.cpp:239] Iteration 89200 (3.49759 iter/s, 2.85912s/10 iters), loss = 6.94728
I0522 23:17:54.864431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94728 (* 1 = 6.94728 loss)
I0522 23:17:54.868232 35003 sgd_solver.cpp:112] Iteration 89200, lr = 0.01
I0522 23:17:57.799398 35003 solver.cpp:239] Iteration 89210 (3.40733 iter/s, 2.93485s/10 iters), loss = 7.1382
I0522 23:17:57.799441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1382 (* 1 = 7.1382 loss)
I0522 23:17:58.539963 35003 sgd_solver.cpp:112] Iteration 89210, lr = 0.01
I0522 23:18:02.819515 35003 solver.cpp:239] Iteration 89220 (1.99209 iter/s, 5.01986s/10 iters), loss = 6.93179
I0522 23:18:02.819576 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93179 (* 1 = 6.93179 loss)
I0522 23:18:02.835675 35003 sgd_solver.cpp:112] Iteration 89220, lr = 0.01
I0522 23:18:05.697175 35003 solver.cpp:239] Iteration 89230 (3.47527 iter/s, 2.87748s/10 iters), loss = 8.21527
I0522 23:18:05.697216 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.21527 (* 1 = 8.21527 loss)
I0522 23:18:05.703218 35003 sgd_solver.cpp:112] Iteration 89230, lr = 0.01
I0522 23:18:10.709300 35003 solver.cpp:239] Iteration 89240 (1.99527 iter/s, 5.01187s/10 iters), loss = 7.35357
I0522 23:18:10.709347 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35357 (* 1 = 7.35357 loss)
I0522 23:18:10.712517 35003 sgd_solver.cpp:112] Iteration 89240, lr = 0.01
I0522 23:18:14.048261 35003 solver.cpp:239] Iteration 89250 (2.99516 iter/s, 3.33872s/10 iters), loss = 5.82136
I0522 23:18:14.048554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.82136 (* 1 = 5.82136 loss)
I0522 23:18:14.729729 35003 sgd_solver.cpp:112] Iteration 89250, lr = 0.01
I0522 23:18:18.651657 35003 solver.cpp:239] Iteration 89260 (2.17253 iter/s, 4.60294s/10 iters), loss = 7.19456
I0522 23:18:18.651706 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19456 (* 1 = 7.19456 loss)
I0522 23:18:19.363068 35003 sgd_solver.cpp:112] Iteration 89260, lr = 0.01
I0522 23:18:22.055884 35003 solver.cpp:239] Iteration 89270 (2.9377 iter/s, 3.40402s/10 iters), loss = 7.13925
I0522 23:18:22.055953 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13925 (* 1 = 7.13925 loss)
I0522 23:18:22.069149 35003 sgd_solver.cpp:112] Iteration 89270, lr = 0.01
I0522 23:18:25.016521 35003 solver.cpp:239] Iteration 89280 (3.37786 iter/s, 2.96045s/10 iters), loss = 7.37601
I0522 23:18:25.016566 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37601 (* 1 = 7.37601 loss)
I0522 23:18:25.295333 35003 sgd_solver.cpp:112] Iteration 89280, lr = 0.01
I0522 23:18:28.142654 35003 solver.cpp:239] Iteration 89290 (3.19902 iter/s, 3.12596s/10 iters), loss = 7.25806
I0522 23:18:28.142732 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25806 (* 1 = 7.25806 loss)
I0522 23:18:28.756341 35003 sgd_solver.cpp:112] Iteration 89290, lr = 0.01
I0522 23:18:32.256129 35003 solver.cpp:239] Iteration 89300 (2.43118 iter/s, 4.11323s/10 iters), loss = 8.15658
I0522 23:18:32.256176 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.15658 (* 1 = 8.15658 loss)
I0522 23:18:32.262933 35003 sgd_solver.cpp:112] Iteration 89300, lr = 0.01
I0522 23:18:35.661245 35003 solver.cpp:239] Iteration 89310 (2.93692 iter/s, 3.40493s/10 iters), loss = 7.87033
I0522 23:18:35.661295 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87033 (* 1 = 7.87033 loss)
I0522 23:18:36.401808 35003 sgd_solver.cpp:112] Iteration 89310, lr = 0.01
I0522 23:18:39.244165 35003 solver.cpp:239] Iteration 89320 (2.79117 iter/s, 3.58272s/10 iters), loss = 7.04966
I0522 23:18:39.244215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04966 (* 1 = 7.04966 loss)
I0522 23:18:39.629865 35003 sgd_solver.cpp:112] Iteration 89320, lr = 0.01
I0522 23:18:42.415383 35003 solver.cpp:239] Iteration 89330 (3.15354 iter/s, 3.17104s/10 iters), loss = 7.019
I0522 23:18:42.415421 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.019 (* 1 = 7.019 loss)
I0522 23:18:42.417626 35003 sgd_solver.cpp:112] Iteration 89330, lr = 0.01
I0522 23:18:45.502769 35003 solver.cpp:239] Iteration 89340 (3.23919 iter/s, 3.08719s/10 iters), loss = 7.11671
I0522 23:18:45.502991 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11671 (* 1 = 7.11671 loss)
I0522 23:18:46.221913 35003 sgd_solver.cpp:112] Iteration 89340, lr = 0.01
I0522 23:18:48.351136 35003 solver.cpp:239] Iteration 89350 (3.5112 iter/s, 2.84803s/10 iters), loss = 6.58475
I0522 23:18:48.351183 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58475 (* 1 = 6.58475 loss)
I0522 23:18:48.364349 35003 sgd_solver.cpp:112] Iteration 89350, lr = 0.01
I0522 23:18:51.689563 35003 solver.cpp:239] Iteration 89360 (2.99559 iter/s, 3.33824s/10 iters), loss = 6.97857
I0522 23:18:51.689604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97857 (* 1 = 6.97857 loss)
I0522 23:18:51.714737 35003 sgd_solver.cpp:112] Iteration 89360, lr = 0.01
I0522 23:18:53.751391 35003 solver.cpp:239] Iteration 89370 (4.85038 iter/s, 2.06169s/10 iters), loss = 7.23745
I0522 23:18:53.751433 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23745 (* 1 = 7.23745 loss)
I0522 23:18:53.775226 35003 sgd_solver.cpp:112] Iteration 89370, lr = 0.01
I0522 23:18:58.102221 35003 solver.cpp:239] Iteration 89380 (2.29853 iter/s, 4.35061s/10 iters), loss = 7.02904
I0522 23:18:58.102270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02904 (* 1 = 7.02904 loss)
I0522 23:18:58.693811 35003 sgd_solver.cpp:112] Iteration 89380, lr = 0.01
I0522 23:19:02.366986 35003 solver.cpp:239] Iteration 89390 (2.3463 iter/s, 4.26203s/10 iters), loss = 6.13811
I0522 23:19:02.367041 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13811 (* 1 = 6.13811 loss)
I0522 23:19:02.380079 35003 sgd_solver.cpp:112] Iteration 89390, lr = 0.01
I0522 23:19:06.408797 35003 solver.cpp:239] Iteration 89400 (2.47428 iter/s, 4.04158s/10 iters), loss = 8.34493
I0522 23:19:06.408850 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.34493 (* 1 = 8.34493 loss)
I0522 23:19:06.416884 35003 sgd_solver.cpp:112] Iteration 89400, lr = 0.01
I0522 23:19:08.940698 35003 solver.cpp:239] Iteration 89410 (3.94985 iter/s, 2.53174s/10 iters), loss = 7.63214
I0522 23:19:08.940743 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63214 (* 1 = 7.63214 loss)
I0522 23:19:08.953390 35003 sgd_solver.cpp:112] Iteration 89410, lr = 0.01
I0522 23:19:12.316846 35003 solver.cpp:239] Iteration 89420 (2.96212 iter/s, 3.37596s/10 iters), loss = 6.86597
I0522 23:19:12.316895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86597 (* 1 = 6.86597 loss)
I0522 23:19:12.329877 35003 sgd_solver.cpp:112] Iteration 89420, lr = 0.01
I0522 23:19:13.721247 35003 solver.cpp:239] Iteration 89430 (7.12103 iter/s, 1.40429s/10 iters), loss = 7.22173
I0522 23:19:13.721297 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22173 (* 1 = 7.22173 loss)
I0522 23:19:14.456230 35003 sgd_solver.cpp:112] Iteration 89430, lr = 0.01
I0522 23:19:16.685984 35003 solver.cpp:239] Iteration 89440 (3.37318 iter/s, 2.96456s/10 iters), loss = 5.64676
I0522 23:19:16.686180 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.64676 (* 1 = 5.64676 loss)
I0522 23:19:17.044296 35003 sgd_solver.cpp:112] Iteration 89440, lr = 0.01
I0522 23:19:19.890933 35003 solver.cpp:239] Iteration 89450 (3.1205 iter/s, 3.20461s/10 iters), loss = 7.03113
I0522 23:19:19.891002 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03113 (* 1 = 7.03113 loss)
I0522 23:19:19.902740 35003 sgd_solver.cpp:112] Iteration 89450, lr = 0.01
I0522 23:19:21.974670 35003 solver.cpp:239] Iteration 89460 (4.79944 iter/s, 2.08358s/10 iters), loss = 8.36007
I0522 23:19:21.974736 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.36007 (* 1 = 8.36007 loss)
I0522 23:19:21.981987 35003 sgd_solver.cpp:112] Iteration 89460, lr = 0.01
I0522 23:19:24.964537 35003 solver.cpp:239] Iteration 89470 (3.34484 iter/s, 2.98968s/10 iters), loss = 8.69184
I0522 23:19:24.964578 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.69184 (* 1 = 8.69184 loss)
I0522 23:19:24.971756 35003 sgd_solver.cpp:112] Iteration 89470, lr = 0.01
I0522 23:19:27.892439 35003 solver.cpp:239] Iteration 89480 (3.41562 iter/s, 2.92773s/10 iters), loss = 7.2565
I0522 23:19:27.892488 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2565 (* 1 = 7.2565 loss)
I0522 23:19:27.920563 35003 sgd_solver.cpp:112] Iteration 89480, lr = 0.01
I0522 23:19:32.043020 35003 solver.cpp:239] Iteration 89490 (2.40943 iter/s, 4.15036s/10 iters), loss = 7.40799
I0522 23:19:32.043063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40799 (* 1 = 7.40799 loss)
I0522 23:19:32.252161 35003 sgd_solver.cpp:112] Iteration 89490, lr = 0.01
I0522 23:19:36.958667 35003 solver.cpp:239] Iteration 89500 (2.03442 iter/s, 4.9154s/10 iters), loss = 6.4989
I0522 23:19:36.958745 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4989 (* 1 = 6.4989 loss)
I0522 23:19:37.032846 35003 sgd_solver.cpp:112] Iteration 89500, lr = 0.01
I0522 23:19:40.629449 35003 solver.cpp:239] Iteration 89510 (2.72438 iter/s, 3.67055s/10 iters), loss = 6.86678
I0522 23:19:40.629487 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86678 (* 1 = 6.86678 loss)
I0522 23:19:40.636409 35003 sgd_solver.cpp:112] Iteration 89510, lr = 0.01
I0522 23:19:44.241839 35003 solver.cpp:239] Iteration 89520 (2.7684 iter/s, 3.6122s/10 iters), loss = 7.70041
I0522 23:19:44.241895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70041 (* 1 = 7.70041 loss)
I0522 23:19:44.972075 35003 sgd_solver.cpp:112] Iteration 89520, lr = 0.01
I0522 23:19:48.429356 35003 solver.cpp:239] Iteration 89530 (2.38818 iter/s, 4.18729s/10 iters), loss = 7.56278
I0522 23:19:48.429613 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56278 (* 1 = 7.56278 loss)
I0522 23:19:49.170277 35003 sgd_solver.cpp:112] Iteration 89530, lr = 0.01
I0522 23:19:54.183331 35003 solver.cpp:239] Iteration 89540 (1.73807 iter/s, 5.75351s/10 iters), loss = 8.01176
I0522 23:19:54.183388 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01176 (* 1 = 8.01176 loss)
I0522 23:19:54.212610 35003 sgd_solver.cpp:112] Iteration 89540, lr = 0.01
I0522 23:19:59.315358 35003 solver.cpp:239] Iteration 89550 (1.94865 iter/s, 5.13176s/10 iters), loss = 8.1378
I0522 23:19:59.315418 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.1378 (* 1 = 8.1378 loss)
I0522 23:19:59.461774 35003 sgd_solver.cpp:112] Iteration 89550, lr = 0.01
I0522 23:20:03.907532 35003 solver.cpp:239] Iteration 89560 (2.17774 iter/s, 4.59192s/10 iters), loss = 7.05562
I0522 23:20:03.907582 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05562 (* 1 = 7.05562 loss)
I0522 23:20:03.915544 35003 sgd_solver.cpp:112] Iteration 89560, lr = 0.01
I0522 23:20:08.114948 35003 solver.cpp:239] Iteration 89570 (2.37688 iter/s, 4.2072s/10 iters), loss = 7.36909
I0522 23:20:08.114991 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36909 (* 1 = 7.36909 loss)
I0522 23:20:08.121462 35003 sgd_solver.cpp:112] Iteration 89570, lr = 0.01
I0522 23:20:10.989442 35003 solver.cpp:239] Iteration 89580 (3.47908 iter/s, 2.87433s/10 iters), loss = 7.09074
I0522 23:20:10.989491 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09074 (* 1 = 7.09074 loss)
I0522 23:20:11.671905 35003 sgd_solver.cpp:112] Iteration 89580, lr = 0.01
I0522 23:20:14.982762 35003 solver.cpp:239] Iteration 89590 (2.50431 iter/s, 3.99311s/10 iters), loss = 7.67745
I0522 23:20:14.982811 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67745 (* 1 = 7.67745 loss)
I0522 23:20:14.996063 35003 sgd_solver.cpp:112] Iteration 89590, lr = 0.01
I0522 23:20:17.742413 35003 solver.cpp:239] Iteration 89600 (3.62387 iter/s, 2.75948s/10 iters), loss = 6.49507
I0522 23:20:17.742460 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49507 (* 1 = 6.49507 loss)
I0522 23:20:18.457206 35003 sgd_solver.cpp:112] Iteration 89600, lr = 0.01
I0522 23:20:22.726385 35003 solver.cpp:239] Iteration 89610 (2.00653 iter/s, 4.98372s/10 iters), loss = 8.24406
I0522 23:20:22.726433 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.24406 (* 1 = 8.24406 loss)
I0522 23:20:22.752748 35003 sgd_solver.cpp:112] Iteration 89610, lr = 0.01
I0522 23:20:26.200899 35003 solver.cpp:239] Iteration 89620 (2.87826 iter/s, 3.47432s/10 iters), loss = 7.68748
I0522 23:20:26.200945 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68748 (* 1 = 7.68748 loss)
I0522 23:20:26.219758 35003 sgd_solver.cpp:112] Iteration 89620, lr = 0.01
I0522 23:20:28.246279 35003 solver.cpp:239] Iteration 89630 (4.88939 iter/s, 2.04525s/10 iters), loss = 6.91375
I0522 23:20:28.246315 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91375 (* 1 = 6.91375 loss)
I0522 23:20:28.271442 35003 sgd_solver.cpp:112] Iteration 89630, lr = 0.01
I0522 23:20:30.735878 35003 solver.cpp:239] Iteration 89640 (4.01695 iter/s, 2.48945s/10 iters), loss = 7.99929
I0522 23:20:30.735921 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99929 (* 1 = 7.99929 loss)
I0522 23:20:30.740157 35003 sgd_solver.cpp:112] Iteration 89640, lr = 0.01
I0522 23:20:34.224772 35003 solver.cpp:239] Iteration 89650 (2.86639 iter/s, 3.48871s/10 iters), loss = 6.3471
I0522 23:20:34.224817 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3471 (* 1 = 6.3471 loss)
I0522 23:20:34.239729 35003 sgd_solver.cpp:112] Iteration 89650, lr = 0.01
I0522 23:20:37.758095 35003 solver.cpp:239] Iteration 89660 (2.83035 iter/s, 3.53313s/10 iters), loss = 7.59255
I0522 23:20:37.758137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59255 (* 1 = 7.59255 loss)
I0522 23:20:37.771503 35003 sgd_solver.cpp:112] Iteration 89660, lr = 0.01
I0522 23:20:40.774866 35003 solver.cpp:239] Iteration 89670 (3.31499 iter/s, 3.0166s/10 iters), loss = 7.42353
I0522 23:20:40.774909 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42353 (* 1 = 7.42353 loss)
I0522 23:20:40.788390 35003 sgd_solver.cpp:112] Iteration 89670, lr = 0.01
I0522 23:20:44.288542 35003 solver.cpp:239] Iteration 89680 (2.84618 iter/s, 3.51348s/10 iters), loss = 7.94985
I0522 23:20:44.288584 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94985 (* 1 = 7.94985 loss)
I0522 23:20:44.302477 35003 sgd_solver.cpp:112] Iteration 89680, lr = 0.01
I0522 23:20:47.388053 35003 solver.cpp:239] Iteration 89690 (3.2265 iter/s, 3.09933s/10 iters), loss = 7.75548
I0522 23:20:47.388098 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75548 (* 1 = 7.75548 loss)
I0522 23:20:47.398890 35003 sgd_solver.cpp:112] Iteration 89690, lr = 0.01
I0522 23:20:51.987874 35003 solver.cpp:239] Iteration 89700 (2.17411 iter/s, 4.59959s/10 iters), loss = 8.42319
I0522 23:20:51.987967 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.42319 (* 1 = 8.42319 loss)
I0522 23:20:51.995219 35003 sgd_solver.cpp:112] Iteration 89700, lr = 0.01
I0522 23:20:54.675951 35003 solver.cpp:239] Iteration 89710 (3.72042 iter/s, 2.68787s/10 iters), loss = 8.49951
I0522 23:20:54.675992 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.49951 (* 1 = 8.49951 loss)
I0522 23:20:54.681242 35003 sgd_solver.cpp:112] Iteration 89710, lr = 0.01
I0522 23:20:59.645411 35003 solver.cpp:239] Iteration 89720 (2.0124 iter/s, 4.96918s/10 iters), loss = 7.56587
I0522 23:20:59.645472 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56587 (* 1 = 7.56587 loss)
I0522 23:20:59.649781 35003 sgd_solver.cpp:112] Iteration 89720, lr = 0.01
I0522 23:21:03.834023 35003 solver.cpp:239] Iteration 89730 (2.38758 iter/s, 4.18835s/10 iters), loss = 7.17961
I0522 23:21:03.834074 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17961 (* 1 = 7.17961 loss)
I0522 23:21:04.534482 35003 sgd_solver.cpp:112] Iteration 89730, lr = 0.01
I0522 23:21:08.159636 35003 solver.cpp:239] Iteration 89740 (2.31193 iter/s, 4.32539s/10 iters), loss = 6.4411
I0522 23:21:08.159674 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4411 (* 1 = 6.4411 loss)
I0522 23:21:08.173724 35003 sgd_solver.cpp:112] Iteration 89740, lr = 0.01
I0522 23:21:10.833323 35003 solver.cpp:239] Iteration 89750 (3.74037 iter/s, 2.67353s/10 iters), loss = 6.25633
I0522 23:21:10.833362 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25633 (* 1 = 6.25633 loss)
I0522 23:21:10.846309 35003 sgd_solver.cpp:112] Iteration 89750, lr = 0.01
I0522 23:21:13.549403 35003 solver.cpp:239] Iteration 89760 (3.68199 iter/s, 2.71592s/10 iters), loss = 7.45383
I0522 23:21:13.549441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45383 (* 1 = 7.45383 loss)
I0522 23:21:13.560820 35003 sgd_solver.cpp:112] Iteration 89760, lr = 0.01
I0522 23:21:16.347434 35003 solver.cpp:239] Iteration 89770 (3.57416 iter/s, 2.79786s/10 iters), loss = 7.08485
I0522 23:21:16.347512 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08485 (* 1 = 7.08485 loss)
I0522 23:21:17.049104 35003 sgd_solver.cpp:112] Iteration 89770, lr = 0.01
I0522 23:21:21.550159 35003 solver.cpp:239] Iteration 89780 (1.92217 iter/s, 5.20244s/10 iters), loss = 6.41844
I0522 23:21:21.550195 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41844 (* 1 = 6.41844 loss)
I0522 23:21:21.557932 35003 sgd_solver.cpp:112] Iteration 89780, lr = 0.01
I0522 23:21:24.700894 35003 solver.cpp:239] Iteration 89790 (3.17404 iter/s, 3.15056s/10 iters), loss = 7.08027
I0522 23:21:24.701041 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08027 (* 1 = 7.08027 loss)
I0522 23:21:24.714040 35003 sgd_solver.cpp:112] Iteration 89790, lr = 0.01
I0522 23:21:27.595213 35003 solver.cpp:239] Iteration 89800 (3.45537 iter/s, 2.89405s/10 iters), loss = 7.80396
I0522 23:21:27.595255 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80396 (* 1 = 7.80396 loss)
I0522 23:21:28.335927 35003 sgd_solver.cpp:112] Iteration 89800, lr = 0.01
I0522 23:21:31.170166 35003 solver.cpp:239] Iteration 89810 (2.7974 iter/s, 3.57475s/10 iters), loss = 6.99025
I0522 23:21:31.170231 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99025 (* 1 = 6.99025 loss)
I0522 23:21:31.889250 35003 sgd_solver.cpp:112] Iteration 89810, lr = 0.01
I0522 23:21:35.512784 35003 solver.cpp:239] Iteration 89820 (2.30289 iter/s, 4.34238s/10 iters), loss = 7.12218
I0522 23:21:35.512837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12218 (* 1 = 7.12218 loss)
I0522 23:21:35.515879 35003 sgd_solver.cpp:112] Iteration 89820, lr = 0.01
I0522 23:21:37.800068 35003 solver.cpp:239] Iteration 89830 (4.3723 iter/s, 2.28713s/10 iters), loss = 7.25639
I0522 23:21:37.800108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25639 (* 1 = 7.25639 loss)
I0522 23:21:37.808295 35003 sgd_solver.cpp:112] Iteration 89830, lr = 0.01
I0522 23:21:41.166848 35003 solver.cpp:239] Iteration 89840 (2.97036 iter/s, 3.3666s/10 iters), loss = 7.35279
I0522 23:21:41.166887 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35279 (* 1 = 7.35279 loss)
I0522 23:21:41.173756 35003 sgd_solver.cpp:112] Iteration 89840, lr = 0.01
I0522 23:21:43.930639 35003 solver.cpp:239] Iteration 89850 (3.61842 iter/s, 2.76364s/10 iters), loss = 7.65852
I0522 23:21:43.930677 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65852 (* 1 = 7.65852 loss)
I0522 23:21:43.949156 35003 sgd_solver.cpp:112] Iteration 89850, lr = 0.01
I0522 23:21:46.699092 35003 solver.cpp:239] Iteration 89860 (3.61238 iter/s, 2.76826s/10 iters), loss = 6.78046
I0522 23:21:46.699160 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78046 (* 1 = 6.78046 loss)
I0522 23:21:46.706084 35003 sgd_solver.cpp:112] Iteration 89860, lr = 0.01
I0522 23:21:51.143277 35003 solver.cpp:239] Iteration 89870 (2.25026 iter/s, 4.44394s/10 iters), loss = 7.87734
I0522 23:21:51.143323 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87734 (* 1 = 7.87734 loss)
I0522 23:21:51.156477 35003 sgd_solver.cpp:112] Iteration 89870, lr = 0.01
I0522 23:21:53.859870 35003 solver.cpp:239] Iteration 89880 (3.6813 iter/s, 2.71643s/10 iters), loss = 8.28808
I0522 23:21:53.859920 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.28808 (* 1 = 8.28808 loss)
I0522 23:21:53.873275 35003 sgd_solver.cpp:112] Iteration 89880, lr = 0.01
I0522 23:21:55.413616 35003 solver.cpp:239] Iteration 89890 (6.43656 iter/s, 1.55363s/10 iters), loss = 6.56219
I0522 23:21:55.413851 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56219 (* 1 = 6.56219 loss)
I0522 23:21:55.426630 35003 sgd_solver.cpp:112] Iteration 89890, lr = 0.01
I0522 23:21:59.072866 35003 solver.cpp:239] Iteration 89900 (2.73309 iter/s, 3.65886s/10 iters), loss = 8.25881
I0522 23:21:59.072926 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.25881 (* 1 = 8.25881 loss)
I0522 23:21:59.080615 35003 sgd_solver.cpp:112] Iteration 89900, lr = 0.01
I0522 23:22:02.096104 35003 solver.cpp:239] Iteration 89910 (3.30792 iter/s, 3.02305s/10 iters), loss = 7.67335
I0522 23:22:02.096173 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67335 (* 1 = 7.67335 loss)
I0522 23:22:02.794646 35003 sgd_solver.cpp:112] Iteration 89910, lr = 0.01
I0522 23:22:05.672297 35003 solver.cpp:239] Iteration 89920 (2.79644 iter/s, 3.57597s/10 iters), loss = 7.65269
I0522 23:22:05.672353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65269 (* 1 = 7.65269 loss)
I0522 23:22:06.413327 35003 sgd_solver.cpp:112] Iteration 89920, lr = 0.01
I0522 23:22:10.620079 35003 solver.cpp:239] Iteration 89930 (2.02122 iter/s, 4.94751s/10 iters), loss = 7.80353
I0522 23:22:10.620146 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80353 (* 1 = 7.80353 loss)
I0522 23:22:10.632985 35003 sgd_solver.cpp:112] Iteration 89930, lr = 0.01
I0522 23:22:12.745863 35003 solver.cpp:239] Iteration 89940 (4.70448 iter/s, 2.12563s/10 iters), loss = 7.77399
I0522 23:22:12.745903 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77399 (* 1 = 7.77399 loss)
I0522 23:22:13.435354 35003 sgd_solver.cpp:112] Iteration 89940, lr = 0.01
I0522 23:22:15.496960 35003 solver.cpp:239] Iteration 89950 (3.63513 iter/s, 2.75093s/10 iters), loss = 6.85141
I0522 23:22:15.497015 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85141 (* 1 = 6.85141 loss)
I0522 23:22:15.499970 35003 sgd_solver.cpp:112] Iteration 89950, lr = 0.01
I0522 23:22:19.067890 35003 solver.cpp:239] Iteration 89960 (2.80056 iter/s, 3.57072s/10 iters), loss = 6.77227
I0522 23:22:19.067934 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77227 (* 1 = 6.77227 loss)
I0522 23:22:19.073397 35003 sgd_solver.cpp:112] Iteration 89960, lr = 0.01
I0522 23:22:22.950889 35003 solver.cpp:239] Iteration 89970 (2.57547 iter/s, 3.88279s/10 iters), loss = 6.97291
I0522 23:22:22.950943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97291 (* 1 = 6.97291 loss)
I0522 23:22:22.957396 35003 sgd_solver.cpp:112] Iteration 89970, lr = 0.01
I0522 23:22:28.059746 35003 solver.cpp:239] Iteration 89980 (1.95749 iter/s, 5.10859s/10 iters), loss = 5.8053
I0522 23:22:28.059895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8053 (* 1 = 5.8053 loss)
I0522 23:22:28.087846 35003 sgd_solver.cpp:112] Iteration 89980, lr = 0.01
I0522 23:22:31.729012 35003 solver.cpp:239] Iteration 89990 (2.72556 iter/s, 3.66897s/10 iters), loss = 6.85883
I0522 23:22:31.729058 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85883 (* 1 = 6.85883 loss)
I0522 23:22:31.742702 35003 sgd_solver.cpp:112] Iteration 89990, lr = 0.01
I0522 23:22:35.139046 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_90000.caffemodel
I0522 23:22:35.648952 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_90000.solverstate
I0522 23:22:35.854372 35003 solver.cpp:239] Iteration 90000 (2.42416 iter/s, 4.12514s/10 iters), loss = 6.77238
I0522 23:22:35.854429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77238 (* 1 = 6.77238 loss)
I0522 23:22:36.533893 35003 sgd_solver.cpp:112] Iteration 90000, lr = 0.01
I0522 23:22:40.774653 35003 solver.cpp:239] Iteration 90010 (2.03251 iter/s, 4.92002s/10 iters), loss = 7.21025
I0522 23:22:40.774735 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21025 (* 1 = 7.21025 loss)
I0522 23:22:40.783108 35003 sgd_solver.cpp:112] Iteration 90010, lr = 0.01
I0522 23:22:45.253428 35003 solver.cpp:239] Iteration 90020 (2.23289 iter/s, 4.47851s/10 iters), loss = 6.59937
I0522 23:22:45.253479 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59937 (* 1 = 6.59937 loss)
I0522 23:22:45.262770 35003 sgd_solver.cpp:112] Iteration 90020, lr = 0.01
I0522 23:22:48.914397 35003 solver.cpp:239] Iteration 90030 (2.73167 iter/s, 3.66076s/10 iters), loss = 7.46151
I0522 23:22:48.914458 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46151 (* 1 = 7.46151 loss)
I0522 23:22:49.610980 35003 sgd_solver.cpp:112] Iteration 90030, lr = 0.01
I0522 23:22:52.010128 35003 solver.cpp:239] Iteration 90040 (3.23045 iter/s, 3.09554s/10 iters), loss = 7.6839
I0522 23:22:52.010176 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6839 (* 1 = 7.6839 loss)
I0522 23:22:52.021697 35003 sgd_solver.cpp:112] Iteration 90040, lr = 0.01
I0522 23:22:54.730732 35003 solver.cpp:239] Iteration 90050 (3.67588 iter/s, 2.72044s/10 iters), loss = 6.15738
I0522 23:22:54.730773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15738 (* 1 = 6.15738 loss)
I0522 23:22:54.748759 35003 sgd_solver.cpp:112] Iteration 90050, lr = 0.01
I0522 23:22:58.584985 35003 solver.cpp:239] Iteration 90060 (2.59468 iter/s, 3.85405s/10 iters), loss = 8.3504
I0522 23:22:58.585181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.3504 (* 1 = 8.3504 loss)
I0522 23:22:58.597625 35003 sgd_solver.cpp:112] Iteration 90060, lr = 0.01
I0522 23:23:00.613380 35003 solver.cpp:239] Iteration 90070 (4.93069 iter/s, 2.02811s/10 iters), loss = 7.16636
I0522 23:23:00.613421 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16636 (* 1 = 7.16636 loss)
I0522 23:23:00.627535 35003 sgd_solver.cpp:112] Iteration 90070, lr = 0.01
I0522 23:23:04.052321 35003 solver.cpp:239] Iteration 90080 (2.90803 iter/s, 3.43875s/10 iters), loss = 7.58637
I0522 23:23:04.052376 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58637 (* 1 = 7.58637 loss)
I0522 23:23:04.072360 35003 sgd_solver.cpp:112] Iteration 90080, lr = 0.01
I0522 23:23:06.850563 35003 solver.cpp:239] Iteration 90090 (3.5739 iter/s, 2.79807s/10 iters), loss = 8.20657
I0522 23:23:06.850610 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.20657 (* 1 = 8.20657 loss)
I0522 23:23:06.863466 35003 sgd_solver.cpp:112] Iteration 90090, lr = 0.01
I0522 23:23:09.936750 35003 solver.cpp:239] Iteration 90100 (3.24043 iter/s, 3.08601s/10 iters), loss = 7.03595
I0522 23:23:09.936803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03595 (* 1 = 7.03595 loss)
I0522 23:23:10.541566 35003 sgd_solver.cpp:112] Iteration 90100, lr = 0.01
I0522 23:23:14.207592 35003 solver.cpp:239] Iteration 90110 (2.34159 iter/s, 4.27061s/10 iters), loss = 8.68947
I0522 23:23:14.207657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.68947 (* 1 = 8.68947 loss)
I0522 23:23:14.928825 35003 sgd_solver.cpp:112] Iteration 90110, lr = 0.01
I0522 23:23:17.752081 35003 solver.cpp:239] Iteration 90120 (2.82145 iter/s, 3.54427s/10 iters), loss = 7.59469
I0522 23:23:17.752130 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59469 (* 1 = 7.59469 loss)
I0522 23:23:18.473872 35003 sgd_solver.cpp:112] Iteration 90120, lr = 0.01
I0522 23:23:20.546980 35003 solver.cpp:239] Iteration 90130 (3.57816 iter/s, 2.79474s/10 iters), loss = 7.52291
I0522 23:23:20.547024 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52291 (* 1 = 7.52291 loss)
I0522 23:23:20.565050 35003 sgd_solver.cpp:112] Iteration 90130, lr = 0.01
I0522 23:23:24.308939 35003 solver.cpp:239] Iteration 90140 (2.65834 iter/s, 3.76174s/10 iters), loss = 7.59741
I0522 23:23:24.309011 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59741 (* 1 = 7.59741 loss)
I0522 23:23:24.987771 35003 sgd_solver.cpp:112] Iteration 90140, lr = 0.01
I0522 23:23:28.586331 35003 solver.cpp:239] Iteration 90150 (2.33801 iter/s, 4.27715s/10 iters), loss = 7.6205
I0522 23:23:28.586642 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6205 (* 1 = 7.6205 loss)
I0522 23:23:29.204339 35003 sgd_solver.cpp:112] Iteration 90150, lr = 0.01
I0522 23:23:32.828337 35003 solver.cpp:239] Iteration 90160 (2.35763 iter/s, 4.24155s/10 iters), loss = 6.68429
I0522 23:23:32.828377 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68429 (* 1 = 6.68429 loss)
I0522 23:23:32.841647 35003 sgd_solver.cpp:112] Iteration 90160, lr = 0.01
I0522 23:23:36.054548 35003 solver.cpp:239] Iteration 90170 (3.09978 iter/s, 3.22603s/10 iters), loss = 6.77769
I0522 23:23:36.054596 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77769 (* 1 = 6.77769 loss)
I0522 23:23:36.071300 35003 sgd_solver.cpp:112] Iteration 90170, lr = 0.01
I0522 23:23:38.631814 35003 solver.cpp:239] Iteration 90180 (3.88034 iter/s, 2.5771s/10 iters), loss = 7.29162
I0522 23:23:38.631870 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29162 (* 1 = 7.29162 loss)
I0522 23:23:39.308512 35003 sgd_solver.cpp:112] Iteration 90180, lr = 0.01
I0522 23:23:41.536733 35003 solver.cpp:239] Iteration 90190 (3.44265 iter/s, 2.90474s/10 iters), loss = 8.02515
I0522 23:23:41.536785 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02515 (* 1 = 8.02515 loss)
I0522 23:23:42.278189 35003 sgd_solver.cpp:112] Iteration 90190, lr = 0.01
I0522 23:23:43.570340 35003 solver.cpp:239] Iteration 90200 (4.91771 iter/s, 2.03347s/10 iters), loss = 6.93491
I0522 23:23:43.570380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93491 (* 1 = 6.93491 loss)
I0522 23:23:43.583154 35003 sgd_solver.cpp:112] Iteration 90200, lr = 0.01
I0522 23:23:46.483628 35003 solver.cpp:239] Iteration 90210 (3.43275 iter/s, 2.91312s/10 iters), loss = 7.29997
I0522 23:23:46.483683 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29997 (* 1 = 7.29997 loss)
I0522 23:23:46.497784 35003 sgd_solver.cpp:112] Iteration 90210, lr = 0.01
I0522 23:23:48.773942 35003 solver.cpp:239] Iteration 90220 (4.36652 iter/s, 2.29016s/10 iters), loss = 7.03892
I0522 23:23:48.773990 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03892 (* 1 = 7.03892 loss)
I0522 23:23:48.787376 35003 sgd_solver.cpp:112] Iteration 90220, lr = 0.01
I0522 23:23:51.188551 35003 solver.cpp:239] Iteration 90230 (4.14172 iter/s, 2.41446s/10 iters), loss = 7.37728
I0522 23:23:51.188590 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37728 (* 1 = 7.37728 loss)
I0522 23:23:51.201895 35003 sgd_solver.cpp:112] Iteration 90230, lr = 0.01
I0522 23:23:54.621317 35003 solver.cpp:239] Iteration 90240 (2.91326 iter/s, 3.43258s/10 iters), loss = 7.58388
I0522 23:23:54.621361 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58388 (* 1 = 7.58388 loss)
I0522 23:23:55.356232 35003 sgd_solver.cpp:112] Iteration 90240, lr = 0.01
I0522 23:23:59.069001 35003 solver.cpp:239] Iteration 90250 (2.24848 iter/s, 4.44745s/10 iters), loss = 6.69291
I0522 23:23:59.069145 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69291 (* 1 = 6.69291 loss)
I0522 23:23:59.074368 35003 sgd_solver.cpp:112] Iteration 90250, lr = 0.01
I0522 23:24:01.942098 35003 solver.cpp:239] Iteration 90260 (3.4809 iter/s, 2.87282s/10 iters), loss = 7.36637
I0522 23:24:01.942147 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36637 (* 1 = 7.36637 loss)
I0522 23:24:01.953411 35003 sgd_solver.cpp:112] Iteration 90260, lr = 0.01
I0522 23:24:06.405227 35003 solver.cpp:239] Iteration 90270 (2.24069 iter/s, 4.4629s/10 iters), loss = 6.67309
I0522 23:24:06.405269 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67309 (* 1 = 6.67309 loss)
I0522 23:24:06.418571 35003 sgd_solver.cpp:112] Iteration 90270, lr = 0.01
I0522 23:24:10.018457 35003 solver.cpp:239] Iteration 90280 (2.76777 iter/s, 3.61301s/10 iters), loss = 7.31309
I0522 23:24:10.018537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31309 (* 1 = 7.31309 loss)
I0522 23:24:10.048132 35003 sgd_solver.cpp:112] Iteration 90280, lr = 0.01
I0522 23:24:14.651170 35003 solver.cpp:239] Iteration 90290 (2.15868 iter/s, 4.63245s/10 iters), loss = 8.6328
I0522 23:24:14.651224 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.6328 (* 1 = 8.6328 loss)
I0522 23:24:15.223786 35003 sgd_solver.cpp:112] Iteration 90290, lr = 0.01
I0522 23:24:19.544595 35003 solver.cpp:239] Iteration 90300 (2.04366 iter/s, 4.89317s/10 iters), loss = 6.87834
I0522 23:24:19.544646 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87834 (* 1 = 6.87834 loss)
I0522 23:24:20.285945 35003 sgd_solver.cpp:112] Iteration 90300, lr = 0.01
I0522 23:24:22.051964 35003 solver.cpp:239] Iteration 90310 (3.9885 iter/s, 2.50721s/10 iters), loss = 8.0436
I0522 23:24:22.052009 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0436 (* 1 = 8.0436 loss)
I0522 23:24:22.059540 35003 sgd_solver.cpp:112] Iteration 90310, lr = 0.01
I0522 23:24:25.962806 35003 solver.cpp:239] Iteration 90320 (2.55713 iter/s, 3.91063s/10 iters), loss = 7.87971
I0522 23:24:25.962873 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87971 (* 1 = 7.87971 loss)
I0522 23:24:25.990870 35003 sgd_solver.cpp:112] Iteration 90320, lr = 0.01
I0522 23:24:28.792039 35003 solver.cpp:239] Iteration 90330 (3.53475 iter/s, 2.82905s/10 iters), loss = 8.07844
I0522 23:24:28.792079 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.07844 (* 1 = 8.07844 loss)
I0522 23:24:29.244530 35003 sgd_solver.cpp:112] Iteration 90330, lr = 0.01
I0522 23:24:32.273794 35003 solver.cpp:239] Iteration 90340 (2.87227 iter/s, 3.48156s/10 iters), loss = 7.79754
I0522 23:24:32.273839 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79754 (* 1 = 7.79754 loss)
I0522 23:24:32.286434 35003 sgd_solver.cpp:112] Iteration 90340, lr = 0.01
I0522 23:24:35.762533 35003 solver.cpp:239] Iteration 90350 (2.86652 iter/s, 3.48855s/10 iters), loss = 7.80663
I0522 23:24:35.762575 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80663 (* 1 = 7.80663 loss)
I0522 23:24:35.775080 35003 sgd_solver.cpp:112] Iteration 90350, lr = 0.01
I0522 23:24:39.360340 35003 solver.cpp:239] Iteration 90360 (2.77964 iter/s, 3.59759s/10 iters), loss = 8.32357
I0522 23:24:39.360411 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.32357 (* 1 = 8.32357 loss)
I0522 23:24:39.373478 35003 sgd_solver.cpp:112] Iteration 90360, lr = 0.01
I0522 23:24:41.456809 35003 solver.cpp:239] Iteration 90370 (4.77029 iter/s, 2.09631s/10 iters), loss = 8.13211
I0522 23:24:41.456851 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.13211 (* 1 = 8.13211 loss)
I0522 23:24:41.469907 35003 sgd_solver.cpp:112] Iteration 90370, lr = 0.01
I0522 23:24:45.824632 35003 solver.cpp:239] Iteration 90380 (2.28959 iter/s, 4.3676s/10 iters), loss = 7.50311
I0522 23:24:45.824673 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50311 (* 1 = 7.50311 loss)
I0522 23:24:45.838037 35003 sgd_solver.cpp:112] Iteration 90380, lr = 0.01
I0522 23:24:50.139048 35003 solver.cpp:239] Iteration 90390 (2.31793 iter/s, 4.31419s/10 iters), loss = 8.00962
I0522 23:24:50.139091 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00962 (* 1 = 8.00962 loss)
I0522 23:24:50.157192 35003 sgd_solver.cpp:112] Iteration 90390, lr = 0.01
I0522 23:24:51.518678 35003 solver.cpp:239] Iteration 90400 (7.24889 iter/s, 1.37952s/10 iters), loss = 8.0521
I0522 23:24:51.518731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0521 (* 1 = 8.0521 loss)
I0522 23:24:51.530743 35003 sgd_solver.cpp:112] Iteration 90400, lr = 0.01
I0522 23:24:54.406081 35003 solver.cpp:239] Iteration 90410 (3.46354 iter/s, 2.88722s/10 iters), loss = 7.02215
I0522 23:24:54.406138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02215 (* 1 = 7.02215 loss)
I0522 23:24:54.417790 35003 sgd_solver.cpp:112] Iteration 90410, lr = 0.01
I0522 23:24:59.414104 35003 solver.cpp:239] Iteration 90420 (1.9969 iter/s, 5.00777s/10 iters), loss = 7.65412
I0522 23:24:59.414381 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65412 (* 1 = 7.65412 loss)
I0522 23:24:59.427503 35003 sgd_solver.cpp:112] Iteration 90420, lr = 0.01
I0522 23:25:03.068811 35003 solver.cpp:239] Iteration 90430 (2.7365 iter/s, 3.65431s/10 iters), loss = 6.75301
I0522 23:25:03.068861 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75301 (* 1 = 6.75301 loss)
I0522 23:25:03.777827 35003 sgd_solver.cpp:112] Iteration 90430, lr = 0.01
I0522 23:25:05.267192 35003 solver.cpp:239] Iteration 90440 (4.54911 iter/s, 2.19823s/10 iters), loss = 6.66423
I0522 23:25:05.267230 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66423 (* 1 = 6.66423 loss)
I0522 23:25:05.285182 35003 sgd_solver.cpp:112] Iteration 90440, lr = 0.01
I0522 23:25:09.980775 35003 solver.cpp:239] Iteration 90450 (2.12163 iter/s, 4.71335s/10 iters), loss = 7.23303
I0522 23:25:09.980814 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23303 (* 1 = 7.23303 loss)
I0522 23:25:09.992326 35003 sgd_solver.cpp:112] Iteration 90450, lr = 0.01
I0522 23:25:13.828966 35003 solver.cpp:239] Iteration 90460 (2.59879 iter/s, 3.84795s/10 iters), loss = 8.12656
I0522 23:25:13.829051 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.12656 (* 1 = 8.12656 loss)
I0522 23:25:13.901831 35003 sgd_solver.cpp:112] Iteration 90460, lr = 0.01
I0522 23:25:17.449718 35003 solver.cpp:239] Iteration 90470 (2.76204 iter/s, 3.62052s/10 iters), loss = 7.52549
I0522 23:25:17.449767 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52549 (* 1 = 7.52549 loss)
I0522 23:25:17.454018 35003 sgd_solver.cpp:112] Iteration 90470, lr = 0.01
I0522 23:25:21.099203 35003 solver.cpp:239] Iteration 90480 (2.74027 iter/s, 3.64928s/10 iters), loss = 6.24133
I0522 23:25:21.099251 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24133 (* 1 = 6.24133 loss)
I0522 23:25:21.110581 35003 sgd_solver.cpp:112] Iteration 90480, lr = 0.01
I0522 23:25:25.221729 35003 solver.cpp:239] Iteration 90490 (2.42582 iter/s, 4.12231s/10 iters), loss = 6.89022
I0522 23:25:25.221776 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89022 (* 1 = 6.89022 loss)
I0522 23:25:25.228983 35003 sgd_solver.cpp:112] Iteration 90490, lr = 0.01
I0522 23:25:29.142455 35003 solver.cpp:239] Iteration 90500 (2.55068 iter/s, 3.92052s/10 iters), loss = 6.7593
I0522 23:25:29.142515 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7593 (* 1 = 6.7593 loss)
I0522 23:25:29.732583 35003 sgd_solver.cpp:112] Iteration 90500, lr = 0.01
I0522 23:25:32.676620 35003 solver.cpp:239] Iteration 90510 (2.82969 iter/s, 3.53395s/10 iters), loss = 6.88771
I0522 23:25:32.676657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88771 (* 1 = 6.88771 loss)
I0522 23:25:33.405239 35003 sgd_solver.cpp:112] Iteration 90510, lr = 0.01
I0522 23:25:36.225128 35003 solver.cpp:239] Iteration 90520 (2.81823 iter/s, 3.54833s/10 iters), loss = 7.4367
I0522 23:25:36.225170 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4367 (* 1 = 7.4367 loss)
I0522 23:25:36.238063 35003 sgd_solver.cpp:112] Iteration 90520, lr = 0.01
I0522 23:25:39.721871 35003 solver.cpp:239] Iteration 90530 (2.85996 iter/s, 3.49655s/10 iters), loss = 6.43444
I0522 23:25:39.721918 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43444 (* 1 = 6.43444 loss)
I0522 23:25:39.734688 35003 sgd_solver.cpp:112] Iteration 90530, lr = 0.01
I0522 23:25:42.611876 35003 solver.cpp:239] Iteration 90540 (3.46041 iter/s, 2.88983s/10 iters), loss = 7.08981
I0522 23:25:42.611925 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08981 (* 1 = 7.08981 loss)
I0522 23:25:42.617923 35003 sgd_solver.cpp:112] Iteration 90540, lr = 0.01
I0522 23:25:44.833475 35003 solver.cpp:239] Iteration 90550 (4.50157 iter/s, 2.22145s/10 iters), loss = 6.45401
I0522 23:25:44.833520 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45401 (* 1 = 6.45401 loss)
I0522 23:25:45.476220 35003 sgd_solver.cpp:112] Iteration 90550, lr = 0.01
I0522 23:25:48.369114 35003 solver.cpp:239] Iteration 90560 (2.8285 iter/s, 3.53544s/10 iters), loss = 7.95084
I0522 23:25:48.369174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95084 (* 1 = 7.95084 loss)
I0522 23:25:49.096320 35003 sgd_solver.cpp:112] Iteration 90560, lr = 0.01
I0522 23:25:53.801309 35003 solver.cpp:239] Iteration 90570 (1.84097 iter/s, 5.43192s/10 iters), loss = 6.41685
I0522 23:25:53.801357 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41685 (* 1 = 6.41685 loss)
I0522 23:25:53.808645 35003 sgd_solver.cpp:112] Iteration 90570, lr = 0.01
I0522 23:25:59.045123 35003 solver.cpp:239] Iteration 90580 (1.90711 iter/s, 5.24354s/10 iters), loss = 7.32914
I0522 23:25:59.045171 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32914 (* 1 = 7.32914 loss)
I0522 23:25:59.058248 35003 sgd_solver.cpp:112] Iteration 90580, lr = 0.01
I0522 23:26:04.938099 35003 solver.cpp:239] Iteration 90590 (1.69702 iter/s, 5.89269s/10 iters), loss = 6.90149
I0522 23:26:04.938304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90149 (* 1 = 6.90149 loss)
I0522 23:26:05.653230 35003 sgd_solver.cpp:112] Iteration 90590, lr = 0.01
I0522 23:26:09.988523 35003 solver.cpp:239] Iteration 90600 (1.98019 iter/s, 5.05002s/10 iters), loss = 7.95122
I0522 23:26:09.988570 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95122 (* 1 = 7.95122 loss)
I0522 23:26:09.997989 35003 sgd_solver.cpp:112] Iteration 90600, lr = 0.01
I0522 23:26:12.806246 35003 solver.cpp:239] Iteration 90610 (3.54917 iter/s, 2.81756s/10 iters), loss = 7.25212
I0522 23:26:12.806282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25212 (* 1 = 7.25212 loss)
I0522 23:26:12.814271 35003 sgd_solver.cpp:112] Iteration 90610, lr = 0.01
I0522 23:26:15.130131 35003 solver.cpp:239] Iteration 90620 (4.30341 iter/s, 2.32374s/10 iters), loss = 6.91819
I0522 23:26:15.130178 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91819 (* 1 = 6.91819 loss)
I0522 23:26:15.133311 35003 sgd_solver.cpp:112] Iteration 90620, lr = 0.01
I0522 23:26:19.572312 35003 solver.cpp:239] Iteration 90630 (2.25126 iter/s, 4.44195s/10 iters), loss = 6.98366
I0522 23:26:19.572358 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98366 (* 1 = 6.98366 loss)
I0522 23:26:20.310236 35003 sgd_solver.cpp:112] Iteration 90630, lr = 0.01
I0522 23:26:22.805342 35003 solver.cpp:239] Iteration 90640 (3.09325 iter/s, 3.23285s/10 iters), loss = 6.81864
I0522 23:26:22.805384 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81864 (* 1 = 6.81864 loss)
I0522 23:26:23.451414 35003 sgd_solver.cpp:112] Iteration 90640, lr = 0.01
I0522 23:26:27.014262 35003 solver.cpp:239] Iteration 90650 (2.37603 iter/s, 4.2087s/10 iters), loss = 7.54881
I0522 23:26:27.014307 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54881 (* 1 = 7.54881 loss)
I0522 23:26:27.027875 35003 sgd_solver.cpp:112] Iteration 90650, lr = 0.01
I0522 23:26:31.380825 35003 solver.cpp:239] Iteration 90660 (2.29025 iter/s, 4.36634s/10 iters), loss = 6.87935
I0522 23:26:31.380863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87935 (* 1 = 6.87935 loss)
I0522 23:26:31.393553 35003 sgd_solver.cpp:112] Iteration 90660, lr = 0.01
I0522 23:26:35.463162 35003 solver.cpp:239] Iteration 90670 (2.4497 iter/s, 4.08213s/10 iters), loss = 6.55338
I0522 23:26:35.463484 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55338 (* 1 = 6.55338 loss)
I0522 23:26:35.476348 35003 sgd_solver.cpp:112] Iteration 90670, lr = 0.01
I0522 23:26:38.157876 35003 solver.cpp:239] Iteration 90680 (3.71153 iter/s, 2.69431s/10 iters), loss = 8.52303
I0522 23:26:38.157918 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.52303 (* 1 = 8.52303 loss)
I0522 23:26:38.465421 35003 sgd_solver.cpp:112] Iteration 90680, lr = 0.01
I0522 23:26:41.044723 35003 solver.cpp:239] Iteration 90690 (3.46419 iter/s, 2.88667s/10 iters), loss = 7.3526
I0522 23:26:41.044783 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3526 (* 1 = 7.3526 loss)
I0522 23:26:41.656790 35003 sgd_solver.cpp:112] Iteration 90690, lr = 0.01
I0522 23:26:43.665083 35003 solver.cpp:239] Iteration 90700 (3.81652 iter/s, 2.62019s/10 iters), loss = 6.43515
I0522 23:26:43.665133 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43515 (* 1 = 6.43515 loss)
I0522 23:26:43.672194 35003 sgd_solver.cpp:112] Iteration 90700, lr = 0.01
I0522 23:26:46.709972 35003 solver.cpp:239] Iteration 90710 (3.28438 iter/s, 3.04472s/10 iters), loss = 7.63798
I0522 23:26:46.710016 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63798 (* 1 = 7.63798 loss)
I0522 23:26:47.230528 35003 sgd_solver.cpp:112] Iteration 90710, lr = 0.01
I0522 23:26:51.393332 35003 solver.cpp:239] Iteration 90720 (2.13533 iter/s, 4.68312s/10 iters), loss = 8.67167
I0522 23:26:51.393371 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.67167 (* 1 = 8.67167 loss)
I0522 23:26:51.418818 35003 sgd_solver.cpp:112] Iteration 90720, lr = 0.01
I0522 23:26:54.984781 35003 solver.cpp:239] Iteration 90730 (2.78454 iter/s, 3.59126s/10 iters), loss = 7.39168
I0522 23:26:54.984838 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39168 (* 1 = 7.39168 loss)
I0522 23:26:55.713373 35003 sgd_solver.cpp:112] Iteration 90730, lr = 0.01
I0522 23:26:57.107462 35003 solver.cpp:239] Iteration 90740 (4.71135 iter/s, 2.12253s/10 iters), loss = 6.93446
I0522 23:26:57.107509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93446 (* 1 = 6.93446 loss)
I0522 23:26:57.120180 35003 sgd_solver.cpp:112] Iteration 90740, lr = 0.01
I0522 23:27:00.715576 35003 solver.cpp:239] Iteration 90750 (2.77168 iter/s, 3.60792s/10 iters), loss = 7.73857
I0522 23:27:00.715615 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73857 (* 1 = 7.73857 loss)
I0522 23:27:00.722990 35003 sgd_solver.cpp:112] Iteration 90750, lr = 0.01
I0522 23:27:03.550801 35003 solver.cpp:239] Iteration 90760 (3.52726 iter/s, 2.83506s/10 iters), loss = 7.5986
I0522 23:27:03.550849 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5986 (* 1 = 7.5986 loss)
I0522 23:27:03.556998 35003 sgd_solver.cpp:112] Iteration 90760, lr = 0.01
I0522 23:27:06.329059 35003 solver.cpp:239] Iteration 90770 (3.5996 iter/s, 2.77809s/10 iters), loss = 8.39251
I0522 23:27:06.329190 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.39251 (* 1 = 8.39251 loss)
I0522 23:27:06.342176 35003 sgd_solver.cpp:112] Iteration 90770, lr = 0.01
I0522 23:27:09.490461 35003 solver.cpp:239] Iteration 90780 (3.16342 iter/s, 3.16114s/10 iters), loss = 6.7184
I0522 23:27:09.490509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7184 (* 1 = 6.7184 loss)
I0522 23:27:09.493391 35003 sgd_solver.cpp:112] Iteration 90780, lr = 0.01
I0522 23:27:11.653229 35003 solver.cpp:239] Iteration 90790 (4.62405 iter/s, 2.1626s/10 iters), loss = 7.61307
I0522 23:27:11.653276 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61307 (* 1 = 7.61307 loss)
I0522 23:27:11.664674 35003 sgd_solver.cpp:112] Iteration 90790, lr = 0.01
I0522 23:27:15.181290 35003 solver.cpp:239] Iteration 90800 (2.83458 iter/s, 3.52786s/10 iters), loss = 7.66092
I0522 23:27:15.181349 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66092 (* 1 = 7.66092 loss)
I0522 23:27:15.189106 35003 sgd_solver.cpp:112] Iteration 90800, lr = 0.01
I0522 23:27:17.992678 35003 solver.cpp:239] Iteration 90810 (3.55719 iter/s, 2.8112s/10 iters), loss = 6.58427
I0522 23:27:17.992723 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58427 (* 1 = 6.58427 loss)
I0522 23:27:18.025275 35003 sgd_solver.cpp:112] Iteration 90810, lr = 0.01
I0522 23:27:21.508528 35003 solver.cpp:239] Iteration 90820 (2.84441 iter/s, 3.51566s/10 iters), loss = 6.39317
I0522 23:27:21.508570 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39317 (* 1 = 6.39317 loss)
I0522 23:27:22.067698 35003 sgd_solver.cpp:112] Iteration 90820, lr = 0.01
I0522 23:27:25.089428 35003 solver.cpp:239] Iteration 90830 (2.79275 iter/s, 3.5807s/10 iters), loss = 7.11036
I0522 23:27:25.089474 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11036 (* 1 = 7.11036 loss)
I0522 23:27:25.097681 35003 sgd_solver.cpp:112] Iteration 90830, lr = 0.01
I0522 23:27:28.597860 35003 solver.cpp:239] Iteration 90840 (2.85044 iter/s, 3.50823s/10 iters), loss = 7.35509
I0522 23:27:28.597925 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35509 (* 1 = 7.35509 loss)
I0522 23:27:28.683207 35003 sgd_solver.cpp:112] Iteration 90840, lr = 0.01
I0522 23:27:32.888694 35003 solver.cpp:239] Iteration 90850 (2.33068 iter/s, 4.2906s/10 iters), loss = 7.87163
I0522 23:27:32.888731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87163 (* 1 = 7.87163 loss)
I0522 23:27:32.901257 35003 sgd_solver.cpp:112] Iteration 90850, lr = 0.01
I0522 23:27:35.753048 35003 solver.cpp:239] Iteration 90860 (3.49139 iter/s, 2.86419s/10 iters), loss = 8.08559
I0522 23:27:35.753103 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08559 (* 1 = 8.08559 loss)
I0522 23:27:35.756911 35003 sgd_solver.cpp:112] Iteration 90860, lr = 0.01
I0522 23:27:39.256384 35003 solver.cpp:239] Iteration 90870 (2.8546 iter/s, 3.50312s/10 iters), loss = 6.53842
I0522 23:27:39.256603 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53842 (* 1 = 6.53842 loss)
I0522 23:27:39.269667 35003 sgd_solver.cpp:112] Iteration 90870, lr = 0.01
I0522 23:27:41.471439 35003 solver.cpp:239] Iteration 90880 (4.51516 iter/s, 2.21476s/10 iters), loss = 7.11024
I0522 23:27:41.471487 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11024 (* 1 = 7.11024 loss)
I0522 23:27:42.177078 35003 sgd_solver.cpp:112] Iteration 90880, lr = 0.01
I0522 23:27:45.767699 35003 solver.cpp:239] Iteration 90890 (2.32772 iter/s, 4.29604s/10 iters), loss = 7.02901
I0522 23:27:45.767742 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02901 (* 1 = 7.02901 loss)
I0522 23:27:45.777493 35003 sgd_solver.cpp:112] Iteration 90890, lr = 0.01
I0522 23:27:48.983893 35003 solver.cpp:239] Iteration 90900 (3.10944 iter/s, 3.21601s/10 iters), loss = 6.30544
I0522 23:27:48.983956 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30544 (* 1 = 6.30544 loss)
I0522 23:27:48.996860 35003 sgd_solver.cpp:112] Iteration 90900, lr = 0.01
I0522 23:27:52.283201 35003 solver.cpp:239] Iteration 90910 (3.03113 iter/s, 3.2991s/10 iters), loss = 7.13583
I0522 23:27:52.283265 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13583 (* 1 = 7.13583 loss)
I0522 23:27:52.309406 35003 sgd_solver.cpp:112] Iteration 90910, lr = 0.01
I0522 23:27:55.108587 35003 solver.cpp:239] Iteration 90920 (3.53958 iter/s, 2.8252s/10 iters), loss = 8.30213
I0522 23:27:55.108640 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.30213 (* 1 = 8.30213 loss)
I0522 23:27:55.112792 35003 sgd_solver.cpp:112] Iteration 90920, lr = 0.01
I0522 23:27:58.015944 35003 solver.cpp:239] Iteration 90930 (3.43976 iter/s, 2.90718s/10 iters), loss = 7.28683
I0522 23:27:58.015980 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28683 (* 1 = 7.28683 loss)
I0522 23:27:58.731266 35003 sgd_solver.cpp:112] Iteration 90930, lr = 0.01
I0522 23:28:03.055804 35003 solver.cpp:239] Iteration 90940 (1.98428 iter/s, 5.03961s/10 iters), loss = 7.72349
I0522 23:28:03.055862 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72349 (* 1 = 7.72349 loss)
I0522 23:28:03.063722 35003 sgd_solver.cpp:112] Iteration 90940, lr = 0.01
I0522 23:28:07.279126 35003 solver.cpp:239] Iteration 90950 (2.3681 iter/s, 4.22279s/10 iters), loss = 7.15713
I0522 23:28:07.279175 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15713 (* 1 = 7.15713 loss)
I0522 23:28:07.287197 35003 sgd_solver.cpp:112] Iteration 90950, lr = 0.01
I0522 23:28:11.594449 35003 solver.cpp:239] Iteration 90960 (2.31744 iter/s, 4.3151s/10 iters), loss = 8.1897
I0522 23:28:11.594774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.1897 (* 1 = 8.1897 loss)
I0522 23:28:11.607816 35003 sgd_solver.cpp:112] Iteration 90960, lr = 0.01
I0522 23:28:14.464056 35003 solver.cpp:239] Iteration 90970 (3.48532 iter/s, 2.86918s/10 iters), loss = 7.62042
I0522 23:28:14.464103 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62042 (* 1 = 7.62042 loss)
I0522 23:28:14.722004 35003 sgd_solver.cpp:112] Iteration 90970, lr = 0.01
I0522 23:28:17.389479 35003 solver.cpp:239] Iteration 90980 (3.41852 iter/s, 2.92524s/10 iters), loss = 7.07806
I0522 23:28:17.389521 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07806 (* 1 = 7.07806 loss)
I0522 23:28:17.396320 35003 sgd_solver.cpp:112] Iteration 90980, lr = 0.01
I0522 23:28:21.741075 35003 solver.cpp:239] Iteration 90990 (2.29812 iter/s, 4.35138s/10 iters), loss = 7.36737
I0522 23:28:21.741119 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36737 (* 1 = 7.36737 loss)
I0522 23:28:21.747012 35003 sgd_solver.cpp:112] Iteration 90990, lr = 0.01
I0522 23:28:25.322430 35003 solver.cpp:239] Iteration 91000 (2.79241 iter/s, 3.58114s/10 iters), loss = 8.16803
I0522 23:28:25.322480 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16803 (* 1 = 8.16803 loss)
I0522 23:28:25.346170 35003 sgd_solver.cpp:112] Iteration 91000, lr = 0.01
I0522 23:28:27.440840 35003 solver.cpp:239] Iteration 91010 (4.72084 iter/s, 2.11827s/10 iters), loss = 7.32629
I0522 23:28:27.440887 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32629 (* 1 = 7.32629 loss)
I0522 23:28:28.046773 35003 sgd_solver.cpp:112] Iteration 91010, lr = 0.01
I0522 23:28:32.061036 35003 solver.cpp:239] Iteration 91020 (2.16453 iter/s, 4.61995s/10 iters), loss = 8.31688
I0522 23:28:32.061079 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.31688 (* 1 = 8.31688 loss)
I0522 23:28:32.079836 35003 sgd_solver.cpp:112] Iteration 91020, lr = 0.01
I0522 23:28:35.018263 35003 solver.cpp:239] Iteration 91030 (3.38173 iter/s, 2.95706s/10 iters), loss = 7.40225
I0522 23:28:35.018304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40225 (* 1 = 7.40225 loss)
I0522 23:28:35.248013 35003 sgd_solver.cpp:112] Iteration 91030, lr = 0.01
I0522 23:28:38.137425 35003 solver.cpp:239] Iteration 91040 (3.20618 iter/s, 3.11898s/10 iters), loss = 8.65545
I0522 23:28:38.137485 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.65545 (* 1 = 8.65545 loss)
I0522 23:28:38.878489 35003 sgd_solver.cpp:112] Iteration 91040, lr = 0.01
I0522 23:28:41.974843 35003 solver.cpp:239] Iteration 91050 (2.60606 iter/s, 3.8372s/10 iters), loss = 7.13301
I0522 23:28:41.975062 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13301 (* 1 = 7.13301 loss)
I0522 23:28:41.993475 35003 sgd_solver.cpp:112] Iteration 91050, lr = 0.01
I0522 23:28:44.743458 35003 solver.cpp:239] Iteration 91060 (3.61232 iter/s, 2.76831s/10 iters), loss = 6.96748
I0522 23:28:44.743497 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96748 (* 1 = 6.96748 loss)
I0522 23:28:44.768046 35003 sgd_solver.cpp:112] Iteration 91060, lr = 0.01
I0522 23:28:48.340093 35003 solver.cpp:239] Iteration 91070 (2.78052 iter/s, 3.59645s/10 iters), loss = 6.76073
I0522 23:28:48.340129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76073 (* 1 = 6.76073 loss)
I0522 23:28:48.346173 35003 sgd_solver.cpp:112] Iteration 91070, lr = 0.01
I0522 23:28:51.802744 35003 solver.cpp:239] Iteration 91080 (2.88814 iter/s, 3.46243s/10 iters), loss = 6.8028
I0522 23:28:51.802803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8028 (* 1 = 6.8028 loss)
I0522 23:28:51.830008 35003 sgd_solver.cpp:112] Iteration 91080, lr = 0.01
I0522 23:28:55.371881 35003 solver.cpp:239] Iteration 91090 (2.80196 iter/s, 3.56893s/10 iters), loss = 7.90913
I0522 23:28:55.371929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90913 (* 1 = 7.90913 loss)
I0522 23:28:55.392534 35003 sgd_solver.cpp:112] Iteration 91090, lr = 0.01
I0522 23:28:59.684530 35003 solver.cpp:239] Iteration 91100 (2.31888 iter/s, 4.31243s/10 iters), loss = 7.61328
I0522 23:28:59.684576 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61328 (* 1 = 7.61328 loss)
I0522 23:28:59.690740 35003 sgd_solver.cpp:112] Iteration 91100, lr = 0.01
I0522 23:29:02.075546 35003 solver.cpp:239] Iteration 91110 (4.1826 iter/s, 2.39086s/10 iters), loss = 7.1163
I0522 23:29:02.075597 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1163 (* 1 = 7.1163 loss)
I0522 23:29:02.088793 35003 sgd_solver.cpp:112] Iteration 91110, lr = 0.01
I0522 23:29:05.927091 35003 solver.cpp:239] Iteration 91120 (2.5965 iter/s, 3.85133s/10 iters), loss = 6.7757
I0522 23:29:05.927130 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7757 (* 1 = 6.7757 loss)
I0522 23:29:05.939908 35003 sgd_solver.cpp:112] Iteration 91120, lr = 0.01
I0522 23:29:08.755275 35003 solver.cpp:239] Iteration 91130 (3.53605 iter/s, 2.82802s/10 iters), loss = 7.34657
I0522 23:29:08.755328 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34657 (* 1 = 7.34657 loss)
I0522 23:29:08.768225 35003 sgd_solver.cpp:112] Iteration 91130, lr = 0.01
I0522 23:29:11.637533 35003 solver.cpp:239] Iteration 91140 (3.46972 iter/s, 2.88208s/10 iters), loss = 7.38856
I0522 23:29:11.637687 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38856 (* 1 = 7.38856 loss)
I0522 23:29:11.658975 35003 sgd_solver.cpp:112] Iteration 91140, lr = 0.01
I0522 23:29:14.051080 35003 solver.cpp:239] Iteration 91150 (4.14371 iter/s, 2.4133s/10 iters), loss = 6.81678
I0522 23:29:14.051300 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81678 (* 1 = 6.81678 loss)
I0522 23:29:14.064486 35003 sgd_solver.cpp:112] Iteration 91150, lr = 0.01
I0522 23:29:16.955799 35003 solver.cpp:239] Iteration 91160 (3.44308 iter/s, 2.90438s/10 iters), loss = 6.26684
I0522 23:29:16.955857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26684 (* 1 = 6.26684 loss)
I0522 23:29:17.696712 35003 sgd_solver.cpp:112] Iteration 91160, lr = 0.01
I0522 23:29:19.681365 35003 solver.cpp:239] Iteration 91170 (3.6692 iter/s, 2.72539s/10 iters), loss = 8.06879
I0522 23:29:19.681411 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.06879 (* 1 = 8.06879 loss)
I0522 23:29:20.370620 35003 sgd_solver.cpp:112] Iteration 91170, lr = 0.01
I0522 23:29:23.332509 35003 solver.cpp:239] Iteration 91180 (2.73902 iter/s, 3.65094s/10 iters), loss = 7.66224
I0522 23:29:23.332571 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66224 (* 1 = 7.66224 loss)
I0522 23:29:23.974010 35003 sgd_solver.cpp:112] Iteration 91180, lr = 0.01
I0522 23:29:28.537303 35003 solver.cpp:239] Iteration 91190 (1.92143 iter/s, 5.20445s/10 iters), loss = 6.82088
I0522 23:29:28.537351 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82088 (* 1 = 6.82088 loss)
I0522 23:29:28.719486 35003 sgd_solver.cpp:112] Iteration 91190, lr = 0.01
I0522 23:29:31.451637 35003 solver.cpp:239] Iteration 91200 (3.43152 iter/s, 2.91416s/10 iters), loss = 7.48428
I0522 23:29:31.451680 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48428 (* 1 = 7.48428 loss)
I0522 23:29:31.459640 35003 sgd_solver.cpp:112] Iteration 91200, lr = 0.01
I0522 23:29:34.274749 35003 solver.cpp:239] Iteration 91210 (3.54242 iter/s, 2.82293s/10 iters), loss = 6.86348
I0522 23:29:34.274807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86348 (* 1 = 6.86348 loss)
I0522 23:29:34.282701 35003 sgd_solver.cpp:112] Iteration 91210, lr = 0.01
I0522 23:29:37.878671 35003 solver.cpp:239] Iteration 91220 (2.77492 iter/s, 3.60371s/10 iters), loss = 6.32138
I0522 23:29:37.878753 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32138 (* 1 = 6.32138 loss)
I0522 23:29:37.919981 35003 sgd_solver.cpp:112] Iteration 91220, lr = 0.01
I0522 23:29:41.369979 35003 solver.cpp:239] Iteration 91230 (2.86444 iter/s, 3.49109s/10 iters), loss = 7.89099
I0522 23:29:41.370016 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89099 (* 1 = 7.89099 loss)
I0522 23:29:41.383584 35003 sgd_solver.cpp:112] Iteration 91230, lr = 0.01
I0522 23:29:44.193282 35003 solver.cpp:239] Iteration 91240 (3.54215 iter/s, 2.82314s/10 iters), loss = 8.22751
I0522 23:29:44.193465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.22751 (* 1 = 8.22751 loss)
I0522 23:29:44.206792 35003 sgd_solver.cpp:112] Iteration 91240, lr = 0.01
I0522 23:29:46.412256 35003 solver.cpp:239] Iteration 91250 (4.50715 iter/s, 2.2187s/10 iters), loss = 6.44019
I0522 23:29:46.412295 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44019 (* 1 = 6.44019 loss)
I0522 23:29:46.430650 35003 sgd_solver.cpp:112] Iteration 91250, lr = 0.01
I0522 23:29:49.288554 35003 solver.cpp:239] Iteration 91260 (3.47689 iter/s, 2.87614s/10 iters), loss = 7.36066
I0522 23:29:49.288602 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36066 (* 1 = 7.36066 loss)
I0522 23:29:50.029269 35003 sgd_solver.cpp:112] Iteration 91260, lr = 0.01
I0522 23:29:52.158419 35003 solver.cpp:239] Iteration 91270 (3.48469 iter/s, 2.8697s/10 iters), loss = 8.01697
I0522 23:29:52.158466 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01697 (* 1 = 8.01697 loss)
I0522 23:29:52.868901 35003 sgd_solver.cpp:112] Iteration 91270, lr = 0.01
I0522 23:29:56.061327 35003 solver.cpp:239] Iteration 91280 (2.56235 iter/s, 3.90266s/10 iters), loss = 6.37368
I0522 23:29:56.061403 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37368 (* 1 = 6.37368 loss)
I0522 23:29:56.073653 35003 sgd_solver.cpp:112] Iteration 91280, lr = 0.01
I0522 23:29:59.599866 35003 solver.cpp:239] Iteration 91290 (2.82621 iter/s, 3.53831s/10 iters), loss = 6.66184
I0522 23:29:59.599931 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66184 (* 1 = 6.66184 loss)
I0522 23:29:59.672431 35003 sgd_solver.cpp:112] Iteration 91290, lr = 0.01
I0522 23:30:02.587121 35003 solver.cpp:239] Iteration 91300 (3.34777 iter/s, 2.98706s/10 iters), loss = 7.65241
I0522 23:30:02.587173 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65241 (* 1 = 7.65241 loss)
I0522 23:30:02.644712 35003 sgd_solver.cpp:112] Iteration 91300, lr = 0.01
I0522 23:30:05.456188 35003 solver.cpp:239] Iteration 91310 (3.48566 iter/s, 2.86889s/10 iters), loss = 6.64254
I0522 23:30:05.456234 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64254 (* 1 = 6.64254 loss)
I0522 23:30:05.463223 35003 sgd_solver.cpp:112] Iteration 91310, lr = 0.01
I0522 23:30:08.028007 35003 solver.cpp:239] Iteration 91320 (3.88854 iter/s, 2.57166s/10 iters), loss = 7.32222
I0522 23:30:08.028051 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32222 (* 1 = 7.32222 loss)
I0522 23:30:08.652245 35003 sgd_solver.cpp:112] Iteration 91320, lr = 0.01
I0522 23:30:13.213835 35003 solver.cpp:239] Iteration 91330 (1.92843 iter/s, 5.18555s/10 iters), loss = 7.69772
I0522 23:30:13.213882 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69772 (* 1 = 7.69772 loss)
I0522 23:30:13.235564 35003 sgd_solver.cpp:112] Iteration 91330, lr = 0.01
I0522 23:30:17.239370 35003 solver.cpp:239] Iteration 91340 (2.48428 iter/s, 4.02532s/10 iters), loss = 7.63062
I0522 23:30:17.239702 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63062 (* 1 = 7.63062 loss)
I0522 23:30:17.245384 35003 sgd_solver.cpp:112] Iteration 91340, lr = 0.01
I0522 23:30:20.074847 35003 solver.cpp:239] Iteration 91350 (3.52737 iter/s, 2.83498s/10 iters), loss = 7.36368
I0522 23:30:20.074887 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36368 (* 1 = 7.36368 loss)
I0522 23:30:20.080335 35003 sgd_solver.cpp:112] Iteration 91350, lr = 0.01
I0522 23:30:22.914605 35003 solver.cpp:239] Iteration 91360 (3.52164 iter/s, 2.83959s/10 iters), loss = 7.17791
I0522 23:30:22.914659 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17791 (* 1 = 7.17791 loss)
I0522 23:30:23.649643 35003 sgd_solver.cpp:112] Iteration 91360, lr = 0.01
I0522 23:30:27.803658 35003 solver.cpp:239] Iteration 91370 (2.04549 iter/s, 4.8888s/10 iters), loss = 6.43044
I0522 23:30:27.803702 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43044 (* 1 = 6.43044 loss)
I0522 23:30:27.816921 35003 sgd_solver.cpp:112] Iteration 91370, lr = 0.01
I0522 23:30:30.583523 35003 solver.cpp:239] Iteration 91380 (3.59751 iter/s, 2.7797s/10 iters), loss = 7.64814
I0522 23:30:30.583572 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64814 (* 1 = 7.64814 loss)
I0522 23:30:30.598979 35003 sgd_solver.cpp:112] Iteration 91380, lr = 0.01
I0522 23:30:36.348696 35003 solver.cpp:239] Iteration 91390 (1.73464 iter/s, 5.76489s/10 iters), loss = 7.23261
I0522 23:30:36.348742 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23261 (* 1 = 7.23261 loss)
I0522 23:30:36.355085 35003 sgd_solver.cpp:112] Iteration 91390, lr = 0.01
I0522 23:30:40.027068 35003 solver.cpp:239] Iteration 91400 (2.71875 iter/s, 3.67816s/10 iters), loss = 7.34813
I0522 23:30:40.027112 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34813 (* 1 = 7.34813 loss)
I0522 23:30:40.034407 35003 sgd_solver.cpp:112] Iteration 91400, lr = 0.01
I0522 23:30:45.115468 35003 solver.cpp:239] Iteration 91410 (1.96535 iter/s, 5.08815s/10 iters), loss = 6.32863
I0522 23:30:45.115516 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32863 (* 1 = 6.32863 loss)
I0522 23:30:45.165082 35003 sgd_solver.cpp:112] Iteration 91410, lr = 0.01
I0522 23:30:48.667235 35003 solver.cpp:239] Iteration 91420 (2.81566 iter/s, 3.55156s/10 iters), loss = 7.88397
I0522 23:30:48.667496 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88397 (* 1 = 7.88397 loss)
I0522 23:30:48.673105 35003 sgd_solver.cpp:112] Iteration 91420, lr = 0.01
I0522 23:30:51.076097 35003 solver.cpp:239] Iteration 91430 (4.15191 iter/s, 2.40853s/10 iters), loss = 7.77592
I0522 23:30:51.076138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77592 (* 1 = 7.77592 loss)
I0522 23:30:51.089534 35003 sgd_solver.cpp:112] Iteration 91430, lr = 0.01
I0522 23:30:54.917011 35003 solver.cpp:239] Iteration 91440 (2.60368 iter/s, 3.84072s/10 iters), loss = 6.6435
I0522 23:30:54.917054 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6435 (* 1 = 6.6435 loss)
I0522 23:30:54.923193 35003 sgd_solver.cpp:112] Iteration 91440, lr = 0.01
I0522 23:30:58.346627 35003 solver.cpp:239] Iteration 91450 (2.91593 iter/s, 3.42944s/10 iters), loss = 7.48877
I0522 23:30:58.346665 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48877 (* 1 = 7.48877 loss)
I0522 23:30:58.356695 35003 sgd_solver.cpp:112] Iteration 91450, lr = 0.01
I0522 23:31:01.205127 35003 solver.cpp:239] Iteration 91460 (3.49854 iter/s, 2.85833s/10 iters), loss = 7.29038
I0522 23:31:01.205176 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29038 (* 1 = 7.29038 loss)
I0522 23:31:01.946224 35003 sgd_solver.cpp:112] Iteration 91460, lr = 0.01
I0522 23:31:05.547236 35003 solver.cpp:239] Iteration 91470 (2.30315 iter/s, 4.34188s/10 iters), loss = 7.0697
I0522 23:31:05.547273 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0697 (* 1 = 7.0697 loss)
I0522 23:31:06.256177 35003 sgd_solver.cpp:112] Iteration 91470, lr = 0.01
I0522 23:31:10.015107 35003 solver.cpp:239] Iteration 91480 (2.23831 iter/s, 4.46765s/10 iters), loss = 7.83566
I0522 23:31:10.015158 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83566 (* 1 = 7.83566 loss)
I0522 23:31:10.025629 35003 sgd_solver.cpp:112] Iteration 91480, lr = 0.01
I0522 23:31:13.592651 35003 solver.cpp:239] Iteration 91490 (2.79537 iter/s, 3.57734s/10 iters), loss = 6.17118
I0522 23:31:13.592705 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17118 (* 1 = 6.17118 loss)
I0522 23:31:14.301774 35003 sgd_solver.cpp:112] Iteration 91490, lr = 0.01
I0522 23:31:17.861383 35003 solver.cpp:239] Iteration 91500 (2.34274 iter/s, 4.2685s/10 iters), loss = 7.69876
I0522 23:31:17.861428 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69876 (* 1 = 7.69876 loss)
I0522 23:31:17.874123 35003 sgd_solver.cpp:112] Iteration 91500, lr = 0.01
I0522 23:31:21.300081 35003 solver.cpp:239] Iteration 91510 (2.90824 iter/s, 3.4385s/10 iters), loss = 6.65133
I0522 23:31:21.300354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65133 (* 1 = 6.65133 loss)
I0522 23:31:21.305968 35003 sgd_solver.cpp:112] Iteration 91510, lr = 0.01
I0522 23:31:24.975891 35003 solver.cpp:239] Iteration 91520 (2.72078 iter/s, 3.67542s/10 iters), loss = 7.82652
I0522 23:31:24.975951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82652 (* 1 = 7.82652 loss)
I0522 23:31:25.028439 35003 sgd_solver.cpp:112] Iteration 91520, lr = 0.01
I0522 23:31:27.865149 35003 solver.cpp:239] Iteration 91530 (3.46131 iter/s, 2.88908s/10 iters), loss = 7.00951
I0522 23:31:27.865187 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00951 (* 1 = 7.00951 loss)
I0522 23:31:27.877310 35003 sgd_solver.cpp:112] Iteration 91530, lr = 0.01
I0522 23:31:32.177909 35003 solver.cpp:239] Iteration 91540 (2.31883 iter/s, 4.31252s/10 iters), loss = 7.69805
I0522 23:31:32.177987 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69805 (* 1 = 7.69805 loss)
I0522 23:31:32.886158 35003 sgd_solver.cpp:112] Iteration 91540, lr = 0.01
I0522 23:31:37.361217 35003 solver.cpp:239] Iteration 91550 (1.92938 iter/s, 5.18302s/10 iters), loss = 8.65352
I0522 23:31:37.361272 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.65352 (* 1 = 8.65352 loss)
I0522 23:31:37.374708 35003 sgd_solver.cpp:112] Iteration 91550, lr = 0.01
I0522 23:31:40.208638 35003 solver.cpp:239] Iteration 91560 (3.51217 iter/s, 2.84724s/10 iters), loss = 7.3454
I0522 23:31:40.208676 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3454 (* 1 = 7.3454 loss)
I0522 23:31:40.221606 35003 sgd_solver.cpp:112] Iteration 91560, lr = 0.01
I0522 23:31:43.671577 35003 solver.cpp:239] Iteration 91570 (2.88788 iter/s, 3.46274s/10 iters), loss = 7.11479
I0522 23:31:43.671648 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11479 (* 1 = 7.11479 loss)
I0522 23:31:44.399682 35003 sgd_solver.cpp:112] Iteration 91570, lr = 0.01
I0522 23:31:49.075551 35003 solver.cpp:239] Iteration 91580 (1.85059 iter/s, 5.40368s/10 iters), loss = 7.50547
I0522 23:31:49.075600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50547 (* 1 = 7.50547 loss)
I0522 23:31:49.079236 35003 sgd_solver.cpp:112] Iteration 91580, lr = 0.01
I0522 23:31:53.515378 35003 solver.cpp:239] Iteration 91590 (2.25246 iter/s, 4.43959s/10 iters), loss = 8.41323
I0522 23:31:53.515545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.41323 (* 1 = 8.41323 loss)
I0522 23:31:53.528611 35003 sgd_solver.cpp:112] Iteration 91590, lr = 0.01
I0522 23:31:55.589450 35003 solver.cpp:239] Iteration 91600 (4.82206 iter/s, 2.0738s/10 iters), loss = 6.67488
I0522 23:31:55.589505 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67488 (* 1 = 6.67488 loss)
I0522 23:31:55.604918 35003 sgd_solver.cpp:112] Iteration 91600, lr = 0.01
I0522 23:31:59.275955 35003 solver.cpp:239] Iteration 91610 (2.71275 iter/s, 3.6863s/10 iters), loss = 8.0329
I0522 23:31:59.276000 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0329 (* 1 = 8.0329 loss)
I0522 23:31:59.297253 35003 sgd_solver.cpp:112] Iteration 91610, lr = 0.01
I0522 23:32:01.403548 35003 solver.cpp:239] Iteration 91620 (4.70045 iter/s, 2.12746s/10 iters), loss = 6.2853
I0522 23:32:01.403592 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2853 (* 1 = 6.2853 loss)
I0522 23:32:02.144093 35003 sgd_solver.cpp:112] Iteration 91620, lr = 0.01
I0522 23:32:06.587373 35003 solver.cpp:239] Iteration 91630 (1.92917 iter/s, 5.18357s/10 iters), loss = 6.83888
I0522 23:32:06.587414 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83888 (* 1 = 6.83888 loss)
I0522 23:32:06.600945 35003 sgd_solver.cpp:112] Iteration 91630, lr = 0.01
I0522 23:32:10.359194 35003 solver.cpp:239] Iteration 91640 (2.65138 iter/s, 3.77162s/10 iters), loss = 7.32088
I0522 23:32:10.359241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32088 (* 1 = 7.32088 loss)
I0522 23:32:11.004984 35003 sgd_solver.cpp:112] Iteration 91640, lr = 0.01
I0522 23:32:13.750275 35003 solver.cpp:239] Iteration 91650 (2.94913 iter/s, 3.39083s/10 iters), loss = 6.59366
I0522 23:32:13.750317 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59366 (* 1 = 6.59366 loss)
I0522 23:32:13.753883 35003 sgd_solver.cpp:112] Iteration 91650, lr = 0.01
I0522 23:32:17.911221 35003 solver.cpp:239] Iteration 91660 (2.40345 iter/s, 4.16069s/10 iters), loss = 6.77229
I0522 23:32:17.911259 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77229 (* 1 = 6.77229 loss)
I0522 23:32:17.924475 35003 sgd_solver.cpp:112] Iteration 91660, lr = 0.01
I0522 23:32:21.367019 35003 solver.cpp:239] Iteration 91670 (2.89384 iter/s, 3.45562s/10 iters), loss = 7.84498
I0522 23:32:21.367058 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84498 (* 1 = 7.84498 loss)
I0522 23:32:21.373023 35003 sgd_solver.cpp:112] Iteration 91670, lr = 0.01
I0522 23:32:24.966069 35003 solver.cpp:239] Iteration 91680 (2.77866 iter/s, 3.59886s/10 iters), loss = 7.36595
I0522 23:32:24.966334 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36595 (* 1 = 7.36595 loss)
I0522 23:32:24.979017 35003 sgd_solver.cpp:112] Iteration 91680, lr = 0.01
I0522 23:32:27.832259 35003 solver.cpp:239] Iteration 91690 (3.48939 iter/s, 2.86583s/10 iters), loss = 7.71871
I0522 23:32:27.832301 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71871 (* 1 = 7.71871 loss)
I0522 23:32:28.387856 35003 sgd_solver.cpp:112] Iteration 91690, lr = 0.01
I0522 23:32:31.677798 35003 solver.cpp:239] Iteration 91700 (2.60055 iter/s, 3.84534s/10 iters), loss = 8.17594
I0522 23:32:31.677835 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17594 (* 1 = 8.17594 loss)
I0522 23:32:31.690627 35003 sgd_solver.cpp:112] Iteration 91700, lr = 0.01
I0522 23:32:36.625700 35003 solver.cpp:239] Iteration 91710 (2.02116 iter/s, 4.94766s/10 iters), loss = 6.69591
I0522 23:32:36.625746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69591 (* 1 = 6.69591 loss)
I0522 23:32:36.632251 35003 sgd_solver.cpp:112] Iteration 91710, lr = 0.01
I0522 23:32:39.421634 35003 solver.cpp:239] Iteration 91720 (3.57684 iter/s, 2.79577s/10 iters), loss = 8.81365
I0522 23:32:39.421674 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.81365 (* 1 = 8.81365 loss)
I0522 23:32:39.434000 35003 sgd_solver.cpp:112] Iteration 91720, lr = 0.01
I0522 23:32:41.605887 35003 solver.cpp:239] Iteration 91730 (4.57852 iter/s, 2.18411s/10 iters), loss = 7.38721
I0522 23:32:41.605937 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38721 (* 1 = 7.38721 loss)
I0522 23:32:42.334193 35003 sgd_solver.cpp:112] Iteration 91730, lr = 0.01
I0522 23:32:44.772740 35003 solver.cpp:239] Iteration 91740 (3.15789 iter/s, 3.16667s/10 iters), loss = 7.09636
I0522 23:32:44.772779 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09636 (* 1 = 7.09636 loss)
I0522 23:32:44.804538 35003 sgd_solver.cpp:112] Iteration 91740, lr = 0.01
I0522 23:32:49.584666 35003 solver.cpp:239] Iteration 91750 (2.07827 iter/s, 4.81169s/10 iters), loss = 5.20649
I0522 23:32:49.584704 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.20649 (* 1 = 5.20649 loss)
I0522 23:32:49.597904 35003 sgd_solver.cpp:112] Iteration 91750, lr = 0.01
I0522 23:32:53.178171 35003 solver.cpp:239] Iteration 91760 (2.78294 iter/s, 3.59332s/10 iters), loss = 6.51758
I0522 23:32:53.178216 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51758 (* 1 = 6.51758 loss)
I0522 23:32:53.191565 35003 sgd_solver.cpp:112] Iteration 91760, lr = 0.01
I0522 23:32:56.014197 35003 solver.cpp:239] Iteration 91770 (3.52627 iter/s, 2.83586s/10 iters), loss = 6.58002
I0522 23:32:56.014500 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58002 (* 1 = 6.58002 loss)
I0522 23:32:56.722558 35003 sgd_solver.cpp:112] Iteration 91770, lr = 0.01
I0522 23:33:00.813428 35003 solver.cpp:239] Iteration 91780 (2.08387 iter/s, 4.79876s/10 iters), loss = 7.54226
I0522 23:33:00.813485 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54226 (* 1 = 7.54226 loss)
I0522 23:33:00.829748 35003 sgd_solver.cpp:112] Iteration 91780, lr = 0.01
I0522 23:33:04.398068 35003 solver.cpp:239] Iteration 91790 (2.78984 iter/s, 3.58444s/10 iters), loss = 7.66185
I0522 23:33:04.398103 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66185 (* 1 = 7.66185 loss)
I0522 23:33:04.410904 35003 sgd_solver.cpp:112] Iteration 91790, lr = 0.01
I0522 23:33:08.626791 35003 solver.cpp:239] Iteration 91800 (2.3649 iter/s, 4.2285s/10 iters), loss = 7.4011
I0522 23:33:08.626883 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4011 (* 1 = 7.4011 loss)
I0522 23:33:08.635480 35003 sgd_solver.cpp:112] Iteration 91800, lr = 0.01
I0522 23:33:11.489450 35003 solver.cpp:239] Iteration 91810 (3.49352 iter/s, 2.86244s/10 iters), loss = 7.73239
I0522 23:33:11.489500 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73239 (* 1 = 7.73239 loss)
I0522 23:33:11.503587 35003 sgd_solver.cpp:112] Iteration 91810, lr = 0.01
I0522 23:33:14.325423 35003 solver.cpp:239] Iteration 91820 (3.52638 iter/s, 2.83577s/10 iters), loss = 7.00216
I0522 23:33:14.325477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00216 (* 1 = 7.00216 loss)
I0522 23:33:15.021409 35003 sgd_solver.cpp:112] Iteration 91820, lr = 0.01
I0522 23:33:18.096953 35003 solver.cpp:239] Iteration 91830 (2.65159 iter/s, 3.77132s/10 iters), loss = 5.77907
I0522 23:33:18.097012 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77907 (* 1 = 5.77907 loss)
I0522 23:33:18.109704 35003 sgd_solver.cpp:112] Iteration 91830, lr = 0.01
I0522 23:33:21.772696 35003 solver.cpp:239] Iteration 91840 (2.72069 iter/s, 3.67553s/10 iters), loss = 6.26895
I0522 23:33:21.772753 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26895 (* 1 = 6.26895 loss)
I0522 23:33:21.785856 35003 sgd_solver.cpp:112] Iteration 91840, lr = 0.01
I0522 23:33:24.402508 35003 solver.cpp:239] Iteration 91850 (3.8028 iter/s, 2.62964s/10 iters), loss = 7.67863
I0522 23:33:24.402555 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67863 (* 1 = 7.67863 loss)
I0522 23:33:24.416090 35003 sgd_solver.cpp:112] Iteration 91850, lr = 0.01
I0522 23:33:27.315757 35003 solver.cpp:239] Iteration 91860 (3.4328 iter/s, 2.91307s/10 iters), loss = 8.6813
I0522 23:33:27.315954 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.6813 (* 1 = 8.6813 loss)
I0522 23:33:27.319058 35003 sgd_solver.cpp:112] Iteration 91860, lr = 0.01
I0522 23:33:30.951503 35003 solver.cpp:239] Iteration 91870 (2.75072 iter/s, 3.63542s/10 iters), loss = 7.26085
I0522 23:33:30.951566 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26085 (* 1 = 7.26085 loss)
I0522 23:33:31.666903 35003 sgd_solver.cpp:112] Iteration 91870, lr = 0.01
I0522 23:33:36.037186 35003 solver.cpp:239] Iteration 91880 (1.96642 iter/s, 5.08539s/10 iters), loss = 7.5676
I0522 23:33:36.037235 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5676 (* 1 = 7.5676 loss)
I0522 23:33:36.752584 35003 sgd_solver.cpp:112] Iteration 91880, lr = 0.01
I0522 23:33:40.230870 35003 solver.cpp:239] Iteration 91890 (2.38467 iter/s, 4.19345s/10 iters), loss = 7.59281
I0522 23:33:40.230943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59281 (* 1 = 7.59281 loss)
I0522 23:33:40.240502 35003 sgd_solver.cpp:112] Iteration 91890, lr = 0.01
I0522 23:33:44.859357 35003 solver.cpp:239] Iteration 91900 (2.16066 iter/s, 4.62822s/10 iters), loss = 7.32372
I0522 23:33:44.859408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32372 (* 1 = 7.32372 loss)
I0522 23:33:45.570832 35003 sgd_solver.cpp:112] Iteration 91900, lr = 0.01
I0522 23:33:47.030175 35003 solver.cpp:239] Iteration 91910 (4.60687 iter/s, 2.17067s/10 iters), loss = 7.33427
I0522 23:33:47.030216 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33427 (* 1 = 7.33427 loss)
I0522 23:33:47.047821 35003 sgd_solver.cpp:112] Iteration 91910, lr = 0.01
I0522 23:33:50.786149 35003 solver.cpp:239] Iteration 91920 (2.66258 iter/s, 3.75576s/10 iters), loss = 7.72572
I0522 23:33:50.786206 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72572 (* 1 = 7.72572 loss)
I0522 23:33:51.419075 35003 sgd_solver.cpp:112] Iteration 91920, lr = 0.01
I0522 23:33:55.688655 35003 solver.cpp:239] Iteration 91930 (2.03988 iter/s, 4.90225s/10 iters), loss = 7.63995
I0522 23:33:55.688691 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63995 (* 1 = 7.63995 loss)
I0522 23:33:55.698276 35003 sgd_solver.cpp:112] Iteration 91930, lr = 0.01
I0522 23:34:00.830101 35003 solver.cpp:239] Iteration 91940 (1.94507 iter/s, 5.14119s/10 iters), loss = 7.66678
I0522 23:34:00.830410 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66678 (* 1 = 7.66678 loss)
I0522 23:34:01.514291 35003 sgd_solver.cpp:112] Iteration 91940, lr = 0.01
I0522 23:34:03.526103 35003 solver.cpp:239] Iteration 91950 (3.70974 iter/s, 2.6956s/10 iters), loss = 7.2496
I0522 23:34:03.526161 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2496 (* 1 = 7.2496 loss)
I0522 23:34:03.537102 35003 sgd_solver.cpp:112] Iteration 91950, lr = 0.01
I0522 23:34:07.132308 35003 solver.cpp:239] Iteration 91960 (2.77316 iter/s, 3.606s/10 iters), loss = 7.01989
I0522 23:34:07.132352 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01989 (* 1 = 7.01989 loss)
I0522 23:34:07.139161 35003 sgd_solver.cpp:112] Iteration 91960, lr = 0.01
I0522 23:34:11.126166 35003 solver.cpp:239] Iteration 91970 (2.50398 iter/s, 3.99365s/10 iters), loss = 7.44297
I0522 23:34:11.126216 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44297 (* 1 = 7.44297 loss)
I0522 23:34:11.244475 35003 sgd_solver.cpp:112] Iteration 91970, lr = 0.01
I0522 23:34:13.895615 35003 solver.cpp:239] Iteration 91980 (3.61104 iter/s, 2.76928s/10 iters), loss = 7.509
I0522 23:34:13.895653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.509 (* 1 = 7.509 loss)
I0522 23:34:13.914006 35003 sgd_solver.cpp:112] Iteration 91980, lr = 0.01
I0522 23:34:17.147804 35003 solver.cpp:239] Iteration 91990 (3.07503 iter/s, 3.252s/10 iters), loss = 6.16825
I0522 23:34:17.147902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16825 (* 1 = 6.16825 loss)
I0522 23:34:17.160789 35003 sgd_solver.cpp:112] Iteration 91990, lr = 0.01
I0522 23:34:20.534529 35003 solver.cpp:239] Iteration 92000 (2.95291 iter/s, 3.38649s/10 iters), loss = 6.90913
I0522 23:34:20.534576 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90913 (* 1 = 6.90913 loss)
I0522 23:34:21.275697 35003 sgd_solver.cpp:112] Iteration 92000, lr = 0.01
I0522 23:34:24.139966 35003 solver.cpp:239] Iteration 92010 (2.77374 iter/s, 3.60524s/10 iters), loss = 7.20855
I0522 23:34:24.140019 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20855 (* 1 = 7.20855 loss)
I0522 23:34:24.148212 35003 sgd_solver.cpp:112] Iteration 92010, lr = 0.01
I0522 23:34:26.967926 35003 solver.cpp:239] Iteration 92020 (3.53633 iter/s, 2.82779s/10 iters), loss = 6.80674
I0522 23:34:26.967972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80674 (* 1 = 6.80674 loss)
I0522 23:34:26.984172 35003 sgd_solver.cpp:112] Iteration 92020, lr = 0.01
I0522 23:34:29.442961 35003 solver.cpp:239] Iteration 92030 (4.0406 iter/s, 2.47488s/10 iters), loss = 7.12443
I0522 23:34:29.443011 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12443 (* 1 = 7.12443 loss)
I0522 23:34:30.180517 35003 sgd_solver.cpp:112] Iteration 92030, lr = 0.01
I0522 23:34:31.521255 35003 solver.cpp:239] Iteration 92040 (4.81196 iter/s, 2.07815s/10 iters), loss = 7.28512
I0522 23:34:31.521553 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28512 (* 1 = 7.28512 loss)
I0522 23:34:31.534332 35003 sgd_solver.cpp:112] Iteration 92040, lr = 0.01
I0522 23:34:33.506583 35003 solver.cpp:239] Iteration 92050 (5.03786 iter/s, 1.98497s/10 iters), loss = 6.86608
I0522 23:34:33.506635 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86608 (* 1 = 6.86608 loss)
I0522 23:34:34.131086 35003 sgd_solver.cpp:112] Iteration 92050, lr = 0.01
I0522 23:34:37.217286 35003 solver.cpp:239] Iteration 92060 (2.69505 iter/s, 3.7105s/10 iters), loss = 6.7006
I0522 23:34:37.217326 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7006 (* 1 = 6.7006 loss)
I0522 23:34:37.228574 35003 sgd_solver.cpp:112] Iteration 92060, lr = 0.01
I0522 23:34:40.396771 35003 solver.cpp:239] Iteration 92070 (3.14534 iter/s, 3.1793s/10 iters), loss = 7.49495
I0522 23:34:40.396821 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49495 (* 1 = 7.49495 loss)
I0522 23:34:41.112177 35003 sgd_solver.cpp:112] Iteration 92070, lr = 0.01
I0522 23:34:43.930516 35003 solver.cpp:239] Iteration 92080 (2.83003 iter/s, 3.53353s/10 iters), loss = 7.30454
I0522 23:34:43.930557 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30454 (* 1 = 7.30454 loss)
I0522 23:34:43.943985 35003 sgd_solver.cpp:112] Iteration 92080, lr = 0.01
I0522 23:34:47.620496 35003 solver.cpp:239] Iteration 92090 (2.7102 iter/s, 3.68977s/10 iters), loss = 7.32046
I0522 23:34:47.620554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32046 (* 1 = 7.32046 loss)
I0522 23:34:47.629469 35003 sgd_solver.cpp:112] Iteration 92090, lr = 0.01
I0522 23:34:52.610605 35003 solver.cpp:239] Iteration 92100 (2.00407 iter/s, 4.98984s/10 iters), loss = 6.39198
I0522 23:34:52.610648 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39198 (* 1 = 6.39198 loss)
I0522 23:34:52.618101 35003 sgd_solver.cpp:112] Iteration 92100, lr = 0.01
I0522 23:34:57.138212 35003 solver.cpp:239] Iteration 92110 (2.20878 iter/s, 4.52738s/10 iters), loss = 7.27265
I0522 23:34:57.138249 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27265 (* 1 = 7.27265 loss)
I0522 23:34:57.150744 35003 sgd_solver.cpp:112] Iteration 92110, lr = 0.01
I0522 23:35:00.025295 35003 solver.cpp:239] Iteration 92120 (3.4639 iter/s, 2.88692s/10 iters), loss = 6.96978
I0522 23:35:00.025336 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96978 (* 1 = 6.96978 loss)
I0522 23:35:00.030426 35003 sgd_solver.cpp:112] Iteration 92120, lr = 0.01
I0522 23:35:02.755816 35003 solver.cpp:239] Iteration 92130 (3.66252 iter/s, 2.73036s/10 iters), loss = 6.87973
I0522 23:35:02.755966 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87973 (* 1 = 6.87973 loss)
I0522 23:35:03.496783 35003 sgd_solver.cpp:112] Iteration 92130, lr = 0.01
I0522 23:35:06.810953 35003 solver.cpp:239] Iteration 92140 (2.4662 iter/s, 4.05482s/10 iters), loss = 7.32726
I0522 23:35:06.810998 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32726 (* 1 = 7.32726 loss)
I0522 23:35:06.817759 35003 sgd_solver.cpp:112] Iteration 92140, lr = 0.01
I0522 23:35:10.587441 35003 solver.cpp:239] Iteration 92150 (2.64811 iter/s, 3.77628s/10 iters), loss = 6.99978
I0522 23:35:10.587486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99978 (* 1 = 6.99978 loss)
I0522 23:35:10.595952 35003 sgd_solver.cpp:112] Iteration 92150, lr = 0.01
I0522 23:35:15.452641 35003 solver.cpp:239] Iteration 92160 (2.05552 iter/s, 4.86495s/10 iters), loss = 7.12779
I0522 23:35:15.452693 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12779 (* 1 = 7.12779 loss)
I0522 23:35:16.160818 35003 sgd_solver.cpp:112] Iteration 92160, lr = 0.01
I0522 23:35:18.223860 35003 solver.cpp:239] Iteration 92170 (3.60875 iter/s, 2.77104s/10 iters), loss = 8.0044
I0522 23:35:18.223907 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0044 (* 1 = 8.0044 loss)
I0522 23:35:18.945222 35003 sgd_solver.cpp:112] Iteration 92170, lr = 0.01
I0522 23:35:21.823040 35003 solver.cpp:239] Iteration 92180 (2.77857 iter/s, 3.59898s/10 iters), loss = 7.29689
I0522 23:35:21.823089 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29689 (* 1 = 7.29689 loss)
I0522 23:35:21.825935 35003 sgd_solver.cpp:112] Iteration 92180, lr = 0.01
I0522 23:35:24.409659 35003 solver.cpp:239] Iteration 92190 (3.8663 iter/s, 2.58645s/10 iters), loss = 8.59813
I0522 23:35:24.409698 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.59813 (* 1 = 8.59813 loss)
I0522 23:35:24.928448 35003 sgd_solver.cpp:112] Iteration 92190, lr = 0.01
I0522 23:35:26.278746 35003 solver.cpp:239] Iteration 92200 (5.35056 iter/s, 1.86896s/10 iters), loss = 6.65853
I0522 23:35:26.278789 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65853 (* 1 = 6.65853 loss)
I0522 23:35:27.009176 35003 sgd_solver.cpp:112] Iteration 92200, lr = 0.01
I0522 23:35:29.665940 35003 solver.cpp:239] Iteration 92210 (2.95246 iter/s, 3.38701s/10 iters), loss = 7.25347
I0522 23:35:29.665985 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25347 (* 1 = 7.25347 loss)
I0522 23:35:30.400797 35003 sgd_solver.cpp:112] Iteration 92210, lr = 0.01
I0522 23:35:33.927620 35003 solver.cpp:239] Iteration 92220 (2.34662 iter/s, 4.26145s/10 iters), loss = 7.2853
I0522 23:35:33.927884 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2853 (* 1 = 7.2853 loss)
I0522 23:35:33.972782 35003 sgd_solver.cpp:112] Iteration 92220, lr = 0.01
I0522 23:35:37.705641 35003 solver.cpp:239] Iteration 92230 (2.64718 iter/s, 3.7776s/10 iters), loss = 7.4006
I0522 23:35:37.705695 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4006 (* 1 = 7.4006 loss)
I0522 23:35:38.397423 35003 sgd_solver.cpp:112] Iteration 92230, lr = 0.01
I0522 23:35:41.985347 35003 solver.cpp:239] Iteration 92240 (2.33674 iter/s, 4.27947s/10 iters), loss = 6.35917
I0522 23:35:41.985394 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35917 (* 1 = 6.35917 loss)
I0522 23:35:41.998432 35003 sgd_solver.cpp:112] Iteration 92240, lr = 0.01
I0522 23:35:44.763046 35003 solver.cpp:239] Iteration 92250 (3.60034 iter/s, 2.77752s/10 iters), loss = 6.39566
I0522 23:35:44.763097 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39566 (* 1 = 6.39566 loss)
I0522 23:35:45.484352 35003 sgd_solver.cpp:112] Iteration 92250, lr = 0.01
I0522 23:35:49.695173 35003 solver.cpp:239] Iteration 92260 (2.02763 iter/s, 4.93187s/10 iters), loss = 6.60078
I0522 23:35:49.695219 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60078 (* 1 = 6.60078 loss)
I0522 23:35:49.702385 35003 sgd_solver.cpp:112] Iteration 92260, lr = 0.01
I0522 23:35:53.441223 35003 solver.cpp:239] Iteration 92270 (2.66962 iter/s, 3.74585s/10 iters), loss = 7.92287
I0522 23:35:53.441270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.92287 (* 1 = 7.92287 loss)
I0522 23:35:54.149773 35003 sgd_solver.cpp:112] Iteration 92270, lr = 0.01
I0522 23:35:58.492897 35003 solver.cpp:239] Iteration 92280 (1.97964 iter/s, 5.05143s/10 iters), loss = 7.43625
I0522 23:35:58.492938 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43625 (* 1 = 7.43625 loss)
I0522 23:35:58.505911 35003 sgd_solver.cpp:112] Iteration 92280, lr = 0.01
I0522 23:36:02.872526 35003 solver.cpp:239] Iteration 92290 (2.28342 iter/s, 4.3794s/10 iters), loss = 7.4039
I0522 23:36:02.872567 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4039 (* 1 = 7.4039 loss)
I0522 23:36:02.881048 35003 sgd_solver.cpp:112] Iteration 92290, lr = 0.01
I0522 23:36:07.703770 35003 solver.cpp:239] Iteration 92300 (2.06996 iter/s, 4.83101s/10 iters), loss = 6.96957
I0522 23:36:07.703938 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96957 (* 1 = 6.96957 loss)
I0522 23:36:07.710734 35003 sgd_solver.cpp:112] Iteration 92300, lr = 0.01
I0522 23:36:11.297813 35003 solver.cpp:239] Iteration 92310 (2.78263 iter/s, 3.59372s/10 iters), loss = 8.48265
I0522 23:36:11.297886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.48265 (* 1 = 8.48265 loss)
I0522 23:36:11.312887 35003 sgd_solver.cpp:112] Iteration 92310, lr = 0.01
I0522 23:36:14.577172 35003 solver.cpp:239] Iteration 92320 (3.04957 iter/s, 3.27915s/10 iters), loss = 6.71815
I0522 23:36:14.577215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71815 (* 1 = 6.71815 loss)
I0522 23:36:14.590572 35003 sgd_solver.cpp:112] Iteration 92320, lr = 0.01
I0522 23:36:16.967684 35003 solver.cpp:239] Iteration 92330 (4.18346 iter/s, 2.39037s/10 iters), loss = 7.43946
I0522 23:36:16.967722 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43946 (* 1 = 7.43946 loss)
I0522 23:36:16.980036 35003 sgd_solver.cpp:112] Iteration 92330, lr = 0.01
I0522 23:36:21.318404 35003 solver.cpp:239] Iteration 92340 (2.29859 iter/s, 4.35049s/10 iters), loss = 7.75464
I0522 23:36:21.318445 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75464 (* 1 = 7.75464 loss)
I0522 23:36:21.332494 35003 sgd_solver.cpp:112] Iteration 92340, lr = 0.01
I0522 23:36:23.836382 35003 solver.cpp:239] Iteration 92350 (3.97167 iter/s, 2.51783s/10 iters), loss = 6.08719
I0522 23:36:23.836419 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08719 (* 1 = 6.08719 loss)
I0522 23:36:23.849232 35003 sgd_solver.cpp:112] Iteration 92350, lr = 0.01
I0522 23:36:27.053885 35003 solver.cpp:239] Iteration 92360 (3.10817 iter/s, 3.21732s/10 iters), loss = 6.6939
I0522 23:36:27.053944 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6939 (* 1 = 6.6939 loss)
I0522 23:36:27.258478 35003 sgd_solver.cpp:112] Iteration 92360, lr = 0.01
I0522 23:36:29.193646 35003 solver.cpp:239] Iteration 92370 (4.67376 iter/s, 2.13961s/10 iters), loss = 6.57388
I0522 23:36:29.193706 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57388 (* 1 = 6.57388 loss)
I0522 23:36:29.196784 35003 sgd_solver.cpp:112] Iteration 92370, lr = 0.01
I0522 23:36:32.045245 35003 solver.cpp:239] Iteration 92380 (3.50703 iter/s, 2.85142s/10 iters), loss = 7.3946
I0522 23:36:32.045305 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3946 (* 1 = 7.3946 loss)
I0522 23:36:32.055202 35003 sgd_solver.cpp:112] Iteration 92380, lr = 0.01
I0522 23:36:34.798104 35003 solver.cpp:239] Iteration 92390 (3.63282 iter/s, 2.75268s/10 iters), loss = 7.24537
I0522 23:36:34.798151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24537 (* 1 = 7.24537 loss)
I0522 23:36:34.811866 35003 sgd_solver.cpp:112] Iteration 92390, lr = 0.01
I0522 23:36:37.779850 35003 solver.cpp:239] Iteration 92400 (3.35394 iter/s, 2.98157s/10 iters), loss = 6.86861
I0522 23:36:37.779992 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86861 (* 1 = 6.86861 loss)
I0522 23:36:38.520699 35003 sgd_solver.cpp:112] Iteration 92400, lr = 0.01
I0522 23:36:41.831715 35003 solver.cpp:239] Iteration 92410 (2.46819 iter/s, 4.05155s/10 iters), loss = 7.39811
I0522 23:36:41.831755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39811 (* 1 = 7.39811 loss)
I0522 23:36:41.850056 35003 sgd_solver.cpp:112] Iteration 92410, lr = 0.01
I0522 23:36:46.817112 35003 solver.cpp:239] Iteration 92420 (2.00596 iter/s, 4.98515s/10 iters), loss = 8.32862
I0522 23:36:46.817165 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.32862 (* 1 = 8.32862 loss)
I0522 23:36:46.839027 35003 sgd_solver.cpp:112] Iteration 92420, lr = 0.01
I0522 23:36:50.967469 35003 solver.cpp:239] Iteration 92430 (2.40956 iter/s, 4.15014s/10 iters), loss = 7.91415
I0522 23:36:50.967506 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91415 (* 1 = 7.91415 loss)
I0522 23:36:50.980881 35003 sgd_solver.cpp:112] Iteration 92430, lr = 0.01
I0522 23:36:54.670226 35003 solver.cpp:239] Iteration 92440 (2.70084 iter/s, 3.70255s/10 iters), loss = 7.98253
I0522 23:36:54.670289 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98253 (* 1 = 7.98253 loss)
I0522 23:36:54.678146 35003 sgd_solver.cpp:112] Iteration 92440, lr = 0.01
I0522 23:36:57.641834 35003 solver.cpp:239] Iteration 92450 (3.3654 iter/s, 2.97141s/10 iters), loss = 6.77987
I0522 23:36:57.641877 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77987 (* 1 = 6.77987 loss)
I0522 23:36:58.357395 35003 sgd_solver.cpp:112] Iteration 92450, lr = 0.01
I0522 23:37:01.800921 35003 solver.cpp:239] Iteration 92460 (2.4045 iter/s, 4.15886s/10 iters), loss = 6.62089
I0522 23:37:01.800976 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62089 (* 1 = 6.62089 loss)
I0522 23:37:02.496883 35003 sgd_solver.cpp:112] Iteration 92460, lr = 0.01
I0522 23:37:06.878870 35003 solver.cpp:239] Iteration 92470 (1.9694 iter/s, 5.07769s/10 iters), loss = 7.26147
I0522 23:37:06.878933 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26147 (* 1 = 7.26147 loss)
I0522 23:37:07.345413 35003 sgd_solver.cpp:112] Iteration 92470, lr = 0.01
I0522 23:37:10.572703 35003 solver.cpp:239] Iteration 92480 (2.70737 iter/s, 3.69362s/10 iters), loss = 7.02297
I0522 23:37:10.572960 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02297 (* 1 = 7.02297 loss)
I0522 23:37:10.585803 35003 sgd_solver.cpp:112] Iteration 92480, lr = 0.01
I0522 23:37:13.454355 35003 solver.cpp:239] Iteration 92490 (3.47067 iter/s, 2.88129s/10 iters), loss = 6.94163
I0522 23:37:13.454416 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94163 (* 1 = 6.94163 loss)
I0522 23:37:13.455662 35003 sgd_solver.cpp:112] Iteration 92490, lr = 0.01
I0522 23:37:17.855921 35003 solver.cpp:239] Iteration 92500 (2.27205 iter/s, 4.40132s/10 iters), loss = 7.50576
I0522 23:37:17.855960 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50576 (* 1 = 7.50576 loss)
I0522 23:37:17.869699 35003 sgd_solver.cpp:112] Iteration 92500, lr = 0.01
I0522 23:37:20.370585 35003 solver.cpp:239] Iteration 92510 (3.97691 iter/s, 2.51451s/10 iters), loss = 6.80816
I0522 23:37:20.370631 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80816 (* 1 = 6.80816 loss)
I0522 23:37:20.395860 35003 sgd_solver.cpp:112] Iteration 92510, lr = 0.01
I0522 23:37:24.376153 35003 solver.cpp:239] Iteration 92520 (2.49666 iter/s, 4.00535s/10 iters), loss = 7.23271
I0522 23:37:24.376197 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23271 (* 1 = 7.23271 loss)
I0522 23:37:25.091488 35003 sgd_solver.cpp:112] Iteration 92520, lr = 0.01
I0522 23:37:27.922811 35003 solver.cpp:239] Iteration 92530 (2.81971 iter/s, 3.54647s/10 iters), loss = 7.02057
I0522 23:37:27.922852 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02057 (* 1 = 7.02057 loss)
I0522 23:37:28.638172 35003 sgd_solver.cpp:112] Iteration 92530, lr = 0.01
I0522 23:37:33.337235 35003 solver.cpp:239] Iteration 92540 (1.84701 iter/s, 5.41415s/10 iters), loss = 7.80653
I0522 23:37:33.337278 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80653 (* 1 = 7.80653 loss)
I0522 23:37:33.345165 35003 sgd_solver.cpp:112] Iteration 92540, lr = 0.01
I0522 23:37:36.424949 35003 solver.cpp:239] Iteration 92550 (3.23882 iter/s, 3.08754s/10 iters), loss = 8.64305
I0522 23:37:36.424998 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.64305 (* 1 = 8.64305 loss)
I0522 23:37:36.825295 35003 sgd_solver.cpp:112] Iteration 92550, lr = 0.01
I0522 23:37:40.947475 35003 solver.cpp:239] Iteration 92560 (2.21128 iter/s, 4.52228s/10 iters), loss = 6.20872
I0522 23:37:40.947684 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20872 (* 1 = 6.20872 loss)
I0522 23:37:40.957551 35003 sgd_solver.cpp:112] Iteration 92560, lr = 0.01
I0522 23:37:44.415951 35003 solver.cpp:239] Iteration 92570 (2.8834 iter/s, 3.46812s/10 iters), loss = 7.30733
I0522 23:37:44.415994 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30733 (* 1 = 7.30733 loss)
I0522 23:37:44.428570 35003 sgd_solver.cpp:112] Iteration 92570, lr = 0.01
I0522 23:37:48.009441 35003 solver.cpp:239] Iteration 92580 (2.78296 iter/s, 3.5933s/10 iters), loss = 6.65169
I0522 23:37:48.009486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65169 (* 1 = 6.65169 loss)
I0522 23:37:48.731387 35003 sgd_solver.cpp:112] Iteration 92580, lr = 0.01
I0522 23:37:52.562563 35003 solver.cpp:239] Iteration 92590 (2.19641 iter/s, 4.55289s/10 iters), loss = 8.08946
I0522 23:37:52.562615 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08946 (* 1 = 8.08946 loss)
I0522 23:37:53.246171 35003 sgd_solver.cpp:112] Iteration 92590, lr = 0.01
I0522 23:37:56.026172 35003 solver.cpp:239] Iteration 92600 (2.88733 iter/s, 3.46341s/10 iters), loss = 7.94415
I0522 23:37:56.026211 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94415 (* 1 = 7.94415 loss)
I0522 23:37:56.035511 35003 sgd_solver.cpp:112] Iteration 92600, lr = 0.01
I0522 23:38:00.777019 35003 solver.cpp:239] Iteration 92610 (2.10499 iter/s, 4.75061s/10 iters), loss = 7.11687
I0522 23:38:00.777070 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11687 (* 1 = 7.11687 loss)
I0522 23:38:00.790043 35003 sgd_solver.cpp:112] Iteration 92610, lr = 0.01
I0522 23:38:05.969094 35003 solver.cpp:239] Iteration 92620 (1.92611 iter/s, 5.19181s/10 iters), loss = 6.54841
I0522 23:38:05.969151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54841 (* 1 = 6.54841 loss)
I0522 23:38:06.658527 35003 sgd_solver.cpp:112] Iteration 92620, lr = 0.01
I0522 23:38:09.536140 35003 solver.cpp:239] Iteration 92630 (2.8036 iter/s, 3.56684s/10 iters), loss = 7.24803
I0522 23:38:09.536188 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24803 (* 1 = 7.24803 loss)
I0522 23:38:10.264688 35003 sgd_solver.cpp:112] Iteration 92630, lr = 0.01
I0522 23:38:13.794864 35003 solver.cpp:239] Iteration 92640 (2.34824 iter/s, 4.2585s/10 iters), loss = 7.87558
I0522 23:38:13.795141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87558 (* 1 = 7.87558 loss)
I0522 23:38:13.800808 35003 sgd_solver.cpp:112] Iteration 92640, lr = 0.01
I0522 23:38:16.708330 35003 solver.cpp:239] Iteration 92650 (3.43278 iter/s, 2.91309s/10 iters), loss = 7.8677
I0522 23:38:16.708382 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8677 (* 1 = 7.8677 loss)
I0522 23:38:16.715709 35003 sgd_solver.cpp:112] Iteration 92650, lr = 0.01
I0522 23:38:21.091022 35003 solver.cpp:239] Iteration 92660 (2.28182 iter/s, 4.38246s/10 iters), loss = 6.88578
I0522 23:38:21.091076 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88578 (* 1 = 6.88578 loss)
I0522 23:38:21.099902 35003 sgd_solver.cpp:112] Iteration 92660, lr = 0.01
I0522 23:38:23.987303 35003 solver.cpp:239] Iteration 92670 (3.45292 iter/s, 2.8961s/10 iters), loss = 7.29716
I0522 23:38:23.987354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29716 (* 1 = 7.29716 loss)
I0522 23:38:24.700474 35003 sgd_solver.cpp:112] Iteration 92670, lr = 0.01
I0522 23:38:27.186889 35003 solver.cpp:239] Iteration 92680 (3.12558 iter/s, 3.1994s/10 iters), loss = 7.35608
I0522 23:38:27.186929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35608 (* 1 = 7.35608 loss)
I0522 23:38:27.211163 35003 sgd_solver.cpp:112] Iteration 92680, lr = 0.01
I0522 23:38:32.456043 35003 solver.cpp:239] Iteration 92690 (1.89793 iter/s, 5.2689s/10 iters), loss = 7.26584
I0522 23:38:32.456082 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26584 (* 1 = 7.26584 loss)
I0522 23:38:32.469324 35003 sgd_solver.cpp:112] Iteration 92690, lr = 0.01
I0522 23:38:34.506564 35003 solver.cpp:239] Iteration 92700 (4.87713 iter/s, 2.05038s/10 iters), loss = 7.67478
I0522 23:38:34.506610 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67478 (* 1 = 7.67478 loss)
I0522 23:38:34.514705 35003 sgd_solver.cpp:112] Iteration 92700, lr = 0.01
I0522 23:38:37.665208 35003 solver.cpp:239] Iteration 92710 (3.16609 iter/s, 3.15847s/10 iters), loss = 8.38791
I0522 23:38:37.665261 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.38791 (* 1 = 8.38791 loss)
I0522 23:38:37.671463 35003 sgd_solver.cpp:112] Iteration 92710, lr = 0.01
I0522 23:38:41.303122 35003 solver.cpp:239] Iteration 92720 (2.74899 iter/s, 3.6377s/10 iters), loss = 5.35003
I0522 23:38:41.303171 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.35003 (* 1 = 5.35003 loss)
I0522 23:38:41.310217 35003 sgd_solver.cpp:112] Iteration 92720, lr = 0.01
I0522 23:38:43.368824 35003 solver.cpp:239] Iteration 92730 (4.8413 iter/s, 2.06556s/10 iters), loss = 7.32078
I0522 23:38:43.368878 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32078 (* 1 = 7.32078 loss)
I0522 23:38:44.058058 35003 sgd_solver.cpp:112] Iteration 92730, lr = 0.01
I0522 23:38:47.762866 35003 solver.cpp:239] Iteration 92740 (2.27593 iter/s, 4.3938s/10 iters), loss = 6.19782
I0522 23:38:47.762946 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19782 (* 1 = 6.19782 loss)
I0522 23:38:47.766891 35003 sgd_solver.cpp:112] Iteration 92740, lr = 0.01
I0522 23:38:49.062024 35003 solver.cpp:239] Iteration 92750 (7.69804 iter/s, 1.29903s/10 iters), loss = 6.38579
I0522 23:38:49.062068 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38579 (* 1 = 6.38579 loss)
I0522 23:38:49.079360 35003 sgd_solver.cpp:112] Iteration 92750, lr = 0.01
I0522 23:38:52.714128 35003 solver.cpp:239] Iteration 92760 (2.7383 iter/s, 3.6519s/10 iters), loss = 8.23674
I0522 23:38:52.714169 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.23674 (* 1 = 8.23674 loss)
I0522 23:38:52.726081 35003 sgd_solver.cpp:112] Iteration 92760, lr = 0.01
I0522 23:38:56.692108 35003 solver.cpp:239] Iteration 92770 (2.51398 iter/s, 3.97776s/10 iters), loss = 7.86317
I0522 23:38:56.692174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86317 (* 1 = 7.86317 loss)
I0522 23:38:56.699537 35003 sgd_solver.cpp:112] Iteration 92770, lr = 0.01
I0522 23:39:00.234000 35003 solver.cpp:239] Iteration 92780 (2.82352 iter/s, 3.54168s/10 iters), loss = 7.32512
I0522 23:39:00.234038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32512 (* 1 = 7.32512 loss)
I0522 23:39:00.244122 35003 sgd_solver.cpp:112] Iteration 92780, lr = 0.01
I0522 23:39:04.618666 35003 solver.cpp:239] Iteration 92790 (2.28079 iter/s, 4.38444s/10 iters), loss = 7.33761
I0522 23:39:04.618746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33761 (* 1 = 7.33761 loss)
I0522 23:39:04.631177 35003 sgd_solver.cpp:112] Iteration 92790, lr = 0.01
I0522 23:39:09.849877 35003 solver.cpp:239] Iteration 92800 (1.91171 iter/s, 5.23092s/10 iters), loss = 8.05494
I0522 23:39:09.849923 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05494 (* 1 = 8.05494 loss)
I0522 23:39:09.853476 35003 sgd_solver.cpp:112] Iteration 92800, lr = 0.01
I0522 23:39:13.277601 35003 solver.cpp:239] Iteration 92810 (2.91756 iter/s, 3.42753s/10 iters), loss = 8.07675
I0522 23:39:13.277649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.07675 (* 1 = 8.07675 loss)
I0522 23:39:13.291362 35003 sgd_solver.cpp:112] Iteration 92810, lr = 0.01
I0522 23:39:16.877230 35003 solver.cpp:239] Iteration 92820 (2.77821 iter/s, 3.59943s/10 iters), loss = 6.94077
I0522 23:39:16.877459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94077 (* 1 = 6.94077 loss)
I0522 23:39:17.579972 35003 sgd_solver.cpp:112] Iteration 92820, lr = 0.01
I0522 23:39:22.030642 35003 solver.cpp:239] Iteration 92830 (1.94062 iter/s, 5.15299s/10 iters), loss = 7.94301
I0522 23:39:22.030725 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94301 (* 1 = 7.94301 loss)
I0522 23:39:22.038627 35003 sgd_solver.cpp:112] Iteration 92830, lr = 0.01
I0522 23:39:26.486671 35003 solver.cpp:239] Iteration 92840 (2.24427 iter/s, 4.45579s/10 iters), loss = 7.44742
I0522 23:39:26.486724 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44742 (* 1 = 7.44742 loss)
I0522 23:39:26.701850 35003 sgd_solver.cpp:112] Iteration 92840, lr = 0.01
I0522 23:39:29.818756 35003 solver.cpp:239] Iteration 92850 (3.0013 iter/s, 3.33189s/10 iters), loss = 8.02282
I0522 23:39:29.818795 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02282 (* 1 = 8.02282 loss)
I0522 23:39:29.831760 35003 sgd_solver.cpp:112] Iteration 92850, lr = 0.01
I0522 23:39:32.687650 35003 solver.cpp:239] Iteration 92860 (3.48586 iter/s, 2.86873s/10 iters), loss = 8.12836
I0522 23:39:32.687690 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.12836 (* 1 = 8.12836 loss)
I0522 23:39:33.329639 35003 sgd_solver.cpp:112] Iteration 92860, lr = 0.01
I0522 23:39:37.255054 35003 solver.cpp:239] Iteration 92870 (2.18954 iter/s, 4.56717s/10 iters), loss = 7.97414
I0522 23:39:37.255101 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97414 (* 1 = 7.97414 loss)
I0522 23:39:37.273718 35003 sgd_solver.cpp:112] Iteration 92870, lr = 0.01
I0522 23:39:40.044849 35003 solver.cpp:239] Iteration 92880 (3.58471 iter/s, 2.78962s/10 iters), loss = 6.92032
I0522 23:39:40.044891 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92032 (* 1 = 6.92032 loss)
I0522 23:39:40.058411 35003 sgd_solver.cpp:112] Iteration 92880, lr = 0.01
I0522 23:39:44.362272 35003 solver.cpp:239] Iteration 92890 (2.31631 iter/s, 4.3172s/10 iters), loss = 9.28417
I0522 23:39:44.362321 35003 solver.cpp:258]     Train net output #0: softmax_loss = 9.28417 (* 1 = 9.28417 loss)
I0522 23:39:44.364289 35003 sgd_solver.cpp:112] Iteration 92890, lr = 0.01
I0522 23:39:47.277590 35003 solver.cpp:239] Iteration 92900 (3.43037 iter/s, 2.91514s/10 iters), loss = 7.35635
I0522 23:39:47.277773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35635 (* 1 = 7.35635 loss)
I0522 23:39:47.284734 35003 sgd_solver.cpp:112] Iteration 92900, lr = 0.01
I0522 23:39:51.663393 35003 solver.cpp:239] Iteration 92910 (2.28027 iter/s, 4.38544s/10 iters), loss = 7.70053
I0522 23:39:51.663439 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70053 (* 1 = 7.70053 loss)
I0522 23:39:52.339834 35003 sgd_solver.cpp:112] Iteration 92910, lr = 0.01
I0522 23:39:53.707621 35003 solver.cpp:239] Iteration 92920 (4.89216 iter/s, 2.04409s/10 iters), loss = 6.86944
I0522 23:39:53.707679 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86944 (* 1 = 6.86944 loss)
I0522 23:39:53.711738 35003 sgd_solver.cpp:112] Iteration 92920, lr = 0.01
I0522 23:39:56.487186 35003 solver.cpp:239] Iteration 92930 (3.59791 iter/s, 2.77939s/10 iters), loss = 7.15602
I0522 23:39:56.487236 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15602 (* 1 = 7.15602 loss)
I0522 23:39:56.495290 35003 sgd_solver.cpp:112] Iteration 92930, lr = 0.01
I0522 23:39:59.633471 35003 solver.cpp:239] Iteration 92940 (3.17853 iter/s, 3.1461s/10 iters), loss = 7.40826
I0522 23:39:59.633513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40826 (* 1 = 7.40826 loss)
I0522 23:39:59.661269 35003 sgd_solver.cpp:112] Iteration 92940, lr = 0.01
I0522 23:40:05.607887 35003 solver.cpp:239] Iteration 92950 (1.67389 iter/s, 5.97412s/10 iters), loss = 8.03836
I0522 23:40:05.607936 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03836 (* 1 = 8.03836 loss)
I0522 23:40:05.616502 35003 sgd_solver.cpp:112] Iteration 92950, lr = 0.01
I0522 23:40:08.885502 35003 solver.cpp:239] Iteration 92960 (3.05118 iter/s, 3.27742s/10 iters), loss = 6.92423
I0522 23:40:08.885550 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92423 (* 1 = 6.92423 loss)
I0522 23:40:08.894641 35003 sgd_solver.cpp:112] Iteration 92960, lr = 0.01
I0522 23:40:12.196120 35003 solver.cpp:239] Iteration 92970 (3.02076 iter/s, 3.31043s/10 iters), loss = 6.98659
I0522 23:40:12.196171 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98659 (* 1 = 6.98659 loss)
I0522 23:40:12.222077 35003 sgd_solver.cpp:112] Iteration 92970, lr = 0.01
I0522 23:40:16.794157 35003 solver.cpp:239] Iteration 92980 (2.17496 iter/s, 4.5978s/10 iters), loss = 7.92661
I0522 23:40:16.794203 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.92661 (* 1 = 7.92661 loss)
I0522 23:40:16.806787 35003 sgd_solver.cpp:112] Iteration 92980, lr = 0.01
I0522 23:40:20.418021 35003 solver.cpp:239] Iteration 92990 (2.75964 iter/s, 3.62366s/10 iters), loss = 6.44469
I0522 23:40:20.418323 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44469 (* 1 = 6.44469 loss)
I0522 23:40:20.426117 35003 sgd_solver.cpp:112] Iteration 92990, lr = 0.01
I0522 23:40:23.741612 35003 solver.cpp:239] Iteration 93000 (3.00917 iter/s, 3.32317s/10 iters), loss = 6.98305
I0522 23:40:23.741672 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98305 (* 1 = 6.98305 loss)
I0522 23:40:23.764212 35003 sgd_solver.cpp:112] Iteration 93000, lr = 0.01
I0522 23:40:28.088037 35003 solver.cpp:239] Iteration 93010 (2.30087 iter/s, 4.34618s/10 iters), loss = 6.83799
I0522 23:40:28.088099 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83799 (* 1 = 6.83799 loss)
I0522 23:40:28.094432 35003 sgd_solver.cpp:112] Iteration 93010, lr = 0.01
I0522 23:40:31.724495 35003 solver.cpp:239] Iteration 93020 (2.75011 iter/s, 3.63622s/10 iters), loss = 7.28444
I0522 23:40:31.724541 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28444 (* 1 = 7.28444 loss)
I0522 23:40:32.411383 35003 sgd_solver.cpp:112] Iteration 93020, lr = 0.01
I0522 23:40:37.273233 35003 solver.cpp:239] Iteration 93030 (1.8023 iter/s, 5.54847s/10 iters), loss = 6.39218
I0522 23:40:37.273281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39218 (* 1 = 6.39218 loss)
I0522 23:40:37.969125 35003 sgd_solver.cpp:112] Iteration 93030, lr = 0.01
I0522 23:40:42.226130 35003 solver.cpp:239] Iteration 93040 (2.01912 iter/s, 4.95265s/10 iters), loss = 7.61354
I0522 23:40:42.226192 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61354 (* 1 = 7.61354 loss)
I0522 23:40:42.234220 35003 sgd_solver.cpp:112] Iteration 93040, lr = 0.01
I0522 23:40:47.275089 35003 solver.cpp:239] Iteration 93050 (1.98071 iter/s, 5.04869s/10 iters), loss = 7.16266
I0522 23:40:47.275130 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16266 (* 1 = 7.16266 loss)
I0522 23:40:47.278152 35003 sgd_solver.cpp:112] Iteration 93050, lr = 0.01
I0522 23:40:50.785418 35003 solver.cpp:239] Iteration 93060 (2.84894 iter/s, 3.51008s/10 iters), loss = 7.00226
I0522 23:40:50.785612 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00226 (* 1 = 7.00226 loss)
I0522 23:40:50.788863 35003 sgd_solver.cpp:112] Iteration 93060, lr = 0.01
I0522 23:40:54.228107 35003 solver.cpp:239] Iteration 93070 (2.90498 iter/s, 3.44236s/10 iters), loss = 6.90314
I0522 23:40:54.228159 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90314 (* 1 = 6.90314 loss)
I0522 23:40:54.246485 35003 sgd_solver.cpp:112] Iteration 93070, lr = 0.01
I0522 23:40:56.783362 35003 solver.cpp:239] Iteration 93080 (3.91375 iter/s, 2.5551s/10 iters), loss = 6.49829
I0522 23:40:56.783407 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49829 (* 1 = 6.49829 loss)
I0522 23:40:57.499053 35003 sgd_solver.cpp:112] Iteration 93080, lr = 0.01
I0522 23:40:59.783906 35003 solver.cpp:239] Iteration 93090 (3.33292 iter/s, 3.00037s/10 iters), loss = 7.08109
I0522 23:40:59.783948 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08109 (* 1 = 7.08109 loss)
I0522 23:41:00.453742 35003 sgd_solver.cpp:112] Iteration 93090, lr = 0.01
I0522 23:41:02.518501 35003 solver.cpp:239] Iteration 93100 (3.65707 iter/s, 2.73443s/10 iters), loss = 6.85727
I0522 23:41:02.518548 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85727 (* 1 = 6.85727 loss)
I0522 23:41:02.532093 35003 sgd_solver.cpp:112] Iteration 93100, lr = 0.01
I0522 23:41:07.895563 35003 solver.cpp:239] Iteration 93110 (1.85984 iter/s, 5.37679s/10 iters), loss = 6.61964
I0522 23:41:07.895624 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61964 (* 1 = 6.61964 loss)
I0522 23:41:07.923032 35003 sgd_solver.cpp:112] Iteration 93110, lr = 0.01
I0522 23:41:12.206449 35003 solver.cpp:239] Iteration 93120 (2.31983 iter/s, 4.31065s/10 iters), loss = 7.32087
I0522 23:41:12.206487 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32087 (* 1 = 7.32087 loss)
I0522 23:41:12.219982 35003 sgd_solver.cpp:112] Iteration 93120, lr = 0.01
I0522 23:41:15.131512 35003 solver.cpp:239] Iteration 93130 (3.41894 iter/s, 2.92489s/10 iters), loss = 7.41568
I0522 23:41:15.131584 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41568 (* 1 = 7.41568 loss)
I0522 23:41:15.869968 35003 sgd_solver.cpp:112] Iteration 93130, lr = 0.01
I0522 23:41:19.909255 35003 solver.cpp:239] Iteration 93140 (2.09315 iter/s, 4.77749s/10 iters), loss = 7.96634
I0522 23:41:19.909307 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96634 (* 1 = 7.96634 loss)
I0522 23:41:19.931371 35003 sgd_solver.cpp:112] Iteration 93140, lr = 0.01
I0522 23:41:22.661701 35003 solver.cpp:239] Iteration 93150 (3.63336 iter/s, 2.75228s/10 iters), loss = 8.02205
I0522 23:41:22.661844 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02205 (* 1 = 8.02205 loss)
I0522 23:41:22.683382 35003 sgd_solver.cpp:112] Iteration 93150, lr = 0.01
I0522 23:41:27.059937 35003 solver.cpp:239] Iteration 93160 (2.2738 iter/s, 4.39793s/10 iters), loss = 7.16646
I0522 23:41:27.059974 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16646 (* 1 = 7.16646 loss)
I0522 23:41:27.065219 35003 sgd_solver.cpp:112] Iteration 93160, lr = 0.01
I0522 23:41:30.776602 35003 solver.cpp:239] Iteration 93170 (2.69072 iter/s, 3.71647s/10 iters), loss = 7.60649
I0522 23:41:30.776639 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60649 (* 1 = 7.60649 loss)
I0522 23:41:30.790282 35003 sgd_solver.cpp:112] Iteration 93170, lr = 0.01
I0522 23:41:34.490954 35003 solver.cpp:239] Iteration 93180 (2.69241 iter/s, 3.71415s/10 iters), loss = 7.68418
I0522 23:41:34.491024 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68418 (* 1 = 7.68418 loss)
I0522 23:41:34.500435 35003 sgd_solver.cpp:112] Iteration 93180, lr = 0.01
I0522 23:41:38.149745 35003 solver.cpp:239] Iteration 93190 (2.73331 iter/s, 3.65857s/10 iters), loss = 7.14418
I0522 23:41:38.149801 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14418 (* 1 = 7.14418 loss)
I0522 23:41:38.159081 35003 sgd_solver.cpp:112] Iteration 93190, lr = 0.01
I0522 23:41:40.806679 35003 solver.cpp:239] Iteration 93200 (3.76398 iter/s, 2.65676s/10 iters), loss = 6.97635
I0522 23:41:40.806740 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97635 (* 1 = 6.97635 loss)
I0522 23:41:40.814941 35003 sgd_solver.cpp:112] Iteration 93200, lr = 0.01
I0522 23:41:43.293426 35003 solver.cpp:239] Iteration 93210 (4.02159 iter/s, 2.48658s/10 iters), loss = 7.01794
I0522 23:41:43.293463 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01794 (* 1 = 7.01794 loss)
I0522 23:41:43.991130 35003 sgd_solver.cpp:112] Iteration 93210, lr = 0.01
I0522 23:41:46.342782 35003 solver.cpp:239] Iteration 93220 (3.27956 iter/s, 3.04919s/10 iters), loss = 6.47496
I0522 23:41:46.342834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47496 (* 1 = 6.47496 loss)
I0522 23:41:46.349874 35003 sgd_solver.cpp:112] Iteration 93220, lr = 0.01
I0522 23:41:49.894450 35003 solver.cpp:239] Iteration 93230 (2.81573 iter/s, 3.55147s/10 iters), loss = 7.98443
I0522 23:41:49.894491 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98443 (* 1 = 7.98443 loss)
I0522 23:41:49.902950 35003 sgd_solver.cpp:112] Iteration 93230, lr = 0.01
I0522 23:41:54.130282 35003 solver.cpp:239] Iteration 93240 (2.36094 iter/s, 4.2356s/10 iters), loss = 6.77574
I0522 23:41:54.130574 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77574 (* 1 = 6.77574 loss)
I0522 23:41:54.135308 35003 sgd_solver.cpp:112] Iteration 93240, lr = 0.01
I0522 23:41:55.427909 35003 solver.cpp:239] Iteration 93250 (7.70831 iter/s, 1.2973s/10 iters), loss = 7.58824
I0522 23:41:55.427949 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58824 (* 1 = 7.58824 loss)
I0522 23:41:55.441229 35003 sgd_solver.cpp:112] Iteration 93250, lr = 0.01
I0522 23:41:58.117105 35003 solver.cpp:239] Iteration 93260 (3.7188 iter/s, 2.68904s/10 iters), loss = 6.8845
I0522 23:41:58.117143 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8845 (* 1 = 6.8845 loss)
I0522 23:41:58.129881 35003 sgd_solver.cpp:112] Iteration 93260, lr = 0.01
I0522 23:42:01.704911 35003 solver.cpp:239] Iteration 93270 (2.78737 iter/s, 3.58761s/10 iters), loss = 7.87637
I0522 23:42:01.704962 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87637 (* 1 = 7.87637 loss)
I0522 23:42:02.439337 35003 sgd_solver.cpp:112] Iteration 93270, lr = 0.01
I0522 23:42:05.315651 35003 solver.cpp:239] Iteration 93280 (2.76967 iter/s, 3.61053s/10 iters), loss = 7.97769
I0522 23:42:05.315711 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97769 (* 1 = 7.97769 loss)
I0522 23:42:05.320833 35003 sgd_solver.cpp:112] Iteration 93280, lr = 0.01
I0522 23:42:08.848260 35003 solver.cpp:239] Iteration 93290 (2.83093 iter/s, 3.5324s/10 iters), loss = 6.75004
I0522 23:42:08.848312 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75004 (* 1 = 6.75004 loss)
I0522 23:42:08.854946 35003 sgd_solver.cpp:112] Iteration 93290, lr = 0.01
I0522 23:42:10.642266 35003 solver.cpp:239] Iteration 93300 (5.57458 iter/s, 1.79386s/10 iters), loss = 8.3764
I0522 23:42:10.642325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.3764 (* 1 = 8.3764 loss)
I0522 23:42:10.658200 35003 sgd_solver.cpp:112] Iteration 93300, lr = 0.01
I0522 23:42:14.324893 35003 solver.cpp:239] Iteration 93310 (2.71561 iter/s, 3.68241s/10 iters), loss = 7.5327
I0522 23:42:14.324945 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5327 (* 1 = 7.5327 loss)
I0522 23:42:15.059296 35003 sgd_solver.cpp:112] Iteration 93310, lr = 0.01
I0522 23:42:16.894965 35003 solver.cpp:239] Iteration 93320 (3.89119 iter/s, 2.56991s/10 iters), loss = 7.91334
I0522 23:42:16.895016 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91334 (* 1 = 7.91334 loss)
I0522 23:42:16.913202 35003 sgd_solver.cpp:112] Iteration 93320, lr = 0.01
I0522 23:42:20.527880 35003 solver.cpp:239] Iteration 93330 (2.75612 iter/s, 3.62829s/10 iters), loss = 6.85989
I0522 23:42:20.527930 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85989 (* 1 = 6.85989 loss)
I0522 23:42:20.533812 35003 sgd_solver.cpp:112] Iteration 93330, lr = 0.01
I0522 23:42:24.140743 35003 solver.cpp:239] Iteration 93340 (2.76804 iter/s, 3.61266s/10 iters), loss = 7.8301
I0522 23:42:24.141014 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8301 (* 1 = 7.8301 loss)
I0522 23:42:24.154177 35003 sgd_solver.cpp:112] Iteration 93340, lr = 0.01
I0522 23:42:27.592037 35003 solver.cpp:239] Iteration 93350 (2.89779 iter/s, 3.4509s/10 iters), loss = 8.04153
I0522 23:42:27.592089 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04153 (* 1 = 8.04153 loss)
I0522 23:42:27.608043 35003 sgd_solver.cpp:112] Iteration 93350, lr = 0.01
I0522 23:42:30.473722 35003 solver.cpp:239] Iteration 93360 (3.4704 iter/s, 2.88151s/10 iters), loss = 6.91666
I0522 23:42:30.473773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91666 (* 1 = 6.91666 loss)
I0522 23:42:30.516731 35003 sgd_solver.cpp:112] Iteration 93360, lr = 0.01
I0522 23:42:33.258177 35003 solver.cpp:239] Iteration 93370 (3.59159 iter/s, 2.78429s/10 iters), loss = 6.41823
I0522 23:42:33.258219 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41823 (* 1 = 6.41823 loss)
I0522 23:42:33.266067 35003 sgd_solver.cpp:112] Iteration 93370, lr = 0.01
I0522 23:42:36.958093 35003 solver.cpp:239] Iteration 93380 (2.70291 iter/s, 3.69972s/10 iters), loss = 7.57009
I0522 23:42:36.958142 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57009 (* 1 = 7.57009 loss)
I0522 23:42:36.962927 35003 sgd_solver.cpp:112] Iteration 93380, lr = 0.01
I0522 23:42:42.442370 35003 solver.cpp:239] Iteration 93390 (1.82348 iter/s, 5.48401s/10 iters), loss = 7.14424
I0522 23:42:42.442412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14424 (* 1 = 7.14424 loss)
I0522 23:42:42.455983 35003 sgd_solver.cpp:112] Iteration 93390, lr = 0.01
I0522 23:42:44.469475 35003 solver.cpp:239] Iteration 93400 (4.93346 iter/s, 2.02697s/10 iters), loss = 7.38757
I0522 23:42:44.469513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38757 (* 1 = 7.38757 loss)
I0522 23:42:44.477809 35003 sgd_solver.cpp:112] Iteration 93400, lr = 0.01
I0522 23:42:47.632917 35003 solver.cpp:239] Iteration 93410 (3.16129 iter/s, 3.16326s/10 iters), loss = 8.18301
I0522 23:42:47.632968 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.18301 (* 1 = 8.18301 loss)
I0522 23:42:47.640260 35003 sgd_solver.cpp:112] Iteration 93410, lr = 0.01
I0522 23:42:48.963619 35003 solver.cpp:239] Iteration 93420 (7.5157 iter/s, 1.33055s/10 iters), loss = 6.96209
I0522 23:42:48.963666 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96209 (* 1 = 6.96209 loss)
I0522 23:42:49.678503 35003 sgd_solver.cpp:112] Iteration 93420, lr = 0.01
I0522 23:42:52.806264 35003 solver.cpp:239] Iteration 93430 (2.60251 iter/s, 3.84244s/10 iters), loss = 6.74224
I0522 23:42:52.806311 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74224 (* 1 = 6.74224 loss)
I0522 23:42:53.544163 35003 sgd_solver.cpp:112] Iteration 93430, lr = 0.01
I0522 23:42:57.764389 35003 solver.cpp:239] Iteration 93440 (2.01699 iter/s, 4.95788s/10 iters), loss = 7.28707
I0522 23:42:57.764652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28707 (* 1 = 7.28707 loss)
I0522 23:42:57.776202 35003 sgd_solver.cpp:112] Iteration 93440, lr = 0.01
I0522 23:42:59.698072 35003 solver.cpp:239] Iteration 93450 (5.17231 iter/s, 1.93337s/10 iters), loss = 7.54778
I0522 23:42:59.698113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54778 (* 1 = 7.54778 loss)
I0522 23:43:00.397862 35003 sgd_solver.cpp:112] Iteration 93450, lr = 0.01
I0522 23:43:03.219532 35003 solver.cpp:239] Iteration 93460 (2.83989 iter/s, 3.52127s/10 iters), loss = 7.92857
I0522 23:43:03.219581 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.92857 (* 1 = 7.92857 loss)
I0522 23:43:03.230890 35003 sgd_solver.cpp:112] Iteration 93460, lr = 0.01
I0522 23:43:07.221913 35003 solver.cpp:239] Iteration 93470 (2.49865 iter/s, 4.00216s/10 iters), loss = 6.8913
I0522 23:43:07.221971 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8913 (* 1 = 6.8913 loss)
I0522 23:43:07.231235 35003 sgd_solver.cpp:112] Iteration 93470, lr = 0.01
I0522 23:43:11.591087 35003 solver.cpp:239] Iteration 93480 (2.28889 iter/s, 4.36894s/10 iters), loss = 7.65749
I0522 23:43:11.591136 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65749 (* 1 = 7.65749 loss)
I0522 23:43:11.604804 35003 sgd_solver.cpp:112] Iteration 93480, lr = 0.01
I0522 23:43:15.241221 35003 solver.cpp:239] Iteration 93490 (2.73978 iter/s, 3.64993s/10 iters), loss = 7.21274
I0522 23:43:15.241279 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21274 (* 1 = 7.21274 loss)
I0522 23:43:15.942629 35003 sgd_solver.cpp:112] Iteration 93490, lr = 0.01
I0522 23:43:20.443011 35003 solver.cpp:239] Iteration 93500 (1.92252 iter/s, 5.20152s/10 iters), loss = 7.69827
I0522 23:43:20.443050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69827 (* 1 = 7.69827 loss)
I0522 23:43:20.456480 35003 sgd_solver.cpp:112] Iteration 93500, lr = 0.01
I0522 23:43:23.145720 35003 solver.cpp:239] Iteration 93510 (3.70022 iter/s, 2.70254s/10 iters), loss = 7.2124
I0522 23:43:23.145759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2124 (* 1 = 7.2124 loss)
I0522 23:43:23.149130 35003 sgd_solver.cpp:112] Iteration 93510, lr = 0.01
I0522 23:43:25.825330 35003 solver.cpp:239] Iteration 93520 (3.73215 iter/s, 2.67942s/10 iters), loss = 6.4854
I0522 23:43:25.825393 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4854 (* 1 = 6.4854 loss)
I0522 23:43:25.830657 35003 sgd_solver.cpp:112] Iteration 93520, lr = 0.01
I0522 23:43:27.133373 35003 solver.cpp:239] Iteration 93530 (7.64576 iter/s, 1.30791s/10 iters), loss = 6.81026
I0522 23:43:27.133432 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81026 (* 1 = 6.81026 loss)
I0522 23:43:27.146351 35003 sgd_solver.cpp:112] Iteration 93530, lr = 0.01
I0522 23:43:29.126508 35003 solver.cpp:239] Iteration 93540 (5.01758 iter/s, 1.99299s/10 iters), loss = 7.2799
I0522 23:43:29.126811 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2799 (* 1 = 7.2799 loss)
I0522 23:43:29.139827 35003 sgd_solver.cpp:112] Iteration 93540, lr = 0.01
I0522 23:43:32.705235 35003 solver.cpp:239] Iteration 93550 (2.79806 iter/s, 3.5739s/10 iters), loss = 7.09251
I0522 23:43:32.705278 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09251 (* 1 = 7.09251 loss)
I0522 23:43:33.393517 35003 sgd_solver.cpp:112] Iteration 93550, lr = 0.01
I0522 23:43:36.853380 35003 solver.cpp:239] Iteration 93560 (2.41084 iter/s, 4.14793s/10 iters), loss = 6.98564
I0522 23:43:36.853435 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98564 (* 1 = 6.98564 loss)
I0522 23:43:37.568823 35003 sgd_solver.cpp:112] Iteration 93560, lr = 0.01
I0522 23:43:41.144588 35003 solver.cpp:239] Iteration 93570 (2.33047 iter/s, 4.29098s/10 iters), loss = 7.71609
I0522 23:43:41.144640 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71609 (* 1 = 7.71609 loss)
I0522 23:43:41.879057 35003 sgd_solver.cpp:112] Iteration 93570, lr = 0.01
I0522 23:43:46.122689 35003 solver.cpp:239] Iteration 93580 (2.0089 iter/s, 4.97785s/10 iters), loss = 7.94208
I0522 23:43:46.122757 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94208 (* 1 = 7.94208 loss)
I0522 23:43:46.831110 35003 sgd_solver.cpp:112] Iteration 93580, lr = 0.01
I0522 23:43:49.655154 35003 solver.cpp:239] Iteration 93590 (2.83106 iter/s, 3.53225s/10 iters), loss = 6.52814
I0522 23:43:49.655205 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52814 (* 1 = 6.52814 loss)
I0522 23:43:49.663920 35003 sgd_solver.cpp:112] Iteration 93590, lr = 0.01
I0522 23:43:54.720892 35003 solver.cpp:239] Iteration 93600 (1.97415 iter/s, 5.06548s/10 iters), loss = 7.08896
I0522 23:43:54.720937 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08896 (* 1 = 7.08896 loss)
I0522 23:43:54.728250 35003 sgd_solver.cpp:112] Iteration 93600, lr = 0.01
I0522 23:43:58.462802 35003 solver.cpp:239] Iteration 93610 (2.67258 iter/s, 3.74171s/10 iters), loss = 7.48062
I0522 23:43:58.462875 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48062 (* 1 = 7.48062 loss)
I0522 23:43:58.600250 35003 sgd_solver.cpp:112] Iteration 93610, lr = 0.01
I0522 23:44:02.249110 35003 solver.cpp:239] Iteration 93620 (2.64125 iter/s, 3.78608s/10 iters), loss = 5.86691
I0522 23:44:02.249372 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.86691 (* 1 = 5.86691 loss)
I0522 23:44:02.984256 35003 sgd_solver.cpp:112] Iteration 93620, lr = 0.01
I0522 23:44:06.155148 35003 solver.cpp:239] Iteration 93630 (2.5604 iter/s, 3.90565s/10 iters), loss = 6.95574
I0522 23:44:06.155190 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95574 (* 1 = 6.95574 loss)
I0522 23:44:06.876096 35003 sgd_solver.cpp:112] Iteration 93630, lr = 0.01
I0522 23:44:08.984333 35003 solver.cpp:239] Iteration 93640 (3.53479 iter/s, 2.82902s/10 iters), loss = 7.12554
I0522 23:44:08.984375 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12554 (* 1 = 7.12554 loss)
I0522 23:44:08.997344 35003 sgd_solver.cpp:112] Iteration 93640, lr = 0.01
I0522 23:44:11.057916 35003 solver.cpp:239] Iteration 93650 (4.82288 iter/s, 2.07345s/10 iters), loss = 6.94729
I0522 23:44:11.057971 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94729 (* 1 = 6.94729 loss)
I0522 23:44:11.776684 35003 sgd_solver.cpp:112] Iteration 93650, lr = 0.01
I0522 23:44:14.494691 35003 solver.cpp:239] Iteration 93660 (2.90987 iter/s, 3.43658s/10 iters), loss = 6.09088
I0522 23:44:14.494755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09088 (* 1 = 6.09088 loss)
I0522 23:44:14.506021 35003 sgd_solver.cpp:112] Iteration 93660, lr = 0.01
I0522 23:44:16.560967 35003 solver.cpp:239] Iteration 93670 (4.84 iter/s, 2.06612s/10 iters), loss = 8.81658
I0522 23:44:16.561009 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.81658 (* 1 = 8.81658 loss)
I0522 23:44:16.586442 35003 sgd_solver.cpp:112] Iteration 93670, lr = 0.01
I0522 23:44:19.450772 35003 solver.cpp:239] Iteration 93680 (3.46064 iter/s, 2.88964s/10 iters), loss = 6.03712
I0522 23:44:19.450809 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03712 (* 1 = 6.03712 loss)
I0522 23:44:19.456310 35003 sgd_solver.cpp:112] Iteration 93680, lr = 0.01
I0522 23:44:22.285913 35003 solver.cpp:239] Iteration 93690 (3.52736 iter/s, 2.83498s/10 iters), loss = 7.26937
I0522 23:44:22.285967 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26937 (* 1 = 7.26937 loss)
I0522 23:44:22.298163 35003 sgd_solver.cpp:112] Iteration 93690, lr = 0.01
I0522 23:44:27.219657 35003 solver.cpp:239] Iteration 93700 (2.02696 iter/s, 4.93349s/10 iters), loss = 6.8203
I0522 23:44:27.219698 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8203 (* 1 = 6.8203 loss)
I0522 23:44:27.232489 35003 sgd_solver.cpp:112] Iteration 93700, lr = 0.01
I0522 23:44:30.828735 35003 solver.cpp:239] Iteration 93710 (2.77094 iter/s, 3.60889s/10 iters), loss = 5.84617
I0522 23:44:30.828780 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84617 (* 1 = 5.84617 loss)
I0522 23:44:31.563721 35003 sgd_solver.cpp:112] Iteration 93710, lr = 0.01
I0522 23:44:33.855190 35003 solver.cpp:239] Iteration 93720 (3.30439 iter/s, 3.02627s/10 iters), loss = 6.68577
I0522 23:44:33.855470 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68577 (* 1 = 6.68577 loss)
I0522 23:44:33.860429 35003 sgd_solver.cpp:112] Iteration 93720, lr = 0.01
I0522 23:44:36.402156 35003 solver.cpp:239] Iteration 93730 (3.92681 iter/s, 2.5466s/10 iters), loss = 6.63387
I0522 23:44:36.402211 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63387 (* 1 = 6.63387 loss)
I0522 23:44:36.665168 35003 sgd_solver.cpp:112] Iteration 93730, lr = 0.01
I0522 23:44:39.387500 35003 solver.cpp:239] Iteration 93740 (3.34991 iter/s, 2.98516s/10 iters), loss = 7.29377
I0522 23:44:39.387558 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29377 (* 1 = 7.29377 loss)
I0522 23:44:39.396298 35003 sgd_solver.cpp:112] Iteration 93740, lr = 0.01
I0522 23:44:42.917723 35003 solver.cpp:239] Iteration 93750 (2.83285 iter/s, 3.53001s/10 iters), loss = 7.37904
I0522 23:44:42.917770 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37904 (* 1 = 7.37904 loss)
I0522 23:44:42.931329 35003 sgd_solver.cpp:112] Iteration 93750, lr = 0.01
I0522 23:44:45.099819 35003 solver.cpp:239] Iteration 93760 (4.58305 iter/s, 2.18195s/10 iters), loss = 7.54616
I0522 23:44:45.099866 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54616 (* 1 = 7.54616 loss)
I0522 23:44:45.109184 35003 sgd_solver.cpp:112] Iteration 93760, lr = 0.01
I0522 23:44:48.887480 35003 solver.cpp:239] Iteration 93770 (2.64029 iter/s, 3.78746s/10 iters), loss = 6.71464
I0522 23:44:48.887521 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71464 (* 1 = 6.71464 loss)
I0522 23:44:48.911394 35003 sgd_solver.cpp:112] Iteration 93770, lr = 0.01
I0522 23:44:52.206456 35003 solver.cpp:239] Iteration 93780 (3.01315 iter/s, 3.31879s/10 iters), loss = 7.68806
I0522 23:44:52.206501 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68806 (* 1 = 7.68806 loss)
I0522 23:44:52.218519 35003 sgd_solver.cpp:112] Iteration 93780, lr = 0.01
I0522 23:44:54.952543 35003 solver.cpp:239] Iteration 93790 (3.64179 iter/s, 2.7459s/10 iters), loss = 7.36116
I0522 23:44:54.952600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36116 (* 1 = 7.36116 loss)
I0522 23:44:54.956056 35003 sgd_solver.cpp:112] Iteration 93790, lr = 0.01
I0522 23:44:57.759284 35003 solver.cpp:239] Iteration 93800 (3.56309 iter/s, 2.80655s/10 iters), loss = 6.39261
I0522 23:44:57.759333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39261 (* 1 = 6.39261 loss)
I0522 23:44:57.767259 35003 sgd_solver.cpp:112] Iteration 93800, lr = 0.01
I0522 23:45:00.316321 35003 solver.cpp:239] Iteration 93810 (3.91101 iter/s, 2.55688s/10 iters), loss = 7.66924
I0522 23:45:00.316368 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66924 (* 1 = 7.66924 loss)
I0522 23:45:00.329293 35003 sgd_solver.cpp:112] Iteration 93810, lr = 0.01
I0522 23:45:03.911087 35003 solver.cpp:239] Iteration 93820 (2.78197 iter/s, 3.59457s/10 iters), loss = 7.3061
I0522 23:45:03.911346 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3061 (* 1 = 7.3061 loss)
I0522 23:45:03.924751 35003 sgd_solver.cpp:112] Iteration 93820, lr = 0.01
I0522 23:45:06.841387 35003 solver.cpp:239] Iteration 93830 (3.41304 iter/s, 2.92994s/10 iters), loss = 5.88824
I0522 23:45:06.841431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.88824 (* 1 = 5.88824 loss)
I0522 23:45:06.851076 35003 sgd_solver.cpp:112] Iteration 93830, lr = 0.01
I0522 23:45:10.507032 35003 solver.cpp:239] Iteration 93840 (2.72818 iter/s, 3.66544s/10 iters), loss = 6.66319
I0522 23:45:10.507081 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66319 (* 1 = 6.66319 loss)
I0522 23:45:11.241312 35003 sgd_solver.cpp:112] Iteration 93840, lr = 0.01
I0522 23:45:14.476392 35003 solver.cpp:239] Iteration 93850 (2.51944 iter/s, 3.96914s/10 iters), loss = 6.82507
I0522 23:45:14.476444 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82507 (* 1 = 6.82507 loss)
I0522 23:45:14.489609 35003 sgd_solver.cpp:112] Iteration 93850, lr = 0.01
I0522 23:45:17.326990 35003 solver.cpp:239] Iteration 93860 (3.50825 iter/s, 2.85043s/10 iters), loss = 8.03174
I0522 23:45:17.327029 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03174 (* 1 = 8.03174 loss)
I0522 23:45:18.035831 35003 sgd_solver.cpp:112] Iteration 93860, lr = 0.01
I0522 23:45:20.940299 35003 solver.cpp:239] Iteration 93870 (2.76769 iter/s, 3.61312s/10 iters), loss = 7.41912
I0522 23:45:20.940343 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41912 (* 1 = 7.41912 loss)
I0522 23:45:20.953653 35003 sgd_solver.cpp:112] Iteration 93870, lr = 0.01
I0522 23:45:24.088429 35003 solver.cpp:239] Iteration 93880 (3.17666 iter/s, 3.14796s/10 iters), loss = 6.79451
I0522 23:45:24.088465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79451 (* 1 = 6.79451 loss)
I0522 23:45:24.750015 35003 sgd_solver.cpp:112] Iteration 93880, lr = 0.01
I0522 23:45:27.710052 35003 solver.cpp:239] Iteration 93890 (2.76134 iter/s, 3.62143s/10 iters), loss = 6.72244
I0522 23:45:27.710100 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72244 (* 1 = 6.72244 loss)
I0522 23:45:27.723244 35003 sgd_solver.cpp:112] Iteration 93890, lr = 0.01
I0522 23:45:30.568310 35003 solver.cpp:239] Iteration 93900 (3.49884 iter/s, 2.85809s/10 iters), loss = 6.87757
I0522 23:45:30.568367 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87757 (* 1 = 6.87757 loss)
I0522 23:45:30.575904 35003 sgd_solver.cpp:112] Iteration 93900, lr = 0.01
I0522 23:45:32.623963 35003 solver.cpp:239] Iteration 93910 (4.86497 iter/s, 2.05551s/10 iters), loss = 6.90638
I0522 23:45:32.624011 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90638 (* 1 = 6.90638 loss)
I0522 23:45:33.271973 35003 sgd_solver.cpp:112] Iteration 93910, lr = 0.01
I0522 23:45:36.867480 35003 solver.cpp:239] Iteration 93920 (2.35666 iter/s, 4.24329s/10 iters), loss = 7.81555
I0522 23:45:36.867662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81555 (* 1 = 7.81555 loss)
I0522 23:45:36.880975 35003 sgd_solver.cpp:112] Iteration 93920, lr = 0.01
I0522 23:45:40.631217 35003 solver.cpp:239] Iteration 93930 (2.65718 iter/s, 3.76339s/10 iters), loss = 6.70202
I0522 23:45:40.631258 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70202 (* 1 = 6.70202 loss)
I0522 23:45:40.649438 35003 sgd_solver.cpp:112] Iteration 93930, lr = 0.01
I0522 23:45:42.762024 35003 solver.cpp:239] Iteration 93940 (4.69335 iter/s, 2.13067s/10 iters), loss = 6.41504
I0522 23:45:42.762065 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41504 (* 1 = 6.41504 loss)
I0522 23:45:43.395087 35003 sgd_solver.cpp:112] Iteration 93940, lr = 0.01
I0522 23:45:46.265041 35003 solver.cpp:239] Iteration 93950 (2.85484 iter/s, 3.50283s/10 iters), loss = 6.70382
I0522 23:45:46.265090 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70382 (* 1 = 6.70382 loss)
I0522 23:45:46.973954 35003 sgd_solver.cpp:112] Iteration 93950, lr = 0.01
I0522 23:45:51.707341 35003 solver.cpp:239] Iteration 93960 (1.83756 iter/s, 5.442s/10 iters), loss = 6.16355
I0522 23:45:51.707422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16355 (* 1 = 6.16355 loss)
I0522 23:45:51.715682 35003 sgd_solver.cpp:112] Iteration 93960, lr = 0.01
I0522 23:45:55.328073 35003 solver.cpp:239] Iteration 93970 (2.76204 iter/s, 3.62051s/10 iters), loss = 8.678
I0522 23:45:55.328111 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.678 (* 1 = 8.678 loss)
I0522 23:45:55.334992 35003 sgd_solver.cpp:112] Iteration 93970, lr = 0.01
I0522 23:45:58.165283 35003 solver.cpp:239] Iteration 93980 (3.52479 iter/s, 2.83705s/10 iters), loss = 7.26051
I0522 23:45:58.165328 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26051 (* 1 = 7.26051 loss)
I0522 23:45:58.894047 35003 sgd_solver.cpp:112] Iteration 93980, lr = 0.01
I0522 23:46:02.415530 35003 solver.cpp:239] Iteration 93990 (2.35293 iter/s, 4.25002s/10 iters), loss = 7.07912
I0522 23:46:02.415580 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07912 (* 1 = 7.07912 loss)
I0522 23:46:02.419508 35003 sgd_solver.cpp:112] Iteration 93990, lr = 0.01
I0522 23:46:04.514258 35003 solver.cpp:239] Iteration 94000 (4.76513 iter/s, 2.09858s/10 iters), loss = 7.29597
I0522 23:46:04.514323 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29597 (* 1 = 7.29597 loss)
I0522 23:46:05.216068 35003 sgd_solver.cpp:112] Iteration 94000, lr = 0.01
I0522 23:46:07.868784 35003 solver.cpp:239] Iteration 94010 (2.98123 iter/s, 3.35432s/10 iters), loss = 7.96
I0522 23:46:07.869093 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96 (* 1 = 7.96 loss)
I0522 23:46:08.057929 35003 sgd_solver.cpp:112] Iteration 94010, lr = 0.01
I0522 23:46:10.142361 35003 solver.cpp:239] Iteration 94020 (4.3991 iter/s, 2.27319s/10 iters), loss = 6.93611
I0522 23:46:10.142405 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93611 (* 1 = 6.93611 loss)
I0522 23:46:10.157176 35003 sgd_solver.cpp:112] Iteration 94020, lr = 0.01
I0522 23:46:15.237761 35003 solver.cpp:239] Iteration 94030 (1.96265 iter/s, 5.09514s/10 iters), loss = 7.55544
I0522 23:46:15.237813 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55544 (* 1 = 7.55544 loss)
I0522 23:46:15.274003 35003 sgd_solver.cpp:112] Iteration 94030, lr = 0.01
I0522 23:46:19.570346 35003 solver.cpp:239] Iteration 94040 (2.30821 iter/s, 4.33236s/10 iters), loss = 6.9366
I0522 23:46:19.570397 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9366 (* 1 = 6.9366 loss)
I0522 23:46:19.583493 35003 sgd_solver.cpp:112] Iteration 94040, lr = 0.01
I0522 23:46:24.462358 35003 solver.cpp:239] Iteration 94050 (2.04425 iter/s, 4.89176s/10 iters), loss = 6.69077
I0522 23:46:24.462409 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69077 (* 1 = 6.69077 loss)
I0522 23:46:24.474920 35003 sgd_solver.cpp:112] Iteration 94050, lr = 0.01
I0522 23:46:27.660081 35003 solver.cpp:239] Iteration 94060 (3.12742 iter/s, 3.19752s/10 iters), loss = 7.81919
I0522 23:46:27.660138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81919 (* 1 = 7.81919 loss)
I0522 23:46:27.666206 35003 sgd_solver.cpp:112] Iteration 94060, lr = 0.01
I0522 23:46:32.082913 35003 solver.cpp:239] Iteration 94070 (2.26112 iter/s, 4.4226s/10 iters), loss = 6.71039
I0522 23:46:32.082959 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71039 (* 1 = 6.71039 loss)
I0522 23:46:32.086249 35003 sgd_solver.cpp:112] Iteration 94070, lr = 0.01
I0522 23:46:35.052119 35003 solver.cpp:239] Iteration 94080 (3.36809 iter/s, 2.96904s/10 iters), loss = 8.18355
I0522 23:46:35.052157 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.18355 (* 1 = 8.18355 loss)
I0522 23:46:35.061784 35003 sgd_solver.cpp:112] Iteration 94080, lr = 0.01
I0522 23:46:39.251965 35003 solver.cpp:239] Iteration 94090 (2.38117 iter/s, 4.19962s/10 iters), loss = 7.72925
I0522 23:46:39.252221 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72925 (* 1 = 7.72925 loss)
I0522 23:46:39.264261 35003 sgd_solver.cpp:112] Iteration 94090, lr = 0.01
I0522 23:46:42.451190 35003 solver.cpp:239] Iteration 94100 (3.12612 iter/s, 3.19886s/10 iters), loss = 7.20117
I0522 23:46:42.451248 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20117 (* 1 = 7.20117 loss)
I0522 23:46:42.459295 35003 sgd_solver.cpp:112] Iteration 94100, lr = 0.01
I0522 23:46:45.966837 35003 solver.cpp:239] Iteration 94110 (2.8446 iter/s, 3.51543s/10 iters), loss = 6.48334
I0522 23:46:45.966902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48334 (* 1 = 6.48334 loss)
I0522 23:46:45.979171 35003 sgd_solver.cpp:112] Iteration 94110, lr = 0.01
I0522 23:46:49.579478 35003 solver.cpp:239] Iteration 94120 (2.76822 iter/s, 3.61243s/10 iters), loss = 6.85338
I0522 23:46:49.579524 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85338 (* 1 = 6.85338 loss)
I0522 23:46:50.288671 35003 sgd_solver.cpp:112] Iteration 94120, lr = 0.01
I0522 23:46:53.815135 35003 solver.cpp:239] Iteration 94130 (2.36103 iter/s, 4.23543s/10 iters), loss = 6.72793
I0522 23:46:53.815177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72793 (* 1 = 6.72793 loss)
I0522 23:46:54.293033 35003 sgd_solver.cpp:112] Iteration 94130, lr = 0.01
I0522 23:46:58.443327 35003 solver.cpp:239] Iteration 94140 (2.16078 iter/s, 4.62795s/10 iters), loss = 7.22074
I0522 23:46:58.443392 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22074 (* 1 = 7.22074 loss)
I0522 23:46:58.463021 35003 sgd_solver.cpp:112] Iteration 94140, lr = 0.01
I0522 23:47:01.175202 35003 solver.cpp:239] Iteration 94150 (3.66073 iter/s, 2.7317s/10 iters), loss = 7.69848
I0522 23:47:01.175247 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69848 (* 1 = 7.69848 loss)
I0522 23:47:01.186802 35003 sgd_solver.cpp:112] Iteration 94150, lr = 0.01
I0522 23:47:06.822460 35003 solver.cpp:239] Iteration 94160 (1.77086 iter/s, 5.64697s/10 iters), loss = 7.52115
I0522 23:47:06.822521 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52115 (* 1 = 7.52115 loss)
I0522 23:47:06.828557 35003 sgd_solver.cpp:112] Iteration 94160, lr = 0.01
I0522 23:47:09.631153 35003 solver.cpp:239] Iteration 94170 (3.56061 iter/s, 2.80851s/10 iters), loss = 7.4371
I0522 23:47:09.631330 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4371 (* 1 = 7.4371 loss)
I0522 23:47:10.243158 35003 sgd_solver.cpp:112] Iteration 94170, lr = 0.01
I0522 23:47:13.772344 35003 solver.cpp:239] Iteration 94180 (2.41496 iter/s, 4.14085s/10 iters), loss = 7.77899
I0522 23:47:13.772385 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77899 (* 1 = 7.77899 loss)
I0522 23:47:13.780228 35003 sgd_solver.cpp:112] Iteration 94180, lr = 0.01
I0522 23:47:18.315994 35003 solver.cpp:239] Iteration 94190 (2.201 iter/s, 4.5434s/10 iters), loss = 6.99034
I0522 23:47:18.316045 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99034 (* 1 = 6.99034 loss)
I0522 23:47:18.323437 35003 sgd_solver.cpp:112] Iteration 94190, lr = 0.01
I0522 23:47:20.337533 35003 solver.cpp:239] Iteration 94200 (4.94707 iter/s, 2.0214s/10 iters), loss = 6.41235
I0522 23:47:20.337572 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41235 (* 1 = 6.41235 loss)
I0522 23:47:20.340422 35003 sgd_solver.cpp:112] Iteration 94200, lr = 0.01
I0522 23:47:23.223081 35003 solver.cpp:239] Iteration 94210 (3.46575 iter/s, 2.88538s/10 iters), loss = 7.41705
I0522 23:47:23.223125 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41705 (* 1 = 7.41705 loss)
I0522 23:47:23.236114 35003 sgd_solver.cpp:112] Iteration 94210, lr = 0.01
I0522 23:47:26.111752 35003 solver.cpp:239] Iteration 94220 (3.462 iter/s, 2.8885s/10 iters), loss = 7.17445
I0522 23:47:26.111822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17445 (* 1 = 7.17445 loss)
I0522 23:47:26.843641 35003 sgd_solver.cpp:112] Iteration 94220, lr = 0.01
I0522 23:47:30.778426 35003 solver.cpp:239] Iteration 94230 (2.14298 iter/s, 4.66641s/10 iters), loss = 7.09393
I0522 23:47:30.778481 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09393 (* 1 = 7.09393 loss)
I0522 23:47:30.785323 35003 sgd_solver.cpp:112] Iteration 94230, lr = 0.01
I0522 23:47:33.617507 35003 solver.cpp:239] Iteration 94240 (3.52252 iter/s, 2.83888s/10 iters), loss = 6.72244
I0522 23:47:33.617559 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72244 (* 1 = 6.72244 loss)
I0522 23:47:34.324190 35003 sgd_solver.cpp:112] Iteration 94240, lr = 0.01
I0522 23:47:36.409034 35003 solver.cpp:239] Iteration 94250 (3.58249 iter/s, 2.79136s/10 iters), loss = 6.20058
I0522 23:47:36.409076 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20058 (* 1 = 6.20058 loss)
I0522 23:47:36.422467 35003 sgd_solver.cpp:112] Iteration 94250, lr = 0.01
I0522 23:47:38.454790 35003 solver.cpp:239] Iteration 94260 (4.8885 iter/s, 2.04562s/10 iters), loss = 7.93951
I0522 23:47:38.454849 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93951 (* 1 = 7.93951 loss)
I0522 23:47:38.468068 35003 sgd_solver.cpp:112] Iteration 94260, lr = 0.01
I0522 23:47:43.479926 35003 solver.cpp:239] Iteration 94270 (1.99011 iter/s, 5.02486s/10 iters), loss = 5.93294
I0522 23:47:43.480228 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93294 (* 1 = 5.93294 loss)
I0522 23:47:43.485430 35003 sgd_solver.cpp:112] Iteration 94270, lr = 0.01
I0522 23:47:47.187839 35003 solver.cpp:239] Iteration 94280 (2.69729 iter/s, 3.70742s/10 iters), loss = 7.44385
I0522 23:47:47.187883 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44385 (* 1 = 7.44385 loss)
I0522 23:47:47.923110 35003 sgd_solver.cpp:112] Iteration 94280, lr = 0.01
I0522 23:47:52.136734 35003 solver.cpp:239] Iteration 94290 (2.02075 iter/s, 4.94865s/10 iters), loss = 7.19414
I0522 23:47:52.136785 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19414 (* 1 = 7.19414 loss)
I0522 23:47:52.189298 35003 sgd_solver.cpp:112] Iteration 94290, lr = 0.01
I0522 23:47:55.109359 35003 solver.cpp:239] Iteration 94300 (3.36423 iter/s, 2.97245s/10 iters), loss = 8.13638
I0522 23:47:55.109402 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.13638 (* 1 = 8.13638 loss)
I0522 23:47:55.843061 35003 sgd_solver.cpp:112] Iteration 94300, lr = 0.01
I0522 23:47:58.653087 35003 solver.cpp:239] Iteration 94310 (2.82204 iter/s, 3.54354s/10 iters), loss = 7.1865
I0522 23:47:58.653129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1865 (* 1 = 7.1865 loss)
I0522 23:47:58.666836 35003 sgd_solver.cpp:112] Iteration 94310, lr = 0.01
I0522 23:48:01.804150 35003 solver.cpp:239] Iteration 94320 (3.17371 iter/s, 3.15089s/10 iters), loss = 7.68202
I0522 23:48:01.804201 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68202 (* 1 = 7.68202 loss)
I0522 23:48:02.470465 35003 sgd_solver.cpp:112] Iteration 94320, lr = 0.01
I0522 23:48:05.266163 35003 solver.cpp:239] Iteration 94330 (2.88866 iter/s, 3.46181s/10 iters), loss = 7.19853
I0522 23:48:05.266227 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19853 (* 1 = 7.19853 loss)
I0522 23:48:05.961390 35003 sgd_solver.cpp:112] Iteration 94330, lr = 0.01
I0522 23:48:09.820324 35003 solver.cpp:239] Iteration 94340 (2.19591 iter/s, 4.55392s/10 iters), loss = 7.07547
I0522 23:48:09.820369 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07547 (* 1 = 7.07547 loss)
I0522 23:48:09.824357 35003 sgd_solver.cpp:112] Iteration 94340, lr = 0.01
I0522 23:48:13.494669 35003 solver.cpp:239] Iteration 94350 (2.72172 iter/s, 3.67415s/10 iters), loss = 7.21289
I0522 23:48:13.495015 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21289 (* 1 = 7.21289 loss)
I0522 23:48:14.223279 35003 sgd_solver.cpp:112] Iteration 94350, lr = 0.01
I0522 23:48:17.124781 35003 solver.cpp:239] Iteration 94360 (2.75509 iter/s, 3.62964s/10 iters), loss = 6.65096
I0522 23:48:17.124836 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65096 (* 1 = 6.65096 loss)
I0522 23:48:17.130652 35003 sgd_solver.cpp:112] Iteration 94360, lr = 0.01
I0522 23:48:19.731799 35003 solver.cpp:239] Iteration 94370 (3.83605 iter/s, 2.60685s/10 iters), loss = 7.13201
I0522 23:48:19.731858 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13201 (* 1 = 7.13201 loss)
I0522 23:48:19.749467 35003 sgd_solver.cpp:112] Iteration 94370, lr = 0.01
I0522 23:48:22.702208 35003 solver.cpp:239] Iteration 94380 (3.36675 iter/s, 2.97023s/10 iters), loss = 7.12214
I0522 23:48:22.702260 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12214 (* 1 = 7.12214 loss)
I0522 23:48:22.715534 35003 sgd_solver.cpp:112] Iteration 94380, lr = 0.01
I0522 23:48:26.267313 35003 solver.cpp:239] Iteration 94390 (2.80513 iter/s, 3.5649s/10 iters), loss = 7.60362
I0522 23:48:26.267364 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60362 (* 1 = 7.60362 loss)
I0522 23:48:26.982748 35003 sgd_solver.cpp:112] Iteration 94390, lr = 0.01
I0522 23:48:29.644459 35003 solver.cpp:239] Iteration 94400 (2.96125 iter/s, 3.37696s/10 iters), loss = 7.65625
I0522 23:48:29.644502 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65625 (* 1 = 7.65625 loss)
I0522 23:48:30.359997 35003 sgd_solver.cpp:112] Iteration 94400, lr = 0.01
I0522 23:48:32.333719 35003 solver.cpp:239] Iteration 94410 (3.71872 iter/s, 2.6891s/10 iters), loss = 7.10829
I0522 23:48:32.333757 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10829 (* 1 = 7.10829 loss)
I0522 23:48:32.347168 35003 sgd_solver.cpp:112] Iteration 94410, lr = 0.01
I0522 23:48:35.924968 35003 solver.cpp:239] Iteration 94420 (2.78469 iter/s, 3.59106s/10 iters), loss = 7.57477
I0522 23:48:35.925021 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57477 (* 1 = 7.57477 loss)
I0522 23:48:36.074595 35003 sgd_solver.cpp:112] Iteration 94420, lr = 0.01
I0522 23:48:40.493546 35003 solver.cpp:239] Iteration 94430 (2.18898 iter/s, 4.56834s/10 iters), loss = 7.78346
I0522 23:48:40.493587 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78346 (* 1 = 7.78346 loss)
I0522 23:48:40.506687 35003 sgd_solver.cpp:112] Iteration 94430, lr = 0.01
I0522 23:48:45.009822 35003 solver.cpp:239] Iteration 94440 (2.21433 iter/s, 4.51605s/10 iters), loss = 8.15825
I0522 23:48:45.009963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.15825 (* 1 = 8.15825 loss)
I0522 23:48:45.744269 35003 sgd_solver.cpp:112] Iteration 94440, lr = 0.01
I0522 23:48:48.359381 35003 solver.cpp:239] Iteration 94450 (2.98572 iter/s, 3.34927s/10 iters), loss = 7.36833
I0522 23:48:48.359438 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36833 (* 1 = 7.36833 loss)
I0522 23:48:48.366078 35003 sgd_solver.cpp:112] Iteration 94450, lr = 0.01
I0522 23:48:52.095825 35003 solver.cpp:239] Iteration 94460 (2.67649 iter/s, 3.73623s/10 iters), loss = 7.63626
I0522 23:48:52.095873 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63626 (* 1 = 7.63626 loss)
I0522 23:48:52.830196 35003 sgd_solver.cpp:112] Iteration 94460, lr = 0.01
I0522 23:48:54.894248 35003 solver.cpp:239] Iteration 94470 (3.57366 iter/s, 2.79825s/10 iters), loss = 7.73682
I0522 23:48:54.894294 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73682 (* 1 = 7.73682 loss)
I0522 23:48:54.921741 35003 sgd_solver.cpp:112] Iteration 94470, lr = 0.01
I0522 23:48:59.148233 35003 solver.cpp:239] Iteration 94480 (2.35086 iter/s, 4.25377s/10 iters), loss = 7.56219
I0522 23:48:59.148279 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56219 (* 1 = 7.56219 loss)
I0522 23:48:59.154716 35003 sgd_solver.cpp:112] Iteration 94480, lr = 0.01
I0522 23:49:03.502519 35003 solver.cpp:239] Iteration 94490 (2.29671 iter/s, 4.35405s/10 iters), loss = 6.69644
I0522 23:49:03.502575 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69644 (* 1 = 6.69644 loss)
I0522 23:49:03.979493 35003 sgd_solver.cpp:112] Iteration 94490, lr = 0.01
I0522 23:49:05.341037 35003 solver.cpp:239] Iteration 94500 (5.43958 iter/s, 1.83838s/10 iters), loss = 6.83835
I0522 23:49:05.341086 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83835 (* 1 = 6.83835 loss)
I0522 23:49:05.406293 35003 sgd_solver.cpp:112] Iteration 94500, lr = 0.01
I0522 23:49:08.204861 35003 solver.cpp:239] Iteration 94510 (3.49204 iter/s, 2.86365s/10 iters), loss = 6.74469
I0522 23:49:08.204901 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74469 (* 1 = 6.74469 loss)
I0522 23:49:08.218530 35003 sgd_solver.cpp:112] Iteration 94510, lr = 0.01
I0522 23:49:10.659582 35003 solver.cpp:239] Iteration 94520 (4.07405 iter/s, 2.45456s/10 iters), loss = 7.98369
I0522 23:49:10.659653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98369 (* 1 = 7.98369 loss)
I0522 23:49:10.673382 35003 sgd_solver.cpp:112] Iteration 94520, lr = 0.01
I0522 23:49:14.000838 35003 solver.cpp:239] Iteration 94530 (2.99307 iter/s, 3.34105s/10 iters), loss = 7.20436
I0522 23:49:14.000876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20436 (* 1 = 7.20436 loss)
I0522 23:49:14.017582 35003 sgd_solver.cpp:112] Iteration 94530, lr = 0.01
I0522 23:49:19.152376 35003 solver.cpp:239] Iteration 94540 (1.94126 iter/s, 5.15129s/10 iters), loss = 8.08232
I0522 23:49:19.152537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08232 (* 1 = 8.08232 loss)
I0522 23:49:19.157935 35003 sgd_solver.cpp:112] Iteration 94540, lr = 0.01
I0522 23:49:21.996330 35003 solver.cpp:239] Iteration 94550 (3.51658 iter/s, 2.84367s/10 iters), loss = 6.70044
I0522 23:49:21.996372 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70044 (* 1 = 6.70044 loss)
I0522 23:49:22.002408 35003 sgd_solver.cpp:112] Iteration 94550, lr = 0.01
I0522 23:49:24.803946 35003 solver.cpp:239] Iteration 94560 (3.56195 iter/s, 2.80745s/10 iters), loss = 7.03673
I0522 23:49:24.803997 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03673 (* 1 = 7.03673 loss)
I0522 23:49:25.544867 35003 sgd_solver.cpp:112] Iteration 94560, lr = 0.01
I0522 23:49:28.829665 35003 solver.cpp:239] Iteration 94570 (2.48416 iter/s, 4.0255s/10 iters), loss = 7.31103
I0522 23:49:28.829720 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31103 (* 1 = 7.31103 loss)
I0522 23:49:29.560881 35003 sgd_solver.cpp:112] Iteration 94570, lr = 0.01
I0522 23:49:32.356901 35003 solver.cpp:239] Iteration 94580 (2.83524 iter/s, 3.52703s/10 iters), loss = 7.37819
I0522 23:49:32.356952 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37819 (* 1 = 7.37819 loss)
I0522 23:49:32.370486 35003 sgd_solver.cpp:112] Iteration 94580, lr = 0.01
I0522 23:49:36.488494 35003 solver.cpp:239] Iteration 94590 (2.4205 iter/s, 4.13137s/10 iters), loss = 6.46781
I0522 23:49:36.488548 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46781 (* 1 = 6.46781 loss)
I0522 23:49:36.501423 35003 sgd_solver.cpp:112] Iteration 94590, lr = 0.01
I0522 23:49:39.278198 35003 solver.cpp:239] Iteration 94600 (3.58484 iter/s, 2.78953s/10 iters), loss = 8.60969
I0522 23:49:39.278245 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.60969 (* 1 = 8.60969 loss)
I0522 23:49:39.288841 35003 sgd_solver.cpp:112] Iteration 94600, lr = 0.01
I0522 23:49:41.009476 35003 solver.cpp:239] Iteration 94610 (5.7765 iter/s, 1.73115s/10 iters), loss = 9.12776
I0522 23:49:41.009519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 9.12776 (* 1 = 9.12776 loss)
I0522 23:49:41.724690 35003 sgd_solver.cpp:112] Iteration 94610, lr = 0.01
I0522 23:49:46.021852 35003 solver.cpp:239] Iteration 94620 (1.99517 iter/s, 5.0121s/10 iters), loss = 8.17188
I0522 23:49:46.021900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17188 (* 1 = 8.17188 loss)
I0522 23:49:46.035277 35003 sgd_solver.cpp:112] Iteration 94620, lr = 0.01
I0522 23:49:50.378468 35003 solver.cpp:239] Iteration 94630 (2.29548 iter/s, 4.35639s/10 iters), loss = 6.554
I0522 23:49:50.378809 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.554 (* 1 = 6.554 loss)
I0522 23:49:50.397018 35003 sgd_solver.cpp:112] Iteration 94630, lr = 0.01
I0522 23:49:53.098219 35003 solver.cpp:239] Iteration 94640 (3.67739 iter/s, 2.71932s/10 iters), loss = 7.96287
I0522 23:49:53.098268 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96287 (* 1 = 7.96287 loss)
I0522 23:49:53.742558 35003 sgd_solver.cpp:112] Iteration 94640, lr = 0.01
I0522 23:49:56.527338 35003 solver.cpp:239] Iteration 94650 (2.91637 iter/s, 3.42892s/10 iters), loss = 6.93915
I0522 23:49:56.527386 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93915 (* 1 = 6.93915 loss)
I0522 23:49:56.544040 35003 sgd_solver.cpp:112] Iteration 94650, lr = 0.01
I0522 23:50:01.403172 35003 solver.cpp:239] Iteration 94660 (2.05104 iter/s, 4.87557s/10 iters), loss = 7.94472
I0522 23:50:01.403223 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94472 (* 1 = 7.94472 loss)
I0522 23:50:01.410816 35003 sgd_solver.cpp:112] Iteration 94660, lr = 0.01
I0522 23:50:05.615061 35003 solver.cpp:239] Iteration 94670 (2.37436 iter/s, 4.21167s/10 iters), loss = 7.17389
I0522 23:50:05.615113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17389 (* 1 = 7.17389 loss)
I0522 23:50:05.665304 35003 sgd_solver.cpp:112] Iteration 94670, lr = 0.01
I0522 23:50:11.077931 35003 solver.cpp:239] Iteration 94680 (1.83064 iter/s, 5.46258s/10 iters), loss = 7.31352
I0522 23:50:11.077994 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31352 (* 1 = 7.31352 loss)
I0522 23:50:11.085861 35003 sgd_solver.cpp:112] Iteration 94680, lr = 0.01
I0522 23:50:14.026335 35003 solver.cpp:239] Iteration 94690 (3.39188 iter/s, 2.94822s/10 iters), loss = 6.47414
I0522 23:50:14.026382 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47414 (* 1 = 6.47414 loss)
I0522 23:50:14.039470 35003 sgd_solver.cpp:112] Iteration 94690, lr = 0.01
I0522 23:50:18.384889 35003 solver.cpp:239] Iteration 94700 (2.29446 iter/s, 4.35833s/10 iters), loss = 7.53213
I0522 23:50:18.384943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53213 (* 1 = 7.53213 loss)
I0522 23:50:18.388561 35003 sgd_solver.cpp:112] Iteration 94700, lr = 0.01
I0522 23:50:23.366853 35003 solver.cpp:239] Iteration 94710 (2.00735 iter/s, 4.9817s/10 iters), loss = 7.56459
I0522 23:50:23.367086 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56459 (* 1 = 7.56459 loss)
I0522 23:50:23.503304 35003 sgd_solver.cpp:112] Iteration 94710, lr = 0.01
I0522 23:50:27.001693 35003 solver.cpp:239] Iteration 94720 (2.75143 iter/s, 3.63447s/10 iters), loss = 6.79488
I0522 23:50:27.001735 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79488 (* 1 = 6.79488 loss)
I0522 23:50:27.014900 35003 sgd_solver.cpp:112] Iteration 94720, lr = 0.01
I0522 23:50:29.861567 35003 solver.cpp:239] Iteration 94730 (3.49686 iter/s, 2.85971s/10 iters), loss = 6.5631
I0522 23:50:29.861608 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5631 (* 1 = 6.5631 loss)
I0522 23:50:29.869001 35003 sgd_solver.cpp:112] Iteration 94730, lr = 0.01
I0522 23:50:32.633097 35003 solver.cpp:239] Iteration 94740 (3.60833 iter/s, 2.77136s/10 iters), loss = 6.82522
I0522 23:50:32.633137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82522 (* 1 = 6.82522 loss)
I0522 23:50:32.636874 35003 sgd_solver.cpp:112] Iteration 94740, lr = 0.01
I0522 23:50:34.952522 35003 solver.cpp:239] Iteration 94750 (4.31173 iter/s, 2.31925s/10 iters), loss = 7.45586
I0522 23:50:34.952570 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45586 (* 1 = 7.45586 loss)
I0522 23:50:34.964998 35003 sgd_solver.cpp:112] Iteration 94750, lr = 0.01
I0522 23:50:38.524282 35003 solver.cpp:239] Iteration 94760 (2.7999 iter/s, 3.57156s/10 iters), loss = 7.1645
I0522 23:50:38.524328 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1645 (* 1 = 7.1645 loss)
I0522 23:50:38.531824 35003 sgd_solver.cpp:112] Iteration 94760, lr = 0.01
I0522 23:50:42.008517 35003 solver.cpp:239] Iteration 94770 (2.87024 iter/s, 3.48403s/10 iters), loss = 7.58599
I0522 23:50:42.008569 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58599 (* 1 = 7.58599 loss)
I0522 23:50:42.710904 35003 sgd_solver.cpp:112] Iteration 94770, lr = 0.01
I0522 23:50:45.533118 35003 solver.cpp:239] Iteration 94780 (2.83737 iter/s, 3.5244s/10 iters), loss = 7.06615
I0522 23:50:45.533192 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06615 (* 1 = 7.06615 loss)
I0522 23:50:46.220504 35003 sgd_solver.cpp:112] Iteration 94780, lr = 0.01
I0522 23:50:50.604828 35003 solver.cpp:239] Iteration 94790 (1.97183 iter/s, 5.07143s/10 iters), loss = 7.81157
I0522 23:50:50.604876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81157 (* 1 = 7.81157 loss)
I0522 23:50:50.610709 35003 sgd_solver.cpp:112] Iteration 94790, lr = 0.01
I0522 23:50:55.001683 35003 solver.cpp:239] Iteration 94800 (2.27447 iter/s, 4.39663s/10 iters), loss = 8.18149
I0522 23:50:55.001931 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.18149 (* 1 = 8.18149 loss)
I0522 23:50:55.015383 35003 sgd_solver.cpp:112] Iteration 94800, lr = 0.01
I0522 23:51:00.055229 35003 solver.cpp:239] Iteration 94810 (1.97898 iter/s, 5.05312s/10 iters), loss = 6.805
I0522 23:51:00.055282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.805 (* 1 = 6.805 loss)
I0522 23:51:00.660223 35003 sgd_solver.cpp:112] Iteration 94810, lr = 0.01
I0522 23:51:03.284979 35003 solver.cpp:239] Iteration 94820 (3.0964 iter/s, 3.22956s/10 iters), loss = 7.39561
I0522 23:51:03.285022 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39561 (* 1 = 7.39561 loss)
I0522 23:51:04.026906 35003 sgd_solver.cpp:112] Iteration 94820, lr = 0.01
I0522 23:51:07.688222 35003 solver.cpp:239] Iteration 94830 (2.27117 iter/s, 4.40302s/10 iters), loss = 7.08917
I0522 23:51:07.688268 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08917 (* 1 = 7.08917 loss)
I0522 23:51:08.384172 35003 sgd_solver.cpp:112] Iteration 94830, lr = 0.01
I0522 23:51:12.204440 35003 solver.cpp:239] Iteration 94840 (2.21436 iter/s, 4.51599s/10 iters), loss = 6.82759
I0522 23:51:12.204491 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82759 (* 1 = 6.82759 loss)
I0522 23:51:12.218540 35003 sgd_solver.cpp:112] Iteration 94840, lr = 0.01
I0522 23:51:15.402765 35003 solver.cpp:239] Iteration 94850 (3.12682 iter/s, 3.19814s/10 iters), loss = 8.50422
I0522 23:51:15.402822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.50422 (* 1 = 8.50422 loss)
I0522 23:51:15.420274 35003 sgd_solver.cpp:112] Iteration 94850, lr = 0.01
I0522 23:51:17.429697 35003 solver.cpp:239] Iteration 94860 (4.93392 iter/s, 2.02679s/10 iters), loss = 6.93606
I0522 23:51:17.429744 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93606 (* 1 = 6.93606 loss)
I0522 23:51:18.164744 35003 sgd_solver.cpp:112] Iteration 94860, lr = 0.01
I0522 23:51:20.844040 35003 solver.cpp:239] Iteration 94870 (2.929 iter/s, 3.41413s/10 iters), loss = 8.05852
I0522 23:51:20.844106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05852 (* 1 = 8.05852 loss)
I0522 23:51:20.857179 35003 sgd_solver.cpp:112] Iteration 94870, lr = 0.01
I0522 23:51:25.701751 35003 solver.cpp:239] Iteration 94880 (2.05869 iter/s, 4.85745s/10 iters), loss = 7.38845
I0522 23:51:25.701989 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38845 (* 1 = 7.38845 loss)
I0522 23:51:25.709483 35003 sgd_solver.cpp:112] Iteration 94880, lr = 0.01
I0522 23:51:27.722919 35003 solver.cpp:239] Iteration 94890 (4.94837 iter/s, 2.02087s/10 iters), loss = 7.15952
I0522 23:51:27.722976 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15952 (* 1 = 7.15952 loss)
I0522 23:51:28.410820 35003 sgd_solver.cpp:112] Iteration 94890, lr = 0.01
I0522 23:51:30.637550 35003 solver.cpp:239] Iteration 94900 (3.43117 iter/s, 2.91446s/10 iters), loss = 7.97894
I0522 23:51:30.637598 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97894 (* 1 = 7.97894 loss)
I0522 23:51:30.644486 35003 sgd_solver.cpp:112] Iteration 94900, lr = 0.01
I0522 23:51:33.254671 35003 solver.cpp:239] Iteration 94910 (3.82122 iter/s, 2.61696s/10 iters), loss = 7.08204
I0522 23:51:33.254740 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08204 (* 1 = 7.08204 loss)
I0522 23:51:33.271087 35003 sgd_solver.cpp:112] Iteration 94910, lr = 0.01
I0522 23:51:36.464536 35003 solver.cpp:239] Iteration 94920 (3.1156 iter/s, 3.20966s/10 iters), loss = 7.8229
I0522 23:51:36.464586 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8229 (* 1 = 7.8229 loss)
I0522 23:51:36.482964 35003 sgd_solver.cpp:112] Iteration 94920, lr = 0.01
I0522 23:51:40.175420 35003 solver.cpp:239] Iteration 94930 (2.69492 iter/s, 3.71068s/10 iters), loss = 6.77561
I0522 23:51:40.175462 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77561 (* 1 = 6.77561 loss)
I0522 23:51:40.188254 35003 sgd_solver.cpp:112] Iteration 94930, lr = 0.01
I0522 23:51:43.641613 35003 solver.cpp:239] Iteration 94940 (2.88516 iter/s, 3.46601s/10 iters), loss = 6.45982
I0522 23:51:43.641651 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45982 (* 1 = 6.45982 loss)
I0522 23:51:43.647145 35003 sgd_solver.cpp:112] Iteration 94940, lr = 0.01
I0522 23:51:47.740020 35003 solver.cpp:239] Iteration 94950 (2.4401 iter/s, 4.0982s/10 iters), loss = 5.55677
I0522 23:51:47.740064 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.55677 (* 1 = 5.55677 loss)
I0522 23:51:48.083976 35003 sgd_solver.cpp:112] Iteration 94950, lr = 0.01
I0522 23:51:50.462523 35003 solver.cpp:239] Iteration 94960 (3.67331 iter/s, 2.72234s/10 iters), loss = 5.83628
I0522 23:51:50.462563 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.83628 (* 1 = 5.83628 loss)
I0522 23:51:50.478905 35003 sgd_solver.cpp:112] Iteration 94960, lr = 0.01
I0522 23:51:55.521242 35003 solver.cpp:239] Iteration 94970 (1.97688 iter/s, 5.05847s/10 iters), loss = 6.97611
I0522 23:51:55.521291 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97611 (* 1 = 6.97611 loss)
I0522 23:51:56.136389 35003 sgd_solver.cpp:112] Iteration 94970, lr = 0.01
I0522 23:51:59.363883 35003 solver.cpp:239] Iteration 94980 (2.60252 iter/s, 3.84243s/10 iters), loss = 6.9892
I0522 23:51:59.363937 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9892 (* 1 = 6.9892 loss)
I0522 23:51:59.372182 35003 sgd_solver.cpp:112] Iteration 94980, lr = 0.01
I0522 23:52:01.771816 35003 solver.cpp:239] Iteration 94990 (4.15321 iter/s, 2.40777s/10 iters), loss = 7.58951
I0522 23:52:01.771863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58951 (* 1 = 7.58951 loss)
I0522 23:52:01.779410 35003 sgd_solver.cpp:112] Iteration 94990, lr = 0.01
I0522 23:52:05.149755 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_95000.caffemodel
I0522 23:52:05.654412 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_95000.solverstate
I0522 23:52:05.782629 35003 solver.cpp:239] Iteration 95000 (2.49339 iter/s, 4.0106s/10 iters), loss = 7.78013
I0522 23:52:05.782667 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78013 (* 1 = 7.78013 loss)
I0522 23:52:05.790587 35003 sgd_solver.cpp:112] Iteration 95000, lr = 0.01
I0522 23:52:09.603853 35003 solver.cpp:239] Iteration 95010 (2.6171 iter/s, 3.82102s/10 iters), loss = 6.50274
I0522 23:52:09.603904 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50274 (* 1 = 6.50274 loss)
I0522 23:52:09.856292 35003 sgd_solver.cpp:112] Iteration 95010, lr = 0.01
I0522 23:52:13.453007 35003 solver.cpp:239] Iteration 95020 (2.59811 iter/s, 3.84895s/10 iters), loss = 6.78207
I0522 23:52:13.453050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78207 (* 1 = 6.78207 loss)
I0522 23:52:13.473584 35003 sgd_solver.cpp:112] Iteration 95020, lr = 0.01
I0522 23:52:16.038823 35003 solver.cpp:239] Iteration 95030 (3.86755 iter/s, 2.58562s/10 iters), loss = 7.41005
I0522 23:52:16.038898 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41005 (* 1 = 7.41005 loss)
I0522 23:52:16.376760 35003 sgd_solver.cpp:112] Iteration 95030, lr = 0.01
I0522 23:52:19.987277 35003 solver.cpp:239] Iteration 95040 (2.53279 iter/s, 3.94821s/10 iters), loss = 6.49928
I0522 23:52:19.987340 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49928 (* 1 = 6.49928 loss)
I0522 23:52:19.996055 35003 sgd_solver.cpp:112] Iteration 95040, lr = 0.01
I0522 23:52:25.016052 35003 solver.cpp:239] Iteration 95050 (1.98866 iter/s, 5.02851s/10 iters), loss = 6.59598
I0522 23:52:25.016120 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59598 (* 1 = 6.59598 loss)
I0522 23:52:25.755916 35003 sgd_solver.cpp:112] Iteration 95050, lr = 0.01
I0522 23:52:28.260653 35003 solver.cpp:239] Iteration 95060 (3.08224 iter/s, 3.24439s/10 iters), loss = 6.34985
I0522 23:52:28.260937 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34985 (* 1 = 6.34985 loss)
I0522 23:52:28.263108 35003 sgd_solver.cpp:112] Iteration 95060, lr = 0.01
I0522 23:52:32.515691 35003 solver.cpp:239] Iteration 95070 (2.3504 iter/s, 4.2546s/10 iters), loss = 7.78921
I0522 23:52:32.515739 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78921 (* 1 = 7.78921 loss)
I0522 23:52:33.205103 35003 sgd_solver.cpp:112] Iteration 95070, lr = 0.01
I0522 23:52:37.007823 35003 solver.cpp:239] Iteration 95080 (2.22623 iter/s, 4.4919s/10 iters), loss = 6.23344
I0522 23:52:37.007863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23344 (* 1 = 6.23344 loss)
I0522 23:52:37.697407 35003 sgd_solver.cpp:112] Iteration 95080, lr = 0.01
I0522 23:52:41.983574 35003 solver.cpp:239] Iteration 95090 (2.00985 iter/s, 4.9755s/10 iters), loss = 7.58352
I0522 23:52:41.983638 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58352 (* 1 = 7.58352 loss)
I0522 23:52:42.588548 35003 sgd_solver.cpp:112] Iteration 95090, lr = 0.01
I0522 23:52:47.190241 35003 solver.cpp:239] Iteration 95100 (1.92072 iter/s, 5.20639s/10 iters), loss = 7.44538
I0522 23:52:47.190291 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44538 (* 1 = 7.44538 loss)
I0522 23:52:47.199276 35003 sgd_solver.cpp:112] Iteration 95100, lr = 0.01
I0522 23:52:50.026204 35003 solver.cpp:239] Iteration 95110 (3.52635 iter/s, 2.83579s/10 iters), loss = 7.4493
I0522 23:52:50.026247 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4493 (* 1 = 7.4493 loss)
I0522 23:52:50.767202 35003 sgd_solver.cpp:112] Iteration 95110, lr = 0.01
I0522 23:52:54.113138 35003 solver.cpp:239] Iteration 95120 (2.44695 iter/s, 4.08672s/10 iters), loss = 6.91827
I0522 23:52:54.113190 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91827 (* 1 = 6.91827 loss)
I0522 23:52:54.120800 35003 sgd_solver.cpp:112] Iteration 95120, lr = 0.01
I0522 23:52:56.355085 35003 solver.cpp:239] Iteration 95130 (4.46072 iter/s, 2.24179s/10 iters), loss = 6.84203
I0522 23:52:56.355152 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84203 (* 1 = 6.84203 loss)
I0522 23:52:57.086485 35003 sgd_solver.cpp:112] Iteration 95130, lr = 0.01
I0522 23:53:01.740427 35003 solver.cpp:239] Iteration 95140 (1.85699 iter/s, 5.38506s/10 iters), loss = 8.24025
I0522 23:53:01.740734 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.24025 (* 1 = 8.24025 loss)
I0522 23:53:01.757139 35003 sgd_solver.cpp:112] Iteration 95140, lr = 0.01
I0522 23:53:05.383486 35003 solver.cpp:239] Iteration 95150 (2.74525 iter/s, 3.64265s/10 iters), loss = 7.84803
I0522 23:53:05.383533 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84803 (* 1 = 7.84803 loss)
I0522 23:53:05.396510 35003 sgd_solver.cpp:112] Iteration 95150, lr = 0.01
I0522 23:53:07.505830 35003 solver.cpp:239] Iteration 95160 (4.71208 iter/s, 2.1222s/10 iters), loss = 6.32753
I0522 23:53:07.505870 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32753 (* 1 = 6.32753 loss)
I0522 23:53:07.518765 35003 sgd_solver.cpp:112] Iteration 95160, lr = 0.01
I0522 23:53:11.123176 35003 solver.cpp:239] Iteration 95170 (2.7646 iter/s, 3.61716s/10 iters), loss = 6.45249
I0522 23:53:11.123220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45249 (* 1 = 6.45249 loss)
I0522 23:53:11.851691 35003 sgd_solver.cpp:112] Iteration 95170, lr = 0.01
I0522 23:53:15.038594 35003 solver.cpp:239] Iteration 95180 (2.55415 iter/s, 3.9152s/10 iters), loss = 6.98796
I0522 23:53:15.038692 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98796 (* 1 = 6.98796 loss)
I0522 23:53:15.042419 35003 sgd_solver.cpp:112] Iteration 95180, lr = 0.01
I0522 23:53:18.784873 35003 solver.cpp:239] Iteration 95190 (2.66949 iter/s, 3.74604s/10 iters), loss = 7.98443
I0522 23:53:18.784919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98443 (* 1 = 7.98443 loss)
I0522 23:53:18.798863 35003 sgd_solver.cpp:112] Iteration 95190, lr = 0.01
I0522 23:53:22.696660 35003 solver.cpp:239] Iteration 95200 (2.55651 iter/s, 3.91158s/10 iters), loss = 7.30146
I0522 23:53:22.696710 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30146 (* 1 = 7.30146 loss)
I0522 23:53:22.702749 35003 sgd_solver.cpp:112] Iteration 95200, lr = 0.01
I0522 23:53:26.297938 35003 solver.cpp:239] Iteration 95210 (2.77694 iter/s, 3.60108s/10 iters), loss = 7.34915
I0522 23:53:26.297982 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34915 (* 1 = 7.34915 loss)
I0522 23:53:27.032950 35003 sgd_solver.cpp:112] Iteration 95210, lr = 0.01
I0522 23:53:30.655984 35003 solver.cpp:239] Iteration 95220 (2.29472 iter/s, 4.35782s/10 iters), loss = 6.89508
I0522 23:53:30.656033 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89508 (* 1 = 6.89508 loss)
I0522 23:53:30.664083 35003 sgd_solver.cpp:112] Iteration 95220, lr = 0.01
I0522 23:53:34.366523 35003 solver.cpp:239] Iteration 95230 (2.69518 iter/s, 3.71033s/10 iters), loss = 6.55521
I0522 23:53:34.366773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55521 (* 1 = 6.55521 loss)
I0522 23:53:34.374356 35003 sgd_solver.cpp:112] Iteration 95230, lr = 0.01
I0522 23:53:37.103556 35003 solver.cpp:239] Iteration 95240 (3.65404 iter/s, 2.7367s/10 iters), loss = 7.13774
I0522 23:53:37.103600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13774 (* 1 = 7.13774 loss)
I0522 23:53:37.113600 35003 sgd_solver.cpp:112] Iteration 95240, lr = 0.01
I0522 23:53:42.030185 35003 solver.cpp:239] Iteration 95250 (2.02989 iter/s, 4.92639s/10 iters), loss = 8.01456
I0522 23:53:42.030227 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01456 (* 1 = 8.01456 loss)
I0522 23:53:42.743860 35003 sgd_solver.cpp:112] Iteration 95250, lr = 0.01
I0522 23:53:45.718189 35003 solver.cpp:239] Iteration 95260 (2.71165 iter/s, 3.6878s/10 iters), loss = 7.24571
I0522 23:53:45.718268 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24571 (* 1 = 7.24571 loss)
I0522 23:53:45.722069 35003 sgd_solver.cpp:112] Iteration 95260, lr = 0.01
I0522 23:53:48.637653 35003 solver.cpp:239] Iteration 95270 (3.42551 iter/s, 2.91927s/10 iters), loss = 7.49443
I0522 23:53:48.637706 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49443 (* 1 = 7.49443 loss)
I0522 23:53:49.372081 35003 sgd_solver.cpp:112] Iteration 95270, lr = 0.01
I0522 23:53:53.413312 35003 solver.cpp:239] Iteration 95280 (2.09406 iter/s, 4.77541s/10 iters), loss = 7.41862
I0522 23:53:53.413410 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41862 (* 1 = 7.41862 loss)
I0522 23:53:53.422165 35003 sgd_solver.cpp:112] Iteration 95280, lr = 0.01
I0522 23:53:56.272589 35003 solver.cpp:239] Iteration 95290 (3.49765 iter/s, 2.85906s/10 iters), loss = 7.3244
I0522 23:53:56.272639 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3244 (* 1 = 7.3244 loss)
I0522 23:53:56.286041 35003 sgd_solver.cpp:112] Iteration 95290, lr = 0.01
I0522 23:53:59.055388 35003 solver.cpp:239] Iteration 95300 (3.59373 iter/s, 2.78263s/10 iters), loss = 7.20009
I0522 23:53:59.055445 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20009 (* 1 = 7.20009 loss)
I0522 23:53:59.068979 35003 sgd_solver.cpp:112] Iteration 95300, lr = 0.01
I0522 23:54:03.981926 35003 solver.cpp:239] Iteration 95310 (2.02993 iter/s, 4.92628s/10 iters), loss = 6.43381
I0522 23:54:03.981981 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43381 (* 1 = 6.43381 loss)
I0522 23:54:03.987921 35003 sgd_solver.cpp:112] Iteration 95310, lr = 0.01
I0522 23:54:07.796476 35003 solver.cpp:239] Iteration 95320 (2.62168 iter/s, 3.81434s/10 iters), loss = 8.16206
I0522 23:54:07.796744 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16206 (* 1 = 8.16206 loss)
I0522 23:54:08.526952 35003 sgd_solver.cpp:112] Iteration 95320, lr = 0.01
I0522 23:54:12.075677 35003 solver.cpp:239] Iteration 95330 (2.33712 iter/s, 4.27877s/10 iters), loss = 8.0957
I0522 23:54:12.075738 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0957 (* 1 = 8.0957 loss)
I0522 23:54:12.096583 35003 sgd_solver.cpp:112] Iteration 95330, lr = 0.01
I0522 23:54:15.784080 35003 solver.cpp:239] Iteration 95340 (2.69673 iter/s, 3.70819s/10 iters), loss = 6.10524
I0522 23:54:15.784123 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10524 (* 1 = 6.10524 loss)
I0522 23:54:16.499562 35003 sgd_solver.cpp:112] Iteration 95340, lr = 0.01
I0522 23:54:20.614899 35003 solver.cpp:239] Iteration 95350 (2.07015 iter/s, 4.83057s/10 iters), loss = 6.46964
I0522 23:54:20.614950 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46964 (* 1 = 6.46964 loss)
I0522 23:54:21.238782 35003 sgd_solver.cpp:112] Iteration 95350, lr = 0.01
I0522 23:54:24.830250 35003 solver.cpp:239] Iteration 95360 (2.37241 iter/s, 4.21512s/10 iters), loss = 7.66035
I0522 23:54:24.830312 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66035 (* 1 = 7.66035 loss)
I0522 23:54:24.840972 35003 sgd_solver.cpp:112] Iteration 95360, lr = 0.01
I0522 23:54:28.890664 35003 solver.cpp:239] Iteration 95370 (2.46294 iter/s, 4.06019s/10 iters), loss = 6.55414
I0522 23:54:28.890722 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55414 (* 1 = 6.55414 loss)
I0522 23:54:28.897614 35003 sgd_solver.cpp:112] Iteration 95370, lr = 0.01
I0522 23:54:31.519048 35003 solver.cpp:239] Iteration 95380 (3.80488 iter/s, 2.6282s/10 iters), loss = 7.33219
I0522 23:54:31.519086 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33219 (* 1 = 7.33219 loss)
I0522 23:54:31.532529 35003 sgd_solver.cpp:112] Iteration 95380, lr = 0.01
I0522 23:54:35.617970 35003 solver.cpp:239] Iteration 95390 (2.43979 iter/s, 4.09871s/10 iters), loss = 8.62052
I0522 23:54:35.618021 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.62052 (* 1 = 8.62052 loss)
I0522 23:54:36.341099 35003 sgd_solver.cpp:112] Iteration 95390, lr = 0.01
I0522 23:54:39.859910 35003 solver.cpp:239] Iteration 95400 (2.35754 iter/s, 4.24172s/10 iters), loss = 6.33103
I0522 23:54:39.860121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33103 (* 1 = 6.33103 loss)
I0522 23:54:39.873322 35003 sgd_solver.cpp:112] Iteration 95400, lr = 0.01
I0522 23:54:43.383694 35003 solver.cpp:239] Iteration 95410 (2.83812 iter/s, 3.52345s/10 iters), loss = 6.69214
I0522 23:54:43.383746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69214 (* 1 = 6.69214 loss)
I0522 23:54:44.017151 35003 sgd_solver.cpp:112] Iteration 95410, lr = 0.01
I0522 23:54:47.494719 35003 solver.cpp:239] Iteration 95420 (2.43262 iter/s, 4.11079s/10 iters), loss = 6.66161
I0522 23:54:47.494767 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66161 (* 1 = 6.66161 loss)
I0522 23:54:47.508687 35003 sgd_solver.cpp:112] Iteration 95420, lr = 0.01
I0522 23:54:50.611783 35003 solver.cpp:239] Iteration 95430 (3.20833 iter/s, 3.11688s/10 iters), loss = 7.91546
I0522 23:54:50.611819 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91546 (* 1 = 7.91546 loss)
I0522 23:54:50.624258 35003 sgd_solver.cpp:112] Iteration 95430, lr = 0.01
I0522 23:54:53.530244 35003 solver.cpp:239] Iteration 95440 (3.42665 iter/s, 2.9183s/10 iters), loss = 7.36156
I0522 23:54:53.530287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36156 (* 1 = 7.36156 loss)
I0522 23:54:54.245721 35003 sgd_solver.cpp:112] Iteration 95440, lr = 0.01
I0522 23:54:58.408221 35003 solver.cpp:239] Iteration 95450 (2.05013 iter/s, 4.87774s/10 iters), loss = 8.00884
I0522 23:54:58.408260 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00884 (* 1 = 8.00884 loss)
I0522 23:54:58.421988 35003 sgd_solver.cpp:112] Iteration 95450, lr = 0.01
I0522 23:55:01.405431 35003 solver.cpp:239] Iteration 95460 (3.33662 iter/s, 2.99704s/10 iters), loss = 6.09237
I0522 23:55:01.405478 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09237 (* 1 = 6.09237 loss)
I0522 23:55:01.411559 35003 sgd_solver.cpp:112] Iteration 95460, lr = 0.01
I0522 23:55:06.578582 35003 solver.cpp:239] Iteration 95470 (1.93316 iter/s, 5.17289s/10 iters), loss = 8.74541
I0522 23:55:06.578639 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.74541 (* 1 = 8.74541 loss)
I0522 23:55:07.260576 35003 sgd_solver.cpp:112] Iteration 95470, lr = 0.01
I0522 23:55:08.570801 35003 solver.cpp:239] Iteration 95480 (5.01989 iter/s, 1.99207s/10 iters), loss = 7.72825
I0522 23:55:08.570842 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72825 (* 1 = 7.72825 loss)
I0522 23:55:08.580449 35003 sgd_solver.cpp:112] Iteration 95480, lr = 0.01
I0522 23:55:10.790033 35003 solver.cpp:239] Iteration 95490 (4.50636 iter/s, 2.21908s/10 iters), loss = 6.69813
I0522 23:55:10.790341 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69813 (* 1 = 6.69813 loss)
I0522 23:55:10.802645 35003 sgd_solver.cpp:112] Iteration 95490, lr = 0.01
I0522 23:55:15.787164 35003 solver.cpp:239] Iteration 95500 (2.00133 iter/s, 4.99668s/10 iters), loss = 8.07552
I0522 23:55:15.787222 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.07552 (* 1 = 8.07552 loss)
I0522 23:55:15.812947 35003 sgd_solver.cpp:112] Iteration 95500, lr = 0.01
I0522 23:55:18.859926 35003 solver.cpp:239] Iteration 95510 (3.25461 iter/s, 3.07257s/10 iters), loss = 7.06217
I0522 23:55:18.859979 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06217 (* 1 = 7.06217 loss)
I0522 23:55:18.879143 35003 sgd_solver.cpp:112] Iteration 95510, lr = 0.01
I0522 23:55:22.447124 35003 solver.cpp:239] Iteration 95520 (2.78784 iter/s, 3.587s/10 iters), loss = 5.5205
I0522 23:55:22.447177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.5205 (* 1 = 5.5205 loss)
I0522 23:55:22.456836 35003 sgd_solver.cpp:112] Iteration 95520, lr = 0.01
I0522 23:55:26.751116 35003 solver.cpp:239] Iteration 95530 (2.32355 iter/s, 4.30376s/10 iters), loss = 7.73441
I0522 23:55:26.751158 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73441 (* 1 = 7.73441 loss)
I0522 23:55:26.758810 35003 sgd_solver.cpp:112] Iteration 95530, lr = 0.01
I0522 23:55:30.124640 35003 solver.cpp:239] Iteration 95540 (2.96442 iter/s, 3.37334s/10 iters), loss = 8.22997
I0522 23:55:30.124698 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.22997 (* 1 = 8.22997 loss)
I0522 23:55:30.859730 35003 sgd_solver.cpp:112] Iteration 95540, lr = 0.01
I0522 23:55:33.740569 35003 solver.cpp:239] Iteration 95550 (2.7657 iter/s, 3.61572s/10 iters), loss = 8.02982
I0522 23:55:33.740607 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02982 (* 1 = 8.02982 loss)
I0522 23:55:33.753645 35003 sgd_solver.cpp:112] Iteration 95550, lr = 0.01
I0522 23:55:35.591236 35003 solver.cpp:239] Iteration 95560 (5.40381 iter/s, 1.85055s/10 iters), loss = 7.89797
I0522 23:55:35.591279 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89797 (* 1 = 7.89797 loss)
I0522 23:55:35.603557 35003 sgd_solver.cpp:112] Iteration 95560, lr = 0.01
I0522 23:55:37.667874 35003 solver.cpp:239] Iteration 95570 (4.8158 iter/s, 2.0765s/10 iters), loss = 6.97964
I0522 23:55:37.667912 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97964 (* 1 = 6.97964 loss)
I0522 23:55:37.681123 35003 sgd_solver.cpp:112] Iteration 95570, lr = 0.01
I0522 23:55:39.962047 35003 solver.cpp:239] Iteration 95580 (4.35914 iter/s, 2.29403s/10 iters), loss = 5.84838
I0522 23:55:39.962102 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84838 (* 1 = 5.84838 loss)
I0522 23:55:40.664513 35003 sgd_solver.cpp:112] Iteration 95580, lr = 0.01
I0522 23:55:44.284600 35003 solver.cpp:239] Iteration 95590 (2.31357 iter/s, 4.32232s/10 iters), loss = 6.69281
I0522 23:55:44.284744 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69281 (* 1 = 6.69281 loss)
I0522 23:55:44.297984 35003 sgd_solver.cpp:112] Iteration 95590, lr = 0.01
I0522 23:55:47.082156 35003 solver.cpp:239] Iteration 95600 (3.57487 iter/s, 2.7973s/10 iters), loss = 7.65392
I0522 23:55:47.082201 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65392 (* 1 = 7.65392 loss)
I0522 23:55:47.090332 35003 sgd_solver.cpp:112] Iteration 95600, lr = 0.01
I0522 23:55:50.387238 35003 solver.cpp:239] Iteration 95610 (3.02581 iter/s, 3.3049s/10 iters), loss = 6.39644
I0522 23:55:50.387280 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39644 (* 1 = 6.39644 loss)
I0522 23:55:51.064581 35003 sgd_solver.cpp:112] Iteration 95610, lr = 0.01
I0522 23:55:53.932253 35003 solver.cpp:239] Iteration 95620 (2.82101 iter/s, 3.54482s/10 iters), loss = 6.67632
I0522 23:55:53.932297 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67632 (* 1 = 6.67632 loss)
I0522 23:55:53.939488 35003 sgd_solver.cpp:112] Iteration 95620, lr = 0.01
I0522 23:55:57.595254 35003 solver.cpp:239] Iteration 95630 (2.73015 iter/s, 3.66281s/10 iters), loss = 6.17684
I0522 23:55:57.595293 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17684 (* 1 = 6.17684 loss)
I0522 23:55:57.598681 35003 sgd_solver.cpp:112] Iteration 95630, lr = 0.01
I0522 23:56:01.238037 35003 solver.cpp:239] Iteration 95640 (2.7453 iter/s, 3.64259s/10 iters), loss = 7.52702
I0522 23:56:01.238080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52702 (* 1 = 7.52702 loss)
I0522 23:56:01.851547 35003 sgd_solver.cpp:112] Iteration 95640, lr = 0.01
I0522 23:56:05.574163 35003 solver.cpp:239] Iteration 95650 (2.30633 iter/s, 4.33589s/10 iters), loss = 6.60429
I0522 23:56:05.574228 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60429 (* 1 = 6.60429 loss)
I0522 23:56:05.600111 35003 sgd_solver.cpp:112] Iteration 95650, lr = 0.01
I0522 23:56:07.819411 35003 solver.cpp:239] Iteration 95660 (4.45417 iter/s, 2.24509s/10 iters), loss = 7.5912
I0522 23:56:07.819452 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5912 (* 1 = 7.5912 loss)
I0522 23:56:07.829639 35003 sgd_solver.cpp:112] Iteration 95660, lr = 0.01
I0522 23:56:11.403132 35003 solver.cpp:239] Iteration 95670 (2.79054 iter/s, 3.58354s/10 iters), loss = 8.64389
I0522 23:56:11.403177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.64389 (* 1 = 8.64389 loss)
I0522 23:56:12.144631 35003 sgd_solver.cpp:112] Iteration 95670, lr = 0.01
I0522 23:56:14.356087 35003 solver.cpp:239] Iteration 95680 (3.38663 iter/s, 2.95279s/10 iters), loss = 6.50421
I0522 23:56:14.356335 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50421 (* 1 = 6.50421 loss)
I0522 23:56:15.062885 35003 sgd_solver.cpp:112] Iteration 95680, lr = 0.01
I0522 23:56:17.878998 35003 solver.cpp:239] Iteration 95690 (2.83886 iter/s, 3.52254s/10 iters), loss = 6.96678
I0522 23:56:17.879060 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96678 (* 1 = 6.96678 loss)
I0522 23:56:17.884680 35003 sgd_solver.cpp:112] Iteration 95690, lr = 0.01
I0522 23:56:22.525624 35003 solver.cpp:239] Iteration 95700 (2.15226 iter/s, 4.64628s/10 iters), loss = 7.6256
I0522 23:56:22.525668 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6256 (* 1 = 7.6256 loss)
I0522 23:56:22.536312 35003 sgd_solver.cpp:112] Iteration 95700, lr = 0.01
I0522 23:56:24.585793 35003 solver.cpp:239] Iteration 95710 (4.85428 iter/s, 2.06004s/10 iters), loss = 6.31645
I0522 23:56:24.585831 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31645 (* 1 = 6.31645 loss)
I0522 23:56:24.614686 35003 sgd_solver.cpp:112] Iteration 95710, lr = 0.01
I0522 23:56:29.239761 35003 solver.cpp:239] Iteration 95720 (2.14882 iter/s, 4.65373s/10 iters), loss = 6.23985
I0522 23:56:29.239822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23985 (* 1 = 6.23985 loss)
I0522 23:56:29.941493 35003 sgd_solver.cpp:112] Iteration 95720, lr = 0.01
I0522 23:56:32.903431 35003 solver.cpp:239] Iteration 95730 (2.72966 iter/s, 3.66346s/10 iters), loss = 6.7556
I0522 23:56:32.903479 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7556 (* 1 = 6.7556 loss)
I0522 23:56:32.908489 35003 sgd_solver.cpp:112] Iteration 95730, lr = 0.01
I0522 23:56:36.332124 35003 solver.cpp:239] Iteration 95740 (2.91673 iter/s, 3.4285s/10 iters), loss = 6.64428
I0522 23:56:36.332166 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64428 (* 1 = 6.64428 loss)
I0522 23:56:36.345832 35003 sgd_solver.cpp:112] Iteration 95740, lr = 0.01
I0522 23:56:39.253211 35003 solver.cpp:239] Iteration 95750 (3.4236 iter/s, 2.9209s/10 iters), loss = 7.10923
I0522 23:56:39.253259 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10923 (* 1 = 7.10923 loss)
I0522 23:56:39.968009 35003 sgd_solver.cpp:112] Iteration 95750, lr = 0.01
I0522 23:56:44.857506 35003 solver.cpp:239] Iteration 95760 (1.78443 iter/s, 5.60402s/10 iters), loss = 7.62641
I0522 23:56:44.857765 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62641 (* 1 = 7.62641 loss)
I0522 23:56:44.870242 35003 sgd_solver.cpp:112] Iteration 95760, lr = 0.01
I0522 23:56:46.852762 35003 solver.cpp:239] Iteration 95770 (5.01269 iter/s, 1.99494s/10 iters), loss = 7.38557
I0522 23:56:46.852808 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38557 (* 1 = 7.38557 loss)
I0522 23:56:46.880252 35003 sgd_solver.cpp:112] Iteration 95770, lr = 0.01
I0522 23:56:50.443930 35003 solver.cpp:239] Iteration 95780 (2.78477 iter/s, 3.59097s/10 iters), loss = 7.42381
I0522 23:56:50.444005 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42381 (* 1 = 7.42381 loss)
I0522 23:56:50.647006 35003 sgd_solver.cpp:112] Iteration 95780, lr = 0.01
I0522 23:56:53.461505 35003 solver.cpp:239] Iteration 95790 (3.31414 iter/s, 3.01738s/10 iters), loss = 7.5601
I0522 23:56:53.461546 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5601 (* 1 = 7.5601 loss)
I0522 23:56:53.495213 35003 sgd_solver.cpp:112] Iteration 95790, lr = 0.01
I0522 23:56:55.757735 35003 solver.cpp:239] Iteration 95800 (4.35523 iter/s, 2.29609s/10 iters), loss = 7.88819
I0522 23:56:55.757777 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88819 (* 1 = 7.88819 loss)
I0522 23:56:56.466770 35003 sgd_solver.cpp:112] Iteration 95800, lr = 0.01
I0522 23:56:58.536803 35003 solver.cpp:239] Iteration 95810 (3.59854 iter/s, 2.7789s/10 iters), loss = 7.81256
I0522 23:56:58.536859 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81256 (* 1 = 7.81256 loss)
I0522 23:56:59.143734 35003 sgd_solver.cpp:112] Iteration 95810, lr = 0.01
I0522 23:57:02.679890 35003 solver.cpp:239] Iteration 95820 (2.41379 iter/s, 4.14287s/10 iters), loss = 6.86386
I0522 23:57:02.679935 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86386 (* 1 = 6.86386 loss)
I0522 23:57:02.702283 35003 sgd_solver.cpp:112] Iteration 95820, lr = 0.01
I0522 23:57:05.417855 35003 solver.cpp:239] Iteration 95830 (3.65256 iter/s, 2.7378s/10 iters), loss = 6.96484
I0522 23:57:05.417892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96484 (* 1 = 6.96484 loss)
I0522 23:57:05.427510 35003 sgd_solver.cpp:112] Iteration 95830, lr = 0.01
I0522 23:57:08.022380 35003 solver.cpp:239] Iteration 95840 (3.8397 iter/s, 2.60437s/10 iters), loss = 7.35297
I0522 23:57:08.022431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35297 (* 1 = 7.35297 loss)
I0522 23:57:08.707795 35003 sgd_solver.cpp:112] Iteration 95840, lr = 0.01
I0522 23:57:11.649314 35003 solver.cpp:239] Iteration 95850 (2.7573 iter/s, 3.62674s/10 iters), loss = 8.5285
I0522 23:57:11.649366 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.5285 (* 1 = 8.5285 loss)
I0522 23:57:11.659917 35003 sgd_solver.cpp:112] Iteration 95850, lr = 0.01
I0522 23:57:15.373481 35003 solver.cpp:239] Iteration 95860 (2.68531 iter/s, 3.72396s/10 iters), loss = 8.5666
I0522 23:57:15.373778 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.5666 (* 1 = 8.5666 loss)
I0522 23:57:15.386757 35003 sgd_solver.cpp:112] Iteration 95860, lr = 0.01
I0522 23:57:19.068500 35003 solver.cpp:239] Iteration 95870 (2.70665 iter/s, 3.69461s/10 iters), loss = 6.73905
I0522 23:57:19.068552 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73905 (* 1 = 6.73905 loss)
I0522 23:57:19.073195 35003 sgd_solver.cpp:112] Iteration 95870, lr = 0.01
I0522 23:57:22.565416 35003 solver.cpp:239] Iteration 95880 (2.85982 iter/s, 3.49672s/10 iters), loss = 7.84264
I0522 23:57:22.565461 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84264 (* 1 = 7.84264 loss)
I0522 23:57:23.280848 35003 sgd_solver.cpp:112] Iteration 95880, lr = 0.01
I0522 23:57:26.135890 35003 solver.cpp:239] Iteration 95890 (2.8009 iter/s, 3.57028s/10 iters), loss = 7.20967
I0522 23:57:26.135931 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20967 (* 1 = 7.20967 loss)
I0522 23:57:26.146584 35003 sgd_solver.cpp:112] Iteration 95890, lr = 0.01
I0522 23:57:28.432534 35003 solver.cpp:239] Iteration 95900 (4.35445 iter/s, 2.2965s/10 iters), loss = 8.49184
I0522 23:57:28.432577 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.49184 (* 1 = 8.49184 loss)
I0522 23:57:28.444928 35003 sgd_solver.cpp:112] Iteration 95900, lr = 0.01
I0522 23:57:31.230660 35003 solver.cpp:239] Iteration 95910 (3.57403 iter/s, 2.79797s/10 iters), loss = 6.96577
I0522 23:57:31.230734 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96577 (* 1 = 6.96577 loss)
I0522 23:57:31.236583 35003 sgd_solver.cpp:112] Iteration 95910, lr = 0.01
I0522 23:57:32.446233 35003 solver.cpp:239] Iteration 95920 (8.22742 iter/s, 1.21545s/10 iters), loss = 6.27971
I0522 23:57:32.446267 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27971 (* 1 = 6.27971 loss)
I0522 23:57:32.453171 35003 sgd_solver.cpp:112] Iteration 95920, lr = 0.01
I0522 23:57:33.252727 35003 solver.cpp:239] Iteration 95930 (12.4005 iter/s, 0.806416s/10 iters), loss = 7.90757
I0522 23:57:33.252765 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90757 (* 1 = 7.90757 loss)
I0522 23:57:33.264523 35003 sgd_solver.cpp:112] Iteration 95930, lr = 0.01
I0522 23:57:34.470894 35003 solver.cpp:239] Iteration 95940 (8.20971 iter/s, 1.21807s/10 iters), loss = 7.70051
I0522 23:57:34.470937 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70051 (* 1 = 7.70051 loss)
I0522 23:57:34.482414 35003 sgd_solver.cpp:112] Iteration 95940, lr = 0.01
I0522 23:57:35.556844 35003 solver.cpp:239] Iteration 95950 (9.20933 iter/s, 1.08586s/10 iters), loss = 8.11739
I0522 23:57:35.556888 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.11739 (* 1 = 8.11739 loss)
I0522 23:57:35.572376 35003 sgd_solver.cpp:112] Iteration 95950, lr = 0.01
I0522 23:57:38.917385 35003 solver.cpp:239] Iteration 95960 (2.97588 iter/s, 3.36035s/10 iters), loss = 7.44249
I0522 23:57:38.917448 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44249 (* 1 = 7.44249 loss)
I0522 23:57:38.920316 35003 sgd_solver.cpp:112] Iteration 95960, lr = 0.01
I0522 23:57:43.016611 35003 solver.cpp:239] Iteration 95970 (2.43963 iter/s, 4.09898s/10 iters), loss = 7.83319
I0522 23:57:43.016665 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83319 (* 1 = 7.83319 loss)
I0522 23:57:43.024232 35003 sgd_solver.cpp:112] Iteration 95970, lr = 0.01
I0522 23:57:45.855106 35003 solver.cpp:239] Iteration 95980 (3.52321 iter/s, 2.83832s/10 iters), loss = 7.62109
I0522 23:57:45.855288 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62109 (* 1 = 7.62109 loss)
I0522 23:57:45.859899 35003 sgd_solver.cpp:112] Iteration 95980, lr = 0.01
I0522 23:57:48.662716 35003 solver.cpp:239] Iteration 95990 (3.56213 iter/s, 2.80731s/10 iters), loss = 8.30209
I0522 23:57:48.662760 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.30209 (* 1 = 8.30209 loss)
I0522 23:57:48.672189 35003 sgd_solver.cpp:112] Iteration 95990, lr = 0.01
I0522 23:57:52.321885 35003 solver.cpp:239] Iteration 96000 (2.73301 iter/s, 3.65897s/10 iters), loss = 7.19057
I0522 23:57:52.321929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19057 (* 1 = 7.19057 loss)
I0522 23:57:53.057085 35003 sgd_solver.cpp:112] Iteration 96000, lr = 0.01
I0522 23:57:56.870975 35003 solver.cpp:239] Iteration 96010 (2.19836 iter/s, 4.54885s/10 iters), loss = 7.18634
I0522 23:57:56.871022 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18634 (* 1 = 7.18634 loss)
I0522 23:57:56.873620 35003 sgd_solver.cpp:112] Iteration 96010, lr = 0.01
I0522 23:58:00.288415 35003 solver.cpp:239] Iteration 96020 (2.92635 iter/s, 3.41723s/10 iters), loss = 6.83349
I0522 23:58:00.288473 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83349 (* 1 = 6.83349 loss)
I0522 23:58:00.411142 35003 sgd_solver.cpp:112] Iteration 96020, lr = 0.01
I0522 23:58:03.969575 35003 solver.cpp:239] Iteration 96030 (2.71669 iter/s, 3.68095s/10 iters), loss = 6.56761
I0522 23:58:03.969624 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56761 (* 1 = 6.56761 loss)
I0522 23:58:03.978269 35003 sgd_solver.cpp:112] Iteration 96030, lr = 0.01
I0522 23:58:07.617276 35003 solver.cpp:239] Iteration 96040 (2.7416 iter/s, 3.6475s/10 iters), loss = 7.30013
I0522 23:58:07.617333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30013 (* 1 = 7.30013 loss)
I0522 23:58:08.336666 35003 sgd_solver.cpp:112] Iteration 96040, lr = 0.01
I0522 23:58:11.900184 35003 solver.cpp:239] Iteration 96050 (2.335 iter/s, 4.28265s/10 iters), loss = 7.09811
I0522 23:58:11.900226 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09811 (* 1 = 7.09811 loss)
I0522 23:58:11.908324 35003 sgd_solver.cpp:112] Iteration 96050, lr = 0.01
I0522 23:58:14.624897 35003 solver.cpp:239] Iteration 96060 (3.67033 iter/s, 2.72455s/10 iters), loss = 7.55424
I0522 23:58:14.624941 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55424 (* 1 = 7.55424 loss)
I0522 23:58:15.256605 35003 sgd_solver.cpp:112] Iteration 96060, lr = 0.01
I0522 23:58:18.739416 35003 solver.cpp:239] Iteration 96070 (2.43055 iter/s, 4.1143s/10 iters), loss = 6.35709
I0522 23:58:18.739562 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35709 (* 1 = 6.35709 loss)
I0522 23:58:19.479907 35003 sgd_solver.cpp:112] Iteration 96070, lr = 0.01
I0522 23:58:22.944690 35003 solver.cpp:239] Iteration 96080 (2.37815 iter/s, 4.20496s/10 iters), loss = 7.39896
I0522 23:58:22.944736 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39896 (* 1 = 7.39896 loss)
I0522 23:58:22.947074 35003 sgd_solver.cpp:112] Iteration 96080, lr = 0.01
I0522 23:58:26.333005 35003 solver.cpp:239] Iteration 96090 (2.95148 iter/s, 3.38813s/10 iters), loss = 5.51918
I0522 23:58:26.333051 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.51918 (* 1 = 5.51918 loss)
I0522 23:58:26.345461 35003 sgd_solver.cpp:112] Iteration 96090, lr = 0.01
I0522 23:58:29.155652 35003 solver.cpp:239] Iteration 96100 (3.54299 iter/s, 2.82247s/10 iters), loss = 6.68044
I0522 23:58:29.155709 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68044 (* 1 = 6.68044 loss)
I0522 23:58:29.168756 35003 sgd_solver.cpp:112] Iteration 96100, lr = 0.01
I0522 23:58:32.369437 35003 solver.cpp:239] Iteration 96110 (3.11178 iter/s, 3.21359s/10 iters), loss = 6.57712
I0522 23:58:32.369479 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57712 (* 1 = 6.57712 loss)
I0522 23:58:32.388090 35003 sgd_solver.cpp:112] Iteration 96110, lr = 0.01
I0522 23:58:33.699776 35003 solver.cpp:239] Iteration 96120 (7.51747 iter/s, 1.33024s/10 iters), loss = 6.89743
I0522 23:58:33.699818 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89743 (* 1 = 6.89743 loss)
I0522 23:58:34.395149 35003 sgd_solver.cpp:112] Iteration 96120, lr = 0.01
I0522 23:58:37.111527 35003 solver.cpp:239] Iteration 96130 (2.9312 iter/s, 3.41157s/10 iters), loss = 7.0463
I0522 23:58:37.111572 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0463 (* 1 = 7.0463 loss)
I0522 23:58:37.125284 35003 sgd_solver.cpp:112] Iteration 96130, lr = 0.01
I0522 23:58:39.957371 35003 solver.cpp:239] Iteration 96140 (3.5141 iter/s, 2.84568s/10 iters), loss = 7.50357
I0522 23:58:39.957406 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50357 (* 1 = 7.50357 loss)
I0522 23:58:39.975556 35003 sgd_solver.cpp:112] Iteration 96140, lr = 0.01
I0522 23:58:43.683405 35003 solver.cpp:239] Iteration 96150 (2.68396 iter/s, 3.72584s/10 iters), loss = 8.6868
I0522 23:58:43.683467 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.6868 (* 1 = 8.6868 loss)
I0522 23:58:43.710918 35003 sgd_solver.cpp:112] Iteration 96150, lr = 0.01
I0522 23:58:47.539973 35003 solver.cpp:239] Iteration 96160 (2.59314 iter/s, 3.85633s/10 iters), loss = 7.25898
I0522 23:58:47.540057 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25898 (* 1 = 7.25898 loss)
I0522 23:58:47.552626 35003 sgd_solver.cpp:112] Iteration 96160, lr = 0.01
I0522 23:58:50.190497 35003 solver.cpp:239] Iteration 96170 (3.77311 iter/s, 2.65033s/10 iters), loss = 7.4951
I0522 23:58:50.190652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4951 (* 1 = 7.4951 loss)
I0522 23:58:50.195161 35003 sgd_solver.cpp:112] Iteration 96170, lr = 0.01
I0522 23:58:52.879474 35003 solver.cpp:239] Iteration 96180 (3.71926 iter/s, 2.68871s/10 iters), loss = 7.71499
I0522 23:58:52.879530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71499 (* 1 = 7.71499 loss)
I0522 23:58:52.891404 35003 sgd_solver.cpp:112] Iteration 96180, lr = 0.01
I0522 23:58:54.170375 35003 solver.cpp:239] Iteration 96190 (7.74726 iter/s, 1.29078s/10 iters), loss = 6.98056
I0522 23:58:54.170414 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98056 (* 1 = 6.98056 loss)
I0522 23:58:54.177492 35003 sgd_solver.cpp:112] Iteration 96190, lr = 0.01
I0522 23:58:57.872661 35003 solver.cpp:239] Iteration 96200 (2.70118 iter/s, 3.70208s/10 iters), loss = 8.19291
I0522 23:58:57.872723 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.19291 (* 1 = 8.19291 loss)
I0522 23:58:58.553967 35003 sgd_solver.cpp:112] Iteration 96200, lr = 0.01
I0522 23:59:01.407424 35003 solver.cpp:239] Iteration 96210 (2.82921 iter/s, 3.53455s/10 iters), loss = 7.36644
I0522 23:59:01.407475 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36644 (* 1 = 7.36644 loss)
I0522 23:59:01.420186 35003 sgd_solver.cpp:112] Iteration 96210, lr = 0.01
I0522 23:59:04.960721 35003 solver.cpp:239] Iteration 96220 (2.81445 iter/s, 3.55309s/10 iters), loss = 8.16667
I0522 23:59:04.960781 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16667 (* 1 = 8.16667 loss)
I0522 23:59:05.701532 35003 sgd_solver.cpp:112] Iteration 96220, lr = 0.01
I0522 23:59:09.199970 35003 solver.cpp:239] Iteration 96230 (2.35904 iter/s, 4.23901s/10 iters), loss = 7.02708
I0522 23:59:09.200026 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02708 (* 1 = 7.02708 loss)
I0522 23:59:09.940686 35003 sgd_solver.cpp:112] Iteration 96230, lr = 0.01
I0522 23:59:11.215764 35003 solver.cpp:239] Iteration 96240 (4.96117 iter/s, 2.01565s/10 iters), loss = 6.82018
I0522 23:59:11.215804 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82018 (* 1 = 6.82018 loss)
I0522 23:59:11.243187 35003 sgd_solver.cpp:112] Iteration 96240, lr = 0.01
I0522 23:59:13.766386 35003 solver.cpp:239] Iteration 96250 (3.92085 iter/s, 2.55047s/10 iters), loss = 7.07615
I0522 23:59:13.766441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07615 (* 1 = 7.07615 loss)
I0522 23:59:13.779681 35003 sgd_solver.cpp:112] Iteration 96250, lr = 0.01
I0522 23:59:18.075186 35003 solver.cpp:239] Iteration 96260 (2.32096 iter/s, 4.30857s/10 iters), loss = 6.87101
I0522 23:59:18.075225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87101 (* 1 = 6.87101 loss)
I0522 23:59:18.087958 35003 sgd_solver.cpp:112] Iteration 96260, lr = 0.01
I0522 23:59:22.609761 35003 solver.cpp:239] Iteration 96270 (2.20539 iter/s, 4.53435s/10 iters), loss = 6.83204
I0522 23:59:22.610009 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83204 (* 1 = 6.83204 loss)
I0522 23:59:23.337121 35003 sgd_solver.cpp:112] Iteration 96270, lr = 0.01
I0522 23:59:25.457041 35003 solver.cpp:239] Iteration 96280 (3.51255 iter/s, 2.84694s/10 iters), loss = 7.77088
I0522 23:59:25.457082 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77088 (* 1 = 7.77088 loss)
I0522 23:59:26.159438 35003 sgd_solver.cpp:112] Iteration 96280, lr = 0.01
I0522 23:59:30.465456 35003 solver.cpp:239] Iteration 96290 (1.99674 iter/s, 5.00817s/10 iters), loss = 7.97234
I0522 23:59:30.465508 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97234 (* 1 = 7.97234 loss)
I0522 23:59:30.473043 35003 sgd_solver.cpp:112] Iteration 96290, lr = 0.01
I0522 23:59:34.077683 35003 solver.cpp:239] Iteration 96300 (2.76853 iter/s, 3.61203s/10 iters), loss = 7.43509
I0522 23:59:34.077724 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43509 (* 1 = 7.43509 loss)
I0522 23:59:34.818581 35003 sgd_solver.cpp:112] Iteration 96300, lr = 0.01
I0522 23:59:39.279505 35003 solver.cpp:239] Iteration 96310 (1.9225 iter/s, 5.20157s/10 iters), loss = 7.71774
I0522 23:59:39.279546 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71774 (* 1 = 7.71774 loss)
I0522 23:59:39.292626 35003 sgd_solver.cpp:112] Iteration 96310, lr = 0.01
I0522 23:59:42.032407 35003 solver.cpp:239] Iteration 96320 (3.63274 iter/s, 2.75274s/10 iters), loss = 7.62318
I0522 23:59:42.032459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62318 (* 1 = 7.62318 loss)
I0522 23:59:42.773389 35003 sgd_solver.cpp:112] Iteration 96320, lr = 0.01
I0522 23:59:47.376971 35003 solver.cpp:239] Iteration 96330 (1.87115 iter/s, 5.3443s/10 iters), loss = 7.90083
I0522 23:59:47.377012 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90083 (* 1 = 7.90083 loss)
I0522 23:59:48.072659 35003 sgd_solver.cpp:112] Iteration 96330, lr = 0.01
I0522 23:59:51.693478 35003 solver.cpp:239] Iteration 96340 (2.31681 iter/s, 4.31629s/10 iters), loss = 8.26014
I0522 23:59:51.693517 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.26014 (* 1 = 8.26014 loss)
I0522 23:59:51.706861 35003 sgd_solver.cpp:112] Iteration 96340, lr = 0.01
I0522 23:59:55.262272 35003 solver.cpp:239] Iteration 96350 (2.80222 iter/s, 3.5686s/10 iters), loss = 8.1605
I0522 23:59:55.262537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.1605 (* 1 = 8.1605 loss)
I0522 23:59:55.977620 35003 sgd_solver.cpp:112] Iteration 96350, lr = 0.01
I0523 00:00:00.428225 35003 solver.cpp:239] Iteration 96360 (1.93591 iter/s, 5.16553s/10 iters), loss = 6.69443
I0523 00:00:00.428282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69443 (* 1 = 6.69443 loss)
I0523 00:00:01.124112 35003 sgd_solver.cpp:112] Iteration 96360, lr = 0.01
I0523 00:00:03.946418 35003 solver.cpp:239] Iteration 96370 (2.84253 iter/s, 3.51799s/10 iters), loss = 7.55946
I0523 00:00:03.946458 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55946 (* 1 = 7.55946 loss)
I0523 00:00:04.687218 35003 sgd_solver.cpp:112] Iteration 96370, lr = 0.01
I0523 00:00:06.011708 35003 solver.cpp:239] Iteration 96380 (4.84224 iter/s, 2.06516s/10 iters), loss = 7.04349
I0523 00:00:06.011755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04349 (* 1 = 7.04349 loss)
I0523 00:00:06.753232 35003 sgd_solver.cpp:112] Iteration 96380, lr = 0.01
I0523 00:00:08.955840 35003 solver.cpp:239] Iteration 96390 (3.39679 iter/s, 2.94396s/10 iters), loss = 7.7067
I0523 00:00:08.955890 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7067 (* 1 = 7.7067 loss)
I0523 00:00:09.684293 35003 sgd_solver.cpp:112] Iteration 96390, lr = 0.01
I0523 00:00:13.125708 35003 solver.cpp:239] Iteration 96400 (2.39829 iter/s, 4.16964s/10 iters), loss = 7.09493
I0523 00:00:13.125756 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09493 (* 1 = 7.09493 loss)
I0523 00:00:13.130841 35003 sgd_solver.cpp:112] Iteration 96400, lr = 0.01
I0523 00:00:15.913823 35003 solver.cpp:239] Iteration 96410 (3.58687 iter/s, 2.78794s/10 iters), loss = 7.78499
I0523 00:00:15.913869 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78499 (* 1 = 7.78499 loss)
I0523 00:00:15.919886 35003 sgd_solver.cpp:112] Iteration 96410, lr = 0.01
I0523 00:00:18.817481 35003 solver.cpp:239] Iteration 96420 (3.44414 iter/s, 2.90348s/10 iters), loss = 7.33366
I0523 00:00:18.817533 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33366 (* 1 = 7.33366 loss)
I0523 00:00:18.824729 35003 sgd_solver.cpp:112] Iteration 96420, lr = 0.01
I0523 00:00:21.669718 35003 solver.cpp:239] Iteration 96430 (3.50623 iter/s, 2.85207s/10 iters), loss = 7.5153
I0523 00:00:21.669762 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5153 (* 1 = 7.5153 loss)
I0523 00:00:22.397392 35003 sgd_solver.cpp:112] Iteration 96430, lr = 0.01
I0523 00:00:25.946252 35003 solver.cpp:239] Iteration 96440 (2.33847 iter/s, 4.2763s/10 iters), loss = 6.94386
I0523 00:00:25.946561 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94386 (* 1 = 6.94386 loss)
I0523 00:00:26.533370 35003 sgd_solver.cpp:112] Iteration 96440, lr = 0.01
I0523 00:00:30.051491 35003 solver.cpp:239] Iteration 96450 (2.43618 iter/s, 4.10479s/10 iters), loss = 6.99271
I0523 00:00:30.051530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99271 (* 1 = 6.99271 loss)
I0523 00:00:30.065055 35003 sgd_solver.cpp:112] Iteration 96450, lr = 0.01
I0523 00:00:34.898195 35003 solver.cpp:239] Iteration 96460 (2.06336 iter/s, 4.84647s/10 iters), loss = 7.0395
I0523 00:00:34.898241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0395 (* 1 = 7.0395 loss)
I0523 00:00:35.492306 35003 sgd_solver.cpp:112] Iteration 96460, lr = 0.01
I0523 00:00:39.839553 35003 solver.cpp:239] Iteration 96470 (2.02384 iter/s, 4.94111s/10 iters), loss = 7.71596
I0523 00:00:39.839603 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71596 (* 1 = 7.71596 loss)
I0523 00:00:39.846597 35003 sgd_solver.cpp:112] Iteration 96470, lr = 0.01
I0523 00:00:43.515563 35003 solver.cpp:239] Iteration 96480 (2.7205 iter/s, 3.6758s/10 iters), loss = 7.1026
I0523 00:00:43.515628 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1026 (* 1 = 7.1026 loss)
I0523 00:00:44.182466 35003 sgd_solver.cpp:112] Iteration 96480, lr = 0.01
I0523 00:00:47.041311 35003 solver.cpp:239] Iteration 96490 (2.83645 iter/s, 3.52553s/10 iters), loss = 7.73474
I0523 00:00:47.041365 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73474 (* 1 = 7.73474 loss)
I0523 00:00:47.782179 35003 sgd_solver.cpp:112] Iteration 96490, lr = 0.01
I0523 00:00:52.045469 35003 solver.cpp:239] Iteration 96500 (1.99845 iter/s, 5.00389s/10 iters), loss = 6.39562
I0523 00:00:52.045521 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39562 (* 1 = 6.39562 loss)
I0523 00:00:52.069406 35003 sgd_solver.cpp:112] Iteration 96500, lr = 0.01
I0523 00:00:54.922950 35003 solver.cpp:239] Iteration 96510 (3.47547 iter/s, 2.87731s/10 iters), loss = 7.17774
I0523 00:00:54.922999 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17774 (* 1 = 7.17774 loss)
I0523 00:00:54.931921 35003 sgd_solver.cpp:112] Iteration 96510, lr = 0.01
I0523 00:00:58.475416 35003 solver.cpp:239] Iteration 96520 (2.8151 iter/s, 3.55227s/10 iters), loss = 6.47876
I0523 00:00:58.475651 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47876 (* 1 = 6.47876 loss)
I0523 00:00:58.490530 35003 sgd_solver.cpp:112] Iteration 96520, lr = 0.01
I0523 00:01:00.552588 35003 solver.cpp:239] Iteration 96530 (4.81495 iter/s, 2.07687s/10 iters), loss = 7.17663
I0523 00:01:00.552631 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17663 (* 1 = 7.17663 loss)
I0523 00:01:00.571135 35003 sgd_solver.cpp:112] Iteration 96530, lr = 0.01
I0523 00:01:04.071672 35003 solver.cpp:239] Iteration 96540 (2.84181 iter/s, 3.51889s/10 iters), loss = 8.89137
I0523 00:01:04.071724 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.89137 (* 1 = 8.89137 loss)
I0523 00:01:04.090102 35003 sgd_solver.cpp:112] Iteration 96540, lr = 0.01
I0523 00:01:06.787650 35003 solver.cpp:239] Iteration 96550 (3.68221 iter/s, 2.71576s/10 iters), loss = 7.60248
I0523 00:01:06.787696 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60248 (* 1 = 7.60248 loss)
I0523 00:01:06.791499 35003 sgd_solver.cpp:112] Iteration 96550, lr = 0.01
I0523 00:01:11.922188 35003 solver.cpp:239] Iteration 96560 (1.94769 iter/s, 5.13428s/10 iters), loss = 6.86076
I0523 00:01:11.922236 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86076 (* 1 = 6.86076 loss)
I0523 00:01:11.993549 35003 sgd_solver.cpp:112] Iteration 96560, lr = 0.01
I0523 00:01:16.424703 35003 solver.cpp:239] Iteration 96570 (2.2211 iter/s, 4.50227s/10 iters), loss = 6.93725
I0523 00:01:16.424754 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93725 (* 1 = 6.93725 loss)
I0523 00:01:16.445802 35003 sgd_solver.cpp:112] Iteration 96570, lr = 0.01
I0523 00:01:20.264694 35003 solver.cpp:239] Iteration 96580 (2.60431 iter/s, 3.83978s/10 iters), loss = 6.59163
I0523 00:01:20.264745 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59163 (* 1 = 6.59163 loss)
I0523 00:01:20.956212 35003 sgd_solver.cpp:112] Iteration 96580, lr = 0.01
I0523 00:01:23.217278 35003 solver.cpp:239] Iteration 96590 (3.38706 iter/s, 2.95241s/10 iters), loss = 8.86736
I0523 00:01:23.217325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.86736 (* 1 = 8.86736 loss)
I0523 00:01:23.229624 35003 sgd_solver.cpp:112] Iteration 96590, lr = 0.01
I0523 00:01:26.767943 35003 solver.cpp:239] Iteration 96600 (2.81653 iter/s, 3.55046s/10 iters), loss = 6.15856
I0523 00:01:26.768009 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15856 (* 1 = 6.15856 loss)
I0523 00:01:27.492095 35003 sgd_solver.cpp:112] Iteration 96600, lr = 0.01
I0523 00:01:30.444953 35003 solver.cpp:239] Iteration 96610 (2.71976 iter/s, 3.67679s/10 iters), loss = 6.22823
I0523 00:01:30.445050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22823 (* 1 = 6.22823 loss)
I0523 00:01:30.452857 35003 sgd_solver.cpp:112] Iteration 96610, lr = 0.01
I0523 00:01:35.495211 35003 solver.cpp:239] Iteration 96620 (1.98021 iter/s, 5.04996s/10 iters), loss = 7.33531
I0523 00:01:35.495259 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33531 (* 1 = 7.33531 loss)
I0523 00:01:36.210678 35003 sgd_solver.cpp:112] Iteration 96620, lr = 0.01
I0523 00:01:39.797523 35003 solver.cpp:239] Iteration 96630 (2.32445 iter/s, 4.30209s/10 iters), loss = 6.75536
I0523 00:01:39.797576 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75536 (* 1 = 6.75536 loss)
I0523 00:01:39.809821 35003 sgd_solver.cpp:112] Iteration 96630, lr = 0.01
I0523 00:01:43.398910 35003 solver.cpp:239] Iteration 96640 (2.77687 iter/s, 3.60118s/10 iters), loss = 7.16074
I0523 00:01:43.398957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16074 (* 1 = 7.16074 loss)
I0523 00:01:43.407114 35003 sgd_solver.cpp:112] Iteration 96640, lr = 0.01
I0523 00:01:46.525945 35003 solver.cpp:239] Iteration 96650 (3.19811 iter/s, 3.12685s/10 iters), loss = 7.18951
I0523 00:01:46.526032 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18951 (* 1 = 7.18951 loss)
I0523 00:01:46.527566 35003 sgd_solver.cpp:112] Iteration 96650, lr = 0.01
I0523 00:01:50.131104 35003 solver.cpp:239] Iteration 96660 (2.77399 iter/s, 3.60492s/10 iters), loss = 6.53511
I0523 00:01:50.131160 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53511 (* 1 = 6.53511 loss)
I0523 00:01:50.236943 35003 sgd_solver.cpp:112] Iteration 96660, lr = 0.01
I0523 00:01:53.918747 35003 solver.cpp:239] Iteration 96670 (2.64031 iter/s, 3.78743s/10 iters), loss = 6.27952
I0523 00:01:53.918794 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27952 (* 1 = 6.27952 loss)
I0523 00:01:54.647302 35003 sgd_solver.cpp:112] Iteration 96670, lr = 0.01
I0523 00:01:56.797776 35003 solver.cpp:239] Iteration 96680 (3.4736 iter/s, 2.87886s/10 iters), loss = 6.8542
I0523 00:01:56.797829 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8542 (* 1 = 6.8542 loss)
I0523 00:01:57.538640 35003 sgd_solver.cpp:112] Iteration 96680, lr = 0.01
I0523 00:02:01.647378 35003 solver.cpp:239] Iteration 96690 (2.06213 iter/s, 4.84936s/10 iters), loss = 7.8505
I0523 00:02:01.647608 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8505 (* 1 = 7.8505 loss)
I0523 00:02:02.379271 35003 sgd_solver.cpp:112] Iteration 96690, lr = 0.01
I0523 00:02:04.698473 35003 solver.cpp:239] Iteration 96700 (3.27789 iter/s, 3.05074s/10 iters), loss = 7.10526
I0523 00:02:04.698516 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10526 (* 1 = 7.10526 loss)
I0523 00:02:05.337900 35003 sgd_solver.cpp:112] Iteration 96700, lr = 0.01
I0523 00:02:07.973508 35003 solver.cpp:239] Iteration 96710 (3.05357 iter/s, 3.27485s/10 iters), loss = 6.93165
I0523 00:02:07.973561 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93165 (* 1 = 6.93165 loss)
I0523 00:02:07.978821 35003 sgd_solver.cpp:112] Iteration 96710, lr = 0.01
I0523 00:02:12.409759 35003 solver.cpp:239] Iteration 96720 (2.25427 iter/s, 4.43602s/10 iters), loss = 7.27412
I0523 00:02:12.409806 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27412 (* 1 = 7.27412 loss)
I0523 00:02:12.419574 35003 sgd_solver.cpp:112] Iteration 96720, lr = 0.01
I0523 00:02:16.731865 35003 solver.cpp:239] Iteration 96730 (2.31381 iter/s, 4.32188s/10 iters), loss = 7.05984
I0523 00:02:16.731910 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05984 (* 1 = 7.05984 loss)
I0523 00:02:16.744788 35003 sgd_solver.cpp:112] Iteration 96730, lr = 0.01
I0523 00:02:20.312431 35003 solver.cpp:239] Iteration 96740 (2.79301 iter/s, 3.58037s/10 iters), loss = 7.33457
I0523 00:02:20.312476 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33457 (* 1 = 7.33457 loss)
I0523 00:02:21.004045 35003 sgd_solver.cpp:112] Iteration 96740, lr = 0.01
I0523 00:02:23.843118 35003 solver.cpp:239] Iteration 96750 (2.83247 iter/s, 3.53049s/10 iters), loss = 6.80021
I0523 00:02:23.843173 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80021 (* 1 = 6.80021 loss)
I0523 00:02:23.856292 35003 sgd_solver.cpp:112] Iteration 96750, lr = 0.01
I0523 00:02:25.917939 35003 solver.cpp:239] Iteration 96760 (4.82004 iter/s, 2.07467s/10 iters), loss = 6.57471
I0523 00:02:25.917992 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57471 (* 1 = 6.57471 loss)
I0523 00:02:25.923391 35003 sgd_solver.cpp:112] Iteration 96760, lr = 0.01
I0523 00:02:29.496841 35003 solver.cpp:239] Iteration 96770 (2.79432 iter/s, 3.57869s/10 iters), loss = 7.0504
I0523 00:02:29.496928 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0504 (* 1 = 7.0504 loss)
I0523 00:02:29.509930 35003 sgd_solver.cpp:112] Iteration 96770, lr = 0.01
I0523 00:02:33.138988 35003 solver.cpp:239] Iteration 96780 (2.74581 iter/s, 3.64191s/10 iters), loss = 7.18559
I0523 00:02:33.139195 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18559 (* 1 = 7.18559 loss)
I0523 00:02:33.152806 35003 sgd_solver.cpp:112] Iteration 96780, lr = 0.01
I0523 00:02:35.320057 35003 solver.cpp:239] Iteration 96790 (4.58548 iter/s, 2.1808s/10 iters), loss = 7.6042
I0523 00:02:35.320099 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6042 (* 1 = 7.6042 loss)
I0523 00:02:35.342475 35003 sgd_solver.cpp:112] Iteration 96790, lr = 0.01
I0523 00:02:37.737823 35003 solver.cpp:239] Iteration 96800 (4.1363 iter/s, 2.41762s/10 iters), loss = 5.97683
I0523 00:02:37.737876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97683 (* 1 = 5.97683 loss)
I0523 00:02:37.744557 35003 sgd_solver.cpp:112] Iteration 96800, lr = 0.01
I0523 00:02:39.985788 35003 solver.cpp:239] Iteration 96810 (4.44877 iter/s, 2.24781s/10 iters), loss = 7.42941
I0523 00:02:39.985827 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42941 (* 1 = 7.42941 loss)
I0523 00:02:39.999017 35003 sgd_solver.cpp:112] Iteration 96810, lr = 0.01
I0523 00:02:42.803836 35003 solver.cpp:239] Iteration 96820 (3.54877 iter/s, 2.81788s/10 iters), loss = 7.52985
I0523 00:02:42.803897 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52985 (* 1 = 7.52985 loss)
I0523 00:02:42.817077 35003 sgd_solver.cpp:112] Iteration 96820, lr = 0.01
I0523 00:02:47.668053 35003 solver.cpp:239] Iteration 96830 (2.05594 iter/s, 4.86397s/10 iters), loss = 7.32316
I0523 00:02:47.668097 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32316 (* 1 = 7.32316 loss)
I0523 00:02:48.389192 35003 sgd_solver.cpp:112] Iteration 96830, lr = 0.01
I0523 00:02:52.818565 35003 solver.cpp:239] Iteration 96840 (1.94165 iter/s, 5.15026s/10 iters), loss = 7.04297
I0523 00:02:52.818612 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04297 (* 1 = 7.04297 loss)
I0523 00:02:52.834271 35003 sgd_solver.cpp:112] Iteration 96840, lr = 0.01
I0523 00:02:56.374863 35003 solver.cpp:239] Iteration 96850 (2.81207 iter/s, 3.55609s/10 iters), loss = 7.43212
I0523 00:02:56.374931 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43212 (* 1 = 7.43212 loss)
I0523 00:02:57.109017 35003 sgd_solver.cpp:112] Iteration 96850, lr = 0.01
I0523 00:02:59.149785 35003 solver.cpp:239] Iteration 96860 (3.60394 iter/s, 2.77474s/10 iters), loss = 7.34705
I0523 00:02:59.149824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34705 (* 1 = 7.34705 loss)
I0523 00:02:59.153203 35003 sgd_solver.cpp:112] Iteration 96860, lr = 0.01
I0523 00:03:01.207231 35003 solver.cpp:239] Iteration 96870 (4.86072 iter/s, 2.05731s/10 iters), loss = 8.30927
I0523 00:03:01.207283 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.30927 (* 1 = 8.30927 loss)
I0523 00:03:01.221328 35003 sgd_solver.cpp:112] Iteration 96870, lr = 0.01
I0523 00:03:03.892407 35003 solver.cpp:239] Iteration 96880 (3.72438 iter/s, 2.68501s/10 iters), loss = 7.9709
I0523 00:03:03.892624 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9709 (* 1 = 7.9709 loss)
I0523 00:03:03.899957 35003 sgd_solver.cpp:112] Iteration 96880, lr = 0.01
I0523 00:03:07.474120 35003 solver.cpp:239] Iteration 96890 (2.79222 iter/s, 3.58137s/10 iters), loss = 7.32787
I0523 00:03:07.474162 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32787 (* 1 = 7.32787 loss)
I0523 00:03:07.482414 35003 sgd_solver.cpp:112] Iteration 96890, lr = 0.01
I0523 00:03:10.969849 35003 solver.cpp:239] Iteration 96900 (2.86079 iter/s, 3.49554s/10 iters), loss = 6.89505
I0523 00:03:10.969902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89505 (* 1 = 6.89505 loss)
I0523 00:03:10.982278 35003 sgd_solver.cpp:112] Iteration 96900, lr = 0.01
I0523 00:03:12.272148 35003 solver.cpp:239] Iteration 96910 (7.67939 iter/s, 1.30219s/10 iters), loss = 8.32722
I0523 00:03:12.272187 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.32722 (* 1 = 8.32722 loss)
I0523 00:03:12.278885 35003 sgd_solver.cpp:112] Iteration 96910, lr = 0.01
I0523 00:03:14.983263 35003 solver.cpp:239] Iteration 96920 (3.68873 iter/s, 2.71096s/10 iters), loss = 7.67037
I0523 00:03:14.983309 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67037 (* 1 = 7.67037 loss)
I0523 00:03:15.503525 35003 sgd_solver.cpp:112] Iteration 96920, lr = 0.01
I0523 00:03:19.515085 35003 solver.cpp:239] Iteration 96930 (2.20673 iter/s, 4.53159s/10 iters), loss = 6.81842
I0523 00:03:19.515130 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81842 (* 1 = 6.81842 loss)
I0523 00:03:19.553097 35003 sgd_solver.cpp:112] Iteration 96930, lr = 0.01
I0523 00:03:23.857475 35003 solver.cpp:239] Iteration 96940 (2.303 iter/s, 4.34217s/10 iters), loss = 8.00737
I0523 00:03:23.857517 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00737 (* 1 = 8.00737 loss)
I0523 00:03:24.553210 35003 sgd_solver.cpp:112] Iteration 96940, lr = 0.01
I0523 00:03:27.389921 35003 solver.cpp:239] Iteration 96950 (2.83106 iter/s, 3.53225s/10 iters), loss = 8.14191
I0523 00:03:27.389972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.14191 (* 1 = 8.14191 loss)
I0523 00:03:27.397889 35003 sgd_solver.cpp:112] Iteration 96950, lr = 0.01
I0523 00:03:31.068150 35003 solver.cpp:239] Iteration 96960 (2.71885 iter/s, 3.67802s/10 iters), loss = 7.4193
I0523 00:03:31.068197 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4193 (* 1 = 7.4193 loss)
I0523 00:03:31.090139 35003 sgd_solver.cpp:112] Iteration 96960, lr = 0.01
I0523 00:03:33.936781 35003 solver.cpp:239] Iteration 96970 (3.48618 iter/s, 2.86847s/10 iters), loss = 6.96996
I0523 00:03:33.936939 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96996 (* 1 = 6.96996 loss)
I0523 00:03:33.942252 35003 sgd_solver.cpp:112] Iteration 96970, lr = 0.01
I0523 00:03:38.353857 35003 solver.cpp:239] Iteration 96980 (2.26412 iter/s, 4.41673s/10 iters), loss = 6.12445
I0523 00:03:38.353902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12445 (* 1 = 6.12445 loss)
I0523 00:03:38.361254 35003 sgd_solver.cpp:112] Iteration 96980, lr = 0.01
I0523 00:03:39.667268 35003 solver.cpp:239] Iteration 96990 (7.61438 iter/s, 1.3133s/10 iters), loss = 6.64176
I0523 00:03:39.667304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64176 (* 1 = 6.64176 loss)
I0523 00:03:39.675385 35003 sgd_solver.cpp:112] Iteration 96990, lr = 0.01
I0523 00:03:42.936597 35003 solver.cpp:239] Iteration 97000 (3.0589 iter/s, 3.26915s/10 iters), loss = 7.58876
I0523 00:03:42.936662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58876 (* 1 = 7.58876 loss)
I0523 00:03:42.950098 35003 sgd_solver.cpp:112] Iteration 97000, lr = 0.01
I0523 00:03:46.694008 35003 solver.cpp:239] Iteration 97010 (2.66156 iter/s, 3.7572s/10 iters), loss = 7.54222
I0523 00:03:46.694061 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54222 (* 1 = 7.54222 loss)
I0523 00:03:47.409139 35003 sgd_solver.cpp:112] Iteration 97010, lr = 0.01
I0523 00:03:51.680294 35003 solver.cpp:239] Iteration 97020 (2.00561 iter/s, 4.98602s/10 iters), loss = 8.56367
I0523 00:03:51.680346 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.56367 (* 1 = 8.56367 loss)
I0523 00:03:51.683544 35003 sgd_solver.cpp:112] Iteration 97020, lr = 0.01
I0523 00:03:55.633954 35003 solver.cpp:239] Iteration 97030 (2.52945 iter/s, 3.95343s/10 iters), loss = 7.13663
I0523 00:03:55.634009 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13663 (* 1 = 7.13663 loss)
I0523 00:03:55.689429 35003 sgd_solver.cpp:112] Iteration 97030, lr = 0.01
I0523 00:03:59.590673 35003 solver.cpp:239] Iteration 97040 (2.52748 iter/s, 3.9565s/10 iters), loss = 7.25463
I0523 00:03:59.590732 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25463 (* 1 = 7.25463 loss)
I0523 00:03:59.604099 35003 sgd_solver.cpp:112] Iteration 97040, lr = 0.01
I0523 00:04:02.212513 35003 solver.cpp:239] Iteration 97050 (3.81437 iter/s, 2.62167s/10 iters), loss = 7.1137
I0523 00:04:02.212568 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1137 (* 1 = 7.1137 loss)
I0523 00:04:02.234004 35003 sgd_solver.cpp:112] Iteration 97050, lr = 0.01
I0523 00:04:04.212113 35003 solver.cpp:239] Iteration 97060 (5.00135 iter/s, 1.99946s/10 iters), loss = 6.5773
I0523 00:04:04.212344 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5773 (* 1 = 6.5773 loss)
I0523 00:04:04.947022 35003 sgd_solver.cpp:112] Iteration 97060, lr = 0.01
I0523 00:04:08.519624 35003 solver.cpp:239] Iteration 97070 (2.32174 iter/s, 4.30711s/10 iters), loss = 7.27228
I0523 00:04:08.519673 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27228 (* 1 = 7.27228 loss)
I0523 00:04:09.258970 35003 sgd_solver.cpp:112] Iteration 97070, lr = 0.01
I0523 00:04:12.138366 35003 solver.cpp:239] Iteration 97080 (2.76355 iter/s, 3.61854s/10 iters), loss = 7.678
I0523 00:04:12.138423 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.678 (* 1 = 7.678 loss)
I0523 00:04:12.870311 35003 sgd_solver.cpp:112] Iteration 97080, lr = 0.01
I0523 00:04:16.452411 35003 solver.cpp:239] Iteration 97090 (2.31814 iter/s, 4.31381s/10 iters), loss = 6.93225
I0523 00:04:16.452455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93225 (* 1 = 6.93225 loss)
I0523 00:04:16.460469 35003 sgd_solver.cpp:112] Iteration 97090, lr = 0.01
I0523 00:04:19.278993 35003 solver.cpp:239] Iteration 97100 (3.53806 iter/s, 2.82641s/10 iters), loss = 7.92443
I0523 00:04:19.279052 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.92443 (* 1 = 7.92443 loss)
I0523 00:04:19.993849 35003 sgd_solver.cpp:112] Iteration 97100, lr = 0.01
I0523 00:04:23.316911 35003 solver.cpp:239] Iteration 97110 (2.47666 iter/s, 4.03769s/10 iters), loss = 8.44143
I0523 00:04:23.316952 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.44143 (* 1 = 8.44143 loss)
I0523 00:04:23.323334 35003 sgd_solver.cpp:112] Iteration 97110, lr = 0.01
I0523 00:04:25.181005 35003 solver.cpp:239] Iteration 97120 (5.36499 iter/s, 1.86393s/10 iters), loss = 7.65791
I0523 00:04:25.181059 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65791 (* 1 = 7.65791 loss)
I0523 00:04:25.864768 35003 sgd_solver.cpp:112] Iteration 97120, lr = 0.01
I0523 00:04:30.341735 35003 solver.cpp:239] Iteration 97130 (1.93781 iter/s, 5.16047s/10 iters), loss = 7.4674
I0523 00:04:30.341774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4674 (* 1 = 7.4674 loss)
I0523 00:04:31.050328 35003 sgd_solver.cpp:112] Iteration 97130, lr = 0.01
I0523 00:04:34.930124 35003 solver.cpp:239] Iteration 97140 (2.17952 iter/s, 4.58816s/10 iters), loss = 7.79559
I0523 00:04:34.930209 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79559 (* 1 = 7.79559 loss)
I0523 00:04:34.942937 35003 sgd_solver.cpp:112] Iteration 97140, lr = 0.01
I0523 00:04:37.163970 35003 solver.cpp:239] Iteration 97150 (4.47695 iter/s, 2.23366s/10 iters), loss = 7.0879
I0523 00:04:37.164007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0879 (* 1 = 7.0879 loss)
I0523 00:04:37.879400 35003 sgd_solver.cpp:112] Iteration 97150, lr = 0.01
I0523 00:04:40.902230 35003 solver.cpp:239] Iteration 97160 (2.6752 iter/s, 3.73804s/10 iters), loss = 7.97774
I0523 00:04:40.902269 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97774 (* 1 = 7.97774 loss)
I0523 00:04:40.915465 35003 sgd_solver.cpp:112] Iteration 97160, lr = 0.01
I0523 00:04:45.229207 35003 solver.cpp:239] Iteration 97170 (2.3112 iter/s, 4.32676s/10 iters), loss = 7.49825
I0523 00:04:45.229250 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49825 (* 1 = 7.49825 loss)
I0523 00:04:45.248603 35003 sgd_solver.cpp:112] Iteration 97170, lr = 0.01
I0523 00:04:47.593248 35003 solver.cpp:239] Iteration 97180 (4.23032 iter/s, 2.36389s/10 iters), loss = 6.20221
I0523 00:04:47.593309 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20221 (* 1 = 6.20221 loss)
I0523 00:04:47.601282 35003 sgd_solver.cpp:112] Iteration 97180, lr = 0.01
I0523 00:04:50.475000 35003 solver.cpp:239] Iteration 97190 (3.47034 iter/s, 2.88156s/10 iters), loss = 7.87237
I0523 00:04:50.475069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87237 (* 1 = 7.87237 loss)
I0523 00:04:50.580742 35003 sgd_solver.cpp:112] Iteration 97190, lr = 0.01
I0523 00:04:52.929139 35003 solver.cpp:239] Iteration 97200 (4.07505 iter/s, 2.45396s/10 iters), loss = 6.80598
I0523 00:04:52.929191 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80598 (* 1 = 6.80598 loss)
I0523 00:04:52.939781 35003 sgd_solver.cpp:112] Iteration 97200, lr = 0.01
I0523 00:04:55.811414 35003 solver.cpp:239] Iteration 97210 (3.46969 iter/s, 2.8821s/10 iters), loss = 8.30543
I0523 00:04:55.811462 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.30543 (* 1 = 8.30543 loss)
I0523 00:04:56.539649 35003 sgd_solver.cpp:112] Iteration 97210, lr = 0.01
I0523 00:04:59.438719 35003 solver.cpp:239] Iteration 97220 (2.75704 iter/s, 3.62708s/10 iters), loss = 7.53931
I0523 00:04:59.438771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53931 (* 1 = 7.53931 loss)
I0523 00:04:59.457115 35003 sgd_solver.cpp:112] Iteration 97220, lr = 0.01
I0523 00:05:02.238076 35003 solver.cpp:239] Iteration 97230 (3.57249 iter/s, 2.79917s/10 iters), loss = 8.05905
I0523 00:05:02.238137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05905 (* 1 = 8.05905 loss)
I0523 00:05:02.969943 35003 sgd_solver.cpp:112] Iteration 97230, lr = 0.01
I0523 00:05:07.317809 35003 solver.cpp:239] Iteration 97240 (1.96871 iter/s, 5.07947s/10 iters), loss = 7.18818
I0523 00:05:07.317957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18818 (* 1 = 7.18818 loss)
I0523 00:05:07.329614 35003 sgd_solver.cpp:112] Iteration 97240, lr = 0.01
I0523 00:05:10.932986 35003 solver.cpp:239] Iteration 97250 (2.76635 iter/s, 3.61488s/10 iters), loss = 8.26828
I0523 00:05:10.933030 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.26828 (* 1 = 8.26828 loss)
I0523 00:05:11.628921 35003 sgd_solver.cpp:112] Iteration 97250, lr = 0.01
I0523 00:05:15.895339 35003 solver.cpp:239] Iteration 97260 (2.01527 iter/s, 4.96211s/10 iters), loss = 7.49725
I0523 00:05:15.895392 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49725 (* 1 = 7.49725 loss)
I0523 00:05:16.018126 35003 sgd_solver.cpp:112] Iteration 97260, lr = 0.01
I0523 00:05:22.239861 35003 solver.cpp:239] Iteration 97270 (1.57624 iter/s, 6.34421s/10 iters), loss = 7.30513
I0523 00:05:22.239912 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30513 (* 1 = 7.30513 loss)
I0523 00:05:22.275329 35003 sgd_solver.cpp:112] Iteration 97270, lr = 0.01
I0523 00:05:26.518076 35003 solver.cpp:239] Iteration 97280 (2.33755 iter/s, 4.27799s/10 iters), loss = 7.86689
I0523 00:05:26.518126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86689 (* 1 = 7.86689 loss)
I0523 00:05:27.259534 35003 sgd_solver.cpp:112] Iteration 97280, lr = 0.01
I0523 00:05:30.552296 35003 solver.cpp:239] Iteration 97290 (2.47893 iter/s, 4.034s/10 iters), loss = 6.99649
I0523 00:05:30.552335 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99649 (* 1 = 6.99649 loss)
I0523 00:05:30.565627 35003 sgd_solver.cpp:112] Iteration 97290, lr = 0.01
I0523 00:05:33.374605 35003 solver.cpp:239] Iteration 97300 (3.54341 iter/s, 2.82214s/10 iters), loss = 7.43412
I0523 00:05:33.374661 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43412 (* 1 = 7.43412 loss)
I0523 00:05:33.380901 35003 sgd_solver.cpp:112] Iteration 97300, lr = 0.01
I0523 00:05:36.873077 35003 solver.cpp:239] Iteration 97310 (2.85856 iter/s, 3.49827s/10 iters), loss = 7.84871
I0523 00:05:36.873126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84871 (* 1 = 7.84871 loss)
I0523 00:05:37.587800 35003 sgd_solver.cpp:112] Iteration 97310, lr = 0.01
I0523 00:05:38.962796 35003 solver.cpp:239] Iteration 97320 (4.78567 iter/s, 2.08957s/10 iters), loss = 7.63303
I0523 00:05:38.962837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63303 (* 1 = 7.63303 loss)
I0523 00:05:38.967052 35003 sgd_solver.cpp:112] Iteration 97320, lr = 0.01
I0523 00:05:42.907665 35003 solver.cpp:239] Iteration 97330 (2.53528 iter/s, 3.94434s/10 iters), loss = 7.71225
I0523 00:05:42.907716 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71225 (* 1 = 7.71225 loss)
I0523 00:05:43.458350 35003 sgd_solver.cpp:112] Iteration 97330, lr = 0.01
I0523 00:05:46.862736 35003 solver.cpp:239] Iteration 97340 (2.52856 iter/s, 3.95482s/10 iters), loss = 6.87433
I0523 00:05:46.862781 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87433 (* 1 = 6.87433 loss)
I0523 00:05:47.603669 35003 sgd_solver.cpp:112] Iteration 97340, lr = 0.01
I0523 00:05:51.954041 35003 solver.cpp:239] Iteration 97350 (1.96423 iter/s, 5.09104s/10 iters), loss = 6.54423
I0523 00:05:51.954094 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54423 (* 1 = 6.54423 loss)
I0523 00:05:51.967682 35003 sgd_solver.cpp:112] Iteration 97350, lr = 0.01
I0523 00:05:54.848335 35003 solver.cpp:239] Iteration 97360 (3.45529 iter/s, 2.89411s/10 iters), loss = 8.02276
I0523 00:05:54.848388 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02276 (* 1 = 8.02276 loss)
I0523 00:05:54.852761 35003 sgd_solver.cpp:112] Iteration 97360, lr = 0.01
I0523 00:05:57.557893 35003 solver.cpp:239] Iteration 97370 (3.69087 iter/s, 2.70939s/10 iters), loss = 7.22924
I0523 00:05:57.557955 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22924 (* 1 = 7.22924 loss)
I0523 00:05:57.614434 35003 sgd_solver.cpp:112] Iteration 97370, lr = 0.01
I0523 00:06:01.904011 35003 solver.cpp:239] Iteration 97380 (2.30103 iter/s, 4.34588s/10 iters), loss = 6.60188
I0523 00:06:01.904057 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60188 (* 1 = 6.60188 loss)
I0523 00:06:02.628453 35003 sgd_solver.cpp:112] Iteration 97380, lr = 0.01
I0523 00:06:05.673643 35003 solver.cpp:239] Iteration 97390 (2.65295 iter/s, 3.76939s/10 iters), loss = 7.06185
I0523 00:06:05.673724 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06185 (* 1 = 7.06185 loss)
I0523 00:06:05.677814 35003 sgd_solver.cpp:112] Iteration 97390, lr = 0.01
I0523 00:06:09.026489 35003 solver.cpp:239] Iteration 97400 (2.98274 iter/s, 3.35262s/10 iters), loss = 7.38858
I0523 00:06:09.026649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38858 (* 1 = 7.38858 loss)
I0523 00:06:09.034091 35003 sgd_solver.cpp:112] Iteration 97400, lr = 0.01
I0523 00:06:12.642143 35003 solver.cpp:239] Iteration 97410 (2.76599 iter/s, 3.61534s/10 iters), loss = 7.44845
I0523 00:06:12.642189 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44845 (* 1 = 7.44845 loss)
I0523 00:06:12.655481 35003 sgd_solver.cpp:112] Iteration 97410, lr = 0.01
I0523 00:06:15.539147 35003 solver.cpp:239] Iteration 97420 (3.45204 iter/s, 2.89684s/10 iters), loss = 8.04091
I0523 00:06:15.539197 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04091 (* 1 = 8.04091 loss)
I0523 00:06:16.273749 35003 sgd_solver.cpp:112] Iteration 97420, lr = 0.01
I0523 00:06:20.604024 35003 solver.cpp:239] Iteration 97430 (1.97448 iter/s, 5.06462s/10 iters), loss = 7.45582
I0523 00:06:20.604069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45582 (* 1 = 7.45582 loss)
I0523 00:06:21.002243 35003 sgd_solver.cpp:112] Iteration 97430, lr = 0.01
I0523 00:06:24.750365 35003 solver.cpp:239] Iteration 97440 (2.41189 iter/s, 4.14612s/10 iters), loss = 7.53533
I0523 00:06:24.750422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53533 (* 1 = 7.53533 loss)
I0523 00:06:25.445432 35003 sgd_solver.cpp:112] Iteration 97440, lr = 0.01
I0523 00:06:29.334110 35003 solver.cpp:239] Iteration 97450 (2.18175 iter/s, 4.58349s/10 iters), loss = 6.94645
I0523 00:06:29.334158 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94645 (* 1 = 6.94645 loss)
I0523 00:06:29.340394 35003 sgd_solver.cpp:112] Iteration 97450, lr = 0.01
I0523 00:06:32.225446 35003 solver.cpp:239] Iteration 97460 (3.45882 iter/s, 2.89116s/10 iters), loss = 7.72221
I0523 00:06:32.225498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72221 (* 1 = 7.72221 loss)
I0523 00:06:32.248366 35003 sgd_solver.cpp:112] Iteration 97460, lr = 0.01
I0523 00:06:35.259582 35003 solver.cpp:239] Iteration 97470 (3.29603 iter/s, 3.03396s/10 iters), loss = 8.21821
I0523 00:06:35.259631 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.21821 (* 1 = 8.21821 loss)
I0523 00:06:35.273057 35003 sgd_solver.cpp:112] Iteration 97470, lr = 0.01
I0523 00:06:38.131016 35003 solver.cpp:239] Iteration 97480 (3.48279 iter/s, 2.87126s/10 iters), loss = 6.39326
I0523 00:06:38.131072 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39326 (* 1 = 6.39326 loss)
I0523 00:06:38.858994 35003 sgd_solver.cpp:112] Iteration 97480, lr = 0.01
I0523 00:06:42.099299 35003 solver.cpp:239] Iteration 97490 (2.52012 iter/s, 3.96806s/10 iters), loss = 6.52945
I0523 00:06:42.099570 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52945 (* 1 = 6.52945 loss)
I0523 00:06:42.245754 35003 sgd_solver.cpp:112] Iteration 97490, lr = 0.01
I0523 00:06:45.777503 35003 solver.cpp:239] Iteration 97500 (2.71901 iter/s, 3.6778s/10 iters), loss = 7.06872
I0523 00:06:45.777546 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06872 (* 1 = 7.06872 loss)
I0523 00:06:45.795650 35003 sgd_solver.cpp:112] Iteration 97500, lr = 0.01
I0523 00:06:49.265693 35003 solver.cpp:239] Iteration 97510 (2.86697 iter/s, 3.488s/10 iters), loss = 8.32898
I0523 00:06:49.265738 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.32898 (* 1 = 8.32898 loss)
I0523 00:06:49.272742 35003 sgd_solver.cpp:112] Iteration 97510, lr = 0.01
I0523 00:06:51.391806 35003 solver.cpp:239] Iteration 97520 (4.70372 iter/s, 2.12598s/10 iters), loss = 7.34947
I0523 00:06:51.391841 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34947 (* 1 = 7.34947 loss)
I0523 00:06:51.403213 35003 sgd_solver.cpp:112] Iteration 97520, lr = 0.01
I0523 00:06:56.236176 35003 solver.cpp:239] Iteration 97530 (2.06436 iter/s, 4.84412s/10 iters), loss = 8.22193
I0523 00:06:56.236224 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.22193 (* 1 = 8.22193 loss)
I0523 00:06:56.253342 35003 sgd_solver.cpp:112] Iteration 97530, lr = 0.01
I0523 00:06:58.328541 35003 solver.cpp:239] Iteration 97540 (4.77961 iter/s, 2.09222s/10 iters), loss = 6.70467
I0523 00:06:58.328591 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70467 (* 1 = 6.70467 loss)
I0523 00:06:58.351294 35003 sgd_solver.cpp:112] Iteration 97540, lr = 0.01
I0523 00:07:00.453079 35003 solver.cpp:239] Iteration 97550 (4.70725 iter/s, 2.12438s/10 iters), loss = 7.32298
I0523 00:07:00.453135 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32298 (* 1 = 7.32298 loss)
I0523 00:07:00.486202 35003 sgd_solver.cpp:112] Iteration 97550, lr = 0.01
I0523 00:07:02.547783 35003 solver.cpp:239] Iteration 97560 (4.77428 iter/s, 2.09456s/10 iters), loss = 7.3829
I0523 00:07:02.547832 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3829 (* 1 = 7.3829 loss)
I0523 00:07:02.557675 35003 sgd_solver.cpp:112] Iteration 97560, lr = 0.01
I0523 00:07:06.151948 35003 solver.cpp:239] Iteration 97570 (2.77472 iter/s, 3.60397s/10 iters), loss = 6.39796
I0523 00:07:06.151995 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39796 (* 1 = 6.39796 loss)
I0523 00:07:06.160032 35003 sgd_solver.cpp:112] Iteration 97570, lr = 0.01
I0523 00:07:10.353948 35003 solver.cpp:239] Iteration 97580 (2.37995 iter/s, 4.20177s/10 iters), loss = 8.11932
I0523 00:07:10.353996 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.11932 (* 1 = 8.11932 loss)
I0523 00:07:10.367220 35003 sgd_solver.cpp:112] Iteration 97580, lr = 0.01
I0523 00:07:13.001968 35003 solver.cpp:239] Iteration 97590 (3.77664 iter/s, 2.64786s/10 iters), loss = 7.46899
I0523 00:07:13.002192 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46899 (* 1 = 7.46899 loss)
I0523 00:07:13.010493 35003 sgd_solver.cpp:112] Iteration 97590, lr = 0.01
I0523 00:07:17.045608 35003 solver.cpp:239] Iteration 97600 (2.47324 iter/s, 4.04327s/10 iters), loss = 6.85945
I0523 00:07:17.045651 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85945 (* 1 = 6.85945 loss)
I0523 00:07:17.073215 35003 sgd_solver.cpp:112] Iteration 97600, lr = 0.01
I0523 00:07:19.138130 35003 solver.cpp:239] Iteration 97610 (4.77925 iter/s, 2.09238s/10 iters), loss = 7.19565
I0523 00:07:19.138197 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19565 (* 1 = 7.19565 loss)
I0523 00:07:19.141275 35003 sgd_solver.cpp:112] Iteration 97610, lr = 0.01
I0523 00:07:23.608981 35003 solver.cpp:239] Iteration 97620 (2.23684 iter/s, 4.4706s/10 iters), loss = 7.50164
I0523 00:07:23.609031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50164 (* 1 = 7.50164 loss)
I0523 00:07:24.323992 35003 sgd_solver.cpp:112] Iteration 97620, lr = 0.01
I0523 00:07:27.084163 35003 solver.cpp:239] Iteration 97630 (2.87771 iter/s, 3.47498s/10 iters), loss = 7.92586
I0523 00:07:27.084215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.92586 (* 1 = 7.92586 loss)
I0523 00:07:27.095239 35003 sgd_solver.cpp:112] Iteration 97630, lr = 0.01
I0523 00:07:29.229817 35003 solver.cpp:239] Iteration 97640 (4.66091 iter/s, 2.1455s/10 iters), loss = 7.57568
I0523 00:07:29.229866 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57568 (* 1 = 7.57568 loss)
I0523 00:07:29.798383 35003 sgd_solver.cpp:112] Iteration 97640, lr = 0.01
I0523 00:07:32.698815 35003 solver.cpp:239] Iteration 97650 (2.88284 iter/s, 3.4688s/10 iters), loss = 6.90753
I0523 00:07:32.698874 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90753 (* 1 = 6.90753 loss)
I0523 00:07:33.427371 35003 sgd_solver.cpp:112] Iteration 97650, lr = 0.01
I0523 00:07:36.853562 35003 solver.cpp:239] Iteration 97660 (2.40702 iter/s, 4.15452s/10 iters), loss = 6.87859
I0523 00:07:36.853615 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87859 (* 1 = 6.87859 loss)
I0523 00:07:36.860039 35003 sgd_solver.cpp:112] Iteration 97660, lr = 0.01
I0523 00:07:40.576298 35003 solver.cpp:239] Iteration 97670 (2.68636 iter/s, 3.72251s/10 iters), loss = 7.54117
I0523 00:07:40.576359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54117 (* 1 = 7.54117 loss)
I0523 00:07:41.291563 35003 sgd_solver.cpp:112] Iteration 97670, lr = 0.01
I0523 00:07:43.437705 35003 solver.cpp:239] Iteration 97680 (3.49502 iter/s, 2.86122s/10 iters), loss = 6.46313
I0523 00:07:43.437978 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46313 (* 1 = 6.46313 loss)
I0523 00:07:44.171631 35003 sgd_solver.cpp:112] Iteration 97680, lr = 0.01
I0523 00:07:46.058678 35003 solver.cpp:239] Iteration 97690 (3.8159 iter/s, 2.62061s/10 iters), loss = 7.99442
I0523 00:07:46.058737 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99442 (* 1 = 7.99442 loss)
I0523 00:07:46.071769 35003 sgd_solver.cpp:112] Iteration 97690, lr = 0.01
I0523 00:07:48.364617 35003 solver.cpp:239] Iteration 97700 (4.33694 iter/s, 2.30577s/10 iters), loss = 7.63002
I0523 00:07:48.364660 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63002 (* 1 = 7.63002 loss)
I0523 00:07:48.371593 35003 sgd_solver.cpp:112] Iteration 97700, lr = 0.01
I0523 00:07:51.043450 35003 solver.cpp:239] Iteration 97710 (3.7332 iter/s, 2.67867s/10 iters), loss = 5.7183
I0523 00:07:51.043496 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7183 (* 1 = 5.7183 loss)
I0523 00:07:51.054190 35003 sgd_solver.cpp:112] Iteration 97710, lr = 0.01
I0523 00:07:53.932387 35003 solver.cpp:239] Iteration 97720 (3.46169 iter/s, 2.88876s/10 iters), loss = 7.13727
I0523 00:07:53.932442 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13727 (* 1 = 7.13727 loss)
I0523 00:07:54.673319 35003 sgd_solver.cpp:112] Iteration 97720, lr = 0.01
I0523 00:07:59.616196 35003 solver.cpp:239] Iteration 97730 (1.75948 iter/s, 5.68351s/10 iters), loss = 7.6574
I0523 00:07:59.616252 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6574 (* 1 = 7.6574 loss)
I0523 00:08:00.066392 35003 sgd_solver.cpp:112] Iteration 97730, lr = 0.01
I0523 00:08:03.134328 35003 solver.cpp:239] Iteration 97740 (2.8426 iter/s, 3.51791s/10 iters), loss = 8.27462
I0523 00:08:03.134377 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.27462 (* 1 = 8.27462 loss)
I0523 00:08:03.145790 35003 sgd_solver.cpp:112] Iteration 97740, lr = 0.01
I0523 00:08:04.984825 35003 solver.cpp:239] Iteration 97750 (5.40436 iter/s, 1.85036s/10 iters), loss = 6.32403
I0523 00:08:04.984868 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32403 (* 1 = 6.32403 loss)
I0523 00:08:05.003862 35003 sgd_solver.cpp:112] Iteration 97750, lr = 0.01
I0523 00:08:09.443192 35003 solver.cpp:239] Iteration 97760 (2.24309 iter/s, 4.45814s/10 iters), loss = 6.34842
I0523 00:08:09.443233 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34842 (* 1 = 6.34842 loss)
I0523 00:08:09.447757 35003 sgd_solver.cpp:112] Iteration 97760, lr = 0.01
I0523 00:08:11.788292 35003 solver.cpp:239] Iteration 97770 (4.26448 iter/s, 2.34495s/10 iters), loss = 6.39904
I0523 00:08:11.788344 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39904 (* 1 = 6.39904 loss)
I0523 00:08:12.529083 35003 sgd_solver.cpp:112] Iteration 97770, lr = 0.01
I0523 00:08:15.448884 35003 solver.cpp:239] Iteration 97780 (2.73195 iter/s, 3.66039s/10 iters), loss = 7.78347
I0523 00:08:15.449076 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78347 (* 1 = 7.78347 loss)
I0523 00:08:15.453130 35003 sgd_solver.cpp:112] Iteration 97780, lr = 0.01
I0523 00:08:19.007377 35003 solver.cpp:239] Iteration 97790 (2.81058 iter/s, 3.55799s/10 iters), loss = 7.47542
I0523 00:08:19.007426 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47542 (* 1 = 7.47542 loss)
I0523 00:08:19.020452 35003 sgd_solver.cpp:112] Iteration 97790, lr = 0.01
I0523 00:08:22.081465 35003 solver.cpp:239] Iteration 97800 (3.25319 iter/s, 3.07391s/10 iters), loss = 7.16839
I0523 00:08:22.081511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16839 (* 1 = 7.16839 loss)
I0523 00:08:22.091063 35003 sgd_solver.cpp:112] Iteration 97800, lr = 0.01
I0523 00:08:25.887159 35003 solver.cpp:239] Iteration 97810 (2.62779 iter/s, 3.80549s/10 iters), loss = 7.11594
I0523 00:08:25.887205 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11594 (* 1 = 7.11594 loss)
I0523 00:08:25.892762 35003 sgd_solver.cpp:112] Iteration 97810, lr = 0.01
I0523 00:08:29.742962 35003 solver.cpp:239] Iteration 97820 (2.59363 iter/s, 3.8556s/10 iters), loss = 7.09139
I0523 00:08:29.743014 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09139 (* 1 = 7.09139 loss)
I0523 00:08:30.428037 35003 sgd_solver.cpp:112] Iteration 97820, lr = 0.01
I0523 00:08:35.614186 35003 solver.cpp:239] Iteration 97830 (1.70331 iter/s, 5.87093s/10 iters), loss = 7.98875
I0523 00:08:35.614224 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98875 (* 1 = 7.98875 loss)
I0523 00:08:35.627604 35003 sgd_solver.cpp:112] Iteration 97830, lr = 0.01
I0523 00:08:39.027542 35003 solver.cpp:239] Iteration 97840 (2.92982 iter/s, 3.41318s/10 iters), loss = 6.45452
I0523 00:08:39.027585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45452 (* 1 = 6.45452 loss)
I0523 00:08:39.742889 35003 sgd_solver.cpp:112] Iteration 97840, lr = 0.01
I0523 00:08:42.598160 35003 solver.cpp:239] Iteration 97850 (2.80079 iter/s, 3.57042s/10 iters), loss = 5.94625
I0523 00:08:42.598214 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.94625 (* 1 = 5.94625 loss)
I0523 00:08:42.611496 35003 sgd_solver.cpp:112] Iteration 97850, lr = 0.01
I0523 00:08:46.220142 35003 solver.cpp:239] Iteration 97860 (2.76107 iter/s, 3.62178s/10 iters), loss = 7.48679
I0523 00:08:46.220330 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48679 (* 1 = 7.48679 loss)
I0523 00:08:46.233973 35003 sgd_solver.cpp:112] Iteration 97860, lr = 0.01
I0523 00:08:48.266749 35003 solver.cpp:239] Iteration 97870 (4.8868 iter/s, 2.04633s/10 iters), loss = 6.70097
I0523 00:08:48.266809 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70097 (* 1 = 6.70097 loss)
I0523 00:08:48.433869 35003 sgd_solver.cpp:112] Iteration 97870, lr = 0.01
I0523 00:08:52.197659 35003 solver.cpp:239] Iteration 97880 (2.54408 iter/s, 3.93069s/10 iters), loss = 7.45889
I0523 00:08:52.197700 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45889 (* 1 = 7.45889 loss)
I0523 00:08:52.211498 35003 sgd_solver.cpp:112] Iteration 97880, lr = 0.01
I0523 00:08:57.105016 35003 solver.cpp:239] Iteration 97890 (2.03786 iter/s, 4.90712s/10 iters), loss = 8.17105
I0523 00:08:57.105067 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17105 (* 1 = 8.17105 loss)
I0523 00:08:57.123647 35003 sgd_solver.cpp:112] Iteration 97890, lr = 0.01
I0523 00:09:00.607368 35003 solver.cpp:239] Iteration 97900 (2.85539 iter/s, 3.50215s/10 iters), loss = 7.30484
I0523 00:09:00.607409 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30484 (* 1 = 7.30484 loss)
I0523 00:09:00.612260 35003 sgd_solver.cpp:112] Iteration 97900, lr = 0.01
I0523 00:09:04.842861 35003 solver.cpp:239] Iteration 97910 (2.36112 iter/s, 4.23528s/10 iters), loss = 6.87366
I0523 00:09:04.842914 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87366 (* 1 = 6.87366 loss)
I0523 00:09:05.570833 35003 sgd_solver.cpp:112] Iteration 97910, lr = 0.01
I0523 00:09:09.273121 35003 solver.cpp:239] Iteration 97920 (2.25732 iter/s, 4.43003s/10 iters), loss = 8.05818
I0523 00:09:09.273177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05818 (* 1 = 8.05818 loss)
I0523 00:09:09.713016 35003 sgd_solver.cpp:112] Iteration 97920, lr = 0.01
I0523 00:09:12.898965 35003 solver.cpp:239] Iteration 97930 (2.75814 iter/s, 3.62563s/10 iters), loss = 7.31146
I0523 00:09:12.899011 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31146 (* 1 = 7.31146 loss)
I0523 00:09:13.620414 35003 sgd_solver.cpp:112] Iteration 97930, lr = 0.01
I0523 00:09:15.684566 35003 solver.cpp:239] Iteration 97940 (3.59011 iter/s, 2.78543s/10 iters), loss = 7.42312
I0523 00:09:15.684623 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42312 (* 1 = 7.42312 loss)
I0523 00:09:15.698928 35003 sgd_solver.cpp:112] Iteration 97940, lr = 0.01
I0523 00:09:20.123769 35003 solver.cpp:239] Iteration 97950 (2.25277 iter/s, 4.43897s/10 iters), loss = 6.73891
I0523 00:09:20.124063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73891 (* 1 = 6.73891 loss)
I0523 00:09:20.789331 35003 sgd_solver.cpp:112] Iteration 97950, lr = 0.01
I0523 00:09:23.125586 35003 solver.cpp:239] Iteration 97960 (3.33175 iter/s, 3.00143s/10 iters), loss = 6.55638
I0523 00:09:23.125629 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55638 (* 1 = 6.55638 loss)
I0523 00:09:23.749765 35003 sgd_solver.cpp:112] Iteration 97960, lr = 0.01
I0523 00:09:27.923359 35003 solver.cpp:239] Iteration 97970 (2.08441 iter/s, 4.79753s/10 iters), loss = 7.29692
I0523 00:09:27.923419 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29692 (* 1 = 7.29692 loss)
I0523 00:09:28.632304 35003 sgd_solver.cpp:112] Iteration 97970, lr = 0.01
I0523 00:09:31.444552 35003 solver.cpp:239] Iteration 97980 (2.84011 iter/s, 3.52099s/10 iters), loss = 6.85304
I0523 00:09:31.444608 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85304 (* 1 = 6.85304 loss)
I0523 00:09:31.457399 35003 sgd_solver.cpp:112] Iteration 97980, lr = 0.01
I0523 00:09:35.256882 35003 solver.cpp:239] Iteration 97990 (2.62323 iter/s, 3.8121s/10 iters), loss = 8.60522
I0523 00:09:35.256953 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.60522 (* 1 = 8.60522 loss)
I0523 00:09:35.282573 35003 sgd_solver.cpp:112] Iteration 97990, lr = 0.01
I0523 00:09:39.364038 35003 solver.cpp:239] Iteration 98000 (2.43491 iter/s, 4.10692s/10 iters), loss = 6.76059
I0523 00:09:39.364076 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76059 (* 1 = 6.76059 loss)
I0523 00:09:39.376843 35003 sgd_solver.cpp:112] Iteration 98000, lr = 0.01
I0523 00:09:40.848577 35003 solver.cpp:239] Iteration 98010 (6.7366 iter/s, 1.48443s/10 iters), loss = 7.50792
I0523 00:09:40.848615 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50792 (* 1 = 7.50792 loss)
I0523 00:09:40.866752 35003 sgd_solver.cpp:112] Iteration 98010, lr = 0.01
I0523 00:09:44.431769 35003 solver.cpp:239] Iteration 98020 (2.79096 iter/s, 3.583s/10 iters), loss = 7.04229
I0523 00:09:44.431813 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04229 (* 1 = 7.04229 loss)
I0523 00:09:44.444154 35003 sgd_solver.cpp:112] Iteration 98020, lr = 0.01
I0523 00:09:49.144570 35003 solver.cpp:239] Iteration 98030 (2.12199 iter/s, 4.71256s/10 iters), loss = 7.93089
I0523 00:09:49.144619 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93089 (* 1 = 7.93089 loss)
I0523 00:09:49.152467 35003 sgd_solver.cpp:112] Iteration 98030, lr = 0.01
I0523 00:09:52.163417 35003 solver.cpp:239] Iteration 98040 (3.31272 iter/s, 3.01867s/10 iters), loss = 8.01224
I0523 00:09:52.163718 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01224 (* 1 = 8.01224 loss)
I0523 00:09:52.897768 35003 sgd_solver.cpp:112] Iteration 98040, lr = 0.01
I0523 00:09:54.982790 35003 solver.cpp:239] Iteration 98050 (3.54739 iter/s, 2.81897s/10 iters), loss = 7.22951
I0523 00:09:54.982851 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22951 (* 1 = 7.22951 loss)
I0523 00:09:54.988169 35003 sgd_solver.cpp:112] Iteration 98050, lr = 0.01
I0523 00:09:57.068498 35003 solver.cpp:239] Iteration 98060 (4.7949 iter/s, 2.08555s/10 iters), loss = 6.88703
I0523 00:09:57.068549 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88703 (* 1 = 6.88703 loss)
I0523 00:09:57.803894 35003 sgd_solver.cpp:112] Iteration 98060, lr = 0.01
I0523 00:10:00.730736 35003 solver.cpp:239] Iteration 98070 (2.73072 iter/s, 3.66204s/10 iters), loss = 7.28928
I0523 00:10:00.730769 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28928 (* 1 = 7.28928 loss)
I0523 00:10:00.743621 35003 sgd_solver.cpp:112] Iteration 98070, lr = 0.01
I0523 00:10:03.600589 35003 solver.cpp:239] Iteration 98080 (3.48469 iter/s, 2.8697s/10 iters), loss = 7.78128
I0523 00:10:03.600632 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78128 (* 1 = 7.78128 loss)
I0523 00:10:03.606268 35003 sgd_solver.cpp:112] Iteration 98080, lr = 0.01
I0523 00:10:06.402638 35003 solver.cpp:239] Iteration 98090 (3.56904 iter/s, 2.80187s/10 iters), loss = 7.45046
I0523 00:10:06.402684 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45046 (* 1 = 7.45046 loss)
I0523 00:10:06.411438 35003 sgd_solver.cpp:112] Iteration 98090, lr = 0.01
I0523 00:10:09.251472 35003 solver.cpp:239] Iteration 98100 (3.51041 iter/s, 2.84867s/10 iters), loss = 7.9426
I0523 00:10:09.251514 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9426 (* 1 = 7.9426 loss)
I0523 00:10:09.262634 35003 sgd_solver.cpp:112] Iteration 98100, lr = 0.01
I0523 00:10:14.364567 35003 solver.cpp:239] Iteration 98110 (1.95586 iter/s, 5.11283s/10 iters), loss = 8.08963
I0523 00:10:14.364625 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08963 (* 1 = 8.08963 loss)
I0523 00:10:14.368589 35003 sgd_solver.cpp:112] Iteration 98110, lr = 0.01
I0523 00:10:16.999958 35003 solver.cpp:239] Iteration 98120 (3.79476 iter/s, 2.63521s/10 iters), loss = 7.02118
I0523 00:10:17.000008 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02118 (* 1 = 7.02118 loss)
I0523 00:10:17.005350 35003 sgd_solver.cpp:112] Iteration 98120, lr = 0.01
I0523 00:10:19.086132 35003 solver.cpp:239] Iteration 98130 (4.79379 iter/s, 2.08603s/10 iters), loss = 7.51055
I0523 00:10:19.086176 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51055 (* 1 = 7.51055 loss)
I0523 00:10:19.109822 35003 sgd_solver.cpp:112] Iteration 98130, lr = 0.01
I0523 00:10:22.771961 35003 solver.cpp:239] Iteration 98140 (2.71324 iter/s, 3.68563s/10 iters), loss = 7.06262
I0523 00:10:22.772231 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06262 (* 1 = 7.06262 loss)
I0523 00:10:22.857004 35003 sgd_solver.cpp:112] Iteration 98140, lr = 0.01
I0523 00:10:26.896410 35003 solver.cpp:239] Iteration 98150 (2.42481 iter/s, 4.12403s/10 iters), loss = 7.25528
I0523 00:10:26.896461 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25528 (* 1 = 7.25528 loss)
I0523 00:10:26.900944 35003 sgd_solver.cpp:112] Iteration 98150, lr = 0.01
I0523 00:10:29.532117 35003 solver.cpp:239] Iteration 98160 (3.7943 iter/s, 2.63553s/10 iters), loss = 7.07515
I0523 00:10:29.532166 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07515 (* 1 = 7.07515 loss)
I0523 00:10:29.541148 35003 sgd_solver.cpp:112] Iteration 98160, lr = 0.01
I0523 00:10:31.476622 35003 solver.cpp:239] Iteration 98170 (5.14305 iter/s, 1.94437s/10 iters), loss = 5.83503
I0523 00:10:31.476662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.83503 (* 1 = 5.83503 loss)
I0523 00:10:31.480082 35003 sgd_solver.cpp:112] Iteration 98170, lr = 0.01
I0523 00:10:36.534816 35003 solver.cpp:239] Iteration 98180 (1.97709 iter/s, 5.05794s/10 iters), loss = 7.93525
I0523 00:10:36.534870 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93525 (* 1 = 7.93525 loss)
I0523 00:10:36.538367 35003 sgd_solver.cpp:112] Iteration 98180, lr = 0.01
I0523 00:10:39.343262 35003 solver.cpp:239] Iteration 98190 (3.5609 iter/s, 2.80828s/10 iters), loss = 7.70589
I0523 00:10:39.343309 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70589 (* 1 = 7.70589 loss)
I0523 00:10:39.479749 35003 sgd_solver.cpp:112] Iteration 98190, lr = 0.01
I0523 00:10:44.587466 35003 solver.cpp:239] Iteration 98200 (1.90696 iter/s, 5.24394s/10 iters), loss = 8.1524
I0523 00:10:44.587519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.1524 (* 1 = 8.1524 loss)
I0523 00:10:44.595150 35003 sgd_solver.cpp:112] Iteration 98200, lr = 0.01
I0523 00:10:47.908922 35003 solver.cpp:239] Iteration 98210 (3.0109 iter/s, 3.32127s/10 iters), loss = 6.49227
I0523 00:10:47.908963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49227 (* 1 = 6.49227 loss)
I0523 00:10:47.923243 35003 sgd_solver.cpp:112] Iteration 98210, lr = 0.01
I0523 00:10:51.499866 35003 solver.cpp:239] Iteration 98220 (2.78494 iter/s, 3.59074s/10 iters), loss = 7.12971
I0523 00:10:51.499922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12971 (* 1 = 7.12971 loss)
I0523 00:10:51.818961 35003 sgd_solver.cpp:112] Iteration 98220, lr = 0.01
I0523 00:10:55.669718 35003 solver.cpp:239] Iteration 98230 (2.3983 iter/s, 4.16962s/10 iters), loss = 7.63335
I0523 00:10:55.669976 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63335 (* 1 = 7.63335 loss)
I0523 00:10:55.687914 35003 sgd_solver.cpp:112] Iteration 98230, lr = 0.01
I0523 00:10:59.026319 35003 solver.cpp:239] Iteration 98240 (2.97954 iter/s, 3.35622s/10 iters), loss = 6.43316
I0523 00:10:59.026365 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43316 (* 1 = 6.43316 loss)
I0523 00:10:59.044751 35003 sgd_solver.cpp:112] Iteration 98240, lr = 0.01
I0523 00:11:01.142673 35003 solver.cpp:239] Iteration 98250 (4.72542 iter/s, 2.11621s/10 iters), loss = 7.9186
I0523 00:11:01.142755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9186 (* 1 = 7.9186 loss)
I0523 00:11:01.148257 35003 sgd_solver.cpp:112] Iteration 98250, lr = 0.01
I0523 00:11:03.948034 35003 solver.cpp:239] Iteration 98260 (3.56486 iter/s, 2.80516s/10 iters), loss = 7.28545
I0523 00:11:03.948084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28545 (* 1 = 7.28545 loss)
I0523 00:11:04.632606 35003 sgd_solver.cpp:112] Iteration 98260, lr = 0.01
I0523 00:11:06.916405 35003 solver.cpp:239] Iteration 98270 (3.36905 iter/s, 2.9682s/10 iters), loss = 7.74095
I0523 00:11:06.916450 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74095 (* 1 = 7.74095 loss)
I0523 00:11:07.613601 35003 sgd_solver.cpp:112] Iteration 98270, lr = 0.01
I0523 00:11:11.884166 35003 solver.cpp:239] Iteration 98280 (2.01308 iter/s, 4.96751s/10 iters), loss = 6.42985
I0523 00:11:11.884214 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42985 (* 1 = 6.42985 loss)
I0523 00:11:11.894047 35003 sgd_solver.cpp:112] Iteration 98280, lr = 0.01
I0523 00:11:15.465786 35003 solver.cpp:239] Iteration 98290 (2.7922 iter/s, 3.58141s/10 iters), loss = 7.23453
I0523 00:11:15.465847 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23453 (* 1 = 7.23453 loss)
I0523 00:11:15.783917 35003 sgd_solver.cpp:112] Iteration 98290, lr = 0.01
I0523 00:11:20.802845 35003 solver.cpp:239] Iteration 98300 (1.8738 iter/s, 5.33676s/10 iters), loss = 8.47758
I0523 00:11:20.802923 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.47758 (* 1 = 8.47758 loss)
I0523 00:11:20.809212 35003 sgd_solver.cpp:112] Iteration 98300, lr = 0.01
I0523 00:11:23.689193 35003 solver.cpp:239] Iteration 98310 (3.46483 iter/s, 2.88615s/10 iters), loss = 8.0131
I0523 00:11:23.689244 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0131 (* 1 = 8.0131 loss)
I0523 00:11:24.410586 35003 sgd_solver.cpp:112] Iteration 98310, lr = 0.01
I0523 00:11:28.014621 35003 solver.cpp:239] Iteration 98320 (2.31204 iter/s, 4.32519s/10 iters), loss = 6.36649
I0523 00:11:28.014955 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36649 (* 1 = 6.36649 loss)
I0523 00:11:28.027892 35003 sgd_solver.cpp:112] Iteration 98320, lr = 0.01
I0523 00:11:30.313208 35003 solver.cpp:239] Iteration 98330 (4.35128 iter/s, 2.29817s/10 iters), loss = 7.80059
I0523 00:11:30.313283 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80059 (* 1 = 7.80059 loss)
I0523 00:11:30.317463 35003 sgd_solver.cpp:112] Iteration 98330, lr = 0.01
I0523 00:11:33.149615 35003 solver.cpp:239] Iteration 98340 (3.52583 iter/s, 2.83621s/10 iters), loss = 6.32845
I0523 00:11:33.149662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32845 (* 1 = 6.32845 loss)
I0523 00:11:33.856462 35003 sgd_solver.cpp:112] Iteration 98340, lr = 0.01
I0523 00:11:36.722614 35003 solver.cpp:239] Iteration 98350 (2.79892 iter/s, 3.5728s/10 iters), loss = 6.66037
I0523 00:11:36.722666 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66037 (* 1 = 6.66037 loss)
I0523 00:11:37.417883 35003 sgd_solver.cpp:112] Iteration 98350, lr = 0.01
I0523 00:11:39.580618 35003 solver.cpp:239] Iteration 98360 (3.49916 iter/s, 2.85783s/10 iters), loss = 7.96609
I0523 00:11:39.580667 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96609 (* 1 = 7.96609 loss)
I0523 00:11:40.226678 35003 sgd_solver.cpp:112] Iteration 98360, lr = 0.01
I0523 00:11:44.745820 35003 solver.cpp:239] Iteration 98370 (1.93613 iter/s, 5.16495s/10 iters), loss = 6.96157
I0523 00:11:44.745858 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96157 (* 1 = 6.96157 loss)
I0523 00:11:44.764498 35003 sgd_solver.cpp:112] Iteration 98370, lr = 0.01
I0523 00:11:47.150413 35003 solver.cpp:239] Iteration 98380 (4.15896 iter/s, 2.40445s/10 iters), loss = 6.9909
I0523 00:11:47.150462 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9909 (* 1 = 6.9909 loss)
I0523 00:11:47.167652 35003 sgd_solver.cpp:112] Iteration 98380, lr = 0.01
I0523 00:11:50.665946 35003 solver.cpp:239] Iteration 98390 (2.84468 iter/s, 3.51534s/10 iters), loss = 7.99672
I0523 00:11:50.665990 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99672 (* 1 = 7.99672 loss)
I0523 00:11:51.300334 35003 sgd_solver.cpp:112] Iteration 98390, lr = 0.01
I0523 00:11:54.187424 35003 solver.cpp:239] Iteration 98400 (2.83987 iter/s, 3.52128s/10 iters), loss = 7.63659
I0523 00:11:54.187470 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63659 (* 1 = 7.63659 loss)
I0523 00:11:54.198240 35003 sgd_solver.cpp:112] Iteration 98400, lr = 0.01
I0523 00:11:57.132385 35003 solver.cpp:239] Iteration 98410 (3.39584 iter/s, 2.94478s/10 iters), loss = 7.22812
I0523 00:11:57.132436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22812 (* 1 = 7.22812 loss)
I0523 00:11:57.142153 35003 sgd_solver.cpp:112] Iteration 98410, lr = 0.01
I0523 00:12:00.672219 35003 solver.cpp:239] Iteration 98420 (2.82516 iter/s, 3.53963s/10 iters), loss = 7.09419
I0523 00:12:00.672446 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09419 (* 1 = 7.09419 loss)
I0523 00:12:01.292284 35003 sgd_solver.cpp:112] Iteration 98420, lr = 0.01
I0523 00:12:05.741041 35003 solver.cpp:239] Iteration 98430 (1.97301 iter/s, 5.06839s/10 iters), loss = 7.44679
I0523 00:12:05.741096 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44679 (* 1 = 7.44679 loss)
I0523 00:12:05.748549 35003 sgd_solver.cpp:112] Iteration 98430, lr = 0.01
I0523 00:12:08.890564 35003 solver.cpp:239] Iteration 98440 (3.17527 iter/s, 3.14934s/10 iters), loss = 6.56997
I0523 00:12:08.890604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56997 (* 1 = 6.56997 loss)
I0523 00:12:09.593235 35003 sgd_solver.cpp:112] Iteration 98440, lr = 0.01
I0523 00:12:12.125658 35003 solver.cpp:239] Iteration 98450 (3.09127 iter/s, 3.23492s/10 iters), loss = 7.18326
I0523 00:12:12.125710 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18326 (* 1 = 7.18326 loss)
I0523 00:12:12.130326 35003 sgd_solver.cpp:112] Iteration 98450, lr = 0.01
I0523 00:12:15.706254 35003 solver.cpp:239] Iteration 98460 (2.79299 iter/s, 3.58039s/10 iters), loss = 6.42041
I0523 00:12:15.706315 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42041 (* 1 = 6.42041 loss)
I0523 00:12:15.718634 35003 sgd_solver.cpp:112] Iteration 98460, lr = 0.01
I0523 00:12:17.813206 35003 solver.cpp:239] Iteration 98470 (4.74658 iter/s, 2.10678s/10 iters), loss = 7.25025
I0523 00:12:17.813246 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25025 (* 1 = 7.25025 loss)
I0523 00:12:17.826365 35003 sgd_solver.cpp:112] Iteration 98470, lr = 0.01
I0523 00:12:19.831508 35003 solver.cpp:239] Iteration 98480 (4.95498 iter/s, 2.01817s/10 iters), loss = 6.50561
I0523 00:12:19.831567 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50561 (* 1 = 6.50561 loss)
I0523 00:12:19.844388 35003 sgd_solver.cpp:112] Iteration 98480, lr = 0.01
I0523 00:12:21.877990 35003 solver.cpp:239] Iteration 98490 (4.88677 iter/s, 2.04634s/10 iters), loss = 7.59143
I0523 00:12:21.878028 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59143 (* 1 = 7.59143 loss)
I0523 00:12:21.890763 35003 sgd_solver.cpp:112] Iteration 98490, lr = 0.01
I0523 00:12:23.915338 35003 solver.cpp:239] Iteration 98500 (4.90867 iter/s, 2.03721s/10 iters), loss = 7.58097
I0523 00:12:23.915393 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58097 (* 1 = 7.58097 loss)
I0523 00:12:23.928843 35003 sgd_solver.cpp:112] Iteration 98500, lr = 0.01
I0523 00:12:27.395500 35003 solver.cpp:239] Iteration 98510 (2.8736 iter/s, 3.47996s/10 iters), loss = 7.19768
I0523 00:12:27.395553 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19768 (* 1 = 7.19768 loss)
I0523 00:12:27.408735 35003 sgd_solver.cpp:112] Iteration 98510, lr = 0.01
I0523 00:12:30.696924 35003 solver.cpp:239] Iteration 98520 (3.02917 iter/s, 3.30123s/10 iters), loss = 6.68736
I0523 00:12:30.697135 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68736 (* 1 = 6.68736 loss)
I0523 00:12:31.431938 35003 sgd_solver.cpp:112] Iteration 98520, lr = 0.01
I0523 00:12:33.524045 35003 solver.cpp:239] Iteration 98530 (3.53758 iter/s, 2.82679s/10 iters), loss = 7.92036
I0523 00:12:33.524106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.92036 (* 1 = 7.92036 loss)
I0523 00:12:34.232533 35003 sgd_solver.cpp:112] Iteration 98530, lr = 0.01
I0523 00:12:36.926460 35003 solver.cpp:239] Iteration 98540 (2.93929 iter/s, 3.40219s/10 iters), loss = 8.0442
I0523 00:12:36.926522 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0442 (* 1 = 8.0442 loss)
I0523 00:12:36.945111 35003 sgd_solver.cpp:112] Iteration 98540, lr = 0.01
I0523 00:12:38.498569 35003 solver.cpp:239] Iteration 98550 (6.36139 iter/s, 1.57198s/10 iters), loss = 7.47883
I0523 00:12:38.498612 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47883 (* 1 = 7.47883 loss)
I0523 00:12:38.505450 35003 sgd_solver.cpp:112] Iteration 98550, lr = 0.01
I0523 00:12:42.697113 35003 solver.cpp:239] Iteration 98560 (2.3819 iter/s, 4.19833s/10 iters), loss = 7.41161
I0523 00:12:42.697151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41161 (* 1 = 7.41161 loss)
I0523 00:12:42.705382 35003 sgd_solver.cpp:112] Iteration 98560, lr = 0.01
I0523 00:12:46.349818 35003 solver.cpp:239] Iteration 98570 (2.73784 iter/s, 3.65251s/10 iters), loss = 7.32394
I0523 00:12:46.349864 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32394 (* 1 = 7.32394 loss)
I0523 00:12:46.353344 35003 sgd_solver.cpp:112] Iteration 98570, lr = 0.01
I0523 00:12:49.494232 35003 solver.cpp:239] Iteration 98580 (3.18044 iter/s, 3.14422s/10 iters), loss = 8.68742
I0523 00:12:49.494292 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.68742 (* 1 = 8.68742 loss)
I0523 00:12:50.112884 35003 sgd_solver.cpp:112] Iteration 98580, lr = 0.01
I0523 00:12:52.989120 35003 solver.cpp:239] Iteration 98590 (2.86149 iter/s, 3.49468s/10 iters), loss = 6.83525
I0523 00:12:52.989181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83525 (* 1 = 6.83525 loss)
I0523 00:12:53.723100 35003 sgd_solver.cpp:112] Iteration 98590, lr = 0.01
I0523 00:12:57.198864 35003 solver.cpp:239] Iteration 98600 (2.37557 iter/s, 4.20952s/10 iters), loss = 7.80485
I0523 00:12:57.198911 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80485 (* 1 = 7.80485 loss)
I0523 00:12:57.218935 35003 sgd_solver.cpp:112] Iteration 98600, lr = 0.01
I0523 00:13:00.757697 35003 solver.cpp:239] Iteration 98610 (2.81007 iter/s, 3.55863s/10 iters), loss = 7.27939
I0523 00:13:00.757999 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27939 (* 1 = 7.27939 loss)
I0523 00:13:01.453516 35003 sgd_solver.cpp:112] Iteration 98610, lr = 0.01
I0523 00:13:05.298113 35003 solver.cpp:239] Iteration 98620 (2.20266 iter/s, 4.53996s/10 iters), loss = 7.61083
I0523 00:13:05.298157 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61083 (* 1 = 7.61083 loss)
I0523 00:13:05.311923 35003 sgd_solver.cpp:112] Iteration 98620, lr = 0.01
I0523 00:13:08.765851 35003 solver.cpp:239] Iteration 98630 (2.8839 iter/s, 3.46753s/10 iters), loss = 7.9965
I0523 00:13:08.765908 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9965 (* 1 = 7.9965 loss)
I0523 00:13:08.827265 35003 sgd_solver.cpp:112] Iteration 98630, lr = 0.01
I0523 00:13:11.732365 35003 solver.cpp:239] Iteration 98640 (3.37117 iter/s, 2.96633s/10 iters), loss = 6.50188
I0523 00:13:11.732412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50188 (* 1 = 6.50188 loss)
I0523 00:13:12.453577 35003 sgd_solver.cpp:112] Iteration 98640, lr = 0.01
I0523 00:13:16.643358 35003 solver.cpp:239] Iteration 98650 (2.03635 iter/s, 4.91074s/10 iters), loss = 7.41936
I0523 00:13:16.643407 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41936 (* 1 = 7.41936 loss)
I0523 00:13:16.649888 35003 sgd_solver.cpp:112] Iteration 98650, lr = 0.01
I0523 00:13:20.899446 35003 solver.cpp:239] Iteration 98660 (2.3497 iter/s, 4.25586s/10 iters), loss = 8.27036
I0523 00:13:20.899487 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.27036 (* 1 = 8.27036 loss)
I0523 00:13:21.588980 35003 sgd_solver.cpp:112] Iteration 98660, lr = 0.01
I0523 00:13:23.488540 35003 solver.cpp:239] Iteration 98670 (3.86258 iter/s, 2.58894s/10 iters), loss = 8.39331
I0523 00:13:23.488579 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.39331 (* 1 = 8.39331 loss)
I0523 00:13:23.495481 35003 sgd_solver.cpp:112] Iteration 98670, lr = 0.01
I0523 00:13:26.374332 35003 solver.cpp:239] Iteration 98680 (3.46545 iter/s, 2.88563s/10 iters), loss = 7.0262
I0523 00:13:26.374377 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0262 (* 1 = 7.0262 loss)
I0523 00:13:26.383821 35003 sgd_solver.cpp:112] Iteration 98680, lr = 0.01
I0523 00:13:30.360821 35003 solver.cpp:239] Iteration 98690 (2.5086 iter/s, 3.98628s/10 iters), loss = 6.46163
I0523 00:13:30.360867 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46163 (* 1 = 6.46163 loss)
I0523 00:13:30.379485 35003 sgd_solver.cpp:112] Iteration 98690, lr = 0.01
I0523 00:13:33.275329 35003 solver.cpp:239] Iteration 98700 (3.43133 iter/s, 2.91432s/10 iters), loss = 8.03321
I0523 00:13:33.275518 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03321 (* 1 = 8.03321 loss)
I0523 00:13:33.280982 35003 sgd_solver.cpp:112] Iteration 98700, lr = 0.01
I0523 00:13:37.588591 35003 solver.cpp:239] Iteration 98710 (2.31863 iter/s, 4.31289s/10 iters), loss = 6.85001
I0523 00:13:37.588655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85001 (* 1 = 6.85001 loss)
I0523 00:13:37.593708 35003 sgd_solver.cpp:112] Iteration 98710, lr = 0.01
I0523 00:13:40.971953 35003 solver.cpp:239] Iteration 98720 (2.95582 iter/s, 3.38316s/10 iters), loss = 7.17895
I0523 00:13:40.971998 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17895 (* 1 = 7.17895 loss)
I0523 00:13:41.188204 35003 sgd_solver.cpp:112] Iteration 98720, lr = 0.01
I0523 00:13:45.614321 35003 solver.cpp:239] Iteration 98730 (2.15419 iter/s, 4.64213s/10 iters), loss = 6.90462
I0523 00:13:45.614372 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90462 (* 1 = 6.90462 loss)
I0523 00:13:46.221197 35003 sgd_solver.cpp:112] Iteration 98730, lr = 0.01
I0523 00:13:48.387584 35003 solver.cpp:239] Iteration 98740 (3.60608 iter/s, 2.7731s/10 iters), loss = 8.54268
I0523 00:13:48.387636 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.54268 (* 1 = 8.54268 loss)
I0523 00:13:49.071064 35003 sgd_solver.cpp:112] Iteration 98740, lr = 0.01
I0523 00:13:53.972887 35003 solver.cpp:239] Iteration 98750 (1.7905 iter/s, 5.58502s/10 iters), loss = 7.43458
I0523 00:13:53.972936 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43458 (* 1 = 7.43458 loss)
I0523 00:13:53.985023 35003 sgd_solver.cpp:112] Iteration 98750, lr = 0.01
I0523 00:13:56.801151 35003 solver.cpp:239] Iteration 98760 (3.53595 iter/s, 2.82809s/10 iters), loss = 7.83811
I0523 00:13:56.801198 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83811 (* 1 = 7.83811 loss)
I0523 00:13:56.814121 35003 sgd_solver.cpp:112] Iteration 98760, lr = 0.01
I0523 00:14:00.315845 35003 solver.cpp:239] Iteration 98770 (2.84536 iter/s, 3.51449s/10 iters), loss = 6.47352
I0523 00:14:00.315892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47352 (* 1 = 6.47352 loss)
I0523 00:14:00.323778 35003 sgd_solver.cpp:112] Iteration 98770, lr = 0.01
I0523 00:14:04.857875 35003 solver.cpp:239] Iteration 98780 (2.20177 iter/s, 4.54179s/10 iters), loss = 7.62434
I0523 00:14:04.858072 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62434 (* 1 = 7.62434 loss)
I0523 00:14:05.586226 35003 sgd_solver.cpp:112] Iteration 98780, lr = 0.01
I0523 00:14:08.444959 35003 solver.cpp:239] Iteration 98790 (2.78802 iter/s, 3.58677s/10 iters), loss = 7.55146
I0523 00:14:08.445003 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55146 (* 1 = 7.55146 loss)
I0523 00:14:08.466503 35003 sgd_solver.cpp:112] Iteration 98790, lr = 0.01
I0523 00:14:12.768934 35003 solver.cpp:239] Iteration 98800 (2.31281 iter/s, 4.32375s/10 iters), loss = 7.17787
I0523 00:14:12.768976 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17787 (* 1 = 7.17787 loss)
I0523 00:14:12.792889 35003 sgd_solver.cpp:112] Iteration 98800, lr = 0.01
I0523 00:14:15.865334 35003 solver.cpp:239] Iteration 98810 (3.22974 iter/s, 3.09622s/10 iters), loss = 7.13845
I0523 00:14:15.865378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13845 (* 1 = 7.13845 loss)
I0523 00:14:16.603065 35003 sgd_solver.cpp:112] Iteration 98810, lr = 0.01
I0523 00:14:19.470850 35003 solver.cpp:239] Iteration 98820 (2.77368 iter/s, 3.60532s/10 iters), loss = 7.72236
I0523 00:14:19.470909 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72236 (* 1 = 7.72236 loss)
I0523 00:14:20.165278 35003 sgd_solver.cpp:112] Iteration 98820, lr = 0.01
I0523 00:14:23.235133 35003 solver.cpp:239] Iteration 98830 (2.6567 iter/s, 3.76406s/10 iters), loss = 7.63528
I0523 00:14:23.235183 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63528 (* 1 = 7.63528 loss)
I0523 00:14:23.239888 35003 sgd_solver.cpp:112] Iteration 98830, lr = 0.01
I0523 00:14:27.607334 35003 solver.cpp:239] Iteration 98840 (2.2873 iter/s, 4.37197s/10 iters), loss = 8.50174
I0523 00:14:27.607385 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.50174 (* 1 = 8.50174 loss)
I0523 00:14:27.621909 35003 sgd_solver.cpp:112] Iteration 98840, lr = 0.01
I0523 00:14:31.245674 35003 solver.cpp:239] Iteration 98850 (2.74866 iter/s, 3.63814s/10 iters), loss = 7.52393
I0523 00:14:31.245728 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52393 (* 1 = 7.52393 loss)
I0523 00:14:31.248366 35003 sgd_solver.cpp:112] Iteration 98850, lr = 0.01
I0523 00:14:34.773156 35003 solver.cpp:239] Iteration 98860 (2.83505 iter/s, 3.52728s/10 iters), loss = 6.14895
I0523 00:14:34.773200 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14895 (* 1 = 6.14895 loss)
I0523 00:14:34.786303 35003 sgd_solver.cpp:112] Iteration 98860, lr = 0.01
I0523 00:14:37.699981 35003 solver.cpp:239] Iteration 98870 (3.41687 iter/s, 2.92666s/10 iters), loss = 8.02628
I0523 00:14:37.700225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02628 (* 1 = 8.02628 loss)
I0523 00:14:38.420907 35003 sgd_solver.cpp:112] Iteration 98870, lr = 0.01
I0523 00:14:41.314311 35003 solver.cpp:239] Iteration 98880 (2.76704 iter/s, 3.61397s/10 iters), loss = 7.39711
I0523 00:14:41.314350 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39711 (* 1 = 7.39711 loss)
I0523 00:14:41.332906 35003 sgd_solver.cpp:112] Iteration 98880, lr = 0.01
I0523 00:14:45.746762 35003 solver.cpp:239] Iteration 98890 (2.2562 iter/s, 4.43223s/10 iters), loss = 7.48019
I0523 00:14:45.746809 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48019 (* 1 = 7.48019 loss)
I0523 00:14:45.753870 35003 sgd_solver.cpp:112] Iteration 98890, lr = 0.01
I0523 00:14:49.331818 35003 solver.cpp:239] Iteration 98900 (2.78951 iter/s, 3.58485s/10 iters), loss = 7.6401
I0523 00:14:49.331902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6401 (* 1 = 7.6401 loss)
I0523 00:14:49.337427 35003 sgd_solver.cpp:112] Iteration 98900, lr = 0.01
I0523 00:14:54.337152 35003 solver.cpp:239] Iteration 98910 (1.99851 iter/s, 5.00374s/10 iters), loss = 6.67538
I0523 00:14:54.337209 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67538 (* 1 = 6.67538 loss)
I0523 00:14:54.350605 35003 sgd_solver.cpp:112] Iteration 98910, lr = 0.01
I0523 00:14:59.209905 35003 solver.cpp:239] Iteration 98920 (2.05233 iter/s, 4.87251s/10 iters), loss = 6.61249
I0523 00:14:59.209945 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61249 (* 1 = 6.61249 loss)
I0523 00:14:59.924490 35003 sgd_solver.cpp:112] Iteration 98920, lr = 0.01
I0523 00:15:04.230079 35003 solver.cpp:239] Iteration 98930 (1.99206 iter/s, 5.01992s/10 iters), loss = 7.71753
I0523 00:15:04.230136 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71753 (* 1 = 7.71753 loss)
I0523 00:15:04.931907 35003 sgd_solver.cpp:112] Iteration 98930, lr = 0.01
I0523 00:15:07.925742 35003 solver.cpp:239] Iteration 98940 (2.70603 iter/s, 3.69545s/10 iters), loss = 8.19255
I0523 00:15:07.925828 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.19255 (* 1 = 8.19255 loss)
I0523 00:15:07.944582 35003 sgd_solver.cpp:112] Iteration 98940, lr = 0.01
I0523 00:15:12.156605 35003 solver.cpp:239] Iteration 98950 (2.36373 iter/s, 4.2306s/10 iters), loss = 7.53857
I0523 00:15:12.156661 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53857 (* 1 = 7.53857 loss)
I0523 00:15:12.160059 35003 sgd_solver.cpp:112] Iteration 98950, lr = 0.01
I0523 00:15:14.910040 35003 solver.cpp:239] Iteration 98960 (3.63205 iter/s, 2.75327s/10 iters), loss = 7.9019
I0523 00:15:14.910081 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9019 (* 1 = 7.9019 loss)
I0523 00:15:14.923121 35003 sgd_solver.cpp:112] Iteration 98960, lr = 0.01
I0523 00:15:19.084846 35003 solver.cpp:239] Iteration 98970 (2.39545 iter/s, 4.17459s/10 iters), loss = 7.0094
I0523 00:15:19.084897 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0094 (* 1 = 7.0094 loss)
I0523 00:15:19.252984 35003 sgd_solver.cpp:112] Iteration 98970, lr = 0.01
I0523 00:15:21.935755 35003 solver.cpp:239] Iteration 98980 (3.50787 iter/s, 2.85074s/10 iters), loss = 6.98856
I0523 00:15:21.935797 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98856 (* 1 = 6.98856 loss)
I0523 00:15:22.657672 35003 sgd_solver.cpp:112] Iteration 98980, lr = 0.01
I0523 00:15:27.718986 35003 solver.cpp:239] Iteration 98990 (1.72922 iter/s, 5.78295s/10 iters), loss = 8.05856
I0523 00:15:27.719038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05856 (* 1 = 8.05856 loss)
I0523 00:15:27.742260 35003 sgd_solver.cpp:112] Iteration 98990, lr = 0.01
I0523 00:15:29.057642 35003 solver.cpp:239] Iteration 99000 (7.47082 iter/s, 1.33854s/10 iters), loss = 7.37951
I0523 00:15:29.057682 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37951 (* 1 = 7.37951 loss)
I0523 00:15:29.063709 35003 sgd_solver.cpp:112] Iteration 99000, lr = 0.01
I0523 00:15:32.824126 35003 solver.cpp:239] Iteration 99010 (2.65514 iter/s, 3.76628s/10 iters), loss = 6.30459
I0523 00:15:32.824164 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30459 (* 1 = 6.30459 loss)
I0523 00:15:32.836423 35003 sgd_solver.cpp:112] Iteration 99010, lr = 0.01
I0523 00:15:34.725356 35003 solver.cpp:239] Iteration 99020 (5.26019 iter/s, 1.90107s/10 iters), loss = 6.77279
I0523 00:15:34.725401 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77279 (* 1 = 6.77279 loss)
I0523 00:15:34.734058 35003 sgd_solver.cpp:112] Iteration 99020, lr = 0.01
I0523 00:15:39.024312 35003 solver.cpp:239] Iteration 99030 (2.32626 iter/s, 4.29874s/10 iters), loss = 7.39009
I0523 00:15:39.024471 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39009 (* 1 = 7.39009 loss)
I0523 00:15:39.033365 35003 sgd_solver.cpp:112] Iteration 99030, lr = 0.01
I0523 00:15:41.900188 35003 solver.cpp:239] Iteration 99040 (3.47754 iter/s, 2.8756s/10 iters), loss = 8.09834
I0523 00:15:41.900241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.09834 (* 1 = 8.09834 loss)
I0523 00:15:41.913506 35003 sgd_solver.cpp:112] Iteration 99040, lr = 0.01
I0523 00:15:45.486681 35003 solver.cpp:239] Iteration 99050 (2.7884 iter/s, 3.58629s/10 iters), loss = 7.72764
I0523 00:15:45.486768 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72764 (* 1 = 7.72764 loss)
I0523 00:15:45.494895 35003 sgd_solver.cpp:112] Iteration 99050, lr = 0.01
I0523 00:15:48.640923 35003 solver.cpp:239] Iteration 99060 (3.17056 iter/s, 3.15402s/10 iters), loss = 8.03714
I0523 00:15:48.640971 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03714 (* 1 = 8.03714 loss)
I0523 00:15:49.347930 35003 sgd_solver.cpp:112] Iteration 99060, lr = 0.01
I0523 00:15:53.652987 35003 solver.cpp:239] Iteration 99070 (1.99529 iter/s, 5.01181s/10 iters), loss = 6.87567
I0523 00:15:53.653034 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87567 (* 1 = 6.87567 loss)
I0523 00:15:53.674505 35003 sgd_solver.cpp:112] Iteration 99070, lr = 0.01
I0523 00:15:55.231567 35003 solver.cpp:239] Iteration 99080 (6.33529 iter/s, 1.57846s/10 iters), loss = 7.09613
I0523 00:15:55.231621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09613 (* 1 = 7.09613 loss)
I0523 00:15:55.852870 35003 sgd_solver.cpp:112] Iteration 99080, lr = 0.01
I0523 00:15:59.062355 35003 solver.cpp:239] Iteration 99090 (2.61058 iter/s, 3.83057s/10 iters), loss = 7.65275
I0523 00:15:59.062394 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65275 (* 1 = 7.65275 loss)
I0523 00:15:59.796494 35003 sgd_solver.cpp:112] Iteration 99090, lr = 0.01
I0523 00:16:03.271844 35003 solver.cpp:239] Iteration 99100 (2.37571 iter/s, 4.20927s/10 iters), loss = 8.26167
I0523 00:16:03.271900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.26167 (* 1 = 8.26167 loss)
I0523 00:16:04.012540 35003 sgd_solver.cpp:112] Iteration 99100, lr = 0.01
I0523 00:16:07.873080 35003 solver.cpp:239] Iteration 99110 (2.17344 iter/s, 4.60099s/10 iters), loss = 7.00754
I0523 00:16:07.873121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00754 (* 1 = 7.00754 loss)
I0523 00:16:07.886076 35003 sgd_solver.cpp:112] Iteration 99110, lr = 0.01
I0523 00:16:09.200276 35003 solver.cpp:239] Iteration 99120 (7.53531 iter/s, 1.32709s/10 iters), loss = 6.4224
I0523 00:16:09.200590 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4224 (* 1 = 6.4224 loss)
I0523 00:16:09.226454 35003 sgd_solver.cpp:112] Iteration 99120, lr = 0.01
I0523 00:16:12.999603 35003 solver.cpp:239] Iteration 99130 (2.63533 iter/s, 3.79459s/10 iters), loss = 6.07348
I0523 00:16:12.999641 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07348 (* 1 = 6.07348 loss)
I0523 00:16:13.012359 35003 sgd_solver.cpp:112] Iteration 99130, lr = 0.01
I0523 00:16:16.557488 35003 solver.cpp:239] Iteration 99140 (2.81081 iter/s, 3.5577s/10 iters), loss = 7.35083
I0523 00:16:16.557538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35083 (* 1 = 7.35083 loss)
I0523 00:16:16.571724 35003 sgd_solver.cpp:112] Iteration 99140, lr = 0.01
I0523 00:16:20.930904 35003 solver.cpp:239] Iteration 99150 (2.28666 iter/s, 4.37319s/10 iters), loss = 7.51168
I0523 00:16:20.930948 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51168 (* 1 = 7.51168 loss)
I0523 00:16:20.973320 35003 sgd_solver.cpp:112] Iteration 99150, lr = 0.01
I0523 00:16:22.785037 35003 solver.cpp:239] Iteration 99160 (5.39376 iter/s, 1.85399s/10 iters), loss = 8.60335
I0523 00:16:22.785078 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.60335 (* 1 = 8.60335 loss)
I0523 00:16:22.797507 35003 sgd_solver.cpp:112] Iteration 99160, lr = 0.01
I0523 00:16:25.521937 35003 solver.cpp:239] Iteration 99170 (3.65399 iter/s, 2.73674s/10 iters), loss = 7.84334
I0523 00:16:25.521993 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84334 (* 1 = 7.84334 loss)
I0523 00:16:26.087759 35003 sgd_solver.cpp:112] Iteration 99170, lr = 0.01
I0523 00:16:27.446763 35003 solver.cpp:239] Iteration 99180 (5.19565 iter/s, 1.92469s/10 iters), loss = 8.11063
I0523 00:16:27.446807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.11063 (* 1 = 8.11063 loss)
I0523 00:16:28.181108 35003 sgd_solver.cpp:112] Iteration 99180, lr = 0.01
I0523 00:16:31.061383 35003 solver.cpp:239] Iteration 99190 (2.76669 iter/s, 3.61443s/10 iters), loss = 7.71546
I0523 00:16:31.061439 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71546 (* 1 = 7.71546 loss)
I0523 00:16:31.067067 35003 sgd_solver.cpp:112] Iteration 99190, lr = 0.01
I0523 00:16:33.131623 35003 solver.cpp:239] Iteration 99200 (4.8307 iter/s, 2.07009s/10 iters), loss = 7.93905
I0523 00:16:33.131670 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93905 (* 1 = 7.93905 loss)
I0523 00:16:33.144297 35003 sgd_solver.cpp:112] Iteration 99200, lr = 0.01
I0523 00:16:37.741035 35003 solver.cpp:239] Iteration 99210 (2.16959 iter/s, 4.60917s/10 iters), loss = 6.91725
I0523 00:16:37.741083 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91725 (* 1 = 6.91725 loss)
I0523 00:16:37.759517 35003 sgd_solver.cpp:112] Iteration 99210, lr = 0.01
I0523 00:16:39.894608 35003 solver.cpp:239] Iteration 99220 (4.64376 iter/s, 2.15343s/10 iters), loss = 7.25837
I0523 00:16:39.894872 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25837 (* 1 = 7.25837 loss)
I0523 00:16:40.635604 35003 sgd_solver.cpp:112] Iteration 99220, lr = 0.01
I0523 00:16:44.185066 35003 solver.cpp:239] Iteration 99230 (2.33098 iter/s, 4.29004s/10 iters), loss = 7.49176
I0523 00:16:44.185119 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49176 (* 1 = 7.49176 loss)
I0523 00:16:44.189532 35003 sgd_solver.cpp:112] Iteration 99230, lr = 0.01
I0523 00:16:46.896250 35003 solver.cpp:239] Iteration 99240 (3.68866 iter/s, 2.71101s/10 iters), loss = 6.93424
I0523 00:16:46.896289 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93424 (* 1 = 6.93424 loss)
I0523 00:16:46.902974 35003 sgd_solver.cpp:112] Iteration 99240, lr = 0.01
I0523 00:16:49.198603 35003 solver.cpp:239] Iteration 99250 (4.34366 iter/s, 2.3022s/10 iters), loss = 6.75018
I0523 00:16:49.198657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75018 (* 1 = 6.75018 loss)
I0523 00:16:49.206323 35003 sgd_solver.cpp:112] Iteration 99250, lr = 0.01
I0523 00:16:51.975076 35003 solver.cpp:239] Iteration 99260 (3.60191 iter/s, 2.7763s/10 iters), loss = 7.94073
I0523 00:16:51.975126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94073 (* 1 = 7.94073 loss)
I0523 00:16:52.715952 35003 sgd_solver.cpp:112] Iteration 99260, lr = 0.01
I0523 00:16:55.954555 35003 solver.cpp:239] Iteration 99270 (2.51303 iter/s, 3.97927s/10 iters), loss = 7.65133
I0523 00:16:55.954602 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65133 (* 1 = 7.65133 loss)
I0523 00:16:55.999692 35003 sgd_solver.cpp:112] Iteration 99270, lr = 0.01
I0523 00:16:58.751082 35003 solver.cpp:239] Iteration 99280 (3.57608 iter/s, 2.79636s/10 iters), loss = 7.0292
I0523 00:16:58.751132 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0292 (* 1 = 7.0292 loss)
I0523 00:16:58.756304 35003 sgd_solver.cpp:112] Iteration 99280, lr = 0.01
I0523 00:17:01.576334 35003 solver.cpp:239] Iteration 99290 (3.53972 iter/s, 2.82508s/10 iters), loss = 7.71838
I0523 00:17:01.576376 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71838 (* 1 = 7.71838 loss)
I0523 00:17:01.589740 35003 sgd_solver.cpp:112] Iteration 99290, lr = 0.01
I0523 00:17:05.790277 35003 solver.cpp:239] Iteration 99300 (2.3732 iter/s, 4.21373s/10 iters), loss = 6.33598
I0523 00:17:05.790326 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33598 (* 1 = 6.33598 loss)
I0523 00:17:05.836699 35003 sgd_solver.cpp:112] Iteration 99300, lr = 0.01
I0523 00:17:09.344421 35003 solver.cpp:239] Iteration 99310 (2.81377 iter/s, 3.55395s/10 iters), loss = 7.34606
I0523 00:17:09.344463 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34606 (* 1 = 7.34606 loss)
I0523 00:17:09.360683 35003 sgd_solver.cpp:112] Iteration 99310, lr = 0.01
I0523 00:17:11.934774 35003 solver.cpp:239] Iteration 99320 (3.8607 iter/s, 2.5902s/10 iters), loss = 7.2774
I0523 00:17:11.935050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2774 (* 1 = 7.2774 loss)
I0523 00:17:11.945957 35003 sgd_solver.cpp:112] Iteration 99320, lr = 0.01
I0523 00:17:15.550626 35003 solver.cpp:239] Iteration 99330 (2.7659 iter/s, 3.61545s/10 iters), loss = 6.21304
I0523 00:17:15.550676 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21304 (* 1 = 6.21304 loss)
I0523 00:17:15.561991 35003 sgd_solver.cpp:112] Iteration 99330, lr = 0.01
I0523 00:17:19.067492 35003 solver.cpp:239] Iteration 99340 (2.8436 iter/s, 3.51667s/10 iters), loss = 6.28948
I0523 00:17:19.067538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28948 (* 1 = 6.28948 loss)
I0523 00:17:19.806859 35003 sgd_solver.cpp:112] Iteration 99340, lr = 0.01
I0523 00:17:23.332903 35003 solver.cpp:239] Iteration 99350 (2.34456 iter/s, 4.26519s/10 iters), loss = 7.53255
I0523 00:17:23.332942 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53255 (* 1 = 7.53255 loss)
I0523 00:17:23.346619 35003 sgd_solver.cpp:112] Iteration 99350, lr = 0.01
I0523 00:17:26.061570 35003 solver.cpp:239] Iteration 99360 (3.665 iter/s, 2.72851s/10 iters), loss = 6.72603
I0523 00:17:26.061619 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72603 (* 1 = 6.72603 loss)
I0523 00:17:26.598589 35003 sgd_solver.cpp:112] Iteration 99360, lr = 0.01
I0523 00:17:30.516296 35003 solver.cpp:239] Iteration 99370 (2.24493 iter/s, 4.45449s/10 iters), loss = 8.0322
I0523 00:17:30.516352 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0322 (* 1 = 8.0322 loss)
I0523 00:17:31.257340 35003 sgd_solver.cpp:112] Iteration 99370, lr = 0.01
I0523 00:17:33.823920 35003 solver.cpp:239] Iteration 99380 (3.0235 iter/s, 3.30743s/10 iters), loss = 6.11519
I0523 00:17:33.823977 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11519 (* 1 = 6.11519 loss)
I0523 00:17:34.564678 35003 sgd_solver.cpp:112] Iteration 99380, lr = 0.01
I0523 00:17:37.424019 35003 solver.cpp:239] Iteration 99390 (2.77787 iter/s, 3.59988s/10 iters), loss = 7.31129
I0523 00:17:37.424089 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31129 (* 1 = 7.31129 loss)
I0523 00:17:38.138715 35003 sgd_solver.cpp:112] Iteration 99390, lr = 0.01
I0523 00:17:40.160934 35003 solver.cpp:239] Iteration 99400 (3.654 iter/s, 2.73673s/10 iters), loss = 6.87274
I0523 00:17:40.160975 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87274 (* 1 = 6.87274 loss)
I0523 00:17:40.169286 35003 sgd_solver.cpp:112] Iteration 99400, lr = 0.01
I0523 00:17:42.246085 35003 solver.cpp:239] Iteration 99410 (4.79612 iter/s, 2.08502s/10 iters), loss = 6.96286
I0523 00:17:42.246285 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96286 (* 1 = 6.96286 loss)
I0523 00:17:42.254496 35003 sgd_solver.cpp:112] Iteration 99410, lr = 0.01
I0523 00:17:46.045136 35003 solver.cpp:239] Iteration 99420 (2.63249 iter/s, 3.79869s/10 iters), loss = 8.05734
I0523 00:17:46.045192 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05734 (* 1 = 8.05734 loss)
I0523 00:17:46.056452 35003 sgd_solver.cpp:112] Iteration 99420, lr = 0.01
I0523 00:17:48.846776 35003 solver.cpp:239] Iteration 99430 (3.56956 iter/s, 2.80147s/10 iters), loss = 7.88156
I0523 00:17:48.846825 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88156 (* 1 = 7.88156 loss)
I0523 00:17:48.973722 35003 sgd_solver.cpp:112] Iteration 99430, lr = 0.01
I0523 00:17:51.847518 35003 solver.cpp:239] Iteration 99440 (3.3327 iter/s, 3.00057s/10 iters), loss = 6.99095
I0523 00:17:51.847563 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99095 (* 1 = 6.99095 loss)
I0523 00:17:51.860774 35003 sgd_solver.cpp:112] Iteration 99440, lr = 0.01
I0523 00:17:55.448014 35003 solver.cpp:239] Iteration 99450 (2.77755 iter/s, 3.6003s/10 iters), loss = 7.19777
I0523 00:17:55.448055 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19777 (* 1 = 7.19777 loss)
I0523 00:17:55.466145 35003 sgd_solver.cpp:112] Iteration 99450, lr = 0.01
I0523 00:17:56.843809 35003 solver.cpp:239] Iteration 99460 (7.16493 iter/s, 1.39569s/10 iters), loss = 7.22389
I0523 00:17:56.843852 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22389 (* 1 = 7.22389 loss)
I0523 00:17:56.851701 35003 sgd_solver.cpp:112] Iteration 99460, lr = 0.01
I0523 00:18:02.025655 35003 solver.cpp:239] Iteration 99470 (1.92991 iter/s, 5.18158s/10 iters), loss = 6.84281
I0523 00:18:02.025702 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84281 (* 1 = 6.84281 loss)
I0523 00:18:02.029559 35003 sgd_solver.cpp:112] Iteration 99470, lr = 0.01
I0523 00:18:07.574410 35003 solver.cpp:239] Iteration 99480 (1.8023 iter/s, 5.54848s/10 iters), loss = 7.1166
I0523 00:18:07.574455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1166 (* 1 = 7.1166 loss)
I0523 00:18:07.582267 35003 sgd_solver.cpp:112] Iteration 99480, lr = 0.01
I0523 00:18:10.435735 35003 solver.cpp:239] Iteration 99490 (3.49509 iter/s, 2.86115s/10 iters), loss = 7.87928
I0523 00:18:10.435789 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87928 (* 1 = 7.87928 loss)
I0523 00:18:11.131242 35003 sgd_solver.cpp:112] Iteration 99490, lr = 0.01
I0523 00:18:14.343611 35003 solver.cpp:239] Iteration 99500 (2.55907 iter/s, 3.90766s/10 iters), loss = 6.3949
I0523 00:18:14.343827 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3949 (* 1 = 6.3949 loss)
I0523 00:18:15.033149 35003 sgd_solver.cpp:112] Iteration 99500, lr = 0.01
I0523 00:18:16.358281 35003 solver.cpp:239] Iteration 99510 (4.96427 iter/s, 2.01439s/10 iters), loss = 7.45669
I0523 00:18:16.358325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45669 (* 1 = 7.45669 loss)
I0523 00:18:16.362363 35003 sgd_solver.cpp:112] Iteration 99510, lr = 0.01
I0523 00:18:20.672667 35003 solver.cpp:239] Iteration 99520 (2.31795 iter/s, 4.31415s/10 iters), loss = 8.02858
I0523 00:18:20.672739 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02858 (* 1 = 8.02858 loss)
I0523 00:18:21.069345 35003 sgd_solver.cpp:112] Iteration 99520, lr = 0.01
I0523 00:18:23.722574 35003 solver.cpp:239] Iteration 99530 (3.279 iter/s, 3.04971s/10 iters), loss = 5.98762
I0523 00:18:23.722612 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98762 (* 1 = 5.98762 loss)
I0523 00:18:23.735841 35003 sgd_solver.cpp:112] Iteration 99530, lr = 0.01
I0523 00:18:27.002132 35003 solver.cpp:239] Iteration 99540 (3.04936 iter/s, 3.27938s/10 iters), loss = 7.916
I0523 00:18:27.002179 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.916 (* 1 = 7.916 loss)
I0523 00:18:27.007629 35003 sgd_solver.cpp:112] Iteration 99540, lr = 0.01
I0523 00:18:30.900580 35003 solver.cpp:239] Iteration 99550 (2.56527 iter/s, 3.89823s/10 iters), loss = 7.04938
I0523 00:18:30.900630 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04938 (* 1 = 7.04938 loss)
I0523 00:18:31.609068 35003 sgd_solver.cpp:112] Iteration 99550, lr = 0.01
I0523 00:18:35.854800 35003 solver.cpp:239] Iteration 99560 (2.01858 iter/s, 4.95397s/10 iters), loss = 6.53786
I0523 00:18:35.854842 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53786 (* 1 = 6.53786 loss)
I0523 00:18:36.589824 35003 sgd_solver.cpp:112] Iteration 99560, lr = 0.01
I0523 00:18:40.150902 35003 solver.cpp:239] Iteration 99570 (2.32781 iter/s, 4.29588s/10 iters), loss = 6.85898
I0523 00:18:40.150954 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85898 (* 1 = 6.85898 loss)
I0523 00:18:40.172665 35003 sgd_solver.cpp:112] Iteration 99570, lr = 0.01
I0523 00:18:43.206193 35003 solver.cpp:239] Iteration 99580 (3.27321 iter/s, 3.0551s/10 iters), loss = 8.0415
I0523 00:18:43.206260 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0415 (* 1 = 8.0415 loss)
I0523 00:18:43.934454 35003 sgd_solver.cpp:112] Iteration 99580, lr = 0.01
I0523 00:18:48.217355 35003 solver.cpp:239] Iteration 99590 (1.99565 iter/s, 5.01089s/10 iters), loss = 6.07685
I0523 00:18:48.217492 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07685 (* 1 = 6.07685 loss)
I0523 00:18:48.228601 35003 sgd_solver.cpp:112] Iteration 99590, lr = 0.01
I0523 00:18:51.663090 35003 solver.cpp:239] Iteration 99600 (2.90237 iter/s, 3.44546s/10 iters), loss = 7.8497
I0523 00:18:51.663131 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8497 (* 1 = 7.8497 loss)
I0523 00:18:51.671344 35003 sgd_solver.cpp:112] Iteration 99600, lr = 0.01
I0523 00:18:54.284973 35003 solver.cpp:239] Iteration 99610 (3.81429 iter/s, 2.62172s/10 iters), loss = 8.46056
I0523 00:18:54.285038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.46056 (* 1 = 8.46056 loss)
I0523 00:18:54.291136 35003 sgd_solver.cpp:112] Iteration 99610, lr = 0.01
I0523 00:18:57.634238 35003 solver.cpp:239] Iteration 99620 (2.98591 iter/s, 3.34906s/10 iters), loss = 6.72523
I0523 00:18:57.634275 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72523 (* 1 = 6.72523 loss)
I0523 00:18:57.644459 35003 sgd_solver.cpp:112] Iteration 99620, lr = 0.01
I0523 00:19:02.491366 35003 solver.cpp:239] Iteration 99630 (2.05893 iter/s, 4.85688s/10 iters), loss = 7.25574
I0523 00:19:02.491420 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25574 (* 1 = 7.25574 loss)
I0523 00:19:03.225625 35003 sgd_solver.cpp:112] Iteration 99630, lr = 0.01
I0523 00:19:06.768159 35003 solver.cpp:239] Iteration 99640 (2.33833 iter/s, 4.27656s/10 iters), loss = 7.75064
I0523 00:19:06.768213 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75064 (* 1 = 7.75064 loss)
I0523 00:19:06.776886 35003 sgd_solver.cpp:112] Iteration 99640, lr = 0.01
I0523 00:19:09.988533 35003 solver.cpp:239] Iteration 99650 (3.10541 iter/s, 3.22019s/10 iters), loss = 8.05482
I0523 00:19:09.988576 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05482 (* 1 = 8.05482 loss)
I0523 00:19:10.002118 35003 sgd_solver.cpp:112] Iteration 99650, lr = 0.01
I0523 00:19:13.613606 35003 solver.cpp:239] Iteration 99660 (2.75872 iter/s, 3.62487s/10 iters), loss = 6.18289
I0523 00:19:13.613654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18289 (* 1 = 6.18289 loss)
I0523 00:19:13.626622 35003 sgd_solver.cpp:112] Iteration 99660, lr = 0.01
I0523 00:19:14.984926 35003 solver.cpp:239] Iteration 99670 (7.29286 iter/s, 1.3712s/10 iters), loss = 6.38204
I0523 00:19:14.984995 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38204 (* 1 = 6.38204 loss)
I0523 00:19:15.705986 35003 sgd_solver.cpp:112] Iteration 99670, lr = 0.01
I0523 00:19:18.485273 35003 solver.cpp:239] Iteration 99680 (2.85705 iter/s, 3.50011s/10 iters), loss = 6.29412
I0523 00:19:18.485536 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29412 (* 1 = 6.29412 loss)
I0523 00:19:18.498374 35003 sgd_solver.cpp:112] Iteration 99680, lr = 0.01
I0523 00:19:21.246456 35003 solver.cpp:239] Iteration 99690 (3.62775 iter/s, 2.75653s/10 iters), loss = 6.69052
I0523 00:19:21.246498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69052 (* 1 = 6.69052 loss)
I0523 00:19:21.260044 35003 sgd_solver.cpp:112] Iteration 99690, lr = 0.01
I0523 00:19:25.549362 35003 solver.cpp:239] Iteration 99700 (2.32413 iter/s, 4.30269s/10 iters), loss = 7.82107
I0523 00:19:25.549412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82107 (* 1 = 7.82107 loss)
I0523 00:19:25.563346 35003 sgd_solver.cpp:112] Iteration 99700, lr = 0.01
I0523 00:19:27.888211 35003 solver.cpp:239] Iteration 99710 (4.27589 iter/s, 2.3387s/10 iters), loss = 6.96114
I0523 00:19:27.888249 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96114 (* 1 = 6.96114 loss)
I0523 00:19:27.901406 35003 sgd_solver.cpp:112] Iteration 99710, lr = 0.01
I0523 00:19:30.684679 35003 solver.cpp:239] Iteration 99720 (3.57615 iter/s, 2.79631s/10 iters), loss = 8.15177
I0523 00:19:30.684718 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.15177 (* 1 = 8.15177 loss)
I0523 00:19:30.703287 35003 sgd_solver.cpp:112] Iteration 99720, lr = 0.01
I0523 00:19:32.810647 35003 solver.cpp:239] Iteration 99730 (4.70404 iter/s, 2.12583s/10 iters), loss = 8.95704
I0523 00:19:32.810734 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.95704 (* 1 = 8.95704 loss)
I0523 00:19:33.551406 35003 sgd_solver.cpp:112] Iteration 99730, lr = 0.01
I0523 00:19:37.093261 35003 solver.cpp:239] Iteration 99740 (2.33514 iter/s, 4.28239s/10 iters), loss = 6.93467
I0523 00:19:37.093304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93467 (* 1 = 6.93467 loss)
I0523 00:19:37.106542 35003 sgd_solver.cpp:112] Iteration 99740, lr = 0.01
I0523 00:19:38.435757 35003 solver.cpp:239] Iteration 99750 (7.4494 iter/s, 1.34239s/10 iters), loss = 6.86041
I0523 00:19:38.435794 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86041 (* 1 = 6.86041 loss)
I0523 00:19:38.454300 35003 sgd_solver.cpp:112] Iteration 99750, lr = 0.01
I0523 00:19:42.489058 35003 solver.cpp:239] Iteration 99760 (2.46725 iter/s, 4.0531s/10 iters), loss = 6.61995
I0523 00:19:42.489105 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61995 (* 1 = 6.61995 loss)
I0523 00:19:42.565322 35003 sgd_solver.cpp:112] Iteration 99760, lr = 0.01
I0523 00:19:46.221715 35003 solver.cpp:239] Iteration 99770 (2.6792 iter/s, 3.73245s/10 iters), loss = 7.83076
I0523 00:19:46.221767 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83076 (* 1 = 7.83076 loss)
I0523 00:19:46.242653 35003 sgd_solver.cpp:112] Iteration 99770, lr = 0.01
I0523 00:19:49.247503 35003 solver.cpp:239] Iteration 99780 (3.30514 iter/s, 3.02559s/10 iters), loss = 7.50219
I0523 00:19:49.247710 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50219 (* 1 = 7.50219 loss)
I0523 00:19:49.252614 35003 sgd_solver.cpp:112] Iteration 99780, lr = 0.01
I0523 00:19:52.148726 35003 solver.cpp:239] Iteration 99790 (3.44718 iter/s, 2.90092s/10 iters), loss = 6.94238
I0523 00:19:52.148777 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94238 (* 1 = 6.94238 loss)
I0523 00:19:52.163300 35003 sgd_solver.cpp:112] Iteration 99790, lr = 0.01
I0523 00:19:56.582578 35003 solver.cpp:239] Iteration 99800 (2.2555 iter/s, 4.43362s/10 iters), loss = 7.17927
I0523 00:19:56.582633 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17927 (* 1 = 7.17927 loss)
I0523 00:19:56.600703 35003 sgd_solver.cpp:112] Iteration 99800, lr = 0.01
I0523 00:20:00.157685 35003 solver.cpp:239] Iteration 99810 (2.79728 iter/s, 3.5749s/10 iters), loss = 6.58456
I0523 00:20:00.157730 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58456 (* 1 = 6.58456 loss)
I0523 00:20:00.170439 35003 sgd_solver.cpp:112] Iteration 99810, lr = 0.01
I0523 00:20:03.134253 35003 solver.cpp:239] Iteration 99820 (3.35976 iter/s, 2.9764s/10 iters), loss = 7.67639
I0523 00:20:03.134301 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67639 (* 1 = 7.67639 loss)
I0523 00:20:03.155933 35003 sgd_solver.cpp:112] Iteration 99820, lr = 0.01
I0523 00:20:08.078251 35003 solver.cpp:239] Iteration 99830 (2.02276 iter/s, 4.94374s/10 iters), loss = 7.83066
I0523 00:20:08.078310 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83066 (* 1 = 7.83066 loss)
I0523 00:20:08.085191 35003 sgd_solver.cpp:112] Iteration 99830, lr = 0.01
I0523 00:20:10.874464 35003 solver.cpp:239] Iteration 99840 (3.57649 iter/s, 2.79604s/10 iters), loss = 7.54156
I0523 00:20:10.874502 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54156 (* 1 = 7.54156 loss)
I0523 00:20:10.882091 35003 sgd_solver.cpp:112] Iteration 99840, lr = 0.01
I0523 00:20:13.549116 35003 solver.cpp:239] Iteration 99850 (3.73903 iter/s, 2.67449s/10 iters), loss = 7.03622
I0523 00:20:13.549165 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03622 (* 1 = 7.03622 loss)
I0523 00:20:13.556769 35003 sgd_solver.cpp:112] Iteration 99850, lr = 0.01
I0523 00:20:16.455135 35003 solver.cpp:239] Iteration 99860 (3.44134 iter/s, 2.90584s/10 iters), loss = 5.92092
I0523 00:20:16.455175 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92092 (* 1 = 5.92092 loss)
I0523 00:20:16.459285 35003 sgd_solver.cpp:112] Iteration 99860, lr = 0.01
I0523 00:20:20.527266 35003 solver.cpp:239] Iteration 99870 (2.45585 iter/s, 4.07192s/10 iters), loss = 6.24972
I0523 00:20:20.527560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24972 (* 1 = 6.24972 loss)
I0523 00:20:20.534867 35003 sgd_solver.cpp:112] Iteration 99870, lr = 0.01
I0523 00:20:24.063844 35003 solver.cpp:239] Iteration 99880 (2.82792 iter/s, 3.53616s/10 iters), loss = 7.70191
I0523 00:20:24.063887 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70191 (* 1 = 7.70191 loss)
I0523 00:20:24.072540 35003 sgd_solver.cpp:112] Iteration 99880, lr = 0.01
I0523 00:20:27.012871 35003 solver.cpp:239] Iteration 99890 (3.39114 iter/s, 2.94886s/10 iters), loss = 7.69247
I0523 00:20:27.012910 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69247 (* 1 = 7.69247 loss)
I0523 00:20:27.697600 35003 sgd_solver.cpp:112] Iteration 99890, lr = 0.01
I0523 00:20:30.284101 35003 solver.cpp:239] Iteration 99900 (3.05712 iter/s, 3.27105s/10 iters), loss = 7.53234
I0523 00:20:30.284150 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53234 (* 1 = 7.53234 loss)
I0523 00:20:30.793007 35003 sgd_solver.cpp:112] Iteration 99900, lr = 0.01
I0523 00:20:34.602067 35003 solver.cpp:239] Iteration 99910 (2.31603 iter/s, 4.31773s/10 iters), loss = 7.26059
I0523 00:20:34.602123 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26059 (* 1 = 7.26059 loss)
I0523 00:20:34.640944 35003 sgd_solver.cpp:112] Iteration 99910, lr = 0.01
I0523 00:20:37.746201 35003 solver.cpp:239] Iteration 99920 (3.18071 iter/s, 3.14395s/10 iters), loss = 7.17317
I0523 00:20:37.746240 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17317 (* 1 = 7.17317 loss)
I0523 00:20:38.226155 35003 sgd_solver.cpp:112] Iteration 99920, lr = 0.01
I0523 00:20:43.050817 35003 solver.cpp:239] Iteration 99930 (1.88524 iter/s, 5.30436s/10 iters), loss = 7.60734
I0523 00:20:43.050874 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60734 (* 1 = 7.60734 loss)
I0523 00:20:43.069075 35003 sgd_solver.cpp:112] Iteration 99930, lr = 0.01
I0523 00:20:48.091888 35003 solver.cpp:239] Iteration 99940 (1.98381 iter/s, 5.04081s/10 iters), loss = 7.73163
I0523 00:20:48.091934 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73163 (* 1 = 7.73163 loss)
I0523 00:20:48.110255 35003 sgd_solver.cpp:112] Iteration 99940, lr = 0.01
I0523 00:20:51.005682 35003 solver.cpp:239] Iteration 99950 (3.43215 iter/s, 2.91362s/10 iters), loss = 7.93705
I0523 00:20:51.006002 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93705 (* 1 = 7.93705 loss)
I0523 00:20:51.719756 35003 sgd_solver.cpp:112] Iteration 99950, lr = 0.01
I0523 00:20:54.554978 35003 solver.cpp:239] Iteration 99960 (2.81781 iter/s, 3.54886s/10 iters), loss = 7.411
I0523 00:20:54.555027 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.411 (* 1 = 7.411 loss)
I0523 00:20:54.563359 35003 sgd_solver.cpp:112] Iteration 99960, lr = 0.01
I0523 00:20:56.281560 35003 solver.cpp:239] Iteration 99970 (5.79222 iter/s, 1.72645s/10 iters), loss = 6.92587
I0523 00:20:56.281615 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92587 (* 1 = 6.92587 loss)
I0523 00:20:56.294806 35003 sgd_solver.cpp:112] Iteration 99970, lr = 0.01
I0523 00:20:59.906950 35003 solver.cpp:239] Iteration 99980 (2.75848 iter/s, 3.62519s/10 iters), loss = 7.57625
I0523 00:20:59.907001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57625 (* 1 = 7.57625 loss)
I0523 00:21:00.641274 35003 sgd_solver.cpp:112] Iteration 99980, lr = 0.01
I0523 00:21:04.914990 35003 solver.cpp:239] Iteration 99990 (1.99689 iter/s, 5.00778s/10 iters), loss = 8.5139
I0523 00:21:04.915050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.5139 (* 1 = 8.5139 loss)
I0523 00:21:04.937649 35003 sgd_solver.cpp:112] Iteration 99990, lr = 0.01
I0523 00:21:06.879794 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_100000.caffemodel
I0523 00:21:07.301748 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_100000.solverstate
I0523 00:21:07.475723 35003 solver.cpp:239] Iteration 100000 (3.90538 iter/s, 2.56057s/10 iters), loss = 6.95587
I0523 00:21:07.475775 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95587 (* 1 = 6.95587 loss)
I0523 00:21:07.485626 35003 sgd_solver.cpp:112] Iteration 100000, lr = 0.01
I0523 00:21:10.716323 35003 solver.cpp:239] Iteration 100010 (3.08602 iter/s, 3.24042s/10 iters), loss = 7.6604
I0523 00:21:10.716361 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6604 (* 1 = 7.6604 loss)
I0523 00:21:10.734576 35003 sgd_solver.cpp:112] Iteration 100010, lr = 0.01
I0523 00:21:14.786073 35003 solver.cpp:239] Iteration 100020 (2.45728 iter/s, 4.06954s/10 iters), loss = 6.58391
I0523 00:21:14.786118 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58391 (* 1 = 6.58391 loss)
I0523 00:21:15.494088 35003 sgd_solver.cpp:112] Iteration 100020, lr = 0.01
I0523 00:21:18.873716 35003 solver.cpp:239] Iteration 100030 (2.44652 iter/s, 4.08743s/10 iters), loss = 7.97641
I0523 00:21:18.873756 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97641 (* 1 = 7.97641 loss)
I0523 00:21:19.146802 35003 sgd_solver.cpp:112] Iteration 100030, lr = 0.01
I0523 00:21:22.726377 35003 solver.cpp:239] Iteration 100040 (2.59577 iter/s, 3.85242s/10 iters), loss = 6.95869
I0523 00:21:22.726568 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95869 (* 1 = 6.95869 loss)
I0523 00:21:23.467370 35003 sgd_solver.cpp:112] Iteration 100040, lr = 0.01
I0523 00:21:27.230046 35003 solver.cpp:239] Iteration 100050 (2.22059 iter/s, 4.5033s/10 iters), loss = 6.86832
I0523 00:21:27.230085 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86832 (* 1 = 6.86832 loss)
I0523 00:21:27.240803 35003 sgd_solver.cpp:112] Iteration 100050, lr = 0.01
I0523 00:21:31.061810 35003 solver.cpp:239] Iteration 100060 (2.6099 iter/s, 3.83157s/10 iters), loss = 6.80191
I0523 00:21:31.061854 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80191 (* 1 = 6.80191 loss)
I0523 00:21:31.074645 35003 sgd_solver.cpp:112] Iteration 100060, lr = 0.01
I0523 00:21:34.934535 35003 solver.cpp:239] Iteration 100070 (2.5823 iter/s, 3.87251s/10 iters), loss = 7.85392
I0523 00:21:34.934586 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85392 (* 1 = 7.85392 loss)
I0523 00:21:34.947425 35003 sgd_solver.cpp:112] Iteration 100070, lr = 0.01
I0523 00:21:38.633922 35003 solver.cpp:239] Iteration 100080 (2.7033 iter/s, 3.69919s/10 iters), loss = 8.03539
I0523 00:21:38.633970 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03539 (* 1 = 8.03539 loss)
I0523 00:21:38.642315 35003 sgd_solver.cpp:112] Iteration 100080, lr = 0.01
I0523 00:21:42.326532 35003 solver.cpp:239] Iteration 100090 (2.70826 iter/s, 3.69241s/10 iters), loss = 7.2994
I0523 00:21:42.326582 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2994 (* 1 = 7.2994 loss)
I0523 00:21:42.959535 35003 sgd_solver.cpp:112] Iteration 100090, lr = 0.01
I0523 00:21:46.744652 35003 solver.cpp:239] Iteration 100100 (2.26353 iter/s, 4.41788s/10 iters), loss = 7.00308
I0523 00:21:46.744699 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00308 (* 1 = 7.00308 loss)
I0523 00:21:46.769690 35003 sgd_solver.cpp:112] Iteration 100100, lr = 0.01
I0523 00:21:49.998292 35003 solver.cpp:239] Iteration 100110 (3.07366 iter/s, 3.25345s/10 iters), loss = 6.90906
I0523 00:21:49.998358 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90906 (* 1 = 6.90906 loss)
I0523 00:21:50.003723 35003 sgd_solver.cpp:112] Iteration 100110, lr = 0.01
I0523 00:21:52.879364 35003 solver.cpp:239] Iteration 100120 (3.47115 iter/s, 2.88089s/10 iters), loss = 7.77814
I0523 00:21:52.879577 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77814 (* 1 = 7.77814 loss)
I0523 00:21:52.883170 35003 sgd_solver.cpp:112] Iteration 100120, lr = 0.01
I0523 00:21:54.813537 35003 solver.cpp:239] Iteration 100130 (5.17093 iter/s, 1.93389s/10 iters), loss = 7.87598
I0523 00:21:54.813577 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87598 (* 1 = 7.87598 loss)
I0523 00:21:54.821621 35003 sgd_solver.cpp:112] Iteration 100130, lr = 0.01
I0523 00:21:58.511072 35003 solver.cpp:239] Iteration 100140 (2.70465 iter/s, 3.69733s/10 iters), loss = 6.9026
I0523 00:21:58.511137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9026 (* 1 = 6.9026 loss)
I0523 00:21:58.524173 35003 sgd_solver.cpp:112] Iteration 100140, lr = 0.01
I0523 00:22:01.315233 35003 solver.cpp:239] Iteration 100150 (3.56636 iter/s, 2.80398s/10 iters), loss = 8.09524
I0523 00:22:01.315274 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.09524 (* 1 = 8.09524 loss)
I0523 00:22:01.328589 35003 sgd_solver.cpp:112] Iteration 100150, lr = 0.01
I0523 00:22:02.682719 35003 solver.cpp:239] Iteration 100160 (7.31334 iter/s, 1.36736s/10 iters), loss = 7.35222
I0523 00:22:02.682777 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35222 (* 1 = 7.35222 loss)
I0523 00:22:02.696008 35003 sgd_solver.cpp:112] Iteration 100160, lr = 0.01
I0523 00:22:06.035993 35003 solver.cpp:239] Iteration 100170 (2.98234 iter/s, 3.35308s/10 iters), loss = 6.3984
I0523 00:22:06.036039 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3984 (* 1 = 6.3984 loss)
I0523 00:22:06.192911 35003 sgd_solver.cpp:112] Iteration 100170, lr = 0.01
I0523 00:22:09.855581 35003 solver.cpp:239] Iteration 100180 (2.61823 iter/s, 3.81937s/10 iters), loss = 7.58506
I0523 00:22:09.855643 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58506 (* 1 = 7.58506 loss)
I0523 00:22:10.537588 35003 sgd_solver.cpp:112] Iteration 100180, lr = 0.01
I0523 00:22:13.476743 35003 solver.cpp:239] Iteration 100190 (2.76171 iter/s, 3.62095s/10 iters), loss = 8.08338
I0523 00:22:13.476806 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08338 (* 1 = 8.08338 loss)
I0523 00:22:13.480085 35003 sgd_solver.cpp:112] Iteration 100190, lr = 0.01
I0523 00:22:17.042903 35003 solver.cpp:239] Iteration 100200 (2.8043 iter/s, 3.56596s/10 iters), loss = 7.81411
I0523 00:22:17.042943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81411 (* 1 = 7.81411 loss)
I0523 00:22:17.051105 35003 sgd_solver.cpp:112] Iteration 100200, lr = 0.01
I0523 00:22:19.127409 35003 solver.cpp:239] Iteration 100210 (4.79761 iter/s, 2.08437s/10 iters), loss = 7.78776
I0523 00:22:19.127477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78776 (* 1 = 7.78776 loss)
I0523 00:22:19.133291 35003 sgd_solver.cpp:112] Iteration 100210, lr = 0.01
I0523 00:22:21.976028 35003 solver.cpp:239] Iteration 100220 (3.51072 iter/s, 2.84842s/10 iters), loss = 7.64278
I0523 00:22:21.976070 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64278 (* 1 = 7.64278 loss)
I0523 00:22:21.978723 35003 sgd_solver.cpp:112] Iteration 100220, lr = 0.01
I0523 00:22:25.684218 35003 solver.cpp:239] Iteration 100230 (2.69688 iter/s, 3.70798s/10 iters), loss = 6.87144
I0523 00:22:25.684465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87144 (* 1 = 6.87144 loss)
I0523 00:22:25.698022 35003 sgd_solver.cpp:112] Iteration 100230, lr = 0.01
I0523 00:22:28.354135 35003 solver.cpp:239] Iteration 100240 (3.74591 iter/s, 2.66958s/10 iters), loss = 6.72308
I0523 00:22:28.354176 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72308 (* 1 = 6.72308 loss)
I0523 00:22:28.959622 35003 sgd_solver.cpp:112] Iteration 100240, lr = 0.01
I0523 00:22:31.713457 35003 solver.cpp:239] Iteration 100250 (2.97695 iter/s, 3.35914s/10 iters), loss = 6.97794
I0523 00:22:31.713498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97794 (* 1 = 6.97794 loss)
I0523 00:22:32.187721 35003 sgd_solver.cpp:112] Iteration 100250, lr = 0.01
I0523 00:22:36.191428 35003 solver.cpp:239] Iteration 100260 (2.23328 iter/s, 4.47772s/10 iters), loss = 6.9343
I0523 00:22:36.191498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9343 (* 1 = 6.9343 loss)
I0523 00:22:36.684712 35003 sgd_solver.cpp:112] Iteration 100260, lr = 0.01
I0523 00:22:40.894779 35003 solver.cpp:239] Iteration 100270 (2.12627 iter/s, 4.70307s/10 iters), loss = 7.0425
I0523 00:22:40.894831 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0425 (* 1 = 7.0425 loss)
I0523 00:22:40.960862 35003 sgd_solver.cpp:112] Iteration 100270, lr = 0.01
I0523 00:22:45.098896 35003 solver.cpp:239] Iteration 100280 (2.37875 iter/s, 4.20389s/10 iters), loss = 6.68874
I0523 00:22:45.098953 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68874 (* 1 = 6.68874 loss)
I0523 00:22:45.106218 35003 sgd_solver.cpp:112] Iteration 100280, lr = 0.01
I0523 00:22:48.706990 35003 solver.cpp:239] Iteration 100290 (2.77171 iter/s, 3.60789s/10 iters), loss = 6.45493
I0523 00:22:48.707033 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45493 (* 1 = 6.45493 loss)
I0523 00:22:49.434021 35003 sgd_solver.cpp:112] Iteration 100290, lr = 0.01
I0523 00:22:53.706094 35003 solver.cpp:239] Iteration 100300 (2.00046 iter/s, 4.99885s/10 iters), loss = 7.28345
I0523 00:22:53.706137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28345 (* 1 = 7.28345 loss)
I0523 00:22:53.731461 35003 sgd_solver.cpp:112] Iteration 100300, lr = 0.01
I0523 00:22:58.245190 35003 solver.cpp:239] Iteration 100310 (2.20319 iter/s, 4.53887s/10 iters), loss = 6.31095
I0523 00:22:58.245380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31095 (* 1 = 6.31095 loss)
I0523 00:22:58.305320 35003 sgd_solver.cpp:112] Iteration 100310, lr = 0.01
I0523 00:23:01.900763 35003 solver.cpp:239] Iteration 100320 (2.73579 iter/s, 3.65525s/10 iters), loss = 6.63803
I0523 00:23:01.900817 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63803 (* 1 = 6.63803 loss)
I0523 00:23:01.925432 35003 sgd_solver.cpp:112] Iteration 100320, lr = 0.01
I0523 00:23:04.009428 35003 solver.cpp:239] Iteration 100330 (4.74268 iter/s, 2.10851s/10 iters), loss = 6.65604
I0523 00:23:04.009469 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65604 (* 1 = 6.65604 loss)
I0523 00:23:04.031405 35003 sgd_solver.cpp:112] Iteration 100330, lr = 0.01
I0523 00:23:06.545584 35003 solver.cpp:239] Iteration 100340 (3.94321 iter/s, 2.536s/10 iters), loss = 6.53611
I0523 00:23:06.545629 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53611 (* 1 = 6.53611 loss)
I0523 00:23:06.559206 35003 sgd_solver.cpp:112] Iteration 100340, lr = 0.01
I0523 00:23:10.603231 35003 solver.cpp:239] Iteration 100350 (2.46462 iter/s, 4.05743s/10 iters), loss = 6.71894
I0523 00:23:10.603298 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71894 (* 1 = 6.71894 loss)
I0523 00:23:10.615500 35003 sgd_solver.cpp:112] Iteration 100350, lr = 0.01
I0523 00:23:14.094331 35003 solver.cpp:239] Iteration 100360 (2.8646 iter/s, 3.49089s/10 iters), loss = 7.95473
I0523 00:23:14.094377 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95473 (* 1 = 7.95473 loss)
I0523 00:23:14.100600 35003 sgd_solver.cpp:112] Iteration 100360, lr = 0.01
I0523 00:23:18.130277 35003 solver.cpp:239] Iteration 100370 (2.47786 iter/s, 4.03573s/10 iters), loss = 7.6205
I0523 00:23:18.130317 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6205 (* 1 = 7.6205 loss)
I0523 00:23:18.143342 35003 sgd_solver.cpp:112] Iteration 100370, lr = 0.01
I0523 00:23:21.760288 35003 solver.cpp:239] Iteration 100380 (2.75496 iter/s, 3.62981s/10 iters), loss = 6.0464
I0523 00:23:21.760330 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0464 (* 1 = 6.0464 loss)
I0523 00:23:22.053315 35003 sgd_solver.cpp:112] Iteration 100380, lr = 0.01
I0523 00:23:26.518492 35003 solver.cpp:239] Iteration 100390 (2.10174 iter/s, 4.75797s/10 iters), loss = 6.1567
I0523 00:23:26.518538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1567 (* 1 = 6.1567 loss)
I0523 00:23:26.535297 35003 sgd_solver.cpp:112] Iteration 100390, lr = 0.01
I0523 00:23:29.604084 35003 solver.cpp:239] Iteration 100400 (3.24105 iter/s, 3.08542s/10 iters), loss = 7.80193
I0523 00:23:29.604382 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80193 (* 1 = 7.80193 loss)
I0523 00:23:29.617113 35003 sgd_solver.cpp:112] Iteration 100400, lr = 0.01
I0523 00:23:33.279317 35003 solver.cpp:239] Iteration 100410 (2.72124 iter/s, 3.6748s/10 iters), loss = 6.04259
I0523 00:23:33.279366 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04259 (* 1 = 6.04259 loss)
I0523 00:23:33.284528 35003 sgd_solver.cpp:112] Iteration 100410, lr = 0.01
I0523 00:23:36.331854 35003 solver.cpp:239] Iteration 100420 (3.27618 iter/s, 3.05234s/10 iters), loss = 7.02687
I0523 00:23:36.331889 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02687 (* 1 = 7.02687 loss)
I0523 00:23:36.344681 35003 sgd_solver.cpp:112] Iteration 100420, lr = 0.01
I0523 00:23:40.050138 35003 solver.cpp:239] Iteration 100430 (2.68956 iter/s, 3.71808s/10 iters), loss = 6.96875
I0523 00:23:40.050205 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96875 (* 1 = 6.96875 loss)
I0523 00:23:40.778035 35003 sgd_solver.cpp:112] Iteration 100430, lr = 0.01
I0523 00:23:43.818531 35003 solver.cpp:239] Iteration 100440 (2.65382 iter/s, 3.76815s/10 iters), loss = 7.13877
I0523 00:23:43.818580 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13877 (* 1 = 7.13877 loss)
I0523 00:23:44.539768 35003 sgd_solver.cpp:112] Iteration 100440, lr = 0.01
I0523 00:23:47.425413 35003 solver.cpp:239] Iteration 100450 (2.77265 iter/s, 3.60666s/10 iters), loss = 7.76625
I0523 00:23:47.425452 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76625 (* 1 = 7.76625 loss)
I0523 00:23:48.134284 35003 sgd_solver.cpp:112] Iteration 100450, lr = 0.01
I0523 00:23:50.926787 35003 solver.cpp:239] Iteration 100460 (2.85618 iter/s, 3.50118s/10 iters), loss = 7.51067
I0523 00:23:50.926831 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51067 (* 1 = 7.51067 loss)
I0523 00:23:51.067582 35003 sgd_solver.cpp:112] Iteration 100460, lr = 0.01
I0523 00:23:55.525679 35003 solver.cpp:239] Iteration 100470 (2.17455 iter/s, 4.59866s/10 iters), loss = 7.74281
I0523 00:23:55.525729 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74281 (* 1 = 7.74281 loss)
I0523 00:23:56.220705 35003 sgd_solver.cpp:112] Iteration 100470, lr = 0.01
I0523 00:23:59.978058 35003 solver.cpp:239] Iteration 100480 (2.24611 iter/s, 4.45215s/10 iters), loss = 7.01197
I0523 00:23:59.978276 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01197 (* 1 = 7.01197 loss)
I0523 00:24:00.715960 35003 sgd_solver.cpp:112] Iteration 100480, lr = 0.01
I0523 00:24:03.630630 35003 solver.cpp:239] Iteration 100490 (2.73807 iter/s, 3.6522s/10 iters), loss = 7.4316
I0523 00:24:03.630677 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4316 (* 1 = 7.4316 loss)
I0523 00:24:04.358458 35003 sgd_solver.cpp:112] Iteration 100490, lr = 0.01
I0523 00:24:08.837396 35003 solver.cpp:239] Iteration 100500 (1.92067 iter/s, 5.20651s/10 iters), loss = 8.24706
I0523 00:24:08.837453 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.24706 (* 1 = 8.24706 loss)
I0523 00:24:09.572433 35003 sgd_solver.cpp:112] Iteration 100500, lr = 0.01
I0523 00:24:13.195133 35003 solver.cpp:239] Iteration 100510 (2.29489 iter/s, 4.3575s/10 iters), loss = 6.65919
I0523 00:24:13.195181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65919 (* 1 = 6.65919 loss)
I0523 00:24:13.381737 35003 sgd_solver.cpp:112] Iteration 100510, lr = 0.01
I0523 00:24:17.588095 35003 solver.cpp:239] Iteration 100520 (2.27649 iter/s, 4.39272s/10 iters), loss = 7.03997
I0523 00:24:17.588140 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03997 (* 1 = 7.03997 loss)
I0523 00:24:17.595826 35003 sgd_solver.cpp:112] Iteration 100520, lr = 0.01
I0523 00:24:20.021041 35003 solver.cpp:239] Iteration 100530 (4.11051 iter/s, 2.43279s/10 iters), loss = 6.70278
I0523 00:24:20.021093 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70278 (* 1 = 6.70278 loss)
I0523 00:24:20.034281 35003 sgd_solver.cpp:112] Iteration 100530, lr = 0.01
I0523 00:24:22.999850 35003 solver.cpp:239] Iteration 100540 (3.35725 iter/s, 2.97863s/10 iters), loss = 5.86007
I0523 00:24:22.999900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.86007 (* 1 = 5.86007 loss)
I0523 00:24:23.739042 35003 sgd_solver.cpp:112] Iteration 100540, lr = 0.01
I0523 00:24:28.327040 35003 solver.cpp:239] Iteration 100550 (1.87726 iter/s, 5.32692s/10 iters), loss = 6.59072
I0523 00:24:28.327082 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59072 (* 1 = 6.59072 loss)
I0523 00:24:28.340239 35003 sgd_solver.cpp:112] Iteration 100550, lr = 0.01
I0523 00:24:30.827028 35003 solver.cpp:239] Iteration 100560 (4.00026 iter/s, 2.49984s/10 iters), loss = 7.0987
I0523 00:24:30.827217 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0987 (* 1 = 7.0987 loss)
I0523 00:24:30.834596 35003 sgd_solver.cpp:112] Iteration 100560, lr = 0.01
I0523 00:24:33.674479 35003 solver.cpp:239] Iteration 100570 (3.51228 iter/s, 2.84716s/10 iters), loss = 6.89952
I0523 00:24:33.674523 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89952 (* 1 = 6.89952 loss)
I0523 00:24:34.412446 35003 sgd_solver.cpp:112] Iteration 100570, lr = 0.01
I0523 00:24:38.021425 35003 solver.cpp:239] Iteration 100580 (2.30059 iter/s, 4.34672s/10 iters), loss = 7.58355
I0523 00:24:38.021481 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58355 (* 1 = 7.58355 loss)
I0523 00:24:38.555104 35003 sgd_solver.cpp:112] Iteration 100580, lr = 0.01
I0523 00:24:42.811326 35003 solver.cpp:239] Iteration 100590 (2.08783 iter/s, 4.78965s/10 iters), loss = 7.14978
I0523 00:24:42.811365 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14978 (* 1 = 7.14978 loss)
I0523 00:24:42.814005 35003 sgd_solver.cpp:112] Iteration 100590, lr = 0.01
I0523 00:24:46.646673 35003 solver.cpp:239] Iteration 100600 (2.60747 iter/s, 3.83513s/10 iters), loss = 8.6108
I0523 00:24:46.646729 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.6108 (* 1 = 8.6108 loss)
I0523 00:24:47.361126 35003 sgd_solver.cpp:112] Iteration 100600, lr = 0.01
I0523 00:24:51.111800 35003 solver.cpp:239] Iteration 100610 (2.2397 iter/s, 4.46489s/10 iters), loss = 7.13554
I0523 00:24:51.111840 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13554 (* 1 = 7.13554 loss)
I0523 00:24:51.125090 35003 sgd_solver.cpp:112] Iteration 100610, lr = 0.01
I0523 00:24:54.050609 35003 solver.cpp:239] Iteration 100620 (3.40293 iter/s, 2.93864s/10 iters), loss = 7.89601
I0523 00:24:54.050653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89601 (* 1 = 7.89601 loss)
I0523 00:24:54.064473 35003 sgd_solver.cpp:112] Iteration 100620, lr = 0.01
I0523 00:24:56.830942 35003 solver.cpp:239] Iteration 100630 (3.5969 iter/s, 2.78017s/10 iters), loss = 8.10268
I0523 00:24:56.830981 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.10268 (* 1 = 8.10268 loss)
I0523 00:24:56.837306 35003 sgd_solver.cpp:112] Iteration 100630, lr = 0.01
I0523 00:24:59.686669 35003 solver.cpp:239] Iteration 100640 (3.50196 iter/s, 2.85554s/10 iters), loss = 8.54955
I0523 00:24:59.686733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.54955 (* 1 = 8.54955 loss)
I0523 00:24:59.710530 35003 sgd_solver.cpp:112] Iteration 100640, lr = 0.01
I0523 00:25:03.723556 35003 solver.cpp:239] Iteration 100650 (2.4773 iter/s, 4.03665s/10 iters), loss = 7.00362
I0523 00:25:03.723779 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00362 (* 1 = 7.00362 loss)
I0523 00:25:04.458137 35003 sgd_solver.cpp:112] Iteration 100650, lr = 0.01
I0523 00:25:08.717522 35003 solver.cpp:239] Iteration 100660 (2.00258 iter/s, 4.99355s/10 iters), loss = 7.36046
I0523 00:25:08.717563 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36046 (* 1 = 7.36046 loss)
I0523 00:25:09.413596 35003 sgd_solver.cpp:112] Iteration 100660, lr = 0.01
I0523 00:25:12.118577 35003 solver.cpp:239] Iteration 100670 (2.94043 iter/s, 3.40087s/10 iters), loss = 7.14717
I0523 00:25:12.118621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14717 (* 1 = 7.14717 loss)
I0523 00:25:12.123636 35003 sgd_solver.cpp:112] Iteration 100670, lr = 0.01
I0523 00:25:15.570574 35003 solver.cpp:239] Iteration 100680 (2.89703 iter/s, 3.45181s/10 iters), loss = 7.26937
I0523 00:25:15.570626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26937 (* 1 = 7.26937 loss)
I0523 00:25:15.583288 35003 sgd_solver.cpp:112] Iteration 100680, lr = 0.01
I0523 00:25:19.225522 35003 solver.cpp:239] Iteration 100690 (2.73617 iter/s, 3.65475s/10 iters), loss = 7.26371
I0523 00:25:19.225567 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26371 (* 1 = 7.26371 loss)
I0523 00:25:19.252866 35003 sgd_solver.cpp:112] Iteration 100690, lr = 0.01
I0523 00:25:22.884886 35003 solver.cpp:239] Iteration 100700 (2.73287 iter/s, 3.65915s/10 iters), loss = 6.16694
I0523 00:25:22.884940 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16694 (* 1 = 6.16694 loss)
I0523 00:25:22.898272 35003 sgd_solver.cpp:112] Iteration 100700, lr = 0.01
I0523 00:25:25.922830 35003 solver.cpp:239] Iteration 100710 (3.29189 iter/s, 3.03776s/10 iters), loss = 7.02886
I0523 00:25:25.922878 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02886 (* 1 = 7.02886 loss)
I0523 00:25:25.930780 35003 sgd_solver.cpp:112] Iteration 100710, lr = 0.01
I0523 00:25:28.029475 35003 solver.cpp:239] Iteration 100720 (4.7472 iter/s, 2.10651s/10 iters), loss = 7.12773
I0523 00:25:28.029513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12773 (* 1 = 7.12773 loss)
I0523 00:25:28.043126 35003 sgd_solver.cpp:112] Iteration 100720, lr = 0.01
I0523 00:25:31.873713 35003 solver.cpp:239] Iteration 100730 (2.60143 iter/s, 3.84404s/10 iters), loss = 6.49024
I0523 00:25:31.873759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49024 (* 1 = 6.49024 loss)
I0523 00:25:31.883255 35003 sgd_solver.cpp:112] Iteration 100730, lr = 0.01
I0523 00:25:36.111070 35003 solver.cpp:239] Iteration 100740 (2.36009 iter/s, 4.23713s/10 iters), loss = 6.53131
I0523 00:25:36.113559 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53131 (* 1 = 6.53131 loss)
I0523 00:25:36.118813 35003 sgd_solver.cpp:112] Iteration 100740, lr = 0.01
I0523 00:25:38.996268 35003 solver.cpp:239] Iteration 100750 (3.47318 iter/s, 2.87921s/10 iters), loss = 6.31365
I0523 00:25:38.996304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31365 (* 1 = 6.31365 loss)
I0523 00:25:39.003160 35003 sgd_solver.cpp:112] Iteration 100750, lr = 0.01
I0523 00:25:41.763237 35003 solver.cpp:239] Iteration 100760 (3.6143 iter/s, 2.76679s/10 iters), loss = 6.38941
I0523 00:25:41.763284 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38941 (* 1 = 6.38941 loss)
I0523 00:25:41.770465 35003 sgd_solver.cpp:112] Iteration 100760, lr = 0.01
I0523 00:25:45.408987 35003 solver.cpp:239] Iteration 100770 (2.74307 iter/s, 3.64555s/10 iters), loss = 7.20944
I0523 00:25:45.409039 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20944 (* 1 = 7.20944 loss)
I0523 00:25:45.415732 35003 sgd_solver.cpp:112] Iteration 100770, lr = 0.01
I0523 00:25:50.141204 35003 solver.cpp:239] Iteration 100780 (2.11328 iter/s, 4.73197s/10 iters), loss = 8.04873
I0523 00:25:50.141250 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04873 (* 1 = 8.04873 loss)
I0523 00:25:50.142052 35003 sgd_solver.cpp:112] Iteration 100780, lr = 0.01
I0523 00:25:53.727113 35003 solver.cpp:239] Iteration 100790 (2.78884 iter/s, 3.58571s/10 iters), loss = 7.28717
I0523 00:25:53.727159 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28717 (* 1 = 7.28717 loss)
I0523 00:25:53.888044 35003 sgd_solver.cpp:112] Iteration 100790, lr = 0.01
I0523 00:25:57.779629 35003 solver.cpp:239] Iteration 100800 (2.46773 iter/s, 4.0523s/10 iters), loss = 8.5717
I0523 00:25:57.779677 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.5717 (* 1 = 8.5717 loss)
I0523 00:25:58.361522 35003 sgd_solver.cpp:112] Iteration 100800, lr = 0.01
I0523 00:26:01.147693 35003 solver.cpp:239] Iteration 100810 (2.96923 iter/s, 3.36787s/10 iters), loss = 7.38533
I0523 00:26:01.147754 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38533 (* 1 = 7.38533 loss)
I0523 00:26:01.188984 35003 sgd_solver.cpp:112] Iteration 100810, lr = 0.01
I0523 00:26:04.625290 35003 solver.cpp:239] Iteration 100820 (2.87572 iter/s, 3.47739s/10 iters), loss = 6.89249
I0523 00:26:04.625331 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89249 (* 1 = 6.89249 loss)
I0523 00:26:04.638834 35003 sgd_solver.cpp:112] Iteration 100820, lr = 0.01
I0523 00:26:08.191973 35003 solver.cpp:239] Iteration 100830 (2.80387 iter/s, 3.56649s/10 iters), loss = 7.83033
I0523 00:26:08.192145 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83033 (* 1 = 7.83033 loss)
I0523 00:26:08.205090 35003 sgd_solver.cpp:112] Iteration 100830, lr = 0.01
I0523 00:26:12.312978 35003 solver.cpp:239] Iteration 100840 (2.42679 iter/s, 4.12067s/10 iters), loss = 6.28794
I0523 00:26:12.313037 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28794 (* 1 = 6.28794 loss)
I0523 00:26:12.320319 35003 sgd_solver.cpp:112] Iteration 100840, lr = 0.01
I0523 00:26:15.756980 35003 solver.cpp:239] Iteration 100850 (2.90377 iter/s, 3.4438s/10 iters), loss = 7.38696
I0523 00:26:15.757035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38696 (* 1 = 7.38696 loss)
I0523 00:26:15.760015 35003 sgd_solver.cpp:112] Iteration 100850, lr = 0.01
I0523 00:26:17.812525 35003 solver.cpp:239] Iteration 100860 (4.86522 iter/s, 2.0554s/10 iters), loss = 7.18097
I0523 00:26:17.812561 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18097 (* 1 = 7.18097 loss)
I0523 00:26:17.820457 35003 sgd_solver.cpp:112] Iteration 100860, lr = 0.01
I0523 00:26:20.849755 35003 solver.cpp:239] Iteration 100870 (3.29266 iter/s, 3.03706s/10 iters), loss = 7.17302
I0523 00:26:20.849805 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17302 (* 1 = 7.17302 loss)
I0523 00:26:20.854384 35003 sgd_solver.cpp:112] Iteration 100870, lr = 0.01
I0523 00:26:24.364713 35003 solver.cpp:239] Iteration 100880 (2.84514 iter/s, 3.51476s/10 iters), loss = 7.33695
I0523 00:26:24.364754 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33695 (* 1 = 7.33695 loss)
I0523 00:26:24.377331 35003 sgd_solver.cpp:112] Iteration 100880, lr = 0.01
I0523 00:26:27.249100 35003 solver.cpp:239] Iteration 100890 (3.46714 iter/s, 2.88422s/10 iters), loss = 7.33054
I0523 00:26:27.249145 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33054 (* 1 = 7.33054 loss)
I0523 00:26:27.859377 35003 sgd_solver.cpp:112] Iteration 100890, lr = 0.01
I0523 00:26:31.357867 35003 solver.cpp:239] Iteration 100900 (2.43395 iter/s, 4.10854s/10 iters), loss = 6.64168
I0523 00:26:31.357926 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64168 (* 1 = 6.64168 loss)
I0523 00:26:31.650553 35003 sgd_solver.cpp:112] Iteration 100900, lr = 0.01
I0523 00:26:36.037714 35003 solver.cpp:239] Iteration 100910 (2.13693 iter/s, 4.6796s/10 iters), loss = 6.72868
I0523 00:26:36.037756 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72868 (* 1 = 6.72868 loss)
I0523 00:26:36.049221 35003 sgd_solver.cpp:112] Iteration 100910, lr = 0.01
I0523 00:26:39.641392 35003 solver.cpp:239] Iteration 100920 (2.77509 iter/s, 3.60348s/10 iters), loss = 7.11266
I0523 00:26:39.641664 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11266 (* 1 = 7.11266 loss)
I0523 00:26:39.691196 35003 sgd_solver.cpp:112] Iteration 100920, lr = 0.01
I0523 00:26:43.572510 35003 solver.cpp:239] Iteration 100930 (2.54408 iter/s, 3.93069s/10 iters), loss = 7.26988
I0523 00:26:43.572577 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26988 (* 1 = 7.26988 loss)
I0523 00:26:44.158004 35003 sgd_solver.cpp:112] Iteration 100930, lr = 0.01
I0523 00:26:46.888149 35003 solver.cpp:239] Iteration 100940 (3.01619 iter/s, 3.31544s/10 iters), loss = 6.9706
I0523 00:26:46.888190 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9706 (* 1 = 6.9706 loss)
I0523 00:26:46.896119 35003 sgd_solver.cpp:112] Iteration 100940, lr = 0.01
I0523 00:26:49.711365 35003 solver.cpp:239] Iteration 100950 (3.54227 iter/s, 2.82305s/10 iters), loss = 7.56621
I0523 00:26:49.711419 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56621 (* 1 = 7.56621 loss)
I0523 00:26:50.426326 35003 sgd_solver.cpp:112] Iteration 100950, lr = 0.01
I0523 00:26:52.518316 35003 solver.cpp:239] Iteration 100960 (3.5628 iter/s, 2.80678s/10 iters), loss = 7.61371
I0523 00:26:52.518365 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61371 (* 1 = 7.61371 loss)
I0523 00:26:53.239871 35003 sgd_solver.cpp:112] Iteration 100960, lr = 0.01
I0523 00:26:56.039947 35003 solver.cpp:239] Iteration 100970 (2.83976 iter/s, 3.52142s/10 iters), loss = 6.81585
I0523 00:26:56.040016 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81585 (* 1 = 6.81585 loss)
I0523 00:26:56.115685 35003 sgd_solver.cpp:112] Iteration 100970, lr = 0.01
I0523 00:27:00.541821 35003 solver.cpp:239] Iteration 100980 (2.22142 iter/s, 4.50163s/10 iters), loss = 6.7889
I0523 00:27:00.541859 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7889 (* 1 = 6.7889 loss)
I0523 00:27:01.257119 35003 sgd_solver.cpp:112] Iteration 100980, lr = 0.01
I0523 00:27:04.128376 35003 solver.cpp:239] Iteration 100990 (2.78834 iter/s, 3.58637s/10 iters), loss = 5.86275
I0523 00:27:04.128413 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.86275 (* 1 = 5.86275 loss)
I0523 00:27:04.146314 35003 sgd_solver.cpp:112] Iteration 100990, lr = 0.01
I0523 00:27:08.263159 35003 solver.cpp:239] Iteration 101000 (2.41863 iter/s, 4.13456s/10 iters), loss = 8.06173
I0523 00:27:08.263211 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.06173 (* 1 = 8.06173 loss)
I0523 00:27:08.991660 35003 sgd_solver.cpp:112] Iteration 101000, lr = 0.01
I0523 00:27:11.424854 35003 solver.cpp:239] Iteration 101010 (3.16305 iter/s, 3.1615s/10 iters), loss = 7.41561
I0523 00:27:11.425024 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41561 (* 1 = 7.41561 loss)
I0523 00:27:11.436049 35003 sgd_solver.cpp:112] Iteration 101010, lr = 0.01
I0523 00:27:15.520123 35003 solver.cpp:239] Iteration 101020 (2.44204 iter/s, 4.09494s/10 iters), loss = 7.12136
I0523 00:27:15.520177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12136 (* 1 = 7.12136 loss)
I0523 00:27:15.542253 35003 sgd_solver.cpp:112] Iteration 101020, lr = 0.01
I0523 00:27:19.204471 35003 solver.cpp:239] Iteration 101030 (2.71434 iter/s, 3.68413s/10 iters), loss = 8.51869
I0523 00:27:19.204522 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.51869 (* 1 = 8.51869 loss)
I0523 00:27:19.210517 35003 sgd_solver.cpp:112] Iteration 101030, lr = 0.01
I0523 00:27:20.973894 35003 solver.cpp:239] Iteration 101040 (5.652 iter/s, 1.76929s/10 iters), loss = 7.29082
I0523 00:27:20.973948 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29082 (* 1 = 7.29082 loss)
I0523 00:27:21.662923 35003 sgd_solver.cpp:112] Iteration 101040, lr = 0.01
I0523 00:27:24.802669 35003 solver.cpp:239] Iteration 101050 (2.61195 iter/s, 3.82856s/10 iters), loss = 7.05282
I0523 00:27:24.802757 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05282 (* 1 = 7.05282 loss)
I0523 00:27:24.815778 35003 sgd_solver.cpp:112] Iteration 101050, lr = 0.01
I0523 00:27:28.429061 35003 solver.cpp:239] Iteration 101060 (2.75774 iter/s, 3.62616s/10 iters), loss = 7.39564
I0523 00:27:28.429101 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39564 (* 1 = 7.39564 loss)
I0523 00:27:29.142143 35003 sgd_solver.cpp:112] Iteration 101060, lr = 0.01
I0523 00:27:31.984063 35003 solver.cpp:239] Iteration 101070 (2.81309 iter/s, 3.55481s/10 iters), loss = 6.7212
I0523 00:27:31.984105 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7212 (* 1 = 6.7212 loss)
I0523 00:27:31.996615 35003 sgd_solver.cpp:112] Iteration 101070, lr = 0.01
I0523 00:27:35.546566 35003 solver.cpp:239] Iteration 101080 (2.81059 iter/s, 3.55797s/10 iters), loss = 7.30148
I0523 00:27:35.546604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30148 (* 1 = 7.30148 loss)
I0523 00:27:35.559551 35003 sgd_solver.cpp:112] Iteration 101080, lr = 0.01
I0523 00:27:38.775342 35003 solver.cpp:239] Iteration 101090 (3.09732 iter/s, 3.2286s/10 iters), loss = 8.20099
I0523 00:27:38.775390 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.20099 (* 1 = 8.20099 loss)
I0523 00:27:39.434495 35003 sgd_solver.cpp:112] Iteration 101090, lr = 0.01
I0523 00:27:43.050843 35003 solver.cpp:239] Iteration 101100 (2.33903 iter/s, 4.27528s/10 iters), loss = 7.08362
I0523 00:27:43.051089 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08362 (* 1 = 7.08362 loss)
I0523 00:27:43.059859 35003 sgd_solver.cpp:112] Iteration 101100, lr = 0.01
I0523 00:27:45.140106 35003 solver.cpp:239] Iteration 101110 (4.78709 iter/s, 2.08895s/10 iters), loss = 6.23159
I0523 00:27:45.140154 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23159 (* 1 = 6.23159 loss)
I0523 00:27:45.149549 35003 sgd_solver.cpp:112] Iteration 101110, lr = 0.01
I0523 00:27:48.571403 35003 solver.cpp:239] Iteration 101120 (2.91451 iter/s, 3.4311s/10 iters), loss = 7.00677
I0523 00:27:48.571444 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00677 (* 1 = 7.00677 loss)
I0523 00:27:48.584527 35003 sgd_solver.cpp:112] Iteration 101120, lr = 0.01
I0523 00:27:52.679141 35003 solver.cpp:239] Iteration 101130 (2.43456 iter/s, 4.10752s/10 iters), loss = 7.1758
I0523 00:27:52.679217 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1758 (* 1 = 7.1758 loss)
I0523 00:27:52.930209 35003 sgd_solver.cpp:112] Iteration 101130, lr = 0.01
I0523 00:27:57.093477 35003 solver.cpp:239] Iteration 101140 (2.2677 iter/s, 4.40976s/10 iters), loss = 7.52909
I0523 00:27:57.093524 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52909 (* 1 = 7.52909 loss)
I0523 00:27:57.100736 35003 sgd_solver.cpp:112] Iteration 101140, lr = 0.01
I0523 00:27:59.772557 35003 solver.cpp:239] Iteration 101150 (3.73288 iter/s, 2.6789s/10 iters), loss = 6.72958
I0523 00:27:59.772603 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72958 (* 1 = 6.72958 loss)
I0523 00:28:00.513339 35003 sgd_solver.cpp:112] Iteration 101150, lr = 0.01
I0523 00:28:03.606132 35003 solver.cpp:239] Iteration 101160 (2.60868 iter/s, 3.83335s/10 iters), loss = 7.9309
I0523 00:28:03.606204 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9309 (* 1 = 7.9309 loss)
I0523 00:28:03.619148 35003 sgd_solver.cpp:112] Iteration 101160, lr = 0.01
I0523 00:28:05.699188 35003 solver.cpp:239] Iteration 101170 (4.77808 iter/s, 2.09289s/10 iters), loss = 8.47628
I0523 00:28:05.699234 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.47628 (* 1 = 8.47628 loss)
I0523 00:28:05.703758 35003 sgd_solver.cpp:112] Iteration 101170, lr = 0.01
I0523 00:28:10.200543 35003 solver.cpp:239] Iteration 101180 (2.22167 iter/s, 4.50111s/10 iters), loss = 6.54279
I0523 00:28:10.200616 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54279 (* 1 = 6.54279 loss)
I0523 00:28:10.940809 35003 sgd_solver.cpp:112] Iteration 101180, lr = 0.01
I0523 00:28:13.608278 35003 solver.cpp:239] Iteration 101190 (2.93468 iter/s, 3.40752s/10 iters), loss = 6.76706
I0523 00:28:13.608570 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76706 (* 1 = 6.76706 loss)
I0523 00:28:14.343330 35003 sgd_solver.cpp:112] Iteration 101190, lr = 0.01
I0523 00:28:17.368836 35003 solver.cpp:239] Iteration 101200 (2.65947 iter/s, 3.76014s/10 iters), loss = 7.52483
I0523 00:28:17.368876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52483 (* 1 = 7.52483 loss)
I0523 00:28:18.064306 35003 sgd_solver.cpp:112] Iteration 101200, lr = 0.01
I0523 00:28:21.814574 35003 solver.cpp:239] Iteration 101210 (2.24946 iter/s, 4.44551s/10 iters), loss = 8.19336
I0523 00:28:21.814627 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.19336 (* 1 = 8.19336 loss)
I0523 00:28:21.827518 35003 sgd_solver.cpp:112] Iteration 101210, lr = 0.01
I0523 00:28:26.904196 35003 solver.cpp:239] Iteration 101220 (1.96488 iter/s, 5.08937s/10 iters), loss = 7.02768
I0523 00:28:26.904242 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02768 (* 1 = 7.02768 loss)
I0523 00:28:26.915824 35003 sgd_solver.cpp:112] Iteration 101220, lr = 0.01
I0523 00:28:31.550055 35003 solver.cpp:239] Iteration 101230 (2.15256 iter/s, 4.64562s/10 iters), loss = 6.96895
I0523 00:28:31.550103 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96895 (* 1 = 6.96895 loss)
I0523 00:28:31.562964 35003 sgd_solver.cpp:112] Iteration 101230, lr = 0.01
I0523 00:28:35.151685 35003 solver.cpp:239] Iteration 101240 (2.77667 iter/s, 3.60143s/10 iters), loss = 8.1656
I0523 00:28:35.151732 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.1656 (* 1 = 8.1656 loss)
I0523 00:28:35.165252 35003 sgd_solver.cpp:112] Iteration 101240, lr = 0.01
I0523 00:28:37.911815 35003 solver.cpp:239] Iteration 101250 (3.62324 iter/s, 2.75996s/10 iters), loss = 6.77989
I0523 00:28:37.911866 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77989 (* 1 = 6.77989 loss)
I0523 00:28:37.925071 35003 sgd_solver.cpp:112] Iteration 101250, lr = 0.01
I0523 00:28:41.935710 35003 solver.cpp:239] Iteration 101260 (2.48529 iter/s, 4.02368s/10 iters), loss = 6.84109
I0523 00:28:41.935755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84109 (* 1 = 6.84109 loss)
I0523 00:28:42.250274 35003 sgd_solver.cpp:112] Iteration 101260, lr = 0.01
I0523 00:28:44.347121 35003 solver.cpp:239] Iteration 101270 (4.1472 iter/s, 2.41126s/10 iters), loss = 6.78164
I0523 00:28:44.347338 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78164 (* 1 = 6.78164 loss)
I0523 00:28:44.967195 35003 sgd_solver.cpp:112] Iteration 101270, lr = 0.01
I0523 00:28:47.893718 35003 solver.cpp:239] Iteration 101280 (2.81989 iter/s, 3.54624s/10 iters), loss = 7.23972
I0523 00:28:47.893764 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23972 (* 1 = 7.23972 loss)
I0523 00:28:48.632922 35003 sgd_solver.cpp:112] Iteration 101280, lr = 0.01
I0523 00:28:50.542986 35003 solver.cpp:239] Iteration 101290 (3.77487 iter/s, 2.6491s/10 iters), loss = 7.14425
I0523 00:28:50.543031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14425 (* 1 = 7.14425 loss)
I0523 00:28:50.551290 35003 sgd_solver.cpp:112] Iteration 101290, lr = 0.01
I0523 00:28:55.649602 35003 solver.cpp:239] Iteration 101300 (1.95834 iter/s, 5.10636s/10 iters), loss = 7.3868
I0523 00:28:55.649659 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3868 (* 1 = 7.3868 loss)
I0523 00:28:55.654371 35003 sgd_solver.cpp:112] Iteration 101300, lr = 0.01
I0523 00:28:58.545702 35003 solver.cpp:239] Iteration 101310 (3.45313 iter/s, 2.89592s/10 iters), loss = 7.09421
I0523 00:28:58.545753 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09421 (* 1 = 7.09421 loss)
I0523 00:28:58.943519 35003 sgd_solver.cpp:112] Iteration 101310, lr = 0.01
I0523 00:29:02.453243 35003 solver.cpp:239] Iteration 101320 (2.5593 iter/s, 3.90733s/10 iters), loss = 6.33941
I0523 00:29:02.453307 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33941 (* 1 = 6.33941 loss)
I0523 00:29:03.181789 35003 sgd_solver.cpp:112] Iteration 101320, lr = 0.01
I0523 00:29:06.006971 35003 solver.cpp:239] Iteration 101330 (2.81411 iter/s, 3.55352s/10 iters), loss = 6.38134
I0523 00:29:06.007007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38134 (* 1 = 6.38134 loss)
I0523 00:29:06.020479 35003 sgd_solver.cpp:112] Iteration 101330, lr = 0.01
I0523 00:29:08.099447 35003 solver.cpp:239] Iteration 101340 (4.77934 iter/s, 2.09234s/10 iters), loss = 7.52528
I0523 00:29:08.099489 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52528 (* 1 = 7.52528 loss)
I0523 00:29:08.106971 35003 sgd_solver.cpp:112] Iteration 101340, lr = 0.01
I0523 00:29:11.319130 35003 solver.cpp:239] Iteration 101350 (3.10607 iter/s, 3.21951s/10 iters), loss = 6.83692
I0523 00:29:11.319180 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83692 (* 1 = 6.83692 loss)
I0523 00:29:11.324798 35003 sgd_solver.cpp:112] Iteration 101350, lr = 0.01
I0523 00:29:14.744828 35003 solver.cpp:239] Iteration 101360 (2.91928 iter/s, 3.42551s/10 iters), loss = 6.95198
I0523 00:29:14.744964 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95198 (* 1 = 6.95198 loss)
I0523 00:29:14.761436 35003 sgd_solver.cpp:112] Iteration 101360, lr = 0.01
I0523 00:29:18.406594 35003 solver.cpp:239] Iteration 101370 (2.73114 iter/s, 3.66148s/10 iters), loss = 7.05882
I0523 00:29:18.406632 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05882 (* 1 = 7.05882 loss)
I0523 00:29:19.081434 35003 sgd_solver.cpp:112] Iteration 101370, lr = 0.01
I0523 00:29:22.019896 35003 solver.cpp:239] Iteration 101380 (2.7677 iter/s, 3.61311s/10 iters), loss = 7.62028
I0523 00:29:22.019942 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62028 (* 1 = 7.62028 loss)
I0523 00:29:22.037920 35003 sgd_solver.cpp:112] Iteration 101380, lr = 0.01
I0523 00:29:27.007319 35003 solver.cpp:239] Iteration 101390 (2.00514 iter/s, 4.98717s/10 iters), loss = 6.93678
I0523 00:29:27.007360 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93678 (* 1 = 6.93678 loss)
I0523 00:29:27.037196 35003 sgd_solver.cpp:112] Iteration 101390, lr = 0.01
I0523 00:29:29.158933 35003 solver.cpp:239] Iteration 101400 (4.64798 iter/s, 2.15147s/10 iters), loss = 6.53375
I0523 00:29:29.158982 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53375 (* 1 = 6.53375 loss)
I0523 00:29:29.166853 35003 sgd_solver.cpp:112] Iteration 101400, lr = 0.01
I0523 00:29:31.220695 35003 solver.cpp:239] Iteration 101410 (4.85054 iter/s, 2.06163s/10 iters), loss = 7.44848
I0523 00:29:31.220736 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44848 (* 1 = 7.44848 loss)
I0523 00:29:31.229174 35003 sgd_solver.cpp:112] Iteration 101410, lr = 0.01
I0523 00:29:34.771431 35003 solver.cpp:239] Iteration 101420 (2.81647 iter/s, 3.55055s/10 iters), loss = 6.67147
I0523 00:29:34.771479 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67147 (* 1 = 6.67147 loss)
I0523 00:29:34.781152 35003 sgd_solver.cpp:112] Iteration 101420, lr = 0.01
I0523 00:29:37.806870 35003 solver.cpp:239] Iteration 101430 (3.29461 iter/s, 3.03526s/10 iters), loss = 6.6222
I0523 00:29:37.806918 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6222 (* 1 = 6.6222 loss)
I0523 00:29:37.855971 35003 sgd_solver.cpp:112] Iteration 101430, lr = 0.01
I0523 00:29:42.276322 35003 solver.cpp:239] Iteration 101440 (2.23753 iter/s, 4.46922s/10 iters), loss = 8.32828
I0523 00:29:42.276367 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.32828 (* 1 = 8.32828 loss)
I0523 00:29:42.286942 35003 sgd_solver.cpp:112] Iteration 101440, lr = 0.01
I0523 00:29:45.790004 35003 solver.cpp:239] Iteration 101450 (2.84618 iter/s, 3.51348s/10 iters), loss = 7.67049
I0523 00:29:45.790216 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67049 (* 1 = 7.67049 loss)
I0523 00:29:45.794728 35003 sgd_solver.cpp:112] Iteration 101450, lr = 0.01
I0523 00:29:49.736351 35003 solver.cpp:239] Iteration 101460 (2.53423 iter/s, 3.94597s/10 iters), loss = 7.34651
I0523 00:29:49.736397 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34651 (* 1 = 7.34651 loss)
I0523 00:29:50.452538 35003 sgd_solver.cpp:112] Iteration 101460, lr = 0.01
I0523 00:29:52.319944 35003 solver.cpp:239] Iteration 101470 (3.87081 iter/s, 2.58344s/10 iters), loss = 6.99618
I0523 00:29:52.319988 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99618 (* 1 = 6.99618 loss)
I0523 00:29:52.556100 35003 sgd_solver.cpp:112] Iteration 101470, lr = 0.01
I0523 00:29:56.704658 35003 solver.cpp:239] Iteration 101480 (2.28077 iter/s, 4.38449s/10 iters), loss = 6.52398
I0523 00:29:56.704696 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52398 (* 1 = 6.52398 loss)
I0523 00:29:56.710448 35003 sgd_solver.cpp:112] Iteration 101480, lr = 0.01
I0523 00:30:00.329006 35003 solver.cpp:239] Iteration 101490 (2.75927 iter/s, 3.62415s/10 iters), loss = 7.38076
I0523 00:30:00.329056 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38076 (* 1 = 7.38076 loss)
I0523 00:30:00.348182 35003 sgd_solver.cpp:112] Iteration 101490, lr = 0.01
I0523 00:30:04.827356 35003 solver.cpp:239] Iteration 101500 (2.22315 iter/s, 4.49812s/10 iters), loss = 7.00133
I0523 00:30:04.827399 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00133 (* 1 = 7.00133 loss)
I0523 00:30:05.529916 35003 sgd_solver.cpp:112] Iteration 101500, lr = 0.01
I0523 00:30:09.982596 35003 solver.cpp:239] Iteration 101510 (1.93987 iter/s, 5.15499s/10 iters), loss = 7.90342
I0523 00:30:09.982642 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90342 (* 1 = 7.90342 loss)
I0523 00:30:09.995764 35003 sgd_solver.cpp:112] Iteration 101510, lr = 0.01
I0523 00:30:10.851501 35003 solver.cpp:239] Iteration 101520 (11.51 iter/s, 0.868807s/10 iters), loss = 6.0821
I0523 00:30:10.851557 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0821 (* 1 = 6.0821 loss)
I0523 00:30:10.857164 35003 sgd_solver.cpp:112] Iteration 101520, lr = 0.01
I0523 00:30:11.895124 35003 solver.cpp:239] Iteration 101530 (9.58299 iter/s, 1.04352s/10 iters), loss = 7.68081
I0523 00:30:11.895169 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68081 (* 1 = 7.68081 loss)
I0523 00:30:11.899015 35003 sgd_solver.cpp:112] Iteration 101530, lr = 0.01
I0523 00:30:14.494653 35003 solver.cpp:239] Iteration 101540 (3.84711 iter/s, 2.59935s/10 iters), loss = 5.98901
I0523 00:30:14.494725 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98901 (* 1 = 5.98901 loss)
I0523 00:30:14.508186 35003 sgd_solver.cpp:112] Iteration 101540, lr = 0.01
I0523 00:30:18.161492 35003 solver.cpp:239] Iteration 101550 (2.72729 iter/s, 3.66665s/10 iters), loss = 7.0256
I0523 00:30:18.161775 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0256 (* 1 = 7.0256 loss)
I0523 00:30:18.168598 35003 sgd_solver.cpp:112] Iteration 101550, lr = 0.01
I0523 00:30:23.759512 35003 solver.cpp:239] Iteration 101560 (1.7865 iter/s, 5.59753s/10 iters), loss = 6.20781
I0523 00:30:23.759565 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20781 (* 1 = 6.20781 loss)
I0523 00:30:24.465836 35003 sgd_solver.cpp:112] Iteration 101560, lr = 0.01
I0523 00:30:28.042654 35003 solver.cpp:239] Iteration 101570 (2.33486 iter/s, 4.28291s/10 iters), loss = 7.09274
I0523 00:30:28.042742 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09274 (* 1 = 7.09274 loss)
I0523 00:30:28.049021 35003 sgd_solver.cpp:112] Iteration 101570, lr = 0.01
I0523 00:30:30.180779 35003 solver.cpp:239] Iteration 101580 (4.67739 iter/s, 2.13795s/10 iters), loss = 7.25462
I0523 00:30:30.180837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25462 (* 1 = 7.25462 loss)
I0523 00:30:30.921886 35003 sgd_solver.cpp:112] Iteration 101580, lr = 0.01
I0523 00:30:34.588436 35003 solver.cpp:239] Iteration 101590 (2.2689 iter/s, 4.40742s/10 iters), loss = 8.38178
I0523 00:30:34.588486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.38178 (* 1 = 8.38178 loss)
I0523 00:30:34.601588 35003 sgd_solver.cpp:112] Iteration 101590, lr = 0.01
I0523 00:30:38.235922 35003 solver.cpp:239] Iteration 101600 (2.74176 iter/s, 3.64729s/10 iters), loss = 7.22222
I0523 00:30:38.235965 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22222 (* 1 = 7.22222 loss)
I0523 00:30:38.973625 35003 sgd_solver.cpp:112] Iteration 101600, lr = 0.01
I0523 00:30:42.623476 35003 solver.cpp:239] Iteration 101610 (2.27929 iter/s, 4.38733s/10 iters), loss = 7.18401
I0523 00:30:42.623528 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18401 (* 1 = 7.18401 loss)
I0523 00:30:43.358327 35003 sgd_solver.cpp:112] Iteration 101610, lr = 0.01
I0523 00:30:46.805307 35003 solver.cpp:239] Iteration 101620 (2.39143 iter/s, 4.1816s/10 iters), loss = 7.11535
I0523 00:30:46.805357 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11535 (* 1 = 7.11535 loss)
I0523 00:30:46.809448 35003 sgd_solver.cpp:112] Iteration 101620, lr = 0.01
I0523 00:30:51.117874 35003 solver.cpp:239] Iteration 101630 (2.31893 iter/s, 4.31233s/10 iters), loss = 6.61051
I0523 00:30:51.118052 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61051 (* 1 = 6.61051 loss)
I0523 00:30:51.127439 35003 sgd_solver.cpp:112] Iteration 101630, lr = 0.01
I0523 00:30:54.575759 35003 solver.cpp:239] Iteration 101640 (2.8922 iter/s, 3.45757s/10 iters), loss = 7.81978
I0523 00:30:54.575803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81978 (* 1 = 7.81978 loss)
I0523 00:30:54.588809 35003 sgd_solver.cpp:112] Iteration 101640, lr = 0.01
I0523 00:30:58.792592 35003 solver.cpp:239] Iteration 101650 (2.37158 iter/s, 4.2166s/10 iters), loss = 7.57702
I0523 00:30:58.792641 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57702 (* 1 = 7.57702 loss)
I0523 00:30:58.799022 35003 sgd_solver.cpp:112] Iteration 101650, lr = 0.01
I0523 00:31:03.219205 35003 solver.cpp:239] Iteration 101660 (2.25919 iter/s, 4.42637s/10 iters), loss = 7.72913
I0523 00:31:03.219250 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72913 (* 1 = 7.72913 loss)
I0523 00:31:03.223814 35003 sgd_solver.cpp:112] Iteration 101660, lr = 0.01
I0523 00:31:05.939054 35003 solver.cpp:239] Iteration 101670 (3.67689 iter/s, 2.71969s/10 iters), loss = 7.13853
I0523 00:31:05.939102 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13853 (* 1 = 7.13853 loss)
I0523 00:31:05.952088 35003 sgd_solver.cpp:112] Iteration 101670, lr = 0.01
I0523 00:31:08.053184 35003 solver.cpp:239] Iteration 101680 (4.73041 iter/s, 2.11398s/10 iters), loss = 6.36345
I0523 00:31:08.053237 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36345 (* 1 = 6.36345 loss)
I0523 00:31:08.067016 35003 sgd_solver.cpp:112] Iteration 101680, lr = 0.01
I0523 00:31:10.106134 35003 solver.cpp:239] Iteration 101690 (4.87137 iter/s, 2.05281s/10 iters), loss = 6.60804
I0523 00:31:10.106171 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60804 (* 1 = 6.60804 loss)
I0523 00:31:10.118422 35003 sgd_solver.cpp:112] Iteration 101690, lr = 0.01
I0523 00:31:13.042599 35003 solver.cpp:239] Iteration 101700 (3.40565 iter/s, 2.9363s/10 iters), loss = 6.0286
I0523 00:31:13.042654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0286 (* 1 = 6.0286 loss)
I0523 00:31:13.055757 35003 sgd_solver.cpp:112] Iteration 101700, lr = 0.01
I0523 00:31:15.948760 35003 solver.cpp:239] Iteration 101710 (3.44119 iter/s, 2.90597s/10 iters), loss = 6.70269
I0523 00:31:15.948822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70269 (* 1 = 6.70269 loss)
I0523 00:31:16.688195 35003 sgd_solver.cpp:112] Iteration 101710, lr = 0.01
I0523 00:31:21.056229 35003 solver.cpp:239] Iteration 101720 (1.95802 iter/s, 5.1072s/10 iters), loss = 6.16542
I0523 00:31:21.056291 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16542 (* 1 = 6.16542 loss)
I0523 00:31:21.064313 35003 sgd_solver.cpp:112] Iteration 101720, lr = 0.01
I0523 00:31:23.417981 35003 solver.cpp:239] Iteration 101730 (4.23444 iter/s, 2.36158s/10 iters), loss = 6.89939
I0523 00:31:23.418277 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89939 (* 1 = 6.89939 loss)
I0523 00:31:23.431426 35003 sgd_solver.cpp:112] Iteration 101730, lr = 0.01
I0523 00:31:26.130095 35003 solver.cpp:239] Iteration 101740 (3.68767 iter/s, 2.71174s/10 iters), loss = 6.76894
I0523 00:31:26.130133 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76894 (* 1 = 6.76894 loss)
I0523 00:31:26.831743 35003 sgd_solver.cpp:112] Iteration 101740, lr = 0.01
I0523 00:31:30.070595 35003 solver.cpp:239] Iteration 101750 (2.53788 iter/s, 3.9403s/10 iters), loss = 7.60234
I0523 00:31:30.070636 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60234 (* 1 = 7.60234 loss)
I0523 00:31:30.089118 35003 sgd_solver.cpp:112] Iteration 101750, lr = 0.01
I0523 00:31:33.212765 35003 solver.cpp:239] Iteration 101760 (3.18269 iter/s, 3.142s/10 iters), loss = 7.26433
I0523 00:31:33.212815 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26433 (* 1 = 7.26433 loss)
I0523 00:31:33.947829 35003 sgd_solver.cpp:112] Iteration 101760, lr = 0.01
I0523 00:31:36.042179 35003 solver.cpp:239] Iteration 101770 (3.53451 iter/s, 2.82925s/10 iters), loss = 5.91771
I0523 00:31:36.042227 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91771 (* 1 = 5.91771 loss)
I0523 00:31:36.067247 35003 sgd_solver.cpp:112] Iteration 101770, lr = 0.01
I0523 00:31:40.395476 35003 solver.cpp:239] Iteration 101780 (2.29724 iter/s, 4.35306s/10 iters), loss = 7.78804
I0523 00:31:40.395540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78804 (* 1 = 7.78804 loss)
I0523 00:31:40.753252 35003 sgd_solver.cpp:112] Iteration 101780, lr = 0.01
I0523 00:31:42.826280 35003 solver.cpp:239] Iteration 101790 (4.11421 iter/s, 2.4306s/10 iters), loss = 7.53535
I0523 00:31:42.826329 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53535 (* 1 = 7.53535 loss)
I0523 00:31:42.839653 35003 sgd_solver.cpp:112] Iteration 101790, lr = 0.01
I0523 00:31:44.718166 35003 solver.cpp:239] Iteration 101800 (5.2861 iter/s, 1.89175s/10 iters), loss = 7.52066
I0523 00:31:44.718206 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52066 (* 1 = 7.52066 loss)
I0523 00:31:44.728549 35003 sgd_solver.cpp:112] Iteration 101800, lr = 0.01
I0523 00:31:46.783972 35003 solver.cpp:239] Iteration 101810 (4.84103 iter/s, 2.06567s/10 iters), loss = 5.87606
I0523 00:31:46.784013 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87606 (* 1 = 5.87606 loss)
I0523 00:31:46.808188 35003 sgd_solver.cpp:112] Iteration 101810, lr = 0.01
I0523 00:31:51.071571 35003 solver.cpp:239] Iteration 101820 (2.33242 iter/s, 4.28739s/10 iters), loss = 7.06132
I0523 00:31:51.071614 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06132 (* 1 = 7.06132 loss)
I0523 00:31:51.220422 35003 sgd_solver.cpp:112] Iteration 101820, lr = 0.01
I0523 00:31:55.606907 35003 solver.cpp:239] Iteration 101830 (2.20506 iter/s, 4.53503s/10 iters), loss = 7.36408
I0523 00:31:55.607142 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36408 (* 1 = 7.36408 loss)
I0523 00:31:55.619248 35003 sgd_solver.cpp:112] Iteration 101830, lr = 0.01
I0523 00:32:00.533555 35003 solver.cpp:239] Iteration 101840 (2.02995 iter/s, 4.92623s/10 iters), loss = 7.84072
I0523 00:32:00.533624 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84072 (* 1 = 7.84072 loss)
I0523 00:32:00.540437 35003 sgd_solver.cpp:112] Iteration 101840, lr = 0.01
I0523 00:32:03.330178 35003 solver.cpp:239] Iteration 101850 (3.57598 iter/s, 2.79644s/10 iters), loss = 6.52301
I0523 00:32:03.330224 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52301 (* 1 = 6.52301 loss)
I0523 00:32:03.343750 35003 sgd_solver.cpp:112] Iteration 101850, lr = 0.01
I0523 00:32:07.014292 35003 solver.cpp:239] Iteration 101860 (2.71451 iter/s, 3.68391s/10 iters), loss = 6.28679
I0523 00:32:07.014340 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28679 (* 1 = 6.28679 loss)
I0523 00:32:07.708989 35003 sgd_solver.cpp:112] Iteration 101860, lr = 0.01
I0523 00:32:11.296221 35003 solver.cpp:239] Iteration 101870 (2.33552 iter/s, 4.2817s/10 iters), loss = 7.52886
I0523 00:32:11.296265 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52886 (* 1 = 7.52886 loss)
I0523 00:32:11.309995 35003 sgd_solver.cpp:112] Iteration 101870, lr = 0.01
I0523 00:32:14.810541 35003 solver.cpp:239] Iteration 101880 (2.84565 iter/s, 3.51413s/10 iters), loss = 7.86006
I0523 00:32:14.810593 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86006 (* 1 = 7.86006 loss)
I0523 00:32:14.824071 35003 sgd_solver.cpp:112] Iteration 101880, lr = 0.01
I0523 00:32:19.134903 35003 solver.cpp:239] Iteration 101890 (2.3126 iter/s, 4.32414s/10 iters), loss = 7.87086
I0523 00:32:19.134943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87086 (* 1 = 7.87086 loss)
I0523 00:32:19.744555 35003 sgd_solver.cpp:112] Iteration 101890, lr = 0.01
I0523 00:32:23.325675 35003 solver.cpp:239] Iteration 101900 (2.38632 iter/s, 4.19056s/10 iters), loss = 6.98992
I0523 00:32:23.325726 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98992 (* 1 = 6.98992 loss)
I0523 00:32:23.333393 35003 sgd_solver.cpp:112] Iteration 101900, lr = 0.01
I0523 00:32:26.463719 35003 solver.cpp:239] Iteration 101910 (3.18689 iter/s, 3.13786s/10 iters), loss = 7.3631
I0523 00:32:26.463969 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3631 (* 1 = 7.3631 loss)
I0523 00:32:27.191807 35003 sgd_solver.cpp:112] Iteration 101910, lr = 0.01
I0523 00:32:30.763862 35003 solver.cpp:239] Iteration 101920 (2.32572 iter/s, 4.29974s/10 iters), loss = 6.20406
I0523 00:32:30.763911 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20406 (* 1 = 6.20406 loss)
I0523 00:32:30.777894 35003 sgd_solver.cpp:112] Iteration 101920, lr = 0.01
I0523 00:32:34.367427 35003 solver.cpp:239] Iteration 101930 (2.77519 iter/s, 3.60336s/10 iters), loss = 6.49833
I0523 00:32:34.367487 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49833 (* 1 = 6.49833 loss)
I0523 00:32:34.551805 35003 sgd_solver.cpp:112] Iteration 101930, lr = 0.01
I0523 00:32:38.669081 35003 solver.cpp:239] Iteration 101940 (2.32482 iter/s, 4.30142s/10 iters), loss = 7.87377
I0523 00:32:38.669136 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87377 (* 1 = 7.87377 loss)
I0523 00:32:38.675572 35003 sgd_solver.cpp:112] Iteration 101940, lr = 0.01
I0523 00:32:42.288707 35003 solver.cpp:239] Iteration 101950 (2.76287 iter/s, 3.61942s/10 iters), loss = 6.38977
I0523 00:32:42.288761 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38977 (* 1 = 6.38977 loss)
I0523 00:32:43.029793 35003 sgd_solver.cpp:112] Iteration 101950, lr = 0.01
I0523 00:32:48.055202 35003 solver.cpp:239] Iteration 101960 (1.73424 iter/s, 5.7662s/10 iters), loss = 7.5832
I0523 00:32:48.055253 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5832 (* 1 = 7.5832 loss)
I0523 00:32:48.103929 35003 sgd_solver.cpp:112] Iteration 101960, lr = 0.01
I0523 00:32:51.793807 35003 solver.cpp:239] Iteration 101970 (2.67494 iter/s, 3.7384s/10 iters), loss = 6.86788
I0523 00:32:51.793850 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86788 (* 1 = 6.86788 loss)
I0523 00:32:52.509388 35003 sgd_solver.cpp:112] Iteration 101970, lr = 0.01
I0523 00:32:56.654690 35003 solver.cpp:239] Iteration 101980 (2.05734 iter/s, 4.86064s/10 iters), loss = 8.75864
I0523 00:32:56.654955 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.75864 (* 1 = 8.75864 loss)
I0523 00:32:56.667915 35003 sgd_solver.cpp:112] Iteration 101980, lr = 0.01
I0523 00:32:59.043342 35003 solver.cpp:239] Iteration 101990 (4.18707 iter/s, 2.38831s/10 iters), loss = 6.68189
I0523 00:32:59.043400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68189 (* 1 = 6.68189 loss)
I0523 00:32:59.046607 35003 sgd_solver.cpp:112] Iteration 101990, lr = 0.01
I0523 00:33:01.805495 35003 solver.cpp:239] Iteration 102000 (3.62062 iter/s, 2.76196s/10 iters), loss = 7.71886
I0523 00:33:01.805548 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71886 (* 1 = 7.71886 loss)
I0523 00:33:01.822978 35003 sgd_solver.cpp:112] Iteration 102000, lr = 0.01
I0523 00:33:05.019371 35003 solver.cpp:239] Iteration 102010 (3.11169 iter/s, 3.21369s/10 iters), loss = 7.89195
I0523 00:33:05.019417 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89195 (* 1 = 7.89195 loss)
I0523 00:33:05.113792 35003 sgd_solver.cpp:112] Iteration 102010, lr = 0.01
I0523 00:33:08.805230 35003 solver.cpp:239] Iteration 102020 (2.64155 iter/s, 3.78566s/10 iters), loss = 7.51219
I0523 00:33:08.805276 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51219 (* 1 = 7.51219 loss)
I0523 00:33:09.520743 35003 sgd_solver.cpp:112] Iteration 102020, lr = 0.01
I0523 00:33:11.674564 35003 solver.cpp:239] Iteration 102030 (3.48537 iter/s, 2.86914s/10 iters), loss = 7.52857
I0523 00:33:11.674615 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52857 (* 1 = 7.52857 loss)
I0523 00:33:12.389473 35003 sgd_solver.cpp:112] Iteration 102030, lr = 0.01
I0523 00:33:16.633096 35003 solver.cpp:239] Iteration 102040 (2.01683 iter/s, 4.95828s/10 iters), loss = 7.98662
I0523 00:33:16.633152 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98662 (* 1 = 7.98662 loss)
I0523 00:33:16.654832 35003 sgd_solver.cpp:112] Iteration 102040, lr = 0.01
I0523 00:33:19.098037 35003 solver.cpp:239] Iteration 102050 (4.05715 iter/s, 2.46478s/10 iters), loss = 8.69556
I0523 00:33:19.098076 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.69556 (* 1 = 8.69556 loss)
I0523 00:33:19.116237 35003 sgd_solver.cpp:112] Iteration 102050, lr = 0.01
I0523 00:33:21.039868 35003 solver.cpp:239] Iteration 102060 (5.15013 iter/s, 1.9417s/10 iters), loss = 6.51277
I0523 00:33:21.039913 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51277 (* 1 = 6.51277 loss)
I0523 00:33:21.043740 35003 sgd_solver.cpp:112] Iteration 102060, lr = 0.01
I0523 00:33:24.617971 35003 solver.cpp:239] Iteration 102070 (2.79494 iter/s, 3.5779s/10 iters), loss = 8.08033
I0523 00:33:24.618019 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08033 (* 1 = 8.08033 loss)
I0523 00:33:24.629884 35003 sgd_solver.cpp:112] Iteration 102070, lr = 0.01
I0523 00:33:29.690152 35003 solver.cpp:239] Iteration 102080 (1.97164 iter/s, 5.07191s/10 iters), loss = 6.34033
I0523 00:33:29.690384 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34033 (* 1 = 6.34033 loss)
I0523 00:33:30.391502 35003 sgd_solver.cpp:112] Iteration 102080, lr = 0.01
I0523 00:33:33.142505 35003 solver.cpp:239] Iteration 102090 (2.89687 iter/s, 3.452s/10 iters), loss = 7.35376
I0523 00:33:33.142554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35376 (* 1 = 7.35376 loss)
I0523 00:33:33.148829 35003 sgd_solver.cpp:112] Iteration 102090, lr = 0.01
I0523 00:33:36.056773 35003 solver.cpp:239] Iteration 102100 (3.4316 iter/s, 2.91409s/10 iters), loss = 7.84965
I0523 00:33:36.056818 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84965 (* 1 = 7.84965 loss)
I0523 00:33:36.625512 35003 sgd_solver.cpp:112] Iteration 102100, lr = 0.01
I0523 00:33:39.583935 35003 solver.cpp:239] Iteration 102110 (2.8353 iter/s, 3.52697s/10 iters), loss = 7.1232
I0523 00:33:39.583981 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1232 (* 1 = 7.1232 loss)
I0523 00:33:40.323173 35003 sgd_solver.cpp:112] Iteration 102110, lr = 0.01
I0523 00:33:42.248260 35003 solver.cpp:239] Iteration 102120 (3.75353 iter/s, 2.66416s/10 iters), loss = 9.28468
I0523 00:33:42.248306 35003 solver.cpp:258]     Train net output #0: softmax_loss = 9.28468 (* 1 = 9.28468 loss)
I0523 00:33:42.266312 35003 sgd_solver.cpp:112] Iteration 102120, lr = 0.01
I0523 00:33:45.871070 35003 solver.cpp:239] Iteration 102130 (2.76044 iter/s, 3.62262s/10 iters), loss = 6.59722
I0523 00:33:45.871117 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59722 (* 1 = 6.59722 loss)
I0523 00:33:45.883419 35003 sgd_solver.cpp:112] Iteration 102130, lr = 0.01
I0523 00:33:48.593075 35003 solver.cpp:239] Iteration 102140 (3.67399 iter/s, 2.72184s/10 iters), loss = 6.44394
I0523 00:33:48.593124 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44394 (* 1 = 6.44394 loss)
I0523 00:33:48.598347 35003 sgd_solver.cpp:112] Iteration 102140, lr = 0.01
I0523 00:33:52.193696 35003 solver.cpp:239] Iteration 102150 (2.77746 iter/s, 3.60041s/10 iters), loss = 6.81517
I0523 00:33:52.193747 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81517 (* 1 = 6.81517 loss)
I0523 00:33:52.206339 35003 sgd_solver.cpp:112] Iteration 102150, lr = 0.01
I0523 00:33:54.177999 35003 solver.cpp:239] Iteration 102160 (5.03991 iter/s, 1.98416s/10 iters), loss = 7.60181
I0523 00:33:54.178051 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60181 (* 1 = 7.60181 loss)
I0523 00:33:54.190752 35003 sgd_solver.cpp:112] Iteration 102160, lr = 0.01
I0523 00:33:57.393978 35003 solver.cpp:239] Iteration 102170 (3.10965 iter/s, 3.2158s/10 iters), loss = 7.07979
I0523 00:33:57.394021 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07979 (* 1 = 7.07979 loss)
I0523 00:33:57.407233 35003 sgd_solver.cpp:112] Iteration 102170, lr = 0.01
I0523 00:34:00.205375 35003 solver.cpp:239] Iteration 102180 (3.55715 iter/s, 2.81124s/10 iters), loss = 7.14857
I0523 00:34:00.205626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14857 (* 1 = 7.14857 loss)
I0523 00:34:00.218636 35003 sgd_solver.cpp:112] Iteration 102180, lr = 0.01
I0523 00:34:04.029979 35003 solver.cpp:239] Iteration 102190 (2.61491 iter/s, 3.82423s/10 iters), loss = 5.97821
I0523 00:34:04.030022 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97821 (* 1 = 5.97821 loss)
I0523 00:34:04.524291 35003 sgd_solver.cpp:112] Iteration 102190, lr = 0.01
I0523 00:34:07.305131 35003 solver.cpp:239] Iteration 102200 (3.05346 iter/s, 3.27497s/10 iters), loss = 7.18313
I0523 00:34:07.305177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18313 (* 1 = 7.18313 loss)
I0523 00:34:07.322500 35003 sgd_solver.cpp:112] Iteration 102200, lr = 0.01
I0523 00:34:10.752956 35003 solver.cpp:239] Iteration 102210 (2.90055 iter/s, 3.44762s/10 iters), loss = 7.69915
I0523 00:34:10.753007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69915 (* 1 = 7.69915 loss)
I0523 00:34:10.760407 35003 sgd_solver.cpp:112] Iteration 102210, lr = 0.01
I0523 00:34:14.298992 35003 solver.cpp:239] Iteration 102220 (2.82022 iter/s, 3.54583s/10 iters), loss = 7.31478
I0523 00:34:14.299054 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31478 (* 1 = 7.31478 loss)
I0523 00:34:14.304366 35003 sgd_solver.cpp:112] Iteration 102220, lr = 0.01
I0523 00:34:17.072718 35003 solver.cpp:239] Iteration 102230 (3.60549 iter/s, 2.77355s/10 iters), loss = 7.06878
I0523 00:34:17.072769 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06878 (* 1 = 7.06878 loss)
I0523 00:34:17.080018 35003 sgd_solver.cpp:112] Iteration 102230, lr = 0.01
I0523 00:34:20.599620 35003 solver.cpp:239] Iteration 102240 (2.83551 iter/s, 3.52671s/10 iters), loss = 7.16563
I0523 00:34:20.599661 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16563 (* 1 = 7.16563 loss)
I0523 00:34:20.612380 35003 sgd_solver.cpp:112] Iteration 102240, lr = 0.01
I0523 00:34:23.520254 35003 solver.cpp:239] Iteration 102250 (3.42411 iter/s, 2.92047s/10 iters), loss = 8.79077
I0523 00:34:23.520296 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.79077 (* 1 = 8.79077 loss)
I0523 00:34:23.533815 35003 sgd_solver.cpp:112] Iteration 102250, lr = 0.01
I0523 00:34:27.567320 35003 solver.cpp:239] Iteration 102260 (2.47105 iter/s, 4.04686s/10 iters), loss = 6.57335
I0523 00:34:27.567361 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57335 (* 1 = 6.57335 loss)
I0523 00:34:27.581113 35003 sgd_solver.cpp:112] Iteration 102260, lr = 0.01
I0523 00:34:30.689673 35003 solver.cpp:239] Iteration 102270 (3.20289 iter/s, 3.12218s/10 iters), loss = 7.40856
I0523 00:34:30.689961 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40856 (* 1 = 7.40856 loss)
I0523 00:34:30.699404 35003 sgd_solver.cpp:112] Iteration 102270, lr = 0.01
I0523 00:34:33.458082 35003 solver.cpp:239] Iteration 102280 (3.61268 iter/s, 2.76803s/10 iters), loss = 6.08669
I0523 00:34:33.458128 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08669 (* 1 = 6.08669 loss)
I0523 00:34:33.469177 35003 sgd_solver.cpp:112] Iteration 102280, lr = 0.01
I0523 00:34:36.015445 35003 solver.cpp:239] Iteration 102290 (3.91052 iter/s, 2.55721s/10 iters), loss = 6.98559
I0523 00:34:36.015492 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98559 (* 1 = 6.98559 loss)
I0523 00:34:36.029062 35003 sgd_solver.cpp:112] Iteration 102290, lr = 0.01
I0523 00:34:38.362707 35003 solver.cpp:239] Iteration 102300 (4.26056 iter/s, 2.34711s/10 iters), loss = 6.54434
I0523 00:34:38.362747 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54434 (* 1 = 6.54434 loss)
I0523 00:34:38.376160 35003 sgd_solver.cpp:112] Iteration 102300, lr = 0.01
I0523 00:34:41.919064 35003 solver.cpp:239] Iteration 102310 (2.81203 iter/s, 3.55615s/10 iters), loss = 7.35736
I0523 00:34:41.919124 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35736 (* 1 = 7.35736 loss)
I0523 00:34:42.429520 35003 sgd_solver.cpp:112] Iteration 102310, lr = 0.01
I0523 00:34:46.467389 35003 solver.cpp:239] Iteration 102320 (2.19873 iter/s, 4.54808s/10 iters), loss = 6.01648
I0523 00:34:46.467440 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01648 (* 1 = 6.01648 loss)
I0523 00:34:47.199425 35003 sgd_solver.cpp:112] Iteration 102320, lr = 0.01
I0523 00:34:49.575736 35003 solver.cpp:239] Iteration 102330 (3.21734 iter/s, 3.10816s/10 iters), loss = 6.95748
I0523 00:34:49.575793 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95748 (* 1 = 6.95748 loss)
I0523 00:34:50.301184 35003 sgd_solver.cpp:112] Iteration 102330, lr = 0.01
I0523 00:34:53.863693 35003 solver.cpp:239] Iteration 102340 (2.33224 iter/s, 4.28772s/10 iters), loss = 7.83968
I0523 00:34:53.863751 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83968 (* 1 = 7.83968 loss)
I0523 00:34:54.604538 35003 sgd_solver.cpp:112] Iteration 102340, lr = 0.01
I0523 00:34:59.510855 35003 solver.cpp:239] Iteration 102350 (1.77089 iter/s, 5.64687s/10 iters), loss = 7.37894
I0523 00:34:59.510915 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37894 (* 1 = 7.37894 loss)
I0523 00:35:00.076064 35003 sgd_solver.cpp:112] Iteration 102350, lr = 0.01
I0523 00:35:02.798082 35003 solver.cpp:239] Iteration 102360 (3.04226 iter/s, 3.28703s/10 iters), loss = 6.92151
I0523 00:35:02.798337 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92151 (* 1 = 6.92151 loss)
I0523 00:35:02.811127 35003 sgd_solver.cpp:112] Iteration 102360, lr = 0.01
I0523 00:35:06.889849 35003 solver.cpp:239] Iteration 102370 (2.44417 iter/s, 4.09136s/10 iters), loss = 7.42707
I0523 00:35:06.889894 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42707 (* 1 = 7.42707 loss)
I0523 00:35:06.896767 35003 sgd_solver.cpp:112] Iteration 102370, lr = 0.01
I0523 00:35:11.772512 35003 solver.cpp:239] Iteration 102380 (2.04817 iter/s, 4.88241s/10 iters), loss = 6.13275
I0523 00:35:11.772560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13275 (* 1 = 6.13275 loss)
I0523 00:35:12.474056 35003 sgd_solver.cpp:112] Iteration 102380, lr = 0.01
I0523 00:35:17.614306 35003 solver.cpp:239] Iteration 102390 (1.71189 iter/s, 5.84151s/10 iters), loss = 8.2018
I0523 00:35:17.614356 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.2018 (* 1 = 8.2018 loss)
I0523 00:35:17.633245 35003 sgd_solver.cpp:112] Iteration 102390, lr = 0.01
I0523 00:35:21.276268 35003 solver.cpp:239] Iteration 102400 (2.73093 iter/s, 3.66176s/10 iters), loss = 6.86037
I0523 00:35:21.276311 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86037 (* 1 = 6.86037 loss)
I0523 00:35:21.283419 35003 sgd_solver.cpp:112] Iteration 102400, lr = 0.01
I0523 00:35:25.709095 35003 solver.cpp:239] Iteration 102410 (2.25602 iter/s, 4.43259s/10 iters), loss = 6.83977
I0523 00:35:25.709137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83977 (* 1 = 6.83977 loss)
I0523 00:35:25.712354 35003 sgd_solver.cpp:112] Iteration 102410, lr = 0.01
I0523 00:35:27.860577 35003 solver.cpp:239] Iteration 102420 (4.64826 iter/s, 2.15134s/10 iters), loss = 8.12384
I0523 00:35:27.860646 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.12384 (* 1 = 8.12384 loss)
I0523 00:35:28.587394 35003 sgd_solver.cpp:112] Iteration 102420, lr = 0.01
I0523 00:35:31.102103 35003 solver.cpp:239] Iteration 102430 (3.08517 iter/s, 3.24131s/10 iters), loss = 7.53853
I0523 00:35:31.102161 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53853 (* 1 = 7.53853 loss)
I0523 00:35:31.107388 35003 sgd_solver.cpp:112] Iteration 102430, lr = 0.01
I0523 00:35:36.122608 35003 solver.cpp:239] Iteration 102440 (1.99194 iter/s, 5.02023s/10 iters), loss = 7.54226
I0523 00:35:36.122872 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54226 (* 1 = 7.54226 loss)
I0523 00:35:36.543865 35003 sgd_solver.cpp:112] Iteration 102440, lr = 0.01
I0523 00:35:38.704774 35003 solver.cpp:239] Iteration 102450 (3.87322 iter/s, 2.58183s/10 iters), loss = 7.22703
I0523 00:35:38.704819 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22703 (* 1 = 7.22703 loss)
I0523 00:35:39.445431 35003 sgd_solver.cpp:112] Iteration 102450, lr = 0.01
I0523 00:35:42.320838 35003 solver.cpp:239] Iteration 102460 (2.76559 iter/s, 3.61586s/10 iters), loss = 8.21164
I0523 00:35:42.320889 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.21164 (* 1 = 8.21164 loss)
I0523 00:35:43.036170 35003 sgd_solver.cpp:112] Iteration 102460, lr = 0.01
I0523 00:35:45.964643 35003 solver.cpp:239] Iteration 102470 (2.74456 iter/s, 3.64357s/10 iters), loss = 7.12753
I0523 00:35:45.964685 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12753 (* 1 = 7.12753 loss)
I0523 00:35:45.969971 35003 sgd_solver.cpp:112] Iteration 102470, lr = 0.01
I0523 00:35:49.608829 35003 solver.cpp:239] Iteration 102480 (2.74424 iter/s, 3.64399s/10 iters), loss = 7.23257
I0523 00:35:49.608885 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23257 (* 1 = 7.23257 loss)
I0523 00:35:50.323544 35003 sgd_solver.cpp:112] Iteration 102480, lr = 0.01
I0523 00:35:54.299413 35003 solver.cpp:239] Iteration 102490 (2.13205 iter/s, 4.69033s/10 iters), loss = 7.51903
I0523 00:35:54.299458 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51903 (* 1 = 7.51903 loss)
I0523 00:35:54.304574 35003 sgd_solver.cpp:112] Iteration 102490, lr = 0.01
I0523 00:35:58.573014 35003 solver.cpp:239] Iteration 102500 (2.34007 iter/s, 4.27338s/10 iters), loss = 7.37405
I0523 00:35:58.573063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37405 (* 1 = 7.37405 loss)
I0523 00:35:58.594686 35003 sgd_solver.cpp:112] Iteration 102500, lr = 0.01
I0523 00:36:01.467413 35003 solver.cpp:239] Iteration 102510 (3.45515 iter/s, 2.89423s/10 iters), loss = 6.73092
I0523 00:36:01.467464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73092 (* 1 = 6.73092 loss)
I0523 00:36:01.475589 35003 sgd_solver.cpp:112] Iteration 102510, lr = 0.01
I0523 00:36:04.348987 35003 solver.cpp:239] Iteration 102520 (3.47053 iter/s, 2.8814s/10 iters), loss = 7.01298
I0523 00:36:04.349037 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01298 (* 1 = 7.01298 loss)
I0523 00:36:05.081557 35003 sgd_solver.cpp:112] Iteration 102520, lr = 0.01
I0523 00:36:08.397850 35003 solver.cpp:239] Iteration 102530 (2.46997 iter/s, 4.04864s/10 iters), loss = 7.05153
I0523 00:36:08.398087 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05153 (* 1 = 7.05153 loss)
I0523 00:36:09.038403 35003 sgd_solver.cpp:112] Iteration 102530, lr = 0.01
I0523 00:36:13.367899 35003 solver.cpp:239] Iteration 102540 (2.01223 iter/s, 4.96962s/10 iters), loss = 6.95628
I0523 00:36:13.367950 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95628 (* 1 = 6.95628 loss)
I0523 00:36:14.105621 35003 sgd_solver.cpp:112] Iteration 102540, lr = 0.01
I0523 00:36:16.160014 35003 solver.cpp:239] Iteration 102550 (3.58173 iter/s, 2.79195s/10 iters), loss = 7.30284
I0523 00:36:16.160049 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30284 (* 1 = 7.30284 loss)
I0523 00:36:16.175793 35003 sgd_solver.cpp:112] Iteration 102550, lr = 0.01
I0523 00:36:19.067760 35003 solver.cpp:239] Iteration 102560 (3.43928 iter/s, 2.90758s/10 iters), loss = 7.82082
I0523 00:36:19.067816 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82082 (* 1 = 7.82082 loss)
I0523 00:36:19.157335 35003 sgd_solver.cpp:112] Iteration 102560, lr = 0.01
I0523 00:36:24.051782 35003 solver.cpp:239] Iteration 102570 (2.00652 iter/s, 4.98376s/10 iters), loss = 7.33816
I0523 00:36:24.051827 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33816 (* 1 = 7.33816 loss)
I0523 00:36:24.760790 35003 sgd_solver.cpp:112] Iteration 102570, lr = 0.01
I0523 00:36:27.705859 35003 solver.cpp:239] Iteration 102580 (2.73681 iter/s, 3.65388s/10 iters), loss = 6.71893
I0523 00:36:27.705922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71893 (* 1 = 6.71893 loss)
I0523 00:36:28.408604 35003 sgd_solver.cpp:112] Iteration 102580, lr = 0.01
I0523 00:36:31.347945 35003 solver.cpp:239] Iteration 102590 (2.74584 iter/s, 3.64187s/10 iters), loss = 8.12024
I0523 00:36:31.347987 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.12024 (* 1 = 8.12024 loss)
I0523 00:36:31.355643 35003 sgd_solver.cpp:112] Iteration 102590, lr = 0.01
I0523 00:36:34.162655 35003 solver.cpp:239] Iteration 102600 (3.55298 iter/s, 2.81454s/10 iters), loss = 6.65276
I0523 00:36:34.162720 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65276 (* 1 = 6.65276 loss)
I0523 00:36:34.166749 35003 sgd_solver.cpp:112] Iteration 102600, lr = 0.01
I0523 00:36:37.546933 35003 solver.cpp:239] Iteration 102610 (2.955 iter/s, 3.38409s/10 iters), loss = 6.76432
I0523 00:36:37.546974 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76432 (* 1 = 6.76432 loss)
I0523 00:36:37.553876 35003 sgd_solver.cpp:112] Iteration 102610, lr = 0.01
I0523 00:36:41.821431 35003 solver.cpp:239] Iteration 102620 (2.33958 iter/s, 4.27428s/10 iters), loss = 6.00494
I0523 00:36:41.821660 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00494 (* 1 = 6.00494 loss)
I0523 00:36:41.831070 35003 sgd_solver.cpp:112] Iteration 102620, lr = 0.01
I0523 00:36:45.481348 35003 solver.cpp:239] Iteration 102630 (2.73257 iter/s, 3.65955s/10 iters), loss = 7.64107
I0523 00:36:45.481405 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64107 (* 1 = 7.64107 loss)
I0523 00:36:46.209007 35003 sgd_solver.cpp:112] Iteration 102630, lr = 0.01
I0523 00:36:47.691875 35003 solver.cpp:239] Iteration 102640 (4.52413 iter/s, 2.21037s/10 iters), loss = 8.34768
I0523 00:36:47.691931 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.34768 (* 1 = 8.34768 loss)
I0523 00:36:47.705276 35003 sgd_solver.cpp:112] Iteration 102640, lr = 0.01
I0523 00:36:52.053078 35003 solver.cpp:239] Iteration 102650 (2.29307 iter/s, 4.36097s/10 iters), loss = 7.91021
I0523 00:36:52.053128 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91021 (* 1 = 7.91021 loss)
I0523 00:36:52.065042 35003 sgd_solver.cpp:112] Iteration 102650, lr = 0.01
I0523 00:36:54.990252 35003 solver.cpp:239] Iteration 102660 (3.40483 iter/s, 2.937s/10 iters), loss = 6.99836
I0523 00:36:54.990298 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99836 (* 1 = 6.99836 loss)
I0523 00:36:55.718080 35003 sgd_solver.cpp:112] Iteration 102660, lr = 0.01
I0523 00:36:59.459153 35003 solver.cpp:239] Iteration 102670 (2.2378 iter/s, 4.46867s/10 iters), loss = 6.86946
I0523 00:36:59.459205 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86946 (* 1 = 6.86946 loss)
I0523 00:37:00.168162 35003 sgd_solver.cpp:112] Iteration 102670, lr = 0.01
I0523 00:37:03.730263 35003 solver.cpp:239] Iteration 102680 (2.34144 iter/s, 4.27088s/10 iters), loss = 7.95157
I0523 00:37:03.730303 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95157 (* 1 = 7.95157 loss)
I0523 00:37:03.738096 35003 sgd_solver.cpp:112] Iteration 102680, lr = 0.01
I0523 00:37:06.902014 35003 solver.cpp:239] Iteration 102690 (3.15301 iter/s, 3.17157s/10 iters), loss = 6.88171
I0523 00:37:06.902071 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88171 (* 1 = 6.88171 loss)
I0523 00:37:07.044569 35003 sgd_solver.cpp:112] Iteration 102690, lr = 0.01
I0523 00:37:11.188000 35003 solver.cpp:239] Iteration 102700 (2.33332 iter/s, 4.28573s/10 iters), loss = 7.38466
I0523 00:37:11.188055 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38466 (* 1 = 7.38466 loss)
I0523 00:37:11.448720 35003 sgd_solver.cpp:112] Iteration 102700, lr = 0.01
I0523 00:37:16.349023 35003 solver.cpp:239] Iteration 102710 (1.9377 iter/s, 5.16076s/10 iters), loss = 7.30115
I0523 00:37:16.349205 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30115 (* 1 = 7.30115 loss)
I0523 00:37:16.674821 35003 sgd_solver.cpp:112] Iteration 102710, lr = 0.01
I0523 00:37:20.303354 35003 solver.cpp:239] Iteration 102720 (2.52909 iter/s, 3.95399s/10 iters), loss = 7.32229
I0523 00:37:20.303396 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32229 (* 1 = 7.32229 loss)
I0523 00:37:20.921655 35003 sgd_solver.cpp:112] Iteration 102720, lr = 0.01
I0523 00:37:23.808034 35003 solver.cpp:239] Iteration 102730 (2.85359 iter/s, 3.50436s/10 iters), loss = 7.72067
I0523 00:37:23.808081 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72067 (* 1 = 7.72067 loss)
I0523 00:37:24.543083 35003 sgd_solver.cpp:112] Iteration 102730, lr = 0.01
I0523 00:37:28.771615 35003 solver.cpp:239] Iteration 102740 (2.01478 iter/s, 4.96333s/10 iters), loss = 7.4494
I0523 00:37:28.771662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4494 (* 1 = 7.4494 loss)
I0523 00:37:28.779546 35003 sgd_solver.cpp:112] Iteration 102740, lr = 0.01
I0523 00:37:32.006677 35003 solver.cpp:239] Iteration 102750 (3.09131 iter/s, 3.23487s/10 iters), loss = 7.99361
I0523 00:37:32.006753 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99361 (* 1 = 7.99361 loss)
I0523 00:37:32.739115 35003 sgd_solver.cpp:112] Iteration 102750, lr = 0.01
I0523 00:37:37.142168 35003 solver.cpp:239] Iteration 102760 (1.94734 iter/s, 5.1352s/10 iters), loss = 6.7769
I0523 00:37:37.142225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7769 (* 1 = 6.7769 loss)
I0523 00:37:37.449740 35003 sgd_solver.cpp:112] Iteration 102760, lr = 0.01
I0523 00:37:40.298177 35003 solver.cpp:239] Iteration 102770 (3.16875 iter/s, 3.15582s/10 iters), loss = 7.13154
I0523 00:37:40.298220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13154 (* 1 = 7.13154 loss)
I0523 00:37:40.316082 35003 sgd_solver.cpp:112] Iteration 102770, lr = 0.01
I0523 00:37:43.795616 35003 solver.cpp:239] Iteration 102780 (2.8594 iter/s, 3.49724s/10 iters), loss = 6.45953
I0523 00:37:43.795665 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45953 (* 1 = 6.45953 loss)
I0523 00:37:44.536242 35003 sgd_solver.cpp:112] Iteration 102780, lr = 0.01
I0523 00:37:47.296011 35003 solver.cpp:239] Iteration 102790 (2.85698 iter/s, 3.5002s/10 iters), loss = 7.3319
I0523 00:37:47.296197 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3319 (* 1 = 7.3319 loss)
I0523 00:37:47.952718 35003 sgd_solver.cpp:112] Iteration 102790, lr = 0.01
I0523 00:37:51.677353 35003 solver.cpp:239] Iteration 102800 (2.28259 iter/s, 4.38098s/10 iters), loss = 7.22782
I0523 00:37:51.677402 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22782 (* 1 = 7.22782 loss)
I0523 00:37:51.690316 35003 sgd_solver.cpp:112] Iteration 102800, lr = 0.01
I0523 00:37:56.512398 35003 solver.cpp:239] Iteration 102810 (2.06835 iter/s, 4.83478s/10 iters), loss = 5.44819
I0523 00:37:56.512454 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.44819 (* 1 = 5.44819 loss)
I0523 00:37:57.233523 35003 sgd_solver.cpp:112] Iteration 102810, lr = 0.01
I0523 00:38:01.595731 35003 solver.cpp:239] Iteration 102820 (1.96731 iter/s, 5.08308s/10 iters), loss = 8.31313
I0523 00:38:01.595772 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.31313 (* 1 = 8.31313 loss)
I0523 00:38:01.603502 35003 sgd_solver.cpp:112] Iteration 102820, lr = 0.01
I0523 00:38:05.991667 35003 solver.cpp:239] Iteration 102830 (2.27495 iter/s, 4.39571s/10 iters), loss = 8.16917
I0523 00:38:05.991726 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16917 (* 1 = 8.16917 loss)
I0523 00:38:06.709497 35003 sgd_solver.cpp:112] Iteration 102830, lr = 0.01
I0523 00:38:08.798509 35003 solver.cpp:239] Iteration 102840 (3.56295 iter/s, 2.80666s/10 iters), loss = 7.04902
I0523 00:38:08.798557 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04902 (* 1 = 7.04902 loss)
I0523 00:38:08.812058 35003 sgd_solver.cpp:112] Iteration 102840, lr = 0.01
I0523 00:38:10.372691 35003 solver.cpp:239] Iteration 102850 (6.353 iter/s, 1.57406s/10 iters), loss = 6.727
I0523 00:38:10.372741 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.727 (* 1 = 6.727 loss)
I0523 00:38:10.892833 35003 sgd_solver.cpp:112] Iteration 102850, lr = 0.01
I0523 00:38:13.742913 35003 solver.cpp:239] Iteration 102860 (2.96733 iter/s, 3.37003s/10 iters), loss = 7.58766
I0523 00:38:13.742959 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58766 (* 1 = 7.58766 loss)
I0523 00:38:13.754235 35003 sgd_solver.cpp:112] Iteration 102860, lr = 0.01
I0523 00:38:16.583178 35003 solver.cpp:239] Iteration 102870 (3.521 iter/s, 2.8401s/10 iters), loss = 7.33531
I0523 00:38:16.583215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33531 (* 1 = 7.33531 loss)
I0523 00:38:16.611315 35003 sgd_solver.cpp:112] Iteration 102870, lr = 0.01
I0523 00:38:20.315716 35003 solver.cpp:239] Iteration 102880 (2.67928 iter/s, 3.73234s/10 iters), loss = 7.11212
I0523 00:38:20.315871 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11212 (* 1 = 7.11212 loss)
I0523 00:38:21.050909 35003 sgd_solver.cpp:112] Iteration 102880, lr = 0.01
I0523 00:38:25.274236 35003 solver.cpp:239] Iteration 102890 (2.01688 iter/s, 4.95816s/10 iters), loss = 7.42035
I0523 00:38:25.274278 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42035 (* 1 = 7.42035 loss)
I0523 00:38:25.287124 35003 sgd_solver.cpp:112] Iteration 102890, lr = 0.01
I0523 00:38:28.918475 35003 solver.cpp:239] Iteration 102900 (2.7442 iter/s, 3.64405s/10 iters), loss = 6.66366
I0523 00:38:28.918519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66366 (* 1 = 6.66366 loss)
I0523 00:38:29.653528 35003 sgd_solver.cpp:112] Iteration 102900, lr = 0.01
I0523 00:38:32.549755 35003 solver.cpp:239] Iteration 102910 (2.754 iter/s, 3.63109s/10 iters), loss = 6.88365
I0523 00:38:32.549798 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88365 (* 1 = 6.88365 loss)
I0523 00:38:33.278455 35003 sgd_solver.cpp:112] Iteration 102910, lr = 0.01
I0523 00:38:36.226249 35003 solver.cpp:239] Iteration 102920 (2.72013 iter/s, 3.6763s/10 iters), loss = 7.22471
I0523 00:38:36.226295 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22471 (* 1 = 7.22471 loss)
I0523 00:38:36.229728 35003 sgd_solver.cpp:112] Iteration 102920, lr = 0.01
I0523 00:38:39.725066 35003 solver.cpp:239] Iteration 102930 (2.85827 iter/s, 3.49862s/10 iters), loss = 8.24163
I0523 00:38:39.725118 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.24163 (* 1 = 8.24163 loss)
I0523 00:38:40.446800 35003 sgd_solver.cpp:112] Iteration 102930, lr = 0.01
I0523 00:38:43.086717 35003 solver.cpp:239] Iteration 102940 (2.9749 iter/s, 3.36145s/10 iters), loss = 7.24583
I0523 00:38:43.086760 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24583 (* 1 = 7.24583 loss)
I0523 00:38:43.104981 35003 sgd_solver.cpp:112] Iteration 102940, lr = 0.01
I0523 00:38:46.103355 35003 solver.cpp:239] Iteration 102950 (3.31514 iter/s, 3.01646s/10 iters), loss = 6.57298
I0523 00:38:46.103404 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57298 (* 1 = 6.57298 loss)
I0523 00:38:46.118491 35003 sgd_solver.cpp:112] Iteration 102950, lr = 0.01
I0523 00:38:50.016352 35003 solver.cpp:239] Iteration 102960 (2.55574 iter/s, 3.91277s/10 iters), loss = 6.95569
I0523 00:38:50.016413 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95569 (* 1 = 6.95569 loss)
I0523 00:38:50.737648 35003 sgd_solver.cpp:112] Iteration 102960, lr = 0.01
I0523 00:38:53.591724 35003 solver.cpp:239] Iteration 102970 (2.79708 iter/s, 3.57516s/10 iters), loss = 7.37701
I0523 00:38:53.591771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37701 (* 1 = 7.37701 loss)
I0523 00:38:53.601472 35003 sgd_solver.cpp:112] Iteration 102970, lr = 0.01
I0523 00:38:57.360695 35003 solver.cpp:239] Iteration 102980 (2.65339 iter/s, 3.76877s/10 iters), loss = 6.94531
I0523 00:38:57.360734 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94531 (* 1 = 6.94531 loss)
I0523 00:38:57.394923 35003 sgd_solver.cpp:112] Iteration 102980, lr = 0.01
I0523 00:38:58.213979 35003 solver.cpp:239] Iteration 102990 (11.7206 iter/s, 0.8532s/10 iters), loss = 7.73717
I0523 00:38:58.214020 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73717 (* 1 = 7.73717 loss)
I0523 00:38:58.221448 35003 sgd_solver.cpp:112] Iteration 102990, lr = 0.01
I0523 00:39:00.119622 35003 solver.cpp:239] Iteration 103000 (5.24793 iter/s, 1.90551s/10 iters), loss = 7.20298
I0523 00:39:00.119668 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20298 (* 1 = 7.20298 loss)
I0523 00:39:00.131218 35003 sgd_solver.cpp:112] Iteration 103000, lr = 0.01
I0523 00:39:02.941890 35003 solver.cpp:239] Iteration 103010 (3.54346 iter/s, 2.8221s/10 iters), loss = 7.8413
I0523 00:39:02.941929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8413 (* 1 = 7.8413 loss)
I0523 00:39:02.969012 35003 sgd_solver.cpp:112] Iteration 103010, lr = 0.01
I0523 00:39:07.206429 35003 solver.cpp:239] Iteration 103020 (2.34504 iter/s, 4.26432s/10 iters), loss = 7.45601
I0523 00:39:07.206486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45601 (* 1 = 7.45601 loss)
I0523 00:39:07.213773 35003 sgd_solver.cpp:112] Iteration 103020, lr = 0.01
I0523 00:39:10.911029 35003 solver.cpp:239] Iteration 103030 (2.6995 iter/s, 3.70439s/10 iters), loss = 8.38098
I0523 00:39:10.911080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.38098 (* 1 = 8.38098 loss)
I0523 00:39:11.606422 35003 sgd_solver.cpp:112] Iteration 103030, lr = 0.01
I0523 00:39:16.114485 35003 solver.cpp:239] Iteration 103040 (1.9219 iter/s, 5.20319s/10 iters), loss = 7.42192
I0523 00:39:16.114544 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42192 (* 1 = 7.42192 loss)
I0523 00:39:16.816838 35003 sgd_solver.cpp:112] Iteration 103040, lr = 0.01
I0523 00:39:19.693390 35003 solver.cpp:239] Iteration 103050 (2.79432 iter/s, 3.57869s/10 iters), loss = 6.87159
I0523 00:39:19.693430 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87159 (* 1 = 6.87159 loss)
I0523 00:39:20.434710 35003 sgd_solver.cpp:112] Iteration 103050, lr = 0.01
I0523 00:39:22.408856 35003 solver.cpp:239] Iteration 103060 (3.68282 iter/s, 2.71531s/10 iters), loss = 7.7783
I0523 00:39:22.409114 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7783 (* 1 = 7.7783 loss)
I0523 00:39:22.422653 35003 sgd_solver.cpp:112] Iteration 103060, lr = 0.01
I0523 00:39:25.578042 35003 solver.cpp:239] Iteration 103070 (3.15576 iter/s, 3.16881s/10 iters), loss = 7.90387
I0523 00:39:25.578116 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90387 (* 1 = 7.90387 loss)
I0523 00:39:26.318958 35003 sgd_solver.cpp:112] Iteration 103070, lr = 0.01
I0523 00:39:28.236747 35003 solver.cpp:239] Iteration 103080 (3.76149 iter/s, 2.65852s/10 iters), loss = 7.51389
I0523 00:39:28.236798 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51389 (* 1 = 7.51389 loss)
I0523 00:39:28.957856 35003 sgd_solver.cpp:112] Iteration 103080, lr = 0.01
I0523 00:39:31.819182 35003 solver.cpp:239] Iteration 103090 (2.79156 iter/s, 3.58223s/10 iters), loss = 7.22765
I0523 00:39:31.819221 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22765 (* 1 = 7.22765 loss)
I0523 00:39:31.833379 35003 sgd_solver.cpp:112] Iteration 103090, lr = 0.01
I0523 00:39:35.212182 35003 solver.cpp:239] Iteration 103100 (2.9474 iter/s, 3.39282s/10 iters), loss = 7.52806
I0523 00:39:35.212231 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52806 (* 1 = 7.52806 loss)
I0523 00:39:35.927199 35003 sgd_solver.cpp:112] Iteration 103100, lr = 0.01
I0523 00:39:39.480903 35003 solver.cpp:239] Iteration 103110 (2.34276 iter/s, 4.26847s/10 iters), loss = 7.71108
I0523 00:39:39.480952 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71108 (* 1 = 7.71108 loss)
I0523 00:39:39.486974 35003 sgd_solver.cpp:112] Iteration 103110, lr = 0.01
I0523 00:39:42.377904 35003 solver.cpp:239] Iteration 103120 (3.45206 iter/s, 2.89682s/10 iters), loss = 6.70742
I0523 00:39:42.377974 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70742 (* 1 = 6.70742 loss)
I0523 00:39:43.105082 35003 sgd_solver.cpp:112] Iteration 103120, lr = 0.01
I0523 00:39:45.239410 35003 solver.cpp:239] Iteration 103130 (3.49489 iter/s, 2.86132s/10 iters), loss = 8.53941
I0523 00:39:45.239455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.53941 (* 1 = 8.53941 loss)
I0523 00:39:45.251456 35003 sgd_solver.cpp:112] Iteration 103130, lr = 0.01
I0523 00:39:49.423777 35003 solver.cpp:239] Iteration 103140 (2.38997 iter/s, 4.18415s/10 iters), loss = 7.81943
I0523 00:39:49.423826 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81943 (* 1 = 7.81943 loss)
I0523 00:39:49.432082 35003 sgd_solver.cpp:112] Iteration 103140, lr = 0.01
I0523 00:39:52.437961 35003 solver.cpp:239] Iteration 103150 (3.31785 iter/s, 3.014s/10 iters), loss = 8.86685
I0523 00:39:52.438204 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.86685 (* 1 = 8.86685 loss)
I0523 00:39:53.178310 35003 sgd_solver.cpp:112] Iteration 103150, lr = 0.01
I0523 00:39:56.041721 35003 solver.cpp:239] Iteration 103160 (2.77516 iter/s, 3.6034s/10 iters), loss = 7.37942
I0523 00:39:56.041764 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37942 (* 1 = 7.37942 loss)
I0523 00:39:56.048962 35003 sgd_solver.cpp:112] Iteration 103160, lr = 0.01
I0523 00:39:58.908174 35003 solver.cpp:239] Iteration 103170 (3.48883 iter/s, 2.86629s/10 iters), loss = 7.68098
I0523 00:39:58.908215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68098 (* 1 = 7.68098 loss)
I0523 00:39:59.615725 35003 sgd_solver.cpp:112] Iteration 103170, lr = 0.01
I0523 00:40:03.187788 35003 solver.cpp:239] Iteration 103180 (2.33678 iter/s, 4.2794s/10 iters), loss = 8.33317
I0523 00:40:03.187836 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.33317 (* 1 = 8.33317 loss)
I0523 00:40:03.281193 35003 sgd_solver.cpp:112] Iteration 103180, lr = 0.01
I0523 00:40:06.102793 35003 solver.cpp:239] Iteration 103190 (3.43073 iter/s, 2.91484s/10 iters), loss = 7.67167
I0523 00:40:06.102833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67167 (* 1 = 7.67167 loss)
I0523 00:40:06.115628 35003 sgd_solver.cpp:112] Iteration 103190, lr = 0.01
I0523 00:40:10.280164 35003 solver.cpp:239] Iteration 103200 (2.39397 iter/s, 4.17716s/10 iters), loss = 6.29018
I0523 00:40:10.280217 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29018 (* 1 = 6.29018 loss)
I0523 00:40:10.282769 35003 sgd_solver.cpp:112] Iteration 103200, lr = 0.01
I0523 00:40:14.814792 35003 solver.cpp:239] Iteration 103210 (2.20537 iter/s, 4.53439s/10 iters), loss = 6.44817
I0523 00:40:14.814829 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44817 (* 1 = 6.44817 loss)
I0523 00:40:14.826637 35003 sgd_solver.cpp:112] Iteration 103210, lr = 0.01
I0523 00:40:18.508808 35003 solver.cpp:239] Iteration 103220 (2.70722 iter/s, 3.69382s/10 iters), loss = 6.54934
I0523 00:40:18.508857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54934 (* 1 = 6.54934 loss)
I0523 00:40:19.243986 35003 sgd_solver.cpp:112] Iteration 103220, lr = 0.01
I0523 00:40:23.470752 35003 solver.cpp:239] Iteration 103230 (2.01546 iter/s, 4.96165s/10 iters), loss = 6.39798
I0523 00:40:23.471041 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39798 (* 1 = 6.39798 loss)
I0523 00:40:24.211621 35003 sgd_solver.cpp:112] Iteration 103230, lr = 0.01
I0523 00:40:26.994431 35003 solver.cpp:239] Iteration 103240 (2.83828 iter/s, 3.52327s/10 iters), loss = 7.94631
I0523 00:40:26.994488 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94631 (* 1 = 7.94631 loss)
I0523 00:40:27.011276 35003 sgd_solver.cpp:112] Iteration 103240, lr = 0.01
I0523 00:40:31.296531 35003 solver.cpp:239] Iteration 103250 (2.32457 iter/s, 4.30187s/10 iters), loss = 7.51309
I0523 00:40:31.296583 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51309 (* 1 = 7.51309 loss)
I0523 00:40:32.037602 35003 sgd_solver.cpp:112] Iteration 103250, lr = 0.01
I0523 00:40:37.266856 35003 solver.cpp:239] Iteration 103260 (1.67503 iter/s, 5.97003s/10 iters), loss = 6.63765
I0523 00:40:37.266904 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63765 (* 1 = 6.63765 loss)
I0523 00:40:38.008049 35003 sgd_solver.cpp:112] Iteration 103260, lr = 0.01
I0523 00:40:42.940898 35003 solver.cpp:239] Iteration 103270 (1.7625 iter/s, 5.67377s/10 iters), loss = 7.47834
I0523 00:40:42.940938 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47834 (* 1 = 7.47834 loss)
I0523 00:40:42.948400 35003 sgd_solver.cpp:112] Iteration 103270, lr = 0.01
I0523 00:40:44.504959 35003 solver.cpp:239] Iteration 103280 (6.39407 iter/s, 1.56395s/10 iters), loss = 7.25988
I0523 00:40:44.505020 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25988 (* 1 = 7.25988 loss)
I0523 00:40:44.510367 35003 sgd_solver.cpp:112] Iteration 103280, lr = 0.01
I0523 00:40:48.033231 35003 solver.cpp:239] Iteration 103290 (2.83441 iter/s, 3.52807s/10 iters), loss = 6.66568
I0523 00:40:48.033267 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66568 (* 1 = 6.66568 loss)
I0523 00:40:48.045815 35003 sgd_solver.cpp:112] Iteration 103290, lr = 0.01
I0523 00:40:49.967638 35003 solver.cpp:239] Iteration 103300 (5.16989 iter/s, 1.93428s/10 iters), loss = 6.65229
I0523 00:40:49.967686 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65229 (* 1 = 6.65229 loss)
I0523 00:40:49.975893 35003 sgd_solver.cpp:112] Iteration 103300, lr = 0.01
I0523 00:40:53.390538 35003 solver.cpp:239] Iteration 103310 (2.92166 iter/s, 3.42271s/10 iters), loss = 7.19355
I0523 00:40:53.390591 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19355 (* 1 = 7.19355 loss)
I0523 00:40:53.466850 35003 sgd_solver.cpp:112] Iteration 103310, lr = 0.01
I0523 00:40:59.410394 35003 solver.cpp:239] Iteration 103320 (1.66125 iter/s, 6.01956s/10 iters), loss = 8.59648
I0523 00:40:59.410583 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.59648 (* 1 = 8.59648 loss)
I0523 00:40:59.435474 35003 sgd_solver.cpp:112] Iteration 103320, lr = 0.01
I0523 00:41:02.983201 35003 solver.cpp:239] Iteration 103330 (2.79918 iter/s, 3.57247s/10 iters), loss = 6.62944
I0523 00:41:02.983259 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62944 (* 1 = 6.62944 loss)
I0523 00:41:02.996170 35003 sgd_solver.cpp:112] Iteration 103330, lr = 0.01
I0523 00:41:07.223346 35003 solver.cpp:239] Iteration 103340 (2.35854 iter/s, 4.23991s/10 iters), loss = 6.4942
I0523 00:41:07.223393 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4942 (* 1 = 6.4942 loss)
I0523 00:41:07.228996 35003 sgd_solver.cpp:112] Iteration 103340, lr = 0.01
I0523 00:41:12.462129 35003 solver.cpp:239] Iteration 103350 (1.90894 iter/s, 5.23852s/10 iters), loss = 6.30064
I0523 00:41:12.462179 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30064 (* 1 = 6.30064 loss)
I0523 00:41:13.164590 35003 sgd_solver.cpp:112] Iteration 103350, lr = 0.01
I0523 00:41:15.889873 35003 solver.cpp:239] Iteration 103360 (2.91753 iter/s, 3.42755s/10 iters), loss = 7.55853
I0523 00:41:15.889907 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55853 (* 1 = 7.55853 loss)
I0523 00:41:15.893400 35003 sgd_solver.cpp:112] Iteration 103360, lr = 0.01
I0523 00:41:19.280666 35003 solver.cpp:239] Iteration 103370 (2.94932 iter/s, 3.39061s/10 iters), loss = 7.07112
I0523 00:41:19.280710 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07112 (* 1 = 7.07112 loss)
I0523 00:41:19.293721 35003 sgd_solver.cpp:112] Iteration 103370, lr = 0.01
I0523 00:41:22.308301 35003 solver.cpp:239] Iteration 103380 (3.30309 iter/s, 3.02746s/10 iters), loss = 7.55517
I0523 00:41:22.308341 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55517 (* 1 = 7.55517 loss)
I0523 00:41:22.321265 35003 sgd_solver.cpp:112] Iteration 103380, lr = 0.01
I0523 00:41:25.895599 35003 solver.cpp:239] Iteration 103390 (2.78776 iter/s, 3.58711s/10 iters), loss = 5.29695
I0523 00:41:25.895638 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.29695 (* 1 = 5.29695 loss)
I0523 00:41:25.902191 35003 sgd_solver.cpp:112] Iteration 103390, lr = 0.01
I0523 00:41:28.816323 35003 solver.cpp:239] Iteration 103400 (3.42401 iter/s, 2.92056s/10 iters), loss = 7.89054
I0523 00:41:28.816373 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89054 (* 1 = 7.89054 loss)
I0523 00:41:29.531231 35003 sgd_solver.cpp:112] Iteration 103400, lr = 0.01
I0523 00:41:34.050267 35003 solver.cpp:239] Iteration 103410 (1.91071 iter/s, 5.23366s/10 iters), loss = 7.52857
I0523 00:41:34.050326 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52857 (* 1 = 7.52857 loss)
I0523 00:41:34.057301 35003 sgd_solver.cpp:112] Iteration 103410, lr = 0.01
I0523 00:41:36.732322 35003 solver.cpp:239] Iteration 103420 (3.72874 iter/s, 2.68187s/10 iters), loss = 6.89718
I0523 00:41:36.732364 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89718 (* 1 = 6.89718 loss)
I0523 00:41:36.748504 35003 sgd_solver.cpp:112] Iteration 103420, lr = 0.01
I0523 00:41:40.093717 35003 solver.cpp:239] Iteration 103430 (2.97513 iter/s, 3.3612s/10 iters), loss = 6.89772
I0523 00:41:40.093777 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89772 (* 1 = 6.89772 loss)
I0523 00:41:40.223129 35003 sgd_solver.cpp:112] Iteration 103430, lr = 0.01
I0523 00:41:42.799232 35003 solver.cpp:239] Iteration 103440 (3.69639 iter/s, 2.70534s/10 iters), loss = 5.99651
I0523 00:41:42.799270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99651 (* 1 = 5.99651 loss)
I0523 00:41:42.825350 35003 sgd_solver.cpp:112] Iteration 103440, lr = 0.01
I0523 00:41:46.328407 35003 solver.cpp:239] Iteration 103450 (2.83367 iter/s, 3.52899s/10 iters), loss = 7.33468
I0523 00:41:46.328454 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33468 (* 1 = 7.33468 loss)
I0523 00:41:46.523118 35003 sgd_solver.cpp:112] Iteration 103450, lr = 0.01
I0523 00:41:51.416530 35003 solver.cpp:239] Iteration 103460 (1.96546 iter/s, 5.08787s/10 iters), loss = 6.46249
I0523 00:41:51.416574 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46249 (* 1 = 6.46249 loss)
I0523 00:41:51.422343 35003 sgd_solver.cpp:112] Iteration 103460, lr = 0.01
I0523 00:41:55.187867 35003 solver.cpp:239] Iteration 103470 (2.65173 iter/s, 3.77113s/10 iters), loss = 8.44005
I0523 00:41:55.187919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.44005 (* 1 = 8.44005 loss)
I0523 00:41:55.903317 35003 sgd_solver.cpp:112] Iteration 103470, lr = 0.01
I0523 00:41:59.468443 35003 solver.cpp:239] Iteration 103480 (2.33626 iter/s, 4.28034s/10 iters), loss = 6.89186
I0523 00:41:59.468499 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89186 (* 1 = 6.89186 loss)
I0523 00:41:59.481051 35003 sgd_solver.cpp:112] Iteration 103480, lr = 0.01
I0523 00:42:00.825285 35003 solver.cpp:239] Iteration 103490 (7.37069 iter/s, 1.35673s/10 iters), loss = 7.00817
I0523 00:42:00.825528 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00817 (* 1 = 7.00817 loss)
I0523 00:42:01.565866 35003 sgd_solver.cpp:112] Iteration 103490, lr = 0.01
I0523 00:42:05.177273 35003 solver.cpp:239] Iteration 103500 (2.29801 iter/s, 4.35159s/10 iters), loss = 6.43734
I0523 00:42:05.177316 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43734 (* 1 = 6.43734 loss)
I0523 00:42:05.858875 35003 sgd_solver.cpp:112] Iteration 103500, lr = 0.01
I0523 00:42:09.329905 35003 solver.cpp:239] Iteration 103510 (2.40824 iter/s, 4.15241s/10 iters), loss = 7.06848
I0523 00:42:09.329975 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06848 (* 1 = 7.06848 loss)
I0523 00:42:09.342787 35003 sgd_solver.cpp:112] Iteration 103510, lr = 0.01
I0523 00:42:12.204782 35003 solver.cpp:239] Iteration 103520 (3.47863 iter/s, 2.87469s/10 iters), loss = 7.17683
I0523 00:42:12.204823 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17683 (* 1 = 7.17683 loss)
I0523 00:42:12.211058 35003 sgd_solver.cpp:112] Iteration 103520, lr = 0.01
I0523 00:42:15.044764 35003 solver.cpp:239] Iteration 103530 (3.52136 iter/s, 2.83981s/10 iters), loss = 6.83506
I0523 00:42:15.044816 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83506 (* 1 = 6.83506 loss)
I0523 00:42:15.051784 35003 sgd_solver.cpp:112] Iteration 103530, lr = 0.01
I0523 00:42:17.906900 35003 solver.cpp:239] Iteration 103540 (3.4941 iter/s, 2.86196s/10 iters), loss = 6.49025
I0523 00:42:17.906949 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49025 (* 1 = 6.49025 loss)
I0523 00:42:17.918628 35003 sgd_solver.cpp:112] Iteration 103540, lr = 0.01
I0523 00:42:22.254156 35003 solver.cpp:239] Iteration 103550 (2.30042 iter/s, 4.34703s/10 iters), loss = 8.90051
I0523 00:42:22.254195 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.90051 (* 1 = 8.90051 loss)
I0523 00:42:22.280411 35003 sgd_solver.cpp:112] Iteration 103550, lr = 0.01
I0523 00:42:26.060732 35003 solver.cpp:239] Iteration 103560 (2.62718 iter/s, 3.80637s/10 iters), loss = 6.34804
I0523 00:42:26.060796 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34804 (* 1 = 6.34804 loss)
I0523 00:42:26.662569 35003 sgd_solver.cpp:112] Iteration 103560, lr = 0.01
I0523 00:42:30.289949 35003 solver.cpp:239] Iteration 103570 (2.36464 iter/s, 4.22898s/10 iters), loss = 7.51095
I0523 00:42:30.289996 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51095 (* 1 = 7.51095 loss)
I0523 00:42:30.565407 35003 sgd_solver.cpp:112] Iteration 103570, lr = 0.01
I0523 00:42:34.668608 35003 solver.cpp:239] Iteration 103580 (2.28392 iter/s, 4.37843s/10 iters), loss = 8.10285
I0523 00:42:34.668895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.10285 (* 1 = 8.10285 loss)
I0523 00:42:34.681797 35003 sgd_solver.cpp:112] Iteration 103580, lr = 0.01
I0523 00:42:38.608237 35003 solver.cpp:239] Iteration 103590 (2.54139 iter/s, 3.93486s/10 iters), loss = 7.02244
I0523 00:42:38.608290 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02244 (* 1 = 7.02244 loss)
I0523 00:42:38.690613 35003 sgd_solver.cpp:112] Iteration 103590, lr = 0.01
I0523 00:42:43.711045 35003 solver.cpp:239] Iteration 103600 (1.9598 iter/s, 5.10255s/10 iters), loss = 6.03632
I0523 00:42:43.711088 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03632 (* 1 = 6.03632 loss)
I0523 00:42:44.449492 35003 sgd_solver.cpp:112] Iteration 103600, lr = 0.01
I0523 00:42:48.727686 35003 solver.cpp:239] Iteration 103610 (1.99347 iter/s, 5.01639s/10 iters), loss = 7.40446
I0523 00:42:48.727740 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40446 (* 1 = 7.40446 loss)
I0523 00:42:49.468735 35003 sgd_solver.cpp:112] Iteration 103610, lr = 0.01
I0523 00:42:52.300240 35003 solver.cpp:239] Iteration 103620 (2.79928 iter/s, 3.57234s/10 iters), loss = 7.03802
I0523 00:42:52.300287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03802 (* 1 = 7.03802 loss)
I0523 00:42:52.306702 35003 sgd_solver.cpp:112] Iteration 103620, lr = 0.01
I0523 00:42:54.928050 35003 solver.cpp:239] Iteration 103630 (3.80569 iter/s, 2.62764s/10 iters), loss = 7.66024
I0523 00:42:54.928107 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66024 (* 1 = 7.66024 loss)
I0523 00:42:55.668889 35003 sgd_solver.cpp:112] Iteration 103630, lr = 0.01
I0523 00:42:58.598779 35003 solver.cpp:239] Iteration 103640 (2.72441 iter/s, 3.67052s/10 iters), loss = 7.65137
I0523 00:42:58.598825 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65137 (* 1 = 7.65137 loss)
I0523 00:42:58.611853 35003 sgd_solver.cpp:112] Iteration 103640, lr = 0.01
I0523 00:43:02.921669 35003 solver.cpp:239] Iteration 103650 (2.31339 iter/s, 4.32267s/10 iters), loss = 7.12643
I0523 00:43:02.921715 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12643 (* 1 = 7.12643 loss)
I0523 00:43:02.935170 35003 sgd_solver.cpp:112] Iteration 103650, lr = 0.01
I0523 00:43:05.718082 35003 solver.cpp:239] Iteration 103660 (3.57622 iter/s, 2.79625s/10 iters), loss = 7.24686
I0523 00:43:05.718314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24686 (* 1 = 7.24686 loss)
I0523 00:43:06.414873 35003 sgd_solver.cpp:112] Iteration 103660, lr = 0.01
I0523 00:43:11.152047 35003 solver.cpp:239] Iteration 103670 (1.84042 iter/s, 5.43354s/10 iters), loss = 7.46763
I0523 00:43:11.152091 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46763 (* 1 = 7.46763 loss)
I0523 00:43:11.161087 35003 sgd_solver.cpp:112] Iteration 103670, lr = 0.01
I0523 00:43:14.616401 35003 solver.cpp:239] Iteration 103680 (2.88894 iter/s, 3.46147s/10 iters), loss = 8.51086
I0523 00:43:14.616457 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.51086 (* 1 = 8.51086 loss)
I0523 00:43:14.621919 35003 sgd_solver.cpp:112] Iteration 103680, lr = 0.01
I0523 00:43:17.346395 35003 solver.cpp:239] Iteration 103690 (3.66324 iter/s, 2.72982s/10 iters), loss = 6.53805
I0523 00:43:17.346442 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53805 (* 1 = 6.53805 loss)
I0523 00:43:17.360375 35003 sgd_solver.cpp:112] Iteration 103690, lr = 0.01
I0523 00:43:19.891522 35003 solver.cpp:239] Iteration 103700 (3.92932 iter/s, 2.54497s/10 iters), loss = 7.84519
I0523 00:43:19.891571 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84519 (* 1 = 7.84519 loss)
I0523 00:43:19.904489 35003 sgd_solver.cpp:112] Iteration 103700, lr = 0.01
I0523 00:43:24.680264 35003 solver.cpp:239] Iteration 103710 (2.08834 iter/s, 4.78849s/10 iters), loss = 7.1349
I0523 00:43:24.680310 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1349 (* 1 = 7.1349 loss)
I0523 00:43:24.688906 35003 sgd_solver.cpp:112] Iteration 103710, lr = 0.01
I0523 00:43:28.314182 35003 solver.cpp:239] Iteration 103720 (2.752 iter/s, 3.63372s/10 iters), loss = 6.3684
I0523 00:43:28.314234 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3684 (* 1 = 6.3684 loss)
I0523 00:43:28.333236 35003 sgd_solver.cpp:112] Iteration 103720, lr = 0.01
I0523 00:43:31.124151 35003 solver.cpp:239] Iteration 103730 (3.55898 iter/s, 2.8098s/10 iters), loss = 6.61061
I0523 00:43:31.124193 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61061 (* 1 = 6.61061 loss)
I0523 00:43:31.128592 35003 sgd_solver.cpp:112] Iteration 103730, lr = 0.01
I0523 00:43:35.647336 35003 solver.cpp:239] Iteration 103740 (2.21095 iter/s, 4.52295s/10 iters), loss = 6.58818
I0523 00:43:35.647387 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58818 (* 1 = 6.58818 loss)
I0523 00:43:35.652395 35003 sgd_solver.cpp:112] Iteration 103740, lr = 0.01
I0523 00:43:38.490123 35003 solver.cpp:239] Iteration 103750 (3.51788 iter/s, 2.84262s/10 iters), loss = 7.505
I0523 00:43:38.490386 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.505 (* 1 = 7.505 loss)
I0523 00:43:38.503358 35003 sgd_solver.cpp:112] Iteration 103750, lr = 0.01
I0523 00:43:41.655818 35003 solver.cpp:239] Iteration 103760 (3.15924 iter/s, 3.16532s/10 iters), loss = 7.19838
I0523 00:43:41.655869 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19838 (* 1 = 7.19838 loss)
I0523 00:43:42.273288 35003 sgd_solver.cpp:112] Iteration 103760, lr = 0.01
I0523 00:43:44.795693 35003 solver.cpp:239] Iteration 103770 (3.18503 iter/s, 3.13969s/10 iters), loss = 6.26106
I0523 00:43:44.795756 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26106 (* 1 = 6.26106 loss)
I0523 00:43:45.503654 35003 sgd_solver.cpp:112] Iteration 103770, lr = 0.01
I0523 00:43:48.659384 35003 solver.cpp:239] Iteration 103780 (2.58834 iter/s, 3.86347s/10 iters), loss = 7.04404
I0523 00:43:48.659435 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04404 (* 1 = 7.04404 loss)
I0523 00:43:48.671968 35003 sgd_solver.cpp:112] Iteration 103780, lr = 0.01
I0523 00:43:51.501039 35003 solver.cpp:239] Iteration 103790 (3.51929 iter/s, 2.84148s/10 iters), loss = 6.00902
I0523 00:43:51.501088 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00902 (* 1 = 6.00902 loss)
I0523 00:43:52.215382 35003 sgd_solver.cpp:112] Iteration 103790, lr = 0.01
I0523 00:43:55.795305 35003 solver.cpp:239] Iteration 103800 (2.32882 iter/s, 4.29402s/10 iters), loss = 7.29743
I0523 00:43:55.795362 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29743 (* 1 = 7.29743 loss)
I0523 00:43:56.509727 35003 sgd_solver.cpp:112] Iteration 103800, lr = 0.01
I0523 00:44:01.582242 35003 solver.cpp:239] Iteration 103810 (1.72812 iter/s, 5.78665s/10 iters), loss = 6.17614
I0523 00:44:01.582295 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17614 (* 1 = 6.17614 loss)
I0523 00:44:01.589836 35003 sgd_solver.cpp:112] Iteration 103810, lr = 0.01
I0523 00:44:05.191515 35003 solver.cpp:239] Iteration 103820 (2.77081 iter/s, 3.60906s/10 iters), loss = 8.43757
I0523 00:44:05.191563 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.43757 (* 1 = 8.43757 loss)
I0523 00:44:05.234699 35003 sgd_solver.cpp:112] Iteration 103820, lr = 0.01
I0523 00:44:09.533187 35003 solver.cpp:239] Iteration 103830 (2.30338 iter/s, 4.34145s/10 iters), loss = 6.86914
I0523 00:44:09.533413 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86914 (* 1 = 6.86914 loss)
I0523 00:44:10.261745 35003 sgd_solver.cpp:112] Iteration 103830, lr = 0.01
I0523 00:44:12.385160 35003 solver.cpp:239] Iteration 103840 (3.50674 iter/s, 2.85165s/10 iters), loss = 6.55367
I0523 00:44:12.385202 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55367 (* 1 = 6.55367 loss)
I0523 00:44:12.700215 35003 sgd_solver.cpp:112] Iteration 103840, lr = 0.01
I0523 00:44:17.290969 35003 solver.cpp:239] Iteration 103850 (2.0385 iter/s, 4.90556s/10 iters), loss = 5.42066
I0523 00:44:17.291014 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.42066 (* 1 = 5.42066 loss)
I0523 00:44:17.945489 35003 sgd_solver.cpp:112] Iteration 103850, lr = 0.01
I0523 00:44:21.681668 35003 solver.cpp:239] Iteration 103860 (2.27766 iter/s, 4.39047s/10 iters), loss = 7.63597
I0523 00:44:21.681730 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63597 (* 1 = 7.63597 loss)
I0523 00:44:22.402532 35003 sgd_solver.cpp:112] Iteration 103860, lr = 0.01
I0523 00:44:25.955418 35003 solver.cpp:239] Iteration 103870 (2.33999 iter/s, 4.27351s/10 iters), loss = 6.97132
I0523 00:44:25.955461 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97132 (* 1 = 6.97132 loss)
I0523 00:44:25.967978 35003 sgd_solver.cpp:112] Iteration 103870, lr = 0.01
I0523 00:44:29.544756 35003 solver.cpp:239] Iteration 103880 (2.78619 iter/s, 3.58913s/10 iters), loss = 7.43745
I0523 00:44:29.544808 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43745 (* 1 = 7.43745 loss)
I0523 00:44:29.548224 35003 sgd_solver.cpp:112] Iteration 103880, lr = 0.01
I0523 00:44:34.677646 35003 solver.cpp:239] Iteration 103890 (1.94832 iter/s, 5.13262s/10 iters), loss = 6.34146
I0523 00:44:34.677690 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34146 (* 1 = 6.34146 loss)
I0523 00:44:35.246819 35003 sgd_solver.cpp:112] Iteration 103890, lr = 0.01
I0523 00:44:37.901053 35003 solver.cpp:239] Iteration 103900 (3.10249 iter/s, 3.22322s/10 iters), loss = 6.73818
I0523 00:44:37.901094 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73818 (* 1 = 6.73818 loss)
I0523 00:44:37.918753 35003 sgd_solver.cpp:112] Iteration 103900, lr = 0.01
I0523 00:44:41.280565 35003 solver.cpp:239] Iteration 103910 (2.95917 iter/s, 3.37932s/10 iters), loss = 6.35091
I0523 00:44:41.280817 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35091 (* 1 = 6.35091 loss)
I0523 00:44:41.293805 35003 sgd_solver.cpp:112] Iteration 103910, lr = 0.01
I0523 00:44:43.463074 35003 solver.cpp:239] Iteration 103920 (4.58256 iter/s, 2.18219s/10 iters), loss = 7.5212
I0523 00:44:43.463130 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5212 (* 1 = 7.5212 loss)
I0523 00:44:43.476379 35003 sgd_solver.cpp:112] Iteration 103920, lr = 0.01
I0523 00:44:47.049981 35003 solver.cpp:239] Iteration 103930 (2.78807 iter/s, 3.5867s/10 iters), loss = 6.48884
I0523 00:44:47.050024 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48884 (* 1 = 6.48884 loss)
I0523 00:44:47.061446 35003 sgd_solver.cpp:112] Iteration 103930, lr = 0.01
I0523 00:44:50.775315 35003 solver.cpp:239] Iteration 103940 (2.68448 iter/s, 3.72512s/10 iters), loss = 7.33955
I0523 00:44:50.775378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33955 (* 1 = 7.33955 loss)
I0523 00:44:51.511706 35003 sgd_solver.cpp:112] Iteration 103940, lr = 0.01
I0523 00:44:54.948237 35003 solver.cpp:239] Iteration 103950 (2.39653 iter/s, 4.17269s/10 iters), loss = 7.42667
I0523 00:44:54.948281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42667 (* 1 = 7.42667 loss)
I0523 00:44:54.961561 35003 sgd_solver.cpp:112] Iteration 103950, lr = 0.01
I0523 00:44:59.257661 35003 solver.cpp:239] Iteration 103960 (2.32061 iter/s, 4.3092s/10 iters), loss = 7.32908
I0523 00:44:59.257699 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32908 (* 1 = 7.32908 loss)
I0523 00:44:59.276258 35003 sgd_solver.cpp:112] Iteration 103960, lr = 0.01
I0523 00:45:02.884086 35003 solver.cpp:239] Iteration 103970 (2.75769 iter/s, 3.62623s/10 iters), loss = 7.03915
I0523 00:45:02.884130 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03915 (* 1 = 7.03915 loss)
I0523 00:45:02.896487 35003 sgd_solver.cpp:112] Iteration 103970, lr = 0.01
I0523 00:45:05.571137 35003 solver.cpp:239] Iteration 103980 (3.72182 iter/s, 2.68686s/10 iters), loss = 7.86731
I0523 00:45:05.571177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86731 (* 1 = 7.86731 loss)
I0523 00:45:05.578822 35003 sgd_solver.cpp:112] Iteration 103980, lr = 0.01
I0523 00:45:09.275477 35003 solver.cpp:239] Iteration 103990 (2.69969 iter/s, 3.70413s/10 iters), loss = 7.67654
I0523 00:45:09.275542 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67654 (* 1 = 7.67654 loss)
I0523 00:45:09.283136 35003 sgd_solver.cpp:112] Iteration 103990, lr = 0.01
I0523 00:45:12.715534 35003 solver.cpp:239] Iteration 104000 (2.9071 iter/s, 3.43986s/10 iters), loss = 7.24565
I0523 00:45:12.715762 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24565 (* 1 = 7.24565 loss)
I0523 00:45:12.734067 35003 sgd_solver.cpp:112] Iteration 104000, lr = 0.01
I0523 00:45:15.481390 35003 solver.cpp:239] Iteration 104010 (3.61594 iter/s, 2.76554s/10 iters), loss = 8.36491
I0523 00:45:15.481432 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.36491 (* 1 = 8.36491 loss)
I0523 00:45:15.486827 35003 sgd_solver.cpp:112] Iteration 104010, lr = 0.01
I0523 00:45:18.990828 35003 solver.cpp:239] Iteration 104020 (2.84962 iter/s, 3.50925s/10 iters), loss = 7.61872
I0523 00:45:18.990888 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61872 (* 1 = 7.61872 loss)
I0523 00:45:19.691934 35003 sgd_solver.cpp:112] Iteration 104020, lr = 0.01
I0523 00:45:23.361493 35003 solver.cpp:239] Iteration 104030 (2.28811 iter/s, 4.37043s/10 iters), loss = 7.30272
I0523 00:45:23.361541 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30272 (* 1 = 7.30272 loss)
I0523 00:45:23.369276 35003 sgd_solver.cpp:112] Iteration 104030, lr = 0.01
I0523 00:45:26.008518 35003 solver.cpp:239] Iteration 104040 (3.77806 iter/s, 2.64686s/10 iters), loss = 6.38707
I0523 00:45:26.008570 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38707 (* 1 = 6.38707 loss)
I0523 00:45:26.174151 35003 sgd_solver.cpp:112] Iteration 104040, lr = 0.01
I0523 00:45:30.983745 35003 solver.cpp:239] Iteration 104050 (2.01006 iter/s, 4.97498s/10 iters), loss = 6.19792
I0523 00:45:30.983790 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19792 (* 1 = 6.19792 loss)
I0523 00:45:31.678584 35003 sgd_solver.cpp:112] Iteration 104050, lr = 0.01
I0523 00:45:36.146540 35003 solver.cpp:239] Iteration 104060 (1.93704 iter/s, 5.16252s/10 iters), loss = 6.22516
I0523 00:45:36.146580 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22516 (* 1 = 6.22516 loss)
I0523 00:45:36.160076 35003 sgd_solver.cpp:112] Iteration 104060, lr = 0.01
I0523 00:45:38.776917 35003 solver.cpp:239] Iteration 104070 (3.80196 iter/s, 2.63022s/10 iters), loss = 7.73862
I0523 00:45:38.776965 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73862 (* 1 = 7.73862 loss)
I0523 00:45:38.790447 35003 sgd_solver.cpp:112] Iteration 104070, lr = 0.01
I0523 00:45:43.955871 35003 solver.cpp:239] Iteration 104080 (1.93099 iter/s, 5.1787s/10 iters), loss = 5.95069
I0523 00:45:43.956143 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95069 (* 1 = 5.95069 loss)
I0523 00:45:44.683800 35003 sgd_solver.cpp:112] Iteration 104080, lr = 0.01
I0523 00:45:46.492302 35003 solver.cpp:239] Iteration 104090 (3.94309 iter/s, 2.53608s/10 iters), loss = 7.63641
I0523 00:45:46.492353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63641 (* 1 = 7.63641 loss)
I0523 00:45:47.232946 35003 sgd_solver.cpp:112] Iteration 104090, lr = 0.01
I0523 00:45:49.299649 35003 solver.cpp:239] Iteration 104100 (3.5623 iter/s, 2.80718s/10 iters), loss = 8.16666
I0523 00:45:49.299686 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16666 (* 1 = 8.16666 loss)
I0523 00:45:49.313020 35003 sgd_solver.cpp:112] Iteration 104100, lr = 0.01
I0523 00:45:53.507827 35003 solver.cpp:239] Iteration 104110 (2.37645 iter/s, 4.20796s/10 iters), loss = 6.75542
I0523 00:45:53.507876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75542 (* 1 = 6.75542 loss)
I0523 00:45:54.163723 35003 sgd_solver.cpp:112] Iteration 104110, lr = 0.01
I0523 00:45:57.262853 35003 solver.cpp:239] Iteration 104120 (2.66324 iter/s, 3.75482s/10 iters), loss = 7.24554
I0523 00:45:57.262897 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24554 (* 1 = 7.24554 loss)
I0523 00:45:57.978078 35003 sgd_solver.cpp:112] Iteration 104120, lr = 0.01
I0523 00:46:02.190517 35003 solver.cpp:239] Iteration 104130 (2.02946 iter/s, 4.92742s/10 iters), loss = 7.12371
I0523 00:46:02.190573 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12371 (* 1 = 7.12371 loss)
I0523 00:46:02.706737 35003 sgd_solver.cpp:112] Iteration 104130, lr = 0.01
I0523 00:46:05.185809 35003 solver.cpp:239] Iteration 104140 (3.33878 iter/s, 2.99511s/10 iters), loss = 7.38206
I0523 00:46:05.185868 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38206 (* 1 = 7.38206 loss)
I0523 00:46:05.738461 35003 sgd_solver.cpp:112] Iteration 104140, lr = 0.01
I0523 00:46:09.290973 35003 solver.cpp:239] Iteration 104150 (2.43609 iter/s, 4.10494s/10 iters), loss = 6.86061
I0523 00:46:09.291021 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86061 (* 1 = 6.86061 loss)
I0523 00:46:09.403137 35003 sgd_solver.cpp:112] Iteration 104150, lr = 0.01
I0523 00:46:13.727915 35003 solver.cpp:239] Iteration 104160 (2.25392 iter/s, 4.43671s/10 iters), loss = 7.54945
I0523 00:46:13.727957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54945 (* 1 = 7.54945 loss)
I0523 00:46:13.741035 35003 sgd_solver.cpp:112] Iteration 104160, lr = 0.01
I0523 00:46:16.540911 35003 solver.cpp:239] Iteration 104170 (3.55514 iter/s, 2.81283s/10 iters), loss = 7.979
I0523 00:46:16.541206 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.979 (* 1 = 7.979 loss)
I0523 00:46:16.565521 35003 sgd_solver.cpp:112] Iteration 104170, lr = 0.01
I0523 00:46:19.358925 35003 solver.cpp:239] Iteration 104180 (3.55127 iter/s, 2.81589s/10 iters), loss = 6.57692
I0523 00:46:19.358988 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57692 (* 1 = 6.57692 loss)
I0523 00:46:19.424190 35003 sgd_solver.cpp:112] Iteration 104180, lr = 0.01
I0523 00:46:24.398828 35003 solver.cpp:239] Iteration 104190 (1.98427 iter/s, 5.03964s/10 iters), loss = 6.5244
I0523 00:46:24.398867 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5244 (* 1 = 6.5244 loss)
I0523 00:46:24.417678 35003 sgd_solver.cpp:112] Iteration 104190, lr = 0.01
I0523 00:46:27.291575 35003 solver.cpp:239] Iteration 104200 (3.46244 iter/s, 2.88814s/10 iters), loss = 7.9589
I0523 00:46:27.291617 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9589 (* 1 = 7.9589 loss)
I0523 00:46:27.296947 35003 sgd_solver.cpp:112] Iteration 104200, lr = 0.01
I0523 00:46:31.721240 35003 solver.cpp:239] Iteration 104210 (2.25763 iter/s, 4.42943s/10 iters), loss = 7.29434
I0523 00:46:31.721287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29434 (* 1 = 7.29434 loss)
I0523 00:46:31.772650 35003 sgd_solver.cpp:112] Iteration 104210, lr = 0.01
I0523 00:46:33.818218 35003 solver.cpp:239] Iteration 104220 (4.76908 iter/s, 2.09684s/10 iters), loss = 5.60649
I0523 00:46:33.818258 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.60649 (* 1 = 5.60649 loss)
I0523 00:46:33.823058 35003 sgd_solver.cpp:112] Iteration 104220, lr = 0.01
I0523 00:46:36.120265 35003 solver.cpp:239] Iteration 104230 (4.34423 iter/s, 2.3019s/10 iters), loss = 7.03752
I0523 00:46:36.120316 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03752 (* 1 = 7.03752 loss)
I0523 00:46:36.133756 35003 sgd_solver.cpp:112] Iteration 104230, lr = 0.01
I0523 00:46:38.488543 35003 solver.cpp:239] Iteration 104240 (4.22274 iter/s, 2.36813s/10 iters), loss = 6.76978
I0523 00:46:38.488587 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76978 (* 1 = 6.76978 loss)
I0523 00:46:38.498639 35003 sgd_solver.cpp:112] Iteration 104240, lr = 0.01
I0523 00:46:42.074306 35003 solver.cpp:239] Iteration 104250 (2.78896 iter/s, 3.58557s/10 iters), loss = 8.27886
I0523 00:46:42.074343 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.27886 (* 1 = 8.27886 loss)
I0523 00:46:42.087074 35003 sgd_solver.cpp:112] Iteration 104250, lr = 0.01
I0523 00:46:47.896098 35003 solver.cpp:239] Iteration 104260 (1.71777 iter/s, 5.82151s/10 iters), loss = 7.45418
I0523 00:46:47.896369 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45418 (* 1 = 7.45418 loss)
I0523 00:46:48.610869 35003 sgd_solver.cpp:112] Iteration 104260, lr = 0.01
I0523 00:46:51.287115 35003 solver.cpp:239] Iteration 104270 (2.9493 iter/s, 3.39063s/10 iters), loss = 6.55482
I0523 00:46:51.287153 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55482 (* 1 = 6.55482 loss)
I0523 00:46:51.291772 35003 sgd_solver.cpp:112] Iteration 104270, lr = 0.01
I0523 00:46:54.153254 35003 solver.cpp:239] Iteration 104280 (3.48921 iter/s, 2.86598s/10 iters), loss = 7.7632
I0523 00:46:54.153311 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7632 (* 1 = 7.7632 loss)
I0523 00:46:54.887511 35003 sgd_solver.cpp:112] Iteration 104280, lr = 0.01
I0523 00:46:57.565935 35003 solver.cpp:239] Iteration 104290 (2.93042 iter/s, 3.41249s/10 iters), loss = 7.27619
I0523 00:46:57.565973 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27619 (* 1 = 7.27619 loss)
I0523 00:46:57.608924 35003 sgd_solver.cpp:112] Iteration 104290, lr = 0.01
I0523 00:46:59.791131 35003 solver.cpp:239] Iteration 104300 (4.49426 iter/s, 2.22506s/10 iters), loss = 6.79209
I0523 00:46:59.791175 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79209 (* 1 = 6.79209 loss)
I0523 00:46:59.800215 35003 sgd_solver.cpp:112] Iteration 104300, lr = 0.01
I0523 00:47:02.780830 35003 solver.cpp:239] Iteration 104310 (3.34501 iter/s, 2.98953s/10 iters), loss = 7.40836
I0523 00:47:02.780877 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40836 (* 1 = 7.40836 loss)
I0523 00:47:02.787138 35003 sgd_solver.cpp:112] Iteration 104310, lr = 0.01
I0523 00:47:06.424060 35003 solver.cpp:239] Iteration 104320 (2.74497 iter/s, 3.64303s/10 iters), loss = 7.89679
I0523 00:47:06.424113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89679 (* 1 = 7.89679 loss)
I0523 00:47:06.433388 35003 sgd_solver.cpp:112] Iteration 104320, lr = 0.01
I0523 00:47:08.393570 35003 solver.cpp:239] Iteration 104330 (5.07778 iter/s, 1.96937s/10 iters), loss = 7.29369
I0523 00:47:08.393621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29369 (* 1 = 7.29369 loss)
I0523 00:47:09.087132 35003 sgd_solver.cpp:112] Iteration 104330, lr = 0.01
I0523 00:47:11.778385 35003 solver.cpp:239] Iteration 104340 (2.95454 iter/s, 3.38462s/10 iters), loss = 6.19771
I0523 00:47:11.778429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19771 (* 1 = 6.19771 loss)
I0523 00:47:12.253144 35003 sgd_solver.cpp:112] Iteration 104340, lr = 0.01
I0523 00:47:16.564625 35003 solver.cpp:239] Iteration 104350 (2.08943 iter/s, 4.78599s/10 iters), loss = 7.35844
I0523 00:47:16.564671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35844 (* 1 = 7.35844 loss)
I0523 00:47:16.605885 35003 sgd_solver.cpp:112] Iteration 104350, lr = 0.01
I0523 00:47:21.697190 35003 solver.cpp:239] Iteration 104360 (1.94844 iter/s, 5.13231s/10 iters), loss = 6.97489
I0523 00:47:21.697377 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97489 (* 1 = 6.97489 loss)
I0523 00:47:21.701112 35003 sgd_solver.cpp:112] Iteration 104360, lr = 0.01
I0523 00:47:25.390362 35003 solver.cpp:239] Iteration 104370 (2.70793 iter/s, 3.69285s/10 iters), loss = 6.62835
I0523 00:47:25.390417 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62835 (* 1 = 6.62835 loss)
I0523 00:47:25.400351 35003 sgd_solver.cpp:112] Iteration 104370, lr = 0.01
I0523 00:47:29.128655 35003 solver.cpp:239] Iteration 104380 (2.67517 iter/s, 3.73808s/10 iters), loss = 6.3466
I0523 00:47:29.128710 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3466 (* 1 = 6.3466 loss)
I0523 00:47:29.830052 35003 sgd_solver.cpp:112] Iteration 104380, lr = 0.01
I0523 00:47:32.785346 35003 solver.cpp:239] Iteration 104390 (2.73486 iter/s, 3.65649s/10 iters), loss = 7.36734
I0523 00:47:32.785387 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36734 (* 1 = 7.36734 loss)
I0523 00:47:32.796833 35003 sgd_solver.cpp:112] Iteration 104390, lr = 0.01
I0523 00:47:36.436724 35003 solver.cpp:239] Iteration 104400 (2.73884 iter/s, 3.65118s/10 iters), loss = 7.44143
I0523 00:47:36.436779 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44143 (* 1 = 7.44143 loss)
I0523 00:47:36.441468 35003 sgd_solver.cpp:112] Iteration 104400, lr = 0.01
I0523 00:47:40.082756 35003 solver.cpp:239] Iteration 104410 (2.74286 iter/s, 3.64583s/10 iters), loss = 8.22892
I0523 00:47:40.082796 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.22892 (* 1 = 8.22892 loss)
I0523 00:47:40.086215 35003 sgd_solver.cpp:112] Iteration 104410, lr = 0.01
I0523 00:47:43.654892 35003 solver.cpp:239] Iteration 104420 (2.7996 iter/s, 3.57193s/10 iters), loss = 7.48511
I0523 00:47:43.654952 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48511 (* 1 = 7.48511 loss)
I0523 00:47:43.667723 35003 sgd_solver.cpp:112] Iteration 104420, lr = 0.01
I0523 00:47:46.646757 35003 solver.cpp:239] Iteration 104430 (3.3426 iter/s, 2.99168s/10 iters), loss = 6.40713
I0523 00:47:46.646796 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40713 (* 1 = 6.40713 loss)
I0523 00:47:47.284958 35003 sgd_solver.cpp:112] Iteration 104430, lr = 0.01
I0523 00:47:50.454075 35003 solver.cpp:239] Iteration 104440 (2.62666 iter/s, 3.80712s/10 iters), loss = 6.5118
I0523 00:47:50.454113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5118 (* 1 = 6.5118 loss)
I0523 00:47:50.460934 35003 sgd_solver.cpp:112] Iteration 104440, lr = 0.01
I0523 00:47:52.494122 35003 solver.cpp:239] Iteration 104450 (4.90215 iter/s, 2.03992s/10 iters), loss = 8.12907
I0523 00:47:52.494302 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.12907 (* 1 = 8.12907 loss)
I0523 00:47:52.517002 35003 sgd_solver.cpp:112] Iteration 104450, lr = 0.01
I0523 00:47:54.567735 35003 solver.cpp:239] Iteration 104460 (4.82314 iter/s, 2.07334s/10 iters), loss = 7.46182
I0523 00:47:54.567787 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46182 (* 1 = 7.46182 loss)
I0523 00:47:54.596437 35003 sgd_solver.cpp:112] Iteration 104460, lr = 0.01
I0523 00:47:58.016968 35003 solver.cpp:239] Iteration 104470 (2.89935 iter/s, 3.44904s/10 iters), loss = 7.42178
I0523 00:47:58.017009 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42178 (* 1 = 7.42178 loss)
I0523 00:47:58.022894 35003 sgd_solver.cpp:112] Iteration 104470, lr = 0.01
I0523 00:48:01.497843 35003 solver.cpp:239] Iteration 104480 (2.873 iter/s, 3.48068s/10 iters), loss = 6.81377
I0523 00:48:01.497901 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81377 (* 1 = 6.81377 loss)
I0523 00:48:01.516293 35003 sgd_solver.cpp:112] Iteration 104480, lr = 0.01
I0523 00:48:03.609421 35003 solver.cpp:239] Iteration 104490 (4.73613 iter/s, 2.11143s/10 iters), loss = 7.16723
I0523 00:48:03.609462 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16723 (* 1 = 7.16723 loss)
I0523 00:48:03.617161 35003 sgd_solver.cpp:112] Iteration 104490, lr = 0.01
I0523 00:48:06.495095 35003 solver.cpp:239] Iteration 104500 (3.46559 iter/s, 2.88551s/10 iters), loss = 7.87567
I0523 00:48:06.495127 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87567 (* 1 = 7.87567 loss)
I0523 00:48:07.236577 35003 sgd_solver.cpp:112] Iteration 104500, lr = 0.01
I0523 00:48:10.135184 35003 solver.cpp:239] Iteration 104510 (2.74733 iter/s, 3.63989s/10 iters), loss = 7.56829
I0523 00:48:10.135236 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56829 (* 1 = 7.56829 loss)
I0523 00:48:10.158119 35003 sgd_solver.cpp:112] Iteration 104510, lr = 0.01
I0523 00:48:12.936764 35003 solver.cpp:239] Iteration 104520 (3.56963 iter/s, 2.80141s/10 iters), loss = 6.26951
I0523 00:48:12.936813 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26951 (* 1 = 6.26951 loss)
I0523 00:48:12.955518 35003 sgd_solver.cpp:112] Iteration 104520, lr = 0.01
I0523 00:48:15.854748 35003 solver.cpp:239] Iteration 104530 (3.42722 iter/s, 2.91781s/10 iters), loss = 6.45152
I0523 00:48:15.854784 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45152 (* 1 = 6.45152 loss)
I0523 00:48:15.881301 35003 sgd_solver.cpp:112] Iteration 104530, lr = 0.01
I0523 00:48:20.316473 35003 solver.cpp:239] Iteration 104540 (2.2414 iter/s, 4.4615s/10 iters), loss = 7.7721
I0523 00:48:20.316519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7721 (* 1 = 7.7721 loss)
I0523 00:48:21.031987 35003 sgd_solver.cpp:112] Iteration 104540, lr = 0.01
I0523 00:48:24.003450 35003 solver.cpp:239] Iteration 104550 (2.71242 iter/s, 3.68675s/10 iters), loss = 8.16408
I0523 00:48:24.003711 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16408 (* 1 = 8.16408 loss)
I0523 00:48:24.031236 35003 sgd_solver.cpp:112] Iteration 104550, lr = 0.01
I0523 00:48:27.013780 35003 solver.cpp:239] Iteration 104560 (3.32229 iter/s, 3.00997s/10 iters), loss = 7.19041
I0523 00:48:27.013821 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19041 (* 1 = 7.19041 loss)
I0523 00:48:27.690686 35003 sgd_solver.cpp:112] Iteration 104560, lr = 0.01
I0523 00:48:31.070142 35003 solver.cpp:239] Iteration 104570 (2.46539 iter/s, 4.05615s/10 iters), loss = 7.69956
I0523 00:48:31.070194 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69956 (* 1 = 7.69956 loss)
I0523 00:48:31.077607 35003 sgd_solver.cpp:112] Iteration 104570, lr = 0.01
I0523 00:48:33.809175 35003 solver.cpp:239] Iteration 104580 (3.65115 iter/s, 2.73887s/10 iters), loss = 6.92176
I0523 00:48:33.809221 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92176 (* 1 = 6.92176 loss)
I0523 00:48:33.814222 35003 sgd_solver.cpp:112] Iteration 104580, lr = 0.01
I0523 00:48:38.231117 35003 solver.cpp:239] Iteration 104590 (2.26157 iter/s, 4.4217s/10 iters), loss = 7.54051
I0523 00:48:38.231171 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54051 (* 1 = 7.54051 loss)
I0523 00:48:38.278417 35003 sgd_solver.cpp:112] Iteration 104590, lr = 0.01
I0523 00:48:41.909252 35003 solver.cpp:239] Iteration 104600 (2.71892 iter/s, 3.67793s/10 iters), loss = 6.83015
I0523 00:48:41.909312 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83015 (* 1 = 6.83015 loss)
I0523 00:48:42.650121 35003 sgd_solver.cpp:112] Iteration 104600, lr = 0.01
I0523 00:48:46.116278 35003 solver.cpp:239] Iteration 104610 (2.3771 iter/s, 4.2068s/10 iters), loss = 6.31225
I0523 00:48:46.116322 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31225 (* 1 = 6.31225 loss)
I0523 00:48:46.831075 35003 sgd_solver.cpp:112] Iteration 104610, lr = 0.01
I0523 00:48:49.937137 35003 solver.cpp:239] Iteration 104620 (2.61735 iter/s, 3.82066s/10 iters), loss = 7.88779
I0523 00:48:49.937183 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88779 (* 1 = 7.88779 loss)
I0523 00:48:49.979679 35003 sgd_solver.cpp:112] Iteration 104620, lr = 0.01
I0523 00:48:52.822544 35003 solver.cpp:239] Iteration 104630 (3.46592 iter/s, 2.88524s/10 iters), loss = 6.21103
I0523 00:48:52.822592 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21103 (* 1 = 6.21103 loss)
I0523 00:48:52.836365 35003 sgd_solver.cpp:112] Iteration 104630, lr = 0.01
I0523 00:48:55.767050 35003 solver.cpp:239] Iteration 104640 (3.39635 iter/s, 2.94434s/10 iters), loss = 6.8951
I0523 00:48:55.767280 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8951 (* 1 = 6.8951 loss)
I0523 00:48:55.792677 35003 sgd_solver.cpp:112] Iteration 104640, lr = 0.01
I0523 00:48:59.339913 35003 solver.cpp:239] Iteration 104650 (2.79915 iter/s, 3.57251s/10 iters), loss = 7.27252
I0523 00:48:59.339958 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27252 (* 1 = 7.27252 loss)
I0523 00:48:59.353489 35003 sgd_solver.cpp:112] Iteration 104650, lr = 0.01
I0523 00:49:02.090914 35003 solver.cpp:239] Iteration 104660 (3.63527 iter/s, 2.75082s/10 iters), loss = 6.8996
I0523 00:49:02.090956 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8996 (* 1 = 6.8996 loss)
I0523 00:49:02.103469 35003 sgd_solver.cpp:112] Iteration 104660, lr = 0.01
I0523 00:49:07.151165 35003 solver.cpp:239] Iteration 104670 (1.97629 iter/s, 5.05999s/10 iters), loss = 6.76149
I0523 00:49:07.151221 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76149 (* 1 = 6.76149 loss)
I0523 00:49:07.153787 35003 sgd_solver.cpp:112] Iteration 104670, lr = 0.01
I0523 00:49:11.185369 35003 solver.cpp:239] Iteration 104680 (2.47895 iter/s, 4.03397s/10 iters), loss = 6.37484
I0523 00:49:11.185423 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37484 (* 1 = 6.37484 loss)
I0523 00:49:11.206810 35003 sgd_solver.cpp:112] Iteration 104680, lr = 0.01
I0523 00:49:16.885043 35003 solver.cpp:239] Iteration 104690 (1.75458 iter/s, 5.69936s/10 iters), loss = 6.96383
I0523 00:49:16.885109 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96383 (* 1 = 6.96383 loss)
I0523 00:49:16.892936 35003 sgd_solver.cpp:112] Iteration 104690, lr = 0.01
I0523 00:49:21.294780 35003 solver.cpp:239] Iteration 104700 (2.26783 iter/s, 4.4095s/10 iters), loss = 8.07943
I0523 00:49:21.294817 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.07943 (* 1 = 8.07943 loss)
I0523 00:49:22.016695 35003 sgd_solver.cpp:112] Iteration 104700, lr = 0.01
I0523 00:49:26.344995 35003 solver.cpp:239] Iteration 104710 (1.98021 iter/s, 5.04997s/10 iters), loss = 7.89876
I0523 00:49:26.345242 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89876 (* 1 = 7.89876 loss)
I0523 00:49:26.358150 35003 sgd_solver.cpp:112] Iteration 104710, lr = 0.01
I0523 00:49:27.665272 35003 solver.cpp:239] Iteration 104720 (7.57577 iter/s, 1.32s/10 iters), loss = 7.6612
I0523 00:49:27.665325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6612 (* 1 = 7.6612 loss)
I0523 00:49:28.238858 35003 sgd_solver.cpp:112] Iteration 104720, lr = 0.01
I0523 00:49:31.219471 35003 solver.cpp:239] Iteration 104730 (2.81374 iter/s, 3.55399s/10 iters), loss = 6.89545
I0523 00:49:31.219521 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89545 (* 1 = 6.89545 loss)
I0523 00:49:31.228785 35003 sgd_solver.cpp:112] Iteration 104730, lr = 0.01
I0523 00:49:34.773721 35003 solver.cpp:239] Iteration 104740 (2.81369 iter/s, 3.55405s/10 iters), loss = 8.16164
I0523 00:49:34.773782 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16164 (* 1 = 8.16164 loss)
I0523 00:49:35.481966 35003 sgd_solver.cpp:112] Iteration 104740, lr = 0.01
I0523 00:49:38.837478 35003 solver.cpp:239] Iteration 104750 (2.46091 iter/s, 4.06353s/10 iters), loss = 6.96532
I0523 00:49:38.837522 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96532 (* 1 = 6.96532 loss)
I0523 00:49:38.850986 35003 sgd_solver.cpp:112] Iteration 104750, lr = 0.01
I0523 00:49:41.986454 35003 solver.cpp:239] Iteration 104760 (3.17582 iter/s, 3.1488s/10 iters), loss = 6.48594
I0523 00:49:41.986505 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48594 (* 1 = 6.48594 loss)
I0523 00:49:41.999766 35003 sgd_solver.cpp:112] Iteration 104760, lr = 0.01
I0523 00:49:45.602344 35003 solver.cpp:239] Iteration 104770 (2.76573 iter/s, 3.61569s/10 iters), loss = 7.93038
I0523 00:49:45.602391 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93038 (* 1 = 7.93038 loss)
I0523 00:49:46.343874 35003 sgd_solver.cpp:112] Iteration 104770, lr = 0.01
I0523 00:49:48.851680 35003 solver.cpp:239] Iteration 104780 (3.07773 iter/s, 3.24915s/10 iters), loss = 7.89508
I0523 00:49:48.851724 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89508 (* 1 = 7.89508 loss)
I0523 00:49:48.856206 35003 sgd_solver.cpp:112] Iteration 104780, lr = 0.01
I0523 00:49:52.397078 35003 solver.cpp:239] Iteration 104790 (2.82072 iter/s, 3.5452s/10 iters), loss = 8.12799
I0523 00:49:52.397132 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.12799 (* 1 = 8.12799 loss)
I0523 00:49:52.404844 35003 sgd_solver.cpp:112] Iteration 104790, lr = 0.01
I0523 00:49:55.846923 35003 solver.cpp:239] Iteration 104800 (2.89884 iter/s, 3.44965s/10 iters), loss = 7.58505
I0523 00:49:55.846968 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58505 (* 1 = 7.58505 loss)
I0523 00:49:55.858124 35003 sgd_solver.cpp:112] Iteration 104800, lr = 0.01
I0523 00:49:57.958421 35003 solver.cpp:239] Iteration 104810 (4.73629 iter/s, 2.11136s/10 iters), loss = 7.19155
I0523 00:49:57.958616 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19155 (* 1 = 7.19155 loss)
I0523 00:49:57.968618 35003 sgd_solver.cpp:112] Iteration 104810, lr = 0.01
I0523 00:49:59.291851 35003 solver.cpp:239] Iteration 104820 (7.50097 iter/s, 1.33316s/10 iters), loss = 7.8389
I0523 00:49:59.291909 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8389 (* 1 = 7.8389 loss)
I0523 00:50:00.026196 35003 sgd_solver.cpp:112] Iteration 104820, lr = 0.01
I0523 00:50:02.833694 35003 solver.cpp:239] Iteration 104830 (2.82355 iter/s, 3.54164s/10 iters), loss = 6.89112
I0523 00:50:02.833741 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89112 (* 1 = 6.89112 loss)
I0523 00:50:02.972280 35003 sgd_solver.cpp:112] Iteration 104830, lr = 0.01
I0523 00:50:06.159219 35003 solver.cpp:239] Iteration 104840 (3.00721 iter/s, 3.32534s/10 iters), loss = 7.53212
I0523 00:50:06.159272 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53212 (* 1 = 7.53212 loss)
I0523 00:50:06.895715 35003 sgd_solver.cpp:112] Iteration 104840, lr = 0.01
I0523 00:50:10.036571 35003 solver.cpp:239] Iteration 104850 (2.57922 iter/s, 3.87714s/10 iters), loss = 6.7286
I0523 00:50:10.036624 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7286 (* 1 = 6.7286 loss)
I0523 00:50:10.745558 35003 sgd_solver.cpp:112] Iteration 104850, lr = 0.01
I0523 00:50:13.327880 35003 solver.cpp:239] Iteration 104860 (3.03848 iter/s, 3.29111s/10 iters), loss = 7.50499
I0523 00:50:13.327929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50499 (* 1 = 7.50499 loss)
I0523 00:50:13.333065 35003 sgd_solver.cpp:112] Iteration 104860, lr = 0.01
I0523 00:50:16.048776 35003 solver.cpp:239] Iteration 104870 (3.6755 iter/s, 2.72072s/10 iters), loss = 6.64899
I0523 00:50:16.048826 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64899 (* 1 = 6.64899 loss)
I0523 00:50:16.058455 35003 sgd_solver.cpp:112] Iteration 104870, lr = 0.01
I0523 00:50:18.853790 35003 solver.cpp:239] Iteration 104880 (3.56526 iter/s, 2.80485s/10 iters), loss = 6.97438
I0523 00:50:18.853832 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97438 (* 1 = 6.97438 loss)
I0523 00:50:18.880338 35003 sgd_solver.cpp:112] Iteration 104880, lr = 0.01
I0523 00:50:21.585652 35003 solver.cpp:239] Iteration 104890 (3.66072 iter/s, 2.7317s/10 iters), loss = 6.94675
I0523 00:50:21.585698 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94675 (* 1 = 6.94675 loss)
I0523 00:50:22.320665 35003 sgd_solver.cpp:112] Iteration 104890, lr = 0.01
I0523 00:50:24.733016 35003 solver.cpp:239] Iteration 104900 (3.17744 iter/s, 3.14719s/10 iters), loss = 6.45724
I0523 00:50:24.733055 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45724 (* 1 = 6.45724 loss)
I0523 00:50:24.738139 35003 sgd_solver.cpp:112] Iteration 104900, lr = 0.01
I0523 00:50:28.272735 35003 solver.cpp:239] Iteration 104910 (2.82524 iter/s, 3.53953s/10 iters), loss = 6.58087
I0523 00:50:28.272970 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58087 (* 1 = 6.58087 loss)
I0523 00:50:28.324059 35003 sgd_solver.cpp:112] Iteration 104910, lr = 0.01
I0523 00:50:31.784281 35003 solver.cpp:239] Iteration 104920 (2.84804 iter/s, 3.51118s/10 iters), loss = 7.978
I0523 00:50:31.784323 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.978 (* 1 = 7.978 loss)
I0523 00:50:31.793663 35003 sgd_solver.cpp:112] Iteration 104920, lr = 0.01
I0523 00:50:35.030488 35003 solver.cpp:239] Iteration 104930 (3.0807 iter/s, 3.24602s/10 iters), loss = 7.0841
I0523 00:50:35.030549 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0841 (* 1 = 7.0841 loss)
I0523 00:50:35.771822 35003 sgd_solver.cpp:112] Iteration 104930, lr = 0.01
I0523 00:50:38.578575 35003 solver.cpp:239] Iteration 104940 (2.81858 iter/s, 3.54788s/10 iters), loss = 7.40672
I0523 00:50:38.578621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40672 (* 1 = 7.40672 loss)
I0523 00:50:39.292965 35003 sgd_solver.cpp:112] Iteration 104940, lr = 0.01
I0523 00:50:43.028327 35003 solver.cpp:239] Iteration 104950 (2.24743 iter/s, 4.44953s/10 iters), loss = 4.52834
I0523 00:50:43.028367 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.52834 (* 1 = 4.52834 loss)
I0523 00:50:43.033864 35003 sgd_solver.cpp:112] Iteration 104950, lr = 0.01
I0523 00:50:46.453052 35003 solver.cpp:239] Iteration 104960 (2.9201 iter/s, 3.42454s/10 iters), loss = 7.70456
I0523 00:50:46.453096 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70456 (* 1 = 7.70456 loss)
I0523 00:50:46.466445 35003 sgd_solver.cpp:112] Iteration 104960, lr = 0.01
I0523 00:50:49.295125 35003 solver.cpp:239] Iteration 104970 (3.51879 iter/s, 2.84189s/10 iters), loss = 6.77123
I0523 00:50:49.295258 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77123 (* 1 = 6.77123 loss)
I0523 00:50:49.298655 35003 sgd_solver.cpp:112] Iteration 104970, lr = 0.01
I0523 00:50:52.136333 35003 solver.cpp:239] Iteration 104980 (3.51996 iter/s, 2.84094s/10 iters), loss = 7.83004
I0523 00:50:52.136389 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83004 (* 1 = 7.83004 loss)
I0523 00:50:52.842749 35003 sgd_solver.cpp:112] Iteration 104980, lr = 0.01
I0523 00:50:55.166599 35003 solver.cpp:239] Iteration 104990 (3.30024 iter/s, 3.03008s/10 iters), loss = 7.16803
I0523 00:50:55.166641 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16803 (* 1 = 7.16803 loss)
I0523 00:50:55.898406 35003 sgd_solver.cpp:112] Iteration 104990, lr = 0.01
I0523 00:50:58.484095 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_105000.caffemodel
I0523 00:50:59.114907 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_105000.solverstate
I0523 00:50:59.262980 35003 solver.cpp:239] Iteration 105000 (2.44131 iter/s, 4.09616s/10 iters), loss = 7.50695
I0523 00:50:59.263043 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50695 (* 1 = 7.50695 loss)
I0523 00:50:59.276347 35003 sgd_solver.cpp:112] Iteration 105000, lr = 0.01
I0523 00:51:03.507095 35003 solver.cpp:239] Iteration 105010 (2.35633 iter/s, 4.24388s/10 iters), loss = 8.90891
I0523 00:51:03.507140 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.90891 (* 1 = 8.90891 loss)
I0523 00:51:03.524674 35003 sgd_solver.cpp:112] Iteration 105010, lr = 0.01
I0523 00:51:07.847177 35003 solver.cpp:239] Iteration 105020 (2.30422 iter/s, 4.33986s/10 iters), loss = 6.74763
I0523 00:51:07.847231 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74763 (* 1 = 6.74763 loss)
I0523 00:51:07.870901 35003 sgd_solver.cpp:112] Iteration 105020, lr = 0.01
I0523 00:51:10.742710 35003 solver.cpp:239] Iteration 105030 (3.45382 iter/s, 2.89534s/10 iters), loss = 6.96081
I0523 00:51:10.742763 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96081 (* 1 = 6.96081 loss)
I0523 00:51:10.755703 35003 sgd_solver.cpp:112] Iteration 105030, lr = 0.01
I0523 00:51:14.037598 35003 solver.cpp:239] Iteration 105040 (3.03518 iter/s, 3.29469s/10 iters), loss = 7.60463
I0523 00:51:14.037647 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60463 (* 1 = 7.60463 loss)
I0523 00:51:14.221072 35003 sgd_solver.cpp:112] Iteration 105040, lr = 0.01
I0523 00:51:17.857760 35003 solver.cpp:239] Iteration 105050 (2.61784 iter/s, 3.81995s/10 iters), loss = 6.70483
I0523 00:51:17.857805 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70483 (* 1 = 6.70483 loss)
I0523 00:51:17.871234 35003 sgd_solver.cpp:112] Iteration 105050, lr = 0.01
I0523 00:51:21.185133 35003 solver.cpp:239] Iteration 105060 (3.00556 iter/s, 3.32717s/10 iters), loss = 7.1272
I0523 00:51:21.185210 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1272 (* 1 = 7.1272 loss)
I0523 00:51:21.899960 35003 sgd_solver.cpp:112] Iteration 105060, lr = 0.01
I0523 00:51:25.402954 35003 solver.cpp:239] Iteration 105070 (2.37105 iter/s, 4.21755s/10 iters), loss = 7.8119
I0523 00:51:25.402997 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8119 (* 1 = 7.8119 loss)
I0523 00:51:25.416214 35003 sgd_solver.cpp:112] Iteration 105070, lr = 0.01
I0523 00:51:30.464032 35003 solver.cpp:239] Iteration 105080 (1.97596 iter/s, 5.06083s/10 iters), loss = 6.77429
I0523 00:51:30.464318 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77429 (* 1 = 6.77429 loss)
I0523 00:51:30.477144 35003 sgd_solver.cpp:112] Iteration 105080, lr = 0.01
I0523 00:51:34.132496 35003 solver.cpp:239] Iteration 105090 (2.72624 iter/s, 3.66805s/10 iters), loss = 6.59477
I0523 00:51:34.132570 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59477 (* 1 = 6.59477 loss)
I0523 00:51:34.847954 35003 sgd_solver.cpp:112] Iteration 105090, lr = 0.01
I0523 00:51:38.419931 35003 solver.cpp:239] Iteration 105100 (2.33254 iter/s, 4.28717s/10 iters), loss = 6.5101
I0523 00:51:38.419973 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5101 (* 1 = 6.5101 loss)
I0523 00:51:38.432706 35003 sgd_solver.cpp:112] Iteration 105100, lr = 0.01
I0523 00:51:40.503345 35003 solver.cpp:239] Iteration 105110 (4.80012 iter/s, 2.08328s/10 iters), loss = 8.34335
I0523 00:51:40.503384 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.34335 (* 1 = 8.34335 loss)
I0523 00:51:41.204984 35003 sgd_solver.cpp:112] Iteration 105110, lr = 0.01
I0523 00:51:44.864279 35003 solver.cpp:239] Iteration 105120 (2.2932 iter/s, 4.36072s/10 iters), loss = 8.27014
I0523 00:51:44.864332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.27014 (* 1 = 8.27014 loss)
I0523 00:51:45.589229 35003 sgd_solver.cpp:112] Iteration 105120, lr = 0.01
I0523 00:51:48.176841 35003 solver.cpp:239] Iteration 105130 (3.01898 iter/s, 3.31237s/10 iters), loss = 7.49404
I0523 00:51:48.176879 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49404 (* 1 = 7.49404 loss)
I0523 00:51:48.202337 35003 sgd_solver.cpp:112] Iteration 105130, lr = 0.01
I0523 00:51:50.080359 35003 solver.cpp:239] Iteration 105140 (5.25378 iter/s, 1.90339s/10 iters), loss = 6.31967
I0523 00:51:50.080405 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31967 (* 1 = 6.31967 loss)
I0523 00:51:50.093489 35003 sgd_solver.cpp:112] Iteration 105140, lr = 0.01
I0523 00:51:52.175905 35003 solver.cpp:239] Iteration 105150 (4.77234 iter/s, 2.09541s/10 iters), loss = 6.90158
I0523 00:51:52.175943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90158 (* 1 = 6.90158 loss)
I0523 00:51:52.188877 35003 sgd_solver.cpp:112] Iteration 105150, lr = 0.01
I0523 00:51:56.523393 35003 solver.cpp:239] Iteration 105160 (2.3003 iter/s, 4.34727s/10 iters), loss = 6.73143
I0523 00:51:56.523447 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73143 (* 1 = 6.73143 loss)
I0523 00:51:56.533862 35003 sgd_solver.cpp:112] Iteration 105160, lr = 0.01
I0523 00:52:00.838001 35003 solver.cpp:239] Iteration 105170 (2.31783 iter/s, 4.31438s/10 iters), loss = 7.59091
I0523 00:52:00.838248 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59091 (* 1 = 7.59091 loss)
I0523 00:52:01.520952 35003 sgd_solver.cpp:112] Iteration 105170, lr = 0.01
I0523 00:52:04.672629 35003 solver.cpp:239] Iteration 105180 (2.60807 iter/s, 3.83425s/10 iters), loss = 7.4037
I0523 00:52:04.672667 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4037 (* 1 = 7.4037 loss)
I0523 00:52:04.678385 35003 sgd_solver.cpp:112] Iteration 105180, lr = 0.01
I0523 00:52:08.311115 35003 solver.cpp:239] Iteration 105190 (2.74855 iter/s, 3.63828s/10 iters), loss = 7.48031
I0523 00:52:08.311167 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48031 (* 1 = 7.48031 loss)
I0523 00:52:09.006139 35003 sgd_solver.cpp:112] Iteration 105190, lr = 0.01
I0523 00:52:13.516036 35003 solver.cpp:239] Iteration 105200 (1.92135 iter/s, 5.20466s/10 iters), loss = 7.15743
I0523 00:52:13.516082 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15743 (* 1 = 7.15743 loss)
I0523 00:52:13.529050 35003 sgd_solver.cpp:112] Iteration 105200, lr = 0.01
I0523 00:52:18.112500 35003 solver.cpp:239] Iteration 105210 (2.1757 iter/s, 4.59623s/10 iters), loss = 8.31752
I0523 00:52:18.112541 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.31752 (* 1 = 8.31752 loss)
I0523 00:52:18.125408 35003 sgd_solver.cpp:112] Iteration 105210, lr = 0.01
I0523 00:52:20.828938 35003 solver.cpp:239] Iteration 105220 (3.68152 iter/s, 2.71627s/10 iters), loss = 7.10976
I0523 00:52:20.828999 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10976 (* 1 = 7.10976 loss)
I0523 00:52:20.833744 35003 sgd_solver.cpp:112] Iteration 105220, lr = 0.01
I0523 00:52:24.459842 35003 solver.cpp:239] Iteration 105230 (2.7543 iter/s, 3.63069s/10 iters), loss = 7.04318
I0523 00:52:24.459888 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04318 (* 1 = 7.04318 loss)
I0523 00:52:24.473619 35003 sgd_solver.cpp:112] Iteration 105230, lr = 0.01
I0523 00:52:28.129050 35003 solver.cpp:239] Iteration 105240 (2.72553 iter/s, 3.66901s/10 iters), loss = 7.44093
I0523 00:52:28.129091 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44093 (* 1 = 7.44093 loss)
I0523 00:52:28.142182 35003 sgd_solver.cpp:112] Iteration 105240, lr = 0.01
I0523 00:52:31.763730 35003 solver.cpp:239] Iteration 105250 (2.75142 iter/s, 3.63449s/10 iters), loss = 7.65636
I0523 00:52:31.764008 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65636 (* 1 = 7.65636 loss)
I0523 00:52:32.504945 35003 sgd_solver.cpp:112] Iteration 105250, lr = 0.01
I0523 00:52:35.989594 35003 solver.cpp:239] Iteration 105260 (2.36662 iter/s, 4.22544s/10 iters), loss = 6.0873
I0523 00:52:35.989645 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0873 (* 1 = 6.0873 loss)
I0523 00:52:35.993156 35003 sgd_solver.cpp:112] Iteration 105260, lr = 0.01
I0523 00:52:39.092645 35003 solver.cpp:239] Iteration 105270 (3.22282 iter/s, 3.10287s/10 iters), loss = 7.60937
I0523 00:52:39.092695 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60937 (* 1 = 7.60937 loss)
I0523 00:52:39.097909 35003 sgd_solver.cpp:112] Iteration 105270, lr = 0.01
I0523 00:52:42.516101 35003 solver.cpp:239] Iteration 105280 (2.9212 iter/s, 3.42326s/10 iters), loss = 7.75715
I0523 00:52:42.516149 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75715 (* 1 = 7.75715 loss)
I0523 00:52:42.532511 35003 sgd_solver.cpp:112] Iteration 105280, lr = 0.01
I0523 00:52:45.960520 35003 solver.cpp:239] Iteration 105290 (2.90341 iter/s, 3.44423s/10 iters), loss = 6.58272
I0523 00:52:45.960559 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58272 (* 1 = 6.58272 loss)
I0523 00:52:45.967794 35003 sgd_solver.cpp:112] Iteration 105290, lr = 0.01
I0523 00:52:49.084759 35003 solver.cpp:239] Iteration 105300 (3.20095 iter/s, 3.12407s/10 iters), loss = 7.84736
I0523 00:52:49.084807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84736 (* 1 = 7.84736 loss)
I0523 00:52:49.091409 35003 sgd_solver.cpp:112] Iteration 105300, lr = 0.01
I0523 00:52:52.813551 35003 solver.cpp:239] Iteration 105310 (2.682 iter/s, 3.72855s/10 iters), loss = 6.63527
I0523 00:52:52.813601 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63527 (* 1 = 6.63527 loss)
I0523 00:52:53.451371 35003 sgd_solver.cpp:112] Iteration 105310, lr = 0.01
I0523 00:52:56.114346 35003 solver.cpp:239] Iteration 105320 (3.02974 iter/s, 3.30061s/10 iters), loss = 6.64135
I0523 00:52:56.114390 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64135 (* 1 = 6.64135 loss)
I0523 00:52:56.136409 35003 sgd_solver.cpp:112] Iteration 105320, lr = 0.01
I0523 00:53:00.034014 35003 solver.cpp:239] Iteration 105330 (2.55137 iter/s, 3.91946s/10 iters), loss = 7.56844
I0523 00:53:00.034065 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56844 (* 1 = 7.56844 loss)
I0523 00:53:00.346839 35003 sgd_solver.cpp:112] Iteration 105330, lr = 0.01
I0523 00:53:04.645964 35003 solver.cpp:239] Iteration 105340 (2.16839 iter/s, 4.61171s/10 iters), loss = 6.52956
I0523 00:53:04.646164 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52956 (* 1 = 6.52956 loss)
I0523 00:53:05.379784 35003 sgd_solver.cpp:112] Iteration 105340, lr = 0.01
I0523 00:53:08.032271 35003 solver.cpp:239] Iteration 105350 (2.95337 iter/s, 3.38596s/10 iters), loss = 6.2937
I0523 00:53:08.032323 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2937 (* 1 = 6.2937 loss)
I0523 00:53:08.037909 35003 sgd_solver.cpp:112] Iteration 105350, lr = 0.01
I0523 00:53:11.554534 35003 solver.cpp:239] Iteration 105360 (2.83925 iter/s, 3.52206s/10 iters), loss = 7.87606
I0523 00:53:11.554601 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87606 (* 1 = 7.87606 loss)
I0523 00:53:11.584705 35003 sgd_solver.cpp:112] Iteration 105360, lr = 0.01
I0523 00:53:15.157546 35003 solver.cpp:239] Iteration 105370 (2.77562 iter/s, 3.6028s/10 iters), loss = 7.09019
I0523 00:53:15.157588 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09019 (* 1 = 7.09019 loss)
I0523 00:53:15.210961 35003 sgd_solver.cpp:112] Iteration 105370, lr = 0.01
I0523 00:53:17.570678 35003 solver.cpp:239] Iteration 105380 (4.14425 iter/s, 2.41298s/10 iters), loss = 7.57857
I0523 00:53:17.570730 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57857 (* 1 = 7.57857 loss)
I0523 00:53:17.583617 35003 sgd_solver.cpp:112] Iteration 105380, lr = 0.01
I0523 00:53:20.934741 35003 solver.cpp:239] Iteration 105390 (2.9728 iter/s, 3.36383s/10 iters), loss = 6.65101
I0523 00:53:20.934799 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65101 (* 1 = 6.65101 loss)
I0523 00:53:20.959766 35003 sgd_solver.cpp:112] Iteration 105390, lr = 0.01
I0523 00:53:24.494015 35003 solver.cpp:239] Iteration 105400 (2.80973 iter/s, 3.55907s/10 iters), loss = 7.58069
I0523 00:53:24.494069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58069 (* 1 = 7.58069 loss)
I0523 00:53:24.965382 35003 sgd_solver.cpp:112] Iteration 105400, lr = 0.01
I0523 00:53:27.708395 35003 solver.cpp:239] Iteration 105410 (3.1112 iter/s, 3.21419s/10 iters), loss = 6.70744
I0523 00:53:27.708437 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70744 (* 1 = 6.70744 loss)
I0523 00:53:27.721319 35003 sgd_solver.cpp:112] Iteration 105410, lr = 0.01
I0523 00:53:32.979874 35003 solver.cpp:239] Iteration 105420 (1.89709 iter/s, 5.27122s/10 iters), loss = 7.5001
I0523 00:53:32.979921 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5001 (* 1 = 7.5001 loss)
I0523 00:53:33.028404 35003 sgd_solver.cpp:112] Iteration 105420, lr = 0.01
I0523 00:53:37.585741 35003 solver.cpp:239] Iteration 105430 (2.17125 iter/s, 4.60563s/10 iters), loss = 6.96149
I0523 00:53:37.585858 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96149 (* 1 = 6.96149 loss)
I0523 00:53:37.614621 35003 sgd_solver.cpp:112] Iteration 105430, lr = 0.01
I0523 00:53:40.582628 35003 solver.cpp:239] Iteration 105440 (3.33707 iter/s, 2.99664s/10 iters), loss = 8.10165
I0523 00:53:40.582679 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.10165 (* 1 = 8.10165 loss)
I0523 00:53:40.605592 35003 sgd_solver.cpp:112] Iteration 105440, lr = 0.01
I0523 00:53:44.993947 35003 solver.cpp:239] Iteration 105450 (2.26703 iter/s, 4.41105s/10 iters), loss = 6.7396
I0523 00:53:44.994001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7396 (* 1 = 6.7396 loss)
I0523 00:53:45.105904 35003 sgd_solver.cpp:112] Iteration 105450, lr = 0.01
I0523 00:53:48.142835 35003 solver.cpp:239] Iteration 105460 (3.17592 iter/s, 3.1487s/10 iters), loss = 7.67019
I0523 00:53:48.142892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67019 (* 1 = 7.67019 loss)
I0523 00:53:48.156299 35003 sgd_solver.cpp:112] Iteration 105460, lr = 0.01
I0523 00:53:51.806609 35003 solver.cpp:239] Iteration 105470 (2.72958 iter/s, 3.66357s/10 iters), loss = 6.66037
I0523 00:53:51.806650 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66037 (* 1 = 6.66037 loss)
I0523 00:53:51.809273 35003 sgd_solver.cpp:112] Iteration 105470, lr = 0.01
I0523 00:53:54.655611 35003 solver.cpp:239] Iteration 105480 (3.5102 iter/s, 2.84884s/10 iters), loss = 7.5424
I0523 00:53:54.655650 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5424 (* 1 = 7.5424 loss)
I0523 00:53:54.666288 35003 sgd_solver.cpp:112] Iteration 105480, lr = 0.01
I0523 00:53:57.528091 35003 solver.cpp:239] Iteration 105490 (3.48151 iter/s, 2.87232s/10 iters), loss = 7.40844
I0523 00:53:57.528146 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40844 (* 1 = 7.40844 loss)
I0523 00:53:57.535027 35003 sgd_solver.cpp:112] Iteration 105490, lr = 0.01
I0523 00:53:59.295557 35003 solver.cpp:239] Iteration 105500 (5.65825 iter/s, 1.76733s/10 iters), loss = 6.61152
I0523 00:53:59.295608 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61152 (* 1 = 6.61152 loss)
I0523 00:53:59.961119 35003 sgd_solver.cpp:112] Iteration 105500, lr = 0.01
I0523 00:54:02.689756 35003 solver.cpp:239] Iteration 105510 (2.94637 iter/s, 3.39401s/10 iters), loss = 6.54718
I0523 00:54:02.689801 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54718 (* 1 = 6.54718 loss)
I0523 00:54:02.703781 35003 sgd_solver.cpp:112] Iteration 105510, lr = 0.01
I0523 00:54:06.966961 35003 solver.cpp:239] Iteration 105520 (2.3381 iter/s, 4.27698s/10 iters), loss = 6.82321
I0523 00:54:06.967002 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82321 (* 1 = 6.82321 loss)
I0523 00:54:07.654311 35003 sgd_solver.cpp:112] Iteration 105520, lr = 0.01
I0523 00:54:12.046063 35003 solver.cpp:239] Iteration 105530 (1.96895 iter/s, 5.07885s/10 iters), loss = 7.07729
I0523 00:54:12.046120 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07729 (* 1 = 7.07729 loss)
I0523 00:54:12.054631 35003 sgd_solver.cpp:112] Iteration 105530, lr = 0.01
I0523 00:54:15.687357 35003 solver.cpp:239] Iteration 105540 (2.74645 iter/s, 3.64107s/10 iters), loss = 7.13704
I0523 00:54:15.687412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13704 (* 1 = 7.13704 loss)
I0523 00:54:15.699381 35003 sgd_solver.cpp:112] Iteration 105540, lr = 0.01
I0523 00:54:19.234407 35003 solver.cpp:239] Iteration 105550 (2.81941 iter/s, 3.54684s/10 iters), loss = 7.21117
I0523 00:54:19.234469 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21117 (* 1 = 7.21117 loss)
I0523 00:54:19.245504 35003 sgd_solver.cpp:112] Iteration 105550, lr = 0.01
I0523 00:54:22.983182 35003 solver.cpp:239] Iteration 105560 (2.66769 iter/s, 3.74856s/10 iters), loss = 7.26362
I0523 00:54:22.983225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26362 (* 1 = 7.26362 loss)
I0523 00:54:23.698729 35003 sgd_solver.cpp:112] Iteration 105560, lr = 0.01
I0523 00:54:26.567522 35003 solver.cpp:239] Iteration 105570 (2.79009 iter/s, 3.58412s/10 iters), loss = 7.68149
I0523 00:54:26.567564 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68149 (* 1 = 7.68149 loss)
I0523 00:54:26.580309 35003 sgd_solver.cpp:112] Iteration 105570, lr = 0.01
I0523 00:54:30.907963 35003 solver.cpp:239] Iteration 105580 (2.30403 iter/s, 4.34022s/10 iters), loss = 7.30986
I0523 00:54:30.908001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30986 (* 1 = 7.30986 loss)
I0523 00:54:30.914433 35003 sgd_solver.cpp:112] Iteration 105580, lr = 0.01
I0523 00:54:35.233258 35003 solver.cpp:239] Iteration 105590 (2.3121 iter/s, 4.32508s/10 iters), loss = 7.49151
I0523 00:54:35.233302 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49151 (* 1 = 7.49151 loss)
I0523 00:54:35.240296 35003 sgd_solver.cpp:112] Iteration 105590, lr = 0.01
I0523 00:54:38.130409 35003 solver.cpp:239] Iteration 105600 (3.45187 iter/s, 2.89698s/10 iters), loss = 7.389
I0523 00:54:38.130724 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.389 (* 1 = 7.389 loss)
I0523 00:54:38.133447 35003 sgd_solver.cpp:112] Iteration 105600, lr = 0.01
I0523 00:54:40.303674 35003 solver.cpp:239] Iteration 105610 (4.6022 iter/s, 2.17287s/10 iters), loss = 7.24223
I0523 00:54:40.303719 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24223 (* 1 = 7.24223 loss)
I0523 00:54:40.310245 35003 sgd_solver.cpp:112] Iteration 105610, lr = 0.01
I0523 00:54:43.735440 35003 solver.cpp:239] Iteration 105620 (2.91411 iter/s, 3.43158s/10 iters), loss = 7.05706
I0523 00:54:43.735491 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05706 (* 1 = 7.05706 loss)
I0523 00:54:43.741621 35003 sgd_solver.cpp:112] Iteration 105620, lr = 0.01
I0523 00:54:46.927762 35003 solver.cpp:239] Iteration 105630 (3.1327 iter/s, 3.19214s/10 iters), loss = 8.54717
I0523 00:54:46.927825 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.54717 (* 1 = 8.54717 loss)
I0523 00:54:46.941167 35003 sgd_solver.cpp:112] Iteration 105630, lr = 0.01
I0523 00:54:50.927655 35003 solver.cpp:239] Iteration 105640 (2.50021 iter/s, 3.99966s/10 iters), loss = 7.25098
I0523 00:54:50.927721 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25098 (* 1 = 7.25098 loss)
I0523 00:54:50.940474 35003 sgd_solver.cpp:112] Iteration 105640, lr = 0.01
I0523 00:54:55.197249 35003 solver.cpp:239] Iteration 105650 (2.34228 iter/s, 4.26934s/10 iters), loss = 7.39866
I0523 00:54:55.197307 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39866 (* 1 = 7.39866 loss)
I0523 00:54:55.791378 35003 sgd_solver.cpp:112] Iteration 105650, lr = 0.01
I0523 00:54:59.351027 35003 solver.cpp:239] Iteration 105660 (2.40758 iter/s, 4.15355s/10 iters), loss = 7.48553
I0523 00:54:59.351073 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48553 (* 1 = 7.48553 loss)
I0523 00:54:59.364735 35003 sgd_solver.cpp:112] Iteration 105660, lr = 0.01
I0523 00:55:02.769250 35003 solver.cpp:239] Iteration 105670 (2.92566 iter/s, 3.41803s/10 iters), loss = 7.01118
I0523 00:55:02.769289 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01118 (* 1 = 7.01118 loss)
I0523 00:55:02.772249 35003 sgd_solver.cpp:112] Iteration 105670, lr = 0.01
I0523 00:55:07.532914 35003 solver.cpp:239] Iteration 105680 (2.09933 iter/s, 4.76342s/10 iters), loss = 6.86409
I0523 00:55:07.532960 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86409 (* 1 = 6.86409 loss)
I0523 00:55:07.546321 35003 sgd_solver.cpp:112] Iteration 105680, lr = 0.01
I0523 00:55:10.437517 35003 solver.cpp:239] Iteration 105690 (3.44301 iter/s, 2.90444s/10 iters), loss = 7.33562
I0523 00:55:10.437762 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33562 (* 1 = 7.33562 loss)
I0523 00:55:11.153115 35003 sgd_solver.cpp:112] Iteration 105690, lr = 0.01
I0523 00:55:13.724117 35003 solver.cpp:239] Iteration 105700 (3.04298 iter/s, 3.28625s/10 iters), loss = 6.96439
I0523 00:55:13.724162 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96439 (* 1 = 6.96439 loss)
I0523 00:55:14.054999 35003 sgd_solver.cpp:112] Iteration 105700, lr = 0.01
I0523 00:55:17.557376 35003 solver.cpp:239] Iteration 105710 (2.60889 iter/s, 3.83304s/10 iters), loss = 6.92733
I0523 00:55:17.557437 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92733 (* 1 = 6.92733 loss)
I0523 00:55:18.291738 35003 sgd_solver.cpp:112] Iteration 105710, lr = 0.01
I0523 00:55:21.803406 35003 solver.cpp:239] Iteration 105720 (2.35527 iter/s, 4.24579s/10 iters), loss = 7.41375
I0523 00:55:21.803447 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41375 (* 1 = 7.41375 loss)
I0523 00:55:21.817183 35003 sgd_solver.cpp:112] Iteration 105720, lr = 0.01
I0523 00:55:24.580693 35003 solver.cpp:239] Iteration 105730 (3.60086 iter/s, 2.77712s/10 iters), loss = 7.63944
I0523 00:55:24.580759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63944 (* 1 = 7.63944 loss)
I0523 00:55:25.263706 35003 sgd_solver.cpp:112] Iteration 105730, lr = 0.01
I0523 00:55:31.329295 35003 solver.cpp:239] Iteration 105740 (1.48186 iter/s, 6.74827s/10 iters), loss = 7.83393
I0523 00:55:31.329339 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83393 (* 1 = 7.83393 loss)
I0523 00:55:31.335253 35003 sgd_solver.cpp:112] Iteration 105740, lr = 0.01
I0523 00:55:34.137907 35003 solver.cpp:239] Iteration 105750 (3.56068 iter/s, 2.80845s/10 iters), loss = 7.99925
I0523 00:55:34.137949 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99925 (* 1 = 7.99925 loss)
I0523 00:55:34.855262 35003 sgd_solver.cpp:112] Iteration 105750, lr = 0.01
I0523 00:55:38.398576 35003 solver.cpp:239] Iteration 105760 (2.34717 iter/s, 4.26045s/10 iters), loss = 6.98668
I0523 00:55:38.398620 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98668 (* 1 = 6.98668 loss)
I0523 00:55:38.425460 35003 sgd_solver.cpp:112] Iteration 105760, lr = 0.01
I0523 00:55:41.152640 35003 solver.cpp:239] Iteration 105770 (3.63123 iter/s, 2.75389s/10 iters), loss = 7.47445
I0523 00:55:41.152879 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47445 (* 1 = 7.47445 loss)
I0523 00:55:41.841310 35003 sgd_solver.cpp:112] Iteration 105770, lr = 0.01
I0523 00:55:44.668654 35003 solver.cpp:239] Iteration 105780 (2.84444 iter/s, 3.51563s/10 iters), loss = 7.16292
I0523 00:55:44.668699 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16292 (* 1 = 7.16292 loss)
I0523 00:55:45.013232 35003 sgd_solver.cpp:112] Iteration 105780, lr = 0.01
I0523 00:55:48.608628 35003 solver.cpp:239] Iteration 105790 (2.53822 iter/s, 3.93977s/10 iters), loss = 6.69613
I0523 00:55:48.608674 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69613 (* 1 = 6.69613 loss)
I0523 00:55:48.632992 35003 sgd_solver.cpp:112] Iteration 105790, lr = 0.01
I0523 00:55:52.212090 35003 solver.cpp:239] Iteration 105800 (2.77526 iter/s, 3.60326s/10 iters), loss = 7.20423
I0523 00:55:52.212146 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20423 (* 1 = 7.20423 loss)
I0523 00:55:52.225415 35003 sgd_solver.cpp:112] Iteration 105800, lr = 0.01
I0523 00:55:54.300058 35003 solver.cpp:239] Iteration 105810 (4.78968 iter/s, 2.08782s/10 iters), loss = 7.24383
I0523 00:55:54.300108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24383 (* 1 = 7.24383 loss)
I0523 00:55:54.307673 35003 sgd_solver.cpp:112] Iteration 105810, lr = 0.01
I0523 00:55:58.023512 35003 solver.cpp:239] Iteration 105820 (2.68583 iter/s, 3.72325s/10 iters), loss = 6.90975
I0523 00:55:58.023560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90975 (* 1 = 6.90975 loss)
I0523 00:55:58.036763 35003 sgd_solver.cpp:112] Iteration 105820, lr = 0.01
I0523 00:56:02.356094 35003 solver.cpp:239] Iteration 105830 (2.30821 iter/s, 4.33235s/10 iters), loss = 7.81758
I0523 00:56:02.356137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81758 (* 1 = 7.81758 loss)
I0523 00:56:02.365933 35003 sgd_solver.cpp:112] Iteration 105830, lr = 0.01
I0523 00:56:05.258098 35003 solver.cpp:239] Iteration 105840 (3.4461 iter/s, 2.90183s/10 iters), loss = 6.70504
I0523 00:56:05.258150 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70504 (* 1 = 6.70504 loss)
I0523 00:56:05.283228 35003 sgd_solver.cpp:112] Iteration 105840, lr = 0.01
I0523 00:56:10.466711 35003 solver.cpp:239] Iteration 105850 (1.92 iter/s, 5.20834s/10 iters), loss = 7.6348
I0523 00:56:10.466776 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6348 (* 1 = 7.6348 loss)
I0523 00:56:11.207582 35003 sgd_solver.cpp:112] Iteration 105850, lr = 0.01
I0523 00:56:13.989745 35003 solver.cpp:239] Iteration 105860 (2.83864 iter/s, 3.52282s/10 iters), loss = 7.59347
I0523 00:56:13.989796 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59347 (* 1 = 7.59347 loss)
I0523 00:56:14.651352 35003 sgd_solver.cpp:112] Iteration 105860, lr = 0.01
I0523 00:56:18.188616 35003 solver.cpp:239] Iteration 105870 (2.38172 iter/s, 4.19865s/10 iters), loss = 7.36606
I0523 00:56:18.188659 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36606 (* 1 = 7.36606 loss)
I0523 00:56:18.206990 35003 sgd_solver.cpp:112] Iteration 105870, lr = 0.01
I0523 00:56:20.865562 35003 solver.cpp:239] Iteration 105880 (3.73583 iter/s, 2.67678s/10 iters), loss = 6.10509
I0523 00:56:20.865612 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10509 (* 1 = 6.10509 loss)
I0523 00:56:20.877663 35003 sgd_solver.cpp:112] Iteration 105880, lr = 0.01
I0523 00:56:23.831758 35003 solver.cpp:239] Iteration 105890 (3.37153 iter/s, 2.96601s/10 iters), loss = 7.60454
I0523 00:56:23.831804 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60454 (* 1 = 7.60454 loss)
I0523 00:56:24.491447 35003 sgd_solver.cpp:112] Iteration 105890, lr = 0.01
I0523 00:56:27.909601 35003 solver.cpp:239] Iteration 105900 (2.45241 iter/s, 4.07762s/10 iters), loss = 7.33725
I0523 00:56:27.909641 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33725 (* 1 = 7.33725 loss)
I0523 00:56:27.922811 35003 sgd_solver.cpp:112] Iteration 105900, lr = 0.01
I0523 00:56:30.058161 35003 solver.cpp:239] Iteration 105910 (4.65459 iter/s, 2.14842s/10 iters), loss = 7.74031
I0523 00:56:30.058244 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74031 (* 1 = 7.74031 loss)
I0523 00:56:30.798804 35003 sgd_solver.cpp:112] Iteration 105910, lr = 0.01
I0523 00:56:34.454836 35003 solver.cpp:239] Iteration 105920 (2.27457 iter/s, 4.39643s/10 iters), loss = 6.65147
I0523 00:56:34.454879 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65147 (* 1 = 6.65147 loss)
I0523 00:56:34.483000 35003 sgd_solver.cpp:112] Iteration 105920, lr = 0.01
I0523 00:56:39.667577 35003 solver.cpp:239] Iteration 105930 (1.91848 iter/s, 5.21247s/10 iters), loss = 8.71183
I0523 00:56:39.667651 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.71183 (* 1 = 8.71183 loss)
I0523 00:56:39.686611 35003 sgd_solver.cpp:112] Iteration 105930, lr = 0.01
I0523 00:56:42.848075 35003 solver.cpp:239] Iteration 105940 (3.14436 iter/s, 3.1803s/10 iters), loss = 6.89277
I0523 00:56:42.848248 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89277 (* 1 = 6.89277 loss)
I0523 00:56:42.852538 35003 sgd_solver.cpp:112] Iteration 105940, lr = 0.01
I0523 00:56:46.288228 35003 solver.cpp:239] Iteration 105950 (2.90711 iter/s, 3.43984s/10 iters), loss = 8.35084
I0523 00:56:46.288270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.35084 (* 1 = 8.35084 loss)
I0523 00:56:46.977875 35003 sgd_solver.cpp:112] Iteration 105950, lr = 0.01
I0523 00:56:51.334398 35003 solver.cpp:239] Iteration 105960 (1.9818 iter/s, 5.04591s/10 iters), loss = 7.28703
I0523 00:56:51.334450 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28703 (* 1 = 7.28703 loss)
I0523 00:56:51.996518 35003 sgd_solver.cpp:112] Iteration 105960, lr = 0.01
I0523 00:56:55.684563 35003 solver.cpp:239] Iteration 105970 (2.29888 iter/s, 4.34994s/10 iters), loss = 7.57181
I0523 00:56:55.684599 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57181 (* 1 = 7.57181 loss)
I0523 00:56:55.697841 35003 sgd_solver.cpp:112] Iteration 105970, lr = 0.01
I0523 00:56:57.788568 35003 solver.cpp:239] Iteration 105980 (4.75314 iter/s, 2.10387s/10 iters), loss = 7.27843
I0523 00:56:57.788606 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27843 (* 1 = 7.27843 loss)
I0523 00:56:57.795836 35003 sgd_solver.cpp:112] Iteration 105980, lr = 0.01
I0523 00:56:59.892189 35003 solver.cpp:239] Iteration 105990 (4.75401 iter/s, 2.10349s/10 iters), loss = 7.40575
I0523 00:56:59.892227 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40575 (* 1 = 7.40575 loss)
I0523 00:56:59.899780 35003 sgd_solver.cpp:112] Iteration 105990, lr = 0.01
I0523 00:57:02.291237 35003 solver.cpp:239] Iteration 106000 (4.16857 iter/s, 2.3989s/10 iters), loss = 7.27864
I0523 00:57:02.291280 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27864 (* 1 = 7.27864 loss)
I0523 00:57:02.303685 35003 sgd_solver.cpp:112] Iteration 106000, lr = 0.01
I0523 00:57:07.493103 35003 solver.cpp:239] Iteration 106010 (1.92248 iter/s, 5.2016s/10 iters), loss = 7.20695
I0523 00:57:07.493155 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20695 (* 1 = 7.20695 loss)
I0523 00:57:08.194965 35003 sgd_solver.cpp:112] Iteration 106010, lr = 0.01
I0523 00:57:10.560254 35003 solver.cpp:239] Iteration 106020 (3.26055 iter/s, 3.06697s/10 iters), loss = 7.2461
I0523 00:57:10.560307 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2461 (* 1 = 7.2461 loss)
I0523 00:57:10.573738 35003 sgd_solver.cpp:112] Iteration 106020, lr = 0.01
I0523 00:57:14.026106 35003 solver.cpp:239] Iteration 106030 (2.88546 iter/s, 3.46565s/10 iters), loss = 8.10601
I0523 00:57:14.026401 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.10601 (* 1 = 8.10601 loss)
I0523 00:57:14.766963 35003 sgd_solver.cpp:112] Iteration 106030, lr = 0.01
I0523 00:57:18.773166 35003 solver.cpp:239] Iteration 106040 (2.10677 iter/s, 4.7466s/10 iters), loss = 7.33448
I0523 00:57:18.773211 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33448 (* 1 = 7.33448 loss)
I0523 00:57:18.790791 35003 sgd_solver.cpp:112] Iteration 106040, lr = 0.01
I0523 00:57:23.770761 35003 solver.cpp:239] Iteration 106050 (2.00106 iter/s, 4.99735s/10 iters), loss = 7.62082
I0523 00:57:23.770803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62082 (* 1 = 7.62082 loss)
I0523 00:57:24.476320 35003 sgd_solver.cpp:112] Iteration 106050, lr = 0.01
I0523 00:57:27.417158 35003 solver.cpp:239] Iteration 106060 (2.74258 iter/s, 3.6462s/10 iters), loss = 7.11067
I0523 00:57:27.417212 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11067 (* 1 = 7.11067 loss)
I0523 00:57:27.422940 35003 sgd_solver.cpp:112] Iteration 106060, lr = 0.01
I0523 00:57:33.024101 35003 solver.cpp:239] Iteration 106070 (1.78363 iter/s, 5.60653s/10 iters), loss = 9.44784
I0523 00:57:33.024145 35003 solver.cpp:258]     Train net output #0: softmax_loss = 9.44784 (* 1 = 9.44784 loss)
I0523 00:57:33.032424 35003 sgd_solver.cpp:112] Iteration 106070, lr = 0.01
I0523 00:57:35.951678 35003 solver.cpp:239] Iteration 106080 (3.41601 iter/s, 2.92739s/10 iters), loss = 6.65938
I0523 00:57:35.951733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65938 (* 1 = 6.65938 loss)
I0523 00:57:35.961174 35003 sgd_solver.cpp:112] Iteration 106080, lr = 0.01
I0523 00:57:38.722950 35003 solver.cpp:239] Iteration 106090 (3.60872 iter/s, 2.77107s/10 iters), loss = 7.83112
I0523 00:57:38.722995 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83112 (* 1 = 7.83112 loss)
I0523 00:57:38.734750 35003 sgd_solver.cpp:112] Iteration 106090, lr = 0.01
I0523 00:57:41.271648 35003 solver.cpp:239] Iteration 106100 (3.9238 iter/s, 2.54855s/10 iters), loss = 7.95566
I0523 00:57:41.271685 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95566 (* 1 = 7.95566 loss)
I0523 00:57:41.524809 35003 sgd_solver.cpp:112] Iteration 106100, lr = 0.01
I0523 00:57:45.636297 35003 solver.cpp:239] Iteration 106110 (2.29125 iter/s, 4.36443s/10 iters), loss = 7.57009
I0523 00:57:45.636464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57009 (* 1 = 7.57009 loss)
I0523 00:57:45.648912 35003 sgd_solver.cpp:112] Iteration 106110, lr = 0.01
I0523 00:57:49.252066 35003 solver.cpp:239] Iteration 106120 (2.7659 iter/s, 3.61546s/10 iters), loss = 7.41474
I0523 00:57:49.252105 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41474 (* 1 = 7.41474 loss)
I0523 00:57:49.266167 35003 sgd_solver.cpp:112] Iteration 106120, lr = 0.01
I0523 00:57:52.920596 35003 solver.cpp:239] Iteration 106130 (2.72603 iter/s, 3.66833s/10 iters), loss = 7.10422
I0523 00:57:52.920653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10422 (* 1 = 7.10422 loss)
I0523 00:57:52.933198 35003 sgd_solver.cpp:112] Iteration 106130, lr = 0.01
I0523 00:57:55.897997 35003 solver.cpp:239] Iteration 106140 (3.35884 iter/s, 2.97722s/10 iters), loss = 7.41733
I0523 00:57:55.898036 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41733 (* 1 = 7.41733 loss)
I0523 00:57:56.512428 35003 sgd_solver.cpp:112] Iteration 106140, lr = 0.01
I0523 00:57:58.772429 35003 solver.cpp:239] Iteration 106150 (3.47914 iter/s, 2.87427s/10 iters), loss = 7.74275
I0523 00:57:58.772483 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74275 (* 1 = 7.74275 loss)
I0523 00:57:58.807101 35003 sgd_solver.cpp:112] Iteration 106150, lr = 0.01
I0523 00:58:03.205214 35003 solver.cpp:239] Iteration 106160 (2.25604 iter/s, 4.43254s/10 iters), loss = 6.43812
I0523 00:58:03.205265 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43812 (* 1 = 6.43812 loss)
I0523 00:58:03.214804 35003 sgd_solver.cpp:112] Iteration 106160, lr = 0.01
I0523 00:58:07.414849 35003 solver.cpp:239] Iteration 106170 (2.37563 iter/s, 4.2094s/10 iters), loss = 7.08677
I0523 00:58:07.414913 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08677 (* 1 = 7.08677 loss)
I0523 00:58:07.427467 35003 sgd_solver.cpp:112] Iteration 106170, lr = 0.01
I0523 00:58:12.542412 35003 solver.cpp:239] Iteration 106180 (1.95035 iter/s, 5.1273s/10 iters), loss = 6.34027
I0523 00:58:12.542457 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34027 (* 1 = 6.34027 loss)
I0523 00:58:13.272589 35003 sgd_solver.cpp:112] Iteration 106180, lr = 0.01
I0523 00:58:18.525158 35003 solver.cpp:239] Iteration 106190 (1.67155 iter/s, 5.98246s/10 iters), loss = 7.7294
I0523 00:58:18.525434 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7294 (* 1 = 7.7294 loss)
I0523 00:58:18.530642 35003 sgd_solver.cpp:112] Iteration 106190, lr = 0.01
I0523 00:58:22.232843 35003 solver.cpp:239] Iteration 106200 (2.69739 iter/s, 3.70729s/10 iters), loss = 7.37362
I0523 00:58:22.232892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37362 (* 1 = 7.37362 loss)
I0523 00:58:22.242712 35003 sgd_solver.cpp:112] Iteration 106200, lr = 0.01
I0523 00:58:24.299193 35003 solver.cpp:239] Iteration 106210 (4.83977 iter/s, 2.06621s/10 iters), loss = 7.20977
I0523 00:58:24.299233 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20977 (* 1 = 7.20977 loss)
I0523 00:58:24.309352 35003 sgd_solver.cpp:112] Iteration 106210, lr = 0.01
I0523 00:58:28.894624 35003 solver.cpp:239] Iteration 106220 (2.17619 iter/s, 4.5952s/10 iters), loss = 7.63735
I0523 00:58:28.894666 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63735 (* 1 = 7.63735 loss)
I0523 00:58:28.907282 35003 sgd_solver.cpp:112] Iteration 106220, lr = 0.01
I0523 00:58:33.499500 35003 solver.cpp:239] Iteration 106230 (2.17172 iter/s, 4.60465s/10 iters), loss = 7.6022
I0523 00:58:33.499536 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6022 (* 1 = 7.6022 loss)
I0523 00:58:33.512735 35003 sgd_solver.cpp:112] Iteration 106230, lr = 0.01
I0523 00:58:36.792613 35003 solver.cpp:239] Iteration 106240 (3.0368 iter/s, 3.29294s/10 iters), loss = 7.48751
I0523 00:58:36.792656 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48751 (* 1 = 7.48751 loss)
I0523 00:58:36.805766 35003 sgd_solver.cpp:112] Iteration 106240, lr = 0.01
I0523 00:58:40.438087 35003 solver.cpp:239] Iteration 106250 (2.7433 iter/s, 3.64525s/10 iters), loss = 7.11654
I0523 00:58:40.438159 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11654 (* 1 = 7.11654 loss)
I0523 00:58:41.064854 35003 sgd_solver.cpp:112] Iteration 106250, lr = 0.01
I0523 00:58:43.900821 35003 solver.cpp:239] Iteration 106260 (2.88986 iter/s, 3.46037s/10 iters), loss = 7.30053
I0523 00:58:43.900858 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30053 (* 1 = 7.30053 loss)
I0523 00:58:43.909301 35003 sgd_solver.cpp:112] Iteration 106260, lr = 0.01
I0523 00:58:49.032310 35003 solver.cpp:239] Iteration 106270 (1.94884 iter/s, 5.13125s/10 iters), loss = 7.99923
I0523 00:58:49.032562 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99923 (* 1 = 7.99923 loss)
I0523 00:58:49.039115 35003 sgd_solver.cpp:112] Iteration 106270, lr = 0.01
I0523 00:58:53.832548 35003 solver.cpp:239] Iteration 106280 (2.08342 iter/s, 4.79981s/10 iters), loss = 5.94636
I0523 00:58:53.832602 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.94636 (* 1 = 5.94636 loss)
I0523 00:58:54.037130 35003 sgd_solver.cpp:112] Iteration 106280, lr = 0.01
I0523 00:58:55.338399 35003 solver.cpp:239] Iteration 106290 (6.64132 iter/s, 1.50573s/10 iters), loss = 6.03397
I0523 00:58:55.338451 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03397 (* 1 = 6.03397 loss)
I0523 00:58:56.079439 35003 sgd_solver.cpp:112] Iteration 106290, lr = 0.01
I0523 00:58:58.903564 35003 solver.cpp:239] Iteration 106300 (2.80508 iter/s, 3.56497s/10 iters), loss = 7.68115
I0523 00:58:58.903611 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68115 (* 1 = 7.68115 loss)
I0523 00:58:58.916291 35003 sgd_solver.cpp:112] Iteration 106300, lr = 0.01
I0523 00:59:00.783381 35003 solver.cpp:239] Iteration 106310 (5.32006 iter/s, 1.87968s/10 iters), loss = 7.79382
I0523 00:59:00.783445 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79382 (* 1 = 7.79382 loss)
I0523 00:59:00.792655 35003 sgd_solver.cpp:112] Iteration 106310, lr = 0.01
I0523 00:59:02.901885 35003 solver.cpp:239] Iteration 106320 (4.72065 iter/s, 2.11835s/10 iters), loss = 7.06331
I0523 00:59:02.901940 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06331 (* 1 = 7.06331 loss)
I0523 00:59:03.454061 35003 sgd_solver.cpp:112] Iteration 106320, lr = 0.01
I0523 00:59:05.499545 35003 solver.cpp:239] Iteration 106330 (3.84988 iter/s, 2.59749s/10 iters), loss = 8.01611
I0523 00:59:05.499588 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01611 (* 1 = 8.01611 loss)
I0523 00:59:05.522455 35003 sgd_solver.cpp:112] Iteration 106330, lr = 0.01
I0523 00:59:10.018211 35003 solver.cpp:239] Iteration 106340 (2.21315 iter/s, 4.51844s/10 iters), loss = 7.1825
I0523 00:59:10.018256 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1825 (* 1 = 7.1825 loss)
I0523 00:59:10.020965 35003 sgd_solver.cpp:112] Iteration 106340, lr = 0.01
I0523 00:59:12.828244 35003 solver.cpp:239] Iteration 106350 (3.55889 iter/s, 2.80986s/10 iters), loss = 6.6601
I0523 00:59:12.828299 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6601 (* 1 = 6.6601 loss)
I0523 00:59:12.831835 35003 sgd_solver.cpp:112] Iteration 106350, lr = 0.01
I0523 00:59:17.656026 35003 solver.cpp:239] Iteration 106360 (2.07146 iter/s, 4.82752s/10 iters), loss = 7.51491
I0523 00:59:17.656077 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51491 (* 1 = 7.51491 loss)
I0523 00:59:17.665586 35003 sgd_solver.cpp:112] Iteration 106360, lr = 0.01
I0523 00:59:22.251521 35003 solver.cpp:239] Iteration 106370 (2.17616 iter/s, 4.59526s/10 iters), loss = 8.13894
I0523 00:59:22.251790 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.13894 (* 1 = 8.13894 loss)
I0523 00:59:22.264163 35003 sgd_solver.cpp:112] Iteration 106370, lr = 0.01
I0523 00:59:27.060519 35003 solver.cpp:239] Iteration 106380 (2.07962 iter/s, 4.80856s/10 iters), loss = 6.52319
I0523 00:59:27.060566 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52319 (* 1 = 6.52319 loss)
I0523 00:59:27.066459 35003 sgd_solver.cpp:112] Iteration 106380, lr = 0.01
I0523 00:59:29.910761 35003 solver.cpp:239] Iteration 106390 (3.50868 iter/s, 2.85008s/10 iters), loss = 7.46384
I0523 00:59:29.910809 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46384 (* 1 = 7.46384 loss)
I0523 00:59:30.394347 35003 sgd_solver.cpp:112] Iteration 106390, lr = 0.01
I0523 00:59:34.064438 35003 solver.cpp:239] Iteration 106400 (2.40763 iter/s, 4.15346s/10 iters), loss = 6.56186
I0523 00:59:34.064494 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56186 (* 1 = 6.56186 loss)
I0523 00:59:34.078037 35003 sgd_solver.cpp:112] Iteration 106400, lr = 0.01
I0523 00:59:36.908409 35003 solver.cpp:239] Iteration 106410 (3.51643 iter/s, 2.8438s/10 iters), loss = 8.75463
I0523 00:59:36.908449 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.75463 (* 1 = 8.75463 loss)
I0523 00:59:37.610875 35003 sgd_solver.cpp:112] Iteration 106410, lr = 0.01
I0523 00:59:39.903584 35003 solver.cpp:239] Iteration 106420 (3.33889 iter/s, 2.99501s/10 iters), loss = 8.49939
I0523 00:59:39.903627 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.49939 (* 1 = 8.49939 loss)
I0523 00:59:40.514446 35003 sgd_solver.cpp:112] Iteration 106420, lr = 0.01
I0523 00:59:43.272135 35003 solver.cpp:239] Iteration 106430 (2.9688 iter/s, 3.36837s/10 iters), loss = 7.66699
I0523 00:59:43.272176 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66699 (* 1 = 7.66699 loss)
I0523 00:59:43.280333 35003 sgd_solver.cpp:112] Iteration 106430, lr = 0.01
I0523 00:59:47.409034 35003 solver.cpp:239] Iteration 106440 (2.41739 iter/s, 4.13668s/10 iters), loss = 7.29015
I0523 00:59:47.409083 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29015 (* 1 = 7.29015 loss)
I0523 00:59:47.419168 35003 sgd_solver.cpp:112] Iteration 106440, lr = 0.01
I0523 00:59:49.667544 35003 solver.cpp:239] Iteration 106450 (4.42798 iter/s, 2.25836s/10 iters), loss = 6.94806
I0523 00:59:49.667588 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94806 (* 1 = 6.94806 loss)
I0523 00:59:50.316107 35003 sgd_solver.cpp:112] Iteration 106450, lr = 0.01
I0523 00:59:53.984676 35003 solver.cpp:239] Iteration 106460 (2.31647 iter/s, 4.31691s/10 iters), loss = 6.33176
I0523 00:59:53.984844 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33176 (* 1 = 6.33176 loss)
I0523 00:59:53.995970 35003 sgd_solver.cpp:112] Iteration 106460, lr = 0.01
I0523 00:59:56.826645 35003 solver.cpp:239] Iteration 106470 (3.51904 iter/s, 2.84169s/10 iters), loss = 6.58381
I0523 00:59:56.826679 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58381 (* 1 = 6.58381 loss)
I0523 00:59:56.839985 35003 sgd_solver.cpp:112] Iteration 106470, lr = 0.01
I0523 00:59:59.648933 35003 solver.cpp:239] Iteration 106480 (3.54343 iter/s, 2.82213s/10 iters), loss = 7.96541
I0523 00:59:59.648980 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96541 (* 1 = 7.96541 loss)
I0523 00:59:59.743429 35003 sgd_solver.cpp:112] Iteration 106480, lr = 0.01
I0523 01:00:03.982244 35003 solver.cpp:239] Iteration 106490 (2.30782 iter/s, 4.33309s/10 iters), loss = 6.84001
I0523 01:00:03.982293 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84001 (* 1 = 6.84001 loss)
I0523 01:00:04.007489 35003 sgd_solver.cpp:112] Iteration 106490, lr = 0.01
I0523 01:00:06.608850 35003 solver.cpp:239] Iteration 106500 (3.80744 iter/s, 2.62644s/10 iters), loss = 6.86733
I0523 01:00:06.608909 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86733 (* 1 = 6.86733 loss)
I0523 01:00:06.628551 35003 sgd_solver.cpp:112] Iteration 106500, lr = 0.01
I0523 01:00:09.409430 35003 solver.cpp:239] Iteration 106510 (3.57091 iter/s, 2.80041s/10 iters), loss = 7.13689
I0523 01:00:09.409466 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13689 (* 1 = 7.13689 loss)
I0523 01:00:09.434267 35003 sgd_solver.cpp:112] Iteration 106510, lr = 0.01
I0523 01:00:13.092010 35003 solver.cpp:239] Iteration 106520 (2.71563 iter/s, 3.68238s/10 iters), loss = 7.27428
I0523 01:00:13.092064 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27428 (* 1 = 7.27428 loss)
I0523 01:00:13.686161 35003 sgd_solver.cpp:112] Iteration 106520, lr = 0.01
I0523 01:00:17.329478 35003 solver.cpp:239] Iteration 106530 (2.36003 iter/s, 4.23724s/10 iters), loss = 5.41337
I0523 01:00:17.329530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.41337 (* 1 = 5.41337 loss)
I0523 01:00:18.021904 35003 sgd_solver.cpp:112] Iteration 106530, lr = 0.01
I0523 01:00:20.894582 35003 solver.cpp:239] Iteration 106540 (2.80512 iter/s, 3.56491s/10 iters), loss = 8.373
I0523 01:00:20.894624 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.373 (* 1 = 8.373 loss)
I0523 01:00:21.632563 35003 sgd_solver.cpp:112] Iteration 106540, lr = 0.01
I0523 01:00:25.791484 35003 solver.cpp:239] Iteration 106550 (2.04221 iter/s, 4.89666s/10 iters), loss = 7.53609
I0523 01:00:25.791672 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53609 (* 1 = 7.53609 loss)
I0523 01:00:25.800933 35003 sgd_solver.cpp:112] Iteration 106550, lr = 0.01
I0523 01:00:30.232373 35003 solver.cpp:239] Iteration 106560 (2.25199 iter/s, 4.44052s/10 iters), loss = 8.38095
I0523 01:00:30.232427 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.38095 (* 1 = 8.38095 loss)
I0523 01:00:30.973388 35003 sgd_solver.cpp:112] Iteration 106560, lr = 0.01
I0523 01:00:33.751684 35003 solver.cpp:239] Iteration 106570 (2.84164 iter/s, 3.5191s/10 iters), loss = 8.02588
I0523 01:00:33.751730 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02588 (* 1 = 8.02588 loss)
I0523 01:00:33.755502 35003 sgd_solver.cpp:112] Iteration 106570, lr = 0.01
I0523 01:00:35.857249 35003 solver.cpp:239] Iteration 106580 (4.74964 iter/s, 2.10542s/10 iters), loss = 6.93219
I0523 01:00:35.857303 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93219 (* 1 = 6.93219 loss)
I0523 01:00:35.870883 35003 sgd_solver.cpp:112] Iteration 106580, lr = 0.01
I0523 01:00:38.532490 35003 solver.cpp:239] Iteration 106590 (3.73821 iter/s, 2.67507s/10 iters), loss = 7.76734
I0523 01:00:38.532537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76734 (* 1 = 7.76734 loss)
I0523 01:00:38.554713 35003 sgd_solver.cpp:112] Iteration 106590, lr = 0.01
I0523 01:00:41.864482 35003 solver.cpp:239] Iteration 106600 (3.00137 iter/s, 3.33181s/10 iters), loss = 6.41931
I0523 01:00:41.864526 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41931 (* 1 = 6.41931 loss)
I0523 01:00:42.578922 35003 sgd_solver.cpp:112] Iteration 106600, lr = 0.01
I0523 01:00:44.720202 35003 solver.cpp:239] Iteration 106610 (3.50195 iter/s, 2.85556s/10 iters), loss = 7.4151
I0523 01:00:44.720243 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4151 (* 1 = 7.4151 loss)
I0523 01:00:44.723311 35003 sgd_solver.cpp:112] Iteration 106610, lr = 0.01
I0523 01:00:47.036432 35003 solver.cpp:239] Iteration 106620 (4.31764 iter/s, 2.31608s/10 iters), loss = 7.45171
I0523 01:00:47.036473 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45171 (* 1 = 7.45171 loss)
I0523 01:00:47.049391 35003 sgd_solver.cpp:112] Iteration 106620, lr = 0.01
I0523 01:00:49.881727 35003 solver.cpp:239] Iteration 106630 (3.51479 iter/s, 2.84512s/10 iters), loss = 7.44095
I0523 01:00:49.881774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44095 (* 1 = 7.44095 loss)
I0523 01:00:49.895192 35003 sgd_solver.cpp:112] Iteration 106630, lr = 0.01
I0523 01:00:52.459089 35003 solver.cpp:239] Iteration 106640 (3.88018 iter/s, 2.5772s/10 iters), loss = 7.62964
I0523 01:00:52.459143 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62964 (* 1 = 7.62964 loss)
I0523 01:00:53.198307 35003 sgd_solver.cpp:112] Iteration 106640, lr = 0.01
I0523 01:00:56.795894 35003 solver.cpp:239] Iteration 106650 (2.30597 iter/s, 4.33658s/10 iters), loss = 5.95479
I0523 01:00:56.796115 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95479 (* 1 = 5.95479 loss)
I0523 01:00:57.524538 35003 sgd_solver.cpp:112] Iteration 106650, lr = 0.01
I0523 01:01:01.071801 35003 solver.cpp:239] Iteration 106660 (2.33889 iter/s, 4.27554s/10 iters), loss = 6.81727
I0523 01:01:01.071840 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81727 (* 1 = 6.81727 loss)
I0523 01:01:01.788695 35003 sgd_solver.cpp:112] Iteration 106660, lr = 0.01
I0523 01:01:03.975908 35003 solver.cpp:239] Iteration 106670 (3.44359 iter/s, 2.90395s/10 iters), loss = 7.46569
I0523 01:01:03.975945 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46569 (* 1 = 7.46569 loss)
I0523 01:01:04.674065 35003 sgd_solver.cpp:112] Iteration 106670, lr = 0.01
I0523 01:01:08.396399 35003 solver.cpp:239] Iteration 106680 (2.26231 iter/s, 4.42026s/10 iters), loss = 6.60163
I0523 01:01:08.396459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60163 (* 1 = 6.60163 loss)
I0523 01:01:08.462859 35003 sgd_solver.cpp:112] Iteration 106680, lr = 0.01
I0523 01:01:11.075462 35003 solver.cpp:239] Iteration 106690 (3.73288 iter/s, 2.67889s/10 iters), loss = 7.68482
I0523 01:01:11.075505 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68482 (* 1 = 7.68482 loss)
I0523 01:01:11.814537 35003 sgd_solver.cpp:112] Iteration 106690, lr = 0.01
I0523 01:01:14.405339 35003 solver.cpp:239] Iteration 106700 (3.00329 iter/s, 3.32969s/10 iters), loss = 7.03263
I0523 01:01:14.405396 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03263 (* 1 = 7.03263 loss)
I0523 01:01:14.713737 35003 sgd_solver.cpp:112] Iteration 106700, lr = 0.01
I0523 01:01:18.125892 35003 solver.cpp:239] Iteration 106710 (2.68792 iter/s, 3.72035s/10 iters), loss = 7.12372
I0523 01:01:18.125931 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12372 (* 1 = 7.12372 loss)
I0523 01:01:18.139873 35003 sgd_solver.cpp:112] Iteration 106710, lr = 0.01
I0523 01:01:21.059716 35003 solver.cpp:239] Iteration 106720 (3.40871 iter/s, 2.93366s/10 iters), loss = 7.14608
I0523 01:01:21.059756 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14608 (* 1 = 7.14608 loss)
I0523 01:01:21.063021 35003 sgd_solver.cpp:112] Iteration 106720, lr = 0.01
I0523 01:01:24.749336 35003 solver.cpp:239] Iteration 106730 (2.71046 iter/s, 3.68942s/10 iters), loss = 7.02906
I0523 01:01:24.749379 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02906 (* 1 = 7.02906 loss)
I0523 01:01:24.762619 35003 sgd_solver.cpp:112] Iteration 106730, lr = 0.01
I0523 01:01:30.021073 35003 solver.cpp:239] Iteration 106740 (1.897 iter/s, 5.27148s/10 iters), loss = 7.86435
I0523 01:01:30.021201 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86435 (* 1 = 7.86435 loss)
I0523 01:01:30.737133 35003 sgd_solver.cpp:112] Iteration 106740, lr = 0.01
I0523 01:01:32.762442 35003 solver.cpp:239] Iteration 106750 (3.64814 iter/s, 2.74112s/10 iters), loss = 6.73792
I0523 01:01:32.762480 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73792 (* 1 = 6.73792 loss)
I0523 01:01:32.776252 35003 sgd_solver.cpp:112] Iteration 106750, lr = 0.01
I0523 01:01:34.898768 35003 solver.cpp:239] Iteration 106760 (4.68122 iter/s, 2.13619s/10 iters), loss = 6.41608
I0523 01:01:34.898815 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41608 (* 1 = 6.41608 loss)
I0523 01:01:35.627317 35003 sgd_solver.cpp:112] Iteration 106760, lr = 0.01
I0523 01:01:38.649410 35003 solver.cpp:239] Iteration 106770 (2.66636 iter/s, 3.75043s/10 iters), loss = 6.72269
I0523 01:01:38.649453 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72269 (* 1 = 6.72269 loss)
I0523 01:01:38.906236 35003 sgd_solver.cpp:112] Iteration 106770, lr = 0.01
I0523 01:01:41.557327 35003 solver.cpp:239] Iteration 106780 (3.4391 iter/s, 2.90774s/10 iters), loss = 6.42103
I0523 01:01:41.557371 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42103 (* 1 = 6.42103 loss)
I0523 01:01:41.561452 35003 sgd_solver.cpp:112] Iteration 106780, lr = 0.01
I0523 01:01:44.988081 35003 solver.cpp:239] Iteration 106790 (2.91499 iter/s, 3.43054s/10 iters), loss = 7.60593
I0523 01:01:44.988128 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60593 (* 1 = 7.60593 loss)
I0523 01:01:45.001184 35003 sgd_solver.cpp:112] Iteration 106790, lr = 0.01
I0523 01:01:48.496704 35003 solver.cpp:239] Iteration 106800 (2.85028 iter/s, 3.50843s/10 iters), loss = 8.47803
I0523 01:01:48.496753 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.47803 (* 1 = 8.47803 loss)
I0523 01:01:48.506140 35003 sgd_solver.cpp:112] Iteration 106800, lr = 0.01
I0523 01:01:51.769228 35003 solver.cpp:239] Iteration 106810 (3.05592 iter/s, 3.27234s/10 iters), loss = 7.75472
I0523 01:01:51.769270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75472 (* 1 = 7.75472 loss)
I0523 01:01:52.503775 35003 sgd_solver.cpp:112] Iteration 106810, lr = 0.01
I0523 01:01:55.545248 35003 solver.cpp:239] Iteration 106820 (2.64843 iter/s, 3.77582s/10 iters), loss = 7.92588
I0523 01:01:55.545286 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.92588 (* 1 = 7.92588 loss)
I0523 01:01:55.559228 35003 sgd_solver.cpp:112] Iteration 106820, lr = 0.01
I0523 01:01:59.200816 35003 solver.cpp:239] Iteration 106830 (2.73571 iter/s, 3.65536s/10 iters), loss = 7.8852
I0523 01:01:59.200881 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8852 (* 1 = 7.8852 loss)
I0523 01:01:59.889701 35003 sgd_solver.cpp:112] Iteration 106830, lr = 0.01
I0523 01:02:03.493983 35003 solver.cpp:239] Iteration 106840 (2.32941 iter/s, 4.29293s/10 iters), loss = 7.1233
I0523 01:02:03.494190 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1233 (* 1 = 7.1233 loss)
I0523 01:02:04.234701 35003 sgd_solver.cpp:112] Iteration 106840, lr = 0.01
I0523 01:02:08.538484 35003 solver.cpp:239] Iteration 106850 (1.98252 iter/s, 5.0441s/10 iters), loss = 7.57183
I0523 01:02:08.538529 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57183 (* 1 = 7.57183 loss)
I0523 01:02:09.269830 35003 sgd_solver.cpp:112] Iteration 106850, lr = 0.01
I0523 01:02:11.315469 35003 solver.cpp:239] Iteration 106860 (3.60125 iter/s, 2.77682s/10 iters), loss = 7.0854
I0523 01:02:11.315521 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0854 (* 1 = 7.0854 loss)
I0523 01:02:11.723855 35003 sgd_solver.cpp:112] Iteration 106860, lr = 0.01
I0523 01:02:17.041823 35003 solver.cpp:239] Iteration 106870 (1.74641 iter/s, 5.72605s/10 iters), loss = 6.96483
I0523 01:02:17.041884 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96483 (* 1 = 6.96483 loss)
I0523 01:02:17.054563 35003 sgd_solver.cpp:112] Iteration 106870, lr = 0.01
I0523 01:02:20.637405 35003 solver.cpp:239] Iteration 106880 (2.78135 iter/s, 3.59537s/10 iters), loss = 7.49531
I0523 01:02:20.637444 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49531 (* 1 = 7.49531 loss)
I0523 01:02:20.650822 35003 sgd_solver.cpp:112] Iteration 106880, lr = 0.01
I0523 01:02:24.285341 35003 solver.cpp:239] Iteration 106890 (2.74142 iter/s, 3.64774s/10 iters), loss = 7.61795
I0523 01:02:24.285408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61795 (* 1 = 7.61795 loss)
I0523 01:02:25.012686 35003 sgd_solver.cpp:112] Iteration 106890, lr = 0.01
I0523 01:02:27.679085 35003 solver.cpp:239] Iteration 106900 (2.94679 iter/s, 3.39352s/10 iters), loss = 7.50321
I0523 01:02:27.679149 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50321 (* 1 = 7.50321 loss)
I0523 01:02:28.380779 35003 sgd_solver.cpp:112] Iteration 106900, lr = 0.01
I0523 01:02:32.397234 35003 solver.cpp:239] Iteration 106910 (2.11959 iter/s, 4.7179s/10 iters), loss = 7.53344
I0523 01:02:32.397292 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53344 (* 1 = 7.53344 loss)
I0523 01:02:32.415526 35003 sgd_solver.cpp:112] Iteration 106910, lr = 0.01
I0523 01:02:37.318541 35003 solver.cpp:239] Iteration 106920 (2.03209 iter/s, 4.92105s/10 iters), loss = 7.86065
I0523 01:02:37.318789 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86065 (* 1 = 7.86065 loss)
I0523 01:02:38.059478 35003 sgd_solver.cpp:112] Iteration 106920, lr = 0.01
I0523 01:02:40.143204 35003 solver.cpp:239] Iteration 106930 (3.54067 iter/s, 2.82432s/10 iters), loss = 7.21453
I0523 01:02:40.143265 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21453 (* 1 = 7.21453 loss)
I0523 01:02:40.880290 35003 sgd_solver.cpp:112] Iteration 106930, lr = 0.01
I0523 01:02:43.620209 35003 solver.cpp:239] Iteration 106940 (2.8762 iter/s, 3.4768s/10 iters), loss = 6.68029
I0523 01:02:43.620249 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68029 (* 1 = 6.68029 loss)
I0523 01:02:43.632217 35003 sgd_solver.cpp:112] Iteration 106940, lr = 0.01
I0523 01:02:45.788223 35003 solver.cpp:239] Iteration 106950 (4.61282 iter/s, 2.16787s/10 iters), loss = 7.42411
I0523 01:02:45.788264 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42411 (* 1 = 7.42411 loss)
I0523 01:02:45.801476 35003 sgd_solver.cpp:112] Iteration 106950, lr = 0.01
I0523 01:02:49.979233 35003 solver.cpp:239] Iteration 106960 (2.38618 iter/s, 4.1908s/10 iters), loss = 8.11259
I0523 01:02:49.979275 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.11259 (* 1 = 8.11259 loss)
I0523 01:02:49.988538 35003 sgd_solver.cpp:112] Iteration 106960, lr = 0.01
I0523 01:02:52.811992 35003 solver.cpp:239] Iteration 106970 (3.53032 iter/s, 2.8326s/10 iters), loss = 7.58647
I0523 01:02:52.812036 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58647 (* 1 = 7.58647 loss)
I0523 01:02:53.544548 35003 sgd_solver.cpp:112] Iteration 106970, lr = 0.01
I0523 01:02:56.397274 35003 solver.cpp:239] Iteration 106980 (2.78933 iter/s, 3.58509s/10 iters), loss = 8.63763
I0523 01:02:56.397312 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.63763 (* 1 = 8.63763 loss)
I0523 01:02:56.415351 35003 sgd_solver.cpp:112] Iteration 106980, lr = 0.01
I0523 01:03:00.692456 35003 solver.cpp:239] Iteration 106990 (2.32831 iter/s, 4.29496s/10 iters), loss = 8.56128
I0523 01:03:00.692502 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.56128 (* 1 = 8.56128 loss)
I0523 01:03:00.698200 35003 sgd_solver.cpp:112] Iteration 106990, lr = 0.01
I0523 01:03:03.709579 35003 solver.cpp:239] Iteration 107000 (3.3146 iter/s, 3.01695s/10 iters), loss = 7.65595
I0523 01:03:03.709625 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65595 (* 1 = 7.65595 loss)
I0523 01:03:04.343068 35003 sgd_solver.cpp:112] Iteration 107000, lr = 0.01
I0523 01:03:09.211087 35003 solver.cpp:239] Iteration 107010 (1.81777 iter/s, 5.50124s/10 iters), loss = 7.21031
I0523 01:03:09.211381 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21031 (* 1 = 7.21031 loss)
I0523 01:03:09.216779 35003 sgd_solver.cpp:112] Iteration 107010, lr = 0.01
I0523 01:03:12.090342 35003 solver.cpp:239] Iteration 107020 (3.47358 iter/s, 2.87887s/10 iters), loss = 7.13524
I0523 01:03:12.090382 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13524 (* 1 = 7.13524 loss)
I0523 01:03:12.108654 35003 sgd_solver.cpp:112] Iteration 107020, lr = 0.01
I0523 01:03:16.367491 35003 solver.cpp:239] Iteration 107030 (2.33813 iter/s, 4.27693s/10 iters), loss = 7.80284
I0523 01:03:16.367544 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80284 (* 1 = 7.80284 loss)
I0523 01:03:17.075779 35003 sgd_solver.cpp:112] Iteration 107030, lr = 0.01
I0523 01:03:19.859992 35003 solver.cpp:239] Iteration 107040 (2.86344 iter/s, 3.4923s/10 iters), loss = 5.5893
I0523 01:03:19.860031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.5893 (* 1 = 5.5893 loss)
I0523 01:03:19.873421 35003 sgd_solver.cpp:112] Iteration 107040, lr = 0.01
I0523 01:03:23.994681 35003 solver.cpp:239] Iteration 107050 (2.41869 iter/s, 4.13447s/10 iters), loss = 6.57713
I0523 01:03:23.994767 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57713 (* 1 = 6.57713 loss)
I0523 01:03:24.000114 35003 sgd_solver.cpp:112] Iteration 107050, lr = 0.01
I0523 01:03:26.755431 35003 solver.cpp:239] Iteration 107060 (3.62248 iter/s, 2.76054s/10 iters), loss = 7.01277
I0523 01:03:26.755489 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01277 (* 1 = 7.01277 loss)
I0523 01:03:26.756443 35003 sgd_solver.cpp:112] Iteration 107060, lr = 0.01
I0523 01:03:29.677078 35003 solver.cpp:239] Iteration 107070 (3.42294 iter/s, 2.92146s/10 iters), loss = 7.35516
I0523 01:03:29.677129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35516 (* 1 = 7.35516 loss)
I0523 01:03:29.690659 35003 sgd_solver.cpp:112] Iteration 107070, lr = 0.01
I0523 01:03:32.501937 35003 solver.cpp:239] Iteration 107080 (3.54021 iter/s, 2.82469s/10 iters), loss = 6.82894
I0523 01:03:32.501978 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82894 (* 1 = 6.82894 loss)
I0523 01:03:33.223563 35003 sgd_solver.cpp:112] Iteration 107080, lr = 0.01
I0523 01:03:35.993314 35003 solver.cpp:239] Iteration 107090 (2.86435 iter/s, 3.49119s/10 iters), loss = 6.01421
I0523 01:03:35.993355 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01421 (* 1 = 6.01421 loss)
I0523 01:03:36.013911 35003 sgd_solver.cpp:112] Iteration 107090, lr = 0.01
I0523 01:03:40.291003 35003 solver.cpp:239] Iteration 107100 (2.32695 iter/s, 4.29746s/10 iters), loss = 7.30795
I0523 01:03:40.291245 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30795 (* 1 = 7.30795 loss)
I0523 01:03:40.298808 35003 sgd_solver.cpp:112] Iteration 107100, lr = 0.01
I0523 01:03:43.886261 35003 solver.cpp:239] Iteration 107110 (2.78174 iter/s, 3.59487s/10 iters), loss = 7.18973
I0523 01:03:43.886301 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18973 (* 1 = 7.18973 loss)
I0523 01:03:43.905315 35003 sgd_solver.cpp:112] Iteration 107110, lr = 0.01
I0523 01:03:46.356660 35003 solver.cpp:239] Iteration 107120 (4.04818 iter/s, 2.47025s/10 iters), loss = 7.34179
I0523 01:03:46.356700 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34179 (* 1 = 7.34179 loss)
I0523 01:03:46.367614 35003 sgd_solver.cpp:112] Iteration 107120, lr = 0.01
I0523 01:03:49.213114 35003 solver.cpp:239] Iteration 107130 (3.50106 iter/s, 2.85627s/10 iters), loss = 7.23479
I0523 01:03:49.213166 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23479 (* 1 = 7.23479 loss)
I0523 01:03:49.951457 35003 sgd_solver.cpp:112] Iteration 107130, lr = 0.01
I0523 01:03:53.421722 35003 solver.cpp:239] Iteration 107140 (2.37621 iter/s, 4.20838s/10 iters), loss = 7.49905
I0523 01:03:53.421777 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49905 (* 1 = 7.49905 loss)
I0523 01:03:54.155747 35003 sgd_solver.cpp:112] Iteration 107140, lr = 0.01
I0523 01:03:57.307540 35003 solver.cpp:239] Iteration 107150 (2.57361 iter/s, 3.8856s/10 iters), loss = 6.45345
I0523 01:03:57.307579 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45345 (* 1 = 6.45345 loss)
I0523 01:03:57.333223 35003 sgd_solver.cpp:112] Iteration 107150, lr = 0.01
I0523 01:04:00.129248 35003 solver.cpp:239] Iteration 107160 (3.54415 iter/s, 2.82155s/10 iters), loss = 6.71787
I0523 01:04:00.129284 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71787 (* 1 = 6.71787 loss)
I0523 01:04:00.837568 35003 sgd_solver.cpp:112] Iteration 107160, lr = 0.01
I0523 01:04:04.297930 35003 solver.cpp:239] Iteration 107170 (2.39896 iter/s, 4.16847s/10 iters), loss = 6.74865
I0523 01:04:04.297991 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74865 (* 1 = 6.74865 loss)
I0523 01:04:04.310850 35003 sgd_solver.cpp:112] Iteration 107170, lr = 0.01
I0523 01:04:08.901545 35003 solver.cpp:239] Iteration 107180 (2.17233 iter/s, 4.60334s/10 iters), loss = 6.39259
I0523 01:04:08.901612 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39259 (* 1 = 6.39259 loss)
I0523 01:04:09.614537 35003 sgd_solver.cpp:112] Iteration 107180, lr = 0.01
I0523 01:04:13.857211 35003 solver.cpp:239] Iteration 107190 (2.01801 iter/s, 4.95539s/10 iters), loss = 6.89708
I0523 01:04:13.857380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89708 (* 1 = 6.89708 loss)
I0523 01:04:13.871332 35003 sgd_solver.cpp:112] Iteration 107190, lr = 0.01
I0523 01:04:17.476315 35003 solver.cpp:239] Iteration 107200 (2.76335 iter/s, 3.61879s/10 iters), loss = 7.81246
I0523 01:04:17.476356 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81246 (* 1 = 7.81246 loss)
I0523 01:04:18.204341 35003 sgd_solver.cpp:112] Iteration 107200, lr = 0.01
I0523 01:04:21.034063 35003 solver.cpp:239] Iteration 107210 (2.81091 iter/s, 3.55756s/10 iters), loss = 7.57744
I0523 01:04:21.034102 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57744 (* 1 = 7.57744 loss)
I0523 01:04:21.040959 35003 sgd_solver.cpp:112] Iteration 107210, lr = 0.01
I0523 01:04:25.568137 35003 solver.cpp:239] Iteration 107220 (2.20563 iter/s, 4.53385s/10 iters), loss = 7.35039
I0523 01:04:25.568186 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35039 (* 1 = 7.35039 loss)
I0523 01:04:25.580827 35003 sgd_solver.cpp:112] Iteration 107220, lr = 0.01
I0523 01:04:27.994107 35003 solver.cpp:239] Iteration 107230 (4.12232 iter/s, 2.42582s/10 iters), loss = 6.32834
I0523 01:04:27.994151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32834 (* 1 = 6.32834 loss)
I0523 01:04:28.000932 35003 sgd_solver.cpp:112] Iteration 107230, lr = 0.01
I0523 01:04:32.396423 35003 solver.cpp:239] Iteration 107240 (2.27165 iter/s, 4.40209s/10 iters), loss = 6.80101
I0523 01:04:32.396469 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80101 (* 1 = 6.80101 loss)
I0523 01:04:32.410192 35003 sgd_solver.cpp:112] Iteration 107240, lr = 0.01
I0523 01:04:36.020099 35003 solver.cpp:239] Iteration 107250 (2.75978 iter/s, 3.62348s/10 iters), loss = 7.21124
I0523 01:04:36.020143 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21124 (* 1 = 7.21124 loss)
I0523 01:04:36.032783 35003 sgd_solver.cpp:112] Iteration 107250, lr = 0.01
I0523 01:04:40.005548 35003 solver.cpp:239] Iteration 107260 (2.50927 iter/s, 3.98523s/10 iters), loss = 6.97599
I0523 01:04:40.005625 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97599 (* 1 = 6.97599 loss)
I0523 01:04:40.016496 35003 sgd_solver.cpp:112] Iteration 107260, lr = 0.01
I0523 01:04:42.136500 35003 solver.cpp:239] Iteration 107270 (4.6931 iter/s, 2.13079s/10 iters), loss = 7.21937
I0523 01:04:42.136540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21937 (* 1 = 7.21937 loss)
I0523 01:04:42.870651 35003 sgd_solver.cpp:112] Iteration 107270, lr = 0.01
I0523 01:04:45.532213 35003 solver.cpp:239] Iteration 107280 (2.94506 iter/s, 3.39552s/10 iters), loss = 6.97322
I0523 01:04:45.532375 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97322 (* 1 = 6.97322 loss)
I0523 01:04:45.536978 35003 sgd_solver.cpp:112] Iteration 107280, lr = 0.01
I0523 01:04:49.201220 35003 solver.cpp:239] Iteration 107290 (2.72576 iter/s, 3.66869s/10 iters), loss = 6.96587
I0523 01:04:49.201267 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96587 (* 1 = 6.96587 loss)
I0523 01:04:49.255254 35003 sgd_solver.cpp:112] Iteration 107290, lr = 0.01
I0523 01:04:51.737043 35003 solver.cpp:239] Iteration 107300 (3.94373 iter/s, 2.53567s/10 iters), loss = 7.16483
I0523 01:04:51.737087 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16483 (* 1 = 7.16483 loss)
I0523 01:04:52.314158 35003 sgd_solver.cpp:112] Iteration 107300, lr = 0.01
I0523 01:04:56.133314 35003 solver.cpp:239] Iteration 107310 (2.27477 iter/s, 4.39604s/10 iters), loss = 7.62186
I0523 01:04:56.133373 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62186 (* 1 = 7.62186 loss)
I0523 01:04:56.720777 35003 sgd_solver.cpp:112] Iteration 107310, lr = 0.01
I0523 01:04:58.730859 35003 solver.cpp:239] Iteration 107320 (3.85004 iter/s, 2.59738s/10 iters), loss = 7.21887
I0523 01:04:58.730902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21887 (* 1 = 7.21887 loss)
I0523 01:04:59.446087 35003 sgd_solver.cpp:112] Iteration 107320, lr = 0.01
I0523 01:05:02.142783 35003 solver.cpp:239] Iteration 107330 (2.93106 iter/s, 3.41173s/10 iters), loss = 6.78365
I0523 01:05:02.142835 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78365 (* 1 = 6.78365 loss)
I0523 01:05:02.876499 35003 sgd_solver.cpp:112] Iteration 107330, lr = 0.01
I0523 01:05:06.300485 35003 solver.cpp:239] Iteration 107340 (2.40531 iter/s, 4.15747s/10 iters), loss = 7.0834
I0523 01:05:06.300536 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0834 (* 1 = 7.0834 loss)
I0523 01:05:06.312466 35003 sgd_solver.cpp:112] Iteration 107340, lr = 0.01
I0523 01:05:09.881721 35003 solver.cpp:239] Iteration 107350 (2.79592 iter/s, 3.57664s/10 iters), loss = 7.69481
I0523 01:05:09.881763 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69481 (* 1 = 7.69481 loss)
I0523 01:05:09.885931 35003 sgd_solver.cpp:112] Iteration 107350, lr = 0.01
I0523 01:05:12.062834 35003 solver.cpp:239] Iteration 107360 (4.58517 iter/s, 2.18094s/10 iters), loss = 6.82236
I0523 01:05:12.062925 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82236 (* 1 = 6.82236 loss)
I0523 01:05:12.803138 35003 sgd_solver.cpp:112] Iteration 107360, lr = 0.01
I0523 01:05:15.581198 35003 solver.cpp:239] Iteration 107370 (2.84242 iter/s, 3.51813s/10 iters), loss = 6.53732
I0523 01:05:15.581429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53732 (* 1 = 6.53732 loss)
I0523 01:05:15.594221 35003 sgd_solver.cpp:112] Iteration 107370, lr = 0.01
I0523 01:05:19.762133 35003 solver.cpp:239] Iteration 107380 (2.39202 iter/s, 4.18056s/10 iters), loss = 7.60419
I0523 01:05:19.762183 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60419 (* 1 = 7.60419 loss)
I0523 01:05:19.775095 35003 sgd_solver.cpp:112] Iteration 107380, lr = 0.01
I0523 01:05:23.891919 35003 solver.cpp:239] Iteration 107390 (2.42156 iter/s, 4.12957s/10 iters), loss = 7.47291
I0523 01:05:23.891957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47291 (* 1 = 7.47291 loss)
I0523 01:05:23.897658 35003 sgd_solver.cpp:112] Iteration 107390, lr = 0.01
I0523 01:05:28.314990 35003 solver.cpp:239] Iteration 107400 (2.26099 iter/s, 4.42285s/10 iters), loss = 7.40674
I0523 01:05:28.315043 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40674 (* 1 = 7.40674 loss)
I0523 01:05:28.320050 35003 sgd_solver.cpp:112] Iteration 107400, lr = 0.01
I0523 01:05:32.588819 35003 solver.cpp:239] Iteration 107410 (2.33995 iter/s, 4.2736s/10 iters), loss = 6.60163
I0523 01:05:32.588865 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60163 (* 1 = 6.60163 loss)
I0523 01:05:32.594074 35003 sgd_solver.cpp:112] Iteration 107410, lr = 0.01
I0523 01:05:35.419483 35003 solver.cpp:239] Iteration 107420 (3.53297 iter/s, 2.83048s/10 iters), loss = 7.59527
I0523 01:05:35.419536 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59527 (* 1 = 7.59527 loss)
I0523 01:05:35.569773 35003 sgd_solver.cpp:112] Iteration 107420, lr = 0.01
I0523 01:05:38.238765 35003 solver.cpp:239] Iteration 107430 (3.54721 iter/s, 2.81911s/10 iters), loss = 5.61647
I0523 01:05:38.238808 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.61647 (* 1 = 5.61647 loss)
I0523 01:05:38.979388 35003 sgd_solver.cpp:112] Iteration 107430, lr = 0.01
I0523 01:05:43.302940 35003 solver.cpp:239] Iteration 107440 (1.97475 iter/s, 5.06392s/10 iters), loss = 6.75188
I0523 01:05:43.302996 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75188 (* 1 = 6.75188 loss)
I0523 01:05:43.351517 35003 sgd_solver.cpp:112] Iteration 107440, lr = 0.01
I0523 01:05:45.938963 35003 solver.cpp:239] Iteration 107450 (3.79384 iter/s, 2.63585s/10 iters), loss = 6.30836
I0523 01:05:45.939101 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30836 (* 1 = 6.30836 loss)
I0523 01:05:45.946835 35003 sgd_solver.cpp:112] Iteration 107450, lr = 0.01
I0523 01:05:48.757506 35003 solver.cpp:239] Iteration 107460 (3.54827 iter/s, 2.81828s/10 iters), loss = 6.98988
I0523 01:05:48.757560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98988 (* 1 = 6.98988 loss)
I0523 01:05:48.770042 35003 sgd_solver.cpp:112] Iteration 107460, lr = 0.01
I0523 01:05:53.633747 35003 solver.cpp:239] Iteration 107470 (2.05087 iter/s, 4.87599s/10 iters), loss = 7.52478
I0523 01:05:53.633800 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52478 (* 1 = 7.52478 loss)
I0523 01:05:53.663065 35003 sgd_solver.cpp:112] Iteration 107470, lr = 0.01
I0523 01:05:58.184550 35003 solver.cpp:239] Iteration 107480 (2.19753 iter/s, 4.55057s/10 iters), loss = 9.39912
I0523 01:05:58.184587 35003 solver.cpp:258]     Train net output #0: softmax_loss = 9.39912 (* 1 = 9.39912 loss)
I0523 01:05:58.227437 35003 sgd_solver.cpp:112] Iteration 107480, lr = 0.01
I0523 01:06:02.514461 35003 solver.cpp:239] Iteration 107490 (2.30963 iter/s, 4.32969s/10 iters), loss = 6.73472
I0523 01:06:02.514508 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73472 (* 1 = 6.73472 loss)
I0523 01:06:02.522523 35003 sgd_solver.cpp:112] Iteration 107490, lr = 0.01
I0523 01:06:05.839573 35003 solver.cpp:239] Iteration 107500 (3.00759 iter/s, 3.32493s/10 iters), loss = 7.3461
I0523 01:06:05.839619 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3461 (* 1 = 7.3461 loss)
I0523 01:06:06.535044 35003 sgd_solver.cpp:112] Iteration 107500, lr = 0.01
I0523 01:06:07.900142 35003 solver.cpp:239] Iteration 107510 (4.85334 iter/s, 2.06044s/10 iters), loss = 6.50212
I0523 01:06:07.900182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50212 (* 1 = 6.50212 loss)
I0523 01:06:08.625303 35003 sgd_solver.cpp:112] Iteration 107510, lr = 0.01
I0523 01:06:10.691274 35003 solver.cpp:239] Iteration 107520 (3.58298 iter/s, 2.79097s/10 iters), loss = 6.6302
I0523 01:06:10.691318 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6302 (* 1 = 6.6302 loss)
I0523 01:06:11.429917 35003 sgd_solver.cpp:112] Iteration 107520, lr = 0.01
I0523 01:06:14.183171 35003 solver.cpp:239] Iteration 107530 (2.86393 iter/s, 3.49171s/10 iters), loss = 8.56644
I0523 01:06:14.183209 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.56644 (* 1 = 8.56644 loss)
I0523 01:06:14.186966 35003 sgd_solver.cpp:112] Iteration 107530, lr = 0.01
I0523 01:06:17.884795 35003 solver.cpp:239] Iteration 107540 (2.70166 iter/s, 3.70143s/10 iters), loss = 6.65891
I0523 01:06:17.885040 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65891 (* 1 = 6.65891 loss)
I0523 01:06:17.911528 35003 sgd_solver.cpp:112] Iteration 107540, lr = 0.01
I0523 01:06:21.935401 35003 solver.cpp:239] Iteration 107550 (2.469 iter/s, 4.05022s/10 iters), loss = 7.19503
I0523 01:06:21.935456 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19503 (* 1 = 7.19503 loss)
I0523 01:06:22.599325 35003 sgd_solver.cpp:112] Iteration 107550, lr = 0.01
I0523 01:06:25.995748 35003 solver.cpp:239] Iteration 107560 (2.46298 iter/s, 4.06013s/10 iters), loss = 6.93869
I0523 01:06:25.995786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93869 (* 1 = 6.93869 loss)
I0523 01:06:26.000347 35003 sgd_solver.cpp:112] Iteration 107560, lr = 0.01
I0523 01:06:29.874325 35003 solver.cpp:239] Iteration 107570 (2.5784 iter/s, 3.87838s/10 iters), loss = 6.86627
I0523 01:06:29.874387 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86627 (* 1 = 6.86627 loss)
I0523 01:06:30.418800 35003 sgd_solver.cpp:112] Iteration 107570, lr = 0.01
I0523 01:06:33.465836 35003 solver.cpp:239] Iteration 107580 (2.78451 iter/s, 3.59129s/10 iters), loss = 6.87325
I0523 01:06:33.465894 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87325 (* 1 = 6.87325 loss)
I0523 01:06:33.474133 35003 sgd_solver.cpp:112] Iteration 107580, lr = 0.01
I0523 01:06:37.865795 35003 solver.cpp:239] Iteration 107590 (2.27287 iter/s, 4.39972s/10 iters), loss = 6.82471
I0523 01:06:37.865837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82471 (* 1 = 6.82471 loss)
I0523 01:06:37.869730 35003 sgd_solver.cpp:112] Iteration 107590, lr = 0.01
I0523 01:06:41.321943 35003 solver.cpp:239] Iteration 107600 (2.89355 iter/s, 3.45596s/10 iters), loss = 8.08303
I0523 01:06:41.321987 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08303 (* 1 = 8.08303 loss)
I0523 01:06:41.335439 35003 sgd_solver.cpp:112] Iteration 107600, lr = 0.01
I0523 01:06:44.178817 35003 solver.cpp:239] Iteration 107610 (3.50054 iter/s, 2.85671s/10 iters), loss = 6.77584
I0523 01:06:44.178864 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77584 (* 1 = 6.77584 loss)
I0523 01:06:44.861624 35003 sgd_solver.cpp:112] Iteration 107610, lr = 0.01
I0523 01:06:49.238181 35003 solver.cpp:239] Iteration 107620 (1.97664 iter/s, 5.0591s/10 iters), loss = 7.89265
I0523 01:06:49.238318 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89265 (* 1 = 7.89265 loss)
I0523 01:06:49.666606 35003 sgd_solver.cpp:112] Iteration 107620, lr = 0.01
I0523 01:06:52.239492 35003 solver.cpp:239] Iteration 107630 (3.33217 iter/s, 3.00105s/10 iters), loss = 6.04556
I0523 01:06:52.239537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04556 (* 1 = 6.04556 loss)
I0523 01:06:52.246820 35003 sgd_solver.cpp:112] Iteration 107630, lr = 0.01
I0523 01:06:53.624403 35003 solver.cpp:239] Iteration 107640 (7.22124 iter/s, 1.3848s/10 iters), loss = 6.47687
I0523 01:06:53.624454 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47687 (* 1 = 6.47687 loss)
I0523 01:06:54.365845 35003 sgd_solver.cpp:112] Iteration 107640, lr = 0.01
I0523 01:06:57.965059 35003 solver.cpp:239] Iteration 107650 (2.30392 iter/s, 4.34043s/10 iters), loss = 7.00655
I0523 01:06:57.965103 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00655 (* 1 = 7.00655 loss)
I0523 01:06:57.970309 35003 sgd_solver.cpp:112] Iteration 107650, lr = 0.01
I0523 01:07:02.298828 35003 solver.cpp:239] Iteration 107660 (2.30758 iter/s, 4.33355s/10 iters), loss = 7.70724
I0523 01:07:02.298882 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70724 (* 1 = 7.70724 loss)
I0523 01:07:02.312693 35003 sgd_solver.cpp:112] Iteration 107660, lr = 0.01
I0523 01:07:05.890499 35003 solver.cpp:239] Iteration 107670 (2.78438 iter/s, 3.59147s/10 iters), loss = 7.69347
I0523 01:07:05.890540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69347 (* 1 = 7.69347 loss)
I0523 01:07:06.009028 35003 sgd_solver.cpp:112] Iteration 107670, lr = 0.01
I0523 01:07:10.127683 35003 solver.cpp:239] Iteration 107680 (2.36019 iter/s, 4.23695s/10 iters), loss = 7.73826
I0523 01:07:10.127743 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73826 (* 1 = 7.73826 loss)
I0523 01:07:10.848862 35003 sgd_solver.cpp:112] Iteration 107680, lr = 0.01
I0523 01:07:12.888319 35003 solver.cpp:239] Iteration 107690 (3.62262 iter/s, 2.76044s/10 iters), loss = 7.27893
I0523 01:07:12.888357 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27893 (* 1 = 7.27893 loss)
I0523 01:07:12.894662 35003 sgd_solver.cpp:112] Iteration 107690, lr = 0.01
I0523 01:07:16.456509 35003 solver.cpp:239] Iteration 107700 (2.80269 iter/s, 3.56801s/10 iters), loss = 7.90012
I0523 01:07:16.456554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90012 (* 1 = 7.90012 loss)
I0523 01:07:16.494915 35003 sgd_solver.cpp:112] Iteration 107700, lr = 0.01
I0523 01:07:21.315512 35003 solver.cpp:239] Iteration 107710 (2.05814 iter/s, 4.85876s/10 iters), loss = 7.13921
I0523 01:07:21.315799 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13921 (* 1 = 7.13921 loss)
I0523 01:07:21.326124 35003 sgd_solver.cpp:112] Iteration 107710, lr = 0.01
I0523 01:07:24.814232 35003 solver.cpp:239] Iteration 107720 (2.85852 iter/s, 3.49831s/10 iters), loss = 8.35319
I0523 01:07:24.814288 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.35319 (* 1 = 8.35319 loss)
I0523 01:07:24.821117 35003 sgd_solver.cpp:112] Iteration 107720, lr = 0.01
I0523 01:07:27.726225 35003 solver.cpp:239] Iteration 107730 (3.43429 iter/s, 2.91181s/10 iters), loss = 7.68653
I0523 01:07:27.726263 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68653 (* 1 = 7.68653 loss)
I0523 01:07:27.752341 35003 sgd_solver.cpp:112] Iteration 107730, lr = 0.01
I0523 01:07:31.305131 35003 solver.cpp:239] Iteration 107740 (2.79429 iter/s, 3.57872s/10 iters), loss = 7.1862
I0523 01:07:31.305168 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1862 (* 1 = 7.1862 loss)
I0523 01:07:31.319109 35003 sgd_solver.cpp:112] Iteration 107740, lr = 0.01
I0523 01:07:32.608788 35003 solver.cpp:239] Iteration 107750 (7.67133 iter/s, 1.30356s/10 iters), loss = 6.92417
I0523 01:07:32.608837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92417 (* 1 = 6.92417 loss)
I0523 01:07:32.617563 35003 sgd_solver.cpp:112] Iteration 107750, lr = 0.01
I0523 01:07:36.240875 35003 solver.cpp:239] Iteration 107760 (2.75339 iter/s, 3.63188s/10 iters), loss = 8.03239
I0523 01:07:36.240919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03239 (* 1 = 8.03239 loss)
I0523 01:07:36.250458 35003 sgd_solver.cpp:112] Iteration 107760, lr = 0.01
I0523 01:07:39.881990 35003 solver.cpp:239] Iteration 107770 (2.74656 iter/s, 3.64092s/10 iters), loss = 7.18685
I0523 01:07:39.882037 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18685 (* 1 = 7.18685 loss)
I0523 01:07:39.887696 35003 sgd_solver.cpp:112] Iteration 107770, lr = 0.01
I0523 01:07:43.766649 35003 solver.cpp:239] Iteration 107780 (2.57437 iter/s, 3.88445s/10 iters), loss = 7.38174
I0523 01:07:43.766713 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38174 (* 1 = 7.38174 loss)
I0523 01:07:43.778281 35003 sgd_solver.cpp:112] Iteration 107780, lr = 0.01
I0523 01:07:47.115479 35003 solver.cpp:239] Iteration 107790 (2.98628 iter/s, 3.34864s/10 iters), loss = 7.20695
I0523 01:07:47.115527 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20695 (* 1 = 7.20695 loss)
I0523 01:07:47.124912 35003 sgd_solver.cpp:112] Iteration 107790, lr = 0.01
I0523 01:07:49.982165 35003 solver.cpp:239] Iteration 107800 (3.48856 iter/s, 2.86651s/10 iters), loss = 8.19252
I0523 01:07:49.982210 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.19252 (* 1 = 8.19252 loss)
I0523 01:07:50.002419 35003 sgd_solver.cpp:112] Iteration 107800, lr = 0.01
I0523 01:07:53.490945 35003 solver.cpp:239] Iteration 107810 (2.85015 iter/s, 3.50859s/10 iters), loss = 6.91303
I0523 01:07:53.491084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91303 (* 1 = 6.91303 loss)
I0523 01:07:53.503226 35003 sgd_solver.cpp:112] Iteration 107810, lr = 0.01
I0523 01:07:58.126821 35003 solver.cpp:239] Iteration 107820 (2.15724 iter/s, 4.63555s/10 iters), loss = 7.39176
I0523 01:07:58.126869 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39176 (* 1 = 7.39176 loss)
I0523 01:07:58.140218 35003 sgd_solver.cpp:112] Iteration 107820, lr = 0.01
I0523 01:08:01.615373 35003 solver.cpp:239] Iteration 107830 (2.86668 iter/s, 3.48836s/10 iters), loss = 7.48885
I0523 01:08:01.615416 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48885 (* 1 = 7.48885 loss)
I0523 01:08:01.634215 35003 sgd_solver.cpp:112] Iteration 107830, lr = 0.01
I0523 01:08:04.894986 35003 solver.cpp:239] Iteration 107840 (3.04931 iter/s, 3.27943s/10 iters), loss = 6.42068
I0523 01:08:04.895027 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42068 (* 1 = 6.42068 loss)
I0523 01:08:04.898820 35003 sgd_solver.cpp:112] Iteration 107840, lr = 0.01
I0523 01:08:07.605782 35003 solver.cpp:239] Iteration 107850 (3.68919 iter/s, 2.71062s/10 iters), loss = 8.07672
I0523 01:08:07.605844 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.07672 (* 1 = 8.07672 loss)
I0523 01:08:07.619357 35003 sgd_solver.cpp:112] Iteration 107850, lr = 0.01
I0523 01:08:11.687644 35003 solver.cpp:239] Iteration 107860 (2.45 iter/s, 4.08163s/10 iters), loss = 6.75545
I0523 01:08:11.687700 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75545 (* 1 = 6.75545 loss)
I0523 01:08:12.403175 35003 sgd_solver.cpp:112] Iteration 107860, lr = 0.01
I0523 01:08:14.465549 35003 solver.cpp:239] Iteration 107870 (3.60006 iter/s, 2.77773s/10 iters), loss = 7.66478
I0523 01:08:14.465593 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66478 (* 1 = 7.66478 loss)
I0523 01:08:14.478782 35003 sgd_solver.cpp:112] Iteration 107870, lr = 0.01
I0523 01:08:20.062499 35003 solver.cpp:239] Iteration 107880 (1.78677 iter/s, 5.59668s/10 iters), loss = 7.02526
I0523 01:08:20.062537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02526 (* 1 = 7.02526 loss)
I0523 01:08:20.758647 35003 sgd_solver.cpp:112] Iteration 107880, lr = 0.01
I0523 01:08:23.631153 35003 solver.cpp:239] Iteration 107890 (2.80232 iter/s, 3.56847s/10 iters), loss = 7.80727
I0523 01:08:23.631408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80727 (* 1 = 7.80727 loss)
I0523 01:08:23.636468 35003 sgd_solver.cpp:112] Iteration 107890, lr = 0.01
I0523 01:08:27.885715 35003 solver.cpp:239] Iteration 107900 (2.35065 iter/s, 4.25414s/10 iters), loss = 7.61715
I0523 01:08:27.885795 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61715 (* 1 = 7.61715 loss)
I0523 01:08:28.593866 35003 sgd_solver.cpp:112] Iteration 107900, lr = 0.01
I0523 01:08:31.007549 35003 solver.cpp:239] Iteration 107910 (3.20346 iter/s, 3.12162s/10 iters), loss = 7.5923
I0523 01:08:31.007596 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5923 (* 1 = 7.5923 loss)
I0523 01:08:31.089748 35003 sgd_solver.cpp:112] Iteration 107910, lr = 0.01
I0523 01:08:33.203518 35003 solver.cpp:239] Iteration 107920 (4.55412 iter/s, 2.19582s/10 iters), loss = 6.65713
I0523 01:08:33.203560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65713 (* 1 = 6.65713 loss)
I0523 01:08:33.214388 35003 sgd_solver.cpp:112] Iteration 107920, lr = 0.01
I0523 01:08:37.699313 35003 solver.cpp:239] Iteration 107930 (2.22441 iter/s, 4.49557s/10 iters), loss = 7.97346
I0523 01:08:37.699363 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97346 (* 1 = 7.97346 loss)
I0523 01:08:38.440860 35003 sgd_solver.cpp:112] Iteration 107930, lr = 0.01
I0523 01:08:42.020565 35003 solver.cpp:239] Iteration 107940 (2.31427 iter/s, 4.32102s/10 iters), loss = 7.53295
I0523 01:08:42.020622 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53295 (* 1 = 7.53295 loss)
I0523 01:08:42.029368 35003 sgd_solver.cpp:112] Iteration 107940, lr = 0.01
I0523 01:08:45.508605 35003 solver.cpp:239] Iteration 107950 (2.8671 iter/s, 3.48784s/10 iters), loss = 7.51484
I0523 01:08:45.508647 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51484 (* 1 = 7.51484 loss)
I0523 01:08:45.923710 35003 sgd_solver.cpp:112] Iteration 107950, lr = 0.01
I0523 01:08:49.484377 35003 solver.cpp:239] Iteration 107960 (2.51537 iter/s, 3.97556s/10 iters), loss = 7.7925
I0523 01:08:49.484418 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7925 (* 1 = 7.7925 loss)
I0523 01:08:49.497511 35003 sgd_solver.cpp:112] Iteration 107960, lr = 0.01
I0523 01:08:52.665166 35003 solver.cpp:239] Iteration 107970 (3.14405 iter/s, 3.18061s/10 iters), loss = 7.08438
I0523 01:08:52.665217 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08438 (* 1 = 7.08438 loss)
I0523 01:08:52.672379 35003 sgd_solver.cpp:112] Iteration 107970, lr = 0.01
I0523 01:08:56.484980 35003 solver.cpp:239] Iteration 107980 (2.61807 iter/s, 3.8196s/10 iters), loss = 7.6589
I0523 01:08:56.485242 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6589 (* 1 = 7.6589 loss)
I0523 01:08:56.493196 35003 sgd_solver.cpp:112] Iteration 107980, lr = 0.01
I0523 01:09:00.842010 35003 solver.cpp:239] Iteration 107990 (2.29536 iter/s, 4.35661s/10 iters), loss = 5.79672
I0523 01:09:00.842059 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.79672 (* 1 = 5.79672 loss)
I0523 01:09:00.855821 35003 sgd_solver.cpp:112] Iteration 107990, lr = 0.01
I0523 01:09:02.878298 35003 solver.cpp:239] Iteration 108000 (4.91123 iter/s, 2.03615s/10 iters), loss = 6.56121
I0523 01:09:02.878342 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56121 (* 1 = 6.56121 loss)
I0523 01:09:02.889984 35003 sgd_solver.cpp:112] Iteration 108000, lr = 0.01
I0523 01:09:05.496666 35003 solver.cpp:239] Iteration 108010 (3.8194 iter/s, 2.61822s/10 iters), loss = 7.64903
I0523 01:09:05.496711 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64903 (* 1 = 7.64903 loss)
I0523 01:09:05.510771 35003 sgd_solver.cpp:112] Iteration 108010, lr = 0.01
I0523 01:09:08.991339 35003 solver.cpp:239] Iteration 108020 (2.86166 iter/s, 3.49447s/10 iters), loss = 7.27082
I0523 01:09:08.991392 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27082 (* 1 = 7.27082 loss)
I0523 01:09:09.657114 35003 sgd_solver.cpp:112] Iteration 108020, lr = 0.01
I0523 01:09:12.643931 35003 solver.cpp:239] Iteration 108030 (2.73793 iter/s, 3.65239s/10 iters), loss = 7.42302
I0523 01:09:12.643977 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42302 (* 1 = 7.42302 loss)
I0523 01:09:12.657052 35003 sgd_solver.cpp:112] Iteration 108030, lr = 0.01
I0523 01:09:15.529717 35003 solver.cpp:239] Iteration 108040 (3.46546 iter/s, 2.88562s/10 iters), loss = 6.94726
I0523 01:09:15.529770 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94726 (* 1 = 6.94726 loss)
I0523 01:09:15.535313 35003 sgd_solver.cpp:112] Iteration 108040, lr = 0.01
I0523 01:09:19.232662 35003 solver.cpp:239] Iteration 108050 (2.7007 iter/s, 3.70274s/10 iters), loss = 6.34678
I0523 01:09:19.232712 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34678 (* 1 = 6.34678 loss)
I0523 01:09:19.943581 35003 sgd_solver.cpp:112] Iteration 108050, lr = 0.01
I0523 01:09:23.617743 35003 solver.cpp:239] Iteration 108060 (2.28058 iter/s, 4.38485s/10 iters), loss = 6.82623
I0523 01:09:23.617795 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82623 (* 1 = 6.82623 loss)
I0523 01:09:23.801676 35003 sgd_solver.cpp:112] Iteration 108060, lr = 0.01
I0523 01:09:27.210043 35003 solver.cpp:239] Iteration 108070 (2.78389 iter/s, 3.5921s/10 iters), loss = 7.24544
I0523 01:09:27.210235 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24544 (* 1 = 7.24544 loss)
I0523 01:09:27.222875 35003 sgd_solver.cpp:112] Iteration 108070, lr = 0.01
I0523 01:09:29.660418 35003 solver.cpp:239] Iteration 108080 (4.08151 iter/s, 2.45007s/10 iters), loss = 7.42311
I0523 01:09:29.660462 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42311 (* 1 = 7.42311 loss)
I0523 01:09:29.674360 35003 sgd_solver.cpp:112] Iteration 108080, lr = 0.01
I0523 01:09:32.553423 35003 solver.cpp:239] Iteration 108090 (3.45682 iter/s, 2.89284s/10 iters), loss = 7.02774
I0523 01:09:32.553472 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02774 (* 1 = 7.02774 loss)
I0523 01:09:32.560305 35003 sgd_solver.cpp:112] Iteration 108090, lr = 0.01
I0523 01:09:35.162375 35003 solver.cpp:239] Iteration 108100 (3.8332 iter/s, 2.60879s/10 iters), loss = 8.19113
I0523 01:09:35.162431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.19113 (* 1 = 8.19113 loss)
I0523 01:09:35.175657 35003 sgd_solver.cpp:112] Iteration 108100, lr = 0.01
I0523 01:09:39.505985 35003 solver.cpp:239] Iteration 108110 (2.30235 iter/s, 4.34338s/10 iters), loss = 6.7276
I0523 01:09:39.506028 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7276 (* 1 = 6.7276 loss)
I0523 01:09:40.241015 35003 sgd_solver.cpp:112] Iteration 108110, lr = 0.01
I0523 01:09:42.757354 35003 solver.cpp:239] Iteration 108120 (3.0758 iter/s, 3.25119s/10 iters), loss = 6.98026
I0523 01:09:42.757411 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98026 (* 1 = 6.98026 loss)
I0523 01:09:42.768272 35003 sgd_solver.cpp:112] Iteration 108120, lr = 0.01
I0523 01:09:46.262858 35003 solver.cpp:239] Iteration 108130 (2.85283 iter/s, 3.50529s/10 iters), loss = 7.92754
I0523 01:09:46.262920 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.92754 (* 1 = 7.92754 loss)
I0523 01:09:46.265985 35003 sgd_solver.cpp:112] Iteration 108130, lr = 0.01
I0523 01:09:49.919695 35003 solver.cpp:239] Iteration 108140 (2.73478 iter/s, 3.65661s/10 iters), loss = 7.17679
I0523 01:09:49.919746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17679 (* 1 = 7.17679 loss)
I0523 01:09:50.528714 35003 sgd_solver.cpp:112] Iteration 108140, lr = 0.01
I0523 01:09:54.905606 35003 solver.cpp:239] Iteration 108150 (2.00576 iter/s, 4.98565s/10 iters), loss = 7.71379
I0523 01:09:54.905653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71379 (* 1 = 7.71379 loss)
I0523 01:09:54.917256 35003 sgd_solver.cpp:112] Iteration 108150, lr = 0.01
I0523 01:09:58.415139 35003 solver.cpp:239] Iteration 108160 (2.84955 iter/s, 3.50933s/10 iters), loss = 6.24135
I0523 01:09:58.415371 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24135 (* 1 = 6.24135 loss)
I0523 01:09:59.156600 35003 sgd_solver.cpp:112] Iteration 108160, lr = 0.01
I0523 01:10:02.238283 35003 solver.cpp:239] Iteration 108170 (2.6159 iter/s, 3.82277s/10 iters), loss = 7.18098
I0523 01:10:02.238335 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18098 (* 1 = 7.18098 loss)
I0523 01:10:02.249208 35003 sgd_solver.cpp:112] Iteration 108170, lr = 0.01
I0523 01:10:05.835379 35003 solver.cpp:239] Iteration 108180 (2.78017 iter/s, 3.5969s/10 iters), loss = 7.76206
I0523 01:10:05.835415 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76206 (* 1 = 7.76206 loss)
I0523 01:10:05.848512 35003 sgd_solver.cpp:112] Iteration 108180, lr = 0.01
I0523 01:10:10.157402 35003 solver.cpp:239] Iteration 108190 (2.31384 iter/s, 4.32181s/10 iters), loss = 6.39799
I0523 01:10:10.157439 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39799 (* 1 = 6.39799 loss)
I0523 01:10:10.164870 35003 sgd_solver.cpp:112] Iteration 108190, lr = 0.01
I0523 01:10:13.023561 35003 solver.cpp:239] Iteration 108200 (3.48919 iter/s, 2.86599s/10 iters), loss = 7.33539
I0523 01:10:13.023617 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33539 (* 1 = 7.33539 loss)
I0523 01:10:13.744508 35003 sgd_solver.cpp:112] Iteration 108200, lr = 0.01
I0523 01:10:16.402981 35003 solver.cpp:239] Iteration 108210 (2.95927 iter/s, 3.37921s/10 iters), loss = 7.91547
I0523 01:10:16.403024 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91547 (* 1 = 7.91547 loss)
I0523 01:10:17.085772 35003 sgd_solver.cpp:112] Iteration 108210, lr = 0.01
I0523 01:10:22.087715 35003 solver.cpp:239] Iteration 108220 (1.75918 iter/s, 5.68446s/10 iters), loss = 7.05775
I0523 01:10:22.087764 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05775 (* 1 = 7.05775 loss)
I0523 01:10:22.828255 35003 sgd_solver.cpp:112] Iteration 108220, lr = 0.01
I0523 01:10:25.965271 35003 solver.cpp:239] Iteration 108230 (2.57909 iter/s, 3.87734s/10 iters), loss = 7.57665
I0523 01:10:25.965333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57665 (* 1 = 7.57665 loss)
I0523 01:10:25.978448 35003 sgd_solver.cpp:112] Iteration 108230, lr = 0.01
I0523 01:10:29.399415 35003 solver.cpp:239] Iteration 108240 (2.91211 iter/s, 3.43394s/10 iters), loss = 7.28995
I0523 01:10:29.399669 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28995 (* 1 = 7.28995 loss)
I0523 01:10:29.411840 35003 sgd_solver.cpp:112] Iteration 108240, lr = 0.01
I0523 01:10:33.723979 35003 solver.cpp:239] Iteration 108250 (2.31259 iter/s, 4.32416s/10 iters), loss = 6.59305
I0523 01:10:33.724040 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59305 (* 1 = 6.59305 loss)
I0523 01:10:33.737123 35003 sgd_solver.cpp:112] Iteration 108250, lr = 0.01
I0523 01:10:36.696197 35003 solver.cpp:239] Iteration 108260 (3.36471 iter/s, 2.97203s/10 iters), loss = 6.22851
I0523 01:10:36.696247 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22851 (* 1 = 6.22851 loss)
I0523 01:10:37.320432 35003 sgd_solver.cpp:112] Iteration 108260, lr = 0.01
I0523 01:10:40.967492 35003 solver.cpp:239] Iteration 108270 (2.34133 iter/s, 4.27107s/10 iters), loss = 6.69558
I0523 01:10:40.967538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69558 (* 1 = 6.69558 loss)
I0523 01:10:40.972044 35003 sgd_solver.cpp:112] Iteration 108270, lr = 0.01
I0523 01:10:44.191694 35003 solver.cpp:239] Iteration 108280 (3.10173 iter/s, 3.22401s/10 iters), loss = 7.45396
I0523 01:10:44.191742 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45396 (* 1 = 7.45396 loss)
I0523 01:10:44.887653 35003 sgd_solver.cpp:112] Iteration 108280, lr = 0.01
I0523 01:10:46.188594 35003 solver.cpp:239] Iteration 108290 (5.0081 iter/s, 1.99677s/10 iters), loss = 8.00711
I0523 01:10:46.188633 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00711 (* 1 = 8.00711 loss)
I0523 01:10:46.214787 35003 sgd_solver.cpp:112] Iteration 108290, lr = 0.01
I0523 01:10:49.055270 35003 solver.cpp:239] Iteration 108300 (3.48855 iter/s, 2.86652s/10 iters), loss = 7.36833
I0523 01:10:49.055315 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36833 (* 1 = 7.36833 loss)
I0523 01:10:49.769969 35003 sgd_solver.cpp:112] Iteration 108300, lr = 0.01
I0523 01:10:53.620257 35003 solver.cpp:239] Iteration 108310 (2.1907 iter/s, 4.56474s/10 iters), loss = 8.0014
I0523 01:10:53.620326 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0014 (* 1 = 8.0014 loss)
I0523 01:10:54.361115 35003 sgd_solver.cpp:112] Iteration 108310, lr = 0.01
I0523 01:10:58.604872 35003 solver.cpp:239] Iteration 108320 (2.00628 iter/s, 4.98435s/10 iters), loss = 6.03052
I0523 01:10:58.604918 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03052 (* 1 = 6.03052 loss)
I0523 01:10:59.293875 35003 sgd_solver.cpp:112] Iteration 108320, lr = 0.01
I0523 01:11:03.015072 35003 solver.cpp:239] Iteration 108330 (2.26759 iter/s, 4.40998s/10 iters), loss = 7.21211
I0523 01:11:03.015333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21211 (* 1 = 7.21211 loss)
I0523 01:11:03.027434 35003 sgd_solver.cpp:112] Iteration 108330, lr = 0.01
I0523 01:11:06.555533 35003 solver.cpp:239] Iteration 108340 (2.8248 iter/s, 3.54008s/10 iters), loss = 7.6861
I0523 01:11:06.555585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6861 (* 1 = 7.6861 loss)
I0523 01:11:06.566980 35003 sgd_solver.cpp:112] Iteration 108340, lr = 0.01
I0523 01:11:09.490805 35003 solver.cpp:239] Iteration 108350 (3.40704 iter/s, 2.9351s/10 iters), loss = 6.82976
I0523 01:11:09.490857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82976 (* 1 = 6.82976 loss)
I0523 01:11:09.510466 35003 sgd_solver.cpp:112] Iteration 108350, lr = 0.01
I0523 01:11:12.436908 35003 solver.cpp:239] Iteration 108360 (3.39455 iter/s, 2.9459s/10 iters), loss = 6.65572
I0523 01:11:12.436956 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65572 (* 1 = 6.65572 loss)
I0523 01:11:13.165392 35003 sgd_solver.cpp:112] Iteration 108360, lr = 0.01
I0523 01:11:16.397816 35003 solver.cpp:239] Iteration 108370 (2.52481 iter/s, 3.96069s/10 iters), loss = 6.89797
I0523 01:11:16.397857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89797 (* 1 = 6.89797 loss)
I0523 01:11:16.419246 35003 sgd_solver.cpp:112] Iteration 108370, lr = 0.01
I0523 01:11:19.939051 35003 solver.cpp:239] Iteration 108380 (2.82403 iter/s, 3.54104s/10 iters), loss = 7.87641
I0523 01:11:19.939102 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87641 (* 1 = 7.87641 loss)
I0523 01:11:19.950902 35003 sgd_solver.cpp:112] Iteration 108380, lr = 0.01
I0523 01:11:22.699257 35003 solver.cpp:239] Iteration 108390 (3.62314 iter/s, 2.76004s/10 iters), loss = 8.43634
I0523 01:11:22.699314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.43634 (* 1 = 8.43634 loss)
I0523 01:11:22.712446 35003 sgd_solver.cpp:112] Iteration 108390, lr = 0.01
I0523 01:11:25.334902 35003 solver.cpp:239] Iteration 108400 (3.79439 iter/s, 2.63547s/10 iters), loss = 7.52422
I0523 01:11:25.334947 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52422 (* 1 = 7.52422 loss)
I0523 01:11:25.346386 35003 sgd_solver.cpp:112] Iteration 108400, lr = 0.01
I0523 01:11:28.171412 35003 solver.cpp:239] Iteration 108410 (3.52568 iter/s, 2.83633s/10 iters), loss = 7.23759
I0523 01:11:28.171458 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23759 (* 1 = 7.23759 loss)
I0523 01:11:28.183599 35003 sgd_solver.cpp:112] Iteration 108410, lr = 0.01
I0523 01:11:32.506196 35003 solver.cpp:239] Iteration 108420 (2.30704 iter/s, 4.33456s/10 iters), loss = 8.06913
I0523 01:11:32.506259 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.06913 (* 1 = 8.06913 loss)
I0523 01:11:32.519145 35003 sgd_solver.cpp:112] Iteration 108420, lr = 0.01
I0523 01:11:37.420698 35003 solver.cpp:239] Iteration 108430 (2.03491 iter/s, 4.91423s/10 iters), loss = 6.68628
I0523 01:11:37.420882 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68628 (* 1 = 6.68628 loss)
I0523 01:11:37.425026 35003 sgd_solver.cpp:112] Iteration 108430, lr = 0.01
I0523 01:11:40.123975 35003 solver.cpp:239] Iteration 108440 (3.69963 iter/s, 2.70297s/10 iters), loss = 7.04451
I0523 01:11:40.124022 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04451 (* 1 = 7.04451 loss)
I0523 01:11:40.131726 35003 sgd_solver.cpp:112] Iteration 108440, lr = 0.01
I0523 01:11:43.748948 35003 solver.cpp:239] Iteration 108450 (2.7588 iter/s, 3.62477s/10 iters), loss = 7.45533
I0523 01:11:43.749011 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45533 (* 1 = 7.45533 loss)
I0523 01:11:43.761644 35003 sgd_solver.cpp:112] Iteration 108450, lr = 0.01
I0523 01:11:46.075119 35003 solver.cpp:239] Iteration 108460 (4.29922 iter/s, 2.32601s/10 iters), loss = 7.67088
I0523 01:11:46.075168 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67088 (* 1 = 7.67088 loss)
I0523 01:11:46.082579 35003 sgd_solver.cpp:112] Iteration 108460, lr = 0.01
I0523 01:11:48.195003 35003 solver.cpp:239] Iteration 108470 (4.71755 iter/s, 2.11974s/10 iters), loss = 6.07635
I0523 01:11:48.195052 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07635 (* 1 = 6.07635 loss)
I0523 01:11:48.207931 35003 sgd_solver.cpp:112] Iteration 108470, lr = 0.01
I0523 01:11:51.104543 35003 solver.cpp:239] Iteration 108480 (3.43717 iter/s, 2.90937s/10 iters), loss = 7.25075
I0523 01:11:51.104583 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25075 (* 1 = 7.25075 loss)
I0523 01:11:51.111421 35003 sgd_solver.cpp:112] Iteration 108480, lr = 0.01
I0523 01:11:55.376585 35003 solver.cpp:239] Iteration 108490 (2.34092 iter/s, 4.27183s/10 iters), loss = 6.85212
I0523 01:11:55.376626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85212 (* 1 = 6.85212 loss)
I0523 01:11:55.557126 35003 sgd_solver.cpp:112] Iteration 108490, lr = 0.01
I0523 01:11:59.269311 35003 solver.cpp:239] Iteration 108500 (2.56924 iter/s, 3.8922s/10 iters), loss = 7.36087
I0523 01:11:59.269363 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36087 (* 1 = 7.36087 loss)
I0523 01:11:59.274058 35003 sgd_solver.cpp:112] Iteration 108500, lr = 0.01
I0523 01:12:02.179029 35003 solver.cpp:239] Iteration 108510 (3.43696 iter/s, 2.90955s/10 iters), loss = 7.58014
I0523 01:12:02.179080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58014 (* 1 = 7.58014 loss)
I0523 01:12:02.793746 35003 sgd_solver.cpp:112] Iteration 108510, lr = 0.01
I0523 01:12:04.782117 35003 solver.cpp:239] Iteration 108520 (3.84184 iter/s, 2.60292s/10 iters), loss = 7.34329
I0523 01:12:04.782177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34329 (* 1 = 7.34329 loss)
I0523 01:12:04.795881 35003 sgd_solver.cpp:112] Iteration 108520, lr = 0.01
I0523 01:12:08.226248 35003 solver.cpp:239] Iteration 108530 (2.90366 iter/s, 3.44393s/10 iters), loss = 7.93689
I0523 01:12:08.226514 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93689 (* 1 = 7.93689 loss)
I0523 01:12:08.240330 35003 sgd_solver.cpp:112] Iteration 108530, lr = 0.01
I0523 01:12:12.602775 35003 solver.cpp:239] Iteration 108540 (2.28513 iter/s, 4.37611s/10 iters), loss = 7.79122
I0523 01:12:12.602823 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79122 (* 1 = 7.79122 loss)
I0523 01:12:12.712705 35003 sgd_solver.cpp:112] Iteration 108540, lr = 0.01
I0523 01:12:13.567349 35003 solver.cpp:239] Iteration 108550 (10.3683 iter/s, 0.964474s/10 iters), loss = 5.99301
I0523 01:12:13.567399 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99301 (* 1 = 5.99301 loss)
I0523 01:12:13.569736 35003 sgd_solver.cpp:112] Iteration 108550, lr = 0.01
I0523 01:12:15.219955 35003 solver.cpp:239] Iteration 108560 (6.05157 iter/s, 1.65246s/10 iters), loss = 7.04344
I0523 01:12:15.220005 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04344 (* 1 = 7.04344 loss)
I0523 01:12:15.531576 35003 sgd_solver.cpp:112] Iteration 108560, lr = 0.01
I0523 01:12:19.425362 35003 solver.cpp:239] Iteration 108570 (2.37803 iter/s, 4.20516s/10 iters), loss = 8.03386
I0523 01:12:19.425424 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03386 (* 1 = 8.03386 loss)
I0523 01:12:19.427173 35003 sgd_solver.cpp:112] Iteration 108570, lr = 0.01
I0523 01:12:24.535658 35003 solver.cpp:239] Iteration 108580 (1.95694 iter/s, 5.11002s/10 iters), loss = 8.31446
I0523 01:12:24.535737 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.31446 (* 1 = 8.31446 loss)
I0523 01:12:24.586367 35003 sgd_solver.cpp:112] Iteration 108580, lr = 0.01
I0523 01:12:28.103428 35003 solver.cpp:239] Iteration 108590 (2.80305 iter/s, 3.56754s/10 iters), loss = 6.85473
I0523 01:12:28.103474 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85473 (* 1 = 6.85473 loss)
I0523 01:12:28.111095 35003 sgd_solver.cpp:112] Iteration 108590, lr = 0.01
I0523 01:12:31.766371 35003 solver.cpp:239] Iteration 108600 (2.7302 iter/s, 3.66274s/10 iters), loss = 6.74336
I0523 01:12:31.766429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74336 (* 1 = 6.74336 loss)
I0523 01:12:32.487408 35003 sgd_solver.cpp:112] Iteration 108600, lr = 0.01
I0523 01:12:35.284466 35003 solver.cpp:239] Iteration 108610 (2.84261 iter/s, 3.51789s/10 iters), loss = 6.98845
I0523 01:12:35.284523 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98845 (* 1 = 6.98845 loss)
I0523 01:12:35.297888 35003 sgd_solver.cpp:112] Iteration 108610, lr = 0.01
I0523 01:12:38.877758 35003 solver.cpp:239] Iteration 108620 (2.78312 iter/s, 3.59309s/10 iters), loss = 8.3526
I0523 01:12:38.877919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.3526 (* 1 = 8.3526 loss)
I0523 01:12:38.890764 35003 sgd_solver.cpp:112] Iteration 108620, lr = 0.01
I0523 01:12:41.554271 35003 solver.cpp:239] Iteration 108630 (3.73657 iter/s, 2.67625s/10 iters), loss = 7.3956
I0523 01:12:41.554308 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3956 (* 1 = 7.3956 loss)
I0523 01:12:41.572546 35003 sgd_solver.cpp:112] Iteration 108630, lr = 0.01
I0523 01:12:46.642537 35003 solver.cpp:239] Iteration 108640 (1.96541 iter/s, 5.08801s/10 iters), loss = 6.92193
I0523 01:12:46.642580 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92193 (* 1 = 6.92193 loss)
I0523 01:12:46.649694 35003 sgd_solver.cpp:112] Iteration 108640, lr = 0.01
I0523 01:12:50.928587 35003 solver.cpp:239] Iteration 108650 (2.33328 iter/s, 4.28581s/10 iters), loss = 8.76564
I0523 01:12:50.928637 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.76564 (* 1 = 8.76564 loss)
I0523 01:12:50.941860 35003 sgd_solver.cpp:112] Iteration 108650, lr = 0.01
I0523 01:12:54.563850 35003 solver.cpp:239] Iteration 108660 (2.75098 iter/s, 3.63506s/10 iters), loss = 7.64451
I0523 01:12:54.563890 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64451 (* 1 = 7.64451 loss)
I0523 01:12:54.573576 35003 sgd_solver.cpp:112] Iteration 108660, lr = 0.01
I0523 01:12:58.179311 35003 solver.cpp:239] Iteration 108670 (2.76605 iter/s, 3.61526s/10 iters), loss = 7.91178
I0523 01:12:58.179378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91178 (* 1 = 7.91178 loss)
I0523 01:12:58.909896 35003 sgd_solver.cpp:112] Iteration 108670, lr = 0.01
I0523 01:13:02.599587 35003 solver.cpp:239] Iteration 108680 (2.26243 iter/s, 4.42003s/10 iters), loss = 6.46258
I0523 01:13:02.599630 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46258 (* 1 = 6.46258 loss)
I0523 01:13:02.605298 35003 sgd_solver.cpp:112] Iteration 108680, lr = 0.01
I0523 01:13:06.579077 35003 solver.cpp:239] Iteration 108690 (2.51302 iter/s, 3.97927s/10 iters), loss = 6.8898
I0523 01:13:06.579128 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8898 (* 1 = 6.8898 loss)
I0523 01:13:06.597221 35003 sgd_solver.cpp:112] Iteration 108690, lr = 0.01
I0523 01:13:09.293056 35003 solver.cpp:239] Iteration 108700 (3.68485 iter/s, 2.71381s/10 iters), loss = 7.09058
I0523 01:13:09.293293 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09058 (* 1 = 7.09058 loss)
I0523 01:13:09.299371 35003 sgd_solver.cpp:112] Iteration 108700, lr = 0.01
I0523 01:13:12.883529 35003 solver.cpp:239] Iteration 108710 (2.78543 iter/s, 3.59011s/10 iters), loss = 6.05071
I0523 01:13:12.883571 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05071 (* 1 = 6.05071 loss)
I0523 01:13:12.897320 35003 sgd_solver.cpp:112] Iteration 108710, lr = 0.01
I0523 01:13:15.764247 35003 solver.cpp:239] Iteration 108720 (3.47156 iter/s, 2.88055s/10 iters), loss = 5.95116
I0523 01:13:15.764299 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95116 (* 1 = 5.95116 loss)
I0523 01:13:16.501438 35003 sgd_solver.cpp:112] Iteration 108720, lr = 0.01
I0523 01:13:20.199321 35003 solver.cpp:239] Iteration 108730 (2.25488 iter/s, 4.43483s/10 iters), loss = 6.89853
I0523 01:13:20.199378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89853 (* 1 = 6.89853 loss)
I0523 01:13:20.213167 35003 sgd_solver.cpp:112] Iteration 108730, lr = 0.01
I0523 01:13:23.824697 35003 solver.cpp:239] Iteration 108740 (2.7585 iter/s, 3.62516s/10 iters), loss = 7.91847
I0523 01:13:23.824751 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91847 (* 1 = 7.91847 loss)
I0523 01:13:23.829428 35003 sgd_solver.cpp:112] Iteration 108740, lr = 0.01
I0523 01:13:27.325685 35003 solver.cpp:239] Iteration 108750 (2.85651 iter/s, 3.50078s/10 iters), loss = 5.95198
I0523 01:13:27.325753 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95198 (* 1 = 5.95198 loss)
I0523 01:13:28.040299 35003 sgd_solver.cpp:112] Iteration 108750, lr = 0.01
I0523 01:13:31.731849 35003 solver.cpp:239] Iteration 108760 (2.26968 iter/s, 4.40591s/10 iters), loss = 8.05404
I0523 01:13:31.731912 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05404 (* 1 = 8.05404 loss)
I0523 01:13:32.472702 35003 sgd_solver.cpp:112] Iteration 108760, lr = 0.01
I0523 01:13:36.316953 35003 solver.cpp:239] Iteration 108770 (2.18109 iter/s, 4.58486s/10 iters), loss = 6.49152
I0523 01:13:36.316994 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49152 (* 1 = 6.49152 loss)
I0523 01:13:36.330233 35003 sgd_solver.cpp:112] Iteration 108770, lr = 0.01
I0523 01:13:39.138828 35003 solver.cpp:239] Iteration 108780 (3.54395 iter/s, 2.82171s/10 iters), loss = 6.53071
I0523 01:13:39.138892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53071 (* 1 = 6.53071 loss)
I0523 01:13:39.151712 35003 sgd_solver.cpp:112] Iteration 108780, lr = 0.01
I0523 01:13:41.151850 35003 solver.cpp:239] Iteration 108790 (4.96802 iter/s, 2.01287s/10 iters), loss = 8.14032
I0523 01:13:41.151971 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.14032 (* 1 = 8.14032 loss)
I0523 01:13:41.852144 35003 sgd_solver.cpp:112] Iteration 108790, lr = 0.01
I0523 01:13:45.552749 35003 solver.cpp:239] Iteration 108800 (2.27242 iter/s, 4.40059s/10 iters), loss = 7.45766
I0523 01:13:45.552803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45766 (* 1 = 7.45766 loss)
I0523 01:13:46.287776 35003 sgd_solver.cpp:112] Iteration 108800, lr = 0.01
I0523 01:13:50.638958 35003 solver.cpp:239] Iteration 108810 (1.9662 iter/s, 5.08595s/10 iters), loss = 6.45861
I0523 01:13:50.639019 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45861 (* 1 = 6.45861 loss)
I0523 01:13:51.210382 35003 sgd_solver.cpp:112] Iteration 108810, lr = 0.01
I0523 01:13:53.349977 35003 solver.cpp:239] Iteration 108820 (3.68887 iter/s, 2.71086s/10 iters), loss = 7.00306
I0523 01:13:53.350023 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00306 (* 1 = 7.00306 loss)
I0523 01:13:54.085036 35003 sgd_solver.cpp:112] Iteration 108820, lr = 0.01
I0523 01:13:57.672927 35003 solver.cpp:239] Iteration 108830 (2.31336 iter/s, 4.32272s/10 iters), loss = 8.00882
I0523 01:13:57.672978 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00882 (* 1 = 8.00882 loss)
I0523 01:13:57.689739 35003 sgd_solver.cpp:112] Iteration 108830, lr = 0.01
I0523 01:14:02.116235 35003 solver.cpp:239] Iteration 108840 (2.25069 iter/s, 4.44308s/10 iters), loss = 7.42693
I0523 01:14:02.116281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42693 (* 1 = 7.42693 loss)
I0523 01:14:02.126194 35003 sgd_solver.cpp:112] Iteration 108840, lr = 0.01
I0523 01:14:04.131386 35003 solver.cpp:239] Iteration 108850 (4.96273 iter/s, 2.01502s/10 iters), loss = 7.38559
I0523 01:14:04.131428 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38559 (* 1 = 7.38559 loss)
I0523 01:14:04.852398 35003 sgd_solver.cpp:112] Iteration 108850, lr = 0.01
I0523 01:14:09.014822 35003 solver.cpp:239] Iteration 108860 (2.04784 iter/s, 4.88319s/10 iters), loss = 6.09777
I0523 01:14:09.014889 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09777 (* 1 = 6.09777 loss)
I0523 01:14:09.028522 35003 sgd_solver.cpp:112] Iteration 108860, lr = 0.01
I0523 01:14:12.536207 35003 solver.cpp:239] Iteration 108870 (2.83996 iter/s, 3.52117s/10 iters), loss = 6.71997
I0523 01:14:12.536471 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71997 (* 1 = 6.71997 loss)
I0523 01:14:13.277145 35003 sgd_solver.cpp:112] Iteration 108870, lr = 0.01
I0523 01:14:16.990105 35003 solver.cpp:239] Iteration 108880 (2.24545 iter/s, 4.45345s/10 iters), loss = 7.48957
I0523 01:14:16.990186 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48957 (* 1 = 7.48957 loss)
I0523 01:14:16.992086 35003 sgd_solver.cpp:112] Iteration 108880, lr = 0.01
I0523 01:14:20.214576 35003 solver.cpp:239] Iteration 108890 (3.10149 iter/s, 3.22426s/10 iters), loss = 7.09927
I0523 01:14:20.214618 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09927 (* 1 = 7.09927 loss)
I0523 01:14:20.234369 35003 sgd_solver.cpp:112] Iteration 108890, lr = 0.01
I0523 01:14:23.875694 35003 solver.cpp:239] Iteration 108900 (2.73155 iter/s, 3.66092s/10 iters), loss = 6.29635
I0523 01:14:23.875746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29635 (* 1 = 6.29635 loss)
I0523 01:14:24.607581 35003 sgd_solver.cpp:112] Iteration 108900, lr = 0.01
I0523 01:14:27.857059 35003 solver.cpp:239] Iteration 108910 (2.51184 iter/s, 3.98114s/10 iters), loss = 8.0141
I0523 01:14:27.857105 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0141 (* 1 = 8.0141 loss)
I0523 01:14:27.862252 35003 sgd_solver.cpp:112] Iteration 108910, lr = 0.01
I0523 01:14:30.606791 35003 solver.cpp:239] Iteration 108920 (3.63694 iter/s, 2.74956s/10 iters), loss = 5.41684
I0523 01:14:30.606848 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.41684 (* 1 = 5.41684 loss)
I0523 01:14:30.625627 35003 sgd_solver.cpp:112] Iteration 108920, lr = 0.01
I0523 01:14:34.133515 35003 solver.cpp:239] Iteration 108930 (2.83566 iter/s, 3.52652s/10 iters), loss = 6.74753
I0523 01:14:34.133576 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74753 (* 1 = 6.74753 loss)
I0523 01:14:34.841754 35003 sgd_solver.cpp:112] Iteration 108930, lr = 0.01
I0523 01:14:37.054096 35003 solver.cpp:239] Iteration 108940 (3.42419 iter/s, 2.9204s/10 iters), loss = 6.96362
I0523 01:14:37.054141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96362 (* 1 = 6.96362 loss)
I0523 01:14:37.506567 35003 sgd_solver.cpp:112] Iteration 108940, lr = 0.01
I0523 01:14:40.256451 35003 solver.cpp:239] Iteration 108950 (3.12289 iter/s, 3.20217s/10 iters), loss = 7.85049
I0523 01:14:40.256531 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85049 (* 1 = 7.85049 loss)
I0523 01:14:40.284092 35003 sgd_solver.cpp:112] Iteration 108950, lr = 0.01
I0523 01:14:45.474416 35003 solver.cpp:239] Iteration 108960 (1.91656 iter/s, 5.21768s/10 iters), loss = 7.21137
I0523 01:14:45.474651 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21137 (* 1 = 7.21137 loss)
I0523 01:14:45.482277 35003 sgd_solver.cpp:112] Iteration 108960, lr = 0.01
I0523 01:14:49.108160 35003 solver.cpp:239] Iteration 108970 (2.75225 iter/s, 3.63339s/10 iters), loss = 7.04861
I0523 01:14:49.108207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04861 (* 1 = 7.04861 loss)
I0523 01:14:49.725973 35003 sgd_solver.cpp:112] Iteration 108970, lr = 0.01
I0523 01:14:52.495108 35003 solver.cpp:239] Iteration 108980 (2.95269 iter/s, 3.38675s/10 iters), loss = 8.05167
I0523 01:14:52.495172 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05167 (* 1 = 8.05167 loss)
I0523 01:14:52.518543 35003 sgd_solver.cpp:112] Iteration 108980, lr = 0.01
I0523 01:14:55.842509 35003 solver.cpp:239] Iteration 108990 (2.98757 iter/s, 3.3472s/10 iters), loss = 7.99951
I0523 01:14:55.842555 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99951 (* 1 = 7.99951 loss)
I0523 01:14:55.880378 35003 sgd_solver.cpp:112] Iteration 108990, lr = 0.01
I0523 01:15:00.312367 35003 solver.cpp:239] Iteration 109000 (2.23732 iter/s, 4.46963s/10 iters), loss = 7.33387
I0523 01:15:00.312418 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33387 (* 1 = 7.33387 loss)
I0523 01:15:00.321925 35003 sgd_solver.cpp:112] Iteration 109000, lr = 0.01
I0523 01:15:05.073978 35003 solver.cpp:239] Iteration 109010 (2.10024 iter/s, 4.76136s/10 iters), loss = 7.69064
I0523 01:15:05.074040 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69064 (* 1 = 7.69064 loss)
I0523 01:15:05.079529 35003 sgd_solver.cpp:112] Iteration 109010, lr = 0.01
I0523 01:15:08.510588 35003 solver.cpp:239] Iteration 109020 (2.91002 iter/s, 3.4364s/10 iters), loss = 7.01502
I0523 01:15:08.510628 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01502 (* 1 = 7.01502 loss)
I0523 01:15:08.515642 35003 sgd_solver.cpp:112] Iteration 109020, lr = 0.01
I0523 01:15:11.287842 35003 solver.cpp:239] Iteration 109030 (3.60089 iter/s, 2.77709s/10 iters), loss = 6.22206
I0523 01:15:11.287886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22206 (* 1 = 6.22206 loss)
I0523 01:15:11.300920 35003 sgd_solver.cpp:112] Iteration 109030, lr = 0.01
I0523 01:15:16.152781 35003 solver.cpp:239] Iteration 109040 (2.05563 iter/s, 4.86469s/10 iters), loss = 7.44881
I0523 01:15:16.152948 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44881 (* 1 = 7.44881 loss)
I0523 01:15:16.165731 35003 sgd_solver.cpp:112] Iteration 109040, lr = 0.01
I0523 01:15:19.013322 35003 solver.cpp:239] Iteration 109050 (3.49618 iter/s, 2.86027s/10 iters), loss = 8.14944
I0523 01:15:19.013375 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.14944 (* 1 = 8.14944 loss)
I0523 01:15:19.035477 35003 sgd_solver.cpp:112] Iteration 109050, lr = 0.01
I0523 01:15:23.480794 35003 solver.cpp:239] Iteration 109060 (2.23852 iter/s, 4.46724s/10 iters), loss = 7.29354
I0523 01:15:23.480844 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29354 (* 1 = 7.29354 loss)
I0523 01:15:23.829136 35003 sgd_solver.cpp:112] Iteration 109060, lr = 0.01
I0523 01:15:25.898888 35003 solver.cpp:239] Iteration 109070 (4.13576 iter/s, 2.41794s/10 iters), loss = 7.77654
I0523 01:15:25.898932 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77654 (* 1 = 7.77654 loss)
I0523 01:15:26.614395 35003 sgd_solver.cpp:112] Iteration 109070, lr = 0.01
I0523 01:15:30.998196 35003 solver.cpp:239] Iteration 109080 (1.96116 iter/s, 5.09901s/10 iters), loss = 8.04407
I0523 01:15:30.998241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04407 (* 1 = 8.04407 loss)
I0523 01:15:31.020227 35003 sgd_solver.cpp:112] Iteration 109080, lr = 0.01
I0523 01:15:35.285042 35003 solver.cpp:239] Iteration 109090 (2.33284 iter/s, 4.28663s/10 iters), loss = 7.05297
I0523 01:15:35.285084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05297 (* 1 = 7.05297 loss)
I0523 01:15:35.372238 35003 sgd_solver.cpp:112] Iteration 109090, lr = 0.01
I0523 01:15:41.242410 35003 solver.cpp:239] Iteration 109100 (1.67867 iter/s, 5.95708s/10 iters), loss = 6.90044
I0523 01:15:41.242458 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90044 (* 1 = 6.90044 loss)
I0523 01:15:41.970103 35003 sgd_solver.cpp:112] Iteration 109100, lr = 0.01
I0523 01:15:44.995610 35003 solver.cpp:239] Iteration 109110 (2.66454 iter/s, 3.753s/10 iters), loss = 6.50601
I0523 01:15:44.995648 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50601 (* 1 = 6.50601 loss)
I0523 01:15:45.008548 35003 sgd_solver.cpp:112] Iteration 109110, lr = 0.01
I0523 01:15:48.527824 35003 solver.cpp:239] Iteration 109120 (2.83124 iter/s, 3.53202s/10 iters), loss = 6.85889
I0523 01:15:48.528102 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85889 (* 1 = 6.85889 loss)
I0523 01:15:48.545614 35003 sgd_solver.cpp:112] Iteration 109120, lr = 0.01
I0523 01:15:51.239212 35003 solver.cpp:239] Iteration 109130 (3.68865 iter/s, 2.71102s/10 iters), loss = 7.03471
I0523 01:15:51.239264 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03471 (* 1 = 7.03471 loss)
I0523 01:15:51.246709 35003 sgd_solver.cpp:112] Iteration 109130, lr = 0.01
I0523 01:15:54.147174 35003 solver.cpp:239] Iteration 109140 (3.43904 iter/s, 2.90779s/10 iters), loss = 7.55383
I0523 01:15:54.147209 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55383 (* 1 = 7.55383 loss)
I0523 01:15:54.154687 35003 sgd_solver.cpp:112] Iteration 109140, lr = 0.01
I0523 01:15:57.761631 35003 solver.cpp:239] Iteration 109150 (2.76681 iter/s, 3.61427s/10 iters), loss = 7.4688
I0523 01:15:57.761682 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4688 (* 1 = 7.4688 loss)
I0523 01:15:57.774778 35003 sgd_solver.cpp:112] Iteration 109150, lr = 0.01
I0523 01:16:02.036010 35003 solver.cpp:239] Iteration 109160 (2.33964 iter/s, 4.27415s/10 iters), loss = 6.70444
I0523 01:16:02.036067 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70444 (* 1 = 6.70444 loss)
I0523 01:16:02.045439 35003 sgd_solver.cpp:112] Iteration 109160, lr = 0.01
I0523 01:16:05.485647 35003 solver.cpp:239] Iteration 109170 (2.89903 iter/s, 3.44943s/10 iters), loss = 7.59417
I0523 01:16:05.485698 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59417 (* 1 = 7.59417 loss)
I0523 01:16:05.495935 35003 sgd_solver.cpp:112] Iteration 109170, lr = 0.01
I0523 01:16:07.725555 35003 solver.cpp:239] Iteration 109180 (4.46478 iter/s, 2.23975s/10 iters), loss = 7.32415
I0523 01:16:07.725610 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32415 (* 1 = 7.32415 loss)
I0523 01:16:07.801590 35003 sgd_solver.cpp:112] Iteration 109180, lr = 0.01
I0523 01:16:14.363648 35003 solver.cpp:239] Iteration 109190 (1.50653 iter/s, 6.63777s/10 iters), loss = 6.90644
I0523 01:16:14.363696 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90644 (* 1 = 6.90644 loss)
I0523 01:16:14.376865 35003 sgd_solver.cpp:112] Iteration 109190, lr = 0.01
I0523 01:16:17.214774 35003 solver.cpp:239] Iteration 109200 (3.5076 iter/s, 2.85095s/10 iters), loss = 6.7761
I0523 01:16:17.214830 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7761 (* 1 = 6.7761 loss)
I0523 01:16:17.227686 35003 sgd_solver.cpp:112] Iteration 109200, lr = 0.01
I0523 01:16:20.007822 35003 solver.cpp:239] Iteration 109210 (3.58059 iter/s, 2.79283s/10 iters), loss = 6.88219
I0523 01:16:20.008016 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88219 (* 1 = 6.88219 loss)
I0523 01:16:20.021235 35003 sgd_solver.cpp:112] Iteration 109210, lr = 0.01
I0523 01:16:22.902489 35003 solver.cpp:239] Iteration 109220 (3.45502 iter/s, 2.89434s/10 iters), loss = 6.60972
I0523 01:16:22.902554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60972 (* 1 = 6.60972 loss)
I0523 01:16:23.237011 35003 sgd_solver.cpp:112] Iteration 109220, lr = 0.01
I0523 01:16:27.921192 35003 solver.cpp:239] Iteration 109230 (1.99265 iter/s, 5.01844s/10 iters), loss = 7.50939
I0523 01:16:27.921233 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50939 (* 1 = 7.50939 loss)
I0523 01:16:27.932943 35003 sgd_solver.cpp:112] Iteration 109230, lr = 0.01
I0523 01:16:32.360301 35003 solver.cpp:239] Iteration 109240 (2.25282 iter/s, 4.43888s/10 iters), loss = 7.61011
I0523 01:16:32.360345 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61011 (* 1 = 7.61011 loss)
I0523 01:16:32.362402 35003 sgd_solver.cpp:112] Iteration 109240, lr = 0.01
I0523 01:16:34.369125 35003 solver.cpp:239] Iteration 109250 (4.97837 iter/s, 2.00869s/10 iters), loss = 6.65265
I0523 01:16:34.369174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65265 (* 1 = 6.65265 loss)
I0523 01:16:34.377555 35003 sgd_solver.cpp:112] Iteration 109250, lr = 0.01
I0523 01:16:36.319257 35003 solver.cpp:239] Iteration 109260 (5.12823 iter/s, 1.94999s/10 iters), loss = 6.14693
I0523 01:16:36.319308 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14693 (* 1 = 6.14693 loss)
I0523 01:16:36.935904 35003 sgd_solver.cpp:112] Iteration 109260, lr = 0.01
I0523 01:16:41.644269 35003 solver.cpp:239] Iteration 109270 (1.87802 iter/s, 5.32474s/10 iters), loss = 7.0016
I0523 01:16:41.644306 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0016 (* 1 = 7.0016 loss)
I0523 01:16:41.647467 35003 sgd_solver.cpp:112] Iteration 109270, lr = 0.01
I0523 01:16:44.397264 35003 solver.cpp:239] Iteration 109280 (3.63262 iter/s, 2.75283s/10 iters), loss = 6.81857
I0523 01:16:44.397325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81857 (* 1 = 6.81857 loss)
I0523 01:16:45.138062 35003 sgd_solver.cpp:112] Iteration 109280, lr = 0.01
I0523 01:16:47.874683 35003 solver.cpp:239] Iteration 109290 (2.87587 iter/s, 3.47721s/10 iters), loss = 6.74838
I0523 01:16:47.874740 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74838 (* 1 = 6.74838 loss)
I0523 01:16:47.893586 35003 sgd_solver.cpp:112] Iteration 109290, lr = 0.01
I0523 01:16:51.526494 35003 solver.cpp:239] Iteration 109300 (2.73854 iter/s, 3.65158s/10 iters), loss = 7.05039
I0523 01:16:51.526720 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05039 (* 1 = 7.05039 loss)
I0523 01:16:51.534512 35003 sgd_solver.cpp:112] Iteration 109300, lr = 0.01
I0523 01:16:55.170289 35003 solver.cpp:239] Iteration 109310 (2.74466 iter/s, 3.64344s/10 iters), loss = 7.10744
I0523 01:16:55.170333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10744 (* 1 = 7.10744 loss)
I0523 01:16:55.188118 35003 sgd_solver.cpp:112] Iteration 109310, lr = 0.01
I0523 01:16:58.406445 35003 solver.cpp:239] Iteration 109320 (3.09026 iter/s, 3.23597s/10 iters), loss = 7.51413
I0523 01:16:58.406487 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51413 (* 1 = 7.51413 loss)
I0523 01:16:58.416575 35003 sgd_solver.cpp:112] Iteration 109320, lr = 0.01
I0523 01:17:01.292837 35003 solver.cpp:239] Iteration 109330 (3.46478 iter/s, 2.88618s/10 iters), loss = 6.69345
I0523 01:17:01.292888 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69345 (* 1 = 6.69345 loss)
I0523 01:17:01.976078 35003 sgd_solver.cpp:112] Iteration 109330, lr = 0.01
I0523 01:17:04.974635 35003 solver.cpp:239] Iteration 109340 (2.71621 iter/s, 3.6816s/10 iters), loss = 7.27745
I0523 01:17:04.974680 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27745 (* 1 = 7.27745 loss)
I0523 01:17:05.715085 35003 sgd_solver.cpp:112] Iteration 109340, lr = 0.01
I0523 01:17:08.521553 35003 solver.cpp:239] Iteration 109350 (2.8195 iter/s, 3.54672s/10 iters), loss = 7.05284
I0523 01:17:08.521594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05284 (* 1 = 7.05284 loss)
I0523 01:17:08.534622 35003 sgd_solver.cpp:112] Iteration 109350, lr = 0.01
I0523 01:17:10.841599 35003 solver.cpp:239] Iteration 109360 (4.31052 iter/s, 2.31991s/10 iters), loss = 7.35328
I0523 01:17:10.841635 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35328 (* 1 = 7.35328 loss)
I0523 01:17:10.854663 35003 sgd_solver.cpp:112] Iteration 109360, lr = 0.01
I0523 01:17:13.701267 35003 solver.cpp:239] Iteration 109370 (3.49711 iter/s, 2.8595s/10 iters), loss = 5.64399
I0523 01:17:13.701318 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.64399 (* 1 = 5.64399 loss)
I0523 01:17:13.704828 35003 sgd_solver.cpp:112] Iteration 109370, lr = 0.01
I0523 01:17:17.305074 35003 solver.cpp:239] Iteration 109380 (2.775 iter/s, 3.6036s/10 iters), loss = 8.74498
I0523 01:17:17.305120 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.74498 (* 1 = 8.74498 loss)
I0523 01:17:17.327358 35003 sgd_solver.cpp:112] Iteration 109380, lr = 0.01
I0523 01:17:20.167930 35003 solver.cpp:239] Iteration 109390 (3.49322 iter/s, 2.86269s/10 iters), loss = 7.66674
I0523 01:17:20.167969 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66674 (* 1 = 7.66674 loss)
I0523 01:17:20.175501 35003 sgd_solver.cpp:112] Iteration 109390, lr = 0.01
I0523 01:17:23.564281 35003 solver.cpp:239] Iteration 109400 (2.9445 iter/s, 3.39616s/10 iters), loss = 7.20972
I0523 01:17:23.564565 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20972 (* 1 = 7.20972 loss)
I0523 01:17:23.568495 35003 sgd_solver.cpp:112] Iteration 109400, lr = 0.01
I0523 01:17:26.391134 35003 solver.cpp:239] Iteration 109410 (3.53798 iter/s, 2.82647s/10 iters), loss = 8.14423
I0523 01:17:26.391194 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.14423 (* 1 = 8.14423 loss)
I0523 01:17:26.396981 35003 sgd_solver.cpp:112] Iteration 109410, lr = 0.01
I0523 01:17:32.280833 35003 solver.cpp:239] Iteration 109420 (1.69797 iter/s, 5.88939s/10 iters), loss = 7.71592
I0523 01:17:32.280877 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71592 (* 1 = 7.71592 loss)
I0523 01:17:32.294832 35003 sgd_solver.cpp:112] Iteration 109420, lr = 0.01
I0523 01:17:35.055605 35003 solver.cpp:239] Iteration 109430 (3.60412 iter/s, 2.7746s/10 iters), loss = 6.58756
I0523 01:17:35.055667 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58756 (* 1 = 6.58756 loss)
I0523 01:17:35.060765 35003 sgd_solver.cpp:112] Iteration 109430, lr = 0.01
I0523 01:17:38.056771 35003 solver.cpp:239] Iteration 109440 (3.33225 iter/s, 3.00098s/10 iters), loss = 7.24752
I0523 01:17:38.056823 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24752 (* 1 = 7.24752 loss)
I0523 01:17:38.761536 35003 sgd_solver.cpp:112] Iteration 109440, lr = 0.01
I0523 01:17:41.083277 35003 solver.cpp:239] Iteration 109450 (3.30435 iter/s, 3.02631s/10 iters), loss = 7.48579
I0523 01:17:41.083354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48579 (* 1 = 7.48579 loss)
I0523 01:17:41.716331 35003 sgd_solver.cpp:112] Iteration 109450, lr = 0.01
I0523 01:17:45.470461 35003 solver.cpp:239] Iteration 109460 (2.2795 iter/s, 4.38692s/10 iters), loss = 7.90015
I0523 01:17:45.470525 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90015 (* 1 = 7.90015 loss)
I0523 01:17:45.482583 35003 sgd_solver.cpp:112] Iteration 109460, lr = 0.01
I0523 01:17:48.386065 35003 solver.cpp:239] Iteration 109470 (3.43004 iter/s, 2.91542s/10 iters), loss = 8.27434
I0523 01:17:48.386116 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.27434 (* 1 = 8.27434 loss)
I0523 01:17:49.062520 35003 sgd_solver.cpp:112] Iteration 109470, lr = 0.01
I0523 01:17:53.402812 35003 solver.cpp:239] Iteration 109480 (1.99343 iter/s, 5.01649s/10 iters), loss = 8.06157
I0523 01:17:53.402863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.06157 (* 1 = 8.06157 loss)
I0523 01:17:53.416182 35003 sgd_solver.cpp:112] Iteration 109480, lr = 0.01
I0523 01:17:56.969745 35003 solver.cpp:239] Iteration 109490 (2.80369 iter/s, 3.56673s/10 iters), loss = 6.81773
I0523 01:17:56.969974 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81773 (* 1 = 6.81773 loss)
I0523 01:17:57.687360 35003 sgd_solver.cpp:112] Iteration 109490, lr = 0.01
I0523 01:18:01.971130 35003 solver.cpp:239] Iteration 109500 (1.99961 iter/s, 5.00098s/10 iters), loss = 7.2396
I0523 01:18:01.971175 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2396 (* 1 = 7.2396 loss)
I0523 01:18:02.692358 35003 sgd_solver.cpp:112] Iteration 109500, lr = 0.01
I0523 01:18:05.493979 35003 solver.cpp:239] Iteration 109510 (2.83877 iter/s, 3.52265s/10 iters), loss = 7.12021
I0523 01:18:05.494030 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12021 (* 1 = 7.12021 loss)
I0523 01:18:06.209312 35003 sgd_solver.cpp:112] Iteration 109510, lr = 0.01
I0523 01:18:10.563843 35003 solver.cpp:239] Iteration 109520 (1.97254 iter/s, 5.06961s/10 iters), loss = 6.44934
I0523 01:18:10.563887 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44934 (* 1 = 6.44934 loss)
I0523 01:18:10.573654 35003 sgd_solver.cpp:112] Iteration 109520, lr = 0.01
I0523 01:18:15.532045 35003 solver.cpp:239] Iteration 109530 (2.01291 iter/s, 4.96794s/10 iters), loss = 6.07746
I0523 01:18:15.532115 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07746 (* 1 = 6.07746 loss)
I0523 01:18:15.543699 35003 sgd_solver.cpp:112] Iteration 109530, lr = 0.01
I0523 01:18:19.706246 35003 solver.cpp:239] Iteration 109540 (2.39581 iter/s, 4.17396s/10 iters), loss = 6.74528
I0523 01:18:19.706286 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74528 (* 1 = 6.74528 loss)
I0523 01:18:19.719280 35003 sgd_solver.cpp:112] Iteration 109540, lr = 0.01
I0523 01:18:23.245792 35003 solver.cpp:239] Iteration 109550 (2.82538 iter/s, 3.53935s/10 iters), loss = 6.24044
I0523 01:18:23.245836 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24044 (* 1 = 6.24044 loss)
I0523 01:18:23.259155 35003 sgd_solver.cpp:112] Iteration 109550, lr = 0.01
I0523 01:18:26.091398 35003 solver.cpp:239] Iteration 109560 (3.51439 iter/s, 2.84545s/10 iters), loss = 7.8526
I0523 01:18:26.091437 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8526 (* 1 = 7.8526 loss)
I0523 01:18:26.819239 35003 sgd_solver.cpp:112] Iteration 109560, lr = 0.01
I0523 01:18:31.098337 35003 solver.cpp:239] Iteration 109570 (1.99733 iter/s, 5.0067s/10 iters), loss = 6.89282
I0523 01:18:31.098573 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89282 (* 1 = 6.89282 loss)
I0523 01:18:31.116789 35003 sgd_solver.cpp:112] Iteration 109570, lr = 0.01
I0523 01:18:34.705013 35003 solver.cpp:239] Iteration 109580 (2.77292 iter/s, 3.60631s/10 iters), loss = 6.56141
I0523 01:18:34.705049 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56141 (* 1 = 6.56141 loss)
I0523 01:18:34.718169 35003 sgd_solver.cpp:112] Iteration 109580, lr = 0.01
I0523 01:18:38.224864 35003 solver.cpp:239] Iteration 109590 (2.84118 iter/s, 3.51967s/10 iters), loss = 6.47541
I0523 01:18:38.224902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47541 (* 1 = 6.47541 loss)
I0523 01:18:38.231755 35003 sgd_solver.cpp:112] Iteration 109590, lr = 0.01
I0523 01:18:40.369438 35003 solver.cpp:239] Iteration 109600 (4.66326 iter/s, 2.14442s/10 iters), loss = 7.03562
I0523 01:18:40.369493 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03562 (* 1 = 7.03562 loss)
I0523 01:18:41.103723 35003 sgd_solver.cpp:112] Iteration 109600, lr = 0.01
I0523 01:18:43.943270 35003 solver.cpp:239] Iteration 109610 (2.79827 iter/s, 3.57364s/10 iters), loss = 8.40372
I0523 01:18:43.943310 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.40372 (* 1 = 8.40372 loss)
I0523 01:18:43.962126 35003 sgd_solver.cpp:112] Iteration 109610, lr = 0.01
I0523 01:18:45.804625 35003 solver.cpp:239] Iteration 109620 (5.3728 iter/s, 1.86123s/10 iters), loss = 6.8254
I0523 01:18:45.804677 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8254 (* 1 = 6.8254 loss)
I0523 01:18:45.817085 35003 sgd_solver.cpp:112] Iteration 109620, lr = 0.01
I0523 01:18:49.408058 35003 solver.cpp:239] Iteration 109630 (2.77529 iter/s, 3.60323s/10 iters), loss = 7.7069
I0523 01:18:49.408103 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7069 (* 1 = 7.7069 loss)
I0523 01:18:49.421402 35003 sgd_solver.cpp:112] Iteration 109630, lr = 0.01
I0523 01:18:52.904700 35003 solver.cpp:239] Iteration 109640 (2.86004 iter/s, 3.49645s/10 iters), loss = 7.05727
I0523 01:18:52.904750 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05727 (* 1 = 7.05727 loss)
I0523 01:18:52.908766 35003 sgd_solver.cpp:112] Iteration 109640, lr = 0.01
I0523 01:18:55.593456 35003 solver.cpp:239] Iteration 109650 (3.71942 iter/s, 2.68859s/10 iters), loss = 7.57835
I0523 01:18:55.593494 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57835 (* 1 = 7.57835 loss)
I0523 01:18:55.607075 35003 sgd_solver.cpp:112] Iteration 109650, lr = 0.01
I0523 01:18:59.331665 35003 solver.cpp:239] Iteration 109660 (2.67522 iter/s, 3.73801s/10 iters), loss = 6.9058
I0523 01:18:59.331713 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9058 (* 1 = 6.9058 loss)
I0523 01:18:59.421185 35003 sgd_solver.cpp:112] Iteration 109660, lr = 0.01
I0523 01:19:00.823312 35003 solver.cpp:239] Iteration 109670 (6.70456 iter/s, 1.49152s/10 iters), loss = 8.21688
I0523 01:19:00.823359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.21688 (* 1 = 8.21688 loss)
I0523 01:19:00.827287 35003 sgd_solver.cpp:112] Iteration 109670, lr = 0.01
I0523 01:19:05.211987 35003 solver.cpp:239] Iteration 109680 (2.27871 iter/s, 4.38844s/10 iters), loss = 7.6416
I0523 01:19:05.212261 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6416 (* 1 = 7.6416 loss)
I0523 01:19:05.227053 35003 sgd_solver.cpp:112] Iteration 109680, lr = 0.01
I0523 01:19:08.771881 35003 solver.cpp:239] Iteration 109690 (2.80938 iter/s, 3.5595s/10 iters), loss = 7.95782
I0523 01:19:08.771922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95782 (* 1 = 7.95782 loss)
I0523 01:19:09.500254 35003 sgd_solver.cpp:112] Iteration 109690, lr = 0.01
I0523 01:19:13.420405 35003 solver.cpp:239] Iteration 109700 (2.15133 iter/s, 4.64828s/10 iters), loss = 6.36209
I0523 01:19:13.420465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36209 (* 1 = 6.36209 loss)
I0523 01:19:13.445462 35003 sgd_solver.cpp:112] Iteration 109700, lr = 0.01
I0523 01:19:18.441107 35003 solver.cpp:239] Iteration 109710 (1.99186 iter/s, 5.02044s/10 iters), loss = 6.88673
I0523 01:19:18.441166 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88673 (* 1 = 6.88673 loss)
I0523 01:19:18.463759 35003 sgd_solver.cpp:112] Iteration 109710, lr = 0.01
I0523 01:19:22.131273 35003 solver.cpp:239] Iteration 109720 (2.71006 iter/s, 3.68996s/10 iters), loss = 6.12829
I0523 01:19:22.131320 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12829 (* 1 = 6.12829 loss)
I0523 01:19:22.859517 35003 sgd_solver.cpp:112] Iteration 109720, lr = 0.01
I0523 01:19:27.832408 35003 solver.cpp:239] Iteration 109730 (1.75412 iter/s, 5.70085s/10 iters), loss = 6.68373
I0523 01:19:27.832464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68373 (* 1 = 6.68373 loss)
I0523 01:19:27.841814 35003 sgd_solver.cpp:112] Iteration 109730, lr = 0.01
I0523 01:19:32.450906 35003 solver.cpp:239] Iteration 109740 (2.16533 iter/s, 4.61824s/10 iters), loss = 7.54799
I0523 01:19:32.450965 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54799 (* 1 = 7.54799 loss)
I0523 01:19:33.178738 35003 sgd_solver.cpp:112] Iteration 109740, lr = 0.01
I0523 01:19:35.709354 35003 solver.cpp:239] Iteration 109750 (3.06913 iter/s, 3.25825s/10 iters), loss = 6.29254
I0523 01:19:35.709573 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29254 (* 1 = 6.29254 loss)
I0523 01:19:35.713661 35003 sgd_solver.cpp:112] Iteration 109750, lr = 0.01
I0523 01:19:41.462206 35003 solver.cpp:239] Iteration 109760 (1.7384 iter/s, 5.75243s/10 iters), loss = 7.5506
I0523 01:19:41.462246 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5506 (* 1 = 7.5506 loss)
I0523 01:19:41.475294 35003 sgd_solver.cpp:112] Iteration 109760, lr = 0.01
I0523 01:19:46.949885 35003 solver.cpp:239] Iteration 109770 (1.82236 iter/s, 5.4874s/10 iters), loss = 7.73089
I0523 01:19:46.949945 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73089 (* 1 = 7.73089 loss)
I0523 01:19:46.962142 35003 sgd_solver.cpp:112] Iteration 109770, lr = 0.01
I0523 01:19:49.851485 35003 solver.cpp:239] Iteration 109780 (3.44659 iter/s, 2.90141s/10 iters), loss = 7.05475
I0523 01:19:49.851526 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05475 (* 1 = 7.05475 loss)
I0523 01:19:49.857269 35003 sgd_solver.cpp:112] Iteration 109780, lr = 0.01
I0523 01:19:53.168956 35003 solver.cpp:239] Iteration 109790 (3.01451 iter/s, 3.31729s/10 iters), loss = 6.5378
I0523 01:19:53.169006 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5378 (* 1 = 6.5378 loss)
I0523 01:19:53.890066 35003 sgd_solver.cpp:112] Iteration 109790, lr = 0.01
I0523 01:19:57.967072 35003 solver.cpp:239] Iteration 109800 (2.08426 iter/s, 4.79787s/10 iters), loss = 7.221
I0523 01:19:57.967123 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.221 (* 1 = 7.221 loss)
I0523 01:19:57.980533 35003 sgd_solver.cpp:112] Iteration 109800, lr = 0.01
I0523 01:20:00.006099 35003 solver.cpp:239] Iteration 109810 (4.90463 iter/s, 2.03889s/10 iters), loss = 8.24485
I0523 01:20:00.006142 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.24485 (* 1 = 8.24485 loss)
I0523 01:20:00.019176 35003 sgd_solver.cpp:112] Iteration 109810, lr = 0.01
I0523 01:20:03.438441 35003 solver.cpp:239] Iteration 109820 (2.91362 iter/s, 3.43216s/10 iters), loss = 6.36097
I0523 01:20:03.438479 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36097 (* 1 = 6.36097 loss)
I0523 01:20:03.443964 35003 sgd_solver.cpp:112] Iteration 109820, lr = 0.01
I0523 01:20:06.339630 35003 solver.cpp:239] Iteration 109830 (3.44707 iter/s, 2.90101s/10 iters), loss = 7.95351
I0523 01:20:06.339795 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95351 (* 1 = 7.95351 loss)
I0523 01:20:06.343165 35003 sgd_solver.cpp:112] Iteration 109830, lr = 0.01
I0523 01:20:08.506460 35003 solver.cpp:239] Iteration 109840 (4.61566 iter/s, 2.16654s/10 iters), loss = 6.99531
I0523 01:20:08.506505 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99531 (* 1 = 6.99531 loss)
I0523 01:20:08.513363 35003 sgd_solver.cpp:112] Iteration 109840, lr = 0.01
I0523 01:20:12.203197 35003 solver.cpp:239] Iteration 109850 (2.70523 iter/s, 3.69654s/10 iters), loss = 6.53354
I0523 01:20:12.203248 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53354 (* 1 = 6.53354 loss)
I0523 01:20:12.213723 35003 sgd_solver.cpp:112] Iteration 109850, lr = 0.01
I0523 01:20:16.304812 35003 solver.cpp:239] Iteration 109860 (2.4382 iter/s, 4.10139s/10 iters), loss = 7.91761
I0523 01:20:16.304864 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91761 (* 1 = 7.91761 loss)
I0523 01:20:16.941956 35003 sgd_solver.cpp:112] Iteration 109860, lr = 0.01
I0523 01:20:20.389010 35003 solver.cpp:239] Iteration 109870 (2.44859 iter/s, 4.08398s/10 iters), loss = 7.03235
I0523 01:20:20.389065 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03235 (* 1 = 7.03235 loss)
I0523 01:20:21.109745 35003 sgd_solver.cpp:112] Iteration 109870, lr = 0.01
I0523 01:20:25.363584 35003 solver.cpp:239] Iteration 109880 (2.01033 iter/s, 4.97431s/10 iters), loss = 6.7222
I0523 01:20:25.363631 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7222 (* 1 = 6.7222 loss)
I0523 01:20:26.094986 35003 sgd_solver.cpp:112] Iteration 109880, lr = 0.01
I0523 01:20:29.738739 35003 solver.cpp:239] Iteration 109890 (2.28575 iter/s, 4.37493s/10 iters), loss = 7.4595
I0523 01:20:29.738786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4595 (* 1 = 7.4595 loss)
I0523 01:20:29.746717 35003 sgd_solver.cpp:112] Iteration 109890, lr = 0.01
I0523 01:20:33.447991 35003 solver.cpp:239] Iteration 109900 (2.69611 iter/s, 3.70905s/10 iters), loss = 7.65466
I0523 01:20:33.448050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65466 (* 1 = 7.65466 loss)
I0523 01:20:34.130009 35003 sgd_solver.cpp:112] Iteration 109900, lr = 0.01
I0523 01:20:37.057718 35003 solver.cpp:239] Iteration 109910 (2.77045 iter/s, 3.60952s/10 iters), loss = 7.72946
I0523 01:20:37.057960 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72946 (* 1 = 7.72946 loss)
I0523 01:20:37.080600 35003 sgd_solver.cpp:112] Iteration 109910, lr = 0.01
I0523 01:20:41.543355 35003 solver.cpp:239] Iteration 109920 (2.22953 iter/s, 4.48525s/10 iters), loss = 6.71515
I0523 01:20:41.543401 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71515 (* 1 = 6.71515 loss)
I0523 01:20:41.557090 35003 sgd_solver.cpp:112] Iteration 109920, lr = 0.01
I0523 01:20:44.388314 35003 solver.cpp:239] Iteration 109930 (3.5152 iter/s, 2.84479s/10 iters), loss = 6.90075
I0523 01:20:44.388358 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90075 (* 1 = 6.90075 loss)
I0523 01:20:44.406703 35003 sgd_solver.cpp:112] Iteration 109930, lr = 0.01
I0523 01:20:48.446264 35003 solver.cpp:239] Iteration 109940 (2.46443 iter/s, 4.05773s/10 iters), loss = 7.74957
I0523 01:20:48.446334 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74957 (* 1 = 7.74957 loss)
I0523 01:20:48.459756 35003 sgd_solver.cpp:112] Iteration 109940, lr = 0.01
I0523 01:20:51.335188 35003 solver.cpp:239] Iteration 109950 (3.46173 iter/s, 2.88873s/10 iters), loss = 6.86988
I0523 01:20:51.335243 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86988 (* 1 = 6.86988 loss)
I0523 01:20:51.348151 35003 sgd_solver.cpp:112] Iteration 109950, lr = 0.01
I0523 01:20:54.591984 35003 solver.cpp:239] Iteration 109960 (3.07068 iter/s, 3.25661s/10 iters), loss = 7.71677
I0523 01:20:54.592033 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71677 (* 1 = 7.71677 loss)
I0523 01:20:55.307291 35003 sgd_solver.cpp:112] Iteration 109960, lr = 0.01
I0523 01:20:58.094918 35003 solver.cpp:239] Iteration 109970 (2.85493 iter/s, 3.50272s/10 iters), loss = 6.91122
I0523 01:20:58.094966 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91122 (* 1 = 6.91122 loss)
I0523 01:20:58.653185 35003 sgd_solver.cpp:112] Iteration 109970, lr = 0.01
I0523 01:21:01.292208 35003 solver.cpp:239] Iteration 109980 (3.12782 iter/s, 3.19711s/10 iters), loss = 8.00239
I0523 01:21:01.292250 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00239 (* 1 = 8.00239 loss)
I0523 01:21:01.584944 35003 sgd_solver.cpp:112] Iteration 109980, lr = 0.01
I0523 01:21:05.577435 35003 solver.cpp:239] Iteration 109990 (2.33373 iter/s, 4.28498s/10 iters), loss = 7.94974
I0523 01:21:05.577493 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94974 (* 1 = 7.94974 loss)
I0523 01:21:05.662127 35003 sgd_solver.cpp:112] Iteration 109990, lr = 0.01
I0523 01:21:06.908133 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_110000.caffemodel
I0523 01:21:07.385126 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_110000.solverstate
I0523 01:21:07.503520 35003 solver.cpp:239] Iteration 110000 (5.19228 iter/s, 1.92594s/10 iters), loss = 8.62047
I0523 01:21:07.503569 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.62047 (* 1 = 8.62047 loss)
I0523 01:21:07.508289 35003 sgd_solver.cpp:112] Iteration 110000, lr = 0.01
I0523 01:21:08.478961 35003 solver.cpp:239] Iteration 110010 (10.2529 iter/s, 0.975333s/10 iters), loss = 6.8288
I0523 01:21:08.479013 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8288 (* 1 = 6.8288 loss)
I0523 01:21:08.481945 35003 sgd_solver.cpp:112] Iteration 110010, lr = 0.01
I0523 01:21:09.447556 35003 solver.cpp:239] Iteration 110020 (10.3253 iter/s, 0.968491s/10 iters), loss = 6.79778
I0523 01:21:09.447599 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79778 (* 1 = 6.79778 loss)
I0523 01:21:09.456892 35003 sgd_solver.cpp:112] Iteration 110020, lr = 0.01
I0523 01:21:10.285732 35003 solver.cpp:239] Iteration 110030 (11.932 iter/s, 0.838085s/10 iters), loss = 6.72275
I0523 01:21:10.285778 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72275 (* 1 = 6.72275 loss)
I0523 01:21:10.291563 35003 sgd_solver.cpp:112] Iteration 110030, lr = 0.01
I0523 01:21:11.405395 35003 solver.cpp:239] Iteration 110040 (8.93207 iter/s, 1.11956s/10 iters), loss = 7.6748
I0523 01:21:11.405450 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6748 (* 1 = 7.6748 loss)
I0523 01:21:11.413981 35003 sgd_solver.cpp:112] Iteration 110040, lr = 0.01
I0523 01:21:13.976881 35003 solver.cpp:239] Iteration 110050 (3.88906 iter/s, 2.57132s/10 iters), loss = 8.11268
I0523 01:21:13.976927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.11268 (* 1 = 8.11268 loss)
I0523 01:21:13.988734 35003 sgd_solver.cpp:112] Iteration 110050, lr = 0.01
I0523 01:21:16.671897 35003 solver.cpp:239] Iteration 110060 (3.71079 iter/s, 2.69484s/10 iters), loss = 7.70734
I0523 01:21:16.671946 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70734 (* 1 = 7.70734 loss)
I0523 01:21:17.316344 35003 sgd_solver.cpp:112] Iteration 110060, lr = 0.01
I0523 01:21:20.163295 35003 solver.cpp:239] Iteration 110070 (2.86434 iter/s, 3.4912s/10 iters), loss = 8.34515
I0523 01:21:20.163342 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.34515 (* 1 = 8.34515 loss)
I0523 01:21:20.423576 35003 sgd_solver.cpp:112] Iteration 110070, lr = 0.01
I0523 01:21:23.963752 35003 solver.cpp:239] Iteration 110080 (2.6314 iter/s, 3.80025s/10 iters), loss = 7.18664
I0523 01:21:23.963793 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18664 (* 1 = 7.18664 loss)
I0523 01:21:23.988869 35003 sgd_solver.cpp:112] Iteration 110080, lr = 0.01
I0523 01:21:26.914811 35003 solver.cpp:239] Iteration 110090 (3.3888 iter/s, 2.95089s/10 iters), loss = 7.83093
I0523 01:21:26.914858 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83093 (* 1 = 7.83093 loss)
I0523 01:21:27.623317 35003 sgd_solver.cpp:112] Iteration 110090, lr = 0.01
I0523 01:21:31.424340 35003 solver.cpp:239] Iteration 110100 (2.21764 iter/s, 4.5093s/10 iters), loss = 6.6913
I0523 01:21:31.424388 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6913 (* 1 = 6.6913 loss)
I0523 01:21:31.427242 35003 sgd_solver.cpp:112] Iteration 110100, lr = 0.01
I0523 01:21:35.321383 35003 solver.cpp:239] Iteration 110110 (2.56618 iter/s, 3.89684s/10 iters), loss = 7.23444
I0523 01:21:35.321434 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23444 (* 1 = 7.23444 loss)
I0523 01:21:36.060688 35003 sgd_solver.cpp:112] Iteration 110110, lr = 0.01
I0523 01:21:38.843044 35003 solver.cpp:239] Iteration 110120 (2.83973 iter/s, 3.52146s/10 iters), loss = 7.7579
I0523 01:21:38.843297 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7579 (* 1 = 7.7579 loss)
I0523 01:21:38.856498 35003 sgd_solver.cpp:112] Iteration 110120, lr = 0.01
I0523 01:21:42.971393 35003 solver.cpp:239] Iteration 110130 (2.4225 iter/s, 4.12796s/10 iters), loss = 7.48228
I0523 01:21:42.971436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48228 (* 1 = 7.48228 loss)
I0523 01:21:42.996006 35003 sgd_solver.cpp:112] Iteration 110130, lr = 0.01
I0523 01:21:47.458639 35003 solver.cpp:239] Iteration 110140 (2.22865 iter/s, 4.48702s/10 iters), loss = 7.13428
I0523 01:21:47.458679 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13428 (* 1 = 7.13428 loss)
I0523 01:21:48.196702 35003 sgd_solver.cpp:112] Iteration 110140, lr = 0.01
I0523 01:21:51.219696 35003 solver.cpp:239] Iteration 110150 (2.65899 iter/s, 3.76083s/10 iters), loss = 6.87002
I0523 01:21:51.219750 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87002 (* 1 = 6.87002 loss)
I0523 01:21:51.939059 35003 sgd_solver.cpp:112] Iteration 110150, lr = 0.01
I0523 01:21:54.595608 35003 solver.cpp:239] Iteration 110160 (2.96234 iter/s, 3.37571s/10 iters), loss = 8.31713
I0523 01:21:54.595651 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.31713 (* 1 = 8.31713 loss)
I0523 01:21:54.607823 35003 sgd_solver.cpp:112] Iteration 110160, lr = 0.01
I0523 01:21:58.639901 35003 solver.cpp:239] Iteration 110170 (2.47275 iter/s, 4.04407s/10 iters), loss = 6.76155
I0523 01:21:58.639962 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76155 (* 1 = 6.76155 loss)
I0523 01:21:58.748484 35003 sgd_solver.cpp:112] Iteration 110170, lr = 0.01
I0523 01:22:01.299952 35003 solver.cpp:239] Iteration 110180 (3.75958 iter/s, 2.65987s/10 iters), loss = 7.13247
I0523 01:22:01.299996 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13247 (* 1 = 7.13247 loss)
I0523 01:22:01.994339 35003 sgd_solver.cpp:112] Iteration 110180, lr = 0.01
I0523 01:22:05.526623 35003 solver.cpp:239] Iteration 110190 (2.36605 iter/s, 4.22646s/10 iters), loss = 6.21131
I0523 01:22:05.526669 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21131 (* 1 = 6.21131 loss)
I0523 01:22:05.539747 35003 sgd_solver.cpp:112] Iteration 110190, lr = 0.01
I0523 01:22:09.082890 35003 solver.cpp:239] Iteration 110200 (2.81209 iter/s, 3.55607s/10 iters), loss = 7.35693
I0523 01:22:09.083117 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35693 (* 1 = 7.35693 loss)
I0523 01:22:09.114382 35003 sgd_solver.cpp:112] Iteration 110200, lr = 0.01
I0523 01:22:12.780787 35003 solver.cpp:239] Iteration 110210 (2.70452 iter/s, 3.69751s/10 iters), loss = 7.33273
I0523 01:22:12.780841 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33273 (* 1 = 7.33273 loss)
I0523 01:22:13.442930 35003 sgd_solver.cpp:112] Iteration 110210, lr = 0.01
I0523 01:22:16.513681 35003 solver.cpp:239] Iteration 110220 (2.67904 iter/s, 3.73268s/10 iters), loss = 8.10536
I0523 01:22:16.513725 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.10536 (* 1 = 8.10536 loss)
I0523 01:22:16.526446 35003 sgd_solver.cpp:112] Iteration 110220, lr = 0.01
I0523 01:22:20.105026 35003 solver.cpp:239] Iteration 110230 (2.78462 iter/s, 3.59115s/10 iters), loss = 6.87678
I0523 01:22:20.105069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87678 (* 1 = 6.87678 loss)
I0523 01:22:20.425946 35003 sgd_solver.cpp:112] Iteration 110230, lr = 0.01
I0523 01:22:23.115430 35003 solver.cpp:239] Iteration 110240 (3.322 iter/s, 3.01023s/10 iters), loss = 6.94871
I0523 01:22:23.115485 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94871 (* 1 = 6.94871 loss)
I0523 01:22:23.517850 35003 sgd_solver.cpp:112] Iteration 110240, lr = 0.01
I0523 01:22:25.181475 35003 solver.cpp:239] Iteration 110250 (4.84052 iter/s, 2.06589s/10 iters), loss = 6.85071
I0523 01:22:25.181536 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85071 (* 1 = 6.85071 loss)
I0523 01:22:25.194631 35003 sgd_solver.cpp:112] Iteration 110250, lr = 0.01
I0523 01:22:29.029076 35003 solver.cpp:239] Iteration 110260 (2.59917 iter/s, 3.84739s/10 iters), loss = 8.60573
I0523 01:22:29.029127 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.60573 (* 1 = 8.60573 loss)
I0523 01:22:29.038688 35003 sgd_solver.cpp:112] Iteration 110260, lr = 0.01
I0523 01:22:31.861765 35003 solver.cpp:239] Iteration 110270 (3.53043 iter/s, 2.83252s/10 iters), loss = 6.98626
I0523 01:22:31.861820 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98626 (* 1 = 6.98626 loss)
I0523 01:22:32.589666 35003 sgd_solver.cpp:112] Iteration 110270, lr = 0.01
I0523 01:22:36.457916 35003 solver.cpp:239] Iteration 110280 (2.17585 iter/s, 4.59591s/10 iters), loss = 7.80238
I0523 01:22:36.457967 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80238 (* 1 = 7.80238 loss)
I0523 01:22:36.477902 35003 sgd_solver.cpp:112] Iteration 110280, lr = 0.01
I0523 01:22:40.648433 35003 solver.cpp:239] Iteration 110290 (2.38647 iter/s, 4.19029s/10 iters), loss = 7.67234
I0523 01:22:40.648562 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67234 (* 1 = 7.67234 loss)
I0523 01:22:40.654829 35003 sgd_solver.cpp:112] Iteration 110290, lr = 0.01
I0523 01:22:44.691829 35003 solver.cpp:239] Iteration 110300 (2.47335 iter/s, 4.0431s/10 iters), loss = 5.49925
I0523 01:22:44.691889 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.49925 (* 1 = 5.49925 loss)
I0523 01:22:44.704550 35003 sgd_solver.cpp:112] Iteration 110300, lr = 0.01
I0523 01:22:48.477936 35003 solver.cpp:239] Iteration 110310 (2.64138 iter/s, 3.78589s/10 iters), loss = 6.87512
I0523 01:22:48.477972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87512 (* 1 = 6.87512 loss)
I0523 01:22:48.491528 35003 sgd_solver.cpp:112] Iteration 110310, lr = 0.01
I0523 01:22:53.219799 35003 solver.cpp:239] Iteration 110320 (2.10898 iter/s, 4.74164s/10 iters), loss = 7.03904
I0523 01:22:53.219846 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03904 (* 1 = 7.03904 loss)
I0523 01:22:53.238322 35003 sgd_solver.cpp:112] Iteration 110320, lr = 0.01
I0523 01:22:56.071132 35003 solver.cpp:239] Iteration 110330 (3.50735 iter/s, 2.85116s/10 iters), loss = 7.62445
I0523 01:22:56.071182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62445 (* 1 = 7.62445 loss)
I0523 01:22:56.779539 35003 sgd_solver.cpp:112] Iteration 110330, lr = 0.01
I0523 01:22:59.739593 35003 solver.cpp:239] Iteration 110340 (2.72609 iter/s, 3.66825s/10 iters), loss = 6.85324
I0523 01:22:59.739655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85324 (* 1 = 6.85324 loss)
I0523 01:23:00.369204 35003 sgd_solver.cpp:112] Iteration 110340, lr = 0.01
I0523 01:23:03.980259 35003 solver.cpp:239] Iteration 110350 (2.35825 iter/s, 4.24043s/10 iters), loss = 7.60826
I0523 01:23:03.980303 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60826 (* 1 = 7.60826 loss)
I0523 01:23:03.993454 35003 sgd_solver.cpp:112] Iteration 110350, lr = 0.01
I0523 01:23:06.125547 35003 solver.cpp:239] Iteration 110360 (4.66168 iter/s, 2.14515s/10 iters), loss = 6.7149
I0523 01:23:06.125594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7149 (* 1 = 6.7149 loss)
I0523 01:23:06.138692 35003 sgd_solver.cpp:112] Iteration 110360, lr = 0.01
I0523 01:23:08.920251 35003 solver.cpp:239] Iteration 110370 (3.57841 iter/s, 2.79453s/10 iters), loss = 7.44932
I0523 01:23:08.920295 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44932 (* 1 = 7.44932 loss)
I0523 01:23:08.925436 35003 sgd_solver.cpp:112] Iteration 110370, lr = 0.01
I0523 01:23:11.694825 35003 solver.cpp:239] Iteration 110380 (3.60438 iter/s, 2.7744s/10 iters), loss = 8.16919
I0523 01:23:11.695149 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16919 (* 1 = 8.16919 loss)
I0523 01:23:12.403143 35003 sgd_solver.cpp:112] Iteration 110380, lr = 0.01
I0523 01:23:15.193451 35003 solver.cpp:239] Iteration 110390 (2.85862 iter/s, 3.49819s/10 iters), loss = 7.06915
I0523 01:23:15.193506 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06915 (* 1 = 7.06915 loss)
I0523 01:23:15.205613 35003 sgd_solver.cpp:112] Iteration 110390, lr = 0.01
I0523 01:23:17.391993 35003 solver.cpp:239] Iteration 110400 (4.5488 iter/s, 2.19838s/10 iters), loss = 6.63942
I0523 01:23:17.392048 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63942 (* 1 = 6.63942 loss)
I0523 01:23:18.063787 35003 sgd_solver.cpp:112] Iteration 110400, lr = 0.01
I0523 01:23:22.969264 35003 solver.cpp:239] Iteration 110410 (1.79308 iter/s, 5.57698s/10 iters), loss = 6.42707
I0523 01:23:22.969321 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42707 (* 1 = 6.42707 loss)
I0523 01:23:23.704006 35003 sgd_solver.cpp:112] Iteration 110410, lr = 0.01
I0523 01:23:28.006198 35003 solver.cpp:239] Iteration 110420 (1.98544 iter/s, 5.03667s/10 iters), loss = 7.19276
I0523 01:23:28.006242 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19276 (* 1 = 7.19276 loss)
I0523 01:23:28.019544 35003 sgd_solver.cpp:112] Iteration 110420, lr = 0.01
I0523 01:23:32.248497 35003 solver.cpp:239] Iteration 110430 (2.35734 iter/s, 4.24208s/10 iters), loss = 7.32765
I0523 01:23:32.248548 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32765 (* 1 = 7.32765 loss)
I0523 01:23:32.254093 35003 sgd_solver.cpp:112] Iteration 110430, lr = 0.01
I0523 01:23:34.914225 35003 solver.cpp:239] Iteration 110440 (3.75155 iter/s, 2.66556s/10 iters), loss = 7.07566
I0523 01:23:34.914263 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07566 (* 1 = 7.07566 loss)
I0523 01:23:34.922459 35003 sgd_solver.cpp:112] Iteration 110440, lr = 0.01
I0523 01:23:37.727775 35003 solver.cpp:239] Iteration 110450 (3.55443 iter/s, 2.81339s/10 iters), loss = 5.97031
I0523 01:23:37.727814 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97031 (* 1 = 5.97031 loss)
I0523 01:23:37.745687 35003 sgd_solver.cpp:112] Iteration 110450, lr = 0.01
I0523 01:23:41.943945 35003 solver.cpp:239] Iteration 110460 (2.37194 iter/s, 4.21595s/10 iters), loss = 8.19703
I0523 01:23:41.944164 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.19703 (* 1 = 8.19703 loss)
I0523 01:23:41.952528 35003 sgd_solver.cpp:112] Iteration 110460, lr = 0.01
I0523 01:23:46.210522 35003 solver.cpp:239] Iteration 110470 (2.344 iter/s, 4.26621s/10 iters), loss = 6.99515
I0523 01:23:46.210572 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99515 (* 1 = 6.99515 loss)
I0523 01:23:46.222695 35003 sgd_solver.cpp:112] Iteration 110470, lr = 0.01
I0523 01:23:48.351018 35003 solver.cpp:239] Iteration 110480 (4.67212 iter/s, 2.14035s/10 iters), loss = 7.7715
I0523 01:23:48.351056 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7715 (* 1 = 7.7715 loss)
I0523 01:23:48.358042 35003 sgd_solver.cpp:112] Iteration 110480, lr = 0.01
I0523 01:23:50.384423 35003 solver.cpp:239] Iteration 110490 (4.91816 iter/s, 2.03328s/10 iters), loss = 6.38539
I0523 01:23:50.384464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38539 (* 1 = 6.38539 loss)
I0523 01:23:50.390111 35003 sgd_solver.cpp:112] Iteration 110490, lr = 0.01
I0523 01:23:52.452597 35003 solver.cpp:239] Iteration 110500 (4.83549 iter/s, 2.06804s/10 iters), loss = 6.33117
I0523 01:23:52.452633 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33117 (* 1 = 6.33117 loss)
I0523 01:23:52.465625 35003 sgd_solver.cpp:112] Iteration 110500, lr = 0.01
I0523 01:23:55.910768 35003 solver.cpp:239] Iteration 110510 (2.89186 iter/s, 3.45798s/10 iters), loss = 6.5922
I0523 01:23:55.910818 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5922 (* 1 = 6.5922 loss)
I0523 01:23:55.917482 35003 sgd_solver.cpp:112] Iteration 110510, lr = 0.01
I0523 01:23:59.493206 35003 solver.cpp:239] Iteration 110520 (2.79157 iter/s, 3.58222s/10 iters), loss = 7.08547
I0523 01:23:59.493263 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08547 (* 1 = 7.08547 loss)
I0523 01:24:00.188102 35003 sgd_solver.cpp:112] Iteration 110520, lr = 0.01
I0523 01:24:02.157879 35003 solver.cpp:239] Iteration 110530 (3.75304 iter/s, 2.6645s/10 iters), loss = 7.23318
I0523 01:24:02.157927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23318 (* 1 = 7.23318 loss)
I0523 01:24:02.279898 35003 sgd_solver.cpp:112] Iteration 110530, lr = 0.01
I0523 01:24:05.111598 35003 solver.cpp:239] Iteration 110540 (3.38577 iter/s, 2.95354s/10 iters), loss = 7.12834
I0523 01:24:05.111657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12834 (* 1 = 7.12834 loss)
I0523 01:24:05.115895 35003 sgd_solver.cpp:112] Iteration 110540, lr = 0.01
I0523 01:24:09.847604 35003 solver.cpp:239] Iteration 110550 (2.11161 iter/s, 4.73572s/10 iters), loss = 7.85108
I0523 01:24:09.847676 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85108 (* 1 = 7.85108 loss)
I0523 01:24:10.588766 35003 sgd_solver.cpp:112] Iteration 110550, lr = 0.01
I0523 01:24:13.767596 35003 solver.cpp:239] Iteration 110560 (2.55118 iter/s, 3.91976s/10 iters), loss = 7.22097
I0523 01:24:13.767763 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22097 (* 1 = 7.22097 loss)
I0523 01:24:13.780911 35003 sgd_solver.cpp:112] Iteration 110560, lr = 0.01
I0523 01:24:17.242499 35003 solver.cpp:239] Iteration 110570 (2.87803 iter/s, 3.47459s/10 iters), loss = 7.63665
I0523 01:24:17.242547 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63665 (* 1 = 7.63665 loss)
I0523 01:24:17.249662 35003 sgd_solver.cpp:112] Iteration 110570, lr = 0.01
I0523 01:24:20.629853 35003 solver.cpp:239] Iteration 110580 (2.95232 iter/s, 3.38716s/10 iters), loss = 6.61022
I0523 01:24:20.629900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61022 (* 1 = 6.61022 loss)
I0523 01:24:21.364256 35003 sgd_solver.cpp:112] Iteration 110580, lr = 0.01
I0523 01:24:25.216630 35003 solver.cpp:239] Iteration 110590 (2.18029 iter/s, 4.58654s/10 iters), loss = 7.01341
I0523 01:24:25.216675 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01341 (* 1 = 7.01341 loss)
I0523 01:24:25.238292 35003 sgd_solver.cpp:112] Iteration 110590, lr = 0.01
I0523 01:24:28.968437 35003 solver.cpp:239] Iteration 110600 (2.66553 iter/s, 3.7516s/10 iters), loss = 7.05381
I0523 01:24:28.968477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05381 (* 1 = 7.05381 loss)
I0523 01:24:28.981385 35003 sgd_solver.cpp:112] Iteration 110600, lr = 0.01
I0523 01:24:32.193461 35003 solver.cpp:239] Iteration 110610 (3.10092 iter/s, 3.22485s/10 iters), loss = 7.53678
I0523 01:24:32.193519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53678 (* 1 = 7.53678 loss)
I0523 01:24:32.928375 35003 sgd_solver.cpp:112] Iteration 110610, lr = 0.01
I0523 01:24:34.893103 35003 solver.cpp:239] Iteration 110620 (3.70446 iter/s, 2.69945s/10 iters), loss = 7.33562
I0523 01:24:34.893149 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33562 (* 1 = 7.33562 loss)
I0523 01:24:34.897080 35003 sgd_solver.cpp:112] Iteration 110620, lr = 0.01
I0523 01:24:39.872969 35003 solver.cpp:239] Iteration 110630 (2.0082 iter/s, 4.9796s/10 iters), loss = 7.96002
I0523 01:24:39.873021 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96002 (* 1 = 7.96002 loss)
I0523 01:24:39.881012 35003 sgd_solver.cpp:112] Iteration 110630, lr = 0.01
I0523 01:24:43.563395 35003 solver.cpp:239] Iteration 110640 (2.70987 iter/s, 3.69022s/10 iters), loss = 7.38868
I0523 01:24:43.563449 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38868 (* 1 = 7.38868 loss)
I0523 01:24:44.297857 35003 sgd_solver.cpp:112] Iteration 110640, lr = 0.01
I0523 01:24:47.169394 35003 solver.cpp:239] Iteration 110650 (2.77332 iter/s, 3.60579s/10 iters), loss = 7.63606
I0523 01:24:47.169445 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63606 (* 1 = 7.63606 loss)
I0523 01:24:47.867487 35003 sgd_solver.cpp:112] Iteration 110650, lr = 0.01
I0523 01:24:49.829654 35003 solver.cpp:239] Iteration 110660 (3.75926 iter/s, 2.6601s/10 iters), loss = 7.96511
I0523 01:24:49.829692 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96511 (* 1 = 7.96511 loss)
I0523 01:24:49.842705 35003 sgd_solver.cpp:112] Iteration 110660, lr = 0.01
I0523 01:24:53.553207 35003 solver.cpp:239] Iteration 110670 (2.68575 iter/s, 3.72336s/10 iters), loss = 7.25554
I0523 01:24:53.553264 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25554 (* 1 = 7.25554 loss)
I0523 01:24:54.273947 35003 sgd_solver.cpp:112] Iteration 110670, lr = 0.01
I0523 01:24:57.383787 35003 solver.cpp:239] Iteration 110680 (2.61072 iter/s, 3.83037s/10 iters), loss = 7.82211
I0523 01:24:57.383836 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82211 (* 1 = 7.82211 loss)
I0523 01:24:57.516831 35003 sgd_solver.cpp:112] Iteration 110680, lr = 0.01
I0523 01:25:00.381997 35003 solver.cpp:239] Iteration 110690 (3.33551 iter/s, 2.99804s/10 iters), loss = 7.64429
I0523 01:25:00.382040 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64429 (* 1 = 7.64429 loss)
I0523 01:25:01.122437 35003 sgd_solver.cpp:112] Iteration 110690, lr = 0.01
I0523 01:25:03.415505 35003 solver.cpp:239] Iteration 110700 (3.2967 iter/s, 3.03333s/10 iters), loss = 7.19665
I0523 01:25:03.415551 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19665 (* 1 = 7.19665 loss)
I0523 01:25:04.141872 35003 sgd_solver.cpp:112] Iteration 110700, lr = 0.01
I0523 01:25:09.084820 35003 solver.cpp:239] Iteration 110710 (1.76397 iter/s, 5.66904s/10 iters), loss = 7.24903
I0523 01:25:09.084864 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24903 (* 1 = 7.24903 loss)
I0523 01:25:09.799489 35003 sgd_solver.cpp:112] Iteration 110710, lr = 0.01
I0523 01:25:13.354717 35003 solver.cpp:239] Iteration 110720 (2.3421 iter/s, 4.26966s/10 iters), loss = 7.87493
I0523 01:25:13.354759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87493 (* 1 = 7.87493 loss)
I0523 01:25:13.368402 35003 sgd_solver.cpp:112] Iteration 110720, lr = 0.01
I0523 01:25:17.625893 35003 solver.cpp:239] Iteration 110730 (2.34139 iter/s, 4.27096s/10 iters), loss = 7.06749
I0523 01:25:17.626098 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06749 (* 1 = 7.06749 loss)
I0523 01:25:17.629097 35003 sgd_solver.cpp:112] Iteration 110730, lr = 0.01
I0523 01:25:18.957295 35003 solver.cpp:239] Iteration 110740 (7.51227 iter/s, 1.33115s/10 iters), loss = 6.77966
I0523 01:25:18.957334 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77966 (* 1 = 6.77966 loss)
I0523 01:25:18.959820 35003 sgd_solver.cpp:112] Iteration 110740, lr = 0.01
I0523 01:25:23.993701 35003 solver.cpp:239] Iteration 110750 (1.98564 iter/s, 5.03616s/10 iters), loss = 7.35078
I0523 01:25:23.993751 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35078 (* 1 = 7.35078 loss)
I0523 01:25:24.708393 35003 sgd_solver.cpp:112] Iteration 110750, lr = 0.01
I0523 01:25:27.593390 35003 solver.cpp:239] Iteration 110760 (2.77817 iter/s, 3.59949s/10 iters), loss = 6.92546
I0523 01:25:27.593441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92546 (* 1 = 6.92546 loss)
I0523 01:25:27.722837 35003 sgd_solver.cpp:112] Iteration 110760, lr = 0.01
I0523 01:25:30.554468 35003 solver.cpp:239] Iteration 110770 (3.37735 iter/s, 2.9609s/10 iters), loss = 6.02351
I0523 01:25:30.554510 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02351 (* 1 = 6.02351 loss)
I0523 01:25:30.581009 35003 sgd_solver.cpp:112] Iteration 110770, lr = 0.01
I0523 01:25:34.097606 35003 solver.cpp:239] Iteration 110780 (2.82251 iter/s, 3.54295s/10 iters), loss = 7.25259
I0523 01:25:34.097642 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25259 (* 1 = 7.25259 loss)
I0523 01:25:34.110957 35003 sgd_solver.cpp:112] Iteration 110780, lr = 0.01
I0523 01:25:38.486551 35003 solver.cpp:239] Iteration 110790 (2.27857 iter/s, 4.38872s/10 iters), loss = 7.6436
I0523 01:25:38.486594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6436 (* 1 = 7.6436 loss)
I0523 01:25:38.493120 35003 sgd_solver.cpp:112] Iteration 110790, lr = 0.01
I0523 01:25:43.334651 35003 solver.cpp:239] Iteration 110800 (2.06277 iter/s, 4.84784s/10 iters), loss = 7.00894
I0523 01:25:43.334764 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00894 (* 1 = 7.00894 loss)
I0523 01:25:43.906930 35003 sgd_solver.cpp:112] Iteration 110800, lr = 0.01
I0523 01:25:46.795002 35003 solver.cpp:239] Iteration 110810 (2.89012 iter/s, 3.46006s/10 iters), loss = 8.03801
I0523 01:25:46.795054 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03801 (* 1 = 8.03801 loss)
I0523 01:25:46.799266 35003 sgd_solver.cpp:112] Iteration 110810, lr = 0.01
I0523 01:25:50.567963 35003 solver.cpp:239] Iteration 110820 (2.65059 iter/s, 3.77275s/10 iters), loss = 8.16438
I0523 01:25:50.569478 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16438 (* 1 = 8.16438 loss)
I0523 01:25:50.664777 35003 sgd_solver.cpp:112] Iteration 110820, lr = 0.01
I0523 01:25:54.146086 35003 solver.cpp:239] Iteration 110830 (2.79919 iter/s, 3.57246s/10 iters), loss = 7.16551
I0523 01:25:54.146150 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16551 (* 1 = 7.16551 loss)
I0523 01:25:54.158581 35003 sgd_solver.cpp:112] Iteration 110830, lr = 0.01
I0523 01:25:55.454211 35003 solver.cpp:239] Iteration 110840 (7.64522 iter/s, 1.30801s/10 iters), loss = 6.73271
I0523 01:25:55.454254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73271 (* 1 = 6.73271 loss)
I0523 01:25:55.468287 35003 sgd_solver.cpp:112] Iteration 110840, lr = 0.01
I0523 01:25:59.272292 35003 solver.cpp:239] Iteration 110850 (2.61926 iter/s, 3.81787s/10 iters), loss = 7.33454
I0523 01:25:59.272363 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33454 (* 1 = 7.33454 loss)
I0523 01:25:59.290607 35003 sgd_solver.cpp:112] Iteration 110850, lr = 0.01
I0523 01:26:02.070102 35003 solver.cpp:239] Iteration 110860 (3.57447 iter/s, 2.79762s/10 iters), loss = 9.16644
I0523 01:26:02.070152 35003 solver.cpp:258]     Train net output #0: softmax_loss = 9.16644 (* 1 = 9.16644 loss)
I0523 01:26:02.076987 35003 sgd_solver.cpp:112] Iteration 110860, lr = 0.01
I0523 01:26:06.496767 35003 solver.cpp:239] Iteration 110870 (2.25916 iter/s, 4.42643s/10 iters), loss = 6.80194
I0523 01:26:06.496824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80194 (* 1 = 6.80194 loss)
I0523 01:26:06.502704 35003 sgd_solver.cpp:112] Iteration 110870, lr = 0.01
I0523 01:26:08.567679 35003 solver.cpp:239] Iteration 110880 (4.82913 iter/s, 2.07077s/10 iters), loss = 6.78079
I0523 01:26:08.567721 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78079 (* 1 = 6.78079 loss)
I0523 01:26:09.257179 35003 sgd_solver.cpp:112] Iteration 110880, lr = 0.01
I0523 01:26:12.193958 35003 solver.cpp:239] Iteration 110890 (2.75779 iter/s, 3.62609s/10 iters), loss = 7.7336
I0523 01:26:12.194008 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7336 (* 1 = 7.7336 loss)
I0523 01:26:12.206709 35003 sgd_solver.cpp:112] Iteration 110890, lr = 0.01
I0523 01:26:14.240705 35003 solver.cpp:239] Iteration 110900 (4.88615 iter/s, 2.0466s/10 iters), loss = 7.73503
I0523 01:26:14.240761 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73503 (* 1 = 7.73503 loss)
I0523 01:26:14.247858 35003 sgd_solver.cpp:112] Iteration 110900, lr = 0.01
I0523 01:26:18.028105 35003 solver.cpp:239] Iteration 110910 (2.64049 iter/s, 3.78718s/10 iters), loss = 6.63618
I0523 01:26:18.028151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63618 (* 1 = 6.63618 loss)
I0523 01:26:18.036350 35003 sgd_solver.cpp:112] Iteration 110910, lr = 0.01
I0523 01:26:23.061584 35003 solver.cpp:239] Iteration 110920 (1.9868 iter/s, 5.03323s/10 iters), loss = 7.22403
I0523 01:26:23.061832 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22403 (* 1 = 7.22403 loss)
I0523 01:26:23.078519 35003 sgd_solver.cpp:112] Iteration 110920, lr = 0.01
I0523 01:26:27.284389 35003 solver.cpp:239] Iteration 110930 (2.36832 iter/s, 4.2224s/10 iters), loss = 5.62147
I0523 01:26:27.284432 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.62147 (* 1 = 5.62147 loss)
I0523 01:26:27.287547 35003 sgd_solver.cpp:112] Iteration 110930, lr = 0.01
I0523 01:26:29.415859 35003 solver.cpp:239] Iteration 110940 (4.6919 iter/s, 2.13133s/10 iters), loss = 6.35791
I0523 01:26:29.415901 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35791 (* 1 = 6.35791 loss)
I0523 01:26:29.438496 35003 sgd_solver.cpp:112] Iteration 110940, lr = 0.01
I0523 01:26:33.213413 35003 solver.cpp:239] Iteration 110950 (2.63341 iter/s, 3.79736s/10 iters), loss = 6.78493
I0523 01:26:33.213454 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78493 (* 1 = 6.78493 loss)
I0523 01:26:33.346639 35003 sgd_solver.cpp:112] Iteration 110950, lr = 0.01
I0523 01:26:36.936139 35003 solver.cpp:239] Iteration 110960 (2.68635 iter/s, 3.72252s/10 iters), loss = 6.59232
I0523 01:26:36.936208 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59232 (* 1 = 6.59232 loss)
I0523 01:26:37.650547 35003 sgd_solver.cpp:112] Iteration 110960, lr = 0.01
I0523 01:26:40.376777 35003 solver.cpp:239] Iteration 110970 (2.90661 iter/s, 3.44043s/10 iters), loss = 6.581
I0523 01:26:40.376821 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.581 (* 1 = 6.581 loss)
I0523 01:26:40.400401 35003 sgd_solver.cpp:112] Iteration 110970, lr = 0.01
I0523 01:26:43.955381 35003 solver.cpp:239] Iteration 110980 (2.79454 iter/s, 3.57841s/10 iters), loss = 7.29207
I0523 01:26:43.955425 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29207 (* 1 = 7.29207 loss)
I0523 01:26:43.958334 35003 sgd_solver.cpp:112] Iteration 110980, lr = 0.01
I0523 01:26:46.866871 35003 solver.cpp:239] Iteration 110990 (3.43488 iter/s, 2.91131s/10 iters), loss = 7.28906
I0523 01:26:46.866928 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28906 (* 1 = 7.28906 loss)
I0523 01:26:47.527526 35003 sgd_solver.cpp:112] Iteration 110990, lr = 0.01
I0523 01:26:49.173933 35003 solver.cpp:239] Iteration 111000 (4.33482 iter/s, 2.3069s/10 iters), loss = 8.22823
I0523 01:26:49.173972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.22823 (* 1 = 8.22823 loss)
I0523 01:26:49.192520 35003 sgd_solver.cpp:112] Iteration 111000, lr = 0.01
I0523 01:26:52.680486 35003 solver.cpp:239] Iteration 111010 (2.85196 iter/s, 3.50636s/10 iters), loss = 7.08924
I0523 01:26:52.680541 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08924 (* 1 = 7.08924 loss)
I0523 01:26:52.693565 35003 sgd_solver.cpp:112] Iteration 111010, lr = 0.01
I0523 01:26:55.550685 35003 solver.cpp:239] Iteration 111020 (3.48429 iter/s, 2.87002s/10 iters), loss = 7.61976
I0523 01:26:55.550894 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61976 (* 1 = 7.61976 loss)
I0523 01:26:55.554512 35003 sgd_solver.cpp:112] Iteration 111020, lr = 0.01
I0523 01:26:59.131665 35003 solver.cpp:239] Iteration 111030 (2.7928 iter/s, 3.58064s/10 iters), loss = 6.66888
I0523 01:26:59.131705 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66888 (* 1 = 6.66888 loss)
I0523 01:26:59.146600 35003 sgd_solver.cpp:112] Iteration 111030, lr = 0.01
I0523 01:27:03.482983 35003 solver.cpp:239] Iteration 111040 (2.29827 iter/s, 4.3511s/10 iters), loss = 7.26758
I0523 01:27:03.483033 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26758 (* 1 = 7.26758 loss)
I0523 01:27:04.101550 35003 sgd_solver.cpp:112] Iteration 111040, lr = 0.01
I0523 01:27:06.125056 35003 solver.cpp:239] Iteration 111050 (3.78514 iter/s, 2.64191s/10 iters), loss = 7.81875
I0523 01:27:06.125097 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81875 (* 1 = 7.81875 loss)
I0523 01:27:06.866389 35003 sgd_solver.cpp:112] Iteration 111050, lr = 0.01
I0523 01:27:11.376256 35003 solver.cpp:239] Iteration 111060 (1.90442 iter/s, 5.25094s/10 iters), loss = 6.81782
I0523 01:27:11.376307 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81782 (* 1 = 6.81782 loss)
I0523 01:27:11.383908 35003 sgd_solver.cpp:112] Iteration 111060, lr = 0.01
I0523 01:27:14.370278 35003 solver.cpp:239] Iteration 111070 (3.34018 iter/s, 2.99385s/10 iters), loss = 6.83299
I0523 01:27:14.370326 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83299 (* 1 = 6.83299 loss)
I0523 01:27:14.383152 35003 sgd_solver.cpp:112] Iteration 111070, lr = 0.01
I0523 01:27:16.466928 35003 solver.cpp:239] Iteration 111080 (4.76983 iter/s, 2.09651s/10 iters), loss = 7.22077
I0523 01:27:16.466969 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22077 (* 1 = 7.22077 loss)
I0523 01:27:16.753352 35003 sgd_solver.cpp:112] Iteration 111080, lr = 0.01
I0523 01:27:19.260970 35003 solver.cpp:239] Iteration 111090 (3.57927 iter/s, 2.79387s/10 iters), loss = 6.78244
I0523 01:27:19.261046 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78244 (* 1 = 6.78244 loss)
I0523 01:27:19.268316 35003 sgd_solver.cpp:112] Iteration 111090, lr = 0.01
I0523 01:27:23.036541 35003 solver.cpp:239] Iteration 111100 (2.64876 iter/s, 3.77535s/10 iters), loss = 7.53595
I0523 01:27:23.036587 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53595 (* 1 = 7.53595 loss)
I0523 01:27:23.412780 35003 sgd_solver.cpp:112] Iteration 111100, lr = 0.01
I0523 01:27:26.238091 35003 solver.cpp:239] Iteration 111110 (3.12368 iter/s, 3.20136s/10 iters), loss = 7.32134
I0523 01:27:26.238332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32134 (* 1 = 7.32134 loss)
I0523 01:27:26.251001 35003 sgd_solver.cpp:112] Iteration 111110, lr = 0.01
I0523 01:27:31.448698 35003 solver.cpp:239] Iteration 111120 (1.91932 iter/s, 5.21018s/10 iters), loss = 7.9
I0523 01:27:31.448740 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9 (* 1 = 7.9 loss)
I0523 01:27:31.472666 35003 sgd_solver.cpp:112] Iteration 111120, lr = 0.01
I0523 01:27:34.578109 35003 solver.cpp:239] Iteration 111130 (3.19567 iter/s, 3.12924s/10 iters), loss = 7.23532
I0523 01:27:34.578148 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23532 (* 1 = 7.23532 loss)
I0523 01:27:34.591214 35003 sgd_solver.cpp:112] Iteration 111130, lr = 0.01
I0523 01:27:38.125167 35003 solver.cpp:239] Iteration 111140 (2.81939 iter/s, 3.54687s/10 iters), loss = 6.10132
I0523 01:27:38.125226 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10132 (* 1 = 6.10132 loss)
I0523 01:27:38.783466 35003 sgd_solver.cpp:112] Iteration 111140, lr = 0.01
I0523 01:27:41.588323 35003 solver.cpp:239] Iteration 111150 (2.88771 iter/s, 3.46295s/10 iters), loss = 7.1129
I0523 01:27:41.588366 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1129 (* 1 = 7.1129 loss)
I0523 01:27:41.602463 35003 sgd_solver.cpp:112] Iteration 111150, lr = 0.01
I0523 01:27:44.612756 35003 solver.cpp:239] Iteration 111160 (3.30659 iter/s, 3.02426s/10 iters), loss = 6.19678
I0523 01:27:44.612793 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19678 (* 1 = 6.19678 loss)
I0523 01:27:44.616421 35003 sgd_solver.cpp:112] Iteration 111160, lr = 0.01
I0523 01:27:47.429672 35003 solver.cpp:239] Iteration 111170 (3.55018 iter/s, 2.81676s/10 iters), loss = 6.5111
I0523 01:27:47.429723 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5111 (* 1 = 6.5111 loss)
I0523 01:27:47.435595 35003 sgd_solver.cpp:112] Iteration 111170, lr = 0.01
I0523 01:27:50.313640 35003 solver.cpp:239] Iteration 111180 (3.46765 iter/s, 2.8838s/10 iters), loss = 7.4796
I0523 01:27:50.313693 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4796 (* 1 = 7.4796 loss)
I0523 01:27:51.048687 35003 sgd_solver.cpp:112] Iteration 111180, lr = 0.01
I0523 01:27:55.351173 35003 solver.cpp:239] Iteration 111190 (1.98521 iter/s, 5.03726s/10 iters), loss = 7.46064
I0523 01:27:55.351235 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46064 (* 1 = 7.46064 loss)
I0523 01:27:55.363804 35003 sgd_solver.cpp:112] Iteration 111190, lr = 0.01
I0523 01:27:58.223697 35003 solver.cpp:239] Iteration 111200 (3.48148 iter/s, 2.87234s/10 iters), loss = 7.26369
I0523 01:27:58.223966 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26369 (* 1 = 7.26369 loss)
I0523 01:27:58.245411 35003 sgd_solver.cpp:112] Iteration 111200, lr = 0.01
I0523 01:28:02.217521 35003 solver.cpp:239] Iteration 111210 (2.50412 iter/s, 3.99342s/10 iters), loss = 5.8601
I0523 01:28:02.217569 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8601 (* 1 = 5.8601 loss)
I0523 01:28:02.949373 35003 sgd_solver.cpp:112] Iteration 111210, lr = 0.01
I0523 01:28:05.876763 35003 solver.cpp:239] Iteration 111220 (2.73297 iter/s, 3.65903s/10 iters), loss = 7.51247
I0523 01:28:05.876827 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51247 (* 1 = 7.51247 loss)
I0523 01:28:05.883625 35003 sgd_solver.cpp:112] Iteration 111220, lr = 0.01
I0523 01:28:09.032598 35003 solver.cpp:239] Iteration 111230 (3.16893 iter/s, 3.15564s/10 iters), loss = 7.47189
I0523 01:28:09.032637 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47189 (* 1 = 7.47189 loss)
I0523 01:28:09.045049 35003 sgd_solver.cpp:112] Iteration 111230, lr = 0.01
I0523 01:28:12.459666 35003 solver.cpp:239] Iteration 111240 (2.9181 iter/s, 3.42688s/10 iters), loss = 6.37372
I0523 01:28:12.459713 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37372 (* 1 = 6.37372 loss)
I0523 01:28:12.472889 35003 sgd_solver.cpp:112] Iteration 111240, lr = 0.01
I0523 01:28:16.068928 35003 solver.cpp:239] Iteration 111250 (2.7708 iter/s, 3.60906s/10 iters), loss = 6.95254
I0523 01:28:16.068975 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95254 (* 1 = 6.95254 loss)
I0523 01:28:16.077644 35003 sgd_solver.cpp:112] Iteration 111250, lr = 0.01
I0523 01:28:18.786859 35003 solver.cpp:239] Iteration 111260 (3.6795 iter/s, 2.71776s/10 iters), loss = 7.23529
I0523 01:28:18.786933 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23529 (* 1 = 7.23529 loss)
I0523 01:28:18.799912 35003 sgd_solver.cpp:112] Iteration 111260, lr = 0.01
I0523 01:28:20.939365 35003 solver.cpp:239] Iteration 111270 (4.64611 iter/s, 2.15234s/10 iters), loss = 8.39082
I0523 01:28:20.939425 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.39082 (* 1 = 8.39082 loss)
I0523 01:28:20.952327 35003 sgd_solver.cpp:112] Iteration 111270, lr = 0.01
I0523 01:28:23.375797 35003 solver.cpp:239] Iteration 111280 (4.10463 iter/s, 2.43627s/10 iters), loss = 6.53961
I0523 01:28:23.375846 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53961 (* 1 = 6.53961 loss)
I0523 01:28:23.392355 35003 sgd_solver.cpp:112] Iteration 111280, lr = 0.01
I0523 01:28:25.459619 35003 solver.cpp:239] Iteration 111290 (4.79921 iter/s, 2.08368s/10 iters), loss = 7.49566
I0523 01:28:25.459664 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49566 (* 1 = 7.49566 loss)
I0523 01:28:25.470080 35003 sgd_solver.cpp:112] Iteration 111290, lr = 0.01
I0523 01:28:28.742476 35003 solver.cpp:239] Iteration 111300 (3.0463 iter/s, 3.28267s/10 iters), loss = 7.5329
I0523 01:28:28.742846 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5329 (* 1 = 7.5329 loss)
I0523 01:28:28.755446 35003 sgd_solver.cpp:112] Iteration 111300, lr = 0.01
I0523 01:28:30.856235 35003 solver.cpp:239] Iteration 111310 (4.73192 iter/s, 2.11331s/10 iters), loss = 6.49527
I0523 01:28:30.856310 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49527 (* 1 = 6.49527 loss)
I0523 01:28:30.860651 35003 sgd_solver.cpp:112] Iteration 111310, lr = 0.01
I0523 01:28:35.093435 35003 solver.cpp:239] Iteration 111320 (2.36018 iter/s, 4.23696s/10 iters), loss = 7.76474
I0523 01:28:35.093482 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76474 (* 1 = 7.76474 loss)
I0523 01:28:35.113689 35003 sgd_solver.cpp:112] Iteration 111320, lr = 0.01
I0523 01:28:37.183053 35003 solver.cpp:239] Iteration 111330 (4.7859 iter/s, 2.08947s/10 iters), loss = 7.35187
I0523 01:28:37.183110 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35187 (* 1 = 7.35187 loss)
I0523 01:28:37.190472 35003 sgd_solver.cpp:112] Iteration 111330, lr = 0.01
I0523 01:28:41.787923 35003 solver.cpp:239] Iteration 111340 (2.17173 iter/s, 4.60463s/10 iters), loss = 7.462
I0523 01:28:41.787961 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.462 (* 1 = 7.462 loss)
I0523 01:28:41.794710 35003 sgd_solver.cpp:112] Iteration 111340, lr = 0.01
I0523 01:28:45.472276 35003 solver.cpp:239] Iteration 111350 (2.71433 iter/s, 3.68415s/10 iters), loss = 6.17934
I0523 01:28:45.472340 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17934 (* 1 = 6.17934 loss)
I0523 01:28:45.539283 35003 sgd_solver.cpp:112] Iteration 111350, lr = 0.01
I0523 01:28:49.002068 35003 solver.cpp:239] Iteration 111360 (2.8332 iter/s, 3.52958s/10 iters), loss = 6.12021
I0523 01:28:49.002115 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12021 (* 1 = 6.12021 loss)
I0523 01:28:49.013787 35003 sgd_solver.cpp:112] Iteration 111360, lr = 0.01
I0523 01:28:52.327878 35003 solver.cpp:239] Iteration 111370 (3.00696 iter/s, 3.32562s/10 iters), loss = 7.67048
I0523 01:28:52.327939 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67048 (* 1 = 7.67048 loss)
I0523 01:28:52.340719 35003 sgd_solver.cpp:112] Iteration 111370, lr = 0.01
I0523 01:28:54.477357 35003 solver.cpp:239] Iteration 111380 (4.65265 iter/s, 2.14931s/10 iters), loss = 7.35618
I0523 01:28:54.477419 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35618 (* 1 = 7.35618 loss)
I0523 01:28:55.211802 35003 sgd_solver.cpp:112] Iteration 111380, lr = 0.01
I0523 01:28:59.091143 35003 solver.cpp:239] Iteration 111390 (2.16754 iter/s, 4.61353s/10 iters), loss = 6.70694
I0523 01:28:59.091428 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70694 (* 1 = 6.70694 loss)
I0523 01:28:59.120771 35003 sgd_solver.cpp:112] Iteration 111390, lr = 0.01
I0523 01:29:02.230046 35003 solver.cpp:239] Iteration 111400 (3.18622 iter/s, 3.13852s/10 iters), loss = 7.03469
I0523 01:29:02.230083 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03469 (* 1 = 7.03469 loss)
I0523 01:29:02.243577 35003 sgd_solver.cpp:112] Iteration 111400, lr = 0.01
I0523 01:29:06.666910 35003 solver.cpp:239] Iteration 111410 (2.25396 iter/s, 4.43664s/10 iters), loss = 6.99598
I0523 01:29:06.666954 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99598 (* 1 = 6.99598 loss)
I0523 01:29:06.679447 35003 sgd_solver.cpp:112] Iteration 111410, lr = 0.01
I0523 01:29:10.059756 35003 solver.cpp:239] Iteration 111420 (2.94754 iter/s, 3.39266s/10 iters), loss = 7.78621
I0523 01:29:10.059793 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78621 (* 1 = 7.78621 loss)
I0523 01:29:10.072909 35003 sgd_solver.cpp:112] Iteration 111420, lr = 0.01
I0523 01:29:14.845522 35003 solver.cpp:239] Iteration 111430 (2.08963 iter/s, 4.78553s/10 iters), loss = 7.81345
I0523 01:29:14.845561 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81345 (* 1 = 7.81345 loss)
I0523 01:29:14.857566 35003 sgd_solver.cpp:112] Iteration 111430, lr = 0.01
I0523 01:29:19.336472 35003 solver.cpp:239] Iteration 111440 (2.22682 iter/s, 4.49071s/10 iters), loss = 7.65383
I0523 01:29:19.336530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65383 (* 1 = 7.65383 loss)
I0523 01:29:20.070832 35003 sgd_solver.cpp:112] Iteration 111440, lr = 0.01
I0523 01:29:22.801228 35003 solver.cpp:239] Iteration 111450 (2.88637 iter/s, 3.46456s/10 iters), loss = 7.19528
I0523 01:29:22.801271 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19528 (* 1 = 7.19528 loss)
I0523 01:29:23.533545 35003 sgd_solver.cpp:112] Iteration 111450, lr = 0.01
I0523 01:29:27.077086 35003 solver.cpp:239] Iteration 111460 (2.33883 iter/s, 4.27564s/10 iters), loss = 8.23866
I0523 01:29:27.077132 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.23866 (* 1 = 8.23866 loss)
I0523 01:29:27.090448 35003 sgd_solver.cpp:112] Iteration 111460, lr = 0.01
I0523 01:29:30.639571 35003 solver.cpp:239] Iteration 111470 (2.80718 iter/s, 3.56229s/10 iters), loss = 5.78256
I0523 01:29:30.639854 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.78256 (* 1 = 5.78256 loss)
I0523 01:29:31.354395 35003 sgd_solver.cpp:112] Iteration 111470, lr = 0.01
I0523 01:29:34.174317 35003 solver.cpp:239] Iteration 111480 (2.82938 iter/s, 3.53434s/10 iters), loss = 6.59569
I0523 01:29:34.174367 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59569 (* 1 = 6.59569 loss)
I0523 01:29:34.915678 35003 sgd_solver.cpp:112] Iteration 111480, lr = 0.01
I0523 01:29:39.854917 35003 solver.cpp:239] Iteration 111490 (1.76047 iter/s, 5.68032s/10 iters), loss = 7.24036
I0523 01:29:39.854959 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24036 (* 1 = 7.24036 loss)
I0523 01:29:39.864420 35003 sgd_solver.cpp:112] Iteration 111490, lr = 0.01
I0523 01:29:42.803700 35003 solver.cpp:239] Iteration 111500 (3.39143 iter/s, 2.94861s/10 iters), loss = 7.68893
I0523 01:29:42.803747 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68893 (* 1 = 7.68893 loss)
I0523 01:29:42.821981 35003 sgd_solver.cpp:112] Iteration 111500, lr = 0.01
I0523 01:29:45.684407 35003 solver.cpp:239] Iteration 111510 (3.47157 iter/s, 2.88054s/10 iters), loss = 8.23008
I0523 01:29:45.684455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.23008 (* 1 = 8.23008 loss)
I0523 01:29:45.704560 35003 sgd_solver.cpp:112] Iteration 111510, lr = 0.01
I0523 01:29:49.925932 35003 solver.cpp:239] Iteration 111520 (2.35776 iter/s, 4.2413s/10 iters), loss = 7.28171
I0523 01:29:49.925971 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28171 (* 1 = 7.28171 loss)
I0523 01:29:49.944783 35003 sgd_solver.cpp:112] Iteration 111520, lr = 0.01
I0523 01:29:52.180057 35003 solver.cpp:239] Iteration 111530 (4.43659 iter/s, 2.25398s/10 iters), loss = 6.52583
I0523 01:29:52.180110 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52583 (* 1 = 6.52583 loss)
I0523 01:29:52.315485 35003 sgd_solver.cpp:112] Iteration 111530, lr = 0.01
I0523 01:29:56.363503 35003 solver.cpp:239] Iteration 111540 (2.39051 iter/s, 4.18322s/10 iters), loss = 7.38572
I0523 01:29:56.363560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38572 (* 1 = 7.38572 loss)
I0523 01:29:56.378029 35003 sgd_solver.cpp:112] Iteration 111540, lr = 0.01
I0523 01:29:59.929586 35003 solver.cpp:239] Iteration 111550 (2.80436 iter/s, 3.56588s/10 iters), loss = 7.06819
I0523 01:29:59.929639 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06819 (* 1 = 7.06819 loss)
I0523 01:29:59.951961 35003 sgd_solver.cpp:112] Iteration 111550, lr = 0.01
I0523 01:30:04.099134 35003 solver.cpp:239] Iteration 111560 (2.39848 iter/s, 4.16931s/10 iters), loss = 7.26807
I0523 01:30:04.099452 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26807 (* 1 = 7.26807 loss)
I0523 01:30:04.827150 35003 sgd_solver.cpp:112] Iteration 111560, lr = 0.01
I0523 01:30:06.944116 35003 solver.cpp:239] Iteration 111570 (3.51546 iter/s, 2.84457s/10 iters), loss = 6.10904
I0523 01:30:06.944164 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10904 (* 1 = 6.10904 loss)
I0523 01:30:06.949473 35003 sgd_solver.cpp:112] Iteration 111570, lr = 0.01
I0523 01:30:10.514436 35003 solver.cpp:239] Iteration 111580 (2.80103 iter/s, 3.57012s/10 iters), loss = 7.28376
I0523 01:30:10.514494 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28376 (* 1 = 7.28376 loss)
I0523 01:30:10.537151 35003 sgd_solver.cpp:112] Iteration 111580, lr = 0.01
I0523 01:30:14.825722 35003 solver.cpp:239] Iteration 111590 (2.31962 iter/s, 4.31105s/10 iters), loss = 6.6062
I0523 01:30:14.825784 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6062 (* 1 = 6.6062 loss)
I0523 01:30:14.832592 35003 sgd_solver.cpp:112] Iteration 111590, lr = 0.01
I0523 01:30:17.665405 35003 solver.cpp:239] Iteration 111600 (3.52175 iter/s, 2.8395s/10 iters), loss = 6.09651
I0523 01:30:17.665457 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09651 (* 1 = 6.09651 loss)
I0523 01:30:17.690546 35003 sgd_solver.cpp:112] Iteration 111600, lr = 0.01
I0523 01:30:20.556017 35003 solver.cpp:239] Iteration 111610 (3.4597 iter/s, 2.89043s/10 iters), loss = 7.10317
I0523 01:30:20.556073 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10317 (* 1 = 7.10317 loss)
I0523 01:30:21.290424 35003 sgd_solver.cpp:112] Iteration 111610, lr = 0.01
I0523 01:30:24.845325 35003 solver.cpp:239] Iteration 111620 (2.3315 iter/s, 4.28908s/10 iters), loss = 7.37467
I0523 01:30:24.845365 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37467 (* 1 = 7.37467 loss)
I0523 01:30:24.864210 35003 sgd_solver.cpp:112] Iteration 111620, lr = 0.01
I0523 01:30:28.433535 35003 solver.cpp:239] Iteration 111630 (2.78705 iter/s, 3.58802s/10 iters), loss = 6.71711
I0523 01:30:28.433585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71711 (* 1 = 6.71711 loss)
I0523 01:30:28.442603 35003 sgd_solver.cpp:112] Iteration 111630, lr = 0.01
I0523 01:30:29.790884 35003 solver.cpp:239] Iteration 111640 (7.36792 iter/s, 1.35724s/10 iters), loss = 6.97291
I0523 01:30:29.790930 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97291 (* 1 = 6.97291 loss)
I0523 01:30:30.453158 35003 sgd_solver.cpp:112] Iteration 111640, lr = 0.01
I0523 01:30:33.354955 35003 solver.cpp:239] Iteration 111650 (2.80594 iter/s, 3.56387s/10 iters), loss = 8.08665
I0523 01:30:33.355003 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08665 (* 1 = 8.08665 loss)
I0523 01:30:34.070452 35003 sgd_solver.cpp:112] Iteration 111650, lr = 0.01
I0523 01:30:37.618654 35003 solver.cpp:239] Iteration 111660 (2.3455 iter/s, 4.26348s/10 iters), loss = 6.7104
I0523 01:30:37.618873 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7104 (* 1 = 6.7104 loss)
I0523 01:30:37.630347 35003 sgd_solver.cpp:112] Iteration 111660, lr = 0.01
I0523 01:30:40.552028 35003 solver.cpp:239] Iteration 111670 (3.40942 iter/s, 2.93305s/10 iters), loss = 7.77595
I0523 01:30:40.552072 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77595 (* 1 = 7.77595 loss)
I0523 01:30:40.558737 35003 sgd_solver.cpp:112] Iteration 111670, lr = 0.01
I0523 01:30:42.646977 35003 solver.cpp:239] Iteration 111680 (4.7737 iter/s, 2.09481s/10 iters), loss = 7.64413
I0523 01:30:42.647029 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64413 (* 1 = 7.64413 loss)
I0523 01:30:42.660115 35003 sgd_solver.cpp:112] Iteration 111680, lr = 0.01
I0523 01:30:45.542387 35003 solver.cpp:239] Iteration 111690 (3.45396 iter/s, 2.89523s/10 iters), loss = 7.83168
I0523 01:30:45.542444 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83168 (* 1 = 7.83168 loss)
I0523 01:30:45.546658 35003 sgd_solver.cpp:112] Iteration 111690, lr = 0.01
I0523 01:30:48.226378 35003 solver.cpp:239] Iteration 111700 (3.72605 iter/s, 2.68381s/10 iters), loss = 6.99866
I0523 01:30:48.226423 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99866 (* 1 = 6.99866 loss)
I0523 01:30:48.361255 35003 sgd_solver.cpp:112] Iteration 111700, lr = 0.01
I0523 01:30:53.334544 35003 solver.cpp:239] Iteration 111710 (1.95777 iter/s, 5.10786s/10 iters), loss = 6.98139
I0523 01:30:53.334596 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98139 (* 1 = 6.98139 loss)
I0523 01:30:53.352635 35003 sgd_solver.cpp:112] Iteration 111710, lr = 0.01
I0523 01:30:58.205356 35003 solver.cpp:239] Iteration 111720 (2.05315 iter/s, 4.87056s/10 iters), loss = 7.97225
I0523 01:30:58.205400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97225 (* 1 = 7.97225 loss)
I0523 01:30:58.215486 35003 sgd_solver.cpp:112] Iteration 111720, lr = 0.01
I0523 01:31:02.537031 35003 solver.cpp:239] Iteration 111730 (2.30869 iter/s, 4.33146s/10 iters), loss = 6.87609
I0523 01:31:02.537072 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87609 (* 1 = 6.87609 loss)
I0523 01:31:02.543423 35003 sgd_solver.cpp:112] Iteration 111730, lr = 0.01
I0523 01:31:06.530375 35003 solver.cpp:239] Iteration 111740 (2.5043 iter/s, 3.99313s/10 iters), loss = 7.57858
I0523 01:31:06.530431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57858 (* 1 = 7.57858 loss)
I0523 01:31:06.539971 35003 sgd_solver.cpp:112] Iteration 111740, lr = 0.01
I0523 01:31:10.158638 35003 solver.cpp:239] Iteration 111750 (2.7563 iter/s, 3.62806s/10 iters), loss = 7.02052
I0523 01:31:10.158934 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02052 (* 1 = 7.02052 loss)
I0523 01:31:10.724412 35003 sgd_solver.cpp:112] Iteration 111750, lr = 0.01
I0523 01:31:14.362861 35003 solver.cpp:239] Iteration 111760 (2.37881 iter/s, 4.20378s/10 iters), loss = 7.88622
I0523 01:31:14.362903 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88622 (* 1 = 7.88622 loss)
I0523 01:31:14.368654 35003 sgd_solver.cpp:112] Iteration 111760, lr = 0.01
I0523 01:31:17.187911 35003 solver.cpp:239] Iteration 111770 (3.53996 iter/s, 2.82489s/10 iters), loss = 6.9677
I0523 01:31:17.187958 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9677 (* 1 = 6.9677 loss)
I0523 01:31:17.194934 35003 sgd_solver.cpp:112] Iteration 111770, lr = 0.01
I0523 01:31:21.327250 35003 solver.cpp:239] Iteration 111780 (2.41597 iter/s, 4.13912s/10 iters), loss = 6.07831
I0523 01:31:21.327304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07831 (* 1 = 6.07831 loss)
I0523 01:31:22.042083 35003 sgd_solver.cpp:112] Iteration 111780, lr = 0.01
I0523 01:31:25.657526 35003 solver.cpp:239] Iteration 111790 (2.30944 iter/s, 4.33005s/10 iters), loss = 8.35873
I0523 01:31:25.657577 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.35873 (* 1 = 8.35873 loss)
I0523 01:31:26.392639 35003 sgd_solver.cpp:112] Iteration 111790, lr = 0.01
I0523 01:31:28.532173 35003 solver.cpp:239] Iteration 111800 (3.47891 iter/s, 2.87446s/10 iters), loss = 7.52028
I0523 01:31:28.532230 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52028 (* 1 = 7.52028 loss)
I0523 01:31:29.266788 35003 sgd_solver.cpp:112] Iteration 111800, lr = 0.01
I0523 01:31:31.868204 35003 solver.cpp:239] Iteration 111810 (2.99775 iter/s, 3.33584s/10 iters), loss = 7.02028
I0523 01:31:31.868252 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02028 (* 1 = 7.02028 loss)
I0523 01:31:31.881312 35003 sgd_solver.cpp:112] Iteration 111810, lr = 0.01
I0523 01:31:35.379395 35003 solver.cpp:239] Iteration 111820 (2.84819 iter/s, 3.511s/10 iters), loss = 7.60314
I0523 01:31:35.379438 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60314 (* 1 = 7.60314 loss)
I0523 01:31:35.392467 35003 sgd_solver.cpp:112] Iteration 111820, lr = 0.01
I0523 01:31:38.851682 35003 solver.cpp:239] Iteration 111830 (2.88011 iter/s, 3.47209s/10 iters), loss = 7.60834
I0523 01:31:38.851732 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60834 (* 1 = 7.60834 loss)
I0523 01:31:38.865527 35003 sgd_solver.cpp:112] Iteration 111830, lr = 0.01
I0523 01:31:41.639000 35003 solver.cpp:239] Iteration 111840 (3.5879 iter/s, 2.78715s/10 iters), loss = 7.17175
I0523 01:31:41.639178 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17175 (* 1 = 7.17175 loss)
I0523 01:31:41.669088 35003 sgd_solver.cpp:112] Iteration 111840, lr = 0.01
I0523 01:31:46.619194 35003 solver.cpp:239] Iteration 111850 (2.00811 iter/s, 4.97981s/10 iters), loss = 7.67117
I0523 01:31:46.619237 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67117 (* 1 = 7.67117 loss)
I0523 01:31:46.632421 35003 sgd_solver.cpp:112] Iteration 111850, lr = 0.01
I0523 01:31:49.343581 35003 solver.cpp:239] Iteration 111860 (3.67079 iter/s, 2.72421s/10 iters), loss = 7.36894
I0523 01:31:49.343642 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36894 (* 1 = 7.36894 loss)
I0523 01:31:49.351104 35003 sgd_solver.cpp:112] Iteration 111860, lr = 0.01
I0523 01:31:51.966235 35003 solver.cpp:239] Iteration 111870 (3.81319 iter/s, 2.62248s/10 iters), loss = 6.16991
I0523 01:31:51.966277 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16991 (* 1 = 6.16991 loss)
I0523 01:31:51.979677 35003 sgd_solver.cpp:112] Iteration 111870, lr = 0.01
I0523 01:31:57.220495 35003 solver.cpp:239] Iteration 111880 (1.90331 iter/s, 5.254s/10 iters), loss = 6.31171
I0523 01:31:57.220543 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31171 (* 1 = 6.31171 loss)
I0523 01:31:57.230419 35003 sgd_solver.cpp:112] Iteration 111880, lr = 0.01
I0523 01:32:01.768450 35003 solver.cpp:239] Iteration 111890 (2.1989 iter/s, 4.54772s/10 iters), loss = 6.57213
I0523 01:32:01.768492 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57213 (* 1 = 6.57213 loss)
I0523 01:32:01.778972 35003 sgd_solver.cpp:112] Iteration 111890, lr = 0.01
I0523 01:32:04.680001 35003 solver.cpp:239] Iteration 111900 (3.43479 iter/s, 2.91138s/10 iters), loss = 7.06837
I0523 01:32:04.680047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06837 (* 1 = 7.06837 loss)
I0523 01:32:05.399327 35003 sgd_solver.cpp:112] Iteration 111900, lr = 0.01
I0523 01:32:08.262166 35003 solver.cpp:239] Iteration 111910 (2.79176 iter/s, 3.58197s/10 iters), loss = 6.77695
I0523 01:32:08.262202 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77695 (* 1 = 6.77695 loss)
I0523 01:32:08.269251 35003 sgd_solver.cpp:112] Iteration 111910, lr = 0.01
I0523 01:32:13.025696 35003 solver.cpp:239] Iteration 111920 (2.09939 iter/s, 4.7633s/10 iters), loss = 6.84314
I0523 01:32:13.025893 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84314 (* 1 = 6.84314 loss)
I0523 01:32:13.031630 35003 sgd_solver.cpp:112] Iteration 111920, lr = 0.01
I0523 01:32:16.545640 35003 solver.cpp:239] Iteration 111930 (2.84121 iter/s, 3.51963s/10 iters), loss = 6.36908
I0523 01:32:16.545681 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36908 (* 1 = 6.36908 loss)
I0523 01:32:16.567126 35003 sgd_solver.cpp:112] Iteration 111930, lr = 0.01
I0523 01:32:20.845016 35003 solver.cpp:239] Iteration 111940 (2.32604 iter/s, 4.29915s/10 iters), loss = 7.34446
I0523 01:32:20.845062 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34446 (* 1 = 7.34446 loss)
I0523 01:32:20.862223 35003 sgd_solver.cpp:112] Iteration 111940, lr = 0.01
I0523 01:32:24.816511 35003 solver.cpp:239] Iteration 111950 (2.51808 iter/s, 3.97128s/10 iters), loss = 5.80498
I0523 01:32:24.816565 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.80498 (* 1 = 5.80498 loss)
I0523 01:32:25.557413 35003 sgd_solver.cpp:112] Iteration 111950, lr = 0.01
I0523 01:32:29.817502 35003 solver.cpp:239] Iteration 111960 (1.9997 iter/s, 5.00074s/10 iters), loss = 7.50839
I0523 01:32:29.817543 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50839 (* 1 = 7.50839 loss)
I0523 01:32:29.956771 35003 sgd_solver.cpp:112] Iteration 111960, lr = 0.01
I0523 01:32:33.688596 35003 solver.cpp:239] Iteration 111970 (2.58339 iter/s, 3.87088s/10 iters), loss = 6.43696
I0523 01:32:33.688657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43696 (* 1 = 6.43696 loss)
I0523 01:32:33.696853 35003 sgd_solver.cpp:112] Iteration 111970, lr = 0.01
I0523 01:32:35.776062 35003 solver.cpp:239] Iteration 111980 (4.79084 iter/s, 2.08732s/10 iters), loss = 7.30908
I0523 01:32:35.776110 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30908 (* 1 = 7.30908 loss)
I0523 01:32:36.512399 35003 sgd_solver.cpp:112] Iteration 111980, lr = 0.01
I0523 01:32:39.259469 35003 solver.cpp:239] Iteration 111990 (2.87092 iter/s, 3.48321s/10 iters), loss = 6.70382
I0523 01:32:39.259519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70382 (* 1 = 6.70382 loss)
I0523 01:32:39.935870 35003 sgd_solver.cpp:112] Iteration 111990, lr = 0.01
I0523 01:32:42.890281 35003 solver.cpp:239] Iteration 112000 (2.75435 iter/s, 3.63062s/10 iters), loss = 6.93039
I0523 01:32:42.890331 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93039 (* 1 = 6.93039 loss)
I0523 01:32:42.912976 35003 sgd_solver.cpp:112] Iteration 112000, lr = 0.01
I0523 01:32:45.529386 35003 solver.cpp:239] Iteration 112010 (3.7894 iter/s, 2.63894s/10 iters), loss = 6.65738
I0523 01:32:45.529577 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65738 (* 1 = 6.65738 loss)
I0523 01:32:45.554467 35003 sgd_solver.cpp:112] Iteration 112010, lr = 0.01
I0523 01:32:48.374728 35003 solver.cpp:239] Iteration 112020 (3.51494 iter/s, 2.845s/10 iters), loss = 7.31318
I0523 01:32:48.374783 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31318 (* 1 = 7.31318 loss)
I0523 01:32:48.380625 35003 sgd_solver.cpp:112] Iteration 112020, lr = 0.01
I0523 01:32:52.653513 35003 solver.cpp:239] Iteration 112030 (2.33724 iter/s, 4.27854s/10 iters), loss = 7.25296
I0523 01:32:52.653551 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25296 (* 1 = 7.25296 loss)
I0523 01:32:52.663187 35003 sgd_solver.cpp:112] Iteration 112030, lr = 0.01
I0523 01:32:55.689288 35003 solver.cpp:239] Iteration 112040 (3.29423 iter/s, 3.03561s/10 iters), loss = 6.7113
I0523 01:32:55.689339 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7113 (* 1 = 6.7113 loss)
I0523 01:32:55.699178 35003 sgd_solver.cpp:112] Iteration 112040, lr = 0.01
I0523 01:32:57.925714 35003 solver.cpp:239] Iteration 112050 (4.47172 iter/s, 2.23627s/10 iters), loss = 7.75225
I0523 01:32:57.925763 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75225 (* 1 = 7.75225 loss)
I0523 01:32:57.950767 35003 sgd_solver.cpp:112] Iteration 112050, lr = 0.01
I0523 01:33:00.607056 35003 solver.cpp:239] Iteration 112060 (3.7297 iter/s, 2.68118s/10 iters), loss = 6.50074
I0523 01:33:00.607097 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50074 (* 1 = 6.50074 loss)
I0523 01:33:00.613291 35003 sgd_solver.cpp:112] Iteration 112060, lr = 0.01
I0523 01:33:04.691028 35003 solver.cpp:239] Iteration 112070 (2.44872 iter/s, 4.08376s/10 iters), loss = 7.83845
I0523 01:33:04.691082 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83845 (* 1 = 7.83845 loss)
I0523 01:33:04.704052 35003 sgd_solver.cpp:112] Iteration 112070, lr = 0.01
I0523 01:33:09.022816 35003 solver.cpp:239] Iteration 112080 (2.30864 iter/s, 4.33155s/10 iters), loss = 6.63869
I0523 01:33:09.022876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63869 (* 1 = 6.63869 loss)
I0523 01:33:09.044947 35003 sgd_solver.cpp:112] Iteration 112080, lr = 0.01
I0523 01:33:10.337316 35003 solver.cpp:239] Iteration 112090 (7.60816 iter/s, 1.31438s/10 iters), loss = 6.14881
I0523 01:33:10.337357 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14881 (* 1 = 6.14881 loss)
I0523 01:33:10.342134 35003 sgd_solver.cpp:112] Iteration 112090, lr = 0.01
I0523 01:33:12.420655 35003 solver.cpp:239] Iteration 112100 (4.8003 iter/s, 2.0832s/10 iters), loss = 7.29305
I0523 01:33:12.420799 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29305 (* 1 = 7.29305 loss)
I0523 01:33:12.448743 35003 sgd_solver.cpp:112] Iteration 112100, lr = 0.01
I0523 01:33:15.949172 35003 solver.cpp:239] Iteration 112110 (2.83429 iter/s, 3.52823s/10 iters), loss = 6.47207
I0523 01:33:15.949432 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47207 (* 1 = 6.47207 loss)
I0523 01:33:15.961678 35003 sgd_solver.cpp:112] Iteration 112110, lr = 0.01
I0523 01:33:18.781289 35003 solver.cpp:239] Iteration 112120 (3.53139 iter/s, 2.83175s/10 iters), loss = 7.25088
I0523 01:33:18.781339 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25088 (* 1 = 7.25088 loss)
I0523 01:33:18.794266 35003 sgd_solver.cpp:112] Iteration 112120, lr = 0.01
I0523 01:33:22.194950 35003 solver.cpp:239] Iteration 112130 (2.92958 iter/s, 3.41346s/10 iters), loss = 6.42242
I0523 01:33:22.195022 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42242 (* 1 = 6.42242 loss)
I0523 01:33:22.201714 35003 sgd_solver.cpp:112] Iteration 112130, lr = 0.01
I0523 01:33:25.856750 35003 solver.cpp:239] Iteration 112140 (2.73107 iter/s, 3.66157s/10 iters), loss = 7.85061
I0523 01:33:25.856817 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85061 (* 1 = 7.85061 loss)
I0523 01:33:26.018712 35003 sgd_solver.cpp:112] Iteration 112140, lr = 0.01
I0523 01:33:28.081535 35003 solver.cpp:239] Iteration 112150 (4.49514 iter/s, 2.22463s/10 iters), loss = 7.24248
I0523 01:33:28.081578 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24248 (* 1 = 7.24248 loss)
I0523 01:33:28.822887 35003 sgd_solver.cpp:112] Iteration 112150, lr = 0.01
I0523 01:33:34.351696 35003 solver.cpp:239] Iteration 112160 (1.59493 iter/s, 6.26986s/10 iters), loss = 6.4954
I0523 01:33:34.351757 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4954 (* 1 = 6.4954 loss)
I0523 01:33:34.735371 35003 sgd_solver.cpp:112] Iteration 112160, lr = 0.01
I0523 01:33:38.572254 35003 solver.cpp:239] Iteration 112170 (2.36949 iter/s, 4.22032s/10 iters), loss = 6.78903
I0523 01:33:38.572314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78903 (* 1 = 6.78903 loss)
I0523 01:33:38.585345 35003 sgd_solver.cpp:112] Iteration 112170, lr = 0.01
I0523 01:33:41.542928 35003 solver.cpp:239] Iteration 112180 (3.36645 iter/s, 2.97049s/10 iters), loss = 6.16669
I0523 01:33:41.542971 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16669 (* 1 = 6.16669 loss)
I0523 01:33:41.551064 35003 sgd_solver.cpp:112] Iteration 112180, lr = 0.01
I0523 01:33:43.404821 35003 solver.cpp:239] Iteration 112190 (5.37124 iter/s, 1.86177s/10 iters), loss = 6.98991
I0523 01:33:43.404870 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98991 (* 1 = 6.98991 loss)
I0523 01:33:43.560190 35003 sgd_solver.cpp:112] Iteration 112190, lr = 0.01
I0523 01:33:46.398160 35003 solver.cpp:239] Iteration 112200 (3.34095 iter/s, 2.99316s/10 iters), loss = 6.5096
I0523 01:33:46.398423 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5096 (* 1 = 6.5096 loss)
I0523 01:33:47.099861 35003 sgd_solver.cpp:112] Iteration 112200, lr = 0.01
I0523 01:33:50.560117 35003 solver.cpp:239] Iteration 112210 (2.40295 iter/s, 4.16155s/10 iters), loss = 7.61847
I0523 01:33:50.560171 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61847 (* 1 = 7.61847 loss)
I0523 01:33:50.572892 35003 sgd_solver.cpp:112] Iteration 112210, lr = 0.01
I0523 01:33:52.604027 35003 solver.cpp:239] Iteration 112220 (4.89294 iter/s, 2.04376s/10 iters), loss = 7.35892
I0523 01:33:52.604073 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35892 (* 1 = 7.35892 loss)
I0523 01:33:52.617449 35003 sgd_solver.cpp:112] Iteration 112220, lr = 0.01
I0523 01:33:56.546563 35003 solver.cpp:239] Iteration 112230 (2.53658 iter/s, 3.94232s/10 iters), loss = 7.19955
I0523 01:33:56.546617 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19955 (* 1 = 7.19955 loss)
I0523 01:33:56.572150 35003 sgd_solver.cpp:112] Iteration 112230, lr = 0.01
I0523 01:33:59.404753 35003 solver.cpp:239] Iteration 112240 (3.49893 iter/s, 2.85801s/10 iters), loss = 5.77608
I0523 01:33:59.404796 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77608 (* 1 = 5.77608 loss)
I0523 01:33:59.407910 35003 sgd_solver.cpp:112] Iteration 112240, lr = 0.01
I0523 01:34:02.997937 35003 solver.cpp:239] Iteration 112250 (2.78321 iter/s, 3.59298s/10 iters), loss = 6.21756
I0523 01:34:02.997983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21756 (* 1 = 6.21756 loss)
I0523 01:34:03.176218 35003 sgd_solver.cpp:112] Iteration 112250, lr = 0.01
I0523 01:34:06.738620 35003 solver.cpp:239] Iteration 112260 (2.67346 iter/s, 3.74047s/10 iters), loss = 7.45123
I0523 01:34:06.738660 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45123 (* 1 = 7.45123 loss)
I0523 01:34:06.752044 35003 sgd_solver.cpp:112] Iteration 112260, lr = 0.01
I0523 01:34:09.584482 35003 solver.cpp:239] Iteration 112270 (3.51408 iter/s, 2.8457s/10 iters), loss = 6.74516
I0523 01:34:09.584524 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74516 (* 1 = 6.74516 loss)
I0523 01:34:09.592396 35003 sgd_solver.cpp:112] Iteration 112270, lr = 0.01
I0523 01:34:12.833906 35003 solver.cpp:239] Iteration 112280 (3.07764 iter/s, 3.24924s/10 iters), loss = 6.74348
I0523 01:34:12.833950 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74348 (* 1 = 6.74348 loss)
I0523 01:34:12.847256 35003 sgd_solver.cpp:112] Iteration 112280, lr = 0.01
I0523 01:34:14.983690 35003 solver.cpp:239] Iteration 112290 (4.65194 iter/s, 2.14964s/10 iters), loss = 7.6896
I0523 01:34:14.983734 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6896 (* 1 = 7.6896 loss)
I0523 01:34:14.997447 35003 sgd_solver.cpp:112] Iteration 112290, lr = 0.01
I0523 01:34:19.000771 35003 solver.cpp:239] Iteration 112300 (2.4895 iter/s, 4.01687s/10 iters), loss = 6.52201
I0523 01:34:19.002805 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52201 (* 1 = 6.52201 loss)
I0523 01:34:19.023417 35003 sgd_solver.cpp:112] Iteration 112300, lr = 0.01
I0523 01:34:23.487051 35003 solver.cpp:239] Iteration 112310 (2.2301 iter/s, 4.48411s/10 iters), loss = 6.30994
I0523 01:34:23.487100 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30994 (* 1 = 6.30994 loss)
I0523 01:34:24.217413 35003 sgd_solver.cpp:112] Iteration 112310, lr = 0.01
I0523 01:34:27.725263 35003 solver.cpp:239] Iteration 112320 (2.35961 iter/s, 4.23799s/10 iters), loss = 6.56991
I0523 01:34:27.725308 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56991 (* 1 = 6.56991 loss)
I0523 01:34:27.766541 35003 sgd_solver.cpp:112] Iteration 112320, lr = 0.01
I0523 01:34:29.779549 35003 solver.cpp:239] Iteration 112330 (4.86821 iter/s, 2.05415s/10 iters), loss = 7.55733
I0523 01:34:29.779616 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55733 (* 1 = 7.55733 loss)
I0523 01:34:29.783776 35003 sgd_solver.cpp:112] Iteration 112330, lr = 0.01
I0523 01:34:31.856956 35003 solver.cpp:239] Iteration 112340 (4.81406 iter/s, 2.07725s/10 iters), loss = 7.76256
I0523 01:34:31.857007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76256 (* 1 = 7.76256 loss)
I0523 01:34:32.488840 35003 sgd_solver.cpp:112] Iteration 112340, lr = 0.01
I0523 01:34:36.730410 35003 solver.cpp:239] Iteration 112350 (2.05204 iter/s, 4.87321s/10 iters), loss = 6.97125
I0523 01:34:36.730450 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97125 (* 1 = 6.97125 loss)
I0523 01:34:36.742435 35003 sgd_solver.cpp:112] Iteration 112350, lr = 0.01
I0523 01:34:39.518172 35003 solver.cpp:239] Iteration 112360 (3.58732 iter/s, 2.7876s/10 iters), loss = 5.71227
I0523 01:34:39.518205 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.71227 (* 1 = 5.71227 loss)
I0523 01:34:39.530886 35003 sgd_solver.cpp:112] Iteration 112360, lr = 0.01
I0523 01:34:41.036561 35003 solver.cpp:239] Iteration 112370 (6.58641 iter/s, 1.51828s/10 iters), loss = 6.57212
I0523 01:34:41.036614 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57212 (* 1 = 6.57212 loss)
I0523 01:34:41.745658 35003 sgd_solver.cpp:112] Iteration 112370, lr = 0.01
I0523 01:34:46.193681 35003 solver.cpp:239] Iteration 112380 (1.93917 iter/s, 5.15686s/10 iters), loss = 6.65304
I0523 01:34:46.193723 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65304 (* 1 = 6.65304 loss)
I0523 01:34:46.200050 35003 sgd_solver.cpp:112] Iteration 112380, lr = 0.01
I0523 01:34:49.721055 35003 solver.cpp:239] Iteration 112390 (2.83512 iter/s, 3.52719s/10 iters), loss = 7.81423
I0523 01:34:49.721297 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81423 (* 1 = 7.81423 loss)
I0523 01:34:49.734107 35003 sgd_solver.cpp:112] Iteration 112390, lr = 0.01
I0523 01:34:52.410630 35003 solver.cpp:239] Iteration 112400 (3.71852 iter/s, 2.68924s/10 iters), loss = 6.99201
I0523 01:34:52.410679 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99201 (* 1 = 6.99201 loss)
I0523 01:34:53.125555 35003 sgd_solver.cpp:112] Iteration 112400, lr = 0.01
I0523 01:34:58.221242 35003 solver.cpp:239] Iteration 112410 (1.72107 iter/s, 5.81033s/10 iters), loss = 6.50424
I0523 01:34:58.221289 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50424 (* 1 = 6.50424 loss)
I0523 01:34:58.930018 35003 sgd_solver.cpp:112] Iteration 112410, lr = 0.01
I0523 01:35:04.149165 35003 solver.cpp:239] Iteration 112420 (1.68702 iter/s, 5.92762s/10 iters), loss = 6.76299
I0523 01:35:04.149229 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76299 (* 1 = 6.76299 loss)
I0523 01:35:04.864172 35003 sgd_solver.cpp:112] Iteration 112420, lr = 0.01
I0523 01:35:09.284310 35003 solver.cpp:239] Iteration 112430 (1.94747 iter/s, 5.13488s/10 iters), loss = 6.28987
I0523 01:35:09.284366 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28987 (* 1 = 6.28987 loss)
I0523 01:35:09.301527 35003 sgd_solver.cpp:112] Iteration 112430, lr = 0.01
I0523 01:35:12.170789 35003 solver.cpp:239] Iteration 112440 (3.46464 iter/s, 2.8863s/10 iters), loss = 6.167
I0523 01:35:12.170830 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.167 (* 1 = 6.167 loss)
I0523 01:35:12.197609 35003 sgd_solver.cpp:112] Iteration 112440, lr = 0.01
I0523 01:35:16.625010 35003 solver.cpp:239] Iteration 112450 (2.24518 iter/s, 4.45399s/10 iters), loss = 6.98393
I0523 01:35:16.625062 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98393 (* 1 = 6.98393 loss)
I0523 01:35:16.631395 35003 sgd_solver.cpp:112] Iteration 112450, lr = 0.01
I0523 01:35:18.551707 35003 solver.cpp:239] Iteration 112460 (5.1906 iter/s, 1.92656s/10 iters), loss = 6.95223
I0523 01:35:18.551745 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95223 (* 1 = 6.95223 loss)
I0523 01:35:18.565002 35003 sgd_solver.cpp:112] Iteration 112460, lr = 0.01
I0523 01:35:21.286831 35003 solver.cpp:239] Iteration 112470 (3.65635 iter/s, 2.73496s/10 iters), loss = 7.59206
I0523 01:35:21.286964 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59206 (* 1 = 7.59206 loss)
I0523 01:35:21.299574 35003 sgd_solver.cpp:112] Iteration 112470, lr = 0.01
I0523 01:35:24.043886 35003 solver.cpp:239] Iteration 112480 (3.62739 iter/s, 2.7568s/10 iters), loss = 7.82975
I0523 01:35:24.043941 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82975 (* 1 = 7.82975 loss)
I0523 01:35:24.062199 35003 sgd_solver.cpp:112] Iteration 112480, lr = 0.01
I0523 01:35:26.714915 35003 solver.cpp:239] Iteration 112490 (3.74411 iter/s, 2.67086s/10 iters), loss = 6.58453
I0523 01:35:26.714970 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58453 (* 1 = 6.58453 loss)
I0523 01:35:27.455816 35003 sgd_solver.cpp:112] Iteration 112490, lr = 0.01
I0523 01:35:31.599277 35003 solver.cpp:239] Iteration 112500 (2.04746 iter/s, 4.88411s/10 iters), loss = 5.9672
I0523 01:35:31.599323 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9672 (* 1 = 5.9672 loss)
I0523 01:35:31.607954 35003 sgd_solver.cpp:112] Iteration 112500, lr = 0.01
I0523 01:35:35.276690 35003 solver.cpp:239] Iteration 112510 (2.71945 iter/s, 3.67721s/10 iters), loss = 7.80078
I0523 01:35:35.276737 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80078 (* 1 = 7.80078 loss)
I0523 01:35:35.704485 35003 sgd_solver.cpp:112] Iteration 112510, lr = 0.01
I0523 01:35:40.377287 35003 solver.cpp:239] Iteration 112520 (1.96065 iter/s, 5.10034s/10 iters), loss = 7.21803
I0523 01:35:40.377336 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21803 (* 1 = 7.21803 loss)
I0523 01:35:40.389946 35003 sgd_solver.cpp:112] Iteration 112520, lr = 0.01
I0523 01:35:44.361138 35003 solver.cpp:239] Iteration 112530 (2.51027 iter/s, 3.98364s/10 iters), loss = 7.2339
I0523 01:35:44.361184 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2339 (* 1 = 7.2339 loss)
I0523 01:35:44.375149 35003 sgd_solver.cpp:112] Iteration 112530, lr = 0.01
I0523 01:35:48.358069 35003 solver.cpp:239] Iteration 112540 (2.50205 iter/s, 3.99672s/10 iters), loss = 6.44454
I0523 01:35:48.358129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44454 (* 1 = 6.44454 loss)
I0523 01:35:48.365100 35003 sgd_solver.cpp:112] Iteration 112540, lr = 0.01
I0523 01:35:51.207695 35003 solver.cpp:239] Iteration 112550 (3.50946 iter/s, 2.84944s/10 iters), loss = 7.16259
I0523 01:35:51.207733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16259 (* 1 = 7.16259 loss)
I0523 01:35:51.220921 35003 sgd_solver.cpp:112] Iteration 112550, lr = 0.01
I0523 01:35:54.142132 35003 solver.cpp:239] Iteration 112560 (3.40801 iter/s, 2.93427s/10 iters), loss = 6.58093
I0523 01:35:54.142458 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58093 (* 1 = 6.58093 loss)
I0523 01:35:54.158011 35003 sgd_solver.cpp:112] Iteration 112560, lr = 0.01
I0523 01:35:58.126185 35003 solver.cpp:239] Iteration 112570 (2.5103 iter/s, 3.98358s/10 iters), loss = 7.19828
I0523 01:35:58.126225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19828 (* 1 = 7.19828 loss)
I0523 01:35:58.133950 35003 sgd_solver.cpp:112] Iteration 112570, lr = 0.01
I0523 01:36:01.848963 35003 solver.cpp:239] Iteration 112580 (2.68631 iter/s, 3.72258s/10 iters), loss = 7.01869
I0523 01:36:01.849018 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01869 (* 1 = 7.01869 loss)
I0523 01:36:01.859916 35003 sgd_solver.cpp:112] Iteration 112580, lr = 0.01
I0523 01:36:05.511044 35003 solver.cpp:239] Iteration 112590 (2.73084 iter/s, 3.66187s/10 iters), loss = 7.03278
I0523 01:36:05.511086 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03278 (* 1 = 7.03278 loss)
I0523 01:36:05.536837 35003 sgd_solver.cpp:112] Iteration 112590, lr = 0.01
I0523 01:36:08.367142 35003 solver.cpp:239] Iteration 112600 (3.50148 iter/s, 2.85594s/10 iters), loss = 7.06707
I0523 01:36:08.367192 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06707 (* 1 = 7.06707 loss)
I0523 01:36:08.376725 35003 sgd_solver.cpp:112] Iteration 112600, lr = 0.01
I0523 01:36:13.525461 35003 solver.cpp:239] Iteration 112610 (1.93871 iter/s, 5.15806s/10 iters), loss = 6.12161
I0523 01:36:13.525514 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12161 (* 1 = 6.12161 loss)
I0523 01:36:14.253446 35003 sgd_solver.cpp:112] Iteration 112610, lr = 0.01
I0523 01:36:17.249485 35003 solver.cpp:239] Iteration 112620 (2.68542 iter/s, 3.72382s/10 iters), loss = 6.22534
I0523 01:36:17.249531 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22534 (* 1 = 6.22534 loss)
I0523 01:36:17.602826 35003 sgd_solver.cpp:112] Iteration 112620, lr = 0.01
I0523 01:36:20.231468 35003 solver.cpp:239] Iteration 112630 (3.35368 iter/s, 2.98179s/10 iters), loss = 7.7984
I0523 01:36:20.231534 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7984 (* 1 = 7.7984 loss)
I0523 01:36:20.248751 35003 sgd_solver.cpp:112] Iteration 112630, lr = 0.01
I0523 01:36:23.868882 35003 solver.cpp:239] Iteration 112640 (2.75259 iter/s, 3.63294s/10 iters), loss = 8.55394
I0523 01:36:23.868937 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.55394 (* 1 = 8.55394 loss)
I0523 01:36:23.880412 35003 sgd_solver.cpp:112] Iteration 112640, lr = 0.01
I0523 01:36:27.197480 35003 solver.cpp:239] Iteration 112650 (3.00446 iter/s, 3.32839s/10 iters), loss = 7.6826
I0523 01:36:27.197633 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6826 (* 1 = 7.6826 loss)
I0523 01:36:27.205471 35003 sgd_solver.cpp:112] Iteration 112650, lr = 0.01
I0523 01:36:28.840523 35003 solver.cpp:239] Iteration 112660 (6.08709 iter/s, 1.64282s/10 iters), loss = 8.0627
I0523 01:36:28.840565 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0627 (* 1 = 8.0627 loss)
I0523 01:36:28.851671 35003 sgd_solver.cpp:112] Iteration 112660, lr = 0.01
I0523 01:36:31.721590 35003 solver.cpp:239] Iteration 112670 (3.47115 iter/s, 2.88089s/10 iters), loss = 8.2015
I0523 01:36:31.721634 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.2015 (* 1 = 8.2015 loss)
I0523 01:36:31.728255 35003 sgd_solver.cpp:112] Iteration 112670, lr = 0.01
I0523 01:36:34.536957 35003 solver.cpp:239] Iteration 112680 (3.55215 iter/s, 2.81519s/10 iters), loss = 7.04646
I0523 01:36:34.537015 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04646 (* 1 = 7.04646 loss)
I0523 01:36:35.277884 35003 sgd_solver.cpp:112] Iteration 112680, lr = 0.01
I0523 01:36:38.489234 35003 solver.cpp:239] Iteration 112690 (2.53033 iter/s, 3.95206s/10 iters), loss = 6.30812
I0523 01:36:38.489338 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30812 (* 1 = 6.30812 loss)
I0523 01:36:38.502095 35003 sgd_solver.cpp:112] Iteration 112690, lr = 0.01
I0523 01:36:41.414765 35003 solver.cpp:239] Iteration 112700 (3.41844 iter/s, 2.92531s/10 iters), loss = 8.00027
I0523 01:36:41.414813 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00027 (* 1 = 8.00027 loss)
I0523 01:36:41.427177 35003 sgd_solver.cpp:112] Iteration 112700, lr = 0.01
I0523 01:36:44.164582 35003 solver.cpp:239] Iteration 112710 (3.63683 iter/s, 2.74965s/10 iters), loss = 7.07717
I0523 01:36:44.164628 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07717 (* 1 = 7.07717 loss)
I0523 01:36:44.167929 35003 sgd_solver.cpp:112] Iteration 112710, lr = 0.01
I0523 01:36:47.727581 35003 solver.cpp:239] Iteration 112720 (2.80678 iter/s, 3.5628s/10 iters), loss = 6.57992
I0523 01:36:47.727623 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57992 (* 1 = 6.57992 loss)
I0523 01:36:47.740752 35003 sgd_solver.cpp:112] Iteration 112720, lr = 0.01
I0523 01:36:52.205935 35003 solver.cpp:239] Iteration 112730 (2.23308 iter/s, 4.47811s/10 iters), loss = 7.71953
I0523 01:36:52.205993 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71953 (* 1 = 7.71953 loss)
I0523 01:36:52.218973 35003 sgd_solver.cpp:112] Iteration 112730, lr = 0.01
I0523 01:36:54.954017 35003 solver.cpp:239] Iteration 112740 (3.63914 iter/s, 2.7479s/10 iters), loss = 7.0532
I0523 01:36:54.954069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0532 (* 1 = 7.0532 loss)
I0523 01:36:54.958808 35003 sgd_solver.cpp:112] Iteration 112740, lr = 0.01
I0523 01:36:58.224136 35003 solver.cpp:239] Iteration 112750 (3.05817 iter/s, 3.26993s/10 iters), loss = 6.69279
I0523 01:36:58.224315 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69279 (* 1 = 6.69279 loss)
I0523 01:36:58.252599 35003 sgd_solver.cpp:112] Iteration 112750, lr = 0.01
I0523 01:37:00.376610 35003 solver.cpp:239] Iteration 112760 (4.64639 iter/s, 2.15221s/10 iters), loss = 7.39153
I0523 01:37:00.376652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39153 (* 1 = 7.39153 loss)
I0523 01:37:01.071727 35003 sgd_solver.cpp:112] Iteration 112760, lr = 0.01
I0523 01:37:03.946728 35003 solver.cpp:239] Iteration 112770 (2.80121 iter/s, 3.56989s/10 iters), loss = 5.74742
I0523 01:37:03.946795 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74742 (* 1 = 5.74742 loss)
I0523 01:37:04.666245 35003 sgd_solver.cpp:112] Iteration 112770, lr = 0.01
I0523 01:37:07.409582 35003 solver.cpp:239] Iteration 112780 (2.88797 iter/s, 3.46264s/10 iters), loss = 6.30393
I0523 01:37:07.409628 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30393 (* 1 = 6.30393 loss)
I0523 01:37:07.422401 35003 sgd_solver.cpp:112] Iteration 112780, lr = 0.01
I0523 01:37:10.080242 35003 solver.cpp:239] Iteration 112790 (3.74463 iter/s, 2.67049s/10 iters), loss = 6.39897
I0523 01:37:10.080296 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39897 (* 1 = 6.39897 loss)
I0523 01:37:10.098460 35003 sgd_solver.cpp:112] Iteration 112790, lr = 0.01
I0523 01:37:12.961419 35003 solver.cpp:239] Iteration 112800 (3.47102 iter/s, 2.881s/10 iters), loss = 6.6848
I0523 01:37:12.961465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6848 (* 1 = 6.6848 loss)
I0523 01:37:12.974709 35003 sgd_solver.cpp:112] Iteration 112800, lr = 0.01
I0523 01:37:15.819324 35003 solver.cpp:239] Iteration 112810 (3.49927 iter/s, 2.85774s/10 iters), loss = 6.34731
I0523 01:37:15.819368 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34731 (* 1 = 6.34731 loss)
I0523 01:37:15.824523 35003 sgd_solver.cpp:112] Iteration 112810, lr = 0.01
I0523 01:37:19.865064 35003 solver.cpp:239] Iteration 112820 (2.47187 iter/s, 4.04552s/10 iters), loss = 6.40897
I0523 01:37:19.865123 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40897 (* 1 = 6.40897 loss)
I0523 01:37:19.891199 35003 sgd_solver.cpp:112] Iteration 112820, lr = 0.01
I0523 01:37:22.944020 35003 solver.cpp:239] Iteration 112830 (3.24805 iter/s, 3.07877s/10 iters), loss = 6.85841
I0523 01:37:22.944059 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85841 (* 1 = 6.85841 loss)
I0523 01:37:22.951469 35003 sgd_solver.cpp:112] Iteration 112830, lr = 0.01
I0523 01:37:26.490259 35003 solver.cpp:239] Iteration 112840 (2.82005 iter/s, 3.54604s/10 iters), loss = 6.77173
I0523 01:37:26.490312 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77173 (* 1 = 6.77173 loss)
I0523 01:37:26.494266 35003 sgd_solver.cpp:112] Iteration 112840, lr = 0.01
I0523 01:37:28.569315 35003 solver.cpp:239] Iteration 112850 (4.81025 iter/s, 2.0789s/10 iters), loss = 8.0686
I0523 01:37:28.569614 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0686 (* 1 = 8.0686 loss)
I0523 01:37:28.582201 35003 sgd_solver.cpp:112] Iteration 112850, lr = 0.01
I0523 01:37:30.744997 35003 solver.cpp:239] Iteration 112860 (4.59701 iter/s, 2.17533s/10 iters), loss = 7.4561
I0523 01:37:30.745036 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4561 (* 1 = 7.4561 loss)
I0523 01:37:31.447343 35003 sgd_solver.cpp:112] Iteration 112860, lr = 0.01
I0523 01:37:33.603896 35003 solver.cpp:239] Iteration 112870 (3.49805 iter/s, 2.85874s/10 iters), loss = 6.31595
I0523 01:37:33.603950 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31595 (* 1 = 6.31595 loss)
I0523 01:37:33.607000 35003 sgd_solver.cpp:112] Iteration 112870, lr = 0.01
I0523 01:37:37.189888 35003 solver.cpp:239] Iteration 112880 (2.78879 iter/s, 3.58579s/10 iters), loss = 7.26155
I0523 01:37:37.189932 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26155 (* 1 = 7.26155 loss)
I0523 01:37:37.199746 35003 sgd_solver.cpp:112] Iteration 112880, lr = 0.01
I0523 01:37:42.931926 35003 solver.cpp:239] Iteration 112890 (1.74163 iter/s, 5.74176s/10 iters), loss = 7.09609
I0523 01:37:42.931967 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09609 (* 1 = 7.09609 loss)
I0523 01:37:42.971482 35003 sgd_solver.cpp:112] Iteration 112890, lr = 0.01
I0523 01:37:45.254990 35003 solver.cpp:239] Iteration 112900 (4.30496 iter/s, 2.3229s/10 iters), loss = 7.62905
I0523 01:37:45.255033 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62905 (* 1 = 7.62905 loss)
I0523 01:37:45.265364 35003 sgd_solver.cpp:112] Iteration 112900, lr = 0.01
I0523 01:37:48.866722 35003 solver.cpp:239] Iteration 112910 (2.76892 iter/s, 3.61152s/10 iters), loss = 7.66468
I0523 01:37:48.866768 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66468 (* 1 = 7.66468 loss)
I0523 01:37:48.875397 35003 sgd_solver.cpp:112] Iteration 112910, lr = 0.01
I0523 01:37:51.908754 35003 solver.cpp:239] Iteration 112920 (3.28746 iter/s, 3.04186s/10 iters), loss = 6.19663
I0523 01:37:51.908797 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19663 (* 1 = 6.19663 loss)
I0523 01:37:51.923509 35003 sgd_solver.cpp:112] Iteration 112920, lr = 0.01
I0523 01:37:54.914778 35003 solver.cpp:239] Iteration 112930 (3.32684 iter/s, 3.00586s/10 iters), loss = 6.62376
I0523 01:37:54.914824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62376 (* 1 = 6.62376 loss)
I0523 01:37:55.618748 35003 sgd_solver.cpp:112] Iteration 112930, lr = 0.01
I0523 01:37:59.060336 35003 solver.cpp:239] Iteration 112940 (2.41235 iter/s, 4.14534s/10 iters), loss = 6.83311
I0523 01:37:59.060504 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83311 (* 1 = 6.83311 loss)
I0523 01:37:59.073626 35003 sgd_solver.cpp:112] Iteration 112940, lr = 0.01
I0523 01:38:03.335063 35003 solver.cpp:239] Iteration 112950 (2.33952 iter/s, 4.27438s/10 iters), loss = 6.93539
I0523 01:38:03.335127 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93539 (* 1 = 6.93539 loss)
I0523 01:38:03.342099 35003 sgd_solver.cpp:112] Iteration 112950, lr = 0.01
I0523 01:38:05.927106 35003 solver.cpp:239] Iteration 112960 (3.85822 iter/s, 2.59187s/10 iters), loss = 6.73096
I0523 01:38:05.927153 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73096 (* 1 = 6.73096 loss)
I0523 01:38:05.933703 35003 sgd_solver.cpp:112] Iteration 112960, lr = 0.01
I0523 01:38:09.630409 35003 solver.cpp:239] Iteration 112970 (2.70045 iter/s, 3.70309s/10 iters), loss = 7.02111
I0523 01:38:09.630465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02111 (* 1 = 7.02111 loss)
I0523 01:38:09.643487 35003 sgd_solver.cpp:112] Iteration 112970, lr = 0.01
I0523 01:38:11.770210 35003 solver.cpp:239] Iteration 112980 (4.67366 iter/s, 2.13965s/10 iters), loss = 6.75295
I0523 01:38:11.770254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75295 (* 1 = 6.75295 loss)
I0523 01:38:12.465993 35003 sgd_solver.cpp:112] Iteration 112980, lr = 0.01
I0523 01:38:15.966792 35003 solver.cpp:239] Iteration 112990 (2.38307 iter/s, 4.19626s/10 iters), loss = 6.84096
I0523 01:38:15.966861 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84096 (* 1 = 6.84096 loss)
I0523 01:38:15.974766 35003 sgd_solver.cpp:112] Iteration 112990, lr = 0.01
I0523 01:38:18.811290 35003 solver.cpp:239] Iteration 113000 (3.51579 iter/s, 2.84431s/10 iters), loss = 6.53426
I0523 01:38:18.811337 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53426 (* 1 = 6.53426 loss)
I0523 01:38:18.821540 35003 sgd_solver.cpp:112] Iteration 113000, lr = 0.01
I0523 01:38:22.932027 35003 solver.cpp:239] Iteration 113010 (2.42688 iter/s, 4.12052s/10 iters), loss = 7.86145
I0523 01:38:22.932076 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86145 (* 1 = 7.86145 loss)
I0523 01:38:23.640224 35003 sgd_solver.cpp:112] Iteration 113010, lr = 0.01
I0523 01:38:26.624167 35003 solver.cpp:239] Iteration 113020 (2.7086 iter/s, 3.69194s/10 iters), loss = 8.4885
I0523 01:38:26.624212 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.4885 (* 1 = 8.4885 loss)
I0523 01:38:26.637255 35003 sgd_solver.cpp:112] Iteration 113020, lr = 0.01
I0523 01:38:31.439492 35003 solver.cpp:239] Iteration 113030 (2.07681 iter/s, 4.81509s/10 iters), loss = 7.60327
I0523 01:38:31.439744 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60327 (* 1 = 7.60327 loss)
I0523 01:38:31.453877 35003 sgd_solver.cpp:112] Iteration 113030, lr = 0.01
I0523 01:38:35.803247 35003 solver.cpp:239] Iteration 113040 (2.29183 iter/s, 4.36333s/10 iters), loss = 6.23719
I0523 01:38:35.803295 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23719 (* 1 = 6.23719 loss)
I0523 01:38:35.815847 35003 sgd_solver.cpp:112] Iteration 113040, lr = 0.01
I0523 01:38:38.757578 35003 solver.cpp:239] Iteration 113050 (3.38507 iter/s, 2.95415s/10 iters), loss = 7.71992
I0523 01:38:38.757630 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71992 (* 1 = 7.71992 loss)
I0523 01:38:38.760789 35003 sgd_solver.cpp:112] Iteration 113050, lr = 0.01
I0523 01:38:42.181145 35003 solver.cpp:239] Iteration 113060 (2.9211 iter/s, 3.42337s/10 iters), loss = 7.8384
I0523 01:38:42.181181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8384 (* 1 = 7.8384 loss)
I0523 01:38:42.194453 35003 sgd_solver.cpp:112] Iteration 113060, lr = 0.01
I0523 01:38:45.771802 35003 solver.cpp:239] Iteration 113070 (2.78515 iter/s, 3.59047s/10 iters), loss = 7.04921
I0523 01:38:45.771854 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04921 (* 1 = 7.04921 loss)
I0523 01:38:45.801389 35003 sgd_solver.cpp:112] Iteration 113070, lr = 0.01
I0523 01:38:50.800319 35003 solver.cpp:239] Iteration 113080 (1.98876 iter/s, 5.02825s/10 iters), loss = 7.09677
I0523 01:38:50.800366 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09677 (* 1 = 7.09677 loss)
I0523 01:38:50.811955 35003 sgd_solver.cpp:112] Iteration 113080, lr = 0.01
I0523 01:38:54.399626 35003 solver.cpp:239] Iteration 113090 (2.7785 iter/s, 3.59907s/10 iters), loss = 7.27624
I0523 01:38:54.399667 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27624 (* 1 = 7.27624 loss)
I0523 01:38:55.115298 35003 sgd_solver.cpp:112] Iteration 113090, lr = 0.01
I0523 01:38:57.215427 35003 solver.cpp:239] Iteration 113100 (3.55159 iter/s, 2.81564s/10 iters), loss = 6.76736
I0523 01:38:57.215476 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76736 (* 1 = 6.76736 loss)
I0523 01:38:57.228121 35003 sgd_solver.cpp:112] Iteration 113100, lr = 0.01
I0523 01:39:01.476781 35003 solver.cpp:239] Iteration 113110 (2.34679 iter/s, 4.26113s/10 iters), loss = 6.53674
I0523 01:39:01.477020 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53674 (* 1 = 6.53674 loss)
I0523 01:39:01.490628 35003 sgd_solver.cpp:112] Iteration 113110, lr = 0.01
I0523 01:39:05.004287 35003 solver.cpp:239] Iteration 113120 (2.83515 iter/s, 3.52715s/10 iters), loss = 7.93084
I0523 01:39:05.004324 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93084 (* 1 = 7.93084 loss)
I0523 01:39:05.020661 35003 sgd_solver.cpp:112] Iteration 113120, lr = 0.01
I0523 01:39:08.438530 35003 solver.cpp:239] Iteration 113130 (2.912 iter/s, 3.43406s/10 iters), loss = 7.06744
I0523 01:39:08.438575 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06744 (* 1 = 7.06744 loss)
I0523 01:39:09.119318 35003 sgd_solver.cpp:112] Iteration 113130, lr = 0.01
I0523 01:39:13.787120 35003 solver.cpp:239] Iteration 113140 (1.86975 iter/s, 5.34831s/10 iters), loss = 6.50431
I0523 01:39:13.787158 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50431 (* 1 = 6.50431 loss)
I0523 01:39:14.429100 35003 sgd_solver.cpp:112] Iteration 113140, lr = 0.01
I0523 01:39:16.542969 35003 solver.cpp:239] Iteration 113150 (3.62885 iter/s, 2.75569s/10 iters), loss = 5.89294
I0523 01:39:16.543010 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89294 (* 1 = 5.89294 loss)
I0523 01:39:16.555930 35003 sgd_solver.cpp:112] Iteration 113150, lr = 0.01
I0523 01:39:20.832497 35003 solver.cpp:239] Iteration 113160 (2.33138 iter/s, 4.28931s/10 iters), loss = 6.15917
I0523 01:39:20.832547 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15917 (* 1 = 6.15917 loss)
I0523 01:39:20.838945 35003 sgd_solver.cpp:112] Iteration 113160, lr = 0.01
I0523 01:39:24.520196 35003 solver.cpp:239] Iteration 113170 (2.71187 iter/s, 3.6875s/10 iters), loss = 7.643
I0523 01:39:24.520237 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.643 (* 1 = 7.643 loss)
I0523 01:39:25.244843 35003 sgd_solver.cpp:112] Iteration 113170, lr = 0.01
I0523 01:39:28.127907 35003 solver.cpp:239] Iteration 113180 (2.77199 iter/s, 3.60752s/10 iters), loss = 7.30775
I0523 01:39:28.127948 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30775 (* 1 = 7.30775 loss)
I0523 01:39:28.843425 35003 sgd_solver.cpp:112] Iteration 113180, lr = 0.01
I0523 01:39:31.918823 35003 solver.cpp:239] Iteration 113190 (2.63803 iter/s, 3.79071s/10 iters), loss = 6.74112
I0523 01:39:31.919196 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74112 (* 1 = 6.74112 loss)
I0523 01:39:32.627485 35003 sgd_solver.cpp:112] Iteration 113190, lr = 0.01
I0523 01:39:35.490646 35003 solver.cpp:239] Iteration 113200 (2.80007 iter/s, 3.57133s/10 iters), loss = 6.53893
I0523 01:39:35.490706 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53893 (* 1 = 6.53893 loss)
I0523 01:39:35.503288 35003 sgd_solver.cpp:112] Iteration 113200, lr = 0.01
I0523 01:39:38.550515 35003 solver.cpp:239] Iteration 113210 (3.2683 iter/s, 3.05969s/10 iters), loss = 7.30071
I0523 01:39:38.550560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30071 (* 1 = 7.30071 loss)
I0523 01:39:38.563771 35003 sgd_solver.cpp:112] Iteration 113210, lr = 0.01
I0523 01:39:42.028244 35003 solver.cpp:239] Iteration 113220 (2.8756 iter/s, 3.47754s/10 iters), loss = 8.01864
I0523 01:39:42.028287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01864 (* 1 = 8.01864 loss)
I0523 01:39:42.036584 35003 sgd_solver.cpp:112] Iteration 113220, lr = 0.01
I0523 01:39:44.272106 35003 solver.cpp:239] Iteration 113230 (4.45691 iter/s, 2.24371s/10 iters), loss = 6.52085
I0523 01:39:44.272164 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52085 (* 1 = 6.52085 loss)
I0523 01:39:44.499104 35003 sgd_solver.cpp:112] Iteration 113230, lr = 0.01
I0523 01:39:47.735582 35003 solver.cpp:239] Iteration 113240 (2.88744 iter/s, 3.46328s/10 iters), loss = 6.93106
I0523 01:39:47.735626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93106 (* 1 = 6.93106 loss)
I0523 01:39:48.086526 35003 sgd_solver.cpp:112] Iteration 113240, lr = 0.01
I0523 01:39:50.883226 35003 solver.cpp:239] Iteration 113250 (3.17716 iter/s, 3.14747s/10 iters), loss = 7.22085
I0523 01:39:50.883266 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22085 (* 1 = 7.22085 loss)
I0523 01:39:50.895843 35003 sgd_solver.cpp:112] Iteration 113250, lr = 0.01
I0523 01:39:54.180574 35003 solver.cpp:239] Iteration 113260 (3.03696 iter/s, 3.29276s/10 iters), loss = 7.3269
I0523 01:39:54.180627 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3269 (* 1 = 7.3269 loss)
I0523 01:39:54.921353 35003 sgd_solver.cpp:112] Iteration 113260, lr = 0.01
I0523 01:39:58.415431 35003 solver.cpp:239] Iteration 113270 (2.36148 iter/s, 4.23463s/10 iters), loss = 6.24152
I0523 01:39:58.415473 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24152 (* 1 = 6.24152 loss)
I0523 01:39:59.152730 35003 sgd_solver.cpp:112] Iteration 113270, lr = 0.01
I0523 01:40:02.680654 35003 solver.cpp:239] Iteration 113280 (2.34466 iter/s, 4.265s/10 iters), loss = 9.0083
I0523 01:40:02.680822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 9.0083 (* 1 = 9.0083 loss)
I0523 01:40:03.061756 35003 sgd_solver.cpp:112] Iteration 113280, lr = 0.01
I0523 01:40:05.904728 35003 solver.cpp:239] Iteration 113290 (3.10196 iter/s, 3.22377s/10 iters), loss = 7.60028
I0523 01:40:05.904772 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60028 (* 1 = 7.60028 loss)
I0523 01:40:05.912324 35003 sgd_solver.cpp:112] Iteration 113290, lr = 0.01
I0523 01:40:08.582756 35003 solver.cpp:239] Iteration 113300 (3.73431 iter/s, 2.67787s/10 iters), loss = 5.63053
I0523 01:40:08.582806 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.63053 (* 1 = 5.63053 loss)
I0523 01:40:08.596312 35003 sgd_solver.cpp:112] Iteration 113300, lr = 0.01
I0523 01:40:12.259210 35003 solver.cpp:239] Iteration 113310 (2.72016 iter/s, 3.67625s/10 iters), loss = 6.18961
I0523 01:40:12.259253 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18961 (* 1 = 6.18961 loss)
I0523 01:40:12.272434 35003 sgd_solver.cpp:112] Iteration 113310, lr = 0.01
I0523 01:40:15.917577 35003 solver.cpp:239] Iteration 113320 (2.73361 iter/s, 3.65817s/10 iters), loss = 7.62488
I0523 01:40:15.917629 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62488 (* 1 = 7.62488 loss)
I0523 01:40:15.930575 35003 sgd_solver.cpp:112] Iteration 113320, lr = 0.01
I0523 01:40:18.824240 35003 solver.cpp:239] Iteration 113330 (3.44058 iter/s, 2.90648s/10 iters), loss = 6.26468
I0523 01:40:18.824288 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26468 (* 1 = 6.26468 loss)
I0523 01:40:18.837734 35003 sgd_solver.cpp:112] Iteration 113330, lr = 0.01
I0523 01:40:23.883906 35003 solver.cpp:239] Iteration 113340 (1.97651 iter/s, 5.05941s/10 iters), loss = 7.01083
I0523 01:40:23.883947 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01083 (* 1 = 7.01083 loss)
I0523 01:40:23.885900 35003 sgd_solver.cpp:112] Iteration 113340, lr = 0.01
I0523 01:40:26.951840 35003 solver.cpp:239] Iteration 113350 (3.25972 iter/s, 3.06775s/10 iters), loss = 7.62444
I0523 01:40:26.951876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62444 (* 1 = 7.62444 loss)
I0523 01:40:26.964962 35003 sgd_solver.cpp:112] Iteration 113350, lr = 0.01
I0523 01:40:31.644073 35003 solver.cpp:239] Iteration 113360 (2.13129 iter/s, 4.692s/10 iters), loss = 6.89799
I0523 01:40:31.644120 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89799 (* 1 = 6.89799 loss)
I0523 01:40:31.656746 35003 sgd_solver.cpp:112] Iteration 113360, lr = 0.01
I0523 01:40:34.557047 35003 solver.cpp:239] Iteration 113370 (3.43312 iter/s, 2.9128s/10 iters), loss = 7.29891
I0523 01:40:34.557225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29891 (* 1 = 7.29891 loss)
I0523 01:40:35.252195 35003 sgd_solver.cpp:112] Iteration 113370, lr = 0.01
I0523 01:40:37.442421 35003 solver.cpp:239] Iteration 113380 (3.46612 iter/s, 2.88507s/10 iters), loss = 7.47294
I0523 01:40:37.442462 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47294 (* 1 = 7.47294 loss)
I0523 01:40:37.448118 35003 sgd_solver.cpp:112] Iteration 113380, lr = 0.01
I0523 01:40:42.106662 35003 solver.cpp:239] Iteration 113390 (2.14408 iter/s, 4.66401s/10 iters), loss = 7.92256
I0523 01:40:42.106746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.92256 (* 1 = 7.92256 loss)
I0523 01:40:42.113921 35003 sgd_solver.cpp:112] Iteration 113390, lr = 0.01
I0523 01:40:45.161561 35003 solver.cpp:239] Iteration 113400 (3.27372 iter/s, 3.05463s/10 iters), loss = 6.78887
I0523 01:40:45.161613 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78887 (* 1 = 6.78887 loss)
I0523 01:40:45.169150 35003 sgd_solver.cpp:112] Iteration 113400, lr = 0.01
I0523 01:40:50.035879 35003 solver.cpp:239] Iteration 113410 (2.05168 iter/s, 4.87405s/10 iters), loss = 6.12968
I0523 01:40:50.035948 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12968 (* 1 = 6.12968 loss)
I0523 01:40:50.751401 35003 sgd_solver.cpp:112] Iteration 113410, lr = 0.01
I0523 01:40:53.787204 35003 solver.cpp:239] Iteration 113420 (2.66588 iter/s, 3.7511s/10 iters), loss = 7.49181
I0523 01:40:53.787242 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49181 (* 1 = 7.49181 loss)
I0523 01:40:53.801169 35003 sgd_solver.cpp:112] Iteration 113420, lr = 0.01
I0523 01:40:57.351166 35003 solver.cpp:239] Iteration 113430 (2.80602 iter/s, 3.56377s/10 iters), loss = 6.58662
I0523 01:40:57.351217 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58662 (* 1 = 6.58662 loss)
I0523 01:40:58.072197 35003 sgd_solver.cpp:112] Iteration 113430, lr = 0.01
I0523 01:41:00.523774 35003 solver.cpp:239] Iteration 113440 (3.15216 iter/s, 3.17243s/10 iters), loss = 7.37621
I0523 01:41:00.523816 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37621 (* 1 = 7.37621 loss)
I0523 01:41:00.528626 35003 sgd_solver.cpp:112] Iteration 113440, lr = 0.01
I0523 01:41:01.843665 35003 solver.cpp:239] Iteration 113450 (7.57698 iter/s, 1.31979s/10 iters), loss = 6.64042
I0523 01:41:01.843708 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64042 (* 1 = 6.64042 loss)
I0523 01:41:01.848589 35003 sgd_solver.cpp:112] Iteration 113450, lr = 0.01
I0523 01:41:06.087036 35003 solver.cpp:239] Iteration 113460 (2.35674 iter/s, 4.24314s/10 iters), loss = 7.22048
I0523 01:41:06.087267 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22048 (* 1 = 7.22048 loss)
I0523 01:41:06.094591 35003 sgd_solver.cpp:112] Iteration 113460, lr = 0.01
I0523 01:41:10.033488 35003 solver.cpp:239] Iteration 113470 (2.53417 iter/s, 3.94606s/10 iters), loss = 8.50524
I0523 01:41:10.033537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.50524 (* 1 = 8.50524 loss)
I0523 01:41:10.040782 35003 sgd_solver.cpp:112] Iteration 113470, lr = 0.01
I0523 01:41:14.172370 35003 solver.cpp:239] Iteration 113480 (2.41624 iter/s, 4.13866s/10 iters), loss = 7.18249
I0523 01:41:14.172417 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18249 (* 1 = 7.18249 loss)
I0523 01:41:14.185036 35003 sgd_solver.cpp:112] Iteration 113480, lr = 0.01
I0523 01:41:17.750483 35003 solver.cpp:239] Iteration 113490 (2.79492 iter/s, 3.57792s/10 iters), loss = 6.61781
I0523 01:41:17.750527 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61781 (* 1 = 6.61781 loss)
I0523 01:41:17.763528 35003 sgd_solver.cpp:112] Iteration 113490, lr = 0.01
I0523 01:41:20.617048 35003 solver.cpp:239] Iteration 113500 (3.48873 iter/s, 2.86637s/10 iters), loss = 7.29121
I0523 01:41:20.617115 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29121 (* 1 = 7.29121 loss)
I0523 01:41:20.630156 35003 sgd_solver.cpp:112] Iteration 113500, lr = 0.01
I0523 01:41:23.521039 35003 solver.cpp:239] Iteration 113510 (3.44376 iter/s, 2.9038s/10 iters), loss = 7.22165
I0523 01:41:23.521096 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22165 (* 1 = 7.22165 loss)
I0523 01:41:23.529212 35003 sgd_solver.cpp:112] Iteration 113510, lr = 0.01
I0523 01:41:27.047271 35003 solver.cpp:239] Iteration 113520 (2.83606 iter/s, 3.52602s/10 iters), loss = 6.81268
I0523 01:41:27.047322 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81268 (* 1 = 6.81268 loss)
I0523 01:41:27.107264 35003 sgd_solver.cpp:112] Iteration 113520, lr = 0.01
I0523 01:41:30.703285 35003 solver.cpp:239] Iteration 113530 (2.73537 iter/s, 3.65581s/10 iters), loss = 6.7071
I0523 01:41:30.703326 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7071 (* 1 = 6.7071 loss)
I0523 01:41:30.716565 35003 sgd_solver.cpp:112] Iteration 113530, lr = 0.01
I0523 01:41:34.607434 35003 solver.cpp:239] Iteration 113540 (2.56151 iter/s, 3.90395s/10 iters), loss = 6.75784
I0523 01:41:34.607478 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75784 (* 1 = 6.75784 loss)
I0523 01:41:34.617744 35003 sgd_solver.cpp:112] Iteration 113540, lr = 0.01
I0523 01:41:39.030939 35003 solver.cpp:239] Iteration 113550 (2.26076 iter/s, 4.42328s/10 iters), loss = 7.14873
I0523 01:41:39.031179 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14873 (* 1 = 7.14873 loss)
I0523 01:41:39.039417 35003 sgd_solver.cpp:112] Iteration 113550, lr = 0.01
I0523 01:41:41.837363 35003 solver.cpp:239] Iteration 113560 (3.56367 iter/s, 2.8061s/10 iters), loss = 7.60824
I0523 01:41:41.837409 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60824 (* 1 = 7.60824 loss)
I0523 01:41:42.576675 35003 sgd_solver.cpp:112] Iteration 113560, lr = 0.01
I0523 01:41:46.762069 35003 solver.cpp:239] Iteration 113570 (2.03068 iter/s, 4.92446s/10 iters), loss = 5.84933
I0523 01:41:46.762120 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84933 (* 1 = 5.84933 loss)
I0523 01:41:46.846441 35003 sgd_solver.cpp:112] Iteration 113570, lr = 0.01
I0523 01:41:49.667801 35003 solver.cpp:239] Iteration 113580 (3.44168 iter/s, 2.90556s/10 iters), loss = 7.29499
I0523 01:41:49.667848 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29499 (* 1 = 7.29499 loss)
I0523 01:41:49.690090 35003 sgd_solver.cpp:112] Iteration 113580, lr = 0.01
I0523 01:41:53.159539 35003 solver.cpp:239] Iteration 113590 (2.86406 iter/s, 3.49154s/10 iters), loss = 7.01551
I0523 01:41:53.159591 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01551 (* 1 = 7.01551 loss)
I0523 01:41:53.167469 35003 sgd_solver.cpp:112] Iteration 113590, lr = 0.01
I0523 01:41:57.051062 35003 solver.cpp:239] Iteration 113600 (2.56983 iter/s, 3.89131s/10 iters), loss = 7.72635
I0523 01:41:57.051102 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72635 (* 1 = 7.72635 loss)
I0523 01:41:57.792601 35003 sgd_solver.cpp:112] Iteration 113600, lr = 0.01
I0523 01:41:59.325873 35003 solver.cpp:239] Iteration 113610 (4.39625 iter/s, 2.27467s/10 iters), loss = 7.17466
I0523 01:41:59.325937 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17466 (* 1 = 7.17466 loss)
I0523 01:42:00.027791 35003 sgd_solver.cpp:112] Iteration 113610, lr = 0.01
I0523 01:42:03.535924 35003 solver.cpp:239] Iteration 113620 (2.3754 iter/s, 4.20982s/10 iters), loss = 8.11461
I0523 01:42:03.535972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.11461 (* 1 = 8.11461 loss)
I0523 01:42:04.251128 35003 sgd_solver.cpp:112] Iteration 113620, lr = 0.01
I0523 01:42:07.050920 35003 solver.cpp:239] Iteration 113630 (2.84513 iter/s, 3.51478s/10 iters), loss = 6.98661
I0523 01:42:07.050963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98661 (* 1 = 6.98661 loss)
I0523 01:42:07.064205 35003 sgd_solver.cpp:112] Iteration 113630, lr = 0.01
I0523 01:42:11.454861 35003 solver.cpp:239] Iteration 113640 (2.27081 iter/s, 4.40371s/10 iters), loss = 5.99908
I0523 01:42:11.455037 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99908 (* 1 = 5.99908 loss)
I0523 01:42:11.468556 35003 sgd_solver.cpp:112] Iteration 113640, lr = 0.01
I0523 01:42:15.772300 35003 solver.cpp:239] Iteration 113650 (2.31637 iter/s, 4.31709s/10 iters), loss = 7.186
I0523 01:42:15.772342 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.186 (* 1 = 7.186 loss)
I0523 01:42:16.494320 35003 sgd_solver.cpp:112] Iteration 113650, lr = 0.01
I0523 01:42:19.478987 35003 solver.cpp:239] Iteration 113660 (2.69798 iter/s, 3.70648s/10 iters), loss = 8.49371
I0523 01:42:19.479043 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.49371 (* 1 = 8.49371 loss)
I0523 01:42:20.212880 35003 sgd_solver.cpp:112] Iteration 113660, lr = 0.01
I0523 01:42:22.990185 35003 solver.cpp:239] Iteration 113670 (2.8482 iter/s, 3.51099s/10 iters), loss = 7.96828
I0523 01:42:22.990238 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96828 (* 1 = 7.96828 loss)
I0523 01:42:23.632968 35003 sgd_solver.cpp:112] Iteration 113670, lr = 0.01
I0523 01:42:26.393609 35003 solver.cpp:239] Iteration 113680 (2.93839 iter/s, 3.40323s/10 iters), loss = 7.85142
I0523 01:42:26.393652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85142 (* 1 = 7.85142 loss)
I0523 01:42:26.401944 35003 sgd_solver.cpp:112] Iteration 113680, lr = 0.01
I0523 01:42:29.247071 35003 solver.cpp:239] Iteration 113690 (3.50472 iter/s, 2.85329s/10 iters), loss = 5.57592
I0523 01:42:29.247108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.57592 (* 1 = 5.57592 loss)
I0523 01:42:29.251920 35003 sgd_solver.cpp:112] Iteration 113690, lr = 0.01
I0523 01:42:31.358733 35003 solver.cpp:239] Iteration 113700 (4.7359 iter/s, 2.11153s/10 iters), loss = 5.6577
I0523 01:42:31.358783 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.6577 (* 1 = 5.6577 loss)
I0523 01:42:32.099269 35003 sgd_solver.cpp:112] Iteration 113700, lr = 0.01
I0523 01:42:34.883478 35003 solver.cpp:239] Iteration 113710 (2.83725 iter/s, 3.52454s/10 iters), loss = 7.8366
I0523 01:42:34.883524 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8366 (* 1 = 7.8366 loss)
I0523 01:42:34.891110 35003 sgd_solver.cpp:112] Iteration 113710, lr = 0.01
I0523 01:42:37.858803 35003 solver.cpp:239] Iteration 113720 (3.36118 iter/s, 2.97515s/10 iters), loss = 6.58973
I0523 01:42:37.858853 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58973 (* 1 = 6.58973 loss)
I0523 01:42:38.560480 35003 sgd_solver.cpp:112] Iteration 113720, lr = 0.01
I0523 01:42:40.642364 35003 solver.cpp:239] Iteration 113730 (3.59274 iter/s, 2.78339s/10 iters), loss = 6.81017
I0523 01:42:40.642421 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81017 (* 1 = 6.81017 loss)
I0523 01:42:40.655660 35003 sgd_solver.cpp:112] Iteration 113730, lr = 0.01
I0523 01:42:44.181742 35003 solver.cpp:239] Iteration 113740 (2.82552 iter/s, 3.53917s/10 iters), loss = 8.35241
I0523 01:42:44.181929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.35241 (* 1 = 8.35241 loss)
I0523 01:42:44.186936 35003 sgd_solver.cpp:112] Iteration 113740, lr = 0.01
I0523 01:42:47.617514 35003 solver.cpp:239] Iteration 113750 (2.91082 iter/s, 3.43546s/10 iters), loss = 6.428
I0523 01:42:47.617563 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.428 (* 1 = 6.428 loss)
I0523 01:42:48.312158 35003 sgd_solver.cpp:112] Iteration 113750, lr = 0.01
I0523 01:42:52.797863 35003 solver.cpp:239] Iteration 113760 (1.93047 iter/s, 5.1801s/10 iters), loss = 7.02496
I0523 01:42:52.797909 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02496 (* 1 = 7.02496 loss)
I0523 01:42:52.879590 35003 sgd_solver.cpp:112] Iteration 113760, lr = 0.01
I0523 01:42:54.965675 35003 solver.cpp:239] Iteration 113770 (4.61326 iter/s, 2.16766s/10 iters), loss = 5.87208
I0523 01:42:54.965731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87208 (* 1 = 5.87208 loss)
I0523 01:42:54.973340 35003 sgd_solver.cpp:112] Iteration 113770, lr = 0.01
I0523 01:42:57.395225 35003 solver.cpp:239] Iteration 113780 (4.11628 iter/s, 2.42938s/10 iters), loss = 6.61882
I0523 01:42:57.395288 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61882 (* 1 = 6.61882 loss)
I0523 01:42:57.409456 35003 sgd_solver.cpp:112] Iteration 113780, lr = 0.01
I0523 01:43:01.006064 35003 solver.cpp:239] Iteration 113790 (2.76961 iter/s, 3.61062s/10 iters), loss = 5.23624
I0523 01:43:01.006115 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.23624 (* 1 = 5.23624 loss)
I0523 01:43:01.016455 35003 sgd_solver.cpp:112] Iteration 113790, lr = 0.01
I0523 01:43:03.088809 35003 solver.cpp:239] Iteration 113800 (4.80169 iter/s, 2.0826s/10 iters), loss = 8.06125
I0523 01:43:03.088860 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.06125 (* 1 = 8.06125 loss)
I0523 01:43:03.098109 35003 sgd_solver.cpp:112] Iteration 113800, lr = 0.01
I0523 01:43:07.399646 35003 solver.cpp:239] Iteration 113810 (2.31986 iter/s, 4.31061s/10 iters), loss = 7.09396
I0523 01:43:07.399699 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09396 (* 1 = 7.09396 loss)
I0523 01:43:07.413230 35003 sgd_solver.cpp:112] Iteration 113810, lr = 0.01
I0523 01:43:10.409291 35003 solver.cpp:239] Iteration 113820 (3.32285 iter/s, 3.00947s/10 iters), loss = 6.73693
I0523 01:43:10.409333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73693 (* 1 = 6.73693 loss)
I0523 01:43:10.423693 35003 sgd_solver.cpp:112] Iteration 113820, lr = 0.01
I0523 01:43:13.444464 35003 solver.cpp:239] Iteration 113830 (3.29489 iter/s, 3.035s/10 iters), loss = 6.21945
I0523 01:43:13.444512 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21945 (* 1 = 6.21945 loss)
I0523 01:43:13.720871 35003 sgd_solver.cpp:112] Iteration 113830, lr = 0.01
I0523 01:43:18.123306 35003 solver.cpp:239] Iteration 113840 (2.1374 iter/s, 4.67858s/10 iters), loss = 7.50291
I0523 01:43:18.123464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50291 (* 1 = 7.50291 loss)
I0523 01:43:18.135704 35003 sgd_solver.cpp:112] Iteration 113840, lr = 0.01
I0523 01:43:21.288375 35003 solver.cpp:239] Iteration 113850 (3.15978 iter/s, 3.16478s/10 iters), loss = 7.23686
I0523 01:43:21.288414 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23686 (* 1 = 7.23686 loss)
I0523 01:43:21.301405 35003 sgd_solver.cpp:112] Iteration 113850, lr = 0.01
I0523 01:43:25.104967 35003 solver.cpp:239] Iteration 113860 (2.62028 iter/s, 3.81638s/10 iters), loss = 7.16923
I0523 01:43:25.105041 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16923 (* 1 = 7.16923 loss)
I0523 01:43:25.839661 35003 sgd_solver.cpp:112] Iteration 113860, lr = 0.01
I0523 01:43:29.884505 35003 solver.cpp:239] Iteration 113870 (2.09237 iter/s, 4.77927s/10 iters), loss = 8.51042
I0523 01:43:29.884567 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.51042 (* 1 = 8.51042 loss)
I0523 01:43:30.619441 35003 sgd_solver.cpp:112] Iteration 113870, lr = 0.01
I0523 01:43:34.841399 35003 solver.cpp:239] Iteration 113880 (2.0175 iter/s, 4.95663s/10 iters), loss = 6.89375
I0523 01:43:34.841454 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89375 (* 1 = 6.89375 loss)
I0523 01:43:34.940343 35003 sgd_solver.cpp:112] Iteration 113880, lr = 0.01
I0523 01:43:40.041368 35003 solver.cpp:239] Iteration 113890 (1.92319 iter/s, 5.1997s/10 iters), loss = 7.06596
I0523 01:43:40.041409 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06596 (* 1 = 7.06596 loss)
I0523 01:43:40.049607 35003 sgd_solver.cpp:112] Iteration 113890, lr = 0.01
I0523 01:43:43.440152 35003 solver.cpp:239] Iteration 113900 (2.94239 iter/s, 3.3986s/10 iters), loss = 7.90469
I0523 01:43:43.440198 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90469 (* 1 = 7.90469 loss)
I0523 01:43:43.550916 35003 sgd_solver.cpp:112] Iteration 113900, lr = 0.01
I0523 01:43:44.923229 35003 solver.cpp:239] Iteration 113910 (6.74326 iter/s, 1.48296s/10 iters), loss = 7.1651
I0523 01:43:44.923271 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1651 (* 1 = 7.1651 loss)
I0523 01:43:44.933351 35003 sgd_solver.cpp:112] Iteration 113910, lr = 0.01
I0523 01:43:46.832041 35003 solver.cpp:239] Iteration 113920 (5.23921 iter/s, 1.90868s/10 iters), loss = 7.0026
I0523 01:43:46.832105 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0026 (* 1 = 7.0026 loss)
I0523 01:43:47.204732 35003 sgd_solver.cpp:112] Iteration 113920, lr = 0.01
I0523 01:43:50.401322 35003 solver.cpp:239] Iteration 113930 (2.80185 iter/s, 3.56907s/10 iters), loss = 7.54662
I0523 01:43:50.401648 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54662 (* 1 = 7.54662 loss)
I0523 01:43:51.129884 35003 sgd_solver.cpp:112] Iteration 113930, lr = 0.01
I0523 01:43:54.429016 35003 solver.cpp:239] Iteration 113940 (2.48309 iter/s, 4.02724s/10 iters), loss = 5.51331
I0523 01:43:54.429069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.51331 (* 1 = 5.51331 loss)
I0523 01:43:54.442005 35003 sgd_solver.cpp:112] Iteration 113940, lr = 0.01
I0523 01:43:57.236732 35003 solver.cpp:239] Iteration 113950 (3.56183 iter/s, 2.80754s/10 iters), loss = 7.03808
I0523 01:43:57.236775 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03808 (* 1 = 7.03808 loss)
I0523 01:43:57.244390 35003 sgd_solver.cpp:112] Iteration 113950, lr = 0.01
I0523 01:44:00.765614 35003 solver.cpp:239] Iteration 113960 (2.83392 iter/s, 3.52868s/10 iters), loss = 6.78791
I0523 01:44:00.765677 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78791 (* 1 = 6.78791 loss)
I0523 01:44:00.770606 35003 sgd_solver.cpp:112] Iteration 113960, lr = 0.01
I0523 01:44:03.103576 35003 solver.cpp:239] Iteration 113970 (4.27754 iter/s, 2.33779s/10 iters), loss = 6.56797
I0523 01:44:03.103621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56797 (* 1 = 6.56797 loss)
I0523 01:44:03.766978 35003 sgd_solver.cpp:112] Iteration 113970, lr = 0.01
I0523 01:44:05.799087 35003 solver.cpp:239] Iteration 113980 (3.71011 iter/s, 2.69534s/10 iters), loss = 6.87733
I0523 01:44:05.799157 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87733 (* 1 = 6.87733 loss)
I0523 01:44:05.806622 35003 sgd_solver.cpp:112] Iteration 113980, lr = 0.01
I0523 01:44:09.303791 35003 solver.cpp:239] Iteration 113990 (2.85348 iter/s, 3.50449s/10 iters), loss = 6.5949
I0523 01:44:09.303833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5949 (* 1 = 6.5949 loss)
I0523 01:44:09.330489 35003 sgd_solver.cpp:112] Iteration 113990, lr = 0.01
I0523 01:44:12.050366 35003 solver.cpp:239] Iteration 114000 (3.64111 iter/s, 2.74642s/10 iters), loss = 8.04723
I0523 01:44:12.050410 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04723 (* 1 = 8.04723 loss)
I0523 01:44:12.752717 35003 sgd_solver.cpp:112] Iteration 114000, lr = 0.01
I0523 01:44:17.069751 35003 solver.cpp:239] Iteration 114010 (1.99238 iter/s, 5.01913s/10 iters), loss = 6.53682
I0523 01:44:17.069805 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53682 (* 1 = 6.53682 loss)
I0523 01:44:17.080632 35003 sgd_solver.cpp:112] Iteration 114010, lr = 0.01
I0523 01:44:20.838163 35003 solver.cpp:239] Iteration 114020 (2.65378 iter/s, 3.76821s/10 iters), loss = 6.27075
I0523 01:44:20.838333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27075 (* 1 = 6.27075 loss)
I0523 01:44:21.505779 35003 sgd_solver.cpp:112] Iteration 114020, lr = 0.01
I0523 01:44:24.262923 35003 solver.cpp:239] Iteration 114030 (2.92018 iter/s, 3.42445s/10 iters), loss = 7.67944
I0523 01:44:24.262964 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67944 (* 1 = 7.67944 loss)
I0523 01:44:24.271159 35003 sgd_solver.cpp:112] Iteration 114030, lr = 0.01
I0523 01:44:27.673964 35003 solver.cpp:239] Iteration 114040 (2.93182 iter/s, 3.41086s/10 iters), loss = 8.53434
I0523 01:44:27.674010 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.53434 (* 1 = 8.53434 loss)
I0523 01:44:27.683689 35003 sgd_solver.cpp:112] Iteration 114040, lr = 0.01
I0523 01:44:31.958107 35003 solver.cpp:239] Iteration 114050 (2.33431 iter/s, 4.28392s/10 iters), loss = 5.84622
I0523 01:44:31.958149 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84622 (* 1 = 5.84622 loss)
I0523 01:44:31.971505 35003 sgd_solver.cpp:112] Iteration 114050, lr = 0.01
I0523 01:44:36.236521 35003 solver.cpp:239] Iteration 114060 (2.33744 iter/s, 4.27818s/10 iters), loss = 6.06927
I0523 01:44:36.236577 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06927 (* 1 = 6.06927 loss)
I0523 01:44:36.954233 35003 sgd_solver.cpp:112] Iteration 114060, lr = 0.01
I0523 01:44:41.264161 35003 solver.cpp:239] Iteration 114070 (1.98911 iter/s, 5.02737s/10 iters), loss = 6.63021
I0523 01:44:41.264206 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63021 (* 1 = 6.63021 loss)
I0523 01:44:41.271531 35003 sgd_solver.cpp:112] Iteration 114070, lr = 0.01
I0523 01:44:44.921347 35003 solver.cpp:239] Iteration 114080 (2.73449 iter/s, 3.65699s/10 iters), loss = 7.4154
I0523 01:44:44.921411 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4154 (* 1 = 7.4154 loss)
I0523 01:44:44.927239 35003 sgd_solver.cpp:112] Iteration 114080, lr = 0.01
I0523 01:44:50.094292 35003 solver.cpp:239] Iteration 114090 (1.93324 iter/s, 5.17267s/10 iters), loss = 6.49473
I0523 01:44:50.094348 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49473 (* 1 = 6.49473 loss)
I0523 01:44:50.101969 35003 sgd_solver.cpp:112] Iteration 114090, lr = 0.01
I0523 01:44:53.021831 35003 solver.cpp:239] Iteration 114100 (3.41604 iter/s, 2.92736s/10 iters), loss = 7.01137
I0523 01:44:53.022011 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01137 (* 1 = 7.01137 loss)
I0523 01:44:53.031751 35003 sgd_solver.cpp:112] Iteration 114100, lr = 0.01
I0523 01:44:56.906080 35003 solver.cpp:239] Iteration 114110 (2.57473 iter/s, 3.88391s/10 iters), loss = 7.43302
I0523 01:44:56.906138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43302 (* 1 = 7.43302 loss)
I0523 01:44:57.647009 35003 sgd_solver.cpp:112] Iteration 114110, lr = 0.01
I0523 01:45:01.105003 35003 solver.cpp:239] Iteration 114120 (2.38169 iter/s, 4.19869s/10 iters), loss = 7.23588
I0523 01:45:01.105049 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23588 (* 1 = 7.23588 loss)
I0523 01:45:01.123569 35003 sgd_solver.cpp:112] Iteration 114120, lr = 0.01
I0523 01:45:04.891239 35003 solver.cpp:239] Iteration 114130 (2.64129 iter/s, 3.78603s/10 iters), loss = 7.1073
I0523 01:45:04.891288 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1073 (* 1 = 7.1073 loss)
I0523 01:45:05.632302 35003 sgd_solver.cpp:112] Iteration 114130, lr = 0.01
I0523 01:45:07.780755 35003 solver.cpp:239] Iteration 114140 (3.46099 iter/s, 2.88934s/10 iters), loss = 5.75918
I0523 01:45:07.780802 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.75918 (* 1 = 5.75918 loss)
I0523 01:45:07.793778 35003 sgd_solver.cpp:112] Iteration 114140, lr = 0.01
I0523 01:45:09.851346 35003 solver.cpp:239] Iteration 114150 (4.82986 iter/s, 2.07045s/10 iters), loss = 7.67551
I0523 01:45:09.851397 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67551 (* 1 = 7.67551 loss)
I0523 01:45:10.592259 35003 sgd_solver.cpp:112] Iteration 114150, lr = 0.01
I0523 01:45:14.180806 35003 solver.cpp:239] Iteration 114160 (2.30989 iter/s, 4.32922s/10 iters), loss = 7.20116
I0523 01:45:14.180876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20116 (* 1 = 7.20116 loss)
I0523 01:45:14.882277 35003 sgd_solver.cpp:112] Iteration 114160, lr = 0.01
I0523 01:45:19.269443 35003 solver.cpp:239] Iteration 114170 (1.96527 iter/s, 5.08837s/10 iters), loss = 6.93257
I0523 01:45:19.269490 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93257 (* 1 = 6.93257 loss)
I0523 01:45:19.276818 35003 sgd_solver.cpp:112] Iteration 114170, lr = 0.01
I0523 01:45:22.814929 35003 solver.cpp:239] Iteration 114180 (2.82064 iter/s, 3.54529s/10 iters), loss = 6.58395
I0523 01:45:22.814975 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58395 (* 1 = 6.58395 loss)
I0523 01:45:22.825124 35003 sgd_solver.cpp:112] Iteration 114180, lr = 0.01
I0523 01:45:25.741639 35003 solver.cpp:239] Iteration 114190 (3.41701 iter/s, 2.92654s/10 iters), loss = 6.78506
I0523 01:45:25.741832 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78506 (* 1 = 6.78506 loss)
I0523 01:45:25.755208 35003 sgd_solver.cpp:112] Iteration 114190, lr = 0.01
I0523 01:45:27.968179 35003 solver.cpp:239] Iteration 114200 (4.49185 iter/s, 2.22625s/10 iters), loss = 6.4215
I0523 01:45:27.968225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4215 (* 1 = 6.4215 loss)
I0523 01:45:27.979323 35003 sgd_solver.cpp:112] Iteration 114200, lr = 0.01
I0523 01:45:31.865732 35003 solver.cpp:239] Iteration 114210 (2.56585 iter/s, 3.89734s/10 iters), loss = 6.68368
I0523 01:45:31.865782 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68368 (* 1 = 6.68368 loss)
I0523 01:45:31.878809 35003 sgd_solver.cpp:112] Iteration 114210, lr = 0.01
I0523 01:45:36.229259 35003 solver.cpp:239] Iteration 114220 (2.29185 iter/s, 4.36329s/10 iters), loss = 6.83223
I0523 01:45:36.229322 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83223 (* 1 = 6.83223 loss)
I0523 01:45:36.237294 35003 sgd_solver.cpp:112] Iteration 114220, lr = 0.01
I0523 01:45:38.714273 35003 solver.cpp:239] Iteration 114230 (4.02441 iter/s, 2.48484s/10 iters), loss = 6.63116
I0523 01:45:38.714313 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63116 (* 1 = 6.63116 loss)
I0523 01:45:39.429596 35003 sgd_solver.cpp:112] Iteration 114230, lr = 0.01
I0523 01:45:43.749065 35003 solver.cpp:239] Iteration 114240 (1.98628 iter/s, 5.03454s/10 iters), loss = 6.08621
I0523 01:45:43.749109 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08621 (* 1 = 6.08621 loss)
I0523 01:45:43.754197 35003 sgd_solver.cpp:112] Iteration 114240, lr = 0.01
I0523 01:45:47.004045 35003 solver.cpp:239] Iteration 114250 (3.0724 iter/s, 3.25478s/10 iters), loss = 6.92483
I0523 01:45:47.004098 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92483 (* 1 = 6.92483 loss)
I0523 01:45:47.016275 35003 sgd_solver.cpp:112] Iteration 114250, lr = 0.01
I0523 01:45:49.806483 35003 solver.cpp:239] Iteration 114260 (3.57413 iter/s, 2.79788s/10 iters), loss = 6.99226
I0523 01:45:49.806542 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99226 (* 1 = 6.99226 loss)
I0523 01:45:49.819998 35003 sgd_solver.cpp:112] Iteration 114260, lr = 0.01
I0523 01:45:52.662279 35003 solver.cpp:239] Iteration 114270 (3.50187 iter/s, 2.85562s/10 iters), loss = 7.13012
I0523 01:45:52.662328 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13012 (* 1 = 7.13012 loss)
I0523 01:45:53.403599 35003 sgd_solver.cpp:112] Iteration 114270, lr = 0.01
I0523 01:45:56.194340 35003 solver.cpp:239] Iteration 114280 (2.83137 iter/s, 3.53186s/10 iters), loss = 7.17623
I0523 01:45:56.194540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17623 (* 1 = 7.17623 loss)
I0523 01:45:56.881937 35003 sgd_solver.cpp:112] Iteration 114280, lr = 0.01
I0523 01:46:00.292369 35003 solver.cpp:239] Iteration 114290 (2.44042 iter/s, 4.09766s/10 iters), loss = 7.21347
I0523 01:46:00.292425 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21347 (* 1 = 7.21347 loss)
I0523 01:46:01.007206 35003 sgd_solver.cpp:112] Iteration 114290, lr = 0.01
I0523 01:46:05.899889 35003 solver.cpp:239] Iteration 114300 (1.78341 iter/s, 5.60723s/10 iters), loss = 6.67405
I0523 01:46:05.899947 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67405 (* 1 = 6.67405 loss)
I0523 01:46:06.614787 35003 sgd_solver.cpp:112] Iteration 114300, lr = 0.01
I0523 01:46:08.770117 35003 solver.cpp:239] Iteration 114310 (3.48426 iter/s, 2.87005s/10 iters), loss = 6.58165
I0523 01:46:08.770171 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58165 (* 1 = 6.58165 loss)
I0523 01:46:08.775846 35003 sgd_solver.cpp:112] Iteration 114310, lr = 0.01
I0523 01:46:11.693682 35003 solver.cpp:239] Iteration 114320 (3.42069 iter/s, 2.92339s/10 iters), loss = 7.46904
I0523 01:46:11.693725 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46904 (* 1 = 7.46904 loss)
I0523 01:46:11.703163 35003 sgd_solver.cpp:112] Iteration 114320, lr = 0.01
I0523 01:46:14.462942 35003 solver.cpp:239] Iteration 114330 (3.61129 iter/s, 2.7691s/10 iters), loss = 7.55354
I0523 01:46:14.462997 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55354 (* 1 = 7.55354 loss)
I0523 01:46:14.474570 35003 sgd_solver.cpp:112] Iteration 114330, lr = 0.01
I0523 01:46:18.083359 35003 solver.cpp:239] Iteration 114340 (2.76227 iter/s, 3.62021s/10 iters), loss = 7.28546
I0523 01:46:18.083402 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28546 (* 1 = 7.28546 loss)
I0523 01:46:18.096333 35003 sgd_solver.cpp:112] Iteration 114340, lr = 0.01
I0523 01:46:20.432917 35003 solver.cpp:239] Iteration 114350 (4.25638 iter/s, 2.34941s/10 iters), loss = 7.73634
I0523 01:46:20.432963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73634 (* 1 = 7.73634 loss)
I0523 01:46:20.445364 35003 sgd_solver.cpp:112] Iteration 114350, lr = 0.01
I0523 01:46:23.189370 35003 solver.cpp:239] Iteration 114360 (3.62807 iter/s, 2.75629s/10 iters), loss = 6.78556
I0523 01:46:23.189419 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78556 (* 1 = 6.78556 loss)
I0523 01:46:23.860565 35003 sgd_solver.cpp:112] Iteration 114360, lr = 0.01
I0523 01:46:27.479660 35003 solver.cpp:239] Iteration 114370 (2.33097 iter/s, 4.29007s/10 iters), loss = 7.1703
I0523 01:46:27.479900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1703 (* 1 = 7.1703 loss)
I0523 01:46:27.574838 35003 sgd_solver.cpp:112] Iteration 114370, lr = 0.01
I0523 01:46:30.463510 35003 solver.cpp:239] Iteration 114380 (3.35176 iter/s, 2.98351s/10 iters), loss = 7.21921
I0523 01:46:30.463572 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21921 (* 1 = 7.21921 loss)
I0523 01:46:30.767204 35003 sgd_solver.cpp:112] Iteration 114380, lr = 0.01
I0523 01:46:34.303647 35003 solver.cpp:239] Iteration 114390 (2.60422 iter/s, 3.83992s/10 iters), loss = 6.74257
I0523 01:46:34.303690 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74257 (* 1 = 6.74257 loss)
I0523 01:46:34.311051 35003 sgd_solver.cpp:112] Iteration 114390, lr = 0.01
I0523 01:46:38.251929 35003 solver.cpp:239] Iteration 114400 (2.53288 iter/s, 3.94807s/10 iters), loss = 7.1343
I0523 01:46:38.251991 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1343 (* 1 = 7.1343 loss)
I0523 01:46:38.259167 35003 sgd_solver.cpp:112] Iteration 114400, lr = 0.01
I0523 01:46:41.694295 35003 solver.cpp:239] Iteration 114410 (2.90516 iter/s, 3.44216s/10 iters), loss = 6.09891
I0523 01:46:41.694345 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09891 (* 1 = 6.09891 loss)
I0523 01:46:42.405247 35003 sgd_solver.cpp:112] Iteration 114410, lr = 0.01
I0523 01:46:45.023862 35003 solver.cpp:239] Iteration 114420 (3.00357 iter/s, 3.32937s/10 iters), loss = 6.89532
I0523 01:46:45.023914 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89532 (* 1 = 6.89532 loss)
I0523 01:46:45.028322 35003 sgd_solver.cpp:112] Iteration 114420, lr = 0.01
I0523 01:46:48.508354 35003 solver.cpp:239] Iteration 114430 (2.87002 iter/s, 3.48429s/10 iters), loss = 6.71265
I0523 01:46:48.508412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71265 (* 1 = 6.71265 loss)
I0523 01:46:48.516116 35003 sgd_solver.cpp:112] Iteration 114430, lr = 0.01
I0523 01:46:52.073189 35003 solver.cpp:239] Iteration 114440 (2.80534 iter/s, 3.56463s/10 iters), loss = 6.22301
I0523 01:46:52.073231 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22301 (* 1 = 6.22301 loss)
I0523 01:46:52.086470 35003 sgd_solver.cpp:112] Iteration 114440, lr = 0.01
I0523 01:46:55.092355 35003 solver.cpp:239] Iteration 114450 (3.31238 iter/s, 3.01898s/10 iters), loss = 7.00873
I0523 01:46:55.092412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00873 (* 1 = 7.00873 loss)
I0523 01:46:55.095178 35003 sgd_solver.cpp:112] Iteration 114450, lr = 0.01
I0523 01:46:57.946411 35003 solver.cpp:239] Iteration 114460 (3.504 iter/s, 2.85388s/10 iters), loss = 7.96387
I0523 01:46:57.946724 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96387 (* 1 = 7.96387 loss)
I0523 01:46:58.674994 35003 sgd_solver.cpp:112] Iteration 114460, lr = 0.01
I0523 01:47:01.464499 35003 solver.cpp:239] Iteration 114470 (2.84278 iter/s, 3.51768s/10 iters), loss = 6.94952
I0523 01:47:01.464540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94952 (* 1 = 6.94952 loss)
I0523 01:47:01.493157 35003 sgd_solver.cpp:112] Iteration 114470, lr = 0.01
I0523 01:47:05.018393 35003 solver.cpp:239] Iteration 114480 (2.81396 iter/s, 3.5537s/10 iters), loss = 7.11909
I0523 01:47:05.018442 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11909 (* 1 = 7.11909 loss)
I0523 01:47:05.024307 35003 sgd_solver.cpp:112] Iteration 114480, lr = 0.01
I0523 01:47:08.614966 35003 solver.cpp:239] Iteration 114490 (2.78059 iter/s, 3.59636s/10 iters), loss = 7.7867
I0523 01:47:08.615025 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7867 (* 1 = 7.7867 loss)
I0523 01:47:08.666301 35003 sgd_solver.cpp:112] Iteration 114490, lr = 0.01
I0523 01:47:11.289317 35003 solver.cpp:239] Iteration 114500 (3.73946 iter/s, 2.67418s/10 iters), loss = 7.18197
I0523 01:47:11.289353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18197 (* 1 = 7.18197 loss)
I0523 01:47:11.302086 35003 sgd_solver.cpp:112] Iteration 114500, lr = 0.01
I0523 01:47:14.233136 35003 solver.cpp:239] Iteration 114510 (3.39714 iter/s, 2.94366s/10 iters), loss = 7.20904
I0523 01:47:14.233188 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20904 (* 1 = 7.20904 loss)
I0523 01:47:14.239841 35003 sgd_solver.cpp:112] Iteration 114510, lr = 0.01
I0523 01:47:17.811508 35003 solver.cpp:239] Iteration 114520 (2.79472 iter/s, 3.57817s/10 iters), loss = 6.50695
I0523 01:47:17.811550 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50695 (* 1 = 6.50695 loss)
I0523 01:47:17.818033 35003 sgd_solver.cpp:112] Iteration 114520, lr = 0.01
I0523 01:47:21.192479 35003 solver.cpp:239] Iteration 114530 (2.95789 iter/s, 3.38078s/10 iters), loss = 6.61596
I0523 01:47:21.192523 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61596 (* 1 = 6.61596 loss)
I0523 01:47:21.205791 35003 sgd_solver.cpp:112] Iteration 114530, lr = 0.01
I0523 01:47:25.511715 35003 solver.cpp:239] Iteration 114540 (2.31534 iter/s, 4.31902s/10 iters), loss = 7.3486
I0523 01:47:25.511759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3486 (* 1 = 7.3486 loss)
I0523 01:47:25.517941 35003 sgd_solver.cpp:112] Iteration 114540, lr = 0.01
I0523 01:47:28.112690 35003 solver.cpp:239] Iteration 114550 (3.84494 iter/s, 2.60082s/10 iters), loss = 7.92742
I0523 01:47:28.112900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.92742 (* 1 = 7.92742 loss)
I0523 01:47:28.126332 35003 sgd_solver.cpp:112] Iteration 114550, lr = 0.01
I0523 01:47:33.320466 35003 solver.cpp:239] Iteration 114560 (1.92036 iter/s, 5.20737s/10 iters), loss = 6.09102
I0523 01:47:33.320514 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09102 (* 1 = 6.09102 loss)
I0523 01:47:33.934181 35003 sgd_solver.cpp:112] Iteration 114560, lr = 0.01
I0523 01:47:38.793215 35003 solver.cpp:239] Iteration 114570 (1.82733 iter/s, 5.47247s/10 iters), loss = 7.69787
I0523 01:47:38.793282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69787 (* 1 = 7.69787 loss)
I0523 01:47:38.800869 35003 sgd_solver.cpp:112] Iteration 114570, lr = 0.01
I0523 01:47:40.892807 35003 solver.cpp:239] Iteration 114580 (4.76318 iter/s, 2.09944s/10 iters), loss = 6.18403
I0523 01:47:40.892845 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18403 (* 1 = 6.18403 loss)
I0523 01:47:41.633857 35003 sgd_solver.cpp:112] Iteration 114580, lr = 0.01
I0523 01:47:44.410877 35003 solver.cpp:239] Iteration 114590 (2.84262 iter/s, 3.51788s/10 iters), loss = 6.73897
I0523 01:47:44.410920 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73897 (* 1 = 6.73897 loss)
I0523 01:47:44.423410 35003 sgd_solver.cpp:112] Iteration 114590, lr = 0.01
I0523 01:47:46.886168 35003 solver.cpp:239] Iteration 114600 (4.04018 iter/s, 2.47514s/10 iters), loss = 7.00166
I0523 01:47:46.886209 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00166 (* 1 = 7.00166 loss)
I0523 01:47:46.899819 35003 sgd_solver.cpp:112] Iteration 114600, lr = 0.01
I0523 01:47:49.587965 35003 solver.cpp:239] Iteration 114610 (3.70145 iter/s, 2.70164s/10 iters), loss = 7.09331
I0523 01:47:49.588006 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09331 (* 1 = 7.09331 loss)
I0523 01:47:49.601824 35003 sgd_solver.cpp:112] Iteration 114610, lr = 0.01
I0523 01:47:52.782142 35003 solver.cpp:239] Iteration 114620 (3.13087 iter/s, 3.194s/10 iters), loss = 6.60969
I0523 01:47:52.782181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60969 (* 1 = 6.60969 loss)
I0523 01:47:53.497907 35003 sgd_solver.cpp:112] Iteration 114620, lr = 0.01
I0523 01:47:57.066072 35003 solver.cpp:239] Iteration 114630 (2.33442 iter/s, 4.28371s/10 iters), loss = 7.22275
I0523 01:47:57.066123 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22275 (* 1 = 7.22275 loss)
I0523 01:47:57.768132 35003 sgd_solver.cpp:112] Iteration 114630, lr = 0.01
I0523 01:48:01.404698 35003 solver.cpp:239] Iteration 114640 (2.30501 iter/s, 4.33838s/10 iters), loss = 8.68961
I0523 01:48:01.404809 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.68961 (* 1 = 8.68961 loss)
I0523 01:48:02.139284 35003 sgd_solver.cpp:112] Iteration 114640, lr = 0.01
I0523 01:48:04.908315 35003 solver.cpp:239] Iteration 114650 (2.8544 iter/s, 3.50336s/10 iters), loss = 7.19249
I0523 01:48:04.908365 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19249 (* 1 = 7.19249 loss)
I0523 01:48:05.646461 35003 sgd_solver.cpp:112] Iteration 114650, lr = 0.01
I0523 01:48:08.384205 35003 solver.cpp:239] Iteration 114660 (2.87712 iter/s, 3.47569s/10 iters), loss = 6.84011
I0523 01:48:08.384248 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84011 (* 1 = 6.84011 loss)
I0523 01:48:08.397672 35003 sgd_solver.cpp:112] Iteration 114660, lr = 0.01
I0523 01:48:11.252043 35003 solver.cpp:239] Iteration 114670 (3.48715 iter/s, 2.86767s/10 iters), loss = 6.71751
I0523 01:48:11.252081 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71751 (* 1 = 6.71751 loss)
I0523 01:48:11.272646 35003 sgd_solver.cpp:112] Iteration 114670, lr = 0.01
I0523 01:48:15.436611 35003 solver.cpp:239] Iteration 114680 (2.38985 iter/s, 4.18436s/10 iters), loss = 8.10072
I0523 01:48:15.436655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.10072 (* 1 = 8.10072 loss)
I0523 01:48:15.449002 35003 sgd_solver.cpp:112] Iteration 114680, lr = 0.01
I0523 01:48:17.100512 35003 solver.cpp:239] Iteration 114690 (6.01044 iter/s, 1.66377s/10 iters), loss = 7.69666
I0523 01:48:17.100554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69666 (* 1 = 7.69666 loss)
I0523 01:48:17.113544 35003 sgd_solver.cpp:112] Iteration 114690, lr = 0.01
I0523 01:48:20.539046 35003 solver.cpp:239] Iteration 114700 (2.90838 iter/s, 3.43834s/10 iters), loss = 6.98121
I0523 01:48:20.539099 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98121 (* 1 = 6.98121 loss)
I0523 01:48:20.542310 35003 sgd_solver.cpp:112] Iteration 114700, lr = 0.01
I0523 01:48:25.792628 35003 solver.cpp:239] Iteration 114710 (1.90356 iter/s, 5.25332s/10 iters), loss = 7.16736
I0523 01:48:25.792673 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16736 (* 1 = 7.16736 loss)
I0523 01:48:26.495532 35003 sgd_solver.cpp:112] Iteration 114710, lr = 0.01
I0523 01:48:29.334635 35003 solver.cpp:239] Iteration 114720 (2.82366 iter/s, 3.5415s/10 iters), loss = 6.87637
I0523 01:48:29.334676 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87637 (* 1 = 6.87637 loss)
I0523 01:48:29.337939 35003 sgd_solver.cpp:112] Iteration 114720, lr = 0.01
I0523 01:48:32.317937 35003 solver.cpp:239] Iteration 114730 (3.35218 iter/s, 2.98313s/10 iters), loss = 6.58209
I0523 01:48:32.318126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58209 (* 1 = 6.58209 loss)
I0523 01:48:32.334475 35003 sgd_solver.cpp:112] Iteration 114730, lr = 0.01
I0523 01:48:35.186353 35003 solver.cpp:239] Iteration 114740 (3.48662 iter/s, 2.86811s/10 iters), loss = 7.64741
I0523 01:48:35.186405 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64741 (* 1 = 7.64741 loss)
I0523 01:48:35.199342 35003 sgd_solver.cpp:112] Iteration 114740, lr = 0.01
I0523 01:48:38.803174 35003 solver.cpp:239] Iteration 114750 (2.76501 iter/s, 3.61662s/10 iters), loss = 6.76262
I0523 01:48:38.803220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76262 (* 1 = 6.76262 loss)
I0523 01:48:39.531814 35003 sgd_solver.cpp:112] Iteration 114750, lr = 0.01
I0523 01:48:43.928460 35003 solver.cpp:239] Iteration 114760 (1.95121 iter/s, 5.12503s/10 iters), loss = 7.27501
I0523 01:48:43.928520 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27501 (* 1 = 7.27501 loss)
I0523 01:48:44.662978 35003 sgd_solver.cpp:112] Iteration 114760, lr = 0.01
I0523 01:48:48.530071 35003 solver.cpp:239] Iteration 114770 (2.17327 iter/s, 4.60136s/10 iters), loss = 6.89205
I0523 01:48:48.530124 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89205 (* 1 = 6.89205 loss)
I0523 01:48:49.271133 35003 sgd_solver.cpp:112] Iteration 114770, lr = 0.01
I0523 01:48:52.851758 35003 solver.cpp:239] Iteration 114780 (2.31404 iter/s, 4.32144s/10 iters), loss = 6.29553
I0523 01:48:52.851822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29553 (* 1 = 6.29553 loss)
I0523 01:48:52.860878 35003 sgd_solver.cpp:112] Iteration 114780, lr = 0.01
I0523 01:48:56.363235 35003 solver.cpp:239] Iteration 114790 (2.84797 iter/s, 3.51127s/10 iters), loss = 7.10336
I0523 01:48:56.363274 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10336 (* 1 = 7.10336 loss)
I0523 01:48:56.376251 35003 sgd_solver.cpp:112] Iteration 114790, lr = 0.01
I0523 01:48:59.182531 35003 solver.cpp:239] Iteration 114800 (3.54719 iter/s, 2.81913s/10 iters), loss = 6.37685
I0523 01:48:59.182581 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37685 (* 1 = 6.37685 loss)
I0523 01:48:59.896056 35003 sgd_solver.cpp:112] Iteration 114800, lr = 0.01
I0523 01:49:03.508525 35003 solver.cpp:239] Iteration 114810 (2.31173 iter/s, 4.32576s/10 iters), loss = 7.38049
I0523 01:49:03.508639 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38049 (* 1 = 7.38049 loss)
I0523 01:49:04.123600 35003 sgd_solver.cpp:112] Iteration 114810, lr = 0.01
I0523 01:49:06.479714 35003 solver.cpp:239] Iteration 114820 (3.36597 iter/s, 2.97091s/10 iters), loss = 7.15968
I0523 01:49:06.479775 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15968 (* 1 = 7.15968 loss)
I0523 01:49:07.220664 35003 sgd_solver.cpp:112] Iteration 114820, lr = 0.01
I0523 01:49:12.434734 35003 solver.cpp:239] Iteration 114830 (1.67934 iter/s, 5.95472s/10 iters), loss = 6.58283
I0523 01:49:12.434778 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58283 (* 1 = 6.58283 loss)
I0523 01:49:13.137202 35003 sgd_solver.cpp:112] Iteration 114830, lr = 0.01
I0523 01:49:17.240777 35003 solver.cpp:239] Iteration 114840 (2.08082 iter/s, 4.80581s/10 iters), loss = 6.81682
I0523 01:49:17.240826 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81682 (* 1 = 6.81682 loss)
I0523 01:49:17.949666 35003 sgd_solver.cpp:112] Iteration 114840, lr = 0.01
I0523 01:49:21.793279 35003 solver.cpp:239] Iteration 114850 (2.19671 iter/s, 4.55227s/10 iters), loss = 6.7188
I0523 01:49:21.793318 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7188 (* 1 = 6.7188 loss)
I0523 01:49:21.871384 35003 sgd_solver.cpp:112] Iteration 114850, lr = 0.01
I0523 01:49:25.096387 35003 solver.cpp:239] Iteration 114860 (3.02762 iter/s, 3.30292s/10 iters), loss = 6.73827
I0523 01:49:25.096444 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73827 (* 1 = 6.73827 loss)
I0523 01:49:25.100289 35003 sgd_solver.cpp:112] Iteration 114860, lr = 0.01
I0523 01:49:28.636477 35003 solver.cpp:239] Iteration 114870 (2.82495 iter/s, 3.53989s/10 iters), loss = 7.47633
I0523 01:49:28.636518 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47633 (* 1 = 7.47633 loss)
I0523 01:49:28.655038 35003 sgd_solver.cpp:112] Iteration 114870, lr = 0.01
I0523 01:49:31.506911 35003 solver.cpp:239] Iteration 114880 (3.48399 iter/s, 2.87027s/10 iters), loss = 6.73517
I0523 01:49:31.506952 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73517 (* 1 = 6.73517 loss)
I0523 01:49:31.513485 35003 sgd_solver.cpp:112] Iteration 114880, lr = 0.01
I0523 01:49:35.646561 35003 solver.cpp:239] Iteration 114890 (2.41579 iter/s, 4.13943s/10 iters), loss = 6.58408
I0523 01:49:35.646797 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58408 (* 1 = 6.58408 loss)
I0523 01:49:35.659744 35003 sgd_solver.cpp:112] Iteration 114890, lr = 0.01
I0523 01:49:38.499745 35003 solver.cpp:239] Iteration 114900 (3.50529 iter/s, 2.85283s/10 iters), loss = 8.85974
I0523 01:49:38.499792 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.85974 (* 1 = 8.85974 loss)
I0523 01:49:38.514032 35003 sgd_solver.cpp:112] Iteration 114900, lr = 0.01
I0523 01:49:41.322659 35003 solver.cpp:239] Iteration 114910 (3.54265 iter/s, 2.82274s/10 iters), loss = 6.23373
I0523 01:49:41.322718 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23373 (* 1 = 6.23373 loss)
I0523 01:49:41.351045 35003 sgd_solver.cpp:112] Iteration 114910, lr = 0.01
I0523 01:49:45.743571 35003 solver.cpp:239] Iteration 114920 (2.2621 iter/s, 4.42067s/10 iters), loss = 6.14587
I0523 01:49:45.743609 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14587 (* 1 = 6.14587 loss)
I0523 01:49:45.786617 35003 sgd_solver.cpp:112] Iteration 114920, lr = 0.01
I0523 01:49:47.053393 35003 solver.cpp:239] Iteration 114930 (7.63521 iter/s, 1.30972s/10 iters), loss = 8.38905
I0523 01:49:47.053436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.38905 (* 1 = 8.38905 loss)
I0523 01:49:47.088186 35003 sgd_solver.cpp:112] Iteration 114930, lr = 0.01
I0523 01:49:51.324676 35003 solver.cpp:239] Iteration 114940 (2.34137 iter/s, 4.271s/10 iters), loss = 6.99624
I0523 01:49:51.324730 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99624 (* 1 = 6.99624 loss)
I0523 01:49:51.329138 35003 sgd_solver.cpp:112] Iteration 114940, lr = 0.01
I0523 01:49:53.276722 35003 solver.cpp:239] Iteration 114950 (5.1232 iter/s, 1.9519s/10 iters), loss = 7.06581
I0523 01:49:53.276773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06581 (* 1 = 7.06581 loss)
I0523 01:49:53.283860 35003 sgd_solver.cpp:112] Iteration 114950, lr = 0.01
I0523 01:49:56.887732 35003 solver.cpp:239] Iteration 114960 (2.76948 iter/s, 3.61078s/10 iters), loss = 7.00942
I0523 01:49:56.887779 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00942 (* 1 = 7.00942 loss)
I0523 01:49:56.950209 35003 sgd_solver.cpp:112] Iteration 114960, lr = 0.01
I0523 01:50:01.617709 35003 solver.cpp:239] Iteration 114970 (2.11428 iter/s, 4.72974s/10 iters), loss = 6.47384
I0523 01:50:01.617748 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47384 (* 1 = 6.47384 loss)
I0523 01:50:01.624388 35003 sgd_solver.cpp:112] Iteration 114970, lr = 0.01
I0523 01:50:05.990059 35003 solver.cpp:239] Iteration 114980 (2.28722 iter/s, 4.37212s/10 iters), loss = 5.92979
I0523 01:50:05.990278 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92979 (* 1 = 5.92979 loss)
I0523 01:50:06.671836 35003 sgd_solver.cpp:112] Iteration 114980, lr = 0.01
I0523 01:50:10.999068 35003 solver.cpp:239] Iteration 114990 (1.99657 iter/s, 5.00859s/10 iters), loss = 8.43969
I0523 01:50:10.999114 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.43969 (* 1 = 8.43969 loss)
I0523 01:50:11.004309 35003 sgd_solver.cpp:112] Iteration 114990, lr = 0.01
I0523 01:50:13.564195 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_115000.caffemodel
I0523 01:50:13.786615 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_115000.solverstate
I0523 01:50:13.988014 35003 solver.cpp:239] Iteration 115000 (3.34586 iter/s, 2.98877s/10 iters), loss = 8.59372
I0523 01:50:13.988072 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.59372 (* 1 = 8.59372 loss)
I0523 01:50:14.722451 35003 sgd_solver.cpp:112] Iteration 115000, lr = 0.01
I0523 01:50:18.763785 35003 solver.cpp:239] Iteration 115010 (2.09401 iter/s, 4.77552s/10 iters), loss = 7.8305
I0523 01:50:18.763821 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8305 (* 1 = 7.8305 loss)
I0523 01:50:18.771313 35003 sgd_solver.cpp:112] Iteration 115010, lr = 0.01
I0523 01:50:20.870481 35003 solver.cpp:239] Iteration 115020 (4.74707 iter/s, 2.10656s/10 iters), loss = 6.90716
I0523 01:50:20.870520 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90716 (* 1 = 6.90716 loss)
I0523 01:50:20.882450 35003 sgd_solver.cpp:112] Iteration 115020, lr = 0.01
I0523 01:50:25.301906 35003 solver.cpp:239] Iteration 115030 (2.25673 iter/s, 4.4312s/10 iters), loss = 8.01836
I0523 01:50:25.301954 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01836 (* 1 = 8.01836 loss)
I0523 01:50:25.344666 35003 sgd_solver.cpp:112] Iteration 115030, lr = 0.01
I0523 01:50:29.665365 35003 solver.cpp:239] Iteration 115040 (2.29188 iter/s, 4.36323s/10 iters), loss = 7.46046
I0523 01:50:29.665421 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46046 (* 1 = 7.46046 loss)
I0523 01:50:29.670352 35003 sgd_solver.cpp:112] Iteration 115040, lr = 0.01
I0523 01:50:33.257107 35003 solver.cpp:239] Iteration 115050 (2.78432 iter/s, 3.59154s/10 iters), loss = 7.29245
I0523 01:50:33.257155 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29245 (* 1 = 7.29245 loss)
I0523 01:50:33.265167 35003 sgd_solver.cpp:112] Iteration 115050, lr = 0.01
I0523 01:50:37.068045 35003 solver.cpp:239] Iteration 115060 (2.62417 iter/s, 3.81073s/10 iters), loss = 8.41392
I0523 01:50:37.068298 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.41392 (* 1 = 8.41392 loss)
I0523 01:50:37.809041 35003 sgd_solver.cpp:112] Iteration 115060, lr = 0.01
I0523 01:50:42.329850 35003 solver.cpp:239] Iteration 115070 (1.90065 iter/s, 5.26137s/10 iters), loss = 6.87969
I0523 01:50:42.329903 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87969 (* 1 = 6.87969 loss)
I0523 01:50:42.336397 35003 sgd_solver.cpp:112] Iteration 115070, lr = 0.01
I0523 01:50:45.246943 35003 solver.cpp:239] Iteration 115080 (3.42828 iter/s, 2.91692s/10 iters), loss = 6.22471
I0523 01:50:45.247000 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22471 (* 1 = 6.22471 loss)
I0523 01:50:45.987860 35003 sgd_solver.cpp:112] Iteration 115080, lr = 0.01
I0523 01:50:50.204541 35003 solver.cpp:239] Iteration 115090 (2.01721 iter/s, 4.95733s/10 iters), loss = 7.52993
I0523 01:50:50.204604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52993 (* 1 = 7.52993 loss)
I0523 01:50:50.216272 35003 sgd_solver.cpp:112] Iteration 115090, lr = 0.01
I0523 01:50:55.363788 35003 solver.cpp:239] Iteration 115100 (1.93837 iter/s, 5.15898s/10 iters), loss = 7.83152
I0523 01:50:55.363834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83152 (* 1 = 7.83152 loss)
I0523 01:50:56.078565 35003 sgd_solver.cpp:112] Iteration 115100, lr = 0.01
I0523 01:50:58.181102 35003 solver.cpp:239] Iteration 115110 (3.54969 iter/s, 2.81714s/10 iters), loss = 6.13842
I0523 01:50:58.181155 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13842 (* 1 = 6.13842 loss)
I0523 01:50:58.916090 35003 sgd_solver.cpp:112] Iteration 115110, lr = 0.01
I0523 01:51:02.293198 35003 solver.cpp:239] Iteration 115120 (2.43198 iter/s, 4.11188s/10 iters), loss = 7.18374
I0523 01:51:02.293237 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18374 (* 1 = 7.18374 loss)
I0523 01:51:02.306015 35003 sgd_solver.cpp:112] Iteration 115120, lr = 0.01
I0523 01:51:05.838228 35003 solver.cpp:239] Iteration 115130 (2.821 iter/s, 3.54484s/10 iters), loss = 6.9342
I0523 01:51:05.838273 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9342 (* 1 = 6.9342 loss)
I0523 01:51:05.841044 35003 sgd_solver.cpp:112] Iteration 115130, lr = 0.01
I0523 01:51:08.694993 35003 solver.cpp:239] Iteration 115140 (3.50067 iter/s, 2.85659s/10 iters), loss = 7.31424
I0523 01:51:08.695156 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31424 (* 1 = 7.31424 loss)
I0523 01:51:08.701596 35003 sgd_solver.cpp:112] Iteration 115140, lr = 0.01
I0523 01:51:11.799810 35003 solver.cpp:239] Iteration 115150 (3.2211 iter/s, 3.10453s/10 iters), loss = 7.52392
I0523 01:51:11.799866 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52392 (* 1 = 7.52392 loss)
I0523 01:51:11.809371 35003 sgd_solver.cpp:112] Iteration 115150, lr = 0.01
I0523 01:51:14.569447 35003 solver.cpp:239] Iteration 115160 (3.61081 iter/s, 2.76946s/10 iters), loss = 6.56309
I0523 01:51:14.569492 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56309 (* 1 = 6.56309 loss)
I0523 01:51:14.577659 35003 sgd_solver.cpp:112] Iteration 115160, lr = 0.01
I0523 01:51:17.913096 35003 solver.cpp:239] Iteration 115170 (2.99091 iter/s, 3.34346s/10 iters), loss = 6.37058
I0523 01:51:17.913141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37058 (* 1 = 6.37058 loss)
I0523 01:51:17.923068 35003 sgd_solver.cpp:112] Iteration 115170, lr = 0.01
I0523 01:51:21.925328 35003 solver.cpp:239] Iteration 115180 (2.49251 iter/s, 4.01201s/10 iters), loss = 8.28168
I0523 01:51:21.925388 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.28168 (* 1 = 8.28168 loss)
I0523 01:51:21.930431 35003 sgd_solver.cpp:112] Iteration 115180, lr = 0.01
I0523 01:51:25.508232 35003 solver.cpp:239] Iteration 115190 (2.7912 iter/s, 3.58269s/10 iters), loss = 7.22433
I0523 01:51:25.508285 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22433 (* 1 = 7.22433 loss)
I0523 01:51:25.513993 35003 sgd_solver.cpp:112] Iteration 115190, lr = 0.01
I0523 01:51:30.165431 35003 solver.cpp:239] Iteration 115200 (2.14732 iter/s, 4.65696s/10 iters), loss = 6.72323
I0523 01:51:30.165482 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72323 (* 1 = 6.72323 loss)
I0523 01:51:30.178146 35003 sgd_solver.cpp:112] Iteration 115200, lr = 0.01
I0523 01:51:31.736443 35003 solver.cpp:239] Iteration 115210 (6.3658 iter/s, 1.57089s/10 iters), loss = 5.86688
I0523 01:51:31.736485 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.86688 (* 1 = 5.86688 loss)
I0523 01:51:32.471237 35003 sgd_solver.cpp:112] Iteration 115210, lr = 0.01
I0523 01:51:34.643448 35003 solver.cpp:239] Iteration 115220 (3.44016 iter/s, 2.90684s/10 iters), loss = 8.23466
I0523 01:51:34.643486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.23466 (* 1 = 8.23466 loss)
I0523 01:51:34.651262 35003 sgd_solver.cpp:112] Iteration 115220, lr = 0.01
I0523 01:51:37.522189 35003 solver.cpp:239] Iteration 115230 (3.47394 iter/s, 2.87857s/10 iters), loss = 8.41372
I0523 01:51:37.522233 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.41372 (* 1 = 8.41372 loss)
I0523 01:51:38.025064 35003 sgd_solver.cpp:112] Iteration 115230, lr = 0.01
I0523 01:51:42.517066 35003 solver.cpp:239] Iteration 115240 (2.00215 iter/s, 4.99463s/10 iters), loss = 7.11007
I0523 01:51:42.517328 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11007 (* 1 = 7.11007 loss)
I0523 01:51:42.530180 35003 sgd_solver.cpp:112] Iteration 115240, lr = 0.01
I0523 01:51:44.838512 35003 solver.cpp:239] Iteration 115250 (4.30829 iter/s, 2.32111s/10 iters), loss = 6.76969
I0523 01:51:44.838554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76969 (* 1 = 6.76969 loss)
I0523 01:51:44.856389 35003 sgd_solver.cpp:112] Iteration 115250, lr = 0.01
I0523 01:51:48.440330 35003 solver.cpp:239] Iteration 115260 (2.77652 iter/s, 3.60162s/10 iters), loss = 6.87396
I0523 01:51:48.440376 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87396 (* 1 = 6.87396 loss)
I0523 01:51:48.448263 35003 sgd_solver.cpp:112] Iteration 115260, lr = 0.01
I0523 01:51:51.285233 35003 solver.cpp:239] Iteration 115270 (3.51526 iter/s, 2.84474s/10 iters), loss = 6.81207
I0523 01:51:51.285270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81207 (* 1 = 6.81207 loss)
I0523 01:51:51.297215 35003 sgd_solver.cpp:112] Iteration 115270, lr = 0.01
I0523 01:51:53.395488 35003 solver.cpp:239] Iteration 115280 (4.73907 iter/s, 2.11012s/10 iters), loss = 6.05758
I0523 01:51:53.395540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05758 (* 1 = 6.05758 loss)
I0523 01:51:54.110488 35003 sgd_solver.cpp:112] Iteration 115280, lr = 0.01
I0523 01:51:56.896378 35003 solver.cpp:239] Iteration 115290 (2.85658 iter/s, 3.50069s/10 iters), loss = 7.03458
I0523 01:51:56.896427 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03458 (* 1 = 7.03458 loss)
I0523 01:51:56.909338 35003 sgd_solver.cpp:112] Iteration 115290, lr = 0.01
I0523 01:51:59.191361 35003 solver.cpp:239] Iteration 115300 (4.35761 iter/s, 2.29484s/10 iters), loss = 7.54032
I0523 01:51:59.191411 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54032 (* 1 = 7.54032 loss)
I0523 01:51:59.206104 35003 sgd_solver.cpp:112] Iteration 115300, lr = 0.01
I0523 01:52:02.729609 35003 solver.cpp:239] Iteration 115310 (2.82641 iter/s, 3.53805s/10 iters), loss = 7.44772
I0523 01:52:02.729657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44772 (* 1 = 7.44772 loss)
I0523 01:52:02.743093 35003 sgd_solver.cpp:112] Iteration 115310, lr = 0.01
I0523 01:52:06.297024 35003 solver.cpp:239] Iteration 115320 (2.8033 iter/s, 3.56722s/10 iters), loss = 7.48237
I0523 01:52:06.297070 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48237 (* 1 = 7.48237 loss)
I0523 01:52:06.315435 35003 sgd_solver.cpp:112] Iteration 115320, lr = 0.01
I0523 01:52:09.131335 35003 solver.cpp:239] Iteration 115330 (3.5284 iter/s, 2.83415s/10 iters), loss = 6.88394
I0523 01:52:09.131376 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88394 (* 1 = 6.88394 loss)
I0523 01:52:09.150030 35003 sgd_solver.cpp:112] Iteration 115330, lr = 0.01
I0523 01:52:11.963805 35003 solver.cpp:239] Iteration 115340 (3.53069 iter/s, 2.83231s/10 iters), loss = 7.46887
I0523 01:52:11.963857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46887 (* 1 = 7.46887 loss)
I0523 01:52:11.976791 35003 sgd_solver.cpp:112] Iteration 115340, lr = 0.01
I0523 01:52:14.870237 35003 solver.cpp:239] Iteration 115350 (3.44086 iter/s, 2.90625s/10 iters), loss = 6.17107
I0523 01:52:14.870530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17107 (* 1 = 6.17107 loss)
I0523 01:52:14.878938 35003 sgd_solver.cpp:112] Iteration 115350, lr = 0.01
I0523 01:52:18.382827 35003 solver.cpp:239] Iteration 115360 (2.84723 iter/s, 3.51218s/10 iters), loss = 7.62092
I0523 01:52:18.382874 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62092 (* 1 = 7.62092 loss)
I0523 01:52:18.962180 35003 sgd_solver.cpp:112] Iteration 115360, lr = 0.01
I0523 01:52:22.836849 35003 solver.cpp:239] Iteration 115370 (2.24528 iter/s, 4.45379s/10 iters), loss = 7.53603
I0523 01:52:22.836901 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53603 (* 1 = 7.53603 loss)
I0523 01:52:23.572096 35003 sgd_solver.cpp:112] Iteration 115370, lr = 0.01
I0523 01:52:27.886811 35003 solver.cpp:239] Iteration 115380 (1.98035 iter/s, 5.0496s/10 iters), loss = 7.33
I0523 01:52:27.886876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33 (* 1 = 7.33 loss)
I0523 01:52:27.929945 35003 sgd_solver.cpp:112] Iteration 115380, lr = 0.01
I0523 01:52:29.248019 35003 solver.cpp:239] Iteration 115390 (7.34709 iter/s, 1.36108s/10 iters), loss = 7.88851
I0523 01:52:29.248070 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88851 (* 1 = 7.88851 loss)
I0523 01:52:29.259600 35003 sgd_solver.cpp:112] Iteration 115390, lr = 0.01
I0523 01:52:32.876312 35003 solver.cpp:239] Iteration 115400 (2.75627 iter/s, 3.62809s/10 iters), loss = 7.36989
I0523 01:52:32.876371 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36989 (* 1 = 7.36989 loss)
I0523 01:52:32.888406 35003 sgd_solver.cpp:112] Iteration 115400, lr = 0.01
I0523 01:52:35.701515 35003 solver.cpp:239] Iteration 115410 (3.53979 iter/s, 2.82502s/10 iters), loss = 6.48663
I0523 01:52:35.701565 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48663 (* 1 = 6.48663 loss)
I0523 01:52:35.720717 35003 sgd_solver.cpp:112] Iteration 115410, lr = 0.01
I0523 01:52:39.105711 35003 solver.cpp:239] Iteration 115420 (2.93772 iter/s, 3.40401s/10 iters), loss = 6.83178
I0523 01:52:39.105754 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83178 (* 1 = 6.83178 loss)
I0523 01:52:39.108809 35003 sgd_solver.cpp:112] Iteration 115420, lr = 0.01
I0523 01:52:41.931869 35003 solver.cpp:239] Iteration 115430 (3.53861 iter/s, 2.82597s/10 iters), loss = 7.05295
I0523 01:52:41.931933 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05295 (* 1 = 7.05295 loss)
I0523 01:52:42.672830 35003 sgd_solver.cpp:112] Iteration 115430, lr = 0.01
I0523 01:52:44.597700 35003 solver.cpp:239] Iteration 115440 (3.75143 iter/s, 2.66565s/10 iters), loss = 6.29529
I0523 01:52:44.597743 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29529 (* 1 = 6.29529 loss)
I0523 01:52:45.152752 35003 sgd_solver.cpp:112] Iteration 115440, lr = 0.01
I0523 01:52:48.266911 35003 solver.cpp:239] Iteration 115450 (2.72553 iter/s, 3.66901s/10 iters), loss = 7.0379
I0523 01:52:48.266955 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0379 (* 1 = 7.0379 loss)
I0523 01:52:48.990991 35003 sgd_solver.cpp:112] Iteration 115450, lr = 0.01
I0523 01:52:53.309739 35003 solver.cpp:239] Iteration 115460 (1.98311 iter/s, 5.04258s/10 iters), loss = 7.86276
I0523 01:52:53.309784 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86276 (* 1 = 7.86276 loss)
I0523 01:52:54.025166 35003 sgd_solver.cpp:112] Iteration 115460, lr = 0.01
I0523 01:52:57.601536 35003 solver.cpp:239] Iteration 115470 (2.33015 iter/s, 4.29158s/10 iters), loss = 7.88794
I0523 01:52:57.601578 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88794 (* 1 = 7.88794 loss)
I0523 01:52:57.611340 35003 sgd_solver.cpp:112] Iteration 115470, lr = 0.01
I0523 01:53:00.422933 35003 solver.cpp:239] Iteration 115480 (3.54455 iter/s, 2.82123s/10 iters), loss = 7.1726
I0523 01:53:00.422991 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1726 (* 1 = 7.1726 loss)
I0523 01:53:00.634176 35003 sgd_solver.cpp:112] Iteration 115480, lr = 0.01
I0523 01:53:04.656424 35003 solver.cpp:239] Iteration 115490 (2.36225 iter/s, 4.23326s/10 iters), loss = 5.8025
I0523 01:53:04.656479 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8025 (* 1 = 5.8025 loss)
I0523 01:53:04.700103 35003 sgd_solver.cpp:112] Iteration 115490, lr = 0.01
I0523 01:53:08.102543 35003 solver.cpp:239] Iteration 115500 (2.90198 iter/s, 3.44592s/10 iters), loss = 6.91959
I0523 01:53:08.102582 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91959 (* 1 = 6.91959 loss)
I0523 01:53:08.115542 35003 sgd_solver.cpp:112] Iteration 115500, lr = 0.01
I0523 01:53:11.093235 35003 solver.cpp:239] Iteration 115510 (3.34389 iter/s, 2.99053s/10 iters), loss = 7.46711
I0523 01:53:11.093288 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46711 (* 1 = 7.46711 loss)
I0523 01:53:11.118974 35003 sgd_solver.cpp:112] Iteration 115510, lr = 0.01
I0523 01:53:15.670737 35003 solver.cpp:239] Iteration 115520 (2.18473 iter/s, 4.57723s/10 iters), loss = 6.60425
I0523 01:53:15.670959 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60425 (* 1 = 6.60425 loss)
I0523 01:53:15.684366 35003 sgd_solver.cpp:112] Iteration 115520, lr = 0.01
I0523 01:53:19.255441 35003 solver.cpp:239] Iteration 115530 (2.78992 iter/s, 3.58433s/10 iters), loss = 7.68927
I0523 01:53:19.255498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68927 (* 1 = 7.68927 loss)
I0523 01:53:19.267127 35003 sgd_solver.cpp:112] Iteration 115530, lr = 0.01
I0523 01:53:22.064481 35003 solver.cpp:239] Iteration 115540 (3.56017 iter/s, 2.80886s/10 iters), loss = 7.12633
I0523 01:53:22.064540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12633 (* 1 = 7.12633 loss)
I0523 01:53:22.779196 35003 sgd_solver.cpp:112] Iteration 115540, lr = 0.01
I0523 01:53:26.428197 35003 solver.cpp:239] Iteration 115550 (2.29175 iter/s, 4.36348s/10 iters), loss = 6.33184
I0523 01:53:26.428247 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33184 (* 1 = 6.33184 loss)
I0523 01:53:27.047452 35003 sgd_solver.cpp:112] Iteration 115550, lr = 0.01
I0523 01:53:29.748773 35003 solver.cpp:239] Iteration 115560 (3.01171 iter/s, 3.32037s/10 iters), loss = 7.67803
I0523 01:53:29.748816 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67803 (* 1 = 7.67803 loss)
I0523 01:53:29.755165 35003 sgd_solver.cpp:112] Iteration 115560, lr = 0.01
I0523 01:53:32.800912 35003 solver.cpp:239] Iteration 115570 (3.27657 iter/s, 3.05197s/10 iters), loss = 7.41518
I0523 01:53:32.800957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41518 (* 1 = 7.41518 loss)
I0523 01:53:32.814509 35003 sgd_solver.cpp:112] Iteration 115570, lr = 0.01
I0523 01:53:33.855621 35003 solver.cpp:239] Iteration 115580 (9.48216 iter/s, 1.05461s/10 iters), loss = 6.80222
I0523 01:53:33.855659 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80222 (* 1 = 6.80222 loss)
I0523 01:53:33.865645 35003 sgd_solver.cpp:112] Iteration 115580, lr = 0.01
I0523 01:53:35.066366 35003 solver.cpp:239] Iteration 115590 (8.26013 iter/s, 1.21064s/10 iters), loss = 7.58618
I0523 01:53:35.066428 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58618 (* 1 = 7.58618 loss)
I0523 01:53:35.774730 35003 sgd_solver.cpp:112] Iteration 115590, lr = 0.01
I0523 01:53:39.095728 35003 solver.cpp:239] Iteration 115600 (2.48192 iter/s, 4.02914s/10 iters), loss = 7.49373
I0523 01:53:39.095782 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49373 (* 1 = 7.49373 loss)
I0523 01:53:39.113737 35003 sgd_solver.cpp:112] Iteration 115600, lr = 0.01
I0523 01:53:42.001804 35003 solver.cpp:239] Iteration 115610 (3.44127 iter/s, 2.9059s/10 iters), loss = 6.2825
I0523 01:53:42.001862 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2825 (* 1 = 6.2825 loss)
I0523 01:53:42.743139 35003 sgd_solver.cpp:112] Iteration 115610, lr = 0.01
I0523 01:53:47.034647 35003 solver.cpp:239] Iteration 115620 (1.98705 iter/s, 5.03258s/10 iters), loss = 7.07678
I0523 01:53:47.034984 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07678 (* 1 = 7.07678 loss)
I0523 01:53:47.041627 35003 sgd_solver.cpp:112] Iteration 115620, lr = 0.01
I0523 01:53:50.556459 35003 solver.cpp:239] Iteration 115630 (2.83982 iter/s, 3.52135s/10 iters), loss = 8.53901
I0523 01:53:50.556512 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.53901 (* 1 = 8.53901 loss)
I0523 01:53:50.568037 35003 sgd_solver.cpp:112] Iteration 115630, lr = 0.01
I0523 01:53:54.792656 35003 solver.cpp:239] Iteration 115640 (2.36074 iter/s, 4.23596s/10 iters), loss = 6.61497
I0523 01:53:54.792706 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61497 (* 1 = 6.61497 loss)
I0523 01:53:54.804075 35003 sgd_solver.cpp:112] Iteration 115640, lr = 0.01
I0523 01:53:56.871639 35003 solver.cpp:239] Iteration 115650 (4.81038 iter/s, 2.07884s/10 iters), loss = 6.56395
I0523 01:53:56.871685 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56395 (* 1 = 6.56395 loss)
I0523 01:53:56.888581 35003 sgd_solver.cpp:112] Iteration 115650, lr = 0.01
I0523 01:53:58.841091 35003 solver.cpp:239] Iteration 115660 (5.07793 iter/s, 1.96931s/10 iters), loss = 6.50115
I0523 01:53:58.841137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50115 (* 1 = 6.50115 loss)
I0523 01:53:58.846365 35003 sgd_solver.cpp:112] Iteration 115660, lr = 0.01
I0523 01:54:02.203657 35003 solver.cpp:239] Iteration 115670 (2.97409 iter/s, 3.36238s/10 iters), loss = 7.54743
I0523 01:54:02.203706 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54743 (* 1 = 7.54743 loss)
I0523 01:54:02.813395 35003 sgd_solver.cpp:112] Iteration 115670, lr = 0.01
I0523 01:54:05.681082 35003 solver.cpp:239] Iteration 115680 (2.87585 iter/s, 3.47723s/10 iters), loss = 7.60867
I0523 01:54:05.681141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60867 (* 1 = 7.60867 loss)
I0523 01:54:06.410847 35003 sgd_solver.cpp:112] Iteration 115680, lr = 0.01
I0523 01:54:09.820541 35003 solver.cpp:239] Iteration 115690 (2.4159 iter/s, 4.13924s/10 iters), loss = 6.90827
I0523 01:54:09.820582 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90827 (* 1 = 6.90827 loss)
I0523 01:54:09.828552 35003 sgd_solver.cpp:112] Iteration 115690, lr = 0.01
I0523 01:54:12.687041 35003 solver.cpp:239] Iteration 115700 (3.48877 iter/s, 2.86634s/10 iters), loss = 6.7243
I0523 01:54:12.687083 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7243 (* 1 = 6.7243 loss)
I0523 01:54:12.692677 35003 sgd_solver.cpp:112] Iteration 115700, lr = 0.01
I0523 01:54:16.837007 35003 solver.cpp:239] Iteration 115710 (2.40978 iter/s, 4.14975s/10 iters), loss = 7.89172
I0523 01:54:16.837064 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89172 (* 1 = 7.89172 loss)
I0523 01:54:16.841146 35003 sgd_solver.cpp:112] Iteration 115710, lr = 0.01
I0523 01:54:19.681398 35003 solver.cpp:239] Iteration 115720 (3.51591 iter/s, 2.84422s/10 iters), loss = 6.40367
I0523 01:54:19.681561 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40367 (* 1 = 6.40367 loss)
I0523 01:54:19.729768 35003 sgd_solver.cpp:112] Iteration 115720, lr = 0.01
I0523 01:54:23.328105 35003 solver.cpp:239] Iteration 115730 (2.74243 iter/s, 3.6464s/10 iters), loss = 6.88838
I0523 01:54:23.328173 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88838 (* 1 = 6.88838 loss)
I0523 01:54:23.340045 35003 sgd_solver.cpp:112] Iteration 115730, lr = 0.01
I0523 01:54:26.557013 35003 solver.cpp:239] Iteration 115740 (3.09722 iter/s, 3.2287s/10 iters), loss = 7.27105
I0523 01:54:26.557071 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27105 (* 1 = 7.27105 loss)
I0523 01:54:27.187718 35003 sgd_solver.cpp:112] Iteration 115740, lr = 0.01
I0523 01:54:29.363229 35003 solver.cpp:239] Iteration 115750 (3.56375 iter/s, 2.80603s/10 iters), loss = 9.12332
I0523 01:54:29.363286 35003 solver.cpp:258]     Train net output #0: softmax_loss = 9.12332 (* 1 = 9.12332 loss)
I0523 01:54:30.104182 35003 sgd_solver.cpp:112] Iteration 115750, lr = 0.01
I0523 01:54:31.435420 35003 solver.cpp:239] Iteration 115760 (4.82614 iter/s, 2.07205s/10 iters), loss = 7.43217
I0523 01:54:31.435468 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43217 (* 1 = 7.43217 loss)
I0523 01:54:31.441408 35003 sgd_solver.cpp:112] Iteration 115760, lr = 0.01
I0523 01:54:34.159062 35003 solver.cpp:239] Iteration 115770 (3.67179 iter/s, 2.72347s/10 iters), loss = 7.91787
I0523 01:54:34.159111 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91787 (* 1 = 7.91787 loss)
I0523 01:54:34.182232 35003 sgd_solver.cpp:112] Iteration 115770, lr = 0.01
I0523 01:54:37.340314 35003 solver.cpp:239] Iteration 115780 (3.1436 iter/s, 3.18106s/10 iters), loss = 7.08135
I0523 01:54:37.340358 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08135 (* 1 = 7.08135 loss)
I0523 01:54:37.343706 35003 sgd_solver.cpp:112] Iteration 115780, lr = 0.01
I0523 01:54:40.215767 35003 solver.cpp:239] Iteration 115790 (3.47792 iter/s, 2.87528s/10 iters), loss = 6.41252
I0523 01:54:40.215809 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41252 (* 1 = 6.41252 loss)
I0523 01:54:40.219898 35003 sgd_solver.cpp:112] Iteration 115790, lr = 0.01
I0523 01:54:43.635633 35003 solver.cpp:239] Iteration 115800 (2.92428 iter/s, 3.41965s/10 iters), loss = 7.28001
I0523 01:54:43.635677 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28001 (* 1 = 7.28001 loss)
I0523 01:54:43.643405 35003 sgd_solver.cpp:112] Iteration 115800, lr = 0.01
I0523 01:54:48.319706 35003 solver.cpp:239] Iteration 115810 (2.135 iter/s, 4.68384s/10 iters), loss = 7.3127
I0523 01:54:48.319746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3127 (* 1 = 7.3127 loss)
I0523 01:54:48.332761 35003 sgd_solver.cpp:112] Iteration 115810, lr = 0.01
I0523 01:54:51.205281 35003 solver.cpp:239] Iteration 115820 (3.46572 iter/s, 2.88541s/10 iters), loss = 7.45273
I0523 01:54:51.205619 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45273 (* 1 = 7.45273 loss)
I0523 01:54:51.919899 35003 sgd_solver.cpp:112] Iteration 115820, lr = 0.01
I0523 01:54:55.303625 35003 solver.cpp:239] Iteration 115830 (2.44029 iter/s, 4.09787s/10 iters), loss = 6.45278
I0523 01:54:55.303670 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45278 (* 1 = 6.45278 loss)
I0523 01:54:56.019011 35003 sgd_solver.cpp:112] Iteration 115830, lr = 0.01
I0523 01:54:58.167248 35003 solver.cpp:239] Iteration 115840 (3.49229 iter/s, 2.86345s/10 iters), loss = 6.84766
I0523 01:54:58.167305 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84766 (* 1 = 6.84766 loss)
I0523 01:54:58.902226 35003 sgd_solver.cpp:112] Iteration 115840, lr = 0.01
I0523 01:55:03.354651 35003 solver.cpp:239] Iteration 115850 (1.92784 iter/s, 5.18714s/10 iters), loss = 6.96282
I0523 01:55:03.354687 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96282 (* 1 = 6.96282 loss)
I0523 01:55:03.373154 35003 sgd_solver.cpp:112] Iteration 115850, lr = 0.01
I0523 01:55:05.386369 35003 solver.cpp:239] Iteration 115860 (4.92227 iter/s, 2.03158s/10 iters), loss = 7.20334
I0523 01:55:05.386425 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20334 (* 1 = 7.20334 loss)
I0523 01:55:05.394780 35003 sgd_solver.cpp:112] Iteration 115860, lr = 0.01
I0523 01:55:09.664059 35003 solver.cpp:239] Iteration 115870 (2.33783 iter/s, 4.27746s/10 iters), loss = 7.6913
I0523 01:55:09.664095 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6913 (* 1 = 7.6913 loss)
I0523 01:55:09.682592 35003 sgd_solver.cpp:112] Iteration 115870, lr = 0.01
I0523 01:55:14.756685 35003 solver.cpp:239] Iteration 115880 (1.96372 iter/s, 5.09238s/10 iters), loss = 7.13908
I0523 01:55:14.756731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13908 (* 1 = 7.13908 loss)
I0523 01:55:14.774410 35003 sgd_solver.cpp:112] Iteration 115880, lr = 0.01
I0523 01:55:17.851294 35003 solver.cpp:239] Iteration 115890 (3.23618 iter/s, 3.09006s/10 iters), loss = 7.05677
I0523 01:55:17.851339 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05677 (* 1 = 7.05677 loss)
I0523 01:55:17.864356 35003 sgd_solver.cpp:112] Iteration 115890, lr = 0.01
I0523 01:55:21.105631 35003 solver.cpp:239] Iteration 115900 (3.073 iter/s, 3.25415s/10 iters), loss = 6.71366
I0523 01:55:21.105674 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71366 (* 1 = 6.71366 loss)
I0523 01:55:21.118196 35003 sgd_solver.cpp:112] Iteration 115900, lr = 0.01
I0523 01:55:23.961452 35003 solver.cpp:239] Iteration 115910 (3.50183 iter/s, 2.85565s/10 iters), loss = 7.71708
I0523 01:55:23.961727 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71708 (* 1 = 7.71708 loss)
I0523 01:55:23.967306 35003 sgd_solver.cpp:112] Iteration 115910, lr = 0.01
I0523 01:55:27.526563 35003 solver.cpp:239] Iteration 115920 (2.80527 iter/s, 3.56472s/10 iters), loss = 7.14158
I0523 01:55:27.526612 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14158 (* 1 = 7.14158 loss)
I0523 01:55:28.241919 35003 sgd_solver.cpp:112] Iteration 115920, lr = 0.01
I0523 01:55:33.183332 35003 solver.cpp:239] Iteration 115930 (1.76788 iter/s, 5.65649s/10 iters), loss = 7.01893
I0523 01:55:33.183393 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01893 (* 1 = 7.01893 loss)
I0523 01:55:33.898044 35003 sgd_solver.cpp:112] Iteration 115930, lr = 0.01
I0523 01:55:38.113050 35003 solver.cpp:239] Iteration 115940 (2.02862 iter/s, 4.92946s/10 iters), loss = 6.29331
I0523 01:55:38.113086 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29331 (* 1 = 6.29331 loss)
I0523 01:55:38.140813 35003 sgd_solver.cpp:112] Iteration 115940, lr = 0.01
I0523 01:55:41.431015 35003 solver.cpp:239] Iteration 115950 (3.01406 iter/s, 3.31778s/10 iters), loss = 6.08801
I0523 01:55:41.431052 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08801 (* 1 = 6.08801 loss)
I0523 01:55:41.449159 35003 sgd_solver.cpp:112] Iteration 115950, lr = 0.01
I0523 01:55:43.502182 35003 solver.cpp:239] Iteration 115960 (4.82849 iter/s, 2.07104s/10 iters), loss = 7.24163
I0523 01:55:43.502229 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24163 (* 1 = 7.24163 loss)
I0523 01:55:43.515861 35003 sgd_solver.cpp:112] Iteration 115960, lr = 0.01
I0523 01:55:47.522776 35003 solver.cpp:239] Iteration 115970 (2.48732 iter/s, 4.02038s/10 iters), loss = 6.12583
I0523 01:55:47.522817 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12583 (* 1 = 6.12583 loss)
I0523 01:55:47.535730 35003 sgd_solver.cpp:112] Iteration 115970, lr = 0.01
I0523 01:55:49.593549 35003 solver.cpp:239] Iteration 115980 (4.82942 iter/s, 2.07064s/10 iters), loss = 6.96219
I0523 01:55:49.593588 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96219 (* 1 = 6.96219 loss)
I0523 01:55:49.606292 35003 sgd_solver.cpp:112] Iteration 115980, lr = 0.01
I0523 01:55:52.780586 35003 solver.cpp:239] Iteration 115990 (3.13789 iter/s, 3.18686s/10 iters), loss = 6.5504
I0523 01:55:52.780633 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5504 (* 1 = 6.5504 loss)
I0523 01:55:52.793822 35003 sgd_solver.cpp:112] Iteration 115990, lr = 0.01
I0523 01:55:56.707314 35003 solver.cpp:239] Iteration 116000 (2.54678 iter/s, 3.92652s/10 iters), loss = 8.00476
I0523 01:55:56.707504 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00476 (* 1 = 8.00476 loss)
I0523 01:55:56.720345 35003 sgd_solver.cpp:112] Iteration 116000, lr = 0.01
I0523 01:56:01.041703 35003 solver.cpp:239] Iteration 116010 (2.30732 iter/s, 4.33404s/10 iters), loss = 6.99151
I0523 01:56:01.041749 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99151 (* 1 = 6.99151 loss)
I0523 01:56:01.054731 35003 sgd_solver.cpp:112] Iteration 116010, lr = 0.01
I0523 01:56:05.355113 35003 solver.cpp:239] Iteration 116020 (2.31847 iter/s, 4.31318s/10 iters), loss = 5.79229
I0523 01:56:05.355222 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.79229 (* 1 = 5.79229 loss)
I0523 01:56:05.367538 35003 sgd_solver.cpp:112] Iteration 116020, lr = 0.01
I0523 01:56:09.478559 35003 solver.cpp:239] Iteration 116030 (2.42532 iter/s, 4.12318s/10 iters), loss = 6.9237
I0523 01:56:09.478595 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9237 (* 1 = 6.9237 loss)
I0523 01:56:09.492085 35003 sgd_solver.cpp:112] Iteration 116030, lr = 0.01
I0523 01:56:12.292928 35003 solver.cpp:239] Iteration 116040 (3.5534 iter/s, 2.8142s/10 iters), loss = 7.41317
I0523 01:56:12.292984 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41317 (* 1 = 7.41317 loss)
I0523 01:56:12.302120 35003 sgd_solver.cpp:112] Iteration 116040, lr = 0.01
I0523 01:56:16.373167 35003 solver.cpp:239] Iteration 116050 (2.45097 iter/s, 4.08001s/10 iters), loss = 7.65943
I0523 01:56:16.373208 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65943 (* 1 = 7.65943 loss)
I0523 01:56:16.381986 35003 sgd_solver.cpp:112] Iteration 116050, lr = 0.01
I0523 01:56:19.063836 35003 solver.cpp:239] Iteration 116060 (3.71676 iter/s, 2.69052s/10 iters), loss = 7.14134
I0523 01:56:19.063876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14134 (* 1 = 7.14134 loss)
I0523 01:56:19.068043 35003 sgd_solver.cpp:112] Iteration 116060, lr = 0.01
I0523 01:56:21.202227 35003 solver.cpp:239] Iteration 116070 (4.67672 iter/s, 2.13825s/10 iters), loss = 7.25057
I0523 01:56:21.202281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25057 (* 1 = 7.25057 loss)
I0523 01:56:21.943130 35003 sgd_solver.cpp:112] Iteration 116070, lr = 0.01
I0523 01:56:25.411173 35003 solver.cpp:239] Iteration 116080 (2.37602 iter/s, 4.20872s/10 iters), loss = 6.01159
I0523 01:56:25.411221 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01159 (* 1 = 6.01159 loss)
I0523 01:56:25.427847 35003 sgd_solver.cpp:112] Iteration 116080, lr = 0.01
I0523 01:56:27.533082 35003 solver.cpp:239] Iteration 116090 (4.71313 iter/s, 2.12173s/10 iters), loss = 7.55733
I0523 01:56:27.533344 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55733 (* 1 = 7.55733 loss)
I0523 01:56:27.539125 35003 sgd_solver.cpp:112] Iteration 116090, lr = 0.01
I0523 01:56:30.233494 35003 solver.cpp:239] Iteration 116100 (3.70365 iter/s, 2.70004s/10 iters), loss = 6.23405
I0523 01:56:30.233537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23405 (* 1 = 6.23405 loss)
I0523 01:56:30.269407 35003 sgd_solver.cpp:112] Iteration 116100, lr = 0.01
I0523 01:56:33.123180 35003 solver.cpp:239] Iteration 116110 (3.4608 iter/s, 2.8895s/10 iters), loss = 7.8458
I0523 01:56:33.123236 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8458 (* 1 = 7.8458 loss)
I0523 01:56:33.569135 35003 sgd_solver.cpp:112] Iteration 116110, lr = 0.01
I0523 01:56:37.623458 35003 solver.cpp:239] Iteration 116120 (2.2222 iter/s, 4.50004s/10 iters), loss = 8.06179
I0523 01:56:37.623509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.06179 (* 1 = 8.06179 loss)
I0523 01:56:37.636684 35003 sgd_solver.cpp:112] Iteration 116120, lr = 0.01
I0523 01:56:41.359556 35003 solver.cpp:239] Iteration 116130 (2.67674 iter/s, 3.73589s/10 iters), loss = 7.29204
I0523 01:56:41.359602 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29204 (* 1 = 7.29204 loss)
I0523 01:56:41.373069 35003 sgd_solver.cpp:112] Iteration 116130, lr = 0.01
I0523 01:56:44.861143 35003 solver.cpp:239] Iteration 116140 (2.85601 iter/s, 3.50139s/10 iters), loss = 6.74093
I0523 01:56:44.861191 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74093 (* 1 = 6.74093 loss)
I0523 01:56:44.871531 35003 sgd_solver.cpp:112] Iteration 116140, lr = 0.01
I0523 01:56:48.355604 35003 solver.cpp:239] Iteration 116150 (2.86183 iter/s, 3.49426s/10 iters), loss = 6.93139
I0523 01:56:48.355656 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93139 (* 1 = 6.93139 loss)
I0523 01:56:49.061112 35003 sgd_solver.cpp:112] Iteration 116150, lr = 0.01
I0523 01:56:53.425091 35003 solver.cpp:239] Iteration 116160 (1.97269 iter/s, 5.06923s/10 iters), loss = 6.83841
I0523 01:56:53.425133 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83841 (* 1 = 6.83841 loss)
I0523 01:56:53.437801 35003 sgd_solver.cpp:112] Iteration 116160, lr = 0.01
I0523 01:56:56.230727 35003 solver.cpp:239] Iteration 116170 (3.56446 iter/s, 2.80547s/10 iters), loss = 7.1491
I0523 01:56:56.230774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1491 (* 1 = 7.1491 loss)
I0523 01:56:56.911226 35003 sgd_solver.cpp:112] Iteration 116170, lr = 0.01
I0523 01:57:00.173310 35003 solver.cpp:239] Iteration 116180 (2.53655 iter/s, 3.94237s/10 iters), loss = 6.14363
I0523 01:57:00.173523 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14363 (* 1 = 6.14363 loss)
I0523 01:57:00.914317 35003 sgd_solver.cpp:112] Iteration 116180, lr = 0.01
I0523 01:57:05.448978 35003 solver.cpp:239] Iteration 116190 (1.89564 iter/s, 5.27525s/10 iters), loss = 7.97459
I0523 01:57:05.449021 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97459 (* 1 = 7.97459 loss)
I0523 01:57:05.452600 35003 sgd_solver.cpp:112] Iteration 116190, lr = 0.01
I0523 01:57:08.519089 35003 solver.cpp:239] Iteration 116200 (3.25741 iter/s, 3.06992s/10 iters), loss = 6.42186
I0523 01:57:08.519140 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42186 (* 1 = 6.42186 loss)
I0523 01:57:08.686825 35003 sgd_solver.cpp:112] Iteration 116200, lr = 0.01
I0523 01:57:11.571235 35003 solver.cpp:239] Iteration 116210 (3.27659 iter/s, 3.05195s/10 iters), loss = 7.18913
I0523 01:57:11.571288 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18913 (* 1 = 7.18913 loss)
I0523 01:57:12.279572 35003 sgd_solver.cpp:112] Iteration 116210, lr = 0.01
I0523 01:57:15.022573 35003 solver.cpp:239] Iteration 116220 (2.89759 iter/s, 3.45114s/10 iters), loss = 7.37751
I0523 01:57:15.022619 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37751 (* 1 = 7.37751 loss)
I0523 01:57:15.034639 35003 sgd_solver.cpp:112] Iteration 116220, lr = 0.01
I0523 01:57:17.126966 35003 solver.cpp:239] Iteration 116230 (4.75228 iter/s, 2.10425s/10 iters), loss = 6.4576
I0523 01:57:17.127014 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4576 (* 1 = 6.4576 loss)
I0523 01:57:17.139338 35003 sgd_solver.cpp:112] Iteration 116230, lr = 0.01
I0523 01:57:19.440162 35003 solver.cpp:239] Iteration 116240 (4.32331 iter/s, 2.31304s/10 iters), loss = 7.50388
I0523 01:57:19.440207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50388 (* 1 = 7.50388 loss)
I0523 01:57:20.170711 35003 sgd_solver.cpp:112] Iteration 116240, lr = 0.01
I0523 01:57:23.686674 35003 solver.cpp:239] Iteration 116250 (2.355 iter/s, 4.24629s/10 iters), loss = 7.53665
I0523 01:57:23.686741 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53665 (* 1 = 7.53665 loss)
I0523 01:57:23.695547 35003 sgd_solver.cpp:112] Iteration 116250, lr = 0.01
I0523 01:57:25.990952 35003 solver.cpp:239] Iteration 116260 (4.34008 iter/s, 2.3041s/10 iters), loss = 7.37964
I0523 01:57:25.991014 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37964 (* 1 = 7.37964 loss)
I0523 01:57:26.003780 35003 sgd_solver.cpp:112] Iteration 116260, lr = 0.01
I0523 01:57:28.935533 35003 solver.cpp:239] Iteration 116270 (3.39629 iter/s, 2.94439s/10 iters), loss = 7.79592
I0523 01:57:28.935595 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79592 (* 1 = 7.79592 loss)
I0523 01:57:29.657023 35003 sgd_solver.cpp:112] Iteration 116270, lr = 0.01
I0523 01:57:31.706960 35003 solver.cpp:239] Iteration 116280 (3.60848 iter/s, 2.77125s/10 iters), loss = 5.97574
I0523 01:57:31.707134 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97574 (* 1 = 5.97574 loss)
I0523 01:57:31.720734 35003 sgd_solver.cpp:112] Iteration 116280, lr = 0.01
I0523 01:57:35.317952 35003 solver.cpp:239] Iteration 116290 (2.76956 iter/s, 3.61068s/10 iters), loss = 6.53698
I0523 01:57:35.318001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53698 (* 1 = 6.53698 loss)
I0523 01:57:35.324436 35003 sgd_solver.cpp:112] Iteration 116290, lr = 0.01
I0523 01:57:39.595468 35003 solver.cpp:239] Iteration 116300 (2.33793 iter/s, 4.27729s/10 iters), loss = 6.83306
I0523 01:57:39.595525 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83306 (* 1 = 6.83306 loss)
I0523 01:57:40.310217 35003 sgd_solver.cpp:112] Iteration 116300, lr = 0.01
I0523 01:57:43.993330 35003 solver.cpp:239] Iteration 116310 (2.27396 iter/s, 4.39762s/10 iters), loss = 8.38902
I0523 01:57:43.993396 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.38902 (* 1 = 8.38902 loss)
I0523 01:57:44.004273 35003 sgd_solver.cpp:112] Iteration 116310, lr = 0.01
I0523 01:57:46.251560 35003 solver.cpp:239] Iteration 116320 (4.42857 iter/s, 2.25806s/10 iters), loss = 7.70367
I0523 01:57:46.251605 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70367 (* 1 = 7.70367 loss)
I0523 01:57:46.256475 35003 sgd_solver.cpp:112] Iteration 116320, lr = 0.01
I0523 01:57:48.430430 35003 solver.cpp:239] Iteration 116330 (4.58983 iter/s, 2.17873s/10 iters), loss = 6.38729
I0523 01:57:48.430479 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38729 (* 1 = 6.38729 loss)
I0523 01:57:48.438519 35003 sgd_solver.cpp:112] Iteration 116330, lr = 0.01
I0523 01:57:51.784873 35003 solver.cpp:239] Iteration 116340 (2.98129 iter/s, 3.35425s/10 iters), loss = 6.18489
I0523 01:57:51.784914 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18489 (* 1 = 6.18489 loss)
I0523 01:57:51.790228 35003 sgd_solver.cpp:112] Iteration 116340, lr = 0.01
I0523 01:57:54.983980 35003 solver.cpp:239] Iteration 116350 (3.12606 iter/s, 3.19892s/10 iters), loss = 6.81543
I0523 01:57:54.984036 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81543 (* 1 = 6.81543 loss)
I0523 01:57:55.705278 35003 sgd_solver.cpp:112] Iteration 116350, lr = 0.01
I0523 01:57:59.411015 35003 solver.cpp:239] Iteration 116360 (2.25897 iter/s, 4.42679s/10 iters), loss = 7.31755
I0523 01:57:59.411059 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31755 (* 1 = 7.31755 loss)
I0523 01:57:59.424583 35003 sgd_solver.cpp:112] Iteration 116360, lr = 0.01
I0523 01:58:02.036509 35003 solver.cpp:239] Iteration 116370 (3.80903 iter/s, 2.62534s/10 iters), loss = 7.90872
I0523 01:58:02.036777 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90872 (* 1 = 7.90872 loss)
I0523 01:58:02.045761 35003 sgd_solver.cpp:112] Iteration 116370, lr = 0.01
I0523 01:58:04.811645 35003 solver.cpp:239] Iteration 116380 (3.60389 iter/s, 2.77478s/10 iters), loss = 6.80445
I0523 01:58:04.811688 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80445 (* 1 = 6.80445 loss)
I0523 01:58:05.216940 35003 sgd_solver.cpp:112] Iteration 116380, lr = 0.01
I0523 01:58:08.798446 35003 solver.cpp:239] Iteration 116390 (2.50841 iter/s, 3.98659s/10 iters), loss = 7.08584
I0523 01:58:08.798483 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08584 (* 1 = 7.08584 loss)
I0523 01:58:08.812031 35003 sgd_solver.cpp:112] Iteration 116390, lr = 0.01
I0523 01:58:12.606523 35003 solver.cpp:239] Iteration 116400 (2.62614 iter/s, 3.80787s/10 iters), loss = 6.49999
I0523 01:58:12.606570 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49999 (* 1 = 6.49999 loss)
I0523 01:58:12.945940 35003 sgd_solver.cpp:112] Iteration 116400, lr = 0.01
I0523 01:58:14.337033 35003 solver.cpp:239] Iteration 116410 (5.77906 iter/s, 1.73038s/10 iters), loss = 6.86875
I0523 01:58:14.337074 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86875 (* 1 = 6.86875 loss)
I0523 01:58:15.052531 35003 sgd_solver.cpp:112] Iteration 116410, lr = 0.01
I0523 01:58:19.450278 35003 solver.cpp:239] Iteration 116420 (1.9558 iter/s, 5.11299s/10 iters), loss = 6.87722
I0523 01:58:19.450338 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87722 (* 1 = 6.87722 loss)
I0523 01:58:19.458142 35003 sgd_solver.cpp:112] Iteration 116420, lr = 0.01
I0523 01:58:23.788815 35003 solver.cpp:239] Iteration 116430 (2.30505 iter/s, 4.3383s/10 iters), loss = 7.01831
I0523 01:58:23.788871 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01831 (* 1 = 7.01831 loss)
I0523 01:58:24.516728 35003 sgd_solver.cpp:112] Iteration 116430, lr = 0.01
I0523 01:58:26.964058 35003 solver.cpp:239] Iteration 116440 (3.14955 iter/s, 3.17505s/10 iters), loss = 6.77727
I0523 01:58:26.964118 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77727 (* 1 = 6.77727 loss)
I0523 01:58:27.639307 35003 sgd_solver.cpp:112] Iteration 116440, lr = 0.01
I0523 01:58:30.670310 35003 solver.cpp:239] Iteration 116450 (2.6983 iter/s, 3.70604s/10 iters), loss = 7.50841
I0523 01:58:30.670353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50841 (* 1 = 7.50841 loss)
I0523 01:58:30.683812 35003 sgd_solver.cpp:112] Iteration 116450, lr = 0.01
I0523 01:58:34.206007 35003 solver.cpp:239] Iteration 116460 (2.82845 iter/s, 3.53551s/10 iters), loss = 6.90107
I0523 01:58:34.206228 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90107 (* 1 = 6.90107 loss)
I0523 01:58:34.941126 35003 sgd_solver.cpp:112] Iteration 116460, lr = 0.01
I0523 01:58:38.409386 35003 solver.cpp:239] Iteration 116470 (2.37926 iter/s, 4.20299s/10 iters), loss = 5.98952
I0523 01:58:38.409427 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98952 (* 1 = 5.98952 loss)
I0523 01:58:38.979645 35003 sgd_solver.cpp:112] Iteration 116470, lr = 0.01
I0523 01:58:41.919231 35003 solver.cpp:239] Iteration 116480 (2.84929 iter/s, 3.50965s/10 iters), loss = 7.70163
I0523 01:58:41.919281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70163 (* 1 = 7.70163 loss)
I0523 01:58:42.647919 35003 sgd_solver.cpp:112] Iteration 116480, lr = 0.01
I0523 01:58:45.298882 35003 solver.cpp:239] Iteration 116490 (2.95906 iter/s, 3.37945s/10 iters), loss = 7.21195
I0523 01:58:45.298929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21195 (* 1 = 7.21195 loss)
I0523 01:58:45.926177 35003 sgd_solver.cpp:112] Iteration 116490, lr = 0.01
I0523 01:58:48.189194 35003 solver.cpp:239] Iteration 116500 (3.46004 iter/s, 2.89014s/10 iters), loss = 6.50877
I0523 01:58:48.189302 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50877 (* 1 = 6.50877 loss)
I0523 01:58:48.195055 35003 sgd_solver.cpp:112] Iteration 116500, lr = 0.01
I0523 01:58:50.720063 35003 solver.cpp:239] Iteration 116510 (3.95157 iter/s, 2.53064s/10 iters), loss = 5.26909
I0523 01:58:50.720113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.26909 (* 1 = 5.26909 loss)
I0523 01:58:51.413739 35003 sgd_solver.cpp:112] Iteration 116510, lr = 0.01
I0523 01:58:53.464337 35003 solver.cpp:239] Iteration 116520 (3.64417 iter/s, 2.74411s/10 iters), loss = 8.42511
I0523 01:58:53.464382 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.42511 (* 1 = 8.42511 loss)
I0523 01:58:54.029799 35003 sgd_solver.cpp:112] Iteration 116520, lr = 0.01
I0523 01:58:56.408151 35003 solver.cpp:239] Iteration 116530 (3.39725 iter/s, 2.94356s/10 iters), loss = 7.4443
I0523 01:58:56.408207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4443 (* 1 = 7.4443 loss)
I0523 01:58:57.149125 35003 sgd_solver.cpp:112] Iteration 116530, lr = 0.01
I0523 01:59:00.080768 35003 solver.cpp:239] Iteration 116540 (2.72301 iter/s, 3.67241s/10 iters), loss = 7.24528
I0523 01:59:00.080817 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24528 (* 1 = 7.24528 loss)
I0523 01:59:00.103333 35003 sgd_solver.cpp:112] Iteration 116540, lr = 0.01
I0523 01:59:03.743463 35003 solver.cpp:239] Iteration 116550 (2.73038 iter/s, 3.66249s/10 iters), loss = 6.59155
I0523 01:59:03.743505 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59155 (* 1 = 6.59155 loss)
I0523 01:59:04.426481 35003 sgd_solver.cpp:112] Iteration 116550, lr = 0.01
I0523 01:59:08.169085 35003 solver.cpp:239] Iteration 116560 (2.25969 iter/s, 4.4254s/10 iters), loss = 6.18346
I0523 01:59:08.169158 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18346 (* 1 = 6.18346 loss)
I0523 01:59:08.889111 35003 sgd_solver.cpp:112] Iteration 116560, lr = 0.01
I0523 01:59:13.540194 35003 solver.cpp:239] Iteration 116570 (1.86191 iter/s, 5.37082s/10 iters), loss = 6.63978
I0523 01:59:13.540236 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63978 (* 1 = 6.63978 loss)
I0523 01:59:13.549515 35003 sgd_solver.cpp:112] Iteration 116570, lr = 0.01
I0523 01:59:17.888844 35003 solver.cpp:239] Iteration 116580 (2.29968 iter/s, 4.34843s/10 iters), loss = 8.36869
I0523 01:59:17.888896 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.36869 (* 1 = 8.36869 loss)
I0523 01:59:17.895741 35003 sgd_solver.cpp:112] Iteration 116580, lr = 0.01
I0523 01:59:20.805425 35003 solver.cpp:239] Iteration 116590 (3.4289 iter/s, 2.91639s/10 iters), loss = 8.14759
I0523 01:59:20.805470 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.14759 (* 1 = 8.14759 loss)
I0523 01:59:20.815964 35003 sgd_solver.cpp:112] Iteration 116590, lr = 0.01
I0523 01:59:24.029973 35003 solver.cpp:239] Iteration 116600 (3.10139 iter/s, 3.22436s/10 iters), loss = 6.66856
I0523 01:59:24.030040 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66856 (* 1 = 6.66856 loss)
I0523 01:59:24.036303 35003 sgd_solver.cpp:112] Iteration 116600, lr = 0.01
I0523 01:59:27.514349 35003 solver.cpp:239] Iteration 116610 (2.87013 iter/s, 3.48417s/10 iters), loss = 8.21482
I0523 01:59:27.514394 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.21482 (* 1 = 8.21482 loss)
I0523 01:59:27.527655 35003 sgd_solver.cpp:112] Iteration 116610, lr = 0.01
I0523 01:59:31.862434 35003 solver.cpp:239] Iteration 116620 (2.29999 iter/s, 4.34784s/10 iters), loss = 7.59917
I0523 01:59:31.862480 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59917 (* 1 = 7.59917 loss)
I0523 01:59:31.870080 35003 sgd_solver.cpp:112] Iteration 116620, lr = 0.01
I0523 01:59:34.927978 35003 solver.cpp:239] Iteration 116630 (3.26227 iter/s, 3.06535s/10 iters), loss = 5.46177
I0523 01:59:34.928150 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.46177 (* 1 = 5.46177 loss)
I0523 01:59:34.940784 35003 sgd_solver.cpp:112] Iteration 116630, lr = 0.01
I0523 01:59:37.790473 35003 solver.cpp:239] Iteration 116640 (3.4938 iter/s, 2.86221s/10 iters), loss = 6.77468
I0523 01:59:37.790509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77468 (* 1 = 6.77468 loss)
I0523 01:59:37.808856 35003 sgd_solver.cpp:112] Iteration 116640, lr = 0.01
I0523 01:59:42.575894 35003 solver.cpp:239] Iteration 116650 (2.08978 iter/s, 4.78519s/10 iters), loss = 7.64621
I0523 01:59:42.575939 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64621 (* 1 = 7.64621 loss)
I0523 01:59:42.589908 35003 sgd_solver.cpp:112] Iteration 116650, lr = 0.01
I0523 01:59:46.930651 35003 solver.cpp:239] Iteration 116660 (2.29646 iter/s, 4.35452s/10 iters), loss = 7.55015
I0523 01:59:46.930773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55015 (* 1 = 7.55015 loss)
I0523 01:59:47.548243 35003 sgd_solver.cpp:112] Iteration 116660, lr = 0.01
I0523 01:59:50.569927 35003 solver.cpp:239] Iteration 116670 (2.74801 iter/s, 3.639s/10 iters), loss = 6.4644
I0523 01:59:50.569977 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4644 (* 1 = 6.4644 loss)
I0523 01:59:50.577778 35003 sgd_solver.cpp:112] Iteration 116670, lr = 0.01
I0523 01:59:53.456710 35003 solver.cpp:239] Iteration 116680 (3.46427 iter/s, 2.88661s/10 iters), loss = 5.87392
I0523 01:59:53.456756 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87392 (* 1 = 5.87392 loss)
I0523 01:59:53.853521 35003 sgd_solver.cpp:112] Iteration 116680, lr = 0.01
I0523 01:59:58.160742 35003 solver.cpp:239] Iteration 116690 (2.12594 iter/s, 4.7038s/10 iters), loss = 6.98321
I0523 01:59:58.160784 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98321 (* 1 = 6.98321 loss)
I0523 01:59:58.174070 35003 sgd_solver.cpp:112] Iteration 116690, lr = 0.01
I0523 02:00:00.966861 35003 solver.cpp:239] Iteration 116700 (3.56385 iter/s, 2.80595s/10 iters), loss = 6.35111
I0523 02:00:00.966904 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35111 (* 1 = 6.35111 loss)
I0523 02:00:00.975931 35003 sgd_solver.cpp:112] Iteration 116700, lr = 0.01
I0523 02:00:03.022534 35003 solver.cpp:239] Iteration 116710 (4.86491 iter/s, 2.05554s/10 iters), loss = 6.994
I0523 02:00:03.022584 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.994 (* 1 = 6.994 loss)
I0523 02:00:03.035998 35003 sgd_solver.cpp:112] Iteration 116710, lr = 0.01
I0523 02:00:06.580240 35003 solver.cpp:239] Iteration 116720 (2.81095 iter/s, 3.55751s/10 iters), loss = 7.46016
I0523 02:00:06.580539 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46016 (* 1 = 7.46016 loss)
I0523 02:00:06.588181 35003 sgd_solver.cpp:112] Iteration 116720, lr = 0.01
I0523 02:00:09.225435 35003 solver.cpp:239] Iteration 116730 (3.78099 iter/s, 2.64481s/10 iters), loss = 6.64714
I0523 02:00:09.225477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64714 (* 1 = 6.64714 loss)
I0523 02:00:09.236755 35003 sgd_solver.cpp:112] Iteration 116730, lr = 0.01
I0523 02:00:11.224898 35003 solver.cpp:239] Iteration 116740 (5.0017 iter/s, 1.99932s/10 iters), loss = 7.76492
I0523 02:00:11.224944 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76492 (* 1 = 7.76492 loss)
I0523 02:00:11.247661 35003 sgd_solver.cpp:112] Iteration 116740, lr = 0.01
I0523 02:00:14.038461 35003 solver.cpp:239] Iteration 116750 (3.55443 iter/s, 2.81339s/10 iters), loss = 6.67679
I0523 02:00:14.038527 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67679 (* 1 = 6.67679 loss)
I0523 02:00:14.779446 35003 sgd_solver.cpp:112] Iteration 116750, lr = 0.01
I0523 02:00:16.810827 35003 solver.cpp:239] Iteration 116760 (3.60728 iter/s, 2.77217s/10 iters), loss = 6.95829
I0523 02:00:16.810871 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95829 (* 1 = 6.95829 loss)
I0523 02:00:16.816849 35003 sgd_solver.cpp:112] Iteration 116760, lr = 0.01
I0523 02:00:19.793632 35003 solver.cpp:239] Iteration 116770 (3.35274 iter/s, 2.98263s/10 iters), loss = 6.51635
I0523 02:00:19.793687 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51635 (* 1 = 6.51635 loss)
I0523 02:00:19.803336 35003 sgd_solver.cpp:112] Iteration 116770, lr = 0.01
I0523 02:00:23.330799 35003 solver.cpp:239] Iteration 116780 (2.82729 iter/s, 3.53696s/10 iters), loss = 6.23274
I0523 02:00:23.330839 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23274 (* 1 = 6.23274 loss)
I0523 02:00:23.344357 35003 sgd_solver.cpp:112] Iteration 116780, lr = 0.01
I0523 02:00:26.926512 35003 solver.cpp:239] Iteration 116790 (2.78125 iter/s, 3.59551s/10 iters), loss = 8.05381
I0523 02:00:26.926550 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05381 (* 1 = 8.05381 loss)
I0523 02:00:26.939046 35003 sgd_solver.cpp:112] Iteration 116790, lr = 0.01
I0523 02:00:29.583051 35003 solver.cpp:239] Iteration 116800 (3.76454 iter/s, 2.65637s/10 iters), loss = 6.64393
I0523 02:00:29.583099 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64393 (* 1 = 6.64393 loss)
I0523 02:00:29.587879 35003 sgd_solver.cpp:112] Iteration 116800, lr = 0.01
I0523 02:00:32.409451 35003 solver.cpp:239] Iteration 116810 (3.53829 iter/s, 2.82623s/10 iters), loss = 6.76443
I0523 02:00:32.409493 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76443 (* 1 = 6.76443 loss)
I0523 02:00:32.414849 35003 sgd_solver.cpp:112] Iteration 116810, lr = 0.01
I0523 02:00:36.736119 35003 solver.cpp:239] Iteration 116820 (2.31138 iter/s, 4.32641s/10 iters), loss = 5.33369
I0523 02:00:36.736361 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.33369 (* 1 = 5.33369 loss)
I0523 02:00:37.319149 35003 sgd_solver.cpp:112] Iteration 116820, lr = 0.01
I0523 02:00:40.947345 35003 solver.cpp:239] Iteration 116830 (2.37483 iter/s, 4.21084s/10 iters), loss = 7.90029
I0523 02:00:40.947398 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90029 (* 1 = 7.90029 loss)
I0523 02:00:41.675889 35003 sgd_solver.cpp:112] Iteration 116830, lr = 0.01
I0523 02:00:45.592224 35003 solver.cpp:239] Iteration 116840 (2.15302 iter/s, 4.64464s/10 iters), loss = 7.19786
I0523 02:00:45.592279 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19786 (* 1 = 7.19786 loss)
I0523 02:00:46.159368 35003 sgd_solver.cpp:112] Iteration 116840, lr = 0.01
I0523 02:00:50.333689 35003 solver.cpp:239] Iteration 116850 (2.10916 iter/s, 4.74122s/10 iters), loss = 5.57454
I0523 02:00:50.333729 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.57454 (* 1 = 5.57454 loss)
I0523 02:00:50.339898 35003 sgd_solver.cpp:112] Iteration 116850, lr = 0.01
I0523 02:00:53.877728 35003 solver.cpp:239] Iteration 116860 (2.8218 iter/s, 3.54383s/10 iters), loss = 6.71169
I0523 02:00:53.877776 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71169 (* 1 = 6.71169 loss)
I0523 02:00:53.891536 35003 sgd_solver.cpp:112] Iteration 116860, lr = 0.01
I0523 02:00:56.625922 35003 solver.cpp:239] Iteration 116870 (3.63897 iter/s, 2.74803s/10 iters), loss = 5.80467
I0523 02:00:56.625965 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.80467 (* 1 = 5.80467 loss)
I0523 02:00:56.635941 35003 sgd_solver.cpp:112] Iteration 116870, lr = 0.01
I0523 02:00:59.345559 35003 solver.cpp:239] Iteration 116880 (3.67718 iter/s, 2.71948s/10 iters), loss = 6.26884
I0523 02:00:59.345594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26884 (* 1 = 6.26884 loss)
I0523 02:00:59.358913 35003 sgd_solver.cpp:112] Iteration 116880, lr = 0.01
I0523 02:01:04.102041 35003 solver.cpp:239] Iteration 116890 (2.1025 iter/s, 4.75625s/10 iters), loss = 5.89581
I0523 02:01:04.102089 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89581 (* 1 = 5.89581 loss)
I0523 02:01:04.106710 35003 sgd_solver.cpp:112] Iteration 116890, lr = 0.01
I0523 02:01:06.864751 35003 solver.cpp:239] Iteration 116900 (3.61985 iter/s, 2.76255s/10 iters), loss = 6.47377
I0523 02:01:06.864939 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47377 (* 1 = 6.47377 loss)
I0523 02:01:07.009109 35003 sgd_solver.cpp:112] Iteration 116900, lr = 0.01
I0523 02:01:09.033416 35003 solver.cpp:239] Iteration 116910 (4.61173 iter/s, 2.16838s/10 iters), loss = 7.13031
I0523 02:01:09.033468 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13031 (* 1 = 7.13031 loss)
I0523 02:01:09.038214 35003 sgd_solver.cpp:112] Iteration 116910, lr = 0.01
I0523 02:01:12.545559 35003 solver.cpp:239] Iteration 116920 (2.84743 iter/s, 3.51194s/10 iters), loss = 7.08165
I0523 02:01:12.545616 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08165 (* 1 = 7.08165 loss)
I0523 02:01:12.552126 35003 sgd_solver.cpp:112] Iteration 116920, lr = 0.01
I0523 02:01:15.699220 35003 solver.cpp:239] Iteration 116930 (3.17111 iter/s, 3.15347s/10 iters), loss = 6.41604
I0523 02:01:15.699261 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41604 (* 1 = 6.41604 loss)
I0523 02:01:15.712703 35003 sgd_solver.cpp:112] Iteration 116930, lr = 0.01
I0523 02:01:19.345983 35003 solver.cpp:239] Iteration 116940 (2.74231 iter/s, 3.64656s/10 iters), loss = 6.27414
I0523 02:01:19.346029 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27414 (* 1 = 6.27414 loss)
I0523 02:01:19.351119 35003 sgd_solver.cpp:112] Iteration 116940, lr = 0.01
I0523 02:01:23.111992 35003 solver.cpp:239] Iteration 116950 (2.65645 iter/s, 3.76442s/10 iters), loss = 6.57343
I0523 02:01:23.112035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57343 (* 1 = 6.57343 loss)
I0523 02:01:23.118737 35003 sgd_solver.cpp:112] Iteration 116950, lr = 0.01
I0523 02:01:26.511857 35003 solver.cpp:239] Iteration 116960 (2.94145 iter/s, 3.39968s/10 iters), loss = 6.97922
I0523 02:01:26.511893 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97922 (* 1 = 6.97922 loss)
I0523 02:01:26.530047 35003 sgd_solver.cpp:112] Iteration 116960, lr = 0.01
I0523 02:01:28.823276 35003 solver.cpp:239] Iteration 116970 (4.3266 iter/s, 2.31128s/10 iters), loss = 5.87856
I0523 02:01:28.823313 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87856 (* 1 = 5.87856 loss)
I0523 02:01:28.836467 35003 sgd_solver.cpp:112] Iteration 116970, lr = 0.01
I0523 02:01:31.572170 35003 solver.cpp:239] Iteration 116980 (3.63803 iter/s, 2.74874s/10 iters), loss = 7.14576
I0523 02:01:31.572203 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14576 (* 1 = 7.14576 loss)
I0523 02:01:31.580276 35003 sgd_solver.cpp:112] Iteration 116980, lr = 0.01
I0523 02:01:34.172623 35003 solver.cpp:239] Iteration 116990 (3.8457 iter/s, 2.60031s/10 iters), loss = 7.15712
I0523 02:01:34.172672 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15712 (* 1 = 7.15712 loss)
I0523 02:01:34.185933 35003 sgd_solver.cpp:112] Iteration 116990, lr = 0.01
I0523 02:01:39.088958 35003 solver.cpp:239] Iteration 117000 (2.03414 iter/s, 4.91608s/10 iters), loss = 7.56406
I0523 02:01:39.089164 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56406 (* 1 = 7.56406 loss)
I0523 02:01:39.101991 35003 sgd_solver.cpp:112] Iteration 117000, lr = 0.01
I0523 02:01:41.128352 35003 solver.cpp:239] Iteration 117010 (4.90412 iter/s, 2.0391s/10 iters), loss = 6.92257
I0523 02:01:41.128394 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92257 (* 1 = 6.92257 loss)
I0523 02:01:41.141984 35003 sgd_solver.cpp:112] Iteration 117010, lr = 0.01
I0523 02:01:45.398649 35003 solver.cpp:239] Iteration 117020 (2.34188 iter/s, 4.27008s/10 iters), loss = 8.04839
I0523 02:01:45.398727 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04839 (* 1 = 8.04839 loss)
I0523 02:01:46.012516 35003 sgd_solver.cpp:112] Iteration 117020, lr = 0.01
I0523 02:01:48.597924 35003 solver.cpp:239] Iteration 117030 (3.12589 iter/s, 3.19909s/10 iters), loss = 6.78812
I0523 02:01:48.597968 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78812 (* 1 = 6.78812 loss)
I0523 02:01:49.256944 35003 sgd_solver.cpp:112] Iteration 117030, lr = 0.01
I0523 02:01:52.613104 35003 solver.cpp:239] Iteration 117040 (2.49068 iter/s, 4.01497s/10 iters), loss = 5.97286
I0523 02:01:52.613147 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97286 (* 1 = 5.97286 loss)
I0523 02:01:52.626603 35003 sgd_solver.cpp:112] Iteration 117040, lr = 0.01
I0523 02:01:57.079717 35003 solver.cpp:239] Iteration 117050 (2.23895 iter/s, 4.46639s/10 iters), loss = 6.6818
I0523 02:01:57.079761 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6818 (* 1 = 6.6818 loss)
I0523 02:01:57.093025 35003 sgd_solver.cpp:112] Iteration 117050, lr = 0.01
I0523 02:01:59.226773 35003 solver.cpp:239] Iteration 117060 (4.65786 iter/s, 2.14691s/10 iters), loss = 7.90336
I0523 02:01:59.226819 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90336 (* 1 = 7.90336 loss)
I0523 02:01:59.942203 35003 sgd_solver.cpp:112] Iteration 117060, lr = 0.01
I0523 02:02:01.918725 35003 solver.cpp:239] Iteration 117070 (3.71503 iter/s, 2.69177s/10 iters), loss = 6.50729
I0523 02:02:01.918781 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50729 (* 1 = 6.50729 loss)
I0523 02:02:01.927819 35003 sgd_solver.cpp:112] Iteration 117070, lr = 0.01
I0523 02:02:03.010406 35003 solver.cpp:239] Iteration 117080 (9.16107 iter/s, 1.09158s/10 iters), loss = 7.23966
I0523 02:02:03.010443 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23966 (* 1 = 7.23966 loss)
I0523 02:02:03.136101 35003 sgd_solver.cpp:112] Iteration 117080, lr = 0.01
I0523 02:02:04.092520 35003 solver.cpp:239] Iteration 117090 (9.24194 iter/s, 1.08202s/10 iters), loss = 6.64918
I0523 02:02:04.092561 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64918 (* 1 = 6.64918 loss)
I0523 02:02:04.111742 35003 sgd_solver.cpp:112] Iteration 117090, lr = 0.01
I0523 02:02:07.602999 35003 solver.cpp:239] Iteration 117100 (2.84878 iter/s, 3.51028s/10 iters), loss = 6.77177
I0523 02:02:07.603055 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77177 (* 1 = 6.77177 loss)
I0523 02:02:07.607856 35003 sgd_solver.cpp:112] Iteration 117100, lr = 0.01
I0523 02:02:11.242990 35003 solver.cpp:239] Iteration 117110 (2.74743 iter/s, 3.63977s/10 iters), loss = 7.3548
I0523 02:02:11.243213 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3548 (* 1 = 7.3548 loss)
I0523 02:02:11.957897 35003 sgd_solver.cpp:112] Iteration 117110, lr = 0.01
I0523 02:02:14.886445 35003 solver.cpp:239] Iteration 117120 (2.74492 iter/s, 3.64309s/10 iters), loss = 7.14659
I0523 02:02:14.886487 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14659 (* 1 = 7.14659 loss)
I0523 02:02:14.899549 35003 sgd_solver.cpp:112] Iteration 117120, lr = 0.01
I0523 02:02:18.676950 35003 solver.cpp:239] Iteration 117130 (2.63831 iter/s, 3.7903s/10 iters), loss = 6.87219
I0523 02:02:18.676991 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87219 (* 1 = 6.87219 loss)
I0523 02:02:19.410810 35003 sgd_solver.cpp:112] Iteration 117130, lr = 0.01
I0523 02:02:23.319444 35003 solver.cpp:239] Iteration 117140 (2.15412 iter/s, 4.64226s/10 iters), loss = 6.76593
I0523 02:02:23.319480 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76593 (* 1 = 6.76593 loss)
I0523 02:02:23.332492 35003 sgd_solver.cpp:112] Iteration 117140, lr = 0.01
I0523 02:02:26.004811 35003 solver.cpp:239] Iteration 117150 (3.7241 iter/s, 2.68521s/10 iters), loss = 6.13233
I0523 02:02:26.004856 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13233 (* 1 = 6.13233 loss)
I0523 02:02:26.017787 35003 sgd_solver.cpp:112] Iteration 117150, lr = 0.01
I0523 02:02:29.494643 35003 solver.cpp:239] Iteration 117160 (2.86563 iter/s, 3.48964s/10 iters), loss = 6.47073
I0523 02:02:29.494714 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47073 (* 1 = 6.47073 loss)
I0523 02:02:29.500488 35003 sgd_solver.cpp:112] Iteration 117160, lr = 0.01
I0523 02:02:31.592486 35003 solver.cpp:239] Iteration 117170 (4.76713 iter/s, 2.0977s/10 iters), loss = 7.34932
I0523 02:02:31.592528 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34932 (* 1 = 7.34932 loss)
I0523 02:02:31.608245 35003 sgd_solver.cpp:112] Iteration 117170, lr = 0.01
I0523 02:02:35.107300 35003 solver.cpp:239] Iteration 117180 (2.84526 iter/s, 3.51462s/10 iters), loss = 7.25622
I0523 02:02:35.107349 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25622 (* 1 = 7.25622 loss)
I0523 02:02:35.133607 35003 sgd_solver.cpp:112] Iteration 117180, lr = 0.01
I0523 02:02:38.794809 35003 solver.cpp:239] Iteration 117190 (2.71201 iter/s, 3.68731s/10 iters), loss = 7.40496
I0523 02:02:38.794860 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40496 (* 1 = 7.40496 loss)
I0523 02:02:38.803223 35003 sgd_solver.cpp:112] Iteration 117190, lr = 0.01
I0523 02:02:40.171484 35003 solver.cpp:239] Iteration 117200 (7.26448 iter/s, 1.37656s/10 iters), loss = 7.03752
I0523 02:02:40.171530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03752 (* 1 = 7.03752 loss)
I0523 02:02:40.912761 35003 sgd_solver.cpp:112] Iteration 117200, lr = 0.01
I0523 02:02:45.226810 35003 solver.cpp:239] Iteration 117210 (1.97821 iter/s, 5.05508s/10 iters), loss = 7.48251
I0523 02:02:45.226953 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48251 (* 1 = 7.48251 loss)
I0523 02:02:45.239724 35003 sgd_solver.cpp:112] Iteration 117210, lr = 0.01
I0523 02:02:47.275686 35003 solver.cpp:239] Iteration 117220 (4.88123 iter/s, 2.04867s/10 iters), loss = 7.14291
I0523 02:02:47.275725 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14291 (* 1 = 7.14291 loss)
I0523 02:02:47.288950 35003 sgd_solver.cpp:112] Iteration 117220, lr = 0.01
I0523 02:02:51.165443 35003 solver.cpp:239] Iteration 117230 (2.57099 iter/s, 3.88955s/10 iters), loss = 7.53592
I0523 02:02:51.165482 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53592 (* 1 = 7.53592 loss)
I0523 02:02:51.178966 35003 sgd_solver.cpp:112] Iteration 117230, lr = 0.01
I0523 02:02:53.536000 35003 solver.cpp:239] Iteration 117240 (4.21868 iter/s, 2.37041s/10 iters), loss = 7.49239
I0523 02:02:53.536046 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49239 (* 1 = 7.49239 loss)
I0523 02:02:53.540207 35003 sgd_solver.cpp:112] Iteration 117240, lr = 0.01
I0523 02:02:56.416448 35003 solver.cpp:239] Iteration 117250 (3.47188 iter/s, 2.88028s/10 iters), loss = 7.42564
I0523 02:02:56.416499 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42564 (* 1 = 7.42564 loss)
I0523 02:02:56.430375 35003 sgd_solver.cpp:112] Iteration 117250, lr = 0.01
I0523 02:02:59.962644 35003 solver.cpp:239] Iteration 117260 (2.82008 iter/s, 3.546s/10 iters), loss = 7.45774
I0523 02:02:59.962746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45774 (* 1 = 7.45774 loss)
I0523 02:03:00.678308 35003 sgd_solver.cpp:112] Iteration 117260, lr = 0.01
I0523 02:03:04.315842 35003 solver.cpp:239] Iteration 117270 (2.29729 iter/s, 4.35295s/10 iters), loss = 6.36201
I0523 02:03:04.315882 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36201 (* 1 = 6.36201 loss)
I0523 02:03:04.407125 35003 sgd_solver.cpp:112] Iteration 117270, lr = 0.01
I0523 02:03:07.436933 35003 solver.cpp:239] Iteration 117280 (3.20418 iter/s, 3.12092s/10 iters), loss = 7.37018
I0523 02:03:07.436978 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37018 (* 1 = 7.37018 loss)
I0523 02:03:08.176249 35003 sgd_solver.cpp:112] Iteration 117280, lr = 0.01
I0523 02:03:12.524957 35003 solver.cpp:239] Iteration 117290 (1.9655 iter/s, 5.08777s/10 iters), loss = 6.80709
I0523 02:03:12.525023 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80709 (* 1 = 6.80709 loss)
I0523 02:03:13.240555 35003 sgd_solver.cpp:112] Iteration 117290, lr = 0.01
I0523 02:03:16.973738 35003 solver.cpp:239] Iteration 117300 (2.24793 iter/s, 4.44854s/10 iters), loss = 7.9704
I0523 02:03:16.974050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9704 (* 1 = 7.9704 loss)
I0523 02:03:17.232429 35003 sgd_solver.cpp:112] Iteration 117300, lr = 0.01
I0523 02:03:20.432183 35003 solver.cpp:239] Iteration 117310 (2.89183 iter/s, 3.45802s/10 iters), loss = 6.63567
I0523 02:03:20.432226 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63567 (* 1 = 6.63567 loss)
I0523 02:03:20.439756 35003 sgd_solver.cpp:112] Iteration 117310, lr = 0.01
I0523 02:03:24.059716 35003 solver.cpp:239] Iteration 117320 (2.75684 iter/s, 3.62734s/10 iters), loss = 6.71327
I0523 02:03:24.059768 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71327 (* 1 = 6.71327 loss)
I0523 02:03:24.065950 35003 sgd_solver.cpp:112] Iteration 117320, lr = 0.01
I0523 02:03:28.846942 35003 solver.cpp:239] Iteration 117330 (2.089 iter/s, 4.78698s/10 iters), loss = 7.26589
I0523 02:03:28.846984 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26589 (* 1 = 7.26589 loss)
I0523 02:03:29.218708 35003 sgd_solver.cpp:112] Iteration 117330, lr = 0.01
I0523 02:03:31.299876 35003 solver.cpp:239] Iteration 117340 (4.077 iter/s, 2.45278s/10 iters), loss = 7.98902
I0523 02:03:31.299927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98902 (* 1 = 7.98902 loss)
I0523 02:03:31.307117 35003 sgd_solver.cpp:112] Iteration 117340, lr = 0.01
I0523 02:03:33.393211 35003 solver.cpp:239] Iteration 117350 (4.77739 iter/s, 2.0932s/10 iters), loss = 7.66863
I0523 02:03:33.393252 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66863 (* 1 = 7.66863 loss)
I0523 02:03:33.415868 35003 sgd_solver.cpp:112] Iteration 117350, lr = 0.01
I0523 02:03:36.015195 35003 solver.cpp:239] Iteration 117360 (3.81414 iter/s, 2.62182s/10 iters), loss = 6.19711
I0523 02:03:36.015238 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19711 (* 1 = 6.19711 loss)
I0523 02:03:36.028214 35003 sgd_solver.cpp:112] Iteration 117360, lr = 0.01
I0523 02:03:39.472378 35003 solver.cpp:239] Iteration 117370 (2.89268 iter/s, 3.457s/10 iters), loss = 7.12453
I0523 02:03:39.472419 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12453 (* 1 = 7.12453 loss)
I0523 02:03:39.485682 35003 sgd_solver.cpp:112] Iteration 117370, lr = 0.01
I0523 02:03:42.182256 35003 solver.cpp:239] Iteration 117380 (3.69042 iter/s, 2.70972s/10 iters), loss = 7.62978
I0523 02:03:42.182299 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62978 (* 1 = 7.62978 loss)
I0523 02:03:42.923658 35003 sgd_solver.cpp:112] Iteration 117380, lr = 0.01
I0523 02:03:45.583465 35003 solver.cpp:239] Iteration 117390 (2.94029 iter/s, 3.40102s/10 iters), loss = 6.97243
I0523 02:03:45.583510 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97243 (* 1 = 6.97243 loss)
I0523 02:03:46.318624 35003 sgd_solver.cpp:112] Iteration 117390, lr = 0.01
I0523 02:03:49.472595 35003 solver.cpp:239] Iteration 117400 (2.57141 iter/s, 3.88892s/10 iters), loss = 7.41191
I0523 02:03:49.472769 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41191 (* 1 = 7.41191 loss)
I0523 02:03:49.480548 35003 sgd_solver.cpp:112] Iteration 117400, lr = 0.01
I0523 02:03:52.315606 35003 solver.cpp:239] Iteration 117410 (3.51775 iter/s, 2.84272s/10 iters), loss = 6.94329
I0523 02:03:52.315652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94329 (* 1 = 6.94329 loss)
I0523 02:03:52.333051 35003 sgd_solver.cpp:112] Iteration 117410, lr = 0.01
I0523 02:03:56.683173 35003 solver.cpp:239] Iteration 117420 (2.28972 iter/s, 4.36734s/10 iters), loss = 6.49293
I0523 02:03:56.683225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49293 (* 1 = 6.49293 loss)
I0523 02:03:56.691015 35003 sgd_solver.cpp:112] Iteration 117420, lr = 0.01
I0523 02:03:59.612224 35003 solver.cpp:239] Iteration 117430 (3.41428 iter/s, 2.92887s/10 iters), loss = 7.48515
I0523 02:03:59.612290 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48515 (* 1 = 7.48515 loss)
I0523 02:03:59.624732 35003 sgd_solver.cpp:112] Iteration 117430, lr = 0.01
I0523 02:04:02.466419 35003 solver.cpp:239] Iteration 117440 (3.50386 iter/s, 2.854s/10 iters), loss = 7.29813
I0523 02:04:02.466482 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29813 (* 1 = 7.29813 loss)
I0523 02:04:03.180737 35003 sgd_solver.cpp:112] Iteration 117440, lr = 0.01
I0523 02:04:06.673012 35003 solver.cpp:239] Iteration 117450 (2.37736 iter/s, 4.20634s/10 iters), loss = 6.61861
I0523 02:04:06.673071 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61861 (* 1 = 6.61861 loss)
I0523 02:04:07.381005 35003 sgd_solver.cpp:112] Iteration 117450, lr = 0.01
I0523 02:04:11.767431 35003 solver.cpp:239] Iteration 117460 (1.96304 iter/s, 5.09415s/10 iters), loss = 7.92916
I0523 02:04:11.767473 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.92916 (* 1 = 7.92916 loss)
I0523 02:04:11.777995 35003 sgd_solver.cpp:112] Iteration 117460, lr = 0.01
I0523 02:04:14.284204 35003 solver.cpp:239] Iteration 117470 (3.97358 iter/s, 2.51662s/10 iters), loss = 6.62703
I0523 02:04:14.284246 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62703 (* 1 = 6.62703 loss)
I0523 02:04:14.285686 35003 sgd_solver.cpp:112] Iteration 117470, lr = 0.01
I0523 02:04:17.109163 35003 solver.cpp:239] Iteration 117480 (3.54008 iter/s, 2.82479s/10 iters), loss = 6.66074
I0523 02:04:17.109210 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66074 (* 1 = 6.66074 loss)
I0523 02:04:17.115391 35003 sgd_solver.cpp:112] Iteration 117480, lr = 0.01
I0523 02:04:19.787304 35003 solver.cpp:239] Iteration 117490 (3.73416 iter/s, 2.67798s/10 iters), loss = 6.85043
I0523 02:04:19.787547 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85043 (* 1 = 6.85043 loss)
I0523 02:04:19.789870 35003 sgd_solver.cpp:112] Iteration 117490, lr = 0.01
I0523 02:04:23.374454 35003 solver.cpp:239] Iteration 117500 (2.78803 iter/s, 3.58677s/10 iters), loss = 7.63304
I0523 02:04:23.374507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63304 (* 1 = 7.63304 loss)
I0523 02:04:23.391698 35003 sgd_solver.cpp:112] Iteration 117500, lr = 0.01
I0523 02:04:26.320513 35003 solver.cpp:239] Iteration 117510 (3.39457 iter/s, 2.94588s/10 iters), loss = 6.87525
I0523 02:04:26.320555 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87525 (* 1 = 6.87525 loss)
I0523 02:04:26.327033 35003 sgd_solver.cpp:112] Iteration 117510, lr = 0.01
I0523 02:04:30.825770 35003 solver.cpp:239] Iteration 117520 (2.21974 iter/s, 4.50503s/10 iters), loss = 8.06076
I0523 02:04:30.825820 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.06076 (* 1 = 8.06076 loss)
I0523 02:04:30.838805 35003 sgd_solver.cpp:112] Iteration 117520, lr = 0.01
I0523 02:04:33.968176 35003 solver.cpp:239] Iteration 117530 (3.18251 iter/s, 3.14217s/10 iters), loss = 7.83008
I0523 02:04:33.968230 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83008 (* 1 = 7.83008 loss)
I0523 02:04:34.708895 35003 sgd_solver.cpp:112] Iteration 117530, lr = 0.01
I0523 02:04:38.807173 35003 solver.cpp:239] Iteration 117540 (2.06665 iter/s, 4.83875s/10 iters), loss = 7.40608
I0523 02:04:38.807217 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40608 (* 1 = 7.40608 loss)
I0523 02:04:39.502420 35003 sgd_solver.cpp:112] Iteration 117540, lr = 0.01
I0523 02:04:42.343767 35003 solver.cpp:239] Iteration 117550 (2.82774 iter/s, 3.5364s/10 iters), loss = 6.37704
I0523 02:04:42.343824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37704 (* 1 = 6.37704 loss)
I0523 02:04:42.356621 35003 sgd_solver.cpp:112] Iteration 117550, lr = 0.01
I0523 02:04:45.470016 35003 solver.cpp:239] Iteration 117560 (3.19892 iter/s, 3.12606s/10 iters), loss = 6.94323
I0523 02:04:45.470060 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94323 (* 1 = 6.94323 loss)
I0523 02:04:45.478426 35003 sgd_solver.cpp:112] Iteration 117560, lr = 0.01
I0523 02:04:49.625102 35003 solver.cpp:239] Iteration 117570 (2.40681 iter/s, 4.15487s/10 iters), loss = 7.10635
I0523 02:04:49.625149 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10635 (* 1 = 7.10635 loss)
I0523 02:04:49.637559 35003 sgd_solver.cpp:112] Iteration 117570, lr = 0.01
I0523 02:04:51.720533 35003 solver.cpp:239] Iteration 117580 (4.7726 iter/s, 2.09529s/10 iters), loss = 6.97317
I0523 02:04:51.720793 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97317 (* 1 = 6.97317 loss)
I0523 02:04:52.436259 35003 sgd_solver.cpp:112] Iteration 117580, lr = 0.01
I0523 02:04:55.315585 35003 solver.cpp:239] Iteration 117590 (2.7819 iter/s, 3.59466s/10 iters), loss = 7.06879
I0523 02:04:55.315629 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06879 (* 1 = 7.06879 loss)
I0523 02:04:55.408772 35003 sgd_solver.cpp:112] Iteration 117590, lr = 0.01
I0523 02:04:59.706964 35003 solver.cpp:239] Iteration 117600 (2.27731 iter/s, 4.39115s/10 iters), loss = 6.94154
I0523 02:04:59.707016 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94154 (* 1 = 6.94154 loss)
I0523 02:04:59.830494 35003 sgd_solver.cpp:112] Iteration 117600, lr = 0.01
I0523 02:05:03.403493 35003 solver.cpp:239] Iteration 117610 (2.70539 iter/s, 3.69632s/10 iters), loss = 6.56967
I0523 02:05:03.403547 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56967 (* 1 = 6.56967 loss)
I0523 02:05:03.411973 35003 sgd_solver.cpp:112] Iteration 117610, lr = 0.01
I0523 02:05:05.940196 35003 solver.cpp:239] Iteration 117620 (3.94237 iter/s, 2.53654s/10 iters), loss = 6.50588
I0523 02:05:05.940238 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50588 (* 1 = 6.50588 loss)
I0523 02:05:06.631971 35003 sgd_solver.cpp:112] Iteration 117620, lr = 0.01
I0523 02:05:08.746403 35003 solver.cpp:239] Iteration 117630 (3.56373 iter/s, 2.80605s/10 iters), loss = 7.53956
I0523 02:05:08.746448 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53956 (* 1 = 7.53956 loss)
I0523 02:05:09.468508 35003 sgd_solver.cpp:112] Iteration 117630, lr = 0.01
I0523 02:05:13.792166 35003 solver.cpp:239] Iteration 117640 (1.98196 iter/s, 5.04552s/10 iters), loss = 7.3323
I0523 02:05:13.792215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3323 (* 1 = 7.3323 loss)
I0523 02:05:13.822775 35003 sgd_solver.cpp:112] Iteration 117640, lr = 0.01
I0523 02:05:15.895532 35003 solver.cpp:239] Iteration 117650 (4.7546 iter/s, 2.10322s/10 iters), loss = 6.68009
I0523 02:05:15.895588 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68009 (* 1 = 6.68009 loss)
I0523 02:05:16.604126 35003 sgd_solver.cpp:112] Iteration 117650, lr = 0.01
I0523 02:05:19.349519 35003 solver.cpp:239] Iteration 117660 (2.89537 iter/s, 3.45379s/10 iters), loss = 7.56242
I0523 02:05:19.349561 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56242 (* 1 = 7.56242 loss)
I0523 02:05:20.060348 35003 sgd_solver.cpp:112] Iteration 117660, lr = 0.01
I0523 02:05:23.679231 35003 solver.cpp:239] Iteration 117670 (2.30974 iter/s, 4.32949s/10 iters), loss = 6.80489
I0523 02:05:23.679473 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80489 (* 1 = 6.80489 loss)
I0523 02:05:23.692459 35003 sgd_solver.cpp:112] Iteration 117670, lr = 0.01
I0523 02:05:25.713176 35003 solver.cpp:239] Iteration 117680 (4.91735 iter/s, 2.03362s/10 iters), loss = 6.69358
I0523 02:05:25.713217 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69358 (* 1 = 6.69358 loss)
I0523 02:05:25.722092 35003 sgd_solver.cpp:112] Iteration 117680, lr = 0.01
I0523 02:05:30.835243 35003 solver.cpp:239] Iteration 117690 (1.95244 iter/s, 5.1218s/10 iters), loss = 7.44436
I0523 02:05:30.835321 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44436 (* 1 = 7.44436 loss)
I0523 02:05:30.862735 35003 sgd_solver.cpp:112] Iteration 117690, lr = 0.01
I0523 02:05:35.497973 35003 solver.cpp:239] Iteration 117700 (2.14479 iter/s, 4.66247s/10 iters), loss = 7.28522
I0523 02:05:35.498014 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28522 (* 1 = 7.28522 loss)
I0523 02:05:36.112892 35003 sgd_solver.cpp:112] Iteration 117700, lr = 0.01
I0523 02:05:38.896384 35003 solver.cpp:239] Iteration 117710 (2.94271 iter/s, 3.39822s/10 iters), loss = 7.49536
I0523 02:05:38.896441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49536 (* 1 = 7.49536 loss)
I0523 02:05:38.902186 35003 sgd_solver.cpp:112] Iteration 117710, lr = 0.01
I0523 02:05:41.683092 35003 solver.cpp:239] Iteration 117720 (3.58869 iter/s, 2.78653s/10 iters), loss = 6.98618
I0523 02:05:41.683145 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98618 (* 1 = 6.98618 loss)
I0523 02:05:41.694933 35003 sgd_solver.cpp:112] Iteration 117720, lr = 0.01
I0523 02:05:44.507100 35003 solver.cpp:239] Iteration 117730 (3.54128 iter/s, 2.82384s/10 iters), loss = 7.33469
I0523 02:05:44.507144 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33469 (* 1 = 7.33469 loss)
I0523 02:05:45.247943 35003 sgd_solver.cpp:112] Iteration 117730, lr = 0.01
I0523 02:05:48.810782 35003 solver.cpp:239] Iteration 117740 (2.32371 iter/s, 4.30346s/10 iters), loss = 6.29859
I0523 02:05:48.810837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29859 (* 1 = 6.29859 loss)
I0523 02:05:48.823679 35003 sgd_solver.cpp:112] Iteration 117740, lr = 0.01
I0523 02:05:51.712237 35003 solver.cpp:239] Iteration 117750 (3.44676 iter/s, 2.90128s/10 iters), loss = 7.67895
I0523 02:05:51.712282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67895 (* 1 = 7.67895 loss)
I0523 02:05:51.718907 35003 sgd_solver.cpp:112] Iteration 117750, lr = 0.01
I0523 02:05:55.586169 35003 solver.cpp:239] Iteration 117760 (2.58151 iter/s, 3.87371s/10 iters), loss = 7.59885
I0523 02:05:55.586318 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59885 (* 1 = 7.59885 loss)
I0523 02:05:55.604262 35003 sgd_solver.cpp:112] Iteration 117760, lr = 0.01
I0523 02:06:00.992259 35003 solver.cpp:239] Iteration 117770 (1.84991 iter/s, 5.40567s/10 iters), loss = 6.22109
I0523 02:06:00.992316 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22109 (* 1 = 6.22109 loss)
I0523 02:06:01.005712 35003 sgd_solver.cpp:112] Iteration 117770, lr = 0.01
I0523 02:06:03.073048 35003 solver.cpp:239] Iteration 117780 (4.8062 iter/s, 2.08064s/10 iters), loss = 6.09082
I0523 02:06:03.073084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09082 (* 1 = 6.09082 loss)
I0523 02:06:03.085453 35003 sgd_solver.cpp:112] Iteration 117780, lr = 0.01
I0523 02:06:05.679970 35003 solver.cpp:239] Iteration 117790 (3.83617 iter/s, 2.60677s/10 iters), loss = 7.29843
I0523 02:06:05.680018 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29843 (* 1 = 7.29843 loss)
I0523 02:06:05.693511 35003 sgd_solver.cpp:112] Iteration 117790, lr = 0.01
I0523 02:06:09.306501 35003 solver.cpp:239] Iteration 117800 (2.75761 iter/s, 3.62633s/10 iters), loss = 6.38673
I0523 02:06:09.306551 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38673 (* 1 = 6.38673 loss)
I0523 02:06:09.319555 35003 sgd_solver.cpp:112] Iteration 117800, lr = 0.01
I0523 02:06:12.084054 35003 solver.cpp:239] Iteration 117810 (3.60051 iter/s, 2.77739s/10 iters), loss = 6.9731
I0523 02:06:12.084102 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9731 (* 1 = 6.9731 loss)
I0523 02:06:12.749702 35003 sgd_solver.cpp:112] Iteration 117810, lr = 0.01
I0523 02:06:16.424625 35003 solver.cpp:239] Iteration 117820 (2.30397 iter/s, 4.34034s/10 iters), loss = 7.32485
I0523 02:06:16.424674 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32485 (* 1 = 7.32485 loss)
I0523 02:06:17.142222 35003 sgd_solver.cpp:112] Iteration 117820, lr = 0.01
I0523 02:06:21.388769 35003 solver.cpp:239] Iteration 117830 (2.01455 iter/s, 4.96389s/10 iters), loss = 7.88029
I0523 02:06:21.388818 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88029 (* 1 = 7.88029 loss)
I0523 02:06:21.402515 35003 sgd_solver.cpp:112] Iteration 117830, lr = 0.01
I0523 02:06:24.697594 35003 solver.cpp:239] Iteration 117840 (3.02239 iter/s, 3.30864s/10 iters), loss = 7.71887
I0523 02:06:24.697644 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71887 (* 1 = 7.71887 loss)
I0523 02:06:24.705597 35003 sgd_solver.cpp:112] Iteration 117840, lr = 0.01
I0523 02:06:27.480113 35003 solver.cpp:239] Iteration 117850 (3.5941 iter/s, 2.78234s/10 iters), loss = 7.13923
I0523 02:06:27.480445 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13923 (* 1 = 7.13923 loss)
I0523 02:06:27.492121 35003 sgd_solver.cpp:112] Iteration 117850, lr = 0.01
I0523 02:06:29.980129 35003 solver.cpp:239] Iteration 117860 (4.00063 iter/s, 2.49961s/10 iters), loss = 6.9715
I0523 02:06:29.980181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9715 (* 1 = 6.9715 loss)
I0523 02:06:29.991884 35003 sgd_solver.cpp:112] Iteration 117860, lr = 0.01
I0523 02:06:34.455162 35003 solver.cpp:239] Iteration 117870 (2.23474 iter/s, 4.47479s/10 iters), loss = 7.61943
I0523 02:06:34.455202 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61943 (* 1 = 7.61943 loss)
I0523 02:06:34.458675 35003 sgd_solver.cpp:112] Iteration 117870, lr = 0.01
I0523 02:06:37.372182 35003 solver.cpp:239] Iteration 117880 (3.42836 iter/s, 2.91685s/10 iters), loss = 6.48493
I0523 02:06:37.372232 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48493 (* 1 = 6.48493 loss)
I0523 02:06:37.386133 35003 sgd_solver.cpp:112] Iteration 117880, lr = 0.01
I0523 02:06:41.020386 35003 solver.cpp:239] Iteration 117890 (2.74123 iter/s, 3.648s/10 iters), loss = 6.70892
I0523 02:06:41.020437 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70892 (* 1 = 6.70892 loss)
I0523 02:06:41.754173 35003 sgd_solver.cpp:112] Iteration 117890, lr = 0.01
I0523 02:06:45.220953 35003 solver.cpp:239] Iteration 117900 (2.38076 iter/s, 4.20034s/10 iters), loss = 6.64198
I0523 02:06:45.221005 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64198 (* 1 = 6.64198 loss)
I0523 02:06:45.229120 35003 sgd_solver.cpp:112] Iteration 117900, lr = 0.01
I0523 02:06:48.009284 35003 solver.cpp:239] Iteration 117910 (3.58659 iter/s, 2.78816s/10 iters), loss = 7.11176
I0523 02:06:48.009325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11176 (* 1 = 7.11176 loss)
I0523 02:06:48.038327 35003 sgd_solver.cpp:112] Iteration 117910, lr = 0.01
I0523 02:06:51.652969 35003 solver.cpp:239] Iteration 117920 (2.74462 iter/s, 3.64349s/10 iters), loss = 7.41085
I0523 02:06:51.653025 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41085 (* 1 = 7.41085 loss)
I0523 02:06:51.664647 35003 sgd_solver.cpp:112] Iteration 117920, lr = 0.01
I0523 02:06:55.237768 35003 solver.cpp:239] Iteration 117930 (2.78972 iter/s, 3.58459s/10 iters), loss = 7.28096
I0523 02:06:55.237817 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28096 (* 1 = 7.28096 loss)
I0523 02:06:55.243424 35003 sgd_solver.cpp:112] Iteration 117930, lr = 0.01
I0523 02:06:58.155747 35003 solver.cpp:239] Iteration 117940 (3.42723 iter/s, 2.91781s/10 iters), loss = 7.00552
I0523 02:06:58.155947 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00552 (* 1 = 7.00552 loss)
I0523 02:06:58.864738 35003 sgd_solver.cpp:112] Iteration 117940, lr = 0.01
I0523 02:07:00.935005 35003 solver.cpp:239] Iteration 117950 (3.59849 iter/s, 2.77894s/10 iters), loss = 7.86785
I0523 02:07:00.935046 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86785 (* 1 = 7.86785 loss)
I0523 02:07:00.953397 35003 sgd_solver.cpp:112] Iteration 117950, lr = 0.01
I0523 02:07:04.477717 35003 solver.cpp:239] Iteration 117960 (2.82285 iter/s, 3.54252s/10 iters), loss = 6.56528
I0523 02:07:04.477774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56528 (* 1 = 6.56528 loss)
I0523 02:07:05.218561 35003 sgd_solver.cpp:112] Iteration 117960, lr = 0.01
I0523 02:07:08.652714 35003 solver.cpp:239] Iteration 117970 (2.39534 iter/s, 4.17477s/10 iters), loss = 6.76948
I0523 02:07:08.652771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76948 (* 1 = 6.76948 loss)
I0523 02:07:09.393479 35003 sgd_solver.cpp:112] Iteration 117970, lr = 0.01
I0523 02:07:12.182466 35003 solver.cpp:239] Iteration 117980 (2.83323 iter/s, 3.52954s/10 iters), loss = 6.67558
I0523 02:07:12.182523 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67558 (* 1 = 6.67558 loss)
I0523 02:07:12.195809 35003 sgd_solver.cpp:112] Iteration 117980, lr = 0.01
I0523 02:07:15.443694 35003 solver.cpp:239] Iteration 117990 (3.06651 iter/s, 3.26104s/10 iters), loss = 7.3697
I0523 02:07:15.443739 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3697 (* 1 = 7.3697 loss)
I0523 02:07:15.456524 35003 sgd_solver.cpp:112] Iteration 117990, lr = 0.01
I0523 02:07:19.715060 35003 solver.cpp:239] Iteration 118000 (2.34129 iter/s, 4.27115s/10 iters), loss = 7.23311
I0523 02:07:19.715112 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23311 (* 1 = 7.23311 loss)
I0523 02:07:19.721000 35003 sgd_solver.cpp:112] Iteration 118000, lr = 0.01
I0523 02:07:21.769783 35003 solver.cpp:239] Iteration 118010 (4.8672 iter/s, 2.05457s/10 iters), loss = 7.51732
I0523 02:07:21.769824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51732 (* 1 = 7.51732 loss)
I0523 02:07:21.783008 35003 sgd_solver.cpp:112] Iteration 118010, lr = 0.01
I0523 02:07:26.994478 35003 solver.cpp:239] Iteration 118020 (1.91409 iter/s, 5.22442s/10 iters), loss = 5.33957
I0523 02:07:26.994535 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.33957 (* 1 = 5.33957 loss)
I0523 02:07:27.715582 35003 sgd_solver.cpp:112] Iteration 118020, lr = 0.01
I0523 02:07:31.147899 35003 solver.cpp:239] Iteration 118030 (2.40781 iter/s, 4.15316s/10 iters), loss = 8.71192
I0523 02:07:31.148165 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.71192 (* 1 = 8.71192 loss)
I0523 02:07:31.193913 35003 sgd_solver.cpp:112] Iteration 118030, lr = 0.01
I0523 02:07:34.099232 35003 solver.cpp:239] Iteration 118040 (3.38873 iter/s, 2.95096s/10 iters), loss = 8.17211
I0523 02:07:34.099287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17211 (* 1 = 8.17211 loss)
I0523 02:07:34.105541 35003 sgd_solver.cpp:112] Iteration 118040, lr = 0.01
I0523 02:07:35.644204 35003 solver.cpp:239] Iteration 118050 (6.47312 iter/s, 1.54485s/10 iters), loss = 6.60368
I0523 02:07:35.644243 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60368 (* 1 = 6.60368 loss)
I0523 02:07:35.649871 35003 sgd_solver.cpp:112] Iteration 118050, lr = 0.01
I0523 02:07:38.446835 35003 solver.cpp:239] Iteration 118060 (3.56828 iter/s, 2.80247s/10 iters), loss = 7.61987
I0523 02:07:38.446876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61987 (* 1 = 7.61987 loss)
I0523 02:07:38.668678 35003 sgd_solver.cpp:112] Iteration 118060, lr = 0.01
I0523 02:07:41.033715 35003 solver.cpp:239] Iteration 118070 (3.86589 iter/s, 2.58672s/10 iters), loss = 7.07719
I0523 02:07:41.033771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07719 (* 1 = 7.07719 loss)
I0523 02:07:41.774607 35003 sgd_solver.cpp:112] Iteration 118070, lr = 0.01
I0523 02:07:44.610368 35003 solver.cpp:239] Iteration 118080 (2.79607 iter/s, 3.57645s/10 iters), loss = 6.94779
I0523 02:07:44.610409 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94779 (* 1 = 6.94779 loss)
I0523 02:07:44.624384 35003 sgd_solver.cpp:112] Iteration 118080, lr = 0.01
I0523 02:07:48.257959 35003 solver.cpp:239] Iteration 118090 (2.74168 iter/s, 3.6474s/10 iters), loss = 7.32624
I0523 02:07:48.257995 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32624 (* 1 = 7.32624 loss)
I0523 02:07:48.966133 35003 sgd_solver.cpp:112] Iteration 118090, lr = 0.01
I0523 02:07:51.836455 35003 solver.cpp:239] Iteration 118100 (2.79462 iter/s, 3.57831s/10 iters), loss = 7.69699
I0523 02:07:51.836515 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69699 (* 1 = 7.69699 loss)
I0523 02:07:52.463563 35003 sgd_solver.cpp:112] Iteration 118100, lr = 0.01
I0523 02:07:57.510329 35003 solver.cpp:239] Iteration 118110 (1.76255 iter/s, 5.67359s/10 iters), loss = 6.42563
I0523 02:07:57.510382 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42563 (* 1 = 6.42563 loss)
I0523 02:07:58.219411 35003 sgd_solver.cpp:112] Iteration 118110, lr = 0.01
I0523 02:08:00.344880 35003 solver.cpp:239] Iteration 118120 (3.52811 iter/s, 2.83438s/10 iters), loss = 6.89045
I0523 02:08:00.344919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89045 (* 1 = 6.89045 loss)
I0523 02:08:01.055759 35003 sgd_solver.cpp:112] Iteration 118120, lr = 0.01
I0523 02:08:02.858891 35003 solver.cpp:239] Iteration 118130 (3.97795 iter/s, 2.51386s/10 iters), loss = 8.29088
I0523 02:08:02.859200 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.29088 (* 1 = 8.29088 loss)
I0523 02:08:02.866762 35003 sgd_solver.cpp:112] Iteration 118130, lr = 0.01
I0523 02:08:06.651279 35003 solver.cpp:239] Iteration 118140 (2.63716 iter/s, 3.79195s/10 iters), loss = 7.6814
I0523 02:08:06.651324 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6814 (* 1 = 7.6814 loss)
I0523 02:08:06.657984 35003 sgd_solver.cpp:112] Iteration 118140, lr = 0.01
I0523 02:08:10.263290 35003 solver.cpp:239] Iteration 118150 (2.76869 iter/s, 3.61182s/10 iters), loss = 8.04277
I0523 02:08:10.263334 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04277 (* 1 = 8.04277 loss)
I0523 02:08:10.279754 35003 sgd_solver.cpp:112] Iteration 118150, lr = 0.01
I0523 02:08:14.389636 35003 solver.cpp:239] Iteration 118160 (2.42358 iter/s, 4.12613s/10 iters), loss = 6.73758
I0523 02:08:14.389684 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73758 (* 1 = 6.73758 loss)
I0523 02:08:14.409791 35003 sgd_solver.cpp:112] Iteration 118160, lr = 0.01
I0523 02:08:18.712465 35003 solver.cpp:239] Iteration 118170 (2.31342 iter/s, 4.3226s/10 iters), loss = 6.71673
I0523 02:08:18.712505 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71673 (* 1 = 6.71673 loss)
I0523 02:08:18.720654 35003 sgd_solver.cpp:112] Iteration 118170, lr = 0.01
I0523 02:08:23.345605 35003 solver.cpp:239] Iteration 118180 (2.15847 iter/s, 4.63291s/10 iters), loss = 7.19073
I0523 02:08:23.345654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19073 (* 1 = 7.19073 loss)
I0523 02:08:23.722702 35003 sgd_solver.cpp:112] Iteration 118180, lr = 0.01
I0523 02:08:26.584467 35003 solver.cpp:239] Iteration 118190 (3.08769 iter/s, 3.23867s/10 iters), loss = 6.25661
I0523 02:08:26.584532 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25661 (* 1 = 6.25661 loss)
I0523 02:08:27.319041 35003 sgd_solver.cpp:112] Iteration 118190, lr = 0.01
I0523 02:08:29.499661 35003 solver.cpp:239] Iteration 118200 (3.43055 iter/s, 2.91498s/10 iters), loss = 7.29085
I0523 02:08:29.499716 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29085 (* 1 = 7.29085 loss)
I0523 02:08:30.215186 35003 sgd_solver.cpp:112] Iteration 118200, lr = 0.01
I0523 02:08:34.616062 35003 solver.cpp:239] Iteration 118210 (1.9546 iter/s, 5.11613s/10 iters), loss = 7.8116
I0523 02:08:34.616226 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8116 (* 1 = 7.8116 loss)
I0523 02:08:34.628865 35003 sgd_solver.cpp:112] Iteration 118210, lr = 0.01
I0523 02:08:36.636307 35003 solver.cpp:239] Iteration 118220 (4.95051 iter/s, 2.01999s/10 iters), loss = 6.37611
I0523 02:08:36.636363 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37611 (* 1 = 6.37611 loss)
I0523 02:08:36.645292 35003 sgd_solver.cpp:112] Iteration 118220, lr = 0.01
I0523 02:08:40.900579 35003 solver.cpp:239] Iteration 118230 (2.34519 iter/s, 4.26404s/10 iters), loss = 6.20568
I0523 02:08:40.900624 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20568 (* 1 = 6.20568 loss)
I0523 02:08:40.913098 35003 sgd_solver.cpp:112] Iteration 118230, lr = 0.01
I0523 02:08:44.627655 35003 solver.cpp:239] Iteration 118240 (2.68321 iter/s, 3.72687s/10 iters), loss = 7.59064
I0523 02:08:44.627696 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59064 (* 1 = 7.59064 loss)
I0523 02:08:44.718881 35003 sgd_solver.cpp:112] Iteration 118240, lr = 0.01
I0523 02:08:47.547958 35003 solver.cpp:239] Iteration 118250 (3.4245 iter/s, 2.92013s/10 iters), loss = 7.87267
I0523 02:08:47.548028 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87267 (* 1 = 7.87267 loss)
I0523 02:08:47.559859 35003 sgd_solver.cpp:112] Iteration 118250, lr = 0.01
I0523 02:08:50.992593 35003 solver.cpp:239] Iteration 118260 (2.90325 iter/s, 3.44442s/10 iters), loss = 7.33246
I0523 02:08:50.992645 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33246 (* 1 = 7.33246 loss)
I0523 02:08:51.093163 35003 sgd_solver.cpp:112] Iteration 118260, lr = 0.01
I0523 02:08:54.746379 35003 solver.cpp:239] Iteration 118270 (2.66412 iter/s, 3.75358s/10 iters), loss = 6.71952
I0523 02:08:54.746420 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71952 (* 1 = 6.71952 loss)
I0523 02:08:55.484323 35003 sgd_solver.cpp:112] Iteration 118270, lr = 0.01
I0523 02:08:58.225373 35003 solver.cpp:239] Iteration 118280 (2.87455 iter/s, 3.47881s/10 iters), loss = 7.93093
I0523 02:08:58.225412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93093 (* 1 = 7.93093 loss)
I0523 02:08:58.232149 35003 sgd_solver.cpp:112] Iteration 118280, lr = 0.01
I0523 02:09:00.348589 35003 solver.cpp:239] Iteration 118290 (4.71016 iter/s, 2.12307s/10 iters), loss = 7.05874
I0523 02:09:00.348660 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05874 (* 1 = 7.05874 loss)
I0523 02:09:01.063423 35003 sgd_solver.cpp:112] Iteration 118290, lr = 0.01
I0523 02:09:04.078985 35003 solver.cpp:239] Iteration 118300 (2.68084 iter/s, 3.73017s/10 iters), loss = 6.34747
I0523 02:09:04.079027 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34747 (* 1 = 6.34747 loss)
I0523 02:09:04.102296 35003 sgd_solver.cpp:112] Iteration 118300, lr = 0.01
I0523 02:09:06.135267 35003 solver.cpp:239] Iteration 118310 (4.86354 iter/s, 2.05611s/10 iters), loss = 6.70789
I0523 02:09:06.135396 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70789 (* 1 = 6.70789 loss)
I0523 02:09:06.869598 35003 sgd_solver.cpp:112] Iteration 118310, lr = 0.01
I0523 02:09:10.021419 35003 solver.cpp:239] Iteration 118320 (2.57343 iter/s, 3.88587s/10 iters), loss = 7.73275
I0523 02:09:10.021463 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73275 (* 1 = 7.73275 loss)
I0523 02:09:10.750124 35003 sgd_solver.cpp:112] Iteration 118320, lr = 0.01
I0523 02:09:13.042173 35003 solver.cpp:239] Iteration 118330 (3.31063 iter/s, 3.02058s/10 iters), loss = 7.66736
I0523 02:09:13.042222 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66736 (* 1 = 7.66736 loss)
I0523 02:09:13.053083 35003 sgd_solver.cpp:112] Iteration 118330, lr = 0.01
I0523 02:09:16.722293 35003 solver.cpp:239] Iteration 118340 (2.71745 iter/s, 3.67992s/10 iters), loss = 6.68152
I0523 02:09:16.722335 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68152 (* 1 = 6.68152 loss)
I0523 02:09:16.729773 35003 sgd_solver.cpp:112] Iteration 118340, lr = 0.01
I0523 02:09:20.170472 35003 solver.cpp:239] Iteration 118350 (2.90024 iter/s, 3.44799s/10 iters), loss = 7.60685
I0523 02:09:20.170516 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60685 (* 1 = 7.60685 loss)
I0523 02:09:20.810204 35003 sgd_solver.cpp:112] Iteration 118350, lr = 0.01
I0523 02:09:25.202642 35003 solver.cpp:239] Iteration 118360 (1.98731 iter/s, 5.03192s/10 iters), loss = 6.36976
I0523 02:09:25.202714 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36976 (* 1 = 6.36976 loss)
I0523 02:09:25.205039 35003 sgd_solver.cpp:112] Iteration 118360, lr = 0.01
I0523 02:09:29.365751 35003 solver.cpp:239] Iteration 118370 (2.40218 iter/s, 4.16288s/10 iters), loss = 6.23495
I0523 02:09:29.365808 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23495 (* 1 = 6.23495 loss)
I0523 02:09:30.106117 35003 sgd_solver.cpp:112] Iteration 118370, lr = 0.01
I0523 02:09:33.671365 35003 solver.cpp:239] Iteration 118380 (2.32268 iter/s, 4.30537s/10 iters), loss = 6.65627
I0523 02:09:33.671428 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65627 (* 1 = 6.65627 loss)
I0523 02:09:34.405268 35003 sgd_solver.cpp:112] Iteration 118380, lr = 0.01
I0523 02:09:36.664868 35003 solver.cpp:239] Iteration 118390 (3.34078 iter/s, 2.99331s/10 iters), loss = 6.84234
I0523 02:09:36.665062 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84234 (* 1 = 6.84234 loss)
I0523 02:09:36.684093 35003 sgd_solver.cpp:112] Iteration 118390, lr = 0.01
I0523 02:09:40.295212 35003 solver.cpp:239] Iteration 118400 (2.7548 iter/s, 3.63002s/10 iters), loss = 8.82795
I0523 02:09:40.295253 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.82795 (* 1 = 8.82795 loss)
I0523 02:09:40.301690 35003 sgd_solver.cpp:112] Iteration 118400, lr = 0.01
I0523 02:09:44.671869 35003 solver.cpp:239] Iteration 118410 (2.28497 iter/s, 4.37643s/10 iters), loss = 7.89903
I0523 02:09:44.671913 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89903 (* 1 = 7.89903 loss)
I0523 02:09:44.676574 35003 sgd_solver.cpp:112] Iteration 118410, lr = 0.01
I0523 02:09:47.532512 35003 solver.cpp:239] Iteration 118420 (3.49592 iter/s, 2.86048s/10 iters), loss = 7.63554
I0523 02:09:47.532559 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63554 (* 1 = 7.63554 loss)
I0523 02:09:47.547269 35003 sgd_solver.cpp:112] Iteration 118420, lr = 0.01
I0523 02:09:51.074292 35003 solver.cpp:239] Iteration 118430 (2.8236 iter/s, 3.54158s/10 iters), loss = 6.67707
I0523 02:09:51.074340 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67707 (* 1 = 6.67707 loss)
I0523 02:09:51.077248 35003 sgd_solver.cpp:112] Iteration 118430, lr = 0.01
I0523 02:09:55.429785 35003 solver.cpp:239] Iteration 118440 (2.29607 iter/s, 4.35526s/10 iters), loss = 7.17566
I0523 02:09:55.429833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17566 (* 1 = 7.17566 loss)
I0523 02:09:55.456209 35003 sgd_solver.cpp:112] Iteration 118440, lr = 0.01
I0523 02:09:57.448957 35003 solver.cpp:239] Iteration 118450 (4.95286 iter/s, 2.01903s/10 iters), loss = 7.76987
I0523 02:09:57.448997 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76987 (* 1 = 7.76987 loss)
I0523 02:09:57.472640 35003 sgd_solver.cpp:112] Iteration 118450, lr = 0.01
I0523 02:10:00.895190 35003 solver.cpp:239] Iteration 118460 (2.90188 iter/s, 3.44604s/10 iters), loss = 6.65676
I0523 02:10:00.895242 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65676 (* 1 = 6.65676 loss)
I0523 02:10:01.536911 35003 sgd_solver.cpp:112] Iteration 118460, lr = 0.01
I0523 02:10:05.126857 35003 solver.cpp:239] Iteration 118470 (2.36326 iter/s, 4.23144s/10 iters), loss = 7.6852
I0523 02:10:05.126902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6852 (* 1 = 7.6852 loss)
I0523 02:10:05.134063 35003 sgd_solver.cpp:112] Iteration 118470, lr = 0.01
I0523 02:10:07.286072 35003 solver.cpp:239] Iteration 118480 (4.63163 iter/s, 2.15907s/10 iters), loss = 5.97713
I0523 02:10:07.286254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97713 (* 1 = 5.97713 loss)
I0523 02:10:07.303932 35003 sgd_solver.cpp:112] Iteration 118480, lr = 0.01
I0523 02:10:10.822567 35003 solver.cpp:239] Iteration 118490 (2.83142 iter/s, 3.5318s/10 iters), loss = 7.02308
I0523 02:10:10.822616 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02308 (* 1 = 7.02308 loss)
I0523 02:10:10.827615 35003 sgd_solver.cpp:112] Iteration 118490, lr = 0.01
I0523 02:10:13.756351 35003 solver.cpp:239] Iteration 118500 (3.40877 iter/s, 2.93361s/10 iters), loss = 6.62846
I0523 02:10:13.756400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62846 (* 1 = 6.62846 loss)
I0523 02:10:14.187530 35003 sgd_solver.cpp:112] Iteration 118500, lr = 0.01
I0523 02:10:19.150360 35003 solver.cpp:239] Iteration 118510 (1.854 iter/s, 5.39374s/10 iters), loss = 6.9901
I0523 02:10:19.150400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9901 (* 1 = 6.9901 loss)
I0523 02:10:19.162961 35003 sgd_solver.cpp:112] Iteration 118510, lr = 0.01
I0523 02:10:23.420851 35003 solver.cpp:239] Iteration 118520 (2.34177 iter/s, 4.27028s/10 iters), loss = 6.59932
I0523 02:10:23.420891 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59932 (* 1 = 6.59932 loss)
I0523 02:10:23.433693 35003 sgd_solver.cpp:112] Iteration 118520, lr = 0.01
I0523 02:10:26.309689 35003 solver.cpp:239] Iteration 118530 (3.4618 iter/s, 2.88867s/10 iters), loss = 8.03019
I0523 02:10:26.309736 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03019 (* 1 = 8.03019 loss)
I0523 02:10:27.030469 35003 sgd_solver.cpp:112] Iteration 118530, lr = 0.01
I0523 02:10:31.396464 35003 solver.cpp:239] Iteration 118540 (1.96598 iter/s, 5.08653s/10 iters), loss = 7.52977
I0523 02:10:31.396502 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52977 (* 1 = 7.52977 loss)
I0523 02:10:31.413372 35003 sgd_solver.cpp:112] Iteration 118540, lr = 0.01
I0523 02:10:35.539670 35003 solver.cpp:239] Iteration 118550 (2.41372 iter/s, 4.14298s/10 iters), loss = 7.35463
I0523 02:10:35.539728 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35463 (* 1 = 7.35463 loss)
I0523 02:10:36.280493 35003 sgd_solver.cpp:112] Iteration 118550, lr = 0.01
I0523 02:10:39.192490 35003 solver.cpp:239] Iteration 118560 (2.73777 iter/s, 3.65261s/10 iters), loss = 6.31655
I0523 02:10:39.192711 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31655 (* 1 = 6.31655 loss)
I0523 02:10:39.901335 35003 sgd_solver.cpp:112] Iteration 118560, lr = 0.01
I0523 02:10:43.447492 35003 solver.cpp:239] Iteration 118570 (2.35038 iter/s, 4.25463s/10 iters), loss = 7.2599
I0523 02:10:43.447531 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2599 (* 1 = 7.2599 loss)
I0523 02:10:43.460750 35003 sgd_solver.cpp:112] Iteration 118570, lr = 0.01
I0523 02:10:47.370134 35003 solver.cpp:239] Iteration 118580 (2.54943 iter/s, 3.92244s/10 iters), loss = 7.59432
I0523 02:10:47.370172 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59432 (* 1 = 7.59432 loss)
I0523 02:10:47.383440 35003 sgd_solver.cpp:112] Iteration 118580, lr = 0.01
I0523 02:10:50.123952 35003 solver.cpp:239] Iteration 118590 (3.63153 iter/s, 2.75366s/10 iters), loss = 6.56341
I0523 02:10:50.124013 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56341 (* 1 = 6.56341 loss)
I0523 02:10:50.136164 35003 sgd_solver.cpp:112] Iteration 118590, lr = 0.01
I0523 02:10:52.983253 35003 solver.cpp:239] Iteration 118600 (3.49758 iter/s, 2.85912s/10 iters), loss = 7.08602
I0523 02:10:52.983289 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08602 (* 1 = 7.08602 loss)
I0523 02:10:53.001251 35003 sgd_solver.cpp:112] Iteration 118600, lr = 0.01
I0523 02:10:56.573019 35003 solver.cpp:239] Iteration 118610 (2.78584 iter/s, 3.58958s/10 iters), loss = 6.82434
I0523 02:10:56.573063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82434 (* 1 = 6.82434 loss)
I0523 02:10:56.600702 35003 sgd_solver.cpp:112] Iteration 118610, lr = 0.01
I0523 02:11:00.947156 35003 solver.cpp:239] Iteration 118620 (2.28628 iter/s, 4.37391s/10 iters), loss = 7.72728
I0523 02:11:00.947206 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72728 (* 1 = 7.72728 loss)
I0523 02:11:00.958663 35003 sgd_solver.cpp:112] Iteration 118620, lr = 0.01
I0523 02:11:04.493605 35003 solver.cpp:239] Iteration 118630 (2.81988 iter/s, 3.54625s/10 iters), loss = 7.72121
I0523 02:11:04.493652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72121 (* 1 = 7.72121 loss)
I0523 02:11:04.497164 35003 sgd_solver.cpp:112] Iteration 118630, lr = 0.01
I0523 02:11:07.995492 35003 solver.cpp:239] Iteration 118640 (2.85577 iter/s, 3.50169s/10 iters), loss = 6.63834
I0523 02:11:07.995545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63834 (* 1 = 6.63834 loss)
I0523 02:11:08.008417 35003 sgd_solver.cpp:112] Iteration 118640, lr = 0.01
I0523 02:11:11.624369 35003 solver.cpp:239] Iteration 118650 (2.75583 iter/s, 3.62867s/10 iters), loss = 6.40678
I0523 02:11:11.624620 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40678 (* 1 = 6.40678 loss)
I0523 02:11:11.636873 35003 sgd_solver.cpp:112] Iteration 118650, lr = 0.01
I0523 02:11:15.104121 35003 solver.cpp:239] Iteration 118660 (2.87408 iter/s, 3.47938s/10 iters), loss = 7.27353
I0523 02:11:15.104168 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27353 (* 1 = 7.27353 loss)
I0523 02:11:15.116772 35003 sgd_solver.cpp:112] Iteration 118660, lr = 0.01
I0523 02:11:19.271873 35003 solver.cpp:239] Iteration 118670 (2.3995 iter/s, 4.16753s/10 iters), loss = 8.01711
I0523 02:11:19.271920 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01711 (* 1 = 8.01711 loss)
I0523 02:11:19.285262 35003 sgd_solver.cpp:112] Iteration 118670, lr = 0.01
I0523 02:11:22.135936 35003 solver.cpp:239] Iteration 118680 (3.49175 iter/s, 2.8639s/10 iters), loss = 6.7281
I0523 02:11:22.135982 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7281 (* 1 = 6.7281 loss)
I0523 02:11:22.813918 35003 sgd_solver.cpp:112] Iteration 118680, lr = 0.01
I0523 02:11:25.807286 35003 solver.cpp:239] Iteration 118690 (2.72395 iter/s, 3.67114s/10 iters), loss = 7.15737
I0523 02:11:25.807337 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15737 (* 1 = 7.15737 loss)
I0523 02:11:25.814641 35003 sgd_solver.cpp:112] Iteration 118690, lr = 0.01
I0523 02:11:28.610399 35003 solver.cpp:239] Iteration 118700 (3.56771 iter/s, 2.80292s/10 iters), loss = 6.43105
I0523 02:11:28.610453 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43105 (* 1 = 6.43105 loss)
I0523 02:11:28.622746 35003 sgd_solver.cpp:112] Iteration 118700, lr = 0.01
I0523 02:11:32.661175 35003 solver.cpp:239] Iteration 118710 (2.46879 iter/s, 4.05056s/10 iters), loss = 8.03641
I0523 02:11:32.661221 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03641 (* 1 = 8.03641 loss)
I0523 02:11:32.722942 35003 sgd_solver.cpp:112] Iteration 118710, lr = 0.01
I0523 02:11:35.670143 35003 solver.cpp:239] Iteration 118720 (3.32359 iter/s, 3.0088s/10 iters), loss = 6.56759
I0523 02:11:35.670187 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56759 (* 1 = 6.56759 loss)
I0523 02:11:35.677569 35003 sgd_solver.cpp:112] Iteration 118720, lr = 0.01
I0523 02:11:38.627035 35003 solver.cpp:239] Iteration 118730 (3.38213 iter/s, 2.95672s/10 iters), loss = 7.41634
I0523 02:11:38.627089 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41634 (* 1 = 7.41634 loss)
I0523 02:11:39.368032 35003 sgd_solver.cpp:112] Iteration 118730, lr = 0.01
I0523 02:11:42.230180 35003 solver.cpp:239] Iteration 118740 (2.77552 iter/s, 3.60293s/10 iters), loss = 6.4227
I0523 02:11:42.230396 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4227 (* 1 = 6.4227 loss)
I0523 02:11:42.971065 35003 sgd_solver.cpp:112] Iteration 118740, lr = 0.01
I0523 02:11:46.429226 35003 solver.cpp:239] Iteration 118750 (2.38172 iter/s, 4.19865s/10 iters), loss = 7.95207
I0523 02:11:46.429280 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95207 (* 1 = 7.95207 loss)
I0523 02:11:46.440718 35003 sgd_solver.cpp:112] Iteration 118750, lr = 0.01
I0523 02:11:48.564107 35003 solver.cpp:239] Iteration 118760 (4.68442 iter/s, 2.13474s/10 iters), loss = 7.60243
I0523 02:11:48.564152 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60243 (* 1 = 7.60243 loss)
I0523 02:11:49.242600 35003 sgd_solver.cpp:112] Iteration 118760, lr = 0.01
I0523 02:11:52.025354 35003 solver.cpp:239] Iteration 118770 (2.88929 iter/s, 3.46106s/10 iters), loss = 6.81449
I0523 02:11:52.025400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81449 (* 1 = 6.81449 loss)
I0523 02:11:52.036092 35003 sgd_solver.cpp:112] Iteration 118770, lr = 0.01
I0523 02:11:54.800396 35003 solver.cpp:239] Iteration 118780 (3.60377 iter/s, 2.77487s/10 iters), loss = 6.99045
I0523 02:11:54.800451 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99045 (* 1 = 6.99045 loss)
I0523 02:11:54.831239 35003 sgd_solver.cpp:112] Iteration 118780, lr = 0.01
I0523 02:11:58.911504 35003 solver.cpp:239] Iteration 118790 (2.43257 iter/s, 4.11087s/10 iters), loss = 8.43734
I0523 02:11:58.911556 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.43734 (* 1 = 8.43734 loss)
I0523 02:11:59.626976 35003 sgd_solver.cpp:112] Iteration 118790, lr = 0.01
I0523 02:12:03.411129 35003 solver.cpp:239] Iteration 118800 (2.22253 iter/s, 4.49939s/10 iters), loss = 6.89758
I0523 02:12:03.411175 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89758 (* 1 = 6.89758 loss)
I0523 02:12:03.417696 35003 sgd_solver.cpp:112] Iteration 118800, lr = 0.01
I0523 02:12:07.127462 35003 solver.cpp:239] Iteration 118810 (2.69097 iter/s, 3.71614s/10 iters), loss = 6.51447
I0523 02:12:07.127504 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51447 (* 1 = 6.51447 loss)
I0523 02:12:07.658073 35003 sgd_solver.cpp:112] Iteration 118810, lr = 0.01
I0523 02:12:09.794910 35003 solver.cpp:239] Iteration 118820 (3.74912 iter/s, 2.66729s/10 iters), loss = 6.64903
I0523 02:12:09.794955 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64903 (* 1 = 6.64903 loss)
I0523 02:12:10.506681 35003 sgd_solver.cpp:112] Iteration 118820, lr = 0.01
I0523 02:12:13.245153 35003 solver.cpp:239] Iteration 118830 (2.89853 iter/s, 3.45003s/10 iters), loss = 7.11895
I0523 02:12:13.245333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11895 (* 1 = 7.11895 loss)
I0523 02:12:13.258474 35003 sgd_solver.cpp:112] Iteration 118830, lr = 0.01
I0523 02:12:16.802567 35003 solver.cpp:239] Iteration 118840 (2.81129 iter/s, 3.55709s/10 iters), loss = 8.45384
I0523 02:12:16.802620 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.45384 (* 1 = 8.45384 loss)
I0523 02:12:17.524382 35003 sgd_solver.cpp:112] Iteration 118840, lr = 0.01
I0523 02:12:21.943192 35003 solver.cpp:239] Iteration 118850 (1.94539 iter/s, 5.14036s/10 iters), loss = 7.61819
I0523 02:12:21.943238 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61819 (* 1 = 7.61819 loss)
I0523 02:12:21.961958 35003 sgd_solver.cpp:112] Iteration 118850, lr = 0.01
I0523 02:12:25.605046 35003 solver.cpp:239] Iteration 118860 (2.73103 iter/s, 3.66163s/10 iters), loss = 7.09492
I0523 02:12:25.605098 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09492 (* 1 = 7.09492 loss)
I0523 02:12:26.346097 35003 sgd_solver.cpp:112] Iteration 118860, lr = 0.01
I0523 02:12:28.447954 35003 solver.cpp:239] Iteration 118870 (3.51774 iter/s, 2.84274s/10 iters), loss = 7.80176
I0523 02:12:28.448000 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80176 (* 1 = 7.80176 loss)
I0523 02:12:28.459931 35003 sgd_solver.cpp:112] Iteration 118870, lr = 0.01
I0523 02:12:31.651100 35003 solver.cpp:239] Iteration 118880 (3.12211 iter/s, 3.20297s/10 iters), loss = 6.59519
I0523 02:12:31.651150 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59519 (* 1 = 6.59519 loss)
I0523 02:12:31.656616 35003 sgd_solver.cpp:112] Iteration 118880, lr = 0.01
I0523 02:12:35.882078 35003 solver.cpp:239] Iteration 118890 (2.36365 iter/s, 4.23075s/10 iters), loss = 6.86405
I0523 02:12:35.882115 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86405 (* 1 = 6.86405 loss)
I0523 02:12:35.886705 35003 sgd_solver.cpp:112] Iteration 118890, lr = 0.01
I0523 02:12:39.524317 35003 solver.cpp:239] Iteration 118900 (2.74573 iter/s, 3.64202s/10 iters), loss = 7.22003
I0523 02:12:39.524370 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22003 (* 1 = 7.22003 loss)
I0523 02:12:39.967963 35003 sgd_solver.cpp:112] Iteration 118900, lr = 0.01
I0523 02:12:44.046269 35003 solver.cpp:239] Iteration 118910 (2.21156 iter/s, 4.5217s/10 iters), loss = 7.36265
I0523 02:12:44.046520 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36265 (* 1 = 7.36265 loss)
I0523 02:12:44.054545 35003 sgd_solver.cpp:112] Iteration 118910, lr = 0.01
I0523 02:12:46.920497 35003 solver.cpp:239] Iteration 118920 (3.47964 iter/s, 2.87386s/10 iters), loss = 6.78991
I0523 02:12:46.920544 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78991 (* 1 = 6.78991 loss)
I0523 02:12:47.661408 35003 sgd_solver.cpp:112] Iteration 118920, lr = 0.01
I0523 02:12:52.002764 35003 solver.cpp:239] Iteration 118930 (1.96775 iter/s, 5.08194s/10 iters), loss = 6.33353
I0523 02:12:52.002823 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33353 (* 1 = 6.33353 loss)
I0523 02:12:52.009711 35003 sgd_solver.cpp:112] Iteration 118930, lr = 0.01
I0523 02:12:54.777869 35003 solver.cpp:239] Iteration 118940 (3.60369 iter/s, 2.77493s/10 iters), loss = 7.79564
I0523 02:12:54.777907 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79564 (* 1 = 7.79564 loss)
I0523 02:12:54.789757 35003 sgd_solver.cpp:112] Iteration 118940, lr = 0.01
I0523 02:12:57.825780 35003 solver.cpp:239] Iteration 118950 (3.28111 iter/s, 3.04775s/10 iters), loss = 7.2406
I0523 02:12:57.825824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2406 (* 1 = 7.2406 loss)
I0523 02:12:58.495270 35003 sgd_solver.cpp:112] Iteration 118950, lr = 0.01
I0523 02:13:01.611918 35003 solver.cpp:239] Iteration 118960 (2.64137 iter/s, 3.78592s/10 iters), loss = 7.2349
I0523 02:13:01.611970 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2349 (* 1 = 7.2349 loss)
I0523 02:13:01.625190 35003 sgd_solver.cpp:112] Iteration 118960, lr = 0.01
I0523 02:13:03.611153 35003 solver.cpp:239] Iteration 118970 (5.00226 iter/s, 1.9991s/10 iters), loss = 6.92373
I0523 02:13:03.611196 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92373 (* 1 = 6.92373 loss)
I0523 02:13:03.814124 35003 sgd_solver.cpp:112] Iteration 118970, lr = 0.01
I0523 02:13:06.168378 35003 solver.cpp:239] Iteration 118980 (3.91074 iter/s, 2.55706s/10 iters), loss = 7.51572
I0523 02:13:06.168447 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51572 (* 1 = 7.51572 loss)
I0523 02:13:06.173734 35003 sgd_solver.cpp:112] Iteration 118980, lr = 0.01
I0523 02:13:10.568068 35003 solver.cpp:239] Iteration 118990 (2.27301 iter/s, 4.39945s/10 iters), loss = 6.45261
I0523 02:13:10.568104 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45261 (* 1 = 6.45261 loss)
I0523 02:13:10.581411 35003 sgd_solver.cpp:112] Iteration 118990, lr = 0.01
I0523 02:13:13.476224 35003 solver.cpp:239] Iteration 119000 (3.4388 iter/s, 2.90799s/10 iters), loss = 7.42539
I0523 02:13:13.476284 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42539 (* 1 = 7.42539 loss)
I0523 02:13:13.501425 35003 sgd_solver.cpp:112] Iteration 119000, lr = 0.01
I0523 02:13:16.249249 35003 solver.cpp:239] Iteration 119010 (3.60639 iter/s, 2.77285s/10 iters), loss = 7.47628
I0523 02:13:16.249538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47628 (* 1 = 7.47628 loss)
I0523 02:13:16.262327 35003 sgd_solver.cpp:112] Iteration 119010, lr = 0.01
I0523 02:13:19.931520 35003 solver.cpp:239] Iteration 119020 (2.71602 iter/s, 3.68185s/10 iters), loss = 6.46068
I0523 02:13:19.931569 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46068 (* 1 = 6.46068 loss)
I0523 02:13:20.633764 35003 sgd_solver.cpp:112] Iteration 119020, lr = 0.01
I0523 02:13:23.505085 35003 solver.cpp:239] Iteration 119030 (2.79848 iter/s, 3.57337s/10 iters), loss = 7.86374
I0523 02:13:23.505136 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86374 (* 1 = 7.86374 loss)
I0523 02:13:24.213145 35003 sgd_solver.cpp:112] Iteration 119030, lr = 0.01
I0523 02:13:26.892457 35003 solver.cpp:239] Iteration 119040 (2.95231 iter/s, 3.38718s/10 iters), loss = 6.66861
I0523 02:13:26.892513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66861 (* 1 = 6.66861 loss)
I0523 02:13:26.899202 35003 sgd_solver.cpp:112] Iteration 119040, lr = 0.01
I0523 02:13:29.314069 35003 solver.cpp:239] Iteration 119050 (4.12976 iter/s, 2.42145s/10 iters), loss = 7.51141
I0523 02:13:29.314121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51141 (* 1 = 7.51141 loss)
I0523 02:13:29.337977 35003 sgd_solver.cpp:112] Iteration 119050, lr = 0.01
I0523 02:13:31.714236 35003 solver.cpp:239] Iteration 119060 (4.16675 iter/s, 2.39995s/10 iters), loss = 6.78382
I0523 02:13:31.714304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78382 (* 1 = 6.78382 loss)
I0523 02:13:31.735653 35003 sgd_solver.cpp:112] Iteration 119060, lr = 0.01
I0523 02:13:35.317004 35003 solver.cpp:239] Iteration 119070 (2.77582 iter/s, 3.60254s/10 iters), loss = 7.17244
I0523 02:13:35.317073 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17244 (* 1 = 7.17244 loss)
I0523 02:13:35.318090 35003 sgd_solver.cpp:112] Iteration 119070, lr = 0.01
I0523 02:13:39.014577 35003 solver.cpp:239] Iteration 119080 (2.70464 iter/s, 3.69736s/10 iters), loss = 6.19762
I0523 02:13:39.014626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19762 (* 1 = 6.19762 loss)
I0523 02:13:39.395360 35003 sgd_solver.cpp:112] Iteration 119080, lr = 0.01
I0523 02:13:42.513908 35003 solver.cpp:239] Iteration 119090 (2.85785 iter/s, 3.49914s/10 iters), loss = 6.97142
I0523 02:13:42.513969 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97142 (* 1 = 6.97142 loss)
I0523 02:13:43.233358 35003 sgd_solver.cpp:112] Iteration 119090, lr = 0.01
I0523 02:13:46.121354 35003 solver.cpp:239] Iteration 119100 (2.77221 iter/s, 3.60723s/10 iters), loss = 6.72104
I0523 02:13:46.121398 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72104 (* 1 = 6.72104 loss)
I0523 02:13:46.127952 35003 sgd_solver.cpp:112] Iteration 119100, lr = 0.01
I0523 02:13:49.660295 35003 solver.cpp:239] Iteration 119110 (2.82586 iter/s, 3.53874s/10 iters), loss = 6.57783
I0523 02:13:49.660501 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57783 (* 1 = 6.57783 loss)
I0523 02:13:49.664013 35003 sgd_solver.cpp:112] Iteration 119110, lr = 0.01
I0523 02:13:53.227319 35003 solver.cpp:239] Iteration 119120 (2.80372 iter/s, 3.56669s/10 iters), loss = 7.11613
I0523 02:13:53.227367 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11613 (* 1 = 7.11613 loss)
I0523 02:13:53.231886 35003 sgd_solver.cpp:112] Iteration 119120, lr = 0.01
I0523 02:13:56.051554 35003 solver.cpp:239] Iteration 119130 (3.54099 iter/s, 2.82407s/10 iters), loss = 6.91777
I0523 02:13:56.051596 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91777 (* 1 = 6.91777 loss)
I0523 02:13:56.786602 35003 sgd_solver.cpp:112] Iteration 119130, lr = 0.01
I0523 02:14:00.413152 35003 solver.cpp:239] Iteration 119140 (2.29286 iter/s, 4.36136s/10 iters), loss = 6.64651
I0523 02:14:00.413198 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64651 (* 1 = 6.64651 loss)
I0523 02:14:00.420837 35003 sgd_solver.cpp:112] Iteration 119140, lr = 0.01
I0523 02:14:02.536299 35003 solver.cpp:239] Iteration 119150 (4.71032 iter/s, 2.123s/10 iters), loss = 6.90231
I0523 02:14:02.536339 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90231 (* 1 = 6.90231 loss)
I0523 02:14:02.540959 35003 sgd_solver.cpp:112] Iteration 119150, lr = 0.01
I0523 02:14:05.158732 35003 solver.cpp:239] Iteration 119160 (3.8135 iter/s, 2.62226s/10 iters), loss = 7.4064
I0523 02:14:05.158773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4064 (* 1 = 7.4064 loss)
I0523 02:14:05.177511 35003 sgd_solver.cpp:112] Iteration 119160, lr = 0.01
I0523 02:14:07.108271 35003 solver.cpp:239] Iteration 119170 (5.12976 iter/s, 1.94941s/10 iters), loss = 5.64849
I0523 02:14:07.108314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.64849 (* 1 = 5.64849 loss)
I0523 02:14:07.126354 35003 sgd_solver.cpp:112] Iteration 119170, lr = 0.01
I0523 02:14:09.880237 35003 solver.cpp:239] Iteration 119180 (3.60775 iter/s, 2.77181s/10 iters), loss = 6.69762
I0523 02:14:09.880280 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69762 (* 1 = 6.69762 loss)
I0523 02:14:10.601199 35003 sgd_solver.cpp:112] Iteration 119180, lr = 0.01
I0523 02:14:13.450395 35003 solver.cpp:239] Iteration 119190 (2.80114 iter/s, 3.56997s/10 iters), loss = 7.17454
I0523 02:14:13.450433 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17454 (* 1 = 7.17454 loss)
I0523 02:14:14.165802 35003 sgd_solver.cpp:112] Iteration 119190, lr = 0.01
I0523 02:14:17.839200 35003 solver.cpp:239] Iteration 119200 (2.27864 iter/s, 4.38857s/10 iters), loss = 6.19087
I0523 02:14:17.839241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19087 (* 1 = 6.19087 loss)
I0523 02:14:18.229799 35003 sgd_solver.cpp:112] Iteration 119200, lr = 0.01
I0523 02:14:21.139267 35003 solver.cpp:239] Iteration 119210 (3.03041 iter/s, 3.29988s/10 iters), loss = 6.9545
I0523 02:14:21.139541 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9545 (* 1 = 6.9545 loss)
I0523 02:14:21.152328 35003 sgd_solver.cpp:112] Iteration 119210, lr = 0.01
I0523 02:14:24.213382 35003 solver.cpp:239] Iteration 119220 (3.2534 iter/s, 3.07371s/10 iters), loss = 7.09251
I0523 02:14:24.213431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09251 (* 1 = 7.09251 loss)
I0523 02:14:24.226374 35003 sgd_solver.cpp:112] Iteration 119220, lr = 0.01
I0523 02:14:27.611385 35003 solver.cpp:239] Iteration 119230 (2.94307 iter/s, 3.39781s/10 iters), loss = 6.96032
I0523 02:14:27.611441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96032 (* 1 = 6.96032 loss)
I0523 02:14:27.621129 35003 sgd_solver.cpp:112] Iteration 119230, lr = 0.01
I0523 02:14:31.947291 35003 solver.cpp:239] Iteration 119240 (2.30645 iter/s, 4.33567s/10 iters), loss = 7.75246
I0523 02:14:31.947332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75246 (* 1 = 7.75246 loss)
I0523 02:14:31.965711 35003 sgd_solver.cpp:112] Iteration 119240, lr = 0.01
I0523 02:14:34.436120 35003 solver.cpp:239] Iteration 119250 (4.01819 iter/s, 2.48868s/10 iters), loss = 6.96024
I0523 02:14:34.436158 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96024 (* 1 = 6.96024 loss)
I0523 02:14:34.958324 35003 sgd_solver.cpp:112] Iteration 119250, lr = 0.01
I0523 02:14:37.985476 35003 solver.cpp:239] Iteration 119260 (2.81756 iter/s, 3.54917s/10 iters), loss = 7.98638
I0523 02:14:37.985536 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98638 (* 1 = 7.98638 loss)
I0523 02:14:38.104558 35003 sgd_solver.cpp:112] Iteration 119260, lr = 0.01
I0523 02:14:41.615294 35003 solver.cpp:239] Iteration 119270 (2.75512 iter/s, 3.62961s/10 iters), loss = 7.19616
I0523 02:14:41.615344 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19616 (* 1 = 7.19616 loss)
I0523 02:14:41.625313 35003 sgd_solver.cpp:112] Iteration 119270, lr = 0.01
I0523 02:14:46.190640 35003 solver.cpp:239] Iteration 119280 (2.18574 iter/s, 4.57511s/10 iters), loss = 6.46379
I0523 02:14:46.190687 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46379 (* 1 = 6.46379 loss)
I0523 02:14:46.198652 35003 sgd_solver.cpp:112] Iteration 119280, lr = 0.01
I0523 02:14:49.769878 35003 solver.cpp:239] Iteration 119290 (2.79404 iter/s, 3.57904s/10 iters), loss = 6.52841
I0523 02:14:49.769923 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52841 (* 1 = 6.52841 loss)
I0523 02:14:49.778583 35003 sgd_solver.cpp:112] Iteration 119290, lr = 0.01
I0523 02:14:54.887310 35003 solver.cpp:239] Iteration 119300 (1.9542 iter/s, 5.11718s/10 iters), loss = 6.31949
I0523 02:14:54.887743 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31949 (* 1 = 6.31949 loss)
I0523 02:14:54.900329 35003 sgd_solver.cpp:112] Iteration 119300, lr = 0.01
I0523 02:14:58.372615 35003 solver.cpp:239] Iteration 119310 (2.86964 iter/s, 3.48475s/10 iters), loss = 8.10188
I0523 02:14:58.372665 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.10188 (* 1 = 8.10188 loss)
I0523 02:14:59.093888 35003 sgd_solver.cpp:112] Iteration 119310, lr = 0.01
I0523 02:15:02.081746 35003 solver.cpp:239] Iteration 119320 (2.69619 iter/s, 3.70893s/10 iters), loss = 7.56423
I0523 02:15:02.081794 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56423 (* 1 = 7.56423 loss)
I0523 02:15:02.822580 35003 sgd_solver.cpp:112] Iteration 119320, lr = 0.01
I0523 02:15:06.333659 35003 solver.cpp:239] Iteration 119330 (2.35201 iter/s, 4.25168s/10 iters), loss = 6.58517
I0523 02:15:06.333714 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58517 (* 1 = 6.58517 loss)
I0523 02:15:06.337136 35003 sgd_solver.cpp:112] Iteration 119330, lr = 0.01
I0523 02:15:09.062304 35003 solver.cpp:239] Iteration 119340 (3.66509 iter/s, 2.72845s/10 iters), loss = 6.32636
I0523 02:15:09.062350 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32636 (* 1 = 6.32636 loss)
I0523 02:15:09.069561 35003 sgd_solver.cpp:112] Iteration 119340, lr = 0.01
I0523 02:15:11.157569 35003 solver.cpp:239] Iteration 119350 (4.77301 iter/s, 2.09512s/10 iters), loss = 7.06402
I0523 02:15:11.157613 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06402 (* 1 = 7.06402 loss)
I0523 02:15:11.167090 35003 sgd_solver.cpp:112] Iteration 119350, lr = 0.01
I0523 02:15:15.166961 35003 solver.cpp:239] Iteration 119360 (2.49427 iter/s, 4.00918s/10 iters), loss = 6.76324
I0523 02:15:15.167007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76324 (* 1 = 6.76324 loss)
I0523 02:15:15.173283 35003 sgd_solver.cpp:112] Iteration 119360, lr = 0.01
I0523 02:15:18.751574 35003 solver.cpp:239] Iteration 119370 (2.78985 iter/s, 3.58442s/10 iters), loss = 7.1458
I0523 02:15:18.751631 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1458 (* 1 = 7.1458 loss)
I0523 02:15:18.764928 35003 sgd_solver.cpp:112] Iteration 119370, lr = 0.01
I0523 02:15:24.108059 35003 solver.cpp:239] Iteration 119380 (1.86699 iter/s, 5.35621s/10 iters), loss = 8.01913
I0523 02:15:24.108109 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01913 (* 1 = 8.01913 loss)
I0523 02:15:24.143865 35003 sgd_solver.cpp:112] Iteration 119380, lr = 0.01
I0523 02:15:29.126087 35003 solver.cpp:239] Iteration 119390 (1.99292 iter/s, 5.01777s/10 iters), loss = 6.81771
I0523 02:15:29.126230 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81771 (* 1 = 6.81771 loss)
I0523 02:15:29.133893 35003 sgd_solver.cpp:112] Iteration 119390, lr = 0.01
I0523 02:15:31.954042 35003 solver.cpp:239] Iteration 119400 (3.53645 iter/s, 2.82769s/10 iters), loss = 7.17576
I0523 02:15:31.954098 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17576 (* 1 = 7.17576 loss)
I0523 02:15:31.968529 35003 sgd_solver.cpp:112] Iteration 119400, lr = 0.01
I0523 02:15:34.779321 35003 solver.cpp:239] Iteration 119410 (3.53969 iter/s, 2.82511s/10 iters), loss = 5.71595
I0523 02:15:34.779364 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.71595 (* 1 = 5.71595 loss)
I0523 02:15:34.792768 35003 sgd_solver.cpp:112] Iteration 119410, lr = 0.01
I0523 02:15:37.610757 35003 solver.cpp:239] Iteration 119420 (3.53199 iter/s, 2.83127s/10 iters), loss = 5.15123
I0523 02:15:37.610815 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.15123 (* 1 = 5.15123 loss)
I0523 02:15:38.351689 35003 sgd_solver.cpp:112] Iteration 119420, lr = 0.01
I0523 02:15:43.288954 35003 solver.cpp:239] Iteration 119430 (1.76121 iter/s, 5.67791s/10 iters), loss = 7.51924
I0523 02:15:43.289007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51924 (* 1 = 7.51924 loss)
I0523 02:15:43.330418 35003 sgd_solver.cpp:112] Iteration 119430, lr = 0.01
I0523 02:15:46.757987 35003 solver.cpp:239] Iteration 119440 (2.88281 iter/s, 3.46884s/10 iters), loss = 7.22148
I0523 02:15:46.758025 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22148 (* 1 = 7.22148 loss)
I0523 02:15:46.770855 35003 sgd_solver.cpp:112] Iteration 119440, lr = 0.01
I0523 02:15:48.871093 35003 solver.cpp:239] Iteration 119450 (4.73268 iter/s, 2.11297s/10 iters), loss = 6.82402
I0523 02:15:48.871152 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82402 (* 1 = 6.82402 loss)
I0523 02:15:49.507478 35003 sgd_solver.cpp:112] Iteration 119450, lr = 0.01
I0523 02:15:52.551872 35003 solver.cpp:239] Iteration 119460 (2.71697 iter/s, 3.68057s/10 iters), loss = 7.89786
I0523 02:15:52.551910 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89786 (* 1 = 7.89786 loss)
I0523 02:15:52.570901 35003 sgd_solver.cpp:112] Iteration 119460, lr = 0.01
I0523 02:15:54.691331 35003 solver.cpp:239] Iteration 119470 (4.67438 iter/s, 2.13932s/10 iters), loss = 7.64435
I0523 02:15:54.691385 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64435 (* 1 = 7.64435 loss)
I0523 02:15:55.406384 35003 sgd_solver.cpp:112] Iteration 119470, lr = 0.01
I0523 02:15:58.130844 35003 solver.cpp:239] Iteration 119480 (2.90755 iter/s, 3.43932s/10 iters), loss = 7.01204
I0523 02:15:58.130904 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01204 (* 1 = 7.01204 loss)
I0523 02:15:58.144492 35003 sgd_solver.cpp:112] Iteration 119480, lr = 0.01
I0523 02:16:01.642282 35003 solver.cpp:239] Iteration 119490 (2.848 iter/s, 3.51123s/10 iters), loss = 7.22996
I0523 02:16:01.642447 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22996 (* 1 = 7.22996 loss)
I0523 02:16:01.722394 35003 sgd_solver.cpp:112] Iteration 119490, lr = 0.01
I0523 02:16:05.327105 35003 solver.cpp:239] Iteration 119500 (2.71407 iter/s, 3.6845s/10 iters), loss = 7.51114
I0523 02:16:05.327159 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51114 (* 1 = 7.51114 loss)
I0523 02:16:05.335733 35003 sgd_solver.cpp:112] Iteration 119500, lr = 0.01
I0523 02:16:08.224138 35003 solver.cpp:239] Iteration 119510 (3.45202 iter/s, 2.89685s/10 iters), loss = 6.58804
I0523 02:16:08.224185 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58804 (* 1 = 6.58804 loss)
I0523 02:16:08.921649 35003 sgd_solver.cpp:112] Iteration 119510, lr = 0.01
I0523 02:16:13.774969 35003 solver.cpp:239] Iteration 119520 (1.80162 iter/s, 5.55056s/10 iters), loss = 7.88795
I0523 02:16:13.775028 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88795 (* 1 = 7.88795 loss)
I0523 02:16:13.781817 35003 sgd_solver.cpp:112] Iteration 119520, lr = 0.01
I0523 02:16:17.259197 35003 solver.cpp:239] Iteration 119530 (2.87024 iter/s, 3.48403s/10 iters), loss = 6.1556
I0523 02:16:17.259240 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1556 (* 1 = 6.1556 loss)
I0523 02:16:17.266691 35003 sgd_solver.cpp:112] Iteration 119530, lr = 0.01
I0523 02:16:20.802976 35003 solver.cpp:239] Iteration 119540 (2.822 iter/s, 3.54359s/10 iters), loss = 7.24374
I0523 02:16:20.803017 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24374 (* 1 = 7.24374 loss)
I0523 02:16:20.813746 35003 sgd_solver.cpp:112] Iteration 119540, lr = 0.01
I0523 02:16:23.649935 35003 solver.cpp:239] Iteration 119550 (3.51272 iter/s, 2.8468s/10 iters), loss = 6.8947
I0523 02:16:23.649986 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8947 (* 1 = 6.8947 loss)
I0523 02:16:23.663089 35003 sgd_solver.cpp:112] Iteration 119550, lr = 0.01
I0523 02:16:26.448369 35003 solver.cpp:239] Iteration 119560 (3.57364 iter/s, 2.79827s/10 iters), loss = 6.83051
I0523 02:16:26.448417 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83051 (* 1 = 6.83051 loss)
I0523 02:16:27.085871 35003 sgd_solver.cpp:112] Iteration 119560, lr = 0.01
I0523 02:16:30.615484 35003 solver.cpp:239] Iteration 119570 (2.39987 iter/s, 4.16689s/10 iters), loss = 6.00796
I0523 02:16:30.615535 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00796 (* 1 = 6.00796 loss)
I0523 02:16:30.661476 35003 sgd_solver.cpp:112] Iteration 119570, lr = 0.01
I0523 02:16:33.952916 35003 solver.cpp:239] Iteration 119580 (2.99648 iter/s, 3.33724s/10 iters), loss = 6.40565
I0523 02:16:33.953119 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40565 (* 1 = 6.40565 loss)
I0523 02:16:34.527410 35003 sgd_solver.cpp:112] Iteration 119580, lr = 0.01
I0523 02:16:38.790339 35003 solver.cpp:239] Iteration 119590 (2.06741 iter/s, 4.83698s/10 iters), loss = 7.74569
I0523 02:16:38.790397 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74569 (* 1 = 7.74569 loss)
I0523 02:16:38.799986 35003 sgd_solver.cpp:112] Iteration 119590, lr = 0.01
I0523 02:16:42.263945 35003 solver.cpp:239] Iteration 119600 (2.87902 iter/s, 3.4734s/10 iters), loss = 7.8096
I0523 02:16:42.264073 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8096 (* 1 = 7.8096 loss)
I0523 02:16:42.279706 35003 sgd_solver.cpp:112] Iteration 119600, lr = 0.01
I0523 02:16:46.588275 35003 solver.cpp:239] Iteration 119610 (2.31497 iter/s, 4.31971s/10 iters), loss = 5.87892
I0523 02:16:46.588320 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87892 (* 1 = 5.87892 loss)
I0523 02:16:46.595340 35003 sgd_solver.cpp:112] Iteration 119610, lr = 0.01
I0523 02:16:50.132381 35003 solver.cpp:239] Iteration 119620 (2.82174 iter/s, 3.54391s/10 iters), loss = 7.2062
I0523 02:16:50.132429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2062 (* 1 = 7.2062 loss)
I0523 02:16:50.142246 35003 sgd_solver.cpp:112] Iteration 119620, lr = 0.01
I0523 02:16:53.616561 35003 solver.cpp:239] Iteration 119630 (2.87028 iter/s, 3.48399s/10 iters), loss = 6.65145
I0523 02:16:53.616614 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65145 (* 1 = 6.65145 loss)
I0523 02:16:53.637717 35003 sgd_solver.cpp:112] Iteration 119630, lr = 0.01
I0523 02:16:58.871206 35003 solver.cpp:239] Iteration 119640 (1.90317 iter/s, 5.25438s/10 iters), loss = 6.40282
I0523 02:16:58.871255 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40282 (* 1 = 6.40282 loss)
I0523 02:16:58.959668 35003 sgd_solver.cpp:112] Iteration 119640, lr = 0.01
I0523 02:17:02.258298 35003 solver.cpp:239] Iteration 119650 (2.95255 iter/s, 3.3869s/10 iters), loss = 7.5499
I0523 02:17:02.258338 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5499 (* 1 = 7.5499 loss)
I0523 02:17:02.266108 35003 sgd_solver.cpp:112] Iteration 119650, lr = 0.01
I0523 02:17:05.161773 35003 solver.cpp:239] Iteration 119660 (3.44434 iter/s, 2.90331s/10 iters), loss = 6.9122
I0523 02:17:05.161995 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9122 (* 1 = 6.9122 loss)
I0523 02:17:05.877249 35003 sgd_solver.cpp:112] Iteration 119660, lr = 0.01
I0523 02:17:08.006392 35003 solver.cpp:239] Iteration 119670 (3.5158 iter/s, 2.8443s/10 iters), loss = 7.495
I0523 02:17:08.006443 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.495 (* 1 = 7.495 loss)
I0523 02:17:08.747920 35003 sgd_solver.cpp:112] Iteration 119670, lr = 0.01
I0523 02:17:12.241550 35003 solver.cpp:239] Iteration 119680 (2.36131 iter/s, 4.23493s/10 iters), loss = 7.71854
I0523 02:17:12.241593 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71854 (* 1 = 7.71854 loss)
I0523 02:17:12.253031 35003 sgd_solver.cpp:112] Iteration 119680, lr = 0.01
I0523 02:17:16.063573 35003 solver.cpp:239] Iteration 119690 (2.61656 iter/s, 3.82181s/10 iters), loss = 6.97122
I0523 02:17:16.063633 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97122 (* 1 = 6.97122 loss)
I0523 02:17:16.076740 35003 sgd_solver.cpp:112] Iteration 119690, lr = 0.01
I0523 02:17:19.676717 35003 solver.cpp:239] Iteration 119700 (2.76783 iter/s, 3.61293s/10 iters), loss = 6.93832
I0523 02:17:19.676774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93832 (* 1 = 6.93832 loss)
I0523 02:17:20.385116 35003 sgd_solver.cpp:112] Iteration 119700, lr = 0.01
I0523 02:17:24.050611 35003 solver.cpp:239] Iteration 119710 (2.28642 iter/s, 4.37366s/10 iters), loss = 7.06272
I0523 02:17:24.050662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06272 (* 1 = 7.06272 loss)
I0523 02:17:24.078065 35003 sgd_solver.cpp:112] Iteration 119710, lr = 0.01
I0523 02:17:25.682368 35003 solver.cpp:239] Iteration 119720 (6.12884 iter/s, 1.63163s/10 iters), loss = 7.46663
I0523 02:17:25.682418 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46663 (* 1 = 7.46663 loss)
I0523 02:17:26.358716 35003 sgd_solver.cpp:112] Iteration 119720, lr = 0.01
I0523 02:17:28.726667 35003 solver.cpp:239] Iteration 119730 (3.28502 iter/s, 3.04412s/10 iters), loss = 6.24867
I0523 02:17:28.726724 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24867 (* 1 = 6.24867 loss)
I0523 02:17:28.733523 35003 sgd_solver.cpp:112] Iteration 119730, lr = 0.01
I0523 02:17:32.912056 35003 solver.cpp:239] Iteration 119740 (2.3894 iter/s, 4.18516s/10 iters), loss = 7.43971
I0523 02:17:32.912108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43971 (* 1 = 7.43971 loss)
I0523 02:17:33.649761 35003 sgd_solver.cpp:112] Iteration 119740, lr = 0.01
I0523 02:17:37.270164 35003 solver.cpp:239] Iteration 119750 (2.2947 iter/s, 4.35788s/10 iters), loss = 6.91424
I0523 02:17:37.270406 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91424 (* 1 = 6.91424 loss)
I0523 02:17:37.282994 35003 sgd_solver.cpp:112] Iteration 119750, lr = 0.01
I0523 02:17:41.829946 35003 solver.cpp:239] Iteration 119760 (2.19329 iter/s, 4.55936s/10 iters), loss = 7.02858
I0523 02:17:41.830004 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02858 (* 1 = 7.02858 loss)
I0523 02:17:41.836164 35003 sgd_solver.cpp:112] Iteration 119760, lr = 0.01
I0523 02:17:44.658407 35003 solver.cpp:239] Iteration 119770 (3.53572 iter/s, 2.82828s/10 iters), loss = 7.61183
I0523 02:17:44.658464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61183 (* 1 = 7.61183 loss)
I0523 02:17:44.671028 35003 sgd_solver.cpp:112] Iteration 119770, lr = 0.01
I0523 02:17:48.197999 35003 solver.cpp:239] Iteration 119780 (2.82535 iter/s, 3.53938s/10 iters), loss = 6.76654
I0523 02:17:48.198051 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76654 (* 1 = 6.76654 loss)
I0523 02:17:48.211246 35003 sgd_solver.cpp:112] Iteration 119780, lr = 0.01
I0523 02:17:51.030489 35003 solver.cpp:239] Iteration 119790 (3.53068 iter/s, 2.83231s/10 iters), loss = 7.87548
I0523 02:17:51.030549 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87548 (* 1 = 7.87548 loss)
I0523 02:17:51.215761 35003 sgd_solver.cpp:112] Iteration 119790, lr = 0.01
I0523 02:17:53.200799 35003 solver.cpp:239] Iteration 119800 (4.60795 iter/s, 2.17016s/10 iters), loss = 6.82494
I0523 02:17:53.200845 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82494 (* 1 = 6.82494 loss)
I0523 02:17:53.203470 35003 sgd_solver.cpp:112] Iteration 119800, lr = 0.01
I0523 02:17:56.798533 35003 solver.cpp:239] Iteration 119810 (2.7797 iter/s, 3.59751s/10 iters), loss = 7.01253
I0523 02:17:56.798580 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01253 (* 1 = 7.01253 loss)
I0523 02:17:56.802906 35003 sgd_solver.cpp:112] Iteration 119810, lr = 0.01
I0523 02:18:00.520328 35003 solver.cpp:239] Iteration 119820 (2.68703 iter/s, 3.72158s/10 iters), loss = 6.90124
I0523 02:18:00.520372 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90124 (* 1 = 6.90124 loss)
I0523 02:18:00.539031 35003 sgd_solver.cpp:112] Iteration 119820, lr = 0.01
I0523 02:18:03.756695 35003 solver.cpp:239] Iteration 119830 (3.09006 iter/s, 3.23618s/10 iters), loss = 6.76207
I0523 02:18:03.756739 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76207 (* 1 = 6.76207 loss)
I0523 02:18:03.769433 35003 sgd_solver.cpp:112] Iteration 119830, lr = 0.01
I0523 02:18:05.944017 35003 solver.cpp:239] Iteration 119840 (4.57209 iter/s, 2.18718s/10 iters), loss = 7.79997
I0523 02:18:05.944062 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79997 (* 1 = 7.79997 loss)
I0523 02:18:05.952152 35003 sgd_solver.cpp:112] Iteration 119840, lr = 0.01
I0523 02:18:08.902982 35003 solver.cpp:239] Iteration 119850 (3.37976 iter/s, 2.95879s/10 iters), loss = 7.29924
I0523 02:18:08.903295 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29924 (* 1 = 7.29924 loss)
I0523 02:18:08.925739 35003 sgd_solver.cpp:112] Iteration 119850, lr = 0.01
I0523 02:18:13.128293 35003 solver.cpp:239] Iteration 119860 (2.3681 iter/s, 4.22279s/10 iters), loss = 6.80377
I0523 02:18:13.128329 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80377 (* 1 = 6.80377 loss)
I0523 02:18:13.141464 35003 sgd_solver.cpp:112] Iteration 119860, lr = 0.01
I0523 02:18:15.973767 35003 solver.cpp:239] Iteration 119870 (3.51455 iter/s, 2.84531s/10 iters), loss = 6.66285
I0523 02:18:15.973814 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66285 (* 1 = 6.66285 loss)
I0523 02:18:15.990267 35003 sgd_solver.cpp:112] Iteration 119870, lr = 0.01
I0523 02:18:18.860008 35003 solver.cpp:239] Iteration 119880 (3.46492 iter/s, 2.88607s/10 iters), loss = 6.74272
I0523 02:18:18.860064 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74272 (* 1 = 6.74272 loss)
I0523 02:18:18.874402 35003 sgd_solver.cpp:112] Iteration 119880, lr = 0.01
I0523 02:18:20.939911 35003 solver.cpp:239] Iteration 119890 (4.80828 iter/s, 2.07975s/10 iters), loss = 6.73285
I0523 02:18:20.939970 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73285 (* 1 = 6.73285 loss)
I0523 02:18:20.943485 35003 sgd_solver.cpp:112] Iteration 119890, lr = 0.01
I0523 02:18:26.129606 35003 solver.cpp:239] Iteration 119900 (1.927 iter/s, 5.18943s/10 iters), loss = 7.4108
I0523 02:18:26.129647 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4108 (* 1 = 7.4108 loss)
I0523 02:18:26.143182 35003 sgd_solver.cpp:112] Iteration 119900, lr = 0.01
I0523 02:18:28.289943 35003 solver.cpp:239] Iteration 119910 (4.62923 iter/s, 2.16018s/10 iters), loss = 7.02177
I0523 02:18:28.290009 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02177 (* 1 = 7.02177 loss)
I0523 02:18:28.303004 35003 sgd_solver.cpp:112] Iteration 119910, lr = 0.01
I0523 02:18:31.059775 35003 solver.cpp:239] Iteration 119920 (3.61056 iter/s, 2.76965s/10 iters), loss = 6.62647
I0523 02:18:31.059825 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62647 (* 1 = 6.62647 loss)
I0523 02:18:31.768191 35003 sgd_solver.cpp:112] Iteration 119920, lr = 0.01
I0523 02:18:34.505431 35003 solver.cpp:239] Iteration 119930 (2.90236 iter/s, 3.44547s/10 iters), loss = 7.25204
I0523 02:18:34.505475 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25204 (* 1 = 7.25204 loss)
I0523 02:18:34.740316 35003 sgd_solver.cpp:112] Iteration 119930, lr = 0.01
I0523 02:18:38.253824 35003 solver.cpp:239] Iteration 119940 (2.66796 iter/s, 3.74819s/10 iters), loss = 5.78889
I0523 02:18:38.253867 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.78889 (* 1 = 5.78889 loss)
I0523 02:18:38.258049 35003 sgd_solver.cpp:112] Iteration 119940, lr = 0.01
I0523 02:18:42.829699 35003 solver.cpp:239] Iteration 119950 (2.18548 iter/s, 4.57564s/10 iters), loss = 7.22605
I0523 02:18:42.829922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22605 (* 1 = 7.22605 loss)
I0523 02:18:42.842922 35003 sgd_solver.cpp:112] Iteration 119950, lr = 0.01
I0523 02:18:45.701010 35003 solver.cpp:239] Iteration 119960 (3.48312 iter/s, 2.87099s/10 iters), loss = 6.3912
I0523 02:18:45.701046 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3912 (* 1 = 6.3912 loss)
I0523 02:18:45.709244 35003 sgd_solver.cpp:112] Iteration 119960, lr = 0.01
I0523 02:18:48.601209 35003 solver.cpp:239] Iteration 119970 (3.44824 iter/s, 2.90003s/10 iters), loss = 6.44272
I0523 02:18:48.601274 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44272 (* 1 = 6.44272 loss)
I0523 02:18:49.342034 35003 sgd_solver.cpp:112] Iteration 119970, lr = 0.01
I0523 02:18:52.809521 35003 solver.cpp:239] Iteration 119980 (2.37638 iter/s, 4.20808s/10 iters), loss = 6.42306
I0523 02:18:52.809573 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42306 (* 1 = 6.42306 loss)
I0523 02:18:52.853174 35003 sgd_solver.cpp:112] Iteration 119980, lr = 0.01
I0523 02:18:57.154388 35003 solver.cpp:239] Iteration 119990 (2.30169 iter/s, 4.34463s/10 iters), loss = 7.75794
I0523 02:18:57.154459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75794 (* 1 = 7.75794 loss)
I0523 02:18:57.627868 35003 sgd_solver.cpp:112] Iteration 119990, lr = 0.01
I0523 02:19:01.094178 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_120000.caffemodel
I0523 02:19:01.514632 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_120000.solverstate
I0523 02:19:01.688776 35003 solver.cpp:239] Iteration 120000 (2.2055 iter/s, 4.53413s/10 iters), loss = 7.17168
I0523 02:19:01.688853 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17168 (* 1 = 7.17168 loss)
I0523 02:19:01.902736 35003 sgd_solver.cpp:112] Iteration 120000, lr = 0.01
I0523 02:19:03.924376 35003 solver.cpp:239] Iteration 120010 (4.47343 iter/s, 2.23542s/10 iters), loss = 6.00098
I0523 02:19:03.924432 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00098 (* 1 = 6.00098 loss)
I0523 02:19:04.632340 35003 sgd_solver.cpp:112] Iteration 120010, lr = 0.01
I0523 02:19:09.014603 35003 solver.cpp:239] Iteration 120020 (1.96465 iter/s, 5.08996s/10 iters), loss = 6.50522
I0523 02:19:09.014652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50522 (* 1 = 6.50522 loss)
I0523 02:19:09.718665 35003 sgd_solver.cpp:112] Iteration 120020, lr = 0.01
I0523 02:19:12.444883 35003 solver.cpp:239] Iteration 120030 (2.91538 iter/s, 3.43009s/10 iters), loss = 7.38651
I0523 02:19:12.444921 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38651 (* 1 = 7.38651 loss)
I0523 02:19:12.454015 35003 sgd_solver.cpp:112] Iteration 120030, lr = 0.01
I0523 02:19:16.369288 35003 solver.cpp:239] Iteration 120040 (2.54829 iter/s, 3.92421s/10 iters), loss = 7.302
I0523 02:19:16.369458 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.302 (* 1 = 7.302 loss)
I0523 02:19:16.384418 35003 sgd_solver.cpp:112] Iteration 120040, lr = 0.01
I0523 02:19:21.391273 35003 solver.cpp:239] Iteration 120050 (1.99144 iter/s, 5.02148s/10 iters), loss = 7.56855
I0523 02:19:21.391329 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56855 (* 1 = 7.56855 loss)
I0523 02:19:21.404713 35003 sgd_solver.cpp:112] Iteration 120050, lr = 0.01
I0523 02:19:24.271277 35003 solver.cpp:239] Iteration 120060 (3.47243 iter/s, 2.87983s/10 iters), loss = 7.55915
I0523 02:19:24.271320 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55915 (* 1 = 7.55915 loss)
I0523 02:19:24.282645 35003 sgd_solver.cpp:112] Iteration 120060, lr = 0.01
I0523 02:19:26.472793 35003 solver.cpp:239] Iteration 120070 (4.54261 iter/s, 2.20138s/10 iters), loss = 6.98163
I0523 02:19:26.472837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98163 (* 1 = 6.98163 loss)
I0523 02:19:27.167930 35003 sgd_solver.cpp:112] Iteration 120070, lr = 0.01
I0523 02:19:29.966842 35003 solver.cpp:239] Iteration 120080 (2.86216 iter/s, 3.49386s/10 iters), loss = 7.03452
I0523 02:19:29.966888 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03452 (* 1 = 7.03452 loss)
I0523 02:19:30.621096 35003 sgd_solver.cpp:112] Iteration 120080, lr = 0.01
I0523 02:19:33.481871 35003 solver.cpp:239] Iteration 120090 (2.84509 iter/s, 3.51483s/10 iters), loss = 7.47746
I0523 02:19:33.481922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47746 (* 1 = 7.47746 loss)
I0523 02:19:34.222649 35003 sgd_solver.cpp:112] Iteration 120090, lr = 0.01
I0523 02:19:37.429190 35003 solver.cpp:239] Iteration 120100 (2.5335 iter/s, 3.94711s/10 iters), loss = 7.75231
I0523 02:19:37.429227 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75231 (* 1 = 7.75231 loss)
I0523 02:19:37.437431 35003 sgd_solver.cpp:112] Iteration 120100, lr = 0.01
I0523 02:19:40.233345 35003 solver.cpp:239] Iteration 120110 (3.56635 iter/s, 2.80398s/10 iters), loss = 7.41195
I0523 02:19:40.233407 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41195 (* 1 = 7.41195 loss)
I0523 02:19:40.241255 35003 sgd_solver.cpp:112] Iteration 120110, lr = 0.01
I0523 02:19:43.840170 35003 solver.cpp:239] Iteration 120120 (2.77268 iter/s, 3.60662s/10 iters), loss = 6.44255
I0523 02:19:43.840217 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44255 (* 1 = 6.44255 loss)
I0523 02:19:44.555902 35003 sgd_solver.cpp:112] Iteration 120120, lr = 0.01
I0523 02:19:48.743542 35003 solver.cpp:239] Iteration 120130 (2.03952 iter/s, 4.90313s/10 iters), loss = 6.48232
I0523 02:19:48.743849 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48232 (* 1 = 6.48232 loss)
I0523 02:19:48.757117 35003 sgd_solver.cpp:112] Iteration 120130, lr = 0.01
I0523 02:19:51.499531 35003 solver.cpp:239] Iteration 120140 (3.62899 iter/s, 2.75559s/10 iters), loss = 7.9896
I0523 02:19:51.499572 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9896 (* 1 = 7.9896 loss)
I0523 02:19:51.522918 35003 sgd_solver.cpp:112] Iteration 120140, lr = 0.01
I0523 02:19:54.345947 35003 solver.cpp:239] Iteration 120150 (3.5134 iter/s, 2.84625s/10 iters), loss = 7.48845
I0523 02:19:54.346007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48845 (* 1 = 7.48845 loss)
I0523 02:19:55.021922 35003 sgd_solver.cpp:112] Iteration 120150, lr = 0.01
I0523 02:19:57.722481 35003 solver.cpp:239] Iteration 120160 (2.9618 iter/s, 3.37632s/10 iters), loss = 7.2163
I0523 02:19:57.722533 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2163 (* 1 = 7.2163 loss)
I0523 02:19:58.424386 35003 sgd_solver.cpp:112] Iteration 120160, lr = 0.01
I0523 02:20:01.324204 35003 solver.cpp:239] Iteration 120170 (2.77661 iter/s, 3.60151s/10 iters), loss = 8.4106
I0523 02:20:01.324255 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.4106 (* 1 = 8.4106 loss)
I0523 02:20:01.331097 35003 sgd_solver.cpp:112] Iteration 120170, lr = 0.01
I0523 02:20:04.975455 35003 solver.cpp:239] Iteration 120180 (2.73894 iter/s, 3.65105s/10 iters), loss = 6.19065
I0523 02:20:04.975494 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19065 (* 1 = 6.19065 loss)
I0523 02:20:04.980271 35003 sgd_solver.cpp:112] Iteration 120180, lr = 0.01
I0523 02:20:07.891417 35003 solver.cpp:239] Iteration 120190 (3.42959 iter/s, 2.9158s/10 iters), loss = 7.67353
I0523 02:20:07.891459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67353 (* 1 = 7.67353 loss)
I0523 02:20:08.352596 35003 sgd_solver.cpp:112] Iteration 120190, lr = 0.01
I0523 02:20:10.304208 35003 solver.cpp:239] Iteration 120200 (4.14483 iter/s, 2.41264s/10 iters), loss = 5.71051
I0523 02:20:10.304249 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.71051 (* 1 = 5.71051 loss)
I0523 02:20:10.317286 35003 sgd_solver.cpp:112] Iteration 120200, lr = 0.01
I0523 02:20:13.187326 35003 solver.cpp:239] Iteration 120210 (3.46867 iter/s, 2.88295s/10 iters), loss = 6.24352
I0523 02:20:13.187369 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24352 (* 1 = 6.24352 loss)
I0523 02:20:13.205679 35003 sgd_solver.cpp:112] Iteration 120210, lr = 0.01
I0523 02:20:18.181751 35003 solver.cpp:239] Iteration 120220 (2.00233 iter/s, 4.99418s/10 iters), loss = 5.74373
I0523 02:20:18.181793 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74373 (* 1 = 5.74373 loss)
I0523 02:20:18.188535 35003 sgd_solver.cpp:112] Iteration 120220, lr = 0.01
I0523 02:20:21.481798 35003 solver.cpp:239] Iteration 120230 (3.03043 iter/s, 3.29986s/10 iters), loss = 7.55976
I0523 02:20:21.482000 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55976 (* 1 = 7.55976 loss)
I0523 02:20:22.188088 35003 sgd_solver.cpp:112] Iteration 120230, lr = 0.01
I0523 02:20:25.779175 35003 solver.cpp:239] Iteration 120240 (2.32721 iter/s, 4.29699s/10 iters), loss = 5.70546
I0523 02:20:25.779224 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.70546 (* 1 = 5.70546 loss)
I0523 02:20:25.788835 35003 sgd_solver.cpp:112] Iteration 120240, lr = 0.01
I0523 02:20:28.091426 35003 solver.cpp:239] Iteration 120250 (4.32508 iter/s, 2.3121s/10 iters), loss = 7.35023
I0523 02:20:28.091482 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35023 (* 1 = 7.35023 loss)
I0523 02:20:28.806996 35003 sgd_solver.cpp:112] Iteration 120250, lr = 0.01
I0523 02:20:31.605955 35003 solver.cpp:239] Iteration 120260 (2.84551 iter/s, 3.51431s/10 iters), loss = 7.34029
I0523 02:20:31.606001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34029 (* 1 = 7.34029 loss)
I0523 02:20:31.629163 35003 sgd_solver.cpp:112] Iteration 120260, lr = 0.01
I0523 02:20:35.959456 35003 solver.cpp:239] Iteration 120270 (2.29712 iter/s, 4.35328s/10 iters), loss = 6.94075
I0523 02:20:35.959509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94075 (* 1 = 6.94075 loss)
I0523 02:20:35.974679 35003 sgd_solver.cpp:112] Iteration 120270, lr = 0.01
I0523 02:20:38.874228 35003 solver.cpp:239] Iteration 120280 (3.43101 iter/s, 2.9146s/10 iters), loss = 6.84443
I0523 02:20:38.874284 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84443 (* 1 = 6.84443 loss)
I0523 02:20:39.615285 35003 sgd_solver.cpp:112] Iteration 120280, lr = 0.01
I0523 02:20:43.177248 35003 solver.cpp:239] Iteration 120290 (2.32408 iter/s, 4.30279s/10 iters), loss = 6.23481
I0523 02:20:43.177296 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23481 (* 1 = 6.23481 loss)
I0523 02:20:43.837810 35003 sgd_solver.cpp:112] Iteration 120290, lr = 0.01
I0523 02:20:48.206169 35003 solver.cpp:239] Iteration 120300 (1.9886 iter/s, 5.02867s/10 iters), loss = 5.48772
I0523 02:20:48.206212 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.48772 (* 1 = 5.48772 loss)
I0523 02:20:48.214246 35003 sgd_solver.cpp:112] Iteration 120300, lr = 0.01
I0523 02:20:50.306044 35003 solver.cpp:239] Iteration 120310 (4.7625 iter/s, 2.09974s/10 iters), loss = 7.44502
I0523 02:20:50.306087 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44502 (* 1 = 7.44502 loss)
I0523 02:20:50.320078 35003 sgd_solver.cpp:112] Iteration 120310, lr = 0.01
I0523 02:20:54.051761 35003 solver.cpp:239] Iteration 120320 (2.66986 iter/s, 3.74552s/10 iters), loss = 6.95054
I0523 02:20:54.051882 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95054 (* 1 = 6.95054 loss)
I0523 02:20:54.767325 35003 sgd_solver.cpp:112] Iteration 120320, lr = 0.01
I0523 02:20:59.205508 35003 solver.cpp:239] Iteration 120330 (1.94046 iter/s, 5.15341s/10 iters), loss = 6.74056
I0523 02:20:59.205570 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74056 (* 1 = 6.74056 loss)
I0523 02:20:59.214607 35003 sgd_solver.cpp:112] Iteration 120330, lr = 0.01
I0523 02:21:02.740502 35003 solver.cpp:239] Iteration 120340 (2.82902 iter/s, 3.53479s/10 iters), loss = 7.55715
I0523 02:21:02.740545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55715 (* 1 = 7.55715 loss)
I0523 02:21:03.449437 35003 sgd_solver.cpp:112] Iteration 120340, lr = 0.01
I0523 02:21:07.756448 35003 solver.cpp:239] Iteration 120350 (1.99374 iter/s, 5.0157s/10 iters), loss = 8.52987
I0523 02:21:07.756500 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.52987 (* 1 = 8.52987 loss)
I0523 02:21:07.764076 35003 sgd_solver.cpp:112] Iteration 120350, lr = 0.01
I0523 02:21:09.933769 35003 solver.cpp:239] Iteration 120360 (4.59311 iter/s, 2.17718s/10 iters), loss = 8.23908
I0523 02:21:09.933810 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.23908 (* 1 = 8.23908 loss)
I0523 02:21:09.958000 35003 sgd_solver.cpp:112] Iteration 120360, lr = 0.01
I0523 02:21:12.819833 35003 solver.cpp:239] Iteration 120370 (3.46513 iter/s, 2.88589s/10 iters), loss = 7.49665
I0523 02:21:12.819888 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49665 (* 1 = 7.49665 loss)
I0523 02:21:12.826145 35003 sgd_solver.cpp:112] Iteration 120370, lr = 0.01
I0523 02:21:17.120144 35003 solver.cpp:239] Iteration 120380 (2.32554 iter/s, 4.30008s/10 iters), loss = 6.88522
I0523 02:21:17.120193 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88522 (* 1 = 6.88522 loss)
I0523 02:21:17.123942 35003 sgd_solver.cpp:112] Iteration 120380, lr = 0.01
I0523 02:21:18.431036 35003 solver.cpp:239] Iteration 120390 (7.62909 iter/s, 1.31077s/10 iters), loss = 6.20145
I0523 02:21:18.431097 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20145 (* 1 = 6.20145 loss)
I0523 02:21:18.437198 35003 sgd_solver.cpp:112] Iteration 120390, lr = 0.01
I0523 02:21:22.768852 35003 solver.cpp:239] Iteration 120400 (2.30543 iter/s, 4.33758s/10 iters), loss = 7.87442
I0523 02:21:22.768906 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87442 (* 1 = 7.87442 loss)
I0523 02:21:23.483834 35003 sgd_solver.cpp:112] Iteration 120400, lr = 0.01
I0523 02:21:26.004287 35003 solver.cpp:239] Iteration 120410 (3.09097 iter/s, 3.23523s/10 iters), loss = 6.74829
I0523 02:21:26.004428 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74829 (* 1 = 6.74829 loss)
I0523 02:21:26.017782 35003 sgd_solver.cpp:112] Iteration 120410, lr = 0.01
I0523 02:21:28.098006 35003 solver.cpp:239] Iteration 120420 (4.77672 iter/s, 2.09349s/10 iters), loss = 7.82339
I0523 02:21:28.098044 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82339 (* 1 = 7.82339 loss)
I0523 02:21:28.110463 35003 sgd_solver.cpp:112] Iteration 120420, lr = 0.01
I0523 02:21:31.806356 35003 solver.cpp:239] Iteration 120430 (2.69676 iter/s, 3.70815s/10 iters), loss = 6.95625
I0523 02:21:31.806408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95625 (* 1 = 6.95625 loss)
I0523 02:21:32.508287 35003 sgd_solver.cpp:112] Iteration 120430, lr = 0.01
I0523 02:21:35.504957 35003 solver.cpp:239] Iteration 120440 (2.70387 iter/s, 3.6984s/10 iters), loss = 7.14802
I0523 02:21:35.505002 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14802 (* 1 = 7.14802 loss)
I0523 02:21:36.058928 35003 sgd_solver.cpp:112] Iteration 120440, lr = 0.01
I0523 02:21:39.044041 35003 solver.cpp:239] Iteration 120450 (2.82575 iter/s, 3.53888s/10 iters), loss = 7.42933
I0523 02:21:39.044093 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42933 (* 1 = 7.42933 loss)
I0523 02:21:39.052558 35003 sgd_solver.cpp:112] Iteration 120450, lr = 0.01
I0523 02:21:42.638145 35003 solver.cpp:239] Iteration 120460 (2.78253 iter/s, 3.59385s/10 iters), loss = 7.6658
I0523 02:21:42.638209 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6658 (* 1 = 7.6658 loss)
I0523 02:21:42.645066 35003 sgd_solver.cpp:112] Iteration 120460, lr = 0.01
I0523 02:21:45.713774 35003 solver.cpp:239] Iteration 120470 (3.25157 iter/s, 3.07544s/10 iters), loss = 6.85861
I0523 02:21:45.713824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85861 (* 1 = 6.85861 loss)
I0523 02:21:45.720715 35003 sgd_solver.cpp:112] Iteration 120470, lr = 0.01
I0523 02:21:47.791589 35003 solver.cpp:239] Iteration 120480 (4.81307 iter/s, 2.07768s/10 iters), loss = 7.74878
I0523 02:21:47.791631 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74878 (* 1 = 7.74878 loss)
I0523 02:21:47.810566 35003 sgd_solver.cpp:112] Iteration 120480, lr = 0.01
I0523 02:21:50.670111 35003 solver.cpp:239] Iteration 120490 (3.47421 iter/s, 2.87835s/10 iters), loss = 6.51846
I0523 02:21:50.670166 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51846 (* 1 = 6.51846 loss)
I0523 02:21:50.684921 35003 sgd_solver.cpp:112] Iteration 120490, lr = 0.01
I0523 02:21:53.384780 35003 solver.cpp:239] Iteration 120500 (3.68392 iter/s, 2.7145s/10 iters), loss = 6.87241
I0523 02:21:53.384819 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87241 (* 1 = 6.87241 loss)
I0523 02:21:53.392469 35003 sgd_solver.cpp:112] Iteration 120500, lr = 0.01
I0523 02:21:55.425087 35003 solver.cpp:239] Iteration 120510 (4.90153 iter/s, 2.04018s/10 iters), loss = 7.34361
I0523 02:21:55.425129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34361 (* 1 = 7.34361 loss)
I0523 02:21:55.443208 35003 sgd_solver.cpp:112] Iteration 120510, lr = 0.01
I0523 02:21:59.804208 35003 solver.cpp:239] Iteration 120520 (2.28368 iter/s, 4.3789s/10 iters), loss = 7.18961
I0523 02:21:59.804463 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18961 (* 1 = 7.18961 loss)
I0523 02:21:59.808769 35003 sgd_solver.cpp:112] Iteration 120520, lr = 0.01
I0523 02:22:02.600630 35003 solver.cpp:239] Iteration 120530 (3.57648 iter/s, 2.79604s/10 iters), loss = 7.2201
I0523 02:22:02.600687 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2201 (* 1 = 7.2201 loss)
I0523 02:22:02.606717 35003 sgd_solver.cpp:112] Iteration 120530, lr = 0.01
I0523 02:22:06.130401 35003 solver.cpp:239] Iteration 120540 (2.83321 iter/s, 3.52956s/10 iters), loss = 7.94002
I0523 02:22:06.130455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94002 (* 1 = 7.94002 loss)
I0523 02:22:06.134114 35003 sgd_solver.cpp:112] Iteration 120540, lr = 0.01
I0523 02:22:08.974771 35003 solver.cpp:239] Iteration 120550 (3.51882 iter/s, 2.84186s/10 iters), loss = 7.34889
I0523 02:22:08.974812 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34889 (* 1 = 7.34889 loss)
I0523 02:22:08.998600 35003 sgd_solver.cpp:112] Iteration 120550, lr = 0.01
I0523 02:22:12.264497 35003 solver.cpp:239] Iteration 120560 (3.03993 iter/s, 3.28955s/10 iters), loss = 7.3579
I0523 02:22:12.264540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3579 (* 1 = 7.3579 loss)
I0523 02:22:12.277454 35003 sgd_solver.cpp:112] Iteration 120560, lr = 0.01
I0523 02:22:15.759127 35003 solver.cpp:239] Iteration 120570 (2.86169 iter/s, 3.49444s/10 iters), loss = 7.77666
I0523 02:22:15.759171 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77666 (* 1 = 7.77666 loss)
I0523 02:22:15.772807 35003 sgd_solver.cpp:112] Iteration 120570, lr = 0.01
I0523 02:22:20.740042 35003 solver.cpp:239] Iteration 120580 (2.00777 iter/s, 4.98066s/10 iters), loss = 7.42991
I0523 02:22:20.740115 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42991 (* 1 = 7.42991 loss)
I0523 02:22:20.746585 35003 sgd_solver.cpp:112] Iteration 120580, lr = 0.01
I0523 02:22:25.333055 35003 solver.cpp:239] Iteration 120590 (2.17734 iter/s, 4.59275s/10 iters), loss = 7.57919
I0523 02:22:25.333094 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57919 (* 1 = 7.57919 loss)
I0523 02:22:25.345695 35003 sgd_solver.cpp:112] Iteration 120590, lr = 0.01
I0523 02:22:28.221000 35003 solver.cpp:239] Iteration 120600 (3.46287 iter/s, 2.88778s/10 iters), loss = 7.57353
I0523 02:22:28.221043 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57353 (* 1 = 7.57353 loss)
I0523 02:22:28.558357 35003 sgd_solver.cpp:112] Iteration 120600, lr = 0.01
I0523 02:22:31.475670 35003 solver.cpp:239] Iteration 120610 (3.07268 iter/s, 3.25449s/10 iters), loss = 8.40138
I0523 02:22:31.475872 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.40138 (* 1 = 8.40138 loss)
I0523 02:22:31.488713 35003 sgd_solver.cpp:112] Iteration 120610, lr = 0.01
I0523 02:22:35.770360 35003 solver.cpp:239] Iteration 120620 (2.32865 iter/s, 4.29433s/10 iters), loss = 7.69554
I0523 02:22:35.770416 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69554 (* 1 = 7.69554 loss)
I0523 02:22:35.777921 35003 sgd_solver.cpp:112] Iteration 120620, lr = 0.01
I0523 02:22:38.529520 35003 solver.cpp:239] Iteration 120630 (3.62452 iter/s, 2.75899s/10 iters), loss = 6.53114
I0523 02:22:38.529562 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53114 (* 1 = 6.53114 loss)
I0523 02:22:38.540272 35003 sgd_solver.cpp:112] Iteration 120630, lr = 0.01
I0523 02:22:42.569043 35003 solver.cpp:239] Iteration 120640 (2.47567 iter/s, 4.03931s/10 iters), loss = 6.12481
I0523 02:22:42.569092 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12481 (* 1 = 6.12481 loss)
I0523 02:22:42.578712 35003 sgd_solver.cpp:112] Iteration 120640, lr = 0.01
I0523 02:22:45.517376 35003 solver.cpp:239] Iteration 120650 (3.39195 iter/s, 2.94816s/10 iters), loss = 7.87605
I0523 02:22:45.517422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87605 (* 1 = 7.87605 loss)
I0523 02:22:46.217849 35003 sgd_solver.cpp:112] Iteration 120650, lr = 0.01
I0523 02:22:48.443605 35003 solver.cpp:239] Iteration 120660 (3.41757 iter/s, 2.92606s/10 iters), loss = 7.50897
I0523 02:22:48.443647 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50897 (* 1 = 7.50897 loss)
I0523 02:22:48.456293 35003 sgd_solver.cpp:112] Iteration 120660, lr = 0.01
I0523 02:22:52.030630 35003 solver.cpp:239] Iteration 120670 (2.78797 iter/s, 3.58684s/10 iters), loss = 5.90314
I0523 02:22:52.030669 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.90314 (* 1 = 5.90314 loss)
I0523 02:22:52.570355 35003 sgd_solver.cpp:112] Iteration 120670, lr = 0.01
I0523 02:22:56.031778 35003 solver.cpp:239] Iteration 120680 (2.49941 iter/s, 4.00094s/10 iters), loss = 8.25909
I0523 02:22:56.031839 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.25909 (* 1 = 8.25909 loss)
I0523 02:22:56.056845 35003 sgd_solver.cpp:112] Iteration 120680, lr = 0.01
I0523 02:22:59.736335 35003 solver.cpp:239] Iteration 120690 (2.69953 iter/s, 3.70434s/10 iters), loss = 6.45959
I0523 02:22:59.736382 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45959 (* 1 = 6.45959 loss)
I0523 02:23:00.431594 35003 sgd_solver.cpp:112] Iteration 120690, lr = 0.01
I0523 02:23:03.288524 35003 solver.cpp:239] Iteration 120700 (2.81532 iter/s, 3.55199s/10 iters), loss = 7.28046
I0523 02:23:03.288789 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28046 (* 1 = 7.28046 loss)
I0523 02:23:03.302430 35003 sgd_solver.cpp:112] Iteration 120700, lr = 0.01
I0523 02:23:08.318545 35003 solver.cpp:239] Iteration 120710 (1.98824 iter/s, 5.02956s/10 iters), loss = 8.60991
I0523 02:23:08.318608 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.60991 (* 1 = 8.60991 loss)
I0523 02:23:08.331408 35003 sgd_solver.cpp:112] Iteration 120710, lr = 0.01
I0523 02:23:11.940152 35003 solver.cpp:239] Iteration 120720 (2.76137 iter/s, 3.62139s/10 iters), loss = 6.85492
I0523 02:23:11.940191 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85492 (* 1 = 6.85492 loss)
I0523 02:23:11.957818 35003 sgd_solver.cpp:112] Iteration 120720, lr = 0.01
I0523 02:23:14.058817 35003 solver.cpp:239] Iteration 120730 (4.72027 iter/s, 2.11852s/10 iters), loss = 5.83577
I0523 02:23:14.058859 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.83577 (* 1 = 5.83577 loss)
I0523 02:23:14.063836 35003 sgd_solver.cpp:112] Iteration 120730, lr = 0.01
I0523 02:23:17.768748 35003 solver.cpp:239] Iteration 120740 (2.69561 iter/s, 3.70973s/10 iters), loss = 7.25445
I0523 02:23:17.768788 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25445 (* 1 = 7.25445 loss)
I0523 02:23:17.780681 35003 sgd_solver.cpp:112] Iteration 120740, lr = 0.01
I0523 02:23:20.100795 35003 solver.cpp:239] Iteration 120750 (4.28835 iter/s, 2.3319s/10 iters), loss = 6.89036
I0523 02:23:20.100836 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89036 (* 1 = 6.89036 loss)
I0523 02:23:20.106469 35003 sgd_solver.cpp:112] Iteration 120750, lr = 0.01
I0523 02:23:22.182190 35003 solver.cpp:239] Iteration 120760 (4.80479 iter/s, 2.08126s/10 iters), loss = 7.03005
I0523 02:23:22.182242 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03005 (* 1 = 7.03005 loss)
I0523 02:23:22.186118 35003 sgd_solver.cpp:112] Iteration 120760, lr = 0.01
I0523 02:23:25.815629 35003 solver.cpp:239] Iteration 120770 (2.75237 iter/s, 3.63323s/10 iters), loss = 5.9057
I0523 02:23:25.815675 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9057 (* 1 = 5.9057 loss)
I0523 02:23:26.556457 35003 sgd_solver.cpp:112] Iteration 120770, lr = 0.01
I0523 02:23:29.262192 35003 solver.cpp:239] Iteration 120780 (2.90161 iter/s, 3.44637s/10 iters), loss = 7.25411
I0523 02:23:29.262255 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25411 (* 1 = 7.25411 loss)
I0523 02:23:29.274968 35003 sgd_solver.cpp:112] Iteration 120780, lr = 0.01
I0523 02:23:32.036029 35003 solver.cpp:239] Iteration 120790 (3.60535 iter/s, 2.77366s/10 iters), loss = 6.79438
I0523 02:23:32.036072 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79438 (* 1 = 6.79438 loss)
I0523 02:23:32.039615 35003 sgd_solver.cpp:112] Iteration 120790, lr = 0.01
I0523 02:23:34.692747 35003 solver.cpp:239] Iteration 120800 (3.76427 iter/s, 2.65656s/10 iters), loss = 7.56137
I0523 02:23:34.692911 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56137 (* 1 = 7.56137 loss)
I0523 02:23:34.711633 35003 sgd_solver.cpp:112] Iteration 120800, lr = 0.01
I0523 02:23:37.595551 35003 solver.cpp:239] Iteration 120810 (3.44528 iter/s, 2.90252s/10 iters), loss = 7.53411
I0523 02:23:37.595592 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53411 (* 1 = 7.53411 loss)
I0523 02:23:38.330499 35003 sgd_solver.cpp:112] Iteration 120810, lr = 0.01
I0523 02:23:41.855298 35003 solver.cpp:239] Iteration 120820 (2.34768 iter/s, 4.25952s/10 iters), loss = 6.90562
I0523 02:23:41.855355 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90562 (* 1 = 6.90562 loss)
I0523 02:23:41.861802 35003 sgd_solver.cpp:112] Iteration 120820, lr = 0.01
I0523 02:23:45.564126 35003 solver.cpp:239] Iteration 120830 (2.69642 iter/s, 3.70862s/10 iters), loss = 7.26729
I0523 02:23:45.564167 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26729 (* 1 = 7.26729 loss)
I0523 02:23:45.590988 35003 sgd_solver.cpp:112] Iteration 120830, lr = 0.01
I0523 02:23:48.035564 35003 solver.cpp:239] Iteration 120840 (4.04647 iter/s, 2.47129s/10 iters), loss = 7.60012
I0523 02:23:48.035612 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60012 (* 1 = 7.60012 loss)
I0523 02:23:48.043035 35003 sgd_solver.cpp:112] Iteration 120840, lr = 0.01
I0523 02:23:51.264643 35003 solver.cpp:239] Iteration 120850 (3.09703 iter/s, 3.2289s/10 iters), loss = 6.44393
I0523 02:23:51.264691 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44393 (* 1 = 6.44393 loss)
I0523 02:23:51.368077 35003 sgd_solver.cpp:112] Iteration 120850, lr = 0.01
I0523 02:23:54.146391 35003 solver.cpp:239] Iteration 120860 (3.47039 iter/s, 2.88152s/10 iters), loss = 6.40438
I0523 02:23:54.146442 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40438 (* 1 = 6.40438 loss)
I0523 02:23:54.165966 35003 sgd_solver.cpp:112] Iteration 120860, lr = 0.01
I0523 02:23:57.666508 35003 solver.cpp:239] Iteration 120870 (2.84097 iter/s, 3.51992s/10 iters), loss = 7.69145
I0523 02:23:57.666563 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69145 (* 1 = 7.69145 loss)
I0523 02:23:58.401098 35003 sgd_solver.cpp:112] Iteration 120870, lr = 0.01
I0523 02:24:01.173938 35003 solver.cpp:239] Iteration 120880 (2.85125 iter/s, 3.50723s/10 iters), loss = 6.78883
I0523 02:24:01.173983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78883 (* 1 = 6.78883 loss)
I0523 02:24:01.179028 35003 sgd_solver.cpp:112] Iteration 120880, lr = 0.01
I0523 02:24:04.545712 35003 solver.cpp:239] Iteration 120890 (2.96596 iter/s, 3.37159s/10 iters), loss = 8.04929
I0523 02:24:04.545766 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04929 (* 1 = 8.04929 loss)
I0523 02:24:05.272209 35003 sgd_solver.cpp:112] Iteration 120890, lr = 0.01
I0523 02:24:08.011646 35003 solver.cpp:239] Iteration 120900 (2.88539 iter/s, 3.46573s/10 iters), loss = 6.42582
I0523 02:24:08.011704 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42582 (* 1 = 6.42582 loss)
I0523 02:24:08.015027 35003 sgd_solver.cpp:112] Iteration 120900, lr = 0.01
I0523 02:24:11.163153 35003 solver.cpp:239] Iteration 120910 (3.1733 iter/s, 3.15129s/10 iters), loss = 6.84475
I0523 02:24:11.163203 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84475 (* 1 = 6.84475 loss)
I0523 02:24:11.176012 35003 sgd_solver.cpp:112] Iteration 120910, lr = 0.01
I0523 02:24:15.415335 35003 solver.cpp:239] Iteration 120920 (2.35186 iter/s, 4.25196s/10 iters), loss = 6.36402
I0523 02:24:15.415400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36402 (* 1 = 6.36402 loss)
I0523 02:24:16.143338 35003 sgd_solver.cpp:112] Iteration 120920, lr = 0.01
I0523 02:24:18.256755 35003 solver.cpp:239] Iteration 120930 (3.51959 iter/s, 2.84124s/10 iters), loss = 7.42675
I0523 02:24:18.256800 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42675 (* 1 = 7.42675 loss)
I0523 02:24:18.270804 35003 sgd_solver.cpp:112] Iteration 120930, lr = 0.01
I0523 02:24:22.614032 35003 solver.cpp:239] Iteration 120940 (2.29513 iter/s, 4.35705s/10 iters), loss = 7.9254
I0523 02:24:22.614078 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9254 (* 1 = 7.9254 loss)
I0523 02:24:22.629910 35003 sgd_solver.cpp:112] Iteration 120940, lr = 0.01
I0523 02:24:25.457734 35003 solver.cpp:239] Iteration 120950 (3.51675 iter/s, 2.84354s/10 iters), loss = 6.50049
I0523 02:24:25.457777 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50049 (* 1 = 6.50049 loss)
I0523 02:24:25.467897 35003 sgd_solver.cpp:112] Iteration 120950, lr = 0.01
I0523 02:24:27.392159 35003 solver.cpp:239] Iteration 120960 (5.16983 iter/s, 1.9343s/10 iters), loss = 7.12254
I0523 02:24:27.392210 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12254 (* 1 = 7.12254 loss)
I0523 02:24:27.802880 35003 sgd_solver.cpp:112] Iteration 120960, lr = 0.01
I0523 02:24:31.307320 35003 solver.cpp:239] Iteration 120970 (2.55431 iter/s, 3.91495s/10 iters), loss = 7.89863
I0523 02:24:31.307363 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89863 (* 1 = 7.89863 loss)
I0523 02:24:31.331779 35003 sgd_solver.cpp:112] Iteration 120970, lr = 0.01
I0523 02:24:34.936161 35003 solver.cpp:239] Iteration 120980 (2.75585 iter/s, 3.62865s/10 iters), loss = 6.52244
I0523 02:24:34.936206 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52244 (* 1 = 6.52244 loss)
I0523 02:24:34.949033 35003 sgd_solver.cpp:112] Iteration 120980, lr = 0.01
I0523 02:24:37.774751 35003 solver.cpp:239] Iteration 120990 (3.52308 iter/s, 2.83842s/10 iters), loss = 7.10782
I0523 02:24:37.774957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10782 (* 1 = 7.10782 loss)
I0523 02:24:37.787744 35003 sgd_solver.cpp:112] Iteration 120990, lr = 0.01
I0523 02:24:42.110893 35003 solver.cpp:239] Iteration 121000 (2.30639 iter/s, 4.33578s/10 iters), loss = 6.26578
I0523 02:24:42.110935 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26578 (* 1 = 6.26578 loss)
I0523 02:24:42.832463 35003 sgd_solver.cpp:112] Iteration 121000, lr = 0.01
I0523 02:24:46.261061 35003 solver.cpp:239] Iteration 121010 (2.40967 iter/s, 4.14994s/10 iters), loss = 8.05713
I0523 02:24:46.261137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05713 (* 1 = 8.05713 loss)
I0523 02:24:46.982515 35003 sgd_solver.cpp:112] Iteration 121010, lr = 0.01
I0523 02:24:51.041139 35003 solver.cpp:239] Iteration 121020 (2.09213 iter/s, 4.77981s/10 iters), loss = 6.94874
I0523 02:24:51.041184 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94874 (* 1 = 6.94874 loss)
I0523 02:24:51.048483 35003 sgd_solver.cpp:112] Iteration 121020, lr = 0.01
I0523 02:24:54.529902 35003 solver.cpp:239] Iteration 121030 (2.8665 iter/s, 3.48857s/10 iters), loss = 7.34943
I0523 02:24:54.529942 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34943 (* 1 = 7.34943 loss)
I0523 02:24:54.549904 35003 sgd_solver.cpp:112] Iteration 121030, lr = 0.01
I0523 02:24:57.997243 35003 solver.cpp:239] Iteration 121040 (2.88421 iter/s, 3.46716s/10 iters), loss = 7.25715
I0523 02:24:57.997287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25715 (* 1 = 7.25715 loss)
I0523 02:24:58.667058 35003 sgd_solver.cpp:112] Iteration 121040, lr = 0.01
I0523 02:25:00.663373 35003 solver.cpp:239] Iteration 121050 (3.75098 iter/s, 2.66597s/10 iters), loss = 7.33857
I0523 02:25:00.663410 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33857 (* 1 = 7.33857 loss)
I0523 02:25:00.676548 35003 sgd_solver.cpp:112] Iteration 121050, lr = 0.01
I0523 02:25:05.281371 35003 solver.cpp:239] Iteration 121060 (2.16555 iter/s, 4.61777s/10 iters), loss = 6.14983
I0523 02:25:05.281409 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14983 (* 1 = 6.14983 loss)
I0523 02:25:05.294812 35003 sgd_solver.cpp:112] Iteration 121060, lr = 0.01
I0523 02:25:10.270210 35003 solver.cpp:239] Iteration 121070 (2.00457 iter/s, 4.9886s/10 iters), loss = 7.11453
I0523 02:25:10.270464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11453 (* 1 = 7.11453 loss)
I0523 02:25:10.281862 35003 sgd_solver.cpp:112] Iteration 121070, lr = 0.01
I0523 02:25:13.848868 35003 solver.cpp:239] Iteration 121080 (2.79465 iter/s, 3.57826s/10 iters), loss = 6.57523
I0523 02:25:13.848920 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57523 (* 1 = 6.57523 loss)
I0523 02:25:14.564383 35003 sgd_solver.cpp:112] Iteration 121080, lr = 0.01
I0523 02:25:17.782280 35003 solver.cpp:239] Iteration 121090 (2.54247 iter/s, 3.93318s/10 iters), loss = 6.16648
I0523 02:25:17.782325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16648 (* 1 = 6.16648 loss)
I0523 02:25:18.460672 35003 sgd_solver.cpp:112] Iteration 121090, lr = 0.01
I0523 02:25:21.902225 35003 solver.cpp:239] Iteration 121100 (2.42735 iter/s, 4.11972s/10 iters), loss = 7.563
I0523 02:25:21.902279 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.563 (* 1 = 7.563 loss)
I0523 02:25:21.910925 35003 sgd_solver.cpp:112] Iteration 121100, lr = 0.01
I0523 02:25:24.268291 35003 solver.cpp:239] Iteration 121110 (4.2267 iter/s, 2.36591s/10 iters), loss = 6.16087
I0523 02:25:24.268342 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16087 (* 1 = 6.16087 loss)
I0523 02:25:24.280709 35003 sgd_solver.cpp:112] Iteration 121110, lr = 0.01
I0523 02:25:28.048342 35003 solver.cpp:239] Iteration 121120 (2.64563 iter/s, 3.77982s/10 iters), loss = 7.34209
I0523 02:25:28.048386 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34209 (* 1 = 7.34209 loss)
I0523 02:25:28.510011 35003 sgd_solver.cpp:112] Iteration 121120, lr = 0.01
I0523 02:25:31.174120 35003 solver.cpp:239] Iteration 121130 (3.19939 iter/s, 3.1256s/10 iters), loss = 6.8112
I0523 02:25:31.174170 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8112 (* 1 = 6.8112 loss)
I0523 02:25:31.893504 35003 sgd_solver.cpp:112] Iteration 121130, lr = 0.01
I0523 02:25:34.738180 35003 solver.cpp:239] Iteration 121140 (2.80594 iter/s, 3.56386s/10 iters), loss = 6.71805
I0523 02:25:34.738225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71805 (* 1 = 6.71805 loss)
I0523 02:25:34.750730 35003 sgd_solver.cpp:112] Iteration 121140, lr = 0.01
I0523 02:25:37.424818 35003 solver.cpp:239] Iteration 121150 (3.72235 iter/s, 2.68647s/10 iters), loss = 7.40343
I0523 02:25:37.424870 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40343 (* 1 = 7.40343 loss)
I0523 02:25:37.434180 35003 sgd_solver.cpp:112] Iteration 121150, lr = 0.01
I0523 02:25:40.837919 35003 solver.cpp:239] Iteration 121160 (2.93005 iter/s, 3.41291s/10 iters), loss = 7.6793
I0523 02:25:40.838129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6793 (* 1 = 7.6793 loss)
I0523 02:25:40.844702 35003 sgd_solver.cpp:112] Iteration 121160, lr = 0.01
I0523 02:25:42.983319 35003 solver.cpp:239] Iteration 121170 (4.66177 iter/s, 2.14511s/10 iters), loss = 7.79315
I0523 02:25:42.983361 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79315 (* 1 = 7.79315 loss)
I0523 02:25:43.644949 35003 sgd_solver.cpp:112] Iteration 121170, lr = 0.01
I0523 02:25:47.893973 35003 solver.cpp:239] Iteration 121180 (2.03649 iter/s, 4.91041s/10 iters), loss = 8.02211
I0523 02:25:47.894026 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02211 (* 1 = 8.02211 loss)
I0523 02:25:47.902400 35003 sgd_solver.cpp:112] Iteration 121180, lr = 0.01
I0523 02:25:52.197609 35003 solver.cpp:239] Iteration 121190 (2.32374 iter/s, 4.30341s/10 iters), loss = 7.4487
I0523 02:25:52.197654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4487 (* 1 = 7.4487 loss)
I0523 02:25:52.210682 35003 sgd_solver.cpp:112] Iteration 121190, lr = 0.01
I0523 02:25:55.834187 35003 solver.cpp:239] Iteration 121200 (2.74999 iter/s, 3.63638s/10 iters), loss = 7.16171
I0523 02:25:55.834233 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16171 (* 1 = 7.16171 loss)
I0523 02:25:55.843871 35003 sgd_solver.cpp:112] Iteration 121200, lr = 0.01
I0523 02:26:00.452944 35003 solver.cpp:239] Iteration 121210 (2.1652 iter/s, 4.61851s/10 iters), loss = 8.16068
I0523 02:26:00.453004 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16068 (* 1 = 8.16068 loss)
I0523 02:26:00.517727 35003 sgd_solver.cpp:112] Iteration 121210, lr = 0.01
I0523 02:26:03.990741 35003 solver.cpp:239] Iteration 121220 (2.82681 iter/s, 3.53755s/10 iters), loss = 6.99461
I0523 02:26:03.990805 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99461 (* 1 = 6.99461 loss)
I0523 02:26:04.047868 35003 sgd_solver.cpp:112] Iteration 121220, lr = 0.01
I0523 02:26:06.980362 35003 solver.cpp:239] Iteration 121230 (3.34512 iter/s, 2.98943s/10 iters), loss = 6.52477
I0523 02:26:06.980422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52477 (* 1 = 6.52477 loss)
I0523 02:26:07.714884 35003 sgd_solver.cpp:112] Iteration 121230, lr = 0.01
I0523 02:26:12.672415 35003 solver.cpp:239] Iteration 121240 (1.75692 iter/s, 5.69176s/10 iters), loss = 7.02354
I0523 02:26:12.672602 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02354 (* 1 = 7.02354 loss)
I0523 02:26:12.827710 35003 sgd_solver.cpp:112] Iteration 121240, lr = 0.01
I0523 02:26:15.347820 35003 solver.cpp:239] Iteration 121250 (3.73817 iter/s, 2.6751s/10 iters), loss = 8.35211
I0523 02:26:15.347869 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.35211 (* 1 = 8.35211 loss)
I0523 02:26:15.365144 35003 sgd_solver.cpp:112] Iteration 121250, lr = 0.01
I0523 02:26:18.778610 35003 solver.cpp:239] Iteration 121260 (2.91494 iter/s, 3.4306s/10 iters), loss = 7.50449
I0523 02:26:18.778654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50449 (* 1 = 7.50449 loss)
I0523 02:26:19.481159 35003 sgd_solver.cpp:112] Iteration 121260, lr = 0.01
I0523 02:26:21.710505 35003 solver.cpp:239] Iteration 121270 (3.41096 iter/s, 2.93173s/10 iters), loss = 6.82982
I0523 02:26:21.710542 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82982 (* 1 = 6.82982 loss)
I0523 02:26:21.718056 35003 sgd_solver.cpp:112] Iteration 121270, lr = 0.01
I0523 02:26:23.864897 35003 solver.cpp:239] Iteration 121280 (4.64197 iter/s, 2.15426s/10 iters), loss = 6.75028
I0523 02:26:23.864951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75028 (* 1 = 6.75028 loss)
I0523 02:26:24.579751 35003 sgd_solver.cpp:112] Iteration 121280, lr = 0.01
I0523 02:26:28.152912 35003 solver.cpp:239] Iteration 121290 (2.33221 iter/s, 4.28779s/10 iters), loss = 7.73262
I0523 02:26:28.152963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73262 (* 1 = 7.73262 loss)
I0523 02:26:28.164579 35003 sgd_solver.cpp:112] Iteration 121290, lr = 0.01
I0523 02:26:30.971885 35003 solver.cpp:239] Iteration 121300 (3.54761 iter/s, 2.8188s/10 iters), loss = 6.3013
I0523 02:26:30.971936 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3013 (* 1 = 6.3013 loss)
I0523 02:26:30.978714 35003 sgd_solver.cpp:112] Iteration 121300, lr = 0.01
I0523 02:26:34.532459 35003 solver.cpp:239] Iteration 121310 (2.8087 iter/s, 3.56037s/10 iters), loss = 5.51821
I0523 02:26:34.532502 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.51821 (* 1 = 5.51821 loss)
I0523 02:26:34.545349 35003 sgd_solver.cpp:112] Iteration 121310, lr = 0.01
I0523 02:26:37.354355 35003 solver.cpp:239] Iteration 121320 (3.54392 iter/s, 2.82173s/10 iters), loss = 7.27837
I0523 02:26:37.354394 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27837 (* 1 = 7.27837 loss)
I0523 02:26:37.840839 35003 sgd_solver.cpp:112] Iteration 121320, lr = 0.01
I0523 02:26:39.551540 35003 solver.cpp:239] Iteration 121330 (4.55156 iter/s, 2.19705s/10 iters), loss = 7.39282
I0523 02:26:39.551585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39282 (* 1 = 7.39282 loss)
I0523 02:26:40.281469 35003 sgd_solver.cpp:112] Iteration 121330, lr = 0.01
I0523 02:26:43.768396 35003 solver.cpp:239] Iteration 121340 (2.37156 iter/s, 4.21664s/10 iters), loss = 7.59819
I0523 02:26:43.768662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59819 (* 1 = 7.59819 loss)
I0523 02:26:43.786926 35003 sgd_solver.cpp:112] Iteration 121340, lr = 0.01
I0523 02:26:46.083271 35003 solver.cpp:239] Iteration 121350 (4.32053 iter/s, 2.31453s/10 iters), loss = 6.94255
I0523 02:26:46.083305 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94255 (* 1 = 6.94255 loss)
I0523 02:26:46.095829 35003 sgd_solver.cpp:112] Iteration 121350, lr = 0.01
I0523 02:26:49.548825 35003 solver.cpp:239] Iteration 121360 (2.88569 iter/s, 3.46537s/10 iters), loss = 7.10836
I0523 02:26:49.548880 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10836 (* 1 = 7.10836 loss)
I0523 02:26:50.233135 35003 sgd_solver.cpp:112] Iteration 121360, lr = 0.01
I0523 02:26:53.257011 35003 solver.cpp:239] Iteration 121370 (2.69689 iter/s, 3.70797s/10 iters), loss = 7.56082
I0523 02:26:53.257062 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56082 (* 1 = 7.56082 loss)
I0523 02:26:53.953728 35003 sgd_solver.cpp:112] Iteration 121370, lr = 0.01
I0523 02:26:56.728337 35003 solver.cpp:239] Iteration 121380 (2.88091 iter/s, 3.47113s/10 iters), loss = 7.86924
I0523 02:26:56.728384 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86924 (* 1 = 7.86924 loss)
I0523 02:26:56.731564 35003 sgd_solver.cpp:112] Iteration 121380, lr = 0.01
I0523 02:27:00.457062 35003 solver.cpp:239] Iteration 121390 (2.68203 iter/s, 3.72852s/10 iters), loss = 6.55179
I0523 02:27:00.457109 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55179 (* 1 = 6.55179 loss)
I0523 02:27:00.463763 35003 sgd_solver.cpp:112] Iteration 121390, lr = 0.01
I0523 02:27:03.226074 35003 solver.cpp:239] Iteration 121400 (3.61161 iter/s, 2.76884s/10 iters), loss = 7.21292
I0523 02:27:03.226116 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21292 (* 1 = 7.21292 loss)
I0523 02:27:03.244849 35003 sgd_solver.cpp:112] Iteration 121400, lr = 0.01
I0523 02:27:06.589751 35003 solver.cpp:239] Iteration 121410 (2.9731 iter/s, 3.36349s/10 iters), loss = 7.19195
I0523 02:27:06.589788 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19195 (* 1 = 7.19195 loss)
I0523 02:27:06.597885 35003 sgd_solver.cpp:112] Iteration 121410, lr = 0.01
I0523 02:27:09.407451 35003 solver.cpp:239] Iteration 121420 (3.54919 iter/s, 2.81754s/10 iters), loss = 6.47502
I0523 02:27:09.407495 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47502 (* 1 = 6.47502 loss)
I0523 02:27:10.146756 35003 sgd_solver.cpp:112] Iteration 121420, lr = 0.01
I0523 02:27:11.536088 35003 solver.cpp:239] Iteration 121430 (4.69815 iter/s, 2.1285s/10 iters), loss = 6.56696
I0523 02:27:11.536139 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56696 (* 1 = 6.56696 loss)
I0523 02:27:12.266466 35003 sgd_solver.cpp:112] Iteration 121430, lr = 0.01
I0523 02:27:15.405772 35003 solver.cpp:239] Iteration 121440 (2.58433 iter/s, 3.86947s/10 iters), loss = 6.70239
I0523 02:27:15.406008 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70239 (* 1 = 6.70239 loss)
I0523 02:27:15.415323 35003 sgd_solver.cpp:112] Iteration 121440, lr = 0.01
I0523 02:27:18.960795 35003 solver.cpp:239] Iteration 121450 (2.81323 iter/s, 3.55463s/10 iters), loss = 6.45082
I0523 02:27:18.960844 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45082 (* 1 = 6.45082 loss)
I0523 02:27:18.966244 35003 sgd_solver.cpp:112] Iteration 121450, lr = 0.01
I0523 02:27:22.693122 35003 solver.cpp:239] Iteration 121460 (2.67944 iter/s, 3.73212s/10 iters), loss = 6.79235
I0523 02:27:22.693173 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79235 (* 1 = 6.79235 loss)
I0523 02:27:22.702289 35003 sgd_solver.cpp:112] Iteration 121460, lr = 0.01
I0523 02:27:27.057592 35003 solver.cpp:239] Iteration 121470 (2.29135 iter/s, 4.36424s/10 iters), loss = 7.63265
I0523 02:27:27.057632 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63265 (* 1 = 7.63265 loss)
I0523 02:27:27.070957 35003 sgd_solver.cpp:112] Iteration 121470, lr = 0.01
I0523 02:27:32.907117 35003 solver.cpp:239] Iteration 121480 (1.70962 iter/s, 5.84925s/10 iters), loss = 7.15351
I0523 02:27:32.907174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15351 (* 1 = 7.15351 loss)
I0523 02:27:32.911267 35003 sgd_solver.cpp:112] Iteration 121480, lr = 0.01
I0523 02:27:35.888054 35003 solver.cpp:239] Iteration 121490 (3.35486 iter/s, 2.98075s/10 iters), loss = 7.59751
I0523 02:27:35.888113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59751 (* 1 = 7.59751 loss)
I0523 02:27:36.622470 35003 sgd_solver.cpp:112] Iteration 121490, lr = 0.01
I0523 02:27:40.173807 35003 solver.cpp:239] Iteration 121500 (2.33344 iter/s, 4.28552s/10 iters), loss = 6.13434
I0523 02:27:40.173848 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13434 (* 1 = 6.13434 loss)
I0523 02:27:40.179585 35003 sgd_solver.cpp:112] Iteration 121500, lr = 0.01
I0523 02:27:43.459079 35003 solver.cpp:239] Iteration 121510 (3.04405 iter/s, 3.28509s/10 iters), loss = 7.84821
I0523 02:27:43.459121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84821 (* 1 = 7.84821 loss)
I0523 02:27:43.480489 35003 sgd_solver.cpp:112] Iteration 121510, lr = 0.01
I0523 02:27:47.745455 35003 solver.cpp:239] Iteration 121520 (2.3331 iter/s, 4.28615s/10 iters), loss = 7.5029
I0523 02:27:47.745683 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5029 (* 1 = 7.5029 loss)
I0523 02:27:48.039103 35003 sgd_solver.cpp:112] Iteration 121520, lr = 0.01
I0523 02:27:51.481695 35003 solver.cpp:239] Iteration 121530 (2.67675 iter/s, 3.73588s/10 iters), loss = 6.2596
I0523 02:27:51.481741 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2596 (* 1 = 6.2596 loss)
I0523 02:27:51.549012 35003 sgd_solver.cpp:112] Iteration 121530, lr = 0.01
I0523 02:27:55.070672 35003 solver.cpp:239] Iteration 121540 (2.78647 iter/s, 3.58877s/10 iters), loss = 6.86408
I0523 02:27:55.070763 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86408 (* 1 = 6.86408 loss)
I0523 02:27:55.076308 35003 sgd_solver.cpp:112] Iteration 121540, lr = 0.01
I0523 02:27:57.867550 35003 solver.cpp:239] Iteration 121550 (3.57568 iter/s, 2.79667s/10 iters), loss = 6.05837
I0523 02:27:57.867594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05837 (* 1 = 6.05837 loss)
I0523 02:27:58.511270 35003 sgd_solver.cpp:112] Iteration 121550, lr = 0.01
I0523 02:27:59.960546 35003 solver.cpp:239] Iteration 121560 (4.77814 iter/s, 2.09286s/10 iters), loss = 6.80468
I0523 02:27:59.960594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80468 (* 1 = 6.80468 loss)
I0523 02:28:00.636875 35003 sgd_solver.cpp:112] Iteration 121560, lr = 0.01
I0523 02:28:02.860071 35003 solver.cpp:239] Iteration 121570 (3.44904 iter/s, 2.89936s/10 iters), loss = 7.34126
I0523 02:28:02.860116 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34126 (* 1 = 7.34126 loss)
I0523 02:28:03.582129 35003 sgd_solver.cpp:112] Iteration 121570, lr = 0.01
I0523 02:28:07.328279 35003 solver.cpp:239] Iteration 121580 (2.23815 iter/s, 4.46797s/10 iters), loss = 6.95549
I0523 02:28:07.328328 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95549 (* 1 = 6.95549 loss)
I0523 02:28:07.334970 35003 sgd_solver.cpp:112] Iteration 121580, lr = 0.01
I0523 02:28:11.735025 35003 solver.cpp:239] Iteration 121590 (2.26936 iter/s, 4.40652s/10 iters), loss = 6.94478
I0523 02:28:11.735070 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94478 (* 1 = 6.94478 loss)
I0523 02:28:12.461843 35003 sgd_solver.cpp:112] Iteration 121590, lr = 0.01
I0523 02:28:16.487423 35003 solver.cpp:239] Iteration 121600 (2.10431 iter/s, 4.75216s/10 iters), loss = 7.33288
I0523 02:28:16.487466 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33288 (* 1 = 7.33288 loss)
I0523 02:28:16.496881 35003 sgd_solver.cpp:112] Iteration 121600, lr = 0.01
I0523 02:28:19.200628 35003 solver.cpp:239] Iteration 121610 (3.6859 iter/s, 2.71304s/10 iters), loss = 7.37595
I0523 02:28:19.200887 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37595 (* 1 = 7.37595 loss)
I0523 02:28:19.882216 35003 sgd_solver.cpp:112] Iteration 121610, lr = 0.01
I0523 02:28:22.711648 35003 solver.cpp:239] Iteration 121620 (2.84849 iter/s, 3.51064s/10 iters), loss = 7.07955
I0523 02:28:22.711691 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07955 (* 1 = 7.07955 loss)
I0523 02:28:22.719141 35003 sgd_solver.cpp:112] Iteration 121620, lr = 0.01
I0523 02:28:25.139463 35003 solver.cpp:239] Iteration 121630 (4.11918 iter/s, 2.42767s/10 iters), loss = 6.50409
I0523 02:28:25.139508 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50409 (* 1 = 6.50409 loss)
I0523 02:28:25.873721 35003 sgd_solver.cpp:112] Iteration 121630, lr = 0.01
I0523 02:28:29.445696 35003 solver.cpp:239] Iteration 121640 (2.32235 iter/s, 4.30599s/10 iters), loss = 7.61607
I0523 02:28:29.445744 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61607 (* 1 = 7.61607 loss)
I0523 02:28:29.450883 35003 sgd_solver.cpp:112] Iteration 121640, lr = 0.01
I0523 02:28:33.057276 35003 solver.cpp:239] Iteration 121650 (2.76904 iter/s, 3.61136s/10 iters), loss = 7.14142
I0523 02:28:33.057327 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14142 (* 1 = 7.14142 loss)
I0523 02:28:33.071473 35003 sgd_solver.cpp:112] Iteration 121650, lr = 0.01
I0523 02:28:36.959442 35003 solver.cpp:239] Iteration 121660 (2.56283 iter/s, 3.90194s/10 iters), loss = 7.46073
I0523 02:28:36.959484 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46073 (* 1 = 7.46073 loss)
I0523 02:28:36.972270 35003 sgd_solver.cpp:112] Iteration 121660, lr = 0.01
I0523 02:28:40.613695 35003 solver.cpp:239] Iteration 121670 (2.73669 iter/s, 3.65405s/10 iters), loss = 7.71733
I0523 02:28:40.613739 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71733 (* 1 = 7.71733 loss)
I0523 02:28:40.627068 35003 sgd_solver.cpp:112] Iteration 121670, lr = 0.01
I0523 02:28:43.391225 35003 solver.cpp:239] Iteration 121680 (3.60053 iter/s, 2.77737s/10 iters), loss = 7.13427
I0523 02:28:43.391265 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13427 (* 1 = 7.13427 loss)
I0523 02:28:43.396270 35003 sgd_solver.cpp:112] Iteration 121680, lr = 0.01
I0523 02:28:47.672443 35003 solver.cpp:239] Iteration 121690 (2.33591 iter/s, 4.28099s/10 iters), loss = 7.1414
I0523 02:28:47.672498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1414 (* 1 = 7.1414 loss)
I0523 02:28:47.715859 35003 sgd_solver.cpp:112] Iteration 121690, lr = 0.01
I0523 02:28:51.987862 35003 solver.cpp:239] Iteration 121700 (2.3174 iter/s, 4.31519s/10 iters), loss = 7.69999
I0523 02:28:51.988077 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69999 (* 1 = 7.69999 loss)
I0523 02:28:52.104931 35003 sgd_solver.cpp:112] Iteration 121700, lr = 0.01
I0523 02:28:55.177947 35003 solver.cpp:239] Iteration 121710 (3.13505 iter/s, 3.18974s/10 iters), loss = 8.65004
I0523 02:28:55.177994 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.65004 (* 1 = 8.65004 loss)
I0523 02:28:55.905835 35003 sgd_solver.cpp:112] Iteration 121710, lr = 0.01
I0523 02:28:58.959666 35003 solver.cpp:239] Iteration 121720 (2.64444 iter/s, 3.78151s/10 iters), loss = 7.04575
I0523 02:28:58.959714 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04575 (* 1 = 7.04575 loss)
I0523 02:28:59.701264 35003 sgd_solver.cpp:112] Iteration 121720, lr = 0.01
I0523 02:29:02.553663 35003 solver.cpp:239] Iteration 121730 (2.78257 iter/s, 3.5938s/10 iters), loss = 6.75131
I0523 02:29:02.553712 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75131 (* 1 = 6.75131 loss)
I0523 02:29:02.562153 35003 sgd_solver.cpp:112] Iteration 121730, lr = 0.01
I0523 02:29:06.633538 35003 solver.cpp:239] Iteration 121740 (2.45119 iter/s, 4.07966s/10 iters), loss = 7.41176
I0523 02:29:06.633594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41176 (* 1 = 7.41176 loss)
I0523 02:29:06.639472 35003 sgd_solver.cpp:112] Iteration 121740, lr = 0.01
I0523 02:29:09.484591 35003 solver.cpp:239] Iteration 121750 (3.50771 iter/s, 2.85086s/10 iters), loss = 6.58246
I0523 02:29:09.484645 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58246 (* 1 = 6.58246 loss)
I0523 02:29:09.488845 35003 sgd_solver.cpp:112] Iteration 121750, lr = 0.01
I0523 02:29:12.433934 35003 solver.cpp:239] Iteration 121760 (3.39079 iter/s, 2.94916s/10 iters), loss = 6.48534
I0523 02:29:12.433975 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48534 (* 1 = 6.48534 loss)
I0523 02:29:12.437819 35003 sgd_solver.cpp:112] Iteration 121760, lr = 0.01
I0523 02:29:15.983814 35003 solver.cpp:239] Iteration 121770 (2.81715 iter/s, 3.54968s/10 iters), loss = 7.1428
I0523 02:29:15.983867 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1428 (* 1 = 7.1428 loss)
I0523 02:29:16.003104 35003 sgd_solver.cpp:112] Iteration 121770, lr = 0.01
I0523 02:29:18.744320 35003 solver.cpp:239] Iteration 121780 (3.62275 iter/s, 2.76034s/10 iters), loss = 6.7243
I0523 02:29:18.744359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7243 (* 1 = 6.7243 loss)
I0523 02:29:18.751827 35003 sgd_solver.cpp:112] Iteration 121780, lr = 0.01
I0523 02:29:21.469573 35003 solver.cpp:239] Iteration 121790 (3.66959 iter/s, 2.7251s/10 iters), loss = 7.09138
I0523 02:29:21.469621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09138 (* 1 = 7.09138 loss)
I0523 02:29:21.499764 35003 sgd_solver.cpp:112] Iteration 121790, lr = 0.01
I0523 02:29:26.007932 35003 solver.cpp:239] Iteration 121800 (2.20355 iter/s, 4.53813s/10 iters), loss = 6.77935
I0523 02:29:26.009215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77935 (* 1 = 6.77935 loss)
I0523 02:29:26.016917 35003 sgd_solver.cpp:112] Iteration 121800, lr = 0.01
I0523 02:29:29.671262 35003 solver.cpp:239] Iteration 121810 (2.73083 iter/s, 3.66189s/10 iters), loss = 7.31343
I0523 02:29:29.671305 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31343 (* 1 = 7.31343 loss)
I0523 02:29:29.696089 35003 sgd_solver.cpp:112] Iteration 121810, lr = 0.01
I0523 02:29:33.778283 35003 solver.cpp:239] Iteration 121820 (2.43498 iter/s, 4.10681s/10 iters), loss = 6.84664
I0523 02:29:33.778327 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84664 (* 1 = 6.84664 loss)
I0523 02:29:33.789414 35003 sgd_solver.cpp:112] Iteration 121820, lr = 0.01
I0523 02:29:38.499282 35003 solver.cpp:239] Iteration 121830 (2.11832 iter/s, 4.72071s/10 iters), loss = 7.2378
I0523 02:29:38.499339 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2378 (* 1 = 7.2378 loss)
I0523 02:29:38.510473 35003 sgd_solver.cpp:112] Iteration 121830, lr = 0.01
I0523 02:29:42.147482 35003 solver.cpp:239] Iteration 121840 (2.74123 iter/s, 3.648s/10 iters), loss = 7.08961
I0523 02:29:42.147526 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08961 (* 1 = 7.08961 loss)
I0523 02:29:42.888895 35003 sgd_solver.cpp:112] Iteration 121840, lr = 0.01
I0523 02:29:47.286461 35003 solver.cpp:239] Iteration 121850 (1.94601 iter/s, 5.13873s/10 iters), loss = 6.91371
I0523 02:29:47.286518 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91371 (* 1 = 6.91371 loss)
I0523 02:29:48.019104 35003 sgd_solver.cpp:112] Iteration 121850, lr = 0.01
I0523 02:29:50.728662 35003 solver.cpp:239] Iteration 121860 (2.90529 iter/s, 3.44199s/10 iters), loss = 7.38569
I0523 02:29:50.728720 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38569 (* 1 = 7.38569 loss)
I0523 02:29:50.767403 35003 sgd_solver.cpp:112] Iteration 121860, lr = 0.01
I0523 02:29:55.224156 35003 solver.cpp:239] Iteration 121870 (2.22457 iter/s, 4.49525s/10 iters), loss = 6.76807
I0523 02:29:55.224215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76807 (* 1 = 6.76807 loss)
I0523 02:29:55.236671 35003 sgd_solver.cpp:112] Iteration 121870, lr = 0.01
I0523 02:29:58.143524 35003 solver.cpp:239] Iteration 121880 (3.42561 iter/s, 2.91919s/10 iters), loss = 6.17283
I0523 02:29:58.143774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17283 (* 1 = 6.17283 loss)
I0523 02:29:58.348712 35003 sgd_solver.cpp:112] Iteration 121880, lr = 0.01
I0523 02:30:03.907619 35003 solver.cpp:239] Iteration 121890 (1.73502 iter/s, 5.76363s/10 iters), loss = 7.73099
I0523 02:30:03.907658 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73099 (* 1 = 7.73099 loss)
I0523 02:30:03.933507 35003 sgd_solver.cpp:112] Iteration 121890, lr = 0.01
I0523 02:30:06.011999 35003 solver.cpp:239] Iteration 121900 (4.75231 iter/s, 2.10424s/10 iters), loss = 8.2327
I0523 02:30:06.012054 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.2327 (* 1 = 8.2327 loss)
I0523 02:30:06.022120 35003 sgd_solver.cpp:112] Iteration 121900, lr = 0.01
I0523 02:30:09.576813 35003 solver.cpp:239] Iteration 121910 (2.80535 iter/s, 3.56461s/10 iters), loss = 6.72413
I0523 02:30:09.576853 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72413 (* 1 = 6.72413 loss)
I0523 02:30:09.590072 35003 sgd_solver.cpp:112] Iteration 121910, lr = 0.01
I0523 02:30:12.545454 35003 solver.cpp:239] Iteration 121920 (3.36873 iter/s, 2.96848s/10 iters), loss = 7.72633
I0523 02:30:12.545492 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72633 (* 1 = 7.72633 loss)
I0523 02:30:12.556013 35003 sgd_solver.cpp:112] Iteration 121920, lr = 0.01
I0523 02:30:15.827200 35003 solver.cpp:239] Iteration 121930 (3.04733 iter/s, 3.28156s/10 iters), loss = 7.64549
I0523 02:30:15.827247 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64549 (* 1 = 7.64549 loss)
I0523 02:30:15.846956 35003 sgd_solver.cpp:112] Iteration 121930, lr = 0.01
I0523 02:30:19.409368 35003 solver.cpp:239] Iteration 121940 (2.79176 iter/s, 3.58197s/10 iters), loss = 7.64815
I0523 02:30:19.409412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64815 (* 1 = 7.64815 loss)
I0523 02:30:20.043525 35003 sgd_solver.cpp:112] Iteration 121940, lr = 0.01
I0523 02:30:23.332547 35003 solver.cpp:239] Iteration 121950 (2.54908 iter/s, 3.92298s/10 iters), loss = 6.0539
I0523 02:30:23.332592 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0539 (* 1 = 6.0539 loss)
I0523 02:30:23.357707 35003 sgd_solver.cpp:112] Iteration 121950, lr = 0.01
I0523 02:30:26.582535 35003 solver.cpp:239] Iteration 121960 (3.07711 iter/s, 3.2498s/10 iters), loss = 6.39622
I0523 02:30:26.582593 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39622 (* 1 = 6.39622 loss)
I0523 02:30:27.297684 35003 sgd_solver.cpp:112] Iteration 121960, lr = 0.01
I0523 02:30:29.624243 35003 solver.cpp:239] Iteration 121970 (3.28783 iter/s, 3.04152s/10 iters), loss = 7.31743
I0523 02:30:29.624481 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31743 (* 1 = 7.31743 loss)
I0523 02:30:29.643136 35003 sgd_solver.cpp:112] Iteration 121970, lr = 0.01
I0523 02:30:32.370190 35003 solver.cpp:239] Iteration 121980 (3.64217 iter/s, 2.74562s/10 iters), loss = 6.85427
I0523 02:30:32.370251 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85427 (* 1 = 6.85427 loss)
I0523 02:30:32.376945 35003 sgd_solver.cpp:112] Iteration 121980, lr = 0.01
I0523 02:30:35.911350 35003 solver.cpp:239] Iteration 121990 (2.8241 iter/s, 3.54096s/10 iters), loss = 7.70598
I0523 02:30:35.911391 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70598 (* 1 = 7.70598 loss)
I0523 02:30:35.916172 35003 sgd_solver.cpp:112] Iteration 121990, lr = 0.01
I0523 02:30:37.244804 35003 solver.cpp:239] Iteration 122000 (7.49995 iter/s, 1.33334s/10 iters), loss = 6.33943
I0523 02:30:37.244843 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33943 (* 1 = 6.33943 loss)
I0523 02:30:37.257093 35003 sgd_solver.cpp:112] Iteration 122000, lr = 0.01
I0523 02:30:39.233326 35003 solver.cpp:239] Iteration 122010 (5.02923 iter/s, 1.98837s/10 iters), loss = 6.71104
I0523 02:30:39.233379 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71104 (* 1 = 6.71104 loss)
I0523 02:30:39.238813 35003 sgd_solver.cpp:112] Iteration 122010, lr = 0.01
I0523 02:30:41.917117 35003 solver.cpp:239] Iteration 122020 (3.72631 iter/s, 2.68362s/10 iters), loss = 5.86269
I0523 02:30:41.917157 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.86269 (* 1 = 5.86269 loss)
I0523 02:30:41.925268 35003 sgd_solver.cpp:112] Iteration 122020, lr = 0.01
I0523 02:30:45.312827 35003 solver.cpp:239] Iteration 122030 (2.94505 iter/s, 3.39553s/10 iters), loss = 7.84046
I0523 02:30:45.312887 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84046 (* 1 = 7.84046 loss)
I0523 02:30:45.727478 35003 sgd_solver.cpp:112] Iteration 122030, lr = 0.01
I0523 02:30:49.885093 35003 solver.cpp:239] Iteration 122040 (2.18722 iter/s, 4.57202s/10 iters), loss = 7.75118
I0523 02:30:49.885141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75118 (* 1 = 7.75118 loss)
I0523 02:30:50.523699 35003 sgd_solver.cpp:112] Iteration 122040, lr = 0.01
I0523 02:30:54.596402 35003 solver.cpp:239] Iteration 122050 (2.12266 iter/s, 4.71106s/10 iters), loss = 5.42682
I0523 02:30:54.596441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.42682 (* 1 = 5.42682 loss)
I0523 02:30:54.609495 35003 sgd_solver.cpp:112] Iteration 122050, lr = 0.01
I0523 02:30:56.702528 35003 solver.cpp:239] Iteration 122060 (4.74835 iter/s, 2.106s/10 iters), loss = 7.42119
I0523 02:30:56.702576 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42119 (* 1 = 7.42119 loss)
I0523 02:30:57.404415 35003 sgd_solver.cpp:112] Iteration 122060, lr = 0.01
I0523 02:31:01.153439 35003 solver.cpp:239] Iteration 122070 (2.24685 iter/s, 4.45068s/10 iters), loss = 7.42401
I0523 02:31:01.153784 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42401 (* 1 = 7.42401 loss)
I0523 02:31:01.158679 35003 sgd_solver.cpp:112] Iteration 122070, lr = 0.01
I0523 02:31:03.204277 35003 solver.cpp:239] Iteration 122080 (4.877 iter/s, 2.05044s/10 iters), loss = 7.62151
I0523 02:31:03.204319 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62151 (* 1 = 7.62151 loss)
I0523 02:31:03.208397 35003 sgd_solver.cpp:112] Iteration 122080, lr = 0.01
I0523 02:31:06.094391 35003 solver.cpp:239] Iteration 122090 (3.46071 iter/s, 2.88958s/10 iters), loss = 5.38644
I0523 02:31:06.094440 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.38644 (* 1 = 5.38644 loss)
I0523 02:31:06.111519 35003 sgd_solver.cpp:112] Iteration 122090, lr = 0.01
I0523 02:31:07.896816 35003 solver.cpp:239] Iteration 122100 (5.54851 iter/s, 1.80229s/10 iters), loss = 6.77392
I0523 02:31:07.896875 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77392 (* 1 = 6.77392 loss)
I0523 02:31:08.040753 35003 sgd_solver.cpp:112] Iteration 122100, lr = 0.01
I0523 02:31:10.792439 35003 solver.cpp:239] Iteration 122110 (3.45371 iter/s, 2.89544s/10 iters), loss = 7.00109
I0523 02:31:10.792495 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00109 (* 1 = 7.00109 loss)
I0523 02:31:10.805640 35003 sgd_solver.cpp:112] Iteration 122110, lr = 0.01
I0523 02:31:14.289104 35003 solver.cpp:239] Iteration 122120 (2.86003 iter/s, 3.49647s/10 iters), loss = 6.7794
I0523 02:31:14.289152 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7794 (* 1 = 6.7794 loss)
I0523 02:31:14.295858 35003 sgd_solver.cpp:112] Iteration 122120, lr = 0.01
I0523 02:31:17.157838 35003 solver.cpp:239] Iteration 122130 (3.48606 iter/s, 2.86857s/10 iters), loss = 6.78855
I0523 02:31:17.157888 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78855 (* 1 = 6.78855 loss)
I0523 02:31:17.886380 35003 sgd_solver.cpp:112] Iteration 122130, lr = 0.01
I0523 02:31:22.234601 35003 solver.cpp:239] Iteration 122140 (1.96986 iter/s, 5.07651s/10 iters), loss = 7.31535
I0523 02:31:22.234638 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31535 (* 1 = 7.31535 loss)
I0523 02:31:22.248134 35003 sgd_solver.cpp:112] Iteration 122140, lr = 0.01
I0523 02:31:25.705724 35003 solver.cpp:239] Iteration 122150 (2.88107 iter/s, 3.47094s/10 iters), loss = 7.13151
I0523 02:31:25.705762 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13151 (* 1 = 7.13151 loss)
I0523 02:31:25.713549 35003 sgd_solver.cpp:112] Iteration 122150, lr = 0.01
I0523 02:31:30.275939 35003 solver.cpp:239] Iteration 122160 (2.18819 iter/s, 4.56999s/10 iters), loss = 8.30115
I0523 02:31:30.275986 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.30115 (* 1 = 8.30115 loss)
I0523 02:31:30.289680 35003 sgd_solver.cpp:112] Iteration 122160, lr = 0.01
I0523 02:31:33.309660 35003 solver.cpp:239] Iteration 122170 (3.29647 iter/s, 3.03355s/10 iters), loss = 7.07045
I0523 02:31:33.309808 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07045 (* 1 = 7.07045 loss)
I0523 02:31:33.323119 35003 sgd_solver.cpp:112] Iteration 122170, lr = 0.01
I0523 02:31:37.531076 35003 solver.cpp:239] Iteration 122180 (2.36905 iter/s, 4.2211s/10 iters), loss = 8.24472
I0523 02:31:37.531123 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.24472 (* 1 = 8.24472 loss)
I0523 02:31:37.542361 35003 sgd_solver.cpp:112] Iteration 122180, lr = 0.01
I0523 02:31:41.025777 35003 solver.cpp:239] Iteration 122190 (2.86164 iter/s, 3.4945s/10 iters), loss = 7.83433
I0523 02:31:41.025831 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83433 (* 1 = 7.83433 loss)
I0523 02:31:41.734767 35003 sgd_solver.cpp:112] Iteration 122190, lr = 0.01
I0523 02:31:45.355454 35003 solver.cpp:239] Iteration 122200 (2.30977 iter/s, 4.32943s/10 iters), loss = 7.34564
I0523 02:31:45.355535 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34564 (* 1 = 7.34564 loss)
I0523 02:31:46.076676 35003 sgd_solver.cpp:112] Iteration 122200, lr = 0.01
I0523 02:31:49.649291 35003 solver.cpp:239] Iteration 122210 (2.32906 iter/s, 4.29358s/10 iters), loss = 7.49816
I0523 02:31:49.649353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49816 (* 1 = 7.49816 loss)
I0523 02:31:50.345870 35003 sgd_solver.cpp:112] Iteration 122210, lr = 0.01
I0523 02:31:53.739832 35003 solver.cpp:239] Iteration 122220 (2.4448 iter/s, 4.09031s/10 iters), loss = 6.87463
I0523 02:31:53.739886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87463 (* 1 = 6.87463 loss)
I0523 02:31:53.746548 35003 sgd_solver.cpp:112] Iteration 122220, lr = 0.01
I0523 02:31:57.331636 35003 solver.cpp:239] Iteration 122230 (2.78427 iter/s, 3.59161s/10 iters), loss = 6.5118
I0523 02:31:57.331676 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5118 (* 1 = 6.5118 loss)
I0523 02:31:57.345090 35003 sgd_solver.cpp:112] Iteration 122230, lr = 0.01
I0523 02:32:00.211436 35003 solver.cpp:239] Iteration 122240 (3.47266 iter/s, 2.87964s/10 iters), loss = 7.3982
I0523 02:32:00.211480 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3982 (* 1 = 7.3982 loss)
I0523 02:32:00.952159 35003 sgd_solver.cpp:112] Iteration 122240, lr = 0.01
I0523 02:32:04.362965 35003 solver.cpp:239] Iteration 122250 (2.40888 iter/s, 4.1513s/10 iters), loss = 7.2983
I0523 02:32:04.363214 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2983 (* 1 = 7.2983 loss)
I0523 02:32:04.385036 35003 sgd_solver.cpp:112] Iteration 122250, lr = 0.01
I0523 02:32:07.114809 35003 solver.cpp:239] Iteration 122260 (3.63438 iter/s, 2.7515s/10 iters), loss = 7.71921
I0523 02:32:07.114854 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71921 (* 1 = 7.71921 loss)
I0523 02:32:07.148228 35003 sgd_solver.cpp:112] Iteration 122260, lr = 0.01
I0523 02:32:09.201052 35003 solver.cpp:239] Iteration 122270 (4.79362 iter/s, 2.0861s/10 iters), loss = 6.28285
I0523 02:32:09.201099 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28285 (* 1 = 6.28285 loss)
I0523 02:32:09.205802 35003 sgd_solver.cpp:112] Iteration 122270, lr = 0.01
I0523 02:32:12.706122 35003 solver.cpp:239] Iteration 122280 (2.85317 iter/s, 3.50487s/10 iters), loss = 8.43102
I0523 02:32:12.706177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.43102 (* 1 = 8.43102 loss)
I0523 02:32:12.719463 35003 sgd_solver.cpp:112] Iteration 122280, lr = 0.01
I0523 02:32:15.589212 35003 solver.cpp:239] Iteration 122290 (3.46871 iter/s, 2.88292s/10 iters), loss = 5.948
I0523 02:32:15.589252 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.948 (* 1 = 5.948 loss)
I0523 02:32:16.297642 35003 sgd_solver.cpp:112] Iteration 122290, lr = 0.01
I0523 02:32:18.437664 35003 solver.cpp:239] Iteration 122300 (3.51088 iter/s, 2.84829s/10 iters), loss = 6.77872
I0523 02:32:18.437708 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77872 (* 1 = 6.77872 loss)
I0523 02:32:19.179136 35003 sgd_solver.cpp:112] Iteration 122300, lr = 0.01
I0523 02:32:21.783882 35003 solver.cpp:239] Iteration 122310 (2.98862 iter/s, 3.34603s/10 iters), loss = 7.17791
I0523 02:32:21.783926 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17791 (* 1 = 7.17791 loss)
I0523 02:32:21.812546 35003 sgd_solver.cpp:112] Iteration 122310, lr = 0.01
I0523 02:32:25.407701 35003 solver.cpp:239] Iteration 122320 (2.75967 iter/s, 3.62362s/10 iters), loss = 6.48173
I0523 02:32:25.407744 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48173 (* 1 = 6.48173 loss)
I0523 02:32:25.421286 35003 sgd_solver.cpp:112] Iteration 122320, lr = 0.01
I0523 02:32:29.221906 35003 solver.cpp:239] Iteration 122330 (2.62192 iter/s, 3.814s/10 iters), loss = 7.33925
I0523 02:32:29.221961 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33925 (* 1 = 7.33925 loss)
I0523 02:32:29.240363 35003 sgd_solver.cpp:112] Iteration 122330, lr = 0.01
I0523 02:32:31.297533 35003 solver.cpp:239] Iteration 122340 (4.81816 iter/s, 2.07548s/10 iters), loss = 6.46405
I0523 02:32:31.297572 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46405 (* 1 = 6.46405 loss)
I0523 02:32:31.323456 35003 sgd_solver.cpp:112] Iteration 122340, lr = 0.01
I0523 02:32:36.219812 35003 solver.cpp:239] Iteration 122350 (2.03168 iter/s, 4.92204s/10 iters), loss = 6.40852
I0523 02:32:36.219964 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40852 (* 1 = 6.40852 loss)
I0523 02:32:36.381711 35003 sgd_solver.cpp:112] Iteration 122350, lr = 0.01
I0523 02:32:39.212872 35003 solver.cpp:239] Iteration 122360 (3.34137 iter/s, 2.99278s/10 iters), loss = 7.35404
I0523 02:32:39.212918 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35404 (* 1 = 7.35404 loss)
I0523 02:32:39.226452 35003 sgd_solver.cpp:112] Iteration 122360, lr = 0.01
I0523 02:32:43.274591 35003 solver.cpp:239] Iteration 122370 (2.46214 iter/s, 4.06151s/10 iters), loss = 6.50694
I0523 02:32:43.274631 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50694 (* 1 = 6.50694 loss)
I0523 02:32:43.996486 35003 sgd_solver.cpp:112] Iteration 122370, lr = 0.01
I0523 02:32:47.662780 35003 solver.cpp:239] Iteration 122380 (2.27896 iter/s, 4.38797s/10 iters), loss = 7.6103
I0523 02:32:47.662823 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6103 (* 1 = 7.6103 loss)
I0523 02:32:47.727833 35003 sgd_solver.cpp:112] Iteration 122380, lr = 0.01
I0523 02:32:52.086500 35003 solver.cpp:239] Iteration 122390 (2.26066 iter/s, 4.4235s/10 iters), loss = 6.64594
I0523 02:32:52.086551 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64594 (* 1 = 6.64594 loss)
I0523 02:32:52.827090 35003 sgd_solver.cpp:112] Iteration 122390, lr = 0.01
I0523 02:32:56.592648 35003 solver.cpp:239] Iteration 122400 (2.21931 iter/s, 4.50591s/10 iters), loss = 7.0581
I0523 02:32:56.592694 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0581 (* 1 = 7.0581 loss)
I0523 02:32:57.166707 35003 sgd_solver.cpp:112] Iteration 122400, lr = 0.01
I0523 02:33:01.601395 35003 solver.cpp:239] Iteration 122410 (1.99661 iter/s, 5.0085s/10 iters), loss = 6.21902
I0523 02:33:01.601439 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21902 (* 1 = 6.21902 loss)
I0523 02:33:02.333295 35003 sgd_solver.cpp:112] Iteration 122410, lr = 0.01
I0523 02:33:05.893537 35003 solver.cpp:239] Iteration 122420 (2.32996 iter/s, 4.29192s/10 iters), loss = 7.62061
I0523 02:33:05.893579 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62061 (* 1 = 7.62061 loss)
I0523 02:33:05.906051 35003 sgd_solver.cpp:112] Iteration 122420, lr = 0.01
I0523 02:33:09.390913 35003 solver.cpp:239] Iteration 122430 (2.85945 iter/s, 3.49718s/10 iters), loss = 6.7525
I0523 02:33:09.391232 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7525 (* 1 = 6.7525 loss)
I0523 02:33:10.086760 35003 sgd_solver.cpp:112] Iteration 122430, lr = 0.01
I0523 02:33:12.900003 35003 solver.cpp:239] Iteration 122440 (2.8501 iter/s, 3.50866s/10 iters), loss = 7.29666
I0523 02:33:12.900038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29666 (* 1 = 7.29666 loss)
I0523 02:33:12.913426 35003 sgd_solver.cpp:112] Iteration 122440, lr = 0.01
I0523 02:33:15.669296 35003 solver.cpp:239] Iteration 122450 (3.61123 iter/s, 2.76914s/10 iters), loss = 6.11968
I0523 02:33:15.669340 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11968 (* 1 = 6.11968 loss)
I0523 02:33:16.399227 35003 sgd_solver.cpp:112] Iteration 122450, lr = 0.01
I0523 02:33:19.169791 35003 solver.cpp:239] Iteration 122460 (2.85689 iter/s, 3.50031s/10 iters), loss = 7.46961
I0523 02:33:19.169829 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46961 (* 1 = 7.46961 loss)
I0523 02:33:19.228094 35003 sgd_solver.cpp:112] Iteration 122460, lr = 0.01
I0523 02:33:23.520598 35003 solver.cpp:239] Iteration 122470 (2.29854 iter/s, 4.35059s/10 iters), loss = 7.73868
I0523 02:33:23.520648 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73868 (* 1 = 7.73868 loss)
I0523 02:33:23.544081 35003 sgd_solver.cpp:112] Iteration 122470, lr = 0.01
I0523 02:33:27.697816 35003 solver.cpp:239] Iteration 122480 (2.39407 iter/s, 4.17699s/10 iters), loss = 6.87564
I0523 02:33:27.697872 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87564 (* 1 = 6.87564 loss)
I0523 02:33:27.706552 35003 sgd_solver.cpp:112] Iteration 122480, lr = 0.01
I0523 02:33:32.030738 35003 solver.cpp:239] Iteration 122490 (2.30804 iter/s, 4.33269s/10 iters), loss = 7.14529
I0523 02:33:32.030786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14529 (* 1 = 7.14529 loss)
I0523 02:33:32.162503 35003 sgd_solver.cpp:112] Iteration 122490, lr = 0.01
I0523 02:33:35.592133 35003 solver.cpp:239] Iteration 122500 (2.80805 iter/s, 3.5612s/10 iters), loss = 8.15514
I0523 02:33:35.592188 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.15514 (* 1 = 8.15514 loss)
I0523 02:33:36.277546 35003 sgd_solver.cpp:112] Iteration 122500, lr = 0.01
I0523 02:33:38.311869 35003 solver.cpp:239] Iteration 122510 (3.67706 iter/s, 2.71956s/10 iters), loss = 7.10854
I0523 02:33:38.311914 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10854 (* 1 = 7.10854 loss)
I0523 02:33:38.315207 35003 sgd_solver.cpp:112] Iteration 122510, lr = 0.01
I0523 02:33:42.372324 35003 solver.cpp:239] Iteration 122520 (2.46291 iter/s, 4.06024s/10 iters), loss = 6.57745
I0523 02:33:42.372452 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57745 (* 1 = 6.57745 loss)
I0523 02:33:42.378270 35003 sgd_solver.cpp:112] Iteration 122520, lr = 0.01
I0523 02:33:47.432888 35003 solver.cpp:239] Iteration 122530 (1.9762 iter/s, 5.06022s/10 iters), loss = 5.76016
I0523 02:33:47.432950 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.76016 (* 1 = 5.76016 loss)
I0523 02:33:47.583771 35003 sgd_solver.cpp:112] Iteration 122530, lr = 0.01
I0523 02:33:49.568518 35003 solver.cpp:239] Iteration 122540 (4.6828 iter/s, 2.13547s/10 iters), loss = 7.194
I0523 02:33:49.568562 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.194 (* 1 = 7.194 loss)
I0523 02:33:49.578311 35003 sgd_solver.cpp:112] Iteration 122540, lr = 0.01
I0523 02:33:52.355439 35003 solver.cpp:239] Iteration 122550 (3.5884 iter/s, 2.78676s/10 iters), loss = 7.26322
I0523 02:33:52.355486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26322 (* 1 = 7.26322 loss)
I0523 02:33:52.365907 35003 sgd_solver.cpp:112] Iteration 122550, lr = 0.01
I0523 02:33:56.609854 35003 solver.cpp:239] Iteration 122560 (2.35062 iter/s, 4.25419s/10 iters), loss = 6.0519
I0523 02:33:56.609912 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0519 (* 1 = 6.0519 loss)
I0523 02:33:56.634851 35003 sgd_solver.cpp:112] Iteration 122560, lr = 0.01
I0523 02:34:00.747839 35003 solver.cpp:239] Iteration 122570 (2.41677 iter/s, 4.13776s/10 iters), loss = 6.72282
I0523 02:34:00.747881 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72282 (* 1 = 6.72282 loss)
I0523 02:34:00.761346 35003 sgd_solver.cpp:112] Iteration 122570, lr = 0.01
I0523 02:34:03.566072 35003 solver.cpp:239] Iteration 122580 (3.54853 iter/s, 2.81807s/10 iters), loss = 7.95858
I0523 02:34:03.566108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95858 (* 1 = 7.95858 loss)
I0523 02:34:04.268440 35003 sgd_solver.cpp:112] Iteration 122580, lr = 0.01
I0523 02:34:09.345479 35003 solver.cpp:239] Iteration 122590 (1.73036 iter/s, 5.77913s/10 iters), loss = 7.16455
I0523 02:34:09.345530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16455 (* 1 = 7.16455 loss)
I0523 02:34:09.784313 35003 sgd_solver.cpp:112] Iteration 122590, lr = 0.01
I0523 02:34:14.078900 35003 solver.cpp:239] Iteration 122600 (2.11276 iter/s, 4.73315s/10 iters), loss = 6.09927
I0523 02:34:14.079095 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09927 (* 1 = 6.09927 loss)
I0523 02:34:14.800281 35003 sgd_solver.cpp:112] Iteration 122600, lr = 0.01
I0523 02:34:16.520251 35003 solver.cpp:239] Iteration 122610 (4.0966 iter/s, 2.44105s/10 iters), loss = 5.70532
I0523 02:34:16.520298 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.70532 (* 1 = 5.70532 loss)
I0523 02:34:16.534133 35003 sgd_solver.cpp:112] Iteration 122610, lr = 0.01
I0523 02:34:17.369025 35003 solver.cpp:239] Iteration 122620 (11.7835 iter/s, 0.848647s/10 iters), loss = 7.30812
I0523 02:34:17.369102 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30812 (* 1 = 7.30812 loss)
I0523 02:34:17.374589 35003 sgd_solver.cpp:112] Iteration 122620, lr = 0.01
I0523 02:34:21.042277 35003 solver.cpp:239] Iteration 122630 (2.72255 iter/s, 3.67303s/10 iters), loss = 7.95813
I0523 02:34:21.042317 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95813 (* 1 = 7.95813 loss)
I0523 02:34:21.050459 35003 sgd_solver.cpp:112] Iteration 122630, lr = 0.01
I0523 02:34:22.885354 35003 solver.cpp:239] Iteration 122640 (5.42608 iter/s, 1.84295s/10 iters), loss = 6.6692
I0523 02:34:22.885397 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6692 (* 1 = 6.6692 loss)
I0523 02:34:22.894448 35003 sgd_solver.cpp:112] Iteration 122640, lr = 0.01
I0523 02:34:26.552834 35003 solver.cpp:239] Iteration 122650 (2.72682 iter/s, 3.66728s/10 iters), loss = 8.33523
I0523 02:34:26.552896 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.33523 (* 1 = 8.33523 loss)
I0523 02:34:26.565562 35003 sgd_solver.cpp:112] Iteration 122650, lr = 0.01
I0523 02:34:30.990051 35003 solver.cpp:239] Iteration 122660 (2.25379 iter/s, 4.43697s/10 iters), loss = 6.94364
I0523 02:34:30.990108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94364 (* 1 = 6.94364 loss)
I0523 02:34:31.601558 35003 sgd_solver.cpp:112] Iteration 122660, lr = 0.01
I0523 02:34:34.610816 35003 solver.cpp:239] Iteration 122670 (2.76201 iter/s, 3.62055s/10 iters), loss = 7.23895
I0523 02:34:34.610865 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23895 (* 1 = 7.23895 loss)
I0523 02:34:34.620973 35003 sgd_solver.cpp:112] Iteration 122670, lr = 0.01
I0523 02:34:35.906788 35003 solver.cpp:239] Iteration 122680 (7.71688 iter/s, 1.29586s/10 iters), loss = 7.98092
I0523 02:34:35.906833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98092 (* 1 = 7.98092 loss)
I0523 02:34:35.917874 35003 sgd_solver.cpp:112] Iteration 122680, lr = 0.01
I0523 02:34:38.101825 35003 solver.cpp:239] Iteration 122690 (4.55602 iter/s, 2.1949s/10 iters), loss = 6.25011
I0523 02:34:38.101862 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25011 (* 1 = 6.25011 loss)
I0523 02:34:38.843325 35003 sgd_solver.cpp:112] Iteration 122690, lr = 0.01
I0523 02:34:42.623040 35003 solver.cpp:239] Iteration 122700 (2.2119 iter/s, 4.52099s/10 iters), loss = 6.47792
I0523 02:34:42.623078 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47792 (* 1 = 6.47792 loss)
I0523 02:34:42.636242 35003 sgd_solver.cpp:112] Iteration 122700, lr = 0.01
I0523 02:34:47.075541 35003 solver.cpp:239] Iteration 122710 (2.24605 iter/s, 4.45227s/10 iters), loss = 7.26891
I0523 02:34:47.075886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26891 (* 1 = 7.26891 loss)
I0523 02:34:47.081248 35003 sgd_solver.cpp:112] Iteration 122710, lr = 0.01
I0523 02:34:51.405319 35003 solver.cpp:239] Iteration 122720 (2.30986 iter/s, 4.32927s/10 iters), loss = 6.99992
I0523 02:34:51.405360 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99992 (* 1 = 6.99992 loss)
I0523 02:34:51.430271 35003 sgd_solver.cpp:112] Iteration 122720, lr = 0.01
I0523 02:34:55.043716 35003 solver.cpp:239] Iteration 122730 (2.74862 iter/s, 3.6382s/10 iters), loss = 6.97498
I0523 02:34:55.043787 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97498 (* 1 = 6.97498 loss)
I0523 02:34:55.049751 35003 sgd_solver.cpp:112] Iteration 122730, lr = 0.01
I0523 02:34:58.445147 35003 solver.cpp:239] Iteration 122740 (2.94012 iter/s, 3.40122s/10 iters), loss = 6.27557
I0523 02:34:58.445209 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27557 (* 1 = 6.27557 loss)
I0523 02:34:59.147496 35003 sgd_solver.cpp:112] Iteration 122740, lr = 0.01
I0523 02:35:02.835278 35003 solver.cpp:239] Iteration 122750 (2.27796 iter/s, 4.38989s/10 iters), loss = 6.59797
I0523 02:35:02.835327 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59797 (* 1 = 6.59797 loss)
I0523 02:35:02.838327 35003 sgd_solver.cpp:112] Iteration 122750, lr = 0.01
I0523 02:35:05.829463 35003 solver.cpp:239] Iteration 122760 (3.34002 iter/s, 2.99399s/10 iters), loss = 7.498
I0523 02:35:05.829504 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.498 (* 1 = 7.498 loss)
I0523 02:35:05.837843 35003 sgd_solver.cpp:112] Iteration 122760, lr = 0.01
I0523 02:35:09.526785 35003 solver.cpp:239] Iteration 122770 (2.7048 iter/s, 3.69713s/10 iters), loss = 6.68494
I0523 02:35:09.526832 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68494 (* 1 = 6.68494 loss)
I0523 02:35:09.536418 35003 sgd_solver.cpp:112] Iteration 122770, lr = 0.01
I0523 02:35:12.282277 35003 solver.cpp:239] Iteration 122780 (3.62934 iter/s, 2.75532s/10 iters), loss = 7.71059
I0523 02:35:12.282332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71059 (* 1 = 7.71059 loss)
I0523 02:35:13.023598 35003 sgd_solver.cpp:112] Iteration 122780, lr = 0.01
I0523 02:35:16.632563 35003 solver.cpp:239] Iteration 122790 (2.29882 iter/s, 4.35005s/10 iters), loss = 6.69069
I0523 02:35:16.632608 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69069 (* 1 = 6.69069 loss)
I0523 02:35:16.637791 35003 sgd_solver.cpp:112] Iteration 122790, lr = 0.01
I0523 02:35:18.743805 35003 solver.cpp:239] Iteration 122800 (4.73687 iter/s, 2.1111s/10 iters), loss = 6.6415
I0523 02:35:18.744045 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6415 (* 1 = 6.6415 loss)
I0523 02:35:19.439544 35003 sgd_solver.cpp:112] Iteration 122800, lr = 0.01
I0523 02:35:22.223070 35003 solver.cpp:239] Iteration 122810 (2.87448 iter/s, 3.47889s/10 iters), loss = 6.57597
I0523 02:35:22.223119 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57597 (* 1 = 6.57597 loss)
I0523 02:35:22.236106 35003 sgd_solver.cpp:112] Iteration 122810, lr = 0.01
I0523 02:35:26.210727 35003 solver.cpp:239] Iteration 122820 (2.50787 iter/s, 3.98744s/10 iters), loss = 8.11963
I0523 02:35:26.210773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.11963 (* 1 = 8.11963 loss)
I0523 02:35:26.219822 35003 sgd_solver.cpp:112] Iteration 122820, lr = 0.01
I0523 02:35:28.116353 35003 solver.cpp:239] Iteration 122830 (5.24799 iter/s, 1.90549s/10 iters), loss = 8.1935
I0523 02:35:28.116406 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.1935 (* 1 = 8.1935 loss)
I0523 02:35:28.252848 35003 sgd_solver.cpp:112] Iteration 122830, lr = 0.01
I0523 02:35:30.569483 35003 solver.cpp:239] Iteration 122840 (4.0767 iter/s, 2.45297s/10 iters), loss = 6.5033
I0523 02:35:30.569531 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5033 (* 1 = 6.5033 loss)
I0523 02:35:30.581724 35003 sgd_solver.cpp:112] Iteration 122840, lr = 0.01
I0523 02:35:32.603055 35003 solver.cpp:239] Iteration 122850 (4.9178 iter/s, 2.03343s/10 iters), loss = 7.57549
I0523 02:35:32.603093 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57549 (* 1 = 7.57549 loss)
I0523 02:35:32.618227 35003 sgd_solver.cpp:112] Iteration 122850, lr = 0.01
I0523 02:35:34.605269 35003 solver.cpp:239] Iteration 122860 (4.99479 iter/s, 2.00209s/10 iters), loss = 7.56831
I0523 02:35:34.605309 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56831 (* 1 = 7.56831 loss)
I0523 02:35:34.615128 35003 sgd_solver.cpp:112] Iteration 122860, lr = 0.01
I0523 02:35:39.110726 35003 solver.cpp:239] Iteration 122870 (2.21965 iter/s, 4.50521s/10 iters), loss = 6.51081
I0523 02:35:39.110774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51081 (* 1 = 6.51081 loss)
I0523 02:35:39.839370 35003 sgd_solver.cpp:112] Iteration 122870, lr = 0.01
I0523 02:35:44.228723 35003 solver.cpp:239] Iteration 122880 (1.95399 iter/s, 5.11773s/10 iters), loss = 7.04488
I0523 02:35:44.228773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04488 (* 1 = 7.04488 loss)
I0523 02:35:44.964040 35003 sgd_solver.cpp:112] Iteration 122880, lr = 0.01
I0523 02:35:47.573335 35003 solver.cpp:239] Iteration 122890 (2.99006 iter/s, 3.34442s/10 iters), loss = 7.27584
I0523 02:35:47.573376 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27584 (* 1 = 7.27584 loss)
I0523 02:35:47.586472 35003 sgd_solver.cpp:112] Iteration 122890, lr = 0.01
I0523 02:35:51.224169 35003 solver.cpp:239] Iteration 122900 (2.73925 iter/s, 3.65064s/10 iters), loss = 6.6439
I0523 02:35:51.224328 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6439 (* 1 = 6.6439 loss)
I0523 02:35:51.958709 35003 sgd_solver.cpp:112] Iteration 122900, lr = 0.01
I0523 02:35:55.604298 35003 solver.cpp:239] Iteration 122910 (2.28325 iter/s, 4.37973s/10 iters), loss = 6.82885
I0523 02:35:55.604353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82885 (* 1 = 6.82885 loss)
I0523 02:35:55.632283 35003 sgd_solver.cpp:112] Iteration 122910, lr = 0.01
I0523 02:35:58.415954 35003 solver.cpp:239] Iteration 122920 (3.55684 iter/s, 2.81149s/10 iters), loss = 7.64639
I0523 02:35:58.415998 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64639 (* 1 = 7.64639 loss)
I0523 02:35:58.429416 35003 sgd_solver.cpp:112] Iteration 122920, lr = 0.01
I0523 02:36:00.295823 35003 solver.cpp:239] Iteration 122930 (5.31991 iter/s, 1.87973s/10 iters), loss = 6.79178
I0523 02:36:00.295886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79178 (* 1 = 6.79178 loss)
I0523 02:36:00.303030 35003 sgd_solver.cpp:112] Iteration 122930, lr = 0.01
I0523 02:36:02.440469 35003 solver.cpp:239] Iteration 122940 (4.66311 iter/s, 2.14449s/10 iters), loss = 7.46564
I0523 02:36:02.440505 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46564 (* 1 = 7.46564 loss)
I0523 02:36:02.449983 35003 sgd_solver.cpp:112] Iteration 122940, lr = 0.01
I0523 02:36:06.487670 35003 solver.cpp:239] Iteration 122950 (2.47097 iter/s, 4.047s/10 iters), loss = 7.73067
I0523 02:36:06.487722 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73067 (* 1 = 7.73067 loss)
I0523 02:36:06.500890 35003 sgd_solver.cpp:112] Iteration 122950, lr = 0.01
I0523 02:36:10.845561 35003 solver.cpp:239] Iteration 122960 (2.29482 iter/s, 4.35764s/10 iters), loss = 6.73021
I0523 02:36:10.845600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73021 (* 1 = 6.73021 loss)
I0523 02:36:10.848915 35003 sgd_solver.cpp:112] Iteration 122960, lr = 0.01
I0523 02:36:13.629321 35003 solver.cpp:239] Iteration 122970 (3.59248 iter/s, 2.78359s/10 iters), loss = 7.36077
I0523 02:36:13.629371 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36077 (* 1 = 7.36077 loss)
I0523 02:36:13.637100 35003 sgd_solver.cpp:112] Iteration 122970, lr = 0.01
I0523 02:36:17.191659 35003 solver.cpp:239] Iteration 122980 (2.8073 iter/s, 3.56214s/10 iters), loss = 7.57503
I0523 02:36:17.191700 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57503 (* 1 = 7.57503 loss)
I0523 02:36:17.199038 35003 sgd_solver.cpp:112] Iteration 122980, lr = 0.01
I0523 02:36:20.976658 35003 solver.cpp:239] Iteration 122990 (2.64215 iter/s, 3.78479s/10 iters), loss = 6.81479
I0523 02:36:20.976701 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81479 (* 1 = 6.81479 loss)
I0523 02:36:21.045244 35003 sgd_solver.cpp:112] Iteration 122990, lr = 0.01
I0523 02:36:24.649377 35003 solver.cpp:239] Iteration 123000 (2.72295 iter/s, 3.67249s/10 iters), loss = 7.14348
I0523 02:36:24.649566 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14348 (* 1 = 7.14348 loss)
I0523 02:36:24.668109 35003 sgd_solver.cpp:112] Iteration 123000, lr = 0.01
I0523 02:36:27.517840 35003 solver.cpp:239] Iteration 123010 (3.48657 iter/s, 2.86815s/10 iters), loss = 5.65718
I0523 02:36:27.517900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.65718 (* 1 = 5.65718 loss)
I0523 02:36:27.529783 35003 sgd_solver.cpp:112] Iteration 123010, lr = 0.01
I0523 02:36:31.036094 35003 solver.cpp:239] Iteration 123020 (2.84249 iter/s, 3.51804s/10 iters), loss = 6.34343
I0523 02:36:31.036162 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34343 (* 1 = 6.34343 loss)
I0523 02:36:31.744746 35003 sgd_solver.cpp:112] Iteration 123020, lr = 0.01
I0523 02:36:34.573235 35003 solver.cpp:239] Iteration 123030 (2.82733 iter/s, 3.5369s/10 iters), loss = 8.20749
I0523 02:36:34.573292 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.20749 (* 1 = 8.20749 loss)
I0523 02:36:34.600100 35003 sgd_solver.cpp:112] Iteration 123030, lr = 0.01
I0523 02:36:36.318771 35003 solver.cpp:239] Iteration 123040 (5.72934 iter/s, 1.7454s/10 iters), loss = 6.6828
I0523 02:36:36.318823 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6828 (* 1 = 6.6828 loss)
I0523 02:36:37.058084 35003 sgd_solver.cpp:112] Iteration 123040, lr = 0.01
I0523 02:36:40.620692 35003 solver.cpp:239] Iteration 123050 (2.32467 iter/s, 4.30169s/10 iters), loss = 6.51126
I0523 02:36:40.620735 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51126 (* 1 = 6.51126 loss)
I0523 02:36:40.624728 35003 sgd_solver.cpp:112] Iteration 123050, lr = 0.01
I0523 02:36:43.970825 35003 solver.cpp:239] Iteration 123060 (2.98513 iter/s, 3.34994s/10 iters), loss = 7.88821
I0523 02:36:43.970866 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88821 (* 1 = 7.88821 loss)
I0523 02:36:43.974581 35003 sgd_solver.cpp:112] Iteration 123060, lr = 0.01
I0523 02:36:47.334337 35003 solver.cpp:239] Iteration 123070 (2.97326 iter/s, 3.36332s/10 iters), loss = 6.85674
I0523 02:36:47.334388 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85674 (* 1 = 6.85674 loss)
I0523 02:36:47.341619 35003 sgd_solver.cpp:112] Iteration 123070, lr = 0.01
I0523 02:36:51.185575 35003 solver.cpp:239] Iteration 123080 (2.59671 iter/s, 3.85103s/10 iters), loss = 5.99714
I0523 02:36:51.185616 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99714 (* 1 = 5.99714 loss)
I0523 02:36:51.194744 35003 sgd_solver.cpp:112] Iteration 123080, lr = 0.01
I0523 02:36:53.956236 35003 solver.cpp:239] Iteration 123090 (3.60945 iter/s, 2.7705s/10 iters), loss = 6.03768
I0523 02:36:53.956277 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03768 (* 1 = 6.03768 loss)
I0523 02:36:53.969316 35003 sgd_solver.cpp:112] Iteration 123090, lr = 0.01
I0523 02:36:55.948915 35003 solver.cpp:239] Iteration 123100 (5.0187 iter/s, 1.99255s/10 iters), loss = 6.3579
I0523 02:36:55.949141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3579 (* 1 = 6.3579 loss)
I0523 02:36:55.957427 35003 sgd_solver.cpp:112] Iteration 123100, lr = 0.01
I0523 02:37:00.018486 35003 solver.cpp:239] Iteration 123110 (2.4575 iter/s, 4.06918s/10 iters), loss = 7.22726
I0523 02:37:00.018523 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22726 (* 1 = 7.22726 loss)
I0523 02:37:00.031735 35003 sgd_solver.cpp:112] Iteration 123110, lr = 0.01
I0523 02:37:02.544085 35003 solver.cpp:239] Iteration 123120 (3.95969 iter/s, 2.52545s/10 iters), loss = 6.72479
I0523 02:37:02.544128 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72479 (* 1 = 6.72479 loss)
I0523 02:37:02.552920 35003 sgd_solver.cpp:112] Iteration 123120, lr = 0.01
I0523 02:37:05.450217 35003 solver.cpp:239] Iteration 123130 (3.44121 iter/s, 2.90596s/10 iters), loss = 8.04473
I0523 02:37:05.450270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04473 (* 1 = 8.04473 loss)
I0523 02:37:06.139237 35003 sgd_solver.cpp:112] Iteration 123130, lr = 0.01
I0523 02:37:10.240384 35003 solver.cpp:239] Iteration 123140 (2.08772 iter/s, 4.78992s/10 iters), loss = 6.64059
I0523 02:37:10.240429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64059 (* 1 = 6.64059 loss)
I0523 02:37:10.254321 35003 sgd_solver.cpp:112] Iteration 123140, lr = 0.01
I0523 02:37:14.185956 35003 solver.cpp:239] Iteration 123150 (2.53462 iter/s, 3.94536s/10 iters), loss = 7.97708
I0523 02:37:14.186003 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97708 (* 1 = 7.97708 loss)
I0523 02:37:14.924608 35003 sgd_solver.cpp:112] Iteration 123150, lr = 0.01
I0523 02:37:20.151564 35003 solver.cpp:239] Iteration 123160 (1.67636 iter/s, 5.96531s/10 iters), loss = 6.13426
I0523 02:37:20.151605 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13426 (* 1 = 6.13426 loss)
I0523 02:37:20.164587 35003 sgd_solver.cpp:112] Iteration 123160, lr = 0.01
I0523 02:37:23.650257 35003 solver.cpp:239] Iteration 123170 (2.85836 iter/s, 3.49851s/10 iters), loss = 7.44207
I0523 02:37:23.650306 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44207 (* 1 = 7.44207 loss)
I0523 02:37:24.345448 35003 sgd_solver.cpp:112] Iteration 123170, lr = 0.01
I0523 02:37:28.605092 35003 solver.cpp:239] Iteration 123180 (2.01833 iter/s, 4.95459s/10 iters), loss = 7.30558
I0523 02:37:28.605345 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30558 (* 1 = 7.30558 loss)
I0523 02:37:28.616343 35003 sgd_solver.cpp:112] Iteration 123180, lr = 0.01
I0523 02:37:32.299355 35003 solver.cpp:239] Iteration 123190 (2.70718 iter/s, 3.69388s/10 iters), loss = 6.93324
I0523 02:37:32.299407 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93324 (* 1 = 6.93324 loss)
I0523 02:37:32.302965 35003 sgd_solver.cpp:112] Iteration 123190, lr = 0.01
I0523 02:37:36.178462 35003 solver.cpp:239] Iteration 123200 (2.57806 iter/s, 3.87888s/10 iters), loss = 7.40088
I0523 02:37:36.178511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40088 (* 1 = 7.40088 loss)
I0523 02:37:36.188580 35003 sgd_solver.cpp:112] Iteration 123200, lr = 0.01
I0523 02:37:40.419891 35003 solver.cpp:239] Iteration 123210 (2.35782 iter/s, 4.24121s/10 iters), loss = 7.69704
I0523 02:37:40.419936 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69704 (* 1 = 7.69704 loss)
I0523 02:37:40.425784 35003 sgd_solver.cpp:112] Iteration 123210, lr = 0.01
I0523 02:37:44.883062 35003 solver.cpp:239] Iteration 123220 (2.24067 iter/s, 4.46294s/10 iters), loss = 6.9693
I0523 02:37:44.883102 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9693 (* 1 = 6.9693 loss)
I0523 02:37:44.896309 35003 sgd_solver.cpp:112] Iteration 123220, lr = 0.01
I0523 02:37:48.990669 35003 solver.cpp:239] Iteration 123230 (2.43463 iter/s, 4.1074s/10 iters), loss = 7.63049
I0523 02:37:48.990731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63049 (* 1 = 7.63049 loss)
I0523 02:37:48.998052 35003 sgd_solver.cpp:112] Iteration 123230, lr = 0.01
I0523 02:37:51.742167 35003 solver.cpp:239] Iteration 123240 (3.63462 iter/s, 2.75132s/10 iters), loss = 7.16784
I0523 02:37:51.742216 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16784 (* 1 = 7.16784 loss)
I0523 02:37:51.752064 35003 sgd_solver.cpp:112] Iteration 123240, lr = 0.01
I0523 02:37:54.237085 35003 solver.cpp:239] Iteration 123250 (4.00841 iter/s, 2.49475s/10 iters), loss = 7.50646
I0523 02:37:54.237143 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50646 (* 1 = 7.50646 loss)
I0523 02:37:54.616466 35003 sgd_solver.cpp:112] Iteration 123250, lr = 0.01
I0523 02:37:57.794654 35003 solver.cpp:239] Iteration 123260 (2.81107 iter/s, 3.55736s/10 iters), loss = 6.48024
I0523 02:37:57.794729 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48024 (* 1 = 6.48024 loss)
I0523 02:37:57.802170 35003 sgd_solver.cpp:112] Iteration 123260, lr = 0.01
I0523 02:38:01.896522 35003 solver.cpp:239] Iteration 123270 (2.43806 iter/s, 4.10162s/10 iters), loss = 5.95487
I0523 02:38:01.896761 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95487 (* 1 = 5.95487 loss)
I0523 02:38:01.908800 35003 sgd_solver.cpp:112] Iteration 123270, lr = 0.01
I0523 02:38:05.547436 35003 solver.cpp:239] Iteration 123280 (2.73931 iter/s, 3.65055s/10 iters), loss = 8.29078
I0523 02:38:05.547478 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.29078 (* 1 = 8.29078 loss)
I0523 02:38:05.560317 35003 sgd_solver.cpp:112] Iteration 123280, lr = 0.01
I0523 02:38:09.017982 35003 solver.cpp:239] Iteration 123290 (2.88155 iter/s, 3.47036s/10 iters), loss = 6.00841
I0523 02:38:09.018034 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00841 (* 1 = 6.00841 loss)
I0523 02:38:09.030738 35003 sgd_solver.cpp:112] Iteration 123290, lr = 0.01
I0523 02:38:12.485615 35003 solver.cpp:239] Iteration 123300 (2.88397 iter/s, 3.46744s/10 iters), loss = 6.38218
I0523 02:38:12.485657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38218 (* 1 = 6.38218 loss)
I0523 02:38:12.498579 35003 sgd_solver.cpp:112] Iteration 123300, lr = 0.01
I0523 02:38:15.993067 35003 solver.cpp:239] Iteration 123310 (2.85123 iter/s, 3.50726s/10 iters), loss = 6.22352
I0523 02:38:15.993108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22352 (* 1 = 6.22352 loss)
I0523 02:38:16.000759 35003 sgd_solver.cpp:112] Iteration 123310, lr = 0.01
I0523 02:38:18.565112 35003 solver.cpp:239] Iteration 123320 (3.88819 iter/s, 2.57189s/10 iters), loss = 6.89236
I0523 02:38:18.565155 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89236 (* 1 = 6.89236 loss)
I0523 02:38:18.570472 35003 sgd_solver.cpp:112] Iteration 123320, lr = 0.01
I0523 02:38:21.387513 35003 solver.cpp:239] Iteration 123330 (3.54348 iter/s, 2.82208s/10 iters), loss = 8.04325
I0523 02:38:21.387567 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04325 (* 1 = 8.04325 loss)
I0523 02:38:22.089468 35003 sgd_solver.cpp:112] Iteration 123330, lr = 0.01
I0523 02:38:23.455790 35003 solver.cpp:239] Iteration 123340 (4.83527 iter/s, 2.06814s/10 iters), loss = 6.93576
I0523 02:38:23.455834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93576 (* 1 = 6.93576 loss)
I0523 02:38:24.190357 35003 sgd_solver.cpp:112] Iteration 123340, lr = 0.01
I0523 02:38:27.174561 35003 solver.cpp:239] Iteration 123350 (2.6892 iter/s, 3.71857s/10 iters), loss = 7.01534
I0523 02:38:27.174602 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01534 (* 1 = 7.01534 loss)
I0523 02:38:27.178810 35003 sgd_solver.cpp:112] Iteration 123350, lr = 0.01
I0523 02:38:32.044255 35003 solver.cpp:239] Iteration 123360 (2.05362 iter/s, 4.86945s/10 iters), loss = 6.72928
I0523 02:38:32.044529 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72928 (* 1 = 6.72928 loss)
I0523 02:38:32.752571 35003 sgd_solver.cpp:112] Iteration 123360, lr = 0.01
I0523 02:38:35.890135 35003 solver.cpp:239] Iteration 123370 (2.60047 iter/s, 3.84546s/10 iters), loss = 8.42002
I0523 02:38:35.890190 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.42002 (* 1 = 8.42002 loss)
I0523 02:38:35.902019 35003 sgd_solver.cpp:112] Iteration 123370, lr = 0.01
I0523 02:38:37.954748 35003 solver.cpp:239] Iteration 123380 (4.84391 iter/s, 2.06445s/10 iters), loss = 7.47493
I0523 02:38:37.954808 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47493 (* 1 = 7.47493 loss)
I0523 02:38:38.662999 35003 sgd_solver.cpp:112] Iteration 123380, lr = 0.01
I0523 02:38:41.945477 35003 solver.cpp:239] Iteration 123390 (2.50595 iter/s, 3.99051s/10 iters), loss = 6.82965
I0523 02:38:41.945525 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82965 (* 1 = 6.82965 loss)
I0523 02:38:42.686795 35003 sgd_solver.cpp:112] Iteration 123390, lr = 0.01
I0523 02:38:46.913441 35003 solver.cpp:239] Iteration 123400 (2.013 iter/s, 4.96771s/10 iters), loss = 8.15051
I0523 02:38:46.913501 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.15051 (* 1 = 8.15051 loss)
I0523 02:38:46.918766 35003 sgd_solver.cpp:112] Iteration 123400, lr = 0.01
I0523 02:38:50.819635 35003 solver.cpp:239] Iteration 123410 (2.56021 iter/s, 3.90593s/10 iters), loss = 7.93467
I0523 02:38:50.819692 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93467 (* 1 = 7.93467 loss)
I0523 02:38:51.048346 35003 sgd_solver.cpp:112] Iteration 123410, lr = 0.01
I0523 02:38:55.039531 35003 solver.cpp:239] Iteration 123420 (2.36987 iter/s, 4.21964s/10 iters), loss = 7.55577
I0523 02:38:55.039604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55577 (* 1 = 7.55577 loss)
I0523 02:38:55.069491 35003 sgd_solver.cpp:112] Iteration 123420, lr = 0.01
I0523 02:38:59.468580 35003 solver.cpp:239] Iteration 123430 (2.25796 iter/s, 4.42878s/10 iters), loss = 7.58761
I0523 02:38:59.468626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58761 (* 1 = 7.58761 loss)
I0523 02:38:59.490628 35003 sgd_solver.cpp:112] Iteration 123430, lr = 0.01
I0523 02:39:03.960566 35003 solver.cpp:239] Iteration 123440 (2.2263 iter/s, 4.49175s/10 iters), loss = 7.61921
I0523 02:39:03.960759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61921 (* 1 = 7.61921 loss)
I0523 02:39:03.974612 35003 sgd_solver.cpp:112] Iteration 123440, lr = 0.01
I0523 02:39:06.097000 35003 solver.cpp:239] Iteration 123450 (4.68128 iter/s, 2.13617s/10 iters), loss = 7.12905
I0523 02:39:06.097050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12905 (* 1 = 7.12905 loss)
I0523 02:39:06.099864 35003 sgd_solver.cpp:112] Iteration 123450, lr = 0.01
I0523 02:39:10.478067 35003 solver.cpp:239] Iteration 123460 (2.28267 iter/s, 4.38083s/10 iters), loss = 7.29598
I0523 02:39:10.478121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29598 (* 1 = 7.29598 loss)
I0523 02:39:11.208169 35003 sgd_solver.cpp:112] Iteration 123460, lr = 0.01
I0523 02:39:14.665544 35003 solver.cpp:239] Iteration 123470 (2.38822 iter/s, 4.18722s/10 iters), loss = 7.5115
I0523 02:39:14.665606 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5115 (* 1 = 7.5115 loss)
I0523 02:39:15.054829 35003 sgd_solver.cpp:112] Iteration 123470, lr = 0.01
I0523 02:39:18.474182 35003 solver.cpp:239] Iteration 123480 (2.62576 iter/s, 3.80842s/10 iters), loss = 6.35117
I0523 02:39:18.474226 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35117 (* 1 = 6.35117 loss)
I0523 02:39:18.478147 35003 sgd_solver.cpp:112] Iteration 123480, lr = 0.01
I0523 02:39:22.011088 35003 solver.cpp:239] Iteration 123490 (2.82748 iter/s, 3.53671s/10 iters), loss = 6.7505
I0523 02:39:22.011127 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7505 (* 1 = 6.7505 loss)
I0523 02:39:22.023320 35003 sgd_solver.cpp:112] Iteration 123490, lr = 0.01
I0523 02:39:24.135213 35003 solver.cpp:239] Iteration 123500 (4.70818 iter/s, 2.12396s/10 iters), loss = 6.60004
I0523 02:39:24.135275 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60004 (* 1 = 6.60004 loss)
I0523 02:39:24.239377 35003 sgd_solver.cpp:112] Iteration 123500, lr = 0.01
I0523 02:39:28.094889 35003 solver.cpp:239] Iteration 123510 (2.5256 iter/s, 3.95945s/10 iters), loss = 7.50103
I0523 02:39:28.094933 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50103 (* 1 = 7.50103 loss)
I0523 02:39:28.816843 35003 sgd_solver.cpp:112] Iteration 123510, lr = 0.01
I0523 02:39:33.030171 35003 solver.cpp:239] Iteration 123520 (2.02633 iter/s, 4.93504s/10 iters), loss = 6.60887
I0523 02:39:33.030207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60887 (* 1 = 6.60887 loss)
I0523 02:39:33.570477 35003 sgd_solver.cpp:112] Iteration 123520, lr = 0.01
I0523 02:39:36.412442 35003 solver.cpp:239] Iteration 123530 (2.95687 iter/s, 3.38195s/10 iters), loss = 6.4441
I0523 02:39:36.412719 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4441 (* 1 = 6.4441 loss)
I0523 02:39:36.420629 35003 sgd_solver.cpp:112] Iteration 123530, lr = 0.01
I0523 02:39:39.267673 35003 solver.cpp:239] Iteration 123540 (3.50279 iter/s, 2.85487s/10 iters), loss = 7.00856
I0523 02:39:39.267716 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00856 (* 1 = 7.00856 loss)
I0523 02:39:39.287829 35003 sgd_solver.cpp:112] Iteration 123540, lr = 0.01
I0523 02:39:42.014927 35003 solver.cpp:239] Iteration 123550 (3.64021 iter/s, 2.74709s/10 iters), loss = 6.67102
I0523 02:39:42.014973 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67102 (* 1 = 6.67102 loss)
I0523 02:39:42.726668 35003 sgd_solver.cpp:112] Iteration 123550, lr = 0.01
I0523 02:39:45.446085 35003 solver.cpp:239] Iteration 123560 (2.91464 iter/s, 3.43096s/10 iters), loss = 7.02969
I0523 02:39:45.446137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02969 (* 1 = 7.02969 loss)
I0523 02:39:45.458088 35003 sgd_solver.cpp:112] Iteration 123560, lr = 0.01
I0523 02:39:48.671003 35003 solver.cpp:239] Iteration 123570 (3.10104 iter/s, 3.22473s/10 iters), loss = 7.31914
I0523 02:39:48.671053 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31914 (* 1 = 7.31914 loss)
I0523 02:39:48.695372 35003 sgd_solver.cpp:112] Iteration 123570, lr = 0.01
I0523 02:39:51.018332 35003 solver.cpp:239] Iteration 123580 (4.26043 iter/s, 2.34718s/10 iters), loss = 6.64124
I0523 02:39:51.018376 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64124 (* 1 = 6.64124 loss)
I0523 02:39:51.658247 35003 sgd_solver.cpp:112] Iteration 123580, lr = 0.01
I0523 02:39:54.047235 35003 solver.cpp:239] Iteration 123590 (3.30173 iter/s, 3.02871s/10 iters), loss = 6.18455
I0523 02:39:54.047273 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18455 (* 1 = 6.18455 loss)
I0523 02:39:54.266978 35003 sgd_solver.cpp:112] Iteration 123590, lr = 0.01
I0523 02:39:56.227962 35003 solver.cpp:239] Iteration 123600 (4.58591 iter/s, 2.18059s/10 iters), loss = 6.12599
I0523 02:39:56.228003 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12599 (* 1 = 6.12599 loss)
I0523 02:39:56.911279 35003 sgd_solver.cpp:112] Iteration 123600, lr = 0.01
I0523 02:39:59.143273 35003 solver.cpp:239] Iteration 123610 (3.43037 iter/s, 2.91514s/10 iters), loss = 7.26661
I0523 02:39:59.143339 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26661 (* 1 = 7.26661 loss)
I0523 02:39:59.864426 35003 sgd_solver.cpp:112] Iteration 123610, lr = 0.01
I0523 02:40:02.162425 35003 solver.cpp:239] Iteration 123620 (3.3124 iter/s, 3.01896s/10 iters), loss = 6.18182
I0523 02:40:02.162469 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18182 (* 1 = 6.18182 loss)
I0523 02:40:02.173163 35003 sgd_solver.cpp:112] Iteration 123620, lr = 0.01
I0523 02:40:04.987045 35003 solver.cpp:239] Iteration 123630 (3.54051 iter/s, 2.82446s/10 iters), loss = 6.30552
I0523 02:40:04.987092 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30552 (* 1 = 6.30552 loss)
I0523 02:40:05.727846 35003 sgd_solver.cpp:112] Iteration 123630, lr = 0.01
I0523 02:40:08.591922 35003 solver.cpp:239] Iteration 123640 (2.77417 iter/s, 3.60468s/10 iters), loss = 7.90623
I0523 02:40:08.592131 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90623 (* 1 = 7.90623 loss)
I0523 02:40:08.604460 35003 sgd_solver.cpp:112] Iteration 123640, lr = 0.01
I0523 02:40:11.296236 35003 solver.cpp:239] Iteration 123650 (3.69823 iter/s, 2.70399s/10 iters), loss = 7.13073
I0523 02:40:11.296270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13073 (* 1 = 7.13073 loss)
I0523 02:40:11.309353 35003 sgd_solver.cpp:112] Iteration 123650, lr = 0.01
I0523 02:40:14.835904 35003 solver.cpp:239] Iteration 123660 (2.82527 iter/s, 3.53948s/10 iters), loss = 7.09966
I0523 02:40:14.835943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09966 (* 1 = 7.09966 loss)
I0523 02:40:14.849185 35003 sgd_solver.cpp:112] Iteration 123660, lr = 0.01
I0523 02:40:19.253213 35003 solver.cpp:239] Iteration 123670 (2.26394 iter/s, 4.41708s/10 iters), loss = 6.89945
I0523 02:40:19.253257 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89945 (* 1 = 6.89945 loss)
I0523 02:40:19.261129 35003 sgd_solver.cpp:112] Iteration 123670, lr = 0.01
I0523 02:40:21.992072 35003 solver.cpp:239] Iteration 123680 (3.65138 iter/s, 2.73869s/10 iters), loss = 6.42696
I0523 02:40:21.992130 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42696 (* 1 = 6.42696 loss)
I0523 02:40:22.199403 35003 sgd_solver.cpp:112] Iteration 123680, lr = 0.01
I0523 02:40:25.217329 35003 solver.cpp:239] Iteration 123690 (3.10071 iter/s, 3.22507s/10 iters), loss = 6.48612
I0523 02:40:25.217371 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48612 (* 1 = 6.48612 loss)
I0523 02:40:25.955005 35003 sgd_solver.cpp:112] Iteration 123690, lr = 0.01
I0523 02:40:28.854110 35003 solver.cpp:239] Iteration 123700 (2.74983 iter/s, 3.63658s/10 iters), loss = 6.68333
I0523 02:40:28.854154 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68333 (* 1 = 6.68333 loss)
I0523 02:40:28.867660 35003 sgd_solver.cpp:112] Iteration 123700, lr = 0.01
I0523 02:40:31.679914 35003 solver.cpp:239] Iteration 123710 (3.53902 iter/s, 2.82564s/10 iters), loss = 6.85215
I0523 02:40:31.679952 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85215 (* 1 = 6.85215 loss)
I0523 02:40:31.693456 35003 sgd_solver.cpp:112] Iteration 123710, lr = 0.01
I0523 02:40:33.937094 35003 solver.cpp:239] Iteration 123720 (4.43058 iter/s, 2.25704s/10 iters), loss = 5.95941
I0523 02:40:33.937145 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95941 (* 1 = 5.95941 loss)
I0523 02:40:34.660723 35003 sgd_solver.cpp:112] Iteration 123720, lr = 0.01
I0523 02:40:37.446625 35003 solver.cpp:239] Iteration 123730 (2.84955 iter/s, 3.50933s/10 iters), loss = 6.51522
I0523 02:40:37.446666 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51522 (* 1 = 6.51522 loss)
I0523 02:40:37.459172 35003 sgd_solver.cpp:112] Iteration 123730, lr = 0.01
I0523 02:40:40.879283 35003 solver.cpp:239] Iteration 123740 (2.91336 iter/s, 3.43247s/10 iters), loss = 6.14687
I0523 02:40:40.879508 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14687 (* 1 = 6.14687 loss)
I0523 02:40:40.892621 35003 sgd_solver.cpp:112] Iteration 123740, lr = 0.01
I0523 02:40:45.199057 35003 solver.cpp:239] Iteration 123750 (2.31514 iter/s, 4.3194s/10 iters), loss = 7.8704
I0523 02:40:45.199108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8704 (* 1 = 7.8704 loss)
I0523 02:40:45.843006 35003 sgd_solver.cpp:112] Iteration 123750, lr = 0.01
I0523 02:40:50.297469 35003 solver.cpp:239] Iteration 123760 (1.9615 iter/s, 5.09814s/10 iters), loss = 6.82704
I0523 02:40:50.297538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82704 (* 1 = 6.82704 loss)
I0523 02:40:51.019554 35003 sgd_solver.cpp:112] Iteration 123760, lr = 0.01
I0523 02:40:55.443202 35003 solver.cpp:239] Iteration 123770 (1.94346 iter/s, 5.14546s/10 iters), loss = 6.71709
I0523 02:40:55.443246 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71709 (* 1 = 6.71709 loss)
I0523 02:40:55.446893 35003 sgd_solver.cpp:112] Iteration 123770, lr = 0.01
I0523 02:40:59.720257 35003 solver.cpp:239] Iteration 123780 (2.33818 iter/s, 4.27684s/10 iters), loss = 7.11717
I0523 02:40:59.720305 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11717 (* 1 = 7.11717 loss)
I0523 02:40:59.733381 35003 sgd_solver.cpp:112] Iteration 123780, lr = 0.01
I0523 02:41:03.225855 35003 solver.cpp:239] Iteration 123790 (2.85274 iter/s, 3.5054s/10 iters), loss = 7.4351
I0523 02:41:03.225900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4351 (* 1 = 7.4351 loss)
I0523 02:41:03.238142 35003 sgd_solver.cpp:112] Iteration 123790, lr = 0.01
I0523 02:41:06.755681 35003 solver.cpp:239] Iteration 123800 (2.83316 iter/s, 3.52963s/10 iters), loss = 6.26265
I0523 02:41:06.755728 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26265 (* 1 = 6.26265 loss)
I0523 02:41:06.764425 35003 sgd_solver.cpp:112] Iteration 123800, lr = 0.01
I0523 02:41:08.052045 35003 solver.cpp:239] Iteration 123810 (7.71454 iter/s, 1.29625s/10 iters), loss = 7.05169
I0523 02:41:08.052103 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05169 (* 1 = 7.05169 loss)
I0523 02:41:08.061511 35003 sgd_solver.cpp:112] Iteration 123810, lr = 0.01
I0523 02:41:11.538800 35003 solver.cpp:239] Iteration 123820 (2.86816 iter/s, 3.48655s/10 iters), loss = 6.94288
I0523 02:41:11.539054 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94288 (* 1 = 6.94288 loss)
I0523 02:41:11.545725 35003 sgd_solver.cpp:112] Iteration 123820, lr = 0.01
I0523 02:41:16.693748 35003 solver.cpp:239] Iteration 123830 (1.94005 iter/s, 5.15451s/10 iters), loss = 6.72781
I0523 02:41:16.693792 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72781 (* 1 = 6.72781 loss)
I0523 02:41:16.706681 35003 sgd_solver.cpp:112] Iteration 123830, lr = 0.01
I0523 02:41:19.578812 35003 solver.cpp:239] Iteration 123840 (3.46633 iter/s, 2.88489s/10 iters), loss = 7.5067
I0523 02:41:19.578861 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5067 (* 1 = 7.5067 loss)
I0523 02:41:19.582176 35003 sgd_solver.cpp:112] Iteration 123840, lr = 0.01
I0523 02:41:21.609056 35003 solver.cpp:239] Iteration 123850 (4.92587 iter/s, 2.0301s/10 iters), loss = 7.2976
I0523 02:41:21.609110 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2976 (* 1 = 7.2976 loss)
I0523 02:41:21.616693 35003 sgd_solver.cpp:112] Iteration 123850, lr = 0.01
I0523 02:41:23.695498 35003 solver.cpp:239] Iteration 123860 (4.79318 iter/s, 2.0863s/10 iters), loss = 7.40599
I0523 02:41:23.695538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40599 (* 1 = 7.40599 loss)
I0523 02:41:24.300657 35003 sgd_solver.cpp:112] Iteration 123860, lr = 0.01
I0523 02:41:27.779155 35003 solver.cpp:239] Iteration 123870 (2.44891 iter/s, 4.08344s/10 iters), loss = 7.71981
I0523 02:41:27.779213 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71981 (* 1 = 7.71981 loss)
I0523 02:41:27.783069 35003 sgd_solver.cpp:112] Iteration 123870, lr = 0.01
I0523 02:41:31.412430 35003 solver.cpp:239] Iteration 123880 (2.75251 iter/s, 3.63305s/10 iters), loss = 6.42036
I0523 02:41:31.412482 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42036 (* 1 = 6.42036 loss)
I0523 02:41:31.434283 35003 sgd_solver.cpp:112] Iteration 123880, lr = 0.01
I0523 02:41:35.715595 35003 solver.cpp:239] Iteration 123890 (2.32399 iter/s, 4.30294s/10 iters), loss = 7.23627
I0523 02:41:35.715636 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23627 (* 1 = 7.23627 loss)
I0523 02:41:35.726711 35003 sgd_solver.cpp:112] Iteration 123890, lr = 0.01
I0523 02:41:40.058235 35003 solver.cpp:239] Iteration 123900 (2.30286 iter/s, 4.34242s/10 iters), loss = 6.62978
I0523 02:41:40.058279 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62978 (* 1 = 6.62978 loss)
I0523 02:41:40.077309 35003 sgd_solver.cpp:112] Iteration 123900, lr = 0.01
I0523 02:41:42.174087 35003 solver.cpp:239] Iteration 123910 (4.72654 iter/s, 2.11571s/10 iters), loss = 6.50367
I0523 02:41:42.174393 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50367 (* 1 = 6.50367 loss)
I0523 02:41:42.177369 35003 sgd_solver.cpp:112] Iteration 123910, lr = 0.01
I0523 02:41:44.982007 35003 solver.cpp:239] Iteration 123920 (3.56186 iter/s, 2.80752s/10 iters), loss = 7.11092
I0523 02:41:44.982060 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11092 (* 1 = 7.11092 loss)
I0523 02:41:44.989079 35003 sgd_solver.cpp:112] Iteration 123920, lr = 0.01
I0523 02:41:47.348634 35003 solver.cpp:239] Iteration 123930 (4.22571 iter/s, 2.36646s/10 iters), loss = 7.03793
I0523 02:41:47.348681 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03793 (* 1 = 7.03793 loss)
I0523 02:41:47.705864 35003 sgd_solver.cpp:112] Iteration 123930, lr = 0.01
I0523 02:41:50.421664 35003 solver.cpp:239] Iteration 123940 (3.25431 iter/s, 3.07285s/10 iters), loss = 7.51516
I0523 02:41:50.421716 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51516 (* 1 = 7.51516 loss)
I0523 02:41:50.432039 35003 sgd_solver.cpp:112] Iteration 123940, lr = 0.01
I0523 02:41:51.808499 35003 solver.cpp:239] Iteration 123950 (7.21128 iter/s, 1.38672s/10 iters), loss = 7.79972
I0523 02:41:51.808552 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79972 (* 1 = 7.79972 loss)
I0523 02:41:52.549654 35003 sgd_solver.cpp:112] Iteration 123950, lr = 0.01
I0523 02:41:55.412421 35003 solver.cpp:239] Iteration 123960 (2.77491 iter/s, 3.60372s/10 iters), loss = 7.01893
I0523 02:41:55.412464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01893 (* 1 = 7.01893 loss)
I0523 02:41:56.147372 35003 sgd_solver.cpp:112] Iteration 123960, lr = 0.01
I0523 02:41:58.950387 35003 solver.cpp:239] Iteration 123970 (2.82663 iter/s, 3.53778s/10 iters), loss = 5.55697
I0523 02:41:58.950431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.55697 (* 1 = 5.55697 loss)
I0523 02:41:58.957831 35003 sgd_solver.cpp:112] Iteration 123970, lr = 0.01
I0523 02:42:03.173775 35003 solver.cpp:239] Iteration 123980 (2.36789 iter/s, 4.22317s/10 iters), loss = 8.04954
I0523 02:42:03.173823 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04954 (* 1 = 8.04954 loss)
I0523 02:42:03.187355 35003 sgd_solver.cpp:112] Iteration 123980, lr = 0.01
I0523 02:42:05.917913 35003 solver.cpp:239] Iteration 123990 (3.64435 iter/s, 2.74397s/10 iters), loss = 5.611
I0523 02:42:05.917961 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.611 (* 1 = 5.611 loss)
I0523 02:42:05.927047 35003 sgd_solver.cpp:112] Iteration 123990, lr = 0.01
I0523 02:42:09.489060 35003 solver.cpp:239] Iteration 124000 (2.80038 iter/s, 3.57095s/10 iters), loss = 7.89864
I0523 02:42:09.489106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89864 (* 1 = 7.89864 loss)
I0523 02:42:09.496433 35003 sgd_solver.cpp:112] Iteration 124000, lr = 0.01
I0523 02:42:12.374801 35003 solver.cpp:239] Iteration 124010 (3.46553 iter/s, 2.88556s/10 iters), loss = 7.82885
I0523 02:42:12.375000 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82885 (* 1 = 7.82885 loss)
I0523 02:42:13.105556 35003 sgd_solver.cpp:112] Iteration 124010, lr = 0.01
I0523 02:42:16.041021 35003 solver.cpp:239] Iteration 124020 (2.72784 iter/s, 3.6659s/10 iters), loss = 7.42889
I0523 02:42:16.041057 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42889 (* 1 = 7.42889 loss)
I0523 02:42:16.054275 35003 sgd_solver.cpp:112] Iteration 124020, lr = 0.01
I0523 02:42:20.416930 35003 solver.cpp:239] Iteration 124030 (2.28535 iter/s, 4.37569s/10 iters), loss = 6.75076
I0523 02:42:20.417001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75076 (* 1 = 6.75076 loss)
I0523 02:42:21.154984 35003 sgd_solver.cpp:112] Iteration 124030, lr = 0.01
I0523 02:42:23.095835 35003 solver.cpp:239] Iteration 124040 (3.73312 iter/s, 2.67873s/10 iters), loss = 6.03997
I0523 02:42:23.095882 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03997 (* 1 = 6.03997 loss)
I0523 02:42:23.105341 35003 sgd_solver.cpp:112] Iteration 124040, lr = 0.01
I0523 02:42:26.661675 35003 solver.cpp:239] Iteration 124050 (2.80455 iter/s, 3.56564s/10 iters), loss = 6.4152
I0523 02:42:26.661716 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4152 (* 1 = 6.4152 loss)
I0523 02:42:26.664052 35003 sgd_solver.cpp:112] Iteration 124050, lr = 0.01
I0523 02:42:29.515331 35003 solver.cpp:239] Iteration 124060 (3.50449 iter/s, 2.85349s/10 iters), loss = 7.36187
I0523 02:42:29.515381 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36187 (* 1 = 7.36187 loss)
I0523 02:42:29.519706 35003 sgd_solver.cpp:112] Iteration 124060, lr = 0.01
I0523 02:42:32.311959 35003 solver.cpp:239] Iteration 124070 (3.57595 iter/s, 2.79646s/10 iters), loss = 6.6516
I0523 02:42:32.312000 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6516 (* 1 = 6.6516 loss)
I0523 02:42:32.994077 35003 sgd_solver.cpp:112] Iteration 124070, lr = 0.01
I0523 02:42:36.592981 35003 solver.cpp:239] Iteration 124080 (2.33601 iter/s, 4.2808s/10 iters), loss = 6.87738
I0523 02:42:36.593047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87738 (* 1 = 6.87738 loss)
I0523 02:42:37.327483 35003 sgd_solver.cpp:112] Iteration 124080, lr = 0.01
I0523 02:42:41.350638 35003 solver.cpp:239] Iteration 124090 (2.10199 iter/s, 4.7574s/10 iters), loss = 7.31028
I0523 02:42:41.350688 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31028 (* 1 = 7.31028 loss)
I0523 02:42:41.358110 35003 sgd_solver.cpp:112] Iteration 124090, lr = 0.01
I0523 02:42:42.165566 35003 solver.cpp:239] Iteration 124100 (12.2724 iter/s, 0.814837s/10 iters), loss = 7.96976
I0523 02:42:42.165609 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96976 (* 1 = 7.96976 loss)
I0523 02:42:42.173398 35003 sgd_solver.cpp:112] Iteration 124100, lr = 0.01
I0523 02:42:42.987082 35003 solver.cpp:239] Iteration 124110 (12.174 iter/s, 0.821425s/10 iters), loss = 6.96433
I0523 02:42:42.987378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96433 (* 1 = 6.96433 loss)
I0523 02:42:42.996295 35003 sgd_solver.cpp:112] Iteration 124110, lr = 0.01
I0523 02:42:45.612896 35003 solver.cpp:239] Iteration 124120 (3.80889 iter/s, 2.62544s/10 iters), loss = 6.85781
I0523 02:42:45.612952 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85781 (* 1 = 6.85781 loss)
I0523 02:42:46.316859 35003 sgd_solver.cpp:112] Iteration 124120, lr = 0.01
I0523 02:42:51.468595 35003 solver.cpp:239] Iteration 124130 (1.70783 iter/s, 5.85538s/10 iters), loss = 6.9141
I0523 02:42:51.468657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9141 (* 1 = 6.9141 loss)
I0523 02:42:51.474133 35003 sgd_solver.cpp:112] Iteration 124130, lr = 0.01
I0523 02:42:56.478618 35003 solver.cpp:239] Iteration 124140 (1.9961 iter/s, 5.00976s/10 iters), loss = 6.65597
I0523 02:42:56.478662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65597 (* 1 = 6.65597 loss)
I0523 02:42:56.485926 35003 sgd_solver.cpp:112] Iteration 124140, lr = 0.01
I0523 02:42:58.517086 35003 solver.cpp:239] Iteration 124150 (4.90598 iter/s, 2.03833s/10 iters), loss = 7.24269
I0523 02:42:58.517128 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24269 (* 1 = 7.24269 loss)
I0523 02:42:59.252068 35003 sgd_solver.cpp:112] Iteration 124150, lr = 0.01
I0523 02:43:02.120165 35003 solver.cpp:239] Iteration 124160 (2.77555 iter/s, 3.60289s/10 iters), loss = 7.06155
I0523 02:43:02.120206 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06155 (* 1 = 7.06155 loss)
I0523 02:43:02.211303 35003 sgd_solver.cpp:112] Iteration 124160, lr = 0.01
I0523 02:43:06.608732 35003 solver.cpp:239] Iteration 124170 (2.228 iter/s, 4.48834s/10 iters), loss = 6.52251
I0523 02:43:06.608778 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52251 (* 1 = 6.52251 loss)
I0523 02:43:06.640280 35003 sgd_solver.cpp:112] Iteration 124170, lr = 0.01
I0523 02:43:10.246008 35003 solver.cpp:239] Iteration 124180 (2.74946 iter/s, 3.63708s/10 iters), loss = 7.08531
I0523 02:43:10.246059 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08531 (* 1 = 7.08531 loss)
I0523 02:43:10.253965 35003 sgd_solver.cpp:112] Iteration 124180, lr = 0.01
I0523 02:43:14.505987 35003 solver.cpp:239] Iteration 124190 (2.34756 iter/s, 4.25975s/10 iters), loss = 5.94618
I0523 02:43:14.506285 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.94618 (* 1 = 5.94618 loss)
I0523 02:43:15.227506 35003 sgd_solver.cpp:112] Iteration 124190, lr = 0.01
I0523 02:43:18.731595 35003 solver.cpp:239] Iteration 124200 (2.36677 iter/s, 4.22516s/10 iters), loss = 6.81207
I0523 02:43:18.731650 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81207 (* 1 = 6.81207 loss)
I0523 02:43:18.749851 35003 sgd_solver.cpp:112] Iteration 124200, lr = 0.01
I0523 02:43:22.391867 35003 solver.cpp:239] Iteration 124210 (2.73219 iter/s, 3.66006s/10 iters), loss = 6.80014
I0523 02:43:22.391927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80014 (* 1 = 6.80014 loss)
I0523 02:43:22.410101 35003 sgd_solver.cpp:112] Iteration 124210, lr = 0.01
I0523 02:43:25.164364 35003 solver.cpp:239] Iteration 124220 (3.60708 iter/s, 2.77232s/10 iters), loss = 6.13399
I0523 02:43:25.164400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13399 (* 1 = 6.13399 loss)
I0523 02:43:25.170900 35003 sgd_solver.cpp:112] Iteration 124220, lr = 0.01
I0523 02:43:27.821808 35003 solver.cpp:239] Iteration 124230 (3.76327 iter/s, 2.65726s/10 iters), loss = 6.63091
I0523 02:43:27.821858 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63091 (* 1 = 6.63091 loss)
I0523 02:43:27.827231 35003 sgd_solver.cpp:112] Iteration 124230, lr = 0.01
I0523 02:43:30.575104 35003 solver.cpp:239] Iteration 124240 (3.63224 iter/s, 2.75312s/10 iters), loss = 7.07922
I0523 02:43:30.575165 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07922 (* 1 = 7.07922 loss)
I0523 02:43:30.580219 35003 sgd_solver.cpp:112] Iteration 124240, lr = 0.01
I0523 02:43:35.696096 35003 solver.cpp:239] Iteration 124250 (1.95285 iter/s, 5.12073s/10 iters), loss = 6.03635
I0523 02:43:35.696141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03635 (* 1 = 6.03635 loss)
I0523 02:43:36.437486 35003 sgd_solver.cpp:112] Iteration 124250, lr = 0.01
I0523 02:43:39.224968 35003 solver.cpp:239] Iteration 124260 (2.83392 iter/s, 3.52868s/10 iters), loss = 7.00279
I0523 02:43:39.225005 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00279 (* 1 = 7.00279 loss)
I0523 02:43:39.247812 35003 sgd_solver.cpp:112] Iteration 124260, lr = 0.01
I0523 02:43:41.687368 35003 solver.cpp:239] Iteration 124270 (4.06131 iter/s, 2.46226s/10 iters), loss = 8.02346
I0523 02:43:41.687412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02346 (* 1 = 8.02346 loss)
I0523 02:43:42.102744 35003 sgd_solver.cpp:112] Iteration 124270, lr = 0.01
I0523 02:43:44.550874 35003 solver.cpp:239] Iteration 124280 (3.49243 iter/s, 2.86334s/10 iters), loss = 8.5666
I0523 02:43:44.551059 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.5666 (* 1 = 8.5666 loss)
I0523 02:43:44.557785 35003 sgd_solver.cpp:112] Iteration 124280, lr = 0.01
I0523 02:43:50.153298 35003 solver.cpp:239] Iteration 124290 (1.78507 iter/s, 5.60202s/10 iters), loss = 6.57074
I0523 02:43:50.153337 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57074 (* 1 = 6.57074 loss)
I0523 02:43:50.166172 35003 sgd_solver.cpp:112] Iteration 124290, lr = 0.01
I0523 02:43:52.917443 35003 solver.cpp:239] Iteration 124300 (3.61797 iter/s, 2.76398s/10 iters), loss = 6.30628
I0523 02:43:52.917495 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30628 (* 1 = 6.30628 loss)
I0523 02:43:53.656759 35003 sgd_solver.cpp:112] Iteration 124300, lr = 0.01
I0523 02:43:56.318647 35003 solver.cpp:239] Iteration 124310 (2.9403 iter/s, 3.40101s/10 iters), loss = 7.37466
I0523 02:43:56.318689 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37466 (* 1 = 7.37466 loss)
I0523 02:43:56.611219 35003 sgd_solver.cpp:112] Iteration 124310, lr = 0.01
I0523 02:43:59.534582 35003 solver.cpp:239] Iteration 124320 (3.10972 iter/s, 3.21573s/10 iters), loss = 7.32347
I0523 02:43:59.534634 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32347 (* 1 = 7.32347 loss)
I0523 02:44:00.250026 35003 sgd_solver.cpp:112] Iteration 124320, lr = 0.01
I0523 02:44:03.158074 35003 solver.cpp:239] Iteration 124330 (2.75992 iter/s, 3.62329s/10 iters), loss = 5.62658
I0523 02:44:03.158120 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.62658 (* 1 = 5.62658 loss)
I0523 02:44:03.795413 35003 sgd_solver.cpp:112] Iteration 124330, lr = 0.01
I0523 02:44:07.962661 35003 solver.cpp:239] Iteration 124340 (2.08145 iter/s, 4.80434s/10 iters), loss = 7.69897
I0523 02:44:07.962729 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69897 (* 1 = 7.69897 loss)
I0523 02:44:07.975525 35003 sgd_solver.cpp:112] Iteration 124340, lr = 0.01
I0523 02:44:12.254119 35003 solver.cpp:239] Iteration 124350 (2.33034 iter/s, 4.29121s/10 iters), loss = 6.43237
I0523 02:44:12.254170 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43237 (* 1 = 6.43237 loss)
I0523 02:44:12.262447 35003 sgd_solver.cpp:112] Iteration 124350, lr = 0.01
I0523 02:44:15.481127 35003 solver.cpp:239] Iteration 124360 (3.09903 iter/s, 3.22682s/10 iters), loss = 7.51582
I0523 02:44:15.481315 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51582 (* 1 = 7.51582 loss)
I0523 02:44:15.488682 35003 sgd_solver.cpp:112] Iteration 124360, lr = 0.01
I0523 02:44:19.104358 35003 solver.cpp:239] Iteration 124370 (2.76024 iter/s, 3.62288s/10 iters), loss = 6.90224
I0523 02:44:19.104413 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90224 (* 1 = 6.90224 loss)
I0523 02:44:19.117321 35003 sgd_solver.cpp:112] Iteration 124370, lr = 0.01
I0523 02:44:22.589503 35003 solver.cpp:239] Iteration 124380 (2.86949 iter/s, 3.48494s/10 iters), loss = 7.56265
I0523 02:44:22.589560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56265 (* 1 = 7.56265 loss)
I0523 02:44:22.596304 35003 sgd_solver.cpp:112] Iteration 124380, lr = 0.01
I0523 02:44:25.954391 35003 solver.cpp:239] Iteration 124390 (2.97204 iter/s, 3.36469s/10 iters), loss = 7.52789
I0523 02:44:25.954438 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52789 (* 1 = 7.52789 loss)
I0523 02:44:25.977598 35003 sgd_solver.cpp:112] Iteration 124390, lr = 0.01
I0523 02:44:28.790127 35003 solver.cpp:239] Iteration 124400 (3.52663 iter/s, 2.83557s/10 iters), loss = 6.77032
I0523 02:44:28.790175 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77032 (* 1 = 6.77032 loss)
I0523 02:44:28.790771 35003 sgd_solver.cpp:112] Iteration 124400, lr = 0.01
I0523 02:44:32.767335 35003 solver.cpp:239] Iteration 124410 (2.51446 iter/s, 3.97699s/10 iters), loss = 6.25253
I0523 02:44:32.767381 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25253 (* 1 = 6.25253 loss)
I0523 02:44:33.482993 35003 sgd_solver.cpp:112] Iteration 124410, lr = 0.01
I0523 02:44:36.404858 35003 solver.cpp:239] Iteration 124420 (2.74929 iter/s, 3.6373s/10 iters), loss = 6.78194
I0523 02:44:36.404911 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78194 (* 1 = 6.78194 loss)
I0523 02:44:36.562922 35003 sgd_solver.cpp:112] Iteration 124420, lr = 0.01
I0523 02:44:40.114764 35003 solver.cpp:239] Iteration 124430 (2.69564 iter/s, 3.7097s/10 iters), loss = 5.83999
I0523 02:44:40.114809 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.83999 (* 1 = 5.83999 loss)
I0523 02:44:40.128231 35003 sgd_solver.cpp:112] Iteration 124430, lr = 0.01
I0523 02:44:43.015872 35003 solver.cpp:239] Iteration 124440 (3.44716 iter/s, 2.90094s/10 iters), loss = 7.45562
I0523 02:44:43.015916 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45562 (* 1 = 7.45562 loss)
I0523 02:44:43.304224 35003 sgd_solver.cpp:112] Iteration 124440, lr = 0.01
I0523 02:44:47.738241 35003 solver.cpp:239] Iteration 124450 (2.1177 iter/s, 4.72211s/10 iters), loss = 6.10289
I0523 02:44:47.738533 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10289 (* 1 = 6.10289 loss)
I0523 02:44:48.453537 35003 sgd_solver.cpp:112] Iteration 124450, lr = 0.01
I0523 02:44:50.528659 35003 solver.cpp:239] Iteration 124460 (3.5842 iter/s, 2.79003s/10 iters), loss = 6.88103
I0523 02:44:50.528707 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88103 (* 1 = 6.88103 loss)
I0523 02:44:50.541694 35003 sgd_solver.cpp:112] Iteration 124460, lr = 0.01
I0523 02:44:52.670234 35003 solver.cpp:239] Iteration 124470 (4.66977 iter/s, 2.14143s/10 iters), loss = 7.48509
I0523 02:44:52.670272 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48509 (* 1 = 7.48509 loss)
I0523 02:44:53.392082 35003 sgd_solver.cpp:112] Iteration 124470, lr = 0.01
I0523 02:44:56.696656 35003 solver.cpp:239] Iteration 124480 (2.48372 iter/s, 4.02622s/10 iters), loss = 6.035
I0523 02:44:56.696707 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.035 (* 1 = 6.035 loss)
I0523 02:44:56.710505 35003 sgd_solver.cpp:112] Iteration 124480, lr = 0.01
I0523 02:44:59.563333 35003 solver.cpp:239] Iteration 124490 (3.48857 iter/s, 2.8665s/10 iters), loss = 6.54434
I0523 02:44:59.563374 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54434 (* 1 = 6.54434 loss)
I0523 02:44:59.576671 35003 sgd_solver.cpp:112] Iteration 124490, lr = 0.01
I0523 02:45:02.923624 35003 solver.cpp:239] Iteration 124500 (2.97609 iter/s, 3.36011s/10 iters), loss = 7.07908
I0523 02:45:02.923678 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07908 (* 1 = 7.07908 loss)
I0523 02:45:03.610786 35003 sgd_solver.cpp:112] Iteration 124500, lr = 0.01
I0523 02:45:07.926080 35003 solver.cpp:239] Iteration 124510 (1.99913 iter/s, 5.00217s/10 iters), loss = 5.79041
I0523 02:45:07.926126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.79041 (* 1 = 5.79041 loss)
I0523 02:45:08.569813 35003 sgd_solver.cpp:112] Iteration 124510, lr = 0.01
I0523 02:45:11.436702 35003 solver.cpp:239] Iteration 124520 (2.84866 iter/s, 3.51043s/10 iters), loss = 6.86362
I0523 02:45:11.436758 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86362 (* 1 = 6.86362 loss)
I0523 02:45:11.521498 35003 sgd_solver.cpp:112] Iteration 124520, lr = 0.01
I0523 02:45:14.938464 35003 solver.cpp:239] Iteration 124530 (2.85587 iter/s, 3.50156s/10 iters), loss = 6.51323
I0523 02:45:14.938515 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51323 (* 1 = 6.51323 loss)
I0523 02:45:14.951462 35003 sgd_solver.cpp:112] Iteration 124530, lr = 0.01
I0523 02:45:17.478828 35003 solver.cpp:239] Iteration 124540 (3.93669 iter/s, 2.5402s/10 iters), loss = 6.43977
I0523 02:45:17.478873 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43977 (* 1 = 6.43977 loss)
I0523 02:45:18.213758 35003 sgd_solver.cpp:112] Iteration 124540, lr = 0.01
I0523 02:45:22.458123 35003 solver.cpp:239] Iteration 124550 (2.00842 iter/s, 4.97904s/10 iters), loss = 7.9099
I0523 02:45:22.458209 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9099 (* 1 = 7.9099 loss)
I0523 02:45:22.471107 35003 sgd_solver.cpp:112] Iteration 124550, lr = 0.01
I0523 02:45:25.382194 35003 solver.cpp:239] Iteration 124560 (3.42014 iter/s, 2.92386s/10 iters), loss = 7.09785
I0523 02:45:25.382236 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09785 (* 1 = 7.09785 loss)
I0523 02:45:25.385206 35003 sgd_solver.cpp:112] Iteration 124560, lr = 0.01
I0523 02:45:29.713210 35003 solver.cpp:239] Iteration 124570 (2.30905 iter/s, 4.33079s/10 iters), loss = 5.95835
I0523 02:45:29.713260 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95835 (* 1 = 5.95835 loss)
I0523 02:45:29.718345 35003 sgd_solver.cpp:112] Iteration 124570, lr = 0.01
I0523 02:45:33.221032 35003 solver.cpp:239] Iteration 124580 (2.85094 iter/s, 3.50762s/10 iters), loss = 7.5245
I0523 02:45:33.221071 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5245 (* 1 = 7.5245 loss)
I0523 02:45:33.776113 35003 sgd_solver.cpp:112] Iteration 124580, lr = 0.01
I0523 02:45:35.270938 35003 solver.cpp:239] Iteration 124590 (4.87858 iter/s, 2.04977s/10 iters), loss = 7.60508
I0523 02:45:35.270975 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60508 (* 1 = 7.60508 loss)
I0523 02:45:35.295344 35003 sgd_solver.cpp:112] Iteration 124590, lr = 0.01
I0523 02:45:38.886067 35003 solver.cpp:239] Iteration 124600 (2.7663 iter/s, 3.61494s/10 iters), loss = 6.76668
I0523 02:45:38.886116 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76668 (* 1 = 6.76668 loss)
I0523 02:45:39.566282 35003 sgd_solver.cpp:112] Iteration 124600, lr = 0.01
I0523 02:45:43.260027 35003 solver.cpp:239] Iteration 124610 (2.28638 iter/s, 4.37373s/10 iters), loss = 7.47699
I0523 02:45:43.260071 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47699 (* 1 = 7.47699 loss)
I0523 02:45:43.267010 35003 sgd_solver.cpp:112] Iteration 124610, lr = 0.01
I0523 02:45:46.216922 35003 solver.cpp:239] Iteration 124620 (3.38213 iter/s, 2.95672s/10 iters), loss = 8.10927
I0523 02:45:46.216972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.10927 (* 1 = 8.10927 loss)
I0523 02:45:46.631232 35003 sgd_solver.cpp:112] Iteration 124620, lr = 0.01
I0523 02:45:49.370324 35003 solver.cpp:239] Iteration 124630 (3.17137 iter/s, 3.15321s/10 iters), loss = 7.51562
I0523 02:45:49.370645 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51562 (* 1 = 7.51562 loss)
I0523 02:45:49.377805 35003 sgd_solver.cpp:112] Iteration 124630, lr = 0.01
I0523 02:45:52.970067 35003 solver.cpp:239] Iteration 124640 (2.77832 iter/s, 3.5993s/10 iters), loss = 7.29534
I0523 02:45:52.970121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29534 (* 1 = 7.29534 loss)
I0523 02:45:52.979447 35003 sgd_solver.cpp:112] Iteration 124640, lr = 0.01
I0523 02:45:56.639645 35003 solver.cpp:239] Iteration 124650 (2.72528 iter/s, 3.66935s/10 iters), loss = 8.37871
I0523 02:45:56.639688 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.37871 (* 1 = 8.37871 loss)
I0523 02:45:56.647910 35003 sgd_solver.cpp:112] Iteration 124650, lr = 0.01
I0523 02:46:00.194918 35003 solver.cpp:239] Iteration 124660 (2.81288 iter/s, 3.55508s/10 iters), loss = 5.53802
I0523 02:46:00.194967 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.53802 (* 1 = 5.53802 loss)
I0523 02:46:00.935876 35003 sgd_solver.cpp:112] Iteration 124660, lr = 0.01
I0523 02:46:05.672202 35003 solver.cpp:239] Iteration 124670 (1.82581 iter/s, 5.47701s/10 iters), loss = 8.72621
I0523 02:46:05.672281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.72621 (* 1 = 8.72621 loss)
I0523 02:46:06.114419 35003 sgd_solver.cpp:112] Iteration 124670, lr = 0.01
I0523 02:46:10.453970 35003 solver.cpp:239] Iteration 124680 (2.09139 iter/s, 4.7815s/10 iters), loss = 7.83738
I0523 02:46:10.454016 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83738 (* 1 = 7.83738 loss)
I0523 02:46:10.467267 35003 sgd_solver.cpp:112] Iteration 124680, lr = 0.01
I0523 02:46:14.138806 35003 solver.cpp:239] Iteration 124690 (2.71397 iter/s, 3.68463s/10 iters), loss = 6.24341
I0523 02:46:14.138864 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24341 (* 1 = 6.24341 loss)
I0523 02:46:14.880195 35003 sgd_solver.cpp:112] Iteration 124690, lr = 0.01
I0523 02:46:17.737862 35003 solver.cpp:239] Iteration 124700 (2.77867 iter/s, 3.59885s/10 iters), loss = 6.27553
I0523 02:46:17.737907 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27553 (* 1 = 6.27553 loss)
I0523 02:46:17.750193 35003 sgd_solver.cpp:112] Iteration 124700, lr = 0.01
I0523 02:46:19.968219 35003 solver.cpp:239] Iteration 124710 (4.48389 iter/s, 2.23021s/10 iters), loss = 6.6295
I0523 02:46:19.968508 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6295 (* 1 = 6.6295 loss)
I0523 02:46:20.703271 35003 sgd_solver.cpp:112] Iteration 124710, lr = 0.01
I0523 02:46:25.700762 35003 solver.cpp:239] Iteration 124720 (1.74458 iter/s, 5.73205s/10 iters), loss = 7.49782
I0523 02:46:25.700806 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49782 (* 1 = 7.49782 loss)
I0523 02:46:25.733057 35003 sgd_solver.cpp:112] Iteration 124720, lr = 0.01
I0523 02:46:30.903134 35003 solver.cpp:239] Iteration 124730 (1.92229 iter/s, 5.20212s/10 iters), loss = 5.39303
I0523 02:46:30.903177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.39303 (* 1 = 5.39303 loss)
I0523 02:46:30.909720 35003 sgd_solver.cpp:112] Iteration 124730, lr = 0.01
I0523 02:46:33.657156 35003 solver.cpp:239] Iteration 124740 (3.63126 iter/s, 2.75386s/10 iters), loss = 6.50445
I0523 02:46:33.657197 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50445 (* 1 = 6.50445 loss)
I0523 02:46:33.664127 35003 sgd_solver.cpp:112] Iteration 124740, lr = 0.01
I0523 02:46:35.614918 35003 solver.cpp:239] Iteration 124750 (5.10951 iter/s, 1.95713s/10 iters), loss = 7.37874
I0523 02:46:35.614969 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37874 (* 1 = 7.37874 loss)
I0523 02:46:35.622982 35003 sgd_solver.cpp:112] Iteration 124750, lr = 0.01
I0523 02:46:39.659409 35003 solver.cpp:239] Iteration 124760 (2.47263 iter/s, 4.04427s/10 iters), loss = 7.43911
I0523 02:46:39.659466 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43911 (* 1 = 7.43911 loss)
I0523 02:46:39.683177 35003 sgd_solver.cpp:112] Iteration 124760, lr = 0.01
I0523 02:46:43.281615 35003 solver.cpp:239] Iteration 124770 (2.76091 iter/s, 3.622s/10 iters), loss = 6.54863
I0523 02:46:43.281664 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54863 (* 1 = 6.54863 loss)
I0523 02:46:43.294487 35003 sgd_solver.cpp:112] Iteration 124770, lr = 0.01
I0523 02:46:45.426511 35003 solver.cpp:239] Iteration 124780 (4.66254 iter/s, 2.14475s/10 iters), loss = 5.49596
I0523 02:46:45.426559 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.49596 (* 1 = 5.49596 loss)
I0523 02:46:46.131170 35003 sgd_solver.cpp:112] Iteration 124780, lr = 0.01
I0523 02:46:48.936172 35003 solver.cpp:239] Iteration 124790 (2.84944 iter/s, 3.50947s/10 iters), loss = 7.79057
I0523 02:46:48.936213 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79057 (* 1 = 7.79057 loss)
I0523 02:46:48.948789 35003 sgd_solver.cpp:112] Iteration 124790, lr = 0.01
I0523 02:46:51.071122 35003 solver.cpp:239] Iteration 124800 (4.68424 iter/s, 2.13482s/10 iters), loss = 6.67495
I0523 02:46:51.071353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67495 (* 1 = 6.67495 loss)
I0523 02:46:51.072374 35003 sgd_solver.cpp:112] Iteration 124800, lr = 0.01
I0523 02:46:54.715247 35003 solver.cpp:239] Iteration 124810 (2.74441 iter/s, 3.64377s/10 iters), loss = 7.28117
I0523 02:46:54.715292 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28117 (* 1 = 7.28117 loss)
I0523 02:46:54.752877 35003 sgd_solver.cpp:112] Iteration 124810, lr = 0.01
I0523 02:46:59.131582 35003 solver.cpp:239] Iteration 124820 (2.26444 iter/s, 4.41611s/10 iters), loss = 7.76044
I0523 02:46:59.131624 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76044 (* 1 = 7.76044 loss)
I0523 02:46:59.508782 35003 sgd_solver.cpp:112] Iteration 124820, lr = 0.01
I0523 02:47:03.818544 35003 solver.cpp:239] Iteration 124830 (2.13369 iter/s, 4.68673s/10 iters), loss = 7.13712
I0523 02:47:03.818583 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13712 (* 1 = 7.13712 loss)
I0523 02:47:03.832614 35003 sgd_solver.cpp:112] Iteration 124830, lr = 0.01
I0523 02:47:08.210844 35003 solver.cpp:239] Iteration 124840 (2.27683 iter/s, 4.39208s/10 iters), loss = 7.72327
I0523 02:47:08.210898 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72327 (* 1 = 7.72327 loss)
I0523 02:47:08.220468 35003 sgd_solver.cpp:112] Iteration 124840, lr = 0.01
I0523 02:47:10.939173 35003 solver.cpp:239] Iteration 124850 (3.66548 iter/s, 2.72816s/10 iters), loss = 7.16703
I0523 02:47:10.939220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16703 (* 1 = 7.16703 loss)
I0523 02:47:10.947937 35003 sgd_solver.cpp:112] Iteration 124850, lr = 0.01
I0523 02:47:13.791357 35003 solver.cpp:239] Iteration 124860 (3.5063 iter/s, 2.85201s/10 iters), loss = 8.18783
I0523 02:47:13.791407 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.18783 (* 1 = 8.18783 loss)
I0523 02:47:13.840226 35003 sgd_solver.cpp:112] Iteration 124860, lr = 0.01
I0523 02:47:18.294198 35003 solver.cpp:239] Iteration 124870 (2.22094 iter/s, 4.50261s/10 iters), loss = 7.37112
I0523 02:47:18.294243 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37112 (* 1 = 7.37112 loss)
I0523 02:47:18.300439 35003 sgd_solver.cpp:112] Iteration 124870, lr = 0.01
I0523 02:47:20.902391 35003 solver.cpp:239] Iteration 124880 (3.83431 iter/s, 2.60803s/10 iters), loss = 6.81889
I0523 02:47:20.902436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81889 (* 1 = 6.81889 loss)
I0523 02:47:20.930891 35003 sgd_solver.cpp:112] Iteration 124880, lr = 0.01
I0523 02:47:23.961004 35003 solver.cpp:239] Iteration 124890 (3.26965 iter/s, 3.05843s/10 iters), loss = 7.39561
I0523 02:47:23.961159 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39561 (* 1 = 7.39561 loss)
I0523 02:47:23.967118 35003 sgd_solver.cpp:112] Iteration 124890, lr = 0.01
I0523 02:47:27.637852 35003 solver.cpp:239] Iteration 124900 (2.71996 iter/s, 3.67653s/10 iters), loss = 7.28539
I0523 02:47:27.637928 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28539 (* 1 = 7.28539 loss)
I0523 02:47:28.378795 35003 sgd_solver.cpp:112] Iteration 124900, lr = 0.01
I0523 02:47:31.718322 35003 solver.cpp:239] Iteration 124910 (2.45084 iter/s, 4.08023s/10 iters), loss = 7.47751
I0523 02:47:31.718363 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47751 (* 1 = 7.47751 loss)
I0523 02:47:31.725872 35003 sgd_solver.cpp:112] Iteration 124910, lr = 0.01
I0523 02:47:35.004003 35003 solver.cpp:239] Iteration 124920 (3.04368 iter/s, 3.28549s/10 iters), loss = 7.20903
I0523 02:47:35.004057 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20903 (* 1 = 7.20903 loss)
I0523 02:47:35.007027 35003 sgd_solver.cpp:112] Iteration 124920, lr = 0.01
I0523 02:47:38.824072 35003 solver.cpp:239] Iteration 124930 (2.61791 iter/s, 3.81984s/10 iters), loss = 7.34397
I0523 02:47:38.824123 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34397 (* 1 = 7.34397 loss)
I0523 02:47:38.831662 35003 sgd_solver.cpp:112] Iteration 124930, lr = 0.01
I0523 02:47:43.165235 35003 solver.cpp:239] Iteration 124940 (2.30365 iter/s, 4.34093s/10 iters), loss = 7.02622
I0523 02:47:43.165287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02622 (* 1 = 7.02622 loss)
I0523 02:47:43.173259 35003 sgd_solver.cpp:112] Iteration 124940, lr = 0.01
I0523 02:47:47.057574 35003 solver.cpp:239] Iteration 124950 (2.56929 iter/s, 3.89213s/10 iters), loss = 7.9052
I0523 02:47:47.057616 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9052 (* 1 = 7.9052 loss)
I0523 02:47:47.070652 35003 sgd_solver.cpp:112] Iteration 124950, lr = 0.01
I0523 02:47:49.303263 35003 solver.cpp:239] Iteration 124960 (4.45327 iter/s, 2.24554s/10 iters), loss = 6.77133
I0523 02:47:49.303314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77133 (* 1 = 6.77133 loss)
I0523 02:47:50.043845 35003 sgd_solver.cpp:112] Iteration 124960, lr = 0.01
I0523 02:47:54.181267 35003 solver.cpp:239] Iteration 124970 (2.05012 iter/s, 4.87776s/10 iters), loss = 6.77605
I0523 02:47:54.181568 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77605 (* 1 = 6.77605 loss)
I0523 02:47:54.915311 35003 sgd_solver.cpp:112] Iteration 124970, lr = 0.01
I0523 02:47:56.538270 35003 solver.cpp:239] Iteration 124980 (4.24336 iter/s, 2.35663s/10 iters), loss = 6.64786
I0523 02:47:56.538319 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64786 (* 1 = 6.64786 loss)
I0523 02:47:57.216737 35003 sgd_solver.cpp:112] Iteration 124980, lr = 0.01
I0523 02:48:00.778188 35003 solver.cpp:239] Iteration 124990 (2.35866 iter/s, 4.2397s/10 iters), loss = 5.47913
I0523 02:48:00.778228 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.47913 (* 1 = 5.47913 loss)
I0523 02:48:00.790640 35003 sgd_solver.cpp:112] Iteration 124990, lr = 0.01
I0523 02:48:02.751603 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_125000.caffemodel
I0523 02:48:03.158741 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_125000.solverstate
I0523 02:48:03.359208 35003 solver.cpp:239] Iteration 125000 (3.87467 iter/s, 2.58087s/10 iters), loss = 6.66617
I0523 02:48:03.359261 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66617 (* 1 = 6.66617 loss)
I0523 02:48:03.396792 35003 sgd_solver.cpp:112] Iteration 125000, lr = 0.01
I0523 02:48:06.035953 35003 solver.cpp:239] Iteration 125010 (3.73611 iter/s, 2.67658s/10 iters), loss = 7.02249
I0523 02:48:06.036005 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02249 (* 1 = 7.02249 loss)
I0523 02:48:06.049429 35003 sgd_solver.cpp:112] Iteration 125010, lr = 0.01
I0523 02:48:08.912598 35003 solver.cpp:239] Iteration 125020 (3.47648 iter/s, 2.87647s/10 iters), loss = 7.63503
I0523 02:48:08.912647 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63503 (* 1 = 7.63503 loss)
I0523 02:48:08.916280 35003 sgd_solver.cpp:112] Iteration 125020, lr = 0.01
I0523 02:48:10.884166 35003 solver.cpp:239] Iteration 125030 (5.07246 iter/s, 1.97143s/10 iters), loss = 6.27569
I0523 02:48:10.884207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27569 (* 1 = 6.27569 loss)
I0523 02:48:10.897271 35003 sgd_solver.cpp:112] Iteration 125030, lr = 0.01
I0523 02:48:15.206135 35003 solver.cpp:239] Iteration 125040 (2.31388 iter/s, 4.32174s/10 iters), loss = 6.84664
I0523 02:48:15.206187 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84664 (* 1 = 6.84664 loss)
I0523 02:48:15.218318 35003 sgd_solver.cpp:112] Iteration 125040, lr = 0.01
I0523 02:48:18.054741 35003 solver.cpp:239] Iteration 125050 (3.51071 iter/s, 2.84843s/10 iters), loss = 6.46079
I0523 02:48:18.054781 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46079 (* 1 = 6.46079 loss)
I0523 02:48:18.057431 35003 sgd_solver.cpp:112] Iteration 125050, lr = 0.01
I0523 02:48:20.883127 35003 solver.cpp:239] Iteration 125060 (3.53581 iter/s, 2.82821s/10 iters), loss = 5.68685
I0523 02:48:20.883172 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.68685 (* 1 = 5.68685 loss)
I0523 02:48:20.889852 35003 sgd_solver.cpp:112] Iteration 125060, lr = 0.01
I0523 02:48:23.702668 35003 solver.cpp:239] Iteration 125070 (3.54689 iter/s, 2.81937s/10 iters), loss = 7.55722
I0523 02:48:23.702739 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55722 (* 1 = 7.55722 loss)
I0523 02:48:23.716197 35003 sgd_solver.cpp:112] Iteration 125070, lr = 0.01
I0523 02:48:27.254406 35003 solver.cpp:239] Iteration 125080 (2.8157 iter/s, 3.55152s/10 iters), loss = 6.43033
I0523 02:48:27.254660 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43033 (* 1 = 6.43033 loss)
I0523 02:48:27.964643 35003 sgd_solver.cpp:112] Iteration 125080, lr = 0.01
I0523 02:48:30.542362 35003 solver.cpp:239] Iteration 125090 (3.04174 iter/s, 3.28759s/10 iters), loss = 7.00205
I0523 02:48:30.542429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00205 (* 1 = 7.00205 loss)
I0523 02:48:31.276916 35003 sgd_solver.cpp:112] Iteration 125090, lr = 0.01
I0523 02:48:33.653719 35003 solver.cpp:239] Iteration 125100 (3.21423 iter/s, 3.11117s/10 iters), loss = 6.77598
I0523 02:48:33.653761 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77598 (* 1 = 6.77598 loss)
I0523 02:48:33.770193 35003 sgd_solver.cpp:112] Iteration 125100, lr = 0.01
I0523 02:48:36.451448 35003 solver.cpp:239] Iteration 125110 (3.57453 iter/s, 2.79757s/10 iters), loss = 8.45694
I0523 02:48:36.451501 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.45694 (* 1 = 8.45694 loss)
I0523 02:48:36.469574 35003 sgd_solver.cpp:112] Iteration 125110, lr = 0.01
I0523 02:48:39.367911 35003 solver.cpp:239] Iteration 125120 (3.42902 iter/s, 2.91628s/10 iters), loss = 6.68874
I0523 02:48:39.367969 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68874 (* 1 = 6.68874 loss)
I0523 02:48:39.380954 35003 sgd_solver.cpp:112] Iteration 125120, lr = 0.01
I0523 02:48:41.796253 35003 solver.cpp:239] Iteration 125130 (4.1183 iter/s, 2.42818s/10 iters), loss = 6.58608
I0523 02:48:41.796293 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58608 (* 1 = 6.58608 loss)
I0523 02:48:41.813256 35003 sgd_solver.cpp:112] Iteration 125130, lr = 0.01
I0523 02:48:46.127935 35003 solver.cpp:239] Iteration 125140 (2.30869 iter/s, 4.33146s/10 iters), loss = 7.12958
I0523 02:48:46.127997 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12958 (* 1 = 7.12958 loss)
I0523 02:48:46.833808 35003 sgd_solver.cpp:112] Iteration 125140, lr = 0.01
I0523 02:48:51.017300 35003 solver.cpp:239] Iteration 125150 (2.04536 iter/s, 4.88912s/10 iters), loss = 6.83058
I0523 02:48:51.017347 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83058 (* 1 = 6.83058 loss)
I0523 02:48:51.030995 35003 sgd_solver.cpp:112] Iteration 125150, lr = 0.01
I0523 02:48:54.831061 35003 solver.cpp:239] Iteration 125160 (2.62223 iter/s, 3.81355s/10 iters), loss = 6.17434
I0523 02:48:54.831113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17434 (* 1 = 6.17434 loss)
I0523 02:48:54.835824 35003 sgd_solver.cpp:112] Iteration 125160, lr = 0.01
I0523 02:48:56.898317 35003 solver.cpp:239] Iteration 125170 (4.83767 iter/s, 2.06711s/10 iters), loss = 7.13184
I0523 02:48:56.898365 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13184 (* 1 = 7.13184 loss)
I0523 02:48:56.904325 35003 sgd_solver.cpp:112] Iteration 125170, lr = 0.01
I0523 02:49:02.095873 35003 solver.cpp:239] Iteration 125180 (1.92409 iter/s, 5.19727s/10 iters), loss = 6.9935
I0523 02:49:02.096161 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9935 (* 1 = 6.9935 loss)
I0523 02:49:02.099447 35003 sgd_solver.cpp:112] Iteration 125180, lr = 0.01
I0523 02:49:04.897317 35003 solver.cpp:239] Iteration 125190 (3.57008 iter/s, 2.80105s/10 iters), loss = 8.18594
I0523 02:49:04.897383 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.18594 (* 1 = 8.18594 loss)
I0523 02:49:04.907904 35003 sgd_solver.cpp:112] Iteration 125190, lr = 0.01
I0523 02:49:09.164502 35003 solver.cpp:239] Iteration 125200 (2.34359 iter/s, 4.26695s/10 iters), loss = 7.25362
I0523 02:49:09.164548 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25362 (* 1 = 7.25362 loss)
I0523 02:49:09.176573 35003 sgd_solver.cpp:112] Iteration 125200, lr = 0.01
I0523 02:49:14.309470 35003 solver.cpp:239] Iteration 125210 (1.94375 iter/s, 5.14471s/10 iters), loss = 7.46991
I0523 02:49:14.309525 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46991 (* 1 = 7.46991 loss)
I0523 02:49:14.312503 35003 sgd_solver.cpp:112] Iteration 125210, lr = 0.01
I0523 02:49:17.883471 35003 solver.cpp:239] Iteration 125220 (2.79817 iter/s, 3.57376s/10 iters), loss = 7.25473
I0523 02:49:17.883519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25473 (* 1 = 7.25473 loss)
I0523 02:49:18.624760 35003 sgd_solver.cpp:112] Iteration 125220, lr = 0.01
I0523 02:49:22.137594 35003 solver.cpp:239] Iteration 125230 (2.35078 iter/s, 4.2539s/10 iters), loss = 7.12459
I0523 02:49:22.137639 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12459 (* 1 = 7.12459 loss)
I0523 02:49:22.858839 35003 sgd_solver.cpp:112] Iteration 125230, lr = 0.01
I0523 02:49:25.639344 35003 solver.cpp:239] Iteration 125240 (2.85587 iter/s, 3.50156s/10 iters), loss = 6.35007
I0523 02:49:25.639387 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35007 (* 1 = 6.35007 loss)
I0523 02:49:25.651332 35003 sgd_solver.cpp:112] Iteration 125240, lr = 0.01
I0523 02:49:29.986143 35003 solver.cpp:239] Iteration 125250 (2.30066 iter/s, 4.34657s/10 iters), loss = 6.69702
I0523 02:49:29.986207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69702 (* 1 = 6.69702 loss)
I0523 02:49:30.701663 35003 sgd_solver.cpp:112] Iteration 125250, lr = 0.01
I0523 02:49:33.728880 35003 solver.cpp:239] Iteration 125260 (2.67201 iter/s, 3.74249s/10 iters), loss = 7.34148
I0523 02:49:33.729199 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34148 (* 1 = 7.34148 loss)
I0523 02:49:34.469575 35003 sgd_solver.cpp:112] Iteration 125260, lr = 0.01
I0523 02:49:39.309981 35003 solver.cpp:239] Iteration 125270 (1.79193 iter/s, 5.58059s/10 iters), loss = 7.17513
I0523 02:49:39.310022 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17513 (* 1 = 7.17513 loss)
I0523 02:49:40.025440 35003 sgd_solver.cpp:112] Iteration 125270, lr = 0.01
I0523 02:49:42.209810 35003 solver.cpp:239] Iteration 125280 (3.44869 iter/s, 2.89965s/10 iters), loss = 7.80313
I0523 02:49:42.209878 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80313 (* 1 = 7.80313 loss)
I0523 02:49:42.930645 35003 sgd_solver.cpp:112] Iteration 125280, lr = 0.01
I0523 02:49:45.774857 35003 solver.cpp:239] Iteration 125290 (2.80518 iter/s, 3.56483s/10 iters), loss = 6.95087
I0523 02:49:45.774915 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95087 (* 1 = 6.95087 loss)
I0523 02:49:46.327105 35003 sgd_solver.cpp:112] Iteration 125290, lr = 0.01
I0523 02:49:50.779572 35003 solver.cpp:239] Iteration 125300 (1.99822 iter/s, 5.00446s/10 iters), loss = 6.5197
I0523 02:49:50.779621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5197 (* 1 = 6.5197 loss)
I0523 02:49:50.784660 35003 sgd_solver.cpp:112] Iteration 125300, lr = 0.01
I0523 02:49:53.957180 35003 solver.cpp:239] Iteration 125310 (3.14721 iter/s, 3.17742s/10 iters), loss = 6.15945
I0523 02:49:53.957242 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15945 (* 1 = 6.15945 loss)
I0523 02:49:53.960299 35003 sgd_solver.cpp:112] Iteration 125310, lr = 0.01
I0523 02:49:56.819622 35003 solver.cpp:239] Iteration 125320 (3.49386 iter/s, 2.86217s/10 iters), loss = 7.04486
I0523 02:49:56.819676 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04486 (* 1 = 7.04486 loss)
I0523 02:49:56.831629 35003 sgd_solver.cpp:112] Iteration 125320, lr = 0.01
I0523 02:50:00.405328 35003 solver.cpp:239] Iteration 125330 (2.78901 iter/s, 3.5855s/10 iters), loss = 7.1418
I0523 02:50:00.405383 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1418 (* 1 = 7.1418 loss)
I0523 02:50:00.996677 35003 sgd_solver.cpp:112] Iteration 125330, lr = 0.01
I0523 02:50:04.688797 35003 solver.cpp:239] Iteration 125340 (2.33469 iter/s, 4.28323s/10 iters), loss = 7.38991
I0523 02:50:04.689044 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38991 (* 1 = 7.38991 loss)
I0523 02:50:04.709029 35003 sgd_solver.cpp:112] Iteration 125340, lr = 0.01
I0523 02:50:06.854624 35003 solver.cpp:239] Iteration 125350 (4.61785 iter/s, 2.16551s/10 iters), loss = 6.9754
I0523 02:50:06.854678 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9754 (* 1 = 6.9754 loss)
I0523 02:50:07.595587 35003 sgd_solver.cpp:112] Iteration 125350, lr = 0.01
I0523 02:50:10.420857 35003 solver.cpp:239] Iteration 125360 (2.80423 iter/s, 3.56604s/10 iters), loss = 7.28881
I0523 02:50:10.420899 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28881 (* 1 = 7.28881 loss)
I0523 02:50:10.434593 35003 sgd_solver.cpp:112] Iteration 125360, lr = 0.01
I0523 02:50:13.759848 35003 solver.cpp:239] Iteration 125370 (2.99508 iter/s, 3.33881s/10 iters), loss = 6.78006
I0523 02:50:13.759894 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78006 (* 1 = 6.78006 loss)
I0523 02:50:13.777909 35003 sgd_solver.cpp:112] Iteration 125370, lr = 0.01
I0523 02:50:17.407759 35003 solver.cpp:239] Iteration 125380 (2.74144 iter/s, 3.64771s/10 iters), loss = 6.37066
I0523 02:50:17.407804 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37066 (* 1 = 6.37066 loss)
I0523 02:50:17.439980 35003 sgd_solver.cpp:112] Iteration 125380, lr = 0.01
I0523 02:50:20.925992 35003 solver.cpp:239] Iteration 125390 (2.8425 iter/s, 3.51803s/10 iters), loss = 7.17509
I0523 02:50:20.926036 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17509 (* 1 = 7.17509 loss)
I0523 02:50:20.931397 35003 sgd_solver.cpp:112] Iteration 125390, lr = 0.01
I0523 02:50:23.796763 35003 solver.cpp:239] Iteration 125400 (3.48359 iter/s, 2.8706s/10 iters), loss = 6.21686
I0523 02:50:23.796808 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21686 (* 1 = 6.21686 loss)
I0523 02:50:23.804906 35003 sgd_solver.cpp:112] Iteration 125400, lr = 0.01
I0523 02:50:28.432842 35003 solver.cpp:239] Iteration 125410 (2.15711 iter/s, 4.63583s/10 iters), loss = 8.52768
I0523 02:50:28.432893 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.52768 (* 1 = 8.52768 loss)
I0523 02:50:29.173755 35003 sgd_solver.cpp:112] Iteration 125410, lr = 0.01
I0523 02:50:31.262924 35003 solver.cpp:239] Iteration 125420 (3.53368 iter/s, 2.82991s/10 iters), loss = 7.19014
I0523 02:50:31.262966 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19014 (* 1 = 7.19014 loss)
I0523 02:50:31.276363 35003 sgd_solver.cpp:112] Iteration 125420, lr = 0.01
I0523 02:50:34.063665 35003 solver.cpp:239] Iteration 125430 (3.57071 iter/s, 2.80056s/10 iters), loss = 6.81796
I0523 02:50:34.063732 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81796 (* 1 = 6.81796 loss)
I0523 02:50:34.782940 35003 sgd_solver.cpp:112] Iteration 125430, lr = 0.01
I0523 02:50:39.658332 35003 solver.cpp:239] Iteration 125440 (1.78751 iter/s, 5.59438s/10 iters), loss = 7.79444
I0523 02:50:39.658375 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79444 (* 1 = 7.79444 loss)
I0523 02:50:39.664999 35003 sgd_solver.cpp:112] Iteration 125440, lr = 0.01
I0523 02:50:44.092264 35003 solver.cpp:239] Iteration 125450 (2.25545 iter/s, 4.43371s/10 iters), loss = 7.39204
I0523 02:50:44.092314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39204 (* 1 = 7.39204 loss)
I0523 02:50:44.108558 35003 sgd_solver.cpp:112] Iteration 125450, lr = 0.01
I0523 02:50:46.094177 35003 solver.cpp:239] Iteration 125460 (4.9956 iter/s, 2.00176s/10 iters), loss = 7.279
I0523 02:50:46.094223 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.279 (* 1 = 7.279 loss)
I0523 02:50:46.099606 35003 sgd_solver.cpp:112] Iteration 125460, lr = 0.01
I0523 02:50:50.445801 35003 solver.cpp:239] Iteration 125470 (2.29811 iter/s, 4.35139s/10 iters), loss = 7.35363
I0523 02:50:50.445858 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35363 (* 1 = 7.35363 loss)
I0523 02:50:50.453655 35003 sgd_solver.cpp:112] Iteration 125470, lr = 0.01
I0523 02:50:54.696166 35003 solver.cpp:239] Iteration 125480 (2.35287 iter/s, 4.25013s/10 iters), loss = 7.98745
I0523 02:50:54.696214 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98745 (* 1 = 7.98745 loss)
I0523 02:50:55.046397 35003 sgd_solver.cpp:112] Iteration 125480, lr = 0.01
I0523 02:50:58.591588 35003 solver.cpp:239] Iteration 125490 (2.56726 iter/s, 3.8952s/10 iters), loss = 7.14656
I0523 02:50:58.591629 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14656 (* 1 = 7.14656 loss)
I0523 02:50:58.602903 35003 sgd_solver.cpp:112] Iteration 125490, lr = 0.01
I0523 02:51:02.019419 35003 solver.cpp:239] Iteration 125500 (2.91745 iter/s, 3.42765s/10 iters), loss = 7.26057
I0523 02:51:02.019465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26057 (* 1 = 7.26057 loss)
I0523 02:51:02.032704 35003 sgd_solver.cpp:112] Iteration 125500, lr = 0.01
I0523 02:51:04.743283 35003 solver.cpp:239] Iteration 125510 (3.67147 iter/s, 2.7237s/10 iters), loss = 6.86544
I0523 02:51:04.743336 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86544 (* 1 = 6.86544 loss)
I0523 02:51:05.465454 35003 sgd_solver.cpp:112] Iteration 125510, lr = 0.01
I0523 02:51:08.965075 35003 solver.cpp:239] Iteration 125520 (2.36879 iter/s, 4.22156s/10 iters), loss = 6.37688
I0523 02:51:08.965121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37688 (* 1 = 6.37688 loss)
I0523 02:51:08.972496 35003 sgd_solver.cpp:112] Iteration 125520, lr = 0.01
I0523 02:51:13.813346 35003 solver.cpp:239] Iteration 125530 (2.0627 iter/s, 4.84802s/10 iters), loss = 7.27781
I0523 02:51:13.813392 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27781 (* 1 = 7.27781 loss)
I0523 02:51:13.950882 35003 sgd_solver.cpp:112] Iteration 125530, lr = 0.01
I0523 02:51:18.499825 35003 solver.cpp:239] Iteration 125540 (2.1339 iter/s, 4.68624s/10 iters), loss = 6.37936
I0523 02:51:18.499876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37936 (* 1 = 6.37936 loss)
I0523 02:51:18.513250 35003 sgd_solver.cpp:112] Iteration 125540, lr = 0.01
I0523 02:51:21.408334 35003 solver.cpp:239] Iteration 125550 (3.43839 iter/s, 2.90833s/10 iters), loss = 7.10334
I0523 02:51:21.408378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10334 (* 1 = 7.10334 loss)
I0523 02:51:22.123901 35003 sgd_solver.cpp:112] Iteration 125550, lr = 0.01
I0523 02:51:25.264822 35003 solver.cpp:239] Iteration 125560 (2.59317 iter/s, 3.85628s/10 iters), loss = 6.85201
I0523 02:51:25.264866 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85201 (* 1 = 6.85201 loss)
I0523 02:51:25.269201 35003 sgd_solver.cpp:112] Iteration 125560, lr = 0.01
I0523 02:51:29.233031 35003 solver.cpp:239] Iteration 125570 (2.52017 iter/s, 3.96799s/10 iters), loss = 6.79837
I0523 02:51:29.233098 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79837 (* 1 = 6.79837 loss)
I0523 02:51:29.240854 35003 sgd_solver.cpp:112] Iteration 125570, lr = 0.01
I0523 02:51:32.871387 35003 solver.cpp:239] Iteration 125580 (2.74866 iter/s, 3.63814s/10 iters), loss = 6.78917
I0523 02:51:32.871443 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78917 (* 1 = 6.78917 loss)
I0523 02:51:32.884013 35003 sgd_solver.cpp:112] Iteration 125580, lr = 0.01
I0523 02:51:36.476886 35003 solver.cpp:239] Iteration 125590 (2.77369 iter/s, 3.6053s/10 iters), loss = 6.83227
I0523 02:51:36.477102 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83227 (* 1 = 6.83227 loss)
I0523 02:51:36.613463 35003 sgd_solver.cpp:112] Iteration 125590, lr = 0.01
I0523 02:51:40.207818 35003 solver.cpp:239] Iteration 125600 (2.68054 iter/s, 3.73059s/10 iters), loss = 7.73521
I0523 02:51:40.207866 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73521 (* 1 = 7.73521 loss)
I0523 02:51:40.923418 35003 sgd_solver.cpp:112] Iteration 125600, lr = 0.01
I0523 02:51:45.961277 35003 solver.cpp:239] Iteration 125610 (1.73817 iter/s, 5.75318s/10 iters), loss = 6.94981
I0523 02:51:45.961325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94981 (* 1 = 6.94981 loss)
I0523 02:51:46.676669 35003 sgd_solver.cpp:112] Iteration 125610, lr = 0.01
I0523 02:51:49.498958 35003 solver.cpp:239] Iteration 125620 (2.82687 iter/s, 3.53748s/10 iters), loss = 7.07863
I0523 02:51:49.499019 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07863 (* 1 = 7.07863 loss)
I0523 02:51:50.239951 35003 sgd_solver.cpp:112] Iteration 125620, lr = 0.01
I0523 02:51:53.035346 35003 solver.cpp:239] Iteration 125630 (2.82791 iter/s, 3.53619s/10 iters), loss = 7.31977
I0523 02:51:53.035387 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31977 (* 1 = 7.31977 loss)
I0523 02:51:53.056849 35003 sgd_solver.cpp:112] Iteration 125630, lr = 0.01
I0523 02:51:56.664965 35003 solver.cpp:239] Iteration 125640 (2.75526 iter/s, 3.62942s/10 iters), loss = 6.49951
I0523 02:51:56.665020 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49951 (* 1 = 6.49951 loss)
I0523 02:51:57.283538 35003 sgd_solver.cpp:112] Iteration 125640, lr = 0.01
I0523 02:52:01.570912 35003 solver.cpp:239] Iteration 125650 (2.03845 iter/s, 4.9057s/10 iters), loss = 7.34417
I0523 02:52:01.570953 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34417 (* 1 = 7.34417 loss)
I0523 02:52:01.588353 35003 sgd_solver.cpp:112] Iteration 125650, lr = 0.01
I0523 02:52:04.349319 35003 solver.cpp:239] Iteration 125660 (3.59939 iter/s, 2.77825s/10 iters), loss = 6.76525
I0523 02:52:04.349357 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76525 (* 1 = 6.76525 loss)
I0523 02:52:04.359266 35003 sgd_solver.cpp:112] Iteration 125660, lr = 0.01
I0523 02:52:08.197669 35003 solver.cpp:239] Iteration 125670 (2.59865 iter/s, 3.84815s/10 iters), loss = 7.24567
I0523 02:52:08.197922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24567 (* 1 = 7.24567 loss)
I0523 02:52:08.210258 35003 sgd_solver.cpp:112] Iteration 125670, lr = 0.01
I0523 02:52:12.507691 35003 solver.cpp:239] Iteration 125680 (2.3204 iter/s, 4.3096s/10 iters), loss = 6.49276
I0523 02:52:12.507746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49276 (* 1 = 6.49276 loss)
I0523 02:52:12.624327 35003 sgd_solver.cpp:112] Iteration 125680, lr = 0.01
I0523 02:52:18.108605 35003 solver.cpp:239] Iteration 125690 (1.78551 iter/s, 5.60064s/10 iters), loss = 6.85741
I0523 02:52:18.108651 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85741 (* 1 = 6.85741 loss)
I0523 02:52:18.121893 35003 sgd_solver.cpp:112] Iteration 125690, lr = 0.01
I0523 02:52:23.165098 35003 solver.cpp:239] Iteration 125700 (1.97775 iter/s, 5.05624s/10 iters), loss = 8.41639
I0523 02:52:23.165138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.41639 (* 1 = 8.41639 loss)
I0523 02:52:23.178045 35003 sgd_solver.cpp:112] Iteration 125700, lr = 0.01
I0523 02:52:26.384114 35003 solver.cpp:239] Iteration 125710 (3.10671 iter/s, 3.21884s/10 iters), loss = 7.14823
I0523 02:52:26.384155 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14823 (* 1 = 7.14823 loss)
I0523 02:52:26.390168 35003 sgd_solver.cpp:112] Iteration 125710, lr = 0.01
I0523 02:52:31.071561 35003 solver.cpp:239] Iteration 125720 (2.13347 iter/s, 4.68721s/10 iters), loss = 6.72782
I0523 02:52:31.071615 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72782 (* 1 = 6.72782 loss)
I0523 02:52:31.765492 35003 sgd_solver.cpp:112] Iteration 125720, lr = 0.01
I0523 02:52:34.631250 35003 solver.cpp:239] Iteration 125730 (2.8094 iter/s, 3.55948s/10 iters), loss = 6.6301
I0523 02:52:34.631317 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6301 (* 1 = 6.6301 loss)
I0523 02:52:35.365135 35003 sgd_solver.cpp:112] Iteration 125730, lr = 0.01
I0523 02:52:39.750340 35003 solver.cpp:239] Iteration 125740 (1.95358 iter/s, 5.11882s/10 iters), loss = 7.08726
I0523 02:52:39.750535 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08726 (* 1 = 7.08726 loss)
I0523 02:52:39.764453 35003 sgd_solver.cpp:112] Iteration 125740, lr = 0.01
I0523 02:52:42.693176 35003 solver.cpp:239] Iteration 125750 (3.39841 iter/s, 2.94255s/10 iters), loss = 7.72065
I0523 02:52:42.693228 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72065 (* 1 = 7.72065 loss)
I0523 02:52:43.402211 35003 sgd_solver.cpp:112] Iteration 125750, lr = 0.01
I0523 02:52:47.758720 35003 solver.cpp:239] Iteration 125760 (1.97424 iter/s, 5.06525s/10 iters), loss = 7.70738
I0523 02:52:47.758780 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70738 (* 1 = 7.70738 loss)
I0523 02:52:48.493829 35003 sgd_solver.cpp:112] Iteration 125760, lr = 0.01
I0523 02:52:49.906891 35003 solver.cpp:239] Iteration 125770 (4.65546 iter/s, 2.14802s/10 iters), loss = 7.0156
I0523 02:52:49.906934 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0156 (* 1 = 7.0156 loss)
I0523 02:52:49.920385 35003 sgd_solver.cpp:112] Iteration 125770, lr = 0.01
I0523 02:52:52.330394 35003 solver.cpp:239] Iteration 125780 (4.12651 iter/s, 2.42335s/10 iters), loss = 6.66672
I0523 02:52:52.330437 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66672 (* 1 = 6.66672 loss)
I0523 02:52:52.343611 35003 sgd_solver.cpp:112] Iteration 125780, lr = 0.01
I0523 02:52:56.651124 35003 solver.cpp:239] Iteration 125790 (2.31454 iter/s, 4.32051s/10 iters), loss = 5.86096
I0523 02:52:56.651186 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.86096 (* 1 = 5.86096 loss)
I0523 02:52:57.366516 35003 sgd_solver.cpp:112] Iteration 125790, lr = 0.01
I0523 02:53:00.927390 35003 solver.cpp:239] Iteration 125800 (2.33862 iter/s, 4.27603s/10 iters), loss = 6.69614
I0523 02:53:00.927436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69614 (* 1 = 6.69614 loss)
I0523 02:53:00.946187 35003 sgd_solver.cpp:112] Iteration 125800, lr = 0.01
I0523 02:53:04.672689 35003 solver.cpp:239] Iteration 125810 (2.67016 iter/s, 3.7451s/10 iters), loss = 7.39009
I0523 02:53:04.672734 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39009 (* 1 = 7.39009 loss)
I0523 02:53:04.685870 35003 sgd_solver.cpp:112] Iteration 125810, lr = 0.01
I0523 02:53:07.139065 35003 solver.cpp:239] Iteration 125820 (4.05479 iter/s, 2.46622s/10 iters), loss = 7.14094
I0523 02:53:07.139116 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14094 (* 1 = 7.14094 loss)
I0523 02:53:07.144907 35003 sgd_solver.cpp:112] Iteration 125820, lr = 0.01
I0523 02:53:10.924713 35003 solver.cpp:239] Iteration 125830 (2.64171 iter/s, 3.78542s/10 iters), loss = 8.17294
I0523 02:53:10.925021 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17294 (* 1 = 8.17294 loss)
I0523 02:53:10.937088 35003 sgd_solver.cpp:112] Iteration 125830, lr = 0.01
I0523 02:53:15.320186 35003 solver.cpp:239] Iteration 125840 (2.27532 iter/s, 4.39499s/10 iters), loss = 6.88423
I0523 02:53:15.320235 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88423 (* 1 = 6.88423 loss)
I0523 02:53:15.331719 35003 sgd_solver.cpp:112] Iteration 125840, lr = 0.01
I0523 02:53:18.796396 35003 solver.cpp:239] Iteration 125850 (2.87686 iter/s, 3.47601s/10 iters), loss = 6.67532
I0523 02:53:18.796443 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67532 (* 1 = 6.67532 loss)
I0523 02:53:18.814996 35003 sgd_solver.cpp:112] Iteration 125850, lr = 0.01
I0523 02:53:22.353858 35003 solver.cpp:239] Iteration 125860 (2.81115 iter/s, 3.55726s/10 iters), loss = 7.02662
I0523 02:53:22.353916 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02662 (* 1 = 7.02662 loss)
I0523 02:53:23.013320 35003 sgd_solver.cpp:112] Iteration 125860, lr = 0.01
I0523 02:53:26.667531 35003 solver.cpp:239] Iteration 125870 (2.31834 iter/s, 4.31343s/10 iters), loss = 7.76053
I0523 02:53:26.667583 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76053 (* 1 = 7.76053 loss)
I0523 02:53:27.330273 35003 sgd_solver.cpp:112] Iteration 125870, lr = 0.01
I0523 02:53:30.130492 35003 solver.cpp:239] Iteration 125880 (2.88786 iter/s, 3.46277s/10 iters), loss = 7.40268
I0523 02:53:30.130537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40268 (* 1 = 7.40268 loss)
I0523 02:53:30.137522 35003 sgd_solver.cpp:112] Iteration 125880, lr = 0.01
I0523 02:53:33.040011 35003 solver.cpp:239] Iteration 125890 (3.4372 iter/s, 2.90935s/10 iters), loss = 7.41953
I0523 02:53:33.040066 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41953 (* 1 = 7.41953 loss)
I0523 02:53:33.719884 35003 sgd_solver.cpp:112] Iteration 125890, lr = 0.01
I0523 02:53:36.591639 35003 solver.cpp:239] Iteration 125900 (2.81577 iter/s, 3.55143s/10 iters), loss = 7.55811
I0523 02:53:36.591682 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55811 (* 1 = 7.55811 loss)
I0523 02:53:37.333056 35003 sgd_solver.cpp:112] Iteration 125900, lr = 0.01
I0523 02:53:39.016286 35003 solver.cpp:239] Iteration 125910 (4.12457 iter/s, 2.4245s/10 iters), loss = 7.38816
I0523 02:53:39.016338 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38816 (* 1 = 7.38816 loss)
I0523 02:53:39.025465 35003 sgd_solver.cpp:112] Iteration 125910, lr = 0.01
I0523 02:53:42.481182 35003 solver.cpp:239] Iteration 125920 (2.88625 iter/s, 3.4647s/10 iters), loss = 6.94782
I0523 02:53:42.481470 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94782 (* 1 = 6.94782 loss)
I0523 02:53:42.495751 35003 sgd_solver.cpp:112] Iteration 125920, lr = 0.01
I0523 02:53:47.235291 35003 solver.cpp:239] Iteration 125930 (2.10365 iter/s, 4.75364s/10 iters), loss = 6.11297
I0523 02:53:47.235380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11297 (* 1 = 6.11297 loss)
I0523 02:53:47.950400 35003 sgd_solver.cpp:112] Iteration 125930, lr = 0.01
I0523 02:53:50.016716 35003 solver.cpp:239] Iteration 125940 (3.59554 iter/s, 2.78122s/10 iters), loss = 6.65381
I0523 02:53:50.016760 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65381 (* 1 = 6.65381 loss)
I0523 02:53:50.757824 35003 sgd_solver.cpp:112] Iteration 125940, lr = 0.01
I0523 02:53:52.854573 35003 solver.cpp:239] Iteration 125950 (3.52399 iter/s, 2.8377s/10 iters), loss = 6.44052
I0523 02:53:52.854614 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44052 (* 1 = 6.44052 loss)
I0523 02:53:52.906463 35003 sgd_solver.cpp:112] Iteration 125950, lr = 0.01
I0523 02:53:56.604962 35003 solver.cpp:239] Iteration 125960 (2.66654 iter/s, 3.75019s/10 iters), loss = 7.08213
I0523 02:53:56.605018 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08213 (* 1 = 7.08213 loss)
I0523 02:53:57.345471 35003 sgd_solver.cpp:112] Iteration 125960, lr = 0.01
I0523 02:53:59.371644 35003 solver.cpp:239] Iteration 125970 (3.61466 iter/s, 2.76651s/10 iters), loss = 6.75092
I0523 02:53:59.371692 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75092 (* 1 = 6.75092 loss)
I0523 02:53:59.381901 35003 sgd_solver.cpp:112] Iteration 125970, lr = 0.01
I0523 02:54:03.568387 35003 solver.cpp:239] Iteration 125980 (2.38293 iter/s, 4.19651s/10 iters), loss = 6.77668
I0523 02:54:03.568429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77668 (* 1 = 6.77668 loss)
I0523 02:54:03.581537 35003 sgd_solver.cpp:112] Iteration 125980, lr = 0.01
I0523 02:54:06.879281 35003 solver.cpp:239] Iteration 125990 (3.0205 iter/s, 3.31071s/10 iters), loss = 6.72589
I0523 02:54:06.879325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72589 (* 1 = 6.72589 loss)
I0523 02:54:06.885424 35003 sgd_solver.cpp:112] Iteration 125990, lr = 0.01
I0523 02:54:09.077728 35003 solver.cpp:239] Iteration 126000 (4.54896 iter/s, 2.1983s/10 iters), loss = 5.99355
I0523 02:54:09.077780 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99355 (* 1 = 5.99355 loss)
I0523 02:54:09.086148 35003 sgd_solver.cpp:112] Iteration 126000, lr = 0.01
I0523 02:54:14.259735 35003 solver.cpp:239] Iteration 126010 (1.92985 iter/s, 5.18174s/10 iters), loss = 6.27099
I0523 02:54:14.259991 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27099 (* 1 = 6.27099 loss)
I0523 02:54:14.327216 35003 sgd_solver.cpp:112] Iteration 126010, lr = 0.01
I0523 02:54:16.361667 35003 solver.cpp:239] Iteration 126020 (4.75825 iter/s, 2.10161s/10 iters), loss = 7.15638
I0523 02:54:16.361716 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15638 (* 1 = 7.15638 loss)
I0523 02:54:16.370245 35003 sgd_solver.cpp:112] Iteration 126020, lr = 0.01
I0523 02:54:18.437543 35003 solver.cpp:239] Iteration 126030 (4.81757 iter/s, 2.07574s/10 iters), loss = 7.94528
I0523 02:54:18.437584 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94528 (* 1 = 7.94528 loss)
I0523 02:54:18.456071 35003 sgd_solver.cpp:112] Iteration 126030, lr = 0.01
I0523 02:54:22.676069 35003 solver.cpp:239] Iteration 126040 (2.35943 iter/s, 4.23831s/10 iters), loss = 7.77232
I0523 02:54:22.676110 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77232 (* 1 = 7.77232 loss)
I0523 02:54:22.693825 35003 sgd_solver.cpp:112] Iteration 126040, lr = 0.01
I0523 02:54:24.741569 35003 solver.cpp:239] Iteration 126050 (4.84175 iter/s, 2.06537s/10 iters), loss = 6.94992
I0523 02:54:24.741626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94992 (* 1 = 6.94992 loss)
I0523 02:54:24.749706 35003 sgd_solver.cpp:112] Iteration 126050, lr = 0.01
I0523 02:54:26.944880 35003 solver.cpp:239] Iteration 126060 (4.53893 iter/s, 2.20316s/10 iters), loss = 6.74913
I0523 02:54:26.944926 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74913 (* 1 = 6.74913 loss)
I0523 02:54:26.967711 35003 sgd_solver.cpp:112] Iteration 126060, lr = 0.01
I0523 02:54:29.072990 35003 solver.cpp:239] Iteration 126070 (4.69933 iter/s, 2.12796s/10 iters), loss = 6.19882
I0523 02:54:29.073056 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19882 (* 1 = 6.19882 loss)
I0523 02:54:29.785920 35003 sgd_solver.cpp:112] Iteration 126070, lr = 0.01
I0523 02:54:33.044462 35003 solver.cpp:239] Iteration 126080 (2.51811 iter/s, 3.97124s/10 iters), loss = 7.48275
I0523 02:54:33.044515 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48275 (* 1 = 7.48275 loss)
I0523 02:54:33.056823 35003 sgd_solver.cpp:112] Iteration 126080, lr = 0.01
I0523 02:54:35.813030 35003 solver.cpp:239] Iteration 126090 (3.6122 iter/s, 2.7684s/10 iters), loss = 7.0227
I0523 02:54:35.813077 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0227 (* 1 = 7.0227 loss)
I0523 02:54:36.541760 35003 sgd_solver.cpp:112] Iteration 126090, lr = 0.01
I0523 02:54:40.082613 35003 solver.cpp:239] Iteration 126100 (2.34227 iter/s, 4.26936s/10 iters), loss = 6.74116
I0523 02:54:40.082654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74116 (* 1 = 6.74116 loss)
I0523 02:54:40.090657 35003 sgd_solver.cpp:112] Iteration 126100, lr = 0.01
I0523 02:54:43.607095 35003 solver.cpp:239] Iteration 126110 (2.83745 iter/s, 3.52429s/10 iters), loss = 7.66417
I0523 02:54:43.607136 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66417 (* 1 = 7.66417 loss)
I0523 02:54:43.629683 35003 sgd_solver.cpp:112] Iteration 126110, lr = 0.01
I0523 02:54:46.503943 35003 solver.cpp:239] Iteration 126120 (3.45222 iter/s, 2.89669s/10 iters), loss = 7.09253
I0523 02:54:46.504211 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09253 (* 1 = 7.09253 loss)
I0523 02:54:46.668341 35003 sgd_solver.cpp:112] Iteration 126120, lr = 0.01
I0523 02:54:51.016747 35003 solver.cpp:239] Iteration 126130 (2.21613 iter/s, 4.51238s/10 iters), loss = 7.54601
I0523 02:54:51.016800 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54601 (* 1 = 7.54601 loss)
I0523 02:54:51.021010 35003 sgd_solver.cpp:112] Iteration 126130, lr = 0.01
I0523 02:54:55.356204 35003 solver.cpp:239] Iteration 126140 (2.30457 iter/s, 4.33921s/10 iters), loss = 7.7976
I0523 02:54:55.356259 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7976 (* 1 = 7.7976 loss)
I0523 02:54:56.090824 35003 sgd_solver.cpp:112] Iteration 126140, lr = 0.01
I0523 02:54:59.077577 35003 solver.cpp:239] Iteration 126150 (2.68734 iter/s, 3.72116s/10 iters), loss = 6.7126
I0523 02:54:59.077627 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7126 (* 1 = 6.7126 loss)
I0523 02:54:59.753973 35003 sgd_solver.cpp:112] Iteration 126150, lr = 0.01
I0523 02:55:02.759310 35003 solver.cpp:239] Iteration 126160 (2.71627 iter/s, 3.68152s/10 iters), loss = 6.99789
I0523 02:55:02.759357 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99789 (* 1 = 6.99789 loss)
I0523 02:55:02.802209 35003 sgd_solver.cpp:112] Iteration 126160, lr = 0.01
I0523 02:55:07.025624 35003 solver.cpp:239] Iteration 126170 (2.34407 iter/s, 4.26609s/10 iters), loss = 6.6106
I0523 02:55:07.025671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6106 (* 1 = 6.6106 loss)
I0523 02:55:07.734642 35003 sgd_solver.cpp:112] Iteration 126170, lr = 0.01
I0523 02:55:12.182801 35003 solver.cpp:239] Iteration 126180 (1.93914 iter/s, 5.15691s/10 iters), loss = 7.37696
I0523 02:55:12.182860 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37696 (* 1 = 7.37696 loss)
I0523 02:55:12.773380 35003 sgd_solver.cpp:112] Iteration 126180, lr = 0.01
I0523 02:55:15.619546 35003 solver.cpp:239] Iteration 126190 (2.90991 iter/s, 3.43653s/10 iters), loss = 7.48518
I0523 02:55:15.619606 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48518 (* 1 = 7.48518 loss)
I0523 02:55:16.314934 35003 sgd_solver.cpp:112] Iteration 126190, lr = 0.01
I0523 02:55:21.552024 35003 solver.cpp:239] Iteration 126200 (1.68572 iter/s, 5.93219s/10 iters), loss = 7.69069
I0523 02:55:21.552290 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69069 (* 1 = 7.69069 loss)
I0523 02:55:22.255137 35003 sgd_solver.cpp:112] Iteration 126200, lr = 0.01
I0523 02:55:24.413980 35003 solver.cpp:239] Iteration 126210 (3.49454 iter/s, 2.8616s/10 iters), loss = 6.5266
I0523 02:55:24.414034 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5266 (* 1 = 6.5266 loss)
I0523 02:55:25.135275 35003 sgd_solver.cpp:112] Iteration 126210, lr = 0.01
I0523 02:55:30.197706 35003 solver.cpp:239] Iteration 126220 (1.72908 iter/s, 5.78342s/10 iters), loss = 8.71393
I0523 02:55:30.197760 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.71393 (* 1 = 8.71393 loss)
I0523 02:55:30.205323 35003 sgd_solver.cpp:112] Iteration 126220, lr = 0.01
I0523 02:55:33.771538 35003 solver.cpp:239] Iteration 126230 (2.79827 iter/s, 3.57363s/10 iters), loss = 6.36208
I0523 02:55:33.771581 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36208 (* 1 = 6.36208 loss)
I0523 02:55:33.789432 35003 sgd_solver.cpp:112] Iteration 126230, lr = 0.01
I0523 02:55:37.387981 35003 solver.cpp:239] Iteration 126240 (2.76867 iter/s, 3.61185s/10 iters), loss = 7.52015
I0523 02:55:37.388026 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52015 (* 1 = 7.52015 loss)
I0523 02:55:37.823173 35003 sgd_solver.cpp:112] Iteration 126240, lr = 0.01
I0523 02:55:41.450479 35003 solver.cpp:239] Iteration 126250 (2.46168 iter/s, 4.06227s/10 iters), loss = 6.1396
I0523 02:55:41.450529 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1396 (* 1 = 6.1396 loss)
I0523 02:55:41.463156 35003 sgd_solver.cpp:112] Iteration 126250, lr = 0.01
I0523 02:55:43.534407 35003 solver.cpp:239] Iteration 126260 (4.79896 iter/s, 2.08378s/10 iters), loss = 7.16188
I0523 02:55:43.534451 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16188 (* 1 = 7.16188 loss)
I0523 02:55:43.733002 35003 sgd_solver.cpp:112] Iteration 126260, lr = 0.01
I0523 02:55:45.845103 35003 solver.cpp:239] Iteration 126270 (4.32798 iter/s, 2.31055s/10 iters), loss = 8.49144
I0523 02:55:45.845155 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.49144 (* 1 = 8.49144 loss)
I0523 02:55:46.528228 35003 sgd_solver.cpp:112] Iteration 126270, lr = 0.01
I0523 02:55:50.159849 35003 solver.cpp:239] Iteration 126280 (2.31776 iter/s, 4.31452s/10 iters), loss = 6.72358
I0523 02:55:50.159900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72358 (* 1 = 6.72358 loss)
I0523 02:55:50.853308 35003 sgd_solver.cpp:112] Iteration 126280, lr = 0.01
I0523 02:55:54.471015 35003 solver.cpp:239] Iteration 126290 (2.31968 iter/s, 4.31094s/10 iters), loss = 8.08007
I0523 02:55:54.471216 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08007 (* 1 = 8.08007 loss)
I0523 02:55:54.484095 35003 sgd_solver.cpp:112] Iteration 126290, lr = 0.01
I0523 02:55:58.836642 35003 solver.cpp:239] Iteration 126300 (2.29082 iter/s, 4.36525s/10 iters), loss = 6.99556
I0523 02:55:58.836683 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99556 (* 1 = 6.99556 loss)
I0523 02:55:58.843155 35003 sgd_solver.cpp:112] Iteration 126300, lr = 0.01
I0523 02:56:01.151296 35003 solver.cpp:239] Iteration 126310 (4.32057 iter/s, 2.31451s/10 iters), loss = 7.37793
I0523 02:56:01.151347 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37793 (* 1 = 7.37793 loss)
I0523 02:56:01.159588 35003 sgd_solver.cpp:112] Iteration 126310, lr = 0.01
I0523 02:56:04.410414 35003 solver.cpp:239] Iteration 126320 (3.0685 iter/s, 3.25893s/10 iters), loss = 7.46744
I0523 02:56:04.410475 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46744 (* 1 = 7.46744 loss)
I0523 02:56:05.151903 35003 sgd_solver.cpp:112] Iteration 126320, lr = 0.01
I0523 02:56:07.880832 35003 solver.cpp:239] Iteration 126330 (2.88166 iter/s, 3.47022s/10 iters), loss = 5.96023
I0523 02:56:07.880872 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96023 (* 1 = 5.96023 loss)
I0523 02:56:07.885123 35003 sgd_solver.cpp:112] Iteration 126330, lr = 0.01
I0523 02:56:11.027262 35003 solver.cpp:239] Iteration 126340 (3.17838 iter/s, 3.14626s/10 iters), loss = 7.88075
I0523 02:56:11.027318 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88075 (* 1 = 7.88075 loss)
I0523 02:56:11.039073 35003 sgd_solver.cpp:112] Iteration 126340, lr = 0.01
I0523 02:56:14.006008 35003 solver.cpp:239] Iteration 126350 (3.35733 iter/s, 2.97855s/10 iters), loss = 7.38232
I0523 02:56:14.006067 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38232 (* 1 = 7.38232 loss)
I0523 02:56:14.616156 35003 sgd_solver.cpp:112] Iteration 126350, lr = 0.01
I0523 02:56:18.197695 35003 solver.cpp:239] Iteration 126360 (2.38581 iter/s, 4.19145s/10 iters), loss = 7.46
I0523 02:56:18.197751 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46 (* 1 = 7.46 loss)
I0523 02:56:18.210777 35003 sgd_solver.cpp:112] Iteration 126360, lr = 0.01
I0523 02:56:20.417023 35003 solver.cpp:239] Iteration 126370 (4.50619 iter/s, 2.21917s/10 iters), loss = 6.19565
I0523 02:56:20.417073 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19565 (* 1 = 6.19565 loss)
I0523 02:56:20.420120 35003 sgd_solver.cpp:112] Iteration 126370, lr = 0.01
I0523 02:56:23.947767 35003 solver.cpp:239] Iteration 126380 (2.83243 iter/s, 3.53053s/10 iters), loss = 6.74056
I0523 02:56:23.947814 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74056 (* 1 = 6.74056 loss)
I0523 02:56:23.968261 35003 sgd_solver.cpp:112] Iteration 126380, lr = 0.01
I0523 02:56:27.405920 35003 solver.cpp:239] Iteration 126390 (2.89387 iter/s, 3.45558s/10 iters), loss = 6.13976
I0523 02:56:27.406172 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13976 (* 1 = 6.13976 loss)
I0523 02:56:27.412631 35003 sgd_solver.cpp:112] Iteration 126390, lr = 0.01
I0523 02:56:30.887753 35003 solver.cpp:239] Iteration 126400 (2.87237 iter/s, 3.48145s/10 iters), loss = 6.10326
I0523 02:56:30.887804 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10326 (* 1 = 6.10326 loss)
I0523 02:56:30.900929 35003 sgd_solver.cpp:112] Iteration 126400, lr = 0.01
I0523 02:56:32.935189 35003 solver.cpp:239] Iteration 126410 (4.88451 iter/s, 2.04729s/10 iters), loss = 6.48925
I0523 02:56:32.935241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48925 (* 1 = 6.48925 loss)
I0523 02:56:32.948091 35003 sgd_solver.cpp:112] Iteration 126410, lr = 0.01
I0523 02:56:36.563717 35003 solver.cpp:239] Iteration 126420 (2.75612 iter/s, 3.62829s/10 iters), loss = 7.38972
I0523 02:56:36.563777 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38972 (* 1 = 7.38972 loss)
I0523 02:56:36.568305 35003 sgd_solver.cpp:112] Iteration 126420, lr = 0.01
I0523 02:56:40.370834 35003 solver.cpp:239] Iteration 126430 (2.62681 iter/s, 3.8069s/10 iters), loss = 8.32186
I0523 02:56:40.370888 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.32186 (* 1 = 8.32186 loss)
I0523 02:56:41.014977 35003 sgd_solver.cpp:112] Iteration 126430, lr = 0.01
I0523 02:56:42.270905 35003 solver.cpp:239] Iteration 126440 (5.26472 iter/s, 1.89944s/10 iters), loss = 5.90441
I0523 02:56:42.270952 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.90441 (* 1 = 5.90441 loss)
I0523 02:56:42.284162 35003 sgd_solver.cpp:112] Iteration 126440, lr = 0.01
I0523 02:56:46.105876 35003 solver.cpp:239] Iteration 126450 (2.60772 iter/s, 3.83476s/10 iters), loss = 7.33291
I0523 02:56:46.105943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33291 (* 1 = 7.33291 loss)
I0523 02:56:46.641446 35003 sgd_solver.cpp:112] Iteration 126450, lr = 0.01
I0523 02:56:49.354601 35003 solver.cpp:239] Iteration 126460 (3.07832 iter/s, 3.24852s/10 iters), loss = 7.6212
I0523 02:56:49.354642 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6212 (* 1 = 7.6212 loss)
I0523 02:56:49.367812 35003 sgd_solver.cpp:112] Iteration 126460, lr = 0.01
I0523 02:56:52.222301 35003 solver.cpp:239] Iteration 126470 (3.48732 iter/s, 2.86753s/10 iters), loss = 7.93009
I0523 02:56:52.222348 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93009 (* 1 = 7.93009 loss)
I0523 02:56:52.235182 35003 sgd_solver.cpp:112] Iteration 126470, lr = 0.01
I0523 02:56:54.301977 35003 solver.cpp:239] Iteration 126480 (4.80876 iter/s, 2.07954s/10 iters), loss = 6.95873
I0523 02:56:54.302018 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95873 (* 1 = 6.95873 loss)
I0523 02:56:54.309655 35003 sgd_solver.cpp:112] Iteration 126480, lr = 0.01
I0523 02:56:57.101675 35003 solver.cpp:239] Iteration 126490 (3.57203 iter/s, 2.79953s/10 iters), loss = 7.31448
I0523 02:56:57.101721 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31448 (* 1 = 7.31448 loss)
I0523 02:56:57.111351 35003 sgd_solver.cpp:112] Iteration 126490, lr = 0.01
I0523 02:57:02.026767 35003 solver.cpp:239] Iteration 126500 (2.03052 iter/s, 4.92484s/10 iters), loss = 8.87583
I0523 02:57:02.027031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.87583 (* 1 = 8.87583 loss)
I0523 02:57:02.034157 35003 sgd_solver.cpp:112] Iteration 126500, lr = 0.01
I0523 02:57:05.511519 35003 solver.cpp:239] Iteration 126510 (2.86996 iter/s, 3.48437s/10 iters), loss = 7.10657
I0523 02:57:05.511559 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10657 (* 1 = 7.10657 loss)
I0523 02:57:05.524500 35003 sgd_solver.cpp:112] Iteration 126510, lr = 0.01
I0523 02:57:08.915432 35003 solver.cpp:239] Iteration 126520 (2.93796 iter/s, 3.40373s/10 iters), loss = 7.0693
I0523 02:57:08.915483 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0693 (* 1 = 7.0693 loss)
I0523 02:57:08.928472 35003 sgd_solver.cpp:112] Iteration 126520, lr = 0.01
I0523 02:57:13.288872 35003 solver.cpp:239] Iteration 126530 (2.28665 iter/s, 4.37321s/10 iters), loss = 6.98574
I0523 02:57:13.288910 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98574 (* 1 = 6.98574 loss)
I0523 02:57:13.301919 35003 sgd_solver.cpp:112] Iteration 126530, lr = 0.01
I0523 02:57:16.921409 35003 solver.cpp:239] Iteration 126540 (2.75305 iter/s, 3.63234s/10 iters), loss = 7.33759
I0523 02:57:16.921461 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33759 (* 1 = 7.33759 loss)
I0523 02:57:17.636751 35003 sgd_solver.cpp:112] Iteration 126540, lr = 0.01
I0523 02:57:21.188163 35003 solver.cpp:239] Iteration 126550 (2.34384 iter/s, 4.2665s/10 iters), loss = 7.06384
I0523 02:57:21.188202 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06384 (* 1 = 7.06384 loss)
I0523 02:57:21.206554 35003 sgd_solver.cpp:112] Iteration 126550, lr = 0.01
I0523 02:57:24.852488 35003 solver.cpp:239] Iteration 126560 (2.72917 iter/s, 3.66412s/10 iters), loss = 7.57296
I0523 02:57:24.852553 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57296 (* 1 = 7.57296 loss)
I0523 02:57:24.877750 35003 sgd_solver.cpp:112] Iteration 126560, lr = 0.01
I0523 02:57:28.411465 35003 solver.cpp:239] Iteration 126570 (2.80995 iter/s, 3.55878s/10 iters), loss = 7.80967
I0523 02:57:28.411507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80967 (* 1 = 7.80967 loss)
I0523 02:57:28.424866 35003 sgd_solver.cpp:112] Iteration 126570, lr = 0.01
I0523 02:57:30.654657 35003 solver.cpp:239] Iteration 126580 (4.45822 iter/s, 2.24305s/10 iters), loss = 6.96558
I0523 02:57:30.654728 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96558 (* 1 = 6.96558 loss)
I0523 02:57:31.395445 35003 sgd_solver.cpp:112] Iteration 126580, lr = 0.01
I0523 02:57:34.355731 35003 solver.cpp:239] Iteration 126590 (2.70208 iter/s, 3.70085s/10 iters), loss = 7.19394
I0523 02:57:34.355975 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19394 (* 1 = 7.19394 loss)
I0523 02:57:34.368180 35003 sgd_solver.cpp:112] Iteration 126590, lr = 0.01
I0523 02:57:37.363399 35003 solver.cpp:239] Iteration 126600 (3.32523 iter/s, 3.00731s/10 iters), loss = 6.37822
I0523 02:57:37.363440 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37822 (* 1 = 6.37822 loss)
I0523 02:57:37.376930 35003 sgd_solver.cpp:112] Iteration 126600, lr = 0.01
I0523 02:57:41.564955 35003 solver.cpp:239] Iteration 126610 (2.3802 iter/s, 4.20133s/10 iters), loss = 7.14604
I0523 02:57:41.565018 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14604 (* 1 = 7.14604 loss)
I0523 02:57:42.225790 35003 sgd_solver.cpp:112] Iteration 126610, lr = 0.01
I0523 02:57:46.430785 35003 solver.cpp:239] Iteration 126620 (2.05526 iter/s, 4.86557s/10 iters), loss = 7.10324
I0523 02:57:46.430831 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10324 (* 1 = 7.10324 loss)
I0523 02:57:46.444494 35003 sgd_solver.cpp:112] Iteration 126620, lr = 0.01
I0523 02:57:49.822803 35003 solver.cpp:239] Iteration 126630 (2.94826 iter/s, 3.39183s/10 iters), loss = 7.30997
I0523 02:57:49.822851 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30997 (* 1 = 7.30997 loss)
I0523 02:57:49.829855 35003 sgd_solver.cpp:112] Iteration 126630, lr = 0.01
I0523 02:57:54.486367 35003 solver.cpp:239] Iteration 126640 (2.1444 iter/s, 4.66331s/10 iters), loss = 6.26221
I0523 02:57:54.486414 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26221 (* 1 = 6.26221 loss)
I0523 02:57:55.201867 35003 sgd_solver.cpp:112] Iteration 126640, lr = 0.01
I0523 02:57:57.316596 35003 solver.cpp:239] Iteration 126650 (3.53349 iter/s, 2.83006s/10 iters), loss = 7.16979
I0523 02:57:57.316646 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16979 (* 1 = 7.16979 loss)
I0523 02:57:57.575954 35003 sgd_solver.cpp:112] Iteration 126650, lr = 0.01
I0523 02:57:59.660990 35003 solver.cpp:239] Iteration 126660 (4.26579 iter/s, 2.34423s/10 iters), loss = 6.48389
I0523 02:57:59.661046 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48389 (* 1 = 6.48389 loss)
I0523 02:57:59.695355 35003 sgd_solver.cpp:112] Iteration 126660, lr = 0.01
I0523 02:58:02.613222 35003 solver.cpp:239] Iteration 126670 (3.38748 iter/s, 2.95205s/10 iters), loss = 6.346
I0523 02:58:02.613287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.346 (* 1 = 6.346 loss)
I0523 02:58:03.334568 35003 sgd_solver.cpp:112] Iteration 126670, lr = 0.01
I0523 02:58:05.703172 35003 solver.cpp:239] Iteration 126680 (3.2365 iter/s, 3.08976s/10 iters), loss = 5.57707
I0523 02:58:05.703364 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.57707 (* 1 = 5.57707 loss)
I0523 02:58:05.719650 35003 sgd_solver.cpp:112] Iteration 126680, lr = 0.01
I0523 02:58:10.798790 35003 solver.cpp:239] Iteration 126690 (1.96262 iter/s, 5.09524s/10 iters), loss = 6.83473
I0523 02:58:10.798835 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83473 (* 1 = 6.83473 loss)
I0523 02:58:10.811974 35003 sgd_solver.cpp:112] Iteration 126690, lr = 0.01
I0523 02:58:14.316962 35003 solver.cpp:239] Iteration 126700 (2.84254 iter/s, 3.51798s/10 iters), loss = 7.01369
I0523 02:58:14.317011 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01369 (* 1 = 7.01369 loss)
I0523 02:58:14.330582 35003 sgd_solver.cpp:112] Iteration 126700, lr = 0.01
I0523 02:58:17.911996 35003 solver.cpp:239] Iteration 126710 (2.78177 iter/s, 3.59484s/10 iters), loss = 7.28724
I0523 02:58:17.912045 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28724 (* 1 = 7.28724 loss)
I0523 02:58:18.653338 35003 sgd_solver.cpp:112] Iteration 126710, lr = 0.01
I0523 02:58:21.842144 35003 solver.cpp:239] Iteration 126720 (2.54459 iter/s, 3.92991s/10 iters), loss = 6.23679
I0523 02:58:21.842193 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23679 (* 1 = 6.23679 loss)
I0523 02:58:21.855120 35003 sgd_solver.cpp:112] Iteration 126720, lr = 0.01
I0523 02:58:25.455732 35003 solver.cpp:239] Iteration 126730 (2.76749 iter/s, 3.61339s/10 iters), loss = 6.55253
I0523 02:58:25.455768 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55253 (* 1 = 6.55253 loss)
I0523 02:58:25.469336 35003 sgd_solver.cpp:112] Iteration 126730, lr = 0.01
I0523 02:58:28.506531 35003 solver.cpp:239] Iteration 126740 (3.27801 iter/s, 3.05063s/10 iters), loss = 7.2922
I0523 02:58:28.506579 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2922 (* 1 = 7.2922 loss)
I0523 02:58:28.519682 35003 sgd_solver.cpp:112] Iteration 126740, lr = 0.01
I0523 02:58:32.621922 35003 solver.cpp:239] Iteration 126750 (2.43003 iter/s, 4.11517s/10 iters), loss = 7.18273
I0523 02:58:32.621963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18273 (* 1 = 7.18273 loss)
I0523 02:58:32.634964 35003 sgd_solver.cpp:112] Iteration 126750, lr = 0.01
I0523 02:58:35.503310 35003 solver.cpp:239] Iteration 126760 (3.47075 iter/s, 2.88122s/10 iters), loss = 6.58945
I0523 02:58:35.503363 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58945 (* 1 = 6.58945 loss)
I0523 02:58:36.237691 35003 sgd_solver.cpp:112] Iteration 126760, lr = 0.01
I0523 02:58:40.610389 35003 solver.cpp:239] Iteration 126770 (1.95817 iter/s, 5.10682s/10 iters), loss = 6.8079
I0523 02:58:40.610435 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8079 (* 1 = 6.8079 loss)
I0523 02:58:40.629112 35003 sgd_solver.cpp:112] Iteration 126770, lr = 0.01
I0523 02:58:41.925832 35003 solver.cpp:239] Iteration 126780 (7.60263 iter/s, 1.31533s/10 iters), loss = 6.80137
I0523 02:58:41.925890 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80137 (* 1 = 6.80137 loss)
I0523 02:58:41.939052 35003 sgd_solver.cpp:112] Iteration 126780, lr = 0.01
I0523 02:58:45.503921 35003 solver.cpp:239] Iteration 126790 (2.79496 iter/s, 3.57787s/10 iters), loss = 6.94224
I0523 02:58:45.503969 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94224 (* 1 = 6.94224 loss)
I0523 02:58:46.097780 35003 sgd_solver.cpp:112] Iteration 126790, lr = 0.01
I0523 02:58:50.501667 35003 solver.cpp:239] Iteration 126800 (2.001 iter/s, 4.99749s/10 iters), loss = 6.46113
I0523 02:58:50.501731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46113 (* 1 = 6.46113 loss)
I0523 02:58:50.871529 35003 sgd_solver.cpp:112] Iteration 126800, lr = 0.01
I0523 02:58:55.216994 35003 solver.cpp:239] Iteration 126810 (2.12086 iter/s, 4.71506s/10 iters), loss = 6.29626
I0523 02:58:55.217041 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29626 (* 1 = 6.29626 loss)
I0523 02:58:55.937995 35003 sgd_solver.cpp:112] Iteration 126810, lr = 0.01
I0523 02:58:58.776876 35003 solver.cpp:239] Iteration 126820 (2.80924 iter/s, 3.55968s/10 iters), loss = 6.66662
I0523 02:58:58.776917 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66662 (* 1 = 6.66662 loss)
I0523 02:58:58.829380 35003 sgd_solver.cpp:112] Iteration 126820, lr = 0.01
I0523 02:59:00.965550 35003 solver.cpp:239] Iteration 126830 (4.56928 iter/s, 2.18853s/10 iters), loss = 6.35411
I0523 02:59:00.965610 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35411 (* 1 = 6.35411 loss)
I0523 02:59:01.665318 35003 sgd_solver.cpp:112] Iteration 126830, lr = 0.01
I0523 02:59:04.399711 35003 solver.cpp:239] Iteration 126840 (2.91209 iter/s, 3.43396s/10 iters), loss = 6.03391
I0523 02:59:04.399754 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03391 (* 1 = 6.03391 loss)
I0523 02:59:04.403528 35003 sgd_solver.cpp:112] Iteration 126840, lr = 0.01
I0523 02:59:07.610450 35003 solver.cpp:239] Iteration 126850 (3.11472 iter/s, 3.21056s/10 iters), loss = 7.84893
I0523 02:59:07.610594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84893 (* 1 = 7.84893 loss)
I0523 02:59:08.342612 35003 sgd_solver.cpp:112] Iteration 126850, lr = 0.01
I0523 02:59:12.495693 35003 solver.cpp:239] Iteration 126860 (2.04712 iter/s, 4.8849s/10 iters), loss = 7.17292
I0523 02:59:12.495740 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17292 (* 1 = 7.17292 loss)
I0523 02:59:12.504031 35003 sgd_solver.cpp:112] Iteration 126860, lr = 0.01
I0523 02:59:14.502156 35003 solver.cpp:239] Iteration 126870 (4.98424 iter/s, 2.00632s/10 iters), loss = 6.93158
I0523 02:59:14.502207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93158 (* 1 = 6.93158 loss)
I0523 02:59:14.512830 35003 sgd_solver.cpp:112] Iteration 126870, lr = 0.01
I0523 02:59:18.626926 35003 solver.cpp:239] Iteration 126880 (2.42451 iter/s, 4.12455s/10 iters), loss = 6.87966
I0523 02:59:18.626965 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87966 (* 1 = 6.87966 loss)
I0523 02:59:18.637172 35003 sgd_solver.cpp:112] Iteration 126880, lr = 0.01
I0523 02:59:23.185922 35003 solver.cpp:239] Iteration 126890 (2.19357 iter/s, 4.55877s/10 iters), loss = 7.409
I0523 02:59:23.185972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.409 (* 1 = 7.409 loss)
I0523 02:59:23.926604 35003 sgd_solver.cpp:112] Iteration 126890, lr = 0.01
I0523 02:59:26.045138 35003 solver.cpp:239] Iteration 126900 (3.49767 iter/s, 2.85905s/10 iters), loss = 6.34012
I0523 02:59:26.045188 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34012 (* 1 = 6.34012 loss)
I0523 02:59:26.156772 35003 sgd_solver.cpp:112] Iteration 126900, lr = 0.01
I0523 02:59:28.950578 35003 solver.cpp:239] Iteration 126910 (3.44202 iter/s, 2.90527s/10 iters), loss = 6.21605
I0523 02:59:28.950625 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21605 (* 1 = 6.21605 loss)
I0523 02:59:28.991478 35003 sgd_solver.cpp:112] Iteration 126910, lr = 0.01
I0523 02:59:32.620254 35003 solver.cpp:239] Iteration 126920 (2.72519 iter/s, 3.66947s/10 iters), loss = 7.0809
I0523 02:59:32.620301 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0809 (* 1 = 7.0809 loss)
I0523 02:59:32.633659 35003 sgd_solver.cpp:112] Iteration 126920, lr = 0.01
I0523 02:59:35.447257 35003 solver.cpp:239] Iteration 126930 (3.53752 iter/s, 2.82684s/10 iters), loss = 6.23677
I0523 02:59:35.447299 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23677 (* 1 = 6.23677 loss)
I0523 02:59:35.452664 35003 sgd_solver.cpp:112] Iteration 126930, lr = 0.01
I0523 02:59:39.032708 35003 solver.cpp:239] Iteration 126940 (2.78922 iter/s, 3.58524s/10 iters), loss = 7.5398
I0523 02:59:39.032923 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5398 (* 1 = 7.5398 loss)
I0523 02:59:39.767571 35003 sgd_solver.cpp:112] Iteration 126940, lr = 0.01
I0523 02:59:42.009553 35003 solver.cpp:239] Iteration 126950 (3.35963 iter/s, 2.97652s/10 iters), loss = 7.77704
I0523 02:59:42.009604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77704 (* 1 = 7.77704 loss)
I0523 02:59:42.025377 35003 sgd_solver.cpp:112] Iteration 126950, lr = 0.01
I0523 02:59:46.387984 35003 solver.cpp:239] Iteration 126960 (2.28404 iter/s, 4.3782s/10 iters), loss = 7.20162
I0523 02:59:46.388027 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20162 (* 1 = 7.20162 loss)
I0523 02:59:47.103469 35003 sgd_solver.cpp:112] Iteration 126960, lr = 0.01
I0523 02:59:49.985219 35003 solver.cpp:239] Iteration 126970 (2.78009 iter/s, 3.59701s/10 iters), loss = 5.53441
I0523 02:59:49.985280 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.53441 (* 1 = 5.53441 loss)
I0523 02:59:50.706632 35003 sgd_solver.cpp:112] Iteration 126970, lr = 0.01
I0523 02:59:55.567602 35003 solver.cpp:239] Iteration 126980 (1.79144 iter/s, 5.5821s/10 iters), loss = 8.5721
I0523 02:59:55.567651 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.5721 (* 1 = 8.5721 loss)
I0523 02:59:56.283280 35003 sgd_solver.cpp:112] Iteration 126980, lr = 0.01
I0523 02:59:58.770934 35003 solver.cpp:239] Iteration 126990 (3.12193 iter/s, 3.20315s/10 iters), loss = 7.67725
I0523 02:59:58.770982 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67725 (* 1 = 7.67725 loss)
I0523 02:59:58.780179 35003 sgd_solver.cpp:112] Iteration 126990, lr = 0.01
I0523 03:00:01.562255 35003 solver.cpp:239] Iteration 127000 (3.58276 iter/s, 2.79115s/10 iters), loss = 6.37405
I0523 03:00:01.562325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37405 (* 1 = 6.37405 loss)
I0523 03:00:01.795660 35003 sgd_solver.cpp:112] Iteration 127000, lr = 0.01
I0523 03:00:06.063858 35003 solver.cpp:239] Iteration 127010 (2.22155 iter/s, 4.50135s/10 iters), loss = 6.67951
I0523 03:00:06.063900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67951 (* 1 = 6.67951 loss)
I0523 03:00:06.779381 35003 sgd_solver.cpp:112] Iteration 127010, lr = 0.01
I0523 03:00:09.665900 35003 solver.cpp:239] Iteration 127020 (2.77638 iter/s, 3.60182s/10 iters), loss = 6.83704
I0523 03:00:09.666060 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83704 (* 1 = 6.83704 loss)
I0523 03:00:10.174993 35003 sgd_solver.cpp:112] Iteration 127020, lr = 0.01
I0523 03:00:13.604080 35003 solver.cpp:239] Iteration 127030 (2.53944 iter/s, 3.93788s/10 iters), loss = 6.72498
I0523 03:00:13.604121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72498 (* 1 = 6.72498 loss)
I0523 03:00:13.819934 35003 sgd_solver.cpp:112] Iteration 127030, lr = 0.01
I0523 03:00:15.792284 35003 solver.cpp:239] Iteration 127040 (4.57025 iter/s, 2.18806s/10 iters), loss = 6.93272
I0523 03:00:15.792335 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93272 (* 1 = 6.93272 loss)
I0523 03:00:15.805757 35003 sgd_solver.cpp:112] Iteration 127040, lr = 0.01
I0523 03:00:20.340067 35003 solver.cpp:239] Iteration 127050 (2.19899 iter/s, 4.54754s/10 iters), loss = 8.22711
I0523 03:00:20.340121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.22711 (* 1 = 8.22711 loss)
I0523 03:00:20.396672 35003 sgd_solver.cpp:112] Iteration 127050, lr = 0.01
I0523 03:00:23.474556 35003 solver.cpp:239] Iteration 127060 (3.1905 iter/s, 3.13431s/10 iters), loss = 6.88375
I0523 03:00:23.474596 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88375 (* 1 = 6.88375 loss)
I0523 03:00:23.487576 35003 sgd_solver.cpp:112] Iteration 127060, lr = 0.01
I0523 03:00:25.569231 35003 solver.cpp:239] Iteration 127070 (4.77433 iter/s, 2.09454s/10 iters), loss = 7.29486
I0523 03:00:25.569296 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29486 (* 1 = 7.29486 loss)
I0523 03:00:25.576592 35003 sgd_solver.cpp:112] Iteration 127070, lr = 0.01
I0523 03:00:29.196970 35003 solver.cpp:239] Iteration 127080 (2.75669 iter/s, 3.62753s/10 iters), loss = 7.35393
I0523 03:00:29.197007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35393 (* 1 = 7.35393 loss)
I0523 03:00:29.210119 35003 sgd_solver.cpp:112] Iteration 127080, lr = 0.01
I0523 03:00:32.724011 35003 solver.cpp:239] Iteration 127090 (2.83539 iter/s, 3.52685s/10 iters), loss = 6.98488
I0523 03:00:32.724054 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98488 (* 1 = 6.98488 loss)
I0523 03:00:33.449546 35003 sgd_solver.cpp:112] Iteration 127090, lr = 0.01
I0523 03:00:36.979836 35003 solver.cpp:239] Iteration 127100 (2.34984 iter/s, 4.2556s/10 iters), loss = 5.93746
I0523 03:00:36.979887 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93746 (* 1 = 5.93746 loss)
I0523 03:00:37.719141 35003 sgd_solver.cpp:112] Iteration 127100, lr = 0.01
I0523 03:00:40.597980 35003 solver.cpp:239] Iteration 127110 (2.76401 iter/s, 3.61794s/10 iters), loss = 5.97768
I0523 03:00:40.598172 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97768 (* 1 = 5.97768 loss)
I0523 03:00:41.319270 35003 sgd_solver.cpp:112] Iteration 127110, lr = 0.01
I0523 03:00:43.561358 35003 solver.cpp:239] Iteration 127120 (3.37488 iter/s, 2.96307s/10 iters), loss = 6.63663
I0523 03:00:43.561408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63663 (* 1 = 6.63663 loss)
I0523 03:00:43.574470 35003 sgd_solver.cpp:112] Iteration 127120, lr = 0.01
I0523 03:00:47.113956 35003 solver.cpp:239] Iteration 127130 (2.815 iter/s, 3.5524s/10 iters), loss = 6.91276
I0523 03:00:47.114006 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91276 (* 1 = 6.91276 loss)
I0523 03:00:47.424155 35003 sgd_solver.cpp:112] Iteration 127130, lr = 0.01
I0523 03:00:50.298033 35003 solver.cpp:239] Iteration 127140 (3.14082 iter/s, 3.18388s/10 iters), loss = 8.43723
I0523 03:00:50.298087 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.43723 (* 1 = 8.43723 loss)
I0523 03:00:50.927515 35003 sgd_solver.cpp:112] Iteration 127140, lr = 0.01
I0523 03:00:54.508121 35003 solver.cpp:239] Iteration 127150 (2.37537 iter/s, 4.20986s/10 iters), loss = 6.21423
I0523 03:00:54.508158 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21423 (* 1 = 6.21423 loss)
I0523 03:00:54.521526 35003 sgd_solver.cpp:112] Iteration 127150, lr = 0.01
I0523 03:00:59.599238 35003 solver.cpp:239] Iteration 127160 (1.9643 iter/s, 5.09087s/10 iters), loss = 6.52787
I0523 03:00:59.599284 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52787 (* 1 = 6.52787 loss)
I0523 03:00:59.624111 35003 sgd_solver.cpp:112] Iteration 127160, lr = 0.01
I0523 03:01:03.649408 35003 solver.cpp:239] Iteration 127170 (2.46916 iter/s, 4.04996s/10 iters), loss = 6.13494
I0523 03:01:03.649456 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13494 (* 1 = 6.13494 loss)
I0523 03:01:04.387656 35003 sgd_solver.cpp:112] Iteration 127170, lr = 0.01
I0523 03:01:08.383915 35003 solver.cpp:239] Iteration 127180 (2.11226 iter/s, 4.73426s/10 iters), loss = 6.80743
I0523 03:01:08.383960 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80743 (* 1 = 6.80743 loss)
I0523 03:01:08.396924 35003 sgd_solver.cpp:112] Iteration 127180, lr = 0.01
I0523 03:01:10.504469 35003 solver.cpp:239] Iteration 127190 (4.71607 iter/s, 2.12041s/10 iters), loss = 6.86027
I0523 03:01:10.504520 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86027 (* 1 = 6.86027 loss)
I0523 03:01:10.511327 35003 sgd_solver.cpp:112] Iteration 127190, lr = 0.01
I0523 03:01:13.602895 35003 solver.cpp:239] Iteration 127200 (3.22765 iter/s, 3.09823s/10 iters), loss = 7.11618
I0523 03:01:13.603137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11618 (* 1 = 7.11618 loss)
I0523 03:01:13.615594 35003 sgd_solver.cpp:112] Iteration 127200, lr = 0.01
I0523 03:01:17.248807 35003 solver.cpp:239] Iteration 127210 (2.74307 iter/s, 3.64555s/10 iters), loss = 7.58621
I0523 03:01:17.248855 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58621 (* 1 = 7.58621 loss)
I0523 03:01:17.256206 35003 sgd_solver.cpp:112] Iteration 127210, lr = 0.01
I0523 03:01:19.768535 35003 solver.cpp:239] Iteration 127220 (3.96894 iter/s, 2.51957s/10 iters), loss = 7.47493
I0523 03:01:19.768576 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47493 (* 1 = 7.47493 loss)
I0523 03:01:20.464536 35003 sgd_solver.cpp:112] Iteration 127220, lr = 0.01
I0523 03:01:22.684695 35003 solver.cpp:239] Iteration 127230 (3.42937 iter/s, 2.91599s/10 iters), loss = 7.96356
I0523 03:01:22.684749 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96356 (* 1 = 7.96356 loss)
I0523 03:01:22.698149 35003 sgd_solver.cpp:112] Iteration 127230, lr = 0.01
I0523 03:01:25.488452 35003 solver.cpp:239] Iteration 127240 (3.56686 iter/s, 2.80358s/10 iters), loss = 6.95225
I0523 03:01:25.488499 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95225 (* 1 = 6.95225 loss)
I0523 03:01:25.501834 35003 sgd_solver.cpp:112] Iteration 127240, lr = 0.01
I0523 03:01:29.023463 35003 solver.cpp:239] Iteration 127250 (2.829 iter/s, 3.53481s/10 iters), loss = 7.51831
I0523 03:01:29.023511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51831 (* 1 = 7.51831 loss)
I0523 03:01:29.028996 35003 sgd_solver.cpp:112] Iteration 127250, lr = 0.01
I0523 03:01:32.413938 35003 solver.cpp:239] Iteration 127260 (2.94962 iter/s, 3.39027s/10 iters), loss = 7.83434
I0523 03:01:32.413980 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83434 (* 1 = 7.83434 loss)
I0523 03:01:32.960747 35003 sgd_solver.cpp:112] Iteration 127260, lr = 0.01
I0523 03:01:36.591399 35003 solver.cpp:239] Iteration 127270 (2.39393 iter/s, 4.17723s/10 iters), loss = 6.36586
I0523 03:01:36.591460 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36586 (* 1 = 6.36586 loss)
I0523 03:01:37.332657 35003 sgd_solver.cpp:112] Iteration 127270, lr = 0.01
I0523 03:01:40.398267 35003 solver.cpp:239] Iteration 127280 (2.62698 iter/s, 3.80665s/10 iters), loss = 6.924
I0523 03:01:40.398303 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.924 (* 1 = 6.924 loss)
I0523 03:01:40.411795 35003 sgd_solver.cpp:112] Iteration 127280, lr = 0.01
I0523 03:01:43.321830 35003 solver.cpp:239] Iteration 127290 (3.42068 iter/s, 2.9234s/10 iters), loss = 6.8351
I0523 03:01:43.321884 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8351 (* 1 = 6.8351 loss)
I0523 03:01:44.019201 35003 sgd_solver.cpp:112] Iteration 127290, lr = 0.01
I0523 03:01:48.246278 35003 solver.cpp:239] Iteration 127300 (2.03079 iter/s, 4.92419s/10 iters), loss = 6.90769
I0523 03:01:48.246328 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90769 (* 1 = 6.90769 loss)
I0523 03:01:48.253799 35003 sgd_solver.cpp:112] Iteration 127300, lr = 0.01
I0523 03:01:51.825980 35003 solver.cpp:239] Iteration 127310 (2.79368 iter/s, 3.5795s/10 iters), loss = 6.33805
I0523 03:01:51.826022 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33805 (* 1 = 6.33805 loss)
I0523 03:01:51.839156 35003 sgd_solver.cpp:112] Iteration 127310, lr = 0.01
I0523 03:01:54.887980 35003 solver.cpp:239] Iteration 127320 (3.26602 iter/s, 3.06183s/10 iters), loss = 7.51023
I0523 03:01:54.888036 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51023 (* 1 = 7.51023 loss)
I0523 03:01:55.628952 35003 sgd_solver.cpp:112] Iteration 127320, lr = 0.01
I0523 03:01:58.309178 35003 solver.cpp:239] Iteration 127330 (2.92312 iter/s, 3.421s/10 iters), loss = 7.32326
I0523 03:01:58.309228 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32326 (* 1 = 7.32326 loss)
I0523 03:01:58.321885 35003 sgd_solver.cpp:112] Iteration 127330, lr = 0.01
I0523 03:02:00.646754 35003 solver.cpp:239] Iteration 127340 (4.27822 iter/s, 2.33742s/10 iters), loss = 6.64192
I0523 03:02:00.646806 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64192 (* 1 = 6.64192 loss)
I0523 03:02:00.667640 35003 sgd_solver.cpp:112] Iteration 127340, lr = 0.01
I0523 03:02:04.206171 35003 solver.cpp:239] Iteration 127350 (2.80961 iter/s, 3.55922s/10 iters), loss = 6.41314
I0523 03:02:04.206214 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41314 (* 1 = 6.41314 loss)
I0523 03:02:04.231230 35003 sgd_solver.cpp:112] Iteration 127350, lr = 0.01
I0523 03:02:07.774529 35003 solver.cpp:239] Iteration 127360 (2.80256 iter/s, 3.56816s/10 iters), loss = 6.77094
I0523 03:02:07.774585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77094 (* 1 = 6.77094 loss)
I0523 03:02:08.457335 35003 sgd_solver.cpp:112] Iteration 127360, lr = 0.01
I0523 03:02:11.188231 35003 solver.cpp:239] Iteration 127370 (2.92954 iter/s, 3.41351s/10 iters), loss = 6.59488
I0523 03:02:11.188272 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59488 (* 1 = 6.59488 loss)
I0523 03:02:11.199497 35003 sgd_solver.cpp:112] Iteration 127370, lr = 0.01
I0523 03:02:13.976789 35003 solver.cpp:239] Iteration 127380 (3.5863 iter/s, 2.78839s/10 iters), loss = 7.47112
I0523 03:02:13.976835 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47112 (* 1 = 7.47112 loss)
I0523 03:02:13.989704 35003 sgd_solver.cpp:112] Iteration 127380, lr = 0.01
I0523 03:02:17.108850 35003 solver.cpp:239] Iteration 127390 (3.19296 iter/s, 3.13189s/10 iters), loss = 6.71603
I0523 03:02:17.108979 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71603 (* 1 = 6.71603 loss)
I0523 03:02:17.824404 35003 sgd_solver.cpp:112] Iteration 127390, lr = 0.01
I0523 03:02:20.183604 35003 solver.cpp:239] Iteration 127400 (3.25257 iter/s, 3.07449s/10 iters), loss = 8.12541
I0523 03:02:20.183648 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.12541 (* 1 = 8.12541 loss)
I0523 03:02:20.186681 35003 sgd_solver.cpp:112] Iteration 127400, lr = 0.01
I0523 03:02:24.520539 35003 solver.cpp:239] Iteration 127410 (2.30641 iter/s, 4.33573s/10 iters), loss = 7.50602
I0523 03:02:24.520577 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50602 (* 1 = 7.50602 loss)
I0523 03:02:24.533418 35003 sgd_solver.cpp:112] Iteration 127410, lr = 0.01
I0523 03:02:26.624017 35003 solver.cpp:239] Iteration 127420 (4.75432 iter/s, 2.10335s/10 iters), loss = 7.42388
I0523 03:02:26.624058 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42388 (* 1 = 7.42388 loss)
I0523 03:02:27.293979 35003 sgd_solver.cpp:112] Iteration 127420, lr = 0.01
I0523 03:02:30.764078 35003 solver.cpp:239] Iteration 127430 (2.41555 iter/s, 4.13985s/10 iters), loss = 7.35188
I0523 03:02:30.764116 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35188 (* 1 = 7.35188 loss)
I0523 03:02:30.770591 35003 sgd_solver.cpp:112] Iteration 127430, lr = 0.01
I0523 03:02:34.010272 35003 solver.cpp:239] Iteration 127440 (3.0807 iter/s, 3.24601s/10 iters), loss = 5.02278
I0523 03:02:34.010306 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.02278 (* 1 = 5.02278 loss)
I0523 03:02:34.023350 35003 sgd_solver.cpp:112] Iteration 127440, lr = 0.01
I0523 03:02:37.694479 35003 solver.cpp:239] Iteration 127450 (2.71443 iter/s, 3.68401s/10 iters), loss = 6.60717
I0523 03:02:37.694535 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60717 (* 1 = 6.60717 loss)
I0523 03:02:38.435431 35003 sgd_solver.cpp:112] Iteration 127450, lr = 0.01
I0523 03:02:43.252431 35003 solver.cpp:239] Iteration 127460 (1.79931 iter/s, 5.55768s/10 iters), loss = 7.4452
I0523 03:02:43.252471 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4452 (* 1 = 7.4452 loss)
I0523 03:02:43.257426 35003 sgd_solver.cpp:112] Iteration 127460, lr = 0.01
I0523 03:02:46.860805 35003 solver.cpp:239] Iteration 127470 (2.77148 iter/s, 3.60818s/10 iters), loss = 7.52062
I0523 03:02:46.860848 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52062 (* 1 = 7.52062 loss)
I0523 03:02:46.874334 35003 sgd_solver.cpp:112] Iteration 127470, lr = 0.01
I0523 03:02:50.460296 35003 solver.cpp:239] Iteration 127480 (2.77833 iter/s, 3.59929s/10 iters), loss = 6.57127
I0523 03:02:50.460561 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57127 (* 1 = 6.57127 loss)
I0523 03:02:50.473984 35003 sgd_solver.cpp:112] Iteration 127480, lr = 0.01
I0523 03:02:54.266659 35003 solver.cpp:239] Iteration 127490 (2.62746 iter/s, 3.80596s/10 iters), loss = 6.27783
I0523 03:02:54.266731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27783 (* 1 = 6.27783 loss)
I0523 03:02:54.273520 35003 sgd_solver.cpp:112] Iteration 127490, lr = 0.01
I0523 03:02:59.337509 35003 solver.cpp:239] Iteration 127500 (1.97217 iter/s, 5.07056s/10 iters), loss = 8.09291
I0523 03:02:59.337563 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.09291 (* 1 = 8.09291 loss)
I0523 03:03:00.078335 35003 sgd_solver.cpp:112] Iteration 127500, lr = 0.01
I0523 03:03:03.658377 35003 solver.cpp:239] Iteration 127510 (2.31447 iter/s, 4.32064s/10 iters), loss = 5.91332
I0523 03:03:03.658427 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91332 (* 1 = 5.91332 loss)
I0523 03:03:04.292950 35003 sgd_solver.cpp:112] Iteration 127510, lr = 0.01
I0523 03:03:07.960691 35003 solver.cpp:239] Iteration 127520 (2.32446 iter/s, 4.30208s/10 iters), loss = 8.18687
I0523 03:03:07.960762 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.18687 (* 1 = 8.18687 loss)
I0523 03:03:07.962654 35003 sgd_solver.cpp:112] Iteration 127520, lr = 0.01
I0523 03:03:12.511656 35003 solver.cpp:239] Iteration 127530 (2.19747 iter/s, 4.55068s/10 iters), loss = 6.65971
I0523 03:03:12.511705 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65971 (* 1 = 6.65971 loss)
I0523 03:03:12.689863 35003 sgd_solver.cpp:112] Iteration 127530, lr = 0.01
I0523 03:03:15.391609 35003 solver.cpp:239] Iteration 127540 (3.47248 iter/s, 2.87978s/10 iters), loss = 6.85155
I0523 03:03:15.391654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85155 (* 1 = 6.85155 loss)
I0523 03:03:16.113435 35003 sgd_solver.cpp:112] Iteration 127540, lr = 0.01
I0523 03:03:20.201061 35003 solver.cpp:239] Iteration 127550 (2.07935 iter/s, 4.8092s/10 iters), loss = 6.98033
I0523 03:03:20.201129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98033 (* 1 = 6.98033 loss)
I0523 03:03:20.207412 35003 sgd_solver.cpp:112] Iteration 127550, lr = 0.01
I0523 03:03:23.357452 35003 solver.cpp:239] Iteration 127560 (3.16839 iter/s, 3.15618s/10 iters), loss = 7.36197
I0523 03:03:23.357729 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36197 (* 1 = 7.36197 loss)
I0523 03:03:24.065876 35003 sgd_solver.cpp:112] Iteration 127560, lr = 0.01
I0523 03:03:29.607393 35003 solver.cpp:239] Iteration 127570 (1.60014 iter/s, 6.24944s/10 iters), loss = 7.33744
I0523 03:03:29.607451 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33744 (* 1 = 7.33744 loss)
I0523 03:03:29.620512 35003 sgd_solver.cpp:112] Iteration 127570, lr = 0.01
I0523 03:03:33.120766 35003 solver.cpp:239] Iteration 127580 (2.84643 iter/s, 3.51317s/10 iters), loss = 7.43586
I0523 03:03:33.120810 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43586 (* 1 = 7.43586 loss)
I0523 03:03:33.134052 35003 sgd_solver.cpp:112] Iteration 127580, lr = 0.01
I0523 03:03:36.102736 35003 solver.cpp:239] Iteration 127590 (3.35368 iter/s, 2.9818s/10 iters), loss = 7.65913
I0523 03:03:36.102779 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65913 (* 1 = 7.65913 loss)
I0523 03:03:36.844317 35003 sgd_solver.cpp:112] Iteration 127590, lr = 0.01
I0523 03:03:41.027055 35003 solver.cpp:239] Iteration 127600 (2.03084 iter/s, 4.92407s/10 iters), loss = 6.12662
I0523 03:03:41.027101 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12662 (* 1 = 6.12662 loss)
I0523 03:03:41.097954 35003 sgd_solver.cpp:112] Iteration 127600, lr = 0.01
I0523 03:03:44.162488 35003 solver.cpp:239] Iteration 127610 (3.18954 iter/s, 3.13525s/10 iters), loss = 7.83932
I0523 03:03:44.162540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83932 (* 1 = 7.83932 loss)
I0523 03:03:44.172987 35003 sgd_solver.cpp:112] Iteration 127610, lr = 0.01
I0523 03:03:47.190418 35003 solver.cpp:239] Iteration 127620 (3.30278 iter/s, 3.02775s/10 iters), loss = 7.45788
I0523 03:03:47.190467 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45788 (* 1 = 7.45788 loss)
I0523 03:03:47.920872 35003 sgd_solver.cpp:112] Iteration 127620, lr = 0.01
I0523 03:03:50.171320 35003 solver.cpp:239] Iteration 127630 (3.35488 iter/s, 2.98073s/10 iters), loss = 6.42216
I0523 03:03:50.171360 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42216 (* 1 = 6.42216 loss)
I0523 03:03:50.198433 35003 sgd_solver.cpp:112] Iteration 127630, lr = 0.01
I0523 03:03:54.648231 35003 solver.cpp:239] Iteration 127640 (2.23381 iter/s, 4.47666s/10 iters), loss = 6.9129
I0523 03:03:54.648489 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9129 (* 1 = 6.9129 loss)
I0523 03:03:54.655367 35003 sgd_solver.cpp:112] Iteration 127640, lr = 0.01
I0523 03:03:58.822047 35003 solver.cpp:239] Iteration 127650 (2.39612 iter/s, 4.17341s/10 iters), loss = 6.22734
I0523 03:03:58.822093 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22734 (* 1 = 6.22734 loss)
I0523 03:03:58.834192 35003 sgd_solver.cpp:112] Iteration 127650, lr = 0.01
I0523 03:04:03.127652 35003 solver.cpp:239] Iteration 127660 (2.32267 iter/s, 4.30538s/10 iters), loss = 7.55021
I0523 03:04:03.127709 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55021 (* 1 = 7.55021 loss)
I0523 03:04:03.769088 35003 sgd_solver.cpp:112] Iteration 127660, lr = 0.01
I0523 03:04:09.027374 35003 solver.cpp:239] Iteration 127670 (1.69508 iter/s, 5.89942s/10 iters), loss = 7.02341
I0523 03:04:09.027416 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02341 (* 1 = 7.02341 loss)
I0523 03:04:09.040534 35003 sgd_solver.cpp:112] Iteration 127670, lr = 0.01
I0523 03:04:11.966315 35003 solver.cpp:239] Iteration 127680 (3.40278 iter/s, 2.93877s/10 iters), loss = 6.00438
I0523 03:04:11.966359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00438 (* 1 = 6.00438 loss)
I0523 03:04:11.979037 35003 sgd_solver.cpp:112] Iteration 127680, lr = 0.01
I0523 03:04:14.067600 35003 solver.cpp:239] Iteration 127690 (4.7593 iter/s, 2.10115s/10 iters), loss = 6.80031
I0523 03:04:14.067643 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80031 (* 1 = 6.80031 loss)
I0523 03:04:14.084628 35003 sgd_solver.cpp:112] Iteration 127690, lr = 0.01
I0523 03:04:16.869316 35003 solver.cpp:239] Iteration 127700 (3.56945 iter/s, 2.80155s/10 iters), loss = 6.62687
I0523 03:04:16.869360 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62687 (* 1 = 6.62687 loss)
I0523 03:04:16.882755 35003 sgd_solver.cpp:112] Iteration 127700, lr = 0.01
I0523 03:04:18.887501 35003 solver.cpp:239] Iteration 127710 (4.95528 iter/s, 2.01805s/10 iters), loss = 8.0519
I0523 03:04:18.887539 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0519 (* 1 = 8.0519 loss)
I0523 03:04:19.581167 35003 sgd_solver.cpp:112] Iteration 127710, lr = 0.01
I0523 03:04:22.606441 35003 solver.cpp:239] Iteration 127720 (2.68908 iter/s, 3.71875s/10 iters), loss = 7.05873
I0523 03:04:22.606479 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05873 (* 1 = 7.05873 loss)
I0523 03:04:23.338366 35003 sgd_solver.cpp:112] Iteration 127720, lr = 0.01
I0523 03:04:27.795220 35003 solver.cpp:239] Iteration 127730 (1.92733 iter/s, 5.18852s/10 iters), loss = 6.47845
I0523 03:04:27.795495 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47845 (* 1 = 6.47845 loss)
I0523 03:04:27.808229 35003 sgd_solver.cpp:112] Iteration 127730, lr = 0.01
I0523 03:04:30.524176 35003 solver.cpp:239] Iteration 127740 (3.66492 iter/s, 2.72857s/10 iters), loss = 6.83417
I0523 03:04:30.524255 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83417 (* 1 = 6.83417 loss)
I0523 03:04:31.252038 35003 sgd_solver.cpp:112] Iteration 127740, lr = 0.01
I0523 03:04:33.094007 35003 solver.cpp:239] Iteration 127750 (3.89159 iter/s, 2.56964s/10 iters), loss = 7.51874
I0523 03:04:33.094053 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51874 (* 1 = 7.51874 loss)
I0523 03:04:33.105407 35003 sgd_solver.cpp:112] Iteration 127750, lr = 0.01
I0523 03:04:36.595348 35003 solver.cpp:239] Iteration 127760 (2.85621 iter/s, 3.50114s/10 iters), loss = 7.16586
I0523 03:04:36.595391 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16586 (* 1 = 7.16586 loss)
I0523 03:04:36.600224 35003 sgd_solver.cpp:112] Iteration 127760, lr = 0.01
I0523 03:04:40.834360 35003 solver.cpp:239] Iteration 127770 (2.35916 iter/s, 4.23879s/10 iters), loss = 8.32019
I0523 03:04:40.834401 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.32019 (* 1 = 8.32019 loss)
I0523 03:04:40.847470 35003 sgd_solver.cpp:112] Iteration 127770, lr = 0.01
I0523 03:04:44.585791 35003 solver.cpp:239] Iteration 127780 (2.66579 iter/s, 3.75123s/10 iters), loss = 8.44728
I0523 03:04:44.585845 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.44728 (* 1 = 8.44728 loss)
I0523 03:04:45.117440 35003 sgd_solver.cpp:112] Iteration 127780, lr = 0.01
I0523 03:04:46.797978 35003 solver.cpp:239] Iteration 127790 (4.52072 iter/s, 2.21204s/10 iters), loss = 6.9739
I0523 03:04:46.798019 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9739 (* 1 = 6.9739 loss)
I0523 03:04:46.827211 35003 sgd_solver.cpp:112] Iteration 127790, lr = 0.01
I0523 03:04:50.353091 35003 solver.cpp:239] Iteration 127800 (2.813 iter/s, 3.55492s/10 iters), loss = 7.5244
I0523 03:04:50.353137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5244 (* 1 = 7.5244 loss)
I0523 03:04:50.361297 35003 sgd_solver.cpp:112] Iteration 127800, lr = 0.01
I0523 03:04:53.943150 35003 solver.cpp:239] Iteration 127810 (2.78562 iter/s, 3.58987s/10 iters), loss = 7.93877
I0523 03:04:53.943188 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93877 (* 1 = 7.93877 loss)
I0523 03:04:53.948277 35003 sgd_solver.cpp:112] Iteration 127810, lr = 0.01
I0523 03:04:58.337280 35003 solver.cpp:239] Iteration 127820 (2.27589 iter/s, 4.3939s/10 iters), loss = 7.00005
I0523 03:04:58.337484 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00005 (* 1 = 7.00005 loss)
I0523 03:04:58.358376 35003 sgd_solver.cpp:112] Iteration 127820, lr = 0.01
I0523 03:05:01.040983 35003 solver.cpp:239] Iteration 127830 (3.69907 iter/s, 2.70338s/10 iters), loss = 6.35126
I0523 03:05:01.041026 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35126 (* 1 = 6.35126 loss)
I0523 03:05:01.054302 35003 sgd_solver.cpp:112] Iteration 127830, lr = 0.01
I0523 03:05:03.970571 35003 solver.cpp:239] Iteration 127840 (3.41364 iter/s, 2.92942s/10 iters), loss = 6.85328
I0523 03:05:03.970613 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85328 (* 1 = 6.85328 loss)
I0523 03:05:04.679483 35003 sgd_solver.cpp:112] Iteration 127840, lr = 0.01
I0523 03:05:07.571915 35003 solver.cpp:239] Iteration 127850 (2.77689 iter/s, 3.60115s/10 iters), loss = 7.61916
I0523 03:05:07.571970 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61916 (* 1 = 7.61916 loss)
I0523 03:05:08.306624 35003 sgd_solver.cpp:112] Iteration 127850, lr = 0.01
I0523 03:05:11.333684 35003 solver.cpp:239] Iteration 127860 (2.65847 iter/s, 3.76156s/10 iters), loss = 8.18426
I0523 03:05:11.333732 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.18426 (* 1 = 8.18426 loss)
I0523 03:05:11.960027 35003 sgd_solver.cpp:112] Iteration 127860, lr = 0.01
I0523 03:05:15.032467 35003 solver.cpp:239] Iteration 127870 (2.70374 iter/s, 3.69858s/10 iters), loss = 7.0627
I0523 03:05:15.032510 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0627 (* 1 = 7.0627 loss)
I0523 03:05:15.037719 35003 sgd_solver.cpp:112] Iteration 127870, lr = 0.01
I0523 03:05:19.292222 35003 solver.cpp:239] Iteration 127880 (2.34767 iter/s, 4.25953s/10 iters), loss = 7.1611
I0523 03:05:19.292268 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1611 (* 1 = 7.1611 loss)
I0523 03:05:19.532685 35003 sgd_solver.cpp:112] Iteration 127880, lr = 0.01
I0523 03:05:21.064739 35003 solver.cpp:239] Iteration 127890 (5.64209 iter/s, 1.77239s/10 iters), loss = 6.72004
I0523 03:05:21.064781 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72004 (* 1 = 6.72004 loss)
I0523 03:05:21.761523 35003 sgd_solver.cpp:112] Iteration 127890, lr = 0.01
I0523 03:05:23.824038 35003 solver.cpp:239] Iteration 127900 (3.62432 iter/s, 2.75914s/10 iters), loss = 6.35906
I0523 03:05:23.824095 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35906 (* 1 = 6.35906 loss)
I0523 03:05:23.835216 35003 sgd_solver.cpp:112] Iteration 127900, lr = 0.01
I0523 03:05:27.056746 35003 solver.cpp:239] Iteration 127910 (3.09356 iter/s, 3.23252s/10 iters), loss = 6.31085
I0523 03:05:27.056788 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31085 (* 1 = 6.31085 loss)
I0523 03:05:27.069936 35003 sgd_solver.cpp:112] Iteration 127910, lr = 0.01
I0523 03:05:31.767789 35003 solver.cpp:239] Iteration 127920 (2.12278 iter/s, 4.71081s/10 iters), loss = 6.81072
I0523 03:05:31.768036 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81072 (* 1 = 6.81072 loss)
I0523 03:05:32.041849 35003 sgd_solver.cpp:112] Iteration 127920, lr = 0.01
I0523 03:05:34.923969 35003 solver.cpp:239] Iteration 127930 (3.16875 iter/s, 3.15582s/10 iters), loss = 7.43411
I0523 03:05:34.924028 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43411 (* 1 = 7.43411 loss)
I0523 03:05:35.639456 35003 sgd_solver.cpp:112] Iteration 127930, lr = 0.01
I0523 03:05:37.678652 35003 solver.cpp:239] Iteration 127940 (3.63042 iter/s, 2.7545s/10 iters), loss = 6.42326
I0523 03:05:37.678719 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42326 (* 1 = 6.42326 loss)
I0523 03:05:37.685451 35003 sgd_solver.cpp:112] Iteration 127940, lr = 0.01
I0523 03:05:42.159312 35003 solver.cpp:239] Iteration 127950 (2.23195 iter/s, 4.4804s/10 iters), loss = 7.33798
I0523 03:05:42.159384 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33798 (* 1 = 7.33798 loss)
I0523 03:05:42.165426 35003 sgd_solver.cpp:112] Iteration 127950, lr = 0.01
I0523 03:05:46.397092 35003 solver.cpp:239] Iteration 127960 (2.35986 iter/s, 4.23754s/10 iters), loss = 7.58372
I0523 03:05:46.397159 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58372 (* 1 = 7.58372 loss)
I0523 03:05:47.117962 35003 sgd_solver.cpp:112] Iteration 127960, lr = 0.01
I0523 03:05:51.267374 35003 solver.cpp:239] Iteration 127970 (2.05338 iter/s, 4.87003s/10 iters), loss = 7.47255
I0523 03:05:51.267411 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47255 (* 1 = 7.47255 loss)
I0523 03:05:51.279597 35003 sgd_solver.cpp:112] Iteration 127970, lr = 0.01
I0523 03:05:55.540001 35003 solver.cpp:239] Iteration 127980 (2.3406 iter/s, 4.27242s/10 iters), loss = 7.90938
I0523 03:05:55.540040 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90938 (* 1 = 7.90938 loss)
I0523 03:05:55.549655 35003 sgd_solver.cpp:112] Iteration 127980, lr = 0.01
I0523 03:05:59.589057 35003 solver.cpp:239] Iteration 127990 (2.46985 iter/s, 4.04883s/10 iters), loss = 7.88423
I0523 03:05:59.589107 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88423 (* 1 = 7.88423 loss)
I0523 03:05:59.595723 35003 sgd_solver.cpp:112] Iteration 127990, lr = 0.01
I0523 03:06:03.153455 35003 solver.cpp:239] Iteration 128000 (2.80568 iter/s, 3.5642s/10 iters), loss = 7.03165
I0523 03:06:03.153719 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03165 (* 1 = 7.03165 loss)
I0523 03:06:03.893909 35003 sgd_solver.cpp:112] Iteration 128000, lr = 0.01
I0523 03:06:08.113294 35003 solver.cpp:239] Iteration 128010 (2.01637 iter/s, 4.9594s/10 iters), loss = 6.97671
I0523 03:06:08.113350 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97671 (* 1 = 6.97671 loss)
I0523 03:06:08.121439 35003 sgd_solver.cpp:112] Iteration 128010, lr = 0.01
I0523 03:06:11.010129 35003 solver.cpp:239] Iteration 128020 (3.45226 iter/s, 2.89666s/10 iters), loss = 6.88939
I0523 03:06:11.010171 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88939 (* 1 = 6.88939 loss)
I0523 03:06:11.692965 35003 sgd_solver.cpp:112] Iteration 128020, lr = 0.01
I0523 03:06:16.626406 35003 solver.cpp:239] Iteration 128030 (1.78062 iter/s, 5.61601s/10 iters), loss = 8.27152
I0523 03:06:16.626451 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.27152 (* 1 = 8.27152 loss)
I0523 03:06:16.632719 35003 sgd_solver.cpp:112] Iteration 128030, lr = 0.01
I0523 03:06:19.542294 35003 solver.cpp:239] Iteration 128040 (3.42968 iter/s, 2.91572s/10 iters), loss = 7.12996
I0523 03:06:19.542337 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12996 (* 1 = 7.12996 loss)
I0523 03:06:20.263737 35003 sgd_solver.cpp:112] Iteration 128040, lr = 0.01
I0523 03:06:23.143652 35003 solver.cpp:239] Iteration 128050 (2.77689 iter/s, 3.60116s/10 iters), loss = 5.95075
I0523 03:06:23.143707 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95075 (* 1 = 5.95075 loss)
I0523 03:06:23.858922 35003 sgd_solver.cpp:112] Iteration 128050, lr = 0.01
I0523 03:06:25.567862 35003 solver.cpp:239] Iteration 128060 (4.12533 iter/s, 2.42405s/10 iters), loss = 7.26517
I0523 03:06:25.567906 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26517 (* 1 = 7.26517 loss)
I0523 03:06:25.575203 35003 sgd_solver.cpp:112] Iteration 128060, lr = 0.01
I0523 03:06:27.658301 35003 solver.cpp:239] Iteration 128070 (4.78401 iter/s, 2.0903s/10 iters), loss = 6.87059
I0523 03:06:27.658351 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87059 (* 1 = 6.87059 loss)
I0523 03:06:27.678975 35003 sgd_solver.cpp:112] Iteration 128070, lr = 0.01
I0523 03:06:30.570539 35003 solver.cpp:239] Iteration 128080 (3.43399 iter/s, 2.91207s/10 iters), loss = 6.77824
I0523 03:06:30.570585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77824 (* 1 = 6.77824 loss)
I0523 03:06:31.201200 35003 sgd_solver.cpp:112] Iteration 128080, lr = 0.01
I0523 03:06:34.235925 35003 solver.cpp:239] Iteration 128090 (2.72839 iter/s, 3.66517s/10 iters), loss = 6.404
I0523 03:06:34.236203 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.404 (* 1 = 6.404 loss)
I0523 03:06:34.931931 35003 sgd_solver.cpp:112] Iteration 128090, lr = 0.01
I0523 03:06:37.774919 35003 solver.cpp:239] Iteration 128100 (2.82598 iter/s, 3.53859s/10 iters), loss = 5.43962
I0523 03:06:37.774962 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.43962 (* 1 = 5.43962 loss)
I0523 03:06:37.784621 35003 sgd_solver.cpp:112] Iteration 128100, lr = 0.01
I0523 03:06:40.467170 35003 solver.cpp:239] Iteration 128110 (3.7146 iter/s, 2.69208s/10 iters), loss = 7.09965
I0523 03:06:40.467212 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09965 (* 1 = 7.09965 loss)
I0523 03:06:40.487866 35003 sgd_solver.cpp:112] Iteration 128110, lr = 0.01
I0523 03:06:45.590746 35003 solver.cpp:239] Iteration 128120 (1.95186 iter/s, 5.12333s/10 iters), loss = 7.87347
I0523 03:06:45.590787 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87347 (* 1 = 7.87347 loss)
I0523 03:06:46.299198 35003 sgd_solver.cpp:112] Iteration 128120, lr = 0.01
I0523 03:06:49.415488 35003 solver.cpp:239] Iteration 128130 (2.6147 iter/s, 3.82453s/10 iters), loss = 5.85761
I0523 03:06:49.415552 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85761 (* 1 = 5.85761 loss)
I0523 03:06:50.105677 35003 sgd_solver.cpp:112] Iteration 128130, lr = 0.01
I0523 03:06:52.305403 35003 solver.cpp:239] Iteration 128140 (3.46052 iter/s, 2.88974s/10 iters), loss = 6.83416
I0523 03:06:52.305449 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83416 (* 1 = 6.83416 loss)
I0523 03:06:52.315268 35003 sgd_solver.cpp:112] Iteration 128140, lr = 0.01
I0523 03:06:56.290385 35003 solver.cpp:239] Iteration 128150 (2.50956 iter/s, 3.98477s/10 iters), loss = 7.17164
I0523 03:06:56.290436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17164 (* 1 = 7.17164 loss)
I0523 03:06:56.295083 35003 sgd_solver.cpp:112] Iteration 128150, lr = 0.01
I0523 03:06:59.018779 35003 solver.cpp:239] Iteration 128160 (3.66538 iter/s, 2.72823s/10 iters), loss = 7.20805
I0523 03:06:59.018823 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20805 (* 1 = 7.20805 loss)
I0523 03:06:59.707407 35003 sgd_solver.cpp:112] Iteration 128160, lr = 0.01
I0523 03:07:03.095096 35003 solver.cpp:239] Iteration 128170 (2.45332 iter/s, 4.07611s/10 iters), loss = 7.26794
I0523 03:07:03.095145 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26794 (* 1 = 7.26794 loss)
I0523 03:07:03.099004 35003 sgd_solver.cpp:112] Iteration 128170, lr = 0.01
I0523 03:07:08.197248 35003 solver.cpp:239] Iteration 128180 (1.96006 iter/s, 5.10188s/10 iters), loss = 7.78891
I0523 03:07:08.197428 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78891 (* 1 = 7.78891 loss)
I0523 03:07:08.205623 35003 sgd_solver.cpp:112] Iteration 128180, lr = 0.01
I0523 03:07:13.113509 35003 solver.cpp:239] Iteration 128190 (2.03422 iter/s, 4.91588s/10 iters), loss = 6.21544
I0523 03:07:13.113560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21544 (* 1 = 6.21544 loss)
I0523 03:07:13.126437 35003 sgd_solver.cpp:112] Iteration 128190, lr = 0.01
I0523 03:07:16.076333 35003 solver.cpp:239] Iteration 128200 (3.37536 iter/s, 2.96265s/10 iters), loss = 6.42343
I0523 03:07:16.076382 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42343 (* 1 = 6.42343 loss)
I0523 03:07:16.084126 35003 sgd_solver.cpp:112] Iteration 128200, lr = 0.01
I0523 03:07:19.670342 35003 solver.cpp:239] Iteration 128210 (2.78256 iter/s, 3.59381s/10 iters), loss = 6.43402
I0523 03:07:19.670388 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43402 (* 1 = 6.43402 loss)
I0523 03:07:19.682921 35003 sgd_solver.cpp:112] Iteration 128210, lr = 0.01
I0523 03:07:22.537289 35003 solver.cpp:239] Iteration 128220 (3.48824 iter/s, 2.86677s/10 iters), loss = 7.22823
I0523 03:07:22.537333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22823 (* 1 = 7.22823 loss)
I0523 03:07:22.543854 35003 sgd_solver.cpp:112] Iteration 128220, lr = 0.01
I0523 03:07:26.152657 35003 solver.cpp:239] Iteration 128230 (2.76612 iter/s, 3.61517s/10 iters), loss = 7.91034
I0523 03:07:26.152696 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91034 (* 1 = 7.91034 loss)
I0523 03:07:26.157171 35003 sgd_solver.cpp:112] Iteration 128230, lr = 0.01
I0523 03:07:29.234174 35003 solver.cpp:239] Iteration 128240 (3.24534 iter/s, 3.08134s/10 iters), loss = 6.14989
I0523 03:07:29.234215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14989 (* 1 = 6.14989 loss)
I0523 03:07:29.252982 35003 sgd_solver.cpp:112] Iteration 128240, lr = 0.01
I0523 03:07:32.798424 35003 solver.cpp:239] Iteration 128250 (2.80579 iter/s, 3.56406s/10 iters), loss = 7.20251
I0523 03:07:32.798467 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20251 (* 1 = 7.20251 loss)
I0523 03:07:32.802109 35003 sgd_solver.cpp:112] Iteration 128250, lr = 0.01
I0523 03:07:35.562752 35003 solver.cpp:239] Iteration 128260 (3.61775 iter/s, 2.76415s/10 iters), loss = 7.23634
I0523 03:07:35.562813 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23634 (* 1 = 7.23634 loss)
I0523 03:07:36.274413 35003 sgd_solver.cpp:112] Iteration 128260, lr = 0.01
I0523 03:07:37.730959 35003 solver.cpp:239] Iteration 128270 (4.61244 iter/s, 2.16805s/10 iters), loss = 6.99021
I0523 03:07:37.731001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99021 (* 1 = 6.99021 loss)
I0523 03:07:37.737159 35003 sgd_solver.cpp:112] Iteration 128270, lr = 0.01
I0523 03:07:40.849826 35003 solver.cpp:239] Iteration 128280 (3.20647 iter/s, 3.11869s/10 iters), loss = 5.58025
I0523 03:07:40.850132 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.58025 (* 1 = 5.58025 loss)
I0523 03:07:41.590335 35003 sgd_solver.cpp:112] Iteration 128280, lr = 0.01
I0523 03:07:45.328096 35003 solver.cpp:239] Iteration 128290 (2.23323 iter/s, 4.47782s/10 iters), loss = 7.5265
I0523 03:07:45.328135 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5265 (* 1 = 7.5265 loss)
I0523 03:07:45.900600 35003 sgd_solver.cpp:112] Iteration 128290, lr = 0.01
I0523 03:07:47.936064 35003 solver.cpp:239] Iteration 128300 (3.83463 iter/s, 2.60781s/10 iters), loss = 7.03642
I0523 03:07:47.936116 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03642 (* 1 = 7.03642 loss)
I0523 03:07:48.671170 35003 sgd_solver.cpp:112] Iteration 128300, lr = 0.01
I0523 03:07:50.648552 35003 solver.cpp:239] Iteration 128310 (3.68688 iter/s, 2.71232s/10 iters), loss = 7.47711
I0523 03:07:50.648597 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47711 (* 1 = 7.47711 loss)
I0523 03:07:51.357823 35003 sgd_solver.cpp:112] Iteration 128310, lr = 0.01
I0523 03:07:53.409700 35003 solver.cpp:239] Iteration 128320 (3.62191 iter/s, 2.76098s/10 iters), loss = 6.40015
I0523 03:07:53.409759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40015 (* 1 = 6.40015 loss)
I0523 03:07:54.150388 35003 sgd_solver.cpp:112] Iteration 128320, lr = 0.01
I0523 03:07:57.476130 35003 solver.cpp:239] Iteration 128330 (2.45929 iter/s, 4.06621s/10 iters), loss = 6.24291
I0523 03:07:57.476167 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24291 (* 1 = 6.24291 loss)
I0523 03:07:57.484431 35003 sgd_solver.cpp:112] Iteration 128330, lr = 0.01
I0523 03:08:00.118839 35003 solver.cpp:239] Iteration 128340 (3.78422 iter/s, 2.64255s/10 iters), loss = 6.50508
I0523 03:08:00.118885 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50508 (* 1 = 6.50508 loss)
I0523 03:08:00.137073 35003 sgd_solver.cpp:112] Iteration 128340, lr = 0.01
I0523 03:08:06.231961 35003 solver.cpp:239] Iteration 128350 (1.6359 iter/s, 6.11283s/10 iters), loss = 6.71305
I0523 03:08:06.232012 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71305 (* 1 = 6.71305 loss)
I0523 03:08:06.244705 35003 sgd_solver.cpp:112] Iteration 128350, lr = 0.01
I0523 03:08:09.124011 35003 solver.cpp:239] Iteration 128360 (3.45796 iter/s, 2.89188s/10 iters), loss = 6.44318
I0523 03:08:09.124053 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44318 (* 1 = 6.44318 loss)
I0523 03:08:09.798511 35003 sgd_solver.cpp:112] Iteration 128360, lr = 0.01
I0523 03:08:12.802184 35003 solver.cpp:239] Iteration 128370 (2.71889 iter/s, 3.67798s/10 iters), loss = 7.14441
I0523 03:08:12.802477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14441 (* 1 = 7.14441 loss)
I0523 03:08:13.520301 35003 sgd_solver.cpp:112] Iteration 128370, lr = 0.01
I0523 03:08:18.027209 35003 solver.cpp:239] Iteration 128380 (1.91404 iter/s, 5.22454s/10 iters), loss = 6.95505
I0523 03:08:18.027271 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95505 (* 1 = 6.95505 loss)
I0523 03:08:18.749239 35003 sgd_solver.cpp:112] Iteration 128380, lr = 0.01
I0523 03:08:21.153358 35003 solver.cpp:239] Iteration 128390 (3.19902 iter/s, 3.12596s/10 iters), loss = 7.57758
I0523 03:08:21.153395 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57758 (* 1 = 7.57758 loss)
I0523 03:08:21.156368 35003 sgd_solver.cpp:112] Iteration 128390, lr = 0.01
I0523 03:08:26.204973 35003 solver.cpp:239] Iteration 128400 (1.97967 iter/s, 5.05136s/10 iters), loss = 7.96238
I0523 03:08:26.205029 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96238 (* 1 = 7.96238 loss)
I0523 03:08:26.210609 35003 sgd_solver.cpp:112] Iteration 128400, lr = 0.01
I0523 03:08:29.869472 35003 solver.cpp:239] Iteration 128410 (2.72905 iter/s, 3.66428s/10 iters), loss = 7.43265
I0523 03:08:29.869586 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43265 (* 1 = 7.43265 loss)
I0523 03:08:29.876490 35003 sgd_solver.cpp:112] Iteration 128410, lr = 0.01
I0523 03:08:35.007760 35003 solver.cpp:239] Iteration 128420 (1.9463 iter/s, 5.13796s/10 iters), loss = 6.09443
I0523 03:08:35.007800 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09443 (* 1 = 6.09443 loss)
I0523 03:08:35.020568 35003 sgd_solver.cpp:112] Iteration 128420, lr = 0.01
I0523 03:08:37.744319 35003 solver.cpp:239] Iteration 128430 (3.65444 iter/s, 2.7364s/10 iters), loss = 7.60244
I0523 03:08:37.744362 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60244 (* 1 = 7.60244 loss)
I0523 03:08:37.749605 35003 sgd_solver.cpp:112] Iteration 128430, lr = 0.01
I0523 03:08:40.496042 35003 solver.cpp:239] Iteration 128440 (3.6343 iter/s, 2.75156s/10 iters), loss = 6.82346
I0523 03:08:40.496083 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82346 (* 1 = 6.82346 loss)
I0523 03:08:40.505197 35003 sgd_solver.cpp:112] Iteration 128440, lr = 0.01
I0523 03:08:43.327404 35003 solver.cpp:239] Iteration 128450 (3.53207 iter/s, 2.8312s/10 iters), loss = 4.96137
I0523 03:08:43.327569 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.96137 (* 1 = 4.96137 loss)
I0523 03:08:43.345723 35003 sgd_solver.cpp:112] Iteration 128450, lr = 0.01
I0523 03:08:45.454639 35003 solver.cpp:239] Iteration 128460 (4.70146 iter/s, 2.127s/10 iters), loss = 7.36205
I0523 03:08:45.454685 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36205 (* 1 = 7.36205 loss)
I0523 03:08:45.805227 35003 sgd_solver.cpp:112] Iteration 128460, lr = 0.01
I0523 03:08:47.818270 35003 solver.cpp:239] Iteration 128470 (4.2311 iter/s, 2.36345s/10 iters), loss = 7.44999
I0523 03:08:47.818341 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44999 (* 1 = 7.44999 loss)
I0523 03:08:47.828091 35003 sgd_solver.cpp:112] Iteration 128470, lr = 0.01
I0523 03:08:51.291880 35003 solver.cpp:239] Iteration 128480 (2.87903 iter/s, 3.47339s/10 iters), loss = 6.44499
I0523 03:08:51.291939 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44499 (* 1 = 6.44499 loss)
I0523 03:08:52.032912 35003 sgd_solver.cpp:112] Iteration 128480, lr = 0.01
I0523 03:08:55.659116 35003 solver.cpp:239] Iteration 128490 (2.2899 iter/s, 4.367s/10 iters), loss = 6.44943
I0523 03:08:55.659178 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44943 (* 1 = 6.44943 loss)
I0523 03:08:55.672348 35003 sgd_solver.cpp:112] Iteration 128490, lr = 0.01
I0523 03:08:59.362483 35003 solver.cpp:239] Iteration 128500 (2.7004 iter/s, 3.70315s/10 iters), loss = 7.19242
I0523 03:08:59.362545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19242 (* 1 = 7.19242 loss)
I0523 03:08:59.370223 35003 sgd_solver.cpp:112] Iteration 128500, lr = 0.01
I0523 03:09:02.922386 35003 solver.cpp:239] Iteration 128510 (2.80923 iter/s, 3.5597s/10 iters), loss = 6.37192
I0523 03:09:02.922428 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37192 (* 1 = 6.37192 loss)
I0523 03:09:02.935533 35003 sgd_solver.cpp:112] Iteration 128510, lr = 0.01
I0523 03:09:05.714010 35003 solver.cpp:239] Iteration 128520 (3.58235 iter/s, 2.79146s/10 iters), loss = 7.68519
I0523 03:09:05.714051 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68519 (* 1 = 7.68519 loss)
I0523 03:09:05.727165 35003 sgd_solver.cpp:112] Iteration 128520, lr = 0.01
I0523 03:09:10.176796 35003 solver.cpp:239] Iteration 128530 (2.24087 iter/s, 4.46256s/10 iters), loss = 7.3515
I0523 03:09:10.176863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3515 (* 1 = 7.3515 loss)
I0523 03:09:10.909320 35003 sgd_solver.cpp:112] Iteration 128530, lr = 0.01
I0523 03:09:14.492900 35003 solver.cpp:239] Iteration 128540 (2.31704 iter/s, 4.31585s/10 iters), loss = 5.66701
I0523 03:09:14.493198 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.66701 (* 1 = 5.66701 loss)
I0523 03:09:15.014474 35003 sgd_solver.cpp:112] Iteration 128540, lr = 0.01
I0523 03:09:16.711799 35003 solver.cpp:239] Iteration 128550 (4.50748 iter/s, 2.21853s/10 iters), loss = 6.75084
I0523 03:09:16.711849 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75084 (* 1 = 6.75084 loss)
I0523 03:09:17.006044 35003 sgd_solver.cpp:112] Iteration 128550, lr = 0.01
I0523 03:09:19.926298 35003 solver.cpp:239] Iteration 128560 (3.11109 iter/s, 3.21431s/10 iters), loss = 7.45777
I0523 03:09:19.926359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45777 (* 1 = 7.45777 loss)
I0523 03:09:20.660781 35003 sgd_solver.cpp:112] Iteration 128560, lr = 0.01
I0523 03:09:24.260323 35003 solver.cpp:239] Iteration 128570 (2.30745 iter/s, 4.33379s/10 iters), loss = 6.92807
I0523 03:09:24.260368 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92807 (* 1 = 6.92807 loss)
I0523 03:09:24.269275 35003 sgd_solver.cpp:112] Iteration 128570, lr = 0.01
I0523 03:09:26.343477 35003 solver.cpp:239] Iteration 128580 (4.80073 iter/s, 2.08302s/10 iters), loss = 6.39169
I0523 03:09:26.343523 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39169 (* 1 = 6.39169 loss)
I0523 03:09:26.365036 35003 sgd_solver.cpp:112] Iteration 128580, lr = 0.01
I0523 03:09:28.362124 35003 solver.cpp:239] Iteration 128590 (4.95415 iter/s, 2.01851s/10 iters), loss = 5.90773
I0523 03:09:28.362169 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.90773 (* 1 = 5.90773 loss)
I0523 03:09:28.365113 35003 sgd_solver.cpp:112] Iteration 128590, lr = 0.01
I0523 03:09:31.702941 35003 solver.cpp:239] Iteration 128600 (2.99345 iter/s, 3.34062s/10 iters), loss = 6.00671
I0523 03:09:31.702996 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00671 (* 1 = 6.00671 loss)
I0523 03:09:31.715662 35003 sgd_solver.cpp:112] Iteration 128600, lr = 0.01
I0523 03:09:36.021406 35003 solver.cpp:239] Iteration 128610 (2.31576 iter/s, 4.31823s/10 iters), loss = 7.25667
I0523 03:09:36.021451 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25667 (* 1 = 7.25667 loss)
I0523 03:09:36.752423 35003 sgd_solver.cpp:112] Iteration 128610, lr = 0.01
I0523 03:09:40.116060 35003 solver.cpp:239] Iteration 128620 (2.44234 iter/s, 4.09443s/10 iters), loss = 7.17069
I0523 03:09:40.116106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17069 (* 1 = 7.17069 loss)
I0523 03:09:40.123092 35003 sgd_solver.cpp:112] Iteration 128620, lr = 0.01
I0523 03:09:43.491700 35003 solver.cpp:239] Iteration 128630 (2.96257 iter/s, 3.37545s/10 iters), loss = 7.60058
I0523 03:09:43.491755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60058 (* 1 = 7.60058 loss)
I0523 03:09:43.500414 35003 sgd_solver.cpp:112] Iteration 128630, lr = 0.01
I0523 03:09:46.976776 35003 solver.cpp:239] Iteration 128640 (2.86955 iter/s, 3.48487s/10 iters), loss = 5.8791
I0523 03:09:46.977051 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8791 (* 1 = 5.8791 loss)
I0523 03:09:46.982059 35003 sgd_solver.cpp:112] Iteration 128640, lr = 0.01
I0523 03:09:51.306072 35003 solver.cpp:239] Iteration 128650 (2.31008 iter/s, 4.32886s/10 iters), loss = 6.54164
I0523 03:09:51.306119 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54164 (* 1 = 6.54164 loss)
I0523 03:09:51.315117 35003 sgd_solver.cpp:112] Iteration 128650, lr = 0.01
I0523 03:09:54.992949 35003 solver.cpp:239] Iteration 128660 (2.71247 iter/s, 3.68668s/10 iters), loss = 7.40413
I0523 03:09:54.992993 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40413 (* 1 = 7.40413 loss)
I0523 03:09:55.006136 35003 sgd_solver.cpp:112] Iteration 128660, lr = 0.01
I0523 03:09:59.322777 35003 solver.cpp:239] Iteration 128670 (2.30968 iter/s, 4.32961s/10 iters), loss = 6.25836
I0523 03:09:59.322818 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25836 (* 1 = 6.25836 loss)
I0523 03:09:59.336810 35003 sgd_solver.cpp:112] Iteration 128670, lr = 0.01
I0523 03:10:02.905236 35003 solver.cpp:239] Iteration 128680 (2.79153 iter/s, 3.58227s/10 iters), loss = 6.16105
I0523 03:10:02.905282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16105 (* 1 = 6.16105 loss)
I0523 03:10:02.917026 35003 sgd_solver.cpp:112] Iteration 128680, lr = 0.01
I0523 03:10:06.570963 35003 solver.cpp:239] Iteration 128690 (2.72812 iter/s, 3.66553s/10 iters), loss = 7.12604
I0523 03:10:06.571007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12604 (* 1 = 7.12604 loss)
I0523 03:10:06.571863 35003 sgd_solver.cpp:112] Iteration 128690, lr = 0.01
I0523 03:10:08.560891 35003 solver.cpp:239] Iteration 128700 (5.02565 iter/s, 1.98979s/10 iters), loss = 7.46403
I0523 03:10:08.560942 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46403 (* 1 = 7.46403 loss)
I0523 03:10:08.660249 35003 sgd_solver.cpp:112] Iteration 128700, lr = 0.01
I0523 03:10:10.600347 35003 solver.cpp:239] Iteration 128710 (4.90362 iter/s, 2.03931s/10 iters), loss = 7.28107
I0523 03:10:10.600396 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28107 (* 1 = 7.28107 loss)
I0523 03:10:10.626380 35003 sgd_solver.cpp:112] Iteration 128710, lr = 0.01
I0523 03:10:14.233955 35003 solver.cpp:239] Iteration 128720 (2.75223 iter/s, 3.63341s/10 iters), loss = 8.44937
I0523 03:10:14.234000 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.44937 (* 1 = 8.44937 loss)
I0523 03:10:14.246855 35003 sgd_solver.cpp:112] Iteration 128720, lr = 0.01
I0523 03:10:17.388442 35003 solver.cpp:239] Iteration 128730 (3.17027 iter/s, 3.1543s/10 iters), loss = 6.70851
I0523 03:10:17.388664 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70851 (* 1 = 6.70851 loss)
I0523 03:10:18.097517 35003 sgd_solver.cpp:112] Iteration 128730, lr = 0.01
I0523 03:10:22.490496 35003 solver.cpp:239] Iteration 128740 (1.96015 iter/s, 5.10164s/10 iters), loss = 6.34654
I0523 03:10:22.490566 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34654 (* 1 = 6.34654 loss)
I0523 03:10:23.199510 35003 sgd_solver.cpp:112] Iteration 128740, lr = 0.01
I0523 03:10:25.281313 35003 solver.cpp:239] Iteration 128750 (3.58342 iter/s, 2.79063s/10 iters), loss = 7.63249
I0523 03:10:25.281355 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63249 (* 1 = 7.63249 loss)
I0523 03:10:25.996820 35003 sgd_solver.cpp:112] Iteration 128750, lr = 0.01
I0523 03:10:29.521680 35003 solver.cpp:239] Iteration 128760 (2.35841 iter/s, 4.24015s/10 iters), loss = 6.10569
I0523 03:10:29.521723 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10569 (* 1 = 6.10569 loss)
I0523 03:10:29.531566 35003 sgd_solver.cpp:112] Iteration 128760, lr = 0.01
I0523 03:10:34.065143 35003 solver.cpp:239] Iteration 128770 (2.20108 iter/s, 4.54323s/10 iters), loss = 7.7296
I0523 03:10:34.065191 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7296 (* 1 = 7.7296 loss)
I0523 03:10:34.078702 35003 sgd_solver.cpp:112] Iteration 128770, lr = 0.01
I0523 03:10:38.539033 35003 solver.cpp:239] Iteration 128780 (2.23531 iter/s, 4.47366s/10 iters), loss = 6.50771
I0523 03:10:38.539078 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50771 (* 1 = 6.50771 loss)
I0523 03:10:39.273175 35003 sgd_solver.cpp:112] Iteration 128780, lr = 0.01
I0523 03:10:42.091611 35003 solver.cpp:239] Iteration 128790 (2.81501 iter/s, 3.55238s/10 iters), loss = 7.22365
I0523 03:10:42.091660 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22365 (* 1 = 7.22365 loss)
I0523 03:10:42.103940 35003 sgd_solver.cpp:112] Iteration 128790, lr = 0.01
I0523 03:10:43.979641 35003 solver.cpp:239] Iteration 128800 (5.29689 iter/s, 1.8879s/10 iters), loss = 7.88732
I0523 03:10:43.979677 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88732 (* 1 = 7.88732 loss)
I0523 03:10:43.997390 35003 sgd_solver.cpp:112] Iteration 128800, lr = 0.01
I0523 03:10:46.927255 35003 solver.cpp:239] Iteration 128810 (3.39784 iter/s, 2.94305s/10 iters), loss = 7.11254
I0523 03:10:46.927304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11254 (* 1 = 7.11254 loss)
I0523 03:10:47.662576 35003 sgd_solver.cpp:112] Iteration 128810, lr = 0.01
I0523 03:10:51.331522 35003 solver.cpp:239] Iteration 128820 (2.27065 iter/s, 4.40402s/10 iters), loss = 7.01771
I0523 03:10:51.331575 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01771 (* 1 = 7.01771 loss)
I0523 03:10:51.334255 35003 sgd_solver.cpp:112] Iteration 128820, lr = 0.01
I0523 03:10:54.940378 35003 solver.cpp:239] Iteration 128830 (2.77116 iter/s, 3.6086s/10 iters), loss = 7.28034
I0523 03:10:54.940438 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28034 (* 1 = 7.28034 loss)
I0523 03:10:54.961185 35003 sgd_solver.cpp:112] Iteration 128830, lr = 0.01
I0523 03:10:57.013190 35003 solver.cpp:239] Iteration 128840 (4.82473 iter/s, 2.07266s/10 iters), loss = 7.8547
I0523 03:10:57.013231 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8547 (* 1 = 7.8547 loss)
I0523 03:10:57.017213 35003 sgd_solver.cpp:112] Iteration 128840, lr = 0.01
I0523 03:11:00.544886 35003 solver.cpp:239] Iteration 128850 (2.83166 iter/s, 3.5315s/10 iters), loss = 6.90767
I0523 03:11:00.544926 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90767 (* 1 = 6.90767 loss)
I0523 03:11:00.558238 35003 sgd_solver.cpp:112] Iteration 128850, lr = 0.01
I0523 03:11:03.302482 35003 solver.cpp:239] Iteration 128860 (3.62655 iter/s, 2.75744s/10 iters), loss = 6.92705
I0523 03:11:03.302526 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92705 (* 1 = 6.92705 loss)
I0523 03:11:03.307967 35003 sgd_solver.cpp:112] Iteration 128860, lr = 0.01
I0523 03:11:06.851876 35003 solver.cpp:239] Iteration 128870 (2.81754 iter/s, 3.5492s/10 iters), loss = 8.57506
I0523 03:11:06.851927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.57506 (* 1 = 8.57506 loss)
I0523 03:11:06.878028 35003 sgd_solver.cpp:112] Iteration 128870, lr = 0.01
I0523 03:11:11.816803 35003 solver.cpp:239] Iteration 128880 (2.01424 iter/s, 4.96466s/10 iters), loss = 6.44449
I0523 03:11:11.816849 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44449 (* 1 = 6.44449 loss)
I0523 03:11:12.532393 35003 sgd_solver.cpp:112] Iteration 128880, lr = 0.01
I0523 03:11:15.388851 35003 solver.cpp:239] Iteration 128890 (2.79969 iter/s, 3.57182s/10 iters), loss = 6.41558
I0523 03:11:15.388898 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41558 (* 1 = 6.41558 loss)
I0523 03:11:15.944583 35003 sgd_solver.cpp:112] Iteration 128890, lr = 0.01
I0523 03:11:19.509065 35003 solver.cpp:239] Iteration 128900 (2.42719 iter/s, 4.12s/10 iters), loss = 7.72534
I0523 03:11:19.509369 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72534 (* 1 = 7.72534 loss)
I0523 03:11:20.223745 35003 sgd_solver.cpp:112] Iteration 128900, lr = 0.01
I0523 03:11:23.714081 35003 solver.cpp:239] Iteration 128910 (2.37837 iter/s, 4.20457s/10 iters), loss = 8.35862
I0523 03:11:23.714118 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.35862 (* 1 = 8.35862 loss)
I0523 03:11:23.719489 35003 sgd_solver.cpp:112] Iteration 128910, lr = 0.01
I0523 03:11:27.350379 35003 solver.cpp:239] Iteration 128920 (2.7502 iter/s, 3.6361s/10 iters), loss = 6.90483
I0523 03:11:27.350422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90483 (* 1 = 6.90483 loss)
I0523 03:11:27.364373 35003 sgd_solver.cpp:112] Iteration 128920, lr = 0.01
I0523 03:11:32.195206 35003 solver.cpp:239] Iteration 128930 (2.06416 iter/s, 4.84458s/10 iters), loss = 5.91964
I0523 03:11:32.195255 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91964 (* 1 = 5.91964 loss)
I0523 03:11:32.871065 35003 sgd_solver.cpp:112] Iteration 128930, lr = 0.01
I0523 03:11:37.197652 35003 solver.cpp:239] Iteration 128940 (1.99912 iter/s, 5.0022s/10 iters), loss = 6.28382
I0523 03:11:37.197715 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28382 (* 1 = 6.28382 loss)
I0523 03:11:37.210403 35003 sgd_solver.cpp:112] Iteration 128940, lr = 0.01
I0523 03:11:41.234419 35003 solver.cpp:239] Iteration 128950 (2.47738 iter/s, 4.03653s/10 iters), loss = 7.43677
I0523 03:11:41.234464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43677 (* 1 = 7.43677 loss)
I0523 03:11:41.242522 35003 sgd_solver.cpp:112] Iteration 128950, lr = 0.01
I0523 03:11:45.689229 35003 solver.cpp:239] Iteration 128960 (2.24488 iter/s, 4.45458s/10 iters), loss = 6.54339
I0523 03:11:45.689273 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54339 (* 1 = 6.54339 loss)
I0523 03:11:46.410315 35003 sgd_solver.cpp:112] Iteration 128960, lr = 0.01
I0523 03:11:48.630939 35003 solver.cpp:239] Iteration 128970 (3.39958 iter/s, 2.94154s/10 iters), loss = 7.15128
I0523 03:11:48.630998 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15128 (* 1 = 7.15128 loss)
I0523 03:11:48.643739 35003 sgd_solver.cpp:112] Iteration 128970, lr = 0.01
I0523 03:11:52.134773 35003 solver.cpp:239] Iteration 128980 (2.85419 iter/s, 3.50362s/10 iters), loss = 7.44099
I0523 03:11:52.134905 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44099 (* 1 = 7.44099 loss)
I0523 03:11:52.743422 35003 sgd_solver.cpp:112] Iteration 128980, lr = 0.01
I0523 03:11:55.599474 35003 solver.cpp:239] Iteration 128990 (2.88648 iter/s, 3.46443s/10 iters), loss = 7.25207
I0523 03:11:55.599509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25207 (* 1 = 7.25207 loss)
I0523 03:11:55.613045 35003 sgd_solver.cpp:112] Iteration 128990, lr = 0.01
I0523 03:11:59.752923 35003 solver.cpp:239] Iteration 129000 (2.40776 iter/s, 4.15324s/10 iters), loss = 7.65997
I0523 03:11:59.752979 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65997 (* 1 = 7.65997 loss)
I0523 03:12:00.308825 35003 sgd_solver.cpp:112] Iteration 129000, lr = 0.01
I0523 03:12:01.790259 35003 solver.cpp:239] Iteration 129010 (4.90872 iter/s, 2.03719s/10 iters), loss = 7.43175
I0523 03:12:01.790316 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43175 (* 1 = 7.43175 loss)
I0523 03:12:01.846019 35003 sgd_solver.cpp:112] Iteration 129010, lr = 0.01
I0523 03:12:05.442988 35003 solver.cpp:239] Iteration 129020 (2.73784 iter/s, 3.65252s/10 iters), loss = 7.70957
I0523 03:12:05.443027 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70957 (* 1 = 7.70957 loss)
I0523 03:12:05.461805 35003 sgd_solver.cpp:112] Iteration 129020, lr = 0.01
I0523 03:12:08.215409 35003 solver.cpp:239] Iteration 129030 (3.60717 iter/s, 2.77226s/10 iters), loss = 6.59536
I0523 03:12:08.215461 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59536 (* 1 = 6.59536 loss)
I0523 03:12:08.247395 35003 sgd_solver.cpp:112] Iteration 129030, lr = 0.01
I0523 03:12:10.927151 35003 solver.cpp:239] Iteration 129040 (3.6879 iter/s, 2.71157s/10 iters), loss = 7.12078
I0523 03:12:10.927199 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12078 (* 1 = 7.12078 loss)
I0523 03:12:11.628381 35003 sgd_solver.cpp:112] Iteration 129040, lr = 0.01
I0523 03:12:15.880903 35003 solver.cpp:239] Iteration 129050 (2.01877 iter/s, 4.9535s/10 iters), loss = 6.41343
I0523 03:12:15.880950 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41343 (* 1 = 6.41343 loss)
I0523 03:12:15.881734 35003 sgd_solver.cpp:112] Iteration 129050, lr = 0.01
I0523 03:12:20.389814 35003 solver.cpp:239] Iteration 129060 (2.21794 iter/s, 4.50868s/10 iters), loss = 7.95624
I0523 03:12:20.389855 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95624 (* 1 = 7.95624 loss)
I0523 03:12:20.415918 35003 sgd_solver.cpp:112] Iteration 129060, lr = 0.01
I0523 03:12:22.858681 35003 solver.cpp:239] Iteration 129070 (4.05069 iter/s, 2.46872s/10 iters), loss = 6.66102
I0523 03:12:22.858971 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66102 (* 1 = 6.66102 loss)
I0523 03:12:22.868199 35003 sgd_solver.cpp:112] Iteration 129070, lr = 0.01
I0523 03:12:25.785711 35003 solver.cpp:239] Iteration 129080 (3.41689 iter/s, 2.92664s/10 iters), loss = 8.31826
I0523 03:12:25.785761 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.31826 (* 1 = 8.31826 loss)
I0523 03:12:25.798774 35003 sgd_solver.cpp:112] Iteration 129080, lr = 0.01
I0523 03:12:27.865506 35003 solver.cpp:239] Iteration 129090 (4.80849 iter/s, 2.07965s/10 iters), loss = 7.96282
I0523 03:12:27.865548 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96282 (* 1 = 7.96282 loss)
I0523 03:12:27.875126 35003 sgd_solver.cpp:112] Iteration 129090, lr = 0.01
I0523 03:12:29.967458 35003 solver.cpp:239] Iteration 129100 (4.75779 iter/s, 2.10182s/10 iters), loss = 6.98071
I0523 03:12:29.967506 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98071 (* 1 = 6.98071 loss)
I0523 03:12:29.981416 35003 sgd_solver.cpp:112] Iteration 129100, lr = 0.01
I0523 03:12:33.550074 35003 solver.cpp:239] Iteration 129110 (2.79141 iter/s, 3.58242s/10 iters), loss = 6.78515
I0523 03:12:33.550113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78515 (* 1 = 6.78515 loss)
I0523 03:12:33.562981 35003 sgd_solver.cpp:112] Iteration 129110, lr = 0.01
I0523 03:12:36.711455 35003 solver.cpp:239] Iteration 129120 (3.16335 iter/s, 3.1612s/10 iters), loss = 7.75161
I0523 03:12:36.711498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75161 (* 1 = 7.75161 loss)
I0523 03:12:36.714624 35003 sgd_solver.cpp:112] Iteration 129120, lr = 0.01
I0523 03:12:38.782547 35003 solver.cpp:239] Iteration 129130 (4.8287 iter/s, 2.07095s/10 iters), loss = 7.38732
I0523 03:12:38.782604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38732 (* 1 = 7.38732 loss)
I0523 03:12:39.479915 35003 sgd_solver.cpp:112] Iteration 129130, lr = 0.01
I0523 03:12:42.878521 35003 solver.cpp:239] Iteration 129140 (2.44156 iter/s, 4.09575s/10 iters), loss = 8.07093
I0523 03:12:42.878568 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.07093 (* 1 = 8.07093 loss)
I0523 03:12:42.887409 35003 sgd_solver.cpp:112] Iteration 129140, lr = 0.01
I0523 03:12:46.390844 35003 solver.cpp:239] Iteration 129150 (2.84728 iter/s, 3.51213s/10 iters), loss = 6.71928
I0523 03:12:46.390887 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71928 (* 1 = 6.71928 loss)
I0523 03:12:46.400836 35003 sgd_solver.cpp:112] Iteration 129150, lr = 0.01
I0523 03:12:48.940183 35003 solver.cpp:239] Iteration 129160 (3.92282 iter/s, 2.54919s/10 iters), loss = 6.72102
I0523 03:12:48.940224 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72102 (* 1 = 6.72102 loss)
I0523 03:12:48.958482 35003 sgd_solver.cpp:112] Iteration 129160, lr = 0.01
I0523 03:12:53.006018 35003 solver.cpp:239] Iteration 129170 (2.45965 iter/s, 4.06563s/10 iters), loss = 7.21871
I0523 03:12:53.006283 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21871 (* 1 = 7.21871 loss)
I0523 03:12:53.708621 35003 sgd_solver.cpp:112] Iteration 129170, lr = 0.01
I0523 03:12:56.660187 35003 solver.cpp:239] Iteration 129180 (2.73689 iter/s, 3.65378s/10 iters), loss = 5.78375
I0523 03:12:56.660233 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.78375 (* 1 = 5.78375 loss)
I0523 03:12:57.390928 35003 sgd_solver.cpp:112] Iteration 129180, lr = 0.01
I0523 03:13:01.281787 35003 solver.cpp:239] Iteration 129190 (2.16386 iter/s, 4.62136s/10 iters), loss = 7.54471
I0523 03:13:01.281832 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54471 (* 1 = 7.54471 loss)
I0523 03:13:01.990062 35003 sgd_solver.cpp:112] Iteration 129190, lr = 0.01
I0523 03:13:05.600522 35003 solver.cpp:239] Iteration 129200 (2.31562 iter/s, 4.3185s/10 iters), loss = 6.73917
I0523 03:13:05.600575 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73917 (* 1 = 6.73917 loss)
I0523 03:13:05.637122 35003 sgd_solver.cpp:112] Iteration 129200, lr = 0.01
I0523 03:13:08.450976 35003 solver.cpp:239] Iteration 129210 (3.50843 iter/s, 2.85028s/10 iters), loss = 6.39805
I0523 03:13:08.451030 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39805 (* 1 = 6.39805 loss)
I0523 03:13:09.185648 35003 sgd_solver.cpp:112] Iteration 129210, lr = 0.01
I0523 03:13:12.037681 35003 solver.cpp:239] Iteration 129220 (2.78824 iter/s, 3.58649s/10 iters), loss = 6.85471
I0523 03:13:12.037748 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85471 (* 1 = 6.85471 loss)
I0523 03:13:12.052943 35003 sgd_solver.cpp:112] Iteration 129220, lr = 0.01
I0523 03:13:14.613428 35003 solver.cpp:239] Iteration 129230 (3.88262 iter/s, 2.57558s/10 iters), loss = 7.5786
I0523 03:13:14.613469 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5786 (* 1 = 7.5786 loss)
I0523 03:13:14.620445 35003 sgd_solver.cpp:112] Iteration 129230, lr = 0.01
I0523 03:13:19.512079 35003 solver.cpp:239] Iteration 129240 (2.04149 iter/s, 4.89839s/10 iters), loss = 7.63492
I0523 03:13:19.512137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63492 (* 1 = 7.63492 loss)
I0523 03:13:19.642464 35003 sgd_solver.cpp:112] Iteration 129240, lr = 0.01
I0523 03:13:23.301919 35003 solver.cpp:239] Iteration 129250 (2.63878 iter/s, 3.78963s/10 iters), loss = 6.65058
I0523 03:13:23.302037 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65058 (* 1 = 6.65058 loss)
I0523 03:13:23.421620 35003 sgd_solver.cpp:112] Iteration 129250, lr = 0.01
I0523 03:13:27.664706 35003 solver.cpp:239] Iteration 129260 (2.29227 iter/s, 4.36249s/10 iters), loss = 8.21718
I0523 03:13:27.664757 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.21718 (* 1 = 8.21718 loss)
I0523 03:13:28.373345 35003 sgd_solver.cpp:112] Iteration 129260, lr = 0.01
I0523 03:13:31.226770 35003 solver.cpp:239] Iteration 129270 (2.80752 iter/s, 3.56187s/10 iters), loss = 6.95807
I0523 03:13:31.226817 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95807 (* 1 = 6.95807 loss)
I0523 03:13:31.967367 35003 sgd_solver.cpp:112] Iteration 129270, lr = 0.01
I0523 03:13:34.834594 35003 solver.cpp:239] Iteration 129280 (2.77191 iter/s, 3.60762s/10 iters), loss = 6.3867
I0523 03:13:34.834653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3867 (* 1 = 6.3867 loss)
I0523 03:13:34.837492 35003 sgd_solver.cpp:112] Iteration 129280, lr = 0.01
I0523 03:13:38.378880 35003 solver.cpp:239] Iteration 129290 (2.82161 iter/s, 3.54407s/10 iters), loss = 7.63761
I0523 03:13:38.378952 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63761 (* 1 = 7.63761 loss)
I0523 03:13:38.390226 35003 sgd_solver.cpp:112] Iteration 129290, lr = 0.01
I0523 03:13:40.559989 35003 solver.cpp:239] Iteration 129300 (4.58518 iter/s, 2.18094s/10 iters), loss = 7.43044
I0523 03:13:40.560047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43044 (* 1 = 7.43044 loss)
I0523 03:13:41.190297 35003 sgd_solver.cpp:112] Iteration 129300, lr = 0.01
I0523 03:13:44.193310 35003 solver.cpp:239] Iteration 129310 (2.75246 iter/s, 3.63312s/10 iters), loss = 7.44142
I0523 03:13:44.193354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44142 (* 1 = 7.44142 loss)
I0523 03:13:44.200783 35003 sgd_solver.cpp:112] Iteration 129310, lr = 0.01
I0523 03:13:48.520131 35003 solver.cpp:239] Iteration 129320 (2.31129 iter/s, 4.32659s/10 iters), loss = 6.06162
I0523 03:13:48.520179 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06162 (* 1 = 6.06162 loss)
I0523 03:13:48.530979 35003 sgd_solver.cpp:112] Iteration 129320, lr = 0.01
I0523 03:13:52.718509 35003 solver.cpp:239] Iteration 129330 (2.382 iter/s, 4.19816s/10 iters), loss = 6.348
I0523 03:13:52.718550 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.348 (* 1 = 6.348 loss)
I0523 03:13:52.722596 35003 sgd_solver.cpp:112] Iteration 129330, lr = 0.01
I0523 03:13:56.999560 35003 solver.cpp:239] Iteration 129340 (2.336 iter/s, 4.28083s/10 iters), loss = 6.23596
I0523 03:13:56.999807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23596 (* 1 = 6.23596 loss)
I0523 03:13:57.016196 35003 sgd_solver.cpp:112] Iteration 129340, lr = 0.01
I0523 03:14:00.627627 35003 solver.cpp:239] Iteration 129350 (2.75658 iter/s, 3.62769s/10 iters), loss = 5.88249
I0523 03:14:00.627678 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.88249 (* 1 = 5.88249 loss)
I0523 03:14:00.636937 35003 sgd_solver.cpp:112] Iteration 129350, lr = 0.01
I0523 03:14:04.949041 35003 solver.cpp:239] Iteration 129360 (2.31418 iter/s, 4.32119s/10 iters), loss = 6.62629
I0523 03:14:04.949080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62629 (* 1 = 6.62629 loss)
I0523 03:14:04.956970 35003 sgd_solver.cpp:112] Iteration 129360, lr = 0.01
I0523 03:14:07.167886 35003 solver.cpp:239] Iteration 129370 (4.50713 iter/s, 2.21871s/10 iters), loss = 7.08723
I0523 03:14:07.167930 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08723 (* 1 = 7.08723 loss)
I0523 03:14:07.902307 35003 sgd_solver.cpp:112] Iteration 129370, lr = 0.01
I0523 03:14:12.236394 35003 solver.cpp:239] Iteration 129380 (1.97307 iter/s, 5.06825s/10 iters), loss = 5.39387
I0523 03:14:12.236438 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.39387 (* 1 = 5.39387 loss)
I0523 03:14:12.249519 35003 sgd_solver.cpp:112] Iteration 129380, lr = 0.01
I0523 03:14:15.894315 35003 solver.cpp:239] Iteration 129390 (2.73394 iter/s, 3.65772s/10 iters), loss = 7.09643
I0523 03:14:15.894352 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09643 (* 1 = 7.09643 loss)
I0523 03:14:15.898206 35003 sgd_solver.cpp:112] Iteration 129390, lr = 0.01
I0523 03:14:18.030289 35003 solver.cpp:239] Iteration 129400 (4.68199 iter/s, 2.13584s/10 iters), loss = 7.26176
I0523 03:14:18.030338 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26176 (* 1 = 7.26176 loss)
I0523 03:14:18.767750 35003 sgd_solver.cpp:112] Iteration 129400, lr = 0.01
I0523 03:14:21.459986 35003 solver.cpp:239] Iteration 129410 (2.91587 iter/s, 3.4295s/10 iters), loss = 7.79995
I0523 03:14:21.460037 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79995 (* 1 = 7.79995 loss)
I0523 03:14:21.468353 35003 sgd_solver.cpp:112] Iteration 129410, lr = 0.01
I0523 03:14:25.119379 35003 solver.cpp:239] Iteration 129420 (2.73285 iter/s, 3.65919s/10 iters), loss = 7.66929
I0523 03:14:25.119427 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66929 (* 1 = 7.66929 loss)
I0523 03:14:25.652618 35003 sgd_solver.cpp:112] Iteration 129420, lr = 0.01
I0523 03:14:30.050091 35003 solver.cpp:239] Iteration 129430 (2.02821 iter/s, 4.93046s/10 iters), loss = 6.03109
I0523 03:14:30.050238 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03109 (* 1 = 6.03109 loss)
I0523 03:14:30.055801 35003 sgd_solver.cpp:112] Iteration 129430, lr = 0.01
I0523 03:14:34.372458 35003 solver.cpp:239] Iteration 129440 (2.31372 iter/s, 4.32204s/10 iters), loss = 7.1086
I0523 03:14:34.372511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1086 (* 1 = 7.1086 loss)
I0523 03:14:34.383373 35003 sgd_solver.cpp:112] Iteration 129440, lr = 0.01
I0523 03:14:36.746199 35003 solver.cpp:239] Iteration 129450 (4.21306 iter/s, 2.37357s/10 iters), loss = 5.97633
I0523 03:14:36.746256 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97633 (* 1 = 5.97633 loss)
I0523 03:14:37.480654 35003 sgd_solver.cpp:112] Iteration 129450, lr = 0.01
I0523 03:14:40.968864 35003 solver.cpp:239] Iteration 129460 (2.36831 iter/s, 4.22243s/10 iters), loss = 6.92027
I0523 03:14:40.968917 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92027 (* 1 = 6.92027 loss)
I0523 03:14:40.982242 35003 sgd_solver.cpp:112] Iteration 129460, lr = 0.01
I0523 03:14:43.638964 35003 solver.cpp:239] Iteration 129470 (3.74542 iter/s, 2.66993s/10 iters), loss = 7.7854
I0523 03:14:43.639027 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7854 (* 1 = 7.7854 loss)
I0523 03:14:43.641501 35003 sgd_solver.cpp:112] Iteration 129470, lr = 0.01
I0523 03:14:46.434476 35003 solver.cpp:239] Iteration 129480 (3.5774 iter/s, 2.79532s/10 iters), loss = 7.07714
I0523 03:14:46.434536 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07714 (* 1 = 7.07714 loss)
I0523 03:14:47.149206 35003 sgd_solver.cpp:112] Iteration 129480, lr = 0.01
I0523 03:14:49.975075 35003 solver.cpp:239] Iteration 129490 (2.82455 iter/s, 3.54039s/10 iters), loss = 7.85335
I0523 03:14:49.975132 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85335 (* 1 = 7.85335 loss)
I0523 03:14:49.993520 35003 sgd_solver.cpp:112] Iteration 129490, lr = 0.01
I0523 03:14:52.835278 35003 solver.cpp:239] Iteration 129500 (3.49647 iter/s, 2.86002s/10 iters), loss = 7.11458
I0523 03:14:52.835321 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11458 (* 1 = 7.11458 loss)
I0523 03:14:53.573575 35003 sgd_solver.cpp:112] Iteration 129500, lr = 0.01
I0523 03:14:55.670250 35003 solver.cpp:239] Iteration 129510 (3.52757 iter/s, 2.83481s/10 iters), loss = 7.62787
I0523 03:14:55.670291 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62787 (* 1 = 7.62787 loss)
I0523 03:14:56.405380 35003 sgd_solver.cpp:112] Iteration 129510, lr = 0.01
I0523 03:15:00.864974 35003 solver.cpp:239] Iteration 129520 (1.92513 iter/s, 5.19446s/10 iters), loss = 7.18503
I0523 03:15:00.865247 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18503 (* 1 = 7.18503 loss)
I0523 03:15:00.871904 35003 sgd_solver.cpp:112] Iteration 129520, lr = 0.01
I0523 03:15:03.154260 35003 solver.cpp:239] Iteration 129530 (4.36888 iter/s, 2.28892s/10 iters), loss = 7.32447
I0523 03:15:03.154305 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32447 (* 1 = 7.32447 loss)
I0523 03:15:03.161592 35003 sgd_solver.cpp:112] Iteration 129530, lr = 0.01
I0523 03:15:06.717310 35003 solver.cpp:239] Iteration 129540 (2.80674 iter/s, 3.56286s/10 iters), loss = 6.85516
I0523 03:15:06.717355 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85516 (* 1 = 6.85516 loss)
I0523 03:15:06.726740 35003 sgd_solver.cpp:112] Iteration 129540, lr = 0.01
I0523 03:15:08.045563 35003 solver.cpp:239] Iteration 129550 (7.52931 iter/s, 1.32814s/10 iters), loss = 6.88329
I0523 03:15:08.045622 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88329 (* 1 = 6.88329 loss)
I0523 03:15:08.055773 35003 sgd_solver.cpp:112] Iteration 129550, lr = 0.01
I0523 03:15:10.857555 35003 solver.cpp:239] Iteration 129560 (3.55643 iter/s, 2.81181s/10 iters), loss = 6.8914
I0523 03:15:10.857605 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8914 (* 1 = 6.8914 loss)
I0523 03:15:11.486121 35003 sgd_solver.cpp:112] Iteration 129560, lr = 0.01
I0523 03:15:14.988327 35003 solver.cpp:239] Iteration 129570 (2.42098 iter/s, 4.13055s/10 iters), loss = 6.8601
I0523 03:15:14.988380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8601 (* 1 = 6.8601 loss)
I0523 03:15:15.696003 35003 sgd_solver.cpp:112] Iteration 129570, lr = 0.01
I0523 03:15:19.364435 35003 solver.cpp:239] Iteration 129580 (2.28526 iter/s, 4.37588s/10 iters), loss = 7.71503
I0523 03:15:19.364476 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71503 (* 1 = 7.71503 loss)
I0523 03:15:19.370564 35003 sgd_solver.cpp:112] Iteration 129580, lr = 0.01
I0523 03:15:23.793954 35003 solver.cpp:239] Iteration 129590 (2.25769 iter/s, 4.4293s/10 iters), loss = 6.58382
I0523 03:15:23.793997 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58382 (* 1 = 6.58382 loss)
I0523 03:15:23.802007 35003 sgd_solver.cpp:112] Iteration 129590, lr = 0.01
I0523 03:15:26.929484 35003 solver.cpp:239] Iteration 129600 (3.18944 iter/s, 3.13535s/10 iters), loss = 5.84663
I0523 03:15:26.929543 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84663 (* 1 = 5.84663 loss)
I0523 03:15:26.933800 35003 sgd_solver.cpp:112] Iteration 129600, lr = 0.01
I0523 03:15:29.768326 35003 solver.cpp:239] Iteration 129610 (3.52279 iter/s, 2.83866s/10 iters), loss = 7.29685
I0523 03:15:29.768364 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29685 (* 1 = 7.29685 loss)
I0523 03:15:29.776706 35003 sgd_solver.cpp:112] Iteration 129610, lr = 0.01
I0523 03:15:33.467063 35003 solver.cpp:239] Iteration 129620 (2.70377 iter/s, 3.69854s/10 iters), loss = 7.23755
I0523 03:15:33.467253 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23755 (* 1 = 7.23755 loss)
I0523 03:15:34.201581 35003 sgd_solver.cpp:112] Iteration 129620, lr = 0.01
I0523 03:15:36.823318 35003 solver.cpp:239] Iteration 129630 (2.9798 iter/s, 3.35593s/10 iters), loss = 7.70074
I0523 03:15:36.823374 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70074 (* 1 = 7.70074 loss)
I0523 03:15:37.563714 35003 sgd_solver.cpp:112] Iteration 129630, lr = 0.01
I0523 03:15:41.193514 35003 solver.cpp:239] Iteration 129640 (2.28835 iter/s, 4.36996s/10 iters), loss = 6.15366
I0523 03:15:41.193552 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15366 (* 1 = 6.15366 loss)
I0523 03:15:41.199532 35003 sgd_solver.cpp:112] Iteration 129640, lr = 0.01
I0523 03:15:42.943552 35003 solver.cpp:239] Iteration 129650 (5.71458 iter/s, 1.74991s/10 iters), loss = 5.91487
I0523 03:15:42.943600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91487 (* 1 = 5.91487 loss)
I0523 03:15:42.950300 35003 sgd_solver.cpp:112] Iteration 129650, lr = 0.01
I0523 03:15:44.612658 35003 solver.cpp:239] Iteration 129660 (5.99167 iter/s, 1.66898s/10 iters), loss = 6.32594
I0523 03:15:44.612699 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32594 (* 1 = 6.32594 loss)
I0523 03:15:44.625681 35003 sgd_solver.cpp:112] Iteration 129660, lr = 0.01
I0523 03:15:47.525602 35003 solver.cpp:239] Iteration 129670 (3.43315 iter/s, 2.91278s/10 iters), loss = 7.24799
I0523 03:15:47.525645 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24799 (* 1 = 7.24799 loss)
I0523 03:15:47.538678 35003 sgd_solver.cpp:112] Iteration 129670, lr = 0.01
I0523 03:15:51.035930 35003 solver.cpp:239] Iteration 129680 (2.8489 iter/s, 3.51013s/10 iters), loss = 7.11384
I0523 03:15:51.035979 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11384 (* 1 = 7.11384 loss)
I0523 03:15:51.049047 35003 sgd_solver.cpp:112] Iteration 129680, lr = 0.01
I0523 03:15:54.665160 35003 solver.cpp:239] Iteration 129690 (2.75557 iter/s, 3.62902s/10 iters), loss = 6.17194
I0523 03:15:54.665216 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17194 (* 1 = 6.17194 loss)
I0523 03:15:54.670822 35003 sgd_solver.cpp:112] Iteration 129690, lr = 0.01
I0523 03:15:59.818737 35003 solver.cpp:239] Iteration 129700 (1.9405 iter/s, 5.1533s/10 iters), loss = 7.09365
I0523 03:15:59.818781 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09365 (* 1 = 7.09365 loss)
I0523 03:16:00.508714 35003 sgd_solver.cpp:112] Iteration 129700, lr = 0.01
I0523 03:16:03.512436 35003 solver.cpp:239] Iteration 129710 (2.70746 iter/s, 3.6935s/10 iters), loss = 6.58357
I0523 03:16:03.512722 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58357 (* 1 = 6.58357 loss)
I0523 03:16:03.547845 35003 sgd_solver.cpp:112] Iteration 129710, lr = 0.01
I0523 03:16:05.616900 35003 solver.cpp:239] Iteration 129720 (4.75259 iter/s, 2.10411s/10 iters), loss = 6.79284
I0523 03:16:05.616945 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79284 (* 1 = 6.79284 loss)
I0523 03:16:05.638723 35003 sgd_solver.cpp:112] Iteration 129720, lr = 0.01
I0523 03:16:10.744218 35003 solver.cpp:239] Iteration 129730 (1.95043 iter/s, 5.12707s/10 iters), loss = 6.89941
I0523 03:16:10.744256 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89941 (* 1 = 6.89941 loss)
I0523 03:16:11.446640 35003 sgd_solver.cpp:112] Iteration 129730, lr = 0.01
I0523 03:16:15.529486 35003 solver.cpp:239] Iteration 129740 (2.08985 iter/s, 4.78503s/10 iters), loss = 7.21908
I0523 03:16:15.529541 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21908 (* 1 = 7.21908 loss)
I0523 03:16:15.533669 35003 sgd_solver.cpp:112] Iteration 129740, lr = 0.01
I0523 03:16:20.516755 35003 solver.cpp:239] Iteration 129750 (2.00522 iter/s, 4.98699s/10 iters), loss = 6.14635
I0523 03:16:20.516803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14635 (* 1 = 6.14635 loss)
I0523 03:16:21.251873 35003 sgd_solver.cpp:112] Iteration 129750, lr = 0.01
I0523 03:16:24.952577 35003 solver.cpp:239] Iteration 129760 (2.25449 iter/s, 4.43559s/10 iters), loss = 7.35808
I0523 03:16:24.952617 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35808 (* 1 = 7.35808 loss)
I0523 03:16:24.965224 35003 sgd_solver.cpp:112] Iteration 129760, lr = 0.01
I0523 03:16:29.171190 35003 solver.cpp:239] Iteration 129770 (2.37057 iter/s, 4.2184s/10 iters), loss = 7.91217
I0523 03:16:29.171339 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91217 (* 1 = 7.91217 loss)
I0523 03:16:29.189113 35003 sgd_solver.cpp:112] Iteration 129770, lr = 0.01
I0523 03:16:34.846788 35003 solver.cpp:239] Iteration 129780 (1.76205 iter/s, 5.67522s/10 iters), loss = 7.24183
I0523 03:16:34.847017 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24183 (* 1 = 7.24183 loss)
I0523 03:16:35.341092 35003 sgd_solver.cpp:112] Iteration 129780, lr = 0.01
I0523 03:16:39.002976 35003 solver.cpp:239] Iteration 129790 (2.40627 iter/s, 4.15582s/10 iters), loss = 6.26198
I0523 03:16:39.003028 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26198 (* 1 = 6.26198 loss)
I0523 03:16:39.004331 35003 sgd_solver.cpp:112] Iteration 129790, lr = 0.01
I0523 03:16:42.551633 35003 solver.cpp:239] Iteration 129800 (2.81813 iter/s, 3.54846s/10 iters), loss = 7.36046
I0523 03:16:42.551672 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36046 (* 1 = 7.36046 loss)
I0523 03:16:42.564432 35003 sgd_solver.cpp:112] Iteration 129800, lr = 0.01
I0523 03:16:46.742344 35003 solver.cpp:239] Iteration 129810 (2.38635 iter/s, 4.19049s/10 iters), loss = 7.51816
I0523 03:16:46.742388 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51816 (* 1 = 7.51816 loss)
I0523 03:16:46.756119 35003 sgd_solver.cpp:112] Iteration 129810, lr = 0.01
I0523 03:16:50.385372 35003 solver.cpp:239] Iteration 129820 (2.74512 iter/s, 3.64283s/10 iters), loss = 7.01571
I0523 03:16:50.385414 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01571 (* 1 = 7.01571 loss)
I0523 03:16:50.403914 35003 sgd_solver.cpp:112] Iteration 129820, lr = 0.01
I0523 03:16:52.710155 35003 solver.cpp:239] Iteration 129830 (4.30176 iter/s, 2.32463s/10 iters), loss = 7.6978
I0523 03:16:52.710201 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6978 (* 1 = 7.6978 loss)
I0523 03:16:53.451459 35003 sgd_solver.cpp:112] Iteration 129830, lr = 0.01
I0523 03:16:56.195435 35003 solver.cpp:239] Iteration 129840 (2.86937 iter/s, 3.48508s/10 iters), loss = 6.30428
I0523 03:16:56.195490 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30428 (* 1 = 6.30428 loss)
I0523 03:16:56.208917 35003 sgd_solver.cpp:112] Iteration 129840, lr = 0.01
I0523 03:16:58.173312 35003 solver.cpp:239] Iteration 129850 (5.05629 iter/s, 1.97774s/10 iters), loss = 6.41596
I0523 03:16:58.173357 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41596 (* 1 = 6.41596 loss)
I0523 03:16:58.184787 35003 sgd_solver.cpp:112] Iteration 129850, lr = 0.01
I0523 03:17:01.659430 35003 solver.cpp:239] Iteration 129860 (2.86868 iter/s, 3.48593s/10 iters), loss = 6.72425
I0523 03:17:01.659474 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72425 (* 1 = 6.72425 loss)
I0523 03:17:02.375144 35003 sgd_solver.cpp:112] Iteration 129860, lr = 0.01
I0523 03:17:06.865015 35003 solver.cpp:239] Iteration 129870 (1.92111 iter/s, 5.20533s/10 iters), loss = 6.53762
I0523 03:17:06.865242 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53762 (* 1 = 6.53762 loss)
I0523 03:17:06.877599 35003 sgd_solver.cpp:112] Iteration 129870, lr = 0.01
I0523 03:17:09.853402 35003 solver.cpp:239] Iteration 129880 (3.34669 iter/s, 2.98803s/10 iters), loss = 7.4563
I0523 03:17:09.853451 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4563 (* 1 = 7.4563 loss)
I0523 03:17:09.866256 35003 sgd_solver.cpp:112] Iteration 129880, lr = 0.01
I0523 03:17:14.834329 35003 solver.cpp:239] Iteration 129890 (2.00776 iter/s, 4.98067s/10 iters), loss = 7.76509
I0523 03:17:14.834376 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76509 (* 1 = 7.76509 loss)
I0523 03:17:14.873242 35003 sgd_solver.cpp:112] Iteration 129890, lr = 0.01
I0523 03:17:17.775593 35003 solver.cpp:239] Iteration 129900 (3.4001 iter/s, 2.94109s/10 iters), loss = 7.91402
I0523 03:17:17.775631 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91402 (* 1 = 7.91402 loss)
I0523 03:17:17.780650 35003 sgd_solver.cpp:112] Iteration 129900, lr = 0.01
I0523 03:17:23.663995 35003 solver.cpp:239] Iteration 129910 (1.69833 iter/s, 5.88813s/10 iters), loss = 7.58539
I0523 03:17:23.664038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58539 (* 1 = 7.58539 loss)
I0523 03:17:23.703223 35003 sgd_solver.cpp:112] Iteration 129910, lr = 0.01
I0523 03:17:25.364672 35003 solver.cpp:239] Iteration 129920 (5.88043 iter/s, 1.70056s/10 iters), loss = 6.73153
I0523 03:17:25.364728 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73153 (* 1 = 6.73153 loss)
I0523 03:17:25.372123 35003 sgd_solver.cpp:112] Iteration 129920, lr = 0.01
I0523 03:17:27.529911 35003 solver.cpp:239] Iteration 129930 (4.61874 iter/s, 2.16509s/10 iters), loss = 7.15896
I0523 03:17:27.529953 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15896 (* 1 = 7.15896 loss)
I0523 03:17:28.264909 35003 sgd_solver.cpp:112] Iteration 129930, lr = 0.01
I0523 03:17:32.555624 35003 solver.cpp:239] Iteration 129940 (1.98987 iter/s, 5.02546s/10 iters), loss = 6.29327
I0523 03:17:32.555665 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29327 (* 1 = 6.29327 loss)
I0523 03:17:32.562795 35003 sgd_solver.cpp:112] Iteration 129940, lr = 0.01
I0523 03:17:36.856477 35003 solver.cpp:239] Iteration 129950 (2.32525 iter/s, 4.30062s/10 iters), loss = 6.23573
I0523 03:17:36.856537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23573 (* 1 = 6.23573 loss)
I0523 03:17:36.867086 35003 sgd_solver.cpp:112] Iteration 129950, lr = 0.01
I0523 03:17:40.723177 35003 solver.cpp:239] Iteration 129960 (2.58633 iter/s, 3.86648s/10 iters), loss = 6.61961
I0523 03:17:40.723225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61961 (* 1 = 6.61961 loss)
I0523 03:17:40.735347 35003 sgd_solver.cpp:112] Iteration 129960, lr = 0.01
I0523 03:17:43.569648 35003 solver.cpp:239] Iteration 129970 (3.51334 iter/s, 2.8463s/10 iters), loss = 7.28146
I0523 03:17:43.569685 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28146 (* 1 = 7.28146 loss)
I0523 03:17:43.582901 35003 sgd_solver.cpp:112] Iteration 129970, lr = 0.01
I0523 03:17:45.715129 35003 solver.cpp:239] Iteration 129980 (4.66125 iter/s, 2.14535s/10 iters), loss = 6.91056
I0523 03:17:45.715176 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91056 (* 1 = 6.91056 loss)
I0523 03:17:45.727689 35003 sgd_solver.cpp:112] Iteration 129980, lr = 0.01
I0523 03:17:48.476505 35003 solver.cpp:239] Iteration 129990 (3.6216 iter/s, 2.76121s/10 iters), loss = 6.42479
I0523 03:17:48.476553 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42479 (* 1 = 6.42479 loss)
I0523 03:17:48.489933 35003 sgd_solver.cpp:112] Iteration 129990, lr = 0.01
I0523 03:17:51.213532 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_130000.caffemodel
I0523 03:17:51.536746 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_130000.solverstate
I0523 03:17:51.688884 35003 solver.cpp:239] Iteration 130000 (3.11314 iter/s, 3.21219s/10 iters), loss = 6.72961
I0523 03:17:51.688931 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72961 (* 1 = 6.72961 loss)
I0523 03:17:52.381336 35003 sgd_solver.cpp:112] Iteration 130000, lr = 0.01
I0523 03:17:55.829501 35003 solver.cpp:239] Iteration 130010 (2.41522 iter/s, 4.1404s/10 iters), loss = 8.45959
I0523 03:17:55.829545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.45959 (* 1 = 8.45959 loss)
I0523 03:17:55.833462 35003 sgd_solver.cpp:112] Iteration 130010, lr = 0.01
I0523 03:18:00.916405 35003 solver.cpp:239] Iteration 130020 (1.96593 iter/s, 5.08665s/10 iters), loss = 6.45892
I0523 03:18:00.916453 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45892 (* 1 = 6.45892 loss)
I0523 03:18:00.928637 35003 sgd_solver.cpp:112] Iteration 130020, lr = 0.01
I0523 03:18:03.722936 35003 solver.cpp:239] Iteration 130030 (3.56334 iter/s, 2.80636s/10 iters), loss = 6.80154
I0523 03:18:03.722975 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80154 (* 1 = 6.80154 loss)
I0523 03:18:03.728408 35003 sgd_solver.cpp:112] Iteration 130030, lr = 0.01
I0523 03:18:07.433405 35003 solver.cpp:239] Iteration 130040 (2.69523 iter/s, 3.71026s/10 iters), loss = 7.93487
I0523 03:18:07.433702 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93487 (* 1 = 7.93487 loss)
I0523 03:18:08.168381 35003 sgd_solver.cpp:112] Iteration 130040, lr = 0.01
I0523 03:18:11.178640 35003 solver.cpp:239] Iteration 130050 (2.67036 iter/s, 3.74481s/10 iters), loss = 6.25911
I0523 03:18:11.178680 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25911 (* 1 = 6.25911 loss)
I0523 03:18:11.191112 35003 sgd_solver.cpp:112] Iteration 130050, lr = 0.01
I0523 03:18:13.658565 35003 solver.cpp:239] Iteration 130060 (4.03264 iter/s, 2.47977s/10 iters), loss = 7.04029
I0523 03:18:13.658639 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04029 (* 1 = 7.04029 loss)
I0523 03:18:13.979048 35003 sgd_solver.cpp:112] Iteration 130060, lr = 0.01
I0523 03:18:16.883038 35003 solver.cpp:239] Iteration 130070 (3.10148 iter/s, 3.22427s/10 iters), loss = 6.8618
I0523 03:18:16.883098 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8618 (* 1 = 6.8618 loss)
I0523 03:18:17.584991 35003 sgd_solver.cpp:112] Iteration 130070, lr = 0.01
I0523 03:18:20.093423 35003 solver.cpp:239] Iteration 130080 (3.11509 iter/s, 3.21018s/10 iters), loss = 7.01289
I0523 03:18:20.093488 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01289 (* 1 = 7.01289 loss)
I0523 03:18:20.729028 35003 sgd_solver.cpp:112] Iteration 130080, lr = 0.01
I0523 03:18:25.920876 35003 solver.cpp:239] Iteration 130090 (1.7161 iter/s, 5.82715s/10 iters), loss = 6.32344
I0523 03:18:25.920941 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32344 (* 1 = 6.32344 loss)
I0523 03:18:26.636433 35003 sgd_solver.cpp:112] Iteration 130090, lr = 0.01
I0523 03:18:30.240593 35003 solver.cpp:239] Iteration 130100 (2.31509 iter/s, 4.31948s/10 iters), loss = 7.26883
I0523 03:18:30.240638 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26883 (* 1 = 7.26883 loss)
I0523 03:18:30.267629 35003 sgd_solver.cpp:112] Iteration 130100, lr = 0.01
I0523 03:18:33.924016 35003 solver.cpp:239] Iteration 130110 (2.71502 iter/s, 3.68322s/10 iters), loss = 5.97142
I0523 03:18:33.924082 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97142 (* 1 = 5.97142 loss)
I0523 03:18:33.930902 35003 sgd_solver.cpp:112] Iteration 130110, lr = 0.01
I0523 03:18:37.298926 35003 solver.cpp:239] Iteration 130120 (2.96323 iter/s, 3.3747s/10 iters), loss = 6.34984
I0523 03:18:37.298990 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34984 (* 1 = 6.34984 loss)
I0523 03:18:37.311992 35003 sgd_solver.cpp:112] Iteration 130120, lr = 0.01
I0523 03:18:39.270874 35003 solver.cpp:239] Iteration 130130 (5.07151 iter/s, 1.9718s/10 iters), loss = 6.87693
I0523 03:18:39.271065 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87693 (* 1 = 6.87693 loss)
I0523 03:18:39.313639 35003 sgd_solver.cpp:112] Iteration 130130, lr = 0.01
I0523 03:18:42.236896 35003 solver.cpp:239] Iteration 130140 (3.3719 iter/s, 2.96569s/10 iters), loss = 6.91715
I0523 03:18:42.236955 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91715 (* 1 = 6.91715 loss)
I0523 03:18:42.244858 35003 sgd_solver.cpp:112] Iteration 130140, lr = 0.01
I0523 03:18:45.118759 35003 solver.cpp:239] Iteration 130150 (3.47019 iter/s, 2.88169s/10 iters), loss = 6.88681
I0523 03:18:45.118801 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88681 (* 1 = 6.88681 loss)
I0523 03:18:45.140794 35003 sgd_solver.cpp:112] Iteration 130150, lr = 0.01
I0523 03:18:48.746685 35003 solver.cpp:239] Iteration 130160 (2.75654 iter/s, 3.62774s/10 iters), loss = 6.57545
I0523 03:18:48.746754 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57545 (* 1 = 6.57545 loss)
I0523 03:18:48.755837 35003 sgd_solver.cpp:112] Iteration 130160, lr = 0.01
I0523 03:18:51.491259 35003 solver.cpp:239] Iteration 130170 (3.64381 iter/s, 2.74438s/10 iters), loss = 6.97614
I0523 03:18:51.491312 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97614 (* 1 = 6.97614 loss)
I0523 03:18:52.206297 35003 sgd_solver.cpp:112] Iteration 130170, lr = 0.01
I0523 03:18:54.883157 35003 solver.cpp:239] Iteration 130180 (2.94837 iter/s, 3.39171s/10 iters), loss = 7.25554
I0523 03:18:54.883194 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25554 (* 1 = 7.25554 loss)
I0523 03:18:54.896407 35003 sgd_solver.cpp:112] Iteration 130180, lr = 0.01
I0523 03:18:58.646288 35003 solver.cpp:239] Iteration 130190 (2.6575 iter/s, 3.76293s/10 iters), loss = 6.43916
I0523 03:18:58.646345 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43916 (* 1 = 6.43916 loss)
I0523 03:18:59.374315 35003 sgd_solver.cpp:112] Iteration 130190, lr = 0.01
I0523 03:19:03.631505 35003 solver.cpp:239] Iteration 130200 (2.00604 iter/s, 4.98494s/10 iters), loss = 6.46586
I0523 03:19:03.631582 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46586 (* 1 = 6.46586 loss)
I0523 03:19:03.637964 35003 sgd_solver.cpp:112] Iteration 130200, lr = 0.01
I0523 03:19:07.203011 35003 solver.cpp:239] Iteration 130210 (2.80013 iter/s, 3.57127s/10 iters), loss = 7.15793
I0523 03:19:07.203058 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15793 (* 1 = 7.15793 loss)
I0523 03:19:07.212013 35003 sgd_solver.cpp:112] Iteration 130210, lr = 0.01
I0523 03:19:09.987020 35003 solver.cpp:239] Iteration 130220 (3.5922 iter/s, 2.78381s/10 iters), loss = 5.50509
I0523 03:19:09.987270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.50509 (* 1 = 5.50509 loss)
I0523 03:19:10.588488 35003 sgd_solver.cpp:112] Iteration 130220, lr = 0.01
I0523 03:19:14.386756 35003 solver.cpp:239] Iteration 130230 (2.27307 iter/s, 4.39933s/10 iters), loss = 7.18272
I0523 03:19:14.386797 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18272 (* 1 = 7.18272 loss)
I0523 03:19:14.411269 35003 sgd_solver.cpp:112] Iteration 130230, lr = 0.01
I0523 03:19:18.877173 35003 solver.cpp:239] Iteration 130240 (2.22709 iter/s, 4.49017s/10 iters), loss = 6.61206
I0523 03:19:18.877225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61206 (* 1 = 6.61206 loss)
I0523 03:19:19.591832 35003 sgd_solver.cpp:112] Iteration 130240, lr = 0.01
I0523 03:19:22.396453 35003 solver.cpp:239] Iteration 130250 (2.84166 iter/s, 3.51908s/10 iters), loss = 7.90154
I0523 03:19:22.396517 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90154 (* 1 = 7.90154 loss)
I0523 03:19:22.836441 35003 sgd_solver.cpp:112] Iteration 130250, lr = 0.01
I0523 03:19:24.366609 35003 solver.cpp:239] Iteration 130260 (5.07614 iter/s, 1.97s/10 iters), loss = 7.23824
I0523 03:19:24.366650 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23824 (* 1 = 7.23824 loss)
I0523 03:19:24.379873 35003 sgd_solver.cpp:112] Iteration 130260, lr = 0.01
I0523 03:19:26.938652 35003 solver.cpp:239] Iteration 130270 (3.88819 iter/s, 2.57189s/10 iters), loss = 6.87287
I0523 03:19:26.938726 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87287 (* 1 = 6.87287 loss)
I0523 03:19:26.954710 35003 sgd_solver.cpp:112] Iteration 130270, lr = 0.01
I0523 03:19:31.226991 35003 solver.cpp:239] Iteration 130280 (2.33202 iter/s, 4.28812s/10 iters), loss = 7.31928
I0523 03:19:31.227033 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31928 (* 1 = 7.31928 loss)
I0523 03:19:31.896709 35003 sgd_solver.cpp:112] Iteration 130280, lr = 0.01
I0523 03:19:35.362762 35003 solver.cpp:239] Iteration 130290 (2.41805 iter/s, 4.13556s/10 iters), loss = 5.61345
I0523 03:19:35.362802 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.61345 (* 1 = 5.61345 loss)
I0523 03:19:35.380739 35003 sgd_solver.cpp:112] Iteration 130290, lr = 0.01
I0523 03:19:39.608697 35003 solver.cpp:239] Iteration 130300 (2.35531 iter/s, 4.24572s/10 iters), loss = 7.0903
I0523 03:19:39.608736 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0903 (* 1 = 7.0903 loss)
I0523 03:19:39.627434 35003 sgd_solver.cpp:112] Iteration 130300, lr = 0.01
I0523 03:19:42.406328 35003 solver.cpp:239] Iteration 130310 (3.57466 iter/s, 2.79747s/10 iters), loss = 8.94062
I0523 03:19:42.406544 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.94062 (* 1 = 8.94062 loss)
I0523 03:19:42.412132 35003 sgd_solver.cpp:112] Iteration 130310, lr = 0.01
I0523 03:19:46.714228 35003 solver.cpp:239] Iteration 130320 (2.32239 iter/s, 4.3059s/10 iters), loss = 6.53543
I0523 03:19:46.714285 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53543 (* 1 = 6.53543 loss)
I0523 03:19:46.738678 35003 sgd_solver.cpp:112] Iteration 130320, lr = 0.01
I0523 03:19:49.583341 35003 solver.cpp:239] Iteration 130330 (3.48561 iter/s, 2.86894s/10 iters), loss = 6.62727
I0523 03:19:49.583380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62727 (* 1 = 6.62727 loss)
I0523 03:19:49.596671 35003 sgd_solver.cpp:112] Iteration 130330, lr = 0.01
I0523 03:19:51.718973 35003 solver.cpp:239] Iteration 130340 (4.68276 iter/s, 2.13549s/10 iters), loss = 7.23979
I0523 03:19:51.719027 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23979 (* 1 = 7.23979 loss)
I0523 03:19:52.453603 35003 sgd_solver.cpp:112] Iteration 130340, lr = 0.01
I0523 03:19:55.286218 35003 solver.cpp:239] Iteration 130350 (2.80344 iter/s, 3.56704s/10 iters), loss = 7.37627
I0523 03:19:55.286264 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37627 (* 1 = 7.37627 loss)
I0523 03:19:55.773820 35003 sgd_solver.cpp:112] Iteration 130350, lr = 0.01
I0523 03:19:59.357817 35003 solver.cpp:239] Iteration 130360 (2.45618 iter/s, 4.07137s/10 iters), loss = 7.19773
I0523 03:19:59.357870 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19773 (* 1 = 7.19773 loss)
I0523 03:19:59.958226 35003 sgd_solver.cpp:112] Iteration 130360, lr = 0.01
I0523 03:20:03.166023 35003 solver.cpp:239] Iteration 130370 (2.62605 iter/s, 3.808s/10 iters), loss = 8.0599
I0523 03:20:03.166060 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0599 (* 1 = 8.0599 loss)
I0523 03:20:03.172680 35003 sgd_solver.cpp:112] Iteration 130370, lr = 0.01
I0523 03:20:07.576369 35003 solver.cpp:239] Iteration 130380 (2.26751 iter/s, 4.41013s/10 iters), loss = 7.1653
I0523 03:20:07.576422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1653 (* 1 = 7.1653 loss)
I0523 03:20:08.253154 35003 sgd_solver.cpp:112] Iteration 130380, lr = 0.01
I0523 03:20:13.184129 35003 solver.cpp:239] Iteration 130390 (1.78333 iter/s, 5.60748s/10 iters), loss = 6.81884
I0523 03:20:13.184428 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81884 (* 1 = 6.81884 loss)
I0523 03:20:13.186429 35003 sgd_solver.cpp:112] Iteration 130390, lr = 0.01
I0523 03:20:16.762945 35003 solver.cpp:239] Iteration 130400 (2.79456 iter/s, 3.57838s/10 iters), loss = 5.9358
I0523 03:20:16.762997 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9358 (* 1 = 5.9358 loss)
I0523 03:20:17.385769 35003 sgd_solver.cpp:112] Iteration 130400, lr = 0.01
I0523 03:20:19.425405 35003 solver.cpp:239] Iteration 130410 (3.75616 iter/s, 2.66229s/10 iters), loss = 7.50186
I0523 03:20:19.425446 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50186 (* 1 = 7.50186 loss)
I0523 03:20:19.438508 35003 sgd_solver.cpp:112] Iteration 130410, lr = 0.01
I0523 03:20:20.794546 35003 solver.cpp:239] Iteration 130420 (7.3044 iter/s, 1.36904s/10 iters), loss = 6.65286
I0523 03:20:20.794594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65286 (* 1 = 6.65286 loss)
I0523 03:20:21.523522 35003 sgd_solver.cpp:112] Iteration 130420, lr = 0.01
I0523 03:20:25.119454 35003 solver.cpp:239] Iteration 130430 (2.31231 iter/s, 4.32468s/10 iters), loss = 7.09545
I0523 03:20:25.119501 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09545 (* 1 = 7.09545 loss)
I0523 03:20:25.126818 35003 sgd_solver.cpp:112] Iteration 130430, lr = 0.01
I0523 03:20:28.322005 35003 solver.cpp:239] Iteration 130440 (3.12269 iter/s, 3.20237s/10 iters), loss = 5.87367
I0523 03:20:28.322046 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87367 (* 1 = 5.87367 loss)
I0523 03:20:28.335101 35003 sgd_solver.cpp:112] Iteration 130440, lr = 0.01
I0523 03:20:30.477936 35003 solver.cpp:239] Iteration 130450 (4.63866 iter/s, 2.1558s/10 iters), loss = 6.77619
I0523 03:20:30.477973 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77619 (* 1 = 6.77619 loss)
I0523 03:20:30.496366 35003 sgd_solver.cpp:112] Iteration 130450, lr = 0.01
I0523 03:20:32.578673 35003 solver.cpp:239] Iteration 130460 (4.76054 iter/s, 2.1006s/10 iters), loss = 7.90538
I0523 03:20:32.578749 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90538 (* 1 = 7.90538 loss)
I0523 03:20:33.298022 35003 sgd_solver.cpp:112] Iteration 130460, lr = 0.01
I0523 03:20:36.966111 35003 solver.cpp:239] Iteration 130470 (2.27937 iter/s, 4.38718s/10 iters), loss = 6.441
I0523 03:20:36.966163 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.441 (* 1 = 6.441 loss)
I0523 03:20:36.979305 35003 sgd_solver.cpp:112] Iteration 130470, lr = 0.01
I0523 03:20:40.551765 35003 solver.cpp:239] Iteration 130480 (2.78904 iter/s, 3.58546s/10 iters), loss = 7.85339
I0523 03:20:40.551800 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85339 (* 1 = 7.85339 loss)
I0523 03:20:40.565011 35003 sgd_solver.cpp:112] Iteration 130480, lr = 0.01
I0523 03:20:42.734668 35003 solver.cpp:239] Iteration 130490 (4.58135 iter/s, 2.18276s/10 iters), loss = 7.01126
I0523 03:20:42.734727 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01126 (* 1 = 7.01126 loss)
I0523 03:20:42.747632 35003 sgd_solver.cpp:112] Iteration 130490, lr = 0.01
I0523 03:20:45.641456 35003 solver.cpp:239] Iteration 130500 (3.44044 iter/s, 2.9066s/10 iters), loss = 6.88914
I0523 03:20:45.641680 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88914 (* 1 = 6.88914 loss)
I0523 03:20:45.660320 35003 sgd_solver.cpp:112] Iteration 130500, lr = 0.01
I0523 03:20:48.903301 35003 solver.cpp:239] Iteration 130510 (3.06608 iter/s, 3.26149s/10 iters), loss = 6.54253
I0523 03:20:48.903345 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54253 (* 1 = 6.54253 loss)
I0523 03:20:49.611778 35003 sgd_solver.cpp:112] Iteration 130510, lr = 0.01
I0523 03:20:53.117750 35003 solver.cpp:239] Iteration 130520 (2.37291 iter/s, 4.21423s/10 iters), loss = 6.34064
I0523 03:20:53.117796 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34064 (* 1 = 6.34064 loss)
I0523 03:20:53.122583 35003 sgd_solver.cpp:112] Iteration 130520, lr = 0.01
I0523 03:20:55.799676 35003 solver.cpp:239] Iteration 130530 (3.72889 iter/s, 2.68176s/10 iters), loss = 6.55967
I0523 03:20:55.799727 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55967 (* 1 = 6.55967 loss)
I0523 03:20:56.234563 35003 sgd_solver.cpp:112] Iteration 130530, lr = 0.01
I0523 03:20:58.321116 35003 solver.cpp:239] Iteration 130540 (3.96624 iter/s, 2.52128s/10 iters), loss = 6.96467
I0523 03:20:58.321166 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96467 (* 1 = 6.96467 loss)
I0523 03:20:59.030097 35003 sgd_solver.cpp:112] Iteration 130540, lr = 0.01
I0523 03:21:02.463215 35003 solver.cpp:239] Iteration 130550 (2.41436 iter/s, 4.14188s/10 iters), loss = 7.02676
I0523 03:21:02.463254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02676 (* 1 = 7.02676 loss)
I0523 03:21:03.159116 35003 sgd_solver.cpp:112] Iteration 130550, lr = 0.01
I0523 03:21:06.618043 35003 solver.cpp:239] Iteration 130560 (2.40696 iter/s, 4.15462s/10 iters), loss = 7.13542
I0523 03:21:06.618080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13542 (* 1 = 7.13542 loss)
I0523 03:21:06.624048 35003 sgd_solver.cpp:112] Iteration 130560, lr = 0.01
I0523 03:21:09.369344 35003 solver.cpp:239] Iteration 130570 (3.63485 iter/s, 2.75115s/10 iters), loss = 6.68358
I0523 03:21:09.369383 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68358 (* 1 = 6.68358 loss)
I0523 03:21:10.065321 35003 sgd_solver.cpp:112] Iteration 130570, lr = 0.01
I0523 03:21:13.472344 35003 solver.cpp:239] Iteration 130580 (2.43737 iter/s, 4.10279s/10 iters), loss = 6.36931
I0523 03:21:13.472389 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36931 (* 1 = 6.36931 loss)
I0523 03:21:13.482317 35003 sgd_solver.cpp:112] Iteration 130580, lr = 0.01
I0523 03:21:15.631942 35003 solver.cpp:239] Iteration 130590 (4.63081 iter/s, 2.15945s/10 iters), loss = 7.64369
I0523 03:21:15.631991 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64369 (* 1 = 7.64369 loss)
I0523 03:21:15.637356 35003 sgd_solver.cpp:112] Iteration 130590, lr = 0.01
I0523 03:21:19.221007 35003 solver.cpp:239] Iteration 130600 (2.7864 iter/s, 3.58886s/10 iters), loss = 5.15528
I0523 03:21:19.221110 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.15528 (* 1 = 5.15528 loss)
I0523 03:21:19.240206 35003 sgd_solver.cpp:112] Iteration 130600, lr = 0.01
I0523 03:21:22.271119 35003 solver.cpp:239] Iteration 130610 (3.27881 iter/s, 3.04988s/10 iters), loss = 6.83978
I0523 03:21:22.271158 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83978 (* 1 = 6.83978 loss)
I0523 03:21:22.279384 35003 sgd_solver.cpp:112] Iteration 130610, lr = 0.01
I0523 03:21:24.371448 35003 solver.cpp:239] Iteration 130620 (4.76147 iter/s, 2.10019s/10 iters), loss = 6.98173
I0523 03:21:24.371486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98173 (* 1 = 6.98173 loss)
I0523 03:21:24.381000 35003 sgd_solver.cpp:112] Iteration 130620, lr = 0.01
I0523 03:21:27.838614 35003 solver.cpp:239] Iteration 130630 (2.88435 iter/s, 3.46698s/10 iters), loss = 6.01765
I0523 03:21:27.838655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01765 (* 1 = 6.01765 loss)
I0523 03:21:27.851982 35003 sgd_solver.cpp:112] Iteration 130630, lr = 0.01
I0523 03:21:32.705799 35003 solver.cpp:239] Iteration 130640 (2.05468 iter/s, 4.86694s/10 iters), loss = 6.13576
I0523 03:21:32.705854 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13576 (* 1 = 6.13576 loss)
I0523 03:21:32.715948 35003 sgd_solver.cpp:112] Iteration 130640, lr = 0.01
I0523 03:21:36.360599 35003 solver.cpp:239] Iteration 130650 (2.73629 iter/s, 3.65459s/10 iters), loss = 6.83931
I0523 03:21:36.360649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83931 (* 1 = 6.83931 loss)
I0523 03:21:37.043439 35003 sgd_solver.cpp:112] Iteration 130650, lr = 0.01
I0523 03:21:41.445050 35003 solver.cpp:239] Iteration 130660 (1.96688 iter/s, 5.08419s/10 iters), loss = 6.55466
I0523 03:21:41.445101 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55466 (* 1 = 6.55466 loss)
I0523 03:21:42.186259 35003 sgd_solver.cpp:112] Iteration 130660, lr = 0.01
I0523 03:21:46.392441 35003 solver.cpp:239] Iteration 130670 (2.02137 iter/s, 4.94714s/10 iters), loss = 5.889
I0523 03:21:46.392478 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.889 (* 1 = 5.889 loss)
I0523 03:21:46.406155 35003 sgd_solver.cpp:112] Iteration 130670, lr = 0.01
I0523 03:21:49.264461 35003 solver.cpp:239] Iteration 130680 (3.48206 iter/s, 2.87186s/10 iters), loss = 7.19861
I0523 03:21:49.264741 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19861 (* 1 = 7.19861 loss)
I0523 03:21:49.291446 35003 sgd_solver.cpp:112] Iteration 130680, lr = 0.01
I0523 03:21:52.674208 35003 solver.cpp:239] Iteration 130690 (2.93311 iter/s, 3.40935s/10 iters), loss = 6.94527
I0523 03:21:52.674249 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94527 (* 1 = 6.94527 loss)
I0523 03:21:52.693084 35003 sgd_solver.cpp:112] Iteration 130690, lr = 0.01
I0523 03:21:55.041605 35003 solver.cpp:239] Iteration 130700 (4.22431 iter/s, 2.36725s/10 iters), loss = 8.29482
I0523 03:21:55.041649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.29482 (* 1 = 8.29482 loss)
I0523 03:21:55.049623 35003 sgd_solver.cpp:112] Iteration 130700, lr = 0.01
I0523 03:21:58.021735 35003 solver.cpp:239] Iteration 130710 (3.35575 iter/s, 2.97996s/10 iters), loss = 6.79719
I0523 03:21:58.021772 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79719 (* 1 = 6.79719 loss)
I0523 03:21:58.040237 35003 sgd_solver.cpp:112] Iteration 130710, lr = 0.01
I0523 03:22:01.201217 35003 solver.cpp:239] Iteration 130720 (3.14534 iter/s, 3.17931s/10 iters), loss = 6.89426
I0523 03:22:01.201264 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89426 (* 1 = 6.89426 loss)
I0523 03:22:01.369732 35003 sgd_solver.cpp:112] Iteration 130720, lr = 0.01
I0523 03:22:03.995373 35003 solver.cpp:239] Iteration 130730 (3.57912 iter/s, 2.79398s/10 iters), loss = 6.26153
I0523 03:22:03.995440 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26153 (* 1 = 6.26153 loss)
I0523 03:22:04.729781 35003 sgd_solver.cpp:112] Iteration 130730, lr = 0.01
I0523 03:22:06.861994 35003 solver.cpp:239] Iteration 130740 (3.48867 iter/s, 2.86642s/10 iters), loss = 6.94535
I0523 03:22:06.862038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94535 (* 1 = 6.94535 loss)
I0523 03:22:06.871736 35003 sgd_solver.cpp:112] Iteration 130740, lr = 0.01
I0523 03:22:11.819547 35003 solver.cpp:239] Iteration 130750 (2.01812 iter/s, 4.9551s/10 iters), loss = 7.23297
I0523 03:22:11.819600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23297 (* 1 = 7.23297 loss)
I0523 03:22:12.534560 35003 sgd_solver.cpp:112] Iteration 130750, lr = 0.01
I0523 03:22:16.862373 35003 solver.cpp:239] Iteration 130760 (1.98311 iter/s, 5.04257s/10 iters), loss = 7.98973
I0523 03:22:16.862417 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98973 (* 1 = 7.98973 loss)
I0523 03:22:16.875705 35003 sgd_solver.cpp:112] Iteration 130760, lr = 0.01
I0523 03:22:18.916983 35003 solver.cpp:239] Iteration 130770 (4.86743 iter/s, 2.05447s/10 iters), loss = 7.15493
I0523 03:22:18.917028 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15493 (* 1 = 7.15493 loss)
I0523 03:22:18.925752 35003 sgd_solver.cpp:112] Iteration 130770, lr = 0.01
I0523 03:22:21.757474 35003 solver.cpp:239] Iteration 130780 (3.52072 iter/s, 2.84033s/10 iters), loss = 6.77453
I0523 03:22:21.757742 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77453 (* 1 = 6.77453 loss)
I0523 03:22:21.776031 35003 sgd_solver.cpp:112] Iteration 130780, lr = 0.01
I0523 03:22:24.894603 35003 solver.cpp:239] Iteration 130790 (3.19254 iter/s, 3.1323s/10 iters), loss = 7.24966
I0523 03:22:24.894649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24966 (* 1 = 7.24966 loss)
I0523 03:22:24.907713 35003 sgd_solver.cpp:112] Iteration 130790, lr = 0.01
I0523 03:22:27.696013 35003 solver.cpp:239] Iteration 130800 (3.56984 iter/s, 2.80125s/10 iters), loss = 6.96581
I0523 03:22:27.696056 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96581 (* 1 = 6.96581 loss)
I0523 03:22:27.702327 35003 sgd_solver.cpp:112] Iteration 130800, lr = 0.01
I0523 03:22:30.406332 35003 solver.cpp:239] Iteration 130810 (3.68984 iter/s, 2.71015s/10 iters), loss = 7.36418
I0523 03:22:30.406374 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36418 (* 1 = 7.36418 loss)
I0523 03:22:30.413031 35003 sgd_solver.cpp:112] Iteration 130810, lr = 0.01
I0523 03:22:33.315587 35003 solver.cpp:239] Iteration 130820 (3.4375 iter/s, 2.90909s/10 iters), loss = 6.63152
I0523 03:22:33.315640 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63152 (* 1 = 6.63152 loss)
I0523 03:22:33.321002 35003 sgd_solver.cpp:112] Iteration 130820, lr = 0.01
I0523 03:22:34.621067 35003 solver.cpp:239] Iteration 130830 (7.66071 iter/s, 1.30536s/10 iters), loss = 6.99871
I0523 03:22:34.621115 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99871 (* 1 = 6.99871 loss)
I0523 03:22:34.632367 35003 sgd_solver.cpp:112] Iteration 130830, lr = 0.01
I0523 03:22:37.817550 35003 solver.cpp:239] Iteration 130840 (3.12862 iter/s, 3.1963s/10 iters), loss = 8.01972
I0523 03:22:37.817605 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01972 (* 1 = 8.01972 loss)
I0523 03:22:37.830566 35003 sgd_solver.cpp:112] Iteration 130840, lr = 0.01
I0523 03:22:41.467875 35003 solver.cpp:239] Iteration 130850 (2.73964 iter/s, 3.65011s/10 iters), loss = 7.12823
I0523 03:22:41.467921 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12823 (* 1 = 7.12823 loss)
I0523 03:22:41.473139 35003 sgd_solver.cpp:112] Iteration 130850, lr = 0.01
I0523 03:22:44.386461 35003 solver.cpp:239] Iteration 130860 (3.42652 iter/s, 2.91841s/10 iters), loss = 7.60678
I0523 03:22:44.386508 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60678 (* 1 = 7.60678 loss)
I0523 03:22:45.120957 35003 sgd_solver.cpp:112] Iteration 130860, lr = 0.01
I0523 03:22:49.515630 35003 solver.cpp:239] Iteration 130870 (1.94973 iter/s, 5.12891s/10 iters), loss = 8.03861
I0523 03:22:49.515678 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03861 (* 1 = 8.03861 loss)
I0523 03:22:50.133632 35003 sgd_solver.cpp:112] Iteration 130870, lr = 0.01
I0523 03:22:53.097744 35003 solver.cpp:239] Iteration 130880 (2.79185 iter/s, 3.58185s/10 iters), loss = 6.2639
I0523 03:22:53.097961 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2639 (* 1 = 6.2639 loss)
I0523 03:22:53.795886 35003 sgd_solver.cpp:112] Iteration 130880, lr = 0.01
I0523 03:22:56.425276 35003 solver.cpp:239] Iteration 130890 (3.00553 iter/s, 3.3272s/10 iters), loss = 6.75639
I0523 03:22:56.425321 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75639 (* 1 = 6.75639 loss)
I0523 03:22:56.433328 35003 sgd_solver.cpp:112] Iteration 130890, lr = 0.01
I0523 03:23:00.115309 35003 solver.cpp:239] Iteration 130900 (2.71015 iter/s, 3.68984s/10 iters), loss = 7.16958
I0523 03:23:00.115347 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16958 (* 1 = 7.16958 loss)
I0523 03:23:00.128720 35003 sgd_solver.cpp:112] Iteration 130900, lr = 0.01
I0523 03:23:03.136855 35003 solver.cpp:239] Iteration 130910 (3.30976 iter/s, 3.02137s/10 iters), loss = 7.10245
I0523 03:23:03.136916 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10245 (* 1 = 7.10245 loss)
I0523 03:23:03.830107 35003 sgd_solver.cpp:112] Iteration 130910, lr = 0.01
I0523 03:23:05.942450 35003 solver.cpp:239] Iteration 130920 (3.56453 iter/s, 2.80542s/10 iters), loss = 6.89858
I0523 03:23:05.942497 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89858 (* 1 = 6.89858 loss)
I0523 03:23:06.592195 35003 sgd_solver.cpp:112] Iteration 130920, lr = 0.01
I0523 03:23:09.068984 35003 solver.cpp:239] Iteration 130930 (3.19862 iter/s, 3.12635s/10 iters), loss = 8.14016
I0523 03:23:09.069034 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.14016 (* 1 = 8.14016 loss)
I0523 03:23:09.078663 35003 sgd_solver.cpp:112] Iteration 130930, lr = 0.01
I0523 03:23:13.345825 35003 solver.cpp:239] Iteration 130940 (2.3383 iter/s, 4.27662s/10 iters), loss = 6.31943
I0523 03:23:13.345863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31943 (* 1 = 6.31943 loss)
I0523 03:23:13.371332 35003 sgd_solver.cpp:112] Iteration 130940, lr = 0.01
I0523 03:23:15.412501 35003 solver.cpp:239] Iteration 130950 (4.83904 iter/s, 2.06653s/10 iters), loss = 6.64493
I0523 03:23:15.412545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64493 (* 1 = 6.64493 loss)
I0523 03:23:15.417158 35003 sgd_solver.cpp:112] Iteration 130950, lr = 0.01
I0523 03:23:18.301795 35003 solver.cpp:239] Iteration 130960 (3.46127 iter/s, 2.88911s/10 iters), loss = 7.14202
I0523 03:23:18.301854 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14202 (* 1 = 7.14202 loss)
I0523 03:23:18.945740 35003 sgd_solver.cpp:112] Iteration 130960, lr = 0.01
I0523 03:23:20.942255 35003 solver.cpp:239] Iteration 130970 (3.7875 iter/s, 2.64027s/10 iters), loss = 6.72232
I0523 03:23:20.942306 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72232 (* 1 = 6.72232 loss)
I0523 03:23:21.290946 35003 sgd_solver.cpp:112] Iteration 130970, lr = 0.01
I0523 03:23:24.086277 35003 solver.cpp:239] Iteration 130980 (3.18083 iter/s, 3.14384s/10 iters), loss = 7.78724
I0523 03:23:24.086472 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78724 (* 1 = 7.78724 loss)
I0523 03:23:24.810729 35003 sgd_solver.cpp:112] Iteration 130980, lr = 0.01
I0523 03:23:29.762362 35003 solver.cpp:239] Iteration 130990 (1.76191 iter/s, 5.67566s/10 iters), loss = 7.34173
I0523 03:23:29.762408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34173 (* 1 = 7.34173 loss)
I0523 03:23:30.477916 35003 sgd_solver.cpp:112] Iteration 130990, lr = 0.01
I0523 03:23:33.816131 35003 solver.cpp:239] Iteration 131000 (2.46697 iter/s, 4.05355s/10 iters), loss = 7.33982
I0523 03:23:33.816182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33982 (* 1 = 7.33982 loss)
I0523 03:23:34.540701 35003 sgd_solver.cpp:112] Iteration 131000, lr = 0.01
I0523 03:23:38.871290 35003 solver.cpp:239] Iteration 131010 (1.97828 iter/s, 5.0549s/10 iters), loss = 6.35217
I0523 03:23:38.871337 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35217 (* 1 = 6.35217 loss)
I0523 03:23:38.899554 35003 sgd_solver.cpp:112] Iteration 131010, lr = 0.01
I0523 03:23:41.418944 35003 solver.cpp:239] Iteration 131020 (3.92542 iter/s, 2.5475s/10 iters), loss = 5.47595
I0523 03:23:41.418992 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.47595 (* 1 = 5.47595 loss)
I0523 03:23:41.432245 35003 sgd_solver.cpp:112] Iteration 131020, lr = 0.01
I0523 03:23:44.911643 35003 solver.cpp:239] Iteration 131030 (2.86327 iter/s, 3.49251s/10 iters), loss = 6.61652
I0523 03:23:44.911684 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61652 (* 1 = 6.61652 loss)
I0523 03:23:44.924835 35003 sgd_solver.cpp:112] Iteration 131030, lr = 0.01
I0523 03:23:47.759374 35003 solver.cpp:239] Iteration 131040 (3.51177 iter/s, 2.84757s/10 iters), loss = 7.67699
I0523 03:23:47.759428 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67699 (* 1 = 7.67699 loss)
I0523 03:23:47.772248 35003 sgd_solver.cpp:112] Iteration 131040, lr = 0.01
I0523 03:23:52.423110 35003 solver.cpp:239] Iteration 131050 (2.14432 iter/s, 4.66349s/10 iters), loss = 7.42461
I0523 03:23:52.423151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42461 (* 1 = 7.42461 loss)
I0523 03:23:52.426911 35003 sgd_solver.cpp:112] Iteration 131050, lr = 0.01
I0523 03:23:55.111841 35003 solver.cpp:239] Iteration 131060 (3.71945 iter/s, 2.68857s/10 iters), loss = 6.90552
I0523 03:23:55.112066 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90552 (* 1 = 6.90552 loss)
I0523 03:23:55.124428 35003 sgd_solver.cpp:112] Iteration 131060, lr = 0.01
I0523 03:23:57.213665 35003 solver.cpp:239] Iteration 131070 (4.75854 iter/s, 2.10149s/10 iters), loss = 7.58858
I0523 03:23:57.213723 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58858 (* 1 = 7.58858 loss)
I0523 03:23:57.217619 35003 sgd_solver.cpp:112] Iteration 131070, lr = 0.01
I0523 03:23:59.439332 35003 solver.cpp:239] Iteration 131080 (4.49336 iter/s, 2.22551s/10 iters), loss = 7.8823
I0523 03:23:59.439369 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8823 (* 1 = 7.8823 loss)
I0523 03:23:59.444763 35003 sgd_solver.cpp:112] Iteration 131080, lr = 0.01
I0523 03:24:02.444470 35003 solver.cpp:239] Iteration 131090 (3.32782 iter/s, 3.00497s/10 iters), loss = 5.77469
I0523 03:24:02.444530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77469 (* 1 = 5.77469 loss)
I0523 03:24:02.751754 35003 sgd_solver.cpp:112] Iteration 131090, lr = 0.01
I0523 03:24:06.319701 35003 solver.cpp:239] Iteration 131100 (2.58065 iter/s, 3.87499s/10 iters), loss = 6.50802
I0523 03:24:06.319747 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50802 (* 1 = 6.50802 loss)
I0523 03:24:06.334508 35003 sgd_solver.cpp:112] Iteration 131100, lr = 0.01
I0523 03:24:08.486886 35003 solver.cpp:239] Iteration 131110 (4.6146 iter/s, 2.16703s/10 iters), loss = 6.53362
I0523 03:24:08.486933 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53362 (* 1 = 6.53362 loss)
I0523 03:24:08.525158 35003 sgd_solver.cpp:112] Iteration 131110, lr = 0.01
I0523 03:24:11.339673 35003 solver.cpp:239] Iteration 131120 (3.50556 iter/s, 2.85262s/10 iters), loss = 6.44711
I0523 03:24:11.339725 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44711 (* 1 = 6.44711 loss)
I0523 03:24:11.345088 35003 sgd_solver.cpp:112] Iteration 131120, lr = 0.01
I0523 03:24:13.894896 35003 solver.cpp:239] Iteration 131130 (3.9138 iter/s, 2.55506s/10 iters), loss = 6.25706
I0523 03:24:13.894935 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25706 (* 1 = 6.25706 loss)
I0523 03:24:13.900203 35003 sgd_solver.cpp:112] Iteration 131130, lr = 0.01
I0523 03:24:16.318871 35003 solver.cpp:239] Iteration 131140 (4.1257 iter/s, 2.42383s/10 iters), loss = 7.00696
I0523 03:24:16.318914 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00696 (* 1 = 7.00696 loss)
I0523 03:24:16.364233 35003 sgd_solver.cpp:112] Iteration 131140, lr = 0.01
I0523 03:24:17.371601 35003 solver.cpp:239] Iteration 131150 (9.5001 iter/s, 1.05262s/10 iters), loss = 7.53443
I0523 03:24:17.371664 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53443 (* 1 = 7.53443 loss)
I0523 03:24:17.376299 35003 sgd_solver.cpp:112] Iteration 131150, lr = 0.01
I0523 03:24:18.229465 35003 solver.cpp:239] Iteration 131160 (11.6585 iter/s, 0.857741s/10 iters), loss = 7.62094
I0523 03:24:18.229528 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62094 (* 1 = 7.62094 loss)
I0523 03:24:18.238021 35003 sgd_solver.cpp:112] Iteration 131160, lr = 0.01
I0523 03:24:19.065918 35003 solver.cpp:239] Iteration 131170 (11.957 iter/s, 0.836331s/10 iters), loss = 6.63848
I0523 03:24:19.065963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63848 (* 1 = 6.63848 loss)
I0523 03:24:19.069845 35003 sgd_solver.cpp:112] Iteration 131170, lr = 0.01
I0523 03:24:19.910571 35003 solver.cpp:239] Iteration 131180 (11.8404 iter/s, 0.844564s/10 iters), loss = 6.63242
I0523 03:24:19.910619 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63242 (* 1 = 6.63242 loss)
I0523 03:24:19.914130 35003 sgd_solver.cpp:112] Iteration 131180, lr = 0.01
I0523 03:24:20.881175 35003 solver.cpp:239] Iteration 131190 (10.3039 iter/s, 0.970508s/10 iters), loss = 6.78154
I0523 03:24:20.881222 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78154 (* 1 = 6.78154 loss)
I0523 03:24:21.027164 35003 sgd_solver.cpp:112] Iteration 131190, lr = 0.01
I0523 03:24:22.039019 35003 solver.cpp:239] Iteration 131200 (8.63757 iter/s, 1.15773s/10 iters), loss = 5.7312
I0523 03:24:22.039069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7312 (* 1 = 5.7312 loss)
I0523 03:24:22.047463 35003 sgd_solver.cpp:112] Iteration 131200, lr = 0.01
I0523 03:24:23.218761 35003 solver.cpp:239] Iteration 131210 (8.47718 iter/s, 1.17964s/10 iters), loss = 7.41903
I0523 03:24:23.218803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41903 (* 1 = 7.41903 loss)
I0523 03:24:23.223456 35003 sgd_solver.cpp:112] Iteration 131210, lr = 0.01
I0523 03:24:24.046785 35003 solver.cpp:239] Iteration 131220 (12.0782 iter/s, 0.827938s/10 iters), loss = 6.7619
I0523 03:24:24.046824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7619 (* 1 = 6.7619 loss)
I0523 03:24:24.053194 35003 sgd_solver.cpp:112] Iteration 131220, lr = 0.01
I0523 03:24:24.870162 35003 solver.cpp:239] Iteration 131230 (12.1465 iter/s, 0.823286s/10 iters), loss = 5.66643
I0523 03:24:24.870218 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.66643 (* 1 = 5.66643 loss)
I0523 03:24:24.879696 35003 sgd_solver.cpp:112] Iteration 131230, lr = 0.01
I0523 03:24:25.685668 35003 solver.cpp:239] Iteration 131240 (12.2638 iter/s, 0.815407s/10 iters), loss = 7.15217
I0523 03:24:25.685948 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15217 (* 1 = 7.15217 loss)
I0523 03:24:25.694525 35003 sgd_solver.cpp:112] Iteration 131240, lr = 0.01
I0523 03:24:26.501148 35003 solver.cpp:239] Iteration 131250 (12.2672 iter/s, 0.815184s/10 iters), loss = 7.12078
I0523 03:24:26.501194 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12078 (* 1 = 7.12078 loss)
I0523 03:24:26.509459 35003 sgd_solver.cpp:112] Iteration 131250, lr = 0.01
I0523 03:24:27.315754 35003 solver.cpp:239] Iteration 131260 (12.2772 iter/s, 0.814516s/10 iters), loss = 7.01292
I0523 03:24:27.315793 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01292 (* 1 = 7.01292 loss)
I0523 03:24:27.324688 35003 sgd_solver.cpp:112] Iteration 131260, lr = 0.01
I0523 03:24:28.116017 35003 solver.cpp:239] Iteration 131270 (12.4972 iter/s, 0.80018s/10 iters), loss = 7.74577
I0523 03:24:28.116075 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74577 (* 1 = 7.74577 loss)
I0523 03:24:28.127338 35003 sgd_solver.cpp:112] Iteration 131270, lr = 0.01
I0523 03:24:28.947852 35003 solver.cpp:239] Iteration 131280 (12.0231 iter/s, 0.831732s/10 iters), loss = 7.03661
I0523 03:24:28.947921 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03661 (* 1 = 7.03661 loss)
I0523 03:24:28.956784 35003 sgd_solver.cpp:112] Iteration 131280, lr = 0.01
I0523 03:24:29.813076 35003 solver.cpp:239] Iteration 131290 (11.5593 iter/s, 0.865108s/10 iters), loss = 7.43607
I0523 03:24:29.813141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43607 (* 1 = 7.43607 loss)
I0523 03:24:29.820318 35003 sgd_solver.cpp:112] Iteration 131290, lr = 0.01
I0523 03:24:30.634469 35003 solver.cpp:239] Iteration 131300 (12.176 iter/s, 0.821288s/10 iters), loss = 7.72677
I0523 03:24:30.634506 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72677 (* 1 = 7.72677 loss)
I0523 03:24:30.643060 35003 sgd_solver.cpp:112] Iteration 131300, lr = 0.01
I0523 03:24:31.439668 35003 solver.cpp:239] Iteration 131310 (12.4206 iter/s, 0.805113s/10 iters), loss = 6.47528
I0523 03:24:31.439716 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47528 (* 1 = 6.47528 loss)
I0523 03:24:31.448652 35003 sgd_solver.cpp:112] Iteration 131310, lr = 0.01
I0523 03:24:32.312289 35003 solver.cpp:239] Iteration 131320 (11.461 iter/s, 0.872522s/10 iters), loss = 7.29843
I0523 03:24:32.312326 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29843 (* 1 = 7.29843 loss)
I0523 03:24:32.321035 35003 sgd_solver.cpp:112] Iteration 131320, lr = 0.01
I0523 03:24:33.129830 35003 solver.cpp:239] Iteration 131330 (12.233 iter/s, 0.817458s/10 iters), loss = 7.37942
I0523 03:24:33.129875 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37942 (* 1 = 7.37942 loss)
I0523 03:24:33.131744 35003 sgd_solver.cpp:112] Iteration 131330, lr = 0.01
I0523 03:24:34.226186 35003 solver.cpp:239] Iteration 131340 (9.12198 iter/s, 1.09625s/10 iters), loss = 8.04307
I0523 03:24:34.226238 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04307 (* 1 = 8.04307 loss)
I0523 03:24:34.234057 35003 sgd_solver.cpp:112] Iteration 131340, lr = 0.01
I0523 03:24:35.049582 35003 solver.cpp:239] Iteration 131350 (12.1462 iter/s, 0.823304s/10 iters), loss = 6.21666
I0523 03:24:35.049629 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21666 (* 1 = 6.21666 loss)
I0523 03:24:35.058485 35003 sgd_solver.cpp:112] Iteration 131350, lr = 0.01
I0523 03:24:35.863839 35003 solver.cpp:239] Iteration 131360 (12.2825 iter/s, 0.814168s/10 iters), loss = 7.71006
I0523 03:24:35.863880 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71006 (* 1 = 7.71006 loss)
I0523 03:24:35.876363 35003 sgd_solver.cpp:112] Iteration 131360, lr = 0.01
I0523 03:24:36.721243 35003 solver.cpp:239] Iteration 131370 (11.6643 iter/s, 0.857319s/10 iters), loss = 7.10104
I0523 03:24:36.721277 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10104 (* 1 = 7.10104 loss)
I0523 03:24:36.730249 35003 sgd_solver.cpp:112] Iteration 131370, lr = 0.01
I0523 03:24:37.530243 35003 solver.cpp:239] Iteration 131380 (12.3623 iter/s, 0.808914s/10 iters), loss = 7.43722
I0523 03:24:37.530292 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43722 (* 1 = 7.43722 loss)
I0523 03:24:37.538760 35003 sgd_solver.cpp:112] Iteration 131380, lr = 0.01
I0523 03:24:38.368361 35003 solver.cpp:239] Iteration 131390 (11.9329 iter/s, 0.838017s/10 iters), loss = 7.09069
I0523 03:24:38.368417 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09069 (* 1 = 7.09069 loss)
I0523 03:24:38.371886 35003 sgd_solver.cpp:112] Iteration 131390, lr = 0.01
I0523 03:24:39.618458 35003 solver.cpp:239] Iteration 131400 (8.00011 iter/s, 1.24998s/10 iters), loss = 6.88205
I0523 03:24:39.618494 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88205 (* 1 = 6.88205 loss)
I0523 03:24:39.630023 35003 sgd_solver.cpp:112] Iteration 131400, lr = 0.01
I0523 03:24:40.970805 35003 solver.cpp:239] Iteration 131410 (7.39512 iter/s, 1.35224s/10 iters), loss = 6.20263
I0523 03:24:40.970847 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20263 (* 1 = 6.20263 loss)
I0523 03:24:40.979748 35003 sgd_solver.cpp:112] Iteration 131410, lr = 0.01
I0523 03:24:42.269165 35003 solver.cpp:239] Iteration 131420 (7.70273 iter/s, 1.29824s/10 iters), loss = 7.44376
I0523 03:24:42.269251 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44376 (* 1 = 7.44376 loss)
I0523 03:24:42.275451 35003 sgd_solver.cpp:112] Iteration 131420, lr = 0.01
I0523 03:24:43.098299 35003 solver.cpp:239] Iteration 131430 (12.0627 iter/s, 0.829s/10 iters), loss = 7.13412
I0523 03:24:43.098338 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13412 (* 1 = 7.13412 loss)
I0523 03:24:43.101800 35003 sgd_solver.cpp:112] Iteration 131430, lr = 0.01
I0523 03:24:43.912256 35003 solver.cpp:239] Iteration 131440 (12.2873 iter/s, 0.813849s/10 iters), loss = 7.10039
I0523 03:24:43.912318 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10039 (* 1 = 7.10039 loss)
I0523 03:24:43.926285 35003 sgd_solver.cpp:112] Iteration 131440, lr = 0.01
I0523 03:24:44.849958 35003 solver.cpp:239] Iteration 131450 (10.6657 iter/s, 0.937587s/10 iters), loss = 7.25738
I0523 03:24:44.850023 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25738 (* 1 = 7.25738 loss)
I0523 03:24:44.855234 35003 sgd_solver.cpp:112] Iteration 131450, lr = 0.01
I0523 03:24:46.124053 35003 solver.cpp:239] Iteration 131460 (7.84947 iter/s, 1.27397s/10 iters), loss = 7.166
I0523 03:24:46.124100 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.166 (* 1 = 7.166 loss)
I0523 03:24:46.132553 35003 sgd_solver.cpp:112] Iteration 131460, lr = 0.01
I0523 03:24:47.285830 35003 solver.cpp:239] Iteration 131470 (8.60825 iter/s, 1.16168s/10 iters), loss = 6.02469
I0523 03:24:47.285866 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02469 (* 1 = 6.02469 loss)
I0523 03:24:47.294826 35003 sgd_solver.cpp:112] Iteration 131470, lr = 0.01
I0523 03:24:48.114054 35003 solver.cpp:239] Iteration 131480 (12.0752 iter/s, 0.828145s/10 iters), loss = 6.82472
I0523 03:24:48.114095 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82472 (* 1 = 6.82472 loss)
I0523 03:24:48.122946 35003 sgd_solver.cpp:112] Iteration 131480, lr = 0.01
I0523 03:24:49.067435 35003 solver.cpp:239] Iteration 131490 (10.49 iter/s, 0.953289s/10 iters), loss = 6.30559
I0523 03:24:49.067492 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30559 (* 1 = 6.30559 loss)
I0523 03:24:49.071328 35003 sgd_solver.cpp:112] Iteration 131490, lr = 0.01
I0523 03:24:50.060606 35003 solver.cpp:239] Iteration 131500 (10.0699 iter/s, 0.993061s/10 iters), loss = 8.05807
I0523 03:24:50.060652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05807 (* 1 = 8.05807 loss)
I0523 03:24:50.320994 35003 sgd_solver.cpp:112] Iteration 131500, lr = 0.01
I0523 03:24:51.130357 35003 solver.cpp:239] Iteration 131510 (9.34888 iter/s, 1.06965s/10 iters), loss = 6.94987
I0523 03:24:51.130405 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94987 (* 1 = 6.94987 loss)
I0523 03:24:51.139173 35003 sgd_solver.cpp:112] Iteration 131510, lr = 0.01
I0523 03:24:52.437088 35003 solver.cpp:239] Iteration 131520 (7.65341 iter/s, 1.30661s/10 iters), loss = 7.32568
I0523 03:24:52.437158 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32568 (* 1 = 7.32568 loss)
I0523 03:24:52.439436 35003 sgd_solver.cpp:112] Iteration 131520, lr = 0.01
I0523 03:24:53.249027 35003 solver.cpp:239] Iteration 131530 (12.3179 iter/s, 0.811826s/10 iters), loss = 7.35602
I0523 03:24:53.249076 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35602 (* 1 = 7.35602 loss)
I0523 03:24:53.257706 35003 sgd_solver.cpp:112] Iteration 131530, lr = 0.01
I0523 03:24:54.070394 35003 solver.cpp:239] Iteration 131540 (12.1762 iter/s, 0.821276s/10 iters), loss = 7.13375
I0523 03:24:54.070436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13375 (* 1 = 7.13375 loss)
I0523 03:24:54.077038 35003 sgd_solver.cpp:112] Iteration 131540, lr = 0.01
I0523 03:24:54.918877 35003 solver.cpp:239] Iteration 131550 (11.7869 iter/s, 0.848397s/10 iters), loss = 6.02845
I0523 03:24:54.918920 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02845 (* 1 = 6.02845 loss)
I0523 03:24:54.926029 35003 sgd_solver.cpp:112] Iteration 131550, lr = 0.01
I0523 03:24:56.285562 35003 solver.cpp:239] Iteration 131560 (7.31755 iter/s, 1.36658s/10 iters), loss = 6.8558
I0523 03:24:56.285835 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8558 (* 1 = 6.8558 loss)
I0523 03:24:56.744583 35003 sgd_solver.cpp:112] Iteration 131560, lr = 0.01
I0523 03:24:57.593827 35003 solver.cpp:239] Iteration 131570 (7.6455 iter/s, 1.30796s/10 iters), loss = 6.82737
I0523 03:24:57.593899 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82737 (* 1 = 6.82737 loss)
I0523 03:24:57.602757 35003 sgd_solver.cpp:112] Iteration 131570, lr = 0.01
I0523 03:24:58.616091 35003 solver.cpp:239] Iteration 131580 (9.78335 iter/s, 1.02214s/10 iters), loss = 7.56954
I0523 03:24:58.616139 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56954 (* 1 = 7.56954 loss)
I0523 03:24:58.624533 35003 sgd_solver.cpp:112] Iteration 131580, lr = 0.01
I0523 03:24:59.911252 35003 solver.cpp:239] Iteration 131590 (7.72176 iter/s, 1.29504s/10 iters), loss = 7.09199
I0523 03:24:59.911312 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09199 (* 1 = 7.09199 loss)
I0523 03:24:59.915637 35003 sgd_solver.cpp:112] Iteration 131590, lr = 0.01
I0523 03:25:00.747274 35003 solver.cpp:239] Iteration 131600 (11.963 iter/s, 0.835911s/10 iters), loss = 6.67784
I0523 03:25:00.747329 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67784 (* 1 = 6.67784 loss)
I0523 03:25:00.759392 35003 sgd_solver.cpp:112] Iteration 131600, lr = 0.01
I0523 03:25:01.717097 35003 solver.cpp:239] Iteration 131610 (10.3124 iter/s, 0.96971s/10 iters), loss = 6.76529
I0523 03:25:01.717154 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76529 (* 1 = 6.76529 loss)
I0523 03:25:01.726222 35003 sgd_solver.cpp:112] Iteration 131610, lr = 0.01
I0523 03:25:02.562330 35003 solver.cpp:239] Iteration 131620 (11.8325 iter/s, 0.845133s/10 iters), loss = 6.31657
I0523 03:25:02.562382 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31657 (* 1 = 6.31657 loss)
I0523 03:25:02.573680 35003 sgd_solver.cpp:112] Iteration 131620, lr = 0.01
I0523 03:25:05.164296 35003 solver.cpp:239] Iteration 131630 (3.84349 iter/s, 2.6018s/10 iters), loss = 8.88168
I0523 03:25:05.164342 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.88168 (* 1 = 8.88168 loss)
I0523 03:25:05.875445 35003 sgd_solver.cpp:112] Iteration 131630, lr = 0.01
I0523 03:25:10.245332 35003 solver.cpp:239] Iteration 131640 (1.9682 iter/s, 5.08078s/10 iters), loss = 7.3716
I0523 03:25:10.245380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3716 (* 1 = 7.3716 loss)
I0523 03:25:10.288856 35003 sgd_solver.cpp:112] Iteration 131640, lr = 0.01
I0523 03:25:13.106165 35003 solver.cpp:239] Iteration 131650 (3.49569 iter/s, 2.86067s/10 iters), loss = 7.7081
I0523 03:25:13.106205 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7081 (* 1 = 7.7081 loss)
I0523 03:25:13.119786 35003 sgd_solver.cpp:112] Iteration 131650, lr = 0.01
I0523 03:25:16.248410 35003 solver.cpp:239] Iteration 131660 (3.18261 iter/s, 3.14207s/10 iters), loss = 7.32644
I0523 03:25:16.248451 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32644 (* 1 = 7.32644 loss)
I0523 03:25:16.297608 35003 sgd_solver.cpp:112] Iteration 131660, lr = 0.01
I0523 03:25:20.050303 35003 solver.cpp:239] Iteration 131670 (2.63042 iter/s, 3.80167s/10 iters), loss = 7.10259
I0523 03:25:20.050359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10259 (* 1 = 7.10259 loss)
I0523 03:25:20.785612 35003 sgd_solver.cpp:112] Iteration 131670, lr = 0.01
I0523 03:25:24.748003 35003 solver.cpp:239] Iteration 131680 (2.12881 iter/s, 4.69745s/10 iters), loss = 7.20271
I0523 03:25:24.748065 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20271 (* 1 = 7.20271 loss)
I0523 03:25:25.446586 35003 sgd_solver.cpp:112] Iteration 131680, lr = 0.01
I0523 03:25:28.088572 35003 solver.cpp:239] Iteration 131690 (2.99368 iter/s, 3.34037s/10 iters), loss = 7.19555
I0523 03:25:28.088835 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19555 (* 1 = 7.19555 loss)
I0523 03:25:28.096345 35003 sgd_solver.cpp:112] Iteration 131690, lr = 0.01
I0523 03:25:30.128486 35003 solver.cpp:239] Iteration 131700 (4.90297 iter/s, 2.03958s/10 iters), loss = 6.87828
I0523 03:25:30.128530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87828 (* 1 = 6.87828 loss)
I0523 03:25:30.189330 35003 sgd_solver.cpp:112] Iteration 131700, lr = 0.01
I0523 03:25:33.094357 35003 solver.cpp:239] Iteration 131710 (3.37188 iter/s, 2.9657s/10 iters), loss = 6.52045
I0523 03:25:33.094408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52045 (* 1 = 6.52045 loss)
I0523 03:25:33.672492 35003 sgd_solver.cpp:112] Iteration 131710, lr = 0.01
I0523 03:25:37.102905 35003 solver.cpp:239] Iteration 131720 (2.4948 iter/s, 4.00833s/10 iters), loss = 6.72045
I0523 03:25:37.102947 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72045 (* 1 = 6.72045 loss)
I0523 03:25:37.108023 35003 sgd_solver.cpp:112] Iteration 131720, lr = 0.01
I0523 03:25:41.173704 35003 solver.cpp:239] Iteration 131730 (2.45665 iter/s, 4.07058s/10 iters), loss = 6.02872
I0523 03:25:41.173741 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02872 (* 1 = 6.02872 loss)
I0523 03:25:41.181450 35003 sgd_solver.cpp:112] Iteration 131730, lr = 0.01
I0523 03:25:43.710402 35003 solver.cpp:239] Iteration 131740 (3.94237 iter/s, 2.53655s/10 iters), loss = 7.89425
I0523 03:25:43.710440 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89425 (* 1 = 7.89425 loss)
I0523 03:25:43.731622 35003 sgd_solver.cpp:112] Iteration 131740, lr = 0.01
I0523 03:25:46.605295 35003 solver.cpp:239] Iteration 131750 (3.45456 iter/s, 2.89473s/10 iters), loss = 7.89385
I0523 03:25:46.605350 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89385 (* 1 = 7.89385 loss)
I0523 03:25:47.285897 35003 sgd_solver.cpp:112] Iteration 131750, lr = 0.01
I0523 03:25:49.472947 35003 solver.cpp:239] Iteration 131760 (3.48739 iter/s, 2.86747s/10 iters), loss = 6.80543
I0523 03:25:49.472986 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80543 (* 1 = 6.80543 loss)
I0523 03:25:49.477531 35003 sgd_solver.cpp:112] Iteration 131760, lr = 0.01
I0523 03:25:53.018234 35003 solver.cpp:239] Iteration 131770 (2.8208 iter/s, 3.5451s/10 iters), loss = 8.14059
I0523 03:25:53.018285 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.14059 (* 1 = 8.14059 loss)
I0523 03:25:53.025738 35003 sgd_solver.cpp:112] Iteration 131770, lr = 0.01
I0523 03:25:55.928292 35003 solver.cpp:239] Iteration 131780 (3.43657 iter/s, 2.90988s/10 iters), loss = 6.10223
I0523 03:25:55.928349 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10223 (* 1 = 6.10223 loss)
I0523 03:25:55.935183 35003 sgd_solver.cpp:112] Iteration 131780, lr = 0.01
I0523 03:25:59.417886 35003 solver.cpp:239] Iteration 131790 (2.86583 iter/s, 3.48939s/10 iters), loss = 6.1959
I0523 03:25:59.418166 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1959 (* 1 = 6.1959 loss)
I0523 03:25:59.422740 35003 sgd_solver.cpp:112] Iteration 131790, lr = 0.01
I0523 03:26:01.491247 35003 solver.cpp:239] Iteration 131800 (4.8239 iter/s, 2.07301s/10 iters), loss = 7.78871
I0523 03:26:01.491313 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78871 (* 1 = 7.78871 loss)
I0523 03:26:01.557585 35003 sgd_solver.cpp:112] Iteration 131800, lr = 0.01
I0523 03:26:04.466558 35003 solver.cpp:239] Iteration 131810 (3.36121 iter/s, 2.97512s/10 iters), loss = 7.17948
I0523 03:26:04.466606 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17948 (* 1 = 7.17948 loss)
I0523 03:26:05.205339 35003 sgd_solver.cpp:112] Iteration 131810, lr = 0.01
I0523 03:26:08.295403 35003 solver.cpp:239] Iteration 131820 (2.6119 iter/s, 3.82862s/10 iters), loss = 6.87516
I0523 03:26:08.295464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87516 (* 1 = 6.87516 loss)
I0523 03:26:08.303283 35003 sgd_solver.cpp:112] Iteration 131820, lr = 0.01
I0523 03:26:11.508468 35003 solver.cpp:239] Iteration 131830 (3.11249 iter/s, 3.21287s/10 iters), loss = 7.28607
I0523 03:26:11.508529 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28607 (* 1 = 7.28607 loss)
I0523 03:26:12.242939 35003 sgd_solver.cpp:112] Iteration 131830, lr = 0.01
I0523 03:26:16.302606 35003 solver.cpp:239] Iteration 131840 (2.08599 iter/s, 4.79388s/10 iters), loss = 7.13199
I0523 03:26:16.302656 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13199 (* 1 = 7.13199 loss)
I0523 03:26:16.316195 35003 sgd_solver.cpp:112] Iteration 131840, lr = 0.01
I0523 03:26:19.053855 35003 solver.cpp:239] Iteration 131850 (3.63494 iter/s, 2.75108s/10 iters), loss = 8.52602
I0523 03:26:19.053902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.52602 (* 1 = 8.52602 loss)
I0523 03:26:19.062925 35003 sgd_solver.cpp:112] Iteration 131850, lr = 0.01
I0523 03:26:23.112185 35003 solver.cpp:239] Iteration 131860 (2.4642 iter/s, 4.05811s/10 iters), loss = 7.1735
I0523 03:26:23.112223 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1735 (* 1 = 7.1735 loss)
I0523 03:26:23.123379 35003 sgd_solver.cpp:112] Iteration 131860, lr = 0.01
I0523 03:26:27.382709 35003 solver.cpp:239] Iteration 131870 (2.34177 iter/s, 4.27028s/10 iters), loss = 6.77529
I0523 03:26:27.382774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77529 (* 1 = 6.77529 loss)
I0523 03:26:28.084789 35003 sgd_solver.cpp:112] Iteration 131870, lr = 0.01
I0523 03:26:31.017279 35003 solver.cpp:239] Iteration 131880 (2.75153 iter/s, 3.63435s/10 iters), loss = 6.98262
I0523 03:26:31.017508 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98262 (* 1 = 6.98262 loss)
I0523 03:26:31.718972 35003 sgd_solver.cpp:112] Iteration 131880, lr = 0.01
I0523 03:26:35.156543 35003 solver.cpp:239] Iteration 131890 (2.41611 iter/s, 4.13888s/10 iters), loss = 8.09371
I0523 03:26:35.156581 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.09371 (* 1 = 8.09371 loss)
I0523 03:26:35.163489 35003 sgd_solver.cpp:112] Iteration 131890, lr = 0.01
I0523 03:26:38.749625 35003 solver.cpp:239] Iteration 131900 (2.78327 iter/s, 3.59289s/10 iters), loss = 7.48397
I0523 03:26:38.749670 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48397 (* 1 = 7.48397 loss)
I0523 03:26:38.767916 35003 sgd_solver.cpp:112] Iteration 131900, lr = 0.01
I0523 03:26:43.010524 35003 solver.cpp:239] Iteration 131910 (2.34704 iter/s, 4.26068s/10 iters), loss = 6.65567
I0523 03:26:43.010571 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65567 (* 1 = 6.65567 loss)
I0523 03:26:43.695209 35003 sgd_solver.cpp:112] Iteration 131910, lr = 0.01
I0523 03:26:46.243988 35003 solver.cpp:239] Iteration 131920 (3.09284 iter/s, 3.23328s/10 iters), loss = 7.2146
I0523 03:26:46.244035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2146 (* 1 = 7.2146 loss)
I0523 03:26:46.265933 35003 sgd_solver.cpp:112] Iteration 131920, lr = 0.01
I0523 03:26:49.873174 35003 solver.cpp:239] Iteration 131930 (2.75559 iter/s, 3.62899s/10 iters), loss = 6.83191
I0523 03:26:49.873226 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83191 (* 1 = 6.83191 loss)
I0523 03:26:49.880813 35003 sgd_solver.cpp:112] Iteration 131930, lr = 0.01
I0523 03:26:52.698478 35003 solver.cpp:239] Iteration 131940 (3.53965 iter/s, 2.82514s/10 iters), loss = 6.65953
I0523 03:26:52.698515 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65953 (* 1 = 6.65953 loss)
I0523 03:26:52.722506 35003 sgd_solver.cpp:112] Iteration 131940, lr = 0.01
I0523 03:26:55.624953 35003 solver.cpp:239] Iteration 131950 (3.41727 iter/s, 2.92631s/10 iters), loss = 6.26681
I0523 03:26:55.625006 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26681 (* 1 = 6.26681 loss)
I0523 03:26:56.253947 35003 sgd_solver.cpp:112] Iteration 131950, lr = 0.01
I0523 03:26:59.191368 35003 solver.cpp:239] Iteration 131960 (2.8041 iter/s, 3.56621s/10 iters), loss = 5.9639
I0523 03:26:59.191416 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9639 (* 1 = 5.9639 loss)
I0523 03:26:59.933051 35003 sgd_solver.cpp:112] Iteration 131960, lr = 0.01
I0523 03:27:02.243186 35003 solver.cpp:239] Iteration 131970 (3.27693 iter/s, 3.05163s/10 iters), loss = 7.8115
I0523 03:27:02.243319 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8115 (* 1 = 7.8115 loss)
I0523 03:27:02.958034 35003 sgd_solver.cpp:112] Iteration 131970, lr = 0.01
I0523 03:27:07.460228 35003 solver.cpp:239] Iteration 131980 (1.91692 iter/s, 5.2167s/10 iters), loss = 6.67793
I0523 03:27:07.460265 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67793 (* 1 = 6.67793 loss)
I0523 03:27:07.473423 35003 sgd_solver.cpp:112] Iteration 131980, lr = 0.01
I0523 03:27:11.037976 35003 solver.cpp:239] Iteration 131990 (2.7952 iter/s, 3.57756s/10 iters), loss = 7.66413
I0523 03:27:11.038013 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66413 (* 1 = 7.66413 loss)
I0523 03:27:11.050941 35003 sgd_solver.cpp:112] Iteration 131990, lr = 0.01
I0523 03:27:14.005250 35003 solver.cpp:239] Iteration 132000 (3.37028 iter/s, 2.96711s/10 iters), loss = 6.72524
I0523 03:27:14.005287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72524 (* 1 = 6.72524 loss)
I0523 03:27:14.122342 35003 sgd_solver.cpp:112] Iteration 132000, lr = 0.01
I0523 03:27:18.056249 35003 solver.cpp:239] Iteration 132010 (2.46865 iter/s, 4.05079s/10 iters), loss = 7.19853
I0523 03:27:18.056289 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19853 (* 1 = 7.19853 loss)
I0523 03:27:18.768539 35003 sgd_solver.cpp:112] Iteration 132010, lr = 0.01
I0523 03:27:20.769320 35003 solver.cpp:239] Iteration 132020 (3.68608 iter/s, 2.71291s/10 iters), loss = 6.55329
I0523 03:27:20.769362 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55329 (* 1 = 6.55329 loss)
I0523 03:27:20.788430 35003 sgd_solver.cpp:112] Iteration 132020, lr = 0.01
I0523 03:27:23.606613 35003 solver.cpp:239] Iteration 132030 (3.52469 iter/s, 2.83713s/10 iters), loss = 6.4506
I0523 03:27:23.606664 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4506 (* 1 = 6.4506 loss)
I0523 03:27:24.314993 35003 sgd_solver.cpp:112] Iteration 132030, lr = 0.01
I0523 03:27:27.996357 35003 solver.cpp:239] Iteration 132040 (2.27816 iter/s, 4.38951s/10 iters), loss = 7.29544
I0523 03:27:27.996397 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29544 (* 1 = 7.29544 loss)
I0523 03:27:28.002703 35003 sgd_solver.cpp:112] Iteration 132040, lr = 0.01
I0523 03:27:29.974531 35003 solver.cpp:239] Iteration 132050 (5.05559 iter/s, 1.97801s/10 iters), loss = 6.02495
I0523 03:27:29.974588 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02495 (* 1 = 6.02495 loss)
I0523 03:27:29.981705 35003 sgd_solver.cpp:112] Iteration 132050, lr = 0.01
I0523 03:27:34.499248 35003 solver.cpp:239] Iteration 132060 (2.21021 iter/s, 4.52446s/10 iters), loss = 8.63714
I0523 03:27:34.499524 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.63714 (* 1 = 8.63714 loss)
I0523 03:27:34.512615 35003 sgd_solver.cpp:112] Iteration 132060, lr = 0.01
I0523 03:27:38.148123 35003 solver.cpp:239] Iteration 132070 (2.74088 iter/s, 3.64847s/10 iters), loss = 7.58093
I0523 03:27:38.148180 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58093 (* 1 = 7.58093 loss)
I0523 03:27:38.836817 35003 sgd_solver.cpp:112] Iteration 132070, lr = 0.01
I0523 03:27:41.617951 35003 solver.cpp:239] Iteration 132080 (2.88215 iter/s, 3.46963s/10 iters), loss = 7.54783
I0523 03:27:41.617998 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54783 (* 1 = 7.54783 loss)
I0523 03:27:41.631834 35003 sgd_solver.cpp:112] Iteration 132080, lr = 0.01
I0523 03:27:45.019750 35003 solver.cpp:239] Iteration 132090 (2.93978 iter/s, 3.40161s/10 iters), loss = 7.25416
I0523 03:27:45.019800 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25416 (* 1 = 7.25416 loss)
I0523 03:27:45.760545 35003 sgd_solver.cpp:112] Iteration 132090, lr = 0.01
I0523 03:27:47.837780 35003 solver.cpp:239] Iteration 132100 (3.54879 iter/s, 2.81786s/10 iters), loss = 7.08497
I0523 03:27:47.837826 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08497 (* 1 = 7.08497 loss)
I0523 03:27:47.847844 35003 sgd_solver.cpp:112] Iteration 132100, lr = 0.01
I0523 03:27:50.588464 35003 solver.cpp:239] Iteration 132110 (3.63568 iter/s, 2.75052s/10 iters), loss = 7.55414
I0523 03:27:50.588517 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55414 (* 1 = 7.55414 loss)
I0523 03:27:50.601429 35003 sgd_solver.cpp:112] Iteration 132110, lr = 0.01
I0523 03:27:54.369405 35003 solver.cpp:239] Iteration 132120 (2.64499 iter/s, 3.78073s/10 iters), loss = 7.61131
I0523 03:27:54.369464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61131 (* 1 = 7.61131 loss)
I0523 03:27:54.382371 35003 sgd_solver.cpp:112] Iteration 132120, lr = 0.01
I0523 03:27:58.704422 35003 solver.cpp:239] Iteration 132130 (2.30692 iter/s, 4.33478s/10 iters), loss = 7.16049
I0523 03:27:58.704463 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16049 (* 1 = 7.16049 loss)
I0523 03:27:58.717401 35003 sgd_solver.cpp:112] Iteration 132130, lr = 0.01
I0523 03:28:01.566923 35003 solver.cpp:239] Iteration 132140 (3.49365 iter/s, 2.86234s/10 iters), loss = 7.94921
I0523 03:28:01.566961 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94921 (* 1 = 7.94921 loss)
I0523 03:28:02.184814 35003 sgd_solver.cpp:112] Iteration 132140, lr = 0.01
I0523 03:28:05.816803 35003 solver.cpp:239] Iteration 132150 (2.35314 iter/s, 4.24965s/10 iters), loss = 7.04118
I0523 03:28:05.817060 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04118 (* 1 = 7.04118 loss)
I0523 03:28:05.828524 35003 sgd_solver.cpp:112] Iteration 132150, lr = 0.01
I0523 03:28:10.116454 35003 solver.cpp:239] Iteration 132160 (2.32842 iter/s, 4.29476s/10 iters), loss = 7.51487
I0523 03:28:10.116497 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51487 (* 1 = 7.51487 loss)
I0523 03:28:10.199287 35003 sgd_solver.cpp:112] Iteration 132160, lr = 0.01
I0523 03:28:15.078480 35003 solver.cpp:239] Iteration 132170 (2.01541 iter/s, 4.96177s/10 iters), loss = 7.70523
I0523 03:28:15.078532 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70523 (* 1 = 7.70523 loss)
I0523 03:28:15.812675 35003 sgd_solver.cpp:112] Iteration 132170, lr = 0.01
I0523 03:28:19.247226 35003 solver.cpp:239] Iteration 132180 (2.39893 iter/s, 4.16852s/10 iters), loss = 5.8375
I0523 03:28:19.247278 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8375 (* 1 = 5.8375 loss)
I0523 03:28:19.611780 35003 sgd_solver.cpp:112] Iteration 132180, lr = 0.01
I0523 03:28:22.561892 35003 solver.cpp:239] Iteration 132190 (3.01707 iter/s, 3.31447s/10 iters), loss = 7.86832
I0523 03:28:22.561951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86832 (* 1 = 7.86832 loss)
I0523 03:28:23.276741 35003 sgd_solver.cpp:112] Iteration 132190, lr = 0.01
I0523 03:28:25.163112 35003 solver.cpp:239] Iteration 132200 (3.84459 iter/s, 2.60105s/10 iters), loss = 7.33102
I0523 03:28:25.163159 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33102 (* 1 = 7.33102 loss)
I0523 03:28:25.172207 35003 sgd_solver.cpp:112] Iteration 132200, lr = 0.01
I0523 03:28:29.064189 35003 solver.cpp:239] Iteration 132210 (2.56353 iter/s, 3.90087s/10 iters), loss = 7.6778
I0523 03:28:29.064229 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6778 (* 1 = 7.6778 loss)
I0523 03:28:29.071133 35003 sgd_solver.cpp:112] Iteration 132210, lr = 0.01
I0523 03:28:32.532554 35003 solver.cpp:239] Iteration 132220 (2.88336 iter/s, 3.46818s/10 iters), loss = 6.24946
I0523 03:28:32.532608 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24946 (* 1 = 6.24946 loss)
I0523 03:28:32.556949 35003 sgd_solver.cpp:112] Iteration 132220, lr = 0.01
I0523 03:28:35.539113 35003 solver.cpp:239] Iteration 132230 (3.32626 iter/s, 3.00638s/10 iters), loss = 6.82883
I0523 03:28:35.539161 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82883 (* 1 = 6.82883 loss)
I0523 03:28:36.266842 35003 sgd_solver.cpp:112] Iteration 132230, lr = 0.01
I0523 03:28:38.508745 35003 solver.cpp:239] Iteration 132240 (3.36762 iter/s, 2.96946s/10 iters), loss = 6.90893
I0523 03:28:38.508788 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90893 (* 1 = 6.90893 loss)
I0523 03:28:38.518266 35003 sgd_solver.cpp:112] Iteration 132240, lr = 0.01
I0523 03:28:43.433492 35003 solver.cpp:239] Iteration 132250 (2.03066 iter/s, 4.9245s/10 iters), loss = 6.29578
I0523 03:28:43.433540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29578 (* 1 = 6.29578 loss)
I0523 03:28:43.452039 35003 sgd_solver.cpp:112] Iteration 132250, lr = 0.01
I0523 03:28:46.198446 35003 solver.cpp:239] Iteration 132260 (3.61692 iter/s, 2.76478s/10 iters), loss = 8.02449
I0523 03:28:46.198498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02449 (* 1 = 8.02449 loss)
I0523 03:28:46.225456 35003 sgd_solver.cpp:112] Iteration 132260, lr = 0.01
I0523 03:28:50.280045 35003 solver.cpp:239] Iteration 132270 (2.45015 iter/s, 4.08138s/10 iters), loss = 6.51471
I0523 03:28:50.280091 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51471 (* 1 = 6.51471 loss)
I0523 03:28:50.283445 35003 sgd_solver.cpp:112] Iteration 132270, lr = 0.01
I0523 03:28:53.117410 35003 solver.cpp:239] Iteration 132280 (3.52462 iter/s, 2.83719s/10 iters), loss = 5.74942
I0523 03:28:53.117457 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74942 (* 1 = 5.74942 loss)
I0523 03:28:53.128579 35003 sgd_solver.cpp:112] Iteration 132280, lr = 0.01
I0523 03:28:56.161180 35003 solver.cpp:239] Iteration 132290 (3.28559 iter/s, 3.0436s/10 iters), loss = 6.74857
I0523 03:28:56.161221 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74857 (* 1 = 6.74857 loss)
I0523 03:28:56.171311 35003 sgd_solver.cpp:112] Iteration 132290, lr = 0.01
I0523 03:29:00.072669 35003 solver.cpp:239] Iteration 132300 (2.55671 iter/s, 3.91128s/10 iters), loss = 6.75981
I0523 03:29:00.072724 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75981 (* 1 = 6.75981 loss)
I0523 03:29:00.091085 35003 sgd_solver.cpp:112] Iteration 132300, lr = 0.01
I0523 03:29:03.741436 35003 solver.cpp:239] Iteration 132310 (2.72586 iter/s, 3.66857s/10 iters), loss = 7.02526
I0523 03:29:03.741474 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02526 (* 1 = 7.02526 loss)
I0523 03:29:03.754628 35003 sgd_solver.cpp:112] Iteration 132310, lr = 0.01
I0523 03:29:08.276537 35003 solver.cpp:239] Iteration 132320 (2.20513 iter/s, 4.53488s/10 iters), loss = 6.87178
I0523 03:29:08.276796 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87178 (* 1 = 6.87178 loss)
I0523 03:29:08.992043 35003 sgd_solver.cpp:112] Iteration 132320, lr = 0.01
I0523 03:29:11.543884 35003 solver.cpp:239] Iteration 132330 (3.06093 iter/s, 3.26698s/10 iters), loss = 5.87837
I0523 03:29:11.543917 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87837 (* 1 = 5.87837 loss)
I0523 03:29:11.557224 35003 sgd_solver.cpp:112] Iteration 132330, lr = 0.01
I0523 03:29:14.474988 35003 solver.cpp:239] Iteration 132340 (3.41187 iter/s, 2.93095s/10 iters), loss = 7.34193
I0523 03:29:14.475029 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34193 (* 1 = 7.34193 loss)
I0523 03:29:14.498510 35003 sgd_solver.cpp:112] Iteration 132340, lr = 0.01
I0523 03:29:17.321666 35003 solver.cpp:239] Iteration 132350 (3.51308 iter/s, 2.84651s/10 iters), loss = 7.11206
I0523 03:29:17.321717 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11206 (* 1 = 7.11206 loss)
I0523 03:29:17.334266 35003 sgd_solver.cpp:112] Iteration 132350, lr = 0.01
I0523 03:29:21.732334 35003 solver.cpp:239] Iteration 132360 (2.26735 iter/s, 4.41044s/10 iters), loss = 7.60775
I0523 03:29:21.732372 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60775 (* 1 = 7.60775 loss)
I0523 03:29:21.745812 35003 sgd_solver.cpp:112] Iteration 132360, lr = 0.01
I0523 03:29:26.020908 35003 solver.cpp:239] Iteration 132370 (2.3319 iter/s, 4.28836s/10 iters), loss = 6.59948
I0523 03:29:26.020951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59948 (* 1 = 6.59948 loss)
I0523 03:29:26.025682 35003 sgd_solver.cpp:112] Iteration 132370, lr = 0.01
I0523 03:29:29.659850 35003 solver.cpp:239] Iteration 132380 (2.74821 iter/s, 3.63873s/10 iters), loss = 7.24352
I0523 03:29:29.659889 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24352 (* 1 = 7.24352 loss)
I0523 03:29:29.664835 35003 sgd_solver.cpp:112] Iteration 132380, lr = 0.01
I0523 03:29:34.636651 35003 solver.cpp:239] Iteration 132390 (2.00942 iter/s, 4.97655s/10 iters), loss = 7.29635
I0523 03:29:34.636693 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29635 (* 1 = 7.29635 loss)
I0523 03:29:34.650094 35003 sgd_solver.cpp:112] Iteration 132390, lr = 0.01
I0523 03:29:39.668193 35003 solver.cpp:239] Iteration 132400 (1.98756 iter/s, 5.03129s/10 iters), loss = 7.5362
I0523 03:29:39.668398 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5362 (* 1 = 7.5362 loss)
I0523 03:29:39.680084 35003 sgd_solver.cpp:112] Iteration 132400, lr = 0.01
I0523 03:29:44.246361 35003 solver.cpp:239] Iteration 132410 (2.18447 iter/s, 4.57778s/10 iters), loss = 8.01445
I0523 03:29:44.246407 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01445 (* 1 = 8.01445 loss)
I0523 03:29:44.959316 35003 sgd_solver.cpp:112] Iteration 132410, lr = 0.01
I0523 03:29:48.211501 35003 solver.cpp:239] Iteration 132420 (2.52211 iter/s, 3.96493s/10 iters), loss = 6.89196
I0523 03:29:48.211549 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89196 (* 1 = 6.89196 loss)
I0523 03:29:48.444849 35003 sgd_solver.cpp:112] Iteration 132420, lr = 0.01
I0523 03:29:52.300644 35003 solver.cpp:239] Iteration 132430 (2.44563 iter/s, 4.08892s/10 iters), loss = 6.84726
I0523 03:29:52.300688 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84726 (* 1 = 6.84726 loss)
I0523 03:29:53.016255 35003 sgd_solver.cpp:112] Iteration 132430, lr = 0.01
I0523 03:29:55.853745 35003 solver.cpp:239] Iteration 132440 (2.8146 iter/s, 3.5529s/10 iters), loss = 6.15231
I0523 03:29:55.853792 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15231 (* 1 = 6.15231 loss)
I0523 03:29:55.919883 35003 sgd_solver.cpp:112] Iteration 132440, lr = 0.01
I0523 03:29:57.996417 35003 solver.cpp:239] Iteration 132450 (4.66738 iter/s, 2.14253s/10 iters), loss = 6.07916
I0523 03:29:57.996455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07916 (* 1 = 6.07916 loss)
I0523 03:29:58.025740 35003 sgd_solver.cpp:112] Iteration 132450, lr = 0.01
I0523 03:30:02.032884 35003 solver.cpp:239] Iteration 132460 (2.47754 iter/s, 4.03626s/10 iters), loss = 7.79807
I0523 03:30:02.032923 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79807 (* 1 = 7.79807 loss)
I0523 03:30:02.037418 35003 sgd_solver.cpp:112] Iteration 132460, lr = 0.01
I0523 03:30:05.640799 35003 solver.cpp:239] Iteration 132470 (2.77183 iter/s, 3.60772s/10 iters), loss = 5.99601
I0523 03:30:05.640851 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99601 (* 1 = 5.99601 loss)
I0523 03:30:05.700925 35003 sgd_solver.cpp:112] Iteration 132470, lr = 0.01
I0523 03:30:07.703238 35003 solver.cpp:239] Iteration 132480 (4.84896 iter/s, 2.0623s/10 iters), loss = 7.19894
I0523 03:30:07.703287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19894 (* 1 = 7.19894 loss)
I0523 03:30:07.725937 35003 sgd_solver.cpp:112] Iteration 132480, lr = 0.01
I0523 03:30:12.088225 35003 solver.cpp:239] Iteration 132490 (2.28063 iter/s, 4.38476s/10 iters), loss = 6.95747
I0523 03:30:12.088357 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95747 (* 1 = 6.95747 loss)
I0523 03:30:12.101675 35003 sgd_solver.cpp:112] Iteration 132490, lr = 0.01
I0523 03:30:15.971607 35003 solver.cpp:239] Iteration 132500 (2.57527 iter/s, 3.88309s/10 iters), loss = 6.9824
I0523 03:30:15.971668 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9824 (* 1 = 6.9824 loss)
I0523 03:30:15.984522 35003 sgd_solver.cpp:112] Iteration 132500, lr = 0.01
I0523 03:30:18.153620 35003 solver.cpp:239] Iteration 132510 (4.58325 iter/s, 2.18186s/10 iters), loss = 7.38014
I0523 03:30:18.153663 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38014 (* 1 = 7.38014 loss)
I0523 03:30:18.882400 35003 sgd_solver.cpp:112] Iteration 132510, lr = 0.01
I0523 03:30:22.687988 35003 solver.cpp:239] Iteration 132520 (2.20549 iter/s, 4.53414s/10 iters), loss = 7.07502
I0523 03:30:22.688036 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07502 (* 1 = 7.07502 loss)
I0523 03:30:22.733968 35003 sgd_solver.cpp:112] Iteration 132520, lr = 0.01
I0523 03:30:25.688838 35003 solver.cpp:239] Iteration 132530 (3.33258 iter/s, 3.00068s/10 iters), loss = 7.08095
I0523 03:30:25.688879 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08095 (* 1 = 7.08095 loss)
I0523 03:30:26.417204 35003 sgd_solver.cpp:112] Iteration 132530, lr = 0.01
I0523 03:30:29.367475 35003 solver.cpp:239] Iteration 132540 (2.71854 iter/s, 3.67844s/10 iters), loss = 6.74642
I0523 03:30:29.367517 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74642 (* 1 = 6.74642 loss)
I0523 03:30:30.108301 35003 sgd_solver.cpp:112] Iteration 132540, lr = 0.01
I0523 03:30:34.018530 35003 solver.cpp:239] Iteration 132550 (2.15016 iter/s, 4.65081s/10 iters), loss = 6.65304
I0523 03:30:34.018597 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65304 (* 1 = 6.65304 loss)
I0523 03:30:34.029151 35003 sgd_solver.cpp:112] Iteration 132550, lr = 0.01
I0523 03:30:36.894855 35003 solver.cpp:239] Iteration 132560 (3.47688 iter/s, 2.87614s/10 iters), loss = 6.63876
I0523 03:30:36.894892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63876 (* 1 = 6.63876 loss)
I0523 03:30:36.902667 35003 sgd_solver.cpp:112] Iteration 132560, lr = 0.01
I0523 03:30:40.681403 35003 solver.cpp:239] Iteration 132570 (2.64107 iter/s, 3.78635s/10 iters), loss = 6.20042
I0523 03:30:40.681458 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20042 (* 1 = 6.20042 loss)
I0523 03:30:40.694502 35003 sgd_solver.cpp:112] Iteration 132570, lr = 0.01
I0523 03:30:45.655452 35003 solver.cpp:239] Iteration 132580 (2.01054 iter/s, 4.97379s/10 iters), loss = 7.4278
I0523 03:30:45.655638 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4278 (* 1 = 7.4278 loss)
I0523 03:30:45.734509 35003 sgd_solver.cpp:112] Iteration 132580, lr = 0.01
I0523 03:30:48.945039 35003 solver.cpp:239] Iteration 132590 (3.04019 iter/s, 3.28926s/10 iters), loss = 6.77953
I0523 03:30:48.945075 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77953 (* 1 = 6.77953 loss)
I0523 03:30:48.958602 35003 sgd_solver.cpp:112] Iteration 132590, lr = 0.01
I0523 03:30:51.264375 35003 solver.cpp:239] Iteration 132600 (4.31183 iter/s, 2.3192s/10 iters), loss = 6.67776
I0523 03:30:51.264422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67776 (* 1 = 6.67776 loss)
I0523 03:30:52.005015 35003 sgd_solver.cpp:112] Iteration 132600, lr = 0.01
I0523 03:30:56.370398 35003 solver.cpp:239] Iteration 132610 (1.95857 iter/s, 5.10577s/10 iters), loss = 6.73478
I0523 03:30:56.370447 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73478 (* 1 = 6.73478 loss)
I0523 03:30:56.964924 35003 sgd_solver.cpp:112] Iteration 132610, lr = 0.01
I0523 03:30:59.745350 35003 solver.cpp:239] Iteration 132620 (2.96318 iter/s, 3.37476s/10 iters), loss = 6.54504
I0523 03:30:59.745405 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54504 (* 1 = 6.54504 loss)
I0523 03:30:59.752892 35003 sgd_solver.cpp:112] Iteration 132620, lr = 0.01
I0523 03:31:02.402513 35003 solver.cpp:239] Iteration 132630 (3.76366 iter/s, 2.65699s/10 iters), loss = 7.02706
I0523 03:31:02.402556 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02706 (* 1 = 7.02706 loss)
I0523 03:31:02.415500 35003 sgd_solver.cpp:112] Iteration 132630, lr = 0.01
I0523 03:31:04.615942 35003 solver.cpp:239] Iteration 132640 (4.51816 iter/s, 2.21329s/10 iters), loss = 7.38232
I0523 03:31:04.615981 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38232 (* 1 = 7.38232 loss)
I0523 03:31:04.632856 35003 sgd_solver.cpp:112] Iteration 132640, lr = 0.01
I0523 03:31:09.617594 35003 solver.cpp:239] Iteration 132650 (1.99944 iter/s, 5.0014s/10 iters), loss = 7.81668
I0523 03:31:09.617648 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81668 (* 1 = 7.81668 loss)
I0523 03:31:10.352001 35003 sgd_solver.cpp:112] Iteration 132650, lr = 0.01
I0523 03:31:12.745801 35003 solver.cpp:239] Iteration 132660 (3.19691 iter/s, 3.12802s/10 iters), loss = 6.46029
I0523 03:31:12.745847 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46029 (* 1 = 6.46029 loss)
I0523 03:31:12.769459 35003 sgd_solver.cpp:112] Iteration 132660, lr = 0.01
I0523 03:31:17.854683 35003 solver.cpp:239] Iteration 132670 (1.95747 iter/s, 5.10862s/10 iters), loss = 7.8039
I0523 03:31:17.854837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8039 (* 1 = 7.8039 loss)
I0523 03:31:17.860810 35003 sgd_solver.cpp:112] Iteration 132670, lr = 0.01
I0523 03:31:21.145615 35003 solver.cpp:239] Iteration 132680 (3.03892 iter/s, 3.29064s/10 iters), loss = 6.87149
I0523 03:31:21.145668 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87149 (* 1 = 6.87149 loss)
I0523 03:31:21.156380 35003 sgd_solver.cpp:112] Iteration 132680, lr = 0.01
I0523 03:31:24.808449 35003 solver.cpp:239] Iteration 132690 (2.73029 iter/s, 3.66262s/10 iters), loss = 7.01807
I0523 03:31:24.808490 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01807 (* 1 = 7.01807 loss)
I0523 03:31:24.820449 35003 sgd_solver.cpp:112] Iteration 132690, lr = 0.01
I0523 03:31:28.521805 35003 solver.cpp:239] Iteration 132700 (2.69313 iter/s, 3.71315s/10 iters), loss = 7.15079
I0523 03:31:28.521857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15079 (* 1 = 7.15079 loss)
I0523 03:31:29.139086 35003 sgd_solver.cpp:112] Iteration 132700, lr = 0.01
I0523 03:31:31.909308 35003 solver.cpp:239] Iteration 132710 (2.9522 iter/s, 3.38731s/10 iters), loss = 7.07223
I0523 03:31:31.909354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07223 (* 1 = 7.07223 loss)
I0523 03:31:31.929215 35003 sgd_solver.cpp:112] Iteration 132710, lr = 0.01
I0523 03:31:33.832546 35003 solver.cpp:239] Iteration 132720 (5.19994 iter/s, 1.9231s/10 iters), loss = 6.29203
I0523 03:31:33.832607 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29203 (* 1 = 6.29203 loss)
I0523 03:31:34.541595 35003 sgd_solver.cpp:112] Iteration 132720, lr = 0.01
I0523 03:31:38.866403 35003 solver.cpp:239] Iteration 132730 (1.98666 iter/s, 5.03358s/10 iters), loss = 6.4929
I0523 03:31:38.866468 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4929 (* 1 = 6.4929 loss)
I0523 03:31:39.607177 35003 sgd_solver.cpp:112] Iteration 132730, lr = 0.01
I0523 03:31:42.588008 35003 solver.cpp:239] Iteration 132740 (2.68719 iter/s, 3.72136s/10 iters), loss = 6.67432
I0523 03:31:42.588054 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67432 (* 1 = 6.67432 loss)
I0523 03:31:43.329097 35003 sgd_solver.cpp:112] Iteration 132740, lr = 0.01
I0523 03:31:46.741832 35003 solver.cpp:239] Iteration 132750 (2.40755 iter/s, 4.1536s/10 iters), loss = 6.68962
I0523 03:31:46.741876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68962 (* 1 = 6.68962 loss)
I0523 03:31:46.754207 35003 sgd_solver.cpp:112] Iteration 132750, lr = 0.01
I0523 03:31:51.159747 35003 solver.cpp:239] Iteration 132760 (2.26363 iter/s, 4.41768s/10 iters), loss = 7.96265
I0523 03:31:51.159951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96265 (* 1 = 7.96265 loss)
I0523 03:31:51.893365 35003 sgd_solver.cpp:112] Iteration 132760, lr = 0.01
I0523 03:31:54.606869 35003 solver.cpp:239] Iteration 132770 (2.90126 iter/s, 3.44678s/10 iters), loss = 8.12387
I0523 03:31:54.606914 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.12387 (* 1 = 8.12387 loss)
I0523 03:31:54.616322 35003 sgd_solver.cpp:112] Iteration 132770, lr = 0.01
I0523 03:31:56.779583 35003 solver.cpp:239] Iteration 132780 (4.60283 iter/s, 2.17258s/10 iters), loss = 7.6226
I0523 03:31:56.779625 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6226 (* 1 = 7.6226 loss)
I0523 03:31:57.488688 35003 sgd_solver.cpp:112] Iteration 132780, lr = 0.01
I0523 03:32:01.780819 35003 solver.cpp:239] Iteration 132790 (1.9996 iter/s, 5.00099s/10 iters), loss = 6.28049
I0523 03:32:01.780858 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28049 (* 1 = 6.28049 loss)
I0523 03:32:01.799500 35003 sgd_solver.cpp:112] Iteration 132790, lr = 0.01
I0523 03:32:03.274034 35003 solver.cpp:239] Iteration 132800 (6.69744 iter/s, 1.49311s/10 iters), loss = 8.18564
I0523 03:32:03.274078 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.18564 (* 1 = 8.18564 loss)
I0523 03:32:03.287556 35003 sgd_solver.cpp:112] Iteration 132800, lr = 0.01
I0523 03:32:07.405926 35003 solver.cpp:239] Iteration 132810 (2.42032 iter/s, 4.13168s/10 iters), loss = 6.94783
I0523 03:32:07.405968 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94783 (* 1 = 6.94783 loss)
I0523 03:32:07.414309 35003 sgd_solver.cpp:112] Iteration 132810, lr = 0.01
I0523 03:32:11.581921 35003 solver.cpp:239] Iteration 132820 (2.39476 iter/s, 4.17579s/10 iters), loss = 6.69563
I0523 03:32:11.581956 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69563 (* 1 = 6.69563 loss)
I0523 03:32:11.600114 35003 sgd_solver.cpp:112] Iteration 132820, lr = 0.01
I0523 03:32:15.759392 35003 solver.cpp:239] Iteration 132830 (2.39391 iter/s, 4.17726s/10 iters), loss = 7.50363
I0523 03:32:15.759441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50363 (* 1 = 7.50363 loss)
I0523 03:32:15.773283 35003 sgd_solver.cpp:112] Iteration 132830, lr = 0.01
I0523 03:32:18.500488 35003 solver.cpp:239] Iteration 132840 (3.6484 iter/s, 2.74093s/10 iters), loss = 7.25494
I0523 03:32:18.500566 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25494 (* 1 = 7.25494 loss)
I0523 03:32:18.508213 35003 sgd_solver.cpp:112] Iteration 132840, lr = 0.01
I0523 03:32:22.150205 35003 solver.cpp:239] Iteration 132850 (2.74011 iter/s, 3.64949s/10 iters), loss = 6.9598
I0523 03:32:22.150507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9598 (* 1 = 6.9598 loss)
I0523 03:32:22.891197 35003 sgd_solver.cpp:112] Iteration 132850, lr = 0.01
I0523 03:32:25.595485 35003 solver.cpp:239] Iteration 132860 (2.90288 iter/s, 3.44486s/10 iters), loss = 6.13872
I0523 03:32:25.595538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13872 (* 1 = 6.13872 loss)
I0523 03:32:26.163610 35003 sgd_solver.cpp:112] Iteration 132860, lr = 0.01
I0523 03:32:30.232056 35003 solver.cpp:239] Iteration 132870 (2.15688 iter/s, 4.63633s/10 iters), loss = 6.35979
I0523 03:32:30.232103 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35979 (* 1 = 6.35979 loss)
I0523 03:32:30.944265 35003 sgd_solver.cpp:112] Iteration 132870, lr = 0.01
I0523 03:32:35.293073 35003 solver.cpp:239] Iteration 132880 (1.97599 iter/s, 5.06076s/10 iters), loss = 7.34548
I0523 03:32:35.293133 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34548 (* 1 = 7.34548 loss)
I0523 03:32:35.316228 35003 sgd_solver.cpp:112] Iteration 132880, lr = 0.01
I0523 03:32:37.424726 35003 solver.cpp:239] Iteration 132890 (4.69155 iter/s, 2.13149s/10 iters), loss = 8.05095
I0523 03:32:37.424773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05095 (* 1 = 8.05095 loss)
I0523 03:32:37.432502 35003 sgd_solver.cpp:112] Iteration 132890, lr = 0.01
I0523 03:32:40.263402 35003 solver.cpp:239] Iteration 132900 (3.52298 iter/s, 2.8385s/10 iters), loss = 6.9871
I0523 03:32:40.263469 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9871 (* 1 = 6.9871 loss)
I0523 03:32:40.890893 35003 sgd_solver.cpp:112] Iteration 132900, lr = 0.01
I0523 03:32:44.388619 35003 solver.cpp:239] Iteration 132910 (2.42425 iter/s, 4.12498s/10 iters), loss = 6.94823
I0523 03:32:44.388664 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94823 (* 1 = 6.94823 loss)
I0523 03:32:44.395907 35003 sgd_solver.cpp:112] Iteration 132910, lr = 0.01
I0523 03:32:47.985268 35003 solver.cpp:239] Iteration 132920 (2.78051 iter/s, 3.59646s/10 iters), loss = 7.21113
I0523 03:32:47.985311 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21113 (* 1 = 7.21113 loss)
I0523 03:32:48.657703 35003 sgd_solver.cpp:112] Iteration 132920, lr = 0.01
I0523 03:32:52.239471 35003 solver.cpp:239] Iteration 132930 (2.35074 iter/s, 4.25398s/10 iters), loss = 6.20787
I0523 03:32:52.239619 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20787 (* 1 = 6.20787 loss)
I0523 03:32:52.248186 35003 sgd_solver.cpp:112] Iteration 132930, lr = 0.01
I0523 03:32:56.405709 35003 solver.cpp:239] Iteration 132940 (2.40044 iter/s, 4.16591s/10 iters), loss = 6.51365
I0523 03:32:56.405757 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51365 (* 1 = 6.51365 loss)
I0523 03:32:57.127213 35003 sgd_solver.cpp:112] Iteration 132940, lr = 0.01
I0523 03:33:00.387982 35003 solver.cpp:239] Iteration 132950 (2.51126 iter/s, 3.98206s/10 iters), loss = 5.91462
I0523 03:33:00.388027 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91462 (* 1 = 5.91462 loss)
I0523 03:33:01.116441 35003 sgd_solver.cpp:112] Iteration 132950, lr = 0.01
I0523 03:33:03.967823 35003 solver.cpp:239] Iteration 132960 (2.79358 iter/s, 3.57964s/10 iters), loss = 7.66568
I0523 03:33:03.967878 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66568 (* 1 = 7.66568 loss)
I0523 03:33:04.652724 35003 sgd_solver.cpp:112] Iteration 132960, lr = 0.01
I0523 03:33:06.729178 35003 solver.cpp:239] Iteration 132970 (3.62164 iter/s, 2.76118s/10 iters), loss = 7.19425
I0523 03:33:06.729233 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19425 (* 1 = 7.19425 loss)
I0523 03:33:06.742383 35003 sgd_solver.cpp:112] Iteration 132970, lr = 0.01
I0523 03:33:10.483853 35003 solver.cpp:239] Iteration 132980 (2.66349 iter/s, 3.75447s/10 iters), loss = 6.408
I0523 03:33:10.483891 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.408 (* 1 = 6.408 loss)
I0523 03:33:10.497159 35003 sgd_solver.cpp:112] Iteration 132980, lr = 0.01
I0523 03:33:12.565171 35003 solver.cpp:239] Iteration 132990 (4.80494 iter/s, 2.08119s/10 iters), loss = 7.98689
I0523 03:33:12.565207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98689 (* 1 = 7.98689 loss)
I0523 03:33:12.577617 35003 sgd_solver.cpp:112] Iteration 132990, lr = 0.01
I0523 03:33:16.719444 35003 solver.cpp:239] Iteration 133000 (2.40728 iter/s, 4.15406s/10 iters), loss = 7.98733
I0523 03:33:16.719496 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98733 (* 1 = 7.98733 loss)
I0523 03:33:16.737478 35003 sgd_solver.cpp:112] Iteration 133000, lr = 0.01
I0523 03:33:20.339256 35003 solver.cpp:239] Iteration 133010 (2.76273 iter/s, 3.61961s/10 iters), loss = 6.91522
I0523 03:33:20.339299 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91522 (* 1 = 6.91522 loss)
I0523 03:33:20.352617 35003 sgd_solver.cpp:112] Iteration 133010, lr = 0.01
I0523 03:33:22.431960 35003 solver.cpp:239] Iteration 133020 (4.77881 iter/s, 2.09257s/10 iters), loss = 5.32725
I0523 03:33:22.432152 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.32725 (* 1 = 5.32725 loss)
I0523 03:33:22.445083 35003 sgd_solver.cpp:112] Iteration 133020, lr = 0.01
I0523 03:33:24.238346 35003 solver.cpp:239] Iteration 133030 (5.53676 iter/s, 1.80611s/10 iters), loss = 6.95975
I0523 03:33:24.238399 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95975 (* 1 = 6.95975 loss)
I0523 03:33:24.972754 35003 sgd_solver.cpp:112] Iteration 133030, lr = 0.01
I0523 03:33:27.715042 35003 solver.cpp:239] Iteration 133040 (2.87646 iter/s, 3.4765s/10 iters), loss = 6.33116
I0523 03:33:27.715085 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33116 (* 1 = 6.33116 loss)
I0523 03:33:27.728082 35003 sgd_solver.cpp:112] Iteration 133040, lr = 0.01
I0523 03:33:31.477814 35003 solver.cpp:239] Iteration 133050 (2.65776 iter/s, 3.76257s/10 iters), loss = 7.05589
I0523 03:33:31.477855 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05589 (* 1 = 7.05589 loss)
I0523 03:33:31.541987 35003 sgd_solver.cpp:112] Iteration 133050, lr = 0.01
I0523 03:33:35.881387 35003 solver.cpp:239] Iteration 133060 (2.27101 iter/s, 4.40333s/10 iters), loss = 7.18235
I0523 03:33:35.881448 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18235 (* 1 = 7.18235 loss)
I0523 03:33:35.887979 35003 sgd_solver.cpp:112] Iteration 133060, lr = 0.01
I0523 03:33:38.512781 35003 solver.cpp:239] Iteration 133070 (3.80052 iter/s, 2.63122s/10 iters), loss = 7.62247
I0523 03:33:38.512827 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62247 (* 1 = 7.62247 loss)
I0523 03:33:38.527750 35003 sgd_solver.cpp:112] Iteration 133070, lr = 0.01
I0523 03:33:42.120349 35003 solver.cpp:239] Iteration 133080 (2.7721 iter/s, 3.60737s/10 iters), loss = 7.7024
I0523 03:33:42.120389 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7024 (* 1 = 7.7024 loss)
I0523 03:33:42.128154 35003 sgd_solver.cpp:112] Iteration 133080, lr = 0.01
I0523 03:33:45.761376 35003 solver.cpp:239] Iteration 133090 (2.74687 iter/s, 3.6405s/10 iters), loss = 7.12143
I0523 03:33:45.761422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12143 (* 1 = 7.12143 loss)
I0523 03:33:45.799324 35003 sgd_solver.cpp:112] Iteration 133090, lr = 0.01
I0523 03:33:50.142200 35003 solver.cpp:239] Iteration 133100 (2.28279 iter/s, 4.3806s/10 iters), loss = 7.59586
I0523 03:33:50.142240 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59586 (* 1 = 7.59586 loss)
I0523 03:33:50.155690 35003 sgd_solver.cpp:112] Iteration 133100, lr = 0.01
I0523 03:33:53.800052 35003 solver.cpp:239] Iteration 133110 (2.73399 iter/s, 3.65766s/10 iters), loss = 7.81185
I0523 03:33:53.800338 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81185 (* 1 = 7.81185 loss)
I0523 03:33:53.813094 35003 sgd_solver.cpp:112] Iteration 133110, lr = 0.01
I0523 03:33:56.591171 35003 solver.cpp:239] Iteration 133120 (3.58326 iter/s, 2.79075s/10 iters), loss = 6.45524
I0523 03:33:56.591208 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45524 (* 1 = 6.45524 loss)
I0523 03:33:56.604537 35003 sgd_solver.cpp:112] Iteration 133120, lr = 0.01
I0523 03:34:01.519480 35003 solver.cpp:239] Iteration 133130 (2.02919 iter/s, 4.92807s/10 iters), loss = 6.30501
I0523 03:34:01.519524 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30501 (* 1 = 6.30501 loss)
I0523 03:34:01.533488 35003 sgd_solver.cpp:112] Iteration 133130, lr = 0.01
I0523 03:34:05.740442 35003 solver.cpp:239] Iteration 133140 (2.36925 iter/s, 4.22074s/10 iters), loss = 6.45096
I0523 03:34:05.740484 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45096 (* 1 = 6.45096 loss)
I0523 03:34:05.748785 35003 sgd_solver.cpp:112] Iteration 133140, lr = 0.01
I0523 03:34:08.516223 35003 solver.cpp:239] Iteration 133150 (3.6028 iter/s, 2.77562s/10 iters), loss = 6.93536
I0523 03:34:08.516275 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93536 (* 1 = 6.93536 loss)
I0523 03:34:08.529398 35003 sgd_solver.cpp:112] Iteration 133150, lr = 0.01
I0523 03:34:10.262738 35003 solver.cpp:239] Iteration 133160 (5.72612 iter/s, 1.74638s/10 iters), loss = 7.23219
I0523 03:34:10.262781 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23219 (* 1 = 7.23219 loss)
I0523 03:34:10.272912 35003 sgd_solver.cpp:112] Iteration 133160, lr = 0.01
I0523 03:34:13.155791 35003 solver.cpp:239] Iteration 133170 (3.45675 iter/s, 2.89289s/10 iters), loss = 6.8668
I0523 03:34:13.155835 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8668 (* 1 = 6.8668 loss)
I0523 03:34:13.162858 35003 sgd_solver.cpp:112] Iteration 133170, lr = 0.01
I0523 03:34:18.344815 35003 solver.cpp:239] Iteration 133180 (1.92724 iter/s, 5.18877s/10 iters), loss = 6.81769
I0523 03:34:18.344857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81769 (* 1 = 6.81769 loss)
I0523 03:34:18.372077 35003 sgd_solver.cpp:112] Iteration 133180, lr = 0.01
I0523 03:34:24.974499 35003 solver.cpp:239] Iteration 133190 (1.50844 iter/s, 6.62936s/10 iters), loss = 6.9578
I0523 03:34:24.974758 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9578 (* 1 = 6.9578 loss)
I0523 03:34:24.994807 35003 sgd_solver.cpp:112] Iteration 133190, lr = 0.01
I0523 03:34:29.317100 35003 solver.cpp:239] Iteration 133200 (2.30297 iter/s, 4.34222s/10 iters), loss = 7.69517
I0523 03:34:29.317143 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69517 (* 1 = 7.69517 loss)
I0523 03:34:29.330343 35003 sgd_solver.cpp:112] Iteration 133200, lr = 0.01
I0523 03:34:31.385138 35003 solver.cpp:239] Iteration 133210 (4.83582 iter/s, 2.0679s/10 iters), loss = 8.02691
I0523 03:34:31.385176 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02691 (* 1 = 8.02691 loss)
I0523 03:34:31.398075 35003 sgd_solver.cpp:112] Iteration 133210, lr = 0.01
I0523 03:34:35.715356 35003 solver.cpp:239] Iteration 133220 (2.30947 iter/s, 4.33s/10 iters), loss = 6.96627
I0523 03:34:35.715404 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96627 (* 1 = 6.96627 loss)
I0523 03:34:35.742913 35003 sgd_solver.cpp:112] Iteration 133220, lr = 0.01
I0523 03:34:38.497790 35003 solver.cpp:239] Iteration 133230 (3.59419 iter/s, 2.78227s/10 iters), loss = 7.4274
I0523 03:34:38.497838 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4274 (* 1 = 7.4274 loss)
I0523 03:34:38.515625 35003 sgd_solver.cpp:112] Iteration 133230, lr = 0.01
I0523 03:34:43.296593 35003 solver.cpp:239] Iteration 133240 (2.08396 iter/s, 4.79856s/10 iters), loss = 6.53132
I0523 03:34:43.296636 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53132 (* 1 = 6.53132 loss)
I0523 03:34:43.304275 35003 sgd_solver.cpp:112] Iteration 133240, lr = 0.01
I0523 03:34:45.504973 35003 solver.cpp:239] Iteration 133250 (4.5285 iter/s, 2.20824s/10 iters), loss = 6.57923
I0523 03:34:45.505022 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57923 (* 1 = 6.57923 loss)
I0523 03:34:46.246567 35003 sgd_solver.cpp:112] Iteration 133250, lr = 0.01
I0523 03:34:49.836767 35003 solver.cpp:239] Iteration 133260 (2.30863 iter/s, 4.33157s/10 iters), loss = 7.29146
I0523 03:34:49.836812 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29146 (* 1 = 7.29146 loss)
I0523 03:34:49.850234 35003 sgd_solver.cpp:112] Iteration 133260, lr = 0.01
I0523 03:34:53.519392 35003 solver.cpp:239] Iteration 133270 (2.71561 iter/s, 3.68242s/10 iters), loss = 7.27389
I0523 03:34:53.519445 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27389 (* 1 = 7.27389 loss)
I0523 03:34:53.534595 35003 sgd_solver.cpp:112] Iteration 133270, lr = 0.01
I0523 03:34:55.644516 35003 solver.cpp:239] Iteration 133280 (4.70593 iter/s, 2.12498s/10 iters), loss = 7.21992
I0523 03:34:55.644635 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21992 (* 1 = 7.21992 loss)
I0523 03:34:56.385017 35003 sgd_solver.cpp:112] Iteration 133280, lr = 0.01
I0523 03:34:59.224062 35003 solver.cpp:239] Iteration 133290 (2.79386 iter/s, 3.57928s/10 iters), loss = 6.59709
I0523 03:34:59.224107 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59709 (* 1 = 6.59709 loss)
I0523 03:34:59.381592 35003 sgd_solver.cpp:112] Iteration 133290, lr = 0.01
I0523 03:35:01.440917 35003 solver.cpp:239] Iteration 133300 (4.51118 iter/s, 2.21672s/10 iters), loss = 6.84684
I0523 03:35:01.440963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84684 (* 1 = 6.84684 loss)
I0523 03:35:02.147420 35003 sgd_solver.cpp:112] Iteration 133300, lr = 0.01
I0523 03:35:05.573948 35003 solver.cpp:239] Iteration 133310 (2.41966 iter/s, 4.13282s/10 iters), loss = 6.55746
I0523 03:35:05.573987 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55746 (* 1 = 6.55746 loss)
I0523 03:35:05.580001 35003 sgd_solver.cpp:112] Iteration 133310, lr = 0.01
I0523 03:35:09.668949 35003 solver.cpp:239] Iteration 133320 (2.44214 iter/s, 4.09478s/10 iters), loss = 7.07182
I0523 03:35:09.669005 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07182 (* 1 = 7.07182 loss)
I0523 03:35:10.403854 35003 sgd_solver.cpp:112] Iteration 133320, lr = 0.01
I0523 03:35:14.292179 35003 solver.cpp:239] Iteration 133330 (2.1631 iter/s, 4.62299s/10 iters), loss = 7.3542
I0523 03:35:14.292222 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3542 (* 1 = 7.3542 loss)
I0523 03:35:14.317124 35003 sgd_solver.cpp:112] Iteration 133330, lr = 0.01
I0523 03:35:18.264605 35003 solver.cpp:239] Iteration 133340 (2.51748 iter/s, 3.97222s/10 iters), loss = 7.85324
I0523 03:35:18.264643 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85324 (* 1 = 7.85324 loss)
I0523 03:35:18.277690 35003 sgd_solver.cpp:112] Iteration 133340, lr = 0.01
I0523 03:35:21.857740 35003 solver.cpp:239] Iteration 133350 (2.78323 iter/s, 3.59294s/10 iters), loss = 6.34988
I0523 03:35:21.857789 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34988 (* 1 = 6.34988 loss)
I0523 03:35:21.867657 35003 sgd_solver.cpp:112] Iteration 133350, lr = 0.01
I0523 03:35:23.957290 35003 solver.cpp:239] Iteration 133360 (4.76324 iter/s, 2.09941s/10 iters), loss = 7.1172
I0523 03:35:23.957337 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1172 (* 1 = 7.1172 loss)
I0523 03:35:24.692262 35003 sgd_solver.cpp:112] Iteration 133360, lr = 0.01
I0523 03:35:27.531059 35003 solver.cpp:239] Iteration 133370 (2.79832 iter/s, 3.57358s/10 iters), loss = 6.30817
I0523 03:35:27.531311 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30817 (* 1 = 6.30817 loss)
I0523 03:35:28.246809 35003 sgd_solver.cpp:112] Iteration 133370, lr = 0.01
I0523 03:35:31.988993 35003 solver.cpp:239] Iteration 133380 (2.2434 iter/s, 4.45752s/10 iters), loss = 7.98025
I0523 03:35:31.989038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98025 (* 1 = 7.98025 loss)
I0523 03:35:32.231806 35003 sgd_solver.cpp:112] Iteration 133380, lr = 0.01
I0523 03:35:35.066789 35003 solver.cpp:239] Iteration 133390 (3.24927 iter/s, 3.07762s/10 iters), loss = 7.07807
I0523 03:35:35.066833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07807 (* 1 = 7.07807 loss)
I0523 03:35:35.074476 35003 sgd_solver.cpp:112] Iteration 133390, lr = 0.01
I0523 03:35:39.275684 35003 solver.cpp:239] Iteration 133400 (2.37605 iter/s, 4.20867s/10 iters), loss = 7.55661
I0523 03:35:39.275733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55661 (* 1 = 7.55661 loss)
I0523 03:35:39.280189 35003 sgd_solver.cpp:112] Iteration 133400, lr = 0.01
I0523 03:35:43.453258 35003 solver.cpp:239] Iteration 133410 (2.39386 iter/s, 4.17735s/10 iters), loss = 6.86997
I0523 03:35:43.453299 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86997 (* 1 = 6.86997 loss)
I0523 03:35:44.194800 35003 sgd_solver.cpp:112] Iteration 133410, lr = 0.01
I0523 03:35:48.580935 35003 solver.cpp:239] Iteration 133420 (1.9503 iter/s, 5.12743s/10 iters), loss = 6.50823
I0523 03:35:48.580973 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50823 (* 1 = 6.50823 loss)
I0523 03:35:48.606537 35003 sgd_solver.cpp:112] Iteration 133420, lr = 0.01
I0523 03:35:51.427211 35003 solver.cpp:239] Iteration 133430 (3.51356 iter/s, 2.84611s/10 iters), loss = 6.79316
I0523 03:35:51.427266 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79316 (* 1 = 6.79316 loss)
I0523 03:35:51.615182 35003 sgd_solver.cpp:112] Iteration 133430, lr = 0.01
I0523 03:35:54.981925 35003 solver.cpp:239] Iteration 133440 (2.81333 iter/s, 3.55451s/10 iters), loss = 6.88732
I0523 03:35:54.981966 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88732 (* 1 = 6.88732 loss)
I0523 03:35:54.991293 35003 sgd_solver.cpp:112] Iteration 133440, lr = 0.01
I0523 03:35:58.927835 35003 solver.cpp:239] Iteration 133450 (2.5344 iter/s, 3.9457s/10 iters), loss = 5.91737
I0523 03:35:58.927948 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91737 (* 1 = 5.91737 loss)
I0523 03:35:58.951220 35003 sgd_solver.cpp:112] Iteration 133450, lr = 0.01
I0523 03:36:01.004909 35003 solver.cpp:239] Iteration 133460 (4.81494 iter/s, 2.07687s/10 iters), loss = 6.27283
I0523 03:36:01.004951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27283 (* 1 = 6.27283 loss)
I0523 03:36:01.011222 35003 sgd_solver.cpp:112] Iteration 133460, lr = 0.01
I0523 03:36:04.574987 35003 solver.cpp:239] Iteration 133470 (2.80122 iter/s, 3.56987s/10 iters), loss = 6.31921
I0523 03:36:04.575044 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31921 (* 1 = 6.31921 loss)
I0523 03:36:04.580976 35003 sgd_solver.cpp:112] Iteration 133470, lr = 0.01
I0523 03:36:07.941623 35003 solver.cpp:239] Iteration 133480 (2.9705 iter/s, 3.36643s/10 iters), loss = 6.85328
I0523 03:36:07.941670 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85328 (* 1 = 6.85328 loss)
I0523 03:36:07.953127 35003 sgd_solver.cpp:112] Iteration 133480, lr = 0.01
I0523 03:36:11.068706 35003 solver.cpp:239] Iteration 133490 (3.19805 iter/s, 3.1269s/10 iters), loss = 7.48739
I0523 03:36:11.068743 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48739 (* 1 = 7.48739 loss)
I0523 03:36:11.081671 35003 sgd_solver.cpp:112] Iteration 133490, lr = 0.01
I0523 03:36:13.849864 35003 solver.cpp:239] Iteration 133500 (3.59582 iter/s, 2.781s/10 iters), loss = 6.44307
I0523 03:36:13.849907 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44307 (* 1 = 6.44307 loss)
I0523 03:36:14.526980 35003 sgd_solver.cpp:112] Iteration 133500, lr = 0.01
I0523 03:36:17.318131 35003 solver.cpp:239] Iteration 133510 (2.88344 iter/s, 3.46808s/10 iters), loss = 7.3382
I0523 03:36:17.318167 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3382 (* 1 = 7.3382 loss)
I0523 03:36:17.329807 35003 sgd_solver.cpp:112] Iteration 133510, lr = 0.01
I0523 03:36:20.189739 35003 solver.cpp:239] Iteration 133520 (3.48257 iter/s, 2.87144s/10 iters), loss = 6.17868
I0523 03:36:20.189779 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17868 (* 1 = 6.17868 loss)
I0523 03:36:20.198851 35003 sgd_solver.cpp:112] Iteration 133520, lr = 0.01
I0523 03:36:22.836794 35003 solver.cpp:239] Iteration 133530 (3.778 iter/s, 2.6469s/10 iters), loss = 6.45744
I0523 03:36:22.836843 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45744 (* 1 = 6.45744 loss)
I0523 03:36:22.844214 35003 sgd_solver.cpp:112] Iteration 133530, lr = 0.01
I0523 03:36:26.210741 35003 solver.cpp:239] Iteration 133540 (2.96406 iter/s, 3.37375s/10 iters), loss = 7.37897
I0523 03:36:26.210794 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37897 (* 1 = 7.37897 loss)
I0523 03:36:26.283071 35003 sgd_solver.cpp:112] Iteration 133540, lr = 0.01
I0523 03:36:30.714479 35003 solver.cpp:239] Iteration 133550 (2.2205 iter/s, 4.5035s/10 iters), loss = 6.73472
I0523 03:36:30.714762 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73472 (* 1 = 6.73472 loss)
I0523 03:36:30.732429 35003 sgd_solver.cpp:112] Iteration 133550, lr = 0.01
I0523 03:36:33.525738 35003 solver.cpp:239] Iteration 133560 (3.5576 iter/s, 2.81088s/10 iters), loss = 7.90637
I0523 03:36:33.525777 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90637 (* 1 = 7.90637 loss)
I0523 03:36:33.531332 35003 sgd_solver.cpp:112] Iteration 133560, lr = 0.01
I0523 03:36:37.321770 35003 solver.cpp:239] Iteration 133570 (2.63448 iter/s, 3.79582s/10 iters), loss = 6.82054
I0523 03:36:37.321840 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82054 (* 1 = 6.82054 loss)
I0523 03:36:38.043287 35003 sgd_solver.cpp:112] Iteration 133570, lr = 0.01
I0523 03:36:40.759866 35003 solver.cpp:239] Iteration 133580 (2.90877 iter/s, 3.43788s/10 iters), loss = 7.1285
I0523 03:36:40.759903 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1285 (* 1 = 7.1285 loss)
I0523 03:36:41.438272 35003 sgd_solver.cpp:112] Iteration 133580, lr = 0.01
I0523 03:36:45.355541 35003 solver.cpp:239] Iteration 133590 (2.17607 iter/s, 4.59545s/10 iters), loss = 5.5978
I0523 03:36:45.355592 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.5978 (* 1 = 5.5978 loss)
I0523 03:36:46.096400 35003 sgd_solver.cpp:112] Iteration 133590, lr = 0.01
I0523 03:36:50.960711 35003 solver.cpp:239] Iteration 133600 (1.78416 iter/s, 5.60489s/10 iters), loss = 7.81301
I0523 03:36:50.960754 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81301 (* 1 = 7.81301 loss)
I0523 03:36:50.971606 35003 sgd_solver.cpp:112] Iteration 133600, lr = 0.01
I0523 03:36:54.177024 35003 solver.cpp:239] Iteration 133610 (3.10932 iter/s, 3.21613s/10 iters), loss = 7.21442
I0523 03:36:54.177070 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21442 (* 1 = 7.21442 loss)
I0523 03:36:54.190322 35003 sgd_solver.cpp:112] Iteration 133610, lr = 0.01
I0523 03:36:59.206456 35003 solver.cpp:239] Iteration 133620 (1.9884 iter/s, 5.02918s/10 iters), loss = 7.72283
I0523 03:36:59.206511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72283 (* 1 = 7.72283 loss)
I0523 03:36:59.374841 35003 sgd_solver.cpp:112] Iteration 133620, lr = 0.01
I0523 03:37:03.577531 35003 solver.cpp:239] Iteration 133630 (2.28789 iter/s, 4.37084s/10 iters), loss = 6.854
I0523 03:37:03.577725 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.854 (* 1 = 6.854 loss)
I0523 03:37:03.669984 35003 sgd_solver.cpp:112] Iteration 133630, lr = 0.01
I0523 03:37:07.996978 35003 solver.cpp:239] Iteration 133640 (2.26291 iter/s, 4.41908s/10 iters), loss = 7.60035
I0523 03:37:07.997016 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60035 (* 1 = 7.60035 loss)
I0523 03:37:08.004992 35003 sgd_solver.cpp:112] Iteration 133640, lr = 0.01
I0523 03:37:10.814842 35003 solver.cpp:239] Iteration 133650 (3.54899 iter/s, 2.8177s/10 iters), loss = 7.60574
I0523 03:37:10.814878 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60574 (* 1 = 7.60574 loss)
I0523 03:37:10.827620 35003 sgd_solver.cpp:112] Iteration 133650, lr = 0.01
I0523 03:37:13.144520 35003 solver.cpp:239] Iteration 133660 (4.29269 iter/s, 2.32954s/10 iters), loss = 7.22728
I0523 03:37:13.144562 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22728 (* 1 = 7.22728 loss)
I0523 03:37:13.872890 35003 sgd_solver.cpp:112] Iteration 133660, lr = 0.01
I0523 03:37:18.266754 35003 solver.cpp:239] Iteration 133670 (1.95237 iter/s, 5.12197s/10 iters), loss = 7.78364
I0523 03:37:18.266821 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78364 (* 1 = 7.78364 loss)
I0523 03:37:19.008332 35003 sgd_solver.cpp:112] Iteration 133670, lr = 0.01
I0523 03:37:21.878368 35003 solver.cpp:239] Iteration 133680 (2.76901 iter/s, 3.61139s/10 iters), loss = 7.49849
I0523 03:37:21.878412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49849 (* 1 = 7.49849 loss)
I0523 03:37:21.908608 35003 sgd_solver.cpp:112] Iteration 133680, lr = 0.01
I0523 03:37:26.319818 35003 solver.cpp:239] Iteration 133690 (2.25163 iter/s, 4.44123s/10 iters), loss = 7.03074
I0523 03:37:26.319857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03074 (* 1 = 7.03074 loss)
I0523 03:37:26.333160 35003 sgd_solver.cpp:112] Iteration 133690, lr = 0.01
I0523 03:37:29.104553 35003 solver.cpp:239] Iteration 133700 (3.59122 iter/s, 2.78457s/10 iters), loss = 7.81908
I0523 03:37:29.104605 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81908 (* 1 = 7.81908 loss)
I0523 03:37:29.125632 35003 sgd_solver.cpp:112] Iteration 133700, lr = 0.01
I0523 03:37:31.880669 35003 solver.cpp:239] Iteration 133710 (3.60237 iter/s, 2.77595s/10 iters), loss = 6.97283
I0523 03:37:31.880719 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97283 (* 1 = 6.97283 loss)
I0523 03:37:31.908202 35003 sgd_solver.cpp:112] Iteration 133710, lr = 0.01
I0523 03:37:34.170770 35003 solver.cpp:239] Iteration 133720 (4.36691 iter/s, 2.28995s/10 iters), loss = 6.4883
I0523 03:37:34.170929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4883 (* 1 = 6.4883 loss)
I0523 03:37:34.864564 35003 sgd_solver.cpp:112] Iteration 133720, lr = 0.01
I0523 03:37:38.539644 35003 solver.cpp:239] Iteration 133730 (2.2891 iter/s, 4.36854s/10 iters), loss = 7.31365
I0523 03:37:38.539686 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31365 (* 1 = 7.31365 loss)
I0523 03:37:38.546540 35003 sgd_solver.cpp:112] Iteration 133730, lr = 0.01
I0523 03:37:42.091724 35003 solver.cpp:239] Iteration 133740 (2.8154 iter/s, 3.55189s/10 iters), loss = 7.56918
I0523 03:37:42.091763 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56918 (* 1 = 7.56918 loss)
I0523 03:37:42.104398 35003 sgd_solver.cpp:112] Iteration 133740, lr = 0.01
I0523 03:37:44.904996 35003 solver.cpp:239] Iteration 133750 (3.55478 iter/s, 2.81311s/10 iters), loss = 6.96852
I0523 03:37:44.905063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96852 (* 1 = 6.96852 loss)
I0523 03:37:45.640063 35003 sgd_solver.cpp:112] Iteration 133750, lr = 0.01
I0523 03:37:49.250594 35003 solver.cpp:239] Iteration 133760 (2.3013 iter/s, 4.34537s/10 iters), loss = 7.82956
I0523 03:37:49.250636 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82956 (* 1 = 7.82956 loss)
I0523 03:37:49.264243 35003 sgd_solver.cpp:112] Iteration 133760, lr = 0.01
I0523 03:37:52.273885 35003 solver.cpp:239] Iteration 133770 (3.30785 iter/s, 3.02312s/10 iters), loss = 7.7847
I0523 03:37:52.273932 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7847 (* 1 = 7.7847 loss)
I0523 03:37:52.299480 35003 sgd_solver.cpp:112] Iteration 133770, lr = 0.01
I0523 03:37:55.067631 35003 solver.cpp:239] Iteration 133780 (3.57963 iter/s, 2.79358s/10 iters), loss = 7.48298
I0523 03:37:55.067675 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48298 (* 1 = 7.48298 loss)
I0523 03:37:55.075893 35003 sgd_solver.cpp:112] Iteration 133780, lr = 0.01
I0523 03:37:58.544944 35003 solver.cpp:239] Iteration 133790 (2.87594 iter/s, 3.47712s/10 iters), loss = 7.49906
I0523 03:37:58.545001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49906 (* 1 = 7.49906 loss)
I0523 03:37:59.265863 35003 sgd_solver.cpp:112] Iteration 133790, lr = 0.01
I0523 03:38:02.929149 35003 solver.cpp:239] Iteration 133800 (2.28104 iter/s, 4.38397s/10 iters), loss = 7.23384
I0523 03:38:02.929198 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23384 (* 1 = 7.23384 loss)
I0523 03:38:02.937476 35003 sgd_solver.cpp:112] Iteration 133800, lr = 0.01
I0523 03:38:05.717133 35003 solver.cpp:239] Iteration 133810 (3.58704 iter/s, 2.78781s/10 iters), loss = 6.89574
I0523 03:38:05.717406 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89574 (* 1 = 6.89574 loss)
I0523 03:38:06.426123 35003 sgd_solver.cpp:112] Iteration 133810, lr = 0.01
I0523 03:38:08.550423 35003 solver.cpp:239] Iteration 133820 (3.52991 iter/s, 2.83293s/10 iters), loss = 6.69081
I0523 03:38:08.550467 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69081 (* 1 = 6.69081 loss)
I0523 03:38:09.292078 35003 sgd_solver.cpp:112] Iteration 133820, lr = 0.01
I0523 03:38:12.047587 35003 solver.cpp:239] Iteration 133830 (2.85962 iter/s, 3.49697s/10 iters), loss = 6.15457
I0523 03:38:12.047632 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15457 (* 1 = 6.15457 loss)
I0523 03:38:12.061019 35003 sgd_solver.cpp:112] Iteration 133830, lr = 0.01
I0523 03:38:15.649511 35003 solver.cpp:239] Iteration 133840 (2.77644 iter/s, 3.60173s/10 iters), loss = 7.82574
I0523 03:38:15.649552 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82574 (* 1 = 7.82574 loss)
I0523 03:38:15.670421 35003 sgd_solver.cpp:112] Iteration 133840, lr = 0.01
I0523 03:38:20.084033 35003 solver.cpp:239] Iteration 133850 (2.25515 iter/s, 4.4343s/10 iters), loss = 7.53968
I0523 03:38:20.084081 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53968 (* 1 = 7.53968 loss)
I0523 03:38:20.090984 35003 sgd_solver.cpp:112] Iteration 133850, lr = 0.01
I0523 03:38:23.384279 35003 solver.cpp:239] Iteration 133860 (3.03025 iter/s, 3.30006s/10 iters), loss = 7.58656
I0523 03:38:23.384330 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58656 (* 1 = 7.58656 loss)
I0523 03:38:24.125239 35003 sgd_solver.cpp:112] Iteration 133860, lr = 0.01
I0523 03:38:29.068727 35003 solver.cpp:239] Iteration 133870 (1.75927 iter/s, 5.68417s/10 iters), loss = 6.92419
I0523 03:38:29.068770 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92419 (* 1 = 6.92419 loss)
I0523 03:38:29.073544 35003 sgd_solver.cpp:112] Iteration 133870, lr = 0.01
I0523 03:38:33.081441 35003 solver.cpp:239] Iteration 133880 (2.49221 iter/s, 4.0125s/10 iters), loss = 7.71972
I0523 03:38:33.081499 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71972 (* 1 = 7.71972 loss)
I0523 03:38:33.808948 35003 sgd_solver.cpp:112] Iteration 133880, lr = 0.01
I0523 03:38:36.692674 35003 solver.cpp:239] Iteration 133890 (2.7693 iter/s, 3.61103s/10 iters), loss = 6.45546
I0523 03:38:36.692889 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45546 (* 1 = 6.45546 loss)
I0523 03:38:36.698463 35003 sgd_solver.cpp:112] Iteration 133890, lr = 0.01
I0523 03:38:40.174959 35003 solver.cpp:239] Iteration 133900 (2.87196 iter/s, 3.48195s/10 iters), loss = 6.43373
I0523 03:38:40.175012 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43373 (* 1 = 6.43373 loss)
I0523 03:38:40.878506 35003 sgd_solver.cpp:112] Iteration 133900, lr = 0.01
I0523 03:38:44.238178 35003 solver.cpp:239] Iteration 133910 (2.46124 iter/s, 4.063s/10 iters), loss = 5.36636
I0523 03:38:44.238219 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.36636 (* 1 = 5.36636 loss)
I0523 03:38:44.251827 35003 sgd_solver.cpp:112] Iteration 133910, lr = 0.01
I0523 03:38:47.904455 35003 solver.cpp:239] Iteration 133920 (2.72771 iter/s, 3.66608s/10 iters), loss = 6.90225
I0523 03:38:47.904500 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90225 (* 1 = 6.90225 loss)
I0523 03:38:47.930591 35003 sgd_solver.cpp:112] Iteration 133920, lr = 0.01
I0523 03:38:51.395920 35003 solver.cpp:239] Iteration 133930 (2.86428 iter/s, 3.49127s/10 iters), loss = 7.48315
I0523 03:38:51.395964 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48315 (* 1 = 7.48315 loss)
I0523 03:38:51.415493 35003 sgd_solver.cpp:112] Iteration 133930, lr = 0.01
I0523 03:38:55.570586 35003 solver.cpp:239] Iteration 133940 (2.39553 iter/s, 4.17445s/10 iters), loss = 6.78355
I0523 03:38:55.570636 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78355 (* 1 = 6.78355 loss)
I0523 03:38:55.575392 35003 sgd_solver.cpp:112] Iteration 133940, lr = 0.01
I0523 03:38:56.870486 35003 solver.cpp:239] Iteration 133950 (7.69357 iter/s, 1.29979s/10 iters), loss = 7.62084
I0523 03:38:56.870599 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62084 (* 1 = 7.62084 loss)
I0523 03:38:56.895866 35003 sgd_solver.cpp:112] Iteration 133950, lr = 0.01
I0523 03:38:58.949101 35003 solver.cpp:239] Iteration 133960 (4.81144 iter/s, 2.07838s/10 iters), loss = 6.90677
I0523 03:38:58.949168 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90677 (* 1 = 6.90677 loss)
I0523 03:38:59.686975 35003 sgd_solver.cpp:112] Iteration 133960, lr = 0.01
I0523 03:39:02.784459 35003 solver.cpp:239] Iteration 133970 (2.60747 iter/s, 3.83513s/10 iters), loss = 7.79312
I0523 03:39:02.784514 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79312 (* 1 = 7.79312 loss)
I0523 03:39:03.523449 35003 sgd_solver.cpp:112] Iteration 133970, lr = 0.01
I0523 03:39:07.387838 35003 solver.cpp:239] Iteration 133980 (2.17243 iter/s, 4.60314s/10 iters), loss = 7.61183
I0523 03:39:07.388047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61183 (* 1 = 7.61183 loss)
I0523 03:39:08.068182 35003 sgd_solver.cpp:112] Iteration 133980, lr = 0.01
I0523 03:39:12.053973 35003 solver.cpp:239] Iteration 133990 (2.14328 iter/s, 4.66574s/10 iters), loss = 6.85584
I0523 03:39:12.054014 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85584 (* 1 = 6.85584 loss)
I0523 03:39:12.076056 35003 sgd_solver.cpp:112] Iteration 133990, lr = 0.01
I0523 03:39:14.555740 35003 solver.cpp:239] Iteration 134000 (3.99741 iter/s, 2.50162s/10 iters), loss = 6.44449
I0523 03:39:14.555783 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44449 (* 1 = 6.44449 loss)
I0523 03:39:14.561935 35003 sgd_solver.cpp:112] Iteration 134000, lr = 0.01
I0523 03:39:18.250277 35003 solver.cpp:239] Iteration 134010 (2.70684 iter/s, 3.69434s/10 iters), loss = 6.42408
I0523 03:39:18.250332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42408 (* 1 = 6.42408 loss)
I0523 03:39:18.893268 35003 sgd_solver.cpp:112] Iteration 134010, lr = 0.01
I0523 03:39:22.341161 35003 solver.cpp:239] Iteration 134020 (2.44459 iter/s, 4.09066s/10 iters), loss = 7.93043
I0523 03:39:22.341205 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93043 (* 1 = 7.93043 loss)
I0523 03:39:22.348062 35003 sgd_solver.cpp:112] Iteration 134020, lr = 0.01
I0523 03:39:26.215323 35003 solver.cpp:239] Iteration 134030 (2.58135 iter/s, 3.87395s/10 iters), loss = 5.91707
I0523 03:39:26.215363 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91707 (* 1 = 5.91707 loss)
I0523 03:39:26.220100 35003 sgd_solver.cpp:112] Iteration 134030, lr = 0.01
I0523 03:39:31.325701 35003 solver.cpp:239] Iteration 134040 (1.95692 iter/s, 5.11008s/10 iters), loss = 6.72087
I0523 03:39:31.325760 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72087 (* 1 = 6.72087 loss)
I0523 03:39:32.067143 35003 sgd_solver.cpp:112] Iteration 134040, lr = 0.01
I0523 03:39:34.959750 35003 solver.cpp:239] Iteration 134050 (2.75191 iter/s, 3.63384s/10 iters), loss = 7.33742
I0523 03:39:34.959805 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33742 (* 1 = 7.33742 loss)
I0523 03:39:34.973043 35003 sgd_solver.cpp:112] Iteration 134050, lr = 0.01
I0523 03:39:37.057416 35003 solver.cpp:239] Iteration 134060 (4.76754 iter/s, 2.09752s/10 iters), loss = 7.86475
I0523 03:39:37.057472 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86475 (* 1 = 7.86475 loss)
I0523 03:39:37.791800 35003 sgd_solver.cpp:112] Iteration 134060, lr = 0.01
I0523 03:39:40.692840 35003 solver.cpp:239] Iteration 134070 (2.75086 iter/s, 3.63522s/10 iters), loss = 8.03676
I0523 03:39:40.692886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03676 (* 1 = 8.03676 loss)
I0523 03:39:41.427968 35003 sgd_solver.cpp:112] Iteration 134070, lr = 0.01
I0523 03:39:44.304092 35003 solver.cpp:239] Iteration 134080 (2.76927 iter/s, 3.61106s/10 iters), loss = 7.10355
I0523 03:39:44.304141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10355 (* 1 = 7.10355 loss)
I0523 03:39:44.313235 35003 sgd_solver.cpp:112] Iteration 134080, lr = 0.01
I0523 03:39:50.713908 35003 solver.cpp:239] Iteration 134090 (1.56018 iter/s, 6.4095s/10 iters), loss = 6.8698
I0523 03:39:50.713953 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8698 (* 1 = 6.8698 loss)
I0523 03:39:50.737869 35003 sgd_solver.cpp:112] Iteration 134090, lr = 0.01
I0523 03:39:55.688870 35003 solver.cpp:239] Iteration 134100 (2.01017 iter/s, 4.97471s/10 iters), loss = 7.97284
I0523 03:39:55.688921 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97284 (* 1 = 7.97284 loss)
I0523 03:39:56.401681 35003 sgd_solver.cpp:112] Iteration 134100, lr = 0.01
I0523 03:39:59.215728 35003 solver.cpp:239] Iteration 134110 (2.83554 iter/s, 3.52666s/10 iters), loss = 6.97538
I0523 03:39:59.215764 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97538 (* 1 = 6.97538 loss)
I0523 03:39:59.223184 35003 sgd_solver.cpp:112] Iteration 134110, lr = 0.01
I0523 03:40:02.143359 35003 solver.cpp:239] Iteration 134120 (3.41592 iter/s, 2.92747s/10 iters), loss = 7.45876
I0523 03:40:02.143422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45876 (* 1 = 7.45876 loss)
I0523 03:40:02.877732 35003 sgd_solver.cpp:112] Iteration 134120, lr = 0.01
I0523 03:40:05.713955 35003 solver.cpp:239] Iteration 134130 (2.80082 iter/s, 3.57038s/10 iters), loss = 7.69851
I0523 03:40:05.714009 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69851 (* 1 = 7.69851 loss)
I0523 03:40:05.727238 35003 sgd_solver.cpp:112] Iteration 134130, lr = 0.01
I0523 03:40:09.690287 35003 solver.cpp:239] Iteration 134140 (2.51503 iter/s, 3.97609s/10 iters), loss = 6.76489
I0523 03:40:09.690538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76489 (* 1 = 6.76489 loss)
I0523 03:40:09.703624 35003 sgd_solver.cpp:112] Iteration 134140, lr = 0.01
I0523 03:40:11.757364 35003 solver.cpp:239] Iteration 134150 (4.83849 iter/s, 2.06676s/10 iters), loss = 6.69333
I0523 03:40:11.757410 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69333 (* 1 = 6.69333 loss)
I0523 03:40:12.485517 35003 sgd_solver.cpp:112] Iteration 134150, lr = 0.01
I0523 03:40:14.375638 35003 solver.cpp:239] Iteration 134160 (3.81955 iter/s, 2.61811s/10 iters), loss = 8.72775
I0523 03:40:14.375692 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.72775 (* 1 = 8.72775 loss)
I0523 03:40:14.386296 35003 sgd_solver.cpp:112] Iteration 134160, lr = 0.01
I0523 03:40:18.364629 35003 solver.cpp:239] Iteration 134170 (2.50704 iter/s, 3.98877s/10 iters), loss = 7.02545
I0523 03:40:18.364675 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02545 (* 1 = 7.02545 loss)
I0523 03:40:18.380591 35003 sgd_solver.cpp:112] Iteration 134170, lr = 0.01
I0523 03:40:20.541731 35003 solver.cpp:239] Iteration 134180 (4.59356 iter/s, 2.17696s/10 iters), loss = 6.45974
I0523 03:40:20.541774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45974 (* 1 = 6.45974 loss)
I0523 03:40:21.250864 35003 sgd_solver.cpp:112] Iteration 134180, lr = 0.01
I0523 03:40:23.786612 35003 solver.cpp:239] Iteration 134190 (3.08195 iter/s, 3.2447s/10 iters), loss = 6.12876
I0523 03:40:23.786665 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12876 (* 1 = 6.12876 loss)
I0523 03:40:23.791115 35003 sgd_solver.cpp:112] Iteration 134190, lr = 0.01
I0523 03:40:27.585088 35003 solver.cpp:239] Iteration 134200 (2.63278 iter/s, 3.79826s/10 iters), loss = 8.09747
I0523 03:40:27.585137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.09747 (* 1 = 8.09747 loss)
I0523 03:40:27.598179 35003 sgd_solver.cpp:112] Iteration 134200, lr = 0.01
I0523 03:40:31.256626 35003 solver.cpp:239] Iteration 134210 (2.72381 iter/s, 3.67133s/10 iters), loss = 6.63123
I0523 03:40:31.256687 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63123 (* 1 = 6.63123 loss)
I0523 03:40:31.964996 35003 sgd_solver.cpp:112] Iteration 134210, lr = 0.01
I0523 03:40:36.916674 35003 solver.cpp:239] Iteration 134220 (1.76686 iter/s, 5.65976s/10 iters), loss = 6.80083
I0523 03:40:36.916725 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80083 (* 1 = 6.80083 loss)
I0523 03:40:37.657603 35003 sgd_solver.cpp:112] Iteration 134220, lr = 0.01
I0523 03:40:41.233291 35003 solver.cpp:239] Iteration 134230 (2.31675 iter/s, 4.31639s/10 iters), loss = 6.91336
I0523 03:40:41.233600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91336 (* 1 = 6.91336 loss)
I0523 03:40:41.246453 35003 sgd_solver.cpp:112] Iteration 134230, lr = 0.01
I0523 03:40:44.988174 35003 solver.cpp:239] Iteration 134240 (2.6635 iter/s, 3.75445s/10 iters), loss = 7.19543
I0523 03:40:44.988226 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19543 (* 1 = 7.19543 loss)
I0523 03:40:45.684000 35003 sgd_solver.cpp:112] Iteration 134240, lr = 0.01
I0523 03:40:50.007083 35003 solver.cpp:239] Iteration 134250 (1.99257 iter/s, 5.01865s/10 iters), loss = 6.86727
I0523 03:40:50.007125 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86727 (* 1 = 6.86727 loss)
I0523 03:40:50.030740 35003 sgd_solver.cpp:112] Iteration 134250, lr = 0.01
I0523 03:40:52.964689 35003 solver.cpp:239] Iteration 134260 (3.3813 iter/s, 2.95744s/10 iters), loss = 7.17643
I0523 03:40:52.964730 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17643 (* 1 = 7.17643 loss)
I0523 03:40:52.970165 35003 sgd_solver.cpp:112] Iteration 134260, lr = 0.01
I0523 03:40:55.090921 35003 solver.cpp:239] Iteration 134270 (4.70348 iter/s, 2.12609s/10 iters), loss = 7.17938
I0523 03:40:55.090973 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17938 (* 1 = 7.17938 loss)
I0523 03:40:55.096071 35003 sgd_solver.cpp:112] Iteration 134270, lr = 0.01
I0523 03:41:00.262817 35003 solver.cpp:239] Iteration 134280 (1.93363 iter/s, 5.17163s/10 iters), loss = 7.0725
I0523 03:41:00.262884 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0725 (* 1 = 7.0725 loss)
I0523 03:41:00.901088 35003 sgd_solver.cpp:112] Iteration 134280, lr = 0.01
I0523 03:41:05.317189 35003 solver.cpp:239] Iteration 134290 (1.97859 iter/s, 5.05411s/10 iters), loss = 6.17233
I0523 03:41:05.317237 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17233 (* 1 = 6.17233 loss)
I0523 03:41:06.051517 35003 sgd_solver.cpp:112] Iteration 134290, lr = 0.01
I0523 03:41:08.479471 35003 solver.cpp:239] Iteration 134300 (3.16245 iter/s, 3.1621s/10 iters), loss = 7.24613
I0523 03:41:08.479523 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24613 (* 1 = 7.24613 loss)
I0523 03:41:08.485450 35003 sgd_solver.cpp:112] Iteration 134300, lr = 0.01
I0523 03:41:11.375385 35003 solver.cpp:239] Iteration 134310 (3.45336 iter/s, 2.89573s/10 iters), loss = 6.7431
I0523 03:41:11.375674 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7431 (* 1 = 6.7431 loss)
I0523 03:41:11.380947 35003 sgd_solver.cpp:112] Iteration 134310, lr = 0.01
I0523 03:41:14.882639 35003 solver.cpp:239] Iteration 134320 (2.85158 iter/s, 3.50682s/10 iters), loss = 8.25418
I0523 03:41:14.882685 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.25418 (* 1 = 8.25418 loss)
I0523 03:41:14.893016 35003 sgd_solver.cpp:112] Iteration 134320, lr = 0.01
I0523 03:41:16.871628 35003 solver.cpp:239] Iteration 134330 (5.02802 iter/s, 1.98885s/10 iters), loss = 6.82959
I0523 03:41:16.871673 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82959 (* 1 = 6.82959 loss)
I0523 03:41:17.602068 35003 sgd_solver.cpp:112] Iteration 134330, lr = 0.01
I0523 03:41:19.124065 35003 solver.cpp:239] Iteration 134340 (4.43992 iter/s, 2.25229s/10 iters), loss = 7.00859
I0523 03:41:19.124109 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00859 (* 1 = 7.00859 loss)
I0523 03:41:19.137318 35003 sgd_solver.cpp:112] Iteration 134340, lr = 0.01
I0523 03:41:23.340024 35003 solver.cpp:239] Iteration 134350 (2.37206 iter/s, 4.21574s/10 iters), loss = 6.66485
I0523 03:41:23.340070 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66485 (* 1 = 6.66485 loss)
I0523 03:41:23.353205 35003 sgd_solver.cpp:112] Iteration 134350, lr = 0.01
I0523 03:41:25.649081 35003 solver.cpp:239] Iteration 134360 (4.33105 iter/s, 2.30891s/10 iters), loss = 7.14615
I0523 03:41:25.649129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14615 (* 1 = 7.14615 loss)
I0523 03:41:25.656095 35003 sgd_solver.cpp:112] Iteration 134360, lr = 0.01
I0523 03:41:29.152256 35003 solver.cpp:239] Iteration 134370 (2.85472 iter/s, 3.50297s/10 iters), loss = 7.54704
I0523 03:41:29.152317 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54704 (* 1 = 7.54704 loss)
I0523 03:41:29.873818 35003 sgd_solver.cpp:112] Iteration 134370, lr = 0.01
I0523 03:41:32.795419 35003 solver.cpp:239] Iteration 134380 (2.74503 iter/s, 3.64295s/10 iters), loss = 7.39886
I0523 03:41:32.795480 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39886 (* 1 = 7.39886 loss)
I0523 03:41:33.510164 35003 sgd_solver.cpp:112] Iteration 134380, lr = 0.01
I0523 03:41:36.465963 35003 solver.cpp:239] Iteration 134390 (2.72455 iter/s, 3.67033s/10 iters), loss = 6.59038
I0523 03:41:36.466006 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59038 (* 1 = 6.59038 loss)
I0523 03:41:36.473325 35003 sgd_solver.cpp:112] Iteration 134390, lr = 0.01
I0523 03:41:40.190147 35003 solver.cpp:239] Iteration 134400 (2.6853 iter/s, 3.72398s/10 iters), loss = 6.98986
I0523 03:41:40.190203 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98986 (* 1 = 6.98986 loss)
I0523 03:41:40.202497 35003 sgd_solver.cpp:112] Iteration 134400, lr = 0.01
I0523 03:41:42.187866 35003 solver.cpp:239] Iteration 134410 (5.0061 iter/s, 1.99756s/10 iters), loss = 7.19798
I0523 03:41:42.188127 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19798 (* 1 = 7.19798 loss)
I0523 03:41:42.194527 35003 sgd_solver.cpp:112] Iteration 134410, lr = 0.01
I0523 03:41:46.373967 35003 solver.cpp:239] Iteration 134420 (2.38909 iter/s, 4.18569s/10 iters), loss = 8.08378
I0523 03:41:46.374035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08378 (* 1 = 8.08378 loss)
I0523 03:41:47.076658 35003 sgd_solver.cpp:112] Iteration 134420, lr = 0.01
I0523 03:41:49.846289 35003 solver.cpp:239] Iteration 134430 (2.88009 iter/s, 3.47211s/10 iters), loss = 8.43239
I0523 03:41:49.846328 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.43239 (* 1 = 8.43239 loss)
I0523 03:41:49.859393 35003 sgd_solver.cpp:112] Iteration 134430, lr = 0.01
I0523 03:41:52.498090 35003 solver.cpp:239] Iteration 134440 (3.77124 iter/s, 2.65165s/10 iters), loss = 6.92216
I0523 03:41:52.498138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92216 (* 1 = 6.92216 loss)
I0523 03:41:53.006536 35003 sgd_solver.cpp:112] Iteration 134440, lr = 0.01
I0523 03:41:55.336232 35003 solver.cpp:239] Iteration 134450 (3.52364 iter/s, 2.83797s/10 iters), loss = 6.96368
I0523 03:41:55.336272 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96368 (* 1 = 6.96368 loss)
I0523 03:41:55.347848 35003 sgd_solver.cpp:112] Iteration 134450, lr = 0.01
I0523 03:41:57.286737 35003 solver.cpp:239] Iteration 134460 (5.12721 iter/s, 1.95038s/10 iters), loss = 7.51753
I0523 03:41:57.286773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51753 (* 1 = 7.51753 loss)
I0523 03:41:57.300580 35003 sgd_solver.cpp:112] Iteration 134460, lr = 0.01
I0523 03:42:00.157193 35003 solver.cpp:239] Iteration 134470 (3.48396 iter/s, 2.8703s/10 iters), loss = 7.75625
I0523 03:42:00.157236 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75625 (* 1 = 7.75625 loss)
I0523 03:42:00.170018 35003 sgd_solver.cpp:112] Iteration 134470, lr = 0.01
I0523 03:42:02.318856 35003 solver.cpp:239] Iteration 134480 (4.62638 iter/s, 2.16152s/10 iters), loss = 6.50595
I0523 03:42:02.318909 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50595 (* 1 = 6.50595 loss)
I0523 03:42:03.053099 35003 sgd_solver.cpp:112] Iteration 134480, lr = 0.01
I0523 03:42:06.989154 35003 solver.cpp:239] Iteration 134490 (2.1413 iter/s, 4.67005s/10 iters), loss = 6.23537
I0523 03:42:06.989203 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23537 (* 1 = 6.23537 loss)
I0523 03:42:07.001984 35003 sgd_solver.cpp:112] Iteration 134490, lr = 0.01
I0523 03:42:11.205729 35003 solver.cpp:239] Iteration 134500 (2.37172 iter/s, 4.21635s/10 iters), loss = 7.27011
I0523 03:42:11.205775 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27011 (* 1 = 7.27011 loss)
I0523 03:42:11.211521 35003 sgd_solver.cpp:112] Iteration 134500, lr = 0.01
I0523 03:42:14.909925 35003 solver.cpp:239] Iteration 134510 (2.69978 iter/s, 3.704s/10 iters), loss = 7.11842
I0523 03:42:14.910215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11842 (* 1 = 7.11842 loss)
I0523 03:42:14.920569 35003 sgd_solver.cpp:112] Iteration 134510, lr = 0.01
I0523 03:42:17.654193 35003 solver.cpp:239] Iteration 134520 (3.64447 iter/s, 2.74388s/10 iters), loss = 7.22188
I0523 03:42:17.654232 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22188 (* 1 = 7.22188 loss)
I0523 03:42:18.341706 35003 sgd_solver.cpp:112] Iteration 134520, lr = 0.01
I0523 03:42:21.169509 35003 solver.cpp:239] Iteration 134530 (2.84484 iter/s, 3.51513s/10 iters), loss = 6.0956
I0523 03:42:21.169548 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0956 (* 1 = 6.0956 loss)
I0523 03:42:21.194402 35003 sgd_solver.cpp:112] Iteration 134530, lr = 0.01
I0523 03:42:24.045007 35003 solver.cpp:239] Iteration 134540 (3.47786 iter/s, 2.87533s/10 iters), loss = 6.77198
I0523 03:42:24.045056 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77198 (* 1 = 6.77198 loss)
I0523 03:42:24.785797 35003 sgd_solver.cpp:112] Iteration 134540, lr = 0.01
I0523 03:42:27.683578 35003 solver.cpp:239] Iteration 134550 (2.74849 iter/s, 3.63837s/10 iters), loss = 6.79906
I0523 03:42:27.683629 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79906 (* 1 = 6.79906 loss)
I0523 03:42:27.697046 35003 sgd_solver.cpp:112] Iteration 134550, lr = 0.01
I0523 03:42:30.323164 35003 solver.cpp:239] Iteration 134560 (3.7887 iter/s, 2.63943s/10 iters), loss = 6.7614
I0523 03:42:30.323210 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7614 (* 1 = 6.7614 loss)
I0523 03:42:30.329807 35003 sgd_solver.cpp:112] Iteration 134560, lr = 0.01
I0523 03:42:33.825347 35003 solver.cpp:239] Iteration 134570 (2.85552 iter/s, 3.50199s/10 iters), loss = 7.96258
I0523 03:42:33.825400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96258 (* 1 = 7.96258 loss)
I0523 03:42:34.559605 35003 sgd_solver.cpp:112] Iteration 134570, lr = 0.01
I0523 03:42:36.618772 35003 solver.cpp:239] Iteration 134580 (3.58006 iter/s, 2.79325s/10 iters), loss = 5.75409
I0523 03:42:36.618831 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.75409 (* 1 = 5.75409 loss)
I0523 03:42:36.626479 35003 sgd_solver.cpp:112] Iteration 134580, lr = 0.01
I0523 03:42:39.641775 35003 solver.cpp:239] Iteration 134590 (3.30817 iter/s, 3.02282s/10 iters), loss = 6.27396
I0523 03:42:39.641824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27396 (* 1 = 6.27396 loss)
I0523 03:42:40.357347 35003 sgd_solver.cpp:112] Iteration 134590, lr = 0.01
I0523 03:42:43.991186 35003 solver.cpp:239] Iteration 134600 (2.29928 iter/s, 4.34918s/10 iters), loss = 7.55241
I0523 03:42:43.991232 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55241 (* 1 = 7.55241 loss)
I0523 03:42:44.024654 35003 sgd_solver.cpp:112] Iteration 134600, lr = 0.01
I0523 03:42:46.562774 35003 solver.cpp:239] Iteration 134610 (3.88889 iter/s, 2.57143s/10 iters), loss = 7.65535
I0523 03:42:46.562939 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65535 (* 1 = 7.65535 loss)
I0523 03:42:46.587815 35003 sgd_solver.cpp:112] Iteration 134610, lr = 0.01
I0523 03:42:48.647891 35003 solver.cpp:239] Iteration 134620 (4.79644 iter/s, 2.08488s/10 iters), loss = 6.8511
I0523 03:42:48.647935 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8511 (* 1 = 6.8511 loss)
I0523 03:42:48.655773 35003 sgd_solver.cpp:112] Iteration 134620, lr = 0.01
I0523 03:42:51.543238 35003 solver.cpp:239] Iteration 134630 (3.45401 iter/s, 2.89518s/10 iters), loss = 6.06083
I0523 03:42:51.543282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06083 (* 1 = 6.06083 loss)
I0523 03:42:52.265226 35003 sgd_solver.cpp:112] Iteration 134630, lr = 0.01
I0523 03:42:55.682893 35003 solver.cpp:239] Iteration 134640 (2.41578 iter/s, 4.13944s/10 iters), loss = 8.0374
I0523 03:42:55.682934 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0374 (* 1 = 8.0374 loss)
I0523 03:42:55.696377 35003 sgd_solver.cpp:112] Iteration 134640, lr = 0.01
I0523 03:42:59.119791 35003 solver.cpp:239] Iteration 134650 (2.90976 iter/s, 3.43671s/10 iters), loss = 6.78151
I0523 03:42:59.119829 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78151 (* 1 = 6.78151 loss)
I0523 03:42:59.142072 35003 sgd_solver.cpp:112] Iteration 134650, lr = 0.01
I0523 03:43:01.762985 35003 solver.cpp:239] Iteration 134660 (3.78353 iter/s, 2.64304s/10 iters), loss = 6.37454
I0523 03:43:01.763027 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37454 (* 1 = 6.37454 loss)
I0523 03:43:02.478523 35003 sgd_solver.cpp:112] Iteration 134660, lr = 0.01
I0523 03:43:05.887192 35003 solver.cpp:239] Iteration 134670 (2.42483 iter/s, 4.12399s/10 iters), loss = 7.55227
I0523 03:43:05.887238 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55227 (* 1 = 7.55227 loss)
I0523 03:43:06.541471 35003 sgd_solver.cpp:112] Iteration 134670, lr = 0.01
I0523 03:43:09.465425 35003 solver.cpp:239] Iteration 134680 (2.79483 iter/s, 3.57804s/10 iters), loss = 6.65629
I0523 03:43:09.465471 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65629 (* 1 = 6.65629 loss)
I0523 03:43:09.472688 35003 sgd_solver.cpp:112] Iteration 134680, lr = 0.01
I0523 03:43:12.267547 35003 solver.cpp:239] Iteration 134690 (3.56893 iter/s, 2.80196s/10 iters), loss = 6.45604
I0523 03:43:12.267587 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45604 (* 1 = 6.45604 loss)
I0523 03:43:12.294198 35003 sgd_solver.cpp:112] Iteration 134690, lr = 0.01
I0523 03:43:15.857453 35003 solver.cpp:239] Iteration 134700 (2.78574 iter/s, 3.58971s/10 iters), loss = 6.01699
I0523 03:43:15.857498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01699 (* 1 = 6.01699 loss)
I0523 03:43:15.863214 35003 sgd_solver.cpp:112] Iteration 134700, lr = 0.01
I0523 03:43:17.940696 35003 solver.cpp:239] Iteration 134710 (4.80054 iter/s, 2.0831s/10 iters), loss = 7.86725
I0523 03:43:17.940928 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86725 (* 1 = 7.86725 loss)
I0523 03:43:17.949450 35003 sgd_solver.cpp:112] Iteration 134710, lr = 0.01
I0523 03:43:22.035071 35003 solver.cpp:239] Iteration 134720 (2.4426 iter/s, 4.094s/10 iters), loss = 6.6597
I0523 03:43:22.035117 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6597 (* 1 = 6.6597 loss)
I0523 03:43:22.043548 35003 sgd_solver.cpp:112] Iteration 134720, lr = 0.01
I0523 03:43:24.087923 35003 solver.cpp:239] Iteration 134730 (4.8716 iter/s, 2.05271s/10 iters), loss = 6.41192
I0523 03:43:24.087968 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41192 (* 1 = 6.41192 loss)
I0523 03:43:24.823254 35003 sgd_solver.cpp:112] Iteration 134730, lr = 0.01
I0523 03:43:26.909762 35003 solver.cpp:239] Iteration 134740 (3.54399 iter/s, 2.82168s/10 iters), loss = 5.93996
I0523 03:43:26.909807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93996 (* 1 = 5.93996 loss)
I0523 03:43:26.918560 35003 sgd_solver.cpp:112] Iteration 134740, lr = 0.01
I0523 03:43:29.805248 35003 solver.cpp:239] Iteration 134750 (3.45386 iter/s, 2.89531s/10 iters), loss = 6.82906
I0523 03:43:29.805310 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82906 (* 1 = 6.82906 loss)
I0523 03:43:29.807641 35003 sgd_solver.cpp:112] Iteration 134750, lr = 0.01
I0523 03:43:31.812441 35003 solver.cpp:239] Iteration 134760 (4.98246 iter/s, 2.00704s/10 iters), loss = 7.21594
I0523 03:43:31.812486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21594 (* 1 = 7.21594 loss)
I0523 03:43:31.820523 35003 sgd_solver.cpp:112] Iteration 134760, lr = 0.01
I0523 03:43:34.821980 35003 solver.cpp:239] Iteration 134770 (3.32297 iter/s, 3.00936s/10 iters), loss = 6.36133
I0523 03:43:34.822021 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36133 (* 1 = 6.36133 loss)
I0523 03:43:34.831938 35003 sgd_solver.cpp:112] Iteration 134770, lr = 0.01
I0523 03:43:37.639775 35003 solver.cpp:239] Iteration 134780 (3.54908 iter/s, 2.81763s/10 iters), loss = 6.74878
I0523 03:43:37.639812 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74878 (* 1 = 6.74878 loss)
I0523 03:43:37.651859 35003 sgd_solver.cpp:112] Iteration 134780, lr = 0.01
I0523 03:43:40.514158 35003 solver.cpp:239] Iteration 134790 (3.47922 iter/s, 2.87421s/10 iters), loss = 7.44964
I0523 03:43:40.514226 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44964 (* 1 = 7.44964 loss)
I0523 03:43:40.516885 35003 sgd_solver.cpp:112] Iteration 134790, lr = 0.01
I0523 03:43:44.772739 35003 solver.cpp:239] Iteration 134800 (2.34834 iter/s, 4.25834s/10 iters), loss = 8.07109
I0523 03:43:44.772785 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.07109 (* 1 = 8.07109 loss)
I0523 03:43:45.494673 35003 sgd_solver.cpp:112] Iteration 134800, lr = 0.01
I0523 03:43:51.203689 35003 solver.cpp:239] Iteration 134810 (1.55505 iter/s, 6.43064s/10 iters), loss = 7.34956
I0523 03:43:51.203984 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34956 (* 1 = 7.34956 loss)
I0523 03:43:51.217381 35003 sgd_solver.cpp:112] Iteration 134810, lr = 0.01
I0523 03:43:54.900254 35003 solver.cpp:239] Iteration 134820 (2.70552 iter/s, 3.69614s/10 iters), loss = 6.26778
I0523 03:43:54.900302 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26778 (* 1 = 6.26778 loss)
I0523 03:43:54.971117 35003 sgd_solver.cpp:112] Iteration 134820, lr = 0.01
I0523 03:43:56.886795 35003 solver.cpp:239] Iteration 134830 (5.03423 iter/s, 1.9864s/10 iters), loss = 7.77269
I0523 03:43:56.886832 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77269 (* 1 = 7.77269 loss)
I0523 03:43:56.892242 35003 sgd_solver.cpp:112] Iteration 134830, lr = 0.01
I0523 03:44:00.789120 35003 solver.cpp:239] Iteration 134840 (2.56271 iter/s, 3.90212s/10 iters), loss = 6.65141
I0523 03:44:00.789176 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65141 (* 1 = 6.65141 loss)
I0523 03:44:00.797546 35003 sgd_solver.cpp:112] Iteration 134840, lr = 0.01
I0523 03:44:03.339803 35003 solver.cpp:239] Iteration 134850 (3.92076 iter/s, 2.55052s/10 iters), loss = 7.0945
I0523 03:44:03.339843 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0945 (* 1 = 7.0945 loss)
I0523 03:44:03.347162 35003 sgd_solver.cpp:112] Iteration 134850, lr = 0.01
I0523 03:44:07.245896 35003 solver.cpp:239] Iteration 134860 (2.56024 iter/s, 3.90589s/10 iters), loss = 6.88451
I0523 03:44:07.245944 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88451 (* 1 = 6.88451 loss)
I0523 03:44:07.257311 35003 sgd_solver.cpp:112] Iteration 134860, lr = 0.01
I0523 03:44:11.451524 35003 solver.cpp:239] Iteration 134870 (2.3779 iter/s, 4.2054s/10 iters), loss = 6.50556
I0523 03:44:11.451572 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50556 (* 1 = 6.50556 loss)
I0523 03:44:11.470085 35003 sgd_solver.cpp:112] Iteration 134870, lr = 0.01
I0523 03:44:16.630573 35003 solver.cpp:239] Iteration 134880 (1.93095 iter/s, 5.17879s/10 iters), loss = 7.36086
I0523 03:44:16.630611 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36086 (* 1 = 7.36086 loss)
I0523 03:44:16.638574 35003 sgd_solver.cpp:112] Iteration 134880, lr = 0.01
I0523 03:44:20.081513 35003 solver.cpp:239] Iteration 134890 (2.89792 iter/s, 3.45075s/10 iters), loss = 5.56846
I0523 03:44:20.081560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.56846 (* 1 = 5.56846 loss)
I0523 03:44:20.088404 35003 sgd_solver.cpp:112] Iteration 134890, lr = 0.01
I0523 03:44:23.294448 35003 solver.cpp:239] Iteration 134900 (3.11261 iter/s, 3.21274s/10 iters), loss = 7.08419
I0523 03:44:23.294734 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08419 (* 1 = 7.08419 loss)
I0523 03:44:23.299932 35003 sgd_solver.cpp:112] Iteration 134900, lr = 0.01
I0523 03:44:26.747551 35003 solver.cpp:239] Iteration 134910 (2.89628 iter/s, 3.4527s/10 iters), loss = 6.26188
I0523 03:44:26.747597 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26188 (* 1 = 6.26188 loss)
I0523 03:44:26.753068 35003 sgd_solver.cpp:112] Iteration 134910, lr = 0.01
I0523 03:44:29.565481 35003 solver.cpp:239] Iteration 134920 (3.54892 iter/s, 2.81776s/10 iters), loss = 6.24511
I0523 03:44:29.565527 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24511 (* 1 = 6.24511 loss)
I0523 03:44:29.569371 35003 sgd_solver.cpp:112] Iteration 134920, lr = 0.01
I0523 03:44:31.682535 35003 solver.cpp:239] Iteration 134930 (4.72387 iter/s, 2.11691s/10 iters), loss = 6.85283
I0523 03:44:31.682584 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85283 (* 1 = 6.85283 loss)
I0523 03:44:31.689040 35003 sgd_solver.cpp:112] Iteration 134930, lr = 0.01
I0523 03:44:35.555388 35003 solver.cpp:239] Iteration 134940 (2.58222 iter/s, 3.87264s/10 iters), loss = 6.95467
I0523 03:44:35.555435 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95467 (* 1 = 6.95467 loss)
I0523 03:44:35.965965 35003 sgd_solver.cpp:112] Iteration 134940, lr = 0.01
I0523 03:44:39.548352 35003 solver.cpp:239] Iteration 134950 (2.50454 iter/s, 3.99275s/10 iters), loss = 7.02515
I0523 03:44:39.548427 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02515 (* 1 = 7.02515 loss)
I0523 03:44:40.256669 35003 sgd_solver.cpp:112] Iteration 134950, lr = 0.01
I0523 03:44:43.894465 35003 solver.cpp:239] Iteration 134960 (2.30105 iter/s, 4.34585s/10 iters), loss = 6.64043
I0523 03:44:43.894521 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64043 (* 1 = 6.64043 loss)
I0523 03:44:44.528291 35003 sgd_solver.cpp:112] Iteration 134960, lr = 0.01
I0523 03:44:46.586057 35003 solver.cpp:239] Iteration 134970 (3.71551 iter/s, 2.69142s/10 iters), loss = 6.97153
I0523 03:44:46.586112 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97153 (* 1 = 6.97153 loss)
I0523 03:44:47.320533 35003 sgd_solver.cpp:112] Iteration 134970, lr = 0.01
I0523 03:44:50.155123 35003 solver.cpp:239] Iteration 134980 (2.80201 iter/s, 3.56887s/10 iters), loss = 7.0308
I0523 03:44:50.155160 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0308 (* 1 = 7.0308 loss)
I0523 03:44:50.161151 35003 sgd_solver.cpp:112] Iteration 134980, lr = 0.01
I0523 03:44:52.713685 35003 solver.cpp:239] Iteration 134990 (3.90867 iter/s, 2.55841s/10 iters), loss = 6.47673
I0523 03:44:52.713732 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47673 (* 1 = 6.47673 loss)
I0523 03:44:52.726961 35003 sgd_solver.cpp:112] Iteration 134990, lr = 0.01
I0523 03:44:54.776075 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_135000.caffemodel
I0523 03:44:56.333427 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_135000.solverstate
I0523 03:44:56.500133 35003 solver.cpp:239] Iteration 135000 (2.64115 iter/s, 3.78623s/10 iters), loss = 6.98797
I0523 03:44:56.500174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98797 (* 1 = 6.98797 loss)
I0523 03:44:56.504839 35003 sgd_solver.cpp:112] Iteration 135000, lr = 0.01
I0523 03:45:00.059546 35003 solver.cpp:239] Iteration 135010 (2.80963 iter/s, 3.55919s/10 iters), loss = 6.1936
I0523 03:45:00.059598 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1936 (* 1 = 6.1936 loss)
I0523 03:45:00.766425 35003 sgd_solver.cpp:112] Iteration 135010, lr = 0.01
I0523 03:45:03.596880 35003 solver.cpp:239] Iteration 135020 (2.82714 iter/s, 3.53714s/10 iters), loss = 6.57065
I0523 03:45:03.596927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57065 (* 1 = 6.57065 loss)
I0523 03:45:04.337750 35003 sgd_solver.cpp:112] Iteration 135020, lr = 0.01
I0523 03:45:07.926630 35003 solver.cpp:239] Iteration 135030 (2.30973 iter/s, 4.32951s/10 iters), loss = 7.52835
I0523 03:45:07.926686 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52835 (* 1 = 7.52835 loss)
I0523 03:45:08.472256 35003 sgd_solver.cpp:112] Iteration 135030, lr = 0.01
I0523 03:45:11.531522 35003 solver.cpp:239] Iteration 135040 (2.77417 iter/s, 3.60468s/10 iters), loss = 7.56143
I0523 03:45:11.531570 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56143 (* 1 = 7.56143 loss)
I0523 03:45:12.191262 35003 sgd_solver.cpp:112] Iteration 135040, lr = 0.01
I0523 03:45:14.913321 35003 solver.cpp:239] Iteration 135050 (2.95717 iter/s, 3.38161s/10 iters), loss = 7.15668
I0523 03:45:14.913370 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15668 (* 1 = 7.15668 loss)
I0523 03:45:14.926231 35003 sgd_solver.cpp:112] Iteration 135050, lr = 0.01
I0523 03:45:17.737298 35003 solver.cpp:239] Iteration 135060 (3.54132 iter/s, 2.8238s/10 iters), loss = 7.51827
I0523 03:45:17.737344 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51827 (* 1 = 7.51827 loss)
I0523 03:45:17.748009 35003 sgd_solver.cpp:112] Iteration 135060, lr = 0.01
I0523 03:45:21.804390 35003 solver.cpp:239] Iteration 135070 (2.45889 iter/s, 4.06687s/10 iters), loss = 6.77444
I0523 03:45:21.804440 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77444 (* 1 = 6.77444 loss)
I0523 03:45:21.807919 35003 sgd_solver.cpp:112] Iteration 135070, lr = 0.01
I0523 03:45:25.408357 35003 solver.cpp:239] Iteration 135080 (2.77487 iter/s, 3.60377s/10 iters), loss = 7.34696
I0523 03:45:25.408568 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34696 (* 1 = 7.34696 loss)
I0523 03:45:25.477182 35003 sgd_solver.cpp:112] Iteration 135080, lr = 0.01
I0523 03:45:29.145543 35003 solver.cpp:239] Iteration 135090 (2.67605 iter/s, 3.73684s/10 iters), loss = 6.58538
I0523 03:45:29.145584 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58538 (* 1 = 6.58538 loss)
I0523 03:45:29.152526 35003 sgd_solver.cpp:112] Iteration 135090, lr = 0.01
I0523 03:45:31.501791 35003 solver.cpp:239] Iteration 135100 (4.2443 iter/s, 2.3561s/10 iters), loss = 6.70045
I0523 03:45:31.501835 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70045 (* 1 = 6.70045 loss)
I0523 03:45:31.506873 35003 sgd_solver.cpp:112] Iteration 135100, lr = 0.01
I0523 03:45:35.045948 35003 solver.cpp:239] Iteration 135110 (2.8217 iter/s, 3.54397s/10 iters), loss = 7.3552
I0523 03:45:35.045991 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3552 (* 1 = 7.3552 loss)
I0523 03:45:35.053333 35003 sgd_solver.cpp:112] Iteration 135110, lr = 0.01
I0523 03:45:39.359467 35003 solver.cpp:239] Iteration 135120 (2.31842 iter/s, 4.31329s/10 iters), loss = 6.22698
I0523 03:45:39.359517 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22698 (* 1 = 6.22698 loss)
I0523 03:45:40.074918 35003 sgd_solver.cpp:112] Iteration 135120, lr = 0.01
I0523 03:45:42.659915 35003 solver.cpp:239] Iteration 135130 (3.03007 iter/s, 3.30026s/10 iters), loss = 7.06775
I0523 03:45:42.659965 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06775 (* 1 = 7.06775 loss)
I0523 03:45:43.361266 35003 sgd_solver.cpp:112] Iteration 135130, lr = 0.01
I0523 03:45:45.364357 35003 solver.cpp:239] Iteration 135140 (3.69785 iter/s, 2.70428s/10 iters), loss = 7.52197
I0523 03:45:45.364398 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52197 (* 1 = 7.52197 loss)
I0523 03:45:45.389624 35003 sgd_solver.cpp:112] Iteration 135140, lr = 0.01
I0523 03:45:49.007251 35003 solver.cpp:239] Iteration 135150 (2.74521 iter/s, 3.6427s/10 iters), loss = 6.82
I0523 03:45:49.007297 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82 (* 1 = 6.82 loss)
I0523 03:45:49.024029 35003 sgd_solver.cpp:112] Iteration 135150, lr = 0.01
I0523 03:45:52.695376 35003 solver.cpp:239] Iteration 135160 (2.71155 iter/s, 3.68792s/10 iters), loss = 7.85234
I0523 03:45:52.695426 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85234 (* 1 = 7.85234 loss)
I0523 03:45:53.418010 35003 sgd_solver.cpp:112] Iteration 135160, lr = 0.01
I0523 03:45:56.156096 35003 solver.cpp:239] Iteration 135170 (2.88974 iter/s, 3.46052s/10 iters), loss = 6.07156
I0523 03:45:56.156301 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07156 (* 1 = 6.07156 loss)
I0523 03:45:56.865170 35003 sgd_solver.cpp:112] Iteration 135170, lr = 0.01
I0523 03:45:59.662840 35003 solver.cpp:239] Iteration 135180 (2.85193 iter/s, 3.5064s/10 iters), loss = 6.40267
I0523 03:45:59.662876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40267 (* 1 = 6.40267 loss)
I0523 03:45:59.676468 35003 sgd_solver.cpp:112] Iteration 135180, lr = 0.01
I0523 03:46:03.813830 35003 solver.cpp:239] Iteration 135190 (2.40919 iter/s, 4.15078s/10 iters), loss = 7.12584
I0523 03:46:03.813889 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12584 (* 1 = 7.12584 loss)
I0523 03:46:04.359519 35003 sgd_solver.cpp:112] Iteration 135190, lr = 0.01
I0523 03:46:06.538609 35003 solver.cpp:239] Iteration 135200 (3.67025 iter/s, 2.72461s/10 iters), loss = 7.0328
I0523 03:46:06.538658 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0328 (* 1 = 7.0328 loss)
I0523 03:46:07.276491 35003 sgd_solver.cpp:112] Iteration 135200, lr = 0.01
I0523 03:46:11.247047 35003 solver.cpp:239] Iteration 135210 (2.12395 iter/s, 4.7082s/10 iters), loss = 7.29559
I0523 03:46:11.247095 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29559 (* 1 = 7.29559 loss)
I0523 03:46:11.257473 35003 sgd_solver.cpp:112] Iteration 135210, lr = 0.01
I0523 03:46:16.263716 35003 solver.cpp:239] Iteration 135220 (1.99346 iter/s, 5.01641s/10 iters), loss = 7.17985
I0523 03:46:16.263764 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17985 (* 1 = 7.17985 loss)
I0523 03:46:16.276679 35003 sgd_solver.cpp:112] Iteration 135220, lr = 0.01
I0523 03:46:18.763468 35003 solver.cpp:239] Iteration 135230 (4.00065 iter/s, 2.49959s/10 iters), loss = 7.36738
I0523 03:46:18.763520 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36738 (* 1 = 7.36738 loss)
I0523 03:46:18.766373 35003 sgd_solver.cpp:112] Iteration 135230, lr = 0.01
I0523 03:46:20.838650 35003 solver.cpp:239] Iteration 135240 (4.81922 iter/s, 2.07503s/10 iters), loss = 7.14952
I0523 03:46:20.838690 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14952 (* 1 = 7.14952 loss)
I0523 03:46:20.843946 35003 sgd_solver.cpp:112] Iteration 135240, lr = 0.01
I0523 03:46:24.247362 35003 solver.cpp:239] Iteration 135250 (2.93382 iter/s, 3.40852s/10 iters), loss = 8.38798
I0523 03:46:24.247404 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.38798 (* 1 = 8.38798 loss)
I0523 03:46:24.265410 35003 sgd_solver.cpp:112] Iteration 135250, lr = 0.01
I0523 03:46:27.073601 35003 solver.cpp:239] Iteration 135260 (3.53849 iter/s, 2.82606s/10 iters), loss = 6.75429
I0523 03:46:27.073913 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75429 (* 1 = 6.75429 loss)
I0523 03:46:27.081262 35003 sgd_solver.cpp:112] Iteration 135260, lr = 0.01
I0523 03:46:29.939391 35003 solver.cpp:239] Iteration 135270 (3.48996 iter/s, 2.86536s/10 iters), loss = 6.01119
I0523 03:46:29.939435 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01119 (* 1 = 6.01119 loss)
I0523 03:46:29.955265 35003 sgd_solver.cpp:112] Iteration 135270, lr = 0.01
I0523 03:46:31.949534 35003 solver.cpp:239] Iteration 135280 (4.9751 iter/s, 2.01001s/10 iters), loss = 7.22418
I0523 03:46:31.949579 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22418 (* 1 = 7.22418 loss)
I0523 03:46:31.967483 35003 sgd_solver.cpp:112] Iteration 135280, lr = 0.01
I0523 03:46:35.399097 35003 solver.cpp:239] Iteration 135290 (2.89908 iter/s, 3.44937s/10 iters), loss = 5.26071
I0523 03:46:35.399147 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.26071 (* 1 = 5.26071 loss)
I0523 03:46:36.095044 35003 sgd_solver.cpp:112] Iteration 135290, lr = 0.01
I0523 03:46:40.467274 35003 solver.cpp:239] Iteration 135300 (1.97319 iter/s, 5.06793s/10 iters), loss = 8.04191
I0523 03:46:40.467319 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04191 (* 1 = 8.04191 loss)
I0523 03:46:41.194716 35003 sgd_solver.cpp:112] Iteration 135300, lr = 0.01
I0523 03:46:44.526446 35003 solver.cpp:239] Iteration 135310 (2.46369 iter/s, 4.05895s/10 iters), loss = 8.45109
I0523 03:46:44.526502 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.45109 (* 1 = 8.45109 loss)
I0523 03:46:44.539664 35003 sgd_solver.cpp:112] Iteration 135310, lr = 0.01
I0523 03:46:49.440749 35003 solver.cpp:239] Iteration 135320 (2.03498 iter/s, 4.91405s/10 iters), loss = 6.52537
I0523 03:46:49.440788 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52537 (* 1 = 6.52537 loss)
I0523 03:46:49.448797 35003 sgd_solver.cpp:112] Iteration 135320, lr = 0.01
I0523 03:46:51.451395 35003 solver.cpp:239] Iteration 135330 (4.97384 iter/s, 2.01052s/10 iters), loss = 6.45105
I0523 03:46:51.451432 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45105 (* 1 = 6.45105 loss)
I0523 03:46:51.458150 35003 sgd_solver.cpp:112] Iteration 135330, lr = 0.01
I0523 03:46:54.902288 35003 solver.cpp:239] Iteration 135340 (2.89796 iter/s, 3.4507s/10 iters), loss = 7.61659
I0523 03:46:54.902354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61659 (* 1 = 7.61659 loss)
I0523 03:46:54.915196 35003 sgd_solver.cpp:112] Iteration 135340, lr = 0.01
I0523 03:46:57.736424 35003 solver.cpp:239] Iteration 135350 (3.52864 iter/s, 2.83396s/10 iters), loss = 7.58052
I0523 03:46:57.736693 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58052 (* 1 = 7.58052 loss)
I0523 03:46:57.746595 35003 sgd_solver.cpp:112] Iteration 135350, lr = 0.01
I0523 03:47:02.076097 35003 solver.cpp:239] Iteration 135360 (2.30454 iter/s, 4.33926s/10 iters), loss = 7.3331
I0523 03:47:02.076141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3331 (* 1 = 7.3331 loss)
I0523 03:47:02.085052 35003 sgd_solver.cpp:112] Iteration 135360, lr = 0.01
I0523 03:47:04.972044 35003 solver.cpp:239] Iteration 135370 (3.45331 iter/s, 2.89578s/10 iters), loss = 7.20641
I0523 03:47:04.972086 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20641 (* 1 = 7.20641 loss)
I0523 03:47:04.980038 35003 sgd_solver.cpp:112] Iteration 135370, lr = 0.01
I0523 03:47:08.486997 35003 solver.cpp:239] Iteration 135380 (2.84514 iter/s, 3.51476s/10 iters), loss = 7.99752
I0523 03:47:08.487061 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99752 (* 1 = 7.99752 loss)
I0523 03:47:08.500635 35003 sgd_solver.cpp:112] Iteration 135380, lr = 0.01
I0523 03:47:12.824616 35003 solver.cpp:239] Iteration 135390 (2.30554 iter/s, 4.33738s/10 iters), loss = 7.74246
I0523 03:47:12.824671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74246 (* 1 = 7.74246 loss)
I0523 03:47:13.540288 35003 sgd_solver.cpp:112] Iteration 135390, lr = 0.01
I0523 03:47:17.185619 35003 solver.cpp:239] Iteration 135400 (2.29317 iter/s, 4.36077s/10 iters), loss = 6.45197
I0523 03:47:17.185658 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45197 (* 1 = 6.45197 loss)
I0523 03:47:17.220597 35003 sgd_solver.cpp:112] Iteration 135400, lr = 0.01
I0523 03:47:19.986054 35003 solver.cpp:239] Iteration 135410 (3.57108 iter/s, 2.80027s/10 iters), loss = 6.97452
I0523 03:47:19.986106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97452 (* 1 = 6.97452 loss)
I0523 03:47:19.998492 35003 sgd_solver.cpp:112] Iteration 135410, lr = 0.01
I0523 03:47:22.843745 35003 solver.cpp:239] Iteration 135420 (3.49955 iter/s, 2.85751s/10 iters), loss = 7.45998
I0523 03:47:22.843807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45998 (* 1 = 7.45998 loss)
I0523 03:47:22.850106 35003 sgd_solver.cpp:112] Iteration 135420, lr = 0.01
I0523 03:47:26.465510 35003 solver.cpp:239] Iteration 135430 (2.76125 iter/s, 3.62155s/10 iters), loss = 7.61576
I0523 03:47:26.465574 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61576 (* 1 = 7.61576 loss)
I0523 03:47:26.472968 35003 sgd_solver.cpp:112] Iteration 135430, lr = 0.01
I0523 03:47:28.581010 35003 solver.cpp:239] Iteration 135440 (4.72735 iter/s, 2.11535s/10 iters), loss = 7.47647
I0523 03:47:28.581280 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47647 (* 1 = 7.47647 loss)
I0523 03:47:28.587841 35003 sgd_solver.cpp:112] Iteration 135440, lr = 0.01
I0523 03:47:31.759699 35003 solver.cpp:239] Iteration 135450 (3.14648 iter/s, 3.17815s/10 iters), loss = 7.56394
I0523 03:47:31.759752 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56394 (* 1 = 7.56394 loss)
I0523 03:47:31.767909 35003 sgd_solver.cpp:112] Iteration 135450, lr = 0.01
I0523 03:47:34.516387 35003 solver.cpp:239] Iteration 135460 (3.62776 iter/s, 2.75652s/10 iters), loss = 6.08741
I0523 03:47:34.516427 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08741 (* 1 = 6.08741 loss)
I0523 03:47:34.528018 35003 sgd_solver.cpp:112] Iteration 135460, lr = 0.01
I0523 03:47:38.038453 35003 solver.cpp:239] Iteration 135470 (2.83939 iter/s, 3.52188s/10 iters), loss = 6.56763
I0523 03:47:38.038507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56763 (* 1 = 6.56763 loss)
I0523 03:47:38.051371 35003 sgd_solver.cpp:112] Iteration 135470, lr = 0.01
I0523 03:47:41.622342 35003 solver.cpp:239] Iteration 135480 (2.79042 iter/s, 3.58368s/10 iters), loss = 7.21301
I0523 03:47:41.622391 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21301 (* 1 = 7.21301 loss)
I0523 03:47:42.357292 35003 sgd_solver.cpp:112] Iteration 135480, lr = 0.01
I0523 03:47:45.907500 35003 solver.cpp:239] Iteration 135490 (2.33376 iter/s, 4.28493s/10 iters), loss = 6.49941
I0523 03:47:45.907541 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49941 (* 1 = 6.49941 loss)
I0523 03:47:45.912161 35003 sgd_solver.cpp:112] Iteration 135490, lr = 0.01
I0523 03:47:47.994362 35003 solver.cpp:239] Iteration 135500 (4.7922 iter/s, 2.08673s/10 iters), loss = 7.52435
I0523 03:47:47.994405 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52435 (* 1 = 7.52435 loss)
I0523 03:47:48.000180 35003 sgd_solver.cpp:112] Iteration 135500, lr = 0.01
I0523 03:47:49.477906 35003 solver.cpp:239] Iteration 135510 (6.74114 iter/s, 1.48343s/10 iters), loss = 6.56131
I0523 03:47:49.477962 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56131 (* 1 = 6.56131 loss)
I0523 03:47:49.483323 35003 sgd_solver.cpp:112] Iteration 135510, lr = 0.01
I0523 03:47:53.896526 35003 solver.cpp:239] Iteration 135520 (2.26327 iter/s, 4.41838s/10 iters), loss = 6.4742
I0523 03:47:53.896579 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4742 (* 1 = 6.4742 loss)
I0523 03:47:54.618245 35003 sgd_solver.cpp:112] Iteration 135520, lr = 0.01
I0523 03:47:57.490142 35003 solver.cpp:239] Iteration 135530 (2.78287 iter/s, 3.59341s/10 iters), loss = 7.2832
I0523 03:47:57.490200 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2832 (* 1 = 7.2832 loss)
I0523 03:47:57.495781 35003 sgd_solver.cpp:112] Iteration 135530, lr = 0.01
I0523 03:48:01.194533 35003 solver.cpp:239] Iteration 135540 (2.69965 iter/s, 3.70418s/10 iters), loss = 6.60519
I0523 03:48:01.194859 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60519 (* 1 = 6.60519 loss)
I0523 03:48:01.207566 35003 sgd_solver.cpp:112] Iteration 135540, lr = 0.01
I0523 03:48:04.809900 35003 solver.cpp:239] Iteration 135550 (2.76632 iter/s, 3.61491s/10 iters), loss = 7.22988
I0523 03:48:04.809963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22988 (* 1 = 7.22988 loss)
I0523 03:48:05.518256 35003 sgd_solver.cpp:112] Iteration 135550, lr = 0.01
I0523 03:48:09.228775 35003 solver.cpp:239] Iteration 135560 (2.26314 iter/s, 4.41863s/10 iters), loss = 7.02178
I0523 03:48:09.228821 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02178 (* 1 = 7.02178 loss)
I0523 03:48:09.969663 35003 sgd_solver.cpp:112] Iteration 135560, lr = 0.01
I0523 03:48:12.822556 35003 solver.cpp:239] Iteration 135570 (2.78274 iter/s, 3.59358s/10 iters), loss = 7.71908
I0523 03:48:12.822604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71908 (* 1 = 7.71908 loss)
I0523 03:48:13.563474 35003 sgd_solver.cpp:112] Iteration 135570, lr = 0.01
I0523 03:48:17.175914 35003 solver.cpp:239] Iteration 135580 (2.2972 iter/s, 4.35313s/10 iters), loss = 7.08165
I0523 03:48:17.175962 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08165 (* 1 = 7.08165 loss)
I0523 03:48:17.609959 35003 sgd_solver.cpp:112] Iteration 135580, lr = 0.01
I0523 03:48:21.795794 35003 solver.cpp:239] Iteration 135590 (2.16467 iter/s, 4.61964s/10 iters), loss = 6.89084
I0523 03:48:21.795856 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89084 (* 1 = 6.89084 loss)
I0523 03:48:21.906543 35003 sgd_solver.cpp:112] Iteration 135590, lr = 0.01
I0523 03:48:23.996029 35003 solver.cpp:239] Iteration 135600 (4.54529 iter/s, 2.20008s/10 iters), loss = 6.76511
I0523 03:48:23.996073 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76511 (* 1 = 6.76511 loss)
I0523 03:48:24.009171 35003 sgd_solver.cpp:112] Iteration 135600, lr = 0.01
I0523 03:48:26.058619 35003 solver.cpp:239] Iteration 135610 (4.8486 iter/s, 2.06245s/10 iters), loss = 7.50278
I0523 03:48:26.058657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50278 (* 1 = 7.50278 loss)
I0523 03:48:26.072080 35003 sgd_solver.cpp:112] Iteration 135610, lr = 0.01
I0523 03:48:28.075072 35003 solver.cpp:239] Iteration 135620 (4.95952 iter/s, 2.01632s/10 iters), loss = 5.61843
I0523 03:48:28.075109 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.61843 (* 1 = 5.61843 loss)
I0523 03:48:28.088903 35003 sgd_solver.cpp:112] Iteration 135620, lr = 0.01
I0523 03:48:32.182415 35003 solver.cpp:239] Iteration 135630 (2.43479 iter/s, 4.10713s/10 iters), loss = 7.67634
I0523 03:48:32.182641 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67634 (* 1 = 7.67634 loss)
I0523 03:48:32.205848 35003 sgd_solver.cpp:112] Iteration 135630, lr = 0.01
I0523 03:48:36.761101 35003 solver.cpp:239] Iteration 135640 (2.18422 iter/s, 4.57829s/10 iters), loss = 7.24496
I0523 03:48:36.761139 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24496 (* 1 = 7.24496 loss)
I0523 03:48:36.773406 35003 sgd_solver.cpp:112] Iteration 135640, lr = 0.01
I0523 03:48:39.572306 35003 solver.cpp:239] Iteration 135650 (3.55739 iter/s, 2.81105s/10 iters), loss = 7.19734
I0523 03:48:39.572347 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19734 (* 1 = 7.19734 loss)
I0523 03:48:39.580531 35003 sgd_solver.cpp:112] Iteration 135650, lr = 0.01
I0523 03:48:41.634745 35003 solver.cpp:239] Iteration 135660 (4.84895 iter/s, 2.0623s/10 iters), loss = 7.39124
I0523 03:48:41.634799 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39124 (* 1 = 7.39124 loss)
I0523 03:48:41.646229 35003 sgd_solver.cpp:112] Iteration 135660, lr = 0.01
I0523 03:48:43.109814 35003 solver.cpp:239] Iteration 135670 (6.77991 iter/s, 1.47495s/10 iters), loss = 7.28919
I0523 03:48:43.109865 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28919 (* 1 = 7.28919 loss)
I0523 03:48:43.836004 35003 sgd_solver.cpp:112] Iteration 135670, lr = 0.01
I0523 03:48:45.170310 35003 solver.cpp:239] Iteration 135680 (4.85354 iter/s, 2.06035s/10 iters), loss = 6.52185
I0523 03:48:45.170361 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52185 (* 1 = 6.52185 loss)
I0523 03:48:45.904942 35003 sgd_solver.cpp:112] Iteration 135680, lr = 0.01
I0523 03:48:48.977064 35003 solver.cpp:239] Iteration 135690 (2.62705 iter/s, 3.80655s/10 iters), loss = 5.99628
I0523 03:48:48.977100 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99628 (* 1 = 5.99628 loss)
I0523 03:48:48.990995 35003 sgd_solver.cpp:112] Iteration 135690, lr = 0.01
I0523 03:48:52.450541 35003 solver.cpp:239] Iteration 135700 (2.87911 iter/s, 3.47329s/10 iters), loss = 7.56526
I0523 03:48:52.450593 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56526 (* 1 = 7.56526 loss)
I0523 03:48:52.595237 35003 sgd_solver.cpp:112] Iteration 135700, lr = 0.01
I0523 03:48:56.193629 35003 solver.cpp:239] Iteration 135710 (2.67176 iter/s, 3.74286s/10 iters), loss = 7.22052
I0523 03:48:56.193667 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22052 (* 1 = 7.22052 loss)
I0523 03:48:56.891171 35003 sgd_solver.cpp:112] Iteration 135710, lr = 0.01
I0523 03:49:01.239389 35003 solver.cpp:239] Iteration 135720 (1.98196 iter/s, 5.04551s/10 iters), loss = 6.97879
I0523 03:49:01.239444 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97879 (* 1 = 6.97879 loss)
I0523 03:49:01.243746 35003 sgd_solver.cpp:112] Iteration 135720, lr = 0.01
I0523 03:49:04.024523 35003 solver.cpp:239] Iteration 135730 (3.59072 iter/s, 2.78495s/10 iters), loss = 6.41534
I0523 03:49:04.024791 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41534 (* 1 = 6.41534 loss)
I0523 03:49:04.038043 35003 sgd_solver.cpp:112] Iteration 135730, lr = 0.01
I0523 03:49:06.877619 35003 solver.cpp:239] Iteration 135740 (3.50541 iter/s, 2.85273s/10 iters), loss = 7.34342
I0523 03:49:06.877670 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34342 (* 1 = 7.34342 loss)
I0523 03:49:06.901485 35003 sgd_solver.cpp:112] Iteration 135740, lr = 0.01
I0523 03:49:09.198896 35003 solver.cpp:239] Iteration 135750 (4.30825 iter/s, 2.32113s/10 iters), loss = 6.5623
I0523 03:49:09.198945 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5623 (* 1 = 6.5623 loss)
I0523 03:49:09.920135 35003 sgd_solver.cpp:112] Iteration 135750, lr = 0.01
I0523 03:49:12.049001 35003 solver.cpp:239] Iteration 135760 (3.50886 iter/s, 2.84993s/10 iters), loss = 7.22365
I0523 03:49:12.049052 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22365 (* 1 = 7.22365 loss)
I0523 03:49:12.061648 35003 sgd_solver.cpp:112] Iteration 135760, lr = 0.01
I0523 03:49:16.124768 35003 solver.cpp:239] Iteration 135770 (2.45365 iter/s, 4.07555s/10 iters), loss = 7.18814
I0523 03:49:16.124812 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18814 (* 1 = 7.18814 loss)
I0523 03:49:16.293838 35003 sgd_solver.cpp:112] Iteration 135770, lr = 0.01
I0523 03:49:19.877816 35003 solver.cpp:239] Iteration 135780 (2.66464 iter/s, 3.75285s/10 iters), loss = 7.26583
I0523 03:49:19.877866 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26583 (* 1 = 7.26583 loss)
I0523 03:49:19.895970 35003 sgd_solver.cpp:112] Iteration 135780, lr = 0.01
I0523 03:49:21.942597 35003 solver.cpp:239] Iteration 135790 (4.84346 iter/s, 2.06464s/10 iters), loss = 6.31406
I0523 03:49:21.942641 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31406 (* 1 = 6.31406 loss)
I0523 03:49:21.955998 35003 sgd_solver.cpp:112] Iteration 135790, lr = 0.01
I0523 03:49:25.053431 35003 solver.cpp:239] Iteration 135800 (3.21475 iter/s, 3.11066s/10 iters), loss = 6.9613
I0523 03:49:25.053478 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9613 (* 1 = 6.9613 loss)
I0523 03:49:25.774655 35003 sgd_solver.cpp:112] Iteration 135800, lr = 0.01
I0523 03:49:28.939848 35003 solver.cpp:239] Iteration 135810 (2.57321 iter/s, 3.8862s/10 iters), loss = 6.00001
I0523 03:49:28.939896 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00001 (* 1 = 6.00001 loss)
I0523 03:49:29.562130 35003 sgd_solver.cpp:112] Iteration 135810, lr = 0.01
I0523 03:49:32.505625 35003 solver.cpp:239] Iteration 135820 (2.80459 iter/s, 3.56558s/10 iters), loss = 7.31059
I0523 03:49:32.505667 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31059 (* 1 = 7.31059 loss)
I0523 03:49:33.234115 35003 sgd_solver.cpp:112] Iteration 135820, lr = 0.01
I0523 03:49:36.010462 35003 solver.cpp:239] Iteration 135830 (2.85336 iter/s, 3.50464s/10 iters), loss = 8.33182
I0523 03:49:36.010768 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.33182 (* 1 = 8.33182 loss)
I0523 03:49:36.016906 35003 sgd_solver.cpp:112] Iteration 135830, lr = 0.01
I0523 03:49:37.311211 35003 solver.cpp:239] Iteration 135840 (7.68995 iter/s, 1.3004s/10 iters), loss = 6.90156
I0523 03:49:37.311259 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90156 (* 1 = 6.90156 loss)
I0523 03:49:37.324213 35003 sgd_solver.cpp:112] Iteration 135840, lr = 0.01
I0523 03:49:41.463587 35003 solver.cpp:239] Iteration 135850 (2.40839 iter/s, 4.15216s/10 iters), loss = 6.37235
I0523 03:49:41.463639 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37235 (* 1 = 6.37235 loss)
I0523 03:49:42.189891 35003 sgd_solver.cpp:112] Iteration 135850, lr = 0.01
I0523 03:49:45.769228 35003 solver.cpp:239] Iteration 135860 (2.32266 iter/s, 4.30541s/10 iters), loss = 5.92115
I0523 03:49:45.769269 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92115 (* 1 = 5.92115 loss)
I0523 03:49:46.471648 35003 sgd_solver.cpp:112] Iteration 135860, lr = 0.01
I0523 03:49:49.369770 35003 solver.cpp:239] Iteration 135870 (2.77751 iter/s, 3.60035s/10 iters), loss = 6.70398
I0523 03:49:49.369819 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70398 (* 1 = 6.70398 loss)
I0523 03:49:50.110767 35003 sgd_solver.cpp:112] Iteration 135870, lr = 0.01
I0523 03:49:52.976490 35003 solver.cpp:239] Iteration 135880 (2.77275 iter/s, 3.60652s/10 iters), loss = 7.18203
I0523 03:49:52.976528 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18203 (* 1 = 7.18203 loss)
I0523 03:49:53.000042 35003 sgd_solver.cpp:112] Iteration 135880, lr = 0.01
I0523 03:49:56.348479 35003 solver.cpp:239] Iteration 135890 (2.96577 iter/s, 3.3718s/10 iters), loss = 6.57332
I0523 03:49:56.348522 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57332 (* 1 = 6.57332 loss)
I0523 03:49:56.361505 35003 sgd_solver.cpp:112] Iteration 135890, lr = 0.01
I0523 03:50:00.450168 35003 solver.cpp:239] Iteration 135900 (2.43815 iter/s, 4.10148s/10 iters), loss = 8.25105
I0523 03:50:00.450218 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.25105 (* 1 = 8.25105 loss)
I0523 03:50:00.461726 35003 sgd_solver.cpp:112] Iteration 135900, lr = 0.01
I0523 03:50:03.952433 35003 solver.cpp:239] Iteration 135910 (2.85545 iter/s, 3.50207s/10 iters), loss = 7.2243
I0523 03:50:03.952477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2243 (* 1 = 7.2243 loss)
I0523 03:50:04.693776 35003 sgd_solver.cpp:112] Iteration 135910, lr = 0.01
I0523 03:50:07.182687 35003 solver.cpp:239] Iteration 135920 (3.0959 iter/s, 3.23007s/10 iters), loss = 8.44642
I0523 03:50:07.183015 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.44642 (* 1 = 8.44642 loss)
I0523 03:50:07.917116 35003 sgd_solver.cpp:112] Iteration 135920, lr = 0.01
I0523 03:50:10.749992 35003 solver.cpp:239] Iteration 135930 (2.80359 iter/s, 3.56686s/10 iters), loss = 6.86655
I0523 03:50:10.750030 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86655 (* 1 = 6.86655 loss)
I0523 03:50:10.763784 35003 sgd_solver.cpp:112] Iteration 135930, lr = 0.01
I0523 03:50:14.446228 35003 solver.cpp:239] Iteration 135940 (2.7056 iter/s, 3.69604s/10 iters), loss = 7.17652
I0523 03:50:14.446269 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17652 (* 1 = 7.17652 loss)
I0523 03:50:14.456995 35003 sgd_solver.cpp:112] Iteration 135940, lr = 0.01
I0523 03:50:18.784878 35003 solver.cpp:239] Iteration 135950 (2.30499 iter/s, 4.33842s/10 iters), loss = 6.51513
I0523 03:50:18.784931 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51513 (* 1 = 6.51513 loss)
I0523 03:50:18.798630 35003 sgd_solver.cpp:112] Iteration 135950, lr = 0.01
I0523 03:50:22.512460 35003 solver.cpp:239] Iteration 135960 (2.68285 iter/s, 3.72738s/10 iters), loss = 6.16873
I0523 03:50:22.512508 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16873 (* 1 = 6.16873 loss)
I0523 03:50:23.134295 35003 sgd_solver.cpp:112] Iteration 135960, lr = 0.01
I0523 03:50:25.283334 35003 solver.cpp:239] Iteration 135970 (3.6092 iter/s, 2.7707s/10 iters), loss = 7.55086
I0523 03:50:25.283390 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55086 (* 1 = 7.55086 loss)
I0523 03:50:26.024256 35003 sgd_solver.cpp:112] Iteration 135970, lr = 0.01
I0523 03:50:28.963696 35003 solver.cpp:239] Iteration 135980 (2.71728 iter/s, 3.68015s/10 iters), loss = 5.99777
I0523 03:50:28.963752 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99777 (* 1 = 5.99777 loss)
I0523 03:50:28.976164 35003 sgd_solver.cpp:112] Iteration 135980, lr = 0.01
I0523 03:50:30.282152 35003 solver.cpp:239] Iteration 135990 (7.58529 iter/s, 1.31834s/10 iters), loss = 7.06076
I0523 03:50:30.282191 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06076 (* 1 = 7.06076 loss)
I0523 03:50:30.295409 35003 sgd_solver.cpp:112] Iteration 135990, lr = 0.01
I0523 03:50:32.688114 35003 solver.cpp:239] Iteration 136000 (4.15659 iter/s, 2.40582s/10 iters), loss = 7.49308
I0523 03:50:32.688159 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49308 (* 1 = 7.49308 loss)
I0523 03:50:32.706183 35003 sgd_solver.cpp:112] Iteration 136000, lr = 0.01
I0523 03:50:36.336016 35003 solver.cpp:239] Iteration 136010 (2.74145 iter/s, 3.6477s/10 iters), loss = 6.65501
I0523 03:50:36.336060 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65501 (* 1 = 6.65501 loss)
I0523 03:50:36.773741 35003 sgd_solver.cpp:112] Iteration 136010, lr = 0.01
I0523 03:50:39.552635 35003 solver.cpp:239] Iteration 136020 (3.10902 iter/s, 3.21644s/10 iters), loss = 6.34898
I0523 03:50:39.552851 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34898 (* 1 = 6.34898 loss)
I0523 03:50:40.151783 35003 sgd_solver.cpp:112] Iteration 136020, lr = 0.01
I0523 03:50:43.690767 35003 solver.cpp:239] Iteration 136030 (2.41676 iter/s, 4.13777s/10 iters), loss = 6.94396
I0523 03:50:43.690807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94396 (* 1 = 6.94396 loss)
I0523 03:50:43.718231 35003 sgd_solver.cpp:112] Iteration 136030, lr = 0.01
I0523 03:50:47.005334 35003 solver.cpp:239] Iteration 136040 (3.01715 iter/s, 3.31439s/10 iters), loss = 6.80962
I0523 03:50:47.005383 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80962 (* 1 = 6.80962 loss)
I0523 03:50:47.014309 35003 sgd_solver.cpp:112] Iteration 136040, lr = 0.01
I0523 03:50:49.895587 35003 solver.cpp:239] Iteration 136050 (3.46011 iter/s, 2.89008s/10 iters), loss = 8.21544
I0523 03:50:49.895648 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.21544 (* 1 = 8.21544 loss)
I0523 03:50:49.903081 35003 sgd_solver.cpp:112] Iteration 136050, lr = 0.01
I0523 03:50:52.801064 35003 solver.cpp:239] Iteration 136060 (3.44199 iter/s, 2.9053s/10 iters), loss = 7.66636
I0523 03:50:52.801105 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66636 (* 1 = 7.66636 loss)
I0523 03:50:53.503681 35003 sgd_solver.cpp:112] Iteration 136060, lr = 0.01
I0523 03:50:57.092555 35003 solver.cpp:239] Iteration 136070 (2.33031 iter/s, 4.29127s/10 iters), loss = 6.14377
I0523 03:50:57.092602 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14377 (* 1 = 6.14377 loss)
I0523 03:50:57.808171 35003 sgd_solver.cpp:112] Iteration 136070, lr = 0.01
I0523 03:51:00.701136 35003 solver.cpp:239] Iteration 136080 (2.77133 iter/s, 3.60838s/10 iters), loss = 7.75773
I0523 03:51:00.701200 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75773 (* 1 = 7.75773 loss)
I0523 03:51:01.397228 35003 sgd_solver.cpp:112] Iteration 136080, lr = 0.01
I0523 03:51:05.246445 35003 solver.cpp:239] Iteration 136090 (2.20019 iter/s, 4.54506s/10 iters), loss = 8.2674
I0523 03:51:05.246492 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.2674 (* 1 = 8.2674 loss)
I0523 03:51:05.265781 35003 sgd_solver.cpp:112] Iteration 136090, lr = 0.01
I0523 03:51:08.362989 35003 solver.cpp:239] Iteration 136100 (3.20889 iter/s, 3.11634s/10 iters), loss = 6.57439
I0523 03:51:08.363026 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57439 (* 1 = 6.57439 loss)
I0523 03:51:08.367806 35003 sgd_solver.cpp:112] Iteration 136100, lr = 0.01
I0523 03:51:11.229161 35003 solver.cpp:239] Iteration 136110 (3.48917 iter/s, 2.86601s/10 iters), loss = 7.23555
I0523 03:51:11.229387 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23555 (* 1 = 7.23555 loss)
I0523 03:51:11.232880 35003 sgd_solver.cpp:112] Iteration 136110, lr = 0.01
I0523 03:51:13.319893 35003 solver.cpp:239] Iteration 136120 (4.78372 iter/s, 2.09042s/10 iters), loss = 5.31401
I0523 03:51:13.319941 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.31401 (* 1 = 5.31401 loss)
I0523 03:51:13.325593 35003 sgd_solver.cpp:112] Iteration 136120, lr = 0.01
I0523 03:51:16.296798 35003 solver.cpp:239] Iteration 136130 (3.3594 iter/s, 2.97673s/10 iters), loss = 5.64862
I0523 03:51:16.296851 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.64862 (* 1 = 5.64862 loss)
I0523 03:51:16.299896 35003 sgd_solver.cpp:112] Iteration 136130, lr = 0.01
I0523 03:51:20.088335 35003 solver.cpp:239] Iteration 136140 (2.6376 iter/s, 3.79132s/10 iters), loss = 7.8371
I0523 03:51:20.088382 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8371 (* 1 = 7.8371 loss)
I0523 03:51:20.097800 35003 sgd_solver.cpp:112] Iteration 136140, lr = 0.01
I0523 03:51:23.555336 35003 solver.cpp:239] Iteration 136150 (2.8845 iter/s, 3.4668s/10 iters), loss = 5.69309
I0523 03:51:23.555394 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.69309 (* 1 = 5.69309 loss)
I0523 03:51:23.568549 35003 sgd_solver.cpp:112] Iteration 136150, lr = 0.01
I0523 03:51:26.413064 35003 solver.cpp:239] Iteration 136160 (3.4995 iter/s, 2.85755s/10 iters), loss = 7.45506
I0523 03:51:26.413105 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45506 (* 1 = 7.45506 loss)
I0523 03:51:27.128669 35003 sgd_solver.cpp:112] Iteration 136160, lr = 0.01
I0523 03:51:29.535306 35003 solver.cpp:239] Iteration 136170 (3.20301 iter/s, 3.12206s/10 iters), loss = 8.15574
I0523 03:51:29.535356 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.15574 (* 1 = 8.15574 loss)
I0523 03:51:29.540216 35003 sgd_solver.cpp:112] Iteration 136170, lr = 0.01
I0523 03:51:31.468708 35003 solver.cpp:239] Iteration 136180 (5.1726 iter/s, 1.93326s/10 iters), loss = 7.58826
I0523 03:51:31.468749 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58826 (* 1 = 7.58826 loss)
I0523 03:51:31.487222 35003 sgd_solver.cpp:112] Iteration 136180, lr = 0.01
I0523 03:51:36.035859 35003 solver.cpp:239] Iteration 136190 (2.18966 iter/s, 4.56692s/10 iters), loss = 6.86103
I0523 03:51:36.035897 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86103 (* 1 = 6.86103 loss)
I0523 03:51:36.049103 35003 sgd_solver.cpp:112] Iteration 136190, lr = 0.01
I0523 03:51:38.199787 35003 solver.cpp:239] Iteration 136200 (4.62151 iter/s, 2.16379s/10 iters), loss = 6.32798
I0523 03:51:38.199832 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32798 (* 1 = 6.32798 loss)
I0523 03:51:38.209755 35003 sgd_solver.cpp:112] Iteration 136200, lr = 0.01
I0523 03:51:42.552336 35003 solver.cpp:239] Iteration 136210 (2.29765 iter/s, 4.35227s/10 iters), loss = 7.17852
I0523 03:51:42.552639 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17852 (* 1 = 7.17852 loss)
I0523 03:51:42.614447 35003 sgd_solver.cpp:112] Iteration 136210, lr = 0.01
I0523 03:51:45.273589 35003 solver.cpp:239] Iteration 136220 (3.67531 iter/s, 2.72086s/10 iters), loss = 7.20827
I0523 03:51:45.273639 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20827 (* 1 = 7.20827 loss)
I0523 03:51:45.286209 35003 sgd_solver.cpp:112] Iteration 136220, lr = 0.01
I0523 03:51:47.383492 35003 solver.cpp:239] Iteration 136230 (4.73988 iter/s, 2.10976s/10 iters), loss = 6.74908
I0523 03:51:47.383540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74908 (* 1 = 6.74908 loss)
I0523 03:51:47.390530 35003 sgd_solver.cpp:112] Iteration 136230, lr = 0.01
I0523 03:51:50.114588 35003 solver.cpp:239] Iteration 136240 (3.66176 iter/s, 2.73093s/10 iters), loss = 6.64728
I0523 03:51:50.114631 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64728 (* 1 = 6.64728 loss)
I0523 03:51:50.136914 35003 sgd_solver.cpp:112] Iteration 136240, lr = 0.01
I0523 03:51:52.929358 35003 solver.cpp:239] Iteration 136250 (3.5529 iter/s, 2.8146s/10 iters), loss = 7.25541
I0523 03:51:52.929409 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25541 (* 1 = 7.25541 loss)
I0523 03:51:53.064452 35003 sgd_solver.cpp:112] Iteration 136250, lr = 0.01
I0523 03:51:55.920822 35003 solver.cpp:239] Iteration 136260 (3.34306 iter/s, 2.99127s/10 iters), loss = 7.78204
I0523 03:51:55.920868 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78204 (* 1 = 7.78204 loss)
I0523 03:51:55.927358 35003 sgd_solver.cpp:112] Iteration 136260, lr = 0.01
I0523 03:51:59.629179 35003 solver.cpp:239] Iteration 136270 (2.69676 iter/s, 3.70815s/10 iters), loss = 7.34338
I0523 03:51:59.629228 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34338 (* 1 = 7.34338 loss)
I0523 03:51:59.636562 35003 sgd_solver.cpp:112] Iteration 136270, lr = 0.01
I0523 03:52:02.061414 35003 solver.cpp:239] Iteration 136280 (4.11171 iter/s, 2.43208s/10 iters), loss = 6.98873
I0523 03:52:02.061465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98873 (* 1 = 6.98873 loss)
I0523 03:52:02.802325 35003 sgd_solver.cpp:112] Iteration 136280, lr = 0.01
I0523 03:52:06.521994 35003 solver.cpp:239] Iteration 136290 (2.24198 iter/s, 4.46035s/10 iters), loss = 8.65555
I0523 03:52:06.522049 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.65555 (* 1 = 8.65555 loss)
I0523 03:52:06.525475 35003 sgd_solver.cpp:112] Iteration 136290, lr = 0.01
I0523 03:52:09.394990 35003 solver.cpp:239] Iteration 136300 (3.48093 iter/s, 2.8728s/10 iters), loss = 6.67659
I0523 03:52:09.395042 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67659 (* 1 = 6.67659 loss)
I0523 03:52:09.401762 35003 sgd_solver.cpp:112] Iteration 136300, lr = 0.01
I0523 03:52:11.537647 35003 solver.cpp:239] Iteration 136310 (4.66743 iter/s, 2.14251s/10 iters), loss = 6.46975
I0523 03:52:11.537691 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46975 (* 1 = 6.46975 loss)
I0523 03:52:11.544718 35003 sgd_solver.cpp:112] Iteration 136310, lr = 0.01
I0523 03:52:13.595999 35003 solver.cpp:239] Iteration 136320 (4.85858 iter/s, 2.05822s/10 iters), loss = 6.63441
I0523 03:52:13.596091 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63441 (* 1 = 6.63441 loss)
I0523 03:52:13.601171 35003 sgd_solver.cpp:112] Iteration 136320, lr = 0.01
I0523 03:52:17.635666 35003 solver.cpp:239] Iteration 136330 (2.47561 iter/s, 4.0394s/10 iters), loss = 6.36093
I0523 03:52:17.635710 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36093 (* 1 = 6.36093 loss)
I0523 03:52:18.338127 35003 sgd_solver.cpp:112] Iteration 136330, lr = 0.01
I0523 03:52:21.689926 35003 solver.cpp:239] Iteration 136340 (2.46667 iter/s, 4.05405s/10 iters), loss = 6.10892
I0523 03:52:21.689976 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10892 (* 1 = 6.10892 loss)
I0523 03:52:22.169574 35003 sgd_solver.cpp:112] Iteration 136340, lr = 0.01
I0523 03:52:24.989601 35003 solver.cpp:239] Iteration 136350 (3.03077 iter/s, 3.29949s/10 iters), loss = 6.7636
I0523 03:52:24.989641 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7636 (* 1 = 6.7636 loss)
I0523 03:52:25.002292 35003 sgd_solver.cpp:112] Iteration 136350, lr = 0.01
I0523 03:52:28.710285 35003 solver.cpp:239] Iteration 136360 (2.68782 iter/s, 3.72049s/10 iters), loss = 7.45907
I0523 03:52:28.710330 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45907 (* 1 = 7.45907 loss)
I0523 03:52:29.225890 35003 sgd_solver.cpp:112] Iteration 136360, lr = 0.01
I0523 03:52:32.002126 35003 solver.cpp:239] Iteration 136370 (3.03799 iter/s, 3.29165s/10 iters), loss = 7.71469
I0523 03:52:32.002180 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71469 (* 1 = 7.71469 loss)
I0523 03:52:32.112206 35003 sgd_solver.cpp:112] Iteration 136370, lr = 0.01
I0523 03:52:34.784281 35003 solver.cpp:239] Iteration 136380 (3.60024 iter/s, 2.77759s/10 iters), loss = 6.65015
I0523 03:52:34.784319 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65015 (* 1 = 6.65015 loss)
I0523 03:52:35.476207 35003 sgd_solver.cpp:112] Iteration 136380, lr = 0.01
I0523 03:52:39.358938 35003 solver.cpp:239] Iteration 136390 (2.18607 iter/s, 4.57443s/10 iters), loss = 6.62877
I0523 03:52:39.358992 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62877 (* 1 = 6.62877 loss)
I0523 03:52:39.372462 35003 sgd_solver.cpp:112] Iteration 136390, lr = 0.01
I0523 03:52:42.301331 35003 solver.cpp:239] Iteration 136400 (3.39881 iter/s, 2.94221s/10 iters), loss = 7.22642
I0523 03:52:42.301373 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22642 (* 1 = 7.22642 loss)
I0523 03:52:42.328753 35003 sgd_solver.cpp:112] Iteration 136400, lr = 0.01
I0523 03:52:45.200037 35003 solver.cpp:239] Iteration 136410 (3.45001 iter/s, 2.89854s/10 iters), loss = 6.51542
I0523 03:52:45.200340 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51542 (* 1 = 6.51542 loss)
I0523 03:52:45.203685 35003 sgd_solver.cpp:112] Iteration 136410, lr = 0.01
I0523 03:52:48.427646 35003 solver.cpp:239] Iteration 136420 (3.09866 iter/s, 3.2272s/10 iters), loss = 7.21952
I0523 03:52:48.427700 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21952 (* 1 = 7.21952 loss)
I0523 03:52:48.442821 35003 sgd_solver.cpp:112] Iteration 136420, lr = 0.01
I0523 03:52:53.536340 35003 solver.cpp:239] Iteration 136430 (1.95755 iter/s, 5.10844s/10 iters), loss = 7.13997
I0523 03:52:53.536378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13997 (* 1 = 7.13997 loss)
I0523 03:52:53.549823 35003 sgd_solver.cpp:112] Iteration 136430, lr = 0.01
I0523 03:52:55.460315 35003 solver.cpp:239] Iteration 136440 (5.19792 iter/s, 1.92385s/10 iters), loss = 7.64307
I0523 03:52:55.460369 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64307 (* 1 = 7.64307 loss)
I0523 03:52:56.174932 35003 sgd_solver.cpp:112] Iteration 136440, lr = 0.01
I0523 03:52:59.596868 35003 solver.cpp:239] Iteration 136450 (2.4176 iter/s, 4.13633s/10 iters), loss = 7.6391
I0523 03:52:59.596912 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6391 (* 1 = 7.6391 loss)
I0523 03:52:59.617982 35003 sgd_solver.cpp:112] Iteration 136450, lr = 0.01
I0523 03:53:02.269676 35003 solver.cpp:239] Iteration 136460 (3.7416 iter/s, 2.67265s/10 iters), loss = 7.70797
I0523 03:53:02.269718 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70797 (* 1 = 7.70797 loss)
I0523 03:53:02.952986 35003 sgd_solver.cpp:112] Iteration 136460, lr = 0.01
I0523 03:53:07.544281 35003 solver.cpp:239] Iteration 136470 (1.89597 iter/s, 5.27435s/10 iters), loss = 6.43619
I0523 03:53:07.544319 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43619 (* 1 = 6.43619 loss)
I0523 03:53:07.557596 35003 sgd_solver.cpp:112] Iteration 136470, lr = 0.01
I0523 03:53:11.270611 35003 solver.cpp:239] Iteration 136480 (2.68375 iter/s, 3.72613s/10 iters), loss = 6.49668
I0523 03:53:11.270653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49668 (* 1 = 6.49668 loss)
I0523 03:53:11.296236 35003 sgd_solver.cpp:112] Iteration 136480, lr = 0.01
I0523 03:53:15.733741 35003 solver.cpp:239] Iteration 136490 (2.2407 iter/s, 4.46289s/10 iters), loss = 7.68729
I0523 03:53:15.734010 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68729 (* 1 = 7.68729 loss)
I0523 03:53:15.740010 35003 sgd_solver.cpp:112] Iteration 136490, lr = 0.01
I0523 03:53:19.357213 35003 solver.cpp:239] Iteration 136500 (2.76008 iter/s, 3.62308s/10 iters), loss = 6.68929
I0523 03:53:19.357259 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68929 (* 1 = 6.68929 loss)
I0523 03:53:20.059592 35003 sgd_solver.cpp:112] Iteration 136500, lr = 0.01
I0523 03:53:22.665578 35003 solver.cpp:239] Iteration 136510 (3.02281 iter/s, 3.30818s/10 iters), loss = 7.18365
I0523 03:53:22.665617 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18365 (* 1 = 7.18365 loss)
I0523 03:53:22.684340 35003 sgd_solver.cpp:112] Iteration 136510, lr = 0.01
I0523 03:53:27.502590 35003 solver.cpp:239] Iteration 136520 (2.0675 iter/s, 4.83677s/10 iters), loss = 6.51299
I0523 03:53:27.502637 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51299 (* 1 = 6.51299 loss)
I0523 03:53:27.509181 35003 sgd_solver.cpp:112] Iteration 136520, lr = 0.01
I0523 03:53:31.025333 35003 solver.cpp:239] Iteration 136530 (2.83886 iter/s, 3.52254s/10 iters), loss = 6.90202
I0523 03:53:31.025388 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90202 (* 1 = 6.90202 loss)
I0523 03:53:31.441283 35003 sgd_solver.cpp:112] Iteration 136530, lr = 0.01
I0523 03:53:36.505794 35003 solver.cpp:239] Iteration 136540 (1.82476 iter/s, 5.48018s/10 iters), loss = 7.44677
I0523 03:53:36.505837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44677 (* 1 = 7.44677 loss)
I0523 03:53:36.513689 35003 sgd_solver.cpp:112] Iteration 136540, lr = 0.01
I0523 03:53:40.130939 35003 solver.cpp:239] Iteration 136550 (2.75866 iter/s, 3.62495s/10 iters), loss = 6.42385
I0523 03:53:40.130975 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42385 (* 1 = 6.42385 loss)
I0523 03:53:40.144029 35003 sgd_solver.cpp:112] Iteration 136550, lr = 0.01
I0523 03:53:42.168426 35003 solver.cpp:239] Iteration 136560 (4.90832 iter/s, 2.03736s/10 iters), loss = 6.79478
I0523 03:53:42.168462 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79478 (* 1 = 6.79478 loss)
I0523 03:53:42.181399 35003 sgd_solver.cpp:112] Iteration 136560, lr = 0.01
I0523 03:53:44.260907 35003 solver.cpp:239] Iteration 136570 (4.77933 iter/s, 2.09235s/10 iters), loss = 6.90323
I0523 03:53:44.260958 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90323 (* 1 = 6.90323 loss)
I0523 03:53:44.297996 35003 sgd_solver.cpp:112] Iteration 136570, lr = 0.01
I0523 03:53:48.649813 35003 solver.cpp:239] Iteration 136580 (2.27859 iter/s, 4.38867s/10 iters), loss = 7.33838
I0523 03:53:48.650063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33838 (* 1 = 7.33838 loss)
I0523 03:53:48.697198 35003 sgd_solver.cpp:112] Iteration 136580, lr = 0.01
I0523 03:53:51.348183 35003 solver.cpp:239] Iteration 136590 (3.70642 iter/s, 2.69802s/10 iters), loss = 6.57007
I0523 03:53:51.348237 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57007 (* 1 = 6.57007 loss)
I0523 03:53:51.355686 35003 sgd_solver.cpp:112] Iteration 136590, lr = 0.01
I0523 03:53:55.683362 35003 solver.cpp:239] Iteration 136600 (2.30683 iter/s, 4.33495s/10 iters), loss = 7.46027
I0523 03:53:55.683408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46027 (* 1 = 7.46027 loss)
I0523 03:53:55.706607 35003 sgd_solver.cpp:112] Iteration 136600, lr = 0.01
I0523 03:53:58.612349 35003 solver.cpp:239] Iteration 136610 (3.41436 iter/s, 2.92881s/10 iters), loss = 6.04995
I0523 03:53:58.612401 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04995 (* 1 = 6.04995 loss)
I0523 03:53:59.353111 35003 sgd_solver.cpp:112] Iteration 136610, lr = 0.01
I0523 03:54:02.201321 35003 solver.cpp:239] Iteration 136620 (2.78647 iter/s, 3.58877s/10 iters), loss = 6.0044
I0523 03:54:02.201370 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0044 (* 1 = 6.0044 loss)
I0523 03:54:02.214489 35003 sgd_solver.cpp:112] Iteration 136620, lr = 0.01
I0523 03:54:06.630970 35003 solver.cpp:239] Iteration 136630 (2.25763 iter/s, 4.42942s/10 iters), loss = 7.48097
I0523 03:54:06.631011 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48097 (* 1 = 7.48097 loss)
I0523 03:54:06.637917 35003 sgd_solver.cpp:112] Iteration 136630, lr = 0.01
I0523 03:54:08.719283 35003 solver.cpp:239] Iteration 136640 (4.78888 iter/s, 2.08817s/10 iters), loss = 6.93544
I0523 03:54:08.719337 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93544 (* 1 = 6.93544 loss)
I0523 03:54:08.727574 35003 sgd_solver.cpp:112] Iteration 136640, lr = 0.01
I0523 03:54:11.914615 35003 solver.cpp:239] Iteration 136650 (3.12975 iter/s, 3.19515s/10 iters), loss = 6.09128
I0523 03:54:11.914664 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09128 (* 1 = 6.09128 loss)
I0523 03:54:12.655999 35003 sgd_solver.cpp:112] Iteration 136650, lr = 0.01
I0523 03:54:16.172163 35003 solver.cpp:239] Iteration 136660 (2.34889 iter/s, 4.25733s/10 iters), loss = 6.6152
I0523 03:54:16.172200 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6152 (* 1 = 6.6152 loss)
I0523 03:54:16.177976 35003 sgd_solver.cpp:112] Iteration 136660, lr = 0.01
I0523 03:54:18.291328 35003 solver.cpp:239] Iteration 136670 (4.71913 iter/s, 2.11903s/10 iters), loss = 7.15351
I0523 03:54:18.291366 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15351 (* 1 = 7.15351 loss)
I0523 03:54:18.293004 35003 sgd_solver.cpp:112] Iteration 136670, lr = 0.01
I0523 03:54:20.886749 35003 solver.cpp:239] Iteration 136680 (3.85316 iter/s, 2.59527s/10 iters), loss = 6.82641
I0523 03:54:20.887007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82641 (* 1 = 6.82641 loss)
I0523 03:54:20.895896 35003 sgd_solver.cpp:112] Iteration 136680, lr = 0.01
I0523 03:54:21.796123 35003 solver.cpp:239] Iteration 136690 (11.0004 iter/s, 0.909059s/10 iters), loss = 6.44764
I0523 03:54:21.796161 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44764 (* 1 = 6.44764 loss)
I0523 03:54:21.803843 35003 sgd_solver.cpp:112] Iteration 136690, lr = 0.01
I0523 03:54:25.520623 35003 solver.cpp:239] Iteration 136700 (2.68506 iter/s, 3.72431s/10 iters), loss = 5.69792
I0523 03:54:25.520668 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.69792 (* 1 = 5.69792 loss)
I0523 03:54:26.089237 35003 sgd_solver.cpp:112] Iteration 136700, lr = 0.01
I0523 03:54:28.955322 35003 solver.cpp:239] Iteration 136710 (2.91164 iter/s, 3.43449s/10 iters), loss = 6.58493
I0523 03:54:28.955375 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58493 (* 1 = 6.58493 loss)
I0523 03:54:29.595506 35003 sgd_solver.cpp:112] Iteration 136710, lr = 0.01
I0523 03:54:33.194871 35003 solver.cpp:239] Iteration 136720 (2.35887 iter/s, 4.23933s/10 iters), loss = 6.50709
I0523 03:54:33.194912 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50709 (* 1 = 6.50709 loss)
I0523 03:54:33.207108 35003 sgd_solver.cpp:112] Iteration 136720, lr = 0.01
I0523 03:54:36.099154 35003 solver.cpp:239] Iteration 136730 (3.44338 iter/s, 2.90412s/10 iters), loss = 7.22418
I0523 03:54:36.099190 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22418 (* 1 = 7.22418 loss)
I0523 03:54:36.117295 35003 sgd_solver.cpp:112] Iteration 136730, lr = 0.01
I0523 03:54:40.005195 35003 solver.cpp:239] Iteration 136740 (2.56027 iter/s, 3.90584s/10 iters), loss = 7.83975
I0523 03:54:40.005235 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83975 (* 1 = 7.83975 loss)
I0523 03:54:40.714165 35003 sgd_solver.cpp:112] Iteration 136740, lr = 0.01
I0523 03:54:43.913254 35003 solver.cpp:239] Iteration 136750 (2.55896 iter/s, 3.90784s/10 iters), loss = 8.01499
I0523 03:54:43.913292 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01499 (* 1 = 8.01499 loss)
I0523 03:54:43.926268 35003 sgd_solver.cpp:112] Iteration 136750, lr = 0.01
I0523 03:54:48.317210 35003 solver.cpp:239] Iteration 136760 (2.2708 iter/s, 4.40373s/10 iters), loss = 6.36664
I0523 03:54:48.317260 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36664 (* 1 = 6.36664 loss)
I0523 03:54:48.321800 35003 sgd_solver.cpp:112] Iteration 136760, lr = 0.01
I0523 03:54:51.753429 35003 solver.cpp:239] Iteration 136770 (2.91035 iter/s, 3.43602s/10 iters), loss = 7.43541
I0523 03:54:51.753672 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43541 (* 1 = 7.43541 loss)
I0523 03:54:51.939118 35003 sgd_solver.cpp:112] Iteration 136770, lr = 0.01
I0523 03:54:55.495857 35003 solver.cpp:239] Iteration 136780 (2.67233 iter/s, 3.74206s/10 iters), loss = 6.90926
I0523 03:54:55.495899 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90926 (* 1 = 6.90926 loss)
I0523 03:54:55.520792 35003 sgd_solver.cpp:112] Iteration 136780, lr = 0.01
I0523 03:54:59.236654 35003 solver.cpp:239] Iteration 136790 (2.67337 iter/s, 3.74059s/10 iters), loss = 6.38562
I0523 03:54:59.236704 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38562 (* 1 = 6.38562 loss)
I0523 03:54:59.958463 35003 sgd_solver.cpp:112] Iteration 136790, lr = 0.01
I0523 03:55:04.215155 35003 solver.cpp:239] Iteration 136800 (2.00874 iter/s, 4.97824s/10 iters), loss = 7.46668
I0523 03:55:04.215209 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46668 (* 1 = 7.46668 loss)
I0523 03:55:04.587504 35003 sgd_solver.cpp:112] Iteration 136800, lr = 0.01
I0523 03:55:08.078444 35003 solver.cpp:239] Iteration 136810 (2.58862 iter/s, 3.86307s/10 iters), loss = 7.9411
I0523 03:55:08.078505 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9411 (* 1 = 7.9411 loss)
I0523 03:55:08.080833 35003 sgd_solver.cpp:112] Iteration 136810, lr = 0.01
I0523 03:55:10.157143 35003 solver.cpp:239] Iteration 136820 (4.81105 iter/s, 2.07855s/10 iters), loss = 7.70753
I0523 03:55:10.157184 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70753 (* 1 = 7.70753 loss)
I0523 03:55:10.168081 35003 sgd_solver.cpp:112] Iteration 136820, lr = 0.01
I0523 03:55:14.107694 35003 solver.cpp:239] Iteration 136830 (2.53142 iter/s, 3.95035s/10 iters), loss = 7.41009
I0523 03:55:14.107743 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41009 (* 1 = 7.41009 loss)
I0523 03:55:14.112951 35003 sgd_solver.cpp:112] Iteration 136830, lr = 0.01
I0523 03:55:18.388150 35003 solver.cpp:239] Iteration 136840 (2.33632 iter/s, 4.28024s/10 iters), loss = 8.02532
I0523 03:55:18.388188 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02532 (* 1 = 8.02532 loss)
I0523 03:55:18.392997 35003 sgd_solver.cpp:112] Iteration 136840, lr = 0.01
I0523 03:55:22.258132 35003 solver.cpp:239] Iteration 136850 (2.58412 iter/s, 3.86978s/10 iters), loss = 6.64385
I0523 03:55:22.258355 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64385 (* 1 = 6.64385 loss)
I0523 03:55:22.270212 35003 sgd_solver.cpp:112] Iteration 136850, lr = 0.01
I0523 03:55:25.662991 35003 solver.cpp:239] Iteration 136860 (2.93727 iter/s, 3.40452s/10 iters), loss = 7.01217
I0523 03:55:25.663033 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01217 (* 1 = 7.01217 loss)
I0523 03:55:26.377480 35003 sgd_solver.cpp:112] Iteration 136860, lr = 0.01
I0523 03:55:29.394469 35003 solver.cpp:239] Iteration 136870 (2.68005 iter/s, 3.73127s/10 iters), loss = 7.57951
I0523 03:55:29.394524 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57951 (* 1 = 7.57951 loss)
I0523 03:55:30.128994 35003 sgd_solver.cpp:112] Iteration 136870, lr = 0.01
I0523 03:55:33.947963 35003 solver.cpp:239] Iteration 136880 (2.19623 iter/s, 4.55325s/10 iters), loss = 6.98315
I0523 03:55:33.948009 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98315 (* 1 = 6.98315 loss)
I0523 03:55:33.956176 35003 sgd_solver.cpp:112] Iteration 136880, lr = 0.01
I0523 03:55:36.582296 35003 solver.cpp:239] Iteration 136890 (3.79627 iter/s, 2.63417s/10 iters), loss = 7.55162
I0523 03:55:36.582353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55162 (* 1 = 7.55162 loss)
I0523 03:55:36.594626 35003 sgd_solver.cpp:112] Iteration 136890, lr = 0.01
I0523 03:55:38.902034 35003 solver.cpp:239] Iteration 136900 (4.31112 iter/s, 2.31958s/10 iters), loss = 6.78503
I0523 03:55:38.902092 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78503 (* 1 = 6.78503 loss)
I0523 03:55:39.600131 35003 sgd_solver.cpp:112] Iteration 136900, lr = 0.01
I0523 03:55:43.884739 35003 solver.cpp:239] Iteration 136910 (2.00705 iter/s, 4.98244s/10 iters), loss = 6.99701
I0523 03:55:43.884795 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99701 (* 1 = 6.99701 loss)
I0523 03:55:44.625730 35003 sgd_solver.cpp:112] Iteration 136910, lr = 0.01
I0523 03:55:46.644750 35003 solver.cpp:239] Iteration 136920 (3.6234 iter/s, 2.75984s/10 iters), loss = 7.04843
I0523 03:55:46.644804 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04843 (* 1 = 7.04843 loss)
I0523 03:55:47.376662 35003 sgd_solver.cpp:112] Iteration 136920, lr = 0.01
I0523 03:55:50.236233 35003 solver.cpp:239] Iteration 136930 (2.78452 iter/s, 3.59128s/10 iters), loss = 7.21106
I0523 03:55:50.236275 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21106 (* 1 = 7.21106 loss)
I0523 03:55:50.977277 35003 sgd_solver.cpp:112] Iteration 136930, lr = 0.01
I0523 03:55:54.056907 35003 solver.cpp:239] Iteration 136940 (2.61748 iter/s, 3.82047s/10 iters), loss = 7.58649
I0523 03:55:54.057063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58649 (* 1 = 7.58649 loss)
I0523 03:55:54.073010 35003 sgd_solver.cpp:112] Iteration 136940, lr = 0.01
I0523 03:55:56.156038 35003 solver.cpp:239] Iteration 136950 (4.76444 iter/s, 2.09888s/10 iters), loss = 6.59525
I0523 03:55:56.156088 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59525 (* 1 = 6.59525 loss)
I0523 03:55:56.834678 35003 sgd_solver.cpp:112] Iteration 136950, lr = 0.01
I0523 03:56:01.622895 35003 solver.cpp:239] Iteration 136960 (1.8293 iter/s, 5.46658s/10 iters), loss = 7.26734
I0523 03:56:01.622947 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26734 (* 1 = 7.26734 loss)
I0523 03:56:01.634531 35003 sgd_solver.cpp:112] Iteration 136960, lr = 0.01
I0523 03:56:05.515828 35003 solver.cpp:239] Iteration 136970 (2.5689 iter/s, 3.89272s/10 iters), loss = 7.13954
I0523 03:56:05.515902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13954 (* 1 = 7.13954 loss)
I0523 03:56:05.806592 35003 sgd_solver.cpp:112] Iteration 136970, lr = 0.01
I0523 03:56:09.001346 35003 solver.cpp:239] Iteration 136980 (2.86919 iter/s, 3.48531s/10 iters), loss = 5.22047
I0523 03:56:09.001390 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.22047 (* 1 = 5.22047 loss)
I0523 03:56:09.197022 35003 sgd_solver.cpp:112] Iteration 136980, lr = 0.01
I0523 03:56:11.567736 35003 solver.cpp:239] Iteration 136990 (3.89676 iter/s, 2.56624s/10 iters), loss = 7.22745
I0523 03:56:11.567778 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22745 (* 1 = 7.22745 loss)
I0523 03:56:12.302847 35003 sgd_solver.cpp:112] Iteration 136990, lr = 0.01
I0523 03:56:15.767421 35003 solver.cpp:239] Iteration 137000 (2.38125 iter/s, 4.19947s/10 iters), loss = 6.80171
I0523 03:56:15.767469 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80171 (* 1 = 6.80171 loss)
I0523 03:56:15.789309 35003 sgd_solver.cpp:112] Iteration 137000, lr = 0.01
I0523 03:56:17.883833 35003 solver.cpp:239] Iteration 137010 (4.72529 iter/s, 2.11627s/10 iters), loss = 6.996
I0523 03:56:17.883874 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.996 (* 1 = 6.996 loss)
I0523 03:56:18.079458 35003 sgd_solver.cpp:112] Iteration 137010, lr = 0.01
I0523 03:56:21.605020 35003 solver.cpp:239] Iteration 137020 (2.68746 iter/s, 3.72099s/10 iters), loss = 7.2384
I0523 03:56:21.605064 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2384 (* 1 = 7.2384 loss)
I0523 03:56:21.618008 35003 sgd_solver.cpp:112] Iteration 137020, lr = 0.01
I0523 03:56:24.311406 35003 solver.cpp:239] Iteration 137030 (3.69518 iter/s, 2.70623s/10 iters), loss = 7.80092
I0523 03:56:24.311684 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80092 (* 1 = 7.80092 loss)
I0523 03:56:24.324564 35003 sgd_solver.cpp:112] Iteration 137030, lr = 0.01
I0523 03:56:29.308288 35003 solver.cpp:239] Iteration 137040 (2.00143 iter/s, 4.99642s/10 iters), loss = 8.04867
I0523 03:56:29.308343 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04867 (* 1 = 8.04867 loss)
I0523 03:56:29.979673 35003 sgd_solver.cpp:112] Iteration 137040, lr = 0.01
I0523 03:56:33.525903 35003 solver.cpp:239] Iteration 137050 (2.37115 iter/s, 4.21737s/10 iters), loss = 8.08103
I0523 03:56:33.525940 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08103 (* 1 = 8.08103 loss)
I0523 03:56:33.539547 35003 sgd_solver.cpp:112] Iteration 137050, lr = 0.01
I0523 03:56:37.708036 35003 solver.cpp:239] Iteration 137060 (2.39124 iter/s, 4.18192s/10 iters), loss = 6.20699
I0523 03:56:37.708086 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20699 (* 1 = 6.20699 loss)
I0523 03:56:37.720800 35003 sgd_solver.cpp:112] Iteration 137060, lr = 0.01
I0523 03:56:39.814102 35003 solver.cpp:239] Iteration 137070 (4.74852 iter/s, 2.10592s/10 iters), loss = 6.55066
I0523 03:56:39.814144 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55066 (* 1 = 6.55066 loss)
I0523 03:56:39.827584 35003 sgd_solver.cpp:112] Iteration 137070, lr = 0.01
I0523 03:56:42.747754 35003 solver.cpp:239] Iteration 137080 (3.40892 iter/s, 2.93348s/10 iters), loss = 6.46078
I0523 03:56:42.747793 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46078 (* 1 = 6.46078 loss)
I0523 03:56:42.760365 35003 sgd_solver.cpp:112] Iteration 137080, lr = 0.01
I0523 03:56:46.391800 35003 solver.cpp:239] Iteration 137090 (2.74435 iter/s, 3.64385s/10 iters), loss = 6.00073
I0523 03:56:46.391844 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00073 (* 1 = 6.00073 loss)
I0523 03:56:46.397125 35003 sgd_solver.cpp:112] Iteration 137090, lr = 0.01
I0523 03:56:49.984606 35003 solver.cpp:239] Iteration 137100 (2.7835 iter/s, 3.5926s/10 iters), loss = 7.25933
I0523 03:56:49.984661 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25933 (* 1 = 7.25933 loss)
I0523 03:56:49.990468 35003 sgd_solver.cpp:112] Iteration 137100, lr = 0.01
I0523 03:56:52.094184 35003 solver.cpp:239] Iteration 137110 (4.7406 iter/s, 2.10944s/10 iters), loss = 7.1577
I0523 03:56:52.094225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1577 (* 1 = 7.1577 loss)
I0523 03:56:52.118935 35003 sgd_solver.cpp:112] Iteration 137110, lr = 0.01
I0523 03:56:56.335033 35003 solver.cpp:239] Iteration 137120 (2.35814 iter/s, 4.24063s/10 iters), loss = 7.42152
I0523 03:56:56.335235 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42152 (* 1 = 7.42152 loss)
I0523 03:56:56.348047 35003 sgd_solver.cpp:112] Iteration 137120, lr = 0.01
I0523 03:56:58.566475 35003 solver.cpp:239] Iteration 137130 (4.48199 iter/s, 2.23115s/10 iters), loss = 7.37569
I0523 03:56:58.566521 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37569 (* 1 = 7.37569 loss)
I0523 03:56:58.579394 35003 sgd_solver.cpp:112] Iteration 137130, lr = 0.01
I0523 03:57:01.336668 35003 solver.cpp:239] Iteration 137140 (3.61007 iter/s, 2.77003s/10 iters), loss = 6.486
I0523 03:57:01.336717 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.486 (* 1 = 6.486 loss)
I0523 03:57:01.344362 35003 sgd_solver.cpp:112] Iteration 137140, lr = 0.01
I0523 03:57:05.609421 35003 solver.cpp:239] Iteration 137150 (2.34055 iter/s, 4.27249s/10 iters), loss = 6.94679
I0523 03:57:05.609489 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94679 (* 1 = 6.94679 loss)
I0523 03:57:05.675773 35003 sgd_solver.cpp:112] Iteration 137150, lr = 0.01
I0523 03:57:09.952366 35003 solver.cpp:239] Iteration 137160 (2.30271 iter/s, 4.34271s/10 iters), loss = 7.29675
I0523 03:57:09.952404 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29675 (* 1 = 7.29675 loss)
I0523 03:57:09.965495 35003 sgd_solver.cpp:112] Iteration 137160, lr = 0.01
I0523 03:57:13.943027 35003 solver.cpp:239] Iteration 137170 (2.50598 iter/s, 3.99045s/10 iters), loss = 7.41051
I0523 03:57:13.943084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41051 (* 1 = 7.41051 loss)
I0523 03:57:13.946475 35003 sgd_solver.cpp:112] Iteration 137170, lr = 0.01
I0523 03:57:17.629647 35003 solver.cpp:239] Iteration 137180 (2.71267 iter/s, 3.6864s/10 iters), loss = 5.7625
I0523 03:57:17.629709 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7625 (* 1 = 5.7625 loss)
I0523 03:57:18.246343 35003 sgd_solver.cpp:112] Iteration 137180, lr = 0.01
I0523 03:57:22.476286 35003 solver.cpp:239] Iteration 137190 (2.0634 iter/s, 4.84638s/10 iters), loss = 7.04329
I0523 03:57:22.476327 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04329 (* 1 = 7.04329 loss)
I0523 03:57:22.492852 35003 sgd_solver.cpp:112] Iteration 137190, lr = 0.01
I0523 03:57:26.066258 35003 solver.cpp:239] Iteration 137200 (2.7857 iter/s, 3.58977s/10 iters), loss = 6.40245
I0523 03:57:26.066323 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40245 (* 1 = 6.40245 loss)
I0523 03:57:26.767961 35003 sgd_solver.cpp:112] Iteration 137200, lr = 0.01
I0523 03:57:29.190516 35003 solver.cpp:239] Iteration 137210 (3.20095 iter/s, 3.12407s/10 iters), loss = 7.38992
I0523 03:57:29.190556 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38992 (* 1 = 7.38992 loss)
I0523 03:57:29.203553 35003 sgd_solver.cpp:112] Iteration 137210, lr = 0.01
I0523 03:57:32.756882 35003 solver.cpp:239] Iteration 137220 (2.80413 iter/s, 3.56617s/10 iters), loss = 8.21613
I0523 03:57:32.756927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.21613 (* 1 = 8.21613 loss)
I0523 03:57:32.770161 35003 sgd_solver.cpp:112] Iteration 137220, lr = 0.01
I0523 03:57:37.048436 35003 solver.cpp:239] Iteration 137230 (2.33028 iter/s, 4.29133s/10 iters), loss = 7.3623
I0523 03:57:37.048487 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3623 (* 1 = 7.3623 loss)
I0523 03:57:37.061632 35003 sgd_solver.cpp:112] Iteration 137230, lr = 0.01
I0523 03:57:41.171443 35003 solver.cpp:239] Iteration 137240 (2.42555 iter/s, 4.12278s/10 iters), loss = 6.92246
I0523 03:57:41.171492 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92246 (* 1 = 6.92246 loss)
I0523 03:57:41.184375 35003 sgd_solver.cpp:112] Iteration 137240, lr = 0.01
I0523 03:57:46.173493 35003 solver.cpp:239] Iteration 137250 (1.99928 iter/s, 5.0018s/10 iters), loss = 7.69342
I0523 03:57:46.173545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69342 (* 1 = 7.69342 loss)
I0523 03:57:46.559511 35003 sgd_solver.cpp:112] Iteration 137250, lr = 0.01
I0523 03:57:49.770074 35003 solver.cpp:239] Iteration 137260 (2.78058 iter/s, 3.59637s/10 iters), loss = 7.53647
I0523 03:57:49.770117 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53647 (* 1 = 7.53647 loss)
I0523 03:57:49.778713 35003 sgd_solver.cpp:112] Iteration 137260, lr = 0.01
I0523 03:57:53.369205 35003 solver.cpp:239] Iteration 137270 (2.7786 iter/s, 3.59893s/10 iters), loss = 6.57788
I0523 03:57:53.369259 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57788 (* 1 = 6.57788 loss)
I0523 03:57:54.103704 35003 sgd_solver.cpp:112] Iteration 137270, lr = 0.01
I0523 03:57:57.643929 35003 solver.cpp:239] Iteration 137280 (2.33946 iter/s, 4.27449s/10 iters), loss = 7.60354
I0523 03:57:57.644201 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60354 (* 1 = 7.60354 loss)
I0523 03:57:57.656934 35003 sgd_solver.cpp:112] Iteration 137280, lr = 0.01
I0523 03:58:00.504523 35003 solver.cpp:239] Iteration 137290 (3.49624 iter/s, 2.86021s/10 iters), loss = 6.69292
I0523 03:58:00.504562 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69292 (* 1 = 6.69292 loss)
I0523 03:58:00.511126 35003 sgd_solver.cpp:112] Iteration 137290, lr = 0.01
I0523 03:58:04.115309 35003 solver.cpp:239] Iteration 137300 (2.76963 iter/s, 3.61059s/10 iters), loss = 6.79227
I0523 03:58:04.115370 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79227 (* 1 = 6.79227 loss)
I0523 03:58:04.133832 35003 sgd_solver.cpp:112] Iteration 137300, lr = 0.01
I0523 03:58:08.456166 35003 solver.cpp:239] Iteration 137310 (2.30382 iter/s, 4.34062s/10 iters), loss = 6.92025
I0523 03:58:08.456298 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92025 (* 1 = 6.92025 loss)
I0523 03:58:08.462498 35003 sgd_solver.cpp:112] Iteration 137310, lr = 0.01
I0523 03:58:12.061915 35003 solver.cpp:239] Iteration 137320 (2.77356 iter/s, 3.60547s/10 iters), loss = 8.37617
I0523 03:58:12.061961 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.37617 (* 1 = 8.37617 loss)
I0523 03:58:12.075639 35003 sgd_solver.cpp:112] Iteration 137320, lr = 0.01
I0523 03:58:14.920070 35003 solver.cpp:239] Iteration 137330 (3.49897 iter/s, 2.85798s/10 iters), loss = 7.16176
I0523 03:58:14.920114 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16176 (* 1 = 7.16176 loss)
I0523 03:58:15.655556 35003 sgd_solver.cpp:112] Iteration 137330, lr = 0.01
I0523 03:58:18.546139 35003 solver.cpp:239] Iteration 137340 (2.75796 iter/s, 3.62587s/10 iters), loss = 7.07757
I0523 03:58:18.546195 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07757 (* 1 = 7.07757 loss)
I0523 03:58:19.092811 35003 sgd_solver.cpp:112] Iteration 137340, lr = 0.01
I0523 03:58:20.409054 35003 solver.cpp:239] Iteration 137350 (5.36834 iter/s, 1.86277s/10 iters), loss = 6.82805
I0523 03:58:20.409099 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82805 (* 1 = 6.82805 loss)
I0523 03:58:20.418807 35003 sgd_solver.cpp:112] Iteration 137350, lr = 0.01
I0523 03:58:22.637708 35003 solver.cpp:239] Iteration 137360 (4.4873 iter/s, 2.22851s/10 iters), loss = 7.25828
I0523 03:58:22.637758 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25828 (* 1 = 7.25828 loss)
I0523 03:58:23.208194 35003 sgd_solver.cpp:112] Iteration 137360, lr = 0.01
I0523 03:58:26.099839 35003 solver.cpp:239] Iteration 137370 (2.88855 iter/s, 3.46194s/10 iters), loss = 6.89871
I0523 03:58:26.099887 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89871 (* 1 = 6.89871 loss)
I0523 03:58:26.821820 35003 sgd_solver.cpp:112] Iteration 137370, lr = 0.01
I0523 03:58:30.422596 35003 solver.cpp:239] Iteration 137380 (2.31346 iter/s, 4.32253s/10 iters), loss = 7.27148
I0523 03:58:30.422780 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27148 (* 1 = 7.27148 loss)
I0523 03:58:30.431840 35003 sgd_solver.cpp:112] Iteration 137380, lr = 0.01
I0523 03:58:32.541628 35003 solver.cpp:239] Iteration 137390 (4.71971 iter/s, 2.11878s/10 iters), loss = 7.19057
I0523 03:58:32.541677 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19057 (* 1 = 7.19057 loss)
I0523 03:58:32.554781 35003 sgd_solver.cpp:112] Iteration 137390, lr = 0.01
I0523 03:58:37.646652 35003 solver.cpp:239] Iteration 137400 (1.95896 iter/s, 5.10476s/10 iters), loss = 6.40165
I0523 03:58:37.646721 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40165 (* 1 = 6.40165 loss)
I0523 03:58:37.659260 35003 sgd_solver.cpp:112] Iteration 137400, lr = 0.01
I0523 03:58:41.965031 35003 solver.cpp:239] Iteration 137410 (2.3158 iter/s, 4.31816s/10 iters), loss = 6.71887
I0523 03:58:41.965080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71887 (* 1 = 6.71887 loss)
I0523 03:58:41.971498 35003 sgd_solver.cpp:112] Iteration 137410, lr = 0.01
I0523 03:58:44.885046 35003 solver.cpp:239] Iteration 137420 (3.42485 iter/s, 2.91984s/10 iters), loss = 7.21482
I0523 03:58:44.885102 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21482 (* 1 = 7.21482 loss)
I0523 03:58:45.555366 35003 sgd_solver.cpp:112] Iteration 137420, lr = 0.01
I0523 03:58:48.429388 35003 solver.cpp:239] Iteration 137430 (2.82156 iter/s, 3.54414s/10 iters), loss = 7.40439
I0523 03:58:48.429437 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40439 (* 1 = 7.40439 loss)
I0523 03:58:48.869472 35003 sgd_solver.cpp:112] Iteration 137430, lr = 0.01
I0523 03:58:52.460618 35003 solver.cpp:239] Iteration 137440 (2.48077 iter/s, 4.03101s/10 iters), loss = 6.79117
I0523 03:58:52.460662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79117 (* 1 = 6.79117 loss)
I0523 03:58:52.473577 35003 sgd_solver.cpp:112] Iteration 137440, lr = 0.01
I0523 03:58:54.515317 35003 solver.cpp:239] Iteration 137450 (4.86723 iter/s, 2.05456s/10 iters), loss = 7.19332
I0523 03:58:54.515369 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19332 (* 1 = 7.19332 loss)
I0523 03:58:54.528686 35003 sgd_solver.cpp:112] Iteration 137450, lr = 0.01
I0523 03:58:58.116557 35003 solver.cpp:239] Iteration 137460 (2.77697 iter/s, 3.60104s/10 iters), loss = 6.92272
I0523 03:58:58.116602 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92272 (* 1 = 6.92272 loss)
I0523 03:58:58.129838 35003 sgd_solver.cpp:112] Iteration 137460, lr = 0.01
I0523 03:59:00.937903 35003 solver.cpp:239] Iteration 137470 (3.54462 iter/s, 2.82118s/10 iters), loss = 5.6447
I0523 03:59:00.938212 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.6447 (* 1 = 5.6447 loss)
I0523 03:59:00.950978 35003 sgd_solver.cpp:112] Iteration 137470, lr = 0.01
I0523 03:59:04.672683 35003 solver.cpp:239] Iteration 137480 (2.67784 iter/s, 3.73435s/10 iters), loss = 6.46473
I0523 03:59:04.672730 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46473 (* 1 = 6.46473 loss)
I0523 03:59:04.958071 35003 sgd_solver.cpp:112] Iteration 137480, lr = 0.01
I0523 03:59:07.721539 35003 solver.cpp:239] Iteration 137490 (3.28011 iter/s, 3.04868s/10 iters), loss = 6.73211
I0523 03:59:07.721585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73211 (* 1 = 6.73211 loss)
I0523 03:59:07.739313 35003 sgd_solver.cpp:112] Iteration 137490, lr = 0.01
I0523 03:59:11.229322 35003 solver.cpp:239] Iteration 137500 (2.85096 iter/s, 3.50759s/10 iters), loss = 7.56211
I0523 03:59:11.229362 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56211 (* 1 = 7.56211 loss)
I0523 03:59:11.394400 35003 sgd_solver.cpp:112] Iteration 137500, lr = 0.01
I0523 03:59:15.854074 35003 solver.cpp:239] Iteration 137510 (2.16239 iter/s, 4.62451s/10 iters), loss = 7.13577
I0523 03:59:15.854138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13577 (* 1 = 7.13577 loss)
I0523 03:59:16.513499 35003 sgd_solver.cpp:112] Iteration 137510, lr = 0.01
I0523 03:59:19.431699 35003 solver.cpp:239] Iteration 137520 (2.79531 iter/s, 3.57742s/10 iters), loss = 7.66439
I0523 03:59:19.431746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66439 (* 1 = 7.66439 loss)
I0523 03:59:20.163954 35003 sgd_solver.cpp:112] Iteration 137520, lr = 0.01
I0523 03:59:23.751672 35003 solver.cpp:239] Iteration 137530 (2.31496 iter/s, 4.31973s/10 iters), loss = 7.13233
I0523 03:59:23.751729 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13233 (* 1 = 7.13233 loss)
I0523 03:59:23.756538 35003 sgd_solver.cpp:112] Iteration 137530, lr = 0.01
I0523 03:59:26.616505 35003 solver.cpp:239] Iteration 137540 (3.49082 iter/s, 2.86466s/10 iters), loss = 7.13271
I0523 03:59:26.616550 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13271 (* 1 = 7.13271 loss)
I0523 03:59:26.624171 35003 sgd_solver.cpp:112] Iteration 137540, lr = 0.01
I0523 03:59:31.538731 35003 solver.cpp:239] Iteration 137550 (2.03171 iter/s, 4.92195s/10 iters), loss = 7.24888
I0523 03:59:31.538982 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24888 (* 1 = 7.24888 loss)
I0523 03:59:32.254387 35003 sgd_solver.cpp:112] Iteration 137550, lr = 0.01
I0523 03:59:34.857234 35003 solver.cpp:239] Iteration 137560 (3.01374 iter/s, 3.31814s/10 iters), loss = 7.33993
I0523 03:59:34.857275 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33993 (* 1 = 7.33993 loss)
I0523 03:59:34.870440 35003 sgd_solver.cpp:112] Iteration 137560, lr = 0.01
I0523 03:59:38.342038 35003 solver.cpp:239] Iteration 137570 (2.86976 iter/s, 3.48462s/10 iters), loss = 6.79985
I0523 03:59:38.342077 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79985 (* 1 = 6.79985 loss)
I0523 03:59:38.355584 35003 sgd_solver.cpp:112] Iteration 137570, lr = 0.01
I0523 03:59:41.901903 35003 solver.cpp:239] Iteration 137580 (2.80925 iter/s, 3.55967s/10 iters), loss = 7.23449
I0523 03:59:41.901957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23449 (* 1 = 7.23449 loss)
I0523 03:59:41.907835 35003 sgd_solver.cpp:112] Iteration 137580, lr = 0.01
I0523 03:59:45.651773 35003 solver.cpp:239] Iteration 137590 (2.66691 iter/s, 3.74966s/10 iters), loss = 8.50556
I0523 03:59:45.651827 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.50556 (* 1 = 8.50556 loss)
I0523 03:59:45.662283 35003 sgd_solver.cpp:112] Iteration 137590, lr = 0.01
I0523 03:59:49.180351 35003 solver.cpp:239] Iteration 137600 (2.83416 iter/s, 3.52838s/10 iters), loss = 6.41335
I0523 03:59:49.180400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41335 (* 1 = 6.41335 loss)
I0523 03:59:49.875324 35003 sgd_solver.cpp:112] Iteration 137600, lr = 0.01
I0523 03:59:51.860185 35003 solver.cpp:239] Iteration 137610 (3.7318 iter/s, 2.67967s/10 iters), loss = 6.90378
I0523 03:59:51.860236 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90378 (* 1 = 6.90378 loss)
I0523 03:59:52.584275 35003 sgd_solver.cpp:112] Iteration 137610, lr = 0.01
I0523 03:59:56.980605 35003 solver.cpp:239] Iteration 137620 (1.95306 iter/s, 5.12016s/10 iters), loss = 6.36627
I0523 03:59:56.980657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36627 (* 1 = 6.36627 loss)
I0523 03:59:57.276489 35003 sgd_solver.cpp:112] Iteration 137620, lr = 0.01
I0523 03:59:59.727149 35003 solver.cpp:239] Iteration 137630 (3.64117 iter/s, 2.74637s/10 iters), loss = 7.01406
I0523 03:59:59.727188 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01406 (* 1 = 7.01406 loss)
I0523 03:59:59.734576 35003 sgd_solver.cpp:112] Iteration 137630, lr = 0.01
I0523 04:00:03.463404 35003 solver.cpp:239] Iteration 137640 (2.67662 iter/s, 3.73606s/10 iters), loss = 7.6695
I0523 04:00:03.463644 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6695 (* 1 = 7.6695 loss)
I0523 04:00:03.467831 35003 sgd_solver.cpp:112] Iteration 137640, lr = 0.01
I0523 04:00:06.310353 35003 solver.cpp:239] Iteration 137650 (3.51295 iter/s, 2.84661s/10 iters), loss = 7.38664
I0523 04:00:06.310415 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38664 (* 1 = 7.38664 loss)
I0523 04:00:06.318382 35003 sgd_solver.cpp:112] Iteration 137650, lr = 0.01
I0523 04:00:08.387693 35003 solver.cpp:239] Iteration 137660 (4.81419 iter/s, 2.07719s/10 iters), loss = 6.80668
I0523 04:00:08.387740 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80668 (* 1 = 6.80668 loss)
I0523 04:00:08.396497 35003 sgd_solver.cpp:112] Iteration 137660, lr = 0.01
I0523 04:00:12.138273 35003 solver.cpp:239] Iteration 137670 (2.6664 iter/s, 3.75037s/10 iters), loss = 6.77439
I0523 04:00:12.138320 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77439 (* 1 = 6.77439 loss)
I0523 04:00:12.142628 35003 sgd_solver.cpp:112] Iteration 137670, lr = 0.01
I0523 04:00:17.490365 35003 solver.cpp:239] Iteration 137680 (1.86852 iter/s, 5.35182s/10 iters), loss = 6.93007
I0523 04:00:17.490416 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93007 (* 1 = 6.93007 loss)
I0523 04:00:18.198642 35003 sgd_solver.cpp:112] Iteration 137680, lr = 0.01
I0523 04:00:22.493495 35003 solver.cpp:239] Iteration 137690 (1.99887 iter/s, 5.00282s/10 iters), loss = 7.803
I0523 04:00:22.493548 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.803 (* 1 = 7.803 loss)
I0523 04:00:23.205322 35003 sgd_solver.cpp:112] Iteration 137690, lr = 0.01
I0523 04:00:26.954957 35003 solver.cpp:239] Iteration 137700 (2.24153 iter/s, 4.46123s/10 iters), loss = 6.81565
I0523 04:00:26.955005 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81565 (* 1 = 6.81565 loss)
I0523 04:00:26.961057 35003 sgd_solver.cpp:112] Iteration 137700, lr = 0.01
I0523 04:00:28.267271 35003 solver.cpp:239] Iteration 137710 (7.62079 iter/s, 1.3122s/10 iters), loss = 6.07998
I0523 04:00:28.267316 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07998 (* 1 = 6.07998 loss)
I0523 04:00:28.280086 35003 sgd_solver.cpp:112] Iteration 137710, lr = 0.01
I0523 04:00:30.863268 35003 solver.cpp:239] Iteration 137720 (3.85232 iter/s, 2.59584s/10 iters), loss = 6.19248
I0523 04:00:30.863325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19248 (* 1 = 6.19248 loss)
I0523 04:00:31.604228 35003 sgd_solver.cpp:112] Iteration 137720, lr = 0.01
I0523 04:00:34.383251 35003 solver.cpp:239] Iteration 137730 (2.84108 iter/s, 3.51978s/10 iters), loss = 7.09167
I0523 04:00:34.383502 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09167 (* 1 = 7.09167 loss)
I0523 04:00:34.395525 35003 sgd_solver.cpp:112] Iteration 137730, lr = 0.01
I0523 04:00:37.899294 35003 solver.cpp:239] Iteration 137740 (2.84442 iter/s, 3.51565s/10 iters), loss = 6.74333
I0523 04:00:37.899339 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74333 (* 1 = 6.74333 loss)
I0523 04:00:37.917114 35003 sgd_solver.cpp:112] Iteration 137740, lr = 0.01
I0523 04:00:40.865272 35003 solver.cpp:239] Iteration 137750 (3.37176 iter/s, 2.96581s/10 iters), loss = 6.17721
I0523 04:00:40.865315 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17721 (* 1 = 6.17721 loss)
I0523 04:00:40.876459 35003 sgd_solver.cpp:112] Iteration 137750, lr = 0.01
I0523 04:00:44.263281 35003 solver.cpp:239] Iteration 137760 (2.94306 iter/s, 3.39782s/10 iters), loss = 6.69793
I0523 04:00:44.263339 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69793 (* 1 = 6.69793 loss)
I0523 04:00:44.276372 35003 sgd_solver.cpp:112] Iteration 137760, lr = 0.01
I0523 04:00:48.048393 35003 solver.cpp:239] Iteration 137770 (2.64208 iter/s, 3.78489s/10 iters), loss = 6.64239
I0523 04:00:48.048436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64239 (* 1 = 6.64239 loss)
I0523 04:00:48.053668 35003 sgd_solver.cpp:112] Iteration 137770, lr = 0.01
I0523 04:00:51.894186 35003 solver.cpp:239] Iteration 137780 (2.60038 iter/s, 3.84559s/10 iters), loss = 5.71125
I0523 04:00:51.894224 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.71125 (* 1 = 5.71125 loss)
I0523 04:00:51.907665 35003 sgd_solver.cpp:112] Iteration 137780, lr = 0.01
I0523 04:00:56.105420 35003 solver.cpp:239] Iteration 137790 (2.37472 iter/s, 4.21102s/10 iters), loss = 6.41023
I0523 04:00:56.105459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41023 (* 1 = 6.41023 loss)
I0523 04:00:56.112088 35003 sgd_solver.cpp:112] Iteration 137790, lr = 0.01
I0523 04:01:00.333837 35003 solver.cpp:239] Iteration 137800 (2.36508 iter/s, 4.22819s/10 iters), loss = 6.41975
I0523 04:01:00.333899 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41975 (* 1 = 6.41975 loss)
I0523 04:01:01.049229 35003 sgd_solver.cpp:112] Iteration 137800, lr = 0.01
I0523 04:01:03.862560 35003 solver.cpp:239] Iteration 137810 (2.83405 iter/s, 3.52852s/10 iters), loss = 6.97869
I0523 04:01:03.862606 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97869 (* 1 = 6.97869 loss)
I0523 04:01:04.038043 35003 sgd_solver.cpp:112] Iteration 137810, lr = 0.01
I0523 04:01:07.676836 35003 solver.cpp:239] Iteration 137820 (2.62187 iter/s, 3.81407s/10 iters), loss = 7.05041
I0523 04:01:07.677055 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05041 (* 1 = 7.05041 loss)
I0523 04:01:07.689427 35003 sgd_solver.cpp:112] Iteration 137820, lr = 0.01
I0523 04:01:09.012307 35003 solver.cpp:239] Iteration 137830 (7.48954 iter/s, 1.3352s/10 iters), loss = 7.01838
I0523 04:01:09.012346 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01838 (* 1 = 7.01838 loss)
I0523 04:01:09.025518 35003 sgd_solver.cpp:112] Iteration 137830, lr = 0.01
I0523 04:01:14.149447 35003 solver.cpp:239] Iteration 137840 (1.9467 iter/s, 5.13689s/10 iters), loss = 6.87375
I0523 04:01:14.149494 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87375 (* 1 = 6.87375 loss)
I0523 04:01:14.878906 35003 sgd_solver.cpp:112] Iteration 137840, lr = 0.01
I0523 04:01:17.836776 35003 solver.cpp:239] Iteration 137850 (2.71214 iter/s, 3.68713s/10 iters), loss = 7.73636
I0523 04:01:17.836817 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73636 (* 1 = 7.73636 loss)
I0523 04:01:17.843693 35003 sgd_solver.cpp:112] Iteration 137850, lr = 0.01
I0523 04:01:22.336673 35003 solver.cpp:239] Iteration 137860 (2.22239 iter/s, 4.49967s/10 iters), loss = 7.28792
I0523 04:01:22.336720 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28792 (* 1 = 7.28792 loss)
I0523 04:01:22.948679 35003 sgd_solver.cpp:112] Iteration 137860, lr = 0.01
I0523 04:01:25.811909 35003 solver.cpp:239] Iteration 137870 (2.87766 iter/s, 3.47504s/10 iters), loss = 6.05898
I0523 04:01:25.811956 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05898 (* 1 = 6.05898 loss)
I0523 04:01:25.819644 35003 sgd_solver.cpp:112] Iteration 137870, lr = 0.01
I0523 04:01:29.924901 35003 solver.cpp:239] Iteration 137880 (2.43145 iter/s, 4.11277s/10 iters), loss = 6.43433
I0523 04:01:29.924938 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43433 (* 1 = 6.43433 loss)
I0523 04:01:29.932147 35003 sgd_solver.cpp:112] Iteration 137880, lr = 0.01
I0523 04:01:33.530521 35003 solver.cpp:239] Iteration 137890 (2.77359 iter/s, 3.60543s/10 iters), loss = 8.59541
I0523 04:01:33.530568 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.59541 (* 1 = 8.59541 loss)
I0523 04:01:33.543941 35003 sgd_solver.cpp:112] Iteration 137890, lr = 0.01
I0523 04:01:37.000061 35003 solver.cpp:239] Iteration 137900 (2.88239 iter/s, 3.46934s/10 iters), loss = 7.32919
I0523 04:01:37.000113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32919 (* 1 = 7.32919 loss)
I0523 04:01:37.642405 35003 sgd_solver.cpp:112] Iteration 137900, lr = 0.01
I0523 04:01:40.592983 35003 solver.cpp:239] Iteration 137910 (2.78341 iter/s, 3.59272s/10 iters), loss = 6.60839
I0523 04:01:40.593233 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60839 (* 1 = 6.60839 loss)
I0523 04:01:40.603436 35003 sgd_solver.cpp:112] Iteration 137910, lr = 0.01
I0523 04:01:44.162531 35003 solver.cpp:239] Iteration 137920 (2.80177 iter/s, 3.56917s/10 iters), loss = 7.57071
I0523 04:01:44.162578 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57071 (* 1 = 7.57071 loss)
I0523 04:01:44.883601 35003 sgd_solver.cpp:112] Iteration 137920, lr = 0.01
I0523 04:01:47.647475 35003 solver.cpp:239] Iteration 137930 (2.86964 iter/s, 3.48475s/10 iters), loss = 5.86621
I0523 04:01:47.647518 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.86621 (* 1 = 5.86621 loss)
I0523 04:01:48.362994 35003 sgd_solver.cpp:112] Iteration 137930, lr = 0.01
I0523 04:01:51.961872 35003 solver.cpp:239] Iteration 137940 (2.31794 iter/s, 4.31417s/10 iters), loss = 6.05841
I0523 04:01:51.961930 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05841 (* 1 = 6.05841 loss)
I0523 04:01:51.975172 35003 sgd_solver.cpp:112] Iteration 137940, lr = 0.01
I0523 04:01:54.914608 35003 solver.cpp:239] Iteration 137950 (3.3869 iter/s, 2.95256s/10 iters), loss = 6.95551
I0523 04:01:54.914649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95551 (* 1 = 6.95551 loss)
I0523 04:01:55.584812 35003 sgd_solver.cpp:112] Iteration 137950, lr = 0.01
I0523 04:02:00.417003 35003 solver.cpp:239] Iteration 137960 (1.81748 iter/s, 5.50214s/10 iters), loss = 7.13749
I0523 04:02:00.417042 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13749 (* 1 = 7.13749 loss)
I0523 04:02:00.429860 35003 sgd_solver.cpp:112] Iteration 137960, lr = 0.01
I0523 04:02:04.773022 35003 solver.cpp:239] Iteration 137970 (2.29579 iter/s, 4.3558s/10 iters), loss = 7.82149
I0523 04:02:04.773062 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82149 (* 1 = 7.82149 loss)
I0523 04:02:04.786368 35003 sgd_solver.cpp:112] Iteration 137970, lr = 0.01
I0523 04:02:09.206638 35003 solver.cpp:239] Iteration 137980 (2.25561 iter/s, 4.43338s/10 iters), loss = 7.46773
I0523 04:02:09.206718 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46773 (* 1 = 7.46773 loss)
I0523 04:02:09.901929 35003 sgd_solver.cpp:112] Iteration 137980, lr = 0.01
I0523 04:02:14.226140 35003 solver.cpp:239] Iteration 137990 (1.99234 iter/s, 5.01922s/10 iters), loss = 6.47446
I0523 04:02:14.226408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47446 (* 1 = 6.47446 loss)
I0523 04:02:14.239204 35003 sgd_solver.cpp:112] Iteration 137990, lr = 0.01
I0523 04:02:18.355728 35003 solver.cpp:239] Iteration 138000 (2.42179 iter/s, 4.12918s/10 iters), loss = 8.12499
I0523 04:02:18.355780 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.12499 (* 1 = 8.12499 loss)
I0523 04:02:18.369060 35003 sgd_solver.cpp:112] Iteration 138000, lr = 0.01
I0523 04:02:22.495163 35003 solver.cpp:239] Iteration 138010 (2.41592 iter/s, 4.13921s/10 iters), loss = 7.69195
I0523 04:02:22.495203 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69195 (* 1 = 7.69195 loss)
I0523 04:02:22.509138 35003 sgd_solver.cpp:112] Iteration 138010, lr = 0.01
I0523 04:02:24.978477 35003 solver.cpp:239] Iteration 138020 (4.02711 iter/s, 2.48317s/10 iters), loss = 5.95635
I0523 04:02:24.978521 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95635 (* 1 = 5.95635 loss)
I0523 04:02:25.205410 35003 sgd_solver.cpp:112] Iteration 138020, lr = 0.01
I0523 04:02:28.614653 35003 solver.cpp:239] Iteration 138030 (2.75029 iter/s, 3.63598s/10 iters), loss = 6.87937
I0523 04:02:28.614723 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87937 (* 1 = 6.87937 loss)
I0523 04:02:28.965901 35003 sgd_solver.cpp:112] Iteration 138030, lr = 0.01
I0523 04:02:31.051007 35003 solver.cpp:239] Iteration 138040 (4.10474 iter/s, 2.43621s/10 iters), loss = 6.63172
I0523 04:02:31.051051 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63172 (* 1 = 6.63172 loss)
I0523 04:02:31.057065 35003 sgd_solver.cpp:112] Iteration 138040, lr = 0.01
I0523 04:02:33.719619 35003 solver.cpp:239] Iteration 138050 (3.74749 iter/s, 2.66845s/10 iters), loss = 6.21826
I0523 04:02:33.719658 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21826 (* 1 = 6.21826 loss)
I0523 04:02:33.732230 35003 sgd_solver.cpp:112] Iteration 138050, lr = 0.01
I0523 04:02:37.377723 35003 solver.cpp:239] Iteration 138060 (2.7338 iter/s, 3.65791s/10 iters), loss = 6.90513
I0523 04:02:37.377768 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90513 (* 1 = 6.90513 loss)
I0523 04:02:38.080088 35003 sgd_solver.cpp:112] Iteration 138060, lr = 0.01
I0523 04:02:41.001884 35003 solver.cpp:239] Iteration 138070 (2.75941 iter/s, 3.62396s/10 iters), loss = 7.82512
I0523 04:02:41.001924 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82512 (* 1 = 7.82512 loss)
I0523 04:02:41.014904 35003 sgd_solver.cpp:112] Iteration 138070, lr = 0.01
I0523 04:02:44.601285 35003 solver.cpp:239] Iteration 138080 (2.7784 iter/s, 3.59919s/10 iters), loss = 6.47157
I0523 04:02:44.601415 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47157 (* 1 = 6.47157 loss)
I0523 04:02:44.613736 35003 sgd_solver.cpp:112] Iteration 138080, lr = 0.01
I0523 04:02:48.167805 35003 solver.cpp:239] Iteration 138090 (2.80408 iter/s, 3.56623s/10 iters), loss = 6.89866
I0523 04:02:48.167863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89866 (* 1 = 6.89866 loss)
I0523 04:02:48.171679 35003 sgd_solver.cpp:112] Iteration 138090, lr = 0.01
I0523 04:02:50.963179 35003 solver.cpp:239] Iteration 138100 (3.57756 iter/s, 2.7952s/10 iters), loss = 6.5008
I0523 04:02:50.963218 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5008 (* 1 = 6.5008 loss)
I0523 04:02:50.976398 35003 sgd_solver.cpp:112] Iteration 138100, lr = 0.01
I0523 04:02:54.881955 35003 solver.cpp:239] Iteration 138110 (2.55195 iter/s, 3.91857s/10 iters), loss = 8.15642
I0523 04:02:54.882002 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.15642 (* 1 = 8.15642 loss)
I0523 04:02:54.900655 35003 sgd_solver.cpp:112] Iteration 138110, lr = 0.01
I0523 04:02:56.975426 35003 solver.cpp:239] Iteration 138120 (4.77707 iter/s, 2.09333s/10 iters), loss = 5.90167
I0523 04:02:56.975462 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.90167 (* 1 = 5.90167 loss)
I0523 04:02:56.988385 35003 sgd_solver.cpp:112] Iteration 138120, lr = 0.01
I0523 04:03:00.670068 35003 solver.cpp:239] Iteration 138130 (2.70676 iter/s, 3.69445s/10 iters), loss = 6.40101
I0523 04:03:00.670115 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40101 (* 1 = 6.40101 loss)
I0523 04:03:01.384419 35003 sgd_solver.cpp:112] Iteration 138130, lr = 0.01
I0523 04:03:04.986167 35003 solver.cpp:239] Iteration 138140 (2.31703 iter/s, 4.31587s/10 iters), loss = 7.02092
I0523 04:03:04.986209 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02092 (* 1 = 7.02092 loss)
I0523 04:03:04.996372 35003 sgd_solver.cpp:112] Iteration 138140, lr = 0.01
I0523 04:03:07.008569 35003 solver.cpp:239] Iteration 138150 (4.94495 iter/s, 2.02226s/10 iters), loss = 8.35629
I0523 04:03:07.008632 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.35629 (* 1 = 8.35629 loss)
I0523 04:03:07.021800 35003 sgd_solver.cpp:112] Iteration 138150, lr = 0.01
I0523 04:03:11.498667 35003 solver.cpp:239] Iteration 138160 (2.22725 iter/s, 4.48985s/10 iters), loss = 8.1632
I0523 04:03:11.498733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.1632 (* 1 = 8.1632 loss)
I0523 04:03:12.207144 35003 sgd_solver.cpp:112] Iteration 138160, lr = 0.01
I0523 04:03:14.826519 35003 solver.cpp:239] Iteration 138170 (3.00513 iter/s, 3.32764s/10 iters), loss = 7.43052
I0523 04:03:14.826879 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43052 (* 1 = 7.43052 loss)
I0523 04:03:14.839458 35003 sgd_solver.cpp:112] Iteration 138170, lr = 0.01
I0523 04:03:18.362849 35003 solver.cpp:239] Iteration 138180 (2.82819 iter/s, 3.53583s/10 iters), loss = 6.50597
I0523 04:03:18.362922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50597 (* 1 = 6.50597 loss)
I0523 04:03:18.385654 35003 sgd_solver.cpp:112] Iteration 138180, lr = 0.01
I0523 04:03:20.900202 35003 solver.cpp:239] Iteration 138190 (3.94139 iter/s, 2.53718s/10 iters), loss = 6.47556
I0523 04:03:20.900254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47556 (* 1 = 6.47556 loss)
I0523 04:03:20.907739 35003 sgd_solver.cpp:112] Iteration 138190, lr = 0.01
I0523 04:03:22.955230 35003 solver.cpp:239] Iteration 138200 (4.86645 iter/s, 2.05489s/10 iters), loss = 7.0844
I0523 04:03:22.955271 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0844 (* 1 = 7.0844 loss)
I0523 04:03:23.586237 35003 sgd_solver.cpp:112] Iteration 138200, lr = 0.01
I0523 04:03:26.060591 35003 solver.cpp:239] Iteration 138210 (3.22041 iter/s, 3.10519s/10 iters), loss = 6.54894
I0523 04:03:26.060636 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54894 (* 1 = 6.54894 loss)
I0523 04:03:26.769317 35003 sgd_solver.cpp:112] Iteration 138210, lr = 0.01
I0523 04:03:28.774338 35003 solver.cpp:239] Iteration 138220 (3.68516 iter/s, 2.71359s/10 iters), loss = 7.8949
I0523 04:03:28.774374 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8949 (* 1 = 7.8949 loss)
I0523 04:03:28.787658 35003 sgd_solver.cpp:112] Iteration 138220, lr = 0.01
I0523 04:03:32.457830 35003 solver.cpp:239] Iteration 138230 (2.71496 iter/s, 3.68329s/10 iters), loss = 6.83016
I0523 04:03:32.457900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83016 (* 1 = 6.83016 loss)
I0523 04:03:33.198751 35003 sgd_solver.cpp:112] Iteration 138230, lr = 0.01
I0523 04:03:36.085664 35003 solver.cpp:239] Iteration 138240 (2.75663 iter/s, 3.62761s/10 iters), loss = 7.65462
I0523 04:03:36.085722 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65462 (* 1 = 7.65462 loss)
I0523 04:03:36.094266 35003 sgd_solver.cpp:112] Iteration 138240, lr = 0.01
I0523 04:03:39.046258 35003 solver.cpp:239] Iteration 138250 (3.37791 iter/s, 2.96041s/10 iters), loss = 6.84544
I0523 04:03:39.046299 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84544 (* 1 = 6.84544 loss)
I0523 04:03:39.059355 35003 sgd_solver.cpp:112] Iteration 138250, lr = 0.01
I0523 04:03:41.834560 35003 solver.cpp:239] Iteration 138260 (3.58662 iter/s, 2.78814s/10 iters), loss = 6.83414
I0523 04:03:41.834612 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83414 (* 1 = 6.83414 loss)
I0523 04:03:41.847596 35003 sgd_solver.cpp:112] Iteration 138260, lr = 0.01
I0523 04:03:45.202455 35003 solver.cpp:239] Iteration 138270 (2.96938 iter/s, 3.36771s/10 iters), loss = 6.84082
I0523 04:03:45.202786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84082 (* 1 = 6.84082 loss)
I0523 04:03:45.842787 35003 sgd_solver.cpp:112] Iteration 138270, lr = 0.01
I0523 04:03:49.760555 35003 solver.cpp:239] Iteration 138280 (2.19413 iter/s, 4.55762s/10 iters), loss = 6.81135
I0523 04:03:49.760599 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81135 (* 1 = 6.81135 loss)
I0523 04:03:50.173426 35003 sgd_solver.cpp:112] Iteration 138280, lr = 0.01
I0523 04:03:53.093415 35003 solver.cpp:239] Iteration 138290 (3.00059 iter/s, 3.33267s/10 iters), loss = 6.94152
I0523 04:03:53.093454 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94152 (* 1 = 6.94152 loss)
I0523 04:03:53.106453 35003 sgd_solver.cpp:112] Iteration 138290, lr = 0.01
I0523 04:03:55.199096 35003 solver.cpp:239] Iteration 138300 (4.74941 iter/s, 2.10552s/10 iters), loss = 6.41035
I0523 04:03:55.199157 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41035 (* 1 = 6.41035 loss)
I0523 04:03:55.940029 35003 sgd_solver.cpp:112] Iteration 138300, lr = 0.01
I0523 04:04:00.141119 35003 solver.cpp:239] Iteration 138310 (2.02359 iter/s, 4.94171s/10 iters), loss = 5.91199
I0523 04:04:00.141165 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91199 (* 1 = 5.91199 loss)
I0523 04:04:00.145853 35003 sgd_solver.cpp:112] Iteration 138310, lr = 0.01
I0523 04:04:02.922459 35003 solver.cpp:239] Iteration 138320 (3.59571 iter/s, 2.78109s/10 iters), loss = 7.01374
I0523 04:04:02.922503 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01374 (* 1 = 7.01374 loss)
I0523 04:04:03.663370 35003 sgd_solver.cpp:112] Iteration 138320, lr = 0.01
I0523 04:04:08.003103 35003 solver.cpp:239] Iteration 138330 (1.96835 iter/s, 5.08039s/10 iters), loss = 5.61459
I0523 04:04:08.003142 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.61459 (* 1 = 5.61459 loss)
I0523 04:04:08.021402 35003 sgd_solver.cpp:112] Iteration 138330, lr = 0.01
I0523 04:04:10.100726 35003 solver.cpp:239] Iteration 138340 (4.76761 iter/s, 2.09749s/10 iters), loss = 6.77557
I0523 04:04:10.100762 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77557 (* 1 = 6.77557 loss)
I0523 04:04:10.113662 35003 sgd_solver.cpp:112] Iteration 138340, lr = 0.01
I0523 04:04:14.420410 35003 solver.cpp:239] Iteration 138350 (2.3151 iter/s, 4.31947s/10 iters), loss = 7.41108
I0523 04:04:14.420459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41108 (* 1 = 7.41108 loss)
I0523 04:04:14.433187 35003 sgd_solver.cpp:112] Iteration 138350, lr = 0.01
I0523 04:04:17.221431 35003 solver.cpp:239] Iteration 138360 (3.57034 iter/s, 2.80085s/10 iters), loss = 7.32013
I0523 04:04:17.221676 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32013 (* 1 = 7.32013 loss)
I0523 04:04:17.234984 35003 sgd_solver.cpp:112] Iteration 138360, lr = 0.01
I0523 04:04:19.173457 35003 solver.cpp:239] Iteration 138370 (5.12375 iter/s, 1.9517s/10 iters), loss = 7.11819
I0523 04:04:19.173507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11819 (* 1 = 7.11819 loss)
I0523 04:04:19.180402 35003 sgd_solver.cpp:112] Iteration 138370, lr = 0.01
I0523 04:04:22.754036 35003 solver.cpp:239] Iteration 138380 (2.793 iter/s, 3.58038s/10 iters), loss = 8.10338
I0523 04:04:22.754077 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.10338 (* 1 = 8.10338 loss)
I0523 04:04:22.767519 35003 sgd_solver.cpp:112] Iteration 138380, lr = 0.01
I0523 04:04:26.266163 35003 solver.cpp:239] Iteration 138390 (2.84743 iter/s, 3.51194s/10 iters), loss = 7.78706
I0523 04:04:26.266216 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78706 (* 1 = 7.78706 loss)
I0523 04:04:26.279083 35003 sgd_solver.cpp:112] Iteration 138390, lr = 0.01
I0523 04:04:28.726521 35003 solver.cpp:239] Iteration 138400 (4.0647 iter/s, 2.4602s/10 iters), loss = 7.93523
I0523 04:04:28.726567 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93523 (* 1 = 7.93523 loss)
I0523 04:04:29.103672 35003 sgd_solver.cpp:112] Iteration 138400, lr = 0.01
I0523 04:04:33.380652 35003 solver.cpp:239] Iteration 138410 (2.14875 iter/s, 4.65388s/10 iters), loss = 6.44488
I0523 04:04:33.380707 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44488 (* 1 = 6.44488 loss)
I0523 04:04:33.409801 35003 sgd_solver.cpp:112] Iteration 138410, lr = 0.01
I0523 04:04:35.389945 35003 solver.cpp:239] Iteration 138420 (4.97723 iter/s, 2.00915s/10 iters), loss = 6.95214
I0523 04:04:35.389983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95214 (* 1 = 6.95214 loss)
I0523 04:04:35.402796 35003 sgd_solver.cpp:112] Iteration 138420, lr = 0.01
I0523 04:04:39.734812 35003 solver.cpp:239] Iteration 138430 (2.30168 iter/s, 4.34465s/10 iters), loss = 6.27025
I0523 04:04:39.734863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27025 (* 1 = 6.27025 loss)
I0523 04:04:39.886407 35003 sgd_solver.cpp:112] Iteration 138430, lr = 0.01
I0523 04:04:44.144742 35003 solver.cpp:239] Iteration 138440 (2.26773 iter/s, 4.40969s/10 iters), loss = 6.90989
I0523 04:04:44.144778 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90989 (* 1 = 6.90989 loss)
I0523 04:04:44.157815 35003 sgd_solver.cpp:112] Iteration 138440, lr = 0.01
I0523 04:04:47.291607 35003 solver.cpp:239] Iteration 138450 (3.17794 iter/s, 3.1467s/10 iters), loss = 6.85723
I0523 04:04:47.291854 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85723 (* 1 = 6.85723 loss)
I0523 04:04:47.305124 35003 sgd_solver.cpp:112] Iteration 138450, lr = 0.01
I0523 04:04:50.566432 35003 solver.cpp:239] Iteration 138460 (3.05393 iter/s, 3.27447s/10 iters), loss = 6.63244
I0523 04:04:50.566504 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63244 (* 1 = 6.63244 loss)
I0523 04:04:50.571843 35003 sgd_solver.cpp:112] Iteration 138460, lr = 0.01
I0523 04:04:53.062252 35003 solver.cpp:239] Iteration 138470 (4.00698 iter/s, 2.49564s/10 iters), loss = 7.29583
I0523 04:04:53.062302 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29583 (* 1 = 7.29583 loss)
I0523 04:04:53.758016 35003 sgd_solver.cpp:112] Iteration 138470, lr = 0.01
I0523 04:04:56.144162 35003 solver.cpp:239] Iteration 138480 (3.24494 iter/s, 3.08173s/10 iters), loss = 7.39674
I0523 04:04:56.144218 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39674 (* 1 = 7.39674 loss)
I0523 04:04:56.878502 35003 sgd_solver.cpp:112] Iteration 138480, lr = 0.01
I0523 04:04:59.672946 35003 solver.cpp:239] Iteration 138490 (2.834 iter/s, 3.52858s/10 iters), loss = 5.87087
I0523 04:04:59.673005 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87087 (* 1 = 5.87087 loss)
I0523 04:05:00.258530 35003 sgd_solver.cpp:112] Iteration 138490, lr = 0.01
I0523 04:05:03.050285 35003 solver.cpp:239] Iteration 138500 (2.96109 iter/s, 3.37714s/10 iters), loss = 6.83649
I0523 04:05:03.050329 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83649 (* 1 = 6.83649 loss)
I0523 04:05:03.055541 35003 sgd_solver.cpp:112] Iteration 138500, lr = 0.01
I0523 04:05:05.841008 35003 solver.cpp:239] Iteration 138510 (3.58351 iter/s, 2.79056s/10 iters), loss = 7.31871
I0523 04:05:05.841044 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31871 (* 1 = 7.31871 loss)
I0523 04:05:05.846232 35003 sgd_solver.cpp:112] Iteration 138510, lr = 0.01
I0523 04:05:10.147522 35003 solver.cpp:239] Iteration 138520 (2.32218 iter/s, 4.3063s/10 iters), loss = 6.02813
I0523 04:05:10.147572 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02813 (* 1 = 6.02813 loss)
I0523 04:05:10.875437 35003 sgd_solver.cpp:112] Iteration 138520, lr = 0.01
I0523 04:05:13.888128 35003 solver.cpp:239] Iteration 138530 (2.67351 iter/s, 3.7404s/10 iters), loss = 6.64234
I0523 04:05:13.888164 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64234 (* 1 = 6.64234 loss)
I0523 04:05:13.902077 35003 sgd_solver.cpp:112] Iteration 138530, lr = 0.01
I0523 04:05:16.774089 35003 solver.cpp:239] Iteration 138540 (3.46524 iter/s, 2.8858s/10 iters), loss = 6.7515
I0523 04:05:16.774129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7515 (* 1 = 6.7515 loss)
I0523 04:05:16.780772 35003 sgd_solver.cpp:112] Iteration 138540, lr = 0.01
I0523 04:05:19.945597 35003 solver.cpp:239] Iteration 138550 (3.15325 iter/s, 3.17133s/10 iters), loss = 7.26999
I0523 04:05:19.945823 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26999 (* 1 = 7.26999 loss)
I0523 04:05:19.969668 35003 sgd_solver.cpp:112] Iteration 138550, lr = 0.01
I0523 04:05:23.331398 35003 solver.cpp:239] Iteration 138560 (2.95383 iter/s, 3.38543s/10 iters), loss = 6.96036
I0523 04:05:23.331444 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96036 (* 1 = 6.96036 loss)
I0523 04:05:23.337811 35003 sgd_solver.cpp:112] Iteration 138560, lr = 0.01
I0523 04:05:26.140246 35003 solver.cpp:239] Iteration 138570 (3.5604 iter/s, 2.80867s/10 iters), loss = 7.31702
I0523 04:05:26.140285 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31702 (* 1 = 7.31702 loss)
I0523 04:05:26.151518 35003 sgd_solver.cpp:112] Iteration 138570, lr = 0.01
I0523 04:05:28.945226 35003 solver.cpp:239] Iteration 138580 (3.56529 iter/s, 2.80482s/10 iters), loss = 7.01866
I0523 04:05:28.945274 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01866 (* 1 = 7.01866 loss)
I0523 04:05:29.553812 35003 sgd_solver.cpp:112] Iteration 138580, lr = 0.01
I0523 04:05:33.139581 35003 solver.cpp:239] Iteration 138590 (2.38428 iter/s, 4.19413s/10 iters), loss = 6.33121
I0523 04:05:33.139624 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33121 (* 1 = 6.33121 loss)
I0523 04:05:33.153435 35003 sgd_solver.cpp:112] Iteration 138590, lr = 0.01
I0523 04:05:34.460140 35003 solver.cpp:239] Iteration 138600 (7.57318 iter/s, 1.32045s/10 iters), loss = 7.3417
I0523 04:05:34.460193 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3417 (* 1 = 7.3417 loss)
I0523 04:05:34.467692 35003 sgd_solver.cpp:112] Iteration 138600, lr = 0.01
I0523 04:05:38.097898 35003 solver.cpp:239] Iteration 138610 (2.74911 iter/s, 3.63754s/10 iters), loss = 7.28156
I0523 04:05:38.097951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28156 (* 1 = 7.28156 loss)
I0523 04:05:38.214314 35003 sgd_solver.cpp:112] Iteration 138610, lr = 0.01
I0523 04:05:41.754801 35003 solver.cpp:239] Iteration 138620 (2.73471 iter/s, 3.6567s/10 iters), loss = 7.3072
I0523 04:05:41.754843 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3072 (* 1 = 7.3072 loss)
I0523 04:05:41.772583 35003 sgd_solver.cpp:112] Iteration 138620, lr = 0.01
I0523 04:05:44.928715 35003 solver.cpp:239] Iteration 138630 (3.15086 iter/s, 3.17374s/10 iters), loss = 6.46406
I0523 04:05:44.928755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46406 (* 1 = 6.46406 loss)
I0523 04:05:44.936220 35003 sgd_solver.cpp:112] Iteration 138630, lr = 0.01
I0523 04:05:45.757627 35003 solver.cpp:239] Iteration 138640 (12.0652 iter/s, 0.828828s/10 iters), loss = 6.52631
I0523 04:05:45.757670 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52631 (* 1 = 6.52631 loss)
I0523 04:05:45.767879 35003 sgd_solver.cpp:112] Iteration 138640, lr = 0.01
I0523 04:05:46.583093 35003 solver.cpp:239] Iteration 138650 (12.1157 iter/s, 0.825376s/10 iters), loss = 7.80082
I0523 04:05:46.583138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80082 (* 1 = 7.80082 loss)
I0523 04:05:46.589743 35003 sgd_solver.cpp:112] Iteration 138650, lr = 0.01
I0523 04:05:47.529748 35003 solver.cpp:239] Iteration 138660 (10.5645 iter/s, 0.946563s/10 iters), loss = 6.3952
I0523 04:05:47.529793 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3952 (* 1 = 6.3952 loss)
I0523 04:05:47.707523 35003 sgd_solver.cpp:112] Iteration 138660, lr = 0.01
I0523 04:05:48.520750 35003 solver.cpp:239] Iteration 138670 (10.0918 iter/s, 0.990907s/10 iters), loss = 7.41176
I0523 04:05:48.520786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41176 (* 1 = 7.41176 loss)
I0523 04:05:48.532475 35003 sgd_solver.cpp:112] Iteration 138670, lr = 0.01
I0523 04:05:49.344159 35003 solver.cpp:239] Iteration 138680 (12.1458 iter/s, 0.823329s/10 iters), loss = 6.65686
I0523 04:05:49.344203 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65686 (* 1 = 6.65686 loss)
I0523 04:05:49.353006 35003 sgd_solver.cpp:112] Iteration 138680, lr = 0.01
I0523 04:05:50.172001 35003 solver.cpp:239] Iteration 138690 (12.0809 iter/s, 0.827756s/10 iters), loss = 6.5132
I0523 04:05:50.172329 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5132 (* 1 = 6.5132 loss)
I0523 04:05:50.181249 35003 sgd_solver.cpp:112] Iteration 138690, lr = 0.01
I0523 04:05:52.781077 35003 solver.cpp:239] Iteration 138700 (3.83336 iter/s, 2.60867s/10 iters), loss = 6.84196
I0523 04:05:52.781117 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84196 (* 1 = 6.84196 loss)
I0523 04:05:52.789849 35003 sgd_solver.cpp:112] Iteration 138700, lr = 0.01
I0523 04:05:55.458343 35003 solver.cpp:239] Iteration 138710 (3.73537 iter/s, 2.67711s/10 iters), loss = 6.83503
I0523 04:05:55.458395 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83503 (* 1 = 6.83503 loss)
I0523 04:05:55.471761 35003 sgd_solver.cpp:112] Iteration 138710, lr = 0.01
I0523 04:05:58.368955 35003 solver.cpp:239] Iteration 138720 (3.43591 iter/s, 2.91044s/10 iters), loss = 6.25464
I0523 04:05:58.368999 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25464 (* 1 = 6.25464 loss)
I0523 04:05:59.107095 35003 sgd_solver.cpp:112] Iteration 138720, lr = 0.01
I0523 04:06:04.802320 35003 solver.cpp:239] Iteration 138730 (1.55447 iter/s, 6.43306s/10 iters), loss = 5.93054
I0523 04:06:04.802379 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93054 (* 1 = 5.93054 loss)
I0523 04:06:05.523550 35003 sgd_solver.cpp:112] Iteration 138730, lr = 0.01
I0523 04:06:09.194175 35003 solver.cpp:239] Iteration 138740 (2.27708 iter/s, 4.3916s/10 iters), loss = 6.85057
I0523 04:06:09.194221 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85057 (* 1 = 6.85057 loss)
I0523 04:06:09.204090 35003 sgd_solver.cpp:112] Iteration 138740, lr = 0.01
I0523 04:06:13.345613 35003 solver.cpp:239] Iteration 138750 (2.40893 iter/s, 4.15122s/10 iters), loss = 7.27237
I0523 04:06:13.345656 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27237 (* 1 = 7.27237 loss)
I0523 04:06:13.352331 35003 sgd_solver.cpp:112] Iteration 138750, lr = 0.01
I0523 04:06:18.403875 35003 solver.cpp:239] Iteration 138760 (1.97706 iter/s, 5.05801s/10 iters), loss = 7.61331
I0523 04:06:18.403923 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61331 (* 1 = 7.61331 loss)
I0523 04:06:19.125901 35003 sgd_solver.cpp:112] Iteration 138760, lr = 0.01
I0523 04:06:21.699306 35003 solver.cpp:239] Iteration 138770 (3.03468 iter/s, 3.29524s/10 iters), loss = 6.50073
I0523 04:06:21.699580 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50073 (* 1 = 6.50073 loss)
I0523 04:06:21.712249 35003 sgd_solver.cpp:112] Iteration 138770, lr = 0.01
I0523 04:06:23.407851 35003 solver.cpp:239] Iteration 138780 (5.85405 iter/s, 1.70822s/10 iters), loss = 8.41149
I0523 04:06:23.407892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.41149 (* 1 = 8.41149 loss)
I0523 04:06:24.136449 35003 sgd_solver.cpp:112] Iteration 138780, lr = 0.01
I0523 04:06:26.905445 35003 solver.cpp:239] Iteration 138790 (2.85926 iter/s, 3.4974s/10 iters), loss = 7.60102
I0523 04:06:26.905494 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60102 (* 1 = 7.60102 loss)
I0523 04:06:26.913100 35003 sgd_solver.cpp:112] Iteration 138790, lr = 0.01
I0523 04:06:29.376338 35003 solver.cpp:239] Iteration 138800 (4.04737 iter/s, 2.47074s/10 iters), loss = 6.9971
I0523 04:06:29.376372 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9971 (* 1 = 6.9971 loss)
I0523 04:06:29.389761 35003 sgd_solver.cpp:112] Iteration 138800, lr = 0.01
I0523 04:06:30.999740 35003 solver.cpp:239] Iteration 138810 (6.16035 iter/s, 1.62328s/10 iters), loss = 7.69674
I0523 04:06:30.999806 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69674 (* 1 = 7.69674 loss)
I0523 04:06:31.714468 35003 sgd_solver.cpp:112] Iteration 138810, lr = 0.01
I0523 04:06:33.810236 35003 solver.cpp:239] Iteration 138820 (3.55832 iter/s, 2.81032s/10 iters), loss = 7.44672
I0523 04:06:33.810281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44672 (* 1 = 7.44672 loss)
I0523 04:06:34.548596 35003 sgd_solver.cpp:112] Iteration 138820, lr = 0.01
I0523 04:06:37.367714 35003 solver.cpp:239] Iteration 138830 (2.81114 iter/s, 3.55728s/10 iters), loss = 6.41622
I0523 04:06:37.367771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41622 (* 1 = 6.41622 loss)
I0523 04:06:37.375329 35003 sgd_solver.cpp:112] Iteration 138830, lr = 0.01
I0523 04:06:41.588532 35003 solver.cpp:239] Iteration 138840 (2.36934 iter/s, 4.22059s/10 iters), loss = 6.66731
I0523 04:06:41.588577 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66731 (* 1 = 6.66731 loss)
I0523 04:06:42.266934 35003 sgd_solver.cpp:112] Iteration 138840, lr = 0.01
I0523 04:06:43.790669 35003 solver.cpp:239] Iteration 138850 (4.54133 iter/s, 2.202s/10 iters), loss = 5.09499
I0523 04:06:43.790738 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.09499 (* 1 = 5.09499 loss)
I0523 04:06:43.793855 35003 sgd_solver.cpp:112] Iteration 138850, lr = 0.01
I0523 04:06:47.412068 35003 solver.cpp:239] Iteration 138860 (2.76255 iter/s, 3.61985s/10 iters), loss = 7.01738
I0523 04:06:47.412108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01738 (* 1 = 7.01738 loss)
I0523 04:06:47.418571 35003 sgd_solver.cpp:112] Iteration 138860, lr = 0.01
I0523 04:06:50.873947 35003 solver.cpp:239] Iteration 138870 (2.89004 iter/s, 3.46016s/10 iters), loss = 7.76387
I0523 04:06:50.873986 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76387 (* 1 = 7.76387 loss)
I0523 04:06:50.892967 35003 sgd_solver.cpp:112] Iteration 138870, lr = 0.01
I0523 04:06:54.509523 35003 solver.cpp:239] Iteration 138880 (2.75074 iter/s, 3.63538s/10 iters), loss = 6.4757
I0523 04:06:54.509671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4757 (* 1 = 6.4757 loss)
I0523 04:06:55.247351 35003 sgd_solver.cpp:112] Iteration 138880, lr = 0.01
I0523 04:06:58.673136 35003 solver.cpp:239] Iteration 138890 (2.40194 iter/s, 4.1633s/10 iters), loss = 6.50371
I0523 04:06:58.673182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50371 (* 1 = 6.50371 loss)
I0523 04:06:59.408174 35003 sgd_solver.cpp:112] Iteration 138890, lr = 0.01
I0523 04:07:02.348762 35003 solver.cpp:239] Iteration 138900 (2.72077 iter/s, 3.67543s/10 iters), loss = 6.91994
I0523 04:07:02.348798 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91994 (* 1 = 6.91994 loss)
I0523 04:07:02.361749 35003 sgd_solver.cpp:112] Iteration 138900, lr = 0.01
I0523 04:07:05.219378 35003 solver.cpp:239] Iteration 138910 (3.48376 iter/s, 2.87046s/10 iters), loss = 7.5806
I0523 04:07:05.219410 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5806 (* 1 = 7.5806 loss)
I0523 04:07:05.932837 35003 sgd_solver.cpp:112] Iteration 138910, lr = 0.01
I0523 04:07:07.803673 35003 solver.cpp:239] Iteration 138920 (3.86975 iter/s, 2.58415s/10 iters), loss = 7.6273
I0523 04:07:07.803719 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6273 (* 1 = 7.6273 loss)
I0523 04:07:07.808784 35003 sgd_solver.cpp:112] Iteration 138920, lr = 0.01
I0523 04:07:10.604156 35003 solver.cpp:239] Iteration 138930 (3.57102 iter/s, 2.80032s/10 iters), loss = 7.33098
I0523 04:07:10.604200 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33098 (* 1 = 7.33098 loss)
I0523 04:07:10.694638 35003 sgd_solver.cpp:112] Iteration 138930, lr = 0.01
I0523 04:07:14.948904 35003 solver.cpp:239] Iteration 138940 (2.30175 iter/s, 4.34453s/10 iters), loss = 5.58347
I0523 04:07:14.948952 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.58347 (* 1 = 5.58347 loss)
I0523 04:07:15.689676 35003 sgd_solver.cpp:112] Iteration 138940, lr = 0.01
I0523 04:07:18.755990 35003 solver.cpp:239] Iteration 138950 (2.62683 iter/s, 3.80688s/10 iters), loss = 6.04725
I0523 04:07:18.756031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04725 (* 1 = 6.04725 loss)
I0523 04:07:19.309991 35003 sgd_solver.cpp:112] Iteration 138950, lr = 0.01
I0523 04:07:22.154835 35003 solver.cpp:239] Iteration 138960 (2.94234 iter/s, 3.39866s/10 iters), loss = 8.10946
I0523 04:07:22.154882 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.10946 (* 1 = 8.10946 loss)
I0523 04:07:22.168073 35003 sgd_solver.cpp:112] Iteration 138960, lr = 0.01
I0523 04:07:25.825187 35003 solver.cpp:239] Iteration 138970 (2.72469 iter/s, 3.67015s/10 iters), loss = 6.97488
I0523 04:07:25.825430 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97488 (* 1 = 6.97488 loss)
I0523 04:07:26.104794 35003 sgd_solver.cpp:112] Iteration 138970, lr = 0.01
I0523 04:07:28.960860 35003 solver.cpp:239] Iteration 138980 (3.18947 iter/s, 3.13532s/10 iters), loss = 6.74188
I0523 04:07:28.960911 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74188 (* 1 = 6.74188 loss)
I0523 04:07:28.972626 35003 sgd_solver.cpp:112] Iteration 138980, lr = 0.01
I0523 04:07:32.454452 35003 solver.cpp:239] Iteration 138990 (2.86254 iter/s, 3.4934s/10 iters), loss = 7.53105
I0523 04:07:32.454495 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53105 (* 1 = 7.53105 loss)
I0523 04:07:33.132062 35003 sgd_solver.cpp:112] Iteration 138990, lr = 0.01
I0523 04:07:36.198421 35003 solver.cpp:239] Iteration 139000 (2.6711 iter/s, 3.74377s/10 iters), loss = 7.68035
I0523 04:07:36.198462 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68035 (* 1 = 7.68035 loss)
I0523 04:07:36.205482 35003 sgd_solver.cpp:112] Iteration 139000, lr = 0.01
I0523 04:07:39.082538 35003 solver.cpp:239] Iteration 139010 (3.46747 iter/s, 2.88395s/10 iters), loss = 5.85725
I0523 04:07:39.082578 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85725 (* 1 = 5.85725 loss)
I0523 04:07:39.823140 35003 sgd_solver.cpp:112] Iteration 139010, lr = 0.01
I0523 04:07:41.883283 35003 solver.cpp:239] Iteration 139020 (3.57068 iter/s, 2.80058s/10 iters), loss = 6.92765
I0523 04:07:41.883332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92765 (* 1 = 6.92765 loss)
I0523 04:07:41.886981 35003 sgd_solver.cpp:112] Iteration 139020, lr = 0.01
I0523 04:07:47.955546 35003 solver.cpp:239] Iteration 139030 (1.64692 iter/s, 6.07195s/10 iters), loss = 6.92656
I0523 04:07:47.955605 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92656 (* 1 = 6.92656 loss)
I0523 04:07:48.493039 35003 sgd_solver.cpp:112] Iteration 139030, lr = 0.01
I0523 04:07:51.292563 35003 solver.cpp:239] Iteration 139040 (2.99686 iter/s, 3.33682s/10 iters), loss = 7.84898
I0523 04:07:51.292605 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84898 (* 1 = 7.84898 loss)
I0523 04:07:52.008302 35003 sgd_solver.cpp:112] Iteration 139040, lr = 0.01
I0523 04:07:55.683616 35003 solver.cpp:239] Iteration 139050 (2.27747 iter/s, 4.39083s/10 iters), loss = 6.24818
I0523 04:07:55.683652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24818 (* 1 = 6.24818 loss)
I0523 04:07:55.685014 35003 sgd_solver.cpp:112] Iteration 139050, lr = 0.01
I0523 04:07:57.808578 35003 solver.cpp:239] Iteration 139060 (4.7063 iter/s, 2.12481s/10 iters), loss = 6.75354
I0523 04:07:57.808882 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75354 (* 1 = 6.75354 loss)
I0523 04:07:57.818049 35003 sgd_solver.cpp:112] Iteration 139060, lr = 0.01
I0523 04:08:03.769119 35003 solver.cpp:239] Iteration 139070 (1.67785 iter/s, 5.96002s/10 iters), loss = 6.5778
I0523 04:08:03.769161 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5778 (* 1 = 6.5778 loss)
I0523 04:08:03.827790 35003 sgd_solver.cpp:112] Iteration 139070, lr = 0.01
I0523 04:08:07.917331 35003 solver.cpp:239] Iteration 139080 (2.41081 iter/s, 4.14799s/10 iters), loss = 6.21964
I0523 04:08:07.917383 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21964 (* 1 = 6.21964 loss)
I0523 04:08:07.933070 35003 sgd_solver.cpp:112] Iteration 139080, lr = 0.01
I0523 04:08:10.763165 35003 solver.cpp:239] Iteration 139090 (3.51413 iter/s, 2.84566s/10 iters), loss = 7.37725
I0523 04:08:10.763208 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37725 (* 1 = 7.37725 loss)
I0523 04:08:10.772164 35003 sgd_solver.cpp:112] Iteration 139090, lr = 0.01
I0523 04:08:14.231710 35003 solver.cpp:239] Iteration 139100 (2.88321 iter/s, 3.46836s/10 iters), loss = 6.22815
I0523 04:08:14.231750 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22815 (* 1 = 6.22815 loss)
I0523 04:08:14.236829 35003 sgd_solver.cpp:112] Iteration 139100, lr = 0.01
I0523 04:08:18.705828 35003 solver.cpp:239] Iteration 139110 (2.2352 iter/s, 4.47388s/10 iters), loss = 8.00546
I0523 04:08:18.705871 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00546 (* 1 = 8.00546 loss)
I0523 04:08:19.401898 35003 sgd_solver.cpp:112] Iteration 139110, lr = 0.01
I0523 04:08:22.087501 35003 solver.cpp:239] Iteration 139120 (2.95728 iter/s, 3.38148s/10 iters), loss = 7.07152
I0523 04:08:22.087553 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07152 (* 1 = 7.07152 loss)
I0523 04:08:22.096163 35003 sgd_solver.cpp:112] Iteration 139120, lr = 0.01
I0523 04:08:26.451364 35003 solver.cpp:239] Iteration 139130 (2.29167 iter/s, 4.36362s/10 iters), loss = 6.3702
I0523 04:08:26.451421 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3702 (* 1 = 6.3702 loss)
I0523 04:08:27.147248 35003 sgd_solver.cpp:112] Iteration 139130, lr = 0.01
I0523 04:08:29.278462 35003 solver.cpp:239] Iteration 139140 (3.53741 iter/s, 2.82693s/10 iters), loss = 7.19779
I0523 04:08:29.278745 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19779 (* 1 = 7.19779 loss)
I0523 04:08:30.020107 35003 sgd_solver.cpp:112] Iteration 139140, lr = 0.01
I0523 04:08:32.083899 35003 solver.cpp:239] Iteration 139150 (3.56494 iter/s, 2.8051s/10 iters), loss = 6.19622
I0523 04:08:32.083947 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19622 (* 1 = 6.19622 loss)
I0523 04:08:32.096794 35003 sgd_solver.cpp:112] Iteration 139150, lr = 0.01
I0523 04:08:35.158535 35003 solver.cpp:239] Iteration 139160 (3.25261 iter/s, 3.07445s/10 iters), loss = 6.78185
I0523 04:08:35.158586 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78185 (* 1 = 6.78185 loss)
I0523 04:08:35.873392 35003 sgd_solver.cpp:112] Iteration 139160, lr = 0.01
I0523 04:08:38.518277 35003 solver.cpp:239] Iteration 139170 (2.97659 iter/s, 3.35955s/10 iters), loss = 7.64905
I0523 04:08:38.518318 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64905 (* 1 = 7.64905 loss)
I0523 04:08:38.522404 35003 sgd_solver.cpp:112] Iteration 139170, lr = 0.01
I0523 04:08:42.631294 35003 solver.cpp:239] Iteration 139180 (2.43148 iter/s, 4.11272s/10 iters), loss = 6.10138
I0523 04:08:42.631359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10138 (* 1 = 6.10138 loss)
I0523 04:08:42.639572 35003 sgd_solver.cpp:112] Iteration 139180, lr = 0.01
I0523 04:08:45.388001 35003 solver.cpp:239] Iteration 139190 (3.62775 iter/s, 2.75653s/10 iters), loss = 6.97335
I0523 04:08:45.388038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97335 (* 1 = 6.97335 loss)
I0523 04:08:45.395587 35003 sgd_solver.cpp:112] Iteration 139190, lr = 0.01
I0523 04:08:47.462934 35003 solver.cpp:239] Iteration 139200 (4.81973 iter/s, 2.0748s/10 iters), loss = 7.80095
I0523 04:08:47.462977 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80095 (* 1 = 7.80095 loss)
I0523 04:08:47.470233 35003 sgd_solver.cpp:112] Iteration 139200, lr = 0.01
I0523 04:08:51.182440 35003 solver.cpp:239] Iteration 139210 (2.68867 iter/s, 3.71931s/10 iters), loss = 6.21789
I0523 04:08:51.182479 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21789 (* 1 = 6.21789 loss)
I0523 04:08:51.194442 35003 sgd_solver.cpp:112] Iteration 139210, lr = 0.01
I0523 04:08:53.683437 35003 solver.cpp:239] Iteration 139220 (3.99865 iter/s, 2.50084s/10 iters), loss = 7.19677
I0523 04:08:53.683472 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19677 (* 1 = 7.19677 loss)
I0523 04:08:53.696327 35003 sgd_solver.cpp:112] Iteration 139220, lr = 0.01
I0523 04:08:56.292652 35003 solver.cpp:239] Iteration 139230 (3.83278 iter/s, 2.60907s/10 iters), loss = 7.82278
I0523 04:08:56.292692 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82278 (* 1 = 7.82278 loss)
I0523 04:08:56.306589 35003 sgd_solver.cpp:112] Iteration 139230, lr = 0.01
I0523 04:09:00.002038 35003 solver.cpp:239] Iteration 139240 (2.69601 iter/s, 3.70919s/10 iters), loss = 6.95442
I0523 04:09:00.002285 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95442 (* 1 = 6.95442 loss)
I0523 04:09:00.021139 35003 sgd_solver.cpp:112] Iteration 139240, lr = 0.01
I0523 04:09:04.353900 35003 solver.cpp:239] Iteration 139250 (2.29808 iter/s, 4.35146s/10 iters), loss = 6.23894
I0523 04:09:04.353946 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23894 (* 1 = 6.23894 loss)
I0523 04:09:05.095605 35003 sgd_solver.cpp:112] Iteration 139250, lr = 0.01
I0523 04:09:07.586573 35003 solver.cpp:239] Iteration 139260 (3.09359 iter/s, 3.23249s/10 iters), loss = 5.77064
I0523 04:09:07.586621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77064 (* 1 = 5.77064 loss)
I0523 04:09:08.273717 35003 sgd_solver.cpp:112] Iteration 139260, lr = 0.01
I0523 04:09:13.440639 35003 solver.cpp:239] Iteration 139270 (1.7083 iter/s, 5.85378s/10 iters), loss = 7.14931
I0523 04:09:13.440680 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14931 (* 1 = 7.14931 loss)
I0523 04:09:13.454191 35003 sgd_solver.cpp:112] Iteration 139270, lr = 0.01
I0523 04:09:17.855232 35003 solver.cpp:239] Iteration 139280 (2.26533 iter/s, 4.41437s/10 iters), loss = 6.76573
I0523 04:09:17.855276 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76573 (* 1 = 6.76573 loss)
I0523 04:09:17.863297 35003 sgd_solver.cpp:112] Iteration 139280, lr = 0.01
I0523 04:09:19.161674 35003 solver.cpp:239] Iteration 139290 (7.655 iter/s, 1.30634s/10 iters), loss = 6.80321
I0523 04:09:19.161715 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80321 (* 1 = 6.80321 loss)
I0523 04:09:19.180868 35003 sgd_solver.cpp:112] Iteration 139290, lr = 0.01
I0523 04:09:22.733983 35003 solver.cpp:239] Iteration 139300 (2.79946 iter/s, 3.57212s/10 iters), loss = 7.00626
I0523 04:09:22.734038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00626 (* 1 = 7.00626 loss)
I0523 04:09:22.740563 35003 sgd_solver.cpp:112] Iteration 139300, lr = 0.01
I0523 04:09:25.717229 35003 solver.cpp:239] Iteration 139310 (3.35226 iter/s, 2.98306s/10 iters), loss = 5.65551
I0523 04:09:25.717270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.65551 (* 1 = 5.65551 loss)
I0523 04:09:25.730422 35003 sgd_solver.cpp:112] Iteration 139310, lr = 0.01
I0523 04:09:28.564977 35003 solver.cpp:239] Iteration 139320 (3.51175 iter/s, 2.84759s/10 iters), loss = 6.5272
I0523 04:09:28.565016 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5272 (* 1 = 6.5272 loss)
I0523 04:09:28.570389 35003 sgd_solver.cpp:112] Iteration 139320, lr = 0.01
I0523 04:09:32.861424 35003 solver.cpp:239] Iteration 139330 (2.32763 iter/s, 4.29622s/10 iters), loss = 6.22119
I0523 04:09:32.861681 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22119 (* 1 = 6.22119 loss)
I0523 04:09:32.869101 35003 sgd_solver.cpp:112] Iteration 139330, lr = 0.01
I0523 04:09:36.978582 35003 solver.cpp:239] Iteration 139340 (2.42909 iter/s, 4.11676s/10 iters), loss = 8.3684
I0523 04:09:36.978626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.3684 (* 1 = 8.3684 loss)
I0523 04:09:37.715579 35003 sgd_solver.cpp:112] Iteration 139340, lr = 0.01
I0523 04:09:40.595679 35003 solver.cpp:239] Iteration 139350 (2.76479 iter/s, 3.61691s/10 iters), loss = 7.73777
I0523 04:09:40.595715 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73777 (* 1 = 7.73777 loss)
I0523 04:09:40.600687 35003 sgd_solver.cpp:112] Iteration 139350, lr = 0.01
I0523 04:09:45.762956 35003 solver.cpp:239] Iteration 139360 (1.93535 iter/s, 5.16702s/10 iters), loss = 8.04681
I0523 04:09:45.763002 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04681 (* 1 = 8.04681 loss)
I0523 04:09:46.071108 35003 sgd_solver.cpp:112] Iteration 139360, lr = 0.01
I0523 04:09:50.382259 35003 solver.cpp:239] Iteration 139370 (2.16494 iter/s, 4.61907s/10 iters), loss = 7.11024
I0523 04:09:50.382302 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11024 (* 1 = 7.11024 loss)
I0523 04:09:50.395107 35003 sgd_solver.cpp:112] Iteration 139370, lr = 0.01
I0523 04:09:54.809025 35003 solver.cpp:239] Iteration 139380 (2.2591 iter/s, 4.42654s/10 iters), loss = 8.77595
I0523 04:09:54.809068 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.77595 (* 1 = 8.77595 loss)
I0523 04:09:55.536046 35003 sgd_solver.cpp:112] Iteration 139380, lr = 0.01
I0523 04:09:58.248072 35003 solver.cpp:239] Iteration 139390 (2.90794 iter/s, 3.43886s/10 iters), loss = 8.00115
I0523 04:09:58.248111 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00115 (* 1 = 8.00115 loss)
I0523 04:09:58.265935 35003 sgd_solver.cpp:112] Iteration 139390, lr = 0.01
I0523 04:10:01.161257 35003 solver.cpp:239] Iteration 139400 (3.43286 iter/s, 2.91302s/10 iters), loss = 7.36403
I0523 04:10:01.161311 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36403 (* 1 = 7.36403 loss)
I0523 04:10:01.895745 35003 sgd_solver.cpp:112] Iteration 139400, lr = 0.01
I0523 04:10:03.830814 35003 solver.cpp:239] Iteration 139410 (3.74617 iter/s, 2.66939s/10 iters), loss = 7.24421
I0523 04:10:03.830953 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24421 (* 1 = 7.24421 loss)
I0523 04:10:03.844003 35003 sgd_solver.cpp:112] Iteration 139410, lr = 0.01
I0523 04:10:07.307071 35003 solver.cpp:239] Iteration 139420 (2.8769 iter/s, 3.47597s/10 iters), loss = 7.75038
I0523 04:10:07.307121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75038 (* 1 = 7.75038 loss)
I0523 04:10:07.324905 35003 sgd_solver.cpp:112] Iteration 139420, lr = 0.01
I0523 04:10:10.164155 35003 solver.cpp:239] Iteration 139430 (3.50029 iter/s, 2.85691s/10 iters), loss = 6.63792
I0523 04:10:10.164193 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63792 (* 1 = 6.63792 loss)
I0523 04:10:10.171331 35003 sgd_solver.cpp:112] Iteration 139430, lr = 0.01
I0523 04:10:12.337225 35003 solver.cpp:239] Iteration 139440 (4.60209 iter/s, 2.17293s/10 iters), loss = 8.01473
I0523 04:10:12.337270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01473 (* 1 = 8.01473 loss)
I0523 04:10:13.078186 35003 sgd_solver.cpp:112] Iteration 139440, lr = 0.01
I0523 04:10:16.538053 35003 solver.cpp:239] Iteration 139450 (2.38061 iter/s, 4.20061s/10 iters), loss = 7.88752
I0523 04:10:16.538110 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88752 (* 1 = 7.88752 loss)
I0523 04:10:17.273221 35003 sgd_solver.cpp:112] Iteration 139450, lr = 0.01
I0523 04:10:20.906047 35003 solver.cpp:239] Iteration 139460 (2.28951 iter/s, 4.36776s/10 iters), loss = 8.4683
I0523 04:10:20.906090 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.4683 (* 1 = 8.4683 loss)
I0523 04:10:20.918951 35003 sgd_solver.cpp:112] Iteration 139460, lr = 0.01
I0523 04:10:23.634172 35003 solver.cpp:239] Iteration 139470 (3.66574 iter/s, 2.72796s/10 iters), loss = 7.36084
I0523 04:10:23.634212 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36084 (* 1 = 7.36084 loss)
I0523 04:10:23.647055 35003 sgd_solver.cpp:112] Iteration 139470, lr = 0.01
I0523 04:10:25.695806 35003 solver.cpp:239] Iteration 139480 (4.85083 iter/s, 2.0615s/10 iters), loss = 7.11685
I0523 04:10:25.695852 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11685 (* 1 = 7.11685 loss)
I0523 04:10:25.708248 35003 sgd_solver.cpp:112] Iteration 139480, lr = 0.01
I0523 04:10:28.726802 35003 solver.cpp:239] Iteration 139490 (3.29944 iter/s, 3.03082s/10 iters), loss = 6.69331
I0523 04:10:28.726855 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69331 (* 1 = 6.69331 loss)
I0523 04:10:29.407264 35003 sgd_solver.cpp:112] Iteration 139490, lr = 0.01
I0523 04:10:31.496688 35003 solver.cpp:239] Iteration 139500 (3.61048 iter/s, 2.76972s/10 iters), loss = 7.98878
I0523 04:10:31.496731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98878 (* 1 = 7.98878 loss)
I0523 04:10:32.231541 35003 sgd_solver.cpp:112] Iteration 139500, lr = 0.01
I0523 04:10:34.362648 35003 solver.cpp:239] Iteration 139510 (3.48944 iter/s, 2.86579s/10 iters), loss = 6.92976
I0523 04:10:34.362962 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92976 (* 1 = 6.92976 loss)
I0523 04:10:35.083953 35003 sgd_solver.cpp:112] Iteration 139510, lr = 0.01
I0523 04:10:37.908797 35003 solver.cpp:239] Iteration 139520 (2.82034 iter/s, 3.54568s/10 iters), loss = 6.17981
I0523 04:10:37.908833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17981 (* 1 = 6.17981 loss)
I0523 04:10:37.911765 35003 sgd_solver.cpp:112] Iteration 139520, lr = 0.01
I0523 04:10:42.318660 35003 solver.cpp:239] Iteration 139530 (2.26779 iter/s, 4.40959s/10 iters), loss = 7.01467
I0523 04:10:42.318733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01467 (* 1 = 7.01467 loss)
I0523 04:10:42.331956 35003 sgd_solver.cpp:112] Iteration 139530, lr = 0.01
I0523 04:10:47.376991 35003 solver.cpp:239] Iteration 139540 (1.97704 iter/s, 5.05806s/10 iters), loss = 6.24966
I0523 04:10:47.377040 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24966 (* 1 = 6.24966 loss)
I0523 04:10:48.091796 35003 sgd_solver.cpp:112] Iteration 139540, lr = 0.01
I0523 04:10:51.996978 35003 solver.cpp:239] Iteration 139550 (2.16462 iter/s, 4.61975s/10 iters), loss = 6.88716
I0523 04:10:51.997014 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88716 (* 1 = 6.88716 loss)
I0523 04:10:52.010653 35003 sgd_solver.cpp:112] Iteration 139550, lr = 0.01
I0523 04:10:56.149067 35003 solver.cpp:239] Iteration 139560 (2.40855 iter/s, 4.15188s/10 iters), loss = 6.92873
I0523 04:10:56.149104 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92873 (* 1 = 6.92873 loss)
I0523 04:10:56.157090 35003 sgd_solver.cpp:112] Iteration 139560, lr = 0.01
I0523 04:10:59.670244 35003 solver.cpp:239] Iteration 139570 (2.84011 iter/s, 3.52099s/10 iters), loss = 6.93472
I0523 04:10:59.670284 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93472 (* 1 = 6.93472 loss)
I0523 04:10:59.684134 35003 sgd_solver.cpp:112] Iteration 139570, lr = 0.01
I0523 04:11:01.668601 35003 solver.cpp:239] Iteration 139580 (5.00445 iter/s, 1.99822s/10 iters), loss = 7.22311
I0523 04:11:01.668651 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22311 (* 1 = 7.22311 loss)
I0523 04:11:02.409654 35003 sgd_solver.cpp:112] Iteration 139580, lr = 0.01
I0523 04:11:05.035612 35003 solver.cpp:239] Iteration 139590 (2.97016 iter/s, 3.36682s/10 iters), loss = 6.45028
I0523 04:11:05.035902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45028 (* 1 = 6.45028 loss)
I0523 04:11:05.053740 35003 sgd_solver.cpp:112] Iteration 139590, lr = 0.01
I0523 04:11:09.175799 35003 solver.cpp:239] Iteration 139600 (2.4156 iter/s, 4.13976s/10 iters), loss = 7.38813
I0523 04:11:09.175843 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38813 (* 1 = 7.38813 loss)
I0523 04:11:09.891306 35003 sgd_solver.cpp:112] Iteration 139600, lr = 0.01
I0523 04:11:13.304132 35003 solver.cpp:239] Iteration 139610 (2.42241 iter/s, 4.12812s/10 iters), loss = 6.97452
I0523 04:11:13.304178 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97452 (* 1 = 6.97452 loss)
I0523 04:11:14.031066 35003 sgd_solver.cpp:112] Iteration 139610, lr = 0.01
I0523 04:11:17.717049 35003 solver.cpp:239] Iteration 139620 (2.26619 iter/s, 4.41269s/10 iters), loss = 6.83742
I0523 04:11:17.717089 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83742 (* 1 = 6.83742 loss)
I0523 04:11:17.729913 35003 sgd_solver.cpp:112] Iteration 139620, lr = 0.01
I0523 04:11:22.207093 35003 solver.cpp:239] Iteration 139630 (2.22726 iter/s, 4.48982s/10 iters), loss = 7.78518
I0523 04:11:22.207130 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78518 (* 1 = 7.78518 loss)
I0523 04:11:22.225275 35003 sgd_solver.cpp:112] Iteration 139630, lr = 0.01
I0523 04:11:25.697190 35003 solver.cpp:239] Iteration 139640 (2.8654 iter/s, 3.48991s/10 iters), loss = 7.62394
I0523 04:11:25.697232 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62394 (* 1 = 7.62394 loss)
I0523 04:11:25.738288 35003 sgd_solver.cpp:112] Iteration 139640, lr = 0.01
I0523 04:11:27.035715 35003 solver.cpp:239] Iteration 139650 (7.47149 iter/s, 1.33842s/10 iters), loss = 6.77012
I0523 04:11:27.035759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77012 (* 1 = 6.77012 loss)
I0523 04:11:27.045845 35003 sgd_solver.cpp:112] Iteration 139650, lr = 0.01
I0523 04:11:29.841135 35003 solver.cpp:239] Iteration 139660 (3.56474 iter/s, 2.80526s/10 iters), loss = 7.22036
I0523 04:11:29.841253 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22036 (* 1 = 7.22036 loss)
I0523 04:11:29.847618 35003 sgd_solver.cpp:112] Iteration 139660, lr = 0.01
I0523 04:11:34.123615 35003 solver.cpp:239] Iteration 139670 (2.33526 iter/s, 4.28218s/10 iters), loss = 7.08083
I0523 04:11:34.123664 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08083 (* 1 = 7.08083 loss)
I0523 04:11:34.133448 35003 sgd_solver.cpp:112] Iteration 139670, lr = 0.01
I0523 04:11:36.668189 35003 solver.cpp:239] Iteration 139680 (3.93018 iter/s, 2.54441s/10 iters), loss = 6.21883
I0523 04:11:36.668355 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21883 (* 1 = 6.21883 loss)
I0523 04:11:36.674810 35003 sgd_solver.cpp:112] Iteration 139680, lr = 0.01
I0523 04:11:41.070410 35003 solver.cpp:239] Iteration 139690 (2.27176 iter/s, 4.40187s/10 iters), loss = 6.50034
I0523 04:11:41.070456 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50034 (* 1 = 6.50034 loss)
I0523 04:11:41.075186 35003 sgd_solver.cpp:112] Iteration 139690, lr = 0.01
I0523 04:11:43.155179 35003 solver.cpp:239] Iteration 139700 (4.79702 iter/s, 2.08463s/10 iters), loss = 7.5033
I0523 04:11:43.155231 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5033 (* 1 = 7.5033 loss)
I0523 04:11:43.158728 35003 sgd_solver.cpp:112] Iteration 139700, lr = 0.01
I0523 04:11:47.206811 35003 solver.cpp:239] Iteration 139710 (2.46828 iter/s, 4.05141s/10 iters), loss = 7.993
I0523 04:11:47.206868 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.993 (* 1 = 7.993 loss)
I0523 04:11:47.215785 35003 sgd_solver.cpp:112] Iteration 139710, lr = 0.01
I0523 04:11:50.133602 35003 solver.cpp:239] Iteration 139720 (3.41693 iter/s, 2.92661s/10 iters), loss = 6.47894
I0523 04:11:50.133657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47894 (* 1 = 6.47894 loss)
I0523 04:11:50.566754 35003 sgd_solver.cpp:112] Iteration 139720, lr = 0.01
I0523 04:11:54.139081 35003 solver.cpp:239] Iteration 139730 (2.49672 iter/s, 4.00526s/10 iters), loss = 7.29544
I0523 04:11:54.139127 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29544 (* 1 = 7.29544 loss)
I0523 04:11:54.146427 35003 sgd_solver.cpp:112] Iteration 139730, lr = 0.01
I0523 04:11:59.165189 35003 solver.cpp:239] Iteration 139740 (1.98971 iter/s, 5.02586s/10 iters), loss = 5.14425
I0523 04:11:59.165225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.14425 (* 1 = 5.14425 loss)
I0523 04:11:59.172880 35003 sgd_solver.cpp:112] Iteration 139740, lr = 0.01
I0523 04:12:04.073971 35003 solver.cpp:239] Iteration 139750 (2.03726 iter/s, 4.90854s/10 iters), loss = 7.50339
I0523 04:12:04.074008 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50339 (* 1 = 7.50339 loss)
I0523 04:12:04.086707 35003 sgd_solver.cpp:112] Iteration 139750, lr = 0.01
I0523 04:12:07.605105 35003 solver.cpp:239] Iteration 139760 (2.8321 iter/s, 3.53095s/10 iters), loss = 7.00748
I0523 04:12:07.605407 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00748 (* 1 = 7.00748 loss)
I0523 04:12:08.300359 35003 sgd_solver.cpp:112] Iteration 139760, lr = 0.01
I0523 04:12:11.625226 35003 solver.cpp:239] Iteration 139770 (2.48776 iter/s, 4.01969s/10 iters), loss = 6.29587
I0523 04:12:11.625273 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29587 (* 1 = 6.29587 loss)
I0523 04:12:12.327548 35003 sgd_solver.cpp:112] Iteration 139770, lr = 0.01
I0523 04:12:16.053664 35003 solver.cpp:239] Iteration 139780 (2.25825 iter/s, 4.42821s/10 iters), loss = 6.5271
I0523 04:12:16.053717 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5271 (* 1 = 6.5271 loss)
I0523 04:12:16.071815 35003 sgd_solver.cpp:112] Iteration 139780, lr = 0.01
I0523 04:12:19.456418 35003 solver.cpp:239] Iteration 139790 (2.93896 iter/s, 3.40256s/10 iters), loss = 7.03297
I0523 04:12:19.456463 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03297 (* 1 = 7.03297 loss)
I0523 04:12:19.470698 35003 sgd_solver.cpp:112] Iteration 139790, lr = 0.01
I0523 04:12:21.451715 35003 solver.cpp:239] Iteration 139800 (5.01214 iter/s, 1.99515s/10 iters), loss = 6.70481
I0523 04:12:21.451781 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70481 (* 1 = 6.70481 loss)
I0523 04:12:21.465204 35003 sgd_solver.cpp:112] Iteration 139800, lr = 0.01
I0523 04:12:24.965021 35003 solver.cpp:239] Iteration 139810 (2.84649 iter/s, 3.5131s/10 iters), loss = 6.38841
I0523 04:12:24.965071 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38841 (* 1 = 6.38841 loss)
I0523 04:12:25.094436 35003 sgd_solver.cpp:112] Iteration 139810, lr = 0.01
I0523 04:12:28.742357 35003 solver.cpp:239] Iteration 139820 (2.64752 iter/s, 3.77712s/10 iters), loss = 6.88003
I0523 04:12:28.742408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88003 (* 1 = 6.88003 loss)
I0523 04:12:29.450855 35003 sgd_solver.cpp:112] Iteration 139820, lr = 0.01
I0523 04:12:33.224861 35003 solver.cpp:239] Iteration 139830 (2.23101 iter/s, 4.48227s/10 iters), loss = 6.72503
I0523 04:12:33.224901 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72503 (* 1 = 6.72503 loss)
I0523 04:12:33.238543 35003 sgd_solver.cpp:112] Iteration 139830, lr = 0.01
I0523 04:12:35.999307 35003 solver.cpp:239] Iteration 139840 (3.60454 iter/s, 2.77428s/10 iters), loss = 6.39836
I0523 04:12:35.999377 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39836 (* 1 = 6.39836 loss)
I0523 04:12:36.012495 35003 sgd_solver.cpp:112] Iteration 139840, lr = 0.01
I0523 04:12:38.778656 35003 solver.cpp:239] Iteration 139850 (3.59821 iter/s, 2.77916s/10 iters), loss = 6.75724
I0523 04:12:38.778897 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75724 (* 1 = 6.75724 loss)
I0523 04:12:38.791260 35003 sgd_solver.cpp:112] Iteration 139850, lr = 0.01
I0523 04:12:41.336426 35003 solver.cpp:239] Iteration 139860 (3.91015 iter/s, 2.55744s/10 iters), loss = 6.49794
I0523 04:12:41.336465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49794 (* 1 = 6.49794 loss)
I0523 04:12:41.342814 35003 sgd_solver.cpp:112] Iteration 139860, lr = 0.01
I0523 04:12:43.404765 35003 solver.cpp:239] Iteration 139870 (4.83513 iter/s, 2.0682s/10 iters), loss = 7.9079
I0523 04:12:43.404820 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9079 (* 1 = 7.9079 loss)
I0523 04:12:43.407343 35003 sgd_solver.cpp:112] Iteration 139870, lr = 0.01
I0523 04:12:47.058789 35003 solver.cpp:239] Iteration 139880 (2.73687 iter/s, 3.65381s/10 iters), loss = 7.07204
I0523 04:12:47.058840 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07204 (* 1 = 7.07204 loss)
I0523 04:12:47.761216 35003 sgd_solver.cpp:112] Iteration 139880, lr = 0.01
I0523 04:12:51.386060 35003 solver.cpp:239] Iteration 139890 (2.31105 iter/s, 4.32704s/10 iters), loss = 7.19696
I0523 04:12:51.386106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19696 (* 1 = 7.19696 loss)
I0523 04:12:52.094424 35003 sgd_solver.cpp:112] Iteration 139890, lr = 0.01
I0523 04:12:54.854812 35003 solver.cpp:239] Iteration 139900 (2.88304 iter/s, 3.46856s/10 iters), loss = 5.81191
I0523 04:12:54.854869 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.81191 (* 1 = 5.81191 loss)
I0523 04:12:54.868494 35003 sgd_solver.cpp:112] Iteration 139900, lr = 0.01
I0523 04:12:58.070880 35003 solver.cpp:239] Iteration 139910 (3.10958 iter/s, 3.21587s/10 iters), loss = 7.53725
I0523 04:12:58.070938 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53725 (* 1 = 7.53725 loss)
I0523 04:12:58.079818 35003 sgd_solver.cpp:112] Iteration 139910, lr = 0.01
I0523 04:13:01.042201 35003 solver.cpp:239] Iteration 139920 (3.36571 iter/s, 2.97114s/10 iters), loss = 6.85194
I0523 04:13:01.042243 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85194 (* 1 = 6.85194 loss)
I0523 04:13:01.731516 35003 sgd_solver.cpp:112] Iteration 139920, lr = 0.01
I0523 04:13:05.263749 35003 solver.cpp:239] Iteration 139930 (2.36892 iter/s, 4.22134s/10 iters), loss = 7.64248
I0523 04:13:05.263784 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64248 (* 1 = 7.64248 loss)
I0523 04:13:05.276461 35003 sgd_solver.cpp:112] Iteration 139930, lr = 0.01
I0523 04:13:08.827538 35003 solver.cpp:239] Iteration 139940 (2.80615 iter/s, 3.5636s/10 iters), loss = 6.34753
I0523 04:13:08.827813 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34753 (* 1 = 6.34753 loss)
I0523 04:13:08.840495 35003 sgd_solver.cpp:112] Iteration 139940, lr = 0.01
I0523 04:13:12.583832 35003 solver.cpp:239] Iteration 139950 (2.66248 iter/s, 3.75589s/10 iters), loss = 7.79714
I0523 04:13:12.583883 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79714 (* 1 = 7.79714 loss)
I0523 04:13:12.592795 35003 sgd_solver.cpp:112] Iteration 139950, lr = 0.01
I0523 04:13:18.233449 35003 solver.cpp:239] Iteration 139960 (1.77012 iter/s, 5.64933s/10 iters), loss = 6.25053
I0523 04:13:18.233500 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25053 (* 1 = 6.25053 loss)
I0523 04:13:18.240114 35003 sgd_solver.cpp:112] Iteration 139960, lr = 0.01
I0523 04:13:21.484647 35003 solver.cpp:239] Iteration 139970 (3.07598 iter/s, 3.251s/10 iters), loss = 7.41622
I0523 04:13:21.484691 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41622 (* 1 = 7.41622 loss)
I0523 04:13:21.492770 35003 sgd_solver.cpp:112] Iteration 139970, lr = 0.01
I0523 04:13:24.915822 35003 solver.cpp:239] Iteration 139980 (2.91461 iter/s, 3.43099s/10 iters), loss = 6.77477
I0523 04:13:24.915861 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77477 (* 1 = 6.77477 loss)
I0523 04:13:24.923439 35003 sgd_solver.cpp:112] Iteration 139980, lr = 0.01
I0523 04:13:27.839449 35003 solver.cpp:239] Iteration 139990 (3.42061 iter/s, 2.92346s/10 iters), loss = 7.69212
I0523 04:13:27.839507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69212 (* 1 = 7.69212 loss)
I0523 04:13:27.922552 35003 sgd_solver.cpp:112] Iteration 139990, lr = 0.01
I0523 04:13:31.041328 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_140000.caffemodel
I0523 04:13:31.831254 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_140000.solverstate
I0523 04:13:32.042131 35003 solver.cpp:239] Iteration 140000 (2.37957 iter/s, 4.20245s/10 iters), loss = 7.6915
I0523 04:13:32.042198 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6915 (* 1 = 7.6915 loss)
I0523 04:13:32.563079 35003 sgd_solver.cpp:112] Iteration 140000, lr = 0.01
I0523 04:13:36.847192 35003 solver.cpp:239] Iteration 140010 (2.08125 iter/s, 4.8048s/10 iters), loss = 6.92472
I0523 04:13:36.847234 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92472 (* 1 = 6.92472 loss)
I0523 04:13:36.976680 35003 sgd_solver.cpp:112] Iteration 140010, lr = 0.01
I0523 04:13:40.717453 35003 solver.cpp:239] Iteration 140020 (2.58394 iter/s, 3.87006s/10 iters), loss = 6.72535
I0523 04:13:40.717658 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72535 (* 1 = 6.72535 loss)
I0523 04:13:40.905325 35003 sgd_solver.cpp:112] Iteration 140020, lr = 0.01
I0523 04:13:45.342542 35003 solver.cpp:239] Iteration 140030 (2.16231 iter/s, 4.62469s/10 iters), loss = 6.05048
I0523 04:13:45.342592 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05048 (* 1 = 6.05048 loss)
I0523 04:13:45.360769 35003 sgd_solver.cpp:112] Iteration 140030, lr = 0.01
I0523 04:13:46.644714 35003 solver.cpp:239] Iteration 140040 (7.68019 iter/s, 1.30205s/10 iters), loss = 6.75838
I0523 04:13:46.644769 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75838 (* 1 = 6.75838 loss)
I0523 04:13:46.662708 35003 sgd_solver.cpp:112] Iteration 140040, lr = 0.01
I0523 04:13:49.501873 35003 solver.cpp:239] Iteration 140050 (3.50019 iter/s, 2.85699s/10 iters), loss = 8.04347
I0523 04:13:49.501910 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04347 (* 1 = 8.04347 loss)
I0523 04:13:49.514667 35003 sgd_solver.cpp:112] Iteration 140050, lr = 0.01
I0523 04:13:51.607213 35003 solver.cpp:239] Iteration 140060 (4.75013 iter/s, 2.1052s/10 iters), loss = 6.93519
I0523 04:13:51.607270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93519 (* 1 = 6.93519 loss)
I0523 04:13:51.612457 35003 sgd_solver.cpp:112] Iteration 140060, lr = 0.01
I0523 04:13:55.301086 35003 solver.cpp:239] Iteration 140070 (2.70734 iter/s, 3.69366s/10 iters), loss = 7.54594
I0523 04:13:55.301142 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54594 (* 1 = 7.54594 loss)
I0523 04:13:55.909935 35003 sgd_solver.cpp:112] Iteration 140070, lr = 0.01
I0523 04:13:58.641028 35003 solver.cpp:239] Iteration 140080 (2.99424 iter/s, 3.33975s/10 iters), loss = 7.17447
I0523 04:13:58.641065 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17447 (* 1 = 7.17447 loss)
I0523 04:13:58.659505 35003 sgd_solver.cpp:112] Iteration 140080, lr = 0.01
I0523 04:14:02.021898 35003 solver.cpp:239] Iteration 140090 (2.95797 iter/s, 3.38069s/10 iters), loss = 7.53922
I0523 04:14:02.021932 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53922 (* 1 = 7.53922 loss)
I0523 04:14:02.034332 35003 sgd_solver.cpp:112] Iteration 140090, lr = 0.01
I0523 04:14:06.436731 35003 solver.cpp:239] Iteration 140100 (2.2652 iter/s, 4.41461s/10 iters), loss = 6.59284
I0523 04:14:06.436774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59284 (* 1 = 6.59284 loss)
I0523 04:14:06.444511 35003 sgd_solver.cpp:112] Iteration 140100, lr = 0.01
I0523 04:14:10.729715 35003 solver.cpp:239] Iteration 140110 (2.3295 iter/s, 4.29277s/10 iters), loss = 7.29369
I0523 04:14:10.729981 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29369 (* 1 = 7.29369 loss)
I0523 04:14:10.743058 35003 sgd_solver.cpp:112] Iteration 140110, lr = 0.01
I0523 04:14:14.657503 35003 solver.cpp:239] Iteration 140120 (2.54623 iter/s, 3.92738s/10 iters), loss = 8.14379
I0523 04:14:14.657564 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.14379 (* 1 = 8.14379 loss)
I0523 04:14:14.670938 35003 sgd_solver.cpp:112] Iteration 140120, lr = 0.01
I0523 04:14:18.135326 35003 solver.cpp:239] Iteration 140130 (2.87553 iter/s, 3.47762s/10 iters), loss = 7.05324
I0523 04:14:18.135370 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05324 (* 1 = 7.05324 loss)
I0523 04:14:18.148859 35003 sgd_solver.cpp:112] Iteration 140130, lr = 0.01
I0523 04:14:21.232641 35003 solver.cpp:239] Iteration 140140 (3.22884 iter/s, 3.09709s/10 iters), loss = 6.30959
I0523 04:14:21.232707 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30959 (* 1 = 6.30959 loss)
I0523 04:14:21.250660 35003 sgd_solver.cpp:112] Iteration 140140, lr = 0.01
I0523 04:14:24.836465 35003 solver.cpp:239] Iteration 140150 (2.77499 iter/s, 3.60361s/10 iters), loss = 6.13937
I0523 04:14:24.836508 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13937 (* 1 = 6.13937 loss)
I0523 04:14:25.571540 35003 sgd_solver.cpp:112] Iteration 140150, lr = 0.01
I0523 04:14:29.821517 35003 solver.cpp:239] Iteration 140160 (2.0061 iter/s, 4.9848s/10 iters), loss = 8.60304
I0523 04:14:29.821573 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.60304 (* 1 = 8.60304 loss)
I0523 04:14:30.536370 35003 sgd_solver.cpp:112] Iteration 140160, lr = 0.01
I0523 04:14:33.772717 35003 solver.cpp:239] Iteration 140170 (2.53102 iter/s, 3.95098s/10 iters), loss = 6.03223
I0523 04:14:33.772758 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03223 (* 1 = 6.03223 loss)
I0523 04:14:33.778085 35003 sgd_solver.cpp:112] Iteration 140170, lr = 0.01
I0523 04:14:37.221163 35003 solver.cpp:239] Iteration 140180 (2.90001 iter/s, 3.44826s/10 iters), loss = 6.59303
I0523 04:14:37.221209 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59303 (* 1 = 6.59303 loss)
I0523 04:14:37.232394 35003 sgd_solver.cpp:112] Iteration 140180, lr = 0.01
I0523 04:14:40.082469 35003 solver.cpp:239] Iteration 140190 (3.49512 iter/s, 2.86113s/10 iters), loss = 7.2271
I0523 04:14:40.082518 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2271 (* 1 = 7.2271 loss)
I0523 04:14:40.100438 35003 sgd_solver.cpp:112] Iteration 140190, lr = 0.01
I0523 04:14:43.717357 35003 solver.cpp:239] Iteration 140200 (2.75127 iter/s, 3.63469s/10 iters), loss = 6.80534
I0523 04:14:43.717556 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80534 (* 1 = 6.80534 loss)
I0523 04:14:43.727363 35003 sgd_solver.cpp:112] Iteration 140200, lr = 0.01
I0523 04:14:48.025420 35003 solver.cpp:239] Iteration 140210 (2.32141 iter/s, 4.30772s/10 iters), loss = 7.57395
I0523 04:14:48.025461 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57395 (* 1 = 7.57395 loss)
I0523 04:14:48.042630 35003 sgd_solver.cpp:112] Iteration 140210, lr = 0.01
I0523 04:14:50.948246 35003 solver.cpp:239] Iteration 140220 (3.42155 iter/s, 2.92265s/10 iters), loss = 7.89844
I0523 04:14:50.948292 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89844 (* 1 = 7.89844 loss)
I0523 04:14:51.684393 35003 sgd_solver.cpp:112] Iteration 140220, lr = 0.01
I0523 04:14:55.296283 35003 solver.cpp:239] Iteration 140230 (2.30001 iter/s, 4.34781s/10 iters), loss = 6.40045
I0523 04:14:55.296336 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40045 (* 1 = 6.40045 loss)
I0523 04:14:55.302312 35003 sgd_solver.cpp:112] Iteration 140230, lr = 0.01
I0523 04:14:58.120961 35003 solver.cpp:239] Iteration 140240 (3.54045 iter/s, 2.8245s/10 iters), loss = 6.54924
I0523 04:14:58.121021 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54924 (* 1 = 6.54924 loss)
I0523 04:14:58.132613 35003 sgd_solver.cpp:112] Iteration 140240, lr = 0.01
I0523 04:15:00.428732 35003 solver.cpp:239] Iteration 140250 (4.3335 iter/s, 2.3076s/10 iters), loss = 7.63135
I0523 04:15:00.428777 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63135 (* 1 = 7.63135 loss)
I0523 04:15:00.452322 35003 sgd_solver.cpp:112] Iteration 140250, lr = 0.01
I0523 04:15:06.092397 35003 solver.cpp:239] Iteration 140260 (1.76573 iter/s, 5.66339s/10 iters), loss = 7.29976
I0523 04:15:06.092434 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29976 (* 1 = 7.29976 loss)
I0523 04:15:06.105427 35003 sgd_solver.cpp:112] Iteration 140260, lr = 0.01
I0523 04:15:09.301409 35003 solver.cpp:239] Iteration 140270 (3.11642 iter/s, 3.20881s/10 iters), loss = 8.04634
I0523 04:15:09.301483 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04634 (* 1 = 8.04634 loss)
I0523 04:15:09.306246 35003 sgd_solver.cpp:112] Iteration 140270, lr = 0.01
I0523 04:15:12.634284 35003 solver.cpp:239] Iteration 140280 (3.0006 iter/s, 3.33267s/10 iters), loss = 6.36542
I0523 04:15:12.634336 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36542 (* 1 = 6.36542 loss)
I0523 04:15:12.642233 35003 sgd_solver.cpp:112] Iteration 140280, lr = 0.01
I0523 04:15:15.564366 35003 solver.cpp:239] Iteration 140290 (3.41309 iter/s, 2.9299s/10 iters), loss = 6.88565
I0523 04:15:15.564558 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88565 (* 1 = 6.88565 loss)
I0523 04:15:16.279901 35003 sgd_solver.cpp:112] Iteration 140290, lr = 0.01
I0523 04:15:19.828254 35003 solver.cpp:239] Iteration 140300 (2.34548 iter/s, 4.26352s/10 iters), loss = 6.92874
I0523 04:15:19.828308 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92874 (* 1 = 6.92874 loss)
I0523 04:15:20.371624 35003 sgd_solver.cpp:112] Iteration 140300, lr = 0.01
I0523 04:15:23.522593 35003 solver.cpp:239] Iteration 140310 (2.707 iter/s, 3.69413s/10 iters), loss = 7.17417
I0523 04:15:23.522657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17417 (* 1 = 7.17417 loss)
I0523 04:15:24.048246 35003 sgd_solver.cpp:112] Iteration 140310, lr = 0.01
I0523 04:15:26.474267 35003 solver.cpp:239] Iteration 140320 (3.38812 iter/s, 2.95149s/10 iters), loss = 6.87885
I0523 04:15:26.474308 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87885 (* 1 = 6.87885 loss)
I0523 04:15:26.487494 35003 sgd_solver.cpp:112] Iteration 140320, lr = 0.01
I0523 04:15:29.329651 35003 solver.cpp:239] Iteration 140330 (3.50236 iter/s, 2.85522s/10 iters), loss = 7.06566
I0523 04:15:29.329692 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06566 (* 1 = 7.06566 loss)
I0523 04:15:29.336673 35003 sgd_solver.cpp:112] Iteration 140330, lr = 0.01
I0523 04:15:34.839156 35003 solver.cpp:239] Iteration 140340 (1.81514 iter/s, 5.50923s/10 iters), loss = 6.91854
I0523 04:15:34.839196 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91854 (* 1 = 6.91854 loss)
I0523 04:15:34.871392 35003 sgd_solver.cpp:112] Iteration 140340, lr = 0.01
I0523 04:15:37.636921 35003 solver.cpp:239] Iteration 140350 (3.57449 iter/s, 2.79761s/10 iters), loss = 7.42342
I0523 04:15:37.636984 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42342 (* 1 = 7.42342 loss)
I0523 04:15:38.351948 35003 sgd_solver.cpp:112] Iteration 140350, lr = 0.01
I0523 04:15:40.506549 35003 solver.cpp:239] Iteration 140360 (3.48501 iter/s, 2.86944s/10 iters), loss = 7.40588
I0523 04:15:40.506594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40588 (* 1 = 7.40588 loss)
I0523 04:15:41.248127 35003 sgd_solver.cpp:112] Iteration 140360, lr = 0.01
I0523 04:15:43.114748 35003 solver.cpp:239] Iteration 140370 (3.83429 iter/s, 2.60804s/10 iters), loss = 6.08617
I0523 04:15:43.114794 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08617 (* 1 = 6.08617 loss)
I0523 04:15:43.122251 35003 sgd_solver.cpp:112] Iteration 140370, lr = 0.01
I0523 04:15:47.170943 35003 solver.cpp:239] Iteration 140380 (2.4655 iter/s, 4.05598s/10 iters), loss = 6.02759
I0523 04:15:47.171111 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02759 (* 1 = 6.02759 loss)
I0523 04:15:47.189050 35003 sgd_solver.cpp:112] Iteration 140380, lr = 0.01
I0523 04:15:50.625164 35003 solver.cpp:239] Iteration 140390 (2.89526 iter/s, 3.45392s/10 iters), loss = 6.73827
I0523 04:15:50.625207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73827 (* 1 = 6.73827 loss)
I0523 04:15:50.632616 35003 sgd_solver.cpp:112] Iteration 140390, lr = 0.01
I0523 04:15:53.627358 35003 solver.cpp:239] Iteration 140400 (3.33108 iter/s, 3.00203s/10 iters), loss = 6.79425
I0523 04:15:53.627398 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79425 (* 1 = 6.79425 loss)
I0523 04:15:53.640271 35003 sgd_solver.cpp:112] Iteration 140400, lr = 0.01
I0523 04:15:55.456699 35003 solver.cpp:239] Iteration 140410 (5.46682 iter/s, 1.82922s/10 iters), loss = 6.41573
I0523 04:15:55.456749 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41573 (* 1 = 6.41573 loss)
I0523 04:15:55.461458 35003 sgd_solver.cpp:112] Iteration 140410, lr = 0.01
I0523 04:15:58.640132 35003 solver.cpp:239] Iteration 140420 (3.14145 iter/s, 3.18325s/10 iters), loss = 7.66298
I0523 04:15:58.640182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66298 (* 1 = 7.66298 loss)
I0523 04:15:59.374301 35003 sgd_solver.cpp:112] Iteration 140420, lr = 0.01
I0523 04:16:03.030269 35003 solver.cpp:239] Iteration 140430 (2.27795 iter/s, 4.38991s/10 iters), loss = 6.19788
I0523 04:16:03.030333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19788 (* 1 = 6.19788 loss)
I0523 04:16:03.188318 35003 sgd_solver.cpp:112] Iteration 140430, lr = 0.01
I0523 04:16:07.643524 35003 solver.cpp:239] Iteration 140440 (2.16779 iter/s, 4.613s/10 iters), loss = 6.45759
I0523 04:16:07.643579 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45759 (* 1 = 6.45759 loss)
I0523 04:16:08.007825 35003 sgd_solver.cpp:112] Iteration 140440, lr = 0.01
I0523 04:16:11.814682 35003 solver.cpp:239] Iteration 140450 (2.39755 iter/s, 4.17093s/10 iters), loss = 7.45684
I0523 04:16:11.814774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45684 (* 1 = 7.45684 loss)
I0523 04:16:11.827757 35003 sgd_solver.cpp:112] Iteration 140450, lr = 0.01
I0523 04:16:15.372231 35003 solver.cpp:239] Iteration 140460 (2.81111 iter/s, 3.55731s/10 iters), loss = 7.62517
I0523 04:16:15.372278 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62517 (* 1 = 7.62517 loss)
I0523 04:16:15.381907 35003 sgd_solver.cpp:112] Iteration 140460, lr = 0.01
I0523 04:16:19.698439 35003 solver.cpp:239] Iteration 140470 (2.31161 iter/s, 4.32598s/10 iters), loss = 7.56385
I0523 04:16:19.698652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56385 (* 1 = 7.56385 loss)
I0523 04:16:19.711455 35003 sgd_solver.cpp:112] Iteration 140470, lr = 0.01
I0523 04:16:24.569703 35003 solver.cpp:239] Iteration 140480 (2.05302 iter/s, 4.87088s/10 iters), loss = 8.14252
I0523 04:16:24.569751 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.14252 (* 1 = 8.14252 loss)
I0523 04:16:24.912860 35003 sgd_solver.cpp:112] Iteration 140480, lr = 0.01
I0523 04:16:27.713343 35003 solver.cpp:239] Iteration 140490 (3.18122 iter/s, 3.14345s/10 iters), loss = 6.32434
I0523 04:16:27.713408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32434 (* 1 = 6.32434 loss)
I0523 04:16:28.344128 35003 sgd_solver.cpp:112] Iteration 140490, lr = 0.01
I0523 04:16:31.246922 35003 solver.cpp:239] Iteration 140500 (2.83016 iter/s, 3.53337s/10 iters), loss = 6.87983
I0523 04:16:31.246964 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87983 (* 1 = 6.87983 loss)
I0523 04:16:31.274442 35003 sgd_solver.cpp:112] Iteration 140500, lr = 0.01
I0523 04:16:35.677161 35003 solver.cpp:239] Iteration 140510 (2.25733 iter/s, 4.43001s/10 iters), loss = 7.75208
I0523 04:16:35.677211 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75208 (* 1 = 7.75208 loss)
I0523 04:16:36.366478 35003 sgd_solver.cpp:112] Iteration 140510, lr = 0.01
I0523 04:16:39.940697 35003 solver.cpp:239] Iteration 140520 (2.34559 iter/s, 4.26331s/10 iters), loss = 7.42328
I0523 04:16:39.940738 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42328 (* 1 = 7.42328 loss)
I0523 04:16:39.948218 35003 sgd_solver.cpp:112] Iteration 140520, lr = 0.01
I0523 04:16:43.761240 35003 solver.cpp:239] Iteration 140530 (2.61757 iter/s, 3.82034s/10 iters), loss = 8.05434
I0523 04:16:43.761288 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05434 (* 1 = 8.05434 loss)
I0523 04:16:44.499223 35003 sgd_solver.cpp:112] Iteration 140530, lr = 0.01
I0523 04:16:48.966823 35003 solver.cpp:239] Iteration 140540 (1.92111 iter/s, 5.20532s/10 iters), loss = 7.33296
I0523 04:16:48.966873 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33296 (* 1 = 7.33296 loss)
I0523 04:16:48.977896 35003 sgd_solver.cpp:112] Iteration 140540, lr = 0.01
I0523 04:16:50.870375 35003 solver.cpp:239] Iteration 140550 (5.25371 iter/s, 1.90342s/10 iters), loss = 7.07387
I0523 04:16:50.870681 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07387 (* 1 = 7.07387 loss)
I0523 04:16:51.611711 35003 sgd_solver.cpp:112] Iteration 140550, lr = 0.01
I0523 04:16:54.075373 35003 solver.cpp:239] Iteration 140560 (3.12052 iter/s, 3.2046s/10 iters), loss = 7.4709
I0523 04:16:54.075422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4709 (* 1 = 7.4709 loss)
I0523 04:16:54.802971 35003 sgd_solver.cpp:112] Iteration 140560, lr = 0.01
I0523 04:16:57.721451 35003 solver.cpp:239] Iteration 140570 (2.74282 iter/s, 3.64588s/10 iters), loss = 7.49097
I0523 04:16:57.721496 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49097 (* 1 = 7.49097 loss)
I0523 04:16:58.430582 35003 sgd_solver.cpp:112] Iteration 140570, lr = 0.01
I0523 04:17:01.487395 35003 solver.cpp:239] Iteration 140580 (2.65552 iter/s, 3.76574s/10 iters), loss = 6.91043
I0523 04:17:01.487437 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91043 (* 1 = 6.91043 loss)
I0523 04:17:01.498839 35003 sgd_solver.cpp:112] Iteration 140580, lr = 0.01
I0523 04:17:04.458861 35003 solver.cpp:239] Iteration 140590 (3.37051 iter/s, 2.96691s/10 iters), loss = 6.59467
I0523 04:17:04.458904 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59467 (* 1 = 6.59467 loss)
I0523 04:17:05.141795 35003 sgd_solver.cpp:112] Iteration 140590, lr = 0.01
I0523 04:17:08.466388 35003 solver.cpp:239] Iteration 140600 (2.49544 iter/s, 4.00731s/10 iters), loss = 6.61724
I0523 04:17:08.466441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61724 (* 1 = 6.61724 loss)
I0523 04:17:09.081833 35003 sgd_solver.cpp:112] Iteration 140600, lr = 0.01
I0523 04:17:12.639286 35003 solver.cpp:239] Iteration 140610 (2.39655 iter/s, 4.17267s/10 iters), loss = 8.02109
I0523 04:17:12.639325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02109 (* 1 = 8.02109 loss)
I0523 04:17:12.652830 35003 sgd_solver.cpp:112] Iteration 140610, lr = 0.01
I0523 04:17:16.376899 35003 solver.cpp:239] Iteration 140620 (2.67564 iter/s, 3.73742s/10 iters), loss = 6.20063
I0523 04:17:16.376942 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20063 (* 1 = 6.20063 loss)
I0523 04:17:16.380813 35003 sgd_solver.cpp:112] Iteration 140620, lr = 0.01
I0523 04:17:20.692456 35003 solver.cpp:239] Iteration 140630 (2.31732 iter/s, 4.31533s/10 iters), loss = 6.91719
I0523 04:17:20.692520 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91719 (* 1 = 6.91719 loss)
I0523 04:17:21.407883 35003 sgd_solver.cpp:112] Iteration 140630, lr = 0.01
I0523 04:17:23.490088 35003 solver.cpp:239] Iteration 140640 (3.57469 iter/s, 2.79745s/10 iters), loss = 7.51197
I0523 04:17:23.490141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51197 (* 1 = 7.51197 loss)
I0523 04:17:23.493273 35003 sgd_solver.cpp:112] Iteration 140640, lr = 0.01
I0523 04:17:27.579751 35003 solver.cpp:239] Iteration 140650 (2.44532 iter/s, 4.08944s/10 iters), loss = 7.15675
I0523 04:17:27.579797 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15675 (* 1 = 7.15675 loss)
I0523 04:17:27.832873 35003 sgd_solver.cpp:112] Iteration 140650, lr = 0.01
I0523 04:17:31.239675 35003 solver.cpp:239] Iteration 140660 (2.73245 iter/s, 3.65972s/10 iters), loss = 5.60828
I0523 04:17:31.239725 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.60828 (* 1 = 5.60828 loss)
I0523 04:17:31.741855 35003 sgd_solver.cpp:112] Iteration 140660, lr = 0.01
I0523 04:17:34.070838 35003 solver.cpp:239] Iteration 140670 (3.53233 iter/s, 2.83099s/10 iters), loss = 6.60639
I0523 04:17:34.070884 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60639 (* 1 = 6.60639 loss)
I0523 04:17:34.809898 35003 sgd_solver.cpp:112] Iteration 140670, lr = 0.01
I0523 04:17:37.779827 35003 solver.cpp:239] Iteration 140680 (2.6963 iter/s, 3.70878s/10 iters), loss = 7.35317
I0523 04:17:37.779886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35317 (* 1 = 7.35317 loss)
I0523 04:17:38.454533 35003 sgd_solver.cpp:112] Iteration 140680, lr = 0.01
I0523 04:17:42.192864 35003 solver.cpp:239] Iteration 140690 (2.26613 iter/s, 4.4128s/10 iters), loss = 7.03534
I0523 04:17:42.192909 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03534 (* 1 = 7.03534 loss)
I0523 04:17:42.742859 35003 sgd_solver.cpp:112] Iteration 140690, lr = 0.01
I0523 04:17:45.560719 35003 solver.cpp:239] Iteration 140700 (2.96942 iter/s, 3.36766s/10 iters), loss = 6.5525
I0523 04:17:45.560787 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5525 (* 1 = 6.5525 loss)
I0523 04:17:45.566162 35003 sgd_solver.cpp:112] Iteration 140700, lr = 0.01
I0523 04:17:48.970741 35003 solver.cpp:239] Iteration 140710 (2.93272 iter/s, 3.40981s/10 iters), loss = 6.29642
I0523 04:17:48.970810 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29642 (* 1 = 6.29642 loss)
I0523 04:17:48.996076 35003 sgd_solver.cpp:112] Iteration 140710, lr = 0.01
I0523 04:17:52.823855 35003 solver.cpp:239] Iteration 140720 (2.59546 iter/s, 3.85288s/10 iters), loss = 6.54801
I0523 04:17:52.824043 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54801 (* 1 = 6.54801 loss)
I0523 04:17:52.830927 35003 sgd_solver.cpp:112] Iteration 140720, lr = 0.01
I0523 04:17:56.304023 35003 solver.cpp:239] Iteration 140730 (2.87371 iter/s, 3.47983s/10 iters), loss = 6.62453
I0523 04:17:56.304076 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62453 (* 1 = 6.62453 loss)
I0523 04:17:57.012138 35003 sgd_solver.cpp:112] Iteration 140730, lr = 0.01
I0523 04:18:02.125355 35003 solver.cpp:239] Iteration 140740 (1.7179 iter/s, 5.82104s/10 iters), loss = 6.94508
I0523 04:18:02.125412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94508 (* 1 = 6.94508 loss)
I0523 04:18:02.846659 35003 sgd_solver.cpp:112] Iteration 140740, lr = 0.01
I0523 04:18:04.785547 35003 solver.cpp:239] Iteration 140750 (3.75937 iter/s, 2.66002s/10 iters), loss = 7.37292
I0523 04:18:04.785598 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37292 (* 1 = 7.37292 loss)
I0523 04:18:04.798580 35003 sgd_solver.cpp:112] Iteration 140750, lr = 0.01
I0523 04:18:09.130367 35003 solver.cpp:239] Iteration 140760 (2.30172 iter/s, 4.34458s/10 iters), loss = 6.91005
I0523 04:18:09.130427 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91005 (* 1 = 6.91005 loss)
I0523 04:18:09.164721 35003 sgd_solver.cpp:112] Iteration 140760, lr = 0.01
I0523 04:18:12.685276 35003 solver.cpp:239] Iteration 140770 (2.81317 iter/s, 3.5547s/10 iters), loss = 7.12145
I0523 04:18:12.685314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12145 (* 1 = 7.12145 loss)
I0523 04:18:12.709750 35003 sgd_solver.cpp:112] Iteration 140770, lr = 0.01
I0523 04:18:16.148665 35003 solver.cpp:239] Iteration 140780 (2.8875 iter/s, 3.4632s/10 iters), loss = 6.53523
I0523 04:18:16.148705 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53523 (* 1 = 6.53523 loss)
I0523 04:18:16.152628 35003 sgd_solver.cpp:112] Iteration 140780, lr = 0.01
I0523 04:18:19.641392 35003 solver.cpp:239] Iteration 140790 (2.86326 iter/s, 3.49252s/10 iters), loss = 7.3678
I0523 04:18:19.641438 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3678 (* 1 = 7.3678 loss)
I0523 04:18:19.654783 35003 sgd_solver.cpp:112] Iteration 140790, lr = 0.01
I0523 04:18:24.011790 35003 solver.cpp:239] Iteration 140800 (2.28824 iter/s, 4.37017s/10 iters), loss = 7.1299
I0523 04:18:24.012507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1299 (* 1 = 7.1299 loss)
I0523 04:18:24.019829 35003 sgd_solver.cpp:112] Iteration 140800, lr = 0.01
I0523 04:18:27.059571 35003 solver.cpp:239] Iteration 140810 (3.28196 iter/s, 3.04696s/10 iters), loss = 7.70969
I0523 04:18:27.059618 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70969 (* 1 = 7.70969 loss)
I0523 04:18:27.780628 35003 sgd_solver.cpp:112] Iteration 140810, lr = 0.01
I0523 04:18:29.925954 35003 solver.cpp:239] Iteration 140820 (3.48893 iter/s, 2.86621s/10 iters), loss = 6.45109
I0523 04:18:29.925993 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45109 (* 1 = 6.45109 loss)
I0523 04:18:29.937533 35003 sgd_solver.cpp:112] Iteration 140820, lr = 0.01
I0523 04:18:33.910315 35003 solver.cpp:239] Iteration 140830 (2.50995 iter/s, 3.98415s/10 iters), loss = 6.58338
I0523 04:18:33.910368 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58338 (* 1 = 6.58338 loss)
I0523 04:18:34.643378 35003 sgd_solver.cpp:112] Iteration 140830, lr = 0.01
I0523 04:18:39.189097 35003 solver.cpp:239] Iteration 140840 (1.89447 iter/s, 5.27851s/10 iters), loss = 7.35775
I0523 04:18:39.189151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35775 (* 1 = 7.35775 loss)
I0523 04:18:39.376946 35003 sgd_solver.cpp:112] Iteration 140840, lr = 0.01
I0523 04:18:43.235801 35003 solver.cpp:239] Iteration 140850 (2.47128 iter/s, 4.04648s/10 iters), loss = 6.5713
I0523 04:18:43.235848 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5713 (* 1 = 6.5713 loss)
I0523 04:18:43.244843 35003 sgd_solver.cpp:112] Iteration 140850, lr = 0.01
I0523 04:18:46.867925 35003 solver.cpp:239] Iteration 140860 (2.75336 iter/s, 3.63193s/10 iters), loss = 7.60277
I0523 04:18:46.867974 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60277 (* 1 = 7.60277 loss)
I0523 04:18:46.872615 35003 sgd_solver.cpp:112] Iteration 140860, lr = 0.01
I0523 04:18:49.817224 35003 solver.cpp:239] Iteration 140870 (3.39084 iter/s, 2.94912s/10 iters), loss = 5.78323
I0523 04:18:49.817282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.78323 (* 1 = 5.78323 loss)
I0523 04:18:50.532269 35003 sgd_solver.cpp:112] Iteration 140870, lr = 0.01
I0523 04:18:52.621084 35003 solver.cpp:239] Iteration 140880 (3.56674 iter/s, 2.80368s/10 iters), loss = 7.18746
I0523 04:18:52.621134 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18746 (* 1 = 7.18746 loss)
I0523 04:18:53.355526 35003 sgd_solver.cpp:112] Iteration 140880, lr = 0.01
I0523 04:18:57.611006 35003 solver.cpp:239] Iteration 140890 (2.00414 iter/s, 4.98967s/10 iters), loss = 7.81054
I0523 04:18:57.611204 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81054 (* 1 = 7.81054 loss)
I0523 04:18:58.336858 35003 sgd_solver.cpp:112] Iteration 140890, lr = 0.01
I0523 04:19:03.409348 35003 solver.cpp:239] Iteration 140900 (1.72476 iter/s, 5.79792s/10 iters), loss = 7.13381
I0523 04:19:03.409405 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13381 (* 1 = 7.13381 loss)
I0523 04:19:04.105553 35003 sgd_solver.cpp:112] Iteration 140900, lr = 0.01
I0523 04:19:07.194447 35003 solver.cpp:239] Iteration 140910 (2.6421 iter/s, 3.78487s/10 iters), loss = 8.0278
I0523 04:19:07.194486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0278 (* 1 = 8.0278 loss)
I0523 04:19:07.203414 35003 sgd_solver.cpp:112] Iteration 140910, lr = 0.01
I0523 04:19:09.846469 35003 solver.cpp:239] Iteration 140920 (3.77093 iter/s, 2.65187s/10 iters), loss = 6.76091
I0523 04:19:09.846511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76091 (* 1 = 6.76091 loss)
I0523 04:19:09.854413 35003 sgd_solver.cpp:112] Iteration 140920, lr = 0.01
I0523 04:19:14.916680 35003 solver.cpp:239] Iteration 140930 (1.9724 iter/s, 5.06996s/10 iters), loss = 7.16453
I0523 04:19:14.916730 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16453 (* 1 = 7.16453 loss)
I0523 04:19:14.919766 35003 sgd_solver.cpp:112] Iteration 140930, lr = 0.01
I0523 04:19:21.480494 35003 solver.cpp:239] Iteration 140940 (1.52359 iter/s, 6.56343s/10 iters), loss = 6.13023
I0523 04:19:21.480537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13023 (* 1 = 6.13023 loss)
I0523 04:19:21.485563 35003 sgd_solver.cpp:112] Iteration 140940, lr = 0.01
I0523 04:19:25.181258 35003 solver.cpp:239] Iteration 140950 (2.7023 iter/s, 3.70056s/10 iters), loss = 7.97887
I0523 04:19:25.181309 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97887 (* 1 = 7.97887 loss)
I0523 04:19:25.909852 35003 sgd_solver.cpp:112] Iteration 140950, lr = 0.01
I0523 04:19:28.737352 35003 solver.cpp:239] Iteration 140960 (2.81223 iter/s, 3.55589s/10 iters), loss = 6.97786
I0523 04:19:28.737584 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97786 (* 1 = 6.97786 loss)
I0523 04:19:28.755976 35003 sgd_solver.cpp:112] Iteration 140960, lr = 0.01
I0523 04:19:31.642316 35003 solver.cpp:239] Iteration 140970 (3.44278 iter/s, 2.90463s/10 iters), loss = 8.21034
I0523 04:19:31.642359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.21034 (* 1 = 8.21034 loss)
I0523 04:19:31.656067 35003 sgd_solver.cpp:112] Iteration 140970, lr = 0.01
I0523 04:19:35.920945 35003 solver.cpp:239] Iteration 140980 (2.33732 iter/s, 4.2784s/10 iters), loss = 6.57271
I0523 04:19:35.921005 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57271 (* 1 = 6.57271 loss)
I0523 04:19:35.934402 35003 sgd_solver.cpp:112] Iteration 140980, lr = 0.01
I0523 04:19:39.186713 35003 solver.cpp:239] Iteration 140990 (3.06227 iter/s, 3.26555s/10 iters), loss = 6.74442
I0523 04:19:39.186769 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74442 (* 1 = 6.74442 loss)
I0523 04:19:39.198580 35003 sgd_solver.cpp:112] Iteration 140990, lr = 0.01
I0523 04:19:41.148458 35003 solver.cpp:239] Iteration 141000 (5.09789 iter/s, 1.9616s/10 iters), loss = 6.69074
I0523 04:19:41.148499 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69074 (* 1 = 6.69074 loss)
I0523 04:19:41.159713 35003 sgd_solver.cpp:112] Iteration 141000, lr = 0.01
I0523 04:19:44.757658 35003 solver.cpp:239] Iteration 141010 (2.77085 iter/s, 3.609s/10 iters), loss = 7.18306
I0523 04:19:44.757722 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18306 (* 1 = 7.18306 loss)
I0523 04:19:45.492745 35003 sgd_solver.cpp:112] Iteration 141010, lr = 0.01
I0523 04:19:50.439584 35003 solver.cpp:239] Iteration 141020 (1.76006 iter/s, 5.68163s/10 iters), loss = 6.65571
I0523 04:19:50.439654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65571 (* 1 = 6.65571 loss)
I0523 04:19:50.563628 35003 sgd_solver.cpp:112] Iteration 141020, lr = 0.01
I0523 04:19:53.429354 35003 solver.cpp:239] Iteration 141030 (3.34495 iter/s, 2.98958s/10 iters), loss = 6.88301
I0523 04:19:53.429397 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88301 (* 1 = 6.88301 loss)
I0523 04:19:54.144701 35003 sgd_solver.cpp:112] Iteration 141030, lr = 0.01
I0523 04:19:56.996887 35003 solver.cpp:239] Iteration 141040 (2.80321 iter/s, 3.56734s/10 iters), loss = 6.96756
I0523 04:19:56.996927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96756 (* 1 = 6.96756 loss)
I0523 04:19:57.693156 35003 sgd_solver.cpp:112] Iteration 141040, lr = 0.01
I0523 04:20:00.280941 35003 solver.cpp:239] Iteration 141050 (3.04519 iter/s, 3.28387s/10 iters), loss = 6.43921
I0523 04:20:00.281188 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43921 (* 1 = 6.43921 loss)
I0523 04:20:00.969790 35003 sgd_solver.cpp:112] Iteration 141050, lr = 0.01
I0523 04:20:04.514119 35003 solver.cpp:239] Iteration 141060 (2.36251 iter/s, 4.23279s/10 iters), loss = 7.24335
I0523 04:20:04.514190 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24335 (* 1 = 7.24335 loss)
I0523 04:20:04.638633 35003 sgd_solver.cpp:112] Iteration 141060, lr = 0.01
I0523 04:20:08.995425 35003 solver.cpp:239] Iteration 141070 (2.23162 iter/s, 4.48106s/10 iters), loss = 6.0332
I0523 04:20:08.995470 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0332 (* 1 = 6.0332 loss)
I0523 04:20:09.006729 35003 sgd_solver.cpp:112] Iteration 141070, lr = 0.01
I0523 04:20:11.887377 35003 solver.cpp:239] Iteration 141080 (3.45807 iter/s, 2.89179s/10 iters), loss = 7.50726
I0523 04:20:11.887419 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50726 (* 1 = 7.50726 loss)
I0523 04:20:11.990731 35003 sgd_solver.cpp:112] Iteration 141080, lr = 0.01
I0523 04:20:16.334342 35003 solver.cpp:239] Iteration 141090 (2.24884 iter/s, 4.44674s/10 iters), loss = 7.01833
I0523 04:20:16.334393 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01833 (* 1 = 7.01833 loss)
I0523 04:20:16.346909 35003 sgd_solver.cpp:112] Iteration 141090, lr = 0.01
I0523 04:20:20.532879 35003 solver.cpp:239] Iteration 141100 (2.38191 iter/s, 4.19831s/10 iters), loss = 6.17116
I0523 04:20:20.532917 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17116 (* 1 = 6.17116 loss)
I0523 04:20:20.546319 35003 sgd_solver.cpp:112] Iteration 141100, lr = 0.01
I0523 04:20:24.003175 35003 solver.cpp:239] Iteration 141110 (2.88175 iter/s, 3.47011s/10 iters), loss = 6.68182
I0523 04:20:24.003216 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68182 (* 1 = 6.68182 loss)
I0523 04:20:24.024421 35003 sgd_solver.cpp:112] Iteration 141110, lr = 0.01
I0523 04:20:28.456636 35003 solver.cpp:239] Iteration 141120 (2.24556 iter/s, 4.45322s/10 iters), loss = 6.39398
I0523 04:20:28.456694 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39398 (* 1 = 6.39398 loss)
I0523 04:20:29.041327 35003 sgd_solver.cpp:112] Iteration 141120, lr = 0.01
I0523 04:20:30.357745 35003 solver.cpp:239] Iteration 141130 (5.26049 iter/s, 1.90096s/10 iters), loss = 6.27973
I0523 04:20:30.357966 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27973 (* 1 = 6.27973 loss)
I0523 04:20:30.370934 35003 sgd_solver.cpp:112] Iteration 141130, lr = 0.01
I0523 04:20:36.118145 35003 solver.cpp:239] Iteration 141140 (1.73613 iter/s, 5.75995s/10 iters), loss = 7.24018
I0523 04:20:36.118196 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24018 (* 1 = 7.24018 loss)
I0523 04:20:36.130939 35003 sgd_solver.cpp:112] Iteration 141140, lr = 0.01
I0523 04:20:38.958506 35003 solver.cpp:239] Iteration 141150 (3.5209 iter/s, 2.84018s/10 iters), loss = 6.89413
I0523 04:20:38.958547 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89413 (* 1 = 6.89413 loss)
I0523 04:20:38.962260 35003 sgd_solver.cpp:112] Iteration 141150, lr = 0.01
I0523 04:20:43.272611 35003 solver.cpp:239] Iteration 141160 (2.3181 iter/s, 4.31388s/10 iters), loss = 6.22878
I0523 04:20:43.272656 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22878 (* 1 = 6.22878 loss)
I0523 04:20:43.275702 35003 sgd_solver.cpp:112] Iteration 141160, lr = 0.01
I0523 04:20:46.471197 35003 solver.cpp:239] Iteration 141170 (3.12656 iter/s, 3.1984s/10 iters), loss = 8.27281
I0523 04:20:46.471240 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.27281 (* 1 = 8.27281 loss)
I0523 04:20:46.479291 35003 sgd_solver.cpp:112] Iteration 141170, lr = 0.01
I0523 04:20:49.229413 35003 solver.cpp:239] Iteration 141180 (3.62574 iter/s, 2.75806s/10 iters), loss = 7.51791
I0523 04:20:49.229456 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51791 (* 1 = 7.51791 loss)
I0523 04:20:49.244299 35003 sgd_solver.cpp:112] Iteration 141180, lr = 0.01
I0523 04:20:50.561352 35003 solver.cpp:239] Iteration 141190 (7.50844 iter/s, 1.33183s/10 iters), loss = 5.96753
I0523 04:20:50.561401 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96753 (* 1 = 5.96753 loss)
I0523 04:20:50.569175 35003 sgd_solver.cpp:112] Iteration 141190, lr = 0.01
I0523 04:20:55.674988 35003 solver.cpp:239] Iteration 141200 (1.95565 iter/s, 5.11338s/10 iters), loss = 6.85801
I0523 04:20:55.675035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85801 (* 1 = 6.85801 loss)
I0523 04:20:55.693055 35003 sgd_solver.cpp:112] Iteration 141200, lr = 0.01
I0523 04:21:00.924443 35003 solver.cpp:239] Iteration 141210 (1.90505 iter/s, 5.24919s/10 iters), loss = 7.31226
I0523 04:21:00.924731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31226 (* 1 = 7.31226 loss)
I0523 04:21:00.936645 35003 sgd_solver.cpp:112] Iteration 141210, lr = 0.01
I0523 04:21:03.829092 35003 solver.cpp:239] Iteration 141220 (3.44321 iter/s, 2.90427s/10 iters), loss = 6.23112
I0523 04:21:03.829136 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23112 (* 1 = 6.23112 loss)
I0523 04:21:03.842185 35003 sgd_solver.cpp:112] Iteration 141220, lr = 0.01
I0523 04:21:06.922207 35003 solver.cpp:239] Iteration 141230 (3.23317 iter/s, 3.09294s/10 iters), loss = 6.83094
I0523 04:21:06.922271 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83094 (* 1 = 6.83094 loss)
I0523 04:21:06.927974 35003 sgd_solver.cpp:112] Iteration 141230, lr = 0.01
I0523 04:21:10.518465 35003 solver.cpp:239] Iteration 141240 (2.78083 iter/s, 3.59605s/10 iters), loss = 7.16632
I0523 04:21:10.518512 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16632 (* 1 = 7.16632 loss)
I0523 04:21:11.077867 35003 sgd_solver.cpp:112] Iteration 141240, lr = 0.01
I0523 04:21:13.416038 35003 solver.cpp:239] Iteration 141250 (3.45137 iter/s, 2.8974s/10 iters), loss = 6.79884
I0523 04:21:13.416086 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79884 (* 1 = 6.79884 loss)
I0523 04:21:13.803330 35003 sgd_solver.cpp:112] Iteration 141250, lr = 0.01
I0523 04:21:16.375499 35003 solver.cpp:239] Iteration 141260 (3.3792 iter/s, 2.95928s/10 iters), loss = 8.00681
I0523 04:21:16.375576 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00681 (* 1 = 8.00681 loss)
I0523 04:21:16.382984 35003 sgd_solver.cpp:112] Iteration 141260, lr = 0.01
I0523 04:21:18.842375 35003 solver.cpp:239] Iteration 141270 (4.054 iter/s, 2.4667s/10 iters), loss = 6.81727
I0523 04:21:18.842417 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81727 (* 1 = 6.81727 loss)
I0523 04:21:18.850651 35003 sgd_solver.cpp:112] Iteration 141270, lr = 0.01
I0523 04:21:20.137414 35003 solver.cpp:239] Iteration 141280 (7.72241 iter/s, 1.29493s/10 iters), loss = 7.27856
I0523 04:21:20.137454 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27856 (* 1 = 7.27856 loss)
I0523 04:21:20.823390 35003 sgd_solver.cpp:112] Iteration 141280, lr = 0.01
I0523 04:21:22.972038 35003 solver.cpp:239] Iteration 141290 (3.528 iter/s, 2.83447s/10 iters), loss = 7.03184
I0523 04:21:22.972081 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03184 (* 1 = 7.03184 loss)
I0523 04:21:23.687856 35003 sgd_solver.cpp:112] Iteration 141290, lr = 0.01
I0523 04:21:27.289876 35003 solver.cpp:239] Iteration 141300 (2.31611 iter/s, 4.31759s/10 iters), loss = 6.84952
I0523 04:21:27.289930 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84952 (* 1 = 6.84952 loss)
I0523 04:21:27.316669 35003 sgd_solver.cpp:112] Iteration 141300, lr = 0.01
I0523 04:21:29.366991 35003 solver.cpp:239] Iteration 141310 (4.8147 iter/s, 2.07697s/10 iters), loss = 7.75601
I0523 04:21:29.367043 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75601 (* 1 = 7.75601 loss)
I0523 04:21:29.379156 35003 sgd_solver.cpp:112] Iteration 141310, lr = 0.01
I0523 04:21:33.983021 35003 solver.cpp:239] Iteration 141320 (2.16648 iter/s, 4.61578s/10 iters), loss = 6.46169
I0523 04:21:33.983186 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46169 (* 1 = 6.46169 loss)
I0523 04:21:34.702289 35003 sgd_solver.cpp:112] Iteration 141320, lr = 0.01
I0523 04:21:37.503594 35003 solver.cpp:239] Iteration 141330 (2.84069 iter/s, 3.52027s/10 iters), loss = 7.17006
I0523 04:21:37.503644 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17006 (* 1 = 7.17006 loss)
I0523 04:21:37.530267 35003 sgd_solver.cpp:112] Iteration 141330, lr = 0.01
I0523 04:21:41.301985 35003 solver.cpp:239] Iteration 141340 (2.63284 iter/s, 3.79818s/10 iters), loss = 5.72815
I0523 04:21:41.302031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.72815 (* 1 = 5.72815 loss)
I0523 04:21:41.694422 35003 sgd_solver.cpp:112] Iteration 141340, lr = 0.01
I0523 04:21:43.820830 35003 solver.cpp:239] Iteration 141350 (3.97032 iter/s, 2.51869s/10 iters), loss = 6.36002
I0523 04:21:43.820883 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36002 (* 1 = 6.36002 loss)
I0523 04:21:44.549247 35003 sgd_solver.cpp:112] Iteration 141350, lr = 0.01
I0523 04:21:45.865528 35003 solver.cpp:239] Iteration 141360 (4.89104 iter/s, 2.04455s/10 iters), loss = 7.09762
I0523 04:21:45.865566 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09762 (* 1 = 7.09762 loss)
I0523 04:21:45.890702 35003 sgd_solver.cpp:112] Iteration 141360, lr = 0.01
I0523 04:21:50.189438 35003 solver.cpp:239] Iteration 141370 (2.31284 iter/s, 4.32369s/10 iters), loss = 6.36549
I0523 04:21:50.189477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36549 (* 1 = 6.36549 loss)
I0523 04:21:50.888877 35003 sgd_solver.cpp:112] Iteration 141370, lr = 0.01
I0523 04:21:53.773149 35003 solver.cpp:239] Iteration 141380 (2.79055 iter/s, 3.58352s/10 iters), loss = 8.01484
I0523 04:21:53.773205 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01484 (* 1 = 8.01484 loss)
I0523 04:21:54.501128 35003 sgd_solver.cpp:112] Iteration 141380, lr = 0.01
I0523 04:21:56.548676 35003 solver.cpp:239] Iteration 141390 (3.60314 iter/s, 2.77536s/10 iters), loss = 6.7556
I0523 04:21:56.548712 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7556 (* 1 = 6.7556 loss)
I0523 04:21:56.561781 35003 sgd_solver.cpp:112] Iteration 141390, lr = 0.01
I0523 04:21:59.333593 35003 solver.cpp:239] Iteration 141400 (3.59098 iter/s, 2.78475s/10 iters), loss = 7.10886
I0523 04:21:59.333645 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10886 (* 1 = 7.10886 loss)
I0523 04:22:00.042567 35003 sgd_solver.cpp:112] Iteration 141400, lr = 0.01
I0523 04:22:03.732990 35003 solver.cpp:239] Iteration 141410 (2.27316 iter/s, 4.39917s/10 iters), loss = 6.56314
I0523 04:22:03.733031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56314 (* 1 = 6.56314 loss)
I0523 04:22:03.757720 35003 sgd_solver.cpp:112] Iteration 141410, lr = 0.01
I0523 04:22:07.377081 35003 solver.cpp:239] Iteration 141420 (2.74431 iter/s, 3.6439s/10 iters), loss = 7.31009
I0523 04:22:07.377348 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31009 (* 1 = 7.31009 loss)
I0523 04:22:07.974545 35003 sgd_solver.cpp:112] Iteration 141420, lr = 0.01
I0523 04:22:11.550578 35003 solver.cpp:239] Iteration 141430 (2.39632 iter/s, 4.17307s/10 iters), loss = 7.32068
I0523 04:22:11.550642 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32068 (* 1 = 7.32068 loss)
I0523 04:22:12.272796 35003 sgd_solver.cpp:112] Iteration 141430, lr = 0.01
I0523 04:22:16.560150 35003 solver.cpp:239] Iteration 141440 (1.99629 iter/s, 5.0093s/10 iters), loss = 7.78897
I0523 04:22:16.560212 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78897 (* 1 = 7.78897 loss)
I0523 04:22:17.301542 35003 sgd_solver.cpp:112] Iteration 141440, lr = 0.01
I0523 04:22:21.689599 35003 solver.cpp:239] Iteration 141450 (1.94963 iter/s, 5.12918s/10 iters), loss = 7.58114
I0523 04:22:21.689652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58114 (* 1 = 7.58114 loss)
I0523 04:22:22.283726 35003 sgd_solver.cpp:112] Iteration 141450, lr = 0.01
I0523 04:22:25.776268 35003 solver.cpp:239] Iteration 141460 (2.44712 iter/s, 4.08644s/10 iters), loss = 6.79191
I0523 04:22:25.776336 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79191 (* 1 = 6.79191 loss)
I0523 04:22:26.484763 35003 sgd_solver.cpp:112] Iteration 141460, lr = 0.01
I0523 04:22:28.520159 35003 solver.cpp:239] Iteration 141470 (3.64471 iter/s, 2.7437s/10 iters), loss = 6.54221
I0523 04:22:28.520200 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54221 (* 1 = 6.54221 loss)
I0523 04:22:28.533147 35003 sgd_solver.cpp:112] Iteration 141470, lr = 0.01
I0523 04:22:32.173315 35003 solver.cpp:239] Iteration 141480 (2.73751 iter/s, 3.65296s/10 iters), loss = 7.85863
I0523 04:22:32.173357 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85863 (* 1 = 7.85863 loss)
I0523 04:22:32.186503 35003 sgd_solver.cpp:112] Iteration 141480, lr = 0.01
I0523 04:22:35.123173 35003 solver.cpp:239] Iteration 141490 (3.3902 iter/s, 2.94968s/10 iters), loss = 6.93502
I0523 04:22:35.123224 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93502 (* 1 = 6.93502 loss)
I0523 04:22:35.864377 35003 sgd_solver.cpp:112] Iteration 141490, lr = 0.01
I0523 04:22:38.255482 35003 solver.cpp:239] Iteration 141500 (3.19272 iter/s, 3.13213s/10 iters), loss = 5.8082
I0523 04:22:38.255738 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8082 (* 1 = 5.8082 loss)
I0523 04:22:38.266842 35003 sgd_solver.cpp:112] Iteration 141500, lr = 0.01
I0523 04:22:40.346212 35003 solver.cpp:239] Iteration 141510 (4.79361 iter/s, 2.08611s/10 iters), loss = 7.35212
I0523 04:22:40.346254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35212 (* 1 = 7.35212 loss)
I0523 04:22:41.054882 35003 sgd_solver.cpp:112] Iteration 141510, lr = 0.01
I0523 04:22:44.915400 35003 solver.cpp:239] Iteration 141520 (2.18869 iter/s, 4.56895s/10 iters), loss = 7.06346
I0523 04:22:44.915452 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06346 (* 1 = 7.06346 loss)
I0523 04:22:45.538095 35003 sgd_solver.cpp:112] Iteration 141520, lr = 0.01
I0523 04:22:48.729274 35003 solver.cpp:239] Iteration 141530 (2.62215 iter/s, 3.81367s/10 iters), loss = 6.73276
I0523 04:22:48.729312 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73276 (* 1 = 6.73276 loss)
I0523 04:22:48.743041 35003 sgd_solver.cpp:112] Iteration 141530, lr = 0.01
I0523 04:22:52.344440 35003 solver.cpp:239] Iteration 141540 (2.76627 iter/s, 3.61497s/10 iters), loss = 5.81915
I0523 04:22:52.344482 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.81915 (* 1 = 5.81915 loss)
I0523 04:22:52.358171 35003 sgd_solver.cpp:112] Iteration 141540, lr = 0.01
I0523 04:22:54.620404 35003 solver.cpp:239] Iteration 141550 (4.39403 iter/s, 2.27582s/10 iters), loss = 5.49047
I0523 04:22:54.620458 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.49047 (* 1 = 5.49047 loss)
I0523 04:22:55.323113 35003 sgd_solver.cpp:112] Iteration 141550, lr = 0.01
I0523 04:22:58.168695 35003 solver.cpp:239] Iteration 141560 (2.81842 iter/s, 3.54809s/10 iters), loss = 6.74027
I0523 04:22:58.168735 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74027 (* 1 = 6.74027 loss)
I0523 04:22:58.175551 35003 sgd_solver.cpp:112] Iteration 141560, lr = 0.01
I0523 04:23:01.797899 35003 solver.cpp:239] Iteration 141570 (2.75558 iter/s, 3.629s/10 iters), loss = 6.90971
I0523 04:23:01.797938 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90971 (* 1 = 6.90971 loss)
I0523 04:23:01.962630 35003 sgd_solver.cpp:112] Iteration 141570, lr = 0.01
I0523 04:23:05.539270 35003 solver.cpp:239] Iteration 141580 (2.67295 iter/s, 3.74118s/10 iters), loss = 7.04383
I0523 04:23:05.539314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04383 (* 1 = 7.04383 loss)
I0523 04:23:05.549461 35003 sgd_solver.cpp:112] Iteration 141580, lr = 0.01
I0523 04:23:08.311015 35003 solver.cpp:239] Iteration 141590 (3.60807 iter/s, 2.77157s/10 iters), loss = 7.62947
I0523 04:23:08.311269 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62947 (* 1 = 7.62947 loss)
I0523 04:23:08.316154 35003 sgd_solver.cpp:112] Iteration 141590, lr = 0.01
I0523 04:23:12.547837 35003 solver.cpp:239] Iteration 141600 (2.36048 iter/s, 4.23642s/10 iters), loss = 6.95549
I0523 04:23:12.547878 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95549 (* 1 = 6.95549 loss)
I0523 04:23:13.282819 35003 sgd_solver.cpp:112] Iteration 141600, lr = 0.01
I0523 04:23:15.441779 35003 solver.cpp:239] Iteration 141610 (3.45569 iter/s, 2.89378s/10 iters), loss = 6.49844
I0523 04:23:15.441829 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49844 (* 1 = 6.49844 loss)
I0523 04:23:16.181282 35003 sgd_solver.cpp:112] Iteration 141610, lr = 0.01
I0523 04:23:19.620594 35003 solver.cpp:239] Iteration 141620 (2.39315 iter/s, 4.17859s/10 iters), loss = 7.75418
I0523 04:23:19.620646 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75418 (* 1 = 7.75418 loss)
I0523 04:23:20.355136 35003 sgd_solver.cpp:112] Iteration 141620, lr = 0.01
I0523 04:23:24.587873 35003 solver.cpp:239] Iteration 141630 (2.01328 iter/s, 4.96702s/10 iters), loss = 7.39103
I0523 04:23:24.587920 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39103 (* 1 = 7.39103 loss)
I0523 04:23:24.617926 35003 sgd_solver.cpp:112] Iteration 141630, lr = 0.01
I0523 04:23:27.429956 35003 solver.cpp:239] Iteration 141640 (3.51877 iter/s, 2.84191s/10 iters), loss = 7.97401
I0523 04:23:27.430006 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97401 (* 1 = 7.97401 loss)
I0523 04:23:27.493774 35003 sgd_solver.cpp:112] Iteration 141640, lr = 0.01
I0523 04:23:30.247166 35003 solver.cpp:239] Iteration 141650 (3.54983 iter/s, 2.81704s/10 iters), loss = 6.72705
I0523 04:23:30.247220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72705 (* 1 = 6.72705 loss)
I0523 04:23:30.265738 35003 sgd_solver.cpp:112] Iteration 141650, lr = 0.01
I0523 04:23:33.799768 35003 solver.cpp:239] Iteration 141660 (2.81499 iter/s, 3.5524s/10 iters), loss = 6.84074
I0523 04:23:33.799810 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84074 (* 1 = 6.84074 loss)
I0523 04:23:33.818086 35003 sgd_solver.cpp:112] Iteration 141660, lr = 0.01
I0523 04:23:35.347115 35003 solver.cpp:239] Iteration 141670 (6.46315 iter/s, 1.54723s/10 iters), loss = 6.17343
I0523 04:23:35.347157 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17343 (* 1 = 6.17343 loss)
I0523 04:23:36.060614 35003 sgd_solver.cpp:112] Iteration 141670, lr = 0.01
I0523 04:23:38.777621 35003 solver.cpp:239] Iteration 141680 (2.91886 iter/s, 3.426s/10 iters), loss = 7.04756
I0523 04:23:38.777810 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04756 (* 1 = 7.04756 loss)
I0523 04:23:38.788790 35003 sgd_solver.cpp:112] Iteration 141680, lr = 0.01
I0523 04:23:41.739612 35003 solver.cpp:239] Iteration 141690 (3.37647 iter/s, 2.96167s/10 iters), loss = 6.54531
I0523 04:23:41.739670 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54531 (* 1 = 6.54531 loss)
I0523 04:23:41.753391 35003 sgd_solver.cpp:112] Iteration 141690, lr = 0.01
I0523 04:23:45.314844 35003 solver.cpp:239] Iteration 141700 (2.79718 iter/s, 3.57502s/10 iters), loss = 6.95075
I0523 04:23:45.314898 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95075 (* 1 = 6.95075 loss)
I0523 04:23:45.328410 35003 sgd_solver.cpp:112] Iteration 141700, lr = 0.01
I0523 04:23:48.224648 35003 solver.cpp:239] Iteration 141710 (3.43686 iter/s, 2.90963s/10 iters), loss = 6.46898
I0523 04:23:48.224689 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46898 (* 1 = 6.46898 loss)
I0523 04:23:48.242483 35003 sgd_solver.cpp:112] Iteration 141710, lr = 0.01
I0523 04:23:51.162389 35003 solver.cpp:239] Iteration 141720 (3.40417 iter/s, 2.93757s/10 iters), loss = 7.41713
I0523 04:23:51.162433 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41713 (* 1 = 7.41713 loss)
I0523 04:23:51.885478 35003 sgd_solver.cpp:112] Iteration 141720, lr = 0.01
I0523 04:23:55.450148 35003 solver.cpp:239] Iteration 141730 (2.33234 iter/s, 4.28753s/10 iters), loss = 6.34486
I0523 04:23:55.450212 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34486 (* 1 = 6.34486 loss)
I0523 04:23:55.490622 35003 sgd_solver.cpp:112] Iteration 141730, lr = 0.01
I0523 04:23:59.029510 35003 solver.cpp:239] Iteration 141740 (2.79396 iter/s, 3.57915s/10 iters), loss = 6.30098
I0523 04:23:59.029563 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30098 (* 1 = 6.30098 loss)
I0523 04:23:59.186403 35003 sgd_solver.cpp:112] Iteration 141740, lr = 0.01
I0523 04:24:02.066682 35003 solver.cpp:239] Iteration 141750 (3.29274 iter/s, 3.03699s/10 iters), loss = 6.50956
I0523 04:24:02.066756 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50956 (* 1 = 6.50956 loss)
I0523 04:24:02.077507 35003 sgd_solver.cpp:112] Iteration 141750, lr = 0.01
I0523 04:24:04.922438 35003 solver.cpp:239] Iteration 141760 (3.50193 iter/s, 2.85557s/10 iters), loss = 6.9508
I0523 04:24:04.922483 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9508 (* 1 = 6.9508 loss)
I0523 04:24:05.656556 35003 sgd_solver.cpp:112] Iteration 141760, lr = 0.01
I0523 04:24:09.315845 35003 solver.cpp:239] Iteration 141770 (2.27625 iter/s, 4.39318s/10 iters), loss = 5.76978
I0523 04:24:09.316048 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.76978 (* 1 = 5.76978 loss)
I0523 04:24:09.323520 35003 sgd_solver.cpp:112] Iteration 141770, lr = 0.01
I0523 04:24:12.041874 35003 solver.cpp:239] Iteration 141780 (3.66877 iter/s, 2.72571s/10 iters), loss = 7.34082
I0523 04:24:12.041925 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34082 (* 1 = 7.34082 loss)
I0523 04:24:12.049501 35003 sgd_solver.cpp:112] Iteration 141780, lr = 0.01
I0523 04:24:16.389191 35003 solver.cpp:239] Iteration 141790 (2.30039 iter/s, 4.34709s/10 iters), loss = 7.96946
I0523 04:24:16.389237 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96946 (* 1 = 7.96946 loss)
I0523 04:24:17.104676 35003 sgd_solver.cpp:112] Iteration 141790, lr = 0.01
I0523 04:24:20.712970 35003 solver.cpp:239] Iteration 141800 (2.31291 iter/s, 4.32355s/10 iters), loss = 7.20631
I0523 04:24:20.713019 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20631 (* 1 = 7.20631 loss)
I0523 04:24:21.447834 35003 sgd_solver.cpp:112] Iteration 141800, lr = 0.01
I0523 04:24:26.283057 35003 solver.cpp:239] Iteration 141810 (1.79539 iter/s, 5.56981s/10 iters), loss = 6.20995
I0523 04:24:26.283119 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20995 (* 1 = 6.20995 loss)
I0523 04:24:26.292726 35003 sgd_solver.cpp:112] Iteration 141810, lr = 0.01
I0523 04:24:30.500790 35003 solver.cpp:239] Iteration 141820 (2.37107 iter/s, 4.2175s/10 iters), loss = 6.42916
I0523 04:24:30.500838 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42916 (* 1 = 6.42916 loss)
I0523 04:24:30.513556 35003 sgd_solver.cpp:112] Iteration 141820, lr = 0.01
I0523 04:24:32.590672 35003 solver.cpp:239] Iteration 141830 (4.78529 iter/s, 2.08974s/10 iters), loss = 8.71273
I0523 04:24:32.590741 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.71273 (* 1 = 8.71273 loss)
I0523 04:24:32.595751 35003 sgd_solver.cpp:112] Iteration 141830, lr = 0.01
I0523 04:24:36.128499 35003 solver.cpp:239] Iteration 141840 (2.82677 iter/s, 3.53761s/10 iters), loss = 7.05411
I0523 04:24:36.128538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05411 (* 1 = 7.05411 loss)
I0523 04:24:36.135213 35003 sgd_solver.cpp:112] Iteration 141840, lr = 0.01
I0523 04:24:39.639331 35003 solver.cpp:239] Iteration 141850 (2.84848 iter/s, 3.51064s/10 iters), loss = 7.07621
I0523 04:24:39.639539 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07621 (* 1 = 7.07621 loss)
I0523 04:24:39.652267 35003 sgd_solver.cpp:112] Iteration 141850, lr = 0.01
I0523 04:24:41.906170 35003 solver.cpp:239] Iteration 141860 (4.412 iter/s, 2.26655s/10 iters), loss = 6.92518
I0523 04:24:41.906244 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92518 (* 1 = 6.92518 loss)
I0523 04:24:41.919143 35003 sgd_solver.cpp:112] Iteration 141860, lr = 0.01
I0523 04:24:45.114464 35003 solver.cpp:239] Iteration 141870 (3.11712 iter/s, 3.20809s/10 iters), loss = 6.05032
I0523 04:24:45.114511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05032 (* 1 = 6.05032 loss)
I0523 04:24:45.132840 35003 sgd_solver.cpp:112] Iteration 141870, lr = 0.01
I0523 04:24:48.167806 35003 solver.cpp:239] Iteration 141880 (3.27529 iter/s, 3.05317s/10 iters), loss = 6.53123
I0523 04:24:48.167848 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53123 (* 1 = 6.53123 loss)
I0523 04:24:48.896098 35003 sgd_solver.cpp:112] Iteration 141880, lr = 0.01
I0523 04:24:53.256780 35003 solver.cpp:239] Iteration 141890 (1.96513 iter/s, 5.08872s/10 iters), loss = 6.47412
I0523 04:24:53.256819 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47412 (* 1 = 6.47412 loss)
I0523 04:24:53.275043 35003 sgd_solver.cpp:112] Iteration 141890, lr = 0.01
I0523 04:24:56.920790 35003 solver.cpp:239] Iteration 141900 (2.72939 iter/s, 3.66382s/10 iters), loss = 7.3479
I0523 04:24:56.920841 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3479 (* 1 = 7.3479 loss)
I0523 04:24:56.932404 35003 sgd_solver.cpp:112] Iteration 141900, lr = 0.01
I0523 04:25:00.471768 35003 solver.cpp:239] Iteration 141910 (2.81628 iter/s, 3.55078s/10 iters), loss = 7.84687
I0523 04:25:00.471812 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84687 (* 1 = 7.84687 loss)
I0523 04:25:01.209764 35003 sgd_solver.cpp:112] Iteration 141910, lr = 0.01
I0523 04:25:04.284144 35003 solver.cpp:239] Iteration 141920 (2.62318 iter/s, 3.81217s/10 iters), loss = 7.05346
I0523 04:25:04.284196 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05346 (* 1 = 7.05346 loss)
I0523 04:25:04.291555 35003 sgd_solver.cpp:112] Iteration 141920, lr = 0.01
I0523 04:25:07.978554 35003 solver.cpp:239] Iteration 141930 (2.70694 iter/s, 3.69421s/10 iters), loss = 7.70449
I0523 04:25:07.978600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70449 (* 1 = 7.70449 loss)
I0523 04:25:08.069067 35003 sgd_solver.cpp:112] Iteration 141930, lr = 0.01
I0523 04:25:12.472000 35003 solver.cpp:239] Iteration 141940 (2.22558 iter/s, 4.49322s/10 iters), loss = 7.87607
I0523 04:25:12.472189 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87607 (* 1 = 7.87607 loss)
I0523 04:25:13.157486 35003 sgd_solver.cpp:112] Iteration 141940, lr = 0.01
I0523 04:25:17.371397 35003 solver.cpp:239] Iteration 141950 (2.04123 iter/s, 4.89901s/10 iters), loss = 6.43176
I0523 04:25:17.371451 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43176 (* 1 = 6.43176 loss)
I0523 04:25:18.064694 35003 sgd_solver.cpp:112] Iteration 141950, lr = 0.01
I0523 04:25:20.828635 35003 solver.cpp:239] Iteration 141960 (2.89265 iter/s, 3.45704s/10 iters), loss = 7.83486
I0523 04:25:20.828672 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83486 (* 1 = 7.83486 loss)
I0523 04:25:20.841862 35003 sgd_solver.cpp:112] Iteration 141960, lr = 0.01
I0523 04:25:24.344030 35003 solver.cpp:239] Iteration 141970 (2.84478 iter/s, 3.51521s/10 iters), loss = 6.43891
I0523 04:25:24.344067 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43891 (* 1 = 6.43891 loss)
I0523 04:25:24.357386 35003 sgd_solver.cpp:112] Iteration 141970, lr = 0.01
I0523 04:25:28.588892 35003 solver.cpp:239] Iteration 141980 (2.35591 iter/s, 4.24464s/10 iters), loss = 6.88923
I0523 04:25:28.588946 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88923 (* 1 = 6.88923 loss)
I0523 04:25:28.601502 35003 sgd_solver.cpp:112] Iteration 141980, lr = 0.01
I0523 04:25:31.381227 35003 solver.cpp:239] Iteration 141990 (3.58145 iter/s, 2.79216s/10 iters), loss = 5.49982
I0523 04:25:31.381276 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.49982 (* 1 = 5.49982 loss)
I0523 04:25:31.391019 35003 sgd_solver.cpp:112] Iteration 141990, lr = 0.01
I0523 04:25:35.724284 35003 solver.cpp:239] Iteration 142000 (2.30265 iter/s, 4.34283s/10 iters), loss = 6.324
I0523 04:25:35.724329 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.324 (* 1 = 6.324 loss)
I0523 04:25:36.094786 35003 sgd_solver.cpp:112] Iteration 142000, lr = 0.01
I0523 04:25:39.676554 35003 solver.cpp:239] Iteration 142010 (2.53033 iter/s, 3.95205s/10 iters), loss = 5.89603
I0523 04:25:39.676600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89603 (* 1 = 5.89603 loss)
I0523 04:25:40.005405 35003 sgd_solver.cpp:112] Iteration 142010, lr = 0.01
I0523 04:25:43.498461 35003 solver.cpp:239] Iteration 142020 (2.61664 iter/s, 3.8217s/10 iters), loss = 7.82263
I0523 04:25:43.498673 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82263 (* 1 = 7.82263 loss)
I0523 04:25:43.521497 35003 sgd_solver.cpp:112] Iteration 142020, lr = 0.01
I0523 04:25:46.389816 35003 solver.cpp:239] Iteration 142030 (3.45899 iter/s, 2.89102s/10 iters), loss = 6.2204
I0523 04:25:46.389861 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2204 (* 1 = 6.2204 loss)
I0523 04:25:46.398767 35003 sgd_solver.cpp:112] Iteration 142030, lr = 0.01
I0523 04:25:50.563398 35003 solver.cpp:239] Iteration 142040 (2.39615 iter/s, 4.17336s/10 iters), loss = 7.48518
I0523 04:25:50.563439 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48518 (* 1 = 7.48518 loss)
I0523 04:25:50.571578 35003 sgd_solver.cpp:112] Iteration 142040, lr = 0.01
I0523 04:25:54.170424 35003 solver.cpp:239] Iteration 142050 (2.77252 iter/s, 3.60683s/10 iters), loss = 6.90481
I0523 04:25:54.170462 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90481 (* 1 = 6.90481 loss)
I0523 04:25:54.183658 35003 sgd_solver.cpp:112] Iteration 142050, lr = 0.01
I0523 04:25:58.410665 35003 solver.cpp:239] Iteration 142060 (2.35848 iter/s, 4.24003s/10 iters), loss = 8.63054
I0523 04:25:58.410739 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.63054 (* 1 = 8.63054 loss)
I0523 04:25:58.417917 35003 sgd_solver.cpp:112] Iteration 142060, lr = 0.01
I0523 04:26:01.408399 35003 solver.cpp:239] Iteration 142070 (3.33611 iter/s, 2.99751s/10 iters), loss = 7.71204
I0523 04:26:01.408449 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71204 (* 1 = 7.71204 loss)
I0523 04:26:02.123174 35003 sgd_solver.cpp:112] Iteration 142070, lr = 0.01
I0523 04:26:05.032577 35003 solver.cpp:239] Iteration 142080 (2.7594 iter/s, 3.62398s/10 iters), loss = 6.12145
I0523 04:26:05.032629 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12145 (* 1 = 6.12145 loss)
I0523 04:26:05.046319 35003 sgd_solver.cpp:112] Iteration 142080, lr = 0.01
I0523 04:26:09.093149 35003 solver.cpp:239] Iteration 142090 (2.46284 iter/s, 4.06035s/10 iters), loss = 5.67408
I0523 04:26:09.093200 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.67408 (* 1 = 5.67408 loss)
I0523 04:26:09.441694 35003 sgd_solver.cpp:112] Iteration 142090, lr = 0.01
I0523 04:26:12.990499 35003 solver.cpp:239] Iteration 142100 (2.56599 iter/s, 3.89714s/10 iters), loss = 6.7372
I0523 04:26:12.990540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7372 (* 1 = 6.7372 loss)
I0523 04:26:13.003427 35003 sgd_solver.cpp:112] Iteration 142100, lr = 0.01
I0523 04:26:15.775523 35003 solver.cpp:239] Iteration 142110 (3.59084 iter/s, 2.78486s/10 iters), loss = 8.28751
I0523 04:26:15.775620 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.28751 (* 1 = 8.28751 loss)
I0523 04:26:15.793805 35003 sgd_solver.cpp:112] Iteration 142110, lr = 0.01
I0523 04:26:18.667440 35003 solver.cpp:239] Iteration 142120 (3.45819 iter/s, 2.89169s/10 iters), loss = 6.48988
I0523 04:26:18.667484 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48988 (* 1 = 6.48988 loss)
I0523 04:26:18.942162 35003 sgd_solver.cpp:112] Iteration 142120, lr = 0.01
I0523 04:26:21.809267 35003 solver.cpp:239] Iteration 142130 (3.18305 iter/s, 3.14165s/10 iters), loss = 6.13772
I0523 04:26:21.809322 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13772 (* 1 = 6.13772 loss)
I0523 04:26:22.550402 35003 sgd_solver.cpp:112] Iteration 142130, lr = 0.01
I0523 04:26:25.933428 35003 solver.cpp:239] Iteration 142140 (2.42487 iter/s, 4.12394s/10 iters), loss = 6.34682
I0523 04:26:25.933478 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34682 (* 1 = 6.34682 loss)
I0523 04:26:25.941313 35003 sgd_solver.cpp:112] Iteration 142140, lr = 0.01
I0523 04:26:28.784034 35003 solver.cpp:239] Iteration 142150 (3.50824 iter/s, 2.85043s/10 iters), loss = 7.55117
I0523 04:26:28.784085 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55117 (* 1 = 7.55117 loss)
I0523 04:26:29.499217 35003 sgd_solver.cpp:112] Iteration 142150, lr = 0.01
I0523 04:26:33.002157 35003 solver.cpp:239] Iteration 142160 (2.37085 iter/s, 4.2179s/10 iters), loss = 7.28511
I0523 04:26:33.002197 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28511 (* 1 = 7.28511 loss)
I0523 04:26:33.013017 35003 sgd_solver.cpp:112] Iteration 142160, lr = 0.01
I0523 04:26:37.019112 35003 solver.cpp:239] Iteration 142170 (2.48959 iter/s, 4.01673s/10 iters), loss = 6.54068
I0523 04:26:37.019170 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54068 (* 1 = 6.54068 loss)
I0523 04:26:37.025920 35003 sgd_solver.cpp:112] Iteration 142170, lr = 0.01
I0523 04:26:41.620712 35003 solver.cpp:239] Iteration 142180 (2.17328 iter/s, 4.60135s/10 iters), loss = 8.93533
I0523 04:26:41.620751 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.93533 (* 1 = 8.93533 loss)
I0523 04:26:41.634176 35003 sgd_solver.cpp:112] Iteration 142180, lr = 0.01
I0523 04:26:45.107738 35003 solver.cpp:239] Iteration 142190 (2.86792 iter/s, 3.48684s/10 iters), loss = 8.01375
I0523 04:26:45.107779 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01375 (* 1 = 8.01375 loss)
I0523 04:26:45.116185 35003 sgd_solver.cpp:112] Iteration 142190, lr = 0.01
I0523 04:26:47.853482 35003 solver.cpp:239] Iteration 142200 (3.64222 iter/s, 2.74558s/10 iters), loss = 7.06774
I0523 04:26:47.853718 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06774 (* 1 = 7.06774 loss)
I0523 04:26:47.867204 35003 sgd_solver.cpp:112] Iteration 142200, lr = 0.01
I0523 04:26:51.560812 35003 solver.cpp:239] Iteration 142210 (2.69763 iter/s, 3.70696s/10 iters), loss = 6.95751
I0523 04:26:51.560863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95751 (* 1 = 6.95751 loss)
I0523 04:26:51.568480 35003 sgd_solver.cpp:112] Iteration 142210, lr = 0.01
I0523 04:26:52.872459 35003 solver.cpp:239] Iteration 142220 (7.62464 iter/s, 1.31154s/10 iters), loss = 6.23247
I0523 04:26:52.872503 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23247 (* 1 = 6.23247 loss)
I0523 04:26:52.877517 35003 sgd_solver.cpp:112] Iteration 142220, lr = 0.01
I0523 04:26:55.618882 35003 solver.cpp:239] Iteration 142230 (3.64132 iter/s, 2.74626s/10 iters), loss = 6.58403
I0523 04:26:55.618927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58403 (* 1 = 6.58403 loss)
I0523 04:26:55.628568 35003 sgd_solver.cpp:112] Iteration 142230, lr = 0.01
I0523 04:26:59.145068 35003 solver.cpp:239] Iteration 142240 (2.83608 iter/s, 3.526s/10 iters), loss = 7.0113
I0523 04:26:59.145110 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0113 (* 1 = 7.0113 loss)
I0523 04:26:59.150236 35003 sgd_solver.cpp:112] Iteration 142240, lr = 0.01
I0523 04:27:01.998663 35003 solver.cpp:239] Iteration 142250 (3.50456 iter/s, 2.85342s/10 iters), loss = 7.07404
I0523 04:27:01.998729 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07404 (* 1 = 7.07404 loss)
I0523 04:27:02.020051 35003 sgd_solver.cpp:112] Iteration 142250, lr = 0.01
I0523 04:27:05.608858 35003 solver.cpp:239] Iteration 142260 (2.7701 iter/s, 3.60998s/10 iters), loss = 6.59592
I0523 04:27:05.608908 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59592 (* 1 = 6.59592 loss)
I0523 04:27:05.628386 35003 sgd_solver.cpp:112] Iteration 142260, lr = 0.01
I0523 04:27:09.226572 35003 solver.cpp:239] Iteration 142270 (2.76433 iter/s, 3.61751s/10 iters), loss = 7.25806
I0523 04:27:09.226629 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25806 (* 1 = 7.25806 loss)
I0523 04:27:09.955106 35003 sgd_solver.cpp:112] Iteration 142270, lr = 0.01
I0523 04:27:13.602589 35003 solver.cpp:239] Iteration 142280 (2.28531 iter/s, 4.37578s/10 iters), loss = 6.98147
I0523 04:27:13.602650 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98147 (* 1 = 6.98147 loss)
I0523 04:27:13.603884 35003 sgd_solver.cpp:112] Iteration 142280, lr = 0.01
I0523 04:27:17.802880 35003 solver.cpp:239] Iteration 142290 (2.38092 iter/s, 4.20006s/10 iters), loss = 8.07819
I0523 04:27:17.802917 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.07819 (* 1 = 8.07819 loss)
I0523 04:27:17.821471 35003 sgd_solver.cpp:112] Iteration 142290, lr = 0.01
I0523 04:27:19.164144 35003 solver.cpp:239] Iteration 142300 (7.34667 iter/s, 1.36116s/10 iters), loss = 7.39665
I0523 04:27:19.164363 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39665 (* 1 = 7.39665 loss)
I0523 04:27:19.905750 35003 sgd_solver.cpp:112] Iteration 142300, lr = 0.01
I0523 04:27:24.841418 35003 solver.cpp:239] Iteration 142310 (1.76155 iter/s, 5.67683s/10 iters), loss = 7.21674
I0523 04:27:24.841470 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21674 (* 1 = 7.21674 loss)
I0523 04:27:25.186547 35003 sgd_solver.cpp:112] Iteration 142310, lr = 0.01
I0523 04:27:29.376672 35003 solver.cpp:239] Iteration 142320 (2.20506 iter/s, 4.53502s/10 iters), loss = 7.24905
I0523 04:27:29.376709 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24905 (* 1 = 7.24905 loss)
I0523 04:27:29.389670 35003 sgd_solver.cpp:112] Iteration 142320, lr = 0.01
I0523 04:27:32.280259 35003 solver.cpp:239] Iteration 142330 (3.44421 iter/s, 2.90342s/10 iters), loss = 6.39956
I0523 04:27:32.280306 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39956 (* 1 = 6.39956 loss)
I0523 04:27:32.298851 35003 sgd_solver.cpp:112] Iteration 142330, lr = 0.01
I0523 04:27:34.825352 35003 solver.cpp:239] Iteration 142340 (3.92938 iter/s, 2.54493s/10 iters), loss = 7.60281
I0523 04:27:34.825392 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60281 (* 1 = 7.60281 loss)
I0523 04:27:34.829324 35003 sgd_solver.cpp:112] Iteration 142340, lr = 0.01
I0523 04:27:38.410848 35003 solver.cpp:239] Iteration 142350 (2.78916 iter/s, 3.58531s/10 iters), loss = 8.08148
I0523 04:27:38.410895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08148 (* 1 = 8.08148 loss)
I0523 04:27:38.423522 35003 sgd_solver.cpp:112] Iteration 142350, lr = 0.01
I0523 04:27:42.155907 35003 solver.cpp:239] Iteration 142360 (2.67033 iter/s, 3.74485s/10 iters), loss = 6.97964
I0523 04:27:42.155958 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97964 (* 1 = 6.97964 loss)
I0523 04:27:42.181488 35003 sgd_solver.cpp:112] Iteration 142360, lr = 0.01
I0523 04:27:44.990651 35003 solver.cpp:239] Iteration 142370 (3.52787 iter/s, 2.83457s/10 iters), loss = 7.6246
I0523 04:27:44.990733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6246 (* 1 = 7.6246 loss)
I0523 04:27:44.996417 35003 sgd_solver.cpp:112] Iteration 142370, lr = 0.01
I0523 04:27:49.116874 35003 solver.cpp:239] Iteration 142380 (2.42367 iter/s, 4.12597s/10 iters), loss = 6.94515
I0523 04:27:49.116922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94515 (* 1 = 6.94515 loss)
I0523 04:27:49.815410 35003 sgd_solver.cpp:112] Iteration 142380, lr = 0.01
I0523 04:27:51.713810 35003 solver.cpp:239] Iteration 142390 (3.85094 iter/s, 2.59677s/10 iters), loss = 7.31354
I0523 04:27:51.713861 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31354 (* 1 = 7.31354 loss)
I0523 04:27:51.718974 35003 sgd_solver.cpp:112] Iteration 142390, lr = 0.01
I0523 04:27:55.263082 35003 solver.cpp:239] Iteration 142400 (2.81764 iter/s, 3.54907s/10 iters), loss = 6.81375
I0523 04:27:55.263134 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81375 (* 1 = 6.81375 loss)
I0523 04:27:55.276227 35003 sgd_solver.cpp:112] Iteration 142400, lr = 0.01
I0523 04:27:58.691668 35003 solver.cpp:239] Iteration 142410 (2.91682 iter/s, 3.42839s/10 iters), loss = 7.2788
I0523 04:27:58.691709 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2788 (* 1 = 7.2788 loss)
I0523 04:27:58.696668 35003 sgd_solver.cpp:112] Iteration 142410, lr = 0.01
I0523 04:28:01.894312 35003 solver.cpp:239] Iteration 142420 (3.1226 iter/s, 3.20246s/10 iters), loss = 5.66355
I0523 04:28:01.894357 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.66355 (* 1 = 5.66355 loss)
I0523 04:28:01.900862 35003 sgd_solver.cpp:112] Iteration 142420, lr = 0.01
I0523 04:28:04.798537 35003 solver.cpp:239] Iteration 142430 (3.44346 iter/s, 2.90406s/10 iters), loss = 6.04197
I0523 04:28:04.798579 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04197 (* 1 = 6.04197 loss)
I0523 04:28:04.812165 35003 sgd_solver.cpp:112] Iteration 142430, lr = 0.01
I0523 04:28:07.483827 35003 solver.cpp:239] Iteration 142440 (3.72421 iter/s, 2.68513s/10 iters), loss = 6.14504
I0523 04:28:07.483867 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14504 (* 1 = 6.14504 loss)
I0523 04:28:07.489339 35003 sgd_solver.cpp:112] Iteration 142440, lr = 0.01
I0523 04:28:10.674206 35003 solver.cpp:239] Iteration 142450 (3.13461 iter/s, 3.19019s/10 iters), loss = 7.9989
I0523 04:28:10.674268 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9989 (* 1 = 7.9989 loss)
I0523 04:28:10.680299 35003 sgd_solver.cpp:112] Iteration 142450, lr = 0.01
I0523 04:28:13.358235 35003 solver.cpp:239] Iteration 142460 (3.72598 iter/s, 2.68386s/10 iters), loss = 6.17115
I0523 04:28:13.358278 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17115 (* 1 = 6.17115 loss)
I0523 04:28:13.852403 35003 sgd_solver.cpp:112] Iteration 142460, lr = 0.01
I0523 04:28:16.326731 35003 solver.cpp:239] Iteration 142470 (3.36892 iter/s, 2.96831s/10 iters), loss = 7.05106
I0523 04:28:16.326792 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05106 (* 1 = 7.05106 loss)
I0523 04:28:17.055238 35003 sgd_solver.cpp:112] Iteration 142470, lr = 0.01
I0523 04:28:19.009239 35003 solver.cpp:239] Iteration 142480 (3.7281 iter/s, 2.68233s/10 iters), loss = 7.12847
I0523 04:28:19.009282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12847 (* 1 = 7.12847 loss)
I0523 04:28:19.022905 35003 sgd_solver.cpp:112] Iteration 142480, lr = 0.01
I0523 04:28:20.976469 35003 solver.cpp:239] Iteration 142490 (5.08362 iter/s, 1.9671s/10 iters), loss = 7.37589
I0523 04:28:20.976558 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37589 (* 1 = 7.37589 loss)
I0523 04:28:20.990361 35003 sgd_solver.cpp:112] Iteration 142490, lr = 0.01
I0523 04:28:24.339357 35003 solver.cpp:239] Iteration 142500 (2.97384 iter/s, 3.36266s/10 iters), loss = 7.06719
I0523 04:28:24.339398 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06719 (* 1 = 7.06719 loss)
I0523 04:28:24.345079 35003 sgd_solver.cpp:112] Iteration 142500, lr = 0.01
I0523 04:28:27.094661 35003 solver.cpp:239] Iteration 142510 (3.62957 iter/s, 2.75514s/10 iters), loss = 7.43926
I0523 04:28:27.094717 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43926 (* 1 = 7.43926 loss)
I0523 04:28:27.118825 35003 sgd_solver.cpp:112] Iteration 142510, lr = 0.01
I0523 04:28:29.181921 35003 solver.cpp:239] Iteration 142520 (4.79131 iter/s, 2.08711s/10 iters), loss = 6.60255
I0523 04:28:29.181973 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60255 (* 1 = 6.60255 loss)
I0523 04:28:29.188248 35003 sgd_solver.cpp:112] Iteration 142520, lr = 0.01
I0523 04:28:33.072247 35003 solver.cpp:239] Iteration 142530 (2.57063 iter/s, 3.8901s/10 iters), loss = 7.01893
I0523 04:28:33.072288 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01893 (* 1 = 7.01893 loss)
I0523 04:28:33.085566 35003 sgd_solver.cpp:112] Iteration 142530, lr = 0.01
I0523 04:28:35.629899 35003 solver.cpp:239] Iteration 142540 (3.91007 iter/s, 2.5575s/10 iters), loss = 6.91965
I0523 04:28:35.629951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91965 (* 1 = 6.91965 loss)
I0523 04:28:35.642211 35003 sgd_solver.cpp:112] Iteration 142540, lr = 0.01
I0523 04:28:38.981636 35003 solver.cpp:239] Iteration 142550 (2.98371 iter/s, 3.35154s/10 iters), loss = 7.21792
I0523 04:28:38.981678 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21792 (* 1 = 7.21792 loss)
I0523 04:28:38.993026 35003 sgd_solver.cpp:112] Iteration 142550, lr = 0.01
I0523 04:28:42.833135 35003 solver.cpp:239] Iteration 142560 (2.59653 iter/s, 3.8513s/10 iters), loss = 5.52631
I0523 04:28:42.833189 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.52631 (* 1 = 5.52631 loss)
I0523 04:28:42.840847 35003 sgd_solver.cpp:112] Iteration 142560, lr = 0.01
I0523 04:28:45.721076 35003 solver.cpp:239] Iteration 142570 (3.46289 iter/s, 2.88776s/10 iters), loss = 6.74698
I0523 04:28:45.721119 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74698 (* 1 = 6.74698 loss)
I0523 04:28:46.423482 35003 sgd_solver.cpp:112] Iteration 142570, lr = 0.01
I0523 04:28:51.002338 35003 solver.cpp:239] Iteration 142580 (1.89358 iter/s, 5.28099s/10 iters), loss = 6.19236
I0523 04:28:51.002595 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19236 (* 1 = 6.19236 loss)
I0523 04:28:51.518196 35003 sgd_solver.cpp:112] Iteration 142580, lr = 0.01
I0523 04:28:55.927114 35003 solver.cpp:239] Iteration 142590 (2.03073 iter/s, 4.92434s/10 iters), loss = 6.79702
I0523 04:28:55.927155 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79702 (* 1 = 6.79702 loss)
I0523 04:28:55.929168 35003 sgd_solver.cpp:112] Iteration 142590, lr = 0.01
I0523 04:28:59.510007 35003 solver.cpp:239] Iteration 142600 (2.79119 iter/s, 3.5827s/10 iters), loss = 6.97664
I0523 04:28:59.510052 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97664 (* 1 = 6.97664 loss)
I0523 04:29:00.140142 35003 sgd_solver.cpp:112] Iteration 142600, lr = 0.01
I0523 04:29:03.039228 35003 solver.cpp:239] Iteration 142610 (2.83365 iter/s, 3.52902s/10 iters), loss = 6.64024
I0523 04:29:03.039286 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64024 (* 1 = 6.64024 loss)
I0523 04:29:03.046705 35003 sgd_solver.cpp:112] Iteration 142610, lr = 0.01
I0523 04:29:05.841850 35003 solver.cpp:239] Iteration 142620 (3.56831 iter/s, 2.80245s/10 iters), loss = 7.74129
I0523 04:29:05.841900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74129 (* 1 = 7.74129 loss)
I0523 04:29:05.851163 35003 sgd_solver.cpp:112] Iteration 142620, lr = 0.01
I0523 04:29:09.784466 35003 solver.cpp:239] Iteration 142630 (2.53652 iter/s, 3.9424s/10 iters), loss = 7.63984
I0523 04:29:09.784510 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63984 (* 1 = 7.63984 loss)
I0523 04:29:10.525598 35003 sgd_solver.cpp:112] Iteration 142630, lr = 0.01
I0523 04:29:12.604022 35003 solver.cpp:239] Iteration 142640 (3.54686 iter/s, 2.81939s/10 iters), loss = 7.12839
I0523 04:29:12.604068 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12839 (* 1 = 7.12839 loss)
I0523 04:29:12.617585 35003 sgd_solver.cpp:112] Iteration 142640, lr = 0.01
I0523 04:29:15.319177 35003 solver.cpp:239] Iteration 142650 (3.68325 iter/s, 2.71499s/10 iters), loss = 6.77074
I0523 04:29:15.319223 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77074 (* 1 = 6.77074 loss)
I0523 04:29:15.345902 35003 sgd_solver.cpp:112] Iteration 142650, lr = 0.01
I0523 04:29:19.446631 35003 solver.cpp:239] Iteration 142660 (2.42293 iter/s, 4.12724s/10 iters), loss = 7.14508
I0523 04:29:19.446671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14508 (* 1 = 7.14508 loss)
I0523 04:29:19.453456 35003 sgd_solver.cpp:112] Iteration 142660, lr = 0.01
I0523 04:29:22.411317 35003 solver.cpp:239] Iteration 142670 (3.37323 iter/s, 2.96452s/10 iters), loss = 6.48992
I0523 04:29:22.411543 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48992 (* 1 = 6.48992 loss)
I0523 04:29:22.418459 35003 sgd_solver.cpp:112] Iteration 142670, lr = 0.01
I0523 04:29:25.356349 35003 solver.cpp:239] Iteration 142680 (3.39592 iter/s, 2.94471s/10 iters), loss = 6.15291
I0523 04:29:25.356405 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15291 (* 1 = 6.15291 loss)
I0523 04:29:25.377719 35003 sgd_solver.cpp:112] Iteration 142680, lr = 0.01
I0523 04:29:29.838786 35003 solver.cpp:239] Iteration 142690 (2.23327 iter/s, 4.47774s/10 iters), loss = 6.50109
I0523 04:29:29.838834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50109 (* 1 = 6.50109 loss)
I0523 04:29:30.571553 35003 sgd_solver.cpp:112] Iteration 142690, lr = 0.01
I0523 04:29:33.383384 35003 solver.cpp:239] Iteration 142700 (2.82135 iter/s, 3.5444s/10 iters), loss = 5.6606
I0523 04:29:33.383438 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.6606 (* 1 = 5.6606 loss)
I0523 04:29:34.091564 35003 sgd_solver.cpp:112] Iteration 142700, lr = 0.01
I0523 04:29:36.947481 35003 solver.cpp:239] Iteration 142710 (2.80592 iter/s, 3.56389s/10 iters), loss = 6.35066
I0523 04:29:36.947536 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35066 (* 1 = 6.35066 loss)
I0523 04:29:36.953402 35003 sgd_solver.cpp:112] Iteration 142710, lr = 0.01
I0523 04:29:39.013483 35003 solver.cpp:239] Iteration 142720 (4.84062 iter/s, 2.06585s/10 iters), loss = 7.21849
I0523 04:29:39.013525 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21849 (* 1 = 7.21849 loss)
I0523 04:29:39.019397 35003 sgd_solver.cpp:112] Iteration 142720, lr = 0.01
I0523 04:29:42.542407 35003 solver.cpp:239] Iteration 142730 (2.83388 iter/s, 3.52873s/10 iters), loss = 7.48438
I0523 04:29:42.542450 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48438 (* 1 = 7.48438 loss)
I0523 04:29:42.546175 35003 sgd_solver.cpp:112] Iteration 142730, lr = 0.01
I0523 04:29:46.126513 35003 solver.cpp:239] Iteration 142740 (2.79026 iter/s, 3.58389s/10 iters), loss = 7.28987
I0523 04:29:46.126555 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28987 (* 1 = 7.28987 loss)
I0523 04:29:46.131650 35003 sgd_solver.cpp:112] Iteration 142740, lr = 0.01
I0523 04:29:47.814580 35003 solver.cpp:239] Iteration 142750 (5.92437 iter/s, 1.68794s/10 iters), loss = 7.22803
I0523 04:29:47.814621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22803 (* 1 = 7.22803 loss)
I0523 04:29:47.828061 35003 sgd_solver.cpp:112] Iteration 142750, lr = 0.01
I0523 04:29:51.363176 35003 solver.cpp:239] Iteration 142760 (2.81817 iter/s, 3.5484s/10 iters), loss = 6.69505
I0523 04:29:51.363216 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69505 (* 1 = 6.69505 loss)
I0523 04:29:51.373114 35003 sgd_solver.cpp:112] Iteration 142760, lr = 0.01
I0523 04:29:53.438196 35003 solver.cpp:239] Iteration 142770 (4.81954 iter/s, 2.07489s/10 iters), loss = 7.67115
I0523 04:29:53.438346 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67115 (* 1 = 7.67115 loss)
I0523 04:29:53.443500 35003 sgd_solver.cpp:112] Iteration 142770, lr = 0.01
I0523 04:29:56.241794 35003 solver.cpp:239] Iteration 142780 (3.56718 iter/s, 2.80333s/10 iters), loss = 6.68843
I0523 04:29:56.241835 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68843 (* 1 = 6.68843 loss)
I0523 04:29:56.257978 35003 sgd_solver.cpp:112] Iteration 142780, lr = 0.01
I0523 04:29:58.969722 35003 solver.cpp:239] Iteration 142790 (3.666 iter/s, 2.72777s/10 iters), loss = 6.43718
I0523 04:29:58.969765 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43718 (* 1 = 6.43718 loss)
I0523 04:29:59.601430 35003 sgd_solver.cpp:112] Iteration 142790, lr = 0.01
I0523 04:30:02.391405 35003 solver.cpp:239] Iteration 142800 (2.9227 iter/s, 3.42149s/10 iters), loss = 5.57066
I0523 04:30:02.391444 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.57066 (* 1 = 5.57066 loss)
I0523 04:30:02.404659 35003 sgd_solver.cpp:112] Iteration 142800, lr = 0.01
I0523 04:30:05.324513 35003 solver.cpp:239] Iteration 142810 (3.40955 iter/s, 2.93294s/10 iters), loss = 6.41706
I0523 04:30:05.324563 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41706 (* 1 = 6.41706 loss)
I0523 04:30:05.332705 35003 sgd_solver.cpp:112] Iteration 142810, lr = 0.01
I0523 04:30:08.146435 35003 solver.cpp:239] Iteration 142820 (3.54389 iter/s, 2.82175s/10 iters), loss = 6.38282
I0523 04:30:08.146477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38282 (* 1 = 6.38282 loss)
I0523 04:30:08.881429 35003 sgd_solver.cpp:112] Iteration 142820, lr = 0.01
I0523 04:30:12.353829 35003 solver.cpp:239] Iteration 142830 (2.37689 iter/s, 4.20717s/10 iters), loss = 6.50705
I0523 04:30:12.353883 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50705 (* 1 = 6.50705 loss)
I0523 04:30:12.361049 35003 sgd_solver.cpp:112] Iteration 142830, lr = 0.01
I0523 04:30:16.542081 35003 solver.cpp:239] Iteration 142840 (2.38776 iter/s, 4.18803s/10 iters), loss = 7.23428
I0523 04:30:16.542120 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23428 (* 1 = 7.23428 loss)
I0523 04:30:17.276569 35003 sgd_solver.cpp:112] Iteration 142840, lr = 0.01
I0523 04:30:20.902220 35003 solver.cpp:239] Iteration 142850 (2.29362 iter/s, 4.35991s/10 iters), loss = 7.33393
I0523 04:30:20.902274 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33393 (* 1 = 7.33393 loss)
I0523 04:30:20.915607 35003 sgd_solver.cpp:112] Iteration 142850, lr = 0.01
I0523 04:30:23.682880 35003 solver.cpp:239] Iteration 142860 (3.59649 iter/s, 2.78049s/10 iters), loss = 7.87738
I0523 04:30:23.683189 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87738 (* 1 = 7.87738 loss)
I0523 04:30:23.700992 35003 sgd_solver.cpp:112] Iteration 142860, lr = 0.01
I0523 04:30:27.363651 35003 solver.cpp:239] Iteration 142870 (2.71714 iter/s, 3.68033s/10 iters), loss = 5.83582
I0523 04:30:27.363687 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.83582 (* 1 = 5.83582 loss)
I0523 04:30:27.373775 35003 sgd_solver.cpp:112] Iteration 142870, lr = 0.01
I0523 04:30:30.503267 35003 solver.cpp:239] Iteration 142880 (3.18528 iter/s, 3.13944s/10 iters), loss = 8.00915
I0523 04:30:30.503309 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00915 (* 1 = 8.00915 loss)
I0523 04:30:30.515622 35003 sgd_solver.cpp:112] Iteration 142880, lr = 0.01
I0523 04:30:33.336908 35003 solver.cpp:239] Iteration 142890 (3.52923 iter/s, 2.83348s/10 iters), loss = 6.78896
I0523 04:30:33.336946 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78896 (* 1 = 6.78896 loss)
I0523 04:30:33.346262 35003 sgd_solver.cpp:112] Iteration 142890, lr = 0.01
I0523 04:30:38.504003 35003 solver.cpp:239] Iteration 142900 (1.93542 iter/s, 5.16683s/10 iters), loss = 6.99475
I0523 04:30:38.504077 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99475 (* 1 = 6.99475 loss)
I0523 04:30:38.522899 35003 sgd_solver.cpp:112] Iteration 142900, lr = 0.01
I0523 04:30:41.377065 35003 solver.cpp:239] Iteration 142910 (3.48083 iter/s, 2.87287s/10 iters), loss = 6.9438
I0523 04:30:41.377106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9438 (* 1 = 6.9438 loss)
I0523 04:30:41.391752 35003 sgd_solver.cpp:112] Iteration 142910, lr = 0.01
I0523 04:30:44.201009 35003 solver.cpp:239] Iteration 142920 (3.54135 iter/s, 2.82378s/10 iters), loss = 7.29628
I0523 04:30:44.201056 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29628 (* 1 = 7.29628 loss)
I0523 04:30:44.915872 35003 sgd_solver.cpp:112] Iteration 142920, lr = 0.01
I0523 04:30:48.406713 35003 solver.cpp:239] Iteration 142930 (2.37786 iter/s, 4.20547s/10 iters), loss = 6.45893
I0523 04:30:48.406761 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45893 (* 1 = 6.45893 loss)
I0523 04:30:48.431267 35003 sgd_solver.cpp:112] Iteration 142930, lr = 0.01
I0523 04:30:52.442381 35003 solver.cpp:239] Iteration 142940 (2.47804 iter/s, 4.03545s/10 iters), loss = 6.41686
I0523 04:30:52.442445 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41686 (* 1 = 6.41686 loss)
I0523 04:30:52.466400 35003 sgd_solver.cpp:112] Iteration 142940, lr = 0.01
I0523 04:30:56.078258 35003 solver.cpp:239] Iteration 142950 (2.75053 iter/s, 3.63567s/10 iters), loss = 7.16605
I0523 04:30:56.078409 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16605 (* 1 = 7.16605 loss)
I0523 04:30:56.087261 35003 sgd_solver.cpp:112] Iteration 142950, lr = 0.01
I0523 04:30:59.008038 35003 solver.cpp:239] Iteration 142960 (3.41355 iter/s, 2.9295s/10 iters), loss = 6.71083
I0523 04:30:59.008080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71083 (* 1 = 6.71083 loss)
I0523 04:30:59.033299 35003 sgd_solver.cpp:112] Iteration 142960, lr = 0.01
I0523 04:31:02.554628 35003 solver.cpp:239] Iteration 142970 (2.81977 iter/s, 3.54638s/10 iters), loss = 6.84491
I0523 04:31:02.554677 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84491 (* 1 = 6.84491 loss)
I0523 04:31:03.249704 35003 sgd_solver.cpp:112] Iteration 142970, lr = 0.01
I0523 04:31:06.174072 35003 solver.cpp:239] Iteration 142980 (2.76301 iter/s, 3.61924s/10 iters), loss = 7.68924
I0523 04:31:06.174125 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68924 (* 1 = 7.68924 loss)
I0523 04:31:06.177986 35003 sgd_solver.cpp:112] Iteration 142980, lr = 0.01
I0523 04:31:11.249558 35003 solver.cpp:239] Iteration 142990 (1.97035 iter/s, 5.07523s/10 iters), loss = 6.82001
I0523 04:31:11.249598 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82001 (* 1 = 6.82001 loss)
I0523 04:31:11.263056 35003 sgd_solver.cpp:112] Iteration 142990, lr = 0.01
I0523 04:31:14.667232 35003 solver.cpp:239] Iteration 143000 (2.92612 iter/s, 3.41749s/10 iters), loss = 7.12539
I0523 04:31:14.667279 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12539 (* 1 = 7.12539 loss)
I0523 04:31:14.675612 35003 sgd_solver.cpp:112] Iteration 143000, lr = 0.01
I0523 04:31:17.465572 35003 solver.cpp:239] Iteration 143010 (3.57376 iter/s, 2.79818s/10 iters), loss = 8.71632
I0523 04:31:17.465616 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.71632 (* 1 = 8.71632 loss)
I0523 04:31:18.174727 35003 sgd_solver.cpp:112] Iteration 143010, lr = 0.01
I0523 04:31:20.830941 35003 solver.cpp:239] Iteration 143020 (2.97161 iter/s, 3.36518s/10 iters), loss = 7.29727
I0523 04:31:20.830981 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29727 (* 1 = 7.29727 loss)
I0523 04:31:20.843273 35003 sgd_solver.cpp:112] Iteration 143020, lr = 0.01
I0523 04:31:22.147107 35003 solver.cpp:239] Iteration 143030 (7.59848 iter/s, 1.31605s/10 iters), loss = 6.92625
I0523 04:31:22.147173 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92625 (* 1 = 6.92625 loss)
I0523 04:31:22.165388 35003 sgd_solver.cpp:112] Iteration 143030, lr = 0.01
I0523 04:31:25.002758 35003 solver.cpp:239] Iteration 143040 (3.50205 iter/s, 2.85547s/10 iters), loss = 8.59555
I0523 04:31:25.002792 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.59555 (* 1 = 8.59555 loss)
I0523 04:31:25.015862 35003 sgd_solver.cpp:112] Iteration 143040, lr = 0.01
I0523 04:31:28.634246 35003 solver.cpp:239] Iteration 143050 (2.75384 iter/s, 3.63129s/10 iters), loss = 7.44531
I0523 04:31:28.634450 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44531 (* 1 = 7.44531 loss)
I0523 04:31:29.355195 35003 sgd_solver.cpp:112] Iteration 143050, lr = 0.01
I0523 04:31:32.892231 35003 solver.cpp:239] Iteration 143060 (2.34874 iter/s, 4.25761s/10 iters), loss = 7.96728
I0523 04:31:32.892287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96728 (* 1 = 7.96728 loss)
I0523 04:31:32.921809 35003 sgd_solver.cpp:112] Iteration 143060, lr = 0.01
I0523 04:31:36.345016 35003 solver.cpp:239] Iteration 143070 (2.89638 iter/s, 3.45259s/10 iters), loss = 8.17464
I0523 04:31:36.345067 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17464 (* 1 = 8.17464 loss)
I0523 04:31:36.358302 35003 sgd_solver.cpp:112] Iteration 143070, lr = 0.01
I0523 04:31:39.120995 35003 solver.cpp:239] Iteration 143080 (3.60255 iter/s, 2.77581s/10 iters), loss = 6.09434
I0523 04:31:39.121047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09434 (* 1 = 6.09434 loss)
I0523 04:31:39.180984 35003 sgd_solver.cpp:112] Iteration 143080, lr = 0.01
I0523 04:31:42.171648 35003 solver.cpp:239] Iteration 143090 (3.27818 iter/s, 3.05048s/10 iters), loss = 7.13124
I0523 04:31:42.171687 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13124 (* 1 = 7.13124 loss)
I0523 04:31:42.185528 35003 sgd_solver.cpp:112] Iteration 143090, lr = 0.01
I0523 04:31:44.246510 35003 solver.cpp:239] Iteration 143100 (4.8199 iter/s, 2.07473s/10 iters), loss = 6.03154
I0523 04:31:44.246546 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03154 (* 1 = 6.03154 loss)
I0523 04:31:44.252629 35003 sgd_solver.cpp:112] Iteration 143100, lr = 0.01
I0523 04:31:47.076341 35003 solver.cpp:239] Iteration 143110 (3.53399 iter/s, 2.82967s/10 iters), loss = 8.00495
I0523 04:31:47.076385 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00495 (* 1 = 8.00495 loss)
I0523 04:31:47.089143 35003 sgd_solver.cpp:112] Iteration 143110, lr = 0.01
I0523 04:31:49.717365 35003 solver.cpp:239] Iteration 143120 (3.78664 iter/s, 2.64087s/10 iters), loss = 6.34062
I0523 04:31:49.717406 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34062 (* 1 = 6.34062 loss)
I0523 04:31:49.724303 35003 sgd_solver.cpp:112] Iteration 143120, lr = 0.01
I0523 04:31:52.375685 35003 solver.cpp:239] Iteration 143130 (3.76199 iter/s, 2.65817s/10 iters), loss = 5.68749
I0523 04:31:52.375725 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.68749 (* 1 = 5.68749 loss)
I0523 04:31:52.389132 35003 sgd_solver.cpp:112] Iteration 143130, lr = 0.01
I0523 04:31:56.275918 35003 solver.cpp:239] Iteration 143140 (2.56409 iter/s, 3.90002s/10 iters), loss = 6.7499
I0523 04:31:56.275990 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7499 (* 1 = 6.7499 loss)
I0523 04:31:56.293947 35003 sgd_solver.cpp:112] Iteration 143140, lr = 0.01
I0523 04:31:58.051617 35003 solver.cpp:239] Iteration 143150 (5.63203 iter/s, 1.77556s/10 iters), loss = 6.24966
I0523 04:31:58.051659 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24966 (* 1 = 6.24966 loss)
I0523 04:31:58.060320 35003 sgd_solver.cpp:112] Iteration 143150, lr = 0.01
I0523 04:32:01.473124 35003 solver.cpp:239] Iteration 143160 (2.92285 iter/s, 3.42132s/10 iters), loss = 6.68821
I0523 04:32:01.473321 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68821 (* 1 = 6.68821 loss)
I0523 04:32:01.481200 35003 sgd_solver.cpp:112] Iteration 143160, lr = 0.01
I0523 04:32:05.497437 35003 solver.cpp:239] Iteration 143170 (2.48512 iter/s, 4.02395s/10 iters), loss = 6.70434
I0523 04:32:05.497476 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70434 (* 1 = 6.70434 loss)
I0523 04:32:06.206506 35003 sgd_solver.cpp:112] Iteration 143170, lr = 0.01
I0523 04:32:09.824333 35003 solver.cpp:239] Iteration 143180 (2.31124 iter/s, 4.32668s/10 iters), loss = 6.66944
I0523 04:32:09.824375 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66944 (* 1 = 6.66944 loss)
I0523 04:32:09.838287 35003 sgd_solver.cpp:112] Iteration 143180, lr = 0.01
I0523 04:32:13.660984 35003 solver.cpp:239] Iteration 143190 (2.60658 iter/s, 3.83645s/10 iters), loss = 6.26905
I0523 04:32:13.661028 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26905 (* 1 = 6.26905 loss)
I0523 04:32:14.402565 35003 sgd_solver.cpp:112] Iteration 143190, lr = 0.01
I0523 04:32:17.279444 35003 solver.cpp:239] Iteration 143200 (2.76376 iter/s, 3.61826s/10 iters), loss = 7.55574
I0523 04:32:17.279480 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55574 (* 1 = 7.55574 loss)
I0523 04:32:17.297329 35003 sgd_solver.cpp:112] Iteration 143200, lr = 0.01
I0523 04:32:18.611382 35003 solver.cpp:239] Iteration 143210 (7.50844 iter/s, 1.33183s/10 iters), loss = 5.99765
I0523 04:32:18.611423 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99765 (* 1 = 5.99765 loss)
I0523 04:32:18.630811 35003 sgd_solver.cpp:112] Iteration 143210, lr = 0.01
I0523 04:32:22.579754 35003 solver.cpp:239] Iteration 143220 (2.52006 iter/s, 3.96817s/10 iters), loss = 6.40426
I0523 04:32:22.579797 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40426 (* 1 = 6.40426 loss)
I0523 04:32:22.586207 35003 sgd_solver.cpp:112] Iteration 143220, lr = 0.01
I0523 04:32:27.731530 35003 solver.cpp:239] Iteration 143230 (1.94118 iter/s, 5.15152s/10 iters), loss = 7.69512
I0523 04:32:27.731573 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69512 (* 1 = 7.69512 loss)
I0523 04:32:28.420964 35003 sgd_solver.cpp:112] Iteration 143230, lr = 0.01
I0523 04:32:31.922227 35003 solver.cpp:239] Iteration 143240 (2.38636 iter/s, 4.19048s/10 iters), loss = 6.13275
I0523 04:32:31.922530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13275 (* 1 = 6.13275 loss)
I0523 04:32:31.935209 35003 sgd_solver.cpp:112] Iteration 143240, lr = 0.01
I0523 04:32:36.276453 35003 solver.cpp:239] Iteration 143250 (2.29686 iter/s, 4.35377s/10 iters), loss = 8.30516
I0523 04:32:36.276495 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.30516 (* 1 = 8.30516 loss)
I0523 04:32:36.297492 35003 sgd_solver.cpp:112] Iteration 143250, lr = 0.01
I0523 04:32:38.987696 35003 solver.cpp:239] Iteration 143260 (3.68856 iter/s, 2.71109s/10 iters), loss = 6.61036
I0523 04:32:38.987733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61036 (* 1 = 6.61036 loss)
I0523 04:32:39.053546 35003 sgd_solver.cpp:112] Iteration 143260, lr = 0.01
I0523 04:32:41.886162 35003 solver.cpp:239] Iteration 143270 (3.45031 iter/s, 2.89829s/10 iters), loss = 7.75842
I0523 04:32:41.886219 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75842 (* 1 = 7.75842 loss)
I0523 04:32:41.888916 35003 sgd_solver.cpp:112] Iteration 143270, lr = 0.01
I0523 04:32:45.457514 35003 solver.cpp:239] Iteration 143280 (2.80022 iter/s, 3.57115s/10 iters), loss = 6.32381
I0523 04:32:45.457557 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32381 (* 1 = 6.32381 loss)
I0523 04:32:46.069299 35003 sgd_solver.cpp:112] Iteration 143280, lr = 0.01
I0523 04:32:49.666182 35003 solver.cpp:239] Iteration 143290 (2.37617 iter/s, 4.20845s/10 iters), loss = 6.55027
I0523 04:32:49.666221 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55027 (* 1 = 6.55027 loss)
I0523 04:32:50.368743 35003 sgd_solver.cpp:112] Iteration 143290, lr = 0.01
I0523 04:32:54.122709 35003 solver.cpp:239] Iteration 143300 (2.24402 iter/s, 4.45629s/10 iters), loss = 6.15014
I0523 04:32:54.122759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15014 (* 1 = 6.15014 loss)
I0523 04:32:54.135659 35003 sgd_solver.cpp:112] Iteration 143300, lr = 0.01
I0523 04:32:56.898545 35003 solver.cpp:239] Iteration 143310 (3.60274 iter/s, 2.77566s/10 iters), loss = 7.31904
I0523 04:32:56.898608 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31904 (* 1 = 7.31904 loss)
I0523 04:32:57.606827 35003 sgd_solver.cpp:112] Iteration 143310, lr = 0.01
I0523 04:33:00.464159 35003 solver.cpp:239] Iteration 143320 (2.80473 iter/s, 3.5654s/10 iters), loss = 6.68406
I0523 04:33:00.464193 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68406 (* 1 = 6.68406 loss)
I0523 04:33:00.482220 35003 sgd_solver.cpp:112] Iteration 143320, lr = 0.01
I0523 04:33:03.276379 35003 solver.cpp:239] Iteration 143330 (3.55611 iter/s, 2.81206s/10 iters), loss = 6.22947
I0523 04:33:03.276592 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22947 (* 1 = 6.22947 loss)
I0523 04:33:03.289849 35003 sgd_solver.cpp:112] Iteration 143330, lr = 0.01
I0523 04:33:07.630611 35003 solver.cpp:239] Iteration 143340 (2.29681 iter/s, 4.35387s/10 iters), loss = 7.10412
I0523 04:33:07.630652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10412 (* 1 = 7.10412 loss)
I0523 04:33:07.634371 35003 sgd_solver.cpp:112] Iteration 143340, lr = 0.01
I0523 04:33:11.943230 35003 solver.cpp:239] Iteration 143350 (2.3189 iter/s, 4.31239s/10 iters), loss = 7.6193
I0523 04:33:11.943280 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6193 (* 1 = 7.6193 loss)
I0523 04:33:11.945544 35003 sgd_solver.cpp:112] Iteration 143350, lr = 0.01
I0523 04:33:15.249884 35003 solver.cpp:239] Iteration 143360 (3.02438 iter/s, 3.30647s/10 iters), loss = 7.24134
I0523 04:33:15.249929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24134 (* 1 = 7.24134 loss)
I0523 04:33:15.393695 35003 sgd_solver.cpp:112] Iteration 143360, lr = 0.01
I0523 04:33:17.726850 35003 solver.cpp:239] Iteration 143370 (4.03744 iter/s, 2.47682s/10 iters), loss = 7.29486
I0523 04:33:17.726897 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29486 (* 1 = 7.29486 loss)
I0523 04:33:17.738190 35003 sgd_solver.cpp:112] Iteration 143370, lr = 0.01
I0523 04:33:21.116371 35003 solver.cpp:239] Iteration 143380 (2.95043 iter/s, 3.38933s/10 iters), loss = 6.43203
I0523 04:33:21.116413 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43203 (* 1 = 6.43203 loss)
I0523 04:33:21.304636 35003 sgd_solver.cpp:112] Iteration 143380, lr = 0.01
I0523 04:33:25.204002 35003 solver.cpp:239] Iteration 143390 (2.44653 iter/s, 4.08742s/10 iters), loss = 6.88068
I0523 04:33:25.204049 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88068 (* 1 = 6.88068 loss)
I0523 04:33:25.226218 35003 sgd_solver.cpp:112] Iteration 143390, lr = 0.01
I0523 04:33:26.636807 35003 solver.cpp:239] Iteration 143400 (6.97994 iter/s, 1.43268s/10 iters), loss = 6.9932
I0523 04:33:26.636857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9932 (* 1 = 6.9932 loss)
I0523 04:33:27.216163 35003 sgd_solver.cpp:112] Iteration 143400, lr = 0.01
I0523 04:33:31.045125 35003 solver.cpp:239] Iteration 143410 (2.26856 iter/s, 4.40808s/10 iters), loss = 7.48093
I0523 04:33:31.045168 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48093 (* 1 = 7.48093 loss)
I0523 04:33:31.248219 35003 sgd_solver.cpp:112] Iteration 143410, lr = 0.01
I0523 04:33:34.825577 35003 solver.cpp:239] Iteration 143420 (2.64533 iter/s, 3.78025s/10 iters), loss = 6.13434
I0523 04:33:34.825902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13434 (* 1 = 6.13434 loss)
I0523 04:33:35.534466 35003 sgd_solver.cpp:112] Iteration 143420, lr = 0.01
I0523 04:33:40.651314 35003 solver.cpp:239] Iteration 143430 (1.71669 iter/s, 5.82516s/10 iters), loss = 7.41329
I0523 04:33:40.651365 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41329 (* 1 = 7.41329 loss)
I0523 04:33:41.389061 35003 sgd_solver.cpp:112] Iteration 143430, lr = 0.01
I0523 04:33:43.377393 35003 solver.cpp:239] Iteration 143440 (3.6685 iter/s, 2.72591s/10 iters), loss = 7.71846
I0523 04:33:43.377439 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71846 (* 1 = 7.71846 loss)
I0523 04:33:43.391110 35003 sgd_solver.cpp:112] Iteration 143440, lr = 0.01
I0523 04:33:46.194686 35003 solver.cpp:239] Iteration 143450 (3.54972 iter/s, 2.81713s/10 iters), loss = 7.06631
I0523 04:33:46.194741 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06631 (* 1 = 7.06631 loss)
I0523 04:33:46.221004 35003 sgd_solver.cpp:112] Iteration 143450, lr = 0.01
I0523 04:33:49.196393 35003 solver.cpp:239] Iteration 143460 (3.33164 iter/s, 3.00153s/10 iters), loss = 5.97032
I0523 04:33:49.196441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97032 (* 1 = 5.97032 loss)
I0523 04:33:49.224673 35003 sgd_solver.cpp:112] Iteration 143460, lr = 0.01
I0523 04:33:52.731174 35003 solver.cpp:239] Iteration 143470 (2.82919 iter/s, 3.53458s/10 iters), loss = 6.60714
I0523 04:33:52.731220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60714 (* 1 = 6.60714 loss)
I0523 04:33:52.741120 35003 sgd_solver.cpp:112] Iteration 143470, lr = 0.01
I0523 04:33:54.110931 35003 solver.cpp:239] Iteration 143480 (7.24831 iter/s, 1.37963s/10 iters), loss = 6.91241
I0523 04:33:54.110985 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91241 (* 1 = 6.91241 loss)
I0523 04:33:54.118854 35003 sgd_solver.cpp:112] Iteration 143480, lr = 0.01
I0523 04:33:56.227795 35003 solver.cpp:239] Iteration 143490 (4.7243 iter/s, 2.11672s/10 iters), loss = 7.1971
I0523 04:33:56.227849 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1971 (* 1 = 7.1971 loss)
I0523 04:33:56.962244 35003 sgd_solver.cpp:112] Iteration 143490, lr = 0.01
I0523 04:34:00.524767 35003 solver.cpp:239] Iteration 143500 (2.32735 iter/s, 4.29674s/10 iters), loss = 7.42747
I0523 04:34:00.524803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42747 (* 1 = 7.42747 loss)
I0523 04:34:00.537710 35003 sgd_solver.cpp:112] Iteration 143500, lr = 0.01
I0523 04:34:04.401406 35003 solver.cpp:239] Iteration 143510 (2.57969 iter/s, 3.87644s/10 iters), loss = 6.2571
I0523 04:34:04.401453 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2571 (* 1 = 6.2571 loss)
I0523 04:34:05.139237 35003 sgd_solver.cpp:112] Iteration 143510, lr = 0.01
I0523 04:34:08.014677 35003 solver.cpp:239] Iteration 143520 (2.76773 iter/s, 3.61307s/10 iters), loss = 6.73869
I0523 04:34:08.014748 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73869 (* 1 = 6.73869 loss)
I0523 04:34:08.222123 35003 sgd_solver.cpp:112] Iteration 143520, lr = 0.01
I0523 04:34:10.472138 35003 solver.cpp:239] Iteration 143530 (4.06953 iter/s, 2.45729s/10 iters), loss = 8.5662
I0523 04:34:10.472185 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.5662 (* 1 = 8.5662 loss)
I0523 04:34:11.029259 35003 sgd_solver.cpp:112] Iteration 143530, lr = 0.01
I0523 04:34:13.842414 35003 solver.cpp:239] Iteration 143540 (2.96728 iter/s, 3.37009s/10 iters), loss = 6.45943
I0523 04:34:13.842459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45943 (* 1 = 6.45943 loss)
I0523 04:34:13.853955 35003 sgd_solver.cpp:112] Iteration 143540, lr = 0.01
I0523 04:34:16.668635 35003 solver.cpp:239] Iteration 143550 (3.53851 iter/s, 2.82605s/10 iters), loss = 7.35629
I0523 04:34:16.668686 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35629 (* 1 = 7.35629 loss)
I0523 04:34:16.724637 35003 sgd_solver.cpp:112] Iteration 143550, lr = 0.01
I0523 04:34:19.509526 35003 solver.cpp:239] Iteration 143560 (3.52023 iter/s, 2.84072s/10 iters), loss = 6.75404
I0523 04:34:19.509565 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75404 (* 1 = 6.75404 loss)
I0523 04:34:19.531440 35003 sgd_solver.cpp:112] Iteration 143560, lr = 0.01
I0523 04:34:22.374420 35003 solver.cpp:239] Iteration 143570 (3.49073 iter/s, 2.86473s/10 iters), loss = 6.37619
I0523 04:34:22.374470 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37619 (* 1 = 6.37619 loss)
I0523 04:34:23.109357 35003 sgd_solver.cpp:112] Iteration 143570, lr = 0.01
I0523 04:34:27.249068 35003 solver.cpp:239] Iteration 143580 (2.05153 iter/s, 4.8744s/10 iters), loss = 6.87961
I0523 04:34:27.249111 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87961 (* 1 = 6.87961 loss)
I0523 04:34:27.255590 35003 sgd_solver.cpp:112] Iteration 143580, lr = 0.01
I0523 04:34:31.309415 35003 solver.cpp:239] Iteration 143590 (2.46297 iter/s, 4.06014s/10 iters), loss = 7.448
I0523 04:34:31.309454 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.448 (* 1 = 7.448 loss)
I0523 04:34:31.322161 35003 sgd_solver.cpp:112] Iteration 143590, lr = 0.01
I0523 04:34:33.608947 35003 solver.cpp:239] Iteration 143600 (4.34899 iter/s, 2.29938s/10 iters), loss = 7.75618
I0523 04:34:33.608996 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75618 (* 1 = 7.75618 loss)
I0523 04:34:34.343966 35003 sgd_solver.cpp:112] Iteration 143600, lr = 0.01
I0523 04:34:37.562791 35003 solver.cpp:239] Iteration 143610 (2.52932 iter/s, 3.95363s/10 iters), loss = 6.65826
I0523 04:34:37.563019 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65826 (* 1 = 6.65826 loss)
I0523 04:34:37.572475 35003 sgd_solver.cpp:112] Iteration 143610, lr = 0.01
I0523 04:34:40.636975 35003 solver.cpp:239] Iteration 143620 (3.25325 iter/s, 3.07385s/10 iters), loss = 5.83266
I0523 04:34:40.637017 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.83266 (* 1 = 5.83266 loss)
I0523 04:34:41.332828 35003 sgd_solver.cpp:112] Iteration 143620, lr = 0.01
I0523 04:34:42.854888 35003 solver.cpp:239] Iteration 143630 (4.50903 iter/s, 2.21777s/10 iters), loss = 7.03095
I0523 04:34:42.854938 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03095 (* 1 = 7.03095 loss)
I0523 04:34:42.868257 35003 sgd_solver.cpp:112] Iteration 143630, lr = 0.01
I0523 04:34:45.926739 35003 solver.cpp:239] Iteration 143640 (3.25558 iter/s, 3.07165s/10 iters), loss = 6.96273
I0523 04:34:45.926786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96273 (* 1 = 6.96273 loss)
I0523 04:34:46.555806 35003 sgd_solver.cpp:112] Iteration 143640, lr = 0.01
I0523 04:34:49.340548 35003 solver.cpp:239] Iteration 143650 (2.92944 iter/s, 3.41362s/10 iters), loss = 6.68396
I0523 04:34:49.340598 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68396 (* 1 = 6.68396 loss)
I0523 04:34:49.358978 35003 sgd_solver.cpp:112] Iteration 143650, lr = 0.01
I0523 04:34:52.189558 35003 solver.cpp:239] Iteration 143660 (3.5102 iter/s, 2.84884s/10 iters), loss = 5.81581
I0523 04:34:52.189604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.81581 (* 1 = 5.81581 loss)
I0523 04:34:52.203173 35003 sgd_solver.cpp:112] Iteration 143660, lr = 0.01
I0523 04:34:55.608299 35003 solver.cpp:239] Iteration 143670 (2.92521 iter/s, 3.41856s/10 iters), loss = 6.84257
I0523 04:34:55.608340 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84257 (* 1 = 6.84257 loss)
I0523 04:34:55.624158 35003 sgd_solver.cpp:112] Iteration 143670, lr = 0.01
I0523 04:34:58.489266 35003 solver.cpp:239] Iteration 143680 (3.47126 iter/s, 2.8808s/10 iters), loss = 8.22938
I0523 04:34:58.489327 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.22938 (* 1 = 8.22938 loss)
I0523 04:34:59.224086 35003 sgd_solver.cpp:112] Iteration 143680, lr = 0.01
I0523 04:35:02.290040 35003 solver.cpp:239] Iteration 143690 (2.6312 iter/s, 3.80055s/10 iters), loss = 8.17223
I0523 04:35:02.290109 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17223 (* 1 = 8.17223 loss)
I0523 04:35:02.958072 35003 sgd_solver.cpp:112] Iteration 143690, lr = 0.01
I0523 04:35:06.169291 35003 solver.cpp:239] Iteration 143700 (2.57797 iter/s, 3.87902s/10 iters), loss = 6.94227
I0523 04:35:06.169347 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94227 (* 1 = 6.94227 loss)
I0523 04:35:06.178439 35003 sgd_solver.cpp:112] Iteration 143700, lr = 0.01
I0523 04:35:06.991590 35003 solver.cpp:239] Iteration 143710 (12.1626 iter/s, 0.822194s/10 iters), loss = 6.86283
I0523 04:35:06.991642 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86283 (* 1 = 6.86283 loss)
I0523 04:35:07.368255 35003 sgd_solver.cpp:112] Iteration 143710, lr = 0.01
I0523 04:35:12.075537 35003 solver.cpp:239] Iteration 143720 (1.96708 iter/s, 5.08369s/10 iters), loss = 7.05977
I0523 04:35:12.075822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05977 (* 1 = 7.05977 loss)
I0523 04:35:12.088701 35003 sgd_solver.cpp:112] Iteration 143720, lr = 0.01
I0523 04:35:16.395866 35003 solver.cpp:239] Iteration 143730 (2.31488 iter/s, 4.31988s/10 iters), loss = 6.92264
I0523 04:35:16.395903 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92264 (* 1 = 6.92264 loss)
I0523 04:35:16.409006 35003 sgd_solver.cpp:112] Iteration 143730, lr = 0.01
I0523 04:35:20.882169 35003 solver.cpp:239] Iteration 143740 (2.22912 iter/s, 4.48608s/10 iters), loss = 7.80127
I0523 04:35:20.882215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80127 (* 1 = 7.80127 loss)
I0523 04:35:20.895164 35003 sgd_solver.cpp:112] Iteration 143740, lr = 0.01
I0523 04:35:24.737085 35003 solver.cpp:239] Iteration 143750 (2.59423 iter/s, 3.85471s/10 iters), loss = 5.75804
I0523 04:35:24.737151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.75804 (* 1 = 5.75804 loss)
I0523 04:35:25.451930 35003 sgd_solver.cpp:112] Iteration 143750, lr = 0.01
I0523 04:35:27.542853 35003 solver.cpp:239] Iteration 143760 (3.56431 iter/s, 2.80559s/10 iters), loss = 6.00892
I0523 04:35:27.542896 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00892 (* 1 = 6.00892 loss)
I0523 04:35:27.556424 35003 sgd_solver.cpp:112] Iteration 143760, lr = 0.01
I0523 04:35:31.859946 35003 solver.cpp:239] Iteration 143770 (2.31649 iter/s, 4.31687s/10 iters), loss = 7.67156
I0523 04:35:31.859993 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67156 (* 1 = 7.67156 loss)
I0523 04:35:32.476583 35003 sgd_solver.cpp:112] Iteration 143770, lr = 0.01
I0523 04:35:35.165354 35003 solver.cpp:239] Iteration 143780 (3.02552 iter/s, 3.30522s/10 iters), loss = 6.18383
I0523 04:35:35.165405 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18383 (* 1 = 6.18383 loss)
I0523 04:35:35.838686 35003 sgd_solver.cpp:112] Iteration 143780, lr = 0.01
I0523 04:35:37.901065 35003 solver.cpp:239] Iteration 143790 (3.65558 iter/s, 2.73554s/10 iters), loss = 7.27477
I0523 04:35:37.901108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27477 (* 1 = 7.27477 loss)
I0523 04:35:37.922950 35003 sgd_solver.cpp:112] Iteration 143790, lr = 0.01
I0523 04:35:41.582100 35003 solver.cpp:239] Iteration 143800 (2.71677 iter/s, 3.68084s/10 iters), loss = 8.21064
I0523 04:35:41.582149 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.21064 (* 1 = 8.21064 loss)
I0523 04:35:42.316529 35003 sgd_solver.cpp:112] Iteration 143800, lr = 0.01
I0523 04:35:44.808518 35003 solver.cpp:239] Iteration 143810 (3.09959 iter/s, 3.22623s/10 iters), loss = 6.25382
I0523 04:35:44.808564 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25382 (* 1 = 6.25382 loss)
I0523 04:35:44.815971 35003 sgd_solver.cpp:112] Iteration 143810, lr = 0.01
I0523 04:35:47.763284 35003 solver.cpp:239] Iteration 143820 (3.38455 iter/s, 2.9546s/10 iters), loss = 6.60877
I0523 04:35:47.763322 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60877 (* 1 = 6.60877 loss)
I0523 04:35:47.774935 35003 sgd_solver.cpp:112] Iteration 143820, lr = 0.01
I0523 04:35:51.161064 35003 solver.cpp:239] Iteration 143830 (2.94326 iter/s, 3.3976s/10 iters), loss = 6.64318
I0523 04:35:51.161106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64318 (* 1 = 6.64318 loss)
I0523 04:35:51.185851 35003 sgd_solver.cpp:112] Iteration 143830, lr = 0.01
I0523 04:35:53.997172 35003 solver.cpp:239] Iteration 143840 (3.53121 iter/s, 2.83189s/10 iters), loss = 7.44752
I0523 04:35:53.997220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44752 (* 1 = 7.44752 loss)
I0523 04:35:54.019649 35003 sgd_solver.cpp:112] Iteration 143840, lr = 0.01
I0523 04:35:57.782021 35003 solver.cpp:239] Iteration 143850 (2.64226 iter/s, 3.78463s/10 iters), loss = 7.09225
I0523 04:35:57.782078 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09225 (* 1 = 7.09225 loss)
I0523 04:35:58.509980 35003 sgd_solver.cpp:112] Iteration 143850, lr = 0.01
I0523 04:35:59.847813 35003 solver.cpp:239] Iteration 143860 (4.84115 iter/s, 2.06563s/10 iters), loss = 6.50912
I0523 04:35:59.847869 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50912 (* 1 = 6.50912 loss)
I0523 04:36:00.567240 35003 sgd_solver.cpp:112] Iteration 143860, lr = 0.01
I0523 04:36:02.607326 35003 solver.cpp:239] Iteration 143870 (3.62407 iter/s, 2.75933s/10 iters), loss = 7.1594
I0523 04:36:02.607368 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1594 (* 1 = 7.1594 loss)
I0523 04:36:02.618738 35003 sgd_solver.cpp:112] Iteration 143870, lr = 0.01
I0523 04:36:06.267395 35003 solver.cpp:239] Iteration 143880 (2.73233 iter/s, 3.65988s/10 iters), loss = 6.37689
I0523 04:36:06.267436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37689 (* 1 = 6.37689 loss)
I0523 04:36:06.644493 35003 sgd_solver.cpp:112] Iteration 143880, lr = 0.01
I0523 04:36:09.475913 35003 solver.cpp:239] Iteration 143890 (3.11688 iter/s, 3.20834s/10 iters), loss = 6.84063
I0523 04:36:09.475955 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84063 (* 1 = 6.84063 loss)
I0523 04:36:09.487891 35003 sgd_solver.cpp:112] Iteration 143890, lr = 0.01
I0523 04:36:13.125481 35003 solver.cpp:239] Iteration 143900 (2.7402 iter/s, 3.64936s/10 iters), loss = 6.53422
I0523 04:36:13.125701 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53422 (* 1 = 6.53422 loss)
I0523 04:36:13.133576 35003 sgd_solver.cpp:112] Iteration 143900, lr = 0.01
I0523 04:36:16.183143 35003 solver.cpp:239] Iteration 143910 (3.27084 iter/s, 3.05732s/10 iters), loss = 6.98715
I0523 04:36:16.183182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98715 (* 1 = 6.98715 loss)
I0523 04:36:16.193296 35003 sgd_solver.cpp:112] Iteration 143910, lr = 0.01
I0523 04:36:20.586207 35003 solver.cpp:239] Iteration 143920 (2.27126 iter/s, 4.40284s/10 iters), loss = 7.47116
I0523 04:36:20.586268 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47116 (* 1 = 7.47116 loss)
I0523 04:36:20.598728 35003 sgd_solver.cpp:112] Iteration 143920, lr = 0.01
I0523 04:36:24.265031 35003 solver.cpp:239] Iteration 143930 (2.71842 iter/s, 3.67861s/10 iters), loss = 7.80713
I0523 04:36:24.265087 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80713 (* 1 = 7.80713 loss)
I0523 04:36:24.979895 35003 sgd_solver.cpp:112] Iteration 143930, lr = 0.01
I0523 04:36:27.119920 35003 solver.cpp:239] Iteration 143940 (3.50298 iter/s, 2.85471s/10 iters), loss = 8.01058
I0523 04:36:27.119972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01058 (* 1 = 8.01058 loss)
I0523 04:36:27.841264 35003 sgd_solver.cpp:112] Iteration 143940, lr = 0.01
I0523 04:36:30.743635 35003 solver.cpp:239] Iteration 143950 (2.75976 iter/s, 3.62351s/10 iters), loss = 6.9433
I0523 04:36:30.743680 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9433 (* 1 = 6.9433 loss)
I0523 04:36:31.339532 35003 sgd_solver.cpp:112] Iteration 143950, lr = 0.01
I0523 04:36:35.739166 35003 solver.cpp:239] Iteration 143960 (2.00189 iter/s, 4.99528s/10 iters), loss = 5.74309
I0523 04:36:35.739215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74309 (* 1 = 5.74309 loss)
I0523 04:36:36.429154 35003 sgd_solver.cpp:112] Iteration 143960, lr = 0.01
I0523 04:36:41.107972 35003 solver.cpp:239] Iteration 143970 (1.86271 iter/s, 5.36854s/10 iters), loss = 6.58636
I0523 04:36:41.108029 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58636 (* 1 = 6.58636 loss)
I0523 04:36:41.111444 35003 sgd_solver.cpp:112] Iteration 143970, lr = 0.01
I0523 04:36:46.221982 35003 solver.cpp:239] Iteration 143980 (1.95552 iter/s, 5.11374s/10 iters), loss = 7.02145
I0523 04:36:46.222174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02145 (* 1 = 7.02145 loss)
I0523 04:36:46.240582 35003 sgd_solver.cpp:112] Iteration 143980, lr = 0.01
I0523 04:36:49.087934 35003 solver.cpp:239] Iteration 143990 (3.48962 iter/s, 2.86564s/10 iters), loss = 6.40157
I0523 04:36:49.087972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40157 (* 1 = 6.40157 loss)
I0523 04:36:49.783354 35003 sgd_solver.cpp:112] Iteration 143990, lr = 0.01
I0523 04:36:52.970481 35003 solver.cpp:239] Iteration 144000 (2.57576 iter/s, 3.88235s/10 iters), loss = 6.27909
I0523 04:36:52.970525 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27909 (* 1 = 6.27909 loss)
I0523 04:36:53.711541 35003 sgd_solver.cpp:112] Iteration 144000, lr = 0.01
I0523 04:36:57.722586 35003 solver.cpp:239] Iteration 144010 (2.10444 iter/s, 4.75187s/10 iters), loss = 7.09913
I0523 04:36:57.722626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09913 (* 1 = 7.09913 loss)
I0523 04:36:57.740424 35003 sgd_solver.cpp:112] Iteration 144010, lr = 0.01
I0523 04:37:00.499920 35003 solver.cpp:239] Iteration 144020 (3.60079 iter/s, 2.77717s/10 iters), loss = 5.59499
I0523 04:37:00.499969 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.59499 (* 1 = 5.59499 loss)
I0523 04:37:00.508121 35003 sgd_solver.cpp:112] Iteration 144020, lr = 0.01
I0523 04:37:04.405110 35003 solver.cpp:239] Iteration 144030 (2.56083 iter/s, 3.90498s/10 iters), loss = 7.52116
I0523 04:37:04.405148 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52116 (* 1 = 7.52116 loss)
I0523 04:37:04.413292 35003 sgd_solver.cpp:112] Iteration 144030, lr = 0.01
I0523 04:37:07.118862 35003 solver.cpp:239] Iteration 144040 (3.68516 iter/s, 2.71359s/10 iters), loss = 7.2346
I0523 04:37:07.118911 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2346 (* 1 = 7.2346 loss)
I0523 04:37:07.124316 35003 sgd_solver.cpp:112] Iteration 144040, lr = 0.01
I0523 04:37:10.953559 35003 solver.cpp:239] Iteration 144050 (2.60793 iter/s, 3.83445s/10 iters), loss = 6.19254
I0523 04:37:10.953609 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19254 (* 1 = 6.19254 loss)
I0523 04:37:10.967268 35003 sgd_solver.cpp:112] Iteration 144050, lr = 0.01
I0523 04:37:14.565611 35003 solver.cpp:239] Iteration 144060 (2.76866 iter/s, 3.61185s/10 iters), loss = 7.55487
I0523 04:37:14.565655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55487 (* 1 = 7.55487 loss)
I0523 04:37:15.239055 35003 sgd_solver.cpp:112] Iteration 144060, lr = 0.01
I0523 04:37:17.946502 35003 solver.cpp:239] Iteration 144070 (2.95796 iter/s, 3.3807s/10 iters), loss = 6.52157
I0523 04:37:17.946774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52157 (* 1 = 6.52157 loss)
I0523 04:37:17.964889 35003 sgd_solver.cpp:112] Iteration 144070, lr = 0.01
I0523 04:37:21.576997 35003 solver.cpp:239] Iteration 144080 (2.75476 iter/s, 3.63008s/10 iters), loss = 7.64731
I0523 04:37:21.577045 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64731 (* 1 = 7.64731 loss)
I0523 04:37:21.588424 35003 sgd_solver.cpp:112] Iteration 144080, lr = 0.01
I0523 04:37:23.360447 35003 solver.cpp:239] Iteration 144090 (5.60753 iter/s, 1.78332s/10 iters), loss = 5.95273
I0523 04:37:23.360512 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95273 (* 1 = 5.95273 loss)
I0523 04:37:24.061616 35003 sgd_solver.cpp:112] Iteration 144090, lr = 0.01
I0523 04:37:26.161417 35003 solver.cpp:239] Iteration 144100 (3.57043 iter/s, 2.80078s/10 iters), loss = 6.87726
I0523 04:37:26.161460 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87726 (* 1 = 6.87726 loss)
I0523 04:37:26.859315 35003 sgd_solver.cpp:112] Iteration 144100, lr = 0.01
I0523 04:37:29.853236 35003 solver.cpp:239] Iteration 144110 (2.70884 iter/s, 3.69162s/10 iters), loss = 6.55375
I0523 04:37:29.853291 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55375 (* 1 = 6.55375 loss)
I0523 04:37:30.561681 35003 sgd_solver.cpp:112] Iteration 144110, lr = 0.01
I0523 04:37:33.990892 35003 solver.cpp:239] Iteration 144120 (2.41696 iter/s, 4.13743s/10 iters), loss = 6.399
I0523 04:37:33.990943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.399 (* 1 = 6.399 loss)
I0523 04:37:33.998664 35003 sgd_solver.cpp:112] Iteration 144120, lr = 0.01
I0523 04:37:37.647204 35003 solver.cpp:239] Iteration 144130 (2.73515 iter/s, 3.65611s/10 iters), loss = 6.82448
I0523 04:37:37.647245 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82448 (* 1 = 6.82448 loss)
I0523 04:37:37.653407 35003 sgd_solver.cpp:112] Iteration 144130, lr = 0.01
I0523 04:37:42.840878 35003 solver.cpp:239] Iteration 144140 (1.92552 iter/s, 5.19341s/10 iters), loss = 7.55838
I0523 04:37:42.840919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55838 (* 1 = 7.55838 loss)
I0523 04:37:43.549711 35003 sgd_solver.cpp:112] Iteration 144140, lr = 0.01
I0523 04:37:45.638448 35003 solver.cpp:239] Iteration 144150 (3.57473 iter/s, 2.79741s/10 iters), loss = 7.77324
I0523 04:37:45.638486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77324 (* 1 = 7.77324 loss)
I0523 04:37:45.648092 35003 sgd_solver.cpp:112] Iteration 144150, lr = 0.01
I0523 04:37:49.016364 35003 solver.cpp:239] Iteration 144160 (2.96056 iter/s, 3.37773s/10 iters), loss = 7.38258
I0523 04:37:49.016511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38258 (* 1 = 7.38258 loss)
I0523 04:37:49.021733 35003 sgd_solver.cpp:112] Iteration 144160, lr = 0.01
I0523 04:37:52.113625 35003 solver.cpp:239] Iteration 144170 (3.22895 iter/s, 3.09698s/10 iters), loss = 7.35798
I0523 04:37:52.113684 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35798 (* 1 = 7.35798 loss)
I0523 04:37:52.118799 35003 sgd_solver.cpp:112] Iteration 144170, lr = 0.01
I0523 04:37:54.941536 35003 solver.cpp:239] Iteration 144180 (3.53641 iter/s, 2.82773s/10 iters), loss = 6.96617
I0523 04:37:54.941592 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96617 (* 1 = 6.96617 loss)
I0523 04:37:54.951822 35003 sgd_solver.cpp:112] Iteration 144180, lr = 0.01
I0523 04:37:57.627851 35003 solver.cpp:239] Iteration 144190 (3.72281 iter/s, 2.68614s/10 iters), loss = 7.49812
I0523 04:37:57.627887 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49812 (* 1 = 7.49812 loss)
I0523 04:37:57.634609 35003 sgd_solver.cpp:112] Iteration 144190, lr = 0.01
I0523 04:37:59.750063 35003 solver.cpp:239] Iteration 144200 (4.71235 iter/s, 2.12208s/10 iters), loss = 6.88604
I0523 04:37:59.750113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88604 (* 1 = 6.88604 loss)
I0523 04:38:00.460543 35003 sgd_solver.cpp:112] Iteration 144200, lr = 0.01
I0523 04:38:01.828593 35003 solver.cpp:239] Iteration 144210 (4.81142 iter/s, 2.07839s/10 iters), loss = 7.63237
I0523 04:38:01.828644 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63237 (* 1 = 7.63237 loss)
I0523 04:38:02.569954 35003 sgd_solver.cpp:112] Iteration 144210, lr = 0.01
I0523 04:38:06.538645 35003 solver.cpp:239] Iteration 144220 (2.12323 iter/s, 4.70981s/10 iters), loss = 8.18662
I0523 04:38:06.538717 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.18662 (* 1 = 8.18662 loss)
I0523 04:38:06.551628 35003 sgd_solver.cpp:112] Iteration 144220, lr = 0.01
I0523 04:38:10.221859 35003 solver.cpp:239] Iteration 144230 (2.71517 iter/s, 3.68301s/10 iters), loss = 6.8249
I0523 04:38:10.221899 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8249 (* 1 = 6.8249 loss)
I0523 04:38:10.235893 35003 sgd_solver.cpp:112] Iteration 144230, lr = 0.01
I0523 04:38:13.818789 35003 solver.cpp:239] Iteration 144240 (2.7803 iter/s, 3.59674s/10 iters), loss = 7.09271
I0523 04:38:13.818828 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09271 (* 1 = 7.09271 loss)
I0523 04:38:13.831948 35003 sgd_solver.cpp:112] Iteration 144240, lr = 0.01
I0523 04:38:15.950072 35003 solver.cpp:239] Iteration 144250 (4.6923 iter/s, 2.13115s/10 iters), loss = 6.89946
I0523 04:38:15.950115 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89946 (* 1 = 6.89946 loss)
I0523 04:38:15.964120 35003 sgd_solver.cpp:112] Iteration 144250, lr = 0.01
I0523 04:38:19.570858 35003 solver.cpp:239] Iteration 144260 (2.76198 iter/s, 3.62059s/10 iters), loss = 7.55142
I0523 04:38:19.571038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55142 (* 1 = 7.55142 loss)
I0523 04:38:19.584430 35003 sgd_solver.cpp:112] Iteration 144260, lr = 0.01
I0523 04:38:23.229573 35003 solver.cpp:239] Iteration 144270 (2.73344 iter/s, 3.65839s/10 iters), loss = 7.58751
I0523 04:38:23.229614 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58751 (* 1 = 7.58751 loss)
I0523 04:38:23.245651 35003 sgd_solver.cpp:112] Iteration 144270, lr = 0.01
I0523 04:38:28.494658 35003 solver.cpp:239] Iteration 144280 (1.8994 iter/s, 5.26483s/10 iters), loss = 6.7254
I0523 04:38:28.494711 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7254 (* 1 = 6.7254 loss)
I0523 04:38:28.502019 35003 sgd_solver.cpp:112] Iteration 144280, lr = 0.01
I0523 04:38:30.211580 35003 solver.cpp:239] Iteration 144290 (5.82484 iter/s, 1.71679s/10 iters), loss = 6.25345
I0523 04:38:30.211639 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25345 (* 1 = 6.25345 loss)
I0523 04:38:30.239967 35003 sgd_solver.cpp:112] Iteration 144290, lr = 0.01
I0523 04:38:33.333415 35003 solver.cpp:239] Iteration 144300 (3.20345 iter/s, 3.12163s/10 iters), loss = 7.20059
I0523 04:38:33.333484 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20059 (* 1 = 7.20059 loss)
I0523 04:38:33.336522 35003 sgd_solver.cpp:112] Iteration 144300, lr = 0.01
I0523 04:38:35.330549 35003 solver.cpp:239] Iteration 144310 (5.00758 iter/s, 1.99697s/10 iters), loss = 6.43576
I0523 04:38:35.330595 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43576 (* 1 = 6.43576 loss)
I0523 04:38:35.343210 35003 sgd_solver.cpp:112] Iteration 144310, lr = 0.01
I0523 04:38:40.067062 35003 solver.cpp:239] Iteration 144320 (2.11137 iter/s, 4.73627s/10 iters), loss = 6.65808
I0523 04:38:40.067126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65808 (* 1 = 6.65808 loss)
I0523 04:38:40.472069 35003 sgd_solver.cpp:112] Iteration 144320, lr = 0.01
I0523 04:38:45.488620 35003 solver.cpp:239] Iteration 144330 (1.84458 iter/s, 5.42128s/10 iters), loss = 6.33672
I0523 04:38:45.488667 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33672 (* 1 = 6.33672 loss)
I0523 04:38:45.502264 35003 sgd_solver.cpp:112] Iteration 144330, lr = 0.01
I0523 04:38:48.381431 35003 solver.cpp:239] Iteration 144340 (3.45704 iter/s, 2.89265s/10 iters), loss = 6.88331
I0523 04:38:48.381474 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88331 (* 1 = 6.88331 loss)
I0523 04:38:48.392055 35003 sgd_solver.cpp:112] Iteration 144340, lr = 0.01
I0523 04:38:50.420758 35003 solver.cpp:239] Iteration 144350 (4.90391 iter/s, 2.03919s/10 iters), loss = 6.93666
I0523 04:38:50.420882 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93666 (* 1 = 6.93666 loss)
I0523 04:38:51.119348 35003 sgd_solver.cpp:112] Iteration 144350, lr = 0.01
I0523 04:38:55.279181 35003 solver.cpp:239] Iteration 144360 (2.05842 iter/s, 4.85809s/10 iters), loss = 7.43539
I0523 04:38:55.279240 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43539 (* 1 = 7.43539 loss)
I0523 04:38:55.291575 35003 sgd_solver.cpp:112] Iteration 144360, lr = 0.01
I0523 04:38:58.591693 35003 solver.cpp:239] Iteration 144370 (3.01904 iter/s, 3.31231s/10 iters), loss = 7.17313
I0523 04:38:58.591732 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17313 (* 1 = 7.17313 loss)
I0523 04:38:58.604470 35003 sgd_solver.cpp:112] Iteration 144370, lr = 0.01
I0523 04:39:01.263581 35003 solver.cpp:239] Iteration 144380 (3.74288 iter/s, 2.67174s/10 iters), loss = 7.21906
I0523 04:39:01.263625 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21906 (* 1 = 7.21906 loss)
I0523 04:39:01.972470 35003 sgd_solver.cpp:112] Iteration 144380, lr = 0.01
I0523 04:39:04.869341 35003 solver.cpp:239] Iteration 144390 (2.77349 iter/s, 3.60557s/10 iters), loss = 7.83867
I0523 04:39:04.869385 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83867 (* 1 = 7.83867 loss)
I0523 04:39:05.584205 35003 sgd_solver.cpp:112] Iteration 144390, lr = 0.01
I0523 04:39:08.902736 35003 solver.cpp:239] Iteration 144400 (2.47943 iter/s, 4.03318s/10 iters), loss = 5.97108
I0523 04:39:08.902783 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97108 (* 1 = 5.97108 loss)
I0523 04:39:08.916220 35003 sgd_solver.cpp:112] Iteration 144400, lr = 0.01
I0523 04:39:10.370411 35003 solver.cpp:239] Iteration 144410 (6.81404 iter/s, 1.46756s/10 iters), loss = 7.83907
I0523 04:39:10.370450 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83907 (* 1 = 7.83907 loss)
I0523 04:39:10.383625 35003 sgd_solver.cpp:112] Iteration 144410, lr = 0.01
I0523 04:39:12.464217 35003 solver.cpp:239] Iteration 144420 (4.7763 iter/s, 2.09367s/10 iters), loss = 7.32001
I0523 04:39:12.464264 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32001 (* 1 = 7.32001 loss)
I0523 04:39:12.472137 35003 sgd_solver.cpp:112] Iteration 144420, lr = 0.01
I0523 04:39:15.942453 35003 solver.cpp:239] Iteration 144430 (2.87518 iter/s, 3.47804s/10 iters), loss = 6.97585
I0523 04:39:15.942512 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97585 (* 1 = 6.97585 loss)
I0523 04:39:16.639814 35003 sgd_solver.cpp:112] Iteration 144430, lr = 0.01
I0523 04:39:19.410125 35003 solver.cpp:239] Iteration 144440 (2.88395 iter/s, 3.46747s/10 iters), loss = 6.6718
I0523 04:39:19.410164 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6718 (* 1 = 6.6718 loss)
I0523 04:39:19.439692 35003 sgd_solver.cpp:112] Iteration 144440, lr = 0.01
I0523 04:39:22.094945 35003 solver.cpp:239] Iteration 144450 (3.72486 iter/s, 2.68466s/10 iters), loss = 7.52109
I0523 04:39:22.095129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52109 (* 1 = 7.52109 loss)
I0523 04:39:22.103413 35003 sgd_solver.cpp:112] Iteration 144450, lr = 0.01
I0523 04:39:25.193982 35003 solver.cpp:239] Iteration 144460 (3.22726 iter/s, 3.0986s/10 iters), loss = 6.75337
I0523 04:39:25.194031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75337 (* 1 = 6.75337 loss)
I0523 04:39:25.203810 35003 sgd_solver.cpp:112] Iteration 144460, lr = 0.01
I0523 04:39:28.759452 35003 solver.cpp:239] Iteration 144470 (2.80484 iter/s, 3.56527s/10 iters), loss = 6.27061
I0523 04:39:28.759513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27061 (* 1 = 6.27061 loss)
I0523 04:39:28.767117 35003 sgd_solver.cpp:112] Iteration 144470, lr = 0.01
I0523 04:39:33.790298 35003 solver.cpp:239] Iteration 144480 (1.98784 iter/s, 5.03058s/10 iters), loss = 7.62806
I0523 04:39:33.790335 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62806 (* 1 = 7.62806 loss)
I0523 04:39:34.498497 35003 sgd_solver.cpp:112] Iteration 144480, lr = 0.01
I0523 04:39:37.410760 35003 solver.cpp:239] Iteration 144490 (2.76222 iter/s, 3.62027s/10 iters), loss = 6.67378
I0523 04:39:37.410812 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67378 (* 1 = 6.67378 loss)
I0523 04:39:37.423897 35003 sgd_solver.cpp:112] Iteration 144490, lr = 0.01
I0523 04:39:39.367512 35003 solver.cpp:239] Iteration 144500 (5.11087 iter/s, 1.95661s/10 iters), loss = 5.71533
I0523 04:39:39.367561 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.71533 (* 1 = 5.71533 loss)
I0523 04:39:39.376912 35003 sgd_solver.cpp:112] Iteration 144500, lr = 0.01
I0523 04:39:40.671391 35003 solver.cpp:239] Iteration 144510 (7.67006 iter/s, 1.30377s/10 iters), loss = 5.39755
I0523 04:39:40.671430 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.39755 (* 1 = 5.39755 loss)
I0523 04:39:40.690713 35003 sgd_solver.cpp:112] Iteration 144510, lr = 0.01
I0523 04:39:44.839964 35003 solver.cpp:239] Iteration 144520 (2.39902 iter/s, 4.16837s/10 iters), loss = 7.74682
I0523 04:39:44.840003 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74682 (* 1 = 7.74682 loss)
I0523 04:39:44.852857 35003 sgd_solver.cpp:112] Iteration 144520, lr = 0.01
I0523 04:39:47.701656 35003 solver.cpp:239] Iteration 144530 (3.49465 iter/s, 2.86152s/10 iters), loss = 7.249
I0523 04:39:47.701704 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.249 (* 1 = 7.249 loss)
I0523 04:39:47.705984 35003 sgd_solver.cpp:112] Iteration 144530, lr = 0.01
I0523 04:39:50.555186 35003 solver.cpp:239] Iteration 144540 (3.50465 iter/s, 2.85335s/10 iters), loss = 6.87575
I0523 04:39:50.555295 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87575 (* 1 = 6.87575 loss)
I0523 04:39:50.578807 35003 sgd_solver.cpp:112] Iteration 144540, lr = 0.01
I0523 04:39:54.048009 35003 solver.cpp:239] Iteration 144550 (2.86323 iter/s, 3.49256s/10 iters), loss = 7.55953
I0523 04:39:54.050043 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55953 (* 1 = 7.55953 loss)
I0523 04:39:54.066373 35003 sgd_solver.cpp:112] Iteration 144550, lr = 0.01
I0523 04:39:57.425477 35003 solver.cpp:239] Iteration 144560 (2.96533 iter/s, 3.37231s/10 iters), loss = 8.08475
I0523 04:39:57.425528 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08475 (* 1 = 8.08475 loss)
I0523 04:39:57.435818 35003 sgd_solver.cpp:112] Iteration 144560, lr = 0.01
I0523 04:40:00.301237 35003 solver.cpp:239] Iteration 144570 (3.47756 iter/s, 2.87558s/10 iters), loss = 6.88414
I0523 04:40:00.301276 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88414 (* 1 = 6.88414 loss)
I0523 04:40:00.314864 35003 sgd_solver.cpp:112] Iteration 144570, lr = 0.01
I0523 04:40:04.002004 35003 solver.cpp:239] Iteration 144580 (2.70229 iter/s, 3.70057s/10 iters), loss = 7.58481
I0523 04:40:04.002044 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58481 (* 1 = 7.58481 loss)
I0523 04:40:04.015352 35003 sgd_solver.cpp:112] Iteration 144580, lr = 0.01
I0523 04:40:07.613615 35003 solver.cpp:239] Iteration 144590 (2.76899 iter/s, 3.61142s/10 iters), loss = 6.17624
I0523 04:40:07.613662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17624 (* 1 = 6.17624 loss)
I0523 04:40:08.329141 35003 sgd_solver.cpp:112] Iteration 144590, lr = 0.01
I0523 04:40:10.412617 35003 solver.cpp:239] Iteration 144600 (3.57291 iter/s, 2.79884s/10 iters), loss = 6.20384
I0523 04:40:10.412658 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20384 (* 1 = 6.20384 loss)
I0523 04:40:10.426298 35003 sgd_solver.cpp:112] Iteration 144600, lr = 0.01
I0523 04:40:13.270748 35003 solver.cpp:239] Iteration 144610 (3.49904 iter/s, 2.85793s/10 iters), loss = 6.64392
I0523 04:40:13.270807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64392 (* 1 = 6.64392 loss)
I0523 04:40:13.283776 35003 sgd_solver.cpp:112] Iteration 144610, lr = 0.01
I0523 04:40:16.409848 35003 solver.cpp:239] Iteration 144620 (3.18581 iter/s, 3.13891s/10 iters), loss = 7.12248
I0523 04:40:16.409886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12248 (* 1 = 7.12248 loss)
I0523 04:40:16.427968 35003 sgd_solver.cpp:112] Iteration 144620, lr = 0.01
I0523 04:40:19.257925 35003 solver.cpp:239] Iteration 144630 (3.51135 iter/s, 2.84791s/10 iters), loss = 7.75648
I0523 04:40:19.257984 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75648 (* 1 = 7.75648 loss)
I0523 04:40:19.445896 35003 sgd_solver.cpp:112] Iteration 144630, lr = 0.01
I0523 04:40:22.120308 35003 solver.cpp:239] Iteration 144640 (3.49381 iter/s, 2.8622s/10 iters), loss = 7.67618
I0523 04:40:22.120357 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67618 (* 1 = 7.67618 loss)
I0523 04:40:22.131661 35003 sgd_solver.cpp:112] Iteration 144640, lr = 0.01
I0523 04:40:24.891607 35003 solver.cpp:239] Iteration 144650 (3.60863 iter/s, 2.77113s/10 iters), loss = 6.87023
I0523 04:40:24.891816 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87023 (* 1 = 6.87023 loss)
I0523 04:40:24.911964 35003 sgd_solver.cpp:112] Iteration 144650, lr = 0.01
I0523 04:40:29.172369 35003 solver.cpp:239] Iteration 144660 (2.33625 iter/s, 4.28036s/10 iters), loss = 6.56812
I0523 04:40:29.172418 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56812 (* 1 = 6.56812 loss)
I0523 04:40:29.184689 35003 sgd_solver.cpp:112] Iteration 144660, lr = 0.01
I0523 04:40:31.924823 35003 solver.cpp:239] Iteration 144670 (3.63337 iter/s, 2.75227s/10 iters), loss = 7.50837
I0523 04:40:31.924888 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50837 (* 1 = 7.50837 loss)
I0523 04:40:32.565268 35003 sgd_solver.cpp:112] Iteration 144670, lr = 0.01
I0523 04:40:35.401099 35003 solver.cpp:239] Iteration 144680 (2.87681 iter/s, 3.47607s/10 iters), loss = 6.84358
I0523 04:40:35.401149 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84358 (* 1 = 6.84358 loss)
I0523 04:40:35.985733 35003 sgd_solver.cpp:112] Iteration 144680, lr = 0.01
I0523 04:40:38.214840 35003 solver.cpp:239] Iteration 144690 (3.5542 iter/s, 2.81357s/10 iters), loss = 6.84753
I0523 04:40:38.214881 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84753 (* 1 = 6.84753 loss)
I0523 04:40:38.220906 35003 sgd_solver.cpp:112] Iteration 144690, lr = 0.01
I0523 04:40:40.314450 35003 solver.cpp:239] Iteration 144700 (4.76316 iter/s, 2.09945s/10 iters), loss = 7.20524
I0523 04:40:40.314491 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20524 (* 1 = 7.20524 loss)
I0523 04:40:40.339381 35003 sgd_solver.cpp:112] Iteration 144700, lr = 0.01
I0523 04:40:42.950045 35003 solver.cpp:239] Iteration 144710 (3.79443 iter/s, 2.63544s/10 iters), loss = 7.14728
I0523 04:40:42.950080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14728 (* 1 = 7.14728 loss)
I0523 04:40:42.963313 35003 sgd_solver.cpp:112] Iteration 144710, lr = 0.01
I0523 04:40:46.370265 35003 solver.cpp:239] Iteration 144720 (2.92394 iter/s, 3.42004s/10 iters), loss = 6.67383
I0523 04:40:46.370309 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67383 (* 1 = 6.67383 loss)
I0523 04:40:46.383699 35003 sgd_solver.cpp:112] Iteration 144720, lr = 0.01
I0523 04:40:49.795897 35003 solver.cpp:239] Iteration 144730 (2.91934 iter/s, 3.42544s/10 iters), loss = 7.03731
I0523 04:40:49.795954 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03731 (* 1 = 7.03731 loss)
I0523 04:40:50.235285 35003 sgd_solver.cpp:112] Iteration 144730, lr = 0.01
I0523 04:40:53.322846 35003 solver.cpp:239] Iteration 144740 (2.83548 iter/s, 3.52674s/10 iters), loss = 6.7138
I0523 04:40:53.322911 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7138 (* 1 = 6.7138 loss)
I0523 04:40:53.357687 35003 sgd_solver.cpp:112] Iteration 144740, lr = 0.01
I0523 04:40:56.221209 35003 solver.cpp:239] Iteration 144750 (3.45054 iter/s, 2.8981s/10 iters), loss = 6.99029
I0523 04:40:56.221513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99029 (* 1 = 6.99029 loss)
I0523 04:40:56.266145 35003 sgd_solver.cpp:112] Iteration 144750, lr = 0.01
I0523 04:40:59.152647 35003 solver.cpp:239] Iteration 144760 (3.41178 iter/s, 2.93102s/10 iters), loss = 6.50105
I0523 04:40:59.152701 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50105 (* 1 = 6.50105 loss)
I0523 04:40:59.165294 35003 sgd_solver.cpp:112] Iteration 144760, lr = 0.01
I0523 04:41:03.508218 35003 solver.cpp:239] Iteration 144770 (2.29603 iter/s, 4.35534s/10 iters), loss = 6.48829
I0523 04:41:03.508265 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48829 (* 1 = 6.48829 loss)
I0523 04:41:03.529002 35003 sgd_solver.cpp:112] Iteration 144770, lr = 0.01
I0523 04:41:06.105819 35003 solver.cpp:239] Iteration 144780 (3.84994 iter/s, 2.59745s/10 iters), loss = 8.17765
I0523 04:41:06.105857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17765 (* 1 = 8.17765 loss)
I0523 04:41:06.113714 35003 sgd_solver.cpp:112] Iteration 144780, lr = 0.01
I0523 04:41:10.264425 35003 solver.cpp:239] Iteration 144790 (2.40478 iter/s, 4.15839s/10 iters), loss = 5.93835
I0523 04:41:10.264470 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93835 (* 1 = 5.93835 loss)
I0523 04:41:10.269541 35003 sgd_solver.cpp:112] Iteration 144790, lr = 0.01
I0523 04:41:15.234354 35003 solver.cpp:239] Iteration 144800 (2.0122 iter/s, 4.96968s/10 iters), loss = 6.47178
I0523 04:41:15.234402 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47178 (* 1 = 6.47178 loss)
I0523 04:41:15.949810 35003 sgd_solver.cpp:112] Iteration 144800, lr = 0.01
I0523 04:41:18.663944 35003 solver.cpp:239] Iteration 144810 (2.91597 iter/s, 3.42939s/10 iters), loss = 7.30137
I0523 04:41:18.663985 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30137 (* 1 = 7.30137 loss)
I0523 04:41:18.677489 35003 sgd_solver.cpp:112] Iteration 144810, lr = 0.01
I0523 04:41:21.486678 35003 solver.cpp:239] Iteration 144820 (3.54288 iter/s, 2.82256s/10 iters), loss = 7.55252
I0523 04:41:21.486747 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55252 (* 1 = 7.55252 loss)
I0523 04:41:21.499339 35003 sgd_solver.cpp:112] Iteration 144820, lr = 0.01
I0523 04:41:24.650615 35003 solver.cpp:239] Iteration 144830 (3.16081 iter/s, 3.16374s/10 iters), loss = 7.2956
I0523 04:41:24.650652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2956 (* 1 = 7.2956 loss)
I0523 04:41:24.658716 35003 sgd_solver.cpp:112] Iteration 144830, lr = 0.01
I0523 04:41:27.453990 35003 solver.cpp:239] Iteration 144840 (3.56734 iter/s, 2.80321s/10 iters), loss = 7.5883
I0523 04:41:27.454802 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5883 (* 1 = 7.5883 loss)
I0523 04:41:27.461601 35003 sgd_solver.cpp:112] Iteration 144840, lr = 0.01
I0523 04:41:31.651688 35003 solver.cpp:239] Iteration 144850 (2.38282 iter/s, 4.19671s/10 iters), loss = 6.9886
I0523 04:41:31.651741 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9886 (* 1 = 6.9886 loss)
I0523 04:41:32.366544 35003 sgd_solver.cpp:112] Iteration 144850, lr = 0.01
I0523 04:41:35.681007 35003 solver.cpp:239] Iteration 144860 (2.48195 iter/s, 4.0291s/10 iters), loss = 6.9528
I0523 04:41:35.681061 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9528 (* 1 = 6.9528 loss)
I0523 04:41:36.415489 35003 sgd_solver.cpp:112] Iteration 144860, lr = 0.01
I0523 04:41:39.875196 35003 solver.cpp:239] Iteration 144870 (2.38438 iter/s, 4.19396s/10 iters), loss = 7.77604
I0523 04:41:39.875241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77604 (* 1 = 7.77604 loss)
I0523 04:41:39.884328 35003 sgd_solver.cpp:112] Iteration 144870, lr = 0.01
I0523 04:41:42.640981 35003 solver.cpp:239] Iteration 144880 (3.61582 iter/s, 2.76562s/10 iters), loss = 7.54336
I0523 04:41:42.641022 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54336 (* 1 = 7.54336 loss)
I0523 04:41:42.659566 35003 sgd_solver.cpp:112] Iteration 144880, lr = 0.01
I0523 04:41:48.277681 35003 solver.cpp:239] Iteration 144890 (1.77417 iter/s, 5.63643s/10 iters), loss = 6.95355
I0523 04:41:48.277724 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95355 (* 1 = 6.95355 loss)
I0523 04:41:48.319833 35003 sgd_solver.cpp:112] Iteration 144890, lr = 0.01
I0523 04:41:51.103549 35003 solver.cpp:239] Iteration 144900 (3.53894 iter/s, 2.82571s/10 iters), loss = 7.7513
I0523 04:41:51.103595 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7513 (* 1 = 7.7513 loss)
I0523 04:41:51.107607 35003 sgd_solver.cpp:112] Iteration 144900, lr = 0.01
I0523 04:41:54.754413 35003 solver.cpp:239] Iteration 144910 (2.73923 iter/s, 3.65066s/10 iters), loss = 7.22495
I0523 04:41:54.754458 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22495 (* 1 = 7.22495 loss)
I0523 04:41:54.759838 35003 sgd_solver.cpp:112] Iteration 144910, lr = 0.01
I0523 04:41:58.234855 35003 solver.cpp:239] Iteration 144920 (2.87336 iter/s, 3.48025s/10 iters), loss = 7.1512
I0523 04:41:58.235131 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1512 (* 1 = 7.1512 loss)
I0523 04:41:58.949266 35003 sgd_solver.cpp:112] Iteration 144920, lr = 0.01
I0523 04:42:02.378803 35003 solver.cpp:239] Iteration 144930 (2.4134 iter/s, 4.14353s/10 iters), loss = 7.04461
I0523 04:42:02.378859 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04461 (* 1 = 7.04461 loss)
I0523 04:42:03.080477 35003 sgd_solver.cpp:112] Iteration 144930, lr = 0.01
I0523 04:42:05.847689 35003 solver.cpp:239] Iteration 144940 (2.88294 iter/s, 3.46869s/10 iters), loss = 6.96746
I0523 04:42:05.847733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96746 (* 1 = 6.96746 loss)
I0523 04:42:06.032781 35003 sgd_solver.cpp:112] Iteration 144940, lr = 0.01
I0523 04:42:09.464413 35003 solver.cpp:239] Iteration 144950 (2.76508 iter/s, 3.61653s/10 iters), loss = 6.80759
I0523 04:42:09.464459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80759 (* 1 = 6.80759 loss)
I0523 04:42:09.482116 35003 sgd_solver.cpp:112] Iteration 144950, lr = 0.01
I0523 04:42:12.328006 35003 solver.cpp:239] Iteration 144960 (3.49233 iter/s, 2.86342s/10 iters), loss = 6.92701
I0523 04:42:12.328044 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92701 (* 1 = 6.92701 loss)
I0523 04:42:12.519976 35003 sgd_solver.cpp:112] Iteration 144960, lr = 0.01
I0523 04:42:15.252126 35003 solver.cpp:239] Iteration 144970 (3.42003 iter/s, 2.92395s/10 iters), loss = 7.16607
I0523 04:42:15.252179 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16607 (* 1 = 7.16607 loss)
I0523 04:42:15.791818 35003 sgd_solver.cpp:112] Iteration 144970, lr = 0.01
I0523 04:42:20.146909 35003 solver.cpp:239] Iteration 144980 (2.0431 iter/s, 4.89453s/10 iters), loss = 7.82656
I0523 04:42:20.146953 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82656 (* 1 = 7.82656 loss)
I0523 04:42:20.885798 35003 sgd_solver.cpp:112] Iteration 144980, lr = 0.01
I0523 04:42:24.477952 35003 solver.cpp:239] Iteration 144990 (2.30903 iter/s, 4.33082s/10 iters), loss = 6.47861
I0523 04:42:24.477994 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47861 (* 1 = 6.47861 loss)
I0523 04:42:24.491281 35003 sgd_solver.cpp:112] Iteration 144990, lr = 0.01
I0523 04:42:27.586203 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_145000.caffemodel
I0523 04:42:27.949692 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_145000.solverstate
I0523 04:42:28.092725 35003 solver.cpp:239] Iteration 145000 (2.76657 iter/s, 3.61458s/10 iters), loss = 7.48305
I0523 04:42:28.092763 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48305 (* 1 = 7.48305 loss)
I0523 04:42:28.102782 35003 sgd_solver.cpp:112] Iteration 145000, lr = 0.01
I0523 04:42:30.983583 35003 solver.cpp:239] Iteration 145010 (3.45938 iter/s, 2.89069s/10 iters), loss = 5.90915
I0523 04:42:30.983880 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.90915 (* 1 = 5.90915 loss)
I0523 04:42:31.725014 35003 sgd_solver.cpp:112] Iteration 145010, lr = 0.01
I0523 04:42:35.014865 35003 solver.cpp:239] Iteration 145020 (2.48087 iter/s, 4.03085s/10 iters), loss = 7.49857
I0523 04:42:35.014904 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49857 (* 1 = 7.49857 loss)
I0523 04:42:35.023187 35003 sgd_solver.cpp:112] Iteration 145020, lr = 0.01
I0523 04:42:39.487684 35003 solver.cpp:239] Iteration 145030 (2.23585 iter/s, 4.47258s/10 iters), loss = 5.98009
I0523 04:42:39.487745 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98009 (* 1 = 5.98009 loss)
I0523 04:42:39.547621 35003 sgd_solver.cpp:112] Iteration 145030, lr = 0.01
I0523 04:42:42.415161 35003 solver.cpp:239] Iteration 145040 (3.41612 iter/s, 2.92729s/10 iters), loss = 6.13931
I0523 04:42:42.415220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13931 (* 1 = 6.13931 loss)
I0523 04:42:42.423868 35003 sgd_solver.cpp:112] Iteration 145040, lr = 0.01
I0523 04:42:46.542310 35003 solver.cpp:239] Iteration 145050 (2.42312 iter/s, 4.12692s/10 iters), loss = 7.86642
I0523 04:42:46.542371 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86642 (* 1 = 7.86642 loss)
I0523 04:42:46.549599 35003 sgd_solver.cpp:112] Iteration 145050, lr = 0.01
I0523 04:42:50.167927 35003 solver.cpp:239] Iteration 145060 (2.75832 iter/s, 3.6254s/10 iters), loss = 7.84376
I0523 04:42:50.167979 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84376 (* 1 = 7.84376 loss)
I0523 04:42:50.409301 35003 sgd_solver.cpp:112] Iteration 145060, lr = 0.01
I0523 04:42:53.491323 35003 solver.cpp:239] Iteration 145070 (3.00914 iter/s, 3.3232s/10 iters), loss = 8.14842
I0523 04:42:53.491380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.14842 (* 1 = 8.14842 loss)
I0523 04:42:53.501416 35003 sgd_solver.cpp:112] Iteration 145070, lr = 0.01
I0523 04:42:57.147009 35003 solver.cpp:239] Iteration 145080 (2.73563 iter/s, 3.65547s/10 iters), loss = 6.24093
I0523 04:42:57.147048 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24093 (* 1 = 6.24093 loss)
I0523 04:42:57.160985 35003 sgd_solver.cpp:112] Iteration 145080, lr = 0.01
I0523 04:42:59.635077 35003 solver.cpp:239] Iteration 145090 (4.01944 iter/s, 2.48791s/10 iters), loss = 8.21389
I0523 04:42:59.635128 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.21389 (* 1 = 8.21389 loss)
I0523 04:42:59.644467 35003 sgd_solver.cpp:112] Iteration 145090, lr = 0.01
I0523 04:43:02.474488 35003 solver.cpp:239] Iteration 145100 (3.52208 iter/s, 2.83923s/10 iters), loss = 7.40752
I0523 04:43:02.474683 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40752 (* 1 = 7.40752 loss)
I0523 04:43:02.488109 35003 sgd_solver.cpp:112] Iteration 145100, lr = 0.01
I0523 04:43:05.346395 35003 solver.cpp:239] Iteration 145110 (3.48236 iter/s, 2.87162s/10 iters), loss = 7.15
I0523 04:43:05.346439 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15 (* 1 = 7.15 loss)
I0523 04:43:05.359972 35003 sgd_solver.cpp:112] Iteration 145110, lr = 0.01
I0523 04:43:07.377408 35003 solver.cpp:239] Iteration 145120 (4.92398 iter/s, 2.03088s/10 iters), loss = 7.03943
I0523 04:43:07.377446 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03943 (* 1 = 7.03943 loss)
I0523 04:43:07.391168 35003 sgd_solver.cpp:112] Iteration 145120, lr = 0.01
I0523 04:43:09.828539 35003 solver.cpp:239] Iteration 145130 (4.08 iter/s, 2.45098s/10 iters), loss = 7.10962
I0523 04:43:09.828588 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10962 (* 1 = 7.10962 loss)
I0523 04:43:10.563010 35003 sgd_solver.cpp:112] Iteration 145130, lr = 0.01
I0523 04:43:13.499426 35003 solver.cpp:239] Iteration 145140 (2.7243 iter/s, 3.67067s/10 iters), loss = 7.2915
I0523 04:43:13.499477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2915 (* 1 = 7.2915 loss)
I0523 04:43:14.240543 35003 sgd_solver.cpp:112] Iteration 145140, lr = 0.01
I0523 04:43:19.430717 35003 solver.cpp:239] Iteration 145150 (1.68606 iter/s, 5.93099s/10 iters), loss = 8.21823
I0523 04:43:19.430771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.21823 (* 1 = 8.21823 loss)
I0523 04:43:19.435712 35003 sgd_solver.cpp:112] Iteration 145150, lr = 0.01
I0523 04:43:22.998190 35003 solver.cpp:239] Iteration 145160 (2.80329 iter/s, 3.56724s/10 iters), loss = 7.30013
I0523 04:43:22.998235 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30013 (* 1 = 7.30013 loss)
I0523 04:43:23.020664 35003 sgd_solver.cpp:112] Iteration 145160, lr = 0.01
I0523 04:43:24.319254 35003 solver.cpp:239] Iteration 145170 (7.57027 iter/s, 1.32096s/10 iters), loss = 7.25886
I0523 04:43:24.319300 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25886 (* 1 = 7.25886 loss)
I0523 04:43:24.323118 35003 sgd_solver.cpp:112] Iteration 145170, lr = 0.01
I0523 04:43:26.663450 35003 solver.cpp:239] Iteration 145180 (4.26613 iter/s, 2.34405s/10 iters), loss = 6.92909
I0523 04:43:26.663509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92909 (* 1 = 6.92909 loss)
I0523 04:43:26.671423 35003 sgd_solver.cpp:112] Iteration 145180, lr = 0.01
I0523 04:43:30.589879 35003 solver.cpp:239] Iteration 145190 (2.54698 iter/s, 3.92621s/10 iters), loss = 7.13874
I0523 04:43:30.589915 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13874 (* 1 = 7.13874 loss)
I0523 04:43:30.597779 35003 sgd_solver.cpp:112] Iteration 145190, lr = 0.01
I0523 04:43:34.176625 35003 solver.cpp:239] Iteration 145200 (2.78819 iter/s, 3.58656s/10 iters), loss = 5.7705
I0523 04:43:34.176841 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7705 (* 1 = 5.7705 loss)
I0523 04:43:34.184571 35003 sgd_solver.cpp:112] Iteration 145200, lr = 0.01
I0523 04:43:37.712481 35003 solver.cpp:239] Iteration 145210 (2.82844 iter/s, 3.53551s/10 iters), loss = 6.57561
I0523 04:43:37.712532 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57561 (* 1 = 6.57561 loss)
I0523 04:43:37.721998 35003 sgd_solver.cpp:112] Iteration 145210, lr = 0.01
I0523 04:43:40.630969 35003 solver.cpp:239] Iteration 145220 (3.42664 iter/s, 2.91831s/10 iters), loss = 7.04186
I0523 04:43:40.631013 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04186 (* 1 = 7.04186 loss)
I0523 04:43:41.355069 35003 sgd_solver.cpp:112] Iteration 145220, lr = 0.01
I0523 04:43:45.685106 35003 solver.cpp:239] Iteration 145230 (1.97868 iter/s, 5.05389s/10 iters), loss = 7.84172
I0523 04:43:45.685163 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84172 (* 1 = 7.84172 loss)
I0523 04:43:46.396397 35003 sgd_solver.cpp:112] Iteration 145230, lr = 0.01
I0523 04:43:49.612094 35003 solver.cpp:239] Iteration 145240 (2.54663 iter/s, 3.92676s/10 iters), loss = 6.66855
I0523 04:43:49.612146 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66855 (* 1 = 6.66855 loss)
I0523 04:43:49.625074 35003 sgd_solver.cpp:112] Iteration 145240, lr = 0.01
I0523 04:43:53.082691 35003 solver.cpp:239] Iteration 145250 (2.88152 iter/s, 3.47039s/10 iters), loss = 7.09888
I0523 04:43:53.082774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09888 (* 1 = 7.09888 loss)
I0523 04:43:53.092921 35003 sgd_solver.cpp:112] Iteration 145250, lr = 0.01
I0523 04:43:56.667780 35003 solver.cpp:239] Iteration 145260 (2.78951 iter/s, 3.58486s/10 iters), loss = 7.35685
I0523 04:43:56.667817 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35685 (* 1 = 7.35685 loss)
I0523 04:43:56.672044 35003 sgd_solver.cpp:112] Iteration 145260, lr = 0.01
I0523 04:43:59.549145 35003 solver.cpp:239] Iteration 145270 (3.47077 iter/s, 2.8812s/10 iters), loss = 7.77013
I0523 04:43:59.549202 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77013 (* 1 = 7.77013 loss)
I0523 04:44:00.170198 35003 sgd_solver.cpp:112] Iteration 145270, lr = 0.01
I0523 04:44:03.107465 35003 solver.cpp:239] Iteration 145280 (2.81047 iter/s, 3.55812s/10 iters), loss = 6.84436
I0523 04:44:03.107508 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84436 (* 1 = 6.84436 loss)
I0523 04:44:03.846655 35003 sgd_solver.cpp:112] Iteration 145280, lr = 0.01
I0523 04:44:07.001698 35003 solver.cpp:239] Iteration 145290 (2.56805 iter/s, 3.894s/10 iters), loss = 5.6141
I0523 04:44:07.001881 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.6141 (* 1 = 5.6141 loss)
I0523 04:44:07.006659 35003 sgd_solver.cpp:112] Iteration 145290, lr = 0.01
I0523 04:44:10.510203 35003 solver.cpp:239] Iteration 145300 (2.85048 iter/s, 3.50818s/10 iters), loss = 7.89727
I0523 04:44:10.510244 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89727 (* 1 = 7.89727 loss)
I0523 04:44:10.516243 35003 sgd_solver.cpp:112] Iteration 145300, lr = 0.01
I0523 04:44:14.171629 35003 solver.cpp:239] Iteration 145310 (2.73132 iter/s, 3.66123s/10 iters), loss = 6.51611
I0523 04:44:14.171685 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51611 (* 1 = 6.51611 loss)
I0523 04:44:14.182093 35003 sgd_solver.cpp:112] Iteration 145310, lr = 0.01
I0523 04:44:18.986449 35003 solver.cpp:239] Iteration 145320 (2.07703 iter/s, 4.81456s/10 iters), loss = 7.21197
I0523 04:44:18.986498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21197 (* 1 = 7.21197 loss)
I0523 04:44:19.704309 35003 sgd_solver.cpp:112] Iteration 145320, lr = 0.01
I0523 04:44:22.473192 35003 solver.cpp:239] Iteration 145330 (2.86817 iter/s, 3.48655s/10 iters), loss = 7.70461
I0523 04:44:22.473237 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70461 (* 1 = 7.70461 loss)
I0523 04:44:22.485164 35003 sgd_solver.cpp:112] Iteration 145330, lr = 0.01
I0523 04:44:26.617372 35003 solver.cpp:239] Iteration 145340 (2.41315 iter/s, 4.14396s/10 iters), loss = 6.87279
I0523 04:44:26.617421 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87279 (* 1 = 6.87279 loss)
I0523 04:44:26.647972 35003 sgd_solver.cpp:112] Iteration 145340, lr = 0.01
I0523 04:44:30.237246 35003 solver.cpp:239] Iteration 145350 (2.76268 iter/s, 3.61967s/10 iters), loss = 6.40187
I0523 04:44:30.237290 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40187 (* 1 = 6.40187 loss)
I0523 04:44:30.270352 35003 sgd_solver.cpp:112] Iteration 145350, lr = 0.01
I0523 04:44:35.010910 35003 solver.cpp:239] Iteration 145360 (2.09495 iter/s, 4.77339s/10 iters), loss = 6.52877
I0523 04:44:35.010993 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52877 (* 1 = 6.52877 loss)
I0523 04:44:35.034749 35003 sgd_solver.cpp:112] Iteration 145360, lr = 0.01
I0523 04:44:39.883666 35003 solver.cpp:239] Iteration 145370 (2.05234 iter/s, 4.87248s/10 iters), loss = 7.16162
I0523 04:44:39.883913 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16162 (* 1 = 7.16162 loss)
I0523 04:44:39.896834 35003 sgd_solver.cpp:112] Iteration 145370, lr = 0.01
I0523 04:44:42.651818 35003 solver.cpp:239] Iteration 145380 (3.61296 iter/s, 2.76781s/10 iters), loss = 8.08938
I0523 04:44:42.651868 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08938 (* 1 = 8.08938 loss)
I0523 04:44:43.319945 35003 sgd_solver.cpp:112] Iteration 145380, lr = 0.01
I0523 04:44:46.098850 35003 solver.cpp:239] Iteration 145390 (2.90121 iter/s, 3.44684s/10 iters), loss = 7.49206
I0523 04:44:46.098892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49206 (* 1 = 7.49206 loss)
I0523 04:44:46.726547 35003 sgd_solver.cpp:112] Iteration 145390, lr = 0.01
I0523 04:44:50.981281 35003 solver.cpp:239] Iteration 145400 (2.04826 iter/s, 4.88219s/10 iters), loss = 6.2729
I0523 04:44:50.981328 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2729 (* 1 = 6.2729 loss)
I0523 04:44:50.993499 35003 sgd_solver.cpp:112] Iteration 145400, lr = 0.01
I0523 04:44:53.806632 35003 solver.cpp:239] Iteration 145410 (3.53961 iter/s, 2.82517s/10 iters), loss = 6.69644
I0523 04:44:53.806687 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69644 (* 1 = 6.69644 loss)
I0523 04:44:53.820049 35003 sgd_solver.cpp:112] Iteration 145410, lr = 0.01
I0523 04:44:57.270535 35003 solver.cpp:239] Iteration 145420 (2.88709 iter/s, 3.4637s/10 iters), loss = 6.59617
I0523 04:44:57.270582 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59617 (* 1 = 6.59617 loss)
I0523 04:44:57.272119 35003 sgd_solver.cpp:112] Iteration 145420, lr = 0.01
I0523 04:44:58.786389 35003 solver.cpp:239] Iteration 145430 (6.59748 iter/s, 1.51573s/10 iters), loss = 6.94523
I0523 04:44:58.786432 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94523 (* 1 = 6.94523 loss)
I0523 04:44:59.119575 35003 sgd_solver.cpp:112] Iteration 145430, lr = 0.01
I0523 04:45:03.203905 35003 solver.cpp:239] Iteration 145440 (2.26383 iter/s, 4.41729s/10 iters), loss = 6.78149
I0523 04:45:03.203950 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78149 (* 1 = 6.78149 loss)
I0523 04:45:03.925374 35003 sgd_solver.cpp:112] Iteration 145440, lr = 0.01
I0523 04:45:06.687379 35003 solver.cpp:239] Iteration 145450 (2.87086 iter/s, 3.48328s/10 iters), loss = 6.68113
I0523 04:45:06.687427 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68113 (* 1 = 6.68113 loss)
I0523 04:45:06.703955 35003 sgd_solver.cpp:112] Iteration 145450, lr = 0.01
I0523 04:45:10.937772 35003 solver.cpp:239] Iteration 145460 (2.35285 iter/s, 4.25017s/10 iters), loss = 7.14821
I0523 04:45:10.938031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14821 (* 1 = 7.14821 loss)
I0523 04:45:10.955762 35003 sgd_solver.cpp:112] Iteration 145460, lr = 0.01
I0523 04:45:13.853672 35003 solver.cpp:239] Iteration 145470 (3.43485 iter/s, 2.91134s/10 iters), loss = 6.69765
I0523 04:45:13.853713 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69765 (* 1 = 6.69765 loss)
I0523 04:45:14.569159 35003 sgd_solver.cpp:112] Iteration 145470, lr = 0.01
I0523 04:45:17.553757 35003 solver.cpp:239] Iteration 145480 (2.70278 iter/s, 3.69989s/10 iters), loss = 6.40082
I0523 04:45:17.553797 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40082 (* 1 = 6.40082 loss)
I0523 04:45:17.566886 35003 sgd_solver.cpp:112] Iteration 145480, lr = 0.01
I0523 04:45:19.062511 35003 solver.cpp:239] Iteration 145490 (6.62846 iter/s, 1.50865s/10 iters), loss = 8.25308
I0523 04:45:19.062556 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.25308 (* 1 = 8.25308 loss)
I0523 04:45:19.071905 35003 sgd_solver.cpp:112] Iteration 145490, lr = 0.01
I0523 04:45:23.416210 35003 solver.cpp:239] Iteration 145500 (2.29702 iter/s, 4.35347s/10 iters), loss = 6.7016
I0523 04:45:23.416247 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7016 (* 1 = 6.7016 loss)
I0523 04:45:23.429213 35003 sgd_solver.cpp:112] Iteration 145500, lr = 0.01
I0523 04:45:26.466100 35003 solver.cpp:239] Iteration 145510 (3.27899 iter/s, 3.04972s/10 iters), loss = 8.20675
I0523 04:45:26.466142 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.20675 (* 1 = 8.20675 loss)
I0523 04:45:26.494633 35003 sgd_solver.cpp:112] Iteration 145510, lr = 0.01
I0523 04:45:30.887836 35003 solver.cpp:239] Iteration 145520 (2.26167 iter/s, 4.42151s/10 iters), loss = 5.86703
I0523 04:45:30.887881 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.86703 (* 1 = 5.86703 loss)
I0523 04:45:30.904340 35003 sgd_solver.cpp:112] Iteration 145520, lr = 0.01
I0523 04:45:34.320058 35003 solver.cpp:239] Iteration 145530 (2.91373 iter/s, 3.43203s/10 iters), loss = 8.58095
I0523 04:45:34.320111 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.58095 (* 1 = 8.58095 loss)
I0523 04:45:34.333158 35003 sgd_solver.cpp:112] Iteration 145530, lr = 0.01
I0523 04:45:37.032042 35003 solver.cpp:239] Iteration 145540 (3.68756 iter/s, 2.71182s/10 iters), loss = 6.19286
I0523 04:45:37.032088 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19286 (* 1 = 6.19286 loss)
I0523 04:45:37.045614 35003 sgd_solver.cpp:112] Iteration 145540, lr = 0.01
I0523 04:45:42.719542 35003 solver.cpp:239] Iteration 145550 (1.75833 iter/s, 5.68722s/10 iters), loss = 6.47259
I0523 04:45:42.719727 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47259 (* 1 = 6.47259 loss)
I0523 04:45:42.782538 35003 sgd_solver.cpp:112] Iteration 145550, lr = 0.01
I0523 04:45:45.397934 35003 solver.cpp:239] Iteration 145560 (3.73402 iter/s, 2.67808s/10 iters), loss = 6.32426
I0523 04:45:45.397987 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32426 (* 1 = 6.32426 loss)
I0523 04:45:45.982450 35003 sgd_solver.cpp:112] Iteration 145560, lr = 0.01
I0523 04:45:49.824818 35003 solver.cpp:239] Iteration 145570 (2.25904 iter/s, 4.42665s/10 iters), loss = 6.95839
I0523 04:45:49.824857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95839 (* 1 = 6.95839 loss)
I0523 04:45:50.520694 35003 sgd_solver.cpp:112] Iteration 145570, lr = 0.01
I0523 04:45:53.387254 35003 solver.cpp:239] Iteration 145580 (2.80722 iter/s, 3.56225s/10 iters), loss = 7.03749
I0523 04:45:53.387300 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03749 (* 1 = 7.03749 loss)
I0523 04:45:53.399931 35003 sgd_solver.cpp:112] Iteration 145580, lr = 0.01
I0523 04:45:55.502415 35003 solver.cpp:239] Iteration 145590 (4.72809 iter/s, 2.11502s/10 iters), loss = 7.50974
I0523 04:45:55.502460 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50974 (* 1 = 7.50974 loss)
I0523 04:45:55.510354 35003 sgd_solver.cpp:112] Iteration 145590, lr = 0.01
I0523 04:45:59.794272 35003 solver.cpp:239] Iteration 145600 (2.33012 iter/s, 4.29163s/10 iters), loss = 7.85639
I0523 04:45:59.794327 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85639 (* 1 = 7.85639 loss)
I0523 04:46:00.505756 35003 sgd_solver.cpp:112] Iteration 145600, lr = 0.01
I0523 04:46:05.368443 35003 solver.cpp:239] Iteration 145610 (1.79408 iter/s, 5.57389s/10 iters), loss = 6.77471
I0523 04:46:05.368499 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77471 (* 1 = 6.77471 loss)
I0523 04:46:06.109787 35003 sgd_solver.cpp:112] Iteration 145610, lr = 0.01
I0523 04:46:09.393316 35003 solver.cpp:239] Iteration 145620 (2.48469 iter/s, 4.02465s/10 iters), loss = 7.05047
I0523 04:46:09.393373 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05047 (* 1 = 7.05047 loss)
I0523 04:46:09.726313 35003 sgd_solver.cpp:112] Iteration 145620, lr = 0.01
I0523 04:46:11.912421 35003 solver.cpp:239] Iteration 145630 (3.96992 iter/s, 2.51894s/10 iters), loss = 6.21441
I0523 04:46:11.912457 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21441 (* 1 = 6.21441 loss)
I0523 04:46:11.926110 35003 sgd_solver.cpp:112] Iteration 145630, lr = 0.01
I0523 04:46:17.266981 35003 solver.cpp:239] Iteration 145640 (1.86766 iter/s, 5.3543s/10 iters), loss = 8.31904
I0523 04:46:17.267160 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.31904 (* 1 = 8.31904 loss)
I0523 04:46:17.284873 35003 sgd_solver.cpp:112] Iteration 145640, lr = 0.01
I0523 04:46:20.715152 35003 solver.cpp:239] Iteration 145650 (2.90035 iter/s, 3.44785s/10 iters), loss = 5.76756
I0523 04:46:20.715205 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.76756 (* 1 = 5.76756 loss)
I0523 04:46:20.721069 35003 sgd_solver.cpp:112] Iteration 145650, lr = 0.01
I0523 04:46:24.524922 35003 solver.cpp:239] Iteration 145660 (2.62497 iter/s, 3.80956s/10 iters), loss = 7.02067
I0523 04:46:24.524966 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02067 (* 1 = 7.02067 loss)
I0523 04:46:24.537906 35003 sgd_solver.cpp:112] Iteration 145660, lr = 0.01
I0523 04:46:29.009403 35003 solver.cpp:239] Iteration 145670 (2.23002 iter/s, 4.48426s/10 iters), loss = 6.45061
I0523 04:46:29.009450 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45061 (* 1 = 6.45061 loss)
I0523 04:46:29.033614 35003 sgd_solver.cpp:112] Iteration 145670, lr = 0.01
I0523 04:46:32.562131 35003 solver.cpp:239] Iteration 145680 (2.81489 iter/s, 3.55253s/10 iters), loss = 6.2904
I0523 04:46:32.562183 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2904 (* 1 = 6.2904 loss)
I0523 04:46:32.573864 35003 sgd_solver.cpp:112] Iteration 145680, lr = 0.01
I0523 04:46:38.327113 35003 solver.cpp:239] Iteration 145690 (1.7347 iter/s, 5.76469s/10 iters), loss = 7.9166
I0523 04:46:38.327173 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9166 (* 1 = 7.9166 loss)
I0523 04:46:38.345552 35003 sgd_solver.cpp:112] Iteration 145690, lr = 0.01
I0523 04:46:40.118113 35003 solver.cpp:239] Iteration 145700 (5.5839 iter/s, 1.79086s/10 iters), loss = 7.16611
I0523 04:46:40.118160 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16611 (* 1 = 7.16611 loss)
I0523 04:46:40.129556 35003 sgd_solver.cpp:112] Iteration 145700, lr = 0.01
I0523 04:46:40.955853 35003 solver.cpp:239] Iteration 145710 (11.9382 iter/s, 0.83765s/10 iters), loss = 8.38201
I0523 04:46:40.955893 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.38201 (* 1 = 8.38201 loss)
I0523 04:46:40.967908 35003 sgd_solver.cpp:112] Iteration 145710, lr = 0.01
I0523 04:46:41.798393 35003 solver.cpp:239] Iteration 145720 (11.8701 iter/s, 0.842455s/10 iters), loss = 6.86526
I0523 04:46:41.798439 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86526 (* 1 = 6.86526 loss)
I0523 04:46:41.808647 35003 sgd_solver.cpp:112] Iteration 145720, lr = 0.01
I0523 04:46:42.652973 35003 solver.cpp:239] Iteration 145730 (11.7029 iter/s, 0.854488s/10 iters), loss = 7.35752
I0523 04:46:42.653023 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35752 (* 1 = 7.35752 loss)
I0523 04:46:42.657781 35003 sgd_solver.cpp:112] Iteration 145730, lr = 0.01
I0523 04:46:43.502355 35003 solver.cpp:239] Iteration 145740 (11.7745 iter/s, 0.849291s/10 iters), loss = 5.90876
I0523 04:46:43.502394 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.90876 (* 1 = 5.90876 loss)
I0523 04:46:43.508643 35003 sgd_solver.cpp:112] Iteration 145740, lr = 0.01
I0523 04:46:44.347517 35003 solver.cpp:239] Iteration 145750 (11.8334 iter/s, 0.845068s/10 iters), loss = 7.20486
I0523 04:46:44.347569 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20486 (* 1 = 7.20486 loss)
I0523 04:46:44.348356 35003 sgd_solver.cpp:112] Iteration 145750, lr = 0.01
I0523 04:46:45.375412 35003 solver.cpp:239] Iteration 145760 (9.72958 iter/s, 1.02779s/10 iters), loss = 6.51177
I0523 04:46:45.375459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51177 (* 1 = 6.51177 loss)
I0523 04:46:45.387485 35003 sgd_solver.cpp:112] Iteration 145760, lr = 0.01
I0523 04:46:46.217667 35003 solver.cpp:239] Iteration 145770 (11.8742 iter/s, 0.84216s/10 iters), loss = 7.57477
I0523 04:46:46.217730 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57477 (* 1 = 7.57477 loss)
I0523 04:46:46.221828 35003 sgd_solver.cpp:112] Iteration 145770, lr = 0.01
I0523 04:46:47.058722 35003 solver.cpp:239] Iteration 145780 (11.8918 iter/s, 0.840916s/10 iters), loss = 7.53427
I0523 04:46:47.058773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53427 (* 1 = 7.53427 loss)
I0523 04:46:47.067273 35003 sgd_solver.cpp:112] Iteration 145780, lr = 0.01
I0523 04:46:48.112835 35003 solver.cpp:239] Iteration 145790 (9.48755 iter/s, 1.05401s/10 iters), loss = 6.61847
I0523 04:46:48.113122 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61847 (* 1 = 6.61847 loss)
I0523 04:46:48.124850 35003 sgd_solver.cpp:112] Iteration 145790, lr = 0.01
I0523 04:46:49.297227 35003 solver.cpp:239] Iteration 145800 (8.46155 iter/s, 1.18182s/10 iters), loss = 7.48704
I0523 04:46:49.297286 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48704 (* 1 = 7.48704 loss)
I0523 04:46:49.307097 35003 sgd_solver.cpp:112] Iteration 145800, lr = 0.01
I0523 04:46:50.956779 35003 solver.cpp:239] Iteration 145810 (6.0262 iter/s, 1.65942s/10 iters), loss = 6.93108
I0523 04:46:50.956818 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93108 (* 1 = 6.93108 loss)
I0523 04:46:50.962765 35003 sgd_solver.cpp:112] Iteration 145810, lr = 0.01
I0523 04:46:53.063719 35003 solver.cpp:239] Iteration 145820 (4.74654 iter/s, 2.1068s/10 iters), loss = 7.37258
I0523 04:46:53.063773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37258 (* 1 = 7.37258 loss)
I0523 04:46:53.078327 35003 sgd_solver.cpp:112] Iteration 145820, lr = 0.01
I0523 04:46:56.554095 35003 solver.cpp:239] Iteration 145830 (2.86518 iter/s, 3.49018s/10 iters), loss = 6.78451
I0523 04:46:56.554133 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78451 (* 1 = 6.78451 loss)
I0523 04:46:56.567230 35003 sgd_solver.cpp:112] Iteration 145830, lr = 0.01
I0523 04:47:00.220448 35003 solver.cpp:239] Iteration 145840 (2.72765 iter/s, 3.66616s/10 iters), loss = 6.00162
I0523 04:47:00.220501 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00162 (* 1 = 6.00162 loss)
I0523 04:47:00.227455 35003 sgd_solver.cpp:112] Iteration 145840, lr = 0.01
I0523 04:47:03.731534 35003 solver.cpp:239] Iteration 145850 (2.84828 iter/s, 3.51089s/10 iters), loss = 6.36021
I0523 04:47:03.731590 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36021 (* 1 = 6.36021 loss)
I0523 04:47:03.739084 35003 sgd_solver.cpp:112] Iteration 145850, lr = 0.01
I0523 04:47:06.596069 35003 solver.cpp:239] Iteration 145860 (3.49119 iter/s, 2.86436s/10 iters), loss = 6.4747
I0523 04:47:06.596119 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4747 (* 1 = 6.4747 loss)
I0523 04:47:06.613277 35003 sgd_solver.cpp:112] Iteration 145860, lr = 0.01
I0523 04:47:10.655200 35003 solver.cpp:239] Iteration 145870 (2.46371 iter/s, 4.05891s/10 iters), loss = 6.68217
I0523 04:47:10.655246 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68217 (* 1 = 6.68217 loss)
I0523 04:47:11.379837 35003 sgd_solver.cpp:112] Iteration 145870, lr = 0.01
I0523 04:47:14.735141 35003 solver.cpp:239] Iteration 145880 (2.45115 iter/s, 4.07972s/10 iters), loss = 7.16987
I0523 04:47:14.735188 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16987 (* 1 = 7.16987 loss)
I0523 04:47:14.746043 35003 sgd_solver.cpp:112] Iteration 145880, lr = 0.01
I0523 04:47:17.642746 35003 solver.cpp:239] Iteration 145890 (3.43946 iter/s, 2.90744s/10 iters), loss = 6.85541
I0523 04:47:17.642791 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85541 (* 1 = 6.85541 loss)
I0523 04:47:17.652169 35003 sgd_solver.cpp:112] Iteration 145890, lr = 0.01
I0523 04:47:20.905022 35003 solver.cpp:239] Iteration 145900 (3.06551 iter/s, 3.2621s/10 iters), loss = 6.4411
I0523 04:47:20.905304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4411 (* 1 = 6.4411 loss)
I0523 04:47:21.497658 35003 sgd_solver.cpp:112] Iteration 145900, lr = 0.01
I0523 04:47:24.914891 35003 solver.cpp:239] Iteration 145910 (2.49411 iter/s, 4.00945s/10 iters), loss = 7.18107
I0523 04:47:24.914942 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18107 (* 1 = 7.18107 loss)
I0523 04:47:24.920276 35003 sgd_solver.cpp:112] Iteration 145910, lr = 0.01
I0523 04:47:29.397622 35003 solver.cpp:239] Iteration 145920 (2.2309 iter/s, 4.4825s/10 iters), loss = 6.59968
I0523 04:47:29.397660 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59968 (* 1 = 6.59968 loss)
I0523 04:47:29.403982 35003 sgd_solver.cpp:112] Iteration 145920, lr = 0.01
I0523 04:47:33.027349 35003 solver.cpp:239] Iteration 145930 (2.75517 iter/s, 3.62954s/10 iters), loss = 6.29942
I0523 04:47:33.027396 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29942 (* 1 = 6.29942 loss)
I0523 04:47:33.749018 35003 sgd_solver.cpp:112] Iteration 145930, lr = 0.01
I0523 04:47:37.227804 35003 solver.cpp:239] Iteration 145940 (2.38082 iter/s, 4.20023s/10 iters), loss = 6.597
I0523 04:47:37.227849 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.597 (* 1 = 6.597 loss)
I0523 04:47:37.236367 35003 sgd_solver.cpp:112] Iteration 145940, lr = 0.01
I0523 04:47:40.154027 35003 solver.cpp:239] Iteration 145950 (3.41757 iter/s, 2.92605s/10 iters), loss = 7.24704
I0523 04:47:40.154070 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24704 (* 1 = 7.24704 loss)
I0523 04:47:40.167640 35003 sgd_solver.cpp:112] Iteration 145950, lr = 0.01
I0523 04:47:43.134140 35003 solver.cpp:239] Iteration 145960 (3.35577 iter/s, 2.97994s/10 iters), loss = 7.77681
I0523 04:47:43.134179 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77681 (* 1 = 7.77681 loss)
I0523 04:47:43.849536 35003 sgd_solver.cpp:112] Iteration 145960, lr = 0.01
I0523 04:47:47.993863 35003 solver.cpp:239] Iteration 145970 (2.05783 iter/s, 4.85948s/10 iters), loss = 6.59901
I0523 04:47:47.993908 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59901 (* 1 = 6.59901 loss)
I0523 04:47:48.017603 35003 sgd_solver.cpp:112] Iteration 145970, lr = 0.01
I0523 04:47:51.870963 35003 solver.cpp:239] Iteration 145980 (2.57939 iter/s, 3.87689s/10 iters), loss = 6.89229
I0523 04:47:51.871279 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89229 (* 1 = 6.89229 loss)
I0523 04:47:51.889509 35003 sgd_solver.cpp:112] Iteration 145980, lr = 0.01
I0523 04:47:56.405472 35003 solver.cpp:239] Iteration 145990 (2.20554 iter/s, 4.53403s/10 iters), loss = 6.64216
I0523 04:47:56.405539 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64216 (* 1 = 6.64216 loss)
I0523 04:47:56.462744 35003 sgd_solver.cpp:112] Iteration 145990, lr = 0.01
I0523 04:47:59.862553 35003 solver.cpp:239] Iteration 146000 (2.89278 iter/s, 3.45688s/10 iters), loss = 7.91516
I0523 04:47:59.862593 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91516 (* 1 = 7.91516 loss)
I0523 04:47:59.875777 35003 sgd_solver.cpp:112] Iteration 146000, lr = 0.01
I0523 04:48:03.428285 35003 solver.cpp:239] Iteration 146010 (2.80463 iter/s, 3.56554s/10 iters), loss = 7.09293
I0523 04:48:03.428351 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09293 (* 1 = 7.09293 loss)
I0523 04:48:04.124387 35003 sgd_solver.cpp:112] Iteration 146010, lr = 0.01
I0523 04:48:09.260078 35003 solver.cpp:239] Iteration 146020 (1.71483 iter/s, 5.83149s/10 iters), loss = 6.68786
I0523 04:48:09.260138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68786 (* 1 = 6.68786 loss)
I0523 04:48:09.273057 35003 sgd_solver.cpp:112] Iteration 146020, lr = 0.01
I0523 04:48:12.128705 35003 solver.cpp:239] Iteration 146030 (3.48621 iter/s, 2.86845s/10 iters), loss = 7.02896
I0523 04:48:12.128752 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02896 (* 1 = 7.02896 loss)
I0523 04:48:12.142537 35003 sgd_solver.cpp:112] Iteration 146030, lr = 0.01
I0523 04:48:16.254962 35003 solver.cpp:239] Iteration 146040 (2.42363 iter/s, 4.12604s/10 iters), loss = 6.41869
I0523 04:48:16.255007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41869 (* 1 = 6.41869 loss)
I0523 04:48:16.277051 35003 sgd_solver.cpp:112] Iteration 146040, lr = 0.01
I0523 04:48:21.338660 35003 solver.cpp:239] Iteration 146050 (1.96717 iter/s, 5.08344s/10 iters), loss = 5.79688
I0523 04:48:21.338759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.79688 (* 1 = 5.79688 loss)
I0523 04:48:21.950863 35003 sgd_solver.cpp:112] Iteration 146050, lr = 0.01
I0523 04:48:25.618993 35003 solver.cpp:239] Iteration 146060 (2.33642 iter/s, 4.28005s/10 iters), loss = 7.62851
I0523 04:48:25.619047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62851 (* 1 = 7.62851 loss)
I0523 04:48:26.341094 35003 sgd_solver.cpp:112] Iteration 146060, lr = 0.01
I0523 04:48:30.175832 35003 solver.cpp:239] Iteration 146070 (2.19462 iter/s, 4.5566s/10 iters), loss = 6.02624
I0523 04:48:30.175876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02624 (* 1 = 6.02624 loss)
I0523 04:48:30.178685 35003 sgd_solver.cpp:112] Iteration 146070, lr = 0.01
I0523 04:48:32.227499 35003 solver.cpp:239] Iteration 146080 (4.87441 iter/s, 2.05153s/10 iters), loss = 7.15538
I0523 04:48:32.227540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15538 (* 1 = 7.15538 loss)
I0523 04:48:32.924100 35003 sgd_solver.cpp:112] Iteration 146080, lr = 0.01
I0523 04:48:36.445462 35003 solver.cpp:239] Iteration 146090 (2.37093 iter/s, 4.21775s/10 iters), loss = 5.76702
I0523 04:48:36.445511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.76702 (* 1 = 5.76702 loss)
I0523 04:48:36.458828 35003 sgd_solver.cpp:112] Iteration 146090, lr = 0.01
I0523 04:48:38.659401 35003 solver.cpp:239] Iteration 146100 (4.51714 iter/s, 2.21379s/10 iters), loss = 6.70593
I0523 04:48:38.659447 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70593 (* 1 = 6.70593 loss)
I0523 04:48:38.668643 35003 sgd_solver.cpp:112] Iteration 146100, lr = 0.01
I0523 04:48:41.631198 35003 solver.cpp:239] Iteration 146110 (3.36517 iter/s, 2.97162s/10 iters), loss = 6.63715
I0523 04:48:41.631249 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63715 (* 1 = 6.63715 loss)
I0523 04:48:41.656889 35003 sgd_solver.cpp:112] Iteration 146110, lr = 0.01
I0523 04:48:44.477588 35003 solver.cpp:239] Iteration 146120 (3.51345 iter/s, 2.84621s/10 iters), loss = 5.68126
I0523 04:48:44.477653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.68126 (* 1 = 5.68126 loss)
I0523 04:48:44.915532 35003 sgd_solver.cpp:112] Iteration 146120, lr = 0.01
I0523 04:48:47.567803 35003 solver.cpp:239] Iteration 146130 (3.23622 iter/s, 3.09002s/10 iters), loss = 7.56318
I0523 04:48:47.567852 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56318 (* 1 = 7.56318 loss)
I0523 04:48:48.308701 35003 sgd_solver.cpp:112] Iteration 146130, lr = 0.01
I0523 04:48:52.129874 35003 solver.cpp:239] Iteration 146140 (2.1921 iter/s, 4.56184s/10 iters), loss = 7.7775
I0523 04:48:52.130089 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7775 (* 1 = 7.7775 loss)
I0523 04:48:52.154153 35003 sgd_solver.cpp:112] Iteration 146140, lr = 0.01
I0523 04:48:55.174293 35003 solver.cpp:239] Iteration 146150 (3.28507 iter/s, 3.04408s/10 iters), loss = 6.27814
I0523 04:48:55.174331 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27814 (* 1 = 6.27814 loss)
I0523 04:48:55.187434 35003 sgd_solver.cpp:112] Iteration 146150, lr = 0.01
I0523 04:48:59.106583 35003 solver.cpp:239] Iteration 146160 (2.54318 iter/s, 3.93209s/10 iters), loss = 6.4988
I0523 04:48:59.106623 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4988 (* 1 = 6.4988 loss)
I0523 04:48:59.115137 35003 sgd_solver.cpp:112] Iteration 146160, lr = 0.01
I0523 04:49:04.150861 35003 solver.cpp:239] Iteration 146170 (1.98254 iter/s, 5.04402s/10 iters), loss = 7.32075
I0523 04:49:04.150918 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32075 (* 1 = 7.32075 loss)
I0523 04:49:04.156941 35003 sgd_solver.cpp:112] Iteration 146170, lr = 0.01
I0523 04:49:06.895925 35003 solver.cpp:239] Iteration 146180 (3.64313 iter/s, 2.74489s/10 iters), loss = 6.64424
I0523 04:49:06.895972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64424 (* 1 = 6.64424 loss)
I0523 04:49:07.631129 35003 sgd_solver.cpp:112] Iteration 146180, lr = 0.01
I0523 04:49:12.212605 35003 solver.cpp:239] Iteration 146190 (1.88097 iter/s, 5.31642s/10 iters), loss = 7.37193
I0523 04:49:12.212646 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37193 (* 1 = 7.37193 loss)
I0523 04:49:12.218016 35003 sgd_solver.cpp:112] Iteration 146190, lr = 0.01
I0523 04:49:16.036447 35003 solver.cpp:239] Iteration 146200 (2.61531 iter/s, 3.82364s/10 iters), loss = 6.2778
I0523 04:49:16.036485 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2778 (* 1 = 6.2778 loss)
I0523 04:49:16.049589 35003 sgd_solver.cpp:112] Iteration 146200, lr = 0.01
I0523 04:49:18.656710 35003 solver.cpp:239] Iteration 146210 (3.81663 iter/s, 2.62011s/10 iters), loss = 7.27112
I0523 04:49:18.656756 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27112 (* 1 = 7.27112 loss)
I0523 04:49:18.664407 35003 sgd_solver.cpp:112] Iteration 146210, lr = 0.01
I0523 04:49:21.390048 35003 solver.cpp:239] Iteration 146220 (3.65875 iter/s, 2.73317s/10 iters), loss = 8.03992
I0523 04:49:21.390085 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03992 (* 1 = 8.03992 loss)
I0523 04:49:21.394592 35003 sgd_solver.cpp:112] Iteration 146220, lr = 0.01
I0523 04:49:25.090951 35003 solver.cpp:239] Iteration 146230 (2.70219 iter/s, 3.70071s/10 iters), loss = 6.96443
I0523 04:49:25.091202 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96443 (* 1 = 6.96443 loss)
I0523 04:49:25.111166 35003 sgd_solver.cpp:112] Iteration 146230, lr = 0.01
I0523 04:49:28.169121 35003 solver.cpp:239] Iteration 146240 (3.24911 iter/s, 3.07777s/10 iters), loss = 6.5594
I0523 04:49:28.169168 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5594 (* 1 = 6.5594 loss)
I0523 04:49:28.763581 35003 sgd_solver.cpp:112] Iteration 146240, lr = 0.01
I0523 04:49:31.556015 35003 solver.cpp:239] Iteration 146250 (2.95272 iter/s, 3.3867s/10 iters), loss = 7.05223
I0523 04:49:31.556066 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05223 (* 1 = 7.05223 loss)
I0523 04:49:31.567790 35003 sgd_solver.cpp:112] Iteration 146250, lr = 0.01
I0523 04:49:35.228247 35003 solver.cpp:239] Iteration 146260 (2.72329 iter/s, 3.67203s/10 iters), loss = 6.89509
I0523 04:49:35.228291 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89509 (* 1 = 6.89509 loss)
I0523 04:49:35.241634 35003 sgd_solver.cpp:112] Iteration 146260, lr = 0.01
I0523 04:49:37.213940 35003 solver.cpp:239] Iteration 146270 (5.03636 iter/s, 1.98556s/10 iters), loss = 7.40049
I0523 04:49:37.213999 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40049 (* 1 = 7.40049 loss)
I0523 04:49:37.848011 35003 sgd_solver.cpp:112] Iteration 146270, lr = 0.01
I0523 04:49:41.309577 35003 solver.cpp:239] Iteration 146280 (2.44176 iter/s, 4.09541s/10 iters), loss = 7.82131
I0523 04:49:41.309631 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82131 (* 1 = 7.82131 loss)
I0523 04:49:42.030640 35003 sgd_solver.cpp:112] Iteration 146280, lr = 0.01
I0523 04:49:43.550294 35003 solver.cpp:239] Iteration 146290 (4.46316 iter/s, 2.24057s/10 iters), loss = 7.24253
I0523 04:49:43.550338 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24253 (* 1 = 7.24253 loss)
I0523 04:49:44.244001 35003 sgd_solver.cpp:112] Iteration 146290, lr = 0.01
I0523 04:49:48.396853 35003 solver.cpp:239] Iteration 146300 (2.06342 iter/s, 4.84632s/10 iters), loss = 6.46238
I0523 04:49:48.396894 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46238 (* 1 = 6.46238 loss)
I0523 04:49:48.403050 35003 sgd_solver.cpp:112] Iteration 146300, lr = 0.01
I0523 04:49:52.551374 35003 solver.cpp:239] Iteration 146310 (2.40715 iter/s, 4.15428s/10 iters), loss = 6.93804
I0523 04:49:52.551429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93804 (* 1 = 6.93804 loss)
I0523 04:49:52.557127 35003 sgd_solver.cpp:112] Iteration 146310, lr = 0.01
I0523 04:49:56.269279 35003 solver.cpp:239] Iteration 146320 (2.68984 iter/s, 3.71769s/10 iters), loss = 8.44395
I0523 04:49:56.269441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.44395 (* 1 = 8.44395 loss)
I0523 04:49:56.272039 35003 sgd_solver.cpp:112] Iteration 146320, lr = 0.01
I0523 04:50:01.324923 35003 solver.cpp:239] Iteration 146330 (1.97813 iter/s, 5.05529s/10 iters), loss = 7.63307
I0523 04:50:01.324959 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63307 (* 1 = 7.63307 loss)
I0523 04:50:01.337455 35003 sgd_solver.cpp:112] Iteration 146330, lr = 0.01
I0523 04:50:04.812649 35003 solver.cpp:239] Iteration 146340 (2.86735 iter/s, 3.48754s/10 iters), loss = 6.33555
I0523 04:50:04.812690 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33555 (* 1 = 6.33555 loss)
I0523 04:50:04.825829 35003 sgd_solver.cpp:112] Iteration 146340, lr = 0.01
I0523 04:50:07.275251 35003 solver.cpp:239] Iteration 146350 (4.06099 iter/s, 2.46245s/10 iters), loss = 7.59649
I0523 04:50:07.275306 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59649 (* 1 = 7.59649 loss)
I0523 04:50:08.016199 35003 sgd_solver.cpp:112] Iteration 146350, lr = 0.01
I0523 04:50:12.320487 35003 solver.cpp:239] Iteration 146360 (1.98217 iter/s, 5.04497s/10 iters), loss = 7.66628
I0523 04:50:12.320545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66628 (* 1 = 7.66628 loss)
I0523 04:50:13.061491 35003 sgd_solver.cpp:112] Iteration 146360, lr = 0.01
I0523 04:50:16.318166 35003 solver.cpp:239] Iteration 146370 (2.50159 iter/s, 3.99745s/10 iters), loss = 7.36676
I0523 04:50:16.318233 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36676 (* 1 = 7.36676 loss)
I0523 04:50:17.005846 35003 sgd_solver.cpp:112] Iteration 146370, lr = 0.01
I0523 04:50:18.346840 35003 solver.cpp:239] Iteration 146380 (4.92969 iter/s, 2.02853s/10 iters), loss = 7.37371
I0523 04:50:18.346892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37371 (* 1 = 7.37371 loss)
I0523 04:50:19.081845 35003 sgd_solver.cpp:112] Iteration 146380, lr = 0.01
I0523 04:50:24.225209 35003 solver.cpp:239] Iteration 146390 (1.70124 iter/s, 5.87806s/10 iters), loss = 6.8714
I0523 04:50:24.225293 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8714 (* 1 = 6.8714 loss)
I0523 04:50:24.228196 35003 sgd_solver.cpp:112] Iteration 146390, lr = 0.01
I0523 04:50:27.816447 35003 solver.cpp:239] Iteration 146400 (2.78475 iter/s, 3.59098s/10 iters), loss = 6.16125
I0523 04:50:27.816745 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16125 (* 1 = 6.16125 loss)
I0523 04:50:28.545231 35003 sgd_solver.cpp:112] Iteration 146400, lr = 0.01
I0523 04:50:29.907384 35003 solver.cpp:239] Iteration 146410 (4.78337 iter/s, 2.09058s/10 iters), loss = 6.20784
I0523 04:50:29.907438 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20784 (* 1 = 6.20784 loss)
I0523 04:50:30.614217 35003 sgd_solver.cpp:112] Iteration 146410, lr = 0.01
I0523 04:50:33.552455 35003 solver.cpp:239] Iteration 146420 (2.74359 iter/s, 3.64486s/10 iters), loss = 5.57879
I0523 04:50:33.552516 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.57879 (* 1 = 5.57879 loss)
I0523 04:50:33.670991 35003 sgd_solver.cpp:112] Iteration 146420, lr = 0.01
I0523 04:50:36.010897 35003 solver.cpp:239] Iteration 146430 (4.0679 iter/s, 2.45827s/10 iters), loss = 6.6406
I0523 04:50:36.010957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6406 (* 1 = 6.6406 loss)
I0523 04:50:36.014909 35003 sgd_solver.cpp:112] Iteration 146430, lr = 0.01
I0523 04:50:38.087633 35003 solver.cpp:239] Iteration 146440 (4.81559 iter/s, 2.07659s/10 iters), loss = 7.88518
I0523 04:50:38.087689 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88518 (* 1 = 7.88518 loss)
I0523 04:50:38.095535 35003 sgd_solver.cpp:112] Iteration 146440, lr = 0.01
I0523 04:50:41.633074 35003 solver.cpp:239] Iteration 146450 (2.82069 iter/s, 3.54523s/10 iters), loss = 7.1145
I0523 04:50:41.633146 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1145 (* 1 = 7.1145 loss)
I0523 04:50:41.636701 35003 sgd_solver.cpp:112] Iteration 146450, lr = 0.01
I0523 04:50:44.749001 35003 solver.cpp:239] Iteration 146460 (3.20952 iter/s, 3.11573s/10 iters), loss = 6.88278
I0523 04:50:44.749047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88278 (* 1 = 6.88278 loss)
I0523 04:50:45.488241 35003 sgd_solver.cpp:112] Iteration 146460, lr = 0.01
I0523 04:50:48.092952 35003 solver.cpp:239] Iteration 146470 (2.99064 iter/s, 3.34376s/10 iters), loss = 6.94314
I0523 04:50:48.092990 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94314 (* 1 = 6.94314 loss)
I0523 04:50:48.106568 35003 sgd_solver.cpp:112] Iteration 146470, lr = 0.01
I0523 04:50:51.837625 35003 solver.cpp:239] Iteration 146480 (2.6706 iter/s, 3.74447s/10 iters), loss = 7.87393
I0523 04:50:51.837666 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87393 (* 1 = 7.87393 loss)
I0523 04:50:51.845597 35003 sgd_solver.cpp:112] Iteration 146480, lr = 0.01
I0523 04:50:54.655378 35003 solver.cpp:239] Iteration 146490 (3.54913 iter/s, 2.8176s/10 iters), loss = 7.72597
I0523 04:50:54.655421 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72597 (* 1 = 7.72597 loss)
I0523 04:50:54.658814 35003 sgd_solver.cpp:112] Iteration 146490, lr = 0.01
I0523 04:50:58.251221 35003 solver.cpp:239] Iteration 146500 (2.78115 iter/s, 3.59564s/10 iters), loss = 7.88877
I0523 04:50:58.251426 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88877 (* 1 = 7.88877 loss)
I0523 04:50:58.263811 35003 sgd_solver.cpp:112] Iteration 146500, lr = 0.01
I0523 04:51:01.124616 35003 solver.cpp:239] Iteration 146510 (3.4806 iter/s, 2.87307s/10 iters), loss = 6.58699
I0523 04:51:01.124655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58699 (* 1 = 6.58699 loss)
I0523 04:51:01.132838 35003 sgd_solver.cpp:112] Iteration 146510, lr = 0.01
I0523 04:51:03.353452 35003 solver.cpp:239] Iteration 146520 (4.48692 iter/s, 2.2287s/10 iters), loss = 6.84085
I0523 04:51:03.353488 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84085 (* 1 = 6.84085 loss)
I0523 04:51:03.371784 35003 sgd_solver.cpp:112] Iteration 146520, lr = 0.01
I0523 04:51:05.985023 35003 solver.cpp:239] Iteration 146530 (3.80024 iter/s, 2.63141s/10 iters), loss = 6.94704
I0523 04:51:05.985072 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94704 (* 1 = 6.94704 loss)
I0523 04:51:05.998283 35003 sgd_solver.cpp:112] Iteration 146530, lr = 0.01
I0523 04:51:08.238627 35003 solver.cpp:239] Iteration 146540 (4.43763 iter/s, 2.25346s/10 iters), loss = 7.49501
I0523 04:51:08.238667 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49501 (* 1 = 7.49501 loss)
I0523 04:51:08.244042 35003 sgd_solver.cpp:112] Iteration 146540, lr = 0.01
I0523 04:51:09.565325 35003 solver.cpp:239] Iteration 146550 (7.53809 iter/s, 1.3266s/10 iters), loss = 6.17744
I0523 04:51:09.565362 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17744 (* 1 = 6.17744 loss)
I0523 04:51:09.588981 35003 sgd_solver.cpp:112] Iteration 146550, lr = 0.01
I0523 04:51:13.071455 35003 solver.cpp:239] Iteration 146560 (2.8523 iter/s, 3.50594s/10 iters), loss = 6.66744
I0523 04:51:13.071504 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66744 (* 1 = 6.66744 loss)
I0523 04:51:13.754384 35003 sgd_solver.cpp:112] Iteration 146560, lr = 0.01
I0523 04:51:16.889164 35003 solver.cpp:239] Iteration 146570 (2.61952 iter/s, 3.8175s/10 iters), loss = 6.8217
I0523 04:51:16.889223 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8217 (* 1 = 6.8217 loss)
I0523 04:51:16.897030 35003 sgd_solver.cpp:112] Iteration 146570, lr = 0.01
I0523 04:51:20.503723 35003 solver.cpp:239] Iteration 146580 (2.76675 iter/s, 3.61435s/10 iters), loss = 6.46753
I0523 04:51:20.503769 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46753 (* 1 = 6.46753 loss)
I0523 04:51:21.245168 35003 sgd_solver.cpp:112] Iteration 146580, lr = 0.01
I0523 04:51:22.527951 35003 solver.cpp:239] Iteration 146590 (4.94051 iter/s, 2.02408s/10 iters), loss = 7.68218
I0523 04:51:22.527989 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68218 (* 1 = 7.68218 loss)
I0523 04:51:22.546133 35003 sgd_solver.cpp:112] Iteration 146590, lr = 0.01
I0523 04:51:25.322808 35003 solver.cpp:239] Iteration 146600 (3.57821 iter/s, 2.7947s/10 iters), loss = 6.46484
I0523 04:51:25.322855 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46484 (* 1 = 6.46484 loss)
I0523 04:51:25.330121 35003 sgd_solver.cpp:112] Iteration 146600, lr = 0.01
I0523 04:51:28.125386 35003 solver.cpp:239] Iteration 146610 (3.56836 iter/s, 2.8024s/10 iters), loss = 7.06344
I0523 04:51:28.125445 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06344 (* 1 = 7.06344 loss)
I0523 04:51:28.144297 35003 sgd_solver.cpp:112] Iteration 146610, lr = 0.01
I0523 04:51:30.875003 35003 solver.cpp:239] Iteration 146620 (3.63709 iter/s, 2.74945s/10 iters), loss = 6.12644
I0523 04:51:30.875254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12644 (* 1 = 6.12644 loss)
I0523 04:51:30.888849 35003 sgd_solver.cpp:112] Iteration 146620, lr = 0.01
I0523 04:51:34.791851 35003 solver.cpp:239] Iteration 146630 (2.55333 iter/s, 3.91645s/10 iters), loss = 5.55258
I0523 04:51:34.791889 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.55258 (* 1 = 5.55258 loss)
I0523 04:51:34.798925 35003 sgd_solver.cpp:112] Iteration 146630, lr = 0.01
I0523 04:51:38.477730 35003 solver.cpp:239] Iteration 146640 (2.7132 iter/s, 3.68568s/10 iters), loss = 7.80044
I0523 04:51:38.477775 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80044 (* 1 = 7.80044 loss)
I0523 04:51:39.097275 35003 sgd_solver.cpp:112] Iteration 146640, lr = 0.01
I0523 04:51:43.810266 35003 solver.cpp:239] Iteration 146650 (1.87537 iter/s, 5.33227s/10 iters), loss = 7.56273
I0523 04:51:43.810322 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56273 (* 1 = 7.56273 loss)
I0523 04:51:43.835184 35003 sgd_solver.cpp:112] Iteration 146650, lr = 0.01
I0523 04:51:49.660753 35003 solver.cpp:239] Iteration 146660 (1.70934 iter/s, 5.8502s/10 iters), loss = 8.99729
I0523 04:51:49.660797 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.99729 (* 1 = 8.99729 loss)
I0523 04:51:50.376147 35003 sgd_solver.cpp:112] Iteration 146660, lr = 0.01
I0523 04:51:53.428131 35003 solver.cpp:239] Iteration 146670 (2.65453 iter/s, 3.76714s/10 iters), loss = 8.32636
I0523 04:51:53.428182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.32636 (* 1 = 8.32636 loss)
I0523 04:51:53.469004 35003 sgd_solver.cpp:112] Iteration 146670, lr = 0.01
I0523 04:51:58.249840 35003 solver.cpp:239] Iteration 146680 (2.07406 iter/s, 4.82146s/10 iters), loss = 6.83353
I0523 04:51:58.249893 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83353 (* 1 = 6.83353 loss)
I0523 04:51:58.964262 35003 sgd_solver.cpp:112] Iteration 146680, lr = 0.01
I0523 04:52:02.535938 35003 solver.cpp:239] Iteration 146690 (2.33325 iter/s, 4.28587s/10 iters), loss = 6.60912
I0523 04:52:02.536147 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60912 (* 1 = 6.60912 loss)
I0523 04:52:02.558354 35003 sgd_solver.cpp:112] Iteration 146690, lr = 0.01
I0523 04:52:04.770009 35003 solver.cpp:239] Iteration 146700 (4.4767 iter/s, 2.23379s/10 iters), loss = 7.65773
I0523 04:52:04.770059 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65773 (* 1 = 7.65773 loss)
I0523 04:52:05.478338 35003 sgd_solver.cpp:112] Iteration 146700, lr = 0.01
I0523 04:52:08.968942 35003 solver.cpp:239] Iteration 146710 (2.38168 iter/s, 4.19871s/10 iters), loss = 7.10305
I0523 04:52:08.968982 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10305 (* 1 = 7.10305 loss)
I0523 04:52:08.981997 35003 sgd_solver.cpp:112] Iteration 146710, lr = 0.01
I0523 04:52:12.842600 35003 solver.cpp:239] Iteration 146720 (2.58167 iter/s, 3.87346s/10 iters), loss = 7.12478
I0523 04:52:12.842656 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12478 (* 1 = 7.12478 loss)
I0523 04:52:13.583523 35003 sgd_solver.cpp:112] Iteration 146720, lr = 0.01
I0523 04:52:16.160589 35003 solver.cpp:239] Iteration 146730 (3.01406 iter/s, 3.31779s/10 iters), loss = 6.85139
I0523 04:52:16.160645 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85139 (* 1 = 6.85139 loss)
I0523 04:52:16.831678 35003 sgd_solver.cpp:112] Iteration 146730, lr = 0.01
I0523 04:52:21.136431 35003 solver.cpp:239] Iteration 146740 (2.00982 iter/s, 4.97558s/10 iters), loss = 8.53959
I0523 04:52:21.136473 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.53959 (* 1 = 8.53959 loss)
I0523 04:52:21.140458 35003 sgd_solver.cpp:112] Iteration 146740, lr = 0.01
I0523 04:52:24.608986 35003 solver.cpp:239] Iteration 146750 (2.87988 iter/s, 3.47236s/10 iters), loss = 7.51096
I0523 04:52:24.609026 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51096 (* 1 = 7.51096 loss)
I0523 04:52:25.323899 35003 sgd_solver.cpp:112] Iteration 146750, lr = 0.01
I0523 04:52:28.360079 35003 solver.cpp:239] Iteration 146760 (2.66603 iter/s, 3.75089s/10 iters), loss = 6.37661
I0523 04:52:28.360141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37661 (* 1 = 6.37661 loss)
I0523 04:52:28.491675 35003 sgd_solver.cpp:112] Iteration 146760, lr = 0.01
I0523 04:52:32.130909 35003 solver.cpp:239] Iteration 146770 (2.65209 iter/s, 3.77062s/10 iters), loss = 5.5122
I0523 04:52:32.130949 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.5122 (* 1 = 5.5122 loss)
I0523 04:52:32.148855 35003 sgd_solver.cpp:112] Iteration 146770, lr = 0.01
I0523 04:52:36.487943 35003 solver.cpp:239] Iteration 146780 (2.29526 iter/s, 4.35681s/10 iters), loss = 6.87623
I0523 04:52:36.488114 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87623 (* 1 = 6.87623 loss)
I0523 04:52:36.500787 35003 sgd_solver.cpp:112] Iteration 146780, lr = 0.01
I0523 04:52:40.816975 35003 solver.cpp:239] Iteration 146790 (2.31016 iter/s, 4.32871s/10 iters), loss = 7.07093
I0523 04:52:40.817019 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07093 (* 1 = 7.07093 loss)
I0523 04:52:40.830827 35003 sgd_solver.cpp:112] Iteration 146790, lr = 0.01
I0523 04:52:44.395228 35003 solver.cpp:239] Iteration 146800 (2.79481 iter/s, 3.57806s/10 iters), loss = 7.44529
I0523 04:52:44.395273 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44529 (* 1 = 7.44529 loss)
I0523 04:52:44.403414 35003 sgd_solver.cpp:112] Iteration 146800, lr = 0.01
I0523 04:52:48.719560 35003 solver.cpp:239] Iteration 146810 (2.31262 iter/s, 4.32411s/10 iters), loss = 7.50552
I0523 04:52:48.719614 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50552 (* 1 = 7.50552 loss)
I0523 04:52:49.441996 35003 sgd_solver.cpp:112] Iteration 146810, lr = 0.01
I0523 04:52:52.183697 35003 solver.cpp:239] Iteration 146820 (2.88689 iter/s, 3.46393s/10 iters), loss = 7.03507
I0523 04:52:52.183735 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03507 (* 1 = 7.03507 loss)
I0523 04:52:52.190678 35003 sgd_solver.cpp:112] Iteration 146820, lr = 0.01
I0523 04:52:56.449821 35003 solver.cpp:239] Iteration 146830 (2.34417 iter/s, 4.2659s/10 iters), loss = 7.62238
I0523 04:52:56.449862 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62238 (* 1 = 7.62238 loss)
I0523 04:52:56.463331 35003 sgd_solver.cpp:112] Iteration 146830, lr = 0.01
I0523 04:53:00.828547 35003 solver.cpp:239] Iteration 146840 (2.28389 iter/s, 4.3785s/10 iters), loss = 6.9856
I0523 04:53:00.828605 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9856 (* 1 = 6.9856 loss)
I0523 04:53:00.865355 35003 sgd_solver.cpp:112] Iteration 146840, lr = 0.01
I0523 04:53:03.936314 35003 solver.cpp:239] Iteration 146850 (3.21794 iter/s, 3.10758s/10 iters), loss = 6.76451
I0523 04:53:03.936352 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76451 (* 1 = 6.76451 loss)
I0523 04:53:03.949863 35003 sgd_solver.cpp:112] Iteration 146850, lr = 0.01
I0523 04:53:07.716686 35003 solver.cpp:239] Iteration 146860 (2.64538 iter/s, 3.78018s/10 iters), loss = 7.68426
I0523 04:53:07.716936 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68426 (* 1 = 7.68426 loss)
I0523 04:53:07.919462 35003 sgd_solver.cpp:112] Iteration 146860, lr = 0.01
I0523 04:53:10.776458 35003 solver.cpp:239] Iteration 146870 (3.26859 iter/s, 3.05943s/10 iters), loss = 7.03052
I0523 04:53:10.776512 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03052 (* 1 = 7.03052 loss)
I0523 04:53:11.467829 35003 sgd_solver.cpp:112] Iteration 146870, lr = 0.01
I0523 04:53:14.794684 35003 solver.cpp:239] Iteration 146880 (2.4888 iter/s, 4.01801s/10 iters), loss = 8.39724
I0523 04:53:14.794766 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.39724 (* 1 = 8.39724 loss)
I0523 04:53:15.510243 35003 sgd_solver.cpp:112] Iteration 146880, lr = 0.01
I0523 04:53:19.790616 35003 solver.cpp:239] Iteration 146890 (2.00174 iter/s, 4.99565s/10 iters), loss = 7.65238
I0523 04:53:19.790675 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65238 (* 1 = 7.65238 loss)
I0523 04:53:19.803570 35003 sgd_solver.cpp:112] Iteration 146890, lr = 0.01
I0523 04:53:21.852027 35003 solver.cpp:239] Iteration 146900 (4.85139 iter/s, 2.06126s/10 iters), loss = 7.38138
I0523 04:53:21.852079 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38138 (* 1 = 7.38138 loss)
I0523 04:53:22.593403 35003 sgd_solver.cpp:112] Iteration 146900, lr = 0.01
I0523 04:53:26.865942 35003 solver.cpp:239] Iteration 146910 (1.99455 iter/s, 5.01366s/10 iters), loss = 6.20496
I0523 04:53:26.865983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20496 (* 1 = 6.20496 loss)
I0523 04:53:26.870683 35003 sgd_solver.cpp:112] Iteration 146910, lr = 0.01
I0523 04:53:29.784406 35003 solver.cpp:239] Iteration 146920 (3.42666 iter/s, 2.91829s/10 iters), loss = 6.43286
I0523 04:53:29.784456 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43286 (* 1 = 6.43286 loss)
I0523 04:53:29.792311 35003 sgd_solver.cpp:112] Iteration 146920, lr = 0.01
I0523 04:53:33.419436 35003 solver.cpp:239] Iteration 146930 (2.75117 iter/s, 3.63482s/10 iters), loss = 7.57182
I0523 04:53:33.419484 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57182 (* 1 = 7.57182 loss)
I0523 04:53:33.427721 35003 sgd_solver.cpp:112] Iteration 146930, lr = 0.01
I0523 04:53:36.428982 35003 solver.cpp:239] Iteration 146940 (3.32295 iter/s, 3.00937s/10 iters), loss = 6.94706
I0523 04:53:36.429035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94706 (* 1 = 6.94706 loss)
I0523 04:53:36.436075 35003 sgd_solver.cpp:112] Iteration 146940, lr = 0.01
I0523 04:53:40.730422 35003 solver.cpp:239] Iteration 146950 (2.32493 iter/s, 4.30121s/10 iters), loss = 6.36151
I0523 04:53:40.730633 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36151 (* 1 = 6.36151 loss)
I0523 04:53:40.743579 35003 sgd_solver.cpp:112] Iteration 146950, lr = 0.01
I0523 04:53:43.500399 35003 solver.cpp:239] Iteration 146960 (3.61054 iter/s, 2.76967s/10 iters), loss = 6.01161
I0523 04:53:43.500447 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01161 (* 1 = 6.01161 loss)
I0523 04:53:43.514005 35003 sgd_solver.cpp:112] Iteration 146960, lr = 0.01
I0523 04:53:46.440925 35003 solver.cpp:239] Iteration 146970 (3.40095 iter/s, 2.94036s/10 iters), loss = 8.07366
I0523 04:53:46.440961 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.07366 (* 1 = 8.07366 loss)
I0523 04:53:46.454208 35003 sgd_solver.cpp:112] Iteration 146970, lr = 0.01
I0523 04:53:49.282469 35003 solver.cpp:239] Iteration 146980 (3.51942 iter/s, 2.84138s/10 iters), loss = 7.08042
I0523 04:53:49.282519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08042 (* 1 = 7.08042 loss)
I0523 04:53:50.017335 35003 sgd_solver.cpp:112] Iteration 146980, lr = 0.01
I0523 04:53:52.749873 35003 solver.cpp:239] Iteration 146990 (2.88417 iter/s, 3.46721s/10 iters), loss = 6.9575
I0523 04:53:52.749922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9575 (* 1 = 6.9575 loss)
I0523 04:53:52.755415 35003 sgd_solver.cpp:112] Iteration 146990, lr = 0.01
I0523 04:53:55.636824 35003 solver.cpp:239] Iteration 147000 (3.46408 iter/s, 2.88677s/10 iters), loss = 6.84281
I0523 04:53:55.636868 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84281 (* 1 = 6.84281 loss)
I0523 04:53:55.642020 35003 sgd_solver.cpp:112] Iteration 147000, lr = 0.01
I0523 04:53:58.537978 35003 solver.cpp:239] Iteration 147010 (3.44711 iter/s, 2.90098s/10 iters), loss = 7.33385
I0523 04:53:58.538039 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33385 (* 1 = 7.33385 loss)
I0523 04:53:59.272348 35003 sgd_solver.cpp:112] Iteration 147010, lr = 0.01
I0523 04:54:01.338476 35003 solver.cpp:239] Iteration 147020 (3.57102 iter/s, 2.80032s/10 iters), loss = 7.0167
I0523 04:54:01.338526 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0167 (* 1 = 7.0167 loss)
I0523 04:54:01.352380 35003 sgd_solver.cpp:112] Iteration 147020, lr = 0.01
I0523 04:54:04.707113 35003 solver.cpp:239] Iteration 147030 (2.96873 iter/s, 3.36845s/10 iters), loss = 7.14175
I0523 04:54:04.707152 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14175 (* 1 = 7.14175 loss)
I0523 04:54:04.720667 35003 sgd_solver.cpp:112] Iteration 147030, lr = 0.01
I0523 04:54:08.813680 35003 solver.cpp:239] Iteration 147040 (2.43525 iter/s, 4.10635s/10 iters), loss = 6.93472
I0523 04:54:08.813725 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93472 (* 1 = 6.93472 loss)
I0523 04:54:08.839968 35003 sgd_solver.cpp:112] Iteration 147040, lr = 0.01
I0523 04:54:11.941709 35003 solver.cpp:239] Iteration 147050 (3.19708 iter/s, 3.12786s/10 iters), loss = 6.82022
I0523 04:54:11.941996 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82022 (* 1 = 6.82022 loss)
I0523 04:54:12.494467 35003 sgd_solver.cpp:112] Iteration 147050, lr = 0.01
I0523 04:54:15.300190 35003 solver.cpp:239] Iteration 147060 (2.97789 iter/s, 3.35808s/10 iters), loss = 6.94899
I0523 04:54:15.300238 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94899 (* 1 = 6.94899 loss)
I0523 04:54:16.009071 35003 sgd_solver.cpp:112] Iteration 147060, lr = 0.01
I0523 04:54:20.207528 35003 solver.cpp:239] Iteration 147070 (2.03787 iter/s, 4.90709s/10 iters), loss = 6.62549
I0523 04:54:20.207578 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62549 (* 1 = 6.62549 loss)
I0523 04:54:20.214531 35003 sgd_solver.cpp:112] Iteration 147070, lr = 0.01
I0523 04:54:23.089870 35003 solver.cpp:239] Iteration 147080 (3.46962 iter/s, 2.88216s/10 iters), loss = 7.752
I0523 04:54:23.089927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.752 (* 1 = 7.752 loss)
I0523 04:54:23.798506 35003 sgd_solver.cpp:112] Iteration 147080, lr = 0.01
I0523 04:54:26.639428 35003 solver.cpp:239] Iteration 147090 (2.81741 iter/s, 3.54935s/10 iters), loss = 7.51529
I0523 04:54:26.639470 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51529 (* 1 = 7.51529 loss)
I0523 04:54:26.644373 35003 sgd_solver.cpp:112] Iteration 147090, lr = 0.01
I0523 04:54:31.426821 35003 solver.cpp:239] Iteration 147100 (2.08892 iter/s, 4.78715s/10 iters), loss = 6.34312
I0523 04:54:31.426862 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34312 (* 1 = 6.34312 loss)
I0523 04:54:31.439862 35003 sgd_solver.cpp:112] Iteration 147100, lr = 0.01
I0523 04:54:34.252038 35003 solver.cpp:239] Iteration 147110 (3.53975 iter/s, 2.82506s/10 iters), loss = 6.97511
I0523 04:54:34.252077 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97511 (* 1 = 6.97511 loss)
I0523 04:54:34.256366 35003 sgd_solver.cpp:112] Iteration 147110, lr = 0.01
I0523 04:54:39.300834 35003 solver.cpp:239] Iteration 147120 (1.98078 iter/s, 5.04852s/10 iters), loss = 6.38783
I0523 04:54:39.300892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38783 (* 1 = 6.38783 loss)
I0523 04:54:39.931139 35003 sgd_solver.cpp:112] Iteration 147120, lr = 0.01
I0523 04:54:44.379498 35003 solver.cpp:239] Iteration 147130 (1.96912 iter/s, 5.0784s/10 iters), loss = 6.88748
I0523 04:54:44.379709 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88748 (* 1 = 6.88748 loss)
I0523 04:54:44.392179 35003 sgd_solver.cpp:112] Iteration 147130, lr = 0.01
I0523 04:54:47.312989 35003 solver.cpp:239] Iteration 147140 (3.40927 iter/s, 2.93318s/10 iters), loss = 7.56093
I0523 04:54:47.313045 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56093 (* 1 = 7.56093 loss)
I0523 04:54:47.332790 35003 sgd_solver.cpp:112] Iteration 147140, lr = 0.01
I0523 04:54:50.255545 35003 solver.cpp:239] Iteration 147150 (3.39862 iter/s, 2.94237s/10 iters), loss = 5.29984
I0523 04:54:50.255604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.29984 (* 1 = 5.29984 loss)
I0523 04:54:50.267750 35003 sgd_solver.cpp:112] Iteration 147150, lr = 0.01
I0523 04:54:53.827283 35003 solver.cpp:239] Iteration 147160 (2.79992 iter/s, 3.57153s/10 iters), loss = 6.83195
I0523 04:54:53.827332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83195 (* 1 = 6.83195 loss)
I0523 04:54:53.830595 35003 sgd_solver.cpp:112] Iteration 147160, lr = 0.01
I0523 04:54:56.451395 35003 solver.cpp:239] Iteration 147170 (3.81104 iter/s, 2.62395s/10 iters), loss = 6.80022
I0523 04:54:56.451432 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80022 (* 1 = 6.80022 loss)
I0523 04:54:56.461271 35003 sgd_solver.cpp:112] Iteration 147170, lr = 0.01
I0523 04:54:59.206358 35003 solver.cpp:239] Iteration 147180 (3.63002 iter/s, 2.75481s/10 iters), loss = 6.05945
I0523 04:54:59.206399 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05945 (* 1 = 6.05945 loss)
I0523 04:54:59.219585 35003 sgd_solver.cpp:112] Iteration 147180, lr = 0.01
I0523 04:55:03.036392 35003 solver.cpp:239] Iteration 147190 (2.61108 iter/s, 3.82983s/10 iters), loss = 7.126
I0523 04:55:03.036442 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.126 (* 1 = 7.126 loss)
I0523 04:55:03.143306 35003 sgd_solver.cpp:112] Iteration 147190, lr = 0.01
I0523 04:55:05.983153 35003 solver.cpp:239] Iteration 147200 (3.39376 iter/s, 2.94658s/10 iters), loss = 7.87359
I0523 04:55:05.983198 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87359 (* 1 = 7.87359 loss)
I0523 04:55:06.716486 35003 sgd_solver.cpp:112] Iteration 147200, lr = 0.01
I0523 04:55:08.836143 35003 solver.cpp:239] Iteration 147210 (3.50531 iter/s, 2.85282s/10 iters), loss = 6.5327
I0523 04:55:08.836201 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5327 (* 1 = 6.5327 loss)
I0523 04:55:09.574822 35003 sgd_solver.cpp:112] Iteration 147210, lr = 0.01
I0523 04:55:12.436467 35003 solver.cpp:239] Iteration 147220 (2.77769 iter/s, 3.60012s/10 iters), loss = 6.79071
I0523 04:55:12.436511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79071 (* 1 = 6.79071 loss)
I0523 04:55:12.449002 35003 sgd_solver.cpp:112] Iteration 147220, lr = 0.01
I0523 04:55:14.501355 35003 solver.cpp:239] Iteration 147230 (4.8432 iter/s, 2.06475s/10 iters), loss = 6.24164
I0523 04:55:14.501672 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24164 (* 1 = 6.24164 loss)
I0523 04:55:14.506675 35003 sgd_solver.cpp:112] Iteration 147230, lr = 0.01
I0523 04:55:17.276599 35003 solver.cpp:239] Iteration 147240 (3.60382 iter/s, 2.77484s/10 iters), loss = 6.70052
I0523 04:55:17.276638 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70052 (* 1 = 6.70052 loss)
I0523 04:55:17.731729 35003 sgd_solver.cpp:112] Iteration 147240, lr = 0.01
I0523 04:55:20.338979 35003 solver.cpp:239] Iteration 147250 (3.26561 iter/s, 3.06221s/10 iters), loss = 5.62371
I0523 04:55:20.339020 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.62371 (* 1 = 5.62371 loss)
I0523 04:55:20.344398 35003 sgd_solver.cpp:112] Iteration 147250, lr = 0.01
I0523 04:55:23.878980 35003 solver.cpp:239] Iteration 147260 (2.82501 iter/s, 3.53981s/10 iters), loss = 7.57236
I0523 04:55:23.879017 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57236 (* 1 = 7.57236 loss)
I0523 04:55:23.892333 35003 sgd_solver.cpp:112] Iteration 147260, lr = 0.01
I0523 04:55:26.786414 35003 solver.cpp:239] Iteration 147270 (3.43966 iter/s, 2.90727s/10 iters), loss = 7.19796
I0523 04:55:26.786454 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19796 (* 1 = 7.19796 loss)
I0523 04:55:27.381778 35003 sgd_solver.cpp:112] Iteration 147270, lr = 0.01
I0523 04:55:30.051136 35003 solver.cpp:239] Iteration 147280 (3.06322 iter/s, 3.26454s/10 iters), loss = 7.5957
I0523 04:55:30.051182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5957 (* 1 = 7.5957 loss)
I0523 04:55:30.059064 35003 sgd_solver.cpp:112] Iteration 147280, lr = 0.01
I0523 04:55:32.232854 35003 solver.cpp:239] Iteration 147290 (4.58386 iter/s, 2.18157s/10 iters), loss = 6.76531
I0523 04:55:32.232923 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76531 (* 1 = 6.76531 loss)
I0523 04:55:32.900192 35003 sgd_solver.cpp:112] Iteration 147290, lr = 0.01
I0523 04:55:34.887665 35003 solver.cpp:239] Iteration 147300 (3.76699 iter/s, 2.65464s/10 iters), loss = 6.64331
I0523 04:55:34.887704 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64331 (* 1 = 6.64331 loss)
I0523 04:55:34.901144 35003 sgd_solver.cpp:112] Iteration 147300, lr = 0.01
I0523 04:55:39.821780 35003 solver.cpp:239] Iteration 147310 (2.0268 iter/s, 4.93388s/10 iters), loss = 8.20028
I0523 04:55:39.821827 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.20028 (* 1 = 8.20028 loss)
I0523 04:55:39.913038 35003 sgd_solver.cpp:112] Iteration 147310, lr = 0.01
I0523 04:55:43.507158 35003 solver.cpp:239] Iteration 147320 (2.71357 iter/s, 3.68518s/10 iters), loss = 6.65054
I0523 04:55:43.507200 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65054 (* 1 = 6.65054 loss)
I0523 04:55:44.216011 35003 sgd_solver.cpp:112] Iteration 147320, lr = 0.01
I0523 04:55:46.377882 35003 solver.cpp:239] Iteration 147330 (3.48364 iter/s, 2.87056s/10 iters), loss = 7.04892
I0523 04:55:46.378015 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04892 (* 1 = 7.04892 loss)
I0523 04:55:47.113052 35003 sgd_solver.cpp:112] Iteration 147330, lr = 0.01
I0523 04:55:51.963601 35003 solver.cpp:239] Iteration 147340 (1.7904 iter/s, 5.58535s/10 iters), loss = 8.01173
I0523 04:55:51.963659 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01173 (* 1 = 8.01173 loss)
I0523 04:55:51.973681 35003 sgd_solver.cpp:112] Iteration 147340, lr = 0.01
I0523 04:55:54.674669 35003 solver.cpp:239] Iteration 147350 (3.68882 iter/s, 2.71089s/10 iters), loss = 6.55908
I0523 04:55:54.674767 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55908 (* 1 = 6.55908 loss)
I0523 04:55:55.326962 35003 sgd_solver.cpp:112] Iteration 147350, lr = 0.01
I0523 04:55:59.210883 35003 solver.cpp:239] Iteration 147360 (2.20462 iter/s, 4.53593s/10 iters), loss = 6.70609
I0523 04:55:59.210922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70609 (* 1 = 6.70609 loss)
I0523 04:55:59.229071 35003 sgd_solver.cpp:112] Iteration 147360, lr = 0.01
I0523 04:56:02.835517 35003 solver.cpp:239] Iteration 147370 (2.75904 iter/s, 3.62444s/10 iters), loss = 7.04196
I0523 04:56:02.835556 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04196 (* 1 = 7.04196 loss)
I0523 04:56:02.848563 35003 sgd_solver.cpp:112] Iteration 147370, lr = 0.01
I0523 04:56:06.430310 35003 solver.cpp:239] Iteration 147380 (2.78195 iter/s, 3.5946s/10 iters), loss = 8.17013
I0523 04:56:06.430351 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17013 (* 1 = 8.17013 loss)
I0523 04:56:06.433511 35003 sgd_solver.cpp:112] Iteration 147380, lr = 0.01
I0523 04:56:10.275614 35003 solver.cpp:239] Iteration 147390 (2.60072 iter/s, 3.84509s/10 iters), loss = 6.93126
I0523 04:56:10.275662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93126 (* 1 = 6.93126 loss)
I0523 04:56:10.288544 35003 sgd_solver.cpp:112] Iteration 147390, lr = 0.01
I0523 04:56:12.135963 35003 solver.cpp:239] Iteration 147400 (5.37572 iter/s, 1.86022s/10 iters), loss = 6.52312
I0523 04:56:12.136008 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52312 (* 1 = 6.52312 loss)
I0523 04:56:12.156980 35003 sgd_solver.cpp:112] Iteration 147400, lr = 0.01
I0523 04:56:16.215065 35003 solver.cpp:239] Iteration 147410 (2.45165 iter/s, 4.07889s/10 iters), loss = 7.17034
I0523 04:56:16.215104 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17034 (* 1 = 7.17034 loss)
I0523 04:56:16.246431 35003 sgd_solver.cpp:112] Iteration 147410, lr = 0.01
I0523 04:56:20.608992 35003 solver.cpp:239] Iteration 147420 (2.27599 iter/s, 4.39369s/10 iters), loss = 6.0654
I0523 04:56:20.609210 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0654 (* 1 = 6.0654 loss)
I0523 04:56:20.612536 35003 sgd_solver.cpp:112] Iteration 147420, lr = 0.01
I0523 04:56:24.172582 35003 solver.cpp:239] Iteration 147430 (2.80643 iter/s, 3.56325s/10 iters), loss = 8.37058
I0523 04:56:24.172636 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.37058 (* 1 = 8.37058 loss)
I0523 04:56:24.246870 35003 sgd_solver.cpp:112] Iteration 147430, lr = 0.01
I0523 04:56:28.519886 35003 solver.cpp:239] Iteration 147440 (2.3004 iter/s, 4.34707s/10 iters), loss = 7.32214
I0523 04:56:28.519942 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32214 (* 1 = 7.32214 loss)
I0523 04:56:28.533023 35003 sgd_solver.cpp:112] Iteration 147440, lr = 0.01
I0523 04:56:30.125059 35003 solver.cpp:239] Iteration 147450 (6.23035 iter/s, 1.60505s/10 iters), loss = 6.67798
I0523 04:56:30.125097 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67798 (* 1 = 6.67798 loss)
I0523 04:56:30.133342 35003 sgd_solver.cpp:112] Iteration 147450, lr = 0.01
I0523 04:56:32.230389 35003 solver.cpp:239] Iteration 147460 (4.75015 iter/s, 2.1052s/10 iters), loss = 5.95854
I0523 04:56:32.230433 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95854 (* 1 = 5.95854 loss)
I0523 04:56:32.958943 35003 sgd_solver.cpp:112] Iteration 147460, lr = 0.01
I0523 04:56:37.640919 35003 solver.cpp:239] Iteration 147470 (1.84834 iter/s, 5.41026s/10 iters), loss = 7.96354
I0523 04:56:37.640971 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96354 (* 1 = 7.96354 loss)
I0523 04:56:37.652472 35003 sgd_solver.cpp:112] Iteration 147470, lr = 0.01
I0523 04:56:41.248713 35003 solver.cpp:239] Iteration 147480 (2.77193 iter/s, 3.60759s/10 iters), loss = 7.5882
I0523 04:56:41.248762 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5882 (* 1 = 7.5882 loss)
I0523 04:56:41.261641 35003 sgd_solver.cpp:112] Iteration 147480, lr = 0.01
I0523 04:56:44.300519 35003 solver.cpp:239] Iteration 147490 (3.27694 iter/s, 3.05163s/10 iters), loss = 7.99244
I0523 04:56:44.300559 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99244 (* 1 = 7.99244 loss)
I0523 04:56:44.520092 35003 sgd_solver.cpp:112] Iteration 147490, lr = 0.01
I0523 04:56:47.998734 35003 solver.cpp:239] Iteration 147500 (2.70416 iter/s, 3.698s/10 iters), loss = 6.14628
I0523 04:56:47.998780 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14628 (* 1 = 6.14628 loss)
I0523 04:56:48.727438 35003 sgd_solver.cpp:112] Iteration 147500, lr = 0.01
I0523 04:56:50.950311 35003 solver.cpp:239] Iteration 147510 (3.38822 iter/s, 2.9514s/10 iters), loss = 6.37687
I0523 04:56:50.950467 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37687 (* 1 = 6.37687 loss)
I0523 04:56:51.672435 35003 sgd_solver.cpp:112] Iteration 147510, lr = 0.01
I0523 04:56:55.736007 35003 solver.cpp:239] Iteration 147520 (2.08972 iter/s, 4.78534s/10 iters), loss = 5.97834
I0523 04:56:55.736063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97834 (* 1 = 5.97834 loss)
I0523 04:56:56.398284 35003 sgd_solver.cpp:112] Iteration 147520, lr = 0.01
I0523 04:56:58.435894 35003 solver.cpp:239] Iteration 147530 (3.70409 iter/s, 2.69972s/10 iters), loss = 7.91028
I0523 04:56:58.435932 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91028 (* 1 = 7.91028 loss)
I0523 04:56:58.458771 35003 sgd_solver.cpp:112] Iteration 147530, lr = 0.01
I0523 04:57:01.896612 35003 solver.cpp:239] Iteration 147540 (2.88973 iter/s, 3.46053s/10 iters), loss = 7.0947
I0523 04:57:01.896666 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0947 (* 1 = 7.0947 loss)
I0523 04:57:01.909893 35003 sgd_solver.cpp:112] Iteration 147540, lr = 0.01
I0523 04:57:06.003093 35003 solver.cpp:239] Iteration 147550 (2.4353 iter/s, 4.10626s/10 iters), loss = 7.10245
I0523 04:57:06.003139 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10245 (* 1 = 7.10245 loss)
I0523 04:57:06.224189 35003 sgd_solver.cpp:112] Iteration 147550, lr = 0.01
I0523 04:57:09.604444 35003 solver.cpp:239] Iteration 147560 (2.77689 iter/s, 3.60115s/10 iters), loss = 8.60268
I0523 04:57:09.604496 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.60268 (* 1 = 8.60268 loss)
I0523 04:57:09.606917 35003 sgd_solver.cpp:112] Iteration 147560, lr = 0.01
I0523 04:57:12.965776 35003 solver.cpp:239] Iteration 147570 (2.9752 iter/s, 3.36112s/10 iters), loss = 8.54455
I0523 04:57:12.965833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.54455 (* 1 = 8.54455 loss)
I0523 04:57:13.415969 35003 sgd_solver.cpp:112] Iteration 147570, lr = 0.01
I0523 04:57:15.559106 35003 solver.cpp:239] Iteration 147580 (3.85629 iter/s, 2.59316s/10 iters), loss = 5.80192
I0523 04:57:15.559154 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.80192 (* 1 = 5.80192 loss)
I0523 04:57:15.567505 35003 sgd_solver.cpp:112] Iteration 147580, lr = 0.01
I0523 04:57:18.885807 35003 solver.cpp:239] Iteration 147590 (3.00615 iter/s, 3.32651s/10 iters), loss = 8.43292
I0523 04:57:18.885857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.43292 (* 1 = 8.43292 loss)
I0523 04:57:19.606992 35003 sgd_solver.cpp:112] Iteration 147590, lr = 0.01
I0523 04:57:24.709245 35003 solver.cpp:239] Iteration 147600 (1.71729 iter/s, 5.82313s/10 iters), loss = 6.76736
I0523 04:57:24.709514 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76736 (* 1 = 6.76736 loss)
I0523 04:57:25.424578 35003 sgd_solver.cpp:112] Iteration 147600, lr = 0.01
I0523 04:57:29.046124 35003 solver.cpp:239] Iteration 147610 (2.30604 iter/s, 4.33644s/10 iters), loss = 7.40969
I0523 04:57:29.046182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40969 (* 1 = 7.40969 loss)
I0523 04:57:29.049486 35003 sgd_solver.cpp:112] Iteration 147610, lr = 0.01
I0523 04:57:31.845185 35003 solver.cpp:239] Iteration 147620 (3.57286 iter/s, 2.79888s/10 iters), loss = 6.19825
I0523 04:57:31.845230 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19825 (* 1 = 6.19825 loss)
I0523 04:57:32.561839 35003 sgd_solver.cpp:112] Iteration 147620, lr = 0.01
I0523 04:57:36.042758 35003 solver.cpp:239] Iteration 147630 (2.38245 iter/s, 4.19735s/10 iters), loss = 7.59358
I0523 04:57:36.042816 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59358 (* 1 = 7.59358 loss)
I0523 04:57:36.044975 35003 sgd_solver.cpp:112] Iteration 147630, lr = 0.01
I0523 04:57:39.182672 35003 solver.cpp:239] Iteration 147640 (3.185 iter/s, 3.13972s/10 iters), loss = 5.38768
I0523 04:57:39.182739 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.38768 (* 1 = 5.38768 loss)
I0523 04:57:39.626698 35003 sgd_solver.cpp:112] Iteration 147640, lr = 0.01
I0523 04:57:41.690887 35003 solver.cpp:239] Iteration 147650 (3.98718 iter/s, 2.50804s/10 iters), loss = 6.33224
I0523 04:57:41.690927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33224 (* 1 = 6.33224 loss)
I0523 04:57:41.708498 35003 sgd_solver.cpp:112] Iteration 147650, lr = 0.01
I0523 04:57:44.536979 35003 solver.cpp:239] Iteration 147660 (3.5138 iter/s, 2.84592s/10 iters), loss = 6.00122
I0523 04:57:44.537025 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00122 (* 1 = 6.00122 loss)
I0523 04:57:44.540606 35003 sgd_solver.cpp:112] Iteration 147660, lr = 0.01
I0523 04:57:48.135766 35003 solver.cpp:239] Iteration 147670 (2.77887 iter/s, 3.59858s/10 iters), loss = 6.78221
I0523 04:57:48.135807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78221 (* 1 = 6.78221 loss)
I0523 04:57:48.140507 35003 sgd_solver.cpp:112] Iteration 147670, lr = 0.01
I0523 04:57:52.470950 35003 solver.cpp:239] Iteration 147680 (2.30682 iter/s, 4.33497s/10 iters), loss = 7.4845
I0523 04:57:52.470994 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4845 (* 1 = 7.4845 loss)
I0523 04:57:52.478662 35003 sgd_solver.cpp:112] Iteration 147680, lr = 0.01
I0523 04:57:54.558197 35003 solver.cpp:239] Iteration 147690 (4.79132 iter/s, 2.08711s/10 iters), loss = 5.45426
I0523 04:57:54.558246 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.45426 (* 1 = 5.45426 loss)
I0523 04:57:54.571508 35003 sgd_solver.cpp:112] Iteration 147690, lr = 0.01
I0523 04:57:58.157084 35003 solver.cpp:239] Iteration 147700 (2.77879 iter/s, 3.59869s/10 iters), loss = 6.95412
I0523 04:57:58.157369 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95412 (* 1 = 6.95412 loss)
I0523 04:57:58.171293 35003 sgd_solver.cpp:112] Iteration 147700, lr = 0.01
I0523 04:58:01.531638 35003 solver.cpp:239] Iteration 147710 (2.9637 iter/s, 3.37416s/10 iters), loss = 5.86214
I0523 04:58:01.531678 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.86214 (* 1 = 5.86214 loss)
I0523 04:58:01.535066 35003 sgd_solver.cpp:112] Iteration 147710, lr = 0.01
I0523 04:58:04.334607 35003 solver.cpp:239] Iteration 147720 (3.56785 iter/s, 2.8028s/10 iters), loss = 6.44476
I0523 04:58:04.334653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44476 (* 1 = 6.44476 loss)
I0523 04:58:04.338461 35003 sgd_solver.cpp:112] Iteration 147720, lr = 0.01
I0523 04:58:08.607542 35003 solver.cpp:239] Iteration 147730 (2.34043 iter/s, 4.27271s/10 iters), loss = 6.25081
I0523 04:58:08.607581 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25081 (* 1 = 6.25081 loss)
I0523 04:58:08.621132 35003 sgd_solver.cpp:112] Iteration 147730, lr = 0.01
I0523 04:58:11.306406 35003 solver.cpp:239] Iteration 147740 (3.70548 iter/s, 2.6987s/10 iters), loss = 6.70543
I0523 04:58:11.306442 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70543 (* 1 = 6.70543 loss)
I0523 04:58:11.323747 35003 sgd_solver.cpp:112] Iteration 147740, lr = 0.01
I0523 04:58:14.753212 35003 solver.cpp:239] Iteration 147750 (2.90139 iter/s, 3.44663s/10 iters), loss = 6.04577
I0523 04:58:14.753254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04577 (* 1 = 6.04577 loss)
I0523 04:58:14.766494 35003 sgd_solver.cpp:112] Iteration 147750, lr = 0.01
I0523 04:58:16.847239 35003 solver.cpp:239] Iteration 147760 (4.77584 iter/s, 2.09387s/10 iters), loss = 6.69248
I0523 04:58:16.847283 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69248 (* 1 = 6.69248 loss)
I0523 04:58:16.854986 35003 sgd_solver.cpp:112] Iteration 147760, lr = 0.01
I0523 04:58:20.942416 35003 solver.cpp:239] Iteration 147770 (2.44203 iter/s, 4.09496s/10 iters), loss = 6.62034
I0523 04:58:20.942479 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62034 (* 1 = 6.62034 loss)
I0523 04:58:20.946281 35003 sgd_solver.cpp:112] Iteration 147770, lr = 0.01
I0523 04:58:23.618772 35003 solver.cpp:239] Iteration 147780 (3.73668 iter/s, 2.67617s/10 iters), loss = 7.31301
I0523 04:58:23.618814 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31301 (* 1 = 7.31301 loss)
I0523 04:58:23.976362 35003 sgd_solver.cpp:112] Iteration 147780, lr = 0.01
I0523 04:58:27.461776 35003 solver.cpp:239] Iteration 147790 (2.60227 iter/s, 3.84281s/10 iters), loss = 5.95134
I0523 04:58:27.461825 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95134 (* 1 = 5.95134 loss)
I0523 04:58:27.473443 35003 sgd_solver.cpp:112] Iteration 147790, lr = 0.01
I0523 04:58:30.896142 35003 solver.cpp:239] Iteration 147800 (2.91192 iter/s, 3.43417s/10 iters), loss = 7.55808
I0523 04:58:30.896359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55808 (* 1 = 7.55808 loss)
I0523 04:58:31.034204 35003 sgd_solver.cpp:112] Iteration 147800, lr = 0.01
I0523 04:58:34.598976 35003 solver.cpp:239] Iteration 147810 (2.70089 iter/s, 3.70249s/10 iters), loss = 6.80845
I0523 04:58:34.599035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80845 (* 1 = 6.80845 loss)
I0523 04:58:34.605038 35003 sgd_solver.cpp:112] Iteration 147810, lr = 0.01
I0523 04:58:37.794396 35003 solver.cpp:239] Iteration 147820 (3.12967 iter/s, 3.19522s/10 iters), loss = 5.69494
I0523 04:58:37.794447 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.69494 (* 1 = 5.69494 loss)
I0523 04:58:37.821787 35003 sgd_solver.cpp:112] Iteration 147820, lr = 0.01
I0523 04:58:42.156147 35003 solver.cpp:239] Iteration 147830 (2.29278 iter/s, 4.36152s/10 iters), loss = 8.47503
I0523 04:58:42.156193 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.47503 (* 1 = 8.47503 loss)
I0523 04:58:42.169940 35003 sgd_solver.cpp:112] Iteration 147830, lr = 0.01
I0523 04:58:46.521549 35003 solver.cpp:239] Iteration 147840 (2.29086 iter/s, 4.36517s/10 iters), loss = 7.02033
I0523 04:58:46.521595 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02033 (* 1 = 7.02033 loss)
I0523 04:58:46.529633 35003 sgd_solver.cpp:112] Iteration 147840, lr = 0.01
I0523 04:58:50.320768 35003 solver.cpp:239] Iteration 147850 (2.63226 iter/s, 3.79902s/10 iters), loss = 6.61986
I0523 04:58:50.320809 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61986 (* 1 = 6.61986 loss)
I0523 04:58:50.324386 35003 sgd_solver.cpp:112] Iteration 147850, lr = 0.01
I0523 04:58:54.471559 35003 solver.cpp:239] Iteration 147860 (2.40931 iter/s, 4.15057s/10 iters), loss = 7.24885
I0523 04:58:54.471596 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24885 (* 1 = 7.24885 loss)
I0523 04:58:54.490372 35003 sgd_solver.cpp:112] Iteration 147860, lr = 0.01
I0523 04:58:58.608825 35003 solver.cpp:239] Iteration 147870 (2.41718 iter/s, 4.13706s/10 iters), loss = 6.98875
I0523 04:58:58.608865 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98875 (* 1 = 6.98875 loss)
I0523 04:58:59.317080 35003 sgd_solver.cpp:112] Iteration 147870, lr = 0.01
I0523 04:59:01.521561 35003 solver.cpp:239] Iteration 147880 (3.4334 iter/s, 2.91257s/10 iters), loss = 7.04842
I0523 04:59:01.521802 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04842 (* 1 = 7.04842 loss)
I0523 04:59:01.546449 35003 sgd_solver.cpp:112] Iteration 147880, lr = 0.01
I0523 04:59:04.313828 35003 solver.cpp:239] Iteration 147890 (3.58176 iter/s, 2.79192s/10 iters), loss = 6.07442
I0523 04:59:04.313868 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07442 (* 1 = 6.07442 loss)
I0523 04:59:04.331693 35003 sgd_solver.cpp:112] Iteration 147890, lr = 0.01
I0523 04:59:07.992310 35003 solver.cpp:239] Iteration 147900 (2.71866 iter/s, 3.67828s/10 iters), loss = 7.16656
I0523 04:59:07.992353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16656 (* 1 = 7.16656 loss)
I0523 04:59:08.017132 35003 sgd_solver.cpp:112] Iteration 147900, lr = 0.01
I0523 04:59:13.970494 35003 solver.cpp:239] Iteration 147910 (1.67283 iter/s, 5.97789s/10 iters), loss = 8.58575
I0523 04:59:13.970544 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.58575 (* 1 = 8.58575 loss)
I0523 04:59:14.685245 35003 sgd_solver.cpp:112] Iteration 147910, lr = 0.01
I0523 04:59:16.801614 35003 solver.cpp:239] Iteration 147920 (3.5324 iter/s, 2.83094s/10 iters), loss = 6.45299
I0523 04:59:16.801674 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45299 (* 1 = 6.45299 loss)
I0523 04:59:17.158483 35003 sgd_solver.cpp:112] Iteration 147920, lr = 0.01
I0523 04:59:19.914101 35003 solver.cpp:239] Iteration 147930 (3.21306 iter/s, 3.1123s/10 iters), loss = 6.06699
I0523 04:59:19.914155 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06699 (* 1 = 6.06699 loss)
I0523 04:59:19.925108 35003 sgd_solver.cpp:112] Iteration 147930, lr = 0.01
I0523 04:59:22.477552 35003 solver.cpp:239] Iteration 147940 (3.90124 iter/s, 2.56329s/10 iters), loss = 6.2635
I0523 04:59:22.477593 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2635 (* 1 = 6.2635 loss)
I0523 04:59:22.482306 35003 sgd_solver.cpp:112] Iteration 147940, lr = 0.01
I0523 04:59:25.334399 35003 solver.cpp:239] Iteration 147950 (3.5006 iter/s, 2.85665s/10 iters), loss = 6.33163
I0523 04:59:25.334450 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33163 (* 1 = 6.33163 loss)
I0523 04:59:25.340062 35003 sgd_solver.cpp:112] Iteration 147950, lr = 0.01
I0523 04:59:27.552610 35003 solver.cpp:239] Iteration 147960 (4.50843 iter/s, 2.21807s/10 iters), loss = 6.10259
I0523 04:59:27.552655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10259 (* 1 = 6.10259 loss)
I0523 04:59:27.689674 35003 sgd_solver.cpp:112] Iteration 147960, lr = 0.01
I0523 04:59:32.127980 35003 solver.cpp:239] Iteration 147970 (2.18573 iter/s, 4.57513s/10 iters), loss = 6.80674
I0523 04:59:32.128219 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80674 (* 1 = 6.80674 loss)
I0523 04:59:32.132921 35003 sgd_solver.cpp:112] Iteration 147970, lr = 0.01
I0523 04:59:33.428755 35003 solver.cpp:239] Iteration 147980 (7.68952 iter/s, 1.30047s/10 iters), loss = 7.14146
I0523 04:59:33.428795 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14146 (* 1 = 7.14146 loss)
I0523 04:59:33.442653 35003 sgd_solver.cpp:112] Iteration 147980, lr = 0.01
I0523 04:59:36.142117 35003 solver.cpp:239] Iteration 147990 (3.68569 iter/s, 2.7132s/10 iters), loss = 7.64739
I0523 04:59:36.142172 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64739 (* 1 = 7.64739 loss)
I0523 04:59:36.153873 35003 sgd_solver.cpp:112] Iteration 147990, lr = 0.01
I0523 04:59:38.891389 35003 solver.cpp:239] Iteration 148000 (3.63756 iter/s, 2.7491s/10 iters), loss = 7.39258
I0523 04:59:38.891435 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39258 (* 1 = 7.39258 loss)
I0523 04:59:38.901368 35003 sgd_solver.cpp:112] Iteration 148000, lr = 0.01
I0523 04:59:42.245999 35003 solver.cpp:239] Iteration 148010 (2.98114 iter/s, 3.35442s/10 iters), loss = 6.44666
I0523 04:59:42.246047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44666 (* 1 = 6.44666 loss)
I0523 04:59:42.270900 35003 sgd_solver.cpp:112] Iteration 148010, lr = 0.01
I0523 04:59:45.965052 35003 solver.cpp:239] Iteration 148020 (2.68901 iter/s, 3.71884s/10 iters), loss = 7.07794
I0523 04:59:45.965109 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07794 (* 1 = 7.07794 loss)
I0523 04:59:45.973126 35003 sgd_solver.cpp:112] Iteration 148020, lr = 0.01
I0523 04:59:49.333989 35003 solver.cpp:239] Iteration 148030 (2.96847 iter/s, 3.36874s/10 iters), loss = 6.8057
I0523 04:59:49.334030 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8057 (* 1 = 6.8057 loss)
I0523 04:59:49.338632 35003 sgd_solver.cpp:112] Iteration 148030, lr = 0.01
I0523 04:59:51.843890 35003 solver.cpp:239] Iteration 148040 (3.98447 iter/s, 2.50974s/10 iters), loss = 6.60041
I0523 04:59:51.843935 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60041 (* 1 = 6.60041 loss)
I0523 04:59:51.867437 35003 sgd_solver.cpp:112] Iteration 148040, lr = 0.01
I0523 04:59:55.107801 35003 solver.cpp:239] Iteration 148050 (3.06398 iter/s, 3.26373s/10 iters), loss = 6.15455
I0523 04:59:55.107848 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15455 (* 1 = 6.15455 loss)
I0523 04:59:55.827039 35003 sgd_solver.cpp:112] Iteration 148050, lr = 0.01
I0523 04:59:58.538528 35003 solver.cpp:239] Iteration 148060 (2.91499 iter/s, 3.43054s/10 iters), loss = 7.42283
I0523 04:59:58.538573 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42283 (* 1 = 7.42283 loss)
I0523 04:59:58.551585 35003 sgd_solver.cpp:112] Iteration 148060, lr = 0.01
I0523 05:00:00.970888 35003 solver.cpp:239] Iteration 148070 (4.11149 iter/s, 2.43221s/10 iters), loss = 6.86477
I0523 05:00:00.970929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86477 (* 1 = 6.86477 loss)
I0523 05:00:00.984793 35003 sgd_solver.cpp:112] Iteration 148070, lr = 0.01
I0523 05:00:05.132448 35003 solver.cpp:239] Iteration 148080 (2.40307 iter/s, 4.16134s/10 iters), loss = 6.12719
I0523 05:00:05.132647 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12719 (* 1 = 6.12719 loss)
I0523 05:00:05.139752 35003 sgd_solver.cpp:112] Iteration 148080, lr = 0.01
I0523 05:00:09.426940 35003 solver.cpp:239] Iteration 148090 (2.32877 iter/s, 4.29412s/10 iters), loss = 6.90963
I0523 05:00:09.426988 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90963 (* 1 = 6.90963 loss)
I0523 05:00:09.437942 35003 sgd_solver.cpp:112] Iteration 148090, lr = 0.01
I0523 05:00:10.740680 35003 solver.cpp:239] Iteration 148100 (7.61252 iter/s, 1.31363s/10 iters), loss = 6.75878
I0523 05:00:10.740731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75878 (* 1 = 6.75878 loss)
I0523 05:00:10.750087 35003 sgd_solver.cpp:112] Iteration 148100, lr = 0.01
I0523 05:00:15.162853 35003 solver.cpp:239] Iteration 148110 (2.26145 iter/s, 4.42194s/10 iters), loss = 7.5888
I0523 05:00:15.162902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5888 (* 1 = 7.5888 loss)
I0523 05:00:15.177207 35003 sgd_solver.cpp:112] Iteration 148110, lr = 0.01
I0523 05:00:17.496114 35003 solver.cpp:239] Iteration 148120 (4.28612 iter/s, 2.33311s/10 iters), loss = 7.25989
I0523 05:00:17.496153 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25989 (* 1 = 7.25989 loss)
I0523 05:00:17.510867 35003 sgd_solver.cpp:112] Iteration 148120, lr = 0.01
I0523 05:00:20.352735 35003 solver.cpp:239] Iteration 148130 (3.50084 iter/s, 2.85646s/10 iters), loss = 6.12302
I0523 05:00:20.352771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12302 (* 1 = 6.12302 loss)
I0523 05:00:20.356421 35003 sgd_solver.cpp:112] Iteration 148130, lr = 0.01
I0523 05:00:24.635476 35003 solver.cpp:239] Iteration 148140 (2.33508 iter/s, 4.28251s/10 iters), loss = 6.53907
I0523 05:00:24.635535 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53907 (* 1 = 6.53907 loss)
I0523 05:00:24.642899 35003 sgd_solver.cpp:112] Iteration 148140, lr = 0.01
I0523 05:00:27.542505 35003 solver.cpp:239] Iteration 148150 (3.44016 iter/s, 2.90684s/10 iters), loss = 7.29821
I0523 05:00:27.542554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29821 (* 1 = 7.29821 loss)
I0523 05:00:27.561177 35003 sgd_solver.cpp:112] Iteration 148150, lr = 0.01
I0523 05:00:30.545996 35003 solver.cpp:239] Iteration 148160 (3.32966 iter/s, 3.00331s/10 iters), loss = 6.80996
I0523 05:00:30.546053 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80996 (* 1 = 6.80996 loss)
I0523 05:00:30.555289 35003 sgd_solver.cpp:112] Iteration 148160, lr = 0.01
I0523 05:00:34.772544 35003 solver.cpp:239] Iteration 148170 (2.36613 iter/s, 4.22632s/10 iters), loss = 6.30504
I0523 05:00:34.772585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30504 (* 1 = 6.30504 loss)
I0523 05:00:35.398727 35003 sgd_solver.cpp:112] Iteration 148170, lr = 0.01
I0523 05:00:38.157244 35003 solver.cpp:239] Iteration 148180 (2.95464 iter/s, 3.38451s/10 iters), loss = 7.91314
I0523 05:00:38.157284 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91314 (* 1 = 7.91314 loss)
I0523 05:00:38.165537 35003 sgd_solver.cpp:112] Iteration 148180, lr = 0.01
I0523 05:00:40.885370 35003 solver.cpp:239] Iteration 148190 (3.66573 iter/s, 2.72797s/10 iters), loss = 6.17331
I0523 05:00:40.885411 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17331 (* 1 = 6.17331 loss)
I0523 05:00:40.891090 35003 sgd_solver.cpp:112] Iteration 148190, lr = 0.01
I0523 05:00:44.488584 35003 solver.cpp:239] Iteration 148200 (2.77545 iter/s, 3.60302s/10 iters), loss = 6.90428
I0523 05:00:44.488626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90428 (* 1 = 6.90428 loss)
I0523 05:00:44.495676 35003 sgd_solver.cpp:112] Iteration 148200, lr = 0.01
I0523 05:00:46.930688 35003 solver.cpp:239] Iteration 148210 (4.0951 iter/s, 2.44195s/10 iters), loss = 7.52183
I0523 05:00:46.930755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52183 (* 1 = 7.52183 loss)
I0523 05:00:47.658274 35003 sgd_solver.cpp:112] Iteration 148210, lr = 0.01
I0523 05:00:51.222479 35003 solver.cpp:239] Iteration 148220 (2.33016 iter/s, 4.29154s/10 iters), loss = 6.76868
I0523 05:00:51.222527 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76868 (* 1 = 6.76868 loss)
I0523 05:00:51.243576 35003 sgd_solver.cpp:112] Iteration 148220, lr = 0.01
I0523 05:00:54.794752 35003 solver.cpp:239] Iteration 148230 (2.79949 iter/s, 3.57207s/10 iters), loss = 7.23209
I0523 05:00:54.794801 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23209 (* 1 = 7.23209 loss)
I0523 05:00:54.800977 35003 sgd_solver.cpp:112] Iteration 148230, lr = 0.01
I0523 05:00:59.051620 35003 solver.cpp:239] Iteration 148240 (2.34927 iter/s, 4.25665s/10 iters), loss = 7.55615
I0523 05:00:59.051659 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55615 (* 1 = 7.55615 loss)
I0523 05:00:59.062674 35003 sgd_solver.cpp:112] Iteration 148240, lr = 0.01
I0523 05:01:03.095134 35003 solver.cpp:239] Iteration 148250 (2.47322 iter/s, 4.04331s/10 iters), loss = 7.02615
I0523 05:01:03.095182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02615 (* 1 = 7.02615 loss)
I0523 05:01:03.108675 35003 sgd_solver.cpp:112] Iteration 148250, lr = 0.01
I0523 05:01:06.353935 35003 solver.cpp:239] Iteration 148260 (3.06879 iter/s, 3.25861s/10 iters), loss = 6.58978
I0523 05:01:06.354199 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58978 (* 1 = 6.58978 loss)
I0523 05:01:07.026161 35003 sgd_solver.cpp:112] Iteration 148260, lr = 0.01
I0523 05:01:10.654227 35003 solver.cpp:239] Iteration 148270 (2.32565 iter/s, 4.29988s/10 iters), loss = 7.82362
I0523 05:01:10.654275 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82362 (* 1 = 7.82362 loss)
I0523 05:01:11.394994 35003 sgd_solver.cpp:112] Iteration 148270, lr = 0.01
I0523 05:01:15.174788 35003 solver.cpp:239] Iteration 148280 (2.21223 iter/s, 4.52033s/10 iters), loss = 7.39882
I0523 05:01:15.174832 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39882 (* 1 = 7.39882 loss)
I0523 05:01:15.871047 35003 sgd_solver.cpp:112] Iteration 148280, lr = 0.01
I0523 05:01:18.141451 35003 solver.cpp:239] Iteration 148290 (3.37099 iter/s, 2.96649s/10 iters), loss = 7.14213
I0523 05:01:18.141508 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14213 (* 1 = 7.14213 loss)
I0523 05:01:18.154979 35003 sgd_solver.cpp:112] Iteration 148290, lr = 0.01
I0523 05:01:21.800969 35003 solver.cpp:239] Iteration 148300 (2.73277 iter/s, 3.65929s/10 iters), loss = 6.89444
I0523 05:01:21.801038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89444 (* 1 = 6.89444 loss)
I0523 05:01:22.535660 35003 sgd_solver.cpp:112] Iteration 148300, lr = 0.01
I0523 05:01:26.463469 35003 solver.cpp:239] Iteration 148310 (2.1449 iter/s, 4.66223s/10 iters), loss = 6.52557
I0523 05:01:26.463531 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52557 (* 1 = 6.52557 loss)
I0523 05:01:26.466953 35003 sgd_solver.cpp:112] Iteration 148310, lr = 0.01
I0523 05:01:30.346194 35003 solver.cpp:239] Iteration 148320 (2.5757 iter/s, 3.88244s/10 iters), loss = 6.00773
I0523 05:01:30.346247 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00773 (* 1 = 6.00773 loss)
I0523 05:01:30.370497 35003 sgd_solver.cpp:112] Iteration 148320, lr = 0.01
I0523 05:01:33.273547 35003 solver.cpp:239] Iteration 148330 (3.41626 iter/s, 2.92718s/10 iters), loss = 6.18745
I0523 05:01:33.273591 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18745 (* 1 = 6.18745 loss)
I0523 05:01:33.909283 35003 sgd_solver.cpp:112] Iteration 148330, lr = 0.01
I0523 05:01:37.418021 35003 solver.cpp:239] Iteration 148340 (2.41298 iter/s, 4.14426s/10 iters), loss = 7.23744
I0523 05:01:37.418239 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23744 (* 1 = 7.23744 loss)
I0523 05:01:37.428958 35003 sgd_solver.cpp:112] Iteration 148340, lr = 0.01
I0523 05:01:40.256757 35003 solver.cpp:239] Iteration 148350 (3.5231 iter/s, 2.83841s/10 iters), loss = 7.5977
I0523 05:01:40.256815 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5977 (* 1 = 7.5977 loss)
I0523 05:01:40.581974 35003 sgd_solver.cpp:112] Iteration 148350, lr = 0.01
I0523 05:01:44.086568 35003 solver.cpp:239] Iteration 148360 (2.61124 iter/s, 3.82959s/10 iters), loss = 7.41288
I0523 05:01:44.086701 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41288 (* 1 = 7.41288 loss)
I0523 05:01:44.099038 35003 sgd_solver.cpp:112] Iteration 148360, lr = 0.01
I0523 05:01:47.405328 35003 solver.cpp:239] Iteration 148370 (3.01341 iter/s, 3.3185s/10 iters), loss = 6.8556
I0523 05:01:47.405370 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8556 (* 1 = 6.8556 loss)
I0523 05:01:47.418604 35003 sgd_solver.cpp:112] Iteration 148370, lr = 0.01
I0523 05:01:51.083155 35003 solver.cpp:239] Iteration 148380 (2.71915 iter/s, 3.67763s/10 iters), loss = 8.10747
I0523 05:01:51.083199 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.10747 (* 1 = 8.10747 loss)
I0523 05:01:51.086230 35003 sgd_solver.cpp:112] Iteration 148380, lr = 0.01
I0523 05:01:53.941673 35003 solver.cpp:239] Iteration 148390 (3.49852 iter/s, 2.85835s/10 iters), loss = 6.21416
I0523 05:01:53.941711 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21416 (* 1 = 6.21416 loss)
I0523 05:01:53.967531 35003 sgd_solver.cpp:112] Iteration 148390, lr = 0.01
I0523 05:01:56.119524 35003 solver.cpp:239] Iteration 148400 (4.59199 iter/s, 2.17771s/10 iters), loss = 6.60212
I0523 05:01:56.119585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60212 (* 1 = 6.60212 loss)
I0523 05:01:56.854620 35003 sgd_solver.cpp:112] Iteration 148400, lr = 0.01
I0523 05:01:59.243456 35003 solver.cpp:239] Iteration 148410 (3.20129 iter/s, 3.12374s/10 iters), loss = 7.2544
I0523 05:01:59.243494 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2544 (* 1 = 7.2544 loss)
I0523 05:01:59.256769 35003 sgd_solver.cpp:112] Iteration 148410, lr = 0.01
I0523 05:02:02.084084 35003 solver.cpp:239] Iteration 148420 (3.52055 iter/s, 2.84047s/10 iters), loss = 6.82954
I0523 05:02:02.084134 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82954 (* 1 = 6.82954 loss)
I0523 05:02:02.090749 35003 sgd_solver.cpp:112] Iteration 148420, lr = 0.01
I0523 05:02:06.297956 35003 solver.cpp:239] Iteration 148430 (2.37324 iter/s, 4.21365s/10 iters), loss = 7.36154
I0523 05:02:06.298003 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36154 (* 1 = 7.36154 loss)
I0523 05:02:06.416481 35003 sgd_solver.cpp:112] Iteration 148430, lr = 0.01
I0523 05:02:09.469880 35003 solver.cpp:239] Iteration 148440 (3.15284 iter/s, 3.17174s/10 iters), loss = 6.7674
I0523 05:02:09.470057 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7674 (* 1 = 6.7674 loss)
I0523 05:02:09.487038 35003 sgd_solver.cpp:112] Iteration 148440, lr = 0.01
I0523 05:02:12.892747 35003 solver.cpp:239] Iteration 148450 (2.92181 iter/s, 3.42254s/10 iters), loss = 6.97488
I0523 05:02:12.892796 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97488 (* 1 = 6.97488 loss)
I0523 05:02:12.899852 35003 sgd_solver.cpp:112] Iteration 148450, lr = 0.01
I0523 05:02:15.930322 35003 solver.cpp:239] Iteration 148460 (3.29229 iter/s, 3.0374s/10 iters), loss = 7.66575
I0523 05:02:15.930361 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66575 (* 1 = 7.66575 loss)
I0523 05:02:15.943234 35003 sgd_solver.cpp:112] Iteration 148460, lr = 0.01
I0523 05:02:20.931993 35003 solver.cpp:239] Iteration 148470 (1.99943 iter/s, 5.00142s/10 iters), loss = 6.15857
I0523 05:02:20.932052 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15857 (* 1 = 6.15857 loss)
I0523 05:02:20.939752 35003 sgd_solver.cpp:112] Iteration 148470, lr = 0.01
I0523 05:02:25.271409 35003 solver.cpp:239] Iteration 148480 (2.30458 iter/s, 4.33918s/10 iters), loss = 7.6032
I0523 05:02:25.271468 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6032 (* 1 = 7.6032 loss)
I0523 05:02:25.276010 35003 sgd_solver.cpp:112] Iteration 148480, lr = 0.01
I0523 05:02:29.218868 35003 solver.cpp:239] Iteration 148490 (2.53342 iter/s, 3.94724s/10 iters), loss = 7.63512
I0523 05:02:29.218919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63512 (* 1 = 7.63512 loss)
I0523 05:02:29.223369 35003 sgd_solver.cpp:112] Iteration 148490, lr = 0.01
I0523 05:02:32.111831 35003 solver.cpp:239] Iteration 148500 (3.45687 iter/s, 2.89279s/10 iters), loss = 7.1771
I0523 05:02:32.111877 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1771 (* 1 = 7.1771 loss)
I0523 05:02:32.823494 35003 sgd_solver.cpp:112] Iteration 148500, lr = 0.01
I0523 05:02:36.426501 35003 solver.cpp:239] Iteration 148510 (2.31779 iter/s, 4.31445s/10 iters), loss = 6.74978
I0523 05:02:36.426546 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74978 (* 1 = 6.74978 loss)
I0523 05:02:36.439417 35003 sgd_solver.cpp:112] Iteration 148510, lr = 0.01
I0523 05:02:40.088510 35003 solver.cpp:239] Iteration 148520 (2.73089 iter/s, 3.66181s/10 iters), loss = 7.73635
I0523 05:02:40.088752 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73635 (* 1 = 7.73635 loss)
I0523 05:02:40.121532 35003 sgd_solver.cpp:112] Iteration 148520, lr = 0.01
I0523 05:02:44.182180 35003 solver.cpp:239] Iteration 148530 (2.44304 iter/s, 4.09326s/10 iters), loss = 6.66074
I0523 05:02:44.182240 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66074 (* 1 = 6.66074 loss)
I0523 05:02:44.520050 35003 sgd_solver.cpp:112] Iteration 148530, lr = 0.01
I0523 05:02:47.693369 35003 solver.cpp:239] Iteration 148540 (2.84821 iter/s, 3.51098s/10 iters), loss = 5.4901
I0523 05:02:47.693420 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.4901 (* 1 = 5.4901 loss)
I0523 05:02:47.699110 35003 sgd_solver.cpp:112] Iteration 148540, lr = 0.01
I0523 05:02:50.602653 35003 solver.cpp:239] Iteration 148550 (3.43749 iter/s, 2.9091s/10 iters), loss = 5.79223
I0523 05:02:50.602716 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.79223 (* 1 = 5.79223 loss)
I0523 05:02:51.342833 35003 sgd_solver.cpp:112] Iteration 148550, lr = 0.01
I0523 05:02:54.129166 35003 solver.cpp:239] Iteration 148560 (2.83582 iter/s, 3.52632s/10 iters), loss = 7.08314
I0523 05:02:54.129223 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08314 (* 1 = 7.08314 loss)
I0523 05:02:54.141666 35003 sgd_solver.cpp:112] Iteration 148560, lr = 0.01
I0523 05:02:56.479250 35003 solver.cpp:239] Iteration 148570 (4.25545 iter/s, 2.34993s/10 iters), loss = 7.35961
I0523 05:02:56.479285 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35961 (* 1 = 7.35961 loss)
I0523 05:02:56.492810 35003 sgd_solver.cpp:112] Iteration 148570, lr = 0.01
I0523 05:02:59.294201 35003 solver.cpp:239] Iteration 148580 (3.55266 iter/s, 2.81479s/10 iters), loss = 6.71207
I0523 05:02:59.294241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71207 (* 1 = 6.71207 loss)
I0523 05:02:59.304241 35003 sgd_solver.cpp:112] Iteration 148580, lr = 0.01
I0523 05:03:02.619837 35003 solver.cpp:239] Iteration 148590 (3.00711 iter/s, 3.32546s/10 iters), loss = 4.99038
I0523 05:03:02.619881 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.99038 (* 1 = 4.99038 loss)
I0523 05:03:02.633211 35003 sgd_solver.cpp:112] Iteration 148590, lr = 0.01
I0523 05:03:04.732278 35003 solver.cpp:239] Iteration 148600 (4.73417 iter/s, 2.1123s/10 iters), loss = 6.72592
I0523 05:03:04.732319 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72592 (* 1 = 6.72592 loss)
I0523 05:03:04.737052 35003 sgd_solver.cpp:112] Iteration 148600, lr = 0.01
I0523 05:03:07.533507 35003 solver.cpp:239] Iteration 148610 (3.57008 iter/s, 2.80106s/10 iters), loss = 6.49573
I0523 05:03:07.533540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49573 (* 1 = 6.49573 loss)
I0523 05:03:07.546653 35003 sgd_solver.cpp:112] Iteration 148610, lr = 0.01
I0523 05:03:12.460351 35003 solver.cpp:239] Iteration 148620 (2.0298 iter/s, 4.9266s/10 iters), loss = 7.12481
I0523 05:03:12.460541 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12481 (* 1 = 7.12481 loss)
I0523 05:03:12.473137 35003 sgd_solver.cpp:112] Iteration 148620, lr = 0.01
I0523 05:03:15.931726 35003 solver.cpp:239] Iteration 148630 (2.88096 iter/s, 3.47106s/10 iters), loss = 6.92426
I0523 05:03:15.931774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92426 (* 1 = 6.92426 loss)
I0523 05:03:15.945430 35003 sgd_solver.cpp:112] Iteration 148630, lr = 0.01
I0523 05:03:18.340461 35003 solver.cpp:239] Iteration 148640 (4.15183 iter/s, 2.40858s/10 iters), loss = 6.99154
I0523 05:03:18.340507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99154 (* 1 = 6.99154 loss)
I0523 05:03:18.348084 35003 sgd_solver.cpp:112] Iteration 148640, lr = 0.01
I0523 05:03:21.887879 35003 solver.cpp:239] Iteration 148650 (2.81911 iter/s, 3.54722s/10 iters), loss = 7.10389
I0523 05:03:21.887925 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10389 (* 1 = 7.10389 loss)
I0523 05:03:21.909869 35003 sgd_solver.cpp:112] Iteration 148650, lr = 0.01
I0523 05:03:26.748381 35003 solver.cpp:239] Iteration 148660 (2.05751 iter/s, 4.86026s/10 iters), loss = 6.82451
I0523 05:03:26.748431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82451 (* 1 = 6.82451 loss)
I0523 05:03:26.756611 35003 sgd_solver.cpp:112] Iteration 148660, lr = 0.01
I0523 05:03:30.847556 35003 solver.cpp:239] Iteration 148670 (2.43965 iter/s, 4.09896s/10 iters), loss = 6.48025
I0523 05:03:30.847600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48025 (* 1 = 6.48025 loss)
I0523 05:03:30.871117 35003 sgd_solver.cpp:112] Iteration 148670, lr = 0.01
I0523 05:03:35.028354 35003 solver.cpp:239] Iteration 148680 (2.39202 iter/s, 4.18057s/10 iters), loss = 7.28476
I0523 05:03:35.028417 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28476 (* 1 = 7.28476 loss)
I0523 05:03:35.033563 35003 sgd_solver.cpp:112] Iteration 148680, lr = 0.01
I0523 05:03:40.205729 35003 solver.cpp:239] Iteration 148690 (1.93159 iter/s, 5.17709s/10 iters), loss = 6.37211
I0523 05:03:40.205777 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37211 (* 1 = 6.37211 loss)
I0523 05:03:40.672489 35003 sgd_solver.cpp:112] Iteration 148690, lr = 0.01
I0523 05:03:43.528192 35003 solver.cpp:239] Iteration 148700 (3.00999 iter/s, 3.32227s/10 iters), loss = 5.91172
I0523 05:03:43.528353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91172 (* 1 = 5.91172 loss)
I0523 05:03:43.539363 35003 sgd_solver.cpp:112] Iteration 148700, lr = 0.01
I0523 05:03:47.156883 35003 solver.cpp:239] Iteration 148710 (2.75605 iter/s, 3.62839s/10 iters), loss = 7.73909
I0523 05:03:47.156929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73909 (* 1 = 7.73909 loss)
I0523 05:03:47.855381 35003 sgd_solver.cpp:112] Iteration 148710, lr = 0.01
I0523 05:03:52.168603 35003 solver.cpp:239] Iteration 148720 (1.99542 iter/s, 5.01147s/10 iters), loss = 7.68977
I0523 05:03:52.168654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68977 (* 1 = 7.68977 loss)
I0523 05:03:52.870076 35003 sgd_solver.cpp:112] Iteration 148720, lr = 0.01
I0523 05:03:55.787907 35003 solver.cpp:239] Iteration 148730 (2.76312 iter/s, 3.6191s/10 iters), loss = 7.58114
I0523 05:03:55.787957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58114 (* 1 = 7.58114 loss)
I0523 05:03:56.462168 35003 sgd_solver.cpp:112] Iteration 148730, lr = 0.01
I0523 05:03:59.671298 35003 solver.cpp:239] Iteration 148740 (2.57521 iter/s, 3.88318s/10 iters), loss = 7.16468
I0523 05:03:59.671341 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16468 (* 1 = 7.16468 loss)
I0523 05:03:59.679610 35003 sgd_solver.cpp:112] Iteration 148740, lr = 0.01
I0523 05:04:04.091326 35003 solver.cpp:239] Iteration 148750 (2.26255 iter/s, 4.41979s/10 iters), loss = 6.98676
I0523 05:04:04.091373 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98676 (* 1 = 6.98676 loss)
I0523 05:04:04.829486 35003 sgd_solver.cpp:112] Iteration 148750, lr = 0.01
I0523 05:04:07.146737 35003 solver.cpp:239] Iteration 148760 (3.27307 iter/s, 3.05523s/10 iters), loss = 6.19309
I0523 05:04:07.146787 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19309 (* 1 = 6.19309 loss)
I0523 05:04:07.159721 35003 sgd_solver.cpp:112] Iteration 148760, lr = 0.01
I0523 05:04:09.224211 35003 solver.cpp:239] Iteration 148770 (4.81385 iter/s, 2.07734s/10 iters), loss = 5.97139
I0523 05:04:09.224251 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97139 (* 1 = 5.97139 loss)
I0523 05:04:09.228194 35003 sgd_solver.cpp:112] Iteration 148770, lr = 0.01
I0523 05:04:12.157124 35003 solver.cpp:239] Iteration 148780 (3.40978 iter/s, 2.93274s/10 iters), loss = 5.92287
I0523 05:04:12.157181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92287 (* 1 = 5.92287 loss)
I0523 05:04:12.898144 35003 sgd_solver.cpp:112] Iteration 148780, lr = 0.01
I0523 05:04:16.866147 35003 solver.cpp:239] Iteration 148790 (2.12369 iter/s, 4.70878s/10 iters), loss = 6.24896
I0523 05:04:16.866416 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24896 (* 1 = 6.24896 loss)
I0523 05:04:16.877137 35003 sgd_solver.cpp:112] Iteration 148790, lr = 0.01
I0523 05:04:21.874017 35003 solver.cpp:239] Iteration 148800 (1.99704 iter/s, 5.00741s/10 iters), loss = 6.49229
I0523 05:04:21.874064 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49229 (* 1 = 6.49229 loss)
I0523 05:04:21.900795 35003 sgd_solver.cpp:112] Iteration 148800, lr = 0.01
I0523 05:04:24.682554 35003 solver.cpp:239] Iteration 148810 (3.56078 iter/s, 2.80837s/10 iters), loss = 6.37689
I0523 05:04:24.682600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37689 (* 1 = 6.37689 loss)
I0523 05:04:25.291501 35003 sgd_solver.cpp:112] Iteration 148810, lr = 0.01
I0523 05:04:27.345890 35003 solver.cpp:239] Iteration 148820 (3.75491 iter/s, 2.66318s/10 iters), loss = 7.17856
I0523 05:04:27.345935 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17856 (* 1 = 7.17856 loss)
I0523 05:04:27.359565 35003 sgd_solver.cpp:112] Iteration 148820, lr = 0.01
I0523 05:04:30.283526 35003 solver.cpp:239] Iteration 148830 (3.4043 iter/s, 2.93746s/10 iters), loss = 6.76256
I0523 05:04:30.283569 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76256 (* 1 = 6.76256 loss)
I0523 05:04:30.288100 35003 sgd_solver.cpp:112] Iteration 148830, lr = 0.01
I0523 05:04:33.772300 35003 solver.cpp:239] Iteration 148840 (2.8665 iter/s, 3.48858s/10 iters), loss = 6.43955
I0523 05:04:33.772348 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43955 (* 1 = 6.43955 loss)
I0523 05:04:33.785276 35003 sgd_solver.cpp:112] Iteration 148840, lr = 0.01
I0523 05:04:36.658949 35003 solver.cpp:239] Iteration 148850 (3.46442 iter/s, 2.88648s/10 iters), loss = 7.37265
I0523 05:04:36.658989 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37265 (* 1 = 7.37265 loss)
I0523 05:04:37.379750 35003 sgd_solver.cpp:112] Iteration 148850, lr = 0.01
I0523 05:04:39.445153 35003 solver.cpp:239] Iteration 148860 (3.58932 iter/s, 2.78604s/10 iters), loss = 6.88436
I0523 05:04:39.445201 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88436 (* 1 = 6.88436 loss)
I0523 05:04:40.179663 35003 sgd_solver.cpp:112] Iteration 148860, lr = 0.01
I0523 05:04:44.370563 35003 solver.cpp:239] Iteration 148870 (2.03039 iter/s, 4.92516s/10 iters), loss = 7.04409
I0523 05:04:44.370611 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04409 (* 1 = 7.04409 loss)
I0523 05:04:44.376353 35003 sgd_solver.cpp:112] Iteration 148870, lr = 0.01
I0523 05:04:48.027554 35003 solver.cpp:239] Iteration 148880 (2.73465 iter/s, 3.65678s/10 iters), loss = 7.09053
I0523 05:04:48.027830 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09053 (* 1 = 7.09053 loss)
I0523 05:04:48.048972 35003 sgd_solver.cpp:112] Iteration 148880, lr = 0.01
I0523 05:04:50.846628 35003 solver.cpp:239] Iteration 148890 (3.54772 iter/s, 2.81871s/10 iters), loss = 7.47507
I0523 05:04:50.846671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47507 (* 1 = 7.47507 loss)
I0523 05:04:51.588228 35003 sgd_solver.cpp:112] Iteration 148890, lr = 0.01
I0523 05:04:56.068778 35003 solver.cpp:239] Iteration 148900 (1.91501 iter/s, 5.22189s/10 iters), loss = 5.72057
I0523 05:04:56.068827 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.72057 (* 1 = 5.72057 loss)
I0523 05:04:56.076112 35003 sgd_solver.cpp:112] Iteration 148900, lr = 0.01
I0523 05:05:00.428685 35003 solver.cpp:239] Iteration 148910 (2.29375 iter/s, 4.35967s/10 iters), loss = 6.62316
I0523 05:05:00.428726 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62316 (* 1 = 6.62316 loss)
I0523 05:05:00.442373 35003 sgd_solver.cpp:112] Iteration 148910, lr = 0.01
I0523 05:05:03.952862 35003 solver.cpp:239] Iteration 148920 (2.8377 iter/s, 3.52398s/10 iters), loss = 6.61713
I0523 05:05:03.952919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61713 (* 1 = 6.61713 loss)
I0523 05:05:04.118923 35003 sgd_solver.cpp:112] Iteration 148920, lr = 0.01
I0523 05:05:05.494523 35003 solver.cpp:239] Iteration 148930 (6.48706 iter/s, 1.54153s/10 iters), loss = 7.63526
I0523 05:05:05.494570 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63526 (* 1 = 7.63526 loss)
I0523 05:05:06.228948 35003 sgd_solver.cpp:112] Iteration 148930, lr = 0.01
I0523 05:05:09.730650 35003 solver.cpp:239] Iteration 148940 (2.36077 iter/s, 4.2359s/10 iters), loss = 6.7082
I0523 05:05:09.730727 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7082 (* 1 = 6.7082 loss)
I0523 05:05:09.737056 35003 sgd_solver.cpp:112] Iteration 148940, lr = 0.01
I0523 05:05:12.615557 35003 solver.cpp:239] Iteration 148950 (3.46656 iter/s, 2.8847s/10 iters), loss = 7.53525
I0523 05:05:12.615605 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53525 (* 1 = 7.53525 loss)
I0523 05:05:12.640168 35003 sgd_solver.cpp:112] Iteration 148950, lr = 0.01
I0523 05:05:17.001790 35003 solver.cpp:239] Iteration 148960 (2.27999 iter/s, 4.38599s/10 iters), loss = 7.21286
I0523 05:05:17.001840 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21286 (* 1 = 7.21286 loss)
I0523 05:05:17.015247 35003 sgd_solver.cpp:112] Iteration 148960, lr = 0.01
I0523 05:05:19.874202 35003 solver.cpp:239] Iteration 148970 (3.48161 iter/s, 2.87224s/10 iters), loss = 6.81877
I0523 05:05:19.874455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81877 (* 1 = 6.81877 loss)
I0523 05:05:20.589015 35003 sgd_solver.cpp:112] Iteration 148970, lr = 0.01
I0523 05:05:24.418743 35003 solver.cpp:239] Iteration 148980 (2.20064 iter/s, 4.54413s/10 iters), loss = 7.29747
I0523 05:05:24.418790 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29747 (* 1 = 7.29747 loss)
I0523 05:05:24.424922 35003 sgd_solver.cpp:112] Iteration 148980, lr = 0.01
I0523 05:05:28.003557 35003 solver.cpp:239] Iteration 148990 (2.7897 iter/s, 3.58462s/10 iters), loss = 7.50815
I0523 05:05:28.003597 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50815 (* 1 = 7.50815 loss)
I0523 05:05:28.009119 35003 sgd_solver.cpp:112] Iteration 148990, lr = 0.01
I0523 05:05:30.847177 35003 solver.cpp:239] Iteration 149000 (3.51687 iter/s, 2.84344s/10 iters), loss = 6.48229
I0523 05:05:30.847234 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48229 (* 1 = 6.48229 loss)
I0523 05:05:30.989548 35003 sgd_solver.cpp:112] Iteration 149000, lr = 0.01
I0523 05:05:33.059864 35003 solver.cpp:239] Iteration 149010 (4.5197 iter/s, 2.21254s/10 iters), loss = 6.09326
I0523 05:05:33.059909 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09326 (* 1 = 6.09326 loss)
I0523 05:05:33.072926 35003 sgd_solver.cpp:112] Iteration 149010, lr = 0.01
I0523 05:05:37.283736 35003 solver.cpp:239] Iteration 149020 (2.36762 iter/s, 4.22365s/10 iters), loss = 7.1787
I0523 05:05:37.283788 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1787 (* 1 = 7.1787 loss)
I0523 05:05:37.456634 35003 sgd_solver.cpp:112] Iteration 149020, lr = 0.01
I0523 05:05:40.316541 35003 solver.cpp:239] Iteration 149030 (3.29748 iter/s, 3.03262s/10 iters), loss = 5.51885
I0523 05:05:40.316609 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.51885 (* 1 = 5.51885 loss)
I0523 05:05:41.058308 35003 sgd_solver.cpp:112] Iteration 149030, lr = 0.01
I0523 05:05:44.121907 35003 solver.cpp:239] Iteration 149040 (2.62802 iter/s, 3.80514s/10 iters), loss = 6.45808
I0523 05:05:44.121951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45808 (* 1 = 6.45808 loss)
I0523 05:05:44.128643 35003 sgd_solver.cpp:112] Iteration 149040, lr = 0.01
I0523 05:05:46.938509 35003 solver.cpp:239] Iteration 149050 (3.55062 iter/s, 2.81641s/10 iters), loss = 6.0071
I0523 05:05:46.938565 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0071 (* 1 = 6.0071 loss)
I0523 05:05:46.951277 35003 sgd_solver.cpp:112] Iteration 149050, lr = 0.01
I0523 05:05:50.460930 35003 solver.cpp:239] Iteration 149060 (2.83912 iter/s, 3.52221s/10 iters), loss = 6.63387
I0523 05:05:50.461169 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63387 (* 1 = 6.63387 loss)
I0523 05:05:51.202488 35003 sgd_solver.cpp:112] Iteration 149060, lr = 0.01
I0523 05:05:54.059468 35003 solver.cpp:239] Iteration 149070 (2.7792 iter/s, 3.59816s/10 iters), loss = 7.58135
I0523 05:05:54.059515 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58135 (* 1 = 7.58135 loss)
I0523 05:05:54.767979 35003 sgd_solver.cpp:112] Iteration 149070, lr = 0.01
I0523 05:05:56.886744 35003 solver.cpp:239] Iteration 149080 (3.53718 iter/s, 2.82711s/10 iters), loss = 7.15786
I0523 05:05:56.886790 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15786 (* 1 = 7.15786 loss)
I0523 05:05:57.569681 35003 sgd_solver.cpp:112] Iteration 149080, lr = 0.01
I0523 05:06:01.368060 35003 solver.cpp:239] Iteration 149090 (2.2316 iter/s, 4.48109s/10 iters), loss = 6.22834
I0523 05:06:01.368103 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22834 (* 1 = 6.22834 loss)
I0523 05:06:01.392877 35003 sgd_solver.cpp:112] Iteration 149090, lr = 0.01
I0523 05:06:05.768460 35003 solver.cpp:239] Iteration 149100 (2.27264 iter/s, 4.40017s/10 iters), loss = 6.4289
I0523 05:06:05.768494 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4289 (* 1 = 6.4289 loss)
I0523 05:06:05.776500 35003 sgd_solver.cpp:112] Iteration 149100, lr = 0.01
I0523 05:06:09.277004 35003 solver.cpp:239] Iteration 149110 (2.85034 iter/s, 3.50836s/10 iters), loss = 6.82866
I0523 05:06:09.277060 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82866 (* 1 = 6.82866 loss)
I0523 05:06:10.012058 35003 sgd_solver.cpp:112] Iteration 149110, lr = 0.01
I0523 05:06:14.939328 35003 solver.cpp:239] Iteration 149120 (1.76615 iter/s, 5.66204s/10 iters), loss = 6.04044
I0523 05:06:14.939388 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04044 (* 1 = 6.04044 loss)
I0523 05:06:15.654428 35003 sgd_solver.cpp:112] Iteration 149120, lr = 0.01
I0523 05:06:19.300163 35003 solver.cpp:239] Iteration 149130 (2.29326 iter/s, 4.3606s/10 iters), loss = 7.04311
I0523 05:06:19.300211 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04311 (* 1 = 7.04311 loss)
I0523 05:06:19.309986 35003 sgd_solver.cpp:112] Iteration 149130, lr = 0.01
I0523 05:06:22.871572 35003 solver.cpp:239] Iteration 149140 (2.80019 iter/s, 3.57118s/10 iters), loss = 8.21938
I0523 05:06:22.871776 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.21938 (* 1 = 8.21938 loss)
I0523 05:06:22.892154 35003 sgd_solver.cpp:112] Iteration 149140, lr = 0.01
I0523 05:06:25.385315 35003 solver.cpp:239] Iteration 149150 (3.97861 iter/s, 2.51344s/10 iters), loss = 7.22334
I0523 05:06:25.385363 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22334 (* 1 = 7.22334 loss)
I0523 05:06:25.397006 35003 sgd_solver.cpp:112] Iteration 149150, lr = 0.01
I0523 05:06:27.487190 35003 solver.cpp:239] Iteration 149160 (4.75798 iter/s, 2.10173s/10 iters), loss = 7.39169
I0523 05:06:27.487228 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39169 (* 1 = 7.39169 loss)
I0523 05:06:27.500672 35003 sgd_solver.cpp:112] Iteration 149160, lr = 0.01
I0523 05:06:30.144847 35003 solver.cpp:239] Iteration 149170 (3.76293 iter/s, 2.65751s/10 iters), loss = 6.87808
I0523 05:06:30.144894 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87808 (* 1 = 6.87808 loss)
I0523 05:06:30.271407 35003 sgd_solver.cpp:112] Iteration 149170, lr = 0.01
I0523 05:06:33.691366 35003 solver.cpp:239] Iteration 149180 (2.81982 iter/s, 3.54632s/10 iters), loss = 6.45028
I0523 05:06:33.691413 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45028 (* 1 = 6.45028 loss)
I0523 05:06:34.412904 35003 sgd_solver.cpp:112] Iteration 149180, lr = 0.01
I0523 05:06:36.987074 35003 solver.cpp:239] Iteration 149190 (3.03442 iter/s, 3.29552s/10 iters), loss = 5.64528
I0523 05:06:36.987124 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.64528 (* 1 = 5.64528 loss)
I0523 05:06:37.669972 35003 sgd_solver.cpp:112] Iteration 149190, lr = 0.01
I0523 05:06:40.792078 35003 solver.cpp:239] Iteration 149200 (2.62827 iter/s, 3.80478s/10 iters), loss = 6.22723
I0523 05:06:40.792131 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22723 (* 1 = 6.22723 loss)
I0523 05:06:41.532981 35003 sgd_solver.cpp:112] Iteration 149200, lr = 0.01
I0523 05:06:44.959472 35003 solver.cpp:239] Iteration 149210 (2.39971 iter/s, 4.16717s/10 iters), loss = 7.28318
I0523 05:06:44.959511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28318 (* 1 = 7.28318 loss)
I0523 05:06:44.962872 35003 sgd_solver.cpp:112] Iteration 149210, lr = 0.01
I0523 05:06:46.752460 35003 solver.cpp:239] Iteration 149220 (5.57769 iter/s, 1.79286s/10 iters), loss = 6.24948
I0523 05:06:46.752507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24948 (* 1 = 6.24948 loss)
I0523 05:06:47.487498 35003 sgd_solver.cpp:112] Iteration 149220, lr = 0.01
I0523 05:06:50.134115 35003 solver.cpp:239] Iteration 149230 (2.95729 iter/s, 3.38147s/10 iters), loss = 7.82117
I0523 05:06:50.134156 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82117 (* 1 = 7.82117 loss)
I0523 05:06:50.142185 35003 sgd_solver.cpp:112] Iteration 149230, lr = 0.01
I0523 05:06:53.786396 35003 solver.cpp:239] Iteration 149240 (2.73817 iter/s, 3.65208s/10 iters), loss = 7.86461
I0523 05:06:53.786741 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86461 (* 1 = 7.86461 loss)
I0523 05:06:53.798768 35003 sgd_solver.cpp:112] Iteration 149240, lr = 0.01
I0523 05:06:57.403224 35003 solver.cpp:239] Iteration 149250 (2.76519 iter/s, 3.61639s/10 iters), loss = 6.8429
I0523 05:06:57.403260 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8429 (* 1 = 6.8429 loss)
I0523 05:06:58.006620 35003 sgd_solver.cpp:112] Iteration 149250, lr = 0.01
I0523 05:07:01.019979 35003 solver.cpp:239] Iteration 149260 (2.76508 iter/s, 3.61654s/10 iters), loss = 6.16464
I0523 05:07:01.020021 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16464 (* 1 = 6.16464 loss)
I0523 05:07:01.033105 35003 sgd_solver.cpp:112] Iteration 149260, lr = 0.01
I0523 05:07:04.435083 35003 solver.cpp:239] Iteration 149270 (2.92832 iter/s, 3.41492s/10 iters), loss = 7.43459
I0523 05:07:04.435119 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43459 (* 1 = 7.43459 loss)
I0523 05:07:04.450996 35003 sgd_solver.cpp:112] Iteration 149270, lr = 0.01
I0523 05:07:07.911213 35003 solver.cpp:239] Iteration 149280 (2.87692 iter/s, 3.47594s/10 iters), loss = 6.25857
I0523 05:07:07.911264 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25857 (* 1 = 6.25857 loss)
I0523 05:07:08.619374 35003 sgd_solver.cpp:112] Iteration 149280, lr = 0.01
I0523 05:07:11.570350 35003 solver.cpp:239] Iteration 149290 (2.73304 iter/s, 3.65893s/10 iters), loss = 7.01199
I0523 05:07:11.570396 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01199 (* 1 = 7.01199 loss)
I0523 05:07:11.581521 35003 sgd_solver.cpp:112] Iteration 149290, lr = 0.01
I0523 05:07:15.292786 35003 solver.cpp:239] Iteration 149300 (2.68656 iter/s, 3.72224s/10 iters), loss = 5.6548
I0523 05:07:15.292829 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.6548 (* 1 = 5.6548 loss)
I0523 05:07:15.300384 35003 sgd_solver.cpp:112] Iteration 149300, lr = 0.01
I0523 05:07:18.091526 35003 solver.cpp:239] Iteration 149310 (3.57324 iter/s, 2.79858s/10 iters), loss = 7.0137
I0523 05:07:18.091567 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0137 (* 1 = 7.0137 loss)
I0523 05:07:18.763854 35003 sgd_solver.cpp:112] Iteration 149310, lr = 0.01
I0523 05:07:21.603114 35003 solver.cpp:239] Iteration 149320 (2.84787 iter/s, 3.5114s/10 iters), loss = 7.63301
I0523 05:07:21.603174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63301 (* 1 = 7.63301 loss)
I0523 05:07:22.310920 35003 sgd_solver.cpp:112] Iteration 149320, lr = 0.01
I0523 05:07:27.146121 35003 solver.cpp:239] Iteration 149330 (1.80417 iter/s, 5.54273s/10 iters), loss = 6.80158
I0523 05:07:27.146250 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80158 (* 1 = 6.80158 loss)
I0523 05:07:27.150784 35003 sgd_solver.cpp:112] Iteration 149330, lr = 0.01
I0523 05:07:30.448449 35003 solver.cpp:239] Iteration 149340 (3.02841 iter/s, 3.30207s/10 iters), loss = 6.67589
I0523 05:07:30.448489 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67589 (* 1 = 6.67589 loss)
I0523 05:07:30.452733 35003 sgd_solver.cpp:112] Iteration 149340, lr = 0.01
I0523 05:07:35.572989 35003 solver.cpp:239] Iteration 149350 (1.95149 iter/s, 5.12429s/10 iters), loss = 7.05541
I0523 05:07:35.573034 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05541 (* 1 = 7.05541 loss)
I0523 05:07:35.585175 35003 sgd_solver.cpp:112] Iteration 149350, lr = 0.01
I0523 05:07:38.424710 35003 solver.cpp:239] Iteration 149360 (3.50686 iter/s, 2.85156s/10 iters), loss = 7.43444
I0523 05:07:38.424755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43444 (* 1 = 7.43444 loss)
I0523 05:07:39.129953 35003 sgd_solver.cpp:112] Iteration 149360, lr = 0.01
I0523 05:07:41.974084 35003 solver.cpp:239] Iteration 149370 (2.81755 iter/s, 3.54918s/10 iters), loss = 6.4035
I0523 05:07:41.974126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4035 (* 1 = 6.4035 loss)
I0523 05:07:42.708633 35003 sgd_solver.cpp:112] Iteration 149370, lr = 0.01
I0523 05:07:44.761793 35003 solver.cpp:239] Iteration 149380 (3.58738 iter/s, 2.78755s/10 iters), loss = 7.24385
I0523 05:07:44.761837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24385 (* 1 = 7.24385 loss)
I0523 05:07:44.774276 35003 sgd_solver.cpp:112] Iteration 149380, lr = 0.01
I0523 05:07:49.766477 35003 solver.cpp:239] Iteration 149390 (1.99823 iter/s, 5.00443s/10 iters), loss = 7.02224
I0523 05:07:49.766530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02224 (* 1 = 7.02224 loss)
I0523 05:07:50.507340 35003 sgd_solver.cpp:112] Iteration 149390, lr = 0.01
I0523 05:07:55.613029 35003 solver.cpp:239] Iteration 149400 (1.7105 iter/s, 5.84626s/10 iters), loss = 6.90159
I0523 05:07:55.613080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90159 (* 1 = 6.90159 loss)
I0523 05:07:55.625833 35003 sgd_solver.cpp:112] Iteration 149400, lr = 0.01
I0523 05:08:00.876433 35003 solver.cpp:239] Iteration 149410 (1.90002 iter/s, 5.26309s/10 iters), loss = 6.35746
I0523 05:08:00.876601 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35746 (* 1 = 6.35746 loss)
I0523 05:08:00.889894 35003 sgd_solver.cpp:112] Iteration 149410, lr = 0.01
I0523 05:08:03.756064 35003 solver.cpp:239] Iteration 149420 (3.47303 iter/s, 2.87933s/10 iters), loss = 6.18599
I0523 05:08:03.756121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18599 (* 1 = 6.18599 loss)
I0523 05:08:03.786864 35003 sgd_solver.cpp:112] Iteration 149420, lr = 0.01
I0523 05:08:06.438890 35003 solver.cpp:239] Iteration 149430 (3.72765 iter/s, 2.68266s/10 iters), loss = 6.63386
I0523 05:08:06.438935 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63386 (* 1 = 6.63386 loss)
I0523 05:08:06.451565 35003 sgd_solver.cpp:112] Iteration 149430, lr = 0.01
I0523 05:08:09.417511 35003 solver.cpp:239] Iteration 149440 (3.35745 iter/s, 2.97845s/10 iters), loss = 7.90884
I0523 05:08:09.417553 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90884 (* 1 = 7.90884 loss)
I0523 05:08:09.425513 35003 sgd_solver.cpp:112] Iteration 149440, lr = 0.01
I0523 05:08:11.235059 35003 solver.cpp:239] Iteration 149450 (5.50231 iter/s, 1.81742s/10 iters), loss = 6.84018
I0523 05:08:11.235108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84018 (* 1 = 6.84018 loss)
I0523 05:08:11.248090 35003 sgd_solver.cpp:112] Iteration 149450, lr = 0.01
I0523 05:08:14.014242 35003 solver.cpp:239] Iteration 149460 (3.5984 iter/s, 2.77902s/10 iters), loss = 7.40951
I0523 05:08:14.014287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40951 (* 1 = 7.40951 loss)
I0523 05:08:14.027696 35003 sgd_solver.cpp:112] Iteration 149460, lr = 0.01
I0523 05:08:17.508222 35003 solver.cpp:239] Iteration 149470 (2.86222 iter/s, 3.49379s/10 iters), loss = 7.85675
I0523 05:08:17.508270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85675 (* 1 = 7.85675 loss)
I0523 05:08:18.235726 35003 sgd_solver.cpp:112] Iteration 149470, lr = 0.01
I0523 05:08:21.955219 35003 solver.cpp:239] Iteration 149480 (2.24882 iter/s, 4.44677s/10 iters), loss = 6.90047
I0523 05:08:21.955271 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90047 (* 1 = 6.90047 loss)
I0523 05:08:21.960026 35003 sgd_solver.cpp:112] Iteration 149480, lr = 0.01
I0523 05:08:24.725313 35003 solver.cpp:239] Iteration 149490 (3.61022 iter/s, 2.76991s/10 iters), loss = 6.97444
I0523 05:08:24.725358 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97444 (* 1 = 6.97444 loss)
I0523 05:08:24.738433 35003 sgd_solver.cpp:112] Iteration 149490, lr = 0.01
I0523 05:08:28.751813 35003 solver.cpp:239] Iteration 149500 (2.48367 iter/s, 4.02629s/10 iters), loss = 7.32011
I0523 05:08:28.751857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32011 (* 1 = 7.32011 loss)
I0523 05:08:28.881680 35003 sgd_solver.cpp:112] Iteration 149500, lr = 0.01
I0523 05:08:31.073482 35003 solver.cpp:239] Iteration 149510 (4.30752 iter/s, 2.32152s/10 iters), loss = 7.80756
I0523 05:08:31.073671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80756 (* 1 = 7.80756 loss)
I0523 05:08:31.807904 35003 sgd_solver.cpp:112] Iteration 149510, lr = 0.01
I0523 05:08:34.650940 35003 solver.cpp:239] Iteration 149520 (2.79555 iter/s, 3.57712s/10 iters), loss = 6.89962
I0523 05:08:34.650987 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89962 (* 1 = 6.89962 loss)
I0523 05:08:35.027572 35003 sgd_solver.cpp:112] Iteration 149520, lr = 0.01
I0523 05:08:37.680274 35003 solver.cpp:239] Iteration 149530 (3.30125 iter/s, 3.02916s/10 iters), loss = 6.99075
I0523 05:08:37.680318 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99075 (* 1 = 6.99075 loss)
I0523 05:08:37.691447 35003 sgd_solver.cpp:112] Iteration 149530, lr = 0.01
I0523 05:08:39.713466 35003 solver.cpp:239] Iteration 149540 (4.91871 iter/s, 2.03305s/10 iters), loss = 6.54529
I0523 05:08:39.713510 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54529 (* 1 = 6.54529 loss)
I0523 05:08:39.738526 35003 sgd_solver.cpp:112] Iteration 149540, lr = 0.01
I0523 05:08:44.531576 35003 solver.cpp:239] Iteration 149550 (2.07561 iter/s, 4.81786s/10 iters), loss = 7.0123
I0523 05:08:44.531638 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0123 (* 1 = 7.0123 loss)
I0523 05:08:44.774585 35003 sgd_solver.cpp:112] Iteration 149550, lr = 0.01
I0523 05:08:47.665083 35003 solver.cpp:239] Iteration 149560 (3.19151 iter/s, 3.13331s/10 iters), loss = 7.79003
I0523 05:08:47.665140 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79003 (* 1 = 7.79003 loss)
I0523 05:08:48.406255 35003 sgd_solver.cpp:112] Iteration 149560, lr = 0.01
I0523 05:08:52.090349 35003 solver.cpp:239] Iteration 149570 (2.25987 iter/s, 4.42503s/10 iters), loss = 6.22941
I0523 05:08:52.090385 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22941 (* 1 = 6.22941 loss)
I0523 05:08:52.098780 35003 sgd_solver.cpp:112] Iteration 149570, lr = 0.01
I0523 05:08:55.482409 35003 solver.cpp:239] Iteration 149580 (2.94822 iter/s, 3.39188s/10 iters), loss = 7.03643
I0523 05:08:55.482458 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03643 (* 1 = 7.03643 loss)
I0523 05:08:56.223156 35003 sgd_solver.cpp:112] Iteration 149580, lr = 0.01
I0523 05:08:59.857089 35003 solver.cpp:239] Iteration 149590 (2.28601 iter/s, 4.37444s/10 iters), loss = 6.54671
I0523 05:08:59.857151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54671 (* 1 = 6.54671 loss)
I0523 05:09:00.588234 35003 sgd_solver.cpp:112] Iteration 149590, lr = 0.01
I0523 05:09:02.682831 35003 solver.cpp:239] Iteration 149600 (3.53913 iter/s, 2.82555s/10 iters), loss = 5.77132
I0523 05:09:02.683126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77132 (* 1 = 5.77132 loss)
I0523 05:09:03.411244 35003 sgd_solver.cpp:112] Iteration 149600, lr = 0.01
I0523 05:09:04.757807 35003 solver.cpp:239] Iteration 149610 (4.82017 iter/s, 2.07461s/10 iters), loss = 6.4381
I0523 05:09:04.757861 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4381 (* 1 = 6.4381 loss)
I0523 05:09:05.485520 35003 sgd_solver.cpp:112] Iteration 149610, lr = 0.01
I0523 05:09:10.169517 35003 solver.cpp:239] Iteration 149620 (1.84794 iter/s, 5.41143s/10 iters), loss = 6.0676
I0523 05:09:10.169587 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0676 (* 1 = 6.0676 loss)
I0523 05:09:10.865633 35003 sgd_solver.cpp:112] Iteration 149620, lr = 0.01
I0523 05:09:13.454030 35003 solver.cpp:239] Iteration 149630 (3.04479 iter/s, 3.2843s/10 iters), loss = 6.40124
I0523 05:09:13.454092 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40124 (* 1 = 6.40124 loss)
I0523 05:09:14.173568 35003 sgd_solver.cpp:112] Iteration 149630, lr = 0.01
I0523 05:09:17.616832 35003 solver.cpp:239] Iteration 149640 (2.40236 iter/s, 4.16257s/10 iters), loss = 7.28176
I0523 05:09:17.616878 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28176 (* 1 = 7.28176 loss)
I0523 05:09:17.629593 35003 sgd_solver.cpp:112] Iteration 149640, lr = 0.01
I0523 05:09:19.002341 35003 solver.cpp:239] Iteration 149650 (7.21816 iter/s, 1.3854s/10 iters), loss = 6.99356
I0523 05:09:19.002389 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99356 (* 1 = 6.99356 loss)
I0523 05:09:19.229059 35003 sgd_solver.cpp:112] Iteration 149650, lr = 0.01
I0523 05:09:22.804227 35003 solver.cpp:239] Iteration 149660 (2.63041 iter/s, 3.80168s/10 iters), loss = 7.35216
I0523 05:09:22.804266 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35216 (* 1 = 7.35216 loss)
I0523 05:09:22.817726 35003 sgd_solver.cpp:112] Iteration 149660, lr = 0.01
I0523 05:09:24.536463 35003 solver.cpp:239] Iteration 149670 (5.77328 iter/s, 1.73212s/10 iters), loss = 7.45872
I0523 05:09:24.536505 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45872 (* 1 = 7.45872 loss)
I0523 05:09:25.155437 35003 sgd_solver.cpp:112] Iteration 149670, lr = 0.01
I0523 05:09:28.230584 35003 solver.cpp:239] Iteration 149680 (2.70715 iter/s, 3.69392s/10 iters), loss = 5.61304
I0523 05:09:28.230638 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.61304 (* 1 = 5.61304 loss)
I0523 05:09:28.234338 35003 sgd_solver.cpp:112] Iteration 149680, lr = 0.01
I0523 05:09:31.169046 35003 solver.cpp:239] Iteration 149690 (3.40334 iter/s, 2.93829s/10 iters), loss = 7.20423
I0523 05:09:31.169090 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20423 (* 1 = 7.20423 loss)
I0523 05:09:31.910722 35003 sgd_solver.cpp:112] Iteration 149690, lr = 0.01
I0523 05:09:36.845666 35003 solver.cpp:239] Iteration 149700 (1.7617 iter/s, 5.67634s/10 iters), loss = 6.21778
I0523 05:09:36.845800 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21778 (* 1 = 6.21778 loss)
I0523 05:09:37.061986 35003 sgd_solver.cpp:112] Iteration 149700, lr = 0.01
I0523 05:09:39.708773 35003 solver.cpp:239] Iteration 149710 (3.49302 iter/s, 2.86285s/10 iters), loss = 7.88936
I0523 05:09:39.708812 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88936 (* 1 = 7.88936 loss)
I0523 05:09:39.727322 35003 sgd_solver.cpp:112] Iteration 149710, lr = 0.01
I0523 05:09:42.543090 35003 solver.cpp:239] Iteration 149720 (3.52839 iter/s, 2.83415s/10 iters), loss = 6.76435
I0523 05:09:42.543135 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76435 (* 1 = 6.76435 loss)
I0523 05:09:43.238778 35003 sgd_solver.cpp:112] Iteration 149720, lr = 0.01
I0523 05:09:47.670330 35003 solver.cpp:239] Iteration 149730 (1.95046 iter/s, 5.12699s/10 iters), loss = 7.02775
I0523 05:09:47.670366 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02775 (* 1 = 7.02775 loss)
I0523 05:09:47.674255 35003 sgd_solver.cpp:112] Iteration 149730, lr = 0.01
I0523 05:09:50.234889 35003 solver.cpp:239] Iteration 149740 (3.89954 iter/s, 2.5644s/10 iters), loss = 6.25813
I0523 05:09:50.234935 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25813 (* 1 = 6.25813 loss)
I0523 05:09:50.247870 35003 sgd_solver.cpp:112] Iteration 149740, lr = 0.01
I0523 05:09:53.147469 35003 solver.cpp:239] Iteration 149750 (3.43358 iter/s, 2.91242s/10 iters), loss = 6.4255
I0523 05:09:53.147511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4255 (* 1 = 6.4255 loss)
I0523 05:09:53.160388 35003 sgd_solver.cpp:112] Iteration 149750, lr = 0.01
I0523 05:09:56.897106 35003 solver.cpp:239] Iteration 149760 (2.66706 iter/s, 3.74944s/10 iters), loss = 7.08341
I0523 05:09:56.897146 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08341 (* 1 = 7.08341 loss)
I0523 05:09:57.638643 35003 sgd_solver.cpp:112] Iteration 149760, lr = 0.01
I0523 05:10:00.538506 35003 solver.cpp:239] Iteration 149770 (2.74634 iter/s, 3.64121s/10 iters), loss = 6.13591
I0523 05:10:00.538552 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13591 (* 1 = 6.13591 loss)
I0523 05:10:00.551872 35003 sgd_solver.cpp:112] Iteration 149770, lr = 0.01
I0523 05:10:03.332355 35003 solver.cpp:239] Iteration 149780 (3.5795 iter/s, 2.79368s/10 iters), loss = 6.58811
I0523 05:10:03.332396 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58811 (* 1 = 6.58811 loss)
I0523 05:10:03.340353 35003 sgd_solver.cpp:112] Iteration 149780, lr = 0.01
I0523 05:10:06.795812 35003 solver.cpp:239] Iteration 149790 (2.88744 iter/s, 3.46327s/10 iters), loss = 6.31972
I0523 05:10:06.795852 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31972 (* 1 = 6.31972 loss)
I0523 05:10:06.809198 35003 sgd_solver.cpp:112] Iteration 149790, lr = 0.01
I0523 05:10:10.166393 35003 solver.cpp:239] Iteration 149800 (2.96701 iter/s, 3.3704s/10 iters), loss = 7.81007
I0523 05:10:10.166621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81007 (* 1 = 7.81007 loss)
I0523 05:10:10.184450 35003 sgd_solver.cpp:112] Iteration 149800, lr = 0.01
I0523 05:10:12.950353 35003 solver.cpp:239] Iteration 149810 (3.59243 iter/s, 2.78363s/10 iters), loss = 6.93212
I0523 05:10:12.950407 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93212 (* 1 = 6.93212 loss)
I0523 05:10:13.658758 35003 sgd_solver.cpp:112] Iteration 149810, lr = 0.01
I0523 05:10:17.177480 35003 solver.cpp:239] Iteration 149820 (2.3658 iter/s, 4.2269s/10 iters), loss = 8.05867
I0523 05:10:17.177531 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05867 (* 1 = 8.05867 loss)
I0523 05:10:17.179898 35003 sgd_solver.cpp:112] Iteration 149820, lr = 0.01
I0523 05:10:19.928051 35003 solver.cpp:239] Iteration 149830 (3.63584 iter/s, 2.75039s/10 iters), loss = 7.88138
I0523 05:10:19.928097 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88138 (* 1 = 7.88138 loss)
I0523 05:10:19.937598 35003 sgd_solver.cpp:112] Iteration 149830, lr = 0.01
I0523 05:10:22.954625 35003 solver.cpp:239] Iteration 149840 (3.30426 iter/s, 3.0264s/10 iters), loss = 8.00758
I0523 05:10:22.954669 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00758 (* 1 = 8.00758 loss)
I0523 05:10:22.962857 35003 sgd_solver.cpp:112] Iteration 149840, lr = 0.01
I0523 05:10:25.804996 35003 solver.cpp:239] Iteration 149850 (3.50852 iter/s, 2.8502s/10 iters), loss = 7.66834
I0523 05:10:25.805042 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66834 (* 1 = 7.66834 loss)
I0523 05:10:26.447685 35003 sgd_solver.cpp:112] Iteration 149850, lr = 0.01
I0523 05:10:28.743052 35003 solver.cpp:239] Iteration 149860 (3.40381 iter/s, 2.93788s/10 iters), loss = 7.62877
I0523 05:10:28.743094 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62877 (* 1 = 7.62877 loss)
I0523 05:10:28.748634 35003 sgd_solver.cpp:112] Iteration 149860, lr = 0.01
I0523 05:10:32.409881 35003 solver.cpp:239] Iteration 149870 (2.72729 iter/s, 3.66664s/10 iters), loss = 7.22091
I0523 05:10:32.409934 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22091 (* 1 = 7.22091 loss)
I0523 05:10:32.434896 35003 sgd_solver.cpp:112] Iteration 149870, lr = 0.01
I0523 05:10:35.994679 35003 solver.cpp:239] Iteration 149880 (2.78971 iter/s, 3.5846s/10 iters), loss = 8.39255
I0523 05:10:35.994756 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.39255 (* 1 = 8.39255 loss)
I0523 05:10:36.000169 35003 sgd_solver.cpp:112] Iteration 149880, lr = 0.01
I0523 05:10:38.840538 35003 solver.cpp:239] Iteration 149890 (3.51412 iter/s, 2.84566s/10 iters), loss = 7.56857
I0523 05:10:38.840577 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56857 (* 1 = 7.56857 loss)
I0523 05:10:38.866947 35003 sgd_solver.cpp:112] Iteration 149890, lr = 0.01
I0523 05:10:42.636795 35003 solver.cpp:239] Iteration 149900 (2.63431 iter/s, 3.79605s/10 iters), loss = 7.47564
I0523 05:10:42.637073 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47564 (* 1 = 7.47564 loss)
I0523 05:10:42.962491 35003 sgd_solver.cpp:112] Iteration 149900, lr = 0.01
I0523 05:10:47.170843 35003 solver.cpp:239] Iteration 149910 (2.20575 iter/s, 4.53361s/10 iters), loss = 7.78456
I0523 05:10:47.170886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78456 (* 1 = 7.78456 loss)
I0523 05:10:47.178787 35003 sgd_solver.cpp:112] Iteration 149910, lr = 0.01
I0523 05:10:49.240509 35003 solver.cpp:239] Iteration 149920 (4.83202 iter/s, 2.06953s/10 iters), loss = 6.51703
I0523 05:10:49.240557 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51703 (* 1 = 6.51703 loss)
I0523 05:10:49.250682 35003 sgd_solver.cpp:112] Iteration 149920, lr = 0.01
I0523 05:10:52.920506 35003 solver.cpp:239] Iteration 149930 (2.71754 iter/s, 3.67979s/10 iters), loss = 7.78355
I0523 05:10:52.920552 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78355 (* 1 = 7.78355 loss)
I0523 05:10:53.575755 35003 sgd_solver.cpp:112] Iteration 149930, lr = 0.01
I0523 05:10:56.096370 35003 solver.cpp:239] Iteration 149940 (3.14893 iter/s, 3.17568s/10 iters), loss = 7.46683
I0523 05:10:56.096411 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46683 (* 1 = 7.46683 loss)
I0523 05:10:56.110234 35003 sgd_solver.cpp:112] Iteration 149940, lr = 0.01
I0523 05:10:59.733105 35003 solver.cpp:239] Iteration 149950 (2.74988 iter/s, 3.63653s/10 iters), loss = 7.06027
I0523 05:10:59.733152 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06027 (* 1 = 7.06027 loss)
I0523 05:11:00.253401 35003 sgd_solver.cpp:112] Iteration 149950, lr = 0.01
I0523 05:11:03.828809 35003 solver.cpp:239] Iteration 149960 (2.44171 iter/s, 4.09549s/10 iters), loss = 6.73288
I0523 05:11:03.828853 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73288 (* 1 = 6.73288 loss)
I0523 05:11:03.836545 35003 sgd_solver.cpp:112] Iteration 149960, lr = 0.01
I0523 05:11:07.520637 35003 solver.cpp:239] Iteration 149970 (2.70883 iter/s, 3.69163s/10 iters), loss = 7.34194
I0523 05:11:07.520691 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34194 (* 1 = 7.34194 loss)
I0523 05:11:08.248390 35003 sgd_solver.cpp:112] Iteration 149970, lr = 0.01
I0523 05:11:11.046247 35003 solver.cpp:239] Iteration 149980 (2.83655 iter/s, 3.52541s/10 iters), loss = 6.50807
I0523 05:11:11.046289 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50807 (* 1 = 6.50807 loss)
I0523 05:11:11.201699 35003 sgd_solver.cpp:112] Iteration 149980, lr = 0.01
I0523 05:11:15.255661 35003 solver.cpp:239] Iteration 149990 (2.37575 iter/s, 4.2092s/10 iters), loss = 7.57044
I0523 05:11:15.255880 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57044 (* 1 = 7.57044 loss)
I0523 05:11:15.261430 35003 sgd_solver.cpp:112] Iteration 149990, lr = 0.01
I0523 05:11:18.664029 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_150000.caffemodel
I0523 05:11:18.824690 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_150000.solverstate
I0523 05:11:19.035356 35003 solver.cpp:239] Iteration 150000 (2.64596 iter/s, 3.77934s/10 iters), loss = 7.68325
I0523 05:11:19.035406 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68325 (* 1 = 7.68325 loss)
I0523 05:11:19.750226 35143 sgd_solver.cpp:50] MultiStep Status: Iteration 150000, step = 1
I0523 05:11:19.754528 35144 sgd_solver.cpp:50] MultiStep Status: Iteration 150000, step = 1
I0523 05:11:19.754534 35145 sgd_solver.cpp:50] MultiStep Status: Iteration 150000, step = 1
I0523 05:11:19.756680 35003 sgd_solver.cpp:50] MultiStep Status: Iteration 150000, step = 1
I0523 05:11:19.756696 35003 sgd_solver.cpp:112] Iteration 150000, lr = 0.001
I0523 05:11:22.882447 35003 solver.cpp:239] Iteration 150010 (2.59951 iter/s, 3.84688s/10 iters), loss = 6.31007
I0523 05:11:22.882500 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31007 (* 1 = 6.31007 loss)
I0523 05:11:22.892004 35003 sgd_solver.cpp:112] Iteration 150010, lr = 0.001
I0523 05:11:25.751456 35003 solver.cpp:239] Iteration 150020 (3.48573 iter/s, 2.86884s/10 iters), loss = 7.93947
I0523 05:11:25.751513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93947 (* 1 = 7.93947 loss)
I0523 05:11:25.761564 35003 sgd_solver.cpp:112] Iteration 150020, lr = 0.001
I0523 05:11:29.210196 35003 solver.cpp:239] Iteration 150030 (2.89139 iter/s, 3.45854s/10 iters), loss = 6.36686
I0523 05:11:29.210253 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36686 (* 1 = 6.36686 loss)
I0523 05:11:29.925568 35003 sgd_solver.cpp:112] Iteration 150030, lr = 0.001
I0523 05:11:32.777761 35003 solver.cpp:239] Iteration 150040 (2.8032 iter/s, 3.56735s/10 iters), loss = 6.48322
I0523 05:11:32.777823 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48322 (* 1 = 6.48322 loss)
I0523 05:11:32.783622 35003 sgd_solver.cpp:112] Iteration 150040, lr = 0.001
I0523 05:11:35.637642 35003 solver.cpp:239] Iteration 150050 (3.49687 iter/s, 2.8597s/10 iters), loss = 7.07224
I0523 05:11:35.637681 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07224 (* 1 = 7.07224 loss)
I0523 05:11:35.642453 35003 sgd_solver.cpp:112] Iteration 150050, lr = 0.001
I0523 05:11:38.480983 35003 solver.cpp:239] Iteration 150060 (3.51719 iter/s, 2.84317s/10 iters), loss = 7.38998
I0523 05:11:38.481029 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38998 (* 1 = 7.38998 loss)
I0523 05:11:39.196475 35003 sgd_solver.cpp:112] Iteration 150060, lr = 0.001
I0523 05:11:42.042656 35003 solver.cpp:239] Iteration 150070 (2.80783 iter/s, 3.56147s/10 iters), loss = 6.78295
I0523 05:11:42.042754 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78295 (* 1 = 6.78295 loss)
I0523 05:11:42.050664 35003 sgd_solver.cpp:112] Iteration 150070, lr = 0.001
I0523 05:11:44.856883 35003 solver.cpp:239] Iteration 150080 (3.55365 iter/s, 2.81401s/10 iters), loss = 6.71751
I0523 05:11:44.856925 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71751 (* 1 = 6.71751 loss)
I0523 05:11:44.864439 35003 sgd_solver.cpp:112] Iteration 150080, lr = 0.001
I0523 05:11:47.663398 35003 solver.cpp:239] Iteration 150090 (3.56336 iter/s, 2.80634s/10 iters), loss = 5.77843
I0523 05:11:47.663599 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77843 (* 1 = 5.77843 loss)
I0523 05:11:48.384604 35003 sgd_solver.cpp:112] Iteration 150090, lr = 0.001
I0523 05:11:52.892155 35003 solver.cpp:239] Iteration 150100 (1.91265 iter/s, 5.22835s/10 iters), loss = 7.5331
I0523 05:11:52.892206 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5331 (* 1 = 7.5331 loss)
I0523 05:11:52.905478 35003 sgd_solver.cpp:112] Iteration 150100, lr = 0.001
I0523 05:11:56.338403 35003 solver.cpp:239] Iteration 150110 (2.90187 iter/s, 3.44605s/10 iters), loss = 6.5481
I0523 05:11:56.338464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5481 (* 1 = 6.5481 loss)
I0523 05:11:56.417860 35003 sgd_solver.cpp:112] Iteration 150110, lr = 0.001
I0523 05:12:00.132138 35003 solver.cpp:239] Iteration 150120 (2.63608 iter/s, 3.79351s/10 iters), loss = 7.05579
I0523 05:12:00.132210 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05579 (* 1 = 7.05579 loss)
I0523 05:12:00.144680 35003 sgd_solver.cpp:112] Iteration 150120, lr = 0.001
I0523 05:12:02.935648 35003 solver.cpp:239] Iteration 150130 (3.56719 iter/s, 2.80332s/10 iters), loss = 7.76203
I0523 05:12:02.935696 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76203 (* 1 = 7.76203 loss)
I0523 05:12:03.670848 35003 sgd_solver.cpp:112] Iteration 150130, lr = 0.001
I0523 05:12:07.120365 35003 solver.cpp:239] Iteration 150140 (2.38978 iter/s, 4.18449s/10 iters), loss = 5.73842
I0523 05:12:07.120415 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.73842 (* 1 = 5.73842 loss)
I0523 05:12:07.529546 35003 sgd_solver.cpp:112] Iteration 150140, lr = 0.001
I0523 05:12:10.348878 35003 solver.cpp:239] Iteration 150150 (3.09758 iter/s, 3.22833s/10 iters), loss = 7.69327
I0523 05:12:10.348919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69327 (* 1 = 7.69327 loss)
I0523 05:12:10.805079 35003 sgd_solver.cpp:112] Iteration 150150, lr = 0.001
I0523 05:12:13.084481 35003 solver.cpp:239] Iteration 150160 (3.65572 iter/s, 2.73544s/10 iters), loss = 6.53405
I0523 05:12:13.084529 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53405 (* 1 = 6.53405 loss)
I0523 05:12:13.098345 35003 sgd_solver.cpp:112] Iteration 150160, lr = 0.001
I0523 05:12:15.802541 35003 solver.cpp:239] Iteration 150170 (3.67932 iter/s, 2.71789s/10 iters), loss = 6.14041
I0523 05:12:15.802598 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14041 (* 1 = 6.14041 loss)
I0523 05:12:15.820654 35003 sgd_solver.cpp:112] Iteration 150170, lr = 0.001
I0523 05:12:18.073037 35003 solver.cpp:239] Iteration 150180 (4.40462 iter/s, 2.27034s/10 iters), loss = 7.08669
I0523 05:12:18.073307 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08669 (* 1 = 7.08669 loss)
I0523 05:12:18.078330 35003 sgd_solver.cpp:112] Iteration 150180, lr = 0.001
I0523 05:12:21.312633 35003 solver.cpp:239] Iteration 150190 (3.08717 iter/s, 3.23921s/10 iters), loss = 6.49128
I0523 05:12:21.312683 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49128 (* 1 = 6.49128 loss)
I0523 05:12:22.027063 35003 sgd_solver.cpp:112] Iteration 150190, lr = 0.001
I0523 05:12:24.902133 35003 solver.cpp:239] Iteration 150200 (2.78606 iter/s, 3.5893s/10 iters), loss = 6.75155
I0523 05:12:24.902180 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75155 (* 1 = 6.75155 loss)
I0523 05:12:24.915369 35003 sgd_solver.cpp:112] Iteration 150200, lr = 0.001
I0523 05:12:29.190397 35003 solver.cpp:239] Iteration 150210 (2.33207 iter/s, 4.28803s/10 iters), loss = 8.00712
I0523 05:12:29.190465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00712 (* 1 = 8.00712 loss)
I0523 05:12:29.236057 35003 sgd_solver.cpp:112] Iteration 150210, lr = 0.001
I0523 05:12:32.106680 35003 solver.cpp:239] Iteration 150220 (3.42924 iter/s, 2.91609s/10 iters), loss = 5.29357
I0523 05:12:32.106760 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.29357 (* 1 = 5.29357 loss)
I0523 05:12:32.841255 35003 sgd_solver.cpp:112] Iteration 150220, lr = 0.001
I0523 05:12:35.639786 35003 solver.cpp:239] Iteration 150230 (2.83055 iter/s, 3.53288s/10 iters), loss = 6.57047
I0523 05:12:35.639830 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57047 (* 1 = 6.57047 loss)
I0523 05:12:36.344789 35003 sgd_solver.cpp:112] Iteration 150230, lr = 0.001
I0523 05:12:39.811827 35003 solver.cpp:239] Iteration 150240 (2.39703 iter/s, 4.17183s/10 iters), loss = 6.36058
I0523 05:12:39.811868 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36058 (* 1 = 6.36058 loss)
I0523 05:12:40.388396 35003 sgd_solver.cpp:112] Iteration 150240, lr = 0.001
I0523 05:12:44.406674 35003 solver.cpp:239] Iteration 150250 (2.17646 iter/s, 4.59462s/10 iters), loss = 7.84129
I0523 05:12:44.406728 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84129 (* 1 = 7.84129 loss)
I0523 05:12:45.135044 35003 sgd_solver.cpp:112] Iteration 150250, lr = 0.001
I0523 05:12:48.797123 35003 solver.cpp:239] Iteration 150260 (2.27779 iter/s, 4.39021s/10 iters), loss = 6.51627
I0523 05:12:48.797332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51627 (* 1 = 6.51627 loss)
I0523 05:12:49.493050 35003 sgd_solver.cpp:112] Iteration 150260, lr = 0.001
I0523 05:12:52.384938 35003 solver.cpp:239] Iteration 150270 (2.78749 iter/s, 3.58746s/10 iters), loss = 6.44982
I0523 05:12:52.384984 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44982 (* 1 = 6.44982 loss)
I0523 05:12:53.119910 35003 sgd_solver.cpp:112] Iteration 150270, lr = 0.001
I0523 05:12:57.916390 35003 solver.cpp:239] Iteration 150280 (1.80793 iter/s, 5.53118s/10 iters), loss = 6.53836
I0523 05:12:57.916431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53836 (* 1 = 6.53836 loss)
I0523 05:12:58.468104 35003 sgd_solver.cpp:112] Iteration 150280, lr = 0.001
I0523 05:13:02.063515 35003 solver.cpp:239] Iteration 150290 (2.41143 iter/s, 4.14691s/10 iters), loss = 7.86049
I0523 05:13:02.063560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86049 (* 1 = 7.86049 loss)
I0523 05:13:02.073074 35003 sgd_solver.cpp:112] Iteration 150290, lr = 0.001
I0523 05:13:04.905572 35003 solver.cpp:239] Iteration 150300 (3.51879 iter/s, 2.84189s/10 iters), loss = 7.00532
I0523 05:13:04.905627 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00532 (* 1 = 7.00532 loss)
I0523 05:13:05.630975 35003 sgd_solver.cpp:112] Iteration 150300, lr = 0.001
I0523 05:13:08.429953 35003 solver.cpp:239] Iteration 150310 (2.83754 iter/s, 3.52418s/10 iters), loss = 7.10529
I0523 05:13:08.430001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10529 (* 1 = 7.10529 loss)
I0523 05:13:08.442937 35003 sgd_solver.cpp:112] Iteration 150310, lr = 0.001
I0523 05:13:10.978669 35003 solver.cpp:239] Iteration 150320 (3.92378 iter/s, 2.54856s/10 iters), loss = 7.36258
I0523 05:13:10.978747 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36258 (* 1 = 7.36258 loss)
I0523 05:13:11.279841 35003 sgd_solver.cpp:112] Iteration 150320, lr = 0.001
I0523 05:13:14.962920 35003 solver.cpp:239] Iteration 150330 (2.51004 iter/s, 3.98401s/10 iters), loss = 7.72996
I0523 05:13:14.962966 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72996 (* 1 = 7.72996 loss)
I0523 05:13:15.048507 35003 sgd_solver.cpp:112] Iteration 150330, lr = 0.001
I0523 05:13:18.524905 35003 solver.cpp:239] Iteration 150340 (2.80758 iter/s, 3.56179s/10 iters), loss = 6.56644
I0523 05:13:18.524948 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56644 (* 1 = 6.56644 loss)
I0523 05:13:18.532459 35003 sgd_solver.cpp:112] Iteration 150340, lr = 0.001
I0523 05:13:21.577921 35003 solver.cpp:239] Iteration 150350 (3.27564 iter/s, 3.05284s/10 iters), loss = 5.93439
I0523 05:13:21.578094 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93439 (* 1 = 5.93439 loss)
I0523 05:13:21.591176 35003 sgd_solver.cpp:112] Iteration 150350, lr = 0.001
I0523 05:13:23.440979 35003 solver.cpp:239] Iteration 150360 (5.36822 iter/s, 1.86282s/10 iters), loss = 6.92965
I0523 05:13:23.441015 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92965 (* 1 = 6.92965 loss)
I0523 05:13:23.458870 35003 sgd_solver.cpp:112] Iteration 150360, lr = 0.001
I0523 05:13:26.361699 35003 solver.cpp:239] Iteration 150370 (3.42401 iter/s, 2.92056s/10 iters), loss = 7.64461
I0523 05:13:26.361747 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64461 (* 1 = 7.64461 loss)
I0523 05:13:27.062412 35003 sgd_solver.cpp:112] Iteration 150370, lr = 0.001
I0523 05:13:30.511559 35003 solver.cpp:239] Iteration 150380 (2.40984 iter/s, 4.14965s/10 iters), loss = 7.02865
I0523 05:13:30.511595 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02865 (* 1 = 7.02865 loss)
I0523 05:13:30.524971 35003 sgd_solver.cpp:112] Iteration 150380, lr = 0.001
I0523 05:13:34.108526 35003 solver.cpp:239] Iteration 150390 (2.78026 iter/s, 3.59678s/10 iters), loss = 7.83105
I0523 05:13:34.108561 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83105 (* 1 = 7.83105 loss)
I0523 05:13:34.116466 35003 sgd_solver.cpp:112] Iteration 150390, lr = 0.001
I0523 05:13:36.590546 35003 solver.cpp:239] Iteration 150400 (4.02922 iter/s, 2.48187s/10 iters), loss = 5.50125
I0523 05:13:36.590585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.50125 (* 1 = 5.50125 loss)
I0523 05:13:36.603801 35003 sgd_solver.cpp:112] Iteration 150400, lr = 0.001
I0523 05:13:40.262660 35003 solver.cpp:239] Iteration 150410 (2.72341 iter/s, 3.67186s/10 iters), loss = 6.07095
I0523 05:13:40.262758 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07095 (* 1 = 6.07095 loss)
I0523 05:13:40.270314 35003 sgd_solver.cpp:112] Iteration 150410, lr = 0.001
I0523 05:13:42.321563 35003 solver.cpp:239] Iteration 150420 (4.85741 iter/s, 2.05871s/10 iters), loss = 6.25124
I0523 05:13:42.321624 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25124 (* 1 = 6.25124 loss)
I0523 05:13:42.331351 35003 sgd_solver.cpp:112] Iteration 150420, lr = 0.001
I0523 05:13:45.825776 35003 solver.cpp:239] Iteration 150430 (2.85388 iter/s, 3.504s/10 iters), loss = 5.89488
I0523 05:13:45.825836 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89488 (* 1 = 5.89488 loss)
I0523 05:13:45.838837 35003 sgd_solver.cpp:112] Iteration 150430, lr = 0.001
I0523 05:13:48.743139 35003 solver.cpp:239] Iteration 150440 (3.42797 iter/s, 2.91718s/10 iters), loss = 6.15411
I0523 05:13:48.743185 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15411 (* 1 = 6.15411 loss)
I0523 05:13:48.755278 35003 sgd_solver.cpp:112] Iteration 150440, lr = 0.001
I0523 05:13:51.172219 35003 solver.cpp:239] Iteration 150450 (4.11704 iter/s, 2.42893s/10 iters), loss = 6.33856
I0523 05:13:51.172255 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33856 (* 1 = 6.33856 loss)
I0523 05:13:51.180399 35003 sgd_solver.cpp:112] Iteration 150450, lr = 0.001
I0523 05:13:54.242385 35003 solver.cpp:239] Iteration 150460 (3.25733 iter/s, 3.07s/10 iters), loss = 7.11391
I0523 05:13:54.242638 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11391 (* 1 = 7.11391 loss)
I0523 05:13:54.957815 35003 sgd_solver.cpp:112] Iteration 150460, lr = 0.001
I0523 05:13:59.520929 35003 solver.cpp:239] Iteration 150470 (1.89465 iter/s, 5.27802s/10 iters), loss = 7.90577
I0523 05:13:59.520983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90577 (* 1 = 7.90577 loss)
I0523 05:13:59.529000 35003 sgd_solver.cpp:112] Iteration 150470, lr = 0.001
I0523 05:14:02.382380 35003 solver.cpp:239] Iteration 150480 (3.49496 iter/s, 2.86127s/10 iters), loss = 6.34513
I0523 05:14:02.382442 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34513 (* 1 = 6.34513 loss)
I0523 05:14:02.388975 35003 sgd_solver.cpp:112] Iteration 150480, lr = 0.001
I0523 05:14:05.247675 35003 solver.cpp:239] Iteration 150490 (3.49026 iter/s, 2.86512s/10 iters), loss = 6.62251
I0523 05:14:05.247717 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62251 (* 1 = 6.62251 loss)
I0523 05:14:05.270711 35003 sgd_solver.cpp:112] Iteration 150490, lr = 0.001
I0523 05:14:09.025131 35003 solver.cpp:239] Iteration 150500 (2.64743 iter/s, 3.77725s/10 iters), loss = 5.9718
I0523 05:14:09.025177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9718 (* 1 = 5.9718 loss)
I0523 05:14:09.034104 35003 sgd_solver.cpp:112] Iteration 150500, lr = 0.001
I0523 05:14:11.999003 35003 solver.cpp:239] Iteration 150510 (3.36281 iter/s, 2.9737s/10 iters), loss = 6.86671
I0523 05:14:11.999047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86671 (* 1 = 6.86671 loss)
I0523 05:14:12.469005 35003 sgd_solver.cpp:112] Iteration 150510, lr = 0.001
I0523 05:14:16.374450 35003 solver.cpp:239] Iteration 150520 (2.2856 iter/s, 4.37521s/10 iters), loss = 7.18285
I0523 05:14:16.374495 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18285 (* 1 = 7.18285 loss)
I0523 05:14:16.402550 35003 sgd_solver.cpp:112] Iteration 150520, lr = 0.001
I0523 05:14:21.353770 35003 solver.cpp:239] Iteration 150530 (2.00841 iter/s, 4.97907s/10 iters), loss = 6.80891
I0523 05:14:21.353821 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80891 (* 1 = 6.80891 loss)
I0523 05:14:21.366653 35003 sgd_solver.cpp:112] Iteration 150530, lr = 0.001
I0523 05:14:25.497655 35003 solver.cpp:239] Iteration 150540 (2.41332 iter/s, 4.14366s/10 iters), loss = 6.66894
I0523 05:14:25.497890 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66894 (* 1 = 6.66894 loss)
I0523 05:14:25.506736 35003 sgd_solver.cpp:112] Iteration 150540, lr = 0.001
I0523 05:14:28.337224 35003 solver.cpp:239] Iteration 150550 (3.52207 iter/s, 2.83924s/10 iters), loss = 6.49329
I0523 05:14:28.337272 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49329 (* 1 = 6.49329 loss)
I0523 05:14:28.967439 35003 sgd_solver.cpp:112] Iteration 150550, lr = 0.001
I0523 05:14:30.941865 35003 solver.cpp:239] Iteration 150560 (3.83953 iter/s, 2.60448s/10 iters), loss = 7.3188
I0523 05:14:30.941905 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3188 (* 1 = 7.3188 loss)
I0523 05:14:30.949180 35003 sgd_solver.cpp:112] Iteration 150560, lr = 0.001
I0523 05:14:35.724436 35003 solver.cpp:239] Iteration 150570 (2.09103 iter/s, 4.78233s/10 iters), loss = 6.7923
I0523 05:14:35.724481 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7923 (* 1 = 6.7923 loss)
I0523 05:14:35.737684 35003 sgd_solver.cpp:112] Iteration 150570, lr = 0.001
I0523 05:14:40.157282 35003 solver.cpp:239] Iteration 150580 (2.25601 iter/s, 4.4326s/10 iters), loss = 7.25605
I0523 05:14:40.157325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25605 (* 1 = 7.25605 loss)
I0523 05:14:40.160217 35003 sgd_solver.cpp:112] Iteration 150580, lr = 0.001
I0523 05:14:43.715559 35003 solver.cpp:239] Iteration 150590 (2.81051 iter/s, 3.55808s/10 iters), loss = 6.58036
I0523 05:14:43.715605 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58036 (* 1 = 6.58036 loss)
I0523 05:14:44.430344 35003 sgd_solver.cpp:112] Iteration 150590, lr = 0.001
I0523 05:14:47.787729 35003 solver.cpp:239] Iteration 150600 (2.45582 iter/s, 4.07196s/10 iters), loss = 7.03337
I0523 05:14:47.787775 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03337 (* 1 = 7.03337 loss)
I0523 05:14:48.496839 35003 sgd_solver.cpp:112] Iteration 150600, lr = 0.001
I0523 05:14:54.352048 35003 solver.cpp:239] Iteration 150610 (1.52346 iter/s, 6.56401s/10 iters), loss = 7.36739
I0523 05:14:54.352087 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36739 (* 1 = 7.36739 loss)
I0523 05:14:54.358299 35003 sgd_solver.cpp:112] Iteration 150610, lr = 0.001
I0523 05:14:57.227107 35003 solver.cpp:239] Iteration 150620 (3.47839 iter/s, 2.87489s/10 iters), loss = 5.95414
I0523 05:14:57.227306 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95414 (* 1 = 5.95414 loss)
I0523 05:14:57.238800 35003 sgd_solver.cpp:112] Iteration 150620, lr = 0.001
I0523 05:15:00.200572 35003 solver.cpp:239] Iteration 150630 (3.36342 iter/s, 2.97316s/10 iters), loss = 5.39953
I0523 05:15:00.200626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.39953 (* 1 = 5.39953 loss)
I0523 05:15:00.859769 35003 sgd_solver.cpp:112] Iteration 150630, lr = 0.001
I0523 05:15:03.938117 35003 solver.cpp:239] Iteration 150640 (2.67571 iter/s, 3.73733s/10 iters), loss = 5.63014
I0523 05:15:03.938182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.63014 (* 1 = 5.63014 loss)
I0523 05:15:03.944113 35003 sgd_solver.cpp:112] Iteration 150640, lr = 0.001
I0523 05:15:07.429123 35003 solver.cpp:239] Iteration 150650 (2.86468 iter/s, 3.49079s/10 iters), loss = 6.75411
I0523 05:15:07.429164 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75411 (* 1 = 6.75411 loss)
I0523 05:15:07.441220 35003 sgd_solver.cpp:112] Iteration 150650, lr = 0.001
I0523 05:15:10.346258 35003 solver.cpp:239] Iteration 150660 (3.42822 iter/s, 2.91696s/10 iters), loss = 7.25822
I0523 05:15:10.346299 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25822 (* 1 = 7.25822 loss)
I0523 05:15:10.364435 35003 sgd_solver.cpp:112] Iteration 150660, lr = 0.001
I0523 05:15:12.393213 35003 solver.cpp:239] Iteration 150670 (4.88563 iter/s, 2.04682s/10 iters), loss = 6.56177
I0523 05:15:12.393250 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56177 (* 1 = 6.56177 loss)
I0523 05:15:12.406549 35003 sgd_solver.cpp:112] Iteration 150670, lr = 0.001
I0523 05:15:15.268234 35003 solver.cpp:239] Iteration 150680 (3.47843 iter/s, 2.87486s/10 iters), loss = 7.33867
I0523 05:15:15.268275 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33867 (* 1 = 7.33867 loss)
I0523 05:15:15.957767 35003 sgd_solver.cpp:112] Iteration 150680, lr = 0.001
I0523 05:15:19.533601 35003 solver.cpp:239] Iteration 150690 (2.34458 iter/s, 4.26515s/10 iters), loss = 5.5464
I0523 05:15:19.533658 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.5464 (* 1 = 5.5464 loss)
I0523 05:15:20.254858 35003 sgd_solver.cpp:112] Iteration 150690, lr = 0.001
I0523 05:15:24.562927 35003 solver.cpp:239] Iteration 150700 (1.98844 iter/s, 5.02906s/10 iters), loss = 7.65825
I0523 05:15:24.562970 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65825 (* 1 = 7.65825 loss)
I0523 05:15:24.571403 35003 sgd_solver.cpp:112] Iteration 150700, lr = 0.001
I0523 05:15:27.517877 35003 solver.cpp:239] Iteration 150710 (3.38435 iter/s, 2.95478s/10 iters), loss = 7.60616
I0523 05:15:27.518121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60616 (* 1 = 7.60616 loss)
I0523 05:15:27.535840 35003 sgd_solver.cpp:112] Iteration 150710, lr = 0.001
I0523 05:15:29.994107 35003 solver.cpp:239] Iteration 150720 (4.03893 iter/s, 2.4759s/10 iters), loss = 6.2629
I0523 05:15:29.994145 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2629 (* 1 = 6.2629 loss)
I0523 05:15:29.995249 35003 sgd_solver.cpp:112] Iteration 150720, lr = 0.001
I0523 05:15:34.354085 35003 solver.cpp:239] Iteration 150730 (2.29371 iter/s, 4.35975s/10 iters), loss = 5.64592
I0523 05:15:34.354136 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.64592 (* 1 = 5.64592 loss)
I0523 05:15:34.368136 35003 sgd_solver.cpp:112] Iteration 150730, lr = 0.001
I0523 05:15:35.334661 35003 solver.cpp:239] Iteration 150740 (10.1992 iter/s, 0.98047s/10 iters), loss = 7.00987
I0523 05:15:35.334748 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00987 (* 1 = 7.00987 loss)
I0523 05:15:35.343612 35003 sgd_solver.cpp:112] Iteration 150740, lr = 0.001
I0523 05:15:36.170811 35003 solver.cpp:239] Iteration 150750 (11.9614 iter/s, 0.836019s/10 iters), loss = 6.22014
I0523 05:15:36.170853 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22014 (* 1 = 6.22014 loss)
I0523 05:15:36.552467 35003 sgd_solver.cpp:112] Iteration 150750, lr = 0.001
I0523 05:15:37.492004 35003 solver.cpp:239] Iteration 150760 (7.56955 iter/s, 1.32108s/10 iters), loss = 6.74111
I0523 05:15:37.492056 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74111 (* 1 = 6.74111 loss)
I0523 05:15:37.500928 35003 sgd_solver.cpp:112] Iteration 150760, lr = 0.001
I0523 05:15:39.299538 35003 solver.cpp:239] Iteration 150770 (5.53282 iter/s, 1.8074s/10 iters), loss = 6.02958
I0523 05:15:39.299592 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02958 (* 1 = 6.02958 loss)
I0523 05:15:40.034190 35003 sgd_solver.cpp:112] Iteration 150770, lr = 0.001
I0523 05:15:44.323985 35003 solver.cpp:239] Iteration 150780 (1.99037 iter/s, 5.02418s/10 iters), loss = 6.62451
I0523 05:15:44.324038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62451 (* 1 = 6.62451 loss)
I0523 05:15:44.330660 35003 sgd_solver.cpp:112] Iteration 150780, lr = 0.001
I0523 05:15:47.045989 35003 solver.cpp:239] Iteration 150790 (3.67399 iter/s, 2.72183s/10 iters), loss = 6.4722
I0523 05:15:47.046037 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4722 (* 1 = 6.4722 loss)
I0523 05:15:47.774411 35003 sgd_solver.cpp:112] Iteration 150790, lr = 0.001
I0523 05:15:50.602459 35003 solver.cpp:239] Iteration 150800 (2.81193 iter/s, 3.55627s/10 iters), loss = 7.25205
I0523 05:15:50.602499 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25205 (* 1 = 7.25205 loss)
I0523 05:15:50.605665 35003 sgd_solver.cpp:112] Iteration 150800, lr = 0.001
I0523 05:15:54.110664 35003 solver.cpp:239] Iteration 150810 (2.85063 iter/s, 3.508s/10 iters), loss = 7.67929
I0523 05:15:54.110733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67929 (* 1 = 7.67929 loss)
I0523 05:15:54.825633 35003 sgd_solver.cpp:112] Iteration 150810, lr = 0.001
I0523 05:15:57.706358 35003 solver.cpp:239] Iteration 150820 (2.78128 iter/s, 3.59547s/10 iters), loss = 7.31292
I0523 05:15:57.706725 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31292 (* 1 = 7.31292 loss)
I0523 05:15:57.711186 35003 sgd_solver.cpp:112] Iteration 150820, lr = 0.001
I0523 05:15:59.810969 35003 solver.cpp:239] Iteration 150830 (4.75239 iter/s, 2.1042s/10 iters), loss = 6.37332
I0523 05:15:59.811025 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37332 (* 1 = 6.37332 loss)
I0523 05:15:59.823451 35003 sgd_solver.cpp:112] Iteration 150830, lr = 0.001
I0523 05:16:05.380687 35003 solver.cpp:239] Iteration 150840 (1.79552 iter/s, 5.56943s/10 iters), loss = 6.45546
I0523 05:16:05.380738 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45546 (* 1 = 6.45546 loss)
I0523 05:16:05.386545 35003 sgd_solver.cpp:112] Iteration 150840, lr = 0.001
I0523 05:16:09.355633 35003 solver.cpp:239] Iteration 150850 (2.51589 iter/s, 3.97473s/10 iters), loss = 6.18484
I0523 05:16:09.355675 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18484 (* 1 = 6.18484 loss)
I0523 05:16:09.366348 35003 sgd_solver.cpp:112] Iteration 150850, lr = 0.001
I0523 05:16:12.973914 35003 solver.cpp:239] Iteration 150860 (2.76389 iter/s, 3.61809s/10 iters), loss = 7.81347
I0523 05:16:12.973958 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81347 (* 1 = 7.81347 loss)
I0523 05:16:13.708884 35003 sgd_solver.cpp:112] Iteration 150860, lr = 0.001
I0523 05:16:15.800987 35003 solver.cpp:239] Iteration 150870 (3.53744 iter/s, 2.82691s/10 iters), loss = 7.5468
I0523 05:16:15.801038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5468 (* 1 = 7.5468 loss)
I0523 05:16:16.520294 35003 sgd_solver.cpp:112] Iteration 150870, lr = 0.001
I0523 05:16:19.156327 35003 solver.cpp:239] Iteration 150880 (2.98052 iter/s, 3.35512s/10 iters), loss = 7.03887
I0523 05:16:19.156369 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03887 (* 1 = 7.03887 loss)
I0523 05:16:19.167824 35003 sgd_solver.cpp:112] Iteration 150880, lr = 0.001
I0523 05:16:23.563498 35003 solver.cpp:239] Iteration 150890 (2.26914 iter/s, 4.40695s/10 iters), loss = 6.23054
I0523 05:16:23.563537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23054 (* 1 = 6.23054 loss)
I0523 05:16:23.576360 35003 sgd_solver.cpp:112] Iteration 150890, lr = 0.001
I0523 05:16:25.793447 35003 solver.cpp:239] Iteration 150900 (4.48468 iter/s, 2.22981s/10 iters), loss = 6.82723
I0523 05:16:25.793495 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82723 (* 1 = 6.82723 loss)
I0523 05:16:26.531519 35003 sgd_solver.cpp:112] Iteration 150900, lr = 0.001
I0523 05:16:30.811813 35003 solver.cpp:239] Iteration 150910 (1.99278 iter/s, 5.01812s/10 iters), loss = 6.39612
I0523 05:16:30.812010 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39612 (* 1 = 6.39612 loss)
I0523 05:16:30.854665 35003 sgd_solver.cpp:112] Iteration 150910, lr = 0.001
I0523 05:16:33.066174 35003 solver.cpp:239] Iteration 150920 (4.43638 iter/s, 2.25409s/10 iters), loss = 6.84943
I0523 05:16:33.066215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84943 (* 1 = 6.84943 loss)
I0523 05:16:33.769848 35003 sgd_solver.cpp:112] Iteration 150920, lr = 0.001
I0523 05:16:37.139026 35003 solver.cpp:239] Iteration 150930 (2.45541 iter/s, 4.07264s/10 iters), loss = 7.00663
I0523 05:16:37.139076 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00663 (* 1 = 7.00663 loss)
I0523 05:16:37.144475 35003 sgd_solver.cpp:112] Iteration 150930, lr = 0.001
I0523 05:16:39.218753 35003 solver.cpp:239] Iteration 150940 (4.80867 iter/s, 2.07958s/10 iters), loss = 5.54624
I0523 05:16:39.218793 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.54624 (* 1 = 5.54624 loss)
I0523 05:16:39.231508 35003 sgd_solver.cpp:112] Iteration 150940, lr = 0.001
I0523 05:16:43.534080 35003 solver.cpp:239] Iteration 150950 (2.31744 iter/s, 4.31511s/10 iters), loss = 7.46558
I0523 05:16:43.534119 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46558 (* 1 = 7.46558 loss)
I0523 05:16:43.690146 35003 sgd_solver.cpp:112] Iteration 150950, lr = 0.001
I0523 05:16:46.656328 35003 solver.cpp:239] Iteration 150960 (3.203 iter/s, 3.12207s/10 iters), loss = 6.04543
I0523 05:16:46.656383 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04543 (* 1 = 6.04543 loss)
I0523 05:16:47.384536 35003 sgd_solver.cpp:112] Iteration 150960, lr = 0.001
I0523 05:16:51.515511 35003 solver.cpp:239] Iteration 150970 (2.05806 iter/s, 4.85893s/10 iters), loss = 8.34475
I0523 05:16:51.515553 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.34475 (* 1 = 8.34475 loss)
I0523 05:16:52.217978 35003 sgd_solver.cpp:112] Iteration 150970, lr = 0.001
I0523 05:16:54.598870 35003 solver.cpp:239] Iteration 150980 (3.2434 iter/s, 3.08318s/10 iters), loss = 5.54895
I0523 05:16:54.598909 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.54895 (* 1 = 5.54895 loss)
I0523 05:16:54.605103 35003 sgd_solver.cpp:112] Iteration 150980, lr = 0.001
I0523 05:16:58.095129 35003 solver.cpp:239] Iteration 150990 (2.86035 iter/s, 3.49608s/10 iters), loss = 8.13521
I0523 05:16:58.095172 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.13521 (* 1 = 8.13521 loss)
I0523 05:16:58.830055 35003 sgd_solver.cpp:112] Iteration 150990, lr = 0.001
I0523 05:17:03.047241 35003 solver.cpp:239] Iteration 151000 (2.01944 iter/s, 4.95186s/10 iters), loss = 7.36915
I0523 05:17:03.047500 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36915 (* 1 = 7.36915 loss)
I0523 05:17:03.051442 35003 sgd_solver.cpp:112] Iteration 151000, lr = 0.001
I0523 05:17:06.785396 35003 solver.cpp:239] Iteration 151010 (2.67551 iter/s, 3.73761s/10 iters), loss = 5.41377
I0523 05:17:06.785454 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.41377 (* 1 = 5.41377 loss)
I0523 05:17:07.513944 35003 sgd_solver.cpp:112] Iteration 151010, lr = 0.001
I0523 05:17:10.368522 35003 solver.cpp:239] Iteration 151020 (2.79102 iter/s, 3.58292s/10 iters), loss = 5.84336
I0523 05:17:10.368554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84336 (* 1 = 5.84336 loss)
I0523 05:17:10.381193 35003 sgd_solver.cpp:112] Iteration 151020, lr = 0.001
I0523 05:17:13.844193 35003 solver.cpp:239] Iteration 151030 (2.88091 iter/s, 3.47112s/10 iters), loss = 6.61886
I0523 05:17:13.844234 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61886 (* 1 = 6.61886 loss)
I0523 05:17:13.853628 35003 sgd_solver.cpp:112] Iteration 151030, lr = 0.001
I0523 05:17:16.715785 35003 solver.cpp:239] Iteration 151040 (3.48259 iter/s, 2.87143s/10 iters), loss = 6.23232
I0523 05:17:16.715821 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23232 (* 1 = 6.23232 loss)
I0523 05:17:16.736214 35003 sgd_solver.cpp:112] Iteration 151040, lr = 0.001
I0523 05:17:20.966440 35003 solver.cpp:239] Iteration 151050 (2.3527 iter/s, 4.25044s/10 iters), loss = 6.49076
I0523 05:17:20.966487 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49076 (* 1 = 6.49076 loss)
I0523 05:17:21.027640 35003 sgd_solver.cpp:112] Iteration 151050, lr = 0.001
I0523 05:17:25.433100 35003 solver.cpp:239] Iteration 151060 (2.23893 iter/s, 4.46642s/10 iters), loss = 6.33744
I0523 05:17:25.433153 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33744 (* 1 = 6.33744 loss)
I0523 05:17:26.048138 35003 sgd_solver.cpp:112] Iteration 151060, lr = 0.001
I0523 05:17:29.095767 35003 solver.cpp:239] Iteration 151070 (2.7304 iter/s, 3.66246s/10 iters), loss = 6.7302
I0523 05:17:29.095809 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7302 (* 1 = 6.7302 loss)
I0523 05:17:29.811179 35003 sgd_solver.cpp:112] Iteration 151070, lr = 0.001
I0523 05:17:33.275132 35003 solver.cpp:239] Iteration 151080 (2.39283 iter/s, 4.17915s/10 iters), loss = 7.05876
I0523 05:17:33.275377 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05876 (* 1 = 7.05876 loss)
I0523 05:17:33.288180 35003 sgd_solver.cpp:112] Iteration 151080, lr = 0.001
I0523 05:17:37.621192 35003 solver.cpp:239] Iteration 151090 (2.30114 iter/s, 4.34567s/10 iters), loss = 6.36964
I0523 05:17:37.621230 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36964 (* 1 = 6.36964 loss)
I0523 05:17:37.629334 35003 sgd_solver.cpp:112] Iteration 151090, lr = 0.001
I0523 05:17:39.734052 35003 solver.cpp:239] Iteration 151100 (4.73322 iter/s, 2.11273s/10 iters), loss = 5.94485
I0523 05:17:39.734099 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.94485 (* 1 = 5.94485 loss)
I0523 05:17:40.455418 35003 sgd_solver.cpp:112] Iteration 151100, lr = 0.001
I0523 05:17:43.381235 35003 solver.cpp:239] Iteration 151110 (2.742 iter/s, 3.64697s/10 iters), loss = 6.70198
I0523 05:17:43.381278 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70198 (* 1 = 6.70198 loss)
I0523 05:17:43.384363 35003 sgd_solver.cpp:112] Iteration 151110, lr = 0.001
I0523 05:17:45.449177 35003 solver.cpp:239] Iteration 151120 (4.83605 iter/s, 2.0678s/10 iters), loss = 6.53561
I0523 05:17:45.449220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53561 (* 1 = 6.53561 loss)
I0523 05:17:45.782960 35003 sgd_solver.cpp:112] Iteration 151120, lr = 0.001
I0523 05:17:47.812885 35003 solver.cpp:239] Iteration 151130 (4.2309 iter/s, 2.36356s/10 iters), loss = 7.06711
I0523 05:17:47.812922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06711 (* 1 = 7.06711 loss)
I0523 05:17:47.822654 35003 sgd_solver.cpp:112] Iteration 151130, lr = 0.001
I0523 05:17:51.377693 35003 solver.cpp:239] Iteration 151140 (2.80536 iter/s, 3.56461s/10 iters), loss = 6.99612
I0523 05:17:51.377744 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99612 (* 1 = 6.99612 loss)
I0523 05:17:51.383361 35003 sgd_solver.cpp:112] Iteration 151140, lr = 0.001
I0523 05:17:55.072729 35003 solver.cpp:239] Iteration 151150 (2.70648 iter/s, 3.69483s/10 iters), loss = 6.85832
I0523 05:17:55.072767 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85832 (* 1 = 6.85832 loss)
I0523 05:17:55.086405 35003 sgd_solver.cpp:112] Iteration 151150, lr = 0.001
I0523 05:17:58.167379 35003 solver.cpp:239] Iteration 151160 (3.23156 iter/s, 3.09448s/10 iters), loss = 5.93112
I0523 05:17:58.167428 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93112 (* 1 = 5.93112 loss)
I0523 05:17:58.856779 35003 sgd_solver.cpp:112] Iteration 151160, lr = 0.001
I0523 05:18:03.183368 35003 solver.cpp:239] Iteration 151170 (1.99373 iter/s, 5.01573s/10 iters), loss = 7.03566
I0523 05:18:03.183416 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03566 (* 1 = 7.03566 loss)
I0523 05:18:03.196599 35003 sgd_solver.cpp:112] Iteration 151170, lr = 0.001
I0523 05:18:06.415560 35003 solver.cpp:239] Iteration 151180 (3.09406 iter/s, 3.232s/10 iters), loss = 7.07429
I0523 05:18:06.415721 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07429 (* 1 = 7.07429 loss)
I0523 05:18:06.826910 35003 sgd_solver.cpp:112] Iteration 151180, lr = 0.001
I0523 05:18:09.864480 35003 solver.cpp:239] Iteration 151190 (2.89971 iter/s, 3.44862s/10 iters), loss = 6.25095
I0523 05:18:09.864529 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25095 (* 1 = 6.25095 loss)
I0523 05:18:10.575448 35003 sgd_solver.cpp:112] Iteration 151190, lr = 0.001
I0523 05:18:14.212429 35003 solver.cpp:239] Iteration 151200 (2.30006 iter/s, 4.34772s/10 iters), loss = 6.18586
I0523 05:18:14.212469 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18586 (* 1 = 6.18586 loss)
I0523 05:18:14.226454 35003 sgd_solver.cpp:112] Iteration 151200, lr = 0.001
I0523 05:18:16.326128 35003 solver.cpp:239] Iteration 151210 (4.73136 iter/s, 2.11356s/10 iters), loss = 6.96581
I0523 05:18:16.326195 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96581 (* 1 = 6.96581 loss)
I0523 05:18:17.067030 35003 sgd_solver.cpp:112] Iteration 151210, lr = 0.001
I0523 05:18:19.934725 35003 solver.cpp:239] Iteration 151220 (2.77134 iter/s, 3.60836s/10 iters), loss = 6.27726
I0523 05:18:19.934768 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27726 (* 1 = 6.27726 loss)
I0523 05:18:19.939967 35003 sgd_solver.cpp:112] Iteration 151220, lr = 0.001
I0523 05:18:23.485234 35003 solver.cpp:239] Iteration 151230 (2.81666 iter/s, 3.5503s/10 iters), loss = 6.92414
I0523 05:18:23.485277 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92414 (* 1 = 6.92414 loss)
I0523 05:18:23.511323 35003 sgd_solver.cpp:112] Iteration 151230, lr = 0.001
I0523 05:18:26.995203 35003 solver.cpp:239] Iteration 151240 (2.8492 iter/s, 3.50976s/10 iters), loss = 6.28946
I0523 05:18:26.995255 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28946 (* 1 = 6.28946 loss)
I0523 05:18:27.008255 35003 sgd_solver.cpp:112] Iteration 151240, lr = 0.001
I0523 05:18:30.522588 35003 solver.cpp:239] Iteration 151250 (2.83512 iter/s, 3.52719s/10 iters), loss = 6.8196
I0523 05:18:30.522634 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8196 (* 1 = 6.8196 loss)
I0523 05:18:31.237493 35003 sgd_solver.cpp:112] Iteration 151250, lr = 0.001
I0523 05:18:34.170614 35003 solver.cpp:239] Iteration 151260 (2.74136 iter/s, 3.64783s/10 iters), loss = 6.44789
I0523 05:18:34.170660 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44789 (* 1 = 6.44789 loss)
I0523 05:18:34.177825 35003 sgd_solver.cpp:112] Iteration 151260, lr = 0.001
I0523 05:18:35.765981 35003 solver.cpp:239] Iteration 151270 (6.26861 iter/s, 1.59525s/10 iters), loss = 7.36984
I0523 05:18:35.766022 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36984 (* 1 = 7.36984 loss)
I0523 05:18:35.777803 35003 sgd_solver.cpp:112] Iteration 151270, lr = 0.001
I0523 05:18:39.798100 35003 solver.cpp:239] Iteration 151280 (2.48021 iter/s, 4.03191s/10 iters), loss = 6.67416
I0523 05:18:39.798300 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67416 (* 1 = 6.67416 loss)
I0523 05:18:39.975934 35003 sgd_solver.cpp:112] Iteration 151280, lr = 0.001
I0523 05:18:42.851845 35003 solver.cpp:239] Iteration 151290 (3.27503 iter/s, 3.05341s/10 iters), loss = 7.23155
I0523 05:18:42.851892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23155 (* 1 = 7.23155 loss)
I0523 05:18:42.865365 35003 sgd_solver.cpp:112] Iteration 151290, lr = 0.001
I0523 05:18:46.147773 35003 solver.cpp:239] Iteration 151300 (3.03422 iter/s, 3.29575s/10 iters), loss = 6.82346
I0523 05:18:46.147814 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82346 (* 1 = 6.82346 loss)
I0523 05:18:46.889158 35003 sgd_solver.cpp:112] Iteration 151300, lr = 0.001
I0523 05:18:49.736757 35003 solver.cpp:239] Iteration 151310 (2.78645 iter/s, 3.58879s/10 iters), loss = 6.2127
I0523 05:18:49.736805 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2127 (* 1 = 6.2127 loss)
I0523 05:18:49.739190 35003 sgd_solver.cpp:112] Iteration 151310, lr = 0.001
I0523 05:18:51.838637 35003 solver.cpp:239] Iteration 151320 (4.75799 iter/s, 2.10173s/10 iters), loss = 7.3594
I0523 05:18:51.838678 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3594 (* 1 = 7.3594 loss)
I0523 05:18:51.859184 35003 sgd_solver.cpp:112] Iteration 151320, lr = 0.001
I0523 05:18:55.377853 35003 solver.cpp:239] Iteration 151330 (2.82564 iter/s, 3.53902s/10 iters), loss = 5.55793
I0523 05:18:55.377919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.55793 (* 1 = 5.55793 loss)
I0523 05:18:55.382508 35003 sgd_solver.cpp:112] Iteration 151330, lr = 0.001
I0523 05:19:00.506127 35003 solver.cpp:239] Iteration 151340 (1.95008 iter/s, 5.128s/10 iters), loss = 7.0715
I0523 05:19:00.506178 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0715 (* 1 = 7.0715 loss)
I0523 05:19:00.534258 35003 sgd_solver.cpp:112] Iteration 151340, lr = 0.001
I0523 05:19:03.931264 35003 solver.cpp:239] Iteration 151350 (2.91975 iter/s, 3.42495s/10 iters), loss = 6.65912
I0523 05:19:03.931306 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65912 (* 1 = 6.65912 loss)
I0523 05:19:03.937003 35003 sgd_solver.cpp:112] Iteration 151350, lr = 0.001
I0523 05:19:07.629834 35003 solver.cpp:239] Iteration 151360 (2.7039 iter/s, 3.69837s/10 iters), loss = 6.15647
I0523 05:19:07.629882 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15647 (* 1 = 6.15647 loss)
I0523 05:19:07.635191 35003 sgd_solver.cpp:112] Iteration 151360, lr = 0.001
I0523 05:19:12.211345 35003 solver.cpp:239] Iteration 151370 (2.18281 iter/s, 4.58125s/10 iters), loss = 6.55635
I0523 05:19:12.211585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55635 (* 1 = 6.55635 loss)
I0523 05:19:12.910157 35003 sgd_solver.cpp:112] Iteration 151370, lr = 0.001
I0523 05:19:16.415989 35003 solver.cpp:239] Iteration 151380 (2.37855 iter/s, 4.20424s/10 iters), loss = 6.94233
I0523 05:19:16.416035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94233 (* 1 = 6.94233 loss)
I0523 05:19:16.429196 35003 sgd_solver.cpp:112] Iteration 151380, lr = 0.001
I0523 05:19:19.193356 35003 solver.cpp:239] Iteration 151390 (3.60075 iter/s, 2.7772s/10 iters), loss = 5.6941
I0523 05:19:19.193393 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.6941 (* 1 = 5.6941 loss)
I0523 05:19:19.200623 35003 sgd_solver.cpp:112] Iteration 151390, lr = 0.001
I0523 05:19:22.008122 35003 solver.cpp:239] Iteration 151400 (3.55289 iter/s, 2.81461s/10 iters), loss = 8.20771
I0523 05:19:22.008164 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.20771 (* 1 = 8.20771 loss)
I0523 05:19:22.021118 35003 sgd_solver.cpp:112] Iteration 151400, lr = 0.001
I0523 05:19:25.584666 35003 solver.cpp:239] Iteration 151410 (2.79614 iter/s, 3.57635s/10 iters), loss = 6.66973
I0523 05:19:25.584713 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66973 (* 1 = 6.66973 loss)
I0523 05:19:25.651165 35003 sgd_solver.cpp:112] Iteration 151410, lr = 0.001
I0523 05:19:28.596257 35003 solver.cpp:239] Iteration 151420 (3.32069 iter/s, 3.01142s/10 iters), loss = 6.70071
I0523 05:19:28.596297 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70071 (* 1 = 6.70071 loss)
I0523 05:19:28.598763 35003 sgd_solver.cpp:112] Iteration 151420, lr = 0.001
I0523 05:19:30.705581 35003 solver.cpp:239] Iteration 151430 (4.74117 iter/s, 2.10918s/10 iters), loss = 7.22752
I0523 05:19:30.705619 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22752 (* 1 = 7.22752 loss)
I0523 05:19:31.389740 35003 sgd_solver.cpp:112] Iteration 151430, lr = 0.001
I0523 05:19:34.231757 35003 solver.cpp:239] Iteration 151440 (2.83608 iter/s, 3.52599s/10 iters), loss = 6.05473
I0523 05:19:34.231807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05473 (* 1 = 6.05473 loss)
I0523 05:19:34.237102 35003 sgd_solver.cpp:112] Iteration 151440, lr = 0.001
I0523 05:19:38.364015 35003 solver.cpp:239] Iteration 151450 (2.42012 iter/s, 4.13203s/10 iters), loss = 7.63188
I0523 05:19:38.364087 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63188 (* 1 = 7.63188 loss)
I0523 05:19:38.377151 35003 sgd_solver.cpp:112] Iteration 151450, lr = 0.001
I0523 05:19:40.489135 35003 solver.cpp:239] Iteration 151460 (4.70597 iter/s, 2.12496s/10 iters), loss = 5.04524
I0523 05:19:40.489186 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.04524 (* 1 = 5.04524 loss)
I0523 05:19:41.210464 35003 sgd_solver.cpp:112] Iteration 151460, lr = 0.001
I0523 05:19:45.344034 35003 solver.cpp:239] Iteration 151470 (2.05988 iter/s, 4.85465s/10 iters), loss = 7.66472
I0523 05:19:45.344272 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66472 (* 1 = 7.66472 loss)
I0523 05:19:46.059511 35003 sgd_solver.cpp:112] Iteration 151470, lr = 0.001
I0523 05:19:50.740761 35003 solver.cpp:239] Iteration 151480 (1.85313 iter/s, 5.39627s/10 iters), loss = 7.42294
I0523 05:19:50.740813 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42294 (* 1 = 7.42294 loss)
I0523 05:19:51.455495 35003 sgd_solver.cpp:112] Iteration 151480, lr = 0.001
I0523 05:19:54.575196 35003 solver.cpp:239] Iteration 151490 (2.60809 iter/s, 3.83422s/10 iters), loss = 6.64318
I0523 05:19:54.575243 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64318 (* 1 = 6.64318 loss)
I0523 05:19:54.893764 35003 sgd_solver.cpp:112] Iteration 151490, lr = 0.001
I0523 05:19:59.283572 35003 solver.cpp:239] Iteration 151500 (2.12399 iter/s, 4.70813s/10 iters), loss = 6.43932
I0523 05:19:59.283638 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43932 (* 1 = 6.43932 loss)
I0523 05:19:59.895844 35003 sgd_solver.cpp:112] Iteration 151500, lr = 0.001
I0523 05:20:01.257369 35003 solver.cpp:239] Iteration 151510 (5.06677 iter/s, 1.97364s/10 iters), loss = 6.78516
I0523 05:20:01.257424 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78516 (* 1 = 6.78516 loss)
I0523 05:20:01.991312 35003 sgd_solver.cpp:112] Iteration 151510, lr = 0.001
I0523 05:20:05.304847 35003 solver.cpp:239] Iteration 151520 (2.47081 iter/s, 4.04726s/10 iters), loss = 6.64708
I0523 05:20:05.304888 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64708 (* 1 = 6.64708 loss)
I0523 05:20:05.318265 35003 sgd_solver.cpp:112] Iteration 151520, lr = 0.001
I0523 05:20:08.446781 35003 solver.cpp:239] Iteration 151530 (3.18293 iter/s, 3.14176s/10 iters), loss = 6.79023
I0523 05:20:08.446822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79023 (* 1 = 6.79023 loss)
I0523 05:20:08.774034 35003 sgd_solver.cpp:112] Iteration 151530, lr = 0.001
I0523 05:20:12.181673 35003 solver.cpp:239] Iteration 151540 (2.6776 iter/s, 3.73469s/10 iters), loss = 6.81424
I0523 05:20:12.181712 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81424 (* 1 = 6.81424 loss)
I0523 05:20:12.189471 35003 sgd_solver.cpp:112] Iteration 151540, lr = 0.001
I0523 05:20:15.773937 35003 solver.cpp:239] Iteration 151550 (2.78391 iter/s, 3.59207s/10 iters), loss = 8.54055
I0523 05:20:15.774168 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.54055 (* 1 = 8.54055 loss)
I0523 05:20:15.781680 35003 sgd_solver.cpp:112] Iteration 151550, lr = 0.001
I0523 05:20:18.258566 35003 solver.cpp:239] Iteration 151560 (4.02526 iter/s, 2.48431s/10 iters), loss = 6.10934
I0523 05:20:18.258610 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10934 (* 1 = 6.10934 loss)
I0523 05:20:18.967183 35003 sgd_solver.cpp:112] Iteration 151560, lr = 0.001
I0523 05:20:21.801677 35003 solver.cpp:239] Iteration 151570 (2.82253 iter/s, 3.54292s/10 iters), loss = 7.00363
I0523 05:20:21.801733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00363 (* 1 = 7.00363 loss)
I0523 05:20:22.523150 35003 sgd_solver.cpp:112] Iteration 151570, lr = 0.001
I0523 05:20:24.155861 35003 solver.cpp:239] Iteration 151580 (4.24803 iter/s, 2.35403s/10 iters), loss = 7.9125
I0523 05:20:24.155905 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9125 (* 1 = 7.9125 loss)
I0523 05:20:24.163589 35003 sgd_solver.cpp:112] Iteration 151580, lr = 0.001
I0523 05:20:26.329022 35003 solver.cpp:239] Iteration 151590 (4.60188 iter/s, 2.17302s/10 iters), loss = 7.27654
I0523 05:20:26.329057 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27654 (* 1 = 7.27654 loss)
I0523 05:20:27.057638 35003 sgd_solver.cpp:112] Iteration 151590, lr = 0.001
I0523 05:20:29.794378 35003 solver.cpp:239] Iteration 151600 (2.88586 iter/s, 3.46517s/10 iters), loss = 6.12161
I0523 05:20:29.794416 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12161 (* 1 = 6.12161 loss)
I0523 05:20:29.804782 35003 sgd_solver.cpp:112] Iteration 151600, lr = 0.001
I0523 05:20:33.974315 35003 solver.cpp:239] Iteration 151610 (2.39251 iter/s, 4.17972s/10 iters), loss = 7.38421
I0523 05:20:33.974361 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38421 (* 1 = 7.38421 loss)
I0523 05:20:33.976862 35003 sgd_solver.cpp:112] Iteration 151610, lr = 0.001
I0523 05:20:37.638207 35003 solver.cpp:239] Iteration 151620 (2.72949 iter/s, 3.66369s/10 iters), loss = 6.97819
I0523 05:20:37.638264 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97819 (* 1 = 6.97819 loss)
I0523 05:20:38.372604 35003 sgd_solver.cpp:112] Iteration 151620, lr = 0.001
I0523 05:20:43.051602 35003 solver.cpp:239] Iteration 151630 (1.84736 iter/s, 5.41312s/10 iters), loss = 5.95147
I0523 05:20:43.051661 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95147 (* 1 = 5.95147 loss)
I0523 05:20:43.119199 35003 sgd_solver.cpp:112] Iteration 151630, lr = 0.001
I0523 05:20:46.402788 35003 solver.cpp:239] Iteration 151640 (2.9842 iter/s, 3.35098s/10 iters), loss = 6.00753
I0523 05:20:46.402983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00753 (* 1 = 6.00753 loss)
I0523 05:20:46.410408 35003 sgd_solver.cpp:112] Iteration 151640, lr = 0.001
I0523 05:20:49.247515 35003 solver.cpp:239] Iteration 151650 (3.51566 iter/s, 2.84442s/10 iters), loss = 6.74606
I0523 05:20:49.247560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74606 (* 1 = 6.74606 loss)
I0523 05:20:49.255750 35003 sgd_solver.cpp:112] Iteration 151650, lr = 0.001
I0523 05:20:52.246049 35003 solver.cpp:239] Iteration 151660 (3.33516 iter/s, 2.99836s/10 iters), loss = 7.39728
I0523 05:20:52.246093 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39728 (* 1 = 7.39728 loss)
I0523 05:20:52.268232 35003 sgd_solver.cpp:112] Iteration 151660, lr = 0.001
I0523 05:20:54.637888 35003 solver.cpp:239] Iteration 151670 (4.18115 iter/s, 2.39169s/10 iters), loss = 7.29695
I0523 05:20:54.637948 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29695 (* 1 = 7.29695 loss)
I0523 05:20:55.353444 35003 sgd_solver.cpp:112] Iteration 151670, lr = 0.001
I0523 05:20:57.638077 35003 solver.cpp:239] Iteration 151680 (3.33333 iter/s, 3s/10 iters), loss = 7.81331
I0523 05:20:57.638124 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81331 (* 1 = 7.81331 loss)
I0523 05:20:57.642683 35003 sgd_solver.cpp:112] Iteration 151680, lr = 0.001
I0523 05:20:59.814399 35003 solver.cpp:239] Iteration 151690 (4.59521 iter/s, 2.17618s/10 iters), loss = 6.51061
I0523 05:20:59.814436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51061 (* 1 = 6.51061 loss)
I0523 05:20:59.822520 35003 sgd_solver.cpp:112] Iteration 151690, lr = 0.001
I0523 05:21:03.183773 35003 solver.cpp:239] Iteration 151700 (2.96807 iter/s, 3.36919s/10 iters), loss = 6.92633
I0523 05:21:03.183820 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92633 (* 1 = 6.92633 loss)
I0523 05:21:03.191993 35003 sgd_solver.cpp:112] Iteration 151700, lr = 0.001
I0523 05:21:07.370784 35003 solver.cpp:239] Iteration 151710 (2.38847 iter/s, 4.18678s/10 iters), loss = 7.67097
I0523 05:21:07.370838 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67097 (* 1 = 7.67097 loss)
I0523 05:21:08.040868 35003 sgd_solver.cpp:112] Iteration 151710, lr = 0.001
I0523 05:21:10.788133 35003 solver.cpp:239] Iteration 151720 (2.92641 iter/s, 3.41716s/10 iters), loss = 6.79123
I0523 05:21:10.788173 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79123 (* 1 = 6.79123 loss)
I0523 05:21:10.796587 35003 sgd_solver.cpp:112] Iteration 151720, lr = 0.001
I0523 05:21:14.414815 35003 solver.cpp:239] Iteration 151730 (2.75749 iter/s, 3.62649s/10 iters), loss = 7.33328
I0523 05:21:14.414860 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33328 (* 1 = 7.33328 loss)
I0523 05:21:15.123515 35003 sgd_solver.cpp:112] Iteration 151730, lr = 0.001
I0523 05:21:18.001875 35003 solver.cpp:239] Iteration 151740 (2.78795 iter/s, 3.58686s/10 iters), loss = 7.49468
I0523 05:21:18.002198 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49468 (* 1 = 7.49468 loss)
I0523 05:21:18.717257 35003 sgd_solver.cpp:112] Iteration 151740, lr = 0.001
I0523 05:21:21.814633 35003 solver.cpp:239] Iteration 151750 (2.62309 iter/s, 3.8123s/10 iters), loss = 7.25567
I0523 05:21:21.814689 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25567 (* 1 = 7.25567 loss)
I0523 05:21:21.828256 35003 sgd_solver.cpp:112] Iteration 151750, lr = 0.001
I0523 05:21:24.616294 35003 solver.cpp:239] Iteration 151760 (3.56954 iter/s, 2.80148s/10 iters), loss = 6.55804
I0523 05:21:24.616344 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55804 (* 1 = 6.55804 loss)
I0523 05:21:25.357229 35003 sgd_solver.cpp:112] Iteration 151760, lr = 0.001
I0523 05:21:29.149792 35003 solver.cpp:239] Iteration 151770 (2.20592 iter/s, 4.53326s/10 iters), loss = 6.68211
I0523 05:21:29.149847 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68211 (* 1 = 6.68211 loss)
I0523 05:21:29.832682 35003 sgd_solver.cpp:112] Iteration 151770, lr = 0.001
I0523 05:21:32.891203 35003 solver.cpp:239] Iteration 151780 (2.67294 iter/s, 3.7412s/10 iters), loss = 6.38321
I0523 05:21:32.891247 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38321 (* 1 = 6.38321 loss)
I0523 05:21:32.904345 35003 sgd_solver.cpp:112] Iteration 151780, lr = 0.001
I0523 05:21:36.461488 35003 solver.cpp:239] Iteration 151790 (2.80105 iter/s, 3.57009s/10 iters), loss = 6.44204
I0523 05:21:36.461546 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44204 (* 1 = 6.44204 loss)
I0523 05:21:36.469736 35003 sgd_solver.cpp:112] Iteration 151790, lr = 0.001
I0523 05:21:38.567339 35003 solver.cpp:239] Iteration 151800 (4.74901 iter/s, 2.1057s/10 iters), loss = 6.91796
I0523 05:21:38.567378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91796 (* 1 = 6.91796 loss)
I0523 05:21:38.580698 35003 sgd_solver.cpp:112] Iteration 151800, lr = 0.001
I0523 05:21:41.385746 35003 solver.cpp:239] Iteration 151810 (3.54831 iter/s, 2.81824s/10 iters), loss = 6.7494
I0523 05:21:41.385789 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7494 (* 1 = 6.7494 loss)
I0523 05:21:41.398958 35003 sgd_solver.cpp:112] Iteration 151810, lr = 0.001
I0523 05:21:43.915827 35003 solver.cpp:239] Iteration 151820 (3.95268 iter/s, 2.52993s/10 iters), loss = 7.34489
I0523 05:21:43.915877 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34489 (* 1 = 7.34489 loss)
I0523 05:21:44.617599 35003 sgd_solver.cpp:112] Iteration 151820, lr = 0.001
I0523 05:21:47.498195 35003 solver.cpp:239] Iteration 151830 (2.7916 iter/s, 3.58217s/10 iters), loss = 6.99258
I0523 05:21:47.498234 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99258 (* 1 = 6.99258 loss)
I0523 05:21:47.511111 35003 sgd_solver.cpp:112] Iteration 151830, lr = 0.001
I0523 05:21:51.149693 35003 solver.cpp:239] Iteration 151840 (2.73875 iter/s, 3.6513s/10 iters), loss = 7.93928
I0523 05:21:51.149885 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93928 (* 1 = 7.93928 loss)
I0523 05:21:51.154968 35003 sgd_solver.cpp:112] Iteration 151840, lr = 0.001
I0523 05:21:53.434391 35003 solver.cpp:239] Iteration 151850 (4.3775 iter/s, 2.28441s/10 iters), loss = 6.4464
I0523 05:21:53.434444 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4464 (* 1 = 6.4464 loss)
I0523 05:21:54.098207 35003 sgd_solver.cpp:112] Iteration 151850, lr = 0.001
I0523 05:21:57.414203 35003 solver.cpp:239] Iteration 151860 (2.51282 iter/s, 3.97959s/10 iters), loss = 6.81447
I0523 05:21:57.414243 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81447 (* 1 = 6.81447 loss)
I0523 05:21:57.418802 35003 sgd_solver.cpp:112] Iteration 151860, lr = 0.001
I0523 05:22:01.055807 35003 solver.cpp:239] Iteration 151870 (2.74619 iter/s, 3.64141s/10 iters), loss = 7.57612
I0523 05:22:01.055846 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57612 (* 1 = 7.57612 loss)
I0523 05:22:01.551198 35003 sgd_solver.cpp:112] Iteration 151870, lr = 0.001
I0523 05:22:04.320853 35003 solver.cpp:239] Iteration 151880 (3.06292 iter/s, 3.26486s/10 iters), loss = 6.30782
I0523 05:22:04.320894 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30782 (* 1 = 6.30782 loss)
I0523 05:22:04.330801 35003 sgd_solver.cpp:112] Iteration 151880, lr = 0.001
I0523 05:22:08.836540 35003 solver.cpp:239] Iteration 151890 (2.21461 iter/s, 4.51546s/10 iters), loss = 6.02933
I0523 05:22:08.836587 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02933 (* 1 = 6.02933 loss)
I0523 05:22:09.038219 35003 sgd_solver.cpp:112] Iteration 151890, lr = 0.001
I0523 05:22:10.341131 35003 solver.cpp:239] Iteration 151900 (6.64684 iter/s, 1.50447s/10 iters), loss = 6.32586
I0523 05:22:10.341173 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32586 (* 1 = 6.32586 loss)
I0523 05:22:10.375124 35003 sgd_solver.cpp:112] Iteration 151900, lr = 0.001
I0523 05:22:13.735283 35003 solver.cpp:239] Iteration 151910 (2.9464 iter/s, 3.39397s/10 iters), loss = 7.22317
I0523 05:22:13.735327 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22317 (* 1 = 7.22317 loss)
I0523 05:22:13.748163 35003 sgd_solver.cpp:112] Iteration 151910, lr = 0.001
I0523 05:22:17.477689 35003 solver.cpp:239] Iteration 151920 (2.67222 iter/s, 3.7422s/10 iters), loss = 6.26252
I0523 05:22:17.477732 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26252 (* 1 = 6.26252 loss)
I0523 05:22:17.485270 35003 sgd_solver.cpp:112] Iteration 151920, lr = 0.001
I0523 05:22:19.616372 35003 solver.cpp:239] Iteration 151930 (4.67609 iter/s, 2.13854s/10 iters), loss = 6.61594
I0523 05:22:19.616425 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61594 (* 1 = 6.61594 loss)
I0523 05:22:19.622459 35003 sgd_solver.cpp:112] Iteration 151930, lr = 0.001
I0523 05:22:23.151280 35003 solver.cpp:239] Iteration 151940 (2.82909 iter/s, 3.53471s/10 iters), loss = 5.7104
I0523 05:22:23.151481 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7104 (* 1 = 5.7104 loss)
I0523 05:22:23.856186 35003 sgd_solver.cpp:112] Iteration 151940, lr = 0.001
I0523 05:22:26.291371 35003 solver.cpp:239] Iteration 151950 (3.18497 iter/s, 3.13975s/10 iters), loss = 6.24563
I0523 05:22:26.291429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24563 (* 1 = 6.24563 loss)
I0523 05:22:26.300107 35003 sgd_solver.cpp:112] Iteration 151950, lr = 0.001
I0523 05:22:30.172083 35003 solver.cpp:239] Iteration 151960 (2.57699 iter/s, 3.8805s/10 iters), loss = 6.44771
I0523 05:22:30.172123 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44771 (* 1 = 6.44771 loss)
I0523 05:22:30.185305 35003 sgd_solver.cpp:112] Iteration 151960, lr = 0.001
I0523 05:22:33.001298 35003 solver.cpp:239] Iteration 151970 (3.53476 iter/s, 2.82905s/10 iters), loss = 6.16041
I0523 05:22:33.001351 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16041 (* 1 = 6.16041 loss)
I0523 05:22:33.023478 35003 sgd_solver.cpp:112] Iteration 151970, lr = 0.001
I0523 05:22:35.884843 35003 solver.cpp:239] Iteration 151980 (3.46817 iter/s, 2.88337s/10 iters), loss = 6.32838
I0523 05:22:35.884891 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32838 (* 1 = 6.32838 loss)
I0523 05:22:35.891809 35003 sgd_solver.cpp:112] Iteration 151980, lr = 0.001
I0523 05:22:40.136708 35003 solver.cpp:239] Iteration 151990 (2.35204 iter/s, 4.25164s/10 iters), loss = 6.77024
I0523 05:22:40.136756 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77024 (* 1 = 6.77024 loss)
I0523 05:22:40.154880 35003 sgd_solver.cpp:112] Iteration 151990, lr = 0.001
I0523 05:22:42.367624 35003 solver.cpp:239] Iteration 152000 (4.48275 iter/s, 2.23077s/10 iters), loss = 6.64509
I0523 05:22:42.367666 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64509 (* 1 = 6.64509 loss)
I0523 05:22:42.377220 35003 sgd_solver.cpp:112] Iteration 152000, lr = 0.001
I0523 05:22:44.472628 35003 solver.cpp:239] Iteration 152010 (4.75089 iter/s, 2.10487s/10 iters), loss = 6.25071
I0523 05:22:44.472671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25071 (* 1 = 6.25071 loss)
I0523 05:22:44.485800 35003 sgd_solver.cpp:112] Iteration 152010, lr = 0.001
I0523 05:22:47.283149 35003 solver.cpp:239] Iteration 152020 (3.55827 iter/s, 2.81035s/10 iters), loss = 7.34373
I0523 05:22:47.283202 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34373 (* 1 = 7.34373 loss)
I0523 05:22:48.005097 35003 sgd_solver.cpp:112] Iteration 152020, lr = 0.001
I0523 05:22:52.227571 35003 solver.cpp:239] Iteration 152030 (2.02259 iter/s, 4.94416s/10 iters), loss = 6.32367
I0523 05:22:52.227607 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32367 (* 1 = 6.32367 loss)
I0523 05:22:52.241119 35003 sgd_solver.cpp:112] Iteration 152030, lr = 0.001
I0523 05:22:55.919150 35003 solver.cpp:239] Iteration 152040 (2.70901 iter/s, 3.69139s/10 iters), loss = 6.92615
I0523 05:22:55.919328 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92615 (* 1 = 6.92615 loss)
I0523 05:22:55.945441 35003 sgd_solver.cpp:112] Iteration 152040, lr = 0.001
I0523 05:22:59.563171 35003 solver.cpp:239] Iteration 152050 (2.74447 iter/s, 3.64369s/10 iters), loss = 6.90188
I0523 05:22:59.563213 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90188 (* 1 = 6.90188 loss)
I0523 05:22:59.571995 35003 sgd_solver.cpp:112] Iteration 152050, lr = 0.001
I0523 05:23:02.554118 35003 solver.cpp:239] Iteration 152060 (3.34361 iter/s, 2.99078s/10 iters), loss = 6.63266
I0523 05:23:02.554162 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63266 (* 1 = 6.63266 loss)
I0523 05:23:03.178488 35003 sgd_solver.cpp:112] Iteration 152060, lr = 0.001
I0523 05:23:05.957299 35003 solver.cpp:239] Iteration 152070 (2.93859 iter/s, 3.40299s/10 iters), loss = 6.9457
I0523 05:23:05.957345 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9457 (* 1 = 6.9457 loss)
I0523 05:23:06.656761 35003 sgd_solver.cpp:112] Iteration 152070, lr = 0.001
I0523 05:23:09.639485 35003 solver.cpp:239] Iteration 152080 (2.71592 iter/s, 3.68199s/10 iters), loss = 7.11397
I0523 05:23:09.639533 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11397 (* 1 = 7.11397 loss)
I0523 05:23:10.361065 35003 sgd_solver.cpp:112] Iteration 152080, lr = 0.001
I0523 05:23:14.809437 35003 solver.cpp:239] Iteration 152090 (1.93435 iter/s, 5.16969s/10 iters), loss = 6.81595
I0523 05:23:14.809484 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81595 (* 1 = 6.81595 loss)
I0523 05:23:15.539958 35003 sgd_solver.cpp:112] Iteration 152090, lr = 0.001
I0523 05:23:18.412400 35003 solver.cpp:239] Iteration 152100 (2.77564 iter/s, 3.60277s/10 iters), loss = 6.03836
I0523 05:23:18.412443 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03836 (* 1 = 6.03836 loss)
I0523 05:23:19.145867 35003 sgd_solver.cpp:112] Iteration 152100, lr = 0.001
I0523 05:23:21.912904 35003 solver.cpp:239] Iteration 152110 (2.85688 iter/s, 3.50032s/10 iters), loss = 7.16772
I0523 05:23:21.912945 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16772 (* 1 = 7.16772 loss)
I0523 05:23:22.440491 35003 sgd_solver.cpp:112] Iteration 152110, lr = 0.001
I0523 05:23:26.693718 35003 solver.cpp:239] Iteration 152120 (2.0918 iter/s, 4.78058s/10 iters), loss = 5.64985
I0523 05:23:26.693958 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.64985 (* 1 = 5.64985 loss)
I0523 05:23:26.702226 35003 sgd_solver.cpp:112] Iteration 152120, lr = 0.001
I0523 05:23:29.542876 35003 solver.cpp:239] Iteration 152130 (3.51023 iter/s, 2.84882s/10 iters), loss = 6.33207
I0523 05:23:29.542920 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33207 (* 1 = 6.33207 loss)
I0523 05:23:29.556026 35003 sgd_solver.cpp:112] Iteration 152130, lr = 0.001
I0523 05:23:30.926645 35003 solver.cpp:239] Iteration 152140 (7.22721 iter/s, 1.38366s/10 iters), loss = 6.34194
I0523 05:23:30.926690 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34194 (* 1 = 6.34194 loss)
I0523 05:23:30.944751 35003 sgd_solver.cpp:112] Iteration 152140, lr = 0.001
I0523 05:23:34.705772 35003 solver.cpp:239] Iteration 152150 (2.64626 iter/s, 3.77892s/10 iters), loss = 6.6328
I0523 05:23:34.705835 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6328 (* 1 = 6.6328 loss)
I0523 05:23:34.719251 35003 sgd_solver.cpp:112] Iteration 152150, lr = 0.001
I0523 05:23:37.582849 35003 solver.cpp:239] Iteration 152160 (3.47598 iter/s, 2.87689s/10 iters), loss = 5.7995
I0523 05:23:37.582901 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7995 (* 1 = 5.7995 loss)
I0523 05:23:38.324306 35003 sgd_solver.cpp:112] Iteration 152160, lr = 0.001
I0523 05:23:42.462314 35003 solver.cpp:239] Iteration 152170 (2.04951 iter/s, 4.87921s/10 iters), loss = 6.47486
I0523 05:23:42.462363 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47486 (* 1 = 6.47486 loss)
I0523 05:23:43.161824 35003 sgd_solver.cpp:112] Iteration 152170, lr = 0.001
I0523 05:23:46.964797 35003 solver.cpp:239] Iteration 152180 (2.22111 iter/s, 4.50225s/10 iters), loss = 7.03843
I0523 05:23:46.964839 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03843 (* 1 = 7.03843 loss)
I0523 05:23:46.970067 35003 sgd_solver.cpp:112] Iteration 152180, lr = 0.001
I0523 05:23:50.526005 35003 solver.cpp:239] Iteration 152190 (2.80819 iter/s, 3.56102s/10 iters), loss = 7.64461
I0523 05:23:50.526051 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64461 (* 1 = 7.64461 loss)
I0523 05:23:50.534080 35003 sgd_solver.cpp:112] Iteration 152190, lr = 0.001
I0523 05:23:55.616928 35003 solver.cpp:239] Iteration 152200 (1.96438 iter/s, 5.09067s/10 iters), loss = 7.25153
I0523 05:23:55.616967 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25153 (* 1 = 7.25153 loss)
I0523 05:23:55.624977 35003 sgd_solver.cpp:112] Iteration 152200, lr = 0.001
I0523 05:23:57.598369 35003 solver.cpp:239] Iteration 152210 (5.04717 iter/s, 1.98131s/10 iters), loss = 6.63542
I0523 05:23:57.598644 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63542 (* 1 = 6.63542 loss)
I0523 05:23:57.612198 35003 sgd_solver.cpp:112] Iteration 152210, lr = 0.001
I0523 05:24:00.534734 35003 solver.cpp:239] Iteration 152220 (3.40604 iter/s, 2.93596s/10 iters), loss = 7.07431
I0523 05:24:00.534772 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07431 (* 1 = 7.07431 loss)
I0523 05:24:00.545608 35003 sgd_solver.cpp:112] Iteration 152220, lr = 0.001
I0523 05:24:04.159281 35003 solver.cpp:239] Iteration 152230 (2.75911 iter/s, 3.62436s/10 iters), loss = 6.59057
I0523 05:24:04.159322 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59057 (* 1 = 6.59057 loss)
I0523 05:24:04.201093 35003 sgd_solver.cpp:112] Iteration 152230, lr = 0.001
I0523 05:24:07.026234 35003 solver.cpp:239] Iteration 152240 (3.48822 iter/s, 2.86679s/10 iters), loss = 6.29437
I0523 05:24:07.026283 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29437 (* 1 = 6.29437 loss)
I0523 05:24:07.049042 35003 sgd_solver.cpp:112] Iteration 152240, lr = 0.001
I0523 05:24:11.338384 35003 solver.cpp:239] Iteration 152250 (2.31915 iter/s, 4.31192s/10 iters), loss = 6.2023
I0523 05:24:11.338433 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2023 (* 1 = 6.2023 loss)
I0523 05:24:12.053131 35003 sgd_solver.cpp:112] Iteration 152250, lr = 0.001
I0523 05:24:14.977636 35003 solver.cpp:239] Iteration 152260 (2.74797 iter/s, 3.63905s/10 iters), loss = 7.0124
I0523 05:24:14.977691 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0124 (* 1 = 7.0124 loss)
I0523 05:24:15.718623 35003 sgd_solver.cpp:112] Iteration 152260, lr = 0.001
I0523 05:24:18.757055 35003 solver.cpp:239] Iteration 152270 (2.64605 iter/s, 3.77921s/10 iters), loss = 7.17261
I0523 05:24:18.757092 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17261 (* 1 = 7.17261 loss)
I0523 05:24:18.782049 35003 sgd_solver.cpp:112] Iteration 152270, lr = 0.001
I0523 05:24:22.729425 35003 solver.cpp:239] Iteration 152280 (2.51752 iter/s, 3.97216s/10 iters), loss = 7.33942
I0523 05:24:22.729472 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33942 (* 1 = 7.33942 loss)
I0523 05:24:22.737527 35003 sgd_solver.cpp:112] Iteration 152280, lr = 0.001
I0523 05:24:26.303902 35003 solver.cpp:239] Iteration 152290 (2.79777 iter/s, 3.57428s/10 iters), loss = 6.10073
I0523 05:24:26.303947 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10073 (* 1 = 6.10073 loss)
I0523 05:24:26.328073 35003 sgd_solver.cpp:112] Iteration 152290, lr = 0.001
I0523 05:24:32.139094 35003 solver.cpp:239] Iteration 152300 (1.71382 iter/s, 5.8349s/10 iters), loss = 7.4109
I0523 05:24:32.139394 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4109 (* 1 = 7.4109 loss)
I0523 05:24:32.152282 35003 sgd_solver.cpp:112] Iteration 152300, lr = 0.001
I0523 05:24:34.942454 35003 solver.cpp:239] Iteration 152310 (3.56768 iter/s, 2.80294s/10 iters), loss = 5.7374
I0523 05:24:34.942518 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7374 (* 1 = 5.7374 loss)
I0523 05:24:34.962781 35003 sgd_solver.cpp:112] Iteration 152310, lr = 0.001
I0523 05:24:36.977614 35003 solver.cpp:239] Iteration 152320 (4.91399 iter/s, 2.03501s/10 iters), loss = 5.5693
I0523 05:24:36.977658 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.5693 (* 1 = 5.5693 loss)
I0523 05:24:36.990576 35003 sgd_solver.cpp:112] Iteration 152320, lr = 0.001
I0523 05:24:40.689642 35003 solver.cpp:239] Iteration 152330 (2.69409 iter/s, 3.71183s/10 iters), loss = 7.0149
I0523 05:24:40.689687 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0149 (* 1 = 7.0149 loss)
I0523 05:24:40.696512 35003 sgd_solver.cpp:112] Iteration 152330, lr = 0.001
I0523 05:24:43.974778 35003 solver.cpp:239] Iteration 152340 (3.04418 iter/s, 3.28495s/10 iters), loss = 6.662
I0523 05:24:43.974820 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.662 (* 1 = 6.662 loss)
I0523 05:24:43.987437 35003 sgd_solver.cpp:112] Iteration 152340, lr = 0.001
I0523 05:24:47.607203 35003 solver.cpp:239] Iteration 152350 (2.75315 iter/s, 3.63221s/10 iters), loss = 6.3833
I0523 05:24:47.607251 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3833 (* 1 = 6.3833 loss)
I0523 05:24:47.610350 35003 sgd_solver.cpp:112] Iteration 152350, lr = 0.001
I0523 05:24:50.969424 35003 solver.cpp:239] Iteration 152360 (2.9744 iter/s, 3.36202s/10 iters), loss = 6.00977
I0523 05:24:50.969463 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00977 (* 1 = 6.00977 loss)
I0523 05:24:51.683603 35003 sgd_solver.cpp:112] Iteration 152360, lr = 0.001
I0523 05:24:55.868201 35003 solver.cpp:239] Iteration 152370 (2.04143 iter/s, 4.89854s/10 iters), loss = 6.52731
I0523 05:24:55.868234 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52731 (* 1 = 6.52731 loss)
I0523 05:24:55.934588 35003 sgd_solver.cpp:112] Iteration 152370, lr = 0.001
I0523 05:24:58.614816 35003 solver.cpp:239] Iteration 152380 (3.64105 iter/s, 2.74646s/10 iters), loss = 6.36997
I0523 05:24:58.614866 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36997 (* 1 = 6.36997 loss)
I0523 05:24:58.619621 35003 sgd_solver.cpp:112] Iteration 152380, lr = 0.001
I0523 05:25:01.848501 35003 solver.cpp:239] Iteration 152390 (3.09262 iter/s, 3.2335s/10 iters), loss = 6.91159
I0523 05:25:01.848539 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91159 (* 1 = 6.91159 loss)
I0523 05:25:01.861868 35003 sgd_solver.cpp:112] Iteration 152390, lr = 0.001
I0523 05:25:05.203207 35003 solver.cpp:239] Iteration 152400 (2.98105 iter/s, 3.35452s/10 iters), loss = 6.88901
I0523 05:25:05.203341 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88901 (* 1 = 6.88901 loss)
I0523 05:25:05.281087 35003 sgd_solver.cpp:112] Iteration 152400, lr = 0.001
I0523 05:25:07.544229 35003 solver.cpp:239] Iteration 152410 (4.27207 iter/s, 2.34079s/10 iters), loss = 7.39357
I0523 05:25:07.544272 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39357 (* 1 = 7.39357 loss)
I0523 05:25:07.553078 35003 sgd_solver.cpp:112] Iteration 152410, lr = 0.001
I0523 05:25:10.706791 35003 solver.cpp:239] Iteration 152420 (3.16218 iter/s, 3.16237s/10 iters), loss = 7.70939
I0523 05:25:10.706841 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70939 (* 1 = 7.70939 loss)
I0523 05:25:10.716143 35003 sgd_solver.cpp:112] Iteration 152420, lr = 0.001
I0523 05:25:13.828063 35003 solver.cpp:239] Iteration 152430 (3.20401 iter/s, 3.12109s/10 iters), loss = 7.02149
I0523 05:25:13.828112 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02149 (* 1 = 7.02149 loss)
I0523 05:25:14.530685 35003 sgd_solver.cpp:112] Iteration 152430, lr = 0.001
I0523 05:25:19.822736 35003 solver.cpp:239] Iteration 152440 (1.66824 iter/s, 5.99436s/10 iters), loss = 6.88908
I0523 05:25:19.822785 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88908 (* 1 = 6.88908 loss)
I0523 05:25:20.156596 35003 sgd_solver.cpp:112] Iteration 152440, lr = 0.001
I0523 05:25:22.013108 35003 solver.cpp:239] Iteration 152450 (4.56575 iter/s, 2.19022s/10 iters), loss = 6.60544
I0523 05:25:22.013154 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60544 (* 1 = 6.60544 loss)
I0523 05:25:22.030984 35003 sgd_solver.cpp:112] Iteration 152450, lr = 0.001
I0523 05:25:26.311020 35003 solver.cpp:239] Iteration 152460 (2.32683 iter/s, 4.29769s/10 iters), loss = 6.3223
I0523 05:25:26.311069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3223 (* 1 = 6.3223 loss)
I0523 05:25:26.324645 35003 sgd_solver.cpp:112] Iteration 152460, lr = 0.001
I0523 05:25:29.298776 35003 solver.cpp:239] Iteration 152470 (3.3472 iter/s, 2.98757s/10 iters), loss = 6.6944
I0523 05:25:29.298827 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6944 (* 1 = 6.6944 loss)
I0523 05:25:29.306825 35003 sgd_solver.cpp:112] Iteration 152470, lr = 0.001
I0523 05:25:33.720605 35003 solver.cpp:239] Iteration 152480 (2.26163 iter/s, 4.4216s/10 iters), loss = 8.0358
I0523 05:25:33.720643 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0358 (* 1 = 8.0358 loss)
I0523 05:25:33.738140 35003 sgd_solver.cpp:112] Iteration 152480, lr = 0.001
I0523 05:25:37.983687 35003 solver.cpp:239] Iteration 152490 (2.34585 iter/s, 4.26285s/10 iters), loss = 6.97455
I0523 05:25:37.983978 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97455 (* 1 = 6.97455 loss)
I0523 05:25:38.718758 35003 sgd_solver.cpp:112] Iteration 152490, lr = 0.001
I0523 05:25:41.822008 35003 solver.cpp:239] Iteration 152500 (2.60559 iter/s, 3.8379s/10 iters), loss = 6.88388
I0523 05:25:41.822067 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88388 (* 1 = 6.88388 loss)
I0523 05:25:42.562923 35003 sgd_solver.cpp:112] Iteration 152500, lr = 0.001
I0523 05:25:47.243834 35003 solver.cpp:239] Iteration 152510 (1.84449 iter/s, 5.42155s/10 iters), loss = 6.88825
I0523 05:25:47.243881 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88825 (* 1 = 6.88825 loss)
I0523 05:25:47.984457 35003 sgd_solver.cpp:112] Iteration 152510, lr = 0.001
I0523 05:25:51.536222 35003 solver.cpp:239] Iteration 152520 (2.32983 iter/s, 4.29216s/10 iters), loss = 6.45546
I0523 05:25:51.536274 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45546 (* 1 = 6.45546 loss)
I0523 05:25:52.277056 35003 sgd_solver.cpp:112] Iteration 152520, lr = 0.001
I0523 05:25:55.710786 35003 solver.cpp:239] Iteration 152530 (2.39559 iter/s, 4.17434s/10 iters), loss = 7.76321
I0523 05:25:55.710839 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76321 (* 1 = 7.76321 loss)
I0523 05:25:55.716696 35003 sgd_solver.cpp:112] Iteration 152530, lr = 0.001
I0523 05:25:58.524235 35003 solver.cpp:239] Iteration 152540 (3.55457 iter/s, 2.81328s/10 iters), loss = 7.03722
I0523 05:25:58.524278 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03722 (* 1 = 7.03722 loss)
I0523 05:25:58.543658 35003 sgd_solver.cpp:112] Iteration 152540, lr = 0.001
I0523 05:26:02.768223 35003 solver.cpp:239] Iteration 152550 (2.3564 iter/s, 4.24375s/10 iters), loss = 6.66043
I0523 05:26:02.768282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66043 (* 1 = 6.66043 loss)
I0523 05:26:02.775372 35003 sgd_solver.cpp:112] Iteration 152550, lr = 0.001
I0523 05:26:06.478075 35003 solver.cpp:239] Iteration 152560 (2.69568 iter/s, 3.70964s/10 iters), loss = 7.9796
I0523 05:26:06.478116 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9796 (* 1 = 7.9796 loss)
I0523 05:26:06.485699 35003 sgd_solver.cpp:112] Iteration 152560, lr = 0.001
I0523 05:26:10.113924 35003 solver.cpp:239] Iteration 152570 (2.75054 iter/s, 3.63565s/10 iters), loss = 5.8923
I0523 05:26:10.114262 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8923 (* 1 = 5.8923 loss)
I0523 05:26:10.482486 35003 sgd_solver.cpp:112] Iteration 152570, lr = 0.001
I0523 05:26:13.545181 35003 solver.cpp:239] Iteration 152580 (2.91478 iter/s, 3.43079s/10 iters), loss = 5.69854
I0523 05:26:13.545224 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.69854 (* 1 = 5.69854 loss)
I0523 05:26:14.275740 35003 sgd_solver.cpp:112] Iteration 152580, lr = 0.001
I0523 05:26:17.299907 35003 solver.cpp:239] Iteration 152590 (2.66345 iter/s, 3.75452s/10 iters), loss = 6.12588
I0523 05:26:17.299954 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12588 (* 1 = 6.12588 loss)
I0523 05:26:17.313124 35003 sgd_solver.cpp:112] Iteration 152590, lr = 0.001
I0523 05:26:20.818555 35003 solver.cpp:239] Iteration 152600 (2.84216 iter/s, 3.51845s/10 iters), loss = 6.05348
I0523 05:26:20.818600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05348 (* 1 = 6.05348 loss)
I0523 05:26:20.831863 35003 sgd_solver.cpp:112] Iteration 152600, lr = 0.001
I0523 05:26:24.851766 35003 solver.cpp:239] Iteration 152610 (2.47954 iter/s, 4.033s/10 iters), loss = 6.07887
I0523 05:26:24.851804 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07887 (* 1 = 6.07887 loss)
I0523 05:26:25.163868 35003 sgd_solver.cpp:112] Iteration 152610, lr = 0.001
I0523 05:26:27.256675 35003 solver.cpp:239] Iteration 152620 (4.15843 iter/s, 2.40476s/10 iters), loss = 7.60245
I0523 05:26:27.256721 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60245 (* 1 = 7.60245 loss)
I0523 05:26:27.727099 35003 sgd_solver.cpp:112] Iteration 152620, lr = 0.001
I0523 05:26:31.816293 35003 solver.cpp:239] Iteration 152630 (2.19328 iter/s, 4.55938s/10 iters), loss = 7.44318
I0523 05:26:31.816347 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44318 (* 1 = 7.44318 loss)
I0523 05:26:32.557358 35003 sgd_solver.cpp:112] Iteration 152630, lr = 0.001
I0523 05:26:35.307024 35003 solver.cpp:239] Iteration 152640 (2.86489 iter/s, 3.49053s/10 iters), loss = 6.60804
I0523 05:26:35.307077 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60804 (* 1 = 6.60804 loss)
I0523 05:26:35.330389 35003 sgd_solver.cpp:112] Iteration 152640, lr = 0.001
I0523 05:26:39.336983 35003 solver.cpp:239] Iteration 152650 (2.48155 iter/s, 4.02974s/10 iters), loss = 6.42805
I0523 05:26:39.337026 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42805 (* 1 = 6.42805 loss)
I0523 05:26:39.350229 35003 sgd_solver.cpp:112] Iteration 152650, lr = 0.001
I0523 05:26:43.820152 35003 solver.cpp:239] Iteration 152660 (2.23068 iter/s, 4.48294s/10 iters), loss = 6.73121
I0523 05:26:43.820353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73121 (* 1 = 6.73121 loss)
I0523 05:26:44.560571 35003 sgd_solver.cpp:112] Iteration 152660, lr = 0.001
I0523 05:26:48.009534 35003 solver.cpp:239] Iteration 152670 (2.3872 iter/s, 4.18901s/10 iters), loss = 6.96687
I0523 05:26:48.009603 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96687 (* 1 = 6.96687 loss)
I0523 05:26:48.715348 35003 sgd_solver.cpp:112] Iteration 152670, lr = 0.001
I0523 05:26:53.087013 35003 solver.cpp:239] Iteration 152680 (1.96959 iter/s, 5.0772s/10 iters), loss = 7.28437
I0523 05:26:53.087069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28437 (* 1 = 7.28437 loss)
I0523 05:26:53.450992 35003 sgd_solver.cpp:112] Iteration 152680, lr = 0.001
I0523 05:26:55.665447 35003 solver.cpp:239] Iteration 152690 (3.87857 iter/s, 2.57827s/10 iters), loss = 6.1588
I0523 05:26:55.665485 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1588 (* 1 = 6.1588 loss)
I0523 05:26:55.683586 35003 sgd_solver.cpp:112] Iteration 152690, lr = 0.001
I0523 05:26:58.563791 35003 solver.cpp:239] Iteration 152700 (3.45044 iter/s, 2.89818s/10 iters), loss = 6.83874
I0523 05:26:58.563841 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83874 (* 1 = 6.83874 loss)
I0523 05:26:59.226897 35003 sgd_solver.cpp:112] Iteration 152700, lr = 0.001
I0523 05:27:01.305979 35003 solver.cpp:239] Iteration 152710 (3.64694 iter/s, 2.74202s/10 iters), loss = 6.46947
I0523 05:27:01.306018 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46947 (* 1 = 6.46947 loss)
I0523 05:27:01.313218 35003 sgd_solver.cpp:112] Iteration 152710, lr = 0.001
I0523 05:27:07.077407 35003 solver.cpp:239] Iteration 152720 (1.73276 iter/s, 5.77114s/10 iters), loss = 5.99936
I0523 05:27:07.077468 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99936 (* 1 = 5.99936 loss)
I0523 05:27:07.134009 35003 sgd_solver.cpp:112] Iteration 152720, lr = 0.001
I0523 05:27:12.157301 35003 solver.cpp:239] Iteration 152730 (1.96865 iter/s, 5.07963s/10 iters), loss = 6.38696
I0523 05:27:12.157343 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38696 (* 1 = 6.38696 loss)
I0523 05:27:12.169510 35003 sgd_solver.cpp:112] Iteration 152730, lr = 0.001
I0523 05:27:17.360277 35003 solver.cpp:239] Iteration 152740 (1.92207 iter/s, 5.20272s/10 iters), loss = 6.72066
I0523 05:27:17.360473 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72066 (* 1 = 6.72066 loss)
I0523 05:27:17.442839 35003 sgd_solver.cpp:112] Iteration 152740, lr = 0.001
I0523 05:27:22.446516 35003 solver.cpp:239] Iteration 152750 (1.96624 iter/s, 5.08584s/10 iters), loss = 6.90761
I0523 05:27:22.446553 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90761 (* 1 = 6.90761 loss)
I0523 05:27:22.468461 35003 sgd_solver.cpp:112] Iteration 152750, lr = 0.001
I0523 05:27:26.102691 35003 solver.cpp:239] Iteration 152760 (2.73525 iter/s, 3.65597s/10 iters), loss = 6.19197
I0523 05:27:26.102789 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19197 (* 1 = 6.19197 loss)
I0523 05:27:26.151633 35003 sgd_solver.cpp:112] Iteration 152760, lr = 0.001
I0523 05:27:28.873158 35003 solver.cpp:239] Iteration 152770 (3.60978 iter/s, 2.77026s/10 iters), loss = 7.43659
I0523 05:27:28.873203 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43659 (* 1 = 7.43659 loss)
I0523 05:27:28.881199 35003 sgd_solver.cpp:112] Iteration 152770, lr = 0.001
I0523 05:27:33.074981 35003 solver.cpp:239] Iteration 152780 (2.38004 iter/s, 4.2016s/10 iters), loss = 6.40995
I0523 05:27:33.075029 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40995 (* 1 = 6.40995 loss)
I0523 05:27:33.088050 35003 sgd_solver.cpp:112] Iteration 152780, lr = 0.001
I0523 05:27:37.511147 35003 solver.cpp:239] Iteration 152790 (2.25432 iter/s, 4.43594s/10 iters), loss = 5.99751
I0523 05:27:37.511196 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99751 (* 1 = 5.99751 loss)
I0523 05:27:37.518616 35003 sgd_solver.cpp:112] Iteration 152790, lr = 0.001
I0523 05:27:40.393733 35003 solver.cpp:239] Iteration 152800 (3.46932 iter/s, 2.88241s/10 iters), loss = 7.28949
I0523 05:27:40.393775 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28949 (* 1 = 7.28949 loss)
I0523 05:27:40.394541 35003 sgd_solver.cpp:112] Iteration 152800, lr = 0.001
I0523 05:27:42.419827 35003 solver.cpp:239] Iteration 152810 (4.93593 iter/s, 2.02596s/10 iters), loss = 7.57314
I0523 05:27:42.419878 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57314 (* 1 = 7.57314 loss)
I0523 05:27:42.427961 35003 sgd_solver.cpp:112] Iteration 152810, lr = 0.001
I0523 05:27:43.648654 35003 solver.cpp:239] Iteration 152820 (8.13865 iter/s, 1.22871s/10 iters), loss = 6.97248
I0523 05:27:43.648727 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97248 (* 1 = 6.97248 loss)
I0523 05:27:43.657732 35003 sgd_solver.cpp:112] Iteration 152820, lr = 0.001
I0523 05:27:44.872284 35003 solver.cpp:239] Iteration 152830 (8.17334 iter/s, 1.22349s/10 iters), loss = 7.65598
I0523 05:27:44.872333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65598 (* 1 = 7.65598 loss)
I0523 05:27:44.884793 35003 sgd_solver.cpp:112] Iteration 152830, lr = 0.001
I0523 05:27:45.692625 35003 solver.cpp:239] Iteration 152840 (12.1915 iter/s, 0.820243s/10 iters), loss = 6.71176
I0523 05:27:45.692667 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71176 (* 1 = 6.71176 loss)
I0523 05:27:45.709105 35003 sgd_solver.cpp:112] Iteration 152840, lr = 0.001
I0523 05:27:46.537066 35003 solver.cpp:239] Iteration 152850 (11.8434 iter/s, 0.844355s/10 iters), loss = 6.37535
I0523 05:27:46.537104 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37535 (* 1 = 6.37535 loss)
I0523 05:27:46.542799 35003 sgd_solver.cpp:112] Iteration 152850, lr = 0.001
I0523 05:27:49.896669 35003 solver.cpp:239] Iteration 152860 (2.9767 iter/s, 3.35942s/10 iters), loss = 7.23533
I0523 05:27:49.897004 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23533 (* 1 = 7.23533 loss)
I0523 05:27:49.914860 35003 sgd_solver.cpp:112] Iteration 152860, lr = 0.001
I0523 05:27:53.570855 35003 solver.cpp:239] Iteration 152870 (2.72203 iter/s, 3.67373s/10 iters), loss = 6.86279
I0523 05:27:53.570899 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86279 (* 1 = 6.86279 loss)
I0523 05:27:53.590646 35003 sgd_solver.cpp:112] Iteration 152870, lr = 0.001
I0523 05:27:57.969936 35003 solver.cpp:239] Iteration 152880 (2.27332 iter/s, 4.39886s/10 iters), loss = 6.06999
I0523 05:27:57.969971 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06999 (* 1 = 6.06999 loss)
I0523 05:27:57.983304 35003 sgd_solver.cpp:112] Iteration 152880, lr = 0.001
I0523 05:28:01.261271 35003 solver.cpp:239] Iteration 152890 (3.03844 iter/s, 3.29116s/10 iters), loss = 6.34057
I0523 05:28:01.261312 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34057 (* 1 = 6.34057 loss)
I0523 05:28:01.268709 35003 sgd_solver.cpp:112] Iteration 152890, lr = 0.001
I0523 05:28:05.720027 35003 solver.cpp:239] Iteration 152900 (2.24289 iter/s, 4.45853s/10 iters), loss = 7.10101
I0523 05:28:05.720073 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10101 (* 1 = 7.10101 loss)
I0523 05:28:05.725836 35003 sgd_solver.cpp:112] Iteration 152900, lr = 0.001
I0523 05:28:07.951774 35003 solver.cpp:239] Iteration 152910 (4.48109 iter/s, 2.2316s/10 iters), loss = 6.51361
I0523 05:28:07.951822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51361 (* 1 = 6.51361 loss)
I0523 05:28:08.686175 35003 sgd_solver.cpp:112] Iteration 152910, lr = 0.001
I0523 05:28:12.043395 35003 solver.cpp:239] Iteration 152920 (2.44415 iter/s, 4.09141s/10 iters), loss = 7.83714
I0523 05:28:12.043447 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83714 (* 1 = 7.83714 loss)
I0523 05:28:12.051159 35003 sgd_solver.cpp:112] Iteration 152920, lr = 0.001
I0523 05:28:15.492067 35003 solver.cpp:239] Iteration 152930 (2.89983 iter/s, 3.44848s/10 iters), loss = 6.16248
I0523 05:28:15.492118 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16248 (* 1 = 6.16248 loss)
I0523 05:28:15.498095 35003 sgd_solver.cpp:112] Iteration 152930, lr = 0.001
I0523 05:28:19.541121 35003 solver.cpp:239] Iteration 152940 (2.46984 iter/s, 4.04884s/10 iters), loss = 7.07612
I0523 05:28:19.541167 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07612 (* 1 = 7.07612 loss)
I0523 05:28:19.549070 35003 sgd_solver.cpp:112] Iteration 152940, lr = 0.001
I0523 05:28:24.447567 35003 solver.cpp:239] Iteration 152950 (2.03824 iter/s, 4.90619s/10 iters), loss = 4.88733
I0523 05:28:24.447814 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.88733 (* 1 = 4.88733 loss)
I0523 05:28:24.746457 35003 sgd_solver.cpp:112] Iteration 152950, lr = 0.001
I0523 05:28:27.577872 35003 solver.cpp:239] Iteration 152960 (3.19494 iter/s, 3.12995s/10 iters), loss = 6.89364
I0523 05:28:27.577919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89364 (* 1 = 6.89364 loss)
I0523 05:28:28.299142 35003 sgd_solver.cpp:112] Iteration 152960, lr = 0.001
I0523 05:28:30.366475 35003 solver.cpp:239] Iteration 152970 (3.58624 iter/s, 2.78843s/10 iters), loss = 6.87963
I0523 05:28:30.366528 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87963 (* 1 = 6.87963 loss)
I0523 05:28:30.374725 35003 sgd_solver.cpp:112] Iteration 152970, lr = 0.001
I0523 05:28:33.044999 35003 solver.cpp:239] Iteration 152980 (3.73363 iter/s, 2.67836s/10 iters), loss = 5.84495
I0523 05:28:33.045032 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84495 (* 1 = 5.84495 loss)
I0523 05:28:33.058113 35003 sgd_solver.cpp:112] Iteration 152980, lr = 0.001
I0523 05:28:37.188302 35003 solver.cpp:239] Iteration 152990 (2.41365 iter/s, 4.1431s/10 iters), loss = 7.0344
I0523 05:28:37.188344 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0344 (* 1 = 7.0344 loss)
I0523 05:28:37.200300 35003 sgd_solver.cpp:112] Iteration 152990, lr = 0.001
I0523 05:28:39.939338 35003 solver.cpp:239] Iteration 153000 (3.63522 iter/s, 2.75086s/10 iters), loss = 5.8769
I0523 05:28:39.939407 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8769 (* 1 = 5.8769 loss)
I0523 05:28:40.660773 35003 sgd_solver.cpp:112] Iteration 153000, lr = 0.001
I0523 05:28:44.267031 35003 solver.cpp:239] Iteration 153010 (2.31084 iter/s, 4.32744s/10 iters), loss = 7.0296
I0523 05:28:44.267074 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0296 (* 1 = 7.0296 loss)
I0523 05:28:44.295254 35003 sgd_solver.cpp:112] Iteration 153010, lr = 0.001
I0523 05:28:48.673126 35003 solver.cpp:239] Iteration 153020 (2.2697 iter/s, 4.40587s/10 iters), loss = 6.82948
I0523 05:28:48.673167 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82948 (* 1 = 6.82948 loss)
I0523 05:28:48.710003 35003 sgd_solver.cpp:112] Iteration 153020, lr = 0.001
I0523 05:28:51.567632 35003 solver.cpp:239] Iteration 153030 (3.45502 iter/s, 2.89434s/10 iters), loss = 5.93109
I0523 05:28:51.567689 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93109 (* 1 = 5.93109 loss)
I0523 05:28:52.301936 35003 sgd_solver.cpp:112] Iteration 153030, lr = 0.001
I0523 05:28:54.987767 35003 solver.cpp:239] Iteration 153040 (2.92403 iter/s, 3.41994s/10 iters), loss = 6.40057
I0523 05:28:54.988014 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40057 (* 1 = 6.40057 loss)
I0523 05:28:55.001857 35003 sgd_solver.cpp:112] Iteration 153040, lr = 0.001
I0523 05:28:59.234321 35003 solver.cpp:239] Iteration 153050 (2.35509 iter/s, 4.24612s/10 iters), loss = 6.72261
I0523 05:28:59.234371 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72261 (* 1 = 6.72261 loss)
I0523 05:28:59.247689 35003 sgd_solver.cpp:112] Iteration 153050, lr = 0.001
I0523 05:29:02.127688 35003 solver.cpp:239] Iteration 153060 (3.45639 iter/s, 2.89319s/10 iters), loss = 5.75811
I0523 05:29:02.127734 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.75811 (* 1 = 5.75811 loss)
I0523 05:29:02.869248 35003 sgd_solver.cpp:112] Iteration 153060, lr = 0.001
I0523 05:29:05.717873 35003 solver.cpp:239] Iteration 153070 (2.78552 iter/s, 3.58999s/10 iters), loss = 7.094
I0523 05:29:05.717914 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.094 (* 1 = 7.094 loss)
I0523 05:29:05.734292 35003 sgd_solver.cpp:112] Iteration 153070, lr = 0.001
I0523 05:29:09.373898 35003 solver.cpp:239] Iteration 153080 (2.73536 iter/s, 3.65582s/10 iters), loss = 7.49034
I0523 05:29:09.373973 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49034 (* 1 = 7.49034 loss)
I0523 05:29:09.393745 35003 sgd_solver.cpp:112] Iteration 153080, lr = 0.001
I0523 05:29:13.074496 35003 solver.cpp:239] Iteration 153090 (2.70243 iter/s, 3.70037s/10 iters), loss = 5.66226
I0523 05:29:13.074542 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.66226 (* 1 = 5.66226 loss)
I0523 05:29:13.699470 35003 sgd_solver.cpp:112] Iteration 153090, lr = 0.001
I0523 05:29:15.024701 35003 solver.cpp:239] Iteration 153100 (5.12802 iter/s, 1.95007s/10 iters), loss = 6.57965
I0523 05:29:15.024744 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57965 (* 1 = 6.57965 loss)
I0523 05:29:15.027174 35003 sgd_solver.cpp:112] Iteration 153100, lr = 0.001
I0523 05:29:17.828845 35003 solver.cpp:239] Iteration 153110 (3.56639 iter/s, 2.80396s/10 iters), loss = 7.34687
I0523 05:29:17.828893 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34687 (* 1 = 7.34687 loss)
I0523 05:29:17.835721 35003 sgd_solver.cpp:112] Iteration 153110, lr = 0.001
I0523 05:29:21.005959 35003 solver.cpp:239] Iteration 153120 (3.1477 iter/s, 3.17693s/10 iters), loss = 6.16189
I0523 05:29:21.006000 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16189 (* 1 = 6.16189 loss)
I0523 05:29:21.721534 35003 sgd_solver.cpp:112] Iteration 153120, lr = 0.001
I0523 05:29:25.201645 35003 solver.cpp:239] Iteration 153130 (2.38352 iter/s, 4.19547s/10 iters), loss = 6.02745
I0523 05:29:25.201930 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02745 (* 1 = 6.02745 loss)
I0523 05:29:25.909927 35003 sgd_solver.cpp:112] Iteration 153130, lr = 0.001
I0523 05:29:29.282948 35003 solver.cpp:239] Iteration 153140 (2.45045 iter/s, 4.08088s/10 iters), loss = 6.23576
I0523 05:29:29.282990 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23576 (* 1 = 6.23576 loss)
I0523 05:29:29.292901 35003 sgd_solver.cpp:112] Iteration 153140, lr = 0.001
I0523 05:29:33.603245 35003 solver.cpp:239] Iteration 153150 (2.31478 iter/s, 4.32007s/10 iters), loss = 7.44294
I0523 05:29:33.603289 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44294 (* 1 = 7.44294 loss)
I0523 05:29:34.334084 35003 sgd_solver.cpp:112] Iteration 153150, lr = 0.001
I0523 05:29:37.169625 35003 solver.cpp:239] Iteration 153160 (2.80412 iter/s, 3.56618s/10 iters), loss = 5.89892
I0523 05:29:37.169682 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89892 (* 1 = 5.89892 loss)
I0523 05:29:37.428383 35003 sgd_solver.cpp:112] Iteration 153160, lr = 0.001
I0523 05:29:39.539687 35003 solver.cpp:239] Iteration 153170 (4.2196 iter/s, 2.36989s/10 iters), loss = 7.26752
I0523 05:29:39.539738 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26752 (* 1 = 7.26752 loss)
I0523 05:29:39.548437 35003 sgd_solver.cpp:112] Iteration 153170, lr = 0.001
I0523 05:29:43.659304 35003 solver.cpp:239] Iteration 153180 (2.42754 iter/s, 4.1194s/10 iters), loss = 6.32537
I0523 05:29:43.659353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32537 (* 1 = 6.32537 loss)
I0523 05:29:43.698532 35003 sgd_solver.cpp:112] Iteration 153180, lr = 0.001
I0523 05:29:47.215770 35003 solver.cpp:239] Iteration 153190 (2.81193 iter/s, 3.55627s/10 iters), loss = 7.0633
I0523 05:29:47.215814 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0633 (* 1 = 7.0633 loss)
I0523 05:29:47.243955 35003 sgd_solver.cpp:112] Iteration 153190, lr = 0.001
I0523 05:29:51.616294 35003 solver.cpp:239] Iteration 153200 (2.27257 iter/s, 4.4003s/10 iters), loss = 7.56343
I0523 05:29:51.616338 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56343 (* 1 = 7.56343 loss)
I0523 05:29:51.629053 35003 sgd_solver.cpp:112] Iteration 153200, lr = 0.001
I0523 05:29:54.523136 35003 solver.cpp:239] Iteration 153210 (3.44036 iter/s, 2.90668s/10 iters), loss = 6.05964
I0523 05:29:54.523180 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05964 (* 1 = 6.05964 loss)
I0523 05:29:54.536026 35003 sgd_solver.cpp:112] Iteration 153210, lr = 0.001
I0523 05:29:57.313987 35003 solver.cpp:239] Iteration 153220 (3.58335 iter/s, 2.79068s/10 iters), loss = 5.82025
I0523 05:29:57.314132 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.82025 (* 1 = 5.82025 loss)
I0523 05:29:57.320466 35003 sgd_solver.cpp:112] Iteration 153220, lr = 0.001
I0523 05:29:58.713649 35003 solver.cpp:239] Iteration 153230 (7.14566 iter/s, 1.39945s/10 iters), loss = 6.50461
I0523 05:29:58.713697 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50461 (* 1 = 6.50461 loss)
I0523 05:29:59.384651 35003 sgd_solver.cpp:112] Iteration 153230, lr = 0.001
I0523 05:30:01.452287 35003 solver.cpp:239] Iteration 153240 (3.65167 iter/s, 2.73848s/10 iters), loss = 6.78675
I0523 05:30:01.452325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78675 (* 1 = 6.78675 loss)
I0523 05:30:01.844558 35003 sgd_solver.cpp:112] Iteration 153240, lr = 0.001
I0523 05:30:06.392904 35003 solver.cpp:239] Iteration 153250 (2.02414 iter/s, 4.94038s/10 iters), loss = 6.31958
I0523 05:30:06.392961 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31958 (* 1 = 6.31958 loss)
I0523 05:30:06.444306 35003 sgd_solver.cpp:112] Iteration 153250, lr = 0.001
I0523 05:30:08.748275 35003 solver.cpp:239] Iteration 153260 (4.2459 iter/s, 2.35521s/10 iters), loss = 7.14662
I0523 05:30:08.748314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14662 (* 1 = 7.14662 loss)
I0523 05:30:08.755595 35003 sgd_solver.cpp:112] Iteration 153260, lr = 0.001
I0523 05:30:14.164119 35003 solver.cpp:239] Iteration 153270 (1.84653 iter/s, 5.41558s/10 iters), loss = 7.26376
I0523 05:30:14.164160 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26376 (* 1 = 7.26376 loss)
I0523 05:30:14.176949 35003 sgd_solver.cpp:112] Iteration 153270, lr = 0.001
I0523 05:30:16.114157 35003 solver.cpp:239] Iteration 153280 (5.12844 iter/s, 1.94991s/10 iters), loss = 7.46066
I0523 05:30:16.114198 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46066 (* 1 = 7.46066 loss)
I0523 05:30:16.119356 35003 sgd_solver.cpp:112] Iteration 153280, lr = 0.001
I0523 05:30:21.185232 35003 solver.cpp:239] Iteration 153290 (1.97206 iter/s, 5.07083s/10 iters), loss = 6.82457
I0523 05:30:21.185272 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82457 (* 1 = 6.82457 loss)
I0523 05:30:21.193256 35003 sgd_solver.cpp:112] Iteration 153290, lr = 0.001
I0523 05:30:22.550016 35003 solver.cpp:239] Iteration 153300 (7.32772 iter/s, 1.36468s/10 iters), loss = 6.8802
I0523 05:30:22.550056 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8802 (* 1 = 6.8802 loss)
I0523 05:30:23.239212 35003 sgd_solver.cpp:112] Iteration 153300, lr = 0.001
I0523 05:30:25.908993 35003 solver.cpp:239] Iteration 153310 (2.97726 iter/s, 3.35879s/10 iters), loss = 7.28488
I0523 05:30:25.909037 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28488 (* 1 = 7.28488 loss)
I0523 05:30:26.444995 35003 sgd_solver.cpp:112] Iteration 153310, lr = 0.001
I0523 05:30:29.136624 35003 solver.cpp:239] Iteration 153320 (3.09842 iter/s, 3.22745s/10 iters), loss = 5.74942
I0523 05:30:29.136889 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74942 (* 1 = 5.74942 loss)
I0523 05:30:29.142247 35003 sgd_solver.cpp:112] Iteration 153320, lr = 0.001
I0523 05:30:30.802397 35003 solver.cpp:239] Iteration 153330 (6.00441 iter/s, 1.66544s/10 iters), loss = 7.5488
I0523 05:30:30.802453 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5488 (* 1 = 7.5488 loss)
I0523 05:30:31.530386 35003 sgd_solver.cpp:112] Iteration 153330, lr = 0.001
I0523 05:30:35.529604 35003 solver.cpp:239] Iteration 153340 (2.11552 iter/s, 4.72696s/10 iters), loss = 6.18903
I0523 05:30:35.529640 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18903 (* 1 = 6.18903 loss)
I0523 05:30:35.542578 35003 sgd_solver.cpp:112] Iteration 153340, lr = 0.001
I0523 05:30:37.678725 35003 solver.cpp:239] Iteration 153350 (4.65342 iter/s, 2.14896s/10 iters), loss = 6.94779
I0523 05:30:37.678786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94779 (* 1 = 6.94779 loss)
I0523 05:30:37.686168 35003 sgd_solver.cpp:112] Iteration 153350, lr = 0.001
I0523 05:30:43.687752 35003 solver.cpp:239] Iteration 153360 (1.66425 iter/s, 6.00872s/10 iters), loss = 6.62759
I0523 05:30:43.687803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62759 (* 1 = 6.62759 loss)
I0523 05:30:44.406926 35003 sgd_solver.cpp:112] Iteration 153360, lr = 0.001
I0523 05:30:47.074848 35003 solver.cpp:239] Iteration 153370 (2.95255 iter/s, 3.3869s/10 iters), loss = 6.73418
I0523 05:30:47.074899 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73418 (* 1 = 6.73418 loss)
I0523 05:30:47.087699 35003 sgd_solver.cpp:112] Iteration 153370, lr = 0.001
I0523 05:30:50.599370 35003 solver.cpp:239] Iteration 153380 (2.83742 iter/s, 3.52433s/10 iters), loss = 6.04399
I0523 05:30:50.599414 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04399 (* 1 = 6.04399 loss)
I0523 05:30:51.321707 35003 sgd_solver.cpp:112] Iteration 153380, lr = 0.001
I0523 05:30:55.469491 35003 solver.cpp:239] Iteration 153390 (2.05344 iter/s, 4.86988s/10 iters), loss = 7.09047
I0523 05:30:55.469543 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09047 (* 1 = 7.09047 loss)
I0523 05:30:55.477375 35003 sgd_solver.cpp:112] Iteration 153390, lr = 0.001
I0523 05:30:59.904323 35003 solver.cpp:239] Iteration 153400 (2.255 iter/s, 4.4346s/10 iters), loss = 5.30368
I0523 05:30:59.904599 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.30368 (* 1 = 5.30368 loss)
I0523 05:30:59.930302 35003 sgd_solver.cpp:112] Iteration 153400, lr = 0.001
I0523 05:31:02.054913 35003 solver.cpp:239] Iteration 153410 (4.65064 iter/s, 2.15024s/10 iters), loss = 7.82479
I0523 05:31:02.054960 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82479 (* 1 = 7.82479 loss)
I0523 05:31:02.739295 35003 sgd_solver.cpp:112] Iteration 153410, lr = 0.001
I0523 05:31:06.574991 35003 solver.cpp:239] Iteration 153420 (2.21247 iter/s, 4.51983s/10 iters), loss = 6.26303
I0523 05:31:06.575047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26303 (* 1 = 6.26303 loss)
I0523 05:31:06.578794 35003 sgd_solver.cpp:112] Iteration 153420, lr = 0.001
I0523 05:31:09.499135 35003 solver.cpp:239] Iteration 153430 (3.42003 iter/s, 2.92395s/10 iters), loss = 5.96906
I0523 05:31:09.499193 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96906 (* 1 = 5.96906 loss)
I0523 05:31:09.551230 35003 sgd_solver.cpp:112] Iteration 153430, lr = 0.001
I0523 05:31:12.402725 35003 solver.cpp:239] Iteration 153440 (3.44423 iter/s, 2.90341s/10 iters), loss = 7.46756
I0523 05:31:12.402766 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46756 (* 1 = 7.46756 loss)
I0523 05:31:12.409540 35003 sgd_solver.cpp:112] Iteration 153440, lr = 0.001
I0523 05:31:16.678791 35003 solver.cpp:239] Iteration 153450 (2.33872 iter/s, 4.27585s/10 iters), loss = 6.75619
I0523 05:31:16.678833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75619 (* 1 = 6.75619 loss)
I0523 05:31:16.687031 35003 sgd_solver.cpp:112] Iteration 153450, lr = 0.001
I0523 05:31:20.342384 35003 solver.cpp:239] Iteration 153460 (2.72971 iter/s, 3.66339s/10 iters), loss = 5.95218
I0523 05:31:20.342434 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95218 (* 1 = 5.95218 loss)
I0523 05:31:20.346434 35003 sgd_solver.cpp:112] Iteration 153460, lr = 0.001
I0523 05:31:23.153277 35003 solver.cpp:239] Iteration 153470 (3.5578 iter/s, 2.81073s/10 iters), loss = 6.49956
I0523 05:31:23.153327 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49956 (* 1 = 6.49956 loss)
I0523 05:31:23.159193 35003 sgd_solver.cpp:112] Iteration 153470, lr = 0.001
I0523 05:31:27.520999 35003 solver.cpp:239] Iteration 153480 (2.28965 iter/s, 4.36747s/10 iters), loss = 6.1882
I0523 05:31:27.521059 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1882 (* 1 = 6.1882 loss)
I0523 05:31:27.586356 35003 sgd_solver.cpp:112] Iteration 153480, lr = 0.001
I0523 05:31:29.726868 35003 solver.cpp:239] Iteration 153490 (4.53372 iter/s, 2.20569s/10 iters), loss = 6.403
I0523 05:31:29.726905 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.403 (* 1 = 6.403 loss)
I0523 05:31:29.759891 35003 sgd_solver.cpp:112] Iteration 153490, lr = 0.001
I0523 05:31:34.105672 35003 solver.cpp:239] Iteration 153500 (2.28384 iter/s, 4.37859s/10 iters), loss = 7.49836
I0523 05:31:34.105974 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49836 (* 1 = 7.49836 loss)
I0523 05:31:34.224279 35003 sgd_solver.cpp:112] Iteration 153500, lr = 0.001
I0523 05:31:39.308362 35003 solver.cpp:239] Iteration 153510 (1.92227 iter/s, 5.20219s/10 iters), loss = 7.33085
I0523 05:31:39.308419 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33085 (* 1 = 7.33085 loss)
I0523 05:31:39.652870 35003 sgd_solver.cpp:112] Iteration 153510, lr = 0.001
I0523 05:31:43.087272 35003 solver.cpp:239] Iteration 153520 (2.64641 iter/s, 3.7787s/10 iters), loss = 6.48699
I0523 05:31:43.087311 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48699 (* 1 = 6.48699 loss)
I0523 05:31:43.091951 35003 sgd_solver.cpp:112] Iteration 153520, lr = 0.001
I0523 05:31:46.224411 35003 solver.cpp:239] Iteration 153530 (3.1878 iter/s, 3.13696s/10 iters), loss = 6.30803
I0523 05:31:46.224465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30803 (* 1 = 6.30803 loss)
I0523 05:31:46.402196 35003 sgd_solver.cpp:112] Iteration 153530, lr = 0.001
I0523 05:31:48.620290 35003 solver.cpp:239] Iteration 153540 (4.17412 iter/s, 2.39572s/10 iters), loss = 6.78677
I0523 05:31:48.620343 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78677 (* 1 = 6.78677 loss)
I0523 05:31:48.625290 35003 sgd_solver.cpp:112] Iteration 153540, lr = 0.001
I0523 05:31:52.246220 35003 solver.cpp:239] Iteration 153550 (2.75809 iter/s, 3.6257s/10 iters), loss = 6.81799
I0523 05:31:52.246286 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81799 (* 1 = 6.81799 loss)
I0523 05:31:52.250859 35003 sgd_solver.cpp:112] Iteration 153550, lr = 0.001
I0523 05:31:56.003608 35003 solver.cpp:239] Iteration 153560 (2.66158 iter/s, 3.75717s/10 iters), loss = 7.27024
I0523 05:31:56.003649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27024 (* 1 = 7.27024 loss)
I0523 05:31:56.012692 35003 sgd_solver.cpp:112] Iteration 153560, lr = 0.001
I0523 05:32:00.318799 35003 solver.cpp:239] Iteration 153570 (2.31751 iter/s, 4.31497s/10 iters), loss = 6.9053
I0523 05:32:00.318845 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9053 (* 1 = 6.9053 loss)
I0523 05:32:00.326514 35003 sgd_solver.cpp:112] Iteration 153570, lr = 0.001
I0523 05:32:02.419677 35003 solver.cpp:239] Iteration 153580 (4.76024 iter/s, 2.10074s/10 iters), loss = 7.971
I0523 05:32:02.419719 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.971 (* 1 = 7.971 loss)
I0523 05:32:02.433046 35003 sgd_solver.cpp:112] Iteration 153580, lr = 0.001
I0523 05:32:05.905588 35003 solver.cpp:239] Iteration 153590 (2.86885 iter/s, 3.48572s/10 iters), loss = 6.43531
I0523 05:32:05.905740 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43531 (* 1 = 6.43531 loss)
I0523 05:32:06.119534 35003 sgd_solver.cpp:112] Iteration 153590, lr = 0.001
I0523 05:32:09.631563 35003 solver.cpp:239] Iteration 153600 (2.68408 iter/s, 3.72567s/10 iters), loss = 5.55331
I0523 05:32:09.631616 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.55331 (* 1 = 5.55331 loss)
I0523 05:32:09.636608 35003 sgd_solver.cpp:112] Iteration 153600, lr = 0.001
I0523 05:32:14.093715 35003 solver.cpp:239] Iteration 153610 (2.24119 iter/s, 4.46192s/10 iters), loss = 7.29815
I0523 05:32:14.093770 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29815 (* 1 = 7.29815 loss)
I0523 05:32:14.835067 35003 sgd_solver.cpp:112] Iteration 153610, lr = 0.001
I0523 05:32:18.733626 35003 solver.cpp:239] Iteration 153620 (2.15533 iter/s, 4.63967s/10 iters), loss = 7.13121
I0523 05:32:18.733661 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13121 (* 1 = 7.13121 loss)
I0523 05:32:18.746582 35003 sgd_solver.cpp:112] Iteration 153620, lr = 0.001
I0523 05:32:22.952039 35003 solver.cpp:239] Iteration 153630 (2.37068 iter/s, 4.2182s/10 iters), loss = 7.0634
I0523 05:32:22.952090 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0634 (* 1 = 7.0634 loss)
I0523 05:32:22.959775 35003 sgd_solver.cpp:112] Iteration 153630, lr = 0.001
I0523 05:32:25.749311 35003 solver.cpp:239] Iteration 153640 (3.57513 iter/s, 2.7971s/10 iters), loss = 6.42109
I0523 05:32:25.749377 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42109 (* 1 = 6.42109 loss)
I0523 05:32:26.477131 35003 sgd_solver.cpp:112] Iteration 153640, lr = 0.001
I0523 05:32:29.167824 35003 solver.cpp:239] Iteration 153650 (2.92542 iter/s, 3.41831s/10 iters), loss = 6.38374
I0523 05:32:29.167867 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38374 (* 1 = 6.38374 loss)
I0523 05:32:29.791092 35003 sgd_solver.cpp:112] Iteration 153650, lr = 0.001
I0523 05:32:33.022809 35003 solver.cpp:239] Iteration 153660 (2.59418 iter/s, 3.85478s/10 iters), loss = 6.66688
I0523 05:32:33.022846 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66688 (* 1 = 6.66688 loss)
I0523 05:32:33.030382 35003 sgd_solver.cpp:112] Iteration 153660, lr = 0.001
I0523 05:32:36.654785 35003 solver.cpp:239] Iteration 153670 (2.75347 iter/s, 3.63178s/10 iters), loss = 5.97054
I0523 05:32:36.655086 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97054 (* 1 = 5.97054 loss)
I0523 05:32:36.670261 35003 sgd_solver.cpp:112] Iteration 153670, lr = 0.001
I0523 05:32:40.975090 35003 solver.cpp:239] Iteration 153680 (2.31489 iter/s, 4.31986s/10 iters), loss = 7.4258
I0523 05:32:40.975138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4258 (* 1 = 7.4258 loss)
I0523 05:32:41.689802 35003 sgd_solver.cpp:112] Iteration 153680, lr = 0.001
I0523 05:32:45.344180 35003 solver.cpp:239] Iteration 153690 (2.28893 iter/s, 4.36885s/10 iters), loss = 6.92344
I0523 05:32:45.344241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92344 (* 1 = 6.92344 loss)
I0523 05:32:45.701500 35003 sgd_solver.cpp:112] Iteration 153690, lr = 0.001
I0523 05:32:49.757675 35003 solver.cpp:239] Iteration 153700 (2.2659 iter/s, 4.41325s/10 iters), loss = 6.46048
I0523 05:32:49.757719 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46048 (* 1 = 6.46048 loss)
I0523 05:32:49.765945 35003 sgd_solver.cpp:112] Iteration 153700, lr = 0.001
I0523 05:32:51.097645 35003 solver.cpp:239] Iteration 153710 (7.46347 iter/s, 1.33986s/10 iters), loss = 5.37754
I0523 05:32:51.097697 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.37754 (* 1 = 5.37754 loss)
I0523 05:32:51.838508 35003 sgd_solver.cpp:112] Iteration 153710, lr = 0.001
I0523 05:32:57.854363 35003 solver.cpp:239] Iteration 153720 (1.48008 iter/s, 6.75638s/10 iters), loss = 6.98669
I0523 05:32:57.854416 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98669 (* 1 = 6.98669 loss)
I0523 05:32:57.866113 35003 sgd_solver.cpp:112] Iteration 153720, lr = 0.001
I0523 05:33:01.291672 35003 solver.cpp:239] Iteration 153730 (2.90943 iter/s, 3.4371s/10 iters), loss = 6.03917
I0523 05:33:01.291731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03917 (* 1 = 6.03917 loss)
I0523 05:33:01.296661 35003 sgd_solver.cpp:112] Iteration 153730, lr = 0.001
I0523 05:33:04.955713 35003 solver.cpp:239] Iteration 153740 (2.72939 iter/s, 3.66382s/10 iters), loss = 7.23962
I0523 05:33:04.955776 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23962 (* 1 = 7.23962 loss)
I0523 05:33:05.696703 35003 sgd_solver.cpp:112] Iteration 153740, lr = 0.001
I0523 05:33:08.669648 35003 solver.cpp:239] Iteration 153750 (2.69272 iter/s, 3.71372s/10 iters), loss = 6.38949
I0523 05:33:08.669888 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38949 (* 1 = 6.38949 loss)
I0523 05:33:08.682826 35003 sgd_solver.cpp:112] Iteration 153750, lr = 0.001
I0523 05:33:12.520786 35003 solver.cpp:239] Iteration 153760 (2.59689 iter/s, 3.85076s/10 iters), loss = 5.23455
I0523 05:33:12.520838 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.23455 (* 1 = 5.23455 loss)
I0523 05:33:12.527112 35003 sgd_solver.cpp:112] Iteration 153760, lr = 0.001
I0523 05:33:15.324820 35003 solver.cpp:239] Iteration 153770 (3.56652 iter/s, 2.80385s/10 iters), loss = 7.07911
I0523 05:33:15.324865 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07911 (* 1 = 7.07911 loss)
I0523 05:33:15.332492 35003 sgd_solver.cpp:112] Iteration 153770, lr = 0.001
I0523 05:33:17.604741 35003 solver.cpp:239] Iteration 153780 (4.38639 iter/s, 2.27978s/10 iters), loss = 6.75578
I0523 05:33:17.604784 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75578 (* 1 = 6.75578 loss)
I0523 05:33:17.618008 35003 sgd_solver.cpp:112] Iteration 153780, lr = 0.001
I0523 05:33:19.272145 35003 solver.cpp:239] Iteration 153790 (5.99777 iter/s, 1.66729s/10 iters), loss = 7.2781
I0523 05:33:19.272187 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2781 (* 1 = 7.2781 loss)
I0523 05:33:19.615654 35003 sgd_solver.cpp:112] Iteration 153790, lr = 0.001
I0523 05:33:21.652994 35003 solver.cpp:239] Iteration 153800 (4.20045 iter/s, 2.3807s/10 iters), loss = 6.82074
I0523 05:33:21.653050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82074 (* 1 = 6.82074 loss)
I0523 05:33:21.671504 35003 sgd_solver.cpp:112] Iteration 153800, lr = 0.001
I0523 05:33:24.588773 35003 solver.cpp:239] Iteration 153810 (3.40646 iter/s, 2.9356s/10 iters), loss = 7.30457
I0523 05:33:24.588824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30457 (* 1 = 7.30457 loss)
I0523 05:33:24.958921 35003 sgd_solver.cpp:112] Iteration 153810, lr = 0.001
I0523 05:33:28.416951 35003 solver.cpp:239] Iteration 153820 (2.61235 iter/s, 3.82797s/10 iters), loss = 7.68926
I0523 05:33:28.416997 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68926 (* 1 = 7.68926 loss)
I0523 05:33:28.429917 35003 sgd_solver.cpp:112] Iteration 153820, lr = 0.001
I0523 05:33:32.284468 35003 solver.cpp:239] Iteration 153830 (2.58577 iter/s, 3.86731s/10 iters), loss = 6.23412
I0523 05:33:32.284512 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23412 (* 1 = 6.23412 loss)
I0523 05:33:32.293881 35003 sgd_solver.cpp:112] Iteration 153830, lr = 0.001
I0523 05:33:35.834336 35003 solver.cpp:239] Iteration 153840 (2.81716 iter/s, 3.54968s/10 iters), loss = 6.49777
I0523 05:33:35.834378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49777 (* 1 = 6.49777 loss)
I0523 05:33:35.851713 35003 sgd_solver.cpp:112] Iteration 153840, lr = 0.001
I0523 05:33:38.584156 35003 solver.cpp:239] Iteration 153850 (3.63682 iter/s, 2.74966s/10 iters), loss = 7.43421
I0523 05:33:38.584208 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43421 (* 1 = 7.43421 loss)
I0523 05:33:39.193871 35003 sgd_solver.cpp:112] Iteration 153850, lr = 0.001
I0523 05:33:42.010478 35003 solver.cpp:239] Iteration 153860 (2.91875 iter/s, 3.42612s/10 iters), loss = 6.59853
I0523 05:33:42.010519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59853 (* 1 = 6.59853 loss)
I0523 05:33:42.018183 35003 sgd_solver.cpp:112] Iteration 153860, lr = 0.001
I0523 05:33:44.886324 35003 solver.cpp:239] Iteration 153870 (3.47743 iter/s, 2.87569s/10 iters), loss = 6.73117
I0523 05:33:44.886364 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73117 (* 1 = 6.73117 loss)
I0523 05:33:45.365628 35003 sgd_solver.cpp:112] Iteration 153870, lr = 0.001
I0523 05:33:48.313287 35003 solver.cpp:239] Iteration 153880 (2.91819 iter/s, 3.42678s/10 iters), loss = 5.71787
I0523 05:33:48.313329 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.71787 (* 1 = 5.71787 loss)
I0523 05:33:49.041891 35003 sgd_solver.cpp:112] Iteration 153880, lr = 0.001
I0523 05:33:51.970073 35003 solver.cpp:239] Iteration 153890 (2.73479 iter/s, 3.65659s/10 iters), loss = 6.17478
I0523 05:33:51.970125 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17478 (* 1 = 6.17478 loss)
I0523 05:33:52.679154 35003 sgd_solver.cpp:112] Iteration 153890, lr = 0.001
I0523 05:33:57.051626 35003 solver.cpp:239] Iteration 153900 (1.968 iter/s, 5.08129s/10 iters), loss = 6.86618
I0523 05:33:57.051671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86618 (* 1 = 6.86618 loss)
I0523 05:33:57.071161 35003 sgd_solver.cpp:112] Iteration 153900, lr = 0.001
I0523 05:34:00.528723 35003 solver.cpp:239] Iteration 153910 (2.87612 iter/s, 3.47691s/10 iters), loss = 6.6834
I0523 05:34:00.528771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6834 (* 1 = 6.6834 loss)
I0523 05:34:00.546818 35003 sgd_solver.cpp:112] Iteration 153910, lr = 0.001
I0523 05:34:02.646330 35003 solver.cpp:239] Iteration 153920 (4.72264 iter/s, 2.11746s/10 iters), loss = 7.5085
I0523 05:34:02.646383 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5085 (* 1 = 7.5085 loss)
I0523 05:34:02.654212 35003 sgd_solver.cpp:112] Iteration 153920, lr = 0.001
I0523 05:34:07.727833 35003 solver.cpp:239] Iteration 153930 (1.96802 iter/s, 5.08125s/10 iters), loss = 7.50247
I0523 05:34:07.727892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50247 (* 1 = 7.50247 loss)
I0523 05:34:07.768640 35003 sgd_solver.cpp:112] Iteration 153930, lr = 0.001
I0523 05:34:10.577286 35003 solver.cpp:239] Iteration 153940 (3.50967 iter/s, 2.84927s/10 iters), loss = 7.62971
I0523 05:34:10.577448 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62971 (* 1 = 7.62971 loss)
I0523 05:34:10.590487 35003 sgd_solver.cpp:112] Iteration 153940, lr = 0.001
I0523 05:34:12.870843 35003 solver.cpp:239] Iteration 153950 (4.36049 iter/s, 2.29332s/10 iters), loss = 7.13341
I0523 05:34:12.870880 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13341 (* 1 = 7.13341 loss)
I0523 05:34:12.895830 35003 sgd_solver.cpp:112] Iteration 153950, lr = 0.001
I0523 05:34:18.544939 35003 solver.cpp:239] Iteration 153960 (1.76248 iter/s, 5.67383s/10 iters), loss = 8.13004
I0523 05:34:18.544977 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.13004 (* 1 = 8.13004 loss)
I0523 05:34:18.557960 35003 sgd_solver.cpp:112] Iteration 153960, lr = 0.001
I0523 05:34:21.940429 35003 solver.cpp:239] Iteration 153970 (2.94524 iter/s, 3.39531s/10 iters), loss = 7.31369
I0523 05:34:21.940469 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31369 (* 1 = 7.31369 loss)
I0523 05:34:22.576823 35003 sgd_solver.cpp:112] Iteration 153970, lr = 0.001
I0523 05:34:26.050792 35003 solver.cpp:239] Iteration 153980 (2.433 iter/s, 4.11015s/10 iters), loss = 6.62542
I0523 05:34:26.050863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62542 (* 1 = 6.62542 loss)
I0523 05:34:26.058586 35003 sgd_solver.cpp:112] Iteration 153980, lr = 0.001
I0523 05:34:27.844678 35003 solver.cpp:239] Iteration 153990 (5.57495 iter/s, 1.79374s/10 iters), loss = 7.3238
I0523 05:34:27.844738 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3238 (* 1 = 7.3238 loss)
I0523 05:34:27.857286 35003 sgd_solver.cpp:112] Iteration 153990, lr = 0.001
I0523 05:34:31.431360 35003 solver.cpp:239] Iteration 154000 (2.78825 iter/s, 3.58647s/10 iters), loss = 8.18181
I0523 05:34:31.431417 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.18181 (* 1 = 8.18181 loss)
I0523 05:34:32.165591 35003 sgd_solver.cpp:112] Iteration 154000, lr = 0.001
I0523 05:34:34.373859 35003 solver.cpp:239] Iteration 154010 (3.39868 iter/s, 2.94232s/10 iters), loss = 5.52324
I0523 05:34:34.373898 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.52324 (* 1 = 5.52324 loss)
I0523 05:34:34.381005 35003 sgd_solver.cpp:112] Iteration 154010, lr = 0.001
I0523 05:34:37.237030 35003 solver.cpp:239] Iteration 154020 (3.49283 iter/s, 2.86301s/10 iters), loss = 5.88817
I0523 05:34:37.237079 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.88817 (* 1 = 5.88817 loss)
I0523 05:34:37.247687 35003 sgd_solver.cpp:112] Iteration 154020, lr = 0.001
I0523 05:34:42.805171 35003 solver.cpp:239] Iteration 154030 (1.79602 iter/s, 5.56787s/10 iters), loss = 7.03302
I0523 05:34:42.805255 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03302 (* 1 = 7.03302 loss)
I0523 05:34:42.818996 35003 sgd_solver.cpp:112] Iteration 154030, lr = 0.001
I0523 05:34:46.255954 35003 solver.cpp:239] Iteration 154040 (2.89809 iter/s, 3.45055s/10 iters), loss = 6.80099
I0523 05:34:46.256008 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80099 (* 1 = 6.80099 loss)
I0523 05:34:46.264513 35003 sgd_solver.cpp:112] Iteration 154040, lr = 0.001
I0523 05:34:49.270934 35003 solver.cpp:239] Iteration 154050 (3.31697 iter/s, 3.0148s/10 iters), loss = 7.27419
I0523 05:34:49.270983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27419 (* 1 = 7.27419 loss)
I0523 05:34:49.280508 35003 sgd_solver.cpp:112] Iteration 154050, lr = 0.001
I0523 05:34:51.972977 35003 solver.cpp:239] Iteration 154060 (3.70113 iter/s, 2.70188s/10 iters), loss = 6.97799
I0523 05:34:51.973031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97799 (* 1 = 6.97799 loss)
I0523 05:34:52.688423 35003 sgd_solver.cpp:112] Iteration 154060, lr = 0.001
I0523 05:34:54.874771 35003 solver.cpp:239] Iteration 154070 (3.44636 iter/s, 2.90161s/10 iters), loss = 6.86264
I0523 05:34:54.874826 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86264 (* 1 = 6.86264 loss)
I0523 05:34:55.006443 35003 sgd_solver.cpp:112] Iteration 154070, lr = 0.001
I0523 05:34:58.991039 35003 solver.cpp:239] Iteration 154080 (2.42952 iter/s, 4.11604s/10 iters), loss = 7.47591
I0523 05:34:58.991086 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47591 (* 1 = 7.47591 loss)
I0523 05:34:59.699434 35003 sgd_solver.cpp:112] Iteration 154080, lr = 0.001
I0523 05:35:01.029412 35003 solver.cpp:239] Iteration 154090 (4.9062 iter/s, 2.03824s/10 iters), loss = 6.65818
I0523 05:35:01.029455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65818 (* 1 = 6.65818 loss)
I0523 05:35:01.045866 35003 sgd_solver.cpp:112] Iteration 154090, lr = 0.001
I0523 05:35:04.078644 35003 solver.cpp:239] Iteration 154100 (3.2797 iter/s, 3.04906s/10 iters), loss = 6.83377
I0523 05:35:04.078723 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83377 (* 1 = 6.83377 loss)
I0523 05:35:04.733536 35003 sgd_solver.cpp:112] Iteration 154100, lr = 0.001
I0523 05:35:09.138645 35003 solver.cpp:239] Iteration 154110 (1.97638 iter/s, 5.05975s/10 iters), loss = 6.69368
I0523 05:35:09.138689 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69368 (* 1 = 6.69368 loss)
I0523 05:35:09.146152 35003 sgd_solver.cpp:112] Iteration 154110, lr = 0.001
I0523 05:35:12.598712 35003 solver.cpp:239] Iteration 154120 (2.89029 iter/s, 3.45986s/10 iters), loss = 7.31626
I0523 05:35:12.598753 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31626 (* 1 = 7.31626 loss)
I0523 05:35:12.611274 35003 sgd_solver.cpp:112] Iteration 154120, lr = 0.001
I0523 05:35:16.106117 35003 solver.cpp:239] Iteration 154130 (2.85126 iter/s, 3.50722s/10 iters), loss = 7.02567
I0523 05:35:16.106379 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02567 (* 1 = 7.02567 loss)
I0523 05:35:16.124070 35003 sgd_solver.cpp:112] Iteration 154130, lr = 0.001
I0523 05:35:19.020135 35003 solver.cpp:239] Iteration 154140 (3.43731 iter/s, 2.90925s/10 iters), loss = 6.90971
I0523 05:35:19.020179 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90971 (* 1 = 6.90971 loss)
I0523 05:35:19.026310 35003 sgd_solver.cpp:112] Iteration 154140, lr = 0.001
I0523 05:35:23.345425 35003 solver.cpp:239] Iteration 154150 (2.31211 iter/s, 4.32506s/10 iters), loss = 6.78977
I0523 05:35:23.345490 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78977 (* 1 = 6.78977 loss)
I0523 05:35:24.060015 35003 sgd_solver.cpp:112] Iteration 154150, lr = 0.001
I0523 05:35:27.793799 35003 solver.cpp:239] Iteration 154160 (2.24814 iter/s, 4.44813s/10 iters), loss = 8.01789
I0523 05:35:27.793848 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01789 (* 1 = 8.01789 loss)
I0523 05:35:28.496289 35003 sgd_solver.cpp:112] Iteration 154160, lr = 0.001
I0523 05:35:33.502899 35003 solver.cpp:239] Iteration 154170 (1.75168 iter/s, 5.70882s/10 iters), loss = 7.06007
I0523 05:35:33.502951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06007 (* 1 = 7.06007 loss)
I0523 05:35:33.510895 35003 sgd_solver.cpp:112] Iteration 154170, lr = 0.001
I0523 05:35:37.021644 35003 solver.cpp:239] Iteration 154180 (2.84235 iter/s, 3.51822s/10 iters), loss = 7.69953
I0523 05:35:37.021682 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69953 (* 1 = 7.69953 loss)
I0523 05:35:37.032335 35003 sgd_solver.cpp:112] Iteration 154180, lr = 0.001
I0523 05:35:41.305079 35003 solver.cpp:239] Iteration 154190 (2.33469 iter/s, 4.28322s/10 iters), loss = 6.02555
I0523 05:35:41.305119 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02555 (* 1 = 6.02555 loss)
I0523 05:35:41.311221 35003 sgd_solver.cpp:112] Iteration 154190, lr = 0.001
I0523 05:35:44.899040 35003 solver.cpp:239] Iteration 154200 (2.7826 iter/s, 3.59377s/10 iters), loss = 5.54564
I0523 05:35:44.899088 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.54564 (* 1 = 5.54564 loss)
I0523 05:35:44.905722 35003 sgd_solver.cpp:112] Iteration 154200, lr = 0.001
I0523 05:35:47.333003 35003 solver.cpp:239] Iteration 154210 (4.10879 iter/s, 2.43381s/10 iters), loss = 7.06653
I0523 05:35:47.333220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06653 (* 1 = 7.06653 loss)
I0523 05:35:47.938665 35003 sgd_solver.cpp:112] Iteration 154210, lr = 0.001
I0523 05:35:52.230860 35003 solver.cpp:239] Iteration 154220 (2.04188 iter/s, 4.89744s/10 iters), loss = 6.66138
I0523 05:35:52.230919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66138 (* 1 = 6.66138 loss)
I0523 05:35:52.939108 35003 sgd_solver.cpp:112] Iteration 154220, lr = 0.001
I0523 05:35:57.350932 35003 solver.cpp:239] Iteration 154230 (1.9532 iter/s, 5.11981s/10 iters), loss = 6.80643
I0523 05:35:57.350977 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80643 (* 1 = 6.80643 loss)
I0523 05:35:57.364363 35003 sgd_solver.cpp:112] Iteration 154230, lr = 0.001
I0523 05:35:59.479656 35003 solver.cpp:239] Iteration 154240 (4.69796 iter/s, 2.12858s/10 iters), loss = 6.8412
I0523 05:35:59.479696 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8412 (* 1 = 6.8412 loss)
I0523 05:35:59.485096 35003 sgd_solver.cpp:112] Iteration 154240, lr = 0.001
I0523 05:36:03.185963 35003 solver.cpp:239] Iteration 154250 (2.69824 iter/s, 3.70611s/10 iters), loss = 6.7138
I0523 05:36:03.186007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7138 (* 1 = 6.7138 loss)
I0523 05:36:03.192509 35003 sgd_solver.cpp:112] Iteration 154250, lr = 0.001
I0523 05:36:07.840127 35003 solver.cpp:239] Iteration 154260 (2.14872 iter/s, 4.65393s/10 iters), loss = 7.9642
I0523 05:36:07.840186 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9642 (* 1 = 7.9642 loss)
I0523 05:36:08.467308 35003 sgd_solver.cpp:112] Iteration 154260, lr = 0.001
I0523 05:36:11.988339 35003 solver.cpp:239] Iteration 154270 (2.41082 iter/s, 4.14797s/10 iters), loss = 6.97629
I0523 05:36:11.988378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97629 (* 1 = 6.97629 loss)
I0523 05:36:11.993731 35003 sgd_solver.cpp:112] Iteration 154270, lr = 0.001
I0523 05:36:14.947887 35003 solver.cpp:239] Iteration 154280 (3.37909 iter/s, 2.95938s/10 iters), loss = 6.86197
I0523 05:36:14.947937 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86197 (* 1 = 6.86197 loss)
I0523 05:36:14.961280 35003 sgd_solver.cpp:112] Iteration 154280, lr = 0.001
I0523 05:36:17.791991 35003 solver.cpp:239] Iteration 154290 (3.51626 iter/s, 2.84393s/10 iters), loss = 6.66804
I0523 05:36:17.792181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66804 (* 1 = 6.66804 loss)
I0523 05:36:17.823314 35003 sgd_solver.cpp:112] Iteration 154290, lr = 0.001
I0523 05:36:21.363658 35003 solver.cpp:239] Iteration 154300 (2.80006 iter/s, 3.57135s/10 iters), loss = 7.01799
I0523 05:36:21.363699 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01799 (* 1 = 7.01799 loss)
I0523 05:36:21.376699 35003 sgd_solver.cpp:112] Iteration 154300, lr = 0.001
I0523 05:36:25.551717 35003 solver.cpp:239] Iteration 154310 (2.38787 iter/s, 4.18784s/10 iters), loss = 7.3946
I0523 05:36:25.551776 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3946 (* 1 = 7.3946 loss)
I0523 05:36:25.557319 35003 sgd_solver.cpp:112] Iteration 154310, lr = 0.001
I0523 05:36:29.094635 35003 solver.cpp:239] Iteration 154320 (2.82269 iter/s, 3.54271s/10 iters), loss = 7.31191
I0523 05:36:29.094672 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31191 (* 1 = 7.31191 loss)
I0523 05:36:29.102571 35003 sgd_solver.cpp:112] Iteration 154320, lr = 0.001
I0523 05:36:31.855644 35003 solver.cpp:239] Iteration 154330 (3.62208 iter/s, 2.76085s/10 iters), loss = 7.59314
I0523 05:36:31.855695 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59314 (* 1 = 7.59314 loss)
I0523 05:36:31.862813 35003 sgd_solver.cpp:112] Iteration 154330, lr = 0.001
I0523 05:36:36.704527 35003 solver.cpp:239] Iteration 154340 (2.06244 iter/s, 4.84863s/10 iters), loss = 6.50345
I0523 05:36:36.704579 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50345 (* 1 = 6.50345 loss)
I0523 05:36:37.413384 35003 sgd_solver.cpp:112] Iteration 154340, lr = 0.001
I0523 05:36:41.014152 35003 solver.cpp:239] Iteration 154350 (2.32052 iter/s, 4.30938s/10 iters), loss = 6.61606
I0523 05:36:41.014230 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61606 (* 1 = 6.61606 loss)
I0523 05:36:41.017338 35003 sgd_solver.cpp:112] Iteration 154350, lr = 0.001
I0523 05:36:43.055503 35003 solver.cpp:239] Iteration 154360 (4.89913 iter/s, 2.04118s/10 iters), loss = 7.60178
I0523 05:36:43.055543 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60178 (* 1 = 7.60178 loss)
I0523 05:36:43.063505 35003 sgd_solver.cpp:112] Iteration 154360, lr = 0.001
I0523 05:36:45.189524 35003 solver.cpp:239] Iteration 154370 (4.68628 iter/s, 2.13389s/10 iters), loss = 6.64882
I0523 05:36:45.189563 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64882 (* 1 = 6.64882 loss)
I0523 05:36:45.196691 35003 sgd_solver.cpp:112] Iteration 154370, lr = 0.001
I0523 05:36:47.224073 35003 solver.cpp:239] Iteration 154380 (4.91542 iter/s, 2.03441s/10 iters), loss = 7.39819
I0523 05:36:47.224136 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39819 (* 1 = 7.39819 loss)
I0523 05:36:47.250473 35003 sgd_solver.cpp:112] Iteration 154380, lr = 0.001
I0523 05:36:52.323071 35003 solver.cpp:239] Iteration 154390 (1.96127 iter/s, 5.09873s/10 iters), loss = 6.41883
I0523 05:36:52.323354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41883 (* 1 = 6.41883 loss)
I0523 05:36:52.328627 35003 sgd_solver.cpp:112] Iteration 154390, lr = 0.001
I0523 05:36:55.730585 35003 solver.cpp:239] Iteration 154400 (2.93503 iter/s, 3.40712s/10 iters), loss = 7.15177
I0523 05:36:55.730625 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15177 (* 1 = 7.15177 loss)
I0523 05:36:55.735003 35003 sgd_solver.cpp:112] Iteration 154400, lr = 0.001
I0523 05:36:59.632439 35003 solver.cpp:239] Iteration 154410 (2.56302 iter/s, 3.90165s/10 iters), loss = 7.03625
I0523 05:36:59.632477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03625 (* 1 = 7.03625 loss)
I0523 05:36:59.646096 35003 sgd_solver.cpp:112] Iteration 154410, lr = 0.001
I0523 05:37:02.844889 35003 solver.cpp:239] Iteration 154420 (3.11306 iter/s, 3.21227s/10 iters), loss = 7.44044
I0523 05:37:02.844933 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44044 (* 1 = 7.44044 loss)
I0523 05:37:02.854282 35003 sgd_solver.cpp:112] Iteration 154420, lr = 0.001
I0523 05:37:05.573832 35003 solver.cpp:239] Iteration 154430 (3.66464 iter/s, 2.72878s/10 iters), loss = 7.06263
I0523 05:37:05.573876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06263 (* 1 = 7.06263 loss)
I0523 05:37:06.308627 35003 sgd_solver.cpp:112] Iteration 154430, lr = 0.001
I0523 05:37:08.282567 35003 solver.cpp:239] Iteration 154440 (3.69199 iter/s, 2.70857s/10 iters), loss = 5.91195
I0523 05:37:08.282615 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91195 (* 1 = 5.91195 loss)
I0523 05:37:08.287816 35003 sgd_solver.cpp:112] Iteration 154440, lr = 0.001
I0523 05:37:10.763983 35003 solver.cpp:239] Iteration 154450 (4.03021 iter/s, 2.48126s/10 iters), loss = 6.27951
I0523 05:37:10.764034 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27951 (* 1 = 6.27951 loss)
I0523 05:37:10.782310 35003 sgd_solver.cpp:112] Iteration 154450, lr = 0.001
I0523 05:37:15.208165 35003 solver.cpp:239] Iteration 154460 (2.25025 iter/s, 4.44394s/10 iters), loss = 6.51238
I0523 05:37:15.208214 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51238 (* 1 = 6.51238 loss)
I0523 05:37:15.215595 35003 sgd_solver.cpp:112] Iteration 154460, lr = 0.001
I0523 05:37:19.642253 35003 solver.cpp:239] Iteration 154470 (2.25537 iter/s, 4.43385s/10 iters), loss = 7.60156
I0523 05:37:19.642295 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60156 (* 1 = 7.60156 loss)
I0523 05:37:20.351148 35003 sgd_solver.cpp:112] Iteration 154470, lr = 0.001
I0523 05:37:22.414561 35003 solver.cpp:239] Iteration 154480 (3.60731 iter/s, 2.77215s/10 iters), loss = 6.70995
I0523 05:37:22.414854 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70995 (* 1 = 6.70995 loss)
I0523 05:37:22.423262 35003 sgd_solver.cpp:112] Iteration 154480, lr = 0.001
I0523 05:37:25.810356 35003 solver.cpp:239] Iteration 154490 (2.94519 iter/s, 3.39537s/10 iters), loss = 6.16008
I0523 05:37:25.810395 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16008 (* 1 = 6.16008 loss)
I0523 05:37:25.823560 35003 sgd_solver.cpp:112] Iteration 154490, lr = 0.001
I0523 05:37:27.921269 35003 solver.cpp:239] Iteration 154500 (4.73758 iter/s, 2.11078s/10 iters), loss = 6.79854
I0523 05:37:27.921306 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79854 (* 1 = 6.79854 loss)
I0523 05:37:27.931968 35003 sgd_solver.cpp:112] Iteration 154500, lr = 0.001
I0523 05:37:31.509832 35003 solver.cpp:239] Iteration 154510 (2.78678 iter/s, 3.58838s/10 iters), loss = 6.10739
I0523 05:37:31.509882 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10739 (* 1 = 6.10739 loss)
I0523 05:37:31.757534 35003 sgd_solver.cpp:112] Iteration 154510, lr = 0.001
I0523 05:37:33.852020 35003 solver.cpp:239] Iteration 154520 (4.26977 iter/s, 2.34205s/10 iters), loss = 7.2093
I0523 05:37:33.852068 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2093 (* 1 = 7.2093 loss)
I0523 05:37:33.863494 35003 sgd_solver.cpp:112] Iteration 154520, lr = 0.001
I0523 05:37:36.225145 35003 solver.cpp:239] Iteration 154530 (4.21413 iter/s, 2.37297s/10 iters), loss = 7.71137
I0523 05:37:36.225212 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71137 (* 1 = 7.71137 loss)
I0523 05:37:36.238880 35003 sgd_solver.cpp:112] Iteration 154530, lr = 0.001
I0523 05:37:39.072453 35003 solver.cpp:239] Iteration 154540 (3.51232 iter/s, 2.84712s/10 iters), loss = 5.99206
I0523 05:37:39.072502 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99206 (* 1 = 5.99206 loss)
I0523 05:37:39.813087 35003 sgd_solver.cpp:112] Iteration 154540, lr = 0.001
I0523 05:37:44.027504 35003 solver.cpp:239] Iteration 154550 (2.01824 iter/s, 4.9548s/10 iters), loss = 6.96142
I0523 05:37:44.027546 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96142 (* 1 = 6.96142 loss)
I0523 05:37:44.139504 35003 sgd_solver.cpp:112] Iteration 154550, lr = 0.001
I0523 05:37:47.755834 35003 solver.cpp:239] Iteration 154560 (2.68231 iter/s, 3.72813s/10 iters), loss = 6.64309
I0523 05:37:47.755879 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64309 (* 1 = 6.64309 loss)
I0523 05:37:47.769179 35003 sgd_solver.cpp:112] Iteration 154560, lr = 0.001
I0523 05:37:50.582415 35003 solver.cpp:239] Iteration 154570 (3.53805 iter/s, 2.82641s/10 iters), loss = 7.30961
I0523 05:37:50.582468 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30961 (* 1 = 7.30961 loss)
I0523 05:37:50.600949 35003 sgd_solver.cpp:112] Iteration 154570, lr = 0.001
I0523 05:37:53.466104 35003 solver.cpp:239] Iteration 154580 (3.46799 iter/s, 2.88351s/10 iters), loss = 6.81031
I0523 05:37:53.466374 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81031 (* 1 = 6.81031 loss)
I0523 05:37:53.469262 35003 sgd_solver.cpp:112] Iteration 154580, lr = 0.001
I0523 05:37:57.724673 35003 solver.cpp:239] Iteration 154590 (2.34845 iter/s, 4.25813s/10 iters), loss = 5.89664
I0523 05:37:57.724714 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89664 (* 1 = 5.89664 loss)
I0523 05:37:57.737097 35003 sgd_solver.cpp:112] Iteration 154590, lr = 0.001
I0523 05:37:59.818363 35003 solver.cpp:239] Iteration 154600 (4.77657 iter/s, 2.09355s/10 iters), loss = 6.67304
I0523 05:37:59.818408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67304 (* 1 = 6.67304 loss)
I0523 05:37:59.823582 35003 sgd_solver.cpp:112] Iteration 154600, lr = 0.001
I0523 05:38:02.293023 35003 solver.cpp:239] Iteration 154610 (4.04121 iter/s, 2.47451s/10 iters), loss = 6.84945
I0523 05:38:02.293069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84945 (* 1 = 6.84945 loss)
I0523 05:38:03.001037 35003 sgd_solver.cpp:112] Iteration 154610, lr = 0.001
I0523 05:38:05.810736 35003 solver.cpp:239] Iteration 154620 (2.84294 iter/s, 3.51749s/10 iters), loss = 6.03909
I0523 05:38:05.810789 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03909 (* 1 = 6.03909 loss)
I0523 05:38:06.551599 35003 sgd_solver.cpp:112] Iteration 154620, lr = 0.001
I0523 05:38:09.123232 35003 solver.cpp:239] Iteration 154630 (3.01905 iter/s, 3.3123s/10 iters), loss = 6.31707
I0523 05:38:09.123283 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31707 (* 1 = 6.31707 loss)
I0523 05:38:09.845288 35003 sgd_solver.cpp:112] Iteration 154630, lr = 0.001
I0523 05:38:13.198760 35003 solver.cpp:239] Iteration 154640 (2.4538 iter/s, 4.0753s/10 iters), loss = 7.63666
I0523 05:38:13.198804 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63666 (* 1 = 7.63666 loss)
I0523 05:38:13.208400 35003 sgd_solver.cpp:112] Iteration 154640, lr = 0.001
I0523 05:38:15.928823 35003 solver.cpp:239] Iteration 154650 (3.66313 iter/s, 2.72991s/10 iters), loss = 7.42539
I0523 05:38:15.928863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42539 (* 1 = 7.42539 loss)
I0523 05:38:15.942028 35003 sgd_solver.cpp:112] Iteration 154650, lr = 0.001
I0523 05:38:18.415976 35003 solver.cpp:239] Iteration 154660 (4.02091 iter/s, 2.487s/10 iters), loss = 7.16378
I0523 05:38:18.416028 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16378 (* 1 = 7.16378 loss)
I0523 05:38:19.150467 35003 sgd_solver.cpp:112] Iteration 154660, lr = 0.001
I0523 05:38:23.154258 35003 solver.cpp:239] Iteration 154670 (2.11058 iter/s, 4.73803s/10 iters), loss = 6.04299
I0523 05:38:23.154311 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04299 (* 1 = 6.04299 loss)
I0523 05:38:23.862504 35003 sgd_solver.cpp:112] Iteration 154670, lr = 0.001
I0523 05:38:28.232915 35003 solver.cpp:239] Iteration 154680 (1.96913 iter/s, 5.0784s/10 iters), loss = 6.60341
I0523 05:38:28.232967 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60341 (* 1 = 6.60341 loss)
I0523 05:38:28.246630 35003 sgd_solver.cpp:112] Iteration 154680, lr = 0.001
I0523 05:38:32.537614 35003 solver.cpp:239] Iteration 154690 (2.32317 iter/s, 4.30447s/10 iters), loss = 7.82446
I0523 05:38:32.537669 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82446 (* 1 = 7.82446 loss)
I0523 05:38:32.550818 35003 sgd_solver.cpp:112] Iteration 154690, lr = 0.001
I0523 05:38:35.708485 35003 solver.cpp:239] Iteration 154700 (3.15389 iter/s, 3.17068s/10 iters), loss = 6.37161
I0523 05:38:35.708534 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37161 (* 1 = 6.37161 loss)
I0523 05:38:36.439499 35003 sgd_solver.cpp:112] Iteration 154700, lr = 0.001
I0523 05:38:41.152243 35003 solver.cpp:239] Iteration 154710 (1.83706 iter/s, 5.44348s/10 iters), loss = 8.16014
I0523 05:38:41.152297 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16014 (* 1 = 8.16014 loss)
I0523 05:38:41.165441 35003 sgd_solver.cpp:112] Iteration 154710, lr = 0.001
I0523 05:38:43.278437 35003 solver.cpp:239] Iteration 154720 (4.70357 iter/s, 2.12604s/10 iters), loss = 6.33488
I0523 05:38:43.278501 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33488 (* 1 = 6.33488 loss)
I0523 05:38:43.980299 35003 sgd_solver.cpp:112] Iteration 154720, lr = 0.001
I0523 05:38:45.287031 35003 solver.cpp:239] Iteration 154730 (4.97897 iter/s, 2.00845s/10 iters), loss = 7.09124
I0523 05:38:45.287073 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09124 (* 1 = 7.09124 loss)
I0523 05:38:45.292623 35003 sgd_solver.cpp:112] Iteration 154730, lr = 0.001
I0523 05:38:48.129262 35003 solver.cpp:239] Iteration 154740 (3.51857 iter/s, 2.84207s/10 iters), loss = 6.25449
I0523 05:38:48.129302 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25449 (* 1 = 6.25449 loss)
I0523 05:38:48.818603 35003 sgd_solver.cpp:112] Iteration 154740, lr = 0.001
I0523 05:38:50.937593 35003 solver.cpp:239] Iteration 154750 (3.56103 iter/s, 2.80817s/10 iters), loss = 6.42946
I0523 05:38:50.937633 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42946 (* 1 = 6.42946 loss)
I0523 05:38:50.951051 35003 sgd_solver.cpp:112] Iteration 154750, lr = 0.001
I0523 05:38:55.054473 35003 solver.cpp:239] Iteration 154760 (2.42915 iter/s, 4.11666s/10 iters), loss = 7.3783
I0523 05:38:55.054764 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3783 (* 1 = 7.3783 loss)
I0523 05:38:55.062276 35003 sgd_solver.cpp:112] Iteration 154760, lr = 0.001
I0523 05:38:58.587044 35003 solver.cpp:239] Iteration 154770 (2.83113 iter/s, 3.53215s/10 iters), loss = 6.68111
I0523 05:38:58.587087 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68111 (* 1 = 6.68111 loss)
I0523 05:38:58.618168 35003 sgd_solver.cpp:112] Iteration 154770, lr = 0.001
I0523 05:39:03.613051 35003 solver.cpp:239] Iteration 154780 (1.98975 iter/s, 5.02576s/10 iters), loss = 7.87616
I0523 05:39:03.613092 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.87616 (* 1 = 7.87616 loss)
I0523 05:39:03.625885 35003 sgd_solver.cpp:112] Iteration 154780, lr = 0.001
I0523 05:39:07.660485 35003 solver.cpp:239] Iteration 154790 (2.47083 iter/s, 4.04722s/10 iters), loss = 6.65926
I0523 05:39:07.660526 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65926 (* 1 = 6.65926 loss)
I0523 05:39:07.674080 35003 sgd_solver.cpp:112] Iteration 154790, lr = 0.001
I0523 05:39:11.343730 35003 solver.cpp:239] Iteration 154800 (2.71514 iter/s, 3.68305s/10 iters), loss = 6.72693
I0523 05:39:11.343775 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72693 (* 1 = 6.72693 loss)
I0523 05:39:11.349359 35003 sgd_solver.cpp:112] Iteration 154800, lr = 0.001
I0523 05:39:15.038952 35003 solver.cpp:239] Iteration 154810 (2.70634 iter/s, 3.69502s/10 iters), loss = 6.45096
I0523 05:39:15.039002 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45096 (* 1 = 6.45096 loss)
I0523 05:39:15.055109 35003 sgd_solver.cpp:112] Iteration 154810, lr = 0.001
I0523 05:39:18.447762 35003 solver.cpp:239] Iteration 154820 (2.93374 iter/s, 3.40861s/10 iters), loss = 6.87991
I0523 05:39:18.447804 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87991 (* 1 = 6.87991 loss)
I0523 05:39:18.460237 35003 sgd_solver.cpp:112] Iteration 154820, lr = 0.001
I0523 05:39:22.056596 35003 solver.cpp:239] Iteration 154830 (2.77113 iter/s, 3.60864s/10 iters), loss = 6.58098
I0523 05:39:22.056634 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58098 (* 1 = 6.58098 loss)
I0523 05:39:22.061079 35003 sgd_solver.cpp:112] Iteration 154830, lr = 0.001
I0523 05:39:25.541853 35003 solver.cpp:239] Iteration 154840 (2.86939 iter/s, 3.48506s/10 iters), loss = 6.80145
I0523 05:39:25.542093 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80145 (* 1 = 6.80145 loss)
I0523 05:39:26.231122 35003 sgd_solver.cpp:112] Iteration 154840, lr = 0.001
I0523 05:39:29.689033 35003 solver.cpp:239] Iteration 154850 (2.4115 iter/s, 4.1468s/10 iters), loss = 6.61544
I0523 05:39:29.689074 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61544 (* 1 = 6.61544 loss)
I0523 05:39:29.693934 35003 sgd_solver.cpp:112] Iteration 154850, lr = 0.001
I0523 05:39:31.888100 35003 solver.cpp:239] Iteration 154860 (4.54767 iter/s, 2.19893s/10 iters), loss = 6.56382
I0523 05:39:31.888144 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56382 (* 1 = 6.56382 loss)
I0523 05:39:31.908674 35003 sgd_solver.cpp:112] Iteration 154860, lr = 0.001
I0523 05:39:34.512451 35003 solver.cpp:239] Iteration 154870 (3.8107 iter/s, 2.62419s/10 iters), loss = 5.74162
I0523 05:39:34.512493 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74162 (* 1 = 5.74162 loss)
I0523 05:39:35.240944 35003 sgd_solver.cpp:112] Iteration 154870, lr = 0.001
I0523 05:39:40.186177 35003 solver.cpp:239] Iteration 154880 (1.76259 iter/s, 5.67346s/10 iters), loss = 5.95582
I0523 05:39:40.186223 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95582 (* 1 = 5.95582 loss)
I0523 05:39:40.911708 35003 sgd_solver.cpp:112] Iteration 154880, lr = 0.001
I0523 05:39:43.270434 35003 solver.cpp:239] Iteration 154890 (3.24246 iter/s, 3.08408s/10 iters), loss = 6.75211
I0523 05:39:43.270476 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75211 (* 1 = 6.75211 loss)
I0523 05:39:43.275387 35003 sgd_solver.cpp:112] Iteration 154890, lr = 0.001
I0523 05:39:46.154199 35003 solver.cpp:239] Iteration 154900 (3.46789 iter/s, 2.8836s/10 iters), loss = 7.1495
I0523 05:39:46.154244 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1495 (* 1 = 7.1495 loss)
I0523 05:39:46.884851 35003 sgd_solver.cpp:112] Iteration 154900, lr = 0.001
I0523 05:39:49.624704 35003 solver.cpp:239] Iteration 154910 (2.88158 iter/s, 3.47032s/10 iters), loss = 7.25644
I0523 05:39:49.624748 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25644 (* 1 = 7.25644 loss)
I0523 05:39:49.638029 35003 sgd_solver.cpp:112] Iteration 154910, lr = 0.001
I0523 05:39:52.428789 35003 solver.cpp:239] Iteration 154920 (3.56643 iter/s, 2.80392s/10 iters), loss = 6.30895
I0523 05:39:52.428834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30895 (* 1 = 6.30895 loss)
I0523 05:39:52.493597 35003 sgd_solver.cpp:112] Iteration 154920, lr = 0.001
I0523 05:39:56.884491 35003 solver.cpp:239] Iteration 154930 (2.24443 iter/s, 4.45547s/10 iters), loss = 5.40548
I0523 05:39:56.884743 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.40548 (* 1 = 5.40548 loss)
I0523 05:39:56.892357 35003 sgd_solver.cpp:112] Iteration 154930, lr = 0.001
I0523 05:39:58.923475 35003 solver.cpp:239] Iteration 154940 (4.90518 iter/s, 2.03866s/10 iters), loss = 5.71942
I0523 05:39:58.923518 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.71942 (* 1 = 5.71942 loss)
I0523 05:39:58.929780 35003 sgd_solver.cpp:112] Iteration 154940, lr = 0.001
I0523 05:40:01.781733 35003 solver.cpp:239] Iteration 154950 (3.49884 iter/s, 2.85809s/10 iters), loss = 7.11932
I0523 05:40:01.781782 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11932 (* 1 = 7.11932 loss)
I0523 05:40:01.789175 35003 sgd_solver.cpp:112] Iteration 154950, lr = 0.001
I0523 05:40:03.824201 35003 solver.cpp:239] Iteration 154960 (4.89637 iter/s, 2.04233s/10 iters), loss = 6.58216
I0523 05:40:03.824249 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58216 (* 1 = 6.58216 loss)
I0523 05:40:03.832901 35003 sgd_solver.cpp:112] Iteration 154960, lr = 0.001
I0523 05:40:06.723289 35003 solver.cpp:239] Iteration 154970 (3.44957 iter/s, 2.89891s/10 iters), loss = 6.66282
I0523 05:40:06.723333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66282 (* 1 = 6.66282 loss)
I0523 05:40:06.728886 35003 sgd_solver.cpp:112] Iteration 154970, lr = 0.001
I0523 05:40:09.645047 35003 solver.cpp:239] Iteration 154980 (3.4228 iter/s, 2.92159s/10 iters), loss = 7.04898
I0523 05:40:09.645097 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04898 (* 1 = 7.04898 loss)
I0523 05:40:09.781085 35003 sgd_solver.cpp:112] Iteration 154980, lr = 0.001
I0523 05:40:11.848305 35003 solver.cpp:239] Iteration 154990 (4.53903 iter/s, 2.20311s/10 iters), loss = 6.14802
I0523 05:40:11.848353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14802 (* 1 = 6.14802 loss)
I0523 05:40:11.861874 35003 sgd_solver.cpp:112] Iteration 154990, lr = 0.001
I0523 05:40:15.394222 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_155000.caffemodel
I0523 05:40:17.126243 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_155000.solverstate
I0523 05:40:17.335407 35003 solver.cpp:239] Iteration 155000 (1.82255 iter/s, 5.48683s/10 iters), loss = 6.67137
I0523 05:40:17.335460 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67137 (* 1 = 6.67137 loss)
I0523 05:40:17.823179 35003 sgd_solver.cpp:112] Iteration 155000, lr = 0.001
I0523 05:40:21.492239 35003 solver.cpp:239] Iteration 155010 (2.40581 iter/s, 4.15661s/10 iters), loss = 7.2038
I0523 05:40:21.492302 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2038 (* 1 = 7.2038 loss)
I0523 05:40:22.232810 35003 sgd_solver.cpp:112] Iteration 155010, lr = 0.001
I0523 05:40:25.971938 35003 solver.cpp:239] Iteration 155020 (2.23241 iter/s, 4.47946s/10 iters), loss = 8.00576
I0523 05:40:25.971983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00576 (* 1 = 8.00576 loss)
I0523 05:40:25.981281 35003 sgd_solver.cpp:112] Iteration 155020, lr = 0.001
I0523 05:40:29.575768 35003 solver.cpp:239] Iteration 155030 (2.77498 iter/s, 3.60363s/10 iters), loss = 6.41498
I0523 05:40:29.575989 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41498 (* 1 = 6.41498 loss)
I0523 05:40:29.620332 35003 sgd_solver.cpp:112] Iteration 155030, lr = 0.001
I0523 05:40:34.402163 35003 solver.cpp:239] Iteration 155040 (2.07211 iter/s, 4.82599s/10 iters), loss = 6.96013
I0523 05:40:34.402204 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96013 (* 1 = 6.96013 loss)
I0523 05:40:34.410270 35003 sgd_solver.cpp:112] Iteration 155040, lr = 0.001
I0523 05:40:36.576329 35003 solver.cpp:239] Iteration 155050 (4.59975 iter/s, 2.17403s/10 iters), loss = 6.87678
I0523 05:40:36.576366 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87678 (* 1 = 6.87678 loss)
I0523 05:40:36.583716 35003 sgd_solver.cpp:112] Iteration 155050, lr = 0.001
I0523 05:40:38.542744 35003 solver.cpp:239] Iteration 155060 (5.08572 iter/s, 1.96629s/10 iters), loss = 5.97452
I0523 05:40:38.542798 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97452 (* 1 = 5.97452 loss)
I0523 05:40:39.284076 35003 sgd_solver.cpp:112] Iteration 155060, lr = 0.001
I0523 05:40:42.875429 35003 solver.cpp:239] Iteration 155070 (2.30816 iter/s, 4.33245s/10 iters), loss = 6.00138
I0523 05:40:42.875474 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00138 (* 1 = 6.00138 loss)
I0523 05:40:42.884920 35003 sgd_solver.cpp:112] Iteration 155070, lr = 0.001
I0523 05:40:46.455426 35003 solver.cpp:239] Iteration 155080 (2.79345 iter/s, 3.5798s/10 iters), loss = 5.5255
I0523 05:40:46.455471 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.5255 (* 1 = 5.5255 loss)
I0523 05:40:46.461706 35003 sgd_solver.cpp:112] Iteration 155080, lr = 0.001
I0523 05:40:49.275482 35003 solver.cpp:239] Iteration 155090 (3.54624 iter/s, 2.81989s/10 iters), loss = 6.6756
I0523 05:40:49.275543 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6756 (* 1 = 6.6756 loss)
I0523 05:40:49.925783 35003 sgd_solver.cpp:112] Iteration 155090, lr = 0.001
I0523 05:40:51.945560 35003 solver.cpp:239] Iteration 155100 (3.74547 iter/s, 2.66989s/10 iters), loss = 7.16061
I0523 05:40:51.945617 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16061 (* 1 = 7.16061 loss)
I0523 05:40:51.965602 35003 sgd_solver.cpp:112] Iteration 155100, lr = 0.001
I0523 05:40:55.672354 35003 solver.cpp:239] Iteration 155110 (2.68342 iter/s, 3.72658s/10 iters), loss = 6.57071
I0523 05:40:55.672399 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57071 (* 1 = 6.57071 loss)
I0523 05:40:56.381392 35003 sgd_solver.cpp:112] Iteration 155110, lr = 0.001
I0523 05:41:00.264142 35003 solver.cpp:239] Iteration 155120 (2.17791 iter/s, 4.59155s/10 iters), loss = 7.64508
I0523 05:41:00.264389 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64508 (* 1 = 7.64508 loss)
I0523 05:41:00.720379 35003 sgd_solver.cpp:112] Iteration 155120, lr = 0.001
I0523 05:41:03.623077 35003 solver.cpp:239] Iteration 155130 (2.97747 iter/s, 3.35856s/10 iters), loss = 7.7023
I0523 05:41:03.623127 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7023 (* 1 = 7.7023 loss)
I0523 05:41:04.223896 35003 sgd_solver.cpp:112] Iteration 155130, lr = 0.001
I0523 05:41:07.141057 35003 solver.cpp:239] Iteration 155140 (2.8427 iter/s, 3.51779s/10 iters), loss = 8.13241
I0523 05:41:07.141109 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.13241 (* 1 = 8.13241 loss)
I0523 05:41:07.881958 35003 sgd_solver.cpp:112] Iteration 155140, lr = 0.001
I0523 05:41:10.796959 35003 solver.cpp:239] Iteration 155150 (2.73545 iter/s, 3.6557s/10 iters), loss = 6.35954
I0523 05:41:10.797004 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35954 (* 1 = 6.35954 loss)
I0523 05:41:11.415107 35003 sgd_solver.cpp:112] Iteration 155150, lr = 0.001
I0523 05:41:14.924208 35003 solver.cpp:239] Iteration 155160 (2.42305 iter/s, 4.12702s/10 iters), loss = 6.91703
I0523 05:41:14.924248 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91703 (* 1 = 6.91703 loss)
I0523 05:41:14.928647 35003 sgd_solver.cpp:112] Iteration 155160, lr = 0.001
I0523 05:41:17.793085 35003 solver.cpp:239] Iteration 155170 (3.48588 iter/s, 2.86871s/10 iters), loss = 6.87061
I0523 05:41:17.793133 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87061 (* 1 = 6.87061 loss)
I0523 05:41:17.807150 35003 sgd_solver.cpp:112] Iteration 155170, lr = 0.001
I0523 05:41:19.918182 35003 solver.cpp:239] Iteration 155180 (4.70598 iter/s, 2.12495s/10 iters), loss = 7.22635
I0523 05:41:19.918232 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22635 (* 1 = 7.22635 loss)
I0523 05:41:19.922475 35003 sgd_solver.cpp:112] Iteration 155180, lr = 0.001
I0523 05:41:24.500048 35003 solver.cpp:239] Iteration 155190 (2.18263 iter/s, 4.58162s/10 iters), loss = 6.40831
I0523 05:41:24.500089 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40831 (* 1 = 6.40831 loss)
I0523 05:41:25.188886 35003 sgd_solver.cpp:112] Iteration 155190, lr = 0.001
I0523 05:41:27.430167 35003 solver.cpp:239] Iteration 155200 (3.41305 iter/s, 2.92993s/10 iters), loss = 6.78938
I0523 05:41:27.430217 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78938 (* 1 = 6.78938 loss)
I0523 05:41:28.126972 35003 sgd_solver.cpp:112] Iteration 155200, lr = 0.001
I0523 05:41:32.561691 35003 solver.cpp:239] Iteration 155210 (1.94887 iter/s, 5.13119s/10 iters), loss = 7.19529
I0523 05:41:32.561908 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19529 (* 1 = 7.19529 loss)
I0523 05:41:32.570565 35003 sgd_solver.cpp:112] Iteration 155210, lr = 0.001
I0523 05:41:36.888046 35003 solver.cpp:239] Iteration 155220 (2.31162 iter/s, 4.32597s/10 iters), loss = 7.17565
I0523 05:41:36.888084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17565 (* 1 = 7.17565 loss)
I0523 05:41:36.914906 35003 sgd_solver.cpp:112] Iteration 155220, lr = 0.001
I0523 05:41:38.247304 35003 solver.cpp:239] Iteration 155230 (7.35755 iter/s, 1.35915s/10 iters), loss = 6.0068
I0523 05:41:38.247365 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0068 (* 1 = 6.0068 loss)
I0523 05:41:38.268415 35003 sgd_solver.cpp:112] Iteration 155230, lr = 0.001
I0523 05:41:42.018242 35003 solver.cpp:239] Iteration 155240 (2.65201 iter/s, 3.77073s/10 iters), loss = 7.27948
I0523 05:41:42.018281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27948 (* 1 = 7.27948 loss)
I0523 05:41:42.464920 35003 sgd_solver.cpp:112] Iteration 155240, lr = 0.001
I0523 05:41:46.048076 35003 solver.cpp:239] Iteration 155250 (2.48163 iter/s, 4.02961s/10 iters), loss = 6.75566
I0523 05:41:46.048140 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75566 (* 1 = 6.75566 loss)
I0523 05:41:46.050442 35003 sgd_solver.cpp:112] Iteration 155250, lr = 0.001
I0523 05:41:49.191114 35003 solver.cpp:239] Iteration 155260 (3.18183 iter/s, 3.14284s/10 iters), loss = 6.6704
I0523 05:41:49.191157 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6704 (* 1 = 6.6704 loss)
I0523 05:41:49.912333 35003 sgd_solver.cpp:112] Iteration 155260, lr = 0.001
I0523 05:41:53.541880 35003 solver.cpp:239] Iteration 155270 (2.29856 iter/s, 4.35054s/10 iters), loss = 7.41645
I0523 05:41:53.541934 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41645 (* 1 = 7.41645 loss)
I0523 05:41:53.554538 35003 sgd_solver.cpp:112] Iteration 155270, lr = 0.001
I0523 05:41:57.083551 35003 solver.cpp:239] Iteration 155280 (2.82368 iter/s, 3.54147s/10 iters), loss = 6.27296
I0523 05:41:57.083597 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27296 (* 1 = 6.27296 loss)
I0523 05:41:57.090262 35003 sgd_solver.cpp:112] Iteration 155280, lr = 0.001
I0523 05:42:01.516307 35003 solver.cpp:239] Iteration 155290 (2.25605 iter/s, 4.43253s/10 iters), loss = 7.20116
I0523 05:42:01.516347 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20116 (* 1 = 7.20116 loss)
I0523 05:42:01.528496 35003 sgd_solver.cpp:112] Iteration 155290, lr = 0.001
I0523 05:42:05.782196 35003 solver.cpp:239] Iteration 155300 (2.3443 iter/s, 4.26567s/10 iters), loss = 7.20294
I0523 05:42:05.782380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20294 (* 1 = 7.20294 loss)
I0523 05:42:05.802280 35003 sgd_solver.cpp:112] Iteration 155300, lr = 0.001
I0523 05:42:11.029222 35003 solver.cpp:239] Iteration 155310 (1.90599 iter/s, 5.24663s/10 iters), loss = 6.88947
I0523 05:42:11.029263 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88947 (* 1 = 6.88947 loss)
I0523 05:42:11.750274 35003 sgd_solver.cpp:112] Iteration 155310, lr = 0.001
I0523 05:42:14.645030 35003 solver.cpp:239] Iteration 155320 (2.76578 iter/s, 3.61562s/10 iters), loss = 7.40308
I0523 05:42:14.645067 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40308 (* 1 = 7.40308 loss)
I0523 05:42:15.366957 35003 sgd_solver.cpp:112] Iteration 155320, lr = 0.001
I0523 05:42:17.494731 35003 solver.cpp:239] Iteration 155330 (3.50935 iter/s, 2.84953s/10 iters), loss = 6.34519
I0523 05:42:17.494773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34519 (* 1 = 6.34519 loss)
I0523 05:42:18.235997 35003 sgd_solver.cpp:112] Iteration 155330, lr = 0.001
I0523 05:42:22.537307 35003 solver.cpp:239] Iteration 155340 (1.98321 iter/s, 5.04233s/10 iters), loss = 6.50223
I0523 05:42:22.537346 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50223 (* 1 = 6.50223 loss)
I0523 05:42:22.608873 35003 sgd_solver.cpp:112] Iteration 155340, lr = 0.001
I0523 05:42:25.946807 35003 solver.cpp:239] Iteration 155350 (2.93314 iter/s, 3.40931s/10 iters), loss = 7.38723
I0523 05:42:25.946848 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38723 (* 1 = 7.38723 loss)
I0523 05:42:25.951629 35003 sgd_solver.cpp:112] Iteration 155350, lr = 0.001
I0523 05:42:29.541155 35003 solver.cpp:239] Iteration 155360 (2.7823 iter/s, 3.59415s/10 iters), loss = 8.25901
I0523 05:42:29.541224 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.25901 (* 1 = 8.25901 loss)
I0523 05:42:29.591329 35003 sgd_solver.cpp:112] Iteration 155360, lr = 0.001
I0523 05:42:32.646942 35003 solver.cpp:239] Iteration 155370 (3.22 iter/s, 3.10559s/10 iters), loss = 7.31462
I0523 05:42:32.646988 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31462 (* 1 = 7.31462 loss)
I0523 05:42:33.252559 35003 sgd_solver.cpp:112] Iteration 155370, lr = 0.001
I0523 05:42:35.440016 35003 solver.cpp:239] Iteration 155380 (3.5805 iter/s, 2.79291s/10 iters), loss = 6.54564
I0523 05:42:35.440063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54564 (* 1 = 6.54564 loss)
I0523 05:42:35.453559 35003 sgd_solver.cpp:112] Iteration 155380, lr = 0.001
I0523 05:42:38.111095 35003 solver.cpp:239] Iteration 155390 (3.74403 iter/s, 2.67092s/10 iters), loss = 7.21852
I0523 05:42:38.111349 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21852 (* 1 = 7.21852 loss)
I0523 05:42:38.124258 35003 sgd_solver.cpp:112] Iteration 155390, lr = 0.001
I0523 05:42:40.896720 35003 solver.cpp:239] Iteration 155400 (3.59031 iter/s, 2.78527s/10 iters), loss = 6.22957
I0523 05:42:40.896771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22957 (* 1 = 6.22957 loss)
I0523 05:42:40.918176 35003 sgd_solver.cpp:112] Iteration 155400, lr = 0.001
I0523 05:42:43.146600 35003 solver.cpp:239] Iteration 155410 (4.44498 iter/s, 2.24973s/10 iters), loss = 6.54973
I0523 05:42:43.146654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54973 (* 1 = 6.54973 loss)
I0523 05:42:43.374548 35003 sgd_solver.cpp:112] Iteration 155410, lr = 0.001
I0523 05:42:46.204849 35003 solver.cpp:239] Iteration 155420 (3.27004 iter/s, 3.05807s/10 iters), loss = 7.72676
I0523 05:42:46.204895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72676 (* 1 = 7.72676 loss)
I0523 05:42:46.939417 35003 sgd_solver.cpp:112] Iteration 155420, lr = 0.001
I0523 05:42:48.631330 35003 solver.cpp:239] Iteration 155430 (4.12145 iter/s, 2.42633s/10 iters), loss = 5.64376
I0523 05:42:48.631367 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.64376 (* 1 = 5.64376 loss)
I0523 05:42:49.124029 35003 sgd_solver.cpp:112] Iteration 155430, lr = 0.001
I0523 05:42:51.922024 35003 solver.cpp:239] Iteration 155440 (3.03904 iter/s, 3.29051s/10 iters), loss = 6.12625
I0523 05:42:51.922058 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12625 (* 1 = 6.12625 loss)
I0523 05:42:51.940676 35003 sgd_solver.cpp:112] Iteration 155440, lr = 0.001
I0523 05:42:55.403584 35003 solver.cpp:239] Iteration 155450 (2.87242 iter/s, 3.48138s/10 iters), loss = 6.35539
I0523 05:42:55.403626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35539 (* 1 = 6.35539 loss)
I0523 05:42:55.408154 35003 sgd_solver.cpp:112] Iteration 155450, lr = 0.001
I0523 05:42:58.987885 35003 solver.cpp:239] Iteration 155460 (2.7901 iter/s, 3.5841s/10 iters), loss = 5.83325
I0523 05:42:58.987957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.83325 (* 1 = 5.83325 loss)
I0523 05:42:58.989089 35003 sgd_solver.cpp:112] Iteration 155460, lr = 0.001
I0523 05:43:03.305477 35003 solver.cpp:239] Iteration 155470 (2.31624 iter/s, 4.31734s/10 iters), loss = 6.45536
I0523 05:43:03.305517 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45536 (* 1 = 6.45536 loss)
I0523 05:43:03.308887 35003 sgd_solver.cpp:112] Iteration 155470, lr = 0.001
I0523 05:43:07.516113 35003 solver.cpp:239] Iteration 155480 (2.37507 iter/s, 4.2104s/10 iters), loss = 7.08957
I0523 05:43:07.516160 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08957 (* 1 = 7.08957 loss)
I0523 05:43:07.519383 35003 sgd_solver.cpp:112] Iteration 155480, lr = 0.001
I0523 05:43:09.739925 35003 solver.cpp:239] Iteration 155490 (4.4971 iter/s, 2.22365s/10 iters), loss = 6.94686
I0523 05:43:09.740136 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94686 (* 1 = 6.94686 loss)
I0523 05:43:09.753392 35003 sgd_solver.cpp:112] Iteration 155490, lr = 0.001
I0523 05:43:14.257613 35003 solver.cpp:239] Iteration 155500 (2.21371 iter/s, 4.51731s/10 iters), loss = 6.10847
I0523 05:43:14.257660 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10847 (* 1 = 6.10847 loss)
I0523 05:43:14.971873 35003 sgd_solver.cpp:112] Iteration 155500, lr = 0.001
I0523 05:43:19.047451 35003 solver.cpp:239] Iteration 155510 (2.08786 iter/s, 4.78959s/10 iters), loss = 7.58775
I0523 05:43:19.047500 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58775 (* 1 = 7.58775 loss)
I0523 05:43:19.060148 35003 sgd_solver.cpp:112] Iteration 155510, lr = 0.001
I0523 05:43:22.639906 35003 solver.cpp:239] Iteration 155520 (2.78376 iter/s, 3.59226s/10 iters), loss = 5.59312
I0523 05:43:22.639945 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.59312 (* 1 = 5.59312 loss)
I0523 05:43:22.731724 35003 sgd_solver.cpp:112] Iteration 155520, lr = 0.001
I0523 05:43:26.042661 35003 solver.cpp:239] Iteration 155530 (2.93895 iter/s, 3.40257s/10 iters), loss = 6.79151
I0523 05:43:26.042726 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79151 (* 1 = 6.79151 loss)
I0523 05:43:26.060693 35003 sgd_solver.cpp:112] Iteration 155530, lr = 0.001
I0523 05:43:27.852849 35003 solver.cpp:239] Iteration 155540 (5.52473 iter/s, 1.81004s/10 iters), loss = 7.12604
I0523 05:43:27.852893 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12604 (* 1 = 7.12604 loss)
I0523 05:43:28.563695 35003 sgd_solver.cpp:112] Iteration 155540, lr = 0.001
I0523 05:43:31.322856 35003 solver.cpp:239] Iteration 155550 (2.882 iter/s, 3.46981s/10 iters), loss = 6.49915
I0523 05:43:31.322903 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49915 (* 1 = 6.49915 loss)
I0523 05:43:31.331733 35003 sgd_solver.cpp:112] Iteration 155550, lr = 0.001
I0523 05:43:34.898787 35003 solver.cpp:239] Iteration 155560 (2.79663 iter/s, 3.57573s/10 iters), loss = 6.86036
I0523 05:43:34.898838 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86036 (* 1 = 6.86036 loss)
I0523 05:43:34.906939 35003 sgd_solver.cpp:112] Iteration 155560, lr = 0.001
I0523 05:43:37.894803 35003 solver.cpp:239] Iteration 155570 (3.33796 iter/s, 2.99584s/10 iters), loss = 7.36669
I0523 05:43:37.894834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36669 (* 1 = 7.36669 loss)
I0523 05:43:38.610569 35003 sgd_solver.cpp:112] Iteration 155570, lr = 0.001
I0523 05:43:40.717325 35003 solver.cpp:239] Iteration 155580 (3.54312 iter/s, 2.82237s/10 iters), loss = 5.80232
I0523 05:43:40.717602 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.80232 (* 1 = 5.80232 loss)
I0523 05:43:40.720218 35003 sgd_solver.cpp:112] Iteration 155580, lr = 0.001
I0523 05:43:43.623611 35003 solver.cpp:239] Iteration 155590 (3.44125 iter/s, 2.90592s/10 iters), loss = 5.89768
I0523 05:43:43.623653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89768 (* 1 = 5.89768 loss)
I0523 05:43:43.633507 35003 sgd_solver.cpp:112] Iteration 155590, lr = 0.001
I0523 05:43:47.940479 35003 solver.cpp:239] Iteration 155600 (2.31661 iter/s, 4.31665s/10 iters), loss = 6.09574
I0523 05:43:47.940513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09574 (* 1 = 6.09574 loss)
I0523 05:43:47.954396 35003 sgd_solver.cpp:112] Iteration 155600, lr = 0.001
I0523 05:43:51.793332 35003 solver.cpp:239] Iteration 155610 (2.59561 iter/s, 3.85265s/10 iters), loss = 8.03266
I0523 05:43:51.793378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03266 (* 1 = 8.03266 loss)
I0523 05:43:51.816301 35003 sgd_solver.cpp:112] Iteration 155610, lr = 0.001
I0523 05:43:56.153966 35003 solver.cpp:239] Iteration 155620 (2.29337 iter/s, 4.3604s/10 iters), loss = 5.43747
I0523 05:43:56.154018 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.43747 (* 1 = 5.43747 loss)
I0523 05:43:56.159771 35003 sgd_solver.cpp:112] Iteration 155620, lr = 0.001
I0523 05:43:58.939730 35003 solver.cpp:239] Iteration 155630 (3.5899 iter/s, 2.78559s/10 iters), loss = 6.95406
I0523 05:43:58.939782 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95406 (* 1 = 6.95406 loss)
I0523 05:43:58.945919 35003 sgd_solver.cpp:112] Iteration 155630, lr = 0.001
I0523 05:44:02.465629 35003 solver.cpp:239] Iteration 155640 (2.83632 iter/s, 3.5257s/10 iters), loss = 6.78484
I0523 05:44:02.465670 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78484 (* 1 = 6.78484 loss)
I0523 05:44:02.477941 35003 sgd_solver.cpp:112] Iteration 155640, lr = 0.001
I0523 05:44:05.956712 35003 solver.cpp:239] Iteration 155650 (2.8646 iter/s, 3.49089s/10 iters), loss = 6.70708
I0523 05:44:05.956771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70708 (* 1 = 6.70708 loss)
I0523 05:44:06.664984 35003 sgd_solver.cpp:112] Iteration 155650, lr = 0.001
I0523 05:44:10.386905 35003 solver.cpp:239] Iteration 155660 (2.25736 iter/s, 4.42995s/10 iters), loss = 6.05669
I0523 05:44:10.386957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05669 (* 1 = 6.05669 loss)
I0523 05:44:10.393900 35003 sgd_solver.cpp:112] Iteration 155660, lr = 0.001
I0523 05:44:14.430058 35003 solver.cpp:239] Iteration 155670 (2.47345 iter/s, 4.04293s/10 iters), loss = 6.61804
I0523 05:44:14.430349 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61804 (* 1 = 6.61804 loss)
I0523 05:44:15.144032 35003 sgd_solver.cpp:112] Iteration 155670, lr = 0.001
I0523 05:44:17.961513 35003 solver.cpp:239] Iteration 155680 (2.83202 iter/s, 3.53105s/10 iters), loss = 6.76178
I0523 05:44:17.961550 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76178 (* 1 = 6.76178 loss)
I0523 05:44:17.966301 35003 sgd_solver.cpp:112] Iteration 155680, lr = 0.001
I0523 05:44:22.312896 35003 solver.cpp:239] Iteration 155690 (2.29824 iter/s, 4.35116s/10 iters), loss = 6.50743
I0523 05:44:22.312933 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50743 (* 1 = 6.50743 loss)
I0523 05:44:22.326565 35003 sgd_solver.cpp:112] Iteration 155690, lr = 0.001
I0523 05:44:25.808352 35003 solver.cpp:239] Iteration 155700 (2.86101 iter/s, 3.49527s/10 iters), loss = 7.75574
I0523 05:44:25.808400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75574 (* 1 = 7.75574 loss)
I0523 05:44:25.818519 35003 sgd_solver.cpp:112] Iteration 155700, lr = 0.001
I0523 05:44:29.996582 35003 solver.cpp:239] Iteration 155710 (2.38777 iter/s, 4.188s/10 iters), loss = 5.71916
I0523 05:44:29.996626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.71916 (* 1 = 5.71916 loss)
I0523 05:44:29.999680 35003 sgd_solver.cpp:112] Iteration 155710, lr = 0.001
I0523 05:44:33.551792 35003 solver.cpp:239] Iteration 155720 (2.81293 iter/s, 3.55501s/10 iters), loss = 7.31278
I0523 05:44:33.551831 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31278 (* 1 = 7.31278 loss)
I0523 05:44:33.564730 35003 sgd_solver.cpp:112] Iteration 155720, lr = 0.001
I0523 05:44:36.403621 35003 solver.cpp:239] Iteration 155730 (3.50672 iter/s, 2.85167s/10 iters), loss = 6.12555
I0523 05:44:36.403668 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12555 (* 1 = 6.12555 loss)
I0523 05:44:36.417043 35003 sgd_solver.cpp:112] Iteration 155730, lr = 0.001
I0523 05:44:39.195807 35003 solver.cpp:239] Iteration 155740 (3.58163 iter/s, 2.79202s/10 iters), loss = 6.92683
I0523 05:44:39.195847 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92683 (* 1 = 6.92683 loss)
I0523 05:44:39.209233 35003 sgd_solver.cpp:112] Iteration 155740, lr = 0.001
I0523 05:44:43.165311 35003 solver.cpp:239] Iteration 155750 (2.51934 iter/s, 3.96929s/10 iters), loss = 6.12229
I0523 05:44:43.165351 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12229 (* 1 = 6.12229 loss)
I0523 05:44:43.795905 35003 sgd_solver.cpp:112] Iteration 155750, lr = 0.001
I0523 05:44:46.794991 35003 solver.cpp:239] Iteration 155760 (2.75522 iter/s, 3.62948s/10 iters), loss = 7.02269
I0523 05:44:46.795186 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02269 (* 1 = 7.02269 loss)
I0523 05:44:46.807566 35003 sgd_solver.cpp:112] Iteration 155760, lr = 0.001
I0523 05:44:49.317291 35003 solver.cpp:239] Iteration 155770 (3.9651 iter/s, 2.522s/10 iters), loss = 7.11557
I0523 05:44:49.317348 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11557 (* 1 = 7.11557 loss)
I0523 05:44:49.335230 35003 sgd_solver.cpp:112] Iteration 155770, lr = 0.001
I0523 05:44:53.544656 35003 solver.cpp:239] Iteration 155780 (2.36567 iter/s, 4.22714s/10 iters), loss = 6.07775
I0523 05:44:53.544703 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07775 (* 1 = 6.07775 loss)
I0523 05:44:54.240545 35003 sgd_solver.cpp:112] Iteration 155780, lr = 0.001
I0523 05:44:56.269585 35003 solver.cpp:239] Iteration 155790 (3.67004 iter/s, 2.72477s/10 iters), loss = 7.64223
I0523 05:44:56.269624 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64223 (* 1 = 7.64223 loss)
I0523 05:44:56.277506 35003 sgd_solver.cpp:112] Iteration 155790, lr = 0.001
I0523 05:44:59.799418 35003 solver.cpp:239] Iteration 155800 (2.83315 iter/s, 3.52964s/10 iters), loss = 6.19955
I0523 05:44:59.799484 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19955 (* 1 = 6.19955 loss)
I0523 05:45:00.534443 35003 sgd_solver.cpp:112] Iteration 155800, lr = 0.001
I0523 05:45:04.880717 35003 solver.cpp:239] Iteration 155810 (1.9681 iter/s, 5.08103s/10 iters), loss = 6.43608
I0523 05:45:04.880767 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43608 (* 1 = 6.43608 loss)
I0523 05:45:05.005164 35003 sgd_solver.cpp:112] Iteration 155810, lr = 0.001
I0523 05:45:07.090302 35003 solver.cpp:239] Iteration 155820 (4.52603 iter/s, 2.20944s/10 iters), loss = 7.68624
I0523 05:45:07.090350 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68624 (* 1 = 7.68624 loss)
I0523 05:45:07.103408 35003 sgd_solver.cpp:112] Iteration 155820, lr = 0.001
I0523 05:45:08.986416 35003 solver.cpp:239] Iteration 155830 (5.27431 iter/s, 1.89598s/10 iters), loss = 6.74443
I0523 05:45:08.986455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74443 (* 1 = 6.74443 loss)
I0523 05:45:08.996747 35003 sgd_solver.cpp:112] Iteration 155830, lr = 0.001
I0523 05:45:12.925549 35003 solver.cpp:239] Iteration 155840 (2.53876 iter/s, 3.93893s/10 iters), loss = 6.42835
I0523 05:45:12.925583 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42835 (* 1 = 6.42835 loss)
I0523 05:45:12.929939 35003 sgd_solver.cpp:112] Iteration 155840, lr = 0.001
I0523 05:45:15.816057 35003 solver.cpp:239] Iteration 155850 (3.4598 iter/s, 2.89034s/10 iters), loss = 6.81665
I0523 05:45:15.816113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81665 (* 1 = 6.81665 loss)
I0523 05:45:15.834743 35003 sgd_solver.cpp:112] Iteration 155850, lr = 0.001
I0523 05:45:19.279093 35003 solver.cpp:239] Iteration 155860 (2.88781 iter/s, 3.46284s/10 iters), loss = 6.65722
I0523 05:45:19.279294 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65722 (* 1 = 6.65722 loss)
I0523 05:45:19.297538 35003 sgd_solver.cpp:112] Iteration 155860, lr = 0.001
I0523 05:45:23.018448 35003 solver.cpp:239] Iteration 155870 (2.67451 iter/s, 3.739s/10 iters), loss = 7.03513
I0523 05:45:23.018488 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03513 (* 1 = 7.03513 loss)
I0523 05:45:23.032205 35003 sgd_solver.cpp:112] Iteration 155870, lr = 0.001
I0523 05:45:27.244226 35003 solver.cpp:239] Iteration 155880 (2.36655 iter/s, 4.22557s/10 iters), loss = 7.9789
I0523 05:45:27.244268 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9789 (* 1 = 7.9789 loss)
I0523 05:45:27.979418 35003 sgd_solver.cpp:112] Iteration 155880, lr = 0.001
I0523 05:45:32.977095 35003 solver.cpp:239] Iteration 155890 (1.74441 iter/s, 5.73259s/10 iters), loss = 6.88076
I0523 05:45:32.977135 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88076 (* 1 = 6.88076 loss)
I0523 05:45:32.991114 35003 sgd_solver.cpp:112] Iteration 155890, lr = 0.001
I0523 05:45:36.493919 35003 solver.cpp:239] Iteration 155900 (2.84363 iter/s, 3.51664s/10 iters), loss = 5.39991
I0523 05:45:36.493969 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.39991 (* 1 = 5.39991 loss)
I0523 05:45:36.501035 35003 sgd_solver.cpp:112] Iteration 155900, lr = 0.001
I0523 05:45:40.026124 35003 solver.cpp:239] Iteration 155910 (2.83126 iter/s, 3.532s/10 iters), loss = 7.21821
I0523 05:45:40.026170 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21821 (* 1 = 7.21821 loss)
I0523 05:45:40.038667 35003 sgd_solver.cpp:112] Iteration 155910, lr = 0.001
I0523 05:45:42.872620 35003 solver.cpp:239] Iteration 155920 (3.51331 iter/s, 2.84632s/10 iters), loss = 6.06881
I0523 05:45:42.872671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06881 (* 1 = 6.06881 loss)
I0523 05:45:43.592094 35003 sgd_solver.cpp:112] Iteration 155920, lr = 0.001
I0523 05:45:47.115249 35003 solver.cpp:239] Iteration 155930 (2.35715 iter/s, 4.2424s/10 iters), loss = 6.56081
I0523 05:45:47.115295 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56081 (* 1 = 6.56081 loss)
I0523 05:45:47.235687 35003 sgd_solver.cpp:112] Iteration 155930, lr = 0.001
I0523 05:45:50.049474 35003 solver.cpp:239] Iteration 155940 (3.40825 iter/s, 2.93405s/10 iters), loss = 7.46532
I0523 05:45:50.049662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46532 (* 1 = 7.46532 loss)
I0523 05:45:50.058380 35003 sgd_solver.cpp:112] Iteration 155940, lr = 0.001
I0523 05:45:52.906581 35003 solver.cpp:239] Iteration 155950 (3.5004 iter/s, 2.85681s/10 iters), loss = 7.14936
I0523 05:45:52.906627 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14936 (* 1 = 7.14936 loss)
I0523 05:45:52.916337 35003 sgd_solver.cpp:112] Iteration 155950, lr = 0.001
I0523 05:45:57.042100 35003 solver.cpp:239] Iteration 155960 (2.41821 iter/s, 4.1353s/10 iters), loss = 6.99858
I0523 05:45:57.042151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99858 (* 1 = 6.99858 loss)
I0523 05:45:57.054639 35003 sgd_solver.cpp:112] Iteration 155960, lr = 0.001
I0523 05:45:59.541954 35003 solver.cpp:239] Iteration 155970 (4.00048 iter/s, 2.4997s/10 iters), loss = 6.86689
I0523 05:45:59.541996 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86689 (* 1 = 6.86689 loss)
I0523 05:45:59.552243 35003 sgd_solver.cpp:112] Iteration 155970, lr = 0.001
I0523 05:46:04.328431 35003 solver.cpp:239] Iteration 155980 (2.08932 iter/s, 4.78624s/10 iters), loss = 6.39322
I0523 05:46:04.328471 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39322 (* 1 = 6.39322 loss)
I0523 05:46:04.341491 35003 sgd_solver.cpp:112] Iteration 155980, lr = 0.001
I0523 05:46:07.083056 35003 solver.cpp:239] Iteration 155990 (3.63047 iter/s, 2.75446s/10 iters), loss = 6.55967
I0523 05:46:07.083112 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55967 (* 1 = 6.55967 loss)
I0523 05:46:07.573469 35003 sgd_solver.cpp:112] Iteration 155990, lr = 0.001
I0523 05:46:10.599406 35003 solver.cpp:239] Iteration 156000 (2.84403 iter/s, 3.51614s/10 iters), loss = 6.84957
I0523 05:46:10.599449 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84957 (* 1 = 6.84957 loss)
I0523 05:46:10.620896 35003 sgd_solver.cpp:112] Iteration 156000, lr = 0.001
I0523 05:46:14.257356 35003 solver.cpp:239] Iteration 156010 (2.73392 iter/s, 3.65775s/10 iters), loss = 6.1028
I0523 05:46:14.257402 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1028 (* 1 = 6.1028 loss)
I0523 05:46:14.273013 35003 sgd_solver.cpp:112] Iteration 156010, lr = 0.001
I0523 05:46:17.821007 35003 solver.cpp:239] Iteration 156020 (2.80627 iter/s, 3.56345s/10 iters), loss = 6.62305
I0523 05:46:17.821053 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62305 (* 1 = 6.62305 loss)
I0523 05:46:18.529954 35003 sgd_solver.cpp:112] Iteration 156020, lr = 0.001
I0523 05:46:21.719616 35003 solver.cpp:239] Iteration 156030 (2.56516 iter/s, 3.89839s/10 iters), loss = 6.89301
I0523 05:46:21.719871 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89301 (* 1 = 6.89301 loss)
I0523 05:46:21.740377 35003 sgd_solver.cpp:112] Iteration 156030, lr = 0.001
I0523 05:46:25.319875 35003 solver.cpp:239] Iteration 156040 (2.77787 iter/s, 3.59988s/10 iters), loss = 7.27417
I0523 05:46:25.319912 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27417 (* 1 = 7.27417 loss)
I0523 05:46:26.032323 35003 sgd_solver.cpp:112] Iteration 156040, lr = 0.001
I0523 05:46:30.331740 35003 solver.cpp:239] Iteration 156050 (1.99536 iter/s, 5.01162s/10 iters), loss = 6.6536
I0523 05:46:30.331779 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6536 (* 1 = 6.6536 loss)
I0523 05:46:30.344440 35003 sgd_solver.cpp:112] Iteration 156050, lr = 0.001
I0523 05:46:34.588724 35003 solver.cpp:239] Iteration 156060 (2.3492 iter/s, 4.25677s/10 iters), loss = 6.90789
I0523 05:46:34.588763 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90789 (* 1 = 6.90789 loss)
I0523 05:46:34.601663 35003 sgd_solver.cpp:112] Iteration 156060, lr = 0.001
I0523 05:46:36.878646 35003 solver.cpp:239] Iteration 156070 (4.36723 iter/s, 2.28978s/10 iters), loss = 5.96081
I0523 05:46:36.878684 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96081 (* 1 = 5.96081 loss)
I0523 05:46:36.891762 35003 sgd_solver.cpp:112] Iteration 156070, lr = 0.001
I0523 05:46:40.357354 35003 solver.cpp:239] Iteration 156080 (2.87478 iter/s, 3.47852s/10 iters), loss = 6.72288
I0523 05:46:40.357399 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72288 (* 1 = 6.72288 loss)
I0523 05:46:40.370229 35003 sgd_solver.cpp:112] Iteration 156080, lr = 0.001
I0523 05:46:43.925241 35003 solver.cpp:239] Iteration 156090 (2.80293 iter/s, 3.56769s/10 iters), loss = 7.04725
I0523 05:46:43.925287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04725 (* 1 = 7.04725 loss)
I0523 05:46:44.594147 35003 sgd_solver.cpp:112] Iteration 156090, lr = 0.001
I0523 05:46:48.016273 35003 solver.cpp:239] Iteration 156100 (2.4445 iter/s, 4.09081s/10 iters), loss = 6.91277
I0523 05:46:48.016324 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91277 (* 1 = 6.91277 loss)
I0523 05:46:48.699039 35003 sgd_solver.cpp:112] Iteration 156100, lr = 0.001
I0523 05:46:51.025831 35003 solver.cpp:239] Iteration 156110 (3.32294 iter/s, 3.00939s/10 iters), loss = 7.21561
I0523 05:46:51.025873 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21561 (* 1 = 7.21561 loss)
I0523 05:46:51.030179 35003 sgd_solver.cpp:112] Iteration 156110, lr = 0.001
I0523 05:46:54.665951 35003 solver.cpp:239] Iteration 156120 (2.74734 iter/s, 3.63988s/10 iters), loss = 6.8707
I0523 05:46:54.666151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8707 (* 1 = 6.8707 loss)
I0523 05:46:55.406872 35003 sgd_solver.cpp:112] Iteration 156120, lr = 0.001
I0523 05:46:59.252676 35003 solver.cpp:239] Iteration 156130 (2.1804 iter/s, 4.58632s/10 iters), loss = 6.7091
I0523 05:46:59.252754 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7091 (* 1 = 6.7091 loss)
I0523 05:46:59.263964 35003 sgd_solver.cpp:112] Iteration 156130, lr = 0.001
I0523 05:47:02.803177 35003 solver.cpp:239] Iteration 156140 (2.81668 iter/s, 3.55028s/10 iters), loss = 7.39382
I0523 05:47:02.803215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39382 (* 1 = 7.39382 loss)
I0523 05:47:02.809557 35003 sgd_solver.cpp:112] Iteration 156140, lr = 0.001
I0523 05:47:07.790966 35003 solver.cpp:239] Iteration 156150 (2.00499 iter/s, 4.98755s/10 iters), loss = 5.73637
I0523 05:47:07.791024 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.73637 (* 1 = 5.73637 loss)
I0523 05:47:07.908321 35003 sgd_solver.cpp:112] Iteration 156150, lr = 0.001
I0523 05:47:10.807840 35003 solver.cpp:239] Iteration 156160 (3.31489 iter/s, 3.01669s/10 iters), loss = 6.70316
I0523 05:47:10.807883 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70316 (* 1 = 6.70316 loss)
I0523 05:47:10.871170 35003 sgd_solver.cpp:112] Iteration 156160, lr = 0.001
I0523 05:47:12.814705 35003 solver.cpp:239] Iteration 156170 (4.98325 iter/s, 2.00672s/10 iters), loss = 6.30345
I0523 05:47:12.814745 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30345 (* 1 = 6.30345 loss)
I0523 05:47:12.827845 35003 sgd_solver.cpp:112] Iteration 156170, lr = 0.001
I0523 05:47:15.520643 35003 solver.cpp:239] Iteration 156180 (3.6958 iter/s, 2.70578s/10 iters), loss = 4.9672
I0523 05:47:15.520691 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.9672 (* 1 = 4.9672 loss)
I0523 05:47:15.532474 35003 sgd_solver.cpp:112] Iteration 156180, lr = 0.001
I0523 05:47:18.943244 35003 solver.cpp:239] Iteration 156190 (2.92192 iter/s, 3.4224s/10 iters), loss = 5.74911
I0523 05:47:18.943295 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74911 (* 1 = 5.74911 loss)
I0523 05:47:19.224992 35003 sgd_solver.cpp:112] Iteration 156190, lr = 0.001
I0523 05:47:22.858186 35003 solver.cpp:239] Iteration 156200 (2.55446 iter/s, 3.91472s/10 iters), loss = 8.19414
I0523 05:47:22.858242 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.19414 (* 1 = 8.19414 loss)
I0523 05:47:23.598930 35003 sgd_solver.cpp:112] Iteration 156200, lr = 0.001
I0523 05:47:28.338488 35003 solver.cpp:239] Iteration 156210 (1.82481 iter/s, 5.48002s/10 iters), loss = 7.09002
I0523 05:47:28.338661 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09002 (* 1 = 7.09002 loss)
I0523 05:47:28.351475 35003 sgd_solver.cpp:112] Iteration 156210, lr = 0.001
I0523 05:47:31.297770 35003 solver.cpp:239] Iteration 156220 (3.37954 iter/s, 2.95898s/10 iters), loss = 4.87604
I0523 05:47:31.297813 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.87604 (* 1 = 4.87604 loss)
I0523 05:47:32.006541 35003 sgd_solver.cpp:112] Iteration 156220, lr = 0.001
I0523 05:47:35.759071 35003 solver.cpp:239] Iteration 156230 (2.24162 iter/s, 4.46106s/10 iters), loss = 7.38236
I0523 05:47:35.759126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38236 (* 1 = 7.38236 loss)
I0523 05:47:35.782204 35003 sgd_solver.cpp:112] Iteration 156230, lr = 0.001
I0523 05:47:40.235355 35003 solver.cpp:239] Iteration 156240 (2.23412 iter/s, 4.47604s/10 iters), loss = 6.20049
I0523 05:47:40.235410 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20049 (* 1 = 6.20049 loss)
I0523 05:47:40.976364 35003 sgd_solver.cpp:112] Iteration 156240, lr = 0.001
I0523 05:47:44.226419 35003 solver.cpp:239] Iteration 156250 (2.50574 iter/s, 3.99084s/10 iters), loss = 6.82568
I0523 05:47:44.226477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82568 (* 1 = 6.82568 loss)
I0523 05:47:44.920115 35003 sgd_solver.cpp:112] Iteration 156250, lr = 0.001
I0523 05:47:49.023900 35003 solver.cpp:239] Iteration 156260 (2.08454 iter/s, 4.79722s/10 iters), loss = 6.11812
I0523 05:47:49.023950 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11812 (* 1 = 6.11812 loss)
I0523 05:47:49.035454 35003 sgd_solver.cpp:112] Iteration 156260, lr = 0.001
I0523 05:47:51.853222 35003 solver.cpp:239] Iteration 156270 (3.53464 iter/s, 2.82914s/10 iters), loss = 6.33628
I0523 05:47:51.853260 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33628 (* 1 = 6.33628 loss)
I0523 05:47:51.866468 35003 sgd_solver.cpp:112] Iteration 156270, lr = 0.001
I0523 05:47:55.422469 35003 solver.cpp:239] Iteration 156280 (2.80186 iter/s, 3.56906s/10 iters), loss = 6.63374
I0523 05:47:55.422511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63374 (* 1 = 6.63374 loss)
I0523 05:47:55.438503 35003 sgd_solver.cpp:112] Iteration 156280, lr = 0.001
I0523 05:47:59.952721 35003 solver.cpp:239] Iteration 156290 (2.20749 iter/s, 4.53003s/10 iters), loss = 7.41289
I0523 05:47:59.952836 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41289 (* 1 = 7.41289 loss)
I0523 05:47:59.964546 35003 sgd_solver.cpp:112] Iteration 156290, lr = 0.001
I0523 05:48:02.876114 35003 solver.cpp:239] Iteration 156300 (3.42097 iter/s, 2.92315s/10 iters), loss = 6.29935
I0523 05:48:02.876163 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29935 (* 1 = 6.29935 loss)
I0523 05:48:03.604914 35003 sgd_solver.cpp:112] Iteration 156300, lr = 0.001
I0523 05:48:07.247712 35003 solver.cpp:239] Iteration 156310 (2.28762 iter/s, 4.37136s/10 iters), loss = 6.86423
I0523 05:48:07.247761 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86423 (* 1 = 6.86423 loss)
I0523 05:48:07.259678 35003 sgd_solver.cpp:112] Iteration 156310, lr = 0.001
I0523 05:48:10.749918 35003 solver.cpp:239] Iteration 156320 (2.85551 iter/s, 3.502s/10 iters), loss = 5.90599
I0523 05:48:10.749961 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.90599 (* 1 = 5.90599 loss)
I0523 05:48:10.757544 35003 sgd_solver.cpp:112] Iteration 156320, lr = 0.001
I0523 05:48:12.999485 35003 solver.cpp:239] Iteration 156330 (4.44566 iter/s, 2.24939s/10 iters), loss = 5.90589
I0523 05:48:12.999531 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.90589 (* 1 = 5.90589 loss)
I0523 05:48:13.009677 35003 sgd_solver.cpp:112] Iteration 156330, lr = 0.001
I0523 05:48:17.867434 35003 solver.cpp:239] Iteration 156340 (2.05436 iter/s, 4.8677s/10 iters), loss = 7.20044
I0523 05:48:17.867477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20044 (* 1 = 7.20044 loss)
I0523 05:48:17.879683 35003 sgd_solver.cpp:112] Iteration 156340, lr = 0.001
I0523 05:48:19.948078 35003 solver.cpp:239] Iteration 156350 (4.80653 iter/s, 2.0805s/10 iters), loss = 7.22182
I0523 05:48:19.948130 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22182 (* 1 = 7.22182 loss)
I0523 05:48:19.960974 35003 sgd_solver.cpp:112] Iteration 156350, lr = 0.001
I0523 05:48:22.561439 35003 solver.cpp:239] Iteration 156360 (3.82673 iter/s, 2.61319s/10 iters), loss = 6.54967
I0523 05:48:22.561483 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54967 (* 1 = 6.54967 loss)
I0523 05:48:22.932783 35003 sgd_solver.cpp:112] Iteration 156360, lr = 0.001
I0523 05:48:26.396286 35003 solver.cpp:239] Iteration 156370 (2.60781 iter/s, 3.83464s/10 iters), loss = 6.38793
I0523 05:48:26.396328 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38793 (* 1 = 6.38793 loss)
I0523 05:48:27.105350 35003 sgd_solver.cpp:112] Iteration 156370, lr = 0.001
I0523 05:48:31.012414 35003 solver.cpp:239] Iteration 156380 (2.16643 iter/s, 4.61589s/10 iters), loss = 6.16481
I0523 05:48:31.012682 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16481 (* 1 = 6.16481 loss)
I0523 05:48:31.160727 35003 sgd_solver.cpp:112] Iteration 156380, lr = 0.001
I0523 05:48:34.776255 35003 solver.cpp:239] Iteration 156390 (2.65714 iter/s, 3.76344s/10 iters), loss = 7.07187
I0523 05:48:34.776299 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07187 (* 1 = 7.07187 loss)
I0523 05:48:34.786005 35003 sgd_solver.cpp:112] Iteration 156390, lr = 0.001
I0523 05:48:36.911239 35003 solver.cpp:239] Iteration 156400 (4.6842 iter/s, 2.13483s/10 iters), loss = 6.47273
I0523 05:48:36.911298 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47273 (* 1 = 6.47273 loss)
I0523 05:48:37.652278 35003 sgd_solver.cpp:112] Iteration 156400, lr = 0.001
I0523 05:48:38.975067 35003 solver.cpp:239] Iteration 156410 (4.84571 iter/s, 2.06368s/10 iters), loss = 5.30095
I0523 05:48:38.975106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.30095 (* 1 = 5.30095 loss)
I0523 05:48:39.715945 35003 sgd_solver.cpp:112] Iteration 156410, lr = 0.001
I0523 05:48:41.841217 35003 solver.cpp:239] Iteration 156420 (3.4892 iter/s, 2.86599s/10 iters), loss = 7.18462
I0523 05:48:41.841274 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18462 (* 1 = 7.18462 loss)
I0523 05:48:42.582206 35003 sgd_solver.cpp:112] Iteration 156420, lr = 0.001
I0523 05:48:46.334861 35003 solver.cpp:239] Iteration 156430 (2.22549 iter/s, 4.4934s/10 iters), loss = 6.49634
I0523 05:48:46.334911 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49634 (* 1 = 6.49634 loss)
I0523 05:48:46.423467 35003 sgd_solver.cpp:112] Iteration 156430, lr = 0.001
I0523 05:48:49.153435 35003 solver.cpp:239] Iteration 156440 (3.54811 iter/s, 2.8184s/10 iters), loss = 6.41044
I0523 05:48:49.153483 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41044 (* 1 = 6.41044 loss)
I0523 05:48:49.161027 35003 sgd_solver.cpp:112] Iteration 156440, lr = 0.001
I0523 05:48:51.936090 35003 solver.cpp:239] Iteration 156450 (3.59392 iter/s, 2.78248s/10 iters), loss = 6.83713
I0523 05:48:51.936136 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83713 (* 1 = 6.83713 loss)
I0523 05:48:51.940440 35003 sgd_solver.cpp:112] Iteration 156450, lr = 0.001
I0523 05:48:54.028841 35003 solver.cpp:239] Iteration 156460 (4.77874 iter/s, 2.0926s/10 iters), loss = 6.95312
I0523 05:48:54.028875 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95312 (* 1 = 6.95312 loss)
I0523 05:48:54.126549 35003 sgd_solver.cpp:112] Iteration 156460, lr = 0.001
I0523 05:48:57.044910 35003 solver.cpp:239] Iteration 156470 (3.31575 iter/s, 3.01591s/10 iters), loss = 7.0282
I0523 05:48:57.044953 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0282 (* 1 = 7.0282 loss)
I0523 05:48:57.739377 35003 sgd_solver.cpp:112] Iteration 156470, lr = 0.001
I0523 05:49:01.048485 35003 solver.cpp:239] Iteration 156480 (2.4979 iter/s, 4.00336s/10 iters), loss = 6.80188
I0523 05:49:01.048774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80188 (* 1 = 6.80188 loss)
I0523 05:49:01.095268 35003 sgd_solver.cpp:112] Iteration 156480, lr = 0.001
I0523 05:49:03.927454 35003 solver.cpp:239] Iteration 156490 (3.47393 iter/s, 2.87858s/10 iters), loss = 6.24244
I0523 05:49:03.927495 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24244 (* 1 = 6.24244 loss)
I0523 05:49:03.934253 35003 sgd_solver.cpp:112] Iteration 156490, lr = 0.001
I0523 05:49:06.854445 35003 solver.cpp:239] Iteration 156500 (3.41667 iter/s, 2.92683s/10 iters), loss = 6.0313
I0523 05:49:06.854490 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0313 (* 1 = 6.0313 loss)
I0523 05:49:06.996012 35003 sgd_solver.cpp:112] Iteration 156500, lr = 0.001
I0523 05:49:11.422138 35003 solver.cpp:239] Iteration 156510 (2.18941 iter/s, 4.56745s/10 iters), loss = 6.72366
I0523 05:49:11.422192 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72366 (* 1 = 6.72366 loss)
I0523 05:49:11.428712 35003 sgd_solver.cpp:112] Iteration 156510, lr = 0.001
I0523 05:49:15.731597 35003 solver.cpp:239] Iteration 156520 (2.3206 iter/s, 4.30922s/10 iters), loss = 6.92228
I0523 05:49:15.731652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92228 (* 1 = 6.92228 loss)
I0523 05:49:15.738611 35003 sgd_solver.cpp:112] Iteration 156520, lr = 0.001
I0523 05:49:18.620687 35003 solver.cpp:239] Iteration 156530 (3.46152 iter/s, 2.8889s/10 iters), loss = 6.93643
I0523 05:49:18.620723 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93643 (* 1 = 6.93643 loss)
I0523 05:49:18.629250 35003 sgd_solver.cpp:112] Iteration 156530, lr = 0.001
I0523 05:49:21.544952 35003 solver.cpp:239] Iteration 156540 (3.41985 iter/s, 2.9241s/10 iters), loss = 6.50481
I0523 05:49:21.544996 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50481 (* 1 = 6.50481 loss)
I0523 05:49:22.259840 35003 sgd_solver.cpp:112] Iteration 156540, lr = 0.001
I0523 05:49:25.853190 35003 solver.cpp:239] Iteration 156550 (2.32125 iter/s, 4.30802s/10 iters), loss = 5.70617
I0523 05:49:25.853241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.70617 (* 1 = 5.70617 loss)
I0523 05:49:25.864660 35003 sgd_solver.cpp:112] Iteration 156550, lr = 0.001
I0523 05:49:30.214529 35003 solver.cpp:239] Iteration 156560 (2.293 iter/s, 4.36111s/10 iters), loss = 7.43134
I0523 05:49:30.214577 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43134 (* 1 = 7.43134 loss)
I0523 05:49:30.222985 35003 sgd_solver.cpp:112] Iteration 156560, lr = 0.001
I0523 05:49:33.710168 35003 solver.cpp:239] Iteration 156570 (2.86087 iter/s, 3.49544s/10 iters), loss = 6.70163
I0523 05:49:33.710363 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70163 (* 1 = 6.70163 loss)
I0523 05:49:33.722805 35003 sgd_solver.cpp:112] Iteration 156570, lr = 0.001
I0523 05:49:37.324662 35003 solver.cpp:239] Iteration 156580 (2.7669 iter/s, 3.61415s/10 iters), loss = 5.65494
I0523 05:49:37.324719 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.65494 (* 1 = 5.65494 loss)
I0523 05:49:37.332319 35003 sgd_solver.cpp:112] Iteration 156580, lr = 0.001
I0523 05:49:40.875089 35003 solver.cpp:239] Iteration 156590 (2.81674 iter/s, 3.5502s/10 iters), loss = 6.8659
I0523 05:49:40.875142 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8659 (* 1 = 6.8659 loss)
I0523 05:49:40.888792 35003 sgd_solver.cpp:112] Iteration 156590, lr = 0.001
I0523 05:49:44.484019 35003 solver.cpp:239] Iteration 156600 (2.77106 iter/s, 3.60873s/10 iters), loss = 6.52917
I0523 05:49:44.484063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52917 (* 1 = 6.52917 loss)
I0523 05:49:45.192868 35003 sgd_solver.cpp:112] Iteration 156600, lr = 0.001
I0523 05:49:47.281702 35003 solver.cpp:239] Iteration 156610 (3.5746 iter/s, 2.79751s/10 iters), loss = 8.173
I0523 05:49:47.281754 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.173 (* 1 = 8.173 loss)
I0523 05:49:47.289752 35003 sgd_solver.cpp:112] Iteration 156610, lr = 0.001
I0523 05:49:50.817761 35003 solver.cpp:239] Iteration 156620 (2.82817 iter/s, 3.53586s/10 iters), loss = 6.34153
I0523 05:49:50.817811 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34153 (* 1 = 6.34153 loss)
I0523 05:49:50.830914 35003 sgd_solver.cpp:112] Iteration 156620, lr = 0.001
I0523 05:49:55.937279 35003 solver.cpp:239] Iteration 156630 (1.95341 iter/s, 5.11926s/10 iters), loss = 7.56898
I0523 05:49:55.937330 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56898 (* 1 = 7.56898 loss)
I0523 05:49:55.945078 35003 sgd_solver.cpp:112] Iteration 156630, lr = 0.001
I0523 05:50:00.197382 35003 solver.cpp:239] Iteration 156640 (2.34748 iter/s, 4.25988s/10 iters), loss = 6.45162
I0523 05:50:00.197422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45162 (* 1 = 6.45162 loss)
I0523 05:50:00.210674 35003 sgd_solver.cpp:112] Iteration 156640, lr = 0.001
I0523 05:50:04.665956 35003 solver.cpp:239] Iteration 156650 (2.23797 iter/s, 4.46833s/10 iters), loss = 6.66754
I0523 05:50:04.666216 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66754 (* 1 = 6.66754 loss)
I0523 05:50:05.376401 35003 sgd_solver.cpp:112] Iteration 156650, lr = 0.001
I0523 05:50:07.959882 35003 solver.cpp:239] Iteration 156660 (3.03624 iter/s, 3.29355s/10 iters), loss = 7.00187
I0523 05:50:07.959930 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00187 (* 1 = 7.00187 loss)
I0523 05:50:08.657614 35003 sgd_solver.cpp:112] Iteration 156660, lr = 0.001
I0523 05:50:11.229905 35003 solver.cpp:239] Iteration 156670 (3.05827 iter/s, 3.26983s/10 iters), loss = 6.04939
I0523 05:50:11.229970 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04939 (* 1 = 6.04939 loss)
I0523 05:50:11.961006 35003 sgd_solver.cpp:112] Iteration 156670, lr = 0.001
I0523 05:50:14.685367 35003 solver.cpp:239] Iteration 156680 (2.89414 iter/s, 3.45526s/10 iters), loss = 6.40497
I0523 05:50:14.685411 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40497 (* 1 = 6.40497 loss)
I0523 05:50:14.698187 35003 sgd_solver.cpp:112] Iteration 156680, lr = 0.001
I0523 05:50:17.796000 35003 solver.cpp:239] Iteration 156690 (3.21496 iter/s, 3.11046s/10 iters), loss = 7.91845
I0523 05:50:17.796039 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91845 (* 1 = 7.91845 loss)
I0523 05:50:17.816068 35003 sgd_solver.cpp:112] Iteration 156690, lr = 0.001
I0523 05:50:20.721983 35003 solver.cpp:239] Iteration 156700 (3.41785 iter/s, 2.92582s/10 iters), loss = 6.17528
I0523 05:50:20.722035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17528 (* 1 = 6.17528 loss)
I0523 05:50:20.735023 35003 sgd_solver.cpp:112] Iteration 156700, lr = 0.001
I0523 05:50:22.825719 35003 solver.cpp:239] Iteration 156710 (4.75379 iter/s, 2.10359s/10 iters), loss = 7.07838
I0523 05:50:22.825769 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07838 (* 1 = 7.07838 loss)
I0523 05:50:23.554126 35003 sgd_solver.cpp:112] Iteration 156710, lr = 0.001
I0523 05:50:26.291896 35003 solver.cpp:239] Iteration 156720 (2.88519 iter/s, 3.46598s/10 iters), loss = 7.09645
I0523 05:50:26.291951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09645 (* 1 = 7.09645 loss)
I0523 05:50:26.294184 35003 sgd_solver.cpp:112] Iteration 156720, lr = 0.001
I0523 05:50:30.437851 35003 solver.cpp:239] Iteration 156730 (2.41213 iter/s, 4.14572s/10 iters), loss = 7.24437
I0523 05:50:30.437891 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24437 (* 1 = 7.24437 loss)
I0523 05:50:30.451318 35003 sgd_solver.cpp:112] Iteration 156730, lr = 0.001
I0523 05:50:32.563575 35003 solver.cpp:239] Iteration 156740 (4.70458 iter/s, 2.12559s/10 iters), loss = 6.20205
I0523 05:50:32.563624 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20205 (* 1 = 6.20205 loss)
I0523 05:50:32.589967 35003 sgd_solver.cpp:112] Iteration 156740, lr = 0.001
I0523 05:50:36.103893 35003 solver.cpp:239] Iteration 156750 (2.82476 iter/s, 3.54012s/10 iters), loss = 7.25674
I0523 05:50:36.104099 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25674 (* 1 = 7.25674 loss)
I0523 05:50:36.136409 35003 sgd_solver.cpp:112] Iteration 156750, lr = 0.001
I0523 05:50:39.784575 35003 solver.cpp:239] Iteration 156760 (2.71715 iter/s, 3.68032s/10 iters), loss = 6.80397
I0523 05:50:39.784622 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80397 (* 1 = 6.80397 loss)
I0523 05:50:39.797756 35003 sgd_solver.cpp:112] Iteration 156760, lr = 0.001
I0523 05:50:43.917074 35003 solver.cpp:239] Iteration 156770 (2.41997 iter/s, 4.13228s/10 iters), loss = 6.75525
I0523 05:50:43.917117 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75525 (* 1 = 6.75525 loss)
I0523 05:50:43.924226 35003 sgd_solver.cpp:112] Iteration 156770, lr = 0.001
I0523 05:50:48.305344 35003 solver.cpp:239] Iteration 156780 (2.27892 iter/s, 4.38804s/10 iters), loss = 7.15273
I0523 05:50:48.305388 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15273 (* 1 = 7.15273 loss)
I0523 05:50:48.312638 35003 sgd_solver.cpp:112] Iteration 156780, lr = 0.001
I0523 05:50:53.197604 35003 solver.cpp:239] Iteration 156790 (2.04415 iter/s, 4.892s/10 iters), loss = 7.0101
I0523 05:50:53.197664 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0101 (* 1 = 7.0101 loss)
I0523 05:50:53.199782 35003 sgd_solver.cpp:112] Iteration 156790, lr = 0.001
I0523 05:50:55.952630 35003 solver.cpp:239] Iteration 156800 (3.62997 iter/s, 2.75485s/10 iters), loss = 7.16652
I0523 05:50:55.952682 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16652 (* 1 = 7.16652 loss)
I0523 05:50:56.451653 35003 sgd_solver.cpp:112] Iteration 156800, lr = 0.001
I0523 05:51:00.662773 35003 solver.cpp:239] Iteration 156810 (2.12319 iter/s, 4.7099s/10 iters), loss = 6.0805
I0523 05:51:00.662825 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0805 (* 1 = 6.0805 loss)
I0523 05:51:01.240530 35003 sgd_solver.cpp:112] Iteration 156810, lr = 0.001
I0523 05:51:05.436575 35003 solver.cpp:239] Iteration 156820 (2.09488 iter/s, 4.77355s/10 iters), loss = 6.12321
I0523 05:51:05.436617 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12321 (* 1 = 6.12321 loss)
I0523 05:51:05.449759 35003 sgd_solver.cpp:112] Iteration 156820, lr = 0.001
I0523 05:51:06.811516 35003 solver.cpp:239] Iteration 156830 (7.27361 iter/s, 1.37483s/10 iters), loss = 6.82291
I0523 05:51:06.811759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82291 (* 1 = 6.82291 loss)
I0523 05:51:07.520586 35003 sgd_solver.cpp:112] Iteration 156830, lr = 0.001
I0523 05:51:11.726385 35003 solver.cpp:239] Iteration 156840 (2.03481 iter/s, 4.91446s/10 iters), loss = 7.53557
I0523 05:51:11.726426 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53557 (* 1 = 7.53557 loss)
I0523 05:51:11.750295 35003 sgd_solver.cpp:112] Iteration 156840, lr = 0.001
I0523 05:51:14.753698 35003 solver.cpp:239] Iteration 156850 (3.30345 iter/s, 3.02714s/10 iters), loss = 7.06658
I0523 05:51:14.753749 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06658 (* 1 = 7.06658 loss)
I0523 05:51:14.759202 35003 sgd_solver.cpp:112] Iteration 156850, lr = 0.001
I0523 05:51:19.716127 35003 solver.cpp:239] Iteration 156860 (2.01525 iter/s, 4.96216s/10 iters), loss = 6.43155
I0523 05:51:19.716174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43155 (* 1 = 6.43155 loss)
I0523 05:51:19.729375 35003 sgd_solver.cpp:112] Iteration 156860, lr = 0.001
I0523 05:51:24.586941 35003 solver.cpp:239] Iteration 156870 (2.05315 iter/s, 4.87056s/10 iters), loss = 6.66065
I0523 05:51:24.586992 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66065 (* 1 = 6.66065 loss)
I0523 05:51:24.600037 35003 sgd_solver.cpp:112] Iteration 156870, lr = 0.001
I0523 05:51:28.961936 35003 solver.cpp:239] Iteration 156880 (2.28584 iter/s, 4.37476s/10 iters), loss = 7.02373
I0523 05:51:28.961971 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02373 (* 1 = 7.02373 loss)
I0523 05:51:28.974817 35003 sgd_solver.cpp:112] Iteration 156880, lr = 0.001
I0523 05:51:32.206413 35003 solver.cpp:239] Iteration 156890 (3.08232 iter/s, 3.24431s/10 iters), loss = 6.38778
I0523 05:51:32.206454 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38778 (* 1 = 6.38778 loss)
I0523 05:51:32.787510 35003 sgd_solver.cpp:112] Iteration 156890, lr = 0.001
I0523 05:51:35.414989 35003 solver.cpp:239] Iteration 156900 (3.11683 iter/s, 3.20839s/10 iters), loss = 7.02593
I0523 05:51:35.415035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02593 (* 1 = 7.02593 loss)
I0523 05:51:35.416643 35003 sgd_solver.cpp:112] Iteration 156900, lr = 0.001
I0523 05:51:38.322595 35003 solver.cpp:239] Iteration 156910 (3.43948 iter/s, 2.90742s/10 iters), loss = 6.26268
I0523 05:51:38.322895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26268 (* 1 = 6.26268 loss)
I0523 05:51:39.057580 35003 sgd_solver.cpp:112] Iteration 156910, lr = 0.001
I0523 05:51:41.766928 35003 solver.cpp:239] Iteration 156920 (2.90369 iter/s, 3.4439s/10 iters), loss = 6.99434
I0523 05:51:41.766971 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99434 (* 1 = 6.99434 loss)
I0523 05:51:41.778699 35003 sgd_solver.cpp:112] Iteration 156920, lr = 0.001
I0523 05:51:44.549079 35003 solver.cpp:239] Iteration 156930 (3.59455 iter/s, 2.78199s/10 iters), loss = 6.81692
I0523 05:51:44.549120 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81692 (* 1 = 6.81692 loss)
I0523 05:51:45.246848 35003 sgd_solver.cpp:112] Iteration 156930, lr = 0.001
I0523 05:51:48.142818 35003 solver.cpp:239] Iteration 156940 (2.78276 iter/s, 3.59355s/10 iters), loss = 6.49566
I0523 05:51:48.142859 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49566 (* 1 = 6.49566 loss)
I0523 05:51:48.880509 35003 sgd_solver.cpp:112] Iteration 156940, lr = 0.001
I0523 05:51:52.363052 35003 solver.cpp:239] Iteration 156950 (2.36966 iter/s, 4.22002s/10 iters), loss = 7.19353
I0523 05:51:52.363101 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19353 (* 1 = 7.19353 loss)
I0523 05:51:52.387455 35003 sgd_solver.cpp:112] Iteration 156950, lr = 0.001
I0523 05:51:55.570482 35003 solver.cpp:239] Iteration 156960 (3.11795 iter/s, 3.20723s/10 iters), loss = 7.07914
I0523 05:51:55.570541 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07914 (* 1 = 7.07914 loss)
I0523 05:51:56.246338 35003 sgd_solver.cpp:112] Iteration 156960, lr = 0.001
I0523 05:51:59.173514 35003 solver.cpp:239] Iteration 156970 (2.7756 iter/s, 3.60283s/10 iters), loss = 7.66923
I0523 05:51:59.173562 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66923 (* 1 = 7.66923 loss)
I0523 05:51:59.861012 35003 sgd_solver.cpp:112] Iteration 156970, lr = 0.001
I0523 05:52:02.593608 35003 solver.cpp:239] Iteration 156980 (2.92406 iter/s, 3.4199s/10 iters), loss = 6.99915
I0523 05:52:02.593657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99915 (* 1 = 6.99915 loss)
I0523 05:52:02.598685 35003 sgd_solver.cpp:112] Iteration 156980, lr = 0.001
I0523 05:52:05.593988 35003 solver.cpp:239] Iteration 156990 (3.33311 iter/s, 3.00021s/10 iters), loss = 7.60595
I0523 05:52:05.594035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60595 (* 1 = 7.60595 loss)
I0523 05:52:05.598600 35003 sgd_solver.cpp:112] Iteration 156990, lr = 0.001
I0523 05:52:08.996475 35003 solver.cpp:239] Iteration 157000 (2.93919 iter/s, 3.4023s/10 iters), loss = 6.39835
I0523 05:52:08.996662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39835 (* 1 = 6.39835 loss)
I0523 05:52:09.014866 35003 sgd_solver.cpp:112] Iteration 157000, lr = 0.001
I0523 05:52:12.086539 35003 solver.cpp:239] Iteration 157010 (3.23651 iter/s, 3.08975s/10 iters), loss = 8.1452
I0523 05:52:12.086578 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.1452 (* 1 = 8.1452 loss)
I0523 05:52:12.099810 35003 sgd_solver.cpp:112] Iteration 157010, lr = 0.001
I0523 05:52:16.558677 35003 solver.cpp:239] Iteration 157020 (2.23618 iter/s, 4.47191s/10 iters), loss = 5.94023
I0523 05:52:16.558753 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.94023 (* 1 = 5.94023 loss)
I0523 05:52:16.565428 35003 sgd_solver.cpp:112] Iteration 157020, lr = 0.001
I0523 05:52:18.607653 35003 solver.cpp:239] Iteration 157030 (4.88088 iter/s, 2.04881s/10 iters), loss = 7.33933
I0523 05:52:18.607702 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33933 (* 1 = 7.33933 loss)
I0523 05:52:18.614339 35003 sgd_solver.cpp:112] Iteration 157030, lr = 0.001
I0523 05:52:22.774058 35003 solver.cpp:239] Iteration 157040 (2.40028 iter/s, 4.16618s/10 iters), loss = 5.8781
I0523 05:52:22.774104 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8781 (* 1 = 5.8781 loss)
I0523 05:52:22.781980 35003 sgd_solver.cpp:112] Iteration 157040, lr = 0.001
I0523 05:52:24.954407 35003 solver.cpp:239] Iteration 157050 (4.58673 iter/s, 2.1802s/10 iters), loss = 6.56267
I0523 05:52:24.954470 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56267 (* 1 = 6.56267 loss)
I0523 05:52:25.523643 35003 sgd_solver.cpp:112] Iteration 157050, lr = 0.001
I0523 05:52:29.026111 35003 solver.cpp:239] Iteration 157060 (2.45611 iter/s, 4.07148s/10 iters), loss = 6.60234
I0523 05:52:29.026149 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60234 (* 1 = 6.60234 loss)
I0523 05:52:29.034329 35003 sgd_solver.cpp:112] Iteration 157060, lr = 0.001
I0523 05:52:31.905918 35003 solver.cpp:239] Iteration 157070 (3.47265 iter/s, 2.87964s/10 iters), loss = 7.64671
I0523 05:52:31.905972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64671 (* 1 = 7.64671 loss)
I0523 05:52:31.918754 35003 sgd_solver.cpp:112] Iteration 157070, lr = 0.001
I0523 05:52:34.659237 35003 solver.cpp:239] Iteration 157080 (3.63221 iter/s, 2.75314s/10 iters), loss = 4.82914
I0523 05:52:34.659281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.82914 (* 1 = 4.82914 loss)
I0523 05:52:34.666976 35003 sgd_solver.cpp:112] Iteration 157080, lr = 0.001
I0523 05:52:38.545148 35003 solver.cpp:239] Iteration 157090 (2.57353 iter/s, 3.88571s/10 iters), loss = 6.67128
I0523 05:52:38.545192 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67128 (* 1 = 6.67128 loss)
I0523 05:52:38.559206 35003 sgd_solver.cpp:112] Iteration 157090, lr = 0.001
I0523 05:52:40.668900 35003 solver.cpp:239] Iteration 157100 (4.70896 iter/s, 2.12361s/10 iters), loss = 6.58655
I0523 05:52:40.669107 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58655 (* 1 = 6.58655 loss)
I0523 05:52:40.682070 35003 sgd_solver.cpp:112] Iteration 157100, lr = 0.001
I0523 05:52:44.203353 35003 solver.cpp:239] Iteration 157110 (2.82955 iter/s, 3.53413s/10 iters), loss = 6.44272
I0523 05:52:44.203393 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44272 (* 1 = 6.44272 loss)
I0523 05:52:44.945055 35003 sgd_solver.cpp:112] Iteration 157110, lr = 0.001
I0523 05:52:47.949350 35003 solver.cpp:239] Iteration 157120 (2.66966 iter/s, 3.7458s/10 iters), loss = 6.72282
I0523 05:52:47.949390 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72282 (* 1 = 6.72282 loss)
I0523 05:52:48.684443 35003 sgd_solver.cpp:112] Iteration 157120, lr = 0.001
I0523 05:52:51.552564 35003 solver.cpp:239] Iteration 157130 (2.77545 iter/s, 3.60302s/10 iters), loss = 5.21383
I0523 05:52:51.552611 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.21383 (* 1 = 5.21383 loss)
I0523 05:52:51.566349 35003 sgd_solver.cpp:112] Iteration 157130, lr = 0.001
I0523 05:52:54.348044 35003 solver.cpp:239] Iteration 157140 (3.57742 iter/s, 2.79531s/10 iters), loss = 6.56163
I0523 05:52:54.348088 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56163 (* 1 = 6.56163 loss)
I0523 05:52:54.356721 35003 sgd_solver.cpp:112] Iteration 157140, lr = 0.001
I0523 05:52:58.031527 35003 solver.cpp:239] Iteration 157150 (2.71497 iter/s, 3.68328s/10 iters), loss = 7.33065
I0523 05:52:58.031569 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33065 (* 1 = 7.33065 loss)
I0523 05:52:58.044736 35003 sgd_solver.cpp:112] Iteration 157150, lr = 0.001
I0523 05:53:00.805491 35003 solver.cpp:239] Iteration 157160 (3.60517 iter/s, 2.7738s/10 iters), loss = 6.64724
I0523 05:53:00.805546 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64724 (* 1 = 6.64724 loss)
I0523 05:53:01.507926 35003 sgd_solver.cpp:112] Iteration 157160, lr = 0.001
I0523 05:53:05.718375 35003 solver.cpp:239] Iteration 157170 (2.03557 iter/s, 4.91263s/10 iters), loss = 6.46603
I0523 05:53:05.718425 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46603 (* 1 = 6.46603 loss)
I0523 05:53:06.148269 35003 sgd_solver.cpp:112] Iteration 157170, lr = 0.001
I0523 05:53:09.878123 35003 solver.cpp:239] Iteration 157180 (2.40412 iter/s, 4.15953s/10 iters), loss = 7.00325
I0523 05:53:09.878163 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00325 (* 1 = 7.00325 loss)
I0523 05:53:10.491469 35003 sgd_solver.cpp:112] Iteration 157180, lr = 0.001
I0523 05:53:13.185926 35003 solver.cpp:239] Iteration 157190 (3.02332 iter/s, 3.30763s/10 iters), loss = 7.19326
I0523 05:53:13.186130 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19326 (* 1 = 7.19326 loss)
I0523 05:53:13.199443 35003 sgd_solver.cpp:112] Iteration 157190, lr = 0.001
I0523 05:53:16.744997 35003 solver.cpp:239] Iteration 157200 (2.80999 iter/s, 3.55873s/10 iters), loss = 7.51826
I0523 05:53:16.745040 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51826 (* 1 = 7.51826 loss)
I0523 05:53:17.478178 35003 sgd_solver.cpp:112] Iteration 157200, lr = 0.001
I0523 05:53:20.671895 35003 solver.cpp:239] Iteration 157210 (2.54667 iter/s, 3.92669s/10 iters), loss = 6.0977
I0523 05:53:20.671934 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0977 (* 1 = 6.0977 loss)
I0523 05:53:20.693089 35003 sgd_solver.cpp:112] Iteration 157210, lr = 0.001
I0523 05:53:25.020675 35003 solver.cpp:239] Iteration 157220 (2.29961 iter/s, 4.34856s/10 iters), loss = 6.77152
I0523 05:53:25.020714 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77152 (* 1 = 6.77152 loss)
I0523 05:53:25.026851 35003 sgd_solver.cpp:112] Iteration 157220, lr = 0.001
I0523 05:53:28.614264 35003 solver.cpp:239] Iteration 157230 (2.78288 iter/s, 3.5934s/10 iters), loss = 6.62871
I0523 05:53:28.614307 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62871 (* 1 = 6.62871 loss)
I0523 05:53:28.622181 35003 sgd_solver.cpp:112] Iteration 157230, lr = 0.001
I0523 05:53:31.419368 35003 solver.cpp:239] Iteration 157240 (3.56514 iter/s, 2.80494s/10 iters), loss = 5.93339
I0523 05:53:31.419410 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93339 (* 1 = 5.93339 loss)
I0523 05:53:31.425616 35003 sgd_solver.cpp:112] Iteration 157240, lr = 0.001
I0523 05:53:33.673895 35003 solver.cpp:239] Iteration 157250 (4.4358 iter/s, 2.25439s/10 iters), loss = 6.44039
I0523 05:53:33.673939 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44039 (* 1 = 6.44039 loss)
I0523 05:53:33.691437 35003 sgd_solver.cpp:112] Iteration 157250, lr = 0.001
I0523 05:53:37.914027 35003 solver.cpp:239] Iteration 157260 (2.35854 iter/s, 4.23992s/10 iters), loss = 6.64147
I0523 05:53:37.914072 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64147 (* 1 = 6.64147 loss)
I0523 05:53:37.940124 35003 sgd_solver.cpp:112] Iteration 157260, lr = 0.001
I0523 05:53:42.360698 35003 solver.cpp:239] Iteration 157270 (2.24899 iter/s, 4.44644s/10 iters), loss = 5.83672
I0523 05:53:42.360746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.83672 (* 1 = 5.83672 loss)
I0523 05:53:42.374181 35003 sgd_solver.cpp:112] Iteration 157270, lr = 0.001
I0523 05:53:46.700745 35003 solver.cpp:239] Iteration 157280 (2.30425 iter/s, 4.33981s/10 iters), loss = 7.18686
I0523 05:53:46.700928 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18686 (* 1 = 7.18686 loss)
I0523 05:53:46.707990 35003 sgd_solver.cpp:112] Iteration 157280, lr = 0.001
I0523 05:53:50.167976 35003 solver.cpp:239] Iteration 157290 (2.88442 iter/s, 3.4669s/10 iters), loss = 5.90465
I0523 05:53:50.168030 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.90465 (* 1 = 5.90465 loss)
I0523 05:53:50.180532 35003 sgd_solver.cpp:112] Iteration 157290, lr = 0.001
I0523 05:53:52.283973 35003 solver.cpp:239] Iteration 157300 (4.72626 iter/s, 2.11584s/10 iters), loss = 7.12517
I0523 05:53:52.284039 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12517 (* 1 = 7.12517 loss)
I0523 05:53:53.024978 35003 sgd_solver.cpp:112] Iteration 157300, lr = 0.001
I0523 05:53:55.958912 35003 solver.cpp:239] Iteration 157310 (2.72129 iter/s, 3.67472s/10 iters), loss = 6.50409
I0523 05:53:55.958964 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50409 (* 1 = 6.50409 loss)
I0523 05:53:55.964334 35003 sgd_solver.cpp:112] Iteration 157310, lr = 0.001
I0523 05:54:00.324528 35003 solver.cpp:239] Iteration 157320 (2.29075 iter/s, 4.36539s/10 iters), loss = 5.78286
I0523 05:54:00.324565 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.78286 (* 1 = 5.78286 loss)
I0523 05:54:00.329763 35003 sgd_solver.cpp:112] Iteration 157320, lr = 0.001
I0523 05:54:03.135224 35003 solver.cpp:239] Iteration 157330 (3.55804 iter/s, 2.81053s/10 iters), loss = 6.79187
I0523 05:54:03.135267 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79187 (* 1 = 6.79187 loss)
I0523 05:54:03.277133 35003 sgd_solver.cpp:112] Iteration 157330, lr = 0.001
I0523 05:54:07.163172 35003 solver.cpp:239] Iteration 157340 (2.48278 iter/s, 4.02774s/10 iters), loss = 7.85661
I0523 05:54:07.163214 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85661 (* 1 = 7.85661 loss)
I0523 05:54:07.176280 35003 sgd_solver.cpp:112] Iteration 157340, lr = 0.001
I0523 05:54:09.262181 35003 solver.cpp:239] Iteration 157350 (4.76446 iter/s, 2.09887s/10 iters), loss = 6.95254
I0523 05:54:09.262224 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95254 (* 1 = 6.95254 loss)
I0523 05:54:09.270481 35003 sgd_solver.cpp:112] Iteration 157350, lr = 0.001
I0523 05:54:14.056191 35003 solver.cpp:239] Iteration 157360 (2.08605 iter/s, 4.79374s/10 iters), loss = 6.52532
I0523 05:54:14.056253 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52532 (* 1 = 6.52532 loss)
I0523 05:54:14.775688 35003 sgd_solver.cpp:112] Iteration 157360, lr = 0.001
I0523 05:54:16.812376 35003 solver.cpp:239] Iteration 157370 (3.62843 iter/s, 2.75601s/10 iters), loss = 6.06233
I0523 05:54:16.812593 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06233 (* 1 = 6.06233 loss)
I0523 05:54:16.825199 35003 sgd_solver.cpp:112] Iteration 157370, lr = 0.001
I0523 05:54:21.287148 35003 solver.cpp:239] Iteration 157380 (2.23494 iter/s, 4.4744s/10 iters), loss = 6.82034
I0523 05:54:21.287190 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82034 (* 1 = 6.82034 loss)
I0523 05:54:21.295135 35003 sgd_solver.cpp:112] Iteration 157380, lr = 0.001
I0523 05:54:24.167878 35003 solver.cpp:239] Iteration 157390 (3.47154 iter/s, 2.88056s/10 iters), loss = 6.86525
I0523 05:54:24.167929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86525 (* 1 = 6.86525 loss)
I0523 05:54:24.180984 35003 sgd_solver.cpp:112] Iteration 157390, lr = 0.001
I0523 05:54:27.579464 35003 solver.cpp:239] Iteration 157400 (2.93135 iter/s, 3.4114s/10 iters), loss = 6.51726
I0523 05:54:27.579509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51726 (* 1 = 6.51726 loss)
I0523 05:54:28.320192 35003 sgd_solver.cpp:112] Iteration 157400, lr = 0.001
I0523 05:54:31.135321 35003 solver.cpp:239] Iteration 157410 (2.81242 iter/s, 3.55566s/10 iters), loss = 5.75322
I0523 05:54:31.135372 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.75322 (* 1 = 5.75322 loss)
I0523 05:54:31.843649 35003 sgd_solver.cpp:112] Iteration 157410, lr = 0.001
I0523 05:54:34.797978 35003 solver.cpp:239] Iteration 157420 (2.73041 iter/s, 3.66245s/10 iters), loss = 7.05163
I0523 05:54:34.798032 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05163 (* 1 = 7.05163 loss)
I0523 05:54:35.418216 35003 sgd_solver.cpp:112] Iteration 157420, lr = 0.001
I0523 05:54:39.026438 35003 solver.cpp:239] Iteration 157430 (2.36505 iter/s, 4.22823s/10 iters), loss = 7.35136
I0523 05:54:39.026480 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35136 (* 1 = 7.35136 loss)
I0523 05:54:39.039438 35003 sgd_solver.cpp:112] Iteration 157430, lr = 0.001
I0523 05:54:41.828500 35003 solver.cpp:239] Iteration 157440 (3.56901 iter/s, 2.8019s/10 iters), loss = 6.77572
I0523 05:54:41.828538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77572 (* 1 = 6.77572 loss)
I0523 05:54:41.842406 35003 sgd_solver.cpp:112] Iteration 157440, lr = 0.001
I0523 05:54:44.686311 35003 solver.cpp:239] Iteration 157450 (3.49938 iter/s, 2.85765s/10 iters), loss = 6.04129
I0523 05:54:44.686352 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04129 (* 1 = 6.04129 loss)
I0523 05:54:44.694334 35003 sgd_solver.cpp:112] Iteration 157450, lr = 0.001
I0523 05:54:48.081109 35003 solver.cpp:239] Iteration 157460 (2.94585 iter/s, 3.3946s/10 iters), loss = 6.85046
I0523 05:54:48.081357 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85046 (* 1 = 6.85046 loss)
I0523 05:54:48.105571 35003 sgd_solver.cpp:112] Iteration 157460, lr = 0.001
I0523 05:54:51.128403 35003 solver.cpp:239] Iteration 157470 (3.28199 iter/s, 3.04694s/10 iters), loss = 6.74493
I0523 05:54:51.128451 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74493 (* 1 = 6.74493 loss)
I0523 05:54:51.141651 35003 sgd_solver.cpp:112] Iteration 157470, lr = 0.001
I0523 05:54:53.006057 35003 solver.cpp:239] Iteration 157480 (5.32618 iter/s, 1.87752s/10 iters), loss = 6.64044
I0523 05:54:53.006095 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64044 (* 1 = 6.64044 loss)
I0523 05:54:53.026188 35003 sgd_solver.cpp:112] Iteration 157480, lr = 0.001
I0523 05:54:55.204098 35003 solver.cpp:239] Iteration 157490 (4.54978 iter/s, 2.19791s/10 iters), loss = 6.66257
I0523 05:54:55.204151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66257 (* 1 = 6.66257 loss)
I0523 05:54:55.210224 35003 sgd_solver.cpp:112] Iteration 157490, lr = 0.001
I0523 05:54:57.982246 35003 solver.cpp:239] Iteration 157500 (3.59974 iter/s, 2.77798s/10 iters), loss = 7.13199
I0523 05:54:57.982293 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13199 (* 1 = 7.13199 loss)
I0523 05:54:57.987918 35003 sgd_solver.cpp:112] Iteration 157500, lr = 0.001
I0523 05:55:02.203752 35003 solver.cpp:239] Iteration 157510 (2.36894 iter/s, 4.22129s/10 iters), loss = 6.62881
I0523 05:55:02.203788 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62881 (* 1 = 6.62881 loss)
I0523 05:55:02.216701 35003 sgd_solver.cpp:112] Iteration 157510, lr = 0.001
I0523 05:55:05.524616 35003 solver.cpp:239] Iteration 157520 (3.01143 iter/s, 3.32068s/10 iters), loss = 6.13188
I0523 05:55:05.524653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13188 (* 1 = 6.13188 loss)
I0523 05:55:05.537371 35003 sgd_solver.cpp:112] Iteration 157520, lr = 0.001
I0523 05:55:08.413431 35003 solver.cpp:239] Iteration 157530 (3.46184 iter/s, 2.88864s/10 iters), loss = 6.89019
I0523 05:55:08.413491 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89019 (* 1 = 6.89019 loss)
I0523 05:55:08.417798 35003 sgd_solver.cpp:112] Iteration 157530, lr = 0.001
I0523 05:55:13.019222 35003 solver.cpp:239] Iteration 157540 (2.1713 iter/s, 4.60554s/10 iters), loss = 8.00019
I0523 05:55:13.019268 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00019 (* 1 = 8.00019 loss)
I0523 05:55:13.032801 35003 sgd_solver.cpp:112] Iteration 157540, lr = 0.001
I0523 05:55:17.144757 35003 solver.cpp:239] Iteration 157550 (2.42406 iter/s, 4.12531s/10 iters), loss = 6.98562
I0523 05:55:17.144794 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98562 (* 1 = 6.98562 loss)
I0523 05:55:17.425202 35003 sgd_solver.cpp:112] Iteration 157550, lr = 0.001
I0523 05:55:21.523769 35003 solver.cpp:239] Iteration 157560 (2.28373 iter/s, 4.37879s/10 iters), loss = 7.66871
I0523 05:55:21.524045 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66871 (* 1 = 7.66871 loss)
I0523 05:55:21.542488 35003 sgd_solver.cpp:112] Iteration 157560, lr = 0.001
I0523 05:55:24.845865 35003 solver.cpp:239] Iteration 157570 (3.0105 iter/s, 3.32171s/10 iters), loss = 5.61351
I0523 05:55:24.845912 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.61351 (* 1 = 5.61351 loss)
I0523 05:55:24.851529 35003 sgd_solver.cpp:112] Iteration 157570, lr = 0.001
I0523 05:55:29.158929 35003 solver.cpp:239] Iteration 157580 (2.31866 iter/s, 4.31283s/10 iters), loss = 6.48887
I0523 05:55:29.158991 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48887 (* 1 = 6.48887 loss)
I0523 05:55:29.880379 35003 sgd_solver.cpp:112] Iteration 157580, lr = 0.001
I0523 05:55:32.806078 35003 solver.cpp:239] Iteration 157590 (2.74203 iter/s, 3.64694s/10 iters), loss = 6.75725
I0523 05:55:32.806123 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75725 (* 1 = 6.75725 loss)
I0523 05:55:32.812146 35003 sgd_solver.cpp:112] Iteration 157590, lr = 0.001
I0523 05:55:37.034256 35003 solver.cpp:239] Iteration 157600 (2.36521 iter/s, 4.22796s/10 iters), loss = 8.13682
I0523 05:55:37.034294 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.13682 (* 1 = 8.13682 loss)
I0523 05:55:37.754400 35003 sgd_solver.cpp:112] Iteration 157600, lr = 0.001
I0523 05:55:42.127828 35003 solver.cpp:239] Iteration 157610 (1.96335 iter/s, 5.09333s/10 iters), loss = 7.10684
I0523 05:55:42.127863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10684 (* 1 = 7.10684 loss)
I0523 05:55:42.145615 35003 sgd_solver.cpp:112] Iteration 157610, lr = 0.001
I0523 05:55:44.795002 35003 solver.cpp:239] Iteration 157620 (3.7495 iter/s, 2.66702s/10 iters), loss = 7.687
I0523 05:55:44.795054 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.687 (* 1 = 7.687 loss)
I0523 05:55:45.226408 35003 sgd_solver.cpp:112] Iteration 157620, lr = 0.001
I0523 05:55:48.341717 35003 solver.cpp:239] Iteration 157630 (2.81967 iter/s, 3.54652s/10 iters), loss = 6.73411
I0523 05:55:48.341755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73411 (* 1 = 6.73411 loss)
I0523 05:55:48.370885 35003 sgd_solver.cpp:112] Iteration 157630, lr = 0.001
I0523 05:55:51.724516 35003 solver.cpp:239] Iteration 157640 (2.95629 iter/s, 3.38262s/10 iters), loss = 6.12191
I0523 05:55:51.728436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12191 (* 1 = 6.12191 loss)
I0523 05:55:51.728467 35003 sgd_solver.cpp:112] Iteration 157640, lr = 0.001
I0523 05:55:56.129514 35003 solver.cpp:239] Iteration 157650 (2.27231 iter/s, 4.40081s/10 iters), loss = 7.28482
I0523 05:55:56.129560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28482 (* 1 = 7.28482 loss)
I0523 05:55:56.146224 35003 sgd_solver.cpp:112] Iteration 157650, lr = 0.001
I0523 05:55:59.792625 35003 solver.cpp:239] Iteration 157660 (2.73007 iter/s, 3.66292s/10 iters), loss = 6.90252
I0523 05:55:59.792670 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90252 (* 1 = 6.90252 loss)
I0523 05:56:00.500349 35003 sgd_solver.cpp:112] Iteration 157660, lr = 0.001
I0523 05:56:03.249002 35003 solver.cpp:239] Iteration 157670 (2.89336 iter/s, 3.45618s/10 iters), loss = 6.70181
I0523 05:56:03.249056 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70181 (* 1 = 6.70181 loss)
I0523 05:56:03.257563 35003 sgd_solver.cpp:112] Iteration 157670, lr = 0.001
I0523 05:56:05.324134 35003 solver.cpp:239] Iteration 157680 (4.81931 iter/s, 2.07499s/10 iters), loss = 6.12855
I0523 05:56:05.324180 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12855 (* 1 = 6.12855 loss)
I0523 05:56:05.346716 35003 sgd_solver.cpp:112] Iteration 157680, lr = 0.001
I0523 05:56:08.200479 35003 solver.cpp:239] Iteration 157690 (3.47702 iter/s, 2.87603s/10 iters), loss = 6.00766
I0523 05:56:08.200521 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00766 (* 1 = 6.00766 loss)
I0523 05:56:08.213541 35003 sgd_solver.cpp:112] Iteration 157690, lr = 0.001
I0523 05:56:09.557313 35003 solver.cpp:239] Iteration 157700 (7.3707 iter/s, 1.35672s/10 iters), loss = 6.82132
I0523 05:56:09.557358 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82132 (* 1 = 6.82132 loss)
I0523 05:56:10.265633 35003 sgd_solver.cpp:112] Iteration 157700, lr = 0.001
I0523 05:56:13.242259 35003 solver.cpp:239] Iteration 157710 (2.71389 iter/s, 3.68475s/10 iters), loss = 7.53581
I0523 05:56:13.242305 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53581 (* 1 = 7.53581 loss)
I0523 05:56:13.255221 35003 sgd_solver.cpp:112] Iteration 157710, lr = 0.001
I0523 05:56:17.645833 35003 solver.cpp:239] Iteration 157720 (2.271 iter/s, 4.40334s/10 iters), loss = 7.24435
I0523 05:56:17.645881 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24435 (* 1 = 7.24435 loss)
I0523 05:56:17.659008 35003 sgd_solver.cpp:112] Iteration 157720, lr = 0.001
I0523 05:56:20.326905 35003 solver.cpp:239] Iteration 157730 (3.73009 iter/s, 2.6809s/10 iters), loss = 7.29311
I0523 05:56:20.326969 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29311 (* 1 = 7.29311 loss)
I0523 05:56:20.335618 35003 sgd_solver.cpp:112] Iteration 157730, lr = 0.001
I0523 05:56:21.564632 35003 solver.cpp:239] Iteration 157740 (8.0801 iter/s, 1.23761s/10 iters), loss = 6.81335
I0523 05:56:21.564690 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81335 (* 1 = 6.81335 loss)
I0523 05:56:21.573930 35003 sgd_solver.cpp:112] Iteration 157740, lr = 0.001
I0523 05:56:22.691980 35003 solver.cpp:239] Iteration 157750 (8.87129 iter/s, 1.12723s/10 iters), loss = 5.91923
I0523 05:56:22.692278 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91923 (* 1 = 5.91923 loss)
I0523 05:56:23.350803 35003 sgd_solver.cpp:112] Iteration 157750, lr = 0.001
I0523 05:56:28.442811 35003 solver.cpp:239] Iteration 157760 (1.73903 iter/s, 5.75032s/10 iters), loss = 6.80729
I0523 05:56:28.442848 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80729 (* 1 = 6.80729 loss)
I0523 05:56:28.456418 35003 sgd_solver.cpp:112] Iteration 157760, lr = 0.001
I0523 05:56:32.729789 35003 solver.cpp:239] Iteration 157770 (2.33276 iter/s, 4.28677s/10 iters), loss = 7.24809
I0523 05:56:32.729835 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24809 (* 1 = 7.24809 loss)
I0523 05:56:33.470317 35003 sgd_solver.cpp:112] Iteration 157770, lr = 0.001
I0523 05:56:35.571934 35003 solver.cpp:239] Iteration 157780 (3.51867 iter/s, 2.84198s/10 iters), loss = 7.03233
I0523 05:56:35.571985 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03233 (* 1 = 7.03233 loss)
I0523 05:56:36.211668 35003 sgd_solver.cpp:112] Iteration 157780, lr = 0.001
I0523 05:56:39.184015 35003 solver.cpp:239] Iteration 157790 (2.76865 iter/s, 3.61187s/10 iters), loss = 7.21462
I0523 05:56:39.184077 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21462 (* 1 = 7.21462 loss)
I0523 05:56:39.918417 35003 sgd_solver.cpp:112] Iteration 157790, lr = 0.001
I0523 05:56:44.087466 35003 solver.cpp:239] Iteration 157800 (2.03949 iter/s, 4.90319s/10 iters), loss = 6.45225
I0523 05:56:44.087527 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45225 (* 1 = 6.45225 loss)
I0523 05:56:44.094807 35003 sgd_solver.cpp:112] Iteration 157800, lr = 0.001
I0523 05:56:46.188746 35003 solver.cpp:239] Iteration 157810 (4.75934 iter/s, 2.10113s/10 iters), loss = 7.28913
I0523 05:56:46.188786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28913 (* 1 = 7.28913 loss)
I0523 05:56:46.208958 35003 sgd_solver.cpp:112] Iteration 157810, lr = 0.001
I0523 05:56:48.258638 35003 solver.cpp:239] Iteration 157820 (4.83147 iter/s, 2.06976s/10 iters), loss = 5.87546
I0523 05:56:48.258677 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87546 (* 1 = 5.87546 loss)
I0523 05:56:48.271790 35003 sgd_solver.cpp:112] Iteration 157820, lr = 0.001
I0523 05:56:51.080899 35003 solver.cpp:239] Iteration 157830 (3.54346 iter/s, 2.8221s/10 iters), loss = 6.15454
I0523 05:56:51.080935 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15454 (* 1 = 6.15454 loss)
I0523 05:56:51.093984 35003 sgd_solver.cpp:112] Iteration 157830, lr = 0.001
I0523 05:56:55.442198 35003 solver.cpp:239] Iteration 157840 (2.29301 iter/s, 4.36108s/10 iters), loss = 6.48557
I0523 05:56:55.442528 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48557 (* 1 = 6.48557 loss)
I0523 05:56:55.449177 35003 sgd_solver.cpp:112] Iteration 157840, lr = 0.001
I0523 05:56:58.487660 35003 solver.cpp:239] Iteration 157850 (3.28403 iter/s, 3.04504s/10 iters), loss = 6.53459
I0523 05:56:58.487699 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53459 (* 1 = 6.53459 loss)
I0523 05:56:58.492667 35003 sgd_solver.cpp:112] Iteration 157850, lr = 0.001
I0523 05:57:00.773360 35003 solver.cpp:239] Iteration 157860 (4.37532 iter/s, 2.28555s/10 iters), loss = 6.70312
I0523 05:57:00.773421 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70312 (* 1 = 6.70312 loss)
I0523 05:57:00.776875 35003 sgd_solver.cpp:112] Iteration 157860, lr = 0.001
I0523 05:57:03.541322 35003 solver.cpp:239] Iteration 157870 (3.61301 iter/s, 2.76778s/10 iters), loss = 6.30155
I0523 05:57:03.541371 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30155 (* 1 = 6.30155 loss)
I0523 05:57:03.554455 35003 sgd_solver.cpp:112] Iteration 157870, lr = 0.001
I0523 05:57:05.620931 35003 solver.cpp:239] Iteration 157880 (4.80892 iter/s, 2.07947s/10 iters), loss = 8.60102
I0523 05:57:05.620980 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.60102 (* 1 = 8.60102 loss)
I0523 05:57:05.624756 35003 sgd_solver.cpp:112] Iteration 157880, lr = 0.001
I0523 05:57:08.096457 35003 solver.cpp:239] Iteration 157890 (4.03981 iter/s, 2.47536s/10 iters), loss = 7.18095
I0523 05:57:08.096506 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18095 (* 1 = 7.18095 loss)
I0523 05:57:08.109663 35003 sgd_solver.cpp:112] Iteration 157890, lr = 0.001
I0523 05:57:10.930073 35003 solver.cpp:239] Iteration 157900 (3.52927 iter/s, 2.83344s/10 iters), loss = 7.52032
I0523 05:57:10.930119 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52032 (* 1 = 7.52032 loss)
I0523 05:57:10.937080 35003 sgd_solver.cpp:112] Iteration 157900, lr = 0.001
I0523 05:57:12.273303 35003 solver.cpp:239] Iteration 157910 (7.44543 iter/s, 1.3431s/10 iters), loss = 7.16882
I0523 05:57:12.273360 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16882 (* 1 = 7.16882 loss)
I0523 05:57:12.435153 35003 sgd_solver.cpp:112] Iteration 157910, lr = 0.001
I0523 05:57:14.666998 35003 solver.cpp:239] Iteration 157920 (4.17792 iter/s, 2.39354s/10 iters), loss = 5.96727
I0523 05:57:14.667035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96727 (* 1 = 5.96727 loss)
I0523 05:57:14.691236 35003 sgd_solver.cpp:112] Iteration 157920, lr = 0.001
I0523 05:57:18.245755 35003 solver.cpp:239] Iteration 157930 (2.79442 iter/s, 3.57857s/10 iters), loss = 6.08768
I0523 05:57:18.245823 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08768 (* 1 = 6.08768 loss)
I0523 05:57:18.980146 35003 sgd_solver.cpp:112] Iteration 157930, lr = 0.001
I0523 05:57:22.398062 35003 solver.cpp:239] Iteration 157940 (2.40844 iter/s, 4.15206s/10 iters), loss = 7.86014
I0523 05:57:22.398111 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86014 (* 1 = 7.86014 loss)
I0523 05:57:22.401249 35003 sgd_solver.cpp:112] Iteration 157940, lr = 0.001
I0523 05:57:24.850420 35003 solver.cpp:239] Iteration 157950 (4.07798 iter/s, 2.4522s/10 iters), loss = 6.91059
I0523 05:57:24.850472 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91059 (* 1 = 6.91059 loss)
I0523 05:57:24.859863 35003 sgd_solver.cpp:112] Iteration 157950, lr = 0.001
I0523 05:57:28.993285 35003 solver.cpp:239] Iteration 157960 (2.41392 iter/s, 4.14264s/10 iters), loss = 7.11886
I0523 05:57:28.993561 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11886 (* 1 = 7.11886 loss)
I0523 05:57:29.006659 35003 sgd_solver.cpp:112] Iteration 157960, lr = 0.001
I0523 05:57:32.619141 35003 solver.cpp:239] Iteration 157970 (2.75828 iter/s, 3.62545s/10 iters), loss = 5.77238
I0523 05:57:32.619185 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77238 (* 1 = 5.77238 loss)
I0523 05:57:33.321096 35003 sgd_solver.cpp:112] Iteration 157970, lr = 0.001
I0523 05:57:36.866719 35003 solver.cpp:239] Iteration 157980 (2.35441 iter/s, 4.24735s/10 iters), loss = 8.17744
I0523 05:57:36.866770 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17744 (* 1 = 8.17744 loss)
I0523 05:57:37.595940 35003 sgd_solver.cpp:112] Iteration 157980, lr = 0.001
I0523 05:57:40.534076 35003 solver.cpp:239] Iteration 157990 (2.72691 iter/s, 3.66715s/10 iters), loss = 6.9996
I0523 05:57:40.534126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9996 (* 1 = 6.9996 loss)
I0523 05:57:41.273247 35003 sgd_solver.cpp:112] Iteration 157990, lr = 0.001
I0523 05:57:44.037434 35003 solver.cpp:239] Iteration 158000 (2.85457 iter/s, 3.50315s/10 iters), loss = 7.32164
I0523 05:57:44.037492 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32164 (* 1 = 7.32164 loss)
I0523 05:57:44.745688 35003 sgd_solver.cpp:112] Iteration 158000, lr = 0.001
I0523 05:57:49.729202 35003 solver.cpp:239] Iteration 158010 (1.75701 iter/s, 5.69148s/10 iters), loss = 5.285
I0523 05:57:49.729254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.285 (* 1 = 5.285 loss)
I0523 05:57:50.470099 35003 sgd_solver.cpp:112] Iteration 158010, lr = 0.001
I0523 05:57:55.348423 35003 solver.cpp:239] Iteration 158020 (1.77969 iter/s, 5.61894s/10 iters), loss = 6.50403
I0523 05:57:55.348469 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50403 (* 1 = 6.50403 loss)
I0523 05:57:55.352957 35003 sgd_solver.cpp:112] Iteration 158020, lr = 0.001
I0523 05:57:59.438313 35003 solver.cpp:239] Iteration 158030 (2.44518 iter/s, 4.08968s/10 iters), loss = 6.82043
I0523 05:57:59.438560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82043 (* 1 = 6.82043 loss)
I0523 05:58:00.179677 35003 sgd_solver.cpp:112] Iteration 158030, lr = 0.001
I0523 05:58:03.075539 35003 solver.cpp:239] Iteration 158040 (2.74962 iter/s, 3.63686s/10 iters), loss = 6.11486
I0523 05:58:03.075583 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11486 (* 1 = 6.11486 loss)
I0523 05:58:03.809595 35003 sgd_solver.cpp:112] Iteration 158040, lr = 0.001
I0523 05:58:07.990046 35003 solver.cpp:239] Iteration 158050 (2.0349 iter/s, 4.91425s/10 iters), loss = 7.13209
I0523 05:58:07.990090 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13209 (* 1 = 7.13209 loss)
I0523 05:58:07.993499 35003 sgd_solver.cpp:112] Iteration 158050, lr = 0.001
I0523 05:58:11.619849 35003 solver.cpp:239] Iteration 158060 (2.75512 iter/s, 3.6296s/10 iters), loss = 7.45688
I0523 05:58:11.619910 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45688 (* 1 = 7.45688 loss)
I0523 05:58:11.628592 35003 sgd_solver.cpp:112] Iteration 158060, lr = 0.001
I0523 05:58:14.525290 35003 solver.cpp:239] Iteration 158070 (3.44203 iter/s, 2.90526s/10 iters), loss = 7.24573
I0523 05:58:14.525332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24573 (* 1 = 7.24573 loss)
I0523 05:58:15.259686 35003 sgd_solver.cpp:112] Iteration 158070, lr = 0.001
I0523 05:58:17.287933 35003 solver.cpp:239] Iteration 158080 (3.61994 iter/s, 2.76248s/10 iters), loss = 5.77914
I0523 05:58:17.287982 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77914 (* 1 = 5.77914 loss)
I0523 05:58:17.300556 35003 sgd_solver.cpp:112] Iteration 158080, lr = 0.001
I0523 05:58:20.446784 35003 solver.cpp:239] Iteration 158090 (3.17026 iter/s, 3.15432s/10 iters), loss = 6.94845
I0523 05:58:20.446822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94845 (* 1 = 6.94845 loss)
I0523 05:58:20.461684 35003 sgd_solver.cpp:112] Iteration 158090, lr = 0.001
I0523 05:58:24.044955 35003 solver.cpp:239] Iteration 158100 (2.77934 iter/s, 3.59798s/10 iters), loss = 6.02952
I0523 05:58:24.045001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02952 (* 1 = 6.02952 loss)
I0523 05:58:24.063946 35003 sgd_solver.cpp:112] Iteration 158100, lr = 0.001
I0523 05:58:27.015306 35003 solver.cpp:239] Iteration 158110 (3.3668 iter/s, 2.97018s/10 iters), loss = 7.58612
I0523 05:58:27.015352 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58612 (* 1 = 7.58612 loss)
I0523 05:58:27.028604 35003 sgd_solver.cpp:112] Iteration 158110, lr = 0.001
I0523 05:58:31.196403 35003 solver.cpp:239] Iteration 158120 (2.39184 iter/s, 4.18088s/10 iters), loss = 6.26408
I0523 05:58:31.196665 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26408 (* 1 = 6.26408 loss)
I0523 05:58:31.209599 35003 sgd_solver.cpp:112] Iteration 158120, lr = 0.001
I0523 05:58:33.271114 35003 solver.cpp:239] Iteration 158130 (4.8207 iter/s, 2.07439s/10 iters), loss = 6.90536
I0523 05:58:33.271155 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90536 (* 1 = 6.90536 loss)
I0523 05:58:33.277859 35003 sgd_solver.cpp:112] Iteration 158130, lr = 0.001
I0523 05:58:37.810083 35003 solver.cpp:239] Iteration 158140 (2.20341 iter/s, 4.53843s/10 iters), loss = 5.95424
I0523 05:58:37.810142 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95424 (* 1 = 5.95424 loss)
I0523 05:58:38.531522 35003 sgd_solver.cpp:112] Iteration 158140, lr = 0.001
I0523 05:58:41.414912 35003 solver.cpp:239] Iteration 158150 (2.77422 iter/s, 3.60462s/10 iters), loss = 6.68258
I0523 05:58:41.414963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68258 (* 1 = 6.68258 loss)
I0523 05:58:42.110736 35003 sgd_solver.cpp:112] Iteration 158150, lr = 0.001
I0523 05:58:44.231730 35003 solver.cpp:239] Iteration 158160 (3.55033 iter/s, 2.81664s/10 iters), loss = 6.94452
I0523 05:58:44.231779 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94452 (* 1 = 6.94452 loss)
I0523 05:58:44.239994 35003 sgd_solver.cpp:112] Iteration 158160, lr = 0.001
I0523 05:58:49.178797 35003 solver.cpp:239] Iteration 158170 (2.02151 iter/s, 4.9468s/10 iters), loss = 6.0443
I0523 05:58:49.178869 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0443 (* 1 = 6.0443 loss)
I0523 05:58:49.425947 35003 sgd_solver.cpp:112] Iteration 158170, lr = 0.001
I0523 05:58:52.858150 35003 solver.cpp:239] Iteration 158180 (2.71803 iter/s, 3.67913s/10 iters), loss = 6.48166
I0523 05:58:52.858196 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48166 (* 1 = 6.48166 loss)
I0523 05:58:52.870528 35003 sgd_solver.cpp:112] Iteration 158180, lr = 0.001
I0523 05:58:55.970407 35003 solver.cpp:239] Iteration 158190 (3.21329 iter/s, 3.11208s/10 iters), loss = 7.91036
I0523 05:58:55.970448 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.91036 (* 1 = 7.91036 loss)
I0523 05:58:55.983366 35003 sgd_solver.cpp:112] Iteration 158190, lr = 0.001
I0523 05:59:00.215771 35003 solver.cpp:239] Iteration 158200 (2.35563 iter/s, 4.24515s/10 iters), loss = 6.5669
I0523 05:59:00.215808 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5669 (* 1 = 6.5669 loss)
I0523 05:59:00.234264 35003 sgd_solver.cpp:112] Iteration 158200, lr = 0.001
I0523 05:59:03.310115 35003 solver.cpp:239] Iteration 158210 (3.23188 iter/s, 3.09417s/10 iters), loss = 7.38843
I0523 05:59:03.310338 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38843 (* 1 = 7.38843 loss)
I0523 05:59:03.323293 35003 sgd_solver.cpp:112] Iteration 158210, lr = 0.001
I0523 05:59:06.795882 35003 solver.cpp:239] Iteration 158220 (2.86909 iter/s, 3.48543s/10 iters), loss = 6.18154
I0523 05:59:06.795933 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18154 (* 1 = 6.18154 loss)
I0523 05:59:06.953171 35003 sgd_solver.cpp:112] Iteration 158220, lr = 0.001
I0523 05:59:10.676749 35003 solver.cpp:239] Iteration 158230 (2.57688 iter/s, 3.88066s/10 iters), loss = 6.86446
I0523 05:59:10.676800 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86446 (* 1 = 6.86446 loss)
I0523 05:59:11.417764 35003 sgd_solver.cpp:112] Iteration 158230, lr = 0.001
I0523 05:59:15.338927 35003 solver.cpp:239] Iteration 158240 (2.14503 iter/s, 4.66194s/10 iters), loss = 6.24636
I0523 05:59:15.338974 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24636 (* 1 = 6.24636 loss)
I0523 05:59:15.350549 35003 sgd_solver.cpp:112] Iteration 158240, lr = 0.001
I0523 05:59:20.238692 35003 solver.cpp:239] Iteration 158250 (2.04102 iter/s, 4.89952s/10 iters), loss = 4.60465
I0523 05:59:20.238760 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.60465 (* 1 = 4.60465 loss)
I0523 05:59:20.251067 35003 sgd_solver.cpp:112] Iteration 158250, lr = 0.001
I0523 05:59:23.249819 35003 solver.cpp:239] Iteration 158260 (3.32123 iter/s, 3.01093s/10 iters), loss = 7.18316
I0523 05:59:23.249861 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18316 (* 1 = 7.18316 loss)
I0523 05:59:23.965550 35003 sgd_solver.cpp:112] Iteration 158260, lr = 0.001
I0523 05:59:26.926760 35003 solver.cpp:239] Iteration 158270 (2.71979 iter/s, 3.67675s/10 iters), loss = 6.51727
I0523 05:59:26.926800 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51727 (* 1 = 6.51727 loss)
I0523 05:59:26.939745 35003 sgd_solver.cpp:112] Iteration 158270, lr = 0.001
I0523 05:59:30.507105 35003 solver.cpp:239] Iteration 158280 (2.79317 iter/s, 3.58016s/10 iters), loss = 6.22493
I0523 05:59:30.507151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22493 (* 1 = 6.22493 loss)
I0523 05:59:31.241475 35003 sgd_solver.cpp:112] Iteration 158280, lr = 0.001
I0523 05:59:35.486630 35003 solver.cpp:239] Iteration 158290 (2.00833 iter/s, 4.97927s/10 iters), loss = 6.22827
I0523 05:59:35.486824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22827 (* 1 = 6.22827 loss)
I0523 05:59:36.214081 35003 sgd_solver.cpp:112] Iteration 158290, lr = 0.001
I0523 05:59:41.235479 35003 solver.cpp:239] Iteration 158300 (1.73961 iter/s, 5.74842s/10 iters), loss = 6.59182
I0523 05:59:41.235541 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59182 (* 1 = 6.59182 loss)
I0523 05:59:41.239558 35003 sgd_solver.cpp:112] Iteration 158300, lr = 0.001
I0523 05:59:43.153153 35003 solver.cpp:239] Iteration 158310 (5.21504 iter/s, 1.91753s/10 iters), loss = 6.55937
I0523 05:59:43.153192 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55937 (* 1 = 6.55937 loss)
I0523 05:59:43.858563 35003 sgd_solver.cpp:112] Iteration 158310, lr = 0.001
I0523 05:59:46.714795 35003 solver.cpp:239] Iteration 158320 (2.80784 iter/s, 3.56145s/10 iters), loss = 7.22202
I0523 05:59:46.714835 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22202 (* 1 = 7.22202 loss)
I0523 05:59:47.430248 35003 sgd_solver.cpp:112] Iteration 158320, lr = 0.001
I0523 05:59:51.447650 35003 solver.cpp:239] Iteration 158330 (2.113 iter/s, 4.73262s/10 iters), loss = 7.02123
I0523 05:59:51.447700 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02123 (* 1 = 7.02123 loss)
I0523 05:59:52.143673 35003 sgd_solver.cpp:112] Iteration 158330, lr = 0.001
I0523 05:59:55.762387 35003 solver.cpp:239] Iteration 158340 (2.31776 iter/s, 4.31451s/10 iters), loss = 6.80846
I0523 05:59:55.762431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80846 (* 1 = 6.80846 loss)
I0523 05:59:56.484539 35003 sgd_solver.cpp:112] Iteration 158340, lr = 0.001
I0523 05:59:59.005043 35003 solver.cpp:239] Iteration 158350 (3.08406 iter/s, 3.24248s/10 iters), loss = 5.88655
I0523 05:59:59.005080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.88655 (* 1 = 5.88655 loss)
I0523 05:59:59.010257 35003 sgd_solver.cpp:112] Iteration 158350, lr = 0.001
I0523 06:00:02.639940 35003 solver.cpp:239] Iteration 158360 (2.75126 iter/s, 3.6347s/10 iters), loss = 6.63978
I0523 06:00:02.639986 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63978 (* 1 = 6.63978 loss)
I0523 06:00:02.661833 35003 sgd_solver.cpp:112] Iteration 158360, lr = 0.001
I0523 06:00:06.506494 35003 solver.cpp:239] Iteration 158370 (2.58642 iter/s, 3.86635s/10 iters), loss = 7.36911
I0523 06:00:06.506708 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36911 (* 1 = 7.36911 loss)
I0523 06:00:07.248075 35003 sgd_solver.cpp:112] Iteration 158370, lr = 0.001
I0523 06:00:10.106642 35003 solver.cpp:239] Iteration 158380 (2.77794 iter/s, 3.59979s/10 iters), loss = 6.35408
I0523 06:00:10.106686 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35408 (* 1 = 6.35408 loss)
I0523 06:00:10.847337 35003 sgd_solver.cpp:112] Iteration 158380, lr = 0.001
I0523 06:00:14.390648 35003 solver.cpp:239] Iteration 158390 (2.33438 iter/s, 4.28378s/10 iters), loss = 6.69557
I0523 06:00:14.390692 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69557 (* 1 = 6.69557 loss)
I0523 06:00:15.099604 35003 sgd_solver.cpp:112] Iteration 158390, lr = 0.001
I0523 06:00:18.442822 35003 solver.cpp:239] Iteration 158400 (2.46794 iter/s, 4.05196s/10 iters), loss = 8.23573
I0523 06:00:18.442862 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.23573 (* 1 = 8.23573 loss)
I0523 06:00:18.455924 35003 sgd_solver.cpp:112] Iteration 158400, lr = 0.001
I0523 06:00:22.712481 35003 solver.cpp:239] Iteration 158410 (2.34223 iter/s, 4.26944s/10 iters), loss = 6.83977
I0523 06:00:22.712522 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83977 (* 1 = 6.83977 loss)
I0523 06:00:22.726063 35003 sgd_solver.cpp:112] Iteration 158410, lr = 0.001
I0523 06:00:26.279682 35003 solver.cpp:239] Iteration 158420 (2.80347 iter/s, 3.56701s/10 iters), loss = 6.64199
I0523 06:00:26.279726 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64199 (* 1 = 6.64199 loss)
I0523 06:00:27.009102 35003 sgd_solver.cpp:112] Iteration 158420, lr = 0.001
I0523 06:00:29.140322 35003 solver.cpp:239] Iteration 158430 (3.49593 iter/s, 2.86047s/10 iters), loss = 7.78351
I0523 06:00:29.140375 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78351 (* 1 = 7.78351 loss)
I0523 06:00:29.862579 35003 sgd_solver.cpp:112] Iteration 158430, lr = 0.001
I0523 06:00:32.449023 35003 solver.cpp:239] Iteration 158440 (3.02252 iter/s, 3.3085s/10 iters), loss = 6.01738
I0523 06:00:32.449090 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01738 (* 1 = 6.01738 loss)
I0523 06:00:32.462232 35003 sgd_solver.cpp:112] Iteration 158440, lr = 0.001
I0523 06:00:35.556072 35003 solver.cpp:239] Iteration 158450 (3.21869 iter/s, 3.10685s/10 iters), loss = 6.34878
I0523 06:00:35.556124 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34878 (* 1 = 6.34878 loss)
I0523 06:00:35.562903 35003 sgd_solver.cpp:112] Iteration 158450, lr = 0.001
I0523 06:00:38.984199 35003 solver.cpp:239] Iteration 158460 (2.91721 iter/s, 3.42793s/10 iters), loss = 6.54706
I0523 06:00:38.984380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54706 (* 1 = 6.54706 loss)
I0523 06:00:38.997479 35003 sgd_solver.cpp:112] Iteration 158460, lr = 0.001
I0523 06:00:41.653908 35003 solver.cpp:239] Iteration 158470 (3.74613 iter/s, 2.66942s/10 iters), loss = 6.87104
I0523 06:00:41.653946 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87104 (* 1 = 6.87104 loss)
I0523 06:00:41.661957 35003 sgd_solver.cpp:112] Iteration 158470, lr = 0.001
I0523 06:00:44.521986 35003 solver.cpp:239] Iteration 158480 (3.48685 iter/s, 2.86792s/10 iters), loss = 6.09063
I0523 06:00:44.522032 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09063 (* 1 = 6.09063 loss)
I0523 06:00:44.540609 35003 sgd_solver.cpp:112] Iteration 158480, lr = 0.001
I0523 06:00:47.829090 35003 solver.cpp:239] Iteration 158490 (3.02397 iter/s, 3.30691s/10 iters), loss = 7.37136
I0523 06:00:47.829138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37136 (* 1 = 7.37136 loss)
I0523 06:00:47.849727 35003 sgd_solver.cpp:112] Iteration 158490, lr = 0.001
I0523 06:00:50.108053 35003 solver.cpp:239] Iteration 158500 (4.38826 iter/s, 2.27881s/10 iters), loss = 8.38036
I0523 06:00:50.108116 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.38036 (* 1 = 8.38036 loss)
I0523 06:00:50.849005 35003 sgd_solver.cpp:112] Iteration 158500, lr = 0.001
I0523 06:00:52.919049 35003 solver.cpp:239] Iteration 158510 (3.55769 iter/s, 2.81082s/10 iters), loss = 7.26491
I0523 06:00:52.919102 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26491 (* 1 = 7.26491 loss)
I0523 06:00:52.932272 35003 sgd_solver.cpp:112] Iteration 158510, lr = 0.001
I0523 06:00:57.011871 35003 solver.cpp:239] Iteration 158520 (2.44343 iter/s, 4.0926s/10 iters), loss = 5.92994
I0523 06:00:57.011929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92994 (* 1 = 5.92994 loss)
I0523 06:00:57.030565 35003 sgd_solver.cpp:112] Iteration 158520, lr = 0.001
I0523 06:01:01.269166 35003 solver.cpp:239] Iteration 158530 (2.34904 iter/s, 4.25707s/10 iters), loss = 7.20265
I0523 06:01:01.269217 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20265 (* 1 = 7.20265 loss)
I0523 06:01:01.310866 35003 sgd_solver.cpp:112] Iteration 158530, lr = 0.001
I0523 06:01:06.088963 35003 solver.cpp:239] Iteration 158540 (2.07488 iter/s, 4.81955s/10 iters), loss = 5.83834
I0523 06:01:06.088999 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.83834 (* 1 = 5.83834 loss)
I0523 06:01:06.102145 35003 sgd_solver.cpp:112] Iteration 158540, lr = 0.001
I0523 06:01:08.971416 35003 solver.cpp:239] Iteration 158550 (3.46946 iter/s, 2.88229s/10 iters), loss = 6.55289
I0523 06:01:08.971464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55289 (* 1 = 6.55289 loss)
I0523 06:01:08.977000 35003 sgd_solver.cpp:112] Iteration 158550, lr = 0.001
I0523 06:01:11.710790 35003 solver.cpp:239] Iteration 158560 (3.6507 iter/s, 2.7392s/10 iters), loss = 5.99108
I0523 06:01:11.710999 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99108 (* 1 = 5.99108 loss)
I0523 06:01:11.734019 35003 sgd_solver.cpp:112] Iteration 158560, lr = 0.001
I0523 06:01:13.009912 35003 solver.cpp:239] Iteration 158570 (7.6991 iter/s, 1.29885s/10 iters), loss = 7.64196
I0523 06:01:13.009965 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64196 (* 1 = 7.64196 loss)
I0523 06:01:13.016317 35003 sgd_solver.cpp:112] Iteration 158570, lr = 0.001
I0523 06:01:17.227731 35003 solver.cpp:239] Iteration 158580 (2.37102 iter/s, 4.21759s/10 iters), loss = 6.54513
I0523 06:01:17.227782 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54513 (* 1 = 6.54513 loss)
I0523 06:01:17.239612 35003 sgd_solver.cpp:112] Iteration 158580, lr = 0.001
I0523 06:01:20.833088 35003 solver.cpp:239] Iteration 158590 (2.77381 iter/s, 3.60515s/10 iters), loss = 7.07713
I0523 06:01:20.833147 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07713 (* 1 = 7.07713 loss)
I0523 06:01:20.845749 35003 sgd_solver.cpp:112] Iteration 158590, lr = 0.001
I0523 06:01:24.452760 35003 solver.cpp:239] Iteration 158600 (2.76284 iter/s, 3.61946s/10 iters), loss = 6.35442
I0523 06:01:24.452807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35442 (* 1 = 6.35442 loss)
I0523 06:01:24.461426 35003 sgd_solver.cpp:112] Iteration 158600, lr = 0.001
I0523 06:01:28.431026 35003 solver.cpp:239] Iteration 158610 (2.51379 iter/s, 3.97806s/10 iters), loss = 7.27844
I0523 06:01:28.431068 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27844 (* 1 = 7.27844 loss)
I0523 06:01:28.443570 35003 sgd_solver.cpp:112] Iteration 158610, lr = 0.001
I0523 06:01:30.254910 35003 solver.cpp:239] Iteration 158620 (5.4833 iter/s, 1.82372s/10 iters), loss = 7.45224
I0523 06:01:30.254978 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45224 (* 1 = 7.45224 loss)
I0523 06:01:30.267365 35003 sgd_solver.cpp:112] Iteration 158620, lr = 0.001
I0523 06:01:35.096194 35003 solver.cpp:239] Iteration 158630 (2.06568 iter/s, 4.84102s/10 iters), loss = 7.64566
I0523 06:01:35.096230 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64566 (* 1 = 7.64566 loss)
I0523 06:01:35.131549 35003 sgd_solver.cpp:112] Iteration 158630, lr = 0.001
I0523 06:01:38.618950 35003 solver.cpp:239] Iteration 158640 (2.83884 iter/s, 3.52257s/10 iters), loss = 6.32331
I0523 06:01:38.618990 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32331 (* 1 = 6.32331 loss)
I0523 06:01:38.625005 35003 sgd_solver.cpp:112] Iteration 158640, lr = 0.001
I0523 06:01:42.808614 35003 solver.cpp:239] Iteration 158650 (2.38695 iter/s, 4.18945s/10 iters), loss = 7.22082
I0523 06:01:42.808902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22082 (* 1 = 7.22082 loss)
I0523 06:01:42.821154 35003 sgd_solver.cpp:112] Iteration 158650, lr = 0.001
I0523 06:01:44.974691 35003 solver.cpp:239] Iteration 158660 (4.6174 iter/s, 2.16572s/10 iters), loss = 7.12355
I0523 06:01:44.974774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12355 (* 1 = 7.12355 loss)
I0523 06:01:45.715548 35003 sgd_solver.cpp:112] Iteration 158660, lr = 0.001
I0523 06:01:49.629185 35003 solver.cpp:239] Iteration 158670 (2.14859 iter/s, 4.65422s/10 iters), loss = 5.88711
I0523 06:01:49.629236 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.88711 (* 1 = 5.88711 loss)
I0523 06:01:50.344796 35003 sgd_solver.cpp:112] Iteration 158670, lr = 0.001
I0523 06:01:53.091353 35003 solver.cpp:239] Iteration 158680 (2.88853 iter/s, 3.46197s/10 iters), loss = 6.69112
I0523 06:01:53.091398 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69112 (* 1 = 6.69112 loss)
I0523 06:01:53.101938 35003 sgd_solver.cpp:112] Iteration 158680, lr = 0.001
I0523 06:01:57.486344 35003 solver.cpp:239] Iteration 158690 (2.27544 iter/s, 4.39476s/10 iters), loss = 7.49553
I0523 06:01:57.486392 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49553 (* 1 = 7.49553 loss)
I0523 06:01:57.496639 35003 sgd_solver.cpp:112] Iteration 158690, lr = 0.001
I0523 06:02:01.935838 35003 solver.cpp:239] Iteration 158700 (2.24757 iter/s, 4.44925s/10 iters), loss = 8.25538
I0523 06:02:01.935886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.25538 (* 1 = 8.25538 loss)
I0523 06:02:01.945479 35003 sgd_solver.cpp:112] Iteration 158700, lr = 0.001
I0523 06:02:04.840023 35003 solver.cpp:239] Iteration 158710 (3.44351 iter/s, 2.90401s/10 iters), loss = 6.91686
I0523 06:02:04.840087 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91686 (* 1 = 6.91686 loss)
I0523 06:02:05.561547 35003 sgd_solver.cpp:112] Iteration 158710, lr = 0.001
I0523 06:02:07.666788 35003 solver.cpp:239] Iteration 158720 (3.53784 iter/s, 2.82659s/10 iters), loss = 6.3911
I0523 06:02:07.666837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3911 (* 1 = 6.3911 loss)
I0523 06:02:08.395455 35003 sgd_solver.cpp:112] Iteration 158720, lr = 0.001
I0523 06:02:12.047786 35003 solver.cpp:239] Iteration 158730 (2.2827 iter/s, 4.38077s/10 iters), loss = 6.91966
I0523 06:02:12.047833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91966 (* 1 = 6.91966 loss)
I0523 06:02:12.788496 35003 sgd_solver.cpp:112] Iteration 158730, lr = 0.001
I0523 06:02:14.916803 35003 solver.cpp:239] Iteration 158740 (3.48572 iter/s, 2.86885s/10 iters), loss = 6.74708
I0523 06:02:14.917050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74708 (* 1 = 6.74708 loss)
I0523 06:02:15.644595 35003 sgd_solver.cpp:112] Iteration 158740, lr = 0.001
I0523 06:02:18.424227 35003 solver.cpp:239] Iteration 158750 (2.85139 iter/s, 3.50706s/10 iters), loss = 6.16696
I0523 06:02:18.424265 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16696 (* 1 = 6.16696 loss)
I0523 06:02:18.443239 35003 sgd_solver.cpp:112] Iteration 158750, lr = 0.001
I0523 06:02:20.524447 35003 solver.cpp:239] Iteration 158760 (4.76173 iter/s, 2.10008s/10 iters), loss = 7.26675
I0523 06:02:20.524497 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26675 (* 1 = 7.26675 loss)
I0523 06:02:20.537333 35003 sgd_solver.cpp:112] Iteration 158760, lr = 0.001
I0523 06:02:23.432437 35003 solver.cpp:239] Iteration 158770 (3.439 iter/s, 2.90782s/10 iters), loss = 6.19003
I0523 06:02:23.432479 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19003 (* 1 = 6.19003 loss)
I0523 06:02:24.167541 35003 sgd_solver.cpp:112] Iteration 158770, lr = 0.001
I0523 06:02:26.888285 35003 solver.cpp:239] Iteration 158780 (2.8938 iter/s, 3.45566s/10 iters), loss = 7.13771
I0523 06:02:26.888330 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13771 (* 1 = 7.13771 loss)
I0523 06:02:26.893944 35003 sgd_solver.cpp:112] Iteration 158780, lr = 0.001
I0523 06:02:30.550148 35003 solver.cpp:239] Iteration 158790 (2.731 iter/s, 3.66166s/10 iters), loss = 6.57358
I0523 06:02:30.550200 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57358 (* 1 = 6.57358 loss)
I0523 06:02:30.582080 35003 sgd_solver.cpp:112] Iteration 158790, lr = 0.001
I0523 06:02:34.151190 35003 solver.cpp:239] Iteration 158800 (2.77714 iter/s, 3.60083s/10 iters), loss = 6.68359
I0523 06:02:34.151249 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68359 (* 1 = 6.68359 loss)
I0523 06:02:34.306627 35003 sgd_solver.cpp:112] Iteration 158800, lr = 0.001
I0523 06:02:37.833791 35003 solver.cpp:239] Iteration 158810 (2.71563 iter/s, 3.68239s/10 iters), loss = 7.12509
I0523 06:02:37.833834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12509 (* 1 = 7.12509 loss)
I0523 06:02:37.947450 35003 sgd_solver.cpp:112] Iteration 158810, lr = 0.001
I0523 06:02:41.473510 35003 solver.cpp:239] Iteration 158820 (2.74761 iter/s, 3.63952s/10 iters), loss = 6.35845
I0523 06:02:41.473563 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35845 (* 1 = 6.35845 loss)
I0523 06:02:41.492775 35003 sgd_solver.cpp:112] Iteration 158820, lr = 0.001
I0523 06:02:43.667567 35003 solver.cpp:239] Iteration 158830 (4.55807 iter/s, 2.19391s/10 iters), loss = 6.77716
I0523 06:02:43.667611 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77716 (* 1 = 6.77716 loss)
I0523 06:02:44.371743 35003 sgd_solver.cpp:112] Iteration 158830, lr = 0.001
I0523 06:02:48.300822 35003 solver.cpp:239] Iteration 158840 (2.15842 iter/s, 4.63302s/10 iters), loss = 6.89832
I0523 06:02:48.301044 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89832 (* 1 = 6.89832 loss)
I0523 06:02:48.306519 35003 sgd_solver.cpp:112] Iteration 158840, lr = 0.001
I0523 06:02:53.002861 35003 solver.cpp:239] Iteration 158850 (2.12692 iter/s, 4.70164s/10 iters), loss = 7.20782
I0523 06:02:53.002928 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20782 (* 1 = 7.20782 loss)
I0523 06:02:53.031237 35003 sgd_solver.cpp:112] Iteration 158850, lr = 0.001
I0523 06:02:56.499697 35003 solver.cpp:239] Iteration 158860 (2.85991 iter/s, 3.49662s/10 iters), loss = 6.83533
I0523 06:02:56.499761 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83533 (* 1 = 6.83533 loss)
I0523 06:02:56.512523 35003 sgd_solver.cpp:112] Iteration 158860, lr = 0.001
I0523 06:02:58.933156 35003 solver.cpp:239] Iteration 158870 (4.10965 iter/s, 2.43329s/10 iters), loss = 5.65546
I0523 06:02:58.933197 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.65546 (* 1 = 5.65546 loss)
I0523 06:02:59.053570 35003 sgd_solver.cpp:112] Iteration 158870, lr = 0.001
I0523 06:03:03.435957 35003 solver.cpp:239] Iteration 158880 (2.22096 iter/s, 4.50257s/10 iters), loss = 5.99959
I0523 06:03:03.436012 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99959 (* 1 = 5.99959 loss)
I0523 06:03:03.439815 35003 sgd_solver.cpp:112] Iteration 158880, lr = 0.001
I0523 06:03:06.845484 35003 solver.cpp:239] Iteration 158890 (2.93312 iter/s, 3.40933s/10 iters), loss = 6.57872
I0523 06:03:06.845525 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57872 (* 1 = 6.57872 loss)
I0523 06:03:07.554574 35003 sgd_solver.cpp:112] Iteration 158890, lr = 0.001
I0523 06:03:11.917937 35003 solver.cpp:239] Iteration 158900 (1.97153 iter/s, 5.0722s/10 iters), loss = 7.08082
I0523 06:03:11.917995 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08082 (* 1 = 7.08082 loss)
I0523 06:03:12.606705 35003 sgd_solver.cpp:112] Iteration 158900, lr = 0.001
I0523 06:03:15.606187 35003 solver.cpp:239] Iteration 158910 (2.71147 iter/s, 3.68804s/10 iters), loss = 7.62114
I0523 06:03:15.606235 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62114 (* 1 = 7.62114 loss)
I0523 06:03:16.321571 35003 sgd_solver.cpp:112] Iteration 158910, lr = 0.001
I0523 06:03:21.008159 35003 solver.cpp:239] Iteration 158920 (1.85128 iter/s, 5.40168s/10 iters), loss = 7.14349
I0523 06:03:21.008458 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14349 (* 1 = 7.14349 loss)
I0523 06:03:21.021823 35003 sgd_solver.cpp:112] Iteration 158920, lr = 0.001
I0523 06:03:24.508500 35003 solver.cpp:239] Iteration 158930 (2.85721 iter/s, 3.49991s/10 iters), loss = 6.76076
I0523 06:03:24.508538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76076 (* 1 = 6.76076 loss)
I0523 06:03:24.512882 35003 sgd_solver.cpp:112] Iteration 158930, lr = 0.001
I0523 06:03:28.843458 35003 solver.cpp:239] Iteration 158940 (2.30711 iter/s, 4.33443s/10 iters), loss = 8.0464
I0523 06:03:28.843508 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.0464 (* 1 = 8.0464 loss)
I0523 06:03:28.848441 35003 sgd_solver.cpp:112] Iteration 158940, lr = 0.001
I0523 06:03:32.056288 35003 solver.cpp:239] Iteration 158950 (3.11271 iter/s, 3.21264s/10 iters), loss = 6.85536
I0523 06:03:32.056346 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85536 (* 1 = 6.85536 loss)
I0523 06:03:32.516126 35003 sgd_solver.cpp:112] Iteration 158950, lr = 0.001
I0523 06:03:36.088192 35003 solver.cpp:239] Iteration 158960 (2.48036 iter/s, 4.03168s/10 iters), loss = 7.12993
I0523 06:03:36.088258 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12993 (* 1 = 7.12993 loss)
I0523 06:03:36.797215 35003 sgd_solver.cpp:112] Iteration 158960, lr = 0.001
I0523 06:03:41.725754 35003 solver.cpp:239] Iteration 158970 (1.77391 iter/s, 5.63727s/10 iters), loss = 7.62242
I0523 06:03:41.725791 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62242 (* 1 = 7.62242 loss)
I0523 06:03:41.733515 35003 sgd_solver.cpp:112] Iteration 158970, lr = 0.001
I0523 06:03:44.139966 35003 solver.cpp:239] Iteration 158980 (4.14238 iter/s, 2.41407s/10 iters), loss = 6.55179
I0523 06:03:44.140004 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55179 (* 1 = 6.55179 loss)
I0523 06:03:44.146509 35003 sgd_solver.cpp:112] Iteration 158980, lr = 0.001
I0523 06:03:48.558818 35003 solver.cpp:239] Iteration 158990 (2.26314 iter/s, 4.41863s/10 iters), loss = 5.9057
I0523 06:03:48.558862 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9057 (* 1 = 5.9057 loss)
I0523 06:03:48.578379 35003 sgd_solver.cpp:112] Iteration 158990, lr = 0.001
I0523 06:03:51.466114 35003 solver.cpp:239] Iteration 159000 (3.43982 iter/s, 2.90713s/10 iters), loss = 6.0034
I0523 06:03:51.466331 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0034 (* 1 = 6.0034 loss)
I0523 06:03:51.472471 35003 sgd_solver.cpp:112] Iteration 159000, lr = 0.001
I0523 06:03:55.101785 35003 solver.cpp:239] Iteration 159010 (2.75079 iter/s, 3.63533s/10 iters), loss = 7.18359
I0523 06:03:55.101826 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18359 (* 1 = 7.18359 loss)
I0523 06:03:55.111317 35003 sgd_solver.cpp:112] Iteration 159010, lr = 0.001
I0523 06:03:58.010562 35003 solver.cpp:239] Iteration 159020 (3.43807 iter/s, 2.90861s/10 iters), loss = 6.96391
I0523 06:03:58.010612 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96391 (* 1 = 6.96391 loss)
I0523 06:03:58.031868 35003 sgd_solver.cpp:112] Iteration 159020, lr = 0.001
I0523 06:04:02.430637 35003 solver.cpp:239] Iteration 159030 (2.26252 iter/s, 4.41984s/10 iters), loss = 7.2413
I0523 06:04:02.430689 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2413 (* 1 = 7.2413 loss)
I0523 06:04:03.165169 35003 sgd_solver.cpp:112] Iteration 159030, lr = 0.001
I0523 06:04:06.042743 35003 solver.cpp:239] Iteration 159040 (2.76862 iter/s, 3.61191s/10 iters), loss = 5.9029
I0523 06:04:06.042783 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9029 (* 1 = 5.9029 loss)
I0523 06:04:06.056133 35003 sgd_solver.cpp:112] Iteration 159040, lr = 0.001
I0523 06:04:08.943675 35003 solver.cpp:239] Iteration 159050 (3.44737 iter/s, 2.90076s/10 iters), loss = 5.77642
I0523 06:04:08.943711 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77642 (* 1 = 5.77642 loss)
I0523 06:04:08.956471 35003 sgd_solver.cpp:112] Iteration 159050, lr = 0.001
I0523 06:04:13.332199 35003 solver.cpp:239] Iteration 159060 (2.27879 iter/s, 4.38829s/10 iters), loss = 7.31641
I0523 06:04:13.332248 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31641 (* 1 = 7.31641 loss)
I0523 06:04:13.998728 35003 sgd_solver.cpp:112] Iteration 159060, lr = 0.001
I0523 06:04:17.091233 35003 solver.cpp:239] Iteration 159070 (2.66041 iter/s, 3.75882s/10 iters), loss = 6.15277
I0523 06:04:17.091296 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15277 (* 1 = 6.15277 loss)
I0523 06:04:17.709378 35003 sgd_solver.cpp:112] Iteration 159070, lr = 0.001
I0523 06:04:19.606881 35003 solver.cpp:239] Iteration 159080 (3.97538 iter/s, 2.51548s/10 iters), loss = 6.30669
I0523 06:04:19.606927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30669 (* 1 = 6.30669 loss)
I0523 06:04:19.621650 35003 sgd_solver.cpp:112] Iteration 159080, lr = 0.001
I0523 06:04:23.398938 35003 solver.cpp:239] Iteration 159090 (2.63723 iter/s, 3.79186s/10 iters), loss = 6.83475
I0523 06:04:23.399219 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83475 (* 1 = 6.83475 loss)
I0523 06:04:24.134030 35003 sgd_solver.cpp:112] Iteration 159090, lr = 0.001
I0523 06:04:27.064584 35003 solver.cpp:239] Iteration 159100 (2.72833 iter/s, 3.66524s/10 iters), loss = 6.16093
I0523 06:04:27.064626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16093 (* 1 = 6.16093 loss)
I0523 06:04:27.068279 35003 sgd_solver.cpp:112] Iteration 159100, lr = 0.001
I0523 06:04:29.187288 35003 solver.cpp:239] Iteration 159110 (4.71128 iter/s, 2.12256s/10 iters), loss = 5.55538
I0523 06:04:29.187325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.55538 (* 1 = 5.55538 loss)
I0523 06:04:29.874722 35003 sgd_solver.cpp:112] Iteration 159110, lr = 0.001
I0523 06:04:32.067935 35003 solver.cpp:239] Iteration 159120 (3.47164 iter/s, 2.88049s/10 iters), loss = 6.70249
I0523 06:04:32.067979 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70249 (* 1 = 6.70249 loss)
I0523 06:04:32.078835 35003 sgd_solver.cpp:112] Iteration 159120, lr = 0.001
I0523 06:04:34.988059 35003 solver.cpp:239] Iteration 159130 (3.42473 iter/s, 2.91994s/10 iters), loss = 5.89583
I0523 06:04:34.988112 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89583 (* 1 = 5.89583 loss)
I0523 06:04:35.001165 35003 sgd_solver.cpp:112] Iteration 159130, lr = 0.001
I0523 06:04:39.223141 35003 solver.cpp:239] Iteration 159140 (2.36136 iter/s, 4.23486s/10 iters), loss = 5.9247
I0523 06:04:39.223181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9247 (* 1 = 5.9247 loss)
I0523 06:04:39.231797 35003 sgd_solver.cpp:112] Iteration 159140, lr = 0.001
I0523 06:04:43.035553 35003 solver.cpp:239] Iteration 159150 (2.62315 iter/s, 3.81221s/10 iters), loss = 5.31594
I0523 06:04:43.035599 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.31594 (* 1 = 5.31594 loss)
I0523 06:04:43.769919 35003 sgd_solver.cpp:112] Iteration 159150, lr = 0.001
I0523 06:04:45.744835 35003 solver.cpp:239] Iteration 159160 (3.69124 iter/s, 2.70912s/10 iters), loss = 6.32374
I0523 06:04:45.744879 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32374 (* 1 = 6.32374 loss)
I0523 06:04:45.758606 35003 sgd_solver.cpp:112] Iteration 159160, lr = 0.001
I0523 06:04:47.982491 35003 solver.cpp:239] Iteration 159170 (4.46924 iter/s, 2.23752s/10 iters), loss = 6.986
I0523 06:04:47.982530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.986 (* 1 = 6.986 loss)
I0523 06:04:47.995064 35003 sgd_solver.cpp:112] Iteration 159170, lr = 0.001
I0523 06:04:50.084106 35003 solver.cpp:239] Iteration 159180 (4.75854 iter/s, 2.10148s/10 iters), loss = 6.99728
I0523 06:04:50.084147 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99728 (* 1 = 6.99728 loss)
I0523 06:04:50.089810 35003 sgd_solver.cpp:112] Iteration 159180, lr = 0.001
I0523 06:04:53.248019 35003 solver.cpp:239] Iteration 159190 (3.16082 iter/s, 3.16374s/10 iters), loss = 5.74765
I0523 06:04:53.248059 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74765 (* 1 = 5.74765 loss)
I0523 06:04:53.254623 35003 sgd_solver.cpp:112] Iteration 159190, lr = 0.001
I0523 06:04:54.906064 35003 solver.cpp:239] Iteration 159200 (6.03162 iter/s, 1.65793s/10 iters), loss = 6.43805
I0523 06:04:54.906392 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43805 (* 1 = 6.43805 loss)
I0523 06:04:54.919648 35003 sgd_solver.cpp:112] Iteration 159200, lr = 0.001
I0523 06:04:58.576418 35003 solver.cpp:239] Iteration 159210 (2.72487 iter/s, 3.66989s/10 iters), loss = 6.22631
I0523 06:04:58.576467 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22631 (* 1 = 6.22631 loss)
I0523 06:04:58.738374 35003 sgd_solver.cpp:112] Iteration 159210, lr = 0.001
I0523 06:05:01.546336 35003 solver.cpp:239] Iteration 159220 (3.3673 iter/s, 2.96974s/10 iters), loss = 6.98712
I0523 06:05:01.546380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98712 (* 1 = 6.98712 loss)
I0523 06:05:01.550276 35003 sgd_solver.cpp:112] Iteration 159220, lr = 0.001
I0523 06:05:04.421195 35003 solver.cpp:239] Iteration 159230 (3.47863 iter/s, 2.87469s/10 iters), loss = 6.93431
I0523 06:05:04.421241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93431 (* 1 = 6.93431 loss)
I0523 06:05:05.116960 35003 sgd_solver.cpp:112] Iteration 159230, lr = 0.001
I0523 06:05:09.504995 35003 solver.cpp:239] Iteration 159240 (1.96713 iter/s, 5.08354s/10 iters), loss = 6.0154
I0523 06:05:09.505049 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0154 (* 1 = 6.0154 loss)
I0523 06:05:10.245864 35003 sgd_solver.cpp:112] Iteration 159240, lr = 0.001
I0523 06:05:12.142912 35003 solver.cpp:239] Iteration 159250 (3.79111 iter/s, 2.63775s/10 iters), loss = 6.28032
I0523 06:05:12.142956 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28032 (* 1 = 6.28032 loss)
I0523 06:05:12.148777 35003 sgd_solver.cpp:112] Iteration 159250, lr = 0.001
I0523 06:05:16.482111 35003 solver.cpp:239] Iteration 159260 (2.3047 iter/s, 4.33897s/10 iters), loss = 6.40877
I0523 06:05:16.482152 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40877 (* 1 = 6.40877 loss)
I0523 06:05:16.495245 35003 sgd_solver.cpp:112] Iteration 159260, lr = 0.001
I0523 06:05:18.981807 35003 solver.cpp:239] Iteration 159270 (4.00072 iter/s, 2.49955s/10 iters), loss = 7.93518
I0523 06:05:18.981850 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93518 (* 1 = 7.93518 loss)
I0523 06:05:18.994767 35003 sgd_solver.cpp:112] Iteration 159270, lr = 0.001
I0523 06:05:21.076289 35003 solver.cpp:239] Iteration 159280 (4.77478 iter/s, 2.09434s/10 iters), loss = 6.73398
I0523 06:05:21.076337 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73398 (* 1 = 6.73398 loss)
I0523 06:05:21.078889 35003 sgd_solver.cpp:112] Iteration 159280, lr = 0.001
I0523 06:05:25.403537 35003 solver.cpp:239] Iteration 159290 (2.31107 iter/s, 4.32701s/10 iters), loss = 6.50605
I0523 06:05:25.403702 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50605 (* 1 = 6.50605 loss)
I0523 06:05:25.407405 35003 sgd_solver.cpp:112] Iteration 159290, lr = 0.001
I0523 06:05:29.199002 35003 solver.cpp:239] Iteration 159300 (2.63495 iter/s, 3.79514s/10 iters), loss = 7.12292
I0523 06:05:29.199044 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12292 (* 1 = 7.12292 loss)
I0523 06:05:29.205619 35003 sgd_solver.cpp:112] Iteration 159300, lr = 0.001
I0523 06:05:32.685988 35003 solver.cpp:239] Iteration 159310 (2.86796 iter/s, 3.4868s/10 iters), loss = 7.33531
I0523 06:05:32.686033 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33531 (* 1 = 7.33531 loss)
I0523 06:05:32.693449 35003 sgd_solver.cpp:112] Iteration 159310, lr = 0.001
I0523 06:05:36.109743 35003 solver.cpp:239] Iteration 159320 (2.92093 iter/s, 3.42356s/10 iters), loss = 5.23051
I0523 06:05:36.109783 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.23051 (* 1 = 5.23051 loss)
I0523 06:05:36.133090 35003 sgd_solver.cpp:112] Iteration 159320, lr = 0.001
I0523 06:05:38.279089 35003 solver.cpp:239] Iteration 159330 (4.60999 iter/s, 2.1692s/10 iters), loss = 6.64108
I0523 06:05:38.279142 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64108 (* 1 = 6.64108 loss)
I0523 06:05:38.658200 35003 sgd_solver.cpp:112] Iteration 159330, lr = 0.001
I0523 06:05:41.476706 35003 solver.cpp:239] Iteration 159340 (3.12751 iter/s, 3.19743s/10 iters), loss = 5.56551
I0523 06:05:41.476747 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.56551 (* 1 = 5.56551 loss)
I0523 06:05:41.484783 35003 sgd_solver.cpp:112] Iteration 159340, lr = 0.001
I0523 06:05:44.251595 35003 solver.cpp:239] Iteration 159350 (3.60396 iter/s, 2.77473s/10 iters), loss = 6.96568
I0523 06:05:44.251646 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96568 (* 1 = 6.96568 loss)
I0523 06:05:44.986737 35003 sgd_solver.cpp:112] Iteration 159350, lr = 0.001
I0523 06:05:49.383143 35003 solver.cpp:239] Iteration 159360 (1.94883 iter/s, 5.13128s/10 iters), loss = 6.90641
I0523 06:05:49.383190 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90641 (* 1 = 6.90641 loss)
I0523 06:05:50.097949 35003 sgd_solver.cpp:112] Iteration 159360, lr = 0.001
I0523 06:05:52.071784 35003 solver.cpp:239] Iteration 159370 (3.71958 iter/s, 2.68848s/10 iters), loss = 6.93658
I0523 06:05:52.071831 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93658 (* 1 = 6.93658 loss)
I0523 06:05:52.080214 35003 sgd_solver.cpp:112] Iteration 159370, lr = 0.001
I0523 06:05:56.902070 35003 solver.cpp:239] Iteration 159380 (2.07038 iter/s, 4.83004s/10 iters), loss = 6.34037
I0523 06:05:56.902369 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34037 (* 1 = 6.34037 loss)
I0523 06:05:56.915251 35003 sgd_solver.cpp:112] Iteration 159380, lr = 0.001
I0523 06:06:01.240201 35003 solver.cpp:239] Iteration 159390 (2.30538 iter/s, 4.33768s/10 iters), loss = 6.69253
I0523 06:06:01.240247 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69253 (* 1 = 6.69253 loss)
I0523 06:06:01.968061 35003 sgd_solver.cpp:112] Iteration 159390, lr = 0.001
I0523 06:06:05.140168 35003 solver.cpp:239] Iteration 159400 (2.56426 iter/s, 3.89976s/10 iters), loss = 6.93706
I0523 06:06:05.140228 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93706 (* 1 = 6.93706 loss)
I0523 06:06:05.855605 35003 sgd_solver.cpp:112] Iteration 159400, lr = 0.001
I0523 06:06:08.809887 35003 solver.cpp:239] Iteration 159410 (2.72516 iter/s, 3.66951s/10 iters), loss = 6.00655
I0523 06:06:08.809932 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00655 (* 1 = 6.00655 loss)
I0523 06:06:08.823320 35003 sgd_solver.cpp:112] Iteration 159410, lr = 0.001
I0523 06:06:10.877631 35003 solver.cpp:239] Iteration 159420 (4.83652 iter/s, 2.0676s/10 iters), loss = 7.02956
I0523 06:06:10.877670 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02956 (* 1 = 7.02956 loss)
I0523 06:06:10.890699 35003 sgd_solver.cpp:112] Iteration 159420, lr = 0.001
I0523 06:06:12.981341 35003 solver.cpp:239] Iteration 159430 (4.75381 iter/s, 2.10358s/10 iters), loss = 7.16817
I0523 06:06:12.981379 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16817 (* 1 = 7.16817 loss)
I0523 06:06:12.988889 35003 sgd_solver.cpp:112] Iteration 159430, lr = 0.001
I0523 06:06:17.957836 35003 solver.cpp:239] Iteration 159440 (2.00954 iter/s, 4.97625s/10 iters), loss = 5.81368
I0523 06:06:17.957880 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.81368 (* 1 = 5.81368 loss)
I0523 06:06:17.970672 35003 sgd_solver.cpp:112] Iteration 159440, lr = 0.001
I0523 06:06:20.779029 35003 solver.cpp:239] Iteration 159450 (3.54481 iter/s, 2.82103s/10 iters), loss = 6.2878
I0523 06:06:20.779086 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2878 (* 1 = 6.2878 loss)
I0523 06:06:21.513123 35003 sgd_solver.cpp:112] Iteration 159450, lr = 0.001
I0523 06:06:25.680392 35003 solver.cpp:239] Iteration 159460 (2.04036 iter/s, 4.9011s/10 iters), loss = 5.82062
I0523 06:06:25.680435 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.82062 (* 1 = 5.82062 loss)
I0523 06:06:25.692323 35003 sgd_solver.cpp:112] Iteration 159460, lr = 0.001
I0523 06:06:29.362895 35003 solver.cpp:239] Iteration 159470 (2.71569 iter/s, 3.6823s/10 iters), loss = 7.69982
I0523 06:06:29.363145 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69982 (* 1 = 7.69982 loss)
I0523 06:06:29.376024 35003 sgd_solver.cpp:112] Iteration 159470, lr = 0.001
I0523 06:06:32.938238 35003 solver.cpp:239] Iteration 159480 (2.79722 iter/s, 3.57498s/10 iters), loss = 6.4066
I0523 06:06:32.938290 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4066 (* 1 = 6.4066 loss)
I0523 06:06:32.950495 35003 sgd_solver.cpp:112] Iteration 159480, lr = 0.001
I0523 06:06:36.878250 35003 solver.cpp:239] Iteration 159490 (2.5382 iter/s, 3.9398s/10 iters), loss = 6.53748
I0523 06:06:36.878285 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53748 (* 1 = 6.53748 loss)
I0523 06:06:36.887773 35003 sgd_solver.cpp:112] Iteration 159490, lr = 0.001
I0523 06:06:38.960249 35003 solver.cpp:239] Iteration 159500 (4.80337 iter/s, 2.08187s/10 iters), loss = 6.73691
I0523 06:06:38.960291 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73691 (* 1 = 6.73691 loss)
I0523 06:06:38.973649 35003 sgd_solver.cpp:112] Iteration 159500, lr = 0.001
I0523 06:06:42.490012 35003 solver.cpp:239] Iteration 159510 (2.83321 iter/s, 3.52956s/10 iters), loss = 6.98987
I0523 06:06:42.490067 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98987 (* 1 = 6.98987 loss)
I0523 06:06:42.496016 35003 sgd_solver.cpp:112] Iteration 159510, lr = 0.001
I0523 06:06:45.179391 35003 solver.cpp:239] Iteration 159520 (3.71858 iter/s, 2.6892s/10 iters), loss = 5.92973
I0523 06:06:45.179437 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92973 (* 1 = 5.92973 loss)
I0523 06:06:45.189338 35003 sgd_solver.cpp:112] Iteration 159520, lr = 0.001
I0523 06:06:48.740242 35003 solver.cpp:239] Iteration 159530 (2.80848 iter/s, 3.56065s/10 iters), loss = 5.63433
I0523 06:06:48.740283 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.63433 (* 1 = 5.63433 loss)
I0523 06:06:48.748222 35003 sgd_solver.cpp:112] Iteration 159530, lr = 0.001
I0523 06:06:53.163987 35003 solver.cpp:239] Iteration 159540 (2.26064 iter/s, 4.42352s/10 iters), loss = 6.89847
I0523 06:06:53.164036 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89847 (* 1 = 6.89847 loss)
I0523 06:06:53.904728 35003 sgd_solver.cpp:112] Iteration 159540, lr = 0.001
I0523 06:06:56.044503 35003 solver.cpp:239] Iteration 159550 (3.47181 iter/s, 2.88034s/10 iters), loss = 7.07771
I0523 06:06:56.044550 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07771 (* 1 = 7.07771 loss)
I0523 06:06:56.778301 35003 sgd_solver.cpp:112] Iteration 159550, lr = 0.001
I0523 06:06:59.580992 35003 solver.cpp:239] Iteration 159560 (2.82782 iter/s, 3.53629s/10 iters), loss = 8.06269
I0523 06:06:59.581166 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.06269 (* 1 = 8.06269 loss)
I0523 06:06:59.588610 35003 sgd_solver.cpp:112] Iteration 159560, lr = 0.001
I0523 06:07:02.449405 35003 solver.cpp:239] Iteration 159570 (3.4866 iter/s, 2.86812s/10 iters), loss = 7.46658
I0523 06:07:02.449445 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46658 (* 1 = 7.46658 loss)
I0523 06:07:02.467942 35003 sgd_solver.cpp:112] Iteration 159570, lr = 0.001
I0523 06:07:08.232376 35003 solver.cpp:239] Iteration 159580 (1.7293 iter/s, 5.7827s/10 iters), loss = 6.47721
I0523 06:07:08.232414 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47721 (* 1 = 6.47721 loss)
I0523 06:07:08.245857 35003 sgd_solver.cpp:112] Iteration 159580, lr = 0.001
I0523 06:07:11.792531 35003 solver.cpp:239] Iteration 159590 (2.80901 iter/s, 3.55997s/10 iters), loss = 6.00957
I0523 06:07:11.792578 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00957 (* 1 = 6.00957 loss)
I0523 06:07:11.800130 35003 sgd_solver.cpp:112] Iteration 159590, lr = 0.001
I0523 06:07:15.488286 35003 solver.cpp:239] Iteration 159600 (2.70596 iter/s, 3.69555s/10 iters), loss = 6.52762
I0523 06:07:15.488337 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52762 (* 1 = 6.52762 loss)
I0523 06:07:16.203263 35003 sgd_solver.cpp:112] Iteration 159600, lr = 0.001
I0523 06:07:19.622740 35003 solver.cpp:239] Iteration 159610 (2.41884 iter/s, 4.13421s/10 iters), loss = 5.82595
I0523 06:07:19.622787 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.82595 (* 1 = 5.82595 loss)
I0523 06:07:20.133947 35003 sgd_solver.cpp:112] Iteration 159610, lr = 0.001
I0523 06:07:22.868441 35003 solver.cpp:239] Iteration 159620 (3.08118 iter/s, 3.24551s/10 iters), loss = 6.85331
I0523 06:07:22.868487 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85331 (* 1 = 6.85331 loss)
I0523 06:07:23.529372 35003 sgd_solver.cpp:112] Iteration 159620, lr = 0.001
I0523 06:07:28.771121 35003 solver.cpp:239] Iteration 159630 (1.69423 iter/s, 5.9024s/10 iters), loss = 6.56475
I0523 06:07:28.771167 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56475 (* 1 = 6.56475 loss)
I0523 06:07:28.774672 35003 sgd_solver.cpp:112] Iteration 159630, lr = 0.001
I0523 06:07:33.120847 35003 solver.cpp:239] Iteration 159640 (2.29911 iter/s, 4.3495s/10 iters), loss = 6.38162
I0523 06:07:33.121095 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38162 (* 1 = 6.38162 loss)
I0523 06:07:33.126677 35003 sgd_solver.cpp:112] Iteration 159640, lr = 0.001
I0523 06:07:36.603025 35003 solver.cpp:239] Iteration 159650 (2.87206 iter/s, 3.48182s/10 iters), loss = 6.66159
I0523 06:07:36.603063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66159 (* 1 = 6.66159 loss)
I0523 06:07:36.626446 35003 sgd_solver.cpp:112] Iteration 159650, lr = 0.001
I0523 06:07:40.207084 35003 solver.cpp:239] Iteration 159660 (2.7748 iter/s, 3.60386s/10 iters), loss = 7.48592
I0523 06:07:40.207126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48592 (* 1 = 7.48592 loss)
I0523 06:07:40.227779 35003 sgd_solver.cpp:112] Iteration 159660, lr = 0.001
I0523 06:07:45.280443 35003 solver.cpp:239] Iteration 159670 (1.97118 iter/s, 5.07311s/10 iters), loss = 7.21045
I0523 06:07:45.280481 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21045 (* 1 = 7.21045 loss)
I0523 06:07:45.293364 35003 sgd_solver.cpp:112] Iteration 159670, lr = 0.001
I0523 06:07:49.445772 35003 solver.cpp:239] Iteration 159680 (2.4009 iter/s, 4.16511s/10 iters), loss = 6.21759
I0523 06:07:49.445825 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21759 (* 1 = 6.21759 loss)
I0523 06:07:49.450361 35003 sgd_solver.cpp:112] Iteration 159680, lr = 0.001
I0523 06:07:53.024152 35003 solver.cpp:239] Iteration 159690 (2.79472 iter/s, 3.57818s/10 iters), loss = 6.74494
I0523 06:07:53.024195 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74494 (* 1 = 6.74494 loss)
I0523 06:07:53.037778 35003 sgd_solver.cpp:112] Iteration 159690, lr = 0.001
I0523 06:07:58.014408 35003 solver.cpp:239] Iteration 159700 (2.00401 iter/s, 4.99001s/10 iters), loss = 6.95272
I0523 06:07:58.014462 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95272 (* 1 = 6.95272 loss)
I0523 06:07:58.703960 35003 sgd_solver.cpp:112] Iteration 159700, lr = 0.001
I0523 06:08:02.374092 35003 solver.cpp:239] Iteration 159710 (2.29387 iter/s, 4.35945s/10 iters), loss = 7.17795
I0523 06:08:02.374142 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17795 (* 1 = 7.17795 loss)
I0523 06:08:02.380846 35003 sgd_solver.cpp:112] Iteration 159710, lr = 0.001
I0523 06:08:06.499953 35003 solver.cpp:239] Iteration 159720 (2.42387 iter/s, 4.12563s/10 iters), loss = 6.82788
I0523 06:08:06.500126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82788 (* 1 = 6.82788 loss)
I0523 06:08:06.505993 35003 sgd_solver.cpp:112] Iteration 159720, lr = 0.001
I0523 06:08:10.000232 35003 solver.cpp:239] Iteration 159730 (2.85718 iter/s, 3.49996s/10 iters), loss = 5.82978
I0523 06:08:10.000279 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.82978 (* 1 = 5.82978 loss)
I0523 06:08:10.015130 35003 sgd_solver.cpp:112] Iteration 159730, lr = 0.001
I0523 06:08:12.824111 35003 solver.cpp:239] Iteration 159740 (3.54144 iter/s, 2.82371s/10 iters), loss = 6.01246
I0523 06:08:12.824151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01246 (* 1 = 6.01246 loss)
I0523 06:08:12.837255 35003 sgd_solver.cpp:112] Iteration 159740, lr = 0.001
I0523 06:08:15.602068 35003 solver.cpp:239] Iteration 159750 (3.59998 iter/s, 2.7778s/10 iters), loss = 6.25876
I0523 06:08:15.602111 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25876 (* 1 = 6.25876 loss)
I0523 06:08:15.611342 35003 sgd_solver.cpp:112] Iteration 159750, lr = 0.001
I0523 06:08:19.848145 35003 solver.cpp:239] Iteration 159760 (2.35524 iter/s, 4.24585s/10 iters), loss = 8.062
I0523 06:08:19.848191 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.062 (* 1 = 8.062 loss)
I0523 06:08:19.854802 35003 sgd_solver.cpp:112] Iteration 159760, lr = 0.001
I0523 06:08:23.095891 35003 solver.cpp:239] Iteration 159770 (3.07924 iter/s, 3.24756s/10 iters), loss = 7.1685
I0523 06:08:23.095934 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1685 (* 1 = 7.1685 loss)
I0523 06:08:23.103523 35003 sgd_solver.cpp:112] Iteration 159770, lr = 0.001
I0523 06:08:26.581948 35003 solver.cpp:239] Iteration 159780 (2.86873 iter/s, 3.48586s/10 iters), loss = 7.05136
I0523 06:08:26.581997 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05136 (* 1 = 7.05136 loss)
I0523 06:08:26.589118 35003 sgd_solver.cpp:112] Iteration 159780, lr = 0.001
I0523 06:08:30.500604 35003 solver.cpp:239] Iteration 159790 (2.55203 iter/s, 3.91844s/10 iters), loss = 6.02626
I0523 06:08:30.500649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02626 (* 1 = 6.02626 loss)
I0523 06:08:30.506386 35003 sgd_solver.cpp:112] Iteration 159790, lr = 0.001
I0523 06:08:34.143810 35003 solver.cpp:239] Iteration 159800 (2.74498 iter/s, 3.64301s/10 iters), loss = 7.38312
I0523 06:08:34.143867 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38312 (* 1 = 7.38312 loss)
I0523 06:08:34.871678 35003 sgd_solver.cpp:112] Iteration 159800, lr = 0.001
I0523 06:08:37.097440 35003 solver.cpp:239] Iteration 159810 (3.38588 iter/s, 2.95345s/10 iters), loss = 6.55679
I0523 06:08:37.097640 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55679 (* 1 = 6.55679 loss)
I0523 06:08:37.835613 35003 sgd_solver.cpp:112] Iteration 159810, lr = 0.001
I0523 06:08:40.287181 35003 solver.cpp:239] Iteration 159820 (3.13537 iter/s, 3.18942s/10 iters), loss = 8.02116
I0523 06:08:40.287222 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02116 (* 1 = 8.02116 loss)
I0523 06:08:40.296679 35003 sgd_solver.cpp:112] Iteration 159820, lr = 0.001
I0523 06:08:42.377176 35003 solver.cpp:239] Iteration 159830 (4.78501 iter/s, 2.08986s/10 iters), loss = 6.7524
I0523 06:08:42.377221 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7524 (* 1 = 6.7524 loss)
I0523 06:08:42.403928 35003 sgd_solver.cpp:112] Iteration 159830, lr = 0.001
I0523 06:08:45.002264 35003 solver.cpp:239] Iteration 159840 (3.80963 iter/s, 2.62492s/10 iters), loss = 5.73334
I0523 06:08:45.002308 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.73334 (* 1 = 5.73334 loss)
I0523 06:08:45.004423 35003 sgd_solver.cpp:112] Iteration 159840, lr = 0.001
I0523 06:08:45.934784 35003 solver.cpp:239] Iteration 159850 (10.7247 iter/s, 0.932424s/10 iters), loss = 6.37157
I0523 06:08:45.934835 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37157 (* 1 = 6.37157 loss)
I0523 06:08:45.943156 35003 sgd_solver.cpp:112] Iteration 159850, lr = 0.001
I0523 06:08:46.889158 35003 solver.cpp:239] Iteration 159860 (10.4792 iter/s, 0.954275s/10 iters), loss = 6.3624
I0523 06:08:46.889199 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3624 (* 1 = 6.3624 loss)
I0523 06:08:46.896529 35003 sgd_solver.cpp:112] Iteration 159860, lr = 0.001
I0523 06:08:47.737238 35003 solver.cpp:239] Iteration 159870 (11.7926 iter/s, 0.847992s/10 iters), loss = 6.69345
I0523 06:08:47.737283 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69345 (* 1 = 6.69345 loss)
I0523 06:08:47.748436 35003 sgd_solver.cpp:112] Iteration 159870, lr = 0.001
I0523 06:08:48.616904 35003 solver.cpp:239] Iteration 159880 (11.3692 iter/s, 0.87957s/10 iters), loss = 5.41649
I0523 06:08:48.616955 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.41649 (* 1 = 5.41649 loss)
I0523 06:08:48.626243 35003 sgd_solver.cpp:112] Iteration 159880, lr = 0.001
I0523 06:08:49.433380 35003 solver.cpp:239] Iteration 159890 (12.2491 iter/s, 0.816384s/10 iters), loss = 5.18501
I0523 06:08:49.433425 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.18501 (* 1 = 5.18501 loss)
I0523 06:08:49.439383 35003 sgd_solver.cpp:112] Iteration 159890, lr = 0.001
I0523 06:08:50.688354 35003 solver.cpp:239] Iteration 159900 (7.96899 iter/s, 1.25486s/10 iters), loss = 6.79371
I0523 06:08:50.688405 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79371 (* 1 = 6.79371 loss)
I0523 06:08:50.692512 35003 sgd_solver.cpp:112] Iteration 159900, lr = 0.001
I0523 06:08:51.816821 35003 solver.cpp:239] Iteration 159910 (8.86242 iter/s, 1.12836s/10 iters), loss = 7.18249
I0523 06:08:51.816860 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18249 (* 1 = 7.18249 loss)
I0523 06:08:52.532271 35003 sgd_solver.cpp:112] Iteration 159910, lr = 0.001
I0523 06:08:56.746899 35003 solver.cpp:239] Iteration 159920 (2.02846 iter/s, 4.92984s/10 iters), loss = 6.89306
I0523 06:08:56.746943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89306 (* 1 = 6.89306 loss)
I0523 06:08:56.760350 35003 sgd_solver.cpp:112] Iteration 159920, lr = 0.001
I0523 06:09:00.401003 35003 solver.cpp:239] Iteration 159930 (2.7368 iter/s, 3.65391s/10 iters), loss = 6.63133
I0523 06:09:00.401049 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63133 (* 1 = 6.63133 loss)
I0523 06:09:01.072645 35003 sgd_solver.cpp:112] Iteration 159930, lr = 0.001
I0523 06:09:04.423205 35003 solver.cpp:239] Iteration 159940 (2.48633 iter/s, 4.02199s/10 iters), loss = 7.92377
I0523 06:09:04.423249 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.92377 (* 1 = 7.92377 loss)
I0523 06:09:04.436094 35003 sgd_solver.cpp:112] Iteration 159940, lr = 0.001
I0523 06:09:07.242230 35003 solver.cpp:239] Iteration 159950 (3.54753 iter/s, 2.81886s/10 iters), loss = 5.7573
I0523 06:09:07.242415 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7573 (* 1 = 5.7573 loss)
I0523 06:09:07.977165 35003 sgd_solver.cpp:112] Iteration 159950, lr = 0.001
I0523 06:09:12.491304 35003 solver.cpp:239] Iteration 159960 (1.90524 iter/s, 5.24868s/10 iters), loss = 7.51621
I0523 06:09:12.491354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51621 (* 1 = 7.51621 loss)
I0523 06:09:12.509253 35003 sgd_solver.cpp:112] Iteration 159960, lr = 0.001
I0523 06:09:15.298094 35003 solver.cpp:239] Iteration 159970 (3.56301 iter/s, 2.80662s/10 iters), loss = 5.75356
I0523 06:09:15.298146 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.75356 (* 1 = 5.75356 loss)
I0523 06:09:16.007242 35003 sgd_solver.cpp:112] Iteration 159970, lr = 0.001
I0523 06:09:19.462666 35003 solver.cpp:239] Iteration 159980 (2.40134 iter/s, 4.16435s/10 iters), loss = 5.85154
I0523 06:09:19.462741 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85154 (* 1 = 5.85154 loss)
I0523 06:09:20.144865 35003 sgd_solver.cpp:112] Iteration 159980, lr = 0.001
I0523 06:09:21.458016 35003 solver.cpp:239] Iteration 159990 (5.01207 iter/s, 1.99518s/10 iters), loss = 6.04101
I0523 06:09:21.458057 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04101 (* 1 = 6.04101 loss)
I0523 06:09:21.465853 35003 sgd_solver.cpp:112] Iteration 159990, lr = 0.001
I0523 06:09:22.715685 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_160000.caffemodel
I0523 06:09:23.812744 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_160000.solverstate
I0523 06:09:24.021358 35003 solver.cpp:239] Iteration 160000 (3.90139 iter/s, 2.56319s/10 iters), loss = 6.25662
I0523 06:09:24.021400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25662 (* 1 = 6.25662 loss)
I0523 06:09:24.734910 35003 sgd_solver.cpp:112] Iteration 160000, lr = 0.001
I0523 06:09:27.628583 35003 solver.cpp:239] Iteration 160010 (2.77237 iter/s, 3.60702s/10 iters), loss = 5.37271
I0523 06:09:27.628646 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.37271 (* 1 = 5.37271 loss)
I0523 06:09:27.649813 35003 sgd_solver.cpp:112] Iteration 160010, lr = 0.001
I0523 06:09:29.749955 35003 solver.cpp:239] Iteration 160020 (4.71428 iter/s, 2.12122s/10 iters), loss = 7.17586
I0523 06:09:29.750006 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17586 (* 1 = 7.17586 loss)
I0523 06:09:29.755473 35003 sgd_solver.cpp:112] Iteration 160020, lr = 0.001
I0523 06:09:33.292358 35003 solver.cpp:239] Iteration 160030 (2.8231 iter/s, 3.5422s/10 iters), loss = 6.78141
I0523 06:09:33.292400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78141 (* 1 = 6.78141 loss)
I0523 06:09:33.295697 35003 sgd_solver.cpp:112] Iteration 160030, lr = 0.001
I0523 06:09:36.965860 35003 solver.cpp:239] Iteration 160040 (2.72235 iter/s, 3.6733s/10 iters), loss = 7.34264
I0523 06:09:36.965900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34264 (* 1 = 7.34264 loss)
I0523 06:09:36.978799 35003 sgd_solver.cpp:112] Iteration 160040, lr = 0.001
I0523 06:09:39.706989 35003 solver.cpp:239] Iteration 160050 (3.64834 iter/s, 2.74097s/10 iters), loss = 7.14036
I0523 06:09:39.707144 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14036 (* 1 = 7.14036 loss)
I0523 06:09:39.783716 35003 sgd_solver.cpp:112] Iteration 160050, lr = 0.001
I0523 06:09:44.233422 35003 solver.cpp:239] Iteration 160060 (2.20941 iter/s, 4.52609s/10 iters), loss = 6.85871
I0523 06:09:44.233465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85871 (* 1 = 6.85871 loss)
I0523 06:09:44.973525 35003 sgd_solver.cpp:112] Iteration 160060, lr = 0.001
I0523 06:09:47.969148 35003 solver.cpp:239] Iteration 160070 (2.677 iter/s, 3.73552s/10 iters), loss = 6.66681
I0523 06:09:47.969192 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66681 (* 1 = 6.66681 loss)
I0523 06:09:47.971859 35003 sgd_solver.cpp:112] Iteration 160070, lr = 0.001
I0523 06:09:51.472180 35003 solver.cpp:239] Iteration 160080 (2.85484 iter/s, 3.50282s/10 iters), loss = 7.78381
I0523 06:09:51.472239 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78381 (* 1 = 7.78381 loss)
I0523 06:09:51.507716 35003 sgd_solver.cpp:112] Iteration 160080, lr = 0.001
I0523 06:09:56.494446 35003 solver.cpp:239] Iteration 160090 (1.99124 iter/s, 5.022s/10 iters), loss = 6.18565
I0523 06:09:56.494489 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18565 (* 1 = 6.18565 loss)
I0523 06:09:56.501879 35003 sgd_solver.cpp:112] Iteration 160090, lr = 0.001
I0523 06:09:58.865439 35003 solver.cpp:239] Iteration 160100 (4.2179 iter/s, 2.37085s/10 iters), loss = 5.8616
I0523 06:09:58.865476 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8616 (* 1 = 5.8616 loss)
I0523 06:09:58.878304 35003 sgd_solver.cpp:112] Iteration 160100, lr = 0.001
I0523 06:10:01.452216 35003 solver.cpp:239] Iteration 160110 (3.86603 iter/s, 2.58663s/10 iters), loss = 7.41905
I0523 06:10:01.452252 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41905 (* 1 = 7.41905 loss)
I0523 06:10:01.465571 35003 sgd_solver.cpp:112] Iteration 160110, lr = 0.001
I0523 06:10:04.558485 35003 solver.cpp:239] Iteration 160120 (3.21948 iter/s, 3.10609s/10 iters), loss = 6.87727
I0523 06:10:04.558526 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87727 (* 1 = 6.87727 loss)
I0523 06:10:04.567939 35003 sgd_solver.cpp:112] Iteration 160120, lr = 0.001
I0523 06:10:07.425174 35003 solver.cpp:239] Iteration 160130 (3.48855 iter/s, 2.86652s/10 iters), loss = 7.90149
I0523 06:10:07.425225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90149 (* 1 = 7.90149 loss)
I0523 06:10:08.166075 35003 sgd_solver.cpp:112] Iteration 160130, lr = 0.001
I0523 06:10:12.487764 35003 solver.cpp:239] Iteration 160140 (1.97537 iter/s, 5.06233s/10 iters), loss = 5.14692
I0523 06:10:12.487946 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.14692 (* 1 = 5.14692 loss)
I0523 06:10:13.184852 35003 sgd_solver.cpp:112] Iteration 160140, lr = 0.001
I0523 06:10:16.216848 35003 solver.cpp:239] Iteration 160150 (2.68186 iter/s, 3.72875s/10 iters), loss = 7.22237
I0523 06:10:16.216897 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22237 (* 1 = 7.22237 loss)
I0523 06:10:16.230124 35003 sgd_solver.cpp:112] Iteration 160150, lr = 0.001
I0523 06:10:19.621569 35003 solver.cpp:239] Iteration 160160 (2.93726 iter/s, 3.40453s/10 iters), loss = 6.47362
I0523 06:10:19.621606 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47362 (* 1 = 6.47362 loss)
I0523 06:10:19.634814 35003 sgd_solver.cpp:112] Iteration 160160, lr = 0.001
I0523 06:10:23.087323 35003 solver.cpp:239] Iteration 160170 (2.88553 iter/s, 3.46557s/10 iters), loss = 6.50688
I0523 06:10:23.087368 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50688 (* 1 = 6.50688 loss)
I0523 06:10:23.807168 35003 sgd_solver.cpp:112] Iteration 160170, lr = 0.001
I0523 06:10:28.029481 35003 solver.cpp:239] Iteration 160180 (2.02351 iter/s, 4.94191s/10 iters), loss = 5.45625
I0523 06:10:28.029531 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.45625 (* 1 = 5.45625 loss)
I0523 06:10:28.085256 35003 sgd_solver.cpp:112] Iteration 160180, lr = 0.001
I0523 06:10:30.959908 35003 solver.cpp:239] Iteration 160190 (3.41268 iter/s, 2.93025s/10 iters), loss = 6.63657
I0523 06:10:30.959954 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63657 (* 1 = 6.63657 loss)
I0523 06:10:30.971841 35003 sgd_solver.cpp:112] Iteration 160190, lr = 0.001
I0523 06:10:34.232915 35003 solver.cpp:239] Iteration 160200 (3.05547 iter/s, 3.27281s/10 iters), loss = 6.42345
I0523 06:10:34.232964 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42345 (* 1 = 6.42345 loss)
I0523 06:10:34.238610 35003 sgd_solver.cpp:112] Iteration 160200, lr = 0.001
I0523 06:10:38.235153 35003 solver.cpp:239] Iteration 160210 (2.49874 iter/s, 4.00202s/10 iters), loss = 5.93767
I0523 06:10:38.235195 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93767 (* 1 = 5.93767 loss)
I0523 06:10:38.274204 35003 sgd_solver.cpp:112] Iteration 160210, lr = 0.001
I0523 06:10:41.173097 35003 solver.cpp:239] Iteration 160220 (3.40394 iter/s, 2.93777s/10 iters), loss = 6.55547
I0523 06:10:41.173157 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55547 (* 1 = 6.55547 loss)
I0523 06:10:41.195122 35003 sgd_solver.cpp:112] Iteration 160220, lr = 0.001
I0523 06:10:44.713856 35003 solver.cpp:239] Iteration 160230 (2.82442 iter/s, 3.54056s/10 iters), loss = 6.20125
I0523 06:10:44.714131 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20125 (* 1 = 6.20125 loss)
I0523 06:10:44.720279 35003 sgd_solver.cpp:112] Iteration 160230, lr = 0.001
I0523 06:10:48.326265 35003 solver.cpp:239] Iteration 160240 (2.76855 iter/s, 3.612s/10 iters), loss = 6.61816
I0523 06:10:48.326318 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61816 (* 1 = 6.61816 loss)
I0523 06:10:48.335808 35003 sgd_solver.cpp:112] Iteration 160240, lr = 0.001
I0523 06:10:53.624591 35003 solver.cpp:239] Iteration 160250 (1.88749 iter/s, 5.29805s/10 iters), loss = 6.60057
I0523 06:10:53.624634 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60057 (* 1 = 6.60057 loss)
I0523 06:10:53.637584 35003 sgd_solver.cpp:112] Iteration 160250, lr = 0.001
I0523 06:10:56.485160 35003 solver.cpp:239] Iteration 160260 (3.49601 iter/s, 2.8604s/10 iters), loss = 6.94624
I0523 06:10:56.485203 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94624 (* 1 = 6.94624 loss)
I0523 06:10:57.220094 35003 sgd_solver.cpp:112] Iteration 160260, lr = 0.001
I0523 06:11:01.562340 35003 solver.cpp:239] Iteration 160270 (1.9697 iter/s, 5.07692s/10 iters), loss = 7.37507
I0523 06:11:01.562396 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37507 (* 1 = 7.37507 loss)
I0523 06:11:01.641822 35003 sgd_solver.cpp:112] Iteration 160270, lr = 0.001
I0523 06:11:05.161969 35003 solver.cpp:239] Iteration 160280 (2.77823 iter/s, 3.59942s/10 iters), loss = 6.70795
I0523 06:11:05.162024 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70795 (* 1 = 6.70795 loss)
I0523 06:11:05.183724 35003 sgd_solver.cpp:112] Iteration 160280, lr = 0.001
I0523 06:11:08.180711 35003 solver.cpp:239] Iteration 160290 (3.31284 iter/s, 3.01856s/10 iters), loss = 6.25677
I0523 06:11:08.180757 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25677 (* 1 = 6.25677 loss)
I0523 06:11:08.185438 35003 sgd_solver.cpp:112] Iteration 160290, lr = 0.001
I0523 06:11:12.473321 35003 solver.cpp:239] Iteration 160300 (2.32971 iter/s, 4.29239s/10 iters), loss = 6.5856
I0523 06:11:12.473364 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5856 (* 1 = 6.5856 loss)
I0523 06:11:12.486685 35003 sgd_solver.cpp:112] Iteration 160300, lr = 0.001
I0523 06:11:15.266364 35003 solver.cpp:239] Iteration 160310 (3.58053 iter/s, 2.79288s/10 iters), loss = 7.13059
I0523 06:11:15.266607 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13059 (* 1 = 7.13059 loss)
I0523 06:11:15.273725 35003 sgd_solver.cpp:112] Iteration 160310, lr = 0.001
I0523 06:11:17.593613 35003 solver.cpp:239] Iteration 160320 (4.29751 iter/s, 2.32693s/10 iters), loss = 6.76856
I0523 06:11:17.593663 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76856 (* 1 = 6.76856 loss)
I0523 06:11:18.095767 35003 sgd_solver.cpp:112] Iteration 160320, lr = 0.001
I0523 06:11:21.157500 35003 solver.cpp:239] Iteration 160330 (2.80608 iter/s, 3.56369s/10 iters), loss = 6.5508
I0523 06:11:21.157539 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5508 (* 1 = 6.5508 loss)
I0523 06:11:21.162304 35003 sgd_solver.cpp:112] Iteration 160330, lr = 0.001
I0523 06:11:23.169766 35003 solver.cpp:239] Iteration 160340 (4.96985 iter/s, 2.01213s/10 iters), loss = 7.37554
I0523 06:11:23.169807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37554 (* 1 = 7.37554 loss)
I0523 06:11:23.202054 35003 sgd_solver.cpp:112] Iteration 160340, lr = 0.001
I0523 06:11:26.718230 35003 solver.cpp:239] Iteration 160350 (2.81828 iter/s, 3.54826s/10 iters), loss = 6.84667
I0523 06:11:26.718304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84667 (* 1 = 6.84667 loss)
I0523 06:11:26.723779 35003 sgd_solver.cpp:112] Iteration 160350, lr = 0.001
I0523 06:11:30.340826 35003 solver.cpp:239] Iteration 160360 (2.76064 iter/s, 3.62234s/10 iters), loss = 6.63016
I0523 06:11:30.340903 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63016 (* 1 = 6.63016 loss)
I0523 06:11:30.348233 35003 sgd_solver.cpp:112] Iteration 160360, lr = 0.001
I0523 06:11:33.474123 35003 solver.cpp:239] Iteration 160370 (3.19174 iter/s, 3.13309s/10 iters), loss = 6.4571
I0523 06:11:33.474170 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4571 (* 1 = 6.4571 loss)
I0523 06:11:33.487041 35003 sgd_solver.cpp:112] Iteration 160370, lr = 0.001
I0523 06:11:35.517407 35003 solver.cpp:239] Iteration 160380 (4.89441 iter/s, 2.04315s/10 iters), loss = 6.64664
I0523 06:11:35.517464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64664 (* 1 = 6.64664 loss)
I0523 06:11:36.159535 35003 sgd_solver.cpp:112] Iteration 160380, lr = 0.001
I0523 06:11:40.513214 35003 solver.cpp:239] Iteration 160390 (2.00178 iter/s, 4.99554s/10 iters), loss = 5.97539
I0523 06:11:40.513273 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97539 (* 1 = 5.97539 loss)
I0523 06:11:40.577601 35003 sgd_solver.cpp:112] Iteration 160390, lr = 0.001
I0523 06:11:43.462097 35003 solver.cpp:239] Iteration 160400 (3.39132 iter/s, 2.94871s/10 iters), loss = 7.09013
I0523 06:11:43.462142 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09013 (* 1 = 7.09013 loss)
I0523 06:11:43.475705 35003 sgd_solver.cpp:112] Iteration 160400, lr = 0.001
I0523 06:11:45.121842 35003 solver.cpp:239] Iteration 160410 (6.02546 iter/s, 1.65962s/10 iters), loss = 6.60597
I0523 06:11:45.121886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60597 (* 1 = 6.60597 loss)
I0523 06:11:45.129529 35003 sgd_solver.cpp:112] Iteration 160410, lr = 0.001
I0523 06:11:48.771306 35003 solver.cpp:239] Iteration 160420 (2.74028 iter/s, 3.64927s/10 iters), loss = 6.92742
I0523 06:11:48.771507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92742 (* 1 = 6.92742 loss)
I0523 06:11:49.506306 35003 sgd_solver.cpp:112] Iteration 160420, lr = 0.001
I0523 06:11:52.414273 35003 solver.cpp:239] Iteration 160430 (2.74528 iter/s, 3.64262s/10 iters), loss = 6.92085
I0523 06:11:52.414314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92085 (* 1 = 6.92085 loss)
I0523 06:11:53.155230 35003 sgd_solver.cpp:112] Iteration 160430, lr = 0.001
I0523 06:11:57.173398 35003 solver.cpp:239] Iteration 160440 (2.10133 iter/s, 4.75889s/10 iters), loss = 6.54149
I0523 06:11:57.173450 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54149 (* 1 = 6.54149 loss)
I0523 06:11:57.179778 35003 sgd_solver.cpp:112] Iteration 160440, lr = 0.001
I0523 06:11:59.230957 35003 solver.cpp:239] Iteration 160450 (4.86046 iter/s, 2.05742s/10 iters), loss = 7.24374
I0523 06:11:59.230998 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24374 (* 1 = 7.24374 loss)
I0523 06:11:59.242239 35003 sgd_solver.cpp:112] Iteration 160450, lr = 0.001
I0523 06:12:01.608639 35003 solver.cpp:239] Iteration 160460 (4.20603 iter/s, 2.37754s/10 iters), loss = 4.67965
I0523 06:12:01.608686 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.67965 (* 1 = 4.67965 loss)
I0523 06:12:01.627225 35003 sgd_solver.cpp:112] Iteration 160460, lr = 0.001
I0523 06:12:05.591790 35003 solver.cpp:239] Iteration 160470 (2.51072 iter/s, 3.98292s/10 iters), loss = 5.5273
I0523 06:12:05.591871 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.5273 (* 1 = 5.5273 loss)
I0523 06:12:06.306738 35003 sgd_solver.cpp:112] Iteration 160470, lr = 0.001
I0523 06:12:08.392521 35003 solver.cpp:239] Iteration 160480 (3.57075 iter/s, 2.80054s/10 iters), loss = 7.85451
I0523 06:12:08.392560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85451 (* 1 = 7.85451 loss)
I0523 06:12:08.405807 35003 sgd_solver.cpp:112] Iteration 160480, lr = 0.001
I0523 06:12:12.063216 35003 solver.cpp:239] Iteration 160490 (2.72443 iter/s, 3.6705s/10 iters), loss = 6.49164
I0523 06:12:12.063266 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49164 (* 1 = 6.49164 loss)
I0523 06:12:12.081210 35003 sgd_solver.cpp:112] Iteration 160490, lr = 0.001
I0523 06:12:15.702464 35003 solver.cpp:239] Iteration 160500 (2.74797 iter/s, 3.63905s/10 iters), loss = 6.46255
I0523 06:12:15.702507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46255 (* 1 = 6.46255 loss)
I0523 06:12:15.707026 35003 sgd_solver.cpp:112] Iteration 160500, lr = 0.001
I0523 06:12:18.738534 35003 solver.cpp:239] Iteration 160510 (3.29393 iter/s, 3.03589s/10 iters), loss = 7.14487
I0523 06:12:18.738577 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14487 (* 1 = 7.14487 loss)
I0523 06:12:18.746712 35003 sgd_solver.cpp:112] Iteration 160510, lr = 0.001
I0523 06:12:21.464735 35003 solver.cpp:239] Iteration 160520 (3.66832 iter/s, 2.72604s/10 iters), loss = 7.44398
I0523 06:12:21.464965 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44398 (* 1 = 7.44398 loss)
I0523 06:12:21.473495 35003 sgd_solver.cpp:112] Iteration 160520, lr = 0.001
I0523 06:12:25.928589 35003 solver.cpp:239] Iteration 160530 (2.24041 iter/s, 4.46347s/10 iters), loss = 7.33821
I0523 06:12:25.928627 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33821 (* 1 = 7.33821 loss)
I0523 06:12:26.637536 35003 sgd_solver.cpp:112] Iteration 160530, lr = 0.001
I0523 06:12:28.705796 35003 solver.cpp:239] Iteration 160540 (3.60095 iter/s, 2.77705s/10 iters), loss = 6.6704
I0523 06:12:28.705834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6704 (* 1 = 6.6704 loss)
I0523 06:12:28.718842 35003 sgd_solver.cpp:112] Iteration 160540, lr = 0.001
I0523 06:12:34.573307 35003 solver.cpp:239] Iteration 160550 (1.70438 iter/s, 5.86722s/10 iters), loss = 5.98728
I0523 06:12:34.573359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98728 (* 1 = 5.98728 loss)
I0523 06:12:34.586452 35003 sgd_solver.cpp:112] Iteration 160550, lr = 0.001
I0523 06:12:36.623757 35003 solver.cpp:239] Iteration 160560 (4.87736 iter/s, 2.05029s/10 iters), loss = 8.09489
I0523 06:12:36.623821 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.09489 (* 1 = 8.09489 loss)
I0523 06:12:36.628707 35003 sgd_solver.cpp:112] Iteration 160560, lr = 0.001
I0523 06:12:39.424021 35003 solver.cpp:239] Iteration 160570 (3.57133 iter/s, 2.80008s/10 iters), loss = 7.14711
I0523 06:12:39.424063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14711 (* 1 = 7.14711 loss)
I0523 06:12:39.431969 35003 sgd_solver.cpp:112] Iteration 160570, lr = 0.001
I0523 06:12:43.020042 35003 solver.cpp:239] Iteration 160580 (2.781 iter/s, 3.59583s/10 iters), loss = 7.03003
I0523 06:12:43.020081 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03003 (* 1 = 7.03003 loss)
I0523 06:12:43.026842 35003 sgd_solver.cpp:112] Iteration 160580, lr = 0.001
I0523 06:12:45.072618 35003 solver.cpp:239] Iteration 160590 (4.87224 iter/s, 2.05244s/10 iters), loss = 6.36851
I0523 06:12:45.072662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36851 (* 1 = 6.36851 loss)
I0523 06:12:45.085333 35003 sgd_solver.cpp:112] Iteration 160590, lr = 0.001
I0523 06:12:48.665650 35003 solver.cpp:239] Iteration 160600 (2.78332 iter/s, 3.59283s/10 iters), loss = 6.58149
I0523 06:12:48.665695 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58149 (* 1 = 6.58149 loss)
I0523 06:12:48.688347 35003 sgd_solver.cpp:112] Iteration 160600, lr = 0.001
I0523 06:12:53.055773 35003 solver.cpp:239] Iteration 160610 (2.27796 iter/s, 4.38989s/10 iters), loss = 7.02205
I0523 06:12:53.055999 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02205 (* 1 = 7.02205 loss)
I0523 06:12:53.069072 35003 sgd_solver.cpp:112] Iteration 160610, lr = 0.001
I0523 06:12:55.522007 35003 solver.cpp:239] Iteration 160620 (4.05526 iter/s, 2.46593s/10 iters), loss = 6.58739
I0523 06:12:55.522053 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58739 (* 1 = 6.58739 loss)
I0523 06:12:55.535836 35003 sgd_solver.cpp:112] Iteration 160620, lr = 0.001
I0523 06:12:59.153774 35003 solver.cpp:239] Iteration 160630 (2.75364 iter/s, 3.63156s/10 iters), loss = 7.45055
I0523 06:12:59.153852 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45055 (* 1 = 7.45055 loss)
I0523 06:12:59.166302 35003 sgd_solver.cpp:112] Iteration 160630, lr = 0.001
I0523 06:13:02.597432 35003 solver.cpp:239] Iteration 160640 (2.90407 iter/s, 3.44344s/10 iters), loss = 7.66354
I0523 06:13:02.597481 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66354 (* 1 = 7.66354 loss)
I0523 06:13:02.609278 35003 sgd_solver.cpp:112] Iteration 160640, lr = 0.001
I0523 06:13:04.700003 35003 solver.cpp:239] Iteration 160650 (4.75641 iter/s, 2.10243s/10 iters), loss = 7.51133
I0523 06:13:04.700048 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51133 (* 1 = 7.51133 loss)
I0523 06:13:04.721608 35003 sgd_solver.cpp:112] Iteration 160650, lr = 0.001
I0523 06:13:09.010224 35003 solver.cpp:239] Iteration 160660 (2.32019 iter/s, 4.31s/10 iters), loss = 6.61992
I0523 06:13:09.010279 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61992 (* 1 = 6.61992 loss)
I0523 06:13:09.023929 35003 sgd_solver.cpp:112] Iteration 160660, lr = 0.001
I0523 06:13:11.086653 35003 solver.cpp:239] Iteration 160670 (4.8163 iter/s, 2.07628s/10 iters), loss = 6.42937
I0523 06:13:11.086691 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42937 (* 1 = 6.42937 loss)
I0523 06:13:11.105060 35003 sgd_solver.cpp:112] Iteration 160670, lr = 0.001
I0523 06:13:15.433769 35003 solver.cpp:239] Iteration 160680 (2.30049 iter/s, 4.3469s/10 iters), loss = 6.32052
I0523 06:13:15.433807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32052 (* 1 = 6.32052 loss)
I0523 06:13:16.025465 35003 sgd_solver.cpp:112] Iteration 160680, lr = 0.001
I0523 06:13:18.300562 35003 solver.cpp:239] Iteration 160690 (3.48841 iter/s, 2.86663s/10 iters), loss = 6.6214
I0523 06:13:18.300603 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6214 (* 1 = 6.6214 loss)
I0523 06:13:19.033519 35003 sgd_solver.cpp:112] Iteration 160690, lr = 0.001
I0523 06:13:23.948925 35003 solver.cpp:239] Iteration 160700 (1.77052 iter/s, 5.64805s/10 iters), loss = 6.09423
I0523 06:13:23.949101 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09423 (* 1 = 6.09423 loss)
I0523 06:13:23.967684 35003 sgd_solver.cpp:112] Iteration 160700, lr = 0.001
I0523 06:13:25.529664 35003 solver.cpp:239] Iteration 160710 (6.32714 iter/s, 1.58049s/10 iters), loss = 6.87544
I0523 06:13:25.529718 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87544 (* 1 = 6.87544 loss)
I0523 06:13:26.018656 35003 sgd_solver.cpp:112] Iteration 160710, lr = 0.001
I0523 06:13:28.908596 35003 solver.cpp:239] Iteration 160720 (2.95969 iter/s, 3.37874s/10 iters), loss = 5.91614
I0523 06:13:28.908645 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91614 (* 1 = 5.91614 loss)
I0523 06:13:29.649492 35003 sgd_solver.cpp:112] Iteration 160720, lr = 0.001
I0523 06:13:31.783082 35003 solver.cpp:239] Iteration 160730 (3.47909 iter/s, 2.87432s/10 iters), loss = 6.96119
I0523 06:13:31.783124 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96119 (* 1 = 6.96119 loss)
I0523 06:13:31.805955 35003 sgd_solver.cpp:112] Iteration 160730, lr = 0.001
I0523 06:13:36.048138 35003 solver.cpp:239] Iteration 160740 (2.34475 iter/s, 4.26484s/10 iters), loss = 6.78554
I0523 06:13:36.048182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78554 (* 1 = 6.78554 loss)
I0523 06:13:36.756470 35003 sgd_solver.cpp:112] Iteration 160740, lr = 0.001
I0523 06:13:40.895332 35003 solver.cpp:239] Iteration 160750 (2.06316 iter/s, 4.84694s/10 iters), loss = 5.98509
I0523 06:13:40.895390 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98509 (* 1 = 5.98509 loss)
I0523 06:13:41.610728 35003 sgd_solver.cpp:112] Iteration 160750, lr = 0.001
I0523 06:13:43.623572 35003 solver.cpp:239] Iteration 160760 (3.66564 iter/s, 2.72804s/10 iters), loss = 6.70521
I0523 06:13:43.623621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70521 (* 1 = 6.70521 loss)
I0523 06:13:43.636433 35003 sgd_solver.cpp:112] Iteration 160760, lr = 0.001
I0523 06:13:47.064947 35003 solver.cpp:239] Iteration 160770 (2.90964 iter/s, 3.43685s/10 iters), loss = 6.58183
I0523 06:13:47.065006 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58183 (* 1 = 6.58183 loss)
I0523 06:13:47.071326 35003 sgd_solver.cpp:112] Iteration 160770, lr = 0.001
I0523 06:13:52.418411 35003 solver.cpp:239] Iteration 160780 (1.86804 iter/s, 5.35319s/10 iters), loss = 6.05704
I0523 06:13:52.418460 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05704 (* 1 = 6.05704 loss)
I0523 06:13:53.081863 35003 sgd_solver.cpp:112] Iteration 160780, lr = 0.001
I0523 06:13:57.810436 35003 solver.cpp:239] Iteration 160790 (1.85469 iter/s, 5.39175s/10 iters), loss = 6.97425
I0523 06:13:57.810681 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97425 (* 1 = 6.97425 loss)
I0523 06:13:58.528551 35003 sgd_solver.cpp:112] Iteration 160790, lr = 0.001
I0523 06:14:02.760401 35003 solver.cpp:239] Iteration 160800 (2.02039 iter/s, 4.94955s/10 iters), loss = 7.10722
I0523 06:14:02.760449 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10722 (* 1 = 7.10722 loss)
I0523 06:14:02.764982 35003 sgd_solver.cpp:112] Iteration 160800, lr = 0.001
I0523 06:14:05.371824 35003 solver.cpp:239] Iteration 160810 (3.82956 iter/s, 2.61127s/10 iters), loss = 6.07941
I0523 06:14:05.371866 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07941 (* 1 = 6.07941 loss)
I0523 06:14:05.394084 35003 sgd_solver.cpp:112] Iteration 160810, lr = 0.001
I0523 06:14:10.428589 35003 solver.cpp:239] Iteration 160820 (1.97765 iter/s, 5.05651s/10 iters), loss = 7.37819
I0523 06:14:10.428653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37819 (* 1 = 7.37819 loss)
I0523 06:14:10.452184 35003 sgd_solver.cpp:112] Iteration 160820, lr = 0.001
I0523 06:14:15.236565 35003 solver.cpp:239] Iteration 160830 (2.07999 iter/s, 4.80772s/10 iters), loss = 6.82333
I0523 06:14:15.236613 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82333 (* 1 = 6.82333 loss)
I0523 06:14:15.284354 35003 sgd_solver.cpp:112] Iteration 160830, lr = 0.001
I0523 06:14:19.464645 35003 solver.cpp:239] Iteration 160840 (2.36527 iter/s, 4.22786s/10 iters), loss = 7.49962
I0523 06:14:19.464689 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49962 (* 1 = 7.49962 loss)
I0523 06:14:19.478701 35003 sgd_solver.cpp:112] Iteration 160840, lr = 0.001
I0523 06:14:23.808046 35003 solver.cpp:239] Iteration 160850 (2.30246 iter/s, 4.34318s/10 iters), loss = 7.31757
I0523 06:14:23.808091 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31757 (* 1 = 7.31757 loss)
I0523 06:14:24.549175 35003 sgd_solver.cpp:112] Iteration 160850, lr = 0.001
I0523 06:14:27.247709 35003 solver.cpp:239] Iteration 160860 (2.90742 iter/s, 3.43947s/10 iters), loss = 7.35698
I0523 06:14:27.247746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35698 (* 1 = 7.35698 loss)
I0523 06:14:27.260674 35003 sgd_solver.cpp:112] Iteration 160860, lr = 0.001
I0523 06:14:30.940726 35003 solver.cpp:239] Iteration 160870 (2.70796 iter/s, 3.69282s/10 iters), loss = 6.90953
I0523 06:14:30.940925 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90953 (* 1 = 6.90953 loss)
I0523 06:14:31.643115 35003 sgd_solver.cpp:112] Iteration 160870, lr = 0.001
I0523 06:14:34.546772 35003 solver.cpp:239] Iteration 160880 (2.77338 iter/s, 3.6057s/10 iters), loss = 7.64803
I0523 06:14:34.546830 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64803 (* 1 = 7.64803 loss)
I0523 06:14:35.287657 35003 sgd_solver.cpp:112] Iteration 160880, lr = 0.001
I0523 06:14:39.986479 35003 solver.cpp:239] Iteration 160890 (1.83843 iter/s, 5.43942s/10 iters), loss = 7.5276
I0523 06:14:39.986536 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5276 (* 1 = 7.5276 loss)
I0523 06:14:39.993335 35003 sgd_solver.cpp:112] Iteration 160890, lr = 0.001
I0523 06:14:44.067262 35003 solver.cpp:239] Iteration 160900 (2.45064 iter/s, 4.08056s/10 iters), loss = 7.81807
I0523 06:14:44.067299 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81807 (* 1 = 7.81807 loss)
I0523 06:14:44.092944 35003 sgd_solver.cpp:112] Iteration 160900, lr = 0.001
I0523 06:14:46.275615 35003 solver.cpp:239] Iteration 160910 (4.52855 iter/s, 2.20821s/10 iters), loss = 6.57782
I0523 06:14:46.275652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57782 (* 1 = 6.57782 loss)
I0523 06:14:46.281286 35003 sgd_solver.cpp:112] Iteration 160910, lr = 0.001
I0523 06:14:50.071370 35003 solver.cpp:239] Iteration 160920 (2.63466 iter/s, 3.79556s/10 iters), loss = 6.04794
I0523 06:14:50.071409 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04794 (* 1 = 6.04794 loss)
I0523 06:14:50.084842 35003 sgd_solver.cpp:112] Iteration 160920, lr = 0.001
I0523 06:14:53.778837 35003 solver.cpp:239] Iteration 160930 (2.6974 iter/s, 3.70727s/10 iters), loss = 6
I0523 06:14:53.778893 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6 (* 1 = 6 loss)
I0523 06:14:54.392632 35003 sgd_solver.cpp:112] Iteration 160930, lr = 0.001
I0523 06:14:57.184307 35003 solver.cpp:239] Iteration 160940 (2.93662 iter/s, 3.40527s/10 iters), loss = 6.18032
I0523 06:14:57.184343 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18032 (* 1 = 6.18032 loss)
I0523 06:14:57.197950 35003 sgd_solver.cpp:112] Iteration 160940, lr = 0.001
I0523 06:14:59.210005 35003 solver.cpp:239] Iteration 160950 (4.93688 iter/s, 2.02557s/10 iters), loss = 7.58977
I0523 06:14:59.210052 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58977 (* 1 = 7.58977 loss)
I0523 06:14:59.911198 35003 sgd_solver.cpp:112] Iteration 160950, lr = 0.001
I0523 06:15:03.807549 35003 solver.cpp:239] Iteration 160960 (2.17519 iter/s, 4.5973s/10 iters), loss = 6.7495
I0523 06:15:03.807816 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7495 (* 1 = 6.7495 loss)
I0523 06:15:03.969434 35003 sgd_solver.cpp:112] Iteration 160960, lr = 0.001
I0523 06:15:07.312005 35003 solver.cpp:239] Iteration 160970 (2.85382 iter/s, 3.50408s/10 iters), loss = 6.75974
I0523 06:15:07.312043 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75974 (* 1 = 6.75974 loss)
I0523 06:15:08.026814 35003 sgd_solver.cpp:112] Iteration 160970, lr = 0.001
I0523 06:15:10.966840 35003 solver.cpp:239] Iteration 160980 (2.73624 iter/s, 3.65464s/10 iters), loss = 6.49942
I0523 06:15:10.966878 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49942 (* 1 = 6.49942 loss)
I0523 06:15:11.382361 35003 sgd_solver.cpp:112] Iteration 160980, lr = 0.001
I0523 06:15:14.139794 35003 solver.cpp:239] Iteration 160990 (3.15181 iter/s, 3.17278s/10 iters), loss = 7.34916
I0523 06:15:14.139839 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34916 (* 1 = 7.34916 loss)
I0523 06:15:14.157255 35003 sgd_solver.cpp:112] Iteration 160990, lr = 0.001
I0523 06:15:18.338026 35003 solver.cpp:239] Iteration 161000 (2.38208 iter/s, 4.19801s/10 iters), loss = 6.95558
I0523 06:15:18.338068 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95558 (* 1 = 6.95558 loss)
I0523 06:15:18.343946 35003 sgd_solver.cpp:112] Iteration 161000, lr = 0.001
I0523 06:15:21.310616 35003 solver.cpp:239] Iteration 161010 (3.36426 iter/s, 2.97242s/10 iters), loss = 5.63878
I0523 06:15:21.310653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.63878 (* 1 = 5.63878 loss)
I0523 06:15:21.320478 35003 sgd_solver.cpp:112] Iteration 161010, lr = 0.001
I0523 06:15:25.304695 35003 solver.cpp:239] Iteration 161020 (2.50383 iter/s, 3.99388s/10 iters), loss = 5.68928
I0523 06:15:25.304734 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.68928 (* 1 = 5.68928 loss)
I0523 06:15:25.317622 35003 sgd_solver.cpp:112] Iteration 161020, lr = 0.001
I0523 06:15:28.731227 35003 solver.cpp:239] Iteration 161030 (2.91856 iter/s, 3.42635s/10 iters), loss = 5.54683
I0523 06:15:28.731266 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.54683 (* 1 = 5.54683 loss)
I0523 06:15:28.745028 35003 sgd_solver.cpp:112] Iteration 161030, lr = 0.001
I0523 06:15:32.444977 35003 solver.cpp:239] Iteration 161040 (2.69285 iter/s, 3.71353s/10 iters), loss = 6.57033
I0523 06:15:32.445039 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57033 (* 1 = 6.57033 loss)
I0523 06:15:32.456852 35003 sgd_solver.cpp:112] Iteration 161040, lr = 0.001
I0523 06:15:37.882308 35003 solver.cpp:239] Iteration 161050 (1.83924 iter/s, 5.43704s/10 iters), loss = 6.48866
I0523 06:15:37.882550 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48866 (* 1 = 6.48866 loss)
I0523 06:15:37.885303 35003 sgd_solver.cpp:112] Iteration 161050, lr = 0.001
I0523 06:15:40.735522 35003 solver.cpp:239] Iteration 161060 (3.50524 iter/s, 2.85287s/10 iters), loss = 5.65416
I0523 06:15:40.735561 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.65416 (* 1 = 5.65416 loss)
I0523 06:15:40.744091 35003 sgd_solver.cpp:112] Iteration 161060, lr = 0.001
I0523 06:15:43.575589 35003 solver.cpp:239] Iteration 161070 (3.52124 iter/s, 2.83991s/10 iters), loss = 6.00224
I0523 06:15:43.575626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00224 (* 1 = 6.00224 loss)
I0523 06:15:43.888691 35003 sgd_solver.cpp:112] Iteration 161070, lr = 0.001
I0523 06:15:47.324682 35003 solver.cpp:239] Iteration 161080 (2.66745 iter/s, 3.74889s/10 iters), loss = 7.09285
I0523 06:15:47.324731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09285 (* 1 = 7.09285 loss)
I0523 06:15:47.478245 35003 sgd_solver.cpp:112] Iteration 161080, lr = 0.001
I0523 06:15:50.526087 35003 solver.cpp:239] Iteration 161090 (3.12381 iter/s, 3.20122s/10 iters), loss = 6.79247
I0523 06:15:50.526125 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79247 (* 1 = 6.79247 loss)
I0523 06:15:50.539171 35003 sgd_solver.cpp:112] Iteration 161090, lr = 0.001
I0523 06:15:55.568663 35003 solver.cpp:239] Iteration 161100 (1.98321 iter/s, 5.04233s/10 iters), loss = 6.33592
I0523 06:15:55.568714 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33592 (* 1 = 6.33592 loss)
I0523 06:15:55.577281 35003 sgd_solver.cpp:112] Iteration 161100, lr = 0.001
I0523 06:15:57.776075 35003 solver.cpp:239] Iteration 161110 (4.53049 iter/s, 2.20726s/10 iters), loss = 5.24447
I0523 06:15:57.776119 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.24447 (* 1 = 5.24447 loss)
I0523 06:15:57.779119 35003 sgd_solver.cpp:112] Iteration 161110, lr = 0.001
I0523 06:16:02.020413 35003 solver.cpp:239] Iteration 161120 (2.35621 iter/s, 4.2441s/10 iters), loss = 6.39295
I0523 06:16:02.020452 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39295 (* 1 = 6.39295 loss)
I0523 06:16:02.042915 35003 sgd_solver.cpp:112] Iteration 161120, lr = 0.001
I0523 06:16:05.525451 35003 solver.cpp:239] Iteration 161130 (2.85319 iter/s, 3.50485s/10 iters), loss = 6.21711
I0523 06:16:05.525496 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21711 (* 1 = 6.21711 loss)
I0523 06:16:05.530923 35003 sgd_solver.cpp:112] Iteration 161130, lr = 0.001
I0523 06:16:09.883975 35003 solver.cpp:239] Iteration 161140 (2.29447 iter/s, 4.3583s/10 iters), loss = 5.84498
I0523 06:16:09.884198 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84498 (* 1 = 5.84498 loss)
I0523 06:16:09.891450 35003 sgd_solver.cpp:112] Iteration 161140, lr = 0.001
I0523 06:16:12.679009 35003 solver.cpp:239] Iteration 161150 (3.57818 iter/s, 2.79471s/10 iters), loss = 7.30097
I0523 06:16:12.679088 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30097 (* 1 = 7.30097 loss)
I0523 06:16:12.691773 35003 sgd_solver.cpp:112] Iteration 161150, lr = 0.001
I0523 06:16:16.931571 35003 solver.cpp:239] Iteration 161160 (2.35166 iter/s, 4.25231s/10 iters), loss = 5.54012
I0523 06:16:16.931610 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.54012 (* 1 = 5.54012 loss)
I0523 06:16:17.633497 35003 sgd_solver.cpp:112] Iteration 161160, lr = 0.001
I0523 06:16:20.590924 35003 solver.cpp:239] Iteration 161170 (2.73287 iter/s, 3.65916s/10 iters), loss = 7.05737
I0523 06:16:20.590962 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05737 (* 1 = 7.05737 loss)
I0523 06:16:20.599107 35003 sgd_solver.cpp:112] Iteration 161170, lr = 0.001
I0523 06:16:24.054172 35003 solver.cpp:239] Iteration 161180 (2.88762 iter/s, 3.46306s/10 iters), loss = 5.32136
I0523 06:16:24.054229 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.32136 (* 1 = 5.32136 loss)
I0523 06:16:24.072490 35003 sgd_solver.cpp:112] Iteration 161180, lr = 0.001
I0523 06:16:28.501956 35003 solver.cpp:239] Iteration 161190 (2.24843 iter/s, 4.44755s/10 iters), loss = 6.75376
I0523 06:16:28.501997 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75376 (* 1 = 6.75376 loss)
I0523 06:16:28.613374 35003 sgd_solver.cpp:112] Iteration 161190, lr = 0.001
I0523 06:16:30.938258 35003 solver.cpp:239] Iteration 161200 (4.10483 iter/s, 2.43616s/10 iters), loss = 6.6159
I0523 06:16:30.938313 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6159 (* 1 = 6.6159 loss)
I0523 06:16:31.428086 35003 sgd_solver.cpp:112] Iteration 161200, lr = 0.001
I0523 06:16:36.030665 35003 solver.cpp:239] Iteration 161210 (1.96381 iter/s, 5.09215s/10 iters), loss = 7.61717
I0523 06:16:36.030707 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61717 (* 1 = 7.61717 loss)
I0523 06:16:36.038395 35003 sgd_solver.cpp:112] Iteration 161210, lr = 0.001
I0523 06:16:40.068338 35003 solver.cpp:239] Iteration 161220 (2.4768 iter/s, 4.03746s/10 iters), loss = 6.85324
I0523 06:16:40.068500 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85324 (* 1 = 6.85324 loss)
I0523 06:16:40.077679 35003 sgd_solver.cpp:112] Iteration 161220, lr = 0.001
I0523 06:16:43.744524 35003 solver.cpp:239] Iteration 161230 (2.72043 iter/s, 3.67589s/10 iters), loss = 6.57738
I0523 06:16:43.744565 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57738 (* 1 = 6.57738 loss)
I0523 06:16:44.479460 35003 sgd_solver.cpp:112] Iteration 161230, lr = 0.001
I0523 06:16:47.958050 35003 solver.cpp:239] Iteration 161240 (2.37344 iter/s, 4.21329s/10 iters), loss = 6.37417
I0523 06:16:47.958093 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37417 (* 1 = 6.37417 loss)
I0523 06:16:47.962482 35003 sgd_solver.cpp:112] Iteration 161240, lr = 0.001
I0523 06:16:50.527245 35003 solver.cpp:239] Iteration 161250 (3.89253 iter/s, 2.56902s/10 iters), loss = 6.53799
I0523 06:16:50.527305 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53799 (* 1 = 6.53799 loss)
I0523 06:16:51.229542 35003 sgd_solver.cpp:112] Iteration 161250, lr = 0.001
I0523 06:16:54.911417 35003 solver.cpp:239] Iteration 161260 (2.28106 iter/s, 4.38394s/10 iters), loss = 6.76194
I0523 06:16:54.911466 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76194 (* 1 = 6.76194 loss)
I0523 06:16:54.930168 35003 sgd_solver.cpp:112] Iteration 161260, lr = 0.001
I0523 06:16:57.826465 35003 solver.cpp:239] Iteration 161270 (3.43068 iter/s, 2.91487s/10 iters), loss = 7.06314
I0523 06:16:57.826505 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06314 (* 1 = 7.06314 loss)
I0523 06:16:57.839915 35003 sgd_solver.cpp:112] Iteration 161270, lr = 0.001
I0523 06:17:02.932540 35003 solver.cpp:239] Iteration 161280 (1.95855 iter/s, 5.10583s/10 iters), loss = 6.89997
I0523 06:17:02.932593 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89997 (* 1 = 6.89997 loss)
I0523 06:17:02.936228 35003 sgd_solver.cpp:112] Iteration 161280, lr = 0.001
I0523 06:17:05.882382 35003 solver.cpp:239] Iteration 161290 (3.39022 iter/s, 2.94966s/10 iters), loss = 8.36297
I0523 06:17:05.882436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.36297 (* 1 = 8.36297 loss)
I0523 06:17:06.271865 35003 sgd_solver.cpp:112] Iteration 161290, lr = 0.001
I0523 06:17:09.206012 35003 solver.cpp:239] Iteration 161300 (3.00894 iter/s, 3.32343s/10 iters), loss = 7.00782
I0523 06:17:09.206059 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00782 (* 1 = 7.00782 loss)
I0523 06:17:09.927156 35003 sgd_solver.cpp:112] Iteration 161300, lr = 0.001
I0523 06:17:13.568541 35003 solver.cpp:239] Iteration 161310 (2.29238 iter/s, 4.36228s/10 iters), loss = 7.60708
I0523 06:17:13.568709 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60708 (* 1 = 7.60708 loss)
I0523 06:17:13.586575 35003 sgd_solver.cpp:112] Iteration 161310, lr = 0.001
I0523 06:17:16.508309 35003 solver.cpp:239] Iteration 161320 (3.40198 iter/s, 2.93947s/10 iters), loss = 6.86964
I0523 06:17:16.508390 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86964 (* 1 = 6.86964 loss)
I0523 06:17:17.217455 35003 sgd_solver.cpp:112] Iteration 161320, lr = 0.001
I0523 06:17:20.742898 35003 solver.cpp:239] Iteration 161330 (2.36164 iter/s, 4.23435s/10 iters), loss = 7.46876
I0523 06:17:20.742943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46876 (* 1 = 7.46876 loss)
I0523 06:17:21.442140 35003 sgd_solver.cpp:112] Iteration 161330, lr = 0.001
I0523 06:17:24.296496 35003 solver.cpp:239] Iteration 161340 (2.8142 iter/s, 3.55341s/10 iters), loss = 5.738
I0523 06:17:24.296543 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.738 (* 1 = 5.738 loss)
I0523 06:17:24.306143 35003 sgd_solver.cpp:112] Iteration 161340, lr = 0.001
I0523 06:17:26.755791 35003 solver.cpp:239] Iteration 161350 (4.06651 iter/s, 2.45911s/10 iters), loss = 7.45764
I0523 06:17:26.755844 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45764 (* 1 = 7.45764 loss)
I0523 06:17:26.768996 35003 sgd_solver.cpp:112] Iteration 161350, lr = 0.001
I0523 06:17:29.605294 35003 solver.cpp:239] Iteration 161360 (3.50961 iter/s, 2.84932s/10 iters), loss = 6.293
I0523 06:17:29.605350 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.293 (* 1 = 6.293 loss)
I0523 06:17:29.609783 35003 sgd_solver.cpp:112] Iteration 161360, lr = 0.001
I0523 06:17:32.253453 35003 solver.cpp:239] Iteration 161370 (3.77646 iter/s, 2.64798s/10 iters), loss = 5.84087
I0523 06:17:32.253497 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84087 (* 1 = 5.84087 loss)
I0523 06:17:32.267082 35003 sgd_solver.cpp:112] Iteration 161370, lr = 0.001
I0523 06:17:35.001446 35003 solver.cpp:239] Iteration 161380 (3.63923 iter/s, 2.74783s/10 iters), loss = 6.39199
I0523 06:17:35.001485 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39199 (* 1 = 6.39199 loss)
I0523 06:17:35.014598 35003 sgd_solver.cpp:112] Iteration 161380, lr = 0.001
I0523 06:17:38.636503 35003 solver.cpp:239] Iteration 161390 (2.75113 iter/s, 3.63487s/10 iters), loss = 6.11618
I0523 06:17:38.636545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11618 (* 1 = 6.11618 loss)
I0523 06:17:39.318215 35003 sgd_solver.cpp:112] Iteration 161390, lr = 0.001
I0523 06:17:41.388990 35003 solver.cpp:239] Iteration 161400 (3.6333 iter/s, 2.75232s/10 iters), loss = 5.64882
I0523 06:17:41.389032 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.64882 (* 1 = 5.64882 loss)
I0523 06:17:42.040057 35003 sgd_solver.cpp:112] Iteration 161400, lr = 0.001
I0523 06:17:44.873468 35003 solver.cpp:239] Iteration 161410 (2.87003 iter/s, 3.48429s/10 iters), loss = 6.4599
I0523 06:17:44.873636 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4599 (* 1 = 6.4599 loss)
I0523 06:17:44.886507 35003 sgd_solver.cpp:112] Iteration 161410, lr = 0.001
I0523 06:17:48.828575 35003 solver.cpp:239] Iteration 161420 (2.52858 iter/s, 3.95478s/10 iters), loss = 6.4168
I0523 06:17:48.828619 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4168 (* 1 = 6.4168 loss)
I0523 06:17:48.832494 35003 sgd_solver.cpp:112] Iteration 161420, lr = 0.001
I0523 06:17:51.831326 35003 solver.cpp:239] Iteration 161430 (3.33048 iter/s, 3.00257s/10 iters), loss = 6.46431
I0523 06:17:51.831382 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46431 (* 1 = 6.46431 loss)
I0523 06:17:52.451073 35003 sgd_solver.cpp:112] Iteration 161430, lr = 0.001
I0523 06:17:54.532061 35003 solver.cpp:239] Iteration 161440 (3.70293 iter/s, 2.70056s/10 iters), loss = 6.75001
I0523 06:17:54.532114 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75001 (* 1 = 6.75001 loss)
I0523 06:17:55.272832 35003 sgd_solver.cpp:112] Iteration 161440, lr = 0.001
I0523 06:17:59.666533 35003 solver.cpp:239] Iteration 161450 (1.94772 iter/s, 5.13421s/10 iters), loss = 6.51971
I0523 06:17:59.666576 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51971 (* 1 = 6.51971 loss)
I0523 06:17:59.679723 35003 sgd_solver.cpp:112] Iteration 161450, lr = 0.001
I0523 06:18:03.320999 35003 solver.cpp:239] Iteration 161460 (2.73653 iter/s, 3.65426s/10 iters), loss = 7.14645
I0523 06:18:03.321051 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14645 (* 1 = 7.14645 loss)
I0523 06:18:04.049556 35003 sgd_solver.cpp:112] Iteration 161460, lr = 0.001
I0523 06:18:06.288290 35003 solver.cpp:239] Iteration 161470 (3.37028 iter/s, 2.96711s/10 iters), loss = 6.16079
I0523 06:18:06.288331 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16079 (* 1 = 6.16079 loss)
I0523 06:18:07.010304 35003 sgd_solver.cpp:112] Iteration 161470, lr = 0.001
I0523 06:18:11.321007 35003 solver.cpp:239] Iteration 161480 (1.9871 iter/s, 5.03247s/10 iters), loss = 5.74348
I0523 06:18:11.321054 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74348 (* 1 = 5.74348 loss)
I0523 06:18:11.340098 35003 sgd_solver.cpp:112] Iteration 161480, lr = 0.001
I0523 06:18:14.896654 35003 solver.cpp:239] Iteration 161490 (2.79685 iter/s, 3.57545s/10 iters), loss = 7.05029
I0523 06:18:14.896780 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05029 (* 1 = 7.05029 loss)
I0523 06:18:14.904748 35003 sgd_solver.cpp:112] Iteration 161490, lr = 0.001
I0523 06:18:19.857110 35003 solver.cpp:239] Iteration 161500 (2.01608 iter/s, 4.96013s/10 iters), loss = 6.50585
I0523 06:18:19.857148 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50585 (* 1 = 6.50585 loss)
I0523 06:18:20.130524 35003 sgd_solver.cpp:112] Iteration 161500, lr = 0.001
I0523 06:18:23.571627 35003 solver.cpp:239] Iteration 161510 (2.69228 iter/s, 3.71432s/10 iters), loss = 6.22743
I0523 06:18:23.571673 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22743 (* 1 = 6.22743 loss)
I0523 06:18:23.578927 35003 sgd_solver.cpp:112] Iteration 161510, lr = 0.001
I0523 06:18:26.422209 35003 solver.cpp:239] Iteration 161520 (3.50828 iter/s, 2.8504s/10 iters), loss = 6.12807
I0523 06:18:26.422258 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12807 (* 1 = 6.12807 loss)
I0523 06:18:26.430835 35003 sgd_solver.cpp:112] Iteration 161520, lr = 0.001
I0523 06:18:29.467548 35003 solver.cpp:239] Iteration 161530 (3.2839 iter/s, 3.04516s/10 iters), loss = 7.31749
I0523 06:18:29.467586 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31749 (* 1 = 7.31749 loss)
I0523 06:18:30.175539 35003 sgd_solver.cpp:112] Iteration 161530, lr = 0.001
I0523 06:18:32.298375 35003 solver.cpp:239] Iteration 161540 (3.53274 iter/s, 2.83066s/10 iters), loss = 7.21486
I0523 06:18:32.298419 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21486 (* 1 = 7.21486 loss)
I0523 06:18:32.446673 35003 sgd_solver.cpp:112] Iteration 161540, lr = 0.001
I0523 06:18:37.137045 35003 solver.cpp:239] Iteration 161550 (2.06679 iter/s, 4.83841s/10 iters), loss = 6.38858
I0523 06:18:37.137104 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38858 (* 1 = 6.38858 loss)
I0523 06:18:37.320719 35003 sgd_solver.cpp:112] Iteration 161550, lr = 0.001
I0523 06:18:40.956517 35003 solver.cpp:239] Iteration 161560 (2.61831 iter/s, 3.81925s/10 iters), loss = 6.60515
I0523 06:18:40.956560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60515 (* 1 = 6.60515 loss)
I0523 06:18:40.968228 35003 sgd_solver.cpp:112] Iteration 161560, lr = 0.001
I0523 06:18:42.302979 35003 solver.cpp:239] Iteration 161570 (7.42748 iter/s, 1.34635s/10 iters), loss = 5.34464
I0523 06:18:42.303032 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.34464 (* 1 = 5.34464 loss)
I0523 06:18:42.978813 35003 sgd_solver.cpp:112] Iteration 161570, lr = 0.001
I0523 06:18:46.478802 35003 solver.cpp:239] Iteration 161580 (2.39487 iter/s, 4.1756s/10 iters), loss = 6.03872
I0523 06:18:46.479104 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03872 (* 1 = 6.03872 loss)
I0523 06:18:46.491847 35003 sgd_solver.cpp:112] Iteration 161580, lr = 0.001
I0523 06:18:49.878543 35003 solver.cpp:239] Iteration 161590 (2.94177 iter/s, 3.39932s/10 iters), loss = 5.72249
I0523 06:18:49.878589 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.72249 (* 1 = 5.72249 loss)
I0523 06:18:49.887300 35003 sgd_solver.cpp:112] Iteration 161590, lr = 0.001
I0523 06:18:51.993906 35003 solver.cpp:239] Iteration 161600 (4.72763 iter/s, 2.11522s/10 iters), loss = 5.96005
I0523 06:18:51.993957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96005 (* 1 = 5.96005 loss)
I0523 06:18:52.715853 35003 sgd_solver.cpp:112] Iteration 161600, lr = 0.001
I0523 06:18:55.742980 35003 solver.cpp:239] Iteration 161610 (2.66747 iter/s, 3.74886s/10 iters), loss = 6.45297
I0523 06:18:55.743023 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45297 (* 1 = 6.45297 loss)
I0523 06:18:55.748682 35003 sgd_solver.cpp:112] Iteration 161610, lr = 0.001
I0523 06:18:58.188201 35003 solver.cpp:239] Iteration 161620 (4.08986 iter/s, 2.44507s/10 iters), loss = 6.86364
I0523 06:18:58.188246 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86364 (* 1 = 6.86364 loss)
I0523 06:18:58.201203 35003 sgd_solver.cpp:112] Iteration 161620, lr = 0.001
I0523 06:19:01.016342 35003 solver.cpp:239] Iteration 161630 (3.5361 iter/s, 2.82797s/10 iters), loss = 5.30206
I0523 06:19:01.016381 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.30206 (* 1 = 5.30206 loss)
I0523 06:19:01.021051 35003 sgd_solver.cpp:112] Iteration 161630, lr = 0.001
I0523 06:19:04.540334 35003 solver.cpp:239] Iteration 161640 (2.83784 iter/s, 3.52381s/10 iters), loss = 6.60697
I0523 06:19:04.540380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60697 (* 1 = 6.60697 loss)
I0523 06:19:04.553812 35003 sgd_solver.cpp:112] Iteration 161640, lr = 0.001
I0523 06:19:07.414273 35003 solver.cpp:239] Iteration 161650 (3.47975 iter/s, 2.87377s/10 iters), loss = 6.63472
I0523 06:19:07.414311 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63472 (* 1 = 6.63472 loss)
I0523 06:19:07.422013 35003 sgd_solver.cpp:112] Iteration 161650, lr = 0.001
I0523 06:19:10.162905 35003 solver.cpp:239] Iteration 161660 (3.63839 iter/s, 2.74847s/10 iters), loss = 6.07781
I0523 06:19:10.162951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07781 (* 1 = 6.07781 loss)
I0523 06:19:10.171048 35003 sgd_solver.cpp:112] Iteration 161660, lr = 0.001
I0523 06:19:12.301081 35003 solver.cpp:239] Iteration 161670 (4.6772 iter/s, 2.13803s/10 iters), loss = 7.67027
I0523 06:19:12.301129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67027 (* 1 = 7.67027 loss)
I0523 06:19:12.311064 35003 sgd_solver.cpp:112] Iteration 161670, lr = 0.001
I0523 06:19:15.886627 35003 solver.cpp:239] Iteration 161680 (2.78913 iter/s, 3.58535s/10 iters), loss = 6.06949
I0523 06:19:15.886674 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06949 (* 1 = 6.06949 loss)
I0523 06:19:15.901744 35003 sgd_solver.cpp:112] Iteration 161680, lr = 0.001
I0523 06:19:19.371882 35003 solver.cpp:239] Iteration 161690 (2.86943 iter/s, 3.48502s/10 iters), loss = 7.21683
I0523 06:19:19.372017 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21683 (* 1 = 7.21683 loss)
I0523 06:19:19.379954 35003 sgd_solver.cpp:112] Iteration 161690, lr = 0.001
I0523 06:19:24.431124 35003 solver.cpp:239] Iteration 161700 (1.97671 iter/s, 5.0589s/10 iters), loss = 7.24654
I0523 06:19:24.431162 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24654 (* 1 = 7.24654 loss)
I0523 06:19:24.445156 35003 sgd_solver.cpp:112] Iteration 161700, lr = 0.001
I0523 06:19:28.061584 35003 solver.cpp:239] Iteration 161710 (2.75462 iter/s, 3.63027s/10 iters), loss = 7.71235
I0523 06:19:28.061636 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71235 (* 1 = 7.71235 loss)
I0523 06:19:28.069232 35003 sgd_solver.cpp:112] Iteration 161710, lr = 0.001
I0523 06:19:31.183074 35003 solver.cpp:239] Iteration 161720 (3.20379 iter/s, 3.1213s/10 iters), loss = 7.49987
I0523 06:19:31.183135 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49987 (* 1 = 7.49987 loss)
I0523 06:19:31.892243 35003 sgd_solver.cpp:112] Iteration 161720, lr = 0.001
I0523 06:19:36.181396 35003 solver.cpp:239] Iteration 161730 (2.00078 iter/s, 4.99806s/10 iters), loss = 6.49237
I0523 06:19:36.181440 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49237 (* 1 = 6.49237 loss)
I0523 06:19:36.194725 35003 sgd_solver.cpp:112] Iteration 161730, lr = 0.001
I0523 06:19:39.033499 35003 solver.cpp:239] Iteration 161740 (3.50639 iter/s, 2.85194s/10 iters), loss = 6.31531
I0523 06:19:39.033547 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31531 (* 1 = 6.31531 loss)
I0523 06:19:39.748870 35003 sgd_solver.cpp:112] Iteration 161740, lr = 0.001
I0523 06:19:43.409690 35003 solver.cpp:239] Iteration 161750 (2.28522 iter/s, 4.37595s/10 iters), loss = 6.92794
I0523 06:19:43.409754 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92794 (* 1 = 6.92794 loss)
I0523 06:19:43.417547 35003 sgd_solver.cpp:112] Iteration 161750, lr = 0.001
I0523 06:19:47.668028 35003 solver.cpp:239] Iteration 161760 (2.34846 iter/s, 4.2581s/10 iters), loss = 6.88502
I0523 06:19:47.668081 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88502 (* 1 = 6.88502 loss)
I0523 06:19:47.680863 35003 sgd_solver.cpp:112] Iteration 161760, lr = 0.001
I0523 06:19:51.224066 35003 solver.cpp:239] Iteration 161770 (2.81228 iter/s, 3.55584s/10 iters), loss = 6.36604
I0523 06:19:51.224294 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36604 (* 1 = 6.36604 loss)
I0523 06:19:51.234789 35003 sgd_solver.cpp:112] Iteration 161770, lr = 0.001
I0523 06:19:56.736790 35003 solver.cpp:239] Iteration 161780 (1.81415 iter/s, 5.51222s/10 iters), loss = 5.658
I0523 06:19:56.736881 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.658 (* 1 = 5.658 loss)
I0523 06:19:57.326930 35003 sgd_solver.cpp:112] Iteration 161780, lr = 0.001
I0523 06:20:00.028019 35003 solver.cpp:239] Iteration 161790 (3.03859 iter/s, 3.291s/10 iters), loss = 7.19127
I0523 06:20:00.028072 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19127 (* 1 = 7.19127 loss)
I0523 06:20:00.046202 35003 sgd_solver.cpp:112] Iteration 161790, lr = 0.001
I0523 06:20:03.376376 35003 solver.cpp:239] Iteration 161800 (2.98672 iter/s, 3.34816s/10 iters), loss = 6.92484
I0523 06:20:03.376442 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92484 (* 1 = 6.92484 loss)
I0523 06:20:04.107745 35003 sgd_solver.cpp:112] Iteration 161800, lr = 0.001
I0523 06:20:06.215265 35003 solver.cpp:239] Iteration 161810 (3.52273 iter/s, 2.8387s/10 iters), loss = 6.082
I0523 06:20:06.215309 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.082 (* 1 = 6.082 loss)
I0523 06:20:06.223019 35003 sgd_solver.cpp:112] Iteration 161810, lr = 0.001
I0523 06:20:09.822130 35003 solver.cpp:239] Iteration 161820 (2.77264 iter/s, 3.60667s/10 iters), loss = 5.56476
I0523 06:20:09.822193 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.56476 (* 1 = 5.56476 loss)
I0523 06:20:10.439033 35003 sgd_solver.cpp:112] Iteration 161820, lr = 0.001
I0523 06:20:14.140239 35003 solver.cpp:239] Iteration 161830 (2.31596 iter/s, 4.31787s/10 iters), loss = 6.9369
I0523 06:20:14.140295 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9369 (* 1 = 6.9369 loss)
I0523 06:20:14.147173 35003 sgd_solver.cpp:112] Iteration 161830, lr = 0.001
I0523 06:20:17.485028 35003 solver.cpp:239] Iteration 161840 (2.9899 iter/s, 3.3446s/10 iters), loss = 6.42041
I0523 06:20:17.485080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42041 (* 1 = 6.42041 loss)
I0523 06:20:18.098811 35003 sgd_solver.cpp:112] Iteration 161840, lr = 0.001
I0523 06:20:21.592772 35003 solver.cpp:239] Iteration 161850 (2.43456 iter/s, 4.10752s/10 iters), loss = 7.43423
I0523 06:20:21.593082 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43423 (* 1 = 7.43423 loss)
I0523 06:20:21.606068 35003 sgd_solver.cpp:112] Iteration 161850, lr = 0.001
I0523 06:20:24.514467 35003 solver.cpp:239] Iteration 161860 (3.42316 iter/s, 2.92128s/10 iters), loss = 6.06697
I0523 06:20:24.514509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06697 (* 1 = 6.06697 loss)
I0523 06:20:25.235569 35003 sgd_solver.cpp:112] Iteration 161860, lr = 0.001
I0523 06:20:28.099351 35003 solver.cpp:239] Iteration 161870 (2.78966 iter/s, 3.58467s/10 iters), loss = 5.85174
I0523 06:20:28.099397 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85174 (* 1 = 5.85174 loss)
I0523 06:20:28.827358 35003 sgd_solver.cpp:112] Iteration 161870, lr = 0.001
I0523 06:20:33.053102 35003 solver.cpp:239] Iteration 161880 (2.01878 iter/s, 4.9535s/10 iters), loss = 5.18215
I0523 06:20:33.053145 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.18215 (* 1 = 5.18215 loss)
I0523 06:20:33.062726 35003 sgd_solver.cpp:112] Iteration 161880, lr = 0.001
I0523 06:20:36.007153 35003 solver.cpp:239] Iteration 161890 (3.38538 iter/s, 2.95388s/10 iters), loss = 6.78356
I0523 06:20:36.007197 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78356 (* 1 = 6.78356 loss)
I0523 06:20:36.015100 35003 sgd_solver.cpp:112] Iteration 161890, lr = 0.001
I0523 06:20:38.058789 35003 solver.cpp:239] Iteration 161900 (4.87448 iter/s, 2.0515s/10 iters), loss = 6.5794
I0523 06:20:38.058827 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5794 (* 1 = 6.5794 loss)
I0523 06:20:38.072219 35003 sgd_solver.cpp:112] Iteration 161900, lr = 0.001
I0523 06:20:40.848629 35003 solver.cpp:239] Iteration 161910 (3.58464 iter/s, 2.78968s/10 iters), loss = 6.58787
I0523 06:20:40.848683 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58787 (* 1 = 6.58787 loss)
I0523 06:20:41.585834 35003 sgd_solver.cpp:112] Iteration 161910, lr = 0.001
I0523 06:20:43.634979 35003 solver.cpp:239] Iteration 161920 (3.58914 iter/s, 2.78618s/10 iters), loss = 6.60229
I0523 06:20:43.635027 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60229 (* 1 = 6.60229 loss)
I0523 06:20:43.648442 35003 sgd_solver.cpp:112] Iteration 161920, lr = 0.001
I0523 06:20:46.481113 35003 solver.cpp:239] Iteration 161930 (3.51376 iter/s, 2.84596s/10 iters), loss = 5.73688
I0523 06:20:46.481178 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.73688 (* 1 = 5.73688 loss)
I0523 06:20:46.616137 35003 sgd_solver.cpp:112] Iteration 161930, lr = 0.001
I0523 06:20:49.494837 35003 solver.cpp:239] Iteration 161940 (3.32315 iter/s, 3.00919s/10 iters), loss = 7.20463
I0523 06:20:49.494891 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20463 (* 1 = 7.20463 loss)
I0523 06:20:50.229223 35003 sgd_solver.cpp:112] Iteration 161940, lr = 0.001
I0523 06:20:53.883508 35003 solver.cpp:239] Iteration 161950 (2.27872 iter/s, 4.38843s/10 iters), loss = 5.55912
I0523 06:20:53.883728 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.55912 (* 1 = 5.55912 loss)
I0523 06:20:53.891224 35003 sgd_solver.cpp:112] Iteration 161950, lr = 0.001
I0523 06:20:57.494132 35003 solver.cpp:239] Iteration 161960 (2.76987 iter/s, 3.61027s/10 iters), loss = 7.29899
I0523 06:20:57.494184 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29899 (* 1 = 7.29899 loss)
I0523 06:20:57.512110 35003 sgd_solver.cpp:112] Iteration 161960, lr = 0.001
I0523 06:21:00.349772 35003 solver.cpp:239] Iteration 161970 (3.50205 iter/s, 2.85547s/10 iters), loss = 6.75443
I0523 06:21:00.349808 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75443 (* 1 = 6.75443 loss)
I0523 06:21:00.363054 35003 sgd_solver.cpp:112] Iteration 161970, lr = 0.001
I0523 06:21:04.478394 35003 solver.cpp:239] Iteration 161980 (2.42224 iter/s, 4.12841s/10 iters), loss = 7.94492
I0523 06:21:04.478440 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94492 (* 1 = 7.94492 loss)
I0523 06:21:04.490938 35003 sgd_solver.cpp:112] Iteration 161980, lr = 0.001
I0523 06:21:07.218554 35003 solver.cpp:239] Iteration 161990 (3.64963 iter/s, 2.74s/10 iters), loss = 5.72552
I0523 06:21:07.218600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.72552 (* 1 = 5.72552 loss)
I0523 06:21:07.957703 35003 sgd_solver.cpp:112] Iteration 161990, lr = 0.001
I0523 06:21:12.341141 35003 solver.cpp:239] Iteration 162000 (1.95224 iter/s, 5.12233s/10 iters), loss = 6.10631
I0523 06:21:12.341193 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10631 (* 1 = 6.10631 loss)
I0523 06:21:12.351084 35003 sgd_solver.cpp:112] Iteration 162000, lr = 0.001
I0523 06:21:13.813503 35003 solver.cpp:239] Iteration 162010 (6.7924 iter/s, 1.47223s/10 iters), loss = 7.37731
I0523 06:21:13.813550 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37731 (* 1 = 7.37731 loss)
I0523 06:21:13.818053 35003 sgd_solver.cpp:112] Iteration 162010, lr = 0.001
I0523 06:21:18.053557 35003 solver.cpp:239] Iteration 162020 (2.35858 iter/s, 4.23983s/10 iters), loss = 7.16899
I0523 06:21:18.053594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16899 (* 1 = 7.16899 loss)
I0523 06:21:18.066454 35003 sgd_solver.cpp:112] Iteration 162020, lr = 0.001
I0523 06:21:21.634491 35003 solver.cpp:239] Iteration 162030 (2.79272 iter/s, 3.58074s/10 iters), loss = 5.85793
I0523 06:21:21.634539 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85793 (* 1 = 5.85793 loss)
I0523 06:21:21.638305 35003 sgd_solver.cpp:112] Iteration 162030, lr = 0.001
I0523 06:21:26.063946 35003 solver.cpp:239] Iteration 162040 (2.25774 iter/s, 4.42921s/10 iters), loss = 6.74653
I0523 06:21:26.064136 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74653 (* 1 = 6.74653 loss)
I0523 06:21:26.766541 35003 sgd_solver.cpp:112] Iteration 162040, lr = 0.001
I0523 06:21:30.531090 35003 solver.cpp:239] Iteration 162050 (2.23875 iter/s, 4.46677s/10 iters), loss = 7.30131
I0523 06:21:30.531141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30131 (* 1 = 7.30131 loss)
I0523 06:21:31.169682 35003 sgd_solver.cpp:112] Iteration 162050, lr = 0.001
I0523 06:21:33.416357 35003 solver.cpp:239] Iteration 162060 (3.46609 iter/s, 2.8851s/10 iters), loss = 7.11879
I0523 06:21:33.416396 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11879 (* 1 = 7.11879 loss)
I0523 06:21:33.448657 35003 sgd_solver.cpp:112] Iteration 162060, lr = 0.001
I0523 06:21:37.093093 35003 solver.cpp:239] Iteration 162070 (2.71995 iter/s, 3.67654s/10 iters), loss = 5.96957
I0523 06:21:37.093135 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96957 (* 1 = 5.96957 loss)
I0523 06:21:37.111693 35003 sgd_solver.cpp:112] Iteration 162070, lr = 0.001
I0523 06:21:39.640673 35003 solver.cpp:239] Iteration 162080 (3.92554 iter/s, 2.54742s/10 iters), loss = 7.09504
I0523 06:21:39.640733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09504 (* 1 = 7.09504 loss)
I0523 06:21:40.375124 35003 sgd_solver.cpp:112] Iteration 162080, lr = 0.001
I0523 06:21:41.668190 35003 solver.cpp:239] Iteration 162090 (4.93251 iter/s, 2.02737s/10 iters), loss = 5.86184
I0523 06:21:41.668251 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.86184 (* 1 = 5.86184 loss)
I0523 06:21:41.681113 35003 sgd_solver.cpp:112] Iteration 162090, lr = 0.001
I0523 06:21:46.004783 35003 solver.cpp:239] Iteration 162100 (2.30608 iter/s, 4.33636s/10 iters), loss = 6.56082
I0523 06:21:46.004828 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56082 (* 1 = 6.56082 loss)
I0523 06:21:46.688410 35003 sgd_solver.cpp:112] Iteration 162100, lr = 0.001
I0523 06:21:48.745566 35003 solver.cpp:239] Iteration 162110 (3.64882 iter/s, 2.74061s/10 iters), loss = 6.70671
I0523 06:21:48.745611 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70671 (* 1 = 6.70671 loss)
I0523 06:21:48.750730 35003 sgd_solver.cpp:112] Iteration 162110, lr = 0.001
I0523 06:21:51.139060 35003 solver.cpp:239] Iteration 162120 (4.17827 iter/s, 2.39333s/10 iters), loss = 7.06526
I0523 06:21:51.139130 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06526 (* 1 = 7.06526 loss)
I0523 06:21:51.827826 35003 sgd_solver.cpp:112] Iteration 162120, lr = 0.001
I0523 06:21:54.874660 35003 solver.cpp:239] Iteration 162130 (2.67711 iter/s, 3.73537s/10 iters), loss = 5.77839
I0523 06:21:54.874732 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77839 (* 1 = 5.77839 loss)
I0523 06:21:54.881727 35003 sgd_solver.cpp:112] Iteration 162130, lr = 0.001
I0523 06:21:58.730832 35003 solver.cpp:239] Iteration 162140 (2.5934 iter/s, 3.85594s/10 iters), loss = 6.35331
I0523 06:21:58.731019 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35331 (* 1 = 6.35331 loss)
I0523 06:21:58.742575 35003 sgd_solver.cpp:112] Iteration 162140, lr = 0.001
I0523 06:22:02.356746 35003 solver.cpp:239] Iteration 162150 (2.75819 iter/s, 3.62557s/10 iters), loss = 7.1797
I0523 06:22:02.356786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1797 (* 1 = 7.1797 loss)
I0523 06:22:02.361268 35003 sgd_solver.cpp:112] Iteration 162150, lr = 0.001
I0523 06:22:05.916250 35003 solver.cpp:239] Iteration 162160 (2.80953 iter/s, 3.55932s/10 iters), loss = 6.90549
I0523 06:22:05.916293 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90549 (* 1 = 6.90549 loss)
I0523 06:22:05.921563 35003 sgd_solver.cpp:112] Iteration 162160, lr = 0.001
I0523 06:22:11.760857 35003 solver.cpp:239] Iteration 162170 (1.71106 iter/s, 5.84432s/10 iters), loss = 7.04052
I0523 06:22:11.761286 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04052 (* 1 = 7.04052 loss)
I0523 06:22:11.779389 35003 sgd_solver.cpp:112] Iteration 162170, lr = 0.001
I0523 06:22:15.357337 35003 solver.cpp:239] Iteration 162180 (2.78094 iter/s, 3.59591s/10 iters), loss = 6.68466
I0523 06:22:15.357394 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68466 (* 1 = 6.68466 loss)
I0523 06:22:15.361191 35003 sgd_solver.cpp:112] Iteration 162180, lr = 0.001
I0523 06:22:20.449126 35003 solver.cpp:239] Iteration 162190 (1.96405 iter/s, 5.09152s/10 iters), loss = 6.75904
I0523 06:22:20.449164 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75904 (* 1 = 6.75904 loss)
I0523 06:22:20.453528 35003 sgd_solver.cpp:112] Iteration 162190, lr = 0.001
I0523 06:22:23.791314 35003 solver.cpp:239] Iteration 162200 (2.99222 iter/s, 3.342s/10 iters), loss = 7.83209
I0523 06:22:23.791359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83209 (* 1 = 7.83209 loss)
I0523 06:22:24.467866 35003 sgd_solver.cpp:112] Iteration 162200, lr = 0.001
I0523 06:22:27.871943 35003 solver.cpp:239] Iteration 162210 (2.45073 iter/s, 4.08041s/10 iters), loss = 6.61347
I0523 06:22:27.871982 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61347 (* 1 = 6.61347 loss)
I0523 06:22:27.885043 35003 sgd_solver.cpp:112] Iteration 162210, lr = 0.001
I0523 06:22:32.178160 35003 solver.cpp:239] Iteration 162220 (2.32234 iter/s, 4.306s/10 iters), loss = 7.65095
I0523 06:22:32.178277 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65095 (* 1 = 7.65095 loss)
I0523 06:22:32.366732 35003 sgd_solver.cpp:112] Iteration 162220, lr = 0.001
I0523 06:22:36.641126 35003 solver.cpp:239] Iteration 162230 (2.24081 iter/s, 4.46266s/10 iters), loss = 4.96353
I0523 06:22:36.641185 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.96353 (* 1 = 4.96353 loss)
I0523 06:22:37.188293 35003 sgd_solver.cpp:112] Iteration 162230, lr = 0.001
I0523 06:22:39.551117 35003 solver.cpp:239] Iteration 162240 (3.43665 iter/s, 2.90981s/10 iters), loss = 6.42655
I0523 06:22:39.551153 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42655 (* 1 = 6.42655 loss)
I0523 06:22:39.567365 35003 sgd_solver.cpp:112] Iteration 162240, lr = 0.001
I0523 06:22:43.206779 35003 solver.cpp:239] Iteration 162250 (2.73563 iter/s, 3.65547s/10 iters), loss = 6.91707
I0523 06:22:43.206825 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91707 (* 1 = 6.91707 loss)
I0523 06:22:43.215698 35003 sgd_solver.cpp:112] Iteration 162250, lr = 0.001
I0523 06:22:47.761198 35003 solver.cpp:239] Iteration 162260 (2.19578 iter/s, 4.55419s/10 iters), loss = 7.25205
I0523 06:22:47.761240 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25205 (* 1 = 7.25205 loss)
I0523 06:22:47.779654 35003 sgd_solver.cpp:112] Iteration 162260, lr = 0.001
I0523 06:22:49.976171 35003 solver.cpp:239] Iteration 162270 (4.51502 iter/s, 2.21483s/10 iters), loss = 6.64684
I0523 06:22:49.976214 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64684 (* 1 = 6.64684 loss)
I0523 06:22:49.983353 35003 sgd_solver.cpp:112] Iteration 162270, lr = 0.001
I0523 06:22:53.653040 35003 solver.cpp:239] Iteration 162280 (2.71985 iter/s, 3.67667s/10 iters), loss = 5.46907
I0523 06:22:53.653090 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.46907 (* 1 = 5.46907 loss)
I0523 06:22:54.361914 35003 sgd_solver.cpp:112] Iteration 162280, lr = 0.001
I0523 06:22:56.987028 35003 solver.cpp:239] Iteration 162290 (2.99958 iter/s, 3.3338s/10 iters), loss = 6.36068
I0523 06:22:56.987072 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36068 (* 1 = 6.36068 loss)
I0523 06:22:57.000324 35003 sgd_solver.cpp:112] Iteration 162290, lr = 0.001
I0523 06:23:00.271478 35003 solver.cpp:239] Iteration 162300 (3.04482 iter/s, 3.28427s/10 iters), loss = 7.28363
I0523 06:23:00.271517 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28363 (* 1 = 7.28363 loss)
I0523 06:23:00.276535 35003 sgd_solver.cpp:112] Iteration 162300, lr = 0.001
I0523 06:23:01.959465 35003 solver.cpp:239] Iteration 162310 (5.92462 iter/s, 1.68787s/10 iters), loss = 6.96512
I0523 06:23:01.959507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96512 (* 1 = 6.96512 loss)
I0523 06:23:01.966112 35003 sgd_solver.cpp:112] Iteration 162310, lr = 0.001
I0523 06:23:05.438799 35003 solver.cpp:239] Iteration 162320 (2.87427 iter/s, 3.47914s/10 iters), loss = 7.44973
I0523 06:23:05.438992 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44973 (* 1 = 7.44973 loss)
I0523 06:23:05.461719 35003 sgd_solver.cpp:112] Iteration 162320, lr = 0.001
I0523 06:23:09.233734 35003 solver.cpp:239] Iteration 162330 (2.63534 iter/s, 3.79458s/10 iters), loss = 6.62866
I0523 06:23:09.233780 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62866 (* 1 = 6.62866 loss)
I0523 06:23:09.246850 35003 sgd_solver.cpp:112] Iteration 162330, lr = 0.001
I0523 06:23:13.614444 35003 solver.cpp:239] Iteration 162340 (2.28285 iter/s, 4.38048s/10 iters), loss = 6.74838
I0523 06:23:13.614485 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74838 (* 1 = 6.74838 loss)
I0523 06:23:13.644924 35003 sgd_solver.cpp:112] Iteration 162340, lr = 0.001
I0523 06:23:16.501142 35003 solver.cpp:239] Iteration 162350 (3.46437 iter/s, 2.88653s/10 iters), loss = 6.79352
I0523 06:23:16.501189 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79352 (* 1 = 6.79352 loss)
I0523 06:23:16.509816 35003 sgd_solver.cpp:112] Iteration 162350, lr = 0.001
I0523 06:23:19.061952 35003 solver.cpp:239] Iteration 162360 (3.90526 iter/s, 2.56065s/10 iters), loss = 6.3851
I0523 06:23:19.062002 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3851 (* 1 = 6.3851 loss)
I0523 06:23:19.075243 35003 sgd_solver.cpp:112] Iteration 162360, lr = 0.001
I0523 06:23:22.652222 35003 solver.cpp:239] Iteration 162370 (2.78546 iter/s, 3.59007s/10 iters), loss = 5.87133
I0523 06:23:22.652267 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87133 (* 1 = 5.87133 loss)
I0523 06:23:22.661353 35003 sgd_solver.cpp:112] Iteration 162370, lr = 0.001
I0523 06:23:25.466909 35003 solver.cpp:239] Iteration 162380 (3.553 iter/s, 2.81452s/10 iters), loss = 7.68647
I0523 06:23:25.466951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68647 (* 1 = 7.68647 loss)
I0523 06:23:25.535136 35003 sgd_solver.cpp:112] Iteration 162380, lr = 0.001
I0523 06:23:29.868044 35003 solver.cpp:239] Iteration 162390 (2.27226 iter/s, 4.4009s/10 iters), loss = 6.35488
I0523 06:23:29.868089 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35488 (* 1 = 6.35488 loss)
I0523 06:23:29.871708 35003 sgd_solver.cpp:112] Iteration 162390, lr = 0.001
I0523 06:23:32.146131 35003 solver.cpp:239] Iteration 162400 (4.38993 iter/s, 2.27794s/10 iters), loss = 6.95944
I0523 06:23:32.146178 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95944 (* 1 = 6.95944 loss)
I0523 06:23:32.153189 35003 sgd_solver.cpp:112] Iteration 162400, lr = 0.001
I0523 06:23:34.208678 35003 solver.cpp:239] Iteration 162410 (4.8487 iter/s, 2.06241s/10 iters), loss = 6.69696
I0523 06:23:34.208721 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69696 (* 1 = 6.69696 loss)
I0523 06:23:34.217034 35003 sgd_solver.cpp:112] Iteration 162410, lr = 0.001
I0523 06:23:36.371378 35003 solver.cpp:239] Iteration 162420 (4.62414 iter/s, 2.16256s/10 iters), loss = 6.2285
I0523 06:23:36.371605 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2285 (* 1 = 6.2285 loss)
I0523 06:23:36.384580 35003 sgd_solver.cpp:112] Iteration 162420, lr = 0.001
I0523 06:23:38.497462 35003 solver.cpp:239] Iteration 162430 (4.70415 iter/s, 2.12578s/10 iters), loss = 6.4588
I0523 06:23:38.497505 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4588 (* 1 = 6.4588 loss)
I0523 06:23:38.510509 35003 sgd_solver.cpp:112] Iteration 162430, lr = 0.001
I0523 06:23:42.039103 35003 solver.cpp:239] Iteration 162440 (2.82371 iter/s, 3.54143s/10 iters), loss = 7.34694
I0523 06:23:42.039153 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34694 (* 1 = 7.34694 loss)
I0523 06:23:42.042692 35003 sgd_solver.cpp:112] Iteration 162440, lr = 0.001
I0523 06:23:45.085058 35003 solver.cpp:239] Iteration 162450 (3.28327 iter/s, 3.04574s/10 iters), loss = 7.59251
I0523 06:23:45.085101 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59251 (* 1 = 7.59251 loss)
I0523 06:23:45.184491 35003 sgd_solver.cpp:112] Iteration 162450, lr = 0.001
I0523 06:23:48.973242 35003 solver.cpp:239] Iteration 162460 (2.57203 iter/s, 3.88798s/10 iters), loss = 5.7552
I0523 06:23:48.973281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7552 (* 1 = 5.7552 loss)
I0523 06:23:48.992975 35003 sgd_solver.cpp:112] Iteration 162460, lr = 0.001
I0523 06:23:52.261860 35003 solver.cpp:239] Iteration 162470 (3.04096 iter/s, 3.28844s/10 iters), loss = 5.57446
I0523 06:23:52.261907 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.57446 (* 1 = 5.57446 loss)
I0523 06:23:52.931049 35003 sgd_solver.cpp:112] Iteration 162470, lr = 0.001
I0523 06:23:57.318137 35003 solver.cpp:239] Iteration 162480 (1.97784 iter/s, 5.05602s/10 iters), loss = 6.87815
I0523 06:23:57.318183 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87815 (* 1 = 6.87815 loss)
I0523 06:23:57.389751 35003 sgd_solver.cpp:112] Iteration 162480, lr = 0.001
I0523 06:24:02.095568 35003 solver.cpp:239] Iteration 162490 (2.09328 iter/s, 4.77719s/10 iters), loss = 6.2894
I0523 06:24:02.095607 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2894 (* 1 = 6.2894 loss)
I0523 06:24:02.109180 35003 sgd_solver.cpp:112] Iteration 162490, lr = 0.001
I0523 06:24:05.739106 35003 solver.cpp:239] Iteration 162500 (2.74473 iter/s, 3.64335s/10 iters), loss = 6.51183
I0523 06:24:05.739151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51183 (* 1 = 6.51183 loss)
I0523 06:24:05.757876 35003 sgd_solver.cpp:112] Iteration 162500, lr = 0.001
I0523 06:24:09.357555 35003 solver.cpp:239] Iteration 162510 (2.76376 iter/s, 3.61826s/10 iters), loss = 6.76728
I0523 06:24:09.357671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76728 (* 1 = 6.76728 loss)
I0523 06:24:09.384306 35003 sgd_solver.cpp:112] Iteration 162510, lr = 0.001
I0523 06:24:12.254956 35003 solver.cpp:239] Iteration 162520 (3.45165 iter/s, 2.89716s/10 iters), loss = 7.60286
I0523 06:24:12.254997 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60286 (* 1 = 7.60286 loss)
I0523 06:24:12.273619 35003 sgd_solver.cpp:112] Iteration 162520, lr = 0.001
I0523 06:24:15.135993 35003 solver.cpp:239] Iteration 162530 (3.47117 iter/s, 2.88087s/10 iters), loss = 6.09024
I0523 06:24:15.136039 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09024 (* 1 = 6.09024 loss)
I0523 06:24:15.877341 35003 sgd_solver.cpp:112] Iteration 162530, lr = 0.001
I0523 06:24:18.754117 35003 solver.cpp:239] Iteration 162540 (2.76402 iter/s, 3.61792s/10 iters), loss = 6.27376
I0523 06:24:18.754159 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27376 (* 1 = 6.27376 loss)
I0523 06:24:18.776829 35003 sgd_solver.cpp:112] Iteration 162540, lr = 0.001
I0523 06:24:22.711345 35003 solver.cpp:239] Iteration 162550 (2.52715 iter/s, 3.95702s/10 iters), loss = 6.56595
I0523 06:24:22.711408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56595 (* 1 = 6.56595 loss)
I0523 06:24:22.901191 35003 sgd_solver.cpp:112] Iteration 162550, lr = 0.001
I0523 06:24:25.700634 35003 solver.cpp:239] Iteration 162560 (3.3455 iter/s, 2.98909s/10 iters), loss = 7.22305
I0523 06:24:25.700692 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22305 (* 1 = 7.22305 loss)
I0523 06:24:26.441445 35003 sgd_solver.cpp:112] Iteration 162560, lr = 0.001
I0523 06:24:29.313330 35003 solver.cpp:239] Iteration 162570 (2.76817 iter/s, 3.61249s/10 iters), loss = 5.04496
I0523 06:24:29.313369 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.04496 (* 1 = 5.04496 loss)
I0523 06:24:29.326328 35003 sgd_solver.cpp:112] Iteration 162570, lr = 0.001
I0523 06:24:32.922915 35003 solver.cpp:239] Iteration 162580 (2.77055 iter/s, 3.6094s/10 iters), loss = 7.41456
I0523 06:24:32.922962 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41456 (* 1 = 7.41456 loss)
I0523 06:24:32.935930 35003 sgd_solver.cpp:112] Iteration 162580, lr = 0.001
I0523 06:24:34.255024 35003 solver.cpp:239] Iteration 162590 (7.50751 iter/s, 1.332s/10 iters), loss = 5.50249
I0523 06:24:34.255074 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.50249 (* 1 = 5.50249 loss)
I0523 06:24:34.970688 35003 sgd_solver.cpp:112] Iteration 162590, lr = 0.001
I0523 06:24:40.681958 35003 solver.cpp:239] Iteration 162600 (1.55603 iter/s, 6.4266s/10 iters), loss = 6.52114
I0523 06:24:40.682142 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52114 (* 1 = 6.52114 loss)
I0523 06:24:40.699843 35003 sgd_solver.cpp:112] Iteration 162600, lr = 0.001
I0523 06:24:44.595432 35003 solver.cpp:239] Iteration 162610 (2.55838 iter/s, 3.90872s/10 iters), loss = 5.80119
I0523 06:24:44.595475 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.80119 (* 1 = 5.80119 loss)
I0523 06:24:44.609356 35003 sgd_solver.cpp:112] Iteration 162610, lr = 0.001
I0523 06:24:46.944437 35003 solver.cpp:239] Iteration 162620 (4.25739 iter/s, 2.34885s/10 iters), loss = 6.22999
I0523 06:24:46.944486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22999 (* 1 = 6.22999 loss)
I0523 06:24:47.685278 35003 sgd_solver.cpp:112] Iteration 162620, lr = 0.001
I0523 06:24:52.834197 35003 solver.cpp:239] Iteration 162630 (1.69794 iter/s, 5.88948s/10 iters), loss = 6.94071
I0523 06:24:52.834231 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94071 (* 1 = 6.94071 loss)
I0523 06:24:52.847764 35003 sgd_solver.cpp:112] Iteration 162630, lr = 0.001
I0523 06:24:55.350057 35003 solver.cpp:239] Iteration 162640 (3.97502 iter/s, 2.51571s/10 iters), loss = 6.3981
I0523 06:24:55.350108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3981 (* 1 = 6.3981 loss)
I0523 06:24:56.084476 35003 sgd_solver.cpp:112] Iteration 162640, lr = 0.001
I0523 06:25:00.489316 35003 solver.cpp:239] Iteration 162650 (1.9459 iter/s, 5.139s/10 iters), loss = 6.97058
I0523 06:25:00.489365 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97058 (* 1 = 6.97058 loss)
I0523 06:25:00.492974 35003 sgd_solver.cpp:112] Iteration 162650, lr = 0.001
I0523 06:25:04.897387 35003 solver.cpp:239] Iteration 162660 (2.26869 iter/s, 4.40783s/10 iters), loss = 5.22581
I0523 06:25:04.897430 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.22581 (* 1 = 5.22581 loss)
I0523 06:25:04.901098 35003 sgd_solver.cpp:112] Iteration 162660, lr = 0.001
I0523 06:25:07.028970 35003 solver.cpp:239] Iteration 162670 (4.69167 iter/s, 2.13144s/10 iters), loss = 7.50655
I0523 06:25:07.029021 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50655 (* 1 = 7.50655 loss)
I0523 06:25:07.252122 35003 sgd_solver.cpp:112] Iteration 162670, lr = 0.001
I0523 06:25:09.672968 35003 solver.cpp:239] Iteration 162680 (3.78239 iter/s, 2.64383s/10 iters), loss = 6.85309
I0523 06:25:09.673012 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85309 (* 1 = 6.85309 loss)
I0523 06:25:09.677520 35003 sgd_solver.cpp:112] Iteration 162680, lr = 0.001
I0523 06:25:12.392446 35003 solver.cpp:239] Iteration 162690 (3.67741 iter/s, 2.71931s/10 iters), loss = 7.26347
I0523 06:25:12.392663 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26347 (* 1 = 7.26347 loss)
I0523 06:25:12.875661 35003 sgd_solver.cpp:112] Iteration 162690, lr = 0.001
I0523 06:25:15.045307 35003 solver.cpp:239] Iteration 162700 (3.76998 iter/s, 2.65253s/10 iters), loss = 6.26866
I0523 06:25:15.045363 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26866 (* 1 = 6.26866 loss)
I0523 06:25:15.052597 35003 sgd_solver.cpp:112] Iteration 162700, lr = 0.001
I0523 06:25:17.797873 35003 solver.cpp:239] Iteration 162710 (3.63321 iter/s, 2.75239s/10 iters), loss = 6.81584
I0523 06:25:17.797922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81584 (* 1 = 6.81584 loss)
I0523 06:25:17.807576 35003 sgd_solver.cpp:112] Iteration 162710, lr = 0.001
I0523 06:25:19.914742 35003 solver.cpp:239] Iteration 162720 (4.72428 iter/s, 2.11672s/10 iters), loss = 6.91035
I0523 06:25:19.914780 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91035 (* 1 = 6.91035 loss)
I0523 06:25:19.920320 35003 sgd_solver.cpp:112] Iteration 162720, lr = 0.001
I0523 06:25:23.342785 35003 solver.cpp:239] Iteration 162730 (2.91727 iter/s, 3.42786s/10 iters), loss = 6.55678
I0523 06:25:23.342828 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55678 (* 1 = 6.55678 loss)
I0523 06:25:23.355264 35003 sgd_solver.cpp:112] Iteration 162730, lr = 0.001
I0523 06:25:26.325398 35003 solver.cpp:239] Iteration 162740 (3.35295 iter/s, 2.98244s/10 iters), loss = 6.17073
I0523 06:25:26.325443 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17073 (* 1 = 6.17073 loss)
I0523 06:25:27.065917 35003 sgd_solver.cpp:112] Iteration 162740, lr = 0.001
I0523 06:25:30.160804 35003 solver.cpp:239] Iteration 162750 (2.60742 iter/s, 3.8352s/10 iters), loss = 6.48875
I0523 06:25:30.160842 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48875 (* 1 = 6.48875 loss)
I0523 06:25:30.168782 35003 sgd_solver.cpp:112] Iteration 162750, lr = 0.001
I0523 06:25:33.672768 35003 solver.cpp:239] Iteration 162760 (2.84756 iter/s, 3.51178s/10 iters), loss = 6.76552
I0523 06:25:33.672816 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76552 (* 1 = 6.76552 loss)
I0523 06:25:33.685910 35003 sgd_solver.cpp:112] Iteration 162760, lr = 0.001
I0523 06:25:37.404814 35003 solver.cpp:239] Iteration 162770 (2.67964 iter/s, 3.73185s/10 iters), loss = 5.86608
I0523 06:25:37.404856 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.86608 (* 1 = 5.86608 loss)
I0523 06:25:38.133536 35003 sgd_solver.cpp:112] Iteration 162770, lr = 0.001
I0523 06:25:41.668699 35003 solver.cpp:239] Iteration 162780 (2.3454 iter/s, 4.26367s/10 iters), loss = 6.37715
I0523 06:25:41.668738 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37715 (* 1 = 6.37715 loss)
I0523 06:25:41.673265 35003 sgd_solver.cpp:112] Iteration 162780, lr = 0.001
I0523 06:25:42.976963 35003 solver.cpp:239] Iteration 162790 (7.64436 iter/s, 1.30815s/10 iters), loss = 6.90387
I0523 06:25:42.977200 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90387 (* 1 = 6.90387 loss)
I0523 06:25:43.609100 35003 sgd_solver.cpp:112] Iteration 162790, lr = 0.001
I0523 06:25:47.281280 35003 solver.cpp:239] Iteration 162800 (2.32346 iter/s, 4.30393s/10 iters), loss = 6.80055
I0523 06:25:47.281329 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80055 (* 1 = 6.80055 loss)
I0523 06:25:47.670727 35003 sgd_solver.cpp:112] Iteration 162800, lr = 0.001
I0523 06:25:51.297262 35003 solver.cpp:239] Iteration 162810 (2.49018 iter/s, 4.01577s/10 iters), loss = 6.62416
I0523 06:25:51.297302 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62416 (* 1 = 6.62416 loss)
I0523 06:25:51.303004 35003 sgd_solver.cpp:112] Iteration 162810, lr = 0.001
I0523 06:25:54.905961 35003 solver.cpp:239] Iteration 162820 (2.77123 iter/s, 3.60851s/10 iters), loss = 6.1565
I0523 06:25:54.906013 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1565 (* 1 = 6.1565 loss)
I0523 06:25:54.912556 35003 sgd_solver.cpp:112] Iteration 162820, lr = 0.001
I0523 06:25:56.856272 35003 solver.cpp:239] Iteration 162830 (5.12776 iter/s, 1.95017s/10 iters), loss = 6.65467
I0523 06:25:56.856323 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65467 (* 1 = 6.65467 loss)
I0523 06:25:56.867300 35003 sgd_solver.cpp:112] Iteration 162830, lr = 0.001
I0523 06:26:00.334990 35003 solver.cpp:239] Iteration 162840 (2.87478 iter/s, 3.47852s/10 iters), loss = 6.50927
I0523 06:26:00.335038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50927 (* 1 = 6.50927 loss)
I0523 06:26:00.348071 35003 sgd_solver.cpp:112] Iteration 162840, lr = 0.001
I0523 06:26:06.053746 35003 solver.cpp:239] Iteration 162850 (1.74872 iter/s, 5.71848s/10 iters), loss = 7.23314
I0523 06:26:06.053783 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23314 (* 1 = 7.23314 loss)
I0523 06:26:06.068092 35003 sgd_solver.cpp:112] Iteration 162850, lr = 0.001
I0523 06:26:09.668977 35003 solver.cpp:239] Iteration 162860 (2.76622 iter/s, 3.61504s/10 iters), loss = 6.36697
I0523 06:26:09.669029 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36697 (* 1 = 6.36697 loss)
I0523 06:26:10.403142 35003 sgd_solver.cpp:112] Iteration 162860, lr = 0.001
I0523 06:26:14.050979 35003 solver.cpp:239] Iteration 162870 (2.28218 iter/s, 4.38177s/10 iters), loss = 6.24311
I0523 06:26:14.051271 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24311 (* 1 = 6.24311 loss)
I0523 06:26:14.064237 35003 sgd_solver.cpp:112] Iteration 162870, lr = 0.001
I0523 06:26:18.347123 35003 solver.cpp:239] Iteration 162880 (2.32791 iter/s, 4.2957s/10 iters), loss = 7.23339
I0523 06:26:18.347177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23339 (* 1 = 7.23339 loss)
I0523 06:26:19.061947 35003 sgd_solver.cpp:112] Iteration 162880, lr = 0.001
I0523 06:26:22.519731 35003 solver.cpp:239] Iteration 162890 (2.39671 iter/s, 4.17238s/10 iters), loss = 5.95377
I0523 06:26:22.519779 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95377 (* 1 = 5.95377 loss)
I0523 06:26:22.532616 35003 sgd_solver.cpp:112] Iteration 162890, lr = 0.001
I0523 06:26:23.832271 35003 solver.cpp:239] Iteration 162900 (7.61945 iter/s, 1.31243s/10 iters), loss = 6.32808
I0523 06:26:23.832312 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32808 (* 1 = 6.32808 loss)
I0523 06:26:23.838943 35003 sgd_solver.cpp:112] Iteration 162900, lr = 0.001
I0523 06:26:26.760134 35003 solver.cpp:239] Iteration 162910 (3.41568 iter/s, 2.92767s/10 iters), loss = 5.42921
I0523 06:26:26.760188 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.42921 (* 1 = 5.42921 loss)
I0523 06:26:27.494709 35003 sgd_solver.cpp:112] Iteration 162910, lr = 0.001
I0523 06:26:31.647462 35003 solver.cpp:239] Iteration 162920 (2.04621 iter/s, 4.88707s/10 iters), loss = 5.4725
I0523 06:26:31.647507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.4725 (* 1 = 5.4725 loss)
I0523 06:26:31.660518 35003 sgd_solver.cpp:112] Iteration 162920, lr = 0.001
I0523 06:26:34.502621 35003 solver.cpp:239] Iteration 162930 (3.50264 iter/s, 2.85499s/10 iters), loss = 6.19564
I0523 06:26:34.502661 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19564 (* 1 = 6.19564 loss)
I0523 06:26:34.522533 35003 sgd_solver.cpp:112] Iteration 162930, lr = 0.001
I0523 06:26:37.600000 35003 solver.cpp:239] Iteration 162940 (3.22872 iter/s, 3.09721s/10 iters), loss = 7.52189
I0523 06:26:37.600046 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52189 (* 1 = 7.52189 loss)
I0523 06:26:37.608312 35003 sgd_solver.cpp:112] Iteration 162940, lr = 0.001
I0523 06:26:41.020906 35003 solver.cpp:239] Iteration 162950 (2.92337 iter/s, 3.42071s/10 iters), loss = 6.60926
I0523 06:26:41.020948 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60926 (* 1 = 6.60926 loss)
I0523 06:26:41.715693 35003 sgd_solver.cpp:112] Iteration 162950, lr = 0.001
I0523 06:26:44.378104 35003 solver.cpp:239] Iteration 162960 (2.97884 iter/s, 3.35701s/10 iters), loss = 6.62807
I0523 06:26:44.378248 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62807 (* 1 = 6.62807 loss)
I0523 06:26:44.396481 35003 sgd_solver.cpp:112] Iteration 162960, lr = 0.001
I0523 06:26:49.151521 35003 solver.cpp:239] Iteration 162970 (2.09508 iter/s, 4.77308s/10 iters), loss = 6.34581
I0523 06:26:49.151564 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34581 (* 1 = 6.34581 loss)
I0523 06:26:49.172205 35003 sgd_solver.cpp:112] Iteration 162970, lr = 0.001
I0523 06:26:52.868104 35003 solver.cpp:239] Iteration 162980 (2.69079 iter/s, 3.71638s/10 iters), loss = 6.8982
I0523 06:26:52.868156 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8982 (* 1 = 6.8982 loss)
I0523 06:26:53.602741 35003 sgd_solver.cpp:112] Iteration 162980, lr = 0.001
I0523 06:26:58.517341 35003 solver.cpp:239] Iteration 162990 (1.77024 iter/s, 5.64896s/10 iters), loss = 6.7121
I0523 06:26:58.517385 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7121 (* 1 = 6.7121 loss)
I0523 06:26:58.535745 35003 sgd_solver.cpp:112] Iteration 162990, lr = 0.001
I0523 06:27:01.834307 35003 solver.cpp:239] Iteration 163000 (3.01497 iter/s, 3.31678s/10 iters), loss = 6.85015
I0523 06:27:01.834357 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85015 (* 1 = 6.85015 loss)
I0523 06:27:02.347431 35003 sgd_solver.cpp:112] Iteration 163000, lr = 0.001
I0523 06:27:04.008473 35003 solver.cpp:239] Iteration 163010 (4.59978 iter/s, 2.17402s/10 iters), loss = 6.97708
I0523 06:27:04.008525 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97708 (* 1 = 6.97708 loss)
I0523 06:27:04.021010 35003 sgd_solver.cpp:112] Iteration 163010, lr = 0.001
I0523 06:27:06.929412 35003 solver.cpp:239] Iteration 163020 (3.42376 iter/s, 2.92077s/10 iters), loss = 6.25232
I0523 06:27:06.929455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25232 (* 1 = 6.25232 loss)
I0523 06:27:06.940481 35003 sgd_solver.cpp:112] Iteration 163020, lr = 0.001
I0523 06:27:11.399504 35003 solver.cpp:239] Iteration 163030 (2.2372 iter/s, 4.46986s/10 iters), loss = 7.56359
I0523 06:27:11.399550 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56359 (* 1 = 7.56359 loss)
I0523 06:27:12.137332 35003 sgd_solver.cpp:112] Iteration 163030, lr = 0.001
I0523 06:27:14.123936 35003 solver.cpp:239] Iteration 163040 (3.67072 iter/s, 2.72426s/10 iters), loss = 6.71712
I0523 06:27:14.124003 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71712 (* 1 = 6.71712 loss)
I0523 06:27:14.864687 35003 sgd_solver.cpp:112] Iteration 163040, lr = 0.001
I0523 06:27:18.510679 35003 solver.cpp:239] Iteration 163050 (2.27972 iter/s, 4.3865s/10 iters), loss = 6.94637
I0523 06:27:18.510754 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94637 (* 1 = 6.94637 loss)
I0523 06:27:18.531520 35003 sgd_solver.cpp:112] Iteration 163050, lr = 0.001
I0523 06:27:21.368120 35003 solver.cpp:239] Iteration 163060 (3.49988 iter/s, 2.85724s/10 iters), loss = 7.14235
I0523 06:27:21.368166 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14235 (* 1 = 7.14235 loss)
I0523 06:27:21.375033 35003 sgd_solver.cpp:112] Iteration 163060, lr = 0.001
I0523 06:27:25.039770 35003 solver.cpp:239] Iteration 163070 (2.72374 iter/s, 3.67143s/10 iters), loss = 6.2803
I0523 06:27:25.039813 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2803 (* 1 = 6.2803 loss)
I0523 06:27:25.053180 35003 sgd_solver.cpp:112] Iteration 163070, lr = 0.001
I0523 06:27:26.995080 35003 solver.cpp:239] Iteration 163080 (5.11462 iter/s, 1.95518s/10 iters), loss = 6.66749
I0523 06:27:26.995127 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66749 (* 1 = 6.66749 loss)
I0523 06:27:27.540966 35003 sgd_solver.cpp:112] Iteration 163080, lr = 0.001
I0523 06:27:31.820776 35003 solver.cpp:239] Iteration 163090 (2.07235 iter/s, 4.82544s/10 iters), loss = 6.70113
I0523 06:27:31.820833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70113 (* 1 = 6.70113 loss)
I0523 06:27:31.828359 35003 sgd_solver.cpp:112] Iteration 163090, lr = 0.001
I0523 06:27:34.657166 35003 solver.cpp:239] Iteration 163100 (3.52583 iter/s, 2.83621s/10 iters), loss = 7.26268
I0523 06:27:34.657212 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26268 (* 1 = 7.26268 loss)
I0523 06:27:35.394412 35003 sgd_solver.cpp:112] Iteration 163100, lr = 0.001
I0523 06:27:39.218156 35003 solver.cpp:239] Iteration 163110 (2.19262 iter/s, 4.56076s/10 iters), loss = 5.92477
I0523 06:27:39.218212 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92477 (* 1 = 5.92477 loss)
I0523 06:27:39.222353 35003 sgd_solver.cpp:112] Iteration 163110, lr = 0.001
I0523 06:27:42.728469 35003 solver.cpp:239] Iteration 163120 (2.84892 iter/s, 3.51011s/10 iters), loss = 7.99881
I0523 06:27:42.728523 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99881 (* 1 = 7.99881 loss)
I0523 06:27:42.734460 35003 sgd_solver.cpp:112] Iteration 163120, lr = 0.001
I0523 06:27:47.918774 35003 solver.cpp:239] Iteration 163130 (1.92677 iter/s, 5.19004s/10 iters), loss = 5.50401
I0523 06:27:47.919064 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.50401 (* 1 = 5.50401 loss)
I0523 06:27:47.932381 35003 sgd_solver.cpp:112] Iteration 163130, lr = 0.001
I0523 06:27:51.555183 35003 solver.cpp:239] Iteration 163140 (2.75029 iter/s, 3.63599s/10 iters), loss = 6.46268
I0523 06:27:51.555225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46268 (* 1 = 6.46268 loss)
I0523 06:27:52.175806 35003 sgd_solver.cpp:112] Iteration 163140, lr = 0.001
I0523 06:27:55.575733 35003 solver.cpp:239] Iteration 163150 (2.48735 iter/s, 4.02034s/10 iters), loss = 5.63102
I0523 06:27:55.575773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.63102 (* 1 = 5.63102 loss)
I0523 06:27:55.582029 35003 sgd_solver.cpp:112] Iteration 163150, lr = 0.001
I0523 06:28:00.263347 35003 solver.cpp:239] Iteration 163160 (2.13339 iter/s, 4.68738s/10 iters), loss = 6.23764
I0523 06:28:00.263386 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23764 (* 1 = 6.23764 loss)
I0523 06:28:00.281759 35003 sgd_solver.cpp:112] Iteration 163160, lr = 0.001
I0523 06:28:04.270822 35003 solver.cpp:239] Iteration 163170 (2.49546 iter/s, 4.00727s/10 iters), loss = 6.97996
I0523 06:28:04.270859 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97996 (* 1 = 6.97996 loss)
I0523 06:28:04.283596 35003 sgd_solver.cpp:112] Iteration 163170, lr = 0.001
I0523 06:28:06.439786 35003 solver.cpp:239] Iteration 163180 (4.61077 iter/s, 2.16883s/10 iters), loss = 7.28767
I0523 06:28:06.439823 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28767 (* 1 = 7.28767 loss)
I0523 06:28:06.458570 35003 sgd_solver.cpp:112] Iteration 163180, lr = 0.001
I0523 06:28:12.580972 35003 solver.cpp:239] Iteration 163190 (1.62843 iter/s, 6.1409s/10 iters), loss = 6.58218
I0523 06:28:12.581024 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58218 (* 1 = 6.58218 loss)
I0523 06:28:12.593895 35003 sgd_solver.cpp:112] Iteration 163190, lr = 0.001
I0523 06:28:14.708192 35003 solver.cpp:239] Iteration 163200 (4.70129 iter/s, 2.12708s/10 iters), loss = 5.81179
I0523 06:28:14.708235 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.81179 (* 1 = 5.81179 loss)
I0523 06:28:15.442356 35003 sgd_solver.cpp:112] Iteration 163200, lr = 0.001
I0523 06:28:17.278796 35003 solver.cpp:239] Iteration 163210 (3.89037 iter/s, 2.57045s/10 iters), loss = 5.50034
I0523 06:28:17.278836 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.50034 (* 1 = 5.50034 loss)
I0523 06:28:17.284466 35003 sgd_solver.cpp:112] Iteration 163210, lr = 0.001
I0523 06:28:21.500138 35003 solver.cpp:239] Iteration 163220 (2.36904 iter/s, 4.22112s/10 iters), loss = 7.1332
I0523 06:28:21.500418 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1332 (* 1 = 7.1332 loss)
I0523 06:28:21.510418 35003 sgd_solver.cpp:112] Iteration 163220, lr = 0.001
I0523 06:28:24.699296 35003 solver.cpp:239] Iteration 163230 (3.1265 iter/s, 3.19847s/10 iters), loss = 6.00879
I0523 06:28:24.699347 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00879 (* 1 = 6.00879 loss)
I0523 06:28:24.704960 35003 sgd_solver.cpp:112] Iteration 163230, lr = 0.001
I0523 06:28:28.237042 35003 solver.cpp:239] Iteration 163240 (2.82682 iter/s, 3.53755s/10 iters), loss = 5.85709
I0523 06:28:28.237090 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85709 (* 1 = 5.85709 loss)
I0523 06:28:28.261873 35003 sgd_solver.cpp:112] Iteration 163240, lr = 0.001
I0523 06:28:31.270210 35003 solver.cpp:239] Iteration 163250 (3.29708 iter/s, 3.03299s/10 iters), loss = 7.2896
I0523 06:28:31.270253 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2896 (* 1 = 7.2896 loss)
I0523 06:28:31.279266 35003 sgd_solver.cpp:112] Iteration 163250, lr = 0.001
I0523 06:28:34.679042 35003 solver.cpp:239] Iteration 163260 (2.93372 iter/s, 3.40864s/10 iters), loss = 6.55296
I0523 06:28:34.679096 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55296 (* 1 = 6.55296 loss)
I0523 06:28:34.693413 35003 sgd_solver.cpp:112] Iteration 163260, lr = 0.001
I0523 06:28:37.057291 35003 solver.cpp:239] Iteration 163270 (4.20505 iter/s, 2.37809s/10 iters), loss = 7.50909
I0523 06:28:37.057341 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50909 (* 1 = 7.50909 loss)
I0523 06:28:37.664845 35003 sgd_solver.cpp:112] Iteration 163270, lr = 0.001
I0523 06:28:40.482208 35003 solver.cpp:239] Iteration 163280 (2.91994 iter/s, 3.42473s/10 iters), loss = 7.24846
I0523 06:28:40.482245 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24846 (* 1 = 7.24846 loss)
I0523 06:28:40.495211 35003 sgd_solver.cpp:112] Iteration 163280, lr = 0.001
I0523 06:28:44.119451 35003 solver.cpp:239] Iteration 163290 (2.74949 iter/s, 3.63704s/10 iters), loss = 6.68961
I0523 06:28:44.119506 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68961 (* 1 = 6.68961 loss)
I0523 06:28:44.141163 35003 sgd_solver.cpp:112] Iteration 163290, lr = 0.001
I0523 06:28:47.003275 35003 solver.cpp:239] Iteration 163300 (3.46783 iter/s, 2.88365s/10 iters), loss = 6.62606
I0523 06:28:47.003322 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62606 (* 1 = 6.62606 loss)
I0523 06:28:47.022186 35003 sgd_solver.cpp:112] Iteration 163300, lr = 0.001
I0523 06:28:49.977443 35003 solver.cpp:239] Iteration 163310 (3.36248 iter/s, 2.974s/10 iters), loss = 7.22766
I0523 06:28:49.977483 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22766 (* 1 = 7.22766 loss)
I0523 06:28:50.692950 35003 sgd_solver.cpp:112] Iteration 163310, lr = 0.001
I0523 06:28:54.925545 35003 solver.cpp:239] Iteration 163320 (2.02107 iter/s, 4.94786s/10 iters), loss = 6.85432
I0523 06:28:54.925673 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85432 (* 1 = 6.85432 loss)
I0523 06:28:55.622161 35003 sgd_solver.cpp:112] Iteration 163320, lr = 0.001
I0523 06:28:59.243417 35003 solver.cpp:239] Iteration 163330 (2.31612 iter/s, 4.31756s/10 iters), loss = 6.57127
I0523 06:28:59.243468 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57127 (* 1 = 6.57127 loss)
I0523 06:28:59.938930 35003 sgd_solver.cpp:112] Iteration 163330, lr = 0.001
I0523 06:29:03.133050 35003 solver.cpp:239] Iteration 163340 (2.57108 iter/s, 3.88942s/10 iters), loss = 6.83801
I0523 06:29:03.133101 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83801 (* 1 = 6.83801 loss)
I0523 06:29:03.873684 35003 sgd_solver.cpp:112] Iteration 163340, lr = 0.001
I0523 06:29:06.062111 35003 solver.cpp:239] Iteration 163350 (3.41427 iter/s, 2.92889s/10 iters), loss = 7.02378
I0523 06:29:06.062157 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02378 (* 1 = 7.02378 loss)
I0523 06:29:06.083710 35003 sgd_solver.cpp:112] Iteration 163350, lr = 0.001
I0523 06:29:08.796221 35003 solver.cpp:239] Iteration 163360 (3.65771 iter/s, 2.73395s/10 iters), loss = 6.787
I0523 06:29:08.796260 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.787 (* 1 = 6.787 loss)
I0523 06:29:08.809289 35003 sgd_solver.cpp:112] Iteration 163360, lr = 0.001
I0523 06:29:11.618966 35003 solver.cpp:239] Iteration 163370 (3.54286 iter/s, 2.82258s/10 iters), loss = 5.62797
I0523 06:29:11.619011 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.62797 (* 1 = 5.62797 loss)
I0523 06:29:11.631002 35003 sgd_solver.cpp:112] Iteration 163370, lr = 0.001
I0523 06:29:15.249405 35003 solver.cpp:239] Iteration 163380 (2.75464 iter/s, 3.63024s/10 iters), loss = 6.91986
I0523 06:29:15.249459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91986 (* 1 = 6.91986 loss)
I0523 06:29:15.267961 35003 sgd_solver.cpp:112] Iteration 163380, lr = 0.001
I0523 06:29:17.812268 35003 solver.cpp:239] Iteration 163390 (3.90213 iter/s, 2.5627s/10 iters), loss = 6.95189
I0523 06:29:17.812330 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95189 (* 1 = 6.95189 loss)
I0523 06:29:18.482728 35003 sgd_solver.cpp:112] Iteration 163390, lr = 0.001
I0523 06:29:20.636531 35003 solver.cpp:239] Iteration 163400 (3.54098 iter/s, 2.82407s/10 iters), loss = 6.43722
I0523 06:29:20.636584 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43722 (* 1 = 6.43722 loss)
I0523 06:29:20.645900 35003 sgd_solver.cpp:112] Iteration 163400, lr = 0.001
I0523 06:29:25.809294 35003 solver.cpp:239] Iteration 163410 (1.9333 iter/s, 5.1725s/10 iters), loss = 6.58753
I0523 06:29:25.809530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58753 (* 1 = 6.58753 loss)
I0523 06:29:25.834908 35003 sgd_solver.cpp:112] Iteration 163410, lr = 0.001
I0523 06:29:30.878316 35003 solver.cpp:239] Iteration 163420 (1.97293 iter/s, 5.0686s/10 iters), loss = 7.86974
I0523 06:29:30.878361 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86974 (* 1 = 7.86974 loss)
I0523 06:29:31.593873 35003 sgd_solver.cpp:112] Iteration 163420, lr = 0.001
I0523 06:29:33.683223 35003 solver.cpp:239] Iteration 163430 (3.56539 iter/s, 2.80474s/10 iters), loss = 5.89194
I0523 06:29:33.683266 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89194 (* 1 = 5.89194 loss)
I0523 06:29:33.999917 35003 sgd_solver.cpp:112] Iteration 163430, lr = 0.001
I0523 06:29:37.396193 35003 solver.cpp:239] Iteration 163440 (2.69341 iter/s, 3.71277s/10 iters), loss = 7.99589
I0523 06:29:37.396231 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.99589 (* 1 = 7.99589 loss)
I0523 06:29:37.405463 35003 sgd_solver.cpp:112] Iteration 163440, lr = 0.001
I0523 06:29:40.953858 35003 solver.cpp:239] Iteration 163450 (2.81098 iter/s, 3.55748s/10 iters), loss = 6.33699
I0523 06:29:40.953907 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33699 (* 1 = 6.33699 loss)
I0523 06:29:40.962231 35003 sgd_solver.cpp:112] Iteration 163450, lr = 0.001
I0523 06:29:44.654057 35003 solver.cpp:239] Iteration 163460 (2.70272 iter/s, 3.69998s/10 iters), loss = 5.39589
I0523 06:29:44.654139 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.39589 (* 1 = 5.39589 loss)
I0523 06:29:44.715399 35003 sgd_solver.cpp:112] Iteration 163460, lr = 0.001
I0523 06:29:46.801981 35003 solver.cpp:239] Iteration 163470 (4.65602 iter/s, 2.14776s/10 iters), loss = 7.1769
I0523 06:29:46.802024 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1769 (* 1 = 7.1769 loss)
I0523 06:29:46.815105 35003 sgd_solver.cpp:112] Iteration 163470, lr = 0.001
I0523 06:29:49.669023 35003 solver.cpp:239] Iteration 163480 (3.48812 iter/s, 2.86688s/10 iters), loss = 5.99144
I0523 06:29:49.669064 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99144 (* 1 = 5.99144 loss)
I0523 06:29:49.687757 35003 sgd_solver.cpp:112] Iteration 163480, lr = 0.001
I0523 06:29:54.144335 35003 solver.cpp:239] Iteration 163490 (2.23459 iter/s, 4.47509s/10 iters), loss = 7.10868
I0523 06:29:54.144379 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10868 (* 1 = 7.10868 loss)
I0523 06:29:54.866199 35003 sgd_solver.cpp:112] Iteration 163490, lr = 0.001
I0523 06:29:57.676219 35003 solver.cpp:239] Iteration 163500 (2.8315 iter/s, 3.53169s/10 iters), loss = 6.67776
I0523 06:29:57.676403 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67776 (* 1 = 6.67776 loss)
I0523 06:29:58.405004 35003 sgd_solver.cpp:112] Iteration 163500, lr = 0.001
I0523 06:30:02.073410 35003 solver.cpp:239] Iteration 163510 (2.27437 iter/s, 4.39682s/10 iters), loss = 7.66686
I0523 06:30:02.073452 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.66686 (* 1 = 7.66686 loss)
I0523 06:30:02.079228 35003 sgd_solver.cpp:112] Iteration 163510, lr = 0.001
I0523 06:30:04.994911 35003 solver.cpp:239] Iteration 163520 (3.4231 iter/s, 2.92133s/10 iters), loss = 5.77276
I0523 06:30:04.994963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77276 (* 1 = 5.77276 loss)
I0523 06:30:05.717296 35003 sgd_solver.cpp:112] Iteration 163520, lr = 0.001
I0523 06:30:09.759330 35003 solver.cpp:239] Iteration 163530 (2.099 iter/s, 4.76417s/10 iters), loss = 7.16338
I0523 06:30:09.759383 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16338 (* 1 = 7.16338 loss)
I0523 06:30:10.468394 35003 sgd_solver.cpp:112] Iteration 163530, lr = 0.001
I0523 06:30:14.036000 35003 solver.cpp:239] Iteration 163540 (2.33839 iter/s, 4.27645s/10 iters), loss = 5.76233
I0523 06:30:14.036038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.76233 (* 1 = 5.76233 loss)
I0523 06:30:14.049690 35003 sgd_solver.cpp:112] Iteration 163540, lr = 0.001
I0523 06:30:16.706070 35003 solver.cpp:239] Iteration 163550 (3.74544 iter/s, 2.66991s/10 iters), loss = 6.32944
I0523 06:30:16.706113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32944 (* 1 = 6.32944 loss)
I0523 06:30:17.436928 35003 sgd_solver.cpp:112] Iteration 163550, lr = 0.001
I0523 06:30:21.731000 35003 solver.cpp:239] Iteration 163560 (1.99018 iter/s, 5.02468s/10 iters), loss = 6.83147
I0523 06:30:21.731045 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83147 (* 1 = 6.83147 loss)
I0523 06:30:21.743821 35003 sgd_solver.cpp:112] Iteration 163560, lr = 0.001
I0523 06:30:24.546876 35003 solver.cpp:239] Iteration 163570 (3.5515 iter/s, 2.81571s/10 iters), loss = 7.23268
I0523 06:30:24.546916 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23268 (* 1 = 7.23268 loss)
I0523 06:30:24.560644 35003 sgd_solver.cpp:112] Iteration 163570, lr = 0.001
I0523 06:30:26.015949 35003 solver.cpp:239] Iteration 163580 (6.80755 iter/s, 1.46896s/10 iters), loss = 6.15248
I0523 06:30:26.016011 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15248 (* 1 = 6.15248 loss)
I0523 06:30:26.737371 35003 sgd_solver.cpp:112] Iteration 163580, lr = 0.001
I0523 06:30:30.375285 35003 solver.cpp:239] Iteration 163590 (2.29405 iter/s, 4.3591s/10 iters), loss = 6.12072
I0523 06:30:30.375406 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12072 (* 1 = 6.12072 loss)
I0523 06:30:30.382549 35003 sgd_solver.cpp:112] Iteration 163590, lr = 0.001
I0523 06:30:33.242919 35003 solver.cpp:239] Iteration 163600 (3.48749 iter/s, 2.86739s/10 iters), loss = 5.95439
I0523 06:30:33.242964 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95439 (* 1 = 5.95439 loss)
I0523 06:30:33.248462 35003 sgd_solver.cpp:112] Iteration 163600, lr = 0.001
I0523 06:30:37.246206 35003 solver.cpp:239] Iteration 163610 (2.49808 iter/s, 4.00308s/10 iters), loss = 6.67009
I0523 06:30:37.246244 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67009 (* 1 = 6.67009 loss)
I0523 06:30:37.257957 35003 sgd_solver.cpp:112] Iteration 163610, lr = 0.001
I0523 06:30:39.548759 35003 solver.cpp:239] Iteration 163620 (4.34327 iter/s, 2.30241s/10 iters), loss = 6.68708
I0523 06:30:39.548805 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68708 (* 1 = 6.68708 loss)
I0523 06:30:40.283488 35003 sgd_solver.cpp:112] Iteration 163620, lr = 0.001
I0523 06:30:43.014843 35003 solver.cpp:239] Iteration 163630 (2.88526 iter/s, 3.46589s/10 iters), loss = 6.12534
I0523 06:30:43.014892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12534 (* 1 = 6.12534 loss)
I0523 06:30:43.038770 35003 sgd_solver.cpp:112] Iteration 163630, lr = 0.001
I0523 06:30:47.458227 35003 solver.cpp:239] Iteration 163640 (2.25066 iter/s, 4.44315s/10 iters), loss = 6.41116
I0523 06:30:47.458277 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41116 (* 1 = 6.41116 loss)
I0523 06:30:47.465796 35003 sgd_solver.cpp:112] Iteration 163640, lr = 0.001
I0523 06:30:50.931004 35003 solver.cpp:239] Iteration 163650 (2.87971 iter/s, 3.47257s/10 iters), loss = 7.04458
I0523 06:30:50.931063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04458 (* 1 = 7.04458 loss)
I0523 06:30:51.427623 35003 sgd_solver.cpp:112] Iteration 163650, lr = 0.001
I0523 06:30:53.494216 35003 solver.cpp:239] Iteration 163660 (3.90161 iter/s, 2.56305s/10 iters), loss = 6.44593
I0523 06:30:53.494258 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44593 (* 1 = 6.44593 loss)
I0523 06:30:53.505631 35003 sgd_solver.cpp:112] Iteration 163660, lr = 0.001
I0523 06:30:56.524488 35003 solver.cpp:239] Iteration 163670 (3.30022 iter/s, 3.0301s/10 iters), loss = 7.38589
I0523 06:30:56.524540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38589 (* 1 = 7.38589 loss)
I0523 06:30:56.529247 35003 sgd_solver.cpp:112] Iteration 163670, lr = 0.001
I0523 06:31:00.316895 35003 solver.cpp:239] Iteration 163680 (2.637 iter/s, 3.79219s/10 iters), loss = 6.62002
I0523 06:31:00.316938 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62002 (* 1 = 6.62002 loss)
I0523 06:31:00.322021 35003 sgd_solver.cpp:112] Iteration 163680, lr = 0.001
I0523 06:31:05.834259 35003 solver.cpp:239] Iteration 163690 (1.81255 iter/s, 5.5171s/10 iters), loss = 6.79563
I0523 06:31:05.834506 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79563 (* 1 = 6.79563 loss)
I0523 06:31:05.839648 35003 sgd_solver.cpp:112] Iteration 163690, lr = 0.001
I0523 06:31:09.489240 35003 solver.cpp:239] Iteration 163700 (2.73628 iter/s, 3.6546s/10 iters), loss = 7.0497
I0523 06:31:09.489298 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0497 (* 1 = 7.0497 loss)
I0523 06:31:09.492964 35003 sgd_solver.cpp:112] Iteration 163700, lr = 0.001
I0523 06:31:11.616322 35003 solver.cpp:239] Iteration 163710 (4.70161 iter/s, 2.12693s/10 iters), loss = 5.85984
I0523 06:31:11.616379 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85984 (* 1 = 5.85984 loss)
I0523 06:31:12.357332 35003 sgd_solver.cpp:112] Iteration 163710, lr = 0.001
I0523 06:31:14.456598 35003 solver.cpp:239] Iteration 163720 (3.521 iter/s, 2.8401s/10 iters), loss = 6.11774
I0523 06:31:14.456650 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11774 (* 1 = 6.11774 loss)
I0523 06:31:15.185155 35003 sgd_solver.cpp:112] Iteration 163720, lr = 0.001
I0523 06:31:20.232175 35003 solver.cpp:239] Iteration 163730 (1.73151 iter/s, 5.77529s/10 iters), loss = 6.68415
I0523 06:31:20.232219 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68415 (* 1 = 6.68415 loss)
I0523 06:31:20.973948 35003 sgd_solver.cpp:112] Iteration 163730, lr = 0.001
I0523 06:31:24.637501 35003 solver.cpp:239] Iteration 163740 (2.2701 iter/s, 4.40509s/10 iters), loss = 6.9194
I0523 06:31:24.637562 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9194 (* 1 = 6.9194 loss)
I0523 06:31:24.762468 35003 sgd_solver.cpp:112] Iteration 163740, lr = 0.001
I0523 06:31:30.575990 35003 solver.cpp:239] Iteration 163750 (1.68401 iter/s, 5.93819s/10 iters), loss = 7.04675
I0523 06:31:30.576035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04675 (* 1 = 7.04675 loss)
I0523 06:31:30.583545 35003 sgd_solver.cpp:112] Iteration 163750, lr = 0.001
I0523 06:31:34.886770 35003 solver.cpp:239] Iteration 163760 (2.31989 iter/s, 4.31056s/10 iters), loss = 7.30186
I0523 06:31:34.886812 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30186 (* 1 = 7.30186 loss)
I0523 06:31:34.899394 35003 sgd_solver.cpp:112] Iteration 163760, lr = 0.001
I0523 06:31:37.666040 35003 solver.cpp:239] Iteration 163770 (3.59827 iter/s, 2.77911s/10 iters), loss = 6.76079
I0523 06:31:37.666345 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76079 (* 1 = 6.76079 loss)
I0523 06:31:38.407588 35003 sgd_solver.cpp:112] Iteration 163770, lr = 0.001
I0523 06:31:41.329115 35003 solver.cpp:239] Iteration 163780 (2.73027 iter/s, 3.66264s/10 iters), loss = 6.44665
I0523 06:31:41.329181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44665 (* 1 = 6.44665 loss)
I0523 06:31:41.337410 35003 sgd_solver.cpp:112] Iteration 163780, lr = 0.001
I0523 06:31:44.736284 35003 solver.cpp:239] Iteration 163790 (2.93517 iter/s, 3.40696s/10 iters), loss = 5.63651
I0523 06:31:44.736325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.63651 (* 1 = 5.63651 loss)
I0523 06:31:44.749629 35003 sgd_solver.cpp:112] Iteration 163790, lr = 0.001
I0523 06:31:48.848893 35003 solver.cpp:239] Iteration 163800 (2.43167 iter/s, 4.1124s/10 iters), loss = 6.6583
I0523 06:31:48.848932 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6583 (* 1 = 6.6583 loss)
I0523 06:31:48.861726 35003 sgd_solver.cpp:112] Iteration 163800, lr = 0.001
I0523 06:31:53.204113 35003 solver.cpp:239] Iteration 163810 (2.29621 iter/s, 4.355s/10 iters), loss = 6.95703
I0523 06:31:53.204159 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95703 (* 1 = 6.95703 loss)
I0523 06:31:53.216672 35003 sgd_solver.cpp:112] Iteration 163810, lr = 0.001
I0523 06:31:57.257269 35003 solver.cpp:239] Iteration 163820 (2.46734 iter/s, 4.05294s/10 iters), loss = 6.28865
I0523 06:31:57.257318 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28865 (* 1 = 6.28865 loss)
I0523 06:31:57.270401 35003 sgd_solver.cpp:112] Iteration 163820, lr = 0.001
I0523 06:32:01.202601 35003 solver.cpp:239] Iteration 163830 (2.53478 iter/s, 3.94512s/10 iters), loss = 7.88421
I0523 06:32:01.202654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88421 (* 1 = 7.88421 loss)
I0523 06:32:01.923748 35003 sgd_solver.cpp:112] Iteration 163830, lr = 0.001
I0523 06:32:05.471905 35003 solver.cpp:239] Iteration 163840 (2.34244 iter/s, 4.26906s/10 iters), loss = 7.64225
I0523 06:32:05.471943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64225 (* 1 = 7.64225 loss)
I0523 06:32:05.490067 35003 sgd_solver.cpp:112] Iteration 163840, lr = 0.001
I0523 06:32:09.155692 35003 solver.cpp:239] Iteration 163850 (2.71475 iter/s, 3.68358s/10 iters), loss = 6.97824
I0523 06:32:09.155833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97824 (* 1 = 6.97824 loss)
I0523 06:32:09.896651 35003 sgd_solver.cpp:112] Iteration 163850, lr = 0.001
I0523 06:32:13.346962 35003 solver.cpp:239] Iteration 163860 (2.38609 iter/s, 4.19096s/10 iters), loss = 5.96634
I0523 06:32:13.347000 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96634 (* 1 = 5.96634 loss)
I0523 06:32:13.365381 35003 sgd_solver.cpp:112] Iteration 163860, lr = 0.001
I0523 06:32:16.151003 35003 solver.cpp:239] Iteration 163870 (3.56649 iter/s, 2.80388s/10 iters), loss = 5.26535
I0523 06:32:16.151049 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.26535 (* 1 = 5.26535 loss)
I0523 06:32:16.153324 35003 sgd_solver.cpp:112] Iteration 163870, lr = 0.001
I0523 06:32:19.821458 35003 solver.cpp:239] Iteration 163880 (2.7246 iter/s, 3.67026s/10 iters), loss = 6.90617
I0523 06:32:19.821497 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90617 (* 1 = 6.90617 loss)
I0523 06:32:19.847024 35003 sgd_solver.cpp:112] Iteration 163880, lr = 0.001
I0523 06:32:22.756037 35003 solver.cpp:239] Iteration 163890 (3.40784 iter/s, 2.93441s/10 iters), loss = 7.63282
I0523 06:32:22.756078 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63282 (* 1 = 7.63282 loss)
I0523 06:32:22.778760 35003 sgd_solver.cpp:112] Iteration 163890, lr = 0.001
I0523 06:32:24.968572 35003 solver.cpp:239] Iteration 163900 (4.51998 iter/s, 2.2124s/10 iters), loss = 7.40478
I0523 06:32:24.968624 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40478 (* 1 = 7.40478 loss)
I0523 06:32:24.977325 35003 sgd_solver.cpp:112] Iteration 163900, lr = 0.001
I0523 06:32:27.812120 35003 solver.cpp:239] Iteration 163910 (3.51695 iter/s, 2.84337s/10 iters), loss = 7.03888
I0523 06:32:27.812168 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03888 (* 1 = 7.03888 loss)
I0523 06:32:27.819239 35003 sgd_solver.cpp:112] Iteration 163910, lr = 0.001
I0523 06:32:32.983162 35003 solver.cpp:239] Iteration 163920 (1.93395 iter/s, 5.17078s/10 iters), loss = 6.77155
I0523 06:32:32.983243 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77155 (* 1 = 6.77155 loss)
I0523 06:32:33.723994 35003 sgd_solver.cpp:112] Iteration 163920, lr = 0.001
I0523 06:32:38.031131 35003 solver.cpp:239] Iteration 163930 (1.98111 iter/s, 5.04768s/10 iters), loss = 5.84495
I0523 06:32:38.031178 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84495 (* 1 = 5.84495 loss)
I0523 06:32:38.037205 35003 sgd_solver.cpp:112] Iteration 163930, lr = 0.001
I0523 06:32:41.872298 35003 solver.cpp:239] Iteration 163940 (2.60352 iter/s, 3.84096s/10 iters), loss = 7.67695
I0523 06:32:41.872465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67695 (* 1 = 7.67695 loss)
I0523 06:32:41.880154 35003 sgd_solver.cpp:112] Iteration 163940, lr = 0.001
I0523 06:32:43.914233 35003 solver.cpp:239] Iteration 163950 (4.89793 iter/s, 2.04168s/10 iters), loss = 5.79913
I0523 06:32:43.914280 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.79913 (* 1 = 5.79913 loss)
I0523 06:32:43.916877 35003 sgd_solver.cpp:112] Iteration 163950, lr = 0.001
I0523 06:32:46.718678 35003 solver.cpp:239] Iteration 163960 (3.56601 iter/s, 2.80425s/10 iters), loss = 6.31076
I0523 06:32:46.718770 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31076 (* 1 = 6.31076 loss)
I0523 06:32:46.730810 35003 sgd_solver.cpp:112] Iteration 163960, lr = 0.001
I0523 06:32:48.808560 35003 solver.cpp:239] Iteration 163970 (4.78538 iter/s, 2.0897s/10 iters), loss = 6.89681
I0523 06:32:48.808601 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89681 (* 1 = 6.89681 loss)
I0523 06:32:48.814175 35003 sgd_solver.cpp:112] Iteration 163970, lr = 0.001
I0523 06:32:52.362661 35003 solver.cpp:239] Iteration 163980 (2.8138 iter/s, 3.55391s/10 iters), loss = 7.84028
I0523 06:32:52.362715 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84028 (* 1 = 7.84028 loss)
I0523 06:32:52.372676 35003 sgd_solver.cpp:112] Iteration 163980, lr = 0.001
I0523 06:32:54.589645 35003 solver.cpp:239] Iteration 163990 (4.49068 iter/s, 2.22683s/10 iters), loss = 6.52708
I0523 06:32:54.589687 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52708 (* 1 = 6.52708 loss)
I0523 06:32:55.159575 35003 sgd_solver.cpp:112] Iteration 163990, lr = 0.001
I0523 06:32:58.000798 35003 solver.cpp:239] Iteration 164000 (2.93172 iter/s, 3.41097s/10 iters), loss = 6.31602
I0523 06:32:58.000838 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31602 (* 1 = 6.31602 loss)
I0523 06:32:58.014484 35003 sgd_solver.cpp:112] Iteration 164000, lr = 0.001
I0523 06:33:01.381172 35003 solver.cpp:239] Iteration 164010 (2.95841 iter/s, 3.38019s/10 iters), loss = 6.99114
I0523 06:33:01.381213 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99114 (* 1 = 6.99114 loss)
I0523 06:33:01.385149 35003 sgd_solver.cpp:112] Iteration 164010, lr = 0.001
I0523 06:33:04.214256 35003 solver.cpp:239] Iteration 164020 (3.52994 iter/s, 2.83291s/10 iters), loss = 7.6436
I0523 06:33:04.214303 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6436 (* 1 = 7.6436 loss)
I0523 06:33:04.240713 35003 sgd_solver.cpp:112] Iteration 164020, lr = 0.001
I0523 06:33:08.426496 35003 solver.cpp:239] Iteration 164030 (2.37416 iter/s, 4.21202s/10 iters), loss = 6.4602
I0523 06:33:08.426534 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4602 (* 1 = 6.4602 loss)
I0523 06:33:08.439801 35003 sgd_solver.cpp:112] Iteration 164030, lr = 0.001
I0523 06:33:11.888249 35003 solver.cpp:239] Iteration 164040 (2.88886 iter/s, 3.46157s/10 iters), loss = 6.96604
I0523 06:33:11.888594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96604 (* 1 = 6.96604 loss)
I0523 06:33:12.013401 35003 sgd_solver.cpp:112] Iteration 164040, lr = 0.001
I0523 06:33:14.866436 35003 solver.cpp:239] Iteration 164050 (3.35828 iter/s, 2.97772s/10 iters), loss = 6.60533
I0523 06:33:14.866482 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60533 (* 1 = 6.60533 loss)
I0523 06:33:15.582108 35003 sgd_solver.cpp:112] Iteration 164050, lr = 0.001
I0523 06:33:18.457213 35003 solver.cpp:239] Iteration 164060 (2.78507 iter/s, 3.59058s/10 iters), loss = 5.9575
I0523 06:33:18.457273 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9575 (* 1 = 5.9575 loss)
I0523 06:33:19.137810 35003 sgd_solver.cpp:112] Iteration 164060, lr = 0.001
I0523 06:33:22.982923 35003 solver.cpp:239] Iteration 164070 (2.20972 iter/s, 4.52547s/10 iters), loss = 6.51877
I0523 06:33:22.982965 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51877 (* 1 = 6.51877 loss)
I0523 06:33:22.987463 35003 sgd_solver.cpp:112] Iteration 164070, lr = 0.001
I0523 06:33:26.466684 35003 solver.cpp:239] Iteration 164080 (2.87062 iter/s, 3.48357s/10 iters), loss = 7.45718
I0523 06:33:26.466751 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45718 (* 1 = 7.45718 loss)
I0523 06:33:27.175400 35003 sgd_solver.cpp:112] Iteration 164080, lr = 0.001
I0523 06:33:29.998550 35003 solver.cpp:239] Iteration 164090 (2.83153 iter/s, 3.53165s/10 iters), loss = 6.06277
I0523 06:33:29.998594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06277 (* 1 = 6.06277 loss)
I0523 06:33:30.016721 35003 sgd_solver.cpp:112] Iteration 164090, lr = 0.001
I0523 06:33:34.250809 35003 solver.cpp:239] Iteration 164100 (2.35181 iter/s, 4.25204s/10 iters), loss = 6.96905
I0523 06:33:34.250857 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96905 (* 1 = 6.96905 loss)
I0523 06:33:34.263914 35003 sgd_solver.cpp:112] Iteration 164100, lr = 0.001
I0523 06:33:39.304402 35003 solver.cpp:239] Iteration 164110 (1.97889 iter/s, 5.05334s/10 iters), loss = 6.38566
I0523 06:33:39.304443 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38566 (* 1 = 6.38566 loss)
I0523 06:33:39.326967 35003 sgd_solver.cpp:112] Iteration 164110, lr = 0.001
I0523 06:33:43.013059 35003 solver.cpp:239] Iteration 164120 (2.69653 iter/s, 3.70847s/10 iters), loss = 6.80404
I0523 06:33:43.013288 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80404 (* 1 = 6.80404 loss)
I0523 06:33:43.024693 35003 sgd_solver.cpp:112] Iteration 164120, lr = 0.001
I0523 06:33:45.079368 35003 solver.cpp:239] Iteration 164130 (4.84023 iter/s, 2.06602s/10 iters), loss = 6.56764
I0523 06:33:45.079412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56764 (* 1 = 6.56764 loss)
I0523 06:33:45.744619 35003 sgd_solver.cpp:112] Iteration 164130, lr = 0.001
I0523 06:33:47.906718 35003 solver.cpp:239] Iteration 164140 (3.53711 iter/s, 2.82717s/10 iters), loss = 6.66459
I0523 06:33:47.906761 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66459 (* 1 = 6.66459 loss)
I0523 06:33:48.644542 35003 sgd_solver.cpp:112] Iteration 164140, lr = 0.001
I0523 06:33:50.723333 35003 solver.cpp:239] Iteration 164150 (3.55057 iter/s, 2.81645s/10 iters), loss = 5.77827
I0523 06:33:50.723378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77827 (* 1 = 5.77827 loss)
I0523 06:33:51.432449 35003 sgd_solver.cpp:112] Iteration 164150, lr = 0.001
I0523 06:33:53.629701 35003 solver.cpp:239] Iteration 164160 (3.44094 iter/s, 2.90618s/10 iters), loss = 6.04545
I0523 06:33:53.629761 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04545 (* 1 = 6.04545 loss)
I0523 06:33:54.266949 35003 sgd_solver.cpp:112] Iteration 164160, lr = 0.001
I0523 06:33:57.128062 35003 solver.cpp:239] Iteration 164170 (2.85865 iter/s, 3.49815s/10 iters), loss = 7.23358
I0523 06:33:57.128106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23358 (* 1 = 7.23358 loss)
I0523 06:33:57.141577 35003 sgd_solver.cpp:112] Iteration 164170, lr = 0.001
I0523 06:34:01.138813 35003 solver.cpp:239] Iteration 164180 (2.49343 iter/s, 4.01054s/10 iters), loss = 6.30591
I0523 06:34:01.138870 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30591 (* 1 = 6.30591 loss)
I0523 06:34:01.152132 35003 sgd_solver.cpp:112] Iteration 164180, lr = 0.001
I0523 06:34:04.268368 35003 solver.cpp:239] Iteration 164190 (3.19553 iter/s, 3.12937s/10 iters), loss = 5.5512
I0523 06:34:04.268414 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.5512 (* 1 = 5.5512 loss)
I0523 06:34:04.990355 35003 sgd_solver.cpp:112] Iteration 164190, lr = 0.001
I0523 06:34:08.280066 35003 solver.cpp:239] Iteration 164200 (2.49284 iter/s, 4.01148s/10 iters), loss = 6.2533
I0523 06:34:08.280105 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2533 (* 1 = 6.2533 loss)
I0523 06:34:08.285261 35003 sgd_solver.cpp:112] Iteration 164200, lr = 0.001
I0523 06:34:12.549001 35003 solver.cpp:239] Iteration 164210 (2.34262 iter/s, 4.26872s/10 iters), loss = 7.31065
I0523 06:34:12.549069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31065 (* 1 = 7.31065 loss)
I0523 06:34:12.562140 35003 sgd_solver.cpp:112] Iteration 164210, lr = 0.001
I0523 06:34:16.634722 35003 solver.cpp:239] Iteration 164220 (2.44769 iter/s, 4.08548s/10 iters), loss = 7.71791
I0523 06:34:16.634897 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71791 (* 1 = 7.71791 loss)
I0523 06:34:16.647929 35003 sgd_solver.cpp:112] Iteration 164220, lr = 0.001
I0523 06:34:18.737370 35003 solver.cpp:239] Iteration 164230 (4.7565 iter/s, 2.10238s/10 iters), loss = 5.44877
I0523 06:34:18.737409 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.44877 (* 1 = 5.44877 loss)
I0523 06:34:19.452924 35003 sgd_solver.cpp:112] Iteration 164230, lr = 0.001
I0523 06:34:23.225025 35003 solver.cpp:239] Iteration 164240 (2.22845 iter/s, 4.48743s/10 iters), loss = 6.47969
I0523 06:34:23.225064 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47969 (* 1 = 6.47969 loss)
I0523 06:34:23.233253 35003 sgd_solver.cpp:112] Iteration 164240, lr = 0.001
I0523 06:34:28.284309 35003 solver.cpp:239] Iteration 164250 (1.97666 iter/s, 5.05904s/10 iters), loss = 6.24634
I0523 06:34:28.284363 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24634 (* 1 = 6.24634 loss)
I0523 06:34:28.328459 35003 sgd_solver.cpp:112] Iteration 164250, lr = 0.001
I0523 06:34:33.550379 35003 solver.cpp:239] Iteration 164260 (1.89905 iter/s, 5.2658s/10 iters), loss = 6.23914
I0523 06:34:33.550424 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23914 (* 1 = 6.23914 loss)
I0523 06:34:33.564136 35003 sgd_solver.cpp:112] Iteration 164260, lr = 0.001
I0523 06:34:35.742419 35003 solver.cpp:239] Iteration 164270 (4.56226 iter/s, 2.1919s/10 iters), loss = 5.70094
I0523 06:34:35.742466 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.70094 (* 1 = 5.70094 loss)
I0523 06:34:35.755612 35003 sgd_solver.cpp:112] Iteration 164270, lr = 0.001
I0523 06:34:37.891511 35003 solver.cpp:239] Iteration 164280 (4.65344 iter/s, 2.14895s/10 iters), loss = 7.07551
I0523 06:34:37.891562 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07551 (* 1 = 7.07551 loss)
I0523 06:34:37.905007 35003 sgd_solver.cpp:112] Iteration 164280, lr = 0.001
I0523 06:34:41.328306 35003 solver.cpp:239] Iteration 164290 (2.90985 iter/s, 3.4366s/10 iters), loss = 6.86136
I0523 06:34:41.328351 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86136 (* 1 = 6.86136 loss)
I0523 06:34:41.342326 35003 sgd_solver.cpp:112] Iteration 164290, lr = 0.001
I0523 06:34:45.080723 35003 solver.cpp:239] Iteration 164300 (2.66509 iter/s, 3.75222s/10 iters), loss = 5.38148
I0523 06:34:45.080768 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.38148 (* 1 = 5.38148 loss)
I0523 06:34:45.143388 35003 sgd_solver.cpp:112] Iteration 164300, lr = 0.001
I0523 06:34:49.795773 35003 solver.cpp:239] Iteration 164310 (2.12098 iter/s, 4.71481s/10 iters), loss = 6.22478
I0523 06:34:49.796069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22478 (* 1 = 6.22478 loss)
I0523 06:34:49.809101 35003 sgd_solver.cpp:112] Iteration 164310, lr = 0.001
I0523 06:34:52.176765 35003 solver.cpp:239] Iteration 164320 (4.20058 iter/s, 2.38062s/10 iters), loss = 7.42832
I0523 06:34:52.176805 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42832 (* 1 = 7.42832 loss)
I0523 06:34:52.184424 35003 sgd_solver.cpp:112] Iteration 164320, lr = 0.001
I0523 06:34:56.078115 35003 solver.cpp:239] Iteration 164330 (2.56335 iter/s, 3.90114s/10 iters), loss = 7.75617
I0523 06:34:56.078161 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75617 (* 1 = 7.75617 loss)
I0523 06:34:56.090939 35003 sgd_solver.cpp:112] Iteration 164330, lr = 0.001
I0523 06:34:59.091398 35003 solver.cpp:239] Iteration 164340 (3.31883 iter/s, 3.01311s/10 iters), loss = 7.01256
I0523 06:34:59.091441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01256 (* 1 = 7.01256 loss)
I0523 06:34:59.812799 35003 sgd_solver.cpp:112] Iteration 164340, lr = 0.001
I0523 06:35:02.652122 35003 solver.cpp:239] Iteration 164350 (2.80857 iter/s, 3.56053s/10 iters), loss = 6.85115
I0523 06:35:02.652174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85115 (* 1 = 6.85115 loss)
I0523 06:35:02.665581 35003 sgd_solver.cpp:112] Iteration 164350, lr = 0.001
I0523 06:35:05.407320 35003 solver.cpp:239] Iteration 164360 (3.62973 iter/s, 2.75503s/10 iters), loss = 7.45976
I0523 06:35:05.407358 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45976 (* 1 = 7.45976 loss)
I0523 06:35:05.413899 35003 sgd_solver.cpp:112] Iteration 164360, lr = 0.001
I0523 06:35:09.699928 35003 solver.cpp:239] Iteration 164370 (2.3297 iter/s, 4.29239s/10 iters), loss = 6.16397
I0523 06:35:09.699980 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16397 (* 1 = 6.16397 loss)
I0523 06:35:10.401381 35003 sgd_solver.cpp:112] Iteration 164370, lr = 0.001
I0523 06:35:14.873401 35003 solver.cpp:239] Iteration 164380 (1.93304 iter/s, 5.17321s/10 iters), loss = 6.50126
I0523 06:35:14.873451 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50126 (* 1 = 6.50126 loss)
I0523 06:35:15.589056 35003 sgd_solver.cpp:112] Iteration 164380, lr = 0.001
I0523 06:35:17.623425 35003 solver.cpp:239] Iteration 164390 (3.63659 iter/s, 2.74983s/10 iters), loss = 6.07144
I0523 06:35:17.623467 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07144 (* 1 = 6.07144 loss)
I0523 06:35:17.631507 35003 sgd_solver.cpp:112] Iteration 164390, lr = 0.001
I0523 06:35:20.413586 35003 solver.cpp:239] Iteration 164400 (3.58424 iter/s, 2.78999s/10 iters), loss = 5.86574
I0523 06:35:20.413803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.86574 (* 1 = 5.86574 loss)
I0523 06:35:20.426844 35003 sgd_solver.cpp:112] Iteration 164400, lr = 0.001
I0523 06:35:23.200357 35003 solver.cpp:239] Iteration 164410 (3.58877 iter/s, 2.78647s/10 iters), loss = 6.21484
I0523 06:35:23.200395 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21484 (* 1 = 6.21484 loss)
I0523 06:35:23.915001 35003 sgd_solver.cpp:112] Iteration 164410, lr = 0.001
I0523 06:35:26.701306 35003 solver.cpp:239] Iteration 164420 (2.85652 iter/s, 3.50076s/10 iters), loss = 7.30573
I0523 06:35:26.701354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30573 (* 1 = 7.30573 loss)
I0523 06:35:26.714470 35003 sgd_solver.cpp:112] Iteration 164420, lr = 0.001
I0523 06:35:29.099263 35003 solver.cpp:239] Iteration 164430 (4.17049 iter/s, 2.3978s/10 iters), loss = 6.56077
I0523 06:35:29.099321 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56077 (* 1 = 6.56077 loss)
I0523 06:35:29.105774 35003 sgd_solver.cpp:112] Iteration 164430, lr = 0.001
I0523 06:35:33.531661 35003 solver.cpp:239] Iteration 164440 (2.25624 iter/s, 4.43216s/10 iters), loss = 6.21871
I0523 06:35:33.531703 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21871 (* 1 = 6.21871 loss)
I0523 06:35:34.221263 35003 sgd_solver.cpp:112] Iteration 164440, lr = 0.001
I0523 06:35:36.928418 35003 solver.cpp:239] Iteration 164450 (2.94415 iter/s, 3.39657s/10 iters), loss = 6.62621
I0523 06:35:36.928457 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62621 (* 1 = 6.62621 loss)
I0523 06:35:36.941540 35003 sgd_solver.cpp:112] Iteration 164450, lr = 0.001
I0523 06:35:40.219823 35003 solver.cpp:239] Iteration 164460 (3.03838 iter/s, 3.29122s/10 iters), loss = 7.96499
I0523 06:35:40.219864 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96499 (* 1 = 7.96499 loss)
I0523 06:35:40.259459 35003 sgd_solver.cpp:112] Iteration 164460, lr = 0.001
I0523 06:35:43.341689 35003 solver.cpp:239] Iteration 164470 (3.20339 iter/s, 3.12169s/10 iters), loss = 6.13186
I0523 06:35:43.341733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13186 (* 1 = 6.13186 loss)
I0523 06:35:43.345774 35003 sgd_solver.cpp:112] Iteration 164470, lr = 0.001
I0523 06:35:46.051944 35003 solver.cpp:239] Iteration 164480 (3.68993 iter/s, 2.71008s/10 iters), loss = 5.77914
I0523 06:35:46.051996 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77914 (* 1 = 5.77914 loss)
I0523 06:35:46.782485 35003 sgd_solver.cpp:112] Iteration 164480, lr = 0.001
I0523 06:35:50.241608 35003 solver.cpp:239] Iteration 164490 (2.38696 iter/s, 4.18943s/10 iters), loss = 6.49888
I0523 06:35:50.241654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49888 (* 1 = 6.49888 loss)
I0523 06:35:50.258116 35003 sgd_solver.cpp:112] Iteration 164490, lr = 0.001
I0523 06:35:52.392629 35003 solver.cpp:239] Iteration 164500 (4.64928 iter/s, 2.15087s/10 iters), loss = 5.87804
I0523 06:35:52.392930 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87804 (* 1 = 5.87804 loss)
I0523 06:35:53.134199 35003 sgd_solver.cpp:112] Iteration 164500, lr = 0.001
I0523 06:35:56.717842 35003 solver.cpp:239] Iteration 164510 (2.31227 iter/s, 4.32476s/10 iters), loss = 5.74338
I0523 06:35:56.717895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74338 (* 1 = 5.74338 loss)
I0523 06:35:56.730903 35003 sgd_solver.cpp:112] Iteration 164510, lr = 0.001
I0523 06:35:59.562933 35003 solver.cpp:239] Iteration 164520 (3.51504 iter/s, 2.84492s/10 iters), loss = 6.50927
I0523 06:35:59.562974 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50927 (* 1 = 6.50927 loss)
I0523 06:35:59.569481 35003 sgd_solver.cpp:112] Iteration 164520, lr = 0.001
I0523 06:36:03.145339 35003 solver.cpp:239] Iteration 164530 (2.79158 iter/s, 3.58221s/10 iters), loss = 6.28556
I0523 06:36:03.145382 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28556 (* 1 = 6.28556 loss)
I0523 06:36:03.173610 35003 sgd_solver.cpp:112] Iteration 164530, lr = 0.001
I0523 06:36:06.979085 35003 solver.cpp:239] Iteration 164540 (2.60855 iter/s, 3.83354s/10 iters), loss = 6.58523
I0523 06:36:06.979136 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58523 (* 1 = 6.58523 loss)
I0523 06:36:07.003720 35003 sgd_solver.cpp:112] Iteration 164540, lr = 0.001
I0523 06:36:09.839571 35003 solver.cpp:239] Iteration 164550 (3.49612 iter/s, 2.86032s/10 iters), loss = 7.63359
I0523 06:36:09.839618 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63359 (* 1 = 7.63359 loss)
I0523 06:36:09.849262 35003 sgd_solver.cpp:112] Iteration 164550, lr = 0.001
I0523 06:36:12.598546 35003 solver.cpp:239] Iteration 164560 (3.62482 iter/s, 2.75876s/10 iters), loss = 6.47091
I0523 06:36:12.598601 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47091 (* 1 = 6.47091 loss)
I0523 06:36:12.610620 35003 sgd_solver.cpp:112] Iteration 164560, lr = 0.001
I0523 06:36:15.431934 35003 solver.cpp:239] Iteration 164570 (3.52956 iter/s, 2.83321s/10 iters), loss = 7.04034
I0523 06:36:15.431972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04034 (* 1 = 7.04034 loss)
I0523 06:36:15.444818 35003 sgd_solver.cpp:112] Iteration 164570, lr = 0.001
I0523 06:36:16.754671 35003 solver.cpp:239] Iteration 164580 (7.5607 iter/s, 1.32263s/10 iters), loss = 6.74901
I0523 06:36:16.754729 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74901 (* 1 = 6.74901 loss)
I0523 06:36:16.767992 35003 sgd_solver.cpp:112] Iteration 164580, lr = 0.001
I0523 06:36:20.291079 35003 solver.cpp:239] Iteration 164590 (2.82789 iter/s, 3.5362s/10 iters), loss = 6.14993
I0523 06:36:20.291124 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14993 (* 1 = 6.14993 loss)
I0523 06:36:20.296321 35003 sgd_solver.cpp:112] Iteration 164590, lr = 0.001
I0523 06:36:23.852988 35003 solver.cpp:239] Iteration 164600 (2.80764 iter/s, 3.56171s/10 iters), loss = 6.80144
I0523 06:36:23.853199 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80144 (* 1 = 6.80144 loss)
I0523 06:36:23.866153 35003 sgd_solver.cpp:112] Iteration 164600, lr = 0.001
I0523 06:36:26.941954 35003 solver.cpp:239] Iteration 164610 (3.23768 iter/s, 3.08863s/10 iters), loss = 5.45495
I0523 06:36:26.942011 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.45495 (* 1 = 5.45495 loss)
I0523 06:36:26.947068 35003 sgd_solver.cpp:112] Iteration 164610, lr = 0.001
I0523 06:36:29.984642 35003 solver.cpp:239] Iteration 164620 (3.28678 iter/s, 3.04249s/10 iters), loss = 6.13722
I0523 06:36:29.984688 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13722 (* 1 = 6.13722 loss)
I0523 06:36:29.997735 35003 sgd_solver.cpp:112] Iteration 164620, lr = 0.001
I0523 06:36:32.764950 35003 solver.cpp:239] Iteration 164630 (3.59694 iter/s, 2.78014s/10 iters), loss = 7.31935
I0523 06:36:32.765000 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31935 (* 1 = 7.31935 loss)
I0523 06:36:32.786514 35003 sgd_solver.cpp:112] Iteration 164630, lr = 0.001
I0523 06:36:35.449229 35003 solver.cpp:239] Iteration 164640 (3.72563 iter/s, 2.68411s/10 iters), loss = 6.56787
I0523 06:36:35.449275 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56787 (* 1 = 6.56787 loss)
I0523 06:36:35.453894 35003 sgd_solver.cpp:112] Iteration 164640, lr = 0.001
I0523 06:36:38.359256 35003 solver.cpp:239] Iteration 164650 (3.43659 iter/s, 2.90986s/10 iters), loss = 7.24333
I0523 06:36:38.359292 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24333 (* 1 = 7.24333 loss)
I0523 06:36:38.372355 35003 sgd_solver.cpp:112] Iteration 164650, lr = 0.001
I0523 06:36:40.824277 35003 solver.cpp:239] Iteration 164660 (4.05701 iter/s, 2.46487s/10 iters), loss = 7.05369
I0523 06:36:40.824337 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05369 (* 1 = 7.05369 loss)
I0523 06:36:40.837590 35003 sgd_solver.cpp:112] Iteration 164660, lr = 0.001
I0523 06:36:43.586222 35003 solver.cpp:239] Iteration 164670 (3.62087 iter/s, 2.76177s/10 iters), loss = 7.03275
I0523 06:36:43.586274 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03275 (* 1 = 7.03275 loss)
I0523 06:36:44.327096 35003 sgd_solver.cpp:112] Iteration 164670, lr = 0.001
I0523 06:36:48.258790 35003 solver.cpp:239] Iteration 164680 (2.14026 iter/s, 4.67232s/10 iters), loss = 6.41609
I0523 06:36:48.258846 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41609 (* 1 = 6.41609 loss)
I0523 06:36:48.986558 35003 sgd_solver.cpp:112] Iteration 164680, lr = 0.001
I0523 06:36:52.529808 35003 solver.cpp:239] Iteration 164690 (2.34149 iter/s, 4.27079s/10 iters), loss = 7.51871
I0523 06:36:52.529912 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51871 (* 1 = 7.51871 loss)
I0523 06:36:52.550026 35003 sgd_solver.cpp:112] Iteration 164690, lr = 0.001
I0523 06:36:55.830421 35003 solver.cpp:239] Iteration 164700 (3.02996 iter/s, 3.30037s/10 iters), loss = 6.83731
I0523 06:36:55.830773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83731 (* 1 = 6.83731 loss)
I0523 06:36:55.836052 35003 sgd_solver.cpp:112] Iteration 164700, lr = 0.001
I0523 06:36:59.497186 35003 solver.cpp:239] Iteration 164710 (2.72755 iter/s, 3.66629s/10 iters), loss = 6.04158
I0523 06:36:59.497227 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04158 (* 1 = 6.04158 loss)
I0523 06:36:59.508985 35003 sgd_solver.cpp:112] Iteration 164710, lr = 0.001
I0523 06:37:03.017035 35003 solver.cpp:239] Iteration 164720 (2.84118 iter/s, 3.51966s/10 iters), loss = 6.71256
I0523 06:37:03.017078 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71256 (* 1 = 6.71256 loss)
I0523 06:37:03.618211 35003 sgd_solver.cpp:112] Iteration 164720, lr = 0.001
I0523 06:37:06.951773 35003 solver.cpp:239] Iteration 164730 (2.5416 iter/s, 3.93453s/10 iters), loss = 6.30993
I0523 06:37:06.951823 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30993 (* 1 = 6.30993 loss)
I0523 06:37:06.979794 35003 sgd_solver.cpp:112] Iteration 164730, lr = 0.001
I0523 06:37:09.013679 35003 solver.cpp:239] Iteration 164740 (4.85022 iter/s, 2.06176s/10 iters), loss = 7.10211
I0523 06:37:09.014755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10211 (* 1 = 7.10211 loss)
I0523 06:37:09.035331 35003 sgd_solver.cpp:112] Iteration 164740, lr = 0.001
I0523 06:37:12.613297 35003 solver.cpp:239] Iteration 164750 (2.78235 iter/s, 3.59408s/10 iters), loss = 6.0531
I0523 06:37:12.613365 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0531 (* 1 = 6.0531 loss)
I0523 06:37:12.627228 35003 sgd_solver.cpp:112] Iteration 164750, lr = 0.001
I0523 06:37:17.606911 35003 solver.cpp:239] Iteration 164760 (2.00267 iter/s, 4.99334s/10 iters), loss = 5.85534
I0523 06:37:17.606972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85534 (* 1 = 5.85534 loss)
I0523 06:37:17.615033 35003 sgd_solver.cpp:112] Iteration 164760, lr = 0.001
I0523 06:37:18.429687 35003 solver.cpp:239] Iteration 164770 (12.1557 iter/s, 0.822657s/10 iters), loss = 7.36867
I0523 06:37:18.429751 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36867 (* 1 = 7.36867 loss)
I0523 06:37:18.432967 35003 sgd_solver.cpp:112] Iteration 164770, lr = 0.001
I0523 06:37:19.421674 35003 solver.cpp:239] Iteration 164780 (10.082 iter/s, 0.991863s/10 iters), loss = 7.07707
I0523 06:37:19.421730 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07707 (* 1 = 7.07707 loss)
I0523 06:37:19.430316 35003 sgd_solver.cpp:112] Iteration 164780, lr = 0.001
I0523 06:37:20.388728 35003 solver.cpp:239] Iteration 164790 (10.3418 iter/s, 0.966947s/10 iters), loss = 5.59965
I0523 06:37:20.388785 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.59965 (* 1 = 5.59965 loss)
I0523 06:37:20.776675 35003 sgd_solver.cpp:112] Iteration 164790, lr = 0.001
I0523 06:37:21.585254 35003 solver.cpp:239] Iteration 164800 (8.35843 iter/s, 1.1964s/10 iters), loss = 6.61427
I0523 06:37:21.585294 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61427 (* 1 = 6.61427 loss)
I0523 06:37:21.596992 35003 sgd_solver.cpp:112] Iteration 164800, lr = 0.001
I0523 06:37:22.482039 35003 solver.cpp:239] Iteration 164810 (11.1521 iter/s, 0.896693s/10 iters), loss = 6.79702
I0523 06:37:22.482090 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79702 (* 1 = 6.79702 loss)
I0523 06:37:22.493474 35003 sgd_solver.cpp:112] Iteration 164810, lr = 0.001
I0523 06:37:25.279086 35003 solver.cpp:239] Iteration 164820 (3.57541 iter/s, 2.79688s/10 iters), loss = 5.67501
I0523 06:37:25.279129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.67501 (* 1 = 5.67501 loss)
I0523 06:37:26.020334 35003 sgd_solver.cpp:112] Iteration 164820, lr = 0.001
I0523 06:37:27.344934 35003 solver.cpp:239] Iteration 164830 (4.84095 iter/s, 2.06571s/10 iters), loss = 5.4646
I0523 06:37:27.344979 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.4646 (* 1 = 5.4646 loss)
I0523 06:37:27.358276 35003 sgd_solver.cpp:112] Iteration 164830, lr = 0.001
I0523 06:37:31.009738 35003 solver.cpp:239] Iteration 164840 (2.72881 iter/s, 3.6646s/10 iters), loss = 6.14621
I0523 06:37:31.009788 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14621 (* 1 = 6.14621 loss)
I0523 06:37:31.040108 35003 sgd_solver.cpp:112] Iteration 164840, lr = 0.001
I0523 06:37:33.134357 35003 solver.cpp:239] Iteration 164850 (4.70704 iter/s, 2.12448s/10 iters), loss = 5.45282
I0523 06:37:33.134398 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.45282 (* 1 = 5.45282 loss)
I0523 06:37:33.164060 35003 sgd_solver.cpp:112] Iteration 164850, lr = 0.001
I0523 06:37:38.206876 35003 solver.cpp:239] Iteration 164860 (1.9715 iter/s, 5.07227s/10 iters), loss = 6.25129
I0523 06:37:38.206928 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25129 (* 1 = 6.25129 loss)
I0523 06:37:38.230052 35003 sgd_solver.cpp:112] Iteration 164860, lr = 0.001
I0523 06:37:42.762931 35003 solver.cpp:239] Iteration 164870 (2.195 iter/s, 4.55582s/10 iters), loss = 6.88558
I0523 06:37:42.762974 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88558 (* 1 = 6.88558 loss)
I0523 06:37:43.107534 35003 sgd_solver.cpp:112] Iteration 164870, lr = 0.001
I0523 06:37:45.952143 35003 solver.cpp:239] Iteration 164880 (3.13574 iter/s, 3.18904s/10 iters), loss = 6.6842
I0523 06:37:45.952185 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6842 (* 1 = 6.6842 loss)
I0523 06:37:45.963409 35003 sgd_solver.cpp:112] Iteration 164880, lr = 0.001
I0523 06:37:51.206496 35003 solver.cpp:239] Iteration 164890 (1.90328 iter/s, 5.25409s/10 iters), loss = 7.16142
I0523 06:37:51.206549 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16142 (* 1 = 7.16142 loss)
I0523 06:37:51.215710 35003 sgd_solver.cpp:112] Iteration 164890, lr = 0.001
I0523 06:37:54.044800 35003 solver.cpp:239] Iteration 164900 (3.52345 iter/s, 2.83813s/10 iters), loss = 5.89572
I0523 06:37:54.044917 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89572 (* 1 = 5.89572 loss)
I0523 06:37:54.056718 35003 sgd_solver.cpp:112] Iteration 164900, lr = 0.001
I0523 06:37:56.198860 35003 solver.cpp:239] Iteration 164910 (4.64285 iter/s, 2.15385s/10 iters), loss = 6.40402
I0523 06:37:56.199146 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40402 (* 1 = 6.40402 loss)
I0523 06:37:56.928546 35003 sgd_solver.cpp:112] Iteration 164910, lr = 0.001
I0523 06:38:00.025892 35003 solver.cpp:239] Iteration 164920 (2.61327 iter/s, 3.82662s/10 iters), loss = 6.35576
I0523 06:38:00.025933 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35576 (* 1 = 6.35576 loss)
I0523 06:38:00.764845 35003 sgd_solver.cpp:112] Iteration 164920, lr = 0.001
I0523 06:38:03.606688 35003 solver.cpp:239] Iteration 164930 (2.79283 iter/s, 3.5806s/10 iters), loss = 7.04476
I0523 06:38:03.606750 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04476 (* 1 = 7.04476 loss)
I0523 06:38:03.619524 35003 sgd_solver.cpp:112] Iteration 164930, lr = 0.001
I0523 06:38:07.287899 35003 solver.cpp:239] Iteration 164940 (2.71666 iter/s, 3.68099s/10 iters), loss = 7.29585
I0523 06:38:07.287940 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29585 (* 1 = 7.29585 loss)
I0523 06:38:07.306037 35003 sgd_solver.cpp:112] Iteration 164940, lr = 0.001
I0523 06:38:10.038405 35003 solver.cpp:239] Iteration 164950 (3.6359 iter/s, 2.75035s/10 iters), loss = 6.92947
I0523 06:38:10.038455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92947 (* 1 = 6.92947 loss)
I0523 06:38:10.056823 35003 sgd_solver.cpp:112] Iteration 164950, lr = 0.001
I0523 06:38:14.261422 35003 solver.cpp:239] Iteration 164960 (2.36811 iter/s, 4.22278s/10 iters), loss = 6.23036
I0523 06:38:14.261478 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23036 (* 1 = 6.23036 loss)
I0523 06:38:14.963266 35003 sgd_solver.cpp:112] Iteration 164960, lr = 0.001
I0523 06:38:18.594952 35003 solver.cpp:239] Iteration 164970 (2.30771 iter/s, 4.33329s/10 iters), loss = 6.49641
I0523 06:38:18.595006 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49641 (* 1 = 6.49641 loss)
I0523 06:38:19.329291 35003 sgd_solver.cpp:112] Iteration 164970, lr = 0.001
I0523 06:38:22.733101 35003 solver.cpp:239] Iteration 164980 (2.41667 iter/s, 4.13793s/10 iters), loss = 6.69048
I0523 06:38:22.733144 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69048 (* 1 = 6.69048 loss)
I0523 06:38:23.442142 35003 sgd_solver.cpp:112] Iteration 164980, lr = 0.001
I0523 06:38:27.039758 35003 solver.cpp:239] Iteration 164990 (2.3221 iter/s, 4.30644s/10 iters), loss = 5.25022
I0523 06:38:27.039983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.25022 (* 1 = 5.25022 loss)
I0523 06:38:27.052893 35003 sgd_solver.cpp:112] Iteration 164990, lr = 0.001
I0523 06:38:31.023249 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_165000.caffemodel
I0523 06:38:31.222971 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_165000.solverstate
I0523 06:38:31.404428 35003 solver.cpp:239] Iteration 165000 (2.29133 iter/s, 4.36428s/10 iters), loss = 6.28057
I0523 06:38:31.404474 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28057 (* 1 = 6.28057 loss)
I0523 06:38:32.106716 35003 sgd_solver.cpp:112] Iteration 165000, lr = 0.001
I0523 06:38:34.713238 35003 solver.cpp:239] Iteration 165010 (3.0224 iter/s, 3.30863s/10 iters), loss = 5.68488
I0523 06:38:34.713284 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.68488 (* 1 = 5.68488 loss)
I0523 06:38:35.448318 35003 sgd_solver.cpp:112] Iteration 165010, lr = 0.001
I0523 06:38:38.210918 35003 solver.cpp:239] Iteration 165020 (2.85919 iter/s, 3.49749s/10 iters), loss = 5.34031
I0523 06:38:38.210963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.34031 (* 1 = 5.34031 loss)
I0523 06:38:38.919920 35003 sgd_solver.cpp:112] Iteration 165020, lr = 0.001
I0523 06:38:40.327100 35003 solver.cpp:239] Iteration 165030 (4.72582 iter/s, 2.11603s/10 iters), loss = 6.22949
I0523 06:38:40.327145 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22949 (* 1 = 6.22949 loss)
I0523 06:38:41.055948 35003 sgd_solver.cpp:112] Iteration 165030, lr = 0.001
I0523 06:38:44.576566 35003 solver.cpp:239] Iteration 165040 (2.35336 iter/s, 4.24925s/10 iters), loss = 6.21971
I0523 06:38:44.576611 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21971 (* 1 = 6.21971 loss)
I0523 06:38:45.292073 35003 sgd_solver.cpp:112] Iteration 165040, lr = 0.001
I0523 06:38:51.061175 35003 solver.cpp:239] Iteration 165050 (1.54219 iter/s, 6.48428s/10 iters), loss = 6.60173
I0523 06:38:51.061228 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60173 (* 1 = 6.60173 loss)
I0523 06:38:51.081811 35003 sgd_solver.cpp:112] Iteration 165050, lr = 0.001
I0523 06:38:54.658731 35003 solver.cpp:239] Iteration 165060 (2.77982 iter/s, 3.59735s/10 iters), loss = 6.29457
I0523 06:38:54.658776 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29457 (* 1 = 6.29457 loss)
I0523 06:38:54.664481 35003 sgd_solver.cpp:112] Iteration 165060, lr = 0.001
I0523 06:38:57.575070 35003 solver.cpp:239] Iteration 165070 (3.42915 iter/s, 2.91617s/10 iters), loss = 7.46289
I0523 06:38:57.575181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46289 (* 1 = 7.46289 loss)
I0523 06:38:57.600235 35003 sgd_solver.cpp:112] Iteration 165070, lr = 0.001
I0523 06:39:01.205628 35003 solver.cpp:239] Iteration 165080 (2.7546 iter/s, 3.63029s/10 iters), loss = 6.25593
I0523 06:39:01.205693 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25593 (* 1 = 6.25593 loss)
I0523 06:39:01.212268 35003 sgd_solver.cpp:112] Iteration 165080, lr = 0.001
I0523 06:39:03.926719 35003 solver.cpp:239] Iteration 165090 (3.67525 iter/s, 2.7209s/10 iters), loss = 7.16899
I0523 06:39:03.926759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16899 (* 1 = 7.16899 loss)
I0523 06:39:03.940193 35003 sgd_solver.cpp:112] Iteration 165090, lr = 0.001
I0523 06:39:08.171353 35003 solver.cpp:239] Iteration 165100 (2.35604 iter/s, 4.2444s/10 iters), loss = 6.17615
I0523 06:39:08.171404 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17615 (* 1 = 6.17615 loss)
I0523 06:39:08.183413 35003 sgd_solver.cpp:112] Iteration 165100, lr = 0.001
I0523 06:39:12.831039 35003 solver.cpp:239] Iteration 165110 (2.14619 iter/s, 4.65942s/10 iters), loss = 5.34708
I0523 06:39:12.831095 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.34708 (* 1 = 5.34708 loss)
I0523 06:39:12.843663 35003 sgd_solver.cpp:112] Iteration 165110, lr = 0.001
I0523 06:39:15.634658 35003 solver.cpp:239] Iteration 165120 (3.56704 iter/s, 2.80344s/10 iters), loss = 5.92155
I0523 06:39:15.634726 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92155 (* 1 = 5.92155 loss)
I0523 06:39:16.350462 35003 sgd_solver.cpp:112] Iteration 165120, lr = 0.001
I0523 06:39:19.240960 35003 solver.cpp:239] Iteration 165130 (2.77309 iter/s, 3.60608s/10 iters), loss = 6.19225
I0523 06:39:19.241006 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19225 (* 1 = 6.19225 loss)
I0523 06:39:19.248992 35003 sgd_solver.cpp:112] Iteration 165130, lr = 0.001
I0523 06:39:22.762593 35003 solver.cpp:239] Iteration 165140 (2.83975 iter/s, 3.52144s/10 iters), loss = 6.49995
I0523 06:39:22.762641 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49995 (* 1 = 6.49995 loss)
I0523 06:39:22.770828 35003 sgd_solver.cpp:112] Iteration 165140, lr = 0.001
I0523 06:39:25.549106 35003 solver.cpp:239] Iteration 165150 (3.58893 iter/s, 2.78635s/10 iters), loss = 7.30521
I0523 06:39:25.549154 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30521 (* 1 = 7.30521 loss)
I0523 06:39:25.564728 35003 sgd_solver.cpp:112] Iteration 165150, lr = 0.001
I0523 06:39:29.200372 35003 solver.cpp:239] Iteration 165160 (2.73893 iter/s, 3.65106s/10 iters), loss = 5.11313
I0523 06:39:29.200671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.11313 (* 1 = 5.11313 loss)
I0523 06:39:29.212496 35003 sgd_solver.cpp:112] Iteration 165160, lr = 0.001
I0523 06:39:31.982386 35003 solver.cpp:239] Iteration 165170 (3.59502 iter/s, 2.78163s/10 iters), loss = 5.46418
I0523 06:39:31.982424 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.46418 (* 1 = 5.46418 loss)
I0523 06:39:31.990331 35003 sgd_solver.cpp:112] Iteration 165170, lr = 0.001
I0523 06:39:34.075300 35003 solver.cpp:239] Iteration 165180 (4.77833 iter/s, 2.09278s/10 iters), loss = 7.55141
I0523 06:39:34.075350 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55141 (* 1 = 7.55141 loss)
I0523 06:39:34.084790 35003 sgd_solver.cpp:112] Iteration 165180, lr = 0.001
I0523 06:39:38.529021 35003 solver.cpp:239] Iteration 165190 (2.24543 iter/s, 4.45349s/10 iters), loss = 7.5081
I0523 06:39:38.529064 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5081 (* 1 = 7.5081 loss)
I0523 06:39:38.548789 35003 sgd_solver.cpp:112] Iteration 165190, lr = 0.001
I0523 06:39:41.847033 35003 solver.cpp:239] Iteration 165200 (3.01403 iter/s, 3.31782s/10 iters), loss = 6.26876
I0523 06:39:41.847100 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26876 (* 1 = 6.26876 loss)
I0523 06:39:41.854089 35003 sgd_solver.cpp:112] Iteration 165200, lr = 0.001
I0523 06:39:46.905174 35003 solver.cpp:239] Iteration 165210 (1.97711 iter/s, 5.05788s/10 iters), loss = 5.67322
I0523 06:39:46.905218 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.67322 (* 1 = 5.67322 loss)
I0523 06:39:47.620525 35003 sgd_solver.cpp:112] Iteration 165210, lr = 0.001
I0523 06:39:50.456172 35003 solver.cpp:239] Iteration 165220 (2.81626 iter/s, 3.55081s/10 iters), loss = 7.05522
I0523 06:39:50.456216 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05522 (* 1 = 7.05522 loss)
I0523 06:39:51.197530 35003 sgd_solver.cpp:112] Iteration 165220, lr = 0.001
I0523 06:39:53.500109 35003 solver.cpp:239] Iteration 165230 (3.28541 iter/s, 3.04376s/10 iters), loss = 6.93521
I0523 06:39:53.500154 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93521 (* 1 = 6.93521 loss)
I0523 06:39:54.208937 35003 sgd_solver.cpp:112] Iteration 165230, lr = 0.001
I0523 06:39:57.184185 35003 solver.cpp:239] Iteration 165240 (2.71453 iter/s, 3.68388s/10 iters), loss = 6.72451
I0523 06:39:57.184233 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72451 (* 1 = 6.72451 loss)
I0523 06:39:57.196754 35003 sgd_solver.cpp:112] Iteration 165240, lr = 0.001
I0523 06:39:59.367924 35003 solver.cpp:239] Iteration 165250 (4.5796 iter/s, 2.18359s/10 iters), loss = 6.12536
I0523 06:39:59.368227 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12536 (* 1 = 6.12536 loss)
I0523 06:39:59.375676 35003 sgd_solver.cpp:112] Iteration 165250, lr = 0.001
I0523 06:40:01.381656 35003 solver.cpp:239] Iteration 165260 (4.96677 iter/s, 2.01338s/10 iters), loss = 5.54577
I0523 06:40:01.381695 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.54577 (* 1 = 5.54577 loss)
I0523 06:40:02.108527 35003 sgd_solver.cpp:112] Iteration 165260, lr = 0.001
I0523 06:40:05.231962 35003 solver.cpp:239] Iteration 165270 (2.59733 iter/s, 3.8501s/10 iters), loss = 7.26607
I0523 06:40:05.232015 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26607 (* 1 = 7.26607 loss)
I0523 06:40:05.236934 35003 sgd_solver.cpp:112] Iteration 165270, lr = 0.001
I0523 06:40:06.511462 35003 solver.cpp:239] Iteration 165280 (7.81626 iter/s, 1.27938s/10 iters), loss = 7.0573
I0523 06:40:06.511519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0573 (* 1 = 7.0573 loss)
I0523 06:40:06.532222 35003 sgd_solver.cpp:112] Iteration 165280, lr = 0.001
I0523 06:40:11.393784 35003 solver.cpp:239] Iteration 165290 (2.04831 iter/s, 4.88207s/10 iters), loss = 7.55774
I0523 06:40:11.393833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55774 (* 1 = 7.55774 loss)
I0523 06:40:11.401954 35003 sgd_solver.cpp:112] Iteration 165290, lr = 0.001
I0523 06:40:15.806443 35003 solver.cpp:239] Iteration 165300 (2.26632 iter/s, 4.41243s/10 iters), loss = 7.94917
I0523 06:40:15.806483 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94917 (* 1 = 7.94917 loss)
I0523 06:40:15.827520 35003 sgd_solver.cpp:112] Iteration 165300, lr = 0.001
I0523 06:40:18.658758 35003 solver.cpp:239] Iteration 165310 (3.51159 iter/s, 2.84771s/10 iters), loss = 6.1736
I0523 06:40:18.658808 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1736 (* 1 = 6.1736 loss)
I0523 06:40:18.668378 35003 sgd_solver.cpp:112] Iteration 165310, lr = 0.001
I0523 06:40:21.429574 35003 solver.cpp:239] Iteration 165320 (3.60927 iter/s, 2.77064s/10 iters), loss = 6.45945
I0523 06:40:21.429621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45945 (* 1 = 6.45945 loss)
I0523 06:40:21.436702 35003 sgd_solver.cpp:112] Iteration 165320, lr = 0.001
I0523 06:40:24.305771 35003 solver.cpp:239] Iteration 165330 (3.47702 iter/s, 2.87603s/10 iters), loss = 7.49895
I0523 06:40:24.305815 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49895 (* 1 = 7.49895 loss)
I0523 06:40:24.326130 35003 sgd_solver.cpp:112] Iteration 165330, lr = 0.001
I0523 06:40:27.864768 35003 solver.cpp:239] Iteration 165340 (2.80994 iter/s, 3.5588s/10 iters), loss = 6.80792
I0523 06:40:27.864816 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80792 (* 1 = 6.80792 loss)
I0523 06:40:28.578294 35003 sgd_solver.cpp:112] Iteration 165340, lr = 0.001
I0523 06:40:31.031057 35003 solver.cpp:239] Iteration 165350 (3.15845 iter/s, 3.16611s/10 iters), loss = 6.57808
I0523 06:40:31.031260 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57808 (* 1 = 6.57808 loss)
I0523 06:40:31.053793 35003 sgd_solver.cpp:112] Iteration 165350, lr = 0.001
I0523 06:40:34.030822 35003 solver.cpp:239] Iteration 165360 (3.33393 iter/s, 2.99946s/10 iters), loss = 5.61527
I0523 06:40:34.030863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.61527 (* 1 = 5.61527 loss)
I0523 06:40:34.035656 35003 sgd_solver.cpp:112] Iteration 165360, lr = 0.001
I0523 06:40:37.327739 35003 solver.cpp:239] Iteration 165370 (3.0333 iter/s, 3.29674s/10 iters), loss = 6.32093
I0523 06:40:37.327788 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32093 (* 1 = 6.32093 loss)
I0523 06:40:38.056915 35003 sgd_solver.cpp:112] Iteration 165370, lr = 0.001
I0523 06:40:40.188289 35003 solver.cpp:239] Iteration 165380 (3.49604 iter/s, 2.86038s/10 iters), loss = 6.06493
I0523 06:40:40.188349 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06493 (* 1 = 6.06493 loss)
I0523 06:40:40.806958 35003 sgd_solver.cpp:112] Iteration 165380, lr = 0.001
I0523 06:40:46.205565 35003 solver.cpp:239] Iteration 165390 (1.66197 iter/s, 6.01697s/10 iters), loss = 6.25891
I0523 06:40:46.205623 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25891 (* 1 = 6.25891 loss)
I0523 06:40:46.230445 35003 sgd_solver.cpp:112] Iteration 165390, lr = 0.001
I0523 06:40:49.121018 35003 solver.cpp:239] Iteration 165400 (3.4302 iter/s, 2.91528s/10 iters), loss = 6.4664
I0523 06:40:49.121063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4664 (* 1 = 6.4664 loss)
I0523 06:40:49.771888 35003 sgd_solver.cpp:112] Iteration 165400, lr = 0.001
I0523 06:40:53.399641 35003 solver.cpp:239] Iteration 165410 (2.33732 iter/s, 4.2784s/10 iters), loss = 7.71308
I0523 06:40:53.399689 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71308 (* 1 = 7.71308 loss)
I0523 06:40:53.413574 35003 sgd_solver.cpp:112] Iteration 165410, lr = 0.001
I0523 06:40:57.578902 35003 solver.cpp:239] Iteration 165420 (2.3929 iter/s, 4.17903s/10 iters), loss = 5.91165
I0523 06:40:57.578955 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91165 (* 1 = 5.91165 loss)
I0523 06:40:57.585623 35003 sgd_solver.cpp:112] Iteration 165420, lr = 0.001
I0523 06:40:59.729192 35003 solver.cpp:239] Iteration 165430 (4.65087 iter/s, 2.15014s/10 iters), loss = 5.73234
I0523 06:40:59.729251 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.73234 (* 1 = 5.73234 loss)
I0523 06:41:00.450506 35003 sgd_solver.cpp:112] Iteration 165430, lr = 0.001
I0523 06:41:02.706467 35003 solver.cpp:239] Iteration 165440 (3.35899 iter/s, 2.97709s/10 iters), loss = 7.5978
I0523 06:41:02.706778 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.5978 (* 1 = 7.5978 loss)
I0523 06:41:02.719821 35003 sgd_solver.cpp:112] Iteration 165440, lr = 0.001
I0523 06:41:05.581094 35003 solver.cpp:239] Iteration 165450 (3.47921 iter/s, 2.87422s/10 iters), loss = 6.29221
I0523 06:41:05.581141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29221 (* 1 = 6.29221 loss)
I0523 06:41:05.590034 35003 sgd_solver.cpp:112] Iteration 165450, lr = 0.001
I0523 06:41:10.055960 35003 solver.cpp:239] Iteration 165460 (2.23482 iter/s, 4.47463s/10 iters), loss = 6.61377
I0523 06:41:10.056008 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61377 (* 1 = 6.61377 loss)
I0523 06:41:10.069116 35003 sgd_solver.cpp:112] Iteration 165460, lr = 0.001
I0523 06:41:12.995208 35003 solver.cpp:239] Iteration 165470 (3.40244 iter/s, 2.93907s/10 iters), loss = 7.4848
I0523 06:41:12.995272 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4848 (* 1 = 7.4848 loss)
I0523 06:41:13.002437 35003 sgd_solver.cpp:112] Iteration 165470, lr = 0.001
I0523 06:41:17.426661 35003 solver.cpp:239] Iteration 165480 (2.25673 iter/s, 4.4312s/10 iters), loss = 6.30034
I0523 06:41:17.426719 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30034 (* 1 = 6.30034 loss)
I0523 06:41:17.440101 35003 sgd_solver.cpp:112] Iteration 165480, lr = 0.001
I0523 06:41:19.465812 35003 solver.cpp:239] Iteration 165490 (4.90435 iter/s, 2.03901s/10 iters), loss = 6.64673
I0523 06:41:19.465863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64673 (* 1 = 6.64673 loss)
I0523 06:41:19.488148 35003 sgd_solver.cpp:112] Iteration 165490, lr = 0.001
I0523 06:41:21.478588 35003 solver.cpp:239] Iteration 165500 (4.96861 iter/s, 2.01264s/10 iters), loss = 6.55679
I0523 06:41:21.478632 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55679 (* 1 = 6.55679 loss)
I0523 06:41:21.496793 35003 sgd_solver.cpp:112] Iteration 165500, lr = 0.001
I0523 06:41:24.299827 35003 solver.cpp:239] Iteration 165510 (3.54475 iter/s, 2.82107s/10 iters), loss = 6.17777
I0523 06:41:24.299877 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17777 (* 1 = 6.17777 loss)
I0523 06:41:24.566159 35003 sgd_solver.cpp:112] Iteration 165510, lr = 0.001
I0523 06:41:28.797613 35003 solver.cpp:239] Iteration 165520 (2.22343 iter/s, 4.49755s/10 iters), loss = 6.43623
I0523 06:41:28.797654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43623 (* 1 = 6.43623 loss)
I0523 06:41:28.817981 35003 sgd_solver.cpp:112] Iteration 165520, lr = 0.001
I0523 06:41:33.048844 35003 solver.cpp:239] Iteration 165530 (2.35239 iter/s, 4.251s/10 iters), loss = 6.18682
I0523 06:41:33.049094 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18682 (* 1 = 6.18682 loss)
I0523 06:41:33.054040 35003 sgd_solver.cpp:112] Iteration 165530, lr = 0.001
I0523 06:41:36.421022 35003 solver.cpp:239] Iteration 165540 (2.96578 iter/s, 3.3718s/10 iters), loss = 6.95052
I0523 06:41:36.421072 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95052 (* 1 = 6.95052 loss)
I0523 06:41:36.433904 35003 sgd_solver.cpp:112] Iteration 165540, lr = 0.001
I0523 06:41:39.346187 35003 solver.cpp:239] Iteration 165550 (3.41881 iter/s, 2.92499s/10 iters), loss = 6.65713
I0523 06:41:39.346235 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65713 (* 1 = 6.65713 loss)
I0523 06:41:39.349730 35003 sgd_solver.cpp:112] Iteration 165550, lr = 0.001
I0523 06:41:43.581153 35003 solver.cpp:239] Iteration 165560 (2.36142 iter/s, 4.23474s/10 iters), loss = 6.56805
I0523 06:41:43.581192 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56805 (* 1 = 6.56805 loss)
I0523 06:41:44.042810 35003 sgd_solver.cpp:112] Iteration 165560, lr = 0.001
I0523 06:41:46.034548 35003 solver.cpp:239] Iteration 165570 (4.07623 iter/s, 2.45325s/10 iters), loss = 6.53711
I0523 06:41:46.034597 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53711 (* 1 = 6.53711 loss)
I0523 06:41:46.042361 35003 sgd_solver.cpp:112] Iteration 165570, lr = 0.001
I0523 06:41:48.898025 35003 solver.cpp:239] Iteration 165580 (3.49246 iter/s, 2.86331s/10 iters), loss = 6.22281
I0523 06:41:48.898075 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22281 (* 1 = 6.22281 loss)
I0523 06:41:49.385067 35003 sgd_solver.cpp:112] Iteration 165580, lr = 0.001
I0523 06:41:52.318531 35003 solver.cpp:239] Iteration 165590 (2.92371 iter/s, 3.42031s/10 iters), loss = 6.55789
I0523 06:41:52.318575 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55789 (* 1 = 6.55789 loss)
I0523 06:41:52.988418 35003 sgd_solver.cpp:112] Iteration 165590, lr = 0.001
I0523 06:41:56.091361 35003 solver.cpp:239] Iteration 165600 (2.65068 iter/s, 3.77262s/10 iters), loss = 6.07618
I0523 06:41:56.091400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07618 (* 1 = 6.07618 loss)
I0523 06:41:56.103832 35003 sgd_solver.cpp:112] Iteration 165600, lr = 0.001
I0523 06:41:58.563808 35003 solver.cpp:239] Iteration 165610 (4.04482 iter/s, 2.4723s/10 iters), loss = 5.87595
I0523 06:41:58.563848 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87595 (* 1 = 5.87595 loss)
I0523 06:41:58.585819 35003 sgd_solver.cpp:112] Iteration 165610, lr = 0.001
I0523 06:42:02.574335 35003 solver.cpp:239] Iteration 165620 (2.49356 iter/s, 4.01032s/10 iters), loss = 6.87067
I0523 06:42:02.574378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87067 (* 1 = 6.87067 loss)
I0523 06:42:02.604718 35003 sgd_solver.cpp:112] Iteration 165620, lr = 0.001
I0523 06:42:06.299527 35003 solver.cpp:239] Iteration 165630 (2.68457 iter/s, 3.725s/10 iters), loss = 5.75057
I0523 06:42:06.299768 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.75057 (* 1 = 5.75057 loss)
I0523 06:42:07.028020 35003 sgd_solver.cpp:112] Iteration 165630, lr = 0.001
I0523 06:42:11.121758 35003 solver.cpp:239] Iteration 165640 (2.07391 iter/s, 4.82182s/10 iters), loss = 5.82668
I0523 06:42:11.121795 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.82668 (* 1 = 5.82668 loss)
I0523 06:42:11.135219 35003 sgd_solver.cpp:112] Iteration 165640, lr = 0.001
I0523 06:42:14.604534 35003 solver.cpp:239] Iteration 165650 (2.87142 iter/s, 3.48259s/10 iters), loss = 5.79887
I0523 06:42:14.604580 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.79887 (* 1 = 5.79887 loss)
I0523 06:42:15.338825 35003 sgd_solver.cpp:112] Iteration 165650, lr = 0.001
I0523 06:42:18.269547 35003 solver.cpp:239] Iteration 165660 (2.72868 iter/s, 3.66478s/10 iters), loss = 5.93496
I0523 06:42:18.269621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93496 (* 1 = 5.93496 loss)
I0523 06:42:18.986260 35003 sgd_solver.cpp:112] Iteration 165660, lr = 0.001
I0523 06:42:23.334427 35003 solver.cpp:239] Iteration 165670 (1.97449 iter/s, 5.06461s/10 iters), loss = 6.59404
I0523 06:42:23.334467 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59404 (* 1 = 6.59404 loss)
I0523 06:42:23.948148 35003 sgd_solver.cpp:112] Iteration 165670, lr = 0.001
I0523 06:42:26.014533 35003 solver.cpp:239] Iteration 165680 (3.73141 iter/s, 2.67995s/10 iters), loss = 5.89719
I0523 06:42:26.014581 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89719 (* 1 = 5.89719 loss)
I0523 06:42:26.753792 35003 sgd_solver.cpp:112] Iteration 165680, lr = 0.001
I0523 06:42:28.778188 35003 solver.cpp:239] Iteration 165690 (3.61861 iter/s, 2.76349s/10 iters), loss = 5.58322
I0523 06:42:28.778228 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.58322 (* 1 = 5.58322 loss)
I0523 06:42:29.466508 35003 sgd_solver.cpp:112] Iteration 165690, lr = 0.001
I0523 06:42:31.539299 35003 solver.cpp:239] Iteration 165700 (3.62194 iter/s, 2.76095s/10 iters), loss = 7.13634
I0523 06:42:31.539345 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13634 (* 1 = 7.13634 loss)
I0523 06:42:31.552587 35003 sgd_solver.cpp:112] Iteration 165700, lr = 0.001
I0523 06:42:35.885273 35003 solver.cpp:239] Iteration 165710 (2.3011 iter/s, 4.34575s/10 iters), loss = 6.49913
I0523 06:42:35.885313 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49913 (* 1 = 6.49913 loss)
I0523 06:42:35.898308 35003 sgd_solver.cpp:112] Iteration 165710, lr = 0.001
I0523 06:42:38.211519 35003 solver.cpp:239] Iteration 165720 (4.29906 iter/s, 2.32609s/10 iters), loss = 6.24724
I0523 06:42:38.211664 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24724 (* 1 = 6.24724 loss)
I0523 06:42:38.295368 35003 sgd_solver.cpp:112] Iteration 165720, lr = 0.001
I0523 06:42:41.965788 35003 solver.cpp:239] Iteration 165730 (2.66386 iter/s, 3.75395s/10 iters), loss = 7.19263
I0523 06:42:41.965838 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19263 (* 1 = 7.19263 loss)
I0523 06:42:41.969108 35003 sgd_solver.cpp:112] Iteration 165730, lr = 0.001
I0523 06:42:44.940958 35003 solver.cpp:239] Iteration 165740 (3.36138 iter/s, 2.97497s/10 iters), loss = 7.11407
I0523 06:42:44.941005 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11407 (* 1 = 7.11407 loss)
I0523 06:42:45.669816 35003 sgd_solver.cpp:112] Iteration 165740, lr = 0.001
I0523 06:42:48.525265 35003 solver.cpp:239] Iteration 165750 (2.7901 iter/s, 3.58411s/10 iters), loss = 6.72229
I0523 06:42:48.525312 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72229 (* 1 = 6.72229 loss)
I0523 06:42:48.533358 35003 sgd_solver.cpp:112] Iteration 165750, lr = 0.001
I0523 06:42:53.345623 35003 solver.cpp:239] Iteration 165760 (2.07464 iter/s, 4.82012s/10 iters), loss = 7.11422
I0523 06:42:53.345661 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11422 (* 1 = 7.11422 loss)
I0523 06:42:54.034557 35003 sgd_solver.cpp:112] Iteration 165760, lr = 0.001
I0523 06:42:56.767025 35003 solver.cpp:239] Iteration 165770 (2.92294 iter/s, 3.42121s/10 iters), loss = 7.04359
I0523 06:42:56.767062 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04359 (* 1 = 7.04359 loss)
I0523 06:42:56.774175 35003 sgd_solver.cpp:112] Iteration 165770, lr = 0.001
I0523 06:43:01.759896 35003 solver.cpp:239] Iteration 165780 (2.00295 iter/s, 4.99262s/10 iters), loss = 6.32224
I0523 06:43:01.759939 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32224 (* 1 = 6.32224 loss)
I0523 06:43:02.468868 35003 sgd_solver.cpp:112] Iteration 165780, lr = 0.001
I0523 06:43:04.586732 35003 solver.cpp:239] Iteration 165790 (3.53777 iter/s, 2.82664s/10 iters), loss = 5.00833
I0523 06:43:04.586791 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.00833 (* 1 = 5.00833 loss)
I0523 06:43:04.594065 35003 sgd_solver.cpp:112] Iteration 165790, lr = 0.001
I0523 06:43:07.053777 35003 solver.cpp:239] Iteration 165800 (4.05369 iter/s, 2.46689s/10 iters), loss = 6.42353
I0523 06:43:07.053818 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42353 (* 1 = 6.42353 loss)
I0523 06:43:07.066702 35003 sgd_solver.cpp:112] Iteration 165800, lr = 0.001
I0523 06:43:10.642976 35003 solver.cpp:239] Iteration 165810 (2.78629 iter/s, 3.589s/10 iters), loss = 6.25015
I0523 06:43:10.643152 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25015 (* 1 = 6.25015 loss)
I0523 06:43:10.661125 35003 sgd_solver.cpp:112] Iteration 165810, lr = 0.001
I0523 06:43:14.178189 35003 solver.cpp:239] Iteration 165820 (2.82894 iter/s, 3.53489s/10 iters), loss = 6.49066
I0523 06:43:14.178231 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49066 (* 1 = 6.49066 loss)
I0523 06:43:14.868250 35003 sgd_solver.cpp:112] Iteration 165820, lr = 0.001
I0523 06:43:17.808797 35003 solver.cpp:239] Iteration 165830 (2.75451 iter/s, 3.63041s/10 iters), loss = 6.8672
I0523 06:43:17.808835 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8672 (* 1 = 6.8672 loss)
I0523 06:43:17.822443 35003 sgd_solver.cpp:112] Iteration 165830, lr = 0.001
I0523 06:43:21.361944 35003 solver.cpp:239] Iteration 165840 (2.81455 iter/s, 3.55296s/10 iters), loss = 6.32628
I0523 06:43:21.361994 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32628 (* 1 = 6.32628 loss)
I0523 06:43:21.375097 35003 sgd_solver.cpp:112] Iteration 165840, lr = 0.001
I0523 06:43:25.765269 35003 solver.cpp:239] Iteration 165850 (2.27113 iter/s, 4.40309s/10 iters), loss = 6.9416
I0523 06:43:25.765311 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9416 (* 1 = 6.9416 loss)
I0523 06:43:26.480943 35003 sgd_solver.cpp:112] Iteration 165850, lr = 0.001
I0523 06:43:30.088552 35003 solver.cpp:239] Iteration 165860 (2.31317 iter/s, 4.32306s/10 iters), loss = 7.6507
I0523 06:43:30.088590 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6507 (* 1 = 7.6507 loss)
I0523 06:43:30.150321 35003 sgd_solver.cpp:112] Iteration 165860, lr = 0.001
I0523 06:43:33.014431 35003 solver.cpp:239] Iteration 165870 (3.41798 iter/s, 2.92571s/10 iters), loss = 6.52779
I0523 06:43:33.014482 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52779 (* 1 = 6.52779 loss)
I0523 06:43:33.018440 35003 sgd_solver.cpp:112] Iteration 165870, lr = 0.001
I0523 06:43:36.881829 35003 solver.cpp:239] Iteration 165880 (2.58588 iter/s, 3.86716s/10 iters), loss = 5.82134
I0523 06:43:36.881870 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.82134 (* 1 = 5.82134 loss)
I0523 06:43:36.888963 35003 sgd_solver.cpp:112] Iteration 165880, lr = 0.001
I0523 06:43:40.357425 35003 solver.cpp:239] Iteration 165890 (2.87736 iter/s, 3.47541s/10 iters), loss = 7.36963
I0523 06:43:40.357478 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36963 (* 1 = 7.36963 loss)
I0523 06:43:40.374496 35003 sgd_solver.cpp:112] Iteration 165890, lr = 0.001
I0523 06:43:45.749328 35003 solver.cpp:239] Iteration 165900 (1.85472 iter/s, 5.39165s/10 iters), loss = 7.24854
I0523 06:43:45.749500 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24854 (* 1 = 7.24854 loss)
I0523 06:43:45.762980 35003 sgd_solver.cpp:112] Iteration 165900, lr = 0.001
I0523 06:43:48.692771 35003 solver.cpp:239] Iteration 165910 (3.39772 iter/s, 2.94315s/10 iters), loss = 7.18079
I0523 06:43:48.692816 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18079 (* 1 = 7.18079 loss)
I0523 06:43:49.413698 35003 sgd_solver.cpp:112] Iteration 165910, lr = 0.001
I0523 06:43:51.402179 35003 solver.cpp:239] Iteration 165920 (3.69106 iter/s, 2.70925s/10 iters), loss = 7.08018
I0523 06:43:51.402218 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08018 (* 1 = 7.08018 loss)
I0523 06:43:51.407613 35003 sgd_solver.cpp:112] Iteration 165920, lr = 0.001
I0523 06:43:54.780822 35003 solver.cpp:239] Iteration 165930 (2.95993 iter/s, 3.37846s/10 iters), loss = 6.67847
I0523 06:43:54.780865 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67847 (* 1 = 6.67847 loss)
I0523 06:43:54.789501 35003 sgd_solver.cpp:112] Iteration 165930, lr = 0.001
I0523 06:43:58.286077 35003 solver.cpp:239] Iteration 165940 (2.85302 iter/s, 3.50506s/10 iters), loss = 5.54241
I0523 06:43:58.286134 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.54241 (* 1 = 5.54241 loss)
I0523 06:43:59.026551 35003 sgd_solver.cpp:112] Iteration 165940, lr = 0.001
I0523 06:44:03.509272 35003 solver.cpp:239] Iteration 165950 (1.91464 iter/s, 5.22293s/10 iters), loss = 5.94107
I0523 06:44:03.509310 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.94107 (* 1 = 5.94107 loss)
I0523 06:44:03.527679 35003 sgd_solver.cpp:112] Iteration 165950, lr = 0.001
I0523 06:44:05.731598 35003 solver.cpp:239] Iteration 165960 (4.50007 iter/s, 2.22219s/10 iters), loss = 6.62152
I0523 06:44:05.731653 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62152 (* 1 = 6.62152 loss)
I0523 06:44:05.736979 35003 sgd_solver.cpp:112] Iteration 165960, lr = 0.001
I0523 06:44:10.052868 35003 solver.cpp:239] Iteration 165970 (2.31425 iter/s, 4.32105s/10 iters), loss = 6.45647
I0523 06:44:10.052906 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45647 (* 1 = 6.45647 loss)
I0523 06:44:10.058449 35003 sgd_solver.cpp:112] Iteration 165970, lr = 0.001
I0523 06:44:13.612412 35003 solver.cpp:239] Iteration 165980 (2.8095 iter/s, 3.55936s/10 iters), loss = 5.88062
I0523 06:44:13.612455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.88062 (* 1 = 5.88062 loss)
I0523 06:44:13.654248 35003 sgd_solver.cpp:112] Iteration 165980, lr = 0.001
I0523 06:44:17.197167 35003 solver.cpp:239] Iteration 165990 (2.78974 iter/s, 3.58456s/10 iters), loss = 5.70707
I0523 06:44:17.197414 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.70707 (* 1 = 5.70707 loss)
I0523 06:44:17.205036 35003 sgd_solver.cpp:112] Iteration 165990, lr = 0.001
I0523 06:44:20.837956 35003 solver.cpp:239] Iteration 166000 (2.74694 iter/s, 3.64042s/10 iters), loss = 5.98568
I0523 06:44:20.837997 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98568 (* 1 = 5.98568 loss)
I0523 06:44:21.547176 35003 sgd_solver.cpp:112] Iteration 166000, lr = 0.001
I0523 06:44:26.644908 35003 solver.cpp:239] Iteration 166010 (1.72216 iter/s, 5.80667s/10 iters), loss = 6.52204
I0523 06:44:26.644960 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52204 (* 1 = 6.52204 loss)
I0523 06:44:26.657939 35003 sgd_solver.cpp:112] Iteration 166010, lr = 0.001
I0523 06:44:29.561774 35003 solver.cpp:239] Iteration 166020 (3.42854 iter/s, 2.91669s/10 iters), loss = 7.039
I0523 06:44:29.561820 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.039 (* 1 = 7.039 loss)
I0523 06:44:29.564334 35003 sgd_solver.cpp:112] Iteration 166020, lr = 0.001
I0523 06:44:33.944355 35003 solver.cpp:239] Iteration 166030 (2.28188 iter/s, 4.38235s/10 iters), loss = 6.68353
I0523 06:44:33.944403 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68353 (* 1 = 6.68353 loss)
I0523 06:44:33.950533 35003 sgd_solver.cpp:112] Iteration 166030, lr = 0.001
I0523 06:44:37.377297 35003 solver.cpp:239] Iteration 166040 (2.91313 iter/s, 3.43274s/10 iters), loss = 6.03531
I0523 06:44:37.377354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03531 (* 1 = 6.03531 loss)
I0523 06:44:38.063226 35003 sgd_solver.cpp:112] Iteration 166040, lr = 0.001
I0523 06:44:41.753312 35003 solver.cpp:239] Iteration 166050 (2.2853 iter/s, 4.37578s/10 iters), loss = 7.12799
I0523 06:44:41.753350 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12799 (* 1 = 7.12799 loss)
I0523 06:44:41.987426 35003 sgd_solver.cpp:112] Iteration 166050, lr = 0.001
I0523 06:44:43.599545 35003 solver.cpp:239] Iteration 166060 (5.4168 iter/s, 1.84611s/10 iters), loss = 6.51408
I0523 06:44:43.599591 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51408 (* 1 = 6.51408 loss)
I0523 06:44:43.607580 35003 sgd_solver.cpp:112] Iteration 166060, lr = 0.001
I0523 06:44:47.137738 35003 solver.cpp:239] Iteration 166070 (2.82646 iter/s, 3.538s/10 iters), loss = 7.16505
I0523 06:44:47.137792 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16505 (* 1 = 7.16505 loss)
I0523 06:44:47.151329 35003 sgd_solver.cpp:112] Iteration 166070, lr = 0.001
I0523 06:44:51.943943 35003 solver.cpp:239] Iteration 166080 (2.08076 iter/s, 4.80594s/10 iters), loss = 6.54406
I0523 06:44:51.944254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54406 (* 1 = 6.54406 loss)
I0523 06:44:51.949120 35003 sgd_solver.cpp:112] Iteration 166080, lr = 0.001
I0523 06:44:54.737782 35003 solver.cpp:239] Iteration 166090 (3.57981 iter/s, 2.79344s/10 iters), loss = 6.16684
I0523 06:44:54.737821 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16684 (* 1 = 6.16684 loss)
I0523 06:44:54.743640 35003 sgd_solver.cpp:112] Iteration 166090, lr = 0.001
I0523 06:44:58.904709 35003 solver.cpp:239] Iteration 166100 (2.39997 iter/s, 4.16671s/10 iters), loss = 6.84891
I0523 06:44:58.904767 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84891 (* 1 = 6.84891 loss)
I0523 06:44:59.639597 35003 sgd_solver.cpp:112] Iteration 166100, lr = 0.001
I0523 06:45:03.252558 35003 solver.cpp:239] Iteration 166110 (2.30014 iter/s, 4.34756s/10 iters), loss = 6.42422
I0523 06:45:03.252614 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42422 (* 1 = 6.42422 loss)
I0523 06:45:03.955581 35003 sgd_solver.cpp:112] Iteration 166110, lr = 0.001
I0523 06:45:08.103538 35003 solver.cpp:239] Iteration 166120 (2.06155 iter/s, 4.85072s/10 iters), loss = 7.67236
I0523 06:45:08.103585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67236 (* 1 = 7.67236 loss)
I0523 06:45:08.109750 35003 sgd_solver.cpp:112] Iteration 166120, lr = 0.001
I0523 06:45:10.564095 35003 solver.cpp:239] Iteration 166130 (4.06438 iter/s, 2.4604s/10 iters), loss = 6.78935
I0523 06:45:10.564142 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78935 (* 1 = 6.78935 loss)
I0523 06:45:10.573093 35003 sgd_solver.cpp:112] Iteration 166130, lr = 0.001
I0523 06:45:15.720311 35003 solver.cpp:239] Iteration 166140 (1.93951 iter/s, 5.15595s/10 iters), loss = 6.44984
I0523 06:45:15.720371 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44984 (* 1 = 6.44984 loss)
I0523 06:45:15.730892 35003 sgd_solver.cpp:112] Iteration 166140, lr = 0.001
I0523 06:45:18.851264 35003 solver.cpp:239] Iteration 166150 (3.19411 iter/s, 3.13076s/10 iters), loss = 7.06883
I0523 06:45:18.851308 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06883 (* 1 = 7.06883 loss)
I0523 06:45:18.858959 35003 sgd_solver.cpp:112] Iteration 166150, lr = 0.001
I0523 06:45:23.323410 35003 solver.cpp:239] Iteration 166160 (2.23618 iter/s, 4.47192s/10 iters), loss = 7.43468
I0523 06:45:23.323536 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43468 (* 1 = 7.43468 loss)
I0523 06:45:23.345871 35003 sgd_solver.cpp:112] Iteration 166160, lr = 0.001
I0523 06:45:27.736260 35003 solver.cpp:239] Iteration 166170 (2.26627 iter/s, 4.41254s/10 iters), loss = 6.06713
I0523 06:45:27.736320 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06713 (* 1 = 6.06713 loss)
I0523 06:45:27.959362 35003 sgd_solver.cpp:112] Iteration 166170, lr = 0.001
I0523 06:45:32.148615 35003 solver.cpp:239] Iteration 166180 (2.26649 iter/s, 4.4121s/10 iters), loss = 6.81113
I0523 06:45:32.148667 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81113 (* 1 = 6.81113 loss)
I0523 06:45:32.713402 35003 sgd_solver.cpp:112] Iteration 166180, lr = 0.001
I0523 06:45:36.502916 35003 solver.cpp:239] Iteration 166190 (2.2967 iter/s, 4.35406s/10 iters), loss = 6.29245
I0523 06:45:36.502974 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29245 (* 1 = 6.29245 loss)
I0523 06:45:36.509356 35003 sgd_solver.cpp:112] Iteration 166190, lr = 0.001
I0523 06:45:39.465128 35003 solver.cpp:239] Iteration 166200 (3.37606 iter/s, 2.96203s/10 iters), loss = 5.88849
I0523 06:45:39.465178 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.88849 (* 1 = 5.88849 loss)
I0523 06:45:39.467777 35003 sgd_solver.cpp:112] Iteration 166200, lr = 0.001
I0523 06:45:43.058255 35003 solver.cpp:239] Iteration 166210 (2.78326 iter/s, 3.59291s/10 iters), loss = 6.60519
I0523 06:45:43.058313 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60519 (* 1 = 6.60519 loss)
I0523 06:45:43.063799 35003 sgd_solver.cpp:112] Iteration 166210, lr = 0.001
I0523 06:45:48.131047 35003 solver.cpp:239] Iteration 166220 (1.9714 iter/s, 5.07252s/10 iters), loss = 6.30144
I0523 06:45:48.131114 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30144 (* 1 = 6.30144 loss)
I0523 06:45:48.155920 35003 sgd_solver.cpp:112] Iteration 166220, lr = 0.001
I0523 06:45:50.980252 35003 solver.cpp:239] Iteration 166230 (3.50997 iter/s, 2.84902s/10 iters), loss = 6.4859
I0523 06:45:50.980294 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4859 (* 1 = 6.4859 loss)
I0523 06:45:51.682824 35003 sgd_solver.cpp:112] Iteration 166230, lr = 0.001
I0523 06:45:55.438047 35003 solver.cpp:239] Iteration 166240 (2.24338 iter/s, 4.45756s/10 iters), loss = 7.3888
I0523 06:45:55.438282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3888 (* 1 = 7.3888 loss)
I0523 06:45:55.445962 35003 sgd_solver.cpp:112] Iteration 166240, lr = 0.001
I0523 06:45:58.226480 35003 solver.cpp:239] Iteration 166250 (3.5867 iter/s, 2.78808s/10 iters), loss = 5.86656
I0523 06:45:58.226521 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.86656 (* 1 = 5.86656 loss)
I0523 06:45:58.239486 35003 sgd_solver.cpp:112] Iteration 166250, lr = 0.001
I0523 06:46:01.444756 35003 solver.cpp:239] Iteration 166260 (3.10743 iter/s, 3.2181s/10 iters), loss = 7.78148
I0523 06:46:01.444792 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78148 (* 1 = 7.78148 loss)
I0523 06:46:01.457794 35003 sgd_solver.cpp:112] Iteration 166260, lr = 0.001
I0523 06:46:04.519433 35003 solver.cpp:239] Iteration 166270 (3.25256 iter/s, 3.0745s/10 iters), loss = 7.12746
I0523 06:46:04.519484 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12746 (* 1 = 7.12746 loss)
I0523 06:46:05.260466 35003 sgd_solver.cpp:112] Iteration 166270, lr = 0.001
I0523 06:46:08.915621 35003 solver.cpp:239] Iteration 166280 (2.27481 iter/s, 4.39596s/10 iters), loss = 6.54168
I0523 06:46:08.915660 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54168 (* 1 = 6.54168 loss)
I0523 06:46:09.602129 35003 sgd_solver.cpp:112] Iteration 166280, lr = 0.001
I0523 06:46:13.078060 35003 solver.cpp:239] Iteration 166290 (2.40256 iter/s, 4.16222s/10 iters), loss = 5.98897
I0523 06:46:13.078111 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98897 (* 1 = 5.98897 loss)
I0523 06:46:13.091266 35003 sgd_solver.cpp:112] Iteration 166290, lr = 0.001
I0523 06:46:14.491994 35003 solver.cpp:239] Iteration 166300 (7.07311 iter/s, 1.41381s/10 iters), loss = 5.93236
I0523 06:46:14.492072 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93236 (* 1 = 5.93236 loss)
I0523 06:46:15.136013 35003 sgd_solver.cpp:112] Iteration 166300, lr = 0.001
I0523 06:46:18.316164 35003 solver.cpp:239] Iteration 166310 (2.61511 iter/s, 3.82393s/10 iters), loss = 5.87825
I0523 06:46:18.316206 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87825 (* 1 = 5.87825 loss)
I0523 06:46:18.320385 35003 sgd_solver.cpp:112] Iteration 166310, lr = 0.001
I0523 06:46:20.545579 35003 solver.cpp:239] Iteration 166320 (4.48577 iter/s, 2.22927s/10 iters), loss = 6.92034
I0523 06:46:20.545627 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92034 (* 1 = 6.92034 loss)
I0523 06:46:20.566329 35003 sgd_solver.cpp:112] Iteration 166320, lr = 0.001
I0523 06:46:23.284283 35003 solver.cpp:239] Iteration 166330 (3.65158 iter/s, 2.73854s/10 iters), loss = 6.10267
I0523 06:46:23.284335 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10267 (* 1 = 6.10267 loss)
I0523 06:46:23.292510 35003 sgd_solver.cpp:112] Iteration 166330, lr = 0.001
I0523 06:46:26.518705 35003 solver.cpp:239] Iteration 166340 (3.09192 iter/s, 3.23424s/10 iters), loss = 8.10846
I0523 06:46:26.518936 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.10846 (* 1 = 8.10846 loss)
I0523 06:46:27.227787 35003 sgd_solver.cpp:112] Iteration 166340, lr = 0.001
I0523 06:46:30.093076 35003 solver.cpp:239] Iteration 166350 (2.798 iter/s, 3.57398s/10 iters), loss = 7.02668
I0523 06:46:30.093134 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02668 (* 1 = 7.02668 loss)
I0523 06:46:30.099747 35003 sgd_solver.cpp:112] Iteration 166350, lr = 0.001
I0523 06:46:33.673741 35003 solver.cpp:239] Iteration 166360 (2.79294 iter/s, 3.58046s/10 iters), loss = 7.03397
I0523 06:46:33.673787 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03397 (* 1 = 7.03397 loss)
I0523 06:46:34.389183 35003 sgd_solver.cpp:112] Iteration 166360, lr = 0.001
I0523 06:46:36.399417 35003 solver.cpp:239] Iteration 166370 (3.66903 iter/s, 2.72551s/10 iters), loss = 7.37449
I0523 06:46:36.399461 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37449 (* 1 = 7.37449 loss)
I0523 06:46:36.413033 35003 sgd_solver.cpp:112] Iteration 166370, lr = 0.001
I0523 06:46:40.745972 35003 solver.cpp:239] Iteration 166380 (2.3008 iter/s, 4.34632s/10 iters), loss = 6.56638
I0523 06:46:40.746017 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56638 (* 1 = 6.56638 loss)
I0523 06:46:40.763234 35003 sgd_solver.cpp:112] Iteration 166380, lr = 0.001
I0523 06:46:45.904033 35003 solver.cpp:239] Iteration 166390 (1.93881 iter/s, 5.15781s/10 iters), loss = 6.59495
I0523 06:46:45.904081 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59495 (* 1 = 6.59495 loss)
I0523 06:46:45.948812 35003 sgd_solver.cpp:112] Iteration 166390, lr = 0.001
I0523 06:46:49.357822 35003 solver.cpp:239] Iteration 166400 (2.89554 iter/s, 3.45359s/10 iters), loss = 6.17521
I0523 06:46:49.357880 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17521 (* 1 = 6.17521 loss)
I0523 06:46:49.359416 35003 sgd_solver.cpp:112] Iteration 166400, lr = 0.001
I0523 06:46:54.470476 35003 solver.cpp:239] Iteration 166410 (1.95603 iter/s, 5.11239s/10 iters), loss = 6.06411
I0523 06:46:54.470513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06411 (* 1 = 6.06411 loss)
I0523 06:46:54.478495 35003 sgd_solver.cpp:112] Iteration 166410, lr = 0.001
I0523 06:46:58.898706 35003 solver.cpp:239] Iteration 166420 (2.25835 iter/s, 4.42801s/10 iters), loss = 5.87873
I0523 06:46:58.898959 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87873 (* 1 = 5.87873 loss)
I0523 06:46:59.505791 35003 sgd_solver.cpp:112] Iteration 166420, lr = 0.001
I0523 06:47:01.525771 35003 solver.cpp:239] Iteration 166430 (3.80702 iter/s, 2.62672s/10 iters), loss = 6.29408
I0523 06:47:01.525825 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29408 (* 1 = 6.29408 loss)
I0523 06:47:02.263661 35003 sgd_solver.cpp:112] Iteration 166430, lr = 0.001
I0523 06:47:05.874441 35003 solver.cpp:239] Iteration 166440 (2.29968 iter/s, 4.34844s/10 iters), loss = 7.58088
I0523 06:47:05.874480 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58088 (* 1 = 7.58088 loss)
I0523 06:47:05.887593 35003 sgd_solver.cpp:112] Iteration 166440, lr = 0.001
I0523 06:47:09.481639 35003 solver.cpp:239] Iteration 166450 (2.77238 iter/s, 3.60701s/10 iters), loss = 7.41639
I0523 06:47:09.481694 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41639 (* 1 = 7.41639 loss)
I0523 06:47:09.503103 35003 sgd_solver.cpp:112] Iteration 166450, lr = 0.001
I0523 06:47:12.391518 35003 solver.cpp:239] Iteration 166460 (3.43678 iter/s, 2.9097s/10 iters), loss = 6.17423
I0523 06:47:12.391566 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17423 (* 1 = 6.17423 loss)
I0523 06:47:12.394187 35003 sgd_solver.cpp:112] Iteration 166460, lr = 0.001
I0523 06:47:15.680213 35003 solver.cpp:239] Iteration 166470 (3.0409 iter/s, 3.2885s/10 iters), loss = 6.13476
I0523 06:47:15.680255 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13476 (* 1 = 6.13476 loss)
I0523 06:47:16.421253 35003 sgd_solver.cpp:112] Iteration 166470, lr = 0.001
I0523 06:47:19.249756 35003 solver.cpp:239] Iteration 166480 (2.80164 iter/s, 3.56934s/10 iters), loss = 6.34814
I0523 06:47:19.249824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34814 (* 1 = 6.34814 loss)
I0523 06:47:19.294757 35003 sgd_solver.cpp:112] Iteration 166480, lr = 0.001
I0523 06:47:22.879024 35003 solver.cpp:239] Iteration 166490 (2.75554 iter/s, 3.62905s/10 iters), loss = 6.42263
I0523 06:47:22.879076 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42263 (* 1 = 6.42263 loss)
I0523 06:47:23.580649 35003 sgd_solver.cpp:112] Iteration 166490, lr = 0.001
I0523 06:47:25.875198 35003 solver.cpp:239] Iteration 166500 (3.3378 iter/s, 2.99599s/10 iters), loss = 7.12435
I0523 06:47:25.875252 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12435 (* 1 = 7.12435 loss)
I0523 06:47:25.880384 35003 sgd_solver.cpp:112] Iteration 166500, lr = 0.001
I0523 06:47:29.857074 35003 solver.cpp:239] Iteration 166510 (2.51153 iter/s, 3.98164s/10 iters), loss = 5.909
I0523 06:47:29.857381 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.909 (* 1 = 5.909 loss)
I0523 06:47:29.881991 35003 sgd_solver.cpp:112] Iteration 166510, lr = 0.001
I0523 06:47:32.563498 35003 solver.cpp:239] Iteration 166520 (3.69544 iter/s, 2.70604s/10 iters), loss = 6.22365
I0523 06:47:32.563546 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22365 (* 1 = 6.22365 loss)
I0523 06:47:32.576097 35003 sgd_solver.cpp:112] Iteration 166520, lr = 0.001
I0523 06:47:36.529494 35003 solver.cpp:239] Iteration 166530 (2.52158 iter/s, 3.96578s/10 iters), loss = 7.1323
I0523 06:47:36.529542 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1323 (* 1 = 7.1323 loss)
I0523 06:47:37.238327 35003 sgd_solver.cpp:112] Iteration 166530, lr = 0.001
I0523 06:47:39.293352 35003 solver.cpp:239] Iteration 166540 (3.61834 iter/s, 2.7637s/10 iters), loss = 6.24564
I0523 06:47:39.293390 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24564 (* 1 = 6.24564 loss)
I0523 06:47:39.312450 35003 sgd_solver.cpp:112] Iteration 166540, lr = 0.001
I0523 06:47:43.117475 35003 solver.cpp:239] Iteration 166550 (2.61512 iter/s, 3.82392s/10 iters), loss = 6.91646
I0523 06:47:43.117516 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91646 (* 1 = 6.91646 loss)
I0523 06:47:43.130075 35003 sgd_solver.cpp:112] Iteration 166550, lr = 0.001
I0523 06:47:46.820459 35003 solver.cpp:239] Iteration 166560 (2.70067 iter/s, 3.70279s/10 iters), loss = 6.48238
I0523 06:47:46.820513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48238 (* 1 = 6.48238 loss)
I0523 06:47:47.529937 35003 sgd_solver.cpp:112] Iteration 166560, lr = 0.001
I0523 06:47:51.115005 35003 solver.cpp:239] Iteration 166570 (2.32867 iter/s, 4.2943s/10 iters), loss = 7.61101
I0523 06:47:51.115072 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61101 (* 1 = 7.61101 loss)
I0523 06:47:51.823192 35003 sgd_solver.cpp:112] Iteration 166570, lr = 0.001
I0523 06:47:56.294256 35003 solver.cpp:239] Iteration 166580 (1.93088 iter/s, 5.17898s/10 iters), loss = 6.5847
I0523 06:47:56.294296 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5847 (* 1 = 6.5847 loss)
I0523 06:47:56.303582 35003 sgd_solver.cpp:112] Iteration 166580, lr = 0.001
I0523 06:48:00.872961 35003 solver.cpp:239] Iteration 166590 (2.18414 iter/s, 4.57847s/10 iters), loss = 7.20887
I0523 06:48:00.873291 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20887 (* 1 = 7.20887 loss)
I0523 06:48:01.573635 35003 sgd_solver.cpp:112] Iteration 166590, lr = 0.001
I0523 06:48:04.351483 35003 solver.cpp:239] Iteration 166600 (2.87877 iter/s, 3.47371s/10 iters), loss = 6.27754
I0523 06:48:04.351531 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27754 (* 1 = 6.27754 loss)
I0523 06:48:05.044878 35003 sgd_solver.cpp:112] Iteration 166600, lr = 0.001
I0523 06:48:07.188108 35003 solver.cpp:239] Iteration 166610 (3.52553 iter/s, 2.83646s/10 iters), loss = 7.19791
I0523 06:48:07.188169 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19791 (* 1 = 7.19791 loss)
I0523 06:48:07.923173 35003 sgd_solver.cpp:112] Iteration 166610, lr = 0.001
I0523 06:48:11.453500 35003 solver.cpp:239] Iteration 166620 (2.34458 iter/s, 4.26515s/10 iters), loss = 6.62475
I0523 06:48:11.453552 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62475 (* 1 = 6.62475 loss)
I0523 06:48:11.458720 35003 sgd_solver.cpp:112] Iteration 166620, lr = 0.001
I0523 06:48:15.617462 35003 solver.cpp:239] Iteration 166630 (2.40172 iter/s, 4.16369s/10 iters), loss = 6.64198
I0523 06:48:15.617519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64198 (* 1 = 6.64198 loss)
I0523 06:48:16.142505 35003 sgd_solver.cpp:112] Iteration 166630, lr = 0.001
I0523 06:48:20.633774 35003 solver.cpp:239] Iteration 166640 (1.9936 iter/s, 5.01604s/10 iters), loss = 5.70371
I0523 06:48:20.633834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.70371 (* 1 = 5.70371 loss)
I0523 06:48:20.650923 35003 sgd_solver.cpp:112] Iteration 166640, lr = 0.001
I0523 06:48:24.856848 35003 solver.cpp:239] Iteration 166650 (2.36807 iter/s, 4.22284s/10 iters), loss = 6.00983
I0523 06:48:24.856896 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00983 (* 1 = 6.00983 loss)
I0523 06:48:24.870287 35003 sgd_solver.cpp:112] Iteration 166650, lr = 0.001
I0523 06:48:28.464797 35003 solver.cpp:239] Iteration 166660 (2.77181 iter/s, 3.60775s/10 iters), loss = 6.24177
I0523 06:48:28.464848 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24177 (* 1 = 6.24177 loss)
I0523 06:48:29.193300 35003 sgd_solver.cpp:112] Iteration 166660, lr = 0.001
I0523 06:48:33.541548 35003 solver.cpp:239] Iteration 166670 (1.96986 iter/s, 5.07649s/10 iters), loss = 6.5468
I0523 06:48:33.541762 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5468 (* 1 = 6.5468 loss)
I0523 06:48:33.564185 35003 sgd_solver.cpp:112] Iteration 166670, lr = 0.001
I0523 06:48:35.663550 35003 solver.cpp:239] Iteration 166680 (4.71316 iter/s, 2.12172s/10 iters), loss = 6.59481
I0523 06:48:35.663589 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59481 (* 1 = 6.59481 loss)
I0523 06:48:35.677022 35003 sgd_solver.cpp:112] Iteration 166680, lr = 0.001
I0523 06:48:39.084976 35003 solver.cpp:239] Iteration 166690 (2.92292 iter/s, 3.42124s/10 iters), loss = 5.69593
I0523 06:48:39.085028 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.69593 (* 1 = 5.69593 loss)
I0523 06:48:39.800343 35003 sgd_solver.cpp:112] Iteration 166690, lr = 0.001
I0523 06:48:43.412853 35003 solver.cpp:239] Iteration 166700 (2.31073 iter/s, 4.32765s/10 iters), loss = 7.05278
I0523 06:48:43.412899 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05278 (* 1 = 7.05278 loss)
I0523 06:48:43.424279 35003 sgd_solver.cpp:112] Iteration 166700, lr = 0.001
I0523 06:48:47.929410 35003 solver.cpp:239] Iteration 166710 (2.21419 iter/s, 4.51632s/10 iters), loss = 6.08335
I0523 06:48:47.929467 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08335 (* 1 = 6.08335 loss)
I0523 06:48:47.955348 35003 sgd_solver.cpp:112] Iteration 166710, lr = 0.001
I0523 06:48:50.732481 35003 solver.cpp:239] Iteration 166720 (3.56775 iter/s, 2.80289s/10 iters), loss = 6.95127
I0523 06:48:50.732528 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95127 (* 1 = 6.95127 loss)
I0523 06:48:50.759660 35003 sgd_solver.cpp:112] Iteration 166720, lr = 0.001
I0523 06:48:53.602180 35003 solver.cpp:239] Iteration 166730 (3.48489 iter/s, 2.86953s/10 iters), loss = 6.97225
I0523 06:48:53.602228 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97225 (* 1 = 6.97225 loss)
I0523 06:48:54.317777 35003 sgd_solver.cpp:112] Iteration 166730, lr = 0.001
I0523 06:48:58.191123 35003 solver.cpp:239] Iteration 166740 (2.17926 iter/s, 4.58871s/10 iters), loss = 6.48163
I0523 06:48:58.191171 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48163 (* 1 = 6.48163 loss)
I0523 06:48:58.204205 35003 sgd_solver.cpp:112] Iteration 166740, lr = 0.001
I0523 06:49:00.181949 35003 solver.cpp:239] Iteration 166750 (5.02339 iter/s, 1.99069s/10 iters), loss = 5.61906
I0523 06:49:00.182003 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.61906 (* 1 = 5.61906 loss)
I0523 06:49:00.891470 35003 sgd_solver.cpp:112] Iteration 166750, lr = 0.001
I0523 06:49:02.371701 35003 solver.cpp:239] Iteration 166760 (4.56703 iter/s, 2.18961s/10 iters), loss = 6.06335
I0523 06:49:02.371745 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06335 (* 1 = 6.06335 loss)
I0523 06:49:03.027961 35003 sgd_solver.cpp:112] Iteration 166760, lr = 0.001
I0523 06:49:06.401593 35003 solver.cpp:239] Iteration 166770 (2.48159 iter/s, 4.02968s/10 iters), loss = 6.11539
I0523 06:49:06.401921 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11539 (* 1 = 6.11539 loss)
I0523 06:49:06.408406 35003 sgd_solver.cpp:112] Iteration 166770, lr = 0.001
I0523 06:49:09.190862 35003 solver.cpp:239] Iteration 166780 (3.5857 iter/s, 2.78885s/10 iters), loss = 7.29081
I0523 06:49:09.190909 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29081 (* 1 = 7.29081 loss)
I0523 06:49:09.204290 35003 sgd_solver.cpp:112] Iteration 166780, lr = 0.001
I0523 06:49:11.682952 35003 solver.cpp:239] Iteration 166790 (4.01295 iter/s, 2.49193s/10 iters), loss = 5.91513
I0523 06:49:11.683002 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91513 (* 1 = 5.91513 loss)
I0523 06:49:11.688021 35003 sgd_solver.cpp:112] Iteration 166790, lr = 0.001
I0523 06:49:14.775070 35003 solver.cpp:239] Iteration 166800 (3.23422 iter/s, 3.09194s/10 iters), loss = 7.27855
I0523 06:49:14.775110 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27855 (* 1 = 7.27855 loss)
I0523 06:49:14.779855 35003 sgd_solver.cpp:112] Iteration 166800, lr = 0.001
I0523 06:49:16.881765 35003 solver.cpp:239] Iteration 166810 (4.74708 iter/s, 2.10656s/10 iters), loss = 6.72763
I0523 06:49:16.881820 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72763 (* 1 = 6.72763 loss)
I0523 06:49:17.484159 35003 sgd_solver.cpp:112] Iteration 166810, lr = 0.001
I0523 06:49:18.937086 35003 solver.cpp:239] Iteration 166820 (4.86577 iter/s, 2.05517s/10 iters), loss = 5.63747
I0523 06:49:18.937139 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.63747 (* 1 = 5.63747 loss)
I0523 06:49:19.522733 35003 sgd_solver.cpp:112] Iteration 166820, lr = 0.001
I0523 06:49:22.110996 35003 solver.cpp:239] Iteration 166830 (3.15088 iter/s, 3.17372s/10 iters), loss = 6.89478
I0523 06:49:22.111049 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89478 (* 1 = 6.89478 loss)
I0523 06:49:22.117063 35003 sgd_solver.cpp:112] Iteration 166830, lr = 0.001
I0523 06:49:27.218833 35003 solver.cpp:239] Iteration 166840 (1.95788 iter/s, 5.10756s/10 iters), loss = 6.32943
I0523 06:49:27.218881 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32943 (* 1 = 6.32943 loss)
I0523 06:49:27.223408 35003 sgd_solver.cpp:112] Iteration 166840, lr = 0.001
I0523 06:49:29.261513 35003 solver.cpp:239] Iteration 166850 (4.89588 iter/s, 2.04253s/10 iters), loss = 7.30864
I0523 06:49:29.261550 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30864 (* 1 = 7.30864 loss)
I0523 06:49:29.274703 35003 sgd_solver.cpp:112] Iteration 166850, lr = 0.001
I0523 06:49:32.014982 35003 solver.cpp:239] Iteration 166860 (3.63199 iter/s, 2.75331s/10 iters), loss = 7.00582
I0523 06:49:32.015027 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00582 (* 1 = 7.00582 loss)
I0523 06:49:32.018182 35003 sgd_solver.cpp:112] Iteration 166860, lr = 0.001
I0523 06:49:34.883298 35003 solver.cpp:239] Iteration 166870 (3.48658 iter/s, 2.86814s/10 iters), loss = 6.42574
I0523 06:49:34.883352 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42574 (* 1 = 6.42574 loss)
I0523 06:49:35.290469 35003 sgd_solver.cpp:112] Iteration 166870, lr = 0.001
I0523 06:49:39.557000 35003 solver.cpp:239] Iteration 166880 (2.13975 iter/s, 4.67344s/10 iters), loss = 6.78954
I0523 06:49:39.557327 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78954 (* 1 = 6.78954 loss)
I0523 06:49:39.559985 35003 sgd_solver.cpp:112] Iteration 166880, lr = 0.001
I0523 06:49:44.021592 35003 solver.cpp:239] Iteration 166890 (2.24011 iter/s, 4.46406s/10 iters), loss = 8.12346
I0523 06:49:44.021634 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.12346 (* 1 = 8.12346 loss)
I0523 06:49:44.760717 35003 sgd_solver.cpp:112] Iteration 166890, lr = 0.001
I0523 06:49:47.436756 35003 solver.cpp:239] Iteration 166900 (2.92829 iter/s, 3.41497s/10 iters), loss = 8.16093
I0523 06:49:47.436807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16093 (* 1 = 8.16093 loss)
I0523 06:49:48.177301 35003 sgd_solver.cpp:112] Iteration 166900, lr = 0.001
I0523 06:49:51.852394 35003 solver.cpp:239] Iteration 166910 (2.2648 iter/s, 4.4154s/10 iters), loss = 6.66793
I0523 06:49:51.852444 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66793 (* 1 = 6.66793 loss)
I0523 06:49:51.855128 35003 sgd_solver.cpp:112] Iteration 166910, lr = 0.001
I0523 06:49:55.425668 35003 solver.cpp:239] Iteration 166920 (2.79873 iter/s, 3.57305s/10 iters), loss = 5.87916
I0523 06:49:55.425729 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87916 (* 1 = 5.87916 loss)
I0523 06:49:55.433755 35003 sgd_solver.cpp:112] Iteration 166920, lr = 0.001
I0523 06:49:56.264449 35003 solver.cpp:239] Iteration 166930 (11.9236 iter/s, 0.838674s/10 iters), loss = 6.38946
I0523 06:49:56.264492 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38946 (* 1 = 6.38946 loss)
I0523 06:49:56.268885 35003 sgd_solver.cpp:112] Iteration 166930, lr = 0.001
I0523 06:49:57.367851 35003 solver.cpp:239] Iteration 166940 (9.06375 iter/s, 1.1033s/10 iters), loss = 6.2145
I0523 06:49:57.367895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2145 (* 1 = 6.2145 loss)
I0523 06:49:57.405506 35003 sgd_solver.cpp:112] Iteration 166940, lr = 0.001
I0523 06:49:58.240665 35003 solver.cpp:239] Iteration 166950 (11.4584 iter/s, 0.872724s/10 iters), loss = 6.51997
I0523 06:49:58.240712 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51997 (* 1 = 6.51997 loss)
I0523 06:49:58.247138 35003 sgd_solver.cpp:112] Iteration 166950, lr = 0.001
I0523 06:49:59.076115 35003 solver.cpp:239] Iteration 166960 (11.9709 iter/s, 0.835358s/10 iters), loss = 6.52024
I0523 06:49:59.076170 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52024 (* 1 = 6.52024 loss)
I0523 06:49:59.080659 35003 sgd_solver.cpp:112] Iteration 166960, lr = 0.001
I0523 06:49:59.894922 35003 solver.cpp:239] Iteration 166970 (12.2144 iter/s, 0.818704s/10 iters), loss = 5.93343
I0523 06:49:59.894963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93343 (* 1 = 5.93343 loss)
I0523 06:49:59.901234 35003 sgd_solver.cpp:112] Iteration 166970, lr = 0.001
I0523 06:50:00.677484 35003 solver.cpp:239] Iteration 166980 (12.78 iter/s, 0.782475s/10 iters), loss = 7.57946
I0523 06:50:00.677534 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57946 (* 1 = 7.57946 loss)
I0523 06:50:00.689271 35003 sgd_solver.cpp:112] Iteration 166980, lr = 0.001
I0523 06:50:01.613905 35003 solver.cpp:239] Iteration 166990 (10.6801 iter/s, 0.936324s/10 iters), loss = 6.50941
I0523 06:50:01.613942 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50941 (* 1 = 6.50941 loss)
I0523 06:50:01.623997 35003 sgd_solver.cpp:112] Iteration 166990, lr = 0.001
I0523 06:50:02.445646 35003 solver.cpp:239] Iteration 167000 (12.0242 iter/s, 0.831658s/10 iters), loss = 6.5276
I0523 06:50:02.445684 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5276 (* 1 = 6.5276 loss)
I0523 06:50:02.454581 35003 sgd_solver.cpp:112] Iteration 167000, lr = 0.001
I0523 06:50:03.260166 35003 solver.cpp:239] Iteration 167010 (12.2785 iter/s, 0.814431s/10 iters), loss = 7.88891
I0523 06:50:03.260207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88891 (* 1 = 7.88891 loss)
I0523 06:50:03.269345 35003 sgd_solver.cpp:112] Iteration 167010, lr = 0.001
I0523 06:50:05.891957 35003 solver.cpp:239] Iteration 167020 (3.79992 iter/s, 2.63164s/10 iters), loss = 5.79833
I0523 06:50:05.891999 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.79833 (* 1 = 5.79833 loss)
I0523 06:50:05.915369 35003 sgd_solver.cpp:112] Iteration 167020, lr = 0.001
I0523 06:50:08.849894 35003 solver.cpp:239] Iteration 167030 (3.38093 iter/s, 2.95776s/10 iters), loss = 6.57072
I0523 06:50:08.849939 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57072 (* 1 = 6.57072 loss)
I0523 06:50:08.876442 35003 sgd_solver.cpp:112] Iteration 167030, lr = 0.001
I0523 06:50:11.692818 35003 solver.cpp:239] Iteration 167040 (3.51771 iter/s, 2.84276s/10 iters), loss = 5.72313
I0523 06:50:11.693065 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.72313 (* 1 = 5.72313 loss)
I0523 06:50:11.711259 35003 sgd_solver.cpp:112] Iteration 167040, lr = 0.001
I0523 06:50:14.474772 35003 solver.cpp:239] Iteration 167050 (3.59504 iter/s, 2.78161s/10 iters), loss = 6.65172
I0523 06:50:14.474818 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65172 (* 1 = 6.65172 loss)
I0523 06:50:15.209919 35003 sgd_solver.cpp:112] Iteration 167050, lr = 0.001
I0523 06:50:20.179069 35003 solver.cpp:239] Iteration 167060 (1.75315 iter/s, 5.70402s/10 iters), loss = 6.52895
I0523 06:50:20.179107 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52895 (* 1 = 6.52895 loss)
I0523 06:50:20.862078 35003 sgd_solver.cpp:112] Iteration 167060, lr = 0.001
I0523 06:50:23.036306 35003 solver.cpp:239] Iteration 167070 (3.50009 iter/s, 2.85707s/10 iters), loss = 6.43072
I0523 06:50:23.036355 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43072 (* 1 = 6.43072 loss)
I0523 06:50:23.043288 35003 sgd_solver.cpp:112] Iteration 167070, lr = 0.001
I0523 06:50:25.885229 35003 solver.cpp:239] Iteration 167080 (3.51031 iter/s, 2.84875s/10 iters), loss = 7.50622
I0523 06:50:25.885272 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50622 (* 1 = 7.50622 loss)
I0523 06:50:26.177147 35003 sgd_solver.cpp:112] Iteration 167080, lr = 0.001
I0523 06:50:28.997728 35003 solver.cpp:239] Iteration 167090 (3.21305 iter/s, 3.11231s/10 iters), loss = 7.84541
I0523 06:50:28.997786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84541 (* 1 = 7.84541 loss)
I0523 06:50:29.738409 35003 sgd_solver.cpp:112] Iteration 167090, lr = 0.001
I0523 06:50:35.485383 35003 solver.cpp:239] Iteration 167100 (1.54146 iter/s, 6.48734s/10 iters), loss = 5.80655
I0523 06:50:35.485426 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.80655 (* 1 = 5.80655 loss)
I0523 06:50:35.497773 35003 sgd_solver.cpp:112] Iteration 167100, lr = 0.001
I0523 06:50:38.977046 35003 solver.cpp:239] Iteration 167110 (2.86412 iter/s, 3.49147s/10 iters), loss = 6.36383
I0523 06:50:38.977092 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36383 (* 1 = 6.36383 loss)
I0523 06:50:38.979564 35003 sgd_solver.cpp:112] Iteration 167110, lr = 0.001
I0523 06:50:41.794342 35003 solver.cpp:239] Iteration 167120 (3.54976 iter/s, 2.81709s/10 iters), loss = 6.72187
I0523 06:50:41.794613 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72187 (* 1 = 6.72187 loss)
I0523 06:50:41.802186 35003 sgd_solver.cpp:112] Iteration 167120, lr = 0.001
I0523 06:50:43.993290 35003 solver.cpp:239] Iteration 167130 (4.54833 iter/s, 2.19861s/10 iters), loss = 5.71013
I0523 06:50:43.993332 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.71013 (* 1 = 5.71013 loss)
I0523 06:50:44.005981 35003 sgd_solver.cpp:112] Iteration 167130, lr = 0.001
I0523 06:50:47.601955 35003 solver.cpp:239] Iteration 167140 (2.77126 iter/s, 3.60846s/10 iters), loss = 7.39597
I0523 06:50:47.602000 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39597 (* 1 = 7.39597 loss)
I0523 06:50:47.609298 35003 sgd_solver.cpp:112] Iteration 167140, lr = 0.001
I0523 06:50:48.901279 35003 solver.cpp:239] Iteration 167150 (7.697 iter/s, 1.29921s/10 iters), loss = 6.01053
I0523 06:50:48.901324 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01053 (* 1 = 6.01053 loss)
I0523 06:50:48.914911 35003 sgd_solver.cpp:112] Iteration 167150, lr = 0.001
I0523 06:50:53.102752 35003 solver.cpp:239] Iteration 167160 (2.38024 iter/s, 4.20125s/10 iters), loss = 6.05654
I0523 06:50:53.102818 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05654 (* 1 = 6.05654 loss)
I0523 06:50:53.129056 35003 sgd_solver.cpp:112] Iteration 167160, lr = 0.001
I0523 06:50:56.020823 35003 solver.cpp:239] Iteration 167170 (3.42714 iter/s, 2.91788s/10 iters), loss = 5.59801
I0523 06:50:56.020860 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.59801 (* 1 = 5.59801 loss)
I0523 06:50:56.029193 35003 sgd_solver.cpp:112] Iteration 167170, lr = 0.001
I0523 06:50:59.279842 35003 solver.cpp:239] Iteration 167180 (3.06858 iter/s, 3.25884s/10 iters), loss = 6.46328
I0523 06:50:59.279894 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46328 (* 1 = 6.46328 loss)
I0523 06:50:59.305961 35003 sgd_solver.cpp:112] Iteration 167180, lr = 0.001
I0523 06:51:02.869563 35003 solver.cpp:239] Iteration 167190 (2.78592 iter/s, 3.58948s/10 iters), loss = 7.2972
I0523 06:51:02.869627 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2972 (* 1 = 7.2972 loss)
I0523 06:51:02.874810 35003 sgd_solver.cpp:112] Iteration 167190, lr = 0.001
I0523 06:51:07.394781 35003 solver.cpp:239] Iteration 167200 (2.20996 iter/s, 4.52498s/10 iters), loss = 6.78295
I0523 06:51:07.394824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78295 (* 1 = 6.78295 loss)
I0523 06:51:08.071923 35003 sgd_solver.cpp:112] Iteration 167200, lr = 0.001
I0523 06:51:11.161689 35003 solver.cpp:239] Iteration 167210 (2.65484 iter/s, 3.7667s/10 iters), loss = 5.75043
I0523 06:51:11.161739 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.75043 (* 1 = 5.75043 loss)
I0523 06:51:11.175293 35003 sgd_solver.cpp:112] Iteration 167210, lr = 0.001
I0523 06:51:14.822846 35003 solver.cpp:239] Iteration 167220 (2.73153 iter/s, 3.66095s/10 iters), loss = 6.69883
I0523 06:51:14.823088 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69883 (* 1 = 6.69883 loss)
I0523 06:51:14.835700 35003 sgd_solver.cpp:112] Iteration 167220, lr = 0.001
I0523 06:51:17.693841 35003 solver.cpp:239] Iteration 167230 (3.48352 iter/s, 2.87066s/10 iters), loss = 5.61626
I0523 06:51:17.693876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.61626 (* 1 = 5.61626 loss)
I0523 06:51:17.712098 35003 sgd_solver.cpp:112] Iteration 167230, lr = 0.001
I0523 06:51:19.927178 35003 solver.cpp:239] Iteration 167240 (4.47788 iter/s, 2.2332s/10 iters), loss = 7.44695
I0523 06:51:19.927225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44695 (* 1 = 7.44695 loss)
I0523 06:51:20.502231 35003 sgd_solver.cpp:112] Iteration 167240, lr = 0.001
I0523 06:51:25.711772 35003 solver.cpp:239] Iteration 167250 (1.72882 iter/s, 5.7843s/10 iters), loss = 6.18576
I0523 06:51:25.711832 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18576 (* 1 = 6.18576 loss)
I0523 06:51:26.426812 35003 sgd_solver.cpp:112] Iteration 167250, lr = 0.001
I0523 06:51:29.928782 35003 solver.cpp:239] Iteration 167260 (2.37148 iter/s, 4.21678s/10 iters), loss = 6.94669
I0523 06:51:29.928822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94669 (* 1 = 6.94669 loss)
I0523 06:51:30.217183 35003 sgd_solver.cpp:112] Iteration 167260, lr = 0.001
I0523 06:51:33.238171 35003 solver.cpp:239] Iteration 167270 (3.02188 iter/s, 3.3092s/10 iters), loss = 6.69145
I0523 06:51:33.238234 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69145 (* 1 = 6.69145 loss)
I0523 06:51:33.501340 35003 sgd_solver.cpp:112] Iteration 167270, lr = 0.001
I0523 06:51:37.168184 35003 solver.cpp:239] Iteration 167280 (2.54467 iter/s, 3.92979s/10 iters), loss = 6.20758
I0523 06:51:37.168241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20758 (* 1 = 6.20758 loss)
I0523 06:51:37.181069 35003 sgd_solver.cpp:112] Iteration 167280, lr = 0.001
I0523 06:51:40.206787 35003 solver.cpp:239] Iteration 167290 (3.29119 iter/s, 3.03842s/10 iters), loss = 6.90833
I0523 06:51:40.206830 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90833 (* 1 = 6.90833 loss)
I0523 06:51:40.216419 35003 sgd_solver.cpp:112] Iteration 167290, lr = 0.001
I0523 06:51:43.114284 35003 solver.cpp:239] Iteration 167300 (3.43959 iter/s, 2.90733s/10 iters), loss = 5.30008
I0523 06:51:43.114336 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.30008 (* 1 = 5.30008 loss)
I0523 06:51:43.840834 35003 sgd_solver.cpp:112] Iteration 167300, lr = 0.001
I0523 06:51:48.156237 35003 solver.cpp:239] Iteration 167310 (1.98346 iter/s, 5.04169s/10 iters), loss = 4.85683
I0523 06:51:48.156482 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.85683 (* 1 = 4.85683 loss)
I0523 06:51:48.176743 35003 sgd_solver.cpp:112] Iteration 167310, lr = 0.001
I0523 06:51:51.109850 35003 solver.cpp:239] Iteration 167320 (3.3861 iter/s, 2.95325s/10 iters), loss = 6.14578
I0523 06:51:51.109887 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14578 (* 1 = 6.14578 loss)
I0523 06:51:51.122905 35003 sgd_solver.cpp:112] Iteration 167320, lr = 0.001
I0523 06:51:55.011030 35003 solver.cpp:239] Iteration 167330 (2.56346 iter/s, 3.90098s/10 iters), loss = 6.42173
I0523 06:51:55.011071 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42173 (* 1 = 6.42173 loss)
I0523 06:51:55.024303 35003 sgd_solver.cpp:112] Iteration 167330, lr = 0.001
I0523 06:51:58.745431 35003 solver.cpp:239] Iteration 167340 (2.67795 iter/s, 3.7342s/10 iters), loss = 6.6093
I0523 06:51:58.745481 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6093 (* 1 = 6.6093 loss)
I0523 06:51:59.480628 35003 sgd_solver.cpp:112] Iteration 167340, lr = 0.001
I0523 06:52:02.975236 35003 solver.cpp:239] Iteration 167350 (2.3643 iter/s, 4.22958s/10 iters), loss = 7.20909
I0523 06:52:02.975282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20909 (* 1 = 7.20909 loss)
I0523 06:52:02.996282 35003 sgd_solver.cpp:112] Iteration 167350, lr = 0.001
I0523 06:52:07.308039 35003 solver.cpp:239] Iteration 167360 (2.30809 iter/s, 4.33258s/10 iters), loss = 6.65934
I0523 06:52:07.308094 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65934 (* 1 = 6.65934 loss)
I0523 06:52:07.316226 35003 sgd_solver.cpp:112] Iteration 167360, lr = 0.001
I0523 06:52:09.419891 35003 solver.cpp:239] Iteration 167370 (4.73552 iter/s, 2.1117s/10 iters), loss = 5.85541
I0523 06:52:09.419932 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85541 (* 1 = 5.85541 loss)
I0523 06:52:09.764991 35003 sgd_solver.cpp:112] Iteration 167370, lr = 0.001
I0523 06:52:13.302304 35003 solver.cpp:239] Iteration 167380 (2.57619 iter/s, 3.88171s/10 iters), loss = 7.03389
I0523 06:52:13.302347 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03389 (* 1 = 7.03389 loss)
I0523 06:52:13.369074 35003 sgd_solver.cpp:112] Iteration 167380, lr = 0.001
I0523 06:52:16.157940 35003 solver.cpp:239] Iteration 167390 (3.50207 iter/s, 2.85545s/10 iters), loss = 5.70576
I0523 06:52:16.157990 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.70576 (* 1 = 5.70576 loss)
I0523 06:52:16.166102 35003 sgd_solver.cpp:112] Iteration 167390, lr = 0.001
I0523 06:52:19.076274 35003 solver.cpp:239] Iteration 167400 (3.42682 iter/s, 2.91816s/10 iters), loss = 6.59872
I0523 06:52:19.076551 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59872 (* 1 = 6.59872 loss)
I0523 06:52:19.810828 35003 sgd_solver.cpp:112] Iteration 167400, lr = 0.001
I0523 06:52:22.734565 35003 solver.cpp:239] Iteration 167410 (2.73382 iter/s, 3.65789s/10 iters), loss = 5.90439
I0523 06:52:22.734658 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.90439 (* 1 = 5.90439 loss)
I0523 06:52:22.747165 35003 sgd_solver.cpp:112] Iteration 167410, lr = 0.001
I0523 06:52:26.585404 35003 solver.cpp:239] Iteration 167420 (2.59701 iter/s, 3.85059s/10 iters), loss = 7.18026
I0523 06:52:26.585464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18026 (* 1 = 7.18026 loss)
I0523 06:52:27.326726 35003 sgd_solver.cpp:112] Iteration 167420, lr = 0.001
I0523 06:52:30.840607 35003 solver.cpp:239] Iteration 167430 (2.35019 iter/s, 4.25497s/10 iters), loss = 7.03072
I0523 06:52:30.840662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03072 (* 1 = 7.03072 loss)
I0523 06:52:30.852377 35003 sgd_solver.cpp:112] Iteration 167430, lr = 0.001
I0523 06:52:33.807596 35003 solver.cpp:239] Iteration 167440 (3.37062 iter/s, 2.96681s/10 iters), loss = 6.14643
I0523 06:52:33.807632 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14643 (* 1 = 6.14643 loss)
I0523 06:52:33.820855 35003 sgd_solver.cpp:112] Iteration 167440, lr = 0.001
I0523 06:52:36.604655 35003 solver.cpp:239] Iteration 167450 (3.57539 iter/s, 2.7969s/10 iters), loss = 6.41488
I0523 06:52:36.604712 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41488 (* 1 = 6.41488 loss)
I0523 06:52:37.237854 35003 sgd_solver.cpp:112] Iteration 167450, lr = 0.001
I0523 06:52:40.505225 35003 solver.cpp:239] Iteration 167460 (2.56387 iter/s, 3.90035s/10 iters), loss = 7.55724
I0523 06:52:40.505278 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55724 (* 1 = 7.55724 loss)
I0523 06:52:40.518981 35003 sgd_solver.cpp:112] Iteration 167460, lr = 0.001
I0523 06:52:42.583776 35003 solver.cpp:239] Iteration 167470 (4.81139 iter/s, 2.0784s/10 iters), loss = 5.97791
I0523 06:52:42.583822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97791 (* 1 = 5.97791 loss)
I0523 06:52:42.596387 35003 sgd_solver.cpp:112] Iteration 167470, lr = 0.001
I0523 06:52:43.902308 35003 solver.cpp:239] Iteration 167480 (7.58485 iter/s, 1.31842s/10 iters), loss = 6.80754
I0523 06:52:43.902364 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80754 (* 1 = 6.80754 loss)
I0523 06:52:43.915637 35003 sgd_solver.cpp:112] Iteration 167480, lr = 0.001
I0523 06:52:47.240320 35003 solver.cpp:239] Iteration 167490 (2.99597 iter/s, 3.33782s/10 iters), loss = 6.76489
I0523 06:52:47.240368 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76489 (* 1 = 6.76489 loss)
I0523 06:52:47.246824 35003 sgd_solver.cpp:112] Iteration 167490, lr = 0.001
I0523 06:52:51.512550 35003 solver.cpp:239] Iteration 167500 (2.34082 iter/s, 4.272s/10 iters), loss = 6.54542
I0523 06:52:51.512665 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54542 (* 1 = 6.54542 loss)
I0523 06:52:51.516752 35003 sgd_solver.cpp:112] Iteration 167500, lr = 0.001
I0523 06:52:53.533597 35003 solver.cpp:239] Iteration 167510 (4.94842 iter/s, 2.02085s/10 iters), loss = 6.96775
I0523 06:52:53.533630 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96775 (* 1 = 6.96775 loss)
I0523 06:52:53.543129 35003 sgd_solver.cpp:112] Iteration 167510, lr = 0.001
I0523 06:52:57.051853 35003 solver.cpp:239] Iteration 167520 (2.84246 iter/s, 3.51808s/10 iters), loss = 6.9456
I0523 06:52:57.051903 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9456 (* 1 = 6.9456 loss)
I0523 06:52:57.064664 35003 sgd_solver.cpp:112] Iteration 167520, lr = 0.001
I0523 06:53:01.059433 35003 solver.cpp:239] Iteration 167530 (2.49541 iter/s, 4.00735s/10 iters), loss = 6.51937
I0523 06:53:01.059494 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51937 (* 1 = 6.51937 loss)
I0523 06:53:01.793781 35003 sgd_solver.cpp:112] Iteration 167530, lr = 0.001
I0523 06:53:05.814381 35003 solver.cpp:239] Iteration 167540 (2.10319 iter/s, 4.75469s/10 iters), loss = 6.02923
I0523 06:53:05.814425 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02923 (* 1 = 6.02923 loss)
I0523 06:53:05.822167 35003 sgd_solver.cpp:112] Iteration 167540, lr = 0.001
I0523 06:53:09.075868 35003 solver.cpp:239] Iteration 167550 (3.06626 iter/s, 3.2613s/10 iters), loss = 5.65557
I0523 06:53:09.075922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.65557 (* 1 = 5.65557 loss)
I0523 06:53:09.080345 35003 sgd_solver.cpp:112] Iteration 167550, lr = 0.001
I0523 06:53:13.304709 35003 solver.cpp:239] Iteration 167560 (2.36485 iter/s, 4.2286s/10 iters), loss = 5.80599
I0523 06:53:13.304764 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.80599 (* 1 = 5.80599 loss)
I0523 06:53:13.312546 35003 sgd_solver.cpp:112] Iteration 167560, lr = 0.001
I0523 06:53:18.421907 35003 solver.cpp:239] Iteration 167570 (1.9543 iter/s, 5.11693s/10 iters), loss = 6.15898
I0523 06:53:18.421983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15898 (* 1 = 6.15898 loss)
I0523 06:53:18.456995 35003 sgd_solver.cpp:112] Iteration 167570, lr = 0.001
I0523 06:53:22.784198 35003 solver.cpp:239] Iteration 167580 (2.29251 iter/s, 4.36204s/10 iters), loss = 6.09372
I0523 06:53:22.784526 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09372 (* 1 = 6.09372 loss)
I0523 06:53:23.479801 35003 sgd_solver.cpp:112] Iteration 167580, lr = 0.001
I0523 06:53:25.578902 35003 solver.cpp:239] Iteration 167590 (3.57873 iter/s, 2.79429s/10 iters), loss = 6.66021
I0523 06:53:25.578954 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66021 (* 1 = 6.66021 loss)
I0523 06:53:26.258107 35003 sgd_solver.cpp:112] Iteration 167590, lr = 0.001
I0523 06:53:29.046262 35003 solver.cpp:239] Iteration 167600 (2.88421 iter/s, 3.46715s/10 iters), loss = 7.73073
I0523 06:53:29.046314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73073 (* 1 = 7.73073 loss)
I0523 06:53:29.182705 35003 sgd_solver.cpp:112] Iteration 167600, lr = 0.001
I0523 06:53:31.241446 35003 solver.cpp:239] Iteration 167610 (4.55574 iter/s, 2.19503s/10 iters), loss = 6.10912
I0523 06:53:31.241484 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10912 (* 1 = 6.10912 loss)
I0523 06:53:31.259506 35003 sgd_solver.cpp:112] Iteration 167610, lr = 0.001
I0523 06:53:35.894510 35003 solver.cpp:239] Iteration 167620 (2.14923 iter/s, 4.65284s/10 iters), loss = 6.93298
I0523 06:53:35.894556 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93298 (* 1 = 6.93298 loss)
I0523 06:53:36.609791 35003 sgd_solver.cpp:112] Iteration 167620, lr = 0.001
I0523 06:53:40.161945 35003 solver.cpp:239] Iteration 167630 (2.34345 iter/s, 4.26721s/10 iters), loss = 7.30778
I0523 06:53:40.161995 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30778 (* 1 = 7.30778 loss)
I0523 06:53:40.174429 35003 sgd_solver.cpp:112] Iteration 167630, lr = 0.001
I0523 06:53:43.691258 35003 solver.cpp:239] Iteration 167640 (2.83357 iter/s, 3.52912s/10 iters), loss = 5.61338
I0523 06:53:43.691310 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.61338 (* 1 = 5.61338 loss)
I0523 06:53:44.296865 35003 sgd_solver.cpp:112] Iteration 167640, lr = 0.001
I0523 06:53:47.138983 35003 solver.cpp:239] Iteration 167650 (2.90064 iter/s, 3.44751s/10 iters), loss = 5.66499
I0523 06:53:47.139031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.66499 (* 1 = 5.66499 loss)
I0523 06:53:47.159135 35003 sgd_solver.cpp:112] Iteration 167650, lr = 0.001
I0523 06:53:52.396805 35003 solver.cpp:239] Iteration 167660 (1.90202 iter/s, 5.25756s/10 iters), loss = 6.26861
I0523 06:53:52.396864 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26861 (* 1 = 6.26861 loss)
I0523 06:53:52.422528 35003 sgd_solver.cpp:112] Iteration 167660, lr = 0.001
I0523 06:53:56.005105 35003 solver.cpp:239] Iteration 167670 (2.77155 iter/s, 3.6081s/10 iters), loss = 6.27741
I0523 06:53:56.005398 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27741 (* 1 = 6.27741 loss)
I0523 06:53:56.067590 35003 sgd_solver.cpp:112] Iteration 167670, lr = 0.001
I0523 06:53:58.155656 35003 solver.cpp:239] Iteration 167680 (4.65074 iter/s, 2.1502s/10 iters), loss = 6.90682
I0523 06:53:58.155702 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90682 (* 1 = 6.90682 loss)
I0523 06:53:58.895326 35003 sgd_solver.cpp:112] Iteration 167680, lr = 0.001
I0523 06:54:01.780936 35003 solver.cpp:239] Iteration 167690 (2.75856 iter/s, 3.62508s/10 iters), loss = 6.04909
I0523 06:54:01.780987 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04909 (* 1 = 6.04909 loss)
I0523 06:54:02.516091 35003 sgd_solver.cpp:112] Iteration 167690, lr = 0.001
I0523 06:54:06.774340 35003 solver.cpp:239] Iteration 167700 (2.00274 iter/s, 4.99315s/10 iters), loss = 7.06396
I0523 06:54:06.774384 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06396 (* 1 = 7.06396 loss)
I0523 06:54:06.801127 35003 sgd_solver.cpp:112] Iteration 167700, lr = 0.001
I0523 06:54:10.399983 35003 solver.cpp:239] Iteration 167710 (2.75828 iter/s, 3.62544s/10 iters), loss = 7.42678
I0523 06:54:10.400023 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42678 (* 1 = 7.42678 loss)
I0523 06:54:10.413702 35003 sgd_solver.cpp:112] Iteration 167710, lr = 0.001
I0523 06:54:13.599032 35003 solver.cpp:239] Iteration 167720 (3.12611 iter/s, 3.19887s/10 iters), loss = 7.39367
I0523 06:54:13.599074 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39367 (* 1 = 7.39367 loss)
I0523 06:54:13.610285 35003 sgd_solver.cpp:112] Iteration 167720, lr = 0.001
I0523 06:54:16.202291 35003 solver.cpp:239] Iteration 167730 (3.84157 iter/s, 2.6031s/10 iters), loss = 7.34526
I0523 06:54:16.202343 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34526 (* 1 = 7.34526 loss)
I0523 06:54:16.508183 35003 sgd_solver.cpp:112] Iteration 167730, lr = 0.001
I0523 06:54:18.589861 35003 solver.cpp:239] Iteration 167740 (4.18863 iter/s, 2.38741s/10 iters), loss = 5.56902
I0523 06:54:18.589913 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.56902 (* 1 = 5.56902 loss)
I0523 06:54:19.330204 35003 sgd_solver.cpp:112] Iteration 167740, lr = 0.001
I0523 06:54:22.244128 35003 solver.cpp:239] Iteration 167750 (2.73668 iter/s, 3.65406s/10 iters), loss = 7.27978
I0523 06:54:22.244175 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27978 (* 1 = 7.27978 loss)
I0523 06:54:22.978534 35003 sgd_solver.cpp:112] Iteration 167750, lr = 0.001
I0523 06:54:27.328166 35003 solver.cpp:239] Iteration 167760 (1.96704 iter/s, 5.08378s/10 iters), loss = 6.84781
I0523 06:54:27.328285 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84781 (* 1 = 6.84781 loss)
I0523 06:54:27.338480 35003 sgd_solver.cpp:112] Iteration 167760, lr = 0.001
I0523 06:54:30.756258 35003 solver.cpp:239] Iteration 167770 (2.9173 iter/s, 3.42783s/10 iters), loss = 6.9376
I0523 06:54:30.756299 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9376 (* 1 = 6.9376 loss)
I0523 06:54:30.769975 35003 sgd_solver.cpp:112] Iteration 167770, lr = 0.001
I0523 06:54:33.652902 35003 solver.cpp:239] Iteration 167780 (3.45247 iter/s, 2.89648s/10 iters), loss = 7.17149
I0523 06:54:33.652951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17149 (* 1 = 7.17149 loss)
I0523 06:54:33.665886 35003 sgd_solver.cpp:112] Iteration 167780, lr = 0.001
I0523 06:54:35.743306 35003 solver.cpp:239] Iteration 167790 (4.7841 iter/s, 2.09026s/10 iters), loss = 5.46121
I0523 06:54:35.743355 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.46121 (* 1 = 5.46121 loss)
I0523 06:54:36.452561 35003 sgd_solver.cpp:112] Iteration 167790, lr = 0.001
I0523 06:54:39.978597 35003 solver.cpp:239] Iteration 167800 (2.36124 iter/s, 4.23507s/10 iters), loss = 8.16815
I0523 06:54:39.978652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16815 (* 1 = 8.16815 loss)
I0523 06:54:40.003590 35003 sgd_solver.cpp:112] Iteration 167800, lr = 0.001
I0523 06:54:43.241791 35003 solver.cpp:239] Iteration 167810 (3.06466 iter/s, 3.26301s/10 iters), loss = 5.81075
I0523 06:54:43.241832 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.81075 (* 1 = 5.81075 loss)
I0523 06:54:43.254946 35003 sgd_solver.cpp:112] Iteration 167810, lr = 0.001
I0523 06:54:46.911345 35003 solver.cpp:239] Iteration 167820 (2.72527 iter/s, 3.66936s/10 iters), loss = 7.57714
I0523 06:54:46.911389 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57714 (* 1 = 7.57714 loss)
I0523 06:54:46.918479 35003 sgd_solver.cpp:112] Iteration 167820, lr = 0.001
I0523 06:54:49.696208 35003 solver.cpp:239] Iteration 167830 (3.59105 iter/s, 2.7847s/10 iters), loss = 6.08462
I0523 06:54:49.696251 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08462 (* 1 = 6.08462 loss)
I0523 06:54:50.361655 35003 sgd_solver.cpp:112] Iteration 167830, lr = 0.001
I0523 06:54:53.289371 35003 solver.cpp:239] Iteration 167840 (2.78321 iter/s, 3.59297s/10 iters), loss = 6.49695
I0523 06:54:53.289412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49695 (* 1 = 6.49695 loss)
I0523 06:54:53.302672 35003 sgd_solver.cpp:112] Iteration 167840, lr = 0.001
I0523 06:54:56.116499 35003 solver.cpp:239] Iteration 167850 (3.53736 iter/s, 2.82696s/10 iters), loss = 8.01539
I0523 06:54:56.116556 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.01539 (* 1 = 8.01539 loss)
I0523 06:54:56.831948 35003 sgd_solver.cpp:112] Iteration 167850, lr = 0.001
I0523 06:54:58.101686 35003 solver.cpp:239] Iteration 167860 (5.03771 iter/s, 1.98503s/10 iters), loss = 7.14914
I0523 06:54:58.101918 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14914 (* 1 = 7.14914 loss)
I0523 06:54:58.115079 35003 sgd_solver.cpp:112] Iteration 167860, lr = 0.001
I0523 06:55:00.874089 35003 solver.cpp:239] Iteration 167870 (3.60741 iter/s, 2.77207s/10 iters), loss = 6.29203
I0523 06:55:00.874133 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29203 (* 1 = 6.29203 loss)
I0523 06:55:00.919087 35003 sgd_solver.cpp:112] Iteration 167870, lr = 0.001
I0523 06:55:04.053359 35003 solver.cpp:239] Iteration 167880 (3.14555 iter/s, 3.17909s/10 iters), loss = 7.07096
I0523 06:55:04.053396 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07096 (* 1 = 7.07096 loss)
I0523 06:55:04.071363 35003 sgd_solver.cpp:112] Iteration 167880, lr = 0.001
I0523 06:55:05.729089 35003 solver.cpp:239] Iteration 167890 (5.96795 iter/s, 1.67562s/10 iters), loss = 6.68248
I0523 06:55:05.729128 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68248 (* 1 = 6.68248 loss)
I0523 06:55:05.743064 35003 sgd_solver.cpp:112] Iteration 167890, lr = 0.001
I0523 06:55:10.457774 35003 solver.cpp:239] Iteration 167900 (2.11486 iter/s, 4.72845s/10 iters), loss = 5.81711
I0523 06:55:10.457810 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.81711 (* 1 = 5.81711 loss)
I0523 06:55:10.465698 35003 sgd_solver.cpp:112] Iteration 167900, lr = 0.001
I0523 06:55:13.294859 35003 solver.cpp:239] Iteration 167910 (3.52495 iter/s, 2.83692s/10 iters), loss = 7.97779
I0523 06:55:13.294920 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97779 (* 1 = 7.97779 loss)
I0523 06:55:13.380184 35003 sgd_solver.cpp:112] Iteration 167910, lr = 0.001
I0523 06:55:18.223922 35003 solver.cpp:239] Iteration 167920 (2.02889 iter/s, 4.9288s/10 iters), loss = 6.21493
I0523 06:55:18.223961 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21493 (* 1 = 6.21493 loss)
I0523 06:55:18.237243 35003 sgd_solver.cpp:112] Iteration 167920, lr = 0.001
I0523 06:55:20.355301 35003 solver.cpp:239] Iteration 167930 (4.69209 iter/s, 2.13125s/10 iters), loss = 6.58193
I0523 06:55:20.355340 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58193 (* 1 = 6.58193 loss)
I0523 06:55:20.368613 35003 sgd_solver.cpp:112] Iteration 167930, lr = 0.001
I0523 06:55:23.854550 35003 solver.cpp:239] Iteration 167940 (2.85791 iter/s, 3.49906s/10 iters), loss = 7.01542
I0523 06:55:23.854591 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01542 (* 1 = 7.01542 loss)
I0523 06:55:23.860591 35003 sgd_solver.cpp:112] Iteration 167940, lr = 0.001
I0523 06:55:26.726119 35003 solver.cpp:239] Iteration 167950 (3.48262 iter/s, 2.8714s/10 iters), loss = 6.14158
I0523 06:55:26.726168 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14158 (* 1 = 6.14158 loss)
I0523 06:55:26.748819 35003 sgd_solver.cpp:112] Iteration 167950, lr = 0.001
I0523 06:55:28.945044 35003 solver.cpp:239] Iteration 167960 (4.50697 iter/s, 2.21878s/10 iters), loss = 6.20443
I0523 06:55:28.945186 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20443 (* 1 = 6.20443 loss)
I0523 06:55:28.961345 35003 sgd_solver.cpp:112] Iteration 167960, lr = 0.001
I0523 06:55:32.492552 35003 solver.cpp:239] Iteration 167970 (2.81911 iter/s, 3.54721s/10 iters), loss = 7.15152
I0523 06:55:32.492594 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15152 (* 1 = 7.15152 loss)
I0523 06:55:32.514994 35003 sgd_solver.cpp:112] Iteration 167970, lr = 0.001
I0523 06:55:35.446748 35003 solver.cpp:239] Iteration 167980 (3.38525 iter/s, 2.95399s/10 iters), loss = 7.19083
I0523 06:55:35.446817 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19083 (* 1 = 7.19083 loss)
I0523 06:55:35.469341 35003 sgd_solver.cpp:112] Iteration 167980, lr = 0.001
I0523 06:55:38.329978 35003 solver.cpp:239] Iteration 167990 (3.46857 iter/s, 2.88303s/10 iters), loss = 6.81719
I0523 06:55:38.330034 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81719 (* 1 = 6.81719 loss)
I0523 06:55:38.349620 35003 sgd_solver.cpp:112] Iteration 167990, lr = 0.001
I0523 06:55:41.593659 35003 solver.cpp:239] Iteration 168000 (3.0642 iter/s, 3.26349s/10 iters), loss = 5.59817
I0523 06:55:41.593701 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.59817 (* 1 = 5.59817 loss)
I0523 06:55:41.616520 35003 sgd_solver.cpp:112] Iteration 168000, lr = 0.001
I0523 06:55:43.538028 35003 solver.cpp:239] Iteration 168010 (5.15336 iter/s, 1.94048s/10 iters), loss = 6.27648
I0523 06:55:43.538069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27648 (* 1 = 6.27648 loss)
I0523 06:55:44.279747 35003 sgd_solver.cpp:112] Iteration 168010, lr = 0.001
I0523 06:55:47.770880 35003 solver.cpp:239] Iteration 168020 (2.36259 iter/s, 4.23264s/10 iters), loss = 6.93155
I0523 06:55:47.770926 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93155 (* 1 = 6.93155 loss)
I0523 06:55:47.776680 35003 sgd_solver.cpp:112] Iteration 168020, lr = 0.001
I0523 06:55:50.441802 35003 solver.cpp:239] Iteration 168030 (3.74429 iter/s, 2.67073s/10 iters), loss = 5.927
I0523 06:55:50.441862 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.927 (* 1 = 5.927 loss)
I0523 06:55:50.448379 35003 sgd_solver.cpp:112] Iteration 168030, lr = 0.001
I0523 06:55:52.545614 35003 solver.cpp:239] Iteration 168040 (4.75364 iter/s, 2.10365s/10 iters), loss = 6.7055
I0523 06:55:52.545660 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7055 (* 1 = 6.7055 loss)
I0523 06:55:52.550945 35003 sgd_solver.cpp:112] Iteration 168040, lr = 0.001
I0523 06:55:56.172279 35003 solver.cpp:239] Iteration 168050 (2.7575 iter/s, 3.62647s/10 iters), loss = 7.31306
I0523 06:55:56.172320 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31306 (* 1 = 7.31306 loss)
I0523 06:55:56.176079 35003 sgd_solver.cpp:112] Iteration 168050, lr = 0.001
I0523 06:55:59.885061 35003 solver.cpp:239] Iteration 168060 (2.69354 iter/s, 3.71258s/10 iters), loss = 5.73807
I0523 06:55:59.885275 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.73807 (* 1 = 5.73807 loss)
I0523 06:56:00.047044 35003 sgd_solver.cpp:112] Iteration 168060, lr = 0.001
I0523 06:56:03.025172 35003 solver.cpp:239] Iteration 168070 (3.18495 iter/s, 3.13977s/10 iters), loss = 6.6488
I0523 06:56:03.025216 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6488 (* 1 = 6.6488 loss)
I0523 06:56:03.719179 35003 sgd_solver.cpp:112] Iteration 168070, lr = 0.001
I0523 06:56:06.520859 35003 solver.cpp:239] Iteration 168080 (2.86082 iter/s, 3.4955s/10 iters), loss = 6.82585
I0523 06:56:06.520905 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82585 (* 1 = 6.82585 loss)
I0523 06:56:06.707767 35003 sgd_solver.cpp:112] Iteration 168080, lr = 0.001
I0523 06:56:11.048878 35003 solver.cpp:239] Iteration 168090 (2.20859 iter/s, 4.52778s/10 iters), loss = 5.97823
I0523 06:56:11.048919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97823 (* 1 = 5.97823 loss)
I0523 06:56:11.066696 35003 sgd_solver.cpp:112] Iteration 168090, lr = 0.001
I0523 06:56:16.123353 35003 solver.cpp:239] Iteration 168100 (1.97075 iter/s, 5.07422s/10 iters), loss = 6.30228
I0523 06:56:16.123414 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30228 (* 1 = 6.30228 loss)
I0523 06:56:16.844095 35003 sgd_solver.cpp:112] Iteration 168100, lr = 0.001
I0523 06:56:19.838152 35003 solver.cpp:239] Iteration 168110 (2.69209 iter/s, 3.71458s/10 iters), loss = 7.24823
I0523 06:56:19.838218 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24823 (* 1 = 7.24823 loss)
I0523 06:56:19.843194 35003 sgd_solver.cpp:112] Iteration 168110, lr = 0.001
I0523 06:56:23.472416 35003 solver.cpp:239] Iteration 168120 (2.75175 iter/s, 3.63405s/10 iters), loss = 6.22535
I0523 06:56:23.472476 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22535 (* 1 = 6.22535 loss)
I0523 06:56:24.193883 35003 sgd_solver.cpp:112] Iteration 168120, lr = 0.001
I0523 06:56:28.160653 35003 solver.cpp:239] Iteration 168130 (2.13312 iter/s, 4.68798s/10 iters), loss = 7.1024
I0523 06:56:28.160707 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1024 (* 1 = 7.1024 loss)
I0523 06:56:28.303722 35003 sgd_solver.cpp:112] Iteration 168130, lr = 0.001
I0523 06:56:30.181612 35003 solver.cpp:239] Iteration 168140 (4.9485 iter/s, 2.02081s/10 iters), loss = 7.10101
I0523 06:56:30.181798 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10101 (* 1 = 7.10101 loss)
I0523 06:56:30.194485 35003 sgd_solver.cpp:112] Iteration 168140, lr = 0.001
I0523 06:56:33.609391 35003 solver.cpp:239] Iteration 168150 (2.91762 iter/s, 3.42745s/10 iters), loss = 6.53024
I0523 06:56:33.609447 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53024 (* 1 = 6.53024 loss)
I0523 06:56:33.622272 35003 sgd_solver.cpp:112] Iteration 168150, lr = 0.001
I0523 06:56:36.445309 35003 solver.cpp:239] Iteration 168160 (3.52642 iter/s, 2.83574s/10 iters), loss = 7.15524
I0523 06:56:36.445356 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15524 (* 1 = 7.15524 loss)
I0523 06:56:36.455307 35003 sgd_solver.cpp:112] Iteration 168160, lr = 0.001
I0523 06:56:38.619261 35003 solver.cpp:239] Iteration 168170 (4.60024 iter/s, 2.1738s/10 iters), loss = 6.51988
I0523 06:56:38.619304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51988 (* 1 = 6.51988 loss)
I0523 06:56:38.633113 35003 sgd_solver.cpp:112] Iteration 168170, lr = 0.001
I0523 06:56:42.689709 35003 solver.cpp:239] Iteration 168180 (2.45686 iter/s, 4.07024s/10 iters), loss = 7.47367
I0523 06:56:42.689746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47367 (* 1 = 7.47367 loss)
I0523 06:56:42.697610 35003 sgd_solver.cpp:112] Iteration 168180, lr = 0.001
I0523 06:56:46.489172 35003 solver.cpp:239] Iteration 168190 (2.63211 iter/s, 3.79924s/10 iters), loss = 7.16889
I0523 06:56:46.489220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16889 (* 1 = 7.16889 loss)
I0523 06:56:46.491654 35003 sgd_solver.cpp:112] Iteration 168190, lr = 0.001
I0523 06:56:50.153409 35003 solver.cpp:239] Iteration 168200 (2.72924 iter/s, 3.66403s/10 iters), loss = 7.30511
I0523 06:56:50.153455 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30511 (* 1 = 7.30511 loss)
I0523 06:56:50.894976 35003 sgd_solver.cpp:112] Iteration 168200, lr = 0.001
I0523 06:56:53.589640 35003 solver.cpp:239] Iteration 168210 (2.91033 iter/s, 3.43604s/10 iters), loss = 6.73698
I0523 06:56:53.589684 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73698 (* 1 = 6.73698 loss)
I0523 06:56:53.599799 35003 sgd_solver.cpp:112] Iteration 168210, lr = 0.001
I0523 06:56:56.402755 35003 solver.cpp:239] Iteration 168220 (3.555 iter/s, 2.81294s/10 iters), loss = 6.94605
I0523 06:56:56.402808 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94605 (* 1 = 6.94605 loss)
I0523 06:56:56.408309 35003 sgd_solver.cpp:112] Iteration 168220, lr = 0.001
I0523 06:57:00.006153 35003 solver.cpp:239] Iteration 168230 (2.77532 iter/s, 3.60319s/10 iters), loss = 6.39924
I0523 06:57:00.006206 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39924 (* 1 = 6.39924 loss)
I0523 06:57:00.017359 35003 sgd_solver.cpp:112] Iteration 168230, lr = 0.001
I0523 06:57:03.569056 35003 solver.cpp:239] Iteration 168240 (2.80685 iter/s, 3.56271s/10 iters), loss = 6.75476
I0523 06:57:03.569346 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75476 (* 1 = 6.75476 loss)
I0523 06:57:03.576670 35003 sgd_solver.cpp:112] Iteration 168240, lr = 0.001
I0523 06:57:06.329187 35003 solver.cpp:239] Iteration 168250 (3.62352 iter/s, 2.75975s/10 iters), loss = 7.52674
I0523 06:57:06.329243 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52674 (* 1 = 7.52674 loss)
I0523 06:57:06.345744 35003 sgd_solver.cpp:112] Iteration 168250, lr = 0.001
I0523 06:57:10.551369 35003 solver.cpp:239] Iteration 168260 (2.36858 iter/s, 4.22195s/10 iters), loss = 7.34863
I0523 06:57:10.551420 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34863 (* 1 = 7.34863 loss)
I0523 06:57:10.558619 35003 sgd_solver.cpp:112] Iteration 168260, lr = 0.001
I0523 06:57:13.940305 35003 solver.cpp:239] Iteration 168270 (2.95096 iter/s, 3.38873s/10 iters), loss = 5.44582
I0523 06:57:13.940349 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.44582 (* 1 = 5.44582 loss)
I0523 06:57:13.944025 35003 sgd_solver.cpp:112] Iteration 168270, lr = 0.001
I0523 06:57:17.847666 35003 solver.cpp:239] Iteration 168280 (2.55941 iter/s, 3.90715s/10 iters), loss = 6.16416
I0523 06:57:17.847710 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16416 (* 1 = 6.16416 loss)
I0523 06:57:18.556675 35003 sgd_solver.cpp:112] Iteration 168280, lr = 0.001
I0523 06:57:23.104686 35003 solver.cpp:239] Iteration 168290 (1.90232 iter/s, 5.25673s/10 iters), loss = 7.11695
I0523 06:57:23.104740 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11695 (* 1 = 7.11695 loss)
I0523 06:57:23.799942 35003 sgd_solver.cpp:112] Iteration 168290, lr = 0.001
I0523 06:57:26.591593 35003 solver.cpp:239] Iteration 168300 (2.86803 iter/s, 3.48671s/10 iters), loss = 7.297
I0523 06:57:26.591641 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.297 (* 1 = 7.297 loss)
I0523 06:57:26.598014 35003 sgd_solver.cpp:112] Iteration 168300, lr = 0.001
I0523 06:57:29.521190 35003 solver.cpp:239] Iteration 168310 (3.41365 iter/s, 2.92942s/10 iters), loss = 6.31555
I0523 06:57:29.521237 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31555 (* 1 = 6.31555 loss)
I0523 06:57:30.234010 35003 sgd_solver.cpp:112] Iteration 168310, lr = 0.001
I0523 06:57:33.330744 35003 solver.cpp:239] Iteration 168320 (2.62512 iter/s, 3.80935s/10 iters), loss = 6.15206
I0523 06:57:33.330793 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15206 (* 1 = 6.15206 loss)
I0523 06:57:33.336457 35003 sgd_solver.cpp:112] Iteration 168320, lr = 0.001
I0523 06:57:38.028885 35003 solver.cpp:239] Iteration 168330 (2.12861 iter/s, 4.69789s/10 iters), loss = 7.47464
I0523 06:57:38.029140 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47464 (* 1 = 7.47464 loss)
I0523 06:57:38.042058 35003 sgd_solver.cpp:112] Iteration 168330, lr = 0.001
I0523 06:57:40.762586 35003 solver.cpp:239] Iteration 168340 (3.65853 iter/s, 2.73334s/10 iters), loss = 5.17224
I0523 06:57:40.762634 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.17224 (* 1 = 5.17224 loss)
I0523 06:57:40.770195 35003 sgd_solver.cpp:112] Iteration 168340, lr = 0.001
I0523 06:57:44.912889 35003 solver.cpp:239] Iteration 168350 (2.40959 iter/s, 4.15008s/10 iters), loss = 6.63514
I0523 06:57:44.912930 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63514 (* 1 = 6.63514 loss)
I0523 06:57:44.917469 35003 sgd_solver.cpp:112] Iteration 168350, lr = 0.001
I0523 06:57:48.491801 35003 solver.cpp:239] Iteration 168360 (2.7943 iter/s, 3.57871s/10 iters), loss = 6.60074
I0523 06:57:48.491845 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60074 (* 1 = 6.60074 loss)
I0523 06:57:48.495256 35003 sgd_solver.cpp:112] Iteration 168360, lr = 0.001
I0523 06:57:52.048054 35003 solver.cpp:239] Iteration 168370 (2.81211 iter/s, 3.55605s/10 iters), loss = 6.41383
I0523 06:57:52.048094 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41383 (* 1 = 6.41383 loss)
I0523 06:57:52.053494 35003 sgd_solver.cpp:112] Iteration 168370, lr = 0.001
I0523 06:57:54.889217 35003 solver.cpp:239] Iteration 168380 (3.51988 iter/s, 2.841s/10 iters), loss = 7.19851
I0523 06:57:54.889263 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19851 (* 1 = 7.19851 loss)
I0523 06:57:54.898762 35003 sgd_solver.cpp:112] Iteration 168380, lr = 0.001
I0523 06:57:57.407397 35003 solver.cpp:239] Iteration 168390 (3.97138 iter/s, 2.51802s/10 iters), loss = 6.34105
I0523 06:57:57.407444 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34105 (* 1 = 6.34105 loss)
I0523 06:57:58.125957 35003 sgd_solver.cpp:112] Iteration 168390, lr = 0.001
I0523 06:58:02.798125 35003 solver.cpp:239] Iteration 168400 (1.85513 iter/s, 5.39046s/10 iters), loss = 5.53178
I0523 06:58:02.798174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.53178 (* 1 = 5.53178 loss)
I0523 06:58:02.805831 35003 sgd_solver.cpp:112] Iteration 168400, lr = 0.001
I0523 06:58:06.168877 35003 solver.cpp:239] Iteration 168410 (2.96688 iter/s, 3.37055s/10 iters), loss = 6.06216
I0523 06:58:06.168915 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06216 (* 1 = 6.06216 loss)
I0523 06:58:06.175814 35003 sgd_solver.cpp:112] Iteration 168410, lr = 0.001
I0523 06:58:09.027856 35003 solver.cpp:239] Iteration 168420 (3.49795 iter/s, 2.85881s/10 iters), loss = 6.94282
I0523 06:58:09.027958 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94282 (* 1 = 6.94282 loss)
I0523 06:58:09.031705 35003 sgd_solver.cpp:112] Iteration 168420, lr = 0.001
I0523 06:58:12.557159 35003 solver.cpp:239] Iteration 168430 (2.83363 iter/s, 3.52904s/10 iters), loss = 6.3147
I0523 06:58:12.557199 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3147 (* 1 = 6.3147 loss)
I0523 06:58:12.570468 35003 sgd_solver.cpp:112] Iteration 168430, lr = 0.001
I0523 06:58:15.772413 35003 solver.cpp:239] Iteration 168440 (3.11034 iter/s, 3.21508s/10 iters), loss = 5.30902
I0523 06:58:15.772459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.30902 (* 1 = 5.30902 loss)
I0523 06:58:15.795035 35003 sgd_solver.cpp:112] Iteration 168440, lr = 0.001
I0523 06:58:19.296247 35003 solver.cpp:239] Iteration 168450 (2.83797 iter/s, 3.52364s/10 iters), loss = 6.67523
I0523 06:58:19.296288 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67523 (* 1 = 6.67523 loss)
I0523 06:58:19.992377 35003 sgd_solver.cpp:112] Iteration 168450, lr = 0.001
I0523 06:58:23.733314 35003 solver.cpp:239] Iteration 168460 (2.25385 iter/s, 4.43685s/10 iters), loss = 6.32541
I0523 06:58:23.733353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32541 (* 1 = 6.32541 loss)
I0523 06:58:23.738497 35003 sgd_solver.cpp:112] Iteration 168460, lr = 0.001
I0523 06:58:26.095402 35003 solver.cpp:239] Iteration 168470 (4.2338 iter/s, 2.36195s/10 iters), loss = 7.23874
I0523 06:58:26.095443 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23874 (* 1 = 7.23874 loss)
I0523 06:58:26.108834 35003 sgd_solver.cpp:112] Iteration 168470, lr = 0.001
I0523 06:58:29.522065 35003 solver.cpp:239] Iteration 168480 (2.91846 iter/s, 3.42647s/10 iters), loss = 5.76074
I0523 06:58:29.522114 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.76074 (* 1 = 5.76074 loss)
I0523 06:58:29.529187 35003 sgd_solver.cpp:112] Iteration 168480, lr = 0.001
I0523 06:58:34.243458 35003 solver.cpp:239] Iteration 168490 (2.11813 iter/s, 4.72114s/10 iters), loss = 5.39312
I0523 06:58:34.243515 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.39312 (* 1 = 5.39312 loss)
I0523 06:58:34.256623 35003 sgd_solver.cpp:112] Iteration 168490, lr = 0.001
I0523 06:58:37.854650 35003 solver.cpp:239] Iteration 168500 (2.76933 iter/s, 3.61098s/10 iters), loss = 6.12968
I0523 06:58:37.854718 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12968 (* 1 = 6.12968 loss)
I0523 06:58:37.860013 35003 sgd_solver.cpp:112] Iteration 168500, lr = 0.001
I0523 06:58:41.078274 35003 solver.cpp:239] Iteration 168510 (3.10227 iter/s, 3.22344s/10 iters), loss = 5.58839
I0523 06:58:41.078567 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.58839 (* 1 = 5.58839 loss)
I0523 06:58:41.092329 35003 sgd_solver.cpp:112] Iteration 168510, lr = 0.001
I0523 06:58:45.178078 35003 solver.cpp:239] Iteration 168520 (2.4394 iter/s, 4.09937s/10 iters), loss = 6.78254
I0523 06:58:45.178122 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78254 (* 1 = 6.78254 loss)
I0523 06:58:45.191382 35003 sgd_solver.cpp:112] Iteration 168520, lr = 0.001
I0523 06:58:48.783583 35003 solver.cpp:239] Iteration 168530 (2.77369 iter/s, 3.60531s/10 iters), loss = 6.25903
I0523 06:58:48.783625 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25903 (* 1 = 6.25903 loss)
I0523 06:58:48.787425 35003 sgd_solver.cpp:112] Iteration 168530, lr = 0.001
I0523 06:58:52.258713 35003 solver.cpp:239] Iteration 168540 (2.87776 iter/s, 3.47492s/10 iters), loss = 5.33464
I0523 06:58:52.258766 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.33464 (* 1 = 5.33464 loss)
I0523 06:58:52.416934 35003 sgd_solver.cpp:112] Iteration 168540, lr = 0.001
I0523 06:58:55.904546 35003 solver.cpp:239] Iteration 168550 (2.74302 iter/s, 3.64562s/10 iters), loss = 6.36953
I0523 06:58:55.904603 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36953 (* 1 = 6.36953 loss)
I0523 06:58:55.907384 35003 sgd_solver.cpp:112] Iteration 168550, lr = 0.001
I0523 06:58:58.675899 35003 solver.cpp:239] Iteration 168560 (3.60859 iter/s, 2.77117s/10 iters), loss = 6.55733
I0523 06:58:58.675976 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55733 (* 1 = 6.55733 loss)
I0523 06:58:59.350400 35003 sgd_solver.cpp:112] Iteration 168560, lr = 0.001
I0523 06:59:02.105479 35003 solver.cpp:239] Iteration 168570 (2.916 iter/s, 3.42936s/10 iters), loss = 6.60106
I0523 06:59:02.105522 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60106 (* 1 = 6.60106 loss)
I0523 06:59:02.118798 35003 sgd_solver.cpp:112] Iteration 168570, lr = 0.001
I0523 06:59:04.127918 35003 solver.cpp:239] Iteration 168580 (4.94485 iter/s, 2.0223s/10 iters), loss = 5.98775
I0523 06:59:04.127956 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98775 (* 1 = 5.98775 loss)
I0523 06:59:04.136106 35003 sgd_solver.cpp:112] Iteration 168580, lr = 0.001
I0523 06:59:07.306454 35003 solver.cpp:239] Iteration 168590 (3.14627 iter/s, 3.17836s/10 iters), loss = 5.40762
I0523 06:59:07.306504 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.40762 (* 1 = 5.40762 loss)
I0523 06:59:08.015460 35003 sgd_solver.cpp:112] Iteration 168590, lr = 0.001
I0523 06:59:12.370148 35003 solver.cpp:239] Iteration 168600 (1.97494 iter/s, 5.06344s/10 iters), loss = 7.01172
I0523 06:59:12.370450 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01172 (* 1 = 7.01172 loss)
I0523 06:59:12.435289 35003 sgd_solver.cpp:112] Iteration 168600, lr = 0.001
I0523 06:59:15.248011 35003 solver.cpp:239] Iteration 168610 (3.47529 iter/s, 2.87745s/10 iters), loss = 6.92162
I0523 06:59:15.248054 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92162 (* 1 = 6.92162 loss)
I0523 06:59:15.260995 35003 sgd_solver.cpp:112] Iteration 168610, lr = 0.001
I0523 06:59:19.436496 35003 solver.cpp:239] Iteration 168620 (2.38762 iter/s, 4.18827s/10 iters), loss = 5.89518
I0523 06:59:19.436542 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89518 (* 1 = 5.89518 loss)
I0523 06:59:19.447527 35003 sgd_solver.cpp:112] Iteration 168620, lr = 0.001
I0523 06:59:22.782760 35003 solver.cpp:239] Iteration 168630 (2.98859 iter/s, 3.34606s/10 iters), loss = 8.49834
I0523 06:59:22.782829 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.49834 (* 1 = 8.49834 loss)
I0523 06:59:22.787895 35003 sgd_solver.cpp:112] Iteration 168630, lr = 0.001
I0523 06:59:26.415443 35003 solver.cpp:239] Iteration 168640 (2.75295 iter/s, 3.63247s/10 iters), loss = 5.64873
I0523 06:59:26.415488 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.64873 (* 1 = 5.64873 loss)
I0523 06:59:27.150540 35003 sgd_solver.cpp:112] Iteration 168640, lr = 0.001
I0523 06:59:29.239948 35003 solver.cpp:239] Iteration 168650 (3.54065 iter/s, 2.82434s/10 iters), loss = 5.79749
I0523 06:59:29.239998 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.79749 (* 1 = 5.79749 loss)
I0523 06:59:29.710405 35003 sgd_solver.cpp:112] Iteration 168650, lr = 0.001
I0523 06:59:34.328774 35003 solver.cpp:239] Iteration 168660 (1.96519 iter/s, 5.08857s/10 iters), loss = 6.43828
I0523 06:59:34.328819 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43828 (* 1 = 6.43828 loss)
I0523 06:59:34.341804 35003 sgd_solver.cpp:112] Iteration 168660, lr = 0.001
I0523 06:59:36.137379 35003 solver.cpp:239] Iteration 168670 (5.5295 iter/s, 1.80848s/10 iters), loss = 5.45292
I0523 06:59:36.137418 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.45292 (* 1 = 5.45292 loss)
I0523 06:59:36.657218 35003 sgd_solver.cpp:112] Iteration 168670, lr = 0.001
I0523 06:59:38.868943 35003 solver.cpp:239] Iteration 168680 (3.66112 iter/s, 2.7314s/10 iters), loss = 7.19366
I0523 06:59:38.868993 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19366 (* 1 = 7.19366 loss)
I0523 06:59:38.878942 35003 sgd_solver.cpp:112] Iteration 168680, lr = 0.001
I0523 06:59:41.007736 35003 solver.cpp:239] Iteration 168690 (4.67589 iter/s, 2.13863s/10 iters), loss = 5.85322
I0523 06:59:41.007802 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85322 (* 1 = 5.85322 loss)
I0523 06:59:41.013381 35003 sgd_solver.cpp:112] Iteration 168690, lr = 0.001
I0523 06:59:43.856576 35003 solver.cpp:239] Iteration 168700 (3.51043 iter/s, 2.84866s/10 iters), loss = 5.56205
I0523 06:59:43.856776 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.56205 (* 1 = 5.56205 loss)
I0523 06:59:43.886265 35003 sgd_solver.cpp:112] Iteration 168700, lr = 0.001
I0523 06:59:46.532374 35003 solver.cpp:239] Iteration 168710 (3.73762 iter/s, 2.6755s/10 iters), loss = 6.76218
I0523 06:59:46.532415 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76218 (* 1 = 6.76218 loss)
I0523 06:59:46.550879 35003 sgd_solver.cpp:112] Iteration 168710, lr = 0.001
I0523 06:59:49.467761 35003 solver.cpp:239] Iteration 168720 (3.4069 iter/s, 2.93522s/10 iters), loss = 6.52051
I0523 06:59:49.467809 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52051 (* 1 = 6.52051 loss)
I0523 06:59:50.176661 35003 sgd_solver.cpp:112] Iteration 168720, lr = 0.001
I0523 06:59:54.197712 35003 solver.cpp:239] Iteration 168730 (2.1143 iter/s, 4.72971s/10 iters), loss = 7.50125
I0523 06:59:54.197772 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50125 (* 1 = 7.50125 loss)
I0523 06:59:54.210449 35003 sgd_solver.cpp:112] Iteration 168730, lr = 0.001
I0523 06:59:59.283820 35003 solver.cpp:239] Iteration 168740 (1.96624 iter/s, 5.08584s/10 iters), loss = 7.63584
I0523 06:59:59.283867 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63584 (* 1 = 7.63584 loss)
I0523 06:59:59.986407 35003 sgd_solver.cpp:112] Iteration 168740, lr = 0.001
I0523 07:00:03.630013 35003 solver.cpp:239] Iteration 168750 (2.30098 iter/s, 4.34597s/10 iters), loss = 5.89077
I0523 07:00:03.630053 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89077 (* 1 = 5.89077 loss)
I0523 07:00:04.289985 35003 sgd_solver.cpp:112] Iteration 168750, lr = 0.001
I0523 07:00:06.274782 35003 solver.cpp:239] Iteration 168760 (3.78127 iter/s, 2.64462s/10 iters), loss = 6.54003
I0523 07:00:06.274826 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54003 (* 1 = 6.54003 loss)
I0523 07:00:06.279956 35003 sgd_solver.cpp:112] Iteration 168760, lr = 0.001
I0523 07:00:09.831701 35003 solver.cpp:239] Iteration 168770 (2.81158 iter/s, 3.55672s/10 iters), loss = 5.438
I0523 07:00:09.831742 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.438 (* 1 = 5.438 loss)
I0523 07:00:09.835602 35003 sgd_solver.cpp:112] Iteration 168770, lr = 0.001
I0523 07:00:13.375788 35003 solver.cpp:239] Iteration 168780 (2.82175 iter/s, 3.5439s/10 iters), loss = 6.94288
I0523 07:00:13.375834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94288 (* 1 = 6.94288 loss)
I0523 07:00:14.091401 35003 sgd_solver.cpp:112] Iteration 168780, lr = 0.001
I0523 07:00:18.092792 35003 solver.cpp:239] Iteration 168790 (2.12011 iter/s, 4.71674s/10 iters), loss = 6.92182
I0523 07:00:18.092828 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92182 (* 1 = 6.92182 loss)
I0523 07:00:18.100821 35003 sgd_solver.cpp:112] Iteration 168790, lr = 0.001
I0523 07:00:21.686437 35003 solver.cpp:239] Iteration 168800 (2.78283 iter/s, 3.59346s/10 iters), loss = 6.74538
I0523 07:00:21.686478 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74538 (* 1 = 6.74538 loss)
I0523 07:00:22.419235 35003 sgd_solver.cpp:112] Iteration 168800, lr = 0.001
I0523 07:00:24.479153 35003 solver.cpp:239] Iteration 168810 (3.58095 iter/s, 2.79256s/10 iters), loss = 5.86214
I0523 07:00:24.479197 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.86214 (* 1 = 5.86214 loss)
I0523 07:00:24.493221 35003 sgd_solver.cpp:112] Iteration 168810, lr = 0.001
I0523 07:00:27.547197 35003 solver.cpp:239] Iteration 168820 (3.25959 iter/s, 3.06787s/10 iters), loss = 7.12459
I0523 07:00:27.547243 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12459 (* 1 = 7.12459 loss)
I0523 07:00:28.243042 35003 sgd_solver.cpp:112] Iteration 168820, lr = 0.001
I0523 07:00:31.083443 35003 solver.cpp:239] Iteration 168830 (2.82801 iter/s, 3.53605s/10 iters), loss = 7.05822
I0523 07:00:31.083492 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05822 (* 1 = 7.05822 loss)
I0523 07:00:31.824162 35003 sgd_solver.cpp:112] Iteration 168830, lr = 0.001
I0523 07:00:34.618340 35003 solver.cpp:239] Iteration 168840 (2.82909 iter/s, 3.5347s/10 iters), loss = 6.81397
I0523 07:00:34.618387 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81397 (* 1 = 6.81397 loss)
I0523 07:00:34.631347 35003 sgd_solver.cpp:112] Iteration 168840, lr = 0.001
I0523 07:00:38.787874 35003 solver.cpp:239] Iteration 168850 (2.39847 iter/s, 4.16932s/10 iters), loss = 6.86097
I0523 07:00:38.787911 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86097 (* 1 = 6.86097 loss)
I0523 07:00:38.995919 35003 sgd_solver.cpp:112] Iteration 168850, lr = 0.001
I0523 07:00:42.573251 35003 solver.cpp:239] Iteration 168860 (2.64188 iter/s, 3.78518s/10 iters), loss = 6.31624
I0523 07:00:42.573287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31624 (* 1 = 6.31624 loss)
I0523 07:00:42.586760 35003 sgd_solver.cpp:112] Iteration 168860, lr = 0.001
I0523 07:00:45.445077 35003 solver.cpp:239] Iteration 168870 (3.4823 iter/s, 2.87166s/10 iters), loss = 6.56794
I0523 07:00:45.445253 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56794 (* 1 = 6.56794 loss)
I0523 07:00:45.451643 35003 sgd_solver.cpp:112] Iteration 168870, lr = 0.001
I0523 07:00:48.999192 35003 solver.cpp:239] Iteration 168880 (2.81389 iter/s, 3.5538s/10 iters), loss = 5.9045
I0523 07:00:48.999243 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9045 (* 1 = 5.9045 loss)
I0523 07:00:49.092044 35003 sgd_solver.cpp:112] Iteration 168880, lr = 0.001
I0523 07:00:53.420631 35003 solver.cpp:239] Iteration 168890 (2.26183 iter/s, 4.42121s/10 iters), loss = 7.03546
I0523 07:00:53.420671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03546 (* 1 = 7.03546 loss)
I0523 07:00:53.660512 35003 sgd_solver.cpp:112] Iteration 168890, lr = 0.001
I0523 07:00:57.134873 35003 solver.cpp:239] Iteration 168900 (2.69248 iter/s, 3.71405s/10 iters), loss = 6.35787
I0523 07:00:57.134917 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35787 (* 1 = 6.35787 loss)
I0523 07:00:57.138998 35003 sgd_solver.cpp:112] Iteration 168900, lr = 0.001
I0523 07:01:00.640681 35003 solver.cpp:239] Iteration 168910 (2.85257 iter/s, 3.50561s/10 iters), loss = 7.11788
I0523 07:01:00.640732 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11788 (* 1 = 7.11788 loss)
I0523 07:01:00.654268 35003 sgd_solver.cpp:112] Iteration 168910, lr = 0.001
I0523 07:01:04.984495 35003 solver.cpp:239] Iteration 168920 (2.30224 iter/s, 4.34359s/10 iters), loss = 6.43223
I0523 07:01:04.984537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43223 (* 1 = 6.43223 loss)
I0523 07:01:05.033774 35003 sgd_solver.cpp:112] Iteration 168920, lr = 0.001
I0523 07:01:08.509083 35003 solver.cpp:239] Iteration 168930 (2.83736 iter/s, 3.5244s/10 iters), loss = 6.74707
I0523 07:01:08.509122 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74707 (* 1 = 6.74707 loss)
I0523 07:01:08.520251 35003 sgd_solver.cpp:112] Iteration 168930, lr = 0.001
I0523 07:01:12.305896 35003 solver.cpp:239] Iteration 168940 (2.63393 iter/s, 3.79662s/10 iters), loss = 6.54683
I0523 07:01:12.305932 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54683 (* 1 = 6.54683 loss)
I0523 07:01:12.324764 35003 sgd_solver.cpp:112] Iteration 168940, lr = 0.001
I0523 07:01:15.921875 35003 solver.cpp:239] Iteration 168950 (2.76565 iter/s, 3.61579s/10 iters), loss = 6.69604
I0523 07:01:15.922158 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69604 (* 1 = 6.69604 loss)
I0523 07:01:15.931669 35003 sgd_solver.cpp:112] Iteration 168950, lr = 0.001
I0523 07:01:20.172775 35003 solver.cpp:239] Iteration 168960 (2.35268 iter/s, 4.25047s/10 iters), loss = 7.07899
I0523 07:01:20.172837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07899 (* 1 = 7.07899 loss)
I0523 07:01:20.177423 35003 sgd_solver.cpp:112] Iteration 168960, lr = 0.001
I0523 07:01:23.791658 35003 solver.cpp:239] Iteration 168970 (2.76345 iter/s, 3.61867s/10 iters), loss = 6.2149
I0523 07:01:23.791714 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2149 (* 1 = 6.2149 loss)
I0523 07:01:24.520509 35003 sgd_solver.cpp:112] Iteration 168970, lr = 0.001
I0523 07:01:27.365245 35003 solver.cpp:239] Iteration 168980 (2.79847 iter/s, 3.57338s/10 iters), loss = 6.01638
I0523 07:01:27.365291 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01638 (* 1 = 6.01638 loss)
I0523 07:01:27.379019 35003 sgd_solver.cpp:112] Iteration 168980, lr = 0.001
I0523 07:01:30.234314 35003 solver.cpp:239] Iteration 168990 (3.48565 iter/s, 2.8689s/10 iters), loss = 6.59868
I0523 07:01:30.234352 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59868 (* 1 = 6.59868 loss)
I0523 07:01:30.239117 35003 sgd_solver.cpp:112] Iteration 168990, lr = 0.001
I0523 07:01:33.834337 35003 solver.cpp:239] Iteration 169000 (2.77791 iter/s, 3.59983s/10 iters), loss = 6.77748
I0523 07:01:33.834375 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77748 (* 1 = 6.77748 loss)
I0523 07:01:33.845661 35003 sgd_solver.cpp:112] Iteration 169000, lr = 0.001
I0523 07:01:37.796484 35003 solver.cpp:239] Iteration 169010 (2.52402 iter/s, 3.96194s/10 iters), loss = 7.19699
I0523 07:01:37.796532 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19699 (* 1 = 7.19699 loss)
I0523 07:01:37.810016 35003 sgd_solver.cpp:112] Iteration 169010, lr = 0.001
I0523 07:01:39.782682 35003 solver.cpp:239] Iteration 169020 (5.03509 iter/s, 1.98606s/10 iters), loss = 8.04365
I0523 07:01:39.782734 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04365 (* 1 = 8.04365 loss)
I0523 07:01:39.795879 35003 sgd_solver.cpp:112] Iteration 169020, lr = 0.001
I0523 07:01:43.327754 35003 solver.cpp:239] Iteration 169030 (2.82098 iter/s, 3.54487s/10 iters), loss = 5.98948
I0523 07:01:43.327805 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98948 (* 1 = 5.98948 loss)
I0523 07:01:44.042511 35003 sgd_solver.cpp:112] Iteration 169030, lr = 0.001
I0523 07:01:47.680934 35003 solver.cpp:239] Iteration 169040 (2.29729 iter/s, 4.35295s/10 iters), loss = 7.57508
I0523 07:01:47.681236 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57508 (* 1 = 7.57508 loss)
I0523 07:01:47.694088 35003 sgd_solver.cpp:112] Iteration 169040, lr = 0.001
I0523 07:01:50.520576 35003 solver.cpp:239] Iteration 169050 (3.52206 iter/s, 2.83925s/10 iters), loss = 7.12079
I0523 07:01:50.520612 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12079 (* 1 = 7.12079 loss)
I0523 07:01:50.533109 35003 sgd_solver.cpp:112] Iteration 169050, lr = 0.001
I0523 07:01:54.225800 35003 solver.cpp:239] Iteration 169060 (2.69903 iter/s, 3.70503s/10 iters), loss = 7.52983
I0523 07:01:54.225848 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52983 (* 1 = 7.52983 loss)
I0523 07:01:54.338946 35003 sgd_solver.cpp:112] Iteration 169060, lr = 0.001
I0523 07:01:58.365128 35003 solver.cpp:239] Iteration 169070 (2.41598 iter/s, 4.13911s/10 iters), loss = 6.65124
I0523 07:01:58.365180 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65124 (* 1 = 6.65124 loss)
I0523 07:01:58.917433 35003 sgd_solver.cpp:112] Iteration 169070, lr = 0.001
I0523 07:02:02.677079 35003 solver.cpp:239] Iteration 169080 (2.31926 iter/s, 4.31172s/10 iters), loss = 5.92259
I0523 07:02:02.677122 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92259 (* 1 = 5.92259 loss)
I0523 07:02:03.008574 35003 sgd_solver.cpp:112] Iteration 169080, lr = 0.001
I0523 07:02:06.547535 35003 solver.cpp:239] Iteration 169090 (2.58381 iter/s, 3.87025s/10 iters), loss = 6.08071
I0523 07:02:06.547586 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08071 (* 1 = 6.08071 loss)
I0523 07:02:06.557632 35003 sgd_solver.cpp:112] Iteration 169090, lr = 0.001
I0523 07:02:08.637881 35003 solver.cpp:239] Iteration 169100 (4.78423 iter/s, 2.0902s/10 iters), loss = 7.06278
I0523 07:02:08.637938 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06278 (* 1 = 7.06278 loss)
I0523 07:02:08.645253 35003 sgd_solver.cpp:112] Iteration 169100, lr = 0.001
I0523 07:02:12.897343 35003 solver.cpp:239] Iteration 169110 (2.34784 iter/s, 4.25923s/10 iters), loss = 6.55767
I0523 07:02:12.897397 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55767 (* 1 = 6.55767 loss)
I0523 07:02:13.599144 35003 sgd_solver.cpp:112] Iteration 169110, lr = 0.001
I0523 07:02:18.113822 35003 solver.cpp:239] Iteration 169120 (1.9171 iter/s, 5.21621s/10 iters), loss = 6.09027
I0523 07:02:18.114058 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09027 (* 1 = 6.09027 loss)
I0523 07:02:18.158118 35003 sgd_solver.cpp:112] Iteration 169120, lr = 0.001
I0523 07:02:22.396517 35003 solver.cpp:239] Iteration 169130 (2.33546 iter/s, 4.2818s/10 iters), loss = 6.49273
I0523 07:02:22.396562 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49273 (* 1 = 6.49273 loss)
I0523 07:02:23.128581 35003 sgd_solver.cpp:112] Iteration 169130, lr = 0.001
I0523 07:02:25.214993 35003 solver.cpp:239] Iteration 169140 (3.54822 iter/s, 2.81831s/10 iters), loss = 6.50949
I0523 07:02:25.215039 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50949 (* 1 = 6.50949 loss)
I0523 07:02:25.224840 35003 sgd_solver.cpp:112] Iteration 169140, lr = 0.001
I0523 07:02:27.979840 35003 solver.cpp:239] Iteration 169150 (3.61705 iter/s, 2.76468s/10 iters), loss = 6.34501
I0523 07:02:27.979881 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34501 (* 1 = 6.34501 loss)
I0523 07:02:27.993043 35003 sgd_solver.cpp:112] Iteration 169150, lr = 0.001
I0523 07:02:32.825100 35003 solver.cpp:239] Iteration 169160 (2.06397 iter/s, 4.84502s/10 iters), loss = 5.48928
I0523 07:02:32.825142 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.48928 (* 1 = 5.48928 loss)
I0523 07:02:32.838110 35003 sgd_solver.cpp:112] Iteration 169160, lr = 0.001
I0523 07:02:36.356875 35003 solver.cpp:239] Iteration 169170 (2.83159 iter/s, 3.53158s/10 iters), loss = 7.08397
I0523 07:02:36.356930 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08397 (* 1 = 7.08397 loss)
I0523 07:02:36.429298 35003 sgd_solver.cpp:112] Iteration 169170, lr = 0.001
I0523 07:02:40.101799 35003 solver.cpp:239] Iteration 169180 (2.67043 iter/s, 3.74472s/10 iters), loss = 6.69214
I0523 07:02:40.101840 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69214 (* 1 = 6.69214 loss)
I0523 07:02:40.125196 35003 sgd_solver.cpp:112] Iteration 169180, lr = 0.001
I0523 07:02:43.732241 35003 solver.cpp:239] Iteration 169190 (2.75463 iter/s, 3.63025s/10 iters), loss = 7.07089
I0523 07:02:43.732290 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07089 (* 1 = 7.07089 loss)
I0523 07:02:44.382776 35003 sgd_solver.cpp:112] Iteration 169190, lr = 0.001
I0523 07:02:48.013103 35003 solver.cpp:239] Iteration 169200 (2.3361 iter/s, 4.28064s/10 iters), loss = 6.35226
I0523 07:02:48.013147 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35226 (* 1 = 6.35226 loss)
I0523 07:02:48.020503 35003 sgd_solver.cpp:112] Iteration 169200, lr = 0.001
I0523 07:02:50.633159 35003 solver.cpp:239] Iteration 169210 (3.81695 iter/s, 2.6199s/10 iters), loss = 5.45953
I0523 07:02:50.633314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.45953 (* 1 = 5.45953 loss)
I0523 07:02:50.638697 35003 sgd_solver.cpp:112] Iteration 169210, lr = 0.001
I0523 07:02:54.992233 35003 solver.cpp:239] Iteration 169220 (2.29423 iter/s, 4.35875s/10 iters), loss = 5.7984
I0523 07:02:54.992282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7984 (* 1 = 5.7984 loss)
I0523 07:02:55.627233 35003 sgd_solver.cpp:112] Iteration 169220, lr = 0.001
I0523 07:02:59.301566 35003 solver.cpp:239] Iteration 169230 (2.32067 iter/s, 4.3091s/10 iters), loss = 5.64017
I0523 07:02:59.301607 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.64017 (* 1 = 5.64017 loss)
I0523 07:02:59.325791 35003 sgd_solver.cpp:112] Iteration 169230, lr = 0.001
I0523 07:03:03.679699 35003 solver.cpp:239] Iteration 169240 (2.2842 iter/s, 4.37791s/10 iters), loss = 6.53462
I0523 07:03:03.679741 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53462 (* 1 = 6.53462 loss)
I0523 07:03:03.692931 35003 sgd_solver.cpp:112] Iteration 169240, lr = 0.001
I0523 07:03:07.239449 35003 solver.cpp:239] Iteration 169250 (2.80934 iter/s, 3.55956s/10 iters), loss = 5.94201
I0523 07:03:07.239509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.94201 (* 1 = 5.94201 loss)
I0523 07:03:07.252336 35003 sgd_solver.cpp:112] Iteration 169250, lr = 0.001
I0523 07:03:11.613710 35003 solver.cpp:239] Iteration 169260 (2.28622 iter/s, 4.37403s/10 iters), loss = 5.55461
I0523 07:03:11.613749 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.55461 (* 1 = 5.55461 loss)
I0523 07:03:11.964828 35003 sgd_solver.cpp:112] Iteration 169260, lr = 0.001
I0523 07:03:15.579190 35003 solver.cpp:239] Iteration 169270 (2.52189 iter/s, 3.96527s/10 iters), loss = 6.85736
I0523 07:03:15.579233 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85736 (* 1 = 6.85736 loss)
I0523 07:03:15.592914 35003 sgd_solver.cpp:112] Iteration 169270, lr = 0.001
I0523 07:03:18.808900 35003 solver.cpp:239] Iteration 169280 (3.09643 iter/s, 3.22953s/10 iters), loss = 7.23572
I0523 07:03:18.808950 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23572 (* 1 = 7.23572 loss)
I0523 07:03:18.820426 35003 sgd_solver.cpp:112] Iteration 169280, lr = 0.001
I0523 07:03:22.432492 35003 solver.cpp:239] Iteration 169290 (2.75985 iter/s, 3.62339s/10 iters), loss = 6.24196
I0523 07:03:22.432651 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24196 (* 1 = 6.24196 loss)
I0523 07:03:22.451222 35003 sgd_solver.cpp:112] Iteration 169290, lr = 0.001
I0523 07:03:24.852661 35003 solver.cpp:239] Iteration 169300 (4.13237 iter/s, 2.41992s/10 iters), loss = 6.82521
I0523 07:03:24.852710 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82521 (* 1 = 6.82521 loss)
I0523 07:03:25.587031 35003 sgd_solver.cpp:112] Iteration 169300, lr = 0.001
I0523 07:03:28.390935 35003 solver.cpp:239] Iteration 169310 (2.82639 iter/s, 3.53808s/10 iters), loss = 5.88537
I0523 07:03:28.390974 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.88537 (* 1 = 5.88537 loss)
I0523 07:03:28.395576 35003 sgd_solver.cpp:112] Iteration 169310, lr = 0.001
I0523 07:03:31.120882 35003 solver.cpp:239] Iteration 169320 (3.66329 iter/s, 2.72979s/10 iters), loss = 7.39051
I0523 07:03:31.120929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39051 (* 1 = 7.39051 loss)
I0523 07:03:31.136981 35003 sgd_solver.cpp:112] Iteration 169320, lr = 0.001
I0523 07:03:33.975795 35003 solver.cpp:239] Iteration 169330 (3.50294 iter/s, 2.85475s/10 iters), loss = 6.20055
I0523 07:03:33.975849 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20055 (* 1 = 6.20055 loss)
I0523 07:03:34.001127 35003 sgd_solver.cpp:112] Iteration 169330, lr = 0.001
I0523 07:03:37.438239 35003 solver.cpp:239] Iteration 169340 (2.8883 iter/s, 3.46225s/10 iters), loss = 6.55139
I0523 07:03:37.438293 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55139 (* 1 = 6.55139 loss)
I0523 07:03:37.451545 35003 sgd_solver.cpp:112] Iteration 169340, lr = 0.001
I0523 07:03:38.842128 35003 solver.cpp:239] Iteration 169350 (7.12367 iter/s, 1.40377s/10 iters), loss = 5.02505
I0523 07:03:38.842173 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.02505 (* 1 = 5.02505 loss)
I0523 07:03:39.306046 35003 sgd_solver.cpp:112] Iteration 169350, lr = 0.001
I0523 07:03:42.653033 35003 solver.cpp:239] Iteration 169360 (2.62421 iter/s, 3.81068s/10 iters), loss = 7.08729
I0523 07:03:42.653079 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08729 (* 1 = 7.08729 loss)
I0523 07:03:43.367835 35003 sgd_solver.cpp:112] Iteration 169360, lr = 0.001
I0523 07:03:48.025825 35003 solver.cpp:239] Iteration 169370 (1.86132 iter/s, 5.37252s/10 iters), loss = 6.68149
I0523 07:03:48.025878 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68149 (* 1 = 6.68149 loss)
I0523 07:03:48.721608 35003 sgd_solver.cpp:112] Iteration 169370, lr = 0.001
I0523 07:03:51.909937 35003 solver.cpp:239] Iteration 169380 (2.57473 iter/s, 3.8839s/10 iters), loss = 5.8666
I0523 07:03:51.909981 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8666 (* 1 = 5.8666 loss)
I0523 07:03:52.632216 35003 sgd_solver.cpp:112] Iteration 169380, lr = 0.001
I0523 07:03:54.664079 35003 solver.cpp:239] Iteration 169390 (3.63111 iter/s, 2.75398s/10 iters), loss = 6.75457
I0523 07:03:54.664130 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75457 (* 1 = 6.75457 loss)
I0523 07:03:54.686112 35003 sgd_solver.cpp:112] Iteration 169390, lr = 0.001
I0523 07:03:58.288342 35003 solver.cpp:239] Iteration 169400 (2.75934 iter/s, 3.62406s/10 iters), loss = 6.37983
I0523 07:03:58.288383 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37983 (* 1 = 6.37983 loss)
I0523 07:03:58.312073 35003 sgd_solver.cpp:112] Iteration 169400, lr = 0.001
I0523 07:04:00.414361 35003 solver.cpp:239] Iteration 169410 (4.70394 iter/s, 2.12588s/10 iters), loss = 5.87631
I0523 07:04:00.414417 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87631 (* 1 = 5.87631 loss)
I0523 07:04:00.442047 35003 sgd_solver.cpp:112] Iteration 169410, lr = 0.001
I0523 07:04:03.218591 35003 solver.cpp:239] Iteration 169420 (3.56626 iter/s, 2.80406s/10 iters), loss = 7.30755
I0523 07:04:03.218631 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30755 (* 1 = 7.30755 loss)
I0523 07:04:03.283201 35003 sgd_solver.cpp:112] Iteration 169420, lr = 0.001
I0523 07:04:06.867063 35003 solver.cpp:239] Iteration 169430 (2.74102 iter/s, 3.64827s/10 iters), loss = 7.28155
I0523 07:04:06.867117 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28155 (* 1 = 7.28155 loss)
I0523 07:04:06.872515 35003 sgd_solver.cpp:112] Iteration 169430, lr = 0.001
I0523 07:04:10.591953 35003 solver.cpp:239] Iteration 169440 (2.68479 iter/s, 3.72469s/10 iters), loss = 6.04156
I0523 07:04:10.591995 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04156 (* 1 = 6.04156 loss)
I0523 07:04:11.332764 35003 sgd_solver.cpp:112] Iteration 169440, lr = 0.001
I0523 07:04:15.655670 35003 solver.cpp:239] Iteration 169450 (1.97493 iter/s, 5.06347s/10 iters), loss = 5.61796
I0523 07:04:15.655719 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.61796 (* 1 = 5.61796 loss)
I0523 07:04:16.183253 35003 sgd_solver.cpp:112] Iteration 169450, lr = 0.001
I0523 07:04:18.242321 35003 solver.cpp:239] Iteration 169460 (3.86625 iter/s, 2.58648s/10 iters), loss = 6.84336
I0523 07:04:18.242369 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84336 (* 1 = 6.84336 loss)
I0523 07:04:18.255854 35003 sgd_solver.cpp:112] Iteration 169460, lr = 0.001
I0523 07:04:20.361961 35003 solver.cpp:239] Iteration 169470 (4.71809 iter/s, 2.1195s/10 iters), loss = 6.66835
I0523 07:04:20.362004 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66835 (* 1 = 6.66835 loss)
I0523 07:04:21.096176 35003 sgd_solver.cpp:112] Iteration 169470, lr = 0.001
I0523 07:04:22.980458 35003 solver.cpp:239] Iteration 169480 (3.81921 iter/s, 2.61834s/10 iters), loss = 7.17593
I0523 07:04:22.980707 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17593 (* 1 = 7.17593 loss)
I0523 07:04:23.702096 35003 sgd_solver.cpp:112] Iteration 169480, lr = 0.001
I0523 07:04:27.140487 35003 solver.cpp:239] Iteration 169490 (2.40406 iter/s, 4.15964s/10 iters), loss = 6.33667
I0523 07:04:27.140532 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33667 (* 1 = 6.33667 loss)
I0523 07:04:27.153652 35003 sgd_solver.cpp:112] Iteration 169490, lr = 0.001
I0523 07:04:29.273316 35003 solver.cpp:239] Iteration 169500 (4.68891 iter/s, 2.13269s/10 iters), loss = 6.48603
I0523 07:04:29.273361 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48603 (* 1 = 6.48603 loss)
I0523 07:04:29.289680 35003 sgd_solver.cpp:112] Iteration 169500, lr = 0.001
I0523 07:04:33.728006 35003 solver.cpp:239] Iteration 169510 (2.24494 iter/s, 4.45445s/10 iters), loss = 7.14773
I0523 07:04:33.728065 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14773 (* 1 = 7.14773 loss)
I0523 07:04:34.456038 35003 sgd_solver.cpp:112] Iteration 169510, lr = 0.001
I0523 07:04:39.756850 35003 solver.cpp:239] Iteration 169520 (1.65878 iter/s, 6.02854s/10 iters), loss = 6.80783
I0523 07:04:39.756909 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80783 (* 1 = 6.80783 loss)
I0523 07:04:39.761184 35003 sgd_solver.cpp:112] Iteration 169520, lr = 0.001
I0523 07:04:44.027498 35003 solver.cpp:239] Iteration 169530 (2.3417 iter/s, 4.27041s/10 iters), loss = 6.57513
I0523 07:04:44.027536 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57513 (* 1 = 6.57513 loss)
I0523 07:04:44.031070 35003 sgd_solver.cpp:112] Iteration 169530, lr = 0.001
I0523 07:04:47.187386 35003 solver.cpp:239] Iteration 169540 (3.16484 iter/s, 3.15971s/10 iters), loss = 8.24776
I0523 07:04:47.187438 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.24776 (* 1 = 8.24776 loss)
I0523 07:04:47.195186 35003 sgd_solver.cpp:112] Iteration 169540, lr = 0.001
I0523 07:04:49.292909 35003 solver.cpp:239] Iteration 169550 (4.74979 iter/s, 2.10536s/10 iters), loss = 6.56335
I0523 07:04:49.292973 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56335 (* 1 = 6.56335 loss)
I0523 07:04:49.302884 35003 sgd_solver.cpp:112] Iteration 169550, lr = 0.001
I0523 07:04:52.176525 35003 solver.cpp:239] Iteration 169560 (3.46809 iter/s, 2.88343s/10 iters), loss = 6.69121
I0523 07:04:52.176571 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69121 (* 1 = 6.69121 loss)
I0523 07:04:52.186206 35003 sgd_solver.cpp:112] Iteration 169560, lr = 0.001
I0523 07:04:55.161341 35003 solver.cpp:239] Iteration 169570 (3.35048 iter/s, 2.98464s/10 iters), loss = 7.7296
I0523 07:04:55.161635 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7296 (* 1 = 7.7296 loss)
I0523 07:04:55.174935 35003 sgd_solver.cpp:112] Iteration 169570, lr = 0.001
I0523 07:04:57.242427 35003 solver.cpp:239] Iteration 169580 (4.80601 iter/s, 2.08073s/10 iters), loss = 6.29948
I0523 07:04:57.242467 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29948 (* 1 = 6.29948 loss)
I0523 07:04:57.255975 35003 sgd_solver.cpp:112] Iteration 169580, lr = 0.001
I0523 07:04:59.825994 35003 solver.cpp:239] Iteration 169590 (3.87084 iter/s, 2.58342s/10 iters), loss = 6.58547
I0523 07:04:59.826040 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58547 (* 1 = 6.58547 loss)
I0523 07:05:00.566679 35003 sgd_solver.cpp:112] Iteration 169590, lr = 0.001
I0523 07:05:02.890619 35003 solver.cpp:239] Iteration 169600 (3.26323 iter/s, 3.06444s/10 iters), loss = 6.55746
I0523 07:05:02.890666 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55746 (* 1 = 6.55746 loss)
I0523 07:05:02.893139 35003 sgd_solver.cpp:112] Iteration 169600, lr = 0.001
I0523 07:05:06.493857 35003 solver.cpp:239] Iteration 169610 (2.77544 iter/s, 3.60303s/10 iters), loss = 7.24143
I0523 07:05:06.493894 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24143 (* 1 = 7.24143 loss)
I0523 07:05:06.507123 35003 sgd_solver.cpp:112] Iteration 169610, lr = 0.001
I0523 07:05:11.200891 35003 solver.cpp:239] Iteration 169620 (2.12458 iter/s, 4.70681s/10 iters), loss = 7.19729
I0523 07:05:11.200928 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19729 (* 1 = 7.19729 loss)
I0523 07:05:11.213727 35003 sgd_solver.cpp:112] Iteration 169620, lr = 0.001
I0523 07:05:14.149368 35003 solver.cpp:239] Iteration 169630 (3.39178 iter/s, 2.9483s/10 iters), loss = 7.6371
I0523 07:05:14.149433 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6371 (* 1 = 7.6371 loss)
I0523 07:05:14.153200 35003 sgd_solver.cpp:112] Iteration 169630, lr = 0.001
I0523 07:05:17.217077 35003 solver.cpp:239] Iteration 169640 (3.25998 iter/s, 3.0675s/10 iters), loss = 6.53424
I0523 07:05:17.217129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53424 (* 1 = 6.53424 loss)
I0523 07:05:17.220214 35003 sgd_solver.cpp:112] Iteration 169640, lr = 0.001
I0523 07:05:21.646602 35003 solver.cpp:239] Iteration 169650 (2.2577 iter/s, 4.42929s/10 iters), loss = 7.57935
I0523 07:05:21.646639 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57935 (* 1 = 7.57935 loss)
I0523 07:05:21.668236 35003 sgd_solver.cpp:112] Iteration 169650, lr = 0.001
I0523 07:05:25.533452 35003 solver.cpp:239] Iteration 169660 (2.57291 iter/s, 3.88664s/10 iters), loss = 6.07883
I0523 07:05:25.533629 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07883 (* 1 = 6.07883 loss)
I0523 07:05:25.546618 35003 sgd_solver.cpp:112] Iteration 169660, lr = 0.001
I0523 07:05:28.373082 35003 solver.cpp:239] Iteration 169670 (3.52194 iter/s, 2.83934s/10 iters), loss = 6.98741
I0523 07:05:28.373126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98741 (* 1 = 6.98741 loss)
I0523 07:05:28.378370 35003 sgd_solver.cpp:112] Iteration 169670, lr = 0.001
I0523 07:05:31.974941 35003 solver.cpp:239] Iteration 169680 (2.77649 iter/s, 3.60166s/10 iters), loss = 7.30232
I0523 07:05:31.974980 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30232 (* 1 = 7.30232 loss)
I0523 07:05:31.978621 35003 sgd_solver.cpp:112] Iteration 169680, lr = 0.001
I0523 07:05:37.039263 35003 solver.cpp:239] Iteration 169690 (1.9747 iter/s, 5.06406s/10 iters), loss = 7.07574
I0523 07:05:37.039309 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07574 (* 1 = 7.07574 loss)
I0523 07:05:37.721400 35003 sgd_solver.cpp:112] Iteration 169690, lr = 0.001
I0523 07:05:40.683147 35003 solver.cpp:239] Iteration 169700 (2.74448 iter/s, 3.64368s/10 iters), loss = 6.65676
I0523 07:05:40.683188 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65676 (* 1 = 6.65676 loss)
I0523 07:05:40.707751 35003 sgd_solver.cpp:112] Iteration 169700, lr = 0.001
I0523 07:05:42.017163 35003 solver.cpp:239] Iteration 169710 (7.49681 iter/s, 1.3339s/10 iters), loss = 5.57721
I0523 07:05:42.017205 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.57721 (* 1 = 5.57721 loss)
I0523 07:05:42.110589 35003 sgd_solver.cpp:112] Iteration 169710, lr = 0.001
I0523 07:05:45.952442 35003 solver.cpp:239] Iteration 169720 (2.54125 iter/s, 3.93507s/10 iters), loss = 6.71157
I0523 07:05:45.952491 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71157 (* 1 = 6.71157 loss)
I0523 07:05:45.965749 35003 sgd_solver.cpp:112] Iteration 169720, lr = 0.001
I0523 07:05:50.182482 35003 solver.cpp:239] Iteration 169730 (2.36417 iter/s, 4.22981s/10 iters), loss = 6.39349
I0523 07:05:50.182526 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39349 (* 1 = 6.39349 loss)
I0523 07:05:50.186139 35003 sgd_solver.cpp:112] Iteration 169730, lr = 0.001
I0523 07:05:53.802393 35003 solver.cpp:239] Iteration 169740 (2.76266 iter/s, 3.6197s/10 iters), loss = 5.96051
I0523 07:05:53.802448 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96051 (* 1 = 5.96051 loss)
I0523 07:05:54.518926 35003 sgd_solver.cpp:112] Iteration 169740, lr = 0.001
I0523 07:05:57.702560 35003 solver.cpp:239] Iteration 169750 (2.56413 iter/s, 3.89995s/10 iters), loss = 6.95635
I0523 07:05:57.702834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95635 (* 1 = 6.95635 loss)
I0523 07:05:58.443370 35003 sgd_solver.cpp:112] Iteration 169750, lr = 0.001
I0523 07:06:02.708920 35003 solver.cpp:239] Iteration 169760 (1.99764 iter/s, 5.0059s/10 iters), loss = 5.71521
I0523 07:06:02.708962 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.71521 (* 1 = 5.71521 loss)
I0523 07:06:03.361531 35003 sgd_solver.cpp:112] Iteration 169760, lr = 0.001
I0523 07:06:05.708818 35003 solver.cpp:239] Iteration 169770 (3.33364 iter/s, 2.99973s/10 iters), loss = 5.92099
I0523 07:06:05.708854 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92099 (* 1 = 5.92099 loss)
I0523 07:06:05.721848 35003 sgd_solver.cpp:112] Iteration 169770, lr = 0.001
I0523 07:06:10.217397 35003 solver.cpp:239] Iteration 169780 (2.2181 iter/s, 4.50836s/10 iters), loss = 6.28284
I0523 07:06:10.217454 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28284 (* 1 = 6.28284 loss)
I0523 07:06:10.282876 35003 sgd_solver.cpp:112] Iteration 169780, lr = 0.001
I0523 07:06:14.311596 35003 solver.cpp:239] Iteration 169790 (2.44261 iter/s, 4.09398s/10 iters), loss = 6.37956
I0523 07:06:14.311643 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37956 (* 1 = 6.37956 loss)
I0523 07:06:14.325135 35003 sgd_solver.cpp:112] Iteration 169790, lr = 0.001
I0523 07:06:19.152452 35003 solver.cpp:239] Iteration 169800 (2.06585 iter/s, 4.84061s/10 iters), loss = 7.02981
I0523 07:06:19.152500 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02981 (* 1 = 7.02981 loss)
I0523 07:06:19.171799 35003 sgd_solver.cpp:112] Iteration 169800, lr = 0.001
I0523 07:06:21.930979 35003 solver.cpp:239] Iteration 169810 (3.59924 iter/s, 2.77836s/10 iters), loss = 6.65588
I0523 07:06:21.931036 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65588 (* 1 = 6.65588 loss)
I0523 07:06:21.942384 35003 sgd_solver.cpp:112] Iteration 169810, lr = 0.001
I0523 07:06:25.378242 35003 solver.cpp:239] Iteration 169820 (2.90102 iter/s, 3.44707s/10 iters), loss = 6.47122
I0523 07:06:25.378285 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47122 (* 1 = 6.47122 loss)
I0523 07:06:25.385566 35003 sgd_solver.cpp:112] Iteration 169820, lr = 0.001
I0523 07:06:28.950348 35003 solver.cpp:239] Iteration 169830 (2.79962 iter/s, 3.57191s/10 iters), loss = 6.01902
I0523 07:06:28.950616 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01902 (* 1 = 6.01902 loss)
I0523 07:06:28.960906 35003 sgd_solver.cpp:112] Iteration 169830, lr = 0.001
I0523 07:06:31.736946 35003 solver.cpp:239] Iteration 169840 (3.58909 iter/s, 2.78622s/10 iters), loss = 5.86818
I0523 07:06:31.736984 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.86818 (* 1 = 5.86818 loss)
I0523 07:06:31.750761 35003 sgd_solver.cpp:112] Iteration 169840, lr = 0.001
I0523 07:06:35.406229 35003 solver.cpp:239] Iteration 169850 (2.72548 iter/s, 3.66908s/10 iters), loss = 6.40821
I0523 07:06:35.406276 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40821 (* 1 = 6.40821 loss)
I0523 07:06:35.428555 35003 sgd_solver.cpp:112] Iteration 169850, lr = 0.001
I0523 07:06:38.907425 35003 solver.cpp:239] Iteration 169860 (2.85633 iter/s, 3.501s/10 iters), loss = 6.82653
I0523 07:06:38.907469 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82653 (* 1 = 6.82653 loss)
I0523 07:06:38.920586 35003 sgd_solver.cpp:112] Iteration 169860, lr = 0.001
I0523 07:06:43.902086 35003 solver.cpp:239] Iteration 169870 (2.00224 iter/s, 4.99441s/10 iters), loss = 5.45059
I0523 07:06:43.902125 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.45059 (* 1 = 5.45059 loss)
I0523 07:06:43.930490 35003 sgd_solver.cpp:112] Iteration 169870, lr = 0.001
I0523 07:06:47.151192 35003 solver.cpp:239] Iteration 169880 (3.07795 iter/s, 3.24892s/10 iters), loss = 6.50594
I0523 07:06:47.151249 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50594 (* 1 = 6.50594 loss)
I0523 07:06:47.164913 35003 sgd_solver.cpp:112] Iteration 169880, lr = 0.001
I0523 07:06:49.998801 35003 solver.cpp:239] Iteration 169890 (3.51193 iter/s, 2.84743s/10 iters), loss = 6.16839
I0523 07:06:49.998860 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16839 (* 1 = 6.16839 loss)
I0523 07:06:50.477478 35003 sgd_solver.cpp:112] Iteration 169890, lr = 0.001
I0523 07:06:53.912467 35003 solver.cpp:239] Iteration 169900 (2.55529 iter/s, 3.91345s/10 iters), loss = 5.96583
I0523 07:06:53.912509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96583 (* 1 = 5.96583 loss)
I0523 07:06:54.625864 35003 sgd_solver.cpp:112] Iteration 169900, lr = 0.001
I0523 07:06:56.755183 35003 solver.cpp:239] Iteration 169910 (3.51796 iter/s, 2.84255s/10 iters), loss = 7.58791
I0523 07:06:56.755228 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58791 (* 1 = 7.58791 loss)
I0523 07:06:57.384866 35003 sgd_solver.cpp:112] Iteration 169910, lr = 0.001
I0523 07:07:00.853442 35003 solver.cpp:239] Iteration 169920 (2.44019 iter/s, 4.09804s/10 iters), loss = 6.746
I0523 07:07:00.853555 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.746 (* 1 = 6.746 loss)
I0523 07:07:00.866756 35003 sgd_solver.cpp:112] Iteration 169920, lr = 0.001
I0523 07:07:05.049094 35003 solver.cpp:239] Iteration 169930 (2.38358 iter/s, 4.19536s/10 iters), loss = 6.0373
I0523 07:07:05.049140 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0373 (* 1 = 6.0373 loss)
I0523 07:07:05.764502 35003 sgd_solver.cpp:112] Iteration 169930, lr = 0.001
I0523 07:07:08.637040 35003 solver.cpp:239] Iteration 169940 (2.78726 iter/s, 3.58775s/10 iters), loss = 6.20662
I0523 07:07:08.637092 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20662 (* 1 = 6.20662 loss)
I0523 07:07:09.371587 35003 sgd_solver.cpp:112] Iteration 169940, lr = 0.001
I0523 07:07:12.213570 35003 solver.cpp:239] Iteration 169950 (2.79616 iter/s, 3.57633s/10 iters), loss = 6.28854
I0523 07:07:12.213613 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28854 (* 1 = 6.28854 loss)
I0523 07:07:12.223471 35003 sgd_solver.cpp:112] Iteration 169950, lr = 0.001
I0523 07:07:17.584630 35003 solver.cpp:239] Iteration 169960 (1.86192 iter/s, 5.3708s/10 iters), loss = 6.39416
I0523 07:07:17.584677 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39416 (* 1 = 6.39416 loss)
I0523 07:07:18.326143 35003 sgd_solver.cpp:112] Iteration 169960, lr = 0.001
I0523 07:07:21.856993 35003 solver.cpp:239] Iteration 169970 (2.34075 iter/s, 4.27214s/10 iters), loss = 6.72704
I0523 07:07:21.857031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72704 (* 1 = 6.72704 loss)
I0523 07:07:21.869931 35003 sgd_solver.cpp:112] Iteration 169970, lr = 0.001
I0523 07:07:23.932332 35003 solver.cpp:239] Iteration 169980 (4.81879 iter/s, 2.07521s/10 iters), loss = 7.58331
I0523 07:07:23.932380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58331 (* 1 = 7.58331 loss)
I0523 07:07:23.946127 35003 sgd_solver.cpp:112] Iteration 169980, lr = 0.001
I0523 07:07:27.599854 35003 solver.cpp:239] Iteration 169990 (2.72679 iter/s, 3.66732s/10 iters), loss = 6.07044
I0523 07:07:27.599892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07044 (* 1 = 6.07044 loss)
I0523 07:07:27.617910 35003 sgd_solver.cpp:112] Iteration 169990, lr = 0.001
I0523 07:07:31.771663 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_170000.caffemodel
I0523 07:07:33.498497 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_170000.solverstate
I0523 07:07:33.697751 35003 solver.cpp:239] Iteration 170000 (1.63999 iter/s, 6.09761s/10 iters), loss = 6.12851
I0523 07:07:33.697803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12851 (* 1 = 6.12851 loss)
I0523 07:07:34.407097 35003 sgd_solver.cpp:112] Iteration 170000, lr = 0.001
I0523 07:07:36.475471 35003 solver.cpp:239] Iteration 170010 (3.60029 iter/s, 2.77755s/10 iters), loss = 5.9603
I0523 07:07:36.475508 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9603 (* 1 = 5.9603 loss)
I0523 07:07:36.496937 35003 sgd_solver.cpp:112] Iteration 170010, lr = 0.001
I0523 07:07:37.855151 35003 solver.cpp:239] Iteration 170020 (7.24858 iter/s, 1.37958s/10 iters), loss = 6.53471
I0523 07:07:37.855203 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53471 (* 1 = 6.53471 loss)
I0523 07:07:38.583721 35003 sgd_solver.cpp:112] Iteration 170020, lr = 0.001
I0523 07:07:42.176597 35003 solver.cpp:239] Iteration 170030 (2.31416 iter/s, 4.32122s/10 iters), loss = 6.57584
I0523 07:07:42.176636 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57584 (* 1 = 6.57584 loss)
I0523 07:07:42.184967 35003 sgd_solver.cpp:112] Iteration 170030, lr = 0.001
I0523 07:07:45.015673 35003 solver.cpp:239] Iteration 170040 (3.52247 iter/s, 2.83892s/10 iters), loss = 6.57183
I0523 07:07:45.015718 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57183 (* 1 = 6.57183 loss)
I0523 07:07:45.755125 35003 sgd_solver.cpp:112] Iteration 170040, lr = 0.001
I0523 07:07:49.900918 35003 solver.cpp:239] Iteration 170050 (2.04709 iter/s, 4.88499s/10 iters), loss = 6.28516
I0523 07:07:49.900967 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28516 (* 1 = 6.28516 loss)
I0523 07:07:49.918905 35003 sgd_solver.cpp:112] Iteration 170050, lr = 0.001
I0523 07:07:52.768945 35003 solver.cpp:239] Iteration 170060 (3.48693 iter/s, 2.86786s/10 iters), loss = 6.37706
I0523 07:07:52.768992 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37706 (* 1 = 6.37706 loss)
I0523 07:07:52.824632 35003 sgd_solver.cpp:112] Iteration 170060, lr = 0.001
I0523 07:07:56.364208 35003 solver.cpp:239] Iteration 170070 (2.78159 iter/s, 3.59507s/10 iters), loss = 7.17318
I0523 07:07:56.364248 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17318 (* 1 = 7.17318 loss)
I0523 07:07:56.377240 35003 sgd_solver.cpp:112] Iteration 170070, lr = 0.001
I0523 07:08:00.650446 35003 solver.cpp:239] Iteration 170080 (2.33317 iter/s, 4.28602s/10 iters), loss = 6.07595
I0523 07:08:00.650497 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07595 (* 1 = 6.07595 loss)
I0523 07:08:00.658282 35003 sgd_solver.cpp:112] Iteration 170080, lr = 0.001
I0523 07:08:04.204139 35003 solver.cpp:239] Iteration 170090 (2.81413 iter/s, 3.5535s/10 iters), loss = 7.12446
I0523 07:08:04.204339 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12446 (* 1 = 7.12446 loss)
I0523 07:08:04.221930 35003 sgd_solver.cpp:112] Iteration 170090, lr = 0.001
I0523 07:08:06.612552 35003 solver.cpp:239] Iteration 170100 (4.16019 iter/s, 2.40374s/10 iters), loss = 5.66259
I0523 07:08:06.612599 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.66259 (* 1 = 5.66259 loss)
I0523 07:08:07.317384 35003 sgd_solver.cpp:112] Iteration 170100, lr = 0.001
I0523 07:08:11.473376 35003 solver.cpp:239] Iteration 170110 (2.05737 iter/s, 4.86058s/10 iters), loss = 6.51971
I0523 07:08:11.473417 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51971 (* 1 = 6.51971 loss)
I0523 07:08:11.482733 35003 sgd_solver.cpp:112] Iteration 170110, lr = 0.001
I0523 07:08:15.791378 35003 solver.cpp:239] Iteration 170120 (2.316 iter/s, 4.31779s/10 iters), loss = 6.37738
I0523 07:08:15.791426 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37738 (* 1 = 6.37738 loss)
I0523 07:08:15.800891 35003 sgd_solver.cpp:112] Iteration 170120, lr = 0.001
I0523 07:08:20.114125 35003 solver.cpp:239] Iteration 170130 (2.31346 iter/s, 4.32252s/10 iters), loss = 6.83377
I0523 07:08:20.114162 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83377 (* 1 = 6.83377 loss)
I0523 07:08:20.126932 35003 sgd_solver.cpp:112] Iteration 170130, lr = 0.001
I0523 07:08:24.846978 35003 solver.cpp:239] Iteration 170140 (2.11299 iter/s, 4.73262s/10 iters), loss = 4.6788
I0523 07:08:24.847018 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.6788 (* 1 = 4.6788 loss)
I0523 07:08:25.529918 35003 sgd_solver.cpp:112] Iteration 170140, lr = 0.001
I0523 07:08:27.638550 35003 solver.cpp:239] Iteration 170150 (3.58242 iter/s, 2.79141s/10 iters), loss = 7.28428
I0523 07:08:27.638597 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28428 (* 1 = 7.28428 loss)
I0523 07:08:27.651561 35003 sgd_solver.cpp:112] Iteration 170150, lr = 0.001
I0523 07:08:28.960834 35003 solver.cpp:239] Iteration 170160 (7.5633 iter/s, 1.32217s/10 iters), loss = 6.27479
I0523 07:08:28.960876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27479 (* 1 = 6.27479 loss)
I0523 07:08:29.663298 35003 sgd_solver.cpp:112] Iteration 170160, lr = 0.001
I0523 07:08:33.482741 35003 solver.cpp:239] Iteration 170170 (2.21157 iter/s, 4.52168s/10 iters), loss = 6.42138
I0523 07:08:33.482789 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42138 (* 1 = 6.42138 loss)
I0523 07:08:33.491580 35003 sgd_solver.cpp:112] Iteration 170170, lr = 0.001
I0523 07:08:35.516912 35003 solver.cpp:239] Iteration 170180 (4.91635 iter/s, 2.03403s/10 iters), loss = 6.39241
I0523 07:08:35.517035 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39241 (* 1 = 6.39241 loss)
I0523 07:08:35.546502 35003 sgd_solver.cpp:112] Iteration 170180, lr = 0.001
I0523 07:08:38.549301 35003 solver.cpp:239] Iteration 170190 (3.29801 iter/s, 3.03213s/10 iters), loss = 6.40035
I0523 07:08:38.549352 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40035 (* 1 = 6.40035 loss)
I0523 07:08:38.573029 35003 sgd_solver.cpp:112] Iteration 170190, lr = 0.001
I0523 07:08:42.273658 35003 solver.cpp:239] Iteration 170200 (2.68519 iter/s, 3.72414s/10 iters), loss = 5.87732
I0523 07:08:42.273727 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87732 (* 1 = 5.87732 loss)
I0523 07:08:42.285024 35003 sgd_solver.cpp:112] Iteration 170200, lr = 0.001
I0523 07:08:44.391660 35003 solver.cpp:239] Iteration 170210 (4.72178 iter/s, 2.11785s/10 iters), loss = 7.13718
I0523 07:08:44.391707 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13718 (* 1 = 7.13718 loss)
I0523 07:08:44.394569 35003 sgd_solver.cpp:112] Iteration 170210, lr = 0.001
I0523 07:08:47.870009 35003 solver.cpp:239] Iteration 170220 (2.87509 iter/s, 3.47815s/10 iters), loss = 7.09207
I0523 07:08:47.870055 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09207 (* 1 = 7.09207 loss)
I0523 07:08:47.879421 35003 sgd_solver.cpp:112] Iteration 170220, lr = 0.001
I0523 07:08:50.748708 35003 solver.cpp:239] Iteration 170230 (3.474 iter/s, 2.87853s/10 iters), loss = 6.60648
I0523 07:08:50.748765 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60648 (* 1 = 6.60648 loss)
I0523 07:08:51.464222 35003 sgd_solver.cpp:112] Iteration 170230, lr = 0.001
I0523 07:08:53.639863 35003 solver.cpp:239] Iteration 170240 (3.45908 iter/s, 2.89094s/10 iters), loss = 6.57993
I0523 07:08:53.639911 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57993 (* 1 = 6.57993 loss)
I0523 07:08:53.647039 35003 sgd_solver.cpp:112] Iteration 170240, lr = 0.001
I0523 07:08:58.057114 35003 solver.cpp:239] Iteration 170250 (2.26397 iter/s, 4.41702s/10 iters), loss = 6.26215
I0523 07:08:58.057160 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26215 (* 1 = 6.26215 loss)
I0523 07:08:58.065402 35003 sgd_solver.cpp:112] Iteration 170250, lr = 0.001
I0523 07:09:01.362125 35003 solver.cpp:239] Iteration 170260 (3.02588 iter/s, 3.30482s/10 iters), loss = 5.36089
I0523 07:09:01.362172 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.36089 (* 1 = 5.36089 loss)
I0523 07:09:01.380501 35003 sgd_solver.cpp:112] Iteration 170260, lr = 0.001
I0523 07:09:04.787858 35003 solver.cpp:239] Iteration 170270 (2.91924 iter/s, 3.42554s/10 iters), loss = 5.10615
I0523 07:09:04.787902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.10615 (* 1 = 5.10615 loss)
I0523 07:09:05.516454 35003 sgd_solver.cpp:112] Iteration 170270, lr = 0.001
I0523 07:09:08.387662 35003 solver.cpp:239] Iteration 170280 (2.77808 iter/s, 3.59961s/10 iters), loss = 5.75281
I0523 07:09:08.387884 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.75281 (* 1 = 5.75281 loss)
I0523 07:09:09.093711 35003 sgd_solver.cpp:112] Iteration 170280, lr = 0.001
I0523 07:09:11.599627 35003 solver.cpp:239] Iteration 170290 (3.11369 iter/s, 3.21163s/10 iters), loss = 7.84774
I0523 07:09:11.599676 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84774 (* 1 = 7.84774 loss)
I0523 07:09:11.607511 35003 sgd_solver.cpp:112] Iteration 170290, lr = 0.001
I0523 07:09:13.628520 35003 solver.cpp:239] Iteration 170300 (4.92916 iter/s, 2.02874s/10 iters), loss = 5.69815
I0523 07:09:13.628576 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.69815 (* 1 = 5.69815 loss)
I0523 07:09:13.631363 35003 sgd_solver.cpp:112] Iteration 170300, lr = 0.001
I0523 07:09:17.155983 35003 solver.cpp:239] Iteration 170310 (2.83507 iter/s, 3.52726s/10 iters), loss = 6.0797
I0523 07:09:17.156046 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0797 (* 1 = 6.0797 loss)
I0523 07:09:17.173805 35003 sgd_solver.cpp:112] Iteration 170310, lr = 0.001
I0523 07:09:20.044767 35003 solver.cpp:239] Iteration 170320 (3.46188 iter/s, 2.8886s/10 iters), loss = 6.55045
I0523 07:09:20.044811 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55045 (* 1 = 6.55045 loss)
I0523 07:09:20.053989 35003 sgd_solver.cpp:112] Iteration 170320, lr = 0.001
I0523 07:09:22.198415 35003 solver.cpp:239] Iteration 170330 (4.6436 iter/s, 2.1535s/10 iters), loss = 6.32333
I0523 07:09:22.198464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32333 (* 1 = 6.32333 loss)
I0523 07:09:22.201448 35003 sgd_solver.cpp:112] Iteration 170330, lr = 0.001
I0523 07:09:23.494104 35003 solver.cpp:239] Iteration 170340 (7.71857 iter/s, 1.29558s/10 iters), loss = 6.31978
I0523 07:09:23.494153 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31978 (* 1 = 6.31978 loss)
I0523 07:09:23.507544 35003 sgd_solver.cpp:112] Iteration 170340, lr = 0.001
I0523 07:09:25.724589 35003 solver.cpp:239] Iteration 170350 (4.48362 iter/s, 2.23034s/10 iters), loss = 7.31656
I0523 07:09:25.724632 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31656 (* 1 = 7.31656 loss)
I0523 07:09:25.730576 35003 sgd_solver.cpp:112] Iteration 170350, lr = 0.001
I0523 07:09:27.776561 35003 solver.cpp:239] Iteration 170360 (4.87367 iter/s, 2.05184s/10 iters), loss = 5.57682
I0523 07:09:27.776602 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.57682 (* 1 = 5.57682 loss)
I0523 07:09:27.799451 35003 sgd_solver.cpp:112] Iteration 170360, lr = 0.001
I0523 07:09:29.678306 35003 solver.cpp:239] Iteration 170370 (5.25869 iter/s, 1.90161s/10 iters), loss = 6.50139
I0523 07:09:29.678356 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50139 (* 1 = 6.50139 loss)
I0523 07:09:29.686293 35003 sgd_solver.cpp:112] Iteration 170370, lr = 0.001
I0523 07:09:33.915609 35003 solver.cpp:239] Iteration 170380 (2.36012 iter/s, 4.23708s/10 iters), loss = 7.44178
I0523 07:09:33.915657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44178 (* 1 = 7.44178 loss)
I0523 07:09:34.616345 35003 sgd_solver.cpp:112] Iteration 170380, lr = 0.001
I0523 07:09:37.329350 35003 solver.cpp:239] Iteration 170390 (2.9295 iter/s, 3.41355s/10 iters), loss = 6.95732
I0523 07:09:37.329394 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95732 (* 1 = 6.95732 loss)
I0523 07:09:37.342859 35003 sgd_solver.cpp:112] Iteration 170390, lr = 0.001
I0523 07:09:40.907438 35003 solver.cpp:239] Iteration 170400 (2.79495 iter/s, 3.57789s/10 iters), loss = 6.29325
I0523 07:09:40.907671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29325 (* 1 = 6.29325 loss)
I0523 07:09:40.917357 35003 sgd_solver.cpp:112] Iteration 170400, lr = 0.001
I0523 07:09:44.537500 35003 solver.cpp:239] Iteration 170410 (2.75507 iter/s, 3.62967s/10 iters), loss = 6.04786
I0523 07:09:44.537544 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04786 (* 1 = 6.04786 loss)
I0523 07:09:44.550456 35003 sgd_solver.cpp:112] Iteration 170410, lr = 0.001
I0523 07:09:48.130451 35003 solver.cpp:239] Iteration 170420 (2.78338 iter/s, 3.59276s/10 iters), loss = 7.33164
I0523 07:09:48.130501 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33164 (* 1 = 7.33164 loss)
I0523 07:09:48.832722 35003 sgd_solver.cpp:112] Iteration 170420, lr = 0.001
I0523 07:09:52.997388 35003 solver.cpp:239] Iteration 170430 (2.05479 iter/s, 4.86668s/10 iters), loss = 7.08099
I0523 07:09:52.997448 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08099 (* 1 = 7.08099 loss)
I0523 07:09:53.002879 35003 sgd_solver.cpp:112] Iteration 170430, lr = 0.001
I0523 07:09:57.400290 35003 solver.cpp:239] Iteration 170440 (2.27136 iter/s, 4.40266s/10 iters), loss = 7.14116
I0523 07:09:57.400339 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14116 (* 1 = 7.14116 loss)
I0523 07:09:58.141185 35003 sgd_solver.cpp:112] Iteration 170440, lr = 0.001
I0523 07:10:00.870129 35003 solver.cpp:239] Iteration 170450 (2.88214 iter/s, 3.46965s/10 iters), loss = 5.75115
I0523 07:10:00.870167 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.75115 (* 1 = 5.75115 loss)
I0523 07:10:00.892881 35003 sgd_solver.cpp:112] Iteration 170450, lr = 0.001
I0523 07:10:02.567057 35003 solver.cpp:239] Iteration 170460 (5.89341 iter/s, 1.69681s/10 iters), loss = 7.0962
I0523 07:10:02.567106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0962 (* 1 = 7.0962 loss)
I0523 07:10:03.308058 35003 sgd_solver.cpp:112] Iteration 170460, lr = 0.001
I0523 07:10:07.643541 35003 solver.cpp:239] Iteration 170470 (1.96997 iter/s, 5.07622s/10 iters), loss = 6.61671
I0523 07:10:07.643604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61671 (* 1 = 6.61671 loss)
I0523 07:10:08.384413 35003 sgd_solver.cpp:112] Iteration 170470, lr = 0.001
I0523 07:10:10.623064 35003 solver.cpp:239] Iteration 170480 (3.35646 iter/s, 2.97933s/10 iters), loss = 6.25328
I0523 07:10:10.623114 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25328 (* 1 = 6.25328 loss)
I0523 07:10:10.630714 35003 sgd_solver.cpp:112] Iteration 170480, lr = 0.001
I0523 07:10:14.200812 35003 solver.cpp:239] Iteration 170490 (2.79521 iter/s, 3.57755s/10 iters), loss = 6.21957
I0523 07:10:14.201009 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21957 (* 1 = 6.21957 loss)
I0523 07:10:14.205669 35003 sgd_solver.cpp:112] Iteration 170490, lr = 0.001
I0523 07:10:16.880002 35003 solver.cpp:239] Iteration 170500 (3.7329 iter/s, 2.67888s/10 iters), loss = 6.92836
I0523 07:10:16.880043 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92836 (* 1 = 6.92836 loss)
I0523 07:10:16.883882 35003 sgd_solver.cpp:112] Iteration 170500, lr = 0.001
I0523 07:10:18.485545 35003 solver.cpp:239] Iteration 170510 (6.22887 iter/s, 1.60543s/10 iters), loss = 6.77629
I0523 07:10:18.485608 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77629 (* 1 = 6.77629 loss)
I0523 07:10:19.201099 35003 sgd_solver.cpp:112] Iteration 170510, lr = 0.001
I0523 07:10:23.658969 35003 solver.cpp:239] Iteration 170520 (1.93306 iter/s, 5.17315s/10 iters), loss = 7.61964
I0523 07:10:23.659019 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61964 (* 1 = 7.61964 loss)
I0523 07:10:24.354809 35003 sgd_solver.cpp:112] Iteration 170520, lr = 0.001
I0523 07:10:27.883741 35003 solver.cpp:239] Iteration 170530 (2.36712 iter/s, 4.22455s/10 iters), loss = 5.47823
I0523 07:10:27.883782 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.47823 (* 1 = 5.47823 loss)
I0523 07:10:28.591030 35003 sgd_solver.cpp:112] Iteration 170530, lr = 0.001
I0523 07:10:32.054646 35003 solver.cpp:239] Iteration 170540 (2.39769 iter/s, 4.17069s/10 iters), loss = 7.62977
I0523 07:10:32.054689 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62977 (* 1 = 7.62977 loss)
I0523 07:10:32.059592 35003 sgd_solver.cpp:112] Iteration 170540, lr = 0.001
I0523 07:10:36.084460 35003 solver.cpp:239] Iteration 170550 (2.48164 iter/s, 4.02959s/10 iters), loss = 6.21509
I0523 07:10:36.084516 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21509 (* 1 = 6.21509 loss)
I0523 07:10:36.092233 35003 sgd_solver.cpp:112] Iteration 170550, lr = 0.001
I0523 07:10:39.891633 35003 solver.cpp:239] Iteration 170560 (2.62678 iter/s, 3.80695s/10 iters), loss = 7.43948
I0523 07:10:39.891674 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43948 (* 1 = 7.43948 loss)
I0523 07:10:39.894492 35003 sgd_solver.cpp:112] Iteration 170560, lr = 0.001
I0523 07:10:41.937542 35003 solver.cpp:239] Iteration 170570 (4.88813 iter/s, 2.04577s/10 iters), loss = 6.9122
I0523 07:10:41.937584 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9122 (* 1 = 6.9122 loss)
I0523 07:10:42.675355 35003 sgd_solver.cpp:112] Iteration 170570, lr = 0.001
I0523 07:10:46.259542 35003 solver.cpp:239] Iteration 170580 (2.31386 iter/s, 4.32178s/10 iters), loss = 6.08217
I0523 07:10:46.259713 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08217 (* 1 = 6.08217 loss)
I0523 07:10:46.283514 35003 sgd_solver.cpp:112] Iteration 170580, lr = 0.001
I0523 07:10:49.187261 35003 solver.cpp:239] Iteration 170590 (3.41596 iter/s, 2.92743s/10 iters), loss = 6.75237
I0523 07:10:49.187305 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75237 (* 1 = 6.75237 loss)
I0523 07:10:49.922277 35003 sgd_solver.cpp:112] Iteration 170590, lr = 0.001
I0523 07:10:54.791069 35003 solver.cpp:239] Iteration 170600 (1.78459 iter/s, 5.60354s/10 iters), loss = 6.06264
I0523 07:10:54.791117 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06264 (* 1 = 6.06264 loss)
I0523 07:10:55.492872 35003 sgd_solver.cpp:112] Iteration 170600, lr = 0.001
I0523 07:10:58.855501 35003 solver.cpp:239] Iteration 170610 (2.4605 iter/s, 4.06422s/10 iters), loss = 6.69852
I0523 07:10:58.855547 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69852 (* 1 = 6.69852 loss)
I0523 07:10:58.862412 35003 sgd_solver.cpp:112] Iteration 170610, lr = 0.001
I0523 07:11:03.120276 35003 solver.cpp:239] Iteration 170620 (2.34491 iter/s, 4.26455s/10 iters), loss = 7.0956
I0523 07:11:03.120327 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0956 (* 1 = 7.0956 loss)
I0523 07:11:03.752714 35003 sgd_solver.cpp:112] Iteration 170620, lr = 0.001
I0523 07:11:06.557489 35003 solver.cpp:239] Iteration 170630 (2.90951 iter/s, 3.43701s/10 iters), loss = 6.49812
I0523 07:11:06.557564 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49812 (* 1 = 6.49812 loss)
I0523 07:11:06.560930 35003 sgd_solver.cpp:112] Iteration 170630, lr = 0.001
I0523 07:11:09.330832 35003 solver.cpp:239] Iteration 170640 (3.60601 iter/s, 2.77315s/10 iters), loss = 6.04844
I0523 07:11:09.330880 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04844 (* 1 = 6.04844 loss)
I0523 07:11:09.344300 35003 sgd_solver.cpp:112] Iteration 170640, lr = 0.001
I0523 07:11:13.627097 35003 solver.cpp:239] Iteration 170650 (2.32773 iter/s, 4.29603s/10 iters), loss = 7.48315
I0523 07:11:13.627140 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48315 (* 1 = 7.48315 loss)
I0523 07:11:13.639470 35003 sgd_solver.cpp:112] Iteration 170650, lr = 0.001
I0523 07:11:15.423794 35003 solver.cpp:239] Iteration 170660 (5.56618 iter/s, 1.79656s/10 iters), loss = 6.33227
I0523 07:11:15.423863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33227 (* 1 = 6.33227 loss)
I0523 07:11:15.431823 35003 sgd_solver.cpp:112] Iteration 170660, lr = 0.001
I0523 07:11:19.525923 35003 solver.cpp:239] Iteration 170670 (2.4379 iter/s, 4.10189s/10 iters), loss = 5.55674
I0523 07:11:19.526232 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.55674 (* 1 = 5.55674 loss)
I0523 07:11:19.538823 35003 sgd_solver.cpp:112] Iteration 170670, lr = 0.001
I0523 07:11:22.744747 35003 solver.cpp:239] Iteration 170680 (3.10712 iter/s, 3.21841s/10 iters), loss = 7.73395
I0523 07:11:22.744794 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73395 (* 1 = 7.73395 loss)
I0523 07:11:22.928973 35003 sgd_solver.cpp:112] Iteration 170680, lr = 0.001
I0523 07:11:25.925530 35003 solver.cpp:239] Iteration 170690 (3.14406 iter/s, 3.1806s/10 iters), loss = 6.35692
I0523 07:11:25.925570 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35692 (* 1 = 6.35692 loss)
I0523 07:11:25.938274 35003 sgd_solver.cpp:112] Iteration 170690, lr = 0.001
I0523 07:11:29.343971 35003 solver.cpp:239] Iteration 170700 (2.92547 iter/s, 3.41826s/10 iters), loss = 6.0825
I0523 07:11:29.344007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0825 (* 1 = 6.0825 loss)
I0523 07:11:29.356896 35003 sgd_solver.cpp:112] Iteration 170700, lr = 0.001
I0523 07:11:33.323699 35003 solver.cpp:239] Iteration 170710 (2.51286 iter/s, 3.97953s/10 iters), loss = 5.16579
I0523 07:11:33.323747 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.16579 (* 1 = 5.16579 loss)
I0523 07:11:34.026206 35003 sgd_solver.cpp:112] Iteration 170710, lr = 0.001
I0523 07:11:38.305084 35003 solver.cpp:239] Iteration 170720 (2.00758 iter/s, 4.98113s/10 iters), loss = 7.41997
I0523 07:11:38.305140 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41997 (* 1 = 7.41997 loss)
I0523 07:11:38.312369 35003 sgd_solver.cpp:112] Iteration 170720, lr = 0.001
I0523 07:11:41.916694 35003 solver.cpp:239] Iteration 170730 (2.76901 iter/s, 3.6114s/10 iters), loss = 6.27363
I0523 07:11:41.916756 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27363 (* 1 = 6.27363 loss)
I0523 07:11:41.921125 35003 sgd_solver.cpp:112] Iteration 170730, lr = 0.001
I0523 07:11:46.316762 35003 solver.cpp:239] Iteration 170740 (2.27282 iter/s, 4.39983s/10 iters), loss = 5.97479
I0523 07:11:46.316800 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97479 (* 1 = 5.97479 loss)
I0523 07:11:46.323772 35003 sgd_solver.cpp:112] Iteration 170740, lr = 0.001
I0523 07:11:49.939512 35003 solver.cpp:239] Iteration 170750 (2.76049 iter/s, 3.62255s/10 iters), loss = 5.91765
I0523 07:11:49.939788 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91765 (* 1 = 5.91765 loss)
I0523 07:11:50.667542 35003 sgd_solver.cpp:112] Iteration 170750, lr = 0.001
I0523 07:11:52.663192 35003 solver.cpp:239] Iteration 170760 (3.672 iter/s, 2.72331s/10 iters), loss = 6.893
I0523 07:11:52.663234 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.893 (* 1 = 6.893 loss)
I0523 07:11:52.668786 35003 sgd_solver.cpp:112] Iteration 170760, lr = 0.001
I0523 07:11:55.535645 35003 solver.cpp:239] Iteration 170770 (3.48155 iter/s, 2.87229s/10 iters), loss = 7.16425
I0523 07:11:55.535689 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16425 (* 1 = 7.16425 loss)
I0523 07:11:55.545094 35003 sgd_solver.cpp:112] Iteration 170770, lr = 0.001
I0523 07:11:58.428200 35003 solver.cpp:239] Iteration 170780 (3.45736 iter/s, 2.89238s/10 iters), loss = 7.33558
I0523 07:11:58.428246 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33558 (* 1 = 7.33558 loss)
I0523 07:11:59.013787 35003 sgd_solver.cpp:112] Iteration 170780, lr = 0.001
I0523 07:12:02.588369 35003 solver.cpp:239] Iteration 170790 (2.40387 iter/s, 4.15995s/10 iters), loss = 6.54291
I0523 07:12:02.588412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54291 (* 1 = 6.54291 loss)
I0523 07:12:02.601238 35003 sgd_solver.cpp:112] Iteration 170790, lr = 0.001
I0523 07:12:05.617974 35003 solver.cpp:239] Iteration 170800 (3.30095 iter/s, 3.02943s/10 iters), loss = 6.38755
I0523 07:12:05.618022 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38755 (* 1 = 6.38755 loss)
I0523 07:12:05.631249 35003 sgd_solver.cpp:112] Iteration 170800, lr = 0.001
I0523 07:12:09.828152 35003 solver.cpp:239] Iteration 170810 (2.37532 iter/s, 4.20996s/10 iters), loss = 5.77098
I0523 07:12:09.828191 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77098 (* 1 = 5.77098 loss)
I0523 07:12:10.509668 35003 sgd_solver.cpp:112] Iteration 170810, lr = 0.001
I0523 07:12:13.444702 35003 solver.cpp:239] Iteration 170820 (2.76521 iter/s, 3.61636s/10 iters), loss = 6.58834
I0523 07:12:13.444737 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58834 (* 1 = 6.58834 loss)
I0523 07:12:13.454442 35003 sgd_solver.cpp:112] Iteration 170820, lr = 0.001
I0523 07:12:15.543076 35003 solver.cpp:239] Iteration 170830 (4.76588 iter/s, 2.09825s/10 iters), loss = 6.74635
I0523 07:12:15.543124 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74635 (* 1 = 6.74635 loss)
I0523 07:12:16.256384 35003 sgd_solver.cpp:112] Iteration 170830, lr = 0.001
I0523 07:12:19.000982 35003 solver.cpp:239] Iteration 170840 (2.89208 iter/s, 3.45772s/10 iters), loss = 6.23176
I0523 07:12:19.001025 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23176 (* 1 = 6.23176 loss)
I0523 07:12:19.026109 35003 sgd_solver.cpp:112] Iteration 170840, lr = 0.001
I0523 07:12:21.767303 35003 solver.cpp:239] Iteration 170850 (3.61512 iter/s, 2.76616s/10 iters), loss = 6.05862
I0523 07:12:21.767429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05862 (* 1 = 6.05862 loss)
I0523 07:12:22.495482 35003 sgd_solver.cpp:112] Iteration 170850, lr = 0.001
I0523 07:12:24.879328 35003 solver.cpp:239] Iteration 170860 (3.21361 iter/s, 3.11177s/10 iters), loss = 6.53624
I0523 07:12:24.879376 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53624 (* 1 = 6.53624 loss)
I0523 07:12:24.891672 35003 sgd_solver.cpp:112] Iteration 170860, lr = 0.001
I0523 07:12:27.717260 35003 solver.cpp:239] Iteration 170870 (3.52391 iter/s, 2.83776s/10 iters), loss = 6.82834
I0523 07:12:27.717308 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82834 (* 1 = 6.82834 loss)
I0523 07:12:28.452364 35003 sgd_solver.cpp:112] Iteration 170870, lr = 0.001
I0523 07:12:32.691378 35003 solver.cpp:239] Iteration 170880 (2.01051 iter/s, 4.97386s/10 iters), loss = 6.4321
I0523 07:12:32.691437 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4321 (* 1 = 6.4321 loss)
I0523 07:12:32.704288 35003 sgd_solver.cpp:112] Iteration 170880, lr = 0.001
I0523 07:12:35.531915 35003 solver.cpp:239] Iteration 170890 (3.52068 iter/s, 2.84036s/10 iters), loss = 5.00387
I0523 07:12:35.531955 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.00387 (* 1 = 5.00387 loss)
I0523 07:12:36.079432 35003 sgd_solver.cpp:112] Iteration 170890, lr = 0.001
I0523 07:12:40.141800 35003 solver.cpp:239] Iteration 170900 (2.16936 iter/s, 4.60966s/10 iters), loss = 7.13284
I0523 07:12:40.141844 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13284 (* 1 = 7.13284 loss)
I0523 07:12:40.156306 35003 sgd_solver.cpp:112] Iteration 170900, lr = 0.001
I0523 07:12:43.787003 35003 solver.cpp:239] Iteration 170910 (2.74524 iter/s, 3.64267s/10 iters), loss = 6.45575
I0523 07:12:43.787053 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45575 (* 1 = 6.45575 loss)
I0523 07:12:44.521459 35003 sgd_solver.cpp:112] Iteration 170910, lr = 0.001
I0523 07:12:48.747153 35003 solver.cpp:239] Iteration 170920 (2.01617 iter/s, 4.9599s/10 iters), loss = 6.7442
I0523 07:12:48.747195 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7442 (* 1 = 6.7442 loss)
I0523 07:12:48.761639 35003 sgd_solver.cpp:112] Iteration 170920, lr = 0.001
I0523 07:12:52.021625 35003 solver.cpp:239] Iteration 170930 (3.05409 iter/s, 3.27429s/10 iters), loss = 6.24886
I0523 07:12:52.021879 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24886 (* 1 = 6.24886 loss)
I0523 07:12:52.732798 35003 sgd_solver.cpp:112] Iteration 170930, lr = 0.001
I0523 07:12:55.169705 35003 solver.cpp:239] Iteration 170940 (3.17691 iter/s, 3.14771s/10 iters), loss = 5.98422
I0523 07:12:55.169764 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98422 (* 1 = 5.98422 loss)
I0523 07:12:55.737725 35003 sgd_solver.cpp:112] Iteration 170940, lr = 0.001
I0523 07:12:58.501075 35003 solver.cpp:239] Iteration 170950 (3.00194 iter/s, 3.33118s/10 iters), loss = 7.15015
I0523 07:12:58.501116 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15015 (* 1 = 7.15015 loss)
I0523 07:12:58.514648 35003 sgd_solver.cpp:112] Iteration 170950, lr = 0.001
I0523 07:13:00.383671 35003 solver.cpp:239] Iteration 170960 (5.31217 iter/s, 1.88247s/10 iters), loss = 7.88162
I0523 07:13:00.383721 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88162 (* 1 = 7.88162 loss)
I0523 07:13:00.400619 35003 sgd_solver.cpp:112] Iteration 170960, lr = 0.001
I0523 07:13:04.399916 35003 solver.cpp:239] Iteration 170970 (2.49002 iter/s, 4.01603s/10 iters), loss = 5.87494
I0523 07:13:04.399961 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87494 (* 1 = 5.87494 loss)
I0523 07:13:04.782358 35003 sgd_solver.cpp:112] Iteration 170970, lr = 0.001
I0523 07:13:08.997001 35003 solver.cpp:239] Iteration 170980 (2.1754 iter/s, 4.59685s/10 iters), loss = 6.51269
I0523 07:13:08.997041 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51269 (* 1 = 6.51269 loss)
I0523 07:13:09.002118 35003 sgd_solver.cpp:112] Iteration 170980, lr = 0.001
I0523 07:13:13.198801 35003 solver.cpp:239] Iteration 170990 (2.38006 iter/s, 4.20157s/10 iters), loss = 6.25166
I0523 07:13:13.198873 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25166 (* 1 = 6.25166 loss)
I0523 07:13:13.203979 35003 sgd_solver.cpp:112] Iteration 170990, lr = 0.001
I0523 07:13:16.040709 35003 solver.cpp:239] Iteration 171000 (3.51901 iter/s, 2.84171s/10 iters), loss = 5.92423
I0523 07:13:16.040781 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92423 (* 1 = 5.92423 loss)
I0523 07:13:16.129801 35003 sgd_solver.cpp:112] Iteration 171000, lr = 0.001
I0523 07:13:20.393220 35003 solver.cpp:239] Iteration 171010 (2.29765 iter/s, 4.35227s/10 iters), loss = 5.80108
I0523 07:13:20.393260 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.80108 (* 1 = 5.80108 loss)
I0523 07:13:21.131126 35003 sgd_solver.cpp:112] Iteration 171010, lr = 0.001
I0523 07:13:24.503134 35003 solver.cpp:239] Iteration 171020 (2.43327 iter/s, 4.1097s/10 iters), loss = 7.06848
I0523 07:13:24.503326 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06848 (* 1 = 7.06848 loss)
I0523 07:13:25.218674 35003 sgd_solver.cpp:112] Iteration 171020, lr = 0.001
I0523 07:13:28.847653 35003 solver.cpp:239] Iteration 171030 (2.30195 iter/s, 4.34415s/10 iters), loss = 6.93064
I0523 07:13:28.847715 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93064 (* 1 = 6.93064 loss)
I0523 07:13:28.857697 35003 sgd_solver.cpp:112] Iteration 171030, lr = 0.001
I0523 07:13:30.997567 35003 solver.cpp:239] Iteration 171040 (4.65169 iter/s, 2.14976s/10 iters), loss = 7.49057
I0523 07:13:30.997617 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49057 (* 1 = 7.49057 loss)
I0523 07:13:31.009891 35003 sgd_solver.cpp:112] Iteration 171040, lr = 0.001
I0523 07:13:33.739279 35003 solver.cpp:239] Iteration 171050 (3.64758 iter/s, 2.74154s/10 iters), loss = 6.09271
I0523 07:13:33.739321 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09271 (* 1 = 6.09271 loss)
I0523 07:13:33.762061 35003 sgd_solver.cpp:112] Iteration 171050, lr = 0.001
I0523 07:13:37.143203 35003 solver.cpp:239] Iteration 171060 (2.93795 iter/s, 3.40374s/10 iters), loss = 6.57977
I0523 07:13:37.143245 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57977 (* 1 = 6.57977 loss)
I0523 07:13:37.156486 35003 sgd_solver.cpp:112] Iteration 171060, lr = 0.001
I0523 07:13:40.653887 35003 solver.cpp:239] Iteration 171070 (2.8486 iter/s, 3.51049s/10 iters), loss = 7.01554
I0523 07:13:40.653939 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01554 (* 1 = 7.01554 loss)
I0523 07:13:40.662124 35003 sgd_solver.cpp:112] Iteration 171070, lr = 0.001
I0523 07:13:44.727797 35003 solver.cpp:239] Iteration 171080 (2.45478 iter/s, 4.07368s/10 iters), loss = 6.88317
I0523 07:13:44.727856 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88317 (* 1 = 6.88317 loss)
I0523 07:13:44.740027 35003 sgd_solver.cpp:112] Iteration 171080, lr = 0.001
I0523 07:13:49.135466 35003 solver.cpp:239] Iteration 171090 (2.2689 iter/s, 4.40742s/10 iters), loss = 6.11978
I0523 07:13:49.135529 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11978 (* 1 = 6.11978 loss)
I0523 07:13:49.138720 35003 sgd_solver.cpp:112] Iteration 171090, lr = 0.001
I0523 07:13:52.956003 35003 solver.cpp:239] Iteration 171100 (2.6176 iter/s, 3.8203s/10 iters), loss = 6.24905
I0523 07:13:52.956053 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24905 (* 1 = 6.24905 loss)
I0523 07:13:52.969516 35003 sgd_solver.cpp:112] Iteration 171100, lr = 0.001
I0523 07:13:56.070735 35003 solver.cpp:239] Iteration 171110 (3.21076 iter/s, 3.11453s/10 iters), loss = 7.17207
I0523 07:13:56.070852 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17207 (* 1 = 7.17207 loss)
I0523 07:13:56.074883 35003 sgd_solver.cpp:112] Iteration 171110, lr = 0.001
I0523 07:13:58.790899 35003 solver.cpp:239] Iteration 171120 (3.67658 iter/s, 2.71992s/10 iters), loss = 6.12852
I0523 07:13:58.790964 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12852 (* 1 = 6.12852 loss)
I0523 07:13:58.803304 35003 sgd_solver.cpp:112] Iteration 171120, lr = 0.001
I0523 07:14:01.212880 35003 solver.cpp:239] Iteration 171130 (4.12914 iter/s, 2.42181s/10 iters), loss = 5.83033
I0523 07:14:01.212921 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.83033 (* 1 = 5.83033 loss)
I0523 07:14:01.881928 35003 sgd_solver.cpp:112] Iteration 171130, lr = 0.001
I0523 07:14:05.402325 35003 solver.cpp:239] Iteration 171140 (2.38708 iter/s, 4.18923s/10 iters), loss = 6.1757
I0523 07:14:05.402369 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1757 (* 1 = 6.1757 loss)
I0523 07:14:05.415047 35003 sgd_solver.cpp:112] Iteration 171140, lr = 0.001
I0523 07:14:08.905387 35003 solver.cpp:239] Iteration 171150 (2.8548 iter/s, 3.50287s/10 iters), loss = 6.07424
I0523 07:14:08.905431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07424 (* 1 = 6.07424 loss)
I0523 07:14:08.913276 35003 sgd_solver.cpp:112] Iteration 171150, lr = 0.001
I0523 07:14:11.636608 35003 solver.cpp:239] Iteration 171160 (3.66158 iter/s, 2.73106s/10 iters), loss = 6.79779
I0523 07:14:11.636652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79779 (* 1 = 6.79779 loss)
I0523 07:14:11.653813 35003 sgd_solver.cpp:112] Iteration 171160, lr = 0.001
I0523 07:14:14.220146 35003 solver.cpp:239] Iteration 171170 (3.87089 iter/s, 2.58338s/10 iters), loss = 6.63819
I0523 07:14:14.220188 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63819 (* 1 = 6.63819 loss)
I0523 07:14:14.236897 35003 sgd_solver.cpp:112] Iteration 171170, lr = 0.001
I0523 07:14:17.957937 35003 solver.cpp:239] Iteration 171180 (2.67552 iter/s, 3.7376s/10 iters), loss = 6.563
I0523 07:14:17.957979 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.563 (* 1 = 6.563 loss)
I0523 07:14:17.967087 35003 sgd_solver.cpp:112] Iteration 171180, lr = 0.001
I0523 07:14:20.007959 35003 solver.cpp:239] Iteration 171190 (4.87831 iter/s, 2.04989s/10 iters), loss = 7.51145
I0523 07:14:20.008002 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51145 (* 1 = 7.51145 loss)
I0523 07:14:20.015781 35003 sgd_solver.cpp:112] Iteration 171190, lr = 0.001
I0523 07:14:24.047688 35003 solver.cpp:239] Iteration 171200 (2.47554 iter/s, 4.03952s/10 iters), loss = 6.67079
I0523 07:14:24.047732 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67079 (* 1 = 6.67079 loss)
I0523 07:14:24.065971 35003 sgd_solver.cpp:112] Iteration 171200, lr = 0.001
I0523 07:14:27.848832 35003 solver.cpp:239] Iteration 171210 (2.63093 iter/s, 3.80094s/10 iters), loss = 7.64788
I0523 07:14:27.849112 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64788 (* 1 = 7.64788 loss)
I0523 07:14:28.555928 35003 sgd_solver.cpp:112] Iteration 171210, lr = 0.001
I0523 07:14:33.655822 35003 solver.cpp:239] Iteration 171220 (1.72221 iter/s, 5.80649s/10 iters), loss = 6.9645
I0523 07:14:33.655867 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9645 (* 1 = 6.9645 loss)
I0523 07:14:33.663827 35003 sgd_solver.cpp:112] Iteration 171220, lr = 0.001
I0523 07:14:36.417986 35003 solver.cpp:239] Iteration 171230 (3.62057 iter/s, 2.762s/10 iters), loss = 6.36923
I0523 07:14:36.418037 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36923 (* 1 = 6.36923 loss)
I0523 07:14:36.424077 35003 sgd_solver.cpp:112] Iteration 171230, lr = 0.001
I0523 07:14:40.777762 35003 solver.cpp:239] Iteration 171240 (2.29381 iter/s, 4.35955s/10 iters), loss = 6.58219
I0523 07:14:40.777806 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58219 (* 1 = 6.58219 loss)
I0523 07:14:40.955271 35003 sgd_solver.cpp:112] Iteration 171240, lr = 0.001
I0523 07:14:44.547660 35003 solver.cpp:239] Iteration 171250 (2.65273 iter/s, 3.7697s/10 iters), loss = 6.7923
I0523 07:14:44.547711 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7923 (* 1 = 6.7923 loss)
I0523 07:14:45.198210 35003 sgd_solver.cpp:112] Iteration 171250, lr = 0.001
I0523 07:14:47.236464 35003 solver.cpp:239] Iteration 171260 (3.71936 iter/s, 2.68863s/10 iters), loss = 7.19264
I0523 07:14:47.236513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19264 (* 1 = 7.19264 loss)
I0523 07:14:47.249454 35003 sgd_solver.cpp:112] Iteration 171260, lr = 0.001
I0523 07:14:50.238335 35003 solver.cpp:239] Iteration 171270 (3.33145 iter/s, 3.0017s/10 iters), loss = 7.20722
I0523 07:14:50.238380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20722 (* 1 = 7.20722 loss)
I0523 07:14:50.946331 35003 sgd_solver.cpp:112] Iteration 171270, lr = 0.001
I0523 07:14:53.775499 35003 solver.cpp:239] Iteration 171280 (2.82728 iter/s, 3.53697s/10 iters), loss = 7.36988
I0523 07:14:53.775544 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36988 (* 1 = 7.36988 loss)
I0523 07:14:53.782006 35003 sgd_solver.cpp:112] Iteration 171280, lr = 0.001
I0523 07:14:57.512338 35003 solver.cpp:239] Iteration 171290 (2.67621 iter/s, 3.73663s/10 iters), loss = 5.82006
I0523 07:14:57.512399 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.82006 (* 1 = 5.82006 loss)
I0523 07:14:58.100436 35003 sgd_solver.cpp:112] Iteration 171290, lr = 0.001
I0523 07:15:01.174216 35003 solver.cpp:239] Iteration 171300 (2.731 iter/s, 3.66166s/10 iters), loss = 7.39071
I0523 07:15:01.174259 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39071 (* 1 = 7.39071 loss)
I0523 07:15:01.187638 35003 sgd_solver.cpp:112] Iteration 171300, lr = 0.001
I0523 07:15:03.931620 35003 solver.cpp:239] Iteration 171310 (3.62681 iter/s, 2.75724s/10 iters), loss = 5.91419
I0523 07:15:03.931668 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91419 (* 1 = 5.91419 loss)
I0523 07:15:03.936856 35003 sgd_solver.cpp:112] Iteration 171310, lr = 0.001
I0523 07:15:09.187201 35003 solver.cpp:239] Iteration 171320 (1.90283 iter/s, 5.25532s/10 iters), loss = 7.21668
I0523 07:15:09.187247 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21668 (* 1 = 7.21668 loss)
I0523 07:15:09.205144 35003 sgd_solver.cpp:112] Iteration 171320, lr = 0.001
I0523 07:15:10.506260 35003 solver.cpp:239] Iteration 171330 (7.5818 iter/s, 1.31895s/10 iters), loss = 6.86486
I0523 07:15:10.506305 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86486 (* 1 = 6.86486 loss)
I0523 07:15:10.519914 35003 sgd_solver.cpp:112] Iteration 171330, lr = 0.001
I0523 07:15:13.008301 35003 solver.cpp:239] Iteration 171340 (3.99699 iter/s, 2.50188s/10 iters), loss = 6.6544
I0523 07:15:13.008345 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6544 (* 1 = 6.6544 loss)
I0523 07:15:13.717362 35003 sgd_solver.cpp:112] Iteration 171340, lr = 0.001
I0523 07:15:16.578909 35003 solver.cpp:239] Iteration 171350 (2.8008 iter/s, 3.57041s/10 iters), loss = 5.76235
I0523 07:15:16.578953 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.76235 (* 1 = 5.76235 loss)
I0523 07:15:16.597537 35003 sgd_solver.cpp:112] Iteration 171350, lr = 0.001
I0523 07:15:21.921996 35003 solver.cpp:239] Iteration 171360 (1.87167 iter/s, 5.34282s/10 iters), loss = 6.60709
I0523 07:15:21.922053 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60709 (* 1 = 6.60709 loss)
I0523 07:15:22.110460 35003 sgd_solver.cpp:112] Iteration 171360, lr = 0.001
I0523 07:15:25.640993 35003 solver.cpp:239] Iteration 171370 (2.68905 iter/s, 3.71879s/10 iters), loss = 6.99306
I0523 07:15:25.641047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99306 (* 1 = 6.99306 loss)
I0523 07:15:26.371960 35003 sgd_solver.cpp:112] Iteration 171370, lr = 0.001
I0523 07:15:30.009125 35003 solver.cpp:239] Iteration 171380 (2.28943 iter/s, 4.3679s/10 iters), loss = 6.52623
I0523 07:15:30.009372 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52623 (* 1 = 6.52623 loss)
I0523 07:15:30.016060 35003 sgd_solver.cpp:112] Iteration 171380, lr = 0.001
I0523 07:15:34.287462 35003 solver.cpp:239] Iteration 171390 (2.33758 iter/s, 4.27792s/10 iters), loss = 7.45922
I0523 07:15:34.287508 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45922 (* 1 = 7.45922 loss)
I0523 07:15:34.331977 35003 sgd_solver.cpp:112] Iteration 171390, lr = 0.001
I0523 07:15:37.939208 35003 solver.cpp:239] Iteration 171400 (2.73857 iter/s, 3.65154s/10 iters), loss = 7.53557
I0523 07:15:37.939262 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53557 (* 1 = 7.53557 loss)
I0523 07:15:37.946568 35003 sgd_solver.cpp:112] Iteration 171400, lr = 0.001
I0523 07:15:39.259274 35003 solver.cpp:239] Iteration 171410 (7.57604 iter/s, 1.31995s/10 iters), loss = 6.61746
I0523 07:15:39.259315 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61746 (* 1 = 6.61746 loss)
I0523 07:15:39.286485 35003 sgd_solver.cpp:112] Iteration 171410, lr = 0.001
I0523 07:15:42.209684 35003 solver.cpp:239] Iteration 171420 (3.38956 iter/s, 2.95023s/10 iters), loss = 6.04388
I0523 07:15:42.209731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04388 (* 1 = 6.04388 loss)
I0523 07:15:42.760484 35003 sgd_solver.cpp:112] Iteration 171420, lr = 0.001
I0523 07:15:47.139295 35003 solver.cpp:239] Iteration 171430 (2.02867 iter/s, 4.92934s/10 iters), loss = 7.39994
I0523 07:15:47.139343 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39994 (* 1 = 7.39994 loss)
I0523 07:15:47.150671 35003 sgd_solver.cpp:112] Iteration 171430, lr = 0.001
I0523 07:15:49.250607 35003 solver.cpp:239] Iteration 171440 (4.73673 iter/s, 2.11116s/10 iters), loss = 5.7935
I0523 07:15:49.250666 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7935 (* 1 = 5.7935 loss)
I0523 07:15:49.263403 35003 sgd_solver.cpp:112] Iteration 171440, lr = 0.001
I0523 07:15:54.473196 35003 solver.cpp:239] Iteration 171450 (1.91486 iter/s, 5.22232s/10 iters), loss = 7.1232
I0523 07:15:54.473249 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1232 (* 1 = 7.1232 loss)
I0523 07:15:54.498122 35003 sgd_solver.cpp:112] Iteration 171450, lr = 0.001
I0523 07:15:58.204309 35003 solver.cpp:239] Iteration 171460 (2.68031 iter/s, 3.73091s/10 iters), loss = 7.68924
I0523 07:15:58.204360 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.68924 (* 1 = 7.68924 loss)
I0523 07:15:58.210122 35003 sgd_solver.cpp:112] Iteration 171460, lr = 0.001
I0523 07:16:01.659097 35003 solver.cpp:239] Iteration 171470 (2.8947 iter/s, 3.45459s/10 iters), loss = 7.05475
I0523 07:16:01.659286 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05475 (* 1 = 7.05475 loss)
I0523 07:16:02.391085 35003 sgd_solver.cpp:112] Iteration 171470, lr = 0.001
I0523 07:16:05.348667 35003 solver.cpp:239] Iteration 171480 (2.71059 iter/s, 3.68923s/10 iters), loss = 6.49983
I0523 07:16:05.348712 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49983 (* 1 = 6.49983 loss)
I0523 07:16:05.360539 35003 sgd_solver.cpp:112] Iteration 171480, lr = 0.001
I0523 07:16:09.623999 35003 solver.cpp:239] Iteration 171490 (2.33912 iter/s, 4.27511s/10 iters), loss = 7.11458
I0523 07:16:09.624043 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11458 (* 1 = 7.11458 loss)
I0523 07:16:09.637331 35003 sgd_solver.cpp:112] Iteration 171490, lr = 0.001
I0523 07:16:13.329821 35003 solver.cpp:239] Iteration 171500 (2.6986 iter/s, 3.70562s/10 iters), loss = 7.67698
I0523 07:16:13.329865 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67698 (* 1 = 7.67698 loss)
I0523 07:16:13.337527 35003 sgd_solver.cpp:112] Iteration 171500, lr = 0.001
I0523 07:16:15.409883 35003 solver.cpp:239] Iteration 171510 (4.80787 iter/s, 2.07993s/10 iters), loss = 7.51028
I0523 07:16:15.409926 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51028 (* 1 = 7.51028 loss)
I0523 07:16:16.039861 35003 sgd_solver.cpp:112] Iteration 171510, lr = 0.001
I0523 07:16:18.566975 35003 solver.cpp:239] Iteration 171520 (3.16765 iter/s, 3.15692s/10 iters), loss = 6.90682
I0523 07:16:18.567019 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90682 (* 1 = 6.90682 loss)
I0523 07:16:18.585194 35003 sgd_solver.cpp:112] Iteration 171520, lr = 0.001
I0523 07:16:21.713907 35003 solver.cpp:239] Iteration 171530 (3.17788 iter/s, 3.14676s/10 iters), loss = 6.59634
I0523 07:16:21.713954 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59634 (* 1 = 6.59634 loss)
I0523 07:16:21.726812 35003 sgd_solver.cpp:112] Iteration 171530, lr = 0.001
I0523 07:16:26.046785 35003 solver.cpp:239] Iteration 171540 (2.30806 iter/s, 4.33264s/10 iters), loss = 6.60367
I0523 07:16:26.046854 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60367 (* 1 = 6.60367 loss)
I0523 07:16:26.762235 35003 sgd_solver.cpp:112] Iteration 171540, lr = 0.001
I0523 07:16:31.173336 35003 solver.cpp:239] Iteration 171550 (1.95074 iter/s, 5.12626s/10 iters), loss = 6.82221
I0523 07:16:31.173382 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82221 (* 1 = 6.82221 loss)
I0523 07:16:31.180697 35003 sgd_solver.cpp:112] Iteration 171550, lr = 0.001
I0523 07:16:34.581110 35003 solver.cpp:239] Iteration 171560 (2.93463 iter/s, 3.40758s/10 iters), loss = 6.05789
I0523 07:16:34.581321 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05789 (* 1 = 6.05789 loss)
I0523 07:16:34.601893 35003 sgd_solver.cpp:112] Iteration 171560, lr = 0.001
I0523 07:16:37.868696 35003 solver.cpp:239] Iteration 171570 (3.04207 iter/s, 3.28723s/10 iters), loss = 6.19794
I0523 07:16:37.868755 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19794 (* 1 = 6.19794 loss)
I0523 07:16:37.891399 35003 sgd_solver.cpp:112] Iteration 171570, lr = 0.001
I0523 07:16:43.017809 35003 solver.cpp:239] Iteration 171580 (1.9422 iter/s, 5.14881s/10 iters), loss = 7.01108
I0523 07:16:43.017856 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01108 (* 1 = 7.01108 loss)
I0523 07:16:43.033934 35003 sgd_solver.cpp:112] Iteration 171580, lr = 0.001
I0523 07:16:47.271765 35003 solver.cpp:239] Iteration 171590 (2.35088 iter/s, 4.25373s/10 iters), loss = 7.31187
I0523 07:16:47.271818 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31187 (* 1 = 7.31187 loss)
I0523 07:16:47.384995 35003 sgd_solver.cpp:112] Iteration 171590, lr = 0.001
I0523 07:16:50.073037 35003 solver.cpp:239] Iteration 171600 (3.57003 iter/s, 2.8011s/10 iters), loss = 6.69965
I0523 07:16:50.073084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69965 (* 1 = 6.69965 loss)
I0523 07:16:50.080453 35003 sgd_solver.cpp:112] Iteration 171600, lr = 0.001
I0523 07:16:53.767385 35003 solver.cpp:239] Iteration 171610 (2.70698 iter/s, 3.69415s/10 iters), loss = 6.45189
I0523 07:16:53.767431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45189 (* 1 = 6.45189 loss)
I0523 07:16:53.775375 35003 sgd_solver.cpp:112] Iteration 171610, lr = 0.001
I0523 07:16:56.817807 35003 solver.cpp:239] Iteration 171620 (3.27844 iter/s, 3.05024s/10 iters), loss = 5.74326
I0523 07:16:56.817859 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74326 (* 1 = 5.74326 loss)
I0523 07:16:56.850785 35003 sgd_solver.cpp:112] Iteration 171620, lr = 0.001
I0523 07:17:01.135107 35003 solver.cpp:239] Iteration 171630 (2.31638 iter/s, 4.31707s/10 iters), loss = 6.33633
I0523 07:17:01.135148 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33633 (* 1 = 6.33633 loss)
I0523 07:17:01.144754 35003 sgd_solver.cpp:112] Iteration 171630, lr = 0.001
I0523 07:17:05.312546 35003 solver.cpp:239] Iteration 171640 (2.39394 iter/s, 4.17722s/10 iters), loss = 5.83196
I0523 07:17:05.312685 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.83196 (* 1 = 5.83196 loss)
I0523 07:17:05.324327 35003 sgd_solver.cpp:112] Iteration 171640, lr = 0.001
I0523 07:17:10.622457 35003 solver.cpp:239] Iteration 171650 (1.8834 iter/s, 5.30956s/10 iters), loss = 6.66543
I0523 07:17:10.622503 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66543 (* 1 = 6.66543 loss)
I0523 07:17:10.635534 35003 sgd_solver.cpp:112] Iteration 171650, lr = 0.001
I0523 07:17:13.494976 35003 solver.cpp:239] Iteration 171660 (3.48146 iter/s, 2.87236s/10 iters), loss = 6.28414
I0523 07:17:13.495012 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28414 (* 1 = 6.28414 loss)
I0523 07:17:13.507642 35003 sgd_solver.cpp:112] Iteration 171660, lr = 0.001
I0523 07:17:16.996078 35003 solver.cpp:239] Iteration 171670 (2.8564 iter/s, 3.50092s/10 iters), loss = 6.30121
I0523 07:17:16.996119 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30121 (* 1 = 6.30121 loss)
I0523 07:17:17.009660 35003 sgd_solver.cpp:112] Iteration 171670, lr = 0.001
I0523 07:17:21.291569 35003 solver.cpp:239] Iteration 171680 (2.32814 iter/s, 4.29527s/10 iters), loss = 7.083
I0523 07:17:21.291625 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.083 (* 1 = 7.083 loss)
I0523 07:17:21.998103 35003 sgd_solver.cpp:112] Iteration 171680, lr = 0.001
I0523 07:17:24.340163 35003 solver.cpp:239] Iteration 171690 (3.2804 iter/s, 3.04841s/10 iters), loss = 6.43322
I0523 07:17:24.340205 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43322 (* 1 = 6.43322 loss)
I0523 07:17:24.353531 35003 sgd_solver.cpp:112] Iteration 171690, lr = 0.001
I0523 07:17:27.246125 35003 solver.cpp:239] Iteration 171700 (3.44141 iter/s, 2.90578s/10 iters), loss = 6.19652
I0523 07:17:27.246196 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19652 (* 1 = 6.19652 loss)
I0523 07:17:27.249233 35003 sgd_solver.cpp:112] Iteration 171700, lr = 0.001
I0523 07:17:30.344802 35003 solver.cpp:239] Iteration 171710 (3.2274 iter/s, 3.09847s/10 iters), loss = 6.72513
I0523 07:17:30.344859 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72513 (* 1 = 6.72513 loss)
I0523 07:17:30.613729 35003 sgd_solver.cpp:112] Iteration 171710, lr = 0.001
I0523 07:17:34.882879 35003 solver.cpp:239] Iteration 171720 (2.20369 iter/s, 4.53784s/10 iters), loss = 6.75084
I0523 07:17:34.882918 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75084 (* 1 = 6.75084 loss)
I0523 07:17:34.889298 35003 sgd_solver.cpp:112] Iteration 171720, lr = 0.001
I0523 07:17:39.307070 35003 solver.cpp:239] Iteration 171730 (2.26041 iter/s, 4.42397s/10 iters), loss = 6.39375
I0523 07:17:39.307260 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39375 (* 1 = 6.39375 loss)
I0523 07:17:39.313118 35003 sgd_solver.cpp:112] Iteration 171730, lr = 0.001
I0523 07:17:42.196630 35003 solver.cpp:239] Iteration 171740 (3.4611 iter/s, 2.88925s/10 iters), loss = 5.69392
I0523 07:17:42.196671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.69392 (* 1 = 5.69392 loss)
I0523 07:17:42.918154 35003 sgd_solver.cpp:112] Iteration 171740, lr = 0.001
I0523 07:17:45.557564 35003 solver.cpp:239] Iteration 171750 (2.97552 iter/s, 3.36075s/10 iters), loss = 7.4285
I0523 07:17:45.557603 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4285 (* 1 = 7.4285 loss)
I0523 07:17:45.576011 35003 sgd_solver.cpp:112] Iteration 171750, lr = 0.001
I0523 07:17:49.149389 35003 solver.cpp:239] Iteration 171760 (2.78425 iter/s, 3.59163s/10 iters), loss = 6.72025
I0523 07:17:49.149433 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72025 (* 1 = 6.72025 loss)
I0523 07:17:49.162533 35003 sgd_solver.cpp:112] Iteration 171760, lr = 0.001
I0523 07:17:52.626333 35003 solver.cpp:239] Iteration 171770 (2.87625 iter/s, 3.47675s/10 iters), loss = 6.26037
I0523 07:17:52.626381 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26037 (* 1 = 6.26037 loss)
I0523 07:17:52.639863 35003 sgd_solver.cpp:112] Iteration 171770, lr = 0.001
I0523 07:17:55.525661 35003 solver.cpp:239] Iteration 171780 (3.44928 iter/s, 2.89916s/10 iters), loss = 5.92277
I0523 07:17:55.525707 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92277 (* 1 = 5.92277 loss)
I0523 07:17:55.544013 35003 sgd_solver.cpp:112] Iteration 171780, lr = 0.001
I0523 07:17:57.659827 35003 solver.cpp:239] Iteration 171790 (4.68599 iter/s, 2.13402s/10 iters), loss = 7.18918
I0523 07:17:57.659884 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18918 (* 1 = 7.18918 loss)
I0523 07:17:58.400418 35003 sgd_solver.cpp:112] Iteration 171790, lr = 0.001
I0523 07:18:00.470808 35003 solver.cpp:239] Iteration 171800 (3.5577 iter/s, 2.81081s/10 iters), loss = 7.29494
I0523 07:18:00.470850 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29494 (* 1 = 7.29494 loss)
I0523 07:18:01.204249 35003 sgd_solver.cpp:112] Iteration 171800, lr = 0.001
I0523 07:18:03.239084 35003 solver.cpp:239] Iteration 171810 (3.61257 iter/s, 2.76811s/10 iters), loss = 5.32369
I0523 07:18:03.239135 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.32369 (* 1 = 5.32369 loss)
I0523 07:18:03.245324 35003 sgd_solver.cpp:112] Iteration 171810, lr = 0.001
I0523 07:18:05.947585 35003 solver.cpp:239] Iteration 171820 (3.69231 iter/s, 2.70833s/10 iters), loss = 7.43696
I0523 07:18:05.947631 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43696 (* 1 = 7.43696 loss)
I0523 07:18:05.964134 35003 sgd_solver.cpp:112] Iteration 171820, lr = 0.001
I0523 07:18:06.938508 35003 solver.cpp:239] Iteration 171830 (10.0927 iter/s, 0.990817s/10 iters), loss = 5.85554
I0523 07:18:06.938552 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85554 (* 1 = 5.85554 loss)
I0523 07:18:06.947739 35003 sgd_solver.cpp:112] Iteration 171830, lr = 0.001
I0523 07:18:07.768149 35003 solver.cpp:239] Iteration 171840 (12.0548 iter/s, 0.829542s/10 iters), loss = 7.26348
I0523 07:18:07.768201 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26348 (* 1 = 7.26348 loss)
I0523 07:18:07.780432 35003 sgd_solver.cpp:112] Iteration 171840, lr = 0.001
I0523 07:18:11.951769 35003 solver.cpp:239] Iteration 171850 (2.3904 iter/s, 4.1834s/10 iters), loss = 5.991
I0523 07:18:11.952018 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.991 (* 1 = 5.991 loss)
I0523 07:18:11.964830 35003 sgd_solver.cpp:112] Iteration 171850, lr = 0.001
I0523 07:18:14.811718 35003 solver.cpp:239] Iteration 171860 (3.49702 iter/s, 2.85958s/10 iters), loss = 6.88052
I0523 07:18:14.811760 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88052 (* 1 = 6.88052 loss)
I0523 07:18:14.824367 35003 sgd_solver.cpp:112] Iteration 171860, lr = 0.001
I0523 07:18:18.381083 35003 solver.cpp:239] Iteration 171870 (2.80177 iter/s, 3.56917s/10 iters), loss = 5.84001
I0523 07:18:18.381121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84001 (* 1 = 5.84001 loss)
I0523 07:18:19.047581 35003 sgd_solver.cpp:112] Iteration 171870, lr = 0.001
I0523 07:18:22.641391 35003 solver.cpp:239] Iteration 171880 (2.34737 iter/s, 4.26009s/10 iters), loss = 6.95633
I0523 07:18:22.641440 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95633 (* 1 = 6.95633 loss)
I0523 07:18:22.650565 35003 sgd_solver.cpp:112] Iteration 171880, lr = 0.001
I0523 07:18:26.092273 35003 solver.cpp:239] Iteration 171890 (2.89797 iter/s, 3.45069s/10 iters), loss = 6.71854
I0523 07:18:26.092319 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71854 (* 1 = 6.71854 loss)
I0523 07:18:26.141789 35003 sgd_solver.cpp:112] Iteration 171890, lr = 0.001
I0523 07:18:30.625362 35003 solver.cpp:239] Iteration 171900 (2.20611 iter/s, 4.53286s/10 iters), loss = 6.94992
I0523 07:18:30.625403 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94992 (* 1 = 6.94992 loss)
I0523 07:18:31.366147 35003 sgd_solver.cpp:112] Iteration 171900, lr = 0.001
I0523 07:18:35.495096 35003 solver.cpp:239] Iteration 171910 (2.0536 iter/s, 4.86949s/10 iters), loss = 6.90957
I0523 07:18:35.495148 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90957 (* 1 = 6.90957 loss)
I0523 07:18:35.519348 35003 sgd_solver.cpp:112] Iteration 171910, lr = 0.001
I0523 07:18:38.250208 35003 solver.cpp:239] Iteration 171920 (3.62984 iter/s, 2.75494s/10 iters), loss = 7.52086
I0523 07:18:38.250265 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52086 (* 1 = 7.52086 loss)
I0523 07:18:38.262743 35003 sgd_solver.cpp:112] Iteration 171920, lr = 0.001
I0523 07:18:41.755756 35003 solver.cpp:239] Iteration 171930 (2.85278 iter/s, 3.50535s/10 iters), loss = 6.54755
I0523 07:18:41.755795 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54755 (* 1 = 6.54755 loss)
I0523 07:18:42.290527 35003 sgd_solver.cpp:112] Iteration 171930, lr = 0.001
I0523 07:18:45.842325 35003 solver.cpp:239] Iteration 171940 (2.44717 iter/s, 4.08635s/10 iters), loss = 7.55345
I0523 07:18:45.842384 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55345 (* 1 = 7.55345 loss)
I0523 07:18:45.855353 35003 sgd_solver.cpp:112] Iteration 171940, lr = 0.001
I0523 07:18:49.365783 35003 solver.cpp:239] Iteration 171950 (2.83828 iter/s, 3.52326s/10 iters), loss = 7.71348
I0523 07:18:49.365818 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71348 (* 1 = 7.71348 loss)
I0523 07:18:50.081022 35003 sgd_solver.cpp:112] Iteration 171950, lr = 0.001
I0523 07:18:53.876782 35003 solver.cpp:239] Iteration 171960 (2.21692 iter/s, 4.51076s/10 iters), loss = 7.64446
I0523 07:18:53.876821 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64446 (* 1 = 7.64446 loss)
I0523 07:18:53.884304 35003 sgd_solver.cpp:112] Iteration 171960, lr = 0.001
I0523 07:18:56.727716 35003 solver.cpp:239] Iteration 171970 (3.50782 iter/s, 2.85077s/10 iters), loss = 6.24015
I0523 07:18:56.727768 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24015 (* 1 = 6.24015 loss)
I0523 07:18:56.737139 35003 sgd_solver.cpp:112] Iteration 171970, lr = 0.001
I0523 07:19:01.742740 35003 solver.cpp:239] Iteration 171980 (1.99411 iter/s, 5.01477s/10 iters), loss = 6.33452
I0523 07:19:01.742786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33452 (* 1 = 6.33452 loss)
I0523 07:19:01.755373 35003 sgd_solver.cpp:112] Iteration 171980, lr = 0.001
I0523 07:19:05.472164 35003 solver.cpp:239] Iteration 171990 (2.68153 iter/s, 3.72922s/10 iters), loss = 6.92541
I0523 07:19:05.472223 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92541 (* 1 = 6.92541 loss)
I0523 07:19:06.187656 35003 sgd_solver.cpp:112] Iteration 171990, lr = 0.001
I0523 07:19:08.256984 35003 solver.cpp:239] Iteration 172000 (3.59112 iter/s, 2.78465s/10 iters), loss = 5.66812
I0523 07:19:08.257026 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.66812 (* 1 = 5.66812 loss)
I0523 07:19:08.270566 35003 sgd_solver.cpp:112] Iteration 172000, lr = 0.001
I0523 07:19:12.928197 35003 solver.cpp:239] Iteration 172010 (2.14088 iter/s, 4.67098s/10 iters), loss = 6.0448
I0523 07:19:12.928504 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0448 (* 1 = 6.0448 loss)
I0523 07:19:12.941067 35003 sgd_solver.cpp:112] Iteration 172010, lr = 0.001
I0523 07:19:17.828930 35003 solver.cpp:239] Iteration 172020 (2.04071 iter/s, 4.90024s/10 iters), loss = 7.26601
I0523 07:19:17.828986 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26601 (* 1 = 7.26601 loss)
I0523 07:19:18.065551 35003 sgd_solver.cpp:112] Iteration 172020, lr = 0.001
I0523 07:19:21.660346 35003 solver.cpp:239] Iteration 172030 (2.61015 iter/s, 3.8312s/10 iters), loss = 6.51361
I0523 07:19:21.660400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51361 (* 1 = 6.51361 loss)
I0523 07:19:21.673372 35003 sgd_solver.cpp:112] Iteration 172030, lr = 0.001
I0523 07:19:25.161445 35003 solver.cpp:239] Iteration 172040 (2.85641 iter/s, 3.5009s/10 iters), loss = 6.15586
I0523 07:19:25.161484 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15586 (* 1 = 6.15586 loss)
I0523 07:19:25.166798 35003 sgd_solver.cpp:112] Iteration 172040, lr = 0.001
I0523 07:19:27.870502 35003 solver.cpp:239] Iteration 172050 (3.69153 iter/s, 2.7089s/10 iters), loss = 5.65374
I0523 07:19:27.870553 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.65374 (* 1 = 5.65374 loss)
I0523 07:19:27.883780 35003 sgd_solver.cpp:112] Iteration 172050, lr = 0.001
I0523 07:19:30.525142 35003 solver.cpp:239] Iteration 172060 (3.76722 iter/s, 2.65448s/10 iters), loss = 6.83302
I0523 07:19:30.525194 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83302 (* 1 = 6.83302 loss)
I0523 07:19:30.529765 35003 sgd_solver.cpp:112] Iteration 172060, lr = 0.001
I0523 07:19:34.056862 35003 solver.cpp:239] Iteration 172070 (2.83165 iter/s, 3.53151s/10 iters), loss = 5.8104
I0523 07:19:34.056919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8104 (* 1 = 5.8104 loss)
I0523 07:19:34.082347 35003 sgd_solver.cpp:112] Iteration 172070, lr = 0.001
I0523 07:19:36.180850 35003 solver.cpp:239] Iteration 172080 (4.70846 iter/s, 2.12384s/10 iters), loss = 6.12975
I0523 07:19:36.180896 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12975 (* 1 = 6.12975 loss)
I0523 07:19:36.185528 35003 sgd_solver.cpp:112] Iteration 172080, lr = 0.001
I0523 07:19:38.943368 35003 solver.cpp:239] Iteration 172090 (3.6201 iter/s, 2.76235s/10 iters), loss = 5.61654
I0523 07:19:38.943408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.61654 (* 1 = 5.61654 loss)
I0523 07:19:38.956631 35003 sgd_solver.cpp:112] Iteration 172090, lr = 0.001
I0523 07:19:42.804198 35003 solver.cpp:239] Iteration 172100 (2.59026 iter/s, 3.86062s/10 iters), loss = 5.96758
I0523 07:19:42.804249 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96758 (* 1 = 5.96758 loss)
I0523 07:19:42.811794 35003 sgd_solver.cpp:112] Iteration 172100, lr = 0.001
I0523 07:19:45.650265 35003 solver.cpp:239] Iteration 172110 (3.51383 iter/s, 2.8459s/10 iters), loss = 7.46415
I0523 07:19:45.650533 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46415 (* 1 = 7.46415 loss)
I0523 07:19:45.656330 35003 sgd_solver.cpp:112] Iteration 172110, lr = 0.001
I0523 07:19:47.874243 35003 solver.cpp:239] Iteration 172120 (4.49713 iter/s, 2.22364s/10 iters), loss = 6.57378
I0523 07:19:47.874289 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57378 (* 1 = 6.57378 loss)
I0523 07:19:47.889598 35003 sgd_solver.cpp:112] Iteration 172120, lr = 0.001
I0523 07:19:50.050678 35003 solver.cpp:239] Iteration 172130 (4.59497 iter/s, 2.17629s/10 iters), loss = 6.25125
I0523 07:19:50.050751 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25125 (* 1 = 6.25125 loss)
I0523 07:19:50.501575 35003 sgd_solver.cpp:112] Iteration 172130, lr = 0.001
I0523 07:19:53.217636 35003 solver.cpp:239] Iteration 172140 (3.15781 iter/s, 3.16675s/10 iters), loss = 6.10161
I0523 07:19:53.217679 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10161 (* 1 = 6.10161 loss)
I0523 07:19:53.228747 35003 sgd_solver.cpp:112] Iteration 172140, lr = 0.001
I0523 07:19:56.116696 35003 solver.cpp:239] Iteration 172150 (3.44959 iter/s, 2.89889s/10 iters), loss = 6.33814
I0523 07:19:56.116739 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33814 (* 1 = 6.33814 loss)
I0523 07:19:56.124608 35003 sgd_solver.cpp:112] Iteration 172150, lr = 0.001
I0523 07:20:00.950294 35003 solver.cpp:239] Iteration 172160 (2.06896 iter/s, 4.83335s/10 iters), loss = 6.84727
I0523 07:20:00.950335 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84727 (* 1 = 6.84727 loss)
I0523 07:20:00.963582 35003 sgd_solver.cpp:112] Iteration 172160, lr = 0.001
I0523 07:20:05.256458 35003 solver.cpp:239] Iteration 172170 (2.32237 iter/s, 4.30594s/10 iters), loss = 6.19565
I0523 07:20:05.256503 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19565 (* 1 = 6.19565 loss)
I0523 07:20:05.977922 35003 sgd_solver.cpp:112] Iteration 172170, lr = 0.001
I0523 07:20:09.501925 35003 solver.cpp:239] Iteration 172180 (2.35557 iter/s, 4.24525s/10 iters), loss = 6.84401
I0523 07:20:09.501971 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84401 (* 1 = 6.84401 loss)
I0523 07:20:10.230598 35003 sgd_solver.cpp:112] Iteration 172180, lr = 0.001
I0523 07:20:13.770602 35003 solver.cpp:239] Iteration 172190 (2.34277 iter/s, 4.26845s/10 iters), loss = 7.03941
I0523 07:20:13.770658 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03941 (* 1 = 7.03941 loss)
I0523 07:20:13.778445 35003 sgd_solver.cpp:112] Iteration 172190, lr = 0.001
I0523 07:20:18.688886 35003 solver.cpp:239] Iteration 172200 (2.03333 iter/s, 4.91803s/10 iters), loss = 6.6683
I0523 07:20:18.689076 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6683 (* 1 = 6.6683 loss)
I0523 07:20:18.711527 35003 sgd_solver.cpp:112] Iteration 172200, lr = 0.001
I0523 07:20:21.481436 35003 solver.cpp:239] Iteration 172210 (3.58133 iter/s, 2.79226s/10 iters), loss = 6.60815
I0523 07:20:21.481478 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60815 (* 1 = 6.60815 loss)
I0523 07:20:22.103847 35003 sgd_solver.cpp:112] Iteration 172210, lr = 0.001
I0523 07:20:25.350044 35003 solver.cpp:239] Iteration 172220 (2.58525 iter/s, 3.86809s/10 iters), loss = 5.8769
I0523 07:20:25.350092 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8769 (* 1 = 5.8769 loss)
I0523 07:20:25.519485 35003 sgd_solver.cpp:112] Iteration 172220, lr = 0.001
I0523 07:20:30.221287 35003 solver.cpp:239] Iteration 172230 (2.05297 iter/s, 4.871s/10 iters), loss = 5.54715
I0523 07:20:30.221324 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.54715 (* 1 = 5.54715 loss)
I0523 07:20:30.226609 35003 sgd_solver.cpp:112] Iteration 172230, lr = 0.001
I0523 07:20:33.096531 35003 solver.cpp:239] Iteration 172240 (3.47816 iter/s, 2.87508s/10 iters), loss = 6.15717
I0523 07:20:33.096573 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15717 (* 1 = 6.15717 loss)
I0523 07:20:33.102704 35003 sgd_solver.cpp:112] Iteration 172240, lr = 0.001
I0523 07:20:37.937144 35003 solver.cpp:239] Iteration 172250 (2.06596 iter/s, 4.84037s/10 iters), loss = 6.5773
I0523 07:20:37.937189 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5773 (* 1 = 6.5773 loss)
I0523 07:20:37.950126 35003 sgd_solver.cpp:112] Iteration 172250, lr = 0.001
I0523 07:20:42.534034 35003 solver.cpp:239] Iteration 172260 (2.17549 iter/s, 4.59666s/10 iters), loss = 6.61678
I0523 07:20:42.534080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61678 (* 1 = 6.61678 loss)
I0523 07:20:42.540925 35003 sgd_solver.cpp:112] Iteration 172260, lr = 0.001
I0523 07:20:46.178226 35003 solver.cpp:239] Iteration 172270 (2.74424 iter/s, 3.64399s/10 iters), loss = 6.2957
I0523 07:20:46.178274 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2957 (* 1 = 6.2957 loss)
I0523 07:20:46.307579 35003 sgd_solver.cpp:112] Iteration 172270, lr = 0.001
I0523 07:20:48.465642 35003 solver.cpp:239] Iteration 172280 (4.37206 iter/s, 2.28725s/10 iters), loss = 8.08974
I0523 07:20:48.465706 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08974 (* 1 = 8.08974 loss)
I0523 07:20:49.206363 35003 sgd_solver.cpp:112] Iteration 172280, lr = 0.001
I0523 07:20:51.949095 35003 solver.cpp:239] Iteration 172290 (2.8709 iter/s, 3.48322s/10 iters), loss = 6.5939
I0523 07:20:51.949148 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5939 (* 1 = 6.5939 loss)
I0523 07:20:52.670585 35003 sgd_solver.cpp:112] Iteration 172290, lr = 0.001
I0523 07:20:54.728004 35003 solver.cpp:239] Iteration 172300 (3.59876 iter/s, 2.77873s/10 iters), loss = 5.98138
I0523 07:20:54.728051 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98138 (* 1 = 5.98138 loss)
I0523 07:20:54.740958 35003 sgd_solver.cpp:112] Iteration 172300, lr = 0.001
I0523 07:20:57.453233 35003 solver.cpp:239] Iteration 172310 (3.66964 iter/s, 2.72507s/10 iters), loss = 6.36363
I0523 07:20:57.453274 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36363 (* 1 = 6.36363 loss)
I0523 07:20:58.116595 35003 sgd_solver.cpp:112] Iteration 172310, lr = 0.001
I0523 07:21:03.259688 35003 solver.cpp:239] Iteration 172320 (1.7223 iter/s, 5.80618s/10 iters), loss = 5.55373
I0523 07:21:03.259732 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.55373 (* 1 = 5.55373 loss)
I0523 07:21:03.298499 35003 sgd_solver.cpp:112] Iteration 172320, lr = 0.001
I0523 07:21:07.204622 35003 solver.cpp:239] Iteration 172330 (2.53503 iter/s, 3.94473s/10 iters), loss = 7.58641
I0523 07:21:07.204668 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58641 (* 1 = 7.58641 loss)
I0523 07:21:07.218006 35003 sgd_solver.cpp:112] Iteration 172330, lr = 0.001
I0523 07:21:10.066926 35003 solver.cpp:239] Iteration 172340 (3.4939 iter/s, 2.86213s/10 iters), loss = 7.17496
I0523 07:21:10.066977 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17496 (* 1 = 7.17496 loss)
I0523 07:21:10.080194 35003 sgd_solver.cpp:112] Iteration 172340, lr = 0.001
I0523 07:21:12.155910 35003 solver.cpp:239] Iteration 172350 (4.78734 iter/s, 2.08884s/10 iters), loss = 6.7447
I0523 07:21:12.155951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7447 (* 1 = 6.7447 loss)
I0523 07:21:12.831250 35003 sgd_solver.cpp:112] Iteration 172350, lr = 0.001
I0523 07:21:15.746997 35003 solver.cpp:239] Iteration 172360 (2.78483 iter/s, 3.59089s/10 iters), loss = 7.03829
I0523 07:21:15.747041 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03829 (* 1 = 7.03829 loss)
I0523 07:21:16.488539 35003 sgd_solver.cpp:112] Iteration 172360, lr = 0.001
I0523 07:21:19.358057 35003 solver.cpp:239] Iteration 172370 (2.76943 iter/s, 3.61085s/10 iters), loss = 6.84865
I0523 07:21:19.358255 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84865 (* 1 = 6.84865 loss)
I0523 07:21:20.093142 35003 sgd_solver.cpp:112] Iteration 172370, lr = 0.001
I0523 07:21:22.941676 35003 solver.cpp:239] Iteration 172380 (2.79074 iter/s, 3.58329s/10 iters), loss = 6.84821
I0523 07:21:22.941730 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84821 (* 1 = 6.84821 loss)
I0523 07:21:23.657049 35003 sgd_solver.cpp:112] Iteration 172380, lr = 0.001
I0523 07:21:26.524309 35003 solver.cpp:239] Iteration 172390 (2.7914 iter/s, 3.58243s/10 iters), loss = 7.62693
I0523 07:21:26.524376 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62693 (* 1 = 7.62693 loss)
I0523 07:21:26.531616 35003 sgd_solver.cpp:112] Iteration 172390, lr = 0.001
I0523 07:21:29.304919 35003 solver.cpp:239] Iteration 172400 (3.59657 iter/s, 2.78043s/10 iters), loss = 6.72743
I0523 07:21:29.304962 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72743 (* 1 = 6.72743 loss)
I0523 07:21:30.014415 35003 sgd_solver.cpp:112] Iteration 172400, lr = 0.001
I0523 07:21:34.188948 35003 solver.cpp:239] Iteration 172410 (2.04759 iter/s, 4.88379s/10 iters), loss = 5.66699
I0523 07:21:34.188988 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.66699 (* 1 = 5.66699 loss)
I0523 07:21:34.306073 35003 sgd_solver.cpp:112] Iteration 172410, lr = 0.001
I0523 07:21:37.788672 35003 solver.cpp:239] Iteration 172420 (2.77815 iter/s, 3.59952s/10 iters), loss = 6.26695
I0523 07:21:37.788722 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26695 (* 1 = 6.26695 loss)
I0523 07:21:37.793236 35003 sgd_solver.cpp:112] Iteration 172420, lr = 0.001
I0523 07:21:40.695066 35003 solver.cpp:239] Iteration 172430 (3.4409 iter/s, 2.90622s/10 iters), loss = 6.97835
I0523 07:21:40.695108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97835 (* 1 = 6.97835 loss)
I0523 07:21:40.700695 35003 sgd_solver.cpp:112] Iteration 172430, lr = 0.001
I0523 07:21:44.177721 35003 solver.cpp:239] Iteration 172440 (2.87153 iter/s, 3.48247s/10 iters), loss = 7.85785
I0523 07:21:44.177758 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85785 (* 1 = 7.85785 loss)
I0523 07:21:44.894584 35003 sgd_solver.cpp:112] Iteration 172440, lr = 0.001
I0523 07:21:47.870900 35003 solver.cpp:239] Iteration 172450 (2.70783 iter/s, 3.69299s/10 iters), loss = 7.93351
I0523 07:21:47.870939 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93351 (* 1 = 7.93351 loss)
I0523 07:21:47.884083 35003 sgd_solver.cpp:112] Iteration 172450, lr = 0.001
I0523 07:21:49.905205 35003 solver.cpp:239] Iteration 172460 (4.91599 iter/s, 2.03418s/10 iters), loss = 7.7061
I0523 07:21:49.905491 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7061 (* 1 = 7.7061 loss)
I0523 07:21:50.493788 35003 sgd_solver.cpp:112] Iteration 172460, lr = 0.001
I0523 07:21:54.141269 35003 solver.cpp:239] Iteration 172470 (2.36092 iter/s, 4.23563s/10 iters), loss = 5.85849
I0523 07:21:54.141319 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85849 (* 1 = 5.85849 loss)
I0523 07:21:54.148336 35003 sgd_solver.cpp:112] Iteration 172470, lr = 0.001
I0523 07:21:55.987890 35003 solver.cpp:239] Iteration 172480 (5.4157 iter/s, 1.84649s/10 iters), loss = 6.04091
I0523 07:21:55.987934 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04091 (* 1 = 6.04091 loss)
I0523 07:21:56.006047 35003 sgd_solver.cpp:112] Iteration 172480, lr = 0.001
I0523 07:22:01.019448 35003 solver.cpp:239] Iteration 172490 (1.98755 iter/s, 5.03131s/10 iters), loss = 6.16026
I0523 07:22:01.019484 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16026 (* 1 = 6.16026 loss)
I0523 07:22:01.033401 35003 sgd_solver.cpp:112] Iteration 172490, lr = 0.001
I0523 07:22:03.778415 35003 solver.cpp:239] Iteration 172500 (3.62475 iter/s, 2.75881s/10 iters), loss = 7.09035
I0523 07:22:03.778456 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09035 (* 1 = 7.09035 loss)
I0523 07:22:03.784277 35003 sgd_solver.cpp:112] Iteration 172500, lr = 0.001
I0523 07:22:07.964504 35003 solver.cpp:239] Iteration 172510 (2.38899 iter/s, 4.18587s/10 iters), loss = 7.26415
I0523 07:22:07.964548 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26415 (* 1 = 7.26415 loss)
I0523 07:22:07.977721 35003 sgd_solver.cpp:112] Iteration 172510, lr = 0.001
I0523 07:22:10.863049 35003 solver.cpp:239] Iteration 172520 (3.45021 iter/s, 2.89837s/10 iters), loss = 6.99331
I0523 07:22:10.863091 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99331 (* 1 = 6.99331 loss)
I0523 07:22:10.875982 35003 sgd_solver.cpp:112] Iteration 172520, lr = 0.001
I0523 07:22:13.799816 35003 solver.cpp:239] Iteration 172530 (3.4053 iter/s, 2.9366s/10 iters), loss = 5.49717
I0523 07:22:13.799852 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.49717 (* 1 = 5.49717 loss)
I0523 07:22:13.813194 35003 sgd_solver.cpp:112] Iteration 172530, lr = 0.001
I0523 07:22:15.114436 35003 solver.cpp:239] Iteration 172540 (7.60734 iter/s, 1.31452s/10 iters), loss = 6.81255
I0523 07:22:15.114476 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81255 (* 1 = 6.81255 loss)
I0523 07:22:15.128381 35003 sgd_solver.cpp:112] Iteration 172540, lr = 0.001
I0523 07:22:18.882962 35003 solver.cpp:239] Iteration 172550 (2.6537 iter/s, 3.76832s/10 iters), loss = 6.31371
I0523 07:22:18.883018 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31371 (* 1 = 6.31371 loss)
I0523 07:22:18.893764 35003 sgd_solver.cpp:112] Iteration 172550, lr = 0.001
I0523 07:22:21.741441 35003 solver.cpp:239] Iteration 172560 (3.49858 iter/s, 2.85831s/10 iters), loss = 6.96437
I0523 07:22:21.741715 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96437 (* 1 = 6.96437 loss)
I0523 07:22:22.120189 35003 sgd_solver.cpp:112] Iteration 172560, lr = 0.001
I0523 07:22:25.698568 35003 solver.cpp:239] Iteration 172570 (2.52735 iter/s, 3.95671s/10 iters), loss = 6.64786
I0523 07:22:25.698618 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64786 (* 1 = 6.64786 loss)
I0523 07:22:25.707703 35003 sgd_solver.cpp:112] Iteration 172570, lr = 0.001
I0523 07:22:28.463066 35003 solver.cpp:239] Iteration 172580 (3.61751 iter/s, 2.76433s/10 iters), loss = 7.56327
I0523 07:22:28.463106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56327 (* 1 = 7.56327 loss)
I0523 07:22:28.476624 35003 sgd_solver.cpp:112] Iteration 172580, lr = 0.001
I0523 07:22:31.623793 35003 solver.cpp:239] Iteration 172590 (3.16401 iter/s, 3.16055s/10 iters), loss = 6.14173
I0523 07:22:31.623844 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14173 (* 1 = 6.14173 loss)
I0523 07:22:31.633769 35003 sgd_solver.cpp:112] Iteration 172590, lr = 0.001
I0523 07:22:34.502249 35003 solver.cpp:239] Iteration 172600 (3.47429 iter/s, 2.87829s/10 iters), loss = 6.68293
I0523 07:22:34.502291 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68293 (* 1 = 6.68293 loss)
I0523 07:22:35.137879 35003 sgd_solver.cpp:112] Iteration 172600, lr = 0.001
I0523 07:22:37.203125 35003 solver.cpp:239] Iteration 172610 (3.70273 iter/s, 2.70071s/10 iters), loss = 5.91742
I0523 07:22:37.203186 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91742 (* 1 = 5.91742 loss)
I0523 07:22:37.211426 35003 sgd_solver.cpp:112] Iteration 172610, lr = 0.001
I0523 07:22:39.874531 35003 solver.cpp:239] Iteration 172620 (3.74359 iter/s, 2.67123s/10 iters), loss = 7.24232
I0523 07:22:39.874581 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24232 (* 1 = 7.24232 loss)
I0523 07:22:40.556730 35003 sgd_solver.cpp:112] Iteration 172620, lr = 0.001
I0523 07:22:44.954536 35003 solver.cpp:239] Iteration 172630 (1.9686 iter/s, 5.07975s/10 iters), loss = 6.26536
I0523 07:22:44.954582 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26536 (* 1 = 6.26536 loss)
I0523 07:22:44.970757 35003 sgd_solver.cpp:112] Iteration 172630, lr = 0.001
I0523 07:22:47.969764 35003 solver.cpp:239] Iteration 172640 (3.31669 iter/s, 3.01505s/10 iters), loss = 6.49356
I0523 07:22:47.969805 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49356 (* 1 = 6.49356 loss)
I0523 07:22:47.974330 35003 sgd_solver.cpp:112] Iteration 172640, lr = 0.001
I0523 07:22:51.577571 35003 solver.cpp:239] Iteration 172650 (2.77192 iter/s, 3.60761s/10 iters), loss = 5.71585
I0523 07:22:51.577611 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.71585 (* 1 = 5.71585 loss)
I0523 07:22:51.591054 35003 sgd_solver.cpp:112] Iteration 172650, lr = 0.001
I0523 07:22:55.061159 35003 solver.cpp:239] Iteration 172660 (2.87076 iter/s, 3.4834s/10 iters), loss = 6.73775
I0523 07:22:55.061436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73775 (* 1 = 6.73775 loss)
I0523 07:22:55.080210 35003 sgd_solver.cpp:112] Iteration 172660, lr = 0.001
I0523 07:22:58.584915 35003 solver.cpp:239] Iteration 172670 (2.8382 iter/s, 3.52336s/10 iters), loss = 6.42939
I0523 07:22:58.584957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42939 (* 1 = 6.42939 loss)
I0523 07:22:58.598258 35003 sgd_solver.cpp:112] Iteration 172670, lr = 0.001
I0523 07:23:02.832978 35003 solver.cpp:239] Iteration 172680 (2.35414 iter/s, 4.24784s/10 iters), loss = 6.57431
I0523 07:23:02.833024 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57431 (* 1 = 6.57431 loss)
I0523 07:23:02.841085 35003 sgd_solver.cpp:112] Iteration 172680, lr = 0.001
I0523 07:23:04.941079 35003 solver.cpp:239] Iteration 172690 (4.74393 iter/s, 2.10796s/10 iters), loss = 6.39315
I0523 07:23:04.941124 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39315 (* 1 = 6.39315 loss)
I0523 07:23:04.944370 35003 sgd_solver.cpp:112] Iteration 172690, lr = 0.001
I0523 07:23:08.743892 35003 solver.cpp:239] Iteration 172700 (2.62978 iter/s, 3.8026s/10 iters), loss = 6.08976
I0523 07:23:08.743934 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08976 (* 1 = 6.08976 loss)
I0523 07:23:08.751924 35003 sgd_solver.cpp:112] Iteration 172700, lr = 0.001
I0523 07:23:11.951251 35003 solver.cpp:239] Iteration 172710 (3.118 iter/s, 3.20718s/10 iters), loss = 7.09775
I0523 07:23:11.951287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09775 (* 1 = 7.09775 loss)
I0523 07:23:11.964895 35003 sgd_solver.cpp:112] Iteration 172710, lr = 0.001
I0523 07:23:15.712193 35003 solver.cpp:239] Iteration 172720 (2.65905 iter/s, 3.76075s/10 iters), loss = 6.35951
I0523 07:23:15.712241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35951 (* 1 = 6.35951 loss)
I0523 07:23:15.723330 35003 sgd_solver.cpp:112] Iteration 172720, lr = 0.001
I0523 07:23:19.756830 35003 solver.cpp:239] Iteration 172730 (2.47254 iter/s, 4.04442s/10 iters), loss = 6.07283
I0523 07:23:19.756871 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07283 (* 1 = 6.07283 loss)
I0523 07:23:20.220743 35003 sgd_solver.cpp:112] Iteration 172730, lr = 0.001
I0523 07:23:24.609369 35003 solver.cpp:239] Iteration 172740 (2.06088 iter/s, 4.8523s/10 iters), loss = 6.4677
I0523 07:23:24.609405 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4677 (* 1 = 6.4677 loss)
I0523 07:23:24.623052 35003 sgd_solver.cpp:112] Iteration 172740, lr = 0.001
I0523 07:23:28.255966 35003 solver.cpp:239] Iteration 172750 (2.74243 iter/s, 3.6464s/10 iters), loss = 7.0007
I0523 07:23:28.256182 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0007 (* 1 = 7.0007 loss)
I0523 07:23:28.267619 35003 sgd_solver.cpp:112] Iteration 172750, lr = 0.001
I0523 07:23:33.100966 35003 solver.cpp:239] Iteration 172760 (2.06415 iter/s, 4.84462s/10 iters), loss = 6.86619
I0523 07:23:33.101006 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86619 (* 1 = 6.86619 loss)
I0523 07:23:33.108467 35003 sgd_solver.cpp:112] Iteration 172760, lr = 0.001
I0523 07:23:37.534662 35003 solver.cpp:239] Iteration 172770 (2.25557 iter/s, 4.43347s/10 iters), loss = 6.45916
I0523 07:23:37.534723 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45916 (* 1 = 6.45916 loss)
I0523 07:23:37.559092 35003 sgd_solver.cpp:112] Iteration 172770, lr = 0.001
I0523 07:23:40.233142 35003 solver.cpp:239] Iteration 172780 (3.70603 iter/s, 2.69831s/10 iters), loss = 6.2896
I0523 07:23:40.233191 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2896 (* 1 = 6.2896 loss)
I0523 07:23:40.242049 35003 sgd_solver.cpp:112] Iteration 172780, lr = 0.001
I0523 07:23:43.956946 35003 solver.cpp:239] Iteration 172790 (2.68557 iter/s, 3.7236s/10 iters), loss = 6.08386
I0523 07:23:43.956990 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08386 (* 1 = 6.08386 loss)
I0523 07:23:44.590278 35003 sgd_solver.cpp:112] Iteration 172790, lr = 0.001
I0523 07:23:48.337757 35003 solver.cpp:239] Iteration 172800 (2.2828 iter/s, 4.38058s/10 iters), loss = 6.74356
I0523 07:23:48.337810 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74356 (* 1 = 6.74356 loss)
I0523 07:23:48.350706 35003 sgd_solver.cpp:112] Iteration 172800, lr = 0.001
I0523 07:23:51.413233 35003 solver.cpp:239] Iteration 172810 (3.25172 iter/s, 3.07529s/10 iters), loss = 7.98477
I0523 07:23:51.413276 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98477 (* 1 = 7.98477 loss)
I0523 07:23:51.425966 35003 sgd_solver.cpp:112] Iteration 172810, lr = 0.001
I0523 07:23:54.294114 35003 solver.cpp:239] Iteration 172820 (3.47136 iter/s, 2.88071s/10 iters), loss = 6.1048
I0523 07:23:54.294162 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1048 (* 1 = 6.1048 loss)
I0523 07:23:54.309725 35003 sgd_solver.cpp:112] Iteration 172820, lr = 0.001
I0523 07:23:58.141512 35003 solver.cpp:239] Iteration 172830 (2.59931 iter/s, 3.84718s/10 iters), loss = 5.76401
I0523 07:23:58.141558 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.76401 (* 1 = 5.76401 loss)
I0523 07:23:58.154289 35003 sgd_solver.cpp:112] Iteration 172830, lr = 0.001
I0523 07:24:01.710645 35003 solver.cpp:239] Iteration 172840 (2.80195 iter/s, 3.56894s/10 iters), loss = 7.39207
I0523 07:24:01.710968 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39207 (* 1 = 7.39207 loss)
I0523 07:24:01.723691 35003 sgd_solver.cpp:112] Iteration 172840, lr = 0.001
I0523 07:24:05.282354 35003 solver.cpp:239] Iteration 172850 (2.80013 iter/s, 3.57126s/10 iters), loss = 6.38967
I0523 07:24:05.282413 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38967 (* 1 = 6.38967 loss)
I0523 07:24:05.294765 35003 sgd_solver.cpp:112] Iteration 172850, lr = 0.001
I0523 07:24:08.860157 35003 solver.cpp:239] Iteration 172860 (2.79518 iter/s, 3.57759s/10 iters), loss = 6.65187
I0523 07:24:08.860198 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65187 (* 1 = 6.65187 loss)
I0523 07:24:08.873281 35003 sgd_solver.cpp:112] Iteration 172860, lr = 0.001
I0523 07:24:13.536500 35003 solver.cpp:239] Iteration 172870 (2.13853 iter/s, 4.67611s/10 iters), loss = 5.73953
I0523 07:24:13.536540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.73953 (* 1 = 5.73953 loss)
I0523 07:24:13.554262 35003 sgd_solver.cpp:112] Iteration 172870, lr = 0.001
I0523 07:24:17.154541 35003 solver.cpp:239] Iteration 172880 (2.76407 iter/s, 3.61785s/10 iters), loss = 6.15138
I0523 07:24:17.154580 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15138 (* 1 = 6.15138 loss)
I0523 07:24:17.850275 35003 sgd_solver.cpp:112] Iteration 172880, lr = 0.001
I0523 07:24:20.340759 35003 solver.cpp:239] Iteration 172890 (3.13869 iter/s, 3.18604s/10 iters), loss = 6.4239
I0523 07:24:20.340807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4239 (* 1 = 6.4239 loss)
I0523 07:24:20.966279 35003 sgd_solver.cpp:112] Iteration 172890, lr = 0.001
I0523 07:24:23.535326 35003 solver.cpp:239] Iteration 172900 (3.13049 iter/s, 3.19438s/10 iters), loss = 5.63293
I0523 07:24:23.535380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.63293 (* 1 = 5.63293 loss)
I0523 07:24:24.250890 35003 sgd_solver.cpp:112] Iteration 172900, lr = 0.001
I0523 07:24:27.093964 35003 solver.cpp:239] Iteration 172910 (2.81023 iter/s, 3.55843s/10 iters), loss = 6.52299
I0523 07:24:27.094012 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52299 (* 1 = 6.52299 loss)
I0523 07:24:27.834800 35003 sgd_solver.cpp:112] Iteration 172910, lr = 0.001
I0523 07:24:32.066851 35003 solver.cpp:239] Iteration 172920 (2.011 iter/s, 4.97264s/10 iters), loss = 7.28903
I0523 07:24:32.067155 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28903 (* 1 = 7.28903 loss)
I0523 07:24:32.080166 35003 sgd_solver.cpp:112] Iteration 172920, lr = 0.001
I0523 07:24:35.347962 35003 solver.cpp:239] Iteration 172930 (3.04813 iter/s, 3.2807s/10 iters), loss = 6.18795
I0523 07:24:35.348012 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18795 (* 1 = 6.18795 loss)
I0523 07:24:36.008874 35003 sgd_solver.cpp:112] Iteration 172930, lr = 0.001
I0523 07:24:38.918527 35003 solver.cpp:239] Iteration 172940 (2.80084 iter/s, 3.57036s/10 iters), loss = 6.42326
I0523 07:24:38.918581 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42326 (* 1 = 6.42326 loss)
I0523 07:24:39.542521 35003 sgd_solver.cpp:112] Iteration 172940, lr = 0.001
I0523 07:24:43.060834 35003 solver.cpp:239] Iteration 172950 (2.41424 iter/s, 4.14208s/10 iters), loss = 7.01563
I0523 07:24:43.060886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01563 (* 1 = 7.01563 loss)
I0523 07:24:43.795547 35003 sgd_solver.cpp:112] Iteration 172950, lr = 0.001
I0523 07:24:46.879040 35003 solver.cpp:239] Iteration 172960 (2.61918 iter/s, 3.818s/10 iters), loss = 6.6748
I0523 07:24:46.879084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6748 (* 1 = 6.6748 loss)
I0523 07:24:47.587837 35003 sgd_solver.cpp:112] Iteration 172960, lr = 0.001
I0523 07:24:50.453210 35003 solver.cpp:239] Iteration 172970 (2.798 iter/s, 3.57398s/10 iters), loss = 5.92281
I0523 07:24:50.453253 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92281 (* 1 = 5.92281 loss)
I0523 07:24:50.460721 35003 sgd_solver.cpp:112] Iteration 172970, lr = 0.001
I0523 07:24:52.559495 35003 solver.cpp:239] Iteration 172980 (4.748 iter/s, 2.10615s/10 iters), loss = 7.33087
I0523 07:24:52.559531 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33087 (* 1 = 7.33087 loss)
I0523 07:24:52.572738 35003 sgd_solver.cpp:112] Iteration 172980, lr = 0.001
I0523 07:24:55.584499 35003 solver.cpp:239] Iteration 172990 (3.30596 iter/s, 3.02484s/10 iters), loss = 6.02053
I0523 07:24:55.584542 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02053 (* 1 = 6.02053 loss)
I0523 07:24:55.597873 35003 sgd_solver.cpp:112] Iteration 172990, lr = 0.001
I0523 07:24:59.212882 35003 solver.cpp:239] Iteration 173000 (2.7562 iter/s, 3.62818s/10 iters), loss = 6.51527
I0523 07:24:59.212924 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51527 (* 1 = 6.51527 loss)
I0523 07:24:59.913668 35003 sgd_solver.cpp:112] Iteration 173000, lr = 0.001
I0523 07:25:02.655810 35003 solver.cpp:239] Iteration 173010 (2.90466 iter/s, 3.44274s/10 iters), loss = 7.18957
I0523 07:25:02.655982 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18957 (* 1 = 7.18957 loss)
I0523 07:25:02.660583 35003 sgd_solver.cpp:112] Iteration 173010, lr = 0.001
I0523 07:25:05.493227 35003 solver.cpp:239] Iteration 173020 (3.52468 iter/s, 2.83713s/10 iters), loss = 7.18307
I0523 07:25:05.493266 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18307 (* 1 = 7.18307 loss)
I0523 07:25:05.497084 35003 sgd_solver.cpp:112] Iteration 173020, lr = 0.001
I0523 07:25:08.931915 35003 solver.cpp:239] Iteration 173030 (2.90825 iter/s, 3.4385s/10 iters), loss = 5.87484
I0523 07:25:08.931967 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87484 (* 1 = 5.87484 loss)
I0523 07:25:08.944851 35003 sgd_solver.cpp:112] Iteration 173030, lr = 0.001
I0523 07:25:13.728534 35003 solver.cpp:239] Iteration 173040 (2.08491 iter/s, 4.79637s/10 iters), loss = 7.71852
I0523 07:25:13.728585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71852 (* 1 = 7.71852 loss)
I0523 07:25:13.733882 35003 sgd_solver.cpp:112] Iteration 173040, lr = 0.001
I0523 07:25:16.618587 35003 solver.cpp:239] Iteration 173050 (3.46036 iter/s, 2.88987s/10 iters), loss = 5.93591
I0523 07:25:16.618625 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93591 (* 1 = 5.93591 loss)
I0523 07:25:17.250705 35003 sgd_solver.cpp:112] Iteration 173050, lr = 0.001
I0523 07:25:20.059736 35003 solver.cpp:239] Iteration 173060 (2.90617 iter/s, 3.44096s/10 iters), loss = 6.41494
I0523 07:25:20.059782 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41494 (* 1 = 6.41494 loss)
I0523 07:25:20.801120 35003 sgd_solver.cpp:112] Iteration 173060, lr = 0.001
I0523 07:25:22.937553 35003 solver.cpp:239] Iteration 173070 (3.47506 iter/s, 2.87764s/10 iters), loss = 7.06302
I0523 07:25:22.937600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06302 (* 1 = 7.06302 loss)
I0523 07:25:22.950819 35003 sgd_solver.cpp:112] Iteration 173070, lr = 0.001
I0523 07:25:25.935137 35003 solver.cpp:239] Iteration 173080 (3.33621 iter/s, 2.99741s/10 iters), loss = 6.38926
I0523 07:25:25.935184 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38926 (* 1 = 6.38926 loss)
I0523 07:25:25.946727 35003 sgd_solver.cpp:112] Iteration 173080, lr = 0.001
I0523 07:25:29.516680 35003 solver.cpp:239] Iteration 173090 (2.79225 iter/s, 3.58135s/10 iters), loss = 6.99308
I0523 07:25:29.516721 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99308 (* 1 = 6.99308 loss)
I0523 07:25:29.530259 35003 sgd_solver.cpp:112] Iteration 173090, lr = 0.001
I0523 07:25:30.866904 35003 solver.cpp:239] Iteration 173100 (7.40676 iter/s, 1.35012s/10 iters), loss = 6.72343
I0523 07:25:30.866950 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72343 (* 1 = 6.72343 loss)
I0523 07:25:30.874228 35003 sgd_solver.cpp:112] Iteration 173100, lr = 0.001
I0523 07:25:34.430375 35003 solver.cpp:239] Iteration 173110 (2.80641 iter/s, 3.56327s/10 iters), loss = 6.69338
I0523 07:25:34.430672 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69338 (* 1 = 6.69338 loss)
I0523 07:25:34.436869 35003 sgd_solver.cpp:112] Iteration 173110, lr = 0.001
I0523 07:25:37.282744 35003 solver.cpp:239] Iteration 173120 (3.50635 iter/s, 2.85197s/10 iters), loss = 7.73305
I0523 07:25:37.282799 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73305 (* 1 = 7.73305 loss)
I0523 07:25:37.304360 35003 sgd_solver.cpp:112] Iteration 173120, lr = 0.001
I0523 07:25:40.842581 35003 solver.cpp:239] Iteration 173130 (2.80929 iter/s, 3.55962s/10 iters), loss = 6.85188
I0523 07:25:40.842649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85188 (* 1 = 6.85188 loss)
I0523 07:25:41.551069 35003 sgd_solver.cpp:112] Iteration 173130, lr = 0.001
I0523 07:25:45.974824 35003 solver.cpp:239] Iteration 173140 (1.94857 iter/s, 5.13197s/10 iters), loss = 5.88315
I0523 07:25:45.974874 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.88315 (* 1 = 5.88315 loss)
I0523 07:25:45.994988 35003 sgd_solver.cpp:112] Iteration 173140, lr = 0.001
I0523 07:25:50.333750 35003 solver.cpp:239] Iteration 173150 (2.29427 iter/s, 4.35869s/10 iters), loss = 6.93285
I0523 07:25:50.333827 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93285 (* 1 = 6.93285 loss)
I0523 07:25:51.074451 35003 sgd_solver.cpp:112] Iteration 173150, lr = 0.001
I0523 07:25:53.695870 35003 solver.cpp:239] Iteration 173160 (2.9745 iter/s, 3.36191s/10 iters), loss = 6.69239
I0523 07:25:53.695907 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69239 (* 1 = 6.69239 loss)
I0523 07:25:53.708815 35003 sgd_solver.cpp:112] Iteration 173160, lr = 0.001
I0523 07:25:55.925295 35003 solver.cpp:239] Iteration 173170 (4.48573 iter/s, 2.22929s/10 iters), loss = 6.49929
I0523 07:25:55.925333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49929 (* 1 = 6.49929 loss)
I0523 07:25:55.938374 35003 sgd_solver.cpp:112] Iteration 173170, lr = 0.001
I0523 07:26:00.165781 35003 solver.cpp:239] Iteration 173180 (2.35834 iter/s, 4.24028s/10 iters), loss = 7.519
I0523 07:26:00.165822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.519 (* 1 = 7.519 loss)
I0523 07:26:00.879307 35003 sgd_solver.cpp:112] Iteration 173180, lr = 0.001
I0523 07:26:04.432241 35003 solver.cpp:239] Iteration 173190 (2.34399 iter/s, 4.26624s/10 iters), loss = 5.97309
I0523 07:26:04.432436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97309 (* 1 = 5.97309 loss)
I0523 07:26:04.443284 35003 sgd_solver.cpp:112] Iteration 173190, lr = 0.001
I0523 07:26:07.185545 35003 solver.cpp:239] Iteration 173200 (3.63241 iter/s, 2.75299s/10 iters), loss = 7.24863
I0523 07:26:07.185595 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24863 (* 1 = 7.24863 loss)
I0523 07:26:07.198602 35003 sgd_solver.cpp:112] Iteration 173200, lr = 0.001
I0523 07:26:10.003203 35003 solver.cpp:239] Iteration 173210 (3.54926 iter/s, 2.81749s/10 iters), loss = 6.47285
I0523 07:26:10.003242 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47285 (* 1 = 6.47285 loss)
I0523 07:26:10.016670 35003 sgd_solver.cpp:112] Iteration 173210, lr = 0.001
I0523 07:26:13.475833 35003 solver.cpp:239] Iteration 173220 (2.87981 iter/s, 3.47245s/10 iters), loss = 6.58016
I0523 07:26:13.475872 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58016 (* 1 = 6.58016 loss)
I0523 07:26:13.488199 35003 sgd_solver.cpp:112] Iteration 173220, lr = 0.001
I0523 07:26:17.075181 35003 solver.cpp:239] Iteration 173230 (2.77843 iter/s, 3.59915s/10 iters), loss = 5.97655
I0523 07:26:17.075238 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97655 (* 1 = 5.97655 loss)
I0523 07:26:17.079213 35003 sgd_solver.cpp:112] Iteration 173230, lr = 0.001
I0523 07:26:22.191290 35003 solver.cpp:239] Iteration 173240 (1.95471 iter/s, 5.11585s/10 iters), loss = 6.06762
I0523 07:26:22.191328 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06762 (* 1 = 6.06762 loss)
I0523 07:26:22.200871 35003 sgd_solver.cpp:112] Iteration 173240, lr = 0.001
I0523 07:26:24.323148 35003 solver.cpp:239] Iteration 173250 (4.69106 iter/s, 2.13172s/10 iters), loss = 7.75147
I0523 07:26:24.323191 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75147 (* 1 = 7.75147 loss)
I0523 07:26:24.329098 35003 sgd_solver.cpp:112] Iteration 173250, lr = 0.001
I0523 07:26:27.879298 35003 solver.cpp:239] Iteration 173260 (2.81218 iter/s, 3.55596s/10 iters), loss = 6.4533
I0523 07:26:27.879336 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4533 (* 1 = 6.4533 loss)
I0523 07:26:27.891424 35003 sgd_solver.cpp:112] Iteration 173260, lr = 0.001
I0523 07:26:30.741592 35003 solver.cpp:239] Iteration 173270 (3.49391 iter/s, 2.86212s/10 iters), loss = 5.87984
I0523 07:26:30.741634 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87984 (* 1 = 5.87984 loss)
I0523 07:26:30.754746 35003 sgd_solver.cpp:112] Iteration 173270, lr = 0.001
I0523 07:26:34.536109 35003 solver.cpp:239] Iteration 173280 (2.63552 iter/s, 3.79431s/10 iters), loss = 6.92142
I0523 07:26:34.536355 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92142 (* 1 = 6.92142 loss)
I0523 07:26:34.555445 35003 sgd_solver.cpp:112] Iteration 173280, lr = 0.001
I0523 07:26:37.755012 35003 solver.cpp:239] Iteration 173290 (3.107 iter/s, 3.21854s/10 iters), loss = 7.226
I0523 07:26:37.755057 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.226 (* 1 = 7.226 loss)
I0523 07:26:37.768084 35003 sgd_solver.cpp:112] Iteration 173290, lr = 0.001
I0523 07:26:41.969383 35003 solver.cpp:239] Iteration 173300 (2.37296 iter/s, 4.21415s/10 iters), loss = 5.29408
I0523 07:26:41.969431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.29408 (* 1 = 5.29408 loss)
I0523 07:26:42.684970 35003 sgd_solver.cpp:112] Iteration 173300, lr = 0.001
I0523 07:26:45.369454 35003 solver.cpp:239] Iteration 173310 (2.94128 iter/s, 3.39988s/10 iters), loss = 6.30916
I0523 07:26:45.369489 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30916 (* 1 = 6.30916 loss)
I0523 07:26:45.375648 35003 sgd_solver.cpp:112] Iteration 173310, lr = 0.001
I0523 07:26:48.345963 35003 solver.cpp:239] Iteration 173320 (3.35984 iter/s, 2.97634s/10 iters), loss = 7.02537
I0523 07:26:48.346009 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02537 (* 1 = 7.02537 loss)
I0523 07:26:48.360039 35003 sgd_solver.cpp:112] Iteration 173320, lr = 0.001
I0523 07:26:51.943863 35003 solver.cpp:239] Iteration 173330 (2.77955 iter/s, 3.5977s/10 iters), loss = 7.07817
I0523 07:26:51.943915 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07817 (* 1 = 7.07817 loss)
I0523 07:26:51.957538 35003 sgd_solver.cpp:112] Iteration 173330, lr = 0.001
I0523 07:26:56.792006 35003 solver.cpp:239] Iteration 173340 (2.06275 iter/s, 4.84789s/10 iters), loss = 5.25504
I0523 07:26:56.792054 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.25504 (* 1 = 5.25504 loss)
I0523 07:26:56.810948 35003 sgd_solver.cpp:112] Iteration 173340, lr = 0.001
I0523 07:27:01.451232 35003 solver.cpp:239] Iteration 173350 (2.14639 iter/s, 4.65898s/10 iters), loss = 5.67771
I0523 07:27:01.451287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.67771 (* 1 = 5.67771 loss)
I0523 07:27:01.464299 35003 sgd_solver.cpp:112] Iteration 173350, lr = 0.001
I0523 07:27:05.352979 35003 solver.cpp:239] Iteration 173360 (2.5631 iter/s, 3.90153s/10 iters), loss = 6.11579
I0523 07:27:05.353245 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11579 (* 1 = 6.11579 loss)
I0523 07:27:06.062089 35003 sgd_solver.cpp:112] Iteration 173360, lr = 0.001
I0523 07:27:07.646183 35003 solver.cpp:239] Iteration 173370 (4.36134 iter/s, 2.29287s/10 iters), loss = 6.08965
I0523 07:27:07.646225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08965 (* 1 = 6.08965 loss)
I0523 07:27:08.387785 35003 sgd_solver.cpp:112] Iteration 173370, lr = 0.001
I0523 07:27:10.344331 35003 solver.cpp:239] Iteration 173380 (3.70646 iter/s, 2.69799s/10 iters), loss = 7.52617
I0523 07:27:10.344375 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52617 (* 1 = 7.52617 loss)
I0523 07:27:10.347790 35003 sgd_solver.cpp:112] Iteration 173380, lr = 0.001
I0523 07:27:14.000053 35003 solver.cpp:239] Iteration 173390 (2.73559 iter/s, 3.65552s/10 iters), loss = 6.46869
I0523 07:27:14.000110 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46869 (* 1 = 6.46869 loss)
I0523 07:27:14.006418 35003 sgd_solver.cpp:112] Iteration 173390, lr = 0.001
I0523 07:27:16.376482 35003 solver.cpp:239] Iteration 173400 (4.20829 iter/s, 2.37626s/10 iters), loss = 7.32631
I0523 07:27:16.376538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32631 (* 1 = 7.32631 loss)
I0523 07:27:17.110674 35003 sgd_solver.cpp:112] Iteration 173400, lr = 0.001
I0523 07:27:20.782277 35003 solver.cpp:239] Iteration 173410 (2.26986 iter/s, 4.40556s/10 iters), loss = 5.59753
I0523 07:27:20.782315 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.59753 (* 1 = 5.59753 loss)
I0523 07:27:20.795091 35003 sgd_solver.cpp:112] Iteration 173410, lr = 0.001
I0523 07:27:24.986976 35003 solver.cpp:239] Iteration 173420 (2.37841 iter/s, 4.20449s/10 iters), loss = 5.73728
I0523 07:27:24.987020 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.73728 (* 1 = 5.73728 loss)
I0523 07:27:25.715414 35003 sgd_solver.cpp:112] Iteration 173420, lr = 0.001
I0523 07:27:28.565521 35003 solver.cpp:239] Iteration 173430 (2.79458 iter/s, 3.57835s/10 iters), loss = 7.1744
I0523 07:27:28.565559 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1744 (* 1 = 7.1744 loss)
I0523 07:27:28.578457 35003 sgd_solver.cpp:112] Iteration 173430, lr = 0.001
I0523 07:27:32.562614 35003 solver.cpp:239] Iteration 173440 (2.50195 iter/s, 3.99688s/10 iters), loss = 7.08288
I0523 07:27:32.562659 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08288 (* 1 = 7.08288 loss)
I0523 07:27:33.175544 35003 sgd_solver.cpp:112] Iteration 173440, lr = 0.001
I0523 07:27:35.974720 35003 solver.cpp:239] Iteration 173450 (2.93091 iter/s, 3.41191s/10 iters), loss = 5.78063
I0523 07:27:35.974943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.78063 (* 1 = 5.78063 loss)
I0523 07:27:35.987728 35003 sgd_solver.cpp:112] Iteration 173450, lr = 0.001
I0523 07:27:40.350360 35003 solver.cpp:239] Iteration 173460 (2.28558 iter/s, 4.37527s/10 iters), loss = 6.80181
I0523 07:27:40.350399 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80181 (* 1 = 6.80181 loss)
I0523 07:27:40.369068 35003 sgd_solver.cpp:112] Iteration 173460, lr = 0.001
I0523 07:27:44.885514 35003 solver.cpp:239] Iteration 173470 (2.20511 iter/s, 4.53493s/10 iters), loss = 6.22647
I0523 07:27:44.885561 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22647 (* 1 = 6.22647 loss)
I0523 07:27:44.892658 35003 sgd_solver.cpp:112] Iteration 173470, lr = 0.001
I0523 07:27:47.677016 35003 solver.cpp:239] Iteration 173480 (3.58252 iter/s, 2.79133s/10 iters), loss = 6.96874
I0523 07:27:47.677060 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96874 (* 1 = 6.96874 loss)
I0523 07:27:47.711220 35003 sgd_solver.cpp:112] Iteration 173480, lr = 0.001
I0523 07:27:50.412158 35003 solver.cpp:239] Iteration 173490 (3.65634 iter/s, 2.73498s/10 iters), loss = 6.65448
I0523 07:27:50.412226 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65448 (* 1 = 6.65448 loss)
I0523 07:27:51.153165 35003 sgd_solver.cpp:112] Iteration 173490, lr = 0.001
I0523 07:27:54.124766 35003 solver.cpp:239] Iteration 173500 (2.69368 iter/s, 3.71239s/10 iters), loss = 8.31944
I0523 07:27:54.124810 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.31944 (* 1 = 8.31944 loss)
I0523 07:27:54.853219 35003 sgd_solver.cpp:112] Iteration 173500, lr = 0.001
I0523 07:27:57.893448 35003 solver.cpp:239] Iteration 173510 (2.65359 iter/s, 3.76848s/10 iters), loss = 6.60171
I0523 07:27:57.893493 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60171 (* 1 = 6.60171 loss)
I0523 07:27:58.271637 35003 sgd_solver.cpp:112] Iteration 173510, lr = 0.001
I0523 07:28:01.969089 35003 solver.cpp:239] Iteration 173520 (2.45373 iter/s, 4.07543s/10 iters), loss = 6.39813
I0523 07:28:01.969130 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39813 (* 1 = 6.39813 loss)
I0523 07:28:01.985185 35003 sgd_solver.cpp:112] Iteration 173520, lr = 0.001
I0523 07:28:06.325103 35003 solver.cpp:239] Iteration 173530 (2.29579 iter/s, 4.35579s/10 iters), loss = 6.7671
I0523 07:28:06.325399 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7671 (* 1 = 6.7671 loss)
I0523 07:28:06.951624 35003 sgd_solver.cpp:112] Iteration 173530, lr = 0.001
I0523 07:28:10.725616 35003 solver.cpp:239] Iteration 173540 (2.27269 iter/s, 4.40007s/10 iters), loss = 8.15542
I0523 07:28:10.725670 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.15542 (* 1 = 8.15542 loss)
I0523 07:28:11.441201 35003 sgd_solver.cpp:112] Iteration 173540, lr = 0.001
I0523 07:28:14.828558 35003 solver.cpp:239] Iteration 173550 (2.43741 iter/s, 4.10272s/10 iters), loss = 7.29814
I0523 07:28:14.828610 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29814 (* 1 = 7.29814 loss)
I0523 07:28:15.541645 35003 sgd_solver.cpp:112] Iteration 173550, lr = 0.001
I0523 07:28:18.988817 35003 solver.cpp:239] Iteration 173560 (2.40382 iter/s, 4.16004s/10 iters), loss = 6.45384
I0523 07:28:18.988860 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45384 (* 1 = 6.45384 loss)
I0523 07:28:19.010587 35003 sgd_solver.cpp:112] Iteration 173560, lr = 0.001
I0523 07:28:22.919208 35003 solver.cpp:239] Iteration 173570 (2.54441 iter/s, 3.93018s/10 iters), loss = 6.94409
I0523 07:28:22.919246 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94409 (* 1 = 6.94409 loss)
I0523 07:28:23.555496 35003 sgd_solver.cpp:112] Iteration 173570, lr = 0.001
I0523 07:28:28.036633 35003 solver.cpp:239] Iteration 173580 (1.9542 iter/s, 5.11718s/10 iters), loss = 5.51674
I0523 07:28:28.036676 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.51674 (* 1 = 5.51674 loss)
I0523 07:28:28.752293 35003 sgd_solver.cpp:112] Iteration 173580, lr = 0.001
I0523 07:28:30.858866 35003 solver.cpp:239] Iteration 173590 (3.5435 iter/s, 2.82207s/10 iters), loss = 7.2041
I0523 07:28:30.858912 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2041 (* 1 = 7.2041 loss)
I0523 07:28:30.868422 35003 sgd_solver.cpp:112] Iteration 173590, lr = 0.001
I0523 07:28:34.533629 35003 solver.cpp:239] Iteration 173600 (2.72141 iter/s, 3.67456s/10 iters), loss = 6.39482
I0523 07:28:34.533681 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39482 (* 1 = 6.39482 loss)
I0523 07:28:35.243038 35003 sgd_solver.cpp:112] Iteration 173600, lr = 0.001
I0523 07:28:38.060834 35003 solver.cpp:239] Iteration 173610 (2.83526 iter/s, 3.52701s/10 iters), loss = 6.13224
I0523 07:28:38.061040 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13224 (* 1 = 6.13224 loss)
I0523 07:28:38.074429 35003 sgd_solver.cpp:112] Iteration 173610, lr = 0.001
I0523 07:28:40.117962 35003 solver.cpp:239] Iteration 173620 (4.86185 iter/s, 2.05683s/10 iters), loss = 6.07692
I0523 07:28:40.118010 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07692 (* 1 = 6.07692 loss)
I0523 07:28:40.848786 35003 sgd_solver.cpp:112] Iteration 173620, lr = 0.001
I0523 07:28:45.228067 35003 solver.cpp:239] Iteration 173630 (1.95701 iter/s, 5.10985s/10 iters), loss = 7.48481
I0523 07:28:45.228116 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48481 (* 1 = 7.48481 loss)
I0523 07:28:45.967036 35003 sgd_solver.cpp:112] Iteration 173630, lr = 0.001
I0523 07:28:47.998276 35003 solver.cpp:239] Iteration 173640 (3.61005 iter/s, 2.77004s/10 iters), loss = 6.86014
I0523 07:28:47.998320 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86014 (* 1 = 6.86014 loss)
I0523 07:28:48.003738 35003 sgd_solver.cpp:112] Iteration 173640, lr = 0.001
I0523 07:28:51.590410 35003 solver.cpp:239] Iteration 173650 (2.78401 iter/s, 3.59194s/10 iters), loss = 6.7
I0523 07:28:51.590454 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7 (* 1 = 6.7 loss)
I0523 07:28:51.597837 35003 sgd_solver.cpp:112] Iteration 173650, lr = 0.001
I0523 07:28:54.350330 35003 solver.cpp:239] Iteration 173660 (3.6235 iter/s, 2.75976s/10 iters), loss = 6.4557
I0523 07:28:54.350373 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4557 (* 1 = 6.4557 loss)
I0523 07:28:55.065982 35003 sgd_solver.cpp:112] Iteration 173660, lr = 0.001
I0523 07:28:57.196794 35003 solver.cpp:239] Iteration 173670 (3.51333 iter/s, 2.8463s/10 iters), loss = 6.35763
I0523 07:28:57.196830 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35763 (* 1 = 6.35763 loss)
I0523 07:28:57.210968 35003 sgd_solver.cpp:112] Iteration 173670, lr = 0.001
I0523 07:29:01.519697 35003 solver.cpp:239] Iteration 173680 (2.31338 iter/s, 4.32268s/10 iters), loss = 6.44514
I0523 07:29:01.519759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44514 (* 1 = 6.44514 loss)
I0523 07:29:01.528036 35003 sgd_solver.cpp:112] Iteration 173680, lr = 0.001
I0523 07:29:04.031018 35003 solver.cpp:239] Iteration 173690 (3.98225 iter/s, 2.51114s/10 iters), loss = 5.52634
I0523 07:29:04.031069 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.52634 (* 1 = 5.52634 loss)
I0523 07:29:04.038173 35003 sgd_solver.cpp:112] Iteration 173690, lr = 0.001
I0523 07:29:06.872586 35003 solver.cpp:239] Iteration 173700 (3.51941 iter/s, 2.84139s/10 iters), loss = 6.38136
I0523 07:29:06.872637 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38136 (* 1 = 6.38136 loss)
I0523 07:29:06.880368 35003 sgd_solver.cpp:112] Iteration 173700, lr = 0.001
I0523 07:29:10.009933 35003 solver.cpp:239] Iteration 173710 (3.18759 iter/s, 3.13717s/10 iters), loss = 6.82961
I0523 07:29:10.010123 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82961 (* 1 = 6.82961 loss)
I0523 07:29:10.025306 35003 sgd_solver.cpp:112] Iteration 173710, lr = 0.001
I0523 07:29:11.999369 35003 solver.cpp:239] Iteration 173720 (5.03301 iter/s, 1.98688s/10 iters), loss = 6.22349
I0523 07:29:11.999419 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22349 (* 1 = 6.22349 loss)
I0523 07:29:12.072882 35003 sgd_solver.cpp:112] Iteration 173720, lr = 0.001
I0523 07:29:15.548116 35003 solver.cpp:239] Iteration 173730 (2.81805 iter/s, 3.54855s/10 iters), loss = 7.165
I0523 07:29:15.548158 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.165 (* 1 = 7.165 loss)
I0523 07:29:15.561359 35003 sgd_solver.cpp:112] Iteration 173730, lr = 0.001
I0523 07:29:18.636040 35003 solver.cpp:239] Iteration 173740 (3.23863 iter/s, 3.08772s/10 iters), loss = 7.94556
I0523 07:29:18.636102 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.94556 (* 1 = 7.94556 loss)
I0523 07:29:19.370991 35003 sgd_solver.cpp:112] Iteration 173740, lr = 0.001
I0523 07:29:22.758608 35003 solver.cpp:239] Iteration 173750 (2.42582 iter/s, 4.12232s/10 iters), loss = 5.70595
I0523 07:29:22.758666 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.70595 (* 1 = 5.70595 loss)
I0523 07:29:23.453681 35003 sgd_solver.cpp:112] Iteration 173750, lr = 0.001
I0523 07:29:27.093802 35003 solver.cpp:239] Iteration 173760 (2.30683 iter/s, 4.33495s/10 iters), loss = 6.1721
I0523 07:29:27.093849 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1721 (* 1 = 6.1721 loss)
I0523 07:29:27.242758 35003 sgd_solver.cpp:112] Iteration 173760, lr = 0.001
I0523 07:29:30.812366 35003 solver.cpp:239] Iteration 173770 (2.68936 iter/s, 3.71836s/10 iters), loss = 6.81411
I0523 07:29:30.812408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81411 (* 1 = 6.81411 loss)
I0523 07:29:30.967967 35003 sgd_solver.cpp:112] Iteration 173770, lr = 0.001
I0523 07:29:34.609153 35003 solver.cpp:239] Iteration 173780 (2.63395 iter/s, 3.79659s/10 iters), loss = 6.71599
I0523 07:29:34.609195 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71599 (* 1 = 6.71599 loss)
I0523 07:29:34.614354 35003 sgd_solver.cpp:112] Iteration 173780, lr = 0.001
I0523 07:29:36.588318 35003 solver.cpp:239] Iteration 173790 (5.053 iter/s, 1.97902s/10 iters), loss = 6.35241
I0523 07:29:36.588357 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35241 (* 1 = 6.35241 loss)
I0523 07:29:36.592552 35003 sgd_solver.cpp:112] Iteration 173790, lr = 0.001
I0523 07:29:39.358448 35003 solver.cpp:239] Iteration 173800 (3.61018 iter/s, 2.76995s/10 iters), loss = 6.59876
I0523 07:29:39.358494 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59876 (* 1 = 6.59876 loss)
I0523 07:29:39.369575 35003 sgd_solver.cpp:112] Iteration 173800, lr = 0.001
I0523 07:29:42.138039 35003 solver.cpp:239] Iteration 173810 (3.59787 iter/s, 2.77942s/10 iters), loss = 5.59642
I0523 07:29:42.138304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.59642 (* 1 = 5.59642 loss)
I0523 07:29:42.143077 35003 sgd_solver.cpp:112] Iteration 173810, lr = 0.001
I0523 07:29:45.618901 35003 solver.cpp:239] Iteration 173820 (2.8732 iter/s, 3.48044s/10 iters), loss = 6.35403
I0523 07:29:45.618944 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35403 (* 1 = 6.35403 loss)
I0523 07:29:45.632200 35003 sgd_solver.cpp:112] Iteration 173820, lr = 0.001
I0523 07:29:48.939757 35003 solver.cpp:239] Iteration 173830 (3.01144 iter/s, 3.32067s/10 iters), loss = 7.56311
I0523 07:29:48.939801 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56311 (* 1 = 7.56311 loss)
I0523 07:29:48.958709 35003 sgd_solver.cpp:112] Iteration 173830, lr = 0.001
I0523 07:29:54.265529 35003 solver.cpp:239] Iteration 173840 (1.87775 iter/s, 5.32551s/10 iters), loss = 6.32616
I0523 07:29:54.265576 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32616 (* 1 = 6.32616 loss)
I0523 07:29:54.273358 35003 sgd_solver.cpp:112] Iteration 173840, lr = 0.001
I0523 07:29:56.972524 35003 solver.cpp:239] Iteration 173850 (3.69435 iter/s, 2.70683s/10 iters), loss = 6.93441
I0523 07:29:56.972564 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93441 (* 1 = 6.93441 loss)
I0523 07:29:56.992347 35003 sgd_solver.cpp:112] Iteration 173850, lr = 0.001
I0523 07:30:00.838135 35003 solver.cpp:239] Iteration 173860 (2.58705 iter/s, 3.8654s/10 iters), loss = 6.50772
I0523 07:30:00.838186 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50772 (* 1 = 6.50772 loss)
I0523 07:30:01.463644 35003 sgd_solver.cpp:112] Iteration 173860, lr = 0.001
I0523 07:30:05.220669 35003 solver.cpp:239] Iteration 173870 (2.28191 iter/s, 4.3823s/10 iters), loss = 6.51112
I0523 07:30:05.220707 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51112 (* 1 = 6.51112 loss)
I0523 07:30:05.242249 35003 sgd_solver.cpp:112] Iteration 173870, lr = 0.001
I0523 07:30:08.600831 35003 solver.cpp:239] Iteration 173880 (2.9586 iter/s, 3.37997s/10 iters), loss = 6.6368
I0523 07:30:08.600893 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6368 (* 1 = 6.6368 loss)
I0523 07:30:09.334669 35003 sgd_solver.cpp:112] Iteration 173880, lr = 0.001
I0523 07:30:11.440887 35003 solver.cpp:239] Iteration 173890 (3.52129 iter/s, 2.83987s/10 iters), loss = 7.31017
I0523 07:30:11.440942 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31017 (* 1 = 7.31017 loss)
I0523 07:30:12.182030 35003 sgd_solver.cpp:112] Iteration 173890, lr = 0.001
I0523 07:30:13.927160 35003 solver.cpp:239] Iteration 173900 (4.02234 iter/s, 2.48611s/10 iters), loss = 6.30738
I0523 07:30:13.927203 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30738 (* 1 = 6.30738 loss)
I0523 07:30:14.294783 35003 sgd_solver.cpp:112] Iteration 173900, lr = 0.001
I0523 07:30:16.854197 35003 solver.cpp:239] Iteration 173910 (3.41664 iter/s, 2.92685s/10 iters), loss = 6.41081
I0523 07:30:16.854245 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41081 (* 1 = 6.41081 loss)
I0523 07:30:16.857040 35003 sgd_solver.cpp:112] Iteration 173910, lr = 0.001
I0523 07:30:21.509861 35003 solver.cpp:239] Iteration 173920 (2.14804 iter/s, 4.6554s/10 iters), loss = 6.30081
I0523 07:30:21.509920 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30081 (* 1 = 6.30081 loss)
I0523 07:30:21.516135 35003 sgd_solver.cpp:112] Iteration 173920, lr = 0.001
I0523 07:30:26.139502 35003 solver.cpp:239] Iteration 173930 (2.16011 iter/s, 4.62939s/10 iters), loss = 7.0147
I0523 07:30:26.139545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0147 (* 1 = 7.0147 loss)
I0523 07:30:26.152808 35003 sgd_solver.cpp:112] Iteration 173930, lr = 0.001
I0523 07:30:27.412879 35003 solver.cpp:239] Iteration 173940 (7.8538 iter/s, 1.27327s/10 iters), loss = 5.40659
I0523 07:30:27.412919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.40659 (* 1 = 5.40659 loss)
I0523 07:30:27.430989 35003 sgd_solver.cpp:112] Iteration 173940, lr = 0.001
I0523 07:30:30.808662 35003 solver.cpp:239] Iteration 173950 (2.94499 iter/s, 3.3956s/10 iters), loss = 6.98378
I0523 07:30:30.808708 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98378 (* 1 = 6.98378 loss)
I0523 07:30:30.814610 35003 sgd_solver.cpp:112] Iteration 173950, lr = 0.001
I0523 07:30:35.804028 35003 solver.cpp:239] Iteration 173960 (2.00196 iter/s, 4.99511s/10 iters), loss = 7.88624
I0523 07:30:35.804078 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.88624 (* 1 = 7.88624 loss)
I0523 07:30:35.821621 35003 sgd_solver.cpp:112] Iteration 173960, lr = 0.001
I0523 07:30:39.385210 35003 solver.cpp:239] Iteration 173970 (2.79253 iter/s, 3.58099s/10 iters), loss = 6.70491
I0523 07:30:39.385262 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70491 (* 1 = 6.70491 loss)
I0523 07:30:39.398723 35003 sgd_solver.cpp:112] Iteration 173970, lr = 0.001
I0523 07:30:42.680559 35003 solver.cpp:239] Iteration 173980 (3.03476 iter/s, 3.29516s/10 iters), loss = 6.41389
I0523 07:30:42.680804 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41389 (* 1 = 6.41389 loss)
I0523 07:30:42.688081 35003 sgd_solver.cpp:112] Iteration 173980, lr = 0.001
I0523 07:30:45.572218 35003 solver.cpp:239] Iteration 173990 (3.45869 iter/s, 2.89127s/10 iters), loss = 6.76422
I0523 07:30:45.572286 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76422 (* 1 = 6.76422 loss)
I0523 07:30:46.131057 35003 sgd_solver.cpp:112] Iteration 173990, lr = 0.001
I0523 07:30:49.423938 35003 solver.cpp:239] Iteration 174000 (2.59639 iter/s, 3.8515s/10 iters), loss = 6.12291
I0523 07:30:49.423979 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12291 (* 1 = 6.12291 loss)
I0523 07:30:50.139807 35003 sgd_solver.cpp:112] Iteration 174000, lr = 0.001
I0523 07:30:53.257959 35003 solver.cpp:239] Iteration 174010 (2.60837 iter/s, 3.83382s/10 iters), loss = 6.66081
I0523 07:30:53.258014 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66081 (* 1 = 6.66081 loss)
I0523 07:30:53.972987 35003 sgd_solver.cpp:112] Iteration 174010, lr = 0.001
I0523 07:30:57.320698 35003 solver.cpp:239] Iteration 174020 (2.46153 iter/s, 4.06251s/10 iters), loss = 6.82212
I0523 07:30:57.320736 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82212 (* 1 = 6.82212 loss)
I0523 07:30:57.333768 35003 sgd_solver.cpp:112] Iteration 174020, lr = 0.001
I0523 07:30:59.305266 35003 solver.cpp:239] Iteration 174030 (5.0392 iter/s, 1.98444s/10 iters), loss = 5.96135
I0523 07:30:59.305308 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96135 (* 1 = 5.96135 loss)
I0523 07:30:59.313968 35003 sgd_solver.cpp:112] Iteration 174030, lr = 0.001
I0523 07:31:00.300674 35003 solver.cpp:239] Iteration 174040 (10.0471 iter/s, 0.995315s/10 iters), loss = 7.47036
I0523 07:31:00.300725 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47036 (* 1 = 7.47036 loss)
I0523 07:31:00.312686 35003 sgd_solver.cpp:112] Iteration 174040, lr = 0.001
I0523 07:31:01.161456 35003 solver.cpp:239] Iteration 174050 (11.6186 iter/s, 0.860689s/10 iters), loss = 7.36715
I0523 07:31:01.161499 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36715 (* 1 = 7.36715 loss)
I0523 07:31:01.166987 35003 sgd_solver.cpp:112] Iteration 174050, lr = 0.001
I0523 07:31:02.043606 35003 solver.cpp:239] Iteration 174060 (11.3371 iter/s, 0.882062s/10 iters), loss = 6.76764
I0523 07:31:02.043644 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76764 (* 1 = 6.76764 loss)
I0523 07:31:02.052711 35003 sgd_solver.cpp:112] Iteration 174060, lr = 0.001
I0523 07:31:02.990258 35003 solver.cpp:239] Iteration 174070 (10.5645 iter/s, 0.946566s/10 iters), loss = 6.80073
I0523 07:31:02.990299 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80073 (* 1 = 6.80073 loss)
I0523 07:31:02.998643 35003 sgd_solver.cpp:112] Iteration 174070, lr = 0.001
I0523 07:31:04.207692 35003 solver.cpp:239] Iteration 174080 (8.21468 iter/s, 1.21733s/10 iters), loss = 6.45288
I0523 07:31:04.207733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45288 (* 1 = 6.45288 loss)
I0523 07:31:04.563328 35003 sgd_solver.cpp:112] Iteration 174080, lr = 0.001
I0523 07:31:05.752352 35003 solver.cpp:239] Iteration 174090 (6.47439 iter/s, 1.54455s/10 iters), loss = 6.80246
I0523 07:31:05.752387 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80246 (* 1 = 6.80246 loss)
I0523 07:31:05.761312 35003 sgd_solver.cpp:112] Iteration 174090, lr = 0.001
I0523 07:31:06.968256 35003 solver.cpp:239] Iteration 174100 (8.22498 iter/s, 1.21581s/10 iters), loss = 6.60081
I0523 07:31:06.968657 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60081 (* 1 = 6.60081 loss)
I0523 07:31:06.977169 35003 sgd_solver.cpp:112] Iteration 174100, lr = 0.001
I0523 07:31:07.798329 35003 solver.cpp:239] Iteration 174110 (12.0536 iter/s, 0.82963s/10 iters), loss = 5.88248
I0523 07:31:07.798386 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.88248 (* 1 = 5.88248 loss)
I0523 07:31:07.810638 35003 sgd_solver.cpp:112] Iteration 174110, lr = 0.001
I0523 07:31:08.653209 35003 solver.cpp:239] Iteration 174120 (11.6991 iter/s, 0.85477s/10 iters), loss = 5.56874
I0523 07:31:08.653246 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.56874 (* 1 = 5.56874 loss)
I0523 07:31:08.662360 35003 sgd_solver.cpp:112] Iteration 174120, lr = 0.001
I0523 07:31:09.887378 35003 solver.cpp:239] Iteration 174130 (8.10334 iter/s, 1.23406s/10 iters), loss = 6.49677
I0523 07:31:09.887437 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49677 (* 1 = 6.49677 loss)
I0523 07:31:09.893079 35003 sgd_solver.cpp:112] Iteration 174130, lr = 0.001
I0523 07:31:10.705315 35003 solver.cpp:239] Iteration 174140 (12.2274 iter/s, 0.817837s/10 iters), loss = 5.46302
I0523 07:31:10.705370 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.46302 (* 1 = 5.46302 loss)
I0523 07:31:10.720435 35003 sgd_solver.cpp:112] Iteration 174140, lr = 0.001
I0523 07:31:13.820176 35003 solver.cpp:239] Iteration 174150 (3.2106 iter/s, 3.11468s/10 iters), loss = 7.22864
I0523 07:31:13.820466 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22864 (* 1 = 7.22864 loss)
I0523 07:31:13.823763 35003 sgd_solver.cpp:112] Iteration 174150, lr = 0.001
I0523 07:31:17.454716 35003 solver.cpp:239] Iteration 174160 (2.7517 iter/s, 3.63412s/10 iters), loss = 6.98453
I0523 07:31:17.454766 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98453 (* 1 = 6.98453 loss)
I0523 07:31:17.468586 35003 sgd_solver.cpp:112] Iteration 174160, lr = 0.001
I0523 07:31:21.097477 35003 solver.cpp:239] Iteration 174170 (2.74533 iter/s, 3.64255s/10 iters), loss = 6.39167
I0523 07:31:21.097528 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39167 (* 1 = 6.39167 loss)
I0523 07:31:21.818675 35003 sgd_solver.cpp:112] Iteration 174170, lr = 0.001
I0523 07:31:24.278914 35003 solver.cpp:239] Iteration 174180 (3.14342 iter/s, 3.18125s/10 iters), loss = 6.69252
I0523 07:31:24.278957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69252 (* 1 = 6.69252 loss)
I0523 07:31:24.287067 35003 sgd_solver.cpp:112] Iteration 174180, lr = 0.001
I0523 07:31:27.085855 35003 solver.cpp:239] Iteration 174190 (3.56281 iter/s, 2.80677s/10 iters), loss = 7.22826
I0523 07:31:27.085903 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22826 (* 1 = 7.22826 loss)
I0523 07:31:27.725512 35003 sgd_solver.cpp:112] Iteration 174190, lr = 0.001
I0523 07:31:29.047847 35003 solver.cpp:239] Iteration 174200 (5.09721 iter/s, 1.96186s/10 iters), loss = 7.13433
I0523 07:31:29.047886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13433 (* 1 = 7.13433 loss)
I0523 07:31:29.057268 35003 sgd_solver.cpp:112] Iteration 174200, lr = 0.001
I0523 07:31:33.088296 35003 solver.cpp:239] Iteration 174210 (2.4751 iter/s, 4.04025s/10 iters), loss = 5.11829
I0523 07:31:33.088336 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.11829 (* 1 = 5.11829 loss)
I0523 07:31:33.797221 35003 sgd_solver.cpp:112] Iteration 174210, lr = 0.001
I0523 07:31:37.386530 35003 solver.cpp:239] Iteration 174220 (2.32666 iter/s, 4.29801s/10 iters), loss = 6.99064
I0523 07:31:37.386579 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99064 (* 1 = 6.99064 loss)
I0523 07:31:38.120784 35003 sgd_solver.cpp:112] Iteration 174220, lr = 0.001
I0523 07:31:40.991286 35003 solver.cpp:239] Iteration 174230 (2.77427 iter/s, 3.60456s/10 iters), loss = 6.80236
I0523 07:31:40.991331 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80236 (* 1 = 6.80236 loss)
I0523 07:31:40.999024 35003 sgd_solver.cpp:112] Iteration 174230, lr = 0.001
I0523 07:31:43.844739 35003 solver.cpp:239] Iteration 174240 (3.50473 iter/s, 2.85329s/10 iters), loss = 6.17981
I0523 07:31:43.844996 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17981 (* 1 = 6.17981 loss)
I0523 07:31:43.859784 35003 sgd_solver.cpp:112] Iteration 174240, lr = 0.001
I0523 07:31:47.321303 35003 solver.cpp:239] Iteration 174250 (2.87671 iter/s, 3.4762s/10 iters), loss = 5.82751
I0523 07:31:47.321346 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.82751 (* 1 = 5.82751 loss)
I0523 07:31:47.326263 35003 sgd_solver.cpp:112] Iteration 174250, lr = 0.001
I0523 07:31:52.361773 35003 solver.cpp:239] Iteration 174260 (1.98404 iter/s, 5.04021s/10 iters), loss = 5.67942
I0523 07:31:52.361832 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.67942 (* 1 = 5.67942 loss)
I0523 07:31:53.050498 35003 sgd_solver.cpp:112] Iteration 174260, lr = 0.001
I0523 07:31:55.443442 35003 solver.cpp:239] Iteration 174270 (3.24519 iter/s, 3.08148s/10 iters), loss = 6.55788
I0523 07:31:55.443490 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55788 (* 1 = 6.55788 loss)
I0523 07:31:56.156687 35003 sgd_solver.cpp:112] Iteration 174270, lr = 0.001
I0523 07:31:59.580843 35003 solver.cpp:239] Iteration 174280 (2.41711 iter/s, 4.13717s/10 iters), loss = 6.62636
I0523 07:31:59.580885 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62636 (* 1 = 6.62636 loss)
I0523 07:31:59.598357 35003 sgd_solver.cpp:112] Iteration 174280, lr = 0.001
I0523 07:32:01.693210 35003 solver.cpp:239] Iteration 174290 (4.73436 iter/s, 2.11222s/10 iters), loss = 6.95368
I0523 07:32:01.693272 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95368 (* 1 = 6.95368 loss)
I0523 07:32:02.412524 35003 sgd_solver.cpp:112] Iteration 174290, lr = 0.001
I0523 07:32:05.404644 35003 solver.cpp:239] Iteration 174300 (2.69457 iter/s, 3.71116s/10 iters), loss = 7.72266
I0523 07:32:05.404714 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.72266 (* 1 = 7.72266 loss)
I0523 07:32:06.126873 35003 sgd_solver.cpp:112] Iteration 174300, lr = 0.001
I0523 07:32:10.835188 35003 solver.cpp:239] Iteration 174310 (1.84153 iter/s, 5.43025s/10 iters), loss = 5.89929
I0523 07:32:10.835250 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89929 (* 1 = 5.89929 loss)
I0523 07:32:10.840042 35003 sgd_solver.cpp:112] Iteration 174310, lr = 0.001
I0523 07:32:12.943944 35003 solver.cpp:239] Iteration 174320 (4.74247 iter/s, 2.10861s/10 iters), loss = 6.2736
I0523 07:32:12.943991 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2736 (* 1 = 6.2736 loss)
I0523 07:32:12.963419 35003 sgd_solver.cpp:112] Iteration 174320, lr = 0.001
I0523 07:32:15.671741 35003 solver.cpp:239] Iteration 174330 (3.66618 iter/s, 2.72763s/10 iters), loss = 6.85358
I0523 07:32:15.671929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85358 (* 1 = 6.85358 loss)
I0523 07:32:16.406183 35003 sgd_solver.cpp:112] Iteration 174330, lr = 0.001
I0523 07:32:18.392984 35003 solver.cpp:239] Iteration 174340 (3.6752 iter/s, 2.72094s/10 iters), loss = 6.1226
I0523 07:32:18.393023 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1226 (* 1 = 6.1226 loss)
I0523 07:32:18.404187 35003 sgd_solver.cpp:112] Iteration 174340, lr = 0.001
I0523 07:32:22.037111 35003 solver.cpp:239] Iteration 174350 (2.74429 iter/s, 3.64394s/10 iters), loss = 6.19629
I0523 07:32:22.037165 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19629 (* 1 = 6.19629 loss)
I0523 07:32:22.046967 35003 sgd_solver.cpp:112] Iteration 174350, lr = 0.001
I0523 07:32:25.159718 35003 solver.cpp:239] Iteration 174360 (3.20264 iter/s, 3.12242s/10 iters), loss = 7.26068
I0523 07:32:25.159757 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26068 (* 1 = 7.26068 loss)
I0523 07:32:25.165451 35003 sgd_solver.cpp:112] Iteration 174360, lr = 0.001
I0523 07:32:29.471446 35003 solver.cpp:239] Iteration 174370 (2.31937 iter/s, 4.31151s/10 iters), loss = 5.40144
I0523 07:32:29.471498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.40144 (* 1 = 5.40144 loss)
I0523 07:32:29.480677 35003 sgd_solver.cpp:112] Iteration 174370, lr = 0.001
I0523 07:32:32.944991 35003 solver.cpp:239] Iteration 174380 (2.87906 iter/s, 3.47335s/10 iters), loss = 6.91343
I0523 07:32:32.945031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91343 (* 1 = 6.91343 loss)
I0523 07:32:32.964797 35003 sgd_solver.cpp:112] Iteration 174380, lr = 0.001
I0523 07:32:38.705081 35003 solver.cpp:239] Iteration 174390 (1.73617 iter/s, 5.75981s/10 iters), loss = 5.3603
I0523 07:32:38.705134 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.3603 (* 1 = 5.3603 loss)
I0523 07:32:39.420766 35003 sgd_solver.cpp:112] Iteration 174390, lr = 0.001
I0523 07:32:41.825407 35003 solver.cpp:239] Iteration 174400 (3.20498 iter/s, 3.12014s/10 iters), loss = 6.61818
I0523 07:32:41.825461 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61818 (* 1 = 6.61818 loss)
I0523 07:32:41.829208 35003 sgd_solver.cpp:112] Iteration 174400, lr = 0.001
I0523 07:32:45.431718 35003 solver.cpp:239] Iteration 174410 (2.77307 iter/s, 3.60611s/10 iters), loss = 6.25703
I0523 07:32:45.431771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25703 (* 1 = 6.25703 loss)
I0523 07:32:45.435451 35003 sgd_solver.cpp:112] Iteration 174410, lr = 0.001
I0523 07:32:49.001241 35003 solver.cpp:239] Iteration 174420 (2.80165 iter/s, 3.56932s/10 iters), loss = 6.28416
I0523 07:32:49.002388 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28416 (* 1 = 6.28416 loss)
I0523 07:32:49.560053 35003 sgd_solver.cpp:112] Iteration 174420, lr = 0.001
I0523 07:32:53.157651 35003 solver.cpp:239] Iteration 174430 (2.40667 iter/s, 4.15512s/10 iters), loss = 7.43092
I0523 07:32:53.157691 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43092 (* 1 = 7.43092 loss)
I0523 07:32:53.165310 35003 sgd_solver.cpp:112] Iteration 174430, lr = 0.001
I0523 07:32:56.870375 35003 solver.cpp:239] Iteration 174440 (2.69358 iter/s, 3.71253s/10 iters), loss = 6.65677
I0523 07:32:56.870429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65677 (* 1 = 6.65677 loss)
I0523 07:32:56.879382 35003 sgd_solver.cpp:112] Iteration 174440, lr = 0.001
I0523 07:33:01.158922 35003 solver.cpp:239] Iteration 174450 (2.33192 iter/s, 4.28831s/10 iters), loss = 5.69426
I0523 07:33:01.158977 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.69426 (* 1 = 5.69426 loss)
I0523 07:33:01.164295 35003 sgd_solver.cpp:112] Iteration 174450, lr = 0.001
I0523 07:33:04.193773 35003 solver.cpp:239] Iteration 174460 (3.29526 iter/s, 3.03466s/10 iters), loss = 6.83901
I0523 07:33:04.193825 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83901 (* 1 = 6.83901 loss)
I0523 07:33:04.220108 35003 sgd_solver.cpp:112] Iteration 174460, lr = 0.001
I0523 07:33:06.712853 35003 solver.cpp:239] Iteration 174470 (3.96996 iter/s, 2.51891s/10 iters), loss = 5.81502
I0523 07:33:06.712904 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.81502 (* 1 = 5.81502 loss)
I0523 07:33:07.453820 35003 sgd_solver.cpp:112] Iteration 174470, lr = 0.001
I0523 07:33:11.148597 35003 solver.cpp:239] Iteration 174480 (2.25453 iter/s, 4.43551s/10 iters), loss = 8.35185
I0523 07:33:11.148639 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.35185 (* 1 = 8.35185 loss)
I0523 07:33:11.288399 35003 sgd_solver.cpp:112] Iteration 174480, lr = 0.001
I0523 07:33:13.000804 35003 solver.cpp:239] Iteration 174490 (5.39934 iter/s, 1.85208s/10 iters), loss = 6.29323
I0523 07:33:13.000849 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29323 (* 1 = 6.29323 loss)
I0523 07:33:13.741755 35003 sgd_solver.cpp:112] Iteration 174490, lr = 0.001
I0523 07:33:16.554628 35003 solver.cpp:239] Iteration 174500 (2.81402 iter/s, 3.55363s/10 iters), loss = 5.49293
I0523 07:33:16.554679 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.49293 (* 1 = 5.49293 loss)
I0523 07:33:16.572530 35003 sgd_solver.cpp:112] Iteration 174500, lr = 0.001
I0523 07:33:20.297518 35003 solver.cpp:239] Iteration 174510 (2.67188 iter/s, 3.74268s/10 iters), loss = 7.11022
I0523 07:33:20.297646 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11022 (* 1 = 7.11022 loss)
I0523 07:33:20.299119 35003 sgd_solver.cpp:112] Iteration 174510, lr = 0.001
I0523 07:33:24.927403 35003 solver.cpp:239] Iteration 174520 (2.1601 iter/s, 4.62942s/10 iters), loss = 7.23352
I0523 07:33:24.927451 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23352 (* 1 = 7.23352 loss)
I0523 07:33:25.029665 35003 sgd_solver.cpp:112] Iteration 174520, lr = 0.001
I0523 07:33:28.514389 35003 solver.cpp:239] Iteration 174530 (2.78801 iter/s, 3.58678s/10 iters), loss = 6.14685
I0523 07:33:28.514452 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14685 (* 1 = 6.14685 loss)
I0523 07:33:29.246523 35003 sgd_solver.cpp:112] Iteration 174530, lr = 0.001
I0523 07:33:33.574790 35003 solver.cpp:239] Iteration 174540 (1.97623 iter/s, 5.06014s/10 iters), loss = 6.48644
I0523 07:33:33.574834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48644 (* 1 = 6.48644 loss)
I0523 07:33:33.603849 35003 sgd_solver.cpp:112] Iteration 174540, lr = 0.001
I0523 07:33:37.132529 35003 solver.cpp:239] Iteration 174550 (2.81093 iter/s, 3.55754s/10 iters), loss = 8.26061
I0523 07:33:37.132575 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.26061 (* 1 = 8.26061 loss)
I0523 07:33:37.848007 35003 sgd_solver.cpp:112] Iteration 174550, lr = 0.001
I0523 07:33:40.003742 35003 solver.cpp:239] Iteration 174560 (3.48308 iter/s, 2.87102s/10 iters), loss = 6.48371
I0523 07:33:40.003782 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48371 (* 1 = 6.48371 loss)
I0523 07:33:40.662593 35003 sgd_solver.cpp:112] Iteration 174560, lr = 0.001
I0523 07:33:44.204277 35003 solver.cpp:239] Iteration 174570 (2.38078 iter/s, 4.20031s/10 iters), loss = 7.43413
I0523 07:33:44.204322 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43413 (* 1 = 7.43413 loss)
I0523 07:33:44.215922 35003 sgd_solver.cpp:112] Iteration 174570, lr = 0.001
I0523 07:33:48.470145 35003 solver.cpp:239] Iteration 174580 (2.34431 iter/s, 4.26565s/10 iters), loss = 7.17749
I0523 07:33:48.470206 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17749 (* 1 = 7.17749 loss)
I0523 07:33:48.483201 35003 sgd_solver.cpp:112] Iteration 174580, lr = 0.001
I0523 07:33:51.802541 35003 solver.cpp:239] Iteration 174590 (3.00102 iter/s, 3.3322s/10 iters), loss = 5.4272
I0523 07:33:51.802759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.4272 (* 1 = 5.4272 loss)
I0523 07:33:52.234060 35003 sgd_solver.cpp:112] Iteration 174590, lr = 0.001
I0523 07:33:54.304014 35003 solver.cpp:239] Iteration 174600 (3.99816 iter/s, 2.50115s/10 iters), loss = 7.19807
I0523 07:33:54.304059 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19807 (* 1 = 7.19807 loss)
I0523 07:33:54.316220 35003 sgd_solver.cpp:112] Iteration 174600, lr = 0.001
I0523 07:33:56.187873 35003 solver.cpp:239] Iteration 174610 (5.30862 iter/s, 1.88373s/10 iters), loss = 6.59529
I0523 07:33:56.187916 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59529 (* 1 = 6.59529 loss)
I0523 07:33:56.203845 35003 sgd_solver.cpp:112] Iteration 174610, lr = 0.001
I0523 07:33:59.051440 35003 solver.cpp:239] Iteration 174620 (3.49235 iter/s, 2.8634s/10 iters), loss = 7.44787
I0523 07:33:59.051486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44787 (* 1 = 7.44787 loss)
I0523 07:33:59.060864 35003 sgd_solver.cpp:112] Iteration 174620, lr = 0.001
I0523 07:34:01.437324 35003 solver.cpp:239] Iteration 174630 (4.19159 iter/s, 2.38573s/10 iters), loss = 5.44601
I0523 07:34:01.437366 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.44601 (* 1 = 5.44601 loss)
I0523 07:34:01.450273 35003 sgd_solver.cpp:112] Iteration 174630, lr = 0.001
I0523 07:34:04.652663 35003 solver.cpp:239] Iteration 174640 (3.11026 iter/s, 3.21516s/10 iters), loss = 7.78032
I0523 07:34:04.652710 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78032 (* 1 = 7.78032 loss)
I0523 07:34:05.387948 35003 sgd_solver.cpp:112] Iteration 174640, lr = 0.001
I0523 07:34:09.855587 35003 solver.cpp:239] Iteration 174650 (1.9221 iter/s, 5.20263s/10 iters), loss = 6.37831
I0523 07:34:09.855643 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37831 (* 1 = 6.37831 loss)
I0523 07:34:09.959643 35003 sgd_solver.cpp:112] Iteration 174650, lr = 0.001
I0523 07:34:11.353636 35003 solver.cpp:239] Iteration 174660 (6.6759 iter/s, 1.49792s/10 iters), loss = 6.69055
I0523 07:34:11.353690 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69055 (* 1 = 6.69055 loss)
I0523 07:34:12.075428 35003 sgd_solver.cpp:112] Iteration 174660, lr = 0.001
I0523 07:34:16.088066 35003 solver.cpp:239] Iteration 174670 (2.1123 iter/s, 4.73418s/10 iters), loss = 6.36244
I0523 07:34:16.088112 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36244 (* 1 = 6.36244 loss)
I0523 07:34:16.803571 35003 sgd_solver.cpp:112] Iteration 174670, lr = 0.001
I0523 07:34:21.622608 35003 solver.cpp:239] Iteration 174680 (1.80692 iter/s, 5.53428s/10 iters), loss = 6.36741
I0523 07:34:21.622648 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36741 (* 1 = 6.36741 loss)
I0523 07:34:21.644162 35003 sgd_solver.cpp:112] Iteration 174680, lr = 0.001
I0523 07:34:25.181883 35003 solver.cpp:239] Iteration 174690 (2.80972 iter/s, 3.55908s/10 iters), loss = 6.48844
I0523 07:34:25.182160 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48844 (* 1 = 6.48844 loss)
I0523 07:34:25.400677 35003 sgd_solver.cpp:112] Iteration 174690, lr = 0.001
I0523 07:34:29.638473 35003 solver.cpp:239] Iteration 174700 (2.2441 iter/s, 4.45614s/10 iters), loss = 6.77292
I0523 07:34:29.638520 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77292 (* 1 = 6.77292 loss)
I0523 07:34:29.658591 35003 sgd_solver.cpp:112] Iteration 174700, lr = 0.001
I0523 07:34:32.561866 35003 solver.cpp:239] Iteration 174710 (3.42089 iter/s, 2.92322s/10 iters), loss = 5.80594
I0523 07:34:32.561905 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.80594 (* 1 = 5.80594 loss)
I0523 07:34:32.567229 35003 sgd_solver.cpp:112] Iteration 174710, lr = 0.001
I0523 07:34:36.838030 35003 solver.cpp:239] Iteration 174720 (2.33867 iter/s, 4.27594s/10 iters), loss = 6.12788
I0523 07:34:36.838080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12788 (* 1 = 6.12788 loss)
I0523 07:34:36.845571 35003 sgd_solver.cpp:112] Iteration 174720, lr = 0.001
I0523 07:34:39.714722 35003 solver.cpp:239] Iteration 174730 (3.47647 iter/s, 2.87648s/10 iters), loss = 6.62503
I0523 07:34:39.714768 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62503 (* 1 = 6.62503 loss)
I0523 07:34:39.717584 35003 sgd_solver.cpp:112] Iteration 174730, lr = 0.001
I0523 07:34:42.196442 35003 solver.cpp:239] Iteration 174740 (4.02976 iter/s, 2.48154s/10 iters), loss = 6.66196
I0523 07:34:42.196485 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66196 (* 1 = 6.66196 loss)
I0523 07:34:42.206358 35003 sgd_solver.cpp:112] Iteration 174740, lr = 0.001
I0523 07:34:46.930394 35003 solver.cpp:239] Iteration 174750 (2.1125 iter/s, 4.73372s/10 iters), loss = 6.05143
I0523 07:34:46.930441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05143 (* 1 = 6.05143 loss)
I0523 07:34:47.606127 35003 sgd_solver.cpp:112] Iteration 174750, lr = 0.001
I0523 07:34:51.763859 35003 solver.cpp:239] Iteration 174760 (2.06902 iter/s, 4.83322s/10 iters), loss = 6.99599
I0523 07:34:51.763906 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99599 (* 1 = 6.99599 loss)
I0523 07:34:51.782217 35003 sgd_solver.cpp:112] Iteration 174760, lr = 0.001
I0523 07:34:54.203876 35003 solver.cpp:239] Iteration 174770 (4.09858 iter/s, 2.43987s/10 iters), loss = 6.12602
I0523 07:34:54.203922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12602 (* 1 = 6.12602 loss)
I0523 07:34:54.213408 35003 sgd_solver.cpp:112] Iteration 174770, lr = 0.001
I0523 07:34:57.674688 35003 solver.cpp:239] Iteration 174780 (2.88133 iter/s, 3.47061s/10 iters), loss = 6.93048
I0523 07:34:57.674943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93048 (* 1 = 6.93048 loss)
I0523 07:34:58.310241 35003 sgd_solver.cpp:112] Iteration 174780, lr = 0.001
I0523 07:35:02.747560 35003 solver.cpp:239] Iteration 174790 (1.97144 iter/s, 5.07243s/10 iters), loss = 6.87581
I0523 07:35:02.747611 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87581 (* 1 = 6.87581 loss)
I0523 07:35:03.453469 35003 sgd_solver.cpp:112] Iteration 174790, lr = 0.001
I0523 07:35:06.971127 35003 solver.cpp:239] Iteration 174800 (2.36779 iter/s, 4.22334s/10 iters), loss = 6.28261
I0523 07:35:06.971169 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28261 (* 1 = 6.28261 loss)
I0523 07:35:06.984284 35003 sgd_solver.cpp:112] Iteration 174800, lr = 0.001
I0523 07:35:08.394857 35003 solver.cpp:239] Iteration 174810 (7.02434 iter/s, 1.42362s/10 iters), loss = 5.33607
I0523 07:35:08.394992 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.33607 (* 1 = 5.33607 loss)
I0523 07:35:08.417320 35003 sgd_solver.cpp:112] Iteration 174810, lr = 0.001
I0523 07:35:11.535784 35003 solver.cpp:239] Iteration 174820 (3.18404 iter/s, 3.14066s/10 iters), loss = 5.87965
I0523 07:35:11.535833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87965 (* 1 = 5.87965 loss)
I0523 07:35:11.543584 35003 sgd_solver.cpp:112] Iteration 174820, lr = 0.001
I0523 07:35:14.532344 35003 solver.cpp:239] Iteration 174830 (3.33736 iter/s, 2.99638s/10 iters), loss = 7.01038
I0523 07:35:14.532384 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01038 (* 1 = 7.01038 loss)
I0523 07:35:14.545145 35003 sgd_solver.cpp:112] Iteration 174830, lr = 0.001
I0523 07:35:18.133965 35003 solver.cpp:239] Iteration 174840 (2.77667 iter/s, 3.60143s/10 iters), loss = 7.00891
I0523 07:35:18.134016 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00891 (* 1 = 7.00891 loss)
I0523 07:35:18.142030 35003 sgd_solver.cpp:112] Iteration 174840, lr = 0.001
I0523 07:35:21.815135 35003 solver.cpp:239] Iteration 174850 (2.71668 iter/s, 3.68097s/10 iters), loss = 6.21124
I0523 07:35:21.815171 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21124 (* 1 = 6.21124 loss)
I0523 07:35:21.828969 35003 sgd_solver.cpp:112] Iteration 174850, lr = 0.001
I0523 07:35:25.549120 35003 solver.cpp:239] Iteration 174860 (2.67825 iter/s, 3.73379s/10 iters), loss = 5.66188
I0523 07:35:25.549177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.66188 (* 1 = 5.66188 loss)
I0523 07:35:26.290078 35003 sgd_solver.cpp:112] Iteration 174860, lr = 0.001
I0523 07:35:29.034291 35003 solver.cpp:239] Iteration 174870 (2.86947 iter/s, 3.48497s/10 iters), loss = 6.48725
I0523 07:35:29.034588 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48725 (* 1 = 6.48725 loss)
I0523 07:35:29.044687 35003 sgd_solver.cpp:112] Iteration 174870, lr = 0.001
I0523 07:35:31.797446 35003 solver.cpp:239] Iteration 174880 (3.61956 iter/s, 2.76277s/10 iters), loss = 6.54402
I0523 07:35:31.797492 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54402 (* 1 = 6.54402 loss)
I0523 07:35:32.506629 35003 sgd_solver.cpp:112] Iteration 174880, lr = 0.001
I0523 07:35:34.620573 35003 solver.cpp:239] Iteration 174890 (3.54238 iter/s, 2.82296s/10 iters), loss = 5.9233
I0523 07:35:34.620616 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9233 (* 1 = 5.9233 loss)
I0523 07:35:34.634382 35003 sgd_solver.cpp:112] Iteration 174890, lr = 0.001
I0523 07:35:38.190099 35003 solver.cpp:239] Iteration 174900 (2.80166 iter/s, 3.56931s/10 iters), loss = 6.88178
I0523 07:35:38.190141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88178 (* 1 = 6.88178 loss)
I0523 07:35:38.207834 35003 sgd_solver.cpp:112] Iteration 174900, lr = 0.001
I0523 07:35:40.535023 35003 solver.cpp:239] Iteration 174910 (4.26489 iter/s, 2.34472s/10 iters), loss = 6.12362
I0523 07:35:40.535060 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12362 (* 1 = 6.12362 loss)
I0523 07:35:40.548043 35003 sgd_solver.cpp:112] Iteration 174910, lr = 0.001
I0523 07:35:42.644527 35003 solver.cpp:239] Iteration 174920 (4.74076 iter/s, 2.10937s/10 iters), loss = 5.91858
I0523 07:35:42.644578 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91858 (* 1 = 5.91858 loss)
I0523 07:35:42.652181 35003 sgd_solver.cpp:112] Iteration 174920, lr = 0.001
I0523 07:35:45.732960 35003 solver.cpp:239] Iteration 174930 (3.23808 iter/s, 3.08825s/10 iters), loss = 6.05414
I0523 07:35:45.733008 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05414 (* 1 = 6.05414 loss)
I0523 07:35:46.429263 35003 sgd_solver.cpp:112] Iteration 174930, lr = 0.001
I0523 07:35:48.753765 35003 solver.cpp:239] Iteration 174940 (3.31057 iter/s, 3.02063s/10 iters), loss = 6.32829
I0523 07:35:48.753813 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32829 (* 1 = 6.32829 loss)
I0523 07:35:49.384316 35003 sgd_solver.cpp:112] Iteration 174940, lr = 0.001
I0523 07:35:51.506898 35003 solver.cpp:239] Iteration 174950 (3.63245 iter/s, 2.75296s/10 iters), loss = 6.8209
I0523 07:35:51.506943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8209 (* 1 = 6.8209 loss)
I0523 07:35:52.215337 35003 sgd_solver.cpp:112] Iteration 174950, lr = 0.001
I0523 07:35:54.883533 35003 solver.cpp:239] Iteration 174960 (2.96169 iter/s, 3.37645s/10 iters), loss = 6.61371
I0523 07:35:54.883581 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61371 (* 1 = 6.61371 loss)
I0523 07:35:55.506361 35003 sgd_solver.cpp:112] Iteration 174960, lr = 0.001
I0523 07:35:57.593902 35003 solver.cpp:239] Iteration 174970 (3.68976 iter/s, 2.7102s/10 iters), loss = 6.57595
I0523 07:35:57.593961 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57595 (* 1 = 6.57595 loss)
I0523 07:35:58.334851 35003 sgd_solver.cpp:112] Iteration 174970, lr = 0.001
I0523 07:36:00.405174 35003 solver.cpp:239] Iteration 174980 (3.55733 iter/s, 2.81109s/10 iters), loss = 6.36672
I0523 07:36:00.405421 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36672 (* 1 = 6.36672 loss)
I0523 07:36:00.418411 35003 sgd_solver.cpp:112] Iteration 174980, lr = 0.001
I0523 07:36:04.536522 35003 solver.cpp:239] Iteration 174990 (2.42075 iter/s, 4.13096s/10 iters), loss = 6.8524
I0523 07:36:04.536567 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8524 (* 1 = 6.8524 loss)
I0523 07:36:04.549356 35003 sgd_solver.cpp:112] Iteration 174990, lr = 0.001
I0523 07:36:08.086714 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_175000.caffemodel
I0523 07:36:08.566836 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_175000.solverstate
I0523 07:36:08.707379 35003 solver.cpp:239] Iteration 175000 (2.39771 iter/s, 4.17064s/10 iters), loss = 7.59226
I0523 07:36:08.707423 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59226 (* 1 = 7.59226 loss)
I0523 07:36:09.416539 35003 sgd_solver.cpp:112] Iteration 175000, lr = 0.001
I0523 07:36:11.462798 35003 solver.cpp:239] Iteration 175010 (3.62943 iter/s, 2.75525s/10 iters), loss = 6.51329
I0523 07:36:11.462852 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51329 (* 1 = 6.51329 loss)
I0523 07:36:11.587502 35003 sgd_solver.cpp:112] Iteration 175010, lr = 0.001
I0523 07:36:14.122033 35003 solver.cpp:239] Iteration 175020 (3.76072 iter/s, 2.65907s/10 iters), loss = 7.81973
I0523 07:36:14.122073 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81973 (* 1 = 7.81973 loss)
I0523 07:36:14.127900 35003 sgd_solver.cpp:112] Iteration 175020, lr = 0.001
I0523 07:36:16.059271 35003 solver.cpp:239] Iteration 175030 (5.16233 iter/s, 1.93711s/10 iters), loss = 6.39243
I0523 07:36:16.059314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39243 (* 1 = 6.39243 loss)
I0523 07:36:16.072847 35003 sgd_solver.cpp:112] Iteration 175030, lr = 0.001
I0523 07:36:21.090732 35003 solver.cpp:239] Iteration 175040 (1.9876 iter/s, 5.03119s/10 iters), loss = 7.62849
I0523 07:36:21.090786 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62849 (* 1 = 7.62849 loss)
I0523 07:36:21.094658 35003 sgd_solver.cpp:112] Iteration 175040, lr = 0.001
I0523 07:36:24.097756 35003 solver.cpp:239] Iteration 175050 (3.32584 iter/s, 3.00676s/10 iters), loss = 7.14841
I0523 07:36:24.097810 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14841 (* 1 = 7.14841 loss)
I0523 07:36:24.767907 35003 sgd_solver.cpp:112] Iteration 175050, lr = 0.001
I0523 07:36:26.788081 35003 solver.cpp:239] Iteration 175060 (3.71726 iter/s, 2.69016s/10 iters), loss = 6.26293
I0523 07:36:26.788120 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26293 (* 1 = 6.26293 loss)
I0523 07:36:26.800914 35003 sgd_solver.cpp:112] Iteration 175060, lr = 0.001
I0523 07:36:30.199005 35003 solver.cpp:239] Iteration 175070 (2.93192 iter/s, 3.41073s/10 iters), loss = 7.24833
I0523 07:36:30.199066 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24833 (* 1 = 7.24833 loss)
I0523 07:36:30.211130 35003 sgd_solver.cpp:112] Iteration 175070, lr = 0.001
I0523 07:36:33.905663 35003 solver.cpp:239] Iteration 175080 (2.69801 iter/s, 3.70644s/10 iters), loss = 5.45741
I0523 07:36:33.905936 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.45741 (* 1 = 5.45741 loss)
I0523 07:36:33.910149 35003 sgd_solver.cpp:112] Iteration 175080, lr = 0.001
I0523 07:36:38.243067 35003 solver.cpp:239] Iteration 175090 (2.30575 iter/s, 4.33698s/10 iters), loss = 7.63997
I0523 07:36:38.243106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63997 (* 1 = 7.63997 loss)
I0523 07:36:38.256824 35003 sgd_solver.cpp:112] Iteration 175090, lr = 0.001
I0523 07:36:41.349510 35003 solver.cpp:239] Iteration 175100 (3.21929 iter/s, 3.10627s/10 iters), loss = 7.1693
I0523 07:36:41.349553 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1693 (* 1 = 7.1693 loss)
I0523 07:36:41.363217 35003 sgd_solver.cpp:112] Iteration 175100, lr = 0.001
I0523 07:36:44.101635 35003 solver.cpp:239] Iteration 175110 (3.63377 iter/s, 2.75197s/10 iters), loss = 8.41392
I0523 07:36:44.101680 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.41392 (* 1 = 8.41392 loss)
I0523 07:36:44.111285 35003 sgd_solver.cpp:112] Iteration 175110, lr = 0.001
I0523 07:36:46.341389 35003 solver.cpp:239] Iteration 175120 (4.46507 iter/s, 2.23961s/10 iters), loss = 6.89872
I0523 07:36:46.341439 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89872 (* 1 = 6.89872 loss)
I0523 07:36:47.014228 35003 sgd_solver.cpp:112] Iteration 175120, lr = 0.001
I0523 07:36:49.144001 35003 solver.cpp:239] Iteration 175130 (3.56831 iter/s, 2.80244s/10 iters), loss = 6.76839
I0523 07:36:49.144043 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76839 (* 1 = 6.76839 loss)
I0523 07:36:49.872509 35003 sgd_solver.cpp:112] Iteration 175130, lr = 0.001
I0523 07:36:51.833393 35003 solver.cpp:239] Iteration 175140 (3.71854 iter/s, 2.68923s/10 iters), loss = 6.29326
I0523 07:36:51.833433 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29326 (* 1 = 6.29326 loss)
I0523 07:36:51.852406 35003 sgd_solver.cpp:112] Iteration 175140, lr = 0.001
I0523 07:36:54.498324 35003 solver.cpp:239] Iteration 175150 (3.75267 iter/s, 2.66477s/10 iters), loss = 6.72814
I0523 07:36:54.498376 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72814 (* 1 = 6.72814 loss)
I0523 07:36:55.238977 35003 sgd_solver.cpp:112] Iteration 175150, lr = 0.001
I0523 07:36:58.014521 35003 solver.cpp:239] Iteration 175160 (2.84414 iter/s, 3.516s/10 iters), loss = 7.18807
I0523 07:36:58.014556 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18807 (* 1 = 7.18807 loss)
I0523 07:36:58.027330 35003 sgd_solver.cpp:112] Iteration 175160, lr = 0.001
I0523 07:37:01.553249 35003 solver.cpp:239] Iteration 175170 (2.82602 iter/s, 3.53855s/10 iters), loss = 6.50907
I0523 07:37:01.553290 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50907 (* 1 = 6.50907 loss)
I0523 07:37:01.740574 35003 sgd_solver.cpp:112] Iteration 175170, lr = 0.001
I0523 07:37:06.201653 35003 solver.cpp:239] Iteration 175180 (2.15139 iter/s, 4.64817s/10 iters), loss = 6.86491
I0523 07:37:06.201846 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86491 (* 1 = 6.86491 loss)
I0523 07:37:06.208364 35003 sgd_solver.cpp:112] Iteration 175180, lr = 0.001
I0523 07:37:09.693765 35003 solver.cpp:239] Iteration 175190 (2.86387 iter/s, 3.49178s/10 iters), loss = 5.97783
I0523 07:37:09.693805 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97783 (* 1 = 5.97783 loss)
I0523 07:37:10.416072 35003 sgd_solver.cpp:112] Iteration 175190, lr = 0.001
I0523 07:37:15.231865 35003 solver.cpp:239] Iteration 175200 (1.80576 iter/s, 5.53783s/10 iters), loss = 6.8156
I0523 07:37:15.231927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8156 (* 1 = 6.8156 loss)
I0523 07:37:15.253387 35003 sgd_solver.cpp:112] Iteration 175200, lr = 0.001
I0523 07:37:19.898401 35003 solver.cpp:239] Iteration 175210 (2.14303 iter/s, 4.66629s/10 iters), loss = 6.78689
I0523 07:37:19.898442 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78689 (* 1 = 6.78689 loss)
I0523 07:37:20.604579 35003 sgd_solver.cpp:112] Iteration 175210, lr = 0.001
I0523 07:37:24.656487 35003 solver.cpp:239] Iteration 175220 (2.10179 iter/s, 4.75785s/10 iters), loss = 5.65923
I0523 07:37:24.656536 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.65923 (* 1 = 5.65923 loss)
I0523 07:37:24.669378 35003 sgd_solver.cpp:112] Iteration 175220, lr = 0.001
I0523 07:37:27.552666 35003 solver.cpp:239] Iteration 175230 (3.45303 iter/s, 2.89601s/10 iters), loss = 6.99484
I0523 07:37:27.552712 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99484 (* 1 = 6.99484 loss)
I0523 07:37:27.561198 35003 sgd_solver.cpp:112] Iteration 175230, lr = 0.001
I0523 07:37:31.578588 35003 solver.cpp:239] Iteration 175240 (2.48404 iter/s, 4.02571s/10 iters), loss = 6.9003
I0523 07:37:31.578630 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9003 (* 1 = 6.9003 loss)
I0523 07:37:31.590988 35003 sgd_solver.cpp:112] Iteration 175240, lr = 0.001
I0523 07:37:36.974359 35003 solver.cpp:239] Iteration 175250 (1.85339 iter/s, 5.39551s/10 iters), loss = 6.8525
I0523 07:37:36.974633 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8525 (* 1 = 6.8525 loss)
I0523 07:37:36.976722 35003 sgd_solver.cpp:112] Iteration 175250, lr = 0.001
I0523 07:37:40.819623 35003 solver.cpp:239] Iteration 175260 (2.60089 iter/s, 3.84484s/10 iters), loss = 4.35637
I0523 07:37:40.819663 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.35637 (* 1 = 4.35637 loss)
I0523 07:37:41.535032 35003 sgd_solver.cpp:112] Iteration 175260, lr = 0.001
I0523 07:37:45.933568 35003 solver.cpp:239] Iteration 175270 (1.95553 iter/s, 5.1137s/10 iters), loss = 7.07522
I0523 07:37:45.933607 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07522 (* 1 = 7.07522 loss)
I0523 07:37:45.946223 35003 sgd_solver.cpp:112] Iteration 175270, lr = 0.001
I0523 07:37:47.895601 35003 solver.cpp:239] Iteration 175280 (5.09709 iter/s, 1.9619s/10 iters), loss = 6.9387
I0523 07:37:47.895655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9387 (* 1 = 6.9387 loss)
I0523 07:37:47.900799 35003 sgd_solver.cpp:112] Iteration 175280, lr = 0.001
I0523 07:37:52.073261 35003 solver.cpp:239] Iteration 175290 (2.39381 iter/s, 4.17744s/10 iters), loss = 7.63593
I0523 07:37:52.073302 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63593 (* 1 = 7.63593 loss)
I0523 07:37:52.711799 35003 sgd_solver.cpp:112] Iteration 175290, lr = 0.001
I0523 07:37:56.453806 35003 solver.cpp:239] Iteration 175300 (2.28294 iter/s, 4.38032s/10 iters), loss = 6.9253
I0523 07:37:56.453855 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9253 (* 1 = 6.9253 loss)
I0523 07:37:56.466835 35003 sgd_solver.cpp:112] Iteration 175300, lr = 0.001
I0523 07:37:59.907210 35003 solver.cpp:239] Iteration 175310 (2.89586 iter/s, 3.45321s/10 iters), loss = 6.93757
I0523 07:37:59.907259 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93757 (* 1 = 6.93757 loss)
I0523 07:37:59.920337 35003 sgd_solver.cpp:112] Iteration 175310, lr = 0.001
I0523 07:38:03.663501 35003 solver.cpp:239] Iteration 175320 (2.66235 iter/s, 3.75608s/10 iters), loss = 6.32133
I0523 07:38:03.663554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32133 (* 1 = 6.32133 loss)
I0523 07:38:04.404456 35003 sgd_solver.cpp:112] Iteration 175320, lr = 0.001
I0523 07:38:06.583487 35003 solver.cpp:239] Iteration 175330 (3.42488 iter/s, 2.91981s/10 iters), loss = 6.06126
I0523 07:38:06.583526 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06126 (* 1 = 6.06126 loss)
I0523 07:38:06.974400 35003 sgd_solver.cpp:112] Iteration 175330, lr = 0.001
I0523 07:38:09.800962 35003 solver.cpp:239] Iteration 175340 (3.10821 iter/s, 3.21729s/10 iters), loss = 7.33667
I0523 07:38:09.801023 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33667 (* 1 = 7.33667 loss)
I0523 07:38:10.336549 35003 sgd_solver.cpp:112] Iteration 175340, lr = 0.001
I0523 07:38:11.643929 35003 solver.cpp:239] Iteration 175350 (5.42647 iter/s, 1.84282s/10 iters), loss = 5.2672
I0523 07:38:11.643972 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.2672 (* 1 = 5.2672 loss)
I0523 07:38:11.653683 35003 sgd_solver.cpp:112] Iteration 175350, lr = 0.001
I0523 07:38:15.843744 35003 solver.cpp:239] Iteration 175360 (2.38118 iter/s, 4.19959s/10 iters), loss = 7.14414
I0523 07:38:15.843806 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14414 (* 1 = 7.14414 loss)
I0523 07:38:15.865933 35003 sgd_solver.cpp:112] Iteration 175360, lr = 0.001
I0523 07:38:18.127027 35003 solver.cpp:239] Iteration 175370 (4.37998 iter/s, 2.28311s/10 iters), loss = 5.58196
I0523 07:38:18.127074 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.58196 (* 1 = 5.58196 loss)
I0523 07:38:18.131769 35003 sgd_solver.cpp:112] Iteration 175370, lr = 0.001
I0523 07:38:21.690220 35003 solver.cpp:239] Iteration 175380 (2.80663 iter/s, 3.56299s/10 iters), loss = 6.97593
I0523 07:38:21.690271 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97593 (* 1 = 6.97593 loss)
I0523 07:38:21.698835 35003 sgd_solver.cpp:112] Iteration 175380, lr = 0.001
I0523 07:38:25.185535 35003 solver.cpp:239] Iteration 175390 (2.86114 iter/s, 3.49512s/10 iters), loss = 6.01966
I0523 07:38:25.185587 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01966 (* 1 = 6.01966 loss)
I0523 07:38:25.192628 35003 sgd_solver.cpp:112] Iteration 175390, lr = 0.001
I0523 07:38:28.095305 35003 solver.cpp:239] Iteration 175400 (3.4369 iter/s, 2.9096s/10 iters), loss = 5.75171
I0523 07:38:28.095350 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.75171 (* 1 = 5.75171 loss)
I0523 07:38:28.714857 35003 sgd_solver.cpp:112] Iteration 175400, lr = 0.001
I0523 07:38:33.135905 35003 solver.cpp:239] Iteration 175410 (1.98399 iter/s, 5.04034s/10 iters), loss = 7.58874
I0523 07:38:33.135962 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.58874 (* 1 = 7.58874 loss)
I0523 07:38:33.870288 35003 sgd_solver.cpp:112] Iteration 175410, lr = 0.001
I0523 07:38:38.159121 35003 solver.cpp:239] Iteration 175420 (1.99086 iter/s, 5.02295s/10 iters), loss = 6.27687
I0523 07:38:38.159281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27687 (* 1 = 6.27687 loss)
I0523 07:38:38.172665 35003 sgd_solver.cpp:112] Iteration 175420, lr = 0.001
I0523 07:38:43.881459 35003 solver.cpp:239] Iteration 175430 (1.74765 iter/s, 5.72196s/10 iters), loss = 7.3186
I0523 07:38:43.881501 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3186 (* 1 = 7.3186 loss)
I0523 07:38:43.907882 35003 sgd_solver.cpp:112] Iteration 175430, lr = 0.001
I0523 07:38:48.193584 35003 solver.cpp:239] Iteration 175440 (2.31916 iter/s, 4.31191s/10 iters), loss = 7.28842
I0523 07:38:48.193629 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28842 (* 1 = 7.28842 loss)
I0523 07:38:48.228200 35003 sgd_solver.cpp:112] Iteration 175440, lr = 0.001
I0523 07:38:51.663476 35003 solver.cpp:239] Iteration 175450 (2.88209 iter/s, 3.4697s/10 iters), loss = 6.6207
I0523 07:38:51.663530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6207 (* 1 = 6.6207 loss)
I0523 07:38:51.675508 35003 sgd_solver.cpp:112] Iteration 175450, lr = 0.001
I0523 07:38:54.545859 35003 solver.cpp:239] Iteration 175460 (3.46956 iter/s, 2.88221s/10 iters), loss = 6.79907
I0523 07:38:54.545902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79907 (* 1 = 6.79907 loss)
I0523 07:38:54.558715 35003 sgd_solver.cpp:112] Iteration 175460, lr = 0.001
I0523 07:38:56.473757 35003 solver.cpp:239] Iteration 175470 (5.18734 iter/s, 1.92777s/10 iters), loss = 6.05922
I0523 07:38:56.473799 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05922 (* 1 = 6.05922 loss)
I0523 07:38:57.214686 35003 sgd_solver.cpp:112] Iteration 175470, lr = 0.001
I0523 07:39:00.722951 35003 solver.cpp:239] Iteration 175480 (2.35351 iter/s, 4.24898s/10 iters), loss = 6.62112
I0523 07:39:00.722993 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62112 (* 1 = 6.62112 loss)
I0523 07:39:01.075837 35003 sgd_solver.cpp:112] Iteration 175480, lr = 0.001
I0523 07:39:03.922508 35003 solver.cpp:239] Iteration 175490 (3.12561 iter/s, 3.19938s/10 iters), loss = 6.6489
I0523 07:39:03.922547 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6489 (* 1 = 6.6489 loss)
I0523 07:39:03.938550 35003 sgd_solver.cpp:112] Iteration 175490, lr = 0.001
I0523 07:39:08.257962 35003 solver.cpp:239] Iteration 175500 (2.30668 iter/s, 4.33523s/10 iters), loss = 6.16264
I0523 07:39:08.258169 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16264 (* 1 = 6.16264 loss)
I0523 07:39:08.281477 35003 sgd_solver.cpp:112] Iteration 175500, lr = 0.001
I0523 07:39:11.603916 35003 solver.cpp:239] Iteration 175510 (2.98899 iter/s, 3.34561s/10 iters), loss = 6.24097
I0523 07:39:11.603965 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24097 (* 1 = 6.24097 loss)
I0523 07:39:11.612025 35003 sgd_solver.cpp:112] Iteration 175510, lr = 0.001
I0523 07:39:14.297130 35003 solver.cpp:239] Iteration 175520 (3.71326 iter/s, 2.69305s/10 iters), loss = 6.05598
I0523 07:39:14.297173 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05598 (* 1 = 6.05598 loss)
I0523 07:39:14.306181 35003 sgd_solver.cpp:112] Iteration 175520, lr = 0.001
I0523 07:39:16.380354 35003 solver.cpp:239] Iteration 175530 (4.80056 iter/s, 2.08309s/10 iters), loss = 6.15999
I0523 07:39:16.380394 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15999 (* 1 = 6.15999 loss)
I0523 07:39:16.409011 35003 sgd_solver.cpp:112] Iteration 175530, lr = 0.001
I0523 07:39:20.706591 35003 solver.cpp:239] Iteration 175540 (2.31159 iter/s, 4.32602s/10 iters), loss = 7.39584
I0523 07:39:20.706640 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39584 (* 1 = 7.39584 loss)
I0523 07:39:20.712733 35003 sgd_solver.cpp:112] Iteration 175540, lr = 0.001
I0523 07:39:23.422137 35003 solver.cpp:239] Iteration 175550 (3.68272 iter/s, 2.71538s/10 iters), loss = 5.62802
I0523 07:39:23.422174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.62802 (* 1 = 5.62802 loss)
I0523 07:39:23.435132 35003 sgd_solver.cpp:112] Iteration 175550, lr = 0.001
I0523 07:39:28.410521 35003 solver.cpp:239] Iteration 175560 (2.00476 iter/s, 4.98814s/10 iters), loss = 6.59368
I0523 07:39:28.410579 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59368 (* 1 = 6.59368 loss)
I0523 07:39:28.476387 35003 sgd_solver.cpp:112] Iteration 175560, lr = 0.001
I0523 07:39:31.070683 35003 solver.cpp:239] Iteration 175570 (3.75942 iter/s, 2.65998s/10 iters), loss = 6.32393
I0523 07:39:31.070744 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32393 (* 1 = 6.32393 loss)
I0523 07:39:31.083907 35003 sgd_solver.cpp:112] Iteration 175570, lr = 0.001
I0523 07:39:33.779381 35003 solver.cpp:239] Iteration 175580 (3.69205 iter/s, 2.70852s/10 iters), loss = 7.3652
I0523 07:39:33.779422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3652 (* 1 = 7.3652 loss)
I0523 07:39:33.781474 35003 sgd_solver.cpp:112] Iteration 175580, lr = 0.001
I0523 07:39:38.801244 35003 solver.cpp:239] Iteration 175590 (1.99139 iter/s, 5.02162s/10 iters), loss = 6.13726
I0523 07:39:38.801412 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13726 (* 1 = 6.13726 loss)
I0523 07:39:38.806069 35003 sgd_solver.cpp:112] Iteration 175590, lr = 0.001
I0523 07:39:41.607517 35003 solver.cpp:239] Iteration 175600 (3.56383 iter/s, 2.80597s/10 iters), loss = 6.39246
I0523 07:39:41.607565 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39246 (* 1 = 6.39246 loss)
I0523 07:39:41.620546 35003 sgd_solver.cpp:112] Iteration 175600, lr = 0.001
I0523 07:39:43.701683 35003 solver.cpp:239] Iteration 175610 (4.77553 iter/s, 2.09401s/10 iters), loss = 7.0531
I0523 07:39:43.701728 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0531 (* 1 = 7.0531 loss)
I0523 07:39:44.138628 35003 sgd_solver.cpp:112] Iteration 175610, lr = 0.001
I0523 07:39:46.992382 35003 solver.cpp:239] Iteration 175620 (3.03906 iter/s, 3.29049s/10 iters), loss = 6.69596
I0523 07:39:46.992441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69596 (* 1 = 6.69596 loss)
I0523 07:39:47.733139 35003 sgd_solver.cpp:112] Iteration 175620, lr = 0.001
I0523 07:39:50.524876 35003 solver.cpp:239] Iteration 175630 (2.83103 iter/s, 3.53229s/10 iters), loss = 6.26302
I0523 07:39:50.524925 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26302 (* 1 = 6.26302 loss)
I0523 07:39:50.536952 35003 sgd_solver.cpp:112] Iteration 175630, lr = 0.001
I0523 07:39:55.687431 35003 solver.cpp:239] Iteration 175640 (1.93712 iter/s, 5.16229s/10 iters), loss = 6.70921
I0523 07:39:55.687476 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70921 (* 1 = 6.70921 loss)
I0523 07:39:55.694728 35003 sgd_solver.cpp:112] Iteration 175640, lr = 0.001
I0523 07:39:58.538895 35003 solver.cpp:239] Iteration 175650 (3.50718 iter/s, 2.8513s/10 iters), loss = 6.35057
I0523 07:39:58.538942 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35057 (* 1 = 6.35057 loss)
I0523 07:39:58.554949 35003 sgd_solver.cpp:112] Iteration 175650, lr = 0.001
I0523 07:40:00.735800 35003 solver.cpp:239] Iteration 175660 (4.55216 iter/s, 2.19676s/10 iters), loss = 6.06238
I0523 07:40:00.735851 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06238 (* 1 = 6.06238 loss)
I0523 07:40:00.757577 35003 sgd_solver.cpp:112] Iteration 175660, lr = 0.001
I0523 07:40:03.633127 35003 solver.cpp:239] Iteration 175670 (3.45167 iter/s, 2.89715s/10 iters), loss = 6.29946
I0523 07:40:03.633179 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29946 (* 1 = 6.29946 loss)
I0523 07:40:03.642447 35003 sgd_solver.cpp:112] Iteration 175670, lr = 0.001
I0523 07:40:07.100812 35003 solver.cpp:239] Iteration 175680 (2.88394 iter/s, 3.46748s/10 iters), loss = 6.67509
I0523 07:40:07.100878 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67509 (* 1 = 6.67509 loss)
I0523 07:40:07.114575 35003 sgd_solver.cpp:112] Iteration 175680, lr = 0.001
I0523 07:40:09.200338 35003 solver.cpp:239] Iteration 175690 (4.76334 iter/s, 2.09937s/10 iters), loss = 5.95234
I0523 07:40:09.200604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95234 (* 1 = 5.95234 loss)
I0523 07:40:09.570546 35003 sgd_solver.cpp:112] Iteration 175690, lr = 0.001
I0523 07:40:12.042018 35003 solver.cpp:239] Iteration 175700 (3.51952 iter/s, 2.84129s/10 iters), loss = 6.44137
I0523 07:40:12.042059 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44137 (* 1 = 6.44137 loss)
I0523 07:40:12.055353 35003 sgd_solver.cpp:112] Iteration 175700, lr = 0.001
I0523 07:40:14.901304 35003 solver.cpp:239] Iteration 175710 (3.49758 iter/s, 2.85912s/10 iters), loss = 6.99004
I0523 07:40:14.901340 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99004 (* 1 = 6.99004 loss)
I0523 07:40:14.909152 35003 sgd_solver.cpp:112] Iteration 175710, lr = 0.001
I0523 07:40:17.519232 35003 solver.cpp:239] Iteration 175720 (3.82004 iter/s, 2.61778s/10 iters), loss = 7.06547
I0523 07:40:17.519274 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06547 (* 1 = 7.06547 loss)
I0523 07:40:17.539151 35003 sgd_solver.cpp:112] Iteration 175720, lr = 0.001
I0523 07:40:20.235625 35003 solver.cpp:239] Iteration 175730 (3.68158 iter/s, 2.71623s/10 iters), loss = 5.87077
I0523 07:40:20.235704 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87077 (* 1 = 5.87077 loss)
I0523 07:40:20.930968 35003 sgd_solver.cpp:112] Iteration 175730, lr = 0.001
I0523 07:40:25.090623 35003 solver.cpp:239] Iteration 175740 (2.05985 iter/s, 4.85472s/10 iters), loss = 7.20305
I0523 07:40:25.090672 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20305 (* 1 = 7.20305 loss)
I0523 07:40:25.799679 35003 sgd_solver.cpp:112] Iteration 175740, lr = 0.001
I0523 07:40:29.387892 35003 solver.cpp:239] Iteration 175750 (2.32718 iter/s, 4.29705s/10 iters), loss = 6.47985
I0523 07:40:29.387941 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47985 (* 1 = 6.47985 loss)
I0523 07:40:29.393455 35003 sgd_solver.cpp:112] Iteration 175750, lr = 0.001
I0523 07:40:33.500336 35003 solver.cpp:239] Iteration 175760 (2.43178 iter/s, 4.11222s/10 iters), loss = 7.17823
I0523 07:40:33.500391 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17823 (* 1 = 7.17823 loss)
I0523 07:40:34.202733 35003 sgd_solver.cpp:112] Iteration 175760, lr = 0.001
I0523 07:40:38.698911 35003 solver.cpp:239] Iteration 175770 (1.9237 iter/s, 5.19831s/10 iters), loss = 5.21315
I0523 07:40:38.698963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.21315 (* 1 = 5.21315 loss)
I0523 07:40:38.712115 35003 sgd_solver.cpp:112] Iteration 175770, lr = 0.001
I0523 07:40:40.789973 35003 solver.cpp:239] Iteration 175780 (4.78259 iter/s, 2.09092s/10 iters), loss = 6.59431
I0523 07:40:40.790199 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59431 (* 1 = 6.59431 loss)
I0523 07:40:40.795187 35003 sgd_solver.cpp:112] Iteration 175780, lr = 0.001
I0523 07:40:44.389504 35003 solver.cpp:239] Iteration 175790 (2.7784 iter/s, 3.59919s/10 iters), loss = 6.05777
I0523 07:40:44.389552 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05777 (* 1 = 6.05777 loss)
I0523 07:40:44.402590 35003 sgd_solver.cpp:112] Iteration 175790, lr = 0.001
I0523 07:40:46.430725 35003 solver.cpp:239] Iteration 175800 (4.8994 iter/s, 2.04107s/10 iters), loss = 6.81092
I0523 07:40:46.430788 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81092 (* 1 = 6.81092 loss)
I0523 07:40:46.444555 35003 sgd_solver.cpp:112] Iteration 175800, lr = 0.001
I0523 07:40:49.997236 35003 solver.cpp:239] Iteration 175810 (2.80402 iter/s, 3.5663s/10 iters), loss = 6.46031
I0523 07:40:49.997275 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46031 (* 1 = 6.46031 loss)
I0523 07:40:50.009997 35003 sgd_solver.cpp:112] Iteration 175810, lr = 0.001
I0523 07:40:52.738082 35003 solver.cpp:239] Iteration 175820 (3.64872 iter/s, 2.74069s/10 iters), loss = 5.40537
I0523 07:40:52.738127 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.40537 (* 1 = 5.40537 loss)
I0523 07:40:52.755882 35003 sgd_solver.cpp:112] Iteration 175820, lr = 0.001
I0523 07:40:56.160359 35003 solver.cpp:239] Iteration 175830 (2.92219 iter/s, 3.42209s/10 iters), loss = 6.2528
I0523 07:40:56.160398 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2528 (* 1 = 6.2528 loss)
I0523 07:40:56.165387 35003 sgd_solver.cpp:112] Iteration 175830, lr = 0.001
I0523 07:41:00.367666 35003 solver.cpp:239] Iteration 175840 (2.37694 iter/s, 4.20709s/10 iters), loss = 6.34436
I0523 07:41:00.367720 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34436 (* 1 = 6.34436 loss)
I0523 07:41:00.388065 35003 sgd_solver.cpp:112] Iteration 175840, lr = 0.001
I0523 07:41:03.613898 35003 solver.cpp:239] Iteration 175850 (3.08067 iter/s, 3.24604s/10 iters), loss = 6.73844
I0523 07:41:03.613946 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73844 (* 1 = 6.73844 loss)
I0523 07:41:04.244940 35003 sgd_solver.cpp:112] Iteration 175850, lr = 0.001
I0523 07:41:07.874683 35003 solver.cpp:239] Iteration 175860 (2.34711 iter/s, 4.26055s/10 iters), loss = 7.02271
I0523 07:41:07.874749 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02271 (* 1 = 7.02271 loss)
I0523 07:41:08.609257 35003 sgd_solver.cpp:112] Iteration 175860, lr = 0.001
I0523 07:41:12.147291 35003 solver.cpp:239] Iteration 175870 (2.34065 iter/s, 4.27231s/10 iters), loss = 6.43713
I0523 07:41:12.147512 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43713 (* 1 = 6.43713 loss)
I0523 07:41:12.888198 35003 sgd_solver.cpp:112] Iteration 175870, lr = 0.001
I0523 07:41:14.970660 35003 solver.cpp:239] Iteration 175880 (3.54226 iter/s, 2.82306s/10 iters), loss = 6.65561
I0523 07:41:14.970731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65561 (* 1 = 6.65561 loss)
I0523 07:41:14.997333 35003 sgd_solver.cpp:112] Iteration 175880, lr = 0.001
I0523 07:41:17.894263 35003 solver.cpp:239] Iteration 175890 (3.42068 iter/s, 2.9234s/10 iters), loss = 5.56213
I0523 07:41:17.894331 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.56213 (* 1 = 5.56213 loss)
I0523 07:41:18.615427 35003 sgd_solver.cpp:112] Iteration 175890, lr = 0.001
I0523 07:41:22.127765 35003 solver.cpp:239] Iteration 175900 (2.36225 iter/s, 4.23325s/10 iters), loss = 6.52156
I0523 07:41:22.127825 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52156 (* 1 = 6.52156 loss)
I0523 07:41:22.140064 35003 sgd_solver.cpp:112] Iteration 175900, lr = 0.001
I0523 07:41:24.933954 35003 solver.cpp:239] Iteration 175910 (3.5638 iter/s, 2.80599s/10 iters), loss = 7.15165
I0523 07:41:24.934007 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15165 (* 1 = 7.15165 loss)
I0523 07:41:24.940203 35003 sgd_solver.cpp:112] Iteration 175910, lr = 0.001
I0523 07:41:27.088374 35003 solver.cpp:239] Iteration 175920 (4.64197 iter/s, 2.15426s/10 iters), loss = 5.65209
I0523 07:41:27.088423 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.65209 (* 1 = 5.65209 loss)
I0523 07:41:27.092787 35003 sgd_solver.cpp:112] Iteration 175920, lr = 0.001
I0523 07:41:31.395952 35003 solver.cpp:239] Iteration 175930 (2.32161 iter/s, 4.30736s/10 iters), loss = 6.69052
I0523 07:41:31.395998 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69052 (* 1 = 6.69052 loss)
I0523 07:41:31.440783 35003 sgd_solver.cpp:112] Iteration 175930, lr = 0.001
I0523 07:41:34.988158 35003 solver.cpp:239] Iteration 175940 (2.78396 iter/s, 3.592s/10 iters), loss = 6.75678
I0523 07:41:34.988220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75678 (* 1 = 6.75678 loss)
I0523 07:41:35.729125 35003 sgd_solver.cpp:112] Iteration 175940, lr = 0.001
I0523 07:41:38.556872 35003 solver.cpp:239] Iteration 175950 (2.8023 iter/s, 3.5685s/10 iters), loss = 5.58726
I0523 07:41:38.556912 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.58726 (* 1 = 5.58726 loss)
I0523 07:41:38.568851 35003 sgd_solver.cpp:112] Iteration 175950, lr = 0.001
I0523 07:41:41.355386 35003 solver.cpp:239] Iteration 175960 (3.57353 iter/s, 2.79835s/10 iters), loss = 6.69504
I0523 07:41:41.355432 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69504 (* 1 = 6.69504 loss)
I0523 07:41:41.366755 35003 sgd_solver.cpp:112] Iteration 175960, lr = 0.001
I0523 07:41:45.775163 35003 solver.cpp:239] Iteration 175970 (2.26268 iter/s, 4.41955s/10 iters), loss = 7.52118
I0523 07:41:45.775429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52118 (* 1 = 7.52118 loss)
I0523 07:41:45.779098 35003 sgd_solver.cpp:112] Iteration 175970, lr = 0.001
I0523 07:41:50.096657 35003 solver.cpp:239] Iteration 175980 (2.31424 iter/s, 4.32107s/10 iters), loss = 6.8927
I0523 07:41:50.096700 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8927 (* 1 = 6.8927 loss)
I0523 07:41:50.111397 35003 sgd_solver.cpp:112] Iteration 175980, lr = 0.001
I0523 07:41:53.982456 35003 solver.cpp:239] Iteration 175990 (2.57361 iter/s, 3.88559s/10 iters), loss = 6.24098
I0523 07:41:53.982512 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24098 (* 1 = 6.24098 loss)
I0523 07:41:53.987716 35003 sgd_solver.cpp:112] Iteration 175990, lr = 0.001
I0523 07:41:58.304240 35003 solver.cpp:239] Iteration 176000 (2.31398 iter/s, 4.32155s/10 iters), loss = 6.32164
I0523 07:41:58.304288 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32164 (* 1 = 6.32164 loss)
I0523 07:41:58.321420 35003 sgd_solver.cpp:112] Iteration 176000, lr = 0.001
I0523 07:42:00.369392 35003 solver.cpp:239] Iteration 176010 (4.84258 iter/s, 2.06502s/10 iters), loss = 6.84928
I0523 07:42:00.369436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84928 (* 1 = 6.84928 loss)
I0523 07:42:01.091543 35003 sgd_solver.cpp:112] Iteration 176010, lr = 0.001
I0523 07:42:04.376061 35003 solver.cpp:239] Iteration 176020 (2.49597 iter/s, 4.00645s/10 iters), loss = 6.80374
I0523 07:42:04.376114 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80374 (* 1 = 6.80374 loss)
I0523 07:42:04.376760 35003 sgd_solver.cpp:112] Iteration 176020, lr = 0.001
I0523 07:42:06.984700 35003 solver.cpp:239] Iteration 176030 (3.83366 iter/s, 2.60847s/10 iters), loss = 6.30635
I0523 07:42:06.984750 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30635 (* 1 = 6.30635 loss)
I0523 07:42:07.162184 35003 sgd_solver.cpp:112] Iteration 176030, lr = 0.001
I0523 07:42:11.261932 35003 solver.cpp:239] Iteration 176040 (2.3381 iter/s, 4.27698s/10 iters), loss = 5.7653
I0523 07:42:11.261984 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7653 (* 1 = 5.7653 loss)
I0523 07:42:11.274960 35003 sgd_solver.cpp:112] Iteration 176040, lr = 0.001
I0523 07:42:14.879438 35003 solver.cpp:239] Iteration 176050 (2.76449 iter/s, 3.6173s/10 iters), loss = 7.7353
I0523 07:42:14.879478 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7353 (* 1 = 7.7353 loss)
I0523 07:42:15.421363 35003 sgd_solver.cpp:112] Iteration 176050, lr = 0.001
I0523 07:42:17.181346 35003 solver.cpp:239] Iteration 176060 (4.3445 iter/s, 2.30176s/10 iters), loss = 8.24537
I0523 07:42:17.181648 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.24537 (* 1 = 8.24537 loss)
I0523 07:42:17.195338 35003 sgd_solver.cpp:112] Iteration 176060, lr = 0.001
I0523 07:42:19.372997 35003 solver.cpp:239] Iteration 176070 (4.56355 iter/s, 2.19127s/10 iters), loss = 5.34535
I0523 07:42:19.373041 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.34535 (* 1 = 5.34535 loss)
I0523 07:42:19.807976 35003 sgd_solver.cpp:112] Iteration 176070, lr = 0.001
I0523 07:42:23.250185 35003 solver.cpp:239] Iteration 176080 (2.57933 iter/s, 3.87698s/10 iters), loss = 5.4199
I0523 07:42:23.250232 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.4199 (* 1 = 5.4199 loss)
I0523 07:42:23.262845 35003 sgd_solver.cpp:112] Iteration 176080, lr = 0.001
I0523 07:42:26.799710 35003 solver.cpp:239] Iteration 176090 (2.81743 iter/s, 3.54933s/10 iters), loss = 7.19468
I0523 07:42:26.799747 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19468 (* 1 = 7.19468 loss)
I0523 07:42:26.818959 35003 sgd_solver.cpp:112] Iteration 176090, lr = 0.001
I0523 07:42:30.774809 35003 solver.cpp:239] Iteration 176100 (2.51579 iter/s, 3.9749s/10 iters), loss = 6.64109
I0523 07:42:30.774858 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64109 (* 1 = 6.64109 loss)
I0523 07:42:30.788331 35003 sgd_solver.cpp:112] Iteration 176100, lr = 0.001
I0523 07:42:32.924506 35003 solver.cpp:239] Iteration 176110 (4.65213 iter/s, 2.14956s/10 iters), loss = 6.1895
I0523 07:42:32.924545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1895 (* 1 = 6.1895 loss)
I0523 07:42:32.932687 35003 sgd_solver.cpp:112] Iteration 176110, lr = 0.001
I0523 07:42:35.630553 35003 solver.cpp:239] Iteration 176120 (3.69564 iter/s, 2.70589s/10 iters), loss = 5.66536
I0523 07:42:35.630596 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.66536 (* 1 = 5.66536 loss)
I0523 07:42:35.653712 35003 sgd_solver.cpp:112] Iteration 176120, lr = 0.001
I0523 07:42:39.050776 35003 solver.cpp:239] Iteration 176130 (2.92394 iter/s, 3.42004s/10 iters), loss = 6.46853
I0523 07:42:39.050817 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46853 (* 1 = 6.46853 loss)
I0523 07:42:39.076989 35003 sgd_solver.cpp:112] Iteration 176130, lr = 0.001
I0523 07:42:41.163391 35003 solver.cpp:239] Iteration 176140 (4.73379 iter/s, 2.11247s/10 iters), loss = 6.24221
I0523 07:42:41.163452 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24221 (* 1 = 6.24221 loss)
I0523 07:42:41.172371 35003 sgd_solver.cpp:112] Iteration 176140, lr = 0.001
I0523 07:42:43.878828 35003 solver.cpp:239] Iteration 176150 (3.68288 iter/s, 2.71526s/10 iters), loss = 5.68065
I0523 07:42:43.878864 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.68065 (* 1 = 5.68065 loss)
I0523 07:42:43.892140 35003 sgd_solver.cpp:112] Iteration 176150, lr = 0.001
I0523 07:42:45.530122 35003 solver.cpp:239] Iteration 176160 (6.05628 iter/s, 1.65118s/10 iters), loss = 5.21261
I0523 07:42:45.530164 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.21261 (* 1 = 5.21261 loss)
I0523 07:42:45.548338 35003 sgd_solver.cpp:112] Iteration 176160, lr = 0.001
I0523 07:42:49.972398 35003 solver.cpp:239] Iteration 176170 (2.25121 iter/s, 4.44206s/10 iters), loss = 4.46016
I0523 07:42:49.972565 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.46016 (* 1 = 4.46016 loss)
I0523 07:42:49.986091 35003 sgd_solver.cpp:112] Iteration 176170, lr = 0.001
I0523 07:42:51.339046 35003 solver.cpp:239] Iteration 176180 (7.31845 iter/s, 1.36641s/10 iters), loss = 7.09764
I0523 07:42:51.339104 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09764 (* 1 = 7.09764 loss)
I0523 07:42:52.080092 35003 sgd_solver.cpp:112] Iteration 176180, lr = 0.001
I0523 07:42:56.553076 35003 solver.cpp:239] Iteration 176190 (1.918 iter/s, 5.21376s/10 iters), loss = 5.37851
I0523 07:42:56.553128 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.37851 (* 1 = 5.37851 loss)
I0523 07:42:57.223037 35003 sgd_solver.cpp:112] Iteration 176190, lr = 0.001
I0523 07:43:00.761910 35003 solver.cpp:239] Iteration 176200 (2.37608 iter/s, 4.20861s/10 iters), loss = 6.31896
I0523 07:43:00.761955 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31896 (* 1 = 6.31896 loss)
I0523 07:43:00.769855 35003 sgd_solver.cpp:112] Iteration 176200, lr = 0.001
I0523 07:43:03.661468 35003 solver.cpp:239] Iteration 176210 (3.44901 iter/s, 2.89939s/10 iters), loss = 6.49493
I0523 07:43:03.661525 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49493 (* 1 = 6.49493 loss)
I0523 07:43:04.370462 35003 sgd_solver.cpp:112] Iteration 176210, lr = 0.001
I0523 07:43:08.084218 35003 solver.cpp:239] Iteration 176220 (2.26116 iter/s, 4.42251s/10 iters), loss = 6.89145
I0523 07:43:08.084278 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89145 (* 1 = 6.89145 loss)
I0523 07:43:08.799665 35003 sgd_solver.cpp:112] Iteration 176220, lr = 0.001
I0523 07:43:12.289784 35003 solver.cpp:239] Iteration 176230 (2.37793 iter/s, 4.20533s/10 iters), loss = 4.86699
I0523 07:43:12.289831 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.86699 (* 1 = 4.86699 loss)
I0523 07:43:12.511174 35003 sgd_solver.cpp:112] Iteration 176230, lr = 0.001
I0523 07:43:15.629559 35003 solver.cpp:239] Iteration 176240 (2.99438 iter/s, 3.33959s/10 iters), loss = 6.37567
I0523 07:43:15.629614 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37567 (* 1 = 6.37567 loss)
I0523 07:43:16.240522 35003 sgd_solver.cpp:112] Iteration 176240, lr = 0.001
I0523 07:43:20.068634 35003 solver.cpp:239] Iteration 176250 (2.25284 iter/s, 4.43883s/10 iters), loss = 6.60669
I0523 07:43:20.068842 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60669 (* 1 = 6.60669 loss)
I0523 07:43:20.082543 35003 sgd_solver.cpp:112] Iteration 176250, lr = 0.001
I0523 07:43:22.860721 35003 solver.cpp:239] Iteration 176260 (3.58193 iter/s, 2.79179s/10 iters), loss = 5.9522
I0523 07:43:22.860766 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9522 (* 1 = 5.9522 loss)
I0523 07:43:23.133123 35003 sgd_solver.cpp:112] Iteration 176260, lr = 0.001
I0523 07:43:27.062731 35003 solver.cpp:239] Iteration 176270 (2.37994 iter/s, 4.20179s/10 iters), loss = 6.74418
I0523 07:43:27.062769 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74418 (* 1 = 6.74418 loss)
I0523 07:43:27.075814 35003 sgd_solver.cpp:112] Iteration 176270, lr = 0.001
I0523 07:43:31.431412 35003 solver.cpp:239] Iteration 176280 (2.28914 iter/s, 4.36846s/10 iters), loss = 6.26967
I0523 07:43:31.431460 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26967 (* 1 = 6.26967 loss)
I0523 07:43:31.445325 35003 sgd_solver.cpp:112] Iteration 176280, lr = 0.001
I0523 07:43:36.528751 35003 solver.cpp:239] Iteration 176290 (1.96191 iter/s, 5.09708s/10 iters), loss = 7.18276
I0523 07:43:36.528800 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18276 (* 1 = 7.18276 loss)
I0523 07:43:36.542017 35003 sgd_solver.cpp:112] Iteration 176290, lr = 0.001
I0523 07:43:39.339488 35003 solver.cpp:239] Iteration 176300 (3.55803 iter/s, 2.81055s/10 iters), loss = 6.91251
I0523 07:43:39.339545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91251 (* 1 = 6.91251 loss)
I0523 07:43:40.060776 35003 sgd_solver.cpp:112] Iteration 176300, lr = 0.001
I0523 07:43:44.142315 35003 solver.cpp:239] Iteration 176310 (2.08223 iter/s, 4.80254s/10 iters), loss = 7.24897
I0523 07:43:44.142370 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24897 (* 1 = 7.24897 loss)
I0523 07:43:44.208925 35003 sgd_solver.cpp:112] Iteration 176310, lr = 0.001
I0523 07:43:46.259521 35003 solver.cpp:239] Iteration 176320 (4.72354 iter/s, 2.11706s/10 iters), loss = 5.41592
I0523 07:43:46.259569 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.41592 (* 1 = 5.41592 loss)
I0523 07:43:46.272538 35003 sgd_solver.cpp:112] Iteration 176320, lr = 0.001
I0523 07:43:50.923081 35003 solver.cpp:239] Iteration 176330 (2.1444 iter/s, 4.66331s/10 iters), loss = 7.13181
I0523 07:43:50.923316 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13181 (* 1 = 7.13181 loss)
I0523 07:43:50.936244 35003 sgd_solver.cpp:112] Iteration 176330, lr = 0.001
I0523 07:43:53.653340 35003 solver.cpp:239] Iteration 176340 (3.66311 iter/s, 2.72992s/10 iters), loss = 6.63723
I0523 07:43:53.653393 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63723 (* 1 = 6.63723 loss)
I0523 07:43:54.215531 35003 sgd_solver.cpp:112] Iteration 176340, lr = 0.001
I0523 07:43:56.115584 35003 solver.cpp:239] Iteration 176350 (4.0616 iter/s, 2.46208s/10 iters), loss = 5.5485
I0523 07:43:56.115620 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.5485 (* 1 = 5.5485 loss)
I0523 07:43:56.122906 35003 sgd_solver.cpp:112] Iteration 176350, lr = 0.001
I0523 07:43:57.431257 35003 solver.cpp:239] Iteration 176360 (7.60125 iter/s, 1.31557s/10 iters), loss = 7.11803
I0523 07:43:57.431300 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11803 (* 1 = 7.11803 loss)
I0523 07:43:57.439472 35003 sgd_solver.cpp:112] Iteration 176360, lr = 0.001
I0523 07:44:00.362908 35003 solver.cpp:239] Iteration 176370 (3.41124 iter/s, 2.93148s/10 iters), loss = 5.99795
I0523 07:44:00.362949 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99795 (* 1 = 5.99795 loss)
I0523 07:44:00.363651 35003 sgd_solver.cpp:112] Iteration 176370, lr = 0.001
I0523 07:44:03.179908 35003 solver.cpp:239] Iteration 176380 (3.55009 iter/s, 2.81683s/10 iters), loss = 6.88396
I0523 07:44:03.179968 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88396 (* 1 = 6.88396 loss)
I0523 07:44:03.183856 35003 sgd_solver.cpp:112] Iteration 176380, lr = 0.001
I0523 07:44:05.199761 35003 solver.cpp:239] Iteration 176390 (4.95122 iter/s, 2.0197s/10 iters), loss = 7.07779
I0523 07:44:05.199803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07779 (* 1 = 7.07779 loss)
I0523 07:44:05.221593 35003 sgd_solver.cpp:112] Iteration 176390, lr = 0.001
I0523 07:44:08.763087 35003 solver.cpp:239] Iteration 176400 (2.80652 iter/s, 3.56313s/10 iters), loss = 6.60393
I0523 07:44:08.763151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60393 (* 1 = 6.60393 loss)
I0523 07:44:08.768316 35003 sgd_solver.cpp:112] Iteration 176400, lr = 0.001
I0523 07:44:11.216878 35003 solver.cpp:239] Iteration 176410 (4.07562 iter/s, 2.45362s/10 iters), loss = 6.7
I0523 07:44:11.216922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7 (* 1 = 6.7 loss)
I0523 07:44:11.224740 35003 sgd_solver.cpp:112] Iteration 176410, lr = 0.001
I0523 07:44:14.514065 35003 solver.cpp:239] Iteration 176420 (3.03305 iter/s, 3.29701s/10 iters), loss = 6.47401
I0523 07:44:14.514106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47401 (* 1 = 6.47401 loss)
I0523 07:44:14.525496 35003 sgd_solver.cpp:112] Iteration 176420, lr = 0.001
I0523 07:44:18.112470 35003 solver.cpp:239] Iteration 176430 (2.77917 iter/s, 3.5982s/10 iters), loss = 6.58372
I0523 07:44:18.112517 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58372 (* 1 = 6.58372 loss)
I0523 07:44:18.119158 35003 sgd_solver.cpp:112] Iteration 176430, lr = 0.001
I0523 07:44:22.951680 35003 solver.cpp:239] Iteration 176440 (2.06656 iter/s, 4.83895s/10 iters), loss = 7.12652
I0523 07:44:22.951871 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12652 (* 1 = 7.12652 loss)
I0523 07:44:22.958911 35003 sgd_solver.cpp:112] Iteration 176440, lr = 0.001
I0523 07:44:25.758404 35003 solver.cpp:239] Iteration 176450 (3.56328 iter/s, 2.8064s/10 iters), loss = 6.02844
I0523 07:44:25.758446 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02844 (* 1 = 6.02844 loss)
I0523 07:44:25.766417 35003 sgd_solver.cpp:112] Iteration 176450, lr = 0.001
I0523 07:44:28.637712 35003 solver.cpp:239] Iteration 176460 (3.47326 iter/s, 2.87914s/10 iters), loss = 6.50861
I0523 07:44:28.637759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50861 (* 1 = 6.50861 loss)
I0523 07:44:28.651226 35003 sgd_solver.cpp:112] Iteration 176460, lr = 0.001
I0523 07:44:32.266012 35003 solver.cpp:239] Iteration 176470 (2.75626 iter/s, 3.6281s/10 iters), loss = 7.2229
I0523 07:44:32.266057 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2229 (* 1 = 7.2229 loss)
I0523 07:44:32.279803 35003 sgd_solver.cpp:112] Iteration 176470, lr = 0.001
I0523 07:44:35.045574 35003 solver.cpp:239] Iteration 176480 (3.5979 iter/s, 2.7794s/10 iters), loss = 5.84099
I0523 07:44:35.045619 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84099 (* 1 = 5.84099 loss)
I0523 07:44:35.058565 35003 sgd_solver.cpp:112] Iteration 176480, lr = 0.001
I0523 07:44:38.063658 35003 solver.cpp:239] Iteration 176490 (3.31356 iter/s, 3.0179s/10 iters), loss = 7.24732
I0523 07:44:38.063697 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24732 (* 1 = 7.24732 loss)
I0523 07:44:38.069152 35003 sgd_solver.cpp:112] Iteration 176490, lr = 0.001
I0523 07:44:41.676170 35003 solver.cpp:239] Iteration 176500 (2.7683 iter/s, 3.61232s/10 iters), loss = 6.84063
I0523 07:44:41.676205 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84063 (* 1 = 6.84063 loss)
I0523 07:44:41.688839 35003 sgd_solver.cpp:112] Iteration 176500, lr = 0.001
I0523 07:44:43.792816 35003 solver.cpp:239] Iteration 176510 (4.72475 iter/s, 2.11652s/10 iters), loss = 7.08065
I0523 07:44:43.792861 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08065 (* 1 = 7.08065 loss)
I0523 07:44:43.802198 35003 sgd_solver.cpp:112] Iteration 176510, lr = 0.001
I0523 07:44:45.925829 35003 solver.cpp:239] Iteration 176520 (4.6885 iter/s, 2.13288s/10 iters), loss = 6.86526
I0523 07:44:45.925874 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86526 (* 1 = 6.86526 loss)
I0523 07:44:46.654582 35003 sgd_solver.cpp:112] Iteration 176520, lr = 0.001
I0523 07:44:50.081656 35003 solver.cpp:239] Iteration 176530 (2.40639 iter/s, 4.15561s/10 iters), loss = 6.64719
I0523 07:44:50.081696 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64719 (* 1 = 6.64719 loss)
I0523 07:44:50.095516 35003 sgd_solver.cpp:112] Iteration 176530, lr = 0.001
I0523 07:44:52.217337 35003 solver.cpp:239] Iteration 176540 (4.68264 iter/s, 2.13555s/10 iters), loss = 6.96079
I0523 07:44:52.217376 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96079 (* 1 = 6.96079 loss)
I0523 07:44:52.230486 35003 sgd_solver.cpp:112] Iteration 176540, lr = 0.001
I0523 07:44:55.670334 35003 solver.cpp:239] Iteration 176550 (2.89619 iter/s, 3.45281s/10 iters), loss = 7.00344
I0523 07:44:55.670496 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00344 (* 1 = 7.00344 loss)
I0523 07:44:55.675770 35003 sgd_solver.cpp:112] Iteration 176550, lr = 0.001
I0523 07:44:59.844522 35003 solver.cpp:239] Iteration 176560 (2.39585 iter/s, 4.17388s/10 iters), loss = 5.87559
I0523 07:44:59.844563 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87559 (* 1 = 5.87559 loss)
I0523 07:44:59.857702 35003 sgd_solver.cpp:112] Iteration 176560, lr = 0.001
I0523 07:45:03.417883 35003 solver.cpp:239] Iteration 176570 (2.79864 iter/s, 3.57317s/10 iters), loss = 7.0971
I0523 07:45:03.417935 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0971 (* 1 = 7.0971 loss)
I0523 07:45:03.431190 35003 sgd_solver.cpp:112] Iteration 176570, lr = 0.001
I0523 07:45:07.629340 35003 solver.cpp:239] Iteration 176580 (2.3746 iter/s, 4.21123s/10 iters), loss = 6.77685
I0523 07:45:07.629395 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77685 (* 1 = 6.77685 loss)
I0523 07:45:07.642540 35003 sgd_solver.cpp:112] Iteration 176580, lr = 0.001
I0523 07:45:12.192759 35003 solver.cpp:239] Iteration 176590 (2.19146 iter/s, 4.56317s/10 iters), loss = 6.07806
I0523 07:45:12.192812 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07806 (* 1 = 6.07806 loss)
I0523 07:45:12.199712 35003 sgd_solver.cpp:112] Iteration 176590, lr = 0.001
I0523 07:45:15.842344 35003 solver.cpp:239] Iteration 176600 (2.7402 iter/s, 3.64937s/10 iters), loss = 7.8318
I0523 07:45:15.842393 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.8318 (* 1 = 7.8318 loss)
I0523 07:45:16.518618 35003 sgd_solver.cpp:112] Iteration 176600, lr = 0.001
I0523 07:45:20.048110 35003 solver.cpp:239] Iteration 176610 (2.37781 iter/s, 4.20554s/10 iters), loss = 5.85119
I0523 07:45:20.048162 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85119 (* 1 = 5.85119 loss)
I0523 07:45:20.783293 35003 sgd_solver.cpp:112] Iteration 176610, lr = 0.001
I0523 07:45:23.572830 35003 solver.cpp:239] Iteration 176620 (2.83727 iter/s, 3.52452s/10 iters), loss = 6.50835
I0523 07:45:23.572877 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50835 (* 1 = 6.50835 loss)
I0523 07:45:23.580935 35003 sgd_solver.cpp:112] Iteration 176620, lr = 0.001
I0523 07:45:25.639181 35003 solver.cpp:239] Iteration 176630 (4.83978 iter/s, 2.06621s/10 iters), loss = 6.12596
I0523 07:45:25.639222 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12596 (* 1 = 6.12596 loss)
I0523 07:45:25.651890 35003 sgd_solver.cpp:112] Iteration 176630, lr = 0.001
I0523 07:45:28.543031 35003 solver.cpp:239] Iteration 176640 (3.4439 iter/s, 2.90368s/10 iters), loss = 4.32081
I0523 07:45:28.543303 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.32081 (* 1 = 4.32081 loss)
I0523 07:45:28.621166 35003 sgd_solver.cpp:112] Iteration 176640, lr = 0.001
I0523 07:45:32.082288 35003 solver.cpp:239] Iteration 176650 (2.82579 iter/s, 3.53884s/10 iters), loss = 6.8613
I0523 07:45:32.082329 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8613 (* 1 = 6.8613 loss)
I0523 07:45:32.095427 35003 sgd_solver.cpp:112] Iteration 176650, lr = 0.001
I0523 07:45:36.402318 35003 solver.cpp:239] Iteration 176660 (2.31492 iter/s, 4.31981s/10 iters), loss = 5.81987
I0523 07:45:36.402364 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.81987 (* 1 = 5.81987 loss)
I0523 07:45:36.415688 35003 sgd_solver.cpp:112] Iteration 176660, lr = 0.001
I0523 07:45:39.970191 35003 solver.cpp:239] Iteration 176670 (2.80294 iter/s, 3.56768s/10 iters), loss = 6.17642
I0523 07:45:39.970232 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17642 (* 1 = 6.17642 loss)
I0523 07:45:39.975929 35003 sgd_solver.cpp:112] Iteration 176670, lr = 0.001
I0523 07:45:42.983286 35003 solver.cpp:239] Iteration 176680 (3.31904 iter/s, 3.01292s/10 iters), loss = 6.41594
I0523 07:45:42.983340 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41594 (* 1 = 6.41594 loss)
I0523 07:45:42.995988 35003 sgd_solver.cpp:112] Iteration 176680, lr = 0.001
I0523 07:45:46.646250 35003 solver.cpp:239] Iteration 176690 (2.73018 iter/s, 3.66276s/10 iters), loss = 7.11738
I0523 07:45:46.646294 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11738 (* 1 = 7.11738 loss)
I0523 07:45:46.653193 35003 sgd_solver.cpp:112] Iteration 176690, lr = 0.001
I0523 07:45:50.096778 35003 solver.cpp:239] Iteration 176700 (2.89827 iter/s, 3.45033s/10 iters), loss = 5.38606
I0523 07:45:50.096833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.38606 (* 1 = 5.38606 loss)
I0523 07:45:50.103065 35003 sgd_solver.cpp:112] Iteration 176700, lr = 0.001
I0523 07:45:53.614890 35003 solver.cpp:239] Iteration 176710 (2.84261 iter/s, 3.5179s/10 iters), loss = 6.92065
I0523 07:45:53.614931 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92065 (* 1 = 6.92065 loss)
I0523 07:45:53.622383 35003 sgd_solver.cpp:112] Iteration 176710, lr = 0.001
I0523 07:45:58.605046 35003 solver.cpp:239] Iteration 176720 (2.00404 iter/s, 4.98991s/10 iters), loss = 6.12445
I0523 07:45:58.605239 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12445 (* 1 = 6.12445 loss)
I0523 07:45:58.618477 35003 sgd_solver.cpp:112] Iteration 176720, lr = 0.001
I0523 07:46:02.560398 35003 solver.cpp:239] Iteration 176730 (2.52845 iter/s, 3.95499s/10 iters), loss = 7.04728
I0523 07:46:02.560458 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04728 (* 1 = 7.04728 loss)
I0523 07:46:03.173856 35003 sgd_solver.cpp:112] Iteration 176730, lr = 0.001
I0523 07:46:06.723842 35003 solver.cpp:239] Iteration 176740 (2.40199 iter/s, 4.16321s/10 iters), loss = 5.16512
I0523 07:46:06.723886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.16512 (* 1 = 5.16512 loss)
I0523 07:46:06.741528 35003 sgd_solver.cpp:112] Iteration 176740, lr = 0.001
I0523 07:46:09.389987 35003 solver.cpp:239] Iteration 176750 (3.75095 iter/s, 2.66599s/10 iters), loss = 7.81579
I0523 07:46:09.390027 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.81579 (* 1 = 7.81579 loss)
I0523 07:46:10.114090 35003 sgd_solver.cpp:112] Iteration 176750, lr = 0.001
I0523 07:46:13.012115 35003 solver.cpp:239] Iteration 176760 (2.76096 iter/s, 3.62192s/10 iters), loss = 6.89824
I0523 07:46:13.012171 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89824 (* 1 = 6.89824 loss)
I0523 07:46:13.684264 35003 sgd_solver.cpp:112] Iteration 176760, lr = 0.001
I0523 07:46:16.639704 35003 solver.cpp:239] Iteration 176770 (2.75682 iter/s, 3.62737s/10 iters), loss = 6.39473
I0523 07:46:16.639765 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39473 (* 1 = 6.39473 loss)
I0523 07:46:17.373980 35003 sgd_solver.cpp:112] Iteration 176770, lr = 0.001
I0523 07:46:20.564059 35003 solver.cpp:239] Iteration 176780 (2.54833 iter/s, 3.92414s/10 iters), loss = 6.35313
I0523 07:46:20.564100 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35313 (* 1 = 6.35313 loss)
I0523 07:46:21.286181 35003 sgd_solver.cpp:112] Iteration 176780, lr = 0.001
I0523 07:46:24.833367 35003 solver.cpp:239] Iteration 176790 (2.34242 iter/s, 4.26909s/10 iters), loss = 6.49854
I0523 07:46:24.833411 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49854 (* 1 = 6.49854 loss)
I0523 07:46:24.837803 35003 sgd_solver.cpp:112] Iteration 176790, lr = 0.001
I0523 07:46:28.908855 35003 solver.cpp:239] Iteration 176800 (2.45383 iter/s, 4.07527s/10 iters), loss = 7.36369
I0523 07:46:28.908998 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36369 (* 1 = 7.36369 loss)
I0523 07:46:28.921365 35003 sgd_solver.cpp:112] Iteration 176800, lr = 0.001
I0523 07:46:32.618382 35003 solver.cpp:239] Iteration 176810 (2.69597 iter/s, 3.70924s/10 iters), loss = 6.98065
I0523 07:46:32.618429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98065 (* 1 = 6.98065 loss)
I0523 07:46:33.340409 35003 sgd_solver.cpp:112] Iteration 176810, lr = 0.001
I0523 07:46:36.819828 35003 solver.cpp:239] Iteration 176820 (2.38026 iter/s, 4.20123s/10 iters), loss = 7.3807
I0523 07:46:36.819872 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3807 (* 1 = 7.3807 loss)
I0523 07:46:36.843236 35003 sgd_solver.cpp:112] Iteration 176820, lr = 0.001
I0523 07:46:39.575917 35003 solver.cpp:239] Iteration 176830 (3.62855 iter/s, 2.75592s/10 iters), loss = 6.48929
I0523 07:46:39.575968 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48929 (* 1 = 6.48929 loss)
I0523 07:46:39.589207 35003 sgd_solver.cpp:112] Iteration 176830, lr = 0.001
I0523 07:46:43.939815 35003 solver.cpp:239] Iteration 176840 (2.29165 iter/s, 4.36366s/10 iters), loss = 6.31199
I0523 07:46:43.939870 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31199 (* 1 = 6.31199 loss)
I0523 07:46:43.943337 35003 sgd_solver.cpp:112] Iteration 176840, lr = 0.001
I0523 07:46:47.090677 35003 solver.cpp:239] Iteration 176850 (3.17393 iter/s, 3.15067s/10 iters), loss = 6.6862
I0523 07:46:47.090744 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6862 (* 1 = 6.6862 loss)
I0523 07:46:47.103312 35003 sgd_solver.cpp:112] Iteration 176850, lr = 0.001
I0523 07:46:50.526566 35003 solver.cpp:239] Iteration 176860 (2.91063 iter/s, 3.43568s/10 iters), loss = 6.2158
I0523 07:46:50.526605 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2158 (* 1 = 6.2158 loss)
I0523 07:46:50.533668 35003 sgd_solver.cpp:112] Iteration 176860, lr = 0.001
I0523 07:46:53.862670 35003 solver.cpp:239] Iteration 176870 (2.99767 iter/s, 3.33592s/10 iters), loss = 6.87278
I0523 07:46:53.862737 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87278 (* 1 = 6.87278 loss)
I0523 07:46:54.148046 35003 sgd_solver.cpp:112] Iteration 176870, lr = 0.001
I0523 07:46:57.294926 35003 solver.cpp:239] Iteration 176880 (2.91372 iter/s, 3.43203s/10 iters), loss = 6.04171
I0523 07:46:57.294981 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04171 (* 1 = 6.04171 loss)
I0523 07:46:57.340514 35003 sgd_solver.cpp:112] Iteration 176880, lr = 0.001
I0523 07:47:01.252362 35003 solver.cpp:239] Iteration 176890 (2.52703 iter/s, 3.95722s/10 iters), loss = 7.05244
I0523 07:47:01.252650 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05244 (* 1 = 7.05244 loss)
I0523 07:47:01.684449 35003 sgd_solver.cpp:112] Iteration 176890, lr = 0.001
I0523 07:47:06.985288 35003 solver.cpp:239] Iteration 176900 (1.74446 iter/s, 5.73243s/10 iters), loss = 6.33326
I0523 07:47:06.985337 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33326 (* 1 = 6.33326 loss)
I0523 07:47:06.997882 35003 sgd_solver.cpp:112] Iteration 176900, lr = 0.001
I0523 07:47:10.074280 35003 solver.cpp:239] Iteration 176910 (3.23749 iter/s, 3.08881s/10 iters), loss = 6.08441
I0523 07:47:10.074335 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08441 (* 1 = 6.08441 loss)
I0523 07:47:10.789659 35003 sgd_solver.cpp:112] Iteration 176910, lr = 0.001
I0523 07:47:13.752491 35003 solver.cpp:239] Iteration 176920 (2.71887 iter/s, 3.67801s/10 iters), loss = 8.08152
I0523 07:47:13.752542 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.08152 (* 1 = 8.08152 loss)
I0523 07:47:14.487619 35003 sgd_solver.cpp:112] Iteration 176920, lr = 0.001
I0523 07:47:17.318301 35003 solver.cpp:239] Iteration 176930 (2.80457 iter/s, 3.56561s/10 iters), loss = 6.05481
I0523 07:47:17.318341 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05481 (* 1 = 6.05481 loss)
I0523 07:47:17.330699 35003 sgd_solver.cpp:112] Iteration 176930, lr = 0.001
I0523 07:47:20.983204 35003 solver.cpp:239] Iteration 176940 (2.72873 iter/s, 3.6647s/10 iters), loss = 5.97245
I0523 07:47:20.983266 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97245 (* 1 = 5.97245 loss)
I0523 07:47:20.995649 35003 sgd_solver.cpp:112] Iteration 176940, lr = 0.001
I0523 07:47:24.603967 35003 solver.cpp:239] Iteration 176950 (2.76201 iter/s, 3.62055s/10 iters), loss = 6.70061
I0523 07:47:24.604014 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70061 (* 1 = 6.70061 loss)
I0523 07:47:24.610916 35003 sgd_solver.cpp:112] Iteration 176950, lr = 0.001
I0523 07:47:27.490394 35003 solver.cpp:239] Iteration 176960 (3.4647 iter/s, 2.88625s/10 iters), loss = 5.8918
I0523 07:47:27.490434 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8918 (* 1 = 5.8918 loss)
I0523 07:47:27.495615 35003 sgd_solver.cpp:112] Iteration 176960, lr = 0.001
I0523 07:47:31.807760 35003 solver.cpp:239] Iteration 176970 (2.31635 iter/s, 4.31714s/10 iters), loss = 6.50664
I0523 07:47:31.807967 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50664 (* 1 = 6.50664 loss)
I0523 07:47:32.482409 35003 sgd_solver.cpp:112] Iteration 176970, lr = 0.001
I0523 07:47:35.258643 35003 solver.cpp:239] Iteration 176980 (2.8981 iter/s, 3.45053s/10 iters), loss = 7.24655
I0523 07:47:35.258690 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24655 (* 1 = 7.24655 loss)
I0523 07:47:35.267341 35003 sgd_solver.cpp:112] Iteration 176980, lr = 0.001
I0523 07:47:39.200152 35003 solver.cpp:239] Iteration 176990 (2.53724 iter/s, 3.9413s/10 iters), loss = 6.20451
I0523 07:47:39.200211 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20451 (* 1 = 6.20451 loss)
I0523 07:47:39.207662 35003 sgd_solver.cpp:112] Iteration 176990, lr = 0.001
I0523 07:47:41.961098 35003 solver.cpp:239] Iteration 177000 (3.62218 iter/s, 2.76077s/10 iters), loss = 5.74178
I0523 07:47:41.961138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74178 (* 1 = 5.74178 loss)
I0523 07:47:41.968555 35003 sgd_solver.cpp:112] Iteration 177000, lr = 0.001
I0523 07:47:45.325712 35003 solver.cpp:239] Iteration 177010 (2.97227 iter/s, 3.36443s/10 iters), loss = 7.31815
I0523 07:47:45.325757 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31815 (* 1 = 7.31815 loss)
I0523 07:47:45.637632 35003 sgd_solver.cpp:112] Iteration 177010, lr = 0.001
I0523 07:47:48.608381 35003 solver.cpp:239] Iteration 177020 (3.04648 iter/s, 3.28248s/10 iters), loss = 5.55259
I0523 07:47:48.608443 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.55259 (* 1 = 5.55259 loss)
I0523 07:47:49.342903 35003 sgd_solver.cpp:112] Iteration 177020, lr = 0.001
I0523 07:47:50.894244 35003 solver.cpp:239] Iteration 177030 (4.37502 iter/s, 2.2857s/10 iters), loss = 6.9583
I0523 07:47:50.894289 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9583 (* 1 = 6.9583 loss)
I0523 07:47:50.907929 35003 sgd_solver.cpp:112] Iteration 177030, lr = 0.001
I0523 07:47:53.019606 35003 solver.cpp:239] Iteration 177040 (4.70539 iter/s, 2.12522s/10 iters), loss = 5.94355
I0523 07:47:53.019652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.94355 (* 1 = 5.94355 loss)
I0523 07:47:53.027750 35003 sgd_solver.cpp:112] Iteration 177040, lr = 0.001
I0523 07:47:56.301523 35003 solver.cpp:239] Iteration 177050 (3.04718 iter/s, 3.28173s/10 iters), loss = 6.81758
I0523 07:47:56.301584 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81758 (* 1 = 6.81758 loss)
I0523 07:47:56.308962 35003 sgd_solver.cpp:112] Iteration 177050, lr = 0.001
I0523 07:47:59.878017 35003 solver.cpp:239] Iteration 177060 (2.79619 iter/s, 3.57629s/10 iters), loss = 6.28285
I0523 07:47:59.878062 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28285 (* 1 = 6.28285 loss)
I0523 07:48:00.618461 35003 sgd_solver.cpp:112] Iteration 177060, lr = 0.001
I0523 07:48:03.342829 35003 solver.cpp:239] Iteration 177070 (2.88632 iter/s, 3.46462s/10 iters), loss = 6.35647
I0523 07:48:03.342949 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35647 (* 1 = 6.35647 loss)
I0523 07:48:03.903197 35003 sgd_solver.cpp:112] Iteration 177070, lr = 0.001
I0523 07:48:07.814996 35003 solver.cpp:239] Iteration 177080 (2.2362 iter/s, 4.47186s/10 iters), loss = 6.29635
I0523 07:48:07.815044 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29635 (* 1 = 6.29635 loss)
I0523 07:48:07.842871 35003 sgd_solver.cpp:112] Iteration 177080, lr = 0.001
I0523 07:48:12.078019 35003 solver.cpp:239] Iteration 177090 (2.34587 iter/s, 4.2628s/10 iters), loss = 5.87575
I0523 07:48:12.078061 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87575 (* 1 = 5.87575 loss)
I0523 07:48:12.086046 35003 sgd_solver.cpp:112] Iteration 177090, lr = 0.001
I0523 07:48:16.399248 35003 solver.cpp:239] Iteration 177100 (2.31428 iter/s, 4.32101s/10 iters), loss = 6.99752
I0523 07:48:16.399307 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99752 (* 1 = 6.99752 loss)
I0523 07:48:17.134395 35003 sgd_solver.cpp:112] Iteration 177100, lr = 0.001
I0523 07:48:21.343646 35003 solver.cpp:239] Iteration 177110 (2.0226 iter/s, 4.94414s/10 iters), loss = 6.56739
I0523 07:48:21.343689 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56739 (* 1 = 6.56739 loss)
I0523 07:48:21.988549 35003 sgd_solver.cpp:112] Iteration 177110, lr = 0.001
I0523 07:48:25.577152 35003 solver.cpp:239] Iteration 177120 (2.36223 iter/s, 4.23329s/10 iters), loss = 4.89835
I0523 07:48:25.577203 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.89835 (* 1 = 4.89835 loss)
I0523 07:48:25.585711 35003 sgd_solver.cpp:112] Iteration 177120, lr = 0.001
I0523 07:48:27.619055 35003 solver.cpp:239] Iteration 177130 (4.89773 iter/s, 2.04176s/10 iters), loss = 6.67915
I0523 07:48:27.619103 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67915 (* 1 = 6.67915 loss)
I0523 07:48:27.628371 35003 sgd_solver.cpp:112] Iteration 177130, lr = 0.001
I0523 07:48:29.715147 35003 solver.cpp:239] Iteration 177140 (4.7711 iter/s, 2.09595s/10 iters), loss = 6.55829
I0523 07:48:29.715188 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55829 (* 1 = 6.55829 loss)
I0523 07:48:29.722445 35003 sgd_solver.cpp:112] Iteration 177140, lr = 0.001
I0523 07:48:32.439225 35003 solver.cpp:239] Iteration 177150 (3.67118 iter/s, 2.72392s/10 iters), loss = 6.20281
I0523 07:48:32.439267 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20281 (* 1 = 6.20281 loss)
I0523 07:48:32.445602 35003 sgd_solver.cpp:112] Iteration 177150, lr = 0.001
I0523 07:48:37.487035 35003 solver.cpp:239] Iteration 177160 (1.98115 iter/s, 5.04756s/10 iters), loss = 7.15647
I0523 07:48:37.487320 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15647 (* 1 = 7.15647 loss)
I0523 07:48:38.195271 35003 sgd_solver.cpp:112] Iteration 177160, lr = 0.001
I0523 07:48:42.565989 35003 solver.cpp:239] Iteration 177170 (1.96909 iter/s, 5.0785s/10 iters), loss = 7.17516
I0523 07:48:42.566033 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17516 (* 1 = 7.17516 loss)
I0523 07:48:42.576772 35003 sgd_solver.cpp:112] Iteration 177170, lr = 0.001
I0523 07:48:46.052758 35003 solver.cpp:239] Iteration 177180 (2.86814 iter/s, 3.48658s/10 iters), loss = 6.11673
I0523 07:48:46.052796 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11673 (* 1 = 6.11673 loss)
I0523 07:48:46.071619 35003 sgd_solver.cpp:112] Iteration 177180, lr = 0.001
I0523 07:48:49.509567 35003 solver.cpp:239] Iteration 177190 (2.893 iter/s, 3.45662s/10 iters), loss = 6.80741
I0523 07:48:49.509609 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80741 (* 1 = 6.80741 loss)
I0523 07:48:49.536085 35003 sgd_solver.cpp:112] Iteration 177190, lr = 0.001
I0523 07:48:53.068634 35003 solver.cpp:239] Iteration 177200 (2.80988 iter/s, 3.55888s/10 iters), loss = 5.7159
I0523 07:48:53.068673 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7159 (* 1 = 5.7159 loss)
I0523 07:48:53.075983 35003 sgd_solver.cpp:112] Iteration 177200, lr = 0.001
I0523 07:48:56.777434 35003 solver.cpp:239] Iteration 177210 (2.69643 iter/s, 3.70861s/10 iters), loss = 7.22223
I0523 07:48:56.777485 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22223 (* 1 = 7.22223 loss)
I0523 07:48:56.952675 35003 sgd_solver.cpp:112] Iteration 177210, lr = 0.001
I0523 07:49:01.186017 35003 solver.cpp:239] Iteration 177220 (2.26843 iter/s, 4.40834s/10 iters), loss = 7.73783
I0523 07:49:01.186074 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73783 (* 1 = 7.73783 loss)
I0523 07:49:01.190966 35003 sgd_solver.cpp:112] Iteration 177220, lr = 0.001
I0523 07:49:04.933315 35003 solver.cpp:239] Iteration 177230 (2.66874 iter/s, 3.74709s/10 iters), loss = 5.96144
I0523 07:49:04.933359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96144 (* 1 = 5.96144 loss)
I0523 07:49:05.642696 35003 sgd_solver.cpp:112] Iteration 177230, lr = 0.001
I0523 07:49:09.381145 35003 solver.cpp:239] Iteration 177240 (2.2484 iter/s, 4.44761s/10 iters), loss = 6.78879
I0523 07:49:09.381459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78879 (* 1 = 6.78879 loss)
I0523 07:49:09.394368 35003 sgd_solver.cpp:112] Iteration 177240, lr = 0.001
I0523 07:49:12.239048 35003 solver.cpp:239] Iteration 177250 (3.49957 iter/s, 2.8575s/10 iters), loss = 7.25865
I0523 07:49:12.239084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25865 (* 1 = 7.25865 loss)
I0523 07:49:12.257086 35003 sgd_solver.cpp:112] Iteration 177250, lr = 0.001
I0523 07:49:15.303045 35003 solver.cpp:239] Iteration 177260 (3.26389 iter/s, 3.06383s/10 iters), loss = 6.71476
I0523 07:49:15.303086 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71476 (* 1 = 6.71476 loss)
I0523 07:49:15.342061 35003 sgd_solver.cpp:112] Iteration 177260, lr = 0.001
I0523 07:49:19.755975 35003 solver.cpp:239] Iteration 177270 (2.24583 iter/s, 4.4527s/10 iters), loss = 5.3205
I0523 07:49:19.756031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.3205 (* 1 = 5.3205 loss)
I0523 07:49:19.769923 35003 sgd_solver.cpp:112] Iteration 177270, lr = 0.001
I0523 07:49:21.721030 35003 solver.cpp:239] Iteration 177280 (5.08928 iter/s, 1.96491s/10 iters), loss = 6.65549
I0523 07:49:21.721071 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65549 (* 1 = 6.65549 loss)
I0523 07:49:21.748123 35003 sgd_solver.cpp:112] Iteration 177280, lr = 0.001
I0523 07:49:24.305053 35003 solver.cpp:239] Iteration 177290 (3.87016 iter/s, 2.58387s/10 iters), loss = 6.5362
I0523 07:49:24.305109 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5362 (* 1 = 6.5362 loss)
I0523 07:49:24.310895 35003 sgd_solver.cpp:112] Iteration 177290, lr = 0.001
I0523 07:49:27.828608 35003 solver.cpp:239] Iteration 177300 (2.8382 iter/s, 3.52336s/10 iters), loss = 6.34181
I0523 07:49:27.828646 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34181 (* 1 = 6.34181 loss)
I0523 07:49:27.842118 35003 sgd_solver.cpp:112] Iteration 177300, lr = 0.001
I0523 07:49:29.842797 35003 solver.cpp:239] Iteration 177310 (4.96512 iter/s, 2.01405s/10 iters), loss = 6.50918
I0523 07:49:29.842860 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50918 (* 1 = 6.50918 loss)
I0523 07:49:29.849810 35003 sgd_solver.cpp:112] Iteration 177310, lr = 0.001
I0523 07:49:33.382571 35003 solver.cpp:239] Iteration 177320 (2.82521 iter/s, 3.53956s/10 iters), loss = 6.88151
I0523 07:49:33.382617 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88151 (* 1 = 6.88151 loss)
I0523 07:49:33.395704 35003 sgd_solver.cpp:112] Iteration 177320, lr = 0.001
I0523 07:49:36.986513 35003 solver.cpp:239] Iteration 177330 (2.7749 iter/s, 3.60373s/10 iters), loss = 7.23044
I0523 07:49:36.986553 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23044 (* 1 = 7.23044 loss)
I0523 07:49:36.999204 35003 sgd_solver.cpp:112] Iteration 177330, lr = 0.001
I0523 07:49:39.768859 35003 solver.cpp:239] Iteration 177340 (3.5972 iter/s, 2.77994s/10 iters), loss = 5.89342
I0523 07:49:39.769107 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89342 (* 1 = 5.89342 loss)
I0523 07:49:39.787008 35003 sgd_solver.cpp:112] Iteration 177340, lr = 0.001
I0523 07:49:42.542330 35003 solver.cpp:239] Iteration 177350 (3.60605 iter/s, 2.77312s/10 iters), loss = 6.70934
I0523 07:49:42.542377 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70934 (* 1 = 6.70934 loss)
I0523 07:49:42.548768 35003 sgd_solver.cpp:112] Iteration 177350, lr = 0.001
I0523 07:49:45.486654 35003 solver.cpp:239] Iteration 177360 (3.39657 iter/s, 2.94415s/10 iters), loss = 6.17619
I0523 07:49:45.486719 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17619 (* 1 = 6.17619 loss)
I0523 07:49:46.184504 35003 sgd_solver.cpp:112] Iteration 177360, lr = 0.001
I0523 07:49:51.276202 35003 solver.cpp:239] Iteration 177370 (1.72733 iter/s, 5.78927s/10 iters), loss = 6.33553
I0523 07:49:51.276255 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33553 (* 1 = 6.33553 loss)
I0523 07:49:51.281486 35003 sgd_solver.cpp:112] Iteration 177370, lr = 0.001
I0523 07:49:53.389441 35003 solver.cpp:239] Iteration 177380 (4.73239 iter/s, 2.1131s/10 iters), loss = 6.53316
I0523 07:49:53.389479 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53316 (* 1 = 6.53316 loss)
I0523 07:49:53.722080 35003 sgd_solver.cpp:112] Iteration 177380, lr = 0.001
I0523 07:49:55.871625 35003 solver.cpp:239] Iteration 177390 (4.02895 iter/s, 2.48203s/10 iters), loss = 7.65316
I0523 07:49:55.871675 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65316 (* 1 = 7.65316 loss)
I0523 07:49:55.880583 35003 sgd_solver.cpp:112] Iteration 177390, lr = 0.001
I0523 07:49:59.488281 35003 solver.cpp:239] Iteration 177400 (2.76514 iter/s, 3.61646s/10 iters), loss = 7.17646
I0523 07:49:59.488322 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17646 (* 1 = 7.17646 loss)
I0523 07:49:59.500802 35003 sgd_solver.cpp:112] Iteration 177400, lr = 0.001
I0523 07:50:04.053856 35003 solver.cpp:239] Iteration 177410 (2.19041 iter/s, 4.56535s/10 iters), loss = 6.7897
I0523 07:50:04.053895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7897 (* 1 = 6.7897 loss)
I0523 07:50:04.072054 35003 sgd_solver.cpp:112] Iteration 177410, lr = 0.001
I0523 07:50:08.091414 35003 solver.cpp:239] Iteration 177420 (2.47688 iter/s, 4.03735s/10 iters), loss = 6.08439
I0523 07:50:08.091467 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08439 (* 1 = 6.08439 loss)
I0523 07:50:08.685169 35003 sgd_solver.cpp:112] Iteration 177420, lr = 0.001
I0523 07:50:13.026998 35003 solver.cpp:239] Iteration 177430 (2.02621 iter/s, 4.93533s/10 iters), loss = 6.19979
I0523 07:50:13.027242 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19979 (* 1 = 6.19979 loss)
I0523 07:50:13.029470 35003 sgd_solver.cpp:112] Iteration 177430, lr = 0.001
I0523 07:50:16.625805 35003 solver.cpp:239] Iteration 177440 (2.77899 iter/s, 3.59843s/10 iters), loss = 5.77035
I0523 07:50:16.625851 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77035 (* 1 = 5.77035 loss)
I0523 07:50:16.639699 35003 sgd_solver.cpp:112] Iteration 177440, lr = 0.001
I0523 07:50:19.939461 35003 solver.cpp:239] Iteration 177450 (3.01798 iter/s, 3.31347s/10 iters), loss = 5.92
I0523 07:50:19.939498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92 (* 1 = 5.92 loss)
I0523 07:50:19.952647 35003 sgd_solver.cpp:112] Iteration 177450, lr = 0.001
I0523 07:50:23.651109 35003 solver.cpp:239] Iteration 177460 (2.69436 iter/s, 3.71145s/10 iters), loss = 6.37838
I0523 07:50:23.651156 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37838 (* 1 = 6.37838 loss)
I0523 07:50:23.658809 35003 sgd_solver.cpp:112] Iteration 177460, lr = 0.001
I0523 07:50:26.449359 35003 solver.cpp:239] Iteration 177470 (3.57388 iter/s, 2.79808s/10 iters), loss = 7.24193
I0523 07:50:26.449421 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24193 (* 1 = 7.24193 loss)
I0523 07:50:26.467921 35003 sgd_solver.cpp:112] Iteration 177470, lr = 0.001
I0523 07:50:29.841176 35003 solver.cpp:239] Iteration 177480 (2.94845 iter/s, 3.39161s/10 iters), loss = 7.14493
I0523 07:50:29.841223 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14493 (* 1 = 7.14493 loss)
I0523 07:50:29.933960 35003 sgd_solver.cpp:112] Iteration 177480, lr = 0.001
I0523 07:50:33.358263 35003 solver.cpp:239] Iteration 177490 (2.84342 iter/s, 3.5169s/10 iters), loss = 7.00228
I0523 07:50:33.358299 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00228 (* 1 = 7.00228 loss)
I0523 07:50:33.372161 35003 sgd_solver.cpp:112] Iteration 177490, lr = 0.001
I0523 07:50:37.021066 35003 solver.cpp:239] Iteration 177500 (2.73029 iter/s, 3.66261s/10 iters), loss = 6.34782
I0523 07:50:37.021111 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34782 (* 1 = 6.34782 loss)
I0523 07:50:37.027986 35003 sgd_solver.cpp:112] Iteration 177500, lr = 0.001
I0523 07:50:39.850970 35003 solver.cpp:239] Iteration 177510 (3.53391 iter/s, 2.82973s/10 iters), loss = 7.24138
I0523 07:50:39.851039 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24138 (* 1 = 7.24138 loss)
I0523 07:50:40.586905 35003 sgd_solver.cpp:112] Iteration 177510, lr = 0.001
I0523 07:50:43.404750 35003 solver.cpp:239] Iteration 177520 (2.81407 iter/s, 3.55357s/10 iters), loss = 6.98936
I0523 07:50:43.405040 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98936 (* 1 = 6.98936 loss)
I0523 07:50:43.809232 35003 sgd_solver.cpp:112] Iteration 177520, lr = 0.001
I0523 07:50:46.644289 35003 solver.cpp:239] Iteration 177530 (3.08724 iter/s, 3.23914s/10 iters), loss = 5.55056
I0523 07:50:46.644330 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.55056 (* 1 = 5.55056 loss)
I0523 07:50:46.656867 35003 sgd_solver.cpp:112] Iteration 177530, lr = 0.001
I0523 07:50:50.663329 35003 solver.cpp:239] Iteration 177540 (2.48829 iter/s, 4.01883s/10 iters), loss = 6.28738
I0523 07:50:50.663383 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28738 (* 1 = 6.28738 loss)
I0523 07:50:50.671224 35003 sgd_solver.cpp:112] Iteration 177540, lr = 0.001
I0523 07:50:54.309612 35003 solver.cpp:239] Iteration 177550 (2.74267 iter/s, 3.64608s/10 iters), loss = 7.73393
I0523 07:50:54.309649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73393 (* 1 = 7.73393 loss)
I0523 07:50:54.357522 35003 sgd_solver.cpp:112] Iteration 177550, lr = 0.001
I0523 07:50:56.996096 35003 solver.cpp:239] Iteration 177560 (3.72256 iter/s, 2.68632s/10 iters), loss = 6.76121
I0523 07:50:56.996148 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76121 (* 1 = 6.76121 loss)
I0523 07:50:57.008975 35003 sgd_solver.cpp:112] Iteration 177560, lr = 0.001
I0523 07:51:01.295792 35003 solver.cpp:239] Iteration 177570 (2.32587 iter/s, 4.29946s/10 iters), loss = 6.87384
I0523 07:51:01.295845 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87384 (* 1 = 6.87384 loss)
I0523 07:51:01.300207 35003 sgd_solver.cpp:112] Iteration 177570, lr = 0.001
I0523 07:51:05.618660 35003 solver.cpp:239] Iteration 177580 (2.31341 iter/s, 4.32263s/10 iters), loss = 7.73229
I0523 07:51:05.618716 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.73229 (* 1 = 7.73229 loss)
I0523 07:51:05.638738 35003 sgd_solver.cpp:112] Iteration 177580, lr = 0.001
I0523 07:51:08.966889 35003 solver.cpp:239] Iteration 177590 (2.98683 iter/s, 3.34803s/10 iters), loss = 6.80663
I0523 07:51:08.966928 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80663 (* 1 = 6.80663 loss)
I0523 07:51:08.971164 35003 sgd_solver.cpp:112] Iteration 177590, lr = 0.001
I0523 07:51:11.642360 35003 solver.cpp:239] Iteration 177600 (3.73788 iter/s, 2.67532s/10 iters), loss = 6.6659
I0523 07:51:11.642403 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6659 (* 1 = 6.6659 loss)
I0523 07:51:11.648254 35003 sgd_solver.cpp:112] Iteration 177600, lr = 0.001
I0523 07:51:16.063730 35003 solver.cpp:239] Iteration 177610 (2.26186 iter/s, 4.42114s/10 iters), loss = 6.69885
I0523 07:51:16.063984 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69885 (* 1 = 6.69885 loss)
I0523 07:51:16.774963 35003 sgd_solver.cpp:112] Iteration 177610, lr = 0.001
I0523 07:51:20.765329 35003 solver.cpp:239] Iteration 177620 (2.12712 iter/s, 4.70119s/10 iters), loss = 5.90131
I0523 07:51:20.765372 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.90131 (* 1 = 5.90131 loss)
I0523 07:51:20.772527 35003 sgd_solver.cpp:112] Iteration 177620, lr = 0.001
I0523 07:51:23.823232 35003 solver.cpp:239] Iteration 177630 (3.27041 iter/s, 3.05772s/10 iters), loss = 7.93458
I0523 07:51:23.823281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93458 (* 1 = 7.93458 loss)
I0523 07:51:23.829854 35003 sgd_solver.cpp:112] Iteration 177630, lr = 0.001
I0523 07:51:27.466838 35003 solver.cpp:239] Iteration 177640 (2.74469 iter/s, 3.6434s/10 iters), loss = 6.65062
I0523 07:51:27.466907 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65062 (* 1 = 6.65062 loss)
I0523 07:51:27.468396 35003 sgd_solver.cpp:112] Iteration 177640, lr = 0.001
I0523 07:51:28.761484 35003 solver.cpp:239] Iteration 177650 (7.72486 iter/s, 1.29452s/10 iters), loss = 7.7077
I0523 07:51:28.761529 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7077 (* 1 = 7.7077 loss)
I0523 07:51:28.768991 35003 sgd_solver.cpp:112] Iteration 177650, lr = 0.001
I0523 07:51:31.846267 35003 solver.cpp:239] Iteration 177660 (3.24191 iter/s, 3.0846s/10 iters), loss = 6.79838
I0523 07:51:31.846312 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79838 (* 1 = 6.79838 loss)
I0523 07:51:31.856652 35003 sgd_solver.cpp:112] Iteration 177660, lr = 0.001
I0523 07:51:34.635447 35003 solver.cpp:239] Iteration 177670 (3.58549 iter/s, 2.78902s/10 iters), loss = 6.89002
I0523 07:51:34.635483 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89002 (* 1 = 6.89002 loss)
I0523 07:51:35.343402 35003 sgd_solver.cpp:112] Iteration 177670, lr = 0.001
I0523 07:51:37.539014 35003 solver.cpp:239] Iteration 177680 (3.44423 iter/s, 2.90341s/10 iters), loss = 6.72686
I0523 07:51:37.539067 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72686 (* 1 = 6.72686 loss)
I0523 07:51:38.279721 35003 sgd_solver.cpp:112] Iteration 177680, lr = 0.001
I0523 07:51:43.167804 35003 solver.cpp:239] Iteration 177690 (1.77668 iter/s, 5.62849s/10 iters), loss = 6.71454
I0523 07:51:43.167861 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71454 (* 1 = 6.71454 loss)
I0523 07:51:43.174643 35003 sgd_solver.cpp:112] Iteration 177690, lr = 0.001
I0523 07:51:46.400354 35003 solver.cpp:239] Iteration 177700 (3.09373 iter/s, 3.23235s/10 iters), loss = 8.72045
I0523 07:51:46.400668 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.72045 (* 1 = 8.72045 loss)
I0523 07:51:46.408067 35003 sgd_solver.cpp:112] Iteration 177700, lr = 0.001
I0523 07:51:50.017215 35003 solver.cpp:239] Iteration 177710 (2.76516 iter/s, 3.61643s/10 iters), loss = 6.38355
I0523 07:51:50.017262 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38355 (* 1 = 6.38355 loss)
I0523 07:51:50.702370 35003 sgd_solver.cpp:112] Iteration 177710, lr = 0.001
I0523 07:51:55.164324 35003 solver.cpp:239] Iteration 177720 (1.94294 iter/s, 5.14685s/10 iters), loss = 6.80647
I0523 07:51:55.164376 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80647 (* 1 = 6.80647 loss)
I0523 07:51:55.905550 35003 sgd_solver.cpp:112] Iteration 177720, lr = 0.001
I0523 07:51:58.723867 35003 solver.cpp:239] Iteration 177730 (2.80951 iter/s, 3.55934s/10 iters), loss = 6.09534
I0523 07:51:58.723908 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09534 (* 1 = 6.09534 loss)
I0523 07:51:58.736735 35003 sgd_solver.cpp:112] Iteration 177730, lr = 0.001
I0523 07:52:01.563721 35003 solver.cpp:239] Iteration 177740 (3.52151 iter/s, 2.83969s/10 iters), loss = 5.88798
I0523 07:52:01.563766 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.88798 (* 1 = 5.88798 loss)
I0523 07:52:01.570174 35003 sgd_solver.cpp:112] Iteration 177740, lr = 0.001
I0523 07:52:05.194455 35003 solver.cpp:239] Iteration 177750 (2.75441 iter/s, 3.63054s/10 iters), loss = 6.62185
I0523 07:52:05.194504 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62185 (* 1 = 6.62185 loss)
I0523 07:52:05.577437 35003 sgd_solver.cpp:112] Iteration 177750, lr = 0.001
I0523 07:52:08.581403 35003 solver.cpp:239] Iteration 177760 (2.95267 iter/s, 3.38676s/10 iters), loss = 6.94104
I0523 07:52:08.581442 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94104 (* 1 = 6.94104 loss)
I0523 07:52:08.587838 35003 sgd_solver.cpp:112] Iteration 177760, lr = 0.001
I0523 07:52:12.263710 35003 solver.cpp:239] Iteration 177770 (2.71583 iter/s, 3.68211s/10 iters), loss = 6.16537
I0523 07:52:12.263767 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16537 (* 1 = 6.16537 loss)
I0523 07:52:12.270077 35003 sgd_solver.cpp:112] Iteration 177770, lr = 0.001
I0523 07:52:15.830529 35003 solver.cpp:239] Iteration 177780 (2.80378 iter/s, 3.56661s/10 iters), loss = 6.5126
I0523 07:52:15.830581 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5126 (* 1 = 6.5126 loss)
I0523 07:52:15.845964 35003 sgd_solver.cpp:112] Iteration 177780, lr = 0.001
I0523 07:52:18.400674 35003 solver.cpp:239] Iteration 177790 (3.89108 iter/s, 2.56998s/10 iters), loss = 7.48824
I0523 07:52:18.400935 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48824 (* 1 = 7.48824 loss)
I0523 07:52:18.410128 35003 sgd_solver.cpp:112] Iteration 177790, lr = 0.001
I0523 07:52:21.551359 35003 solver.cpp:239] Iteration 177800 (3.17429 iter/s, 3.15031s/10 iters), loss = 5.87039
I0523 07:52:21.551400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87039 (* 1 = 5.87039 loss)
I0523 07:52:22.286249 35003 sgd_solver.cpp:112] Iteration 177800, lr = 0.001
I0523 07:52:25.114866 35003 solver.cpp:239] Iteration 177810 (2.80638 iter/s, 3.56332s/10 iters), loss = 6.12498
I0523 07:52:25.114917 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12498 (* 1 = 6.12498 loss)
I0523 07:52:25.853994 35003 sgd_solver.cpp:112] Iteration 177810, lr = 0.001
I0523 07:52:28.655006 35003 solver.cpp:239] Iteration 177820 (2.82491 iter/s, 3.53994s/10 iters), loss = 6.62108
I0523 07:52:28.655043 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62108 (* 1 = 6.62108 loss)
I0523 07:52:28.668241 35003 sgd_solver.cpp:112] Iteration 177820, lr = 0.001
I0523 07:52:31.048725 35003 solver.cpp:239] Iteration 177830 (4.17785 iter/s, 2.39358s/10 iters), loss = 6.39748
I0523 07:52:31.048771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39748 (* 1 = 6.39748 loss)
I0523 07:52:31.054440 35003 sgd_solver.cpp:112] Iteration 177830, lr = 0.001
I0523 07:52:33.853725 35003 solver.cpp:239] Iteration 177840 (3.56527 iter/s, 2.80483s/10 iters), loss = 7.27308
I0523 07:52:33.853766 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27308 (* 1 = 7.27308 loss)
I0523 07:52:33.861094 35003 sgd_solver.cpp:112] Iteration 177840, lr = 0.001
I0523 07:52:37.831324 35003 solver.cpp:239] Iteration 177850 (2.51421 iter/s, 3.97739s/10 iters), loss = 6.73404
I0523 07:52:37.831362 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73404 (* 1 = 6.73404 loss)
I0523 07:52:37.841434 35003 sgd_solver.cpp:112] Iteration 177850, lr = 0.001
I0523 07:52:39.831380 35003 solver.cpp:239] Iteration 177860 (5.0002 iter/s, 1.99992s/10 iters), loss = 7.49169
I0523 07:52:39.831425 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49169 (* 1 = 7.49169 loss)
I0523 07:52:39.844414 35003 sgd_solver.cpp:112] Iteration 177860, lr = 0.001
I0523 07:52:41.909622 35003 solver.cpp:239] Iteration 177870 (4.81212 iter/s, 2.07809s/10 iters), loss = 6.79888
I0523 07:52:41.909667 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79888 (* 1 = 6.79888 loss)
I0523 07:52:42.523443 35003 sgd_solver.cpp:112] Iteration 177870, lr = 0.001
I0523 07:52:47.036892 35003 solver.cpp:239] Iteration 177880 (1.95045 iter/s, 5.12702s/10 iters), loss = 7.3206
I0523 07:52:47.036957 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3206 (* 1 = 7.3206 loss)
I0523 07:52:47.050511 35003 sgd_solver.cpp:112] Iteration 177880, lr = 0.001
I0523 07:52:49.999658 35003 solver.cpp:239] Iteration 177890 (3.37544 iter/s, 2.96257s/10 iters), loss = 7.1379
I0523 07:52:49.999822 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1379 (* 1 = 7.1379 loss)
I0523 07:52:50.009217 35003 sgd_solver.cpp:112] Iteration 177890, lr = 0.001
I0523 07:52:54.455862 35003 solver.cpp:239] Iteration 177900 (2.24423 iter/s, 4.45587s/10 iters), loss = 6.40365
I0523 07:52:54.455901 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40365 (* 1 = 6.40365 loss)
I0523 07:52:54.467867 35003 sgd_solver.cpp:112] Iteration 177900, lr = 0.001
I0523 07:52:57.470135 35003 solver.cpp:239] Iteration 177910 (3.31774 iter/s, 3.0141s/10 iters), loss = 6.76934
I0523 07:52:57.470180 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76934 (* 1 = 6.76934 loss)
I0523 07:52:57.480710 35003 sgd_solver.cpp:112] Iteration 177910, lr = 0.001
I0523 07:53:01.013425 35003 solver.cpp:239] Iteration 177920 (2.82239 iter/s, 3.5431s/10 iters), loss = 7.18204
I0523 07:53:01.013463 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18204 (* 1 = 7.18204 loss)
I0523 07:53:01.032644 35003 sgd_solver.cpp:112] Iteration 177920, lr = 0.001
I0523 07:53:03.060772 35003 solver.cpp:239] Iteration 177930 (4.88469 iter/s, 2.04721s/10 iters), loss = 7.16117
I0523 07:53:03.060828 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16117 (* 1 = 7.16117 loss)
I0523 07:53:03.772326 35003 sgd_solver.cpp:112] Iteration 177930, lr = 0.001
I0523 07:53:07.353513 35003 solver.cpp:239] Iteration 177940 (2.32964 iter/s, 4.29251s/10 iters), loss = 5.74592
I0523 07:53:07.353552 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74592 (* 1 = 5.74592 loss)
I0523 07:53:07.371613 35003 sgd_solver.cpp:112] Iteration 177940, lr = 0.001
I0523 07:53:09.390362 35003 solver.cpp:239] Iteration 177950 (4.90985 iter/s, 2.03672s/10 iters), loss = 6.6599
I0523 07:53:09.390404 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6599 (* 1 = 6.6599 loss)
I0523 07:53:09.412870 35003 sgd_solver.cpp:112] Iteration 177950, lr = 0.001
I0523 07:53:13.095492 35003 solver.cpp:239] Iteration 177960 (2.6991 iter/s, 3.70494s/10 iters), loss = 5.7651
I0523 07:53:13.095537 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7651 (* 1 = 5.7651 loss)
I0523 07:53:13.804592 35003 sgd_solver.cpp:112] Iteration 177960, lr = 0.001
I0523 07:53:16.700359 35003 solver.cpp:239] Iteration 177970 (2.77418 iter/s, 3.60467s/10 iters), loss = 7.1536
I0523 07:53:16.700402 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1536 (* 1 = 7.1536 loss)
I0523 07:53:17.432603 35003 sgd_solver.cpp:112] Iteration 177970, lr = 0.001
I0523 07:53:22.320111 35003 solver.cpp:239] Iteration 177980 (1.77952 iter/s, 5.61948s/10 iters), loss = 7.60577
I0523 07:53:22.320359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60577 (* 1 = 7.60577 loss)
I0523 07:53:22.365154 35003 sgd_solver.cpp:112] Iteration 177980, lr = 0.001
I0523 07:53:25.091538 35003 solver.cpp:239] Iteration 177990 (3.60873 iter/s, 2.77106s/10 iters), loss = 6.30576
I0523 07:53:25.091590 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30576 (* 1 = 6.30576 loss)
I0523 07:53:25.104027 35003 sgd_solver.cpp:112] Iteration 177990, lr = 0.001
I0523 07:53:28.714397 35003 solver.cpp:239] Iteration 178000 (2.7604 iter/s, 3.62266s/10 iters), loss = 6.45739
I0523 07:53:28.714439 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45739 (* 1 = 6.45739 loss)
I0523 07:53:28.719471 35003 sgd_solver.cpp:112] Iteration 178000, lr = 0.001
I0523 07:53:33.383682 35003 solver.cpp:239] Iteration 178010 (2.14177 iter/s, 4.66904s/10 iters), loss = 6.29923
I0523 07:53:33.383731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29923 (* 1 = 6.29923 loss)
I0523 07:53:33.396909 35003 sgd_solver.cpp:112] Iteration 178010, lr = 0.001
I0523 07:53:35.027092 35003 solver.cpp:239] Iteration 178020 (6.08536 iter/s, 1.64329s/10 iters), loss = 6.10186
I0523 07:53:35.027140 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10186 (* 1 = 6.10186 loss)
I0523 07:53:35.040422 35003 sgd_solver.cpp:112] Iteration 178020, lr = 0.001
I0523 07:53:36.435294 35003 solver.cpp:239] Iteration 178030 (7.10185 iter/s, 1.40808s/10 iters), loss = 7.42697
I0523 07:53:36.435348 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42697 (* 1 = 7.42697 loss)
I0523 07:53:37.020006 35003 sgd_solver.cpp:112] Iteration 178030, lr = 0.001
I0523 07:53:39.842973 35003 solver.cpp:239] Iteration 178040 (2.93472 iter/s, 3.40748s/10 iters), loss = 5.81482
I0523 07:53:39.843021 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.81482 (* 1 = 5.81482 loss)
I0523 07:53:40.485517 35003 sgd_solver.cpp:112] Iteration 178040, lr = 0.001
I0523 07:53:43.367710 35003 solver.cpp:239] Iteration 178050 (2.83725 iter/s, 3.52454s/10 iters), loss = 6.23455
I0523 07:53:43.367751 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23455 (* 1 = 6.23455 loss)
I0523 07:53:44.051622 35003 sgd_solver.cpp:112] Iteration 178050, lr = 0.001
I0523 07:53:48.621711 35003 solver.cpp:239] Iteration 178060 (1.9034 iter/s, 5.25375s/10 iters), loss = 7.96922
I0523 07:53:48.621752 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96922 (* 1 = 7.96922 loss)
I0523 07:53:49.356578 35003 sgd_solver.cpp:112] Iteration 178060, lr = 0.001
I0523 07:53:53.271128 35003 solver.cpp:239] Iteration 178070 (2.15091 iter/s, 4.64919s/10 iters), loss = 6.35115
I0523 07:53:53.271364 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35115 (* 1 = 6.35115 loss)
I0523 07:53:53.943545 35003 sgd_solver.cpp:112] Iteration 178070, lr = 0.001
I0523 07:53:56.248020 35003 solver.cpp:239] Iteration 178080 (3.35962 iter/s, 2.97653s/10 iters), loss = 6.98593
I0523 07:53:56.248062 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98593 (* 1 = 6.98593 loss)
I0523 07:53:56.445053 35003 sgd_solver.cpp:112] Iteration 178080, lr = 0.001
I0523 07:53:59.524219 35003 solver.cpp:239] Iteration 178090 (3.05249 iter/s, 3.27602s/10 iters), loss = 5.87313
I0523 07:53:59.524262 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87313 (* 1 = 5.87313 loss)
I0523 07:53:59.535323 35003 sgd_solver.cpp:112] Iteration 178090, lr = 0.001
I0523 07:54:01.594390 35003 solver.cpp:239] Iteration 178100 (4.83086 iter/s, 2.07003s/10 iters), loss = 6.14751
I0523 07:54:01.594439 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14751 (* 1 = 6.14751 loss)
I0523 07:54:01.607563 35003 sgd_solver.cpp:112] Iteration 178100, lr = 0.001
I0523 07:54:06.092758 35003 solver.cpp:239] Iteration 178110 (2.22315 iter/s, 4.49813s/10 iters), loss = 6.71003
I0523 07:54:06.092813 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71003 (* 1 = 6.71003 loss)
I0523 07:54:06.097923 35003 sgd_solver.cpp:112] Iteration 178110, lr = 0.001
I0523 07:54:09.719523 35003 solver.cpp:239] Iteration 178120 (2.75744 iter/s, 3.62656s/10 iters), loss = 6.22624
I0523 07:54:09.719568 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22624 (* 1 = 6.22624 loss)
I0523 07:54:09.733801 35003 sgd_solver.cpp:112] Iteration 178120, lr = 0.001
I0523 07:54:13.651118 35003 solver.cpp:239] Iteration 178130 (2.54363 iter/s, 3.93139s/10 iters), loss = 6.57175
I0523 07:54:13.651155 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57175 (* 1 = 6.57175 loss)
I0523 07:54:13.663920 35003 sgd_solver.cpp:112] Iteration 178130, lr = 0.001
I0523 07:54:17.164158 35003 solver.cpp:239] Iteration 178140 (2.84669 iter/s, 3.51286s/10 iters), loss = 7.40542
I0523 07:54:17.164193 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40542 (* 1 = 7.40542 loss)
I0523 07:54:17.182819 35003 sgd_solver.cpp:112] Iteration 178140, lr = 0.001
I0523 07:54:20.505654 35003 solver.cpp:239] Iteration 178150 (2.99283 iter/s, 3.34132s/10 iters), loss = 6.61508
I0523 07:54:20.505694 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61508 (* 1 = 6.61508 loss)
I0523 07:54:20.518935 35003 sgd_solver.cpp:112] Iteration 178150, lr = 0.001
I0523 07:54:22.650552 35003 solver.cpp:239] Iteration 178160 (4.66253 iter/s, 2.14476s/10 iters), loss = 5.63544
I0523 07:54:22.650598 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.63544 (* 1 = 5.63544 loss)
I0523 07:54:22.883967 35003 sgd_solver.cpp:112] Iteration 178160, lr = 0.001
I0523 07:54:24.861472 35003 solver.cpp:239] Iteration 178170 (4.52329 iter/s, 2.21078s/10 iters), loss = 7.59954
I0523 07:54:24.861749 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59954 (* 1 = 7.59954 loss)
I0523 07:54:24.874459 35003 sgd_solver.cpp:112] Iteration 178170, lr = 0.001
I0523 07:54:27.089897 35003 solver.cpp:239] Iteration 178180 (4.48817 iter/s, 2.22808s/10 iters), loss = 5.87694
I0523 07:54:27.089933 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87694 (* 1 = 5.87694 loss)
I0523 07:54:27.102859 35003 sgd_solver.cpp:112] Iteration 178180, lr = 0.001
I0523 07:54:31.640231 35003 solver.cpp:239] Iteration 178190 (2.19775 iter/s, 4.55011s/10 iters), loss = 6.47406
I0523 07:54:31.640287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47406 (* 1 = 6.47406 loss)
I0523 07:54:32.268110 35003 sgd_solver.cpp:112] Iteration 178190, lr = 0.001
I0523 07:54:34.971488 35003 solver.cpp:239] Iteration 178200 (3.00204 iter/s, 3.33106s/10 iters), loss = 6.0642
I0523 07:54:34.971534 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0642 (* 1 = 6.0642 loss)
I0523 07:54:35.692960 35003 sgd_solver.cpp:112] Iteration 178200, lr = 0.001
I0523 07:54:40.375394 35003 solver.cpp:239] Iteration 178210 (1.8506 iter/s, 5.40364s/10 iters), loss = 6.60978
I0523 07:54:40.375435 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60978 (* 1 = 6.60978 loss)
I0523 07:54:40.397531 35003 sgd_solver.cpp:112] Iteration 178210, lr = 0.001
I0523 07:54:43.284324 35003 solver.cpp:239] Iteration 178220 (3.43793 iter/s, 2.90872s/10 iters), loss = 6.67477
I0523 07:54:43.284391 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67477 (* 1 = 6.67477 loss)
I0523 07:54:44.011579 35003 sgd_solver.cpp:112] Iteration 178220, lr = 0.001
I0523 07:54:47.435305 35003 solver.cpp:239] Iteration 178230 (2.40921 iter/s, 4.15075s/10 iters), loss = 6.16025
I0523 07:54:47.435350 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16025 (* 1 = 6.16025 loss)
I0523 07:54:47.458943 35003 sgd_solver.cpp:112] Iteration 178230, lr = 0.001
I0523 07:54:50.268837 35003 solver.cpp:239] Iteration 178240 (3.52937 iter/s, 2.83336s/10 iters), loss = 5.38835
I0523 07:54:50.268892 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.38835 (* 1 = 5.38835 loss)
I0523 07:54:51.009603 35003 sgd_solver.cpp:112] Iteration 178240, lr = 0.001
I0523 07:54:54.408166 35003 solver.cpp:239] Iteration 178250 (2.41598 iter/s, 4.13911s/10 iters), loss = 7.14981
I0523 07:54:54.408207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14981 (* 1 = 7.14981 loss)
I0523 07:54:54.427057 35003 sgd_solver.cpp:112] Iteration 178250, lr = 0.001
I0523 07:54:59.072695 35003 solver.cpp:239] Iteration 178260 (2.14395 iter/s, 4.66429s/10 iters), loss = 5.88892
I0523 07:54:59.072955 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.88892 (* 1 = 5.88892 loss)
I0523 07:54:59.079147 35003 sgd_solver.cpp:112] Iteration 178260, lr = 0.001
I0523 07:55:02.594738 35003 solver.cpp:239] Iteration 178270 (2.8396 iter/s, 3.52163s/10 iters), loss = 5.95545
I0523 07:55:02.594792 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95545 (* 1 = 5.95545 loss)
I0523 07:55:02.602509 35003 sgd_solver.cpp:112] Iteration 178270, lr = 0.001
I0523 07:55:05.307889 35003 solver.cpp:239] Iteration 178280 (3.68598 iter/s, 2.71298s/10 iters), loss = 7.55341
I0523 07:55:05.307927 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55341 (* 1 = 7.55341 loss)
I0523 07:55:05.321023 35003 sgd_solver.cpp:112] Iteration 178280, lr = 0.001
I0523 07:55:09.595429 35003 solver.cpp:239] Iteration 178290 (2.33246 iter/s, 4.28732s/10 iters), loss = 7.22917
I0523 07:55:09.595474 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22917 (* 1 = 7.22917 loss)
I0523 07:55:09.609287 35003 sgd_solver.cpp:112] Iteration 178290, lr = 0.001
I0523 07:55:13.260233 35003 solver.cpp:239] Iteration 178300 (2.72881 iter/s, 3.6646s/10 iters), loss = 6.53485
I0523 07:55:13.260273 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53485 (* 1 = 6.53485 loss)
I0523 07:55:13.263769 35003 sgd_solver.cpp:112] Iteration 178300, lr = 0.001
I0523 07:55:15.509340 35003 solver.cpp:239] Iteration 178310 (4.44649 iter/s, 2.24897s/10 iters), loss = 7.67855
I0523 07:55:15.509385 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.67855 (* 1 = 7.67855 loss)
I0523 07:55:16.155318 35003 sgd_solver.cpp:112] Iteration 178310, lr = 0.001
I0523 07:55:17.863432 35003 solver.cpp:239] Iteration 178320 (4.24819 iter/s, 2.35394s/10 iters), loss = 6.35513
I0523 07:55:17.863471 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35513 (* 1 = 6.35513 loss)
I0523 07:55:17.889050 35003 sgd_solver.cpp:112] Iteration 178320, lr = 0.001
I0523 07:55:19.832820 35003 solver.cpp:239] Iteration 178330 (5.07805 iter/s, 1.96926s/10 iters), loss = 6.66881
I0523 07:55:19.832860 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66881 (* 1 = 6.66881 loss)
I0523 07:55:19.839190 35003 sgd_solver.cpp:112] Iteration 178330, lr = 0.001
I0523 07:55:22.640074 35003 solver.cpp:239] Iteration 178340 (3.56241 iter/s, 2.80709s/10 iters), loss = 6.94613
I0523 07:55:22.640118 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94613 (* 1 = 6.94613 loss)
I0523 07:55:23.336772 35003 sgd_solver.cpp:112] Iteration 178340, lr = 0.001
I0523 07:55:27.753353 35003 solver.cpp:239] Iteration 178350 (1.95579 iter/s, 5.11302s/10 iters), loss = 7.26719
I0523 07:55:27.753391 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26719 (* 1 = 7.26719 loss)
I0523 07:55:27.766643 35003 sgd_solver.cpp:112] Iteration 178350, lr = 0.001
I0523 07:55:31.371706 35003 solver.cpp:239] Iteration 178360 (2.76387 iter/s, 3.61812s/10 iters), loss = 5.64398
I0523 07:55:31.372002 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.64398 (* 1 = 5.64398 loss)
I0523 07:55:32.085861 35003 sgd_solver.cpp:112] Iteration 178360, lr = 0.001
I0523 07:55:36.449337 35003 solver.cpp:239] Iteration 178370 (1.96961 iter/s, 5.07714s/10 iters), loss = 5.58966
I0523 07:55:36.449384 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.58966 (* 1 = 5.58966 loss)
I0523 07:55:37.157486 35003 sgd_solver.cpp:112] Iteration 178370, lr = 0.001
I0523 07:55:41.393808 35003 solver.cpp:239] Iteration 178380 (2.02256 iter/s, 4.94423s/10 iters), loss = 6.27006
I0523 07:55:41.393851 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27006 (* 1 = 6.27006 loss)
I0523 07:55:41.412546 35003 sgd_solver.cpp:112] Iteration 178380, lr = 0.001
I0523 07:55:44.212308 35003 solver.cpp:239] Iteration 178390 (3.54819 iter/s, 2.81834s/10 iters), loss = 6.31589
I0523 07:55:44.212347 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31589 (* 1 = 6.31589 loss)
I0523 07:55:44.771234 35003 sgd_solver.cpp:112] Iteration 178390, lr = 0.001
I0523 07:55:47.809566 35003 solver.cpp:239] Iteration 178400 (2.78005 iter/s, 3.59706s/10 iters), loss = 7.14116
I0523 07:55:47.809614 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14116 (* 1 = 7.14116 loss)
I0523 07:55:48.052111 35003 sgd_solver.cpp:112] Iteration 178400, lr = 0.001
I0523 07:55:49.347009 35003 solver.cpp:239] Iteration 178410 (6.50482 iter/s, 1.53732s/10 iters), loss = 6.2448
I0523 07:55:49.347052 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2448 (* 1 = 6.2448 loss)
I0523 07:55:49.360407 35003 sgd_solver.cpp:112] Iteration 178410, lr = 0.001
I0523 07:55:52.917886 35003 solver.cpp:239] Iteration 178420 (2.80058 iter/s, 3.57069s/10 iters), loss = 7.56606
I0523 07:55:52.917935 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56606 (* 1 = 7.56606 loss)
I0523 07:55:52.921702 35003 sgd_solver.cpp:112] Iteration 178420, lr = 0.001
I0523 07:55:57.115962 35003 solver.cpp:239] Iteration 178430 (2.38218 iter/s, 4.19784s/10 iters), loss = 6.65228
I0523 07:55:57.116006 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65228 (* 1 = 6.65228 loss)
I0523 07:55:57.138919 35003 sgd_solver.cpp:112] Iteration 178430, lr = 0.001
I0523 07:56:01.836153 35003 solver.cpp:239] Iteration 178440 (2.11867 iter/s, 4.71995s/10 iters), loss = 6.72812
I0523 07:56:01.836378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72812 (* 1 = 6.72812 loss)
I0523 07:56:01.849359 35003 sgd_solver.cpp:112] Iteration 178440, lr = 0.001
I0523 07:56:06.813158 35003 solver.cpp:239] Iteration 178450 (2.0094 iter/s, 4.9766s/10 iters), loss = 7.45966
I0523 07:56:06.813207 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45966 (* 1 = 7.45966 loss)
I0523 07:56:07.533957 35003 sgd_solver.cpp:112] Iteration 178450, lr = 0.001
I0523 07:56:11.944494 35003 solver.cpp:239] Iteration 178460 (1.94891 iter/s, 5.13108s/10 iters), loss = 6.4417
I0523 07:56:11.944535 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4417 (* 1 = 6.4417 loss)
I0523 07:56:12.678329 35003 sgd_solver.cpp:112] Iteration 178460, lr = 0.001
I0523 07:56:17.029752 35003 solver.cpp:239] Iteration 178470 (1.96657 iter/s, 5.085s/10 iters), loss = 7.23419
I0523 07:56:17.029801 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23419 (* 1 = 7.23419 loss)
I0523 07:56:17.033143 35003 sgd_solver.cpp:112] Iteration 178470, lr = 0.001
I0523 07:56:21.187463 35003 solver.cpp:239] Iteration 178480 (2.4053 iter/s, 4.15748s/10 iters), loss = 7.15037
I0523 07:56:21.187511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15037 (* 1 = 7.15037 loss)
I0523 07:56:21.417614 35003 sgd_solver.cpp:112] Iteration 178480, lr = 0.001
I0523 07:56:25.718843 35003 solver.cpp:239] Iteration 178490 (2.20695 iter/s, 4.53114s/10 iters), loss = 6.70713
I0523 07:56:25.718895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70713 (* 1 = 6.70713 loss)
I0523 07:56:25.723628 35003 sgd_solver.cpp:112] Iteration 178490, lr = 0.001
I0523 07:56:30.389040 35003 solver.cpp:239] Iteration 178500 (2.14135 iter/s, 4.66995s/10 iters), loss = 7.59778
I0523 07:56:30.389093 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59778 (* 1 = 7.59778 loss)
I0523 07:56:30.395973 35003 sgd_solver.cpp:112] Iteration 178500, lr = 0.001
I0523 07:56:32.491885 35003 solver.cpp:239] Iteration 178510 (4.75581 iter/s, 2.10269s/10 iters), loss = 5.14401
I0523 07:56:32.492146 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.14401 (* 1 = 5.14401 loss)
I0523 07:56:32.505100 35003 sgd_solver.cpp:112] Iteration 178510, lr = 0.001
I0523 07:56:35.351934 35003 solver.cpp:239] Iteration 178520 (3.49689 iter/s, 2.85969s/10 iters), loss = 6.62844
I0523 07:56:35.351971 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62844 (* 1 = 6.62844 loss)
I0523 07:56:35.356786 35003 sgd_solver.cpp:112] Iteration 178520, lr = 0.001
I0523 07:56:39.020134 35003 solver.cpp:239] Iteration 178530 (2.72628 iter/s, 3.66801s/10 iters), loss = 6.39349
I0523 07:56:39.020180 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39349 (* 1 = 6.39349 loss)
I0523 07:56:39.030395 35003 sgd_solver.cpp:112] Iteration 178530, lr = 0.001
I0523 07:56:41.844035 35003 solver.cpp:239] Iteration 178540 (3.54141 iter/s, 2.82373s/10 iters), loss = 6.92845
I0523 07:56:41.844084 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92845 (* 1 = 6.92845 loss)
I0523 07:56:41.857508 35003 sgd_solver.cpp:112] Iteration 178540, lr = 0.001
I0523 07:56:44.757877 35003 solver.cpp:239] Iteration 178550 (3.4321 iter/s, 2.91366s/10 iters), loss = 5.95141
I0523 07:56:44.757923 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95141 (* 1 = 5.95141 loss)
I0523 07:56:45.486491 35003 sgd_solver.cpp:112] Iteration 178550, lr = 0.001
I0523 07:56:49.386322 35003 solver.cpp:239] Iteration 178560 (2.16066 iter/s, 4.62821s/10 iters), loss = 5.87588
I0523 07:56:49.386361 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87588 (* 1 = 5.87588 loss)
I0523 07:56:49.405036 35003 sgd_solver.cpp:112] Iteration 178560, lr = 0.001
I0523 07:56:52.752084 35003 solver.cpp:239] Iteration 178570 (2.97127 iter/s, 3.36557s/10 iters), loss = 5.52806
I0523 07:56:52.752143 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.52806 (* 1 = 5.52806 loss)
I0523 07:56:52.756474 35003 sgd_solver.cpp:112] Iteration 178570, lr = 0.001
I0523 07:56:56.377853 35003 solver.cpp:239] Iteration 178580 (2.75819 iter/s, 3.62556s/10 iters), loss = 6.71397
I0523 07:56:56.377897 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71397 (* 1 = 6.71397 loss)
I0523 07:56:56.390895 35003 sgd_solver.cpp:112] Iteration 178580, lr = 0.001
I0523 07:56:58.295615 35003 solver.cpp:239] Iteration 178590 (5.21477 iter/s, 1.91763s/10 iters), loss = 6.10201
I0523 07:56:58.295658 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10201 (* 1 = 6.10201 loss)
I0523 07:56:59.036581 35003 sgd_solver.cpp:112] Iteration 178590, lr = 0.001
I0523 07:57:01.132191 35003 solver.cpp:239] Iteration 178600 (3.52558 iter/s, 2.83641s/10 iters), loss = 6.36287
I0523 07:57:01.132232 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36287 (* 1 = 6.36287 loss)
I0523 07:57:01.150349 35003 sgd_solver.cpp:112] Iteration 178600, lr = 0.001
I0523 07:57:02.453868 35003 solver.cpp:239] Iteration 178610 (7.56678 iter/s, 1.32157s/10 iters), loss = 6.39327
I0523 07:57:02.453915 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39327 (* 1 = 6.39327 loss)
I0523 07:57:02.467007 35003 sgd_solver.cpp:112] Iteration 178610, lr = 0.001
I0523 07:57:05.263278 35003 solver.cpp:239] Iteration 178620 (3.55968 iter/s, 2.80924s/10 iters), loss = 6.38014
I0523 07:57:05.263480 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38014 (* 1 = 6.38014 loss)
I0523 07:57:05.270150 35003 sgd_solver.cpp:112] Iteration 178620, lr = 0.001
I0523 07:57:08.877053 35003 solver.cpp:239] Iteration 178630 (2.76747 iter/s, 3.6134s/10 iters), loss = 6.76944
I0523 07:57:08.877097 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76944 (* 1 = 6.76944 loss)
I0523 07:57:08.884806 35003 sgd_solver.cpp:112] Iteration 178630, lr = 0.001
I0523 07:57:11.753002 35003 solver.cpp:239] Iteration 178640 (3.47732 iter/s, 2.87578s/10 iters), loss = 7.00854
I0523 07:57:11.753053 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00854 (* 1 = 7.00854 loss)
I0523 07:57:11.758550 35003 sgd_solver.cpp:112] Iteration 178640, lr = 0.001
I0523 07:57:13.785708 35003 solver.cpp:239] Iteration 178650 (4.91989 iter/s, 2.03257s/10 iters), loss = 6.72021
I0523 07:57:13.785754 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72021 (* 1 = 6.72021 loss)
I0523 07:57:13.876075 35003 sgd_solver.cpp:112] Iteration 178650, lr = 0.001
I0523 07:57:17.170150 35003 solver.cpp:239] Iteration 178660 (2.95486 iter/s, 3.38425s/10 iters), loss = 5.91007
I0523 07:57:17.170192 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91007 (* 1 = 5.91007 loss)
I0523 07:57:17.183240 35003 sgd_solver.cpp:112] Iteration 178660, lr = 0.001
I0523 07:57:19.905164 35003 solver.cpp:239] Iteration 178670 (3.6565 iter/s, 2.73485s/10 iters), loss = 6.29776
I0523 07:57:19.905212 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29776 (* 1 = 6.29776 loss)
I0523 07:57:19.917647 35003 sgd_solver.cpp:112] Iteration 178670, lr = 0.001
I0523 07:57:22.667989 35003 solver.cpp:239] Iteration 178680 (3.6197 iter/s, 2.76266s/10 iters), loss = 4.71943
I0523 07:57:22.668040 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.71943 (* 1 = 4.71943 loss)
I0523 07:57:23.382786 35003 sgd_solver.cpp:112] Iteration 178680, lr = 0.001
I0523 07:57:25.733350 35003 solver.cpp:239] Iteration 178690 (3.26245 iter/s, 3.06518s/10 iters), loss = 6.99444
I0523 07:57:25.733387 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99444 (* 1 = 6.99444 loss)
I0523 07:57:25.755755 35003 sgd_solver.cpp:112] Iteration 178690, lr = 0.001
I0523 07:57:27.888692 35003 solver.cpp:239] Iteration 178700 (4.63993 iter/s, 2.1552s/10 iters), loss = 6.23787
I0523 07:57:27.888747 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23787 (* 1 = 6.23787 loss)
I0523 07:57:28.629667 35003 sgd_solver.cpp:112] Iteration 178700, lr = 0.001
I0523 07:57:31.164412 35003 solver.cpp:239] Iteration 178710 (3.05294 iter/s, 3.27553s/10 iters), loss = 7.27104
I0523 07:57:31.164453 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27104 (* 1 = 7.27104 loss)
I0523 07:57:31.193189 35003 sgd_solver.cpp:112] Iteration 178710, lr = 0.001
I0523 07:57:35.216907 35003 solver.cpp:239] Iteration 178720 (2.46774 iter/s, 4.05229s/10 iters), loss = 6.32132
I0523 07:57:35.216949 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32132 (* 1 = 6.32132 loss)
I0523 07:57:35.229861 35003 sgd_solver.cpp:112] Iteration 178720, lr = 0.001
I0523 07:57:38.516284 35003 solver.cpp:239] Iteration 178730 (3.03104 iter/s, 3.29919s/10 iters), loss = 6.07974
I0523 07:57:38.516530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07974 (* 1 = 6.07974 loss)
I0523 07:57:38.529886 35003 sgd_solver.cpp:112] Iteration 178730, lr = 0.001
I0523 07:57:41.393738 35003 solver.cpp:239] Iteration 178740 (3.47571 iter/s, 2.87711s/10 iters), loss = 6.0771
I0523 07:57:41.393781 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0771 (* 1 = 6.0771 loss)
I0523 07:57:41.405164 35003 sgd_solver.cpp:112] Iteration 178740, lr = 0.001
I0523 07:57:44.910908 35003 solver.cpp:239] Iteration 178750 (2.84335 iter/s, 3.51698s/10 iters), loss = 7.28822
I0523 07:57:44.910964 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28822 (* 1 = 7.28822 loss)
I0523 07:57:45.050335 35003 sgd_solver.cpp:112] Iteration 178750, lr = 0.001
I0523 07:57:48.638979 35003 solver.cpp:239] Iteration 178760 (2.6825 iter/s, 3.72787s/10 iters), loss = 7.79457
I0523 07:57:48.639017 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.79457 (* 1 = 7.79457 loss)
I0523 07:57:48.657214 35003 sgd_solver.cpp:112] Iteration 178760, lr = 0.001
I0523 07:57:52.318747 35003 solver.cpp:239] Iteration 178770 (2.7177 iter/s, 3.67958s/10 iters), loss = 7.23476
I0523 07:57:52.318787 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23476 (* 1 = 7.23476 loss)
I0523 07:57:52.337306 35003 sgd_solver.cpp:112] Iteration 178770, lr = 0.001
I0523 07:57:56.302049 35003 solver.cpp:239] Iteration 178780 (2.51061 iter/s, 3.9831s/10 iters), loss = 5.92911
I0523 07:57:56.302085 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92911 (* 1 = 5.92911 loss)
I0523 07:57:56.337662 35003 sgd_solver.cpp:112] Iteration 178780, lr = 0.001
I0523 07:58:00.990717 35003 solver.cpp:239] Iteration 178790 (2.13292 iter/s, 4.68842s/10 iters), loss = 7.27347
I0523 07:58:00.990768 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27347 (* 1 = 7.27347 loss)
I0523 07:58:00.991984 35003 sgd_solver.cpp:112] Iteration 178790, lr = 0.001
I0523 07:58:04.853016 35003 solver.cpp:239] Iteration 178800 (2.58928 iter/s, 3.86208s/10 iters), loss = 6.11319
I0523 07:58:04.853050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11319 (* 1 = 6.11319 loss)
I0523 07:58:04.866307 35003 sgd_solver.cpp:112] Iteration 178800, lr = 0.001
I0523 07:58:09.282958 35003 solver.cpp:239] Iteration 178810 (2.25749 iter/s, 4.42971s/10 iters), loss = 6.89133
I0523 07:58:09.283176 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89133 (* 1 = 6.89133 loss)
I0523 07:58:09.985395 35003 sgd_solver.cpp:112] Iteration 178810, lr = 0.001
I0523 07:58:13.299648 35003 solver.cpp:239] Iteration 178820 (2.48985 iter/s, 4.01631s/10 iters), loss = 5.40557
I0523 07:58:13.299695 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.40557 (* 1 = 5.40557 loss)
I0523 07:58:13.313236 35003 sgd_solver.cpp:112] Iteration 178820, lr = 0.001
I0523 07:58:16.942648 35003 solver.cpp:239] Iteration 178830 (2.74515 iter/s, 3.64279s/10 iters), loss = 5.85374
I0523 07:58:16.942718 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85374 (* 1 = 5.85374 loss)
I0523 07:58:16.950006 35003 sgd_solver.cpp:112] Iteration 178830, lr = 0.001
I0523 07:58:19.802734 35003 solver.cpp:239] Iteration 178840 (3.49661 iter/s, 2.85991s/10 iters), loss = 6.43753
I0523 07:58:19.802775 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43753 (* 1 = 6.43753 loss)
I0523 07:58:20.518110 35003 sgd_solver.cpp:112] Iteration 178840, lr = 0.001
I0523 07:58:22.673635 35003 solver.cpp:239] Iteration 178850 (3.48346 iter/s, 2.87071s/10 iters), loss = 5.94471
I0523 07:58:22.673696 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.94471 (* 1 = 5.94471 loss)
I0523 07:58:22.715224 35003 sgd_solver.cpp:112] Iteration 178850, lr = 0.001
I0523 07:58:24.911396 35003 solver.cpp:239] Iteration 178860 (4.47008 iter/s, 2.23709s/10 iters), loss = 5.39842
I0523 07:58:24.911448 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.39842 (* 1 = 5.39842 loss)
I0523 07:58:24.920588 35003 sgd_solver.cpp:112] Iteration 178860, lr = 0.001
I0523 07:58:25.733052 35003 solver.cpp:239] Iteration 178870 (12.1723 iter/s, 0.821537s/10 iters), loss = 5.89283
I0523 07:58:25.733121 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89283 (* 1 = 5.89283 loss)
I0523 07:58:26.374429 35003 sgd_solver.cpp:112] Iteration 178870, lr = 0.001
I0523 07:58:29.177531 35003 solver.cpp:239] Iteration 178880 (2.90338 iter/s, 3.44426s/10 iters), loss = 7.10653
I0523 07:58:29.177585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.10653 (* 1 = 7.10653 loss)
I0523 07:58:29.190685 35003 sgd_solver.cpp:112] Iteration 178880, lr = 0.001
I0523 07:58:31.240319 35003 solver.cpp:239] Iteration 178890 (4.84816 iter/s, 2.06264s/10 iters), loss = 6.87879
I0523 07:58:31.240368 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87879 (* 1 = 6.87879 loss)
I0523 07:58:31.981395 35003 sgd_solver.cpp:112] Iteration 178890, lr = 0.001
I0523 07:58:34.797077 35003 solver.cpp:239] Iteration 178900 (2.81171 iter/s, 3.55656s/10 iters), loss = 5.4862
I0523 07:58:34.797122 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.4862 (* 1 = 5.4862 loss)
I0523 07:58:34.810153 35003 sgd_solver.cpp:112] Iteration 178900, lr = 0.001
I0523 07:58:39.101750 35003 solver.cpp:239] Iteration 178910 (2.32318 iter/s, 4.30445s/10 iters), loss = 6.17619
I0523 07:58:39.101789 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17619 (* 1 = 6.17619 loss)
I0523 07:58:39.828359 35003 sgd_solver.cpp:112] Iteration 178910, lr = 0.001
I0523 07:58:44.263115 35003 solver.cpp:239] Iteration 178920 (1.93757 iter/s, 5.16112s/10 iters), loss = 6.97702
I0523 07:58:44.263159 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97702 (* 1 = 6.97702 loss)
I0523 07:58:44.266906 35003 sgd_solver.cpp:112] Iteration 178920, lr = 0.001
I0523 07:58:47.088229 35003 solver.cpp:239] Iteration 178930 (3.53988 iter/s, 2.82495s/10 iters), loss = 6.70277
I0523 07:58:47.088269 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70277 (* 1 = 6.70277 loss)
I0523 07:58:47.099700 35003 sgd_solver.cpp:112] Iteration 178930, lr = 0.001
I0523 07:58:51.617259 35003 solver.cpp:239] Iteration 178940 (2.20809 iter/s, 4.5288s/10 iters), loss = 8.3078
I0523 07:58:51.617301 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.3078 (* 1 = 8.3078 loss)
I0523 07:58:52.330435 35003 sgd_solver.cpp:112] Iteration 178940, lr = 0.001
I0523 07:58:54.418756 35003 solver.cpp:239] Iteration 178950 (3.56973 iter/s, 2.80133s/10 iters), loss = 5.12067
I0523 07:58:54.418810 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.12067 (* 1 = 5.12067 loss)
I0523 07:58:55.119050 35003 sgd_solver.cpp:112] Iteration 178950, lr = 0.001
I0523 07:59:00.202852 35003 solver.cpp:239] Iteration 178960 (1.72897 iter/s, 5.7838s/10 iters), loss = 7.18667
I0523 07:59:00.202915 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18667 (* 1 = 7.18667 loss)
I0523 07:59:00.216959 35003 sgd_solver.cpp:112] Iteration 178960, lr = 0.001
I0523 07:59:03.148205 35003 solver.cpp:239] Iteration 178970 (3.39539 iter/s, 2.94517s/10 iters), loss = 7.35366
I0523 07:59:03.148241 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35366 (* 1 = 7.35366 loss)
I0523 07:59:03.166658 35003 sgd_solver.cpp:112] Iteration 178970, lr = 0.001
I0523 07:59:05.631680 35003 solver.cpp:239] Iteration 178980 (4.02685 iter/s, 2.48333s/10 iters), loss = 7.2356
I0523 07:59:05.631721 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2356 (* 1 = 7.2356 loss)
I0523 07:59:05.659821 35003 sgd_solver.cpp:112] Iteration 178980, lr = 0.001
I0523 07:59:09.111753 35003 solver.cpp:239] Iteration 178990 (2.87366 iter/s, 3.47989s/10 iters), loss = 5.94957
I0523 07:59:09.111799 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.94957 (* 1 = 5.94957 loss)
I0523 07:59:09.846156 35003 sgd_solver.cpp:112] Iteration 178990, lr = 0.001
I0523 07:59:13.444355 35003 solver.cpp:239] Iteration 179000 (2.3082 iter/s, 4.33238s/10 iters), loss = 7.42338
I0523 07:59:13.444406 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42338 (* 1 = 7.42338 loss)
I0523 07:59:13.456212 35003 sgd_solver.cpp:112] Iteration 179000, lr = 0.001
I0523 07:59:16.784474 35003 solver.cpp:239] Iteration 179010 (2.99408 iter/s, 3.33993s/10 iters), loss = 5.69233
I0523 07:59:16.784523 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.69233 (* 1 = 5.69233 loss)
I0523 07:59:17.420079 35003 sgd_solver.cpp:112] Iteration 179010, lr = 0.001
I0523 07:59:20.736603 35003 solver.cpp:239] Iteration 179020 (2.53042 iter/s, 3.95192s/10 iters), loss = 5.67383
I0523 07:59:20.736644 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.67383 (* 1 = 5.67383 loss)
I0523 07:59:20.750020 35003 sgd_solver.cpp:112] Iteration 179020, lr = 0.001
I0523 07:59:25.809011 35003 solver.cpp:239] Iteration 179030 (1.97155 iter/s, 5.07216s/10 iters), loss = 7.17273
I0523 07:59:25.809049 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17273 (* 1 = 7.17273 loss)
I0523 07:59:25.832923 35003 sgd_solver.cpp:112] Iteration 179030, lr = 0.001
I0523 07:59:28.721837 35003 solver.cpp:239] Iteration 179040 (3.43328 iter/s, 2.91266s/10 iters), loss = 6.78859
I0523 07:59:28.721880 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78859 (* 1 = 6.78859 loss)
I0523 07:59:29.462853 35003 sgd_solver.cpp:112] Iteration 179040, lr = 0.001
I0523 07:59:32.520490 35003 solver.cpp:239] Iteration 179050 (2.63265 iter/s, 3.79845s/10 iters), loss = 6.60965
I0523 07:59:32.520529 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60965 (* 1 = 6.60965 loss)
I0523 07:59:32.534097 35003 sgd_solver.cpp:112] Iteration 179050, lr = 0.001
I0523 07:59:37.167809 35003 solver.cpp:239] Iteration 179060 (2.15189 iter/s, 4.64709s/10 iters), loss = 7.90373
I0523 07:59:37.167863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90373 (* 1 = 7.90373 loss)
I0523 07:59:37.192667 35003 sgd_solver.cpp:112] Iteration 179060, lr = 0.001
I0523 07:59:40.081934 35003 solver.cpp:239] Iteration 179070 (3.43177 iter/s, 2.91395s/10 iters), loss = 6.03189
I0523 07:59:40.082186 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03189 (* 1 = 6.03189 loss)
I0523 07:59:40.086926 35003 sgd_solver.cpp:112] Iteration 179070, lr = 0.001
I0523 07:59:43.559067 35003 solver.cpp:239] Iteration 179080 (2.87624 iter/s, 3.47676s/10 iters), loss = 7.20916
I0523 07:59:43.559131 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20916 (* 1 = 7.20916 loss)
I0523 07:59:44.261740 35003 sgd_solver.cpp:112] Iteration 179080, lr = 0.001
I0523 07:59:45.592999 35003 solver.cpp:239] Iteration 179090 (4.91695 iter/s, 2.03378s/10 iters), loss = 6.75423
I0523 07:59:45.593060 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75423 (* 1 = 6.75423 loss)
I0523 07:59:45.969559 35003 sgd_solver.cpp:112] Iteration 179090, lr = 0.001
I0523 07:59:48.743562 35003 solver.cpp:239] Iteration 179100 (3.17423 iter/s, 3.15037s/10 iters), loss = 5.93789
I0523 07:59:48.743610 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93789 (* 1 = 5.93789 loss)
I0523 07:59:48.756891 35003 sgd_solver.cpp:112] Iteration 179100, lr = 0.001
I0523 07:59:53.088531 35003 solver.cpp:239] Iteration 179110 (2.30163 iter/s, 4.34475s/10 iters), loss = 6.13645
I0523 07:59:53.088579 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13645 (* 1 = 6.13645 loss)
I0523 07:59:53.175762 35003 sgd_solver.cpp:112] Iteration 179110, lr = 0.001
I0523 07:59:58.486626 35003 solver.cpp:239] Iteration 179120 (1.8526 iter/s, 5.39782s/10 iters), loss = 5.68179
I0523 07:59:58.486687 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.68179 (* 1 = 5.68179 loss)
I0523 07:59:58.494331 35003 sgd_solver.cpp:112] Iteration 179120, lr = 0.001
I0523 08:00:00.649199 35003 solver.cpp:239] Iteration 179130 (4.62444 iter/s, 2.16242s/10 iters), loss = 6.43235
I0523 08:00:00.649240 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43235 (* 1 = 6.43235 loss)
I0523 08:00:00.662223 35003 sgd_solver.cpp:112] Iteration 179130, lr = 0.001
I0523 08:00:03.595391 35003 solver.cpp:239] Iteration 179140 (3.3944 iter/s, 2.94603s/10 iters), loss = 6.97108
I0523 08:00:03.595429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97108 (* 1 = 6.97108 loss)
I0523 08:00:03.608768 35003 sgd_solver.cpp:112] Iteration 179140, lr = 0.001
I0523 08:00:06.902490 35003 solver.cpp:239] Iteration 179150 (3.02396 iter/s, 3.30692s/10 iters), loss = 6.03695
I0523 08:00:06.902523 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03695 (* 1 = 6.03695 loss)
I0523 08:00:07.617962 35003 sgd_solver.cpp:112] Iteration 179150, lr = 0.001
I0523 08:00:10.577744 35003 solver.cpp:239] Iteration 179160 (2.72104 iter/s, 3.67506s/10 iters), loss = 5.43106
I0523 08:00:10.578019 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.43106 (* 1 = 5.43106 loss)
I0523 08:00:10.983485 35003 sgd_solver.cpp:112] Iteration 179160, lr = 0.001
I0523 08:00:13.790482 35003 solver.cpp:239] Iteration 179170 (3.11299 iter/s, 3.21235s/10 iters), loss = 6.87608
I0523 08:00:13.790534 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87608 (* 1 = 6.87608 loss)
I0523 08:00:13.810582 35003 sgd_solver.cpp:112] Iteration 179170, lr = 0.001
I0523 08:00:17.348649 35003 solver.cpp:239] Iteration 179180 (2.81059 iter/s, 3.55797s/10 iters), loss = 7.43953
I0523 08:00:17.348690 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43953 (* 1 = 7.43953 loss)
I0523 08:00:17.352650 35003 sgd_solver.cpp:112] Iteration 179180, lr = 0.001
I0523 08:00:20.291332 35003 solver.cpp:239] Iteration 179190 (3.39845 iter/s, 2.94252s/10 iters), loss = 7.60849
I0523 08:00:20.291379 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60849 (* 1 = 7.60849 loss)
I0523 08:00:20.967463 35003 sgd_solver.cpp:112] Iteration 179190, lr = 0.001
I0523 08:00:24.550575 35003 solver.cpp:239] Iteration 179200 (2.34796 iter/s, 4.25902s/10 iters), loss = 6.01926
I0523 08:00:24.550621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01926 (* 1 = 6.01926 loss)
I0523 08:00:24.562644 35003 sgd_solver.cpp:112] Iteration 179200, lr = 0.001
I0523 08:00:28.067770 35003 solver.cpp:239] Iteration 179210 (2.84333 iter/s, 3.517s/10 iters), loss = 7.0228
I0523 08:00:28.067807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0228 (* 1 = 7.0228 loss)
I0523 08:00:28.081123 35003 sgd_solver.cpp:112] Iteration 179210, lr = 0.001
I0523 08:00:31.543998 35003 solver.cpp:239] Iteration 179220 (2.87684 iter/s, 3.47604s/10 iters), loss = 6.92564
I0523 08:00:31.544041 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92564 (* 1 = 6.92564 loss)
I0523 08:00:32.285269 35003 sgd_solver.cpp:112] Iteration 179220, lr = 0.001
I0523 08:00:35.889765 35003 solver.cpp:239] Iteration 179230 (2.30121 iter/s, 4.34554s/10 iters), loss = 6.20852
I0523 08:00:35.889812 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20852 (* 1 = 6.20852 loss)
I0523 08:00:35.903230 35003 sgd_solver.cpp:112] Iteration 179230, lr = 0.001
I0523 08:00:40.666366 35003 solver.cpp:239] Iteration 179240 (2.09364 iter/s, 4.77636s/10 iters), loss = 5.72934
I0523 08:00:40.666502 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.72934 (* 1 = 5.72934 loss)
I0523 08:00:41.361544 35003 sgd_solver.cpp:112] Iteration 179240, lr = 0.001
I0523 08:00:44.998216 35003 solver.cpp:239] Iteration 179250 (2.30865 iter/s, 4.33154s/10 iters), loss = 6.71845
I0523 08:00:44.998260 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71845 (* 1 = 6.71845 loss)
I0523 08:00:45.733153 35003 sgd_solver.cpp:112] Iteration 179250, lr = 0.001
I0523 08:00:49.833619 35003 solver.cpp:239] Iteration 179260 (2.06818 iter/s, 4.83516s/10 iters), loss = 7.09016
I0523 08:00:49.833669 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09016 (* 1 = 7.09016 loss)
I0523 08:00:50.548588 35003 sgd_solver.cpp:112] Iteration 179260, lr = 0.001
I0523 08:00:54.015904 35003 solver.cpp:239] Iteration 179270 (2.39117 iter/s, 4.18205s/10 iters), loss = 6.53087
I0523 08:00:54.015949 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53087 (* 1 = 6.53087 loss)
I0523 08:00:54.019901 35003 sgd_solver.cpp:112] Iteration 179270, lr = 0.001
I0523 08:00:57.217350 35003 solver.cpp:239] Iteration 179280 (3.12378 iter/s, 3.20125s/10 iters), loss = 7.90003
I0523 08:00:57.217391 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90003 (* 1 = 7.90003 loss)
I0523 08:00:57.230855 35003 sgd_solver.cpp:112] Iteration 179280, lr = 0.001
I0523 08:00:59.989780 35003 solver.cpp:239] Iteration 179290 (3.60717 iter/s, 2.77226s/10 iters), loss = 6.09372
I0523 08:00:59.989835 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09372 (* 1 = 6.09372 loss)
I0523 08:01:00.323886 35003 sgd_solver.cpp:112] Iteration 179290, lr = 0.001
I0523 08:01:03.177140 35003 solver.cpp:239] Iteration 179300 (3.13758 iter/s, 3.18717s/10 iters), loss = 5.03677
I0523 08:01:03.177181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.03677 (* 1 = 5.03677 loss)
I0523 08:01:03.220067 35003 sgd_solver.cpp:112] Iteration 179300, lr = 0.001
I0523 08:01:05.295428 35003 solver.cpp:239] Iteration 179310 (4.7211 iter/s, 2.11815s/10 iters), loss = 7.47777
I0523 08:01:05.295465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47777 (* 1 = 7.47777 loss)
I0523 08:01:05.308701 35003 sgd_solver.cpp:112] Iteration 179310, lr = 0.001
I0523 08:01:08.158517 35003 solver.cpp:239] Iteration 179320 (3.49292 iter/s, 2.86293s/10 iters), loss = 7.31859
I0523 08:01:08.158566 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31859 (* 1 = 7.31859 loss)
I0523 08:01:08.168321 35003 sgd_solver.cpp:112] Iteration 179320, lr = 0.001
I0523 08:01:11.151352 35003 solver.cpp:239] Iteration 179330 (3.34152 iter/s, 2.99265s/10 iters), loss = 6.06057
I0523 08:01:11.151590 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06057 (* 1 = 6.06057 loss)
I0523 08:01:11.865844 35003 sgd_solver.cpp:112] Iteration 179330, lr = 0.001
I0523 08:01:14.549429 35003 solver.cpp:239] Iteration 179340 (2.94317 iter/s, 3.3977s/10 iters), loss = 6.88885
I0523 08:01:14.549468 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88885 (* 1 = 6.88885 loss)
I0523 08:01:14.555281 35003 sgd_solver.cpp:112] Iteration 179340, lr = 0.001
I0523 08:01:20.935988 35003 solver.cpp:239] Iteration 179350 (1.56586 iter/s, 6.38626s/10 iters), loss = 6.97396
I0523 08:01:20.936031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97396 (* 1 = 6.97396 loss)
I0523 08:01:20.948995 35003 sgd_solver.cpp:112] Iteration 179350, lr = 0.001
I0523 08:01:24.451532 35003 solver.cpp:239] Iteration 179360 (2.84467 iter/s, 3.51535s/10 iters), loss = 6.75664
I0523 08:01:24.451583 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75664 (* 1 = 6.75664 loss)
I0523 08:01:24.465059 35003 sgd_solver.cpp:112] Iteration 179360, lr = 0.001
I0523 08:01:26.576232 35003 solver.cpp:239] Iteration 179370 (4.7069 iter/s, 2.12454s/10 iters), loss = 6.86363
I0523 08:01:26.576328 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86363 (* 1 = 6.86363 loss)
I0523 08:01:26.588867 35003 sgd_solver.cpp:112] Iteration 179370, lr = 0.001
I0523 08:01:29.447665 35003 solver.cpp:239] Iteration 179380 (3.48283 iter/s, 2.87123s/10 iters), loss = 7.78633
I0523 08:01:29.447703 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78633 (* 1 = 7.78633 loss)
I0523 08:01:29.461540 35003 sgd_solver.cpp:112] Iteration 179380, lr = 0.001
I0523 08:01:31.766466 35003 solver.cpp:239] Iteration 179390 (4.31284 iter/s, 2.31866s/10 iters), loss = 6.25713
I0523 08:01:31.766511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25713 (* 1 = 6.25713 loss)
I0523 08:01:32.481727 35003 sgd_solver.cpp:112] Iteration 179390, lr = 0.001
I0523 08:01:36.177106 35003 solver.cpp:239] Iteration 179400 (2.26736 iter/s, 4.41041s/10 iters), loss = 6.48224
I0523 08:01:36.177155 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48224 (* 1 = 6.48224 loss)
I0523 08:01:36.191488 35003 sgd_solver.cpp:112] Iteration 179400, lr = 0.001
I0523 08:01:38.970296 35003 solver.cpp:239] Iteration 179410 (3.58035 iter/s, 2.79302s/10 iters), loss = 5.85059
I0523 08:01:38.970347 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85059 (* 1 = 5.85059 loss)
I0523 08:01:39.178697 35003 sgd_solver.cpp:112] Iteration 179410, lr = 0.001
I0523 08:01:42.065129 35003 solver.cpp:239] Iteration 179420 (3.23138 iter/s, 3.09465s/10 iters), loss = 6.26472
I0523 08:01:42.065407 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26472 (* 1 = 6.26472 loss)
I0523 08:01:42.805958 35003 sgd_solver.cpp:112] Iteration 179420, lr = 0.001
I0523 08:01:46.190371 35003 solver.cpp:239] Iteration 179430 (2.42435 iter/s, 4.12483s/10 iters), loss = 6.8496
I0523 08:01:46.190408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8496 (* 1 = 6.8496 loss)
I0523 08:01:46.210058 35003 sgd_solver.cpp:112] Iteration 179430, lr = 0.001
I0523 08:01:48.923025 35003 solver.cpp:239] Iteration 179440 (3.65965 iter/s, 2.7325s/10 iters), loss = 6.83324
I0523 08:01:48.923063 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83324 (* 1 = 6.83324 loss)
I0523 08:01:48.931164 35003 sgd_solver.cpp:112] Iteration 179440, lr = 0.001
I0523 08:01:52.411658 35003 solver.cpp:239] Iteration 179450 (2.86661 iter/s, 3.48844s/10 iters), loss = 6.66475
I0523 08:01:52.411710 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66475 (* 1 = 6.66475 loss)
I0523 08:01:52.415165 35003 sgd_solver.cpp:112] Iteration 179450, lr = 0.001
I0523 08:01:56.767271 35003 solver.cpp:239] Iteration 179460 (2.29601 iter/s, 4.35537s/10 iters), loss = 6.82276
I0523 08:01:56.767316 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82276 (* 1 = 6.82276 loss)
I0523 08:01:56.835611 35003 sgd_solver.cpp:112] Iteration 179460, lr = 0.001
I0523 08:02:01.626307 35003 solver.cpp:239] Iteration 179470 (2.05813 iter/s, 4.85879s/10 iters), loss = 6.48733
I0523 08:02:01.626361 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48733 (* 1 = 6.48733 loss)
I0523 08:02:01.638917 35003 sgd_solver.cpp:112] Iteration 179470, lr = 0.001
I0523 08:02:05.179538 35003 solver.cpp:239] Iteration 179480 (2.8145 iter/s, 3.55303s/10 iters), loss = 6.41258
I0523 08:02:05.179577 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41258 (* 1 = 6.41258 loss)
I0523 08:02:05.192939 35003 sgd_solver.cpp:112] Iteration 179480, lr = 0.001
I0523 08:02:07.928026 35003 solver.cpp:239] Iteration 179490 (3.63858 iter/s, 2.74833s/10 iters), loss = 5.61701
I0523 08:02:07.928076 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.61701 (* 1 = 5.61701 loss)
I0523 08:02:08.619596 35003 sgd_solver.cpp:112] Iteration 179490, lr = 0.001
I0523 08:02:10.835140 35003 solver.cpp:239] Iteration 179500 (3.44004 iter/s, 2.90694s/10 iters), loss = 6.64155
I0523 08:02:10.835184 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64155 (* 1 = 6.64155 loss)
I0523 08:02:10.846518 35003 sgd_solver.cpp:112] Iteration 179500, lr = 0.001
I0523 08:02:15.829190 35003 solver.cpp:239] Iteration 179510 (2.00248 iter/s, 4.9938s/10 iters), loss = 7.26802
I0523 08:02:15.829423 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26802 (* 1 = 7.26802 loss)
I0523 08:02:15.847798 35003 sgd_solver.cpp:112] Iteration 179510, lr = 0.001
I0523 08:02:17.913283 35003 solver.cpp:239] Iteration 179520 (4.79895 iter/s, 2.08379s/10 iters), loss = 6.67805
I0523 08:02:17.913334 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67805 (* 1 = 6.67805 loss)
I0523 08:02:17.926043 35003 sgd_solver.cpp:112] Iteration 179520, lr = 0.001
I0523 08:02:20.875298 35003 solver.cpp:239] Iteration 179530 (3.37628 iter/s, 2.96184s/10 iters), loss = 7.057
I0523 08:02:20.875330 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.057 (* 1 = 7.057 loss)
I0523 08:02:20.888540 35003 sgd_solver.cpp:112] Iteration 179530, lr = 0.001
I0523 08:02:25.120170 35003 solver.cpp:239] Iteration 179540 (2.3559 iter/s, 4.24466s/10 iters), loss = 7.50089
I0523 08:02:25.120211 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50089 (* 1 = 7.50089 loss)
I0523 08:02:25.143719 35003 sgd_solver.cpp:112] Iteration 179540, lr = 0.001
I0523 08:02:28.312098 35003 solver.cpp:239] Iteration 179550 (3.13308 iter/s, 3.19175s/10 iters), loss = 6.68821
I0523 08:02:28.312151 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68821 (* 1 = 6.68821 loss)
I0523 08:02:28.316205 35003 sgd_solver.cpp:112] Iteration 179550, lr = 0.001
I0523 08:02:32.545284 35003 solver.cpp:239] Iteration 179560 (2.36242 iter/s, 4.23295s/10 iters), loss = 6.19005
I0523 08:02:32.545337 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19005 (* 1 = 6.19005 loss)
I0523 08:02:33.239707 35003 sgd_solver.cpp:112] Iteration 179560, lr = 0.001
I0523 08:02:35.254979 35003 solver.cpp:239] Iteration 179570 (3.69068 iter/s, 2.70953s/10 iters), loss = 7.7678
I0523 08:02:35.255025 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7678 (* 1 = 7.7678 loss)
I0523 08:02:35.261907 35003 sgd_solver.cpp:112] Iteration 179570, lr = 0.001
I0523 08:02:38.727545 35003 solver.cpp:239] Iteration 179580 (2.87987 iter/s, 3.47237s/10 iters), loss = 7.0909
I0523 08:02:38.727591 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0909 (* 1 = 7.0909 loss)
I0523 08:02:38.755682 35003 sgd_solver.cpp:112] Iteration 179580, lr = 0.001
I0523 08:02:42.833384 35003 solver.cpp:239] Iteration 179590 (2.43714 iter/s, 4.10318s/10 iters), loss = 6.99919
I0523 08:02:42.833422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99919 (* 1 = 6.99919 loss)
I0523 08:02:42.841006 35003 sgd_solver.cpp:112] Iteration 179590, lr = 0.001
I0523 08:02:45.646944 35003 solver.cpp:239] Iteration 179600 (3.55442 iter/s, 2.8134s/10 iters), loss = 6.99905
I0523 08:02:45.646991 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99905 (* 1 = 6.99905 loss)
I0523 08:02:45.665170 35003 sgd_solver.cpp:112] Iteration 179600, lr = 0.001
I0523 08:02:49.245932 35003 solver.cpp:239] Iteration 179610 (2.77871 iter/s, 3.59879s/10 iters), loss = 5.41993
I0523 08:02:49.246161 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.41993 (* 1 = 5.41993 loss)
I0523 08:02:49.987408 35003 sgd_solver.cpp:112] Iteration 179610, lr = 0.001
I0523 08:02:52.697243 35003 solver.cpp:239] Iteration 179620 (2.89776 iter/s, 3.45094s/10 iters), loss = 7.23278
I0523 08:02:52.697288 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23278 (* 1 = 7.23278 loss)
I0523 08:02:52.705610 35003 sgd_solver.cpp:112] Iteration 179620, lr = 0.001
I0523 08:02:54.717033 35003 solver.cpp:239] Iteration 179630 (4.95136 iter/s, 2.01965s/10 iters), loss = 7.65249
I0523 08:02:54.717092 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.65249 (* 1 = 7.65249 loss)
I0523 08:02:54.754573 35003 sgd_solver.cpp:112] Iteration 179630, lr = 0.001
I0523 08:02:57.919662 35003 solver.cpp:239] Iteration 179640 (3.12484 iter/s, 3.20016s/10 iters), loss = 7.4233
I0523 08:02:57.919706 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.4233 (* 1 = 7.4233 loss)
I0523 08:02:58.397442 35003 sgd_solver.cpp:112] Iteration 179640, lr = 0.001
I0523 08:03:02.680363 35003 solver.cpp:239] Iteration 179650 (2.10064 iter/s, 4.76046s/10 iters), loss = 5.57862
I0523 08:03:02.680413 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.57862 (* 1 = 5.57862 loss)
I0523 08:03:02.841881 35003 sgd_solver.cpp:112] Iteration 179650, lr = 0.001
I0523 08:03:08.381268 35003 solver.cpp:239] Iteration 179660 (1.75419 iter/s, 5.70063s/10 iters), loss = 6.75594
I0523 08:03:08.381314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75594 (* 1 = 6.75594 loss)
I0523 08:03:09.096658 35003 sgd_solver.cpp:112] Iteration 179660, lr = 0.001
I0523 08:03:12.936254 35003 solver.cpp:239] Iteration 179670 (2.19552 iter/s, 4.55474s/10 iters), loss = 5.95533
I0523 08:03:12.936306 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95533 (* 1 = 5.95533 loss)
I0523 08:03:13.503396 35003 sgd_solver.cpp:112] Iteration 179670, lr = 0.001
I0523 08:03:17.805425 35003 solver.cpp:239] Iteration 179680 (2.05384 iter/s, 4.86892s/10 iters), loss = 7.22458
I0523 08:03:17.805481 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22458 (* 1 = 7.22458 loss)
I0523 08:03:17.818617 35003 sgd_solver.cpp:112] Iteration 179680, lr = 0.001
I0523 08:03:21.412312 35003 solver.cpp:239] Iteration 179690 (2.77263 iter/s, 3.60668s/10 iters), loss = 5.44882
I0523 08:03:21.412910 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.44882 (* 1 = 5.44882 loss)
I0523 08:03:21.425467 35003 sgd_solver.cpp:112] Iteration 179690, lr = 0.001
I0523 08:03:24.168972 35003 solver.cpp:239] Iteration 179700 (3.6341 iter/s, 2.75171s/10 iters), loss = 5.31072
I0523 08:03:24.169025 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.31072 (* 1 = 5.31072 loss)
I0523 08:03:24.172407 35003 sgd_solver.cpp:112] Iteration 179700, lr = 0.001
I0523 08:03:28.610985 35003 solver.cpp:239] Iteration 179710 (2.25135 iter/s, 4.44177s/10 iters), loss = 6.32856
I0523 08:03:28.611047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32856 (* 1 = 6.32856 loss)
I0523 08:03:28.635280 35003 sgd_solver.cpp:112] Iteration 179710, lr = 0.001
I0523 08:03:32.935307 35003 solver.cpp:239] Iteration 179720 (2.31263 iter/s, 4.32409s/10 iters), loss = 6.75139
I0523 08:03:32.935353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75139 (* 1 = 6.75139 loss)
I0523 08:03:33.676602 35003 sgd_solver.cpp:112] Iteration 179720, lr = 0.001
I0523 08:03:37.272656 35003 solver.cpp:239] Iteration 179730 (2.30568 iter/s, 4.33712s/10 iters), loss = 6.52797
I0523 08:03:37.272706 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52797 (* 1 = 6.52797 loss)
I0523 08:03:37.290604 35003 sgd_solver.cpp:112] Iteration 179730, lr = 0.001
I0523 08:03:40.058807 35003 solver.cpp:239] Iteration 179740 (3.58942 iter/s, 2.78597s/10 iters), loss = 7.1639
I0523 08:03:40.058863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1639 (* 1 = 7.1639 loss)
I0523 08:03:40.064692 35003 sgd_solver.cpp:112] Iteration 179740, lr = 0.001
I0523 08:03:41.964161 35003 solver.cpp:239] Iteration 179750 (5.24876 iter/s, 1.90521s/10 iters), loss = 5.94468
I0523 08:03:41.964200 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.94468 (* 1 = 5.94468 loss)
I0523 08:03:41.977190 35003 sgd_solver.cpp:112] Iteration 179750, lr = 0.001
I0523 08:03:45.509697 35003 solver.cpp:239] Iteration 179760 (2.8206 iter/s, 3.54535s/10 iters), loss = 7.21981
I0523 08:03:45.509740 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21981 (* 1 = 7.21981 loss)
I0523 08:03:45.522498 35003 sgd_solver.cpp:112] Iteration 179760, lr = 0.001
I0523 08:03:48.964116 35003 solver.cpp:239] Iteration 179770 (2.89501 iter/s, 3.45422s/10 iters), loss = 6.32428
I0523 08:03:48.964174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32428 (* 1 = 6.32428 loss)
I0523 08:03:48.967722 35003 sgd_solver.cpp:112] Iteration 179770, lr = 0.001
I0523 08:03:51.803561 35003 solver.cpp:239] Iteration 179780 (3.52203 iter/s, 2.83927s/10 iters), loss = 5.76572
I0523 08:03:51.803799 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.76572 (* 1 = 5.76572 loss)
I0523 08:03:51.816705 35003 sgd_solver.cpp:112] Iteration 179780, lr = 0.001
I0523 08:03:54.912253 35003 solver.cpp:239] Iteration 179790 (3.21714 iter/s, 3.10835s/10 iters), loss = 5.74423
I0523 08:03:54.912304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74423 (* 1 = 5.74423 loss)
I0523 08:03:55.646626 35003 sgd_solver.cpp:112] Iteration 179790, lr = 0.001
I0523 08:03:59.883893 35003 solver.cpp:239] Iteration 179800 (2.01151 iter/s, 4.97139s/10 iters), loss = 6.54285
I0523 08:03:59.883940 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54285 (* 1 = 6.54285 loss)
I0523 08:04:00.619014 35003 sgd_solver.cpp:112] Iteration 179800, lr = 0.001
I0523 08:04:02.725718 35003 solver.cpp:239] Iteration 179810 (3.51908 iter/s, 2.84165s/10 iters), loss = 7.42493
I0523 08:04:02.725764 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42493 (* 1 = 7.42493 loss)
I0523 08:04:03.460319 35003 sgd_solver.cpp:112] Iteration 179810, lr = 0.001
I0523 08:04:04.751031 35003 solver.cpp:239] Iteration 179820 (4.93783 iter/s, 2.02518s/10 iters), loss = 5.74758
I0523 08:04:04.751067 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74758 (* 1 = 5.74758 loss)
I0523 08:04:04.764044 35003 sgd_solver.cpp:112] Iteration 179820, lr = 0.001
I0523 08:04:09.749006 35003 solver.cpp:239] Iteration 179830 (2.00091 iter/s, 4.99773s/10 iters), loss = 6.64823
I0523 08:04:09.749047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64823 (* 1 = 6.64823 loss)
I0523 08:04:09.756805 35003 sgd_solver.cpp:112] Iteration 179830, lr = 0.001
I0523 08:04:12.811213 35003 solver.cpp:239] Iteration 179840 (3.2658 iter/s, 3.06203s/10 iters), loss = 7.00562
I0523 08:04:12.811259 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00562 (* 1 = 7.00562 loss)
I0523 08:04:12.816156 35003 sgd_solver.cpp:112] Iteration 179840, lr = 0.001
I0523 08:04:16.060024 35003 solver.cpp:239] Iteration 179850 (3.07822 iter/s, 3.24863s/10 iters), loss = 6.11232
I0523 08:04:16.060072 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11232 (* 1 = 6.11232 loss)
I0523 08:04:16.134788 35003 sgd_solver.cpp:112] Iteration 179850, lr = 0.001
I0523 08:04:20.387871 35003 solver.cpp:239] Iteration 179860 (2.31074 iter/s, 4.32761s/10 iters), loss = 6.27485
I0523 08:04:20.387917 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27485 (* 1 = 6.27485 loss)
I0523 08:04:20.391175 35003 sgd_solver.cpp:112] Iteration 179860, lr = 0.001
I0523 08:04:24.021833 35003 solver.cpp:239] Iteration 179870 (2.75197 iter/s, 3.63377s/10 iters), loss = 6.6244
I0523 08:04:24.022133 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6244 (* 1 = 6.6244 loss)
I0523 08:04:24.034891 35003 sgd_solver.cpp:112] Iteration 179870, lr = 0.001
I0523 08:04:25.366794 35003 solver.cpp:239] Iteration 179880 (7.43701 iter/s, 1.34463s/10 iters), loss = 6.389
I0523 08:04:25.366844 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.389 (* 1 = 6.389 loss)
I0523 08:04:26.049862 35003 sgd_solver.cpp:112] Iteration 179880, lr = 0.001
I0523 08:04:30.142539 35003 solver.cpp:239] Iteration 179890 (2.09402 iter/s, 4.7755s/10 iters), loss = 5.68132
I0523 08:04:30.142591 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.68132 (* 1 = 5.68132 loss)
I0523 08:04:30.876921 35003 sgd_solver.cpp:112] Iteration 179890, lr = 0.001
I0523 08:04:34.385603 35003 solver.cpp:239] Iteration 179900 (2.35691 iter/s, 4.24284s/10 iters), loss = 6.34145
I0523 08:04:34.385650 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34145 (* 1 = 6.34145 loss)
I0523 08:04:34.397423 35003 sgd_solver.cpp:112] Iteration 179900, lr = 0.001
I0523 08:04:39.268111 35003 solver.cpp:239] Iteration 179910 (2.04823 iter/s, 4.88227s/10 iters), loss = 6.79045
I0523 08:04:39.268149 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79045 (* 1 = 6.79045 loss)
I0523 08:04:39.278749 35003 sgd_solver.cpp:112] Iteration 179910, lr = 0.001
I0523 08:04:42.836616 35003 solver.cpp:239] Iteration 179920 (2.80245 iter/s, 3.56831s/10 iters), loss = 6.25929
I0523 08:04:42.836663 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25929 (* 1 = 6.25929 loss)
I0523 08:04:42.840839 35003 sgd_solver.cpp:112] Iteration 179920, lr = 0.001
I0523 08:04:46.446660 35003 solver.cpp:239] Iteration 179930 (2.7702 iter/s, 3.60985s/10 iters), loss = 6.56687
I0523 08:04:46.446733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56687 (* 1 = 6.56687 loss)
I0523 08:04:47.185869 35003 sgd_solver.cpp:112] Iteration 179930, lr = 0.001
I0523 08:04:48.721369 35003 solver.cpp:239] Iteration 179940 (4.39651 iter/s, 2.27453s/10 iters), loss = 6.92851
I0523 08:04:48.721420 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92851 (* 1 = 6.92851 loss)
I0523 08:04:48.727288 35003 sgd_solver.cpp:112] Iteration 179940, lr = 0.001
I0523 08:04:51.424998 35003 solver.cpp:239] Iteration 179950 (3.69899 iter/s, 2.70344s/10 iters), loss = 6.06105
I0523 08:04:51.425047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06105 (* 1 = 6.06105 loss)
I0523 08:04:51.443084 35003 sgd_solver.cpp:112] Iteration 179950, lr = 0.001
I0523 08:04:53.944859 35003 solver.cpp:239] Iteration 179960 (3.97575 iter/s, 2.51525s/10 iters), loss = 6.22665
I0523 08:04:53.944900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22665 (* 1 = 6.22665 loss)
I0523 08:04:53.949587 35003 sgd_solver.cpp:112] Iteration 179960, lr = 0.001
I0523 08:04:57.607872 35003 solver.cpp:239] Iteration 179970 (2.73014 iter/s, 3.66282s/10 iters), loss = 5.62734
I0523 08:04:57.608047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.62734 (* 1 = 5.62734 loss)
I0523 08:04:57.620854 35003 sgd_solver.cpp:112] Iteration 179970, lr = 0.001
I0523 08:05:00.380239 35003 solver.cpp:239] Iteration 179980 (3.60739 iter/s, 2.77208s/10 iters), loss = 6.36062
I0523 08:05:00.380308 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36062 (* 1 = 6.36062 loss)
I0523 08:05:00.388289 35003 sgd_solver.cpp:112] Iteration 179980, lr = 0.001
I0523 08:05:04.542729 35003 solver.cpp:239] Iteration 179990 (2.40256 iter/s, 4.16223s/10 iters), loss = 6.69319
I0523 08:05:04.542773 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69319 (* 1 = 6.69319 loss)
I0523 08:05:04.887915 35003 sgd_solver.cpp:112] Iteration 179990, lr = 0.001
I0523 08:05:07.596647 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_180000.caffemodel
I0523 08:05:08.106070 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_180000.solverstate
I0523 08:05:08.263700 35003 solver.cpp:239] Iteration 180000 (2.68762 iter/s, 3.72077s/10 iters), loss = 5.72798
I0523 08:05:08.263741 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.72798 (* 1 = 5.72798 loss)
I0523 08:05:08.273306 35003 sgd_solver.cpp:112] Iteration 180000, lr = 0.001
I0523 08:05:11.153566 35003 solver.cpp:239] Iteration 180010 (3.46057 iter/s, 2.8897s/10 iters), loss = 6.40739
I0523 08:05:11.153604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40739 (* 1 = 6.40739 loss)
I0523 08:05:11.167201 35003 sgd_solver.cpp:112] Iteration 180010, lr = 0.001
I0523 08:05:13.827569 35003 solver.cpp:239] Iteration 180020 (3.73992 iter/s, 2.67385s/10 iters), loss = 5.57984
I0523 08:05:13.827606 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.57984 (* 1 = 5.57984 loss)
I0523 08:05:13.857663 35003 sgd_solver.cpp:112] Iteration 180020, lr = 0.001
I0523 08:05:16.874500 35003 solver.cpp:239] Iteration 180030 (3.28217 iter/s, 3.04676s/10 iters), loss = 7.16258
I0523 08:05:16.874552 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16258 (* 1 = 7.16258 loss)
I0523 08:05:16.878743 35003 sgd_solver.cpp:112] Iteration 180030, lr = 0.001
I0523 08:05:19.331990 35003 solver.cpp:239] Iteration 180040 (4.06945 iter/s, 2.45733s/10 iters), loss = 6.36158
I0523 08:05:19.332038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36158 (* 1 = 6.36158 loss)
I0523 08:05:19.335806 35003 sgd_solver.cpp:112] Iteration 180040, lr = 0.001
I0523 08:05:22.950330 35003 solver.cpp:239] Iteration 180050 (2.76385 iter/s, 3.61814s/10 iters), loss = 7.35511
I0523 08:05:22.950381 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35511 (* 1 = 7.35511 loss)
I0523 08:05:22.993165 35003 sgd_solver.cpp:112] Iteration 180050, lr = 0.001
I0523 08:05:26.812551 35003 solver.cpp:239] Iteration 180060 (2.58933 iter/s, 3.862s/10 iters), loss = 6.96289
I0523 08:05:26.812628 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96289 (* 1 = 6.96289 loss)
I0523 08:05:26.836460 35003 sgd_solver.cpp:112] Iteration 180060, lr = 0.001
I0523 08:05:29.636250 35003 solver.cpp:239] Iteration 180070 (3.5417 iter/s, 2.82351s/10 iters), loss = 8.10224
I0523 08:05:29.636523 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.10224 (* 1 = 8.10224 loss)
I0523 08:05:29.642174 35003 sgd_solver.cpp:112] Iteration 180070, lr = 0.001
I0523 08:05:32.873311 35003 solver.cpp:239] Iteration 180080 (3.08959 iter/s, 3.23668s/10 iters), loss = 6.6239
I0523 08:05:32.873351 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6239 (* 1 = 6.6239 loss)
I0523 08:05:32.929107 35003 sgd_solver.cpp:112] Iteration 180080, lr = 0.001
I0523 08:05:36.441992 35003 solver.cpp:239] Iteration 180090 (2.80231 iter/s, 3.56849s/10 iters), loss = 6.93972
I0523 08:05:36.442050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93972 (* 1 = 6.93972 loss)
I0523 08:05:36.638171 35003 sgd_solver.cpp:112] Iteration 180090, lr = 0.001
I0523 08:05:40.709282 35003 solver.cpp:239] Iteration 180100 (2.34354 iter/s, 4.26706s/10 iters), loss = 6.81712
I0523 08:05:40.709321 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81712 (* 1 = 6.81712 loss)
I0523 08:05:40.717166 35003 sgd_solver.cpp:112] Iteration 180100, lr = 0.001
I0523 08:05:44.299015 35003 solver.cpp:239] Iteration 180110 (2.78588 iter/s, 3.58953s/10 iters), loss = 6.44373
I0523 08:05:44.299062 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44373 (* 1 = 6.44373 loss)
I0523 08:05:44.312265 35003 sgd_solver.cpp:112] Iteration 180110, lr = 0.001
I0523 08:05:47.114011 35003 solver.cpp:239] Iteration 180120 (3.55261 iter/s, 2.81483s/10 iters), loss = 8.52496
I0523 08:05:47.114053 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.52496 (* 1 = 8.52496 loss)
I0523 08:05:47.244035 35003 sgd_solver.cpp:112] Iteration 180120, lr = 0.001
I0523 08:05:49.270787 35003 solver.cpp:239] Iteration 180130 (4.63685 iter/s, 2.15664s/10 iters), loss = 6.84836
I0523 08:05:49.270843 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84836 (* 1 = 6.84836 loss)
I0523 08:05:49.974994 35003 sgd_solver.cpp:112] Iteration 180130, lr = 0.001
I0523 08:05:53.562331 35003 solver.cpp:239] Iteration 180140 (2.33031 iter/s, 4.29128s/10 iters), loss = 6.5733
I0523 08:05:53.562405 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5733 (* 1 = 6.5733 loss)
I0523 08:05:53.575742 35003 sgd_solver.cpp:112] Iteration 180140, lr = 0.001
I0523 08:05:57.973175 35003 solver.cpp:239] Iteration 180150 (2.26727 iter/s, 4.41059s/10 iters), loss = 6.79935
I0523 08:05:57.973225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79935 (* 1 = 6.79935 loss)
I0523 08:05:58.688717 35003 sgd_solver.cpp:112] Iteration 180150, lr = 0.001
I0523 08:06:01.173079 35003 solver.cpp:239] Iteration 180160 (3.12529 iter/s, 3.1997s/10 iters), loss = 6.64599
I0523 08:06:01.173346 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64599 (* 1 = 6.64599 loss)
I0523 08:06:01.194211 35003 sgd_solver.cpp:112] Iteration 180160, lr = 0.001
I0523 08:06:04.029601 35003 solver.cpp:239] Iteration 180170 (3.5012 iter/s, 2.85616s/10 iters), loss = 5.78726
I0523 08:06:04.029644 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.78726 (* 1 = 5.78726 loss)
I0523 08:06:04.659871 35003 sgd_solver.cpp:112] Iteration 180170, lr = 0.001
I0523 08:06:08.505512 35003 solver.cpp:239] Iteration 180180 (2.2343 iter/s, 4.47567s/10 iters), loss = 5.87205
I0523 08:06:08.505580 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87205 (* 1 = 5.87205 loss)
I0523 08:06:09.233891 35003 sgd_solver.cpp:112] Iteration 180180, lr = 0.001
I0523 08:06:12.878981 35003 solver.cpp:239] Iteration 180190 (2.28664 iter/s, 4.37322s/10 iters), loss = 6.29176
I0523 08:06:12.879029 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29176 (* 1 = 6.29176 loss)
I0523 08:06:13.595722 35003 sgd_solver.cpp:112] Iteration 180190, lr = 0.001
I0523 08:06:16.332938 35003 solver.cpp:239] Iteration 180200 (2.89539 iter/s, 3.45377s/10 iters), loss = 7.20281
I0523 08:06:16.332983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20281 (* 1 = 7.20281 loss)
I0523 08:06:16.348171 35003 sgd_solver.cpp:112] Iteration 180200, lr = 0.001
I0523 08:06:20.458094 35003 solver.cpp:239] Iteration 180210 (2.42428 iter/s, 4.12494s/10 iters), loss = 6.63305
I0523 08:06:20.458156 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63305 (* 1 = 6.63305 loss)
I0523 08:06:20.588800 35003 sgd_solver.cpp:112] Iteration 180210, lr = 0.001
I0523 08:06:23.931846 35003 solver.cpp:239] Iteration 180220 (2.8789 iter/s, 3.47355s/10 iters), loss = 6.22655
I0523 08:06:23.931896 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22655 (* 1 = 6.22655 loss)
I0523 08:06:24.238108 35003 sgd_solver.cpp:112] Iteration 180220, lr = 0.001
I0523 08:06:27.191087 35003 solver.cpp:239] Iteration 180230 (3.06837 iter/s, 3.25906s/10 iters), loss = 7.49928
I0523 08:06:27.191129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49928 (* 1 = 7.49928 loss)
I0523 08:06:27.202525 35003 sgd_solver.cpp:112] Iteration 180230, lr = 0.001
I0523 08:06:29.905620 35003 solver.cpp:239] Iteration 180240 (3.68411 iter/s, 2.71436s/10 iters), loss = 6.78084
I0523 08:06:29.905673 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78084 (* 1 = 6.78084 loss)
I0523 08:06:29.909858 35003 sgd_solver.cpp:112] Iteration 180240, lr = 0.001
I0523 08:06:31.974370 35003 solver.cpp:239] Iteration 180250 (4.8342 iter/s, 2.06859s/10 iters), loss = 4.46144
I0523 08:06:31.974635 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.46144 (* 1 = 4.46144 loss)
I0523 08:06:31.979887 35003 sgd_solver.cpp:112] Iteration 180250, lr = 0.001
I0523 08:06:36.302846 35003 solver.cpp:239] Iteration 180260 (2.31052 iter/s, 4.32804s/10 iters), loss = 5.72496
I0523 08:06:36.302896 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.72496 (* 1 = 5.72496 loss)
I0523 08:06:36.311379 35003 sgd_solver.cpp:112] Iteration 180260, lr = 0.001
I0523 08:06:39.808245 35003 solver.cpp:239] Iteration 180270 (2.8529 iter/s, 3.5052s/10 iters), loss = 5.70022
I0523 08:06:39.808298 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.70022 (* 1 = 5.70022 loss)
I0523 08:06:39.821707 35003 sgd_solver.cpp:112] Iteration 180270, lr = 0.001
I0523 08:06:42.629853 35003 solver.cpp:239] Iteration 180280 (3.5443 iter/s, 2.82144s/10 iters), loss = 6.94903
I0523 08:06:42.629905 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94903 (* 1 = 6.94903 loss)
I0523 08:06:43.367532 35003 sgd_solver.cpp:112] Iteration 180280, lr = 0.001
I0523 08:06:47.659031 35003 solver.cpp:239] Iteration 180290 (1.9885 iter/s, 5.02892s/10 iters), loss = 5.43818
I0523 08:06:47.659078 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.43818 (* 1 = 5.43818 loss)
I0523 08:06:47.782528 35003 sgd_solver.cpp:112] Iteration 180290, lr = 0.001
I0523 08:06:50.157822 35003 solver.cpp:239] Iteration 180300 (4.00219 iter/s, 2.49863s/10 iters), loss = 6.19043
I0523 08:06:50.157871 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19043 (* 1 = 6.19043 loss)
I0523 08:06:50.865041 35003 sgd_solver.cpp:112] Iteration 180300, lr = 0.001
I0523 08:06:54.436386 35003 solver.cpp:239] Iteration 180310 (2.33736 iter/s, 4.27833s/10 iters), loss = 6.36575
I0523 08:06:54.436439 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36575 (* 1 = 6.36575 loss)
I0523 08:06:55.097252 35003 sgd_solver.cpp:112] Iteration 180310, lr = 0.001
I0523 08:06:58.005460 35003 solver.cpp:239] Iteration 180320 (2.802 iter/s, 3.56888s/10 iters), loss = 7.38474
I0523 08:06:58.005509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38474 (* 1 = 7.38474 loss)
I0523 08:06:58.016588 35003 sgd_solver.cpp:112] Iteration 180320, lr = 0.001
I0523 08:07:01.323081 35003 solver.cpp:239] Iteration 180330 (3.01439 iter/s, 3.31742s/10 iters), loss = 5.91464
I0523 08:07:01.323143 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91464 (* 1 = 5.91464 loss)
I0523 08:07:01.353840 35003 sgd_solver.cpp:112] Iteration 180330, lr = 0.001
I0523 08:07:04.903921 35003 solver.cpp:239] Iteration 180340 (2.7928 iter/s, 3.58063s/10 iters), loss = 6.61516
I0523 08:07:04.904214 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61516 (* 1 = 6.61516 loss)
I0523 08:07:04.906602 35003 sgd_solver.cpp:112] Iteration 180340, lr = 0.001
I0523 08:07:07.242308 35003 solver.cpp:239] Iteration 180350 (4.27714 iter/s, 2.33801s/10 iters), loss = 6.80268
I0523 08:07:07.242358 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80268 (* 1 = 6.80268 loss)
I0523 08:07:07.254959 35003 sgd_solver.cpp:112] Iteration 180350, lr = 0.001
I0523 08:07:10.086002 35003 solver.cpp:239] Iteration 180360 (3.51677 iter/s, 2.84352s/10 iters), loss = 5.92388
I0523 08:07:10.086048 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92388 (* 1 = 5.92388 loss)
I0523 08:07:10.099674 35003 sgd_solver.cpp:112] Iteration 180360, lr = 0.001
I0523 08:07:12.940317 35003 solver.cpp:239] Iteration 180370 (3.50369 iter/s, 2.85413s/10 iters), loss = 7.39808
I0523 08:07:12.940384 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39808 (* 1 = 7.39808 loss)
I0523 08:07:12.947474 35003 sgd_solver.cpp:112] Iteration 180370, lr = 0.001
I0523 08:07:15.517561 35003 solver.cpp:239] Iteration 180380 (3.88039 iter/s, 2.57706s/10 iters), loss = 6.3024
I0523 08:07:15.517609 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3024 (* 1 = 6.3024 loss)
I0523 08:07:15.525810 35003 sgd_solver.cpp:112] Iteration 180380, lr = 0.001
I0523 08:07:18.324589 35003 solver.cpp:239] Iteration 180390 (3.5627 iter/s, 2.80686s/10 iters), loss = 6.83213
I0523 08:07:18.324645 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83213 (* 1 = 6.83213 loss)
I0523 08:07:18.331032 35003 sgd_solver.cpp:112] Iteration 180390, lr = 0.001
I0523 08:07:21.422765 35003 solver.cpp:239] Iteration 180400 (3.2279 iter/s, 3.09799s/10 iters), loss = 7.57562
I0523 08:07:21.422806 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57562 (* 1 = 7.57562 loss)
I0523 08:07:21.435750 35003 sgd_solver.cpp:112] Iteration 180400, lr = 0.001
I0523 08:07:23.548581 35003 solver.cpp:239] Iteration 180410 (4.70439 iter/s, 2.12568s/10 iters), loss = 5.65972
I0523 08:07:23.548648 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.65972 (* 1 = 5.65972 loss)
I0523 08:07:23.561895 35003 sgd_solver.cpp:112] Iteration 180410, lr = 0.001
I0523 08:07:26.340667 35003 solver.cpp:239] Iteration 180420 (3.5818 iter/s, 2.79189s/10 iters), loss = 6.31167
I0523 08:07:26.340708 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31167 (* 1 = 6.31167 loss)
I0523 08:07:26.353514 35003 sgd_solver.cpp:112] Iteration 180420, lr = 0.001
I0523 08:07:27.682035 35003 solver.cpp:239] Iteration 180430 (7.45567 iter/s, 1.34126s/10 iters), loss = 6.97827
I0523 08:07:27.682083 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97827 (* 1 = 6.97827 loss)
I0523 08:07:27.694993 35003 sgd_solver.cpp:112] Iteration 180430, lr = 0.001
I0523 08:07:31.994138 35003 solver.cpp:239] Iteration 180440 (2.31918 iter/s, 4.31188s/10 iters), loss = 6.13488
I0523 08:07:31.994184 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13488 (* 1 = 6.13488 loss)
I0523 08:07:32.620013 35003 sgd_solver.cpp:112] Iteration 180440, lr = 0.001
I0523 08:07:34.664688 35003 solver.cpp:239] Iteration 180450 (3.74477 iter/s, 2.67039s/10 iters), loss = 6.56869
I0523 08:07:34.664746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56869 (* 1 = 6.56869 loss)
I0523 08:07:34.678227 35003 sgd_solver.cpp:112] Iteration 180450, lr = 0.001
I0523 08:07:38.754957 35003 solver.cpp:239] Iteration 180460 (2.44496 iter/s, 4.09004s/10 iters), loss = 6.57656
I0523 08:07:38.755139 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57656 (* 1 = 6.57656 loss)
I0523 08:07:38.832988 35003 sgd_solver.cpp:112] Iteration 180460, lr = 0.001
I0523 08:07:42.367790 35003 solver.cpp:239] Iteration 180470 (2.76816 iter/s, 3.6125s/10 iters), loss = 6.82548
I0523 08:07:42.367838 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82548 (* 1 = 6.82548 loss)
I0523 08:07:42.405639 35003 sgd_solver.cpp:112] Iteration 180470, lr = 0.001
I0523 08:07:46.964740 35003 solver.cpp:239] Iteration 180480 (2.17547 iter/s, 4.59671s/10 iters), loss = 7.53846
I0523 08:07:46.964797 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53846 (* 1 = 7.53846 loss)
I0523 08:07:46.968006 35003 sgd_solver.cpp:112] Iteration 180480, lr = 0.001
I0523 08:07:50.633589 35003 solver.cpp:239] Iteration 180490 (2.72581 iter/s, 3.66864s/10 iters), loss = 6.32074
I0523 08:07:50.633632 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32074 (* 1 = 6.32074 loss)
I0523 08:07:50.643303 35003 sgd_solver.cpp:112] Iteration 180490, lr = 0.001
I0523 08:07:53.491261 35003 solver.cpp:239] Iteration 180500 (3.49956 iter/s, 2.8575s/10 iters), loss = 6.71668
I0523 08:07:53.491314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71668 (* 1 = 6.71668 loss)
I0523 08:07:53.497175 35003 sgd_solver.cpp:112] Iteration 180500, lr = 0.001
I0523 08:07:56.343695 35003 solver.cpp:239] Iteration 180510 (3.50602 iter/s, 2.85224s/10 iters), loss = 6.65689
I0523 08:07:56.343741 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65689 (* 1 = 6.65689 loss)
I0523 08:07:56.350703 35003 sgd_solver.cpp:112] Iteration 180510, lr = 0.001
I0523 08:08:01.085721 35003 solver.cpp:239] Iteration 180520 (2.10891 iter/s, 4.74178s/10 iters), loss = 6.176
I0523 08:08:01.085765 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.176 (* 1 = 6.176 loss)
I0523 08:08:01.094895 35003 sgd_solver.cpp:112] Iteration 180520, lr = 0.001
I0523 08:08:04.649441 35003 solver.cpp:239] Iteration 180530 (2.80621 iter/s, 3.56353s/10 iters), loss = 6.40093
I0523 08:08:04.649477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40093 (* 1 = 6.40093 loss)
I0523 08:08:04.662859 35003 sgd_solver.cpp:112] Iteration 180530, lr = 0.001
I0523 08:08:09.134232 35003 solver.cpp:239] Iteration 180540 (2.22987 iter/s, 4.48457s/10 iters), loss = 6.86598
I0523 08:08:09.134513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86598 (* 1 = 6.86598 loss)
I0523 08:08:09.849297 35003 sgd_solver.cpp:112] Iteration 180540, lr = 0.001
I0523 08:08:12.501957 35003 solver.cpp:239] Iteration 180550 (2.9697 iter/s, 3.36734s/10 iters), loss = 6.83368
I0523 08:08:12.501997 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83368 (* 1 = 6.83368 loss)
I0523 08:08:12.665498 35003 sgd_solver.cpp:112] Iteration 180550, lr = 0.001
I0523 08:08:14.767664 35003 solver.cpp:239] Iteration 180560 (4.41392 iter/s, 2.26556s/10 iters), loss = 5.78404
I0523 08:08:14.767719 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.78404 (* 1 = 5.78404 loss)
I0523 08:08:14.781275 35003 sgd_solver.cpp:112] Iteration 180560, lr = 0.001
I0523 08:08:18.928493 35003 solver.cpp:239] Iteration 180570 (2.4035 iter/s, 4.1606s/10 iters), loss = 7.71225
I0523 08:08:18.928534 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.71225 (* 1 = 7.71225 loss)
I0523 08:08:18.935467 35003 sgd_solver.cpp:112] Iteration 180570, lr = 0.001
I0523 08:08:23.193238 35003 solver.cpp:239] Iteration 180580 (2.34493 iter/s, 4.26452s/10 iters), loss = 6.65932
I0523 08:08:23.193284 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65932 (* 1 = 6.65932 loss)
I0523 08:08:23.268920 35003 sgd_solver.cpp:112] Iteration 180580, lr = 0.001
I0523 08:08:26.975143 35003 solver.cpp:239] Iteration 180590 (2.64432 iter/s, 3.78169s/10 iters), loss = 6.70042
I0523 08:08:26.975204 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70042 (* 1 = 6.70042 loss)
I0523 08:08:26.994848 35003 sgd_solver.cpp:112] Iteration 180590, lr = 0.001
I0523 08:08:29.603247 35003 solver.cpp:239] Iteration 180600 (3.80528 iter/s, 2.62793s/10 iters), loss = 5.88391
I0523 08:08:29.603297 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.88391 (* 1 = 5.88391 loss)
I0523 08:08:30.318588 35003 sgd_solver.cpp:112] Iteration 180600, lr = 0.001
I0523 08:08:33.412993 35003 solver.cpp:239] Iteration 180610 (2.62501 iter/s, 3.80951s/10 iters), loss = 6.35678
I0523 08:08:33.413033 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35678 (* 1 = 6.35678 loss)
I0523 08:08:34.144106 35003 sgd_solver.cpp:112] Iteration 180610, lr = 0.001
I0523 08:08:38.145946 35003 solver.cpp:239] Iteration 180620 (2.11295 iter/s, 4.73271s/10 iters), loss = 5.67976
I0523 08:08:38.146011 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.67976 (* 1 = 5.67976 loss)
I0523 08:08:38.158928 35003 sgd_solver.cpp:112] Iteration 180620, lr = 0.001
I0523 08:08:40.438051 35003 solver.cpp:239] Iteration 180630 (4.36311 iter/s, 2.29194s/10 iters), loss = 6.99123
I0523 08:08:40.438294 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99123 (* 1 = 6.99123 loss)
I0523 08:08:40.451274 35003 sgd_solver.cpp:112] Iteration 180630, lr = 0.001
I0523 08:08:43.151921 35003 solver.cpp:239] Iteration 180640 (3.68523 iter/s, 2.71353s/10 iters), loss = 5.81406
I0523 08:08:43.151980 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.81406 (* 1 = 5.81406 loss)
I0523 08:08:43.165400 35003 sgd_solver.cpp:112] Iteration 180640, lr = 0.001
I0523 08:08:46.659271 35003 solver.cpp:239] Iteration 180650 (2.85132 iter/s, 3.50715s/10 iters), loss = 6.72411
I0523 08:08:46.659309 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72411 (* 1 = 6.72411 loss)
I0523 08:08:46.662267 35003 sgd_solver.cpp:112] Iteration 180650, lr = 0.001
I0523 08:08:50.215243 35003 solver.cpp:239] Iteration 180660 (2.81233 iter/s, 3.55577s/10 iters), loss = 6.38321
I0523 08:08:50.215292 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38321 (* 1 = 6.38321 loss)
I0523 08:08:50.227525 35003 sgd_solver.cpp:112] Iteration 180660, lr = 0.001
I0523 08:08:53.847127 35003 solver.cpp:239] Iteration 180670 (2.75354 iter/s, 3.63169s/10 iters), loss = 6.07832
I0523 08:08:53.847167 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07832 (* 1 = 6.07832 loss)
I0523 08:08:53.853752 35003 sgd_solver.cpp:112] Iteration 180670, lr = 0.001
I0523 08:08:56.325170 35003 solver.cpp:239] Iteration 180680 (4.03568 iter/s, 2.47789s/10 iters), loss = 6.27587
I0523 08:08:56.325215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27587 (* 1 = 6.27587 loss)
I0523 08:08:56.347604 35003 sgd_solver.cpp:112] Iteration 180680, lr = 0.001
I0523 08:08:59.199424 35003 solver.cpp:239] Iteration 180690 (3.47937 iter/s, 2.87409s/10 iters), loss = 7.34167
I0523 08:08:59.199476 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34167 (* 1 = 7.34167 loss)
I0523 08:08:59.206112 35003 sgd_solver.cpp:112] Iteration 180690, lr = 0.001
I0523 08:09:02.014948 35003 solver.cpp:239] Iteration 180700 (3.55195 iter/s, 2.81535s/10 iters), loss = 5.35757
I0523 08:09:02.014994 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.35757 (* 1 = 5.35757 loss)
I0523 08:09:02.750109 35003 sgd_solver.cpp:112] Iteration 180700, lr = 0.001
I0523 08:09:07.892478 35003 solver.cpp:239] Iteration 180710 (1.70148 iter/s, 5.87725s/10 iters), loss = 7.63197
I0523 08:09:07.892530 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63197 (* 1 = 7.63197 loss)
I0523 08:09:07.901731 35003 sgd_solver.cpp:112] Iteration 180710, lr = 0.001
I0523 08:09:11.535508 35003 solver.cpp:239] Iteration 180720 (2.74512 iter/s, 3.64283s/10 iters), loss = 4.71621
I0523 08:09:11.535748 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.71621 (* 1 = 4.71621 loss)
I0523 08:09:11.549357 35003 sgd_solver.cpp:112] Iteration 180720, lr = 0.001
I0523 08:09:15.061062 35003 solver.cpp:239] Iteration 180730 (2.83672 iter/s, 3.5252s/10 iters), loss = 5.45544
I0523 08:09:15.061100 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.45544 (* 1 = 5.45544 loss)
I0523 08:09:15.069272 35003 sgd_solver.cpp:112] Iteration 180730, lr = 0.001
I0523 08:09:19.934026 35003 solver.cpp:239] Iteration 180740 (2.05224 iter/s, 4.87272s/10 iters), loss = 6.70407
I0523 08:09:19.934089 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70407 (* 1 = 6.70407 loss)
I0523 08:09:19.945822 35003 sgd_solver.cpp:112] Iteration 180740, lr = 0.001
I0523 08:09:22.759636 35003 solver.cpp:239] Iteration 180750 (3.53929 iter/s, 2.82543s/10 iters), loss = 5.75222
I0523 08:09:22.759675 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.75222 (* 1 = 5.75222 loss)
I0523 08:09:22.781985 35003 sgd_solver.cpp:112] Iteration 180750, lr = 0.001
I0523 08:09:25.129233 35003 solver.cpp:239] Iteration 180760 (4.22038 iter/s, 2.36945s/10 iters), loss = 6.59532
I0523 08:09:25.129281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59532 (* 1 = 6.59532 loss)
I0523 08:09:25.860342 35003 sgd_solver.cpp:112] Iteration 180760, lr = 0.001
I0523 08:09:29.168656 35003 solver.cpp:239] Iteration 180770 (2.47573 iter/s, 4.03921s/10 iters), loss = 6.11366
I0523 08:09:29.168699 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11366 (* 1 = 6.11366 loss)
I0523 08:09:29.907532 35003 sgd_solver.cpp:112] Iteration 180770, lr = 0.001
I0523 08:09:32.702148 35003 solver.cpp:239] Iteration 180780 (2.83021 iter/s, 3.5333s/10 iters), loss = 5.33034
I0523 08:09:32.702193 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.33034 (* 1 = 5.33034 loss)
I0523 08:09:33.430713 35003 sgd_solver.cpp:112] Iteration 180780, lr = 0.001
I0523 08:09:35.426682 35003 solver.cpp:239] Iteration 180790 (3.67058 iter/s, 2.72437s/10 iters), loss = 7.39011
I0523 08:09:35.426762 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39011 (* 1 = 7.39011 loss)
I0523 08:09:35.429745 35003 sgd_solver.cpp:112] Iteration 180790, lr = 0.001
I0523 08:09:38.956385 35003 solver.cpp:239] Iteration 180800 (2.83329 iter/s, 3.52946s/10 iters), loss = 6.20721
I0523 08:09:38.956441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20721 (* 1 = 6.20721 loss)
I0523 08:09:39.669443 35003 sgd_solver.cpp:112] Iteration 180800, lr = 0.001
I0523 08:09:43.074805 35003 solver.cpp:239] Iteration 180810 (2.42825 iter/s, 4.1182s/10 iters), loss = 6.58164
I0523 08:09:43.075049 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58164 (* 1 = 6.58164 loss)
I0523 08:09:43.088433 35003 sgd_solver.cpp:112] Iteration 180810, lr = 0.001
I0523 08:09:44.422755 35003 solver.cpp:239] Iteration 180820 (7.42025 iter/s, 1.34766s/10 iters), loss = 7.64945
I0523 08:09:44.422808 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64945 (* 1 = 7.64945 loss)
I0523 08:09:44.435994 35003 sgd_solver.cpp:112] Iteration 180820, lr = 0.001
I0523 08:09:47.947816 35003 solver.cpp:239] Iteration 180830 (2.83699 iter/s, 3.52486s/10 iters), loss = 6.8703
I0523 08:09:47.947867 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8703 (* 1 = 6.8703 loss)
I0523 08:09:47.955032 35003 sgd_solver.cpp:112] Iteration 180830, lr = 0.001
I0523 08:09:51.616971 35003 solver.cpp:239] Iteration 180840 (2.72558 iter/s, 3.66894s/10 iters), loss = 6.63688
I0523 08:09:51.617017 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63688 (* 1 = 6.63688 loss)
I0523 08:09:51.630563 35003 sgd_solver.cpp:112] Iteration 180840, lr = 0.001
I0523 08:09:55.205099 35003 solver.cpp:239] Iteration 180850 (2.78712 iter/s, 3.58793s/10 iters), loss = 6.84136
I0523 08:09:55.205142 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84136 (* 1 = 6.84136 loss)
I0523 08:09:55.946341 35003 sgd_solver.cpp:112] Iteration 180850, lr = 0.001
I0523 08:10:01.531857 35003 solver.cpp:239] Iteration 180860 (1.58066 iter/s, 6.32646s/10 iters), loss = 6.4389
I0523 08:10:01.531896 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4389 (* 1 = 6.4389 loss)
I0523 08:10:01.545945 35003 sgd_solver.cpp:112] Iteration 180860, lr = 0.001
I0523 08:10:04.964100 35003 solver.cpp:239] Iteration 180870 (2.91371 iter/s, 3.43206s/10 iters), loss = 7.24899
I0523 08:10:04.964139 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24899 (* 1 = 7.24899 loss)
I0523 08:10:04.977658 35003 sgd_solver.cpp:112] Iteration 180870, lr = 0.001
I0523 08:10:08.475450 35003 solver.cpp:239] Iteration 180880 (2.84806 iter/s, 3.51116s/10 iters), loss = 6.26775
I0523 08:10:08.475492 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26775 (* 1 = 6.26775 loss)
I0523 08:10:08.488690 35003 sgd_solver.cpp:112] Iteration 180880, lr = 0.001
I0523 08:10:12.347138 35003 solver.cpp:239] Iteration 180890 (2.58299 iter/s, 3.87148s/10 iters), loss = 6.48832
I0523 08:10:12.347187 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48832 (* 1 = 6.48832 loss)
I0523 08:10:12.350843 35003 sgd_solver.cpp:112] Iteration 180890, lr = 0.001
I0523 08:10:15.838186 35003 solver.cpp:239] Iteration 180900 (2.86464 iter/s, 3.49084s/10 iters), loss = 6.82974
I0523 08:10:15.838402 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82974 (* 1 = 6.82974 loss)
I0523 08:10:15.845049 35003 sgd_solver.cpp:112] Iteration 180900, lr = 0.001
I0523 08:10:20.722180 35003 solver.cpp:239] Iteration 180910 (2.04768 iter/s, 4.88359s/10 iters), loss = 6.09551
I0523 08:10:20.722226 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09551 (* 1 = 6.09551 loss)
I0523 08:10:20.735061 35003 sgd_solver.cpp:112] Iteration 180910, lr = 0.001
I0523 08:10:23.937146 35003 solver.cpp:239] Iteration 180920 (3.11063 iter/s, 3.21478s/10 iters), loss = 5.74808
I0523 08:10:23.937193 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74808 (* 1 = 5.74808 loss)
I0523 08:10:23.941609 35003 sgd_solver.cpp:112] Iteration 180920, lr = 0.001
I0523 08:10:26.974028 35003 solver.cpp:239] Iteration 180930 (3.29304 iter/s, 3.0367s/10 iters), loss = 6.27236
I0523 08:10:26.974071 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27236 (* 1 = 6.27236 loss)
I0523 08:10:27.568243 35003 sgd_solver.cpp:112] Iteration 180930, lr = 0.001
I0523 08:10:31.528594 35003 solver.cpp:239] Iteration 180940 (2.19571 iter/s, 4.55433s/10 iters), loss = 7.78641
I0523 08:10:31.528646 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78641 (* 1 = 7.78641 loss)
I0523 08:10:31.536604 35003 sgd_solver.cpp:112] Iteration 180940, lr = 0.001
I0523 08:10:35.179644 35003 solver.cpp:239] Iteration 180950 (2.73909 iter/s, 3.65085s/10 iters), loss = 5.57518
I0523 08:10:35.179685 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.57518 (* 1 = 5.57518 loss)
I0523 08:10:35.348835 35003 sgd_solver.cpp:112] Iteration 180950, lr = 0.001
I0523 08:10:39.031028 35003 solver.cpp:239] Iteration 180960 (2.5966 iter/s, 3.85118s/10 iters), loss = 6.56809
I0523 08:10:39.031078 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56809 (* 1 = 6.56809 loss)
I0523 08:10:39.043890 35003 sgd_solver.cpp:112] Iteration 180960, lr = 0.001
I0523 08:10:43.533643 35003 solver.cpp:239] Iteration 180970 (2.22105 iter/s, 4.50237s/10 iters), loss = 6.69848
I0523 08:10:43.533700 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69848 (* 1 = 6.69848 loss)
I0523 08:10:43.592152 35003 sgd_solver.cpp:112] Iteration 180970, lr = 0.001
I0523 08:10:46.407573 35003 solver.cpp:239] Iteration 180980 (3.47977 iter/s, 2.87376s/10 iters), loss = 6.65403
I0523 08:10:46.407819 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65403 (* 1 = 6.65403 loss)
I0523 08:10:47.123097 35003 sgd_solver.cpp:112] Iteration 180980, lr = 0.001
I0523 08:10:51.460049 35003 solver.cpp:239] Iteration 180990 (1.97939 iter/s, 5.05205s/10 iters), loss = 6.09791
I0523 08:10:51.460095 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09791 (* 1 = 6.09791 loss)
I0523 08:10:51.473485 35003 sgd_solver.cpp:112] Iteration 180990, lr = 0.001
I0523 08:10:54.860363 35003 solver.cpp:239] Iteration 181000 (2.94107 iter/s, 3.40012s/10 iters), loss = 7.52051
I0523 08:10:54.860409 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.52051 (* 1 = 7.52051 loss)
I0523 08:10:54.868068 35003 sgd_solver.cpp:112] Iteration 181000, lr = 0.001
I0523 08:10:58.298458 35003 solver.cpp:239] Iteration 181010 (2.90875 iter/s, 3.4379s/10 iters), loss = 5.95791
I0523 08:10:58.298501 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95791 (* 1 = 5.95791 loss)
I0523 08:10:58.305747 35003 sgd_solver.cpp:112] Iteration 181010, lr = 0.001
I0523 08:11:02.556294 35003 solver.cpp:239] Iteration 181020 (2.34873 iter/s, 4.25762s/10 iters), loss = 7.05291
I0523 08:11:02.556334 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05291 (* 1 = 7.05291 loss)
I0523 08:11:02.569375 35003 sgd_solver.cpp:112] Iteration 181020, lr = 0.001
I0523 08:11:04.637773 35003 solver.cpp:239] Iteration 181030 (4.80458 iter/s, 2.08135s/10 iters), loss = 7.01656
I0523 08:11:04.637811 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01656 (* 1 = 7.01656 loss)
I0523 08:11:04.650786 35003 sgd_solver.cpp:112] Iteration 181030, lr = 0.001
I0523 08:11:07.606940 35003 solver.cpp:239] Iteration 181040 (3.36815 iter/s, 2.96899s/10 iters), loss = 6.97884
I0523 08:11:07.606992 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97884 (* 1 = 6.97884 loss)
I0523 08:11:08.324744 35003 sgd_solver.cpp:112] Iteration 181040, lr = 0.001
I0523 08:11:12.623689 35003 solver.cpp:239] Iteration 181050 (1.99343 iter/s, 5.01649s/10 iters), loss = 6.8511
I0523 08:11:12.623739 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8511 (* 1 = 6.8511 loss)
I0523 08:11:13.344677 35003 sgd_solver.cpp:112] Iteration 181050, lr = 0.001
I0523 08:11:17.747992 35003 solver.cpp:239] Iteration 181060 (1.95159 iter/s, 5.12403s/10 iters), loss = 6.79957
I0523 08:11:17.748245 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79957 (* 1 = 6.79957 loss)
I0523 08:11:17.955229 35003 sgd_solver.cpp:112] Iteration 181060, lr = 0.001
I0523 08:11:21.574026 35003 solver.cpp:239] Iteration 181070 (2.61396 iter/s, 3.82562s/10 iters), loss = 7.80404
I0523 08:11:21.574077 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.80404 (* 1 = 7.80404 loss)
I0523 08:11:21.633802 35003 sgd_solver.cpp:112] Iteration 181070, lr = 0.001
I0523 08:11:24.375016 35003 solver.cpp:239] Iteration 181080 (3.57039 iter/s, 2.80082s/10 iters), loss = 6.19584
I0523 08:11:24.375068 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19584 (* 1 = 6.19584 loss)
I0523 08:11:24.386122 35003 sgd_solver.cpp:112] Iteration 181080, lr = 0.001
I0523 08:11:27.919169 35003 solver.cpp:239] Iteration 181090 (2.82171 iter/s, 3.54395s/10 iters), loss = 7.00101
I0523 08:11:27.919217 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00101 (* 1 = 7.00101 loss)
I0523 08:11:28.627913 35003 sgd_solver.cpp:112] Iteration 181090, lr = 0.001
I0523 08:11:30.730782 35003 solver.cpp:239] Iteration 181100 (3.55689 iter/s, 2.81144s/10 iters), loss = 7.15282
I0523 08:11:30.730834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15282 (* 1 = 7.15282 loss)
I0523 08:11:30.739593 35003 sgd_solver.cpp:112] Iteration 181100, lr = 0.001
I0523 08:11:33.514791 35003 solver.cpp:239] Iteration 181110 (3.5953 iter/s, 2.78141s/10 iters), loss = 6.4172
I0523 08:11:33.514830 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4172 (* 1 = 6.4172 loss)
I0523 08:11:33.528070 35003 sgd_solver.cpp:112] Iteration 181110, lr = 0.001
I0523 08:11:35.611124 35003 solver.cpp:239] Iteration 181120 (4.77053 iter/s, 2.0962s/10 iters), loss = 5.83253
I0523 08:11:35.611171 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.83253 (* 1 = 5.83253 loss)
I0523 08:11:35.624353 35003 sgd_solver.cpp:112] Iteration 181120, lr = 0.001
I0523 08:11:39.188788 35003 solver.cpp:239] Iteration 181130 (2.79529 iter/s, 3.57745s/10 iters), loss = 6.23025
I0523 08:11:39.188866 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23025 (* 1 = 6.23025 loss)
I0523 08:11:39.213930 35003 sgd_solver.cpp:112] Iteration 181130, lr = 0.001
I0523 08:11:43.312929 35003 solver.cpp:239] Iteration 181140 (2.42489 iter/s, 4.1239s/10 iters), loss = 6.6912
I0523 08:11:43.312968 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6912 (* 1 = 6.6912 loss)
I0523 08:11:43.317423 35003 sgd_solver.cpp:112] Iteration 181140, lr = 0.001
I0523 08:11:46.795904 35003 solver.cpp:239] Iteration 181150 (2.87127 iter/s, 3.48278s/10 iters), loss = 6.09712
I0523 08:11:46.795955 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09712 (* 1 = 6.09712 loss)
I0523 08:11:46.804612 35003 sgd_solver.cpp:112] Iteration 181150, lr = 0.001
I0523 08:11:47.648931 35003 solver.cpp:239] Iteration 181160 (11.7242 iter/s, 0.852934s/10 iters), loss = 6.54997
I0523 08:11:47.648970 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54997 (* 1 = 6.54997 loss)
I0523 08:11:47.661089 35003 sgd_solver.cpp:112] Iteration 181160, lr = 0.001
I0523 08:11:48.509486 35003 solver.cpp:239] Iteration 181170 (11.6215 iter/s, 0.860471s/10 iters), loss = 6.69675
I0523 08:11:48.509771 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69675 (* 1 = 6.69675 loss)
I0523 08:11:48.518795 35003 sgd_solver.cpp:112] Iteration 181170, lr = 0.001
I0523 08:11:49.623908 35003 solver.cpp:239] Iteration 181180 (8.97581 iter/s, 1.11411s/10 iters), loss = 7.55487
I0523 08:11:49.623944 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55487 (* 1 = 7.55487 loss)
I0523 08:11:49.633035 35003 sgd_solver.cpp:112] Iteration 181180, lr = 0.001
I0523 08:11:50.833359 35003 solver.cpp:239] Iteration 181190 (8.26887 iter/s, 1.20936s/10 iters), loss = 6.20866
I0523 08:11:50.833396 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20866 (* 1 = 6.20866 loss)
I0523 08:11:50.842262 35003 sgd_solver.cpp:112] Iteration 181190, lr = 0.001
I0523 08:11:55.129344 35003 solver.cpp:239] Iteration 181200 (2.32788 iter/s, 4.29576s/10 iters), loss = 6.56943
I0523 08:11:55.129390 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56943 (* 1 = 6.56943 loss)
I0523 08:11:55.134290 35003 sgd_solver.cpp:112] Iteration 181200, lr = 0.001
I0523 08:11:58.703363 35003 solver.cpp:239] Iteration 181210 (2.79813 iter/s, 3.57382s/10 iters), loss = 6.75961
I0523 08:11:58.703414 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75961 (* 1 = 6.75961 loss)
I0523 08:11:58.862752 35003 sgd_solver.cpp:112] Iteration 181210, lr = 0.001
I0523 08:12:02.844142 35003 solver.cpp:239] Iteration 181220 (2.41513 iter/s, 4.14056s/10 iters), loss = 6.42633
I0523 08:12:02.844183 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42633 (* 1 = 6.42633 loss)
I0523 08:12:02.857393 35003 sgd_solver.cpp:112] Iteration 181220, lr = 0.001
I0523 08:12:04.880656 35003 solver.cpp:239] Iteration 181230 (4.91067 iter/s, 2.03638s/10 iters), loss = 8.28551
I0523 08:12:04.880700 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.28551 (* 1 = 8.28551 loss)
I0523 08:12:05.583456 35003 sgd_solver.cpp:112] Iteration 181230, lr = 0.001
I0523 08:12:07.623215 35003 solver.cpp:239] Iteration 181240 (3.64645 iter/s, 2.74239s/10 iters), loss = 6.96524
I0523 08:12:07.623265 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96524 (* 1 = 6.96524 loss)
I0523 08:12:07.626093 35003 sgd_solver.cpp:112] Iteration 181240, lr = 0.001
I0523 08:12:12.067634 35003 solver.cpp:239] Iteration 181250 (2.25013 iter/s, 4.44418s/10 iters), loss = 7.97447
I0523 08:12:12.067673 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.97447 (* 1 = 7.97447 loss)
I0523 08:12:12.138495 35003 sgd_solver.cpp:112] Iteration 181250, lr = 0.001
I0523 08:12:15.548058 35003 solver.cpp:239] Iteration 181260 (2.87339 iter/s, 3.48021s/10 iters), loss = 7.49012
I0523 08:12:15.548126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.49012 (* 1 = 7.49012 loss)
I0523 08:12:15.551738 35003 sgd_solver.cpp:112] Iteration 181260, lr = 0.001
I0523 08:12:16.863668 35003 solver.cpp:239] Iteration 181270 (7.6018 iter/s, 1.31548s/10 iters), loss = 6.29883
I0523 08:12:16.863710 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29883 (* 1 = 6.29883 loss)
I0523 08:12:16.874572 35003 sgd_solver.cpp:112] Iteration 181270, lr = 0.001
I0523 08:12:20.482152 35003 solver.cpp:239] Iteration 181280 (2.76373 iter/s, 3.61829s/10 iters), loss = 7.55432
I0523 08:12:20.482419 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55432 (* 1 = 7.55432 loss)
I0523 08:12:20.495708 35003 sgd_solver.cpp:112] Iteration 181280, lr = 0.001
I0523 08:12:24.316489 35003 solver.cpp:239] Iteration 181290 (2.60829 iter/s, 3.83393s/10 iters), loss = 5.21853
I0523 08:12:24.316540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.21853 (* 1 = 5.21853 loss)
I0523 08:12:24.877619 35003 sgd_solver.cpp:112] Iteration 181290, lr = 0.001
I0523 08:12:27.865058 35003 solver.cpp:239] Iteration 181300 (2.8182 iter/s, 3.54837s/10 iters), loss = 6.89122
I0523 08:12:27.865104 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89122 (* 1 = 6.89122 loss)
I0523 08:12:27.883183 35003 sgd_solver.cpp:112] Iteration 181300, lr = 0.001
I0523 08:12:29.988729 35003 solver.cpp:239] Iteration 181310 (4.70915 iter/s, 2.12352s/10 iters), loss = 7.69169
I0523 08:12:29.988768 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69169 (* 1 = 7.69169 loss)
I0523 08:12:30.002297 35003 sgd_solver.cpp:112] Iteration 181310, lr = 0.001
I0523 08:12:34.375581 35003 solver.cpp:239] Iteration 181320 (2.27965 iter/s, 4.38664s/10 iters), loss = 6.7862
I0523 08:12:34.375627 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7862 (* 1 = 6.7862 loss)
I0523 08:12:35.108461 35003 sgd_solver.cpp:112] Iteration 181320, lr = 0.001
I0523 08:12:38.595902 35003 solver.cpp:239] Iteration 181330 (2.36962 iter/s, 4.22009s/10 iters), loss = 7.25985
I0523 08:12:38.595981 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25985 (* 1 = 7.25985 loss)
I0523 08:12:38.608840 35003 sgd_solver.cpp:112] Iteration 181330, lr = 0.001
I0523 08:12:41.470404 35003 solver.cpp:239] Iteration 181340 (3.4791 iter/s, 2.8743s/10 iters), loss = 5.76694
I0523 08:12:41.470449 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.76694 (* 1 = 5.76694 loss)
I0523 08:12:41.479724 35003 sgd_solver.cpp:112] Iteration 181340, lr = 0.001
I0523 08:12:44.043628 35003 solver.cpp:239] Iteration 181350 (3.88641 iter/s, 2.57307s/10 iters), loss = 6.98014
I0523 08:12:44.043670 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98014 (* 1 = 6.98014 loss)
I0523 08:12:44.776124 35003 sgd_solver.cpp:112] Iteration 181350, lr = 0.001
I0523 08:12:49.016568 35003 solver.cpp:239] Iteration 181360 (2.01098 iter/s, 4.97269s/10 iters), loss = 8.00849
I0523 08:12:49.016621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00849 (* 1 = 8.00849 loss)
I0523 08:12:49.672755 35003 sgd_solver.cpp:112] Iteration 181360, lr = 0.001
I0523 08:12:53.687048 35003 solver.cpp:239] Iteration 181370 (2.14122 iter/s, 4.67023s/10 iters), loss = 6.70679
I0523 08:12:53.687238 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70679 (* 1 = 6.70679 loss)
I0523 08:12:53.714215 35003 sgd_solver.cpp:112] Iteration 181370, lr = 0.001
I0523 08:12:56.516222 35003 solver.cpp:239] Iteration 181380 (3.53495 iter/s, 2.82889s/10 iters), loss = 5.35192
I0523 08:12:56.516269 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.35192 (* 1 = 5.35192 loss)
I0523 08:12:56.525763 35003 sgd_solver.cpp:112] Iteration 181380, lr = 0.001
I0523 08:12:58.769841 35003 solver.cpp:239] Iteration 181390 (4.43759 iter/s, 2.25348s/10 iters), loss = 7.70823
I0523 08:12:58.769882 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70823 (* 1 = 7.70823 loss)
I0523 08:12:58.797309 35003 sgd_solver.cpp:112] Iteration 181390, lr = 0.001
I0523 08:13:02.829072 35003 solver.cpp:239] Iteration 181400 (2.46365 iter/s, 4.05902s/10 iters), loss = 7.29707
I0523 08:13:02.829123 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29707 (* 1 = 7.29707 loss)
I0523 08:13:03.343140 35003 sgd_solver.cpp:112] Iteration 181400, lr = 0.001
I0523 08:13:05.440052 35003 solver.cpp:239] Iteration 181410 (3.83022 iter/s, 2.61082s/10 iters), loss = 7.26079
I0523 08:13:05.440099 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26079 (* 1 = 7.26079 loss)
I0523 08:13:06.175024 35003 sgd_solver.cpp:112] Iteration 181410, lr = 0.001
I0523 08:13:10.516549 35003 solver.cpp:239] Iteration 181420 (1.96996 iter/s, 5.07624s/10 iters), loss = 7.2998
I0523 08:13:10.516598 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2998 (* 1 = 7.2998 loss)
I0523 08:13:10.525838 35003 sgd_solver.cpp:112] Iteration 181420, lr = 0.001
I0523 08:13:14.859128 35003 solver.cpp:239] Iteration 181430 (2.3029 iter/s, 4.34235s/10 iters), loss = 6.97786
I0523 08:13:14.859177 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97786 (* 1 = 6.97786 loss)
I0523 08:13:14.872265 35003 sgd_solver.cpp:112] Iteration 181430, lr = 0.001
I0523 08:13:18.277567 35003 solver.cpp:239] Iteration 181440 (2.92547 iter/s, 3.41825s/10 iters), loss = 6.80877
I0523 08:13:18.277612 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80877 (* 1 = 6.80877 loss)
I0523 08:13:18.287477 35003 sgd_solver.cpp:112] Iteration 181440, lr = 0.001
I0523 08:13:21.895869 35003 solver.cpp:239] Iteration 181450 (2.76388 iter/s, 3.6181s/10 iters), loss = 6.77263
I0523 08:13:21.895913 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77263 (* 1 = 6.77263 loss)
I0523 08:13:21.978729 35003 sgd_solver.cpp:112] Iteration 181450, lr = 0.001
I0523 08:13:23.680397 35003 solver.cpp:239] Iteration 181460 (5.60412 iter/s, 1.7844s/10 iters), loss = 6.6208
I0523 08:13:23.680435 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6208 (* 1 = 6.6208 loss)
I0523 08:13:23.688168 35003 sgd_solver.cpp:112] Iteration 181460, lr = 0.001
I0523 08:13:28.111508 35003 solver.cpp:239] Iteration 181470 (2.25689 iter/s, 4.43088s/10 iters), loss = 8.11827
I0523 08:13:28.111575 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.11827 (* 1 = 8.11827 loss)
I0523 08:13:28.820389 35003 sgd_solver.cpp:112] Iteration 181470, lr = 0.001
I0523 08:13:31.685104 35003 solver.cpp:239] Iteration 181480 (2.79847 iter/s, 3.57338s/10 iters), loss = 5.77356
I0523 08:13:31.685154 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77356 (* 1 = 5.77356 loss)
I0523 08:13:32.054392 35003 sgd_solver.cpp:112] Iteration 181480, lr = 0.001
I0523 08:13:35.721671 35003 solver.cpp:239] Iteration 181490 (2.47748 iter/s, 4.03635s/10 iters), loss = 6.5152
I0523 08:13:35.721710 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5152 (* 1 = 6.5152 loss)
I0523 08:13:35.735146 35003 sgd_solver.cpp:112] Iteration 181490, lr = 0.001
I0523 08:13:39.396409 35003 solver.cpp:239] Iteration 181500 (2.72143 iter/s, 3.67454s/10 iters), loss = 6.18936
I0523 08:13:39.396456 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.18936 (* 1 = 6.18936 loss)
I0523 08:13:39.408670 35003 sgd_solver.cpp:112] Iteration 181500, lr = 0.001
I0523 08:13:43.059417 35003 solver.cpp:239] Iteration 181510 (2.73014 iter/s, 3.66281s/10 iters), loss = 6.36392
I0523 08:13:43.059465 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36392 (* 1 = 6.36392 loss)
I0523 08:13:43.800804 35003 sgd_solver.cpp:112] Iteration 181510, lr = 0.001
I0523 08:13:46.116577 35003 solver.cpp:239] Iteration 181520 (3.27121 iter/s, 3.05697s/10 iters), loss = 5.96115
I0523 08:13:46.116623 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96115 (* 1 = 5.96115 loss)
I0523 08:13:46.122443 35003 sgd_solver.cpp:112] Iteration 181520, lr = 0.001
I0523 08:13:48.762542 35003 solver.cpp:239] Iteration 181530 (3.77957 iter/s, 2.6458s/10 iters), loss = 6.73376
I0523 08:13:48.762585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73376 (* 1 = 6.73376 loss)
I0523 08:13:48.775339 35003 sgd_solver.cpp:112] Iteration 181530, lr = 0.001
I0523 08:13:52.260010 35003 solver.cpp:239] Iteration 181540 (2.85937 iter/s, 3.49728s/10 iters), loss = 5.76388
I0523 08:13:52.260056 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.76388 (* 1 = 5.76388 loss)
I0523 08:13:52.285396 35003 sgd_solver.cpp:112] Iteration 181540, lr = 0.001
I0523 08:13:55.109632 35003 solver.cpp:239] Iteration 181550 (3.50945 iter/s, 2.84945s/10 iters), loss = 7.34102
I0523 08:13:55.109935 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34102 (* 1 = 7.34102 loss)
I0523 08:13:55.142693 35003 sgd_solver.cpp:112] Iteration 181550, lr = 0.001
I0523 08:13:58.042834 35003 solver.cpp:239] Iteration 181560 (3.4097 iter/s, 2.93281s/10 iters), loss = 5.87741
I0523 08:13:58.042879 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87741 (* 1 = 5.87741 loss)
I0523 08:13:58.053973 35003 sgd_solver.cpp:112] Iteration 181560, lr = 0.001
I0523 08:14:02.421069 35003 solver.cpp:239] Iteration 181570 (2.28415 iter/s, 4.378s/10 iters), loss = 7.09033
I0523 08:14:02.421119 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09033 (* 1 = 7.09033 loss)
I0523 08:14:03.159049 35003 sgd_solver.cpp:112] Iteration 181570, lr = 0.001
I0523 08:14:07.252020 35003 solver.cpp:239] Iteration 181580 (2.07009 iter/s, 4.8307s/10 iters), loss = 7.21074
I0523 08:14:07.252081 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21074 (* 1 = 7.21074 loss)
I0523 08:14:07.256561 35003 sgd_solver.cpp:112] Iteration 181580, lr = 0.001
I0523 08:14:11.202759 35003 solver.cpp:239] Iteration 181590 (2.53132 iter/s, 3.95051s/10 iters), loss = 6.4878
I0523 08:14:11.202803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4878 (* 1 = 6.4878 loss)
I0523 08:14:11.943135 35003 sgd_solver.cpp:112] Iteration 181590, lr = 0.001
I0523 08:14:13.999639 35003 solver.cpp:239] Iteration 181600 (3.57563 iter/s, 2.79671s/10 iters), loss = 6.83246
I0523 08:14:13.999691 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83246 (* 1 = 6.83246 loss)
I0523 08:14:14.740027 35003 sgd_solver.cpp:112] Iteration 181600, lr = 0.001
I0523 08:14:17.969156 35003 solver.cpp:239] Iteration 181610 (2.51933 iter/s, 3.9693s/10 iters), loss = 7.47415
I0523 08:14:17.969198 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47415 (* 1 = 7.47415 loss)
I0523 08:14:17.980119 35003 sgd_solver.cpp:112] Iteration 181610, lr = 0.001
I0523 08:14:20.653451 35003 solver.cpp:239] Iteration 181620 (3.7256 iter/s, 2.68413s/10 iters), loss = 6.70255
I0523 08:14:20.653513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70255 (* 1 = 6.70255 loss)
I0523 08:14:21.012178 35003 sgd_solver.cpp:112] Iteration 181620, lr = 0.001
I0523 08:14:23.674715 35003 solver.cpp:239] Iteration 181630 (3.3101 iter/s, 3.02105s/10 iters), loss = 7.89102
I0523 08:14:23.674765 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.89102 (* 1 = 7.89102 loss)
I0523 08:14:23.680073 35003 sgd_solver.cpp:112] Iteration 181630, lr = 0.001
I0523 08:14:27.957021 35003 solver.cpp:239] Iteration 181640 (2.33531 iter/s, 4.28209s/10 iters), loss = 6.27706
I0523 08:14:27.957227 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27706 (* 1 = 6.27706 loss)
I0523 08:14:28.672623 35003 sgd_solver.cpp:112] Iteration 181640, lr = 0.001
I0523 08:14:32.258203 35003 solver.cpp:239] Iteration 181650 (2.32513 iter/s, 4.30083s/10 iters), loss = 6.58059
I0523 08:14:32.258242 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58059 (* 1 = 6.58059 loss)
I0523 08:14:32.271399 35003 sgd_solver.cpp:112] Iteration 181650, lr = 0.001
I0523 08:14:36.625447 35003 solver.cpp:239] Iteration 181660 (2.28989 iter/s, 4.36702s/10 iters), loss = 5.6445
I0523 08:14:36.625486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.6445 (* 1 = 5.6445 loss)
I0523 08:14:36.638017 35003 sgd_solver.cpp:112] Iteration 181660, lr = 0.001
I0523 08:14:39.607298 35003 solver.cpp:239] Iteration 181670 (3.35381 iter/s, 2.98169s/10 iters), loss = 6.30874
I0523 08:14:39.607343 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30874 (* 1 = 6.30874 loss)
I0523 08:14:40.333861 35003 sgd_solver.cpp:112] Iteration 181670, lr = 0.001
I0523 08:14:42.452643 35003 solver.cpp:239] Iteration 181680 (3.51473 iter/s, 2.84517s/10 iters), loss = 6.54713
I0523 08:14:42.452697 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54713 (* 1 = 6.54713 loss)
I0523 08:14:42.459494 35003 sgd_solver.cpp:112] Iteration 181680, lr = 0.001
I0523 08:14:46.072440 35003 solver.cpp:239] Iteration 181690 (2.76274 iter/s, 3.61959s/10 iters), loss = 6.4621
I0523 08:14:46.072479 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4621 (* 1 = 6.4621 loss)
I0523 08:14:46.080415 35003 sgd_solver.cpp:112] Iteration 181690, lr = 0.001
I0523 08:14:51.132625 35003 solver.cpp:239] Iteration 181700 (1.97631 iter/s, 5.05993s/10 iters), loss = 6.44921
I0523 08:14:51.132669 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44921 (* 1 = 6.44921 loss)
I0523 08:14:51.145494 35003 sgd_solver.cpp:112] Iteration 181700, lr = 0.001
I0523 08:14:53.225747 35003 solver.cpp:239] Iteration 181710 (4.77786 iter/s, 2.09299s/10 iters), loss = 5.19084
I0523 08:14:53.225785 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.19084 (* 1 = 5.19084 loss)
I0523 08:14:53.239361 35003 sgd_solver.cpp:112] Iteration 181710, lr = 0.001
I0523 08:14:58.414408 35003 solver.cpp:239] Iteration 181720 (1.92738 iter/s, 5.1884s/10 iters), loss = 7.07153
I0523 08:14:58.414613 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07153 (* 1 = 7.07153 loss)
I0523 08:14:59.116046 35003 sgd_solver.cpp:112] Iteration 181720, lr = 0.001
I0523 08:15:02.282955 35003 solver.cpp:239] Iteration 181730 (2.58519 iter/s, 3.86819s/10 iters), loss = 6.04378
I0523 08:15:02.282994 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04378 (* 1 = 6.04378 loss)
I0523 08:15:02.288365 35003 sgd_solver.cpp:112] Iteration 181730, lr = 0.001
I0523 08:15:03.680387 35003 solver.cpp:239] Iteration 181740 (7.15658 iter/s, 1.39731s/10 iters), loss = 7.21307
I0523 08:15:03.680428 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21307 (* 1 = 7.21307 loss)
I0523 08:15:03.698782 35003 sgd_solver.cpp:112] Iteration 181740, lr = 0.001
I0523 08:15:08.590569 35003 solver.cpp:239] Iteration 181750 (2.03669 iter/s, 4.90993s/10 iters), loss = 7.192
I0523 08:15:08.590629 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.192 (* 1 = 7.192 loss)
I0523 08:15:08.621111 35003 sgd_solver.cpp:112] Iteration 181750, lr = 0.001
I0523 08:15:11.535444 35003 solver.cpp:239] Iteration 181760 (3.39594 iter/s, 2.94469s/10 iters), loss = 5.87094
I0523 08:15:11.535490 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87094 (* 1 = 5.87094 loss)
I0523 08:15:11.547056 35003 sgd_solver.cpp:112] Iteration 181760, lr = 0.001
I0523 08:15:15.046468 35003 solver.cpp:239] Iteration 181770 (2.84833 iter/s, 3.51082s/10 iters), loss = 6.43992
I0523 08:15:15.046540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43992 (* 1 = 6.43992 loss)
I0523 08:15:15.123558 35003 sgd_solver.cpp:112] Iteration 181770, lr = 0.001
I0523 08:15:18.213110 35003 solver.cpp:239] Iteration 181780 (3.15812 iter/s, 3.16644s/10 iters), loss = 7.30099
I0523 08:15:18.213156 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30099 (* 1 = 7.30099 loss)
I0523 08:15:18.225131 35003 sgd_solver.cpp:112] Iteration 181780, lr = 0.001
I0523 08:15:21.349850 35003 solver.cpp:239] Iteration 181790 (3.18821 iter/s, 3.13656s/10 iters), loss = 6.52286
I0523 08:15:21.349895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52286 (* 1 = 6.52286 loss)
I0523 08:15:22.064689 35003 sgd_solver.cpp:112] Iteration 181790, lr = 0.001
I0523 08:15:26.461401 35003 solver.cpp:239] Iteration 181800 (1.95645 iter/s, 5.1113s/10 iters), loss = 5.78036
I0523 08:15:26.461436 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.78036 (* 1 = 5.78036 loss)
I0523 08:15:26.474598 35003 sgd_solver.cpp:112] Iteration 181800, lr = 0.001
I0523 08:15:28.688468 35003 solver.cpp:239] Iteration 181810 (4.49048 iter/s, 2.22693s/10 iters), loss = 6.7402
I0523 08:15:28.688673 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7402 (* 1 = 6.7402 loss)
I0523 08:15:28.706818 35003 sgd_solver.cpp:112] Iteration 181810, lr = 0.001
I0523 08:15:32.312474 35003 solver.cpp:239] Iteration 181820 (2.75962 iter/s, 3.62368s/10 iters), loss = 7.37106
I0523 08:15:32.312513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37106 (* 1 = 7.37106 loss)
I0523 08:15:32.330708 35003 sgd_solver.cpp:112] Iteration 181820, lr = 0.001
I0523 08:15:35.971288 35003 solver.cpp:239] Iteration 181830 (2.73327 iter/s, 3.65862s/10 iters), loss = 7.04683
I0523 08:15:35.971326 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04683 (* 1 = 7.04683 loss)
I0523 08:15:35.988210 35003 sgd_solver.cpp:112] Iteration 181830, lr = 0.001
I0523 08:15:38.808405 35003 solver.cpp:239] Iteration 181840 (3.52491 iter/s, 2.83695s/10 iters), loss = 6.2193
I0523 08:15:38.808449 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2193 (* 1 = 6.2193 loss)
I0523 08:15:38.813894 35003 sgd_solver.cpp:112] Iteration 181840, lr = 0.001
I0523 08:15:42.419967 35003 solver.cpp:239] Iteration 181850 (2.76905 iter/s, 3.61135s/10 iters), loss = 6.89295
I0523 08:15:42.420017 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89295 (* 1 = 6.89295 loss)
I0523 08:15:42.434203 35003 sgd_solver.cpp:112] Iteration 181850, lr = 0.001
I0523 08:15:44.464494 35003 solver.cpp:239] Iteration 181860 (4.89145 iter/s, 2.04439s/10 iters), loss = 7.02552
I0523 08:15:44.464545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02552 (* 1 = 7.02552 loss)
I0523 08:15:45.088421 35003 sgd_solver.cpp:112] Iteration 181860, lr = 0.001
I0523 08:15:47.989436 35003 solver.cpp:239] Iteration 181870 (2.83708 iter/s, 3.52475s/10 iters), loss = 6.03176
I0523 08:15:47.989480 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03176 (* 1 = 6.03176 loss)
I0523 08:15:48.731096 35003 sgd_solver.cpp:112] Iteration 181870, lr = 0.001
I0523 08:15:52.981940 35003 solver.cpp:239] Iteration 181880 (2.00311 iter/s, 4.99224s/10 iters), loss = 7.47885
I0523 08:15:52.982019 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47885 (* 1 = 7.47885 loss)
I0523 08:15:53.416708 35003 sgd_solver.cpp:112] Iteration 181880, lr = 0.001
I0523 08:15:57.190053 35003 solver.cpp:239] Iteration 181890 (2.3765 iter/s, 4.20786s/10 iters), loss = 6.46207
I0523 08:15:57.190098 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46207 (* 1 = 6.46207 loss)
I0523 08:15:57.199965 35003 sgd_solver.cpp:112] Iteration 181890, lr = 0.001
I0523 08:16:01.311383 35003 solver.cpp:239] Iteration 181900 (2.42653 iter/s, 4.12111s/10 iters), loss = 7.47308
I0523 08:16:01.311650 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47308 (* 1 = 7.47308 loss)
I0523 08:16:01.324210 35003 sgd_solver.cpp:112] Iteration 181900, lr = 0.001
I0523 08:16:05.151279 35003 solver.cpp:239] Iteration 181910 (2.60451 iter/s, 3.83949s/10 iters), loss = 6.43696
I0523 08:16:05.151320 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43696 (* 1 = 6.43696 loss)
I0523 08:16:05.175447 35003 sgd_solver.cpp:112] Iteration 181910, lr = 0.001
I0523 08:16:08.064579 35003 solver.cpp:239] Iteration 181920 (3.43273 iter/s, 2.91313s/10 iters), loss = 6.70264
I0523 08:16:08.064623 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70264 (* 1 = 6.70264 loss)
I0523 08:16:08.084825 35003 sgd_solver.cpp:112] Iteration 181920, lr = 0.001
I0523 08:16:12.380363 35003 solver.cpp:239] Iteration 181930 (2.3172 iter/s, 4.31556s/10 iters), loss = 5.51921
I0523 08:16:12.380403 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.51921 (* 1 = 5.51921 loss)
I0523 08:16:12.386649 35003 sgd_solver.cpp:112] Iteration 181930, lr = 0.001
I0523 08:16:16.149441 35003 solver.cpp:239] Iteration 181940 (2.65331 iter/s, 3.76888s/10 iters), loss = 5.95407
I0523 08:16:16.149479 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95407 (* 1 = 5.95407 loss)
I0523 08:16:16.156498 35003 sgd_solver.cpp:112] Iteration 181940, lr = 0.001
I0523 08:16:20.491469 35003 solver.cpp:239] Iteration 181950 (2.30319 iter/s, 4.34181s/10 iters), loss = 7.0826
I0523 08:16:20.491511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0826 (* 1 = 7.0826 loss)
I0523 08:16:20.505360 35003 sgd_solver.cpp:112] Iteration 181950, lr = 0.001
I0523 08:16:23.430392 35003 solver.cpp:239] Iteration 181960 (3.4028 iter/s, 2.93876s/10 iters), loss = 6.26076
I0523 08:16:23.430433 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26076 (* 1 = 6.26076 loss)
I0523 08:16:24.125844 35003 sgd_solver.cpp:112] Iteration 181960, lr = 0.001
I0523 08:16:28.445097 35003 solver.cpp:239] Iteration 181970 (1.99423 iter/s, 5.01446s/10 iters), loss = 6.46249
I0523 08:16:28.445138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46249 (* 1 = 6.46249 loss)
I0523 08:16:28.451997 35003 sgd_solver.cpp:112] Iteration 181970, lr = 0.001
I0523 08:16:30.574553 35003 solver.cpp:239] Iteration 181980 (4.69634 iter/s, 2.12932s/10 iters), loss = 6.59299
I0523 08:16:30.574597 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59299 (* 1 = 6.59299 loss)
I0523 08:16:30.581784 35003 sgd_solver.cpp:112] Iteration 181980, lr = 0.001
I0523 08:16:32.736371 35003 solver.cpp:239] Iteration 181990 (4.62603 iter/s, 2.16168s/10 iters), loss = 7.47941
I0523 08:16:32.736795 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47941 (* 1 = 7.47941 loss)
I0523 08:16:32.763360 35003 sgd_solver.cpp:112] Iteration 181990, lr = 0.001
I0523 08:16:36.286737 35003 solver.cpp:239] Iteration 182000 (2.81706 iter/s, 3.5498s/10 iters), loss = 6.61508
I0523 08:16:36.286785 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61508 (* 1 = 6.61508 loss)
I0523 08:16:36.292681 35003 sgd_solver.cpp:112] Iteration 182000, lr = 0.001
I0523 08:16:39.713762 35003 solver.cpp:239] Iteration 182010 (2.91815 iter/s, 3.42683s/10 iters), loss = 5.9372
I0523 08:16:39.713809 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9372 (* 1 = 5.9372 loss)
I0523 08:16:39.732316 35003 sgd_solver.cpp:112] Iteration 182010, lr = 0.001
I0523 08:16:45.431077 35003 solver.cpp:239] Iteration 182020 (1.74916 iter/s, 5.71703s/10 iters), loss = 6.93868
I0523 08:16:45.431125 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93868 (* 1 = 6.93868 loss)
I0523 08:16:46.146701 35003 sgd_solver.cpp:112] Iteration 182020, lr = 0.001
I0523 08:16:48.947890 35003 solver.cpp:239] Iteration 182030 (2.84364 iter/s, 3.51662s/10 iters), loss = 7.83181
I0523 08:16:48.947932 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83181 (* 1 = 7.83181 loss)
I0523 08:16:49.663359 35003 sgd_solver.cpp:112] Iteration 182030, lr = 0.001
I0523 08:16:51.609642 35003 solver.cpp:239] Iteration 182040 (3.75718 iter/s, 2.66157s/10 iters), loss = 5.72732
I0523 08:16:51.609686 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.72732 (* 1 = 5.72732 loss)
I0523 08:16:51.622295 35003 sgd_solver.cpp:112] Iteration 182040, lr = 0.001
I0523 08:16:54.397289 35003 solver.cpp:239] Iteration 182050 (3.58747 iter/s, 2.78748s/10 iters), loss = 6.90398
I0523 08:16:54.397346 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90398 (* 1 = 6.90398 loss)
I0523 08:16:55.111929 35003 sgd_solver.cpp:112] Iteration 182050, lr = 0.001
I0523 08:16:58.742055 35003 solver.cpp:239] Iteration 182060 (2.30175 iter/s, 4.34452s/10 iters), loss = 5.92934
I0523 08:16:58.742106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92934 (* 1 = 5.92934 loss)
I0523 08:16:59.450469 35003 sgd_solver.cpp:112] Iteration 182060, lr = 0.001
I0523 08:17:01.607589 35003 solver.cpp:239] Iteration 182070 (3.48996 iter/s, 2.86536s/10 iters), loss = 6.97583
I0523 08:17:01.607635 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97583 (* 1 = 6.97583 loss)
I0523 08:17:02.221161 35003 sgd_solver.cpp:112] Iteration 182070, lr = 0.001
I0523 08:17:05.434011 35003 solver.cpp:239] Iteration 182080 (2.61355 iter/s, 3.82622s/10 iters), loss = 5.73609
I0523 08:17:05.434253 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.73609 (* 1 = 5.73609 loss)
I0523 08:17:05.441902 35003 sgd_solver.cpp:112] Iteration 182080, lr = 0.001
I0523 08:17:09.630015 35003 solver.cpp:239] Iteration 182090 (2.38344 iter/s, 4.19562s/10 iters), loss = 6.62937
I0523 08:17:09.630064 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62937 (* 1 = 6.62937 loss)
I0523 08:17:09.636548 35003 sgd_solver.cpp:112] Iteration 182090, lr = 0.001
I0523 08:17:13.258781 35003 solver.cpp:239] Iteration 182100 (2.75593 iter/s, 3.62854s/10 iters), loss = 7.32658
I0523 08:17:13.258824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32658 (* 1 = 7.32658 loss)
I0523 08:17:13.265027 35003 sgd_solver.cpp:112] Iteration 182100, lr = 0.001
I0523 08:17:17.589100 35003 solver.cpp:239] Iteration 182110 (2.30942 iter/s, 4.3301s/10 iters), loss = 7.03089
I0523 08:17:17.589150 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03089 (* 1 = 7.03089 loss)
I0523 08:17:17.596686 35003 sgd_solver.cpp:112] Iteration 182110, lr = 0.001
I0523 08:17:19.797224 35003 solver.cpp:239] Iteration 182120 (4.52903 iter/s, 2.20798s/10 iters), loss = 6.00043
I0523 08:17:19.797266 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00043 (* 1 = 6.00043 loss)
I0523 08:17:20.404145 35003 sgd_solver.cpp:112] Iteration 182120, lr = 0.001
I0523 08:17:23.231770 35003 solver.cpp:239] Iteration 182130 (2.91175 iter/s, 3.43436s/10 iters), loss = 7.12184
I0523 08:17:23.231807 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12184 (* 1 = 7.12184 loss)
I0523 08:17:23.850522 35003 sgd_solver.cpp:112] Iteration 182130, lr = 0.001
I0523 08:17:25.825476 35003 solver.cpp:239] Iteration 182140 (3.85571 iter/s, 2.59355s/10 iters), loss = 6.52565
I0523 08:17:25.825528 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52565 (* 1 = 6.52565 loss)
I0523 08:17:25.841189 35003 sgd_solver.cpp:112] Iteration 182140, lr = 0.001
I0523 08:17:27.871893 35003 solver.cpp:239] Iteration 182150 (4.88694 iter/s, 2.04627s/10 iters), loss = 5.76851
I0523 08:17:27.871934 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.76851 (* 1 = 5.76851 loss)
I0523 08:17:28.502207 35003 sgd_solver.cpp:112] Iteration 182150, lr = 0.001
I0523 08:17:31.229014 35003 solver.cpp:239] Iteration 182160 (2.97891 iter/s, 3.35693s/10 iters), loss = 6.44287
I0523 08:17:31.229051 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44287 (* 1 = 6.44287 loss)
I0523 08:17:31.239068 35003 sgd_solver.cpp:112] Iteration 182160, lr = 0.001
I0523 08:17:35.933449 35003 solver.cpp:239] Iteration 182170 (2.12576 iter/s, 4.7042s/10 iters), loss = 5.85438
I0523 08:17:35.933684 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85438 (* 1 = 5.85438 loss)
I0523 08:17:35.939344 35003 sgd_solver.cpp:112] Iteration 182170, lr = 0.001
I0523 08:17:40.198397 35003 solver.cpp:239] Iteration 182180 (2.34492 iter/s, 4.26454s/10 iters), loss = 6.99786
I0523 08:17:40.198459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99786 (* 1 = 6.99786 loss)
I0523 08:17:40.913836 35003 sgd_solver.cpp:112] Iteration 182180, lr = 0.001
I0523 08:17:44.537362 35003 solver.cpp:239] Iteration 182190 (2.30482 iter/s, 4.33873s/10 iters), loss = 6.50602
I0523 08:17:44.537408 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50602 (* 1 = 6.50602 loss)
I0523 08:17:44.550776 35003 sgd_solver.cpp:112] Iteration 182190, lr = 0.001
I0523 08:17:47.947636 35003 solver.cpp:239] Iteration 182200 (2.93248 iter/s, 3.41008s/10 iters), loss = 5.26255
I0523 08:17:47.947676 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.26255 (* 1 = 5.26255 loss)
I0523 08:17:48.648147 35003 sgd_solver.cpp:112] Iteration 182200, lr = 0.001
I0523 08:17:52.235937 35003 solver.cpp:239] Iteration 182210 (2.33205 iter/s, 4.28808s/10 iters), loss = 6.00416
I0523 08:17:52.235996 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00416 (* 1 = 6.00416 loss)
I0523 08:17:52.271457 35003 sgd_solver.cpp:112] Iteration 182210, lr = 0.001
I0523 08:17:55.104298 35003 solver.cpp:239] Iteration 182220 (3.48653 iter/s, 2.86818s/10 iters), loss = 6.58882
I0523 08:17:55.104344 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58882 (* 1 = 6.58882 loss)
I0523 08:17:55.110067 35003 sgd_solver.cpp:112] Iteration 182220, lr = 0.001
I0523 08:17:58.779429 35003 solver.cpp:239] Iteration 182230 (2.72114 iter/s, 3.67493s/10 iters), loss = 5.92204
I0523 08:17:58.779471 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92204 (* 1 = 5.92204 loss)
I0523 08:17:58.802539 35003 sgd_solver.cpp:112] Iteration 182230, lr = 0.001
I0523 08:18:01.636947 35003 solver.cpp:239] Iteration 182240 (3.49974 iter/s, 2.85735s/10 iters), loss = 7.53688
I0523 08:18:01.636986 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.53688 (* 1 = 7.53688 loss)
I0523 08:18:01.644461 35003 sgd_solver.cpp:112] Iteration 182240, lr = 0.001
I0523 08:18:04.670500 35003 solver.cpp:239] Iteration 182250 (3.29666 iter/s, 3.03337s/10 iters), loss = 7.6385
I0523 08:18:04.670545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.6385 (* 1 = 7.6385 loss)
I0523 08:18:04.683789 35003 sgd_solver.cpp:112] Iteration 182250, lr = 0.001
I0523 08:18:08.226882 35003 solver.cpp:239] Iteration 182260 (2.812 iter/s, 3.55619s/10 iters), loss = 8.27213
I0523 08:18:08.227126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.27213 (* 1 = 8.27213 loss)
I0523 08:18:08.617275 35003 sgd_solver.cpp:112] Iteration 182260, lr = 0.001
I0523 08:18:12.306251 35003 solver.cpp:239] Iteration 182270 (2.4516 iter/s, 4.07897s/10 iters), loss = 6.16021
I0523 08:18:12.306306 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16021 (* 1 = 6.16021 loss)
I0523 08:18:13.015323 35003 sgd_solver.cpp:112] Iteration 182270, lr = 0.001
I0523 08:18:17.345741 35003 solver.cpp:239] Iteration 182280 (1.98443 iter/s, 5.03923s/10 iters), loss = 6.50222
I0523 08:18:17.345787 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50222 (* 1 = 6.50222 loss)
I0523 08:18:18.046841 35003 sgd_solver.cpp:112] Iteration 182280, lr = 0.001
I0523 08:18:20.899785 35003 solver.cpp:239] Iteration 182290 (2.81385 iter/s, 3.55385s/10 iters), loss = 7.84323
I0523 08:18:20.899834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84323 (* 1 = 7.84323 loss)
I0523 08:18:21.608156 35003 sgd_solver.cpp:112] Iteration 182290, lr = 0.001
I0523 08:18:25.054081 35003 solver.cpp:239] Iteration 182300 (2.40727 iter/s, 4.15408s/10 iters), loss = 6.73088
I0523 08:18:25.054118 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73088 (* 1 = 6.73088 loss)
I0523 08:18:25.383941 35003 sgd_solver.cpp:112] Iteration 182300, lr = 0.001
I0523 08:18:28.164343 35003 solver.cpp:239] Iteration 182310 (3.21534 iter/s, 3.11009s/10 iters), loss = 5.713
I0523 08:18:28.164383 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.713 (* 1 = 5.713 loss)
I0523 08:18:28.182790 35003 sgd_solver.cpp:112] Iteration 182310, lr = 0.001
I0523 08:18:31.799509 35003 solver.cpp:239] Iteration 182320 (2.75106 iter/s, 3.63497s/10 iters), loss = 5.49681
I0523 08:18:31.799557 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.49681 (* 1 = 5.49681 loss)
I0523 08:18:31.811959 35003 sgd_solver.cpp:112] Iteration 182320, lr = 0.001
I0523 08:18:34.638016 35003 solver.cpp:239] Iteration 182330 (3.52319 iter/s, 2.83834s/10 iters), loss = 6.17413
I0523 08:18:34.638058 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17413 (* 1 = 6.17413 loss)
I0523 08:18:34.650589 35003 sgd_solver.cpp:112] Iteration 182330, lr = 0.001
I0523 08:18:37.578147 35003 solver.cpp:239] Iteration 182340 (3.4014 iter/s, 2.93997s/10 iters), loss = 6.1089
I0523 08:18:37.578191 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1089 (* 1 = 6.1089 loss)
I0523 08:18:38.316484 35003 sgd_solver.cpp:112] Iteration 182340, lr = 0.001
I0523 08:18:40.506492 35003 solver.cpp:239] Iteration 182350 (3.41509 iter/s, 2.92818s/10 iters), loss = 7.16484
I0523 08:18:40.506533 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16484 (* 1 = 7.16484 loss)
I0523 08:18:41.246966 35003 sgd_solver.cpp:112] Iteration 182350, lr = 0.001
I0523 08:18:44.538019 35003 solver.cpp:239] Iteration 182360 (2.48058 iter/s, 4.03132s/10 iters), loss = 6.58523
I0523 08:18:44.538066 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58523 (* 1 = 6.58523 loss)
I0523 08:18:44.551100 35003 sgd_solver.cpp:112] Iteration 182360, lr = 0.001
I0523 08:18:47.373608 35003 solver.cpp:239] Iteration 182370 (3.52681 iter/s, 2.83542s/10 iters), loss = 7.17375
I0523 08:18:47.373651 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17375 (* 1 = 7.17375 loss)
I0523 08:18:47.387038 35003 sgd_solver.cpp:112] Iteration 182370, lr = 0.001
I0523 08:18:49.475828 35003 solver.cpp:239] Iteration 182380 (4.75718 iter/s, 2.10208s/10 iters), loss = 7.06932
I0523 08:18:49.475873 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06932 (* 1 = 7.06932 loss)
I0523 08:18:50.159173 35003 sgd_solver.cpp:112] Iteration 182380, lr = 0.001
I0523 08:18:53.768672 35003 solver.cpp:239] Iteration 182390 (2.3296 iter/s, 4.29259s/10 iters), loss = 8.17484
I0523 08:18:53.768736 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.17484 (* 1 = 8.17484 loss)
I0523 08:18:53.777276 35003 sgd_solver.cpp:112] Iteration 182390, lr = 0.001
I0523 08:18:57.861202 35003 solver.cpp:239] Iteration 182400 (2.44361 iter/s, 4.0923s/10 iters), loss = 7.48847
I0523 08:18:57.861243 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48847 (* 1 = 7.48847 loss)
I0523 08:18:58.177764 35003 sgd_solver.cpp:112] Iteration 182400, lr = 0.001
I0523 08:19:01.197016 35003 solver.cpp:239] Iteration 182410 (2.99793 iter/s, 3.33563s/10 iters), loss = 6.10806
I0523 08:19:01.197068 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10806 (* 1 = 6.10806 loss)
I0523 08:19:01.925036 35003 sgd_solver.cpp:112] Iteration 182410, lr = 0.001
I0523 08:19:05.547762 35003 solver.cpp:239] Iteration 182420 (2.29858 iter/s, 4.35051s/10 iters), loss = 6.13663
I0523 08:19:05.547816 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13663 (* 1 = 6.13663 loss)
I0523 08:19:05.553170 35003 sgd_solver.cpp:112] Iteration 182420, lr = 0.001
I0523 08:19:09.706128 35003 solver.cpp:239] Iteration 182430 (2.40492 iter/s, 4.15814s/10 iters), loss = 7.35414
I0523 08:19:09.706413 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.35414 (* 1 = 7.35414 loss)
I0523 08:19:09.719306 35003 sgd_solver.cpp:112] Iteration 182430, lr = 0.001
I0523 08:19:14.060376 35003 solver.cpp:239] Iteration 182440 (2.29684 iter/s, 4.35381s/10 iters), loss = 6.4551
I0523 08:19:14.060443 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4551 (* 1 = 6.4551 loss)
I0523 08:19:14.083158 35003 sgd_solver.cpp:112] Iteration 182440, lr = 0.001
I0523 08:19:17.604725 35003 solver.cpp:239] Iteration 182450 (2.82156 iter/s, 3.54414s/10 iters), loss = 7.32792
I0523 08:19:17.604763 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32792 (* 1 = 7.32792 loss)
I0523 08:19:17.609263 35003 sgd_solver.cpp:112] Iteration 182450, lr = 0.001
I0523 08:19:21.078003 35003 solver.cpp:239] Iteration 182460 (2.87928 iter/s, 3.4731s/10 iters), loss = 6.60081
I0523 08:19:21.078055 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60081 (* 1 = 6.60081 loss)
I0523 08:19:21.081377 35003 sgd_solver.cpp:112] Iteration 182460, lr = 0.001
I0523 08:19:23.151264 35003 solver.cpp:239] Iteration 182470 (4.82365 iter/s, 2.07312s/10 iters), loss = 6.71998
I0523 08:19:23.151314 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71998 (* 1 = 6.71998 loss)
I0523 08:19:23.160565 35003 sgd_solver.cpp:112] Iteration 182470, lr = 0.001
I0523 08:19:27.139973 35003 solver.cpp:239] Iteration 182480 (2.50721 iter/s, 3.9885s/10 iters), loss = 6.99425
I0523 08:19:27.140012 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99425 (* 1 = 6.99425 loss)
I0523 08:19:27.151579 35003 sgd_solver.cpp:112] Iteration 182480, lr = 0.001
I0523 08:19:30.649399 35003 solver.cpp:239] Iteration 182490 (2.84962 iter/s, 3.50924s/10 iters), loss = 5.74742
I0523 08:19:30.649439 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74742 (* 1 = 5.74742 loss)
I0523 08:19:31.263943 35003 sgd_solver.cpp:112] Iteration 182490, lr = 0.001
I0523 08:19:34.036679 35003 solver.cpp:239] Iteration 182500 (2.95238 iter/s, 3.38709s/10 iters), loss = 5.92376
I0523 08:19:34.036727 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92376 (* 1 = 5.92376 loss)
I0523 08:19:34.049660 35003 sgd_solver.cpp:112] Iteration 182500, lr = 0.001
I0523 08:19:39.180533 35003 solver.cpp:239] Iteration 182510 (1.94417 iter/s, 5.1436s/10 iters), loss = 5.87727
I0523 08:19:39.180596 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87727 (* 1 = 5.87727 loss)
I0523 08:19:39.222106 35003 sgd_solver.cpp:112] Iteration 182510, lr = 0.001
I0523 08:19:42.569424 35003 solver.cpp:239] Iteration 182520 (2.951 iter/s, 3.38868s/10 iters), loss = 7.16934
I0523 08:19:42.569664 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16934 (* 1 = 7.16934 loss)
I0523 08:19:42.582684 35003 sgd_solver.cpp:112] Iteration 182520, lr = 0.001
I0523 08:19:43.974601 35003 solver.cpp:239] Iteration 182530 (7.11805 iter/s, 1.40488s/10 iters), loss = 6.24143
I0523 08:19:43.974645 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24143 (* 1 = 6.24143 loss)
I0523 08:19:44.716161 35003 sgd_solver.cpp:112] Iteration 182530, lr = 0.001
I0523 08:19:47.973915 35003 solver.cpp:239] Iteration 182540 (2.50056 iter/s, 3.9991s/10 iters), loss = 7.14576
I0523 08:19:47.973963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14576 (* 1 = 7.14576 loss)
I0523 08:19:47.987099 35003 sgd_solver.cpp:112] Iteration 182540, lr = 0.001
I0523 08:19:50.705235 35003 solver.cpp:239] Iteration 182550 (3.66147 iter/s, 2.73114s/10 iters), loss = 6.1853
I0523 08:19:50.705299 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1853 (* 1 = 6.1853 loss)
I0523 08:19:50.798041 35003 sgd_solver.cpp:112] Iteration 182550, lr = 0.001
I0523 08:19:53.250568 35003 solver.cpp:239] Iteration 182560 (3.92902 iter/s, 2.54516s/10 iters), loss = 5.6033
I0523 08:19:53.250613 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.6033 (* 1 = 5.6033 loss)
I0523 08:19:53.972097 35003 sgd_solver.cpp:112] Iteration 182560, lr = 0.001
I0523 08:19:57.459127 35003 solver.cpp:239] Iteration 182570 (2.37624 iter/s, 4.20834s/10 iters), loss = 5.81859
I0523 08:19:57.459179 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.81859 (* 1 = 5.81859 loss)
I0523 08:19:57.958144 35003 sgd_solver.cpp:112] Iteration 182570, lr = 0.001
I0523 08:20:00.044260 35003 solver.cpp:239] Iteration 182580 (3.86851 iter/s, 2.58497s/10 iters), loss = 6.16309
I0523 08:20:00.044306 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16309 (* 1 = 6.16309 loss)
I0523 08:20:00.056143 35003 sgd_solver.cpp:112] Iteration 182580, lr = 0.001
I0523 08:20:03.041523 35003 solver.cpp:239] Iteration 182590 (3.33657 iter/s, 2.99709s/10 iters), loss = 6.61477
I0523 08:20:03.041584 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61477 (* 1 = 6.61477 loss)
I0523 08:20:03.049952 35003 sgd_solver.cpp:112] Iteration 182590, lr = 0.001
I0523 08:20:07.337321 35003 solver.cpp:239] Iteration 182600 (2.32798 iter/s, 4.29557s/10 iters), loss = 7.29179
I0523 08:20:07.337363 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.29179 (* 1 = 7.29179 loss)
I0523 08:20:07.345032 35003 sgd_solver.cpp:112] Iteration 182600, lr = 0.001
I0523 08:20:09.192926 35003 solver.cpp:239] Iteration 182610 (5.38944 iter/s, 1.85548s/10 iters), loss = 6.38027
I0523 08:20:09.192979 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38027 (* 1 = 6.38027 loss)
I0523 08:20:09.927227 35003 sgd_solver.cpp:112] Iteration 182610, lr = 0.001
I0523 08:20:13.638972 35003 solver.cpp:239] Iteration 182620 (2.24931 iter/s, 4.44581s/10 iters), loss = 6.14471
I0523 08:20:13.639206 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14471 (* 1 = 6.14471 loss)
I0523 08:20:13.643455 35003 sgd_solver.cpp:112] Iteration 182620, lr = 0.001
I0523 08:20:17.042156 35003 solver.cpp:239] Iteration 182630 (2.93874 iter/s, 3.40282s/10 iters), loss = 7.16112
I0523 08:20:17.042199 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16112 (* 1 = 7.16112 loss)
I0523 08:20:17.045415 35003 sgd_solver.cpp:112] Iteration 182630, lr = 0.001
I0523 08:20:20.720543 35003 solver.cpp:239] Iteration 182640 (2.71873 iter/s, 3.67819s/10 iters), loss = 6.30496
I0523 08:20:20.720598 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30496 (* 1 = 6.30496 loss)
I0523 08:20:20.726557 35003 sgd_solver.cpp:112] Iteration 182640, lr = 0.001
I0523 08:20:22.215114 35003 solver.cpp:239] Iteration 182650 (6.69142 iter/s, 1.49445s/10 iters), loss = 6.84392
I0523 08:20:22.215163 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84392 (* 1 = 6.84392 loss)
I0523 08:20:22.850422 35003 sgd_solver.cpp:112] Iteration 182650, lr = 0.001
I0523 08:20:26.532147 35003 solver.cpp:239] Iteration 182660 (2.31653 iter/s, 4.31681s/10 iters), loss = 6.9079
I0523 08:20:26.532183 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9079 (* 1 = 6.9079 loss)
I0523 08:20:26.545857 35003 sgd_solver.cpp:112] Iteration 182660, lr = 0.001
I0523 08:20:30.260517 35003 solver.cpp:239] Iteration 182670 (2.68228 iter/s, 3.72818s/10 iters), loss = 6.15722
I0523 08:20:30.260560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15722 (* 1 = 6.15722 loss)
I0523 08:20:31.001305 35003 sgd_solver.cpp:112] Iteration 182670, lr = 0.001
I0523 08:20:33.149281 35003 solver.cpp:239] Iteration 182680 (3.46189 iter/s, 2.88859s/10 iters), loss = 5.4389
I0523 08:20:33.149333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.4389 (* 1 = 5.4389 loss)
I0523 08:20:33.162302 35003 sgd_solver.cpp:112] Iteration 182680, lr = 0.001
I0523 08:20:35.788002 35003 solver.cpp:239] Iteration 182690 (3.78996 iter/s, 2.63855s/10 iters), loss = 6.6569
I0523 08:20:35.788055 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6569 (* 1 = 6.6569 loss)
I0523 08:20:36.529512 35003 sgd_solver.cpp:112] Iteration 182690, lr = 0.001
I0523 08:20:38.136390 35003 solver.cpp:239] Iteration 182700 (4.25852 iter/s, 2.34823s/10 iters), loss = 7.03763
I0523 08:20:38.136441 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.03763 (* 1 = 7.03763 loss)
I0523 08:20:38.145437 35003 sgd_solver.cpp:112] Iteration 182700, lr = 0.001
I0523 08:20:40.171074 35003 solver.cpp:239] Iteration 182710 (4.91511 iter/s, 2.03454s/10 iters), loss = 6.51939
I0523 08:20:40.171120 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51939 (* 1 = 6.51939 loss)
I0523 08:20:40.184834 35003 sgd_solver.cpp:112] Iteration 182710, lr = 0.001
I0523 08:20:42.249330 35003 solver.cpp:239] Iteration 182720 (4.81205 iter/s, 2.07811s/10 iters), loss = 5.76903
I0523 08:20:42.249372 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.76903 (* 1 = 5.76903 loss)
I0523 08:20:42.262223 35003 sgd_solver.cpp:112] Iteration 182720, lr = 0.001
I0523 08:20:45.097401 35003 solver.cpp:239] Iteration 182730 (3.51135 iter/s, 2.84791s/10 iters), loss = 6.24456
I0523 08:20:45.097671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24456 (* 1 = 6.24456 loss)
I0523 08:20:45.110945 35003 sgd_solver.cpp:112] Iteration 182730, lr = 0.001
I0523 08:20:48.346242 35003 solver.cpp:239] Iteration 182740 (3.07837 iter/s, 3.24847s/10 iters), loss = 6.97069
I0523 08:20:48.346292 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97069 (* 1 = 6.97069 loss)
I0523 08:20:49.062064 35003 sgd_solver.cpp:112] Iteration 182740, lr = 0.001
I0523 08:20:53.247748 35003 solver.cpp:239] Iteration 182750 (2.04029 iter/s, 4.90126s/10 iters), loss = 6.30912
I0523 08:20:53.247797 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30912 (* 1 = 6.30912 loss)
I0523 08:20:53.968462 35003 sgd_solver.cpp:112] Iteration 182750, lr = 0.001
I0523 08:20:58.261777 35003 solver.cpp:239] Iteration 182760 (1.99451 iter/s, 5.01377s/10 iters), loss = 5.68919
I0523 08:20:58.261834 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.68919 (* 1 = 5.68919 loss)
I0523 08:20:58.991469 35003 sgd_solver.cpp:112] Iteration 182760, lr = 0.001
I0523 08:21:01.417502 35003 solver.cpp:239] Iteration 182770 (3.16904 iter/s, 3.15553s/10 iters), loss = 5.7221
I0523 08:21:01.417552 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7221 (* 1 = 5.7221 loss)
I0523 08:21:01.421303 35003 sgd_solver.cpp:112] Iteration 182770, lr = 0.001
I0523 08:21:04.791806 35003 solver.cpp:239] Iteration 182780 (2.96376 iter/s, 3.37409s/10 iters), loss = 6.01788
I0523 08:21:04.791847 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01788 (* 1 = 6.01788 loss)
I0523 08:21:04.797350 35003 sgd_solver.cpp:112] Iteration 182780, lr = 0.001
I0523 08:21:07.044411 35003 solver.cpp:239] Iteration 182790 (4.43958 iter/s, 2.25246s/10 iters), loss = 6.15969
I0523 08:21:07.044464 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15969 (* 1 = 6.15969 loss)
I0523 08:21:07.784739 35003 sgd_solver.cpp:112] Iteration 182790, lr = 0.001
I0523 08:21:11.468446 35003 solver.cpp:239] Iteration 182800 (2.2605 iter/s, 4.4238s/10 iters), loss = 5.1586
I0523 08:21:11.468497 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.1586 (* 1 = 5.1586 loss)
I0523 08:21:12.183421 35003 sgd_solver.cpp:112] Iteration 182800, lr = 0.001
I0523 08:21:16.300357 35003 solver.cpp:239] Iteration 182810 (2.06968 iter/s, 4.83166s/10 iters), loss = 6.14685
I0523 08:21:16.300647 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14685 (* 1 = 6.14685 loss)
I0523 08:21:16.613381 35003 sgd_solver.cpp:112] Iteration 182810, lr = 0.001
I0523 08:21:20.244877 35003 solver.cpp:239] Iteration 182820 (2.53544 iter/s, 3.94409s/10 iters), loss = 6.40453
I0523 08:21:20.244930 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40453 (* 1 = 6.40453 loss)
I0523 08:21:20.976774 35003 sgd_solver.cpp:112] Iteration 182820, lr = 0.001
I0523 08:21:24.587090 35003 solver.cpp:239] Iteration 182830 (2.3031 iter/s, 4.34198s/10 iters), loss = 6.77746
I0523 08:21:24.587139 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77746 (* 1 = 6.77746 loss)
I0523 08:21:24.599964 35003 sgd_solver.cpp:112] Iteration 182830, lr = 0.001
I0523 08:21:27.529537 35003 solver.cpp:239] Iteration 182840 (3.39875 iter/s, 2.94226s/10 iters), loss = 6.655
I0523 08:21:27.529613 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.655 (* 1 = 6.655 loss)
I0523 08:21:27.555299 35003 sgd_solver.cpp:112] Iteration 182840, lr = 0.001
I0523 08:21:31.087205 35003 solver.cpp:239] Iteration 182850 (2.811 iter/s, 3.55745s/10 iters), loss = 6.87772
I0523 08:21:31.087250 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87772 (* 1 = 6.87772 loss)
I0523 08:21:31.104341 35003 sgd_solver.cpp:112] Iteration 182850, lr = 0.001
I0523 08:21:32.533771 35003 solver.cpp:239] Iteration 182860 (6.91347 iter/s, 1.44645s/10 iters), loss = 5.82153
I0523 08:21:32.533818 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.82153 (* 1 = 5.82153 loss)
I0523 08:21:33.271014 35003 sgd_solver.cpp:112] Iteration 182860, lr = 0.001
I0523 08:21:36.756135 35003 solver.cpp:239] Iteration 182870 (2.36846 iter/s, 4.22215s/10 iters), loss = 6.32076
I0523 08:21:36.756184 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32076 (* 1 = 6.32076 loss)
I0523 08:21:36.762174 35003 sgd_solver.cpp:112] Iteration 182870, lr = 0.001
I0523 08:21:40.195418 35003 solver.cpp:239] Iteration 182880 (2.90775 iter/s, 3.43909s/10 iters), loss = 6.00666
I0523 08:21:40.195477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00666 (* 1 = 6.00666 loss)
I0523 08:21:40.208268 35003 sgd_solver.cpp:112] Iteration 182880, lr = 0.001
I0523 08:21:43.774650 35003 solver.cpp:239] Iteration 182890 (2.79406 iter/s, 3.57902s/10 iters), loss = 6.05859
I0523 08:21:43.774721 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05859 (* 1 = 6.05859 loss)
I0523 08:21:43.787598 35003 sgd_solver.cpp:112] Iteration 182890, lr = 0.001
I0523 08:21:47.353665 35003 solver.cpp:239] Iteration 182900 (2.79424 iter/s, 3.57879s/10 iters), loss = 6.65726
I0523 08:21:47.358839 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65726 (* 1 = 6.65726 loss)
I0523 08:21:47.421957 35003 sgd_solver.cpp:112] Iteration 182900, lr = 0.001
I0523 08:21:50.120606 35003 solver.cpp:239] Iteration 182910 (3.621 iter/s, 2.76167s/10 iters), loss = 6.30478
I0523 08:21:50.120645 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30478 (* 1 = 6.30478 loss)
I0523 08:21:50.129571 35003 sgd_solver.cpp:112] Iteration 182910, lr = 0.001
I0523 08:21:54.293874 35003 solver.cpp:239] Iteration 182920 (2.39633 iter/s, 4.17305s/10 iters), loss = 5.67147
I0523 08:21:54.293922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.67147 (* 1 = 5.67147 loss)
I0523 08:21:54.987664 35003 sgd_solver.cpp:112] Iteration 182920, lr = 0.001
I0523 08:21:58.471772 35003 solver.cpp:239] Iteration 182930 (2.39368 iter/s, 4.17768s/10 iters), loss = 6.5663
I0523 08:21:58.471813 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5663 (* 1 = 6.5663 loss)
I0523 08:21:58.494516 35003 sgd_solver.cpp:112] Iteration 182930, lr = 0.001
I0523 08:22:00.539037 35003 solver.cpp:239] Iteration 182940 (4.83762 iter/s, 2.06713s/10 iters), loss = 6.46543
I0523 08:22:00.539088 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46543 (* 1 = 6.46543 loss)
I0523 08:22:00.556882 35003 sgd_solver.cpp:112] Iteration 182940, lr = 0.001
I0523 08:22:03.805922 35003 solver.cpp:239] Iteration 182950 (3.0612 iter/s, 3.26669s/10 iters), loss = 7.16378
I0523 08:22:03.805975 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16378 (* 1 = 7.16378 loss)
I0523 08:22:03.819419 35003 sgd_solver.cpp:112] Iteration 182950, lr = 0.001
I0523 08:22:07.348906 35003 solver.cpp:239] Iteration 182960 (2.82264 iter/s, 3.54278s/10 iters), loss = 7.12267
I0523 08:22:07.348942 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12267 (* 1 = 7.12267 loss)
I0523 08:22:08.016079 35003 sgd_solver.cpp:112] Iteration 182960, lr = 0.001
I0523 08:22:12.078079 35003 solver.cpp:239] Iteration 182970 (2.11465 iter/s, 4.72893s/10 iters), loss = 7.9024
I0523 08:22:12.078140 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.9024 (* 1 = 7.9024 loss)
I0523 08:22:12.794347 35003 sgd_solver.cpp:112] Iteration 182970, lr = 0.001
I0523 08:22:16.113668 35003 solver.cpp:239] Iteration 182980 (2.47809 iter/s, 4.03536s/10 iters), loss = 6.27525
I0523 08:22:16.113714 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27525 (* 1 = 6.27525 loss)
I0523 08:22:16.593791 35003 sgd_solver.cpp:112] Iteration 182980, lr = 0.001
I0523 08:22:20.084357 35003 solver.cpp:239] Iteration 182990 (2.51859 iter/s, 3.97048s/10 iters), loss = 6.43783
I0523 08:22:20.084489 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43783 (* 1 = 6.43783 loss)
I0523 08:22:20.093773 35003 sgd_solver.cpp:112] Iteration 182990, lr = 0.001
I0523 08:22:21.833652 35003 solver.cpp:239] Iteration 183000 (5.71729 iter/s, 1.74908s/10 iters), loss = 6.17874
I0523 08:22:21.833716 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17874 (* 1 = 6.17874 loss)
I0523 08:22:22.542101 35003 sgd_solver.cpp:112] Iteration 183000, lr = 0.001
I0523 08:22:23.869164 35003 solver.cpp:239] Iteration 183010 (4.91313 iter/s, 2.03536s/10 iters), loss = 6.22852
I0523 08:22:23.869213 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22852 (* 1 = 6.22852 loss)
I0523 08:22:23.882005 35003 sgd_solver.cpp:112] Iteration 183010, lr = 0.001
I0523 08:22:28.596298 35003 solver.cpp:239] Iteration 183020 (2.11556 iter/s, 4.72689s/10 iters), loss = 6.1255
I0523 08:22:28.596362 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1255 (* 1 = 6.1255 loss)
I0523 08:22:28.603384 35003 sgd_solver.cpp:112] Iteration 183020, lr = 0.001
I0523 08:22:31.401132 35003 solver.cpp:239] Iteration 183030 (3.5655 iter/s, 2.80466s/10 iters), loss = 8.00332
I0523 08:22:31.401170 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00332 (* 1 = 8.00332 loss)
I0523 08:22:31.411702 35003 sgd_solver.cpp:112] Iteration 183030, lr = 0.001
I0523 08:22:33.543591 35003 solver.cpp:239] Iteration 183040 (4.66782 iter/s, 2.14233s/10 iters), loss = 5.8228
I0523 08:22:33.543637 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.8228 (* 1 = 5.8228 loss)
I0523 08:22:34.272061 35003 sgd_solver.cpp:112] Iteration 183040, lr = 0.001
I0523 08:22:38.467223 35003 solver.cpp:239] Iteration 183050 (2.03112 iter/s, 4.92339s/10 iters), loss = 6.85454
I0523 08:22:38.467267 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85454 (* 1 = 6.85454 loss)
I0523 08:22:39.197906 35003 sgd_solver.cpp:112] Iteration 183050, lr = 0.001
I0523 08:22:42.133440 35003 solver.cpp:239] Iteration 183060 (2.72776 iter/s, 3.66602s/10 iters), loss = 6.22019
I0523 08:22:42.133484 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22019 (* 1 = 6.22019 loss)
I0523 08:22:42.867084 35003 sgd_solver.cpp:112] Iteration 183060, lr = 0.001
I0523 08:22:46.753830 35003 solver.cpp:239] Iteration 183070 (2.16444 iter/s, 4.62013s/10 iters), loss = 6.21964
I0523 08:22:46.753926 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21964 (* 1 = 6.21964 loss)
I0523 08:22:47.380666 35003 sgd_solver.cpp:112] Iteration 183070, lr = 0.001
I0523 08:22:50.847056 35003 solver.cpp:239] Iteration 183080 (2.44323 iter/s, 4.09295s/10 iters), loss = 7.05311
I0523 08:22:50.847256 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05311 (* 1 = 7.05311 loss)
I0523 08:22:51.562350 35003 sgd_solver.cpp:112] Iteration 183080, lr = 0.001
I0523 08:22:52.969887 35003 solver.cpp:239] Iteration 183090 (4.71134 iter/s, 2.12254s/10 iters), loss = 8.07578
I0523 08:22:52.969925 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.07578 (* 1 = 8.07578 loss)
I0523 08:22:52.991683 35003 sgd_solver.cpp:112] Iteration 183090, lr = 0.001
I0523 08:22:56.095449 35003 solver.cpp:239] Iteration 183100 (3.1996 iter/s, 3.12539s/10 iters), loss = 6.58722
I0523 08:22:56.095486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58722 (* 1 = 6.58722 loss)
I0523 08:22:56.108418 35003 sgd_solver.cpp:112] Iteration 183100, lr = 0.001
I0523 08:23:00.626551 35003 solver.cpp:239] Iteration 183110 (2.20708 iter/s, 4.53088s/10 iters), loss = 6.29473
I0523 08:23:00.626596 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29473 (* 1 = 6.29473 loss)
I0523 08:23:00.659842 35003 sgd_solver.cpp:112] Iteration 183110, lr = 0.001
I0523 08:23:03.700317 35003 solver.cpp:239] Iteration 183120 (3.25352 iter/s, 3.07359s/10 iters), loss = 5.96636
I0523 08:23:03.700372 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96636 (* 1 = 5.96636 loss)
I0523 08:23:03.713834 35003 sgd_solver.cpp:112] Iteration 183120, lr = 0.001
I0523 08:23:07.647132 35003 solver.cpp:239] Iteration 183130 (2.53383 iter/s, 3.94659s/10 iters), loss = 7.86794
I0523 08:23:07.647186 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.86794 (* 1 = 7.86794 loss)
I0523 08:23:08.381295 35003 sgd_solver.cpp:112] Iteration 183130, lr = 0.001
I0523 08:23:13.563988 35003 solver.cpp:239] Iteration 183140 (1.69017 iter/s, 5.91656s/10 iters), loss = 7.0358
I0523 08:23:13.564025 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0358 (* 1 = 7.0358 loss)
I0523 08:23:13.577503 35003 sgd_solver.cpp:112] Iteration 183140, lr = 0.001
I0523 08:23:15.689326 35003 solver.cpp:239] Iteration 183150 (4.70543 iter/s, 2.1252s/10 iters), loss = 5.78455
I0523 08:23:15.689378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.78455 (* 1 = 5.78455 loss)
I0523 08:23:16.430608 35003 sgd_solver.cpp:112] Iteration 183150, lr = 0.001
I0523 08:23:19.109329 35003 solver.cpp:239] Iteration 183160 (2.92415 iter/s, 3.4198s/10 iters), loss = 6.97434
I0523 08:23:19.109390 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97434 (* 1 = 6.97434 loss)
I0523 08:23:19.115674 35003 sgd_solver.cpp:112] Iteration 183160, lr = 0.001
I0523 08:23:20.669756 35003 solver.cpp:239] Iteration 183170 (6.40903 iter/s, 1.5603s/10 iters), loss = 6.20235
I0523 08:23:20.669801 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20235 (* 1 = 6.20235 loss)
I0523 08:23:21.407454 35003 sgd_solver.cpp:112] Iteration 183170, lr = 0.001
I0523 08:23:25.054859 35003 solver.cpp:239] Iteration 183180 (2.28057 iter/s, 4.38487s/10 iters), loss = 6.63643
I0523 08:23:25.054919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63643 (* 1 = 6.63643 loss)
I0523 08:23:25.795686 35003 sgd_solver.cpp:112] Iteration 183180, lr = 0.001
I0523 08:23:28.374372 35003 solver.cpp:239] Iteration 183190 (3.01267 iter/s, 3.31931s/10 iters), loss = 7.98883
I0523 08:23:28.374424 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.98883 (* 1 = 7.98883 loss)
I0523 08:23:28.390518 35003 sgd_solver.cpp:112] Iteration 183190, lr = 0.001
I0523 08:23:32.552141 35003 solver.cpp:239] Iteration 183200 (2.39375 iter/s, 4.17755s/10 iters), loss = 8.16164
I0523 08:23:32.552181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.16164 (* 1 = 8.16164 loss)
I0523 08:23:32.570281 35003 sgd_solver.cpp:112] Iteration 183200, lr = 0.001
I0523 08:23:36.973707 35003 solver.cpp:239] Iteration 183210 (2.26176 iter/s, 4.42134s/10 iters), loss = 6.35716
I0523 08:23:36.973765 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35716 (* 1 = 6.35716 loss)
I0523 08:23:37.674320 35003 sgd_solver.cpp:112] Iteration 183210, lr = 0.001
I0523 08:23:40.835340 35003 solver.cpp:239] Iteration 183220 (2.58972 iter/s, 3.86142s/10 iters), loss = 5.17305
I0523 08:23:40.835377 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.17305 (* 1 = 5.17305 loss)
I0523 08:23:40.848420 35003 sgd_solver.cpp:112] Iteration 183220, lr = 0.001
I0523 08:23:43.716240 35003 solver.cpp:239] Iteration 183230 (3.47134 iter/s, 2.88073s/10 iters), loss = 8.35917
I0523 08:23:43.716315 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.35917 (* 1 = 8.35917 loss)
I0523 08:23:44.456987 35003 sgd_solver.cpp:112] Iteration 183230, lr = 0.001
I0523 08:23:48.312225 35003 solver.cpp:239] Iteration 183240 (2.17594 iter/s, 4.59572s/10 iters), loss = 5.54846
I0523 08:23:48.312284 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.54846 (* 1 = 5.54846 loss)
I0523 08:23:49.046612 35003 sgd_solver.cpp:112] Iteration 183240, lr = 0.001
I0523 08:23:53.305991 35003 solver.cpp:239] Iteration 183250 (2.0026 iter/s, 4.99351s/10 iters), loss = 7.33923
I0523 08:23:53.306222 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33923 (* 1 = 7.33923 loss)
I0523 08:23:53.324054 35003 sgd_solver.cpp:112] Iteration 183250, lr = 0.001
I0523 08:23:57.191861 35003 solver.cpp:239] Iteration 183260 (2.57367 iter/s, 3.8855s/10 iters), loss = 8.68739
I0523 08:23:57.191907 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.68739 (* 1 = 8.68739 loss)
I0523 08:23:57.208498 35003 sgd_solver.cpp:112] Iteration 183260, lr = 0.001
I0523 08:24:00.881590 35003 solver.cpp:239] Iteration 183270 (2.71038 iter/s, 3.68952s/10 iters), loss = 7.04381
I0523 08:24:00.881633 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04381 (* 1 = 7.04381 loss)
I0523 08:24:00.903223 35003 sgd_solver.cpp:112] Iteration 183270, lr = 0.001
I0523 08:24:02.852043 35003 solver.cpp:239] Iteration 183280 (5.07532 iter/s, 1.97032s/10 iters), loss = 6.74564
I0523 08:24:02.852097 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74564 (* 1 = 6.74564 loss)
I0523 08:24:03.592945 35003 sgd_solver.cpp:112] Iteration 183280, lr = 0.001
I0523 08:24:06.969619 35003 solver.cpp:239] Iteration 183290 (2.42874 iter/s, 4.11735s/10 iters), loss = 6.38451
I0523 08:24:06.969666 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38451 (* 1 = 6.38451 loss)
I0523 08:24:07.665591 35003 sgd_solver.cpp:112] Iteration 183290, lr = 0.001
I0523 08:24:10.863293 35003 solver.cpp:239] Iteration 183300 (2.5684 iter/s, 3.89347s/10 iters), loss = 6.23665
I0523 08:24:10.863335 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23665 (* 1 = 6.23665 loss)
I0523 08:24:10.869868 35003 sgd_solver.cpp:112] Iteration 183300, lr = 0.001
I0523 08:24:14.392855 35003 solver.cpp:239] Iteration 183310 (2.83336 iter/s, 3.52938s/10 iters), loss = 6.95672
I0523 08:24:14.392900 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95672 (* 1 = 6.95672 loss)
I0523 08:24:14.434227 35003 sgd_solver.cpp:112] Iteration 183310, lr = 0.001
I0523 08:24:16.502790 35003 solver.cpp:239] Iteration 183320 (4.73981 iter/s, 2.10979s/10 iters), loss = 7.37172
I0523 08:24:16.502841 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37172 (* 1 = 7.37172 loss)
I0523 08:24:16.516458 35003 sgd_solver.cpp:112] Iteration 183320, lr = 0.001
I0523 08:24:21.099206 35003 solver.cpp:239] Iteration 183330 (2.17573 iter/s, 4.59616s/10 iters), loss = 6.20135
I0523 08:24:21.099261 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20135 (* 1 = 6.20135 loss)
I0523 08:24:21.160022 35003 sgd_solver.cpp:112] Iteration 183330, lr = 0.001
I0523 08:24:25.463167 35003 solver.cpp:239] Iteration 183340 (2.29163 iter/s, 4.36372s/10 iters), loss = 6.10171
I0523 08:24:25.463407 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10171 (* 1 = 6.10171 loss)
I0523 08:24:25.474874 35003 sgd_solver.cpp:112] Iteration 183340, lr = 0.001
I0523 08:24:28.970222 35003 solver.cpp:239] Iteration 183350 (2.8517 iter/s, 3.50668s/10 iters), loss = 6.07239
I0523 08:24:28.970275 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07239 (* 1 = 6.07239 loss)
I0523 08:24:28.976749 35003 sgd_solver.cpp:112] Iteration 183350, lr = 0.001
I0523 08:24:32.574544 35003 solver.cpp:239] Iteration 183360 (2.77461 iter/s, 3.60411s/10 iters), loss = 6.83434
I0523 08:24:32.574599 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83434 (* 1 = 6.83434 loss)
I0523 08:24:32.577581 35003 sgd_solver.cpp:112] Iteration 183360, lr = 0.001
I0523 08:24:34.494267 35003 solver.cpp:239] Iteration 183370 (5.20946 iter/s, 1.91958s/10 iters), loss = 7.77194
I0523 08:24:34.494316 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.77194 (* 1 = 7.77194 loss)
I0523 08:24:34.503684 35003 sgd_solver.cpp:112] Iteration 183370, lr = 0.001
I0523 08:24:37.901705 35003 solver.cpp:239] Iteration 183380 (2.93493 iter/s, 3.40724s/10 iters), loss = 6.04582
I0523 08:24:37.901756 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04582 (* 1 = 6.04582 loss)
I0523 08:24:37.907145 35003 sgd_solver.cpp:112] Iteration 183380, lr = 0.001
I0523 08:24:42.088888 35003 solver.cpp:239] Iteration 183390 (2.38837 iter/s, 4.18695s/10 iters), loss = 7.23869
I0523 08:24:42.088929 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23869 (* 1 = 7.23869 loss)
I0523 08:24:42.107167 35003 sgd_solver.cpp:112] Iteration 183390, lr = 0.001
I0523 08:24:46.164866 35003 solver.cpp:239] Iteration 183400 (2.45353 iter/s, 4.07575s/10 iters), loss = 7.69669
I0523 08:24:46.164914 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69669 (* 1 = 7.69669 loss)
I0523 08:24:46.905567 35003 sgd_solver.cpp:112] Iteration 183400, lr = 0.001
I0523 08:24:50.087468 35003 solver.cpp:239] Iteration 183410 (2.54947 iter/s, 3.92239s/10 iters), loss = 5.87578
I0523 08:24:50.087519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87578 (* 1 = 5.87578 loss)
I0523 08:24:50.090732 35003 sgd_solver.cpp:112] Iteration 183410, lr = 0.001
I0523 08:24:52.840715 35003 solver.cpp:239] Iteration 183420 (3.63229 iter/s, 2.75308s/10 iters), loss = 5.97935
I0523 08:24:52.840767 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97935 (* 1 = 5.97935 loss)
I0523 08:24:52.879525 35003 sgd_solver.cpp:112] Iteration 183420, lr = 0.001
I0523 08:24:55.663908 35003 solver.cpp:239] Iteration 183430 (3.54231 iter/s, 2.82302s/10 iters), loss = 6.41405
I0523 08:24:55.664139 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41405 (* 1 = 6.41405 loss)
I0523 08:24:56.405341 35003 sgd_solver.cpp:112] Iteration 183430, lr = 0.001
I0523 08:25:00.066871 35003 solver.cpp:239] Iteration 183440 (2.2714 iter/s, 4.40257s/10 iters), loss = 6.40607
I0523 08:25:00.066925 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40607 (* 1 = 6.40607 loss)
I0523 08:25:00.797713 35003 sgd_solver.cpp:112] Iteration 183440, lr = 0.001
I0523 08:25:02.889008 35003 solver.cpp:239] Iteration 183450 (3.54364 iter/s, 2.82196s/10 iters), loss = 5.74613
I0523 08:25:02.889071 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74613 (* 1 = 5.74613 loss)
I0523 08:25:03.629799 35003 sgd_solver.cpp:112] Iteration 183450, lr = 0.001
I0523 08:25:08.057525 35003 solver.cpp:239] Iteration 183460 (1.9349 iter/s, 5.16823s/10 iters), loss = 8.05834
I0523 08:25:08.057592 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.05834 (* 1 = 8.05834 loss)
I0523 08:25:08.200647 35003 sgd_solver.cpp:112] Iteration 183460, lr = 0.001
I0523 08:25:12.401104 35003 solver.cpp:239] Iteration 183470 (2.30238 iter/s, 4.34333s/10 iters), loss = 7.40116
I0523 08:25:12.401154 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.40116 (* 1 = 7.40116 loss)
I0523 08:25:12.535802 35003 sgd_solver.cpp:112] Iteration 183470, lr = 0.001
I0523 08:25:16.389976 35003 solver.cpp:239] Iteration 183480 (2.50711 iter/s, 3.98866s/10 iters), loss = 6.66013
I0523 08:25:16.390015 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66013 (* 1 = 6.66013 loss)
I0523 08:25:16.964706 35003 sgd_solver.cpp:112] Iteration 183480, lr = 0.001
I0523 08:25:20.108281 35003 solver.cpp:239] Iteration 183490 (2.68954 iter/s, 3.71811s/10 iters), loss = 7.15874
I0523 08:25:20.108325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15874 (* 1 = 7.15874 loss)
I0523 08:25:20.127022 35003 sgd_solver.cpp:112] Iteration 183490, lr = 0.001
I0523 08:25:25.842396 35003 solver.cpp:239] Iteration 183500 (1.74403 iter/s, 5.73384s/10 iters), loss = 7.01213
I0523 08:25:25.842664 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01213 (* 1 = 7.01213 loss)
I0523 08:25:26.557886 35003 sgd_solver.cpp:112] Iteration 183500, lr = 0.001
I0523 08:25:30.295250 35003 solver.cpp:239] Iteration 183510 (2.24596 iter/s, 4.45243s/10 iters), loss = 6.42156
I0523 08:25:30.295294 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42156 (* 1 = 6.42156 loss)
I0523 08:25:30.939347 35003 sgd_solver.cpp:112] Iteration 183510, lr = 0.001
I0523 08:25:33.868757 35003 solver.cpp:239] Iteration 183520 (2.79852 iter/s, 3.57331s/10 iters), loss = 7.61542
I0523 08:25:33.868800 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61542 (* 1 = 7.61542 loss)
I0523 08:25:34.552316 35003 sgd_solver.cpp:112] Iteration 183520, lr = 0.001
I0523 08:25:38.194437 35003 solver.cpp:239] Iteration 183530 (2.31189 iter/s, 4.32546s/10 iters), loss = 6.69381
I0523 08:25:38.194486 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69381 (* 1 = 6.69381 loss)
I0523 08:25:38.199362 35003 sgd_solver.cpp:112] Iteration 183530, lr = 0.001
I0523 08:25:41.066138 35003 solver.cpp:239] Iteration 183540 (3.48246 iter/s, 2.87153s/10 iters), loss = 6.81235
I0523 08:25:41.066179 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81235 (* 1 = 6.81235 loss)
I0523 08:25:41.084411 35003 sgd_solver.cpp:112] Iteration 183540, lr = 0.001
I0523 08:25:43.796458 35003 solver.cpp:239] Iteration 183550 (3.6628 iter/s, 2.73015s/10 iters), loss = 7.16885
I0523 08:25:43.796509 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16885 (* 1 = 7.16885 loss)
I0523 08:25:44.537267 35003 sgd_solver.cpp:112] Iteration 183550, lr = 0.001
I0523 08:25:49.752367 35003 solver.cpp:239] Iteration 183560 (1.67909 iter/s, 5.95562s/10 iters), loss = 6.30133
I0523 08:25:49.752409 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30133 (* 1 = 6.30133 loss)
I0523 08:25:49.765450 35003 sgd_solver.cpp:112] Iteration 183560, lr = 0.001
I0523 08:25:51.852250 35003 solver.cpp:239] Iteration 183570 (4.76249 iter/s, 2.09974s/10 iters), loss = 6.32393
I0523 08:25:51.852304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32393 (* 1 = 6.32393 loss)
I0523 08:25:52.593022 35003 sgd_solver.cpp:112] Iteration 183570, lr = 0.001
I0523 08:25:55.485841 35003 solver.cpp:239] Iteration 183580 (2.75225 iter/s, 3.63338s/10 iters), loss = 5.81787
I0523 08:25:55.485894 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.81787 (* 1 = 5.81787 loss)
I0523 08:25:55.498111 35003 sgd_solver.cpp:112] Iteration 183580, lr = 0.001
I0523 08:25:57.549793 35003 solver.cpp:239] Iteration 183590 (4.84542 iter/s, 2.0638s/10 iters), loss = 6.19813
I0523 08:25:57.550118 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19813 (* 1 = 6.19813 loss)
I0523 08:25:57.561954 35003 sgd_solver.cpp:112] Iteration 183590, lr = 0.001
I0523 08:26:00.786594 35003 solver.cpp:239] Iteration 183600 (3.08988 iter/s, 3.23637s/10 iters), loss = 7.06495
I0523 08:26:00.786648 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06495 (* 1 = 7.06495 loss)
I0523 08:26:00.888548 35003 sgd_solver.cpp:112] Iteration 183600, lr = 0.001
I0523 08:26:05.374171 35003 solver.cpp:239] Iteration 183610 (2.17992 iter/s, 4.58733s/10 iters), loss = 6.48427
I0523 08:26:05.374215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48427 (* 1 = 6.48427 loss)
I0523 08:26:05.386914 35003 sgd_solver.cpp:112] Iteration 183610, lr = 0.001
I0523 08:26:09.057723 35003 solver.cpp:239] Iteration 183620 (2.71492 iter/s, 3.68335s/10 iters), loss = 5.82021
I0523 08:26:09.057765 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.82021 (* 1 = 5.82021 loss)
I0523 08:26:09.765859 35003 sgd_solver.cpp:112] Iteration 183620, lr = 0.001
I0523 08:26:12.101461 35003 solver.cpp:239] Iteration 183630 (3.28562 iter/s, 3.04357s/10 iters), loss = 6.91139
I0523 08:26:12.101505 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91139 (* 1 = 6.91139 loss)
I0523 08:26:12.773489 35003 sgd_solver.cpp:112] Iteration 183630, lr = 0.001
I0523 08:26:14.896361 35003 solver.cpp:239] Iteration 183640 (3.57816 iter/s, 2.79473s/10 iters), loss = 6.39081
I0523 08:26:14.896420 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39081 (* 1 = 6.39081 loss)
I0523 08:26:15.520227 35003 sgd_solver.cpp:112] Iteration 183640, lr = 0.001
I0523 08:26:17.683187 35003 solver.cpp:239] Iteration 183650 (3.58854 iter/s, 2.78665s/10 iters), loss = 5.76856
I0523 08:26:17.683233 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.76856 (* 1 = 5.76856 loss)
I0523 08:26:17.696429 35003 sgd_solver.cpp:112] Iteration 183650, lr = 0.001
I0523 08:26:21.414969 35003 solver.cpp:239] Iteration 183660 (2.67983 iter/s, 3.73158s/10 iters), loss = 6.94108
I0523 08:26:21.415021 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94108 (* 1 = 6.94108 loss)
I0523 08:26:22.150029 35003 sgd_solver.cpp:112] Iteration 183660, lr = 0.001
I0523 08:26:26.628696 35003 solver.cpp:239] Iteration 183670 (1.91811 iter/s, 5.21346s/10 iters), loss = 6.10689
I0523 08:26:26.628756 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10689 (* 1 = 6.10689 loss)
I0523 08:26:26.641891 35003 sgd_solver.cpp:112] Iteration 183670, lr = 0.001
I0523 08:26:28.717828 35003 solver.cpp:239] Iteration 183680 (4.78702 iter/s, 2.08898s/10 iters), loss = 5.4507
I0523 08:26:28.718062 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.4507 (* 1 = 5.4507 loss)
I0523 08:26:28.734879 35003 sgd_solver.cpp:112] Iteration 183680, lr = 0.001
I0523 08:26:33.378737 35003 solver.cpp:239] Iteration 183690 (2.1457 iter/s, 4.66048s/10 iters), loss = 6.1437
I0523 08:26:33.378779 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1437 (* 1 = 6.1437 loss)
I0523 08:26:33.383054 35003 sgd_solver.cpp:112] Iteration 183690, lr = 0.001
I0523 08:26:36.610565 35003 solver.cpp:239] Iteration 183700 (3.09439 iter/s, 3.23165s/10 iters), loss = 7.64247
I0523 08:26:36.610605 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64247 (* 1 = 7.64247 loss)
I0523 08:26:36.613193 35003 sgd_solver.cpp:112] Iteration 183700, lr = 0.001
I0523 08:26:41.804327 35003 solver.cpp:239] Iteration 183710 (1.92549 iter/s, 5.19349s/10 iters), loss = 6.55248
I0523 08:26:41.804368 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55248 (* 1 = 6.55248 loss)
I0523 08:26:41.822409 35003 sgd_solver.cpp:112] Iteration 183710, lr = 0.001
I0523 08:26:43.457667 35003 solver.cpp:239] Iteration 183720 (6.04881 iter/s, 1.65322s/10 iters), loss = 7.00271
I0523 08:26:43.457723 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00271 (* 1 = 7.00271 loss)
I0523 08:26:44.192191 35003 sgd_solver.cpp:112] Iteration 183720, lr = 0.001
I0523 08:26:46.720242 35003 solver.cpp:239] Iteration 183730 (3.06524 iter/s, 3.26239s/10 iters), loss = 6.02086
I0523 08:26:46.720281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02086 (* 1 = 6.02086 loss)
I0523 08:26:46.742632 35003 sgd_solver.cpp:112] Iteration 183730, lr = 0.001
I0523 08:26:48.841642 35003 solver.cpp:239] Iteration 183740 (4.71417 iter/s, 2.12127s/10 iters), loss = 7.36288
I0523 08:26:48.841689 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36288 (* 1 = 7.36288 loss)
I0523 08:26:48.855168 35003 sgd_solver.cpp:112] Iteration 183740, lr = 0.001
I0523 08:26:51.829408 35003 solver.cpp:239] Iteration 183750 (3.34718 iter/s, 2.98759s/10 iters), loss = 7.3469
I0523 08:26:51.829447 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3469 (* 1 = 7.3469 loss)
I0523 08:26:51.847066 35003 sgd_solver.cpp:112] Iteration 183750, lr = 0.001
I0523 08:26:57.549422 35003 solver.cpp:239] Iteration 183760 (1.74833 iter/s, 5.71973s/10 iters), loss = 7.57981
I0523 08:26:57.549564 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57981 (* 1 = 7.57981 loss)
I0523 08:26:57.561956 35003 sgd_solver.cpp:112] Iteration 183760, lr = 0.001
I0523 08:27:00.483209 35003 solver.cpp:239] Iteration 183770 (3.40885 iter/s, 2.93354s/10 iters), loss = 6.08388
I0523 08:27:00.483450 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08388 (* 1 = 6.08388 loss)
I0523 08:27:00.502300 35003 sgd_solver.cpp:112] Iteration 183770, lr = 0.001
I0523 08:27:03.932157 35003 solver.cpp:239] Iteration 183780 (2.89973 iter/s, 3.44859s/10 iters), loss = 6.05432
I0523 08:27:03.932212 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05432 (* 1 = 6.05432 loss)
I0523 08:27:03.938648 35003 sgd_solver.cpp:112] Iteration 183780, lr = 0.001
I0523 08:27:06.021494 35003 solver.cpp:239] Iteration 183790 (4.78655 iter/s, 2.08919s/10 iters), loss = 6.25439
I0523 08:27:06.021538 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25439 (* 1 = 6.25439 loss)
I0523 08:27:06.731822 35003 sgd_solver.cpp:112] Iteration 183790, lr = 0.001
I0523 08:27:08.881355 35003 solver.cpp:239] Iteration 183800 (3.49688 iter/s, 2.85969s/10 iters), loss = 6.38666
I0523 08:27:08.881402 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38666 (* 1 = 6.38666 loss)
I0523 08:27:09.503326 35003 sgd_solver.cpp:112] Iteration 183800, lr = 0.001
I0523 08:27:13.606180 35003 solver.cpp:239] Iteration 183810 (2.11659 iter/s, 4.72458s/10 iters), loss = 7.74534
I0523 08:27:13.606230 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.74534 (* 1 = 7.74534 loss)
I0523 08:27:13.612959 35003 sgd_solver.cpp:112] Iteration 183810, lr = 0.001
I0523 08:27:16.488333 35003 solver.cpp:239] Iteration 183820 (3.46984 iter/s, 2.88198s/10 iters), loss = 5.9428
I0523 08:27:16.488386 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9428 (* 1 = 5.9428 loss)
I0523 08:27:16.497417 35003 sgd_solver.cpp:112] Iteration 183820, lr = 0.001
I0523 08:27:18.139649 35003 solver.cpp:239] Iteration 183830 (6.05626 iter/s, 1.65118s/10 iters), loss = 5.85025
I0523 08:27:18.139708 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85025 (* 1 = 5.85025 loss)
I0523 08:27:18.150844 35003 sgd_solver.cpp:112] Iteration 183830, lr = 0.001
I0523 08:27:20.889827 35003 solver.cpp:239] Iteration 183840 (3.63636 iter/s, 2.75s/10 iters), loss = 5.68144
I0523 08:27:20.889869 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.68144 (* 1 = 5.68144 loss)
I0523 08:27:20.903669 35003 sgd_solver.cpp:112] Iteration 183840, lr = 0.001
I0523 08:27:23.695178 35003 solver.cpp:239] Iteration 183850 (3.56483 iter/s, 2.80519s/10 iters), loss = 6.81861
I0523 08:27:23.695225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81861 (* 1 = 6.81861 loss)
I0523 08:27:23.702589 35003 sgd_solver.cpp:112] Iteration 183850, lr = 0.001
I0523 08:27:28.835758 35003 solver.cpp:239] Iteration 183860 (1.9454 iter/s, 5.14032s/10 iters), loss = 7.23014
I0523 08:27:28.835811 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23014 (* 1 = 7.23014 loss)
I0523 08:27:29.557238 35003 sgd_solver.cpp:112] Iteration 183860, lr = 0.001
I0523 08:27:32.372032 35003 solver.cpp:239] Iteration 183870 (2.82799 iter/s, 3.53608s/10 iters), loss = 5.2507
I0523 08:27:32.372377 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.2507 (* 1 = 5.2507 loss)
I0523 08:27:32.386051 35003 sgd_solver.cpp:112] Iteration 183870, lr = 0.001
I0523 08:27:34.979347 35003 solver.cpp:239] Iteration 183880 (3.83599 iter/s, 2.60689s/10 iters), loss = 5.35519
I0523 08:27:34.979393 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.35519 (* 1 = 5.35519 loss)
I0523 08:27:34.987129 35003 sgd_solver.cpp:112] Iteration 183880, lr = 0.001
I0523 08:27:38.680058 35003 solver.cpp:239] Iteration 183890 (2.70233 iter/s, 3.70051s/10 iters), loss = 7.23277
I0523 08:27:38.680104 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.23277 (* 1 = 7.23277 loss)
I0523 08:27:39.415395 35003 sgd_solver.cpp:112] Iteration 183890, lr = 0.001
I0523 08:27:42.783141 35003 solver.cpp:239] Iteration 183900 (2.43732 iter/s, 4.10286s/10 iters), loss = 6.75304
I0523 08:27:42.783191 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75304 (* 1 = 6.75304 loss)
I0523 08:27:43.523918 35003 sgd_solver.cpp:112] Iteration 183900, lr = 0.001
I0523 08:27:47.764087 35003 solver.cpp:239] Iteration 183910 (2.00775 iter/s, 4.98069s/10 iters), loss = 6.66356
I0523 08:27:47.764125 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66356 (* 1 = 6.66356 loss)
I0523 08:27:47.776795 35003 sgd_solver.cpp:112] Iteration 183910, lr = 0.001
I0523 08:27:50.424216 35003 solver.cpp:239] Iteration 183920 (3.75944 iter/s, 2.65997s/10 iters), loss = 5.3416
I0523 08:27:50.424283 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.3416 (* 1 = 5.3416 loss)
I0523 08:27:50.434805 35003 sgd_solver.cpp:112] Iteration 183920, lr = 0.001
I0523 08:27:53.086928 35003 solver.cpp:239] Iteration 183930 (3.75581 iter/s, 2.66254s/10 iters), loss = 6.21927
I0523 08:27:53.086979 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21927 (* 1 = 6.21927 loss)
I0523 08:27:53.108971 35003 sgd_solver.cpp:112] Iteration 183930, lr = 0.001
I0523 08:27:56.579056 35003 solver.cpp:239] Iteration 183940 (2.86374 iter/s, 3.49194s/10 iters), loss = 6.03352
I0523 08:27:56.579097 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03352 (* 1 = 6.03352 loss)
I0523 08:27:56.591954 35003 sgd_solver.cpp:112] Iteration 183940, lr = 0.001
I0523 08:27:58.997057 35003 solver.cpp:239] Iteration 183950 (4.13589 iter/s, 2.41786s/10 iters), loss = 6.44494
I0523 08:27:58.997107 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44494 (* 1 = 6.44494 loss)
I0523 08:27:59.712433 35003 sgd_solver.cpp:112] Iteration 183950, lr = 0.001
I0523 08:28:02.529273 35003 solver.cpp:239] Iteration 183960 (2.83124 iter/s, 3.53202s/10 iters), loss = 6.95329
I0523 08:28:02.529368 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95329 (* 1 = 6.95329 loss)
I0523 08:28:02.551478 35003 sgd_solver.cpp:112] Iteration 183960, lr = 0.001
I0523 08:28:06.095090 35003 solver.cpp:239] Iteration 183970 (2.8046 iter/s, 3.56556s/10 iters), loss = 5.89149
I0523 08:28:06.095140 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89149 (* 1 = 5.89149 loss)
I0523 08:28:06.101454 35003 sgd_solver.cpp:112] Iteration 183970, lr = 0.001
I0523 08:28:08.961421 35003 solver.cpp:239] Iteration 183980 (3.489 iter/s, 2.86615s/10 iters), loss = 6.52336
I0523 08:28:08.961467 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52336 (* 1 = 6.52336 loss)
I0523 08:28:08.968019 35003 sgd_solver.cpp:112] Iteration 183980, lr = 0.001
I0523 08:28:11.069788 35003 solver.cpp:239] Iteration 183990 (4.74334 iter/s, 2.10822s/10 iters), loss = 6.47837
I0523 08:28:11.069869 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47837 (* 1 = 6.47837 loss)
I0523 08:28:11.074756 35003 sgd_solver.cpp:112] Iteration 183990, lr = 0.001
I0523 08:28:13.099598 35003 solver.cpp:239] Iteration 184000 (4.92697 iter/s, 2.02964s/10 iters), loss = 7.05932
I0523 08:28:13.099637 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.05932 (* 1 = 7.05932 loss)
I0523 08:28:13.117861 35003 sgd_solver.cpp:112] Iteration 184000, lr = 0.001
I0523 08:28:17.608559 35003 solver.cpp:239] Iteration 184010 (2.21792 iter/s, 4.50873s/10 iters), loss = 6.97805
I0523 08:28:17.608600 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97805 (* 1 = 6.97805 loss)
I0523 08:28:17.616724 35003 sgd_solver.cpp:112] Iteration 184010, lr = 0.001
I0523 08:28:21.865350 35003 solver.cpp:239] Iteration 184020 (2.34931 iter/s, 4.25657s/10 iters), loss = 6.66819
I0523 08:28:21.865402 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66819 (* 1 = 6.66819 loss)
I0523 08:28:21.878600 35003 sgd_solver.cpp:112] Iteration 184020, lr = 0.001
I0523 08:28:25.913985 35003 solver.cpp:239] Iteration 184030 (2.4701 iter/s, 4.04842s/10 iters), loss = 7.30618
I0523 08:28:25.914027 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30618 (* 1 = 7.30618 loss)
I0523 08:28:26.033951 35003 sgd_solver.cpp:112] Iteration 184030, lr = 0.001
I0523 08:28:28.918153 35003 solver.cpp:239] Iteration 184040 (3.3289 iter/s, 3.00399s/10 iters), loss = 6.44745
I0523 08:28:28.918195 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44745 (* 1 = 6.44745 loss)
I0523 08:28:29.295213 35003 sgd_solver.cpp:112] Iteration 184040, lr = 0.001
I0523 08:28:32.249887 35003 solver.cpp:239] Iteration 184050 (3.0016 iter/s, 3.33155s/10 iters), loss = 5.74142
I0523 08:28:32.249943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74142 (* 1 = 5.74142 loss)
I0523 08:28:32.990629 35003 sgd_solver.cpp:112] Iteration 184050, lr = 0.001
I0523 08:28:36.661793 35003 solver.cpp:239] Iteration 184060 (2.26672 iter/s, 4.41167s/10 iters), loss = 7.09267
I0523 08:28:36.661840 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09267 (* 1 = 7.09267 loss)
I0523 08:28:36.679967 35003 sgd_solver.cpp:112] Iteration 184060, lr = 0.001
I0523 08:28:40.215922 35003 solver.cpp:239] Iteration 184070 (2.81379 iter/s, 3.55393s/10 iters), loss = 7.47129
I0523 08:28:40.215970 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47129 (* 1 = 7.47129 loss)
I0523 08:28:40.237866 35003 sgd_solver.cpp:112] Iteration 184070, lr = 0.001
I0523 08:28:44.545097 35003 solver.cpp:239] Iteration 184080 (2.31003 iter/s, 4.32895s/10 iters), loss = 7.57946
I0523 08:28:44.545138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57946 (* 1 = 7.57946 loss)
I0523 08:28:44.552166 35003 sgd_solver.cpp:112] Iteration 184080, lr = 0.001
I0523 08:28:46.654254 35003 solver.cpp:239] Iteration 184090 (4.74157 iter/s, 2.10901s/10 iters), loss = 6.33804
I0523 08:28:46.654307 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33804 (* 1 = 6.33804 loss)
I0523 08:28:46.662372 35003 sgd_solver.cpp:112] Iteration 184090, lr = 0.001
I0523 08:28:52.092387 35003 solver.cpp:239] Iteration 184100 (1.83896 iter/s, 5.43786s/10 iters), loss = 5.63753
I0523 08:28:52.092439 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.63753 (* 1 = 5.63753 loss)
I0523 08:28:52.235512 35003 sgd_solver.cpp:112] Iteration 184100, lr = 0.001
I0523 08:28:55.352128 35003 solver.cpp:239] Iteration 184110 (3.06791 iter/s, 3.25955s/10 iters), loss = 7.45525
I0523 08:28:55.352174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45525 (* 1 = 7.45525 loss)
I0523 08:28:56.066926 35003 sgd_solver.cpp:112] Iteration 184110, lr = 0.001
I0523 08:29:00.025184 35003 solver.cpp:239] Iteration 184120 (2.14004 iter/s, 4.67281s/10 iters), loss = 7.32168
I0523 08:29:00.025240 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32168 (* 1 = 7.32168 loss)
I0523 08:29:00.740597 35003 sgd_solver.cpp:112] Iteration 184120, lr = 0.001
I0523 08:29:02.861937 35003 solver.cpp:239] Iteration 184130 (3.52542 iter/s, 2.83654s/10 iters), loss = 8.02884
I0523 08:29:02.861999 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.02884 (* 1 = 8.02884 loss)
I0523 08:29:02.881968 35003 sgd_solver.cpp:112] Iteration 184130, lr = 0.001
I0523 08:29:06.169075 35003 solver.cpp:239] Iteration 184140 (3.02395 iter/s, 3.30694s/10 iters), loss = 5.61539
I0523 08:29:06.169397 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.61539 (* 1 = 5.61539 loss)
I0523 08:29:06.172663 35003 sgd_solver.cpp:112] Iteration 184140, lr = 0.001
I0523 08:29:09.952188 35003 solver.cpp:239] Iteration 184150 (2.64365 iter/s, 3.78265s/10 iters), loss = 6.56608
I0523 08:29:09.952244 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56608 (* 1 = 6.56608 loss)
I0523 08:29:10.693251 35003 sgd_solver.cpp:112] Iteration 184150, lr = 0.001
I0523 08:29:14.947027 35003 solver.cpp:239] Iteration 184160 (2.00218 iter/s, 4.99457s/10 iters), loss = 7.16754
I0523 08:29:14.947098 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.16754 (* 1 = 7.16754 loss)
I0523 08:29:14.960013 35003 sgd_solver.cpp:112] Iteration 184160, lr = 0.001
I0523 08:29:17.851590 35003 solver.cpp:239] Iteration 184170 (3.4431 iter/s, 2.90436s/10 iters), loss = 7.83859
I0523 08:29:17.851671 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.83859 (* 1 = 7.83859 loss)
I0523 08:29:17.859292 35003 sgd_solver.cpp:112] Iteration 184170, lr = 0.001
I0523 08:29:20.255455 35003 solver.cpp:239] Iteration 184180 (4.16028 iter/s, 2.40368s/10 iters), loss = 7.30852
I0523 08:29:20.255507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30852 (* 1 = 7.30852 loss)
I0523 08:29:20.989905 35003 sgd_solver.cpp:112] Iteration 184180, lr = 0.001
I0523 08:29:23.826759 35003 solver.cpp:239] Iteration 184190 (2.80025 iter/s, 3.57111s/10 iters), loss = 7.24978
I0523 08:29:23.826804 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24978 (* 1 = 7.24978 loss)
I0523 08:29:24.568022 35003 sgd_solver.cpp:112] Iteration 184190, lr = 0.001
I0523 08:29:28.109916 35003 solver.cpp:239] Iteration 184200 (2.33485 iter/s, 4.28294s/10 iters), loss = 6.62368
I0523 08:29:28.109958 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62368 (* 1 = 6.62368 loss)
I0523 08:29:28.844079 35003 sgd_solver.cpp:112] Iteration 184200, lr = 0.001
I0523 08:29:33.466986 35003 solver.cpp:239] Iteration 184210 (1.86678 iter/s, 5.35681s/10 iters), loss = 7.7775
I0523 08:29:33.467028 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.7775 (* 1 = 7.7775 loss)
I0523 08:29:33.474359 35003 sgd_solver.cpp:112] Iteration 184210, lr = 0.001
I0523 08:29:37.095530 35003 solver.cpp:239] Iteration 184220 (2.75607 iter/s, 3.62835s/10 iters), loss = 7.26525
I0523 08:29:37.095751 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26525 (* 1 = 7.26525 loss)
I0523 08:29:37.250921 35003 sgd_solver.cpp:112] Iteration 184220, lr = 0.001
I0523 08:29:40.891330 35003 solver.cpp:239] Iteration 184230 (2.63777 iter/s, 3.79108s/10 iters), loss = 6.2325
I0523 08:29:40.891378 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2325 (* 1 = 6.2325 loss)
I0523 08:29:41.600261 35003 sgd_solver.cpp:112] Iteration 184230, lr = 0.001
I0523 08:29:45.080040 35003 solver.cpp:239] Iteration 184240 (2.38749 iter/s, 4.18849s/10 iters), loss = 6.33143
I0523 08:29:45.080087 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33143 (* 1 = 6.33143 loss)
I0523 08:29:45.788007 35003 sgd_solver.cpp:112] Iteration 184240, lr = 0.001
I0523 08:29:50.214067 35003 solver.cpp:239] Iteration 184250 (1.94788 iter/s, 5.13377s/10 iters), loss = 6.43296
I0523 08:29:50.214104 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43296 (* 1 = 6.43296 loss)
I0523 08:29:50.227471 35003 sgd_solver.cpp:112] Iteration 184250, lr = 0.001
I0523 08:29:54.629364 35003 solver.cpp:239] Iteration 184260 (2.26498 iter/s, 4.41505s/10 iters), loss = 7.3989
I0523 08:29:54.629431 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3989 (* 1 = 7.3989 loss)
I0523 08:29:55.369895 35003 sgd_solver.cpp:112] Iteration 184260, lr = 0.001
I0523 08:29:58.983518 35003 solver.cpp:239] Iteration 184270 (2.29679 iter/s, 4.35391s/10 iters), loss = 5.33939
I0523 08:29:58.983570 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.33939 (* 1 = 5.33939 loss)
I0523 08:29:58.988659 35003 sgd_solver.cpp:112] Iteration 184270, lr = 0.001
I0523 08:30:03.486835 35003 solver.cpp:239] Iteration 184280 (2.2207 iter/s, 4.50308s/10 iters), loss = 5.93004
I0523 08:30:03.486893 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.93004 (* 1 = 5.93004 loss)
I0523 08:30:04.221315 35003 sgd_solver.cpp:112] Iteration 184280, lr = 0.001
I0523 08:30:07.747305 35003 solver.cpp:239] Iteration 184290 (2.34729 iter/s, 4.26023s/10 iters), loss = 6.65315
I0523 08:30:07.747602 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65315 (* 1 = 6.65315 loss)
I0523 08:30:07.757498 35003 sgd_solver.cpp:112] Iteration 184290, lr = 0.001
I0523 08:30:10.429034 35003 solver.cpp:239] Iteration 184300 (3.72947 iter/s, 2.68134s/10 iters), loss = 6.68912
I0523 08:30:10.429074 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68912 (* 1 = 6.68912 loss)
I0523 08:30:10.441967 35003 sgd_solver.cpp:112] Iteration 184300, lr = 0.001
I0523 08:30:14.426175 35003 solver.cpp:239] Iteration 184310 (2.50192 iter/s, 3.99694s/10 iters), loss = 6.56111
I0523 08:30:14.426215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56111 (* 1 = 6.56111 loss)
I0523 08:30:14.439015 35003 sgd_solver.cpp:112] Iteration 184310, lr = 0.001
I0523 08:30:18.171519 35003 solver.cpp:239] Iteration 184320 (2.67014 iter/s, 3.74513s/10 iters), loss = 6.90938
I0523 08:30:18.171571 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90938 (* 1 = 6.90938 loss)
I0523 08:30:18.866719 35003 sgd_solver.cpp:112] Iteration 184320, lr = 0.001
I0523 08:30:22.568663 35003 solver.cpp:239] Iteration 184330 (2.27432 iter/s, 4.39691s/10 iters), loss = 6.07356
I0523 08:30:22.568717 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07356 (* 1 = 6.07356 loss)
I0523 08:30:23.263757 35003 sgd_solver.cpp:112] Iteration 184330, lr = 0.001
I0523 08:30:27.667197 35003 solver.cpp:239] Iteration 184340 (1.96145 iter/s, 5.09827s/10 iters), loss = 6.38312
I0523 08:30:27.667250 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38312 (* 1 = 6.38312 loss)
I0523 08:30:28.375432 35003 sgd_solver.cpp:112] Iteration 184340, lr = 0.001
I0523 08:30:33.603220 35003 solver.cpp:239] Iteration 184350 (1.68471 iter/s, 5.93573s/10 iters), loss = 6.08912
I0523 08:30:33.603271 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08912 (* 1 = 6.08912 loss)
I0523 08:30:33.609371 35003 sgd_solver.cpp:112] Iteration 184350, lr = 0.001
I0523 08:30:37.810659 35003 solver.cpp:239] Iteration 184360 (2.37687 iter/s, 4.20721s/10 iters), loss = 6.50037
I0523 08:30:37.810863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50037 (* 1 = 6.50037 loss)
I0523 08:30:37.817297 35003 sgd_solver.cpp:112] Iteration 184360, lr = 0.001
I0523 08:30:40.945905 35003 solver.cpp:239] Iteration 184370 (3.18986 iter/s, 3.13493s/10 iters), loss = 6.83937
I0523 08:30:40.945956 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83937 (* 1 = 6.83937 loss)
I0523 08:30:40.959002 35003 sgd_solver.cpp:112] Iteration 184370, lr = 0.001
I0523 08:30:44.501230 35003 solver.cpp:239] Iteration 184380 (2.81284 iter/s, 3.55512s/10 iters), loss = 6.8808
I0523 08:30:44.501283 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8808 (* 1 = 6.8808 loss)
I0523 08:30:44.538470 35003 sgd_solver.cpp:112] Iteration 184380, lr = 0.001
I0523 08:30:46.560104 35003 solver.cpp:239] Iteration 184390 (4.85737 iter/s, 2.05873s/10 iters), loss = 7.2028
I0523 08:30:46.560149 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2028 (* 1 = 7.2028 loss)
I0523 08:30:46.578606 35003 sgd_solver.cpp:112] Iteration 184390, lr = 0.001
I0523 08:30:49.982056 35003 solver.cpp:239] Iteration 184400 (2.92247 iter/s, 3.42177s/10 iters), loss = 7.41987
I0523 08:30:49.982096 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41987 (* 1 = 7.41987 loss)
I0523 08:30:50.027629 35003 sgd_solver.cpp:112] Iteration 184400, lr = 0.001
I0523 08:30:54.157579 35003 solver.cpp:239] Iteration 184410 (2.39504 iter/s, 4.1753s/10 iters), loss = 6.05187
I0523 08:30:54.157626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05187 (* 1 = 6.05187 loss)
I0523 08:30:54.886111 35003 sgd_solver.cpp:112] Iteration 184410, lr = 0.001
I0523 08:30:57.013111 35003 solver.cpp:239] Iteration 184420 (3.50219 iter/s, 2.85536s/10 iters), loss = 6.64311
I0523 08:30:57.013165 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64311 (* 1 = 6.64311 loss)
I0523 08:30:57.723521 35003 sgd_solver.cpp:112] Iteration 184420, lr = 0.001
I0523 08:31:01.163257 35003 solver.cpp:239] Iteration 184430 (2.40968 iter/s, 4.14992s/10 iters), loss = 5.87353
I0523 08:31:01.163298 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87353 (* 1 = 5.87353 loss)
I0523 08:31:01.175318 35003 sgd_solver.cpp:112] Iteration 184430, lr = 0.001
I0523 08:31:03.701648 35003 solver.cpp:239] Iteration 184440 (3.93974 iter/s, 2.53824s/10 iters), loss = 7.30911
I0523 08:31:03.701686 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30911 (* 1 = 7.30911 loss)
I0523 08:31:03.714568 35003 sgd_solver.cpp:112] Iteration 184440, lr = 0.001
I0523 08:31:07.642742 35003 solver.cpp:239] Iteration 184450 (2.5375 iter/s, 3.94089s/10 iters), loss = 6.9148
I0523 08:31:07.642798 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9148 (* 1 = 6.9148 loss)
I0523 08:31:07.930274 35003 sgd_solver.cpp:112] Iteration 184450, lr = 0.001
I0523 08:31:10.724514 35003 solver.cpp:239] Iteration 184460 (3.24508 iter/s, 3.08159s/10 iters), loss = 7.18422
I0523 08:31:10.724553 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18422 (* 1 = 7.18422 loss)
I0523 08:31:11.413786 35003 sgd_solver.cpp:112] Iteration 184460, lr = 0.001
I0523 08:31:15.818719 35003 solver.cpp:239] Iteration 184470 (1.96312 iter/s, 5.09394s/10 iters), loss = 5.35298
I0523 08:31:15.818761 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.35298 (* 1 = 5.35298 loss)
I0523 08:31:15.831815 35003 sgd_solver.cpp:112] Iteration 184470, lr = 0.001
I0523 08:31:18.745754 35003 solver.cpp:239] Iteration 184480 (3.41663 iter/s, 2.92686s/10 iters), loss = 6.41878
I0523 08:31:18.745797 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41878 (* 1 = 6.41878 loss)
I0523 08:31:18.929749 35003 sgd_solver.cpp:112] Iteration 184480, lr = 0.001
I0523 08:31:22.347105 35003 solver.cpp:239] Iteration 184490 (2.77689 iter/s, 3.60115s/10 iters), loss = 6.88028
I0523 08:31:22.347162 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88028 (* 1 = 6.88028 loss)
I0523 08:31:22.360237 35003 sgd_solver.cpp:112] Iteration 184490, lr = 0.001
I0523 08:31:25.159373 35003 solver.cpp:239] Iteration 184500 (3.55607 iter/s, 2.81209s/10 iters), loss = 5.48838
I0523 08:31:25.159415 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.48838 (* 1 = 5.48838 loss)
I0523 08:31:25.186414 35003 sgd_solver.cpp:112] Iteration 184500, lr = 0.001
I0523 08:31:28.257800 35003 solver.cpp:239] Iteration 184510 (3.22762 iter/s, 3.09826s/10 iters), loss = 6.42653
I0523 08:31:28.257853 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42653 (* 1 = 6.42653 loss)
I0523 08:31:28.991161 35003 sgd_solver.cpp:112] Iteration 184510, lr = 0.001
I0523 08:31:32.725520 35003 solver.cpp:239] Iteration 184520 (2.2384 iter/s, 4.46747s/10 iters), loss = 7.04266
I0523 08:31:32.725566 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04266 (* 1 = 7.04266 loss)
I0523 08:31:32.744343 35003 sgd_solver.cpp:112] Iteration 184520, lr = 0.001
I0523 08:31:37.157852 35003 solver.cpp:239] Iteration 184530 (2.25627 iter/s, 4.4321s/10 iters), loss = 6.32404
I0523 08:31:37.157899 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32404 (* 1 = 6.32404 loss)
I0523 08:31:37.171105 35003 sgd_solver.cpp:112] Iteration 184530, lr = 0.001
I0523 08:31:40.781352 35003 solver.cpp:239] Iteration 184540 (2.75992 iter/s, 3.6233s/10 iters), loss = 6.45887
I0523 08:31:40.781605 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.45887 (* 1 = 6.45887 loss)
I0523 08:31:40.793014 35003 sgd_solver.cpp:112] Iteration 184540, lr = 0.001
I0523 08:31:45.224030 35003 solver.cpp:239] Iteration 184550 (2.25111 iter/s, 4.44226s/10 iters), loss = 6.88337
I0523 08:31:45.224076 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88337 (* 1 = 6.88337 loss)
I0523 08:31:45.239754 35003 sgd_solver.cpp:112] Iteration 184550, lr = 0.001
I0523 08:31:47.956838 35003 solver.cpp:239] Iteration 184560 (3.65945 iter/s, 2.73265s/10 iters), loss = 7.70187
I0523 08:31:47.956876 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70187 (* 1 = 7.70187 loss)
I0523 08:31:48.698105 35003 sgd_solver.cpp:112] Iteration 184560, lr = 0.001
I0523 08:31:52.325525 35003 solver.cpp:239] Iteration 184570 (2.28913 iter/s, 4.36846s/10 iters), loss = 6.12696
I0523 08:31:52.325578 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12696 (* 1 = 6.12696 loss)
I0523 08:31:53.059887 35003 sgd_solver.cpp:112] Iteration 184570, lr = 0.001
I0523 08:31:55.187094 35003 solver.cpp:239] Iteration 184580 (3.49481 iter/s, 2.86138s/10 iters), loss = 6.87006
I0523 08:31:55.187150 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87006 (* 1 = 6.87006 loss)
I0523 08:31:55.921424 35003 sgd_solver.cpp:112] Iteration 184580, lr = 0.001
I0523 08:31:59.416429 35003 solver.cpp:239] Iteration 184590 (2.36456 iter/s, 4.22911s/10 iters), loss = 5.92474
I0523 08:31:59.416477 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.92474 (* 1 = 5.92474 loss)
I0523 08:31:59.420047 35003 sgd_solver.cpp:112] Iteration 184590, lr = 0.001
I0523 08:32:01.509142 35003 solver.cpp:239] Iteration 184600 (4.77886 iter/s, 2.09255s/10 iters), loss = 6.31013
I0523 08:32:01.509186 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31013 (* 1 = 6.31013 loss)
I0523 08:32:01.514766 35003 sgd_solver.cpp:112] Iteration 184600, lr = 0.001
I0523 08:32:06.539744 35003 solver.cpp:239] Iteration 184610 (1.98793 iter/s, 5.03035s/10 iters), loss = 6.82216
I0523 08:32:06.539785 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82216 (* 1 = 6.82216 loss)
I0523 08:32:06.545873 35003 sgd_solver.cpp:112] Iteration 184610, lr = 0.001
I0523 08:32:10.118360 35003 solver.cpp:239] Iteration 184620 (2.79453 iter/s, 3.57842s/10 iters), loss = 6.1039
I0523 08:32:10.118405 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1039 (* 1 = 6.1039 loss)
I0523 08:32:10.142235 35003 sgd_solver.cpp:112] Iteration 184620, lr = 0.001
I0523 08:32:13.713409 35003 solver.cpp:239] Iteration 184630 (2.78176 iter/s, 3.59485s/10 iters), loss = 7.229
I0523 08:32:13.713655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.229 (* 1 = 7.229 loss)
I0523 08:32:13.726796 35003 sgd_solver.cpp:112] Iteration 184630, lr = 0.001
I0523 08:32:15.891741 35003 solver.cpp:239] Iteration 184640 (4.59132 iter/s, 2.17802s/10 iters), loss = 6.85723
I0523 08:32:15.891784 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85723 (* 1 = 6.85723 loss)
I0523 08:32:15.905261 35003 sgd_solver.cpp:112] Iteration 184640, lr = 0.001
I0523 08:32:18.711359 35003 solver.cpp:239] Iteration 184650 (3.54679 iter/s, 2.81945s/10 iters), loss = 6.05721
I0523 08:32:18.711401 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05721 (* 1 = 6.05721 loss)
I0523 08:32:18.720521 35003 sgd_solver.cpp:112] Iteration 184650, lr = 0.001
I0523 08:32:21.554328 35003 solver.cpp:239] Iteration 184660 (3.51765 iter/s, 2.84281s/10 iters), loss = 6.31611
I0523 08:32:21.554379 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31611 (* 1 = 6.31611 loss)
I0523 08:32:22.294895 35003 sgd_solver.cpp:112] Iteration 184660, lr = 0.001
I0523 08:32:27.261078 35003 solver.cpp:239] Iteration 184670 (1.7524 iter/s, 5.70647s/10 iters), loss = 6.94508
I0523 08:32:27.261126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94508 (* 1 = 6.94508 loss)
I0523 08:32:27.274184 35003 sgd_solver.cpp:112] Iteration 184670, lr = 0.001
I0523 08:32:30.056896 35003 solver.cpp:239] Iteration 184680 (3.57699 iter/s, 2.79565s/10 iters), loss = 6.30532
I0523 08:32:30.056951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30532 (* 1 = 6.30532 loss)
I0523 08:32:30.676353 35003 sgd_solver.cpp:112] Iteration 184680, lr = 0.001
I0523 08:32:33.764875 35003 solver.cpp:239] Iteration 184690 (2.69705 iter/s, 3.70776s/10 iters), loss = 7.37771
I0523 08:32:33.764916 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37771 (* 1 = 7.37771 loss)
I0523 08:32:33.777577 35003 sgd_solver.cpp:112] Iteration 184690, lr = 0.001
I0523 08:32:38.003672 35003 solver.cpp:239] Iteration 184700 (2.35928 iter/s, 4.23857s/10 iters), loss = 7.39256
I0523 08:32:38.003724 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.39256 (* 1 = 7.39256 loss)
I0523 08:32:38.743983 35003 sgd_solver.cpp:112] Iteration 184700, lr = 0.001
I0523 08:32:41.630527 35003 solver.cpp:239] Iteration 184710 (2.75736 iter/s, 3.62665s/10 iters), loss = 6.3498
I0523 08:32:41.630573 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3498 (* 1 = 6.3498 loss)
I0523 08:32:41.632545 35003 sgd_solver.cpp:112] Iteration 184710, lr = 0.001
I0523 08:32:44.445611 35003 solver.cpp:239] Iteration 184720 (3.55251 iter/s, 2.81491s/10 iters), loss = 6.74907
I0523 08:32:44.445899 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74907 (* 1 = 6.74907 loss)
I0523 08:32:44.454478 35003 sgd_solver.cpp:112] Iteration 184720, lr = 0.001
I0523 08:32:48.786058 35003 solver.cpp:239] Iteration 184730 (2.30415 iter/s, 4.33999s/10 iters), loss = 7.02189
I0523 08:32:48.786165 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02189 (* 1 = 7.02189 loss)
I0523 08:32:48.820564 35003 sgd_solver.cpp:112] Iteration 184730, lr = 0.001
I0523 08:32:53.231739 35003 solver.cpp:239] Iteration 184740 (2.24952 iter/s, 4.4454s/10 iters), loss = 6.89809
I0523 08:32:53.231784 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89809 (* 1 = 6.89809 loss)
I0523 08:32:53.248073 35003 sgd_solver.cpp:112] Iteration 184740, lr = 0.001
I0523 08:32:56.892292 35003 solver.cpp:239] Iteration 184750 (2.73198 iter/s, 3.66035s/10 iters), loss = 5.53741
I0523 08:32:56.892335 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.53741 (* 1 = 5.53741 loss)
I0523 08:32:56.910979 35003 sgd_solver.cpp:112] Iteration 184750, lr = 0.001
I0523 08:33:00.469871 35003 solver.cpp:239] Iteration 184760 (2.79534 iter/s, 3.57739s/10 iters), loss = 6.20757
I0523 08:33:00.469921 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20757 (* 1 = 6.20757 loss)
I0523 08:33:00.482831 35003 sgd_solver.cpp:112] Iteration 184760, lr = 0.001
I0523 08:33:05.026319 35003 solver.cpp:239] Iteration 184770 (2.1948 iter/s, 4.55621s/10 iters), loss = 5.61803
I0523 08:33:05.026370 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.61803 (* 1 = 5.61803 loss)
I0523 08:33:05.042398 35003 sgd_solver.cpp:112] Iteration 184770, lr = 0.001
I0523 08:33:07.069078 35003 solver.cpp:239] Iteration 184780 (4.89567 iter/s, 2.04262s/10 iters), loss = 5.20064
I0523 08:33:07.069125 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.20064 (* 1 = 5.20064 loss)
I0523 08:33:07.077834 35003 sgd_solver.cpp:112] Iteration 184780, lr = 0.001
I0523 08:33:09.904042 35003 solver.cpp:239] Iteration 184790 (3.52759 iter/s, 2.8348s/10 iters), loss = 7.26056
I0523 08:33:09.904091 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26056 (* 1 = 7.26056 loss)
I0523 08:33:10.529641 35003 sgd_solver.cpp:112] Iteration 184790, lr = 0.001
I0523 08:33:13.530789 35003 solver.cpp:239] Iteration 184800 (2.75745 iter/s, 3.62655s/10 iters), loss = 7.07502
I0523 08:33:13.530848 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07502 (* 1 = 7.07502 loss)
I0523 08:33:13.832451 35003 sgd_solver.cpp:112] Iteration 184800, lr = 0.001
I0523 08:33:18.062199 35003 solver.cpp:239] Iteration 184810 (2.20694 iter/s, 4.53116s/10 iters), loss = 5.78272
I0523 08:33:18.062507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.78272 (* 1 = 5.78272 loss)
I0523 08:33:18.790767 35003 sgd_solver.cpp:112] Iteration 184810, lr = 0.001
I0523 08:33:22.914465 35003 solver.cpp:239] Iteration 184820 (2.0611 iter/s, 4.85179s/10 iters), loss = 6.5363
I0523 08:33:22.914515 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5363 (* 1 = 6.5363 loss)
I0523 08:33:22.923589 35003 sgd_solver.cpp:112] Iteration 184820, lr = 0.001
I0523 08:33:26.670665 35003 solver.cpp:239] Iteration 184830 (2.66241 iter/s, 3.756s/10 iters), loss = 5.9965
I0523 08:33:26.670737 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9965 (* 1 = 5.9965 loss)
I0523 08:33:27.405532 35003 sgd_solver.cpp:112] Iteration 184830, lr = 0.001
I0523 08:33:30.201802 35003 solver.cpp:239] Iteration 184840 (2.83212 iter/s, 3.53092s/10 iters), loss = 6.84148
I0523 08:33:30.201843 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84148 (* 1 = 6.84148 loss)
I0523 08:33:30.214969 35003 sgd_solver.cpp:112] Iteration 184840, lr = 0.001
I0523 08:33:34.531675 35003 solver.cpp:239] Iteration 184850 (2.30966 iter/s, 4.32965s/10 iters), loss = 6.19352
I0523 08:33:34.531711 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19352 (* 1 = 6.19352 loss)
I0523 08:33:34.544052 35003 sgd_solver.cpp:112] Iteration 184850, lr = 0.001
I0523 08:33:39.170477 35003 solver.cpp:239] Iteration 184860 (2.15584 iter/s, 4.63857s/10 iters), loss = 6.12608
I0523 08:33:39.170527 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.12608 (* 1 = 6.12608 loss)
I0523 08:33:39.885951 35003 sgd_solver.cpp:112] Iteration 184860, lr = 0.001
I0523 08:33:42.875525 35003 solver.cpp:239] Iteration 184870 (2.69917 iter/s, 3.70485s/10 iters), loss = 6.54459
I0523 08:33:42.875566 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54459 (* 1 = 6.54459 loss)
I0523 08:33:43.051136 35003 sgd_solver.cpp:112] Iteration 184870, lr = 0.001
I0523 08:33:46.623795 35003 solver.cpp:239] Iteration 184880 (2.66804 iter/s, 3.74807s/10 iters), loss = 7.70411
I0523 08:33:46.623843 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.70411 (* 1 = 7.70411 loss)
I0523 08:33:46.744369 35003 sgd_solver.cpp:112] Iteration 184880, lr = 0.001
I0523 08:33:51.102751 35003 solver.cpp:239] Iteration 184890 (2.23278 iter/s, 4.47872s/10 iters), loss = 7.43728
I0523 08:33:51.102896 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43728 (* 1 = 7.43728 loss)
I0523 08:33:51.119951 35003 sgd_solver.cpp:112] Iteration 184890, lr = 0.001
I0523 08:33:54.514606 35003 solver.cpp:239] Iteration 184900 (2.9312 iter/s, 3.41157s/10 iters), loss = 6.07183
I0523 08:33:54.514652 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07183 (* 1 = 6.07183 loss)
I0523 08:33:54.523941 35003 sgd_solver.cpp:112] Iteration 184900, lr = 0.001
I0523 08:33:58.683737 35003 solver.cpp:239] Iteration 184910 (2.39871 iter/s, 4.1689s/10 iters), loss = 6.63149
I0523 08:33:58.683789 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.63149 (* 1 = 6.63149 loss)
I0523 08:33:58.691222 35003 sgd_solver.cpp:112] Iteration 184910, lr = 0.001
I0523 08:34:00.964241 35003 solver.cpp:239] Iteration 184920 (4.38528 iter/s, 2.28036s/10 iters), loss = 6.97883
I0523 08:34:00.964287 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97883 (* 1 = 6.97883 loss)
I0523 08:34:00.973613 35003 sgd_solver.cpp:112] Iteration 184920, lr = 0.001
I0523 08:34:02.991883 35003 solver.cpp:239] Iteration 184930 (4.93218 iter/s, 2.0275s/10 iters), loss = 6.40224
I0523 08:34:02.991922 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40224 (* 1 = 6.40224 loss)
I0523 08:34:03.004567 35003 sgd_solver.cpp:112] Iteration 184930, lr = 0.001
I0523 08:34:04.948323 35003 solver.cpp:239] Iteration 184940 (5.11166 iter/s, 1.95631s/10 iters), loss = 6.15816
I0523 08:34:04.948371 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15816 (* 1 = 6.15816 loss)
I0523 08:34:04.957751 35003 sgd_solver.cpp:112] Iteration 184940, lr = 0.001
I0523 08:34:07.214363 35003 solver.cpp:239] Iteration 184950 (4.41327 iter/s, 2.26589s/10 iters), loss = 6.96332
I0523 08:34:07.214409 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96332 (* 1 = 6.96332 loss)
I0523 08:34:07.911430 35003 sgd_solver.cpp:112] Iteration 184950, lr = 0.001
I0523 08:34:10.008579 35003 solver.cpp:239] Iteration 184960 (3.57903 iter/s, 2.79405s/10 iters), loss = 7.3029
I0523 08:34:10.008618 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3029 (* 1 = 7.3029 loss)
I0523 08:34:10.016199 35003 sgd_solver.cpp:112] Iteration 184960, lr = 0.001
I0523 08:34:13.419062 35003 solver.cpp:239] Iteration 184970 (2.93229 iter/s, 3.4103s/10 iters), loss = 6.52129
I0523 08:34:13.419108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52129 (* 1 = 6.52129 loss)
I0523 08:34:13.431769 35003 sgd_solver.cpp:112] Iteration 184970, lr = 0.001
I0523 08:34:16.854758 35003 solver.cpp:239] Iteration 184980 (2.91078 iter/s, 3.43551s/10 iters), loss = 7.45806
I0523 08:34:16.854796 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45806 (* 1 = 7.45806 loss)
I0523 08:34:16.873167 35003 sgd_solver.cpp:112] Iteration 184980, lr = 0.001
I0523 08:34:19.197012 35003 solver.cpp:239] Iteration 184990 (4.26968 iter/s, 2.3421s/10 iters), loss = 7.37251
I0523 08:34:19.197080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.37251 (* 1 = 7.37251 loss)
I0523 08:34:19.209091 35003 sgd_solver.cpp:112] Iteration 184990, lr = 0.001
I0523 08:34:21.860591 35003 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_185000.caffemodel
I0523 08:34:22.360363 35003 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AdditMarginLmdbImage/AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbImage-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_185000.solverstate
I0523 08:34:22.503058 35003 solver.cpp:239] Iteration 185000 (3.02494 iter/s, 3.30585s/10 iters), loss = 6.87702
I0523 08:34:22.503109 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87702 (* 1 = 6.87702 loss)
I0523 08:34:22.510445 35003 sgd_solver.cpp:112] Iteration 185000, lr = 0.001
I0523 08:34:25.811172 35003 solver.cpp:239] Iteration 185010 (3.02305 iter/s, 3.30792s/10 iters), loss = 6.36542
I0523 08:34:25.811236 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36542 (* 1 = 6.36542 loss)
I0523 08:34:26.512619 35003 sgd_solver.cpp:112] Iteration 185010, lr = 0.001
I0523 08:34:29.833302 35003 solver.cpp:239] Iteration 185020 (2.48639 iter/s, 4.02189s/10 iters), loss = 7.14373
I0523 08:34:29.833354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14373 (* 1 = 7.14373 loss)
I0523 08:34:29.846563 35003 sgd_solver.cpp:112] Iteration 185020, lr = 0.001
I0523 08:34:34.927213 35003 solver.cpp:239] Iteration 185030 (1.96323 iter/s, 5.09365s/10 iters), loss = 6.46491
I0523 08:34:34.927260 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46491 (* 1 = 6.46491 loss)
I0523 08:34:34.940284 35003 sgd_solver.cpp:112] Iteration 185030, lr = 0.001
I0523 08:34:38.628268 35003 solver.cpp:239] Iteration 185040 (2.70208 iter/s, 3.70085s/10 iters), loss = 5.67128
I0523 08:34:38.628326 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.67128 (* 1 = 5.67128 loss)
I0523 08:34:39.369113 35003 sgd_solver.cpp:112] Iteration 185040, lr = 0.001
I0523 08:34:42.205631 35003 solver.cpp:239] Iteration 185050 (2.79551 iter/s, 3.57716s/10 iters), loss = 5.4295
I0523 08:34:42.205677 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.4295 (* 1 = 5.4295 loss)
I0523 08:34:42.217191 35003 sgd_solver.cpp:112] Iteration 185050, lr = 0.001
I0523 08:34:44.266691 35003 solver.cpp:239] Iteration 185060 (4.8522 iter/s, 2.06092s/10 iters), loss = 7.24244
I0523 08:34:44.266742 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24244 (* 1 = 7.24244 loss)
I0523 08:34:44.280078 35003 sgd_solver.cpp:112] Iteration 185060, lr = 0.001
I0523 08:34:47.816774 35003 solver.cpp:239] Iteration 185070 (2.817 iter/s, 3.54988s/10 iters), loss = 7.08221
I0523 08:34:47.816830 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08221 (* 1 = 7.08221 loss)
I0523 08:34:47.826874 35003 sgd_solver.cpp:112] Iteration 185070, lr = 0.001
I0523 08:34:51.381378 35003 solver.cpp:239] Iteration 185080 (2.80553 iter/s, 3.56439s/10 iters), loss = 7.11187
I0523 08:34:51.381449 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11187 (* 1 = 7.11187 loss)
I0523 08:34:51.406814 35003 sgd_solver.cpp:112] Iteration 185080, lr = 0.001
I0523 08:34:55.122665 35003 solver.cpp:239] Iteration 185090 (2.67304 iter/s, 3.74106s/10 iters), loss = 7.59898
I0523 08:34:55.122866 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59898 (* 1 = 7.59898 loss)
I0523 08:34:55.130576 35003 sgd_solver.cpp:112] Iteration 185090, lr = 0.001
I0523 08:34:59.638240 35003 solver.cpp:239] Iteration 185100 (2.21475 iter/s, 4.51518s/10 iters), loss = 6.13709
I0523 08:34:59.638285 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13709 (* 1 = 6.13709 loss)
I0523 08:34:59.644054 35003 sgd_solver.cpp:112] Iteration 185100, lr = 0.001
I0523 08:35:04.803350 35003 solver.cpp:239] Iteration 185110 (1.93617 iter/s, 5.16484s/10 iters), loss = 6.73144
I0523 08:35:04.803400 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73144 (* 1 = 6.73144 loss)
I0523 08:35:05.475492 35003 sgd_solver.cpp:112] Iteration 185110, lr = 0.001
I0523 08:35:07.680472 35003 solver.cpp:239] Iteration 185120 (3.47591 iter/s, 2.87694s/10 iters), loss = 5.62602
I0523 08:35:07.680516 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.62602 (* 1 = 5.62602 loss)
I0523 08:35:08.414793 35003 sgd_solver.cpp:112] Iteration 185120, lr = 0.001
I0523 08:35:11.347077 35003 solver.cpp:239] Iteration 185130 (2.72747 iter/s, 3.66641s/10 iters), loss = 5.91058
I0523 08:35:11.347115 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.91058 (* 1 = 5.91058 loss)
I0523 08:35:11.358664 35003 sgd_solver.cpp:112] Iteration 185130, lr = 0.001
I0523 08:35:15.709800 35003 solver.cpp:239] Iteration 185140 (2.29226 iter/s, 4.3625s/10 iters), loss = 5.71813
I0523 08:35:15.709846 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.71813 (* 1 = 5.71813 loss)
I0523 08:35:15.737567 35003 sgd_solver.cpp:112] Iteration 185140, lr = 0.001
I0523 08:35:17.333839 35003 solver.cpp:239] Iteration 185150 (6.15795 iter/s, 1.62392s/10 iters), loss = 6.72847
I0523 08:35:17.333889 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72847 (* 1 = 6.72847 loss)
I0523 08:35:17.339133 35003 sgd_solver.cpp:112] Iteration 185150, lr = 0.001
I0523 08:35:21.303906 35003 solver.cpp:239] Iteration 185160 (2.51898 iter/s, 3.96986s/10 iters), loss = 7.21474
I0523 08:35:21.303952 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21474 (* 1 = 7.21474 loss)
I0523 08:35:21.935681 35003 sgd_solver.cpp:112] Iteration 185160, lr = 0.001
I0523 08:35:25.631021 35003 solver.cpp:239] Iteration 185170 (2.31113 iter/s, 4.32688s/10 iters), loss = 5.95887
I0523 08:35:25.631233 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.95887 (* 1 = 5.95887 loss)
I0523 08:35:25.636801 35003 sgd_solver.cpp:112] Iteration 185170, lr = 0.001
I0523 08:35:28.224825 35003 solver.cpp:239] Iteration 185180 (3.8558 iter/s, 2.5935s/10 iters), loss = 7.3793
I0523 08:35:28.224885 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3793 (* 1 = 7.3793 loss)
I0523 08:35:28.237901 35003 sgd_solver.cpp:112] Iteration 185180, lr = 0.001
I0523 08:35:30.310747 35003 solver.cpp:239] Iteration 185190 (4.79439 iter/s, 2.08577s/10 iters), loss = 7.11901
I0523 08:35:30.310791 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11901 (* 1 = 7.11901 loss)
I0523 08:35:30.318637 35003 sgd_solver.cpp:112] Iteration 185190, lr = 0.001
I0523 08:35:33.717581 35003 solver.cpp:239] Iteration 185200 (2.93543 iter/s, 3.40665s/10 iters), loss = 7.61273
I0523 08:35:33.717614 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61273 (* 1 = 7.61273 loss)
I0523 08:35:33.731097 35003 sgd_solver.cpp:112] Iteration 185200, lr = 0.001
I0523 08:35:37.247145 35003 solver.cpp:239] Iteration 185210 (2.83336 iter/s, 3.52938s/10 iters), loss = 7.50355
I0523 08:35:37.247189 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.50355 (* 1 = 7.50355 loss)
I0523 08:35:37.265635 35003 sgd_solver.cpp:112] Iteration 185210, lr = 0.001
I0523 08:35:42.313995 35003 solver.cpp:239] Iteration 185220 (1.97371 iter/s, 5.0666s/10 iters), loss = 5.77164
I0523 08:35:42.314031 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77164 (* 1 = 5.77164 loss)
I0523 08:35:42.326449 35003 sgd_solver.cpp:112] Iteration 185220, lr = 0.001
I0523 08:35:46.662031 35003 solver.cpp:239] Iteration 185230 (2.30001 iter/s, 4.34781s/10 iters), loss = 7.2453
I0523 08:35:46.662094 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.2453 (* 1 = 7.2453 loss)
I0523 08:35:46.670611 35003 sgd_solver.cpp:112] Iteration 185230, lr = 0.001
I0523 08:35:50.095613 35003 solver.cpp:239] Iteration 185240 (2.91258 iter/s, 3.43338s/10 iters), loss = 7.32863
I0523 08:35:50.095656 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32863 (* 1 = 7.32863 loss)
I0523 08:35:50.108476 35003 sgd_solver.cpp:112] Iteration 185240, lr = 0.001
I0523 08:35:53.491235 35003 solver.cpp:239] Iteration 185250 (2.94513 iter/s, 3.39544s/10 iters), loss = 5.94555
I0523 08:35:53.491272 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.94555 (* 1 = 5.94555 loss)
I0523 08:35:53.499789 35003 sgd_solver.cpp:112] Iteration 185250, lr = 0.001
I0523 08:35:57.272485 35003 solver.cpp:239] Iteration 185260 (2.64477 iter/s, 3.78105s/10 iters), loss = 7.14982
I0523 08:35:57.272675 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14982 (* 1 = 7.14982 loss)
I0523 08:35:57.279903 35003 sgd_solver.cpp:112] Iteration 185260, lr = 0.001
I0523 08:36:00.792515 35003 solver.cpp:239] Iteration 185270 (2.84116 iter/s, 3.51969s/10 iters), loss = 5.98905
I0523 08:36:00.792567 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98905 (* 1 = 5.98905 loss)
I0523 08:36:01.516887 35003 sgd_solver.cpp:112] Iteration 185270, lr = 0.001
I0523 08:36:05.273999 35003 solver.cpp:239] Iteration 185280 (2.23152 iter/s, 4.48125s/10 iters), loss = 5.99405
I0523 08:36:05.274044 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.99405 (* 1 = 5.99405 loss)
I0523 08:36:05.986431 35003 sgd_solver.cpp:112] Iteration 185280, lr = 0.001
I0523 08:36:09.432699 35003 solver.cpp:239] Iteration 185290 (2.40472 iter/s, 4.15849s/10 iters), loss = 7.25597
I0523 08:36:09.432744 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25597 (* 1 = 7.25597 loss)
I0523 08:36:09.528187 35003 sgd_solver.cpp:112] Iteration 185290, lr = 0.001
I0523 08:36:12.195909 35003 solver.cpp:239] Iteration 185300 (3.61922 iter/s, 2.76303s/10 iters), loss = 6.31948
I0523 08:36:12.195958 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31948 (* 1 = 6.31948 loss)
I0523 08:36:12.685797 35003 sgd_solver.cpp:112] Iteration 185300, lr = 0.001
I0523 08:36:15.644439 35003 solver.cpp:239] Iteration 185310 (2.89996 iter/s, 3.44833s/10 iters), loss = 5.67192
I0523 08:36:15.644498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.67192 (* 1 = 5.67192 loss)
I0523 08:36:16.385807 35003 sgd_solver.cpp:112] Iteration 185310, lr = 0.001
I0523 08:36:19.968758 35003 solver.cpp:239] Iteration 185320 (2.31263 iter/s, 4.32408s/10 iters), loss = 6.71916
I0523 08:36:19.968806 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71916 (* 1 = 6.71916 loss)
I0523 08:36:19.981628 35003 sgd_solver.cpp:112] Iteration 185320, lr = 0.001
I0523 08:36:24.348126 35003 solver.cpp:239] Iteration 185330 (2.28355 iter/s, 4.37914s/10 iters), loss = 6.68738
I0523 08:36:24.348176 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68738 (* 1 = 6.68738 loss)
I0523 08:36:24.356740 35003 sgd_solver.cpp:112] Iteration 185330, lr = 0.001
I0523 08:36:28.103519 35003 solver.cpp:239] Iteration 185340 (2.66299 iter/s, 3.75518s/10 iters), loss = 6.67858
I0523 08:36:28.103837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.67858 (* 1 = 6.67858 loss)
I0523 08:36:28.108626 35003 sgd_solver.cpp:112] Iteration 185340, lr = 0.001
I0523 08:36:31.753967 35003 solver.cpp:239] Iteration 185350 (2.73972 iter/s, 3.65001s/10 iters), loss = 6.53404
I0523 08:36:31.754014 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53404 (* 1 = 6.53404 loss)
I0523 08:36:31.762380 35003 sgd_solver.cpp:112] Iteration 185350, lr = 0.001
I0523 08:36:36.042484 35003 solver.cpp:239] Iteration 185360 (2.33193 iter/s, 4.2883s/10 iters), loss = 7.60174
I0523 08:36:36.042536 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60174 (* 1 = 7.60174 loss)
I0523 08:36:36.048537 35003 sgd_solver.cpp:112] Iteration 185360, lr = 0.001
I0523 08:36:38.113281 35003 solver.cpp:239] Iteration 185370 (4.82942 iter/s, 2.07064s/10 iters), loss = 6.1415
I0523 08:36:38.113333 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1415 (* 1 = 6.1415 loss)
I0523 08:36:38.827848 35003 sgd_solver.cpp:112] Iteration 185370, lr = 0.001
I0523 08:36:43.181337 35003 solver.cpp:239] Iteration 185380 (1.97324 iter/s, 5.0678s/10 iters), loss = 5.71247
I0523 08:36:43.181383 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.71247 (* 1 = 5.71247 loss)
I0523 08:36:43.196211 35003 sgd_solver.cpp:112] Iteration 185380, lr = 0.001
I0523 08:36:46.691823 35003 solver.cpp:239] Iteration 185390 (2.84876 iter/s, 3.5103s/10 iters), loss = 6.82549
I0523 08:36:46.691864 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82549 (* 1 = 6.82549 loss)
I0523 08:36:46.710139 35003 sgd_solver.cpp:112] Iteration 185390, lr = 0.001
I0523 08:36:49.500355 35003 solver.cpp:239] Iteration 185400 (3.56078 iter/s, 2.80837s/10 iters), loss = 6.97631
I0523 08:36:49.500401 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97631 (* 1 = 6.97631 loss)
I0523 08:36:49.649910 35003 sgd_solver.cpp:112] Iteration 185400, lr = 0.001
I0523 08:36:51.610285 35003 solver.cpp:239] Iteration 185410 (4.73981 iter/s, 2.10979s/10 iters), loss = 5.39079
I0523 08:36:51.610334 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.39079 (* 1 = 5.39079 loss)
I0523 08:36:52.169153 35003 sgd_solver.cpp:112] Iteration 185410, lr = 0.001
I0523 08:36:55.267807 35003 solver.cpp:239] Iteration 185420 (2.73424 iter/s, 3.65732s/10 iters), loss = 6.56787
I0523 08:36:55.267854 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56787 (* 1 = 6.56787 loss)
I0523 08:36:55.279620 35003 sgd_solver.cpp:112] Iteration 185420, lr = 0.001
I0523 08:36:59.267417 35003 solver.cpp:239] Iteration 185430 (2.50038 iter/s, 3.99939s/10 iters), loss = 6.5989
I0523 08:36:59.267585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5989 (* 1 = 6.5989 loss)
I0523 08:36:59.983019 35003 sgd_solver.cpp:112] Iteration 185430, lr = 0.001
I0523 08:37:04.259587 35003 solver.cpp:239] Iteration 185440 (2.00328 iter/s, 4.9918s/10 iters), loss = 7.3324
I0523 08:37:04.259632 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3324 (* 1 = 7.3324 loss)
I0523 08:37:04.267243 35003 sgd_solver.cpp:112] Iteration 185440, lr = 0.001
I0523 08:37:07.088955 35003 solver.cpp:239] Iteration 185450 (3.53456 iter/s, 2.8292s/10 iters), loss = 5.7448
I0523 08:37:07.089013 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.7448 (* 1 = 5.7448 loss)
I0523 08:37:07.096432 35003 sgd_solver.cpp:112] Iteration 185450, lr = 0.001
I0523 08:37:09.154721 35003 solver.cpp:239] Iteration 185460 (4.84116 iter/s, 2.06562s/10 iters), loss = 7.42743
I0523 08:37:09.154765 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.42743 (* 1 = 7.42743 loss)
I0523 08:37:09.165691 35003 sgd_solver.cpp:112] Iteration 185460, lr = 0.001
I0523 08:37:13.077090 35003 solver.cpp:239] Iteration 185470 (2.54962 iter/s, 3.92216s/10 iters), loss = 5.68683
I0523 08:37:13.077157 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.68683 (* 1 = 5.68683 loss)
I0523 08:37:13.772244 35003 sgd_solver.cpp:112] Iteration 185470, lr = 0.001
I0523 08:37:15.180238 35003 solver.cpp:239] Iteration 185480 (4.75512 iter/s, 2.103s/10 iters), loss = 7.24093
I0523 08:37:15.180275 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.24093 (* 1 = 7.24093 loss)
I0523 08:37:15.869760 35003 sgd_solver.cpp:112] Iteration 185480, lr = 0.001
I0523 08:37:19.205473 35003 solver.cpp:239] Iteration 185490 (2.48446 iter/s, 4.02503s/10 iters), loss = 6.69525
I0523 08:37:19.205526 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69525 (* 1 = 6.69525 loss)
I0523 08:37:19.939759 35003 sgd_solver.cpp:112] Iteration 185490, lr = 0.001
I0523 08:37:22.246503 35003 solver.cpp:239] Iteration 185500 (3.28856 iter/s, 3.04085s/10 iters), loss = 6.56493
I0523 08:37:22.246541 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56493 (* 1 = 6.56493 loss)
I0523 08:37:22.276142 35003 sgd_solver.cpp:112] Iteration 185500, lr = 0.001
I0523 08:37:25.801282 35003 solver.cpp:239] Iteration 185510 (2.81326 iter/s, 3.55459s/10 iters), loss = 7.26106
I0523 08:37:25.801326 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26106 (* 1 = 7.26106 loss)
I0523 08:37:25.823899 35003 sgd_solver.cpp:112] Iteration 185510, lr = 0.001
I0523 08:37:30.618629 35003 solver.cpp:239] Iteration 185520 (2.07594 iter/s, 4.81711s/10 iters), loss = 6.0699
I0523 08:37:30.618908 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0699 (* 1 = 6.0699 loss)
I0523 08:37:30.631281 35003 sgd_solver.cpp:112] Iteration 185520, lr = 0.001
I0523 08:37:33.124395 35003 solver.cpp:239] Iteration 185530 (3.99137 iter/s, 2.50541s/10 iters), loss = 6.73622
I0523 08:37:33.124435 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73622 (* 1 = 6.73622 loss)
I0523 08:37:33.137316 35003 sgd_solver.cpp:112] Iteration 185530, lr = 0.001
I0523 08:37:36.754382 35003 solver.cpp:239] Iteration 185540 (2.75497 iter/s, 3.6298s/10 iters), loss = 5.82328
I0523 08:37:36.754422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.82328 (* 1 = 5.82328 loss)
I0523 08:37:36.768069 35003 sgd_solver.cpp:112] Iteration 185540, lr = 0.001
I0523 08:37:39.813700 35003 solver.cpp:239] Iteration 185550 (3.26889 iter/s, 3.05915s/10 iters), loss = 6.70151
I0523 08:37:39.813746 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70151 (* 1 = 6.70151 loss)
I0523 08:37:40.522536 35003 sgd_solver.cpp:112] Iteration 185550, lr = 0.001
I0523 08:37:44.885300 35003 solver.cpp:239] Iteration 185560 (1.97186 iter/s, 5.07134s/10 iters), loss = 6.79742
I0523 08:37:44.885361 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79742 (* 1 = 6.79742 loss)
I0523 08:37:44.893262 35003 sgd_solver.cpp:112] Iteration 185560, lr = 0.001
I0523 08:37:47.723637 35003 solver.cpp:239] Iteration 185570 (3.52342 iter/s, 2.83815s/10 iters), loss = 6.83455
I0523 08:37:47.723692 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.83455 (* 1 = 6.83455 loss)
I0523 08:37:47.736889 35003 sgd_solver.cpp:112] Iteration 185570, lr = 0.001
I0523 08:37:49.795702 35003 solver.cpp:239] Iteration 185580 (4.82646 iter/s, 2.07191s/10 iters), loss = 6.701
I0523 08:37:49.795759 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.701 (* 1 = 6.701 loss)
I0523 08:37:50.510668 35003 sgd_solver.cpp:112] Iteration 185580, lr = 0.001
I0523 08:37:54.253993 35003 solver.cpp:239] Iteration 185590 (2.24313 iter/s, 4.45805s/10 iters), loss = 6.60822
I0523 08:37:54.254030 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60822 (* 1 = 6.60822 loss)
I0523 08:37:54.263674 35003 sgd_solver.cpp:112] Iteration 185590, lr = 0.001
I0523 08:37:57.646870 35003 solver.cpp:239] Iteration 185600 (2.94751 iter/s, 3.39269s/10 iters), loss = 6.81428
I0523 08:37:57.646916 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81428 (* 1 = 6.81428 loss)
I0523 08:37:57.654742 35003 sgd_solver.cpp:112] Iteration 185600, lr = 0.001
I0523 08:38:02.565968 35003 solver.cpp:239] Iteration 185610 (2.033 iter/s, 4.91885s/10 iters), loss = 6.19201
I0523 08:38:02.566254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19201 (* 1 = 6.19201 loss)
I0523 08:38:02.569864 35003 sgd_solver.cpp:112] Iteration 185610, lr = 0.001
I0523 08:38:05.325533 35003 solver.cpp:239] Iteration 185620 (3.62426 iter/s, 2.75918s/10 iters), loss = 6.02686
I0523 08:38:05.325577 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02686 (* 1 = 6.02686 loss)
I0523 08:38:06.051292 35003 sgd_solver.cpp:112] Iteration 185620, lr = 0.001
I0523 08:38:08.149049 35003 solver.cpp:239] Iteration 185630 (3.54189 iter/s, 2.82335s/10 iters), loss = 6.79006
I0523 08:38:08.149106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79006 (* 1 = 6.79006 loss)
I0523 08:38:08.877157 35003 sgd_solver.cpp:112] Iteration 185630, lr = 0.001
I0523 08:38:12.365356 35003 solver.cpp:239] Iteration 185640 (2.37187 iter/s, 4.21607s/10 iters), loss = 8.2609
I0523 08:38:12.365399 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.2609 (* 1 = 8.2609 loss)
I0523 08:38:12.378618 35003 sgd_solver.cpp:112] Iteration 185640, lr = 0.001
I0523 08:38:16.349460 35003 solver.cpp:239] Iteration 185650 (2.51011 iter/s, 3.98389s/10 iters), loss = 6.78611
I0523 08:38:16.349511 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78611 (* 1 = 6.78611 loss)
I0523 08:38:17.064123 35003 sgd_solver.cpp:112] Iteration 185650, lr = 0.001
I0523 08:38:20.520969 35003 solver.cpp:239] Iteration 185660 (2.39734 iter/s, 4.17128s/10 iters), loss = 7.17452
I0523 08:38:20.521015 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.17452 (* 1 = 7.17452 loss)
I0523 08:38:20.527308 35003 sgd_solver.cpp:112] Iteration 185660, lr = 0.001
I0523 08:38:24.103644 35003 solver.cpp:239] Iteration 185670 (2.79136 iter/s, 3.58248s/10 iters), loss = 6.33539
I0523 08:38:24.103688 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33539 (* 1 = 6.33539 loss)
I0523 08:38:24.126135 35003 sgd_solver.cpp:112] Iteration 185670, lr = 0.001
I0523 08:38:29.202095 35003 solver.cpp:239] Iteration 185680 (1.96148 iter/s, 5.0982s/10 iters), loss = 6.17765
I0523 08:38:29.202152 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17765 (* 1 = 6.17765 loss)
I0523 08:38:29.219969 35003 sgd_solver.cpp:112] Iteration 185680, lr = 0.001
I0523 08:38:34.043601 35003 solver.cpp:239] Iteration 185690 (2.06558 iter/s, 4.84125s/10 iters), loss = 6.44026
I0523 08:38:34.043861 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44026 (* 1 = 6.44026 loss)
I0523 08:38:34.051800 35003 sgd_solver.cpp:112] Iteration 185690, lr = 0.001
I0523 08:38:38.375310 35003 solver.cpp:239] Iteration 185700 (2.30877 iter/s, 4.3313s/10 iters), loss = 6.02735
I0523 08:38:38.375355 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02735 (* 1 = 6.02735 loss)
I0523 08:38:38.382658 35003 sgd_solver.cpp:112] Iteration 185700, lr = 0.001
I0523 08:38:40.089550 35003 solver.cpp:239] Iteration 185710 (5.83395 iter/s, 1.71411s/10 iters), loss = 5.98048
I0523 08:38:40.089587 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98048 (* 1 = 5.98048 loss)
I0523 08:38:40.108186 35003 sgd_solver.cpp:112] Iteration 185710, lr = 0.001
I0523 08:38:43.483616 35003 solver.cpp:239] Iteration 185720 (2.94649 iter/s, 3.39387s/10 iters), loss = 6.29421
I0523 08:38:43.483666 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29421 (* 1 = 6.29421 loss)
I0523 08:38:44.120697 35003 sgd_solver.cpp:112] Iteration 185720, lr = 0.001
I0523 08:38:45.457032 35003 solver.cpp:239] Iteration 185730 (5.0677 iter/s, 1.97328s/10 iters), loss = 6.84855
I0523 08:38:45.457080 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84855 (* 1 = 6.84855 loss)
I0523 08:38:46.198376 35003 sgd_solver.cpp:112] Iteration 185730, lr = 0.001
I0523 08:38:50.434883 35003 solver.cpp:239] Iteration 185740 (2.009 iter/s, 4.9776s/10 iters), loss = 5.98268
I0523 08:38:50.434926 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.98268 (* 1 = 5.98268 loss)
I0523 08:38:50.446314 35003 sgd_solver.cpp:112] Iteration 185740, lr = 0.001
I0523 08:38:53.309085 35003 solver.cpp:239] Iteration 185750 (3.47943 iter/s, 2.87403s/10 iters), loss = 7.02833
I0523 08:38:53.309137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.02833 (* 1 = 7.02833 loss)
I0523 08:38:53.986227 35003 sgd_solver.cpp:112] Iteration 185750, lr = 0.001
I0523 08:38:58.763458 35003 solver.cpp:239] Iteration 185760 (1.83349 iter/s, 5.45409s/10 iters), loss = 6.27378
I0523 08:38:58.763506 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.27378 (* 1 = 6.27378 loss)
I0523 08:38:58.777160 35003 sgd_solver.cpp:112] Iteration 185760, lr = 0.001
I0523 08:39:02.866943 35003 solver.cpp:239] Iteration 185770 (2.43708 iter/s, 4.10327s/10 iters), loss = 5.82706
I0523 08:39:02.866999 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.82706 (* 1 = 5.82706 loss)
I0523 08:39:02.874644 35003 sgd_solver.cpp:112] Iteration 185770, lr = 0.001
I0523 08:39:06.471967 35003 solver.cpp:239] Iteration 185780 (2.77407 iter/s, 3.60481s/10 iters), loss = 6.64104
I0523 08:39:06.472174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64104 (* 1 = 6.64104 loss)
I0523 08:39:06.477582 35003 sgd_solver.cpp:112] Iteration 185780, lr = 0.001
I0523 08:39:09.835253 35003 solver.cpp:239] Iteration 185790 (2.97358 iter/s, 3.36295s/10 iters), loss = 6.70675
I0523 08:39:09.835289 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70675 (* 1 = 6.70675 loss)
I0523 08:39:09.848157 35003 sgd_solver.cpp:112] Iteration 185790, lr = 0.001
I0523 08:39:14.082505 35003 solver.cpp:239] Iteration 185800 (2.35458 iter/s, 4.24704s/10 iters), loss = 8.3689
I0523 08:39:14.082551 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.3689 (* 1 = 8.3689 loss)
I0523 08:39:14.088732 35003 sgd_solver.cpp:112] Iteration 185800, lr = 0.001
I0523 08:39:17.323443 35003 solver.cpp:239] Iteration 185810 (3.08572 iter/s, 3.24073s/10 iters), loss = 6.7317
I0523 08:39:17.323489 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7317 (* 1 = 6.7317 loss)
I0523 08:39:17.328680 35003 sgd_solver.cpp:112] Iteration 185810, lr = 0.001
I0523 08:39:20.946502 35003 solver.cpp:239] Iteration 185820 (2.76025 iter/s, 3.62286s/10 iters), loss = 6.61637
I0523 08:39:20.946540 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61637 (* 1 = 6.61637 loss)
I0523 08:39:20.971505 35003 sgd_solver.cpp:112] Iteration 185820, lr = 0.001
I0523 08:39:23.930331 35003 solver.cpp:239] Iteration 185830 (3.35158 iter/s, 2.98366s/10 iters), loss = 6.7473
I0523 08:39:23.930374 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.7473 (* 1 = 6.7473 loss)
I0523 08:39:24.668135 35003 sgd_solver.cpp:112] Iteration 185830, lr = 0.001
I0523 08:39:27.403026 35003 solver.cpp:239] Iteration 185840 (2.87976 iter/s, 3.47251s/10 iters), loss = 7.26382
I0523 08:39:27.403074 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26382 (* 1 = 7.26382 loss)
I0523 08:39:27.415586 35003 sgd_solver.cpp:112] Iteration 185840, lr = 0.001
I0523 08:39:31.593942 35003 solver.cpp:239] Iteration 185850 (2.38624 iter/s, 4.1907s/10 iters), loss = 6.49212
I0523 08:39:31.593984 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49212 (* 1 = 6.49212 loss)
I0523 08:39:32.263972 35003 sgd_solver.cpp:112] Iteration 185850, lr = 0.001
I0523 08:39:35.248863 35003 solver.cpp:239] Iteration 185860 (2.73618 iter/s, 3.65473s/10 iters), loss = 6.21069
I0523 08:39:35.248909 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21069 (* 1 = 6.21069 loss)
I0523 08:39:35.262564 35003 sgd_solver.cpp:112] Iteration 185860, lr = 0.001
I0523 08:39:38.838661 35003 solver.cpp:239] Iteration 185870 (2.78582 iter/s, 3.5896s/10 iters), loss = 5.46651
I0523 08:39:38.838943 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.46651 (* 1 = 5.46651 loss)
I0523 08:39:38.844240 35003 sgd_solver.cpp:112] Iteration 185870, lr = 0.001
I0523 08:39:42.922001 35003 solver.cpp:239] Iteration 185880 (2.4493 iter/s, 4.08279s/10 iters), loss = 7.06746
I0523 08:39:42.922044 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.06746 (* 1 = 7.06746 loss)
I0523 08:39:42.926257 35003 sgd_solver.cpp:112] Iteration 185880, lr = 0.001
I0523 08:39:43.739531 35003 solver.cpp:239] Iteration 185890 (12.2333 iter/s, 0.817438s/10 iters), loss = 6.87835
I0523 08:39:43.739575 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87835 (* 1 = 6.87835 loss)
I0523 08:39:43.745741 35003 sgd_solver.cpp:112] Iteration 185890, lr = 0.001
I0523 08:39:47.886128 35003 solver.cpp:239] Iteration 185900 (2.41174 iter/s, 4.14638s/10 iters), loss = 5.48236
I0523 08:39:47.886170 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.48236 (* 1 = 5.48236 loss)
I0523 08:39:47.959345 35003 sgd_solver.cpp:112] Iteration 185900, lr = 0.001
I0523 08:39:50.028007 35003 solver.cpp:239] Iteration 185910 (4.6691 iter/s, 2.14174s/10 iters), loss = 7.21689
I0523 08:39:50.028050 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21689 (* 1 = 7.21689 loss)
I0523 08:39:50.045318 35003 sgd_solver.cpp:112] Iteration 185910, lr = 0.001
I0523 08:39:54.262537 35003 solver.cpp:239] Iteration 185920 (2.36166 iter/s, 4.23432s/10 iters), loss = 6.55353
I0523 08:39:54.262574 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55353 (* 1 = 6.55353 loss)
I0523 08:39:54.275398 35003 sgd_solver.cpp:112] Iteration 185920, lr = 0.001
I0523 08:39:58.134973 35003 solver.cpp:239] Iteration 185930 (2.58249 iter/s, 3.87224s/10 iters), loss = 6.73425
I0523 08:39:58.135020 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73425 (* 1 = 6.73425 loss)
I0523 08:39:58.145679 35003 sgd_solver.cpp:112] Iteration 185930, lr = 0.001
I0523 08:40:02.365950 35003 solver.cpp:239] Iteration 185940 (2.36364 iter/s, 4.23076s/10 iters), loss = 5.40876
I0523 08:40:02.365995 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.40876 (* 1 = 5.40876 loss)
I0523 08:40:02.384560 35003 sgd_solver.cpp:112] Iteration 185940, lr = 0.001
I0523 08:40:05.881500 35003 solver.cpp:239] Iteration 185950 (2.84466 iter/s, 3.51536s/10 iters), loss = 6.22212
I0523 08:40:05.881539 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22212 (* 1 = 6.22212 loss)
I0523 08:40:05.888142 35003 sgd_solver.cpp:112] Iteration 185950, lr = 0.001
I0523 08:40:09.144480 35003 solver.cpp:239] Iteration 185960 (3.06485 iter/s, 3.2628s/10 iters), loss = 8.04673
I0523 08:40:09.144698 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.04673 (* 1 = 8.04673 loss)
I0523 08:40:09.147969 35003 sgd_solver.cpp:112] Iteration 185960, lr = 0.001
I0523 08:40:10.463810 35003 solver.cpp:239] Iteration 185970 (7.58119 iter/s, 1.31905s/10 iters), loss = 7.14516
I0523 08:40:10.463860 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14516 (* 1 = 7.14516 loss)
I0523 08:40:10.483031 35003 sgd_solver.cpp:112] Iteration 185970, lr = 0.001
I0523 08:40:13.303261 35003 solver.cpp:239] Iteration 185980 (3.52202 iter/s, 2.83928s/10 iters), loss = 6.21584
I0523 08:40:13.303305 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21584 (* 1 = 6.21584 loss)
I0523 08:40:13.309381 35003 sgd_solver.cpp:112] Iteration 185980, lr = 0.001
I0523 08:40:17.254536 35003 solver.cpp:239] Iteration 185990 (2.53096 iter/s, 3.95107s/10 iters), loss = 7.28673
I0523 08:40:17.254585 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.28673 (* 1 = 7.28673 loss)
I0523 08:40:17.262136 35003 sgd_solver.cpp:112] Iteration 185990, lr = 0.001
I0523 08:40:20.968014 35003 solver.cpp:239] Iteration 186000 (2.69304 iter/s, 3.71327s/10 iters), loss = 6.60314
I0523 08:40:20.968065 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.60314 (* 1 = 6.60314 loss)
I0523 08:40:21.682792 35003 sgd_solver.cpp:112] Iteration 186000, lr = 0.001
I0523 08:40:24.249966 35003 solver.cpp:239] Iteration 186010 (3.04714 iter/s, 3.28176s/10 iters), loss = 6.03497
I0523 08:40:24.250010 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.03497 (* 1 = 6.03497 loss)
I0523 08:40:24.964349 35003 sgd_solver.cpp:112] Iteration 186010, lr = 0.001
I0523 08:40:27.841799 35003 solver.cpp:239] Iteration 186020 (2.78425 iter/s, 3.59163s/10 iters), loss = 6.00398
I0523 08:40:27.841862 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.00398 (* 1 = 6.00398 loss)
I0523 08:40:27.849750 35003 sgd_solver.cpp:112] Iteration 186020, lr = 0.001
I0523 08:40:32.581795 35003 solver.cpp:239] Iteration 186030 (2.10982 iter/s, 4.73974s/10 iters), loss = 6.58028
I0523 08:40:32.581859 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58028 (* 1 = 6.58028 loss)
I0523 08:40:33.296699 35003 sgd_solver.cpp:112] Iteration 186030, lr = 0.001
I0523 08:40:37.121407 35003 solver.cpp:239] Iteration 186040 (2.20295 iter/s, 4.53936s/10 iters), loss = 6.70269
I0523 08:40:37.121461 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70269 (* 1 = 6.70269 loss)
I0523 08:40:37.802539 35003 sgd_solver.cpp:112] Iteration 186040, lr = 0.001
I0523 08:40:42.188030 35003 solver.cpp:239] Iteration 186050 (1.9738 iter/s, 5.06636s/10 iters), loss = 6.44589
I0523 08:40:42.188148 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44589 (* 1 = 6.44589 loss)
I0523 08:40:42.198606 35003 sgd_solver.cpp:112] Iteration 186050, lr = 0.001
I0523 08:40:45.833129 35003 solver.cpp:239] Iteration 186060 (2.74361 iter/s, 3.64483s/10 iters), loss = 6.06851
I0523 08:40:45.833173 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06851 (* 1 = 6.06851 loss)
I0523 08:40:45.855439 35003 sgd_solver.cpp:112] Iteration 186060, lr = 0.001
I0523 08:40:49.250691 35003 solver.cpp:239] Iteration 186070 (2.92623 iter/s, 3.41737s/10 iters), loss = 6.9212
I0523 08:40:49.250774 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9212 (* 1 = 6.9212 loss)
I0523 08:40:49.985512 35003 sgd_solver.cpp:112] Iteration 186070, lr = 0.001
I0523 08:40:54.578369 35003 solver.cpp:239] Iteration 186080 (1.8771 iter/s, 5.32738s/10 iters), loss = 6.1386
I0523 08:40:54.578421 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1386 (* 1 = 6.1386 loss)
I0523 08:40:54.586096 35003 sgd_solver.cpp:112] Iteration 186080, lr = 0.001
I0523 08:40:59.074594 35003 solver.cpp:239] Iteration 186090 (2.22421 iter/s, 4.49598s/10 iters), loss = 6.89295
I0523 08:40:59.074646 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89295 (* 1 = 6.89295 loss)
I0523 08:40:59.516439 35003 sgd_solver.cpp:112] Iteration 186090, lr = 0.001
I0523 08:41:02.316515 35003 solver.cpp:239] Iteration 186100 (3.08478 iter/s, 3.24172s/10 iters), loss = 4.78641
I0523 08:41:02.316557 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.78641 (* 1 = 4.78641 loss)
I0523 08:41:03.038554 35003 sgd_solver.cpp:112] Iteration 186100, lr = 0.001
I0523 08:41:05.167923 35003 solver.cpp:239] Iteration 186110 (3.50724 iter/s, 2.85124s/10 iters), loss = 7.00217
I0523 08:41:05.167966 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00217 (* 1 = 7.00217 loss)
I0523 08:41:05.180580 35003 sgd_solver.cpp:112] Iteration 186110, lr = 0.001
I0523 08:41:07.153664 35003 solver.cpp:239] Iteration 186120 (5.03623 iter/s, 1.98561s/10 iters), loss = 6.55831
I0523 08:41:07.153700 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55831 (* 1 = 6.55831 loss)
I0523 08:41:07.159443 35003 sgd_solver.cpp:112] Iteration 186120, lr = 0.001
I0523 08:41:09.910683 35003 solver.cpp:239] Iteration 186130 (3.62731 iter/s, 2.75686s/10 iters), loss = 6.13455
I0523 08:41:09.910743 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13455 (* 1 = 6.13455 loss)
I0523 08:41:09.918684 35003 sgd_solver.cpp:112] Iteration 186130, lr = 0.001
I0523 08:41:13.386976 35003 solver.cpp:239] Iteration 186140 (2.8768 iter/s, 3.47608s/10 iters), loss = 5.24819
I0523 08:41:13.387158 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.24819 (* 1 = 5.24819 loss)
I0523 08:41:13.388118 35003 sgd_solver.cpp:112] Iteration 186140, lr = 0.001
I0523 08:41:16.147121 35003 solver.cpp:239] Iteration 186150 (3.6234 iter/s, 2.75984s/10 iters), loss = 6.2607
I0523 08:41:16.147168 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2607 (* 1 = 6.2607 loss)
I0523 08:41:16.831187 35003 sgd_solver.cpp:112] Iteration 186150, lr = 0.001
I0523 08:41:20.509443 35003 solver.cpp:239] Iteration 186160 (2.29247 iter/s, 4.3621s/10 iters), loss = 6.29962
I0523 08:41:20.509485 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29962 (* 1 = 6.29962 loss)
I0523 08:41:20.522902 35003 sgd_solver.cpp:112] Iteration 186160, lr = 0.001
I0523 08:41:24.810139 35003 solver.cpp:239] Iteration 186170 (2.32532 iter/s, 4.30048s/10 iters), loss = 6.37864
I0523 08:41:24.810194 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37864 (* 1 = 6.37864 loss)
I0523 08:41:25.531023 35003 sgd_solver.cpp:112] Iteration 186170, lr = 0.001
I0523 08:41:28.105507 35003 solver.cpp:239] Iteration 186180 (3.03474 iter/s, 3.29518s/10 iters), loss = 6.35405
I0523 08:41:28.105545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35405 (* 1 = 6.35405 loss)
I0523 08:41:28.538897 35003 sgd_solver.cpp:112] Iteration 186180, lr = 0.001
I0523 08:41:31.454588 35003 solver.cpp:239] Iteration 186190 (2.98606 iter/s, 3.34889s/10 iters), loss = 5.71876
I0523 08:41:31.454649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.71876 (* 1 = 5.71876 loss)
I0523 08:41:31.461807 35003 sgd_solver.cpp:112] Iteration 186190, lr = 0.001
I0523 08:41:34.289265 35003 solver.cpp:239] Iteration 186200 (3.52796 iter/s, 2.8345s/10 iters), loss = 6.79839
I0523 08:41:34.289324 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79839 (* 1 = 6.79839 loss)
I0523 08:41:35.004815 35003 sgd_solver.cpp:112] Iteration 186200, lr = 0.001
I0523 08:41:37.435422 35003 solver.cpp:239] Iteration 186210 (3.17867 iter/s, 3.14597s/10 iters), loss = 6.05168
I0523 08:41:37.435472 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.05168 (* 1 = 6.05168 loss)
I0523 08:41:37.448261 35003 sgd_solver.cpp:112] Iteration 186210, lr = 0.001
I0523 08:41:40.907682 35003 solver.cpp:239] Iteration 186220 (2.88013 iter/s, 3.47206s/10 iters), loss = 6.35767
I0523 08:41:40.907722 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.35767 (* 1 = 6.35767 loss)
I0523 08:41:40.931936 35003 sgd_solver.cpp:112] Iteration 186220, lr = 0.001
I0523 08:41:44.283289 35003 solver.cpp:239] Iteration 186230 (2.96259 iter/s, 3.37542s/10 iters), loss = 7.26204
I0523 08:41:44.283440 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.26204 (* 1 = 7.26204 loss)
I0523 08:41:44.885411 35003 sgd_solver.cpp:112] Iteration 186230, lr = 0.001
I0523 08:41:49.217702 35003 solver.cpp:239] Iteration 186240 (2.02673 iter/s, 4.93406s/10 iters), loss = 6.06605
I0523 08:41:49.217762 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06605 (* 1 = 6.06605 loss)
I0523 08:41:49.958725 35003 sgd_solver.cpp:112] Iteration 186240, lr = 0.001
I0523 08:41:54.398785 35003 solver.cpp:239] Iteration 186250 (1.9302 iter/s, 5.18081s/10 iters), loss = 6.52966
I0523 08:41:54.398856 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52966 (* 1 = 6.52966 loss)
I0523 08:41:55.008697 35003 sgd_solver.cpp:112] Iteration 186250, lr = 0.001
I0523 08:41:59.434461 35003 solver.cpp:239] Iteration 186260 (1.98594 iter/s, 5.0354s/10 iters), loss = 6.19877
I0523 08:41:59.434514 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19877 (* 1 = 6.19877 loss)
I0523 08:42:00.175217 35003 sgd_solver.cpp:112] Iteration 186260, lr = 0.001
I0523 08:42:03.682318 35003 solver.cpp:239] Iteration 186270 (2.35425 iter/s, 4.24763s/10 iters), loss = 7.20335
I0523 08:42:03.682364 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20335 (* 1 = 7.20335 loss)
I0523 08:42:03.786691 35003 sgd_solver.cpp:112] Iteration 186270, lr = 0.001
I0523 08:42:08.010033 35003 solver.cpp:239] Iteration 186280 (2.31081 iter/s, 4.32749s/10 iters), loss = 6.30758
I0523 08:42:08.010082 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30758 (* 1 = 6.30758 loss)
I0523 08:42:08.014911 35003 sgd_solver.cpp:112] Iteration 186280, lr = 0.001
I0523 08:42:13.112857 35003 solver.cpp:239] Iteration 186290 (1.95983 iter/s, 5.10247s/10 iters), loss = 7.61428
I0523 08:42:13.112924 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61428 (* 1 = 7.61428 loss)
I0523 08:42:13.120807 35003 sgd_solver.cpp:112] Iteration 186290, lr = 0.001
I0523 08:42:18.127271 35003 solver.cpp:239] Iteration 186300 (1.99436 iter/s, 5.01415s/10 iters), loss = 6.6661
I0523 08:42:18.127535 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.6661 (* 1 = 6.6661 loss)
I0523 08:42:18.131697 35003 sgd_solver.cpp:112] Iteration 186300, lr = 0.001
I0523 08:42:21.790752 35003 solver.cpp:239] Iteration 186310 (2.73154 iter/s, 3.66094s/10 iters), loss = 5.82012
I0523 08:42:21.790809 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.82012 (* 1 = 5.82012 loss)
I0523 08:42:21.798137 35003 sgd_solver.cpp:112] Iteration 186310, lr = 0.001
I0523 08:42:24.918220 35003 solver.cpp:239] Iteration 186320 (3.19766 iter/s, 3.12728s/10 iters), loss = 7.25839
I0523 08:42:24.918258 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25839 (* 1 = 7.25839 loss)
I0523 08:42:24.931840 35003 sgd_solver.cpp:112] Iteration 186320, lr = 0.001
I0523 08:42:27.013973 35003 solver.cpp:239] Iteration 186330 (4.77186 iter/s, 2.09562s/10 iters), loss = 6.87664
I0523 08:42:27.014016 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87664 (* 1 = 6.87664 loss)
I0523 08:42:27.689548 35003 sgd_solver.cpp:112] Iteration 186330, lr = 0.001
I0523 08:42:30.606055 35003 solver.cpp:239] Iteration 186340 (2.78406 iter/s, 3.59188s/10 iters), loss = 6.4145
I0523 08:42:30.606106 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4145 (* 1 = 6.4145 loss)
I0523 08:42:30.897347 35003 sgd_solver.cpp:112] Iteration 186340, lr = 0.001
I0523 08:42:33.966199 35003 solver.cpp:239] Iteration 186350 (2.97624 iter/s, 3.35995s/10 iters), loss = 7.18844
I0523 08:42:33.966243 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18844 (* 1 = 7.18844 loss)
I0523 08:42:33.974645 35003 sgd_solver.cpp:112] Iteration 186350, lr = 0.001
I0523 08:42:37.576625 35003 solver.cpp:239] Iteration 186360 (2.76991 iter/s, 3.61023s/10 iters), loss = 6.4412
I0523 08:42:37.576679 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4412 (* 1 = 6.4412 loss)
I0523 08:42:37.587512 35003 sgd_solver.cpp:112] Iteration 186360, lr = 0.001
I0523 08:42:40.899754 35003 solver.cpp:239] Iteration 186370 (3.00939 iter/s, 3.32293s/10 iters), loss = 7.55234
I0523 08:42:40.899802 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.55234 (* 1 = 7.55234 loss)
I0523 08:42:40.912945 35003 sgd_solver.cpp:112] Iteration 186370, lr = 0.001
I0523 08:42:43.075309 35003 solver.cpp:239] Iteration 186380 (4.59684 iter/s, 2.17541s/10 iters), loss = 7.60633
I0523 08:42:43.075354 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60633 (* 1 = 7.60633 loss)
I0523 08:42:43.090222 35003 sgd_solver.cpp:112] Iteration 186380, lr = 0.001
I0523 08:42:45.882110 35003 solver.cpp:239] Iteration 186390 (3.56299 iter/s, 2.80663s/10 iters), loss = 6.75398
I0523 08:42:45.882149 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75398 (* 1 = 6.75398 loss)
I0523 08:42:45.904850 35003 sgd_solver.cpp:112] Iteration 186390, lr = 0.001
I0523 08:42:48.260581 35003 solver.cpp:239] Iteration 186400 (4.20464 iter/s, 2.37832s/10 iters), loss = 5.89328
I0523 08:42:48.260913 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.89328 (* 1 = 5.89328 loss)
I0523 08:42:48.265259 35003 sgd_solver.cpp:112] Iteration 186400, lr = 0.001
I0523 08:42:51.826870 35003 solver.cpp:239] Iteration 186410 (2.80439 iter/s, 3.56583s/10 iters), loss = 6.56869
I0523 08:42:51.826908 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56869 (* 1 = 6.56869 loss)
I0523 08:42:51.840121 35003 sgd_solver.cpp:112] Iteration 186410, lr = 0.001
I0523 08:42:53.921201 35003 solver.cpp:239] Iteration 186420 (4.77512 iter/s, 2.09419s/10 iters), loss = 6.01177
I0523 08:42:53.921264 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01177 (* 1 = 6.01177 loss)
I0523 08:42:53.928933 35003 sgd_solver.cpp:112] Iteration 186420, lr = 0.001
I0523 08:42:56.638628 35003 solver.cpp:239] Iteration 186430 (3.68022 iter/s, 2.71723s/10 iters), loss = 7.0653
I0523 08:42:56.638667 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.0653 (* 1 = 7.0653 loss)
I0523 08:42:56.657055 35003 sgd_solver.cpp:112] Iteration 186430, lr = 0.001
I0523 08:42:59.498605 35003 solver.cpp:239] Iteration 186440 (3.49673 iter/s, 2.85981s/10 iters), loss = 5.53101
I0523 08:42:59.498664 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.53101 (* 1 = 5.53101 loss)
I0523 08:42:59.516870 35003 sgd_solver.cpp:112] Iteration 186440, lr = 0.001
I0523 08:43:03.516973 35003 solver.cpp:239] Iteration 186450 (2.48871 iter/s, 4.01815s/10 iters), loss = 6.4218
I0523 08:43:03.517009 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4218 (* 1 = 6.4218 loss)
I0523 08:43:03.543336 35003 sgd_solver.cpp:112] Iteration 186450, lr = 0.001
I0523 08:43:09.053681 35003 solver.cpp:239] Iteration 186460 (1.80621 iter/s, 5.53645s/10 iters), loss = 6.96005
I0523 08:43:09.053726 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.96005 (* 1 = 6.96005 loss)
I0523 08:43:09.775681 35003 sgd_solver.cpp:112] Iteration 186460, lr = 0.001
I0523 08:43:14.087218 35003 solver.cpp:239] Iteration 186470 (1.98678 iter/s, 5.03328s/10 iters), loss = 7.08536
I0523 08:43:14.087272 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08536 (* 1 = 7.08536 loss)
I0523 08:43:14.828244 35003 sgd_solver.cpp:112] Iteration 186470, lr = 0.001
I0523 08:43:19.499074 35003 solver.cpp:239] Iteration 186480 (1.84789 iter/s, 5.41158s/10 iters), loss = 6.68984
I0523 08:43:19.499183 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68984 (* 1 = 6.68984 loss)
I0523 08:43:19.508611 35003 sgd_solver.cpp:112] Iteration 186480, lr = 0.001
I0523 08:43:23.577563 35003 solver.cpp:239] Iteration 186490 (2.45205 iter/s, 4.07821s/10 iters), loss = 6.76863
I0523 08:43:23.577610 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76863 (* 1 = 6.76863 loss)
I0523 08:43:23.585614 35003 sgd_solver.cpp:112] Iteration 186490, lr = 0.001
I0523 08:43:26.438026 35003 solver.cpp:239] Iteration 186500 (3.49615 iter/s, 2.86029s/10 iters), loss = 6.58059
I0523 08:43:26.438068 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58059 (* 1 = 6.58059 loss)
I0523 08:43:26.451375 35003 sgd_solver.cpp:112] Iteration 186500, lr = 0.001
I0523 08:43:29.970809 35003 solver.cpp:239] Iteration 186510 (2.83079 iter/s, 3.53259s/10 iters), loss = 7.82728
I0523 08:43:29.970849 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.82728 (* 1 = 7.82728 loss)
I0523 08:43:29.983691 35003 sgd_solver.cpp:112] Iteration 186510, lr = 0.001
I0523 08:43:33.479353 35003 solver.cpp:239] Iteration 186520 (2.85034 iter/s, 3.50835s/10 iters), loss = 6.74541
I0523 08:43:33.479406 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74541 (* 1 = 6.74541 loss)
I0523 08:43:33.490619 35003 sgd_solver.cpp:112] Iteration 186520, lr = 0.001
I0523 08:43:36.869807 35003 solver.cpp:239] Iteration 186530 (2.94963 iter/s, 3.39026s/10 iters), loss = 6.43944
I0523 08:43:36.869863 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43944 (* 1 = 6.43944 loss)
I0523 08:43:36.884521 35003 sgd_solver.cpp:112] Iteration 186530, lr = 0.001
I0523 08:43:41.025847 35003 solver.cpp:239] Iteration 186540 (2.40627 iter/s, 4.15581s/10 iters), loss = 7.62971
I0523 08:43:41.025923 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.62971 (* 1 = 7.62971 loss)
I0523 08:43:41.766738 35003 sgd_solver.cpp:112] Iteration 186540, lr = 0.001
I0523 08:43:44.064368 35003 solver.cpp:239] Iteration 186550 (3.29129 iter/s, 3.03832s/10 iters), loss = 5.00257
I0523 08:43:44.064419 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.00257 (* 1 = 5.00257 loss)
I0523 08:43:44.080971 35003 sgd_solver.cpp:112] Iteration 186550, lr = 0.001
I0523 08:43:49.007494 35003 solver.cpp:239] Iteration 186560 (2.02312 iter/s, 4.94287s/10 iters), loss = 6.5804
I0523 08:43:49.007542 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5804 (* 1 = 6.5804 loss)
I0523 08:43:49.018991 35003 sgd_solver.cpp:112] Iteration 186560, lr = 0.001
I0523 08:43:52.594748 35003 solver.cpp:239] Iteration 186570 (2.78781 iter/s, 3.58704s/10 iters), loss = 6.70945
I0523 08:43:52.595001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.70945 (* 1 = 6.70945 loss)
I0523 08:43:52.774169 35003 sgd_solver.cpp:112] Iteration 186570, lr = 0.001
I0523 08:43:55.706826 35003 solver.cpp:239] Iteration 186580 (3.21367 iter/s, 3.1117s/10 iters), loss = 6.47833
I0523 08:43:55.706869 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47833 (* 1 = 6.47833 loss)
I0523 08:43:56.395643 35003 sgd_solver.cpp:112] Iteration 186580, lr = 0.001
I0523 08:44:00.851780 35003 solver.cpp:239] Iteration 186590 (1.94375 iter/s, 5.1447s/10 iters), loss = 6.02426
I0523 08:44:00.851824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02426 (* 1 = 6.02426 loss)
I0523 08:44:01.587947 35003 sgd_solver.cpp:112] Iteration 186590, lr = 0.001
I0523 08:44:05.765372 35003 solver.cpp:239] Iteration 186600 (2.03527 iter/s, 4.91335s/10 iters), loss = 6.42087
I0523 08:44:05.765422 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42087 (* 1 = 6.42087 loss)
I0523 08:44:06.500155 35003 sgd_solver.cpp:112] Iteration 186600, lr = 0.001
I0523 08:44:09.260073 35003 solver.cpp:239] Iteration 186610 (2.86163 iter/s, 3.49451s/10 iters), loss = 7.07342
I0523 08:44:09.260113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07342 (* 1 = 7.07342 loss)
I0523 08:44:09.288065 35003 sgd_solver.cpp:112] Iteration 186610, lr = 0.001
I0523 08:44:12.093943 35003 solver.cpp:239] Iteration 186620 (3.52894 iter/s, 2.83371s/10 iters), loss = 5.05894
I0523 08:44:12.093983 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.05894 (* 1 = 5.05894 loss)
I0523 08:44:12.107316 35003 sgd_solver.cpp:112] Iteration 186620, lr = 0.001
I0523 08:44:14.115438 35003 solver.cpp:239] Iteration 186630 (4.94715 iter/s, 2.02137s/10 iters), loss = 7.43418
I0523 08:44:14.115474 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.43418 (* 1 = 7.43418 loss)
I0523 08:44:14.123510 35003 sgd_solver.cpp:112] Iteration 186630, lr = 0.001
I0523 08:44:17.375584 35003 solver.cpp:239] Iteration 186640 (3.06751 iter/s, 3.25998s/10 iters), loss = 6.85838
I0523 08:44:17.375622 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85838 (* 1 = 6.85838 loss)
I0523 08:44:17.389205 35003 sgd_solver.cpp:112] Iteration 186640, lr = 0.001
I0523 08:44:20.970585 35003 solver.cpp:239] Iteration 186650 (2.78179 iter/s, 3.59481s/10 iters), loss = 6.58059
I0523 08:44:20.970625 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.58059 (* 1 = 6.58059 loss)
I0523 08:44:20.975533 35003 sgd_solver.cpp:112] Iteration 186650, lr = 0.001
I0523 08:44:24.372792 35003 solver.cpp:239] Iteration 186660 (2.93944 iter/s, 3.402s/10 iters), loss = 6.0726
I0523 08:44:24.373010 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0726 (* 1 = 6.0726 loss)
I0523 08:44:24.378307 35003 sgd_solver.cpp:112] Iteration 186660, lr = 0.001
I0523 08:44:27.992569 35003 solver.cpp:239] Iteration 186670 (2.76286 iter/s, 3.61944s/10 iters), loss = 6.59379
I0523 08:44:27.992619 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59379 (* 1 = 6.59379 loss)
I0523 08:44:28.032930 35003 sgd_solver.cpp:112] Iteration 186670, lr = 0.001
I0523 08:44:30.066138 35003 solver.cpp:239] Iteration 186680 (4.82293 iter/s, 2.07343s/10 iters), loss = 6.41271
I0523 08:44:30.066181 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41271 (* 1 = 6.41271 loss)
I0523 08:44:30.074811 35003 sgd_solver.cpp:112] Iteration 186680, lr = 0.001
I0523 08:44:32.193384 35003 solver.cpp:239] Iteration 186690 (4.70121 iter/s, 2.12711s/10 iters), loss = 6.25414
I0523 08:44:32.193429 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25414 (* 1 = 6.25414 loss)
I0523 08:44:32.934051 35003 sgd_solver.cpp:112] Iteration 186690, lr = 0.001
I0523 08:44:35.754310 35003 solver.cpp:239] Iteration 186700 (2.80841 iter/s, 3.56073s/10 iters), loss = 6.28864
I0523 08:44:35.754350 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28864 (* 1 = 6.28864 loss)
I0523 08:44:35.757699 35003 sgd_solver.cpp:112] Iteration 186700, lr = 0.001
I0523 08:44:39.303058 35003 solver.cpp:239] Iteration 186710 (2.8181 iter/s, 3.54849s/10 iters), loss = 6.42028
I0523 08:44:39.303131 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42028 (* 1 = 6.42028 loss)
I0523 08:44:40.040267 35003 sgd_solver.cpp:112] Iteration 186710, lr = 0.001
I0523 08:44:42.111173 35003 solver.cpp:239] Iteration 186720 (3.56135 iter/s, 2.80793s/10 iters), loss = 6.33831
I0523 08:44:42.111213 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33831 (* 1 = 6.33831 loss)
I0523 08:44:42.130239 35003 sgd_solver.cpp:112] Iteration 186720, lr = 0.001
I0523 08:44:44.839836 35003 solver.cpp:239] Iteration 186730 (3.665 iter/s, 2.72851s/10 iters), loss = 6.9946
I0523 08:44:44.839884 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9946 (* 1 = 6.9946 loss)
I0523 08:44:44.849890 35003 sgd_solver.cpp:112] Iteration 186730, lr = 0.001
I0523 08:44:46.966331 35003 solver.cpp:239] Iteration 186740 (4.7029 iter/s, 2.12635s/10 iters), loss = 6.01526
I0523 08:44:46.966379 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.01526 (* 1 = 6.01526 loss)
I0523 08:44:47.669050 35003 sgd_solver.cpp:112] Iteration 186740, lr = 0.001
I0523 08:44:50.511548 35003 solver.cpp:239] Iteration 186750 (2.82086 iter/s, 3.54502s/10 iters), loss = 5.85885
I0523 08:44:50.511584 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.85885 (* 1 = 5.85885 loss)
I0523 08:44:50.524766 35003 sgd_solver.cpp:112] Iteration 186750, lr = 0.001
I0523 08:44:53.302011 35003 solver.cpp:239] Iteration 186760 (3.58384 iter/s, 2.7903s/10 iters), loss = 6.56504
I0523 08:44:53.302049 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56504 (* 1 = 6.56504 loss)
I0523 08:44:53.315385 35003 sgd_solver.cpp:112] Iteration 186760, lr = 0.001
I0523 08:44:56.316083 35003 solver.cpp:239] Iteration 186770 (3.31795 iter/s, 3.01391s/10 iters), loss = 6.76385
I0523 08:44:56.316270 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76385 (* 1 = 6.76385 loss)
I0523 08:44:57.029911 35003 sgd_solver.cpp:112] Iteration 186770, lr = 0.001
I0523 08:45:00.383111 35003 solver.cpp:239] Iteration 186780 (2.45901 iter/s, 4.06668s/10 iters), loss = 7.21634
I0523 08:45:00.383152 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21634 (* 1 = 7.21634 loss)
I0523 08:45:00.401695 35003 sgd_solver.cpp:112] Iteration 186780, lr = 0.001
I0523 08:45:03.922619 35003 solver.cpp:239] Iteration 186790 (2.82541 iter/s, 3.53931s/10 iters), loss = 6.06629
I0523 08:45:03.922659 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.06629 (* 1 = 6.06629 loss)
I0523 08:45:03.927435 35003 sgd_solver.cpp:112] Iteration 186790, lr = 0.001
I0523 08:45:06.377614 35003 solver.cpp:239] Iteration 186800 (4.07357 iter/s, 2.45485s/10 iters), loss = 6.74352
I0523 08:45:06.377655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74352 (* 1 = 6.74352 loss)
I0523 08:45:06.841028 35003 sgd_solver.cpp:112] Iteration 186800, lr = 0.001
I0523 08:45:10.919735 35003 solver.cpp:239] Iteration 186810 (2.20173 iter/s, 4.54189s/10 iters), loss = 6.34852
I0523 08:45:10.919783 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34852 (* 1 = 6.34852 loss)
I0523 08:45:11.010560 35003 sgd_solver.cpp:112] Iteration 186810, lr = 0.001
I0523 08:45:13.754173 35003 solver.cpp:239] Iteration 186820 (3.52824 iter/s, 2.83427s/10 iters), loss = 6.307
I0523 08:45:13.754209 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.307 (* 1 = 6.307 loss)
I0523 08:45:13.767290 35003 sgd_solver.cpp:112] Iteration 186820, lr = 0.001
I0523 08:45:15.849722 35003 solver.cpp:239] Iteration 186830 (4.77234 iter/s, 2.09541s/10 iters), loss = 5.80923
I0523 08:45:15.849784 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.80923 (* 1 = 5.80923 loss)
I0523 08:45:15.972668 35003 sgd_solver.cpp:112] Iteration 186830, lr = 0.001
I0523 08:45:19.379786 35003 solver.cpp:239] Iteration 186840 (2.83298 iter/s, 3.52986s/10 iters), loss = 7.12724
I0523 08:45:19.379824 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.12724 (* 1 = 7.12724 loss)
I0523 08:45:19.393543 35003 sgd_solver.cpp:112] Iteration 186840, lr = 0.001
I0523 08:45:22.944159 35003 solver.cpp:239] Iteration 186850 (2.8057 iter/s, 3.56418s/10 iters), loss = 6.76154
I0523 08:45:22.944200 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76154 (* 1 = 6.76154 loss)
I0523 08:45:22.953835 35003 sgd_solver.cpp:112] Iteration 186850, lr = 0.001
I0523 08:45:24.946344 35003 solver.cpp:239] Iteration 186860 (4.99487 iter/s, 2.00206s/10 iters), loss = 6.1594
I0523 08:45:24.946382 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1594 (* 1 = 6.1594 loss)
I0523 08:45:24.952081 35003 sgd_solver.cpp:112] Iteration 186860, lr = 0.001
I0523 08:45:27.700204 35003 solver.cpp:239] Iteration 186870 (3.63147 iter/s, 2.7537s/10 iters), loss = 6.21539
I0523 08:45:27.700453 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.21539 (* 1 = 6.21539 loss)
I0523 08:45:28.356449 35003 sgd_solver.cpp:112] Iteration 186870, lr = 0.001
I0523 08:45:31.247155 35003 solver.cpp:239] Iteration 186880 (2.81962 iter/s, 3.54658s/10 iters), loss = 5.37915
I0523 08:45:31.247192 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.37915 (* 1 = 5.37915 loss)
I0523 08:45:31.260438 35003 sgd_solver.cpp:112] Iteration 186880, lr = 0.001
I0523 08:45:35.567728 35003 solver.cpp:239] Iteration 186890 (2.31463 iter/s, 4.32035s/10 iters), loss = 7.64694
I0523 08:45:35.567770 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.64694 (* 1 = 7.64694 loss)
I0523 08:45:35.572082 35003 sgd_solver.cpp:112] Iteration 186890, lr = 0.001
I0523 08:45:38.386512 35003 solver.cpp:239] Iteration 186900 (3.54786 iter/s, 2.8186s/10 iters), loss = 6.84303
I0523 08:45:38.386549 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.84303 (* 1 = 6.84303 loss)
I0523 08:45:38.400585 35003 sgd_solver.cpp:112] Iteration 186900, lr = 0.001
I0523 08:45:41.883435 35003 solver.cpp:239] Iteration 186910 (2.85981 iter/s, 3.49673s/10 iters), loss = 6.17247
I0523 08:45:41.883481 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17247 (* 1 = 6.17247 loss)
I0523 08:45:41.886660 35003 sgd_solver.cpp:112] Iteration 186910, lr = 0.001
I0523 08:45:45.129451 35003 solver.cpp:239] Iteration 186920 (3.08088 iter/s, 3.24583s/10 iters), loss = 6.41359
I0523 08:45:45.129498 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41359 (* 1 = 6.41359 loss)
I0523 08:45:45.138113 35003 sgd_solver.cpp:112] Iteration 186920, lr = 0.001
I0523 08:45:47.937649 35003 solver.cpp:239] Iteration 186930 (3.56121 iter/s, 2.80804s/10 iters), loss = 6.46919
I0523 08:45:47.937686 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46919 (* 1 = 6.46919 loss)
I0523 08:45:47.950951 35003 sgd_solver.cpp:112] Iteration 186930, lr = 0.001
I0523 08:45:52.208629 35003 solver.cpp:239] Iteration 186940 (2.3415 iter/s, 4.27077s/10 iters), loss = 6.0581
I0523 08:45:52.208668 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.0581 (* 1 = 6.0581 loss)
I0523 08:45:52.233129 35003 sgd_solver.cpp:112] Iteration 186940, lr = 0.001
I0523 08:45:55.756995 35003 solver.cpp:239] Iteration 186950 (2.81835 iter/s, 3.54818s/10 iters), loss = 6.98781
I0523 08:45:55.757038 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98781 (* 1 = 6.98781 loss)
I0523 08:45:56.472247 35003 sgd_solver.cpp:112] Iteration 186950, lr = 0.001
I0523 08:46:00.017978 35003 solver.cpp:239] Iteration 186960 (2.34701 iter/s, 4.26075s/10 iters), loss = 6.691
I0523 08:46:00.018254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.691 (* 1 = 6.691 loss)
I0523 08:46:00.031919 35003 sgd_solver.cpp:112] Iteration 186960, lr = 0.001
I0523 08:46:02.858861 35003 solver.cpp:239] Iteration 186970 (3.52049 iter/s, 2.84051s/10 iters), loss = 7.01677
I0523 08:46:02.858916 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.01677 (* 1 = 7.01677 loss)
I0523 08:46:03.596586 35003 sgd_solver.cpp:112] Iteration 186970, lr = 0.001
I0523 08:46:06.415215 35003 solver.cpp:239] Iteration 186980 (2.81203 iter/s, 3.55615s/10 iters), loss = 6.08288
I0523 08:46:06.415254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08288 (* 1 = 6.08288 loss)
I0523 08:46:06.422982 35003 sgd_solver.cpp:112] Iteration 186980, lr = 0.001
I0523 08:46:08.740792 35003 solver.cpp:239] Iteration 186990 (4.30027 iter/s, 2.32544s/10 iters), loss = 7.22319
I0523 08:46:08.740833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22319 (* 1 = 7.22319 loss)
I0523 08:46:09.441835 35003 sgd_solver.cpp:112] Iteration 186990, lr = 0.001
I0523 08:46:12.899448 35003 solver.cpp:239] Iteration 187000 (2.40475 iter/s, 4.15844s/10 iters), loss = 5.60397
I0523 08:46:12.899488 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.60397 (* 1 = 5.60397 loss)
I0523 08:46:12.911692 35003 sgd_solver.cpp:112] Iteration 187000, lr = 0.001
I0523 08:46:17.273957 35003 solver.cpp:239] Iteration 187010 (2.28609 iter/s, 4.37428s/10 iters), loss = 6.88718
I0523 08:46:17.274008 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88718 (* 1 = 6.88718 loss)
I0523 08:46:17.329537 35003 sgd_solver.cpp:112] Iteration 187010, lr = 0.001
I0523 08:46:23.031235 35003 solver.cpp:239] Iteration 187020 (1.73702 iter/s, 5.75699s/10 iters), loss = 6.82047
I0523 08:46:23.031282 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82047 (* 1 = 6.82047 loss)
I0523 08:46:23.041049 35003 sgd_solver.cpp:112] Iteration 187020, lr = 0.001
I0523 08:46:25.798280 35003 solver.cpp:239] Iteration 187030 (3.61418 iter/s, 2.76688s/10 iters), loss = 7.38161
I0523 08:46:25.798316 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.38161 (* 1 = 7.38161 loss)
I0523 08:46:25.821470 35003 sgd_solver.cpp:112] Iteration 187030, lr = 0.001
I0523 08:46:29.322763 35003 solver.cpp:239] Iteration 187040 (2.83745 iter/s, 3.52429s/10 iters), loss = 6.25236
I0523 08:46:29.322803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25236 (* 1 = 6.25236 loss)
I0523 08:46:29.335958 35003 sgd_solver.cpp:112] Iteration 187040, lr = 0.001
I0523 08:46:33.738577 35003 solver.cpp:239] Iteration 187050 (2.2647 iter/s, 4.41559s/10 iters), loss = 6.53045
I0523 08:46:33.738833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.53045 (* 1 = 6.53045 loss)
I0523 08:46:33.771325 35003 sgd_solver.cpp:112] Iteration 187050, lr = 0.001
I0523 08:46:35.954414 35003 solver.cpp:239] Iteration 187060 (4.51364 iter/s, 2.2155s/10 iters), loss = 7.31009
I0523 08:46:35.954459 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.31009 (* 1 = 7.31009 loss)
I0523 08:46:35.986488 35003 sgd_solver.cpp:112] Iteration 187060, lr = 0.001
I0523 08:46:38.803133 35003 solver.cpp:239] Iteration 187070 (3.51055 iter/s, 2.84855s/10 iters), loss = 6.72421
I0523 08:46:38.803174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72421 (* 1 = 6.72421 loss)
I0523 08:46:38.831063 35003 sgd_solver.cpp:112] Iteration 187070, lr = 0.001
I0523 08:46:40.507519 35003 solver.cpp:239] Iteration 187080 (5.86764 iter/s, 1.70426s/10 iters), loss = 6.59141
I0523 08:46:40.507560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59141 (* 1 = 6.59141 loss)
I0523 08:46:41.196671 35003 sgd_solver.cpp:112] Iteration 187080, lr = 0.001
I0523 08:46:43.868449 35003 solver.cpp:239] Iteration 187090 (2.97553 iter/s, 3.36075s/10 iters), loss = 6.44134
I0523 08:46:43.868489 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44134 (* 1 = 6.44134 loss)
I0523 08:46:43.880028 35003 sgd_solver.cpp:112] Iteration 187090, lr = 0.001
I0523 08:46:46.524623 35003 solver.cpp:239] Iteration 187100 (3.76504 iter/s, 2.65602s/10 iters), loss = 5.88574
I0523 08:46:46.524662 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.88574 (* 1 = 5.88574 loss)
I0523 08:46:46.537827 35003 sgd_solver.cpp:112] Iteration 187100, lr = 0.001
I0523 08:46:50.073535 35003 solver.cpp:239] Iteration 187110 (2.81791 iter/s, 3.54873s/10 iters), loss = 6.80314
I0523 08:46:50.073573 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80314 (* 1 = 6.80314 loss)
I0523 08:46:50.087510 35003 sgd_solver.cpp:112] Iteration 187110, lr = 0.001
I0523 08:46:52.908212 35003 solver.cpp:239] Iteration 187120 (3.52794 iter/s, 2.83452s/10 iters), loss = 7.60299
I0523 08:46:52.908258 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60299 (* 1 = 7.60299 loss)
I0523 08:46:52.920554 35003 sgd_solver.cpp:112] Iteration 187120, lr = 0.001
I0523 08:46:56.784704 35003 solver.cpp:239] Iteration 187130 (2.57979 iter/s, 3.87628s/10 iters), loss = 6.39906
I0523 08:46:56.784747 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39906 (* 1 = 6.39906 loss)
I0523 08:46:56.791306 35003 sgd_solver.cpp:112] Iteration 187130, lr = 0.001
I0523 08:47:00.459031 35003 solver.cpp:239] Iteration 187140 (2.72174 iter/s, 3.67413s/10 iters), loss = 6.78569
I0523 08:47:00.459071 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78569 (* 1 = 6.78569 loss)
I0523 08:47:00.633927 35003 sgd_solver.cpp:112] Iteration 187140, lr = 0.001
I0523 08:47:03.566599 35003 solver.cpp:239] Iteration 187150 (3.21813 iter/s, 3.1074s/10 iters), loss = 6.14493
I0523 08:47:03.566642 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14493 (* 1 = 6.14493 loss)
I0523 08:47:04.281890 35003 sgd_solver.cpp:112] Iteration 187150, lr = 0.001
I0523 08:47:06.195556 35003 solver.cpp:239] Iteration 187160 (3.80402 iter/s, 2.6288s/10 iters), loss = 6.33783
I0523 08:47:06.195598 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.33783 (* 1 = 6.33783 loss)
I0523 08:47:06.208323 35003 sgd_solver.cpp:112] Iteration 187160, lr = 0.001
I0523 08:47:08.926414 35003 solver.cpp:239] Iteration 187170 (3.66207 iter/s, 2.7307s/10 iters), loss = 6.25006
I0523 08:47:08.926453 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25006 (* 1 = 6.25006 loss)
I0523 08:47:08.937461 35003 sgd_solver.cpp:112] Iteration 187170, lr = 0.001
I0523 08:47:13.331321 35003 solver.cpp:239] Iteration 187180 (2.27031 iter/s, 4.40469s/10 iters), loss = 6.55716
I0523 08:47:13.331362 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.55716 (* 1 = 6.55716 loss)
I0523 08:47:13.342494 35003 sgd_solver.cpp:112] Iteration 187180, lr = 0.001
I0523 08:47:15.848826 35003 solver.cpp:239] Iteration 187190 (3.97243 iter/s, 2.51735s/10 iters), loss = 5.46878
I0523 08:47:15.848870 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.46878 (* 1 = 5.46878 loss)
I0523 08:47:15.862396 35003 sgd_solver.cpp:112] Iteration 187190, lr = 0.001
I0523 08:47:20.221906 35003 solver.cpp:239] Iteration 187200 (2.28684 iter/s, 4.37285s/10 iters), loss = 7.85497
I0523 08:47:20.221958 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.85497 (* 1 = 7.85497 loss)
I0523 08:47:20.230839 35003 sgd_solver.cpp:112] Iteration 187200, lr = 0.001
I0523 08:47:24.676689 35003 solver.cpp:239] Iteration 187210 (2.2449 iter/s, 4.45454s/10 iters), loss = 7.56054
I0523 08:47:24.676733 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.56054 (* 1 = 7.56054 loss)
I0523 08:47:24.754055 35003 sgd_solver.cpp:112] Iteration 187210, lr = 0.001
I0523 08:47:28.315289 35003 solver.cpp:239] Iteration 187220 (2.74847 iter/s, 3.63839s/10 iters), loss = 6.14381
I0523 08:47:28.315341 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14381 (* 1 = 6.14381 loss)
I0523 08:47:28.359596 35003 sgd_solver.cpp:112] Iteration 187220, lr = 0.001
I0523 08:47:31.543815 35003 solver.cpp:239] Iteration 187230 (3.09757 iter/s, 3.22834s/10 iters), loss = 6.26235
I0523 08:47:31.543856 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26235 (* 1 = 6.26235 loss)
I0523 08:47:31.547960 35003 sgd_solver.cpp:112] Iteration 187230, lr = 0.001
I0523 08:47:35.485626 35003 solver.cpp:239] Iteration 187240 (2.53705 iter/s, 3.94159s/10 iters), loss = 6.90975
I0523 08:47:35.485888 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90975 (* 1 = 6.90975 loss)
I0523 08:47:35.490988 35003 sgd_solver.cpp:112] Iteration 187240, lr = 0.001
I0523 08:47:38.286775 35003 solver.cpp:239] Iteration 187250 (3.57084 iter/s, 2.80046s/10 iters), loss = 7.09396
I0523 08:47:38.286849 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.09396 (* 1 = 7.09396 loss)
I0523 08:47:38.297457 35003 sgd_solver.cpp:112] Iteration 187250, lr = 0.001
I0523 08:47:43.201252 35003 solver.cpp:239] Iteration 187260 (2.03492 iter/s, 4.9142s/10 iters), loss = 6.24346
I0523 08:47:43.201313 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.24346 (* 1 = 6.24346 loss)
I0523 08:47:43.203047 35003 sgd_solver.cpp:112] Iteration 187260, lr = 0.001
I0523 08:47:46.202752 35003 solver.cpp:239] Iteration 187270 (3.33187 iter/s, 3.00132s/10 iters), loss = 6.78621
I0523 08:47:46.202792 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78621 (* 1 = 6.78621 loss)
I0523 08:47:46.818682 35003 sgd_solver.cpp:112] Iteration 187270, lr = 0.001
I0523 08:47:48.527180 35003 solver.cpp:239] Iteration 187280 (4.3024 iter/s, 2.32428s/10 iters), loss = 7.46595
I0523 08:47:48.527236 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.46595 (* 1 = 7.46595 loss)
I0523 08:47:49.267774 35003 sgd_solver.cpp:112] Iteration 187280, lr = 0.001
I0523 08:47:52.719313 35003 solver.cpp:239] Iteration 187290 (2.38559 iter/s, 4.19183s/10 iters), loss = 7.30969
I0523 08:47:52.719365 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.30969 (* 1 = 7.30969 loss)
I0523 08:47:52.723333 35003 sgd_solver.cpp:112] Iteration 187290, lr = 0.001
I0523 08:47:57.363972 35003 solver.cpp:239] Iteration 187300 (2.15313 iter/s, 4.6444s/10 iters), loss = 6.07274
I0523 08:47:57.364033 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.07274 (* 1 = 6.07274 loss)
I0523 08:47:58.083439 35003 sgd_solver.cpp:112] Iteration 187300, lr = 0.001
I0523 08:48:01.787921 35003 solver.cpp:239] Iteration 187310 (2.26055 iter/s, 4.42371s/10 iters), loss = 6.9351
I0523 08:48:01.787963 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9351 (* 1 = 6.9351 loss)
I0523 08:48:02.522940 35003 sgd_solver.cpp:112] Iteration 187310, lr = 0.001
I0523 08:48:04.579501 35003 solver.cpp:239] Iteration 187320 (3.58241 iter/s, 2.79142s/10 iters), loss = 6.93391
I0523 08:48:04.579541 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.93391 (* 1 = 6.93391 loss)
I0523 08:48:04.606909 35003 sgd_solver.cpp:112] Iteration 187320, lr = 0.001
I0523 08:48:09.325093 35003 solver.cpp:239] Iteration 187330 (2.10732 iter/s, 4.74536s/10 iters), loss = 6.82625
I0523 08:48:09.325304 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82625 (* 1 = 6.82625 loss)
I0523 08:48:09.337903 35003 sgd_solver.cpp:112] Iteration 187330, lr = 0.001
I0523 08:48:12.144170 35003 solver.cpp:239] Iteration 187340 (3.54764 iter/s, 2.81877s/10 iters), loss = 5.69834
I0523 08:48:12.144220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.69834 (* 1 = 5.69834 loss)
I0523 08:48:12.154517 35003 sgd_solver.cpp:112] Iteration 187340, lr = 0.001
I0523 08:48:15.135856 35003 solver.cpp:239] Iteration 187350 (3.34279 iter/s, 2.99151s/10 iters), loss = 5.69768
I0523 08:48:15.135902 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.69768 (* 1 = 5.69768 loss)
I0523 08:48:15.876485 35003 sgd_solver.cpp:112] Iteration 187350, lr = 0.001
I0523 08:48:18.642405 35003 solver.cpp:239] Iteration 187360 (2.85196 iter/s, 3.50635s/10 iters), loss = 7.34959
I0523 08:48:18.642451 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34959 (* 1 = 7.34959 loss)
I0523 08:48:18.649708 35003 sgd_solver.cpp:112] Iteration 187360, lr = 0.001
I0523 08:48:22.164088 35003 solver.cpp:239] Iteration 187370 (2.83971 iter/s, 3.52149s/10 iters), loss = 6.47109
I0523 08:48:22.164129 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47109 (* 1 = 6.47109 loss)
I0523 08:48:22.804430 35003 sgd_solver.cpp:112] Iteration 187370, lr = 0.001
I0523 08:48:26.654659 35003 solver.cpp:239] Iteration 187380 (2.227 iter/s, 4.49034s/10 iters), loss = 6.44109
I0523 08:48:26.654726 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44109 (* 1 = 6.44109 loss)
I0523 08:48:26.670428 35003 sgd_solver.cpp:112] Iteration 187380, lr = 0.001
I0523 08:48:30.201220 35003 solver.cpp:239] Iteration 187390 (2.8198 iter/s, 3.54635s/10 iters), loss = 7.11761
I0523 08:48:30.201261 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.11761 (* 1 = 7.11761 loss)
I0523 08:48:30.682456 35003 sgd_solver.cpp:112] Iteration 187390, lr = 0.001
I0523 08:48:32.886189 35003 solver.cpp:239] Iteration 187400 (3.72465 iter/s, 2.68481s/10 iters), loss = 6.41992
I0523 08:48:32.886229 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41992 (* 1 = 6.41992 loss)
I0523 08:48:32.890602 35003 sgd_solver.cpp:112] Iteration 187400, lr = 0.001
I0523 08:48:36.514324 35003 solver.cpp:239] Iteration 187410 (2.75639 iter/s, 3.62793s/10 iters), loss = 6.99246
I0523 08:48:36.514384 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99246 (* 1 = 6.99246 loss)
I0523 08:48:36.532546 35003 sgd_solver.cpp:112] Iteration 187410, lr = 0.001
I0523 08:48:39.300207 35003 solver.cpp:239] Iteration 187420 (3.58975 iter/s, 2.78571s/10 iters), loss = 6.48676
I0523 08:48:39.300257 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48676 (* 1 = 6.48676 loss)
I0523 08:48:39.313197 35003 sgd_solver.cpp:112] Iteration 187420, lr = 0.001
I0523 08:48:43.497862 35003 solver.cpp:239] Iteration 187430 (2.38241 iter/s, 4.19743s/10 iters), loss = 7.13436
I0523 08:48:43.498034 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.13436 (* 1 = 7.13436 loss)
I0523 08:48:44.232094 35003 sgd_solver.cpp:112] Iteration 187430, lr = 0.001
I0523 08:48:47.874339 35003 solver.cpp:239] Iteration 187440 (2.28513 iter/s, 4.37612s/10 iters), loss = 6.1979
I0523 08:48:47.874384 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1979 (* 1 = 6.1979 loss)
I0523 08:48:47.879717 35003 sgd_solver.cpp:112] Iteration 187440, lr = 0.001
I0523 08:48:51.090826 35003 solver.cpp:239] Iteration 187450 (3.10915 iter/s, 3.21631s/10 iters), loss = 5.84993
I0523 08:48:51.090862 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84993 (* 1 = 5.84993 loss)
I0523 08:48:51.109233 35003 sgd_solver.cpp:112] Iteration 187450, lr = 0.001
I0523 08:48:52.433115 35003 solver.cpp:239] Iteration 187460 (7.45054 iter/s, 1.34219s/10 iters), loss = 6.59673
I0523 08:48:52.433156 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59673 (* 1 = 6.59673 loss)
I0523 08:48:52.445983 35003 sgd_solver.cpp:112] Iteration 187460, lr = 0.001
I0523 08:48:56.716619 35003 solver.cpp:239] Iteration 187470 (2.33466 iter/s, 4.28329s/10 iters), loss = 7.61145
I0523 08:48:56.716655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.61145 (* 1 = 7.61145 loss)
I0523 08:48:56.735007 35003 sgd_solver.cpp:112] Iteration 187470, lr = 0.001
I0523 08:48:59.424893 35003 solver.cpp:239] Iteration 187480 (3.6926 iter/s, 2.70812s/10 iters), loss = 5.45982
I0523 08:48:59.424947 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.45982 (* 1 = 5.45982 loss)
I0523 08:49:00.159420 35003 sgd_solver.cpp:112] Iteration 187480, lr = 0.001
I0523 08:49:03.743388 35003 solver.cpp:239] Iteration 187490 (2.31574 iter/s, 4.31827s/10 iters), loss = 5.70992
I0523 08:49:03.743427 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.70992 (* 1 = 5.70992 loss)
I0523 08:49:03.756001 35003 sgd_solver.cpp:112] Iteration 187490, lr = 0.001
I0523 08:49:08.333458 35003 solver.cpp:239] Iteration 187500 (2.17872 iter/s, 4.58984s/10 iters), loss = 5.84252
I0523 08:49:08.333513 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84252 (* 1 = 5.84252 loss)
I0523 08:49:08.341562 35003 sgd_solver.cpp:112] Iteration 187500, lr = 0.001
I0523 08:49:12.463204 35003 solver.cpp:239] Iteration 187510 (2.42159 iter/s, 4.12952s/10 iters), loss = 7.63687
I0523 08:49:12.463245 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.63687 (* 1 = 7.63687 loss)
I0523 08:49:12.465649 35003 sgd_solver.cpp:112] Iteration 187510, lr = 0.001
I0523 08:49:14.529945 35003 solver.cpp:239] Iteration 187520 (4.83887 iter/s, 2.0666s/10 iters), loss = 5.52043
I0523 08:49:14.530198 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.52043 (* 1 = 5.52043 loss)
I0523 08:49:14.533435 35003 sgd_solver.cpp:112] Iteration 187520, lr = 0.001
I0523 08:49:18.846282 35003 solver.cpp:239] Iteration 187530 (2.317 iter/s, 4.31592s/10 iters), loss = 5.9257
I0523 08:49:18.846324 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9257 (* 1 = 5.9257 loss)
I0523 08:49:18.859916 35003 sgd_solver.cpp:112] Iteration 187530, lr = 0.001
I0523 08:49:21.698088 35003 solver.cpp:239] Iteration 187540 (3.50675 iter/s, 2.85164s/10 iters), loss = 7.76048
I0523 08:49:21.698143 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76048 (* 1 = 7.76048 loss)
I0523 08:49:22.438127 35003 sgd_solver.cpp:112] Iteration 187540, lr = 0.001
I0523 08:49:24.550853 35003 solver.cpp:239] Iteration 187550 (3.50558 iter/s, 2.85259s/10 iters), loss = 6.02054
I0523 08:49:24.550897 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.02054 (* 1 = 6.02054 loss)
I0523 08:49:25.149693 35003 sgd_solver.cpp:112] Iteration 187550, lr = 0.001
I0523 08:49:28.133313 35003 solver.cpp:239] Iteration 187560 (2.79153 iter/s, 3.58226s/10 iters), loss = 6.66381
I0523 08:49:28.133352 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.66381 (* 1 = 6.66381 loss)
I0523 08:49:28.143329 35003 sgd_solver.cpp:112] Iteration 187560, lr = 0.001
I0523 08:49:30.217772 35003 solver.cpp:239] Iteration 187570 (4.79771 iter/s, 2.08433s/10 iters), loss = 6.4725
I0523 08:49:30.217810 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4725 (* 1 = 6.4725 loss)
I0523 08:49:30.223510 35003 sgd_solver.cpp:112] Iteration 187570, lr = 0.001
I0523 08:49:33.784535 35003 solver.cpp:239] Iteration 187580 (2.80381 iter/s, 3.56657s/10 iters), loss = 7.15544
I0523 08:49:33.784581 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15544 (* 1 = 7.15544 loss)
I0523 08:49:33.797255 35003 sgd_solver.cpp:112] Iteration 187580, lr = 0.001
I0523 08:49:35.882829 35003 solver.cpp:239] Iteration 187590 (4.76609 iter/s, 2.09816s/10 iters), loss = 6.88343
I0523 08:49:35.882865 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.88343 (* 1 = 6.88343 loss)
I0523 08:49:35.895879 35003 sgd_solver.cpp:112] Iteration 187590, lr = 0.001
I0523 08:49:39.454205 35003 solver.cpp:239] Iteration 187600 (2.80019 iter/s, 3.57119s/10 iters), loss = 7.07108
I0523 08:49:39.454263 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07108 (* 1 = 7.07108 loss)
I0523 08:49:39.778702 35003 sgd_solver.cpp:112] Iteration 187600, lr = 0.001
I0523 08:49:43.360436 35003 solver.cpp:239] Iteration 187610 (2.56015 iter/s, 3.90602s/10 iters), loss = 6.99233
I0523 08:49:43.360476 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99233 (* 1 = 6.99233 loss)
I0523 08:49:43.361632 35003 sgd_solver.cpp:112] Iteration 187610, lr = 0.001
I0523 08:49:46.807894 35003 solver.cpp:239] Iteration 187620 (2.90087 iter/s, 3.44724s/10 iters), loss = 6.57762
I0523 08:49:46.808111 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.57762 (* 1 = 6.57762 loss)
I0523 08:49:46.820968 35003 sgd_solver.cpp:112] Iteration 187620, lr = 0.001
I0523 08:49:48.904382 35003 solver.cpp:239] Iteration 187630 (4.77058 iter/s, 2.09618s/10 iters), loss = 6.47444
I0523 08:49:48.904425 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47444 (* 1 = 6.47444 loss)
I0523 08:49:48.910370 35003 sgd_solver.cpp:112] Iteration 187630, lr = 0.001
I0523 08:49:52.909124 35003 solver.cpp:239] Iteration 187640 (2.49717 iter/s, 4.00453s/10 iters), loss = 6.25582
I0523 08:49:52.909173 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25582 (* 1 = 6.25582 loss)
I0523 08:49:52.915024 35003 sgd_solver.cpp:112] Iteration 187640, lr = 0.001
I0523 08:49:54.984208 35003 solver.cpp:239] Iteration 187650 (4.81941 iter/s, 2.07494s/10 iters), loss = 6.56018
I0523 08:49:54.984251 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56018 (* 1 = 6.56018 loss)
I0523 08:49:55.614204 35003 sgd_solver.cpp:112] Iteration 187650, lr = 0.001
I0523 08:49:59.825580 35003 solver.cpp:239] Iteration 187660 (2.06564 iter/s, 4.84112s/10 iters), loss = 6.74578
I0523 08:49:59.825641 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74578 (* 1 = 6.74578 loss)
I0523 08:50:00.512452 35003 sgd_solver.cpp:112] Iteration 187660, lr = 0.001
I0523 08:50:04.636832 35003 solver.cpp:239] Iteration 187670 (2.07858 iter/s, 4.81097s/10 iters), loss = 6.87385
I0523 08:50:04.636893 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87385 (* 1 = 6.87385 loss)
I0523 08:50:04.653609 35003 sgd_solver.cpp:112] Iteration 187670, lr = 0.001
I0523 08:50:06.733691 35003 solver.cpp:239] Iteration 187680 (4.76938 iter/s, 2.09671s/10 iters), loss = 8.00719
I0523 08:50:06.733731 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.00719 (* 1 = 8.00719 loss)
I0523 08:50:06.738906 35003 sgd_solver.cpp:112] Iteration 187680, lr = 0.001
I0523 08:50:08.829823 35003 solver.cpp:239] Iteration 187690 (4.77102 iter/s, 2.09599s/10 iters), loss = 6.46928
I0523 08:50:08.829866 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46928 (* 1 = 6.46928 loss)
I0523 08:50:08.838438 35003 sgd_solver.cpp:112] Iteration 187690, lr = 0.001
I0523 08:50:11.599670 35003 solver.cpp:239] Iteration 187700 (3.61053 iter/s, 2.76968s/10 iters), loss = 6.91108
I0523 08:50:11.599716 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.91108 (* 1 = 6.91108 loss)
I0523 08:50:11.642457 35003 sgd_solver.cpp:112] Iteration 187700, lr = 0.001
I0523 08:50:17.861358 35003 solver.cpp:239] Iteration 187710 (1.59709 iter/s, 6.26137s/10 iters), loss = 5.96881
I0523 08:50:17.861745 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.96881 (* 1 = 5.96881 loss)
I0523 08:50:17.898524 35003 sgd_solver.cpp:112] Iteration 187710, lr = 0.001
I0523 08:50:21.548759 35003 solver.cpp:239] Iteration 187720 (2.71228 iter/s, 3.68693s/10 iters), loss = 7.78265
I0523 08:50:21.548801 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78265 (* 1 = 7.78265 loss)
I0523 08:50:21.554316 35003 sgd_solver.cpp:112] Iteration 187720, lr = 0.001
I0523 08:50:23.635483 35003 solver.cpp:239] Iteration 187730 (4.79253 iter/s, 2.08658s/10 iters), loss = 6.59446
I0523 08:50:23.635535 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59446 (* 1 = 6.59446 loss)
I0523 08:50:23.648862 35003 sgd_solver.cpp:112] Iteration 187730, lr = 0.001
I0523 08:50:25.658471 35003 solver.cpp:239] Iteration 187740 (4.94353 iter/s, 2.02285s/10 iters), loss = 5.18364
I0523 08:50:25.658520 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.18364 (* 1 = 5.18364 loss)
I0523 08:50:25.667953 35003 sgd_solver.cpp:112] Iteration 187740, lr = 0.001
I0523 08:50:29.149296 35003 solver.cpp:239] Iteration 187750 (2.86482 iter/s, 3.49062s/10 iters), loss = 5.79472
I0523 08:50:29.149381 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.79472 (* 1 = 5.79472 loss)
I0523 08:50:29.603596 35003 sgd_solver.cpp:112] Iteration 187750, lr = 0.001
I0523 08:50:32.401459 35003 solver.cpp:239] Iteration 187760 (3.07508 iter/s, 3.25195s/10 iters), loss = 7.76081
I0523 08:50:32.401496 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76081 (* 1 = 7.76081 loss)
I0523 08:50:32.415107 35003 sgd_solver.cpp:112] Iteration 187760, lr = 0.001
I0523 08:50:35.836455 35003 solver.cpp:239] Iteration 187770 (2.91136 iter/s, 3.43482s/10 iters), loss = 6.95659
I0523 08:50:35.836503 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.95659 (* 1 = 6.95659 loss)
I0523 08:50:36.545281 35003 sgd_solver.cpp:112] Iteration 187770, lr = 0.001
I0523 08:50:39.553917 35003 solver.cpp:239] Iteration 187780 (2.69016 iter/s, 3.71726s/10 iters), loss = 6.50716
I0523 08:50:39.553978 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50716 (* 1 = 6.50716 loss)
I0523 08:50:40.292102 35003 sgd_solver.cpp:112] Iteration 187780, lr = 0.001
I0523 08:50:42.345132 35003 solver.cpp:239] Iteration 187790 (3.5829 iter/s, 2.79103s/10 iters), loss = 6.5761
I0523 08:50:42.345190 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.5761 (* 1 = 6.5761 loss)
I0523 08:50:43.086030 35003 sgd_solver.cpp:112] Iteration 187790, lr = 0.001
I0523 08:50:47.389503 35003 solver.cpp:239] Iteration 187800 (1.98251 iter/s, 5.04411s/10 iters), loss = 6.62068
I0523 08:50:47.389560 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.62068 (* 1 = 6.62068 loss)
I0523 08:50:48.117947 35003 sgd_solver.cpp:112] Iteration 187800, lr = 0.001
I0523 08:50:52.094112 35003 solver.cpp:239] Iteration 187810 (2.12569 iter/s, 4.70436s/10 iters), loss = 7.04259
I0523 08:50:52.094157 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.04259 (* 1 = 7.04259 loss)
I0523 08:50:52.107549 35003 sgd_solver.cpp:112] Iteration 187810, lr = 0.001
I0523 08:50:56.477432 35003 solver.cpp:239] Iteration 187820 (2.28149 iter/s, 4.38309s/10 iters), loss = 6.9623
I0523 08:50:56.477473 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9623 (* 1 = 6.9623 loss)
I0523 08:50:57.201103 35003 sgd_solver.cpp:112] Iteration 187820, lr = 0.001
I0523 08:51:00.074857 35003 solver.cpp:239] Iteration 187830 (2.77992 iter/s, 3.59723s/10 iters), loss = 5.40682
I0523 08:51:00.074919 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.40682 (* 1 = 5.40682 loss)
I0523 08:51:00.815685 35003 sgd_solver.cpp:112] Iteration 187830, lr = 0.001
I0523 08:51:05.329461 35003 solver.cpp:239] Iteration 187840 (1.90319 iter/s, 5.25432s/10 iters), loss = 7.41615
I0523 08:51:05.329506 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.41615 (* 1 = 7.41615 loss)
I0523 08:51:05.342641 35003 sgd_solver.cpp:112] Iteration 187840, lr = 0.001
I0523 08:51:08.705665 35003 solver.cpp:239] Iteration 187850 (2.96207 iter/s, 3.37602s/10 iters), loss = 5.72835
I0523 08:51:08.705703 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.72835 (* 1 = 5.72835 loss)
I0523 08:51:08.713621 35003 sgd_solver.cpp:112] Iteration 187850, lr = 0.001
I0523 08:51:12.177067 35003 solver.cpp:239] Iteration 187860 (2.88083 iter/s, 3.47122s/10 iters), loss = 5.74307
I0523 08:51:12.177115 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74307 (* 1 = 5.74307 loss)
I0523 08:51:12.253729 35003 sgd_solver.cpp:112] Iteration 187860, lr = 0.001
I0523 08:51:15.928359 35003 solver.cpp:239] Iteration 187870 (2.6659 iter/s, 3.75108s/10 iters), loss = 6.11746
I0523 08:51:15.928418 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11746 (* 1 = 6.11746 loss)
I0523 08:51:15.937523 35003 sgd_solver.cpp:112] Iteration 187870, lr = 0.001
I0523 08:51:18.048557 35003 solver.cpp:239] Iteration 187880 (4.71687 iter/s, 2.12005s/10 iters), loss = 6.04321
I0523 08:51:18.048604 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.04321 (* 1 = 6.04321 loss)
I0523 08:51:18.057883 35003 sgd_solver.cpp:112] Iteration 187880, lr = 0.001
I0523 08:51:21.536885 35003 solver.cpp:239] Iteration 187890 (2.86686 iter/s, 3.48814s/10 iters), loss = 6.87636
I0523 08:51:21.537137 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.87636 (* 1 = 6.87636 loss)
I0523 08:51:22.250527 35003 sgd_solver.cpp:112] Iteration 187890, lr = 0.001
I0523 08:51:24.317639 35003 solver.cpp:239] Iteration 187900 (3.59657 iter/s, 2.78042s/10 iters), loss = 6.69257
I0523 08:51:24.317685 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.69257 (* 1 = 6.69257 loss)
I0523 08:51:25.051715 35003 sgd_solver.cpp:112] Iteration 187900, lr = 0.001
I0523 08:51:28.047713 35003 solver.cpp:239] Iteration 187910 (2.68106 iter/s, 3.72987s/10 iters), loss = 6.25271
I0523 08:51:28.047762 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25271 (* 1 = 6.25271 loss)
I0523 08:51:28.061333 35003 sgd_solver.cpp:112] Iteration 187910, lr = 0.001
I0523 08:51:30.660735 35003 solver.cpp:239] Iteration 187920 (3.82723 iter/s, 2.61285s/10 iters), loss = 6.19707
I0523 08:51:30.660794 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.19707 (* 1 = 6.19707 loss)
I0523 08:51:31.215422 35003 sgd_solver.cpp:112] Iteration 187920, lr = 0.001
I0523 08:51:35.573779 35003 solver.cpp:239] Iteration 187930 (2.03551 iter/s, 4.91278s/10 iters), loss = 6.13137
I0523 08:51:35.573837 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13137 (* 1 = 6.13137 loss)
I0523 08:51:35.577647 35003 sgd_solver.cpp:112] Iteration 187930, lr = 0.001
I0523 08:51:37.880504 35003 solver.cpp:239] Iteration 187940 (4.33544 iter/s, 2.30657s/10 iters), loss = 5.23847
I0523 08:51:37.880545 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.23847 (* 1 = 5.23847 loss)
I0523 08:51:37.888804 35003 sgd_solver.cpp:112] Iteration 187940, lr = 0.001
I0523 08:51:41.401304 35003 solver.cpp:239] Iteration 187950 (2.84042 iter/s, 3.5206s/10 iters), loss = 7.59813
I0523 08:51:41.401367 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.59813 (* 1 = 7.59813 loss)
I0523 08:51:42.142347 35003 sgd_solver.cpp:112] Iteration 187950, lr = 0.001
I0523 08:51:46.441819 35003 solver.cpp:239] Iteration 187960 (1.98403 iter/s, 5.04025s/10 iters), loss = 6.8422
I0523 08:51:46.441862 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8422 (* 1 = 6.8422 loss)
I0523 08:51:46.448655 35003 sgd_solver.cpp:112] Iteration 187960, lr = 0.001
I0523 08:51:49.243913 35003 solver.cpp:239] Iteration 187970 (3.56897 iter/s, 2.80193s/10 iters), loss = 6.8808
I0523 08:51:49.243985 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8808 (* 1 = 6.8808 loss)
I0523 08:51:49.961942 35003 sgd_solver.cpp:112] Iteration 187970, lr = 0.001
I0523 08:51:53.607156 35003 solver.cpp:239] Iteration 187980 (2.292 iter/s, 4.363s/10 iters), loss = 6.46478
I0523 08:51:53.607286 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46478 (* 1 = 6.46478 loss)
I0523 08:51:53.619019 35003 sgd_solver.cpp:112] Iteration 187980, lr = 0.001
I0523 08:51:55.752388 35003 solver.cpp:239] Iteration 187990 (4.66199 iter/s, 2.14501s/10 iters), loss = 5.70065
I0523 08:51:55.752427 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.70065 (* 1 = 5.70065 loss)
I0523 08:51:56.486855 35003 sgd_solver.cpp:112] Iteration 187990, lr = 0.001
I0523 08:52:00.461808 35003 solver.cpp:239] Iteration 188000 (2.12351 iter/s, 4.70918s/10 iters), loss = 6.29197
I0523 08:52:00.461855 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.29197 (* 1 = 6.29197 loss)
I0523 08:52:01.176965 35003 sgd_solver.cpp:112] Iteration 188000, lr = 0.001
I0523 08:52:03.633808 35003 solver.cpp:239] Iteration 188010 (3.15278 iter/s, 3.17181s/10 iters), loss = 6.25628
I0523 08:52:03.633862 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.25628 (* 1 = 6.25628 loss)
I0523 08:52:03.641681 35003 sgd_solver.cpp:112] Iteration 188010, lr = 0.001
I0523 08:52:07.061481 35003 solver.cpp:239] Iteration 188020 (2.91759 iter/s, 3.42748s/10 iters), loss = 6.75394
I0523 08:52:07.061520 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75394 (* 1 = 6.75394 loss)
I0523 08:52:07.081068 35003 sgd_solver.cpp:112] Iteration 188020, lr = 0.001
I0523 08:52:09.853601 35003 solver.cpp:239] Iteration 188030 (3.58172 iter/s, 2.79196s/10 iters), loss = 6.97136
I0523 08:52:09.853690 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97136 (* 1 = 6.97136 loss)
I0523 08:52:10.549299 35003 sgd_solver.cpp:112] Iteration 188030, lr = 0.001
I0523 08:52:13.227586 35003 solver.cpp:239] Iteration 188040 (2.96405 iter/s, 3.37376s/10 iters), loss = 4.74171
I0523 08:52:13.227641 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.74171 (* 1 = 4.74171 loss)
I0523 08:52:13.957063 35003 sgd_solver.cpp:112] Iteration 188040, lr = 0.001
I0523 08:52:16.605804 35003 solver.cpp:239] Iteration 188050 (2.96031 iter/s, 3.37803s/10 iters), loss = 5.87507
I0523 08:52:16.605849 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.87507 (* 1 = 5.87507 loss)
I0523 08:52:17.015393 35003 sgd_solver.cpp:112] Iteration 188050, lr = 0.001
I0523 08:52:21.866906 35003 solver.cpp:239] Iteration 188060 (1.90084 iter/s, 5.26084s/10 iters), loss = 6.51026
I0523 08:52:21.866951 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51026 (* 1 = 6.51026 loss)
I0523 08:52:22.575642 35003 sgd_solver.cpp:112] Iteration 188060, lr = 0.001
I0523 08:52:26.013202 35003 solver.cpp:239] Iteration 188070 (2.41192 iter/s, 4.14608s/10 iters), loss = 7.69861
I0523 08:52:26.013489 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.69861 (* 1 = 7.69861 loss)
I0523 08:52:26.031417 35003 sgd_solver.cpp:112] Iteration 188070, lr = 0.001
I0523 08:52:29.556453 35003 solver.cpp:239] Iteration 188080 (2.82259 iter/s, 3.54284s/10 iters), loss = 6.31219
I0523 08:52:29.556514 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.31219 (* 1 = 6.31219 loss)
I0523 08:52:29.566038 35003 sgd_solver.cpp:112] Iteration 188080, lr = 0.001
I0523 08:52:32.432760 35003 solver.cpp:239] Iteration 188090 (3.47692 iter/s, 2.8761s/10 iters), loss = 6.23088
I0523 08:52:32.432806 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.23088 (* 1 = 6.23088 loss)
I0523 08:52:33.140874 35003 sgd_solver.cpp:112] Iteration 188090, lr = 0.001
I0523 08:52:36.813434 35003 solver.cpp:239] Iteration 188100 (2.28287 iter/s, 4.38045s/10 iters), loss = 7.96712
I0523 08:52:36.813489 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.96712 (* 1 = 7.96712 loss)
I0523 08:52:36.834873 35003 sgd_solver.cpp:112] Iteration 188100, lr = 0.001
I0523 08:52:40.046583 35003 solver.cpp:239] Iteration 188110 (3.09314 iter/s, 3.23296s/10 iters), loss = 6.36403
I0523 08:52:40.046625 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36403 (* 1 = 6.36403 loss)
I0523 08:52:40.783586 35003 sgd_solver.cpp:112] Iteration 188110, lr = 0.001
I0523 08:52:43.147166 35003 solver.cpp:239] Iteration 188120 (3.22538 iter/s, 3.10041s/10 iters), loss = 6.14879
I0523 08:52:43.147213 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.14879 (* 1 = 6.14879 loss)
I0523 08:52:43.627218 35003 sgd_solver.cpp:112] Iteration 188120, lr = 0.001
I0523 08:52:47.605757 35003 solver.cpp:239] Iteration 188130 (2.24298 iter/s, 4.45836s/10 iters), loss = 6.81476
I0523 08:52:47.605803 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.81476 (* 1 = 6.81476 loss)
I0523 08:52:48.204619 35003 sgd_solver.cpp:112] Iteration 188130, lr = 0.001
I0523 08:52:50.350018 35003 solver.cpp:239] Iteration 188140 (3.64458 iter/s, 2.7438s/10 iters), loss = 6.41573
I0523 08:52:50.350059 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.41573 (* 1 = 6.41573 loss)
I0523 08:52:51.053995 35003 sgd_solver.cpp:112] Iteration 188140, lr = 0.001
I0523 08:52:55.475260 35003 solver.cpp:239] Iteration 188150 (1.95122 iter/s, 5.12499s/10 iters), loss = 6.20384
I0523 08:52:55.475307 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20384 (* 1 = 6.20384 loss)
I0523 08:52:55.481971 35003 sgd_solver.cpp:112] Iteration 188150, lr = 0.001
I0523 08:52:58.386076 35003 solver.cpp:239] Iteration 188160 (3.43567 iter/s, 2.91064s/10 iters), loss = 5.76937
I0523 08:52:58.386293 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.76937 (* 1 = 5.76937 loss)
I0523 08:52:58.392745 35003 sgd_solver.cpp:112] Iteration 188160, lr = 0.001
I0523 08:53:00.450477 35003 solver.cpp:239] Iteration 188170 (4.8447 iter/s, 2.06411s/10 iters), loss = 4.73192
I0523 08:53:00.450531 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.73192 (* 1 = 4.73192 loss)
I0523 08:53:00.463600 35003 sgd_solver.cpp:112] Iteration 188170, lr = 0.001
I0523 08:53:02.837232 35003 solver.cpp:239] Iteration 188180 (4.19006 iter/s, 2.3866s/10 iters), loss = 6.3534
I0523 08:53:02.837272 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.3534 (* 1 = 6.3534 loss)
I0523 08:53:02.860570 35003 sgd_solver.cpp:112] Iteration 188180, lr = 0.001
I0523 08:53:04.903252 35003 solver.cpp:239] Iteration 188190 (4.84054 iter/s, 2.06588s/10 iters), loss = 5.84547
I0523 08:53:04.903300 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.84547 (* 1 = 5.84547 loss)
I0523 08:53:05.592185 35003 sgd_solver.cpp:112] Iteration 188190, lr = 0.001
I0523 08:53:08.102325 35003 solver.cpp:239] Iteration 188200 (3.12608 iter/s, 3.19889s/10 iters), loss = 5.9754
I0523 08:53:08.102370 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.9754 (* 1 = 5.9754 loss)
I0523 08:53:08.107228 35003 sgd_solver.cpp:112] Iteration 188200, lr = 0.001
I0523 08:53:11.587872 35003 solver.cpp:239] Iteration 188210 (2.86915 iter/s, 3.48536s/10 iters), loss = 6.75622
I0523 08:53:11.587911 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.75622 (* 1 = 6.75622 loss)
I0523 08:53:11.600980 35003 sgd_solver.cpp:112] Iteration 188210, lr = 0.001
I0523 08:53:14.246158 35003 solver.cpp:239] Iteration 188220 (3.76204 iter/s, 2.65813s/10 iters), loss = 6.64985
I0523 08:53:14.246196 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64985 (* 1 = 6.64985 loss)
I0523 08:53:14.258095 35003 sgd_solver.cpp:112] Iteration 188220, lr = 0.001
I0523 08:53:15.095635 35003 solver.cpp:239] Iteration 188230 (11.7732 iter/s, 0.849388s/10 iters), loss = 6.76071
I0523 08:53:15.095685 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76071 (* 1 = 6.76071 loss)
I0523 08:53:15.107606 35003 sgd_solver.cpp:112] Iteration 188230, lr = 0.001
I0523 08:53:16.348042 35003 solver.cpp:239] Iteration 188240 (7.9853 iter/s, 1.2523s/10 iters), loss = 6.77211
I0523 08:53:16.348083 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77211 (* 1 = 6.77211 loss)
I0523 08:53:16.356791 35003 sgd_solver.cpp:112] Iteration 188240, lr = 0.001
I0523 08:53:17.179778 35003 solver.cpp:239] Iteration 188250 (12.0243 iter/s, 0.831647s/10 iters), loss = 6.47442
I0523 08:53:17.179895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.47442 (* 1 = 6.47442 loss)
I0523 08:53:17.185516 35003 sgd_solver.cpp:112] Iteration 188250, lr = 0.001
I0523 08:53:17.999155 35003 solver.cpp:239] Iteration 188260 (12.2067 iter/s, 0.819219s/10 iters), loss = 5.74124
I0523 08:53:17.999194 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.74124 (* 1 = 5.74124 loss)
I0523 08:53:18.005878 35003 sgd_solver.cpp:112] Iteration 188260, lr = 0.001
I0523 08:53:18.818823 35003 solver.cpp:239] Iteration 188270 (12.2013 iter/s, 0.819584s/10 iters), loss = 6.46374
I0523 08:53:18.818864 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46374 (* 1 = 6.46374 loss)
I0523 08:53:18.825156 35003 sgd_solver.cpp:112] Iteration 188270, lr = 0.001
I0523 08:53:19.925679 35003 solver.cpp:239] Iteration 188280 (9.0354 iter/s, 1.10676s/10 iters), loss = 7.00519
I0523 08:53:19.925729 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.00519 (* 1 = 7.00519 loss)
I0523 08:53:19.927085 35003 sgd_solver.cpp:112] Iteration 188280, lr = 0.001
I0523 08:53:21.076680 35003 solver.cpp:239] Iteration 188290 (8.68895 iter/s, 1.15089s/10 iters), loss = 7.60958
I0523 08:53:21.076725 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.60958 (* 1 = 7.60958 loss)
I0523 08:53:21.081876 35003 sgd_solver.cpp:112] Iteration 188290, lr = 0.001
I0523 08:53:21.903475 35003 solver.cpp:239] Iteration 188300 (12.0962 iter/s, 0.826706s/10 iters), loss = 4.44697
I0523 08:53:21.903515 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.44697 (* 1 = 4.44697 loss)
I0523 08:53:21.909590 35003 sgd_solver.cpp:112] Iteration 188300, lr = 0.001
I0523 08:53:23.050379 35003 solver.cpp:239] Iteration 188310 (8.71994 iter/s, 1.1468s/10 iters), loss = 7.08905
I0523 08:53:23.050442 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08905 (* 1 = 7.08905 loss)
I0523 08:53:23.058614 35003 sgd_solver.cpp:112] Iteration 188310, lr = 0.001
I0523 08:53:24.332589 35003 solver.cpp:239] Iteration 188320 (7.79976 iter/s, 1.28209s/10 iters), loss = 6.72837
I0523 08:53:24.332626 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72837 (* 1 = 6.72837 loss)
I0523 08:53:24.341457 35003 sgd_solver.cpp:112] Iteration 188320, lr = 0.001
I0523 08:53:28.098920 35003 solver.cpp:239] Iteration 188330 (2.65524 iter/s, 3.76614s/10 iters), loss = 6.51655
I0523 08:53:28.098973 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.51655 (* 1 = 6.51655 loss)
I0523 08:53:28.112033 35003 sgd_solver.cpp:112] Iteration 188330, lr = 0.001
I0523 08:53:30.242022 35003 solver.cpp:239] Iteration 188340 (4.66646 iter/s, 2.14295s/10 iters), loss = 6.42124
I0523 08:53:30.242220 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42124 (* 1 = 6.42124 loss)
I0523 08:53:30.259171 35003 sgd_solver.cpp:112] Iteration 188340, lr = 0.001
I0523 08:53:32.745398 35003 solver.cpp:239] Iteration 188350 (3.99506 iter/s, 2.50309s/10 iters), loss = 7.19966
I0523 08:53:32.745445 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.19966 (* 1 = 7.19966 loss)
I0523 08:53:32.776401 35003 sgd_solver.cpp:112] Iteration 188350, lr = 0.001
I0523 08:53:37.024356 35003 solver.cpp:239] Iteration 188360 (2.33715 iter/s, 4.27872s/10 iters), loss = 6.98523
I0523 08:53:37.024430 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.98523 (* 1 = 6.98523 loss)
I0523 08:53:37.231700 35003 sgd_solver.cpp:112] Iteration 188360, lr = 0.001
I0523 08:53:39.967193 35003 solver.cpp:239] Iteration 188370 (3.39831 iter/s, 2.94264s/10 iters), loss = 6.9245
I0523 08:53:39.967244 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9245 (* 1 = 6.9245 loss)
I0523 08:53:39.972605 35003 sgd_solver.cpp:112] Iteration 188370, lr = 0.001
I0523 08:53:44.212087 35003 solver.cpp:239] Iteration 188380 (2.3559 iter/s, 4.24466s/10 iters), loss = 5.32154
I0523 08:53:44.212126 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.32154 (* 1 = 5.32154 loss)
I0523 08:53:44.230374 35003 sgd_solver.cpp:112] Iteration 188380, lr = 0.001
I0523 08:53:47.235713 35003 solver.cpp:239] Iteration 188390 (3.30747 iter/s, 3.02346s/10 iters), loss = 7.14692
I0523 08:53:47.235756 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.14692 (* 1 = 7.14692 loss)
I0523 08:53:47.247566 35003 sgd_solver.cpp:112] Iteration 188390, lr = 0.001
I0523 08:53:51.362015 35003 solver.cpp:239] Iteration 188400 (2.4236 iter/s, 4.12609s/10 iters), loss = 7.15863
I0523 08:53:51.362052 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15863 (* 1 = 7.15863 loss)
I0523 08:53:52.064465 35003 sgd_solver.cpp:112] Iteration 188400, lr = 0.001
I0523 08:53:55.991479 35003 solver.cpp:239] Iteration 188410 (2.16019 iter/s, 4.62923s/10 iters), loss = 6.56168
I0523 08:53:55.991521 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.56168 (* 1 = 6.56168 loss)
I0523 08:53:56.005652 35003 sgd_solver.cpp:112] Iteration 188410, lr = 0.001
I0523 08:53:58.353842 35003 solver.cpp:239] Iteration 188420 (4.23331 iter/s, 2.36222s/10 iters), loss = 7.95504
I0523 08:53:58.353891 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.95504 (* 1 = 7.95504 loss)
I0523 08:53:59.064350 35003 sgd_solver.cpp:112] Iteration 188420, lr = 0.001
I0523 08:54:01.468791 35003 solver.cpp:239] Iteration 188430 (3.21053 iter/s, 3.11475s/10 iters), loss = 6.61945
I0523 08:54:01.469012 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61945 (* 1 = 6.61945 loss)
I0523 08:54:01.472666 35003 sgd_solver.cpp:112] Iteration 188430, lr = 0.001
I0523 08:54:04.115011 35003 solver.cpp:239] Iteration 188440 (3.77943 iter/s, 2.6459s/10 iters), loss = 7.21428
I0523 08:54:04.115056 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21428 (* 1 = 7.21428 loss)
I0523 08:54:04.128140 35003 sgd_solver.cpp:112] Iteration 188440, lr = 0.001
I0523 08:54:08.269579 35003 solver.cpp:239] Iteration 188450 (2.40711 iter/s, 4.15436s/10 iters), loss = 6.54382
I0523 08:54:08.269618 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54382 (* 1 = 6.54382 loss)
I0523 08:54:08.287199 35003 sgd_solver.cpp:112] Iteration 188450, lr = 0.001
I0523 08:54:12.717721 35003 solver.cpp:239] Iteration 188460 (2.24824 iter/s, 4.44792s/10 iters), loss = 6.82934
I0523 08:54:12.717761 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.82934 (* 1 = 6.82934 loss)
I0523 08:54:12.741850 35003 sgd_solver.cpp:112] Iteration 188460, lr = 0.001
I0523 08:54:15.630239 35003 solver.cpp:239] Iteration 188470 (3.43367 iter/s, 2.91234s/10 iters), loss = 6.20409
I0523 08:54:15.630291 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20409 (* 1 = 6.20409 loss)
I0523 08:54:16.351202 35003 sgd_solver.cpp:112] Iteration 188470, lr = 0.001
I0523 08:54:19.886548 35003 solver.cpp:239] Iteration 188480 (2.34958 iter/s, 4.25609s/10 iters), loss = 6.92748
I0523 08:54:19.886592 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.92748 (* 1 = 6.92748 loss)
I0523 08:54:19.899264 35003 sgd_solver.cpp:112] Iteration 188480, lr = 0.001
I0523 08:54:21.947801 35003 solver.cpp:239] Iteration 188490 (4.85174 iter/s, 2.06112s/10 iters), loss = 6.90682
I0523 08:54:21.947842 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.90682 (* 1 = 6.90682 loss)
I0523 08:54:21.953461 35003 sgd_solver.cpp:112] Iteration 188490, lr = 0.001
I0523 08:54:24.039777 35003 solver.cpp:239] Iteration 188500 (4.78048 iter/s, 2.09184s/10 iters), loss = 7.57146
I0523 08:54:24.039820 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.57146 (* 1 = 7.57146 loss)
I0523 08:54:24.735141 35003 sgd_solver.cpp:112] Iteration 188500, lr = 0.001
I0523 08:54:28.882493 35003 solver.cpp:239] Iteration 188510 (2.06506 iter/s, 4.84247s/10 iters), loss = 6.54574
I0523 08:54:28.882534 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54574 (* 1 = 6.54574 loss)
I0523 08:54:28.895750 35003 sgd_solver.cpp:112] Iteration 188510, lr = 0.001
I0523 08:54:33.321142 35003 solver.cpp:239] Iteration 188520 (2.25305 iter/s, 4.43842s/10 iters), loss = 6.68162
I0523 08:54:33.321293 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.68162 (* 1 = 6.68162 loss)
I0523 08:54:33.329438 35003 sgd_solver.cpp:112] Iteration 188520, lr = 0.001
I0523 08:54:35.419028 35003 solver.cpp:239] Iteration 188530 (4.76726 iter/s, 2.09764s/10 iters), loss = 6.17353
I0523 08:54:35.419086 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17353 (* 1 = 6.17353 loss)
I0523 08:54:35.432250 35003 sgd_solver.cpp:112] Iteration 188530, lr = 0.001
I0523 08:54:39.791929 35003 solver.cpp:239] Iteration 188540 (2.28693 iter/s, 4.37267s/10 iters), loss = 6.4337
I0523 08:54:39.791970 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.4337 (* 1 = 6.4337 loss)
I0523 08:54:39.805296 35003 sgd_solver.cpp:112] Iteration 188540, lr = 0.001
I0523 08:54:41.712831 35003 solver.cpp:239] Iteration 188550 (5.20626 iter/s, 1.92076s/10 iters), loss = 6.43587
I0523 08:54:41.712895 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43587 (* 1 = 6.43587 loss)
I0523 08:54:41.719492 35003 sgd_solver.cpp:112] Iteration 188550, lr = 0.001
I0523 08:54:46.121649 35003 solver.cpp:239] Iteration 188560 (2.26831 iter/s, 4.40857s/10 iters), loss = 6.38666
I0523 08:54:46.121686 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38666 (* 1 = 6.38666 loss)
I0523 08:54:46.142294 35003 sgd_solver.cpp:112] Iteration 188560, lr = 0.001
I0523 08:54:48.227334 35003 solver.cpp:239] Iteration 188570 (4.74934 iter/s, 2.10556s/10 iters), loss = 6.48795
I0523 08:54:48.227380 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48795 (* 1 = 6.48795 loss)
I0523 08:54:48.233225 35003 sgd_solver.cpp:112] Iteration 188570, lr = 0.001
I0523 08:54:52.386927 35003 solver.cpp:239] Iteration 188580 (2.4042 iter/s, 4.15938s/10 iters), loss = 6.8496
I0523 08:54:52.386968 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.8496 (* 1 = 6.8496 loss)
I0523 08:54:52.400532 35003 sgd_solver.cpp:112] Iteration 188580, lr = 0.001
I0523 08:54:55.289168 35003 solver.cpp:239] Iteration 188590 (3.44581 iter/s, 2.90208s/10 iters), loss = 5.4975
I0523 08:54:55.289213 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.4975 (* 1 = 5.4975 loss)
I0523 08:54:55.295547 35003 sgd_solver.cpp:112] Iteration 188590, lr = 0.001
I0523 08:54:58.040966 35003 solver.cpp:239] Iteration 188600 (3.6342 iter/s, 2.75164s/10 iters), loss = 7.54979
I0523 08:54:58.041003 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.54979 (* 1 = 7.54979 loss)
I0523 08:54:58.045610 35003 sgd_solver.cpp:112] Iteration 188600, lr = 0.001
I0523 08:55:02.331050 35003 solver.cpp:239] Iteration 188610 (2.33121 iter/s, 4.28962s/10 iters), loss = 6.48141
I0523 08:55:02.331094 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.48141 (* 1 = 6.48141 loss)
I0523 08:55:02.345744 35003 sgd_solver.cpp:112] Iteration 188610, lr = 0.001
I0523 08:55:06.940817 35003 solver.cpp:239] Iteration 188620 (2.16942 iter/s, 4.60954s/10 iters), loss = 6.44291
I0523 08:55:06.941017 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44291 (* 1 = 6.44291 loss)
I0523 08:55:07.675912 35003 sgd_solver.cpp:112] Iteration 188620, lr = 0.001
I0523 08:55:12.552058 35003 solver.cpp:239] Iteration 188630 (1.78228 iter/s, 5.6108s/10 iters), loss = 6.36719
I0523 08:55:12.552141 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.36719 (* 1 = 6.36719 loss)
I0523 08:55:12.576644 35003 sgd_solver.cpp:112] Iteration 188630, lr = 0.001
I0523 08:55:15.447851 35003 solver.cpp:239] Iteration 188640 (3.45352 iter/s, 2.8956s/10 iters), loss = 5.40909
I0523 08:55:15.447906 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.40909 (* 1 = 5.40909 loss)
I0523 08:55:16.188596 35003 sgd_solver.cpp:112] Iteration 188640, lr = 0.001
I0523 08:55:18.193811 35003 solver.cpp:239] Iteration 188650 (3.64195 iter/s, 2.74579s/10 iters), loss = 6.76146
I0523 08:55:18.193866 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.76146 (* 1 = 6.76146 loss)
I0523 08:55:18.878857 35003 sgd_solver.cpp:112] Iteration 188650, lr = 0.001
I0523 08:55:25.902782 35003 solver.cpp:239] Iteration 188660 (1.29725 iter/s, 7.70861s/10 iters), loss = 6.37047
I0523 08:55:25.902833 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37047 (* 1 = 6.37047 loss)
I0523 08:55:25.928892 35003 sgd_solver.cpp:112] Iteration 188660, lr = 0.001
I0523 08:55:30.263587 35003 solver.cpp:239] Iteration 188670 (2.29328 iter/s, 4.36057s/10 iters), loss = 6.78308
I0523 08:55:30.263660 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.78308 (* 1 = 6.78308 loss)
I0523 08:55:30.293870 35003 sgd_solver.cpp:112] Iteration 188670, lr = 0.001
I0523 08:55:34.553820 35003 solver.cpp:239] Iteration 188680 (2.33101 iter/s, 4.28999s/10 iters), loss = 7.32634
I0523 08:55:34.553859 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.32634 (* 1 = 7.32634 loss)
I0523 08:55:34.559137 35003 sgd_solver.cpp:112] Iteration 188680, lr = 0.001
I0523 08:55:39.261749 35003 solver.cpp:239] Iteration 188690 (2.12419 iter/s, 4.70769s/10 iters), loss = 6.9625
I0523 08:55:39.261935 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.9625 (* 1 = 6.9625 loss)
I0523 08:55:39.269351 35003 sgd_solver.cpp:112] Iteration 188690, lr = 0.001
I0523 08:55:43.380945 35003 solver.cpp:239] Iteration 188700 (2.42787 iter/s, 4.11884s/10 iters), loss = 6.77083
I0523 08:55:43.380990 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.77083 (* 1 = 6.77083 loss)
I0523 08:55:43.398300 35003 sgd_solver.cpp:112] Iteration 188700, lr = 0.001
I0523 08:55:47.797849 35003 solver.cpp:239] Iteration 188710 (2.26415 iter/s, 4.41667s/10 iters), loss = 7.3092
I0523 08:55:47.797912 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3092 (* 1 = 7.3092 loss)
I0523 08:55:48.255908 35003 sgd_solver.cpp:112] Iteration 188710, lr = 0.001
I0523 08:55:51.975987 35003 solver.cpp:239] Iteration 188720 (2.39354 iter/s, 4.17791s/10 iters), loss = 6.86728
I0523 08:55:51.976032 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.86728 (* 1 = 6.86728 loss)
I0523 08:55:51.981597 35003 sgd_solver.cpp:112] Iteration 188720, lr = 0.001
I0523 08:55:56.285203 35003 solver.cpp:239] Iteration 188730 (2.32073 iter/s, 4.309s/10 iters), loss = 6.26864
I0523 08:55:56.285254 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26864 (* 1 = 6.26864 loss)
I0523 08:55:57.012034 35003 sgd_solver.cpp:112] Iteration 188730, lr = 0.001
I0523 08:56:01.765916 35003 solver.cpp:239] Iteration 188740 (1.82467 iter/s, 5.48044s/10 iters), loss = 7.1796
I0523 08:56:01.765964 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.1796 (* 1 = 7.1796 loss)
I0523 08:56:02.500854 35003 sgd_solver.cpp:112] Iteration 188740, lr = 0.001
I0523 08:56:05.254104 35003 solver.cpp:239] Iteration 188750 (2.86698 iter/s, 3.48799s/10 iters), loss = 6.89214
I0523 08:56:05.254158 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89214 (* 1 = 6.89214 loss)
I0523 08:56:05.267568 35003 sgd_solver.cpp:112] Iteration 188750, lr = 0.001
I0523 08:56:09.858487 35003 solver.cpp:239] Iteration 188760 (2.17195 iter/s, 4.60415s/10 iters), loss = 5.77412
I0523 08:56:09.858817 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.77412 (* 1 = 5.77412 loss)
I0523 08:56:10.465267 35003 sgd_solver.cpp:112] Iteration 188760, lr = 0.001
I0523 08:56:13.962235 35003 solver.cpp:239] Iteration 188770 (2.43707 iter/s, 4.10329s/10 iters), loss = 6.59873
I0523 08:56:13.962277 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.59873 (* 1 = 6.59873 loss)
I0523 08:56:14.683105 35003 sgd_solver.cpp:112] Iteration 188770, lr = 0.001
I0523 08:56:16.818848 35003 solver.cpp:239] Iteration 188780 (3.50084 iter/s, 2.85645s/10 iters), loss = 5.03172
I0523 08:56:16.818897 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.03172 (* 1 = 5.03172 loss)
I0523 08:56:17.553293 35003 sgd_solver.cpp:112] Iteration 188780, lr = 0.001
I0523 08:56:21.643340 35003 solver.cpp:239] Iteration 188790 (2.07286 iter/s, 4.82425s/10 iters), loss = 6.30496
I0523 08:56:21.643384 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.30496 (* 1 = 6.30496 loss)
I0523 08:56:22.383958 35003 sgd_solver.cpp:112] Iteration 188790, lr = 0.001
I0523 08:56:24.508671 35003 solver.cpp:239] Iteration 188800 (3.4902 iter/s, 2.86516s/10 iters), loss = 5.81529
I0523 08:56:24.508718 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.81529 (* 1 = 5.81529 loss)
I0523 08:56:25.239447 35003 sgd_solver.cpp:112] Iteration 188800, lr = 0.001
I0523 08:56:28.011620 35003 solver.cpp:239] Iteration 188810 (2.8549 iter/s, 3.50275s/10 iters), loss = 6.42511
I0523 08:56:28.011665 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.42511 (* 1 = 6.42511 loss)
I0523 08:56:28.698339 35003 sgd_solver.cpp:112] Iteration 188810, lr = 0.001
I0523 08:56:31.626395 35003 solver.cpp:239] Iteration 188820 (2.76658 iter/s, 3.61457s/10 iters), loss = 6.37927
I0523 08:56:31.626456 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.37927 (* 1 = 6.37927 loss)
I0523 08:56:32.337502 35003 sgd_solver.cpp:112] Iteration 188820, lr = 0.001
I0523 08:56:36.677057 35003 solver.cpp:239] Iteration 188830 (1.98004 iter/s, 5.0504s/10 iters), loss = 6.1247
I0523 08:56:36.677108 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.1247 (* 1 = 6.1247 loss)
I0523 08:56:37.274711 35003 sgd_solver.cpp:112] Iteration 188830, lr = 0.001
I0523 08:56:40.420578 35003 solver.cpp:239] Iteration 188840 (2.67143 iter/s, 3.74331s/10 iters), loss = 6.17932
I0523 08:56:40.420843 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17932 (* 1 = 6.17932 loss)
I0523 08:56:40.429353 35003 sgd_solver.cpp:112] Iteration 188840, lr = 0.001
I0523 08:56:44.681409 35003 solver.cpp:239] Iteration 188850 (2.34718 iter/s, 4.26043s/10 iters), loss = 6.52075
I0523 08:56:44.681453 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.52075 (* 1 = 6.52075 loss)
I0523 08:56:45.421737 35003 sgd_solver.cpp:112] Iteration 188850, lr = 0.001
I0523 08:56:47.658042 35003 solver.cpp:239] Iteration 188860 (3.3597 iter/s, 2.97645s/10 iters), loss = 6.38908
I0523 08:56:47.658113 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.38908 (* 1 = 6.38908 loss)
I0523 08:56:48.392263 35003 sgd_solver.cpp:112] Iteration 188860, lr = 0.001
I0523 08:56:52.608587 35003 solver.cpp:239] Iteration 188870 (2.02009 iter/s, 4.95028s/10 iters), loss = 7.3009
I0523 08:56:52.608640 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.3009 (* 1 = 7.3009 loss)
I0523 08:56:52.719913 35003 sgd_solver.cpp:112] Iteration 188870, lr = 0.001
I0523 08:56:57.064131 35003 solver.cpp:239] Iteration 188880 (2.24451 iter/s, 4.45531s/10 iters), loss = 7.18769
I0523 08:56:57.064174 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18769 (* 1 = 7.18769 loss)
I0523 08:56:57.074627 35003 sgd_solver.cpp:112] Iteration 188880, lr = 0.001
I0523 08:57:00.126726 35003 solver.cpp:239] Iteration 188890 (3.26543 iter/s, 3.06239s/10 iters), loss = 7.22358
I0523 08:57:00.126763 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22358 (* 1 = 7.22358 loss)
I0523 08:57:00.133172 35003 sgd_solver.cpp:112] Iteration 188890, lr = 0.001
I0523 08:57:02.896492 35003 solver.cpp:239] Iteration 188900 (3.61061 iter/s, 2.76961s/10 iters), loss = 6.73523
I0523 08:57:02.896531 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.73523 (* 1 = 6.73523 loss)
I0523 08:57:02.914814 35003 sgd_solver.cpp:112] Iteration 188900, lr = 0.001
I0523 08:57:06.988090 35003 solver.cpp:239] Iteration 188910 (2.44416 iter/s, 4.09139s/10 iters), loss = 6.49468
I0523 08:57:06.988138 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.49468 (* 1 = 6.49468 loss)
I0523 08:57:07.722810 35003 sgd_solver.cpp:112] Iteration 188910, lr = 0.001
I0523 08:57:12.038460 35003 solver.cpp:239] Iteration 188920 (1.98015 iter/s, 5.05011s/10 iters), loss = 6.65263
I0523 08:57:12.038743 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.65263 (* 1 = 6.65263 loss)
I0523 08:57:12.753953 35003 sgd_solver.cpp:112] Iteration 188920, lr = 0.001
I0523 08:57:14.679167 35003 solver.cpp:239] Iteration 188930 (3.78735 iter/s, 2.64037s/10 iters), loss = 6.72853
I0523 08:57:14.679225 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.72853 (* 1 = 6.72853 loss)
I0523 08:57:14.684725 35003 sgd_solver.cpp:112] Iteration 188930, lr = 0.001
I0523 08:57:17.533583 35003 solver.cpp:239] Iteration 188940 (3.50358 iter/s, 2.85423s/10 iters), loss = 7.15436
I0523 08:57:17.533649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.15436 (* 1 = 7.15436 loss)
I0523 08:57:17.613601 35003 sgd_solver.cpp:112] Iteration 188940, lr = 0.001
I0523 08:57:18.917853 35003 solver.cpp:239] Iteration 188950 (7.22469 iter/s, 1.38414s/10 iters), loss = 6.2705
I0523 08:57:18.917894 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.2705 (* 1 = 6.2705 loss)
I0523 08:57:18.924293 35003 sgd_solver.cpp:112] Iteration 188950, lr = 0.001
I0523 08:57:20.994822 35003 solver.cpp:239] Iteration 188960 (4.81503 iter/s, 2.07683s/10 iters), loss = 7.25816
I0523 08:57:20.994874 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.25816 (* 1 = 7.25816 loss)
I0523 08:57:21.001817 35003 sgd_solver.cpp:112] Iteration 188960, lr = 0.001
I0523 08:57:25.503063 35003 solver.cpp:239] Iteration 188970 (2.21828 iter/s, 4.508s/10 iters), loss = 6.80195
I0523 08:57:25.503105 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.80195 (* 1 = 6.80195 loss)
I0523 08:57:25.516733 35003 sgd_solver.cpp:112] Iteration 188970, lr = 0.001
I0523 08:57:28.333514 35003 solver.cpp:239] Iteration 188980 (3.53321 iter/s, 2.83029s/10 iters), loss = 5.38696
I0523 08:57:28.333564 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.38696 (* 1 = 5.38696 loss)
I0523 08:57:28.339680 35003 sgd_solver.cpp:112] Iteration 188980, lr = 0.001
I0523 08:57:31.110033 35003 solver.cpp:239] Iteration 188990 (3.60186 iter/s, 2.77635s/10 iters), loss = 6.22374
I0523 08:57:31.110090 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.22374 (* 1 = 6.22374 loss)
I0523 08:57:31.122887 35003 sgd_solver.cpp:112] Iteration 188990, lr = 0.001
I0523 08:57:36.203616 35003 solver.cpp:239] Iteration 189000 (1.96335 iter/s, 5.09333s/10 iters), loss = 6.79959
I0523 08:57:36.203655 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.79959 (* 1 = 6.79959 loss)
I0523 08:57:36.210481 35003 sgd_solver.cpp:112] Iteration 189000, lr = 0.001
I0523 08:57:38.322666 35003 solver.cpp:239] Iteration 189010 (4.71938 iter/s, 2.11892s/10 iters), loss = 6.32494
I0523 08:57:38.322738 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32494 (* 1 = 6.32494 loss)
I0523 08:57:39.053982 35003 sgd_solver.cpp:112] Iteration 189010, lr = 0.001
I0523 08:57:41.679039 35003 solver.cpp:239] Iteration 189020 (2.9796 iter/s, 3.35616s/10 iters), loss = 5.27374
I0523 08:57:41.679086 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.27374 (* 1 = 5.27374 loss)
I0523 08:57:41.691725 35003 sgd_solver.cpp:112] Iteration 189020, lr = 0.001
I0523 08:57:45.234846 35003 solver.cpp:239] Iteration 189030 (2.81246 iter/s, 3.55561s/10 iters), loss = 6.20414
I0523 08:57:45.235054 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20414 (* 1 = 6.20414 loss)
I0523 08:57:45.969290 35003 sgd_solver.cpp:112] Iteration 189030, lr = 0.001
I0523 08:57:48.425137 35003 solver.cpp:239] Iteration 189040 (3.13484 iter/s, 3.18995s/10 iters), loss = 7.33557
I0523 08:57:48.425173 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.33557 (* 1 = 7.33557 loss)
I0523 08:57:48.433627 35003 sgd_solver.cpp:112] Iteration 189040, lr = 0.001
I0523 08:57:52.021195 35003 solver.cpp:239] Iteration 189050 (2.78098 iter/s, 3.59585s/10 iters), loss = 6.092
I0523 08:57:52.021253 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.092 (* 1 = 6.092 loss)
I0523 08:57:52.028544 35003 sgd_solver.cpp:112] Iteration 189050, lr = 0.001
I0523 08:57:55.682433 35003 solver.cpp:239] Iteration 189060 (2.73148 iter/s, 3.66102s/10 iters), loss = 6.26649
I0523 08:57:55.682489 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26649 (* 1 = 6.26649 loss)
I0523 08:57:56.314112 35003 sgd_solver.cpp:112] Iteration 189060, lr = 0.001
I0523 08:57:59.941375 35003 solver.cpp:239] Iteration 189070 (2.34813 iter/s, 4.25871s/10 iters), loss = 6.20267
I0523 08:57:59.941409 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.20267 (* 1 = 6.20267 loss)
I0523 08:57:59.954669 35003 sgd_solver.cpp:112] Iteration 189070, lr = 0.001
I0523 08:58:04.181696 35003 solver.cpp:239] Iteration 189080 (2.35843 iter/s, 4.24011s/10 iters), loss = 6.99209
I0523 08:58:04.181735 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99209 (* 1 = 6.99209 loss)
I0523 08:58:04.188117 35003 sgd_solver.cpp:112] Iteration 189080, lr = 0.001
I0523 08:58:07.200908 35003 solver.cpp:239] Iteration 189090 (3.31231 iter/s, 3.01904s/10 iters), loss = 6.99975
I0523 08:58:07.200958 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.99975 (* 1 = 6.99975 loss)
I0523 08:58:07.222342 35003 sgd_solver.cpp:112] Iteration 189090, lr = 0.001
I0523 08:58:11.513751 35003 solver.cpp:239] Iteration 189100 (2.31888 iter/s, 4.31242s/10 iters), loss = 6.40172
I0523 08:58:11.513809 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.40172 (* 1 = 6.40172 loss)
I0523 08:58:11.535614 35003 sgd_solver.cpp:112] Iteration 189100, lr = 0.001
I0523 08:58:14.327991 35003 solver.cpp:239] Iteration 189110 (3.55648 iter/s, 2.81177s/10 iters), loss = 5.97471
I0523 08:58:14.328040 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97471 (* 1 = 5.97471 loss)
I0523 08:58:15.009214 35003 sgd_solver.cpp:112] Iteration 189110, lr = 0.001
I0523 08:58:17.841320 35003 solver.cpp:239] Iteration 189120 (2.84646 iter/s, 3.51313s/10 iters), loss = 6.17824
I0523 08:58:17.841507 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17824 (* 1 = 6.17824 loss)
I0523 08:58:18.582090 35003 sgd_solver.cpp:112] Iteration 189120, lr = 0.001
I0523 08:58:22.102388 35003 solver.cpp:239] Iteration 189130 (2.34703 iter/s, 4.26071s/10 iters), loss = 7.22416
I0523 08:58:22.102432 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.22416 (* 1 = 7.22416 loss)
I0523 08:58:22.115556 35003 sgd_solver.cpp:112] Iteration 189130, lr = 0.001
I0523 08:58:24.395357 35003 solver.cpp:239] Iteration 189140 (4.36143 iter/s, 2.29283s/10 iters), loss = 7.76442
I0523 08:58:24.395401 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.76442 (* 1 = 7.76442 loss)
I0523 08:58:25.136525 35003 sgd_solver.cpp:112] Iteration 189140, lr = 0.001
I0523 08:58:27.650285 35003 solver.cpp:239] Iteration 189150 (3.07243 iter/s, 3.25475s/10 iters), loss = 6.11294
I0523 08:58:27.650327 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.11294 (* 1 = 6.11294 loss)
I0523 08:58:27.665781 35003 sgd_solver.cpp:112] Iteration 189150, lr = 0.001
I0523 08:58:32.332958 35003 solver.cpp:239] Iteration 189160 (2.13564 iter/s, 4.68244s/10 iters), loss = 6.89321
I0523 08:58:32.333009 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89321 (* 1 = 6.89321 loss)
I0523 08:58:32.339552 35003 sgd_solver.cpp:112] Iteration 189160, lr = 0.001
I0523 08:58:34.391674 35003 solver.cpp:239] Iteration 189170 (4.85774 iter/s, 2.05857s/10 iters), loss = 6.94576
I0523 08:58:34.391716 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.94576 (* 1 = 6.94576 loss)
I0523 08:58:34.398708 35003 sgd_solver.cpp:112] Iteration 189170, lr = 0.001
I0523 08:58:40.945608 35003 solver.cpp:239] Iteration 189180 (1.52587 iter/s, 6.55363s/10 iters), loss = 5.75563
I0523 08:58:40.945649 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.75563 (* 1 = 5.75563 loss)
I0523 08:58:40.983180 35003 sgd_solver.cpp:112] Iteration 189180, lr = 0.001
I0523 08:58:44.907961 35003 solver.cpp:239] Iteration 189190 (2.52388 iter/s, 3.96215s/10 iters), loss = 5.70572
I0523 08:58:44.908000 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.70572 (* 1 = 5.70572 loss)
I0523 08:58:44.920889 35003 sgd_solver.cpp:112] Iteration 189190, lr = 0.001
I0523 08:58:47.342402 35003 solver.cpp:239] Iteration 189200 (4.10796 iter/s, 2.4343s/10 iters), loss = 6.17155
I0523 08:58:47.342449 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17155 (* 1 = 6.17155 loss)
I0523 08:58:47.360843 35003 sgd_solver.cpp:112] Iteration 189200, lr = 0.001
I0523 08:58:51.040535 35003 solver.cpp:239] Iteration 189210 (2.70421 iter/s, 3.69793s/10 iters), loss = 7.21931
I0523 08:58:51.040789 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.21931 (* 1 = 7.21931 loss)
I0523 08:58:51.052278 35003 sgd_solver.cpp:112] Iteration 189210, lr = 0.001
I0523 08:58:53.972615 35003 solver.cpp:239] Iteration 189220 (3.41095 iter/s, 2.93173s/10 iters), loss = 5.41511
I0523 08:58:53.972654 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.41511 (* 1 = 5.41511 loss)
I0523 08:58:53.979411 35003 sgd_solver.cpp:112] Iteration 189220, lr = 0.001
I0523 08:58:55.935242 35003 solver.cpp:239] Iteration 189230 (5.09554 iter/s, 1.9625s/10 iters), loss = 7.93136
I0523 08:58:55.935281 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.93136 (* 1 = 7.93136 loss)
I0523 08:58:55.942564 35003 sgd_solver.cpp:112] Iteration 189230, lr = 0.001
I0523 08:58:59.440836 35003 solver.cpp:239] Iteration 189240 (2.85274 iter/s, 3.5054s/10 iters), loss = 6.32385
I0523 08:58:59.440886 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.32385 (* 1 = 6.32385 loss)
I0523 08:58:59.894821 35003 sgd_solver.cpp:112] Iteration 189240, lr = 0.001
I0523 08:59:03.941102 35003 solver.cpp:239] Iteration 189250 (2.22221 iter/s, 4.50003s/10 iters), loss = 7.75467
I0523 08:59:03.941146 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.75467 (* 1 = 7.75467 loss)
I0523 08:59:03.961521 35003 sgd_solver.cpp:112] Iteration 189250, lr = 0.001
I0523 08:59:07.503275 35003 solver.cpp:239] Iteration 189260 (2.80744 iter/s, 3.56196s/10 iters), loss = 6.61763
I0523 08:59:07.503353 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.61763 (* 1 = 6.61763 loss)
I0523 08:59:07.653717 35003 sgd_solver.cpp:112] Iteration 189260, lr = 0.001
I0523 08:59:09.795816 35003 solver.cpp:239] Iteration 189270 (4.3623 iter/s, 2.29237s/10 iters), loss = 6.10973
I0523 08:59:09.795861 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.10973 (* 1 = 6.10973 loss)
I0523 08:59:10.536412 35003 sgd_solver.cpp:112] Iteration 189270, lr = 0.001
I0523 08:59:13.797719 35003 solver.cpp:239] Iteration 189280 (2.49894 iter/s, 4.00169s/10 iters), loss = 7.84824
I0523 08:59:13.797766 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.84824 (* 1 = 7.84824 loss)
I0523 08:59:13.810868 35003 sgd_solver.cpp:112] Iteration 189280, lr = 0.001
I0523 08:59:17.488759 35003 solver.cpp:239] Iteration 189290 (2.70941 iter/s, 3.69084s/10 iters), loss = 6.89006
I0523 08:59:17.488806 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.89006 (* 1 = 6.89006 loss)
I0523 08:59:18.190438 35003 sgd_solver.cpp:112] Iteration 189290, lr = 0.001
I0523 08:59:20.574196 35003 solver.cpp:239] Iteration 189300 (3.24122 iter/s, 3.08526s/10 iters), loss = 6.97235
I0523 08:59:20.574237 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97235 (* 1 = 6.97235 loss)
I0523 08:59:20.587410 35003 sgd_solver.cpp:112] Iteration 189300, lr = 0.001
I0523 08:59:24.816925 35003 solver.cpp:239] Iteration 189310 (2.3571 iter/s, 4.2425s/10 iters), loss = 7.44557
I0523 08:59:24.817215 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44557 (* 1 = 7.44557 loss)
I0523 08:59:24.824780 35003 sgd_solver.cpp:112] Iteration 189310, lr = 0.001
I0523 08:59:29.177326 35003 solver.cpp:239] Iteration 189320 (2.29361 iter/s, 4.35995s/10 iters), loss = 6.16313
I0523 08:59:29.177376 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.16313 (* 1 = 6.16313 loss)
I0523 08:59:29.190641 35003 sgd_solver.cpp:112] Iteration 189320, lr = 0.001
I0523 08:59:34.199054 35003 solver.cpp:239] Iteration 189330 (1.99145 iter/s, 5.02147s/10 iters), loss = 6.13613
I0523 08:59:34.199111 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.13613 (* 1 = 6.13613 loss)
I0523 08:59:34.212865 35003 sgd_solver.cpp:112] Iteration 189330, lr = 0.001
I0523 08:59:37.057971 35003 solver.cpp:239] Iteration 189340 (3.49804 iter/s, 2.85874s/10 iters), loss = 7.08389
I0523 08:59:37.058017 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.08389 (* 1 = 7.08389 loss)
I0523 08:59:37.736379 35003 sgd_solver.cpp:112] Iteration 189340, lr = 0.001
I0523 08:59:41.219374 35003 solver.cpp:239] Iteration 189350 (2.40316 iter/s, 4.16118s/10 iters), loss = 6.50093
I0523 08:59:41.219426 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.50093 (* 1 = 6.50093 loss)
I0523 08:59:41.242982 35003 sgd_solver.cpp:112] Iteration 189350, lr = 0.001
I0523 08:59:45.933476 35003 solver.cpp:239] Iteration 189360 (2.12141 iter/s, 4.71386s/10 iters), loss = 6.09673
I0523 08:59:45.933519 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09673 (* 1 = 6.09673 loss)
I0523 08:59:45.946382 35003 sgd_solver.cpp:112] Iteration 189360, lr = 0.001
I0523 08:59:49.605238 35003 solver.cpp:239] Iteration 189370 (2.72364 iter/s, 3.67156s/10 iters), loss = 7.20451
I0523 08:59:49.605305 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.20451 (* 1 = 7.20451 loss)
I0523 08:59:49.609357 35003 sgd_solver.cpp:112] Iteration 189370, lr = 0.001
I0523 08:59:51.661607 35003 solver.cpp:239] Iteration 189380 (4.86331 iter/s, 2.05621s/10 iters), loss = 6.43748
I0523 08:59:51.661651 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.43748 (* 1 = 6.43748 loss)
I0523 08:59:51.666290 35003 sgd_solver.cpp:112] Iteration 189380, lr = 0.001
I0523 08:59:54.380594 35003 solver.cpp:239] Iteration 189390 (3.67809 iter/s, 2.7188s/10 iters), loss = 7.18373
I0523 08:59:54.380630 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.18373 (* 1 = 7.18373 loss)
I0523 08:59:54.385570 35003 sgd_solver.cpp:112] Iteration 189390, lr = 0.001
I0523 08:59:57.659139 35003 solver.cpp:239] Iteration 189400 (3.0503 iter/s, 3.27837s/10 iters), loss = 6.85113
I0523 08:59:57.659404 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85113 (* 1 = 6.85113 loss)
I0523 08:59:58.385946 35003 sgd_solver.cpp:112] Iteration 189400, lr = 0.001
I0523 09:00:01.253453 35003 solver.cpp:239] Iteration 189410 (2.78587 iter/s, 3.58954s/10 iters), loss = 6.15282
I0523 09:00:01.253510 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.15282 (* 1 = 6.15282 loss)
I0523 09:00:01.276281 35003 sgd_solver.cpp:112] Iteration 189410, lr = 0.001
I0523 09:00:03.365811 35003 solver.cpp:239] Iteration 189420 (4.73439 iter/s, 2.11221s/10 iters), loss = 7.51152
I0523 09:00:03.365865 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.51152 (* 1 = 7.51152 loss)
I0523 09:00:04.106647 35003 sgd_solver.cpp:112] Iteration 189420, lr = 0.001
I0523 09:00:07.311192 35003 solver.cpp:239] Iteration 189430 (2.53475 iter/s, 3.94517s/10 iters), loss = 7.44773
I0523 09:00:07.311235 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44773 (* 1 = 7.44773 loss)
I0523 09:00:07.360620 35003 sgd_solver.cpp:112] Iteration 189430, lr = 0.001
I0523 09:00:11.099800 35003 solver.cpp:239] Iteration 189440 (2.63963 iter/s, 3.78841s/10 iters), loss = 6.28628
I0523 09:00:11.099854 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.28628 (* 1 = 6.28628 loss)
I0523 09:00:11.112265 35003 sgd_solver.cpp:112] Iteration 189440, lr = 0.001
I0523 09:00:13.165179 35003 solver.cpp:239] Iteration 189450 (4.84206 iter/s, 2.06524s/10 iters), loss = 7.36588
I0523 09:00:13.165227 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.36588 (* 1 = 7.36588 loss)
I0523 09:00:13.178223 35003 sgd_solver.cpp:112] Iteration 189450, lr = 0.001
I0523 09:00:15.579004 35003 solver.cpp:239] Iteration 189460 (4.14306 iter/s, 2.41368s/10 iters), loss = 7.78174
I0523 09:00:15.579041 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.78174 (* 1 = 7.78174 loss)
I0523 09:00:15.592263 35003 sgd_solver.cpp:112] Iteration 189460, lr = 0.001
I0523 09:00:19.220140 35003 solver.cpp:239] Iteration 189470 (2.74654 iter/s, 3.64095s/10 iters), loss = 7.45396
I0523 09:00:19.220175 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45396 (* 1 = 7.45396 loss)
I0523 09:00:19.238879 35003 sgd_solver.cpp:112] Iteration 189470, lr = 0.001
I0523 09:00:23.546128 35003 solver.cpp:239] Iteration 189480 (2.31407 iter/s, 4.32139s/10 iters), loss = 6.71553
I0523 09:00:23.546162 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71553 (* 1 = 6.71553 loss)
I0523 09:00:23.559447 35003 sgd_solver.cpp:112] Iteration 189480, lr = 0.001
I0523 09:00:28.043143 35003 solver.cpp:239] Iteration 189490 (2.22381 iter/s, 4.49679s/10 iters), loss = 6.39312
I0523 09:00:28.043359 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.39312 (* 1 = 6.39312 loss)
I0523 09:00:28.047297 35003 sgd_solver.cpp:112] Iteration 189490, lr = 0.001
I0523 09:00:30.729516 35003 solver.cpp:239] Iteration 189500 (3.72292 iter/s, 2.68606s/10 iters), loss = 5.30793
I0523 09:00:30.729629 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.30793 (* 1 = 5.30793 loss)
I0523 09:00:30.733788 35003 sgd_solver.cpp:112] Iteration 189500, lr = 0.001
I0523 09:00:34.848668 35003 solver.cpp:239] Iteration 189510 (2.42785 iter/s, 4.11886s/10 iters), loss = 5.20585
I0523 09:00:34.848714 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.20585 (* 1 = 5.20585 loss)
I0523 09:00:35.583700 35003 sgd_solver.cpp:112] Iteration 189510, lr = 0.001
I0523 09:00:39.419101 35003 solver.cpp:239] Iteration 189520 (2.18809 iter/s, 4.57019s/10 iters), loss = 5.97283
I0523 09:00:39.419145 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.97283 (* 1 = 5.97283 loss)
I0523 09:00:39.623970 35003 sgd_solver.cpp:112] Iteration 189520, lr = 0.001
I0523 09:00:43.182425 35003 solver.cpp:239] Iteration 189530 (2.65737 iter/s, 3.76311s/10 iters), loss = 5.64969
I0523 09:00:43.182476 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.64969 (* 1 = 5.64969 loss)
I0523 09:00:43.190588 35003 sgd_solver.cpp:112] Iteration 189530, lr = 0.001
I0523 09:00:46.811861 35003 solver.cpp:239] Iteration 189540 (2.7554 iter/s, 3.62923s/10 iters), loss = 7.27547
I0523 09:00:46.811911 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.27547 (* 1 = 7.27547 loss)
I0523 09:00:47.396543 35003 sgd_solver.cpp:112] Iteration 189540, lr = 0.001
I0523 09:00:51.034679 35003 solver.cpp:239] Iteration 189550 (2.36821 iter/s, 4.22259s/10 iters), loss = 5.943
I0523 09:00:51.034742 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.943 (* 1 = 5.943 loss)
I0523 09:00:51.043781 35003 sgd_solver.cpp:112] Iteration 189550, lr = 0.001
I0523 09:00:54.623231 35003 solver.cpp:239] Iteration 189560 (2.7868 iter/s, 3.58834s/10 iters), loss = 6.46989
I0523 09:00:54.623278 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.46989 (* 1 = 6.46989 loss)
I0523 09:00:54.635099 35003 sgd_solver.cpp:112] Iteration 189560, lr = 0.001
I0523 09:00:57.330538 35003 solver.cpp:239] Iteration 189570 (3.69393 iter/s, 2.70714s/10 iters), loss = 4.94157
I0523 09:00:57.330581 35003 solver.cpp:258]     Train net output #0: softmax_loss = 4.94157 (* 1 = 4.94157 loss)
I0523 09:00:57.339303 35003 sgd_solver.cpp:112] Iteration 189570, lr = 0.001
I0523 09:01:01.118798 35003 solver.cpp:239] Iteration 189580 (2.63995 iter/s, 3.78795s/10 iters), loss = 6.71072
I0523 09:01:01.119047 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.71072 (* 1 = 6.71072 loss)
I0523 09:01:01.125994 35003 sgd_solver.cpp:112] Iteration 189580, lr = 0.001
I0523 09:01:04.131593 35003 solver.cpp:239] Iteration 189590 (3.31959 iter/s, 3.01242s/10 iters), loss = 6.17637
I0523 09:01:04.131638 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.17637 (* 1 = 6.17637 loss)
I0523 09:01:04.845674 35003 sgd_solver.cpp:112] Iteration 189590, lr = 0.001
I0523 09:01:06.926306 35003 solver.cpp:239] Iteration 189600 (3.57839 iter/s, 2.79455s/10 iters), loss = 6.26761
I0523 09:01:06.926343 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.26761 (* 1 = 6.26761 loss)
I0523 09:01:06.940116 35003 sgd_solver.cpp:112] Iteration 189600, lr = 0.001
I0523 09:01:09.823365 35003 solver.cpp:239] Iteration 189610 (3.45198 iter/s, 2.89689s/10 iters), loss = 5.94653
I0523 09:01:09.823426 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.94653 (* 1 = 5.94653 loss)
I0523 09:01:09.835608 35003 sgd_solver.cpp:112] Iteration 189610, lr = 0.001
I0523 09:01:14.242826 35003 solver.cpp:239] Iteration 189620 (2.26285 iter/s, 4.4192s/10 iters), loss = 7.34904
I0523 09:01:14.242879 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34904 (* 1 = 7.34904 loss)
I0523 09:01:14.977255 35003 sgd_solver.cpp:112] Iteration 189620, lr = 0.001
I0523 09:01:20.150754 35003 solver.cpp:239] Iteration 189630 (1.69274 iter/s, 5.90758s/10 iters), loss = 6.85087
I0523 09:01:20.150815 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.85087 (* 1 = 6.85087 loss)
I0523 09:01:20.164202 35003 sgd_solver.cpp:112] Iteration 189630, lr = 0.001
I0523 09:01:23.045053 35003 solver.cpp:239] Iteration 189640 (3.45529 iter/s, 2.89412s/10 iters), loss = 6.09496
I0523 09:01:23.045094 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.09496 (* 1 = 6.09496 loss)
I0523 09:01:23.049906 35003 sgd_solver.cpp:112] Iteration 189640, lr = 0.001
I0523 09:01:26.478498 35003 solver.cpp:239] Iteration 189650 (2.91269 iter/s, 3.43325s/10 iters), loss = 6.97519
I0523 09:01:26.478554 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.97519 (* 1 = 6.97519 loss)
I0523 09:01:26.481824 35003 sgd_solver.cpp:112] Iteration 189650, lr = 0.001
I0523 09:01:29.190850 35003 solver.cpp:239] Iteration 189660 (3.68708 iter/s, 2.71217s/10 iters), loss = 7.44406
I0523 09:01:29.190907 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.44406 (* 1 = 7.44406 loss)
I0523 09:01:29.198748 35003 sgd_solver.cpp:112] Iteration 189660, lr = 0.001
I0523 09:01:31.713186 35003 solver.cpp:239] Iteration 189670 (3.96484 iter/s, 2.52217s/10 iters), loss = 7.45613
I0523 09:01:31.713410 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.45613 (* 1 = 7.45613 loss)
I0523 09:01:31.726703 35003 sgd_solver.cpp:112] Iteration 189670, lr = 0.001
I0523 09:01:35.267206 35003 solver.cpp:239] Iteration 189680 (2.81401 iter/s, 3.55365s/10 iters), loss = 8.03854
I0523 09:01:35.267251 35003 solver.cpp:258]     Train net output #0: softmax_loss = 8.03854 (* 1 = 8.03854 loss)
I0523 09:01:35.277401 35003 sgd_solver.cpp:112] Iteration 189680, lr = 0.001
I0523 09:01:37.780777 35003 solver.cpp:239] Iteration 189690 (3.97865 iter/s, 2.51341s/10 iters), loss = 6.44208
I0523 09:01:37.780830 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.44208 (* 1 = 6.44208 loss)
I0523 09:01:37.799602 35003 sgd_solver.cpp:112] Iteration 189690, lr = 0.001
I0523 09:01:40.563354 35003 solver.cpp:239] Iteration 189700 (3.59402 iter/s, 2.7824s/10 iters), loss = 7.47539
I0523 09:01:40.563395 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.47539 (* 1 = 7.47539 loss)
I0523 09:01:40.576174 35003 sgd_solver.cpp:112] Iteration 189700, lr = 0.001
I0523 09:01:43.584913 35003 solver.cpp:239] Iteration 189710 (3.30974 iter/s, 3.02139s/10 iters), loss = 7.48195
I0523 09:01:43.584965 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.48195 (* 1 = 7.48195 loss)
I0523 09:01:43.772863 35003 sgd_solver.cpp:112] Iteration 189710, lr = 0.001
I0523 09:01:45.797582 35003 solver.cpp:239] Iteration 189720 (4.51973 iter/s, 2.21252s/10 iters), loss = 5.16977
I0523 09:01:45.797621 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.16977 (* 1 = 5.16977 loss)
I0523 09:01:45.804648 35003 sgd_solver.cpp:112] Iteration 189720, lr = 0.001
I0523 09:01:49.502727 35003 solver.cpp:239] Iteration 189730 (2.69909 iter/s, 3.70495s/10 iters), loss = 7.07388
I0523 09:01:49.502768 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.07388 (* 1 = 7.07388 loss)
I0523 09:01:49.520784 35003 sgd_solver.cpp:112] Iteration 189730, lr = 0.001
I0523 09:01:52.428282 35003 solver.cpp:239] Iteration 189740 (3.42356 iter/s, 2.92094s/10 iters), loss = 5.66063
I0523 09:01:52.428325 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.66063 (* 1 = 5.66063 loss)
I0523 09:01:52.433174 35003 sgd_solver.cpp:112] Iteration 189740, lr = 0.001
I0523 09:01:56.683048 35003 solver.cpp:239] Iteration 189750 (2.35043 iter/s, 4.25455s/10 iters), loss = 7.90589
I0523 09:01:56.683089 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.90589 (* 1 = 7.90589 loss)
I0523 09:01:56.695272 35003 sgd_solver.cpp:112] Iteration 189750, lr = 0.001
I0523 09:02:00.804309 35003 solver.cpp:239] Iteration 189760 (2.42657 iter/s, 4.12105s/10 iters), loss = 6.54258
I0523 09:02:00.804355 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.54258 (* 1 = 6.54258 loss)
I0523 09:02:01.545102 35003 sgd_solver.cpp:112] Iteration 189760, lr = 0.001
I0523 09:02:05.133790 35003 solver.cpp:239] Iteration 189770 (2.30987 iter/s, 4.32924s/10 iters), loss = 7.34824
I0523 09:02:05.134042 35003 solver.cpp:258]     Train net output #0: softmax_loss = 7.34824 (* 1 = 7.34824 loss)
I0523 09:02:05.147285 35003 sgd_solver.cpp:112] Iteration 189770, lr = 0.001
I0523 09:02:08.006356 35003 solver.cpp:239] Iteration 189780 (3.48162 iter/s, 2.87222s/10 iters), loss = 6.64835
I0523 09:02:08.006402 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.64835 (* 1 = 6.64835 loss)
I0523 09:02:08.047101 35003 sgd_solver.cpp:112] Iteration 189780, lr = 0.001
I0523 09:02:12.138962 35003 solver.cpp:239] Iteration 189790 (2.41991 iter/s, 4.13239s/10 iters), loss = 6.74539
I0523 09:02:12.139001 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.74539 (* 1 = 6.74539 loss)
I0523 09:02:12.141681 35003 sgd_solver.cpp:112] Iteration 189790, lr = 0.001
I0523 09:02:14.226189 35003 solver.cpp:239] Iteration 189800 (4.79137 iter/s, 2.08709s/10 iters), loss = 6.34979
I0523 09:02:14.226236 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.34979 (* 1 = 6.34979 loss)
I0523 09:02:14.961037 35003 sgd_solver.cpp:112] Iteration 189800, lr = 0.001
I0523 09:02:16.273443 35003 solver.cpp:239] Iteration 189810 (4.88492 iter/s, 2.04712s/10 iters), loss = 5.94916
I0523 09:02:16.273494 35003 solver.cpp:258]     Train net output #0: softmax_loss = 5.94916 (* 1 = 5.94916 loss)
I0523 09:02:16.286841 35003 sgd_solver.cpp:112] Iteration 189810, lr = 0.001
I0523 09:02:19.791414 35003 solver.cpp:239] Iteration 189820 (2.84271 iter/s, 3.51777s/10 iters), loss = 6.08617
I0523 09:02:19.791463 35003 solver.cpp:258]     Train net output #0: softmax_loss = 6.08617 (* 1 = 6.08617 loss)
I0523 09:02:20.177103 35003 sgd_solver.cpp:112] Iteration 189820, lr = 0.001
*** Aborted at 1527037340 (unix time) try "date -d @1527037340" if you are using GNU date ***
PC: @     0x7f19330d3f1c __lll_lock_wait
*** SIGTERM (@0x3e90000441d) received by PID 35003 (TID 0x7f1944677780) from PID 17437; stack trace: ***
    @     0x7f1942229cb0 (unknown)
    @     0x7f19330d3f1c __lll_lock_wait
    @     0x7f19330cf664 _L_lock_952
    @     0x7f19330cf4c6 __GI___pthread_mutex_lock
    @     0x7f190b9a4cbd (unknown)
    @     0x7f190bafcba0 (unknown)
    @     0x7f19366475f1 (unknown)
    @     0x7f1936661e6d (unknown)
    @     0x7f19364db77d (unknown)
    @     0x7f19364e0f09 (unknown)
    @     0x7f19364e574e (unknown)
    @     0x7f19364e2afe (unknown)
    @     0x7f19364de33d (unknown)
    @     0x7f1943925d93 caffe::CuDNNConvolutionLayer<>::Forward_gpu()
    @     0x7f19438c61d3 caffe::Net<>::ForwardFromTo()
    @     0x7f19438c6587 caffe::Net<>::Forward()
    @     0x7f194376bcc7 caffe::Solver<>::Step()
    @     0x7f194376c4bf caffe::Solver<>::Solve()
    @     0x7f19438a84e6 caffe::NCCL<>::Run()
    @           0x40c1e3 train()
    @           0x40982c main
    @     0x7f1942214f45 (unknown)
    @           0x40a111 (unknown)
