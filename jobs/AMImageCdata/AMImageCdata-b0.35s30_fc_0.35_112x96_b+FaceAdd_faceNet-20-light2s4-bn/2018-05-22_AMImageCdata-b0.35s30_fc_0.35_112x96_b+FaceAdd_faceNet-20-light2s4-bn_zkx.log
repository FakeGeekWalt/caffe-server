./build/tools/caffe: /home/zkx/anaconda2/lib/libtiff.so.5: no version information available (required by /home/zkx/env/opencv/lib/libopencv_highgui.so.2.4)
I0522 21:55:16.278669 34682 upgrade_proto.cpp:1084] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': models/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/solver.prototxt
I0522 21:55:16.278952 34682 upgrade_proto.cpp:1091] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W0522 21:55:16.278964 34682 upgrade_proto.cpp:1093] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I0522 21:55:16.279095 34682 caffe.cpp:204] Using GPUs 0, 1, 2, 3
I0522 21:55:19.471038 34682 caffe.cpp:209] GPU 0: TITAN Xp
I0522 21:55:19.471674 34682 caffe.cpp:209] GPU 1: TITAN Xp
I0522 21:55:19.472296 34682 caffe.cpp:209] GPU 2: TITAN Xp
I0522 21:55:19.472915 34682 caffe.cpp:209] GPU 3: TITAN Xp
I0522 21:55:20.068477 34682 solver.cpp:45] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 500000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx"
solver_mode: GPU
device_id: 0
net: "models/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/train.prototxt"
train_state {
  level: 0
  stage: ""
}
snapshot_after_train: true
stepvalue: 150000
stepvalue: 300000
iter_size: 1
type: "SGD"
I0522 21:55:20.068650 34682 solver.cpp:102] Creating training net from net file: models/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/train.prototxt
I0522 21:55:20.070408 34682 net.cpp:51] Initializing net from parameters: 
name: "2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_train"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  image_data_param {
    source: "/home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/image_list/train_list/Combine_base_Cmul-Asia-beid-cap10-ZheD_list.txt_label.txt"
    batch_size: 64
    shuffle: true
    root_folder: "/home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/fc_0.35_112x96/"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_1"
  type: "PReLU"
  bottom: "conv1_1"
  top: "conv1_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "conv1_3"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_3"
  type: "PReLU"
  bottom: "conv1_3"
  top: "conv1_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res1_3"
  type: "Eltwise"
  bottom: "conv1_1"
  bottom: "conv1_3"
  top: "res1_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res1_3_reduce"
  type: "Convolution"
  bottom: "res1_3"
  top: "res1_3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "res1_3_reduce"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "res1_3"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1"
  type: "PReLU"
  bottom: "conv2_1"
  top: "conv2_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res1_3_p"
  type: "Eltwise"
  bottom: "pool1"
  bottom: "conv2_1"
  top: "res1_3_p"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_3"
  type: "Convolution"
  bottom: "res1_3_p"
  top: "conv2_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_3"
  type: "PReLU"
  bottom: "conv2_3"
  top: "conv2_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res2_3"
  type: "Eltwise"
  bottom: "res1_3_p"
  bottom: "conv2_3"
  top: "res2_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_5"
  type: "Convolution"
  bottom: "res2_3"
  top: "conv2_5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_5"
  type: "PReLU"
  bottom: "conv2_5"
  top: "conv2_5"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res2_5"
  type: "Eltwise"
  bottom: "res2_3"
  bottom: "conv2_5"
  top: "res2_5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res2_5_reduce"
  type: "Convolution"
  bottom: "res2_5"
  top: "res2_5_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2_5_reduce"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "res2_5"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1"
  type: "PReLU"
  bottom: "conv3_1"
  top: "conv3_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res2_5_p"
  type: "Eltwise"
  bottom: "pool2"
  bottom: "conv3_1"
  top: "res2_5_p"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "res2_5_p"
  top: "conv3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_3"
  type: "PReLU"
  bottom: "conv3_3"
  top: "conv3_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res3_3"
  type: "Eltwise"
  bottom: "res2_5_p"
  bottom: "conv3_3"
  top: "res3_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_5"
  type: "Convolution"
  bottom: "res3_3"
  top: "conv3_5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_5"
  type: "PReLU"
  bottom: "conv3_5"
  top: "conv3_5"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res3_5"
  type: "Eltwise"
  bottom: "res3_3"
  bottom: "conv3_5"
  top: "res3_5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res3_5_reduce"
  type: "Convolution"
  bottom: "res3_5"
  top: "res3_5_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 144
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3_5_reduce"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "res3_5"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 144
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1"
  type: "PReLU"
  bottom: "conv4_1"
  top: "conv4_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res3_5_p"
  type: "Eltwise"
  bottom: "pool3"
  bottom: "conv4_1"
  top: "res3_5_p"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "res3_5_p"
  top: "conv4_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 144
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_3"
  type: "PReLU"
  bottom: "conv4_3"
  top: "conv4_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res4_3"
  type: "Eltwise"
  bottom: "res3_5_p"
  bottom: "conv4_3"
  top: "res4_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "fc5"
  type: "InnerProduct"
  bottom: "res4_3"
  top: "fc5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc5_bn"
  type: "BatchNorm"
  bottom: "fc5"
  top: "fc5"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "norm1"
  type: "Normalize"
  bottom: "fc5"
  top: "norm1"
}
layer {
  name: "fc-6_l2"
  type: "InnerProduct"
  bottom: "norm1"
  top: "fc-6_l2"
  param {
    lr_mult: 1
  }
  inner_product_param {
    num_output: 54547
    bias_term: false
    weight_filler {
      type: "xavier"
    }
    normalize: true
  }
}
layer {
  name: "fc-6_margin"
  type: "LabelSpecificAdd"
  bottom: "fc-6_l2"
  bottom: "label"
  top: "fc-6_margin"
  label_specific_add_param {
    bias: -0.35
  }
}
layer {
  name: "fc-6_margin_scale"
  type: "Scale"
  bottom: "fc-6_margin"
  top: "fc-6_margin_scale"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      type: "constant"
      value: 30
    }
  }
}
layer {
  name: "softmax_loss"
  type: "SoftmaxWithLoss"
  bottom: "fc-6_margin_scale"
  bottom: "label"
  top: "softmax_loss"
}
I0522 21:55:20.070750 34682 layer_factory.hpp:77] Creating layer data
I0522 21:55:20.070806 34682 net.cpp:84] Creating Layer data
I0522 21:55:20.070816 34682 net.cpp:380] data -> data
I0522 21:55:20.070839 34682 net.cpp:380] data -> label
I0522 21:55:20.070854 34682 image_data_layer.cpp:38] Opening file /home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/image_list/train_list/Combine_base_Cmul-Asia-beid-cap10-ZheD_list.txt_label.txt
I0522 21:55:20.912896 34682 image_data_layer.cpp:53] Shuffling data
I0522 21:55:21.336107 34682 image_data_layer.cpp:63] A total of 2075438 images.
I0522 21:55:21.338205 34682 image_data_layer.cpp:90] output data size: 64,3,112,96
I0522 21:55:21.370313 34682 net.cpp:122] Setting up data
I0522 21:55:21.370365 34682 net.cpp:129] Top shape: 64 3 112 96 (2064384)
I0522 21:55:21.370373 34682 net.cpp:129] Top shape: 64 (64)
I0522 21:55:21.370375 34682 net.cpp:137] Memory required for data: 8257792
I0522 21:55:21.370383 34682 layer_factory.hpp:77] Creating layer label_data_1_split
I0522 21:55:21.370419 34682 net.cpp:84] Creating Layer label_data_1_split
I0522 21:55:21.370427 34682 net.cpp:406] label_data_1_split <- label
I0522 21:55:21.370455 34682 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0522 21:55:21.370470 34682 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0522 21:55:21.370550 34682 net.cpp:122] Setting up label_data_1_split
I0522 21:55:21.370559 34682 net.cpp:129] Top shape: 64 (64)
I0522 21:55:21.370564 34682 net.cpp:129] Top shape: 64 (64)
I0522 21:55:21.370568 34682 net.cpp:137] Memory required for data: 8258304
I0522 21:55:21.370570 34682 layer_factory.hpp:77] Creating layer conv1_1
I0522 21:55:21.370595 34682 net.cpp:84] Creating Layer conv1_1
I0522 21:55:21.370601 34682 net.cpp:406] conv1_1 <- data
I0522 21:55:21.370609 34682 net.cpp:380] conv1_1 -> conv1_1
I0522 21:55:21.943423 34682 net.cpp:122] Setting up conv1_1
I0522 21:55:21.943460 34682 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:21.943466 34682 net.cpp:137] Memory required for data: 24773376
I0522 21:55:21.943490 34682 layer_factory.hpp:77] Creating layer relu1_1
I0522 21:55:21.943511 34682 net.cpp:84] Creating Layer relu1_1
I0522 21:55:21.943516 34682 net.cpp:406] relu1_1 <- conv1_1
I0522 21:55:21.943523 34682 net.cpp:367] relu1_1 -> conv1_1 (in-place)
I0522 21:55:21.944955 34682 net.cpp:122] Setting up relu1_1
I0522 21:55:21.944972 34682 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:21.944977 34682 net.cpp:137] Memory required for data: 41288448
I0522 21:55:21.945000 34682 layer_factory.hpp:77] Creating layer conv1_1_relu1_1_0_split
I0522 21:55:21.945013 34682 net.cpp:84] Creating Layer conv1_1_relu1_1_0_split
I0522 21:55:21.945017 34682 net.cpp:406] conv1_1_relu1_1_0_split <- conv1_1
I0522 21:55:21.945024 34682 net.cpp:380] conv1_1_relu1_1_0_split -> conv1_1_relu1_1_0_split_0
I0522 21:55:21.945034 34682 net.cpp:380] conv1_1_relu1_1_0_split -> conv1_1_relu1_1_0_split_1
I0522 21:55:21.945078 34682 net.cpp:122] Setting up conv1_1_relu1_1_0_split
I0522 21:55:21.945085 34682 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:21.945089 34682 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:21.945092 34682 net.cpp:137] Memory required for data: 74318592
I0522 21:55:21.945096 34682 layer_factory.hpp:77] Creating layer conv1_3
I0522 21:55:21.945111 34682 net.cpp:84] Creating Layer conv1_3
I0522 21:55:21.945116 34682 net.cpp:406] conv1_3 <- conv1_1_relu1_1_0_split_0
I0522 21:55:21.945121 34682 net.cpp:380] conv1_3 -> conv1_3
I0522 21:55:21.947865 34682 net.cpp:122] Setting up conv1_3
I0522 21:55:21.947880 34682 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:21.947885 34682 net.cpp:137] Memory required for data: 90833664
I0522 21:55:21.947911 34682 layer_factory.hpp:77] Creating layer relu1_3
I0522 21:55:21.947934 34682 net.cpp:84] Creating Layer relu1_3
I0522 21:55:21.947940 34682 net.cpp:406] relu1_3 <- conv1_3
I0522 21:55:21.947947 34682 net.cpp:367] relu1_3 -> conv1_3 (in-place)
I0522 21:55:21.948081 34682 net.cpp:122] Setting up relu1_3
I0522 21:55:21.948088 34682 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:21.948092 34682 net.cpp:137] Memory required for data: 107348736
I0522 21:55:21.948097 34682 layer_factory.hpp:77] Creating layer res1_3
I0522 21:55:21.948104 34682 net.cpp:84] Creating Layer res1_3
I0522 21:55:21.948108 34682 net.cpp:406] res1_3 <- conv1_1_relu1_1_0_split_1
I0522 21:55:21.948112 34682 net.cpp:406] res1_3 <- conv1_3
I0522 21:55:21.948117 34682 net.cpp:380] res1_3 -> res1_3
I0522 21:55:21.948145 34682 net.cpp:122] Setting up res1_3
I0522 21:55:21.948168 34682 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:21.948173 34682 net.cpp:137] Memory required for data: 123863808
I0522 21:55:21.948177 34682 layer_factory.hpp:77] Creating layer res1_3_res1_3_0_split
I0522 21:55:21.948184 34682 net.cpp:84] Creating Layer res1_3_res1_3_0_split
I0522 21:55:21.948187 34682 net.cpp:406] res1_3_res1_3_0_split <- res1_3
I0522 21:55:21.948192 34682 net.cpp:380] res1_3_res1_3_0_split -> res1_3_res1_3_0_split_0
I0522 21:55:21.948199 34682 net.cpp:380] res1_3_res1_3_0_split -> res1_3_res1_3_0_split_1
I0522 21:55:21.948235 34682 net.cpp:122] Setting up res1_3_res1_3_0_split
I0522 21:55:21.948241 34682 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:21.948246 34682 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:55:21.948248 34682 net.cpp:137] Memory required for data: 156893952
I0522 21:55:21.948253 34682 layer_factory.hpp:77] Creating layer res1_3_reduce
I0522 21:55:21.948266 34682 net.cpp:84] Creating Layer res1_3_reduce
I0522 21:55:21.948271 34682 net.cpp:406] res1_3_reduce <- res1_3_res1_3_0_split_0
I0522 21:55:21.948276 34682 net.cpp:380] res1_3_reduce -> res1_3_reduce
I0522 21:55:21.949021 34682 net.cpp:122] Setting up res1_3_reduce
I0522 21:55:21.949034 34682 net.cpp:129] Top shape: 64 48 56 48 (8257536)
I0522 21:55:21.949039 34682 net.cpp:137] Memory required for data: 189924096
I0522 21:55:21.949046 34682 layer_factory.hpp:77] Creating layer pool1
I0522 21:55:21.949057 34682 net.cpp:84] Creating Layer pool1
I0522 21:55:21.949064 34682 net.cpp:406] pool1 <- res1_3_reduce
I0522 21:55:21.949069 34682 net.cpp:380] pool1 -> pool1
I0522 21:55:21.949121 34682 net.cpp:122] Setting up pool1
I0522 21:55:21.949128 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.949132 34682 net.cpp:137] Memory required for data: 198181632
I0522 21:55:21.949136 34682 layer_factory.hpp:77] Creating layer conv2_1
I0522 21:55:21.949146 34682 net.cpp:84] Creating Layer conv2_1
I0522 21:55:21.949151 34682 net.cpp:406] conv2_1 <- res1_3_res1_3_0_split_1
I0522 21:55:21.949156 34682 net.cpp:380] conv2_1 -> conv2_1
I0522 21:55:21.950565 34682 net.cpp:122] Setting up conv2_1
I0522 21:55:21.950582 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.950587 34682 net.cpp:137] Memory required for data: 206439168
I0522 21:55:21.950600 34682 layer_factory.hpp:77] Creating layer relu2_1
I0522 21:55:21.950609 34682 net.cpp:84] Creating Layer relu2_1
I0522 21:55:21.950614 34682 net.cpp:406] relu2_1 <- conv2_1
I0522 21:55:21.950623 34682 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I0522 21:55:21.950757 34682 net.cpp:122] Setting up relu2_1
I0522 21:55:21.950767 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.950769 34682 net.cpp:137] Memory required for data: 214696704
I0522 21:55:21.950774 34682 layer_factory.hpp:77] Creating layer res1_3_p
I0522 21:55:21.950780 34682 net.cpp:84] Creating Layer res1_3_p
I0522 21:55:21.950785 34682 net.cpp:406] res1_3_p <- pool1
I0522 21:55:21.950790 34682 net.cpp:406] res1_3_p <- conv2_1
I0522 21:55:21.950795 34682 net.cpp:380] res1_3_p -> res1_3_p
I0522 21:55:21.950820 34682 net.cpp:122] Setting up res1_3_p
I0522 21:55:21.950826 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.950830 34682 net.cpp:137] Memory required for data: 222954240
I0522 21:55:21.950834 34682 layer_factory.hpp:77] Creating layer res1_3_p_res1_3_p_0_split
I0522 21:55:21.950839 34682 net.cpp:84] Creating Layer res1_3_p_res1_3_p_0_split
I0522 21:55:21.950841 34682 net.cpp:406] res1_3_p_res1_3_p_0_split <- res1_3_p
I0522 21:55:21.950846 34682 net.cpp:380] res1_3_p_res1_3_p_0_split -> res1_3_p_res1_3_p_0_split_0
I0522 21:55:21.950852 34682 net.cpp:380] res1_3_p_res1_3_p_0_split -> res1_3_p_res1_3_p_0_split_1
I0522 21:55:21.950884 34682 net.cpp:122] Setting up res1_3_p_res1_3_p_0_split
I0522 21:55:21.950891 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.950896 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.950898 34682 net.cpp:137] Memory required for data: 239469312
I0522 21:55:21.950913 34682 layer_factory.hpp:77] Creating layer conv2_3
I0522 21:55:21.950927 34682 net.cpp:84] Creating Layer conv2_3
I0522 21:55:21.950932 34682 net.cpp:406] conv2_3 <- res1_3_p_res1_3_p_0_split_0
I0522 21:55:21.950937 34682 net.cpp:380] conv2_3 -> conv2_3
I0522 21:55:21.952546 34682 net.cpp:122] Setting up conv2_3
I0522 21:55:21.952561 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.952582 34682 net.cpp:137] Memory required for data: 247726848
I0522 21:55:21.952590 34682 layer_factory.hpp:77] Creating layer relu2_3
I0522 21:55:21.952600 34682 net.cpp:84] Creating Layer relu2_3
I0522 21:55:21.952605 34682 net.cpp:406] relu2_3 <- conv2_3
I0522 21:55:21.952613 34682 net.cpp:367] relu2_3 -> conv2_3 (in-place)
I0522 21:55:21.952734 34682 net.cpp:122] Setting up relu2_3
I0522 21:55:21.952742 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.952746 34682 net.cpp:137] Memory required for data: 255984384
I0522 21:55:21.952751 34682 layer_factory.hpp:77] Creating layer res2_3
I0522 21:55:21.952759 34682 net.cpp:84] Creating Layer res2_3
I0522 21:55:21.952764 34682 net.cpp:406] res2_3 <- res1_3_p_res1_3_p_0_split_1
I0522 21:55:21.952769 34682 net.cpp:406] res2_3 <- conv2_3
I0522 21:55:21.952775 34682 net.cpp:380] res2_3 -> res2_3
I0522 21:55:21.952800 34682 net.cpp:122] Setting up res2_3
I0522 21:55:21.952807 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.952811 34682 net.cpp:137] Memory required for data: 264241920
I0522 21:55:21.952813 34682 layer_factory.hpp:77] Creating layer res2_3_res2_3_0_split
I0522 21:55:21.952819 34682 net.cpp:84] Creating Layer res2_3_res2_3_0_split
I0522 21:55:21.952822 34682 net.cpp:406] res2_3_res2_3_0_split <- res2_3
I0522 21:55:21.952827 34682 net.cpp:380] res2_3_res2_3_0_split -> res2_3_res2_3_0_split_0
I0522 21:55:21.952833 34682 net.cpp:380] res2_3_res2_3_0_split -> res2_3_res2_3_0_split_1
I0522 21:55:21.952867 34682 net.cpp:122] Setting up res2_3_res2_3_0_split
I0522 21:55:21.952873 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.952878 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.952881 34682 net.cpp:137] Memory required for data: 280756992
I0522 21:55:21.952886 34682 layer_factory.hpp:77] Creating layer conv2_5
I0522 21:55:21.952898 34682 net.cpp:84] Creating Layer conv2_5
I0522 21:55:21.952903 34682 net.cpp:406] conv2_5 <- res2_3_res2_3_0_split_0
I0522 21:55:21.952911 34682 net.cpp:380] conv2_5 -> conv2_5
I0522 21:55:21.954268 34682 net.cpp:122] Setting up conv2_5
I0522 21:55:21.954283 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.954289 34682 net.cpp:137] Memory required for data: 289014528
I0522 21:55:21.954296 34682 layer_factory.hpp:77] Creating layer relu2_5
I0522 21:55:21.954304 34682 net.cpp:84] Creating Layer relu2_5
I0522 21:55:21.954310 34682 net.cpp:406] relu2_5 <- conv2_5
I0522 21:55:21.954315 34682 net.cpp:367] relu2_5 -> conv2_5 (in-place)
I0522 21:55:21.954425 34682 net.cpp:122] Setting up relu2_5
I0522 21:55:21.954433 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.954437 34682 net.cpp:137] Memory required for data: 297272064
I0522 21:55:21.954445 34682 layer_factory.hpp:77] Creating layer res2_5
I0522 21:55:21.954452 34682 net.cpp:84] Creating Layer res2_5
I0522 21:55:21.954457 34682 net.cpp:406] res2_5 <- res2_3_res2_3_0_split_1
I0522 21:55:21.954463 34682 net.cpp:406] res2_5 <- conv2_5
I0522 21:55:21.954468 34682 net.cpp:380] res2_5 -> res2_5
I0522 21:55:21.954490 34682 net.cpp:122] Setting up res2_5
I0522 21:55:21.954497 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.954500 34682 net.cpp:137] Memory required for data: 305529600
I0522 21:55:21.954504 34682 layer_factory.hpp:77] Creating layer res2_5_res2_5_0_split
I0522 21:55:21.954509 34682 net.cpp:84] Creating Layer res2_5_res2_5_0_split
I0522 21:55:21.954512 34682 net.cpp:406] res2_5_res2_5_0_split <- res2_5
I0522 21:55:21.954517 34682 net.cpp:380] res2_5_res2_5_0_split -> res2_5_res2_5_0_split_0
I0522 21:55:21.954524 34682 net.cpp:380] res2_5_res2_5_0_split -> res2_5_res2_5_0_split_1
I0522 21:55:21.954568 34682 net.cpp:122] Setting up res2_5_res2_5_0_split
I0522 21:55:21.954576 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.954581 34682 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:55:21.954583 34682 net.cpp:137] Memory required for data: 322044672
I0522 21:55:21.954588 34682 layer_factory.hpp:77] Creating layer res2_5_reduce
I0522 21:55:21.954601 34682 net.cpp:84] Creating Layer res2_5_reduce
I0522 21:55:21.954605 34682 net.cpp:406] res2_5_reduce <- res2_5_res2_5_0_split_0
I0522 21:55:21.954612 34682 net.cpp:380] res2_5_reduce -> res2_5_reduce
I0522 21:55:21.955718 34682 net.cpp:122] Setting up res2_5_reduce
I0522 21:55:21.955734 34682 net.cpp:129] Top shape: 64 72 28 24 (3096576)
I0522 21:55:21.955740 34682 net.cpp:137] Memory required for data: 334430976
I0522 21:55:21.955747 34682 layer_factory.hpp:77] Creating layer pool2
I0522 21:55:21.955756 34682 net.cpp:84] Creating Layer pool2
I0522 21:55:21.955762 34682 net.cpp:406] pool2 <- res2_5_reduce
I0522 21:55:21.955767 34682 net.cpp:380] pool2 -> pool2
I0522 21:55:21.955807 34682 net.cpp:122] Setting up pool2
I0522 21:55:21.955814 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.955818 34682 net.cpp:137] Memory required for data: 337527552
I0522 21:55:21.955821 34682 layer_factory.hpp:77] Creating layer conv3_1
I0522 21:55:21.955838 34682 net.cpp:84] Creating Layer conv3_1
I0522 21:55:21.955843 34682 net.cpp:406] conv3_1 <- res2_5_res2_5_0_split_1
I0522 21:55:21.955848 34682 net.cpp:380] conv3_1 -> conv3_1
I0522 21:55:21.957392 34682 net.cpp:122] Setting up conv3_1
I0522 21:55:21.957407 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.957412 34682 net.cpp:137] Memory required for data: 340624128
I0522 21:55:21.957437 34682 layer_factory.hpp:77] Creating layer relu3_1
I0522 21:55:21.957446 34682 net.cpp:84] Creating Layer relu3_1
I0522 21:55:21.957451 34682 net.cpp:406] relu3_1 <- conv3_1
I0522 21:55:21.957458 34682 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I0522 21:55:21.957556 34682 net.cpp:122] Setting up relu3_1
I0522 21:55:21.957564 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.957567 34682 net.cpp:137] Memory required for data: 343720704
I0522 21:55:21.957573 34682 layer_factory.hpp:77] Creating layer res2_5_p
I0522 21:55:21.957581 34682 net.cpp:84] Creating Layer res2_5_p
I0522 21:55:21.957586 34682 net.cpp:406] res2_5_p <- pool2
I0522 21:55:21.957589 34682 net.cpp:406] res2_5_p <- conv3_1
I0522 21:55:21.957595 34682 net.cpp:380] res2_5_p -> res2_5_p
I0522 21:55:21.957619 34682 net.cpp:122] Setting up res2_5_p
I0522 21:55:21.957626 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.957629 34682 net.cpp:137] Memory required for data: 346817280
I0522 21:55:21.957633 34682 layer_factory.hpp:77] Creating layer res2_5_p_res2_5_p_0_split
I0522 21:55:21.957638 34682 net.cpp:84] Creating Layer res2_5_p_res2_5_p_0_split
I0522 21:55:21.957641 34682 net.cpp:406] res2_5_p_res2_5_p_0_split <- res2_5_p
I0522 21:55:21.957646 34682 net.cpp:380] res2_5_p_res2_5_p_0_split -> res2_5_p_res2_5_p_0_split_0
I0522 21:55:21.957653 34682 net.cpp:380] res2_5_p_res2_5_p_0_split -> res2_5_p_res2_5_p_0_split_1
I0522 21:55:21.957684 34682 net.cpp:122] Setting up res2_5_p_res2_5_p_0_split
I0522 21:55:21.957690 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.957695 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.957697 34682 net.cpp:137] Memory required for data: 353010432
I0522 21:55:21.957701 34682 layer_factory.hpp:77] Creating layer conv3_3
I0522 21:55:21.957712 34682 net.cpp:84] Creating Layer conv3_3
I0522 21:55:21.957717 34682 net.cpp:406] conv3_3 <- res2_5_p_res2_5_p_0_split_0
I0522 21:55:21.957723 34682 net.cpp:380] conv3_3 -> conv3_3
I0522 21:55:21.959717 34682 net.cpp:122] Setting up conv3_3
I0522 21:55:21.959733 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.959738 34682 net.cpp:137] Memory required for data: 356107008
I0522 21:55:21.959744 34682 layer_factory.hpp:77] Creating layer relu3_3
I0522 21:55:21.959764 34682 net.cpp:84] Creating Layer relu3_3
I0522 21:55:21.959770 34682 net.cpp:406] relu3_3 <- conv3_3
I0522 21:55:21.959776 34682 net.cpp:367] relu3_3 -> conv3_3 (in-place)
I0522 21:55:21.959880 34682 net.cpp:122] Setting up relu3_3
I0522 21:55:21.959888 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.959892 34682 net.cpp:137] Memory required for data: 359203584
I0522 21:55:21.959897 34682 layer_factory.hpp:77] Creating layer res3_3
I0522 21:55:21.959903 34682 net.cpp:84] Creating Layer res3_3
I0522 21:55:21.959906 34682 net.cpp:406] res3_3 <- res2_5_p_res2_5_p_0_split_1
I0522 21:55:21.959911 34682 net.cpp:406] res3_3 <- conv3_3
I0522 21:55:21.959916 34682 net.cpp:380] res3_3 -> res3_3
I0522 21:55:21.959941 34682 net.cpp:122] Setting up res3_3
I0522 21:55:21.959949 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.959952 34682 net.cpp:137] Memory required for data: 362300160
I0522 21:55:21.959955 34682 layer_factory.hpp:77] Creating layer res3_3_res3_3_0_split
I0522 21:55:21.959964 34682 net.cpp:84] Creating Layer res3_3_res3_3_0_split
I0522 21:55:21.959967 34682 net.cpp:406] res3_3_res3_3_0_split <- res3_3
I0522 21:55:21.959972 34682 net.cpp:380] res3_3_res3_3_0_split -> res3_3_res3_3_0_split_0
I0522 21:55:21.959978 34682 net.cpp:380] res3_3_res3_3_0_split -> res3_3_res3_3_0_split_1
I0522 21:55:21.960023 34682 net.cpp:122] Setting up res3_3_res3_3_0_split
I0522 21:55:21.960029 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.960034 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.960036 34682 net.cpp:137] Memory required for data: 368493312
I0522 21:55:21.960042 34682 layer_factory.hpp:77] Creating layer conv3_5
I0522 21:55:21.960052 34682 net.cpp:84] Creating Layer conv3_5
I0522 21:55:21.960057 34682 net.cpp:406] conv3_5 <- res3_3_res3_3_0_split_0
I0522 21:55:21.960062 34682 net.cpp:380] conv3_5 -> conv3_5
I0522 21:55:21.962051 34682 net.cpp:122] Setting up conv3_5
I0522 21:55:21.962067 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.962074 34682 net.cpp:137] Memory required for data: 371589888
I0522 21:55:21.962080 34682 layer_factory.hpp:77] Creating layer relu3_5
I0522 21:55:21.962090 34682 net.cpp:84] Creating Layer relu3_5
I0522 21:55:21.962095 34682 net.cpp:406] relu3_5 <- conv3_5
I0522 21:55:21.962100 34682 net.cpp:367] relu3_5 -> conv3_5 (in-place)
I0522 21:55:21.962216 34682 net.cpp:122] Setting up relu3_5
I0522 21:55:21.962224 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.962227 34682 net.cpp:137] Memory required for data: 374686464
I0522 21:55:21.962231 34682 layer_factory.hpp:77] Creating layer res3_5
I0522 21:55:21.962239 34682 net.cpp:84] Creating Layer res3_5
I0522 21:55:21.962242 34682 net.cpp:406] res3_5 <- res3_3_res3_3_0_split_1
I0522 21:55:21.962246 34682 net.cpp:406] res3_5 <- conv3_5
I0522 21:55:21.962251 34682 net.cpp:380] res3_5 -> res3_5
I0522 21:55:21.962273 34682 net.cpp:122] Setting up res3_5
I0522 21:55:21.962280 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.962283 34682 net.cpp:137] Memory required for data: 377783040
I0522 21:55:21.962286 34682 layer_factory.hpp:77] Creating layer res3_5_res3_5_0_split
I0522 21:55:21.962294 34682 net.cpp:84] Creating Layer res3_5_res3_5_0_split
I0522 21:55:21.962299 34682 net.cpp:406] res3_5_res3_5_0_split <- res3_5
I0522 21:55:21.962304 34682 net.cpp:380] res3_5_res3_5_0_split -> res3_5_res3_5_0_split_0
I0522 21:55:21.962311 34682 net.cpp:380] res3_5_res3_5_0_split -> res3_5_res3_5_0_split_1
I0522 21:55:21.962340 34682 net.cpp:122] Setting up res3_5_res3_5_0_split
I0522 21:55:21.962347 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.962350 34682 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:55:21.962353 34682 net.cpp:137] Memory required for data: 383976192
I0522 21:55:21.962357 34682 layer_factory.hpp:77] Creating layer res3_5_reduce
I0522 21:55:21.962366 34682 net.cpp:84] Creating Layer res3_5_reduce
I0522 21:55:21.962371 34682 net.cpp:406] res3_5_reduce <- res3_5_res3_5_0_split_0
I0522 21:55:21.962388 34682 net.cpp:380] res3_5_reduce -> res3_5_reduce
I0522 21:55:21.963620 34682 net.cpp:122] Setting up res3_5_reduce
I0522 21:55:21.963636 34682 net.cpp:129] Top shape: 64 144 14 12 (1548288)
I0522 21:55:21.963640 34682 net.cpp:137] Memory required for data: 390169344
I0522 21:55:21.963647 34682 layer_factory.hpp:77] Creating layer pool3
I0522 21:55:21.963656 34682 net.cpp:84] Creating Layer pool3
I0522 21:55:21.963661 34682 net.cpp:406] pool3 <- res3_5_reduce
I0522 21:55:21.963667 34682 net.cpp:380] pool3 -> pool3
I0522 21:55:21.963706 34682 net.cpp:122] Setting up pool3
I0522 21:55:21.963713 34682 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:21.963716 34682 net.cpp:137] Memory required for data: 391717632
I0522 21:55:21.963719 34682 layer_factory.hpp:77] Creating layer conv4_1
I0522 21:55:21.963728 34682 net.cpp:84] Creating Layer conv4_1
I0522 21:55:21.963733 34682 net.cpp:406] conv4_1 <- res3_5_res3_5_0_split_1
I0522 21:55:21.963739 34682 net.cpp:380] conv4_1 -> conv4_1
I0522 21:55:21.966558 34682 net.cpp:122] Setting up conv4_1
I0522 21:55:21.966573 34682 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:21.966589 34682 net.cpp:137] Memory required for data: 393265920
I0522 21:55:21.966598 34682 layer_factory.hpp:77] Creating layer relu4_1
I0522 21:55:21.966620 34682 net.cpp:84] Creating Layer relu4_1
I0522 21:55:21.966626 34682 net.cpp:406] relu4_1 <- conv4_1
I0522 21:55:21.966631 34682 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I0522 21:55:21.966747 34682 net.cpp:122] Setting up relu4_1
I0522 21:55:21.966755 34682 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:21.966759 34682 net.cpp:137] Memory required for data: 394814208
I0522 21:55:21.966769 34682 layer_factory.hpp:77] Creating layer res3_5_p
I0522 21:55:21.966776 34682 net.cpp:84] Creating Layer res3_5_p
I0522 21:55:21.966780 34682 net.cpp:406] res3_5_p <- pool3
I0522 21:55:21.966785 34682 net.cpp:406] res3_5_p <- conv4_1
I0522 21:55:21.966790 34682 net.cpp:380] res3_5_p -> res3_5_p
I0522 21:55:21.966814 34682 net.cpp:122] Setting up res3_5_p
I0522 21:55:21.966821 34682 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:21.966825 34682 net.cpp:137] Memory required for data: 396362496
I0522 21:55:21.966828 34682 layer_factory.hpp:77] Creating layer res3_5_p_res3_5_p_0_split
I0522 21:55:21.966835 34682 net.cpp:84] Creating Layer res3_5_p_res3_5_p_0_split
I0522 21:55:21.966837 34682 net.cpp:406] res3_5_p_res3_5_p_0_split <- res3_5_p
I0522 21:55:21.966842 34682 net.cpp:380] res3_5_p_res3_5_p_0_split -> res3_5_p_res3_5_p_0_split_0
I0522 21:55:21.966848 34682 net.cpp:380] res3_5_p_res3_5_p_0_split -> res3_5_p_res3_5_p_0_split_1
I0522 21:55:21.966881 34682 net.cpp:122] Setting up res3_5_p_res3_5_p_0_split
I0522 21:55:21.966887 34682 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:21.966892 34682 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:21.966894 34682 net.cpp:137] Memory required for data: 399459072
I0522 21:55:21.966897 34682 layer_factory.hpp:77] Creating layer conv4_3
I0522 21:55:21.966908 34682 net.cpp:84] Creating Layer conv4_3
I0522 21:55:21.966913 34682 net.cpp:406] conv4_3 <- res3_5_p_res3_5_p_0_split_0
I0522 21:55:21.966919 34682 net.cpp:380] conv4_3 -> conv4_3
I0522 21:55:21.970394 34682 net.cpp:122] Setting up conv4_3
I0522 21:55:21.970408 34682 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:21.970424 34682 net.cpp:137] Memory required for data: 401007360
I0522 21:55:21.970433 34682 layer_factory.hpp:77] Creating layer relu4_3
I0522 21:55:21.970458 34682 net.cpp:84] Creating Layer relu4_3
I0522 21:55:21.970463 34682 net.cpp:406] relu4_3 <- conv4_3
I0522 21:55:21.970469 34682 net.cpp:367] relu4_3 -> conv4_3 (in-place)
I0522 21:55:21.970569 34682 net.cpp:122] Setting up relu4_3
I0522 21:55:21.970577 34682 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:21.970580 34682 net.cpp:137] Memory required for data: 402555648
I0522 21:55:21.970584 34682 layer_factory.hpp:77] Creating layer res4_3
I0522 21:55:21.970590 34682 net.cpp:84] Creating Layer res4_3
I0522 21:55:21.970607 34682 net.cpp:406] res4_3 <- res3_5_p_res3_5_p_0_split_1
I0522 21:55:21.970613 34682 net.cpp:406] res4_3 <- conv4_3
I0522 21:55:21.970618 34682 net.cpp:380] res4_3 -> res4_3
I0522 21:55:21.970643 34682 net.cpp:122] Setting up res4_3
I0522 21:55:21.970649 34682 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:55:21.970652 34682 net.cpp:137] Memory required for data: 404103936
I0522 21:55:21.970655 34682 layer_factory.hpp:77] Creating layer fc5
I0522 21:55:21.970664 34682 net.cpp:84] Creating Layer fc5
I0522 21:55:21.970667 34682 net.cpp:406] fc5 <- res4_3
I0522 21:55:21.970674 34682 net.cpp:380] fc5 -> fc5
I0522 21:55:21.982486 34682 net.cpp:122] Setting up fc5
I0522 21:55:21.982502 34682 net.cpp:129] Top shape: 64 256 (16384)
I0522 21:55:21.982506 34682 net.cpp:137] Memory required for data: 404169472
I0522 21:55:21.982530 34682 layer_factory.hpp:77] Creating layer fc5_bn
I0522 21:55:21.982558 34682 net.cpp:84] Creating Layer fc5_bn
I0522 21:55:21.982578 34682 net.cpp:406] fc5_bn <- fc5
I0522 21:55:21.982585 34682 net.cpp:367] fc5_bn -> fc5 (in-place)
I0522 21:55:21.982774 34682 net.cpp:122] Setting up fc5_bn
I0522 21:55:21.982782 34682 net.cpp:129] Top shape: 64 256 (16384)
I0522 21:55:21.982791 34682 net.cpp:137] Memory required for data: 404235008
I0522 21:55:21.982798 34682 layer_factory.hpp:77] Creating layer norm1
I0522 21:55:21.982810 34682 net.cpp:84] Creating Layer norm1
I0522 21:55:21.982815 34682 net.cpp:406] norm1 <- fc5
I0522 21:55:21.982821 34682 net.cpp:380] norm1 -> norm1
I0522 21:55:21.982872 34682 net.cpp:122] Setting up norm1
I0522 21:55:21.982878 34682 net.cpp:129] Top shape: 64 256 1 1 (16384)
I0522 21:55:21.982882 34682 net.cpp:137] Memory required for data: 404300544
I0522 21:55:21.982884 34682 layer_factory.hpp:77] Creating layer fc-6_l2
I0522 21:55:21.982892 34682 net.cpp:84] Creating Layer fc-6_l2
I0522 21:55:21.982897 34682 net.cpp:406] fc-6_l2 <- norm1
I0522 21:55:21.982903 34682 net.cpp:380] fc-6_l2 -> fc-6_l2
I0522 21:55:22.094566 34682 net.cpp:122] Setting up fc-6_l2
I0522 21:55:22.094606 34682 net.cpp:129] Top shape: 64 54547 (3491008)
I0522 21:55:22.094614 34682 net.cpp:137] Memory required for data: 418264576
I0522 21:55:22.094625 34682 layer_factory.hpp:77] Creating layer fc-6_margin
I0522 21:55:22.094656 34682 net.cpp:84] Creating Layer fc-6_margin
I0522 21:55:22.094722 34682 net.cpp:406] fc-6_margin <- fc-6_l2
I0522 21:55:22.094738 34682 net.cpp:406] fc-6_margin <- label_data_1_split_0
I0522 21:55:22.094782 34682 net.cpp:380] fc-6_margin -> fc-6_margin
I0522 21:55:22.094837 34682 net.cpp:122] Setting up fc-6_margin
I0522 21:55:22.094848 34682 net.cpp:129] Top shape: 64 54547 (3491008)
I0522 21:55:22.094853 34682 net.cpp:137] Memory required for data: 432228608
I0522 21:55:22.094888 34682 layer_factory.hpp:77] Creating layer fc-6_margin_scale
I0522 21:55:22.094905 34682 net.cpp:84] Creating Layer fc-6_margin_scale
I0522 21:55:22.094913 34682 net.cpp:406] fc-6_margin_scale <- fc-6_margin
I0522 21:55:22.094923 34682 net.cpp:380] fc-6_margin_scale -> fc-6_margin_scale
I0522 21:55:22.095104 34682 net.cpp:122] Setting up fc-6_margin_scale
I0522 21:55:22.095115 34682 net.cpp:129] Top shape: 64 54547 (3491008)
I0522 21:55:22.095120 34682 net.cpp:137] Memory required for data: 446192640
I0522 21:55:22.095140 34682 layer_factory.hpp:77] Creating layer softmax_loss
I0522 21:55:22.095152 34682 net.cpp:84] Creating Layer softmax_loss
I0522 21:55:22.095160 34682 net.cpp:406] softmax_loss <- fc-6_margin_scale
I0522 21:55:22.095175 34682 net.cpp:406] softmax_loss <- label_data_1_split_1
I0522 21:55:22.095187 34682 net.cpp:380] softmax_loss -> softmax_loss
I0522 21:55:22.095203 34682 layer_factory.hpp:77] Creating layer softmax_loss
I0522 21:55:22.106300 34682 net.cpp:122] Setting up softmax_loss
I0522 21:55:22.106324 34682 net.cpp:129] Top shape: (1)
I0522 21:55:22.106331 34682 net.cpp:132]     with loss weight 1
I0522 21:55:22.106400 34682 net.cpp:137] Memory required for data: 446192644
I0522 21:55:22.106420 34682 net.cpp:198] softmax_loss needs backward computation.
I0522 21:55:22.106446 34682 net.cpp:198] fc-6_margin_scale needs backward computation.
I0522 21:55:22.106475 34682 net.cpp:198] fc-6_margin needs backward computation.
I0522 21:55:22.106485 34682 net.cpp:198] fc-6_l2 needs backward computation.
I0522 21:55:22.106492 34682 net.cpp:198] norm1 needs backward computation.
I0522 21:55:22.106516 34682 net.cpp:198] fc5_bn needs backward computation.
I0522 21:55:22.106523 34682 net.cpp:198] fc5 needs backward computation.
I0522 21:55:22.106529 34682 net.cpp:198] res4_3 needs backward computation.
I0522 21:55:22.106537 34682 net.cpp:198] relu4_3 needs backward computation.
I0522 21:55:22.106551 34682 net.cpp:198] conv4_3 needs backward computation.
I0522 21:55:22.106559 34682 net.cpp:198] res3_5_p_res3_5_p_0_split needs backward computation.
I0522 21:55:22.106564 34682 net.cpp:198] res3_5_p needs backward computation.
I0522 21:55:22.106572 34682 net.cpp:198] relu4_1 needs backward computation.
I0522 21:55:22.106585 34682 net.cpp:198] conv4_1 needs backward computation.
I0522 21:55:22.106592 34682 net.cpp:198] pool3 needs backward computation.
I0522 21:55:22.106598 34682 net.cpp:198] res3_5_reduce needs backward computation.
I0522 21:55:22.106606 34682 net.cpp:198] res3_5_res3_5_0_split needs backward computation.
I0522 21:55:22.106614 34682 net.cpp:198] res3_5 needs backward computation.
I0522 21:55:22.106621 34682 net.cpp:198] relu3_5 needs backward computation.
I0522 21:55:22.106628 34682 net.cpp:198] conv3_5 needs backward computation.
I0522 21:55:22.106637 34682 net.cpp:198] res3_3_res3_3_0_split needs backward computation.
I0522 21:55:22.106644 34682 net.cpp:198] res3_3 needs backward computation.
I0522 21:55:22.106653 34682 net.cpp:198] relu3_3 needs backward computation.
I0522 21:55:22.106660 34682 net.cpp:198] conv3_3 needs backward computation.
I0522 21:55:22.106667 34682 net.cpp:198] res2_5_p_res2_5_p_0_split needs backward computation.
I0522 21:55:22.106673 34682 net.cpp:198] res2_5_p needs backward computation.
I0522 21:55:22.106688 34682 net.cpp:198] relu3_1 needs backward computation.
I0522 21:55:22.106700 34682 net.cpp:198] conv3_1 needs backward computation.
I0522 21:55:22.106709 34682 net.cpp:198] pool2 needs backward computation.
I0522 21:55:22.106715 34682 net.cpp:198] res2_5_reduce needs backward computation.
I0522 21:55:22.106730 34682 net.cpp:198] res2_5_res2_5_0_split needs backward computation.
I0522 21:55:22.106737 34682 net.cpp:198] res2_5 needs backward computation.
I0522 21:55:22.106751 34682 net.cpp:198] relu2_5 needs backward computation.
I0522 21:55:22.106757 34682 net.cpp:198] conv2_5 needs backward computation.
I0522 21:55:22.106765 34682 net.cpp:198] res2_3_res2_3_0_split needs backward computation.
I0522 21:55:22.106771 34682 net.cpp:198] res2_3 needs backward computation.
I0522 21:55:22.106786 34682 net.cpp:198] relu2_3 needs backward computation.
I0522 21:55:22.106791 34682 net.cpp:198] conv2_3 needs backward computation.
I0522 21:55:22.106797 34682 net.cpp:198] res1_3_p_res1_3_p_0_split needs backward computation.
I0522 21:55:22.106804 34682 net.cpp:198] res1_3_p needs backward computation.
I0522 21:55:22.106812 34682 net.cpp:198] relu2_1 needs backward computation.
I0522 21:55:22.106827 34682 net.cpp:198] conv2_1 needs backward computation.
I0522 21:55:22.106845 34682 net.cpp:198] pool1 needs backward computation.
I0522 21:55:22.106863 34682 net.cpp:198] res1_3_reduce needs backward computation.
I0522 21:55:22.106870 34682 net.cpp:198] res1_3_res1_3_0_split needs backward computation.
I0522 21:55:22.106878 34682 net.cpp:198] res1_3 needs backward computation.
I0522 21:55:22.106892 34682 net.cpp:198] relu1_3 needs backward computation.
I0522 21:55:22.106899 34682 net.cpp:198] conv1_3 needs backward computation.
I0522 21:55:22.106905 34682 net.cpp:198] conv1_1_relu1_1_0_split needs backward computation.
I0522 21:55:22.106911 34682 net.cpp:198] relu1_1 needs backward computation.
I0522 21:55:22.106926 34682 net.cpp:198] conv1_1 needs backward computation.
I0522 21:55:22.106935 34682 net.cpp:200] label_data_1_split does not need backward computation.
I0522 21:55:22.106953 34682 net.cpp:200] data does not need backward computation.
I0522 21:55:22.106961 34682 net.cpp:242] This network produces output softmax_loss
I0522 21:55:22.107004 34682 net.cpp:255] Network initialization done.
I0522 21:55:22.107185 34682 solver.cpp:57] Solver scaffolding done.
I0522 21:55:22.108734 34682 caffe.cpp:235] Resuming from ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_1062.solverstate
I0522 21:55:23.190603 34682 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_1062.caffemodel
I0522 21:55:23.190640 34682 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:55:23.218405 34682 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:55:23.359657 34682 caffe.cpp:239] Starting Optimization
I0522 21:55:27.474009 34846 image_data_layer.cpp:38] Opening file /home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/image_list/train_list/Combine_base_Cmul-Asia-beid-cap10-ZheD_list.txt_label.txt
I0522 21:55:27.601068 34848 image_data_layer.cpp:38] Opening file /home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/image_list/train_list/Combine_base_Cmul-Asia-beid-cap10-ZheD_list.txt_label.txt
I0522 21:55:27.682821 34847 image_data_layer.cpp:38] Opening file /home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/image_list/train_list/Combine_base_Cmul-Asia-beid-cap10-ZheD_list.txt_label.txt
I0522 21:55:31.001051 34848 image_data_layer.cpp:53] Shuffling data
I0522 21:55:31.019392 34846 image_data_layer.cpp:53] Shuffling data
I0522 21:55:31.086335 34847 image_data_layer.cpp:53] Shuffling data
I0522 21:55:31.877384 34848 image_data_layer.cpp:63] A total of 2075438 images.
I0522 21:55:31.881158 34848 image_data_layer.cpp:90] output data size: 64,3,112,96
I0522 21:55:31.893182 34846 image_data_layer.cpp:63] A total of 2075438 images.
I0522 21:55:31.903583 34846 image_data_layer.cpp:90] output data size: 64,3,112,96
I0522 21:55:31.957789 34847 image_data_layer.cpp:63] A total of 2075438 images.
I0522 21:55:32.048393 34847 image_data_layer.cpp:90] output data size: 64,3,112,96
I0522 21:55:35.815294 34846 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_1062.caffemodel
I0522 21:55:35.815343 34846 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:55:35.867673 34846 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:55:36.095836 34848 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_1062.caffemodel
I0522 21:55:36.095865 34848 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:55:36.147594 34848 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:55:37.723770 34847 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_1062.caffemodel
I0522 21:55:37.723810 34847 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:55:37.741714 34847 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:55:38.041244 34682 solver.cpp:293] Solving 2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_train
I0522 21:55:38.041334 34682 solver.cpp:294] Learning Rate Policy: multistep
I0522 21:55:39.678511 34682 solver.cpp:239] Iteration 1070 (661.529 iter/s, 1.61747s/10 iters), loss = 10.6174
I0522 21:55:39.678603 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.6174 (* 1 = 10.6174 loss)
I0522 21:55:39.716877 34682 sgd_solver.cpp:112] Iteration 1070, lr = 0.01
I0522 21:55:41.025509 34682 solver.cpp:239] Iteration 1080 (7.42477 iter/s, 1.34684s/10 iters), loss = 10.9876
I0522 21:55:41.025589 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.9876 (* 1 = 10.9876 loss)
I0522 21:55:41.066615 34682 sgd_solver.cpp:112] Iteration 1080, lr = 0.01
I0522 21:55:42.404657 34682 solver.cpp:239] Iteration 1090 (7.25158 iter/s, 1.37901s/10 iters), loss = 10.824
I0522 21:55:42.404711 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.824 (* 1 = 10.824 loss)
I0522 21:55:42.447216 34682 sgd_solver.cpp:112] Iteration 1090, lr = 0.01
I0522 21:55:44.317440 34682 solver.cpp:239] Iteration 1100 (5.22839 iter/s, 1.91264s/10 iters), loss = 10.2648
I0522 21:55:44.317488 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2648 (* 1 = 10.2648 loss)
I0522 21:55:44.366215 34682 sgd_solver.cpp:112] Iteration 1100, lr = 0.01
I0522 21:55:45.639207 34682 solver.cpp:239] Iteration 1110 (7.56634 iter/s, 1.32164s/10 iters), loss = 11.0488
I0522 21:55:45.639271 34682 solver.cpp:258]     Train net output #0: softmax_loss = 11.0488 (* 1 = 11.0488 loss)
I0522 21:55:45.679239 34682 sgd_solver.cpp:112] Iteration 1110, lr = 0.01
I0522 21:55:47.088569 34682 solver.cpp:239] Iteration 1120 (6.90029 iter/s, 1.44921s/10 iters), loss = 10.249
I0522 21:55:47.088874 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.249 (* 1 = 10.249 loss)
I0522 21:55:47.500886 34682 sgd_solver.cpp:112] Iteration 1120, lr = 0.01
I0522 21:55:48.814873 34682 solver.cpp:239] Iteration 1130 (5.79402 iter/s, 1.72592s/10 iters), loss = 10.3341
I0522 21:55:48.814923 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3341 (* 1 = 10.3341 loss)
I0522 21:55:48.849927 34682 sgd_solver.cpp:112] Iteration 1130, lr = 0.01
I0522 21:55:50.372985 34682 solver.cpp:239] Iteration 1140 (6.41859 iter/s, 1.55798s/10 iters), loss = 10.6735
I0522 21:55:50.373054 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.6735 (* 1 = 10.6735 loss)
I0522 21:55:50.409740 34682 sgd_solver.cpp:112] Iteration 1140, lr = 0.01
I0522 21:55:51.836983 34682 solver.cpp:239] Iteration 1150 (6.83123 iter/s, 1.46387s/10 iters), loss = 10.4844
I0522 21:55:51.837044 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.4844 (* 1 = 10.4844 loss)
I0522 21:55:51.890910 34682 sgd_solver.cpp:112] Iteration 1150, lr = 0.01
I0522 21:55:53.343575 34682 solver.cpp:239] Iteration 1160 (6.63814 iter/s, 1.50645s/10 iters), loss = 10.5557
I0522 21:55:53.343663 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.5557 (* 1 = 10.5557 loss)
I0522 21:55:53.489617 34682 sgd_solver.cpp:112] Iteration 1160, lr = 0.01
I0522 21:55:54.963659 34682 solver.cpp:239] Iteration 1170 (6.17309 iter/s, 1.61993s/10 iters), loss = 10.1414
I0522 21:55:54.963698 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1414 (* 1 = 10.1414 loss)
I0522 21:55:55.019534 34682 sgd_solver.cpp:112] Iteration 1170, lr = 0.01
I0522 21:55:56.391203 34682 solver.cpp:239] Iteration 1180 (7.0056 iter/s, 1.42743s/10 iters), loss = 11.0817
I0522 21:55:56.391268 34682 solver.cpp:258]     Train net output #0: softmax_loss = 11.0817 (* 1 = 11.0817 loss)
I0522 21:55:56.440135 34682 sgd_solver.cpp:112] Iteration 1180, lr = 0.01
I0522 21:55:57.692025 34682 solver.cpp:239] Iteration 1190 (7.68815 iter/s, 1.3007s/10 iters), loss = 10.4961
I0522 21:55:57.692092 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.4961 (* 1 = 10.4961 loss)
I0522 21:55:57.727124 34682 sgd_solver.cpp:112] Iteration 1190, lr = 0.01
I0522 21:55:59.050626 34682 solver.cpp:239] Iteration 1200 (7.36126 iter/s, 1.35846s/10 iters), loss = 10.4349
I0522 21:55:59.050685 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.4349 (* 1 = 10.4349 loss)
I0522 21:55:59.092423 34682 sgd_solver.cpp:112] Iteration 1200, lr = 0.01
I0522 21:56:00.754767 34682 solver.cpp:239] Iteration 1210 (5.86853 iter/s, 1.704s/10 iters), loss = 10.4293
I0522 21:56:00.754833 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.4293 (* 1 = 10.4293 loss)
I0522 21:56:00.799443 34682 sgd_solver.cpp:112] Iteration 1210, lr = 0.01
I0522 21:56:02.051576 34682 solver.cpp:239] Iteration 1220 (7.71197 iter/s, 1.29669s/10 iters), loss = 10.4739
I0522 21:56:02.051653 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.4739 (* 1 = 10.4739 loss)
I0522 21:56:02.101583 34682 sgd_solver.cpp:112] Iteration 1220, lr = 0.01
I0522 21:56:03.802415 34682 solver.cpp:239] Iteration 1230 (5.71197 iter/s, 1.75071s/10 iters), loss = 10.2118
I0522 21:56:03.802459 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2118 (* 1 = 10.2118 loss)
I0522 21:56:03.849503 34682 sgd_solver.cpp:112] Iteration 1230, lr = 0.01
I0522 21:56:05.036283 34682 solver.cpp:239] Iteration 1240 (8.10542 iter/s, 1.23374s/10 iters), loss = 10.8141
I0522 21:56:05.036355 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.8141 (* 1 = 10.8141 loss)
I0522 21:56:05.088016 34682 sgd_solver.cpp:112] Iteration 1240, lr = 0.01
I0522 21:56:06.672549 34682 solver.cpp:239] Iteration 1250 (6.11207 iter/s, 1.63611s/10 iters), loss = 10.4846
I0522 21:56:06.672608 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.4846 (* 1 = 10.4846 loss)
I0522 21:56:06.721776 34682 sgd_solver.cpp:112] Iteration 1250, lr = 0.01
I0522 21:56:08.401268 34682 solver.cpp:239] Iteration 1260 (5.78508 iter/s, 1.72858s/10 iters), loss = 10.2272
I0522 21:56:08.401350 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2272 (* 1 = 10.2272 loss)
I0522 21:56:08.449537 34682 sgd_solver.cpp:112] Iteration 1260, lr = 0.01
I0522 21:56:09.632618 34682 solver.cpp:239] Iteration 1270 (8.12208 iter/s, 1.23121s/10 iters), loss = 10.5823
I0522 21:56:09.632669 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.5823 (* 1 = 10.5823 loss)
I0522 21:56:09.688033 34682 sgd_solver.cpp:112] Iteration 1270, lr = 0.01
I0522 21:56:11.232321 34682 solver.cpp:239] Iteration 1280 (6.25169 iter/s, 1.59957s/10 iters), loss = 10.6107
I0522 21:56:11.232403 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.6107 (* 1 = 10.6107 loss)
I0522 21:56:11.281069 34682 sgd_solver.cpp:112] Iteration 1280, lr = 0.01
I0522 21:56:12.494405 34682 solver.cpp:239] Iteration 1290 (7.92429 iter/s, 1.26194s/10 iters), loss = 10.0828
I0522 21:56:12.494468 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0828 (* 1 = 10.0828 loss)
I0522 21:56:12.540484 34682 sgd_solver.cpp:112] Iteration 1290, lr = 0.01
I0522 21:56:14.002485 34682 solver.cpp:239] Iteration 1300 (6.63158 iter/s, 1.50794s/10 iters), loss = 10.0566
I0522 21:56:14.002538 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0566 (* 1 = 10.0566 loss)
I0522 21:56:14.048779 34682 sgd_solver.cpp:112] Iteration 1300, lr = 0.01
I0522 21:56:17.319175 34682 solver.cpp:239] Iteration 1310 (3.01523 iter/s, 3.31649s/10 iters), loss = 10.217
I0522 21:56:17.319449 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.217 (* 1 = 10.217 loss)
I0522 21:56:17.382611 34682 sgd_solver.cpp:112] Iteration 1310, lr = 0.01
I0522 21:56:20.662468 34682 solver.cpp:239] Iteration 1320 (2.99142 iter/s, 3.3429s/10 iters), loss = 10.4324
I0522 21:56:20.662528 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.4324 (* 1 = 10.4324 loss)
I0522 21:56:20.912590 34682 sgd_solver.cpp:112] Iteration 1320, lr = 0.01
I0522 21:56:25.553159 34682 solver.cpp:239] Iteration 1330 (2.04482 iter/s, 4.89042s/10 iters), loss = 10.724
I0522 21:56:25.553218 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.724 (* 1 = 10.724 loss)
I0522 21:56:26.362202 34682 sgd_solver.cpp:112] Iteration 1330, lr = 0.01
I0522 21:56:30.694528 34682 solver.cpp:239] Iteration 1340 (1.94511 iter/s, 5.14108s/10 iters), loss = 10.3796
I0522 21:56:30.694605 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3796 (* 1 = 10.3796 loss)
I0522 21:56:30.759022 34682 sgd_solver.cpp:112] Iteration 1340, lr = 0.01
I0522 21:56:34.688493 34682 solver.cpp:239] Iteration 1350 (2.50394 iter/s, 3.9937s/10 iters), loss = 10.2779
I0522 21:56:34.688593 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2779 (* 1 = 10.2779 loss)
I0522 21:56:35.575435 34682 sgd_solver.cpp:112] Iteration 1350, lr = 0.01
I0522 21:56:39.147018 34682 solver.cpp:239] Iteration 1360 (2.24303 iter/s, 4.45825s/10 iters), loss = 10.1621
I0522 21:56:39.147071 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1621 (* 1 = 10.1621 loss)
I0522 21:56:39.220558 34682 sgd_solver.cpp:112] Iteration 1360, lr = 0.01
I0522 21:56:42.255164 34682 solver.cpp:239] Iteration 1370 (3.21756 iter/s, 3.10795s/10 iters), loss = 10.1218
I0522 21:56:42.255208 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1218 (* 1 = 10.1218 loss)
I0522 21:56:43.083925 34682 sgd_solver.cpp:112] Iteration 1370, lr = 0.01
I0522 21:56:49.684703 34682 solver.cpp:239] Iteration 1380 (1.34604 iter/s, 7.42919s/10 iters), loss = 10.4007
I0522 21:56:49.685060 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.4007 (* 1 = 10.4007 loss)
I0522 21:56:49.806733 34682 sgd_solver.cpp:112] Iteration 1380, lr = 0.01
I0522 21:56:55.044440 34682 solver.cpp:239] Iteration 1390 (1.86596 iter/s, 5.35918s/10 iters), loss = 10.1941
I0522 21:56:55.044497 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1941 (* 1 = 10.1941 loss)
I0522 21:56:55.112509 34682 sgd_solver.cpp:112] Iteration 1390, lr = 0.01
I0522 21:57:00.330947 34682 solver.cpp:239] Iteration 1400 (1.89171 iter/s, 5.28623s/10 iters), loss = 10.1926
I0522 21:57:00.331018 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1926 (* 1 = 10.1926 loss)
I0522 21:57:00.404238 34682 sgd_solver.cpp:112] Iteration 1400, lr = 0.01
I0522 21:57:04.861558 34682 solver.cpp:239] Iteration 1410 (2.20732 iter/s, 4.53037s/10 iters), loss = 10.3199
I0522 21:57:04.861618 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3199 (* 1 = 10.3199 loss)
I0522 21:57:04.938652 34682 sgd_solver.cpp:112] Iteration 1410, lr = 0.01
I0522 21:57:08.349704 34682 solver.cpp:239] Iteration 1420 (2.86703 iter/s, 3.48793s/10 iters), loss = 10.4021
I0522 21:57:08.349748 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.4021 (* 1 = 10.4021 loss)
I0522 21:57:09.173949 34682 sgd_solver.cpp:112] Iteration 1420, lr = 0.01
I0522 21:57:13.644826 34682 solver.cpp:239] Iteration 1430 (1.88862 iter/s, 5.29486s/10 iters), loss = 10.498
I0522 21:57:13.644894 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.498 (* 1 = 10.498 loss)
I0522 21:57:13.728993 34682 sgd_solver.cpp:112] Iteration 1430, lr = 0.01
I0522 21:57:17.094959 34682 solver.cpp:239] Iteration 1440 (2.89862 iter/s, 3.44992s/10 iters), loss = 10.5703
I0522 21:57:17.095021 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.5703 (* 1 = 10.5703 loss)
I0522 21:57:17.151880 34682 sgd_solver.cpp:112] Iteration 1440, lr = 0.01
I0522 21:57:23.483345 34682 solver.cpp:239] Iteration 1450 (1.56544 iter/s, 6.38799s/10 iters), loss = 10.3649
I0522 21:57:23.483613 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3649 (* 1 = 10.3649 loss)
I0522 21:57:23.547087 34682 sgd_solver.cpp:112] Iteration 1450, lr = 0.01
I0522 21:57:26.346926 34682 solver.cpp:239] Iteration 1460 (3.49257 iter/s, 2.86322s/10 iters), loss = 10.1568
I0522 21:57:26.346967 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1568 (* 1 = 10.1568 loss)
I0522 21:57:27.084203 34682 sgd_solver.cpp:112] Iteration 1460, lr = 0.01
I0522 21:57:34.263932 34682 solver.cpp:239] Iteration 1470 (1.26316 iter/s, 7.91665s/10 iters), loss = 10.3637
I0522 21:57:34.263996 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3637 (* 1 = 10.3637 loss)
I0522 21:57:34.321771 34682 sgd_solver.cpp:112] Iteration 1470, lr = 0.01
I0522 21:57:40.589243 34682 solver.cpp:239] Iteration 1480 (1.58103 iter/s, 6.32498s/10 iters), loss = 10.1627
I0522 21:57:40.589310 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1627 (* 1 = 10.1627 loss)
I0522 21:57:41.444893 34682 sgd_solver.cpp:112] Iteration 1480, lr = 0.01
I0522 21:57:45.905813 34682 solver.cpp:239] Iteration 1490 (1.88101 iter/s, 5.31628s/10 iters), loss = 10.2285
I0522 21:57:45.905865 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2285 (* 1 = 10.2285 loss)
I0522 21:57:46.671583 34682 sgd_solver.cpp:112] Iteration 1490, lr = 0.01
I0522 21:57:51.651679 34682 solver.cpp:239] Iteration 1500 (1.74048 iter/s, 5.74556s/10 iters), loss = 10.4878
I0522 21:57:51.651748 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.4878 (* 1 = 10.4878 loss)
I0522 21:57:52.450870 34682 sgd_solver.cpp:112] Iteration 1500, lr = 0.01
I0522 21:57:57.801249 34682 solver.cpp:239] Iteration 1510 (1.62622 iter/s, 6.14925s/10 iters), loss = 10.3575
I0522 21:57:57.801558 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3575 (* 1 = 10.3575 loss)
I0522 21:57:57.861006 34682 sgd_solver.cpp:112] Iteration 1510, lr = 0.01
I0522 21:58:04.258971 34682 solver.cpp:239] Iteration 1520 (1.54866 iter/s, 6.45718s/10 iters), loss = 10.4397
I0522 21:58:04.259037 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.4397 (* 1 = 10.4397 loss)
I0522 21:58:04.324029 34682 sgd_solver.cpp:112] Iteration 1520, lr = 0.01
I0522 21:58:07.773702 34682 solver.cpp:239] Iteration 1530 (2.84534 iter/s, 3.51452s/10 iters), loss = 10.1908
I0522 21:58:07.773751 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1908 (* 1 = 10.1908 loss)
I0522 21:58:07.829960 34682 sgd_solver.cpp:112] Iteration 1530, lr = 0.01
I0522 21:58:10.651616 34682 solver.cpp:239] Iteration 1540 (3.47496 iter/s, 2.87773s/10 iters), loss = 10.0682
I0522 21:58:10.651677 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0682 (* 1 = 10.0682 loss)
I0522 21:58:11.458453 34682 sgd_solver.cpp:112] Iteration 1540, lr = 0.01
I0522 21:58:14.722362 34682 solver.cpp:239] Iteration 1550 (2.45671 iter/s, 4.07048s/10 iters), loss = 9.8721
I0522 21:58:14.722447 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.8721 (* 1 = 9.8721 loss)
I0522 21:58:15.577391 34682 sgd_solver.cpp:112] Iteration 1550, lr = 0.01
I0522 21:58:21.120483 34682 solver.cpp:239] Iteration 1560 (1.56304 iter/s, 6.39778s/10 iters), loss = 10.2437
I0522 21:58:21.120550 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2437 (* 1 = 10.2437 loss)
I0522 21:58:21.191004 34682 sgd_solver.cpp:112] Iteration 1560, lr = 0.01
I0522 21:58:24.663909 34682 solver.cpp:239] Iteration 1570 (2.8223 iter/s, 3.54321s/10 iters), loss = 10
I0522 21:58:24.663961 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10 (* 1 = 10 loss)
I0522 21:58:25.369837 34682 sgd_solver.cpp:112] Iteration 1570, lr = 0.01
I0522 21:58:29.874796 34682 solver.cpp:239] Iteration 1580 (1.91916 iter/s, 5.21061s/10 iters), loss = 10.5559
I0522 21:58:29.875058 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.5559 (* 1 = 10.5559 loss)
I0522 21:58:30.050710 34682 sgd_solver.cpp:112] Iteration 1580, lr = 0.01
I0522 21:58:34.185611 34682 solver.cpp:239] Iteration 1590 (2.31997 iter/s, 4.3104s/10 iters), loss = 10.3505
I0522 21:58:34.185672 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3505 (* 1 = 10.3505 loss)
I0522 21:58:34.764497 34682 sgd_solver.cpp:112] Iteration 1590, lr = 0.01
I0522 21:58:38.584663 34682 solver.cpp:239] Iteration 1600 (2.27335 iter/s, 4.3988s/10 iters), loss = 10.2214
I0522 21:58:38.584740 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2214 (* 1 = 10.2214 loss)
I0522 21:58:39.171901 34682 sgd_solver.cpp:112] Iteration 1600, lr = 0.01
I0522 21:58:43.913727 34682 solver.cpp:239] Iteration 1610 (1.87661 iter/s, 5.32877s/10 iters), loss = 10.0653
I0522 21:58:43.913781 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0653 (* 1 = 10.0653 loss)
I0522 21:58:43.980996 34682 sgd_solver.cpp:112] Iteration 1610, lr = 0.01
I0522 21:58:47.336555 34682 solver.cpp:239] Iteration 1620 (2.92173 iter/s, 3.42263s/10 iters), loss = 10.3632
I0522 21:58:47.336611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3632 (* 1 = 10.3632 loss)
I0522 21:58:48.235581 34682 sgd_solver.cpp:112] Iteration 1620, lr = 0.01
I0522 21:58:51.356618 34682 solver.cpp:239] Iteration 1630 (2.48766 iter/s, 4.01984s/10 iters), loss = 10.1771
I0522 21:58:51.356693 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1771 (* 1 = 10.1771 loss)
I0522 21:58:51.437821 34682 sgd_solver.cpp:112] Iteration 1630, lr = 0.01
I0522 21:58:55.521638 34682 solver.cpp:239] Iteration 1640 (2.40108 iter/s, 4.16479s/10 iters), loss = 10.3375
I0522 21:58:55.521682 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3375 (* 1 = 10.3375 loss)
I0522 21:58:56.382596 34682 sgd_solver.cpp:112] Iteration 1640, lr = 0.01
I0522 21:59:00.006301 34682 solver.cpp:239] Iteration 1650 (2.22994 iter/s, 4.48442s/10 iters), loss = 10.3457
I0522 21:59:00.006522 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3457 (* 1 = 10.3457 loss)
I0522 21:59:00.079869 34682 sgd_solver.cpp:112] Iteration 1650, lr = 0.01
I0522 21:59:08.546555 34682 solver.cpp:239] Iteration 1660 (1.171 iter/s, 8.53969s/10 iters), loss = 10.2889
I0522 21:59:08.546604 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2889 (* 1 = 10.2889 loss)
I0522 21:59:08.613205 34682 sgd_solver.cpp:112] Iteration 1660, lr = 0.01
I0522 21:59:14.559273 34682 solver.cpp:239] Iteration 1670 (1.66322 iter/s, 6.01242s/10 iters), loss = 10.6633
I0522 21:59:14.559320 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.6633 (* 1 = 10.6633 loss)
I0522 21:59:14.623412 34682 sgd_solver.cpp:112] Iteration 1670, lr = 0.01
I0522 21:59:19.340560 34682 solver.cpp:239] Iteration 1680 (2.0916 iter/s, 4.78104s/10 iters), loss = 10.3084
I0522 21:59:19.340622 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3084 (* 1 = 10.3084 loss)
I0522 21:59:19.413951 34682 sgd_solver.cpp:112] Iteration 1680, lr = 0.01
I0522 21:59:25.090904 34682 solver.cpp:239] Iteration 1690 (1.73912 iter/s, 5.75004s/10 iters), loss = 10.3393
I0522 21:59:25.090971 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3393 (* 1 = 10.3393 loss)
I0522 21:59:25.156123 34682 sgd_solver.cpp:112] Iteration 1690, lr = 0.01
I0522 21:59:31.745851 34682 solver.cpp:239] Iteration 1700 (1.50272 iter/s, 6.65462s/10 iters), loss = 9.94462
I0522 21:59:31.746042 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.94462 (* 1 = 9.94462 loss)
I0522 21:59:31.807185 34682 sgd_solver.cpp:112] Iteration 1700, lr = 0.01
I0522 21:59:37.456836 34682 solver.cpp:239] Iteration 1710 (1.75182 iter/s, 5.70835s/10 iters), loss = 10.3397
I0522 21:59:37.456892 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3397 (* 1 = 10.3397 loss)
I0522 21:59:37.523385 34682 sgd_solver.cpp:112] Iteration 1710, lr = 0.01
I0522 21:59:41.517231 34682 solver.cpp:239] Iteration 1720 (2.46296 iter/s, 4.06016s/10 iters), loss = 10.3035
I0522 21:59:41.517279 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3035 (* 1 = 10.3035 loss)
I0522 21:59:41.576292 34682 sgd_solver.cpp:112] Iteration 1720, lr = 0.01
I0522 21:59:46.465128 34682 solver.cpp:239] Iteration 1730 (2.02116 iter/s, 4.94765s/10 iters), loss = 10.0591
I0522 21:59:46.465188 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0591 (* 1 = 10.0591 loss)
I0522 21:59:47.300248 34682 sgd_solver.cpp:112] Iteration 1730, lr = 0.01
I0522 21:59:51.965173 34682 solver.cpp:239] Iteration 1740 (1.81826 iter/s, 5.49975s/10 iters), loss = 10.0244
I0522 21:59:51.965243 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0244 (* 1 = 10.0244 loss)
I0522 21:59:52.046963 34682 sgd_solver.cpp:112] Iteration 1740, lr = 0.01
I0522 21:59:56.818604 34682 solver.cpp:239] Iteration 1750 (2.06052 iter/s, 4.85314s/10 iters), loss = 10.186
I0522 21:59:56.818661 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.186 (* 1 = 10.186 loss)
I0522 21:59:57.642560 34682 sgd_solver.cpp:112] Iteration 1750, lr = 0.01
I0522 22:00:02.151656 34682 solver.cpp:239] Iteration 1760 (1.8752 iter/s, 5.33278s/10 iters), loss = 10.3944
I0522 22:00:02.151947 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3944 (* 1 = 10.3944 loss)
I0522 22:00:03.016360 34682 sgd_solver.cpp:112] Iteration 1760, lr = 0.01
I0522 22:00:08.021467 34682 solver.cpp:239] Iteration 1770 (1.70378 iter/s, 5.86929s/10 iters), loss = 10.2207
I0522 22:00:08.021525 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2207 (* 1 = 10.2207 loss)
I0522 22:00:08.091614 34682 sgd_solver.cpp:112] Iteration 1770, lr = 0.01
I0522 22:00:13.844609 34682 solver.cpp:239] Iteration 1780 (1.71737 iter/s, 5.82285s/10 iters), loss = 10.178
I0522 22:00:13.844655 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.178 (* 1 = 10.178 loss)
I0522 22:00:14.665894 34682 sgd_solver.cpp:112] Iteration 1780, lr = 0.01
I0522 22:00:17.615677 34682 solver.cpp:239] Iteration 1790 (2.65191 iter/s, 3.77087s/10 iters), loss = 9.79375
I0522 22:00:17.615718 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.79375 (* 1 = 9.79375 loss)
I0522 22:00:17.694084 34682 sgd_solver.cpp:112] Iteration 1790, lr = 0.01
I0522 22:00:22.442401 34682 solver.cpp:239] Iteration 1800 (2.0719 iter/s, 4.82648s/10 iters), loss = 10.0296
I0522 22:00:22.442463 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0296 (* 1 = 10.0296 loss)
I0522 22:00:22.619946 34682 sgd_solver.cpp:112] Iteration 1800, lr = 0.01
I0522 22:00:27.483640 34682 solver.cpp:239] Iteration 1810 (1.98375 iter/s, 5.04095s/10 iters), loss = 10.0951
I0522 22:00:27.483707 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0951 (* 1 = 10.0951 loss)
I0522 22:00:27.561879 34682 sgd_solver.cpp:112] Iteration 1810, lr = 0.01
I0522 22:00:33.565088 34682 solver.cpp:239] Iteration 1820 (1.64443 iter/s, 6.08113s/10 iters), loss = 10.1539
I0522 22:00:33.565285 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1539 (* 1 = 10.1539 loss)
I0522 22:00:33.637703 34682 sgd_solver.cpp:112] Iteration 1820, lr = 0.01
I0522 22:00:38.032897 34682 solver.cpp:239] Iteration 1830 (2.23842 iter/s, 4.46743s/10 iters), loss = 10.2795
I0522 22:00:38.032976 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2795 (* 1 = 10.2795 loss)
I0522 22:00:38.901125 34682 sgd_solver.cpp:112] Iteration 1830, lr = 0.01
I0522 22:00:43.088500 34682 solver.cpp:239] Iteration 1840 (1.97813 iter/s, 5.05529s/10 iters), loss = 9.93855
I0522 22:00:43.088599 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.93855 (* 1 = 9.93855 loss)
I0522 22:00:43.599799 34682 sgd_solver.cpp:112] Iteration 1840, lr = 0.01
I0522 22:00:48.768456 34682 solver.cpp:239] Iteration 1850 (1.76068 iter/s, 5.67963s/10 iters), loss = 10.5245
I0522 22:00:48.768508 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.5245 (* 1 = 10.5245 loss)
I0522 22:00:49.577973 34682 sgd_solver.cpp:112] Iteration 1850, lr = 0.01
I0522 22:00:52.937538 34682 solver.cpp:239] Iteration 1860 (2.39874 iter/s, 4.16886s/10 iters), loss = 9.56315
I0522 22:00:52.937588 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56315 (* 1 = 9.56315 loss)
I0522 22:00:53.013711 34682 sgd_solver.cpp:112] Iteration 1860, lr = 0.01
I0522 22:00:57.390213 34682 solver.cpp:239] Iteration 1870 (2.24597 iter/s, 4.45243s/10 iters), loss = 10.2358
I0522 22:00:57.390269 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2358 (* 1 = 10.2358 loss)
I0522 22:00:57.454563 34682 sgd_solver.cpp:112] Iteration 1870, lr = 0.01
I0522 22:01:00.810420 34682 solver.cpp:239] Iteration 1880 (2.92398 iter/s, 3.42s/10 iters), loss = 10.053
I0522 22:01:00.810473 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.053 (* 1 = 10.053 loss)
I0522 22:01:01.330096 34682 sgd_solver.cpp:112] Iteration 1880, lr = 0.01
I0522 22:01:06.305202 34682 solver.cpp:239] Iteration 1890 (1.82 iter/s, 5.49451s/10 iters), loss = 9.94663
I0522 22:01:06.305488 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.94663 (* 1 = 9.94663 loss)
I0522 22:01:06.367276 34682 sgd_solver.cpp:112] Iteration 1890, lr = 0.01
I0522 22:01:11.041033 34682 solver.cpp:239] Iteration 1900 (2.11176 iter/s, 4.73538s/10 iters), loss = 10.0437
I0522 22:01:11.041088 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0437 (* 1 = 10.0437 loss)
I0522 22:01:11.110038 34682 sgd_solver.cpp:112] Iteration 1900, lr = 0.01
I0522 22:01:17.341850 34682 solver.cpp:239] Iteration 1910 (1.58718 iter/s, 6.3005s/10 iters), loss = 10.3215
I0522 22:01:17.341909 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3215 (* 1 = 10.3215 loss)
I0522 22:01:17.411018 34682 sgd_solver.cpp:112] Iteration 1910, lr = 0.01
I0522 22:01:22.092257 34682 solver.cpp:239] Iteration 1920 (2.1052 iter/s, 4.75014s/10 iters), loss = 10.2095
I0522 22:01:22.092326 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2095 (* 1 = 10.2095 loss)
I0522 22:01:22.920578 34682 sgd_solver.cpp:112] Iteration 1920, lr = 0.01
I0522 22:01:26.659627 34682 solver.cpp:239] Iteration 1930 (2.18957 iter/s, 4.5671s/10 iters), loss = 10.3238
I0522 22:01:26.659680 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3238 (* 1 = 10.3238 loss)
I0522 22:01:27.359719 34682 sgd_solver.cpp:112] Iteration 1930, lr = 0.01
I0522 22:01:32.374377 34682 solver.cpp:239] Iteration 1940 (1.74995 iter/s, 5.71444s/10 iters), loss = 10.238
I0522 22:01:32.374465 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.238 (* 1 = 10.238 loss)
I0522 22:01:32.433153 34682 sgd_solver.cpp:112] Iteration 1940, lr = 0.01
I0522 22:01:36.890257 34682 solver.cpp:239] Iteration 1950 (2.21454 iter/s, 4.51561s/10 iters), loss = 9.77857
I0522 22:01:36.890518 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.77857 (* 1 = 9.77857 loss)
I0522 22:01:36.957455 34682 sgd_solver.cpp:112] Iteration 1950, lr = 0.01
I0522 22:01:41.884454 34682 solver.cpp:239] Iteration 1960 (2.0025 iter/s, 4.99375s/10 iters), loss = 9.84846
I0522 22:01:41.884510 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.84846 (* 1 = 9.84846 loss)
I0522 22:01:42.716081 34682 sgd_solver.cpp:112] Iteration 1960, lr = 0.01
I0522 22:01:47.116554 34682 solver.cpp:239] Iteration 1970 (1.91138 iter/s, 5.23183s/10 iters), loss = 10.0298
I0522 22:01:47.116607 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0298 (* 1 = 10.0298 loss)
I0522 22:01:47.580138 34682 sgd_solver.cpp:112] Iteration 1970, lr = 0.01
I0522 22:01:52.513427 34682 solver.cpp:239] Iteration 1980 (1.85302 iter/s, 5.3966s/10 iters), loss = 10.0478
I0522 22:01:52.513480 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0478 (* 1 = 10.0478 loss)
I0522 22:01:52.570498 34682 sgd_solver.cpp:112] Iteration 1980, lr = 0.01
I0522 22:01:57.051420 34682 solver.cpp:239] Iteration 1990 (2.20374 iter/s, 4.53775s/10 iters), loss = 9.56688
I0522 22:01:57.051479 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56688 (* 1 = 9.56688 loss)
I0522 22:01:57.121290 34682 sgd_solver.cpp:112] Iteration 1990, lr = 0.01
I0522 22:02:01.328665 34682 solver.cpp:239] Iteration 2000 (2.33809 iter/s, 4.27699s/10 iters), loss = 10.3856
I0522 22:02:01.328757 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3856 (* 1 = 10.3856 loss)
I0522 22:02:01.385341 34682 sgd_solver.cpp:112] Iteration 2000, lr = 0.01
I0522 22:02:05.899802 34682 solver.cpp:239] Iteration 2010 (2.18777 iter/s, 4.57086s/10 iters), loss = 10.0505
I0522 22:02:05.899842 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0505 (* 1 = 10.0505 loss)
I0522 22:02:05.973407 34682 sgd_solver.cpp:112] Iteration 2010, lr = 0.01
I0522 22:02:12.093369 34682 solver.cpp:239] Iteration 2020 (1.61466 iter/s, 6.19326s/10 iters), loss = 9.99524
I0522 22:02:12.093626 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.99524 (* 1 = 9.99524 loss)
I0522 22:02:12.155310 34682 sgd_solver.cpp:112] Iteration 2020, lr = 0.01
I0522 22:02:15.520509 34682 solver.cpp:239] Iteration 2030 (2.91821 iter/s, 3.42676s/10 iters), loss = 10.1161
I0522 22:02:15.520568 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1161 (* 1 = 10.1161 loss)
I0522 22:02:16.334105 34682 sgd_solver.cpp:112] Iteration 2030, lr = 0.01
I0522 22:02:20.602376 34682 solver.cpp:239] Iteration 2040 (1.96789 iter/s, 5.08158s/10 iters), loss = 10.2411
I0522 22:02:20.602452 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2411 (* 1 = 10.2411 loss)
I0522 22:02:20.675526 34682 sgd_solver.cpp:112] Iteration 2040, lr = 0.01
I0522 22:02:24.790532 34682 solver.cpp:239] Iteration 2050 (2.38782 iter/s, 4.18791s/10 iters), loss = 10.2833
I0522 22:02:24.790602 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2833 (* 1 = 10.2833 loss)
I0522 22:02:25.425156 34682 sgd_solver.cpp:112] Iteration 2050, lr = 0.01
I0522 22:02:29.396489 34682 solver.cpp:239] Iteration 2060 (2.17122 iter/s, 4.60572s/10 iters), loss = 9.86045
I0522 22:02:29.396534 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.86045 (* 1 = 9.86045 loss)
I0522 22:02:29.463218 34682 sgd_solver.cpp:112] Iteration 2060, lr = 0.01
I0522 22:02:34.059216 34682 solver.cpp:239] Iteration 2070 (2.14478 iter/s, 4.66249s/10 iters), loss = 9.93222
I0522 22:02:34.059273 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.93222 (* 1 = 9.93222 loss)
I0522 22:02:34.520699 34682 sgd_solver.cpp:112] Iteration 2070, lr = 0.01
I0522 22:02:37.868017 34682 solver.cpp:239] Iteration 2080 (2.62565 iter/s, 3.80858s/10 iters), loss = 9.71459
I0522 22:02:37.868073 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.71459 (* 1 = 9.71459 loss)
I0522 22:02:37.942616 34682 sgd_solver.cpp:112] Iteration 2080, lr = 0.01
I0522 22:02:43.729002 34682 solver.cpp:239] Iteration 2090 (1.70629 iter/s, 5.86066s/10 iters), loss = 10.2234
I0522 22:02:43.729202 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2234 (* 1 = 10.2234 loss)
I0522 22:02:43.797262 34682 sgd_solver.cpp:112] Iteration 2090, lr = 0.01
I0522 22:02:49.198985 34682 solver.cpp:239] Iteration 2100 (1.82975 iter/s, 5.46524s/10 iters), loss = 9.62954
I0522 22:02:49.199076 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.62954 (* 1 = 9.62954 loss)
I0522 22:02:49.353224 34682 sgd_solver.cpp:112] Iteration 2100, lr = 0.01
I0522 22:02:53.497308 34682 solver.cpp:239] Iteration 2110 (2.32663 iter/s, 4.29806s/10 iters), loss = 9.93859
I0522 22:02:53.497362 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.93859 (* 1 = 9.93859 loss)
I0522 22:02:53.553931 34682 sgd_solver.cpp:112] Iteration 2110, lr = 0.01
I0522 22:02:57.724512 34682 solver.cpp:239] Iteration 2120 (2.36577 iter/s, 4.22696s/10 iters), loss = 9.99687
I0522 22:02:57.724581 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.99687 (* 1 = 9.99687 loss)
I0522 22:02:57.788856 34682 sgd_solver.cpp:112] Iteration 2120, lr = 0.01
I0522 22:03:02.665251 34682 solver.cpp:239] Iteration 2130 (2.0241 iter/s, 4.94047s/10 iters), loss = 9.70857
I0522 22:03:02.665302 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.70857 (* 1 = 9.70857 loss)
I0522 22:03:02.723273 34682 sgd_solver.cpp:112] Iteration 2130, lr = 0.01
I0522 22:03:07.521275 34682 solver.cpp:239] Iteration 2140 (2.0594 iter/s, 4.85578s/10 iters), loss = 10.0125
I0522 22:03:07.521327 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0125 (* 1 = 10.0125 loss)
I0522 22:03:07.592443 34682 sgd_solver.cpp:112] Iteration 2140, lr = 0.01
I0522 22:03:10.649771 34682 solver.cpp:239] Iteration 2150 (3.19663 iter/s, 3.1283s/10 iters), loss = 10.0552
I0522 22:03:10.649830 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0552 (* 1 = 10.0552 loss)
I0522 22:03:11.312750 34682 sgd_solver.cpp:112] Iteration 2150, lr = 0.01
I0522 22:03:17.106209 34682 solver.cpp:239] Iteration 2160 (1.54892 iter/s, 6.45611s/10 iters), loss = 10.0692
I0522 22:03:17.106396 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0692 (* 1 = 10.0692 loss)
I0522 22:03:17.173013 34682 sgd_solver.cpp:112] Iteration 2160, lr = 0.01
I0522 22:03:22.801815 34682 solver.cpp:239] Iteration 2170 (1.75587 iter/s, 5.69518s/10 iters), loss = 9.71036
I0522 22:03:22.801880 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.71036 (* 1 = 9.71036 loss)
I0522 22:03:23.623005 34682 sgd_solver.cpp:112] Iteration 2170, lr = 0.01
I0522 22:03:27.787030 34682 solver.cpp:239] Iteration 2180 (2.00604 iter/s, 4.98493s/10 iters), loss = 9.73477
I0522 22:03:27.787098 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.73477 (* 1 = 9.73477 loss)
I0522 22:03:28.555824 34682 sgd_solver.cpp:112] Iteration 2180, lr = 0.01
I0522 22:03:33.367925 34682 solver.cpp:239] Iteration 2190 (1.79192 iter/s, 5.5806s/10 iters), loss = 9.91081
I0522 22:03:33.367971 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.91081 (* 1 = 9.91081 loss)
I0522 22:03:33.447456 34682 sgd_solver.cpp:112] Iteration 2190, lr = 0.01
I0522 22:03:39.006773 34682 solver.cpp:239] Iteration 2200 (1.77351 iter/s, 5.63854s/10 iters), loss = 10.0534
I0522 22:03:39.006822 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0534 (* 1 = 10.0534 loss)
I0522 22:03:39.070451 34682 sgd_solver.cpp:112] Iteration 2200, lr = 0.01
I0522 22:03:44.454627 34682 solver.cpp:239] Iteration 2210 (1.83568 iter/s, 5.44759s/10 iters), loss = 9.52072
I0522 22:03:44.454690 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.52072 (* 1 = 9.52072 loss)
I0522 22:03:44.534322 34682 sgd_solver.cpp:112] Iteration 2210, lr = 0.01
I0522 22:03:48.229912 34682 solver.cpp:239] Iteration 2220 (2.64896 iter/s, 3.77506s/10 iters), loss = 9.89878
I0522 22:03:48.230154 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.89878 (* 1 = 9.89878 loss)
I0522 22:03:48.290879 34682 sgd_solver.cpp:112] Iteration 2220, lr = 0.01
I0522 22:03:52.150816 34682 solver.cpp:239] Iteration 2230 (2.55069 iter/s, 3.92051s/10 iters), loss = 10.1996
I0522 22:03:52.150878 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1996 (* 1 = 10.1996 loss)
I0522 22:03:52.222569 34682 sgd_solver.cpp:112] Iteration 2230, lr = 0.01
I0522 22:03:56.595966 34682 solver.cpp:239] Iteration 2240 (2.24977 iter/s, 4.4449s/10 iters), loss = 10.1339
I0522 22:03:56.596016 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1339 (* 1 = 10.1339 loss)
I0522 22:03:56.653416 34682 sgd_solver.cpp:112] Iteration 2240, lr = 0.01
I0522 22:04:02.068265 34682 solver.cpp:239] Iteration 2250 (1.82748 iter/s, 5.47202s/10 iters), loss = 9.94249
I0522 22:04:02.068320 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.94249 (* 1 = 9.94249 loss)
I0522 22:04:02.145965 34682 sgd_solver.cpp:112] Iteration 2250, lr = 0.01
I0522 22:04:06.884374 34682 solver.cpp:239] Iteration 2260 (2.07648 iter/s, 4.81585s/10 iters), loss = 9.53521
I0522 22:04:06.884449 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.53521 (* 1 = 9.53521 loss)
I0522 22:04:07.485728 34682 sgd_solver.cpp:112] Iteration 2260, lr = 0.01
I0522 22:04:12.137949 34682 solver.cpp:239] Iteration 2270 (1.90357 iter/s, 5.25329s/10 iters), loss = 9.73847
I0522 22:04:12.137989 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.73847 (* 1 = 9.73847 loss)
I0522 22:04:12.210767 34682 sgd_solver.cpp:112] Iteration 2270, lr = 0.01
I0522 22:04:18.082948 34682 solver.cpp:239] Iteration 2280 (1.68217 iter/s, 5.94472s/10 iters), loss = 9.92744
I0522 22:04:18.082996 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.92744 (* 1 = 9.92744 loss)
I0522 22:04:18.832038 34682 sgd_solver.cpp:112] Iteration 2280, lr = 0.01
I0522 22:04:23.606748 34682 solver.cpp:239] Iteration 2290 (1.81045 iter/s, 5.52348s/10 iters), loss = 10.2836
I0522 22:04:23.606842 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2836 (* 1 = 10.2836 loss)
I0522 22:04:24.400367 34682 sgd_solver.cpp:112] Iteration 2290, lr = 0.01
I0522 22:04:28.543085 34682 solver.cpp:239] Iteration 2300 (2.02592 iter/s, 4.93603s/10 iters), loss = 9.7533
I0522 22:04:28.543145 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.7533 (* 1 = 9.7533 loss)
I0522 22:04:28.607305 34682 sgd_solver.cpp:112] Iteration 2300, lr = 0.01
I0522 22:04:34.414036 34682 solver.cpp:239] Iteration 2310 (1.70339 iter/s, 5.87065s/10 iters), loss = 9.74325
I0522 22:04:34.414084 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.74325 (* 1 = 9.74325 loss)
I0522 22:04:34.478857 34682 sgd_solver.cpp:112] Iteration 2310, lr = 0.01
I0522 22:04:40.046576 34682 solver.cpp:239] Iteration 2320 (1.77549 iter/s, 5.63225s/10 iters), loss = 10.1051
I0522 22:04:40.046646 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1051 (* 1 = 10.1051 loss)
I0522 22:04:40.865772 34682 sgd_solver.cpp:112] Iteration 2320, lr = 0.01
I0522 22:04:47.397724 34682 solver.cpp:239] Iteration 2330 (1.3604 iter/s, 7.35077s/10 iters), loss = 9.59905
I0522 22:04:47.397799 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59905 (* 1 = 9.59905 loss)
I0522 22:04:48.112934 34682 sgd_solver.cpp:112] Iteration 2330, lr = 0.01
I0522 22:04:51.348103 34682 solver.cpp:239] Iteration 2340 (2.53157 iter/s, 3.95012s/10 iters), loss = 9.36556
I0522 22:04:51.348362 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36556 (* 1 = 9.36556 loss)
I0522 22:04:51.410421 34682 sgd_solver.cpp:112] Iteration 2340, lr = 0.01
I0522 22:04:56.128835 34682 solver.cpp:239] Iteration 2350 (2.09192 iter/s, 4.78029s/10 iters), loss = 9.68085
I0522 22:04:56.128873 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.68085 (* 1 = 9.68085 loss)
I0522 22:04:56.185725 34682 sgd_solver.cpp:112] Iteration 2350, lr = 0.01
I0522 22:05:01.284355 34682 solver.cpp:239] Iteration 2360 (1.93976 iter/s, 5.15527s/10 iters), loss = 9.86587
I0522 22:05:01.284407 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.86587 (* 1 = 9.86587 loss)
I0522 22:05:01.922066 34682 sgd_solver.cpp:112] Iteration 2360, lr = 0.01
I0522 22:05:06.657820 34682 solver.cpp:239] Iteration 2370 (1.8611 iter/s, 5.37317s/10 iters), loss = 9.81239
I0522 22:05:06.657881 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.81239 (* 1 = 9.81239 loss)
I0522 22:05:06.725445 34682 sgd_solver.cpp:112] Iteration 2370, lr = 0.01
I0522 22:05:09.869321 34682 solver.cpp:239] Iteration 2380 (3.114 iter/s, 3.2113s/10 iters), loss = 9.71333
I0522 22:05:09.869370 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.71333 (* 1 = 9.71333 loss)
I0522 22:05:10.637784 34682 sgd_solver.cpp:112] Iteration 2380, lr = 0.01
I0522 22:05:14.608023 34682 solver.cpp:239] Iteration 2390 (2.11039 iter/s, 4.73846s/10 iters), loss = 9.75023
I0522 22:05:14.608067 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.75023 (* 1 = 9.75023 loss)
I0522 22:05:14.671461 34682 sgd_solver.cpp:112] Iteration 2390, lr = 0.01
I0522 22:05:20.852562 34682 solver.cpp:239] Iteration 2400 (1.60148 iter/s, 6.24423s/10 iters), loss = 9.90787
I0522 22:05:20.852617 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.90787 (* 1 = 9.90787 loss)
I0522 22:05:20.924959 34682 sgd_solver.cpp:112] Iteration 2400, lr = 0.01
I0522 22:05:26.701967 34682 solver.cpp:239] Iteration 2410 (1.70966 iter/s, 5.84911s/10 iters), loss = 9.94999
I0522 22:05:26.702189 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.94999 (* 1 = 9.94999 loss)
I0522 22:05:26.773838 34682 sgd_solver.cpp:112] Iteration 2410, lr = 0.01
I0522 22:05:31.079571 34682 solver.cpp:239] Iteration 2420 (2.28455 iter/s, 4.37723s/10 iters), loss = 9.98083
I0522 22:05:31.079635 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.98083 (* 1 = 9.98083 loss)
I0522 22:05:31.141163 34682 sgd_solver.cpp:112] Iteration 2420, lr = 0.01
I0522 22:05:36.279196 34682 solver.cpp:239] Iteration 2430 (1.92332 iter/s, 5.19934s/10 iters), loss = 10.2984
I0522 22:05:36.279249 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2984 (* 1 = 10.2984 loss)
I0522 22:05:36.351495 34682 sgd_solver.cpp:112] Iteration 2430, lr = 0.01
I0522 22:05:41.319699 34682 solver.cpp:239] Iteration 2440 (1.98404 iter/s, 5.04023s/10 iters), loss = 9.61983
I0522 22:05:41.319747 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.61983 (* 1 = 9.61983 loss)
I0522 22:05:41.382015 34682 sgd_solver.cpp:112] Iteration 2440, lr = 0.01
I0522 22:05:45.187688 34682 solver.cpp:239] Iteration 2450 (2.58547 iter/s, 3.86777s/10 iters), loss = 9.45864
I0522 22:05:45.187752 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.45864 (* 1 = 9.45864 loss)
I0522 22:05:45.251649 34682 sgd_solver.cpp:112] Iteration 2450, lr = 0.01
I0522 22:05:48.600126 34682 solver.cpp:239] Iteration 2460 (2.93063 iter/s, 3.41224s/10 iters), loss = 9.53349
I0522 22:05:48.600188 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.53349 (* 1 = 9.53349 loss)
I0522 22:05:48.669075 34682 sgd_solver.cpp:112] Iteration 2460, lr = 0.01
I0522 22:05:51.911010 34682 solver.cpp:239] Iteration 2470 (3.02053 iter/s, 3.31067s/10 iters), loss = 9.96135
I0522 22:05:51.911072 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.96135 (* 1 = 9.96135 loss)
I0522 22:05:51.973212 34682 sgd_solver.cpp:112] Iteration 2470, lr = 0.01
I0522 22:05:56.145431 34682 solver.cpp:239] Iteration 2480 (2.36174 iter/s, 4.23418s/10 iters), loss = 9.76393
I0522 22:05:56.145489 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.76393 (* 1 = 9.76393 loss)
I0522 22:05:56.225280 34682 sgd_solver.cpp:112] Iteration 2480, lr = 0.01
I0522 22:06:00.265295 34682 solver.cpp:239] Iteration 2490 (2.4274 iter/s, 4.11963s/10 iters), loss = 10.1292
I0522 22:06:00.265568 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1292 (* 1 = 10.1292 loss)
I0522 22:06:00.338656 34682 sgd_solver.cpp:112] Iteration 2490, lr = 0.01
I0522 22:06:05.325072 34682 solver.cpp:239] Iteration 2500 (1.97655 iter/s, 5.05932s/10 iters), loss = 9.39857
I0522 22:06:05.325129 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.39857 (* 1 = 9.39857 loss)
I0522 22:06:05.388181 34682 sgd_solver.cpp:112] Iteration 2500, lr = 0.01
I0522 22:06:09.425026 34682 solver.cpp:239] Iteration 2510 (2.43919 iter/s, 4.09971s/10 iters), loss = 9.2164
I0522 22:06:09.425091 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2164 (* 1 = 9.2164 loss)
I0522 22:06:10.248000 34682 sgd_solver.cpp:112] Iteration 2510, lr = 0.01
I0522 22:06:13.836755 34682 solver.cpp:239] Iteration 2520 (2.26682 iter/s, 4.41146s/10 iters), loss = 9.56572
I0522 22:06:13.836830 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56572 (* 1 = 9.56572 loss)
I0522 22:06:14.452482 34682 sgd_solver.cpp:112] Iteration 2520, lr = 0.01
I0522 22:06:18.727934 34682 solver.cpp:239] Iteration 2530 (2.04462 iter/s, 4.89088s/10 iters), loss = 9.59125
I0522 22:06:18.727990 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59125 (* 1 = 9.59125 loss)
I0522 22:06:18.785260 34682 sgd_solver.cpp:112] Iteration 2530, lr = 0.01
I0522 22:06:23.475821 34682 solver.cpp:239] Iteration 2540 (2.10632 iter/s, 4.74763s/10 iters), loss = 9.04745
I0522 22:06:23.475893 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04745 (* 1 = 9.04745 loss)
I0522 22:06:24.320062 34682 sgd_solver.cpp:112] Iteration 2540, lr = 0.01
I0522 22:06:28.352159 34682 solver.cpp:239] Iteration 2550 (2.05083 iter/s, 4.87608s/10 iters), loss = 9.57726
I0522 22:06:28.352208 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.57726 (* 1 = 9.57726 loss)
I0522 22:06:28.940790 34682 sgd_solver.cpp:112] Iteration 2550, lr = 0.01
I0522 22:06:33.625111 34682 solver.cpp:239] Iteration 2560 (1.89657 iter/s, 5.27269s/10 iters), loss = 10.1168
I0522 22:06:33.625332 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1168 (* 1 = 10.1168 loss)
I0522 22:06:33.823621 34682 sgd_solver.cpp:112] Iteration 2560, lr = 0.01
I0522 22:06:38.142871 34682 solver.cpp:239] Iteration 2570 (2.21367 iter/s, 4.51738s/10 iters), loss = 9.89349
I0522 22:06:38.142913 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.89349 (* 1 = 9.89349 loss)
I0522 22:06:38.702365 34682 sgd_solver.cpp:112] Iteration 2570, lr = 0.01
I0522 22:06:41.964835 34682 solver.cpp:239] Iteration 2580 (2.6166 iter/s, 3.82175s/10 iters), loss = 9.83126
I0522 22:06:41.964903 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.83126 (* 1 = 9.83126 loss)
I0522 22:06:42.711671 34682 sgd_solver.cpp:112] Iteration 2580, lr = 0.01
I0522 22:06:46.251487 34682 solver.cpp:239] Iteration 2590 (2.33297 iter/s, 4.28639s/10 iters), loss = 9.74125
I0522 22:06:46.251551 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.74125 (* 1 = 9.74125 loss)
I0522 22:06:46.316032 34682 sgd_solver.cpp:112] Iteration 2590, lr = 0.01
I0522 22:06:49.498114 34682 solver.cpp:239] Iteration 2600 (3.08032 iter/s, 3.24642s/10 iters), loss = 9.8469
I0522 22:06:49.498174 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.8469 (* 1 = 9.8469 loss)
I0522 22:06:50.354457 34682 sgd_solver.cpp:112] Iteration 2600, lr = 0.01
I0522 22:06:53.135416 34682 solver.cpp:239] Iteration 2610 (2.74946 iter/s, 3.63708s/10 iters), loss = 9.43883
I0522 22:06:53.135464 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43883 (* 1 = 9.43883 loss)
I0522 22:06:53.212707 34682 sgd_solver.cpp:112] Iteration 2610, lr = 0.01
I0522 22:06:56.589360 34682 solver.cpp:239] Iteration 2620 (2.89541 iter/s, 3.45375s/10 iters), loss = 9.94263
I0522 22:06:56.589432 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.94263 (* 1 = 9.94263 loss)
I0522 22:06:57.306460 34682 sgd_solver.cpp:112] Iteration 2620, lr = 0.01
I0522 22:07:01.306332 34682 solver.cpp:239] Iteration 2630 (2.12013 iter/s, 4.7167s/10 iters), loss = 9.62457
I0522 22:07:01.306404 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.62457 (* 1 = 9.62457 loss)
I0522 22:07:02.155179 34682 sgd_solver.cpp:112] Iteration 2630, lr = 0.01
I0522 22:07:08.994230 34682 solver.cpp:239] Iteration 2640 (1.30081 iter/s, 7.68752s/10 iters), loss = 9.61796
I0522 22:07:08.994457 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.61796 (* 1 = 9.61796 loss)
I0522 22:07:09.591260 34682 sgd_solver.cpp:112] Iteration 2640, lr = 0.01
I0522 22:07:13.696513 34682 solver.cpp:239] Iteration 2650 (2.12682 iter/s, 4.70186s/10 iters), loss = 9.60763
I0522 22:07:13.696571 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60763 (* 1 = 9.60763 loss)
I0522 22:07:13.756096 34682 sgd_solver.cpp:112] Iteration 2650, lr = 0.01
I0522 22:07:20.232276 34682 solver.cpp:239] Iteration 2660 (1.53012 iter/s, 6.53542s/10 iters), loss = 9.46946
I0522 22:07:20.232363 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.46946 (* 1 = 9.46946 loss)
I0522 22:07:21.095057 34682 sgd_solver.cpp:112] Iteration 2660, lr = 0.01
I0522 22:07:26.388295 34682 solver.cpp:239] Iteration 2670 (1.62452 iter/s, 6.15567s/10 iters), loss = 9.52595
I0522 22:07:26.388362 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.52595 (* 1 = 9.52595 loss)
I0522 22:07:26.462123 34682 sgd_solver.cpp:112] Iteration 2670, lr = 0.01
I0522 22:07:32.830536 34682 solver.cpp:239] Iteration 2680 (1.55233 iter/s, 6.44191s/10 iters), loss = 9.89485
I0522 22:07:32.830590 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.89485 (* 1 = 9.89485 loss)
I0522 22:07:33.477603 34682 sgd_solver.cpp:112] Iteration 2680, lr = 0.01
I0522 22:07:37.920070 34682 solver.cpp:239] Iteration 2690 (1.96492 iter/s, 5.08928s/10 iters), loss = 9.42237
I0522 22:07:37.920112 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42237 (* 1 = 9.42237 loss)
I0522 22:07:37.987752 34682 sgd_solver.cpp:112] Iteration 2690, lr = 0.01
I0522 22:07:41.738499 34682 solver.cpp:239] Iteration 2700 (2.61903 iter/s, 3.81821s/10 iters), loss = 9.26682
I0522 22:07:41.738816 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.26682 (* 1 = 9.26682 loss)
I0522 22:07:41.793776 34682 sgd_solver.cpp:112] Iteration 2700, lr = 0.01
I0522 22:07:46.907145 34682 solver.cpp:239] Iteration 2710 (1.93493 iter/s, 5.16815s/10 iters), loss = 9.71667
I0522 22:07:46.907193 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.71667 (* 1 = 9.71667 loss)
I0522 22:07:47.711030 34682 sgd_solver.cpp:112] Iteration 2710, lr = 0.01
I0522 22:07:53.361513 34682 solver.cpp:239] Iteration 2720 (1.54942 iter/s, 6.45404s/10 iters), loss = 9.5071
I0522 22:07:53.361579 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.5071 (* 1 = 9.5071 loss)
I0522 22:07:53.425323 34682 sgd_solver.cpp:112] Iteration 2720, lr = 0.01
I0522 22:07:57.046115 34682 solver.cpp:239] Iteration 2730 (2.71417 iter/s, 3.68436s/10 iters), loss = 8.9885
I0522 22:07:57.046178 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9885 (* 1 = 8.9885 loss)
I0522 22:07:57.106557 34682 sgd_solver.cpp:112] Iteration 2730, lr = 0.01
I0522 22:08:02.468641 34682 solver.cpp:239] Iteration 2740 (1.84426 iter/s, 5.42223s/10 iters), loss = 9.83854
I0522 22:08:02.468703 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.83854 (* 1 = 9.83854 loss)
I0522 22:08:02.528010 34682 sgd_solver.cpp:112] Iteration 2740, lr = 0.01
I0522 22:08:06.839996 34682 solver.cpp:239] Iteration 2750 (2.28774 iter/s, 4.37112s/10 iters), loss = 10.0861
I0522 22:08:06.840045 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0861 (* 1 = 10.0861 loss)
I0522 22:08:06.905920 34682 sgd_solver.cpp:112] Iteration 2750, lr = 0.01
I0522 22:08:14.676906 34682 solver.cpp:239] Iteration 2760 (1.27607 iter/s, 7.83653s/10 iters), loss = 9.51751
I0522 22:08:14.677202 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51751 (* 1 = 9.51751 loss)
I0522 22:08:15.477500 34682 sgd_solver.cpp:112] Iteration 2760, lr = 0.01
I0522 22:08:20.528405 34682 solver.cpp:239] Iteration 2770 (1.70911 iter/s, 5.851s/10 iters), loss = 9.14836
I0522 22:08:20.528465 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14836 (* 1 = 9.14836 loss)
I0522 22:08:20.611680 34682 sgd_solver.cpp:112] Iteration 2770, lr = 0.01
I0522 22:08:25.405875 34682 solver.cpp:239] Iteration 2780 (2.05036 iter/s, 4.8772s/10 iters), loss = 9.44448
I0522 22:08:25.405926 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44448 (* 1 = 9.44448 loss)
I0522 22:08:25.467372 34682 sgd_solver.cpp:112] Iteration 2780, lr = 0.01
I0522 22:08:31.449285 34682 solver.cpp:239] Iteration 2790 (1.65478 iter/s, 6.04311s/10 iters), loss = 9.65446
I0522 22:08:31.449339 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.65446 (* 1 = 9.65446 loss)
I0522 22:08:31.561836 34682 sgd_solver.cpp:112] Iteration 2790, lr = 0.01
I0522 22:08:38.187551 34682 solver.cpp:239] Iteration 2800 (1.48414 iter/s, 6.73793s/10 iters), loss = 9.52277
I0522 22:08:38.187614 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.52277 (* 1 = 9.52277 loss)
I0522 22:08:39.056272 34682 sgd_solver.cpp:112] Iteration 2800, lr = 0.01
I0522 22:08:43.952832 34682 solver.cpp:239] Iteration 2810 (1.73461 iter/s, 5.76498s/10 iters), loss = 9.65808
I0522 22:08:43.952906 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.65808 (* 1 = 9.65808 loss)
I0522 22:08:44.014194 34682 sgd_solver.cpp:112] Iteration 2810, lr = 0.01
I0522 22:08:47.984979 34682 solver.cpp:239] Iteration 2820 (2.48023 iter/s, 4.03189s/10 iters), loss = 9.65002
I0522 22:08:47.985214 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.65002 (* 1 = 9.65002 loss)
I0522 22:08:48.044574 34682 sgd_solver.cpp:112] Iteration 2820, lr = 0.01
I0522 22:08:52.243984 34682 solver.cpp:239] Iteration 2830 (2.34819 iter/s, 4.2586s/10 iters), loss = 9.76366
I0522 22:08:52.244045 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.76366 (* 1 = 9.76366 loss)
I0522 22:08:52.318806 34682 sgd_solver.cpp:112] Iteration 2830, lr = 0.01
I0522 22:08:57.036159 34682 solver.cpp:239] Iteration 2840 (2.08684 iter/s, 4.79192s/10 iters), loss = 8.9393
I0522 22:08:57.036204 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9393 (* 1 = 8.9393 loss)
I0522 22:08:57.833025 34682 sgd_solver.cpp:112] Iteration 2840, lr = 0.01
I0522 22:09:02.709138 34682 solver.cpp:239] Iteration 2850 (1.76283 iter/s, 5.6727s/10 iters), loss = 9.36853
I0522 22:09:02.709189 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36853 (* 1 = 9.36853 loss)
I0522 22:09:02.773505 34682 sgd_solver.cpp:112] Iteration 2850, lr = 0.01
I0522 22:09:07.501132 34682 solver.cpp:239] Iteration 2860 (2.08694 iter/s, 4.79172s/10 iters), loss = 9.38308
I0522 22:09:07.501193 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.38308 (* 1 = 9.38308 loss)
I0522 22:09:07.564190 34682 sgd_solver.cpp:112] Iteration 2860, lr = 0.01
I0522 22:09:13.179105 34682 solver.cpp:239] Iteration 2870 (1.7613 iter/s, 5.67762s/10 iters), loss = 9.43035
I0522 22:09:13.179160 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43035 (* 1 = 9.43035 loss)
I0522 22:09:14.015581 34682 sgd_solver.cpp:112] Iteration 2870, lr = 0.01
I0522 22:09:19.416205 34682 solver.cpp:239] Iteration 2880 (1.60339 iter/s, 6.23678s/10 iters), loss = 9.56869
I0522 22:09:19.416508 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56869 (* 1 = 9.56869 loss)
I0522 22:09:20.244206 34682 sgd_solver.cpp:112] Iteration 2880, lr = 0.01
I0522 22:09:25.058670 34682 solver.cpp:239] Iteration 2890 (1.77244 iter/s, 5.64195s/10 iters), loss = 9.98799
I0522 22:09:25.058817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.98799 (* 1 = 9.98799 loss)
I0522 22:09:25.135772 34682 sgd_solver.cpp:112] Iteration 2890, lr = 0.01
I0522 22:09:29.743400 34682 solver.cpp:239] Iteration 2900 (2.13475 iter/s, 4.6844s/10 iters), loss = 9.92044
I0522 22:09:29.743504 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.92044 (* 1 = 9.92044 loss)
I0522 22:09:30.516227 34682 sgd_solver.cpp:112] Iteration 2900, lr = 0.01
I0522 22:09:34.402279 34682 solver.cpp:239] Iteration 2910 (2.14656 iter/s, 4.65861s/10 iters), loss = 9.37241
I0522 22:09:34.402344 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.37241 (* 1 = 9.37241 loss)
I0522 22:09:34.471668 34682 sgd_solver.cpp:112] Iteration 2910, lr = 0.01
I0522 22:09:40.982596 34682 solver.cpp:239] Iteration 2920 (1.51976 iter/s, 6.58s/10 iters), loss = 8.71781
I0522 22:09:40.982652 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71781 (* 1 = 8.71781 loss)
I0522 22:09:41.676252 34682 sgd_solver.cpp:112] Iteration 2920, lr = 0.01
I0522 22:09:46.309516 34682 solver.cpp:239] Iteration 2930 (1.87736 iter/s, 5.32664s/10 iters), loss = 9.43172
I0522 22:09:46.309566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43172 (* 1 = 9.43172 loss)
I0522 22:09:46.371460 34682 sgd_solver.cpp:112] Iteration 2930, lr = 0.01
I0522 22:09:49.815652 34682 solver.cpp:239] Iteration 2940 (2.8523 iter/s, 3.50594s/10 iters), loss = 8.81315
I0522 22:09:49.815817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81315 (* 1 = 8.81315 loss)
I0522 22:09:50.622184 34682 sgd_solver.cpp:112] Iteration 2940, lr = 0.01
I0522 22:09:53.949579 34682 solver.cpp:239] Iteration 2950 (2.41921 iter/s, 4.13357s/10 iters), loss = 9.64826
I0522 22:09:53.949631 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.64826 (* 1 = 9.64826 loss)
I0522 22:09:54.016167 34682 sgd_solver.cpp:112] Iteration 2950, lr = 0.01
I0522 22:09:59.485299 34682 solver.cpp:239] Iteration 2960 (1.80654 iter/s, 5.53543s/10 iters), loss = 9.68653
I0522 22:09:59.485371 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.68653 (* 1 = 9.68653 loss)
I0522 22:10:00.342839 34682 sgd_solver.cpp:112] Iteration 2960, lr = 0.01
I0522 22:10:05.129525 34682 solver.cpp:239] Iteration 2970 (1.77182 iter/s, 5.64392s/10 iters), loss = 9.69862
I0522 22:10:05.129583 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.69862 (* 1 = 9.69862 loss)
I0522 22:10:05.189530 34682 sgd_solver.cpp:112] Iteration 2970, lr = 0.01
I0522 22:10:08.633024 34682 solver.cpp:239] Iteration 2980 (2.85446 iter/s, 3.50329s/10 iters), loss = 9.97337
I0522 22:10:08.633088 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.97337 (* 1 = 9.97337 loss)
I0522 22:10:08.705950 34682 sgd_solver.cpp:112] Iteration 2980, lr = 0.01
I0522 22:10:12.908247 34682 solver.cpp:239] Iteration 2990 (2.33919 iter/s, 4.27498s/10 iters), loss = 9.76171
I0522 22:10:12.908304 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.76171 (* 1 = 9.76171 loss)
I0522 22:10:12.972292 34682 sgd_solver.cpp:112] Iteration 2990, lr = 0.01
I0522 22:10:19.018501 34682 solver.cpp:239] Iteration 3000 (1.63667 iter/s, 6.10995s/10 iters), loss = 9.59417
I0522 22:10:19.018555 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59417 (* 1 = 9.59417 loss)
I0522 22:10:19.074172 34682 sgd_solver.cpp:112] Iteration 3000, lr = 0.01
I0522 22:10:25.402252 34682 solver.cpp:239] Iteration 3010 (1.56655 iter/s, 6.38344s/10 iters), loss = 9.22792
I0522 22:10:25.402503 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22792 (* 1 = 9.22792 loss)
I0522 22:10:25.483173 34682 sgd_solver.cpp:112] Iteration 3010, lr = 0.01
I0522 22:10:30.194811 34682 solver.cpp:239] Iteration 3020 (2.08676 iter/s, 4.79212s/10 iters), loss = 9.30399
I0522 22:10:30.194862 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30399 (* 1 = 9.30399 loss)
I0522 22:10:30.270951 34682 sgd_solver.cpp:112] Iteration 3020, lr = 0.01
I0522 22:10:35.941282 34682 solver.cpp:239] Iteration 3030 (1.74029 iter/s, 5.74617s/10 iters), loss = 9.34594
I0522 22:10:35.941342 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34594 (* 1 = 9.34594 loss)
I0522 22:10:36.008441 34682 sgd_solver.cpp:112] Iteration 3030, lr = 0.01
I0522 22:10:41.593534 34682 solver.cpp:239] Iteration 3040 (1.7693 iter/s, 5.65196s/10 iters), loss = 9.45818
I0522 22:10:41.593592 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.45818 (* 1 = 9.45818 loss)
I0522 22:10:41.665885 34682 sgd_solver.cpp:112] Iteration 3040, lr = 0.01
I0522 22:10:47.532086 34682 solver.cpp:239] Iteration 3050 (1.684 iter/s, 5.93825s/10 iters), loss = 9.14417
I0522 22:10:47.532156 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14417 (* 1 = 9.14417 loss)
I0522 22:10:48.134423 34682 sgd_solver.cpp:112] Iteration 3050, lr = 0.01
I0522 22:10:54.458334 34682 solver.cpp:239] Iteration 3060 (1.44386 iter/s, 6.9259s/10 iters), loss = 9.45925
I0522 22:10:54.458384 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.45925 (* 1 = 9.45925 loss)
I0522 22:10:54.522931 34682 sgd_solver.cpp:112] Iteration 3060, lr = 0.01
I0522 22:10:57.969992 34682 solver.cpp:239] Iteration 3070 (2.84782 iter/s, 3.51146s/10 iters), loss = 8.93162
I0522 22:10:57.970185 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93162 (* 1 = 8.93162 loss)
I0522 22:10:58.618142 34682 sgd_solver.cpp:112] Iteration 3070, lr = 0.01
I0522 22:11:02.602407 34682 solver.cpp:239] Iteration 3080 (2.15889 iter/s, 4.63201s/10 iters), loss = 9.08166
I0522 22:11:02.602478 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08166 (* 1 = 9.08166 loss)
I0522 22:11:03.439203 34682 sgd_solver.cpp:112] Iteration 3080, lr = 0.01
I0522 22:11:05.995826 34682 solver.cpp:239] Iteration 3090 (2.94706 iter/s, 3.39321s/10 iters), loss = 9.35866
I0522 22:11:05.995870 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35866 (* 1 = 9.35866 loss)
I0522 22:11:06.059403 34682 sgd_solver.cpp:112] Iteration 3090, lr = 0.01
I0522 22:11:10.591095 34682 solver.cpp:239] Iteration 3100 (2.17627 iter/s, 4.59501s/10 iters), loss = 9.59587
I0522 22:11:10.591158 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59587 (* 1 = 9.59587 loss)
I0522 22:11:11.413975 34682 sgd_solver.cpp:112] Iteration 3100, lr = 0.01
I0522 22:11:16.009573 34682 solver.cpp:239] Iteration 3110 (1.84563 iter/s, 5.41819s/10 iters), loss = 9.32267
I0522 22:11:16.009622 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32267 (* 1 = 9.32267 loss)
I0522 22:11:16.081363 34682 sgd_solver.cpp:112] Iteration 3110, lr = 0.01
I0522 22:11:19.304433 34682 solver.cpp:239] Iteration 3120 (3.03521 iter/s, 3.29466s/10 iters), loss = 9.50797
I0522 22:11:19.304497 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.50797 (* 1 = 9.50797 loss)
I0522 22:11:20.105947 34682 sgd_solver.cpp:112] Iteration 3120, lr = 0.01
I0522 22:11:24.582442 34682 solver.cpp:239] Iteration 3130 (1.89476 iter/s, 5.27772s/10 iters), loss = 9.17479
I0522 22:11:24.582509 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17479 (* 1 = 9.17479 loss)
I0522 22:11:25.411185 34682 sgd_solver.cpp:112] Iteration 3130, lr = 0.01
I0522 22:11:30.023250 34682 solver.cpp:239] Iteration 3140 (1.83806 iter/s, 5.44051s/10 iters), loss = 9.49407
I0522 22:11:30.023597 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.49407 (* 1 = 9.49407 loss)
I0522 22:11:30.809674 34682 sgd_solver.cpp:112] Iteration 3140, lr = 0.01
I0522 22:11:35.801962 34682 solver.cpp:239] Iteration 3150 (1.73066 iter/s, 5.77815s/10 iters), loss = 9.33766
I0522 22:11:35.802059 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33766 (* 1 = 9.33766 loss)
I0522 22:11:36.026383 34682 sgd_solver.cpp:112] Iteration 3150, lr = 0.01
I0522 22:11:41.228596 34682 solver.cpp:239] Iteration 3160 (1.84288 iter/s, 5.4263s/10 iters), loss = 9.36546
I0522 22:11:41.228677 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36546 (* 1 = 9.36546 loss)
I0522 22:11:41.283566 34682 sgd_solver.cpp:112] Iteration 3160, lr = 0.01
I0522 22:11:46.127615 34682 solver.cpp:239] Iteration 3170 (2.04134 iter/s, 4.89874s/10 iters), loss = 9.3134
I0522 22:11:46.127662 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3134 (* 1 = 9.3134 loss)
I0522 22:11:46.185281 34682 sgd_solver.cpp:112] Iteration 3170, lr = 0.01
I0522 22:11:50.024386 34682 solver.cpp:239] Iteration 3180 (2.56636 iter/s, 3.89656s/10 iters), loss = 9.79453
I0522 22:11:50.024435 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.79453 (* 1 = 9.79453 loss)
I0522 22:11:50.643338 34682 sgd_solver.cpp:112] Iteration 3180, lr = 0.01
I0522 22:11:55.895625 34682 solver.cpp:239] Iteration 3190 (1.7033 iter/s, 5.87096s/10 iters), loss = 9.30273
I0522 22:11:55.895668 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30273 (* 1 = 9.30273 loss)
I0522 22:11:55.959571 34682 sgd_solver.cpp:112] Iteration 3190, lr = 0.01
I0522 22:12:00.867285 34682 solver.cpp:239] Iteration 3200 (2.0115 iter/s, 4.97141s/10 iters), loss = 9.55987
I0522 22:12:00.867499 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.55987 (* 1 = 9.55987 loss)
I0522 22:12:01.594759 34682 sgd_solver.cpp:112] Iteration 3200, lr = 0.01
I0522 22:12:05.868050 34682 solver.cpp:239] Iteration 3210 (1.99985 iter/s, 5.00037s/10 iters), loss = 9.52058
I0522 22:12:05.868103 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.52058 (* 1 = 9.52058 loss)
I0522 22:12:05.942679 34682 sgd_solver.cpp:112] Iteration 3210, lr = 0.01
I0522 22:12:09.317878 34682 solver.cpp:239] Iteration 3220 (2.89888 iter/s, 3.4496s/10 iters), loss = 9.87688
I0522 22:12:09.317935 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.87688 (* 1 = 9.87688 loss)
I0522 22:12:10.154177 34682 sgd_solver.cpp:112] Iteration 3220, lr = 0.01
I0522 22:12:15.164685 34682 solver.cpp:239] Iteration 3230 (1.71042 iter/s, 5.84651s/10 iters), loss = 9.61783
I0522 22:12:15.164734 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.61783 (* 1 = 9.61783 loss)
I0522 22:12:15.996532 34682 sgd_solver.cpp:112] Iteration 3230, lr = 0.01
I0522 22:12:23.950346 34682 solver.cpp:239] Iteration 3240 (1.13827 iter/s, 8.78526s/10 iters), loss = 9.4474
I0522 22:12:23.950410 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4474 (* 1 = 9.4474 loss)
I0522 22:12:24.727422 34682 sgd_solver.cpp:112] Iteration 3240, lr = 0.01
I0522 22:12:31.769260 34682 solver.cpp:239] Iteration 3250 (1.27901 iter/s, 7.81853s/10 iters), loss = 9.21316
I0522 22:12:31.769505 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21316 (* 1 = 9.21316 loss)
I0522 22:12:31.820261 34682 sgd_solver.cpp:112] Iteration 3250, lr = 0.01
I0522 22:12:34.386476 34682 solver.cpp:239] Iteration 3260 (3.82478 iter/s, 2.61453s/10 iters), loss = 8.91215
I0522 22:12:34.386543 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91215 (* 1 = 8.91215 loss)
I0522 22:12:35.060173 34682 sgd_solver.cpp:112] Iteration 3260, lr = 0.01
I0522 22:12:39.143851 34682 solver.cpp:239] Iteration 3270 (2.10213 iter/s, 4.75709s/10 iters), loss = 9.88864
I0522 22:12:39.143903 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.88864 (* 1 = 9.88864 loss)
I0522 22:12:39.213378 34682 sgd_solver.cpp:112] Iteration 3270, lr = 0.01
I0522 22:12:44.841272 34682 solver.cpp:239] Iteration 3280 (1.75527 iter/s, 5.69714s/10 iters), loss = 9.50616
I0522 22:12:44.841328 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.50616 (* 1 = 9.50616 loss)
I0522 22:12:44.909201 34682 sgd_solver.cpp:112] Iteration 3280, lr = 0.01
I0522 22:12:50.295126 34682 solver.cpp:239] Iteration 3290 (1.83452 iter/s, 5.451s/10 iters), loss = 9.51234
I0522 22:12:50.295168 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51234 (* 1 = 9.51234 loss)
I0522 22:12:50.361788 34682 sgd_solver.cpp:112] Iteration 3290, lr = 0.01
I0522 22:12:54.927781 34682 solver.cpp:239] Iteration 3300 (2.1587 iter/s, 4.63241s/10 iters), loss = 8.98835
I0522 22:12:54.927830 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98835 (* 1 = 8.98835 loss)
I0522 22:12:55.801173 34682 sgd_solver.cpp:112] Iteration 3300, lr = 0.01
I0522 22:12:59.076476 34682 solver.cpp:239] Iteration 3310 (2.41053 iter/s, 4.14846s/10 iters), loss = 9.21117
I0522 22:12:59.076534 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21117 (* 1 = 9.21117 loss)
I0522 22:12:59.902878 34682 sgd_solver.cpp:112] Iteration 3310, lr = 0.01
I0522 22:13:05.508651 34682 solver.cpp:239] Iteration 3320 (1.55476 iter/s, 6.43186s/10 iters), loss = 9.19963
I0522 22:13:05.508910 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19963 (* 1 = 9.19963 loss)
I0522 22:13:05.577325 34682 sgd_solver.cpp:112] Iteration 3320, lr = 0.01
I0522 22:13:11.042088 34682 solver.cpp:239] Iteration 3330 (1.80879 iter/s, 5.52857s/10 iters), loss = 9.07586
I0522 22:13:11.042153 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07586 (* 1 = 9.07586 loss)
I0522 22:13:11.125265 34682 sgd_solver.cpp:112] Iteration 3330, lr = 0.01
I0522 22:13:15.391139 34682 solver.cpp:239] Iteration 3340 (2.29949 iter/s, 4.34879s/10 iters), loss = 9.30786
I0522 22:13:15.391204 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30786 (* 1 = 9.30786 loss)
I0522 22:13:15.453140 34682 sgd_solver.cpp:112] Iteration 3340, lr = 0.01
I0522 22:13:20.494585 34682 solver.cpp:239] Iteration 3350 (1.95958 iter/s, 5.10314s/10 iters), loss = 8.40674
I0522 22:13:20.494666 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40674 (* 1 = 8.40674 loss)
I0522 22:13:21.342651 34682 sgd_solver.cpp:112] Iteration 3350, lr = 0.01
I0522 22:13:27.231209 34682 solver.cpp:239] Iteration 3360 (1.4845 iter/s, 6.73627s/10 iters), loss = 9.15213
I0522 22:13:27.231258 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15213 (* 1 = 9.15213 loss)
I0522 22:13:28.013751 34682 sgd_solver.cpp:112] Iteration 3360, lr = 0.01
I0522 22:13:34.128373 34682 solver.cpp:239] Iteration 3370 (1.44994 iter/s, 6.89683s/10 iters), loss = 9.13664
I0522 22:13:34.128443 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13664 (* 1 = 9.13664 loss)
I0522 22:13:34.200567 34682 sgd_solver.cpp:112] Iteration 3370, lr = 0.01
I0522 22:13:37.640316 34682 solver.cpp:239] Iteration 3380 (2.8476 iter/s, 3.51173s/10 iters), loss = 9.51171
I0522 22:13:37.640486 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51171 (* 1 = 9.51171 loss)
I0522 22:13:37.875152 34682 sgd_solver.cpp:112] Iteration 3380, lr = 0.01
I0522 22:13:42.444450 34682 solver.cpp:239] Iteration 3390 (2.0817 iter/s, 4.80377s/10 iters), loss = 9.22527
I0522 22:13:42.444505 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22527 (* 1 = 9.22527 loss)
I0522 22:13:43.303586 34682 sgd_solver.cpp:112] Iteration 3390, lr = 0.01
I0522 22:13:47.585683 34682 solver.cpp:239] Iteration 3400 (1.94516 iter/s, 5.14097s/10 iters), loss = 9.21157
I0522 22:13:47.585736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21157 (* 1 = 9.21157 loss)
I0522 22:13:47.642001 34682 sgd_solver.cpp:112] Iteration 3400, lr = 0.01
I0522 22:13:52.502761 34682 solver.cpp:239] Iteration 3410 (2.03384 iter/s, 4.91682s/10 iters), loss = 9.15327
I0522 22:13:52.502820 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15327 (* 1 = 9.15327 loss)
I0522 22:13:53.315186 34682 sgd_solver.cpp:112] Iteration 3410, lr = 0.01
I0522 22:13:57.437327 34682 solver.cpp:239] Iteration 3420 (2.02663 iter/s, 4.93431s/10 iters), loss = 9.12774
I0522 22:13:57.437391 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12774 (* 1 = 9.12774 loss)
I0522 22:13:57.519946 34682 sgd_solver.cpp:112] Iteration 3420, lr = 0.01
I0522 22:14:01.721940 34682 solver.cpp:239] Iteration 3430 (2.33406 iter/s, 4.28437s/10 iters), loss = 9.31299
I0522 22:14:01.721987 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31299 (* 1 = 9.31299 loss)
I0522 22:14:01.780189 34682 sgd_solver.cpp:112] Iteration 3430, lr = 0.01
I0522 22:14:05.763346 34682 solver.cpp:239] Iteration 3440 (2.47452 iter/s, 4.04118s/10 iters), loss = 9.05954
I0522 22:14:05.763401 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05954 (* 1 = 9.05954 loss)
I0522 22:14:06.401438 34682 sgd_solver.cpp:112] Iteration 3440, lr = 0.01
I0522 22:14:10.410692 34682 solver.cpp:239] Iteration 3450 (2.15188 iter/s, 4.64709s/10 iters), loss = 9.26481
I0522 22:14:10.410970 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.26481 (* 1 = 9.26481 loss)
I0522 22:14:10.475399 34682 sgd_solver.cpp:112] Iteration 3450, lr = 0.01
I0522 22:14:16.554142 34682 solver.cpp:239] Iteration 3460 (1.62789 iter/s, 6.14292s/10 iters), loss = 9.44958
I0522 22:14:16.554210 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44958 (* 1 = 9.44958 loss)
I0522 22:14:16.622898 34682 sgd_solver.cpp:112] Iteration 3460, lr = 0.01
I0522 22:14:21.593094 34682 solver.cpp:239] Iteration 3470 (1.98464 iter/s, 5.03869s/10 iters), loss = 8.77107
I0522 22:14:21.593147 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77107 (* 1 = 8.77107 loss)
I0522 22:14:22.246937 34682 sgd_solver.cpp:112] Iteration 3470, lr = 0.01
I0522 22:14:27.818682 34682 solver.cpp:239] Iteration 3480 (1.60636 iter/s, 6.22526s/10 iters), loss = 9.4733
I0522 22:14:27.818768 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4733 (* 1 = 9.4733 loss)
I0522 22:14:27.897085 34682 sgd_solver.cpp:112] Iteration 3480, lr = 0.01
I0522 22:14:32.076823 34682 solver.cpp:239] Iteration 3490 (2.34858 iter/s, 4.25788s/10 iters), loss = 9.37235
I0522 22:14:32.076890 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.37235 (* 1 = 9.37235 loss)
I0522 22:14:32.154530 34682 sgd_solver.cpp:112] Iteration 3490, lr = 0.01
I0522 22:14:38.172979 34682 solver.cpp:239] Iteration 3500 (1.64046 iter/s, 6.09585s/10 iters), loss = 9.58222
I0522 22:14:38.173032 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.58222 (* 1 = 9.58222 loss)
I0522 22:14:38.230190 34682 sgd_solver.cpp:112] Iteration 3500, lr = 0.01
I0522 22:14:42.722106 34682 solver.cpp:239] Iteration 3510 (2.19835 iter/s, 4.54888s/10 iters), loss = 9.63971
I0522 22:14:42.722306 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.63971 (* 1 = 9.63971 loss)
I0522 22:14:43.163908 34682 sgd_solver.cpp:112] Iteration 3510, lr = 0.01
I0522 22:14:48.804203 34682 solver.cpp:239] Iteration 3520 (1.64429 iter/s, 6.08166s/10 iters), loss = 9.33332
I0522 22:14:48.804285 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33332 (* 1 = 9.33332 loss)
I0522 22:14:48.856791 34682 sgd_solver.cpp:112] Iteration 3520, lr = 0.01
I0522 22:14:52.683364 34682 solver.cpp:239] Iteration 3530 (2.57804 iter/s, 3.87892s/10 iters), loss = 9.21766
I0522 22:14:52.683418 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21766 (* 1 = 9.21766 loss)
I0522 22:14:52.745863 34682 sgd_solver.cpp:112] Iteration 3530, lr = 0.01
I0522 22:14:56.936280 34682 solver.cpp:239] Iteration 3540 (2.35145 iter/s, 4.25269s/10 iters), loss = 9.32302
I0522 22:14:56.936331 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32302 (* 1 = 9.32302 loss)
I0522 22:14:57.773471 34682 sgd_solver.cpp:112] Iteration 3540, lr = 0.01
I0522 22:15:03.649405 34682 solver.cpp:239] Iteration 3550 (1.4897 iter/s, 6.71278s/10 iters), loss = 8.78527
I0522 22:15:03.649482 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78527 (* 1 = 8.78527 loss)
I0522 22:15:03.703441 34682 sgd_solver.cpp:112] Iteration 3550, lr = 0.01
I0522 22:15:09.198335 34682 solver.cpp:239] Iteration 3560 (1.80225 iter/s, 5.54863s/10 iters), loss = 9.60958
I0522 22:15:09.198388 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60958 (* 1 = 9.60958 loss)
I0522 22:15:09.258489 34682 sgd_solver.cpp:112] Iteration 3560, lr = 0.01
I0522 22:15:14.601418 34682 solver.cpp:239] Iteration 3570 (1.85089 iter/s, 5.40279s/10 iters), loss = 9.68717
I0522 22:15:14.601610 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.68717 (* 1 = 9.68717 loss)
I0522 22:15:14.665386 34682 sgd_solver.cpp:112] Iteration 3570, lr = 0.01
I0522 22:15:20.051527 34682 solver.cpp:239] Iteration 3580 (1.8364 iter/s, 5.44544s/10 iters), loss = 9.00921
I0522 22:15:20.051589 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00921 (* 1 = 9.00921 loss)
I0522 22:15:20.884708 34682 sgd_solver.cpp:112] Iteration 3580, lr = 0.01
I0522 22:15:25.336683 34682 solver.cpp:239] Iteration 3590 (1.89219 iter/s, 5.28488s/10 iters), loss = 9.11085
I0522 22:15:25.336736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11085 (* 1 = 9.11085 loss)
I0522 22:15:26.107270 34682 sgd_solver.cpp:112] Iteration 3590, lr = 0.01
I0522 22:15:31.159885 34682 solver.cpp:239] Iteration 3600 (1.71736 iter/s, 5.8229s/10 iters), loss = 9.36774
I0522 22:15:31.159958 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36774 (* 1 = 9.36774 loss)
I0522 22:15:31.229122 34682 sgd_solver.cpp:112] Iteration 3600, lr = 0.01
I0522 22:15:35.491621 34682 solver.cpp:239] Iteration 3610 (2.30868 iter/s, 4.33149s/10 iters), loss = 8.99889
I0522 22:15:35.491681 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99889 (* 1 = 8.99889 loss)
I0522 22:15:35.560183 34682 sgd_solver.cpp:112] Iteration 3610, lr = 0.01
I0522 22:15:39.584113 34682 solver.cpp:239] Iteration 3620 (2.44364 iter/s, 4.09226s/10 iters), loss = 8.75954
I0522 22:15:39.584174 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75954 (* 1 = 8.75954 loss)
I0522 22:15:39.652881 34682 sgd_solver.cpp:112] Iteration 3620, lr = 0.01
I0522 22:15:42.882197 34682 solver.cpp:239] Iteration 3630 (3.03225 iter/s, 3.29788s/10 iters), loss = 8.84389
I0522 22:15:42.882256 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84389 (* 1 = 8.84389 loss)
I0522 22:15:42.940678 34682 sgd_solver.cpp:112] Iteration 3630, lr = 0.01
I0522 22:15:47.464596 34682 solver.cpp:239] Iteration 3640 (2.18238 iter/s, 4.58216s/10 iters), loss = 8.56878
I0522 22:15:47.464712 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56878 (* 1 = 8.56878 loss)
I0522 22:15:47.545645 34682 sgd_solver.cpp:112] Iteration 3640, lr = 0.01
I0522 22:15:52.340885 34682 solver.cpp:239] Iteration 3650 (2.05087 iter/s, 4.87597s/10 iters), loss = 9.09932
I0522 22:15:52.340934 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09932 (* 1 = 9.09932 loss)
I0522 22:15:52.408375 34682 sgd_solver.cpp:112] Iteration 3650, lr = 0.01
I0522 22:15:55.859263 34682 solver.cpp:239] Iteration 3660 (2.84239 iter/s, 3.51817s/10 iters), loss = 8.69294
I0522 22:15:55.859364 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69294 (* 1 = 8.69294 loss)
I0522 22:15:55.937738 34682 sgd_solver.cpp:112] Iteration 3660, lr = 0.01
I0522 22:16:00.375594 34682 solver.cpp:239] Iteration 3670 (2.21432 iter/s, 4.51606s/10 iters), loss = 9.35602
I0522 22:16:00.375638 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35602 (* 1 = 9.35602 loss)
I0522 22:16:00.448393 34682 sgd_solver.cpp:112] Iteration 3670, lr = 0.01
I0522 22:16:06.280268 34682 solver.cpp:239] Iteration 3680 (1.69366 iter/s, 5.90439s/10 iters), loss = 9.72265
I0522 22:16:06.280335 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.72265 (* 1 = 9.72265 loss)
I0522 22:16:07.144886 34682 sgd_solver.cpp:112] Iteration 3680, lr = 0.01
I0522 22:16:10.944350 34682 solver.cpp:239] Iteration 3690 (2.14417 iter/s, 4.66382s/10 iters), loss = 9.46907
I0522 22:16:10.944406 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.46907 (* 1 = 9.46907 loss)
I0522 22:16:11.283812 34682 sgd_solver.cpp:112] Iteration 3690, lr = 0.01
I0522 22:16:15.489383 34682 solver.cpp:239] Iteration 3700 (2.20033 iter/s, 4.54478s/10 iters), loss = 8.83123
I0522 22:16:15.489431 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83123 (* 1 = 8.83123 loss)
I0522 22:16:15.546434 34682 sgd_solver.cpp:112] Iteration 3700, lr = 0.01
I0522 22:16:19.415212 34682 solver.cpp:239] Iteration 3710 (2.54738 iter/s, 3.9256s/10 iters), loss = 8.6988
I0522 22:16:19.415431 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6988 (* 1 = 8.6988 loss)
I0522 22:16:19.487885 34682 sgd_solver.cpp:112] Iteration 3710, lr = 0.01
I0522 22:16:23.569782 34682 solver.cpp:239] Iteration 3720 (2.40721 iter/s, 4.15419s/10 iters), loss = 9.8002
I0522 22:16:23.569839 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.8002 (* 1 = 9.8002 loss)
I0522 22:16:24.389883 34682 sgd_solver.cpp:112] Iteration 3720, lr = 0.01
I0522 22:16:28.498113 34682 solver.cpp:239] Iteration 3730 (2.02919 iter/s, 4.92807s/10 iters), loss = 9.42841
I0522 22:16:28.498155 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42841 (* 1 = 9.42841 loss)
I0522 22:16:28.575314 34682 sgd_solver.cpp:112] Iteration 3730, lr = 0.01
I0522 22:16:32.592977 34682 solver.cpp:239] Iteration 3740 (2.44221 iter/s, 4.09465s/10 iters), loss = 9.07829
I0522 22:16:32.593019 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07829 (* 1 = 9.07829 loss)
I0522 22:16:32.648401 34682 sgd_solver.cpp:112] Iteration 3740, lr = 0.01
I0522 22:16:36.619063 34682 solver.cpp:239] Iteration 3750 (2.48393 iter/s, 4.02587s/10 iters), loss = 9.40168
I0522 22:16:36.619117 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40168 (* 1 = 9.40168 loss)
I0522 22:16:36.681334 34682 sgd_solver.cpp:112] Iteration 3750, lr = 0.01
I0522 22:16:41.125898 34682 solver.cpp:239] Iteration 3760 (2.21897 iter/s, 4.5066s/10 iters), loss = 9.51614
I0522 22:16:41.125957 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51614 (* 1 = 9.51614 loss)
I0522 22:16:41.951702 34682 sgd_solver.cpp:112] Iteration 3760, lr = 0.01
I0522 22:16:45.752485 34682 solver.cpp:239] Iteration 3770 (2.16154 iter/s, 4.62634s/10 iters), loss = 9.02514
I0522 22:16:45.752540 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02514 (* 1 = 9.02514 loss)
I0522 22:16:45.820425 34682 sgd_solver.cpp:112] Iteration 3770, lr = 0.01
I0522 22:16:50.725734 34682 solver.cpp:239] Iteration 3780 (2.01086 iter/s, 4.97299s/10 iters), loss = 8.55637
I0522 22:16:50.725867 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55637 (* 1 = 8.55637 loss)
I0522 22:16:51.479998 34682 sgd_solver.cpp:112] Iteration 3780, lr = 0.01
I0522 22:16:55.990805 34682 solver.cpp:239] Iteration 3790 (1.89943 iter/s, 5.26474s/10 iters), loss = 9.33889
I0522 22:16:55.990852 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33889 (* 1 = 9.33889 loss)
I0522 22:16:56.874027 34682 sgd_solver.cpp:112] Iteration 3790, lr = 0.01
I0522 22:17:02.576203 34682 solver.cpp:239] Iteration 3800 (1.51859 iter/s, 6.58508s/10 iters), loss = 9.44894
I0522 22:17:02.576267 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44894 (* 1 = 9.44894 loss)
I0522 22:17:02.640239 34682 sgd_solver.cpp:112] Iteration 3800, lr = 0.01
I0522 22:17:06.870033 34682 solver.cpp:239] Iteration 3810 (2.32906 iter/s, 4.29358s/10 iters), loss = 8.7571
I0522 22:17:06.870141 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7571 (* 1 = 8.7571 loss)
I0522 22:17:06.935655 34682 sgd_solver.cpp:112] Iteration 3810, lr = 0.01
I0522 22:17:10.647475 34682 solver.cpp:239] Iteration 3820 (2.64747 iter/s, 3.77719s/10 iters), loss = 9.20697
I0522 22:17:10.647516 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20697 (* 1 = 9.20697 loss)
I0522 22:17:10.707293 34682 sgd_solver.cpp:112] Iteration 3820, lr = 0.01
I0522 22:17:14.775452 34682 solver.cpp:239] Iteration 3830 (2.42263 iter/s, 4.12775s/10 iters), loss = 9.24579
I0522 22:17:14.775518 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24579 (* 1 = 9.24579 loss)
I0522 22:17:14.842730 34682 sgd_solver.cpp:112] Iteration 3830, lr = 0.01
I0522 22:17:20.108256 34682 solver.cpp:239] Iteration 3840 (1.87529 iter/s, 5.33251s/10 iters), loss = 9.04694
I0522 22:17:20.108345 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04694 (* 1 = 9.04694 loss)
I0522 22:17:20.753185 34682 sgd_solver.cpp:112] Iteration 3840, lr = 0.01
I0522 22:17:24.649287 34682 solver.cpp:239] Iteration 3850 (2.20227 iter/s, 4.54076s/10 iters), loss = 8.58961
I0522 22:17:24.649336 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58961 (* 1 = 8.58961 loss)
I0522 22:17:24.709177 34682 sgd_solver.cpp:112] Iteration 3850, lr = 0.01
I0522 22:17:30.467275 34682 solver.cpp:239] Iteration 3860 (1.71889 iter/s, 5.8177s/10 iters), loss = 9.17414
I0522 22:17:30.467331 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17414 (* 1 = 9.17414 loss)
I0522 22:17:31.275887 34682 sgd_solver.cpp:112] Iteration 3860, lr = 0.01
I0522 22:17:35.522776 34682 solver.cpp:239] Iteration 3870 (1.97815 iter/s, 5.05523s/10 iters), loss = 8.89033
I0522 22:17:35.522832 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89033 (* 1 = 8.89033 loss)
I0522 22:17:35.587385 34682 sgd_solver.cpp:112] Iteration 3870, lr = 0.01
I0522 22:17:41.568114 34682 solver.cpp:239] Iteration 3880 (1.65425 iter/s, 6.04503s/10 iters), loss = 9.36221
I0522 22:17:41.568163 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36221 (* 1 = 9.36221 loss)
I0522 22:17:41.632526 34682 sgd_solver.cpp:112] Iteration 3880, lr = 0.01
I0522 22:17:47.178573 34682 solver.cpp:239] Iteration 3890 (1.78248 iter/s, 5.61018s/10 iters), loss = 9.82866
I0522 22:17:47.178648 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.82866 (* 1 = 9.82866 loss)
I0522 22:17:47.256207 34682 sgd_solver.cpp:112] Iteration 3890, lr = 0.01
I0522 22:17:52.021140 34682 solver.cpp:239] Iteration 3900 (2.06514 iter/s, 4.84229s/10 iters), loss = 8.83168
I0522 22:17:52.021267 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83168 (* 1 = 8.83168 loss)
I0522 22:17:52.096024 34682 sgd_solver.cpp:112] Iteration 3900, lr = 0.01
I0522 22:17:55.530784 34682 solver.cpp:239] Iteration 3910 (2.84951 iter/s, 3.50937s/10 iters), loss = 9.02286
I0522 22:17:55.530827 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02286 (* 1 = 9.02286 loss)
I0522 22:17:55.604460 34682 sgd_solver.cpp:112] Iteration 3910, lr = 0.01
I0522 22:18:00.947476 34682 solver.cpp:239] Iteration 3920 (1.84624 iter/s, 5.41642s/10 iters), loss = 9.45965
I0522 22:18:00.947541 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.45965 (* 1 = 9.45965 loss)
I0522 22:18:01.010946 34682 sgd_solver.cpp:112] Iteration 3920, lr = 0.01
I0522 22:18:05.000465 34682 solver.cpp:239] Iteration 3930 (2.46746 iter/s, 4.05275s/10 iters), loss = 9.27979
I0522 22:18:05.000522 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27979 (* 1 = 9.27979 loss)
I0522 22:18:05.066272 34682 sgd_solver.cpp:112] Iteration 3930, lr = 0.01
I0522 22:18:08.545261 34682 solver.cpp:239] Iteration 3940 (2.8212 iter/s, 3.5446s/10 iters), loss = 9.43295
I0522 22:18:08.545300 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43295 (* 1 = 9.43295 loss)
I0522 22:18:08.622604 34682 sgd_solver.cpp:112] Iteration 3940, lr = 0.01
I0522 22:18:15.697419 34682 solver.cpp:239] Iteration 3950 (1.39825 iter/s, 7.15182s/10 iters), loss = 8.87208
I0522 22:18:15.697489 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87208 (* 1 = 8.87208 loss)
I0522 22:18:16.392134 34682 sgd_solver.cpp:112] Iteration 3950, lr = 0.01
I0522 22:18:20.738795 34682 solver.cpp:239] Iteration 3960 (1.9837 iter/s, 5.04109s/10 iters), loss = 9.15171
I0522 22:18:20.738847 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15171 (* 1 = 9.15171 loss)
I0522 22:18:20.816347 34682 sgd_solver.cpp:112] Iteration 3960, lr = 0.01
I0522 22:18:24.841269 34682 solver.cpp:239] Iteration 3970 (2.43769 iter/s, 4.10224s/10 iters), loss = 9.29751
I0522 22:18:24.841500 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29751 (* 1 = 9.29751 loss)
I0522 22:18:25.685400 34682 sgd_solver.cpp:112] Iteration 3970, lr = 0.01
I0522 22:18:29.125902 34682 solver.cpp:239] Iteration 3980 (2.33415 iter/s, 4.28422s/10 iters), loss = 9.68867
I0522 22:18:29.125995 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.68867 (* 1 = 9.68867 loss)
I0522 22:18:29.946982 34682 sgd_solver.cpp:112] Iteration 3980, lr = 0.01
I0522 22:18:35.358104 34682 solver.cpp:239] Iteration 3990 (1.6055 iter/s, 6.22861s/10 iters), loss = 8.8073
I0522 22:18:35.358173 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8073 (* 1 = 8.8073 loss)
I0522 22:18:36.005918 34682 sgd_solver.cpp:112] Iteration 3990, lr = 0.01
I0522 22:18:40.416330 34682 solver.cpp:239] Iteration 4000 (1.97709 iter/s, 5.05795s/10 iters), loss = 8.84956
I0522 22:18:40.416375 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84956 (* 1 = 8.84956 loss)
I0522 22:18:41.247478 34682 sgd_solver.cpp:112] Iteration 4000, lr = 0.01
I0522 22:18:46.098495 34682 solver.cpp:239] Iteration 4010 (1.75998 iter/s, 5.68188s/10 iters), loss = 8.61738
I0522 22:18:46.098556 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61738 (* 1 = 8.61738 loss)
I0522 22:18:46.908958 34682 sgd_solver.cpp:112] Iteration 4010, lr = 0.01
I0522 22:18:49.879045 34682 solver.cpp:239] Iteration 4020 (2.64527 iter/s, 3.78033s/10 iters), loss = 8.79052
I0522 22:18:49.879094 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79052 (* 1 = 8.79052 loss)
I0522 22:18:50.190742 34682 sgd_solver.cpp:112] Iteration 4020, lr = 0.01
I0522 22:18:53.528599 34682 solver.cpp:239] Iteration 4030 (2.74021 iter/s, 3.64935s/10 iters), loss = 8.43047
I0522 22:18:53.528652 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43047 (* 1 = 8.43047 loss)
I0522 22:18:53.580179 34682 sgd_solver.cpp:112] Iteration 4030, lr = 0.01
I0522 22:18:58.412596 34682 solver.cpp:239] Iteration 4040 (2.04761 iter/s, 4.88374s/10 iters), loss = 8.97857
I0522 22:18:58.412801 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97857 (* 1 = 8.97857 loss)
I0522 22:18:58.471283 34682 sgd_solver.cpp:112] Iteration 4040, lr = 0.01
I0522 22:19:03.458083 34682 solver.cpp:239] Iteration 4050 (1.98213 iter/s, 5.04509s/10 iters), loss = 9.04821
I0522 22:19:03.458129 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04821 (* 1 = 9.04821 loss)
I0522 22:19:03.531467 34682 sgd_solver.cpp:112] Iteration 4050, lr = 0.01
I0522 22:19:09.231678 34682 solver.cpp:239] Iteration 4060 (1.73211 iter/s, 5.77329s/10 iters), loss = 9.62804
I0522 22:19:09.231771 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.62804 (* 1 = 9.62804 loss)
I0522 22:19:09.300160 34682 sgd_solver.cpp:112] Iteration 4060, lr = 0.01
I0522 22:19:12.582201 34682 solver.cpp:239] Iteration 4070 (2.98481 iter/s, 3.3503s/10 iters), loss = 9.34175
I0522 22:19:12.582248 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34175 (* 1 = 9.34175 loss)
I0522 22:19:12.658704 34682 sgd_solver.cpp:112] Iteration 4070, lr = 0.01
I0522 22:19:18.973197 34682 solver.cpp:239] Iteration 4080 (1.56478 iter/s, 6.39068s/10 iters), loss = 9.37595
I0522 22:19:18.973248 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.37595 (* 1 = 9.37595 loss)
I0522 22:19:19.042026 34682 sgd_solver.cpp:112] Iteration 4080, lr = 0.01
I0522 22:19:24.380568 34682 solver.cpp:239] Iteration 4090 (1.84943 iter/s, 5.40709s/10 iters), loss = 9.29341
I0522 22:19:24.380650 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29341 (* 1 = 9.29341 loss)
I0522 22:19:24.443898 34682 sgd_solver.cpp:112] Iteration 4090, lr = 0.01
I0522 22:19:29.984124 34682 solver.cpp:239] Iteration 4100 (1.78468 iter/s, 5.60325s/10 iters), loss = 8.63838
I0522 22:19:29.984338 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63838 (* 1 = 8.63838 loss)
I0522 22:19:30.805460 34682 sgd_solver.cpp:112] Iteration 4100, lr = 0.01
I0522 22:19:35.322281 34682 solver.cpp:239] Iteration 4110 (1.87346 iter/s, 5.33772s/10 iters), loss = 8.42039
I0522 22:19:35.322338 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42039 (* 1 = 8.42039 loss)
I0522 22:19:35.391569 34682 sgd_solver.cpp:112] Iteration 4110, lr = 0.01
I0522 22:19:38.240679 34682 solver.cpp:239] Iteration 4120 (3.42674 iter/s, 2.91822s/10 iters), loss = 9.00268
I0522 22:19:38.240718 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00268 (* 1 = 9.00268 loss)
I0522 22:19:38.328584 34682 sgd_solver.cpp:112] Iteration 4120, lr = 0.01
I0522 22:19:43.189966 34682 solver.cpp:239] Iteration 4130 (2.0206 iter/s, 4.94902s/10 iters), loss = 9.02986
I0522 22:19:43.190045 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02986 (* 1 = 9.02986 loss)
I0522 22:19:44.021939 34682 sgd_solver.cpp:112] Iteration 4130, lr = 0.01
I0522 22:19:48.342504 34682 solver.cpp:239] Iteration 4140 (1.9409 iter/s, 5.15224s/10 iters), loss = 8.93063
I0522 22:19:48.342571 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93063 (* 1 = 8.93063 loss)
I0522 22:19:48.404840 34682 sgd_solver.cpp:112] Iteration 4140, lr = 0.01
I0522 22:19:52.843380 34682 solver.cpp:239] Iteration 4150 (2.22191 iter/s, 4.50062s/10 iters), loss = 8.73341
I0522 22:19:52.843441 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73341 (* 1 = 8.73341 loss)
I0522 22:19:52.913067 34682 sgd_solver.cpp:112] Iteration 4150, lr = 0.01
I0522 22:19:58.356125 34682 solver.cpp:239] Iteration 4160 (1.81407 iter/s, 5.51245s/10 iters), loss = 8.82837
I0522 22:19:58.356184 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82837 (* 1 = 8.82837 loss)
I0522 22:19:59.176997 34682 sgd_solver.cpp:112] Iteration 4160, lr = 0.01
I0522 22:20:03.955756 34682 solver.cpp:239] Iteration 4170 (1.78592 iter/s, 5.59935s/10 iters), loss = 8.66054
I0522 22:20:03.955981 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66054 (* 1 = 8.66054 loss)
I0522 22:20:04.774868 34682 sgd_solver.cpp:112] Iteration 4170, lr = 0.01
I0522 22:20:10.542145 34682 solver.cpp:239] Iteration 4180 (1.51839 iter/s, 6.58591s/10 iters), loss = 8.73304
I0522 22:20:10.542194 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73304 (* 1 = 8.73304 loss)
I0522 22:20:10.606078 34682 sgd_solver.cpp:112] Iteration 4180, lr = 0.01
I0522 22:20:15.510684 34682 solver.cpp:239] Iteration 4190 (2.01408 iter/s, 4.96506s/10 iters), loss = 9.33445
I0522 22:20:15.510766 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33445 (* 1 = 9.33445 loss)
I0522 22:20:15.600738 34682 sgd_solver.cpp:112] Iteration 4190, lr = 0.01
I0522 22:20:21.092211 34682 solver.cpp:239] Iteration 4200 (1.79173 iter/s, 5.5812s/10 iters), loss = 9.28179
I0522 22:20:21.092279 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28179 (* 1 = 9.28179 loss)
I0522 22:20:21.883086 34682 sgd_solver.cpp:112] Iteration 4200, lr = 0.01
I0522 22:20:29.520162 34682 solver.cpp:239] Iteration 4210 (1.18659 iter/s, 8.42754s/10 iters), loss = 8.52076
I0522 22:20:29.520220 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52076 (* 1 = 8.52076 loss)
I0522 22:20:30.340327 34682 sgd_solver.cpp:112] Iteration 4210, lr = 0.01
I0522 22:20:34.060544 34682 solver.cpp:239] Iteration 4220 (2.20258 iter/s, 4.54013s/10 iters), loss = 8.94998
I0522 22:20:34.060950 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94998 (* 1 = 8.94998 loss)
I0522 22:20:34.132282 34682 sgd_solver.cpp:112] Iteration 4220, lr = 0.01
I0522 22:20:37.526230 34682 solver.cpp:239] Iteration 4230 (2.88585 iter/s, 3.46518s/10 iters), loss = 9.53271
I0522 22:20:37.526296 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.53271 (* 1 = 9.53271 loss)
I0522 22:20:38.170794 34682 sgd_solver.cpp:112] Iteration 4230, lr = 0.01
I0522 22:20:41.567056 34682 solver.cpp:239] Iteration 4240 (2.47488 iter/s, 4.0406s/10 iters), loss = 8.89079
I0522 22:20:41.567112 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89079 (* 1 = 8.89079 loss)
I0522 22:20:42.350950 34682 sgd_solver.cpp:112] Iteration 4240, lr = 0.01
I0522 22:20:47.439313 34682 solver.cpp:239] Iteration 4250 (1.70301 iter/s, 5.87197s/10 iters), loss = 9.49019
I0522 22:20:47.439359 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.49019 (* 1 = 9.49019 loss)
I0522 22:20:47.513820 34682 sgd_solver.cpp:112] Iteration 4250, lr = 0.01
I0522 22:20:51.573678 34682 solver.cpp:239] Iteration 4260 (2.41889 iter/s, 4.13412s/10 iters), loss = 8.6986
I0522 22:20:51.573757 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6986 (* 1 = 8.6986 loss)
I0522 22:20:52.419478 34682 sgd_solver.cpp:112] Iteration 4260, lr = 0.01
I0522 22:20:55.676764 34682 solver.cpp:239] Iteration 4270 (2.43735 iter/s, 4.10282s/10 iters), loss = 8.52817
I0522 22:20:55.676846 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52817 (* 1 = 8.52817 loss)
I0522 22:20:56.460750 34682 sgd_solver.cpp:112] Iteration 4270, lr = 0.01
I0522 22:21:02.263407 34682 solver.cpp:239] Iteration 4280 (1.5183 iter/s, 6.58629s/10 iters), loss = 9.72702
I0522 22:21:02.263468 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.72702 (* 1 = 9.72702 loss)
I0522 22:21:02.977105 34682 sgd_solver.cpp:112] Iteration 4280, lr = 0.01
I0522 22:21:04.896250 34682 solver.cpp:239] Iteration 4290 (3.79844 iter/s, 2.63266s/10 iters), loss = 9.20612
I0522 22:21:04.896471 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20612 (* 1 = 9.20612 loss)
I0522 22:21:05.498886 34682 sgd_solver.cpp:112] Iteration 4290, lr = 0.01
I0522 22:21:10.193277 34682 solver.cpp:239] Iteration 4300 (1.88801 iter/s, 5.29659s/10 iters), loss = 9.06816
I0522 22:21:10.193338 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06816 (* 1 = 9.06816 loss)
I0522 22:21:10.979898 34682 sgd_solver.cpp:112] Iteration 4300, lr = 0.01
I0522 22:21:15.936247 34682 solver.cpp:239] Iteration 4310 (1.74135 iter/s, 5.74267s/10 iters), loss = 8.69962
I0522 22:21:15.936319 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69962 (* 1 = 8.69962 loss)
I0522 22:21:16.751015 34682 sgd_solver.cpp:112] Iteration 4310, lr = 0.01
I0522 22:21:20.260403 34682 solver.cpp:239] Iteration 4320 (2.31273 iter/s, 4.3239s/10 iters), loss = 8.82668
I0522 22:21:20.260463 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82668 (* 1 = 8.82668 loss)
I0522 22:21:20.321431 34682 sgd_solver.cpp:112] Iteration 4320, lr = 0.01
I0522 22:21:22.942917 34682 solver.cpp:239] Iteration 4330 (3.72809 iter/s, 2.68234s/10 iters), loss = 8.93872
I0522 22:21:22.942965 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93872 (* 1 = 8.93872 loss)
I0522 22:21:23.002990 34682 sgd_solver.cpp:112] Iteration 4330, lr = 0.01
I0522 22:21:26.338922 34682 solver.cpp:239] Iteration 4340 (2.94481 iter/s, 3.3958s/10 iters), loss = 8.88547
I0522 22:21:26.338979 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88547 (* 1 = 8.88547 loss)
I0522 22:21:26.396082 34682 sgd_solver.cpp:112] Iteration 4340, lr = 0.01
I0522 22:21:30.680474 34682 solver.cpp:239] Iteration 4350 (2.30346 iter/s, 4.34129s/10 iters), loss = 8.9674
I0522 22:21:30.680552 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9674 (* 1 = 8.9674 loss)
I0522 22:21:30.754797 34682 sgd_solver.cpp:112] Iteration 4350, lr = 0.01
I0522 22:21:36.508034 34682 solver.cpp:239] Iteration 4360 (1.71608 iter/s, 5.82724s/10 iters), loss = 9.21599
I0522 22:21:36.508322 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21599 (* 1 = 9.21599 loss)
I0522 22:21:37.309936 34682 sgd_solver.cpp:112] Iteration 4360, lr = 0.01
I0522 22:21:43.054244 34682 solver.cpp:239] Iteration 4370 (1.52772 iter/s, 6.54569s/10 iters), loss = 9.08483
I0522 22:21:43.054316 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08483 (* 1 = 9.08483 loss)
I0522 22:21:43.872706 34682 sgd_solver.cpp:112] Iteration 4370, lr = 0.01
I0522 22:21:49.740345 34682 solver.cpp:239] Iteration 4380 (1.49572 iter/s, 6.68576s/10 iters), loss = 8.71821
I0522 22:21:49.740406 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71821 (* 1 = 8.71821 loss)
I0522 22:21:50.586753 34682 sgd_solver.cpp:112] Iteration 4380, lr = 0.01
I0522 22:21:55.343057 34682 solver.cpp:239] Iteration 4390 (1.78495 iter/s, 5.6024s/10 iters), loss = 8.73347
I0522 22:21:55.343173 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73347 (* 1 = 8.73347 loss)
I0522 22:21:56.014729 34682 sgd_solver.cpp:112] Iteration 4390, lr = 0.01
I0522 22:22:00.740494 34682 solver.cpp:239] Iteration 4400 (1.85284 iter/s, 5.39712s/10 iters), loss = 8.81951
I0522 22:22:00.740548 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81951 (* 1 = 8.81951 loss)
I0522 22:22:00.818109 34682 sgd_solver.cpp:112] Iteration 4400, lr = 0.01
I0522 22:22:05.509544 34682 solver.cpp:239] Iteration 4410 (2.09697 iter/s, 4.76878s/10 iters), loss = 9.19073
I0522 22:22:05.509624 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19073 (* 1 = 9.19073 loss)
I0522 22:22:06.339449 34682 sgd_solver.cpp:112] Iteration 4410, lr = 0.01
I0522 22:22:11.818657 34682 solver.cpp:239] Iteration 4420 (1.5851 iter/s, 6.30874s/10 iters), loss = 8.69817
I0522 22:22:11.819034 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69817 (* 1 = 8.69817 loss)
I0522 22:22:11.883486 34682 sgd_solver.cpp:112] Iteration 4420, lr = 0.01
I0522 22:22:15.342758 34682 solver.cpp:239] Iteration 4430 (2.83797 iter/s, 3.52364s/10 iters), loss = 9.23947
I0522 22:22:15.342808 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23947 (* 1 = 9.23947 loss)
I0522 22:22:15.413910 34682 sgd_solver.cpp:112] Iteration 4430, lr = 0.01
I0522 22:22:20.982218 34682 solver.cpp:239] Iteration 4440 (1.77331 iter/s, 5.63918s/10 iters), loss = 8.54121
I0522 22:22:20.982271 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54121 (* 1 = 8.54121 loss)
I0522 22:22:21.045858 34682 sgd_solver.cpp:112] Iteration 4440, lr = 0.01
I0522 22:22:25.202023 34682 solver.cpp:239] Iteration 4450 (2.3699 iter/s, 4.21958s/10 iters), loss = 8.51572
I0522 22:22:25.202069 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51572 (* 1 = 8.51572 loss)
I0522 22:22:25.270568 34682 sgd_solver.cpp:112] Iteration 4450, lr = 0.01
I0522 22:22:30.670642 34682 solver.cpp:239] Iteration 4460 (1.82871 iter/s, 5.46835s/10 iters), loss = 8.748
I0522 22:22:30.670688 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.748 (* 1 = 8.748 loss)
I0522 22:22:30.745220 34682 sgd_solver.cpp:112] Iteration 4460, lr = 0.01
I0522 22:22:37.096031 34682 solver.cpp:239] Iteration 4470 (1.5564 iter/s, 6.42507s/10 iters), loss = 9.11606
I0522 22:22:37.096089 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11606 (* 1 = 9.11606 loss)
I0522 22:22:37.153075 34682 sgd_solver.cpp:112] Iteration 4470, lr = 0.01
I0522 22:22:42.163494 34682 solver.cpp:239] Iteration 4480 (1.97348 iter/s, 5.06719s/10 iters), loss = 8.56331
I0522 22:22:42.163694 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56331 (* 1 = 8.56331 loss)
I0522 22:22:42.931196 34682 sgd_solver.cpp:112] Iteration 4480, lr = 0.01
I0522 22:22:46.608855 34682 solver.cpp:239] Iteration 4490 (2.24972 iter/s, 4.44499s/10 iters), loss = 8.28045
I0522 22:22:46.608897 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28045 (* 1 = 8.28045 loss)
I0522 22:22:46.677806 34682 sgd_solver.cpp:112] Iteration 4490, lr = 0.01
I0522 22:22:51.833503 34682 solver.cpp:239] Iteration 4500 (1.9141 iter/s, 5.22439s/10 iters), loss = 9.26618
I0522 22:22:51.833551 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.26618 (* 1 = 9.26618 loss)
I0522 22:22:51.899626 34682 sgd_solver.cpp:112] Iteration 4500, lr = 0.01
I0522 22:22:56.649029 34682 solver.cpp:239] Iteration 4510 (2.07672 iter/s, 4.81528s/10 iters), loss = 9.38619
I0522 22:22:56.649085 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.38619 (* 1 = 9.38619 loss)
I0522 22:22:57.485618 34682 sgd_solver.cpp:112] Iteration 4510, lr = 0.01
I0522 22:23:03.910430 34682 solver.cpp:239] Iteration 4520 (1.37721 iter/s, 7.26105s/10 iters), loss = 8.78601
I0522 22:23:03.910488 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78601 (* 1 = 8.78601 loss)
I0522 22:23:04.629576 34682 sgd_solver.cpp:112] Iteration 4520, lr = 0.01
I0522 22:23:07.914081 34682 solver.cpp:239] Iteration 4530 (2.49786 iter/s, 4.00342s/10 iters), loss = 8.82351
I0522 22:23:07.914151 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82351 (* 1 = 8.82351 loss)
I0522 22:23:07.969907 34682 sgd_solver.cpp:112] Iteration 4530, lr = 0.01
I0522 22:23:09.933079 34682 solver.cpp:239] Iteration 4540 (4.95334 iter/s, 2.01884s/10 iters), loss = 9.08094
I0522 22:23:09.933130 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08094 (* 1 = 9.08094 loss)
I0522 22:23:09.990506 34682 sgd_solver.cpp:112] Iteration 4540, lr = 0.01
I0522 22:23:17.253780 34682 solver.cpp:239] Iteration 4550 (1.36605 iter/s, 7.32035s/10 iters), loss = 8.96021
I0522 22:23:17.253957 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96021 (* 1 = 8.96021 loss)
I0522 22:23:17.333042 34682 sgd_solver.cpp:112] Iteration 4550, lr = 0.01
I0522 22:23:19.935482 34682 solver.cpp:239] Iteration 4560 (3.72937 iter/s, 2.68142s/10 iters), loss = 9.635
I0522 22:23:19.935519 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.635 (* 1 = 9.635 loss)
I0522 22:23:20.001543 34682 sgd_solver.cpp:112] Iteration 4560, lr = 0.01
I0522 22:23:23.398918 34682 solver.cpp:239] Iteration 4570 (2.88747 iter/s, 3.46324s/10 iters), loss = 9.09137
I0522 22:23:23.398977 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09137 (* 1 = 9.09137 loss)
I0522 22:23:23.460733 34682 sgd_solver.cpp:112] Iteration 4570, lr = 0.01
I0522 22:23:28.301334 34682 solver.cpp:239] Iteration 4580 (2.03992 iter/s, 4.90216s/10 iters), loss = 9.04184
I0522 22:23:28.301385 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04184 (* 1 = 9.04184 loss)
I0522 22:23:28.374753 34682 sgd_solver.cpp:112] Iteration 4580, lr = 0.01
I0522 22:23:32.234232 34682 solver.cpp:239] Iteration 4590 (2.5428 iter/s, 3.93267s/10 iters), loss = 9.1621
I0522 22:23:32.234313 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1621 (* 1 = 9.1621 loss)
I0522 22:23:32.308305 34682 sgd_solver.cpp:112] Iteration 4590, lr = 0.01
I0522 22:23:38.075428 34682 solver.cpp:239] Iteration 4600 (1.71207 iter/s, 5.84088s/10 iters), loss = 8.81498
I0522 22:23:38.075484 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81498 (* 1 = 8.81498 loss)
I0522 22:23:38.884874 34682 sgd_solver.cpp:112] Iteration 4600, lr = 0.01
I0522 22:23:44.491348 34682 solver.cpp:239] Iteration 4610 (1.5587 iter/s, 6.41561s/10 iters), loss = 8.94207
I0522 22:23:44.491395 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94207 (* 1 = 8.94207 loss)
I0522 22:23:45.009722 34682 sgd_solver.cpp:112] Iteration 4610, lr = 0.01
I0522 22:23:48.464869 34682 solver.cpp:239] Iteration 4620 (2.5168 iter/s, 3.9733s/10 iters), loss = 9.01763
I0522 22:23:48.465042 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01763 (* 1 = 9.01763 loss)
I0522 22:23:49.296514 34682 sgd_solver.cpp:112] Iteration 4620, lr = 0.01
I0522 22:23:54.118661 34682 solver.cpp:239] Iteration 4630 (1.76885 iter/s, 5.65339s/10 iters), loss = 9.01827
I0522 22:23:54.118762 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01827 (* 1 = 9.01827 loss)
I0522 22:23:54.185781 34682 sgd_solver.cpp:112] Iteration 4630, lr = 0.01
I0522 22:23:57.873860 34682 solver.cpp:239] Iteration 4640 (2.66316 iter/s, 3.75494s/10 iters), loss = 8.8641
I0522 22:23:57.873927 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8641 (* 1 = 8.8641 loss)
I0522 22:23:57.952348 34682 sgd_solver.cpp:112] Iteration 4640, lr = 0.01
I0522 22:24:01.239266 34682 solver.cpp:239] Iteration 4650 (2.97161 iter/s, 3.36518s/10 iters), loss = 8.48101
I0522 22:24:01.239331 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48101 (* 1 = 8.48101 loss)
I0522 22:24:02.064395 34682 sgd_solver.cpp:112] Iteration 4650, lr = 0.01
I0522 22:24:06.578579 34682 solver.cpp:239] Iteration 4660 (1.873 iter/s, 5.33902s/10 iters), loss = 9.3405
I0522 22:24:06.578635 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3405 (* 1 = 9.3405 loss)
I0522 22:24:07.354998 34682 sgd_solver.cpp:112] Iteration 4660, lr = 0.01
I0522 22:24:14.228075 34682 solver.cpp:239] Iteration 4670 (1.30734 iter/s, 7.64911s/10 iters), loss = 9.22779
I0522 22:24:14.228144 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22779 (* 1 = 9.22779 loss)
I0522 22:24:14.519114 34682 sgd_solver.cpp:112] Iteration 4670, lr = 0.01
I0522 22:24:19.361371 34682 solver.cpp:239] Iteration 4680 (1.94817 iter/s, 5.13302s/10 iters), loss = 8.96957
I0522 22:24:19.361511 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96957 (* 1 = 8.96957 loss)
I0522 22:24:19.426367 34682 sgd_solver.cpp:112] Iteration 4680, lr = 0.01
I0522 22:24:24.282483 34682 solver.cpp:239] Iteration 4690 (2.0322 iter/s, 4.92077s/10 iters), loss = 8.32825
I0522 22:24:24.282536 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32825 (* 1 = 8.32825 loss)
I0522 22:24:25.053786 34682 sgd_solver.cpp:112] Iteration 4690, lr = 0.01
I0522 22:24:27.576700 34682 solver.cpp:239] Iteration 4700 (3.0358 iter/s, 3.29402s/10 iters), loss = 8.75516
I0522 22:24:27.576752 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75516 (* 1 = 8.75516 loss)
I0522 22:24:28.315322 34682 sgd_solver.cpp:112] Iteration 4700, lr = 0.01
I0522 22:24:33.476768 34682 solver.cpp:239] Iteration 4710 (1.69498 iter/s, 5.89978s/10 iters), loss = 8.4526
I0522 22:24:33.476824 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4526 (* 1 = 8.4526 loss)
I0522 22:24:34.289239 34682 sgd_solver.cpp:112] Iteration 4710, lr = 0.01
I0522 22:24:39.282418 34682 solver.cpp:239] Iteration 4720 (1.72255 iter/s, 5.80535s/10 iters), loss = 9.39339
I0522 22:24:39.282470 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.39339 (* 1 = 9.39339 loss)
I0522 22:24:39.346946 34682 sgd_solver.cpp:112] Iteration 4720, lr = 0.01
I0522 22:24:43.420089 34682 solver.cpp:239] Iteration 4730 (2.41695 iter/s, 4.13745s/10 iters), loss = 9.32554
I0522 22:24:43.420132 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32554 (* 1 = 9.32554 loss)
I0522 22:24:43.481660 34682 sgd_solver.cpp:112] Iteration 4730, lr = 0.01
I0522 22:24:47.720635 34682 solver.cpp:239] Iteration 4740 (2.32541 iter/s, 4.30032s/10 iters), loss = 8.81756
I0522 22:24:47.720688 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81756 (* 1 = 8.81756 loss)
I0522 22:24:47.782716 34682 sgd_solver.cpp:112] Iteration 4740, lr = 0.01
I0522 22:24:51.483340 34682 solver.cpp:239] Iteration 4750 (2.65781 iter/s, 3.76249s/10 iters), loss = 8.79183
I0522 22:24:51.483605 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79183 (* 1 = 8.79183 loss)
I0522 22:24:52.196880 34682 sgd_solver.cpp:112] Iteration 4750, lr = 0.01
I0522 22:24:57.205482 34682 solver.cpp:239] Iteration 4760 (1.74774 iter/s, 5.72167s/10 iters), loss = 9.0078
I0522 22:24:57.205531 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0078 (* 1 = 9.0078 loss)
I0522 22:24:57.272214 34682 sgd_solver.cpp:112] Iteration 4760, lr = 0.01
I0522 22:25:03.153102 34682 solver.cpp:239] Iteration 4770 (1.68143 iter/s, 5.94733s/10 iters), loss = 8.26172
I0522 22:25:03.153156 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26172 (* 1 = 8.26172 loss)
I0522 22:25:03.214483 34682 sgd_solver.cpp:112] Iteration 4770, lr = 0.01
I0522 22:25:09.262536 34682 solver.cpp:239] Iteration 4780 (1.63689 iter/s, 6.10913s/10 iters), loss = 8.37125
I0522 22:25:09.262595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37125 (* 1 = 8.37125 loss)
I0522 22:25:09.867962 34682 sgd_solver.cpp:112] Iteration 4780, lr = 0.01
I0522 22:25:12.690740 34682 solver.cpp:239] Iteration 4790 (2.91715 iter/s, 3.42801s/10 iters), loss = 8.6682
I0522 22:25:12.690783 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6682 (* 1 = 8.6682 loss)
I0522 22:25:12.759852 34682 sgd_solver.cpp:112] Iteration 4790, lr = 0.01
I0522 22:25:18.406026 34682 solver.cpp:239] Iteration 4800 (1.74978 iter/s, 5.715s/10 iters), loss = 8.38819
I0522 22:25:18.406090 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38819 (* 1 = 8.38819 loss)
I0522 22:25:18.479326 34682 sgd_solver.cpp:112] Iteration 4800, lr = 0.01
I0522 22:25:23.168733 34682 solver.cpp:239] Iteration 4810 (2.09976 iter/s, 4.76245s/10 iters), loss = 8.38842
I0522 22:25:23.169072 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38842 (* 1 = 8.38842 loss)
I0522 22:25:23.250169 34682 sgd_solver.cpp:112] Iteration 4810, lr = 0.01
I0522 22:25:27.400387 34682 solver.cpp:239] Iteration 4820 (2.36582 iter/s, 4.22687s/10 iters), loss = 9.01442
I0522 22:25:27.400444 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01442 (* 1 = 9.01442 loss)
I0522 22:25:27.465378 34682 sgd_solver.cpp:112] Iteration 4820, lr = 0.01
I0522 22:25:31.080462 34682 solver.cpp:239] Iteration 4830 (2.71749 iter/s, 3.67987s/10 iters), loss = 9.67801
I0522 22:25:31.080514 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.67801 (* 1 = 9.67801 loss)
I0522 22:25:31.908396 34682 sgd_solver.cpp:112] Iteration 4830, lr = 0.01
I0522 22:25:37.558523 34682 solver.cpp:239] Iteration 4840 (1.54375 iter/s, 6.47774s/10 iters), loss = 8.79047
I0522 22:25:37.558588 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79047 (* 1 = 8.79047 loss)
I0522 22:25:37.622237 34682 sgd_solver.cpp:112] Iteration 4840, lr = 0.01
I0522 22:25:42.351555 34682 solver.cpp:239] Iteration 4850 (2.08648 iter/s, 4.79275s/10 iters), loss = 8.55206
I0522 22:25:42.351613 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55206 (* 1 = 8.55206 loss)
I0522 22:25:43.156095 34682 sgd_solver.cpp:112] Iteration 4850, lr = 0.01
I0522 22:25:47.981081 34682 solver.cpp:239] Iteration 4860 (1.77644 iter/s, 5.62924s/10 iters), loss = 8.2344
I0522 22:25:47.981129 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2344 (* 1 = 8.2344 loss)
I0522 22:25:48.043812 34682 sgd_solver.cpp:112] Iteration 4860, lr = 0.01
I0522 22:25:53.386802 34682 solver.cpp:239] Iteration 4870 (1.84998 iter/s, 5.40545s/10 iters), loss = 8.97252
I0522 22:25:53.386934 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97252 (* 1 = 8.97252 loss)
I0522 22:25:53.445078 34682 sgd_solver.cpp:112] Iteration 4870, lr = 0.01
I0522 22:25:58.004622 34682 solver.cpp:239] Iteration 4880 (2.16568 iter/s, 4.61749s/10 iters), loss = 8.78549
I0522 22:25:58.004693 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78549 (* 1 = 8.78549 loss)
I0522 22:25:58.069711 34682 sgd_solver.cpp:112] Iteration 4880, lr = 0.01
I0522 22:26:02.676163 34682 solver.cpp:239] Iteration 4890 (2.14074 iter/s, 4.67129s/10 iters), loss = 9.06056
I0522 22:26:02.676210 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06056 (* 1 = 9.06056 loss)
I0522 22:26:02.739586 34682 sgd_solver.cpp:112] Iteration 4890, lr = 0.01
I0522 22:26:07.540180 34682 solver.cpp:239] Iteration 4900 (2.05602 iter/s, 4.86376s/10 iters), loss = 8.62059
I0522 22:26:07.540248 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62059 (* 1 = 8.62059 loss)
I0522 22:26:08.338477 34682 sgd_solver.cpp:112] Iteration 4900, lr = 0.01
I0522 22:26:13.984413 34682 solver.cpp:239] Iteration 4910 (1.55185 iter/s, 6.44391s/10 iters), loss = 8.50758
I0522 22:26:13.984463 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50758 (* 1 = 8.50758 loss)
I0522 22:26:14.658368 34682 sgd_solver.cpp:112] Iteration 4910, lr = 0.01
I0522 22:26:19.723004 34682 solver.cpp:239] Iteration 4920 (1.74268 iter/s, 5.7383s/10 iters), loss = 9.03306
I0522 22:26:19.723065 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03306 (* 1 = 9.03306 loss)
I0522 22:26:19.786435 34682 sgd_solver.cpp:112] Iteration 4920, lr = 0.01
I0522 22:26:23.230207 34682 solver.cpp:239] Iteration 4930 (2.85144 iter/s, 3.507s/10 iters), loss = 8.81079
I0522 22:26:23.230252 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81079 (* 1 = 8.81079 loss)
I0522 22:26:23.297325 34682 sgd_solver.cpp:112] Iteration 4930, lr = 0.01
I0522 22:26:28.252215 34682 solver.cpp:239] Iteration 4940 (1.99134 iter/s, 5.02175s/10 iters), loss = 9.21901
I0522 22:26:28.252449 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21901 (* 1 = 9.21901 loss)
I0522 22:26:28.563504 34682 sgd_solver.cpp:112] Iteration 4940, lr = 0.01
I0522 22:26:33.481834 34682 solver.cpp:239] Iteration 4950 (1.91234 iter/s, 5.2292s/10 iters), loss = 8.8629
I0522 22:26:33.481884 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8629 (* 1 = 8.8629 loss)
I0522 22:26:33.555025 34682 sgd_solver.cpp:112] Iteration 4950, lr = 0.01
I0522 22:26:37.609438 34682 solver.cpp:239] Iteration 4960 (2.42285 iter/s, 4.12738s/10 iters), loss = 9.05777
I0522 22:26:37.609490 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05777 (* 1 = 9.05777 loss)
I0522 22:26:38.413666 34682 sgd_solver.cpp:112] Iteration 4960, lr = 0.01
I0522 22:26:41.368273 34682 solver.cpp:239] Iteration 4970 (2.66055 iter/s, 3.75862s/10 iters), loss = 8.80354
I0522 22:26:41.368327 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80354 (* 1 = 8.80354 loss)
I0522 22:26:41.503671 34682 sgd_solver.cpp:112] Iteration 4970, lr = 0.01
I0522 22:26:45.224670 34682 solver.cpp:239] Iteration 4980 (2.59324 iter/s, 3.85618s/10 iters), loss = 8.39831
I0522 22:26:45.224725 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39831 (* 1 = 8.39831 loss)
I0522 22:26:46.015239 34682 sgd_solver.cpp:112] Iteration 4980, lr = 0.01
I0522 22:26:51.595239 34682 solver.cpp:239] Iteration 4990 (1.5698 iter/s, 6.37026s/10 iters), loss = 8.2388
I0522 22:26:51.595283 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2388 (* 1 = 8.2388 loss)
I0522 22:26:51.661859 34682 sgd_solver.cpp:112] Iteration 4990, lr = 0.01
I0522 22:26:55.706924 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_5000.caffemodel
I0522 22:26:56.921802 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_5000.solverstate
I0522 22:26:57.125118 34682 solver.cpp:239] Iteration 5000 (1.80845 iter/s, 5.52961s/10 iters), loss = 8.65432
I0522 22:26:57.125157 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65432 (* 1 = 8.65432 loss)
I0522 22:26:57.199048 34682 sgd_solver.cpp:112] Iteration 5000, lr = 0.01
I0522 22:27:01.203562 34682 solver.cpp:239] Iteration 5010 (2.45205 iter/s, 4.07822s/10 iters), loss = 8.42456
I0522 22:27:01.203820 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42456 (* 1 = 8.42456 loss)
I0522 22:27:01.338904 34682 sgd_solver.cpp:112] Iteration 5010, lr = 0.01
I0522 22:27:05.567381 34682 solver.cpp:239] Iteration 5020 (2.29178 iter/s, 4.36342s/10 iters), loss = 8.81849
I0522 22:27:05.567420 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81849 (* 1 = 8.81849 loss)
I0522 22:27:05.640170 34682 sgd_solver.cpp:112] Iteration 5020, lr = 0.01
I0522 22:27:10.690670 34682 solver.cpp:239] Iteration 5030 (1.95197 iter/s, 5.12304s/10 iters), loss = 8.76102
I0522 22:27:10.690732 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76102 (* 1 = 8.76102 loss)
I0522 22:27:11.502355 34682 sgd_solver.cpp:112] Iteration 5030, lr = 0.01
I0522 22:27:16.263957 34682 solver.cpp:239] Iteration 5040 (1.79437 iter/s, 5.573s/10 iters), loss = 8.61783
I0522 22:27:16.264017 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61783 (* 1 = 8.61783 loss)
I0522 22:27:16.918515 34682 sgd_solver.cpp:112] Iteration 5040, lr = 0.01
I0522 22:27:18.978005 34682 solver.cpp:239] Iteration 5050 (3.68477 iter/s, 2.71388s/10 iters), loss = 8.26705
I0522 22:27:18.978060 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26705 (* 1 = 8.26705 loss)
I0522 22:27:19.333647 34682 sgd_solver.cpp:112] Iteration 5050, lr = 0.01
I0522 22:27:24.294463 34682 solver.cpp:239] Iteration 5060 (1.88105 iter/s, 5.31618s/10 iters), loss = 9.02573
I0522 22:27:24.294514 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02573 (* 1 = 9.02573 loss)
I0522 22:27:25.146872 34682 sgd_solver.cpp:112] Iteration 5060, lr = 0.01
I0522 22:27:30.622653 34682 solver.cpp:239] Iteration 5070 (1.58031 iter/s, 6.32788s/10 iters), loss = 8.90537
I0522 22:27:30.622740 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90537 (* 1 = 8.90537 loss)
I0522 22:27:30.710796 34682 sgd_solver.cpp:112] Iteration 5070, lr = 0.01
I0522 22:27:37.019460 34682 solver.cpp:239] Iteration 5080 (1.56336 iter/s, 6.39647s/10 iters), loss = 8.87435
I0522 22:27:37.019750 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87435 (* 1 = 8.87435 loss)
I0522 22:27:37.092497 34682 sgd_solver.cpp:112] Iteration 5080, lr = 0.01
I0522 22:27:42.875097 34682 solver.cpp:239] Iteration 5090 (1.7079 iter/s, 5.85513s/10 iters), loss = 8.82379
I0522 22:27:42.875159 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82379 (* 1 = 8.82379 loss)
I0522 22:27:42.945554 34682 sgd_solver.cpp:112] Iteration 5090, lr = 0.01
I0522 22:27:46.504470 34682 solver.cpp:239] Iteration 5100 (2.75545 iter/s, 3.62917s/10 iters), loss = 8.78263
I0522 22:27:46.504513 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78263 (* 1 = 8.78263 loss)
I0522 22:27:46.561518 34682 sgd_solver.cpp:112] Iteration 5100, lr = 0.01
I0522 22:27:54.289901 34682 solver.cpp:239] Iteration 5110 (1.28451 iter/s, 7.78506s/10 iters), loss = 8.76749
I0522 22:27:54.289993 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76749 (* 1 = 8.76749 loss)
I0522 22:27:54.357892 34682 sgd_solver.cpp:112] Iteration 5110, lr = 0.01
I0522 22:27:58.394752 34682 solver.cpp:239] Iteration 5120 (2.43631 iter/s, 4.10456s/10 iters), loss = 7.84231
I0522 22:27:58.394820 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84231 (* 1 = 7.84231 loss)
I0522 22:27:59.237463 34682 sgd_solver.cpp:112] Iteration 5120, lr = 0.01
I0522 22:28:04.202034 34682 solver.cpp:239] Iteration 5130 (1.72207 iter/s, 5.80698s/10 iters), loss = 8.70167
I0522 22:28:04.202085 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70167 (* 1 = 8.70167 loss)
I0522 22:28:05.075942 34682 sgd_solver.cpp:112] Iteration 5130, lr = 0.01
I0522 22:28:10.733223 34682 solver.cpp:239] Iteration 5140 (1.53119 iter/s, 6.53088s/10 iters), loss = 9.10312
I0522 22:28:10.733491 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10312 (* 1 = 9.10312 loss)
I0522 22:28:10.812273 34682 sgd_solver.cpp:112] Iteration 5140, lr = 0.01
I0522 22:28:15.946619 34682 solver.cpp:239] Iteration 5150 (1.91918 iter/s, 5.21056s/10 iters), loss = 8.97336
I0522 22:28:15.946681 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97336 (* 1 = 8.97336 loss)
I0522 22:28:16.151342 34682 sgd_solver.cpp:112] Iteration 5150, lr = 0.01
I0522 22:28:20.270514 34682 solver.cpp:239] Iteration 5160 (2.31286 iter/s, 4.32365s/10 iters), loss = 8.62717
I0522 22:28:20.270572 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62717 (* 1 = 8.62717 loss)
I0522 22:28:21.041777 34682 sgd_solver.cpp:112] Iteration 5160, lr = 0.01
I0522 22:28:27.175581 34682 solver.cpp:239] Iteration 5170 (1.44828 iter/s, 6.90473s/10 iters), loss = 8.67474
I0522 22:28:27.175628 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67474 (* 1 = 8.67474 loss)
I0522 22:28:27.244498 34682 sgd_solver.cpp:112] Iteration 5170, lr = 0.01
I0522 22:28:32.055414 34682 solver.cpp:239] Iteration 5180 (2.05121 iter/s, 4.87516s/10 iters), loss = 8.40916
I0522 22:28:32.055469 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40916 (* 1 = 8.40916 loss)
I0522 22:28:32.112314 34682 sgd_solver.cpp:112] Iteration 5180, lr = 0.01
I0522 22:28:37.016674 34682 solver.cpp:239] Iteration 5190 (2.01572 iter/s, 4.961s/10 iters), loss = 8.70771
I0522 22:28:37.016724 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70771 (* 1 = 8.70771 loss)
I0522 22:28:37.673449 34682 sgd_solver.cpp:112] Iteration 5190, lr = 0.01
I0522 22:28:41.003919 34682 solver.cpp:239] Iteration 5200 (2.50813 iter/s, 3.98703s/10 iters), loss = 8.47292
I0522 22:28:41.004202 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47292 (* 1 = 8.47292 loss)
I0522 22:28:41.808955 34682 sgd_solver.cpp:112] Iteration 5200, lr = 0.01
I0522 22:28:44.410117 34682 solver.cpp:239] Iteration 5210 (2.93615 iter/s, 3.40582s/10 iters), loss = 9.17582
I0522 22:28:44.410167 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17582 (* 1 = 9.17582 loss)
I0522 22:28:45.234422 34682 sgd_solver.cpp:112] Iteration 5210, lr = 0.01
I0522 22:28:49.462388 34682 solver.cpp:239] Iteration 5220 (1.97941 iter/s, 5.052s/10 iters), loss = 9.26408
I0522 22:28:49.462457 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.26408 (* 1 = 9.26408 loss)
I0522 22:28:49.523183 34682 sgd_solver.cpp:112] Iteration 5220, lr = 0.01
I0522 22:28:54.271944 34682 solver.cpp:239] Iteration 5230 (2.07931 iter/s, 4.80929s/10 iters), loss = 8.57955
I0522 22:28:54.271996 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57955 (* 1 = 8.57955 loss)
I0522 22:28:54.359442 34682 sgd_solver.cpp:112] Iteration 5230, lr = 0.01
I0522 22:29:00.511816 34682 solver.cpp:239] Iteration 5240 (1.60268 iter/s, 6.23955s/10 iters), loss = 8.73862
I0522 22:29:00.511886 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73862 (* 1 = 8.73862 loss)
I0522 22:29:01.337033 34682 sgd_solver.cpp:112] Iteration 5240, lr = 0.01
I0522 22:29:06.155194 34682 solver.cpp:239] Iteration 5250 (1.77208 iter/s, 5.64308s/10 iters), loss = 8.8426
I0522 22:29:06.155241 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8426 (* 1 = 8.8426 loss)
I0522 22:29:07.007536 34682 sgd_solver.cpp:112] Iteration 5250, lr = 0.01
I0522 22:29:12.409647 34682 solver.cpp:239] Iteration 5260 (1.59894 iter/s, 6.25414s/10 iters), loss = 8.562
I0522 22:29:12.409873 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.562 (* 1 = 8.562 loss)
I0522 22:29:13.277329 34682 sgd_solver.cpp:112] Iteration 5260, lr = 0.01
I0522 22:29:16.565636 34682 solver.cpp:239] Iteration 5270 (2.40639 iter/s, 4.1556s/10 iters), loss = 8.62058
I0522 22:29:16.565696 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62058 (* 1 = 8.62058 loss)
I0522 22:29:17.431788 34682 sgd_solver.cpp:112] Iteration 5270, lr = 0.01
I0522 22:29:21.603042 34682 solver.cpp:239] Iteration 5280 (1.98525 iter/s, 5.03715s/10 iters), loss = 8.50269
I0522 22:29:21.603091 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50269 (* 1 = 8.50269 loss)
I0522 22:29:22.189779 34682 sgd_solver.cpp:112] Iteration 5280, lr = 0.01
I0522 22:29:27.241307 34682 solver.cpp:239] Iteration 5290 (1.77369 iter/s, 5.63797s/10 iters), loss = 9.55426
I0522 22:29:27.241394 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.55426 (* 1 = 9.55426 loss)
I0522 22:29:27.919045 34682 sgd_solver.cpp:112] Iteration 5290, lr = 0.01
I0522 22:29:32.745012 34682 solver.cpp:239] Iteration 5300 (1.81706 iter/s, 5.50339s/10 iters), loss = 9.05181
I0522 22:29:32.745064 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05181 (* 1 = 9.05181 loss)
I0522 22:29:32.807190 34682 sgd_solver.cpp:112] Iteration 5300, lr = 0.01
I0522 22:29:36.790153 34682 solver.cpp:239] Iteration 5310 (2.47224 iter/s, 4.04492s/10 iters), loss = 9.30227
I0522 22:29:36.790199 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30227 (* 1 = 9.30227 loss)
I0522 22:29:36.869648 34682 sgd_solver.cpp:112] Iteration 5310, lr = 0.01
I0522 22:29:42.637423 34682 solver.cpp:239] Iteration 5320 (1.71028 iter/s, 5.84698s/10 iters), loss = 8.48837
I0522 22:29:42.637617 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48837 (* 1 = 8.48837 loss)
I0522 22:29:42.707150 34682 sgd_solver.cpp:112] Iteration 5320, lr = 0.01
I0522 22:29:47.067018 34682 solver.cpp:239] Iteration 5330 (2.25774 iter/s, 4.42921s/10 iters), loss = 8.1338
I0522 22:29:47.067070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1338 (* 1 = 8.1338 loss)
I0522 22:29:47.130568 34682 sgd_solver.cpp:112] Iteration 5330, lr = 0.01
I0522 22:29:51.364920 34682 solver.cpp:239] Iteration 5340 (2.32685 iter/s, 4.29766s/10 iters), loss = 8.59517
I0522 22:29:51.364969 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59517 (* 1 = 8.59517 loss)
I0522 22:29:51.442039 34682 sgd_solver.cpp:112] Iteration 5340, lr = 0.01
I0522 22:29:54.695019 34682 solver.cpp:239] Iteration 5350 (3.0031 iter/s, 3.32989s/10 iters), loss = 8.91514
I0522 22:29:54.695071 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91514 (* 1 = 8.91514 loss)
I0522 22:29:55.439218 34682 sgd_solver.cpp:112] Iteration 5350, lr = 0.01
I0522 22:30:00.937969 34682 solver.cpp:239] Iteration 5360 (1.60189 iter/s, 6.24262s/10 iters), loss = 8.25515
I0522 22:30:00.938014 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25515 (* 1 = 8.25515 loss)
I0522 22:30:01.003268 34682 sgd_solver.cpp:112] Iteration 5360, lr = 0.01
I0522 22:30:03.547430 34682 solver.cpp:239] Iteration 5370 (3.83244 iter/s, 2.6093s/10 iters), loss = 8.83362
I0522 22:30:03.547473 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83362 (* 1 = 8.83362 loss)
I0522 22:30:03.603597 34682 sgd_solver.cpp:112] Iteration 5370, lr = 0.01
I0522 22:30:07.844416 34682 solver.cpp:239] Iteration 5380 (2.32734 iter/s, 4.29676s/10 iters), loss = 8.72912
I0522 22:30:07.844465 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72912 (* 1 = 8.72912 loss)
I0522 22:30:08.657866 34682 sgd_solver.cpp:112] Iteration 5380, lr = 0.01
I0522 22:30:12.089192 34682 solver.cpp:239] Iteration 5390 (2.35596 iter/s, 4.24455s/10 iters), loss = 8.52119
I0522 22:30:12.089242 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52119 (* 1 = 8.52119 loss)
I0522 22:30:12.150871 34682 sgd_solver.cpp:112] Iteration 5390, lr = 0.01
I0522 22:30:17.182114 34682 solver.cpp:239] Iteration 5400 (1.96361 iter/s, 5.09266s/10 iters), loss = 8.95824
I0522 22:30:17.182301 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95824 (* 1 = 8.95824 loss)
I0522 22:30:17.257596 34682 sgd_solver.cpp:112] Iteration 5400, lr = 0.01
I0522 22:30:21.369489 34682 solver.cpp:239] Iteration 5410 (2.38832 iter/s, 4.18704s/10 iters), loss = 8.4682
I0522 22:30:21.369537 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4682 (* 1 = 8.4682 loss)
I0522 22:30:22.158107 34682 sgd_solver.cpp:112] Iteration 5410, lr = 0.01
I0522 22:30:27.627254 34682 solver.cpp:239] Iteration 5420 (1.59809 iter/s, 6.25746s/10 iters), loss = 8.00831
I0522 22:30:27.627310 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00831 (* 1 = 8.00831 loss)
I0522 22:30:28.337699 34682 sgd_solver.cpp:112] Iteration 5420, lr = 0.01
I0522 22:30:32.553382 34682 solver.cpp:239] Iteration 5430 (2.0301 iter/s, 4.92586s/10 iters), loss = 8.98783
I0522 22:30:32.553432 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98783 (* 1 = 8.98783 loss)
I0522 22:30:33.276266 34682 sgd_solver.cpp:112] Iteration 5430, lr = 0.01
I0522 22:30:38.283869 34682 solver.cpp:239] Iteration 5440 (1.74514 iter/s, 5.7302s/10 iters), loss = 8.53039
I0522 22:30:38.283911 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53039 (* 1 = 8.53039 loss)
I0522 22:30:38.357425 34682 sgd_solver.cpp:112] Iteration 5440, lr = 0.01
I0522 22:30:41.769414 34682 solver.cpp:239] Iteration 5450 (2.86915 iter/s, 3.48535s/10 iters), loss = 8.69214
I0522 22:30:41.769462 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69214 (* 1 = 8.69214 loss)
I0522 22:30:41.841711 34682 sgd_solver.cpp:112] Iteration 5450, lr = 0.01
I0522 22:30:46.589525 34682 solver.cpp:239] Iteration 5460 (2.07474 iter/s, 4.81987s/10 iters), loss = 8.75314
I0522 22:30:46.589563 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75314 (* 1 = 8.75314 loss)
I0522 22:30:47.302561 34682 sgd_solver.cpp:112] Iteration 5460, lr = 0.01
I0522 22:30:50.727946 34682 solver.cpp:239] Iteration 5470 (2.41651 iter/s, 4.13821s/10 iters), loss = 9.03382
I0522 22:30:50.728006 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03382 (* 1 = 9.03382 loss)
I0522 22:30:50.788661 34682 sgd_solver.cpp:112] Iteration 5470, lr = 0.01
I0522 22:30:53.875353 34682 solver.cpp:239] Iteration 5480 (3.17741 iter/s, 3.14722s/10 iters), loss = 8.67684
I0522 22:30:53.875411 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67684 (* 1 = 8.67684 loss)
I0522 22:30:53.952673 34682 sgd_solver.cpp:112] Iteration 5480, lr = 0.01
I0522 22:30:58.154130 34682 solver.cpp:239] Iteration 5490 (2.33724 iter/s, 4.27855s/10 iters), loss = 8.10416
I0522 22:30:58.154175 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10416 (* 1 = 8.10416 loss)
I0522 22:30:58.484465 34682 sgd_solver.cpp:112] Iteration 5490, lr = 0.01
I0522 22:31:02.807435 34682 solver.cpp:239] Iteration 5500 (2.14913 iter/s, 4.65305s/10 iters), loss = 9.09657
I0522 22:31:02.807495 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09657 (* 1 = 9.09657 loss)
I0522 22:31:02.884035 34682 sgd_solver.cpp:112] Iteration 5500, lr = 0.01
I0522 22:31:08.517882 34682 solver.cpp:239] Iteration 5510 (1.75127 iter/s, 5.71015s/10 iters), loss = 8.73316
I0522 22:31:08.517940 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73316 (* 1 = 8.73316 loss)
I0522 22:31:09.017643 34682 sgd_solver.cpp:112] Iteration 5510, lr = 0.01
I0522 22:31:14.229866 34682 solver.cpp:239] Iteration 5520 (1.7508 iter/s, 5.71168s/10 iters), loss = 8.64813
I0522 22:31:14.229941 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64813 (* 1 = 8.64813 loss)
I0522 22:31:14.294077 34682 sgd_solver.cpp:112] Iteration 5520, lr = 0.01
I0522 22:31:18.341361 34682 solver.cpp:239] Iteration 5530 (2.43236 iter/s, 4.11123s/10 iters), loss = 8.55328
I0522 22:31:18.341624 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55328 (* 1 = 8.55328 loss)
I0522 22:31:19.202435 34682 sgd_solver.cpp:112] Iteration 5530, lr = 0.01
I0522 22:31:22.354027 34682 solver.cpp:239] Iteration 5540 (2.49236 iter/s, 4.01226s/10 iters), loss = 7.97162
I0522 22:31:22.354080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97162 (* 1 = 7.97162 loss)
I0522 22:31:22.424252 34682 sgd_solver.cpp:112] Iteration 5540, lr = 0.01
I0522 22:31:28.815436 34682 solver.cpp:239] Iteration 5550 (1.54773 iter/s, 6.46108s/10 iters), loss = 9.01307
I0522 22:31:28.815485 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01307 (* 1 = 9.01307 loss)
I0522 22:31:28.887001 34682 sgd_solver.cpp:112] Iteration 5550, lr = 0.01
I0522 22:31:33.503137 34682 solver.cpp:239] Iteration 5560 (2.13335 iter/s, 4.68746s/10 iters), loss = 8.18639
I0522 22:31:33.503268 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18639 (* 1 = 8.18639 loss)
I0522 22:31:33.561148 34682 sgd_solver.cpp:112] Iteration 5560, lr = 0.01
I0522 22:31:36.385833 34682 solver.cpp:239] Iteration 5570 (3.46927 iter/s, 2.88245s/10 iters), loss = 8.69428
I0522 22:31:36.385884 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69428 (* 1 = 8.69428 loss)
I0522 22:31:36.449436 34682 sgd_solver.cpp:112] Iteration 5570, lr = 0.01
I0522 22:31:39.899664 34682 solver.cpp:239] Iteration 5580 (2.84606 iter/s, 3.51363s/10 iters), loss = 9.1887
I0522 22:31:39.899724 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1887 (* 1 = 9.1887 loss)
I0522 22:31:40.724871 34682 sgd_solver.cpp:112] Iteration 5580, lr = 0.01
I0522 22:31:44.578982 34682 solver.cpp:239] Iteration 5590 (2.13718 iter/s, 4.67907s/10 iters), loss = 9.90364
I0522 22:31:44.579039 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.90364 (* 1 = 9.90364 loss)
I0522 22:31:45.332584 34682 sgd_solver.cpp:112] Iteration 5590, lr = 0.01
I0522 22:31:49.288718 34682 solver.cpp:239] Iteration 5600 (2.12337 iter/s, 4.70949s/10 iters), loss = 8.6819
I0522 22:31:49.289024 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6819 (* 1 = 8.6819 loss)
I0522 22:31:50.109869 34682 sgd_solver.cpp:112] Iteration 5600, lr = 0.01
I0522 22:31:55.806670 34682 solver.cpp:239] Iteration 5610 (1.53435 iter/s, 6.5174s/10 iters), loss = 8.09825
I0522 22:31:55.806776 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09825 (* 1 = 8.09825 loss)
I0522 22:31:55.879899 34682 sgd_solver.cpp:112] Iteration 5610, lr = 0.01
I0522 22:32:01.514487 34682 solver.cpp:239] Iteration 5620 (1.75209 iter/s, 5.70747s/10 iters), loss = 8.17992
I0522 22:32:01.514546 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17992 (* 1 = 8.17992 loss)
I0522 22:32:01.574955 34682 sgd_solver.cpp:112] Iteration 5620, lr = 0.01
I0522 22:32:07.522631 34682 solver.cpp:239] Iteration 5630 (1.66449 iter/s, 6.00784s/10 iters), loss = 9.04544
I0522 22:32:07.522681 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04544 (* 1 = 9.04544 loss)
I0522 22:32:08.373888 34682 sgd_solver.cpp:112] Iteration 5630, lr = 0.01
I0522 22:32:12.407508 34682 solver.cpp:239] Iteration 5640 (2.04724 iter/s, 4.88463s/10 iters), loss = 8.74246
I0522 22:32:12.407554 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74246 (* 1 = 8.74246 loss)
I0522 22:32:12.463697 34682 sgd_solver.cpp:112] Iteration 5640, lr = 0.01
I0522 22:32:16.573642 34682 solver.cpp:239] Iteration 5650 (2.40044 iter/s, 4.1659s/10 iters), loss = 9.11433
I0522 22:32:16.573693 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11433 (* 1 = 9.11433 loss)
I0522 22:32:16.635736 34682 sgd_solver.cpp:112] Iteration 5650, lr = 0.01
I0522 22:32:21.513509 34682 solver.cpp:239] Iteration 5660 (2.02446 iter/s, 4.93958s/10 iters), loss = 8.74879
I0522 22:32:21.513712 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74879 (* 1 = 8.74879 loss)
I0522 22:32:22.326102 34682 sgd_solver.cpp:112] Iteration 5660, lr = 0.01
I0522 22:32:27.294540 34682 solver.cpp:239] Iteration 5670 (1.72992 iter/s, 5.78061s/10 iters), loss = 8.96478
I0522 22:32:27.294589 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96478 (* 1 = 8.96478 loss)
I0522 22:32:27.368352 34682 sgd_solver.cpp:112] Iteration 5670, lr = 0.01
I0522 22:32:31.636176 34682 solver.cpp:239] Iteration 5680 (2.3034 iter/s, 4.34141s/10 iters), loss = 8.35336
I0522 22:32:31.636225 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35336 (* 1 = 8.35336 loss)
I0522 22:32:32.469372 34682 sgd_solver.cpp:112] Iteration 5680, lr = 0.01
I0522 22:32:35.792727 34682 solver.cpp:239] Iteration 5690 (2.40598 iter/s, 4.15631s/10 iters), loss = 8.7536
I0522 22:32:35.792786 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7536 (* 1 = 8.7536 loss)
I0522 22:32:35.865599 34682 sgd_solver.cpp:112] Iteration 5690, lr = 0.01
I0522 22:32:39.384191 34682 solver.cpp:239] Iteration 5700 (2.78455 iter/s, 3.59125s/10 iters), loss = 8.971
I0522 22:32:39.384243 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.971 (* 1 = 8.971 loss)
I0522 22:32:39.447420 34682 sgd_solver.cpp:112] Iteration 5700, lr = 0.01
I0522 22:32:43.691488 34682 solver.cpp:239] Iteration 5710 (2.32177 iter/s, 4.30706s/10 iters), loss = 7.99634
I0522 22:32:43.691561 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99634 (* 1 = 7.99634 loss)
I0522 22:32:44.583827 34682 sgd_solver.cpp:112] Iteration 5710, lr = 0.01
I0522 22:32:49.296375 34682 solver.cpp:239] Iteration 5720 (1.78425 iter/s, 5.6046s/10 iters), loss = 9.14778
I0522 22:32:49.296425 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14778 (* 1 = 9.14778 loss)
I0522 22:32:49.355309 34682 sgd_solver.cpp:112] Iteration 5720, lr = 0.01
I0522 22:32:52.084309 34682 solver.cpp:239] Iteration 5730 (3.5871 iter/s, 2.78777s/10 iters), loss = 9.22708
I0522 22:32:52.084575 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22708 (* 1 = 9.22708 loss)
I0522 22:32:52.153671 34682 sgd_solver.cpp:112] Iteration 5730, lr = 0.01
I0522 22:32:57.550009 34682 solver.cpp:239] Iteration 5740 (1.82975 iter/s, 5.46524s/10 iters), loss = 9.29267
I0522 22:32:57.550063 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29267 (* 1 = 9.29267 loss)
I0522 22:32:57.613399 34682 sgd_solver.cpp:112] Iteration 5740, lr = 0.01
I0522 22:33:01.811226 34682 solver.cpp:239] Iteration 5750 (2.34688 iter/s, 4.26098s/10 iters), loss = 8.89653
I0522 22:33:01.811272 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89653 (* 1 = 8.89653 loss)
I0522 22:33:01.894172 34682 sgd_solver.cpp:112] Iteration 5750, lr = 0.01
I0522 22:33:05.974532 34682 solver.cpp:239] Iteration 5760 (2.40206 iter/s, 4.16309s/10 iters), loss = 8.52877
I0522 22:33:05.974580 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52877 (* 1 = 8.52877 loss)
I0522 22:33:06.057201 34682 sgd_solver.cpp:112] Iteration 5760, lr = 0.01
I0522 22:33:11.585391 34682 solver.cpp:239] Iteration 5770 (1.78235 iter/s, 5.61058s/10 iters), loss = 9.00334
I0522 22:33:11.585438 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00334 (* 1 = 9.00334 loss)
I0522 22:33:12.410814 34682 sgd_solver.cpp:112] Iteration 5770, lr = 0.01
I0522 22:33:17.169373 34682 solver.cpp:239] Iteration 5780 (1.79093 iter/s, 5.5837s/10 iters), loss = 8.47253
I0522 22:33:17.169421 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47253 (* 1 = 8.47253 loss)
I0522 22:33:17.228586 34682 sgd_solver.cpp:112] Iteration 5780, lr = 0.01
I0522 22:33:22.712704 34682 solver.cpp:239] Iteration 5790 (1.80406 iter/s, 5.54305s/10 iters), loss = 8.09115
I0522 22:33:22.712900 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09115 (* 1 = 8.09115 loss)
I0522 22:33:22.774669 34682 sgd_solver.cpp:112] Iteration 5790, lr = 0.01
I0522 22:33:28.000260 34682 solver.cpp:239] Iteration 5800 (1.89137 iter/s, 5.28717s/10 iters), loss = 8.2503
I0522 22:33:28.000311 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2503 (* 1 = 8.2503 loss)
I0522 22:33:28.174029 34682 sgd_solver.cpp:112] Iteration 5800, lr = 0.01
I0522 22:33:32.294260 34682 solver.cpp:239] Iteration 5810 (2.32896 iter/s, 4.29377s/10 iters), loss = 8.76916
I0522 22:33:32.294304 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76916 (* 1 = 8.76916 loss)
I0522 22:33:32.374063 34682 sgd_solver.cpp:112] Iteration 5810, lr = 0.01
I0522 22:33:37.068480 34682 solver.cpp:239] Iteration 5820 (2.09469 iter/s, 4.77398s/10 iters), loss = 8.06933
I0522 22:33:37.068539 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06933 (* 1 = 8.06933 loss)
I0522 22:33:37.866027 34682 sgd_solver.cpp:112] Iteration 5820, lr = 0.01
I0522 22:33:42.268143 34682 solver.cpp:239] Iteration 5830 (1.92331 iter/s, 5.19938s/10 iters), loss = 8.69147
I0522 22:33:42.268194 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69147 (* 1 = 8.69147 loss)
I0522 22:33:43.131443 34682 sgd_solver.cpp:112] Iteration 5830, lr = 0.01
I0522 22:33:47.742036 34682 solver.cpp:239] Iteration 5840 (1.82694 iter/s, 5.47362s/10 iters), loss = 8.51368
I0522 22:33:47.742084 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51368 (* 1 = 8.51368 loss)
I0522 22:33:47.832790 34682 sgd_solver.cpp:112] Iteration 5840, lr = 0.01
I0522 22:33:52.473240 34682 solver.cpp:239] Iteration 5850 (2.11374 iter/s, 4.73096s/10 iters), loss = 8.52439
I0522 22:33:52.473290 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52439 (* 1 = 8.52439 loss)
I0522 22:33:53.347271 34682 sgd_solver.cpp:112] Iteration 5850, lr = 0.01
I0522 22:33:56.791405 34682 solver.cpp:239] Iteration 5860 (2.31593 iter/s, 4.31792s/10 iters), loss = 8.68216
I0522 22:33:56.791476 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68216 (* 1 = 8.68216 loss)
I0522 22:33:57.643470 34682 sgd_solver.cpp:112] Iteration 5860, lr = 0.01
I0522 22:34:01.697852 34682 solver.cpp:239] Iteration 5870 (2.03825 iter/s, 4.90617s/10 iters), loss = 9.0856
I0522 22:34:01.697928 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0856 (* 1 = 9.0856 loss)
I0522 22:34:02.495239 34682 sgd_solver.cpp:112] Iteration 5870, lr = 0.01
I0522 22:34:05.906244 34682 solver.cpp:239] Iteration 5880 (2.37634 iter/s, 4.20815s/10 iters), loss = 8.61426
I0522 22:34:05.906291 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61426 (* 1 = 8.61426 loss)
I0522 22:34:06.046097 34682 sgd_solver.cpp:112] Iteration 5880, lr = 0.01
I0522 22:34:10.182126 34682 solver.cpp:239] Iteration 5890 (2.33882 iter/s, 4.27565s/10 iters), loss = 8.65601
I0522 22:34:10.182174 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65601 (* 1 = 8.65601 loss)
I0522 22:34:11.006137 34682 sgd_solver.cpp:112] Iteration 5890, lr = 0.01
I0522 22:34:15.918884 34682 solver.cpp:239] Iteration 5900 (1.74323 iter/s, 5.73647s/10 iters), loss = 8.62844
I0522 22:34:15.918926 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62844 (* 1 = 8.62844 loss)
I0522 22:34:15.993866 34682 sgd_solver.cpp:112] Iteration 5900, lr = 0.01
I0522 22:34:20.876037 34682 solver.cpp:239] Iteration 5910 (2.01739 iter/s, 4.9569s/10 iters), loss = 7.88112
I0522 22:34:20.876093 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88112 (* 1 = 7.88112 loss)
I0522 22:34:21.660590 34682 sgd_solver.cpp:112] Iteration 5910, lr = 0.01
I0522 22:34:27.343003 34682 solver.cpp:239] Iteration 5920 (1.5464 iter/s, 6.46665s/10 iters), loss = 8.17623
I0522 22:34:27.343235 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17623 (* 1 = 8.17623 loss)
I0522 22:34:28.014643 34682 sgd_solver.cpp:112] Iteration 5920, lr = 0.01
I0522 22:34:32.836099 34682 solver.cpp:239] Iteration 5930 (1.82061 iter/s, 5.49266s/10 iters), loss = 8.75089
I0522 22:34:32.836165 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75089 (* 1 = 8.75089 loss)
I0522 22:34:33.663393 34682 sgd_solver.cpp:112] Iteration 5930, lr = 0.01
I0522 22:34:36.242904 34682 solver.cpp:239] Iteration 5940 (2.93548 iter/s, 3.4066s/10 iters), loss = 8.60545
I0522 22:34:36.242945 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60545 (* 1 = 8.60545 loss)
I0522 22:34:36.324136 34682 sgd_solver.cpp:112] Iteration 5940, lr = 0.01
I0522 22:34:41.033365 34682 solver.cpp:239] Iteration 5950 (2.08759 iter/s, 4.79022s/10 iters), loss = 8.61096
I0522 22:34:41.033427 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61096 (* 1 = 8.61096 loss)
I0522 22:34:41.099300 34682 sgd_solver.cpp:112] Iteration 5950, lr = 0.01
I0522 22:34:45.036738 34682 solver.cpp:239] Iteration 5960 (2.49803 iter/s, 4.00315s/10 iters), loss = 9.17819
I0522 22:34:45.036792 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17819 (* 1 = 9.17819 loss)
I0522 22:34:45.873818 34682 sgd_solver.cpp:112] Iteration 5960, lr = 0.01
I0522 22:34:49.972651 34682 solver.cpp:239] Iteration 5970 (2.02607 iter/s, 4.93566s/10 iters), loss = 8.60205
I0522 22:34:49.972697 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60205 (* 1 = 8.60205 loss)
I0522 22:34:50.045395 34682 sgd_solver.cpp:112] Iteration 5970, lr = 0.01
I0522 22:34:56.588335 34682 solver.cpp:239] Iteration 5980 (1.51163 iter/s, 6.61536s/10 iters), loss = 8.38183
I0522 22:34:56.588408 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38183 (* 1 = 8.38183 loss)
I0522 22:34:57.470273 34682 sgd_solver.cpp:112] Iteration 5980, lr = 0.01
I0522 22:35:02.134419 34682 solver.cpp:239] Iteration 5990 (1.80317 iter/s, 5.54578s/10 iters), loss = 8.18382
I0522 22:35:02.134472 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18382 (* 1 = 8.18382 loss)
I0522 22:35:02.207339 34682 sgd_solver.cpp:112] Iteration 5990, lr = 0.01
I0522 22:35:05.110484 34682 solver.cpp:239] Iteration 6000 (3.36035 iter/s, 2.97588s/10 iters), loss = 8.77956
I0522 22:35:05.110559 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77956 (* 1 = 8.77956 loss)
I0522 22:35:05.158507 34682 sgd_solver.cpp:112] Iteration 6000, lr = 0.01
I0522 22:35:06.774513 34682 solver.cpp:239] Iteration 6010 (6.01005 iter/s, 1.66388s/10 iters), loss = 8.00829
I0522 22:35:06.774559 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00829 (* 1 = 8.00829 loss)
I0522 22:35:06.818730 34682 sgd_solver.cpp:112] Iteration 6010, lr = 0.01
I0522 22:35:10.646981 34682 solver.cpp:239] Iteration 6020 (2.58247 iter/s, 3.87226s/10 iters), loss = 8.85522
I0522 22:35:10.647039 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85522 (* 1 = 8.85522 loss)
I0522 22:35:10.721858 34682 sgd_solver.cpp:112] Iteration 6020, lr = 0.01
I0522 22:35:14.048728 34682 solver.cpp:239] Iteration 6030 (2.93984 iter/s, 3.40154s/10 iters), loss = 8.10452
I0522 22:35:14.048781 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10452 (* 1 = 8.10452 loss)
I0522 22:35:14.801928 34682 sgd_solver.cpp:112] Iteration 6030, lr = 0.01
I0522 22:35:18.834570 34682 solver.cpp:239] Iteration 6040 (2.08961 iter/s, 4.78558s/10 iters), loss = 8.28564
I0522 22:35:18.834630 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28564 (* 1 = 8.28564 loss)
I0522 22:35:18.901545 34682 sgd_solver.cpp:112] Iteration 6040, lr = 0.01
I0522 22:35:23.537113 34682 solver.cpp:239] Iteration 6050 (2.12662 iter/s, 4.70229s/10 iters), loss = 9.56584
I0522 22:35:23.537160 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56584 (* 1 = 9.56584 loss)
I0522 22:35:24.302862 34682 sgd_solver.cpp:112] Iteration 6050, lr = 0.01
I0522 22:35:30.697052 34682 solver.cpp:239] Iteration 6060 (1.39673 iter/s, 7.15959s/10 iters), loss = 9.18075
I0522 22:35:30.697340 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18075 (* 1 = 9.18075 loss)
I0522 22:35:31.508028 34682 sgd_solver.cpp:112] Iteration 6060, lr = 0.01
I0522 22:35:35.747910 34682 solver.cpp:239] Iteration 6070 (1.98005 iter/s, 5.05038s/10 iters), loss = 8.62863
I0522 22:35:35.747977 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62863 (* 1 = 8.62863 loss)
I0522 22:35:36.539515 34682 sgd_solver.cpp:112] Iteration 6070, lr = 0.01
I0522 22:35:40.760262 34682 solver.cpp:239] Iteration 6080 (1.99518 iter/s, 5.01209s/10 iters), loss = 9.19976
I0522 22:35:40.760314 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19976 (* 1 = 9.19976 loss)
I0522 22:35:41.594367 34682 sgd_solver.cpp:112] Iteration 6080, lr = 0.01
I0522 22:35:47.599159 34682 solver.cpp:239] Iteration 6090 (1.4623 iter/s, 6.83856s/10 iters), loss = 8.92408
I0522 22:35:47.599205 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92408 (* 1 = 8.92408 loss)
I0522 22:35:47.668568 34682 sgd_solver.cpp:112] Iteration 6090, lr = 0.01
I0522 22:35:52.232607 34682 solver.cpp:239] Iteration 6100 (2.15833 iter/s, 4.63321s/10 iters), loss = 8.40339
I0522 22:35:52.232658 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40339 (* 1 = 8.40339 loss)
I0522 22:35:52.309691 34682 sgd_solver.cpp:112] Iteration 6100, lr = 0.01
I0522 22:35:57.223187 34682 solver.cpp:239] Iteration 6110 (2.00388 iter/s, 4.99032s/10 iters), loss = 8.48682
I0522 22:35:57.223242 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48682 (* 1 = 8.48682 loss)
I0522 22:35:58.037511 34682 sgd_solver.cpp:112] Iteration 6110, lr = 0.01
I0522 22:36:00.509306 34682 solver.cpp:239] Iteration 6120 (3.04328 iter/s, 3.28593s/10 iters), loss = 9.24596
I0522 22:36:00.509346 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24596 (* 1 = 9.24596 loss)
I0522 22:36:00.571494 34682 sgd_solver.cpp:112] Iteration 6120, lr = 0.01
I0522 22:36:06.971132 34682 solver.cpp:239] Iteration 6130 (1.54763 iter/s, 6.46151s/10 iters), loss = 8.34294
I0522 22:36:06.971462 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34294 (* 1 = 8.34294 loss)
I0522 22:36:07.706495 34682 sgd_solver.cpp:112] Iteration 6130, lr = 0.01
I0522 22:36:11.130434 34682 solver.cpp:239] Iteration 6140 (2.40451 iter/s, 4.15885s/10 iters), loss = 8.61245
I0522 22:36:11.130492 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61245 (* 1 = 8.61245 loss)
I0522 22:36:11.260965 34682 sgd_solver.cpp:112] Iteration 6140, lr = 0.01
I0522 22:36:14.427292 34682 solver.cpp:239] Iteration 6150 (3.03337 iter/s, 3.29667s/10 iters), loss = 8.74597
I0522 22:36:14.427333 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74597 (* 1 = 8.74597 loss)
I0522 22:36:14.516024 34682 sgd_solver.cpp:112] Iteration 6150, lr = 0.01
I0522 22:36:19.385121 34682 solver.cpp:239] Iteration 6160 (2.01711 iter/s, 4.95758s/10 iters), loss = 8.29598
I0522 22:36:19.385166 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29598 (* 1 = 8.29598 loss)
I0522 22:36:19.461856 34682 sgd_solver.cpp:112] Iteration 6160, lr = 0.01
I0522 22:36:24.517712 34682 solver.cpp:239] Iteration 6170 (1.94844 iter/s, 5.13232s/10 iters), loss = 8.76037
I0522 22:36:24.517788 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76037 (* 1 = 8.76037 loss)
I0522 22:36:24.581017 34682 sgd_solver.cpp:112] Iteration 6170, lr = 0.01
I0522 22:36:27.812918 34682 solver.cpp:239] Iteration 6180 (3.03492 iter/s, 3.29498s/10 iters), loss = 8.28784
I0522 22:36:27.812978 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28784 (* 1 = 8.28784 loss)
I0522 22:36:28.684345 34682 sgd_solver.cpp:112] Iteration 6180, lr = 0.01
I0522 22:36:33.810942 34682 solver.cpp:239] Iteration 6190 (1.6673 iter/s, 5.99771s/10 iters), loss = 8.20842
I0522 22:36:33.811000 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20842 (* 1 = 8.20842 loss)
I0522 22:36:33.880070 34682 sgd_solver.cpp:112] Iteration 6190, lr = 0.01
I0522 22:36:38.494376 34682 solver.cpp:239] Iteration 6200 (2.1353 iter/s, 4.68319s/10 iters), loss = 8.35028
I0522 22:36:38.494621 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35028 (* 1 = 8.35028 loss)
I0522 22:36:39.302804 34682 sgd_solver.cpp:112] Iteration 6200, lr = 0.01
I0522 22:36:45.468204 34682 solver.cpp:239] Iteration 6210 (1.43404 iter/s, 6.9733s/10 iters), loss = 8.93658
I0522 22:36:45.468292 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93658 (* 1 = 8.93658 loss)
I0522 22:36:45.544037 34682 sgd_solver.cpp:112] Iteration 6210, lr = 0.01
I0522 22:36:50.575359 34682 solver.cpp:239] Iteration 6220 (1.95815 iter/s, 5.10685s/10 iters), loss = 8.36044
I0522 22:36:50.575419 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36044 (* 1 = 8.36044 loss)
I0522 22:36:51.457314 34682 sgd_solver.cpp:112] Iteration 6220, lr = 0.01
I0522 22:36:56.527034 34682 solver.cpp:239] Iteration 6230 (1.68029 iter/s, 5.95137s/10 iters), loss = 8.51023
I0522 22:36:56.527089 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51023 (* 1 = 8.51023 loss)
I0522 22:36:57.388151 34682 sgd_solver.cpp:112] Iteration 6230, lr = 0.01
I0522 22:37:02.305603 34682 solver.cpp:239] Iteration 6240 (1.73062 iter/s, 5.77827s/10 iters), loss = 8.41192
I0522 22:37:02.305677 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41192 (* 1 = 8.41192 loss)
I0522 22:37:03.008489 34682 sgd_solver.cpp:112] Iteration 6240, lr = 0.01
I0522 22:37:07.783699 34682 solver.cpp:239] Iteration 6250 (1.82555 iter/s, 5.4778s/10 iters), loss = 8.2447
I0522 22:37:07.783756 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2447 (* 1 = 8.2447 loss)
I0522 22:37:07.856946 34682 sgd_solver.cpp:112] Iteration 6250, lr = 0.01
I0522 22:37:14.290118 34682 solver.cpp:239] Iteration 6260 (1.53702 iter/s, 6.50609s/10 iters), loss = 8.1072
I0522 22:37:14.290386 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1072 (* 1 = 8.1072 loss)
I0522 22:37:14.353027 34682 sgd_solver.cpp:112] Iteration 6260, lr = 0.01
I0522 22:37:18.628026 34682 solver.cpp:239] Iteration 6270 (2.30548 iter/s, 4.33749s/10 iters), loss = 8.44646
I0522 22:37:18.628079 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44646 (* 1 = 8.44646 loss)
I0522 22:37:19.457918 34682 sgd_solver.cpp:112] Iteration 6270, lr = 0.01
I0522 22:37:23.959949 34682 solver.cpp:239] Iteration 6280 (1.87559 iter/s, 5.33165s/10 iters), loss = 8.58924
I0522 22:37:23.959997 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58924 (* 1 = 8.58924 loss)
I0522 22:37:24.806371 34682 sgd_solver.cpp:112] Iteration 6280, lr = 0.01
I0522 22:37:30.662134 34682 solver.cpp:239] Iteration 6290 (1.49212 iter/s, 6.70186s/10 iters), loss = 9.04293
I0522 22:37:30.662190 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04293 (* 1 = 9.04293 loss)
I0522 22:37:31.421072 34682 sgd_solver.cpp:112] Iteration 6290, lr = 0.01
I0522 22:37:37.101171 34682 solver.cpp:239] Iteration 6300 (1.5531 iter/s, 6.43872s/10 iters), loss = 8.08976
I0522 22:37:37.101233 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08976 (* 1 = 8.08976 loss)
I0522 22:37:37.796666 34682 sgd_solver.cpp:112] Iteration 6300, lr = 0.01
I0522 22:37:42.501636 34682 solver.cpp:239] Iteration 6310 (1.85179 iter/s, 5.40018s/10 iters), loss = 8.60437
I0522 22:37:42.501677 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60437 (* 1 = 8.60437 loss)
I0522 22:37:42.578559 34682 sgd_solver.cpp:112] Iteration 6310, lr = 0.01
I0522 22:37:46.544075 34682 solver.cpp:239] Iteration 6320 (2.47388 iter/s, 4.04223s/10 iters), loss = 8.85781
I0522 22:37:46.544405 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85781 (* 1 = 8.85781 loss)
I0522 22:37:47.362846 34682 sgd_solver.cpp:112] Iteration 6320, lr = 0.01
I0522 22:37:53.451064 34682 solver.cpp:239] Iteration 6330 (1.44793 iter/s, 6.90643s/10 iters), loss = 8.25302
I0522 22:37:53.451128 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25302 (* 1 = 8.25302 loss)
I0522 22:37:53.507861 34682 sgd_solver.cpp:112] Iteration 6330, lr = 0.01
I0522 22:37:57.546720 34682 solver.cpp:239] Iteration 6340 (2.44176 iter/s, 4.09541s/10 iters), loss = 8.55194
I0522 22:37:57.546769 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55194 (* 1 = 8.55194 loss)
I0522 22:37:57.627398 34682 sgd_solver.cpp:112] Iteration 6340, lr = 0.01
I0522 22:38:01.739315 34682 solver.cpp:239] Iteration 6350 (2.38528 iter/s, 4.19238s/10 iters), loss = 9.1258
I0522 22:38:01.739362 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1258 (* 1 = 9.1258 loss)
I0522 22:38:02.510556 34682 sgd_solver.cpp:112] Iteration 6350, lr = 0.01
I0522 22:38:07.627245 34682 solver.cpp:239] Iteration 6360 (1.69847 iter/s, 5.88764s/10 iters), loss = 8.61861
I0522 22:38:07.627295 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61861 (* 1 = 8.61861 loss)
I0522 22:38:08.176853 34682 sgd_solver.cpp:112] Iteration 6360, lr = 0.01
I0522 22:38:11.437746 34682 solver.cpp:239] Iteration 6370 (2.62447 iter/s, 3.81029s/10 iters), loss = 8.92784
I0522 22:38:11.437788 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92784 (* 1 = 8.92784 loss)
I0522 22:38:11.507009 34682 sgd_solver.cpp:112] Iteration 6370, lr = 0.01
I0522 22:38:16.051362 34682 solver.cpp:239] Iteration 6380 (2.16761 iter/s, 4.61337s/10 iters), loss = 8.45931
I0522 22:38:16.051419 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45931 (* 1 = 8.45931 loss)
I0522 22:38:16.120841 34682 sgd_solver.cpp:112] Iteration 6380, lr = 0.01
I0522 22:38:20.698175 34682 solver.cpp:239] Iteration 6390 (2.15213 iter/s, 4.64657s/10 iters), loss = 8.66539
I0522 22:38:20.698457 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66539 (* 1 = 8.66539 loss)
I0522 22:38:20.768152 34682 sgd_solver.cpp:112] Iteration 6390, lr = 0.01
I0522 22:38:24.815043 34682 solver.cpp:239] Iteration 6400 (2.42928 iter/s, 4.11644s/10 iters), loss = 8.17834
I0522 22:38:24.815098 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17834 (* 1 = 8.17834 loss)
I0522 22:38:24.887141 34682 sgd_solver.cpp:112] Iteration 6400, lr = 0.01
I0522 22:38:30.034914 34682 solver.cpp:239] Iteration 6410 (1.91586 iter/s, 5.2196s/10 iters), loss = 8.76621
I0522 22:38:30.034965 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76621 (* 1 = 8.76621 loss)
I0522 22:38:30.803658 34682 sgd_solver.cpp:112] Iteration 6410, lr = 0.01
I0522 22:38:34.818074 34682 solver.cpp:239] Iteration 6420 (2.09078 iter/s, 4.78291s/10 iters), loss = 8.46625
I0522 22:38:34.818126 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46625 (* 1 = 8.46625 loss)
I0522 22:38:34.876966 34682 sgd_solver.cpp:112] Iteration 6420, lr = 0.01
I0522 22:38:40.151298 34682 solver.cpp:239] Iteration 6430 (1.87513 iter/s, 5.33296s/10 iters), loss = 7.95974
I0522 22:38:40.151346 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95974 (* 1 = 7.95974 loss)
I0522 22:38:40.833226 34682 sgd_solver.cpp:112] Iteration 6430, lr = 0.01
I0522 22:38:45.781082 34682 solver.cpp:239] Iteration 6440 (1.77636 iter/s, 5.62949s/10 iters), loss = 8.35884
I0522 22:38:45.781134 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35884 (* 1 = 8.35884 loss)
I0522 22:38:45.845562 34682 sgd_solver.cpp:112] Iteration 6440, lr = 0.01
I0522 22:38:52.120635 34682 solver.cpp:239] Iteration 6450 (1.57748 iter/s, 6.33924s/10 iters), loss = 8.69582
I0522 22:38:52.120945 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69582 (* 1 = 8.69582 loss)
I0522 22:38:52.779496 34682 sgd_solver.cpp:112] Iteration 6450, lr = 0.01
I0522 22:38:56.923854 34682 solver.cpp:239] Iteration 6460 (2.08215 iter/s, 4.80272s/10 iters), loss = 9.12261
I0522 22:38:56.923962 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12261 (* 1 = 9.12261 loss)
I0522 22:38:57.005396 34682 sgd_solver.cpp:112] Iteration 6460, lr = 0.01
I0522 22:39:00.364650 34682 solver.cpp:239] Iteration 6470 (2.90649 iter/s, 3.44057s/10 iters), loss = 8.44978
I0522 22:39:00.364701 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44978 (* 1 = 8.44978 loss)
I0522 22:39:00.427654 34682 sgd_solver.cpp:112] Iteration 6470, lr = 0.01
I0522 22:39:04.584372 34682 solver.cpp:239] Iteration 6480 (2.36995 iter/s, 4.2195s/10 iters), loss = 8.60233
I0522 22:39:04.584420 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60233 (* 1 = 8.60233 loss)
I0522 22:39:04.661248 34682 sgd_solver.cpp:112] Iteration 6480, lr = 0.01
I0522 22:39:08.159808 34682 solver.cpp:239] Iteration 6490 (2.79702 iter/s, 3.57523s/10 iters), loss = 8.5058
I0522 22:39:08.159889 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5058 (* 1 = 8.5058 loss)
I0522 22:39:08.218361 34682 sgd_solver.cpp:112] Iteration 6490, lr = 0.01
I0522 22:39:13.133736 34682 solver.cpp:239] Iteration 6500 (2.01059 iter/s, 4.97366s/10 iters), loss = 8.88503
I0522 22:39:13.133798 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88503 (* 1 = 8.88503 loss)
I0522 22:39:13.993501 34682 sgd_solver.cpp:112] Iteration 6500, lr = 0.01
I0522 22:39:18.283278 34682 solver.cpp:239] Iteration 6510 (1.94202 iter/s, 5.14927s/10 iters), loss = 8.84825
I0522 22:39:18.283331 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84825 (* 1 = 8.84825 loss)
I0522 22:39:18.364872 34682 sgd_solver.cpp:112] Iteration 6510, lr = 0.01
I0522 22:39:24.100102 34682 solver.cpp:239] Iteration 6520 (1.71924 iter/s, 5.81653s/10 iters), loss = 8.64649
I0522 22:39:24.100311 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64649 (* 1 = 8.64649 loss)
I0522 22:39:24.165215 34682 sgd_solver.cpp:112] Iteration 6520, lr = 0.01
I0522 22:39:29.647022 34682 solver.cpp:239] Iteration 6530 (1.80293 iter/s, 5.54652s/10 iters), loss = 8.65963
I0522 22:39:29.647073 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65963 (* 1 = 8.65963 loss)
I0522 22:39:30.418522 34682 sgd_solver.cpp:112] Iteration 6530, lr = 0.01
I0522 22:39:33.812198 34682 solver.cpp:239] Iteration 6540 (2.401 iter/s, 4.16494s/10 iters), loss = 8.46427
I0522 22:39:33.812268 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46427 (* 1 = 8.46427 loss)
I0522 22:39:33.875142 34682 sgd_solver.cpp:112] Iteration 6540, lr = 0.01
I0522 22:39:37.686645 34682 solver.cpp:239] Iteration 6550 (2.58117 iter/s, 3.87421s/10 iters), loss = 8.21039
I0522 22:39:37.686714 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21039 (* 1 = 8.21039 loss)
I0522 22:39:37.768362 34682 sgd_solver.cpp:112] Iteration 6550, lr = 0.01
I0522 22:39:43.067546 34682 solver.cpp:239] Iteration 6560 (1.85852 iter/s, 5.38063s/10 iters), loss = 8.96531
I0522 22:39:43.067591 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96531 (* 1 = 8.96531 loss)
I0522 22:39:43.143309 34682 sgd_solver.cpp:112] Iteration 6560, lr = 0.01
I0522 22:39:47.850452 34682 solver.cpp:239] Iteration 6570 (2.09089 iter/s, 4.78266s/10 iters), loss = 8.81575
I0522 22:39:47.850499 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81575 (* 1 = 8.81575 loss)
I0522 22:39:47.927105 34682 sgd_solver.cpp:112] Iteration 6570, lr = 0.01
I0522 22:39:53.551542 34682 solver.cpp:239] Iteration 6580 (1.75414 iter/s, 5.7008s/10 iters), loss = 8.55361
I0522 22:39:53.551632 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55361 (* 1 = 8.55361 loss)
I0522 22:39:54.348533 34682 sgd_solver.cpp:112] Iteration 6580, lr = 0.01
I0522 22:39:59.917162 34682 solver.cpp:239] Iteration 6590 (1.57102 iter/s, 6.36527s/10 iters), loss = 8.52069
I0522 22:39:59.917237 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52069 (* 1 = 8.52069 loss)
I0522 22:39:59.990530 34682 sgd_solver.cpp:112] Iteration 6590, lr = 0.01
I0522 22:40:03.312444 34682 solver.cpp:239] Iteration 6600 (2.94545 iter/s, 3.39507s/10 iters), loss = 8.56305
I0522 22:40:03.312507 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56305 (* 1 = 8.56305 loss)
I0522 22:40:04.030889 34682 sgd_solver.cpp:112] Iteration 6600, lr = 0.01
I0522 22:40:08.970795 34682 solver.cpp:239] Iteration 6610 (1.76739 iter/s, 5.65806s/10 iters), loss = 9.1542
I0522 22:40:08.970845 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1542 (* 1 = 9.1542 loss)
I0522 22:40:09.047394 34682 sgd_solver.cpp:112] Iteration 6610, lr = 0.01
I0522 22:40:14.502178 34682 solver.cpp:239] Iteration 6620 (1.80796 iter/s, 5.5311s/10 iters), loss = 8.49552
I0522 22:40:14.502224 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49552 (* 1 = 8.49552 loss)
I0522 22:40:14.573843 34682 sgd_solver.cpp:112] Iteration 6620, lr = 0.01
I0522 22:40:18.760609 34682 solver.cpp:239] Iteration 6630 (2.34841 iter/s, 4.2582s/10 iters), loss = 8.13697
I0522 22:40:18.760668 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13697 (* 1 = 8.13697 loss)
I0522 22:40:19.574698 34682 sgd_solver.cpp:112] Iteration 6630, lr = 0.01
I0522 22:40:23.647162 34682 solver.cpp:239] Iteration 6640 (2.04656 iter/s, 4.88625s/10 iters), loss = 9.01609
I0522 22:40:23.647269 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01609 (* 1 = 9.01609 loss)
I0522 22:40:24.459838 34682 sgd_solver.cpp:112] Iteration 6640, lr = 0.01
I0522 22:40:27.851780 34682 solver.cpp:239] Iteration 6650 (2.37849 iter/s, 4.20435s/10 iters), loss = 8.17705
I0522 22:40:27.851827 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17705 (* 1 = 8.17705 loss)
I0522 22:40:27.911355 34682 sgd_solver.cpp:112] Iteration 6650, lr = 0.01
I0522 22:40:31.943529 34682 solver.cpp:239] Iteration 6660 (2.44409 iter/s, 4.09149s/10 iters), loss = 8.9175
I0522 22:40:31.943614 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9175 (* 1 = 8.9175 loss)
I0522 22:40:32.806999 34682 sgd_solver.cpp:112] Iteration 6660, lr = 0.01
I0522 22:40:38.465970 34682 solver.cpp:239] Iteration 6670 (1.53325 iter/s, 6.5221s/10 iters), loss = 8.76086
I0522 22:40:38.466023 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76086 (* 1 = 8.76086 loss)
I0522 22:40:38.542258 34682 sgd_solver.cpp:112] Iteration 6670, lr = 0.01
I0522 22:40:42.134346 34682 solver.cpp:239] Iteration 6680 (2.72616 iter/s, 3.66817s/10 iters), loss = 8.32298
I0522 22:40:42.134394 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32298 (* 1 = 8.32298 loss)
I0522 22:40:42.635330 34682 sgd_solver.cpp:112] Iteration 6680, lr = 0.01
I0522 22:40:46.018585 34682 solver.cpp:239] Iteration 6690 (2.57465 iter/s, 3.88402s/10 iters), loss = 8.89352
I0522 22:40:46.018654 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89352 (* 1 = 8.89352 loss)
I0522 22:40:46.780357 34682 sgd_solver.cpp:112] Iteration 6690, lr = 0.01
I0522 22:40:51.182539 34682 solver.cpp:239] Iteration 6700 (1.93661 iter/s, 5.16367s/10 iters), loss = 8.39952
I0522 22:40:51.182595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39952 (* 1 = 8.39952 loss)
I0522 22:40:51.247056 34682 sgd_solver.cpp:112] Iteration 6700, lr = 0.01
I0522 22:40:57.198488 34682 solver.cpp:239] Iteration 6710 (1.66233 iter/s, 6.01565s/10 iters), loss = 8.84615
I0522 22:40:57.198773 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84615 (* 1 = 8.84615 loss)
I0522 22:40:57.279024 34682 sgd_solver.cpp:112] Iteration 6710, lr = 0.01
I0522 22:41:02.147611 34682 solver.cpp:239] Iteration 6720 (2.02075 iter/s, 4.94865s/10 iters), loss = 7.854
I0522 22:41:02.147687 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.854 (* 1 = 7.854 loss)
I0522 22:41:02.944924 34682 sgd_solver.cpp:112] Iteration 6720, lr = 0.01
I0522 22:41:06.839295 34682 solver.cpp:239] Iteration 6730 (2.13155 iter/s, 4.69142s/10 iters), loss = 9.5059
I0522 22:41:06.839347 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.5059 (* 1 = 9.5059 loss)
I0522 22:41:07.415833 34682 sgd_solver.cpp:112] Iteration 6730, lr = 0.01
I0522 22:41:13.755195 34682 solver.cpp:239] Iteration 6740 (1.44601 iter/s, 6.91557s/10 iters), loss = 8.56201
I0522 22:41:13.755250 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56201 (* 1 = 8.56201 loss)
I0522 22:41:14.533854 34682 sgd_solver.cpp:112] Iteration 6740, lr = 0.01
I0522 22:41:18.517094 34682 solver.cpp:239] Iteration 6750 (2.10011 iter/s, 4.76165s/10 iters), loss = 8.75287
I0522 22:41:18.517146 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75287 (* 1 = 8.75287 loss)
I0522 22:41:18.597944 34682 sgd_solver.cpp:112] Iteration 6750, lr = 0.01
I0522 22:41:24.846771 34682 solver.cpp:239] Iteration 6760 (1.57994 iter/s, 6.32936s/10 iters), loss = 8.78638
I0522 22:41:24.846819 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78638 (* 1 = 8.78638 loss)
I0522 22:41:24.908401 34682 sgd_solver.cpp:112] Iteration 6760, lr = 0.01
I0522 22:41:29.855906 34682 solver.cpp:239] Iteration 6770 (1.99645 iter/s, 5.00888s/10 iters), loss = 8.6457
I0522 22:41:29.856043 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6457 (* 1 = 8.6457 loss)
I0522 22:41:29.928205 34682 sgd_solver.cpp:112] Iteration 6770, lr = 0.01
I0522 22:41:33.215687 34682 solver.cpp:239] Iteration 6780 (2.97663 iter/s, 3.3595s/10 iters), loss = 8.43706
I0522 22:41:33.215737 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43706 (* 1 = 8.43706 loss)
I0522 22:41:34.107101 34682 sgd_solver.cpp:112] Iteration 6780, lr = 0.01
I0522 22:41:36.748962 34682 solver.cpp:239] Iteration 6790 (2.8304 iter/s, 3.53307s/10 iters), loss = 8.49105
I0522 22:41:36.749016 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49105 (* 1 = 8.49105 loss)
I0522 22:41:36.818682 34682 sgd_solver.cpp:112] Iteration 6790, lr = 0.01
I0522 22:41:42.367022 34682 solver.cpp:239] Iteration 6800 (1.78006 iter/s, 5.61778s/10 iters), loss = 8.4066
I0522 22:41:42.367070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4066 (* 1 = 8.4066 loss)
I0522 22:41:42.442760 34682 sgd_solver.cpp:112] Iteration 6800, lr = 0.01
I0522 22:41:46.859838 34682 solver.cpp:239] Iteration 6810 (2.22589 iter/s, 4.49258s/10 iters), loss = 9.04372
I0522 22:41:46.859881 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04372 (* 1 = 9.04372 loss)
I0522 22:41:46.932648 34682 sgd_solver.cpp:112] Iteration 6810, lr = 0.01
I0522 22:41:52.510607 34682 solver.cpp:239] Iteration 6820 (1.76976 iter/s, 5.65048s/10 iters), loss = 8.28929
I0522 22:41:52.510663 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28929 (* 1 = 8.28929 loss)
I0522 22:41:52.570758 34682 sgd_solver.cpp:112] Iteration 6820, lr = 0.01
I0522 22:41:57.839440 34682 solver.cpp:239] Iteration 6830 (1.87668 iter/s, 5.32856s/10 iters), loss = 8.41912
I0522 22:41:57.839498 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41912 (* 1 = 8.41912 loss)
I0522 22:41:58.493609 34682 sgd_solver.cpp:112] Iteration 6830, lr = 0.01
I0522 22:42:04.709262 34682 solver.cpp:239] Iteration 6840 (1.45571 iter/s, 6.86948s/10 iters), loss = 8.92569
I0522 22:42:04.709504 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92569 (* 1 = 8.92569 loss)
I0522 22:42:05.446867 34682 sgd_solver.cpp:112] Iteration 6840, lr = 0.01
I0522 22:42:08.788630 34682 solver.cpp:239] Iteration 6850 (2.45161 iter/s, 4.07896s/10 iters), loss = 8.84903
I0522 22:42:08.788740 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84903 (* 1 = 8.84903 loss)
I0522 22:42:08.860285 34682 sgd_solver.cpp:112] Iteration 6850, lr = 0.01
I0522 22:42:16.680730 34682 solver.cpp:239] Iteration 6860 (1.26716 iter/s, 7.89169s/10 iters), loss = 8.41366
I0522 22:42:16.680775 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41366 (* 1 = 8.41366 loss)
I0522 22:42:16.738405 34682 sgd_solver.cpp:112] Iteration 6860, lr = 0.01
I0522 22:42:21.245160 34682 solver.cpp:239] Iteration 6870 (2.19096 iter/s, 4.56421s/10 iters), loss = 8.02033
I0522 22:42:21.245203 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02033 (* 1 = 8.02033 loss)
I0522 22:42:21.315184 34682 sgd_solver.cpp:112] Iteration 6870, lr = 0.01
I0522 22:42:25.074131 34682 solver.cpp:239] Iteration 6880 (2.61181 iter/s, 3.82876s/10 iters), loss = 8.61564
I0522 22:42:25.074192 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61564 (* 1 = 8.61564 loss)
I0522 22:42:25.154474 34682 sgd_solver.cpp:112] Iteration 6880, lr = 0.01
I0522 22:42:29.272465 34682 solver.cpp:239] Iteration 6890 (2.38203 iter/s, 4.19809s/10 iters), loss = 8.76455
I0522 22:42:29.272519 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76455 (* 1 = 8.76455 loss)
I0522 22:42:29.341605 34682 sgd_solver.cpp:112] Iteration 6890, lr = 0.01
I0522 22:42:35.946148 34682 solver.cpp:239] Iteration 6900 (1.4985 iter/s, 6.67336s/10 iters), loss = 7.76734
I0522 22:42:35.946422 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.76734 (* 1 = 7.76734 loss)
I0522 22:42:36.776542 34682 sgd_solver.cpp:112] Iteration 6900, lr = 0.01
I0522 22:42:44.768903 34682 solver.cpp:239] Iteration 6910 (1.13351 iter/s, 8.82217s/10 iters), loss = 8.62622
I0522 22:42:44.768944 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62622 (* 1 = 8.62622 loss)
I0522 22:42:44.849172 34682 sgd_solver.cpp:112] Iteration 6910, lr = 0.01
I0522 22:42:48.786727 34682 solver.cpp:239] Iteration 6920 (2.48905 iter/s, 4.0176s/10 iters), loss = 9.03478
I0522 22:42:48.786778 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03478 (* 1 = 9.03478 loss)
I0522 22:42:49.511945 34682 sgd_solver.cpp:112] Iteration 6920, lr = 0.01
I0522 22:42:52.004389 34682 solver.cpp:239] Iteration 6930 (3.10804 iter/s, 3.21747s/10 iters), loss = 9.43952
I0522 22:42:52.004436 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43952 (* 1 = 9.43952 loss)
I0522 22:42:52.063321 34682 sgd_solver.cpp:112] Iteration 6930, lr = 0.01
I0522 22:42:56.424221 34682 solver.cpp:239] Iteration 6940 (2.26265 iter/s, 4.4196s/10 iters), loss = 8.57625
I0522 22:42:56.424268 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57625 (* 1 = 8.57625 loss)
I0522 22:42:56.494559 34682 sgd_solver.cpp:112] Iteration 6940, lr = 0.01
I0522 22:43:01.504626 34682 solver.cpp:239] Iteration 6950 (1.96845 iter/s, 5.08015s/10 iters), loss = 8.47415
I0522 22:43:01.504683 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47415 (* 1 = 8.47415 loss)
I0522 22:43:02.108248 34682 sgd_solver.cpp:112] Iteration 6950, lr = 0.01
I0522 22:43:07.000478 34682 solver.cpp:239] Iteration 6960 (1.81965 iter/s, 5.49556s/10 iters), loss = 8.78449
I0522 22:43:07.000780 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78449 (* 1 = 8.78449 loss)
I0522 22:43:07.771471 34682 sgd_solver.cpp:112] Iteration 6960, lr = 0.01
I0522 22:43:12.165436 34682 solver.cpp:239] Iteration 6970 (1.93631 iter/s, 5.16447s/10 iters), loss = 8.42445
I0522 22:43:12.165503 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42445 (* 1 = 8.42445 loss)
I0522 22:43:12.300237 34682 sgd_solver.cpp:112] Iteration 6970, lr = 0.01
I0522 22:43:17.290740 34682 solver.cpp:239] Iteration 6980 (1.95121 iter/s, 5.12502s/10 iters), loss = 8.77198
I0522 22:43:17.290793 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77198 (* 1 = 8.77198 loss)
I0522 22:43:17.975944 34682 sgd_solver.cpp:112] Iteration 6980, lr = 0.01
I0522 22:43:24.351850 34682 solver.cpp:239] Iteration 6990 (1.41628 iter/s, 7.06077s/10 iters), loss = 8.73759
I0522 22:43:24.351905 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73759 (* 1 = 8.73759 loss)
I0522 22:43:25.047649 34682 sgd_solver.cpp:112] Iteration 6990, lr = 0.01
I0522 22:43:28.501096 34682 solver.cpp:239] Iteration 7000 (2.41022 iter/s, 4.14901s/10 iters), loss = 9.35414
I0522 22:43:28.501144 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35414 (* 1 = 9.35414 loss)
I0522 22:43:28.561522 34682 sgd_solver.cpp:112] Iteration 7000, lr = 0.01
I0522 22:43:36.699836 34682 solver.cpp:239] Iteration 7010 (1.21975 iter/s, 8.19837s/10 iters), loss = 8.90707
I0522 22:43:36.699890 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90707 (* 1 = 8.90707 loss)
I0522 22:43:37.375432 34682 sgd_solver.cpp:112] Iteration 7010, lr = 0.01
I0522 22:43:41.557437 34682 solver.cpp:239] Iteration 7020 (2.05874 iter/s, 4.85734s/10 iters), loss = 8.76005
I0522 22:43:41.557498 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76005 (* 1 = 8.76005 loss)
I0522 22:43:41.625474 34682 sgd_solver.cpp:112] Iteration 7020, lr = 0.01
I0522 22:43:46.550457 34682 solver.cpp:239] Iteration 7030 (2.0029 iter/s, 4.99276s/10 iters), loss = 9.02914
I0522 22:43:46.550499 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02914 (* 1 = 9.02914 loss)
I0522 22:43:46.617159 34682 sgd_solver.cpp:112] Iteration 7030, lr = 0.01
I0522 22:43:49.786685 34682 solver.cpp:239] Iteration 7040 (3.0902 iter/s, 3.23603s/10 iters), loss = 9.27861
I0522 22:43:49.786775 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27861 (* 1 = 9.27861 loss)
I0522 22:43:50.396697 34682 sgd_solver.cpp:112] Iteration 7040, lr = 0.01
I0522 22:43:55.776530 34682 solver.cpp:239] Iteration 7050 (1.6696 iter/s, 5.98947s/10 iters), loss = 8.456
I0522 22:43:55.776633 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.456 (* 1 = 8.456 loss)
I0522 22:43:56.652601 34682 sgd_solver.cpp:112] Iteration 7050, lr = 0.01
I0522 22:44:00.850441 34682 solver.cpp:239] Iteration 7060 (1.97098 iter/s, 5.07361s/10 iters), loss = 7.86857
I0522 22:44:00.850486 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86857 (* 1 = 7.86857 loss)
I0522 22:44:00.912185 34682 sgd_solver.cpp:112] Iteration 7060, lr = 0.01
I0522 22:44:04.453629 34682 solver.cpp:239] Iteration 7070 (2.77548 iter/s, 3.60298s/10 iters), loss = 8.48476
I0522 22:44:04.453680 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48476 (* 1 = 8.48476 loss)
I0522 22:44:04.528986 34682 sgd_solver.cpp:112] Iteration 7070, lr = 0.01
I0522 22:44:09.081629 34682 solver.cpp:239] Iteration 7080 (2.16291 iter/s, 4.6234s/10 iters), loss = 8.53709
I0522 22:44:09.081864 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53709 (* 1 = 8.53709 loss)
I0522 22:44:09.153995 34682 sgd_solver.cpp:112] Iteration 7080, lr = 0.01
I0522 22:44:14.088537 34682 solver.cpp:239] Iteration 7090 (1.9974 iter/s, 5.00651s/10 iters), loss = 8.74382
I0522 22:44:14.088579 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74382 (* 1 = 8.74382 loss)
I0522 22:44:14.156630 34682 sgd_solver.cpp:112] Iteration 7090, lr = 0.01
I0522 22:44:18.015456 34682 solver.cpp:239] Iteration 7100 (2.54666 iter/s, 3.92671s/10 iters), loss = 8.61324
I0522 22:44:18.015516 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61324 (* 1 = 8.61324 loss)
I0522 22:44:18.609791 34682 sgd_solver.cpp:112] Iteration 7100, lr = 0.01
I0522 22:44:23.306785 34682 solver.cpp:239] Iteration 7110 (1.88998 iter/s, 5.29105s/10 iters), loss = 7.88168
I0522 22:44:23.306838 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88168 (* 1 = 7.88168 loss)
I0522 22:44:23.382346 34682 sgd_solver.cpp:112] Iteration 7110, lr = 0.01
I0522 22:44:28.148748 34682 solver.cpp:239] Iteration 7120 (2.06539 iter/s, 4.84171s/10 iters), loss = 8.28437
I0522 22:44:28.148802 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28437 (* 1 = 8.28437 loss)
I0522 22:44:28.981062 34682 sgd_solver.cpp:112] Iteration 7120, lr = 0.01
I0522 22:44:32.919229 34682 solver.cpp:239] Iteration 7130 (2.09633 iter/s, 4.77023s/10 iters), loss = 8.25093
I0522 22:44:32.919276 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25093 (* 1 = 8.25093 loss)
I0522 22:44:32.994873 34682 sgd_solver.cpp:112] Iteration 7130, lr = 0.01
I0522 22:44:36.545366 34682 solver.cpp:239] Iteration 7140 (2.75791 iter/s, 3.62593s/10 iters), loss = 8.3685
I0522 22:44:36.545421 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3685 (* 1 = 8.3685 loss)
I0522 22:44:36.620122 34682 sgd_solver.cpp:112] Iteration 7140, lr = 0.01
I0522 22:44:41.951387 34682 solver.cpp:239] Iteration 7150 (1.84988 iter/s, 5.40574s/10 iters), loss = 9.13568
I0522 22:44:41.951648 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13568 (* 1 = 9.13568 loss)
I0522 22:44:42.588482 34682 sgd_solver.cpp:112] Iteration 7150, lr = 0.01
I0522 22:44:46.558068 34682 solver.cpp:239] Iteration 7160 (2.17096 iter/s, 4.60626s/10 iters), loss = 8.81553
I0522 22:44:46.558126 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81553 (* 1 = 8.81553 loss)
I0522 22:44:46.622877 34682 sgd_solver.cpp:112] Iteration 7160, lr = 0.01
I0522 22:44:51.235425 34682 solver.cpp:239] Iteration 7170 (2.13808 iter/s, 4.6771s/10 iters), loss = 8.50558
I0522 22:44:51.235474 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50558 (* 1 = 8.50558 loss)
I0522 22:44:51.293294 34682 sgd_solver.cpp:112] Iteration 7170, lr = 0.01
I0522 22:44:53.855574 34682 solver.cpp:239] Iteration 7180 (3.81681 iter/s, 2.61999s/10 iters), loss = 9.1811
I0522 22:44:53.855621 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1811 (* 1 = 9.1811 loss)
I0522 22:44:53.935765 34682 sgd_solver.cpp:112] Iteration 7180, lr = 0.01
I0522 22:44:58.129287 34682 solver.cpp:239] Iteration 7190 (2.34001 iter/s, 4.27348s/10 iters), loss = 8.46607
I0522 22:44:58.129343 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46607 (* 1 = 8.46607 loss)
I0522 22:44:58.200697 34682 sgd_solver.cpp:112] Iteration 7190, lr = 0.01
I0522 22:45:01.668606 34682 solver.cpp:239] Iteration 7200 (2.82558 iter/s, 3.5391s/10 iters), loss = 8.88632
I0522 22:45:01.668673 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88632 (* 1 = 8.88632 loss)
I0522 22:45:02.503293 34682 sgd_solver.cpp:112] Iteration 7200, lr = 0.01
I0522 22:45:07.358974 34682 solver.cpp:239] Iteration 7210 (1.75745 iter/s, 5.69007s/10 iters), loss = 8.66921
I0522 22:45:07.359017 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66921 (* 1 = 8.66921 loss)
I0522 22:45:07.433009 34682 sgd_solver.cpp:112] Iteration 7210, lr = 0.01
I0522 22:45:11.354343 34682 solver.cpp:239] Iteration 7220 (2.50303 iter/s, 3.99515s/10 iters), loss = 7.85986
I0522 22:45:11.354403 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.85986 (* 1 = 7.85986 loss)
I0522 22:45:11.426173 34682 sgd_solver.cpp:112] Iteration 7220, lr = 0.01
I0522 22:45:14.851131 34682 solver.cpp:239] Iteration 7230 (2.8635 iter/s, 3.49223s/10 iters), loss = 9.13412
I0522 22:45:14.851434 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13412 (* 1 = 9.13412 loss)
I0522 22:45:15.410939 34682 sgd_solver.cpp:112] Iteration 7230, lr = 0.01
I0522 22:45:19.443014 34682 solver.cpp:239] Iteration 7240 (2.17797 iter/s, 4.59142s/10 iters), loss = 8.96235
I0522 22:45:19.443071 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96235 (* 1 = 8.96235 loss)
I0522 22:45:20.245296 34682 sgd_solver.cpp:112] Iteration 7240, lr = 0.01
I0522 22:45:25.804602 34682 solver.cpp:239] Iteration 7250 (1.57201 iter/s, 6.36127s/10 iters), loss = 8.69111
I0522 22:45:25.804666 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69111 (* 1 = 8.69111 loss)
I0522 22:45:25.874207 34682 sgd_solver.cpp:112] Iteration 7250, lr = 0.01
I0522 22:45:29.986038 34682 solver.cpp:239] Iteration 7260 (2.39165 iter/s, 4.18121s/10 iters), loss = 8.6286
I0522 22:45:29.986083 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6286 (* 1 = 8.6286 loss)
I0522 22:45:30.074785 34682 sgd_solver.cpp:112] Iteration 7260, lr = 0.01
I0522 22:45:34.645591 34682 solver.cpp:239] Iteration 7270 (2.14624 iter/s, 4.65931s/10 iters), loss = 9.17446
I0522 22:45:34.645651 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17446 (* 1 = 9.17446 loss)
I0522 22:45:34.715559 34682 sgd_solver.cpp:112] Iteration 7270, lr = 0.01
I0522 22:45:39.086895 34682 solver.cpp:239] Iteration 7280 (2.25171 iter/s, 4.44106s/10 iters), loss = 8.56651
I0522 22:45:39.086946 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56651 (* 1 = 8.56651 loss)
I0522 22:45:39.151666 34682 sgd_solver.cpp:112] Iteration 7280, lr = 0.01
I0522 22:45:42.953933 34682 solver.cpp:239] Iteration 7290 (2.5861 iter/s, 3.86683s/10 iters), loss = 8.71
I0522 22:45:42.953974 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71 (* 1 = 8.71 loss)
I0522 22:45:43.022778 34682 sgd_solver.cpp:112] Iteration 7290, lr = 0.01
I0522 22:45:47.871615 34682 solver.cpp:239] Iteration 7300 (2.03358 iter/s, 4.91743s/10 iters), loss = 8.87449
I0522 22:45:47.871898 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87449 (* 1 = 8.87449 loss)
I0522 22:45:47.939745 34682 sgd_solver.cpp:112] Iteration 7300, lr = 0.01
I0522 22:45:52.086954 34682 solver.cpp:239] Iteration 7310 (2.37252 iter/s, 4.21493s/10 iters), loss = 8.65421
I0522 22:45:52.087009 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65421 (* 1 = 8.65421 loss)
I0522 22:45:52.618096 34682 sgd_solver.cpp:112] Iteration 7310, lr = 0.01
I0522 22:45:55.337015 34682 solver.cpp:239] Iteration 7320 (3.07704 iter/s, 3.24987s/10 iters), loss = 8.98876
I0522 22:45:55.337062 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98876 (* 1 = 8.98876 loss)
I0522 22:45:55.400926 34682 sgd_solver.cpp:112] Iteration 7320, lr = 0.01
I0522 22:46:00.076835 34682 solver.cpp:239] Iteration 7330 (2.1099 iter/s, 4.73957s/10 iters), loss = 9.60913
I0522 22:46:00.076902 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60913 (* 1 = 9.60913 loss)
I0522 22:46:00.134114 34682 sgd_solver.cpp:112] Iteration 7330, lr = 0.01
I0522 22:46:05.787247 34682 solver.cpp:239] Iteration 7340 (1.75128 iter/s, 5.71012s/10 iters), loss = 8.57066
I0522 22:46:05.787295 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57066 (* 1 = 8.57066 loss)
I0522 22:46:05.862612 34682 sgd_solver.cpp:112] Iteration 7340, lr = 0.01
I0522 22:46:09.288641 34682 solver.cpp:239] Iteration 7350 (2.85617 iter/s, 3.50119s/10 iters), loss = 8.77617
I0522 22:46:09.288698 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77617 (* 1 = 8.77617 loss)
I0522 22:46:09.931057 34682 sgd_solver.cpp:112] Iteration 7350, lr = 0.01
I0522 22:46:14.586453 34682 solver.cpp:239] Iteration 7360 (1.88767 iter/s, 5.29754s/10 iters), loss = 8.6489
I0522 22:46:14.586494 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6489 (* 1 = 8.6489 loss)
I0522 22:46:14.649055 34682 sgd_solver.cpp:112] Iteration 7360, lr = 0.01
I0522 22:46:19.889209 34682 solver.cpp:239] Iteration 7370 (1.8859 iter/s, 5.3025s/10 iters), loss = 8.61122
I0522 22:46:19.889503 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61122 (* 1 = 8.61122 loss)
I0522 22:46:19.953722 34682 sgd_solver.cpp:112] Iteration 7370, lr = 0.01
I0522 22:46:24.381163 34682 solver.cpp:239] Iteration 7380 (2.22642 iter/s, 4.49151s/10 iters), loss = 8.1548
I0522 22:46:24.381222 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1548 (* 1 = 8.1548 loss)
I0522 22:46:24.516683 34682 sgd_solver.cpp:112] Iteration 7380, lr = 0.01
I0522 22:46:28.537206 34682 solver.cpp:239] Iteration 7390 (2.40627 iter/s, 4.15581s/10 iters), loss = 8.74811
I0522 22:46:28.537261 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74811 (* 1 = 8.74811 loss)
I0522 22:46:29.371068 34682 sgd_solver.cpp:112] Iteration 7390, lr = 0.01
I0522 22:46:33.875851 34682 solver.cpp:239] Iteration 7400 (1.87323 iter/s, 5.33837s/10 iters), loss = 8.74931
I0522 22:46:33.875905 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74931 (* 1 = 8.74931 loss)
I0522 22:46:33.943814 34682 sgd_solver.cpp:112] Iteration 7400, lr = 0.01
I0522 22:46:38.198424 34682 solver.cpp:239] Iteration 7410 (2.31356 iter/s, 4.32235s/10 iters), loss = 8.65768
I0522 22:46:38.198470 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65768 (* 1 = 8.65768 loss)
I0522 22:46:38.263268 34682 sgd_solver.cpp:112] Iteration 7410, lr = 0.01
I0522 22:46:42.050330 34682 solver.cpp:239] Iteration 7420 (2.59626 iter/s, 3.8517s/10 iters), loss = 8.57813
I0522 22:46:42.050376 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57813 (* 1 = 8.57813 loss)
I0522 22:46:42.118178 34682 sgd_solver.cpp:112] Iteration 7420, lr = 0.01
I0522 22:46:46.951457 34682 solver.cpp:239] Iteration 7430 (2.04045 iter/s, 4.90087s/10 iters), loss = 8.15449
I0522 22:46:46.951517 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15449 (* 1 = 8.15449 loss)
I0522 22:46:47.024669 34682 sgd_solver.cpp:112] Iteration 7430, lr = 0.01
I0522 22:46:52.064616 34682 solver.cpp:239] Iteration 7440 (1.95584 iter/s, 5.11289s/10 iters), loss = 9.02071
I0522 22:46:52.064867 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02071 (* 1 = 9.02071 loss)
I0522 22:46:52.129993 34682 sgd_solver.cpp:112] Iteration 7440, lr = 0.01
I0522 22:46:56.190779 34682 solver.cpp:239] Iteration 7450 (2.42378 iter/s, 4.12578s/10 iters), loss = 8.45338
I0522 22:46:56.190819 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45338 (* 1 = 8.45338 loss)
I0522 22:46:56.263056 34682 sgd_solver.cpp:112] Iteration 7450, lr = 0.01
I0522 22:46:59.980916 34682 solver.cpp:239] Iteration 7460 (2.63858 iter/s, 3.78992s/10 iters), loss = 9.31906
I0522 22:46:59.980975 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31906 (* 1 = 9.31906 loss)
I0522 22:47:00.048151 34682 sgd_solver.cpp:112] Iteration 7460, lr = 0.01
I0522 22:47:06.276443 34682 solver.cpp:239] Iteration 7470 (1.58851 iter/s, 6.2952s/10 iters), loss = 8.34661
I0522 22:47:06.276499 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34661 (* 1 = 8.34661 loss)
I0522 22:47:06.534952 34682 sgd_solver.cpp:112] Iteration 7470, lr = 0.01
I0522 22:47:12.090315 34682 solver.cpp:239] Iteration 7480 (1.72011 iter/s, 5.81357s/10 iters), loss = 9.12532
I0522 22:47:12.090386 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12532 (* 1 = 9.12532 loss)
I0522 22:47:12.987226 34682 sgd_solver.cpp:112] Iteration 7480, lr = 0.01
I0522 22:47:17.938769 34682 solver.cpp:239] Iteration 7490 (1.70995 iter/s, 5.84814s/10 iters), loss = 9.11531
I0522 22:47:17.938835 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11531 (* 1 = 9.11531 loss)
I0522 22:47:18.651770 34682 sgd_solver.cpp:112] Iteration 7490, lr = 0.01
I0522 22:47:22.719946 34682 solver.cpp:239] Iteration 7500 (2.09165 iter/s, 4.78091s/10 iters), loss = 8.88769
I0522 22:47:22.720178 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88769 (* 1 = 8.88769 loss)
I0522 22:47:22.795941 34682 sgd_solver.cpp:112] Iteration 7500, lr = 0.01
I0522 22:47:26.850819 34682 solver.cpp:239] Iteration 7510 (2.42103 iter/s, 4.13047s/10 iters), loss = 8.26832
I0522 22:47:26.850888 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26832 (* 1 = 8.26832 loss)
I0522 22:47:27.692610 34682 sgd_solver.cpp:112] Iteration 7510, lr = 0.01
I0522 22:47:31.010901 34682 solver.cpp:239] Iteration 7520 (2.40394 iter/s, 4.15984s/10 iters), loss = 9.4415
I0522 22:47:31.010960 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4415 (* 1 = 9.4415 loss)
I0522 22:47:31.091037 34682 sgd_solver.cpp:112] Iteration 7520, lr = 0.01
I0522 22:47:36.544641 34682 solver.cpp:239] Iteration 7530 (1.80719 iter/s, 5.53345s/10 iters), loss = 8.36342
I0522 22:47:36.544698 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36342 (* 1 = 8.36342 loss)
I0522 22:47:36.604478 34682 sgd_solver.cpp:112] Iteration 7530, lr = 0.01
I0522 22:47:40.601351 34682 solver.cpp:239] Iteration 7540 (2.4652 iter/s, 4.05647s/10 iters), loss = 9.33138
I0522 22:47:40.601438 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33138 (* 1 = 9.33138 loss)
I0522 22:47:41.388134 34682 sgd_solver.cpp:112] Iteration 7540, lr = 0.01
I0522 22:47:46.331084 34682 solver.cpp:239] Iteration 7550 (1.74538 iter/s, 5.72942s/10 iters), loss = 9.32364
I0522 22:47:46.331140 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32364 (* 1 = 9.32364 loss)
I0522 22:47:46.973721 34682 sgd_solver.cpp:112] Iteration 7550, lr = 0.01
I0522 22:47:51.167430 34682 solver.cpp:239] Iteration 7560 (2.06778 iter/s, 4.8361s/10 iters), loss = 9.10665
I0522 22:47:51.167479 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10665 (* 1 = 9.10665 loss)
I0522 22:47:51.228057 34682 sgd_solver.cpp:112] Iteration 7560, lr = 0.01
I0522 22:47:59.167838 34682 solver.cpp:239] Iteration 7570 (1.24999 iter/s, 8.00004s/10 iters), loss = 8.59916
I0522 22:47:59.168107 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59916 (* 1 = 8.59916 loss)
I0522 22:47:59.234064 34682 sgd_solver.cpp:112] Iteration 7570, lr = 0.01
I0522 22:48:04.909019 34682 solver.cpp:239] Iteration 7580 (1.74327 iter/s, 5.73636s/10 iters), loss = 8.74824
I0522 22:48:04.909063 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74824 (* 1 = 8.74824 loss)
I0522 22:48:04.978827 34682 sgd_solver.cpp:112] Iteration 7580, lr = 0.01
I0522 22:48:09.521652 34682 solver.cpp:239] Iteration 7590 (2.16808 iter/s, 4.61238s/10 iters), loss = 8.37516
I0522 22:48:09.521718 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37516 (* 1 = 8.37516 loss)
I0522 22:48:09.629631 34682 sgd_solver.cpp:112] Iteration 7590, lr = 0.01
I0522 22:48:13.642526 34682 solver.cpp:239] Iteration 7600 (2.42681 iter/s, 4.12064s/10 iters), loss = 8.44169
I0522 22:48:13.642588 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44169 (* 1 = 8.44169 loss)
I0522 22:48:13.712707 34682 sgd_solver.cpp:112] Iteration 7600, lr = 0.01
I0522 22:48:19.161406 34682 solver.cpp:239] Iteration 7610 (1.81206 iter/s, 5.51857s/10 iters), loss = 9.03504
I0522 22:48:19.161478 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03504 (* 1 = 9.03504 loss)
I0522 22:48:20.064255 34682 sgd_solver.cpp:112] Iteration 7610, lr = 0.01
I0522 22:48:24.656276 34682 solver.cpp:239] Iteration 7620 (1.81999 iter/s, 5.49455s/10 iters), loss = 8.56899
I0522 22:48:24.656329 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56899 (* 1 = 8.56899 loss)
I0522 22:48:24.729987 34682 sgd_solver.cpp:112] Iteration 7620, lr = 0.01
I0522 22:48:28.085100 34682 solver.cpp:239] Iteration 7630 (2.91661 iter/s, 3.42863s/10 iters), loss = 9.12983
I0522 22:48:28.085158 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12983 (* 1 = 9.12983 loss)
I0522 22:48:28.950448 34682 sgd_solver.cpp:112] Iteration 7630, lr = 0.01
I0522 22:48:35.272554 34682 solver.cpp:239] Iteration 7640 (1.39138 iter/s, 7.18711s/10 iters), loss = 9.05283
I0522 22:48:35.272846 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05283 (* 1 = 9.05283 loss)
I0522 22:48:35.335894 34682 sgd_solver.cpp:112] Iteration 7640, lr = 0.01
I0522 22:48:39.881567 34682 solver.cpp:239] Iteration 7650 (2.16987 iter/s, 4.60857s/10 iters), loss = 8.62779
I0522 22:48:39.881615 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62779 (* 1 = 8.62779 loss)
I0522 22:48:40.707068 34682 sgd_solver.cpp:112] Iteration 7650, lr = 0.01
I0522 22:48:45.342821 34682 solver.cpp:239] Iteration 7660 (1.83117 iter/s, 5.46098s/10 iters), loss = 9.60972
I0522 22:48:45.342876 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60972 (* 1 = 9.60972 loss)
I0522 22:48:45.415076 34682 sgd_solver.cpp:112] Iteration 7660, lr = 0.01
I0522 22:48:50.399461 34682 solver.cpp:239] Iteration 7670 (1.9777 iter/s, 5.05638s/10 iters), loss = 9.13352
I0522 22:48:50.399513 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13352 (* 1 = 9.13352 loss)
I0522 22:48:51.222980 34682 sgd_solver.cpp:112] Iteration 7670, lr = 0.01
I0522 22:48:56.758723 34682 solver.cpp:239] Iteration 7680 (1.57259 iter/s, 6.35893s/10 iters), loss = 8.98939
I0522 22:48:56.758769 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98939 (* 1 = 8.98939 loss)
I0522 22:48:57.594154 34682 sgd_solver.cpp:112] Iteration 7680, lr = 0.01
I0522 22:49:01.408610 34682 solver.cpp:239] Iteration 7690 (2.1507 iter/s, 4.64965s/10 iters), loss = 8.87627
I0522 22:49:01.408653 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87627 (* 1 = 8.87627 loss)
I0522 22:49:01.471364 34682 sgd_solver.cpp:112] Iteration 7690, lr = 0.01
I0522 22:49:06.286242 34682 solver.cpp:239] Iteration 7700 (2.05028 iter/s, 4.87739s/10 iters), loss = 8.46269
I0522 22:49:06.286502 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46269 (* 1 = 8.46269 loss)
I0522 22:49:07.118556 34682 sgd_solver.cpp:112] Iteration 7700, lr = 0.01
I0522 22:49:11.233564 34682 solver.cpp:239] Iteration 7710 (2.02147 iter/s, 4.94689s/10 iters), loss = 8.31085
I0522 22:49:11.233613 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31085 (* 1 = 8.31085 loss)
I0522 22:49:11.306749 34682 sgd_solver.cpp:112] Iteration 7710, lr = 0.01
I0522 22:49:16.013631 34682 solver.cpp:239] Iteration 7720 (2.09213 iter/s, 4.77981s/10 iters), loss = 8.47959
I0522 22:49:16.013680 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47959 (* 1 = 8.47959 loss)
I0522 22:49:16.858129 34682 sgd_solver.cpp:112] Iteration 7720, lr = 0.01
I0522 22:49:21.037775 34682 solver.cpp:239] Iteration 7730 (1.99051 iter/s, 5.02384s/10 iters), loss = 8.43507
I0522 22:49:21.037845 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43507 (* 1 = 8.43507 loss)
I0522 22:49:21.110011 34682 sgd_solver.cpp:112] Iteration 7730, lr = 0.01
I0522 22:49:24.659970 34682 solver.cpp:239] Iteration 7740 (2.76092 iter/s, 3.62198s/10 iters), loss = 9.16804
I0522 22:49:24.660030 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16804 (* 1 = 9.16804 loss)
I0522 22:49:25.427898 34682 sgd_solver.cpp:112] Iteration 7740, lr = 0.01
I0522 22:49:31.013028 34682 solver.cpp:239] Iteration 7750 (1.57413 iter/s, 6.35272s/10 iters), loss = 8.66127
I0522 22:49:31.013097 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66127 (* 1 = 8.66127 loss)
I0522 22:49:31.816906 34682 sgd_solver.cpp:112] Iteration 7750, lr = 0.01
I0522 22:49:35.946494 34682 solver.cpp:239] Iteration 7760 (2.02708 iter/s, 4.93321s/10 iters), loss = 8.20727
I0522 22:49:35.946547 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20727 (* 1 = 8.20727 loss)
I0522 22:49:36.696579 34682 sgd_solver.cpp:112] Iteration 7760, lr = 0.01
I0522 22:49:40.352047 34682 solver.cpp:239] Iteration 7770 (2.26998 iter/s, 4.40532s/10 iters), loss = 8.054
I0522 22:49:40.352092 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.054 (* 1 = 8.054 loss)
I0522 22:49:40.440734 34682 sgd_solver.cpp:112] Iteration 7770, lr = 0.01
I0522 22:49:44.012760 34682 solver.cpp:239] Iteration 7780 (2.73186 iter/s, 3.66051s/10 iters), loss = 8.84792
I0522 22:49:44.012817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84792 (* 1 = 8.84792 loss)
I0522 22:49:44.085850 34682 sgd_solver.cpp:112] Iteration 7780, lr = 0.01
I0522 22:49:49.522123 34682 solver.cpp:239] Iteration 7790 (1.81519 iter/s, 5.50908s/10 iters), loss = 8.39879
I0522 22:49:49.522182 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39879 (* 1 = 8.39879 loss)
I0522 22:49:50.153111 34682 sgd_solver.cpp:112] Iteration 7790, lr = 0.01
I0522 22:49:53.403081 34682 solver.cpp:239] Iteration 7800 (2.57684 iter/s, 3.88073s/10 iters), loss = 9.10481
I0522 22:49:53.403151 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10481 (* 1 = 9.10481 loss)
I0522 22:49:53.477638 34682 sgd_solver.cpp:112] Iteration 7800, lr = 0.01
I0522 22:49:59.000942 34682 solver.cpp:239] Iteration 7810 (1.78649 iter/s, 5.59755s/10 iters), loss = 9.19281
I0522 22:49:59.001008 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19281 (* 1 = 9.19281 loss)
I0522 22:49:59.684821 34682 sgd_solver.cpp:112] Iteration 7810, lr = 0.01
I0522 22:50:05.366667 34682 solver.cpp:239] Iteration 7820 (1.57099 iter/s, 6.3654s/10 iters), loss = 8.35755
I0522 22:50:05.366741 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35755 (* 1 = 8.35755 loss)
I0522 22:50:05.424660 34682 sgd_solver.cpp:112] Iteration 7820, lr = 0.01
I0522 22:50:09.679875 34682 solver.cpp:239] Iteration 7830 (2.3186 iter/s, 4.31295s/10 iters), loss = 8.94665
I0522 22:50:09.680176 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94665 (* 1 = 8.94665 loss)
I0522 22:50:09.742056 34682 sgd_solver.cpp:112] Iteration 7830, lr = 0.01
I0522 22:50:14.548439 34682 solver.cpp:239] Iteration 7840 (2.05419 iter/s, 4.8681s/10 iters), loss = 9.0047
I0522 22:50:14.548494 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0047 (* 1 = 9.0047 loss)
I0522 22:50:15.442972 34682 sgd_solver.cpp:112] Iteration 7840, lr = 0.01
I0522 22:50:21.038662 34682 solver.cpp:239] Iteration 7850 (1.54086 iter/s, 6.4899s/10 iters), loss = 9.10744
I0522 22:50:21.038749 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10744 (* 1 = 9.10744 loss)
I0522 22:50:21.809919 34682 sgd_solver.cpp:112] Iteration 7850, lr = 0.01
I0522 22:50:26.109911 34682 solver.cpp:239] Iteration 7860 (1.97201 iter/s, 5.07096s/10 iters), loss = 8.12008
I0522 22:50:26.109962 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12008 (* 1 = 8.12008 loss)
I0522 22:50:26.171356 34682 sgd_solver.cpp:112] Iteration 7860, lr = 0.01
I0522 22:50:31.001387 34682 solver.cpp:239] Iteration 7870 (2.04448 iter/s, 4.89122s/10 iters), loss = 9.14729
I0522 22:50:31.001443 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14729 (* 1 = 9.14729 loss)
I0522 22:50:31.763314 34682 sgd_solver.cpp:112] Iteration 7870, lr = 0.01
I0522 22:50:36.520817 34682 solver.cpp:239] Iteration 7880 (1.81187 iter/s, 5.51915s/10 iters), loss = 8.91124
I0522 22:50:36.520884 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91124 (* 1 = 8.91124 loss)
I0522 22:50:36.581140 34682 sgd_solver.cpp:112] Iteration 7880, lr = 0.01
I0522 22:50:40.585317 34682 solver.cpp:239] Iteration 7890 (2.46047 iter/s, 4.06426s/10 iters), loss = 8.27207
I0522 22:50:40.585587 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27207 (* 1 = 8.27207 loss)
I0522 22:50:40.661590 34682 sgd_solver.cpp:112] Iteration 7890, lr = 0.01
I0522 22:50:47.063288 34682 solver.cpp:239] Iteration 7900 (1.54382 iter/s, 6.47742s/10 iters), loss = 9.13703
I0522 22:50:47.063524 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13703 (* 1 = 9.13703 loss)
I0522 22:50:47.720916 34682 sgd_solver.cpp:112] Iteration 7900, lr = 0.01
I0522 22:50:52.562778 34682 solver.cpp:239] Iteration 7910 (1.81849 iter/s, 5.49907s/10 iters), loss = 9.50853
I0522 22:50:52.562872 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.50853 (* 1 = 9.50853 loss)
I0522 22:50:52.625007 34682 sgd_solver.cpp:112] Iteration 7910, lr = 0.01
I0522 22:50:56.694999 34682 solver.cpp:239] Iteration 7920 (2.42015 iter/s, 4.13197s/10 iters), loss = 9.36357
I0522 22:50:56.695044 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36357 (* 1 = 9.36357 loss)
I0522 22:50:56.766767 34682 sgd_solver.cpp:112] Iteration 7920, lr = 0.01
I0522 22:51:01.002825 34682 solver.cpp:239] Iteration 7930 (2.32148 iter/s, 4.30759s/10 iters), loss = 8.50476
I0522 22:51:01.002895 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50476 (* 1 = 8.50476 loss)
I0522 22:51:01.779011 34682 sgd_solver.cpp:112] Iteration 7930, lr = 0.01
I0522 22:51:05.492238 34682 solver.cpp:239] Iteration 7940 (2.22759 iter/s, 4.48915s/10 iters), loss = 9.0882
I0522 22:51:05.492311 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0882 (* 1 = 9.0882 loss)
I0522 22:51:06.297657 34682 sgd_solver.cpp:112] Iteration 7940, lr = 0.01
I0522 22:51:12.229116 34682 solver.cpp:239] Iteration 7950 (1.48444 iter/s, 6.73653s/10 iters), loss = 9.19463
I0522 22:51:12.229404 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19463 (* 1 = 9.19463 loss)
I0522 22:51:12.869889 34682 sgd_solver.cpp:112] Iteration 7950, lr = 0.01
I0522 22:51:16.071638 34682 solver.cpp:239] Iteration 7960 (2.60274 iter/s, 3.8421s/10 iters), loss = 8.56836
I0522 22:51:16.071688 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56836 (* 1 = 8.56836 loss)
I0522 22:51:16.131732 34682 sgd_solver.cpp:112] Iteration 7960, lr = 0.01
I0522 22:51:21.676954 34682 solver.cpp:239] Iteration 7970 (1.78411 iter/s, 5.60504s/10 iters), loss = 8.8464
I0522 22:51:21.677006 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8464 (* 1 = 8.8464 loss)
I0522 22:51:22.297946 34682 sgd_solver.cpp:112] Iteration 7970, lr = 0.01
I0522 22:51:28.203115 34682 solver.cpp:239] Iteration 7980 (1.53237 iter/s, 6.52585s/10 iters), loss = 9.29466
I0522 22:51:28.203160 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29466 (* 1 = 9.29466 loss)
I0522 22:51:29.057246 34682 sgd_solver.cpp:112] Iteration 7980, lr = 0.01
I0522 22:51:35.372997 34682 solver.cpp:239] Iteration 7990 (1.39479 iter/s, 7.16954s/10 iters), loss = 8.91244
I0522 22:51:35.373080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91244 (* 1 = 8.91244 loss)
I0522 22:51:36.119233 34682 sgd_solver.cpp:112] Iteration 7990, lr = 0.01
I0522 22:51:41.302006 34682 solver.cpp:239] Iteration 8000 (1.68671 iter/s, 5.92869s/10 iters), loss = 9.35108
I0522 22:51:41.302058 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35108 (* 1 = 9.35108 loss)
I0522 22:51:42.144727 34682 sgd_solver.cpp:112] Iteration 8000, lr = 0.01
I0522 22:51:46.118930 34682 solver.cpp:239] Iteration 8010 (2.07612 iter/s, 4.81668s/10 iters), loss = 9.18023
I0522 22:51:46.119204 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18023 (* 1 = 9.18023 loss)
I0522 22:51:46.825867 34682 sgd_solver.cpp:112] Iteration 8010, lr = 0.01
I0522 22:51:51.657368 34682 solver.cpp:239] Iteration 8020 (1.80572 iter/s, 5.53797s/10 iters), loss = 8.87125
I0522 22:51:51.657433 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87125 (* 1 = 8.87125 loss)
I0522 22:51:51.719597 34682 sgd_solver.cpp:112] Iteration 8020, lr = 0.01
I0522 22:51:57.402369 34682 solver.cpp:239] Iteration 8030 (1.74073 iter/s, 5.7447s/10 iters), loss = 8.51479
I0522 22:51:57.402431 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51479 (* 1 = 8.51479 loss)
I0522 22:51:57.476881 34682 sgd_solver.cpp:112] Iteration 8030, lr = 0.01
I0522 22:52:02.384059 34682 solver.cpp:239] Iteration 8040 (2.00746 iter/s, 4.98142s/10 iters), loss = 8.60439
I0522 22:52:02.384107 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60439 (* 1 = 8.60439 loss)
I0522 22:52:02.470491 34682 sgd_solver.cpp:112] Iteration 8040, lr = 0.01
I0522 22:52:05.862308 34682 solver.cpp:239] Iteration 8050 (2.87517 iter/s, 3.47806s/10 iters), loss = 8.87508
I0522 22:52:05.862357 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87508 (* 1 = 8.87508 loss)
I0522 22:52:06.627743 34682 sgd_solver.cpp:112] Iteration 8050, lr = 0.01
I0522 22:52:10.098443 34682 solver.cpp:239] Iteration 8060 (2.36077 iter/s, 4.23591s/10 iters), loss = 8.50955
I0522 22:52:10.098500 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50955 (* 1 = 8.50955 loss)
I0522 22:52:10.427754 34682 sgd_solver.cpp:112] Iteration 8060, lr = 0.01
I0522 22:52:15.338026 34682 solver.cpp:239] Iteration 8070 (1.90865 iter/s, 5.2393s/10 iters), loss = 8.41997
I0522 22:52:15.338088 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41997 (* 1 = 8.41997 loss)
I0522 22:52:16.165552 34682 sgd_solver.cpp:112] Iteration 8070, lr = 0.01
I0522 22:52:20.340967 34682 solver.cpp:239] Iteration 8080 (1.99893 iter/s, 5.00267s/10 iters), loss = 8.41838
I0522 22:52:20.341014 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41838 (* 1 = 8.41838 loss)
I0522 22:52:20.409098 34682 sgd_solver.cpp:112] Iteration 8080, lr = 0.01
I0522 22:52:26.456766 34682 solver.cpp:239] Iteration 8090 (1.63519 iter/s, 6.11551s/10 iters), loss = 9.43329
I0522 22:52:26.456809 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43329 (* 1 = 9.43329 loss)
I0522 22:52:27.230458 34682 sgd_solver.cpp:112] Iteration 8090, lr = 0.01
I0522 22:52:31.276139 34682 solver.cpp:239] Iteration 8100 (2.07506 iter/s, 4.81913s/10 iters), loss = 8.91937
I0522 22:52:31.276192 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91937 (* 1 = 8.91937 loss)
I0522 22:52:31.340214 34682 sgd_solver.cpp:112] Iteration 8100, lr = 0.01
I0522 22:52:35.784045 34682 solver.cpp:239] Iteration 8110 (2.21846 iter/s, 4.50763s/10 iters), loss = 8.98453
I0522 22:52:35.784091 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98453 (* 1 = 8.98453 loss)
I0522 22:52:35.857843 34682 sgd_solver.cpp:112] Iteration 8110, lr = 0.01
I0522 22:52:40.904029 34682 solver.cpp:239] Iteration 8120 (1.95323 iter/s, 5.11973s/10 iters), loss = 8.27339
I0522 22:52:40.904079 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27339 (* 1 = 8.27339 loss)
I0522 22:52:40.976817 34682 sgd_solver.cpp:112] Iteration 8120, lr = 0.01
I0522 22:52:46.359803 34682 solver.cpp:239] Iteration 8130 (1.83301 iter/s, 5.4555s/10 iters), loss = 9.61803
I0522 22:52:46.360049 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.61803 (* 1 = 9.61803 loss)
I0522 22:52:47.188810 34682 sgd_solver.cpp:112] Iteration 8130, lr = 0.01
I0522 22:52:52.155036 34682 solver.cpp:239] Iteration 8140 (1.72569 iter/s, 5.79478s/10 iters), loss = 9.07756
I0522 22:52:52.155084 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07756 (* 1 = 9.07756 loss)
I0522 22:52:52.222237 34682 sgd_solver.cpp:112] Iteration 8140, lr = 0.01
I0522 22:52:56.812005 34682 solver.cpp:239] Iteration 8150 (2.14743 iter/s, 4.65672s/10 iters), loss = 8.70725
I0522 22:52:56.812079 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70725 (* 1 = 8.70725 loss)
I0522 22:52:57.556812 34682 sgd_solver.cpp:112] Iteration 8150, lr = 0.01
I0522 22:53:01.956113 34682 solver.cpp:239] Iteration 8160 (1.94408 iter/s, 5.14382s/10 iters), loss = 9.22584
I0522 22:53:01.956174 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22584 (* 1 = 9.22584 loss)
I0522 22:53:02.743963 34682 sgd_solver.cpp:112] Iteration 8160, lr = 0.01
I0522 22:53:07.259588 34682 solver.cpp:239] Iteration 8170 (1.88566 iter/s, 5.30319s/10 iters), loss = 8.69974
I0522 22:53:07.259632 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69974 (* 1 = 8.69974 loss)
I0522 22:53:07.332758 34682 sgd_solver.cpp:112] Iteration 8170, lr = 0.01
I0522 22:53:12.786442 34682 solver.cpp:239] Iteration 8180 (1.80944 iter/s, 5.52657s/10 iters), loss = 9.06975
I0522 22:53:12.786522 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06975 (* 1 = 9.06975 loss)
I0522 22:53:13.579131 34682 sgd_solver.cpp:112] Iteration 8180, lr = 0.01
I0522 22:53:17.094002 34682 solver.cpp:239] Iteration 8190 (2.32163 iter/s, 4.30731s/10 iters), loss = 9.33652
I0522 22:53:17.094323 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33652 (* 1 = 9.33652 loss)
I0522 22:53:17.922734 34682 sgd_solver.cpp:112] Iteration 8190, lr = 0.01
I0522 22:53:22.231525 34682 solver.cpp:239] Iteration 8200 (1.94665 iter/s, 5.13703s/10 iters), loss = 9.43201
I0522 22:53:22.231570 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43201 (* 1 = 9.43201 loss)
I0522 22:53:22.297333 34682 sgd_solver.cpp:112] Iteration 8200, lr = 0.01
I0522 22:53:27.158012 34682 solver.cpp:239] Iteration 8210 (2.02995 iter/s, 4.92623s/10 iters), loss = 8.89902
I0522 22:53:27.158069 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89902 (* 1 = 8.89902 loss)
I0522 22:53:27.938606 34682 sgd_solver.cpp:112] Iteration 8210, lr = 0.01
I0522 22:53:33.087276 34682 solver.cpp:239] Iteration 8220 (1.68663 iter/s, 5.92897s/10 iters), loss = 9.31483
I0522 22:53:33.087333 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31483 (* 1 = 9.31483 loss)
I0522 22:53:33.914000 34682 sgd_solver.cpp:112] Iteration 8220, lr = 0.01
I0522 22:53:41.192718 34682 solver.cpp:239] Iteration 8230 (1.2338 iter/s, 8.10506s/10 iters), loss = 8.7704
I0522 22:53:41.192772 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7704 (* 1 = 8.7704 loss)
I0522 22:53:41.273399 34682 sgd_solver.cpp:112] Iteration 8230, lr = 0.01
I0522 22:53:45.196326 34682 solver.cpp:239] Iteration 8240 (2.49788 iter/s, 4.00339s/10 iters), loss = 8.93314
I0522 22:53:45.196378 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93314 (* 1 = 8.93314 loss)
I0522 22:53:45.241338 34682 sgd_solver.cpp:112] Iteration 8240, lr = 0.01
I0522 22:53:49.193858 34682 solver.cpp:239] Iteration 8250 (2.50168 iter/s, 3.99731s/10 iters), loss = 8.38139
I0522 22:53:49.194114 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38139 (* 1 = 8.38139 loss)
I0522 22:53:50.043131 34682 sgd_solver.cpp:112] Iteration 8250, lr = 0.01
I0522 22:53:55.413301 34682 solver.cpp:239] Iteration 8260 (1.60798 iter/s, 6.21897s/10 iters), loss = 8.56649
I0522 22:53:55.413362 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56649 (* 1 = 8.56649 loss)
I0522 22:53:56.253001 34682 sgd_solver.cpp:112] Iteration 8260, lr = 0.01
I0522 22:54:00.758821 34682 solver.cpp:239] Iteration 8270 (1.87083 iter/s, 5.34523s/10 iters), loss = 9.15165
I0522 22:54:00.758882 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15165 (* 1 = 9.15165 loss)
I0522 22:54:00.828830 34682 sgd_solver.cpp:112] Iteration 8270, lr = 0.01
I0522 22:54:04.834589 34682 solver.cpp:239] Iteration 8280 (2.45366 iter/s, 4.07554s/10 iters), loss = 8.94522
I0522 22:54:04.834646 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94522 (* 1 = 8.94522 loss)
I0522 22:54:05.642453 34682 sgd_solver.cpp:112] Iteration 8280, lr = 0.01
I0522 22:54:08.942157 34682 solver.cpp:239] Iteration 8290 (2.43467 iter/s, 4.10733s/10 iters), loss = 9.42119
I0522 22:54:08.942219 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42119 (* 1 = 9.42119 loss)
I0522 22:54:09.766608 34682 sgd_solver.cpp:112] Iteration 8290, lr = 0.01
I0522 22:54:13.943200 34682 solver.cpp:239] Iteration 8300 (1.99969 iter/s, 5.00077s/10 iters), loss = 9.00535
I0522 22:54:13.943248 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00535 (* 1 = 9.00535 loss)
I0522 22:54:14.024394 34682 sgd_solver.cpp:112] Iteration 8300, lr = 0.01
I0522 22:54:18.175855 34682 solver.cpp:239] Iteration 8310 (2.36271 iter/s, 4.23242s/10 iters), loss = 8.45414
I0522 22:54:18.175932 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45414 (* 1 = 8.45414 loss)
I0522 22:54:18.241186 34682 sgd_solver.cpp:112] Iteration 8310, lr = 0.01
I0522 22:54:21.906247 34682 solver.cpp:239] Iteration 8320 (2.68084 iter/s, 3.73017s/10 iters), loss = 9.11679
I0522 22:54:21.906493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11679 (* 1 = 9.11679 loss)
I0522 22:54:21.967198 34682 sgd_solver.cpp:112] Iteration 8320, lr = 0.01
I0522 22:54:27.476681 34682 solver.cpp:239] Iteration 8330 (1.79533 iter/s, 5.57s/10 iters), loss = 8.67984
I0522 22:54:27.476728 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67984 (* 1 = 8.67984 loss)
I0522 22:54:27.540616 34682 sgd_solver.cpp:112] Iteration 8330, lr = 0.01
I0522 22:54:31.226110 34682 solver.cpp:239] Iteration 8340 (2.66721 iter/s, 3.74923s/10 iters), loss = 9.13529
I0522 22:54:31.226150 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13529 (* 1 = 9.13529 loss)
I0522 22:54:31.299005 34682 sgd_solver.cpp:112] Iteration 8340, lr = 0.01
I0522 22:54:34.677366 34682 solver.cpp:239] Iteration 8350 (2.89766 iter/s, 3.45106s/10 iters), loss = 8.65092
I0522 22:54:34.677423 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65092 (* 1 = 8.65092 loss)
I0522 22:54:35.541690 34682 sgd_solver.cpp:112] Iteration 8350, lr = 0.01
I0522 22:54:41.201843 34682 solver.cpp:239] Iteration 8360 (1.53277 iter/s, 6.52415s/10 iters), loss = 8.702
I0522 22:54:41.201907 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.702 (* 1 = 8.702 loss)
I0522 22:54:41.900352 34682 sgd_solver.cpp:112] Iteration 8360, lr = 0.01
I0522 22:54:47.854111 34682 solver.cpp:239] Iteration 8370 (1.50332 iter/s, 6.65193s/10 iters), loss = 8.28587
I0522 22:54:47.854173 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28587 (* 1 = 8.28587 loss)
I0522 22:54:47.918723 34682 sgd_solver.cpp:112] Iteration 8370, lr = 0.01
I0522 22:54:52.036859 34682 solver.cpp:239] Iteration 8380 (2.39091 iter/s, 4.18251s/10 iters), loss = 9.29158
I0522 22:54:52.037168 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29158 (* 1 = 9.29158 loss)
I0522 22:54:52.099148 34682 sgd_solver.cpp:112] Iteration 8380, lr = 0.01
I0522 22:54:57.144884 34682 solver.cpp:239] Iteration 8390 (1.95788 iter/s, 5.10756s/10 iters), loss = 8.27508
I0522 22:54:57.144940 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27508 (* 1 = 8.27508 loss)
I0522 22:54:57.209012 34682 sgd_solver.cpp:112] Iteration 8390, lr = 0.01
I0522 22:55:01.991868 34682 solver.cpp:239] Iteration 8400 (2.06325 iter/s, 4.84673s/10 iters), loss = 9.30966
I0522 22:55:01.991919 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30966 (* 1 = 9.30966 loss)
I0522 22:55:02.054184 34682 sgd_solver.cpp:112] Iteration 8400, lr = 0.01
I0522 22:55:04.658977 34682 solver.cpp:239] Iteration 8410 (3.7496 iter/s, 2.66695s/10 iters), loss = 8.77024
I0522 22:55:04.659020 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77024 (* 1 = 8.77024 loss)
I0522 22:55:04.724167 34682 sgd_solver.cpp:112] Iteration 8410, lr = 0.01
I0522 22:55:07.711377 34682 solver.cpp:239] Iteration 8420 (3.2763 iter/s, 3.05222s/10 iters), loss = 8.6604
I0522 22:55:07.711421 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6604 (* 1 = 8.6604 loss)
I0522 22:55:08.561164 34682 sgd_solver.cpp:112] Iteration 8420, lr = 0.01
I0522 22:55:13.520229 34682 solver.cpp:239] Iteration 8430 (1.72159 iter/s, 5.80858s/10 iters), loss = 8.2908
I0522 22:55:13.520275 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2908 (* 1 = 8.2908 loss)
I0522 22:55:13.582754 34682 sgd_solver.cpp:112] Iteration 8430, lr = 0.01
I0522 22:55:17.559013 34682 solver.cpp:239] Iteration 8440 (2.47613 iter/s, 4.03856s/10 iters), loss = 8.20748
I0522 22:55:17.559075 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20748 (* 1 = 8.20748 loss)
I0522 22:55:17.633116 34682 sgd_solver.cpp:112] Iteration 8440, lr = 0.01
I0522 22:55:22.583405 34682 solver.cpp:239] Iteration 8450 (1.9904 iter/s, 5.02411s/10 iters), loss = 8.16413
I0522 22:55:22.583629 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16413 (* 1 = 8.16413 loss)
I0522 22:55:23.443073 34682 sgd_solver.cpp:112] Iteration 8450, lr = 0.01
I0522 22:55:26.748126 34682 solver.cpp:239] Iteration 8460 (2.40134 iter/s, 4.16435s/10 iters), loss = 9.04421
I0522 22:55:26.748184 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04421 (* 1 = 9.04421 loss)
I0522 22:55:27.525045 34682 sgd_solver.cpp:112] Iteration 8460, lr = 0.01
I0522 22:55:33.235102 34682 solver.cpp:239] Iteration 8470 (1.54163 iter/s, 6.48666s/10 iters), loss = 9.2735
I0522 22:55:33.235139 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2735 (* 1 = 9.2735 loss)
I0522 22:55:33.308075 34682 sgd_solver.cpp:112] Iteration 8470, lr = 0.01
I0522 22:55:37.077750 34682 solver.cpp:239] Iteration 8480 (2.60251 iter/s, 3.84245s/10 iters), loss = 9.42558
I0522 22:55:37.077798 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42558 (* 1 = 9.42558 loss)
I0522 22:55:37.137197 34682 sgd_solver.cpp:112] Iteration 8480, lr = 0.01
I0522 22:55:41.294615 34682 solver.cpp:239] Iteration 8490 (2.37155 iter/s, 4.21665s/10 iters), loss = 8.89499
I0522 22:55:41.294652 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89499 (* 1 = 8.89499 loss)
I0522 22:55:41.365123 34682 sgd_solver.cpp:112] Iteration 8490, lr = 0.01
I0522 22:55:44.713477 34682 solver.cpp:239] Iteration 8500 (2.92512 iter/s, 3.41867s/10 iters), loss = 8.48259
I0522 22:55:44.713534 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48259 (* 1 = 8.48259 loss)
I0522 22:55:45.381716 34682 sgd_solver.cpp:112] Iteration 8500, lr = 0.01
I0522 22:55:50.243130 34682 solver.cpp:239] Iteration 8510 (1.80852 iter/s, 5.52937s/10 iters), loss = 8.67914
I0522 22:55:50.243187 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67914 (* 1 = 8.67914 loss)
I0522 22:55:51.098199 34682 sgd_solver.cpp:112] Iteration 8510, lr = 0.01
I0522 22:55:56.371908 34682 solver.cpp:239] Iteration 8520 (1.63173 iter/s, 6.12847s/10 iters), loss = 9.04539
I0522 22:55:56.372092 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04539 (* 1 = 9.04539 loss)
I0522 22:55:57.141376 34682 sgd_solver.cpp:112] Iteration 8520, lr = 0.01
I0522 22:56:01.335772 34682 solver.cpp:239] Iteration 8530 (2.01472 iter/s, 4.96347s/10 iters), loss = 8.38292
I0522 22:56:01.335837 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38292 (* 1 = 8.38292 loss)
I0522 22:56:02.126435 34682 sgd_solver.cpp:112] Iteration 8530, lr = 0.01
I0522 22:56:04.891953 34682 solver.cpp:239] Iteration 8540 (2.81218 iter/s, 3.55596s/10 iters), loss = 9.26172
I0522 22:56:04.892007 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.26172 (* 1 = 9.26172 loss)
I0522 22:56:05.640446 34682 sgd_solver.cpp:112] Iteration 8540, lr = 0.01
I0522 22:56:09.027063 34682 solver.cpp:239] Iteration 8550 (2.41845 iter/s, 4.13488s/10 iters), loss = 9.33957
I0522 22:56:09.027128 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33957 (* 1 = 9.33957 loss)
I0522 22:56:09.885638 34682 sgd_solver.cpp:112] Iteration 8550, lr = 0.01
I0522 22:56:13.431363 34682 solver.cpp:239] Iteration 8560 (2.27063 iter/s, 4.40406s/10 iters), loss = 8.43301
I0522 22:56:13.431418 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43301 (* 1 = 8.43301 loss)
I0522 22:56:13.505156 34682 sgd_solver.cpp:112] Iteration 8560, lr = 0.01
I0522 22:56:16.895846 34682 solver.cpp:239] Iteration 8570 (2.8866 iter/s, 3.46428s/10 iters), loss = 8.72757
I0522 22:56:16.895895 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72757 (* 1 = 8.72757 loss)
I0522 22:56:17.758591 34682 sgd_solver.cpp:112] Iteration 8570, lr = 0.01
I0522 22:56:21.936085 34682 solver.cpp:239] Iteration 8580 (1.98413 iter/s, 5.03998s/10 iters), loss = 8.8838
I0522 22:56:21.936142 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8838 (* 1 = 8.8838 loss)
I0522 22:56:22.756462 34682 sgd_solver.cpp:112] Iteration 8580, lr = 0.01
I0522 22:56:27.480394 34682 solver.cpp:239] Iteration 8590 (1.80374 iter/s, 5.54403s/10 iters), loss = 8.52599
I0522 22:56:27.480711 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52599 (* 1 = 8.52599 loss)
I0522 22:56:27.544103 34682 sgd_solver.cpp:112] Iteration 8590, lr = 0.01
I0522 22:56:32.195264 34682 solver.cpp:239] Iteration 8600 (2.12115 iter/s, 4.71441s/10 iters), loss = 9.61316
I0522 22:56:32.195322 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.61316 (* 1 = 9.61316 loss)
I0522 22:56:32.268314 34682 sgd_solver.cpp:112] Iteration 8600, lr = 0.01
I0522 22:56:37.178831 34682 solver.cpp:239] Iteration 8610 (2.0067 iter/s, 4.9833s/10 iters), loss = 8.75722
I0522 22:56:37.178880 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75722 (* 1 = 8.75722 loss)
I0522 22:56:37.251080 34682 sgd_solver.cpp:112] Iteration 8610, lr = 0.01
I0522 22:56:40.222815 34682 solver.cpp:239] Iteration 8620 (3.28537 iter/s, 3.0438s/10 iters), loss = 8.969
I0522 22:56:40.222874 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.969 (* 1 = 8.969 loss)
I0522 22:56:41.050065 34682 sgd_solver.cpp:112] Iteration 8620, lr = 0.01
I0522 22:56:46.442456 34682 solver.cpp:239] Iteration 8630 (1.60789 iter/s, 6.21932s/10 iters), loss = 9.12586
I0522 22:56:46.442517 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12586 (* 1 = 9.12586 loss)
I0522 22:56:47.135936 34682 sgd_solver.cpp:112] Iteration 8630, lr = 0.01
I0522 22:56:51.982221 34682 solver.cpp:239] Iteration 8640 (1.80522 iter/s, 5.53948s/10 iters), loss = 8.51577
I0522 22:56:51.982282 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51577 (* 1 = 8.51577 loss)
I0522 22:56:52.050755 34682 sgd_solver.cpp:112] Iteration 8640, lr = 0.01
I0522 22:56:56.857893 34682 solver.cpp:239] Iteration 8650 (2.05111 iter/s, 4.87541s/10 iters), loss = 8.56979
I0522 22:56:56.857947 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56979 (* 1 = 8.56979 loss)
I0522 22:56:56.920501 34682 sgd_solver.cpp:112] Iteration 8650, lr = 0.01
I0522 22:57:02.336179 34682 solver.cpp:239] Iteration 8660 (1.82548 iter/s, 5.47801s/10 iters), loss = 8.78069
I0522 22:57:02.336494 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78069 (* 1 = 8.78069 loss)
I0522 22:57:03.049598 34682 sgd_solver.cpp:112] Iteration 8660, lr = 0.01
I0522 22:57:07.966745 34682 solver.cpp:239] Iteration 8670 (1.77619 iter/s, 5.63003s/10 iters), loss = 9.17073
I0522 22:57:07.966821 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17073 (* 1 = 9.17073 loss)
I0522 22:57:08.796185 34682 sgd_solver.cpp:112] Iteration 8670, lr = 0.01
I0522 22:57:12.098587 34682 solver.cpp:239] Iteration 8680 (2.42037 iter/s, 4.1316s/10 iters), loss = 9.0597
I0522 22:57:12.098635 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0597 (* 1 = 9.0597 loss)
I0522 22:57:12.178113 34682 sgd_solver.cpp:112] Iteration 8680, lr = 0.01
I0522 22:57:15.475896 34682 solver.cpp:239] Iteration 8690 (2.96111 iter/s, 3.37712s/10 iters), loss = 9.60886
I0522 22:57:15.475952 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60886 (* 1 = 9.60886 loss)
I0522 22:57:15.549929 34682 sgd_solver.cpp:112] Iteration 8690, lr = 0.01
I0522 22:57:20.147753 34682 solver.cpp:239] Iteration 8700 (2.14059 iter/s, 4.6716s/10 iters), loss = 9.15543
I0522 22:57:20.147812 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15543 (* 1 = 9.15543 loss)
I0522 22:57:20.925225 34682 sgd_solver.cpp:112] Iteration 8700, lr = 0.01
I0522 22:57:25.133347 34682 solver.cpp:239] Iteration 8710 (2.00589 iter/s, 4.98533s/10 iters), loss = 8.83266
I0522 22:57:25.133399 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83266 (* 1 = 8.83266 loss)
I0522 22:57:25.798215 34682 sgd_solver.cpp:112] Iteration 8710, lr = 0.01
I0522 22:57:29.173054 34682 solver.cpp:239] Iteration 8720 (2.47556 iter/s, 4.03949s/10 iters), loss = 9.04954
I0522 22:57:29.173115 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04954 (* 1 = 9.04954 loss)
I0522 22:57:29.964179 34682 sgd_solver.cpp:112] Iteration 8720, lr = 0.01
I0522 22:57:33.993150 34682 solver.cpp:239] Iteration 8730 (2.07476 iter/s, 4.81983s/10 iters), loss = 9.51266
I0522 22:57:33.993396 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51266 (* 1 = 9.51266 loss)
I0522 22:57:34.051520 34682 sgd_solver.cpp:112] Iteration 8730, lr = 0.01
I0522 22:57:38.167376 34682 solver.cpp:239] Iteration 8740 (2.39587 iter/s, 4.17384s/10 iters), loss = 9.41464
I0522 22:57:38.167426 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.41464 (* 1 = 9.41464 loss)
I0522 22:57:38.237012 34682 sgd_solver.cpp:112] Iteration 8740, lr = 0.01
I0522 22:57:42.307472 34682 solver.cpp:239] Iteration 8750 (2.41553 iter/s, 4.13988s/10 iters), loss = 9.38831
I0522 22:57:42.307521 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.38831 (* 1 = 9.38831 loss)
I0522 22:57:43.096716 34682 sgd_solver.cpp:112] Iteration 8750, lr = 0.01
I0522 22:57:47.139050 34682 solver.cpp:239] Iteration 8760 (2.06982 iter/s, 4.83133s/10 iters), loss = 9.14988
I0522 22:57:47.139094 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14988 (* 1 = 9.14988 loss)
I0522 22:57:47.227483 34682 sgd_solver.cpp:112] Iteration 8760, lr = 0.01
I0522 22:57:51.250862 34682 solver.cpp:239] Iteration 8770 (2.43214 iter/s, 4.1116s/10 iters), loss = 9.58985
I0522 22:57:51.250903 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.58985 (* 1 = 9.58985 loss)
I0522 22:57:51.309763 34682 sgd_solver.cpp:112] Iteration 8770, lr = 0.01
I0522 22:57:55.894975 34682 solver.cpp:239] Iteration 8780 (2.15339 iter/s, 4.64385s/10 iters), loss = 9.10686
I0522 22:57:55.895040 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10686 (* 1 = 9.10686 loss)
I0522 22:57:56.787107 34682 sgd_solver.cpp:112] Iteration 8780, lr = 0.01
I0522 22:57:58.845029 34682 solver.cpp:239] Iteration 8790 (3.39 iter/s, 2.94985s/10 iters), loss = 8.48889
I0522 22:57:58.845108 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48889 (* 1 = 8.48889 loss)
I0522 22:57:58.902798 34682 sgd_solver.cpp:112] Iteration 8790, lr = 0.01
I0522 22:58:03.715262 34682 solver.cpp:239] Iteration 8800 (2.0534 iter/s, 4.86996s/10 iters), loss = 9.74899
I0522 22:58:03.715312 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.74899 (* 1 = 9.74899 loss)
I0522 22:58:04.582123 34682 sgd_solver.cpp:112] Iteration 8800, lr = 0.01
I0522 22:58:08.317160 34682 solver.cpp:239] Iteration 8810 (2.17313 iter/s, 4.60165s/10 iters), loss = 8.50332
I0522 22:58:08.317205 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50332 (* 1 = 8.50332 loss)
I0522 22:58:08.393489 34682 sgd_solver.cpp:112] Iteration 8810, lr = 0.01
I0522 22:58:11.747918 34682 solver.cpp:239] Iteration 8820 (2.91874 iter/s, 3.42613s/10 iters), loss = 8.33995
I0522 22:58:11.747961 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33995 (* 1 = 8.33995 loss)
I0522 22:58:11.831610 34682 sgd_solver.cpp:112] Iteration 8820, lr = 0.01
I0522 22:58:17.292767 34682 solver.cpp:239] Iteration 8830 (1.80357 iter/s, 5.54457s/10 iters), loss = 9.23831
I0522 22:58:17.292810 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23831 (* 1 = 9.23831 loss)
I0522 22:58:17.372673 34682 sgd_solver.cpp:112] Iteration 8830, lr = 0.01
I0522 22:58:22.384590 34682 solver.cpp:239] Iteration 8840 (1.96403 iter/s, 5.09157s/10 iters), loss = 8.90932
I0522 22:58:22.384635 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90932 (* 1 = 8.90932 loss)
I0522 22:58:22.461721 34682 sgd_solver.cpp:112] Iteration 8840, lr = 0.01
I0522 22:58:27.766539 34682 solver.cpp:239] Iteration 8850 (1.85816 iter/s, 5.38168s/10 iters), loss = 9.02378
I0522 22:58:27.766588 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02378 (* 1 = 9.02378 loss)
I0522 22:58:27.830024 34682 sgd_solver.cpp:112] Iteration 8850, lr = 0.01
I0522 22:58:32.651517 34682 solver.cpp:239] Iteration 8860 (2.0472 iter/s, 4.88472s/10 iters), loss = 8.44668
I0522 22:58:32.651577 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44668 (* 1 = 8.44668 loss)
I0522 22:58:32.721503 34682 sgd_solver.cpp:112] Iteration 8860, lr = 0.01
I0522 22:58:35.494922 34682 solver.cpp:239] Iteration 8870 (3.51714 iter/s, 2.84322s/10 iters), loss = 8.69963
I0522 22:58:35.495065 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69963 (* 1 = 8.69963 loss)
I0522 22:58:35.578912 34682 sgd_solver.cpp:112] Iteration 8870, lr = 0.01
I0522 22:58:41.610730 34682 solver.cpp:239] Iteration 8880 (1.6364 iter/s, 6.11098s/10 iters), loss = 9.15932
I0522 22:58:41.610787 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15932 (* 1 = 9.15932 loss)
I0522 22:58:42.179020 34682 sgd_solver.cpp:112] Iteration 8880, lr = 0.01
I0522 22:58:48.557404 34682 solver.cpp:239] Iteration 8890 (1.43961 iter/s, 6.94634s/10 iters), loss = 9.03697
I0522 22:58:48.557445 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03697 (* 1 = 9.03697 loss)
I0522 22:58:48.630743 34682 sgd_solver.cpp:112] Iteration 8890, lr = 0.01
I0522 22:58:52.549665 34682 solver.cpp:239] Iteration 8900 (2.50498 iter/s, 3.99205s/10 iters), loss = 8.61205
I0522 22:58:52.549712 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61205 (* 1 = 8.61205 loss)
I0522 22:58:52.618358 34682 sgd_solver.cpp:112] Iteration 8900, lr = 0.01
I0522 22:58:54.525789 34682 solver.cpp:239] Iteration 8910 (5.06075 iter/s, 1.97599s/10 iters), loss = 8.83708
I0522 22:58:54.525831 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83708 (* 1 = 8.83708 loss)
I0522 22:58:55.118918 34682 sgd_solver.cpp:112] Iteration 8910, lr = 0.01
I0522 22:58:58.367318 34682 solver.cpp:239] Iteration 8920 (2.60327 iter/s, 3.84132s/10 iters), loss = 9.06651
I0522 22:58:58.367379 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06651 (* 1 = 9.06651 loss)
I0522 22:58:59.191114 34682 sgd_solver.cpp:112] Iteration 8920, lr = 0.01
I0522 22:59:03.998687 34682 solver.cpp:239] Iteration 8930 (1.77586 iter/s, 5.63108s/10 iters), loss = 9.42984
I0522 22:59:03.998762 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42984 (* 1 = 9.42984 loss)
I0522 22:59:04.822180 34682 sgd_solver.cpp:112] Iteration 8930, lr = 0.01
I0522 22:59:10.557698 34682 solver.cpp:239] Iteration 8940 (1.5247 iter/s, 6.55867s/10 iters), loss = 9.17699
I0522 22:59:10.557968 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17699 (* 1 = 9.17699 loss)
I0522 22:59:11.387882 34682 sgd_solver.cpp:112] Iteration 8940, lr = 0.01
I0522 22:59:15.315193 34682 solver.cpp:239] Iteration 8950 (2.10214 iter/s, 4.75705s/10 iters), loss = 8.88072
I0522 22:59:15.315253 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88072 (* 1 = 8.88072 loss)
I0522 22:59:16.131152 34682 sgd_solver.cpp:112] Iteration 8950, lr = 0.01
I0522 22:59:19.914221 34682 solver.cpp:239] Iteration 8960 (2.17449 iter/s, 4.59878s/10 iters), loss = 8.357
I0522 22:59:19.914271 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.357 (* 1 = 8.357 loss)
I0522 22:59:19.984167 34682 sgd_solver.cpp:112] Iteration 8960, lr = 0.01
I0522 22:59:23.275780 34682 solver.cpp:239] Iteration 8970 (2.97498 iter/s, 3.36136s/10 iters), loss = 9.3919
I0522 22:59:23.275837 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3919 (* 1 = 9.3919 loss)
I0522 22:59:23.332433 34682 sgd_solver.cpp:112] Iteration 8970, lr = 0.01
I0522 22:59:28.728919 34682 solver.cpp:239] Iteration 8980 (1.8339 iter/s, 5.45286s/10 iters), loss = 8.78556
I0522 22:59:28.728973 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78556 (* 1 = 8.78556 loss)
I0522 22:59:29.614886 34682 sgd_solver.cpp:112] Iteration 8980, lr = 0.01
I0522 22:59:34.566411 34682 solver.cpp:239] Iteration 8990 (1.71315 iter/s, 5.8372s/10 iters), loss = 8.15911
I0522 22:59:34.566454 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15911 (* 1 = 8.15911 loss)
I0522 22:59:34.638645 34682 sgd_solver.cpp:112] Iteration 8990, lr = 0.01
I0522 22:59:38.493551 34682 solver.cpp:239] Iteration 9000 (2.54652 iter/s, 3.92693s/10 iters), loss = 8.67884
I0522 22:59:38.493599 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67884 (* 1 = 8.67884 loss)
I0522 22:59:38.553782 34682 sgd_solver.cpp:112] Iteration 9000, lr = 0.01
I0522 22:59:44.744678 34682 solver.cpp:239] Iteration 9010 (1.59979 iter/s, 6.2508s/10 iters), loss = 8.82564
I0522 22:59:44.744994 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82564 (* 1 = 8.82564 loss)
I0522 22:59:45.385938 34682 sgd_solver.cpp:112] Iteration 9010, lr = 0.01
I0522 22:59:50.236519 34682 solver.cpp:239] Iteration 9020 (1.82105 iter/s, 5.49133s/10 iters), loss = 9.10051
I0522 22:59:50.236583 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10051 (* 1 = 9.10051 loss)
I0522 22:59:50.997653 34682 sgd_solver.cpp:112] Iteration 9020, lr = 0.01
I0522 22:59:54.402989 34682 solver.cpp:239] Iteration 9030 (2.40025 iter/s, 4.16624s/10 iters), loss = 8.82428
I0522 22:59:54.403036 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82428 (* 1 = 8.82428 loss)
I0522 22:59:55.028304 34682 sgd_solver.cpp:112] Iteration 9030, lr = 0.01
I0522 22:59:58.201438 34682 solver.cpp:239] Iteration 9040 (2.63281 iter/s, 3.79822s/10 iters), loss = 8.97777
I0522 22:59:58.201545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97777 (* 1 = 8.97777 loss)
I0522 22:59:58.263872 34682 sgd_solver.cpp:112] Iteration 9040, lr = 0.01
I0522 23:00:02.441426 34682 solver.cpp:239] Iteration 9050 (2.35865 iter/s, 4.23972s/10 iters), loss = 9.2939
I0522 23:00:02.441481 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2939 (* 1 = 9.2939 loss)
I0522 23:00:03.200014 34682 sgd_solver.cpp:112] Iteration 9050, lr = 0.01
I0522 23:00:06.539865 34682 solver.cpp:239] Iteration 9060 (2.44009 iter/s, 4.09821s/10 iters), loss = 9.76718
I0522 23:00:06.539903 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.76718 (* 1 = 9.76718 loss)
I0522 23:00:06.603193 34682 sgd_solver.cpp:112] Iteration 9060, lr = 0.01
I0522 23:00:10.852557 34682 solver.cpp:239] Iteration 9070 (2.31886 iter/s, 4.31246s/10 iters), loss = 8.94895
I0522 23:00:10.852619 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94895 (* 1 = 8.94895 loss)
I0522 23:00:11.683961 34682 sgd_solver.cpp:112] Iteration 9070, lr = 0.01
I0522 23:00:17.536823 34682 solver.cpp:239] Iteration 9080 (1.49612 iter/s, 6.68394s/10 iters), loss = 8.99341
I0522 23:00:17.537153 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99341 (* 1 = 8.99341 loss)
I0522 23:00:18.386181 34682 sgd_solver.cpp:112] Iteration 9080, lr = 0.01
I0522 23:00:24.095846 34682 solver.cpp:239] Iteration 9090 (1.52475 iter/s, 6.55846s/10 iters), loss = 9.55822
I0522 23:00:24.095904 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.55822 (* 1 = 9.55822 loss)
I0522 23:00:24.792026 34682 sgd_solver.cpp:112] Iteration 9090, lr = 0.01
I0522 23:00:29.660172 34682 solver.cpp:239] Iteration 9100 (1.79725 iter/s, 5.56404s/10 iters), loss = 9.07691
I0522 23:00:29.660224 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07691 (* 1 = 9.07691 loss)
I0522 23:00:30.303442 34682 sgd_solver.cpp:112] Iteration 9100, lr = 0.01
I0522 23:00:36.484781 34682 solver.cpp:239] Iteration 9110 (1.46536 iter/s, 6.82428s/10 iters), loss = 8.69519
I0522 23:00:36.484835 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69519 (* 1 = 8.69519 loss)
I0522 23:00:36.548557 34682 sgd_solver.cpp:112] Iteration 9110, lr = 0.01
I0522 23:00:41.542143 34682 solver.cpp:239] Iteration 9120 (1.97742 iter/s, 5.0571s/10 iters), loss = 9.00985
I0522 23:00:41.542202 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00985 (* 1 = 9.00985 loss)
I0522 23:00:42.439525 34682 sgd_solver.cpp:112] Iteration 9120, lr = 0.01
I0522 23:00:48.180636 34682 solver.cpp:239] Iteration 9130 (1.50645 iter/s, 6.63814s/10 iters), loss = 8.81731
I0522 23:00:48.180909 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81731 (* 1 = 8.81731 loss)
I0522 23:00:48.655304 34682 sgd_solver.cpp:112] Iteration 9130, lr = 0.01
I0522 23:00:53.427239 34682 solver.cpp:239] Iteration 9140 (1.90616 iter/s, 5.24616s/10 iters), loss = 9.43087
I0522 23:00:53.427311 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43087 (* 1 = 9.43087 loss)
I0522 23:00:54.198174 34682 sgd_solver.cpp:112] Iteration 9140, lr = 0.01
I0522 23:00:57.793467 34682 solver.cpp:239] Iteration 9150 (2.29043 iter/s, 4.36599s/10 iters), loss = 8.79448
I0522 23:00:57.793515 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79448 (* 1 = 8.79448 loss)
I0522 23:00:58.673714 34682 sgd_solver.cpp:112] Iteration 9150, lr = 0.01
I0522 23:01:02.739437 34682 solver.cpp:239] Iteration 9160 (2.02196 iter/s, 4.94571s/10 iters), loss = 8.7995
I0522 23:01:02.739487 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7995 (* 1 = 8.7995 loss)
I0522 23:01:02.808395 34682 sgd_solver.cpp:112] Iteration 9160, lr = 0.01
I0522 23:01:06.075947 34682 solver.cpp:239] Iteration 9170 (2.99732 iter/s, 3.33632s/10 iters), loss = 9.53147
I0522 23:01:06.075992 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.53147 (* 1 = 9.53147 loss)
I0522 23:01:06.156005 34682 sgd_solver.cpp:112] Iteration 9170, lr = 0.01
I0522 23:01:10.731338 34682 solver.cpp:239] Iteration 9180 (2.14816 iter/s, 4.65514s/10 iters), loss = 9.41892
I0522 23:01:10.731384 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.41892 (* 1 = 9.41892 loss)
I0522 23:01:10.793285 34682 sgd_solver.cpp:112] Iteration 9180, lr = 0.01
I0522 23:01:15.094316 34682 solver.cpp:239] Iteration 9190 (2.29213 iter/s, 4.36275s/10 iters), loss = 9.05859
I0522 23:01:15.094364 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05859 (* 1 = 9.05859 loss)
I0522 23:01:15.904724 34682 sgd_solver.cpp:112] Iteration 9190, lr = 0.01
I0522 23:01:20.135787 34682 solver.cpp:239] Iteration 9200 (1.98366 iter/s, 5.04118s/10 iters), loss = 9.19698
I0522 23:01:20.136041 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19698 (* 1 = 9.19698 loss)
I0522 23:01:21.010094 34682 sgd_solver.cpp:112] Iteration 9200, lr = 0.01
I0522 23:01:27.183015 34682 solver.cpp:239] Iteration 9210 (1.41911 iter/s, 7.04669s/10 iters), loss = 8.88441
I0522 23:01:27.183105 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88441 (* 1 = 8.88441 loss)
I0522 23:01:27.318616 34682 sgd_solver.cpp:112] Iteration 9210, lr = 0.01
I0522 23:01:32.149087 34682 solver.cpp:239] Iteration 9220 (2.01378 iter/s, 4.96578s/10 iters), loss = 9.21379
I0522 23:01:32.149138 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21379 (* 1 = 9.21379 loss)
I0522 23:01:32.216230 34682 sgd_solver.cpp:112] Iteration 9220, lr = 0.01
I0522 23:01:36.826812 34682 solver.cpp:239] Iteration 9230 (2.13791 iter/s, 4.67747s/10 iters), loss = 9.59803
I0522 23:01:36.826858 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59803 (* 1 = 9.59803 loss)
I0522 23:01:36.908259 34682 sgd_solver.cpp:112] Iteration 9230, lr = 0.01
I0522 23:01:39.855151 34682 solver.cpp:239] Iteration 9240 (3.30234 iter/s, 3.02816s/10 iters), loss = 9.30755
I0522 23:01:39.855199 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30755 (* 1 = 9.30755 loss)
I0522 23:01:39.940340 34682 sgd_solver.cpp:112] Iteration 9240, lr = 0.01
I0522 23:01:44.785207 34682 solver.cpp:239] Iteration 9250 (2.02848 iter/s, 4.92979s/10 iters), loss = 9.67764
I0522 23:01:44.785267 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.67764 (* 1 = 9.67764 loss)
I0522 23:01:45.543720 34682 sgd_solver.cpp:112] Iteration 9250, lr = 0.01
I0522 23:01:50.488212 34682 solver.cpp:239] Iteration 9260 (1.75355 iter/s, 5.70271s/10 iters), loss = 8.71536
I0522 23:01:50.488337 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71536 (* 1 = 8.71536 loss)
I0522 23:01:51.353179 34682 sgd_solver.cpp:112] Iteration 9260, lr = 0.01
I0522 23:01:56.916900 34682 solver.cpp:239] Iteration 9270 (1.55562 iter/s, 6.42831s/10 iters), loss = 9.08799
I0522 23:01:56.916945 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08799 (* 1 = 9.08799 loss)
I0522 23:01:56.988461 34682 sgd_solver.cpp:112] Iteration 9270, lr = 0.01
I0522 23:02:01.097350 34682 solver.cpp:239] Iteration 9280 (2.39221 iter/s, 4.18023s/10 iters), loss = 8.83868
I0522 23:02:01.097400 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83868 (* 1 = 8.83868 loss)
I0522 23:02:01.889914 34682 sgd_solver.cpp:112] Iteration 9280, lr = 0.01
I0522 23:02:04.486109 34682 solver.cpp:239] Iteration 9290 (2.9511 iter/s, 3.38857s/10 iters), loss = 8.51825
I0522 23:02:04.486155 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51825 (* 1 = 8.51825 loss)
I0522 23:02:04.548303 34682 sgd_solver.cpp:112] Iteration 9290, lr = 0.01
I0522 23:02:08.690421 34682 solver.cpp:239] Iteration 9300 (2.37865 iter/s, 4.20407s/10 iters), loss = 9.51698
I0522 23:02:08.690500 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51698 (* 1 = 9.51698 loss)
I0522 23:02:08.760287 34682 sgd_solver.cpp:112] Iteration 9300, lr = 0.01
I0522 23:02:12.974457 34682 solver.cpp:239] Iteration 9310 (2.33438 iter/s, 4.28378s/10 iters), loss = 8.63823
I0522 23:02:12.974503 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63823 (* 1 = 8.63823 loss)
I0522 23:02:13.041978 34682 sgd_solver.cpp:112] Iteration 9310, lr = 0.01
I0522 23:02:18.312383 34682 solver.cpp:239] Iteration 9320 (1.87348 iter/s, 5.33766s/10 iters), loss = 9.79864
I0522 23:02:18.312428 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.79864 (* 1 = 9.79864 loss)
I0522 23:02:18.381469 34682 sgd_solver.cpp:112] Iteration 9320, lr = 0.01
I0522 23:02:23.083335 34682 solver.cpp:239] Iteration 9330 (2.09612 iter/s, 4.77071s/10 iters), loss = 8.90241
I0522 23:02:23.083600 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90241 (* 1 = 8.90241 loss)
I0522 23:02:23.422116 34682 sgd_solver.cpp:112] Iteration 9330, lr = 0.01
I0522 23:02:28.832715 34682 solver.cpp:239] Iteration 9340 (1.73946 iter/s, 5.7489s/10 iters), loss = 8.57199
I0522 23:02:28.832765 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57199 (* 1 = 8.57199 loss)
I0522 23:02:29.008594 34682 sgd_solver.cpp:112] Iteration 9340, lr = 0.01
I0522 23:02:34.070969 34682 solver.cpp:239] Iteration 9350 (1.90913 iter/s, 5.23798s/10 iters), loss = 8.83864
I0522 23:02:34.071030 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83864 (* 1 = 8.83864 loss)
I0522 23:02:34.497776 34682 sgd_solver.cpp:112] Iteration 9350, lr = 0.01
I0522 23:02:38.542462 34682 solver.cpp:239] Iteration 9360 (2.23652 iter/s, 4.47124s/10 iters), loss = 9.15706
I0522 23:02:38.542528 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15706 (* 1 = 9.15706 loss)
I0522 23:02:38.603166 34682 sgd_solver.cpp:112] Iteration 9360, lr = 0.01
I0522 23:02:42.713838 34682 solver.cpp:239] Iteration 9370 (2.39742 iter/s, 4.17114s/10 iters), loss = 9.04463
I0522 23:02:42.713883 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04463 (* 1 = 9.04463 loss)
I0522 23:02:43.553411 34682 sgd_solver.cpp:112] Iteration 9370, lr = 0.01
I0522 23:02:47.890628 34682 solver.cpp:239] Iteration 9380 (1.93179 iter/s, 5.17653s/10 iters), loss = 9.0624
I0522 23:02:47.890674 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0624 (* 1 = 9.0624 loss)
I0522 23:02:48.623744 34682 sgd_solver.cpp:112] Iteration 9380, lr = 0.01
I0522 23:02:53.164875 34682 solver.cpp:239] Iteration 9390 (1.8961 iter/s, 5.27397s/10 iters), loss = 9.34102
I0522 23:02:53.165005 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34102 (* 1 = 9.34102 loss)
I0522 23:02:53.232843 34682 sgd_solver.cpp:112] Iteration 9390, lr = 0.01
I0522 23:02:56.349869 34682 solver.cpp:239] Iteration 9400 (3.13997 iter/s, 3.18474s/10 iters), loss = 9.60365
I0522 23:02:56.349922 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60365 (* 1 = 9.60365 loss)
I0522 23:02:56.405889 34682 sgd_solver.cpp:112] Iteration 9400, lr = 0.01
I0522 23:03:02.164646 34682 solver.cpp:239] Iteration 9410 (1.71984 iter/s, 5.81448s/10 iters), loss = 8.75556
I0522 23:03:02.164696 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75556 (* 1 = 8.75556 loss)
I0522 23:03:02.231815 34682 sgd_solver.cpp:112] Iteration 9410, lr = 0.01
I0522 23:03:07.217064 34682 solver.cpp:239] Iteration 9420 (1.97936 iter/s, 5.05214s/10 iters), loss = 8.45493
I0522 23:03:07.217124 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45493 (* 1 = 8.45493 loss)
I0522 23:03:08.003850 34682 sgd_solver.cpp:112] Iteration 9420, lr = 0.01
I0522 23:03:13.200984 34682 solver.cpp:239] Iteration 9430 (1.67124 iter/s, 5.98359s/10 iters), loss = 9.99668
I0522 23:03:13.201071 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.99668 (* 1 = 9.99668 loss)
I0522 23:03:14.040208 34682 sgd_solver.cpp:112] Iteration 9430, lr = 0.01
I0522 23:03:18.286723 34682 solver.cpp:239] Iteration 9440 (1.9664 iter/s, 5.08543s/10 iters), loss = 8.90662
I0522 23:03:18.286804 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90662 (* 1 = 8.90662 loss)
I0522 23:03:18.671598 34682 sgd_solver.cpp:112] Iteration 9440, lr = 0.01
I0522 23:03:22.006397 34682 solver.cpp:239] Iteration 9450 (2.68858 iter/s, 3.71944s/10 iters), loss = 9.20709
I0522 23:03:22.006453 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20709 (* 1 = 9.20709 loss)
I0522 23:03:22.073365 34682 sgd_solver.cpp:112] Iteration 9450, lr = 0.01
I0522 23:03:27.615686 34682 solver.cpp:239] Iteration 9460 (1.78285 iter/s, 5.60901s/10 iters), loss = 9.308
I0522 23:03:27.615932 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.308 (* 1 = 9.308 loss)
I0522 23:03:28.462245 34682 sgd_solver.cpp:112] Iteration 9460, lr = 0.01
I0522 23:03:32.597290 34682 solver.cpp:239] Iteration 9470 (2.00757 iter/s, 4.98114s/10 iters), loss = 8.9267
I0522 23:03:32.597368 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9267 (* 1 = 8.9267 loss)
I0522 23:03:32.658810 34682 sgd_solver.cpp:112] Iteration 9470, lr = 0.01
I0522 23:03:35.221096 34682 solver.cpp:239] Iteration 9480 (3.81153 iter/s, 2.62362s/10 iters), loss = 9.53571
I0522 23:03:35.221155 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.53571 (* 1 = 9.53571 loss)
I0522 23:03:36.015416 34682 sgd_solver.cpp:112] Iteration 9480, lr = 0.01
I0522 23:03:40.105605 34682 solver.cpp:239] Iteration 9490 (2.0474 iter/s, 4.88423s/10 iters), loss = 9.66589
I0522 23:03:40.105698 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.66589 (* 1 = 9.66589 loss)
I0522 23:03:40.965720 34682 sgd_solver.cpp:112] Iteration 9490, lr = 0.01
I0522 23:03:46.004765 34682 solver.cpp:239] Iteration 9500 (1.69525 iter/s, 5.89884s/10 iters), loss = 8.81092
I0522 23:03:46.004812 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81092 (* 1 = 8.81092 loss)
I0522 23:03:46.068517 34682 sgd_solver.cpp:112] Iteration 9500, lr = 0.01
I0522 23:03:50.190343 34682 solver.cpp:239] Iteration 9510 (2.38928 iter/s, 4.18535s/10 iters), loss = 9.36123
I0522 23:03:50.190402 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36123 (* 1 = 9.36123 loss)
I0522 23:03:50.998595 34682 sgd_solver.cpp:112] Iteration 9510, lr = 0.01
I0522 23:03:55.724185 34682 solver.cpp:239] Iteration 9520 (1.80716 iter/s, 5.53356s/10 iters), loss = 8.76578
I0522 23:03:55.724233 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76578 (* 1 = 8.76578 loss)
I0522 23:03:55.787668 34682 sgd_solver.cpp:112] Iteration 9520, lr = 0.01
I0522 23:04:01.453744 34682 solver.cpp:239] Iteration 9530 (1.74542 iter/s, 5.72928s/10 iters), loss = 8.89116
I0522 23:04:01.453955 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89116 (* 1 = 8.89116 loss)
I0522 23:04:02.129992 34682 sgd_solver.cpp:112] Iteration 9530, lr = 0.01
I0522 23:04:05.332756 34682 solver.cpp:239] Iteration 9540 (2.5782 iter/s, 3.87867s/10 iters), loss = 8.64633
I0522 23:04:05.332809 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64633 (* 1 = 8.64633 loss)
I0522 23:04:05.409044 34682 sgd_solver.cpp:112] Iteration 9540, lr = 0.01
I0522 23:04:08.667577 34682 solver.cpp:239] Iteration 9550 (2.99883 iter/s, 3.33463s/10 iters), loss = 8.99436
I0522 23:04:08.667632 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99436 (* 1 = 8.99436 loss)
I0522 23:04:09.499634 34682 sgd_solver.cpp:112] Iteration 9550, lr = 0.01
I0522 23:04:13.583230 34682 solver.cpp:239] Iteration 9560 (2.03443 iter/s, 4.91537s/10 iters), loss = 9.5955
I0522 23:04:13.583323 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.5955 (* 1 = 9.5955 loss)
I0522 23:04:13.653182 34682 sgd_solver.cpp:112] Iteration 9560, lr = 0.01
I0522 23:04:19.313419 34682 solver.cpp:239] Iteration 9570 (1.74524 iter/s, 5.72987s/10 iters), loss = 9.13659
I0522 23:04:19.313473 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13659 (* 1 = 9.13659 loss)
I0522 23:04:20.066500 34682 sgd_solver.cpp:112] Iteration 9570, lr = 0.01
I0522 23:04:23.534540 34682 solver.cpp:239] Iteration 9580 (2.36918 iter/s, 4.22087s/10 iters), loss = 9.64243
I0522 23:04:23.534600 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.64243 (* 1 = 9.64243 loss)
I0522 23:04:23.595742 34682 sgd_solver.cpp:112] Iteration 9580, lr = 0.01
I0522 23:04:27.483333 34682 solver.cpp:239] Iteration 9590 (2.53257 iter/s, 3.94856s/10 iters), loss = 9.24505
I0522 23:04:27.483388 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24505 (* 1 = 9.24505 loss)
I0522 23:04:28.222802 34682 sgd_solver.cpp:112] Iteration 9590, lr = 0.01
I0522 23:04:32.660843 34682 solver.cpp:239] Iteration 9600 (1.93153 iter/s, 5.17724s/10 iters), loss = 9.1721
I0522 23:04:32.661170 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1721 (* 1 = 9.1721 loss)
I0522 23:04:32.722205 34682 sgd_solver.cpp:112] Iteration 9600, lr = 0.01
I0522 23:04:39.059051 34682 solver.cpp:239] Iteration 9610 (1.56307 iter/s, 6.39765s/10 iters), loss = 8.92793
I0522 23:04:39.059110 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92793 (* 1 = 8.92793 loss)
I0522 23:04:39.138417 34682 sgd_solver.cpp:112] Iteration 9610, lr = 0.01
I0522 23:04:42.283918 34682 solver.cpp:239] Iteration 9620 (3.1011 iter/s, 3.22466s/10 iters), loss = 8.53196
I0522 23:04:42.283980 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53196 (* 1 = 8.53196 loss)
I0522 23:04:42.347921 34682 sgd_solver.cpp:112] Iteration 9620, lr = 0.01
I0522 23:04:47.316159 34682 solver.cpp:239] Iteration 9630 (1.98729 iter/s, 5.03198s/10 iters), loss = 8.99608
I0522 23:04:47.316210 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99608 (* 1 = 8.99608 loss)
I0522 23:04:47.397816 34682 sgd_solver.cpp:112] Iteration 9630, lr = 0.01
I0522 23:04:52.180116 34682 solver.cpp:239] Iteration 9640 (2.05605 iter/s, 4.8637s/10 iters), loss = 9.646
I0522 23:04:52.180176 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.646 (* 1 = 9.646 loss)
I0522 23:04:52.238240 34682 sgd_solver.cpp:112] Iteration 9640, lr = 0.01
I0522 23:04:56.026433 34682 solver.cpp:239] Iteration 9650 (2.60004 iter/s, 3.8461s/10 iters), loss = 9.75441
I0522 23:04:56.026490 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.75441 (* 1 = 9.75441 loss)
I0522 23:04:56.630432 34682 sgd_solver.cpp:112] Iteration 9650, lr = 0.01
I0522 23:05:01.623109 34682 solver.cpp:239] Iteration 9660 (1.78686 iter/s, 5.5964s/10 iters), loss = 8.77499
I0522 23:05:01.623165 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77499 (* 1 = 8.77499 loss)
I0522 23:05:02.295959 34682 sgd_solver.cpp:112] Iteration 9660, lr = 0.01
I0522 23:05:06.363152 34682 solver.cpp:239] Iteration 9670 (2.1098 iter/s, 4.73979s/10 iters), loss = 9.32458
I0522 23:05:06.363436 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32458 (* 1 = 9.32458 loss)
I0522 23:05:06.442080 34682 sgd_solver.cpp:112] Iteration 9670, lr = 0.01
I0522 23:05:10.751608 34682 solver.cpp:239] Iteration 9680 (2.27893 iter/s, 4.38803s/10 iters), loss = 9.59708
I0522 23:05:10.751652 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59708 (* 1 = 9.59708 loss)
I0522 23:05:10.810765 34682 sgd_solver.cpp:112] Iteration 9680, lr = 0.01
I0522 23:05:13.449098 34682 solver.cpp:239] Iteration 9690 (3.70739 iter/s, 2.69732s/10 iters), loss = 9.48043
I0522 23:05:13.449156 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.48043 (* 1 = 9.48043 loss)
I0522 23:05:13.691004 34682 sgd_solver.cpp:112] Iteration 9690, lr = 0.01
I0522 23:05:17.388660 34682 solver.cpp:239] Iteration 9700 (2.53849 iter/s, 3.93934s/10 iters), loss = 9.36524
I0522 23:05:17.388706 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36524 (* 1 = 9.36524 loss)
I0522 23:05:18.200228 34682 sgd_solver.cpp:112] Iteration 9700, lr = 0.01
I0522 23:05:24.039038 34682 solver.cpp:239] Iteration 9710 (1.50375 iter/s, 6.65006s/10 iters), loss = 8.79102
I0522 23:05:24.039109 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79102 (* 1 = 8.79102 loss)
I0522 23:05:24.743358 34682 sgd_solver.cpp:112] Iteration 9710, lr = 0.01
I0522 23:05:29.333616 34682 solver.cpp:239] Iteration 9720 (1.88883 iter/s, 5.29429s/10 iters), loss = 9.03828
I0522 23:05:29.333669 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03828 (* 1 = 9.03828 loss)
I0522 23:05:30.215353 34682 sgd_solver.cpp:112] Iteration 9720, lr = 0.01
I0522 23:05:34.867027 34682 solver.cpp:239] Iteration 9730 (1.8073 iter/s, 5.53313s/10 iters), loss = 9.35632
I0522 23:05:34.867095 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35632 (* 1 = 9.35632 loss)
I0522 23:05:34.937849 34682 sgd_solver.cpp:112] Iteration 9730, lr = 0.01
I0522 23:05:39.721555 34682 solver.cpp:239] Iteration 9740 (2.06004 iter/s, 4.85428s/10 iters), loss = 8.91105
I0522 23:05:39.721856 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91105 (* 1 = 8.91105 loss)
I0522 23:05:39.780705 34682 sgd_solver.cpp:112] Iteration 9740, lr = 0.01
I0522 23:05:45.153316 34682 solver.cpp:239] Iteration 9750 (1.84119 iter/s, 5.43127s/10 iters), loss = 9.46948
I0522 23:05:45.153372 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.46948 (* 1 = 9.46948 loss)
I0522 23:05:45.217535 34682 sgd_solver.cpp:112] Iteration 9750, lr = 0.01
I0522 23:05:49.600944 34682 solver.cpp:239] Iteration 9760 (2.24851 iter/s, 4.44738s/10 iters), loss = 9.33679
I0522 23:05:49.601004 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33679 (* 1 = 9.33679 loss)
I0522 23:05:50.443567 34682 sgd_solver.cpp:112] Iteration 9760, lr = 0.01
I0522 23:05:54.833731 34682 solver.cpp:239] Iteration 9770 (1.91113 iter/s, 5.23251s/10 iters), loss = 8.67815
I0522 23:05:54.833782 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67815 (* 1 = 8.67815 loss)
I0522 23:05:54.896189 34682 sgd_solver.cpp:112] Iteration 9770, lr = 0.01
I0522 23:06:00.033746 34682 solver.cpp:239] Iteration 9780 (1.92317 iter/s, 5.19974s/10 iters), loss = 8.85666
I0522 23:06:00.033809 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85666 (* 1 = 8.85666 loss)
I0522 23:06:00.805315 34682 sgd_solver.cpp:112] Iteration 9780, lr = 0.01
I0522 23:06:05.006391 34682 solver.cpp:239] Iteration 9790 (2.01111 iter/s, 4.97239s/10 iters), loss = 8.33041
I0522 23:06:05.006431 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33041 (* 1 = 8.33041 loss)
I0522 23:06:05.761193 34682 sgd_solver.cpp:112] Iteration 9790, lr = 0.01
I0522 23:06:09.936506 34682 solver.cpp:239] Iteration 9800 (2.02845 iter/s, 4.92987s/10 iters), loss = 8.79288
I0522 23:06:09.936717 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79288 (* 1 = 8.79288 loss)
I0522 23:06:09.997432 34682 sgd_solver.cpp:112] Iteration 9800, lr = 0.01
I0522 23:06:12.836999 34682 solver.cpp:239] Iteration 9810 (3.44807 iter/s, 2.90017s/10 iters), loss = 8.97233
I0522 23:06:12.837065 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97233 (* 1 = 8.97233 loss)
I0522 23:06:12.896770 34682 sgd_solver.cpp:112] Iteration 9810, lr = 0.01
I0522 23:06:16.698278 34682 solver.cpp:239] Iteration 9820 (2.58996 iter/s, 3.86106s/10 iters), loss = 8.9262
I0522 23:06:16.698318 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9262 (* 1 = 8.9262 loss)
I0522 23:06:16.780724 34682 sgd_solver.cpp:112] Iteration 9820, lr = 0.01
I0522 23:06:21.785820 34682 solver.cpp:239] Iteration 9830 (1.96568 iter/s, 5.08729s/10 iters), loss = 8.86384
I0522 23:06:21.785872 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86384 (* 1 = 8.86384 loss)
I0522 23:06:21.845731 34682 sgd_solver.cpp:112] Iteration 9830, lr = 0.01
I0522 23:06:26.255322 34682 solver.cpp:239] Iteration 9840 (2.23751 iter/s, 4.46925s/10 iters), loss = 8.97027
I0522 23:06:26.255386 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97027 (* 1 = 8.97027 loss)
I0522 23:06:26.822999 34682 sgd_solver.cpp:112] Iteration 9840, lr = 0.01
I0522 23:06:31.500560 34682 solver.cpp:239] Iteration 9850 (1.90659 iter/s, 5.24496s/10 iters), loss = 9.66712
I0522 23:06:31.500612 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.66712 (* 1 = 9.66712 loss)
I0522 23:06:31.572677 34682 sgd_solver.cpp:112] Iteration 9850, lr = 0.01
I0522 23:06:35.652329 34682 solver.cpp:239] Iteration 9860 (2.40874 iter/s, 4.15155s/10 iters), loss = 8.87382
I0522 23:06:35.652384 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87382 (* 1 = 8.87382 loss)
I0522 23:06:36.490787 34682 sgd_solver.cpp:112] Iteration 9860, lr = 0.01
I0522 23:06:40.621848 34682 solver.cpp:239] Iteration 9870 (2.01238 iter/s, 4.96925s/10 iters), loss = 9.15577
I0522 23:06:40.622057 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15577 (* 1 = 9.15577 loss)
I0522 23:06:40.686549 34682 sgd_solver.cpp:112] Iteration 9870, lr = 0.01
I0522 23:06:45.382256 34682 solver.cpp:239] Iteration 9880 (2.10084 iter/s, 4.76001s/10 iters), loss = 8.77687
I0522 23:06:45.382304 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77687 (* 1 = 8.77687 loss)
I0522 23:06:45.440037 34682 sgd_solver.cpp:112] Iteration 9880, lr = 0.01
I0522 23:06:52.376354 34682 solver.cpp:239] Iteration 9890 (1.42985 iter/s, 6.99376s/10 iters), loss = 8.92297
I0522 23:06:52.376420 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92297 (* 1 = 8.92297 loss)
I0522 23:06:53.213197 34682 sgd_solver.cpp:112] Iteration 9890, lr = 0.01
I0522 23:06:57.309336 34682 solver.cpp:239] Iteration 9900 (2.02728 iter/s, 4.93272s/10 iters), loss = 8.48979
I0522 23:06:57.309388 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48979 (* 1 = 8.48979 loss)
I0522 23:06:57.372531 34682 sgd_solver.cpp:112] Iteration 9900, lr = 0.01
I0522 23:07:02.548251 34682 solver.cpp:239] Iteration 9910 (1.90889 iter/s, 5.23865s/10 iters), loss = 9.22503
I0522 23:07:02.548296 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22503 (* 1 = 9.22503 loss)
I0522 23:07:02.622277 34682 sgd_solver.cpp:112] Iteration 9910, lr = 0.01
I0522 23:07:07.659485 34682 solver.cpp:239] Iteration 9920 (1.95657 iter/s, 5.11097s/10 iters), loss = 8.74606
I0522 23:07:07.659545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74606 (* 1 = 8.74606 loss)
I0522 23:07:08.437861 34682 sgd_solver.cpp:112] Iteration 9920, lr = 0.01
I0522 23:07:12.772578 34682 solver.cpp:239] Iteration 9930 (1.95587 iter/s, 5.11281s/10 iters), loss = 9.13319
I0522 23:07:12.772802 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13319 (* 1 = 9.13319 loss)
I0522 23:07:13.541198 34682 sgd_solver.cpp:112] Iteration 9930, lr = 0.01
I0522 23:07:16.764962 34682 solver.cpp:239] Iteration 9940 (2.50499 iter/s, 3.99202s/10 iters), loss = 9.03156
I0522 23:07:16.765013 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03156 (* 1 = 9.03156 loss)
I0522 23:07:16.831679 34682 sgd_solver.cpp:112] Iteration 9940, lr = 0.01
I0522 23:07:21.299593 34682 solver.cpp:239] Iteration 9950 (2.20537 iter/s, 4.53439s/10 iters), loss = 9.05629
I0522 23:07:21.299640 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05629 (* 1 = 9.05629 loss)
I0522 23:07:21.367434 34682 sgd_solver.cpp:112] Iteration 9950, lr = 0.01
I0522 23:07:26.023385 34682 solver.cpp:239] Iteration 9960 (2.11705 iter/s, 4.72355s/10 iters), loss = 9.12355
I0522 23:07:26.023438 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12355 (* 1 = 9.12355 loss)
I0522 23:07:26.103149 34682 sgd_solver.cpp:112] Iteration 9960, lr = 0.01
I0522 23:07:29.633007 34682 solver.cpp:239] Iteration 9970 (2.77054 iter/s, 3.60941s/10 iters), loss = 9.49836
I0522 23:07:29.633067 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.49836 (* 1 = 9.49836 loss)
I0522 23:07:30.458760 34682 sgd_solver.cpp:112] Iteration 9970, lr = 0.01
I0522 23:07:33.822891 34682 solver.cpp:239] Iteration 9980 (2.38683 iter/s, 4.18966s/10 iters), loss = 9.25106
I0522 23:07:33.822932 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25106 (* 1 = 9.25106 loss)
I0522 23:07:33.896553 34682 sgd_solver.cpp:112] Iteration 9980, lr = 0.01
I0522 23:07:38.022291 34682 solver.cpp:239] Iteration 9990 (2.38142 iter/s, 4.19918s/10 iters), loss = 9.07704
I0522 23:07:38.022357 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07704 (* 1 = 9.07704 loss)
I0522 23:07:38.201937 34682 sgd_solver.cpp:112] Iteration 9990, lr = 0.01
I0522 23:07:43.094856 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_10000.caffemodel
I0522 23:07:44.801044 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_10000.solverstate
I0522 23:07:45.029331 34682 solver.cpp:239] Iteration 10000 (1.42721 iter/s, 7.0067s/10 iters), loss = 8.77962
I0522 23:07:45.029383 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77962 (* 1 = 8.77962 loss)
I0522 23:07:45.782666 34682 sgd_solver.cpp:112] Iteration 10000, lr = 0.01
I0522 23:07:51.434437 34682 solver.cpp:239] Iteration 10010 (1.56133 iter/s, 6.40479s/10 iters), loss = 9.0453
I0522 23:07:51.434487 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0453 (* 1 = 9.0453 loss)
I0522 23:07:52.212225 34682 sgd_solver.cpp:112] Iteration 10010, lr = 0.01
I0522 23:07:56.154772 34682 solver.cpp:239] Iteration 10020 (2.11861 iter/s, 4.72008s/10 iters), loss = 9.53325
I0522 23:07:56.154831 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.53325 (* 1 = 9.53325 loss)
I0522 23:07:56.216783 34682 sgd_solver.cpp:112] Iteration 10020, lr = 0.01
I0522 23:08:02.021172 34682 solver.cpp:239] Iteration 10030 (1.70471 iter/s, 5.86611s/10 iters), loss = 8.94231
I0522 23:08:02.021224 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94231 (* 1 = 8.94231 loss)
I0522 23:08:02.102048 34682 sgd_solver.cpp:112] Iteration 10030, lr = 0.01
I0522 23:08:05.543017 34682 solver.cpp:239] Iteration 10040 (2.83959 iter/s, 3.52164s/10 iters), loss = 9.48677
I0522 23:08:05.543069 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.48677 (* 1 = 9.48677 loss)
I0522 23:08:06.376235 34682 sgd_solver.cpp:112] Iteration 10040, lr = 0.01
I0522 23:08:11.342404 34682 solver.cpp:239] Iteration 10050 (1.72441 iter/s, 5.79909s/10 iters), loss = 8.6693
I0522 23:08:11.342452 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6693 (* 1 = 8.6693 loss)
I0522 23:08:11.410019 34682 sgd_solver.cpp:112] Iteration 10050, lr = 0.01
I0522 23:08:17.281620 34682 solver.cpp:239] Iteration 10060 (1.68381 iter/s, 5.93892s/10 iters), loss = 9.20311
I0522 23:08:17.281770 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20311 (* 1 = 9.20311 loss)
I0522 23:08:17.344563 34682 sgd_solver.cpp:112] Iteration 10060, lr = 0.01
I0522 23:08:22.193123 34682 solver.cpp:239] Iteration 10070 (2.03618 iter/s, 4.91115s/10 iters), loss = 9.3036
I0522 23:08:22.193184 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3036 (* 1 = 9.3036 loss)
I0522 23:08:22.996826 34682 sgd_solver.cpp:112] Iteration 10070, lr = 0.01
I0522 23:08:27.005183 34682 solver.cpp:239] Iteration 10080 (2.07822 iter/s, 4.8118s/10 iters), loss = 8.38162
I0522 23:08:27.005228 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38162 (* 1 = 8.38162 loss)
I0522 23:08:27.068838 34682 sgd_solver.cpp:112] Iteration 10080, lr = 0.01
I0522 23:08:32.290545 34682 solver.cpp:239] Iteration 10090 (1.89211 iter/s, 5.2851s/10 iters), loss = 9.51642
I0522 23:08:32.290601 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51642 (* 1 = 9.51642 loss)
I0522 23:08:32.902024 34682 sgd_solver.cpp:112] Iteration 10090, lr = 0.01
I0522 23:08:38.537016 34682 solver.cpp:239] Iteration 10100 (1.60098 iter/s, 6.24616s/10 iters), loss = 9.11034
I0522 23:08:38.537065 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11034 (* 1 = 9.11034 loss)
I0522 23:08:38.607937 34682 sgd_solver.cpp:112] Iteration 10100, lr = 0.01
I0522 23:08:41.687510 34682 solver.cpp:239] Iteration 10110 (3.1743 iter/s, 3.15031s/10 iters), loss = 10.1383
I0522 23:08:41.687561 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1383 (* 1 = 10.1383 loss)
I0522 23:08:42.518424 34682 sgd_solver.cpp:112] Iteration 10110, lr = 0.01
I0522 23:08:48.753660 34682 solver.cpp:239] Iteration 10120 (1.41527 iter/s, 7.06581s/10 iters), loss = 8.9337
I0522 23:08:48.753895 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9337 (* 1 = 8.9337 loss)
I0522 23:08:48.832463 34682 sgd_solver.cpp:112] Iteration 10120, lr = 0.01
I0522 23:08:53.395293 34682 solver.cpp:239] Iteration 10130 (2.1546 iter/s, 4.64123s/10 iters), loss = 8.68786
I0522 23:08:53.395344 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68786 (* 1 = 8.68786 loss)
I0522 23:08:53.465250 34682 sgd_solver.cpp:112] Iteration 10130, lr = 0.01
I0522 23:08:58.044736 34682 solver.cpp:239] Iteration 10140 (2.15091 iter/s, 4.6492s/10 iters), loss = 9.061
I0522 23:08:58.044786 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.061 (* 1 = 9.061 loss)
I0522 23:08:58.124753 34682 sgd_solver.cpp:112] Iteration 10140, lr = 0.01
I0522 23:09:03.949040 34682 solver.cpp:239] Iteration 10150 (1.69408 iter/s, 5.90291s/10 iters), loss = 8.52789
I0522 23:09:03.949100 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52789 (* 1 = 8.52789 loss)
I0522 23:09:04.315219 34682 sgd_solver.cpp:112] Iteration 10150, lr = 0.01
I0522 23:09:09.976930 34682 solver.cpp:239] Iteration 10160 (1.65904 iter/s, 6.02758s/10 iters), loss = 8.25413
I0522 23:09:09.976992 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25413 (* 1 = 8.25413 loss)
I0522 23:09:10.855558 34682 sgd_solver.cpp:112] Iteration 10160, lr = 0.01
I0522 23:09:15.874092 34682 solver.cpp:239] Iteration 10170 (1.69582 iter/s, 5.89685s/10 iters), loss = 9.18048
I0522 23:09:15.874155 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18048 (* 1 = 9.18048 loss)
I0522 23:09:15.943833 34682 sgd_solver.cpp:112] Iteration 10170, lr = 0.01
I0522 23:09:19.886994 34682 solver.cpp:239] Iteration 10180 (2.4921 iter/s, 4.01268s/10 iters), loss = 8.74299
I0522 23:09:19.887205 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74299 (* 1 = 8.74299 loss)
I0522 23:09:19.953397 34682 sgd_solver.cpp:112] Iteration 10180, lr = 0.01
I0522 23:09:23.242589 34682 solver.cpp:239] Iteration 10190 (2.98038 iter/s, 3.35528s/10 iters), loss = 9.05202
I0522 23:09:23.242640 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05202 (* 1 = 9.05202 loss)
I0522 23:09:24.043241 34682 sgd_solver.cpp:112] Iteration 10190, lr = 0.01
I0522 23:09:27.499214 34682 solver.cpp:239] Iteration 10200 (2.3494 iter/s, 4.2564s/10 iters), loss = 8.87818
I0522 23:09:27.499259 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87818 (* 1 = 8.87818 loss)
I0522 23:09:27.580199 34682 sgd_solver.cpp:112] Iteration 10200, lr = 0.01
I0522 23:09:32.122805 34682 solver.cpp:239] Iteration 10210 (2.16294 iter/s, 4.62334s/10 iters), loss = 9.5936
I0522 23:09:32.122876 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.5936 (* 1 = 9.5936 loss)
I0522 23:09:32.185528 34682 sgd_solver.cpp:112] Iteration 10210, lr = 0.01
I0522 23:09:37.766343 34682 solver.cpp:239] Iteration 10220 (1.77203 iter/s, 5.64324s/10 iters), loss = 9.00664
I0522 23:09:37.766398 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00664 (* 1 = 9.00664 loss)
I0522 23:09:38.557919 34682 sgd_solver.cpp:112] Iteration 10220, lr = 0.01
I0522 23:09:41.695907 34682 solver.cpp:239] Iteration 10230 (2.54495 iter/s, 3.92935s/10 iters), loss = 9.48425
I0522 23:09:41.695955 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.48425 (* 1 = 9.48425 loss)
I0522 23:09:42.474498 34682 sgd_solver.cpp:112] Iteration 10230, lr = 0.01
I0522 23:09:48.531110 34682 solver.cpp:239] Iteration 10240 (1.46308 iter/s, 6.83488s/10 iters), loss = 8.85054
I0522 23:09:48.531153 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85054 (* 1 = 8.85054 loss)
I0522 23:09:48.609496 34682 sgd_solver.cpp:112] Iteration 10240, lr = 0.01
I0522 23:09:54.095536 34682 solver.cpp:239] Iteration 10250 (1.79722 iter/s, 5.56415s/10 iters), loss = 9.42059
I0522 23:09:54.095827 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42059 (* 1 = 9.42059 loss)
I0522 23:09:54.175770 34682 sgd_solver.cpp:112] Iteration 10250, lr = 0.01
I0522 23:09:59.160344 34682 solver.cpp:239] Iteration 10260 (1.97459 iter/s, 5.06434s/10 iters), loss = 9.09307
I0522 23:09:59.160392 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09307 (* 1 = 9.09307 loss)
I0522 23:09:59.225594 34682 sgd_solver.cpp:112] Iteration 10260, lr = 0.01
I0522 23:10:02.703092 34682 solver.cpp:239] Iteration 10270 (2.82283 iter/s, 3.54255s/10 iters), loss = 9.36569
I0522 23:10:02.703145 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36569 (* 1 = 9.36569 loss)
I0522 23:10:03.455042 34682 sgd_solver.cpp:112] Iteration 10270, lr = 0.01
I0522 23:10:06.764212 34682 solver.cpp:239] Iteration 10280 (2.46251 iter/s, 4.0609s/10 iters), loss = 9.34921
I0522 23:10:06.764264 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34921 (* 1 = 9.34921 loss)
I0522 23:10:06.828824 34682 sgd_solver.cpp:112] Iteration 10280, lr = 0.01
I0522 23:10:11.945574 34682 solver.cpp:239] Iteration 10290 (1.9301 iter/s, 5.18107s/10 iters), loss = 8.90983
I0522 23:10:11.945636 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90983 (* 1 = 8.90983 loss)
I0522 23:10:12.662396 34682 sgd_solver.cpp:112] Iteration 10290, lr = 0.01
I0522 23:10:16.828320 34682 solver.cpp:239] Iteration 10300 (2.04816 iter/s, 4.88243s/10 iters), loss = 9.14387
I0522 23:10:16.828466 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14387 (* 1 = 9.14387 loss)
I0522 23:10:17.651312 34682 sgd_solver.cpp:112] Iteration 10300, lr = 0.01
I0522 23:10:24.273999 34682 solver.cpp:239] Iteration 10310 (1.34314 iter/s, 7.44526s/10 iters), loss = 9.60626
I0522 23:10:24.274269 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60626 (* 1 = 9.60626 loss)
I0522 23:10:24.915750 34682 sgd_solver.cpp:112] Iteration 10310, lr = 0.01
I0522 23:10:28.962951 34682 solver.cpp:239] Iteration 10320 (2.13287 iter/s, 4.68852s/10 iters), loss = 8.74798
I0522 23:10:28.963007 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74798 (* 1 = 8.74798 loss)
I0522 23:10:29.566218 34682 sgd_solver.cpp:112] Iteration 10320, lr = 0.01
I0522 23:10:34.732945 34682 solver.cpp:239] Iteration 10330 (1.7332 iter/s, 5.76969s/10 iters), loss = 9.43787
I0522 23:10:34.732993 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43787 (* 1 = 9.43787 loss)
I0522 23:10:34.807621 34682 sgd_solver.cpp:112] Iteration 10330, lr = 0.01
I0522 23:10:39.780107 34682 solver.cpp:239] Iteration 10340 (1.98141 iter/s, 5.04691s/10 iters), loss = 9.27183
I0522 23:10:39.780148 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27183 (* 1 = 9.27183 loss)
I0522 23:10:39.838065 34682 sgd_solver.cpp:112] Iteration 10340, lr = 0.01
I0522 23:10:43.688556 34682 solver.cpp:239] Iteration 10350 (2.5587 iter/s, 3.90824s/10 iters), loss = 7.9477
I0522 23:10:43.688602 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9477 (* 1 = 7.9477 loss)
I0522 23:10:43.767308 34682 sgd_solver.cpp:112] Iteration 10350, lr = 0.01
I0522 23:10:46.416247 34682 solver.cpp:239] Iteration 10360 (3.66632 iter/s, 2.72753s/10 iters), loss = 8.6215
I0522 23:10:46.416290 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6215 (* 1 = 8.6215 loss)
I0522 23:10:46.474975 34682 sgd_solver.cpp:112] Iteration 10360, lr = 0.01
I0522 23:10:50.278861 34682 solver.cpp:239] Iteration 10370 (2.58906 iter/s, 3.86241s/10 iters), loss = 9.38758
I0522 23:10:50.278908 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.38758 (* 1 = 9.38758 loss)
I0522 23:10:50.362836 34682 sgd_solver.cpp:112] Iteration 10370, lr = 0.01
I0522 23:10:54.411340 34682 solver.cpp:239] Iteration 10380 (2.41999 iter/s, 4.13224s/10 iters), loss = 9.04247
I0522 23:10:54.411694 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04247 (* 1 = 9.04247 loss)
I0522 23:10:54.475034 34682 sgd_solver.cpp:112] Iteration 10380, lr = 0.01
I0522 23:10:59.290452 34682 solver.cpp:239] Iteration 10390 (2.04977 iter/s, 4.8786s/10 iters), loss = 9.23546
I0522 23:10:59.290508 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23546 (* 1 = 9.23546 loss)
I0522 23:11:00.149642 34682 sgd_solver.cpp:112] Iteration 10390, lr = 0.01
I0522 23:11:05.655313 34682 solver.cpp:239] Iteration 10400 (1.5712 iter/s, 6.36455s/10 iters), loss = 9.06763
I0522 23:11:05.655364 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06763 (* 1 = 9.06763 loss)
I0522 23:11:06.307085 34682 sgd_solver.cpp:112] Iteration 10400, lr = 0.01
I0522 23:11:13.332228 34682 solver.cpp:239] Iteration 10410 (1.30267 iter/s, 7.67656s/10 iters), loss = 9.34189
I0522 23:11:13.332279 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34189 (* 1 = 9.34189 loss)
I0522 23:11:13.409643 34682 sgd_solver.cpp:112] Iteration 10410, lr = 0.01
I0522 23:11:20.557294 34682 solver.cpp:239] Iteration 10420 (1.38414 iter/s, 7.22472s/10 iters), loss = 8.67457
I0522 23:11:20.557343 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67457 (* 1 = 8.67457 loss)
I0522 23:11:21.444969 34682 sgd_solver.cpp:112] Iteration 10420, lr = 0.01
I0522 23:11:26.543714 34682 solver.cpp:239] Iteration 10430 (1.67053 iter/s, 5.98612s/10 iters), loss = 9.51791
I0522 23:11:26.543864 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51791 (* 1 = 9.51791 loss)
I0522 23:11:26.612480 34682 sgd_solver.cpp:112] Iteration 10430, lr = 0.01
I0522 23:11:29.277379 34682 solver.cpp:239] Iteration 10440 (3.65846 iter/s, 2.73339s/10 iters), loss = 9.57078
I0522 23:11:29.277441 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.57078 (* 1 = 9.57078 loss)
I0522 23:11:29.700181 34682 sgd_solver.cpp:112] Iteration 10440, lr = 0.01
I0522 23:11:34.195240 34682 solver.cpp:239] Iteration 10450 (2.03351 iter/s, 4.9176s/10 iters), loss = 9.47901
I0522 23:11:34.195297 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47901 (* 1 = 9.47901 loss)
I0522 23:11:34.256572 34682 sgd_solver.cpp:112] Iteration 10450, lr = 0.01
I0522 23:11:38.752338 34682 solver.cpp:239] Iteration 10460 (2.1945 iter/s, 4.55686s/10 iters), loss = 8.98321
I0522 23:11:38.752384 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98321 (* 1 = 8.98321 loss)
I0522 23:11:38.815428 34682 sgd_solver.cpp:112] Iteration 10460, lr = 0.01
I0522 23:11:42.338302 34682 solver.cpp:239] Iteration 10470 (2.7888 iter/s, 3.58577s/10 iters), loss = 8.8463
I0522 23:11:42.338346 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8463 (* 1 = 8.8463 loss)
I0522 23:11:42.399174 34682 sgd_solver.cpp:112] Iteration 10470, lr = 0.01
I0522 23:11:47.502475 34682 solver.cpp:239] Iteration 10480 (1.93652 iter/s, 5.1639s/10 iters), loss = 9.05252
I0522 23:11:47.502527 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05252 (* 1 = 9.05252 loss)
I0522 23:11:48.392719 34682 sgd_solver.cpp:112] Iteration 10480, lr = 0.01
I0522 23:11:52.228222 34682 solver.cpp:239] Iteration 10490 (2.11618 iter/s, 4.7255s/10 iters), loss = 9.00072
I0522 23:11:52.228283 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00072 (* 1 = 9.00072 loss)
I0522 23:11:52.503907 34682 sgd_solver.cpp:112] Iteration 10490, lr = 0.01
I0522 23:11:56.680768 34682 solver.cpp:239] Iteration 10500 (2.24603 iter/s, 4.45229s/10 iters), loss = 8.81285
I0522 23:11:56.680999 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81285 (* 1 = 8.81285 loss)
I0522 23:11:56.744037 34682 sgd_solver.cpp:112] Iteration 10500, lr = 0.01
I0522 23:12:01.896029 34682 solver.cpp:239] Iteration 10510 (1.91761 iter/s, 5.21483s/10 iters), loss = 9.56963
I0522 23:12:01.896095 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56963 (* 1 = 9.56963 loss)
I0522 23:12:02.687746 34682 sgd_solver.cpp:112] Iteration 10510, lr = 0.01
I0522 23:12:08.370126 34682 solver.cpp:239] Iteration 10520 (1.5447 iter/s, 6.47377s/10 iters), loss = 8.76234
I0522 23:12:08.370168 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76234 (* 1 = 8.76234 loss)
I0522 23:12:08.448222 34682 sgd_solver.cpp:112] Iteration 10520, lr = 0.01
I0522 23:12:13.071027 34682 solver.cpp:239] Iteration 10530 (2.12736 iter/s, 4.70066s/10 iters), loss = 9.0076
I0522 23:12:13.071082 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0076 (* 1 = 9.0076 loss)
I0522 23:12:13.126616 34682 sgd_solver.cpp:112] Iteration 10530, lr = 0.01
I0522 23:12:17.138808 34682 solver.cpp:239] Iteration 10540 (2.45849 iter/s, 4.06753s/10 iters), loss = 9.27591
I0522 23:12:17.138865 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27591 (* 1 = 9.27591 loss)
I0522 23:12:17.854321 34682 sgd_solver.cpp:112] Iteration 10540, lr = 0.01
I0522 23:12:23.482347 34682 solver.cpp:239] Iteration 10550 (1.57648 iter/s, 6.34323s/10 iters), loss = 9.81573
I0522 23:12:23.482390 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.81573 (* 1 = 9.81573 loss)
I0522 23:12:23.557642 34682 sgd_solver.cpp:112] Iteration 10550, lr = 0.01
I0522 23:12:27.562412 34682 solver.cpp:239] Iteration 10560 (2.45108 iter/s, 4.07984s/10 iters), loss = 9.67186
I0522 23:12:27.562721 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.67186 (* 1 = 9.67186 loss)
I0522 23:12:28.040187 34682 sgd_solver.cpp:112] Iteration 10560, lr = 0.01
I0522 23:12:32.306004 34682 solver.cpp:239] Iteration 10570 (2.10831 iter/s, 4.74314s/10 iters), loss = 8.71937
I0522 23:12:32.306057 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71937 (* 1 = 8.71937 loss)
I0522 23:12:33.151827 34682 sgd_solver.cpp:112] Iteration 10570, lr = 0.01
I0522 23:12:39.529999 34682 solver.cpp:239] Iteration 10580 (1.38434 iter/s, 7.22365s/10 iters), loss = 9.09806
I0522 23:12:39.530067 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09806 (* 1 = 9.09806 loss)
I0522 23:12:40.329568 34682 sgd_solver.cpp:112] Iteration 10580, lr = 0.01
I0522 23:12:45.822942 34682 solver.cpp:239] Iteration 10590 (1.58916 iter/s, 6.29262s/10 iters), loss = 8.89178
I0522 23:12:45.823021 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89178 (* 1 = 8.89178 loss)
I0522 23:12:46.492343 34682 sgd_solver.cpp:112] Iteration 10590, lr = 0.01
I0522 23:12:50.567116 34682 solver.cpp:239] Iteration 10600 (2.10797 iter/s, 4.7439s/10 iters), loss = 8.91475
I0522 23:12:50.567159 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91475 (* 1 = 8.91475 loss)
I0522 23:12:51.437242 34682 sgd_solver.cpp:112] Iteration 10600, lr = 0.01
I0522 23:12:54.619024 34682 solver.cpp:239] Iteration 10610 (2.46811 iter/s, 4.05169s/10 iters), loss = 9.30731
I0522 23:12:54.619073 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30731 (* 1 = 9.30731 loss)
I0522 23:12:54.687126 34682 sgd_solver.cpp:112] Iteration 10610, lr = 0.01
I0522 23:12:59.071132 34682 solver.cpp:239] Iteration 10620 (2.24625 iter/s, 4.45187s/10 iters), loss = 8.83005
I0522 23:12:59.071333 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83005 (* 1 = 8.83005 loss)
I0522 23:12:59.134377 34682 sgd_solver.cpp:112] Iteration 10620, lr = 0.01
I0522 23:13:05.655071 34682 solver.cpp:239] Iteration 10630 (1.51895 iter/s, 6.5835s/10 iters), loss = 8.97543
I0522 23:13:05.655124 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97543 (* 1 = 8.97543 loss)
I0522 23:13:05.724439 34682 sgd_solver.cpp:112] Iteration 10630, lr = 0.01
I0522 23:13:10.616549 34682 solver.cpp:239] Iteration 10640 (2.01563 iter/s, 4.96122s/10 iters), loss = 8.67366
I0522 23:13:10.616592 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67366 (* 1 = 8.67366 loss)
I0522 23:13:10.697173 34682 sgd_solver.cpp:112] Iteration 10640, lr = 0.01
I0522 23:13:14.612934 34682 solver.cpp:239] Iteration 10650 (2.5024 iter/s, 3.99617s/10 iters), loss = 8.34503
I0522 23:13:14.612998 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34503 (* 1 = 8.34503 loss)
I0522 23:13:15.349861 34682 sgd_solver.cpp:112] Iteration 10650, lr = 0.01
I0522 23:13:19.173588 34682 solver.cpp:239] Iteration 10660 (2.1928 iter/s, 4.56039s/10 iters), loss = 9.50151
I0522 23:13:19.173645 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.50151 (* 1 = 9.50151 loss)
I0522 23:13:20.016360 34682 sgd_solver.cpp:112] Iteration 10660, lr = 0.01
I0522 23:13:25.766538 34682 solver.cpp:239] Iteration 10670 (1.51685 iter/s, 6.59263s/10 iters), loss = 8.32608
I0522 23:13:25.766593 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32608 (* 1 = 8.32608 loss)
I0522 23:13:26.388406 34682 sgd_solver.cpp:112] Iteration 10670, lr = 0.01
I0522 23:13:30.538332 34682 solver.cpp:239] Iteration 10680 (2.09576 iter/s, 4.77154s/10 iters), loss = 9.15908
I0522 23:13:30.538630 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15908 (* 1 = 9.15908 loss)
I0522 23:13:30.616160 34682 sgd_solver.cpp:112] Iteration 10680, lr = 0.01
I0522 23:13:36.974432 34682 solver.cpp:239] Iteration 10690 (1.55386 iter/s, 6.43558s/10 iters), loss = 9.65092
I0522 23:13:36.974503 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.65092 (* 1 = 9.65092 loss)
I0522 23:13:37.029893 34682 sgd_solver.cpp:112] Iteration 10690, lr = 0.01
I0522 23:13:41.900658 34682 solver.cpp:239] Iteration 10700 (2.03006 iter/s, 4.92596s/10 iters), loss = 8.341
I0522 23:13:41.900710 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.341 (* 1 = 8.341 loss)
I0522 23:13:41.964503 34682 sgd_solver.cpp:112] Iteration 10700, lr = 0.01
I0522 23:13:46.160995 34682 solver.cpp:239] Iteration 10710 (2.34736 iter/s, 4.26011s/10 iters), loss = 9.45096
I0522 23:13:46.161036 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.45096 (* 1 = 9.45096 loss)
I0522 23:13:46.232519 34682 sgd_solver.cpp:112] Iteration 10710, lr = 0.01
I0522 23:13:51.155395 34682 solver.cpp:239] Iteration 10720 (2.00235 iter/s, 4.99414s/10 iters), loss = 8.91421
I0522 23:13:51.155447 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91421 (* 1 = 8.91421 loss)
I0522 23:13:51.226887 34682 sgd_solver.cpp:112] Iteration 10720, lr = 0.01
I0522 23:13:54.660079 34682 solver.cpp:239] Iteration 10730 (2.85349 iter/s, 3.50449s/10 iters), loss = 9.67614
I0522 23:13:54.660143 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.67614 (* 1 = 9.67614 loss)
I0522 23:13:55.327754 34682 sgd_solver.cpp:112] Iteration 10730, lr = 0.01
I0522 23:13:58.717134 34682 solver.cpp:239] Iteration 10740 (2.46498 iter/s, 4.05682s/10 iters), loss = 9.22828
I0522 23:13:58.717191 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22828 (* 1 = 9.22828 loss)
I0522 23:13:59.365857 34682 sgd_solver.cpp:112] Iteration 10740, lr = 0.01
I0522 23:14:04.299332 34682 solver.cpp:239] Iteration 10750 (1.7915 iter/s, 5.5819s/10 iters), loss = 8.44544
I0522 23:14:04.299574 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44544 (* 1 = 8.44544 loss)
I0522 23:14:04.373275 34682 sgd_solver.cpp:112] Iteration 10750, lr = 0.01
I0522 23:14:10.551045 34682 solver.cpp:239] Iteration 10760 (1.59968 iter/s, 6.25125s/10 iters), loss = 9.22526
I0522 23:14:10.551100 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22526 (* 1 = 9.22526 loss)
I0522 23:14:11.392549 34682 sgd_solver.cpp:112] Iteration 10760, lr = 0.01
I0522 23:14:16.910959 34682 solver.cpp:239] Iteration 10770 (1.57243 iter/s, 6.35957s/10 iters), loss = 9.67153
I0522 23:14:16.911022 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.67153 (* 1 = 9.67153 loss)
I0522 23:14:16.965986 34682 sgd_solver.cpp:112] Iteration 10770, lr = 0.01
I0522 23:14:21.563318 34682 solver.cpp:239] Iteration 10780 (2.14956 iter/s, 4.6521s/10 iters), loss = 8.93495
I0522 23:14:21.563385 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93495 (* 1 = 8.93495 loss)
I0522 23:14:21.631479 34682 sgd_solver.cpp:112] Iteration 10780, lr = 0.01
I0522 23:14:25.835764 34682 solver.cpp:239] Iteration 10790 (2.34071 iter/s, 4.2722s/10 iters), loss = 9.47853
I0522 23:14:25.835808 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47853 (* 1 = 9.47853 loss)
I0522 23:14:25.896399 34682 sgd_solver.cpp:112] Iteration 10790, lr = 0.01
I0522 23:14:30.703604 34682 solver.cpp:239] Iteration 10800 (2.05441 iter/s, 4.86759s/10 iters), loss = 9.31534
I0522 23:14:30.703661 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31534 (* 1 = 9.31534 loss)
I0522 23:14:31.505765 34682 sgd_solver.cpp:112] Iteration 10800, lr = 0.01
I0522 23:14:35.383613 34682 solver.cpp:239] Iteration 10810 (2.13686 iter/s, 4.67976s/10 iters), loss = 9.52159
I0522 23:14:35.383776 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.52159 (* 1 = 9.52159 loss)
I0522 23:14:35.449823 34682 sgd_solver.cpp:112] Iteration 10810, lr = 0.01
I0522 23:14:39.613844 34682 solver.cpp:239] Iteration 10820 (2.36413 iter/s, 4.22989s/10 iters), loss = 8.82238
I0522 23:14:39.613893 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82238 (* 1 = 8.82238 loss)
I0522 23:14:39.677073 34682 sgd_solver.cpp:112] Iteration 10820, lr = 0.01
I0522 23:14:43.657729 34682 solver.cpp:239] Iteration 10830 (2.473 iter/s, 4.04367s/10 iters), loss = 9.19263
I0522 23:14:43.657773 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19263 (* 1 = 9.19263 loss)
I0522 23:14:43.722398 34682 sgd_solver.cpp:112] Iteration 10830, lr = 0.01
I0522 23:14:47.815737 34682 solver.cpp:239] Iteration 10840 (2.40513 iter/s, 4.15778s/10 iters), loss = 9.3135
I0522 23:14:47.815806 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3135 (* 1 = 9.3135 loss)
I0522 23:14:48.563429 34682 sgd_solver.cpp:112] Iteration 10840, lr = 0.01
I0522 23:14:53.563602 34682 solver.cpp:239] Iteration 10850 (1.73987 iter/s, 5.74757s/10 iters), loss = 8.91352
I0522 23:14:53.563658 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91352 (* 1 = 8.91352 loss)
I0522 23:14:54.183722 34682 sgd_solver.cpp:112] Iteration 10850, lr = 0.01
I0522 23:15:00.362974 34682 solver.cpp:239] Iteration 10860 (1.4708 iter/s, 6.79903s/10 iters), loss = 8.72432
I0522 23:15:00.363029 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72432 (* 1 = 8.72432 loss)
I0522 23:15:00.426427 34682 sgd_solver.cpp:112] Iteration 10860, lr = 0.01
I0522 23:15:04.474972 34682 solver.cpp:239] Iteration 10870 (2.43204 iter/s, 4.11177s/10 iters), loss = 8.45266
I0522 23:15:04.475029 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45266 (* 1 = 8.45266 loss)
I0522 23:15:04.532249 34682 sgd_solver.cpp:112] Iteration 10870, lr = 0.01
I0522 23:15:09.990748 34682 solver.cpp:239] Iteration 10880 (1.81307 iter/s, 5.51549s/10 iters), loss = 8.93757
I0522 23:15:09.990895 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93757 (* 1 = 8.93757 loss)
I0522 23:15:10.067214 34682 sgd_solver.cpp:112] Iteration 10880, lr = 0.01
I0522 23:15:13.456943 34682 solver.cpp:239] Iteration 10890 (2.88526 iter/s, 3.46589s/10 iters), loss = 8.95831
I0522 23:15:13.456990 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95831 (* 1 = 8.95831 loss)
I0522 23:15:14.334600 34682 sgd_solver.cpp:112] Iteration 10890, lr = 0.01
I0522 23:15:19.747051 34682 solver.cpp:239] Iteration 10900 (1.58988 iter/s, 6.2898s/10 iters), loss = 9.21081
I0522 23:15:19.747100 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21081 (* 1 = 9.21081 loss)
I0522 23:15:19.808943 34682 sgd_solver.cpp:112] Iteration 10900, lr = 0.01
I0522 23:15:25.087178 34682 solver.cpp:239] Iteration 10910 (1.87271 iter/s, 5.33984s/10 iters), loss = 9.60666
I0522 23:15:25.087236 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60666 (* 1 = 9.60666 loss)
I0522 23:15:25.156410 34682 sgd_solver.cpp:112] Iteration 10910, lr = 0.01
I0522 23:15:29.981984 34682 solver.cpp:239] Iteration 10920 (2.04309 iter/s, 4.89455s/10 iters), loss = 8.83721
I0522 23:15:29.982038 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83721 (* 1 = 8.83721 loss)
I0522 23:15:30.517791 34682 sgd_solver.cpp:112] Iteration 10920, lr = 0.01
I0522 23:15:35.963423 34682 solver.cpp:239] Iteration 10930 (1.67192 iter/s, 5.98114s/10 iters), loss = 8.73296
I0522 23:15:35.963480 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73296 (* 1 = 8.73296 loss)
I0522 23:15:36.610973 34682 sgd_solver.cpp:112] Iteration 10930, lr = 0.01
I0522 23:15:40.592389 34682 solver.cpp:239] Iteration 10940 (2.16043 iter/s, 4.62871s/10 iters), loss = 8.81838
I0522 23:15:40.592684 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81838 (* 1 = 8.81838 loss)
I0522 23:15:41.315448 34682 sgd_solver.cpp:112] Iteration 10940, lr = 0.01
I0522 23:15:46.863160 34682 solver.cpp:239] Iteration 10950 (1.59483 iter/s, 6.27026s/10 iters), loss = 9.20893
I0522 23:15:46.863209 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20893 (* 1 = 9.20893 loss)
I0522 23:15:47.044287 34682 sgd_solver.cpp:112] Iteration 10950, lr = 0.01
I0522 23:15:51.831807 34682 solver.cpp:239] Iteration 10960 (2.01272 iter/s, 4.9684s/10 iters), loss = 8.27847
I0522 23:15:51.831861 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27847 (* 1 = 8.27847 loss)
I0522 23:15:52.675045 34682 sgd_solver.cpp:112] Iteration 10960, lr = 0.01
I0522 23:15:58.239723 34682 solver.cpp:239] Iteration 10970 (1.56065 iter/s, 6.4076s/10 iters), loss = 9.28897
I0522 23:15:58.239776 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28897 (* 1 = 9.28897 loss)
I0522 23:15:58.747097 34682 sgd_solver.cpp:112] Iteration 10970, lr = 0.01
I0522 23:16:03.881572 34682 solver.cpp:239] Iteration 10980 (1.77256 iter/s, 5.64157s/10 iters), loss = 9.51198
I0522 23:16:03.881621 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51198 (* 1 = 9.51198 loss)
I0522 23:16:03.943711 34682 sgd_solver.cpp:112] Iteration 10980, lr = 0.01
I0522 23:16:08.590795 34682 solver.cpp:239] Iteration 10990 (2.1236 iter/s, 4.70898s/10 iters), loss = 8.74011
I0522 23:16:08.590867 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74011 (* 1 = 8.74011 loss)
I0522 23:16:08.662806 34682 sgd_solver.cpp:112] Iteration 10990, lr = 0.01
I0522 23:16:12.825908 34682 solver.cpp:239] Iteration 11000 (2.36136 iter/s, 4.23485s/10 iters), loss = 8.90154
I0522 23:16:12.826177 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90154 (* 1 = 8.90154 loss)
I0522 23:16:13.467155 34682 sgd_solver.cpp:112] Iteration 11000, lr = 0.01
I0522 23:16:16.379878 34682 solver.cpp:239] Iteration 11010 (2.81407 iter/s, 3.55357s/10 iters), loss = 8.91352
I0522 23:16:16.380012 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91352 (* 1 = 8.91352 loss)
I0522 23:16:16.417402 34682 sgd_solver.cpp:112] Iteration 11010, lr = 0.01
I0522 23:16:17.691373 34682 solver.cpp:239] Iteration 11020 (7.626 iter/s, 1.3113s/10 iters), loss = 8.84768
I0522 23:16:17.691421 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84768 (* 1 = 8.84768 loss)
I0522 23:16:17.731034 34682 sgd_solver.cpp:112] Iteration 11020, lr = 0.01
I0522 23:16:20.955406 34682 solver.cpp:239] Iteration 11030 (3.06396 iter/s, 3.26375s/10 iters), loss = 9.93602
I0522 23:16:20.955457 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.93602 (* 1 = 9.93602 loss)
I0522 23:16:21.024292 34682 sgd_solver.cpp:112] Iteration 11030, lr = 0.01
I0522 23:16:25.907353 34682 solver.cpp:239] Iteration 11040 (2.01951 iter/s, 4.95169s/10 iters), loss = 9.32011
I0522 23:16:25.907397 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32011 (* 1 = 9.32011 loss)
I0522 23:16:25.982305 34682 sgd_solver.cpp:112] Iteration 11040, lr = 0.01
I0522 23:16:32.499297 34682 solver.cpp:239] Iteration 11050 (1.51708 iter/s, 6.59163s/10 iters), loss = 8.85767
I0522 23:16:32.499348 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85767 (* 1 = 8.85767 loss)
I0522 23:16:32.570497 34682 sgd_solver.cpp:112] Iteration 11050, lr = 0.01
I0522 23:16:37.124915 34682 solver.cpp:239] Iteration 11060 (2.16199 iter/s, 4.62538s/10 iters), loss = 8.91575
I0522 23:16:37.124976 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91575 (* 1 = 8.91575 loss)
I0522 23:16:37.195415 34682 sgd_solver.cpp:112] Iteration 11060, lr = 0.01
I0522 23:16:42.005123 34682 solver.cpp:239] Iteration 11070 (2.04921 iter/s, 4.87993s/10 iters), loss = 9.08009
I0522 23:16:42.005195 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08009 (* 1 = 9.08009 loss)
I0522 23:16:42.078248 34682 sgd_solver.cpp:112] Iteration 11070, lr = 0.01
I0522 23:16:46.194085 34682 solver.cpp:239] Iteration 11080 (2.38737 iter/s, 4.18871s/10 iters), loss = 8.82121
I0522 23:16:46.194380 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82121 (* 1 = 8.82121 loss)
I0522 23:16:46.850253 34682 sgd_solver.cpp:112] Iteration 11080, lr = 0.01
I0522 23:16:50.942010 34682 solver.cpp:239] Iteration 11090 (2.10639 iter/s, 4.74745s/10 iters), loss = 9.50423
I0522 23:16:50.942080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.50423 (* 1 = 9.50423 loss)
I0522 23:16:51.530704 34682 sgd_solver.cpp:112] Iteration 11090, lr = 0.01
I0522 23:16:57.932473 34682 solver.cpp:239] Iteration 11100 (1.43059 iter/s, 6.99012s/10 iters), loss = 9.33992
I0522 23:16:57.932526 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33992 (* 1 = 9.33992 loss)
I0522 23:16:58.047178 34682 sgd_solver.cpp:112] Iteration 11100, lr = 0.01
I0522 23:17:01.235183 34682 solver.cpp:239] Iteration 11110 (3.02799 iter/s, 3.30252s/10 iters), loss = 9.24831
I0522 23:17:01.235232 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24831 (* 1 = 9.24831 loss)
I0522 23:17:01.304782 34682 sgd_solver.cpp:112] Iteration 11110, lr = 0.01
I0522 23:17:06.218880 34682 solver.cpp:239] Iteration 11120 (2.00664 iter/s, 4.98345s/10 iters), loss = 9.73342
I0522 23:17:06.218930 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.73342 (* 1 = 9.73342 loss)
I0522 23:17:06.292608 34682 sgd_solver.cpp:112] Iteration 11120, lr = 0.01
I0522 23:17:12.118438 34682 solver.cpp:239] Iteration 11130 (1.69513 iter/s, 5.89926s/10 iters), loss = 9.72718
I0522 23:17:12.118500 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.72718 (* 1 = 9.72718 loss)
I0522 23:17:12.887349 34682 sgd_solver.cpp:112] Iteration 11130, lr = 0.01
I0522 23:17:17.609163 34682 solver.cpp:239] Iteration 11140 (1.82135 iter/s, 5.49045s/10 iters), loss = 8.94545
I0522 23:17:17.609283 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94545 (* 1 = 8.94545 loss)
I0522 23:17:18.326078 34682 sgd_solver.cpp:112] Iteration 11140, lr = 0.01
I0522 23:17:22.923024 34682 solver.cpp:239] Iteration 11150 (1.88199 iter/s, 5.31352s/10 iters), loss = 9.11949
I0522 23:17:22.923069 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11949 (* 1 = 9.11949 loss)
I0522 23:17:23.593614 34682 sgd_solver.cpp:112] Iteration 11150, lr = 0.01
I0522 23:17:28.273659 34682 solver.cpp:239] Iteration 11160 (1.86903 iter/s, 5.35036s/10 iters), loss = 8.98531
I0522 23:17:28.273712 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98531 (* 1 = 8.98531 loss)
I0522 23:17:28.355665 34682 sgd_solver.cpp:112] Iteration 11160, lr = 0.01
I0522 23:17:32.312985 34682 solver.cpp:239] Iteration 11170 (2.4758 iter/s, 4.0391s/10 iters), loss = 9.91144
I0522 23:17:32.313045 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.91144 (* 1 = 9.91144 loss)
I0522 23:17:32.540136 34682 sgd_solver.cpp:112] Iteration 11170, lr = 0.01
I0522 23:17:35.773144 34682 solver.cpp:239] Iteration 11180 (2.89021 iter/s, 3.45996s/10 iters), loss = 8.8658
I0522 23:17:35.773186 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8658 (* 1 = 8.8658 loss)
I0522 23:17:36.574723 34682 sgd_solver.cpp:112] Iteration 11180, lr = 0.01
I0522 23:17:41.393599 34682 solver.cpp:239] Iteration 11190 (1.7793 iter/s, 5.62017s/10 iters), loss = 8.65869
I0522 23:17:41.393642 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65869 (* 1 = 8.65869 loss)
I0522 23:17:41.471524 34682 sgd_solver.cpp:112] Iteration 11190, lr = 0.01
I0522 23:17:45.137558 34682 solver.cpp:239] Iteration 11200 (2.67111 iter/s, 3.74376s/10 iters), loss = 9.85945
I0522 23:17:45.137601 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.85945 (* 1 = 9.85945 loss)
I0522 23:17:45.998826 34682 sgd_solver.cpp:112] Iteration 11200, lr = 0.01
I0522 23:17:49.573025 34682 solver.cpp:239] Iteration 11210 (2.25467 iter/s, 4.43524s/10 iters), loss = 8.40423
I0522 23:17:49.573285 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40423 (* 1 = 8.40423 loss)
I0522 23:17:49.647635 34682 sgd_solver.cpp:112] Iteration 11210, lr = 0.01
I0522 23:17:52.254091 34682 solver.cpp:239] Iteration 11220 (3.73033 iter/s, 2.68072s/10 iters), loss = 8.83271
I0522 23:17:52.254135 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83271 (* 1 = 8.83271 loss)
I0522 23:17:53.071455 34682 sgd_solver.cpp:112] Iteration 11220, lr = 0.01
I0522 23:17:56.519755 34682 solver.cpp:239] Iteration 11230 (2.34442 iter/s, 4.26544s/10 iters), loss = 8.3674
I0522 23:17:56.519814 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3674 (* 1 = 8.3674 loss)
I0522 23:17:56.587235 34682 sgd_solver.cpp:112] Iteration 11230, lr = 0.01
I0522 23:18:02.896338 34682 solver.cpp:239] Iteration 11240 (1.56831 iter/s, 6.37627s/10 iters), loss = 9.98868
I0522 23:18:02.896389 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.98868 (* 1 = 9.98868 loss)
I0522 23:18:03.702281 34682 sgd_solver.cpp:112] Iteration 11240, lr = 0.01
I0522 23:18:07.620740 34682 solver.cpp:239] Iteration 11250 (2.11678 iter/s, 4.72416s/10 iters), loss = 9.40998
I0522 23:18:07.620791 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40998 (* 1 = 9.40998 loss)
I0522 23:18:07.693579 34682 sgd_solver.cpp:112] Iteration 11250, lr = 0.01
I0522 23:18:13.219533 34682 solver.cpp:239] Iteration 11260 (1.7862 iter/s, 5.59848s/10 iters), loss = 9.20203
I0522 23:18:13.219599 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20203 (* 1 = 9.20203 loss)
I0522 23:18:13.976248 34682 sgd_solver.cpp:112] Iteration 11260, lr = 0.01
I0522 23:18:20.324965 34682 solver.cpp:239] Iteration 11270 (1.40745 iter/s, 7.10507s/10 iters), loss = 9.06145
I0522 23:18:20.325193 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06145 (* 1 = 9.06145 loss)
I0522 23:18:20.401865 34682 sgd_solver.cpp:112] Iteration 11270, lr = 0.01
I0522 23:18:23.525300 34682 solver.cpp:239] Iteration 11280 (3.12501 iter/s, 3.19999s/10 iters), loss = 8.99615
I0522 23:18:23.525362 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99615 (* 1 = 8.99615 loss)
I0522 23:18:23.603893 34682 sgd_solver.cpp:112] Iteration 11280, lr = 0.01
I0522 23:18:27.988023 34682 solver.cpp:239] Iteration 11290 (2.2409 iter/s, 4.46249s/10 iters), loss = 9.67721
I0522 23:18:27.988062 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.67721 (* 1 = 9.67721 loss)
I0522 23:18:28.756428 34682 sgd_solver.cpp:112] Iteration 11290, lr = 0.01
I0522 23:18:33.540184 34682 solver.cpp:239] Iteration 11300 (1.80119 iter/s, 5.55189s/10 iters), loss = 8.32752
I0522 23:18:33.540238 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32752 (* 1 = 8.32752 loss)
I0522 23:18:34.161715 34682 sgd_solver.cpp:112] Iteration 11300, lr = 0.01
I0522 23:18:38.043321 34682 solver.cpp:239] Iteration 11310 (2.2208 iter/s, 4.50289s/10 iters), loss = 9.5065
I0522 23:18:38.043388 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.5065 (* 1 = 9.5065 loss)
I0522 23:18:38.815599 34682 sgd_solver.cpp:112] Iteration 11310, lr = 0.01
I0522 23:18:42.479296 34682 solver.cpp:239] Iteration 11320 (2.25442 iter/s, 4.43573s/10 iters), loss = 8.2465
I0522 23:18:42.479348 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2465 (* 1 = 8.2465 loss)
I0522 23:18:42.539067 34682 sgd_solver.cpp:112] Iteration 11320, lr = 0.01
I0522 23:18:46.721398 34682 solver.cpp:239] Iteration 11330 (2.35745 iter/s, 4.24187s/10 iters), loss = 9.4271
I0522 23:18:46.721443 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4271 (* 1 = 9.4271 loss)
I0522 23:18:46.792598 34682 sgd_solver.cpp:112] Iteration 11330, lr = 0.01
I0522 23:18:50.622359 34682 solver.cpp:239] Iteration 11340 (2.56362 iter/s, 3.90074s/10 iters), loss = 8.87098
I0522 23:18:50.622684 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87098 (* 1 = 8.87098 loss)
I0522 23:18:51.470415 34682 sgd_solver.cpp:112] Iteration 11340, lr = 0.01
I0522 23:18:54.051787 34682 solver.cpp:239] Iteration 11350 (2.9163 iter/s, 3.429s/10 iters), loss = 9.23557
I0522 23:18:54.051836 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23557 (* 1 = 9.23557 loss)
I0522 23:18:54.794668 34682 sgd_solver.cpp:112] Iteration 11350, lr = 0.01
I0522 23:19:00.808524 34682 solver.cpp:239] Iteration 11360 (1.48008 iter/s, 6.75641s/10 iters), loss = 9.07865
I0522 23:19:00.808575 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07865 (* 1 = 9.07865 loss)
I0522 23:19:00.884759 34682 sgd_solver.cpp:112] Iteration 11360, lr = 0.01
I0522 23:19:04.728021 34682 solver.cpp:239] Iteration 11370 (2.55149 iter/s, 3.91928s/10 iters), loss = 9.15638
I0522 23:19:04.728076 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15638 (* 1 = 9.15638 loss)
I0522 23:19:05.486779 34682 sgd_solver.cpp:112] Iteration 11370, lr = 0.01
I0522 23:19:09.357841 34682 solver.cpp:239] Iteration 11380 (2.16003 iter/s, 4.62956s/10 iters), loss = 8.98377
I0522 23:19:09.357899 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98377 (* 1 = 8.98377 loss)
I0522 23:19:10.184090 34682 sgd_solver.cpp:112] Iteration 11380, lr = 0.01
I0522 23:19:13.093009 34682 solver.cpp:239] Iteration 11390 (2.67741 iter/s, 3.73496s/10 iters), loss = 9.09142
I0522 23:19:13.093056 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09142 (* 1 = 9.09142 loss)
I0522 23:19:13.153559 34682 sgd_solver.cpp:112] Iteration 11390, lr = 0.01
I0522 23:19:16.649150 34682 solver.cpp:239] Iteration 11400 (2.81219 iter/s, 3.55594s/10 iters), loss = 9.31798
I0522 23:19:16.649206 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31798 (* 1 = 9.31798 loss)
I0522 23:19:17.044587 34682 sgd_solver.cpp:112] Iteration 11400, lr = 0.01
I0522 23:19:21.148016 34682 solver.cpp:239] Iteration 11410 (2.2229 iter/s, 4.49862s/10 iters), loss = 8.67074
I0522 23:19:21.148231 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67074 (* 1 = 8.67074 loss)
I0522 23:19:21.227134 34682 sgd_solver.cpp:112] Iteration 11410, lr = 0.01
I0522 23:19:24.051766 34682 solver.cpp:239] Iteration 11420 (3.44418 iter/s, 2.90345s/10 iters), loss = 9.75021
I0522 23:19:24.051810 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.75021 (* 1 = 9.75021 loss)
I0522 23:19:24.725662 34682 sgd_solver.cpp:112] Iteration 11420, lr = 0.01
I0522 23:19:28.996198 34682 solver.cpp:239] Iteration 11430 (2.02258 iter/s, 4.94418s/10 iters), loss = 9.2677
I0522 23:19:28.996251 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2677 (* 1 = 9.2677 loss)
I0522 23:19:29.058955 34682 sgd_solver.cpp:112] Iteration 11430, lr = 0.01
I0522 23:19:34.477962 34682 solver.cpp:239] Iteration 11440 (1.82432 iter/s, 5.48149s/10 iters), loss = 9.11856
I0522 23:19:34.478015 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11856 (* 1 = 9.11856 loss)
I0522 23:19:35.149427 34682 sgd_solver.cpp:112] Iteration 11440, lr = 0.01
I0522 23:19:40.825436 34682 solver.cpp:239] Iteration 11450 (1.57551 iter/s, 6.34717s/10 iters), loss = 8.65366
I0522 23:19:40.825485 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65366 (* 1 = 8.65366 loss)
I0522 23:19:40.891082 34682 sgd_solver.cpp:112] Iteration 11450, lr = 0.01
I0522 23:19:46.804561 34682 solver.cpp:239] Iteration 11460 (1.67257 iter/s, 5.97882s/10 iters), loss = 8.89914
I0522 23:19:46.804611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89914 (* 1 = 8.89914 loss)
I0522 23:19:46.872792 34682 sgd_solver.cpp:112] Iteration 11460, lr = 0.01
I0522 23:19:52.530472 34682 solver.cpp:239] Iteration 11470 (1.74654 iter/s, 5.72561s/10 iters), loss = 9.58899
I0522 23:19:52.530818 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.58899 (* 1 = 9.58899 loss)
I0522 23:19:53.152431 34682 sgd_solver.cpp:112] Iteration 11470, lr = 0.01
I0522 23:19:59.795686 34682 solver.cpp:239] Iteration 11480 (1.37653 iter/s, 7.26462s/10 iters), loss = 8.28157
I0522 23:19:59.795737 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28157 (* 1 = 8.28157 loss)
I0522 23:20:00.619781 34682 sgd_solver.cpp:112] Iteration 11480, lr = 0.01
I0522 23:20:07.150775 34682 solver.cpp:239] Iteration 11490 (1.35967 iter/s, 7.35473s/10 iters), loss = 8.97962
I0522 23:20:07.150852 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97962 (* 1 = 8.97962 loss)
I0522 23:20:07.892144 34682 sgd_solver.cpp:112] Iteration 11490, lr = 0.01
I0522 23:20:12.029155 34682 solver.cpp:239] Iteration 11500 (2.04998 iter/s, 4.8781s/10 iters), loss = 9.0033
I0522 23:20:12.029201 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0033 (* 1 = 9.0033 loss)
I0522 23:20:12.512137 34682 sgd_solver.cpp:112] Iteration 11500, lr = 0.01
I0522 23:20:16.481037 34682 solver.cpp:239] Iteration 11510 (2.24636 iter/s, 4.45165s/10 iters), loss = 8.33804
I0522 23:20:16.481087 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33804 (* 1 = 8.33804 loss)
I0522 23:20:16.547732 34682 sgd_solver.cpp:112] Iteration 11510, lr = 0.01
I0522 23:20:21.884608 34682 solver.cpp:239] Iteration 11520 (1.85072 iter/s, 5.4033s/10 iters), loss = 8.55181
I0522 23:20:21.884670 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55181 (* 1 = 8.55181 loss)
I0522 23:20:22.684301 34682 sgd_solver.cpp:112] Iteration 11520, lr = 0.01
I0522 23:20:26.580621 34682 solver.cpp:239] Iteration 11530 (2.12958 iter/s, 4.69576s/10 iters), loss = 8.559
I0522 23:20:26.580672 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.559 (* 1 = 8.559 loss)
I0522 23:20:26.647845 34682 sgd_solver.cpp:112] Iteration 11530, lr = 0.01
I0522 23:20:29.140961 34682 solver.cpp:239] Iteration 11540 (3.90597 iter/s, 2.56018s/10 iters), loss = 9.50805
I0522 23:20:29.141010 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.50805 (* 1 = 9.50805 loss)
I0522 23:20:29.675848 34682 sgd_solver.cpp:112] Iteration 11540, lr = 0.01
I0522 23:20:34.157356 34682 solver.cpp:239] Iteration 11550 (1.99357 iter/s, 5.01613s/10 iters), loss = 8.87831
I0522 23:20:34.157415 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87831 (* 1 = 8.87831 loss)
I0522 23:20:34.222566 34682 sgd_solver.cpp:112] Iteration 11550, lr = 0.01
I0522 23:20:39.875509 34682 solver.cpp:239] Iteration 11560 (1.74891 iter/s, 5.71785s/10 iters), loss = 9.58639
I0522 23:20:39.875564 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.58639 (* 1 = 9.58639 loss)
I0522 23:20:39.937489 34682 sgd_solver.cpp:112] Iteration 11560, lr = 0.01
I0522 23:20:42.825779 34682 solver.cpp:239] Iteration 11570 (3.38974 iter/s, 2.95008s/10 iters), loss = 8.90015
I0522 23:20:42.825844 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90015 (* 1 = 8.90015 loss)
I0522 23:20:43.646427 34682 sgd_solver.cpp:112] Iteration 11570, lr = 0.01
I0522 23:20:47.328068 34682 solver.cpp:239] Iteration 11580 (2.22122 iter/s, 4.50203s/10 iters), loss = 8.86446
I0522 23:20:47.328112 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86446 (* 1 = 8.86446 loss)
I0522 23:20:47.390429 34682 sgd_solver.cpp:112] Iteration 11580, lr = 0.01
I0522 23:20:52.991812 34682 solver.cpp:239] Iteration 11590 (1.7657 iter/s, 5.66346s/10 iters), loss = 8.75311
I0522 23:20:52.992082 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75311 (* 1 = 8.75311 loss)
I0522 23:20:53.055580 34682 sgd_solver.cpp:112] Iteration 11590, lr = 0.01
I0522 23:20:58.655493 34682 solver.cpp:239] Iteration 11600 (1.76578 iter/s, 5.66321s/10 iters), loss = 8.89574
I0522 23:20:58.655557 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89574 (* 1 = 8.89574 loss)
I0522 23:20:59.330972 34682 sgd_solver.cpp:112] Iteration 11600, lr = 0.01
I0522 23:21:04.905863 34682 solver.cpp:239] Iteration 11610 (1.59999 iter/s, 6.25005s/10 iters), loss = 8.71507
I0522 23:21:04.905915 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71507 (* 1 = 8.71507 loss)
I0522 23:21:05.684590 34682 sgd_solver.cpp:112] Iteration 11610, lr = 0.01
I0522 23:21:09.619134 34682 solver.cpp:239] Iteration 11620 (2.12178 iter/s, 4.71302s/10 iters), loss = 9.05818
I0522 23:21:09.619185 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05818 (* 1 = 9.05818 loss)
I0522 23:21:09.934003 34682 sgd_solver.cpp:112] Iteration 11620, lr = 0.01
I0522 23:21:13.264029 34682 solver.cpp:239] Iteration 11630 (2.74372 iter/s, 3.64469s/10 iters), loss = 9.40479
I0522 23:21:13.264071 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40479 (* 1 = 9.40479 loss)
I0522 23:21:13.350560 34682 sgd_solver.cpp:112] Iteration 11630, lr = 0.01
I0522 23:21:17.318636 34682 solver.cpp:239] Iteration 11640 (2.46646 iter/s, 4.05439s/10 iters), loss = 8.64514
I0522 23:21:17.318724 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64514 (* 1 = 8.64514 loss)
I0522 23:21:18.189910 34682 sgd_solver.cpp:112] Iteration 11640, lr = 0.01
I0522 23:21:24.102926 34682 solver.cpp:239] Iteration 11650 (1.47407 iter/s, 6.78395s/10 iters), loss = 8.96846
I0522 23:21:24.103174 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96846 (* 1 = 8.96846 loss)
I0522 23:21:24.166576 34682 sgd_solver.cpp:112] Iteration 11650, lr = 0.01
I0522 23:21:27.558382 34682 solver.cpp:239] Iteration 11660 (2.8943 iter/s, 3.45506s/10 iters), loss = 8.83056
I0522 23:21:27.558437 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83056 (* 1 = 8.83056 loss)
I0522 23:21:28.358888 34682 sgd_solver.cpp:112] Iteration 11660, lr = 0.01
I0522 23:21:33.222084 34682 solver.cpp:239] Iteration 11670 (1.76572 iter/s, 5.66341s/10 iters), loss = 9.13339
I0522 23:21:33.222141 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13339 (* 1 = 9.13339 loss)
I0522 23:21:34.116375 34682 sgd_solver.cpp:112] Iteration 11670, lr = 0.01
I0522 23:21:37.626543 34682 solver.cpp:239] Iteration 11680 (2.27056 iter/s, 4.40421s/10 iters), loss = 9.63916
I0522 23:21:37.626602 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.63916 (* 1 = 9.63916 loss)
I0522 23:21:37.696164 34682 sgd_solver.cpp:112] Iteration 11680, lr = 0.01
I0522 23:21:41.460299 34682 solver.cpp:239] Iteration 11690 (2.60856 iter/s, 3.83354s/10 iters), loss = 9.57549
I0522 23:21:41.460341 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.57549 (* 1 = 9.57549 loss)
I0522 23:21:42.256716 34682 sgd_solver.cpp:112] Iteration 11690, lr = 0.01
I0522 23:21:45.479820 34682 solver.cpp:239] Iteration 11700 (2.488 iter/s, 4.0193s/10 iters), loss = 9.3237
I0522 23:21:45.479872 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3237 (* 1 = 9.3237 loss)
I0522 23:21:45.540402 34682 sgd_solver.cpp:112] Iteration 11700, lr = 0.01
I0522 23:21:51.132220 34682 solver.cpp:239] Iteration 11710 (1.76925 iter/s, 5.65212s/10 iters), loss = 8.98996
I0522 23:21:51.132269 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98996 (* 1 = 8.98996 loss)
I0522 23:21:51.208119 34682 sgd_solver.cpp:112] Iteration 11710, lr = 0.01
I0522 23:21:54.417654 34682 solver.cpp:239] Iteration 11720 (3.04391 iter/s, 3.28525s/10 iters), loss = 9.21211
I0522 23:21:54.417867 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21211 (* 1 = 9.21211 loss)
I0522 23:21:54.481891 34682 sgd_solver.cpp:112] Iteration 11720, lr = 0.01
I0522 23:21:58.895128 34682 solver.cpp:239] Iteration 11730 (2.2336 iter/s, 4.47707s/10 iters), loss = 9.3878
I0522 23:21:58.895189 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3878 (* 1 = 9.3878 loss)
I0522 23:21:58.965456 34682 sgd_solver.cpp:112] Iteration 11730, lr = 0.01
I0522 23:22:03.094300 34682 solver.cpp:239] Iteration 11740 (2.38156 iter/s, 4.19893s/10 iters), loss = 8.86102
I0522 23:22:03.094348 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86102 (* 1 = 8.86102 loss)
I0522 23:22:03.151173 34682 sgd_solver.cpp:112] Iteration 11740, lr = 0.01
I0522 23:22:08.751868 34682 solver.cpp:239] Iteration 11750 (1.76763 iter/s, 5.65729s/10 iters), loss = 9.2875
I0522 23:22:08.751919 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2875 (* 1 = 9.2875 loss)
I0522 23:22:09.327556 34682 sgd_solver.cpp:112] Iteration 11750, lr = 0.01
I0522 23:22:12.788137 34682 solver.cpp:239] Iteration 11760 (2.47767 iter/s, 4.03606s/10 iters), loss = 9.55845
I0522 23:22:12.788182 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.55845 (* 1 = 9.55845 loss)
I0522 23:22:13.499784 34682 sgd_solver.cpp:112] Iteration 11760, lr = 0.01
I0522 23:22:16.600566 34682 solver.cpp:239] Iteration 11770 (2.62314 iter/s, 3.81222s/10 iters), loss = 8.41799
I0522 23:22:16.600620 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41799 (* 1 = 8.41799 loss)
I0522 23:22:16.673825 34682 sgd_solver.cpp:112] Iteration 11770, lr = 0.01
I0522 23:22:21.668797 34682 solver.cpp:239] Iteration 11780 (1.97318 iter/s, 5.06796s/10 iters), loss = 8.87655
I0522 23:22:21.668869 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87655 (* 1 = 8.87655 loss)
I0522 23:22:22.442173 34682 sgd_solver.cpp:112] Iteration 11780, lr = 0.01
I0522 23:22:29.154194 34682 solver.cpp:239] Iteration 11790 (1.336 iter/s, 7.48503s/10 iters), loss = 8.96815
I0522 23:22:29.154305 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96815 (* 1 = 8.96815 loss)
I0522 23:22:29.215441 34682 sgd_solver.cpp:112] Iteration 11790, lr = 0.01
I0522 23:22:34.133257 34682 solver.cpp:239] Iteration 11800 (2.00854 iter/s, 4.97875s/10 iters), loss = 9.00253
I0522 23:22:34.133306 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00253 (* 1 = 9.00253 loss)
I0522 23:22:34.211436 34682 sgd_solver.cpp:112] Iteration 11800, lr = 0.01
I0522 23:22:39.489549 34682 solver.cpp:239] Iteration 11810 (1.86706 iter/s, 5.35603s/10 iters), loss = 8.35795
I0522 23:22:39.489606 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35795 (* 1 = 8.35795 loss)
I0522 23:22:39.566555 34682 sgd_solver.cpp:112] Iteration 11810, lr = 0.01
I0522 23:22:45.404999 34682 solver.cpp:239] Iteration 11820 (1.69088 iter/s, 5.91407s/10 iters), loss = 9.28598
I0522 23:22:45.405058 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28598 (* 1 = 9.28598 loss)
I0522 23:22:46.229352 34682 sgd_solver.cpp:112] Iteration 11820, lr = 0.01
I0522 23:22:50.615124 34682 solver.cpp:239] Iteration 11830 (1.91944 iter/s, 5.20986s/10 iters), loss = 8.35265
I0522 23:22:50.615175 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35265 (* 1 = 8.35265 loss)
I0522 23:22:50.673605 34682 sgd_solver.cpp:112] Iteration 11830, lr = 0.01
I0522 23:22:54.564158 34682 solver.cpp:239] Iteration 11840 (2.5324 iter/s, 3.94882s/10 iters), loss = 8.97731
I0522 23:22:54.564208 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97731 (* 1 = 8.97731 loss)
I0522 23:22:54.627776 34682 sgd_solver.cpp:112] Iteration 11840, lr = 0.01
I0522 23:22:59.016840 34682 solver.cpp:239] Iteration 11850 (2.24595 iter/s, 4.45245s/10 iters), loss = 8.6326
I0522 23:22:59.016887 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6326 (* 1 = 8.6326 loss)
I0522 23:22:59.734284 34682 sgd_solver.cpp:112] Iteration 11850, lr = 0.01
I0522 23:23:02.957476 34682 solver.cpp:239] Iteration 11860 (2.53779 iter/s, 3.94043s/10 iters), loss = 8.87062
I0522 23:23:02.957525 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87062 (* 1 = 8.87062 loss)
I0522 23:23:03.672637 34682 sgd_solver.cpp:112] Iteration 11860, lr = 0.01
I0522 23:23:06.997413 34682 solver.cpp:239] Iteration 11870 (2.47542 iter/s, 4.03972s/10 iters), loss = 8.37752
I0522 23:23:06.997458 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37752 (* 1 = 8.37752 loss)
I0522 23:23:07.067376 34682 sgd_solver.cpp:112] Iteration 11870, lr = 0.01
I0522 23:23:12.217996 34682 solver.cpp:239] Iteration 11880 (1.91559 iter/s, 5.22033s/10 iters), loss = 8.48465
I0522 23:23:12.218039 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48465 (* 1 = 8.48465 loss)
I0522 23:23:12.281880 34682 sgd_solver.cpp:112] Iteration 11880, lr = 0.01
I0522 23:23:17.581370 34682 solver.cpp:239] Iteration 11890 (1.86459 iter/s, 5.36311s/10 iters), loss = 8.27239
I0522 23:23:17.581419 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27239 (* 1 = 8.27239 loss)
I0522 23:23:17.647130 34682 sgd_solver.cpp:112] Iteration 11890, lr = 0.01
I0522 23:23:21.101877 34682 solver.cpp:239] Iteration 11900 (2.84067 iter/s, 3.5203s/10 iters), loss = 8.45241
I0522 23:23:21.101935 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45241 (* 1 = 8.45241 loss)
I0522 23:23:21.174424 34682 sgd_solver.cpp:112] Iteration 11900, lr = 0.01
I0522 23:23:26.470649 34682 solver.cpp:239] Iteration 11910 (1.86272 iter/s, 5.3685s/10 iters), loss = 9.27787
I0522 23:23:26.470719 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27787 (* 1 = 9.27787 loss)
I0522 23:23:27.285748 34682 sgd_solver.cpp:112] Iteration 11910, lr = 0.01
I0522 23:23:32.866623 34682 solver.cpp:239] Iteration 11920 (1.56356 iter/s, 6.39565s/10 iters), loss = 8.94593
I0522 23:23:32.866873 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94593 (* 1 = 8.94593 loss)
I0522 23:23:32.934132 34682 sgd_solver.cpp:112] Iteration 11920, lr = 0.01
I0522 23:23:36.519333 34682 solver.cpp:239] Iteration 11930 (2.73798 iter/s, 3.65233s/10 iters), loss = 8.84814
I0522 23:23:36.519392 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84814 (* 1 = 8.84814 loss)
I0522 23:23:36.588157 34682 sgd_solver.cpp:112] Iteration 11930, lr = 0.01
I0522 23:23:41.131597 34682 solver.cpp:239] Iteration 11940 (2.16825 iter/s, 4.61201s/10 iters), loss = 9.5004
I0522 23:23:41.131649 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.5004 (* 1 = 9.5004 loss)
I0522 23:23:41.192121 34682 sgd_solver.cpp:112] Iteration 11940, lr = 0.01
I0522 23:23:43.827172 34682 solver.cpp:239] Iteration 11950 (3.71001 iter/s, 2.69541s/10 iters), loss = 8.95067
I0522 23:23:43.827216 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95067 (* 1 = 8.95067 loss)
I0522 23:23:44.607285 34682 sgd_solver.cpp:112] Iteration 11950, lr = 0.01
I0522 23:23:47.567734 34682 solver.cpp:239] Iteration 11960 (2.67355 iter/s, 3.74035s/10 iters), loss = 8.78023
I0522 23:23:47.567800 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78023 (* 1 = 8.78023 loss)
I0522 23:23:48.280635 34682 sgd_solver.cpp:112] Iteration 11960, lr = 0.01
I0522 23:23:51.011319 34682 solver.cpp:239] Iteration 11970 (2.90412 iter/s, 3.44338s/10 iters), loss = 8.97183
I0522 23:23:51.011363 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97183 (* 1 = 8.97183 loss)
I0522 23:23:51.079854 34682 sgd_solver.cpp:112] Iteration 11970, lr = 0.01
I0522 23:23:55.733481 34682 solver.cpp:239] Iteration 11980 (2.11779 iter/s, 4.72191s/10 iters), loss = 9.58328
I0522 23:23:55.733551 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.58328 (* 1 = 9.58328 loss)
I0522 23:23:56.579316 34682 sgd_solver.cpp:112] Iteration 11980, lr = 0.01
I0522 23:24:01.668301 34682 solver.cpp:239] Iteration 11990 (1.68506 iter/s, 5.93451s/10 iters), loss = 8.80547
I0522 23:24:01.668349 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80547 (* 1 = 8.80547 loss)
I0522 23:24:01.736224 34682 sgd_solver.cpp:112] Iteration 11990, lr = 0.01
I0522 23:24:06.655925 34682 solver.cpp:239] Iteration 12000 (2.00506 iter/s, 4.98737s/10 iters), loss = 8.67209
I0522 23:24:06.656147 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67209 (* 1 = 8.67209 loss)
I0522 23:24:07.477155 34682 sgd_solver.cpp:112] Iteration 12000, lr = 0.01
I0522 23:24:12.262507 34682 solver.cpp:239] Iteration 12010 (1.78376 iter/s, 5.60615s/10 iters), loss = 8.87768
I0522 23:24:12.262560 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87768 (* 1 = 8.87768 loss)
I0522 23:24:12.326489 34682 sgd_solver.cpp:112] Iteration 12010, lr = 0.01
I0522 23:24:19.435752 34682 solver.cpp:239] Iteration 12020 (1.39414 iter/s, 7.1729s/10 iters), loss = 8.69077
I0522 23:24:19.435811 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69077 (* 1 = 8.69077 loss)
I0522 23:24:20.286258 34682 sgd_solver.cpp:112] Iteration 12020, lr = 0.01
I0522 23:24:22.725512 34682 solver.cpp:239] Iteration 12030 (3.03992 iter/s, 3.28957s/10 iters), loss = 8.37089
I0522 23:24:22.725555 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37089 (* 1 = 8.37089 loss)
I0522 23:24:22.796515 34682 sgd_solver.cpp:112] Iteration 12030, lr = 0.01
I0522 23:24:28.467394 34682 solver.cpp:239] Iteration 12040 (1.74167 iter/s, 5.7416s/10 iters), loss = 8.66257
I0522 23:24:28.467443 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66257 (* 1 = 8.66257 loss)
I0522 23:24:28.527206 34682 sgd_solver.cpp:112] Iteration 12040, lr = 0.01
I0522 23:24:32.481722 34682 solver.cpp:239] Iteration 12050 (2.49121 iter/s, 4.01412s/10 iters), loss = 9.21779
I0522 23:24:32.481776 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21779 (* 1 = 9.21779 loss)
I0522 23:24:32.557258 34682 sgd_solver.cpp:112] Iteration 12050, lr = 0.01
I0522 23:24:37.290680 34682 solver.cpp:239] Iteration 12060 (2.07956 iter/s, 4.8087s/10 iters), loss = 8.95166
I0522 23:24:37.291007 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95166 (* 1 = 8.95166 loss)
I0522 23:24:38.108208 34682 sgd_solver.cpp:112] Iteration 12060, lr = 0.01
I0522 23:24:41.489770 34682 solver.cpp:239] Iteration 12070 (2.38173 iter/s, 4.19862s/10 iters), loss = 9.18713
I0522 23:24:41.489817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18713 (* 1 = 9.18713 loss)
I0522 23:24:41.556344 34682 sgd_solver.cpp:112] Iteration 12070, lr = 0.01
I0522 23:24:47.845365 34682 solver.cpp:239] Iteration 12080 (1.5735 iter/s, 6.35528s/10 iters), loss = 8.11771
I0522 23:24:47.845412 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11771 (* 1 = 8.11771 loss)
I0522 23:24:47.909020 34682 sgd_solver.cpp:112] Iteration 12080, lr = 0.01
I0522 23:24:51.257692 34682 solver.cpp:239] Iteration 12090 (2.93072 iter/s, 3.41213s/10 iters), loss = 8.69907
I0522 23:24:51.257737 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69907 (* 1 = 8.69907 loss)
I0522 23:24:51.323022 34682 sgd_solver.cpp:112] Iteration 12090, lr = 0.01
I0522 23:24:54.743717 34682 solver.cpp:239] Iteration 12100 (2.86875 iter/s, 3.48584s/10 iters), loss = 8.9488
I0522 23:24:54.743765 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9488 (* 1 = 8.9488 loss)
I0522 23:24:55.566017 34682 sgd_solver.cpp:112] Iteration 12100, lr = 0.01
I0522 23:25:01.128650 34682 solver.cpp:239] Iteration 12110 (1.56626 iter/s, 6.38462s/10 iters), loss = 8.60046
I0522 23:25:01.128705 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60046 (* 1 = 8.60046 loss)
I0522 23:25:01.980221 34682 sgd_solver.cpp:112] Iteration 12110, lr = 0.01
I0522 23:25:05.484500 34682 solver.cpp:239] Iteration 12120 (2.29589 iter/s, 4.35562s/10 iters), loss = 8.84903
I0522 23:25:05.484571 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84903 (* 1 = 8.84903 loss)
I0522 23:25:05.549156 34682 sgd_solver.cpp:112] Iteration 12120, lr = 0.01
I0522 23:25:11.541486 34682 solver.cpp:239] Iteration 12130 (1.65107 iter/s, 6.05667s/10 iters), loss = 9.06231
I0522 23:25:11.541759 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06231 (* 1 = 9.06231 loss)
I0522 23:25:11.605036 34682 sgd_solver.cpp:112] Iteration 12130, lr = 0.01
I0522 23:25:17.135222 34682 solver.cpp:239] Iteration 12140 (1.78786 iter/s, 5.59327s/10 iters), loss = 9.08296
I0522 23:25:17.135279 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08296 (* 1 = 9.08296 loss)
I0522 23:25:17.198704 34682 sgd_solver.cpp:112] Iteration 12140, lr = 0.01
I0522 23:25:22.206444 34682 solver.cpp:239] Iteration 12150 (1.97201 iter/s, 5.07096s/10 iters), loss = 9.35932
I0522 23:25:22.206501 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35932 (* 1 = 9.35932 loss)
I0522 23:25:22.270138 34682 sgd_solver.cpp:112] Iteration 12150, lr = 0.01
I0522 23:25:27.811934 34682 solver.cpp:239] Iteration 12160 (1.78406 iter/s, 5.60521s/10 iters), loss = 8.65101
I0522 23:25:27.811978 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65101 (* 1 = 8.65101 loss)
I0522 23:25:27.876803 34682 sgd_solver.cpp:112] Iteration 12160, lr = 0.01
I0522 23:25:32.640796 34682 solver.cpp:239] Iteration 12170 (2.07099 iter/s, 4.82862s/10 iters), loss = 9.25726
I0522 23:25:32.640848 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25726 (* 1 = 9.25726 loss)
I0522 23:25:33.447726 34682 sgd_solver.cpp:112] Iteration 12170, lr = 0.01
I0522 23:25:37.610046 34682 solver.cpp:239] Iteration 12180 (2.01248 iter/s, 4.96899s/10 iters), loss = 9.34694
I0522 23:25:37.610100 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34694 (* 1 = 9.34694 loss)
I0522 23:25:38.353883 34682 sgd_solver.cpp:112] Iteration 12180, lr = 0.01
I0522 23:25:41.679864 34682 solver.cpp:239] Iteration 12190 (2.45725 iter/s, 4.0696s/10 iters), loss = 8.5673
I0522 23:25:41.680059 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5673 (* 1 = 8.5673 loss)
I0522 23:25:42.545713 34682 sgd_solver.cpp:112] Iteration 12190, lr = 0.01
I0522 23:25:46.020511 34682 solver.cpp:239] Iteration 12200 (2.304 iter/s, 4.34027s/10 iters), loss = 9.34028
I0522 23:25:46.020567 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34028 (* 1 = 9.34028 loss)
I0522 23:25:46.800994 34682 sgd_solver.cpp:112] Iteration 12200, lr = 0.01
I0522 23:25:52.762791 34682 solver.cpp:239] Iteration 12210 (1.48325 iter/s, 6.74194s/10 iters), loss = 8.91556
I0522 23:25:52.762850 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91556 (* 1 = 8.91556 loss)
I0522 23:25:52.832224 34682 sgd_solver.cpp:112] Iteration 12210, lr = 0.01
I0522 23:26:00.319051 34682 solver.cpp:239] Iteration 12220 (1.32347 iter/s, 7.5559s/10 iters), loss = 8.37706
I0522 23:26:00.319118 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37706 (* 1 = 8.37706 loss)
I0522 23:26:01.011301 34682 sgd_solver.cpp:112] Iteration 12220, lr = 0.01
I0522 23:26:08.779217 34682 solver.cpp:239] Iteration 12230 (1.18207 iter/s, 8.45976s/10 iters), loss = 8.61079
I0522 23:26:08.779271 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61079 (* 1 = 8.61079 loss)
I0522 23:26:09.608844 34682 sgd_solver.cpp:112] Iteration 12230, lr = 0.01
I0522 23:26:12.951925 34682 solver.cpp:239] Iteration 12240 (2.39666 iter/s, 4.17248s/10 iters), loss = 9.17642
I0522 23:26:12.952078 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17642 (* 1 = 9.17642 loss)
I0522 23:26:13.189199 34682 sgd_solver.cpp:112] Iteration 12240, lr = 0.01
I0522 23:26:17.318828 34682 solver.cpp:239] Iteration 12250 (2.29013 iter/s, 4.36657s/10 iters), loss = 8.36561
I0522 23:26:17.318888 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36561 (* 1 = 8.36561 loss)
I0522 23:26:17.388365 34682 sgd_solver.cpp:112] Iteration 12250, lr = 0.01
I0522 23:26:22.794538 34682 solver.cpp:239] Iteration 12260 (1.82634 iter/s, 5.47542s/10 iters), loss = 8.31391
I0522 23:26:22.794601 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31391 (* 1 = 8.31391 loss)
I0522 23:26:23.451752 34682 sgd_solver.cpp:112] Iteration 12260, lr = 0.01
I0522 23:26:27.631997 34682 solver.cpp:239] Iteration 12270 (2.06732 iter/s, 4.83718s/10 iters), loss = 8.73125
I0522 23:26:27.632063 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73125 (* 1 = 8.73125 loss)
I0522 23:26:28.294368 34682 sgd_solver.cpp:112] Iteration 12270, lr = 0.01
I0522 23:26:34.605748 34682 solver.cpp:239] Iteration 12280 (1.43402 iter/s, 6.97338s/10 iters), loss = 9.55667
I0522 23:26:34.605809 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.55667 (* 1 = 9.55667 loss)
I0522 23:26:35.370563 34682 sgd_solver.cpp:112] Iteration 12280, lr = 0.01
I0522 23:26:39.201282 34682 solver.cpp:239] Iteration 12290 (2.17615 iter/s, 4.59528s/10 iters), loss = 8.37836
I0522 23:26:39.201330 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37836 (* 1 = 8.37836 loss)
I0522 23:26:39.281270 34682 sgd_solver.cpp:112] Iteration 12290, lr = 0.01
I0522 23:26:43.148551 34682 solver.cpp:239] Iteration 12300 (2.53354 iter/s, 3.94704s/10 iters), loss = 9.24037
I0522 23:26:43.148852 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24037 (* 1 = 9.24037 loss)
I0522 23:26:43.211307 34682 sgd_solver.cpp:112] Iteration 12300, lr = 0.01
I0522 23:26:47.373713 34682 solver.cpp:239] Iteration 12310 (2.36702 iter/s, 4.22472s/10 iters), loss = 8.57485
I0522 23:26:47.373769 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57485 (* 1 = 8.57485 loss)
I0522 23:26:48.096082 34682 sgd_solver.cpp:112] Iteration 12310, lr = 0.01
I0522 23:26:53.248188 34682 solver.cpp:239] Iteration 12320 (1.70237 iter/s, 5.87417s/10 iters), loss = 9.41556
I0522 23:26:53.248245 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.41556 (* 1 = 9.41556 loss)
I0522 23:26:53.318996 34682 sgd_solver.cpp:112] Iteration 12320, lr = 0.01
I0522 23:26:56.771922 34682 solver.cpp:239] Iteration 12330 (2.83806 iter/s, 3.52353s/10 iters), loss = 8.34978
I0522 23:26:56.771971 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34978 (* 1 = 8.34978 loss)
I0522 23:26:56.833034 34682 sgd_solver.cpp:112] Iteration 12330, lr = 0.01
I0522 23:27:00.781725 34682 solver.cpp:239] Iteration 12340 (2.49404 iter/s, 4.00957s/10 iters), loss = 8.37679
I0522 23:27:00.781813 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37679 (* 1 = 8.37679 loss)
I0522 23:27:01.581923 34682 sgd_solver.cpp:112] Iteration 12340, lr = 0.01
I0522 23:27:04.903641 34682 solver.cpp:239] Iteration 12350 (2.4262 iter/s, 4.12167s/10 iters), loss = 8.52703
I0522 23:27:04.903690 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52703 (* 1 = 8.52703 loss)
I0522 23:27:04.980823 34682 sgd_solver.cpp:112] Iteration 12350, lr = 0.01
I0522 23:27:09.359493 34682 solver.cpp:239] Iteration 12360 (2.24436 iter/s, 4.45562s/10 iters), loss = 8.66883
I0522 23:27:09.359547 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66883 (* 1 = 8.66883 loss)
I0522 23:27:09.425388 34682 sgd_solver.cpp:112] Iteration 12360, lr = 0.01
I0522 23:27:12.164224 34682 solver.cpp:239] Iteration 12370 (3.56562 iter/s, 2.80456s/10 iters), loss = 8.99313
I0522 23:27:12.164280 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99313 (* 1 = 8.99313 loss)
I0522 23:27:12.749420 34682 sgd_solver.cpp:112] Iteration 12370, lr = 0.01
I0522 23:27:17.701611 34682 solver.cpp:239] Iteration 12380 (1.806 iter/s, 5.5371s/10 iters), loss = 8.59985
I0522 23:27:17.701802 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59985 (* 1 = 8.59985 loss)
I0522 23:27:17.775701 34682 sgd_solver.cpp:112] Iteration 12380, lr = 0.01
I0522 23:27:22.528200 34682 solver.cpp:239] Iteration 12390 (2.07201 iter/s, 4.82623s/10 iters), loss = 8.61709
I0522 23:27:22.528264 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61709 (* 1 = 8.61709 loss)
I0522 23:27:22.603516 34682 sgd_solver.cpp:112] Iteration 12390, lr = 0.01
I0522 23:27:26.351096 34682 solver.cpp:239] Iteration 12400 (2.61597 iter/s, 3.82267s/10 iters), loss = 8.00751
I0522 23:27:26.351141 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00751 (* 1 = 8.00751 loss)
I0522 23:27:26.427150 34682 sgd_solver.cpp:112] Iteration 12400, lr = 0.01
I0522 23:27:31.961344 34682 solver.cpp:239] Iteration 12410 (1.78254 iter/s, 5.60997s/10 iters), loss = 9.49674
I0522 23:27:31.961390 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.49674 (* 1 = 9.49674 loss)
I0522 23:27:32.801097 34682 sgd_solver.cpp:112] Iteration 12410, lr = 0.01
I0522 23:27:37.854522 34682 solver.cpp:239] Iteration 12420 (1.69696 iter/s, 5.89289s/10 iters), loss = 8.70678
I0522 23:27:37.854573 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70678 (* 1 = 8.70678 loss)
I0522 23:27:37.926267 34682 sgd_solver.cpp:112] Iteration 12420, lr = 0.01
I0522 23:27:40.260630 34682 solver.cpp:239] Iteration 12430 (4.15637 iter/s, 2.40595s/10 iters), loss = 9.14285
I0522 23:27:40.260689 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14285 (* 1 = 9.14285 loss)
I0522 23:27:41.123224 34682 sgd_solver.cpp:112] Iteration 12430, lr = 0.01
I0522 23:27:45.870767 34682 solver.cpp:239] Iteration 12440 (1.78258 iter/s, 5.60985s/10 iters), loss = 9.18118
I0522 23:27:45.870821 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18118 (* 1 = 9.18118 loss)
I0522 23:27:45.949139 34682 sgd_solver.cpp:112] Iteration 12440, lr = 0.01
I0522 23:27:50.707285 34682 solver.cpp:239] Iteration 12450 (2.06771 iter/s, 4.83627s/10 iters), loss = 8.52325
I0522 23:27:50.707540 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52325 (* 1 = 8.52325 loss)
I0522 23:27:50.787215 34682 sgd_solver.cpp:112] Iteration 12450, lr = 0.01
I0522 23:27:54.794144 34682 solver.cpp:239] Iteration 12460 (2.4471 iter/s, 4.08647s/10 iters), loss = 8.98622
I0522 23:27:54.794186 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98622 (* 1 = 8.98622 loss)
I0522 23:27:54.855957 34682 sgd_solver.cpp:112] Iteration 12460, lr = 0.01
I0522 23:27:58.920794 34682 solver.cpp:239] Iteration 12470 (2.42341 iter/s, 4.12642s/10 iters), loss = 8.74408
I0522 23:27:58.920861 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74408 (* 1 = 8.74408 loss)
I0522 23:27:59.586771 34682 sgd_solver.cpp:112] Iteration 12470, lr = 0.01
I0522 23:28:05.938932 34682 solver.cpp:239] Iteration 12480 (1.42495 iter/s, 7.01779s/10 iters), loss = 8.46545
I0522 23:28:05.938997 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46545 (* 1 = 8.46545 loss)
I0522 23:28:05.999207 34682 sgd_solver.cpp:112] Iteration 12480, lr = 0.01
I0522 23:28:11.609338 34682 solver.cpp:239] Iteration 12490 (1.76363 iter/s, 5.67012s/10 iters), loss = 8.78574
I0522 23:28:11.609385 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78574 (* 1 = 8.78574 loss)
I0522 23:28:11.682996 34682 sgd_solver.cpp:112] Iteration 12490, lr = 0.01
I0522 23:28:15.764641 34682 solver.cpp:239] Iteration 12500 (2.4067 iter/s, 4.15507s/10 iters), loss = 9.22126
I0522 23:28:15.764684 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22126 (* 1 = 9.22126 loss)
I0522 23:28:15.829010 34682 sgd_solver.cpp:112] Iteration 12500, lr = 0.01
I0522 23:28:20.609673 34682 solver.cpp:239] Iteration 12510 (2.06407 iter/s, 4.84479s/10 iters), loss = 8.99246
I0522 23:28:20.609716 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99246 (* 1 = 8.99246 loss)
I0522 23:28:20.681164 34682 sgd_solver.cpp:112] Iteration 12510, lr = 0.01
I0522 23:28:26.375416 34682 solver.cpp:239] Iteration 12520 (1.73447 iter/s, 5.76546s/10 iters), loss = 8.7786
I0522 23:28:26.375510 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7786 (* 1 = 8.7786 loss)
I0522 23:28:26.444396 34682 sgd_solver.cpp:112] Iteration 12520, lr = 0.01
I0522 23:28:29.575294 34682 solver.cpp:239] Iteration 12530 (3.12535 iter/s, 3.19964s/10 iters), loss = 8.57201
I0522 23:28:29.575356 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57201 (* 1 = 8.57201 loss)
I0522 23:28:30.372336 34682 sgd_solver.cpp:112] Iteration 12530, lr = 0.01
I0522 23:28:34.922947 34682 solver.cpp:239] Iteration 12540 (1.87008 iter/s, 5.34737s/10 iters), loss = 8.88928
I0522 23:28:34.923010 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88928 (* 1 = 8.88928 loss)
I0522 23:28:35.248591 34682 sgd_solver.cpp:112] Iteration 12540, lr = 0.01
I0522 23:28:39.668911 34682 solver.cpp:239] Iteration 12550 (2.10717 iter/s, 4.74571s/10 iters), loss = 9.08974
I0522 23:28:39.668962 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08974 (* 1 = 9.08974 loss)
I0522 23:28:39.732772 34682 sgd_solver.cpp:112] Iteration 12550, lr = 0.01
I0522 23:28:44.455032 34682 solver.cpp:239] Iteration 12560 (2.08948 iter/s, 4.78587s/10 iters), loss = 8.80691
I0522 23:28:44.455078 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80691 (* 1 = 8.80691 loss)
I0522 23:28:44.539810 34682 sgd_solver.cpp:112] Iteration 12560, lr = 0.01
I0522 23:28:48.576622 34682 solver.cpp:239] Iteration 12570 (2.42638 iter/s, 4.12137s/10 iters), loss = 8.92272
I0522 23:28:48.576665 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92272 (* 1 = 8.92272 loss)
I0522 23:28:48.658449 34682 sgd_solver.cpp:112] Iteration 12570, lr = 0.01
I0522 23:28:54.236714 34682 solver.cpp:239] Iteration 12580 (1.76684 iter/s, 5.65981s/10 iters), loss = 9.06904
I0522 23:28:54.236779 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06904 (* 1 = 9.06904 loss)
I0522 23:28:54.304023 34682 sgd_solver.cpp:112] Iteration 12580, lr = 0.01
I0522 23:28:59.740572 34682 solver.cpp:239] Iteration 12590 (1.817 iter/s, 5.50357s/10 iters), loss = 8.05394
I0522 23:28:59.740805 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05394 (* 1 = 8.05394 loss)
I0522 23:28:59.797402 34682 sgd_solver.cpp:112] Iteration 12590, lr = 0.01
I0522 23:29:02.697573 34682 solver.cpp:239] Iteration 12600 (3.38218 iter/s, 2.95667s/10 iters), loss = 9.59924
I0522 23:29:02.697624 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59924 (* 1 = 9.59924 loss)
I0522 23:29:03.528466 34682 sgd_solver.cpp:112] Iteration 12600, lr = 0.01
I0522 23:29:08.597620 34682 solver.cpp:239] Iteration 12610 (1.69499 iter/s, 5.89976s/10 iters), loss = 8.9074
I0522 23:29:08.597671 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9074 (* 1 = 8.9074 loss)
I0522 23:29:08.664942 34682 sgd_solver.cpp:112] Iteration 12610, lr = 0.01
I0522 23:29:12.904911 34682 solver.cpp:239] Iteration 12620 (2.32413 iter/s, 4.30269s/10 iters), loss = 8.7047
I0522 23:29:12.904958 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7047 (* 1 = 8.7047 loss)
I0522 23:29:12.972468 34682 sgd_solver.cpp:112] Iteration 12620, lr = 0.01
I0522 23:29:16.522645 34682 solver.cpp:239] Iteration 12630 (2.76431 iter/s, 3.61754s/10 iters), loss = 9.42
I0522 23:29:16.522683 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42 (* 1 = 9.42 loss)
I0522 23:29:16.611135 34682 sgd_solver.cpp:112] Iteration 12630, lr = 0.01
I0522 23:29:20.734134 34682 solver.cpp:239] Iteration 12640 (2.37458 iter/s, 4.21127s/10 iters), loss = 8.78041
I0522 23:29:20.734185 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78041 (* 1 = 8.78041 loss)
I0522 23:29:20.826236 34682 sgd_solver.cpp:112] Iteration 12640, lr = 0.01
I0522 23:29:26.974151 34682 solver.cpp:239] Iteration 12650 (1.60264 iter/s, 6.23971s/10 iters), loss = 9.098
I0522 23:29:26.974218 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.098 (* 1 = 9.098 loss)
I0522 23:29:27.625452 34682 sgd_solver.cpp:112] Iteration 12650, lr = 0.01
I0522 23:29:31.310938 34682 solver.cpp:239] Iteration 12660 (2.30598 iter/s, 4.33654s/10 iters), loss = 9.09739
I0522 23:29:31.311127 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09739 (* 1 = 9.09739 loss)
I0522 23:29:31.388294 34682 sgd_solver.cpp:112] Iteration 12660, lr = 0.01
I0522 23:29:34.714090 34682 solver.cpp:239] Iteration 12670 (2.93872 iter/s, 3.40284s/10 iters), loss = 9.08498
I0522 23:29:34.714161 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08498 (* 1 = 9.08498 loss)
I0522 23:29:34.853678 34682 sgd_solver.cpp:112] Iteration 12670, lr = 0.01
I0522 23:29:40.531719 34682 solver.cpp:239] Iteration 12680 (1.719 iter/s, 5.81732s/10 iters), loss = 9.77924
I0522 23:29:40.531777 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.77924 (* 1 = 9.77924 loss)
I0522 23:29:41.268242 34682 sgd_solver.cpp:112] Iteration 12680, lr = 0.01
I0522 23:29:44.576287 34682 solver.cpp:239] Iteration 12690 (2.47259 iter/s, 4.04434s/10 iters), loss = 9.22863
I0522 23:29:44.576339 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22863 (* 1 = 9.22863 loss)
I0522 23:29:44.660796 34682 sgd_solver.cpp:112] Iteration 12690, lr = 0.01
I0522 23:29:48.169418 34682 solver.cpp:239] Iteration 12700 (2.78325 iter/s, 3.59292s/10 iters), loss = 8.84602
I0522 23:29:48.169486 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84602 (* 1 = 8.84602 loss)
I0522 23:29:48.866168 34682 sgd_solver.cpp:112] Iteration 12700, lr = 0.01
I0522 23:29:52.972316 34682 solver.cpp:239] Iteration 12710 (2.08219 iter/s, 4.80264s/10 iters), loss = 9.16032
I0522 23:29:52.972355 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16032 (* 1 = 9.16032 loss)
I0522 23:29:53.046924 34682 sgd_solver.cpp:112] Iteration 12710, lr = 0.01
I0522 23:29:57.473477 34682 solver.cpp:239] Iteration 12720 (2.22176 iter/s, 4.50093s/10 iters), loss = 9.39377
I0522 23:29:57.473527 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.39377 (* 1 = 9.39377 loss)
I0522 23:29:58.311106 34682 sgd_solver.cpp:112] Iteration 12720, lr = 0.01
I0522 23:30:01.766774 34682 solver.cpp:239] Iteration 12730 (2.32934 iter/s, 4.29307s/10 iters), loss = 8.38813
I0522 23:30:01.766997 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38813 (* 1 = 8.38813 loss)
I0522 23:30:02.607692 34682 sgd_solver.cpp:112] Iteration 12730, lr = 0.01
I0522 23:30:06.516355 34682 solver.cpp:239] Iteration 12740 (2.10563 iter/s, 4.74917s/10 iters), loss = 8.71869
I0522 23:30:06.516412 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71869 (* 1 = 8.71869 loss)
I0522 23:30:06.585597 34682 sgd_solver.cpp:112] Iteration 12740, lr = 0.01
I0522 23:30:12.256620 34682 solver.cpp:239] Iteration 12750 (1.74217 iter/s, 5.73997s/10 iters), loss = 9.15488
I0522 23:30:12.256700 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15488 (* 1 = 9.15488 loss)
I0522 23:30:12.989233 34682 sgd_solver.cpp:112] Iteration 12750, lr = 0.01
I0522 23:30:17.464555 34682 solver.cpp:239] Iteration 12760 (1.92025 iter/s, 5.20765s/10 iters), loss = 8.68686
I0522 23:30:17.464612 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68686 (* 1 = 8.68686 loss)
I0522 23:30:17.528689 34682 sgd_solver.cpp:112] Iteration 12760, lr = 0.01
I0522 23:30:22.437269 34682 solver.cpp:239] Iteration 12770 (2.01108 iter/s, 4.97245s/10 iters), loss = 9.21586
I0522 23:30:22.437320 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21586 (* 1 = 9.21586 loss)
I0522 23:30:22.509583 34682 sgd_solver.cpp:112] Iteration 12770, lr = 0.01
I0522 23:30:27.813819 34682 solver.cpp:239] Iteration 12780 (1.86002 iter/s, 5.37627s/10 iters), loss = 9.25835
I0522 23:30:27.813868 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25835 (* 1 = 9.25835 loss)
I0522 23:30:27.887354 34682 sgd_solver.cpp:112] Iteration 12780, lr = 0.01
I0522 23:30:33.359670 34682 solver.cpp:239] Iteration 12790 (1.80324 iter/s, 5.54557s/10 iters), loss = 9.74554
I0522 23:30:33.359943 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.74554 (* 1 = 9.74554 loss)
I0522 23:30:34.151316 34682 sgd_solver.cpp:112] Iteration 12790, lr = 0.01
I0522 23:30:38.291916 34682 solver.cpp:239] Iteration 12800 (2.02767 iter/s, 4.93178s/10 iters), loss = 9.0587
I0522 23:30:38.291988 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0587 (* 1 = 9.0587 loss)
I0522 23:30:39.082810 34682 sgd_solver.cpp:112] Iteration 12800, lr = 0.01
I0522 23:30:45.455991 34682 solver.cpp:239] Iteration 12810 (1.39592 iter/s, 7.16372s/10 iters), loss = 8.90378
I0522 23:30:45.456050 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90378 (* 1 = 8.90378 loss)
I0522 23:30:46.309157 34682 sgd_solver.cpp:112] Iteration 12810, lr = 0.01
I0522 23:30:50.357195 34682 solver.cpp:239] Iteration 12820 (2.04043 iter/s, 4.90094s/10 iters), loss = 9.28054
I0522 23:30:50.357265 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28054 (* 1 = 9.28054 loss)
I0522 23:30:50.575577 34682 sgd_solver.cpp:112] Iteration 12820, lr = 0.01
I0522 23:30:54.053609 34682 solver.cpp:239] Iteration 12830 (2.70548 iter/s, 3.6962s/10 iters), loss = 9.72926
I0522 23:30:54.053675 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.72926 (* 1 = 9.72926 loss)
I0522 23:30:54.848803 34682 sgd_solver.cpp:112] Iteration 12830, lr = 0.01
I0522 23:30:58.609144 34682 solver.cpp:239] Iteration 12840 (2.19526 iter/s, 4.55528s/10 iters), loss = 8.88339
I0522 23:30:58.609189 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88339 (* 1 = 8.88339 loss)
I0522 23:30:58.683393 34682 sgd_solver.cpp:112] Iteration 12840, lr = 0.01
I0522 23:31:03.678259 34682 solver.cpp:239] Iteration 12850 (1.97283 iter/s, 5.06886s/10 iters), loss = 8.70569
I0522 23:31:03.678491 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70569 (* 1 = 8.70569 loss)
I0522 23:31:04.515496 34682 sgd_solver.cpp:112] Iteration 12850, lr = 0.01
I0522 23:31:10.295965 34682 solver.cpp:239] Iteration 12860 (1.51121 iter/s, 6.6172s/10 iters), loss = 9.23766
I0522 23:31:10.296037 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23766 (* 1 = 9.23766 loss)
I0522 23:31:11.076823 34682 sgd_solver.cpp:112] Iteration 12860, lr = 0.01
I0522 23:31:16.499043 34682 solver.cpp:239] Iteration 12870 (1.61218 iter/s, 6.20276s/10 iters), loss = 9.77408
I0522 23:31:16.499089 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.77408 (* 1 = 9.77408 loss)
I0522 23:31:16.562611 34682 sgd_solver.cpp:112] Iteration 12870, lr = 0.01
I0522 23:31:21.372694 34682 solver.cpp:239] Iteration 12880 (2.05195 iter/s, 4.87341s/10 iters), loss = 8.5898
I0522 23:31:21.372752 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5898 (* 1 = 8.5898 loss)
I0522 23:31:21.433712 34682 sgd_solver.cpp:112] Iteration 12880, lr = 0.01
I0522 23:31:26.406086 34682 solver.cpp:239] Iteration 12890 (1.98683 iter/s, 5.03313s/10 iters), loss = 8.73999
I0522 23:31:26.406126 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73999 (* 1 = 8.73999 loss)
I0522 23:31:26.482976 34682 sgd_solver.cpp:112] Iteration 12890, lr = 0.01
I0522 23:31:31.340142 34682 solver.cpp:239] Iteration 12900 (2.02683 iter/s, 4.9338s/10 iters), loss = 9.21436
I0522 23:31:31.340212 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21436 (* 1 = 9.21436 loss)
I0522 23:31:32.072603 34682 sgd_solver.cpp:112] Iteration 12900, lr = 0.01
I0522 23:31:37.631990 34682 solver.cpp:239] Iteration 12910 (1.58944 iter/s, 6.29153s/10 iters), loss = 9.19214
I0522 23:31:37.632109 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19214 (* 1 = 9.19214 loss)
I0522 23:31:38.436574 34682 sgd_solver.cpp:112] Iteration 12910, lr = 0.01
I0522 23:31:41.744101 34682 solver.cpp:239] Iteration 12920 (2.43201 iter/s, 4.11182s/10 iters), loss = 9.45341
I0522 23:31:41.744163 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.45341 (* 1 = 9.45341 loss)
I0522 23:31:42.467784 34682 sgd_solver.cpp:112] Iteration 12920, lr = 0.01
I0522 23:31:48.647480 34682 solver.cpp:239] Iteration 12930 (1.44864 iter/s, 6.90304s/10 iters), loss = 8.73302
I0522 23:31:48.647534 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73302 (* 1 = 8.73302 loss)
I0522 23:31:49.007287 34682 sgd_solver.cpp:112] Iteration 12930, lr = 0.01
I0522 23:31:53.929802 34682 solver.cpp:239] Iteration 12940 (1.8932 iter/s, 5.28206s/10 iters), loss = 9.3587
I0522 23:31:53.929844 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3587 (* 1 = 9.3587 loss)
I0522 23:31:54.002306 34682 sgd_solver.cpp:112] Iteration 12940, lr = 0.01
I0522 23:31:58.226583 34682 solver.cpp:239] Iteration 12950 (2.32744 iter/s, 4.29656s/10 iters), loss = 8.45261
I0522 23:31:58.226640 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45261 (* 1 = 8.45261 loss)
I0522 23:31:59.087813 34682 sgd_solver.cpp:112] Iteration 12950, lr = 0.01
I0522 23:32:03.107553 34682 solver.cpp:239] Iteration 12960 (2.04888 iter/s, 4.88071s/10 iters), loss = 9.10044
I0522 23:32:03.107599 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10044 (* 1 = 9.10044 loss)
I0522 23:32:03.184262 34682 sgd_solver.cpp:112] Iteration 12960, lr = 0.01
I0522 23:32:09.228983 34682 solver.cpp:239] Iteration 12970 (1.63434 iter/s, 6.11868s/10 iters), loss = 9.20989
I0522 23:32:09.229239 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20989 (* 1 = 9.20989 loss)
I0522 23:32:10.091725 34682 sgd_solver.cpp:112] Iteration 12970, lr = 0.01
I0522 23:32:13.983080 34682 solver.cpp:239] Iteration 12980 (2.10363 iter/s, 4.75368s/10 iters), loss = 8.64767
I0522 23:32:13.983129 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64767 (* 1 = 8.64767 loss)
I0522 23:32:14.052995 34682 sgd_solver.cpp:112] Iteration 12980, lr = 0.01
I0522 23:32:20.089711 34682 solver.cpp:239] Iteration 12990 (1.63765 iter/s, 6.10633s/10 iters), loss = 8.81478
I0522 23:32:20.089773 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81478 (* 1 = 8.81478 loss)
I0522 23:32:20.163694 34682 sgd_solver.cpp:112] Iteration 12990, lr = 0.01
I0522 23:32:24.974933 34682 solver.cpp:239] Iteration 13000 (2.0471 iter/s, 4.88497s/10 iters), loss = 8.92623
I0522 23:32:24.974974 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92623 (* 1 = 8.92623 loss)
I0522 23:32:25.048696 34682 sgd_solver.cpp:112] Iteration 13000, lr = 0.01
I0522 23:32:29.667117 34682 solver.cpp:239] Iteration 13010 (2.13131 iter/s, 4.69195s/10 iters), loss = 8.63803
I0522 23:32:29.667156 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63803 (* 1 = 8.63803 loss)
I0522 23:32:29.744261 34682 sgd_solver.cpp:112] Iteration 13010, lr = 0.01
I0522 23:32:34.694818 34682 solver.cpp:239] Iteration 13020 (1.98908 iter/s, 5.02745s/10 iters), loss = 8.90987
I0522 23:32:34.694872 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90987 (* 1 = 8.90987 loss)
I0522 23:32:35.395678 34682 sgd_solver.cpp:112] Iteration 13020, lr = 0.01
I0522 23:32:39.429894 34682 solver.cpp:239] Iteration 13030 (2.11201 iter/s, 4.73482s/10 iters), loss = 8.80399
I0522 23:32:39.430168 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80399 (* 1 = 8.80399 loss)
I0522 23:32:39.508400 34682 sgd_solver.cpp:112] Iteration 13030, lr = 0.01
I0522 23:32:43.138120 34682 solver.cpp:239] Iteration 13040 (2.697 iter/s, 3.70783s/10 iters), loss = 8.61395
I0522 23:32:43.138171 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61395 (* 1 = 8.61395 loss)
I0522 23:32:43.198539 34682 sgd_solver.cpp:112] Iteration 13040, lr = 0.01
I0522 23:32:47.761865 34682 solver.cpp:239] Iteration 13050 (2.16286 iter/s, 4.62351s/10 iters), loss = 9.52701
I0522 23:32:47.761912 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.52701 (* 1 = 9.52701 loss)
I0522 23:32:48.611335 34682 sgd_solver.cpp:112] Iteration 13050, lr = 0.01
I0522 23:32:53.400840 34682 solver.cpp:239] Iteration 13060 (1.77346 iter/s, 5.6387s/10 iters), loss = 8.45929
I0522 23:32:53.400889 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45929 (* 1 = 8.45929 loss)
I0522 23:32:53.462061 34682 sgd_solver.cpp:112] Iteration 13060, lr = 0.01
I0522 23:32:58.206905 34682 solver.cpp:239] Iteration 13070 (2.08082 iter/s, 4.80581s/10 iters), loss = 8.9664
I0522 23:32:58.206977 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9664 (* 1 = 8.9664 loss)
I0522 23:32:59.030185 34682 sgd_solver.cpp:112] Iteration 13070, lr = 0.01
I0522 23:33:03.505422 34682 solver.cpp:239] Iteration 13080 (1.88742 iter/s, 5.29823s/10 iters), loss = 8.0929
I0522 23:33:03.505475 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0929 (* 1 = 8.0929 loss)
I0522 23:33:04.293083 34682 sgd_solver.cpp:112] Iteration 13080, lr = 0.01
I0522 23:33:09.881469 34682 solver.cpp:239] Iteration 13090 (1.56845 iter/s, 6.37573s/10 iters), loss = 9.19925
I0522 23:33:09.881703 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19925 (* 1 = 9.19925 loss)
I0522 23:33:10.694363 34682 sgd_solver.cpp:112] Iteration 13090, lr = 0.01
I0522 23:33:14.087385 34682 solver.cpp:239] Iteration 13100 (2.37782 iter/s, 4.20553s/10 iters), loss = 8.68954
I0522 23:33:14.087438 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68954 (* 1 = 8.68954 loss)
I0522 23:33:14.152510 34682 sgd_solver.cpp:112] Iteration 13100, lr = 0.01
I0522 23:33:18.326046 34682 solver.cpp:239] Iteration 13110 (2.35937 iter/s, 4.23843s/10 iters), loss = 8.85156
I0522 23:33:18.326097 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85156 (* 1 = 8.85156 loss)
I0522 23:33:19.150748 34682 sgd_solver.cpp:112] Iteration 13110, lr = 0.01
I0522 23:33:23.263592 34682 solver.cpp:239] Iteration 13120 (2.0254 iter/s, 4.93729s/10 iters), loss = 8.90602
I0522 23:33:23.263646 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90602 (* 1 = 8.90602 loss)
I0522 23:33:23.337345 34682 sgd_solver.cpp:112] Iteration 13120, lr = 0.01
I0522 23:33:27.152206 34682 solver.cpp:239] Iteration 13130 (2.57175 iter/s, 3.8884s/10 iters), loss = 7.77683
I0522 23:33:27.152256 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77683 (* 1 = 7.77683 loss)
I0522 23:33:27.245429 34682 sgd_solver.cpp:112] Iteration 13130, lr = 0.01
I0522 23:33:32.837461 34682 solver.cpp:239] Iteration 13140 (1.75902 iter/s, 5.68497s/10 iters), loss = 8.65987
I0522 23:33:32.837518 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65987 (* 1 = 8.65987 loss)
I0522 23:33:32.910945 34682 sgd_solver.cpp:112] Iteration 13140, lr = 0.01
I0522 23:33:38.492386 34682 solver.cpp:239] Iteration 13150 (1.76846 iter/s, 5.65463s/10 iters), loss = 9.04277
I0522 23:33:38.492446 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04277 (* 1 = 9.04277 loss)
I0522 23:33:39.254129 34682 sgd_solver.cpp:112] Iteration 13150, lr = 0.01
I0522 23:33:45.678484 34682 solver.cpp:239] Iteration 13160 (1.39164 iter/s, 7.18575s/10 iters), loss = 8.32705
I0522 23:33:45.678773 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32705 (* 1 = 8.32705 loss)
I0522 23:33:45.751163 34682 sgd_solver.cpp:112] Iteration 13160, lr = 0.01
I0522 23:33:48.415666 34682 solver.cpp:239] Iteration 13170 (3.65389 iter/s, 2.73681s/10 iters), loss = 8.32253
I0522 23:33:48.415711 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32253 (* 1 = 8.32253 loss)
I0522 23:33:48.493026 34682 sgd_solver.cpp:112] Iteration 13170, lr = 0.01
I0522 23:33:55.621822 34682 solver.cpp:239] Iteration 13180 (1.38777 iter/s, 7.20581s/10 iters), loss = 9.34261
I0522 23:33:55.621884 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34261 (* 1 = 9.34261 loss)
I0522 23:33:55.685809 34682 sgd_solver.cpp:112] Iteration 13180, lr = 0.01
I0522 23:34:01.933392 34682 solver.cpp:239] Iteration 13190 (1.58447 iter/s, 6.31125s/10 iters), loss = 8.74971
I0522 23:34:01.933445 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74971 (* 1 = 8.74971 loss)
I0522 23:34:02.012554 34682 sgd_solver.cpp:112] Iteration 13190, lr = 0.01
I0522 23:34:06.106516 34682 solver.cpp:239] Iteration 13200 (2.39641 iter/s, 4.1729s/10 iters), loss = 9.19489
I0522 23:34:06.106566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19489 (* 1 = 9.19489 loss)
I0522 23:34:06.895069 34682 sgd_solver.cpp:112] Iteration 13200, lr = 0.01
I0522 23:34:11.534656 34682 solver.cpp:239] Iteration 13210 (1.84235 iter/s, 5.42786s/10 iters), loss = 9.21781
I0522 23:34:11.534732 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21781 (* 1 = 9.21781 loss)
I0522 23:34:11.587816 34682 sgd_solver.cpp:112] Iteration 13210, lr = 0.01
I0522 23:34:15.398777 34682 solver.cpp:239] Iteration 13220 (2.58807 iter/s, 3.86389s/10 iters), loss = 8.55396
I0522 23:34:15.398833 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55396 (* 1 = 8.55396 loss)
I0522 23:34:16.179242 34682 sgd_solver.cpp:112] Iteration 13220, lr = 0.01
I0522 23:34:21.408263 34682 solver.cpp:239] Iteration 13230 (1.66412 iter/s, 6.00918s/10 iters), loss = 8.56754
I0522 23:34:21.408324 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56754 (* 1 = 8.56754 loss)
I0522 23:34:21.467267 34682 sgd_solver.cpp:112] Iteration 13230, lr = 0.01
I0522 23:34:24.816337 34682 solver.cpp:239] Iteration 13240 (2.93438 iter/s, 3.40787s/10 iters), loss = 8.29025
I0522 23:34:24.816392 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29025 (* 1 = 8.29025 loss)
I0522 23:34:25.615005 34682 sgd_solver.cpp:112] Iteration 13240, lr = 0.01
I0522 23:34:29.342245 34682 solver.cpp:239] Iteration 13250 (2.20962 iter/s, 4.52566s/10 iters), loss = 9.05411
I0522 23:34:29.342295 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05411 (* 1 = 9.05411 loss)
I0522 23:34:30.190918 34682 sgd_solver.cpp:112] Iteration 13250, lr = 0.01
I0522 23:34:32.069710 34682 solver.cpp:239] Iteration 13260 (3.66664 iter/s, 2.7273s/10 iters), loss = 8.78803
I0522 23:34:32.069754 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78803 (* 1 = 8.78803 loss)
I0522 23:34:32.133201 34682 sgd_solver.cpp:112] Iteration 13260, lr = 0.01
I0522 23:34:36.953871 34682 solver.cpp:239] Iteration 13270 (2.04754 iter/s, 4.88391s/10 iters), loss = 9.00924
I0522 23:34:36.953917 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00924 (* 1 = 9.00924 loss)
I0522 23:34:37.027839 34682 sgd_solver.cpp:112] Iteration 13270, lr = 0.01
I0522 23:34:41.378986 34682 solver.cpp:239] Iteration 13280 (2.25995 iter/s, 4.42488s/10 iters), loss = 9.06934
I0522 23:34:41.379037 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06934 (* 1 = 9.06934 loss)
I0522 23:34:42.206377 34682 sgd_solver.cpp:112] Iteration 13280, lr = 0.01
I0522 23:34:46.377907 34682 solver.cpp:239] Iteration 13290 (2.00053 iter/s, 4.99866s/10 iters), loss = 9.37258
I0522 23:34:46.378188 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.37258 (* 1 = 9.37258 loss)
I0522 23:34:46.439949 34682 sgd_solver.cpp:112] Iteration 13290, lr = 0.01
I0522 23:34:52.620220 34682 solver.cpp:239] Iteration 13300 (1.6021 iter/s, 6.24181s/10 iters), loss = 9.55808
I0522 23:34:52.620265 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.55808 (* 1 = 9.55808 loss)
I0522 23:34:52.696617 34682 sgd_solver.cpp:112] Iteration 13300, lr = 0.01
I0522 23:34:59.326787 34682 solver.cpp:239] Iteration 13310 (1.49171 iter/s, 6.70372s/10 iters), loss = 8.93397
I0522 23:34:59.326856 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93397 (* 1 = 8.93397 loss)
I0522 23:34:59.404676 34682 sgd_solver.cpp:112] Iteration 13310, lr = 0.01
I0522 23:35:02.652727 34682 solver.cpp:239] Iteration 13320 (3.00685 iter/s, 3.32574s/10 iters), loss = 8.73872
I0522 23:35:02.652770 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73872 (* 1 = 8.73872 loss)
I0522 23:35:03.496883 34682 sgd_solver.cpp:112] Iteration 13320, lr = 0.01
I0522 23:35:08.195166 34682 solver.cpp:239] Iteration 13330 (1.80435 iter/s, 5.54216s/10 iters), loss = 8.71718
I0522 23:35:08.195235 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71718 (* 1 = 8.71718 loss)
I0522 23:35:09.018230 34682 sgd_solver.cpp:112] Iteration 13330, lr = 0.01
I0522 23:35:13.684183 34682 solver.cpp:239] Iteration 13340 (1.82191 iter/s, 5.48874s/10 iters), loss = 8.69444
I0522 23:35:13.684238 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69444 (* 1 = 8.69444 loss)
I0522 23:35:14.563043 34682 sgd_solver.cpp:112] Iteration 13340, lr = 0.01
I0522 23:35:19.112022 34682 solver.cpp:239] Iteration 13350 (1.84245 iter/s, 5.42756s/10 iters), loss = 8.91752
I0522 23:35:19.112257 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91752 (* 1 = 8.91752 loss)
I0522 23:35:19.165769 34682 sgd_solver.cpp:112] Iteration 13350, lr = 0.01
I0522 23:35:23.889130 34682 solver.cpp:239] Iteration 13360 (2.0935 iter/s, 4.7767s/10 iters), loss = 9.25761
I0522 23:35:23.889174 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25761 (* 1 = 9.25761 loss)
I0522 23:35:23.962185 34682 sgd_solver.cpp:112] Iteration 13360, lr = 0.01
I0522 23:35:26.248431 34682 solver.cpp:239] Iteration 13370 (4.23881 iter/s, 2.35915s/10 iters), loss = 9.48815
I0522 23:35:26.248491 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.48815 (* 1 = 9.48815 loss)
I0522 23:35:27.037914 34682 sgd_solver.cpp:112] Iteration 13370, lr = 0.01
I0522 23:35:30.993278 34682 solver.cpp:239] Iteration 13380 (2.10766 iter/s, 4.74459s/10 iters), loss = 8.91387
I0522 23:35:30.993326 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91387 (* 1 = 8.91387 loss)
I0522 23:35:31.782054 34682 sgd_solver.cpp:112] Iteration 13380, lr = 0.01
I0522 23:35:37.579514 34682 solver.cpp:239] Iteration 13390 (1.51839 iter/s, 6.58591s/10 iters), loss = 8.41671
I0522 23:35:37.579571 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41671 (* 1 = 8.41671 loss)
I0522 23:35:38.390195 34682 sgd_solver.cpp:112] Iteration 13390, lr = 0.01
I0522 23:35:43.333243 34682 solver.cpp:239] Iteration 13400 (1.73809 iter/s, 5.75343s/10 iters), loss = 7.87432
I0522 23:35:43.333307 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87432 (* 1 = 7.87432 loss)
I0522 23:35:44.086730 34682 sgd_solver.cpp:112] Iteration 13400, lr = 0.01
I0522 23:35:48.560564 34682 solver.cpp:239] Iteration 13410 (1.91313 iter/s, 5.22703s/10 iters), loss = 8.45853
I0522 23:35:48.560631 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45853 (* 1 = 8.45853 loss)
I0522 23:35:48.619046 34682 sgd_solver.cpp:112] Iteration 13410, lr = 0.01
I0522 23:35:55.263244 34682 solver.cpp:239] Iteration 13420 (1.49202 iter/s, 6.70235s/10 iters), loss = 8.33199
I0522 23:35:55.263557 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33199 (* 1 = 8.33199 loss)
I0522 23:35:56.126591 34682 sgd_solver.cpp:112] Iteration 13420, lr = 0.01
I0522 23:36:00.948683 34682 solver.cpp:239] Iteration 13430 (1.75904 iter/s, 5.68492s/10 iters), loss = 8.25655
I0522 23:36:00.948753 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25655 (* 1 = 8.25655 loss)
I0522 23:36:01.756763 34682 sgd_solver.cpp:112] Iteration 13430, lr = 0.01
I0522 23:36:07.972898 34682 solver.cpp:239] Iteration 13440 (1.42372 iter/s, 7.02385s/10 iters), loss = 8.90184
I0522 23:36:07.972960 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90184 (* 1 = 8.90184 loss)
I0522 23:36:08.750751 34682 sgd_solver.cpp:112] Iteration 13440, lr = 0.01
I0522 23:36:13.375020 34682 solver.cpp:239] Iteration 13450 (1.85122 iter/s, 5.40184s/10 iters), loss = 8.82963
I0522 23:36:13.375078 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82963 (* 1 = 8.82963 loss)
I0522 23:36:14.193039 34682 sgd_solver.cpp:112] Iteration 13450, lr = 0.01
I0522 23:36:16.999877 34682 solver.cpp:239] Iteration 13460 (2.75889 iter/s, 3.62465s/10 iters), loss = 9.03124
I0522 23:36:16.999920 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03124 (* 1 = 9.03124 loss)
I0522 23:36:17.773370 34682 sgd_solver.cpp:112] Iteration 13460, lr = 0.01
I0522 23:36:21.916263 34682 solver.cpp:239] Iteration 13470 (2.03412 iter/s, 4.91614s/10 iters), loss = 8.48279
I0522 23:36:21.916317 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48279 (* 1 = 8.48279 loss)
I0522 23:36:22.703197 34682 sgd_solver.cpp:112] Iteration 13470, lr = 0.01
I0522 23:36:25.552109 34682 solver.cpp:239] Iteration 13480 (2.75055 iter/s, 3.63564s/10 iters), loss = 9.52394
I0522 23:36:25.552348 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.52394 (* 1 = 9.52394 loss)
I0522 23:36:26.248330 34682 sgd_solver.cpp:112] Iteration 13480, lr = 0.01
I0522 23:36:29.472857 34682 solver.cpp:239] Iteration 13490 (2.55078 iter/s, 3.92037s/10 iters), loss = 8.94101
I0522 23:36:29.472918 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94101 (* 1 = 8.94101 loss)
I0522 23:36:30.288131 34682 sgd_solver.cpp:112] Iteration 13490, lr = 0.01
I0522 23:36:34.375022 34682 solver.cpp:239] Iteration 13500 (2.04002 iter/s, 4.90191s/10 iters), loss = 9.29755
I0522 23:36:34.375067 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29755 (* 1 = 9.29755 loss)
I0522 23:36:34.444064 34682 sgd_solver.cpp:112] Iteration 13500, lr = 0.01
I0522 23:36:38.625763 34682 solver.cpp:239] Iteration 13510 (2.35265 iter/s, 4.25052s/10 iters), loss = 8.63377
I0522 23:36:38.625811 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63377 (* 1 = 8.63377 loss)
I0522 23:36:38.700891 34682 sgd_solver.cpp:112] Iteration 13510, lr = 0.01
I0522 23:36:43.220082 34682 solver.cpp:239] Iteration 13520 (2.17671 iter/s, 4.59409s/10 iters), loss = 9.15815
I0522 23:36:43.220130 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15815 (* 1 = 9.15815 loss)
I0522 23:36:43.834215 34682 sgd_solver.cpp:112] Iteration 13520, lr = 0.01
I0522 23:36:49.269520 34682 solver.cpp:239] Iteration 13530 (1.65313 iter/s, 6.04912s/10 iters), loss = 8.71659
I0522 23:36:49.269590 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71659 (* 1 = 8.71659 loss)
I0522 23:36:50.011106 34682 sgd_solver.cpp:112] Iteration 13530, lr = 0.01
I0522 23:36:54.933831 34682 solver.cpp:239] Iteration 13540 (1.76553 iter/s, 5.66401s/10 iters), loss = 8.56336
I0522 23:36:54.933882 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56336 (* 1 = 8.56336 loss)
I0522 23:36:55.738288 34682 sgd_solver.cpp:112] Iteration 13540, lr = 0.01
I0522 23:37:00.450603 34682 solver.cpp:239] Iteration 13550 (1.81275 iter/s, 5.51648s/10 iters), loss = 8.64841
I0522 23:37:00.450675 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64841 (* 1 = 8.64841 loss)
I0522 23:37:00.519769 34682 sgd_solver.cpp:112] Iteration 13550, lr = 0.01
I0522 23:37:07.016636 34682 solver.cpp:239] Iteration 13560 (1.52307 iter/s, 6.56568s/10 iters), loss = 8.87056
I0522 23:37:07.016702 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87056 (* 1 = 8.87056 loss)
I0522 23:37:07.852092 34682 sgd_solver.cpp:112] Iteration 13560, lr = 0.01
I0522 23:37:11.982329 34682 solver.cpp:239] Iteration 13570 (2.01393 iter/s, 4.96542s/10 iters), loss = 8.43385
I0522 23:37:11.982383 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43385 (* 1 = 8.43385 loss)
I0522 23:37:12.842106 34682 sgd_solver.cpp:112] Iteration 13570, lr = 0.01
I0522 23:37:17.710587 34682 solver.cpp:239] Iteration 13580 (1.74582 iter/s, 5.72797s/10 iters), loss = 8.61503
I0522 23:37:17.710630 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61503 (* 1 = 8.61503 loss)
I0522 23:37:17.772104 34682 sgd_solver.cpp:112] Iteration 13580, lr = 0.01
I0522 23:37:21.396164 34682 solver.cpp:239] Iteration 13590 (2.71342 iter/s, 3.68538s/10 iters), loss = 8.90297
I0522 23:37:21.396209 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90297 (* 1 = 8.90297 loss)
I0522 23:37:21.466171 34682 sgd_solver.cpp:112] Iteration 13590, lr = 0.01
I0522 23:37:25.766355 34682 solver.cpp:239] Iteration 13600 (2.28835 iter/s, 4.36996s/10 iters), loss = 8.8299
I0522 23:37:25.766574 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8299 (* 1 = 8.8299 loss)
I0522 23:37:25.824724 34682 sgd_solver.cpp:112] Iteration 13600, lr = 0.01
I0522 23:37:33.208411 34682 solver.cpp:239] Iteration 13610 (1.3438 iter/s, 7.44157s/10 iters), loss = 8.98763
I0522 23:37:33.208465 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98763 (* 1 = 8.98763 loss)
I0522 23:37:33.272423 34682 sgd_solver.cpp:112] Iteration 13610, lr = 0.01
I0522 23:37:38.129047 34682 solver.cpp:239] Iteration 13620 (2.03236 iter/s, 4.92038s/10 iters), loss = 8.23623
I0522 23:37:38.129101 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23623 (* 1 = 8.23623 loss)
I0522 23:37:38.706212 34682 sgd_solver.cpp:112] Iteration 13620, lr = 0.01
I0522 23:37:44.102912 34682 solver.cpp:239] Iteration 13630 (1.67405 iter/s, 5.97355s/10 iters), loss = 8.54176
I0522 23:37:44.102993 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54176 (* 1 = 8.54176 loss)
I0522 23:37:44.162797 34682 sgd_solver.cpp:112] Iteration 13630, lr = 0.01
I0522 23:37:48.941339 34682 solver.cpp:239] Iteration 13640 (2.06691 iter/s, 4.83814s/10 iters), loss = 8.85143
I0522 23:37:48.941406 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85143 (* 1 = 8.85143 loss)
I0522 23:37:49.525758 34682 sgd_solver.cpp:112] Iteration 13640, lr = 0.01
I0522 23:37:54.669643 34682 solver.cpp:239] Iteration 13650 (1.74581 iter/s, 5.72801s/10 iters), loss = 8.5455
I0522 23:37:54.669698 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5455 (* 1 = 8.5455 loss)
I0522 23:37:54.732445 34682 sgd_solver.cpp:112] Iteration 13650, lr = 0.01
I0522 23:37:59.965615 34682 solver.cpp:239] Iteration 13660 (1.88833 iter/s, 5.2957s/10 iters), loss = 8.91839
I0522 23:37:59.965896 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91839 (* 1 = 8.91839 loss)
I0522 23:38:00.681356 34682 sgd_solver.cpp:112] Iteration 13660, lr = 0.01
I0522 23:38:06.992445 34682 solver.cpp:239] Iteration 13670 (1.42323 iter/s, 7.02627s/10 iters), loss = 9.36337
I0522 23:38:06.992507 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36337 (* 1 = 9.36337 loss)
I0522 23:38:07.060169 34682 sgd_solver.cpp:112] Iteration 13670, lr = 0.01
I0522 23:38:12.784517 34682 solver.cpp:239] Iteration 13680 (1.72659 iter/s, 5.79177s/10 iters), loss = 8.43919
I0522 23:38:12.784582 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43919 (* 1 = 8.43919 loss)
I0522 23:38:13.568294 34682 sgd_solver.cpp:112] Iteration 13680, lr = 0.01
I0522 23:38:17.738576 34682 solver.cpp:239] Iteration 13690 (2.01866 iter/s, 4.95377s/10 iters), loss = 8.97526
I0522 23:38:17.738654 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97526 (* 1 = 8.97526 loss)
I0522 23:38:17.857146 34682 sgd_solver.cpp:112] Iteration 13690, lr = 0.01
I0522 23:38:22.739614 34682 solver.cpp:239] Iteration 13700 (1.9997 iter/s, 5.00076s/10 iters), loss = 8.27794
I0522 23:38:22.739668 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27794 (* 1 = 8.27794 loss)
I0522 23:38:23.547380 34682 sgd_solver.cpp:112] Iteration 13700, lr = 0.01
I0522 23:38:27.273411 34682 solver.cpp:239] Iteration 13710 (2.20577 iter/s, 4.53356s/10 iters), loss = 9.13229
I0522 23:38:27.273454 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13229 (* 1 = 9.13229 loss)
I0522 23:38:28.117635 34682 sgd_solver.cpp:112] Iteration 13710, lr = 0.01
I0522 23:38:32.858515 34682 solver.cpp:239] Iteration 13720 (1.79057 iter/s, 5.58483s/10 iters), loss = 9.15203
I0522 23:38:32.858778 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15203 (* 1 = 9.15203 loss)
I0522 23:38:32.927911 34682 sgd_solver.cpp:112] Iteration 13720, lr = 0.01
I0522 23:38:36.594219 34682 solver.cpp:239] Iteration 13730 (2.68029 iter/s, 3.73094s/10 iters), loss = 8.48507
I0522 23:38:36.594291 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48507 (* 1 = 8.48507 loss)
I0522 23:38:37.287642 34682 sgd_solver.cpp:112] Iteration 13730, lr = 0.01
I0522 23:38:41.367605 34682 solver.cpp:239] Iteration 13740 (2.09507 iter/s, 4.77311s/10 iters), loss = 8.838
I0522 23:38:41.367666 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.838 (* 1 = 8.838 loss)
I0522 23:38:41.425405 34682 sgd_solver.cpp:112] Iteration 13740, lr = 0.01
I0522 23:38:45.469076 34682 solver.cpp:239] Iteration 13750 (2.43829 iter/s, 4.10124s/10 iters), loss = 9.21801
I0522 23:38:45.469126 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21801 (* 1 = 9.21801 loss)
I0522 23:38:45.538758 34682 sgd_solver.cpp:112] Iteration 13750, lr = 0.01
I0522 23:38:48.972399 34682 solver.cpp:239] Iteration 13760 (2.85459 iter/s, 3.50313s/10 iters), loss = 8.55864
I0522 23:38:48.972441 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55864 (* 1 = 8.55864 loss)
I0522 23:38:49.030609 34682 sgd_solver.cpp:112] Iteration 13760, lr = 0.01
I0522 23:38:54.404541 34682 solver.cpp:239] Iteration 13770 (1.84098 iter/s, 5.43188s/10 iters), loss = 9.36934
I0522 23:38:54.404585 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36934 (* 1 = 9.36934 loss)
I0522 23:38:54.477200 34682 sgd_solver.cpp:112] Iteration 13770, lr = 0.01
I0522 23:38:59.964371 34682 solver.cpp:239] Iteration 13780 (1.7987 iter/s, 5.55956s/10 iters), loss = 8.91747
I0522 23:38:59.964419 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91747 (* 1 = 8.91747 loss)
I0522 23:39:00.032395 34682 sgd_solver.cpp:112] Iteration 13780, lr = 0.01
I0522 23:39:05.809924 34682 solver.cpp:239] Iteration 13790 (1.71079 iter/s, 5.84527s/10 iters), loss = 8.96917
I0522 23:39:05.810127 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96917 (* 1 = 8.96917 loss)
I0522 23:39:06.659654 34682 sgd_solver.cpp:112] Iteration 13790, lr = 0.01
I0522 23:39:13.020720 34682 solver.cpp:239] Iteration 13800 (1.3869 iter/s, 7.21033s/10 iters), loss = 8.29248
I0522 23:39:13.020787 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29248 (* 1 = 8.29248 loss)
I0522 23:39:13.081588 34682 sgd_solver.cpp:112] Iteration 13800, lr = 0.01
I0522 23:39:18.855000 34682 solver.cpp:239] Iteration 13810 (1.7141 iter/s, 5.83398s/10 iters), loss = 8.94989
I0522 23:39:18.855057 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94989 (* 1 = 8.94989 loss)
I0522 23:39:19.708916 34682 sgd_solver.cpp:112] Iteration 13810, lr = 0.01
I0522 23:39:25.597609 34682 solver.cpp:239] Iteration 13820 (1.48318 iter/s, 6.74227s/10 iters), loss = 8.74576
I0522 23:39:25.597656 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74576 (* 1 = 8.74576 loss)
I0522 23:39:25.674263 34682 sgd_solver.cpp:112] Iteration 13820, lr = 0.01
I0522 23:39:30.055444 34682 solver.cpp:239] Iteration 13830 (2.24336 iter/s, 4.4576s/10 iters), loss = 8.90285
I0522 23:39:30.055498 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90285 (* 1 = 8.90285 loss)
I0522 23:39:30.125428 34682 sgd_solver.cpp:112] Iteration 13830, lr = 0.01
I0522 23:39:35.948597 34682 solver.cpp:239] Iteration 13840 (1.69697 iter/s, 5.89285s/10 iters), loss = 8.7488
I0522 23:39:35.948940 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7488 (* 1 = 8.7488 loss)
I0522 23:39:36.794018 34682 sgd_solver.cpp:112] Iteration 13840, lr = 0.01
I0522 23:39:40.021689 34682 solver.cpp:239] Iteration 13850 (2.45543 iter/s, 4.07261s/10 iters), loss = 8.57631
I0522 23:39:40.021729 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57631 (* 1 = 8.57631 loss)
I0522 23:39:40.109431 34682 sgd_solver.cpp:112] Iteration 13850, lr = 0.01
I0522 23:39:44.786646 34682 solver.cpp:239] Iteration 13860 (2.09877 iter/s, 4.7647s/10 iters), loss = 9.17691
I0522 23:39:44.786725 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17691 (* 1 = 9.17691 loss)
I0522 23:39:45.661242 34682 sgd_solver.cpp:112] Iteration 13860, lr = 0.01
I0522 23:39:50.498924 34682 solver.cpp:239] Iteration 13870 (1.75071 iter/s, 5.71196s/10 iters), loss = 8.77904
I0522 23:39:50.498986 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77904 (* 1 = 8.77904 loss)
I0522 23:39:51.315801 34682 sgd_solver.cpp:112] Iteration 13870, lr = 0.01
I0522 23:39:53.979281 34682 solver.cpp:239] Iteration 13880 (2.87344 iter/s, 3.48015s/10 iters), loss = 8.48001
I0522 23:39:53.979339 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48001 (* 1 = 8.48001 loss)
I0522 23:39:54.689417 34682 sgd_solver.cpp:112] Iteration 13880, lr = 0.01
I0522 23:39:58.309548 34682 solver.cpp:239] Iteration 13890 (2.30945 iter/s, 4.33003s/10 iters), loss = 9.59451
I0522 23:39:58.309610 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59451 (* 1 = 9.59451 loss)
I0522 23:39:58.378705 34682 sgd_solver.cpp:112] Iteration 13890, lr = 0.01
I0522 23:40:05.663288 34682 solver.cpp:239] Iteration 13900 (1.35992 iter/s, 7.35338s/10 iters), loss = 9.58247
I0522 23:40:05.663347 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.58247 (* 1 = 9.58247 loss)
I0522 23:40:05.724850 34682 sgd_solver.cpp:112] Iteration 13900, lr = 0.01
I0522 23:40:09.406790 34682 solver.cpp:239] Iteration 13910 (2.67144 iter/s, 3.74329s/10 iters), loss = 8.65963
I0522 23:40:09.407054 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65963 (* 1 = 8.65963 loss)
I0522 23:40:10.252941 34682 sgd_solver.cpp:112] Iteration 13910, lr = 0.01
I0522 23:40:15.683022 34682 solver.cpp:239] Iteration 13920 (1.59344 iter/s, 6.27574s/10 iters), loss = 8.6888
I0522 23:40:15.683070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6888 (* 1 = 8.6888 loss)
I0522 23:40:15.751874 34682 sgd_solver.cpp:112] Iteration 13920, lr = 0.01
I0522 23:40:20.530990 34682 solver.cpp:239] Iteration 13930 (2.06283 iter/s, 4.84771s/10 iters), loss = 9.09106
I0522 23:40:20.531033 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09106 (* 1 = 9.09106 loss)
I0522 23:40:20.609614 34682 sgd_solver.cpp:112] Iteration 13930, lr = 0.01
I0522 23:40:26.752826 34682 solver.cpp:239] Iteration 13940 (1.60732 iter/s, 6.22153s/10 iters), loss = 8.75378
I0522 23:40:26.752894 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75378 (* 1 = 8.75378 loss)
I0522 23:40:26.831703 34682 sgd_solver.cpp:112] Iteration 13940, lr = 0.01
I0522 23:40:32.560395 34682 solver.cpp:239] Iteration 13950 (1.72198 iter/s, 5.80727s/10 iters), loss = 8.79335
I0522 23:40:32.560446 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79335 (* 1 = 8.79335 loss)
I0522 23:40:33.326345 34682 sgd_solver.cpp:112] Iteration 13950, lr = 0.01
I0522 23:40:40.064010 34682 solver.cpp:239] Iteration 13960 (1.33275 iter/s, 7.50326s/10 iters), loss = 9.21136
I0522 23:40:40.064306 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21136 (* 1 = 9.21136 loss)
I0522 23:40:40.820986 34682 sgd_solver.cpp:112] Iteration 13960, lr = 0.01
I0522 23:40:46.480420 34682 solver.cpp:239] Iteration 13970 (1.55863 iter/s, 6.41589s/10 iters), loss = 9.01397
I0522 23:40:46.480470 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01397 (* 1 = 9.01397 loss)
I0522 23:40:46.552273 34682 sgd_solver.cpp:112] Iteration 13970, lr = 0.01
I0522 23:40:50.645567 34682 solver.cpp:239] Iteration 13980 (2.401 iter/s, 4.16493s/10 iters), loss = 8.31631
I0522 23:40:50.645611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31631 (* 1 = 8.31631 loss)
I0522 23:40:50.722618 34682 sgd_solver.cpp:112] Iteration 13980, lr = 0.01
I0522 23:40:55.904953 34682 solver.cpp:239] Iteration 13990 (1.90146 iter/s, 5.25912s/10 iters), loss = 8.96996
I0522 23:40:55.904999 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96996 (* 1 = 8.96996 loss)
I0522 23:40:55.982318 34682 sgd_solver.cpp:112] Iteration 13990, lr = 0.01
I0522 23:40:59.454443 34682 solver.cpp:239] Iteration 14000 (2.81746 iter/s, 3.5493s/10 iters), loss = 8.97673
I0522 23:40:59.454483 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97673 (* 1 = 8.97673 loss)
I0522 23:40:59.534986 34682 sgd_solver.cpp:112] Iteration 14000, lr = 0.01
I0522 23:41:03.693171 34682 solver.cpp:239] Iteration 14010 (2.35933 iter/s, 4.2385s/10 iters), loss = 8.56158
I0522 23:41:03.693241 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56158 (* 1 = 8.56158 loss)
I0522 23:41:04.556077 34682 sgd_solver.cpp:112] Iteration 14010, lr = 0.01
I0522 23:41:10.089720 34682 solver.cpp:239] Iteration 14020 (1.56343 iter/s, 6.3962s/10 iters), loss = 8.47193
I0522 23:41:10.089952 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47193 (* 1 = 8.47193 loss)
I0522 23:41:10.153816 34682 sgd_solver.cpp:112] Iteration 14020, lr = 0.01
I0522 23:41:15.025732 34682 solver.cpp:239] Iteration 14030 (2.02609 iter/s, 4.93561s/10 iters), loss = 9.17257
I0522 23:41:15.025789 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17257 (* 1 = 9.17257 loss)
I0522 23:41:15.869840 34682 sgd_solver.cpp:112] Iteration 14030, lr = 0.01
I0522 23:41:21.760507 34682 solver.cpp:239] Iteration 14040 (1.4849 iter/s, 6.73445s/10 iters), loss = 8.73594
I0522 23:41:21.760552 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73594 (* 1 = 8.73594 loss)
I0522 23:41:21.829577 34682 sgd_solver.cpp:112] Iteration 14040, lr = 0.01
I0522 23:41:25.917675 34682 solver.cpp:239] Iteration 14050 (2.40561 iter/s, 4.15695s/10 iters), loss = 9.38752
I0522 23:41:25.917740 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.38752 (* 1 = 9.38752 loss)
I0522 23:41:26.779861 34682 sgd_solver.cpp:112] Iteration 14050, lr = 0.01
I0522 23:41:31.002748 34682 solver.cpp:239] Iteration 14060 (1.96666 iter/s, 5.08477s/10 iters), loss = 8.84082
I0522 23:41:31.002799 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84082 (* 1 = 8.84082 loss)
I0522 23:41:31.846796 34682 sgd_solver.cpp:112] Iteration 14060, lr = 0.01
I0522 23:41:36.966574 34682 solver.cpp:239] Iteration 14070 (1.67686 iter/s, 5.96352s/10 iters), loss = 9.30305
I0522 23:41:36.966634 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30305 (* 1 = 9.30305 loss)
I0522 23:41:37.764518 34682 sgd_solver.cpp:112] Iteration 14070, lr = 0.01
I0522 23:41:40.913631 34682 solver.cpp:239] Iteration 14080 (2.53367 iter/s, 3.94684s/10 iters), loss = 8.49412
I0522 23:41:40.913878 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49412 (* 1 = 8.49412 loss)
I0522 23:41:40.994319 34682 sgd_solver.cpp:112] Iteration 14080, lr = 0.01
I0522 23:41:44.470587 34682 solver.cpp:239] Iteration 14090 (2.81169 iter/s, 3.55658s/10 iters), loss = 8.369
I0522 23:41:44.470636 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.369 (* 1 = 8.369 loss)
I0522 23:41:44.554358 34682 sgd_solver.cpp:112] Iteration 14090, lr = 0.01
I0522 23:41:49.010355 34682 solver.cpp:239] Iteration 14100 (2.20288 iter/s, 4.53952s/10 iters), loss = 9.29848
I0522 23:41:49.010421 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29848 (* 1 = 9.29848 loss)
I0522 23:41:49.653931 34682 sgd_solver.cpp:112] Iteration 14100, lr = 0.01
I0522 23:41:54.349789 34682 solver.cpp:239] Iteration 14110 (1.87296 iter/s, 5.33915s/10 iters), loss = 8.76843
I0522 23:41:54.349848 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76843 (* 1 = 8.76843 loss)
I0522 23:41:54.418964 34682 sgd_solver.cpp:112] Iteration 14110, lr = 0.01
I0522 23:41:56.932277 34682 solver.cpp:239] Iteration 14120 (3.87249 iter/s, 2.58232s/10 iters), loss = 9.30161
I0522 23:41:56.932323 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30161 (* 1 = 9.30161 loss)
I0522 23:41:57.626508 34682 sgd_solver.cpp:112] Iteration 14120, lr = 0.01
I0522 23:42:01.661609 34682 solver.cpp:239] Iteration 14130 (2.11458 iter/s, 4.72907s/10 iters), loss = 8.81627
I0522 23:42:01.661667 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81627 (* 1 = 8.81627 loss)
I0522 23:42:02.439517 34682 sgd_solver.cpp:112] Iteration 14130, lr = 0.01
I0522 23:42:05.773656 34682 solver.cpp:239] Iteration 14140 (2.43202 iter/s, 4.11181s/10 iters), loss = 9.47781
I0522 23:42:05.773705 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47781 (* 1 = 9.47781 loss)
I0522 23:42:06.597721 34682 sgd_solver.cpp:112] Iteration 14140, lr = 0.01
I0522 23:42:09.850055 34682 solver.cpp:239] Iteration 14150 (2.45328 iter/s, 4.07618s/10 iters), loss = 8.62085
I0522 23:42:09.850105 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62085 (* 1 = 8.62085 loss)
I0522 23:42:10.396428 34682 sgd_solver.cpp:112] Iteration 14150, lr = 0.01
I0522 23:42:15.455099 34682 solver.cpp:239] Iteration 14160 (1.7842 iter/s, 5.60476s/10 iters), loss = 8.96913
I0522 23:42:15.455260 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96913 (* 1 = 8.96913 loss)
I0522 23:42:15.514433 34682 sgd_solver.cpp:112] Iteration 14160, lr = 0.01
I0522 23:42:19.238193 34682 solver.cpp:239] Iteration 14170 (2.64355 iter/s, 3.78279s/10 iters), loss = 9.2656
I0522 23:42:19.238250 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2656 (* 1 = 9.2656 loss)
I0522 23:42:19.311046 34682 sgd_solver.cpp:112] Iteration 14170, lr = 0.01
I0522 23:42:24.205142 34682 solver.cpp:239] Iteration 14180 (2.01341 iter/s, 4.96669s/10 iters), loss = 8.57065
I0522 23:42:24.205199 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57065 (* 1 = 8.57065 loss)
I0522 23:42:24.266299 34682 sgd_solver.cpp:112] Iteration 14180, lr = 0.01
I0522 23:42:28.828553 34682 solver.cpp:239] Iteration 14190 (2.16302 iter/s, 4.62316s/10 iters), loss = 9.60381
I0522 23:42:28.828608 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60381 (* 1 = 9.60381 loss)
I0522 23:42:28.888618 34682 sgd_solver.cpp:112] Iteration 14190, lr = 0.01
I0522 23:42:32.942989 34682 solver.cpp:239] Iteration 14200 (2.43061 iter/s, 4.1142s/10 iters), loss = 8.10762
I0522 23:42:32.943037 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10762 (* 1 = 8.10762 loss)
I0522 23:42:33.006886 34682 sgd_solver.cpp:112] Iteration 14200, lr = 0.01
I0522 23:42:38.017573 34682 solver.cpp:239] Iteration 14210 (1.97071 iter/s, 5.07432s/10 iters), loss = 8.81571
I0522 23:42:38.017634 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81571 (* 1 = 8.81571 loss)
I0522 23:42:38.740970 34682 sgd_solver.cpp:112] Iteration 14210, lr = 0.01
I0522 23:42:43.956228 34682 solver.cpp:239] Iteration 14220 (1.68397 iter/s, 5.93835s/10 iters), loss = 8.76697
I0522 23:42:43.956271 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76697 (* 1 = 8.76697 loss)
I0522 23:42:44.030853 34682 sgd_solver.cpp:112] Iteration 14220, lr = 0.01
I0522 23:42:47.645491 34682 solver.cpp:239] Iteration 14230 (2.71072 iter/s, 3.68906s/10 iters), loss = 8.29297
I0522 23:42:47.645694 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29297 (* 1 = 8.29297 loss)
I0522 23:42:47.718802 34682 sgd_solver.cpp:112] Iteration 14230, lr = 0.01
I0522 23:42:51.057927 34682 solver.cpp:239] Iteration 14240 (2.93075 iter/s, 3.41209s/10 iters), loss = 8.19393
I0522 23:42:51.057981 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19393 (* 1 = 8.19393 loss)
I0522 23:42:51.698351 34682 sgd_solver.cpp:112] Iteration 14240, lr = 0.01
I0522 23:42:56.842947 34682 solver.cpp:239] Iteration 14250 (1.72869 iter/s, 5.78472s/10 iters), loss = 8.3633
I0522 23:42:56.843013 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3633 (* 1 = 8.3633 loss)
I0522 23:42:57.521739 34682 sgd_solver.cpp:112] Iteration 14250, lr = 0.01
I0522 23:43:00.705220 34682 solver.cpp:239] Iteration 14260 (2.5893 iter/s, 3.86205s/10 iters), loss = 8.13677
I0522 23:43:00.705277 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13677 (* 1 = 8.13677 loss)
I0522 23:43:00.770481 34682 sgd_solver.cpp:112] Iteration 14260, lr = 0.01
I0522 23:43:05.613020 34682 solver.cpp:239] Iteration 14270 (2.03768 iter/s, 4.90755s/10 iters), loss = 9.09954
I0522 23:43:05.613065 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09954 (* 1 = 9.09954 loss)
I0522 23:43:06.123097 34682 sgd_solver.cpp:112] Iteration 14270, lr = 0.01
I0522 23:43:11.547606 34682 solver.cpp:239] Iteration 14280 (1.68512 iter/s, 5.93429s/10 iters), loss = 9.02282
I0522 23:43:11.547657 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02282 (* 1 = 9.02282 loss)
I0522 23:43:11.614383 34682 sgd_solver.cpp:112] Iteration 14280, lr = 0.01
I0522 23:43:18.016674 34682 solver.cpp:239] Iteration 14290 (1.54589 iter/s, 6.46876s/10 iters), loss = 8.68536
I0522 23:43:18.016906 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68536 (* 1 = 8.68536 loss)
I0522 23:43:18.249439 34682 sgd_solver.cpp:112] Iteration 14290, lr = 0.01
I0522 23:43:22.061188 34682 solver.cpp:239] Iteration 14300 (2.47271 iter/s, 4.04414s/10 iters), loss = 8.7286
I0522 23:43:22.061239 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7286 (* 1 = 8.7286 loss)
I0522 23:43:22.900171 34682 sgd_solver.cpp:112] Iteration 14300, lr = 0.01
I0522 23:43:26.065397 34682 solver.cpp:239] Iteration 14310 (2.49751 iter/s, 4.00398s/10 iters), loss = 8.25335
I0522 23:43:26.065445 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25335 (* 1 = 8.25335 loss)
I0522 23:43:26.139406 34682 sgd_solver.cpp:112] Iteration 14310, lr = 0.01
I0522 23:43:28.002636 34682 solver.cpp:239] Iteration 14320 (5.16236 iter/s, 1.9371s/10 iters), loss = 8.33179
I0522 23:43:28.002686 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33179 (* 1 = 8.33179 loss)
I0522 23:43:28.653214 34682 sgd_solver.cpp:112] Iteration 14320, lr = 0.01
I0522 23:43:32.797950 34682 solver.cpp:239] Iteration 14330 (2.08548 iter/s, 4.79505s/10 iters), loss = 8.89639
I0522 23:43:32.798010 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89639 (* 1 = 8.89639 loss)
I0522 23:43:33.394038 34682 sgd_solver.cpp:112] Iteration 14330, lr = 0.01
I0522 23:43:38.783933 34682 solver.cpp:239] Iteration 14340 (1.67066 iter/s, 5.98566s/10 iters), loss = 8.18107
I0522 23:43:38.784005 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18107 (* 1 = 8.18107 loss)
I0522 23:43:39.561796 34682 sgd_solver.cpp:112] Iteration 14340, lr = 0.01
I0522 23:43:45.812448 34682 solver.cpp:239] Iteration 14350 (1.42285 iter/s, 7.02816s/10 iters), loss = 8.44636
I0522 23:43:45.812490 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44636 (* 1 = 8.44636 loss)
I0522 23:43:45.884971 34682 sgd_solver.cpp:112] Iteration 14350, lr = 0.01
I0522 23:43:50.644953 34682 solver.cpp:239] Iteration 14360 (2.06942 iter/s, 4.83226s/10 iters), loss = 9.14849
I0522 23:43:50.645217 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14849 (* 1 = 9.14849 loss)
I0522 23:43:51.539765 34682 sgd_solver.cpp:112] Iteration 14360, lr = 0.01
I0522 23:43:56.996932 34682 solver.cpp:239] Iteration 14370 (1.57443 iter/s, 6.35149s/10 iters), loss = 9.07352
I0522 23:43:56.996981 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07352 (* 1 = 9.07352 loss)
I0522 23:43:57.069602 34682 sgd_solver.cpp:112] Iteration 14370, lr = 0.01
I0522 23:44:02.143503 34682 solver.cpp:239] Iteration 14380 (1.94314 iter/s, 5.14631s/10 iters), loss = 9.13442
I0522 23:44:02.143563 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13442 (* 1 = 9.13442 loss)
I0522 23:44:02.986892 34682 sgd_solver.cpp:112] Iteration 14380, lr = 0.01
I0522 23:44:08.218931 34682 solver.cpp:239] Iteration 14390 (1.64606 iter/s, 6.07513s/10 iters), loss = 8.45142
I0522 23:44:08.218981 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45142 (* 1 = 8.45142 loss)
I0522 23:44:08.288866 34682 sgd_solver.cpp:112] Iteration 14390, lr = 0.01
I0522 23:44:10.725250 34682 solver.cpp:239] Iteration 14400 (3.99017 iter/s, 2.50616s/10 iters), loss = 9.06423
I0522 23:44:10.725307 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06423 (* 1 = 9.06423 loss)
I0522 23:44:10.813529 34682 sgd_solver.cpp:112] Iteration 14400, lr = 0.01
I0522 23:44:14.802127 34682 solver.cpp:239] Iteration 14410 (2.45299 iter/s, 4.07665s/10 iters), loss = 9.28698
I0522 23:44:14.802184 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28698 (* 1 = 9.28698 loss)
I0522 23:44:14.864579 34682 sgd_solver.cpp:112] Iteration 14410, lr = 0.01
I0522 23:44:18.993005 34682 solver.cpp:239] Iteration 14420 (2.38627 iter/s, 4.19065s/10 iters), loss = 7.92592
I0522 23:44:18.993067 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92592 (* 1 = 7.92592 loss)
I0522 23:44:19.050834 34682 sgd_solver.cpp:112] Iteration 14420, lr = 0.01
I0522 23:44:22.311466 34682 solver.cpp:239] Iteration 14430 (3.01362 iter/s, 3.31826s/10 iters), loss = 8.37122
I0522 23:44:22.311585 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37122 (* 1 = 8.37122 loss)
I0522 23:44:23.058744 34682 sgd_solver.cpp:112] Iteration 14430, lr = 0.01
I0522 23:44:27.651793 34682 solver.cpp:239] Iteration 14440 (1.8742 iter/s, 5.3356s/10 iters), loss = 8.96891
I0522 23:44:27.651865 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96891 (* 1 = 8.96891 loss)
I0522 23:44:28.418189 34682 sgd_solver.cpp:112] Iteration 14440, lr = 0.01
I0522 23:44:32.497247 34682 solver.cpp:239] Iteration 14450 (2.0639 iter/s, 4.84519s/10 iters), loss = 8.09933
I0522 23:44:32.497301 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09933 (* 1 = 8.09933 loss)
I0522 23:44:33.293530 34682 sgd_solver.cpp:112] Iteration 14450, lr = 0.01
I0522 23:44:36.781570 34682 solver.cpp:239] Iteration 14460 (2.33422 iter/s, 4.28409s/10 iters), loss = 8.40404
I0522 23:44:36.781623 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40404 (* 1 = 8.40404 loss)
I0522 23:44:36.848851 34682 sgd_solver.cpp:112] Iteration 14460, lr = 0.01
I0522 23:44:41.620709 34682 solver.cpp:239] Iteration 14470 (2.06659 iter/s, 4.83889s/10 iters), loss = 8.20812
I0522 23:44:41.620757 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20812 (* 1 = 8.20812 loss)
I0522 23:44:41.686722 34682 sgd_solver.cpp:112] Iteration 14470, lr = 0.01
I0522 23:44:44.826669 34682 solver.cpp:239] Iteration 14480 (3.11938 iter/s, 3.20577s/10 iters), loss = 9.15741
I0522 23:44:44.826758 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15741 (* 1 = 9.15741 loss)
I0522 23:44:44.904202 34682 sgd_solver.cpp:112] Iteration 14480, lr = 0.01
I0522 23:44:48.986482 34682 solver.cpp:239] Iteration 14490 (2.40411 iter/s, 4.15954s/10 iters), loss = 8.4967
I0522 23:44:48.986541 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4967 (* 1 = 8.4967 loss)
I0522 23:44:49.864051 34682 sgd_solver.cpp:112] Iteration 14490, lr = 0.01
I0522 23:44:53.500282 34682 solver.cpp:239] Iteration 14500 (2.21555 iter/s, 4.51355s/10 iters), loss = 8.72146
I0522 23:44:53.500545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72146 (* 1 = 8.72146 loss)
I0522 23:44:53.579990 34682 sgd_solver.cpp:112] Iteration 14500, lr = 0.01
I0522 23:44:56.940233 34682 solver.cpp:239] Iteration 14510 (2.91098 iter/s, 3.43527s/10 iters), loss = 8.4585
I0522 23:44:56.940300 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4585 (* 1 = 8.4585 loss)
I0522 23:44:57.693806 34682 sgd_solver.cpp:112] Iteration 14510, lr = 0.01
I0522 23:45:00.654275 34682 solver.cpp:239] Iteration 14520 (2.69265 iter/s, 3.71382s/10 iters), loss = 9.10522
I0522 23:45:00.654328 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10522 (* 1 = 9.10522 loss)
I0522 23:45:01.505741 34682 sgd_solver.cpp:112] Iteration 14520, lr = 0.01
I0522 23:45:05.590760 34682 solver.cpp:239] Iteration 14530 (2.02584 iter/s, 4.93623s/10 iters), loss = 8.60572
I0522 23:45:05.590818 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60572 (* 1 = 8.60572 loss)
I0522 23:45:05.678660 34682 sgd_solver.cpp:112] Iteration 14530, lr = 0.01
I0522 23:45:10.417325 34682 solver.cpp:239] Iteration 14540 (2.07197 iter/s, 4.82632s/10 iters), loss = 9.58643
I0522 23:45:10.417367 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.58643 (* 1 = 9.58643 loss)
I0522 23:45:11.270772 34682 sgd_solver.cpp:112] Iteration 14540, lr = 0.01
I0522 23:45:15.859859 34682 solver.cpp:239] Iteration 14550 (1.83747 iter/s, 5.44227s/10 iters), loss = 8.95554
I0522 23:45:15.859908 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95554 (* 1 = 8.95554 loss)
I0522 23:45:15.926210 34682 sgd_solver.cpp:112] Iteration 14550, lr = 0.01
I0522 23:45:19.996300 34682 solver.cpp:239] Iteration 14560 (2.41767 iter/s, 4.13621s/10 iters), loss = 8.99994
I0522 23:45:19.996366 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99994 (* 1 = 8.99994 loss)
I0522 23:45:20.849265 34682 sgd_solver.cpp:112] Iteration 14560, lr = 0.01
I0522 23:45:25.273965 34682 solver.cpp:239] Iteration 14570 (1.89488 iter/s, 5.27738s/10 iters), loss = 8.74688
I0522 23:45:25.274080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74688 (* 1 = 8.74688 loss)
I0522 23:45:26.011835 34682 sgd_solver.cpp:112] Iteration 14570, lr = 0.01
I0522 23:45:29.457510 34682 solver.cpp:239] Iteration 14580 (2.39049 iter/s, 4.18324s/10 iters), loss = 9.09973
I0522 23:45:29.457597 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09973 (* 1 = 9.09973 loss)
I0522 23:45:30.263576 34682 sgd_solver.cpp:112] Iteration 14580, lr = 0.01
I0522 23:45:33.374668 34682 solver.cpp:239] Iteration 14590 (2.55303 iter/s, 3.91692s/10 iters), loss = 8.09143
I0522 23:45:33.374743 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09143 (* 1 = 8.09143 loss)
I0522 23:45:33.444380 34682 sgd_solver.cpp:112] Iteration 14590, lr = 0.01
I0522 23:45:38.585479 34682 solver.cpp:239] Iteration 14600 (1.91919 iter/s, 5.21052s/10 iters), loss = 8.88159
I0522 23:45:38.585528 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88159 (* 1 = 8.88159 loss)
I0522 23:45:39.395790 34682 sgd_solver.cpp:112] Iteration 14600, lr = 0.01
I0522 23:45:42.718751 34682 solver.cpp:239] Iteration 14610 (2.41952 iter/s, 4.13305s/10 iters), loss = 8.42523
I0522 23:45:42.718801 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42523 (* 1 = 8.42523 loss)
I0522 23:45:43.395332 34682 sgd_solver.cpp:112] Iteration 14610, lr = 0.01
I0522 23:45:47.768606 34682 solver.cpp:239] Iteration 14620 (1.98036 iter/s, 5.0496s/10 iters), loss = 8.86748
I0522 23:45:47.768651 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86748 (* 1 = 8.86748 loss)
I0522 23:45:47.837040 34682 sgd_solver.cpp:112] Iteration 14620, lr = 0.01
I0522 23:45:53.415709 34682 solver.cpp:239] Iteration 14630 (1.77091 iter/s, 5.64682s/10 iters), loss = 8.55902
I0522 23:45:53.415766 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55902 (* 1 = 8.55902 loss)
I0522 23:45:54.295183 34682 sgd_solver.cpp:112] Iteration 14630, lr = 0.01
I0522 23:45:59.000170 34682 solver.cpp:239] Iteration 14640 (1.79077 iter/s, 5.58418s/10 iters), loss = 9.05616
I0522 23:45:59.002436 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05616 (* 1 = 9.05616 loss)
I0522 23:45:59.077569 34682 sgd_solver.cpp:112] Iteration 14640, lr = 0.01
I0522 23:46:03.029196 34682 solver.cpp:239] Iteration 14650 (2.48398 iter/s, 4.0258s/10 iters), loss = 8.04999
I0522 23:46:03.029242 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04999 (* 1 = 8.04999 loss)
I0522 23:46:03.096077 34682 sgd_solver.cpp:112] Iteration 14650, lr = 0.01
I0522 23:46:06.977370 34682 solver.cpp:239] Iteration 14660 (2.53295 iter/s, 3.94796s/10 iters), loss = 8.40777
I0522 23:46:06.977421 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40777 (* 1 = 8.40777 loss)
I0522 23:46:07.055251 34682 sgd_solver.cpp:112] Iteration 14660, lr = 0.01
I0522 23:46:11.145663 34682 solver.cpp:239] Iteration 14670 (2.39919 iter/s, 4.16807s/10 iters), loss = 8.09609
I0522 23:46:11.145715 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09609 (* 1 = 8.09609 loss)
I0522 23:46:11.222419 34682 sgd_solver.cpp:112] Iteration 14670, lr = 0.01
I0522 23:46:18.370815 34682 solver.cpp:239] Iteration 14680 (1.38412 iter/s, 7.2248s/10 iters), loss = 9.06807
I0522 23:46:18.370867 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06807 (* 1 = 9.06807 loss)
I0522 23:46:18.426722 34682 sgd_solver.cpp:112] Iteration 14680, lr = 0.01
I0522 23:46:24.471848 34682 solver.cpp:239] Iteration 14690 (1.63915 iter/s, 6.10074s/10 iters), loss = 7.97221
I0522 23:46:24.471887 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97221 (* 1 = 7.97221 loss)
I0522 23:46:24.549651 34682 sgd_solver.cpp:112] Iteration 14690, lr = 0.01
I0522 23:46:29.124950 34682 solver.cpp:239] Iteration 14700 (2.14923 iter/s, 4.65284s/10 iters), loss = 8.49849
I0522 23:46:29.125174 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49849 (* 1 = 8.49849 loss)
I0522 23:46:29.950701 34682 sgd_solver.cpp:112] Iteration 14700, lr = 0.01
I0522 23:46:34.985785 34682 solver.cpp:239] Iteration 14710 (1.70637 iter/s, 5.8604s/10 iters), loss = 8.61767
I0522 23:46:34.985841 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61767 (* 1 = 8.61767 loss)
I0522 23:46:35.048840 34682 sgd_solver.cpp:112] Iteration 14710, lr = 0.01
I0522 23:46:40.557498 34682 solver.cpp:239] Iteration 14720 (1.79487 iter/s, 5.57142s/10 iters), loss = 8.51987
I0522 23:46:40.557557 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51987 (* 1 = 8.51987 loss)
I0522 23:46:41.395936 34682 sgd_solver.cpp:112] Iteration 14720, lr = 0.01
I0522 23:46:44.210058 34682 solver.cpp:239] Iteration 14730 (2.73797 iter/s, 3.65235s/10 iters), loss = 8.68114
I0522 23:46:44.210110 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68114 (* 1 = 8.68114 loss)
I0522 23:46:45.021441 34682 sgd_solver.cpp:112] Iteration 14730, lr = 0.01
I0522 23:46:50.436436 34682 solver.cpp:239] Iteration 14740 (1.60615 iter/s, 6.22607s/10 iters), loss = 7.70202
I0522 23:46:50.436494 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.70202 (* 1 = 7.70202 loss)
I0522 23:46:50.502120 34682 sgd_solver.cpp:112] Iteration 14740, lr = 0.01
I0522 23:46:57.140535 34682 solver.cpp:239] Iteration 14750 (1.4917 iter/s, 6.70376s/10 iters), loss = 8.389
I0522 23:46:57.140583 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.389 (* 1 = 8.389 loss)
I0522 23:46:57.213914 34682 sgd_solver.cpp:112] Iteration 14750, lr = 0.01
I0522 23:47:01.282480 34682 solver.cpp:239] Iteration 14760 (2.41445 iter/s, 4.14172s/10 iters), loss = 8.78683
I0522 23:47:01.282608 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78683 (* 1 = 8.78683 loss)
I0522 23:47:01.372759 34682 sgd_solver.cpp:112] Iteration 14760, lr = 0.01
I0522 23:47:08.198957 34682 solver.cpp:239] Iteration 14770 (1.44591 iter/s, 6.91608s/10 iters), loss = 9.31852
I0522 23:47:08.199009 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31852 (* 1 = 9.31852 loss)
I0522 23:47:08.261178 34682 sgd_solver.cpp:112] Iteration 14770, lr = 0.01
I0522 23:47:13.708529 34682 solver.cpp:239] Iteration 14780 (1.81511 iter/s, 5.50929s/10 iters), loss = 8.84629
I0522 23:47:13.708578 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84629 (* 1 = 8.84629 loss)
I0522 23:47:13.768929 34682 sgd_solver.cpp:112] Iteration 14780, lr = 0.01
I0522 23:47:18.698065 34682 solver.cpp:239] Iteration 14790 (2.0043 iter/s, 4.98928s/10 iters), loss = 8.87356
I0522 23:47:18.698117 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87356 (* 1 = 8.87356 loss)
I0522 23:47:18.774260 34682 sgd_solver.cpp:112] Iteration 14790, lr = 0.01
I0522 23:47:22.182585 34682 solver.cpp:239] Iteration 14800 (2.87001 iter/s, 3.48431s/10 iters), loss = 8.62759
I0522 23:47:22.182637 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62759 (* 1 = 8.62759 loss)
I0522 23:47:22.250284 34682 sgd_solver.cpp:112] Iteration 14800, lr = 0.01
I0522 23:47:26.948385 34682 solver.cpp:239] Iteration 14810 (2.09839 iter/s, 4.76556s/10 iters), loss = 8.37445
I0522 23:47:26.948431 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37445 (* 1 = 8.37445 loss)
I0522 23:47:27.023897 34682 sgd_solver.cpp:112] Iteration 14810, lr = 0.01
I0522 23:47:32.078549 34682 solver.cpp:239] Iteration 14820 (1.94935 iter/s, 5.12991s/10 iters), loss = 8.95036
I0522 23:47:32.078670 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95036 (* 1 = 8.95036 loss)
I0522 23:47:32.161314 34682 sgd_solver.cpp:112] Iteration 14820, lr = 0.01
I0522 23:47:36.364784 34682 solver.cpp:239] Iteration 14830 (2.33322 iter/s, 4.28593s/10 iters), loss = 8.45334
I0522 23:47:36.364832 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45334 (* 1 = 8.45334 loss)
I0522 23:47:36.434581 34682 sgd_solver.cpp:112] Iteration 14830, lr = 0.01
I0522 23:47:40.676343 34682 solver.cpp:239] Iteration 14840 (2.31947 iter/s, 4.31133s/10 iters), loss = 8.5764
I0522 23:47:40.676400 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5764 (* 1 = 8.5764 loss)
I0522 23:47:41.343987 34682 sgd_solver.cpp:112] Iteration 14840, lr = 0.01
I0522 23:47:46.113739 34682 solver.cpp:239] Iteration 14850 (1.83922 iter/s, 5.4371s/10 iters), loss = 9.38101
I0522 23:47:46.113798 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.38101 (* 1 = 9.38101 loss)
I0522 23:47:46.948245 34682 sgd_solver.cpp:112] Iteration 14850, lr = 0.01
I0522 23:47:53.182265 34682 solver.cpp:239] Iteration 14860 (1.41479 iter/s, 7.06818s/10 iters), loss = 7.70959
I0522 23:47:53.182332 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.70959 (* 1 = 7.70959 loss)
I0522 23:47:53.262003 34682 sgd_solver.cpp:112] Iteration 14860, lr = 0.01
I0522 23:47:57.520087 34682 solver.cpp:239] Iteration 14870 (2.30544 iter/s, 4.33757s/10 iters), loss = 8.37736
I0522 23:47:57.520133 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37736 (* 1 = 8.37736 loss)
I0522 23:47:57.589463 34682 sgd_solver.cpp:112] Iteration 14870, lr = 0.01
I0522 23:48:02.581146 34682 solver.cpp:239] Iteration 14880 (1.97597 iter/s, 5.0608s/10 iters), loss = 8.95137
I0522 23:48:02.581377 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95137 (* 1 = 8.95137 loss)
I0522 23:48:02.650743 34682 sgd_solver.cpp:112] Iteration 14880, lr = 0.01
I0522 23:48:07.475097 34682 solver.cpp:239] Iteration 14890 (2.04351 iter/s, 4.89355s/10 iters), loss = 8.64249
I0522 23:48:07.475147 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64249 (* 1 = 8.64249 loss)
I0522 23:48:07.537418 34682 sgd_solver.cpp:112] Iteration 14890, lr = 0.01
I0522 23:48:13.419462 34682 solver.cpp:239] Iteration 14900 (1.68235 iter/s, 5.94406s/10 iters), loss = 9.12388
I0522 23:48:13.419531 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12388 (* 1 = 9.12388 loss)
I0522 23:48:14.223444 34682 sgd_solver.cpp:112] Iteration 14900, lr = 0.01
I0522 23:48:17.943133 34682 solver.cpp:239] Iteration 14910 (2.21072 iter/s, 4.5234s/10 iters), loss = 8.79646
I0522 23:48:17.943181 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79646 (* 1 = 8.79646 loss)
I0522 23:48:18.002431 34682 sgd_solver.cpp:112] Iteration 14910, lr = 0.01
I0522 23:48:22.312405 34682 solver.cpp:239] Iteration 14920 (2.28884 iter/s, 4.36902s/10 iters), loss = 8.16797
I0522 23:48:22.312500 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16797 (* 1 = 8.16797 loss)
I0522 23:48:22.388746 34682 sgd_solver.cpp:112] Iteration 14920, lr = 0.01
I0522 23:48:27.133114 34682 solver.cpp:239] Iteration 14930 (2.07451 iter/s, 4.82041s/10 iters), loss = 9.06359
I0522 23:48:27.133163 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06359 (* 1 = 9.06359 loss)
I0522 23:48:27.822685 34682 sgd_solver.cpp:112] Iteration 14930, lr = 0.01
I0522 23:48:31.776540 34682 solver.cpp:239] Iteration 14940 (2.1537 iter/s, 4.64318s/10 iters), loss = 8.26124
I0522 23:48:31.776597 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26124 (* 1 = 8.26124 loss)
I0522 23:48:31.840095 34682 sgd_solver.cpp:112] Iteration 14940, lr = 0.01
I0522 23:48:35.992799 34682 solver.cpp:239] Iteration 14950 (2.37191 iter/s, 4.21601s/10 iters), loss = 8.27598
I0522 23:48:35.993093 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27598 (* 1 = 8.27598 loss)
I0522 23:48:36.865530 34682 sgd_solver.cpp:112] Iteration 14950, lr = 0.01
I0522 23:48:42.609944 34682 solver.cpp:239] Iteration 14960 (1.51135 iter/s, 6.61661s/10 iters), loss = 8.95304
I0522 23:48:42.609999 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95304 (* 1 = 8.95304 loss)
I0522 23:48:42.682267 34682 sgd_solver.cpp:112] Iteration 14960, lr = 0.01
I0522 23:48:48.220715 34682 solver.cpp:239] Iteration 14970 (1.78238 iter/s, 5.61049s/10 iters), loss = 8.62211
I0522 23:48:48.220762 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62211 (* 1 = 8.62211 loss)
I0522 23:48:48.294121 34682 sgd_solver.cpp:112] Iteration 14970, lr = 0.01
I0522 23:48:53.133960 34682 solver.cpp:239] Iteration 14980 (2.03542 iter/s, 4.91299s/10 iters), loss = 8.98114
I0522 23:48:53.134011 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98114 (* 1 = 8.98114 loss)
I0522 23:48:53.194270 34682 sgd_solver.cpp:112] Iteration 14980, lr = 0.01
I0522 23:48:56.541368 34682 solver.cpp:239] Iteration 14990 (2.93497 iter/s, 3.40719s/10 iters), loss = 9.0595
I0522 23:48:56.541431 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0595 (* 1 = 9.0595 loss)
I0522 23:48:57.208874 34682 sgd_solver.cpp:112] Iteration 14990, lr = 0.01
I0522 23:49:03.980185 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_15000.caffemodel
I0522 23:49:04.908876 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_15000.solverstate
I0522 23:49:05.187360 34682 solver.cpp:239] Iteration 15000 (1.15666 iter/s, 8.64559s/10 iters), loss = 8.76432
I0522 23:49:05.187399 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76432 (* 1 = 8.76432 loss)
I0522 23:49:05.402789 34682 sgd_solver.cpp:112] Iteration 15000, lr = 0.01
I0522 23:49:08.592749 34682 solver.cpp:239] Iteration 15010 (2.93668 iter/s, 3.4052s/10 iters), loss = 8.77065
I0522 23:49:08.592960 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77065 (* 1 = 8.77065 loss)
I0522 23:49:08.666086 34682 sgd_solver.cpp:112] Iteration 15010, lr = 0.01
I0522 23:49:12.706017 34682 solver.cpp:239] Iteration 15020 (2.43138 iter/s, 4.1129s/10 iters), loss = 8.33542
I0522 23:49:12.706069 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33542 (* 1 = 8.33542 loss)
I0522 23:49:12.766542 34682 sgd_solver.cpp:112] Iteration 15020, lr = 0.01
I0522 23:49:19.017963 34682 solver.cpp:239] Iteration 15030 (1.58438 iter/s, 6.31163s/10 iters), loss = 8.56305
I0522 23:49:19.018009 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56305 (* 1 = 8.56305 loss)
I0522 23:49:19.087349 34682 sgd_solver.cpp:112] Iteration 15030, lr = 0.01
I0522 23:49:23.241400 34682 solver.cpp:239] Iteration 15040 (2.36786 iter/s, 4.22321s/10 iters), loss = 8.67508
I0522 23:49:23.241451 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67508 (* 1 = 8.67508 loss)
I0522 23:49:23.305809 34682 sgd_solver.cpp:112] Iteration 15040, lr = 0.01
I0522 23:49:28.362778 34682 solver.cpp:239] Iteration 15050 (1.9527 iter/s, 5.12112s/10 iters), loss = 8.72932
I0522 23:49:28.362828 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72932 (* 1 = 8.72932 loss)
I0522 23:49:28.441942 34682 sgd_solver.cpp:112] Iteration 15050, lr = 0.01
I0522 23:49:32.586670 34682 solver.cpp:239] Iteration 15060 (2.36762 iter/s, 4.22365s/10 iters), loss = 8.83013
I0522 23:49:32.586778 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83013 (* 1 = 8.83013 loss)
I0522 23:49:33.195880 34682 sgd_solver.cpp:112] Iteration 15060, lr = 0.01
I0522 23:49:38.661067 34682 solver.cpp:239] Iteration 15070 (1.64635 iter/s, 6.07404s/10 iters), loss = 8.93928
I0522 23:49:38.661219 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93928 (* 1 = 8.93928 loss)
I0522 23:49:38.724642 34682 sgd_solver.cpp:112] Iteration 15070, lr = 0.01
I0522 23:49:40.806911 34682 solver.cpp:239] Iteration 15080 (4.66071 iter/s, 2.14559s/10 iters), loss = 9.30089
I0522 23:49:40.806970 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30089 (* 1 = 9.30089 loss)
I0522 23:49:40.872284 34682 sgd_solver.cpp:112] Iteration 15080, lr = 0.01
I0522 23:49:47.052860 34682 solver.cpp:239] Iteration 15090 (1.60112 iter/s, 6.24564s/10 iters), loss = 8.17476
I0522 23:49:47.052911 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17476 (* 1 = 8.17476 loss)
I0522 23:49:47.127393 34682 sgd_solver.cpp:112] Iteration 15090, lr = 0.01
I0522 23:49:51.817059 34682 solver.cpp:239] Iteration 15100 (2.0991 iter/s, 4.76395s/10 iters), loss = 8.95271
I0522 23:49:51.817103 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95271 (* 1 = 8.95271 loss)
I0522 23:49:51.898236 34682 sgd_solver.cpp:112] Iteration 15100, lr = 0.01
I0522 23:49:56.628697 34682 solver.cpp:239] Iteration 15110 (2.0784 iter/s, 4.81139s/10 iters), loss = 9.10493
I0522 23:49:56.628754 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10493 (* 1 = 9.10493 loss)
I0522 23:49:56.709702 34682 sgd_solver.cpp:112] Iteration 15110, lr = 0.01
I0522 23:50:02.599596 34682 solver.cpp:239] Iteration 15120 (1.67487 iter/s, 5.9706s/10 iters), loss = 7.25238
I0522 23:50:02.599645 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.25238 (* 1 = 7.25238 loss)
I0522 23:50:03.426740 34682 sgd_solver.cpp:112] Iteration 15120, lr = 0.01
I0522 23:50:09.857425 34682 solver.cpp:239] Iteration 15130 (1.37789 iter/s, 7.25749s/10 iters), loss = 8.33264
I0522 23:50:09.857683 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33264 (* 1 = 8.33264 loss)
I0522 23:50:10.549474 34682 sgd_solver.cpp:112] Iteration 15130, lr = 0.01
I0522 23:50:14.237840 34682 solver.cpp:239] Iteration 15140 (2.2831 iter/s, 4.38s/10 iters), loss = 9.15867
I0522 23:50:14.237896 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15867 (* 1 = 9.15867 loss)
I0522 23:50:15.094516 34682 sgd_solver.cpp:112] Iteration 15140, lr = 0.01
I0522 23:50:20.881803 34682 solver.cpp:239] Iteration 15150 (1.5052 iter/s, 6.64364s/10 iters), loss = 7.61677
I0522 23:50:20.881893 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.61677 (* 1 = 7.61677 loss)
I0522 23:50:21.719859 34682 sgd_solver.cpp:112] Iteration 15150, lr = 0.01
I0522 23:50:27.045578 34682 solver.cpp:239] Iteration 15160 (1.62247 iter/s, 6.16344s/10 iters), loss = 9.21369
I0522 23:50:27.045624 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21369 (* 1 = 9.21369 loss)
I0522 23:50:27.113605 34682 sgd_solver.cpp:112] Iteration 15160, lr = 0.01
I0522 23:50:31.356806 34682 solver.cpp:239] Iteration 15170 (2.31964 iter/s, 4.31101s/10 iters), loss = 8.75044
I0522 23:50:31.356853 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75044 (* 1 = 8.75044 loss)
I0522 23:50:32.004662 34682 sgd_solver.cpp:112] Iteration 15170, lr = 0.01
I0522 23:50:34.716742 34682 solver.cpp:239] Iteration 15180 (2.97642 iter/s, 3.35974s/10 iters), loss = 8.28721
I0522 23:50:34.716799 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28721 (* 1 = 8.28721 loss)
I0522 23:50:34.784509 34682 sgd_solver.cpp:112] Iteration 15180, lr = 0.01
I0522 23:50:38.900704 34682 solver.cpp:239] Iteration 15190 (2.39021 iter/s, 4.18373s/10 iters), loss = 8.78361
I0522 23:50:38.900748 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78361 (* 1 = 8.78361 loss)
I0522 23:50:38.973234 34682 sgd_solver.cpp:112] Iteration 15190, lr = 0.01
I0522 23:50:45.181792 34682 solver.cpp:239] Iteration 15200 (1.59216 iter/s, 6.28078s/10 iters), loss = 9.13045
I0522 23:50:45.182034 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13045 (* 1 = 9.13045 loss)
I0522 23:50:45.239145 34682 sgd_solver.cpp:112] Iteration 15200, lr = 0.01
I0522 23:50:50.713479 34682 solver.cpp:239] Iteration 15210 (1.80792 iter/s, 5.53122s/10 iters), loss = 9.40956
I0522 23:50:50.713524 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40956 (* 1 = 9.40956 loss)
I0522 23:50:50.793169 34682 sgd_solver.cpp:112] Iteration 15210, lr = 0.01
I0522 23:50:57.237610 34682 solver.cpp:239] Iteration 15220 (1.53284 iter/s, 6.52382s/10 iters), loss = 8.77917
I0522 23:50:57.237664 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77917 (* 1 = 8.77917 loss)
I0522 23:50:58.039374 34682 sgd_solver.cpp:112] Iteration 15220, lr = 0.01
I0522 23:51:03.261178 34682 solver.cpp:239] Iteration 15230 (1.66023 iter/s, 6.02327s/10 iters), loss = 8.71468
I0522 23:51:03.261225 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71468 (* 1 = 8.71468 loss)
I0522 23:51:04.099753 34682 sgd_solver.cpp:112] Iteration 15230, lr = 0.01
I0522 23:51:10.092499 34682 solver.cpp:239] Iteration 15240 (1.46392 iter/s, 6.83099s/10 iters), loss = 8.88004
I0522 23:51:10.092563 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88004 (* 1 = 8.88004 loss)
I0522 23:51:10.695117 34682 sgd_solver.cpp:112] Iteration 15240, lr = 0.01
I0522 23:51:15.130586 34682 solver.cpp:239] Iteration 15250 (1.98499 iter/s, 5.03781s/10 iters), loss = 8.55953
I0522 23:51:15.130633 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55953 (* 1 = 8.55953 loss)
I0522 23:51:15.204366 34682 sgd_solver.cpp:112] Iteration 15250, lr = 0.01
I0522 23:51:19.298964 34682 solver.cpp:239] Iteration 15260 (2.39914 iter/s, 4.16816s/10 iters), loss = 9.86488
I0522 23:51:19.299003 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.86488 (* 1 = 9.86488 loss)
I0522 23:51:19.375516 34682 sgd_solver.cpp:112] Iteration 15260, lr = 0.01
I0522 23:51:24.615710 34682 solver.cpp:239] Iteration 15270 (1.88095 iter/s, 5.31647s/10 iters), loss = 8.58926
I0522 23:51:24.615770 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58926 (* 1 = 8.58926 loss)
I0522 23:51:25.322257 34682 sgd_solver.cpp:112] Iteration 15270, lr = 0.01
I0522 23:51:27.821276 34682 solver.cpp:239] Iteration 15280 (3.11976 iter/s, 3.20537s/10 iters), loss = 8.68866
I0522 23:51:27.821332 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68866 (* 1 = 8.68866 loss)
I0522 23:51:28.690196 34682 sgd_solver.cpp:112] Iteration 15280, lr = 0.01
I0522 23:51:31.850291 34682 solver.cpp:239] Iteration 15290 (2.48213 iter/s, 4.02879s/10 iters), loss = 8.84506
I0522 23:51:31.850353 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84506 (* 1 = 8.84506 loss)
I0522 23:51:31.927008 34682 sgd_solver.cpp:112] Iteration 15290, lr = 0.01
I0522 23:51:35.596163 34682 solver.cpp:239] Iteration 15300 (2.66976 iter/s, 3.74566s/10 iters), loss = 8.8876
I0522 23:51:35.596217 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8876 (* 1 = 8.8876 loss)
I0522 23:51:36.376353 34682 sgd_solver.cpp:112] Iteration 15300, lr = 0.01
I0522 23:51:41.130385 34682 solver.cpp:239] Iteration 15310 (1.80703 iter/s, 5.53395s/10 iters), loss = 8.67814
I0522 23:51:41.130431 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67814 (* 1 = 8.67814 loss)
I0522 23:51:41.198557 34682 sgd_solver.cpp:112] Iteration 15310, lr = 0.01
I0522 23:51:45.969535 34682 solver.cpp:239] Iteration 15320 (2.06658 iter/s, 4.83891s/10 iters), loss = 8.2783
I0522 23:51:45.969858 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2783 (* 1 = 8.2783 loss)
I0522 23:51:46.036550 34682 sgd_solver.cpp:112] Iteration 15320, lr = 0.01
I0522 23:51:50.335769 34682 solver.cpp:239] Iteration 15330 (2.29056 iter/s, 4.36575s/10 iters), loss = 8.37783
I0522 23:51:50.335839 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37783 (* 1 = 8.37783 loss)
I0522 23:51:50.409096 34682 sgd_solver.cpp:112] Iteration 15330, lr = 0.01
I0522 23:51:56.475947 34682 solver.cpp:239] Iteration 15340 (1.6287 iter/s, 6.13986s/10 iters), loss = 8.89959
I0522 23:51:56.475996 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89959 (* 1 = 8.89959 loss)
I0522 23:51:57.308883 34682 sgd_solver.cpp:112] Iteration 15340, lr = 0.01
I0522 23:52:00.585532 34682 solver.cpp:239] Iteration 15350 (2.43348 iter/s, 4.10934s/10 iters), loss = 8.34579
I0522 23:52:00.585603 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34579 (* 1 = 8.34579 loss)
I0522 23:52:01.400738 34682 sgd_solver.cpp:112] Iteration 15350, lr = 0.01
I0522 23:52:05.394189 34682 solver.cpp:239] Iteration 15360 (2.07969 iter/s, 4.8084s/10 iters), loss = 8.88089
I0522 23:52:05.394233 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88089 (* 1 = 8.88089 loss)
I0522 23:52:05.442992 34682 sgd_solver.cpp:112] Iteration 15360, lr = 0.01
I0522 23:52:09.963496 34682 solver.cpp:239] Iteration 15370 (2.18863 iter/s, 4.56908s/10 iters), loss = 9.7014
I0522 23:52:09.963541 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.7014 (* 1 = 9.7014 loss)
I0522 23:52:10.040632 34682 sgd_solver.cpp:112] Iteration 15370, lr = 0.01
I0522 23:52:14.109174 34682 solver.cpp:239] Iteration 15380 (2.41228 iter/s, 4.14545s/10 iters), loss = 8.95736
I0522 23:52:14.109243 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95736 (* 1 = 8.95736 loss)
I0522 23:52:14.176959 34682 sgd_solver.cpp:112] Iteration 15380, lr = 0.01
I0522 23:52:19.047310 34682 solver.cpp:239] Iteration 15390 (2.02517 iter/s, 4.93786s/10 iters), loss = 9.04971
I0522 23:52:19.047490 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04971 (* 1 = 9.04971 loss)
I0522 23:52:19.923285 34682 sgd_solver.cpp:112] Iteration 15390, lr = 0.01
I0522 23:52:26.922083 34682 solver.cpp:239] Iteration 15400 (1.26996 iter/s, 7.87428s/10 iters), loss = 9.04667
I0522 23:52:26.922134 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04667 (* 1 = 9.04667 loss)
I0522 23:52:27.000689 34682 sgd_solver.cpp:112] Iteration 15400, lr = 0.01
I0522 23:52:31.223229 34682 solver.cpp:239] Iteration 15410 (2.32509 iter/s, 4.30091s/10 iters), loss = 8.5327
I0522 23:52:31.223289 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5327 (* 1 = 8.5327 loss)
I0522 23:52:31.296196 34682 sgd_solver.cpp:112] Iteration 15410, lr = 0.01
I0522 23:52:36.131093 34682 solver.cpp:239] Iteration 15420 (2.03766 iter/s, 4.9076s/10 iters), loss = 8.34977
I0522 23:52:36.131146 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34977 (* 1 = 8.34977 loss)
I0522 23:52:36.902808 34682 sgd_solver.cpp:112] Iteration 15420, lr = 0.01
I0522 23:52:43.109647 34682 solver.cpp:239] Iteration 15430 (1.43303 iter/s, 6.97821s/10 iters), loss = 8.41626
I0522 23:52:43.109727 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41626 (* 1 = 8.41626 loss)
I0522 23:52:43.906332 34682 sgd_solver.cpp:112] Iteration 15430, lr = 0.01
I0522 23:52:49.607869 34682 solver.cpp:239] Iteration 15440 (1.53896 iter/s, 6.49788s/10 iters), loss = 8.89998
I0522 23:52:49.608064 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89998 (* 1 = 8.89998 loss)
I0522 23:52:49.682907 34682 sgd_solver.cpp:112] Iteration 15440, lr = 0.01
I0522 23:52:54.317768 34682 solver.cpp:239] Iteration 15450 (2.12337 iter/s, 4.7095s/10 iters), loss = 8.91875
I0522 23:52:54.317822 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91875 (* 1 = 8.91875 loss)
I0522 23:52:54.378599 34682 sgd_solver.cpp:112] Iteration 15450, lr = 0.01
I0522 23:52:59.756479 34682 solver.cpp:239] Iteration 15460 (1.83877 iter/s, 5.43843s/10 iters), loss = 9.25216
I0522 23:52:59.756553 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25216 (* 1 = 9.25216 loss)
I0522 23:53:00.495770 34682 sgd_solver.cpp:112] Iteration 15460, lr = 0.01
I0522 23:53:05.338644 34682 solver.cpp:239] Iteration 15470 (1.79152 iter/s, 5.58185s/10 iters), loss = 9.4398
I0522 23:53:05.338714 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4398 (* 1 = 9.4398 loss)
I0522 23:53:05.403097 34682 sgd_solver.cpp:112] Iteration 15470, lr = 0.01
I0522 23:53:08.858026 34682 solver.cpp:239] Iteration 15480 (2.84158 iter/s, 3.51917s/10 iters), loss = 9.24341
I0522 23:53:08.858070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24341 (* 1 = 9.24341 loss)
I0522 23:53:08.915130 34682 sgd_solver.cpp:112] Iteration 15480, lr = 0.01
I0522 23:53:14.076874 34682 solver.cpp:239] Iteration 15490 (1.91623 iter/s, 5.21859s/10 iters), loss = 8.79392
I0522 23:53:14.076930 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79392 (* 1 = 8.79392 loss)
I0522 23:53:14.813073 34682 sgd_solver.cpp:112] Iteration 15490, lr = 0.01
I0522 23:53:18.926775 34682 solver.cpp:239] Iteration 15500 (2.06202 iter/s, 4.84962s/10 iters), loss = 9.04662
I0522 23:53:18.926834 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04662 (* 1 = 9.04662 loss)
I0522 23:53:18.995851 34682 sgd_solver.cpp:112] Iteration 15500, lr = 0.01
I0522 23:53:25.667588 34682 solver.cpp:239] Iteration 15510 (1.48439 iter/s, 6.73679s/10 iters), loss = 8.31042
I0522 23:53:25.667815 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31042 (* 1 = 8.31042 loss)
I0522 23:53:25.728123 34682 sgd_solver.cpp:112] Iteration 15510, lr = 0.01
I0522 23:53:30.665980 34682 solver.cpp:239] Iteration 15520 (2.00081 iter/s, 4.99798s/10 iters), loss = 9.77948
I0522 23:53:30.666031 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.77948 (* 1 = 9.77948 loss)
I0522 23:53:30.742031 34682 sgd_solver.cpp:112] Iteration 15520, lr = 0.01
I0522 23:53:35.683465 34682 solver.cpp:239] Iteration 15530 (1.99313 iter/s, 5.01723s/10 iters), loss = 9.19929
I0522 23:53:35.683513 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19929 (* 1 = 9.19929 loss)
I0522 23:53:36.451895 34682 sgd_solver.cpp:112] Iteration 15530, lr = 0.01
I0522 23:53:41.052415 34682 solver.cpp:239] Iteration 15540 (1.86265 iter/s, 5.36869s/10 iters), loss = 9.07308
I0522 23:53:41.052459 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07308 (* 1 = 9.07308 loss)
I0522 23:53:41.899132 34682 sgd_solver.cpp:112] Iteration 15540, lr = 0.01
I0522 23:53:45.988613 34682 solver.cpp:239] Iteration 15550 (2.02595 iter/s, 4.93595s/10 iters), loss = 8.74563
I0522 23:53:45.988654 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74563 (* 1 = 8.74563 loss)
I0522 23:53:46.054077 34682 sgd_solver.cpp:112] Iteration 15550, lr = 0.01
I0522 23:53:51.403837 34682 solver.cpp:239] Iteration 15560 (1.84674 iter/s, 5.41494s/10 iters), loss = 8.34788
I0522 23:53:51.403923 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34788 (* 1 = 8.34788 loss)
I0522 23:53:52.234452 34682 sgd_solver.cpp:112] Iteration 15560, lr = 0.01
I0522 23:53:56.402361 34682 solver.cpp:239] Iteration 15570 (2.0007 iter/s, 4.99824s/10 iters), loss = 9.1226
I0522 23:53:56.402642 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1226 (* 1 = 9.1226 loss)
I0522 23:53:56.477646 34682 sgd_solver.cpp:112] Iteration 15570, lr = 0.01
I0522 23:54:01.129300 34682 solver.cpp:239] Iteration 15580 (2.11573 iter/s, 4.7265s/10 iters), loss = 8.98515
I0522 23:54:01.129343 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98515 (* 1 = 8.98515 loss)
I0522 23:54:01.591617 34682 sgd_solver.cpp:112] Iteration 15580, lr = 0.01
I0522 23:54:07.486335 34682 solver.cpp:239] Iteration 15590 (1.57314 iter/s, 6.35673s/10 iters), loss = 9.01178
I0522 23:54:07.486389 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01178 (* 1 = 9.01178 loss)
I0522 23:54:07.558856 34682 sgd_solver.cpp:112] Iteration 15590, lr = 0.01
I0522 23:54:13.179493 34682 solver.cpp:239] Iteration 15600 (1.75658 iter/s, 5.69287s/10 iters), loss = 8.88096
I0522 23:54:13.179620 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88096 (* 1 = 8.88096 loss)
I0522 23:54:13.239403 34682 sgd_solver.cpp:112] Iteration 15600, lr = 0.01
I0522 23:54:19.631758 34682 solver.cpp:239] Iteration 15610 (1.54994 iter/s, 6.45187s/10 iters), loss = 9.19263
I0522 23:54:19.631809 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19263 (* 1 = 9.19263 loss)
I0522 23:54:20.370836 34682 sgd_solver.cpp:112] Iteration 15610, lr = 0.01
I0522 23:54:24.967628 34682 solver.cpp:239] Iteration 15620 (1.8742 iter/s, 5.3356s/10 iters), loss = 8.51276
I0522 23:54:24.967682 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51276 (* 1 = 8.51276 loss)
I0522 23:54:25.798734 34682 sgd_solver.cpp:112] Iteration 15620, lr = 0.01
I0522 23:54:30.830631 34682 solver.cpp:239] Iteration 15630 (1.7057 iter/s, 5.8627s/10 iters), loss = 8.72947
I0522 23:54:30.830899 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72947 (* 1 = 8.72947 loss)
I0522 23:54:30.895925 34682 sgd_solver.cpp:112] Iteration 15630, lr = 0.01
I0522 23:54:35.300107 34682 solver.cpp:239] Iteration 15640 (2.23762 iter/s, 4.46904s/10 iters), loss = 8.2822
I0522 23:54:35.300159 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2822 (* 1 = 8.2822 loss)
I0522 23:54:35.369843 34682 sgd_solver.cpp:112] Iteration 15640, lr = 0.01
I0522 23:54:40.269942 34682 solver.cpp:239] Iteration 15650 (2.01225 iter/s, 4.96956s/10 iters), loss = 8.60256
I0522 23:54:40.270025 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60256 (* 1 = 8.60256 loss)
I0522 23:54:40.952397 34682 sgd_solver.cpp:112] Iteration 15650, lr = 0.01
I0522 23:54:46.435381 34682 solver.cpp:239] Iteration 15660 (1.62203 iter/s, 6.1651s/10 iters), loss = 8.51651
I0522 23:54:46.435441 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51651 (* 1 = 8.51651 loss)
I0522 23:54:47.158709 34682 sgd_solver.cpp:112] Iteration 15660, lr = 0.01
I0522 23:54:50.637456 34682 solver.cpp:239] Iteration 15670 (2.37991 iter/s, 4.20184s/10 iters), loss = 8.41026
I0522 23:54:50.637502 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41026 (* 1 = 8.41026 loss)
I0522 23:54:50.712105 34682 sgd_solver.cpp:112] Iteration 15670, lr = 0.01
I0522 23:54:55.400305 34682 solver.cpp:239] Iteration 15680 (2.09969 iter/s, 4.7626s/10 iters), loss = 8.66027
I0522 23:54:55.400367 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66027 (* 1 = 8.66027 loss)
I0522 23:54:56.149314 34682 sgd_solver.cpp:112] Iteration 15680, lr = 0.01
I0522 23:55:01.265202 34682 solver.cpp:239] Iteration 15690 (1.70515 iter/s, 5.8646s/10 iters), loss = 8.71043
I0522 23:55:01.265380 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71043 (* 1 = 8.71043 loss)
I0522 23:55:01.341572 34682 sgd_solver.cpp:112] Iteration 15690, lr = 0.01
I0522 23:55:07.733469 34682 solver.cpp:239] Iteration 15700 (1.54611 iter/s, 6.46783s/10 iters), loss = 8.78964
I0522 23:55:07.733508 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78964 (* 1 = 8.78964 loss)
I0522 23:55:07.813593 34682 sgd_solver.cpp:112] Iteration 15700, lr = 0.01
I0522 23:55:10.389649 34682 solver.cpp:239] Iteration 15710 (3.76504 iter/s, 2.65601s/10 iters), loss = 8.73267
I0522 23:55:10.389704 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73267 (* 1 = 8.73267 loss)
I0522 23:55:10.469135 34682 sgd_solver.cpp:112] Iteration 15710, lr = 0.01
I0522 23:55:15.893954 34682 solver.cpp:239] Iteration 15720 (1.81686 iter/s, 5.50401s/10 iters), loss = 8.34149
I0522 23:55:15.894028 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34149 (* 1 = 8.34149 loss)
I0522 23:55:16.749161 34682 sgd_solver.cpp:112] Iteration 15720, lr = 0.01
I0522 23:55:21.147420 34682 solver.cpp:239] Iteration 15730 (1.90361 iter/s, 5.25317s/10 iters), loss = 7.88478
I0522 23:55:21.147464 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88478 (* 1 = 7.88478 loss)
I0522 23:55:21.208360 34682 sgd_solver.cpp:112] Iteration 15730, lr = 0.01
I0522 23:55:26.684146 34682 solver.cpp:239] Iteration 15740 (1.80621 iter/s, 5.53644s/10 iters), loss = 8.20923
I0522 23:55:26.684196 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20923 (* 1 = 8.20923 loss)
I0522 23:55:26.747865 34682 sgd_solver.cpp:112] Iteration 15740, lr = 0.01
I0522 23:55:31.264902 34682 solver.cpp:239] Iteration 15750 (2.18317 iter/s, 4.5805s/10 iters), loss = 8.33144
I0522 23:55:31.264993 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33144 (* 1 = 8.33144 loss)
I0522 23:55:32.125618 34682 sgd_solver.cpp:112] Iteration 15750, lr = 0.01
I0522 23:55:34.917920 34682 solver.cpp:239] Iteration 15760 (2.73765 iter/s, 3.65277s/10 iters), loss = 8.05293
I0522 23:55:34.917981 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05293 (* 1 = 8.05293 loss)
I0522 23:55:34.984362 34682 sgd_solver.cpp:112] Iteration 15760, lr = 0.01
I0522 23:55:37.706125 34682 solver.cpp:239] Iteration 15770 (3.58676 iter/s, 2.78803s/10 iters), loss = 9.35797
I0522 23:55:37.706172 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35797 (* 1 = 9.35797 loss)
I0522 23:55:37.773753 34682 sgd_solver.cpp:112] Iteration 15770, lr = 0.01
I0522 23:55:42.058092 34682 solver.cpp:239] Iteration 15780 (2.29794 iter/s, 4.35173s/10 iters), loss = 9.11902
I0522 23:55:42.058146 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11902 (* 1 = 9.11902 loss)
I0522 23:55:42.121521 34682 sgd_solver.cpp:112] Iteration 15780, lr = 0.01
I0522 23:55:46.201625 34682 solver.cpp:239] Iteration 15790 (2.41353 iter/s, 4.14331s/10 iters), loss = 8.59465
I0522 23:55:46.201674 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59465 (* 1 = 8.59465 loss)
I0522 23:55:46.986559 34682 sgd_solver.cpp:112] Iteration 15790, lr = 0.01
I0522 23:55:52.092391 34682 solver.cpp:239] Iteration 15800 (1.69766 iter/s, 5.89045s/10 iters), loss = 8.47076
I0522 23:55:52.092494 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47076 (* 1 = 8.47076 loss)
I0522 23:55:52.210296 34682 sgd_solver.cpp:112] Iteration 15800, lr = 0.01
I0522 23:55:56.299058 34682 solver.cpp:239] Iteration 15810 (2.37733 iter/s, 4.2064s/10 iters), loss = 9.1498
I0522 23:55:56.299104 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1498 (* 1 = 9.1498 loss)
I0522 23:55:56.372663 34682 sgd_solver.cpp:112] Iteration 15810, lr = 0.01
I0522 23:56:02.003260 34682 solver.cpp:239] Iteration 15820 (1.75318 iter/s, 5.70391s/10 iters), loss = 9.13512
I0522 23:56:02.003325 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13512 (* 1 = 9.13512 loss)
I0522 23:56:02.715370 34682 sgd_solver.cpp:112] Iteration 15820, lr = 0.01
I0522 23:56:07.012147 34682 solver.cpp:239] Iteration 15830 (1.99656 iter/s, 5.00862s/10 iters), loss = 7.9643
I0522 23:56:07.012188 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9643 (* 1 = 7.9643 loss)
I0522 23:56:07.084910 34682 sgd_solver.cpp:112] Iteration 15830, lr = 0.01
I0522 23:56:11.088203 34682 solver.cpp:239] Iteration 15840 (2.45348 iter/s, 4.07585s/10 iters), loss = 8.03146
I0522 23:56:11.088253 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03146 (* 1 = 8.03146 loss)
I0522 23:56:11.163838 34682 sgd_solver.cpp:112] Iteration 15840, lr = 0.01
I0522 23:56:15.365155 34682 solver.cpp:239] Iteration 15850 (2.33824 iter/s, 4.27671s/10 iters), loss = 8.89443
I0522 23:56:15.365214 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89443 (* 1 = 8.89443 loss)
I0522 23:56:15.427026 34682 sgd_solver.cpp:112] Iteration 15850, lr = 0.01
I0522 23:56:20.593539 34682 solver.cpp:239] Iteration 15860 (1.91274 iter/s, 5.22811s/10 iters), loss = 9.08912
I0522 23:56:20.593593 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08912 (* 1 = 9.08912 loss)
I0522 23:56:21.339763 34682 sgd_solver.cpp:112] Iteration 15860, lr = 0.01
I0522 23:56:24.504429 34682 solver.cpp:239] Iteration 15870 (2.5571 iter/s, 3.91067s/10 iters), loss = 8.57703
I0522 23:56:24.504472 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57703 (* 1 = 8.57703 loss)
I0522 23:56:24.561214 34682 sgd_solver.cpp:112] Iteration 15870, lr = 0.01
I0522 23:56:30.506973 34682 solver.cpp:239] Iteration 15880 (1.66604 iter/s, 6.00224s/10 iters), loss = 8.63342
I0522 23:56:30.507038 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63342 (* 1 = 8.63342 loss)
I0522 23:56:31.293366 34682 sgd_solver.cpp:112] Iteration 15880, lr = 0.01
I0522 23:56:35.301378 34682 solver.cpp:239] Iteration 15890 (2.08588 iter/s, 4.79414s/10 iters), loss = 8.37293
I0522 23:56:35.301666 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37293 (* 1 = 8.37293 loss)
I0522 23:56:35.365556 34682 sgd_solver.cpp:112] Iteration 15890, lr = 0.01
I0522 23:56:40.164943 34682 solver.cpp:239] Iteration 15900 (2.0563 iter/s, 4.86311s/10 iters), loss = 8.78887
I0522 23:56:40.164989 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78887 (* 1 = 8.78887 loss)
I0522 23:56:40.836900 34682 sgd_solver.cpp:112] Iteration 15900, lr = 0.01
I0522 23:56:45.459173 34682 solver.cpp:239] Iteration 15910 (1.88894 iter/s, 5.29396s/10 iters), loss = 9.29072
I0522 23:56:45.459228 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29072 (* 1 = 9.29072 loss)
I0522 23:56:46.095799 34682 sgd_solver.cpp:112] Iteration 15910, lr = 0.01
I0522 23:56:50.133194 34682 solver.cpp:239] Iteration 15920 (2.1396 iter/s, 4.67377s/10 iters), loss = 8.79955
I0522 23:56:50.133241 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79955 (* 1 = 8.79955 loss)
I0522 23:56:50.213026 34682 sgd_solver.cpp:112] Iteration 15920, lr = 0.01
I0522 23:56:53.877167 34682 solver.cpp:239] Iteration 15930 (2.67111 iter/s, 3.74376s/10 iters), loss = 8.85848
I0522 23:56:53.877218 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85848 (* 1 = 8.85848 loss)
I0522 23:56:53.936033 34682 sgd_solver.cpp:112] Iteration 15930, lr = 0.01
I0522 23:56:58.138427 34682 solver.cpp:239] Iteration 15940 (2.34685 iter/s, 4.26103s/10 iters), loss = 8.42219
I0522 23:56:58.138474 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42219 (* 1 = 8.42219 loss)
I0522 23:56:58.200325 34682 sgd_solver.cpp:112] Iteration 15940, lr = 0.01
I0522 23:57:02.773233 34682 solver.cpp:239] Iteration 15950 (2.1577 iter/s, 4.63456s/10 iters), loss = 8.24869
I0522 23:57:02.773283 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24869 (* 1 = 8.24869 loss)
I0522 23:57:02.842394 34682 sgd_solver.cpp:112] Iteration 15950, lr = 0.01
I0522 23:57:06.073550 34682 solver.cpp:239] Iteration 15960 (3.03018 iter/s, 3.30013s/10 iters), loss = 9.22276
I0522 23:57:06.073807 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22276 (* 1 = 9.22276 loss)
I0522 23:57:06.873888 34682 sgd_solver.cpp:112] Iteration 15960, lr = 0.01
I0522 23:57:11.585992 34682 solver.cpp:239] Iteration 15970 (1.81423 iter/s, 5.51199s/10 iters), loss = 8.43975
I0522 23:57:11.586036 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43975 (* 1 = 8.43975 loss)
I0522 23:57:11.657318 34682 sgd_solver.cpp:112] Iteration 15970, lr = 0.01
I0522 23:57:16.712894 34682 solver.cpp:239] Iteration 15980 (1.95059 iter/s, 5.12664s/10 iters), loss = 9.24931
I0522 23:57:16.712940 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24931 (* 1 = 9.24931 loss)
I0522 23:57:16.775446 34682 sgd_solver.cpp:112] Iteration 15980, lr = 0.01
I0522 23:57:21.610420 34682 solver.cpp:239] Iteration 15990 (2.04195 iter/s, 4.89727s/10 iters), loss = 8.78087
I0522 23:57:21.610472 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78087 (* 1 = 8.78087 loss)
I0522 23:57:22.440052 34682 sgd_solver.cpp:112] Iteration 15990, lr = 0.01
I0522 23:57:26.504302 34682 solver.cpp:239] Iteration 16000 (2.04348 iter/s, 4.89362s/10 iters), loss = 8.43631
I0522 23:57:26.504356 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43631 (* 1 = 8.43631 loss)
I0522 23:57:27.013376 34682 sgd_solver.cpp:112] Iteration 16000, lr = 0.01
I0522 23:57:29.860471 34682 solver.cpp:239] Iteration 16010 (2.97976 iter/s, 3.35597s/10 iters), loss = 9.02749
I0522 23:57:29.860512 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02749 (* 1 = 9.02749 loss)
I0522 23:57:29.937438 34682 sgd_solver.cpp:112] Iteration 16010, lr = 0.01
I0522 23:57:32.404114 34682 solver.cpp:239] Iteration 16020 (3.93161 iter/s, 2.54349s/10 iters), loss = 8.45911
I0522 23:57:32.404160 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45911 (* 1 = 8.45911 loss)
I0522 23:57:32.445101 34682 sgd_solver.cpp:112] Iteration 16020, lr = 0.01
I0522 23:57:33.624508 34682 solver.cpp:239] Iteration 16030 (8.19485 iter/s, 1.22028s/10 iters), loss = 8.58452
I0522 23:57:33.624552 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58452 (* 1 = 8.58452 loss)
I0522 23:57:34.067229 34682 sgd_solver.cpp:112] Iteration 16030, lr = 0.01
I0522 23:57:35.473444 34682 solver.cpp:239] Iteration 16040 (5.4089 iter/s, 1.84881s/10 iters), loss = 8.0902
I0522 23:57:35.473486 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0902 (* 1 = 8.0902 loss)
I0522 23:57:35.523200 34682 sgd_solver.cpp:112] Iteration 16040, lr = 0.01
I0522 23:57:40.787318 34682 solver.cpp:239] Iteration 16050 (1.88196 iter/s, 5.3136s/10 iters), loss = 8.77439
I0522 23:57:40.787580 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77439 (* 1 = 8.77439 loss)
I0522 23:57:40.860523 34682 sgd_solver.cpp:112] Iteration 16050, lr = 0.01
I0522 23:57:44.567126 34682 solver.cpp:239] Iteration 16060 (2.64591 iter/s, 3.77942s/10 iters), loss = 9.00557
I0522 23:57:44.567189 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00557 (* 1 = 9.00557 loss)
I0522 23:57:45.435879 34682 sgd_solver.cpp:112] Iteration 16060, lr = 0.01
I0522 23:57:48.808724 34682 solver.cpp:239] Iteration 16070 (2.35773 iter/s, 4.24136s/10 iters), loss = 8.83705
I0522 23:57:48.808787 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83705 (* 1 = 8.83705 loss)
I0522 23:57:49.553946 34682 sgd_solver.cpp:112] Iteration 16070, lr = 0.01
I0522 23:57:55.217458 34682 solver.cpp:239] Iteration 16080 (1.56045 iter/s, 6.4084s/10 iters), loss = 8.69606
I0522 23:57:55.217511 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69606 (* 1 = 8.69606 loss)
I0522 23:57:55.469023 34682 sgd_solver.cpp:112] Iteration 16080, lr = 0.01
I0522 23:57:59.554792 34682 solver.cpp:239] Iteration 16090 (2.30568 iter/s, 4.33711s/10 iters), loss = 8.41279
I0522 23:57:59.554842 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41279 (* 1 = 8.41279 loss)
I0522 23:58:00.221129 34682 sgd_solver.cpp:112] Iteration 16090, lr = 0.01
I0522 23:58:04.323274 34682 solver.cpp:239] Iteration 16100 (2.09721 iter/s, 4.76823s/10 iters), loss = 8.43279
I0522 23:58:04.323334 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43279 (* 1 = 8.43279 loss)
I0522 23:58:05.165837 34682 sgd_solver.cpp:112] Iteration 16100, lr = 0.01
I0522 23:58:10.650712 34682 solver.cpp:239] Iteration 16110 (1.5805 iter/s, 6.32711s/10 iters), loss = 8.48228
I0522 23:58:10.650766 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48228 (* 1 = 8.48228 loss)
I0522 23:58:11.512600 34682 sgd_solver.cpp:112] Iteration 16110, lr = 0.01
I0522 23:58:15.331025 34682 solver.cpp:239] Iteration 16120 (2.13672 iter/s, 4.68006s/10 iters), loss = 7.94805
I0522 23:58:15.331071 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94805 (* 1 = 7.94805 loss)
I0522 23:58:15.410928 34682 sgd_solver.cpp:112] Iteration 16120, lr = 0.01
I0522 23:58:21.014495 34682 solver.cpp:239] Iteration 16130 (1.75957 iter/s, 5.68319s/10 iters), loss = 8.31563
I0522 23:58:21.014549 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31563 (* 1 = 8.31563 loss)
I0522 23:58:21.578343 34682 sgd_solver.cpp:112] Iteration 16130, lr = 0.01
I0522 23:58:25.474107 34682 solver.cpp:239] Iteration 16140 (2.24247 iter/s, 4.45937s/10 iters), loss = 8.30702
I0522 23:58:25.474148 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30702 (* 1 = 8.30702 loss)
I0522 23:58:26.234710 34682 sgd_solver.cpp:112] Iteration 16140, lr = 0.01
I0522 23:58:29.597942 34682 solver.cpp:239] Iteration 16150 (2.42506 iter/s, 4.1236s/10 iters), loss = 8.77269
I0522 23:58:29.598019 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77269 (* 1 = 8.77269 loss)
I0522 23:58:30.339108 34682 sgd_solver.cpp:112] Iteration 16150, lr = 0.01
I0522 23:58:33.151723 34682 solver.cpp:239] Iteration 16160 (2.81408 iter/s, 3.55356s/10 iters), loss = 8.95908
I0522 23:58:33.151772 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95908 (* 1 = 8.95908 loss)
I0522 23:58:33.233440 34682 sgd_solver.cpp:112] Iteration 16160, lr = 0.01
I0522 23:58:37.068037 34682 solver.cpp:239] Iteration 16170 (2.55357 iter/s, 3.91609s/10 iters), loss = 9.44794
I0522 23:58:37.068114 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44794 (* 1 = 9.44794 loss)
I0522 23:58:37.134513 34682 sgd_solver.cpp:112] Iteration 16170, lr = 0.01
I0522 23:58:42.091644 34682 solver.cpp:239] Iteration 16180 (1.99071 iter/s, 5.02333s/10 iters), loss = 8.84669
I0522 23:58:42.091778 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84669 (* 1 = 8.84669 loss)
I0522 23:58:42.169735 34682 sgd_solver.cpp:112] Iteration 16180, lr = 0.01
I0522 23:58:47.124414 34682 solver.cpp:239] Iteration 16190 (1.98711 iter/s, 5.03243s/10 iters), loss = 9.22745
I0522 23:58:47.124472 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22745 (* 1 = 9.22745 loss)
I0522 23:58:47.458412 34682 sgd_solver.cpp:112] Iteration 16190, lr = 0.01
I0522 23:58:50.422019 34682 solver.cpp:239] Iteration 16200 (3.03268 iter/s, 3.29741s/10 iters), loss = 9.4538
I0522 23:58:50.422062 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4538 (* 1 = 9.4538 loss)
I0522 23:58:50.503360 34682 sgd_solver.cpp:112] Iteration 16200, lr = 0.01
I0522 23:58:53.652097 34682 solver.cpp:239] Iteration 16210 (3.09608 iter/s, 3.22989s/10 iters), loss = 9.29068
I0522 23:58:53.652149 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29068 (* 1 = 9.29068 loss)
I0522 23:58:53.735442 34682 sgd_solver.cpp:112] Iteration 16210, lr = 0.01
I0522 23:58:57.964079 34682 solver.cpp:239] Iteration 16220 (2.31924 iter/s, 4.31175s/10 iters), loss = 8.5211
I0522 23:58:57.964136 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5211 (* 1 = 8.5211 loss)
I0522 23:58:58.720350 34682 sgd_solver.cpp:112] Iteration 16220, lr = 0.01
I0522 23:59:02.755615 34682 solver.cpp:239] Iteration 16230 (2.08712 iter/s, 4.79128s/10 iters), loss = 9.1154
I0522 23:59:02.755662 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1154 (* 1 = 9.1154 loss)
I0522 23:59:02.814488 34682 sgd_solver.cpp:112] Iteration 16230, lr = 0.01
I0522 23:59:08.317502 34682 solver.cpp:239] Iteration 16240 (1.79804 iter/s, 5.56161s/10 iters), loss = 9.32609
I0522 23:59:08.317559 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32609 (* 1 = 9.32609 loss)
I0522 23:59:09.061837 34682 sgd_solver.cpp:112] Iteration 16240, lr = 0.01
I0522 23:59:11.503898 34682 solver.cpp:239] Iteration 16250 (3.13854 iter/s, 3.1862s/10 iters), loss = 8.45077
I0522 23:59:11.503952 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45077 (* 1 = 8.45077 loss)
I0522 23:59:12.138792 34682 sgd_solver.cpp:112] Iteration 16250, lr = 0.01
I0522 23:59:17.760254 34682 solver.cpp:239] Iteration 16260 (1.59845 iter/s, 6.25605s/10 iters), loss = 9.04802
I0522 23:59:17.760309 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04802 (* 1 = 9.04802 loss)
I0522 23:59:17.817339 34682 sgd_solver.cpp:112] Iteration 16260, lr = 0.01
I0522 23:59:23.639590 34682 solver.cpp:239] Iteration 16270 (1.70096 iter/s, 5.87904s/10 iters), loss = 8.43856
I0522 23:59:23.639637 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43856 (* 1 = 8.43856 loss)
I0522 23:59:23.699000 34682 sgd_solver.cpp:112] Iteration 16270, lr = 0.01
I0522 23:59:28.463877 34682 solver.cpp:239] Iteration 16280 (2.07295 iter/s, 4.82404s/10 iters), loss = 7.93645
I0522 23:59:28.463944 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93645 (* 1 = 7.93645 loss)
I0522 23:59:28.544629 34682 sgd_solver.cpp:112] Iteration 16280, lr = 0.01
I0522 23:59:34.060395 34682 solver.cpp:239] Iteration 16290 (1.78832 iter/s, 5.59183s/10 iters), loss = 8.45654
I0522 23:59:34.060456 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45654 (* 1 = 8.45654 loss)
I0522 23:59:34.848146 34682 sgd_solver.cpp:112] Iteration 16290, lr = 0.01
I0522 23:59:39.916265 34682 solver.cpp:239] Iteration 16300 (1.70777 iter/s, 5.85557s/10 iters), loss = 7.77359
I0522 23:59:39.916319 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77359 (* 1 = 7.77359 loss)
I0522 23:59:40.652631 34682 sgd_solver.cpp:112] Iteration 16300, lr = 0.01
I0522 23:59:46.426596 34682 solver.cpp:239] Iteration 16310 (1.53609 iter/s, 6.51001s/10 iters), loss = 8.69167
I0522 23:59:46.426760 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69167 (* 1 = 8.69167 loss)
I0522 23:59:47.293345 34682 sgd_solver.cpp:112] Iteration 16310, lr = 0.01
I0522 23:59:52.937165 34682 solver.cpp:239] Iteration 16320 (1.53606 iter/s, 6.51014s/10 iters), loss = 8.59809
I0522 23:59:52.937217 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59809 (* 1 = 8.59809 loss)
I0522 23:59:53.573693 34682 sgd_solver.cpp:112] Iteration 16320, lr = 0.01
I0522 23:59:58.442440 34682 solver.cpp:239] Iteration 16330 (1.81653 iter/s, 5.505s/10 iters), loss = 8.56002
I0522 23:59:58.442489 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56002 (* 1 = 8.56002 loss)
I0522 23:59:58.502897 34682 sgd_solver.cpp:112] Iteration 16330, lr = 0.01
I0523 00:00:03.896562 34682 solver.cpp:239] Iteration 16340 (1.83357 iter/s, 5.45385s/10 iters), loss = 8.27914
I0523 00:00:03.896631 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27914 (* 1 = 8.27914 loss)
I0523 00:00:04.687383 34682 sgd_solver.cpp:112] Iteration 16340, lr = 0.01
I0523 00:00:08.083364 34682 solver.cpp:239] Iteration 16350 (2.3886 iter/s, 4.18656s/10 iters), loss = 8.78733
I0523 00:00:08.083410 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78733 (* 1 = 8.78733 loss)
I0523 00:00:08.161664 34682 sgd_solver.cpp:112] Iteration 16350, lr = 0.01
I0523 00:00:12.183025 34682 solver.cpp:239] Iteration 16360 (2.43936 iter/s, 4.09944s/10 iters), loss = 8.92701
I0523 00:00:12.183079 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92701 (* 1 = 8.92701 loss)
I0523 00:00:12.883738 34682 sgd_solver.cpp:112] Iteration 16360, lr = 0.01
I0523 00:00:16.194016 34682 solver.cpp:239] Iteration 16370 (2.49329 iter/s, 4.01077s/10 iters), loss = 8.52275
I0523 00:00:16.194080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52275 (* 1 = 8.52275 loss)
I0523 00:00:17.022639 34682 sgd_solver.cpp:112] Iteration 16370, lr = 0.01
I0523 00:00:20.420728 34682 solver.cpp:239] Iteration 16380 (2.36604 iter/s, 4.22648s/10 iters), loss = 7.77009
I0523 00:00:20.420770 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77009 (* 1 = 7.77009 loss)
I0523 00:00:21.245054 34682 sgd_solver.cpp:112] Iteration 16380, lr = 0.01
I0523 00:00:26.054283 34682 solver.cpp:239] Iteration 16390 (1.77517 iter/s, 5.63328s/10 iters), loss = 7.87597
I0523 00:00:26.054339 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87597 (* 1 = 7.87597 loss)
I0523 00:00:26.583729 34682 sgd_solver.cpp:112] Iteration 16390, lr = 0.01
I0523 00:00:31.231175 34682 solver.cpp:239] Iteration 16400 (1.93176 iter/s, 5.17662s/10 iters), loss = 8.878
I0523 00:00:31.231225 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.878 (* 1 = 8.878 loss)
I0523 00:00:31.300310 34682 sgd_solver.cpp:112] Iteration 16400, lr = 0.01
I0523 00:00:38.735234 34682 solver.cpp:239] Iteration 16410 (1.33268 iter/s, 7.5037s/10 iters), loss = 8.18927
I0523 00:00:38.735289 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18927 (* 1 = 8.18927 loss)
I0523 00:00:39.448782 34682 sgd_solver.cpp:112] Iteration 16410, lr = 0.01
I0523 00:00:44.284942 34682 solver.cpp:239] Iteration 16420 (1.80199 iter/s, 5.54943s/10 iters), loss = 9.32817
I0523 00:00:44.284994 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32817 (* 1 = 9.32817 loss)
I0523 00:00:44.356297 34682 sgd_solver.cpp:112] Iteration 16420, lr = 0.01
I0523 00:00:49.190284 34682 solver.cpp:239] Iteration 16430 (2.0387 iter/s, 4.90508s/10 iters), loss = 8.49006
I0523 00:00:49.190556 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49006 (* 1 = 8.49006 loss)
I0523 00:00:50.018422 34682 sgd_solver.cpp:112] Iteration 16430, lr = 0.01
I0523 00:00:54.857302 34682 solver.cpp:239] Iteration 16440 (1.76474 iter/s, 5.66656s/10 iters), loss = 8.77899
I0523 00:00:54.857354 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77899 (* 1 = 8.77899 loss)
I0523 00:00:54.920409 34682 sgd_solver.cpp:112] Iteration 16440, lr = 0.01
I0523 00:00:59.635071 34682 solver.cpp:239] Iteration 16450 (2.09313 iter/s, 4.77752s/10 iters), loss = 8.34494
I0523 00:00:59.635121 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34494 (* 1 = 8.34494 loss)
I0523 00:00:59.692350 34682 sgd_solver.cpp:112] Iteration 16450, lr = 0.01
I0523 00:01:02.291399 34682 solver.cpp:239] Iteration 16460 (3.76484 iter/s, 2.65616s/10 iters), loss = 8.62621
I0523 00:01:02.291445 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62621 (* 1 = 8.62621 loss)
I0523 00:01:03.130865 34682 sgd_solver.cpp:112] Iteration 16460, lr = 0.01
I0523 00:01:06.852596 34682 solver.cpp:239] Iteration 16470 (2.19252 iter/s, 4.56097s/10 iters), loss = 8.55207
I0523 00:01:06.852649 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55207 (* 1 = 8.55207 loss)
I0523 00:01:06.911226 34682 sgd_solver.cpp:112] Iteration 16470, lr = 0.01
I0523 00:01:14.889358 34682 solver.cpp:239] Iteration 16480 (1.24434 iter/s, 8.03639s/10 iters), loss = 8.51824
I0523 00:01:14.889407 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51824 (* 1 = 8.51824 loss)
I0523 00:01:14.963562 34682 sgd_solver.cpp:112] Iteration 16480, lr = 0.01
I0523 00:01:20.142802 34682 solver.cpp:239] Iteration 16490 (1.90361 iter/s, 5.25318s/10 iters), loss = 8.69689
I0523 00:01:20.142976 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69689 (* 1 = 8.69689 loss)
I0523 00:01:20.955788 34682 sgd_solver.cpp:112] Iteration 16490, lr = 0.01
I0523 00:01:24.582008 34682 solver.cpp:239] Iteration 16500 (2.25284 iter/s, 4.43885s/10 iters), loss = 8.24445
I0523 00:01:24.582070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24445 (* 1 = 8.24445 loss)
I0523 00:01:25.187291 34682 sgd_solver.cpp:112] Iteration 16500, lr = 0.01
I0523 00:01:29.456820 34682 solver.cpp:239] Iteration 16510 (2.05147 iter/s, 4.87456s/10 iters), loss = 9.33816
I0523 00:01:29.456863 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33816 (* 1 = 9.33816 loss)
I0523 00:01:30.344265 34682 sgd_solver.cpp:112] Iteration 16510, lr = 0.01
I0523 00:01:36.441800 34682 solver.cpp:239] Iteration 16520 (1.43171 iter/s, 6.98464s/10 iters), loss = 9.32959
I0523 00:01:36.441880 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32959 (* 1 = 9.32959 loss)
I0523 00:01:37.340313 34682 sgd_solver.cpp:112] Iteration 16520, lr = 0.01
I0523 00:01:42.872828 34682 solver.cpp:239] Iteration 16530 (1.55504 iter/s, 6.4307s/10 iters), loss = 8.51984
I0523 00:01:42.872879 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51984 (* 1 = 8.51984 loss)
I0523 00:01:42.948709 34682 sgd_solver.cpp:112] Iteration 16530, lr = 0.01
I0523 00:01:46.577445 34682 solver.cpp:239] Iteration 16540 (2.69949 iter/s, 3.70441s/10 iters), loss = 7.90333
I0523 00:01:46.577491 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90333 (* 1 = 7.90333 loss)
I0523 00:01:47.409797 34682 sgd_solver.cpp:112] Iteration 16540, lr = 0.01
I0523 00:01:51.626529 34682 solver.cpp:239] Iteration 16550 (1.98066 iter/s, 5.04883s/10 iters), loss = 9.08967
I0523 00:01:51.626821 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08967 (* 1 = 9.08967 loss)
I0523 00:01:51.698962 34682 sgd_solver.cpp:112] Iteration 16550, lr = 0.01
I0523 00:01:56.721973 34682 solver.cpp:239] Iteration 16560 (1.96437 iter/s, 5.09068s/10 iters), loss = 8.40391
I0523 00:01:56.722040 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40391 (* 1 = 8.40391 loss)
I0523 00:01:57.549836 34682 sgd_solver.cpp:112] Iteration 16560, lr = 0.01
I0523 00:02:03.699597 34682 solver.cpp:239] Iteration 16570 (1.43322 iter/s, 6.97728s/10 iters), loss = 8.75276
I0523 00:02:03.699652 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75276 (* 1 = 8.75276 loss)
I0523 00:02:03.771347 34682 sgd_solver.cpp:112] Iteration 16570, lr = 0.01
I0523 00:02:06.977658 34682 solver.cpp:239] Iteration 16580 (3.05077 iter/s, 3.27787s/10 iters), loss = 9.23799
I0523 00:02:06.977713 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23799 (* 1 = 9.23799 loss)
I0523 00:02:07.049728 34682 sgd_solver.cpp:112] Iteration 16580, lr = 0.01
I0523 00:02:12.557719 34682 solver.cpp:239] Iteration 16590 (1.79219 iter/s, 5.57977s/10 iters), loss = 8.98066
I0523 00:02:12.557785 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98066 (* 1 = 8.98066 loss)
I0523 00:02:13.277097 34682 sgd_solver.cpp:112] Iteration 16590, lr = 0.01
I0523 00:02:18.349838 34682 solver.cpp:239] Iteration 16600 (1.72658 iter/s, 5.79181s/10 iters), loss = 9.27802
I0523 00:02:18.349902 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27802 (* 1 = 9.27802 loss)
I0523 00:02:19.137807 34682 sgd_solver.cpp:112] Iteration 16600, lr = 0.01
I0523 00:02:23.846771 34682 solver.cpp:239] Iteration 16610 (1.81929 iter/s, 5.49665s/10 iters), loss = 8.34343
I0523 00:02:23.846963 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34343 (* 1 = 8.34343 loss)
I0523 00:02:23.924587 34682 sgd_solver.cpp:112] Iteration 16610, lr = 0.01
I0523 00:02:26.594048 34682 solver.cpp:239] Iteration 16620 (3.64034 iter/s, 2.74699s/10 iters), loss = 9.2062
I0523 00:02:26.594102 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2062 (* 1 = 9.2062 loss)
I0523 00:02:27.451118 34682 sgd_solver.cpp:112] Iteration 16620, lr = 0.01
I0523 00:02:33.184201 34682 solver.cpp:239] Iteration 16630 (1.51749 iter/s, 6.58983s/10 iters), loss = 8.93925
I0523 00:02:33.184252 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93925 (* 1 = 8.93925 loss)
I0523 00:02:33.243615 34682 sgd_solver.cpp:112] Iteration 16630, lr = 0.01
I0523 00:02:35.778576 34682 solver.cpp:239] Iteration 16640 (3.85473 iter/s, 2.59422s/10 iters), loss = 9.18459
I0523 00:02:35.778625 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18459 (* 1 = 9.18459 loss)
I0523 00:02:35.841239 34682 sgd_solver.cpp:112] Iteration 16640, lr = 0.01
I0523 00:02:39.312248 34682 solver.cpp:239] Iteration 16650 (2.83008 iter/s, 3.53347s/10 iters), loss = 9.06224
I0523 00:02:39.312299 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06224 (* 1 = 9.06224 loss)
I0523 00:02:39.373986 34682 sgd_solver.cpp:112] Iteration 16650, lr = 0.01
I0523 00:02:42.823189 34682 solver.cpp:239] Iteration 16660 (2.8484 iter/s, 3.51074s/10 iters), loss = 8.78642
I0523 00:02:42.823231 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78642 (* 1 = 8.78642 loss)
I0523 00:02:42.894140 34682 sgd_solver.cpp:112] Iteration 16660, lr = 0.01
I0523 00:02:50.566411 34682 solver.cpp:239] Iteration 16670 (1.29151 iter/s, 7.74287s/10 iters), loss = 8.92313
I0523 00:02:50.566471 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92313 (* 1 = 8.92313 loss)
I0523 00:02:51.394589 34682 sgd_solver.cpp:112] Iteration 16670, lr = 0.01
I0523 00:02:56.285974 34682 solver.cpp:239] Iteration 16680 (1.74848 iter/s, 5.71925s/10 iters), loss = 8.67126
I0523 00:02:56.286177 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67126 (* 1 = 8.67126 loss)
I0523 00:02:57.152082 34682 sgd_solver.cpp:112] Iteration 16680, lr = 0.01
I0523 00:03:00.338387 34682 solver.cpp:239] Iteration 16690 (2.46789 iter/s, 4.05204s/10 iters), loss = 8.95904
I0523 00:03:00.338438 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95904 (* 1 = 8.95904 loss)
I0523 00:03:00.398843 34682 sgd_solver.cpp:112] Iteration 16690, lr = 0.01
I0523 00:03:03.569383 34682 solver.cpp:239] Iteration 16700 (3.0952 iter/s, 3.23081s/10 iters), loss = 8.22007
I0523 00:03:03.569437 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22007 (* 1 = 8.22007 loss)
I0523 00:03:03.636137 34682 sgd_solver.cpp:112] Iteration 16700, lr = 0.01
I0523 00:03:07.742796 34682 solver.cpp:239] Iteration 16710 (2.39625 iter/s, 4.17319s/10 iters), loss = 9.14813
I0523 00:03:07.742852 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14813 (* 1 = 9.14813 loss)
I0523 00:03:08.540388 34682 sgd_solver.cpp:112] Iteration 16710, lr = 0.01
I0523 00:03:11.762543 34682 solver.cpp:239] Iteration 16720 (2.48786 iter/s, 4.01952s/10 iters), loss = 8.05299
I0523 00:03:11.762595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05299 (* 1 = 8.05299 loss)
I0523 00:03:11.833498 34682 sgd_solver.cpp:112] Iteration 16720, lr = 0.01
I0523 00:03:15.168282 34682 solver.cpp:239] Iteration 16730 (2.93639 iter/s, 3.40554s/10 iters), loss = 9.16749
I0523 00:03:15.168346 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16749 (* 1 = 9.16749 loss)
I0523 00:03:16.029960 34682 sgd_solver.cpp:112] Iteration 16730, lr = 0.01
I0523 00:03:20.950359 34682 solver.cpp:239] Iteration 16740 (1.72957 iter/s, 5.78177s/10 iters), loss = 9.24084
I0523 00:03:20.950407 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24084 (* 1 = 9.24084 loss)
I0523 00:03:21.035131 34682 sgd_solver.cpp:112] Iteration 16740, lr = 0.01
I0523 00:03:27.127130 34682 solver.cpp:239] Iteration 16750 (1.61905 iter/s, 6.17647s/10 iters), loss = 8.91072
I0523 00:03:27.127369 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91072 (* 1 = 8.91072 loss)
I0523 00:03:27.204222 34682 sgd_solver.cpp:112] Iteration 16750, lr = 0.01
I0523 00:03:31.334450 34682 solver.cpp:239] Iteration 16760 (2.37703 iter/s, 4.20693s/10 iters), loss = 8.60878
I0523 00:03:31.334501 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60878 (* 1 = 8.60878 loss)
I0523 00:03:31.416883 34682 sgd_solver.cpp:112] Iteration 16760, lr = 0.01
I0523 00:03:36.135090 34682 solver.cpp:239] Iteration 16770 (2.08316 iter/s, 4.80039s/10 iters), loss = 8.52724
I0523 00:03:36.135149 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52724 (* 1 = 8.52724 loss)
I0523 00:03:36.854218 34682 sgd_solver.cpp:112] Iteration 16770, lr = 0.01
I0523 00:03:39.520817 34682 solver.cpp:239] Iteration 16780 (2.95376 iter/s, 3.38552s/10 iters), loss = 8.95274
I0523 00:03:39.520861 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95274 (* 1 = 8.95274 loss)
I0523 00:03:39.604109 34682 sgd_solver.cpp:112] Iteration 16780, lr = 0.01
I0523 00:03:43.207746 34682 solver.cpp:239] Iteration 16790 (2.71243 iter/s, 3.68673s/10 iters), loss = 8.12058
I0523 00:03:43.207795 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12058 (* 1 = 8.12058 loss)
I0523 00:03:43.995756 34682 sgd_solver.cpp:112] Iteration 16790, lr = 0.01
I0523 00:03:49.772414 34682 solver.cpp:239] Iteration 16800 (1.52338 iter/s, 6.56435s/10 iters), loss = 8.41634
I0523 00:03:49.772475 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41634 (* 1 = 8.41634 loss)
I0523 00:03:50.452875 34682 sgd_solver.cpp:112] Iteration 16800, lr = 0.01
I0523 00:03:54.798306 34682 solver.cpp:239] Iteration 16810 (1.98981 iter/s, 5.02561s/10 iters), loss = 8.55681
I0523 00:03:54.798363 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55681 (* 1 = 8.55681 loss)
I0523 00:03:55.641944 34682 sgd_solver.cpp:112] Iteration 16810, lr = 0.01
I0523 00:03:59.916142 34682 solver.cpp:239] Iteration 16820 (1.95406 iter/s, 5.11754s/10 iters), loss = 8.86703
I0523 00:03:59.916389 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86703 (* 1 = 8.86703 loss)
I0523 00:03:59.985378 34682 sgd_solver.cpp:112] Iteration 16820, lr = 0.01
I0523 00:04:03.083158 34682 solver.cpp:239] Iteration 16830 (3.15791 iter/s, 3.16666s/10 iters), loss = 8.52011
I0523 00:04:03.083214 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52011 (* 1 = 8.52011 loss)
I0523 00:04:03.655521 34682 sgd_solver.cpp:112] Iteration 16830, lr = 0.01
I0523 00:04:08.418881 34682 solver.cpp:239] Iteration 16840 (1.87426 iter/s, 5.33545s/10 iters), loss = 9.33167
I0523 00:04:08.418936 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33167 (* 1 = 9.33167 loss)
I0523 00:04:09.259006 34682 sgd_solver.cpp:112] Iteration 16840, lr = 0.01
I0523 00:04:14.832644 34682 solver.cpp:239] Iteration 16850 (1.55922 iter/s, 6.41345s/10 iters), loss = 9.06948
I0523 00:04:14.832695 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06948 (* 1 = 9.06948 loss)
I0523 00:04:14.922991 34682 sgd_solver.cpp:112] Iteration 16850, lr = 0.01
I0523 00:04:18.808313 34682 solver.cpp:239] Iteration 16860 (2.51544 iter/s, 3.97545s/10 iters), loss = 8.88885
I0523 00:04:18.808363 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88885 (* 1 = 8.88885 loss)
I0523 00:04:18.890396 34682 sgd_solver.cpp:112] Iteration 16860, lr = 0.01
I0523 00:04:23.349731 34682 solver.cpp:239] Iteration 16870 (2.20207 iter/s, 4.54118s/10 iters), loss = 8.44621
I0523 00:04:23.349772 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44621 (* 1 = 8.44621 loss)
I0523 00:04:23.411908 34682 sgd_solver.cpp:112] Iteration 16870, lr = 0.01
I0523 00:04:27.125162 34682 solver.cpp:239] Iteration 16880 (2.64885 iter/s, 3.77522s/10 iters), loss = 8.11722
I0523 00:04:27.125231 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11722 (* 1 = 8.11722 loss)
I0523 00:04:27.202296 34682 sgd_solver.cpp:112] Iteration 16880, lr = 0.01
I0523 00:04:34.077826 34682 solver.cpp:239] Iteration 16890 (1.43837 iter/s, 6.95232s/10 iters), loss = 8.97981
I0523 00:04:34.078033 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97981 (* 1 = 8.97981 loss)
I0523 00:04:34.158519 34682 sgd_solver.cpp:112] Iteration 16890, lr = 0.01
I0523 00:04:38.019409 34682 solver.cpp:239] Iteration 16900 (2.53727 iter/s, 3.94124s/10 iters), loss = 8.40766
I0523 00:04:38.019459 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40766 (* 1 = 8.40766 loss)
I0523 00:04:38.082700 34682 sgd_solver.cpp:112] Iteration 16900, lr = 0.01
I0523 00:04:43.097010 34682 solver.cpp:239] Iteration 16910 (1.96953 iter/s, 5.07734s/10 iters), loss = 8.78747
I0523 00:04:43.097067 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78747 (* 1 = 8.78747 loss)
I0523 00:04:43.979005 34682 sgd_solver.cpp:112] Iteration 16910, lr = 0.01
I0523 00:04:47.269878 34682 solver.cpp:239] Iteration 16920 (2.39657 iter/s, 4.17264s/10 iters), loss = 9.07318
I0523 00:04:47.269933 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07318 (* 1 = 9.07318 loss)
I0523 00:04:47.495380 34682 sgd_solver.cpp:112] Iteration 16920, lr = 0.01
I0523 00:04:50.948236 34682 solver.cpp:239] Iteration 16930 (2.71876 iter/s, 3.67815s/10 iters), loss = 8.46301
I0523 00:04:50.948297 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46301 (* 1 = 8.46301 loss)
I0523 00:04:51.031661 34682 sgd_solver.cpp:112] Iteration 16930, lr = 0.01
I0523 00:04:55.553130 34682 solver.cpp:239] Iteration 16940 (2.17172 iter/s, 4.60465s/10 iters), loss = 8.19596
I0523 00:04:55.553211 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19596 (* 1 = 8.19596 loss)
I0523 00:04:55.616397 34682 sgd_solver.cpp:112] Iteration 16940, lr = 0.01
I0523 00:04:59.858961 34682 solver.cpp:239] Iteration 16950 (2.32257 iter/s, 4.30557s/10 iters), loss = 7.85839
I0523 00:04:59.859012 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.85839 (* 1 = 7.85839 loss)
I0523 00:04:59.929621 34682 sgd_solver.cpp:112] Iteration 16950, lr = 0.01
I0523 00:05:05.964948 34682 solver.cpp:239] Iteration 16960 (1.63782 iter/s, 6.10569s/10 iters), loss = 8.23511
I0523 00:05:05.965216 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23511 (* 1 = 8.23511 loss)
I0523 00:05:06.034027 34682 sgd_solver.cpp:112] Iteration 16960, lr = 0.01
I0523 00:05:11.013195 34682 solver.cpp:239] Iteration 16970 (1.98106 iter/s, 5.04779s/10 iters), loss = 8.48684
I0523 00:05:11.013247 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48684 (* 1 = 8.48684 loss)
I0523 00:05:11.629418 34682 sgd_solver.cpp:112] Iteration 16970, lr = 0.01
I0523 00:05:18.527590 34682 solver.cpp:239] Iteration 16980 (1.33084 iter/s, 7.51404s/10 iters), loss = 8.08696
I0523 00:05:18.527643 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08696 (* 1 = 8.08696 loss)
I0523 00:05:19.324795 34682 sgd_solver.cpp:112] Iteration 16980, lr = 0.01
I0523 00:05:25.564808 34682 solver.cpp:239] Iteration 16990 (1.42108 iter/s, 7.03688s/10 iters), loss = 8.14968
I0523 00:05:25.564870 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14968 (* 1 = 8.14968 loss)
I0523 00:05:26.314270 34682 sgd_solver.cpp:112] Iteration 16990, lr = 0.01
I0523 00:05:30.785861 34682 solver.cpp:239] Iteration 17000 (1.91543 iter/s, 5.22077s/10 iters), loss = 9.20496
I0523 00:05:30.785928 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20496 (* 1 = 9.20496 loss)
I0523 00:05:31.637878 34682 sgd_solver.cpp:112] Iteration 17000, lr = 0.01
I0523 00:05:35.617787 34682 solver.cpp:239] Iteration 17010 (2.06968 iter/s, 4.83166s/10 iters), loss = 8.54043
I0523 00:05:35.617832 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54043 (* 1 = 8.54043 loss)
I0523 00:05:35.685142 34682 sgd_solver.cpp:112] Iteration 17010, lr = 0.01
I0523 00:05:39.036707 34682 solver.cpp:239] Iteration 17020 (2.92507 iter/s, 3.41873s/10 iters), loss = 8.57635
I0523 00:05:39.036931 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57635 (* 1 = 8.57635 loss)
I0523 00:05:39.864557 34682 sgd_solver.cpp:112] Iteration 17020, lr = 0.01
I0523 00:05:45.571105 34682 solver.cpp:239] Iteration 17030 (1.53048 iter/s, 6.53391s/10 iters), loss = 9.2054
I0523 00:05:45.571171 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2054 (* 1 = 9.2054 loss)
I0523 00:05:45.641340 34682 sgd_solver.cpp:112] Iteration 17030, lr = 0.01
I0523 00:05:51.161890 34682 solver.cpp:239] Iteration 17040 (1.78875 iter/s, 5.59048s/10 iters), loss = 9.04431
I0523 00:05:51.161967 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04431 (* 1 = 9.04431 loss)
I0523 00:05:51.874140 34682 sgd_solver.cpp:112] Iteration 17040, lr = 0.01
I0523 00:05:55.969246 34682 solver.cpp:239] Iteration 17050 (2.08026 iter/s, 4.80708s/10 iters), loss = 8.67752
I0523 00:05:55.969297 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67752 (* 1 = 8.67752 loss)
I0523 00:05:56.027649 34682 sgd_solver.cpp:112] Iteration 17050, lr = 0.01
I0523 00:06:01.522833 34682 solver.cpp:239] Iteration 17060 (1.80073 iter/s, 5.5533s/10 iters), loss = 8.7652
I0523 00:06:01.522878 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7652 (* 1 = 8.7652 loss)
I0523 00:06:01.597836 34682 sgd_solver.cpp:112] Iteration 17060, lr = 0.01
I0523 00:06:05.916065 34682 solver.cpp:239] Iteration 17070 (2.27634 iter/s, 4.39301s/10 iters), loss = 9.10954
I0523 00:06:05.916112 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10954 (* 1 = 9.10954 loss)
I0523 00:06:05.987866 34682 sgd_solver.cpp:112] Iteration 17070, lr = 0.01
I0523 00:06:10.507966 34682 solver.cpp:239] Iteration 17080 (2.17786 iter/s, 4.59167s/10 iters), loss = 8.46986
I0523 00:06:10.508239 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46986 (* 1 = 8.46986 loss)
I0523 00:06:11.325606 34682 sgd_solver.cpp:112] Iteration 17080, lr = 0.01
I0523 00:06:15.450951 34682 solver.cpp:239] Iteration 17090 (2.02326 iter/s, 4.94252s/10 iters), loss = 8.60981
I0523 00:06:15.451015 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60981 (* 1 = 8.60981 loss)
I0523 00:06:16.311691 34682 sgd_solver.cpp:112] Iteration 17090, lr = 0.01
I0523 00:06:21.640291 34682 solver.cpp:239] Iteration 17100 (1.61576 iter/s, 6.18902s/10 iters), loss = 8.78818
I0523 00:06:21.640352 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78818 (* 1 = 8.78818 loss)
I0523 00:06:22.466233 34682 sgd_solver.cpp:112] Iteration 17100, lr = 0.01
I0523 00:06:28.396847 34682 solver.cpp:239] Iteration 17110 (1.48012 iter/s, 6.75623s/10 iters), loss = 9.46623
I0523 00:06:28.396891 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.46623 (* 1 = 9.46623 loss)
I0523 00:06:29.230556 34682 sgd_solver.cpp:112] Iteration 17110, lr = 0.01
I0523 00:06:33.449908 34682 solver.cpp:239] Iteration 17120 (1.9791 iter/s, 5.05279s/10 iters), loss = 9.09631
I0523 00:06:33.449967 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09631 (* 1 = 9.09631 loss)
I0523 00:06:33.515132 34682 sgd_solver.cpp:112] Iteration 17120, lr = 0.01
I0523 00:06:37.673239 34682 solver.cpp:239] Iteration 17130 (2.36793 iter/s, 4.22309s/10 iters), loss = 8.5447
I0523 00:06:37.673302 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5447 (* 1 = 8.5447 loss)
I0523 00:06:37.755823 34682 sgd_solver.cpp:112] Iteration 17130, lr = 0.01
I0523 00:06:42.357568 34682 solver.cpp:239] Iteration 17140 (2.1349 iter/s, 4.68407s/10 iters), loss = 8.75414
I0523 00:06:42.357683 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75414 (* 1 = 8.75414 loss)
I0523 00:06:42.437664 34682 sgd_solver.cpp:112] Iteration 17140, lr = 0.01
I0523 00:06:47.957726 34682 solver.cpp:239] Iteration 17150 (1.78577 iter/s, 5.59982s/10 iters), loss = 8.62148
I0523 00:06:47.957774 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62148 (* 1 = 8.62148 loss)
I0523 00:06:48.638377 34682 sgd_solver.cpp:112] Iteration 17150, lr = 0.01
I0523 00:06:51.327949 34682 solver.cpp:239] Iteration 17160 (2.96733 iter/s, 3.37003s/10 iters), loss = 7.77433
I0523 00:06:51.327996 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77433 (* 1 = 7.77433 loss)
I0523 00:06:51.390489 34682 sgd_solver.cpp:112] Iteration 17160, lr = 0.01
I0523 00:06:56.742635 34682 solver.cpp:239] Iteration 17170 (1.84693 iter/s, 5.41439s/10 iters), loss = 8.48592
I0523 00:06:56.742723 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48592 (* 1 = 8.48592 loss)
I0523 00:06:57.539916 34682 sgd_solver.cpp:112] Iteration 17170, lr = 0.01
I0523 00:06:59.399039 34682 solver.cpp:239] Iteration 17180 (3.76473 iter/s, 2.65623s/10 iters), loss = 9.45114
I0523 00:06:59.399092 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.45114 (* 1 = 9.45114 loss)
I0523 00:07:00.236975 34682 sgd_solver.cpp:112] Iteration 17180, lr = 0.01
I0523 00:07:02.885761 34682 solver.cpp:239] Iteration 17190 (2.86819 iter/s, 3.48652s/10 iters), loss = 8.0608
I0523 00:07:02.885808 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0608 (* 1 = 8.0608 loss)
I0523 00:07:03.750246 34682 sgd_solver.cpp:112] Iteration 17190, lr = 0.01
I0523 00:07:08.736106 34682 solver.cpp:239] Iteration 17200 (1.70939 iter/s, 5.85005s/10 iters), loss = 8.75403
I0523 00:07:08.736160 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75403 (* 1 = 8.75403 loss)
I0523 00:07:08.804790 34682 sgd_solver.cpp:112] Iteration 17200, lr = 0.01
I0523 00:07:13.095378 34682 solver.cpp:239] Iteration 17210 (2.29408 iter/s, 4.35904s/10 iters), loss = 8.89539
I0523 00:07:13.095633 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89539 (* 1 = 8.89539 loss)
I0523 00:07:13.953779 34682 sgd_solver.cpp:112] Iteration 17210, lr = 0.01
I0523 00:07:17.678387 34682 solver.cpp:239] Iteration 17220 (2.18217 iter/s, 4.58259s/10 iters), loss = 8.9574
I0523 00:07:17.678429 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9574 (* 1 = 8.9574 loss)
I0523 00:07:17.762420 34682 sgd_solver.cpp:112] Iteration 17220, lr = 0.01
I0523 00:07:22.816018 34682 solver.cpp:239] Iteration 17230 (1.94652 iter/s, 5.13736s/10 iters), loss = 8.67761
I0523 00:07:22.816089 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67761 (* 1 = 8.67761 loss)
I0523 00:07:23.483713 34682 sgd_solver.cpp:112] Iteration 17230, lr = 0.01
I0523 00:07:27.405766 34682 solver.cpp:239] Iteration 17240 (2.1789 iter/s, 4.58948s/10 iters), loss = 8.31479
I0523 00:07:27.405822 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31479 (* 1 = 8.31479 loss)
I0523 00:07:27.464491 34682 sgd_solver.cpp:112] Iteration 17240, lr = 0.01
I0523 00:07:31.486830 34682 solver.cpp:239] Iteration 17250 (2.45048 iter/s, 4.08084s/10 iters), loss = 8.663
I0523 00:07:31.486894 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.663 (* 1 = 8.663 loss)
I0523 00:07:32.350473 34682 sgd_solver.cpp:112] Iteration 17250, lr = 0.01
I0523 00:07:37.932138 34682 solver.cpp:239] Iteration 17260 (1.55159 iter/s, 6.44498s/10 iters), loss = 8.85361
I0523 00:07:37.932190 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85361 (* 1 = 8.85361 loss)
I0523 00:07:37.994748 34682 sgd_solver.cpp:112] Iteration 17260, lr = 0.01
I0523 00:07:42.022079 34682 solver.cpp:239] Iteration 17270 (2.44515 iter/s, 4.08972s/10 iters), loss = 8.49686
I0523 00:07:42.022130 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49686 (* 1 = 8.49686 loss)
I0523 00:07:42.877166 34682 sgd_solver.cpp:112] Iteration 17270, lr = 0.01
I0523 00:07:45.900152 34682 solver.cpp:239] Iteration 17280 (2.57874 iter/s, 3.87786s/10 iters), loss = 8.58512
I0523 00:07:45.900353 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58512 (* 1 = 8.58512 loss)
I0523 00:07:45.969734 34682 sgd_solver.cpp:112] Iteration 17280, lr = 0.01
I0523 00:07:49.372460 34682 solver.cpp:239] Iteration 17290 (2.8802 iter/s, 3.47198s/10 iters), loss = 8.67838
I0523 00:07:49.372527 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67838 (* 1 = 8.67838 loss)
I0523 00:07:49.429692 34682 sgd_solver.cpp:112] Iteration 17290, lr = 0.01
I0523 00:07:52.644640 34682 solver.cpp:239] Iteration 17300 (3.05626 iter/s, 3.27197s/10 iters), loss = 9.59796
I0523 00:07:52.644695 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59796 (* 1 = 9.59796 loss)
I0523 00:07:53.506642 34682 sgd_solver.cpp:112] Iteration 17300, lr = 0.01
I0523 00:07:59.624238 34682 solver.cpp:239] Iteration 17310 (1.43282 iter/s, 6.97926s/10 iters), loss = 7.91021
I0523 00:07:59.624281 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91021 (* 1 = 7.91021 loss)
I0523 00:08:00.471134 34682 sgd_solver.cpp:112] Iteration 17310, lr = 0.01
I0523 00:08:03.826814 34682 solver.cpp:239] Iteration 17320 (2.37962 iter/s, 4.20235s/10 iters), loss = 7.87343
I0523 00:08:03.826870 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87343 (* 1 = 7.87343 loss)
I0523 00:08:03.885088 34682 sgd_solver.cpp:112] Iteration 17320, lr = 0.01
I0523 00:08:09.489929 34682 solver.cpp:239] Iteration 17330 (1.7659 iter/s, 5.66282s/10 iters), loss = 8.51445
I0523 00:08:09.489979 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51445 (* 1 = 8.51445 loss)
I0523 00:08:09.552520 34682 sgd_solver.cpp:112] Iteration 17330, lr = 0.01
I0523 00:08:13.012621 34682 solver.cpp:239] Iteration 17340 (2.83889 iter/s, 3.5225s/10 iters), loss = 9.07409
I0523 00:08:13.012665 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07409 (* 1 = 9.07409 loss)
I0523 00:08:13.089614 34682 sgd_solver.cpp:112] Iteration 17340, lr = 0.01
I0523 00:08:17.106370 34682 solver.cpp:239] Iteration 17350 (2.44288 iter/s, 4.09353s/10 iters), loss = 9.27326
I0523 00:08:17.106680 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27326 (* 1 = 9.27326 loss)
I0523 00:08:17.926376 34682 sgd_solver.cpp:112] Iteration 17350, lr = 0.01
I0523 00:08:22.184931 34682 solver.cpp:239] Iteration 17360 (1.96925 iter/s, 5.07806s/10 iters), loss = 8.76603
I0523 00:08:22.184978 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76603 (* 1 = 8.76603 loss)
I0523 00:08:22.258767 34682 sgd_solver.cpp:112] Iteration 17360, lr = 0.01
I0523 00:08:27.356542 34682 solver.cpp:239] Iteration 17370 (1.93373 iter/s, 5.17135s/10 iters), loss = 8.72167
I0523 00:08:27.356590 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72167 (* 1 = 8.72167 loss)
I0523 00:08:27.432947 34682 sgd_solver.cpp:112] Iteration 17370, lr = 0.01
I0523 00:08:35.629523 34682 solver.cpp:239] Iteration 17380 (1.20881 iter/s, 8.2726s/10 iters), loss = 8.70687
I0523 00:08:35.629578 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70687 (* 1 = 8.70687 loss)
I0523 00:08:35.701136 34682 sgd_solver.cpp:112] Iteration 17380, lr = 0.01
I0523 00:08:40.291662 34682 solver.cpp:239] Iteration 17390 (2.14505 iter/s, 4.66189s/10 iters), loss = 8.03663
I0523 00:08:40.291725 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03663 (* 1 = 8.03663 loss)
I0523 00:08:40.358811 34682 sgd_solver.cpp:112] Iteration 17390, lr = 0.01
I0523 00:08:46.017896 34682 solver.cpp:239] Iteration 17400 (1.74644 iter/s, 5.72594s/10 iters), loss = 8.71938
I0523 00:08:46.017946 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71938 (* 1 = 8.71938 loss)
I0523 00:08:46.151583 34682 sgd_solver.cpp:112] Iteration 17400, lr = 0.01
I0523 00:08:49.797472 34682 solver.cpp:239] Iteration 17410 (2.64595 iter/s, 3.77936s/10 iters), loss = 8.71681
I0523 00:08:49.797621 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71681 (* 1 = 8.71681 loss)
I0523 00:08:50.258154 34682 sgd_solver.cpp:112] Iteration 17410, lr = 0.01
I0523 00:08:55.903189 34682 solver.cpp:239] Iteration 17420 (1.63792 iter/s, 6.10532s/10 iters), loss = 8.79205
I0523 00:08:55.903239 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79205 (* 1 = 8.79205 loss)
I0523 00:08:56.688982 34682 sgd_solver.cpp:112] Iteration 17420, lr = 0.01
I0523 00:09:00.693822 34682 solver.cpp:239] Iteration 17430 (2.08752 iter/s, 4.79038s/10 iters), loss = 8.18951
I0523 00:09:00.693876 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18951 (* 1 = 8.18951 loss)
I0523 00:09:01.569680 34682 sgd_solver.cpp:112] Iteration 17430, lr = 0.01
I0523 00:09:07.877038 34682 solver.cpp:239] Iteration 17440 (1.3922 iter/s, 7.18286s/10 iters), loss = 9.44649
I0523 00:09:07.877095 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44649 (* 1 = 9.44649 loss)
I0523 00:09:07.930217 34682 sgd_solver.cpp:112] Iteration 17440, lr = 0.01
I0523 00:09:13.726493 34682 solver.cpp:239] Iteration 17450 (1.70965 iter/s, 5.84915s/10 iters), loss = 8.81379
I0523 00:09:13.726538 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81379 (* 1 = 8.81379 loss)
I0523 00:09:13.803550 34682 sgd_solver.cpp:112] Iteration 17450, lr = 0.01
I0523 00:09:17.831943 34682 solver.cpp:239] Iteration 17460 (2.43592 iter/s, 4.10523s/10 iters), loss = 8.42364
I0523 00:09:17.831998 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42364 (* 1 = 8.42364 loss)
I0523 00:09:18.678393 34682 sgd_solver.cpp:112] Iteration 17460, lr = 0.01
I0523 00:09:22.867825 34682 solver.cpp:239] Iteration 17470 (1.98585 iter/s, 5.03562s/10 iters), loss = 8.94604
I0523 00:09:22.868088 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94604 (* 1 = 8.94604 loss)
I0523 00:09:23.054260 34682 sgd_solver.cpp:112] Iteration 17470, lr = 0.01
I0523 00:09:28.960952 34682 solver.cpp:239] Iteration 17480 (1.64133 iter/s, 6.09264s/10 iters), loss = 9.47079
I0523 00:09:28.961007 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47079 (* 1 = 9.47079 loss)
I0523 00:09:29.792469 34682 sgd_solver.cpp:112] Iteration 17480, lr = 0.01
I0523 00:09:33.176642 34682 solver.cpp:239] Iteration 17490 (2.37222 iter/s, 4.21546s/10 iters), loss = 8.92734
I0523 00:09:33.176714 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92734 (* 1 = 8.92734 loss)
I0523 00:09:33.824982 34682 sgd_solver.cpp:112] Iteration 17490, lr = 0.01
I0523 00:09:38.766031 34682 solver.cpp:239] Iteration 17500 (1.7892 iter/s, 5.58909s/10 iters), loss = 8.16477
I0523 00:09:38.766098 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16477 (* 1 = 8.16477 loss)
I0523 00:09:39.271689 34682 sgd_solver.cpp:112] Iteration 17500, lr = 0.01
I0523 00:09:41.947314 34682 solver.cpp:239] Iteration 17510 (3.14358 iter/s, 3.18109s/10 iters), loss = 8.10757
I0523 00:09:41.947358 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10757 (* 1 = 8.10757 loss)
I0523 00:09:42.020429 34682 sgd_solver.cpp:112] Iteration 17510, lr = 0.01
I0523 00:09:48.103102 34682 solver.cpp:239] Iteration 17520 (1.62457 iter/s, 6.15549s/10 iters), loss = 8.61319
I0523 00:09:48.103163 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61319 (* 1 = 8.61319 loss)
I0523 00:09:48.953187 34682 sgd_solver.cpp:112] Iteration 17520, lr = 0.01
I0523 00:09:53.092120 34682 solver.cpp:239] Iteration 17530 (2.00451 iter/s, 4.98876s/10 iters), loss = 7.98858
I0523 00:09:53.092226 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98858 (* 1 = 7.98858 loss)
I0523 00:09:53.156766 34682 sgd_solver.cpp:112] Iteration 17530, lr = 0.01
I0523 00:09:55.876541 34682 solver.cpp:239] Iteration 17540 (3.5917 iter/s, 2.7842s/10 iters), loss = 8.84838
I0523 00:09:55.876590 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84838 (* 1 = 8.84838 loss)
I0523 00:09:56.518138 34682 sgd_solver.cpp:112] Iteration 17540, lr = 0.01
I0523 00:10:00.726048 34682 solver.cpp:239] Iteration 17550 (2.06217 iter/s, 4.84926s/10 iters), loss = 8.31609
I0523 00:10:00.726094 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31609 (* 1 = 8.31609 loss)
I0523 00:10:00.803975 34682 sgd_solver.cpp:112] Iteration 17550, lr = 0.01
I0523 00:10:04.255228 34682 solver.cpp:239] Iteration 17560 (2.83368 iter/s, 3.52898s/10 iters), loss = 7.64366
I0523 00:10:04.255295 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64366 (* 1 = 7.64366 loss)
I0523 00:10:05.073120 34682 sgd_solver.cpp:112] Iteration 17560, lr = 0.01
I0523 00:10:09.178823 34682 solver.cpp:239] Iteration 17570 (2.03296 iter/s, 4.91893s/10 iters), loss = 9.08548
I0523 00:10:09.178867 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08548 (* 1 = 9.08548 loss)
I0523 00:10:09.238489 34682 sgd_solver.cpp:112] Iteration 17570, lr = 0.01
I0523 00:10:15.599941 34682 solver.cpp:239] Iteration 17580 (1.55743 iter/s, 6.42081s/10 iters), loss = 8.94811
I0523 00:10:15.599990 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94811 (* 1 = 8.94811 loss)
I0523 00:10:15.665562 34682 sgd_solver.cpp:112] Iteration 17580, lr = 0.01
I0523 00:10:18.787158 34682 solver.cpp:239] Iteration 17590 (3.13772 iter/s, 3.18702s/10 iters), loss = 8.62814
I0523 00:10:18.787236 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62814 (* 1 = 8.62814 loss)
I0523 00:10:18.856588 34682 sgd_solver.cpp:112] Iteration 17590, lr = 0.01
I0523 00:10:23.914067 34682 solver.cpp:239] Iteration 17600 (1.9506 iter/s, 5.12663s/10 iters), loss = 9.15262
I0523 00:10:23.914245 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15262 (* 1 = 9.15262 loss)
I0523 00:10:24.363251 34682 sgd_solver.cpp:112] Iteration 17600, lr = 0.01
I0523 00:10:28.438752 34682 solver.cpp:239] Iteration 17610 (2.21027 iter/s, 4.52433s/10 iters), loss = 8.44161
I0523 00:10:28.438796 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44161 (* 1 = 8.44161 loss)
I0523 00:10:28.515944 34682 sgd_solver.cpp:112] Iteration 17610, lr = 0.01
I0523 00:10:31.337388 34682 solver.cpp:239] Iteration 17620 (3.4501 iter/s, 2.89847s/10 iters), loss = 9.18751
I0523 00:10:31.337436 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18751 (* 1 = 9.18751 loss)
I0523 00:10:31.412123 34682 sgd_solver.cpp:112] Iteration 17620, lr = 0.01
I0523 00:10:37.706907 34682 solver.cpp:239] Iteration 17630 (1.57005 iter/s, 6.36921s/10 iters), loss = 8.92647
I0523 00:10:37.706962 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92647 (* 1 = 8.92647 loss)
I0523 00:10:37.776244 34682 sgd_solver.cpp:112] Iteration 17630, lr = 0.01
I0523 00:10:43.432513 34682 solver.cpp:239] Iteration 17640 (1.74663 iter/s, 5.72532s/10 iters), loss = 8.21495
I0523 00:10:43.432565 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21495 (* 1 = 8.21495 loss)
I0523 00:10:43.513536 34682 sgd_solver.cpp:112] Iteration 17640, lr = 0.01
I0523 00:10:48.161523 34682 solver.cpp:239] Iteration 17650 (2.11472 iter/s, 4.72876s/10 iters), loss = 8.46255
I0523 00:10:48.161586 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46255 (* 1 = 8.46255 loss)
I0523 00:10:49.030588 34682 sgd_solver.cpp:112] Iteration 17650, lr = 0.01
I0523 00:10:54.621706 34682 solver.cpp:239] Iteration 17660 (1.54802 iter/s, 6.45985s/10 iters), loss = 9.22171
I0523 00:10:54.622006 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22171 (* 1 = 9.22171 loss)
I0523 00:10:54.688515 34682 sgd_solver.cpp:112] Iteration 17660, lr = 0.01
I0523 00:10:59.289206 34682 solver.cpp:239] Iteration 17670 (2.14269 iter/s, 4.66703s/10 iters), loss = 9.3039
I0523 00:10:59.289253 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3039 (* 1 = 9.3039 loss)
I0523 00:11:00.106155 34682 sgd_solver.cpp:112] Iteration 17670, lr = 0.01
I0523 00:11:03.437508 34682 solver.cpp:239] Iteration 17680 (2.41076 iter/s, 4.14807s/10 iters), loss = 8.34862
I0523 00:11:03.437546 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34862 (* 1 = 8.34862 loss)
I0523 00:11:03.504504 34682 sgd_solver.cpp:112] Iteration 17680, lr = 0.01
I0523 00:11:07.010947 34682 solver.cpp:239] Iteration 17690 (2.79857 iter/s, 3.57325s/10 iters), loss = 8.82931
I0523 00:11:07.010995 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82931 (* 1 = 8.82931 loss)
I0523 00:11:07.686928 34682 sgd_solver.cpp:112] Iteration 17690, lr = 0.01
I0523 00:11:13.214335 34682 solver.cpp:239] Iteration 17700 (1.6121 iter/s, 6.20308s/10 iters), loss = 9.61614
I0523 00:11:13.214396 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.61614 (* 1 = 9.61614 loss)
I0523 00:11:13.955381 34682 sgd_solver.cpp:112] Iteration 17700, lr = 0.01
I0523 00:11:19.122457 34682 solver.cpp:239] Iteration 17710 (1.69267 iter/s, 5.90782s/10 iters), loss = 8.73506
I0523 00:11:19.122511 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73506 (* 1 = 8.73506 loss)
I0523 00:11:19.837863 34682 sgd_solver.cpp:112] Iteration 17710, lr = 0.01
I0523 00:11:25.570519 34682 solver.cpp:239] Iteration 17720 (1.55093 iter/s, 6.44775s/10 iters), loss = 8.96126
I0523 00:11:25.570775 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96126 (* 1 = 8.96126 loss)
I0523 00:11:26.341958 34682 sgd_solver.cpp:112] Iteration 17720, lr = 0.01
I0523 00:11:29.484414 34682 solver.cpp:239] Iteration 17730 (2.55526 iter/s, 3.91349s/10 iters), loss = 8.86559
I0523 00:11:29.484482 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86559 (* 1 = 8.86559 loss)
I0523 00:11:29.538758 34682 sgd_solver.cpp:112] Iteration 17730, lr = 0.01
I0523 00:11:33.223284 34682 solver.cpp:239] Iteration 17740 (2.67476 iter/s, 3.73865s/10 iters), loss = 8.94042
I0523 00:11:33.223341 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94042 (* 1 = 8.94042 loss)
I0523 00:11:33.856004 34682 sgd_solver.cpp:112] Iteration 17740, lr = 0.01
I0523 00:11:37.785030 34682 solver.cpp:239] Iteration 17750 (2.19226 iter/s, 4.5615s/10 iters), loss = 8.61548
I0523 00:11:37.785080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61548 (* 1 = 8.61548 loss)
I0523 00:11:37.853792 34682 sgd_solver.cpp:112] Iteration 17750, lr = 0.01
I0523 00:11:43.457276 34682 solver.cpp:239] Iteration 17760 (1.76306 iter/s, 5.67196s/10 iters), loss = 8.29392
I0523 00:11:43.457324 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29392 (* 1 = 8.29392 loss)
I0523 00:11:43.524358 34682 sgd_solver.cpp:112] Iteration 17760, lr = 0.01
I0523 00:11:47.031858 34682 solver.cpp:239] Iteration 17770 (2.79769 iter/s, 3.57438s/10 iters), loss = 9.25176
I0523 00:11:47.031918 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25176 (* 1 = 9.25176 loss)
I0523 00:11:47.102612 34682 sgd_solver.cpp:112] Iteration 17770, lr = 0.01
I0523 00:11:51.713220 34682 solver.cpp:239] Iteration 17780 (2.13624 iter/s, 4.68111s/10 iters), loss = 9.55186
I0523 00:11:51.713276 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.55186 (* 1 = 9.55186 loss)
I0523 00:11:51.785578 34682 sgd_solver.cpp:112] Iteration 17780, lr = 0.01
I0523 00:11:56.861482 34682 solver.cpp:239] Iteration 17790 (1.9425 iter/s, 5.148s/10 iters), loss = 9.15339
I0523 00:11:56.861723 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15339 (* 1 = 9.15339 loss)
I0523 00:11:56.937898 34682 sgd_solver.cpp:112] Iteration 17790, lr = 0.01
I0523 00:12:01.624698 34682 solver.cpp:239] Iteration 17800 (2.09961 iter/s, 4.76279s/10 iters), loss = 8.11617
I0523 00:12:01.624760 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11617 (* 1 = 8.11617 loss)
I0523 00:12:02.490111 34682 sgd_solver.cpp:112] Iteration 17800, lr = 0.01
I0523 00:12:08.416579 34682 solver.cpp:239] Iteration 17810 (1.47242 iter/s, 6.79154s/10 iters), loss = 9.06965
I0523 00:12:08.416647 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06965 (* 1 = 9.06965 loss)
I0523 00:12:08.478869 34682 sgd_solver.cpp:112] Iteration 17810, lr = 0.01
I0523 00:12:12.180719 34682 solver.cpp:239] Iteration 17820 (2.65681 iter/s, 3.76392s/10 iters), loss = 8.32112
I0523 00:12:12.180770 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32112 (* 1 = 8.32112 loss)
I0523 00:12:12.244040 34682 sgd_solver.cpp:112] Iteration 17820, lr = 0.01
I0523 00:12:16.291286 34682 solver.cpp:239] Iteration 17830 (2.43288 iter/s, 4.11035s/10 iters), loss = 8.03581
I0523 00:12:16.291334 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03581 (* 1 = 8.03581 loss)
I0523 00:12:16.351676 34682 sgd_solver.cpp:112] Iteration 17830, lr = 0.01
I0523 00:12:19.661872 34682 solver.cpp:239] Iteration 17840 (2.96701 iter/s, 3.3704s/10 iters), loss = 9.42021
I0523 00:12:19.661921 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42021 (* 1 = 9.42021 loss)
I0523 00:12:19.728088 34682 sgd_solver.cpp:112] Iteration 17840, lr = 0.01
I0523 00:12:22.279516 34682 solver.cpp:239] Iteration 17850 (3.82048 iter/s, 2.61748s/10 iters), loss = 8.98281
I0523 00:12:22.279569 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98281 (* 1 = 8.98281 loss)
I0523 00:12:22.343209 34682 sgd_solver.cpp:112] Iteration 17850, lr = 0.01
I0523 00:12:25.673604 34682 solver.cpp:239] Iteration 17860 (2.94648 iter/s, 3.39388s/10 iters), loss = 8.98933
I0523 00:12:25.673656 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98933 (* 1 = 8.98933 loss)
I0523 00:12:26.442178 34682 sgd_solver.cpp:112] Iteration 17860, lr = 0.01
I0523 00:12:31.637688 34682 solver.cpp:239] Iteration 17870 (1.67679 iter/s, 5.96379s/10 iters), loss = 8.34236
I0523 00:12:31.637882 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34236 (* 1 = 8.34236 loss)
I0523 00:12:31.707466 34682 sgd_solver.cpp:112] Iteration 17870, lr = 0.01
I0523 00:12:35.114075 34682 solver.cpp:239] Iteration 17880 (2.8768 iter/s, 3.47608s/10 iters), loss = 7.78651
I0523 00:12:35.114130 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.78651 (* 1 = 7.78651 loss)
I0523 00:12:35.833214 34682 sgd_solver.cpp:112] Iteration 17880, lr = 0.01
I0523 00:12:38.671779 34682 solver.cpp:239] Iteration 17890 (2.81098 iter/s, 3.55748s/10 iters), loss = 8.63619
I0523 00:12:38.671844 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63619 (* 1 = 8.63619 loss)
I0523 00:12:39.365372 34682 sgd_solver.cpp:112] Iteration 17890, lr = 0.01
I0523 00:12:45.830524 34682 solver.cpp:239] Iteration 17900 (1.39696 iter/s, 7.1584s/10 iters), loss = 7.30232
I0523 00:12:45.830574 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.30232 (* 1 = 7.30232 loss)
I0523 00:12:45.900544 34682 sgd_solver.cpp:112] Iteration 17900, lr = 0.01
I0523 00:12:50.238199 34682 solver.cpp:239] Iteration 17910 (2.26889 iter/s, 4.40744s/10 iters), loss = 8.50437
I0523 00:12:50.238252 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50437 (* 1 = 8.50437 loss)
I0523 00:12:50.304286 34682 sgd_solver.cpp:112] Iteration 17910, lr = 0.01
I0523 00:12:55.308130 34682 solver.cpp:239] Iteration 17920 (1.97252 iter/s, 5.06967s/10 iters), loss = 9.1701
I0523 00:12:55.308182 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1701 (* 1 = 9.1701 loss)
I0523 00:12:56.032035 34682 sgd_solver.cpp:112] Iteration 17920, lr = 0.01
I0523 00:13:00.857424 34682 solver.cpp:239] Iteration 17930 (1.80212 iter/s, 5.54902s/10 iters), loss = 8.00873
I0523 00:13:00.857471 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00873 (* 1 = 8.00873 loss)
I0523 00:13:01.743863 34682 sgd_solver.cpp:112] Iteration 17930, lr = 0.01
I0523 00:13:05.924554 34682 solver.cpp:239] Iteration 17940 (1.9736 iter/s, 5.06687s/10 iters), loss = 8.01952
I0523 00:13:05.924597 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01952 (* 1 = 8.01952 loss)
I0523 00:13:05.983837 34682 sgd_solver.cpp:112] Iteration 17940, lr = 0.01
I0523 00:13:11.171921 34682 solver.cpp:239] Iteration 17950 (1.90581 iter/s, 5.24711s/10 iters), loss = 8.50465
I0523 00:13:11.171979 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50465 (* 1 = 8.50465 loss)
I0523 00:13:11.235250 34682 sgd_solver.cpp:112] Iteration 17950, lr = 0.01
I0523 00:13:16.930626 34682 solver.cpp:239] Iteration 17960 (1.73659 iter/s, 5.75841s/10 iters), loss = 8.75639
I0523 00:13:16.930685 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75639 (* 1 = 8.75639 loss)
I0523 00:13:17.700914 34682 sgd_solver.cpp:112] Iteration 17960, lr = 0.01
I0523 00:13:22.976999 34682 solver.cpp:239] Iteration 17970 (1.65397 iter/s, 6.04607s/10 iters), loss = 9.48437
I0523 00:13:22.977059 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.48437 (* 1 = 9.48437 loss)
I0523 00:13:23.053966 34682 sgd_solver.cpp:112] Iteration 17970, lr = 0.01
I0523 00:13:26.448559 34682 solver.cpp:239] Iteration 17980 (2.88072 iter/s, 3.47135s/10 iters), loss = 8.15597
I0523 00:13:26.448614 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15597 (* 1 = 8.15597 loss)
I0523 00:13:26.515063 34682 sgd_solver.cpp:112] Iteration 17980, lr = 0.01
I0523 00:13:31.797189 34682 solver.cpp:239] Iteration 17990 (1.86973 iter/s, 5.34836s/10 iters), loss = 9.0602
I0523 00:13:31.797426 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0602 (* 1 = 9.0602 loss)
I0523 00:13:31.869721 34682 sgd_solver.cpp:112] Iteration 17990, lr = 0.01
I0523 00:13:36.701010 34682 solver.cpp:239] Iteration 18000 (2.04041 iter/s, 4.90097s/10 iters), loss = 8.5341
I0523 00:13:36.701066 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5341 (* 1 = 8.5341 loss)
I0523 00:13:37.526242 34682 sgd_solver.cpp:112] Iteration 18000, lr = 0.01
I0523 00:13:43.316193 34682 solver.cpp:239] Iteration 18010 (1.51175 iter/s, 6.61486s/10 iters), loss = 8.65588
I0523 00:13:43.316258 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65588 (* 1 = 8.65588 loss)
I0523 00:13:44.051235 34682 sgd_solver.cpp:112] Iteration 18010, lr = 0.01
I0523 00:13:48.060057 34682 solver.cpp:239] Iteration 18020 (2.10811 iter/s, 4.74359s/10 iters), loss = 8.5665
I0523 00:13:48.060117 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5665 (* 1 = 8.5665 loss)
I0523 00:13:48.135478 34682 sgd_solver.cpp:112] Iteration 18020, lr = 0.01
I0523 00:13:55.054988 34682 solver.cpp:239] Iteration 18030 (1.42968 iter/s, 6.99459s/10 iters), loss = 8.52873
I0523 00:13:55.055038 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52873 (* 1 = 8.52873 loss)
I0523 00:13:55.118975 34682 sgd_solver.cpp:112] Iteration 18030, lr = 0.01
I0523 00:13:59.292560 34682 solver.cpp:239] Iteration 18040 (2.35998 iter/s, 4.23732s/10 iters), loss = 9.35437
I0523 00:13:59.292620 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35437 (* 1 = 9.35437 loss)
I0523 00:14:00.114907 34682 sgd_solver.cpp:112] Iteration 18040, lr = 0.01
I0523 00:14:06.544669 34682 solver.cpp:239] Iteration 18050 (1.37897 iter/s, 7.25177s/10 iters), loss = 8.51022
I0523 00:14:06.544893 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51022 (* 1 = 8.51022 loss)
I0523 00:14:06.606343 34682 sgd_solver.cpp:112] Iteration 18050, lr = 0.01
I0523 00:14:11.563308 34682 solver.cpp:239] Iteration 18060 (1.99273 iter/s, 5.01823s/10 iters), loss = 8.61349
I0523 00:14:11.563356 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61349 (* 1 = 8.61349 loss)
I0523 00:14:12.347667 34682 sgd_solver.cpp:112] Iteration 18060, lr = 0.01
I0523 00:14:15.814904 34682 solver.cpp:239] Iteration 18070 (2.35218 iter/s, 4.25137s/10 iters), loss = 8.48916
I0523 00:14:15.814949 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48916 (* 1 = 8.48916 loss)
I0523 00:14:16.602850 34682 sgd_solver.cpp:112] Iteration 18070, lr = 0.01
I0523 00:14:21.204473 34682 solver.cpp:239] Iteration 18080 (1.85554 iter/s, 5.38928s/10 iters), loss = 9.67346
I0523 00:14:21.204517 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.67346 (* 1 = 9.67346 loss)
I0523 00:14:21.260527 34682 sgd_solver.cpp:112] Iteration 18080, lr = 0.01
I0523 00:14:26.539124 34682 solver.cpp:239] Iteration 18090 (1.87463 iter/s, 5.33438s/10 iters), loss = 8.56927
I0523 00:14:26.539173 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56927 (* 1 = 8.56927 loss)
I0523 00:14:27.385509 34682 sgd_solver.cpp:112] Iteration 18090, lr = 0.01
I0523 00:14:31.519886 34682 solver.cpp:239] Iteration 18100 (2.00783 iter/s, 4.98051s/10 iters), loss = 9.81991
I0523 00:14:31.519939 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.81991 (* 1 = 9.81991 loss)
I0523 00:14:32.313686 34682 sgd_solver.cpp:112] Iteration 18100, lr = 0.01
I0523 00:14:37.104938 34682 solver.cpp:239] Iteration 18110 (1.79059 iter/s, 5.58476s/10 iters), loss = 7.9657
I0523 00:14:37.105154 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9657 (* 1 = 7.9657 loss)
I0523 00:14:37.182525 34682 sgd_solver.cpp:112] Iteration 18110, lr = 0.01
I0523 00:14:41.329530 34682 solver.cpp:239] Iteration 18120 (2.36729 iter/s, 4.22424s/10 iters), loss = 7.97248
I0523 00:14:41.329563 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97248 (* 1 = 7.97248 loss)
I0523 00:14:41.402639 34682 sgd_solver.cpp:112] Iteration 18120, lr = 0.01
I0523 00:14:47.193954 34682 solver.cpp:239] Iteration 18130 (1.70528 iter/s, 5.86415s/10 iters), loss = 8.91571
I0523 00:14:47.194008 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91571 (* 1 = 8.91571 loss)
I0523 00:14:47.933903 34682 sgd_solver.cpp:112] Iteration 18130, lr = 0.01
I0523 00:14:54.363692 34682 solver.cpp:239] Iteration 18140 (1.39482 iter/s, 7.1694s/10 iters), loss = 8.33032
I0523 00:14:54.363734 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33032 (* 1 = 8.33032 loss)
I0523 00:14:54.428771 34682 sgd_solver.cpp:112] Iteration 18140, lr = 0.01
I0523 00:15:01.098712 34682 solver.cpp:239] Iteration 18150 (1.48485 iter/s, 6.73469s/10 iters), loss = 8.69702
I0523 00:15:01.098759 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69702 (* 1 = 8.69702 loss)
I0523 00:15:01.958678 34682 sgd_solver.cpp:112] Iteration 18150, lr = 0.01
I0523 00:15:07.732961 34682 solver.cpp:239] Iteration 18160 (1.5074 iter/s, 6.63392s/10 iters), loss = 9.02941
I0523 00:15:07.733224 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02941 (* 1 = 9.02941 loss)
I0523 00:15:07.794534 34682 sgd_solver.cpp:112] Iteration 18160, lr = 0.01
I0523 00:15:13.234462 34682 solver.cpp:239] Iteration 18170 (1.81784 iter/s, 5.50104s/10 iters), loss = 8.74899
I0523 00:15:13.234519 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74899 (* 1 = 8.74899 loss)
I0523 00:15:14.021201 34682 sgd_solver.cpp:112] Iteration 18170, lr = 0.01
I0523 00:15:17.238302 34682 solver.cpp:239] Iteration 18180 (2.49774 iter/s, 4.00362s/10 iters), loss = 9.22186
I0523 00:15:17.238350 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22186 (* 1 = 9.22186 loss)
I0523 00:15:18.082496 34682 sgd_solver.cpp:112] Iteration 18180, lr = 0.01
I0523 00:15:22.809478 34682 solver.cpp:239] Iteration 18190 (1.79505 iter/s, 5.57089s/10 iters), loss = 9.22004
I0523 00:15:22.809541 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22004 (* 1 = 9.22004 loss)
I0523 00:15:23.512306 34682 sgd_solver.cpp:112] Iteration 18190, lr = 0.01
I0523 00:15:28.357676 34682 solver.cpp:239] Iteration 18200 (1.80248 iter/s, 5.54791s/10 iters), loss = 8.28659
I0523 00:15:28.357718 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28659 (* 1 = 8.28659 loss)
I0523 00:15:28.438943 34682 sgd_solver.cpp:112] Iteration 18200, lr = 0.01
I0523 00:15:31.943107 34682 solver.cpp:239] Iteration 18210 (2.78922 iter/s, 3.58523s/10 iters), loss = 8.80651
I0523 00:15:31.943163 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80651 (* 1 = 8.80651 loss)
I0523 00:15:32.747365 34682 sgd_solver.cpp:112] Iteration 18210, lr = 0.01
I0523 00:15:35.960098 34682 solver.cpp:239] Iteration 18220 (2.48956 iter/s, 4.01678s/10 iters), loss = 7.88156
I0523 00:15:35.960144 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88156 (* 1 = 7.88156 loss)
I0523 00:15:36.569097 34682 sgd_solver.cpp:112] Iteration 18220, lr = 0.01
I0523 00:15:40.694217 34682 solver.cpp:239] Iteration 18230 (2.11244 iter/s, 4.73386s/10 iters), loss = 7.83933
I0523 00:15:40.694429 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83933 (* 1 = 7.83933 loss)
I0523 00:15:41.517575 34682 sgd_solver.cpp:112] Iteration 18230, lr = 0.01
I0523 00:15:45.549536 34682 solver.cpp:239] Iteration 18240 (2.05977 iter/s, 4.85492s/10 iters), loss = 8.9029
I0523 00:15:45.549585 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9029 (* 1 = 8.9029 loss)
I0523 00:15:45.610996 34682 sgd_solver.cpp:112] Iteration 18240, lr = 0.01
I0523 00:15:50.719112 34682 solver.cpp:239] Iteration 18250 (1.93449 iter/s, 5.16931s/10 iters), loss = 8.44392
I0523 00:15:50.719173 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44392 (* 1 = 8.44392 loss)
I0523 00:15:51.539605 34682 sgd_solver.cpp:112] Iteration 18250, lr = 0.01
I0523 00:15:54.793690 34682 solver.cpp:239] Iteration 18260 (2.45438 iter/s, 4.07434s/10 iters), loss = 8.23525
I0523 00:15:54.793742 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23525 (* 1 = 8.23525 loss)
I0523 00:15:54.997537 34682 sgd_solver.cpp:112] Iteration 18260, lr = 0.01
I0523 00:15:59.050839 34682 solver.cpp:239] Iteration 18270 (2.34913 iter/s, 4.2569s/10 iters), loss = 9.03616
I0523 00:15:59.050889 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03616 (* 1 = 9.03616 loss)
I0523 00:15:59.796756 34682 sgd_solver.cpp:112] Iteration 18270, lr = 0.01
I0523 00:16:04.637204 34682 solver.cpp:239] Iteration 18280 (1.79016 iter/s, 5.58609s/10 iters), loss = 7.99742
I0523 00:16:04.637257 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99742 (* 1 = 7.99742 loss)
I0523 00:16:05.482934 34682 sgd_solver.cpp:112] Iteration 18280, lr = 0.01
I0523 00:16:08.876636 34682 solver.cpp:239] Iteration 18290 (2.35894 iter/s, 4.23919s/10 iters), loss = 9.66741
I0523 00:16:08.876683 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.66741 (* 1 = 9.66741 loss)
I0523 00:16:08.944051 34682 sgd_solver.cpp:112] Iteration 18290, lr = 0.01
I0523 00:16:13.001767 34682 solver.cpp:239] Iteration 18300 (2.42429 iter/s, 4.12491s/10 iters), loss = 9.05359
I0523 00:16:13.002024 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05359 (* 1 = 9.05359 loss)
I0523 00:16:13.081871 34682 sgd_solver.cpp:112] Iteration 18300, lr = 0.01
I0523 00:16:18.758203 34682 solver.cpp:239] Iteration 18310 (1.73733 iter/s, 5.75596s/10 iters), loss = 8.56203
I0523 00:16:18.758246 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56203 (* 1 = 8.56203 loss)
I0523 00:16:18.842212 34682 sgd_solver.cpp:112] Iteration 18310, lr = 0.01
I0523 00:16:22.211891 34682 solver.cpp:239] Iteration 18320 (2.89931 iter/s, 3.44909s/10 iters), loss = 8.97925
I0523 00:16:22.211951 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97925 (* 1 = 8.97925 loss)
I0523 00:16:22.690753 34682 sgd_solver.cpp:112] Iteration 18320, lr = 0.01
I0523 00:16:26.193701 34682 solver.cpp:239] Iteration 18330 (2.51157 iter/s, 3.98158s/10 iters), loss = 9.00792
I0523 00:16:26.193765 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00792 (* 1 = 9.00792 loss)
I0523 00:16:26.266238 34682 sgd_solver.cpp:112] Iteration 18330, lr = 0.01
I0523 00:16:29.500156 34682 solver.cpp:239] Iteration 18340 (3.02457 iter/s, 3.30625s/10 iters), loss = 8.59466
I0523 00:16:29.500197 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59466 (* 1 = 8.59466 loss)
I0523 00:16:29.579494 34682 sgd_solver.cpp:112] Iteration 18340, lr = 0.01
I0523 00:16:32.972605 34682 solver.cpp:239] Iteration 18350 (2.87998 iter/s, 3.47225s/10 iters), loss = 8.71328
I0523 00:16:32.972652 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71328 (* 1 = 8.71328 loss)
I0523 00:16:33.062979 34682 sgd_solver.cpp:112] Iteration 18350, lr = 0.01
I0523 00:16:37.883817 34682 solver.cpp:239] Iteration 18360 (2.03626 iter/s, 4.91096s/10 iters), loss = 9.65077
I0523 00:16:37.883863 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.65077 (* 1 = 9.65077 loss)
I0523 00:16:37.954907 34682 sgd_solver.cpp:112] Iteration 18360, lr = 0.01
I0523 00:16:41.360422 34682 solver.cpp:239] Iteration 18370 (2.87652 iter/s, 3.47642s/10 iters), loss = 9.35284
I0523 00:16:41.360466 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35284 (* 1 = 9.35284 loss)
I0523 00:16:41.509593 34682 sgd_solver.cpp:112] Iteration 18370, lr = 0.01
I0523 00:16:46.570070 34682 solver.cpp:239] Iteration 18380 (1.91961 iter/s, 5.20939s/10 iters), loss = 9.21159
I0523 00:16:46.570164 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21159 (* 1 = 9.21159 loss)
I0523 00:16:46.633796 34682 sgd_solver.cpp:112] Iteration 18380, lr = 0.01
I0523 00:16:49.439064 34682 solver.cpp:239] Iteration 18390 (3.4858 iter/s, 2.86878s/10 iters), loss = 8.29481
I0523 00:16:49.439128 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29481 (* 1 = 8.29481 loss)
I0523 00:16:49.868142 34682 sgd_solver.cpp:112] Iteration 18390, lr = 0.01
I0523 00:16:54.183919 34682 solver.cpp:239] Iteration 18400 (2.10766 iter/s, 4.7446s/10 iters), loss = 9.01572
I0523 00:16:54.183961 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01572 (* 1 = 9.01572 loss)
I0523 00:16:54.248524 34682 sgd_solver.cpp:112] Iteration 18400, lr = 0.01
I0523 00:16:57.798169 34682 solver.cpp:239] Iteration 18410 (2.76698 iter/s, 3.61405s/10 iters), loss = 8.13402
I0523 00:16:57.798235 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13402 (* 1 = 8.13402 loss)
I0523 00:16:58.682121 34682 sgd_solver.cpp:112] Iteration 18410, lr = 0.01
I0523 00:17:02.773490 34682 solver.cpp:239] Iteration 18420 (2.01003 iter/s, 4.97506s/10 iters), loss = 9.14425
I0523 00:17:02.773545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14425 (* 1 = 9.14425 loss)
I0523 00:17:02.835134 34682 sgd_solver.cpp:112] Iteration 18420, lr = 0.01
I0523 00:17:09.041144 34682 solver.cpp:239] Iteration 18430 (1.59558 iter/s, 6.26733s/10 iters), loss = 8.32225
I0523 00:17:09.041206 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32225 (* 1 = 8.32225 loss)
I0523 00:17:09.131350 34682 sgd_solver.cpp:112] Iteration 18430, lr = 0.01
I0523 00:17:12.051072 34682 solver.cpp:239] Iteration 18440 (3.32255 iter/s, 3.00973s/10 iters), loss = 9.32763
I0523 00:17:12.051120 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32763 (* 1 = 9.32763 loss)
I0523 00:17:12.127720 34682 sgd_solver.cpp:112] Iteration 18440, lr = 0.01
I0523 00:17:17.613530 34682 solver.cpp:239] Iteration 18450 (1.79785 iter/s, 5.56219s/10 iters), loss = 9.09516
I0523 00:17:17.613801 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09516 (* 1 = 9.09516 loss)
I0523 00:17:17.680986 34682 sgd_solver.cpp:112] Iteration 18450, lr = 0.01
I0523 00:17:23.260993 34682 solver.cpp:239] Iteration 18460 (1.77086 iter/s, 5.64698s/10 iters), loss = 7.83007
I0523 00:17:23.261046 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83007 (* 1 = 7.83007 loss)
I0523 00:17:23.321492 34682 sgd_solver.cpp:112] Iteration 18460, lr = 0.01
I0523 00:17:28.104362 34682 solver.cpp:239] Iteration 18470 (2.06479 iter/s, 4.84311s/10 iters), loss = 8.65312
I0523 00:17:28.104415 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65312 (* 1 = 8.65312 loss)
I0523 00:17:28.988328 34682 sgd_solver.cpp:112] Iteration 18470, lr = 0.01
I0523 00:17:33.519698 34682 solver.cpp:239] Iteration 18480 (1.8467 iter/s, 5.41506s/10 iters), loss = 10.3144
I0523 00:17:33.519738 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.3144 (* 1 = 10.3144 loss)
I0523 00:17:33.594163 34682 sgd_solver.cpp:112] Iteration 18480, lr = 0.01
I0523 00:17:38.478308 34682 solver.cpp:239] Iteration 18490 (2.01681 iter/s, 4.95832s/10 iters), loss = 8.85009
I0523 00:17:38.478374 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85009 (* 1 = 8.85009 loss)
I0523 00:17:38.540437 34682 sgd_solver.cpp:112] Iteration 18490, lr = 0.01
I0523 00:17:41.923981 34682 solver.cpp:239] Iteration 18500 (2.90237 iter/s, 3.44545s/10 iters), loss = 8.58721
I0523 00:17:41.924048 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58721 (* 1 = 8.58721 loss)
I0523 00:17:41.988157 34682 sgd_solver.cpp:112] Iteration 18500, lr = 0.01
I0523 00:17:46.150256 34682 solver.cpp:239] Iteration 18510 (2.36628 iter/s, 4.22604s/10 iters), loss = 9.07772
I0523 00:17:46.150311 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07772 (* 1 = 9.07772 loss)
I0523 00:17:46.225709 34682 sgd_solver.cpp:112] Iteration 18510, lr = 0.01
I0523 00:17:50.450634 34682 solver.cpp:239] Iteration 18520 (2.32551 iter/s, 4.30013s/10 iters), loss = 8.8157
I0523 00:17:50.450865 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8157 (* 1 = 8.8157 loss)
I0523 00:17:51.286638 34682 sgd_solver.cpp:112] Iteration 18520, lr = 0.01
I0523 00:17:55.456712 34682 solver.cpp:239] Iteration 18530 (1.99774 iter/s, 5.00566s/10 iters), loss = 8.78704
I0523 00:17:55.456753 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78704 (* 1 = 8.78704 loss)
I0523 00:17:55.531452 34682 sgd_solver.cpp:112] Iteration 18530, lr = 0.01
I0523 00:17:58.941184 34682 solver.cpp:239] Iteration 18540 (2.87003 iter/s, 3.48428s/10 iters), loss = 9.00818
I0523 00:17:58.941226 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00818 (* 1 = 9.00818 loss)
I0523 00:17:59.009743 34682 sgd_solver.cpp:112] Iteration 18540, lr = 0.01
I0523 00:18:06.068172 34682 solver.cpp:239] Iteration 18550 (1.40318 iter/s, 7.12665s/10 iters), loss = 8.20976
I0523 00:18:06.068235 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20976 (* 1 = 8.20976 loss)
I0523 00:18:06.723984 34682 sgd_solver.cpp:112] Iteration 18550, lr = 0.01
I0523 00:18:11.489320 34682 solver.cpp:239] Iteration 18560 (1.84472 iter/s, 5.42086s/10 iters), loss = 8.33033
I0523 00:18:11.489372 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33033 (* 1 = 8.33033 loss)
I0523 00:18:12.317075 34682 sgd_solver.cpp:112] Iteration 18560, lr = 0.01
I0523 00:18:15.839597 34682 solver.cpp:239] Iteration 18570 (2.29883 iter/s, 4.35005s/10 iters), loss = 8.46403
I0523 00:18:15.839637 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46403 (* 1 = 8.46403 loss)
I0523 00:18:15.915148 34682 sgd_solver.cpp:112] Iteration 18570, lr = 0.01
I0523 00:18:20.733391 34682 solver.cpp:239] Iteration 18580 (2.04351 iter/s, 4.89355s/10 iters), loss = 7.75261
I0523 00:18:20.733583 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75261 (* 1 = 7.75261 loss)
I0523 00:18:21.094151 34682 sgd_solver.cpp:112] Iteration 18580, lr = 0.01
I0523 00:18:25.031872 34682 solver.cpp:239] Iteration 18590 (2.3266 iter/s, 4.29811s/10 iters), loss = 8.13451
I0523 00:18:25.031924 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13451 (* 1 = 8.13451 loss)
I0523 00:18:25.098101 34682 sgd_solver.cpp:112] Iteration 18590, lr = 0.01
I0523 00:18:29.558867 34682 solver.cpp:239] Iteration 18600 (2.20909 iter/s, 4.52675s/10 iters), loss = 8.21807
I0523 00:18:29.558923 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21807 (* 1 = 8.21807 loss)
I0523 00:18:29.627569 34682 sgd_solver.cpp:112] Iteration 18600, lr = 0.01
I0523 00:18:35.760112 34682 solver.cpp:239] Iteration 18610 (1.61266 iter/s, 6.20093s/10 iters), loss = 8.43272
I0523 00:18:35.760170 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43272 (* 1 = 8.43272 loss)
I0523 00:18:36.638036 34682 sgd_solver.cpp:112] Iteration 18610, lr = 0.01
I0523 00:18:41.125865 34682 solver.cpp:239] Iteration 18620 (1.86377 iter/s, 5.36547s/10 iters), loss = 8.69951
I0523 00:18:41.125922 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69951 (* 1 = 8.69951 loss)
I0523 00:18:41.207819 34682 sgd_solver.cpp:112] Iteration 18620, lr = 0.01
I0523 00:18:48.085027 34682 solver.cpp:239] Iteration 18630 (1.43702 iter/s, 6.95883s/10 iters), loss = 8.31841
I0523 00:18:48.085073 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31841 (* 1 = 8.31841 loss)
I0523 00:18:48.160949 34682 sgd_solver.cpp:112] Iteration 18630, lr = 0.01
I0523 00:18:52.293910 34682 solver.cpp:239] Iteration 18640 (2.37605 iter/s, 4.20866s/10 iters), loss = 8.49356
I0523 00:18:52.294056 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49356 (* 1 = 8.49356 loss)
I0523 00:18:52.875795 34682 sgd_solver.cpp:112] Iteration 18640, lr = 0.01
I0523 00:18:57.313184 34682 solver.cpp:239] Iteration 18650 (1.99246 iter/s, 5.01892s/10 iters), loss = 7.99773
I0523 00:18:57.313233 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99773 (* 1 = 7.99773 loss)
I0523 00:18:57.382555 34682 sgd_solver.cpp:112] Iteration 18650, lr = 0.01
I0523 00:19:03.607707 34682 solver.cpp:239] Iteration 18660 (1.58876 iter/s, 6.29421s/10 iters), loss = 8.64222
I0523 00:19:03.607764 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64222 (* 1 = 8.64222 loss)
I0523 00:19:04.463407 34682 sgd_solver.cpp:112] Iteration 18660, lr = 0.01
I0523 00:19:08.869082 34682 solver.cpp:239] Iteration 18670 (1.90075 iter/s, 5.26109s/10 iters), loss = 9.09905
I0523 00:19:08.869143 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09905 (* 1 = 9.09905 loss)
I0523 00:19:09.478333 34682 sgd_solver.cpp:112] Iteration 18670, lr = 0.01
I0523 00:19:13.628423 34682 solver.cpp:239] Iteration 18680 (2.10124 iter/s, 4.75909s/10 iters), loss = 8.48733
I0523 00:19:13.628463 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48733 (* 1 = 8.48733 loss)
I0523 00:19:13.704795 34682 sgd_solver.cpp:112] Iteration 18680, lr = 0.01
I0523 00:19:16.326436 34682 solver.cpp:239] Iteration 18690 (3.70665 iter/s, 2.69785s/10 iters), loss = 8.87044
I0523 00:19:16.326493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87044 (* 1 = 8.87044 loss)
I0523 00:19:17.159456 34682 sgd_solver.cpp:112] Iteration 18690, lr = 0.01
I0523 00:19:20.365532 34682 solver.cpp:239] Iteration 18700 (2.47594 iter/s, 4.03888s/10 iters), loss = 8.34964
I0523 00:19:20.365574 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34964 (* 1 = 8.34964 loss)
I0523 00:19:21.150024 34682 sgd_solver.cpp:112] Iteration 18700, lr = 0.01
I0523 00:19:25.924428 34682 solver.cpp:239] Iteration 18710 (1.79901 iter/s, 5.55863s/10 iters), loss = 8.49784
I0523 00:19:25.924677 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49784 (* 1 = 8.49784 loss)
I0523 00:19:26.406620 34682 sgd_solver.cpp:112] Iteration 18710, lr = 0.01
I0523 00:19:29.415133 34682 solver.cpp:239] Iteration 18720 (2.86505 iter/s, 3.49034s/10 iters), loss = 9.46078
I0523 00:19:29.415176 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.46078 (* 1 = 9.46078 loss)
I0523 00:19:29.485709 34682 sgd_solver.cpp:112] Iteration 18720, lr = 0.01
I0523 00:19:32.753674 34682 solver.cpp:239] Iteration 18730 (2.99549 iter/s, 3.33835s/10 iters), loss = 8.9715
I0523 00:19:32.753736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9715 (* 1 = 8.9715 loss)
I0523 00:19:33.573856 34682 sgd_solver.cpp:112] Iteration 18730, lr = 0.01
I0523 00:19:37.561689 34682 solver.cpp:239] Iteration 18740 (2.07998 iter/s, 4.80775s/10 iters), loss = 8.20909
I0523 00:19:37.561748 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20909 (* 1 = 8.20909 loss)
I0523 00:19:37.627863 34682 sgd_solver.cpp:112] Iteration 18740, lr = 0.01
I0523 00:19:41.427887 34682 solver.cpp:239] Iteration 18750 (2.58666 iter/s, 3.86598s/10 iters), loss = 8.53675
I0523 00:19:41.427935 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53675 (* 1 = 8.53675 loss)
I0523 00:19:41.489776 34682 sgd_solver.cpp:112] Iteration 18750, lr = 0.01
I0523 00:19:47.230882 34682 solver.cpp:239] Iteration 18760 (1.72333 iter/s, 5.80271s/10 iters), loss = 8.60514
I0523 00:19:47.230934 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60514 (* 1 = 8.60514 loss)
I0523 00:19:47.302268 34682 sgd_solver.cpp:112] Iteration 18760, lr = 0.01
I0523 00:19:50.937362 34682 solver.cpp:239] Iteration 18770 (2.69812 iter/s, 3.70628s/10 iters), loss = 8.14804
I0523 00:19:50.937404 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14804 (* 1 = 8.14804 loss)
I0523 00:19:51.767696 34682 sgd_solver.cpp:112] Iteration 18770, lr = 0.01
I0523 00:19:56.780652 34682 solver.cpp:239] Iteration 18780 (1.71145 iter/s, 5.843s/10 iters), loss = 8.86969
I0523 00:19:56.780840 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86969 (* 1 = 8.86969 loss)
I0523 00:19:56.838202 34682 sgd_solver.cpp:112] Iteration 18780, lr = 0.01
I0523 00:20:01.081221 34682 solver.cpp:239] Iteration 18790 (2.32546 iter/s, 4.30022s/10 iters), loss = 8.75406
I0523 00:20:01.081277 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75406 (* 1 = 8.75406 loss)
I0523 00:20:01.875134 34682 sgd_solver.cpp:112] Iteration 18790, lr = 0.01
I0523 00:20:07.297332 34682 solver.cpp:239] Iteration 18800 (1.60881 iter/s, 6.21579s/10 iters), loss = 8.607
I0523 00:20:07.297402 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.607 (* 1 = 8.607 loss)
I0523 00:20:08.013089 34682 sgd_solver.cpp:112] Iteration 18800, lr = 0.01
I0523 00:20:12.048014 34682 solver.cpp:239] Iteration 18810 (2.10508 iter/s, 4.75041s/10 iters), loss = 7.99211
I0523 00:20:12.048076 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99211 (* 1 = 7.99211 loss)
I0523 00:20:12.108757 34682 sgd_solver.cpp:112] Iteration 18810, lr = 0.01
I0523 00:20:15.480530 34682 solver.cpp:239] Iteration 18820 (2.91349 iter/s, 3.43231s/10 iters), loss = 8.52969
I0523 00:20:15.480582 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52969 (* 1 = 8.52969 loss)
I0523 00:20:15.545622 34682 sgd_solver.cpp:112] Iteration 18820, lr = 0.01
I0523 00:20:20.674918 34682 solver.cpp:239] Iteration 18830 (1.92526 iter/s, 5.19411s/10 iters), loss = 7.86911
I0523 00:20:20.674983 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86911 (* 1 = 7.86911 loss)
I0523 00:20:21.443318 34682 sgd_solver.cpp:112] Iteration 18830, lr = 0.01
I0523 00:20:25.520812 34682 solver.cpp:239] Iteration 18840 (2.06372 iter/s, 4.84562s/10 iters), loss = 8.53708
I0523 00:20:25.520859 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53708 (* 1 = 8.53708 loss)
I0523 00:20:25.574777 34682 sgd_solver.cpp:112] Iteration 18840, lr = 0.01
I0523 00:20:29.787561 34682 solver.cpp:239] Iteration 18850 (2.34383 iter/s, 4.26653s/10 iters), loss = 9.69101
I0523 00:20:29.787804 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.69101 (* 1 = 9.69101 loss)
I0523 00:20:29.865315 34682 sgd_solver.cpp:112] Iteration 18850, lr = 0.01
I0523 00:20:35.624524 34682 solver.cpp:239] Iteration 18860 (1.71335 iter/s, 5.83651s/10 iters), loss = 8.65221
I0523 00:20:35.624572 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65221 (* 1 = 8.65221 loss)
I0523 00:20:35.699827 34682 sgd_solver.cpp:112] Iteration 18860, lr = 0.01
I0523 00:20:41.284325 34682 solver.cpp:239] Iteration 18870 (1.76693 iter/s, 5.65952s/10 iters), loss = 8.24729
I0523 00:20:41.284371 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24729 (* 1 = 8.24729 loss)
I0523 00:20:41.358661 34682 sgd_solver.cpp:112] Iteration 18870, lr = 0.01
I0523 00:20:48.129878 34682 solver.cpp:239] Iteration 18880 (1.46087 iter/s, 6.84523s/10 iters), loss = 9.10401
I0523 00:20:48.129926 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10401 (* 1 = 9.10401 loss)
I0523 00:20:48.199065 34682 sgd_solver.cpp:112] Iteration 18880, lr = 0.01
I0523 00:20:52.233144 34682 solver.cpp:239] Iteration 18890 (2.43722 iter/s, 4.10303s/10 iters), loss = 8.99206
I0523 00:20:52.233211 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99206 (* 1 = 8.99206 loss)
I0523 00:20:53.019942 34682 sgd_solver.cpp:112] Iteration 18890, lr = 0.01
I0523 00:20:56.110060 34682 solver.cpp:239] Iteration 18900 (2.57952 iter/s, 3.87669s/10 iters), loss = 9.13505
I0523 00:20:56.110103 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13505 (* 1 = 9.13505 loss)
I0523 00:20:56.178879 34682 sgd_solver.cpp:112] Iteration 18900, lr = 0.01
I0523 00:21:01.572366 34682 solver.cpp:239] Iteration 18910 (1.83082 iter/s, 5.46204s/10 iters), loss = 8.94304
I0523 00:21:01.572599 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94304 (* 1 = 8.94304 loss)
I0523 00:21:02.322116 34682 sgd_solver.cpp:112] Iteration 18910, lr = 0.01
I0523 00:21:05.826277 34682 solver.cpp:239] Iteration 18920 (2.35098 iter/s, 4.25354s/10 iters), loss = 8.58398
I0523 00:21:05.826318 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58398 (* 1 = 8.58398 loss)
I0523 00:21:05.897133 34682 sgd_solver.cpp:112] Iteration 18920, lr = 0.01
I0523 00:21:08.155871 34682 solver.cpp:239] Iteration 18930 (4.29288 iter/s, 2.32944s/10 iters), loss = 8.93954
I0523 00:21:08.155925 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93954 (* 1 = 8.93954 loss)
I0523 00:21:08.213860 34682 sgd_solver.cpp:112] Iteration 18930, lr = 0.01
I0523 00:21:13.618156 34682 solver.cpp:239] Iteration 18940 (1.83083 iter/s, 5.46201s/10 iters), loss = 8.74105
I0523 00:21:13.618199 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74105 (* 1 = 8.74105 loss)
I0523 00:21:13.694838 34682 sgd_solver.cpp:112] Iteration 18940, lr = 0.01
I0523 00:21:19.300571 34682 solver.cpp:239] Iteration 18950 (1.7599 iter/s, 5.68214s/10 iters), loss = 8.88027
I0523 00:21:19.300631 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88027 (* 1 = 8.88027 loss)
I0523 00:21:20.082206 34682 sgd_solver.cpp:112] Iteration 18950, lr = 0.01
I0523 00:21:25.235954 34682 solver.cpp:239] Iteration 18960 (1.6849 iter/s, 5.93508s/10 iters), loss = 8.49521
I0523 00:21:25.236006 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49521 (* 1 = 8.49521 loss)
I0523 00:21:25.740393 34682 sgd_solver.cpp:112] Iteration 18960, lr = 0.01
I0523 00:21:29.911401 34682 solver.cpp:239] Iteration 18970 (2.13895 iter/s, 4.67519s/10 iters), loss = 9.23805
I0523 00:21:29.911447 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23805 (* 1 = 9.23805 loss)
I0523 00:21:29.989109 34682 sgd_solver.cpp:112] Iteration 18970, lr = 0.01
I0523 00:21:35.181702 34682 solver.cpp:239] Iteration 18980 (1.89752 iter/s, 5.27004s/10 iters), loss = 9.11874
I0523 00:21:35.181882 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11874 (* 1 = 9.11874 loss)
I0523 00:21:36.073988 34682 sgd_solver.cpp:112] Iteration 18980, lr = 0.01
I0523 00:21:41.015558 34682 solver.cpp:239] Iteration 18990 (1.71425 iter/s, 5.83344s/10 iters), loss = 8.71943
I0523 00:21:41.015606 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71943 (* 1 = 8.71943 loss)
I0523 00:21:41.093055 34682 sgd_solver.cpp:112] Iteration 18990, lr = 0.01
I0523 00:21:46.827939 34682 solver.cpp:239] Iteration 19000 (1.72055 iter/s, 5.8121s/10 iters), loss = 8.26797
I0523 00:21:46.827986 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26797 (* 1 = 8.26797 loss)
I0523 00:21:46.901139 34682 sgd_solver.cpp:112] Iteration 19000, lr = 0.01
I0523 00:21:51.400171 34682 solver.cpp:239] Iteration 19010 (2.18723 iter/s, 4.572s/10 iters), loss = 8.49256
I0523 00:21:51.400216 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49256 (* 1 = 8.49256 loss)
I0523 00:21:51.469390 34682 sgd_solver.cpp:112] Iteration 19010, lr = 0.01
I0523 00:21:54.629882 34682 solver.cpp:239] Iteration 19020 (3.09643 iter/s, 3.22952s/10 iters), loss = 8.88935
I0523 00:21:54.629930 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88935 (* 1 = 8.88935 loss)
I0523 00:21:54.688083 34682 sgd_solver.cpp:112] Iteration 19020, lr = 0.01
I0523 00:21:58.949507 34682 solver.cpp:239] Iteration 19030 (2.31514 iter/s, 4.3194s/10 iters), loss = 8.57065
I0523 00:21:58.949568 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57065 (* 1 = 8.57065 loss)
I0523 00:21:59.664865 34682 sgd_solver.cpp:112] Iteration 19030, lr = 0.01
I0523 00:22:02.287401 34682 solver.cpp:239] Iteration 19040 (2.99608 iter/s, 3.33769s/10 iters), loss = 8.66985
I0523 00:22:02.287451 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66985 (* 1 = 8.66985 loss)
I0523 00:22:02.360205 34682 sgd_solver.cpp:112] Iteration 19040, lr = 0.01
I0523 00:22:06.122557 34682 solver.cpp:239] Iteration 19050 (2.6076 iter/s, 3.83494s/10 iters), loss = 8.66595
I0523 00:22:06.122673 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66595 (* 1 = 8.66595 loss)
I0523 00:22:06.252457 34682 sgd_solver.cpp:112] Iteration 19050, lr = 0.01
I0523 00:22:11.237893 34682 solver.cpp:239] Iteration 19060 (1.95503 iter/s, 5.11501s/10 iters), loss = 8.30314
I0523 00:22:11.237951 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30314 (* 1 = 8.30314 loss)
I0523 00:22:11.301839 34682 sgd_solver.cpp:112] Iteration 19060, lr = 0.01
I0523 00:22:16.870304 34682 solver.cpp:239] Iteration 19070 (1.77553 iter/s, 5.63212s/10 iters), loss = 8.97609
I0523 00:22:16.870354 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97609 (* 1 = 8.97609 loss)
I0523 00:22:16.939932 34682 sgd_solver.cpp:112] Iteration 19070, lr = 0.01
I0523 00:22:20.287627 34682 solver.cpp:239] Iteration 19080 (2.92643 iter/s, 3.41713s/10 iters), loss = 8.06105
I0523 00:22:20.287673 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06105 (* 1 = 8.06105 loss)
I0523 00:22:20.357880 34682 sgd_solver.cpp:112] Iteration 19080, lr = 0.01
I0523 00:22:24.764032 34682 solver.cpp:239] Iteration 19090 (2.23405 iter/s, 4.47617s/10 iters), loss = 9.02127
I0523 00:22:24.764086 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02127 (* 1 = 9.02127 loss)
I0523 00:22:25.446944 34682 sgd_solver.cpp:112] Iteration 19090, lr = 0.01
I0523 00:22:29.068186 34682 solver.cpp:239] Iteration 19100 (2.32346 iter/s, 4.30392s/10 iters), loss = 8.72921
I0523 00:22:29.068235 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72921 (* 1 = 8.72921 loss)
I0523 00:22:29.141204 34682 sgd_solver.cpp:112] Iteration 19100, lr = 0.01
I0523 00:22:34.343410 34682 solver.cpp:239] Iteration 19110 (1.89575 iter/s, 5.27495s/10 iters), loss = 8.24697
I0523 00:22:34.343458 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24697 (* 1 = 8.24697 loss)
I0523 00:22:34.420716 34682 sgd_solver.cpp:112] Iteration 19110, lr = 0.01
I0523 00:22:39.109547 34682 solver.cpp:239] Iteration 19120 (2.09824 iter/s, 4.76589s/10 iters), loss = 8.136
I0523 00:22:39.109879 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.136 (* 1 = 8.136 loss)
I0523 00:22:39.915799 34682 sgd_solver.cpp:112] Iteration 19120, lr = 0.01
I0523 00:22:45.321270 34682 solver.cpp:239] Iteration 19130 (1.61 iter/s, 6.21117s/10 iters), loss = 8.3133
I0523 00:22:45.321321 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3133 (* 1 = 8.3133 loss)
I0523 00:22:45.388478 34682 sgd_solver.cpp:112] Iteration 19130, lr = 0.01
I0523 00:22:51.567906 34682 solver.cpp:239] Iteration 19140 (1.60094 iter/s, 6.24633s/10 iters), loss = 9.78221
I0523 00:22:51.567965 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.78221 (* 1 = 9.78221 loss)
I0523 00:22:51.636175 34682 sgd_solver.cpp:112] Iteration 19140, lr = 0.01
I0523 00:22:57.411978 34682 solver.cpp:239] Iteration 19150 (1.71122 iter/s, 5.84378s/10 iters), loss = 8.50679
I0523 00:22:57.412027 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50679 (* 1 = 8.50679 loss)
I0523 00:22:58.242949 34682 sgd_solver.cpp:112] Iteration 19150, lr = 0.01
I0523 00:23:02.382567 34682 solver.cpp:239] Iteration 19160 (2.01194 iter/s, 4.97033s/10 iters), loss = 9.1932
I0523 00:23:02.382616 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1932 (* 1 = 9.1932 loss)
I0523 00:23:02.439723 34682 sgd_solver.cpp:112] Iteration 19160, lr = 0.01
I0523 00:23:05.498426 34682 solver.cpp:239] Iteration 19170 (3.20958 iter/s, 3.11567s/10 iters), loss = 8.20875
I0523 00:23:05.498483 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20875 (* 1 = 8.20875 loss)
I0523 00:23:06.382490 34682 sgd_solver.cpp:112] Iteration 19170, lr = 0.01
I0523 00:23:10.972795 34682 solver.cpp:239] Iteration 19180 (1.82679 iter/s, 5.47409s/10 iters), loss = 8.93138
I0523 00:23:10.973029 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93138 (* 1 = 8.93138 loss)
I0523 00:23:11.047879 34682 sgd_solver.cpp:112] Iteration 19180, lr = 0.01
I0523 00:23:16.993355 34682 solver.cpp:239] Iteration 19190 (1.6611 iter/s, 6.02012s/10 iters), loss = 8.11927
I0523 00:23:16.993394 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11927 (* 1 = 8.11927 loss)
I0523 00:23:17.067559 34682 sgd_solver.cpp:112] Iteration 19190, lr = 0.01
I0523 00:23:22.166786 34682 solver.cpp:239] Iteration 19200 (1.93305 iter/s, 5.17318s/10 iters), loss = 8.19683
I0523 00:23:22.166836 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19683 (* 1 = 8.19683 loss)
I0523 00:23:22.241396 34682 sgd_solver.cpp:112] Iteration 19200, lr = 0.01
I0523 00:23:28.046108 34682 solver.cpp:239] Iteration 19210 (1.70096 iter/s, 5.87903s/10 iters), loss = 8.85317
I0523 00:23:28.046165 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85317 (* 1 = 8.85317 loss)
I0523 00:23:28.118037 34682 sgd_solver.cpp:112] Iteration 19210, lr = 0.01
I0523 00:23:32.199676 34682 solver.cpp:239] Iteration 19220 (2.4077 iter/s, 4.15335s/10 iters), loss = 8.55472
I0523 00:23:32.199728 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55472 (* 1 = 8.55472 loss)
I0523 00:23:33.072284 34682 sgd_solver.cpp:112] Iteration 19220, lr = 0.01
I0523 00:23:36.528266 34682 solver.cpp:239] Iteration 19230 (2.31035 iter/s, 4.32835s/10 iters), loss = 8.59532
I0523 00:23:36.528316 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59532 (* 1 = 8.59532 loss)
I0523 00:23:36.585674 34682 sgd_solver.cpp:112] Iteration 19230, lr = 0.01
I0523 00:23:43.093165 34682 solver.cpp:239] Iteration 19240 (1.52386 iter/s, 6.56227s/10 iters), loss = 8.86165
I0523 00:23:43.093431 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86165 (* 1 = 8.86165 loss)
I0523 00:23:43.165390 34682 sgd_solver.cpp:112] Iteration 19240, lr = 0.01
I0523 00:23:47.447701 34682 solver.cpp:239] Iteration 19250 (2.29668 iter/s, 4.35412s/10 iters), loss = 9.81784
I0523 00:23:47.447754 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.81784 (* 1 = 9.81784 loss)
I0523 00:23:48.189038 34682 sgd_solver.cpp:112] Iteration 19250, lr = 0.01
I0523 00:23:53.195184 34682 solver.cpp:239] Iteration 19260 (1.73998 iter/s, 5.74719s/10 iters), loss = 9.11302
I0523 00:23:53.195241 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11302 (* 1 = 9.11302 loss)
I0523 00:23:53.258787 34682 sgd_solver.cpp:112] Iteration 19260, lr = 0.01
I0523 00:23:59.896364 34682 solver.cpp:239] Iteration 19270 (1.49235 iter/s, 6.70084s/10 iters), loss = 9.5064
I0523 00:23:59.896422 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.5064 (* 1 = 9.5064 loss)
I0523 00:24:00.715768 34682 sgd_solver.cpp:112] Iteration 19270, lr = 0.01
I0523 00:24:05.783262 34682 solver.cpp:239] Iteration 19280 (1.69877 iter/s, 5.8866s/10 iters), loss = 8.38707
I0523 00:24:05.783306 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38707 (* 1 = 8.38707 loss)
I0523 00:24:06.627720 34682 sgd_solver.cpp:112] Iteration 19280, lr = 0.01
I0523 00:24:13.300031 34682 solver.cpp:239] Iteration 19290 (1.33042 iter/s, 7.51641s/10 iters), loss = 8.78065
I0523 00:24:13.300251 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78065 (* 1 = 8.78065 loss)
I0523 00:24:14.183765 34682 sgd_solver.cpp:112] Iteration 19290, lr = 0.01
I0523 00:24:18.463599 34682 solver.cpp:239] Iteration 19300 (1.9368 iter/s, 5.16315s/10 iters), loss = 8.80985
I0523 00:24:18.463641 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80985 (* 1 = 8.80985 loss)
I0523 00:24:18.533002 34682 sgd_solver.cpp:112] Iteration 19300, lr = 0.01
I0523 00:24:21.996520 34682 solver.cpp:239] Iteration 19310 (2.83068 iter/s, 3.53272s/10 iters), loss = 8.72228
I0523 00:24:21.996580 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72228 (* 1 = 8.72228 loss)
I0523 00:24:22.817065 34682 sgd_solver.cpp:112] Iteration 19310, lr = 0.01
I0523 00:24:28.555400 34682 solver.cpp:239] Iteration 19320 (1.52473 iter/s, 6.55855s/10 iters), loss = 8.09551
I0523 00:24:28.555464 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09551 (* 1 = 8.09551 loss)
I0523 00:24:28.613781 34682 sgd_solver.cpp:112] Iteration 19320, lr = 0.01
I0523 00:24:32.233966 34682 solver.cpp:239] Iteration 19330 (2.71862 iter/s, 3.67834s/10 iters), loss = 8.8152
I0523 00:24:32.234019 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8152 (* 1 = 8.8152 loss)
I0523 00:24:33.028182 34682 sgd_solver.cpp:112] Iteration 19330, lr = 0.01
I0523 00:24:37.862275 34682 solver.cpp:239] Iteration 19340 (1.77683 iter/s, 5.628s/10 iters), loss = 8.81737
I0523 00:24:37.862327 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81737 (* 1 = 8.81737 loss)
I0523 00:24:38.555112 34682 sgd_solver.cpp:112] Iteration 19340, lr = 0.01
I0523 00:24:44.026883 34682 solver.cpp:239] Iteration 19350 (1.62224 iter/s, 6.1643s/10 iters), loss = 8.45738
I0523 00:24:44.027117 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45738 (* 1 = 8.45738 loss)
I0523 00:24:44.089617 34682 sgd_solver.cpp:112] Iteration 19350, lr = 0.01
I0523 00:24:49.257452 34682 solver.cpp:239] Iteration 19360 (1.91285 iter/s, 5.22779s/10 iters), loss = 9.64982
I0523 00:24:49.257531 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.64982 (* 1 = 9.64982 loss)
I0523 00:24:50.060856 34682 sgd_solver.cpp:112] Iteration 19360, lr = 0.01
I0523 00:24:54.205765 34682 solver.cpp:239] Iteration 19370 (2.021 iter/s, 4.94804s/10 iters), loss = 9.11933
I0523 00:24:54.205809 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11933 (* 1 = 9.11933 loss)
I0523 00:24:54.274466 34682 sgd_solver.cpp:112] Iteration 19370, lr = 0.01
I0523 00:24:58.433972 34682 solver.cpp:239] Iteration 19380 (2.3652 iter/s, 4.22797s/10 iters), loss = 8.71185
I0523 00:24:58.434046 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71185 (* 1 = 8.71185 loss)
I0523 00:24:58.489802 34682 sgd_solver.cpp:112] Iteration 19380, lr = 0.01
I0523 00:25:04.567267 34682 solver.cpp:239] Iteration 19390 (1.63053 iter/s, 6.13298s/10 iters), loss = 8.32396
I0523 00:25:04.567337 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32396 (* 1 = 8.32396 loss)
I0523 00:25:04.638507 34682 sgd_solver.cpp:112] Iteration 19390, lr = 0.01
I0523 00:25:10.868048 34682 solver.cpp:239] Iteration 19400 (1.58719 iter/s, 6.30045s/10 iters), loss = 9.05674
I0523 00:25:10.868109 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05674 (* 1 = 9.05674 loss)
I0523 00:25:11.483583 34682 sgd_solver.cpp:112] Iteration 19400, lr = 0.01
I0523 00:25:14.672377 34682 solver.cpp:239] Iteration 19410 (2.62874 iter/s, 3.8041s/10 iters), loss = 9.00973
I0523 00:25:14.672669 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00973 (* 1 = 9.00973 loss)
I0523 00:25:15.482837 34682 sgd_solver.cpp:112] Iteration 19410, lr = 0.01
I0523 00:25:19.735538 34682 solver.cpp:239] Iteration 19420 (1.97523 iter/s, 5.06269s/10 iters), loss = 8.83284
I0523 00:25:19.735605 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83284 (* 1 = 8.83284 loss)
I0523 00:25:20.608186 34682 sgd_solver.cpp:112] Iteration 19420, lr = 0.01
I0523 00:25:24.643909 34682 solver.cpp:239] Iteration 19430 (2.03745 iter/s, 4.90811s/10 iters), loss = 9.22291
I0523 00:25:24.643961 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22291 (* 1 = 9.22291 loss)
I0523 00:25:24.765832 34682 sgd_solver.cpp:112] Iteration 19430, lr = 0.01
I0523 00:25:28.218580 34682 solver.cpp:239] Iteration 19440 (2.79763 iter/s, 3.57445s/10 iters), loss = 8.83864
I0523 00:25:28.218639 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83864 (* 1 = 8.83864 loss)
I0523 00:25:29.033994 34682 sgd_solver.cpp:112] Iteration 19440, lr = 0.01
I0523 00:25:33.359436 34682 solver.cpp:239] Iteration 19450 (1.9453 iter/s, 5.14059s/10 iters), loss = 8.48086
I0523 00:25:33.359491 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48086 (* 1 = 8.48086 loss)
I0523 00:25:34.212733 34682 sgd_solver.cpp:112] Iteration 19450, lr = 0.01
I0523 00:25:38.127205 34682 solver.cpp:239] Iteration 19460 (2.09753 iter/s, 4.76752s/10 iters), loss = 8.90835
I0523 00:25:38.127259 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90835 (* 1 = 8.90835 loss)
I0523 00:25:38.916692 34682 sgd_solver.cpp:112] Iteration 19460, lr = 0.01
I0523 00:25:42.118615 34682 solver.cpp:239] Iteration 19470 (2.50552 iter/s, 3.99119s/10 iters), loss = 8.40043
I0523 00:25:42.118670 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40043 (* 1 = 8.40043 loss)
I0523 00:25:42.660117 34682 sgd_solver.cpp:112] Iteration 19470, lr = 0.01
I0523 00:25:47.533556 34682 solver.cpp:239] Iteration 19480 (1.84685 iter/s, 5.41464s/10 iters), loss = 8.78602
I0523 00:25:47.533713 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78602 (* 1 = 8.78602 loss)
I0523 00:25:48.022753 34682 sgd_solver.cpp:112] Iteration 19480, lr = 0.01
I0523 00:25:52.964282 34682 solver.cpp:239] Iteration 19490 (1.8415 iter/s, 5.43035s/10 iters), loss = 9.42772
I0523 00:25:52.964331 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42772 (* 1 = 9.42772 loss)
I0523 00:25:53.852586 34682 sgd_solver.cpp:112] Iteration 19490, lr = 0.01
I0523 00:25:58.832504 34682 solver.cpp:239] Iteration 19500 (1.70418 iter/s, 5.86793s/10 iters), loss = 8.32972
I0523 00:25:58.832566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32972 (* 1 = 8.32972 loss)
I0523 00:25:58.899379 34682 sgd_solver.cpp:112] Iteration 19500, lr = 0.01
I0523 00:26:04.228564 34682 solver.cpp:239] Iteration 19510 (1.8533 iter/s, 5.39578s/10 iters), loss = 8.40365
I0523 00:26:04.228617 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40365 (* 1 = 8.40365 loss)
I0523 00:26:04.304791 34682 sgd_solver.cpp:112] Iteration 19510, lr = 0.01
I0523 00:26:08.395179 34682 solver.cpp:239] Iteration 19520 (2.40016 iter/s, 4.16639s/10 iters), loss = 9.58058
I0523 00:26:08.395232 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.58058 (* 1 = 9.58058 loss)
I0523 00:26:08.455001 34682 sgd_solver.cpp:112] Iteration 19520, lr = 0.01
I0523 00:26:13.525802 34682 solver.cpp:239] Iteration 19530 (1.94918 iter/s, 5.13036s/10 iters), loss = 8.55018
I0523 00:26:13.525856 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55018 (* 1 = 8.55018 loss)
I0523 00:26:14.335917 34682 sgd_solver.cpp:112] Iteration 19530, lr = 0.01
I0523 00:26:17.680765 34682 solver.cpp:239] Iteration 19540 (2.40689 iter/s, 4.15475s/10 iters), loss = 8.75607
I0523 00:26:17.680955 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75607 (* 1 = 8.75607 loss)
I0523 00:26:17.744352 34682 sgd_solver.cpp:112] Iteration 19540, lr = 0.01
I0523 00:26:21.284687 34682 solver.cpp:239] Iteration 19550 (2.77502 iter/s, 3.60358s/10 iters), loss = 8.36034
I0523 00:26:21.284747 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36034 (* 1 = 8.36034 loss)
I0523 00:26:21.344377 34682 sgd_solver.cpp:112] Iteration 19550, lr = 0.01
I0523 00:26:26.917130 34682 solver.cpp:239] Iteration 19560 (1.77552 iter/s, 5.63215s/10 iters), loss = 8.75059
I0523 00:26:26.917197 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75059 (* 1 = 8.75059 loss)
I0523 00:26:26.994164 34682 sgd_solver.cpp:112] Iteration 19560, lr = 0.01
I0523 00:26:32.489634 34682 solver.cpp:239] Iteration 19570 (1.79462 iter/s, 5.5722s/10 iters), loss = 8.56753
I0523 00:26:32.489701 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56753 (* 1 = 8.56753 loss)
I0523 00:26:33.075819 34682 sgd_solver.cpp:112] Iteration 19570, lr = 0.01
I0523 00:26:37.405050 34682 solver.cpp:239] Iteration 19580 (2.03452 iter/s, 4.91515s/10 iters), loss = 9.24766
I0523 00:26:37.405100 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24766 (* 1 = 9.24766 loss)
I0523 00:26:37.485049 34682 sgd_solver.cpp:112] Iteration 19580, lr = 0.01
I0523 00:26:43.251487 34682 solver.cpp:239] Iteration 19590 (1.71053 iter/s, 5.84615s/10 iters), loss = 8.83603
I0523 00:26:43.251545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83603 (* 1 = 8.83603 loss)
I0523 00:26:44.113975 34682 sgd_solver.cpp:112] Iteration 19590, lr = 0.01
I0523 00:26:48.116703 34682 solver.cpp:239] Iteration 19600 (2.05552 iter/s, 4.86496s/10 iters), loss = 8.96671
I0523 00:26:48.116906 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96671 (* 1 = 8.96671 loss)
I0523 00:26:48.185992 34682 sgd_solver.cpp:112] Iteration 19600, lr = 0.01
I0523 00:26:52.329061 34682 solver.cpp:239] Iteration 19610 (2.37417 iter/s, 4.212s/10 iters), loss = 8.88929
I0523 00:26:52.329107 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88929 (* 1 = 8.88929 loss)
I0523 00:26:52.393276 34682 sgd_solver.cpp:112] Iteration 19610, lr = 0.01
I0523 00:26:57.237839 34682 solver.cpp:239] Iteration 19620 (2.03727 iter/s, 4.90852s/10 iters), loss = 9.14264
I0523 00:26:57.237888 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14264 (* 1 = 9.14264 loss)
I0523 00:26:57.305505 34682 sgd_solver.cpp:112] Iteration 19620, lr = 0.01
I0523 00:27:03.013435 34682 solver.cpp:239] Iteration 19630 (1.73151 iter/s, 5.77531s/10 iters), loss = 8.78392
I0523 00:27:03.013490 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78392 (* 1 = 8.78392 loss)
I0523 00:27:03.830188 34682 sgd_solver.cpp:112] Iteration 19630, lr = 0.01
I0523 00:27:09.177670 34682 solver.cpp:239] Iteration 19640 (1.62234 iter/s, 6.16393s/10 iters), loss = 8.40715
I0523 00:27:09.177727 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40715 (* 1 = 8.40715 loss)
I0523 00:27:09.235219 34682 sgd_solver.cpp:112] Iteration 19640, lr = 0.01
I0523 00:27:13.293025 34682 solver.cpp:239] Iteration 19650 (2.43006 iter/s, 4.11513s/10 iters), loss = 8.64592
I0523 00:27:13.293066 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64592 (* 1 = 8.64592 loss)
I0523 00:27:13.369720 34682 sgd_solver.cpp:112] Iteration 19650, lr = 0.01
I0523 00:27:18.082782 34682 solver.cpp:239] Iteration 19660 (2.0879 iter/s, 4.78951s/10 iters), loss = 8.6416
I0523 00:27:18.082840 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6416 (* 1 = 8.6416 loss)
I0523 00:27:18.955071 34682 sgd_solver.cpp:112] Iteration 19660, lr = 0.01
I0523 00:27:22.042994 34682 solver.cpp:239] Iteration 19670 (2.52526 iter/s, 3.95998s/10 iters), loss = 8.76509
I0523 00:27:22.043057 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76509 (* 1 = 8.76509 loss)
I0523 00:27:22.103042 34682 sgd_solver.cpp:112] Iteration 19670, lr = 0.01
I0523 00:27:27.884245 34682 solver.cpp:239] Iteration 19680 (1.71205 iter/s, 5.84095s/10 iters), loss = 9.11645
I0523 00:27:27.884289 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11645 (* 1 = 9.11645 loss)
I0523 00:27:27.965416 34682 sgd_solver.cpp:112] Iteration 19680, lr = 0.01
I0523 00:27:31.971158 34682 solver.cpp:239] Iteration 19690 (2.44697 iter/s, 4.08669s/10 iters), loss = 9.05967
I0523 00:27:31.971212 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05967 (* 1 = 9.05967 loss)
I0523 00:27:32.047566 34682 sgd_solver.cpp:112] Iteration 19690, lr = 0.01
I0523 00:27:36.924921 34682 solver.cpp:239] Iteration 19700 (2.01877 iter/s, 4.9535s/10 iters), loss = 9.12717
I0523 00:27:36.924969 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12717 (* 1 = 9.12717 loss)
I0523 00:27:36.988373 34682 sgd_solver.cpp:112] Iteration 19700, lr = 0.01
I0523 00:27:42.156141 34682 solver.cpp:239] Iteration 19710 (1.9117 iter/s, 5.23096s/10 iters), loss = 8.25432
I0523 00:27:42.156195 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25432 (* 1 = 8.25432 loss)
I0523 00:27:42.993340 34682 sgd_solver.cpp:112] Iteration 19710, lr = 0.01
I0523 00:27:46.436606 34682 solver.cpp:239] Iteration 19720 (2.33632 iter/s, 4.28024s/10 iters), loss = 7.6704
I0523 00:27:46.436655 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6704 (* 1 = 7.6704 loss)
I0523 00:27:46.965071 34682 sgd_solver.cpp:112] Iteration 19720, lr = 0.01
I0523 00:27:51.572860 34682 solver.cpp:239] Iteration 19730 (1.94704 iter/s, 5.13599s/10 iters), loss = 8.59966
I0523 00:27:51.573070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59966 (* 1 = 8.59966 loss)
I0523 00:27:51.647490 34682 sgd_solver.cpp:112] Iteration 19730, lr = 0.01
I0523 00:27:57.332329 34682 solver.cpp:239] Iteration 19740 (1.7364 iter/s, 5.75905s/10 iters), loss = 7.97709
I0523 00:27:57.332376 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97709 (* 1 = 7.97709 loss)
I0523 00:27:58.179486 34682 sgd_solver.cpp:112] Iteration 19740, lr = 0.01
I0523 00:28:01.933091 34682 solver.cpp:239] Iteration 19750 (2.17367 iter/s, 4.60051s/10 iters), loss = 8.37317
I0523 00:28:01.933169 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37317 (* 1 = 8.37317 loss)
I0523 00:28:02.359510 34682 sgd_solver.cpp:112] Iteration 19750, lr = 0.01
I0523 00:28:05.769258 34682 solver.cpp:239] Iteration 19760 (2.60693 iter/s, 3.83592s/10 iters), loss = 9.36657
I0523 00:28:05.769325 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36657 (* 1 = 9.36657 loss)
I0523 00:28:06.631747 34682 sgd_solver.cpp:112] Iteration 19760, lr = 0.01
I0523 00:28:12.118192 34682 solver.cpp:239] Iteration 19770 (1.57515 iter/s, 6.3486s/10 iters), loss = 8.35835
I0523 00:28:12.118252 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35835 (* 1 = 8.35835 loss)
I0523 00:28:12.178311 34682 sgd_solver.cpp:112] Iteration 19770, lr = 0.01
I0523 00:28:17.461953 34682 solver.cpp:239] Iteration 19780 (1.87144 iter/s, 5.34348s/10 iters), loss = 8.45253
I0523 00:28:17.462021 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45253 (* 1 = 8.45253 loss)
I0523 00:28:18.123558 34682 sgd_solver.cpp:112] Iteration 19780, lr = 0.01
I0523 00:28:24.514536 34682 solver.cpp:239] Iteration 19790 (1.41799 iter/s, 7.05221s/10 iters), loss = 9.12745
I0523 00:28:24.514748 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12745 (* 1 = 9.12745 loss)
I0523 00:28:25.402359 34682 sgd_solver.cpp:112] Iteration 19790, lr = 0.01
I0523 00:28:30.888733 34682 solver.cpp:239] Iteration 19800 (1.56894 iter/s, 6.37373s/10 iters), loss = 8.96158
I0523 00:28:30.888801 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96158 (* 1 = 8.96158 loss)
I0523 00:28:31.458585 34682 sgd_solver.cpp:112] Iteration 19800, lr = 0.01
I0523 00:28:35.511593 34682 solver.cpp:239] Iteration 19810 (2.16328 iter/s, 4.62261s/10 iters), loss = 8.50173
I0523 00:28:35.511636 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50173 (* 1 = 8.50173 loss)
I0523 00:28:36.274837 34682 sgd_solver.cpp:112] Iteration 19810, lr = 0.01
I0523 00:28:40.367400 34682 solver.cpp:239] Iteration 19820 (2.05949 iter/s, 4.85556s/10 iters), loss = 8.63423
I0523 00:28:40.367460 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63423 (* 1 = 8.63423 loss)
I0523 00:28:41.239783 34682 sgd_solver.cpp:112] Iteration 19820, lr = 0.01
I0523 00:28:45.307878 34682 solver.cpp:239] Iteration 19830 (2.0242 iter/s, 4.94022s/10 iters), loss = 9.08131
I0523 00:28:45.307927 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08131 (* 1 = 9.08131 loss)
I0523 00:28:46.135169 34682 sgd_solver.cpp:112] Iteration 19830, lr = 0.01
I0523 00:28:50.006296 34682 solver.cpp:239] Iteration 19840 (2.12849 iter/s, 4.69817s/10 iters), loss = 8.93018
I0523 00:28:50.006345 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93018 (* 1 = 8.93018 loss)
I0523 00:28:50.088927 34682 sgd_solver.cpp:112] Iteration 19840, lr = 0.01
I0523 00:28:55.753723 34682 solver.cpp:239] Iteration 19850 (1.74 iter/s, 5.74711s/10 iters), loss = 9.65449
I0523 00:28:55.753962 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.65449 (* 1 = 9.65449 loss)
I0523 00:28:56.557744 34682 sgd_solver.cpp:112] Iteration 19850, lr = 0.01
I0523 00:29:01.096664 34682 solver.cpp:239] Iteration 19860 (1.87178 iter/s, 5.34251s/10 iters), loss = 8.37233
I0523 00:29:01.096727 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37233 (* 1 = 8.37233 loss)
I0523 00:29:01.918869 34682 sgd_solver.cpp:112] Iteration 19860, lr = 0.01
I0523 00:29:06.015621 34682 solver.cpp:239] Iteration 19870 (2.03308 iter/s, 4.91865s/10 iters), loss = 9.13332
I0523 00:29:06.015658 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13332 (* 1 = 9.13332 loss)
I0523 00:29:06.091696 34682 sgd_solver.cpp:112] Iteration 19870, lr = 0.01
I0523 00:29:09.712517 34682 solver.cpp:239] Iteration 19880 (2.70512 iter/s, 3.69669s/10 iters), loss = 8.74775
I0523 00:29:09.712579 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74775 (* 1 = 8.74775 loss)
I0523 00:29:09.770097 34682 sgd_solver.cpp:112] Iteration 19880, lr = 0.01
I0523 00:29:13.000447 34682 solver.cpp:239] Iteration 19890 (3.04161 iter/s, 3.28773s/10 iters), loss = 8.97039
I0523 00:29:13.000500 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97039 (* 1 = 8.97039 loss)
I0523 00:29:13.609278 34682 sgd_solver.cpp:112] Iteration 19890, lr = 0.01
I0523 00:29:18.505419 34682 solver.cpp:239] Iteration 19900 (1.81663 iter/s, 5.50469s/10 iters), loss = 8.99641
I0523 00:29:18.505492 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99641 (* 1 = 8.99641 loss)
I0523 00:29:19.349192 34682 sgd_solver.cpp:112] Iteration 19900, lr = 0.01
I0523 00:29:24.341914 34682 solver.cpp:239] Iteration 19910 (1.71345 iter/s, 5.83618s/10 iters), loss = 8.71196
I0523 00:29:24.341992 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71196 (* 1 = 8.71196 loss)
I0523 00:29:24.795195 34682 sgd_solver.cpp:112] Iteration 19910, lr = 0.01
I0523 00:29:28.892670 34682 solver.cpp:239] Iteration 19920 (2.19756 iter/s, 4.5505s/10 iters), loss = 8.38938
I0523 00:29:28.892807 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38938 (* 1 = 8.38938 loss)
I0523 00:29:29.066666 34682 sgd_solver.cpp:112] Iteration 19920, lr = 0.01
I0523 00:29:32.417763 34682 solver.cpp:239] Iteration 19930 (2.83703 iter/s, 3.52481s/10 iters), loss = 8.26785
I0523 00:29:32.417816 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26785 (* 1 = 8.26785 loss)
I0523 00:29:33.221942 34682 sgd_solver.cpp:112] Iteration 19930, lr = 0.01
I0523 00:29:35.970749 34682 solver.cpp:239] Iteration 19940 (2.8147 iter/s, 3.55278s/10 iters), loss = 8.97472
I0523 00:29:35.970803 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97472 (* 1 = 8.97472 loss)
I0523 00:29:36.768707 34682 sgd_solver.cpp:112] Iteration 19940, lr = 0.01
I0523 00:29:42.607125 34682 solver.cpp:239] Iteration 19950 (1.50692 iter/s, 6.63605s/10 iters), loss = 8.22555
I0523 00:29:42.607170 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22555 (* 1 = 8.22555 loss)
I0523 00:29:42.663256 34682 sgd_solver.cpp:112] Iteration 19950, lr = 0.01
I0523 00:29:48.458012 34682 solver.cpp:239] Iteration 19960 (1.70923 iter/s, 5.85059s/10 iters), loss = 8.44773
I0523 00:29:48.458072 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44773 (* 1 = 8.44773 loss)
I0523 00:29:48.535125 34682 sgd_solver.cpp:112] Iteration 19960, lr = 0.01
I0523 00:29:52.174831 34682 solver.cpp:239] Iteration 19970 (2.69063 iter/s, 3.7166s/10 iters), loss = 8.54773
I0523 00:29:52.174876 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54773 (* 1 = 8.54773 loss)
I0523 00:29:52.253715 34682 sgd_solver.cpp:112] Iteration 19970, lr = 0.01
I0523 00:29:58.731989 34682 solver.cpp:239] Iteration 19980 (1.52512 iter/s, 6.55684s/10 iters), loss = 8.58924
I0523 00:29:58.732056 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58924 (* 1 = 8.58924 loss)
I0523 00:29:59.562578 34682 sgd_solver.cpp:112] Iteration 19980, lr = 0.01
I0523 00:30:03.144368 34682 solver.cpp:239] Iteration 19990 (2.26648 iter/s, 4.41213s/10 iters), loss = 8.14763
I0523 00:30:03.144428 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14763 (* 1 = 8.14763 loss)
I0523 00:30:03.886564 34682 sgd_solver.cpp:112] Iteration 19990, lr = 0.01
I0523 00:30:10.074367 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_20000.caffemodel
I0523 00:30:11.172549 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_20000.solverstate
I0523 00:30:11.378204 34682 solver.cpp:239] Iteration 20000 (1.21456 iter/s, 8.23346s/10 iters), loss = 8.48017
I0523 00:30:11.378247 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48017 (* 1 = 8.48017 loss)
I0523 00:30:11.451638 34682 sgd_solver.cpp:112] Iteration 20000, lr = 0.01
I0523 00:30:14.510234 34682 solver.cpp:239] Iteration 20010 (3.19299 iter/s, 3.13186s/10 iters), loss = 8.62172
I0523 00:30:14.510273 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62172 (* 1 = 8.62172 loss)
I0523 00:30:14.590204 34682 sgd_solver.cpp:112] Iteration 20010, lr = 0.01
I0523 00:30:21.139174 34682 solver.cpp:239] Iteration 20020 (1.50861 iter/s, 6.62862s/10 iters), loss = 9.52061
I0523 00:30:21.139241 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.52061 (* 1 = 9.52061 loss)
I0523 00:30:21.848803 34682 sgd_solver.cpp:112] Iteration 20020, lr = 0.01
I0523 00:30:26.940279 34682 solver.cpp:239] Iteration 20030 (1.7239 iter/s, 5.80081s/10 iters), loss = 9.19818
I0523 00:30:26.940328 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19818 (* 1 = 9.19818 loss)
I0523 00:30:27.761806 34682 sgd_solver.cpp:112] Iteration 20030, lr = 0.01
I0523 00:30:32.103998 34682 solver.cpp:239] Iteration 20040 (1.93669 iter/s, 5.16346s/10 iters), loss = 8.56167
I0523 00:30:32.104188 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56167 (* 1 = 8.56167 loss)
I0523 00:30:32.180876 34682 sgd_solver.cpp:112] Iteration 20040, lr = 0.01
I0523 00:30:37.920658 34682 solver.cpp:239] Iteration 20050 (1.71933 iter/s, 5.81623s/10 iters), loss = 8.52151
I0523 00:30:37.920737 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52151 (* 1 = 8.52151 loss)
I0523 00:30:37.995890 34682 sgd_solver.cpp:112] Iteration 20050, lr = 0.01
I0523 00:30:44.329069 34682 solver.cpp:239] Iteration 20060 (1.56053 iter/s, 6.40807s/10 iters), loss = 8.86037
I0523 00:30:44.329138 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86037 (* 1 = 8.86037 loss)
I0523 00:30:44.392110 34682 sgd_solver.cpp:112] Iteration 20060, lr = 0.01
I0523 00:30:49.896229 34682 solver.cpp:239] Iteration 20070 (1.79634 iter/s, 5.56687s/10 iters), loss = 8.51499
I0523 00:30:49.896278 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51499 (* 1 = 8.51499 loss)
I0523 00:30:50.740080 34682 sgd_solver.cpp:112] Iteration 20070, lr = 0.01
I0523 00:30:54.590132 34682 solver.cpp:239] Iteration 20080 (2.13053 iter/s, 4.69367s/10 iters), loss = 8.79837
I0523 00:30:54.590167 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79837 (* 1 = 8.79837 loss)
I0523 00:30:54.665060 34682 sgd_solver.cpp:112] Iteration 20080, lr = 0.01
I0523 00:31:00.246209 34682 solver.cpp:239] Iteration 20090 (1.7681 iter/s, 5.6558s/10 iters), loss = 9.04406
I0523 00:31:00.246284 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04406 (* 1 = 9.04406 loss)
I0523 00:31:01.017434 34682 sgd_solver.cpp:112] Iteration 20090, lr = 0.01
I0523 00:31:05.225826 34682 solver.cpp:239] Iteration 20100 (2.0083 iter/s, 4.97934s/10 iters), loss = 8.72389
I0523 00:31:05.226018 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72389 (* 1 = 8.72389 loss)
I0523 00:31:05.843194 34682 sgd_solver.cpp:112] Iteration 20100, lr = 0.01
I0523 00:31:08.391307 34682 solver.cpp:239] Iteration 20110 (3.15941 iter/s, 3.16515s/10 iters), loss = 9.65666
I0523 00:31:08.391362 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.65666 (* 1 = 9.65666 loss)
I0523 00:31:08.459992 34682 sgd_solver.cpp:112] Iteration 20110, lr = 0.01
I0523 00:31:12.683182 34682 solver.cpp:239] Iteration 20120 (2.33011 iter/s, 4.29165s/10 iters), loss = 8.46479
I0523 00:31:12.683221 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46479 (* 1 = 8.46479 loss)
I0523 00:31:12.755226 34682 sgd_solver.cpp:112] Iteration 20120, lr = 0.01
I0523 00:31:16.789811 34682 solver.cpp:239] Iteration 20130 (2.43522 iter/s, 4.10641s/10 iters), loss = 7.84791
I0523 00:31:16.789873 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84791 (* 1 = 7.84791 loss)
I0523 00:31:16.869441 34682 sgd_solver.cpp:112] Iteration 20130, lr = 0.01
I0523 00:31:22.603852 34682 solver.cpp:239] Iteration 20140 (1.72006 iter/s, 5.81375s/10 iters), loss = 9.47998
I0523 00:31:22.603901 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47998 (* 1 = 9.47998 loss)
I0523 00:31:22.683674 34682 sgd_solver.cpp:112] Iteration 20140, lr = 0.01
I0523 00:31:25.359465 34682 solver.cpp:239] Iteration 20150 (3.62918 iter/s, 2.75544s/10 iters), loss = 8.6529
I0523 00:31:25.359519 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6529 (* 1 = 8.6529 loss)
I0523 00:31:26.050354 34682 sgd_solver.cpp:112] Iteration 20150, lr = 0.01
I0523 00:31:30.585189 34682 solver.cpp:239] Iteration 20160 (1.91371 iter/s, 5.22545s/10 iters), loss = 8.51592
I0523 00:31:30.585253 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51592 (* 1 = 8.51592 loss)
I0523 00:31:31.483700 34682 sgd_solver.cpp:112] Iteration 20160, lr = 0.01
I0523 00:31:35.505578 34682 solver.cpp:239] Iteration 20170 (2.03247 iter/s, 4.92013s/10 iters), loss = 9.43595
I0523 00:31:35.505810 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43595 (* 1 = 9.43595 loss)
I0523 00:31:35.579210 34682 sgd_solver.cpp:112] Iteration 20170, lr = 0.01
I0523 00:31:40.489425 34682 solver.cpp:239] Iteration 20180 (2.00665 iter/s, 4.98344s/10 iters), loss = 8.28001
I0523 00:31:40.489472 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28001 (* 1 = 8.28001 loss)
I0523 00:31:40.758904 34682 sgd_solver.cpp:112] Iteration 20180, lr = 0.01
I0523 00:31:43.331531 34682 solver.cpp:239] Iteration 20190 (3.51873 iter/s, 2.84193s/10 iters), loss = 8.89926
I0523 00:31:43.331569 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89926 (* 1 = 8.89926 loss)
I0523 00:31:43.405030 34682 sgd_solver.cpp:112] Iteration 20190, lr = 0.01
I0523 00:31:46.480293 34682 solver.cpp:239] Iteration 20200 (3.17603 iter/s, 3.14858s/10 iters), loss = 8.57296
I0523 00:31:46.480348 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57296 (* 1 = 8.57296 loss)
I0523 00:31:46.550202 34682 sgd_solver.cpp:112] Iteration 20200, lr = 0.01
I0523 00:31:51.579499 34682 solver.cpp:239] Iteration 20210 (1.96119 iter/s, 5.09894s/10 iters), loss = 8.8114
I0523 00:31:51.579555 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8114 (* 1 = 8.8114 loss)
I0523 00:31:52.411588 34682 sgd_solver.cpp:112] Iteration 20210, lr = 0.01
I0523 00:31:59.498216 34682 solver.cpp:239] Iteration 20220 (1.26289 iter/s, 7.91835s/10 iters), loss = 8.75458
I0523 00:31:59.498271 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75458 (* 1 = 8.75458 loss)
I0523 00:32:00.289747 34682 sgd_solver.cpp:112] Iteration 20220, lr = 0.01
I0523 00:32:03.541036 34682 solver.cpp:239] Iteration 20230 (2.47366 iter/s, 4.04259s/10 iters), loss = 8.57726
I0523 00:32:03.541092 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57726 (* 1 = 8.57726 loss)
I0523 00:32:04.353438 34682 sgd_solver.cpp:112] Iteration 20230, lr = 0.01
I0523 00:32:10.937415 34682 solver.cpp:239] Iteration 20240 (1.35208 iter/s, 7.39602s/10 iters), loss = 8.49833
I0523 00:32:10.937733 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49833 (* 1 = 8.49833 loss)
I0523 00:32:11.003700 34682 sgd_solver.cpp:112] Iteration 20240, lr = 0.01
I0523 00:32:14.941602 34682 solver.cpp:239] Iteration 20250 (2.50026 iter/s, 3.99959s/10 iters), loss = 8.27027
I0523 00:32:14.941649 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27027 (* 1 = 8.27027 loss)
I0523 00:32:15.009855 34682 sgd_solver.cpp:112] Iteration 20250, lr = 0.01
I0523 00:32:21.972604 34682 solver.cpp:239] Iteration 20260 (1.42234 iter/s, 7.03066s/10 iters), loss = 8.39015
I0523 00:32:21.972662 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39015 (* 1 = 8.39015 loss)
I0523 00:32:22.035583 34682 sgd_solver.cpp:112] Iteration 20260, lr = 0.01
I0523 00:32:26.381381 34682 solver.cpp:239] Iteration 20270 (2.26997 iter/s, 4.40534s/10 iters), loss = 9.45496
I0523 00:32:26.381448 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.45496 (* 1 = 9.45496 loss)
I0523 00:32:27.242137 34682 sgd_solver.cpp:112] Iteration 20270, lr = 0.01
I0523 00:32:31.316272 34682 solver.cpp:239] Iteration 20280 (2.0265 iter/s, 4.93462s/10 iters), loss = 9.04564
I0523 00:32:31.316330 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04564 (* 1 = 9.04564 loss)
I0523 00:32:32.077884 34682 sgd_solver.cpp:112] Iteration 20280, lr = 0.01
I0523 00:32:37.616305 34682 solver.cpp:239] Iteration 20290 (1.58737 iter/s, 6.29971s/10 iters), loss = 9.39815
I0523 00:32:37.616382 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.39815 (* 1 = 9.39815 loss)
I0523 00:32:38.427343 34682 sgd_solver.cpp:112] Iteration 20290, lr = 0.01
I0523 00:32:43.333612 34682 solver.cpp:239] Iteration 20300 (1.74917 iter/s, 5.71699s/10 iters), loss = 9.01654
I0523 00:32:43.333753 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01654 (* 1 = 9.01654 loss)
I0523 00:32:43.393707 34682 sgd_solver.cpp:112] Iteration 20300, lr = 0.01
I0523 00:32:50.586045 34682 solver.cpp:239] Iteration 20310 (1.37893 iter/s, 7.25199s/10 iters), loss = 8.8875
I0523 00:32:50.586112 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8875 (* 1 = 8.8875 loss)
I0523 00:32:51.383502 34682 sgd_solver.cpp:112] Iteration 20310, lr = 0.01
I0523 00:32:56.801028 34682 solver.cpp:239] Iteration 20320 (1.6091 iter/s, 6.21467s/10 iters), loss = 8.70302
I0523 00:32:56.801080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70302 (* 1 = 8.70302 loss)
I0523 00:32:56.868477 34682 sgd_solver.cpp:112] Iteration 20320, lr = 0.01
I0523 00:33:00.408015 34682 solver.cpp:239] Iteration 20330 (2.77256 iter/s, 3.60678s/10 iters), loss = 8.73749
I0523 00:33:00.408071 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73749 (* 1 = 8.73749 loss)
I0523 00:33:00.489913 34682 sgd_solver.cpp:112] Iteration 20330, lr = 0.01
I0523 00:33:04.878124 34682 solver.cpp:239] Iteration 20340 (2.2372 iter/s, 4.46987s/10 iters), loss = 9.54819
I0523 00:33:04.878167 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.54819 (* 1 = 9.54819 loss)
I0523 00:33:04.951347 34682 sgd_solver.cpp:112] Iteration 20340, lr = 0.01
I0523 00:33:09.907616 34682 solver.cpp:239] Iteration 20350 (1.98837 iter/s, 5.02923s/10 iters), loss = 8.83691
I0523 00:33:09.907662 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83691 (* 1 = 8.83691 loss)
I0523 00:33:09.985733 34682 sgd_solver.cpp:112] Iteration 20350, lr = 0.01
I0523 00:33:14.788003 34682 solver.cpp:239] Iteration 20360 (2.04912 iter/s, 4.88014s/10 iters), loss = 8.97765
I0523 00:33:14.788213 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97765 (* 1 = 8.97765 loss)
I0523 00:33:15.627851 34682 sgd_solver.cpp:112] Iteration 20360, lr = 0.01
I0523 00:33:19.301560 34682 solver.cpp:239] Iteration 20370 (2.21574 iter/s, 4.51316s/10 iters), loss = 9.77621
I0523 00:33:19.301611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.77621 (* 1 = 9.77621 loss)
I0523 00:33:19.362401 34682 sgd_solver.cpp:112] Iteration 20370, lr = 0.01
I0523 00:33:23.262815 34682 solver.cpp:239] Iteration 20380 (2.5246 iter/s, 3.96103s/10 iters), loss = 8.243
I0523 00:33:23.262868 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.243 (* 1 = 8.243 loss)
I0523 00:33:23.335470 34682 sgd_solver.cpp:112] Iteration 20380, lr = 0.01
I0523 00:33:28.766782 34682 solver.cpp:239] Iteration 20390 (1.81696 iter/s, 5.50369s/10 iters), loss = 9.32718
I0523 00:33:28.766844 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32718 (* 1 = 9.32718 loss)
I0523 00:33:29.601162 34682 sgd_solver.cpp:112] Iteration 20390, lr = 0.01
I0523 00:33:33.549119 34682 solver.cpp:239] Iteration 20400 (2.09114 iter/s, 4.78208s/10 iters), loss = 8.58481
I0523 00:33:33.549172 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58481 (* 1 = 8.58481 loss)
I0523 00:33:33.619824 34682 sgd_solver.cpp:112] Iteration 20400, lr = 0.01
I0523 00:33:39.085427 34682 solver.cpp:239] Iteration 20410 (1.80635 iter/s, 5.53602s/10 iters), loss = 8.53097
I0523 00:33:39.085475 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53097 (* 1 = 8.53097 loss)
I0523 00:33:39.163992 34682 sgd_solver.cpp:112] Iteration 20410, lr = 0.01
I0523 00:33:42.460896 34682 solver.cpp:239] Iteration 20420 (2.96272 iter/s, 3.37528s/10 iters), loss = 8.5466
I0523 00:33:42.460950 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5466 (* 1 = 8.5466 loss)
I0523 00:33:42.531908 34682 sgd_solver.cpp:112] Iteration 20420, lr = 0.01
I0523 00:33:48.055802 34682 solver.cpp:239] Iteration 20430 (1.78743 iter/s, 5.59462s/10 iters), loss = 7.27308
I0523 00:33:48.056008 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.27308 (* 1 = 7.27308 loss)
I0523 00:33:48.145133 34682 sgd_solver.cpp:112] Iteration 20430, lr = 0.01
I0523 00:33:52.286064 34682 solver.cpp:239] Iteration 20440 (2.36412 iter/s, 4.2299s/10 iters), loss = 9.71478
I0523 00:33:52.286115 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.71478 (* 1 = 9.71478 loss)
I0523 00:33:52.360313 34682 sgd_solver.cpp:112] Iteration 20440, lr = 0.01
I0523 00:33:54.792289 34682 solver.cpp:239] Iteration 20450 (3.99032 iter/s, 2.50607s/10 iters), loss = 8.89679
I0523 00:33:54.792343 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89679 (* 1 = 8.89679 loss)
I0523 00:33:55.046516 34682 sgd_solver.cpp:112] Iteration 20450, lr = 0.01
I0523 00:34:00.028640 34682 solver.cpp:239] Iteration 20460 (1.90983 iter/s, 5.23608s/10 iters), loss = 8.20371
I0523 00:34:00.028687 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20371 (* 1 = 8.20371 loss)
I0523 00:34:00.093389 34682 sgd_solver.cpp:112] Iteration 20460, lr = 0.01
I0523 00:34:04.849800 34682 solver.cpp:239] Iteration 20470 (2.0743 iter/s, 4.82091s/10 iters), loss = 8.96129
I0523 00:34:04.849858 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96129 (* 1 = 8.96129 loss)
I0523 00:34:04.919642 34682 sgd_solver.cpp:112] Iteration 20470, lr = 0.01
I0523 00:34:09.660554 34682 solver.cpp:239] Iteration 20480 (2.07879 iter/s, 4.81049s/10 iters), loss = 8.00203
I0523 00:34:09.660604 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00203 (* 1 = 8.00203 loss)
I0523 00:34:09.740515 34682 sgd_solver.cpp:112] Iteration 20480, lr = 0.01
I0523 00:34:14.538131 34682 solver.cpp:239] Iteration 20490 (2.0503 iter/s, 4.87732s/10 iters), loss = 8.48096
I0523 00:34:14.538192 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48096 (* 1 = 8.48096 loss)
I0523 00:34:14.614935 34682 sgd_solver.cpp:112] Iteration 20490, lr = 0.01
I0523 00:34:19.304200 34682 solver.cpp:239] Iteration 20500 (2.09828 iter/s, 4.76581s/10 iters), loss = 9.12359
I0523 00:34:19.304481 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12359 (* 1 = 9.12359 loss)
I0523 00:34:19.383764 34682 sgd_solver.cpp:112] Iteration 20500, lr = 0.01
I0523 00:34:23.495880 34682 solver.cpp:239] Iteration 20510 (2.38838 iter/s, 4.18694s/10 iters), loss = 8.11505
I0523 00:34:23.495925 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11505 (* 1 = 8.11505 loss)
I0523 00:34:23.584233 34682 sgd_solver.cpp:112] Iteration 20510, lr = 0.01
I0523 00:34:29.513381 34682 solver.cpp:239] Iteration 20520 (1.6619 iter/s, 6.01721s/10 iters), loss = 8.50043
I0523 00:34:29.513440 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50043 (* 1 = 8.50043 loss)
I0523 00:34:29.582343 34682 sgd_solver.cpp:112] Iteration 20520, lr = 0.01
I0523 00:34:32.517101 34682 solver.cpp:239] Iteration 20530 (3.32942 iter/s, 3.00353s/10 iters), loss = 8.466
I0523 00:34:32.517161 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.466 (* 1 = 8.466 loss)
I0523 00:34:33.341918 34682 sgd_solver.cpp:112] Iteration 20530, lr = 0.01
I0523 00:34:37.242350 34682 solver.cpp:239] Iteration 20540 (2.11641 iter/s, 4.72499s/10 iters), loss = 8.15908
I0523 00:34:37.242403 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15908 (* 1 = 8.15908 loss)
I0523 00:34:37.492441 34682 sgd_solver.cpp:112] Iteration 20540, lr = 0.01
I0523 00:34:40.126073 34682 solver.cpp:239] Iteration 20550 (3.46795 iter/s, 2.88355s/10 iters), loss = 8.87138
I0523 00:34:40.126122 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87138 (* 1 = 8.87138 loss)
I0523 00:34:40.999534 34682 sgd_solver.cpp:112] Iteration 20550, lr = 0.01
I0523 00:34:47.521697 34682 solver.cpp:239] Iteration 20560 (1.35221 iter/s, 7.39528s/10 iters), loss = 9.11546
I0523 00:34:47.521754 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11546 (* 1 = 9.11546 loss)
I0523 00:34:48.357638 34682 sgd_solver.cpp:112] Iteration 20560, lr = 0.01
I0523 00:34:51.948967 34682 solver.cpp:239] Iteration 20570 (2.25885 iter/s, 4.42704s/10 iters), loss = 9.10755
I0523 00:34:51.949101 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10755 (* 1 = 9.10755 loss)
I0523 00:34:52.765015 34682 sgd_solver.cpp:112] Iteration 20570, lr = 0.01
I0523 00:34:59.518887 34682 solver.cpp:239] Iteration 20580 (1.32109 iter/s, 7.56949s/10 iters), loss = 8.35675
I0523 00:34:59.518934 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35675 (* 1 = 8.35675 loss)
I0523 00:35:00.123896 34682 sgd_solver.cpp:112] Iteration 20580, lr = 0.01
I0523 00:35:04.248280 34682 solver.cpp:239] Iteration 20590 (2.11454 iter/s, 4.72916s/10 iters), loss = 8.26826
I0523 00:35:04.248335 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26826 (* 1 = 8.26826 loss)
I0523 00:35:05.057754 34682 sgd_solver.cpp:112] Iteration 20590, lr = 0.01
I0523 00:35:10.936174 34682 solver.cpp:239] Iteration 20600 (1.49531 iter/s, 6.68757s/10 iters), loss = 8.70105
I0523 00:35:10.936233 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70105 (* 1 = 8.70105 loss)
I0523 00:35:11.675070 34682 sgd_solver.cpp:112] Iteration 20600, lr = 0.01
I0523 00:35:18.050843 34682 solver.cpp:239] Iteration 20610 (1.40561 iter/s, 7.11432s/10 iters), loss = 9.23473
I0523 00:35:18.050904 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23473 (* 1 = 9.23473 loss)
I0523 00:35:18.805147 34682 sgd_solver.cpp:112] Iteration 20610, lr = 0.01
I0523 00:35:24.634799 34682 solver.cpp:239] Iteration 20620 (1.51892 iter/s, 6.58363s/10 iters), loss = 8.00089
I0523 00:35:24.635002 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00089 (* 1 = 8.00089 loss)
I0523 00:35:25.483362 34682 sgd_solver.cpp:112] Iteration 20620, lr = 0.01
I0523 00:35:28.959862 34682 solver.cpp:239] Iteration 20630 (2.31231 iter/s, 4.32468s/10 iters), loss = 9.18093
I0523 00:35:28.959911 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18093 (* 1 = 9.18093 loss)
I0523 00:35:29.771621 34682 sgd_solver.cpp:112] Iteration 20630, lr = 0.01
I0523 00:35:34.178807 34682 solver.cpp:239] Iteration 20640 (1.91619 iter/s, 5.21868s/10 iters), loss = 8.19708
I0523 00:35:34.178856 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19708 (* 1 = 8.19708 loss)
I0523 00:35:34.921960 34682 sgd_solver.cpp:112] Iteration 20640, lr = 0.01
I0523 00:35:38.641855 34682 solver.cpp:239] Iteration 20650 (2.24074 iter/s, 4.46281s/10 iters), loss = 9.29757
I0523 00:35:38.641906 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29757 (* 1 = 9.29757 loss)
I0523 00:35:39.445832 34682 sgd_solver.cpp:112] Iteration 20650, lr = 0.01
I0523 00:35:43.712374 34682 solver.cpp:239] Iteration 20660 (1.97229 iter/s, 5.07026s/10 iters), loss = 8.53262
I0523 00:35:43.712424 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53262 (* 1 = 8.53262 loss)
I0523 00:35:44.536403 34682 sgd_solver.cpp:112] Iteration 20660, lr = 0.01
I0523 00:35:49.399595 34682 solver.cpp:239] Iteration 20670 (1.75841 iter/s, 5.68694s/10 iters), loss = 9.27791
I0523 00:35:49.399643 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27791 (* 1 = 9.27791 loss)
I0523 00:35:49.468399 34682 sgd_solver.cpp:112] Iteration 20670, lr = 0.01
I0523 00:35:54.580847 34682 solver.cpp:239] Iteration 20680 (1.93013 iter/s, 5.18099s/10 iters), loss = 8.80989
I0523 00:35:54.580912 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80989 (* 1 = 8.80989 loss)
I0523 00:35:55.352042 34682 sgd_solver.cpp:112] Iteration 20680, lr = 0.01
I0523 00:36:00.207826 34682 solver.cpp:239] Iteration 20690 (1.77724 iter/s, 5.62669s/10 iters), loss = 9.19313
I0523 00:36:00.207876 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19313 (* 1 = 9.19313 loss)
I0523 00:36:00.263875 34682 sgd_solver.cpp:112] Iteration 20690, lr = 0.01
I0523 00:36:05.186873 34682 solver.cpp:239] Iteration 20700 (2.00852 iter/s, 4.97879s/10 iters), loss = 9.08854
I0523 00:36:05.186921 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08854 (* 1 = 9.08854 loss)
I0523 00:36:05.261963 34682 sgd_solver.cpp:112] Iteration 20700, lr = 0.01
I0523 00:36:11.088990 34682 solver.cpp:239] Iteration 20710 (1.69439 iter/s, 5.90183s/10 iters), loss = 8.85006
I0523 00:36:11.089046 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85006 (* 1 = 8.85006 loss)
I0523 00:36:11.156409 34682 sgd_solver.cpp:112] Iteration 20710, lr = 0.01
I0523 00:36:15.194835 34682 solver.cpp:239] Iteration 20720 (2.43568 iter/s, 4.10563s/10 iters), loss = 8.9323
I0523 00:36:15.194881 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9323 (* 1 = 8.9323 loss)
I0523 00:36:15.925127 34682 sgd_solver.cpp:112] Iteration 20720, lr = 0.01
I0523 00:36:20.238569 34682 solver.cpp:239] Iteration 20730 (1.98276 iter/s, 5.04348s/10 iters), loss = 8.32481
I0523 00:36:20.238612 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32481 (* 1 = 8.32481 loss)
I0523 00:36:21.023286 34682 sgd_solver.cpp:112] Iteration 20730, lr = 0.01
I0523 00:36:26.319936 34682 solver.cpp:239] Iteration 20740 (1.64445 iter/s, 6.08107s/10 iters), loss = 8.56742
I0523 00:36:26.320268 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56742 (* 1 = 8.56742 loss)
I0523 00:36:26.385560 34682 sgd_solver.cpp:112] Iteration 20740, lr = 0.01
I0523 00:36:31.281200 34682 solver.cpp:239] Iteration 20750 (2.01582 iter/s, 4.96075s/10 iters), loss = 8.79478
I0523 00:36:31.281253 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79478 (* 1 = 8.79478 loss)
I0523 00:36:31.349470 34682 sgd_solver.cpp:112] Iteration 20750, lr = 0.01
I0523 00:36:35.381708 34682 solver.cpp:239] Iteration 20760 (2.43886 iter/s, 4.10028s/10 iters), loss = 8.23808
I0523 00:36:35.381762 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23808 (* 1 = 8.23808 loss)
I0523 00:36:35.925792 34682 sgd_solver.cpp:112] Iteration 20760, lr = 0.01
I0523 00:36:40.722403 34682 solver.cpp:239] Iteration 20770 (1.87252 iter/s, 5.34039s/10 iters), loss = 8.95077
I0523 00:36:40.722473 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95077 (* 1 = 8.95077 loss)
I0523 00:36:41.453050 34682 sgd_solver.cpp:112] Iteration 20770, lr = 0.01
I0523 00:36:46.357257 34682 solver.cpp:239] Iteration 20780 (1.77476 iter/s, 5.63456s/10 iters), loss = 8.18253
I0523 00:36:46.357306 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18253 (* 1 = 8.18253 loss)
I0523 00:36:46.426712 34682 sgd_solver.cpp:112] Iteration 20780, lr = 0.01
I0523 00:36:50.677979 34682 solver.cpp:239] Iteration 20790 (2.31455 iter/s, 4.32049s/10 iters), loss = 8.69806
I0523 00:36:50.678040 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69806 (* 1 = 8.69806 loss)
I0523 00:36:51.507333 34682 sgd_solver.cpp:112] Iteration 20790, lr = 0.01
I0523 00:36:54.946358 34682 solver.cpp:239] Iteration 20800 (2.34294 iter/s, 4.26813s/10 iters), loss = 8.22127
I0523 00:36:54.946403 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22127 (* 1 = 8.22127 loss)
I0523 00:36:55.748235 34682 sgd_solver.cpp:112] Iteration 20800, lr = 0.01
I0523 00:37:01.550058 34682 solver.cpp:239] Iteration 20810 (1.51437 iter/s, 6.60339s/10 iters), loss = 8.43816
I0523 00:37:01.550200 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43816 (* 1 = 8.43816 loss)
I0523 00:37:01.621029 34682 sgd_solver.cpp:112] Iteration 20810, lr = 0.01
I0523 00:37:06.592380 34682 solver.cpp:239] Iteration 20820 (1.98335 iter/s, 5.04198s/10 iters), loss = 8.51299
I0523 00:37:06.592427 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51299 (* 1 = 8.51299 loss)
I0523 00:37:06.667158 34682 sgd_solver.cpp:112] Iteration 20820, lr = 0.01
I0523 00:37:12.438329 34682 solver.cpp:239] Iteration 20830 (1.71067 iter/s, 5.84567s/10 iters), loss = 8.25077
I0523 00:37:12.438369 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25077 (* 1 = 8.25077 loss)
I0523 00:37:12.514348 34682 sgd_solver.cpp:112] Iteration 20830, lr = 0.01
I0523 00:37:18.364687 34682 solver.cpp:239] Iteration 20840 (1.68832 iter/s, 5.92304s/10 iters), loss = 8.80132
I0523 00:37:18.364733 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80132 (* 1 = 8.80132 loss)
I0523 00:37:19.226120 34682 sgd_solver.cpp:112] Iteration 20840, lr = 0.01
I0523 00:37:23.495457 34682 solver.cpp:239] Iteration 20850 (1.94913 iter/s, 5.1305s/10 iters), loss = 7.84123
I0523 00:37:23.495506 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84123 (* 1 = 7.84123 loss)
I0523 00:37:23.569195 34682 sgd_solver.cpp:112] Iteration 20850, lr = 0.01
I0523 00:37:29.112463 34682 solver.cpp:239] Iteration 20860 (1.7804 iter/s, 5.61672s/10 iters), loss = 7.95011
I0523 00:37:29.112529 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95011 (* 1 = 7.95011 loss)
I0523 00:37:29.991693 34682 sgd_solver.cpp:112] Iteration 20860, lr = 0.01
I0523 00:37:35.239935 34682 solver.cpp:239] Iteration 20870 (1.63208 iter/s, 6.12716s/10 iters), loss = 8.88077
I0523 00:37:35.240149 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88077 (* 1 = 8.88077 loss)
I0523 00:37:36.074559 34682 sgd_solver.cpp:112] Iteration 20870, lr = 0.01
I0523 00:37:40.512087 34682 solver.cpp:239] Iteration 20880 (1.89691 iter/s, 5.27173s/10 iters), loss = 8.56054
I0523 00:37:40.512136 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56054 (* 1 = 8.56054 loss)
I0523 00:37:40.568256 34682 sgd_solver.cpp:112] Iteration 20880, lr = 0.01
I0523 00:37:46.054138 34682 solver.cpp:239] Iteration 20890 (1.80448 iter/s, 5.54177s/10 iters), loss = 9.28433
I0523 00:37:46.054189 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28433 (* 1 = 9.28433 loss)
I0523 00:37:46.669476 34682 sgd_solver.cpp:112] Iteration 20890, lr = 0.01
I0523 00:37:51.623364 34682 solver.cpp:239] Iteration 20900 (1.79569 iter/s, 5.56889s/10 iters), loss = 9.27013
I0523 00:37:51.623435 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27013 (* 1 = 9.27013 loss)
I0523 00:37:51.695451 34682 sgd_solver.cpp:112] Iteration 20900, lr = 0.01
I0523 00:37:58.309011 34682 solver.cpp:239] Iteration 20910 (1.49631 iter/s, 6.6831s/10 iters), loss = 7.80564
I0523 00:37:58.309063 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80564 (* 1 = 7.80564 loss)
I0523 00:37:58.381319 34682 sgd_solver.cpp:112] Iteration 20910, lr = 0.01
I0523 00:38:03.920627 34682 solver.cpp:239] Iteration 20920 (1.7821 iter/s, 5.61134s/10 iters), loss = 8.17343
I0523 00:38:03.920682 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17343 (* 1 = 8.17343 loss)
I0523 00:38:04.685410 34682 sgd_solver.cpp:112] Iteration 20920, lr = 0.01
I0523 00:38:08.787585 34682 solver.cpp:239] Iteration 20930 (2.05478 iter/s, 4.8667s/10 iters), loss = 8.88237
I0523 00:38:08.787824 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88237 (* 1 = 8.88237 loss)
I0523 00:38:08.860137 34682 sgd_solver.cpp:112] Iteration 20930, lr = 0.01
I0523 00:38:12.360600 34682 solver.cpp:239] Iteration 20940 (2.79904 iter/s, 3.57265s/10 iters), loss = 8.75016
I0523 00:38:12.360648 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75016 (* 1 = 8.75016 loss)
I0523 00:38:12.425638 34682 sgd_solver.cpp:112] Iteration 20940, lr = 0.01
I0523 00:38:16.519049 34682 solver.cpp:239] Iteration 20950 (2.40487 iter/s, 4.15824s/10 iters), loss = 9.44081
I0523 00:38:16.519090 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44081 (* 1 = 9.44081 loss)
I0523 00:38:16.581799 34682 sgd_solver.cpp:112] Iteration 20950, lr = 0.01
I0523 00:38:22.172564 34682 solver.cpp:239] Iteration 20960 (1.7689 iter/s, 5.65324s/10 iters), loss = 8.97131
I0523 00:38:22.172616 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97131 (* 1 = 8.97131 loss)
I0523 00:38:23.013571 34682 sgd_solver.cpp:112] Iteration 20960, lr = 0.01
I0523 00:38:27.778226 34682 solver.cpp:239] Iteration 20970 (1.784 iter/s, 5.60538s/10 iters), loss = 8.80219
I0523 00:38:27.778282 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80219 (* 1 = 8.80219 loss)
I0523 00:38:27.857807 34682 sgd_solver.cpp:112] Iteration 20970, lr = 0.01
I0523 00:38:33.451143 34682 solver.cpp:239] Iteration 20980 (1.76285 iter/s, 5.67262s/10 iters), loss = 8.54099
I0523 00:38:33.451216 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54099 (* 1 = 8.54099 loss)
I0523 00:38:34.279966 34682 sgd_solver.cpp:112] Iteration 20980, lr = 0.01
I0523 00:38:37.613011 34682 solver.cpp:239] Iteration 20990 (2.4029 iter/s, 4.16163s/10 iters), loss = 8.53697
I0523 00:38:37.613067 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53697 (* 1 = 8.53697 loss)
I0523 00:38:38.104153 34682 sgd_solver.cpp:112] Iteration 20990, lr = 0.01
I0523 00:38:42.925535 34682 solver.cpp:239] Iteration 21000 (1.88244 iter/s, 5.31225s/10 iters), loss = 8.52822
I0523 00:38:42.925853 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52822 (* 1 = 8.52822 loss)
I0523 00:38:42.991574 34682 sgd_solver.cpp:112] Iteration 21000, lr = 0.01
I0523 00:38:47.124440 34682 solver.cpp:239] Iteration 21010 (2.38184 iter/s, 4.19844s/10 iters), loss = 9.35912
I0523 00:38:47.124485 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35912 (* 1 = 9.35912 loss)
I0523 00:38:47.201553 34682 sgd_solver.cpp:112] Iteration 21010, lr = 0.01
I0523 00:38:52.252475 34682 solver.cpp:239] Iteration 21020 (1.95016 iter/s, 5.12777s/10 iters), loss = 8.26648
I0523 00:38:52.252522 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26648 (* 1 = 8.26648 loss)
I0523 00:38:52.322404 34682 sgd_solver.cpp:112] Iteration 21020, lr = 0.01
I0523 00:38:56.456451 34682 solver.cpp:239] Iteration 21030 (2.37883 iter/s, 4.20375s/10 iters), loss = 9.6446
I0523 00:38:56.456523 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.6446 (* 1 = 9.6446 loss)
I0523 00:38:57.339962 34682 sgd_solver.cpp:112] Iteration 21030, lr = 0.01
I0523 00:38:58.658183 34682 solver.cpp:239] Iteration 21040 (4.54222 iter/s, 2.20156s/10 iters), loss = 8.71521
I0523 00:38:58.658239 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71521 (* 1 = 8.71521 loss)
I0523 00:38:59.389241 34682 sgd_solver.cpp:112] Iteration 21040, lr = 0.01
I0523 00:39:02.680541 34682 solver.cpp:239] Iteration 21050 (2.48625 iter/s, 4.02213s/10 iters), loss = 8.73764
I0523 00:39:02.680598 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73764 (* 1 = 8.73764 loss)
I0523 00:39:02.738672 34682 sgd_solver.cpp:112] Iteration 21050, lr = 0.01
I0523 00:39:07.486541 34682 solver.cpp:239] Iteration 21060 (2.08084 iter/s, 4.80575s/10 iters), loss = 9.3028
I0523 00:39:07.486599 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3028 (* 1 = 9.3028 loss)
I0523 00:39:08.264466 34682 sgd_solver.cpp:112] Iteration 21060, lr = 0.01
I0523 00:39:14.891477 34682 solver.cpp:239] Iteration 21070 (1.35052 iter/s, 7.40458s/10 iters), loss = 8.7796
I0523 00:39:14.891721 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7796 (* 1 = 8.7796 loss)
I0523 00:39:15.605068 34682 sgd_solver.cpp:112] Iteration 21070, lr = 0.01
I0523 00:39:19.663058 34682 solver.cpp:239] Iteration 21080 (2.09592 iter/s, 4.77118s/10 iters), loss = 8.93743
I0523 00:39:19.663107 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93743 (* 1 = 8.93743 loss)
I0523 00:39:20.466157 34682 sgd_solver.cpp:112] Iteration 21080, lr = 0.01
I0523 00:39:22.871264 34682 solver.cpp:239] Iteration 21090 (3.11719 iter/s, 3.20801s/10 iters), loss = 9.12574
I0523 00:39:22.871320 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12574 (* 1 = 9.12574 loss)
I0523 00:39:23.731153 34682 sgd_solver.cpp:112] Iteration 21090, lr = 0.01
I0523 00:39:27.207083 34682 solver.cpp:239] Iteration 21100 (2.3065 iter/s, 4.33558s/10 iters), loss = 9.11315
I0523 00:39:27.207129 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11315 (* 1 = 9.11315 loss)
I0523 00:39:27.273908 34682 sgd_solver.cpp:112] Iteration 21100, lr = 0.01
I0523 00:39:32.037669 34682 solver.cpp:239] Iteration 21110 (2.07025 iter/s, 4.83033s/10 iters), loss = 8.67536
I0523 00:39:32.037744 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67536 (* 1 = 8.67536 loss)
I0523 00:39:32.101783 34682 sgd_solver.cpp:112] Iteration 21110, lr = 0.01
I0523 00:39:37.414324 34682 solver.cpp:239] Iteration 21120 (1.85999 iter/s, 5.37636s/10 iters), loss = 7.8865
I0523 00:39:37.414369 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8865 (* 1 = 7.8865 loss)
I0523 00:39:37.491075 34682 sgd_solver.cpp:112] Iteration 21120, lr = 0.01
I0523 00:39:42.289134 34682 solver.cpp:239] Iteration 21130 (2.05147 iter/s, 4.87455s/10 iters), loss = 9.08789
I0523 00:39:42.289193 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08789 (* 1 = 9.08789 loss)
I0523 00:39:43.105218 34682 sgd_solver.cpp:112] Iteration 21130, lr = 0.01
I0523 00:39:47.180078 34682 solver.cpp:239] Iteration 21140 (2.0447 iter/s, 4.89069s/10 iters), loss = 8.22273
I0523 00:39:47.180251 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22273 (* 1 = 8.22273 loss)
I0523 00:39:47.262022 34682 sgd_solver.cpp:112] Iteration 21140, lr = 0.01
I0523 00:39:51.390895 34682 solver.cpp:239] Iteration 21150 (2.37503 iter/s, 4.21047s/10 iters), loss = 8.76714
I0523 00:39:51.390949 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76714 (* 1 = 8.76714 loss)
I0523 00:39:52.235445 34682 sgd_solver.cpp:112] Iteration 21150, lr = 0.01
I0523 00:39:57.201870 34682 solver.cpp:239] Iteration 21160 (1.72097 iter/s, 5.81068s/10 iters), loss = 8.94155
I0523 00:39:57.201925 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94155 (* 1 = 8.94155 loss)
I0523 00:39:57.274668 34682 sgd_solver.cpp:112] Iteration 21160, lr = 0.01
I0523 00:40:01.191093 34682 solver.cpp:239] Iteration 21170 (2.50689 iter/s, 3.98901s/10 iters), loss = 8.9222
I0523 00:40:01.191133 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9222 (* 1 = 8.9222 loss)
I0523 00:40:01.253203 34682 sgd_solver.cpp:112] Iteration 21170, lr = 0.01
I0523 00:40:06.358908 34682 solver.cpp:239] Iteration 21180 (1.93515 iter/s, 5.16756s/10 iters), loss = 8.79849
I0523 00:40:06.358968 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79849 (* 1 = 8.79849 loss)
I0523 00:40:06.912622 34682 sgd_solver.cpp:112] Iteration 21180, lr = 0.01
I0523 00:40:12.084552 34682 solver.cpp:239] Iteration 21190 (1.74662 iter/s, 5.72535s/10 iters), loss = 10.0912
I0523 00:40:12.084614 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0912 (* 1 = 10.0912 loss)
I0523 00:40:12.751669 34682 sgd_solver.cpp:112] Iteration 21190, lr = 0.01
I0523 00:40:18.283267 34682 solver.cpp:239] Iteration 21200 (1.61332 iter/s, 6.1984s/10 iters), loss = 8.40754
I0523 00:40:18.283386 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40754 (* 1 = 8.40754 loss)
I0523 00:40:18.346915 34682 sgd_solver.cpp:112] Iteration 21200, lr = 0.01
I0523 00:40:24.530664 34682 solver.cpp:239] Iteration 21210 (1.60156 iter/s, 6.2439s/10 iters), loss = 9.19341
I0523 00:40:24.530740 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19341 (* 1 = 9.19341 loss)
I0523 00:40:24.586885 34682 sgd_solver.cpp:112] Iteration 21210, lr = 0.01
I0523 00:40:30.442873 34682 solver.cpp:239] Iteration 21220 (1.69151 iter/s, 5.91188s/10 iters), loss = 7.64392
I0523 00:40:30.442925 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64392 (* 1 = 7.64392 loss)
I0523 00:40:30.871862 34682 sgd_solver.cpp:112] Iteration 21220, lr = 0.01
I0523 00:40:38.114668 34682 solver.cpp:239] Iteration 21230 (1.30354 iter/s, 7.67143s/10 iters), loss = 8.71343
I0523 00:40:38.114742 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71343 (* 1 = 8.71343 loss)
I0523 00:40:38.191493 34682 sgd_solver.cpp:112] Iteration 21230, lr = 0.01
I0523 00:40:43.983885 34682 solver.cpp:239] Iteration 21240 (1.7039 iter/s, 5.8689s/10 iters), loss = 9.06815
I0523 00:40:43.983939 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06815 (* 1 = 9.06815 loss)
I0523 00:40:44.049934 34682 sgd_solver.cpp:112] Iteration 21240, lr = 0.01
I0523 00:40:48.015638 34682 solver.cpp:239] Iteration 21250 (2.48044 iter/s, 4.03154s/10 iters), loss = 9.38985
I0523 00:40:48.015678 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.38985 (* 1 = 9.38985 loss)
I0523 00:40:48.095219 34682 sgd_solver.cpp:112] Iteration 21250, lr = 0.01
I0523 00:40:50.582623 34682 solver.cpp:239] Iteration 21260 (3.89586 iter/s, 2.56683s/10 iters), loss = 8.19343
I0523 00:40:50.582878 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19343 (* 1 = 8.19343 loss)
I0523 00:40:50.644881 34682 sgd_solver.cpp:112] Iteration 21260, lr = 0.01
I0523 00:40:57.674299 34682 solver.cpp:239] Iteration 21270 (1.41021 iter/s, 7.09117s/10 iters), loss = 9.09894
I0523 00:40:57.674348 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09894 (* 1 = 9.09894 loss)
I0523 00:40:58.417037 34682 sgd_solver.cpp:112] Iteration 21270, lr = 0.01
I0523 00:41:03.334935 34682 solver.cpp:239] Iteration 21280 (1.76668 iter/s, 5.66035s/10 iters), loss = 8.89247
I0523 00:41:03.334991 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89247 (* 1 = 8.89247 loss)
I0523 00:41:04.193135 34682 sgd_solver.cpp:112] Iteration 21280, lr = 0.01
I0523 00:41:09.637886 34682 solver.cpp:239] Iteration 21290 (1.58664 iter/s, 6.30264s/10 iters), loss = 9.04948
I0523 00:41:09.637939 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04948 (* 1 = 9.04948 loss)
I0523 00:41:10.529595 34682 sgd_solver.cpp:112] Iteration 21290, lr = 0.01
I0523 00:41:15.817181 34682 solver.cpp:239] Iteration 21300 (1.61839 iter/s, 6.17899s/10 iters), loss = 7.6599
I0523 00:41:15.817227 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6599 (* 1 = 7.6599 loss)
I0523 00:41:15.888315 34682 sgd_solver.cpp:112] Iteration 21300, lr = 0.01
I0523 00:41:20.801609 34682 solver.cpp:239] Iteration 21310 (2.00635 iter/s, 4.98417s/10 iters), loss = 8.95811
I0523 00:41:20.801905 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95811 (* 1 = 8.95811 loss)
I0523 00:41:20.863203 34682 sgd_solver.cpp:112] Iteration 21310, lr = 0.01
I0523 00:41:25.757727 34682 solver.cpp:239] Iteration 21320 (2.0179 iter/s, 4.95566s/10 iters), loss = 8.7328
I0523 00:41:25.757793 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7328 (* 1 = 8.7328 loss)
I0523 00:41:25.831611 34682 sgd_solver.cpp:112] Iteration 21320, lr = 0.01
I0523 00:41:30.898759 34682 solver.cpp:239] Iteration 21330 (1.94524 iter/s, 5.14076s/10 iters), loss = 8.18115
I0523 00:41:30.898815 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18115 (* 1 = 8.18115 loss)
I0523 00:41:31.006309 34682 sgd_solver.cpp:112] Iteration 21330, lr = 0.01
I0523 00:41:36.664894 34682 solver.cpp:239] Iteration 21340 (1.73435 iter/s, 5.76584s/10 iters), loss = 8.19509
I0523 00:41:36.664947 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19509 (* 1 = 8.19509 loss)
I0523 00:41:36.729159 34682 sgd_solver.cpp:112] Iteration 21340, lr = 0.01
I0523 00:41:40.530988 34682 solver.cpp:239] Iteration 21350 (2.58823 iter/s, 3.86364s/10 iters), loss = 8.55891
I0523 00:41:40.531030 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55891 (* 1 = 8.55891 loss)
I0523 00:41:40.591151 34682 sgd_solver.cpp:112] Iteration 21350, lr = 0.01
I0523 00:41:44.404604 34682 solver.cpp:239] Iteration 21360 (2.58171 iter/s, 3.8734s/10 iters), loss = 8.39062
I0523 00:41:44.404655 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39062 (* 1 = 8.39062 loss)
I0523 00:41:44.468077 34682 sgd_solver.cpp:112] Iteration 21360, lr = 0.01
I0523 00:41:50.755597 34682 solver.cpp:239] Iteration 21370 (1.57464 iter/s, 6.35067s/10 iters), loss = 8.54082
I0523 00:41:50.755671 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54082 (* 1 = 8.54082 loss)
I0523 00:41:51.346217 34682 sgd_solver.cpp:112] Iteration 21370, lr = 0.01
I0523 00:41:57.165874 34682 solver.cpp:239] Iteration 21380 (1.56008 iter/s, 6.40994s/10 iters), loss = 9.18762
I0523 00:41:57.165940 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18762 (* 1 = 9.18762 loss)
I0523 00:41:57.813874 34682 sgd_solver.cpp:112] Iteration 21380, lr = 0.01
I0523 00:42:00.365068 34682 solver.cpp:239] Iteration 21390 (3.12599 iter/s, 3.19899s/10 iters), loss = 8.60913
I0523 00:42:00.365123 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60913 (* 1 = 8.60913 loss)
I0523 00:42:00.447468 34682 sgd_solver.cpp:112] Iteration 21390, lr = 0.01
I0523 00:42:06.029574 34682 solver.cpp:239] Iteration 21400 (1.76547 iter/s, 5.66422s/10 iters), loss = 9.89724
I0523 00:42:06.029620 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.89724 (* 1 = 9.89724 loss)
I0523 00:42:06.086742 34682 sgd_solver.cpp:112] Iteration 21400, lr = 0.01
I0523 00:42:10.696568 34682 solver.cpp:239] Iteration 21410 (2.14282 iter/s, 4.66675s/10 iters), loss = 8.60365
I0523 00:42:10.696624 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60365 (* 1 = 8.60365 loss)
I0523 00:42:10.760000 34682 sgd_solver.cpp:112] Iteration 21410, lr = 0.01
I0523 00:42:14.907418 34682 solver.cpp:239] Iteration 21420 (2.37495 iter/s, 4.21062s/10 iters), loss = 8.74041
I0523 00:42:14.907469 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74041 (* 1 = 8.74041 loss)
I0523 00:42:14.978034 34682 sgd_solver.cpp:112] Iteration 21420, lr = 0.01
I0523 00:42:19.119410 34682 solver.cpp:239] Iteration 21430 (2.3743 iter/s, 4.21177s/10 iters), loss = 9.40025
I0523 00:42:19.119457 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40025 (* 1 = 9.40025 loss)
I0523 00:42:19.967592 34682 sgd_solver.cpp:112] Iteration 21430, lr = 0.01
I0523 00:42:24.990386 34682 solver.cpp:239] Iteration 21440 (1.70338 iter/s, 5.87069s/10 iters), loss = 8.70618
I0523 00:42:24.990525 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70618 (* 1 = 8.70618 loss)
I0523 00:42:25.255733 34682 sgd_solver.cpp:112] Iteration 21440, lr = 0.01
I0523 00:42:30.386670 34682 solver.cpp:239] Iteration 21450 (1.85325 iter/s, 5.39592s/10 iters), loss = 7.63431
I0523 00:42:30.386744 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63431 (* 1 = 7.63431 loss)
I0523 00:42:31.209159 34682 sgd_solver.cpp:112] Iteration 21450, lr = 0.01
I0523 00:42:35.200986 34682 solver.cpp:239] Iteration 21460 (2.07725 iter/s, 4.81405s/10 iters), loss = 8.9892
I0523 00:42:35.201035 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9892 (* 1 = 8.9892 loss)
I0523 00:42:35.274169 34682 sgd_solver.cpp:112] Iteration 21460, lr = 0.01
I0523 00:42:41.787288 34682 solver.cpp:239] Iteration 21470 (1.51838 iter/s, 6.58599s/10 iters), loss = 8.09461
I0523 00:42:41.787348 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09461 (* 1 = 8.09461 loss)
I0523 00:42:42.530315 34682 sgd_solver.cpp:112] Iteration 21470, lr = 0.01
I0523 00:42:49.695900 34682 solver.cpp:239] Iteration 21480 (1.26451 iter/s, 7.90823s/10 iters), loss = 8.27831
I0523 00:42:49.695956 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27831 (* 1 = 8.27831 loss)
I0523 00:42:50.502553 34682 sgd_solver.cpp:112] Iteration 21480, lr = 0.01
I0523 00:42:53.840719 34682 solver.cpp:239] Iteration 21490 (2.41279 iter/s, 4.14459s/10 iters), loss = 9.02641
I0523 00:42:53.840764 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02641 (* 1 = 9.02641 loss)
I0523 00:42:53.921581 34682 sgd_solver.cpp:112] Iteration 21490, lr = 0.01
I0523 00:42:58.854722 34682 solver.cpp:239] Iteration 21500 (1.99452 iter/s, 5.01374s/10 iters), loss = 8.17114
I0523 00:42:58.854995 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17114 (* 1 = 8.17114 loss)
I0523 00:42:59.714104 34682 sgd_solver.cpp:112] Iteration 21500, lr = 0.01
I0523 00:43:04.410866 34682 solver.cpp:239] Iteration 21510 (1.79996 iter/s, 5.55568s/10 iters), loss = 7.97311
I0523 00:43:04.410912 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97311 (* 1 = 7.97311 loss)
I0523 00:43:04.487061 34682 sgd_solver.cpp:112] Iteration 21510, lr = 0.01
I0523 00:43:10.293751 34682 solver.cpp:239] Iteration 21520 (1.69993 iter/s, 5.88258s/10 iters), loss = 8.72609
I0523 00:43:10.293828 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72609 (* 1 = 8.72609 loss)
I0523 00:43:11.113698 34682 sgd_solver.cpp:112] Iteration 21520, lr = 0.01
I0523 00:43:15.108520 34682 solver.cpp:239] Iteration 21530 (2.07708 iter/s, 4.81446s/10 iters), loss = 8.62671
I0523 00:43:15.108603 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62671 (* 1 = 8.62671 loss)
I0523 00:43:15.946050 34682 sgd_solver.cpp:112] Iteration 21530, lr = 0.01
I0523 00:43:18.785501 34682 solver.cpp:239] Iteration 21540 (2.7198 iter/s, 3.67674s/10 iters), loss = 8.60227
I0523 00:43:18.785576 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60227 (* 1 = 8.60227 loss)
I0523 00:43:19.583847 34682 sgd_solver.cpp:112] Iteration 21540, lr = 0.01
I0523 00:43:24.798661 34682 solver.cpp:239] Iteration 21550 (1.66311 iter/s, 6.01285s/10 iters), loss = 8.90045
I0523 00:43:24.798732 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90045 (* 1 = 8.90045 loss)
I0523 00:43:25.579603 34682 sgd_solver.cpp:112] Iteration 21550, lr = 0.01
I0523 00:43:29.990027 34682 solver.cpp:239] Iteration 21560 (1.92639 iter/s, 5.19107s/10 iters), loss = 8.70316
I0523 00:43:29.990264 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70316 (* 1 = 8.70316 loss)
I0523 00:43:30.490371 34682 sgd_solver.cpp:112] Iteration 21560, lr = 0.01
I0523 00:43:35.426687 34682 solver.cpp:239] Iteration 21570 (1.83952 iter/s, 5.4362s/10 iters), loss = 8.72088
I0523 00:43:35.426767 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72088 (* 1 = 8.72088 loss)
I0523 00:43:35.586390 34682 sgd_solver.cpp:112] Iteration 21570, lr = 0.01
I0523 00:43:38.875794 34682 solver.cpp:239] Iteration 21580 (2.89949 iter/s, 3.44888s/10 iters), loss = 9.15292
I0523 00:43:38.875843 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15292 (* 1 = 9.15292 loss)
I0523 00:43:38.954493 34682 sgd_solver.cpp:112] Iteration 21580, lr = 0.01
I0523 00:43:43.419306 34682 solver.cpp:239] Iteration 21590 (2.20105 iter/s, 4.54328s/10 iters), loss = 8.81052
I0523 00:43:43.419351 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81052 (* 1 = 8.81052 loss)
I0523 00:43:43.489593 34682 sgd_solver.cpp:112] Iteration 21590, lr = 0.01
I0523 00:43:48.672910 34682 solver.cpp:239] Iteration 21600 (1.90356 iter/s, 5.25333s/10 iters), loss = 8.05656
I0523 00:43:48.672961 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05656 (* 1 = 8.05656 loss)
I0523 00:43:48.738067 34682 sgd_solver.cpp:112] Iteration 21600, lr = 0.01
I0523 00:43:53.479183 34682 solver.cpp:239] Iteration 21610 (2.08073 iter/s, 4.80601s/10 iters), loss = 9.02544
I0523 00:43:53.479248 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02544 (* 1 = 9.02544 loss)
I0523 00:43:54.299572 34682 sgd_solver.cpp:112] Iteration 21610, lr = 0.01
I0523 00:44:00.652657 34682 solver.cpp:239] Iteration 21620 (1.3941 iter/s, 7.17309s/10 iters), loss = 9.23387
I0523 00:44:00.652871 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23387 (* 1 = 9.23387 loss)
I0523 00:44:01.504195 34682 sgd_solver.cpp:112] Iteration 21620, lr = 0.01
I0523 00:44:06.509583 34682 solver.cpp:239] Iteration 21630 (1.7075 iter/s, 5.85651s/10 iters), loss = 7.86287
I0523 00:44:06.509635 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86287 (* 1 = 7.86287 loss)
I0523 00:44:07.345227 34682 sgd_solver.cpp:112] Iteration 21630, lr = 0.01
I0523 00:44:12.113975 34682 solver.cpp:239] Iteration 21640 (1.78441 iter/s, 5.6041s/10 iters), loss = 9.28162
I0523 00:44:12.114029 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28162 (* 1 = 9.28162 loss)
I0523 00:44:12.173000 34682 sgd_solver.cpp:112] Iteration 21640, lr = 0.01
I0523 00:44:18.264822 34682 solver.cpp:239] Iteration 21650 (1.62587 iter/s, 6.15054s/10 iters), loss = 9.56946
I0523 00:44:18.264870 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56946 (* 1 = 9.56946 loss)
I0523 00:44:18.326464 34682 sgd_solver.cpp:112] Iteration 21650, lr = 0.01
I0523 00:44:24.042083 34682 solver.cpp:239] Iteration 21660 (1.73101 iter/s, 5.77697s/10 iters), loss = 9.65082
I0523 00:44:24.042142 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.65082 (* 1 = 9.65082 loss)
I0523 00:44:24.618274 34682 sgd_solver.cpp:112] Iteration 21660, lr = 0.01
I0523 00:44:29.612288 34682 solver.cpp:239] Iteration 21670 (1.79536 iter/s, 5.56992s/10 iters), loss = 9.85287
I0523 00:44:29.612336 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.85287 (* 1 = 9.85287 loss)
I0523 00:44:30.452608 34682 sgd_solver.cpp:112] Iteration 21670, lr = 0.01
I0523 00:44:36.485766 34682 solver.cpp:239] Iteration 21680 (1.45494 iter/s, 6.87315s/10 iters), loss = 8.97904
I0523 00:44:36.486006 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97904 (* 1 = 8.97904 loss)
I0523 00:44:36.559013 34682 sgd_solver.cpp:112] Iteration 21680, lr = 0.01
I0523 00:44:40.387465 34682 solver.cpp:239] Iteration 21690 (2.56323 iter/s, 3.90133s/10 iters), loss = 9.6849
I0523 00:44:40.387507 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.6849 (* 1 = 9.6849 loss)
I0523 00:44:40.457180 34682 sgd_solver.cpp:112] Iteration 21690, lr = 0.01
I0523 00:44:43.687269 34682 solver.cpp:239] Iteration 21700 (3.03066 iter/s, 3.29961s/10 iters), loss = 7.80513
I0523 00:44:43.687331 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80513 (* 1 = 7.80513 loss)
I0523 00:44:43.751209 34682 sgd_solver.cpp:112] Iteration 21700, lr = 0.01
I0523 00:44:48.596113 34682 solver.cpp:239] Iteration 21710 (2.03725 iter/s, 4.90858s/10 iters), loss = 8.08559
I0523 00:44:48.596174 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08559 (* 1 = 8.08559 loss)
I0523 00:44:49.411738 34682 sgd_solver.cpp:112] Iteration 21710, lr = 0.01
I0523 00:44:54.957209 34682 solver.cpp:239] Iteration 21720 (1.57213 iter/s, 6.36078s/10 iters), loss = 7.7491
I0523 00:44:54.957252 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7491 (* 1 = 7.7491 loss)
I0523 00:44:55.035719 34682 sgd_solver.cpp:112] Iteration 21720, lr = 0.01
I0523 00:45:01.419818 34682 solver.cpp:239] Iteration 21730 (1.54744 iter/s, 6.46229s/10 iters), loss = 8.73294
I0523 00:45:01.419893 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73294 (* 1 = 8.73294 loss)
I0523 00:45:02.265414 34682 sgd_solver.cpp:112] Iteration 21730, lr = 0.01
I0523 00:45:05.443467 34682 solver.cpp:239] Iteration 21740 (2.48545 iter/s, 4.02341s/10 iters), loss = 8.48145
I0523 00:45:05.443509 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48145 (* 1 = 8.48145 loss)
I0523 00:45:05.510314 34682 sgd_solver.cpp:112] Iteration 21740, lr = 0.01
I0523 00:45:09.676903 34682 solver.cpp:239] Iteration 21750 (2.36473 iter/s, 4.22882s/10 iters), loss = 8.92722
I0523 00:45:09.677094 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92722 (* 1 = 8.92722 loss)
I0523 00:45:09.744714 34682 sgd_solver.cpp:112] Iteration 21750, lr = 0.01
I0523 00:45:13.724763 34682 solver.cpp:239] Iteration 21760 (2.47066 iter/s, 4.04749s/10 iters), loss = 8.50776
I0523 00:45:13.724828 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50776 (* 1 = 8.50776 loss)
I0523 00:45:14.434180 34682 sgd_solver.cpp:112] Iteration 21760, lr = 0.01
I0523 00:45:19.045001 34682 solver.cpp:239] Iteration 21770 (1.87972 iter/s, 5.31995s/10 iters), loss = 8.87503
I0523 00:45:19.045063 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87503 (* 1 = 8.87503 loss)
I0523 00:45:19.828044 34682 sgd_solver.cpp:112] Iteration 21770, lr = 0.01
I0523 00:45:23.998396 34682 solver.cpp:239] Iteration 21780 (2.01893 iter/s, 4.95313s/10 iters), loss = 8.22276
I0523 00:45:23.998456 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22276 (* 1 = 8.22276 loss)
I0523 00:45:24.081868 34682 sgd_solver.cpp:112] Iteration 21780, lr = 0.01
I0523 00:45:30.839323 34682 solver.cpp:239] Iteration 21790 (1.46186 iter/s, 6.84059s/10 iters), loss = 8.7259
I0523 00:45:30.839378 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7259 (* 1 = 8.7259 loss)
I0523 00:45:30.916374 34682 sgd_solver.cpp:112] Iteration 21790, lr = 0.01
I0523 00:45:37.409196 34682 solver.cpp:239] Iteration 21800 (1.52217 iter/s, 6.56955s/10 iters), loss = 9.21239
I0523 00:45:37.409253 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21239 (* 1 = 9.21239 loss)
I0523 00:45:37.467914 34682 sgd_solver.cpp:112] Iteration 21800, lr = 0.01
I0523 00:45:41.980624 34682 solver.cpp:239] Iteration 21810 (2.18762 iter/s, 4.57118s/10 iters), loss = 8.96566
I0523 00:45:41.980834 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96566 (* 1 = 8.96566 loss)
I0523 00:45:42.753093 34682 sgd_solver.cpp:112] Iteration 21810, lr = 0.01
I0523 00:45:47.341871 34682 solver.cpp:239] Iteration 21820 (1.86538 iter/s, 5.36085s/10 iters), loss = 8.90318
I0523 00:45:47.341933 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90318 (* 1 = 8.90318 loss)
I0523 00:45:47.411857 34682 sgd_solver.cpp:112] Iteration 21820, lr = 0.01
I0523 00:45:51.488095 34682 solver.cpp:239] Iteration 21830 (2.41197 iter/s, 4.14599s/10 iters), loss = 9.22547
I0523 00:45:51.488139 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22547 (* 1 = 9.22547 loss)
I0523 00:45:51.560609 34682 sgd_solver.cpp:112] Iteration 21830, lr = 0.01
I0523 00:45:55.937855 34682 solver.cpp:239] Iteration 21840 (2.24743 iter/s, 4.44953s/10 iters), loss = 8.66686
I0523 00:45:55.937913 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66686 (* 1 = 8.66686 loss)
I0523 00:45:56.293400 34682 sgd_solver.cpp:112] Iteration 21840, lr = 0.01
I0523 00:46:02.806161 34682 solver.cpp:239] Iteration 21850 (1.45603 iter/s, 6.86797s/10 iters), loss = 8.53564
I0523 00:46:02.806216 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53564 (* 1 = 8.53564 loss)
I0523 00:46:02.888279 34682 sgd_solver.cpp:112] Iteration 21850, lr = 0.01
I0523 00:46:07.190691 34682 solver.cpp:239] Iteration 21860 (2.28087 iter/s, 4.3843s/10 iters), loss = 9.12587
I0523 00:46:07.190747 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12587 (* 1 = 9.12587 loss)
I0523 00:46:07.256880 34682 sgd_solver.cpp:112] Iteration 21860, lr = 0.01
I0523 00:46:13.565702 34682 solver.cpp:239] Iteration 21870 (1.5687 iter/s, 6.3747s/10 iters), loss = 8.73764
I0523 00:46:13.565897 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73764 (* 1 = 8.73764 loss)
I0523 00:46:13.636631 34682 sgd_solver.cpp:112] Iteration 21870, lr = 0.01
I0523 00:46:17.782343 34682 solver.cpp:239] Iteration 21880 (2.37177 iter/s, 4.21626s/10 iters), loss = 8.41154
I0523 00:46:17.782409 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41154 (* 1 = 8.41154 loss)
I0523 00:46:17.852614 34682 sgd_solver.cpp:112] Iteration 21880, lr = 0.01
I0523 00:46:22.479884 34682 solver.cpp:239] Iteration 21890 (2.12889 iter/s, 4.69729s/10 iters), loss = 8.85735
I0523 00:46:22.479952 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85735 (* 1 = 8.85735 loss)
I0523 00:46:23.313346 34682 sgd_solver.cpp:112] Iteration 21890, lr = 0.01
I0523 00:46:27.565380 34682 solver.cpp:239] Iteration 21900 (1.96649 iter/s, 5.08521s/10 iters), loss = 8.73848
I0523 00:46:27.565443 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73848 (* 1 = 8.73848 loss)
I0523 00:46:28.365370 34682 sgd_solver.cpp:112] Iteration 21900, lr = 0.01
I0523 00:46:32.663131 34682 solver.cpp:239] Iteration 21910 (1.96175 iter/s, 5.09748s/10 iters), loss = 8.31864
I0523 00:46:32.663178 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31864 (* 1 = 8.31864 loss)
I0523 00:46:32.730245 34682 sgd_solver.cpp:112] Iteration 21910, lr = 0.01
I0523 00:46:36.145503 34682 solver.cpp:239] Iteration 21920 (2.87176 iter/s, 3.48218s/10 iters), loss = 8.49339
I0523 00:46:36.145546 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49339 (* 1 = 8.49339 loss)
I0523 00:46:36.217039 34682 sgd_solver.cpp:112] Iteration 21920, lr = 0.01
I0523 00:46:39.773638 34682 solver.cpp:239] Iteration 21930 (2.75639 iter/s, 3.62793s/10 iters), loss = 9.00353
I0523 00:46:39.773694 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00353 (* 1 = 9.00353 loss)
I0523 00:46:39.846550 34682 sgd_solver.cpp:112] Iteration 21930, lr = 0.01
I0523 00:46:46.956969 34682 solver.cpp:239] Iteration 21940 (1.39218 iter/s, 7.18298s/10 iters), loss = 8.93195
I0523 00:46:46.957201 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93195 (* 1 = 8.93195 loss)
I0523 00:46:47.755123 34682 sgd_solver.cpp:112] Iteration 21940, lr = 0.01
I0523 00:46:52.512842 34682 solver.cpp:239] Iteration 21950 (1.80003 iter/s, 5.55545s/10 iters), loss = 9.18888
I0523 00:46:52.512907 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18888 (* 1 = 9.18888 loss)
I0523 00:46:52.581398 34682 sgd_solver.cpp:112] Iteration 21950, lr = 0.01
I0523 00:46:56.623737 34682 solver.cpp:239] Iteration 21960 (2.4327 iter/s, 4.11065s/10 iters), loss = 9.17324
I0523 00:46:56.623788 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17324 (* 1 = 9.17324 loss)
I0523 00:46:56.697963 34682 sgd_solver.cpp:112] Iteration 21960, lr = 0.01
I0523 00:47:00.866526 34682 solver.cpp:239] Iteration 21970 (2.35707 iter/s, 4.24256s/10 iters), loss = 8.88283
I0523 00:47:00.866576 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88283 (* 1 = 8.88283 loss)
I0523 00:47:00.923758 34682 sgd_solver.cpp:112] Iteration 21970, lr = 0.01
I0523 00:47:05.320299 34682 solver.cpp:239] Iteration 21980 (2.24541 iter/s, 4.45353s/10 iters), loss = 8.85986
I0523 00:47:05.320363 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85986 (* 1 = 8.85986 loss)
I0523 00:47:06.039882 34682 sgd_solver.cpp:112] Iteration 21980, lr = 0.01
I0523 00:47:08.467206 34682 solver.cpp:239] Iteration 21990 (3.17792 iter/s, 3.14671s/10 iters), loss = 9.18465
I0523 00:47:08.467252 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18465 (* 1 = 9.18465 loss)
I0523 00:47:09.086555 34682 sgd_solver.cpp:112] Iteration 21990, lr = 0.01
I0523 00:47:12.607056 34682 solver.cpp:239] Iteration 22000 (2.41568 iter/s, 4.13962s/10 iters), loss = 7.95541
I0523 00:47:12.607136 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95541 (* 1 = 7.95541 loss)
I0523 00:47:13.410846 34682 sgd_solver.cpp:112] Iteration 22000, lr = 0.01
I0523 00:47:19.855211 34682 solver.cpp:239] Iteration 22010 (1.37973 iter/s, 7.24779s/10 iters), loss = 8.44894
I0523 00:47:19.855465 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44894 (* 1 = 8.44894 loss)
I0523 00:47:20.690579 34682 sgd_solver.cpp:112] Iteration 22010, lr = 0.01
I0523 00:47:25.536103 34682 solver.cpp:239] Iteration 22020 (1.76043 iter/s, 5.68043s/10 iters), loss = 9.64894
I0523 00:47:25.536164 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.64894 (* 1 = 9.64894 loss)
I0523 00:47:26.371044 34682 sgd_solver.cpp:112] Iteration 22020, lr = 0.01
I0523 00:47:31.538823 34682 solver.cpp:239] Iteration 22030 (1.666 iter/s, 6.00241s/10 iters), loss = 9.01822
I0523 00:47:31.538868 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01822 (* 1 = 9.01822 loss)
I0523 00:47:31.594589 34682 sgd_solver.cpp:112] Iteration 22030, lr = 0.01
I0523 00:47:36.546356 34682 solver.cpp:239] Iteration 22040 (1.99709 iter/s, 5.00728s/10 iters), loss = 9.27195
I0523 00:47:36.546404 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27195 (* 1 = 9.27195 loss)
I0523 00:47:37.339980 34682 sgd_solver.cpp:112] Iteration 22040, lr = 0.01
I0523 00:47:41.437885 34682 solver.cpp:239] Iteration 22050 (2.04445 iter/s, 4.89128s/10 iters), loss = 8.38826
I0523 00:47:41.437937 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38826 (* 1 = 8.38826 loss)
I0523 00:47:42.304270 34682 sgd_solver.cpp:112] Iteration 22050, lr = 0.01
I0523 00:47:46.497210 34682 solver.cpp:239] Iteration 22060 (1.97665 iter/s, 5.05907s/10 iters), loss = 9.17036
I0523 00:47:46.497251 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17036 (* 1 = 9.17036 loss)
I0523 00:47:46.577236 34682 sgd_solver.cpp:112] Iteration 22060, lr = 0.01
I0523 00:47:51.449246 34682 solver.cpp:239] Iteration 22070 (2.01947 iter/s, 4.95179s/10 iters), loss = 9.92231
I0523 00:47:51.449441 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.92231 (* 1 = 9.92231 loss)
I0523 00:47:51.525233 34682 sgd_solver.cpp:112] Iteration 22070, lr = 0.01
I0523 00:47:53.318181 34682 solver.cpp:239] Iteration 22080 (5.35134 iter/s, 1.86869s/10 iters), loss = 8.44036
I0523 00:47:53.318222 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44036 (* 1 = 8.44036 loss)
I0523 00:47:53.381992 34682 sgd_solver.cpp:112] Iteration 22080, lr = 0.01
I0523 00:47:58.157755 34682 solver.cpp:239] Iteration 22090 (2.0664 iter/s, 4.83933s/10 iters), loss = 8.89282
I0523 00:47:58.157812 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89282 (* 1 = 8.89282 loss)
I0523 00:47:58.983513 34682 sgd_solver.cpp:112] Iteration 22090, lr = 0.01
I0523 00:48:02.281272 34682 solver.cpp:239] Iteration 22100 (2.42525 iter/s, 4.12329s/10 iters), loss = 8.43979
I0523 00:48:02.281322 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43979 (* 1 = 8.43979 loss)
I0523 00:48:02.344954 34682 sgd_solver.cpp:112] Iteration 22100, lr = 0.01
I0523 00:48:06.490762 34682 solver.cpp:239] Iteration 22110 (2.37571 iter/s, 4.20927s/10 iters), loss = 8.46381
I0523 00:48:06.490833 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46381 (* 1 = 8.46381 loss)
I0523 00:48:07.333771 34682 sgd_solver.cpp:112] Iteration 22110, lr = 0.01
I0523 00:48:11.622647 34682 solver.cpp:239] Iteration 22120 (1.94871 iter/s, 5.13161s/10 iters), loss = 9.25941
I0523 00:48:11.622692 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25941 (* 1 = 9.25941 loss)
I0523 00:48:11.682684 34682 sgd_solver.cpp:112] Iteration 22120, lr = 0.01
I0523 00:48:15.733850 34682 solver.cpp:239] Iteration 22130 (2.43251 iter/s, 4.11098s/10 iters), loss = 8.34308
I0523 00:48:15.733911 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34308 (* 1 = 8.34308 loss)
I0523 00:48:15.808534 34682 sgd_solver.cpp:112] Iteration 22130, lr = 0.01
I0523 00:48:22.338539 34682 solver.cpp:239] Iteration 22140 (1.51415 iter/s, 6.60437s/10 iters), loss = 8.85245
I0523 00:48:22.338817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85245 (* 1 = 8.85245 loss)
I0523 00:48:23.236901 34682 sgd_solver.cpp:112] Iteration 22140, lr = 0.01
I0523 00:48:26.846415 34682 solver.cpp:239] Iteration 22150 (2.21856 iter/s, 4.50743s/10 iters), loss = 8.90956
I0523 00:48:26.846480 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90956 (* 1 = 8.90956 loss)
I0523 00:48:26.912906 34682 sgd_solver.cpp:112] Iteration 22150, lr = 0.01
I0523 00:48:31.424154 34682 solver.cpp:239] Iteration 22160 (2.1846 iter/s, 4.57749s/10 iters), loss = 8.64357
I0523 00:48:31.424203 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64357 (* 1 = 8.64357 loss)
I0523 00:48:32.254967 34682 sgd_solver.cpp:112] Iteration 22160, lr = 0.01
I0523 00:48:36.180997 34682 solver.cpp:239] Iteration 22170 (2.10234 iter/s, 4.75659s/10 iters), loss = 8.52353
I0523 00:48:36.181074 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52353 (* 1 = 8.52353 loss)
I0523 00:48:37.022142 34682 sgd_solver.cpp:112] Iteration 22170, lr = 0.01
I0523 00:48:42.803633 34682 solver.cpp:239] Iteration 22180 (1.51005 iter/s, 6.62229s/10 iters), loss = 9.3925
I0523 00:48:42.803686 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3925 (* 1 = 9.3925 loss)
I0523 00:48:43.514705 34682 sgd_solver.cpp:112] Iteration 22180, lr = 0.01
I0523 00:48:47.506183 34682 solver.cpp:239] Iteration 22190 (2.12661 iter/s, 4.70231s/10 iters), loss = 8.60651
I0523 00:48:47.506225 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60651 (* 1 = 8.60651 loss)
I0523 00:48:47.565102 34682 sgd_solver.cpp:112] Iteration 22190, lr = 0.01
I0523 00:48:52.592548 34682 solver.cpp:239] Iteration 22200 (1.96614 iter/s, 5.08611s/10 iters), loss = 9.05769
I0523 00:48:52.592746 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05769 (* 1 = 9.05769 loss)
I0523 00:48:52.658115 34682 sgd_solver.cpp:112] Iteration 22200, lr = 0.01
I0523 00:48:56.925228 34682 solver.cpp:239] Iteration 22210 (2.30823 iter/s, 4.33233s/10 iters), loss = 8.1897
I0523 00:48:56.925287 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1897 (* 1 = 8.1897 loss)
I0523 00:48:57.757405 34682 sgd_solver.cpp:112] Iteration 22210, lr = 0.01
I0523 00:49:00.232638 34682 solver.cpp:239] Iteration 22220 (3.02369 iter/s, 3.30722s/10 iters), loss = 8.75696
I0523 00:49:00.232684 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75696 (* 1 = 8.75696 loss)
I0523 00:49:00.315341 34682 sgd_solver.cpp:112] Iteration 22220, lr = 0.01
I0523 00:49:07.247009 34682 solver.cpp:239] Iteration 22230 (1.42571 iter/s, 7.01403s/10 iters), loss = 8.4462
I0523 00:49:07.247077 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4462 (* 1 = 8.4462 loss)
I0523 00:49:08.098632 34682 sgd_solver.cpp:112] Iteration 22230, lr = 0.01
I0523 00:49:12.831463 34682 solver.cpp:239] Iteration 22240 (1.79078 iter/s, 5.58417s/10 iters), loss = 9.02019
I0523 00:49:12.831514 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02019 (* 1 = 9.02019 loss)
I0523 00:49:13.563966 34682 sgd_solver.cpp:112] Iteration 22240, lr = 0.01
I0523 00:49:20.186028 34682 solver.cpp:239] Iteration 22250 (1.35976 iter/s, 7.35422s/10 iters), loss = 9.19732
I0523 00:49:20.186075 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19732 (* 1 = 9.19732 loss)
I0523 00:49:20.262591 34682 sgd_solver.cpp:112] Iteration 22250, lr = 0.01
I0523 00:49:26.566195 34682 solver.cpp:239] Iteration 22260 (1.56743 iter/s, 6.37986s/10 iters), loss = 8.96847
I0523 00:49:26.566478 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96847 (* 1 = 8.96847 loss)
I0523 00:49:26.629616 34682 sgd_solver.cpp:112] Iteration 22260, lr = 0.01
I0523 00:49:29.319311 34682 solver.cpp:239] Iteration 22270 (3.63273 iter/s, 2.75275s/10 iters), loss = 8.81627
I0523 00:49:29.319355 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81627 (* 1 = 8.81627 loss)
I0523 00:49:30.123831 34682 sgd_solver.cpp:112] Iteration 22270, lr = 0.01
I0523 00:49:34.684329 34682 solver.cpp:239] Iteration 22280 (1.86402 iter/s, 5.36475s/10 iters), loss = 8.43529
I0523 00:49:34.684381 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43529 (* 1 = 8.43529 loss)
I0523 00:49:35.501540 34682 sgd_solver.cpp:112] Iteration 22280, lr = 0.01
I0523 00:49:39.897900 34682 solver.cpp:239] Iteration 22290 (1.91817 iter/s, 5.21331s/10 iters), loss = 8.77819
I0523 00:49:39.897948 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77819 (* 1 = 8.77819 loss)
I0523 00:49:40.311820 34682 sgd_solver.cpp:112] Iteration 22290, lr = 0.01
I0523 00:49:45.157726 34682 solver.cpp:239] Iteration 22300 (1.9013 iter/s, 5.25956s/10 iters), loss = 9.14795
I0523 00:49:45.157768 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14795 (* 1 = 9.14795 loss)
I0523 00:49:45.235162 34682 sgd_solver.cpp:112] Iteration 22300, lr = 0.01
I0523 00:49:48.910686 34682 solver.cpp:239] Iteration 22310 (2.66471 iter/s, 3.75276s/10 iters), loss = 9.56739
I0523 00:49:48.910769 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56739 (* 1 = 9.56739 loss)
I0523 00:49:48.974480 34682 sgd_solver.cpp:112] Iteration 22310, lr = 0.01
I0523 00:49:53.579449 34682 solver.cpp:239] Iteration 22320 (2.14202 iter/s, 4.66849s/10 iters), loss = 8.54441
I0523 00:49:53.579494 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54441 (* 1 = 8.54441 loss)
I0523 00:49:53.652474 34682 sgd_solver.cpp:112] Iteration 22320, lr = 0.01
I0523 00:49:57.707209 34682 solver.cpp:239] Iteration 22330 (2.42276 iter/s, 4.12753s/10 iters), loss = 8.56038
I0523 00:49:57.707345 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56038 (* 1 = 8.56038 loss)
I0523 00:49:57.767280 34682 sgd_solver.cpp:112] Iteration 22330, lr = 0.01
I0523 00:50:00.410951 34682 solver.cpp:239] Iteration 22340 (3.69895 iter/s, 2.70347s/10 iters), loss = 8.16995
I0523 00:50:00.411001 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16995 (* 1 = 8.16995 loss)
I0523 00:50:01.225160 34682 sgd_solver.cpp:112] Iteration 22340, lr = 0.01
I0523 00:50:04.905647 34682 solver.cpp:239] Iteration 22350 (2.22496 iter/s, 4.49446s/10 iters), loss = 8.20695
I0523 00:50:04.905691 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20695 (* 1 = 8.20695 loss)
I0523 00:50:05.735466 34682 sgd_solver.cpp:112] Iteration 22350, lr = 0.01
I0523 00:50:10.881148 34682 solver.cpp:239] Iteration 22360 (1.67358 iter/s, 5.97521s/10 iters), loss = 8.39812
I0523 00:50:10.881191 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39812 (* 1 = 8.39812 loss)
I0523 00:50:11.624285 34682 sgd_solver.cpp:112] Iteration 22360, lr = 0.01
I0523 00:50:13.939903 34682 solver.cpp:239] Iteration 22370 (3.26951 iter/s, 3.05857s/10 iters), loss = 9.29135
I0523 00:50:13.939956 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29135 (* 1 = 9.29135 loss)
I0523 00:50:14.011734 34682 sgd_solver.cpp:112] Iteration 22370, lr = 0.01
I0523 00:50:17.997805 34682 solver.cpp:239] Iteration 22380 (2.46446 iter/s, 4.05768s/10 iters), loss = 8.74871
I0523 00:50:17.997864 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74871 (* 1 = 8.74871 loss)
I0523 00:50:18.653978 34682 sgd_solver.cpp:112] Iteration 22380, lr = 0.01
I0523 00:50:22.458343 34682 solver.cpp:239] Iteration 22390 (2.242 iter/s, 4.4603s/10 iters), loss = 8.66247
I0523 00:50:22.458387 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66247 (* 1 = 8.66247 loss)
I0523 00:50:22.525633 34682 sgd_solver.cpp:112] Iteration 22390, lr = 0.01
I0523 00:50:26.144115 34682 solver.cpp:239] Iteration 22400 (2.71329 iter/s, 3.68556s/10 iters), loss = 8.25023
I0523 00:50:26.144181 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25023 (* 1 = 8.25023 loss)
I0523 00:50:26.217164 34682 sgd_solver.cpp:112] Iteration 22400, lr = 0.01
I0523 00:50:31.722038 34682 solver.cpp:239] Iteration 22410 (1.79288 iter/s, 5.57763s/10 iters), loss = 8.39076
I0523 00:50:31.722229 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39076 (* 1 = 8.39076 loss)
I0523 00:50:31.782467 34682 sgd_solver.cpp:112] Iteration 22410, lr = 0.01
I0523 00:50:36.717864 34682 solver.cpp:239] Iteration 22420 (2.00183 iter/s, 4.99543s/10 iters), loss = 8.83567
I0523 00:50:36.717911 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83567 (* 1 = 8.83567 loss)
I0523 00:50:36.789058 34682 sgd_solver.cpp:112] Iteration 22420, lr = 0.01
I0523 00:50:41.702805 34682 solver.cpp:239] Iteration 22430 (2.00615 iter/s, 4.98468s/10 iters), loss = 8.75912
I0523 00:50:41.702848 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75912 (* 1 = 8.75912 loss)
I0523 00:50:41.778831 34682 sgd_solver.cpp:112] Iteration 22430, lr = 0.01
I0523 00:50:46.677737 34682 solver.cpp:239] Iteration 22440 (2.01018 iter/s, 4.97467s/10 iters), loss = 7.9688
I0523 00:50:46.677796 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9688 (* 1 = 7.9688 loss)
I0523 00:50:46.742727 34682 sgd_solver.cpp:112] Iteration 22440, lr = 0.01
I0523 00:50:51.601303 34682 solver.cpp:239] Iteration 22450 (2.03116 iter/s, 4.9233s/10 iters), loss = 9.60874
I0523 00:50:51.601351 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60874 (* 1 = 9.60874 loss)
I0523 00:50:51.667820 34682 sgd_solver.cpp:112] Iteration 22450, lr = 0.01
I0523 00:50:55.079198 34682 solver.cpp:239] Iteration 22460 (2.87547 iter/s, 3.4777s/10 iters), loss = 8.96017
I0523 00:50:55.079254 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96017 (* 1 = 8.96017 loss)
I0523 00:50:55.898195 34682 sgd_solver.cpp:112] Iteration 22460, lr = 0.01
I0523 00:50:58.871006 34682 solver.cpp:239] Iteration 22470 (2.63743 iter/s, 3.79157s/10 iters), loss = 8.81112
I0523 00:50:58.871069 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81112 (* 1 = 8.81112 loss)
I0523 00:50:58.919687 34682 sgd_solver.cpp:112] Iteration 22470, lr = 0.01
I0523 00:51:03.436666 34682 solver.cpp:239] Iteration 22480 (2.19038 iter/s, 4.56541s/10 iters), loss = 7.38931
I0523 00:51:03.436780 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.38931 (* 1 = 7.38931 loss)
I0523 00:51:03.496457 34682 sgd_solver.cpp:112] Iteration 22480, lr = 0.01
I0523 00:51:08.299618 34682 solver.cpp:239] Iteration 22490 (2.0565 iter/s, 4.86264s/10 iters), loss = 8.784
I0523 00:51:08.299661 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.784 (* 1 = 8.784 loss)
I0523 00:51:08.363939 34682 sgd_solver.cpp:112] Iteration 22490, lr = 0.01
I0523 00:51:12.954114 34682 solver.cpp:239] Iteration 22500 (2.14857 iter/s, 4.65425s/10 iters), loss = 9.11567
I0523 00:51:12.954169 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11567 (* 1 = 9.11567 loss)
I0523 00:51:13.792148 34682 sgd_solver.cpp:112] Iteration 22500, lr = 0.01
I0523 00:51:17.991649 34682 solver.cpp:239] Iteration 22510 (1.9852 iter/s, 5.03727s/10 iters), loss = 9.88333
I0523 00:51:17.991703 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.88333 (* 1 = 9.88333 loss)
I0523 00:51:18.065627 34682 sgd_solver.cpp:112] Iteration 22510, lr = 0.01
I0523 00:51:23.551625 34682 solver.cpp:239] Iteration 22520 (1.79866 iter/s, 5.5597s/10 iters), loss = 8.97375
I0523 00:51:23.551672 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97375 (* 1 = 8.97375 loss)
I0523 00:51:24.130710 34682 sgd_solver.cpp:112] Iteration 22520, lr = 0.01
I0523 00:51:30.473460 34682 solver.cpp:239] Iteration 22530 (1.44477 iter/s, 6.92151s/10 iters), loss = 9.05926
I0523 00:51:30.473506 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05926 (* 1 = 9.05926 loss)
I0523 00:51:30.545841 34682 sgd_solver.cpp:112] Iteration 22530, lr = 0.01
I0523 00:51:36.299441 34682 solver.cpp:239] Iteration 22540 (1.71653 iter/s, 5.82571s/10 iters), loss = 8.95624
I0523 00:51:36.299583 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95624 (* 1 = 8.95624 loss)
I0523 00:51:36.386342 34682 sgd_solver.cpp:112] Iteration 22540, lr = 0.01
I0523 00:51:39.650872 34682 solver.cpp:239] Iteration 22550 (2.98405 iter/s, 3.35115s/10 iters), loss = 8.83774
I0523 00:51:39.650926 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83774 (* 1 = 8.83774 loss)
I0523 00:51:40.396771 34682 sgd_solver.cpp:112] Iteration 22550, lr = 0.01
I0523 00:51:45.896692 34682 solver.cpp:239] Iteration 22560 (1.60115 iter/s, 6.24551s/10 iters), loss = 8.92481
I0523 00:51:45.896744 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92481 (* 1 = 8.92481 loss)
I0523 00:51:46.603312 34682 sgd_solver.cpp:112] Iteration 22560, lr = 0.01
I0523 00:51:49.510345 34682 solver.cpp:239] Iteration 22570 (2.76744 iter/s, 3.61345s/10 iters), loss = 8.41281
I0523 00:51:49.510390 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41281 (* 1 = 8.41281 loss)
I0523 00:51:49.577069 34682 sgd_solver.cpp:112] Iteration 22570, lr = 0.01
I0523 00:51:52.194059 34682 solver.cpp:239] Iteration 22580 (3.7264 iter/s, 2.68355s/10 iters), loss = 8.47332
I0523 00:51:52.194102 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47332 (* 1 = 8.47332 loss)
I0523 00:51:52.266700 34682 sgd_solver.cpp:112] Iteration 22580, lr = 0.01
I0523 00:51:57.795795 34682 solver.cpp:239] Iteration 22590 (1.78525 iter/s, 5.60146s/10 iters), loss = 8.35134
I0523 00:51:57.795847 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35134 (* 1 = 8.35134 loss)
I0523 00:51:58.617259 34682 sgd_solver.cpp:112] Iteration 22590, lr = 0.01
I0523 00:52:03.394997 34682 solver.cpp:239] Iteration 22600 (1.78606 iter/s, 5.59892s/10 iters), loss = 9.43638
I0523 00:52:03.395064 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43638 (* 1 = 9.43638 loss)
I0523 00:52:04.236924 34682 sgd_solver.cpp:112] Iteration 22600, lr = 0.01
I0523 00:52:08.390674 34682 solver.cpp:239] Iteration 22610 (2.00184 iter/s, 4.99541s/10 iters), loss = 8.93526
I0523 00:52:08.390898 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93526 (* 1 = 8.93526 loss)
I0523 00:52:09.022037 34682 sgd_solver.cpp:112] Iteration 22610, lr = 0.01
I0523 00:52:14.723701 34682 solver.cpp:239] Iteration 22620 (1.57914 iter/s, 6.33256s/10 iters), loss = 8.41343
I0523 00:52:14.723748 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41343 (* 1 = 8.41343 loss)
I0523 00:52:14.790647 34682 sgd_solver.cpp:112] Iteration 22620, lr = 0.01
I0523 00:52:19.812008 34682 solver.cpp:239] Iteration 22630 (1.96539 iter/s, 5.08805s/10 iters), loss = 8.02732
I0523 00:52:19.812075 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02732 (* 1 = 8.02732 loss)
I0523 00:52:20.576943 34682 sgd_solver.cpp:112] Iteration 22630, lr = 0.01
I0523 00:52:25.630877 34682 solver.cpp:239] Iteration 22640 (1.71864 iter/s, 5.81857s/10 iters), loss = 9.1082
I0523 00:52:25.630925 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1082 (* 1 = 9.1082 loss)
I0523 00:52:25.694277 34682 sgd_solver.cpp:112] Iteration 22640, lr = 0.01
I0523 00:52:31.324801 34682 solver.cpp:239] Iteration 22650 (1.75635 iter/s, 5.69364s/10 iters), loss = 8.80362
I0523 00:52:31.324851 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80362 (* 1 = 8.80362 loss)
I0523 00:52:31.400861 34682 sgd_solver.cpp:112] Iteration 22650, lr = 0.01
I0523 00:52:36.060273 34682 solver.cpp:239] Iteration 22660 (2.11183 iter/s, 4.73522s/10 iters), loss = 8.75823
I0523 00:52:36.060333 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75823 (* 1 = 8.75823 loss)
I0523 00:52:36.904000 34682 sgd_solver.cpp:112] Iteration 22660, lr = 0.01
I0523 00:52:40.531713 34682 solver.cpp:239] Iteration 22670 (2.23654 iter/s, 4.4712s/10 iters), loss = 9.04495
I0523 00:52:40.531852 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04495 (* 1 = 9.04495 loss)
I0523 00:52:41.272886 34682 sgd_solver.cpp:112] Iteration 22670, lr = 0.01
I0523 00:52:44.900069 34682 solver.cpp:239] Iteration 22680 (2.28935 iter/s, 4.36804s/10 iters), loss = 8.70347
I0523 00:52:44.900113 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70347 (* 1 = 8.70347 loss)
I0523 00:52:45.701834 34682 sgd_solver.cpp:112] Iteration 22680, lr = 0.01
I0523 00:52:49.142192 34682 solver.cpp:239] Iteration 22690 (2.35744 iter/s, 4.24189s/10 iters), loss = 8.85572
I0523 00:52:49.142248 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85572 (* 1 = 8.85572 loss)
I0523 00:52:49.199468 34682 sgd_solver.cpp:112] Iteration 22690, lr = 0.01
I0523 00:52:54.924598 34682 solver.cpp:239] Iteration 22700 (1.72947 iter/s, 5.78211s/10 iters), loss = 9.04461
I0523 00:52:54.924659 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04461 (* 1 = 9.04461 loss)
I0523 00:52:54.986913 34682 sgd_solver.cpp:112] Iteration 22700, lr = 0.01
I0523 00:52:59.512370 34682 solver.cpp:239] Iteration 22710 (2.17983 iter/s, 4.58752s/10 iters), loss = 8.80879
I0523 00:52:59.512435 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80879 (* 1 = 8.80879 loss)
I0523 00:53:00.302917 34682 sgd_solver.cpp:112] Iteration 22710, lr = 0.01
I0523 00:53:06.431007 34682 solver.cpp:239] Iteration 22720 (1.44544 iter/s, 6.91829s/10 iters), loss = 7.89653
I0523 00:53:06.431056 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89653 (* 1 = 7.89653 loss)
I0523 00:53:06.492825 34682 sgd_solver.cpp:112] Iteration 22720, lr = 0.01
I0523 00:53:10.455096 34682 solver.cpp:239] Iteration 22730 (2.48517 iter/s, 4.02388s/10 iters), loss = 9.27273
I0523 00:53:10.455138 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27273 (* 1 = 9.27273 loss)
I0523 00:53:10.524446 34682 sgd_solver.cpp:112] Iteration 22730, lr = 0.01
I0523 00:53:15.501199 34682 solver.cpp:239] Iteration 22740 (1.98183 iter/s, 5.04585s/10 iters), loss = 8.59164
I0523 00:53:15.501425 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59164 (* 1 = 8.59164 loss)
I0523 00:53:15.924852 34682 sgd_solver.cpp:112] Iteration 22740, lr = 0.01
I0523 00:53:19.327025 34682 solver.cpp:239] Iteration 22750 (2.61405 iter/s, 3.82548s/10 iters), loss = 9.1286
I0523 00:53:19.327076 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1286 (* 1 = 9.1286 loss)
I0523 00:53:19.906229 34682 sgd_solver.cpp:112] Iteration 22750, lr = 0.01
I0523 00:53:25.276695 34682 solver.cpp:239] Iteration 22760 (1.68085 iter/s, 5.94938s/10 iters), loss = 8.93371
I0523 00:53:25.276746 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93371 (* 1 = 8.93371 loss)
I0523 00:53:25.335397 34682 sgd_solver.cpp:112] Iteration 22760, lr = 0.01
I0523 00:53:30.159749 34682 solver.cpp:239] Iteration 22770 (2.048 iter/s, 4.8828s/10 iters), loss = 8.96188
I0523 00:53:30.159811 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96188 (* 1 = 8.96188 loss)
I0523 00:53:31.027703 34682 sgd_solver.cpp:112] Iteration 22770, lr = 0.01
I0523 00:53:37.733487 34682 solver.cpp:239] Iteration 22780 (1.32042 iter/s, 7.57332s/10 iters), loss = 9.05676
I0523 00:53:37.733551 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05676 (* 1 = 9.05676 loss)
I0523 00:53:38.564853 34682 sgd_solver.cpp:112] Iteration 22780, lr = 0.01
I0523 00:53:42.868189 34682 solver.cpp:239] Iteration 22790 (1.94765 iter/s, 5.13441s/10 iters), loss = 8.77924
I0523 00:53:42.868245 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77924 (* 1 = 8.77924 loss)
I0523 00:53:43.708794 34682 sgd_solver.cpp:112] Iteration 22790, lr = 0.01
I0523 00:53:47.867832 34682 solver.cpp:239] Iteration 22800 (2.00025 iter/s, 4.99938s/10 iters), loss = 8.97241
I0523 00:53:47.868077 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97241 (* 1 = 8.97241 loss)
I0523 00:53:48.066563 34682 sgd_solver.cpp:112] Iteration 22800, lr = 0.01
I0523 00:53:52.279934 34682 solver.cpp:239] Iteration 22810 (2.2667 iter/s, 4.41169s/10 iters), loss = 7.75489
I0523 00:53:52.279989 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75489 (* 1 = 7.75489 loss)
I0523 00:53:52.407161 34682 sgd_solver.cpp:112] Iteration 22810, lr = 0.01
I0523 00:53:57.058053 34682 solver.cpp:239] Iteration 22820 (2.09299 iter/s, 4.77786s/10 iters), loss = 9.76679
I0523 00:53:57.058107 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.76679 (* 1 = 9.76679 loss)
I0523 00:53:57.131134 34682 sgd_solver.cpp:112] Iteration 22820, lr = 0.01
I0523 00:54:00.069298 34682 solver.cpp:239] Iteration 22830 (3.32108 iter/s, 3.01107s/10 iters), loss = 8.92508
I0523 00:54:00.069345 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92508 (* 1 = 8.92508 loss)
I0523 00:54:00.142382 34682 sgd_solver.cpp:112] Iteration 22830, lr = 0.01
I0523 00:54:04.920981 34682 solver.cpp:239] Iteration 22840 (2.06124 iter/s, 4.85144s/10 iters), loss = 8.62258
I0523 00:54:04.921030 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62258 (* 1 = 8.62258 loss)
I0523 00:54:05.755288 34682 sgd_solver.cpp:112] Iteration 22840, lr = 0.01
I0523 00:54:11.916534 34682 solver.cpp:239] Iteration 22850 (1.42955 iter/s, 6.99522s/10 iters), loss = 8.70449
I0523 00:54:11.916589 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70449 (* 1 = 8.70449 loss)
I0523 00:54:11.981513 34682 sgd_solver.cpp:112] Iteration 22850, lr = 0.01
I0523 00:54:16.920408 34682 solver.cpp:239] Iteration 22860 (1.99856 iter/s, 5.00361s/10 iters), loss = 8.37317
I0523 00:54:16.920462 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37317 (* 1 = 8.37317 loss)
I0523 00:54:17.000128 34682 sgd_solver.cpp:112] Iteration 22860, lr = 0.01
I0523 00:54:22.316596 34682 solver.cpp:239] Iteration 22870 (1.85326 iter/s, 5.39591s/10 iters), loss = 8.66666
I0523 00:54:22.316721 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66666 (* 1 = 8.66666 loss)
I0523 00:54:22.388999 34682 sgd_solver.cpp:112] Iteration 22870, lr = 0.01
I0523 00:54:26.776047 34682 solver.cpp:239] Iteration 22880 (2.24258 iter/s, 4.45915s/10 iters), loss = 9.1781
I0523 00:54:26.776091 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1781 (* 1 = 9.1781 loss)
I0523 00:54:26.835214 34682 sgd_solver.cpp:112] Iteration 22880, lr = 0.01
I0523 00:54:32.326741 34682 solver.cpp:239] Iteration 22890 (1.80167 iter/s, 5.55042s/10 iters), loss = 9.30039
I0523 00:54:32.326788 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30039 (* 1 = 9.30039 loss)
I0523 00:54:33.046752 34682 sgd_solver.cpp:112] Iteration 22890, lr = 0.01
I0523 00:54:37.998167 34682 solver.cpp:239] Iteration 22900 (1.76331 iter/s, 5.67114s/10 iters), loss = 8.09795
I0523 00:54:37.998217 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09795 (* 1 = 8.09795 loss)
I0523 00:54:38.062757 34682 sgd_solver.cpp:112] Iteration 22900, lr = 0.01
I0523 00:54:40.754365 34682 solver.cpp:239] Iteration 22910 (3.62842 iter/s, 2.75602s/10 iters), loss = 8.4106
I0523 00:54:40.754429 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4106 (* 1 = 8.4106 loss)
I0523 00:54:41.517130 34682 sgd_solver.cpp:112] Iteration 22910, lr = 0.01
I0523 00:54:45.893918 34682 solver.cpp:239] Iteration 22920 (1.9458 iter/s, 5.13928s/10 iters), loss = 8.83782
I0523 00:54:45.894002 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83782 (* 1 = 8.83782 loss)
I0523 00:54:45.957065 34682 sgd_solver.cpp:112] Iteration 22920, lr = 0.01
I0523 00:54:51.152705 34682 solver.cpp:239] Iteration 22930 (1.90169 iter/s, 5.25848s/10 iters), loss = 9.64176
I0523 00:54:51.152752 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.64176 (* 1 = 9.64176 loss)
I0523 00:54:51.219969 34682 sgd_solver.cpp:112] Iteration 22930, lr = 0.01
I0523 00:54:57.287369 34682 solver.cpp:239] Iteration 22940 (1.63016 iter/s, 6.13437s/10 iters), loss = 7.84296
I0523 00:54:57.287550 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84296 (* 1 = 7.84296 loss)
I0523 00:54:58.111379 34682 sgd_solver.cpp:112] Iteration 22940, lr = 0.01
I0523 00:55:02.833523 34682 solver.cpp:239] Iteration 22950 (1.80318 iter/s, 5.54575s/10 iters), loss = 8.29418
I0523 00:55:02.833570 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29418 (* 1 = 8.29418 loss)
I0523 00:55:02.892657 34682 sgd_solver.cpp:112] Iteration 22950, lr = 0.01
I0523 00:55:08.841478 34682 solver.cpp:239] Iteration 22960 (1.66454 iter/s, 6.00767s/10 iters), loss = 8.62813
I0523 00:55:08.841522 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62813 (* 1 = 8.62813 loss)
I0523 00:55:08.914887 34682 sgd_solver.cpp:112] Iteration 22960, lr = 0.01
I0523 00:55:12.629528 34682 solver.cpp:239] Iteration 22970 (2.64002 iter/s, 3.78784s/10 iters), loss = 8.05899
I0523 00:55:12.629582 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05899 (* 1 = 8.05899 loss)
I0523 00:55:13.447674 34682 sgd_solver.cpp:112] Iteration 22970, lr = 0.01
I0523 00:55:19.376593 34682 solver.cpp:239] Iteration 22980 (1.4822 iter/s, 6.74673s/10 iters), loss = 9.20743
I0523 00:55:19.376642 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20743 (* 1 = 9.20743 loss)
I0523 00:55:19.455039 34682 sgd_solver.cpp:112] Iteration 22980, lr = 0.01
I0523 00:55:23.390130 34682 solver.cpp:239] Iteration 22990 (2.4917 iter/s, 4.01332s/10 iters), loss = 8.49408
I0523 00:55:23.390178 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49408 (* 1 = 8.49408 loss)
I0523 00:55:23.479015 34682 sgd_solver.cpp:112] Iteration 22990, lr = 0.01
I0523 00:55:31.203156 34682 solver.cpp:239] Iteration 23000 (1.27997 iter/s, 7.81266s/10 iters), loss = 8.37314
I0523 00:55:31.203294 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37314 (* 1 = 8.37314 loss)
I0523 00:55:31.268235 34682 sgd_solver.cpp:112] Iteration 23000, lr = 0.01
I0523 00:55:35.189615 34682 solver.cpp:239] Iteration 23010 (2.50868 iter/s, 3.98616s/10 iters), loss = 8.62919
I0523 00:55:35.189664 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62919 (* 1 = 8.62919 loss)
I0523 00:55:36.048529 34682 sgd_solver.cpp:112] Iteration 23010, lr = 0.01
I0523 00:55:39.908394 34682 solver.cpp:239] Iteration 23020 (2.1193 iter/s, 4.71853s/10 iters), loss = 9.11253
I0523 00:55:39.908454 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11253 (* 1 = 9.11253 loss)
I0523 00:55:40.692293 34682 sgd_solver.cpp:112] Iteration 23020, lr = 0.01
I0523 00:55:44.657845 34682 solver.cpp:239] Iteration 23030 (2.10562 iter/s, 4.74919s/10 iters), loss = 8.06697
I0523 00:55:44.657923 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06697 (* 1 = 8.06697 loss)
I0523 00:55:45.014230 34682 sgd_solver.cpp:112] Iteration 23030, lr = 0.01
I0523 00:55:50.685883 34682 solver.cpp:239] Iteration 23040 (1.659 iter/s, 6.02772s/10 iters), loss = 7.50514
I0523 00:55:50.685933 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50514 (* 1 = 7.50514 loss)
I0523 00:55:50.765002 34682 sgd_solver.cpp:112] Iteration 23040, lr = 0.01
I0523 00:55:53.413456 34682 solver.cpp:239] Iteration 23050 (3.66651 iter/s, 2.72739s/10 iters), loss = 9.41163
I0523 00:55:53.413535 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.41163 (* 1 = 9.41163 loss)
I0523 00:55:54.204197 34682 sgd_solver.cpp:112] Iteration 23050, lr = 0.01
I0523 00:55:59.264672 34682 solver.cpp:239] Iteration 23060 (1.70914 iter/s, 5.85089s/10 iters), loss = 9.32335
I0523 00:55:59.264719 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32335 (* 1 = 9.32335 loss)
I0523 00:56:00.093952 34682 sgd_solver.cpp:112] Iteration 23060, lr = 0.01
I0523 00:56:04.144574 34682 solver.cpp:239] Iteration 23070 (2.04932 iter/s, 4.87966s/10 iters), loss = 8.20676
I0523 00:56:04.144734 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20676 (* 1 = 8.20676 loss)
I0523 00:56:04.215972 34682 sgd_solver.cpp:112] Iteration 23070, lr = 0.01
I0523 00:56:11.671020 34682 solver.cpp:239] Iteration 23080 (1.32873 iter/s, 7.526s/10 iters), loss = 9.1694
I0523 00:56:11.671066 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1694 (* 1 = 9.1694 loss)
I0523 00:56:12.487857 34682 sgd_solver.cpp:112] Iteration 23080, lr = 0.01
I0523 00:56:17.592324 34682 solver.cpp:239] Iteration 23090 (1.6889 iter/s, 5.92102s/10 iters), loss = 9.10361
I0523 00:56:17.592387 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10361 (* 1 = 9.10361 loss)
I0523 00:56:17.665333 34682 sgd_solver.cpp:112] Iteration 23090, lr = 0.01
I0523 00:56:20.897320 34682 solver.cpp:239] Iteration 23100 (3.0259 iter/s, 3.3048s/10 iters), loss = 8.47244
I0523 00:56:20.897361 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47244 (* 1 = 8.47244 loss)
I0523 00:56:20.965833 34682 sgd_solver.cpp:112] Iteration 23100, lr = 0.01
I0523 00:56:25.888054 34682 solver.cpp:239] Iteration 23110 (2.00381 iter/s, 4.99049s/10 iters), loss = 8.6608
I0523 00:56:25.888098 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6608 (* 1 = 8.6608 loss)
I0523 00:56:25.968075 34682 sgd_solver.cpp:112] Iteration 23110, lr = 0.01
I0523 00:56:30.001780 34682 solver.cpp:239] Iteration 23120 (2.43102 iter/s, 4.1135s/10 iters), loss = 8.87569
I0523 00:56:30.001843 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87569 (* 1 = 8.87569 loss)
I0523 00:56:30.853559 34682 sgd_solver.cpp:112] Iteration 23120, lr = 0.01
I0523 00:56:36.690668 34682 solver.cpp:239] Iteration 23130 (1.49509 iter/s, 6.68856s/10 iters), loss = 9.77935
I0523 00:56:36.690937 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.77935 (* 1 = 9.77935 loss)
I0523 00:56:37.493880 34682 sgd_solver.cpp:112] Iteration 23130, lr = 0.01
I0523 00:56:42.704598 34682 solver.cpp:239] Iteration 23140 (1.66294 iter/s, 6.01345s/10 iters), loss = 8.79506
I0523 00:56:42.704653 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79506 (* 1 = 8.79506 loss)
I0523 00:56:42.784152 34682 sgd_solver.cpp:112] Iteration 23140, lr = 0.01
I0523 00:56:47.505388 34682 solver.cpp:239] Iteration 23150 (2.0831 iter/s, 4.80054s/10 iters), loss = 8.88397
I0523 00:56:47.505439 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88397 (* 1 = 8.88397 loss)
I0523 00:56:48.045235 34682 sgd_solver.cpp:112] Iteration 23150, lr = 0.01
I0523 00:56:53.911576 34682 solver.cpp:239] Iteration 23160 (1.56107 iter/s, 6.40586s/10 iters), loss = 8.92598
I0523 00:56:53.911645 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92598 (* 1 = 8.92598 loss)
I0523 00:56:54.437937 34682 sgd_solver.cpp:112] Iteration 23160, lr = 0.01
I0523 00:56:57.845698 34682 solver.cpp:239] Iteration 23170 (2.54201 iter/s, 3.93389s/10 iters), loss = 8.5357
I0523 00:56:57.845751 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5357 (* 1 = 8.5357 loss)
I0523 00:56:57.905447 34682 sgd_solver.cpp:112] Iteration 23170, lr = 0.01
I0523 00:57:00.528993 34682 solver.cpp:239] Iteration 23180 (3.727 iter/s, 2.68312s/10 iters), loss = 8.82671
I0523 00:57:00.529058 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82671 (* 1 = 8.82671 loss)
I0523 00:57:00.917263 34682 sgd_solver.cpp:112] Iteration 23180, lr = 0.01
I0523 00:57:06.614759 34682 solver.cpp:239] Iteration 23190 (1.64326 iter/s, 6.08546s/10 iters), loss = 9.31152
I0523 00:57:06.614810 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31152 (* 1 = 9.31152 loss)
I0523 00:57:07.403874 34682 sgd_solver.cpp:112] Iteration 23190, lr = 0.01
I0523 00:57:10.985512 34682 solver.cpp:239] Iteration 23200 (2.28806 iter/s, 4.37052s/10 iters), loss = 8.95243
I0523 00:57:10.985571 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95243 (* 1 = 8.95243 loss)
I0523 00:57:11.810592 34682 sgd_solver.cpp:112] Iteration 23200, lr = 0.01
I0523 00:57:17.533495 34682 solver.cpp:239] Iteration 23210 (1.52726 iter/s, 6.54766s/10 iters), loss = 9.22571
I0523 00:57:17.533557 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22571 (* 1 = 9.22571 loss)
I0523 00:57:17.591166 34682 sgd_solver.cpp:112] Iteration 23210, lr = 0.01
I0523 00:57:23.818455 34682 solver.cpp:239] Iteration 23220 (1.59118 iter/s, 6.28465s/10 iters), loss = 8.54461
I0523 00:57:23.818507 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54461 (* 1 = 8.54461 loss)
I0523 00:57:24.475724 34682 sgd_solver.cpp:112] Iteration 23220, lr = 0.01
I0523 00:57:28.686393 34682 solver.cpp:239] Iteration 23230 (2.05437 iter/s, 4.86768s/10 iters), loss = 9.00243
I0523 00:57:28.686447 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00243 (* 1 = 9.00243 loss)
I0523 00:57:29.543153 34682 sgd_solver.cpp:112] Iteration 23230, lr = 0.01
I0523 00:57:35.644460 34682 solver.cpp:239] Iteration 23240 (1.43725 iter/s, 6.95772s/10 iters), loss = 8.86515
I0523 00:57:35.644520 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86515 (* 1 = 8.86515 loss)
I0523 00:57:35.705809 34682 sgd_solver.cpp:112] Iteration 23240, lr = 0.01
I0523 00:57:38.971644 34682 solver.cpp:239] Iteration 23250 (3.00572 iter/s, 3.32699s/10 iters), loss = 9.14918
I0523 00:57:38.972175 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14918 (* 1 = 9.14918 loss)
I0523 00:57:39.053069 34682 sgd_solver.cpp:112] Iteration 23250, lr = 0.01
I0523 00:57:43.351361 34682 solver.cpp:239] Iteration 23260 (2.28362 iter/s, 4.37901s/10 iters), loss = 7.66676
I0523 00:57:43.351404 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66676 (* 1 = 7.66676 loss)
I0523 00:57:43.422879 34682 sgd_solver.cpp:112] Iteration 23260, lr = 0.01
I0523 00:57:49.107894 34682 solver.cpp:239] Iteration 23270 (1.73724 iter/s, 5.75624s/10 iters), loss = 8.77428
I0523 00:57:49.107940 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77428 (* 1 = 8.77428 loss)
I0523 00:57:49.177225 34682 sgd_solver.cpp:112] Iteration 23270, lr = 0.01
I0523 00:57:54.102355 34682 solver.cpp:239] Iteration 23280 (2.00232 iter/s, 4.99421s/10 iters), loss = 9.00019
I0523 00:57:54.102397 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00019 (* 1 = 9.00019 loss)
I0523 00:57:54.179554 34682 sgd_solver.cpp:112] Iteration 23280, lr = 0.01
I0523 00:57:58.507797 34682 solver.cpp:239] Iteration 23290 (2.27004 iter/s, 4.40521s/10 iters), loss = 8.51563
I0523 00:57:58.507844 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51563 (* 1 = 8.51563 loss)
I0523 00:57:58.581127 34682 sgd_solver.cpp:112] Iteration 23290, lr = 0.01
I0523 00:58:03.566403 34682 solver.cpp:239] Iteration 23300 (1.97693 iter/s, 5.05835s/10 iters), loss = 8.74118
I0523 00:58:03.566471 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74118 (* 1 = 8.74118 loss)
I0523 00:58:04.130193 34682 sgd_solver.cpp:112] Iteration 23300, lr = 0.01
I0523 00:58:10.606967 34682 solver.cpp:239] Iteration 23310 (1.42041 iter/s, 7.04022s/10 iters), loss = 8.93806
I0523 00:58:10.607203 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93806 (* 1 = 8.93806 loss)
I0523 00:58:11.264642 34682 sgd_solver.cpp:112] Iteration 23310, lr = 0.01
I0523 00:58:18.590416 34682 solver.cpp:239] Iteration 23320 (1.25267 iter/s, 7.98293s/10 iters), loss = 8.86132
I0523 00:58:18.590472 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86132 (* 1 = 8.86132 loss)
I0523 00:58:18.650209 34682 sgd_solver.cpp:112] Iteration 23320, lr = 0.01
I0523 00:58:23.598917 34682 solver.cpp:239] Iteration 23330 (1.99671 iter/s, 5.00823s/10 iters), loss = 9.16193
I0523 00:58:23.598975 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16193 (* 1 = 9.16193 loss)
I0523 00:58:23.660691 34682 sgd_solver.cpp:112] Iteration 23330, lr = 0.01
I0523 00:58:28.726424 34682 solver.cpp:239] Iteration 23340 (1.95036 iter/s, 5.12725s/10 iters), loss = 8.69164
I0523 00:58:28.726470 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69164 (* 1 = 8.69164 loss)
I0523 00:58:28.818305 34682 sgd_solver.cpp:112] Iteration 23340, lr = 0.01
I0523 00:58:34.543529 34682 solver.cpp:239] Iteration 23350 (1.71915 iter/s, 5.81681s/10 iters), loss = 8.44526
I0523 00:58:34.543586 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44526 (* 1 = 8.44526 loss)
I0523 00:58:34.610124 34682 sgd_solver.cpp:112] Iteration 23350, lr = 0.01
I0523 00:58:38.551194 34682 solver.cpp:239] Iteration 23360 (2.49535 iter/s, 4.00745s/10 iters), loss = 8.39541
I0523 00:58:38.551236 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39541 (* 1 = 8.39541 loss)
I0523 00:58:39.333154 34682 sgd_solver.cpp:112] Iteration 23360, lr = 0.01
I0523 00:58:44.995975 34682 solver.cpp:239] Iteration 23370 (1.55172 iter/s, 6.44447s/10 iters), loss = 9.11793
I0523 00:58:44.996289 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11793 (* 1 = 9.11793 loss)
I0523 00:58:45.056856 34682 sgd_solver.cpp:112] Iteration 23370, lr = 0.01
I0523 00:58:52.173655 34682 solver.cpp:239] Iteration 23380 (1.39332 iter/s, 7.1771s/10 iters), loss = 8.72836
I0523 00:58:52.173727 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72836 (* 1 = 8.72836 loss)
I0523 00:58:52.815105 34682 sgd_solver.cpp:112] Iteration 23380, lr = 0.01
I0523 00:58:56.292712 34682 solver.cpp:239] Iteration 23390 (2.42788 iter/s, 4.11882s/10 iters), loss = 8.2861
I0523 00:58:56.292764 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2861 (* 1 = 8.2861 loss)
I0523 00:58:56.368008 34682 sgd_solver.cpp:112] Iteration 23390, lr = 0.01
I0523 00:58:59.668052 34682 solver.cpp:239] Iteration 23400 (2.96284 iter/s, 3.37514s/10 iters), loss = 9.56889
I0523 00:58:59.668110 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56889 (* 1 = 9.56889 loss)
I0523 00:58:59.872645 34682 sgd_solver.cpp:112] Iteration 23400, lr = 0.01
I0523 00:59:02.735790 34682 solver.cpp:239] Iteration 23410 (3.25992 iter/s, 3.06756s/10 iters), loss = 9.36284
I0523 00:59:02.735828 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36284 (* 1 = 9.36284 loss)
I0523 00:59:03.404857 34682 sgd_solver.cpp:112] Iteration 23410, lr = 0.01
I0523 00:59:06.808595 34682 solver.cpp:239] Iteration 23420 (2.45811 iter/s, 4.06817s/10 iters), loss = 8.64071
I0523 00:59:06.808691 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64071 (* 1 = 8.64071 loss)
I0523 00:59:07.620165 34682 sgd_solver.cpp:112] Iteration 23420, lr = 0.01
I0523 00:59:12.519659 34682 solver.cpp:239] Iteration 23430 (1.75109 iter/s, 5.71074s/10 iters), loss = 8.49364
I0523 00:59:12.519713 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49364 (* 1 = 8.49364 loss)
I0523 00:59:12.580258 34682 sgd_solver.cpp:112] Iteration 23430, lr = 0.01
I0523 00:59:18.722046 34682 solver.cpp:239] Iteration 23440 (1.61236 iter/s, 6.20208s/10 iters), loss = 8.6087
I0523 00:59:18.722326 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6087 (* 1 = 8.6087 loss)
I0523 00:59:19.385885 34682 sgd_solver.cpp:112] Iteration 23440, lr = 0.01
I0523 00:59:23.889029 34682 solver.cpp:239] Iteration 23450 (1.93554 iter/s, 5.16652s/10 iters), loss = 8.95974
I0523 00:59:23.889075 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95974 (* 1 = 8.95974 loss)
I0523 00:59:23.958151 34682 sgd_solver.cpp:112] Iteration 23450, lr = 0.01
I0523 00:59:29.751009 34682 solver.cpp:239] Iteration 23460 (1.70599 iter/s, 5.86169s/10 iters), loss = 8.57342
I0523 00:59:29.751056 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57342 (* 1 = 8.57342 loss)
I0523 00:59:30.368286 34682 sgd_solver.cpp:112] Iteration 23460, lr = 0.01
I0523 00:59:35.375775 34682 solver.cpp:239] Iteration 23470 (1.77794 iter/s, 5.62449s/10 iters), loss = 9.35572
I0523 00:59:35.375816 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35572 (* 1 = 9.35572 loss)
I0523 00:59:35.449604 34682 sgd_solver.cpp:112] Iteration 23470, lr = 0.01
I0523 00:59:39.390029 34682 solver.cpp:239] Iteration 23480 (2.49125 iter/s, 4.01405s/10 iters), loss = 8.70327
I0523 00:59:39.390074 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70327 (* 1 = 8.70327 loss)
I0523 00:59:39.461385 34682 sgd_solver.cpp:112] Iteration 23480, lr = 0.01
I0523 00:59:44.322777 34682 solver.cpp:239] Iteration 23490 (2.02737 iter/s, 4.9325s/10 iters), loss = 8.70326
I0523 00:59:44.322832 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70326 (* 1 = 8.70326 loss)
I0523 00:59:44.932094 34682 sgd_solver.cpp:112] Iteration 23490, lr = 0.01
I0523 00:59:48.969336 34682 solver.cpp:239] Iteration 23500 (2.15224 iter/s, 4.64631s/10 iters), loss = 8.61416
I0523 00:59:48.969633 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61416 (* 1 = 8.61416 loss)
I0523 00:59:49.045379 34682 sgd_solver.cpp:112] Iteration 23500, lr = 0.01
I0523 00:59:54.066709 34682 solver.cpp:239] Iteration 23510 (1.96198 iter/s, 5.0969s/10 iters), loss = 9.04079
I0523 00:59:54.066752 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04079 (* 1 = 9.04079 loss)
I0523 00:59:54.125210 34682 sgd_solver.cpp:112] Iteration 23510, lr = 0.01
I0523 00:59:58.204740 34682 solver.cpp:239] Iteration 23520 (2.41674 iter/s, 4.1378s/10 iters), loss = 8.83148
I0523 00:59:58.204809 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83148 (* 1 = 8.83148 loss)
I0523 00:59:59.031463 34682 sgd_solver.cpp:112] Iteration 23520, lr = 0.01
I0523 01:00:03.703567 34682 solver.cpp:239] Iteration 23530 (1.81866 iter/s, 5.49854s/10 iters), loss = 9.28913
I0523 01:00:03.703616 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28913 (* 1 = 9.28913 loss)
I0523 01:00:03.774652 34682 sgd_solver.cpp:112] Iteration 23530, lr = 0.01
I0523 01:00:06.874850 34682 solver.cpp:239] Iteration 23540 (3.15347 iter/s, 3.17111s/10 iters), loss = 8.29916
I0523 01:00:06.874891 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29916 (* 1 = 8.29916 loss)
I0523 01:00:06.944106 34682 sgd_solver.cpp:112] Iteration 23540, lr = 0.01
I0523 01:00:11.868005 34682 solver.cpp:239] Iteration 23550 (2.00284 iter/s, 4.9929s/10 iters), loss = 8.53587
I0523 01:00:11.868052 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53587 (* 1 = 8.53587 loss)
I0523 01:00:11.929162 34682 sgd_solver.cpp:112] Iteration 23550, lr = 0.01
I0523 01:00:17.402508 34682 solver.cpp:239] Iteration 23560 (1.80694 iter/s, 5.53423s/10 iters), loss = 8.47029
I0523 01:00:17.402559 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47029 (* 1 = 8.47029 loss)
I0523 01:00:18.021353 34682 sgd_solver.cpp:112] Iteration 23560, lr = 0.01
I0523 01:00:22.897816 34682 solver.cpp:239] Iteration 23570 (1.81983 iter/s, 5.49503s/10 iters), loss = 8.69493
I0523 01:00:22.898098 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69493 (* 1 = 8.69493 loss)
I0523 01:00:22.979341 34682 sgd_solver.cpp:112] Iteration 23570, lr = 0.01
I0523 01:00:28.314549 34682 solver.cpp:239] Iteration 23580 (1.84629 iter/s, 5.41626s/10 iters), loss = 9.60612
I0523 01:00:28.314597 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60612 (* 1 = 9.60612 loss)
I0523 01:00:29.028195 34682 sgd_solver.cpp:112] Iteration 23580, lr = 0.01
I0523 01:00:33.819648 34682 solver.cpp:239] Iteration 23590 (1.81659 iter/s, 5.50481s/10 iters), loss = 8.02189
I0523 01:00:33.819699 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02189 (* 1 = 8.02189 loss)
I0523 01:00:34.664582 34682 sgd_solver.cpp:112] Iteration 23590, lr = 0.01
I0523 01:00:37.269515 34682 solver.cpp:239] Iteration 23600 (2.89884 iter/s, 3.44966s/10 iters), loss = 7.97367
I0523 01:00:37.269567 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97367 (* 1 = 7.97367 loss)
I0523 01:00:37.331264 34682 sgd_solver.cpp:112] Iteration 23600, lr = 0.01
I0523 01:00:41.866662 34682 solver.cpp:239] Iteration 23610 (2.17537 iter/s, 4.59692s/10 iters), loss = 8.03396
I0523 01:00:41.866731 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03396 (* 1 = 8.03396 loss)
I0523 01:00:42.699796 34682 sgd_solver.cpp:112] Iteration 23610, lr = 0.01
I0523 01:00:46.263355 34682 solver.cpp:239] Iteration 23620 (2.27457 iter/s, 4.39644s/10 iters), loss = 8.42637
I0523 01:00:46.263397 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42637 (* 1 = 8.42637 loss)
I0523 01:00:46.336462 34682 sgd_solver.cpp:112] Iteration 23620, lr = 0.01
I0523 01:00:49.621013 34682 solver.cpp:239] Iteration 23630 (2.97844 iter/s, 3.35747s/10 iters), loss = 8.45954
I0523 01:00:49.621070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45954 (* 1 = 8.45954 loss)
I0523 01:00:49.689225 34682 sgd_solver.cpp:112] Iteration 23630, lr = 0.01
I0523 01:00:54.165689 34682 solver.cpp:239] Iteration 23640 (2.20049 iter/s, 4.54444s/10 iters), loss = 8.73118
I0523 01:00:54.165866 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73118 (* 1 = 8.73118 loss)
I0523 01:00:54.237820 34682 sgd_solver.cpp:112] Iteration 23640, lr = 0.01
I0523 01:00:59.733916 34682 solver.cpp:239] Iteration 23650 (1.79604 iter/s, 5.56781s/10 iters), loss = 9.00117
I0523 01:00:59.733968 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00117 (* 1 = 9.00117 loss)
I0523 01:00:59.794385 34682 sgd_solver.cpp:112] Iteration 23650, lr = 0.01
I0523 01:01:04.056118 34682 solver.cpp:239] Iteration 23660 (2.31376 iter/s, 4.32198s/10 iters), loss = 8.96204
I0523 01:01:04.056161 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96204 (* 1 = 8.96204 loss)
I0523 01:01:04.884104 34682 sgd_solver.cpp:112] Iteration 23660, lr = 0.01
I0523 01:01:09.091295 34682 solver.cpp:239] Iteration 23670 (1.98613 iter/s, 5.03493s/10 iters), loss = 8.45599
I0523 01:01:09.091341 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45599 (* 1 = 8.45599 loss)
I0523 01:01:09.165683 34682 sgd_solver.cpp:112] Iteration 23670, lr = 0.01
I0523 01:01:13.632652 34682 solver.cpp:239] Iteration 23680 (2.20211 iter/s, 4.54111s/10 iters), loss = 9.32954
I0523 01:01:13.632714 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32954 (* 1 = 9.32954 loss)
I0523 01:01:13.709609 34682 sgd_solver.cpp:112] Iteration 23680, lr = 0.01
I0523 01:01:18.364248 34682 solver.cpp:239] Iteration 23690 (2.11357 iter/s, 4.73134s/10 iters), loss = 9.49029
I0523 01:01:18.364300 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.49029 (* 1 = 9.49029 loss)
I0523 01:01:19.194106 34682 sgd_solver.cpp:112] Iteration 23690, lr = 0.01
I0523 01:01:22.527768 34682 solver.cpp:239] Iteration 23700 (2.40195 iter/s, 4.16328s/10 iters), loss = 10.0627
I0523 01:01:22.527838 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0627 (* 1 = 10.0627 loss)
I0523 01:01:23.392334 34682 sgd_solver.cpp:112] Iteration 23700, lr = 0.01
I0523 01:01:29.915405 34682 solver.cpp:239] Iteration 23710 (1.35368 iter/s, 7.38727s/10 iters), loss = 8.49902
I0523 01:01:29.915640 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49902 (* 1 = 8.49902 loss)
I0523 01:01:30.734935 34682 sgd_solver.cpp:112] Iteration 23710, lr = 0.01
I0523 01:01:33.352192 34682 solver.cpp:239] Iteration 23720 (2.90999 iter/s, 3.43644s/10 iters), loss = 9.28918
I0523 01:01:33.352236 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28918 (* 1 = 9.28918 loss)
I0523 01:01:33.431000 34682 sgd_solver.cpp:112] Iteration 23720, lr = 0.01
I0523 01:01:37.543303 34682 solver.cpp:239] Iteration 23730 (2.38612 iter/s, 4.1909s/10 iters), loss = 8.73229
I0523 01:01:37.543344 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73229 (* 1 = 8.73229 loss)
I0523 01:01:37.613049 34682 sgd_solver.cpp:112] Iteration 23730, lr = 0.01
I0523 01:01:41.616690 34682 solver.cpp:239] Iteration 23740 (2.45509 iter/s, 4.07317s/10 iters), loss = 8.3217
I0523 01:01:41.616744 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3217 (* 1 = 8.3217 loss)
I0523 01:01:41.675261 34682 sgd_solver.cpp:112] Iteration 23740, lr = 0.01
I0523 01:01:46.104102 34682 solver.cpp:239] Iteration 23750 (2.22857 iter/s, 4.48718s/10 iters), loss = 7.75622
I0523 01:01:46.104149 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75622 (* 1 = 7.75622 loss)
I0523 01:01:46.162704 34682 sgd_solver.cpp:112] Iteration 23750, lr = 0.01
I0523 01:01:51.462224 34682 solver.cpp:239] Iteration 23760 (1.86642 iter/s, 5.35786s/10 iters), loss = 8.55102
I0523 01:01:51.462265 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55102 (* 1 = 8.55102 loss)
I0523 01:01:51.535769 34682 sgd_solver.cpp:112] Iteration 23760, lr = 0.01
I0523 01:01:56.719524 34682 solver.cpp:239] Iteration 23770 (1.90221 iter/s, 5.25704s/10 iters), loss = 8.8433
I0523 01:01:56.719578 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8433 (* 1 = 8.8433 loss)
I0523 01:01:56.792188 34682 sgd_solver.cpp:112] Iteration 23770, lr = 0.01
I0523 01:02:02.539821 34682 solver.cpp:239] Iteration 23780 (1.71821 iter/s, 5.82001s/10 iters), loss = 8.50092
I0523 01:02:02.540109 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50092 (* 1 = 8.50092 loss)
I0523 01:02:03.078110 34682 sgd_solver.cpp:112] Iteration 23780, lr = 0.01
I0523 01:02:09.378444 34682 solver.cpp:239] Iteration 23790 (1.4624 iter/s, 6.83809s/10 iters), loss = 8.48128
I0523 01:02:09.378499 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48128 (* 1 = 8.48128 loss)
I0523 01:02:09.447543 34682 sgd_solver.cpp:112] Iteration 23790, lr = 0.01
I0523 01:02:14.552755 34682 solver.cpp:239] Iteration 23800 (1.93272 iter/s, 5.17405s/10 iters), loss = 8.56771
I0523 01:02:14.552804 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56771 (* 1 = 8.56771 loss)
I0523 01:02:14.948153 34682 sgd_solver.cpp:112] Iteration 23800, lr = 0.01
I0523 01:02:19.780799 34682 solver.cpp:239] Iteration 23810 (1.91287 iter/s, 5.22776s/10 iters), loss = 8.53757
I0523 01:02:19.780864 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53757 (* 1 = 8.53757 loss)
I0523 01:02:20.547462 34682 sgd_solver.cpp:112] Iteration 23810, lr = 0.01
I0523 01:02:25.404886 34682 solver.cpp:239] Iteration 23820 (1.77816 iter/s, 5.62379s/10 iters), loss = 7.8377
I0523 01:02:25.404942 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8377 (* 1 = 7.8377 loss)
I0523 01:02:25.819964 34682 sgd_solver.cpp:112] Iteration 23820, lr = 0.01
I0523 01:02:29.786494 34682 solver.cpp:239] Iteration 23830 (2.28239 iter/s, 4.38136s/10 iters), loss = 8.01077
I0523 01:02:29.786545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01077 (* 1 = 8.01077 loss)
I0523 01:02:30.476800 34682 sgd_solver.cpp:112] Iteration 23830, lr = 0.01
I0523 01:02:36.225262 34682 solver.cpp:239] Iteration 23840 (1.55317 iter/s, 6.43846s/10 iters), loss = 8.82179
I0523 01:02:36.225528 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82179 (* 1 = 8.82179 loss)
I0523 01:02:36.948674 34682 sgd_solver.cpp:112] Iteration 23840, lr = 0.01
I0523 01:02:40.984097 34682 solver.cpp:239] Iteration 23850 (2.10154 iter/s, 4.75841s/10 iters), loss = 7.87441
I0523 01:02:40.984143 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87441 (* 1 = 7.87441 loss)
I0523 01:02:41.060948 34682 sgd_solver.cpp:112] Iteration 23850, lr = 0.01
I0523 01:02:44.387305 34682 solver.cpp:239] Iteration 23860 (2.93858 iter/s, 3.40301s/10 iters), loss = 8.35337
I0523 01:02:44.387362 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35337 (* 1 = 8.35337 loss)
I0523 01:02:44.456550 34682 sgd_solver.cpp:112] Iteration 23860, lr = 0.01
I0523 01:02:49.912895 34682 solver.cpp:239] Iteration 23870 (1.80985 iter/s, 5.52532s/10 iters), loss = 8.76556
I0523 01:02:49.912936 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76556 (* 1 = 8.76556 loss)
I0523 01:02:49.977200 34682 sgd_solver.cpp:112] Iteration 23870, lr = 0.01
I0523 01:02:54.046980 34682 solver.cpp:239] Iteration 23880 (2.41904 iter/s, 4.13387s/10 iters), loss = 8.73267
I0523 01:02:54.047035 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73267 (* 1 = 8.73267 loss)
I0523 01:02:54.119758 34682 sgd_solver.cpp:112] Iteration 23880, lr = 0.01
I0523 01:02:59.791553 34682 solver.cpp:239] Iteration 23890 (1.74086 iter/s, 5.74428s/10 iters), loss = 7.71122
I0523 01:02:59.791615 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.71122 (* 1 = 7.71122 loss)
I0523 01:03:00.443393 34682 sgd_solver.cpp:112] Iteration 23890, lr = 0.01
I0523 01:03:05.436071 34682 solver.cpp:239] Iteration 23900 (1.77172 iter/s, 5.64423s/10 iters), loss = 8.6346
I0523 01:03:05.436120 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6346 (* 1 = 8.6346 loss)
I0523 01:03:05.498333 34682 sgd_solver.cpp:112] Iteration 23900, lr = 0.01
I0523 01:03:10.077713 34682 solver.cpp:239] Iteration 23910 (2.15452 iter/s, 4.6414s/10 iters), loss = 9.42065
I0523 01:03:10.077965 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42065 (* 1 = 9.42065 loss)
I0523 01:03:10.908036 34682 sgd_solver.cpp:112] Iteration 23910, lr = 0.01
I0523 01:03:16.421629 34682 solver.cpp:239] Iteration 23920 (1.57643 iter/s, 6.34345s/10 iters), loss = 8.42844
I0523 01:03:16.421677 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42844 (* 1 = 8.42844 loss)
I0523 01:03:17.146644 34682 sgd_solver.cpp:112] Iteration 23920, lr = 0.01
I0523 01:03:21.179711 34682 solver.cpp:239] Iteration 23930 (2.10179 iter/s, 4.75784s/10 iters), loss = 8.91162
I0523 01:03:21.179761 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91162 (* 1 = 8.91162 loss)
I0523 01:03:21.921315 34682 sgd_solver.cpp:112] Iteration 23930, lr = 0.01
I0523 01:03:25.786757 34682 solver.cpp:239] Iteration 23940 (2.17071 iter/s, 4.60678s/10 iters), loss = 8.0977
I0523 01:03:25.786813 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0977 (* 1 = 8.0977 loss)
I0523 01:03:26.505602 34682 sgd_solver.cpp:112] Iteration 23940, lr = 0.01
I0523 01:03:30.032346 34682 solver.cpp:239] Iteration 23950 (2.35552 iter/s, 4.24534s/10 iters), loss = 8.53469
I0523 01:03:30.032399 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53469 (* 1 = 8.53469 loss)
I0523 01:03:30.842232 34682 sgd_solver.cpp:112] Iteration 23950, lr = 0.01
I0523 01:03:34.959668 34682 solver.cpp:239] Iteration 23960 (2.02961 iter/s, 4.92707s/10 iters), loss = 8.76513
I0523 01:03:34.959712 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76513 (* 1 = 8.76513 loss)
I0523 01:03:35.037832 34682 sgd_solver.cpp:112] Iteration 23960, lr = 0.01
I0523 01:03:40.345450 34682 solver.cpp:239] Iteration 23970 (1.85683 iter/s, 5.38551s/10 iters), loss = 8.09442
I0523 01:03:40.345541 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09442 (* 1 = 8.09442 loss)
I0523 01:03:40.406985 34682 sgd_solver.cpp:112] Iteration 23970, lr = 0.01
I0523 01:03:45.376842 34682 solver.cpp:239] Iteration 23980 (1.98764 iter/s, 5.03109s/10 iters), loss = 8.58532
I0523 01:03:45.376893 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58532 (* 1 = 8.58532 loss)
I0523 01:03:45.437364 34682 sgd_solver.cpp:112] Iteration 23980, lr = 0.01
I0523 01:03:49.109221 34682 solver.cpp:239] Iteration 23990 (2.6794 iter/s, 3.73218s/10 iters), loss = 8.84278
I0523 01:03:49.109262 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84278 (* 1 = 8.84278 loss)
I0523 01:03:49.951771 34682 sgd_solver.cpp:112] Iteration 23990, lr = 0.01
I0523 01:03:55.165108 34682 solver.cpp:239] Iteration 24000 (1.65137 iter/s, 6.05559s/10 iters), loss = 8.85377
I0523 01:03:55.165154 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85377 (* 1 = 8.85377 loss)
I0523 01:03:55.938102 34682 sgd_solver.cpp:112] Iteration 24000, lr = 0.01
I0523 01:03:59.188894 34682 solver.cpp:239] Iteration 24010 (2.48536 iter/s, 4.02357s/10 iters), loss = 9.30529
I0523 01:03:59.188946 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30529 (* 1 = 9.30529 loss)
I0523 01:04:00.025581 34682 sgd_solver.cpp:112] Iteration 24010, lr = 0.01
I0523 01:04:05.533933 34682 solver.cpp:239] Iteration 24020 (1.57611 iter/s, 6.34473s/10 iters), loss = 7.928
I0523 01:04:05.533995 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.928 (* 1 = 7.928 loss)
I0523 01:04:06.369145 34682 sgd_solver.cpp:112] Iteration 24020, lr = 0.01
I0523 01:04:12.950150 34682 solver.cpp:239] Iteration 24030 (1.34846 iter/s, 7.41586s/10 iters), loss = 9.35754
I0523 01:04:12.950492 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35754 (* 1 = 9.35754 loss)
I0523 01:04:13.026401 34682 sgd_solver.cpp:112] Iteration 24030, lr = 0.01
I0523 01:04:18.375880 34682 solver.cpp:239] Iteration 24040 (1.84407 iter/s, 5.42279s/10 iters), loss = 7.82296
I0523 01:04:18.375928 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82296 (* 1 = 7.82296 loss)
I0523 01:04:18.439388 34682 sgd_solver.cpp:112] Iteration 24040, lr = 0.01
I0523 01:04:23.373594 34682 solver.cpp:239] Iteration 24050 (2.00102 iter/s, 4.99746s/10 iters), loss = 8.62776
I0523 01:04:23.373652 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62776 (* 1 = 8.62776 loss)
I0523 01:04:23.544781 34682 sgd_solver.cpp:112] Iteration 24050, lr = 0.01
I0523 01:04:27.923457 34682 solver.cpp:239] Iteration 24060 (2.19799 iter/s, 4.54961s/10 iters), loss = 8.22448
I0523 01:04:27.923519 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22448 (* 1 = 8.22448 loss)
I0523 01:04:27.982174 34682 sgd_solver.cpp:112] Iteration 24060, lr = 0.01
I0523 01:04:33.697233 34682 solver.cpp:239] Iteration 24070 (1.73206 iter/s, 5.77348s/10 iters), loss = 7.97584
I0523 01:04:33.697285 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97584 (* 1 = 7.97584 loss)
I0523 01:04:33.770933 34682 sgd_solver.cpp:112] Iteration 24070, lr = 0.01
I0523 01:04:39.779170 34682 solver.cpp:239] Iteration 24080 (1.6443 iter/s, 6.08162s/10 iters), loss = 8.91734
I0523 01:04:39.779217 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91734 (* 1 = 8.91734 loss)
I0523 01:04:39.838714 34682 sgd_solver.cpp:112] Iteration 24080, lr = 0.01
I0523 01:04:43.282069 34682 solver.cpp:239] Iteration 24090 (2.85493 iter/s, 3.50271s/10 iters), loss = 9.20867
I0523 01:04:43.282308 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20867 (* 1 = 9.20867 loss)
I0523 01:04:43.874402 34682 sgd_solver.cpp:112] Iteration 24090, lr = 0.01
I0523 01:04:48.120905 34682 solver.cpp:239] Iteration 24100 (2.06678 iter/s, 4.83844s/10 iters), loss = 9.64798
I0523 01:04:48.120967 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.64798 (* 1 = 9.64798 loss)
I0523 01:04:48.222676 34682 sgd_solver.cpp:112] Iteration 24100, lr = 0.01
I0523 01:04:52.660814 34682 solver.cpp:239] Iteration 24110 (2.2028 iter/s, 4.53967s/10 iters), loss = 9.16196
I0523 01:04:52.660861 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16196 (* 1 = 9.16196 loss)
I0523 01:04:53.552863 34682 sgd_solver.cpp:112] Iteration 24110, lr = 0.01
I0523 01:04:58.299494 34682 solver.cpp:239] Iteration 24120 (1.77355 iter/s, 5.63841s/10 iters), loss = 8.91954
I0523 01:04:58.299540 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91954 (* 1 = 8.91954 loss)
I0523 01:04:58.374481 34682 sgd_solver.cpp:112] Iteration 24120, lr = 0.01
I0523 01:05:02.066902 34682 solver.cpp:239] Iteration 24130 (2.65449 iter/s, 3.7672s/10 iters), loss = 8.38386
I0523 01:05:02.066956 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38386 (* 1 = 8.38386 loss)
I0523 01:05:02.876907 34682 sgd_solver.cpp:112] Iteration 24130, lr = 0.01
I0523 01:05:07.483182 34682 solver.cpp:239] Iteration 24140 (1.84638 iter/s, 5.41601s/10 iters), loss = 8.84352
I0523 01:05:07.483232 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84352 (* 1 = 8.84352 loss)
I0523 01:05:07.557183 34682 sgd_solver.cpp:112] Iteration 24140, lr = 0.01
I0523 01:05:11.601658 34682 solver.cpp:239] Iteration 24150 (2.42821 iter/s, 4.11825s/10 iters), loss = 9.37408
I0523 01:05:11.601711 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.37408 (* 1 = 9.37408 loss)
I0523 01:05:11.665036 34682 sgd_solver.cpp:112] Iteration 24150, lr = 0.01
I0523 01:05:15.824692 34682 solver.cpp:239] Iteration 24160 (2.36809 iter/s, 4.22281s/10 iters), loss = 9.06204
I0523 01:05:15.824905 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06204 (* 1 = 9.06204 loss)
I0523 01:05:16.473690 34682 sgd_solver.cpp:112] Iteration 24160, lr = 0.01
I0523 01:05:21.974172 34682 solver.cpp:239] Iteration 24170 (1.62628 iter/s, 6.14902s/10 iters), loss = 9.2797
I0523 01:05:21.974225 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2797 (* 1 = 9.2797 loss)
I0523 01:05:22.618336 34682 sgd_solver.cpp:112] Iteration 24170, lr = 0.01
I0523 01:05:28.087672 34682 solver.cpp:239] Iteration 24180 (1.63581 iter/s, 6.11319s/10 iters), loss = 9.67156
I0523 01:05:28.087728 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.67156 (* 1 = 9.67156 loss)
I0523 01:05:28.272729 34682 sgd_solver.cpp:112] Iteration 24180, lr = 0.01
I0523 01:05:33.019853 34682 solver.cpp:239] Iteration 24190 (2.02761 iter/s, 4.93192s/10 iters), loss = 8.73968
I0523 01:05:33.019915 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73968 (* 1 = 8.73968 loss)
I0523 01:05:33.081584 34682 sgd_solver.cpp:112] Iteration 24190, lr = 0.01
I0523 01:05:37.739303 34682 solver.cpp:239] Iteration 24200 (2.11901 iter/s, 4.7192s/10 iters), loss = 8.69048
I0523 01:05:37.739362 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69048 (* 1 = 8.69048 loss)
I0523 01:05:37.812907 34682 sgd_solver.cpp:112] Iteration 24200, lr = 0.01
I0523 01:05:43.417356 34682 solver.cpp:239] Iteration 24210 (1.76126 iter/s, 5.67777s/10 iters), loss = 7.88787
I0523 01:05:43.417399 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88787 (* 1 = 7.88787 loss)
I0523 01:05:43.486726 34682 sgd_solver.cpp:112] Iteration 24210, lr = 0.01
I0523 01:05:47.443500 34682 solver.cpp:239] Iteration 24220 (2.4839 iter/s, 4.02593s/10 iters), loss = 9.51358
I0523 01:05:47.443630 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51358 (* 1 = 9.51358 loss)
I0523 01:05:47.503737 34682 sgd_solver.cpp:112] Iteration 24220, lr = 0.01
I0523 01:05:51.843405 34682 solver.cpp:239] Iteration 24230 (2.27294 iter/s, 4.39959s/10 iters), loss = 8.67809
I0523 01:05:51.843448 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67809 (* 1 = 8.67809 loss)
I0523 01:05:52.621404 34682 sgd_solver.cpp:112] Iteration 24230, lr = 0.01
I0523 01:05:59.225297 34682 solver.cpp:239] Iteration 24240 (1.35473 iter/s, 7.38155s/10 iters), loss = 9.07363
I0523 01:05:59.225349 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07363 (* 1 = 9.07363 loss)
I0523 01:05:59.300423 34682 sgd_solver.cpp:112] Iteration 24240, lr = 0.01
I0523 01:06:04.657781 34682 solver.cpp:239] Iteration 24250 (1.84087 iter/s, 5.43221s/10 iters), loss = 8.31432
I0523 01:06:04.657822 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31432 (* 1 = 8.31432 loss)
I0523 01:06:04.735276 34682 sgd_solver.cpp:112] Iteration 24250, lr = 0.01
I0523 01:06:08.734452 34682 solver.cpp:239] Iteration 24260 (2.45311 iter/s, 4.07646s/10 iters), loss = 8.59518
I0523 01:06:08.734493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59518 (* 1 = 8.59518 loss)
I0523 01:06:08.803002 34682 sgd_solver.cpp:112] Iteration 24260, lr = 0.01
I0523 01:06:13.533004 34682 solver.cpp:239] Iteration 24270 (2.08407 iter/s, 4.79831s/10 iters), loss = 7.74601
I0523 01:06:13.533049 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74601 (* 1 = 7.74601 loss)
I0523 01:06:13.601600 34682 sgd_solver.cpp:112] Iteration 24270, lr = 0.01
I0523 01:06:17.803812 34682 solver.cpp:239] Iteration 24280 (2.3416 iter/s, 4.27059s/10 iters), loss = 8.8385
I0523 01:06:17.804028 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8385 (* 1 = 8.8385 loss)
I0523 01:06:17.866343 34682 sgd_solver.cpp:112] Iteration 24280, lr = 0.01
I0523 01:06:22.983839 34682 solver.cpp:239] Iteration 24290 (1.93064 iter/s, 5.17963s/10 iters), loss = 8.72211
I0523 01:06:22.983891 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72211 (* 1 = 8.72211 loss)
I0523 01:06:23.661826 34682 sgd_solver.cpp:112] Iteration 24290, lr = 0.01
I0523 01:06:28.547106 34682 solver.cpp:239] Iteration 24300 (1.7976 iter/s, 5.56299s/10 iters), loss = 8.40969
I0523 01:06:28.547170 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40969 (* 1 = 8.40969 loss)
I0523 01:06:29.377943 34682 sgd_solver.cpp:112] Iteration 24300, lr = 0.01
I0523 01:06:33.514042 34682 solver.cpp:239] Iteration 24310 (2.01342 iter/s, 4.96667s/10 iters), loss = 8.234
I0523 01:06:33.514094 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.234 (* 1 = 8.234 loss)
I0523 01:06:33.572721 34682 sgd_solver.cpp:112] Iteration 24310, lr = 0.01
I0523 01:06:39.276659 34682 solver.cpp:239] Iteration 24320 (1.73541 iter/s, 5.76232s/10 iters), loss = 8.55197
I0523 01:06:39.276705 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55197 (* 1 = 8.55197 loss)
I0523 01:06:39.362187 34682 sgd_solver.cpp:112] Iteration 24320, lr = 0.01
I0523 01:06:44.008435 34682 solver.cpp:239] Iteration 24330 (2.11348 iter/s, 4.73154s/10 iters), loss = 8.96224
I0523 01:06:44.008482 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96224 (* 1 = 8.96224 loss)
I0523 01:06:44.093737 34682 sgd_solver.cpp:112] Iteration 24330, lr = 0.01
I0523 01:06:49.986151 34682 solver.cpp:239] Iteration 24340 (1.67296 iter/s, 5.97743s/10 iters), loss = 8.94395
I0523 01:06:49.986418 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94395 (* 1 = 8.94395 loss)
I0523 01:06:50.610087 34682 sgd_solver.cpp:112] Iteration 24340, lr = 0.01
I0523 01:06:53.174525 34682 solver.cpp:239] Iteration 24350 (3.13676 iter/s, 3.188s/10 iters), loss = 8.55337
I0523 01:06:53.174576 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55337 (* 1 = 8.55337 loss)
I0523 01:06:53.247189 34682 sgd_solver.cpp:112] Iteration 24350, lr = 0.01
I0523 01:06:58.216575 34682 solver.cpp:239] Iteration 24360 (1.98342 iter/s, 5.0418s/10 iters), loss = 9.08933
I0523 01:06:58.216615 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08933 (* 1 = 9.08933 loss)
I0523 01:06:58.286733 34682 sgd_solver.cpp:112] Iteration 24360, lr = 0.01
I0523 01:07:03.882366 34682 solver.cpp:239] Iteration 24370 (1.76506 iter/s, 5.66552s/10 iters), loss = 8.924
I0523 01:07:03.882408 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.924 (* 1 = 8.924 loss)
I0523 01:07:04.016680 34682 sgd_solver.cpp:112] Iteration 24370, lr = 0.01
I0523 01:07:09.936967 34682 solver.cpp:239] Iteration 24380 (1.65172 iter/s, 6.05431s/10 iters), loss = 8.20998
I0523 01:07:09.937024 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20998 (* 1 = 8.20998 loss)
I0523 01:07:10.011032 34682 sgd_solver.cpp:112] Iteration 24380, lr = 0.01
I0523 01:07:13.176960 34682 solver.cpp:239] Iteration 24390 (3.08661 iter/s, 3.2398s/10 iters), loss = 8.85979
I0523 01:07:13.177022 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85979 (* 1 = 8.85979 loss)
I0523 01:07:13.927544 34682 sgd_solver.cpp:112] Iteration 24390, lr = 0.01
I0523 01:07:18.551421 34682 solver.cpp:239] Iteration 24400 (1.86075 iter/s, 5.37418s/10 iters), loss = 8.30869
I0523 01:07:18.551483 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30869 (* 1 = 8.30869 loss)
I0523 01:07:19.377682 34682 sgd_solver.cpp:112] Iteration 24400, lr = 0.01
I0523 01:07:24.136523 34682 solver.cpp:239] Iteration 24410 (1.79057 iter/s, 5.58481s/10 iters), loss = 8.6133
I0523 01:07:24.136783 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6133 (* 1 = 8.6133 loss)
I0523 01:07:24.719585 34682 sgd_solver.cpp:112] Iteration 24410, lr = 0.01
I0523 01:07:28.956115 34682 solver.cpp:239] Iteration 24420 (2.07504 iter/s, 4.81918s/10 iters), loss = 9.07764
I0523 01:07:28.956164 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07764 (* 1 = 9.07764 loss)
I0523 01:07:29.604398 34682 sgd_solver.cpp:112] Iteration 24420, lr = 0.01
I0523 01:07:32.284847 34682 solver.cpp:239] Iteration 24430 (3.00431 iter/s, 3.32855s/10 iters), loss = 8.91733
I0523 01:07:32.284890 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91733 (* 1 = 8.91733 loss)
I0523 01:07:32.345623 34682 sgd_solver.cpp:112] Iteration 24430, lr = 0.01
I0523 01:07:37.343505 34682 solver.cpp:239] Iteration 24440 (1.97692 iter/s, 5.05838s/10 iters), loss = 8.44585
I0523 01:07:37.343574 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44585 (* 1 = 8.44585 loss)
I0523 01:07:38.172857 34682 sgd_solver.cpp:112] Iteration 24440, lr = 0.01
I0523 01:07:41.833081 34682 solver.cpp:239] Iteration 24450 (2.22752 iter/s, 4.48931s/10 iters), loss = 9.48868
I0523 01:07:41.833135 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.48868 (* 1 = 9.48868 loss)
I0523 01:07:42.603660 34682 sgd_solver.cpp:112] Iteration 24450, lr = 0.01
I0523 01:07:47.214654 34682 solver.cpp:239] Iteration 24460 (1.85829 iter/s, 5.38129s/10 iters), loss = 8.67268
I0523 01:07:47.214747 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67268 (* 1 = 8.67268 loss)
I0523 01:07:47.300638 34682 sgd_solver.cpp:112] Iteration 24460, lr = 0.01
I0523 01:07:51.418033 34682 solver.cpp:239] Iteration 24470 (2.37919 iter/s, 4.20311s/10 iters), loss = 8.5127
I0523 01:07:51.418076 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5127 (* 1 = 8.5127 loss)
I0523 01:07:51.494591 34682 sgd_solver.cpp:112] Iteration 24470, lr = 0.01
I0523 01:07:57.379779 34682 solver.cpp:239] Iteration 24480 (1.67744 iter/s, 5.96146s/10 iters), loss = 8.86698
I0523 01:07:57.379962 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86698 (* 1 = 8.86698 loss)
I0523 01:07:57.453294 34682 sgd_solver.cpp:112] Iteration 24480, lr = 0.01
I0523 01:08:01.865345 34682 solver.cpp:239] Iteration 24490 (2.22956 iter/s, 4.4852s/10 iters), loss = 9.71103
I0523 01:08:01.865387 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.71103 (* 1 = 9.71103 loss)
I0523 01:08:01.941807 34682 sgd_solver.cpp:112] Iteration 24490, lr = 0.01
I0523 01:08:06.448489 34682 solver.cpp:239] Iteration 24500 (2.18202 iter/s, 4.5829s/10 iters), loss = 8.3228
I0523 01:08:06.448566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3228 (* 1 = 8.3228 loss)
I0523 01:08:06.526569 34682 sgd_solver.cpp:112] Iteration 24500, lr = 0.01
I0523 01:08:11.683928 34682 solver.cpp:239] Iteration 24510 (1.91017 iter/s, 5.23514s/10 iters), loss = 9.01221
I0523 01:08:11.683990 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01221 (* 1 = 9.01221 loss)
I0523 01:08:12.426744 34682 sgd_solver.cpp:112] Iteration 24510, lr = 0.01
I0523 01:08:16.429361 34682 solver.cpp:239] Iteration 24520 (2.1074 iter/s, 4.74518s/10 iters), loss = 9.0333
I0523 01:08:16.429414 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0333 (* 1 = 9.0333 loss)
I0523 01:08:17.165508 34682 sgd_solver.cpp:112] Iteration 24520, lr = 0.01
I0523 01:08:23.307807 34682 solver.cpp:239] Iteration 24530 (1.45388 iter/s, 6.87812s/10 iters), loss = 8.60927
I0523 01:08:23.307852 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60927 (* 1 = 8.60927 loss)
I0523 01:08:23.375063 34682 sgd_solver.cpp:112] Iteration 24530, lr = 0.01
I0523 01:08:28.794420 34682 solver.cpp:239] Iteration 24540 (1.82271 iter/s, 5.48634s/10 iters), loss = 8.38012
I0523 01:08:28.794667 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38012 (* 1 = 8.38012 loss)
I0523 01:08:29.065052 34682 sgd_solver.cpp:112] Iteration 24540, lr = 0.01
I0523 01:08:32.505384 34682 solver.cpp:239] Iteration 24550 (2.69498 iter/s, 3.71061s/10 iters), loss = 9.25559
I0523 01:08:32.505435 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25559 (* 1 = 9.25559 loss)
I0523 01:08:32.570102 34682 sgd_solver.cpp:112] Iteration 24550, lr = 0.01
I0523 01:08:36.659893 34682 solver.cpp:239] Iteration 24560 (2.40715 iter/s, 4.15429s/10 iters), loss = 8.81076
I0523 01:08:36.659946 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81076 (* 1 = 8.81076 loss)
I0523 01:08:37.483708 34682 sgd_solver.cpp:112] Iteration 24560, lr = 0.01
I0523 01:08:43.041290 34682 solver.cpp:239] Iteration 24570 (1.56713 iter/s, 6.38108s/10 iters), loss = 9.12717
I0523 01:08:43.041355 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12717 (* 1 = 9.12717 loss)
I0523 01:08:43.108646 34682 sgd_solver.cpp:112] Iteration 24570, lr = 0.01
I0523 01:08:48.171131 34682 solver.cpp:239] Iteration 24580 (1.94948 iter/s, 5.12957s/10 iters), loss = 8.2701
I0523 01:08:48.171190 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2701 (* 1 = 8.2701 loss)
I0523 01:08:48.995015 34682 sgd_solver.cpp:112] Iteration 24580, lr = 0.01
I0523 01:08:52.596998 34682 solver.cpp:239] Iteration 24590 (2.26075 iter/s, 4.4233s/10 iters), loss = 8.39104
I0523 01:08:52.597038 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39104 (* 1 = 8.39104 loss)
I0523 01:08:52.666956 34682 sgd_solver.cpp:112] Iteration 24590, lr = 0.01
I0523 01:08:57.704982 34682 solver.cpp:239] Iteration 24600 (1.95782 iter/s, 5.10772s/10 iters), loss = 8.97516
I0523 01:08:57.705034 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97516 (* 1 = 8.97516 loss)
I0523 01:08:57.770860 34682 sgd_solver.cpp:112] Iteration 24600, lr = 0.01
I0523 01:09:02.590934 34682 solver.cpp:239] Iteration 24610 (2.04679 iter/s, 4.8857s/10 iters), loss = 8.95499
I0523 01:09:02.591226 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95499 (* 1 = 8.95499 loss)
I0523 01:09:02.672837 34682 sgd_solver.cpp:112] Iteration 24610, lr = 0.01
I0523 01:09:06.455909 34682 solver.cpp:239] Iteration 24620 (2.58762 iter/s, 3.86456s/10 iters), loss = 9.04439
I0523 01:09:06.455960 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04439 (* 1 = 9.04439 loss)
I0523 01:09:06.528372 34682 sgd_solver.cpp:112] Iteration 24620, lr = 0.01
I0523 01:09:11.319941 34682 solver.cpp:239] Iteration 24630 (2.05601 iter/s, 4.86379s/10 iters), loss = 9.98673
I0523 01:09:11.319988 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.98673 (* 1 = 9.98673 loss)
I0523 01:09:12.157600 34682 sgd_solver.cpp:112] Iteration 24630, lr = 0.01
I0523 01:09:15.588490 34682 solver.cpp:239] Iteration 24640 (2.34284 iter/s, 4.26833s/10 iters), loss = 7.62067
I0523 01:09:15.588538 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62067 (* 1 = 7.62067 loss)
I0523 01:09:15.653115 34682 sgd_solver.cpp:112] Iteration 24640, lr = 0.01
I0523 01:09:22.214902 34682 solver.cpp:239] Iteration 24650 (1.50919 iter/s, 6.62608s/10 iters), loss = 8.50967
I0523 01:09:22.214952 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50967 (* 1 = 8.50967 loss)
I0523 01:09:22.275246 34682 sgd_solver.cpp:112] Iteration 24650, lr = 0.01
I0523 01:09:26.475508 34682 solver.cpp:239] Iteration 24660 (2.3472 iter/s, 4.26039s/10 iters), loss = 8.7758
I0523 01:09:26.475553 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7758 (* 1 = 8.7758 loss)
I0523 01:09:27.117131 34682 sgd_solver.cpp:112] Iteration 24660, lr = 0.01
I0523 01:09:30.036181 34682 solver.cpp:239] Iteration 24670 (2.80863 iter/s, 3.56046s/10 iters), loss = 7.96351
I0523 01:09:30.036248 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96351 (* 1 = 7.96351 loss)
I0523 01:09:30.878265 34682 sgd_solver.cpp:112] Iteration 24670, lr = 0.01
I0523 01:09:34.269341 34682 solver.cpp:239] Iteration 24680 (2.36244 iter/s, 4.23291s/10 iters), loss = 8.54098
I0523 01:09:34.269484 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54098 (* 1 = 8.54098 loss)
I0523 01:09:34.811130 34682 sgd_solver.cpp:112] Iteration 24680, lr = 0.01
I0523 01:09:40.426168 34682 solver.cpp:239] Iteration 24690 (1.62432 iter/s, 6.15644s/10 iters), loss = 7.7071
I0523 01:09:40.426214 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7071 (* 1 = 7.7071 loss)
I0523 01:09:40.801565 34682 sgd_solver.cpp:112] Iteration 24690, lr = 0.01
I0523 01:09:44.070331 34682 solver.cpp:239] Iteration 24700 (2.74427 iter/s, 3.64396s/10 iters), loss = 9.1378
I0523 01:09:44.070389 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1378 (* 1 = 9.1378 loss)
I0523 01:09:44.863865 34682 sgd_solver.cpp:112] Iteration 24700, lr = 0.01
I0523 01:09:49.761479 34682 solver.cpp:239] Iteration 24710 (1.7572 iter/s, 5.69086s/10 iters), loss = 8.59391
I0523 01:09:49.761528 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59391 (* 1 = 8.59391 loss)
I0523 01:09:50.528437 34682 sgd_solver.cpp:112] Iteration 24710, lr = 0.01
I0523 01:09:56.008301 34682 solver.cpp:239] Iteration 24720 (1.60089 iter/s, 6.24652s/10 iters), loss = 7.88015
I0523 01:09:56.008352 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88015 (* 1 = 7.88015 loss)
I0523 01:09:56.664952 34682 sgd_solver.cpp:112] Iteration 24720, lr = 0.01
I0523 01:10:01.321947 34682 solver.cpp:239] Iteration 24730 (1.88204 iter/s, 5.31338s/10 iters), loss = 9.1855
I0523 01:10:01.321987 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1855 (* 1 = 9.1855 loss)
I0523 01:10:01.402334 34682 sgd_solver.cpp:112] Iteration 24730, lr = 0.01
I0523 01:10:05.845827 34682 solver.cpp:239] Iteration 24740 (2.21061 iter/s, 4.52365s/10 iters), loss = 9.15053
I0523 01:10:05.846081 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15053 (* 1 = 9.15053 loss)
I0523 01:10:05.926651 34682 sgd_solver.cpp:112] Iteration 24740, lr = 0.01
I0523 01:10:11.445062 34682 solver.cpp:239] Iteration 24750 (1.7869 iter/s, 5.59629s/10 iters), loss = 9.19779
I0523 01:10:11.445119 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19779 (* 1 = 9.19779 loss)
I0523 01:10:11.505436 34682 sgd_solver.cpp:112] Iteration 24750, lr = 0.01
I0523 01:10:16.129766 34682 solver.cpp:239] Iteration 24760 (2.13472 iter/s, 4.68446s/10 iters), loss = 8.99598
I0523 01:10:16.129814 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99598 (* 1 = 8.99598 loss)
I0523 01:10:16.193330 34682 sgd_solver.cpp:112] Iteration 24760, lr = 0.01
I0523 01:10:23.170526 34682 solver.cpp:239] Iteration 24770 (1.42037 iter/s, 7.04042s/10 iters), loss = 9.54804
I0523 01:10:23.170593 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.54804 (* 1 = 9.54804 loss)
I0523 01:10:23.886065 34682 sgd_solver.cpp:112] Iteration 24770, lr = 0.01
I0523 01:10:28.180948 34682 solver.cpp:239] Iteration 24780 (1.99595 iter/s, 5.01015s/10 iters), loss = 9.16002
I0523 01:10:28.180995 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16002 (* 1 = 9.16002 loss)
I0523 01:10:28.239588 34682 sgd_solver.cpp:112] Iteration 24780, lr = 0.01
I0523 01:10:33.225958 34682 solver.cpp:239] Iteration 24790 (1.98226 iter/s, 5.04475s/10 iters), loss = 8.25888
I0523 01:10:33.226016 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25888 (* 1 = 8.25888 loss)
I0523 01:10:33.633332 34682 sgd_solver.cpp:112] Iteration 24790, lr = 0.01
I0523 01:10:37.653331 34682 solver.cpp:239] Iteration 24800 (2.25879 iter/s, 4.42714s/10 iters), loss = 9.55816
I0523 01:10:37.653450 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.55816 (* 1 = 9.55816 loss)
I0523 01:10:38.489472 34682 sgd_solver.cpp:112] Iteration 24800, lr = 0.01
I0523 01:10:42.843713 34682 solver.cpp:239] Iteration 24810 (1.92676 iter/s, 5.19006s/10 iters), loss = 7.70239
I0523 01:10:42.843753 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.70239 (* 1 = 7.70239 loss)
I0523 01:10:42.913007 34682 sgd_solver.cpp:112] Iteration 24810, lr = 0.01
I0523 01:10:46.109071 34682 solver.cpp:239] Iteration 24820 (3.06468 iter/s, 3.26299s/10 iters), loss = 8.27105
I0523 01:10:46.109127 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27105 (* 1 = 8.27105 loss)
I0523 01:10:46.176007 34682 sgd_solver.cpp:112] Iteration 24820, lr = 0.01
I0523 01:10:51.051311 34682 solver.cpp:239] Iteration 24830 (2.02348 iter/s, 4.94198s/10 iters), loss = 8.1069
I0523 01:10:51.051373 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1069 (* 1 = 8.1069 loss)
I0523 01:10:51.343555 34682 sgd_solver.cpp:112] Iteration 24830, lr = 0.01
I0523 01:10:56.062623 34682 solver.cpp:239] Iteration 24840 (1.99559 iter/s, 5.01105s/10 iters), loss = 8.83692
I0523 01:10:56.062667 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83692 (* 1 = 8.83692 loss)
I0523 01:10:56.833451 34682 sgd_solver.cpp:112] Iteration 24840, lr = 0.01
I0523 01:11:03.182919 34682 solver.cpp:239] Iteration 24850 (1.4045 iter/s, 7.11997s/10 iters), loss = 8.78624
I0523 01:11:03.182972 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78624 (* 1 = 8.78624 loss)
I0523 01:11:03.883090 34682 sgd_solver.cpp:112] Iteration 24850, lr = 0.01
I0523 01:11:07.979619 34682 solver.cpp:239] Iteration 24860 (2.08487 iter/s, 4.79645s/10 iters), loss = 8.83331
I0523 01:11:07.979876 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83331 (* 1 = 8.83331 loss)
I0523 01:11:08.048058 34682 sgd_solver.cpp:112] Iteration 24860, lr = 0.01
I0523 01:11:12.347936 34682 solver.cpp:239] Iteration 24870 (2.28943 iter/s, 4.3679s/10 iters), loss = 9.21372
I0523 01:11:12.347987 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21372 (* 1 = 9.21372 loss)
I0523 01:11:13.214776 34682 sgd_solver.cpp:112] Iteration 24870, lr = 0.01
I0523 01:11:17.615006 34682 solver.cpp:239] Iteration 24880 (1.89868 iter/s, 5.2668s/10 iters), loss = 8.46843
I0523 01:11:17.615059 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46843 (* 1 = 8.46843 loss)
I0523 01:11:18.275993 34682 sgd_solver.cpp:112] Iteration 24880, lr = 0.01
I0523 01:11:21.613690 34682 solver.cpp:239] Iteration 24890 (2.50096 iter/s, 3.99846s/10 iters), loss = 8.94561
I0523 01:11:21.613740 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94561 (* 1 = 8.94561 loss)
I0523 01:11:21.699767 34682 sgd_solver.cpp:112] Iteration 24890, lr = 0.01
I0523 01:11:25.396279 34682 solver.cpp:239] Iteration 24900 (2.64384 iter/s, 3.78238s/10 iters), loss = 9.29738
I0523 01:11:25.396335 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29738 (* 1 = 9.29738 loss)
I0523 01:11:25.455885 34682 sgd_solver.cpp:112] Iteration 24900, lr = 0.01
I0523 01:11:30.004487 34682 solver.cpp:239] Iteration 24910 (2.17015 iter/s, 4.60797s/10 iters), loss = 8.80097
I0523 01:11:30.004539 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80097 (* 1 = 8.80097 loss)
I0523 01:11:30.087016 34682 sgd_solver.cpp:112] Iteration 24910, lr = 0.01
I0523 01:11:36.916714 34682 solver.cpp:239] Iteration 24920 (1.44678 iter/s, 6.9119s/10 iters), loss = 8.9357
I0523 01:11:36.916759 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9357 (* 1 = 8.9357 loss)
I0523 01:11:37.355532 34682 sgd_solver.cpp:112] Iteration 24920, lr = 0.01
I0523 01:11:41.250052 34682 solver.cpp:239] Iteration 24930 (2.30782 iter/s, 4.3331s/10 iters), loss = 9.40198
I0523 01:11:41.250212 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40198 (* 1 = 9.40198 loss)
I0523 01:11:41.321938 34682 sgd_solver.cpp:112] Iteration 24930, lr = 0.01
I0523 01:11:45.358883 34682 solver.cpp:239] Iteration 24940 (2.43398 iter/s, 4.1085s/10 iters), loss = 8.65089
I0523 01:11:45.358932 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65089 (* 1 = 8.65089 loss)
I0523 01:11:45.440297 34682 sgd_solver.cpp:112] Iteration 24940, lr = 0.01
I0523 01:11:48.466310 34682 solver.cpp:239] Iteration 24950 (3.21828 iter/s, 3.10725s/10 iters), loss = 8.88284
I0523 01:11:48.466356 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88284 (* 1 = 8.88284 loss)
I0523 01:11:49.274493 34682 sgd_solver.cpp:112] Iteration 24950, lr = 0.01
I0523 01:11:53.318331 34682 solver.cpp:239] Iteration 24960 (2.0611 iter/s, 4.85177s/10 iters), loss = 8.96261
I0523 01:11:53.318403 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96261 (* 1 = 8.96261 loss)
I0523 01:11:53.973219 34682 sgd_solver.cpp:112] Iteration 24960, lr = 0.01
I0523 01:11:59.158519 34682 solver.cpp:239] Iteration 24970 (1.71236 iter/s, 5.83988s/10 iters), loss = 8.43466
I0523 01:11:59.158572 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43466 (* 1 = 8.43466 loss)
I0523 01:11:59.227159 34682 sgd_solver.cpp:112] Iteration 24970, lr = 0.01
I0523 01:12:03.935744 34682 solver.cpp:239] Iteration 24980 (2.09338 iter/s, 4.77697s/10 iters), loss = 9.15524
I0523 01:12:03.935788 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15524 (* 1 = 9.15524 loss)
I0523 01:12:04.002821 34682 sgd_solver.cpp:112] Iteration 24980, lr = 0.01
I0523 01:12:06.586230 34682 solver.cpp:239] Iteration 24990 (3.77314 iter/s, 2.65031s/10 iters), loss = 8.54216
I0523 01:12:06.586287 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54216 (* 1 = 8.54216 loss)
I0523 01:12:07.412578 34682 sgd_solver.cpp:112] Iteration 24990, lr = 0.01
I0523 01:12:12.780673 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_25000.caffemodel
I0523 01:12:14.040249 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_25000.solverstate
I0523 01:12:14.233064 34682 solver.cpp:239] Iteration 25000 (1.30779 iter/s, 7.64648s/10 iters), loss = 8.21696
I0523 01:12:14.233101 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21696 (* 1 = 8.21696 loss)
I0523 01:12:14.315436 34682 sgd_solver.cpp:112] Iteration 25000, lr = 0.01
I0523 01:12:20.481003 34682 solver.cpp:239] Iteration 25010 (1.6006 iter/s, 6.24765s/10 iters), loss = 8.80141
I0523 01:12:20.481050 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80141 (* 1 = 8.80141 loss)
I0523 01:12:20.554713 34682 sgd_solver.cpp:112] Iteration 25010, lr = 0.01
I0523 01:12:26.998775 34682 solver.cpp:239] Iteration 25020 (1.53538 iter/s, 6.51306s/10 iters), loss = 9.10498
I0523 01:12:26.998821 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10498 (* 1 = 9.10498 loss)
I0523 01:12:27.605381 34682 sgd_solver.cpp:112] Iteration 25020, lr = 0.01
I0523 01:12:31.702440 34682 solver.cpp:239] Iteration 25030 (2.12611 iter/s, 4.70342s/10 iters), loss = 8.81217
I0523 01:12:31.702495 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81217 (* 1 = 8.81217 loss)
I0523 01:12:32.506893 34682 sgd_solver.cpp:112] Iteration 25030, lr = 0.01
I0523 01:12:36.574584 34682 solver.cpp:239] Iteration 25040 (2.05259 iter/s, 4.87189s/10 iters), loss = 9.00228
I0523 01:12:36.574632 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00228 (* 1 = 9.00228 loss)
I0523 01:12:36.652026 34682 sgd_solver.cpp:112] Iteration 25040, lr = 0.01
I0523 01:12:41.209146 34682 solver.cpp:239] Iteration 25050 (2.15781 iter/s, 4.63432s/10 iters), loss = 8.43569
I0523 01:12:41.209192 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43569 (* 1 = 8.43569 loss)
I0523 01:12:41.273844 34682 sgd_solver.cpp:112] Iteration 25050, lr = 0.01
I0523 01:12:46.842365 34682 solver.cpp:239] Iteration 25060 (1.77527 iter/s, 5.63295s/10 iters), loss = 9.51035
I0523 01:12:46.842609 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51035 (* 1 = 9.51035 loss)
I0523 01:12:46.902688 34682 sgd_solver.cpp:112] Iteration 25060, lr = 0.01
I0523 01:12:53.370312 34682 solver.cpp:239] Iteration 25070 (1.53199 iter/s, 6.52746s/10 iters), loss = 8.43062
I0523 01:12:53.370374 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43062 (* 1 = 8.43062 loss)
I0523 01:12:53.440050 34682 sgd_solver.cpp:112] Iteration 25070, lr = 0.01
I0523 01:12:58.174971 34682 solver.cpp:239] Iteration 25080 (2.08142 iter/s, 4.8044s/10 iters), loss = 9.35148
I0523 01:12:58.175019 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35148 (* 1 = 9.35148 loss)
I0523 01:12:58.989065 34682 sgd_solver.cpp:112] Iteration 25080, lr = 0.01
I0523 01:13:05.209264 34682 solver.cpp:239] Iteration 25090 (1.42167 iter/s, 7.03396s/10 iters), loss = 9.14321
I0523 01:13:05.209324 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14321 (* 1 = 9.14321 loss)
I0523 01:13:05.285964 34682 sgd_solver.cpp:112] Iteration 25090, lr = 0.01
I0523 01:13:08.253510 34682 solver.cpp:239] Iteration 25100 (3.28509 iter/s, 3.04406s/10 iters), loss = 8.29967
I0523 01:13:08.253566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29967 (* 1 = 8.29967 loss)
I0523 01:13:09.045112 34682 sgd_solver.cpp:112] Iteration 25100, lr = 0.01
I0523 01:13:13.111898 34682 solver.cpp:239] Iteration 25110 (2.05841 iter/s, 4.85812s/10 iters), loss = 8.81054
I0523 01:13:13.111970 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81054 (* 1 = 8.81054 loss)
I0523 01:13:13.973835 34682 sgd_solver.cpp:112] Iteration 25110, lr = 0.01
I0523 01:13:18.502542 34682 solver.cpp:239] Iteration 25120 (1.85517 iter/s, 5.39035s/10 iters), loss = 7.83616
I0523 01:13:18.502830 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83616 (* 1 = 7.83616 loss)
I0523 01:13:18.569687 34682 sgd_solver.cpp:112] Iteration 25120, lr = 0.01
I0523 01:13:22.927866 34682 solver.cpp:239] Iteration 25130 (2.25995 iter/s, 4.42488s/10 iters), loss = 8.97746
I0523 01:13:22.927923 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97746 (* 1 = 8.97746 loss)
I0523 01:13:23.758044 34682 sgd_solver.cpp:112] Iteration 25130, lr = 0.01
I0523 01:13:29.324357 34682 solver.cpp:239] Iteration 25140 (1.56343 iter/s, 6.39618s/10 iters), loss = 8.91187
I0523 01:13:29.324404 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91187 (* 1 = 8.91187 loss)
I0523 01:13:29.398066 34682 sgd_solver.cpp:112] Iteration 25140, lr = 0.01
I0523 01:13:35.421850 34682 solver.cpp:239] Iteration 25150 (1.6401 iter/s, 6.09719s/10 iters), loss = 8.81544
I0523 01:13:35.421916 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81544 (* 1 = 8.81544 loss)
I0523 01:13:36.202980 34682 sgd_solver.cpp:112] Iteration 25150, lr = 0.01
I0523 01:13:39.524238 34682 solver.cpp:239] Iteration 25160 (2.43775 iter/s, 4.10215s/10 iters), loss = 8.3772
I0523 01:13:39.524286 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3772 (* 1 = 8.3772 loss)
I0523 01:13:39.600771 34682 sgd_solver.cpp:112] Iteration 25160, lr = 0.01
I0523 01:13:45.087921 34682 solver.cpp:239] Iteration 25170 (1.79746 iter/s, 5.56341s/10 iters), loss = 8.87981
I0523 01:13:45.087961 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87981 (* 1 = 8.87981 loss)
I0523 01:13:45.160131 34682 sgd_solver.cpp:112] Iteration 25170, lr = 0.01
I0523 01:13:51.316723 34682 solver.cpp:239] Iteration 25180 (1.60552 iter/s, 6.22851s/10 iters), loss = 8.66992
I0523 01:13:51.316840 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66992 (* 1 = 8.66992 loss)
I0523 01:13:51.392897 34682 sgd_solver.cpp:112] Iteration 25180, lr = 0.01
I0523 01:13:55.622540 34682 solver.cpp:239] Iteration 25190 (2.3226 iter/s, 4.30552s/10 iters), loss = 8.87126
I0523 01:13:55.622596 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87126 (* 1 = 8.87126 loss)
I0523 01:13:56.409086 34682 sgd_solver.cpp:112] Iteration 25190, lr = 0.01
I0523 01:14:02.050691 34682 solver.cpp:239] Iteration 25200 (1.55574 iter/s, 6.42782s/10 iters), loss = 8.27559
I0523 01:14:02.050760 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27559 (* 1 = 8.27559 loss)
I0523 01:14:02.113646 34682 sgd_solver.cpp:112] Iteration 25200, lr = 0.01
I0523 01:14:06.560456 34682 solver.cpp:239] Iteration 25210 (2.21753 iter/s, 4.50952s/10 iters), loss = 8.91344
I0523 01:14:06.560495 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91344 (* 1 = 8.91344 loss)
I0523 01:14:06.632818 34682 sgd_solver.cpp:112] Iteration 25210, lr = 0.01
I0523 01:14:12.232143 34682 solver.cpp:239] Iteration 25220 (1.76323 iter/s, 5.67142s/10 iters), loss = 9.36663
I0523 01:14:12.232204 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36663 (* 1 = 9.36663 loss)
I0523 01:14:12.304942 34682 sgd_solver.cpp:112] Iteration 25220, lr = 0.01
I0523 01:14:17.080092 34682 solver.cpp:239] Iteration 25230 (2.06284 iter/s, 4.84769s/10 iters), loss = 7.86014
I0523 01:14:17.080145 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86014 (* 1 = 7.86014 loss)
I0523 01:14:17.942996 34682 sgd_solver.cpp:112] Iteration 25230, lr = 0.01
I0523 01:14:22.380883 34682 solver.cpp:239] Iteration 25240 (1.88661 iter/s, 5.30052s/10 iters), loss = 9.28519
I0523 01:14:22.381146 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28519 (* 1 = 9.28519 loss)
I0523 01:14:23.152061 34682 sgd_solver.cpp:112] Iteration 25240, lr = 0.01
I0523 01:14:27.724964 34682 solver.cpp:239] Iteration 25250 (1.87139 iter/s, 5.34361s/10 iters), loss = 8.73484
I0523 01:14:27.725008 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73484 (* 1 = 8.73484 loss)
I0523 01:14:27.794234 34682 sgd_solver.cpp:112] Iteration 25250, lr = 0.01
I0523 01:14:31.056478 34682 solver.cpp:239] Iteration 25260 (3.00181 iter/s, 3.33132s/10 iters), loss = 8.52975
I0523 01:14:31.056536 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52975 (* 1 = 8.52975 loss)
I0523 01:14:31.910897 34682 sgd_solver.cpp:112] Iteration 25260, lr = 0.01
I0523 01:14:36.700260 34682 solver.cpp:239] Iteration 25270 (1.77195 iter/s, 5.64349s/10 iters), loss = 8.98088
I0523 01:14:36.700309 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98088 (* 1 = 8.98088 loss)
I0523 01:14:36.768201 34682 sgd_solver.cpp:112] Iteration 25270, lr = 0.01
I0523 01:14:41.319066 34682 solver.cpp:239] Iteration 25280 (2.16517 iter/s, 4.61856s/10 iters), loss = 9.13238
I0523 01:14:41.319119 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13238 (* 1 = 9.13238 loss)
I0523 01:14:42.056876 34682 sgd_solver.cpp:112] Iteration 25280, lr = 0.01
I0523 01:14:46.846863 34682 solver.cpp:239] Iteration 25290 (1.80913 iter/s, 5.52751s/10 iters), loss = 8.72668
I0523 01:14:46.846918 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72668 (* 1 = 8.72668 loss)
I0523 01:14:47.643852 34682 sgd_solver.cpp:112] Iteration 25290, lr = 0.01
I0523 01:14:52.420186 34682 solver.cpp:239] Iteration 25300 (1.79435 iter/s, 5.57304s/10 iters), loss = 9.03698
I0523 01:14:52.420341 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03698 (* 1 = 9.03698 loss)
I0523 01:14:52.482550 34682 sgd_solver.cpp:112] Iteration 25300, lr = 0.01
I0523 01:14:57.148586 34682 solver.cpp:239] Iteration 25310 (2.11504 iter/s, 4.72805s/10 iters), loss = 9.01286
I0523 01:14:57.148639 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01286 (* 1 = 9.01286 loss)
I0523 01:14:57.223781 34682 sgd_solver.cpp:112] Iteration 25310, lr = 0.01
I0523 01:15:03.536255 34682 solver.cpp:239] Iteration 25320 (1.56559 iter/s, 6.38736s/10 iters), loss = 8.74243
I0523 01:15:03.536309 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74243 (* 1 = 8.74243 loss)
I0523 01:15:04.161382 34682 sgd_solver.cpp:112] Iteration 25320, lr = 0.01
I0523 01:15:08.794261 34682 solver.cpp:239] Iteration 25330 (1.90196 iter/s, 5.25774s/10 iters), loss = 8.74687
I0523 01:15:08.794322 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74687 (* 1 = 8.74687 loss)
I0523 01:15:09.543725 34682 sgd_solver.cpp:112] Iteration 25330, lr = 0.01
I0523 01:15:13.601812 34682 solver.cpp:239] Iteration 25340 (2.08018 iter/s, 4.80728s/10 iters), loss = 8.31898
I0523 01:15:13.601889 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31898 (* 1 = 8.31898 loss)
I0523 01:15:14.309581 34682 sgd_solver.cpp:112] Iteration 25340, lr = 0.01
I0523 01:15:19.099210 34682 solver.cpp:239] Iteration 25350 (1.81915 iter/s, 5.49709s/10 iters), loss = 9.06182
I0523 01:15:19.099264 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06182 (* 1 = 9.06182 loss)
I0523 01:15:19.925849 34682 sgd_solver.cpp:112] Iteration 25350, lr = 0.01
I0523 01:15:24.586918 34682 solver.cpp:239] Iteration 25360 (1.82235 iter/s, 5.48743s/10 iters), loss = 8.80181
I0523 01:15:24.587170 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80181 (* 1 = 8.80181 loss)
I0523 01:15:24.657516 34682 sgd_solver.cpp:112] Iteration 25360, lr = 0.01
I0523 01:15:30.876415 34682 solver.cpp:239] Iteration 25370 (1.59008 iter/s, 6.28901s/10 iters), loss = 9.1383
I0523 01:15:30.876461 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1383 (* 1 = 9.1383 loss)
I0523 01:15:30.952736 34682 sgd_solver.cpp:112] Iteration 25370, lr = 0.01
I0523 01:15:37.075744 34682 solver.cpp:239] Iteration 25380 (1.61315 iter/s, 6.19903s/10 iters), loss = 9.12217
I0523 01:15:37.075793 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12217 (* 1 = 9.12217 loss)
I0523 01:15:37.900997 34682 sgd_solver.cpp:112] Iteration 25380, lr = 0.01
I0523 01:15:44.412184 34682 solver.cpp:239] Iteration 25390 (1.36312 iter/s, 7.33609s/10 iters), loss = 8.9918
I0523 01:15:44.412235 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9918 (* 1 = 8.9918 loss)
I0523 01:15:44.484408 34682 sgd_solver.cpp:112] Iteration 25390, lr = 0.01
I0523 01:15:48.608635 34682 solver.cpp:239] Iteration 25400 (2.3831 iter/s, 4.19622s/10 iters), loss = 9.03812
I0523 01:15:48.608716 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03812 (* 1 = 9.03812 loss)
I0523 01:15:48.674748 34682 sgd_solver.cpp:112] Iteration 25400, lr = 0.01
I0523 01:15:51.876533 34682 solver.cpp:239] Iteration 25410 (3.06027 iter/s, 3.26769s/10 iters), loss = 8.2023
I0523 01:15:51.876583 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2023 (* 1 = 8.2023 loss)
I0523 01:15:52.743576 34682 sgd_solver.cpp:112] Iteration 25410, lr = 0.01
I0523 01:15:56.819365 34682 solver.cpp:239] Iteration 25420 (2.02323 iter/s, 4.94259s/10 iters), loss = 9.06368
I0523 01:15:56.819545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06368 (* 1 = 9.06368 loss)
I0523 01:15:57.644906 34682 sgd_solver.cpp:112] Iteration 25420, lr = 0.01
I0523 01:16:03.109340 34682 solver.cpp:239] Iteration 25430 (1.58994 iter/s, 6.28955s/10 iters), loss = 8.6052
I0523 01:16:03.109417 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6052 (* 1 = 8.6052 loss)
I0523 01:16:03.445835 34682 sgd_solver.cpp:112] Iteration 25430, lr = 0.01
I0523 01:16:06.315877 34682 solver.cpp:239] Iteration 25440 (3.11884 iter/s, 3.20633s/10 iters), loss = 8.73294
I0523 01:16:06.315924 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73294 (* 1 = 8.73294 loss)
I0523 01:16:06.386202 34682 sgd_solver.cpp:112] Iteration 25440, lr = 0.01
I0523 01:16:13.481218 34682 solver.cpp:239] Iteration 25450 (1.39567 iter/s, 7.165s/10 iters), loss = 8.22114
I0523 01:16:13.481272 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22114 (* 1 = 8.22114 loss)
I0523 01:16:14.285269 34682 sgd_solver.cpp:112] Iteration 25450, lr = 0.01
I0523 01:16:17.583736 34682 solver.cpp:239] Iteration 25460 (2.43766 iter/s, 4.10229s/10 iters), loss = 8.29468
I0523 01:16:17.583787 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29468 (* 1 = 8.29468 loss)
I0523 01:16:18.395400 34682 sgd_solver.cpp:112] Iteration 25460, lr = 0.01
I0523 01:16:21.738191 34682 solver.cpp:239] Iteration 25470 (2.40718 iter/s, 4.15423s/10 iters), loss = 8.96066
I0523 01:16:21.738239 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96066 (* 1 = 8.96066 loss)
I0523 01:16:21.803650 34682 sgd_solver.cpp:112] Iteration 25470, lr = 0.01
I0523 01:16:28.948659 34682 solver.cpp:239] Iteration 25480 (1.38694 iter/s, 7.21013s/10 iters), loss = 8.53555
I0523 01:16:28.948910 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53555 (* 1 = 8.53555 loss)
I0523 01:16:29.024834 34682 sgd_solver.cpp:112] Iteration 25480, lr = 0.01
I0523 01:16:33.913935 34682 solver.cpp:239] Iteration 25490 (2.01415 iter/s, 4.96487s/10 iters), loss = 9.03934
I0523 01:16:33.913980 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03934 (* 1 = 9.03934 loss)
I0523 01:16:33.974946 34682 sgd_solver.cpp:112] Iteration 25490, lr = 0.01
I0523 01:16:36.418573 34682 solver.cpp:239] Iteration 25500 (3.99283 iter/s, 2.50449s/10 iters), loss = 8.94015
I0523 01:16:36.418622 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94015 (* 1 = 8.94015 loss)
I0523 01:16:37.169781 34682 sgd_solver.cpp:112] Iteration 25500, lr = 0.01
I0523 01:16:43.040482 34682 solver.cpp:239] Iteration 25510 (1.51021 iter/s, 6.62159s/10 iters), loss = 8.10296
I0523 01:16:43.040529 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10296 (* 1 = 8.10296 loss)
I0523 01:16:43.126112 34682 sgd_solver.cpp:112] Iteration 25510, lr = 0.01
I0523 01:16:47.089697 34682 solver.cpp:239] Iteration 25520 (2.46974 iter/s, 4.049s/10 iters), loss = 8.52939
I0523 01:16:47.089745 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52939 (* 1 = 8.52939 loss)
I0523 01:16:47.776362 34682 sgd_solver.cpp:112] Iteration 25520, lr = 0.01
I0523 01:16:52.794492 34682 solver.cpp:239] Iteration 25530 (1.753 iter/s, 5.70451s/10 iters), loss = 8.9462
I0523 01:16:52.794539 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9462 (* 1 = 8.9462 loss)
I0523 01:16:53.628939 34682 sgd_solver.cpp:112] Iteration 25530, lr = 0.01
I0523 01:16:57.435593 34682 solver.cpp:239] Iteration 25540 (2.15477 iter/s, 4.64086s/10 iters), loss = 8.71716
I0523 01:16:57.435648 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71716 (* 1 = 8.71716 loss)
I0523 01:16:58.188001 34682 sgd_solver.cpp:112] Iteration 25540, lr = 0.01
I0523 01:17:02.268110 34682 solver.cpp:239] Iteration 25550 (2.06943 iter/s, 4.83225s/10 iters), loss = 8.43869
I0523 01:17:02.268296 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43869 (* 1 = 8.43869 loss)
I0523 01:17:02.327702 34682 sgd_solver.cpp:112] Iteration 25550, lr = 0.01
I0523 01:17:07.385628 34682 solver.cpp:239] Iteration 25560 (1.95422 iter/s, 5.11713s/10 iters), loss = 9.08005
I0523 01:17:07.385671 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08005 (* 1 = 9.08005 loss)
I0523 01:17:07.457967 34682 sgd_solver.cpp:112] Iteration 25560, lr = 0.01
I0523 01:17:11.065790 34682 solver.cpp:239] Iteration 25570 (2.71742 iter/s, 3.67996s/10 iters), loss = 8.599
I0523 01:17:11.065845 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.599 (* 1 = 8.599 loss)
I0523 01:17:11.135881 34682 sgd_solver.cpp:112] Iteration 25570, lr = 0.01
I0523 01:17:16.091611 34682 solver.cpp:239] Iteration 25580 (1.98983 iter/s, 5.02554s/10 iters), loss = 8.59884
I0523 01:17:16.091662 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59884 (* 1 = 8.59884 loss)
I0523 01:17:16.910507 34682 sgd_solver.cpp:112] Iteration 25580, lr = 0.01
I0523 01:17:20.226441 34682 solver.cpp:239] Iteration 25590 (2.41861 iter/s, 4.1346s/10 iters), loss = 8.83903
I0523 01:17:20.226501 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83903 (* 1 = 8.83903 loss)
I0523 01:17:20.288102 34682 sgd_solver.cpp:112] Iteration 25590, lr = 0.01
I0523 01:17:24.743888 34682 solver.cpp:239] Iteration 25600 (2.21377 iter/s, 4.51719s/10 iters), loss = 8.56336
I0523 01:17:24.743966 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56336 (* 1 = 8.56336 loss)
I0523 01:17:24.802690 34682 sgd_solver.cpp:112] Iteration 25600, lr = 0.01
I0523 01:17:31.110775 34682 solver.cpp:239] Iteration 25610 (1.57071 iter/s, 6.36656s/10 iters), loss = 8.31792
I0523 01:17:31.110828 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31792 (* 1 = 8.31792 loss)
I0523 01:17:31.960455 34682 sgd_solver.cpp:112] Iteration 25610, lr = 0.01
I0523 01:17:35.168545 34682 solver.cpp:239] Iteration 25620 (2.46454 iter/s, 4.05755s/10 iters), loss = 8.35423
I0523 01:17:35.168666 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35423 (* 1 = 8.35423 loss)
I0523 01:17:35.289774 34682 sgd_solver.cpp:112] Iteration 25620, lr = 0.01
I0523 01:17:39.599071 34682 solver.cpp:239] Iteration 25630 (2.25722 iter/s, 4.43022s/10 iters), loss = 8.8657
I0523 01:17:39.599117 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8657 (* 1 = 8.8657 loss)
I0523 01:17:40.447787 34682 sgd_solver.cpp:112] Iteration 25630, lr = 0.01
I0523 01:17:45.348054 34682 solver.cpp:239] Iteration 25640 (1.73952 iter/s, 5.7487s/10 iters), loss = 9.50673
I0523 01:17:45.348098 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.50673 (* 1 = 9.50673 loss)
I0523 01:17:45.423496 34682 sgd_solver.cpp:112] Iteration 25640, lr = 0.01
I0523 01:17:50.360704 34682 solver.cpp:239] Iteration 25650 (1.99505 iter/s, 5.0124s/10 iters), loss = 8.94269
I0523 01:17:50.360765 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94269 (* 1 = 8.94269 loss)
I0523 01:17:50.427026 34682 sgd_solver.cpp:112] Iteration 25650, lr = 0.01
I0523 01:17:56.754931 34682 solver.cpp:239] Iteration 25660 (1.56399 iter/s, 6.39391s/10 iters), loss = 8.97726
I0523 01:17:56.755023 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97726 (* 1 = 8.97726 loss)
I0523 01:17:56.818768 34682 sgd_solver.cpp:112] Iteration 25660, lr = 0.01
I0523 01:18:03.165025 34682 solver.cpp:239] Iteration 25670 (1.56012 iter/s, 6.40975s/10 iters), loss = 8.21534
I0523 01:18:03.165072 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21534 (* 1 = 8.21534 loss)
I0523 01:18:03.229243 34682 sgd_solver.cpp:112] Iteration 25670, lr = 0.01
I0523 01:18:08.841672 34682 solver.cpp:239] Iteration 25680 (1.76169 iter/s, 5.67636s/10 iters), loss = 8.28579
I0523 01:18:08.841985 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28579 (* 1 = 8.28579 loss)
I0523 01:18:09.626871 34682 sgd_solver.cpp:112] Iteration 25680, lr = 0.01
I0523 01:18:16.445993 34682 solver.cpp:239] Iteration 25690 (1.31514 iter/s, 7.60374s/10 iters), loss = 9.17497
I0523 01:18:16.446043 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17497 (* 1 = 9.17497 loss)
I0523 01:18:16.529790 34682 sgd_solver.cpp:112] Iteration 25690, lr = 0.01
I0523 01:18:21.168359 34682 solver.cpp:239] Iteration 25700 (2.11769 iter/s, 4.72213s/10 iters), loss = 8.29411
I0523 01:18:21.168397 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29411 (* 1 = 8.29411 loss)
I0523 01:18:21.248078 34682 sgd_solver.cpp:112] Iteration 25700, lr = 0.01
I0523 01:18:26.001063 34682 solver.cpp:239] Iteration 25710 (2.06934 iter/s, 4.83246s/10 iters), loss = 9.63858
I0523 01:18:26.001111 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.63858 (* 1 = 9.63858 loss)
I0523 01:18:26.819361 34682 sgd_solver.cpp:112] Iteration 25710, lr = 0.01
I0523 01:18:32.341452 34682 solver.cpp:239] Iteration 25720 (1.57727 iter/s, 6.34008s/10 iters), loss = 9.38827
I0523 01:18:32.341524 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.38827 (* 1 = 9.38827 loss)
I0523 01:18:33.207494 34682 sgd_solver.cpp:112] Iteration 25720, lr = 0.01
I0523 01:18:36.985414 34682 solver.cpp:239] Iteration 25730 (2.15345 iter/s, 4.6437s/10 iters), loss = 9.58092
I0523 01:18:36.985458 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.58092 (* 1 = 9.58092 loss)
I0523 01:18:37.056948 34682 sgd_solver.cpp:112] Iteration 25730, lr = 0.01
I0523 01:18:40.352131 34682 solver.cpp:239] Iteration 25740 (2.97042 iter/s, 3.36652s/10 iters), loss = 8.0984
I0523 01:18:40.352316 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0984 (* 1 = 8.0984 loss)
I0523 01:18:41.127478 34682 sgd_solver.cpp:112] Iteration 25740, lr = 0.01
I0523 01:18:44.527545 34682 solver.cpp:239] Iteration 25750 (2.39519 iter/s, 4.17504s/10 iters), loss = 8.54246
I0523 01:18:44.527676 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54246 (* 1 = 8.54246 loss)
I0523 01:18:44.580689 34682 sgd_solver.cpp:112] Iteration 25750, lr = 0.01
I0523 01:18:49.238517 34682 solver.cpp:239] Iteration 25760 (2.12285 iter/s, 4.71065s/10 iters), loss = 8.84933
I0523 01:18:49.238564 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84933 (* 1 = 8.84933 loss)
I0523 01:18:49.298908 34682 sgd_solver.cpp:112] Iteration 25760, lr = 0.01
I0523 01:18:53.151499 34682 solver.cpp:239] Iteration 25770 (2.55574 iter/s, 3.91276s/10 iters), loss = 8.56314
I0523 01:18:53.151548 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56314 (* 1 = 8.56314 loss)
I0523 01:18:53.232825 34682 sgd_solver.cpp:112] Iteration 25770, lr = 0.01
I0523 01:18:57.917655 34682 solver.cpp:239] Iteration 25780 (2.09823 iter/s, 4.76591s/10 iters), loss = 8.48248
I0523 01:18:57.917693 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48248 (* 1 = 8.48248 loss)
I0523 01:18:57.997262 34682 sgd_solver.cpp:112] Iteration 25780, lr = 0.01
I0523 01:19:00.676766 34682 solver.cpp:239] Iteration 25790 (3.62456 iter/s, 2.75896s/10 iters), loss = 9.02627
I0523 01:19:00.676812 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02627 (* 1 = 9.02627 loss)
I0523 01:19:00.764142 34682 sgd_solver.cpp:112] Iteration 25790, lr = 0.01
I0523 01:19:06.408080 34682 solver.cpp:239] Iteration 25800 (1.74489 iter/s, 5.73102s/10 iters), loss = 9.14107
I0523 01:19:06.408128 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14107 (* 1 = 9.14107 loss)
I0523 01:19:07.184185 34682 sgd_solver.cpp:112] Iteration 25800, lr = 0.01
I0523 01:19:12.318132 34682 solver.cpp:239] Iteration 25810 (1.69212 iter/s, 5.90976s/10 iters), loss = 8.74997
I0523 01:19:12.318308 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74997 (* 1 = 8.74997 loss)
I0523 01:19:12.383291 34682 sgd_solver.cpp:112] Iteration 25810, lr = 0.01
I0523 01:19:19.572680 34682 solver.cpp:239] Iteration 25820 (1.37853 iter/s, 7.2541s/10 iters), loss = 9.19788
I0523 01:19:19.572736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19788 (* 1 = 9.19788 loss)
I0523 01:19:20.380141 34682 sgd_solver.cpp:112] Iteration 25820, lr = 0.01
I0523 01:19:26.603073 34682 solver.cpp:239] Iteration 25830 (1.42247 iter/s, 7.03005s/10 iters), loss = 8.76963
I0523 01:19:26.603137 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76963 (* 1 = 8.76963 loss)
I0523 01:19:27.447064 34682 sgd_solver.cpp:112] Iteration 25830, lr = 0.01
I0523 01:19:33.285637 34682 solver.cpp:239] Iteration 25840 (1.4965 iter/s, 6.68224s/10 iters), loss = 8.1006
I0523 01:19:33.285683 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1006 (* 1 = 8.1006 loss)
I0523 01:19:33.355392 34682 sgd_solver.cpp:112] Iteration 25840, lr = 0.01
I0523 01:19:39.682288 34682 solver.cpp:239] Iteration 25850 (1.56339 iter/s, 6.39634s/10 iters), loss = 9.73032
I0523 01:19:39.682341 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.73032 (* 1 = 9.73032 loss)
I0523 01:19:40.488900 34682 sgd_solver.cpp:112] Iteration 25850, lr = 0.01
I0523 01:19:46.103572 34682 solver.cpp:239] Iteration 25860 (1.5574 iter/s, 6.42096s/10 iters), loss = 8.88783
I0523 01:19:46.103821 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88783 (* 1 = 8.88783 loss)
I0523 01:19:46.881335 34682 sgd_solver.cpp:112] Iteration 25860, lr = 0.01
I0523 01:19:51.450304 34682 solver.cpp:239] Iteration 25870 (1.87045 iter/s, 5.3463s/10 iters), loss = 8.69902
I0523 01:19:51.450350 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69902 (* 1 = 8.69902 loss)
I0523 01:19:52.295619 34682 sgd_solver.cpp:112] Iteration 25870, lr = 0.01
I0523 01:19:56.821470 34682 solver.cpp:239] Iteration 25880 (1.86189 iter/s, 5.37089s/10 iters), loss = 8.64919
I0523 01:19:56.821564 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64919 (* 1 = 8.64919 loss)
I0523 01:19:56.895261 34682 sgd_solver.cpp:112] Iteration 25880, lr = 0.01
I0523 01:20:00.250169 34682 solver.cpp:239] Iteration 25890 (2.91675 iter/s, 3.42847s/10 iters), loss = 7.79916
I0523 01:20:00.250224 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79916 (* 1 = 7.79916 loss)
I0523 01:20:00.965333 34682 sgd_solver.cpp:112] Iteration 25890, lr = 0.01
I0523 01:20:04.100277 34682 solver.cpp:239] Iteration 25900 (2.59748 iter/s, 3.84989s/10 iters), loss = 8.47405
I0523 01:20:04.100332 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47405 (* 1 = 8.47405 loss)
I0523 01:20:04.878136 34682 sgd_solver.cpp:112] Iteration 25900, lr = 0.01
I0523 01:20:08.399452 34682 solver.cpp:239] Iteration 25910 (2.32615 iter/s, 4.29895s/10 iters), loss = 8.896
I0523 01:20:08.399508 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.896 (* 1 = 8.896 loss)
I0523 01:20:08.464056 34682 sgd_solver.cpp:112] Iteration 25910, lr = 0.01
I0523 01:20:12.547426 34682 solver.cpp:239] Iteration 25920 (2.41094 iter/s, 4.14777s/10 iters), loss = 8.66899
I0523 01:20:12.547473 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66899 (* 1 = 8.66899 loss)
I0523 01:20:13.156958 34682 sgd_solver.cpp:112] Iteration 25920, lr = 0.01
I0523 01:20:19.163457 34682 solver.cpp:239] Iteration 25930 (1.51155 iter/s, 6.61571s/10 iters), loss = 8.07696
I0523 01:20:19.163733 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07696 (* 1 = 8.07696 loss)
I0523 01:20:19.759629 34682 sgd_solver.cpp:112] Iteration 25930, lr = 0.01
I0523 01:20:25.326767 34682 solver.cpp:239] Iteration 25940 (1.62263 iter/s, 6.16282s/10 iters), loss = 7.96624
I0523 01:20:25.326817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96624 (* 1 = 7.96624 loss)
I0523 01:20:26.094738 34682 sgd_solver.cpp:112] Iteration 25940, lr = 0.01
I0523 01:20:31.011605 34682 solver.cpp:239] Iteration 25950 (1.75916 iter/s, 5.68454s/10 iters), loss = 8.93255
I0523 01:20:31.011669 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93255 (* 1 = 8.93255 loss)
I0523 01:20:31.741389 34682 sgd_solver.cpp:112] Iteration 25950, lr = 0.01
I0523 01:20:35.972909 34682 solver.cpp:239] Iteration 25960 (2.01571 iter/s, 4.96104s/10 iters), loss = 8.85748
I0523 01:20:35.972952 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85748 (* 1 = 8.85748 loss)
I0523 01:20:36.833839 34682 sgd_solver.cpp:112] Iteration 25960, lr = 0.01
I0523 01:20:42.622774 34682 solver.cpp:239] Iteration 25970 (1.50386 iter/s, 6.64954s/10 iters), loss = 9.02822
I0523 01:20:42.622833 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02822 (* 1 = 9.02822 loss)
I0523 01:20:42.753639 34682 sgd_solver.cpp:112] Iteration 25970, lr = 0.01
I0523 01:20:46.563781 34682 solver.cpp:239] Iteration 25980 (2.54035 iter/s, 3.93646s/10 iters), loss = 8.37704
I0523 01:20:46.563827 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37704 (* 1 = 8.37704 loss)
I0523 01:20:46.623026 34682 sgd_solver.cpp:112] Iteration 25980, lr = 0.01
I0523 01:20:51.149719 34682 solver.cpp:239] Iteration 25990 (2.1807 iter/s, 4.58569s/10 iters), loss = 8.72146
I0523 01:20:51.149994 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72146 (* 1 = 8.72146 loss)
I0523 01:20:51.220459 34682 sgd_solver.cpp:112] Iteration 25990, lr = 0.01
I0523 01:20:55.439460 34682 solver.cpp:239] Iteration 26000 (2.33137 iter/s, 4.28932s/10 iters), loss = 8.80697
I0523 01:20:55.439498 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80697 (* 1 = 8.80697 loss)
I0523 01:20:55.509299 34682 sgd_solver.cpp:112] Iteration 26000, lr = 0.01
I0523 01:20:59.772217 34682 solver.cpp:239] Iteration 26010 (2.30812 iter/s, 4.33254s/10 iters), loss = 7.44132
I0523 01:20:59.772264 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44132 (* 1 = 7.44132 loss)
I0523 01:20:59.847275 34682 sgd_solver.cpp:112] Iteration 26010, lr = 0.01
I0523 01:21:04.966533 34682 solver.cpp:239] Iteration 26020 (1.92528 iter/s, 5.19404s/10 iters), loss = 8.02536
I0523 01:21:04.966591 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02536 (* 1 = 8.02536 loss)
I0523 01:21:05.433596 34682 sgd_solver.cpp:112] Iteration 26020, lr = 0.01
I0523 01:21:07.060842 34682 solver.cpp:239] Iteration 26030 (4.77519 iter/s, 2.09416s/10 iters), loss = 8.55328
I0523 01:21:07.060895 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55328 (* 1 = 8.55328 loss)
I0523 01:21:07.086822 34682 sgd_solver.cpp:112] Iteration 26030, lr = 0.01
I0523 01:21:07.942435 34682 solver.cpp:239] Iteration 26040 (11.3445 iter/s, 0.881481s/10 iters), loss = 8.57142
I0523 01:21:07.942502 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57142 (* 1 = 8.57142 loss)
I0523 01:21:08.070643 34682 sgd_solver.cpp:112] Iteration 26040, lr = 0.01
I0523 01:21:09.460276 34682 solver.cpp:239] Iteration 26050 (6.58892 iter/s, 1.5177s/10 iters), loss = 9.1789
I0523 01:21:09.460338 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1789 (* 1 = 9.1789 loss)
I0523 01:21:09.499936 34682 sgd_solver.cpp:112] Iteration 26050, lr = 0.01
I0523 01:21:10.680990 34682 solver.cpp:239] Iteration 26060 (8.19277 iter/s, 1.22059s/10 iters), loss = 8.16658
I0523 01:21:10.681059 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16658 (* 1 = 8.16658 loss)
I0523 01:21:10.726974 34682 sgd_solver.cpp:112] Iteration 26060, lr = 0.01
I0523 01:21:13.127784 34682 solver.cpp:239] Iteration 26070 (4.08726 iter/s, 2.44663s/10 iters), loss = 9.10922
I0523 01:21:13.127825 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10922 (* 1 = 9.10922 loss)
I0523 01:21:13.194464 34682 sgd_solver.cpp:112] Iteration 26070, lr = 0.01
I0523 01:21:17.421548 34682 solver.cpp:239] Iteration 26080 (2.32908 iter/s, 4.29354s/10 iters), loss = 8.61248
I0523 01:21:17.421604 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61248 (* 1 = 8.61248 loss)
I0523 01:21:17.498210 34682 sgd_solver.cpp:112] Iteration 26080, lr = 0.01
I0523 01:21:22.528748 34682 solver.cpp:239] Iteration 26090 (1.95812 iter/s, 5.10693s/10 iters), loss = 9.4729
I0523 01:21:22.528991 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4729 (* 1 = 9.4729 loss)
I0523 01:21:22.610863 34682 sgd_solver.cpp:112] Iteration 26090, lr = 0.01
I0523 01:21:26.812288 34682 solver.cpp:239] Iteration 26100 (2.33712 iter/s, 4.27877s/10 iters), loss = 8.85838
I0523 01:21:26.812333 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85838 (* 1 = 8.85838 loss)
I0523 01:21:27.683037 34682 sgd_solver.cpp:112] Iteration 26100, lr = 0.01
I0523 01:21:32.665750 34682 solver.cpp:239] Iteration 26110 (1.70847 iter/s, 5.85318s/10 iters), loss = 9.74066
I0523 01:21:32.665797 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.74066 (* 1 = 9.74066 loss)
I0523 01:21:33.468811 34682 sgd_solver.cpp:112] Iteration 26110, lr = 0.01
I0523 01:21:37.777719 34682 solver.cpp:239] Iteration 26120 (1.95629 iter/s, 5.11171s/10 iters), loss = 9.11747
I0523 01:21:37.777770 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11747 (* 1 = 9.11747 loss)
I0523 01:21:37.850344 34682 sgd_solver.cpp:112] Iteration 26120, lr = 0.01
I0523 01:21:44.033464 34682 solver.cpp:239] Iteration 26130 (1.59861 iter/s, 6.25544s/10 iters), loss = 9.16475
I0523 01:21:44.033520 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16475 (* 1 = 9.16475 loss)
I0523 01:21:44.895884 34682 sgd_solver.cpp:112] Iteration 26130, lr = 0.01
I0523 01:21:49.904124 34682 solver.cpp:239] Iteration 26140 (1.70347 iter/s, 5.87037s/10 iters), loss = 8.11249
I0523 01:21:49.904171 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11249 (* 1 = 8.11249 loss)
I0523 01:21:50.690301 34682 sgd_solver.cpp:112] Iteration 26140, lr = 0.01
I0523 01:21:55.287528 34682 solver.cpp:239] Iteration 26150 (1.85765 iter/s, 5.38313s/10 iters), loss = 9.06917
I0523 01:21:55.287760 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06917 (* 1 = 9.06917 loss)
I0523 01:21:55.346879 34682 sgd_solver.cpp:112] Iteration 26150, lr = 0.01
I0523 01:22:00.269052 34682 solver.cpp:239] Iteration 26160 (2.00759 iter/s, 4.9811s/10 iters), loss = 8.26106
I0523 01:22:00.269121 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26106 (* 1 = 8.26106 loss)
I0523 01:22:00.676805 34682 sgd_solver.cpp:112] Iteration 26160, lr = 0.01
I0523 01:22:04.457197 34682 solver.cpp:239] Iteration 26170 (2.38783 iter/s, 4.18791s/10 iters), loss = 9.46773
I0523 01:22:04.457242 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.46773 (* 1 = 9.46773 loss)
I0523 01:22:04.521600 34682 sgd_solver.cpp:112] Iteration 26170, lr = 0.01
I0523 01:22:09.184015 34682 solver.cpp:239] Iteration 26180 (2.1157 iter/s, 4.72657s/10 iters), loss = 9.13338
I0523 01:22:09.184069 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13338 (* 1 = 9.13338 loss)
I0523 01:22:10.012318 34682 sgd_solver.cpp:112] Iteration 26180, lr = 0.01
I0523 01:22:14.940073 34682 solver.cpp:239] Iteration 26190 (1.73739 iter/s, 5.75577s/10 iters), loss = 9.08431
I0523 01:22:14.940129 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08431 (* 1 = 9.08431 loss)
I0523 01:22:15.317663 34682 sgd_solver.cpp:112] Iteration 26190, lr = 0.01
I0523 01:22:20.207110 34682 solver.cpp:239] Iteration 26200 (1.8987 iter/s, 5.26676s/10 iters), loss = 8.36746
I0523 01:22:20.207171 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36746 (* 1 = 8.36746 loss)
I0523 01:22:20.481526 34682 sgd_solver.cpp:112] Iteration 26200, lr = 0.01
I0523 01:22:24.164726 34682 solver.cpp:239] Iteration 26210 (2.52692 iter/s, 3.95739s/10 iters), loss = 8.62086
I0523 01:22:24.164777 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62086 (* 1 = 8.62086 loss)
I0523 01:22:24.228488 34682 sgd_solver.cpp:112] Iteration 26210, lr = 0.01
I0523 01:22:28.020086 34682 solver.cpp:239] Iteration 26220 (2.59395 iter/s, 3.85513s/10 iters), loss = 9.10375
I0523 01:22:28.020431 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10375 (* 1 = 9.10375 loss)
I0523 01:22:28.226855 34682 sgd_solver.cpp:112] Iteration 26220, lr = 0.01
I0523 01:22:31.760004 34682 solver.cpp:239] Iteration 26230 (2.67419 iter/s, 3.73945s/10 iters), loss = 9.02801
I0523 01:22:31.760063 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02801 (* 1 = 9.02801 loss)
I0523 01:22:32.618702 34682 sgd_solver.cpp:112] Iteration 26230, lr = 0.01
I0523 01:22:36.950261 34682 solver.cpp:239] Iteration 26240 (1.92678 iter/s, 5.18999s/10 iters), loss = 9.3903
I0523 01:22:36.950310 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3903 (* 1 = 9.3903 loss)
I0523 01:22:37.768519 34682 sgd_solver.cpp:112] Iteration 26240, lr = 0.01
I0523 01:22:44.097074 34682 solver.cpp:239] Iteration 26250 (1.39929 iter/s, 7.14648s/10 iters), loss = 8.59046
I0523 01:22:44.097129 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59046 (* 1 = 8.59046 loss)
I0523 01:22:44.347239 34682 sgd_solver.cpp:112] Iteration 26250, lr = 0.01
I0523 01:22:48.533226 34682 solver.cpp:239] Iteration 26260 (2.25433 iter/s, 4.43591s/10 iters), loss = 9.15708
I0523 01:22:48.533277 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15708 (* 1 = 9.15708 loss)
I0523 01:22:49.293834 34682 sgd_solver.cpp:112] Iteration 26260, lr = 0.01
I0523 01:22:53.775440 34682 solver.cpp:239] Iteration 26270 (1.90769 iter/s, 5.24194s/10 iters), loss = 9.12568
I0523 01:22:53.775491 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12568 (* 1 = 9.12568 loss)
I0523 01:22:53.835198 34682 sgd_solver.cpp:112] Iteration 26270, lr = 0.01
I0523 01:22:57.811655 34682 solver.cpp:239] Iteration 26280 (2.4777 iter/s, 4.036s/10 iters), loss = 7.914
I0523 01:22:57.811697 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.914 (* 1 = 7.914 loss)
I0523 01:22:57.871362 34682 sgd_solver.cpp:112] Iteration 26280, lr = 0.01
I0523 01:23:03.995138 34682 solver.cpp:239] Iteration 26290 (1.61729 iter/s, 6.18318s/10 iters), loss = 9.58495
I0523 01:23:03.995369 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.58495 (* 1 = 9.58495 loss)
I0523 01:23:04.071303 34682 sgd_solver.cpp:112] Iteration 26290, lr = 0.01
I0523 01:23:06.714403 34682 solver.cpp:239] Iteration 26300 (3.67789 iter/s, 2.71895s/10 iters), loss = 8.85166
I0523 01:23:06.714447 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85166 (* 1 = 8.85166 loss)
I0523 01:23:07.558395 34682 sgd_solver.cpp:112] Iteration 26300, lr = 0.01
I0523 01:23:11.490164 34682 solver.cpp:239] Iteration 26310 (2.09402 iter/s, 4.77551s/10 iters), loss = 8.41249
I0523 01:23:11.490212 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41249 (* 1 = 8.41249 loss)
I0523 01:23:11.557520 34682 sgd_solver.cpp:112] Iteration 26310, lr = 0.01
I0523 01:23:15.545228 34682 solver.cpp:239] Iteration 26320 (2.46619 iter/s, 4.05484s/10 iters), loss = 8.85472
I0523 01:23:15.545289 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85472 (* 1 = 8.85472 loss)
I0523 01:23:16.392171 34682 sgd_solver.cpp:112] Iteration 26320, lr = 0.01
I0523 01:23:21.718766 34682 solver.cpp:239] Iteration 26330 (1.61991 iter/s, 6.17318s/10 iters), loss = 8.41956
I0523 01:23:21.718868 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41956 (* 1 = 8.41956 loss)
I0523 01:23:22.318795 34682 sgd_solver.cpp:112] Iteration 26330, lr = 0.01
I0523 01:23:28.015802 34682 solver.cpp:239] Iteration 26340 (1.58813 iter/s, 6.2967s/10 iters), loss = 8.9356
I0523 01:23:28.015846 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9356 (* 1 = 8.9356 loss)
I0523 01:23:28.090036 34682 sgd_solver.cpp:112] Iteration 26340, lr = 0.01
I0523 01:23:32.729802 34682 solver.cpp:239] Iteration 26350 (2.12145 iter/s, 4.71376s/10 iters), loss = 7.6681
I0523 01:23:32.729857 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6681 (* 1 = 7.6681 loss)
I0523 01:23:32.790582 34682 sgd_solver.cpp:112] Iteration 26350, lr = 0.01
I0523 01:23:36.591404 34682 solver.cpp:239] Iteration 26360 (2.58974 iter/s, 3.86138s/10 iters), loss = 9.15379
I0523 01:23:36.591647 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15379 (* 1 = 9.15379 loss)
I0523 01:23:37.350133 34682 sgd_solver.cpp:112] Iteration 26360, lr = 0.01
I0523 01:23:42.002259 34682 solver.cpp:239] Iteration 26370 (1.84829 iter/s, 5.41041s/10 iters), loss = 9.46736
I0523 01:23:42.002305 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.46736 (* 1 = 9.46736 loss)
I0523 01:23:42.065331 34682 sgd_solver.cpp:112] Iteration 26370, lr = 0.01
I0523 01:23:46.858781 34682 solver.cpp:239] Iteration 26380 (2.0592 iter/s, 4.85626s/10 iters), loss = 9.05046
I0523 01:23:46.858829 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05046 (* 1 = 9.05046 loss)
I0523 01:23:47.722705 34682 sgd_solver.cpp:112] Iteration 26380, lr = 0.01
I0523 01:23:50.242266 34682 solver.cpp:239] Iteration 26390 (2.9557 iter/s, 3.38329s/10 iters), loss = 7.59292
I0523 01:23:50.242328 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.59292 (* 1 = 7.59292 loss)
I0523 01:23:50.316996 34682 sgd_solver.cpp:112] Iteration 26390, lr = 0.01
I0523 01:23:53.562399 34682 solver.cpp:239] Iteration 26400 (3.01211 iter/s, 3.31993s/10 iters), loss = 8.84808
I0523 01:23:53.562440 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84808 (* 1 = 8.84808 loss)
I0523 01:23:53.621522 34682 sgd_solver.cpp:112] Iteration 26400, lr = 0.01
I0523 01:23:57.661877 34682 solver.cpp:239] Iteration 26410 (2.43946 iter/s, 4.09927s/10 iters), loss = 8.4608
I0523 01:23:57.661921 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4608 (* 1 = 8.4608 loss)
I0523 01:23:58.471400 34682 sgd_solver.cpp:112] Iteration 26410, lr = 0.01
I0523 01:24:03.288683 34682 solver.cpp:239] Iteration 26420 (1.77729 iter/s, 5.62653s/10 iters), loss = 7.53469
I0523 01:24:03.288736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.53469 (* 1 = 7.53469 loss)
I0523 01:24:03.361763 34682 sgd_solver.cpp:112] Iteration 26420, lr = 0.01
I0523 01:24:07.902220 34682 solver.cpp:239] Iteration 26430 (2.16765 iter/s, 4.6133s/10 iters), loss = 9.05329
I0523 01:24:07.902441 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05329 (* 1 = 9.05329 loss)
I0523 01:24:08.666263 34682 sgd_solver.cpp:112] Iteration 26430, lr = 0.01
I0523 01:24:13.964088 34682 solver.cpp:239] Iteration 26440 (1.64978 iter/s, 6.06143s/10 iters), loss = 9.05129
I0523 01:24:13.964138 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05129 (* 1 = 9.05129 loss)
I0523 01:24:14.747248 34682 sgd_solver.cpp:112] Iteration 26440, lr = 0.01
I0523 01:24:18.635782 34682 solver.cpp:239] Iteration 26450 (2.14067 iter/s, 4.67144s/10 iters), loss = 8.14912
I0523 01:24:18.635833 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14912 (* 1 = 8.14912 loss)
I0523 01:24:19.344780 34682 sgd_solver.cpp:112] Iteration 26450, lr = 0.01
I0523 01:24:25.112073 34682 solver.cpp:239] Iteration 26460 (1.54417 iter/s, 6.47597s/10 iters), loss = 8.60394
I0523 01:24:25.112131 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60394 (* 1 = 8.60394 loss)
I0523 01:24:25.170161 34682 sgd_solver.cpp:112] Iteration 26460, lr = 0.01
I0523 01:24:29.357949 34682 solver.cpp:239] Iteration 26470 (2.35537 iter/s, 4.24563s/10 iters), loss = 9.37007
I0523 01:24:29.358039 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.37007 (* 1 = 9.37007 loss)
I0523 01:24:29.446866 34682 sgd_solver.cpp:112] Iteration 26470, lr = 0.01
I0523 01:24:34.365514 34682 solver.cpp:239] Iteration 26480 (1.99709 iter/s, 5.00727s/10 iters), loss = 8.68912
I0523 01:24:34.365561 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68912 (* 1 = 8.68912 loss)
I0523 01:24:34.437312 34682 sgd_solver.cpp:112] Iteration 26480, lr = 0.01
I0523 01:24:39.808913 34682 solver.cpp:239] Iteration 26490 (1.83718 iter/s, 5.44313s/10 iters), loss = 8.58562
I0523 01:24:39.809276 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58562 (* 1 = 8.58562 loss)
I0523 01:24:39.875195 34682 sgd_solver.cpp:112] Iteration 26490, lr = 0.01
I0523 01:24:45.549690 34682 solver.cpp:239] Iteration 26500 (1.74209 iter/s, 5.74023s/10 iters), loss = 7.95774
I0523 01:24:45.549731 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95774 (* 1 = 7.95774 loss)
I0523 01:24:45.627449 34682 sgd_solver.cpp:112] Iteration 26500, lr = 0.01
I0523 01:24:48.948194 34682 solver.cpp:239] Iteration 26510 (2.94263 iter/s, 3.39832s/10 iters), loss = 9.09797
I0523 01:24:48.948240 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09797 (* 1 = 9.09797 loss)
I0523 01:24:49.722455 34682 sgd_solver.cpp:112] Iteration 26510, lr = 0.01
I0523 01:24:54.707645 34682 solver.cpp:239] Iteration 26520 (1.73637 iter/s, 5.75915s/10 iters), loss = 8.619
I0523 01:24:54.707718 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.619 (* 1 = 8.619 loss)
I0523 01:24:55.562445 34682 sgd_solver.cpp:112] Iteration 26520, lr = 0.01
I0523 01:24:59.252831 34682 solver.cpp:239] Iteration 26530 (2.20026 iter/s, 4.54493s/10 iters), loss = 9.08006
I0523 01:24:59.252876 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08006 (* 1 = 9.08006 loss)
I0523 01:24:59.970316 34682 sgd_solver.cpp:112] Iteration 26530, lr = 0.01
I0523 01:25:04.325382 34682 solver.cpp:239] Iteration 26540 (1.97149 iter/s, 5.07229s/10 iters), loss = 9.0711
I0523 01:25:04.325438 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0711 (* 1 = 9.0711 loss)
I0523 01:25:05.168668 34682 sgd_solver.cpp:112] Iteration 26540, lr = 0.01
I0523 01:25:11.843066 34682 solver.cpp:239] Iteration 26550 (1.33026 iter/s, 7.51733s/10 iters), loss = 8.81676
I0523 01:25:11.846853 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81676 (* 1 = 8.81676 loss)
I0523 01:25:11.904358 34682 sgd_solver.cpp:112] Iteration 26550, lr = 0.01
I0523 01:25:16.817432 34682 solver.cpp:239] Iteration 26560 (2.01191 iter/s, 4.9704s/10 iters), loss = 9.45989
I0523 01:25:16.817493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.45989 (* 1 = 9.45989 loss)
I0523 01:25:17.562767 34682 sgd_solver.cpp:112] Iteration 26560, lr = 0.01
I0523 01:25:20.789047 34682 solver.cpp:239] Iteration 26570 (2.51803 iter/s, 3.97136s/10 iters), loss = 9.03044
I0523 01:25:20.789103 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03044 (* 1 = 9.03044 loss)
I0523 01:25:21.630614 34682 sgd_solver.cpp:112] Iteration 26570, lr = 0.01
I0523 01:25:27.105826 34682 solver.cpp:239] Iteration 26580 (1.58316 iter/s, 6.31646s/10 iters), loss = 8.51811
I0523 01:25:27.105873 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51811 (* 1 = 8.51811 loss)
I0523 01:25:27.188900 34682 sgd_solver.cpp:112] Iteration 26580, lr = 0.01
I0523 01:25:31.546721 34682 solver.cpp:239] Iteration 26590 (2.25192 iter/s, 4.44065s/10 iters), loss = 8.67717
I0523 01:25:31.546766 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67717 (* 1 = 8.67717 loss)
I0523 01:25:31.700991 34682 sgd_solver.cpp:112] Iteration 26590, lr = 0.01
I0523 01:25:36.421268 34682 solver.cpp:239] Iteration 26600 (2.05158 iter/s, 4.8743s/10 iters), loss = 9.16456
I0523 01:25:36.421324 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16456 (* 1 = 9.16456 loss)
I0523 01:25:36.495184 34682 sgd_solver.cpp:112] Iteration 26600, lr = 0.01
I0523 01:25:43.337416 34682 solver.cpp:239] Iteration 26610 (1.44596 iter/s, 6.91582s/10 iters), loss = 8.71332
I0523 01:25:43.337647 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71332 (* 1 = 8.71332 loss)
I0523 01:25:44.032156 34682 sgd_solver.cpp:112] Iteration 26610, lr = 0.01
I0523 01:25:48.252022 34682 solver.cpp:239] Iteration 26620 (2.03493 iter/s, 4.91418s/10 iters), loss = 8.41472
I0523 01:25:48.252070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41472 (* 1 = 8.41472 loss)
I0523 01:25:48.312058 34682 sgd_solver.cpp:112] Iteration 26620, lr = 0.01
I0523 01:25:53.244426 34682 solver.cpp:239] Iteration 26630 (2.00314 iter/s, 4.99216s/10 iters), loss = 9.09054
I0523 01:25:53.244468 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09054 (* 1 = 9.09054 loss)
I0523 01:25:54.046437 34682 sgd_solver.cpp:112] Iteration 26630, lr = 0.01
I0523 01:25:56.561527 34682 solver.cpp:239] Iteration 26640 (3.01485 iter/s, 3.31691s/10 iters), loss = 9.13199
I0523 01:25:56.561592 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13199 (* 1 = 9.13199 loss)
I0523 01:25:56.716842 34682 sgd_solver.cpp:112] Iteration 26640, lr = 0.01
I0523 01:26:00.781272 34682 solver.cpp:239] Iteration 26650 (2.36995 iter/s, 4.2195s/10 iters), loss = 9.27118
I0523 01:26:00.781325 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27118 (* 1 = 9.27118 loss)
I0523 01:26:00.858636 34682 sgd_solver.cpp:112] Iteration 26650, lr = 0.01
I0523 01:26:06.569113 34682 solver.cpp:239] Iteration 26660 (1.72785 iter/s, 5.78755s/10 iters), loss = 8.7122
I0523 01:26:06.569159 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7122 (* 1 = 8.7122 loss)
I0523 01:26:06.637711 34682 sgd_solver.cpp:112] Iteration 26660, lr = 0.01
I0523 01:26:10.851778 34682 solver.cpp:239] Iteration 26670 (2.33512 iter/s, 4.28244s/10 iters), loss = 9.052
I0523 01:26:10.851830 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.052 (* 1 = 9.052 loss)
I0523 01:26:11.559083 34682 sgd_solver.cpp:112] Iteration 26670, lr = 0.01
I0523 01:26:13.406491 34682 solver.cpp:239] Iteration 26680 (3.91459 iter/s, 2.55455s/10 iters), loss = 8.30257
I0523 01:26:13.406661 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30257 (* 1 = 8.30257 loss)
I0523 01:26:14.160691 34682 sgd_solver.cpp:112] Iteration 26680, lr = 0.01
I0523 01:26:19.304072 34682 solver.cpp:239] Iteration 26690 (1.69573 iter/s, 5.89718s/10 iters), loss = 9.49392
I0523 01:26:19.304126 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.49392 (* 1 = 9.49392 loss)
I0523 01:26:20.083513 34682 sgd_solver.cpp:112] Iteration 26690, lr = 0.01
I0523 01:26:26.507309 34682 solver.cpp:239] Iteration 26700 (1.38833 iter/s, 7.2029s/10 iters), loss = 8.68994
I0523 01:26:26.507362 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68994 (* 1 = 8.68994 loss)
I0523 01:26:27.045513 34682 sgd_solver.cpp:112] Iteration 26700, lr = 0.01
I0523 01:26:30.510044 34682 solver.cpp:239] Iteration 26710 (2.49843 iter/s, 4.00252s/10 iters), loss = 9.17283
I0523 01:26:30.510107 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17283 (* 1 = 9.17283 loss)
I0523 01:26:31.148116 34682 sgd_solver.cpp:112] Iteration 26710, lr = 0.01
I0523 01:26:35.719015 34682 solver.cpp:239] Iteration 26720 (1.91987 iter/s, 5.20869s/10 iters), loss = 8.4998
I0523 01:26:35.719071 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4998 (* 1 = 8.4998 loss)
I0523 01:26:36.540771 34682 sgd_solver.cpp:112] Iteration 26720, lr = 0.01
I0523 01:26:40.457845 34682 solver.cpp:239] Iteration 26730 (2.11034 iter/s, 4.73857s/10 iters), loss = 9.09304
I0523 01:26:40.457901 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09304 (* 1 = 9.09304 loss)
I0523 01:26:40.522027 34682 sgd_solver.cpp:112] Iteration 26730, lr = 0.01
I0523 01:26:45.319635 34682 solver.cpp:239] Iteration 26740 (2.05697 iter/s, 4.86153s/10 iters), loss = 9.17526
I0523 01:26:45.319839 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17526 (* 1 = 9.17526 loss)
I0523 01:26:45.387300 34682 sgd_solver.cpp:112] Iteration 26740, lr = 0.01
I0523 01:26:48.878577 34682 solver.cpp:239] Iteration 26750 (2.8101 iter/s, 3.5586s/10 iters), loss = 8.67212
I0523 01:26:48.878625 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67212 (* 1 = 8.67212 loss)
I0523 01:26:49.084348 34682 sgd_solver.cpp:112] Iteration 26750, lr = 0.01
I0523 01:26:53.082989 34682 solver.cpp:239] Iteration 26760 (2.37858 iter/s, 4.20419s/10 iters), loss = 7.95744
I0523 01:26:53.083053 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95744 (* 1 = 7.95744 loss)
I0523 01:26:53.917426 34682 sgd_solver.cpp:112] Iteration 26760, lr = 0.01
I0523 01:26:58.838824 34682 solver.cpp:239] Iteration 26770 (1.73746 iter/s, 5.75554s/10 iters), loss = 8.81192
I0523 01:26:58.838881 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81192 (* 1 = 8.81192 loss)
I0523 01:26:58.913547 34682 sgd_solver.cpp:112] Iteration 26770, lr = 0.01
I0523 01:27:04.422848 34682 solver.cpp:239] Iteration 26780 (1.79091 iter/s, 5.58374s/10 iters), loss = 8.76522
I0523 01:27:04.422896 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76522 (* 1 = 8.76522 loss)
I0523 01:27:04.486538 34682 sgd_solver.cpp:112] Iteration 26780, lr = 0.01
I0523 01:27:08.834775 34682 solver.cpp:239] Iteration 26790 (2.2667 iter/s, 4.41169s/10 iters), loss = 8.01379
I0523 01:27:08.834839 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01379 (* 1 = 8.01379 loss)
I0523 01:27:09.697881 34682 sgd_solver.cpp:112] Iteration 26790, lr = 0.01
I0523 01:27:14.409927 34682 solver.cpp:239] Iteration 26800 (1.79376 iter/s, 5.57487s/10 iters), loss = 8.96957
I0523 01:27:14.409965 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96957 (* 1 = 8.96957 loss)
I0523 01:27:14.469022 34682 sgd_solver.cpp:112] Iteration 26800, lr = 0.01
I0523 01:27:17.821631 34682 solver.cpp:239] Iteration 26810 (2.93125 iter/s, 3.41152s/10 iters), loss = 8.82817
I0523 01:27:17.821775 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82817 (* 1 = 8.82817 loss)
I0523 01:27:17.900214 34682 sgd_solver.cpp:112] Iteration 26810, lr = 0.01
I0523 01:27:22.933594 34682 solver.cpp:239] Iteration 26820 (1.95633 iter/s, 5.11161s/10 iters), loss = 9.0228
I0523 01:27:22.933660 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0228 (* 1 = 9.0228 loss)
I0523 01:27:23.414598 34682 sgd_solver.cpp:112] Iteration 26820, lr = 0.01
I0523 01:27:28.082058 34682 solver.cpp:239] Iteration 26830 (1.94243 iter/s, 5.14818s/10 iters), loss = 8.87922
I0523 01:27:28.082128 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87922 (* 1 = 8.87922 loss)
I0523 01:27:28.752208 34682 sgd_solver.cpp:112] Iteration 26830, lr = 0.01
I0523 01:27:33.246475 34682 solver.cpp:239] Iteration 26840 (1.93643 iter/s, 5.16414s/10 iters), loss = 9.56667
I0523 01:27:33.246520 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56667 (* 1 = 9.56667 loss)
I0523 01:27:33.316130 34682 sgd_solver.cpp:112] Iteration 26840, lr = 0.01
I0523 01:27:37.889209 34682 solver.cpp:239] Iteration 26850 (2.15402 iter/s, 4.64249s/10 iters), loss = 9.14087
I0523 01:27:37.889269 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14087 (* 1 = 9.14087 loss)
I0523 01:27:38.783507 34682 sgd_solver.cpp:112] Iteration 26850, lr = 0.01
I0523 01:27:43.070385 34682 solver.cpp:239] Iteration 26860 (1.93016 iter/s, 5.1809s/10 iters), loss = 8.96352
I0523 01:27:43.070426 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96352 (* 1 = 8.96352 loss)
I0523 01:27:43.145001 34682 sgd_solver.cpp:112] Iteration 26860, lr = 0.01
I0523 01:27:46.351608 34682 solver.cpp:239] Iteration 26870 (3.04782 iter/s, 3.28103s/10 iters), loss = 8.57664
I0523 01:27:46.351657 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57664 (* 1 = 8.57664 loss)
I0523 01:27:47.189824 34682 sgd_solver.cpp:112] Iteration 26870, lr = 0.01
I0523 01:27:52.128980 34682 solver.cpp:239] Iteration 26880 (1.73098 iter/s, 5.77708s/10 iters), loss = 8.46357
I0523 01:27:52.129206 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46357 (* 1 = 8.46357 loss)
I0523 01:27:52.936753 34682 sgd_solver.cpp:112] Iteration 26880, lr = 0.01
I0523 01:27:57.918939 34682 solver.cpp:239] Iteration 26890 (1.72727 iter/s, 5.78949s/10 iters), loss = 8.70277
I0523 01:27:57.918998 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70277 (* 1 = 8.70277 loss)
I0523 01:27:57.987689 34682 sgd_solver.cpp:112] Iteration 26890, lr = 0.01
I0523 01:28:03.056722 34682 solver.cpp:239] Iteration 26900 (1.94647 iter/s, 5.13751s/10 iters), loss = 9.48035
I0523 01:28:03.056767 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.48035 (* 1 = 9.48035 loss)
I0523 01:28:03.129752 34682 sgd_solver.cpp:112] Iteration 26900, lr = 0.01
I0523 01:28:07.617641 34682 solver.cpp:239] Iteration 26910 (2.19265 iter/s, 4.56068s/10 iters), loss = 8.25308
I0523 01:28:07.617698 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25308 (* 1 = 8.25308 loss)
I0523 01:28:07.686869 34682 sgd_solver.cpp:112] Iteration 26910, lr = 0.01
I0523 01:28:11.652171 34682 solver.cpp:239] Iteration 26920 (2.47874 iter/s, 4.0343s/10 iters), loss = 8.26674
I0523 01:28:11.652231 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26674 (* 1 = 8.26674 loss)
I0523 01:28:12.365191 34682 sgd_solver.cpp:112] Iteration 26920, lr = 0.01
I0523 01:28:16.565655 34682 solver.cpp:239] Iteration 26930 (2.03532 iter/s, 4.91322s/10 iters), loss = 8.56626
I0523 01:28:16.565706 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56626 (* 1 = 8.56626 loss)
I0523 01:28:17.145733 34682 sgd_solver.cpp:112] Iteration 26930, lr = 0.01
I0523 01:28:20.385934 34682 solver.cpp:239] Iteration 26940 (2.61776 iter/s, 3.82006s/10 iters), loss = 8.96101
I0523 01:28:20.385983 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96101 (* 1 = 8.96101 loss)
I0523 01:28:20.456357 34682 sgd_solver.cpp:112] Iteration 26940, lr = 0.01
I0523 01:28:23.308589 34682 solver.cpp:239] Iteration 26950 (3.42175 iter/s, 2.92248s/10 iters), loss = 8.23665
I0523 01:28:23.308812 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23665 (* 1 = 8.23665 loss)
I0523 01:28:23.372279 34682 sgd_solver.cpp:112] Iteration 26950, lr = 0.01
I0523 01:28:26.343492 34682 solver.cpp:239] Iteration 26960 (3.29535 iter/s, 3.03458s/10 iters), loss = 9.59985
I0523 01:28:26.343545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59985 (* 1 = 9.59985 loss)
I0523 01:28:26.412729 34682 sgd_solver.cpp:112] Iteration 26960, lr = 0.01
I0523 01:28:30.535303 34682 solver.cpp:239] Iteration 26970 (2.38573 iter/s, 4.19159s/10 iters), loss = 9.3572
I0523 01:28:30.535346 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3572 (* 1 = 9.3572 loss)
I0523 01:28:30.609688 34682 sgd_solver.cpp:112] Iteration 26970, lr = 0.01
I0523 01:28:35.336648 34682 solver.cpp:239] Iteration 26980 (2.08286 iter/s, 4.80109s/10 iters), loss = 8.95133
I0523 01:28:35.336699 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95133 (* 1 = 8.95133 loss)
I0523 01:28:35.421006 34682 sgd_solver.cpp:112] Iteration 26980, lr = 0.01
I0523 01:28:40.490916 34682 solver.cpp:239] Iteration 26990 (1.94024 iter/s, 5.154s/10 iters), loss = 9.07289
I0523 01:28:40.490991 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07289 (* 1 = 9.07289 loss)
I0523 01:28:40.560304 34682 sgd_solver.cpp:112] Iteration 26990, lr = 0.01
I0523 01:28:45.620457 34682 solver.cpp:239] Iteration 27000 (1.9496 iter/s, 5.12926s/10 iters), loss = 8.82237
I0523 01:28:45.620499 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82237 (* 1 = 8.82237 loss)
I0523 01:28:45.693713 34682 sgd_solver.cpp:112] Iteration 27000, lr = 0.01
I0523 01:28:50.368985 34682 solver.cpp:239] Iteration 27010 (2.10602 iter/s, 4.74829s/10 iters), loss = 8.57144
I0523 01:28:50.369042 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57144 (* 1 = 8.57144 loss)
I0523 01:28:50.436126 34682 sgd_solver.cpp:112] Iteration 27010, lr = 0.01
I0523 01:28:54.023414 34682 solver.cpp:239] Iteration 27020 (2.73656 iter/s, 3.65423s/10 iters), loss = 8.92087
I0523 01:28:54.023736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92087 (* 1 = 8.92087 loss)
I0523 01:28:54.095700 34682 sgd_solver.cpp:112] Iteration 27020, lr = 0.01
I0523 01:28:59.227526 34682 solver.cpp:239] Iteration 27030 (1.92175 iter/s, 5.20359s/10 iters), loss = 8.83468
I0523 01:28:59.227591 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83468 (* 1 = 8.83468 loss)
I0523 01:29:00.064018 34682 sgd_solver.cpp:112] Iteration 27030, lr = 0.01
I0523 01:29:03.621140 34682 solver.cpp:239] Iteration 27040 (2.27616 iter/s, 4.39337s/10 iters), loss = 8.67182
I0523 01:29:03.621197 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67182 (* 1 = 8.67182 loss)
I0523 01:29:04.439712 34682 sgd_solver.cpp:112] Iteration 27040, lr = 0.01
I0523 01:29:09.173676 34682 solver.cpp:239] Iteration 27050 (1.80107 iter/s, 5.55225s/10 iters), loss = 9.07919
I0523 01:29:09.173722 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07919 (* 1 = 9.07919 loss)
I0523 01:29:09.969005 34682 sgd_solver.cpp:112] Iteration 27050, lr = 0.01
I0523 01:29:15.913033 34682 solver.cpp:239] Iteration 27060 (1.48389 iter/s, 6.73904s/10 iters), loss = 9.64279
I0523 01:29:15.913086 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.64279 (* 1 = 9.64279 loss)
I0523 01:29:15.974247 34682 sgd_solver.cpp:112] Iteration 27060, lr = 0.01
I0523 01:29:21.681676 34682 solver.cpp:239] Iteration 27070 (1.7336 iter/s, 5.76834s/10 iters), loss = 8.69273
I0523 01:29:21.681740 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69273 (* 1 = 8.69273 loss)
I0523 01:29:22.384268 34682 sgd_solver.cpp:112] Iteration 27070, lr = 0.01
I0523 01:29:27.243216 34682 solver.cpp:239] Iteration 27080 (1.79816 iter/s, 5.56125s/10 iters), loss = 8.87762
I0523 01:29:27.243458 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87762 (* 1 = 8.87762 loss)
I0523 01:29:28.105159 34682 sgd_solver.cpp:112] Iteration 27080, lr = 0.01
I0523 01:29:32.731412 34682 solver.cpp:239] Iteration 27090 (1.82223 iter/s, 5.48778s/10 iters), loss = 9.00238
I0523 01:29:32.731456 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00238 (* 1 = 9.00238 loss)
I0523 01:29:32.792716 34682 sgd_solver.cpp:112] Iteration 27090, lr = 0.01
I0523 01:29:39.788959 34682 solver.cpp:239] Iteration 27100 (1.41699 iter/s, 7.05721s/10 iters), loss = 8.18392
I0523 01:29:39.789014 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18392 (* 1 = 8.18392 loss)
I0523 01:29:39.852716 34682 sgd_solver.cpp:112] Iteration 27100, lr = 0.01
I0523 01:29:44.218163 34682 solver.cpp:239] Iteration 27110 (2.25787 iter/s, 4.42895s/10 iters), loss = 8.71504
I0523 01:29:44.218241 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71504 (* 1 = 8.71504 loss)
I0523 01:29:44.940289 34682 sgd_solver.cpp:112] Iteration 27110, lr = 0.01
I0523 01:29:48.851963 34682 solver.cpp:239] Iteration 27120 (2.15818 iter/s, 4.63353s/10 iters), loss = 8.91457
I0523 01:29:48.852011 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91457 (* 1 = 8.91457 loss)
I0523 01:29:49.703371 34682 sgd_solver.cpp:112] Iteration 27120, lr = 0.01
I0523 01:29:53.079668 34682 solver.cpp:239] Iteration 27130 (2.36548 iter/s, 4.22747s/10 iters), loss = 8.42218
I0523 01:29:53.079723 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42218 (* 1 = 8.42218 loss)
I0523 01:29:53.163375 34682 sgd_solver.cpp:112] Iteration 27130, lr = 0.01
I0523 01:29:58.704629 34682 solver.cpp:239] Iteration 27140 (1.77788 iter/s, 5.62468s/10 iters), loss = 8.24111
I0523 01:29:58.704917 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24111 (* 1 = 8.24111 loss)
I0523 01:29:59.497119 34682 sgd_solver.cpp:112] Iteration 27140, lr = 0.01
I0523 01:30:04.932880 34682 solver.cpp:239] Iteration 27150 (1.60572 iter/s, 6.22774s/10 iters), loss = 8.98254
I0523 01:30:04.932924 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98254 (* 1 = 8.98254 loss)
I0523 01:30:05.009173 34682 sgd_solver.cpp:112] Iteration 27150, lr = 0.01
I0523 01:30:08.357143 34682 solver.cpp:239] Iteration 27160 (2.92051 iter/s, 3.42406s/10 iters), loss = 9.48295
I0523 01:30:08.357197 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.48295 (* 1 = 9.48295 loss)
I0523 01:30:09.152701 34682 sgd_solver.cpp:112] Iteration 27160, lr = 0.01
I0523 01:30:13.920687 34682 solver.cpp:239] Iteration 27170 (1.79751 iter/s, 5.56325s/10 iters), loss = 8.10918
I0523 01:30:13.920758 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10918 (* 1 = 8.10918 loss)
I0523 01:30:14.758723 34682 sgd_solver.cpp:112] Iteration 27170, lr = 0.01
I0523 01:30:18.928786 34682 solver.cpp:239] Iteration 27180 (1.99687 iter/s, 5.00782s/10 iters), loss = 9.24451
I0523 01:30:18.928851 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24451 (* 1 = 9.24451 loss)
I0523 01:30:18.995182 34682 sgd_solver.cpp:112] Iteration 27180, lr = 0.01
I0523 01:30:24.668997 34682 solver.cpp:239] Iteration 27190 (1.74219 iter/s, 5.7399s/10 iters), loss = 9.21468
I0523 01:30:24.669050 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21468 (* 1 = 9.21468 loss)
I0523 01:30:24.737598 34682 sgd_solver.cpp:112] Iteration 27190, lr = 0.01
I0523 01:30:28.745522 34682 solver.cpp:239] Iteration 27200 (2.45321 iter/s, 4.0763s/10 iters), loss = 9.0418
I0523 01:30:28.745654 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0418 (* 1 = 9.0418 loss)
I0523 01:30:28.813475 34682 sgd_solver.cpp:112] Iteration 27200, lr = 0.01
I0523 01:30:32.164742 34682 solver.cpp:239] Iteration 27210 (2.92487 iter/s, 3.41895s/10 iters), loss = 8.92159
I0523 01:30:32.164783 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92159 (* 1 = 8.92159 loss)
I0523 01:30:32.228289 34682 sgd_solver.cpp:112] Iteration 27210, lr = 0.01
I0523 01:30:37.770202 34682 solver.cpp:239] Iteration 27220 (1.78406 iter/s, 5.60518s/10 iters), loss = 8.35342
I0523 01:30:37.770261 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35342 (* 1 = 8.35342 loss)
I0523 01:30:38.487479 34682 sgd_solver.cpp:112] Iteration 27220, lr = 0.01
I0523 01:30:41.149731 34682 solver.cpp:239] Iteration 27230 (2.95916 iter/s, 3.37934s/10 iters), loss = 8.36412
I0523 01:30:41.149776 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36412 (* 1 = 8.36412 loss)
I0523 01:30:41.213976 34682 sgd_solver.cpp:112] Iteration 27230, lr = 0.01
I0523 01:30:45.405603 34682 solver.cpp:239] Iteration 27240 (2.34982 iter/s, 4.25565s/10 iters), loss = 8.14635
I0523 01:30:45.405645 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14635 (* 1 = 8.14635 loss)
I0523 01:30:45.479874 34682 sgd_solver.cpp:112] Iteration 27240, lr = 0.01
I0523 01:30:50.296311 34682 solver.cpp:239] Iteration 27250 (2.0448 iter/s, 4.89044s/10 iters), loss = 8.65465
I0523 01:30:50.296378 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65465 (* 1 = 8.65465 loss)
I0523 01:30:51.168108 34682 sgd_solver.cpp:112] Iteration 27250, lr = 0.01
I0523 01:30:57.945654 34682 solver.cpp:239] Iteration 27260 (1.30736 iter/s, 7.64898s/10 iters), loss = 8.36827
I0523 01:30:57.945693 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36827 (* 1 = 8.36827 loss)
I0523 01:30:58.014077 34682 sgd_solver.cpp:112] Iteration 27260, lr = 0.01
I0523 01:31:02.780663 34682 solver.cpp:239] Iteration 27270 (2.06835 iter/s, 4.83476s/10 iters), loss = 8.7915
I0523 01:31:02.780899 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7915 (* 1 = 8.7915 loss)
I0523 01:31:02.862848 34682 sgd_solver.cpp:112] Iteration 27270, lr = 0.01
I0523 01:31:08.049248 34682 solver.cpp:239] Iteration 27280 (1.89819 iter/s, 5.26818s/10 iters), loss = 9.58166
I0523 01:31:08.049315 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.58166 (* 1 = 9.58166 loss)
I0523 01:31:08.762802 34682 sgd_solver.cpp:112] Iteration 27280, lr = 0.01
I0523 01:31:14.224920 34682 solver.cpp:239] Iteration 27290 (1.61934 iter/s, 6.17535s/10 iters), loss = 8.56464
I0523 01:31:14.224968 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56464 (* 1 = 8.56464 loss)
I0523 01:31:14.297763 34682 sgd_solver.cpp:112] Iteration 27290, lr = 0.01
I0523 01:31:18.332387 34682 solver.cpp:239] Iteration 27300 (2.43472 iter/s, 4.10725s/10 iters), loss = 8.17177
I0523 01:31:18.332429 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17177 (* 1 = 8.17177 loss)
I0523 01:31:19.090045 34682 sgd_solver.cpp:112] Iteration 27300, lr = 0.01
I0523 01:31:24.347609 34682 solver.cpp:239] Iteration 27310 (1.66253 iter/s, 6.01493s/10 iters), loss = 9.33691
I0523 01:31:24.347661 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33691 (* 1 = 9.33691 loss)
I0523 01:31:24.413028 34682 sgd_solver.cpp:112] Iteration 27310, lr = 0.01
I0523 01:31:29.374207 34682 solver.cpp:239] Iteration 27320 (1.98952 iter/s, 5.02634s/10 iters), loss = 8.9242
I0523 01:31:29.374269 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9242 (* 1 = 8.9242 loss)
I0523 01:31:29.447592 34682 sgd_solver.cpp:112] Iteration 27320, lr = 0.01
I0523 01:31:34.077014 34682 solver.cpp:239] Iteration 27330 (2.1265 iter/s, 4.70256s/10 iters), loss = 8.62541
I0523 01:31:34.077304 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62541 (* 1 = 8.62541 loss)
I0523 01:31:34.906847 34682 sgd_solver.cpp:112] Iteration 27330, lr = 0.01
I0523 01:31:38.866843 34682 solver.cpp:239] Iteration 27340 (2.08795 iter/s, 4.78939s/10 iters), loss = 8.47654
I0523 01:31:38.866884 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47654 (* 1 = 8.47654 loss)
I0523 01:31:38.942979 34682 sgd_solver.cpp:112] Iteration 27340, lr = 0.01
I0523 01:31:42.865224 34682 solver.cpp:239] Iteration 27350 (2.50114 iter/s, 3.99817s/10 iters), loss = 8.82901
I0523 01:31:42.865270 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82901 (* 1 = 8.82901 loss)
I0523 01:31:42.924691 34682 sgd_solver.cpp:112] Iteration 27350, lr = 0.01
I0523 01:31:48.269242 34682 solver.cpp:239] Iteration 27360 (1.85057 iter/s, 5.40375s/10 iters), loss = 8.64808
I0523 01:31:48.269296 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64808 (* 1 = 8.64808 loss)
I0523 01:31:48.333164 34682 sgd_solver.cpp:112] Iteration 27360, lr = 0.01
I0523 01:31:51.979993 34682 solver.cpp:239] Iteration 27370 (2.69502 iter/s, 3.71055s/10 iters), loss = 7.81554
I0523 01:31:51.980038 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81554 (* 1 = 7.81554 loss)
I0523 01:31:52.045601 34682 sgd_solver.cpp:112] Iteration 27370, lr = 0.01
I0523 01:31:59.599040 34682 solver.cpp:239] Iteration 27380 (1.31256 iter/s, 7.61869s/10 iters), loss = 8.55984
I0523 01:31:59.599093 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55984 (* 1 = 8.55984 loss)
I0523 01:31:59.667973 34682 sgd_solver.cpp:112] Iteration 27380, lr = 0.01
I0523 01:32:04.681542 34682 solver.cpp:239] Iteration 27390 (1.96764 iter/s, 5.08224s/10 iters), loss = 9.43648
I0523 01:32:04.681789 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43648 (* 1 = 9.43648 loss)
I0523 01:32:05.399044 34682 sgd_solver.cpp:112] Iteration 27390, lr = 0.01
I0523 01:32:09.666082 34682 solver.cpp:239] Iteration 27400 (2.00637 iter/s, 4.98411s/10 iters), loss = 9.04978
I0523 01:32:09.666136 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04978 (* 1 = 9.04978 loss)
I0523 01:32:10.479277 34682 sgd_solver.cpp:112] Iteration 27400, lr = 0.01
I0523 01:32:15.619963 34682 solver.cpp:239] Iteration 27410 (1.67966 iter/s, 5.95358s/10 iters), loss = 8.17394
I0523 01:32:15.620019 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17394 (* 1 = 8.17394 loss)
I0523 01:32:16.317142 34682 sgd_solver.cpp:112] Iteration 27410, lr = 0.01
I0523 01:32:21.232317 34682 solver.cpp:239] Iteration 27420 (1.78187 iter/s, 5.61207s/10 iters), loss = 7.79657
I0523 01:32:21.232375 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79657 (* 1 = 7.79657 loss)
I0523 01:32:21.891867 34682 sgd_solver.cpp:112] Iteration 27420, lr = 0.01
I0523 01:32:29.310832 34682 solver.cpp:239] Iteration 27430 (1.23791 iter/s, 8.07813s/10 iters), loss = 9.20571
I0523 01:32:29.310904 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20571 (* 1 = 9.20571 loss)
I0523 01:32:29.381289 34682 sgd_solver.cpp:112] Iteration 27430, lr = 0.01
I0523 01:32:33.697239 34682 solver.cpp:239] Iteration 27440 (2.2799 iter/s, 4.38615s/10 iters), loss = 8.73048
I0523 01:32:33.697288 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73048 (* 1 = 8.73048 loss)
I0523 01:32:33.770295 34682 sgd_solver.cpp:112] Iteration 27440, lr = 0.01
I0523 01:32:37.718412 34682 solver.cpp:239] Iteration 27450 (2.48697 iter/s, 4.02096s/10 iters), loss = 9.29455
I0523 01:32:37.718730 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29455 (* 1 = 9.29455 loss)
I0523 01:32:37.791786 34682 sgd_solver.cpp:112] Iteration 27450, lr = 0.01
I0523 01:32:42.593968 34682 solver.cpp:239] Iteration 27460 (2.05124 iter/s, 4.8751s/10 iters), loss = 9.269
I0523 01:32:42.594010 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.269 (* 1 = 9.269 loss)
I0523 01:32:42.657392 34682 sgd_solver.cpp:112] Iteration 27460, lr = 0.01
I0523 01:32:45.780570 34682 solver.cpp:239] Iteration 27470 (3.13831 iter/s, 3.18642s/10 iters), loss = 8.63147
I0523 01:32:45.780609 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63147 (* 1 = 8.63147 loss)
I0523 01:32:45.859673 34682 sgd_solver.cpp:112] Iteration 27470, lr = 0.01
I0523 01:32:51.378809 34682 solver.cpp:239] Iteration 27480 (1.78636 iter/s, 5.59796s/10 iters), loss = 9.1908
I0523 01:32:51.378867 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1908 (* 1 = 9.1908 loss)
I0523 01:32:51.448791 34682 sgd_solver.cpp:112] Iteration 27480, lr = 0.01
I0523 01:32:55.621130 34682 solver.cpp:239] Iteration 27490 (2.35733 iter/s, 4.24209s/10 iters), loss = 8.79568
I0523 01:32:55.621170 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79568 (* 1 = 8.79568 loss)
I0523 01:32:55.697167 34682 sgd_solver.cpp:112] Iteration 27490, lr = 0.01
I0523 01:32:58.405251 34682 solver.cpp:239] Iteration 27500 (3.59201 iter/s, 2.78396s/10 iters), loss = 8.75643
I0523 01:32:58.405292 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75643 (* 1 = 8.75643 loss)
I0523 01:32:59.228320 34682 sgd_solver.cpp:112] Iteration 27500, lr = 0.01
I0523 01:33:04.331173 34682 solver.cpp:239] Iteration 27510 (1.68758 iter/s, 5.92564s/10 iters), loss = 9.06068
I0523 01:33:04.331223 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06068 (* 1 = 9.06068 loss)
I0523 01:33:04.400352 34682 sgd_solver.cpp:112] Iteration 27510, lr = 0.01
I0523 01:33:09.310241 34682 solver.cpp:239] Iteration 27520 (2.00851 iter/s, 4.97882s/10 iters), loss = 9.61958
I0523 01:33:09.310431 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.61958 (* 1 = 9.61958 loss)
I0523 01:33:09.375319 34682 sgd_solver.cpp:112] Iteration 27520, lr = 0.01
I0523 01:33:11.949553 34682 solver.cpp:239] Iteration 27530 (3.78927 iter/s, 2.63903s/10 iters), loss = 8.51002
I0523 01:33:11.949599 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51002 (* 1 = 8.51002 loss)
I0523 01:33:12.006940 34682 sgd_solver.cpp:112] Iteration 27530, lr = 0.01
I0523 01:33:15.974990 34682 solver.cpp:239] Iteration 27540 (2.48433 iter/s, 4.02523s/10 iters), loss = 8.81954
I0523 01:33:15.975033 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81954 (* 1 = 8.81954 loss)
I0523 01:33:16.047145 34682 sgd_solver.cpp:112] Iteration 27540, lr = 0.01
I0523 01:33:19.971480 34682 solver.cpp:239] Iteration 27550 (2.50233 iter/s, 3.99627s/10 iters), loss = 8.33752
I0523 01:33:19.971540 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33752 (* 1 = 8.33752 loss)
I0523 01:33:20.785387 34682 sgd_solver.cpp:112] Iteration 27550, lr = 0.01
I0523 01:33:25.774463 34682 solver.cpp:239] Iteration 27560 (1.72334 iter/s, 5.80268s/10 iters), loss = 10.064
I0523 01:33:25.774538 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.064 (* 1 = 10.064 loss)
I0523 01:33:26.023226 34682 sgd_solver.cpp:112] Iteration 27560, lr = 0.01
I0523 01:33:30.900579 34682 solver.cpp:239] Iteration 27570 (1.95091 iter/s, 5.12582s/10 iters), loss = 8.89184
I0523 01:33:30.900647 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89184 (* 1 = 8.89184 loss)
I0523 01:33:31.686758 34682 sgd_solver.cpp:112] Iteration 27570, lr = 0.01
I0523 01:33:37.509598 34682 solver.cpp:239] Iteration 27580 (1.51316 iter/s, 6.60868s/10 iters), loss = 8.93531
I0523 01:33:37.509651 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93531 (* 1 = 8.93531 loss)
I0523 01:33:37.572167 34682 sgd_solver.cpp:112] Iteration 27580, lr = 0.01
I0523 01:33:41.777547 34682 solver.cpp:239] Iteration 27590 (2.34317 iter/s, 4.26773s/10 iters), loss = 9.71709
I0523 01:33:41.777782 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.71709 (* 1 = 9.71709 loss)
I0523 01:33:42.386137 34682 sgd_solver.cpp:112] Iteration 27590, lr = 0.01
I0523 01:33:44.765636 34682 solver.cpp:239] Iteration 27600 (3.347 iter/s, 2.98775s/10 iters), loss = 8.48831
I0523 01:33:44.765678 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48831 (* 1 = 8.48831 loss)
I0523 01:33:44.845021 34682 sgd_solver.cpp:112] Iteration 27600, lr = 0.01
I0523 01:33:49.590772 34682 solver.cpp:239] Iteration 27610 (2.07259 iter/s, 4.82489s/10 iters), loss = 9.02796
I0523 01:33:49.590827 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02796 (* 1 = 9.02796 loss)
I0523 01:33:50.459173 34682 sgd_solver.cpp:112] Iteration 27610, lr = 0.01
I0523 01:33:53.528981 34682 solver.cpp:239] Iteration 27620 (2.53937 iter/s, 3.93798s/10 iters), loss = 8.38883
I0523 01:33:53.529040 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38883 (* 1 = 8.38883 loss)
I0523 01:33:53.594298 34682 sgd_solver.cpp:112] Iteration 27620, lr = 0.01
I0523 01:33:57.959359 34682 solver.cpp:239] Iteration 27630 (2.25727 iter/s, 4.43014s/10 iters), loss = 8.5651
I0523 01:33:57.959408 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5651 (* 1 = 8.5651 loss)
I0523 01:33:58.039135 34682 sgd_solver.cpp:112] Iteration 27630, lr = 0.01
I0523 01:34:02.858316 34682 solver.cpp:239] Iteration 27640 (2.04136 iter/s, 4.89871s/10 iters), loss = 9.15519
I0523 01:34:02.858361 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15519 (* 1 = 9.15519 loss)
I0523 01:34:02.932543 34682 sgd_solver.cpp:112] Iteration 27640, lr = 0.01
I0523 01:34:07.245299 34682 solver.cpp:239] Iteration 27650 (2.27959 iter/s, 4.38675s/10 iters), loss = 7.43782
I0523 01:34:07.245365 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43782 (* 1 = 7.43782 loss)
I0523 01:34:08.077353 34682 sgd_solver.cpp:112] Iteration 27650, lr = 0.01
I0523 01:34:12.477293 34682 solver.cpp:239] Iteration 27660 (1.91142 iter/s, 5.23171s/10 iters), loss = 9.01833
I0523 01:34:12.477509 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01833 (* 1 = 9.01833 loss)
I0523 01:34:12.541810 34682 sgd_solver.cpp:112] Iteration 27660, lr = 0.01
I0523 01:34:15.103971 34682 solver.cpp:239] Iteration 27670 (3.80752 iter/s, 2.62638s/10 iters), loss = 7.98639
I0523 01:34:15.104018 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98639 (* 1 = 7.98639 loss)
I0523 01:34:15.940979 34682 sgd_solver.cpp:112] Iteration 27670, lr = 0.01
I0523 01:34:22.010825 34682 solver.cpp:239] Iteration 27680 (1.44791 iter/s, 6.90653s/10 iters), loss = 8.80456
I0523 01:34:22.010892 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80456 (* 1 = 8.80456 loss)
I0523 01:34:22.086107 34682 sgd_solver.cpp:112] Iteration 27680, lr = 0.01
I0523 01:34:27.012799 34682 solver.cpp:239] Iteration 27690 (1.99932 iter/s, 5.0017s/10 iters), loss = 9.63998
I0523 01:34:27.012850 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.63998 (* 1 = 9.63998 loss)
I0523 01:34:27.725183 34682 sgd_solver.cpp:112] Iteration 27690, lr = 0.01
I0523 01:34:30.214881 34682 solver.cpp:239] Iteration 27700 (3.12315 iter/s, 3.2019s/10 iters), loss = 8.48474
I0523 01:34:30.214923 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48474 (* 1 = 8.48474 loss)
I0523 01:34:30.272315 34682 sgd_solver.cpp:112] Iteration 27700, lr = 0.01
I0523 01:34:34.938072 34682 solver.cpp:239] Iteration 27710 (2.11732 iter/s, 4.72295s/10 iters), loss = 8.57454
I0523 01:34:34.938133 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57454 (* 1 = 8.57454 loss)
I0523 01:34:35.010187 34682 sgd_solver.cpp:112] Iteration 27710, lr = 0.01
I0523 01:34:39.507344 34682 solver.cpp:239] Iteration 27720 (2.18865 iter/s, 4.56902s/10 iters), loss = 8.40647
I0523 01:34:39.507383 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40647 (* 1 = 8.40647 loss)
I0523 01:34:39.582680 34682 sgd_solver.cpp:112] Iteration 27720, lr = 0.01
I0523 01:34:43.226866 34682 solver.cpp:239] Iteration 27730 (2.68866 iter/s, 3.71932s/10 iters), loss = 7.95913
I0523 01:34:43.227118 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95913 (* 1 = 7.95913 loss)
I0523 01:34:44.011406 34682 sgd_solver.cpp:112] Iteration 27730, lr = 0.01
I0523 01:34:49.683094 34682 solver.cpp:239] Iteration 27740 (1.54901 iter/s, 6.45573s/10 iters), loss = 8.9439
I0523 01:34:49.683147 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9439 (* 1 = 8.9439 loss)
I0523 01:34:49.753327 34682 sgd_solver.cpp:112] Iteration 27740, lr = 0.01
I0523 01:34:54.461848 34682 solver.cpp:239] Iteration 27750 (2.0927 iter/s, 4.77851s/10 iters), loss = 9.56903
I0523 01:34:54.461906 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56903 (* 1 = 9.56903 loss)
I0523 01:34:55.133261 34682 sgd_solver.cpp:112] Iteration 27750, lr = 0.01
I0523 01:35:03.076977 34682 solver.cpp:239] Iteration 27760 (1.1608 iter/s, 8.61472s/10 iters), loss = 9.31651
I0523 01:35:03.077029 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31651 (* 1 = 9.31651 loss)
I0523 01:35:03.879670 34682 sgd_solver.cpp:112] Iteration 27760, lr = 0.01
I0523 01:35:09.749168 34682 solver.cpp:239] Iteration 27770 (1.49883 iter/s, 6.67187s/10 iters), loss = 8.2904
I0523 01:35:09.749219 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2904 (* 1 = 8.2904 loss)
I0523 01:35:09.811550 34682 sgd_solver.cpp:112] Iteration 27770, lr = 0.01
I0523 01:35:15.498782 34682 solver.cpp:239] Iteration 27780 (1.73933 iter/s, 5.74933s/10 iters), loss = 8.27314
I0523 01:35:15.499022 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27314 (* 1 = 8.27314 loss)
I0523 01:35:15.557351 34682 sgd_solver.cpp:112] Iteration 27780, lr = 0.01
I0523 01:35:18.795467 34682 solver.cpp:239] Iteration 27790 (3.03367 iter/s, 3.29634s/10 iters), loss = 8.95502
I0523 01:35:18.795514 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95502 (* 1 = 8.95502 loss)
I0523 01:35:19.651263 34682 sgd_solver.cpp:112] Iteration 27790, lr = 0.01
I0523 01:35:22.066761 34682 solver.cpp:239] Iteration 27800 (3.05707 iter/s, 3.27111s/10 iters), loss = 8.82666
I0523 01:35:22.066804 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82666 (* 1 = 8.82666 loss)
I0523 01:35:22.150683 34682 sgd_solver.cpp:112] Iteration 27800, lr = 0.01
I0523 01:35:26.677456 34682 solver.cpp:239] Iteration 27810 (2.16898 iter/s, 4.61045s/10 iters), loss = 8.82393
I0523 01:35:26.677512 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82393 (* 1 = 8.82393 loss)
I0523 01:35:27.520138 34682 sgd_solver.cpp:112] Iteration 27810, lr = 0.01
I0523 01:35:33.975994 34682 solver.cpp:239] Iteration 27820 (1.3702 iter/s, 7.29819s/10 iters), loss = 8.77661
I0523 01:35:33.976044 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77661 (* 1 = 8.77661 loss)
I0523 01:35:34.821310 34682 sgd_solver.cpp:112] Iteration 27820, lr = 0.01
I0523 01:35:40.396509 34682 solver.cpp:239] Iteration 27830 (1.55758 iter/s, 6.42021s/10 iters), loss = 8.95941
I0523 01:35:40.396549 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95941 (* 1 = 8.95941 loss)
I0523 01:35:40.469266 34682 sgd_solver.cpp:112] Iteration 27830, lr = 0.01
I0523 01:35:45.493765 34682 solver.cpp:239] Iteration 27840 (1.96194 iter/s, 5.097s/10 iters), loss = 8.44709
I0523 01:35:45.493827 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44709 (* 1 = 8.44709 loss)
I0523 01:35:45.557202 34682 sgd_solver.cpp:112] Iteration 27840, lr = 0.01
I0523 01:35:50.108919 34682 solver.cpp:239] Iteration 27850 (2.16689 iter/s, 4.61491s/10 iters), loss = 9.13131
I0523 01:35:50.108966 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13131 (* 1 = 9.13131 loss)
I0523 01:35:50.165588 34682 sgd_solver.cpp:112] Iteration 27850, lr = 0.01
I0523 01:35:54.431761 34682 solver.cpp:239] Iteration 27860 (2.31343 iter/s, 4.32259s/10 iters), loss = 9.71203
I0523 01:35:54.431831 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.71203 (* 1 = 9.71203 loss)
I0523 01:35:54.676769 34682 sgd_solver.cpp:112] Iteration 27860, lr = 0.01
I0523 01:36:01.235391 34682 solver.cpp:239] Iteration 27870 (1.46988 iter/s, 6.80327s/10 iters), loss = 8.89674
I0523 01:36:01.235442 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89674 (* 1 = 8.89674 loss)
I0523 01:36:01.295272 34682 sgd_solver.cpp:112] Iteration 27870, lr = 0.01
I0523 01:36:05.604907 34682 solver.cpp:239] Iteration 27880 (2.2887 iter/s, 4.36929s/10 iters), loss = 8.96049
I0523 01:36:05.604945 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96049 (* 1 = 8.96049 loss)
I0523 01:36:05.673799 34682 sgd_solver.cpp:112] Iteration 27880, lr = 0.01
I0523 01:36:11.309342 34682 solver.cpp:239] Iteration 27890 (1.7531 iter/s, 5.70417s/10 iters), loss = 8.95287
I0523 01:36:11.309386 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95287 (* 1 = 8.95287 loss)
I0523 01:36:12.182576 34682 sgd_solver.cpp:112] Iteration 27890, lr = 0.01
I0523 01:36:17.111502 34682 solver.cpp:239] Iteration 27900 (1.72358 iter/s, 5.80188s/10 iters), loss = 7.87318
I0523 01:36:17.111634 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87318 (* 1 = 7.87318 loss)
I0523 01:36:17.183697 34682 sgd_solver.cpp:112] Iteration 27900, lr = 0.01
I0523 01:36:22.246323 34682 solver.cpp:239] Iteration 27910 (1.94762 iter/s, 5.13448s/10 iters), loss = 9.66987
I0523 01:36:22.246372 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.66987 (* 1 = 9.66987 loss)
I0523 01:36:23.087218 34682 sgd_solver.cpp:112] Iteration 27910, lr = 0.01
I0523 01:36:26.450633 34682 solver.cpp:239] Iteration 27920 (2.37864 iter/s, 4.20408s/10 iters), loss = 9.30345
I0523 01:36:26.450675 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30345 (* 1 = 9.30345 loss)
I0523 01:36:26.530129 34682 sgd_solver.cpp:112] Iteration 27920, lr = 0.01
I0523 01:36:29.058229 34682 solver.cpp:239] Iteration 27930 (3.83518 iter/s, 2.60744s/10 iters), loss = 8.64355
I0523 01:36:29.058269 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64355 (* 1 = 8.64355 loss)
I0523 01:36:29.130440 34682 sgd_solver.cpp:112] Iteration 27930, lr = 0.01
I0523 01:36:34.065122 34682 solver.cpp:239] Iteration 27940 (1.99735 iter/s, 5.00664s/10 iters), loss = 8.69651
I0523 01:36:34.065171 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69651 (* 1 = 8.69651 loss)
I0523 01:36:34.131587 34682 sgd_solver.cpp:112] Iteration 27940, lr = 0.01
I0523 01:36:38.708250 34682 solver.cpp:239] Iteration 27950 (2.15384 iter/s, 4.64288s/10 iters), loss = 9.14398
I0523 01:36:38.708298 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14398 (* 1 = 9.14398 loss)
I0523 01:36:38.776811 34682 sgd_solver.cpp:112] Iteration 27950, lr = 0.01
I0523 01:36:42.192620 34682 solver.cpp:239] Iteration 27960 (2.87012 iter/s, 3.48418s/10 iters), loss = 9.10014
I0523 01:36:42.192664 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10014 (* 1 = 9.10014 loss)
I0523 01:36:42.744051 34682 sgd_solver.cpp:112] Iteration 27960, lr = 0.01
I0523 01:36:46.917024 34682 solver.cpp:239] Iteration 27970 (2.11679 iter/s, 4.72413s/10 iters), loss = 9.19901
I0523 01:36:46.917101 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19901 (* 1 = 9.19901 loss)
I0523 01:36:47.633839 34682 sgd_solver.cpp:112] Iteration 27970, lr = 0.01
I0523 01:36:53.287292 34682 solver.cpp:239] Iteration 27980 (1.56988 iter/s, 6.36993s/10 iters), loss = 8.60834
I0523 01:36:53.287335 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60834 (* 1 = 8.60834 loss)
I0523 01:36:53.353077 34682 sgd_solver.cpp:112] Iteration 27980, lr = 0.01
I0523 01:36:57.136363 34682 solver.cpp:239] Iteration 27990 (2.59817 iter/s, 3.84886s/10 iters), loss = 8.75726
I0523 01:36:57.136410 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75726 (* 1 = 8.75726 loss)
I0523 01:36:57.202359 34682 sgd_solver.cpp:112] Iteration 27990, lr = 0.01
I0523 01:37:01.380779 34682 solver.cpp:239] Iteration 28000 (2.35616 iter/s, 4.24419s/10 iters), loss = 7.96175
I0523 01:37:01.380832 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96175 (* 1 = 7.96175 loss)
I0523 01:37:01.450597 34682 sgd_solver.cpp:112] Iteration 28000, lr = 0.01
I0523 01:37:06.983307 34682 solver.cpp:239] Iteration 28010 (1.785 iter/s, 5.60225s/10 iters), loss = 8.90176
I0523 01:37:06.983350 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90176 (* 1 = 8.90176 loss)
I0523 01:37:07.067605 34682 sgd_solver.cpp:112] Iteration 28010, lr = 0.01
I0523 01:37:10.095645 34682 solver.cpp:239] Iteration 28020 (3.21321 iter/s, 3.11216s/10 iters), loss = 9.70161
I0523 01:37:10.095698 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.70161 (* 1 = 9.70161 loss)
I0523 01:37:10.173315 34682 sgd_solver.cpp:112] Iteration 28020, lr = 0.01
I0523 01:37:14.351542 34682 solver.cpp:239] Iteration 28030 (2.35135 iter/s, 4.25287s/10 iters), loss = 8.57427
I0523 01:37:14.351590 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57427 (* 1 = 8.57427 loss)
I0523 01:37:14.418190 34682 sgd_solver.cpp:112] Iteration 28030, lr = 0.01
I0523 01:37:19.782073 34682 solver.cpp:239] Iteration 28040 (1.84153 iter/s, 5.43026s/10 iters), loss = 8.22102
I0523 01:37:19.782286 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22102 (* 1 = 8.22102 loss)
I0523 01:37:19.845170 34682 sgd_solver.cpp:112] Iteration 28040, lr = 0.01
I0523 01:37:24.183650 34682 solver.cpp:239] Iteration 28050 (2.2721 iter/s, 4.40121s/10 iters), loss = 9.1016
I0523 01:37:24.183701 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1016 (* 1 = 9.1016 loss)
I0523 01:37:24.244776 34682 sgd_solver.cpp:112] Iteration 28050, lr = 0.01
I0523 01:37:28.242179 34682 solver.cpp:239] Iteration 28060 (2.46409 iter/s, 4.0583s/10 iters), loss = 8.76667
I0523 01:37:28.242236 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76667 (* 1 = 8.76667 loss)
I0523 01:37:28.302961 34682 sgd_solver.cpp:112] Iteration 28060, lr = 0.01
I0523 01:37:31.783409 34682 solver.cpp:239] Iteration 28070 (2.82405 iter/s, 3.54101s/10 iters), loss = 8.72534
I0523 01:37:31.783465 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72534 (* 1 = 8.72534 loss)
I0523 01:37:32.609460 34682 sgd_solver.cpp:112] Iteration 28070, lr = 0.01
I0523 01:37:36.744182 34682 solver.cpp:239] Iteration 28080 (2.01592 iter/s, 4.96052s/10 iters), loss = 9.47865
I0523 01:37:36.744225 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47865 (* 1 = 9.47865 loss)
I0523 01:37:36.816841 34682 sgd_solver.cpp:112] Iteration 28080, lr = 0.01
I0523 01:37:43.018227 34682 solver.cpp:239] Iteration 28090 (1.59395 iter/s, 6.27374s/10 iters), loss = 8.41651
I0523 01:37:43.018301 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41651 (* 1 = 8.41651 loss)
I0523 01:37:43.076036 34682 sgd_solver.cpp:112] Iteration 28090, lr = 0.01
I0523 01:37:47.424840 34682 solver.cpp:239] Iteration 28100 (2.26945 iter/s, 4.40636s/10 iters), loss = 9.17984
I0523 01:37:47.424912 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17984 (* 1 = 9.17984 loss)
I0523 01:37:47.495374 34682 sgd_solver.cpp:112] Iteration 28100, lr = 0.01
I0523 01:37:50.929381 34682 solver.cpp:239] Iteration 28110 (2.85362 iter/s, 3.50432s/10 iters), loss = 8.41952
I0523 01:37:50.929579 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41952 (* 1 = 8.41952 loss)
I0523 01:37:51.672199 34682 sgd_solver.cpp:112] Iteration 28110, lr = 0.01
I0523 01:37:56.325165 34682 solver.cpp:239] Iteration 28120 (1.85344 iter/s, 5.39537s/10 iters), loss = 8.75091
I0523 01:37:56.325212 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75091 (* 1 = 8.75091 loss)
I0523 01:37:56.397266 34682 sgd_solver.cpp:112] Iteration 28120, lr = 0.01
I0523 01:38:00.449306 34682 solver.cpp:239] Iteration 28130 (2.42488 iter/s, 4.12392s/10 iters), loss = 8.38568
I0523 01:38:00.449369 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38568 (* 1 = 8.38568 loss)
I0523 01:38:01.194591 34682 sgd_solver.cpp:112] Iteration 28130, lr = 0.01
I0523 01:38:05.381223 34682 solver.cpp:239] Iteration 28140 (2.02772 iter/s, 4.93164s/10 iters), loss = 9.36333
I0523 01:38:05.381279 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36333 (* 1 = 9.36333 loss)
I0523 01:38:05.827983 34682 sgd_solver.cpp:112] Iteration 28140, lr = 0.01
I0523 01:38:10.816134 34682 solver.cpp:239] Iteration 28150 (1.84006 iter/s, 5.43461s/10 iters), loss = 8.90802
I0523 01:38:10.816195 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90802 (* 1 = 8.90802 loss)
I0523 01:38:10.892241 34682 sgd_solver.cpp:112] Iteration 28150, lr = 0.01
I0523 01:38:14.671983 34682 solver.cpp:239] Iteration 28160 (2.59361 iter/s, 3.85563s/10 iters), loss = 8.62574
I0523 01:38:14.672032 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62574 (* 1 = 8.62574 loss)
I0523 01:38:14.746371 34682 sgd_solver.cpp:112] Iteration 28160, lr = 0.01
I0523 01:38:18.924170 34682 solver.cpp:239] Iteration 28170 (2.35186 iter/s, 4.25196s/10 iters), loss = 8.33762
I0523 01:38:18.924213 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33762 (* 1 = 8.33762 loss)
I0523 01:38:18.998816 34682 sgd_solver.cpp:112] Iteration 28170, lr = 0.01
I0523 01:38:25.514170 34682 solver.cpp:239] Iteration 28180 (1.51753 iter/s, 6.58968s/10 iters), loss = 8.74125
I0523 01:38:25.514434 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74125 (* 1 = 8.74125 loss)
I0523 01:38:26.131170 34682 sgd_solver.cpp:112] Iteration 28180, lr = 0.01
I0523 01:38:31.177145 34682 solver.cpp:239] Iteration 28190 (1.766 iter/s, 5.66251s/10 iters), loss = 9.96846
I0523 01:38:31.177193 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.96846 (* 1 = 9.96846 loss)
I0523 01:38:31.366163 34682 sgd_solver.cpp:112] Iteration 28190, lr = 0.01
I0523 01:38:36.208395 34682 solver.cpp:239] Iteration 28200 (1.98768 iter/s, 5.03099s/10 iters), loss = 9.12606
I0523 01:38:36.208441 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12606 (* 1 = 9.12606 loss)
I0523 01:38:36.281661 34682 sgd_solver.cpp:112] Iteration 28200, lr = 0.01
I0523 01:38:41.086378 34682 solver.cpp:239] Iteration 28210 (2.05014 iter/s, 4.87773s/10 iters), loss = 8.79663
I0523 01:38:41.086436 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79663 (* 1 = 8.79663 loss)
I0523 01:38:41.144976 34682 sgd_solver.cpp:112] Iteration 28210, lr = 0.01
I0523 01:38:46.914621 34682 solver.cpp:239] Iteration 28220 (1.71587 iter/s, 5.82795s/10 iters), loss = 9.19658
I0523 01:38:46.914671 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19658 (* 1 = 9.19658 loss)
I0523 01:38:47.605556 34682 sgd_solver.cpp:112] Iteration 28220, lr = 0.01
I0523 01:38:53.043001 34682 solver.cpp:239] Iteration 28230 (1.63183 iter/s, 6.12809s/10 iters), loss = 8.37342
I0523 01:38:53.043041 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37342 (* 1 = 8.37342 loss)
I0523 01:38:53.134474 34682 sgd_solver.cpp:112] Iteration 28230, lr = 0.01
I0523 01:38:57.229465 34682 solver.cpp:239] Iteration 28240 (2.38877 iter/s, 4.18625s/10 iters), loss = 8.60121
I0523 01:38:57.229635 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60121 (* 1 = 8.60121 loss)
I0523 01:38:57.306905 34682 sgd_solver.cpp:112] Iteration 28240, lr = 0.01
I0523 01:39:02.874855 34682 solver.cpp:239] Iteration 28250 (1.77147 iter/s, 5.64502s/10 iters), loss = 8.41943
I0523 01:39:02.874908 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41943 (* 1 = 8.41943 loss)
I0523 01:39:03.490409 34682 sgd_solver.cpp:112] Iteration 28250, lr = 0.01
I0523 01:39:08.103643 34682 solver.cpp:239] Iteration 28260 (1.91259 iter/s, 5.22852s/10 iters), loss = 8.86256
I0523 01:39:08.103691 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86256 (* 1 = 8.86256 loss)
I0523 01:39:08.180409 34682 sgd_solver.cpp:112] Iteration 28260, lr = 0.01
I0523 01:39:14.818076 34682 solver.cpp:239] Iteration 28270 (1.4894 iter/s, 6.71412s/10 iters), loss = 8.87539
I0523 01:39:14.818122 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87539 (* 1 = 8.87539 loss)
I0523 01:39:14.882897 34682 sgd_solver.cpp:112] Iteration 28270, lr = 0.01
I0523 01:39:18.833209 34682 solver.cpp:239] Iteration 28280 (2.49071 iter/s, 4.01492s/10 iters), loss = 8.96706
I0523 01:39:18.833256 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96706 (* 1 = 8.96706 loss)
I0523 01:39:18.923959 34682 sgd_solver.cpp:112] Iteration 28280, lr = 0.01
I0523 01:39:24.218461 34682 solver.cpp:239] Iteration 28290 (1.85702 iter/s, 5.38499s/10 iters), loss = 9.40461
I0523 01:39:24.218506 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40461 (* 1 = 9.40461 loss)
I0523 01:39:24.274565 34682 sgd_solver.cpp:112] Iteration 28290, lr = 0.01
I0523 01:39:30.008859 34682 solver.cpp:239] Iteration 28300 (1.72708 iter/s, 5.79011s/10 iters), loss = 8.66576
I0523 01:39:30.009125 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66576 (* 1 = 8.66576 loss)
I0523 01:39:30.078438 34682 sgd_solver.cpp:112] Iteration 28300, lr = 0.01
I0523 01:39:34.175269 34682 solver.cpp:239] Iteration 28310 (2.40038 iter/s, 4.16602s/10 iters), loss = 9.02924
I0523 01:39:34.175313 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02924 (* 1 = 9.02924 loss)
I0523 01:39:34.245735 34682 sgd_solver.cpp:112] Iteration 28310, lr = 0.01
I0523 01:39:38.559661 34682 solver.cpp:239] Iteration 28320 (2.28094 iter/s, 4.38417s/10 iters), loss = 8.89108
I0523 01:39:38.559711 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89108 (* 1 = 8.89108 loss)
I0523 01:39:38.637784 34682 sgd_solver.cpp:112] Iteration 28320, lr = 0.01
I0523 01:39:42.685897 34682 solver.cpp:239] Iteration 28330 (2.42365 iter/s, 4.12601s/10 iters), loss = 8.67803
I0523 01:39:42.685974 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67803 (* 1 = 8.67803 loss)
I0523 01:39:43.505522 34682 sgd_solver.cpp:112] Iteration 28330, lr = 0.01
I0523 01:39:46.324396 34682 solver.cpp:239] Iteration 28340 (2.74856 iter/s, 3.63826s/10 iters), loss = 9.30818
I0523 01:39:46.324460 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30818 (* 1 = 9.30818 loss)
I0523 01:39:47.003703 34682 sgd_solver.cpp:112] Iteration 28340, lr = 0.01
I0523 01:39:51.123855 34682 solver.cpp:239] Iteration 28350 (2.08368 iter/s, 4.7992s/10 iters), loss = 8.4157
I0523 01:39:51.123910 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4157 (* 1 = 8.4157 loss)
I0523 01:39:51.583606 34682 sgd_solver.cpp:112] Iteration 28350, lr = 0.01
I0523 01:39:56.454071 34682 solver.cpp:239] Iteration 28360 (1.8762 iter/s, 5.32993s/10 iters), loss = 8.59678
I0523 01:39:56.454133 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59678 (* 1 = 8.59678 loss)
I0523 01:39:57.110491 34682 sgd_solver.cpp:112] Iteration 28360, lr = 0.01
I0523 01:40:02.152488 34682 solver.cpp:239] Iteration 28370 (1.75496 iter/s, 5.69813s/10 iters), loss = 9.13054
I0523 01:40:02.152763 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13054 (* 1 = 9.13054 loss)
I0523 01:40:03.021154 34682 sgd_solver.cpp:112] Iteration 28370, lr = 0.01
I0523 01:40:06.431128 34682 solver.cpp:239] Iteration 28380 (2.33743 iter/s, 4.27821s/10 iters), loss = 8.71886
I0523 01:40:06.431193 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71886 (* 1 = 8.71886 loss)
I0523 01:40:07.221560 34682 sgd_solver.cpp:112] Iteration 28380, lr = 0.01
I0523 01:40:12.107154 34682 solver.cpp:239] Iteration 28390 (1.76189 iter/s, 5.67573s/10 iters), loss = 8.82295
I0523 01:40:12.107205 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82295 (* 1 = 8.82295 loss)
I0523 01:40:12.189373 34682 sgd_solver.cpp:112] Iteration 28390, lr = 0.01
I0523 01:40:16.305652 34682 solver.cpp:239] Iteration 28400 (2.38194 iter/s, 4.19827s/10 iters), loss = 8.44802
I0523 01:40:16.305697 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44802 (* 1 = 8.44802 loss)
I0523 01:40:16.386714 34682 sgd_solver.cpp:112] Iteration 28400, lr = 0.01
I0523 01:40:22.703125 34682 solver.cpp:239] Iteration 28410 (1.56319 iter/s, 6.39717s/10 iters), loss = 8.2908
I0523 01:40:22.703169 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2908 (* 1 = 8.2908 loss)
I0523 01:40:22.782269 34682 sgd_solver.cpp:112] Iteration 28410, lr = 0.01
I0523 01:40:27.171428 34682 solver.cpp:239] Iteration 28420 (2.2381 iter/s, 4.46807s/10 iters), loss = 9.27036
I0523 01:40:27.171540 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27036 (* 1 = 9.27036 loss)
I0523 01:40:27.232470 34682 sgd_solver.cpp:112] Iteration 28420, lr = 0.01
I0523 01:40:34.027575 34682 solver.cpp:239] Iteration 28430 (1.45863 iter/s, 6.85577s/10 iters), loss = 8.68238
I0523 01:40:34.027786 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68238 (* 1 = 8.68238 loss)
I0523 01:40:34.091109 34682 sgd_solver.cpp:112] Iteration 28430, lr = 0.01
I0523 01:40:37.518846 34682 solver.cpp:239] Iteration 28440 (2.86455 iter/s, 3.49095s/10 iters), loss = 9.50311
I0523 01:40:37.518889 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.50311 (* 1 = 9.50311 loss)
I0523 01:40:38.339112 34682 sgd_solver.cpp:112] Iteration 28440, lr = 0.01
I0523 01:40:43.491598 34682 solver.cpp:239] Iteration 28450 (1.67435 iter/s, 5.97246s/10 iters), loss = 8.42422
I0523 01:40:43.491655 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42422 (* 1 = 8.42422 loss)
I0523 01:40:44.318207 34682 sgd_solver.cpp:112] Iteration 28450, lr = 0.01
I0523 01:40:49.348392 34682 solver.cpp:239] Iteration 28460 (1.70751 iter/s, 5.8565s/10 iters), loss = 10.2035
I0523 01:40:49.348460 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2035 (* 1 = 10.2035 loss)
I0523 01:40:49.892386 34682 sgd_solver.cpp:112] Iteration 28460, lr = 0.01
I0523 01:40:53.999682 34682 solver.cpp:239] Iteration 28470 (2.15006 iter/s, 4.65102s/10 iters), loss = 7.80012
I0523 01:40:53.999742 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80012 (* 1 = 7.80012 loss)
I0523 01:40:54.063848 34682 sgd_solver.cpp:112] Iteration 28470, lr = 0.01
I0523 01:40:59.160485 34682 solver.cpp:239] Iteration 28480 (1.93778 iter/s, 5.16054s/10 iters), loss = 8.0667
I0523 01:40:59.160531 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0667 (* 1 = 8.0667 loss)
I0523 01:40:59.883956 34682 sgd_solver.cpp:112] Iteration 28480, lr = 0.01
I0523 01:41:01.697779 34682 solver.cpp:239] Iteration 28490 (3.94147 iter/s, 2.53712s/10 iters), loss = 7.94502
I0523 01:41:01.697824 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94502 (* 1 = 7.94502 loss)
I0523 01:41:01.781018 34682 sgd_solver.cpp:112] Iteration 28490, lr = 0.01
I0523 01:41:06.501471 34682 solver.cpp:239] Iteration 28500 (2.08184 iter/s, 4.80345s/10 iters), loss = 9.05361
I0523 01:41:06.501590 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05361 (* 1 = 9.05361 loss)
I0523 01:41:06.581938 34682 sgd_solver.cpp:112] Iteration 28500, lr = 0.01
I0523 01:41:12.314640 34682 solver.cpp:239] Iteration 28510 (1.72034 iter/s, 5.81281s/10 iters), loss = 8.99882
I0523 01:41:12.314709 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99882 (* 1 = 8.99882 loss)
I0523 01:41:12.376271 34682 sgd_solver.cpp:112] Iteration 28510, lr = 0.01
I0523 01:41:17.758206 34682 solver.cpp:239] Iteration 28520 (1.83713 iter/s, 5.44329s/10 iters), loss = 10.2399
I0523 01:41:17.758251 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2399 (* 1 = 10.2399 loss)
I0523 01:41:17.832789 34682 sgd_solver.cpp:112] Iteration 28520, lr = 0.01
I0523 01:41:21.280364 34682 solver.cpp:239] Iteration 28530 (2.83933 iter/s, 3.52196s/10 iters), loss = 7.97971
I0523 01:41:21.280421 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97971 (* 1 = 7.97971 loss)
I0523 01:41:22.148809 34682 sgd_solver.cpp:112] Iteration 28530, lr = 0.01
I0523 01:41:26.109411 34682 solver.cpp:239] Iteration 28540 (2.07092 iter/s, 4.82878s/10 iters), loss = 9.74627
I0523 01:41:26.109467 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.74627 (* 1 = 9.74627 loss)
I0523 01:41:26.975706 34682 sgd_solver.cpp:112] Iteration 28540, lr = 0.01
I0523 01:41:31.896217 34682 solver.cpp:239] Iteration 28550 (1.72816 iter/s, 5.78652s/10 iters), loss = 8.17017
I0523 01:41:31.896267 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17017 (* 1 = 8.17017 loss)
I0523 01:41:32.280023 34682 sgd_solver.cpp:112] Iteration 28550, lr = 0.01
I0523 01:41:36.594501 34682 solver.cpp:239] Iteration 28560 (2.12855 iter/s, 4.69804s/10 iters), loss = 8.92095
I0523 01:41:36.594756 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92095 (* 1 = 8.92095 loss)
I0523 01:41:37.418375 34682 sgd_solver.cpp:112] Iteration 28560, lr = 0.01
I0523 01:41:41.755038 34682 solver.cpp:239] Iteration 28570 (1.93796 iter/s, 5.16007s/10 iters), loss = 8.91225
I0523 01:41:41.755089 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91225 (* 1 = 8.91225 loss)
I0523 01:41:42.576458 34682 sgd_solver.cpp:112] Iteration 28570, lr = 0.01
I0523 01:41:48.109163 34682 solver.cpp:239] Iteration 28580 (1.57386 iter/s, 6.35381s/10 iters), loss = 9.12977
I0523 01:41:48.109221 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12977 (* 1 = 9.12977 loss)
I0523 01:41:48.880354 34682 sgd_solver.cpp:112] Iteration 28580, lr = 0.01
I0523 01:41:52.036438 34682 solver.cpp:239] Iteration 28590 (2.54644 iter/s, 3.92705s/10 iters), loss = 8.70964
I0523 01:41:52.036492 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70964 (* 1 = 8.70964 loss)
I0523 01:41:52.850296 34682 sgd_solver.cpp:112] Iteration 28590, lr = 0.01
I0523 01:41:57.916602 34682 solver.cpp:239] Iteration 28600 (1.70072 iter/s, 5.87987s/10 iters), loss = 9.19639
I0523 01:41:57.916651 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19639 (* 1 = 9.19639 loss)
I0523 01:41:57.987025 34682 sgd_solver.cpp:112] Iteration 28600, lr = 0.01
I0523 01:42:01.381237 34682 solver.cpp:239] Iteration 28610 (2.88647 iter/s, 3.46444s/10 iters), loss = 8.22572
I0523 01:42:01.381276 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22572 (* 1 = 8.22572 loss)
I0523 01:42:01.455281 34682 sgd_solver.cpp:112] Iteration 28610, lr = 0.01
I0523 01:42:06.981389 34682 solver.cpp:239] Iteration 28620 (1.78575 iter/s, 5.59988s/10 iters), loss = 9.37109
I0523 01:42:06.981600 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.37109 (* 1 = 9.37109 loss)
I0523 01:42:07.046962 34682 sgd_solver.cpp:112] Iteration 28620, lr = 0.01
I0523 01:42:12.718308 34682 solver.cpp:239] Iteration 28630 (1.74322 iter/s, 5.7365s/10 iters), loss = 8.0014
I0523 01:42:12.718370 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0014 (* 1 = 8.0014 loss)
I0523 01:42:13.375661 34682 sgd_solver.cpp:112] Iteration 28630, lr = 0.01
I0523 01:42:19.016434 34682 solver.cpp:239] Iteration 28640 (1.58785 iter/s, 6.29781s/10 iters), loss = 8.52746
I0523 01:42:19.016495 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52746 (* 1 = 8.52746 loss)
I0523 01:42:19.086810 34682 sgd_solver.cpp:112] Iteration 28640, lr = 0.01
I0523 01:42:23.832182 34682 solver.cpp:239] Iteration 28650 (2.07663 iter/s, 4.81549s/10 iters), loss = 8.913
I0523 01:42:23.832244 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.913 (* 1 = 8.913 loss)
I0523 01:42:24.466770 34682 sgd_solver.cpp:112] Iteration 28650, lr = 0.01
I0523 01:42:27.785491 34682 solver.cpp:239] Iteration 28660 (2.52967 iter/s, 3.95309s/10 iters), loss = 8.25509
I0523 01:42:27.785537 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25509 (* 1 = 8.25509 loss)
I0523 01:42:27.855613 34682 sgd_solver.cpp:112] Iteration 28660, lr = 0.01
I0523 01:42:31.255300 34682 solver.cpp:239] Iteration 28670 (2.88216 iter/s, 3.46961s/10 iters), loss = 8.6765
I0523 01:42:31.255348 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6765 (* 1 = 8.6765 loss)
I0523 01:42:32.099354 34682 sgd_solver.cpp:112] Iteration 28670, lr = 0.01
I0523 01:42:35.499292 34682 solver.cpp:239] Iteration 28680 (2.3564 iter/s, 4.24376s/10 iters), loss = 8.87674
I0523 01:42:35.499351 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87674 (* 1 = 8.87674 loss)
I0523 01:42:36.368667 34682 sgd_solver.cpp:112] Iteration 28680, lr = 0.01
I0523 01:42:40.446671 34682 solver.cpp:239] Iteration 28690 (2.02138 iter/s, 4.94712s/10 iters), loss = 9.20479
I0523 01:42:40.447001 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20479 (* 1 = 9.20479 loss)
I0523 01:42:40.511065 34682 sgd_solver.cpp:112] Iteration 28690, lr = 0.01
I0523 01:42:45.320252 34682 solver.cpp:239] Iteration 28700 (2.05208 iter/s, 4.8731s/10 iters), loss = 9.15144
I0523 01:42:45.320293 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15144 (* 1 = 9.15144 loss)
I0523 01:42:45.393242 34682 sgd_solver.cpp:112] Iteration 28700, lr = 0.01
I0523 01:42:51.033300 34682 solver.cpp:239] Iteration 28710 (1.75047 iter/s, 5.71275s/10 iters), loss = 7.35678
I0523 01:42:51.033372 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.35678 (* 1 = 7.35678 loss)
I0523 01:42:51.852854 34682 sgd_solver.cpp:112] Iteration 28710, lr = 0.01
I0523 01:42:55.340611 34682 solver.cpp:239] Iteration 28720 (2.32176 iter/s, 4.30707s/10 iters), loss = 8.27145
I0523 01:42:55.340662 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27145 (* 1 = 8.27145 loss)
I0523 01:42:56.185256 34682 sgd_solver.cpp:112] Iteration 28720, lr = 0.01
I0523 01:42:59.817093 34682 solver.cpp:239] Iteration 28730 (2.23401 iter/s, 4.47625s/10 iters), loss = 8.92442
I0523 01:42:59.817139 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92442 (* 1 = 8.92442 loss)
I0523 01:43:00.640535 34682 sgd_solver.cpp:112] Iteration 28730, lr = 0.01
I0523 01:43:03.183293 34682 solver.cpp:239] Iteration 28740 (2.97087 iter/s, 3.36602s/10 iters), loss = 9.46368
I0523 01:43:03.183331 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.46368 (* 1 = 9.46368 loss)
I0523 01:43:03.269268 34682 sgd_solver.cpp:112] Iteration 28740, lr = 0.01
I0523 01:43:09.729082 34682 solver.cpp:239] Iteration 28750 (1.52777 iter/s, 6.54547s/10 iters), loss = 8.52575
I0523 01:43:09.729233 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52575 (* 1 = 8.52575 loss)
I0523 01:43:09.801448 34682 sgd_solver.cpp:112] Iteration 28750, lr = 0.01
I0523 01:43:13.548151 34682 solver.cpp:239] Iteration 28760 (2.61865 iter/s, 3.81877s/10 iters), loss = 8.14955
I0523 01:43:13.548316 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14955 (* 1 = 8.14955 loss)
I0523 01:43:13.780892 34682 sgd_solver.cpp:112] Iteration 28760, lr = 0.01
I0523 01:43:18.755277 34682 solver.cpp:239] Iteration 28770 (1.92059 iter/s, 5.20674s/10 iters), loss = 8.11763
I0523 01:43:18.755342 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11763 (* 1 = 8.11763 loss)
I0523 01:43:19.362587 34682 sgd_solver.cpp:112] Iteration 28770, lr = 0.01
I0523 01:43:23.282740 34682 solver.cpp:239] Iteration 28780 (2.20887 iter/s, 4.52721s/10 iters), loss = 8.82461
I0523 01:43:23.282788 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82461 (* 1 = 8.82461 loss)
I0523 01:43:23.933595 34682 sgd_solver.cpp:112] Iteration 28780, lr = 0.01
I0523 01:43:30.851884 34682 solver.cpp:239] Iteration 28790 (1.32122 iter/s, 7.56879s/10 iters), loss = 8.25544
I0523 01:43:30.851936 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25544 (* 1 = 8.25544 loss)
I0523 01:43:31.464345 34682 sgd_solver.cpp:112] Iteration 28790, lr = 0.01
I0523 01:43:37.174412 34682 solver.cpp:239] Iteration 28800 (1.58172 iter/s, 6.32222s/10 iters), loss = 7.94119
I0523 01:43:37.174465 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94119 (* 1 = 7.94119 loss)
I0523 01:43:37.958142 34682 sgd_solver.cpp:112] Iteration 28800, lr = 0.01
I0523 01:43:43.442338 34682 solver.cpp:239] Iteration 28810 (1.59551 iter/s, 6.26761s/10 iters), loss = 8.63329
I0523 01:43:43.442417 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63329 (* 1 = 8.63329 loss)
I0523 01:43:43.593992 34682 sgd_solver.cpp:112] Iteration 28810, lr = 0.01
I0523 01:43:45.402509 34682 solver.cpp:239] Iteration 28820 (5.10202 iter/s, 1.96001s/10 iters), loss = 8.16057
I0523 01:43:45.402549 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16057 (* 1 = 8.16057 loss)
I0523 01:43:45.462185 34682 sgd_solver.cpp:112] Iteration 28820, lr = 0.01
I0523 01:43:49.284569 34682 solver.cpp:239] Iteration 28830 (2.57609 iter/s, 3.88185s/10 iters), loss = 8.06325
I0523 01:43:49.284610 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06325 (* 1 = 8.06325 loss)
I0523 01:43:49.982183 34682 sgd_solver.cpp:112] Iteration 28830, lr = 0.01
I0523 01:43:54.437554 34682 solver.cpp:239] Iteration 28840 (1.94072 iter/s, 5.15273s/10 iters), loss = 8.75121
I0523 01:43:54.437598 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75121 (* 1 = 8.75121 loss)
I0523 01:43:54.516486 34682 sgd_solver.cpp:112] Iteration 28840, lr = 0.01
I0523 01:43:58.642376 34682 solver.cpp:239] Iteration 28850 (2.37835 iter/s, 4.2046s/10 iters), loss = 8.0276
I0523 01:43:58.642427 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0276 (* 1 = 8.0276 loss)
I0523 01:43:59.346227 34682 sgd_solver.cpp:112] Iteration 28850, lr = 0.01
I0523 01:44:02.876104 34682 solver.cpp:239] Iteration 28860 (2.36211 iter/s, 4.2335s/10 iters), loss = 8.23956
I0523 01:44:02.876157 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23956 (* 1 = 8.23956 loss)
I0523 01:44:02.936503 34682 sgd_solver.cpp:112] Iteration 28860, lr = 0.01
I0523 01:44:06.722373 34682 solver.cpp:239] Iteration 28870 (2.60007 iter/s, 3.84606s/10 iters), loss = 9.65392
I0523 01:44:06.722421 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.65392 (* 1 = 9.65392 loss)
I0523 01:44:06.789621 34682 sgd_solver.cpp:112] Iteration 28870, lr = 0.01
I0523 01:44:11.539875 34682 solver.cpp:239] Iteration 28880 (2.07587 iter/s, 4.81726s/10 iters), loss = 8.4733
I0523 01:44:11.539918 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4733 (* 1 = 8.4733 loss)
I0523 01:44:11.612785 34682 sgd_solver.cpp:112] Iteration 28880, lr = 0.01
I0523 01:44:17.215088 34682 solver.cpp:239] Iteration 28890 (1.76213 iter/s, 5.67494s/10 iters), loss = 8.95145
I0523 01:44:17.215239 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95145 (* 1 = 8.95145 loss)
I0523 01:44:18.015535 34682 sgd_solver.cpp:112] Iteration 28890, lr = 0.01
I0523 01:44:22.886608 34682 solver.cpp:239] Iteration 28900 (1.76331 iter/s, 5.67114s/10 iters), loss = 8.72796
I0523 01:44:22.886667 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72796 (* 1 = 8.72796 loss)
I0523 01:44:23.625442 34682 sgd_solver.cpp:112] Iteration 28900, lr = 0.01
I0523 01:44:27.402743 34682 solver.cpp:239] Iteration 28910 (2.2144 iter/s, 4.5159s/10 iters), loss = 8.79074
I0523 01:44:27.402786 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79074 (* 1 = 8.79074 loss)
I0523 01:44:27.482082 34682 sgd_solver.cpp:112] Iteration 28910, lr = 0.01
I0523 01:44:32.988442 34682 solver.cpp:239] Iteration 28920 (1.79037 iter/s, 5.58543s/10 iters), loss = 8.79959
I0523 01:44:32.988493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79959 (* 1 = 8.79959 loss)
I0523 01:44:33.053115 34682 sgd_solver.cpp:112] Iteration 28920, lr = 0.01
I0523 01:44:39.166000 34682 solver.cpp:239] Iteration 28930 (1.61884 iter/s, 6.17725s/10 iters), loss = 8.25884
I0523 01:44:39.166054 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25884 (* 1 = 8.25884 loss)
I0523 01:44:39.970692 34682 sgd_solver.cpp:112] Iteration 28930, lr = 0.01
I0523 01:44:44.782333 34682 solver.cpp:239] Iteration 28940 (1.78061 iter/s, 5.61606s/10 iters), loss = 8.49333
I0523 01:44:44.782380 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49333 (* 1 = 8.49333 loss)
I0523 01:44:44.855738 34682 sgd_solver.cpp:112] Iteration 28940, lr = 0.01
I0523 01:44:50.375860 34682 solver.cpp:239] Iteration 28950 (1.78787 iter/s, 5.59325s/10 iters), loss = 9.21806
I0523 01:44:50.376144 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21806 (* 1 = 9.21806 loss)
I0523 01:44:51.211325 34682 sgd_solver.cpp:112] Iteration 28950, lr = 0.01
I0523 01:44:56.213994 34682 solver.cpp:239] Iteration 28960 (1.71303 iter/s, 5.83762s/10 iters), loss = 8.88627
I0523 01:44:56.214056 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88627 (* 1 = 8.88627 loss)
I0523 01:44:56.487478 34682 sgd_solver.cpp:112] Iteration 28960, lr = 0.01
I0523 01:45:01.339823 34682 solver.cpp:239] Iteration 28970 (1.95101 iter/s, 5.12556s/10 iters), loss = 8.5438
I0523 01:45:01.339870 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5438 (* 1 = 8.5438 loss)
I0523 01:45:02.148267 34682 sgd_solver.cpp:112] Iteration 28970, lr = 0.01
I0523 01:45:06.382165 34682 solver.cpp:239] Iteration 28980 (1.98331 iter/s, 5.04207s/10 iters), loss = 9.10426
I0523 01:45:06.382218 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10426 (* 1 = 9.10426 loss)
I0523 01:45:06.449745 34682 sgd_solver.cpp:112] Iteration 28980, lr = 0.01
I0523 01:45:09.755514 34682 solver.cpp:239] Iteration 28990 (2.96458 iter/s, 3.37316s/10 iters), loss = 8.48745
I0523 01:45:09.755563 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48745 (* 1 = 8.48745 loss)
I0523 01:45:10.610038 34682 sgd_solver.cpp:112] Iteration 28990, lr = 0.01
I0523 01:45:16.413098 34682 solver.cpp:239] Iteration 29000 (1.50212 iter/s, 6.65726s/10 iters), loss = 8.83163
I0523 01:45:16.413161 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83163 (* 1 = 8.83163 loss)
I0523 01:45:17.150115 34682 sgd_solver.cpp:112] Iteration 29000, lr = 0.01
I0523 01:45:22.753937 34682 solver.cpp:239] Iteration 29010 (1.57716 iter/s, 6.34052s/10 iters), loss = 9.23721
I0523 01:45:22.754133 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23721 (* 1 = 9.23721 loss)
I0523 01:45:22.817170 34682 sgd_solver.cpp:112] Iteration 29010, lr = 0.01
I0523 01:45:26.238785 34682 solver.cpp:239] Iteration 29020 (2.86983 iter/s, 3.48452s/10 iters), loss = 8.69442
I0523 01:45:26.238858 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69442 (* 1 = 8.69442 loss)
I0523 01:45:26.326705 34682 sgd_solver.cpp:112] Iteration 29020, lr = 0.01
I0523 01:45:30.822124 34682 solver.cpp:239] Iteration 29030 (2.18401 iter/s, 4.57873s/10 iters), loss = 8.8119
I0523 01:45:30.822170 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8119 (* 1 = 8.8119 loss)
I0523 01:45:30.887709 34682 sgd_solver.cpp:112] Iteration 29030, lr = 0.01
I0523 01:45:36.543371 34682 solver.cpp:239] Iteration 29040 (1.74796 iter/s, 5.72097s/10 iters), loss = 8.47992
I0523 01:45:36.543427 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47992 (* 1 = 8.47992 loss)
I0523 01:45:37.323377 34682 sgd_solver.cpp:112] Iteration 29040, lr = 0.01
I0523 01:45:42.452242 34682 solver.cpp:239] Iteration 29050 (1.69246 iter/s, 5.90857s/10 iters), loss = 8.40713
I0523 01:45:42.452296 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40713 (* 1 = 8.40713 loss)
I0523 01:45:42.521512 34682 sgd_solver.cpp:112] Iteration 29050, lr = 0.01
I0523 01:45:47.051906 34682 solver.cpp:239] Iteration 29060 (2.17418 iter/s, 4.59943s/10 iters), loss = 8.86005
I0523 01:45:47.051946 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86005 (* 1 = 8.86005 loss)
I0523 01:45:47.107170 34682 sgd_solver.cpp:112] Iteration 29060, lr = 0.01
I0523 01:45:51.063637 34682 solver.cpp:239] Iteration 29070 (2.49282 iter/s, 4.01152s/10 iters), loss = 9.09824
I0523 01:45:51.063683 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09824 (* 1 = 9.09824 loss)
I0523 01:45:51.136253 34682 sgd_solver.cpp:112] Iteration 29070, lr = 0.01
I0523 01:45:55.870661 34682 solver.cpp:239] Iteration 29080 (2.08039 iter/s, 4.80678s/10 iters), loss = 9.01227
I0523 01:45:55.870954 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01227 (* 1 = 9.01227 loss)
I0523 01:45:55.944605 34682 sgd_solver.cpp:112] Iteration 29080, lr = 0.01
I0523 01:46:02.115494 34682 solver.cpp:239] Iteration 29090 (1.60146 iter/s, 6.24431s/10 iters), loss = 8.56049
I0523 01:46:02.115551 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56049 (* 1 = 8.56049 loss)
I0523 01:46:02.184751 34682 sgd_solver.cpp:112] Iteration 29090, lr = 0.01
I0523 01:46:08.474805 34682 solver.cpp:239] Iteration 29100 (1.57257 iter/s, 6.359s/10 iters), loss = 8.68689
I0523 01:46:08.474844 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68689 (* 1 = 8.68689 loss)
I0523 01:46:08.554396 34682 sgd_solver.cpp:112] Iteration 29100, lr = 0.01
I0523 01:46:12.007467 34682 solver.cpp:239] Iteration 29110 (2.83089 iter/s, 3.53246s/10 iters), loss = 8.42258
I0523 01:46:12.007517 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42258 (* 1 = 8.42258 loss)
I0523 01:46:12.795558 34682 sgd_solver.cpp:112] Iteration 29110, lr = 0.01
I0523 01:46:16.182030 34682 solver.cpp:239] Iteration 29120 (2.39559 iter/s, 4.17434s/10 iters), loss = 9.25761
I0523 01:46:16.182080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25761 (* 1 = 9.25761 loss)
I0523 01:46:17.038069 34682 sgd_solver.cpp:112] Iteration 29120, lr = 0.01
I0523 01:46:20.675258 34682 solver.cpp:239] Iteration 29130 (2.22569 iter/s, 4.49298s/10 iters), loss = 9.32435
I0523 01:46:20.675312 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32435 (* 1 = 9.32435 loss)
I0523 01:46:21.499182 34682 sgd_solver.cpp:112] Iteration 29130, lr = 0.01
I0523 01:46:25.316859 34682 solver.cpp:239] Iteration 29140 (2.15454 iter/s, 4.64136s/10 iters), loss = 8.62935
I0523 01:46:25.316897 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62935 (* 1 = 8.62935 loss)
I0523 01:46:25.388800 34682 sgd_solver.cpp:112] Iteration 29140, lr = 0.01
I0523 01:46:29.950312 34682 solver.cpp:239] Iteration 29150 (2.15833 iter/s, 4.63321s/10 iters), loss = 9.12008
I0523 01:46:29.950588 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12008 (* 1 = 9.12008 loss)
I0523 01:46:30.767273 34682 sgd_solver.cpp:112] Iteration 29150, lr = 0.01
I0523 01:46:36.753898 34682 solver.cpp:239] Iteration 29160 (1.46992 iter/s, 6.80308s/10 iters), loss = 9.20216
I0523 01:46:36.753948 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20216 (* 1 = 9.20216 loss)
I0523 01:46:36.813885 34682 sgd_solver.cpp:112] Iteration 29160, lr = 0.01
I0523 01:46:40.193727 34682 solver.cpp:239] Iteration 29170 (2.90728 iter/s, 3.43964s/10 iters), loss = 9.07669
I0523 01:46:40.193779 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07669 (* 1 = 9.07669 loss)
I0523 01:46:40.813997 34682 sgd_solver.cpp:112] Iteration 29170, lr = 0.01
I0523 01:46:46.027673 34682 solver.cpp:239] Iteration 29180 (1.71419 iter/s, 5.83365s/10 iters), loss = 8.51921
I0523 01:46:46.027722 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51921 (* 1 = 8.51921 loss)
I0523 01:46:46.102826 34682 sgd_solver.cpp:112] Iteration 29180, lr = 0.01
I0523 01:46:50.098965 34682 solver.cpp:239] Iteration 29190 (2.45636 iter/s, 4.07106s/10 iters), loss = 8.86085
I0523 01:46:50.099028 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86085 (* 1 = 8.86085 loss)
I0523 01:46:50.858283 34682 sgd_solver.cpp:112] Iteration 29190, lr = 0.01
I0523 01:46:55.159158 34682 solver.cpp:239] Iteration 29200 (1.97631 iter/s, 5.05993s/10 iters), loss = 9.09677
I0523 01:46:55.159202 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09677 (* 1 = 9.09677 loss)
I0523 01:46:55.223302 34682 sgd_solver.cpp:112] Iteration 29200, lr = 0.01
I0523 01:47:00.150346 34682 solver.cpp:239] Iteration 29210 (2.00363 iter/s, 4.99094s/10 iters), loss = 8.73818
I0523 01:47:00.150532 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73818 (* 1 = 8.73818 loss)
I0523 01:47:00.222826 34682 sgd_solver.cpp:112] Iteration 29210, lr = 0.01
I0523 01:47:04.881356 34682 solver.cpp:239] Iteration 29220 (2.11388 iter/s, 4.73063s/10 iters), loss = 8.63687
I0523 01:47:04.881409 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63687 (* 1 = 8.63687 loss)
I0523 01:47:04.954519 34682 sgd_solver.cpp:112] Iteration 29220, lr = 0.01
I0523 01:47:09.783046 34682 solver.cpp:239] Iteration 29230 (2.04022 iter/s, 4.90144s/10 iters), loss = 8.96254
I0523 01:47:09.783087 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96254 (* 1 = 8.96254 loss)
I0523 01:47:09.852448 34682 sgd_solver.cpp:112] Iteration 29230, lr = 0.01
I0523 01:47:13.922879 34682 solver.cpp:239] Iteration 29240 (2.41569 iter/s, 4.13961s/10 iters), loss = 8.74856
I0523 01:47:13.922937 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74856 (* 1 = 8.74856 loss)
I0523 01:47:13.991317 34682 sgd_solver.cpp:112] Iteration 29240, lr = 0.01
I0523 01:47:18.133651 34682 solver.cpp:239] Iteration 29250 (2.37499 iter/s, 4.21054s/10 iters), loss = 9.2236
I0523 01:47:18.133699 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2236 (* 1 = 9.2236 loss)
I0523 01:47:18.967262 34682 sgd_solver.cpp:112] Iteration 29250, lr = 0.01
I0523 01:47:23.711431 34682 solver.cpp:239] Iteration 29260 (1.79292 iter/s, 5.57751s/10 iters), loss = 9.14992
I0523 01:47:23.711489 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14992 (* 1 = 9.14992 loss)
I0523 01:47:24.464408 34682 sgd_solver.cpp:112] Iteration 29260, lr = 0.01
I0523 01:47:29.183533 34682 solver.cpp:239] Iteration 29270 (1.82754 iter/s, 5.47183s/10 iters), loss = 8.46892
I0523 01:47:29.183598 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46892 (* 1 = 8.46892 loss)
I0523 01:47:29.240350 34682 sgd_solver.cpp:112] Iteration 29270, lr = 0.01
I0523 01:47:36.212677 34682 solver.cpp:239] Iteration 29280 (1.42272 iter/s, 7.02881s/10 iters), loss = 8.85296
I0523 01:47:36.212916 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85296 (* 1 = 8.85296 loss)
I0523 01:47:36.919036 34682 sgd_solver.cpp:112] Iteration 29280, lr = 0.01
I0523 01:47:40.888494 34682 solver.cpp:239] Iteration 29290 (2.13885 iter/s, 4.67542s/10 iters), loss = 9.18796
I0523 01:47:40.888556 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18796 (* 1 = 9.18796 loss)
I0523 01:47:41.669512 34682 sgd_solver.cpp:112] Iteration 29290, lr = 0.01
I0523 01:47:44.998179 34682 solver.cpp:239] Iteration 29300 (2.43341 iter/s, 4.10946s/10 iters), loss = 9.35847
I0523 01:47:44.998239 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35847 (* 1 = 9.35847 loss)
I0523 01:47:45.847163 34682 sgd_solver.cpp:112] Iteration 29300, lr = 0.01
I0523 01:47:49.355451 34682 solver.cpp:239] Iteration 29310 (2.29514 iter/s, 4.35703s/10 iters), loss = 9.24228
I0523 01:47:49.355491 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24228 (* 1 = 9.24228 loss)
I0523 01:47:49.422343 34682 sgd_solver.cpp:112] Iteration 29310, lr = 0.01
I0523 01:47:53.802937 34682 solver.cpp:239] Iteration 29320 (2.24858 iter/s, 4.44725s/10 iters), loss = 8.66171
I0523 01:47:53.803000 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66171 (* 1 = 8.66171 loss)
I0523 01:47:54.649412 34682 sgd_solver.cpp:112] Iteration 29320, lr = 0.01
I0523 01:47:59.923503 34682 solver.cpp:239] Iteration 29330 (1.63392 iter/s, 6.12026s/10 iters), loss = 8.77997
I0523 01:47:59.923544 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77997 (* 1 = 8.77997 loss)
I0523 01:47:59.990473 34682 sgd_solver.cpp:112] Iteration 29330, lr = 0.01
I0523 01:48:04.819540 34682 solver.cpp:239] Iteration 29340 (2.04257 iter/s, 4.89579s/10 iters), loss = 9.43379
I0523 01:48:04.819595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43379 (* 1 = 9.43379 loss)
I0523 01:48:05.646284 34682 sgd_solver.cpp:112] Iteration 29340, lr = 0.01
I0523 01:48:08.943030 34682 solver.cpp:239] Iteration 29350 (2.42526 iter/s, 4.12327s/10 iters), loss = 7.68099
I0523 01:48:08.943203 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68099 (* 1 = 7.68099 loss)
I0523 01:48:09.018556 34682 sgd_solver.cpp:112] Iteration 29350, lr = 0.01
I0523 01:48:15.046319 34682 solver.cpp:239] Iteration 29360 (1.63857 iter/s, 6.10287s/10 iters), loss = 7.9499
I0523 01:48:15.046373 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9499 (* 1 = 7.9499 loss)
I0523 01:48:15.122772 34682 sgd_solver.cpp:112] Iteration 29360, lr = 0.01
I0523 01:48:17.379804 34682 solver.cpp:239] Iteration 29370 (4.28571 iter/s, 2.33333s/10 iters), loss = 8.47811
I0523 01:48:17.379855 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47811 (* 1 = 8.47811 loss)
I0523 01:48:17.900411 34682 sgd_solver.cpp:112] Iteration 29370, lr = 0.01
I0523 01:48:22.987177 34682 solver.cpp:239] Iteration 29380 (1.78345 iter/s, 5.6071s/10 iters), loss = 8.91263
I0523 01:48:22.987231 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91263 (* 1 = 8.91263 loss)
I0523 01:48:23.841675 34682 sgd_solver.cpp:112] Iteration 29380, lr = 0.01
I0523 01:48:29.191529 34682 solver.cpp:239] Iteration 29390 (1.61185 iter/s, 6.20405s/10 iters), loss = 9.02406
I0523 01:48:29.191576 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02406 (* 1 = 9.02406 loss)
I0523 01:48:29.267551 34682 sgd_solver.cpp:112] Iteration 29390, lr = 0.01
I0523 01:48:33.624452 34682 solver.cpp:239] Iteration 29400 (2.25597 iter/s, 4.43269s/10 iters), loss = 8.61369
I0523 01:48:33.624516 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61369 (* 1 = 8.61369 loss)
I0523 01:48:34.449396 34682 sgd_solver.cpp:112] Iteration 29400, lr = 0.01
I0523 01:48:38.479665 34682 solver.cpp:239] Iteration 29410 (2.05975 iter/s, 4.85495s/10 iters), loss = 8.47444
I0523 01:48:38.479708 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47444 (* 1 = 8.47444 loss)
I0523 01:48:38.552441 34682 sgd_solver.cpp:112] Iteration 29410, lr = 0.01
I0523 01:48:45.701092 34682 solver.cpp:239] Iteration 29420 (1.38483 iter/s, 7.22109s/10 iters), loss = 9.84082
I0523 01:48:45.701339 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.84082 (* 1 = 9.84082 loss)
I0523 01:48:46.393870 34682 sgd_solver.cpp:112] Iteration 29420, lr = 0.01
I0523 01:48:51.427870 34682 solver.cpp:239] Iteration 29430 (1.74632 iter/s, 5.72633s/10 iters), loss = 8.43587
I0523 01:48:51.427917 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43587 (* 1 = 8.43587 loss)
I0523 01:48:52.293694 34682 sgd_solver.cpp:112] Iteration 29430, lr = 0.01
I0523 01:48:55.503123 34682 solver.cpp:239] Iteration 29440 (2.45397 iter/s, 4.07503s/10 iters), loss = 8.80601
I0523 01:48:55.503190 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80601 (* 1 = 8.80601 loss)
I0523 01:48:56.296283 34682 sgd_solver.cpp:112] Iteration 29440, lr = 0.01
I0523 01:49:01.163895 34682 solver.cpp:239] Iteration 29450 (1.76663 iter/s, 5.66048s/10 iters), loss = 8.48539
I0523 01:49:01.163942 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48539 (* 1 = 8.48539 loss)
I0523 01:49:01.228368 34682 sgd_solver.cpp:112] Iteration 29450, lr = 0.01
I0523 01:49:06.161108 34682 solver.cpp:239] Iteration 29460 (2.00122 iter/s, 4.99696s/10 iters), loss = 8.95142
I0523 01:49:06.161162 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95142 (* 1 = 8.95142 loss)
I0523 01:49:06.235296 34682 sgd_solver.cpp:112] Iteration 29460, lr = 0.01
I0523 01:49:13.501938 34682 solver.cpp:239] Iteration 29470 (1.36231 iter/s, 7.34048s/10 iters), loss = 8.2922
I0523 01:49:13.502005 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2922 (* 1 = 8.2922 loss)
I0523 01:49:14.303673 34682 sgd_solver.cpp:112] Iteration 29470, lr = 0.01
I0523 01:49:20.637655 34682 solver.cpp:239] Iteration 29480 (1.40147 iter/s, 7.13536s/10 iters), loss = 8.83131
I0523 01:49:20.637938 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83131 (* 1 = 8.83131 loss)
I0523 01:49:20.710830 34682 sgd_solver.cpp:112] Iteration 29480, lr = 0.01
I0523 01:49:25.159719 34682 solver.cpp:239] Iteration 29490 (2.2116 iter/s, 4.52162s/10 iters), loss = 8.27132
I0523 01:49:25.159759 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27132 (* 1 = 8.27132 loss)
I0523 01:49:25.228108 34682 sgd_solver.cpp:112] Iteration 29490, lr = 0.01
I0523 01:49:30.047622 34682 solver.cpp:239] Iteration 29500 (2.04597 iter/s, 4.88765s/10 iters), loss = 8.44604
I0523 01:49:30.047672 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44604 (* 1 = 8.44604 loss)
I0523 01:49:30.110903 34682 sgd_solver.cpp:112] Iteration 29500, lr = 0.01
I0523 01:49:34.725674 34682 solver.cpp:239] Iteration 29510 (2.13775 iter/s, 4.67781s/10 iters), loss = 8.843
I0523 01:49:34.725718 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.843 (* 1 = 8.843 loss)
I0523 01:49:35.524564 34682 sgd_solver.cpp:112] Iteration 29510, lr = 0.01
I0523 01:49:38.736543 34682 solver.cpp:239] Iteration 29520 (2.49336 iter/s, 4.01065s/10 iters), loss = 8.66186
I0523 01:49:38.736598 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66186 (* 1 = 8.66186 loss)
I0523 01:49:38.812541 34682 sgd_solver.cpp:112] Iteration 29520, lr = 0.01
I0523 01:49:43.772982 34682 solver.cpp:239] Iteration 29530 (1.98563 iter/s, 5.03618s/10 iters), loss = 8.61549
I0523 01:49:43.773025 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61549 (* 1 = 8.61549 loss)
I0523 01:49:43.832720 34682 sgd_solver.cpp:112] Iteration 29530, lr = 0.01
I0523 01:49:47.198822 34682 solver.cpp:239] Iteration 29540 (2.91917 iter/s, 3.42564s/10 iters), loss = 9.01591
I0523 01:49:47.198879 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01591 (* 1 = 9.01591 loss)
I0523 01:49:48.009657 34682 sgd_solver.cpp:112] Iteration 29540, lr = 0.01
I0523 01:49:51.925593 34682 solver.cpp:239] Iteration 29550 (2.11572 iter/s, 4.72652s/10 iters), loss = 9.11829
I0523 01:49:51.925824 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11829 (* 1 = 9.11829 loss)
I0523 01:49:51.995642 34682 sgd_solver.cpp:112] Iteration 29550, lr = 0.01
I0523 01:49:56.752777 34682 solver.cpp:239] Iteration 29560 (2.07177 iter/s, 4.82678s/10 iters), loss = 8.54679
I0523 01:49:56.752826 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54679 (* 1 = 8.54679 loss)
I0523 01:49:56.822408 34682 sgd_solver.cpp:112] Iteration 29560, lr = 0.01
I0523 01:50:02.871562 34682 solver.cpp:239] Iteration 29570 (1.63439 iter/s, 6.11849s/10 iters), loss = 8.9091
I0523 01:50:02.871626 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9091 (* 1 = 8.9091 loss)
I0523 01:50:03.702834 34682 sgd_solver.cpp:112] Iteration 29570, lr = 0.01
I0523 01:50:08.764385 34682 solver.cpp:239] Iteration 29580 (1.69707 iter/s, 5.89252s/10 iters), loss = 10.0217
I0523 01:50:08.764436 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0217 (* 1 = 10.0217 loss)
I0523 01:50:09.388981 34682 sgd_solver.cpp:112] Iteration 29580, lr = 0.01
I0523 01:50:13.642108 34682 solver.cpp:239] Iteration 29590 (2.05025 iter/s, 4.87746s/10 iters), loss = 9.07518
I0523 01:50:13.642174 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07518 (* 1 = 9.07518 loss)
I0523 01:50:13.689157 34682 sgd_solver.cpp:112] Iteration 29590, lr = 0.01
I0523 01:50:18.975101 34682 solver.cpp:239] Iteration 29600 (1.87522 iter/s, 5.33272s/10 iters), loss = 8.0124
I0523 01:50:18.975143 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0124 (* 1 = 8.0124 loss)
I0523 01:50:19.043699 34682 sgd_solver.cpp:112] Iteration 29600, lr = 0.01
I0523 01:50:22.526849 34682 solver.cpp:239] Iteration 29610 (2.81568 iter/s, 3.55154s/10 iters), loss = 8.85904
I0523 01:50:22.527096 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85904 (* 1 = 8.85904 loss)
I0523 01:50:23.330521 34682 sgd_solver.cpp:112] Iteration 29610, lr = 0.01
I0523 01:50:29.729972 34682 solver.cpp:239] Iteration 29620 (1.38839 iter/s, 7.20259s/10 iters), loss = 9.0952
I0523 01:50:29.730026 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0952 (* 1 = 9.0952 loss)
I0523 01:50:30.558977 34682 sgd_solver.cpp:112] Iteration 29620, lr = 0.01
I0523 01:50:34.665411 34682 solver.cpp:239] Iteration 29630 (2.02626 iter/s, 4.93519s/10 iters), loss = 9.63055
I0523 01:50:34.665458 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.63055 (* 1 = 9.63055 loss)
I0523 01:50:35.496060 34682 sgd_solver.cpp:112] Iteration 29630, lr = 0.01
I0523 01:50:41.205516 34682 solver.cpp:239] Iteration 29640 (1.5291 iter/s, 6.53979s/10 iters), loss = 9.29494
I0523 01:50:41.205590 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29494 (* 1 = 9.29494 loss)
I0523 01:50:41.263278 34682 sgd_solver.cpp:112] Iteration 29640, lr = 0.01
I0523 01:50:46.916261 34682 solver.cpp:239] Iteration 29650 (1.75118 iter/s, 5.71045s/10 iters), loss = 9.0824
I0523 01:50:46.916316 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0824 (* 1 = 9.0824 loss)
I0523 01:50:46.990380 34682 sgd_solver.cpp:112] Iteration 29650, lr = 0.01
I0523 01:50:54.886900 34682 solver.cpp:239] Iteration 29660 (1.25466 iter/s, 7.97026s/10 iters), loss = 8.59302
I0523 01:50:54.887151 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59302 (* 1 = 8.59302 loss)
I0523 01:50:55.223304 34682 sgd_solver.cpp:112] Iteration 29660, lr = 0.01
I0523 01:50:59.903328 34682 solver.cpp:239] Iteration 29670 (1.99362 iter/s, 5.016s/10 iters), loss = 8.75336
I0523 01:50:59.903398 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75336 (* 1 = 8.75336 loss)
I0523 01:51:00.568670 34682 sgd_solver.cpp:112] Iteration 29670, lr = 0.01
I0523 01:51:04.595686 34682 solver.cpp:239] Iteration 29680 (2.13124 iter/s, 4.6921s/10 iters), loss = 8.11248
I0523 01:51:04.595731 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11248 (* 1 = 8.11248 loss)
I0523 01:51:04.672569 34682 sgd_solver.cpp:112] Iteration 29680, lr = 0.01
I0523 01:51:08.768430 34682 solver.cpp:239] Iteration 29690 (2.39663 iter/s, 4.17253s/10 iters), loss = 9.10256
I0523 01:51:08.768476 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10256 (* 1 = 9.10256 loss)
I0523 01:51:08.838338 34682 sgd_solver.cpp:112] Iteration 29690, lr = 0.01
I0523 01:51:12.302175 34682 solver.cpp:239] Iteration 29700 (2.83001 iter/s, 3.53355s/10 iters), loss = 8.42719
I0523 01:51:12.302232 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42719 (* 1 = 8.42719 loss)
I0523 01:51:12.375783 34682 sgd_solver.cpp:112] Iteration 29700, lr = 0.01
I0523 01:51:17.648784 34682 solver.cpp:239] Iteration 29710 (1.87044 iter/s, 5.34633s/10 iters), loss = 8.59321
I0523 01:51:17.648828 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59321 (* 1 = 8.59321 loss)
I0523 01:51:17.721798 34682 sgd_solver.cpp:112] Iteration 29710, lr = 0.01
I0523 01:51:22.160073 34682 solver.cpp:239] Iteration 29720 (2.21677 iter/s, 4.51106s/10 iters), loss = 9.56571
I0523 01:51:22.160125 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56571 (* 1 = 9.56571 loss)
I0523 01:51:22.239516 34682 sgd_solver.cpp:112] Iteration 29720, lr = 0.01
I0523 01:51:27.727931 34682 solver.cpp:239] Iteration 29730 (1.79611 iter/s, 5.56758s/10 iters), loss = 8.91423
I0523 01:51:27.728081 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91423 (* 1 = 8.91423 loss)
I0523 01:51:28.523792 34682 sgd_solver.cpp:112] Iteration 29730, lr = 0.01
I0523 01:51:31.700316 34682 solver.cpp:239] Iteration 29740 (2.51757 iter/s, 3.97208s/10 iters), loss = 8.99975
I0523 01:51:31.700371 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99975 (* 1 = 8.99975 loss)
I0523 01:51:32.539913 34682 sgd_solver.cpp:112] Iteration 29740, lr = 0.01
I0523 01:51:36.050084 34682 solver.cpp:239] Iteration 29750 (2.2991 iter/s, 4.34953s/10 iters), loss = 8.39065
I0523 01:51:36.050134 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39065 (* 1 = 8.39065 loss)
I0523 01:51:36.874032 34682 sgd_solver.cpp:112] Iteration 29750, lr = 0.01
I0523 01:51:41.602366 34682 solver.cpp:239] Iteration 29760 (1.80115 iter/s, 5.552s/10 iters), loss = 8.72527
I0523 01:51:41.602422 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72527 (* 1 = 8.72527 loss)
I0523 01:51:42.426643 34682 sgd_solver.cpp:112] Iteration 29760, lr = 0.01
I0523 01:51:45.705505 34682 solver.cpp:239] Iteration 29770 (2.43729 iter/s, 4.10292s/10 iters), loss = 8.85254
I0523 01:51:45.705554 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85254 (* 1 = 8.85254 loss)
I0523 01:51:46.216830 34682 sgd_solver.cpp:112] Iteration 29770, lr = 0.01
I0523 01:51:49.814267 34682 solver.cpp:239] Iteration 29780 (2.43395 iter/s, 4.10854s/10 iters), loss = 8.59073
I0523 01:51:49.814309 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59073 (* 1 = 8.59073 loss)
I0523 01:51:49.878538 34682 sgd_solver.cpp:112] Iteration 29780, lr = 0.01
I0523 01:51:53.182190 34682 solver.cpp:239] Iteration 29790 (2.96936 iter/s, 3.36773s/10 iters), loss = 8.90748
I0523 01:51:53.182240 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90748 (* 1 = 8.90748 loss)
I0523 01:51:53.251864 34682 sgd_solver.cpp:112] Iteration 29790, lr = 0.01
I0523 01:51:57.111438 34682 solver.cpp:239] Iteration 29800 (2.54515 iter/s, 3.92904s/10 iters), loss = 8.89651
I0523 01:51:57.111492 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89651 (* 1 = 8.89651 loss)
I0523 01:51:57.174650 34682 sgd_solver.cpp:112] Iteration 29800, lr = 0.01
I0523 01:52:01.200559 34682 solver.cpp:239] Iteration 29810 (2.44565 iter/s, 4.0889s/10 iters), loss = 8.11877
I0523 01:52:01.200834 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11877 (* 1 = 8.11877 loss)
I0523 01:52:01.269831 34682 sgd_solver.cpp:112] Iteration 29810, lr = 0.01
I0523 01:52:05.389135 34682 solver.cpp:239] Iteration 29820 (2.38769 iter/s, 4.18815s/10 iters), loss = 8.66556
I0523 01:52:05.389183 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66556 (* 1 = 8.66556 loss)
I0523 01:52:06.232090 34682 sgd_solver.cpp:112] Iteration 29820, lr = 0.01
I0523 01:52:10.293483 34682 solver.cpp:239] Iteration 29830 (2.03911 iter/s, 4.9041s/10 iters), loss = 8.83572
I0523 01:52:10.293537 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83572 (* 1 = 8.83572 loss)
I0523 01:52:10.362681 34682 sgd_solver.cpp:112] Iteration 29830, lr = 0.01
I0523 01:52:14.613144 34682 solver.cpp:239] Iteration 29840 (2.31512 iter/s, 4.31942s/10 iters), loss = 8.79525
I0523 01:52:14.613190 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79525 (* 1 = 8.79525 loss)
I0523 01:52:14.684048 34682 sgd_solver.cpp:112] Iteration 29840, lr = 0.01
I0523 01:52:19.971981 34682 solver.cpp:239] Iteration 29850 (1.86617 iter/s, 5.35857s/10 iters), loss = 9.22768
I0523 01:52:19.972043 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22768 (* 1 = 9.22768 loss)
I0523 01:52:20.710541 34682 sgd_solver.cpp:112] Iteration 29850, lr = 0.01
I0523 01:52:26.568039 34682 solver.cpp:239] Iteration 29860 (1.51613 iter/s, 6.59573s/10 iters), loss = 9.40756
I0523 01:52:26.568089 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40756 (* 1 = 9.40756 loss)
I0523 01:52:26.645102 34682 sgd_solver.cpp:112] Iteration 29860, lr = 0.01
I0523 01:52:29.306419 34682 solver.cpp:239] Iteration 29870 (3.65202 iter/s, 2.73821s/10 iters), loss = 9.49042
I0523 01:52:29.306473 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.49042 (* 1 = 9.49042 loss)
I0523 01:52:29.364043 34682 sgd_solver.cpp:112] Iteration 29870, lr = 0.01
I0523 01:52:34.330739 34682 solver.cpp:239] Iteration 29880 (1.99044 iter/s, 5.02401s/10 iters), loss = 10.1408
I0523 01:52:34.330974 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1408 (* 1 = 10.1408 loss)
I0523 01:52:35.037165 34682 sgd_solver.cpp:112] Iteration 29880, lr = 0.01
I0523 01:52:38.962123 34682 solver.cpp:239] Iteration 29890 (2.15937 iter/s, 4.63097s/10 iters), loss = 8.02912
I0523 01:52:38.962188 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02912 (* 1 = 8.02912 loss)
I0523 01:52:39.040858 34682 sgd_solver.cpp:112] Iteration 29890, lr = 0.01
I0523 01:52:43.012902 34682 solver.cpp:239] Iteration 29900 (2.46881 iter/s, 4.05054s/10 iters), loss = 8.90129
I0523 01:52:43.012989 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90129 (* 1 = 8.90129 loss)
I0523 01:52:43.605433 34682 sgd_solver.cpp:112] Iteration 29900, lr = 0.01
I0523 01:52:46.991770 34682 solver.cpp:239] Iteration 29910 (2.51343 iter/s, 3.97862s/10 iters), loss = 8.84216
I0523 01:52:46.991813 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84216 (* 1 = 8.84216 loss)
I0523 01:52:47.052336 34682 sgd_solver.cpp:112] Iteration 29910, lr = 0.01
I0523 01:52:53.311916 34682 solver.cpp:239] Iteration 29920 (1.58232 iter/s, 6.31984s/10 iters), loss = 8.99811
I0523 01:52:53.311964 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99811 (* 1 = 8.99811 loss)
I0523 01:52:54.050806 34682 sgd_solver.cpp:112] Iteration 29920, lr = 0.01
I0523 01:52:58.108934 34682 solver.cpp:239] Iteration 29930 (2.08473 iter/s, 4.79677s/10 iters), loss = 8.87893
I0523 01:52:58.108983 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87893 (* 1 = 8.87893 loss)
I0523 01:52:58.964130 34682 sgd_solver.cpp:112] Iteration 29930, lr = 0.01
I0523 01:53:03.320096 34682 solver.cpp:239] Iteration 29940 (1.91905 iter/s, 5.2109s/10 iters), loss = 8.80719
I0523 01:53:03.320152 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80719 (* 1 = 8.80719 loss)
I0523 01:53:03.780534 34682 sgd_solver.cpp:112] Iteration 29940, lr = 0.01
I0523 01:53:08.315083 34682 solver.cpp:239] Iteration 29950 (2.00211 iter/s, 4.99473s/10 iters), loss = 8.86974
I0523 01:53:08.315214 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86974 (* 1 = 8.86974 loss)
I0523 01:53:08.379871 34682 sgd_solver.cpp:112] Iteration 29950, lr = 0.01
I0523 01:53:13.284196 34682 solver.cpp:239] Iteration 29960 (2.01257 iter/s, 4.96878s/10 iters), loss = 8.74607
I0523 01:53:13.284265 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74607 (* 1 = 8.74607 loss)
I0523 01:53:13.355518 34682 sgd_solver.cpp:112] Iteration 29960, lr = 0.01
I0523 01:53:18.315141 34682 solver.cpp:239] Iteration 29970 (1.98781 iter/s, 5.03067s/10 iters), loss = 8.85068
I0523 01:53:18.315191 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85068 (* 1 = 8.85068 loss)
I0523 01:53:19.163213 34682 sgd_solver.cpp:112] Iteration 29970, lr = 0.01
I0523 01:53:24.010442 34682 solver.cpp:239] Iteration 29980 (1.75592 iter/s, 5.69501s/10 iters), loss = 8.56952
I0523 01:53:24.010514 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56952 (* 1 = 8.56952 loss)
I0523 01:53:24.071682 34682 sgd_solver.cpp:112] Iteration 29980, lr = 0.01
I0523 01:53:28.425282 34682 solver.cpp:239] Iteration 29990 (2.26522 iter/s, 4.41459s/10 iters), loss = 8.7456
I0523 01:53:28.425341 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7456 (* 1 = 8.7456 loss)
I0523 01:53:28.502620 34682 sgd_solver.cpp:112] Iteration 29990, lr = 0.01
I0523 01:53:32.864364 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_30000.caffemodel
I0523 01:53:34.001184 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_30000.solverstate
I0523 01:53:34.215308 34682 solver.cpp:239] Iteration 30000 (1.72719 iter/s, 5.78974s/10 iters), loss = 8.22689
I0523 01:53:34.215356 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22689 (* 1 = 8.22689 loss)
I0523 01:53:34.286310 34682 sgd_solver.cpp:112] Iteration 30000, lr = 0.01
I0523 01:53:37.978804 34682 solver.cpp:239] Iteration 30010 (2.65725 iter/s, 3.76328s/10 iters), loss = 9.39812
I0523 01:53:37.978860 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.39812 (* 1 = 9.39812 loss)
I0523 01:53:38.064959 34682 sgd_solver.cpp:112] Iteration 30010, lr = 0.01
I0523 01:53:42.967380 34682 solver.cpp:239] Iteration 30020 (2.00469 iter/s, 4.98831s/10 iters), loss = 9.04954
I0523 01:53:42.967671 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04954 (* 1 = 9.04954 loss)
I0523 01:53:43.790294 34682 sgd_solver.cpp:112] Iteration 30020, lr = 0.01
I0523 01:53:49.213230 34682 solver.cpp:239] Iteration 30030 (1.6012 iter/s, 6.24532s/10 iters), loss = 9.04517
I0523 01:53:49.213274 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04517 (* 1 = 9.04517 loss)
I0523 01:53:49.283407 34682 sgd_solver.cpp:112] Iteration 30030, lr = 0.01
I0523 01:53:54.890097 34682 solver.cpp:239] Iteration 30040 (1.76162 iter/s, 5.6766s/10 iters), loss = 8.97523
I0523 01:53:54.890138 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97523 (* 1 = 8.97523 loss)
I0523 01:53:54.969961 34682 sgd_solver.cpp:112] Iteration 30040, lr = 0.01
I0523 01:53:57.532023 34682 solver.cpp:239] Iteration 30050 (3.78534 iter/s, 2.64177s/10 iters), loss = 8.08709
I0523 01:53:57.532063 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08709 (* 1 = 8.08709 loss)
I0523 01:53:57.607699 34682 sgd_solver.cpp:112] Iteration 30050, lr = 0.01
I0523 01:54:02.063287 34682 solver.cpp:239] Iteration 30060 (2.207 iter/s, 4.53103s/10 iters), loss = 7.95084
I0523 01:54:02.063333 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95084 (* 1 = 7.95084 loss)
I0523 01:54:02.138115 34682 sgd_solver.cpp:112] Iteration 30060, lr = 0.01
I0523 01:54:07.386420 34682 solver.cpp:239] Iteration 30070 (1.87869 iter/s, 5.32287s/10 iters), loss = 9.44063
I0523 01:54:07.386483 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44063 (* 1 = 9.44063 loss)
I0523 01:54:08.028054 34682 sgd_solver.cpp:112] Iteration 30070, lr = 0.01
I0523 01:54:12.168989 34682 solver.cpp:239] Iteration 30080 (2.09104 iter/s, 4.7823s/10 iters), loss = 9.05775
I0523 01:54:12.169035 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05775 (* 1 = 9.05775 loss)
I0523 01:54:12.237027 34682 sgd_solver.cpp:112] Iteration 30080, lr = 0.01
I0523 01:54:16.897899 34682 solver.cpp:239] Iteration 30090 (2.11476 iter/s, 4.72867s/10 iters), loss = 8.39502
I0523 01:54:16.898171 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39502 (* 1 = 8.39502 loss)
I0523 01:54:16.959856 34682 sgd_solver.cpp:112] Iteration 30090, lr = 0.01
I0523 01:54:22.733006 34682 solver.cpp:239] Iteration 30100 (1.71391 iter/s, 5.83462s/10 iters), loss = 8.78874
I0523 01:54:22.733072 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78874 (* 1 = 8.78874 loss)
I0523 01:54:22.796990 34682 sgd_solver.cpp:112] Iteration 30100, lr = 0.01
I0523 01:54:25.761957 34682 solver.cpp:239] Iteration 30110 (3.30168 iter/s, 3.02876s/10 iters), loss = 8.69634
I0523 01:54:25.762012 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69634 (* 1 = 8.69634 loss)
I0523 01:54:26.485378 34682 sgd_solver.cpp:112] Iteration 30110, lr = 0.01
I0523 01:54:30.479164 34682 solver.cpp:239] Iteration 30120 (2.12001 iter/s, 4.71696s/10 iters), loss = 8.43179
I0523 01:54:30.479218 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43179 (* 1 = 8.43179 loss)
I0523 01:54:30.556499 34682 sgd_solver.cpp:112] Iteration 30120, lr = 0.01
I0523 01:54:33.859081 34682 solver.cpp:239] Iteration 30130 (2.95883 iter/s, 3.37972s/10 iters), loss = 8.44782
I0523 01:54:33.859136 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44782 (* 1 = 8.44782 loss)
I0523 01:54:33.939563 34682 sgd_solver.cpp:112] Iteration 30130, lr = 0.01
I0523 01:54:37.404250 34682 solver.cpp:239] Iteration 30140 (2.8209 iter/s, 3.54497s/10 iters), loss = 9.25954
I0523 01:54:37.404297 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25954 (* 1 = 9.25954 loss)
I0523 01:54:37.459859 34682 sgd_solver.cpp:112] Iteration 30140, lr = 0.01
I0523 01:54:41.687249 34682 solver.cpp:239] Iteration 30150 (2.33493 iter/s, 4.28277s/10 iters), loss = 8.09943
I0523 01:54:41.687304 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09943 (* 1 = 8.09943 loss)
I0523 01:54:42.379554 34682 sgd_solver.cpp:112] Iteration 30150, lr = 0.01
I0523 01:54:47.568990 34682 solver.cpp:239] Iteration 30160 (1.70026 iter/s, 5.88144s/10 iters), loss = 8.56197
I0523 01:54:47.569211 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56197 (* 1 = 8.56197 loss)
I0523 01:54:48.223474 34682 sgd_solver.cpp:112] Iteration 30160, lr = 0.01
I0523 01:54:52.937803 34682 solver.cpp:239] Iteration 30170 (1.86276 iter/s, 5.36838s/10 iters), loss = 9.8679
I0523 01:54:52.937858 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.8679 (* 1 = 9.8679 loss)
I0523 01:54:53.676823 34682 sgd_solver.cpp:112] Iteration 30170, lr = 0.01
I0523 01:54:57.742373 34682 solver.cpp:239] Iteration 30180 (2.08146 iter/s, 4.80432s/10 iters), loss = 9.33233
I0523 01:54:57.742414 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33233 (* 1 = 9.33233 loss)
I0523 01:54:57.813269 34682 sgd_solver.cpp:112] Iteration 30180, lr = 0.01
I0523 01:55:03.612175 34682 solver.cpp:239] Iteration 30190 (1.70372 iter/s, 5.86953s/10 iters), loss = 8.50694
I0523 01:55:03.612226 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50694 (* 1 = 8.50694 loss)
I0523 01:55:04.336616 34682 sgd_solver.cpp:112] Iteration 30190, lr = 0.01
I0523 01:55:07.699826 34682 solver.cpp:239] Iteration 30200 (2.44653 iter/s, 4.08743s/10 iters), loss = 8.72451
I0523 01:55:07.699887 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72451 (* 1 = 8.72451 loss)
I0523 01:55:08.447005 34682 sgd_solver.cpp:112] Iteration 30200, lr = 0.01
I0523 01:55:14.758772 34682 solver.cpp:239] Iteration 30210 (1.41671 iter/s, 7.0586s/10 iters), loss = 8.70688
I0523 01:55:14.758821 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70688 (* 1 = 8.70688 loss)
I0523 01:55:14.834913 34682 sgd_solver.cpp:112] Iteration 30210, lr = 0.01
I0523 01:55:18.239954 34682 solver.cpp:239] Iteration 30220 (2.87275 iter/s, 3.48099s/10 iters), loss = 9.04269
I0523 01:55:18.240205 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04269 (* 1 = 9.04269 loss)
I0523 01:55:18.314400 34682 sgd_solver.cpp:112] Iteration 30220, lr = 0.01
I0523 01:55:23.640128 34682 solver.cpp:239] Iteration 30230 (1.85195 iter/s, 5.39973s/10 iters), loss = 8.58715
I0523 01:55:23.640187 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58715 (* 1 = 8.58715 loss)
I0523 01:55:23.703156 34682 sgd_solver.cpp:112] Iteration 30230, lr = 0.01
I0523 01:55:28.640141 34682 solver.cpp:239] Iteration 30240 (2.0001 iter/s, 4.99975s/10 iters), loss = 9.10331
I0523 01:55:28.640193 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10331 (* 1 = 9.10331 loss)
I0523 01:55:29.470360 34682 sgd_solver.cpp:112] Iteration 30240, lr = 0.01
I0523 01:55:37.095046 34682 solver.cpp:239] Iteration 30250 (1.1828 iter/s, 8.45451s/10 iters), loss = 8.7186
I0523 01:55:37.095098 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7186 (* 1 = 8.7186 loss)
I0523 01:55:37.165484 34682 sgd_solver.cpp:112] Iteration 30250, lr = 0.01
I0523 01:55:41.420215 34682 solver.cpp:239] Iteration 30260 (2.31217 iter/s, 4.32493s/10 iters), loss = 8.54498
I0523 01:55:41.420279 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54498 (* 1 = 8.54498 loss)
I0523 01:55:41.501010 34682 sgd_solver.cpp:112] Iteration 30260, lr = 0.01
I0523 01:55:45.187877 34682 solver.cpp:239] Iteration 30270 (2.65432 iter/s, 3.76745s/10 iters), loss = 9.2168
I0523 01:55:45.187927 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2168 (* 1 = 9.2168 loss)
I0523 01:55:45.260854 34682 sgd_solver.cpp:112] Iteration 30270, lr = 0.01
I0523 01:55:48.670934 34682 solver.cpp:239] Iteration 30280 (2.8712 iter/s, 3.48286s/10 iters), loss = 8.92384
I0523 01:55:48.671133 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92384 (* 1 = 8.92384 loss)
I0523 01:55:49.497200 34682 sgd_solver.cpp:112] Iteration 30280, lr = 0.01
I0523 01:55:53.384408 34682 solver.cpp:239] Iteration 30290 (2.12176 iter/s, 4.71308s/10 iters), loss = 8.22764
I0523 01:55:53.384469 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22764 (* 1 = 8.22764 loss)
I0523 01:55:53.448199 34682 sgd_solver.cpp:112] Iteration 30290, lr = 0.01
I0523 01:55:59.941459 34682 solver.cpp:239] Iteration 30300 (1.52515 iter/s, 6.55671s/10 iters), loss = 8.63109
I0523 01:55:59.941534 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63109 (* 1 = 8.63109 loss)
I0523 01:56:00.742544 34682 sgd_solver.cpp:112] Iteration 30300, lr = 0.01
I0523 01:56:05.585852 34682 solver.cpp:239] Iteration 30310 (1.77176 iter/s, 5.6441s/10 iters), loss = 10.4754
I0523 01:56:05.585906 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.4754 (* 1 = 10.4754 loss)
I0523 01:56:06.412174 34682 sgd_solver.cpp:112] Iteration 30310, lr = 0.01
I0523 01:56:11.069455 34682 solver.cpp:239] Iteration 30320 (1.82371 iter/s, 5.48332s/10 iters), loss = 8.5468
I0523 01:56:11.069504 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5468 (* 1 = 8.5468 loss)
I0523 01:56:11.147869 34682 sgd_solver.cpp:112] Iteration 30320, lr = 0.01
I0523 01:56:16.091974 34682 solver.cpp:239] Iteration 30330 (1.99113 iter/s, 5.02227s/10 iters), loss = 9.12193
I0523 01:56:16.092017 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12193 (* 1 = 9.12193 loss)
I0523 01:56:16.174351 34682 sgd_solver.cpp:112] Iteration 30330, lr = 0.01
I0523 01:56:19.518498 34682 solver.cpp:239] Iteration 30340 (2.91858 iter/s, 3.42632s/10 iters), loss = 8.81339
I0523 01:56:19.518682 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81339 (* 1 = 8.81339 loss)
I0523 01:56:20.156810 34682 sgd_solver.cpp:112] Iteration 30340, lr = 0.01
I0523 01:56:24.363729 34682 solver.cpp:239] Iteration 30350 (2.06405 iter/s, 4.84485s/10 iters), loss = 9.08169
I0523 01:56:24.363772 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08169 (* 1 = 9.08169 loss)
I0523 01:56:24.427448 34682 sgd_solver.cpp:112] Iteration 30350, lr = 0.01
I0523 01:56:27.770428 34682 solver.cpp:239] Iteration 30360 (2.93555 iter/s, 3.40651s/10 iters), loss = 8.71061
I0523 01:56:27.770473 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71061 (* 1 = 8.71061 loss)
I0523 01:56:27.850450 34682 sgd_solver.cpp:112] Iteration 30360, lr = 0.01
I0523 01:56:31.987896 34682 solver.cpp:239] Iteration 30370 (2.37122 iter/s, 4.21724s/10 iters), loss = 8.20426
I0523 01:56:31.987944 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20426 (* 1 = 8.20426 loss)
I0523 01:56:32.061940 34682 sgd_solver.cpp:112] Iteration 30370, lr = 0.01
I0523 01:56:37.780217 34682 solver.cpp:239] Iteration 30380 (1.72651 iter/s, 5.79203s/10 iters), loss = 8.3139
I0523 01:56:37.780282 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3139 (* 1 = 8.3139 loss)
I0523 01:56:38.646430 34682 sgd_solver.cpp:112] Iteration 30380, lr = 0.01
I0523 01:56:42.831567 34682 solver.cpp:239] Iteration 30390 (1.97978 iter/s, 5.05107s/10 iters), loss = 8.08546
I0523 01:56:42.831626 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08546 (* 1 = 8.08546 loss)
I0523 01:56:43.464326 34682 sgd_solver.cpp:112] Iteration 30390, lr = 0.01
I0523 01:56:47.499301 34682 solver.cpp:239] Iteration 30400 (2.14248 iter/s, 4.66748s/10 iters), loss = 8.81818
I0523 01:56:47.499358 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81818 (* 1 = 8.81818 loss)
I0523 01:56:48.256603 34682 sgd_solver.cpp:112] Iteration 30400, lr = 0.01
I0523 01:56:54.578773 34682 solver.cpp:239] Iteration 30410 (1.4126 iter/s, 7.07913s/10 iters), loss = 9.41581
I0523 01:56:54.578996 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.41581 (* 1 = 9.41581 loss)
I0523 01:56:54.651515 34682 sgd_solver.cpp:112] Iteration 30410, lr = 0.01
I0523 01:56:58.922358 34682 solver.cpp:239] Iteration 30420 (2.30245 iter/s, 4.3432s/10 iters), loss = 8.89034
I0523 01:56:58.922411 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89034 (* 1 = 8.89034 loss)
I0523 01:56:59.783114 34682 sgd_solver.cpp:112] Iteration 30420, lr = 0.01
I0523 01:57:05.713536 34682 solver.cpp:239] Iteration 30430 (1.47257 iter/s, 6.79084s/10 iters), loss = 9.05238
I0523 01:57:05.713585 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05238 (* 1 = 9.05238 loss)
I0523 01:57:05.779824 34682 sgd_solver.cpp:112] Iteration 30430, lr = 0.01
I0523 01:57:10.180272 34682 solver.cpp:239] Iteration 30440 (2.23889 iter/s, 4.4665s/10 iters), loss = 9.49717
I0523 01:57:10.180336 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.49717 (* 1 = 9.49717 loss)
I0523 01:57:10.250129 34682 sgd_solver.cpp:112] Iteration 30440, lr = 0.01
I0523 01:57:14.848237 34682 solver.cpp:239] Iteration 30450 (2.14237 iter/s, 4.66772s/10 iters), loss = 8.85282
I0523 01:57:14.848280 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85282 (* 1 = 8.85282 loss)
I0523 01:57:14.918323 34682 sgd_solver.cpp:112] Iteration 30450, lr = 0.01
I0523 01:57:17.528342 34682 solver.cpp:239] Iteration 30460 (3.73142 iter/s, 2.67994s/10 iters), loss = 9.03135
I0523 01:57:17.528393 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03135 (* 1 = 9.03135 loss)
I0523 01:57:17.853303 34682 sgd_solver.cpp:112] Iteration 30460, lr = 0.01
I0523 01:57:23.395201 34682 solver.cpp:239] Iteration 30470 (1.70457 iter/s, 5.86657s/10 iters), loss = 9.62257
I0523 01:57:23.395257 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.62257 (* 1 = 9.62257 loss)
I0523 01:57:23.472818 34682 sgd_solver.cpp:112] Iteration 30470, lr = 0.01
I0523 01:57:26.198366 34682 solver.cpp:239] Iteration 30480 (3.56764 iter/s, 2.80298s/10 iters), loss = 9.71089
I0523 01:57:26.198499 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.71089 (* 1 = 9.71089 loss)
I0523 01:57:26.264362 34682 sgd_solver.cpp:112] Iteration 30480, lr = 0.01
I0523 01:57:30.368060 34682 solver.cpp:239] Iteration 30490 (2.39843 iter/s, 4.16939s/10 iters), loss = 9.07578
I0523 01:57:30.368103 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07578 (* 1 = 9.07578 loss)
I0523 01:57:30.427733 34682 sgd_solver.cpp:112] Iteration 30490, lr = 0.01
I0523 01:57:35.250424 34682 solver.cpp:239] Iteration 30500 (2.04829 iter/s, 4.88211s/10 iters), loss = 8.54372
I0523 01:57:35.250473 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54372 (* 1 = 8.54372 loss)
I0523 01:57:35.319723 34682 sgd_solver.cpp:112] Iteration 30500, lr = 0.01
I0523 01:57:41.633739 34682 solver.cpp:239] Iteration 30510 (1.56666 iter/s, 6.38301s/10 iters), loss = 8.92015
I0523 01:57:41.633783 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92015 (* 1 = 8.92015 loss)
I0523 01:57:42.463889 34682 sgd_solver.cpp:112] Iteration 30510, lr = 0.01
I0523 01:57:45.205516 34682 solver.cpp:239] Iteration 30520 (2.79988 iter/s, 3.57158s/10 iters), loss = 8.46678
I0523 01:57:45.205567 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46678 (* 1 = 8.46678 loss)
I0523 01:57:45.991155 34682 sgd_solver.cpp:112] Iteration 30520, lr = 0.01
I0523 01:57:49.304769 34682 solver.cpp:239] Iteration 30530 (2.4396 iter/s, 4.09904s/10 iters), loss = 8.29552
I0523 01:57:49.304810 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29552 (* 1 = 8.29552 loss)
I0523 01:57:49.384368 34682 sgd_solver.cpp:112] Iteration 30530, lr = 0.01
I0523 01:57:52.941294 34682 solver.cpp:239] Iteration 30540 (2.75003 iter/s, 3.63632s/10 iters), loss = 9.20132
I0523 01:57:52.941362 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20132 (* 1 = 9.20132 loss)
I0523 01:57:53.736385 34682 sgd_solver.cpp:112] Iteration 30540, lr = 0.01
I0523 01:57:59.351068 34682 solver.cpp:239] Iteration 30550 (1.5602 iter/s, 6.40945s/10 iters), loss = 9.3908
I0523 01:57:59.351250 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3908 (* 1 = 9.3908 loss)
I0523 01:57:59.425458 34682 sgd_solver.cpp:112] Iteration 30550, lr = 0.01
I0523 01:58:03.884778 34682 solver.cpp:239] Iteration 30560 (2.20588 iter/s, 4.53334s/10 iters), loss = 8.57689
I0523 01:58:03.884821 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57689 (* 1 = 8.57689 loss)
I0523 01:58:03.962674 34682 sgd_solver.cpp:112] Iteration 30560, lr = 0.01
I0523 01:58:07.064498 34682 solver.cpp:239] Iteration 30570 (3.14511 iter/s, 3.17954s/10 iters), loss = 8.99474
I0523 01:58:07.064545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99474 (* 1 = 8.99474 loss)
I0523 01:58:07.848362 34682 sgd_solver.cpp:112] Iteration 30570, lr = 0.01
I0523 01:58:13.053258 34682 solver.cpp:239] Iteration 30580 (1.66988 iter/s, 5.98846s/10 iters), loss = 8.65667
I0523 01:58:13.053320 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65667 (* 1 = 8.65667 loss)
I0523 01:58:13.120965 34682 sgd_solver.cpp:112] Iteration 30580, lr = 0.01
I0523 01:58:17.394407 34682 solver.cpp:239] Iteration 30590 (2.30367 iter/s, 4.3409s/10 iters), loss = 8.87934
I0523 01:58:17.394469 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87934 (* 1 = 8.87934 loss)
I0523 01:58:18.252571 34682 sgd_solver.cpp:112] Iteration 30590, lr = 0.01
I0523 01:58:23.719491 34682 solver.cpp:239] Iteration 30600 (1.58109 iter/s, 6.32477s/10 iters), loss = 8.34238
I0523 01:58:23.719550 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34238 (* 1 = 8.34238 loss)
I0523 01:58:24.519292 34682 sgd_solver.cpp:112] Iteration 30600, lr = 0.01
I0523 01:58:28.104209 34682 solver.cpp:239] Iteration 30610 (2.28077 iter/s, 4.38448s/10 iters), loss = 8.19593
I0523 01:58:28.104262 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19593 (* 1 = 8.19593 loss)
I0523 01:58:28.172947 34682 sgd_solver.cpp:112] Iteration 30610, lr = 0.01
I0523 01:58:33.795578 34682 solver.cpp:239] Iteration 30620 (1.75714 iter/s, 5.69108s/10 iters), loss = 8.81746
I0523 01:58:33.795810 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81746 (* 1 = 8.81746 loss)
I0523 01:58:33.854933 34682 sgd_solver.cpp:112] Iteration 30620, lr = 0.01
I0523 01:58:39.128950 34682 solver.cpp:239] Iteration 30630 (1.87513 iter/s, 5.33295s/10 iters), loss = 9.00268
I0523 01:58:39.129001 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00268 (* 1 = 9.00268 loss)
I0523 01:58:39.202697 34682 sgd_solver.cpp:112] Iteration 30630, lr = 0.01
I0523 01:58:43.892290 34682 solver.cpp:239] Iteration 30640 (2.09948 iter/s, 4.76309s/10 iters), loss = 9.15892
I0523 01:58:43.892347 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15892 (* 1 = 9.15892 loss)
I0523 01:58:44.677040 34682 sgd_solver.cpp:112] Iteration 30640, lr = 0.01
I0523 01:58:48.080667 34682 solver.cpp:239] Iteration 30650 (2.38769 iter/s, 4.18815s/10 iters), loss = 8.81482
I0523 01:58:48.080715 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81482 (* 1 = 8.81482 loss)
I0523 01:58:48.147609 34682 sgd_solver.cpp:112] Iteration 30650, lr = 0.01
I0523 01:58:52.395543 34682 solver.cpp:239] Iteration 30660 (2.31769 iter/s, 4.31465s/10 iters), loss = 8.27447
I0523 01:58:52.395591 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27447 (* 1 = 8.27447 loss)
I0523 01:58:52.463479 34682 sgd_solver.cpp:112] Iteration 30660, lr = 0.01
I0523 01:58:55.776614 34682 solver.cpp:239] Iteration 30670 (2.95782 iter/s, 3.38087s/10 iters), loss = 8.1701
I0523 01:58:55.776666 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1701 (* 1 = 8.1701 loss)
I0523 01:58:55.840528 34682 sgd_solver.cpp:112] Iteration 30670, lr = 0.01
I0523 01:58:59.996845 34682 solver.cpp:239] Iteration 30680 (2.36967 iter/s, 4.22s/10 iters), loss = 8.96449
I0523 01:58:59.996886 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96449 (* 1 = 8.96449 loss)
I0523 01:59:00.061723 34682 sgd_solver.cpp:112] Iteration 30680, lr = 0.01
I0523 01:59:05.647609 34682 solver.cpp:239] Iteration 30690 (1.76976 iter/s, 5.65049s/10 iters), loss = 9.36741
I0523 01:59:05.647840 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36741 (* 1 = 9.36741 loss)
I0523 01:59:05.717623 34682 sgd_solver.cpp:112] Iteration 30690, lr = 0.01
I0523 01:59:11.322723 34682 solver.cpp:239] Iteration 30700 (1.76223 iter/s, 5.67464s/10 iters), loss = 8.64981
I0523 01:59:11.322821 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64981 (* 1 = 8.64981 loss)
I0523 01:59:12.216917 34682 sgd_solver.cpp:112] Iteration 30700, lr = 0.01
I0523 01:59:17.937870 34682 solver.cpp:239] Iteration 30710 (1.51176 iter/s, 6.6148s/10 iters), loss = 8.74123
I0523 01:59:17.937917 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74123 (* 1 = 8.74123 loss)
I0523 01:59:17.992137 34682 sgd_solver.cpp:112] Iteration 30710, lr = 0.01
I0523 01:59:21.512626 34682 solver.cpp:239] Iteration 30720 (2.79754 iter/s, 3.57456s/10 iters), loss = 8.59494
I0523 01:59:21.512675 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59494 (* 1 = 8.59494 loss)
I0523 01:59:21.915602 34682 sgd_solver.cpp:112] Iteration 30720, lr = 0.01
I0523 01:59:27.332818 34682 solver.cpp:239] Iteration 30730 (1.71824 iter/s, 5.8199s/10 iters), loss = 9.00295
I0523 01:59:27.332872 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00295 (* 1 = 9.00295 loss)
I0523 01:59:27.389349 34682 sgd_solver.cpp:112] Iteration 30730, lr = 0.01
I0523 01:59:32.826558 34682 solver.cpp:239] Iteration 30740 (1.82035 iter/s, 5.49345s/10 iters), loss = 8.30293
I0523 01:59:32.826622 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30293 (* 1 = 8.30293 loss)
I0523 01:59:32.901937 34682 sgd_solver.cpp:112] Iteration 30740, lr = 0.01
I0523 01:59:36.524312 34682 solver.cpp:239] Iteration 30750 (2.7045 iter/s, 3.69755s/10 iters), loss = 8.50545
I0523 01:59:36.524488 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50545 (* 1 = 8.50545 loss)
I0523 01:59:37.346114 34682 sgd_solver.cpp:112] Iteration 30750, lr = 0.01
I0523 01:59:42.799504 34682 solver.cpp:239] Iteration 30760 (1.59368 iter/s, 6.27477s/10 iters), loss = 9.07279
I0523 01:59:42.799556 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07279 (* 1 = 9.07279 loss)
I0523 01:59:43.604485 34682 sgd_solver.cpp:112] Iteration 30760, lr = 0.01
I0523 01:59:49.342773 34682 solver.cpp:239] Iteration 30770 (1.52836 iter/s, 6.54295s/10 iters), loss = 8.82153
I0523 01:59:49.342829 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82153 (* 1 = 8.82153 loss)
I0523 01:59:49.409481 34682 sgd_solver.cpp:112] Iteration 30770, lr = 0.01
I0523 01:59:53.997256 34682 solver.cpp:239] Iteration 30780 (2.14858 iter/s, 4.65423s/10 iters), loss = 8.58427
I0523 01:59:53.997308 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58427 (* 1 = 8.58427 loss)
I0523 01:59:54.768532 34682 sgd_solver.cpp:112] Iteration 30780, lr = 0.01
I0523 02:00:00.358726 34682 solver.cpp:239] Iteration 30790 (1.57204 iter/s, 6.36114s/10 iters), loss = 8.83813
I0523 02:00:00.358791 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83813 (* 1 = 8.83813 loss)
I0523 02:00:00.423970 34682 sgd_solver.cpp:112] Iteration 30790, lr = 0.01
I0523 02:00:02.985589 34682 solver.cpp:239] Iteration 30800 (3.80707 iter/s, 2.62669s/10 iters), loss = 8.08522
I0523 02:00:02.985632 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08522 (* 1 = 8.08522 loss)
I0523 02:00:03.055645 34682 sgd_solver.cpp:112] Iteration 30800, lr = 0.01
I0523 02:00:07.201618 34682 solver.cpp:239] Iteration 30810 (2.37203 iter/s, 4.21581s/10 iters), loss = 8.29169
I0523 02:00:07.201809 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29169 (* 1 = 8.29169 loss)
I0523 02:00:07.811409 34682 sgd_solver.cpp:112] Iteration 30810, lr = 0.01
I0523 02:00:11.115816 34682 solver.cpp:239] Iteration 30820 (2.55503 iter/s, 3.91384s/10 iters), loss = 9.47326
I0523 02:00:11.115890 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47326 (* 1 = 9.47326 loss)
I0523 02:00:11.179376 34682 sgd_solver.cpp:112] Iteration 30820, lr = 0.01
I0523 02:00:15.092033 34682 solver.cpp:239] Iteration 30830 (2.5151 iter/s, 3.97598s/10 iters), loss = 9.02849
I0523 02:00:15.092087 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02849 (* 1 = 9.02849 loss)
I0523 02:00:15.160307 34682 sgd_solver.cpp:112] Iteration 30830, lr = 0.01
I0523 02:00:18.675952 34682 solver.cpp:239] Iteration 30840 (2.7904 iter/s, 3.58372s/10 iters), loss = 9.44516
I0523 02:00:18.676003 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44516 (* 1 = 9.44516 loss)
I0523 02:00:19.409235 34682 sgd_solver.cpp:112] Iteration 30840, lr = 0.01
I0523 02:00:23.573206 34682 solver.cpp:239] Iteration 30850 (2.04207 iter/s, 4.89699s/10 iters), loss = 8.45674
I0523 02:00:23.573268 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45674 (* 1 = 8.45674 loss)
I0523 02:00:24.442355 34682 sgd_solver.cpp:112] Iteration 30850, lr = 0.01
I0523 02:00:27.766121 34682 solver.cpp:239] Iteration 30860 (2.38511 iter/s, 4.19268s/10 iters), loss = 8.6621
I0523 02:00:27.766175 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6621 (* 1 = 8.6621 loss)
I0523 02:00:28.593566 34682 sgd_solver.cpp:112] Iteration 30860, lr = 0.01
I0523 02:00:32.476987 34682 solver.cpp:239] Iteration 30870 (2.12287 iter/s, 4.71061s/10 iters), loss = 8.36488
I0523 02:00:32.477037 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36488 (* 1 = 8.36488 loss)
I0523 02:00:32.533861 34682 sgd_solver.cpp:112] Iteration 30870, lr = 0.01
I0523 02:00:39.410207 34682 solver.cpp:239] Iteration 30880 (1.4424 iter/s, 6.93289s/10 iters), loss = 8.65476
I0523 02:00:39.410333 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65476 (* 1 = 8.65476 loss)
I0523 02:00:39.475749 34682 sgd_solver.cpp:112] Iteration 30880, lr = 0.01
I0523 02:00:44.283435 34682 solver.cpp:239] Iteration 30890 (2.05217 iter/s, 4.87289s/10 iters), loss = 8.49858
I0523 02:00:44.283504 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49858 (* 1 = 8.49858 loss)
I0523 02:00:45.101987 34682 sgd_solver.cpp:112] Iteration 30890, lr = 0.01
I0523 02:00:50.628031 34682 solver.cpp:239] Iteration 30900 (1.57623 iter/s, 6.34427s/10 iters), loss = 8.45051
I0523 02:00:50.628094 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45051 (* 1 = 8.45051 loss)
I0523 02:00:51.382683 34682 sgd_solver.cpp:112] Iteration 30900, lr = 0.01
I0523 02:00:54.655683 34682 solver.cpp:239] Iteration 30910 (2.48298 iter/s, 4.02741s/10 iters), loss = 8.70214
I0523 02:00:54.655740 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70214 (* 1 = 8.70214 loss)
I0523 02:00:55.390058 34682 sgd_solver.cpp:112] Iteration 30910, lr = 0.01
I0523 02:00:58.712162 34682 solver.cpp:239] Iteration 30920 (2.46533 iter/s, 4.05626s/10 iters), loss = 8.62908
I0523 02:00:58.712206 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62908 (* 1 = 8.62908 loss)
I0523 02:00:59.259146 34682 sgd_solver.cpp:112] Iteration 30920, lr = 0.01
I0523 02:01:05.303850 34682 solver.cpp:239] Iteration 30930 (1.51714 iter/s, 6.59137s/10 iters), loss = 8.83115
I0523 02:01:05.303900 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83115 (* 1 = 8.83115 loss)
I0523 02:01:05.371867 34682 sgd_solver.cpp:112] Iteration 30930, lr = 0.01
I0523 02:01:08.069319 34682 solver.cpp:239] Iteration 30940 (3.61624 iter/s, 2.76531s/10 iters), loss = 9.29117
I0523 02:01:08.069370 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29117 (* 1 = 9.29117 loss)
I0523 02:01:08.786864 34682 sgd_solver.cpp:112] Iteration 30940, lr = 0.01
I0523 02:01:12.807350 34682 solver.cpp:239] Iteration 30950 (2.11069 iter/s, 4.73779s/10 iters), loss = 8.99474
I0523 02:01:12.807476 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99474 (* 1 = 8.99474 loss)
I0523 02:01:13.066304 34682 sgd_solver.cpp:112] Iteration 30950, lr = 0.01
I0523 02:01:17.326158 34682 solver.cpp:239] Iteration 30960 (2.21313 iter/s, 4.51849s/10 iters), loss = 8.70016
I0523 02:01:17.326215 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70016 (* 1 = 8.70016 loss)
I0523 02:01:18.111115 34682 sgd_solver.cpp:112] Iteration 30960, lr = 0.01
I0523 02:01:23.216588 34682 solver.cpp:239] Iteration 30970 (1.69775 iter/s, 5.89013s/10 iters), loss = 9.79101
I0523 02:01:23.216634 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.79101 (* 1 = 9.79101 loss)
I0523 02:01:24.052271 34682 sgd_solver.cpp:112] Iteration 30970, lr = 0.01
I0523 02:01:27.301196 34682 solver.cpp:239] Iteration 30980 (2.44835 iter/s, 4.08439s/10 iters), loss = 9.20597
I0523 02:01:27.301245 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20597 (* 1 = 9.20597 loss)
I0523 02:01:27.381690 34682 sgd_solver.cpp:112] Iteration 30980, lr = 0.01
I0523 02:01:31.542006 34682 solver.cpp:239] Iteration 30990 (2.35816 iter/s, 4.24059s/10 iters), loss = 9.33272
I0523 02:01:31.542045 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33272 (* 1 = 9.33272 loss)
I0523 02:01:31.618609 34682 sgd_solver.cpp:112] Iteration 30990, lr = 0.01
I0523 02:01:35.538641 34682 solver.cpp:239] Iteration 31000 (2.50223 iter/s, 3.99643s/10 iters), loss = 8.75689
I0523 02:01:35.538702 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75689 (* 1 = 8.75689 loss)
I0523 02:01:36.231680 34682 sgd_solver.cpp:112] Iteration 31000, lr = 0.01
I0523 02:01:40.731549 34682 solver.cpp:239] Iteration 31010 (1.9258 iter/s, 5.19264s/10 iters), loss = 9.47188
I0523 02:01:40.731595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47188 (* 1 = 9.47188 loss)
I0523 02:01:40.808187 34682 sgd_solver.cpp:112] Iteration 31010, lr = 0.01
I0523 02:01:46.159478 34682 solver.cpp:239] Iteration 31020 (1.84241 iter/s, 5.42766s/10 iters), loss = 8.49626
I0523 02:01:46.159723 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49626 (* 1 = 8.49626 loss)
I0523 02:01:46.399677 34682 sgd_solver.cpp:112] Iteration 31020, lr = 0.01
I0523 02:01:51.119937 34682 solver.cpp:239] Iteration 31030 (2.01612 iter/s, 4.96003s/10 iters), loss = 9.23967
I0523 02:01:51.119992 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23967 (* 1 = 9.23967 loss)
I0523 02:01:51.179111 34682 sgd_solver.cpp:112] Iteration 31030, lr = 0.01
I0523 02:01:56.161948 34682 solver.cpp:239] Iteration 31040 (1.98344 iter/s, 5.04175s/10 iters), loss = 7.99887
I0523 02:01:56.161985 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99887 (* 1 = 7.99887 loss)
I0523 02:01:56.991653 34682 sgd_solver.cpp:112] Iteration 31040, lr = 0.01
I0523 02:02:01.120532 34682 solver.cpp:239] Iteration 31050 (2.01681 iter/s, 4.95833s/10 iters), loss = 9.89198
I0523 02:02:01.120599 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.89198 (* 1 = 9.89198 loss)
I0523 02:02:01.206411 34682 sgd_solver.cpp:112] Iteration 31050, lr = 0.01
I0523 02:02:02.538427 34682 solver.cpp:239] Iteration 31060 (7.05336 iter/s, 1.41776s/10 iters), loss = 8.43821
I0523 02:02:02.538482 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43821 (* 1 = 8.43821 loss)
I0523 02:02:02.574872 34682 sgd_solver.cpp:112] Iteration 31060, lr = 0.01
I0523 02:02:04.352782 34682 solver.cpp:239] Iteration 31070 (5.51205 iter/s, 1.81421s/10 iters), loss = 9.01201
I0523 02:02:04.352829 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01201 (* 1 = 9.01201 loss)
I0523 02:02:05.113512 34682 sgd_solver.cpp:112] Iteration 31070, lr = 0.01
I0523 02:02:09.946326 34682 solver.cpp:239] Iteration 31080 (1.78787 iter/s, 5.59326s/10 iters), loss = 9.48576
I0523 02:02:09.946369 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.48576 (* 1 = 9.48576 loss)
I0523 02:02:10.016319 34682 sgd_solver.cpp:112] Iteration 31080, lr = 0.01
I0523 02:02:14.882148 34682 solver.cpp:239] Iteration 31090 (2.0261 iter/s, 4.93558s/10 iters), loss = 8.61568
I0523 02:02:14.882189 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61568 (* 1 = 8.61568 loss)
I0523 02:02:14.960013 34682 sgd_solver.cpp:112] Iteration 31090, lr = 0.01
I0523 02:02:20.561949 34682 solver.cpp:239] Iteration 31100 (1.76071 iter/s, 5.67951s/10 iters), loss = 8.34832
I0523 02:02:20.562151 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34832 (* 1 = 8.34832 loss)
I0523 02:02:20.998445 34682 sgd_solver.cpp:112] Iteration 31100, lr = 0.01
I0523 02:02:25.031767 34682 solver.cpp:239] Iteration 31110 (2.23742 iter/s, 4.46944s/10 iters), loss = 9.40424
I0523 02:02:25.031816 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40424 (* 1 = 9.40424 loss)
I0523 02:02:25.911746 34682 sgd_solver.cpp:112] Iteration 31110, lr = 0.01
I0523 02:02:29.958824 34682 solver.cpp:239] Iteration 31120 (2.02971 iter/s, 4.92681s/10 iters), loss = 9.07333
I0523 02:02:29.958868 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07333 (* 1 = 9.07333 loss)
I0523 02:02:30.024792 34682 sgd_solver.cpp:112] Iteration 31120, lr = 0.01
I0523 02:02:34.163316 34682 solver.cpp:239] Iteration 31130 (2.37855 iter/s, 4.20425s/10 iters), loss = 8.15348
I0523 02:02:34.163398 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15348 (* 1 = 8.15348 loss)
I0523 02:02:34.882410 34682 sgd_solver.cpp:112] Iteration 31130, lr = 0.01
I0523 02:02:38.821841 34682 solver.cpp:239] Iteration 31140 (2.14673 iter/s, 4.65826s/10 iters), loss = 8.69898
I0523 02:02:38.821885 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69898 (* 1 = 8.69898 loss)
I0523 02:02:38.880666 34682 sgd_solver.cpp:112] Iteration 31140, lr = 0.01
I0523 02:02:43.000953 34682 solver.cpp:239] Iteration 31150 (2.39297 iter/s, 4.1789s/10 iters), loss = 8.35161
I0523 02:02:43.000998 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35161 (* 1 = 8.35161 loss)
I0523 02:02:43.061482 34682 sgd_solver.cpp:112] Iteration 31150, lr = 0.01
I0523 02:02:47.115684 34682 solver.cpp:239] Iteration 31160 (2.43043 iter/s, 4.1145s/10 iters), loss = 9.3823
I0523 02:02:47.115736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3823 (* 1 = 9.3823 loss)
I0523 02:02:47.187114 34682 sgd_solver.cpp:112] Iteration 31160, lr = 0.01
I0523 02:02:51.381661 34682 solver.cpp:239] Iteration 31170 (2.34425 iter/s, 4.26575s/10 iters), loss = 8.97354
I0523 02:02:51.381872 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97354 (* 1 = 8.97354 loss)
I0523 02:02:51.452116 34682 sgd_solver.cpp:112] Iteration 31170, lr = 0.01
I0523 02:02:55.930116 34682 solver.cpp:239] Iteration 31180 (2.19873 iter/s, 4.54808s/10 iters), loss = 8.85453
I0523 02:02:55.930176 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85453 (* 1 = 8.85453 loss)
I0523 02:02:55.991384 34682 sgd_solver.cpp:112] Iteration 31180, lr = 0.01
I0523 02:02:59.964805 34682 solver.cpp:239] Iteration 31190 (2.47864 iter/s, 4.03447s/10 iters), loss = 8.82958
I0523 02:02:59.964840 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82958 (* 1 = 8.82958 loss)
I0523 02:03:00.678648 34682 sgd_solver.cpp:112] Iteration 31190, lr = 0.01
I0523 02:03:05.926941 34682 solver.cpp:239] Iteration 31200 (1.67733 iter/s, 5.96186s/10 iters), loss = 8.44321
I0523 02:03:05.926985 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44321 (* 1 = 8.44321 loss)
I0523 02:03:05.995820 34682 sgd_solver.cpp:112] Iteration 31200, lr = 0.01
I0523 02:03:12.566092 34682 solver.cpp:239] Iteration 31210 (1.50629 iter/s, 6.63883s/10 iters), loss = 8.96269
I0523 02:03:12.566153 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96269 (* 1 = 8.96269 loss)
I0523 02:03:13.270843 34682 sgd_solver.cpp:112] Iteration 31210, lr = 0.01
I0523 02:03:17.717640 34682 solver.cpp:239] Iteration 31220 (1.94127 iter/s, 5.15127s/10 iters), loss = 9.23531
I0523 02:03:17.717689 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23531 (* 1 = 9.23531 loss)
I0523 02:03:17.797866 34682 sgd_solver.cpp:112] Iteration 31220, lr = 0.01
I0523 02:03:22.900202 34682 solver.cpp:239] Iteration 31230 (1.92965 iter/s, 5.1823s/10 iters), loss = 8.99495
I0523 02:03:22.900473 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99495 (* 1 = 8.99495 loss)
I0523 02:03:23.791065 34682 sgd_solver.cpp:112] Iteration 31230, lr = 0.01
I0523 02:03:29.554755 34682 solver.cpp:239] Iteration 31240 (1.50284 iter/s, 6.65405s/10 iters), loss = 8.82888
I0523 02:03:29.554800 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82888 (* 1 = 8.82888 loss)
I0523 02:03:30.385166 34682 sgd_solver.cpp:112] Iteration 31240, lr = 0.01
I0523 02:03:32.240936 34682 solver.cpp:239] Iteration 31250 (3.72298 iter/s, 2.68602s/10 iters), loss = 9.25695
I0523 02:03:32.240988 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25695 (* 1 = 9.25695 loss)
I0523 02:03:33.009506 34682 sgd_solver.cpp:112] Iteration 31250, lr = 0.01
I0523 02:03:36.017509 34682 solver.cpp:239] Iteration 31260 (2.64805 iter/s, 3.77636s/10 iters), loss = 9.56712
I0523 02:03:36.017562 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56712 (* 1 = 9.56712 loss)
I0523 02:03:36.096870 34682 sgd_solver.cpp:112] Iteration 31260, lr = 0.01
I0523 02:03:40.596396 34682 solver.cpp:239] Iteration 31270 (2.18405 iter/s, 4.57864s/10 iters), loss = 8.21474
I0523 02:03:40.596451 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21474 (* 1 = 8.21474 loss)
I0523 02:03:41.432240 34682 sgd_solver.cpp:112] Iteration 31270, lr = 0.01
I0523 02:03:45.350788 34682 solver.cpp:239] Iteration 31280 (2.10344 iter/s, 4.75413s/10 iters), loss = 8.78541
I0523 02:03:45.350848 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78541 (* 1 = 8.78541 loss)
I0523 02:03:45.410147 34682 sgd_solver.cpp:112] Iteration 31280, lr = 0.01
I0523 02:03:50.466348 34682 solver.cpp:239] Iteration 31290 (1.95492 iter/s, 5.11529s/10 iters), loss = 9.33316
I0523 02:03:50.466394 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33316 (* 1 = 9.33316 loss)
I0523 02:03:50.533556 34682 sgd_solver.cpp:112] Iteration 31290, lr = 0.01
I0523 02:03:55.532181 34682 solver.cpp:239] Iteration 31300 (1.97411 iter/s, 5.06558s/10 iters), loss = 8.81239
I0523 02:03:55.532330 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81239 (* 1 = 8.81239 loss)
I0523 02:03:56.309237 34682 sgd_solver.cpp:112] Iteration 31300, lr = 0.01
I0523 02:03:59.731031 34682 solver.cpp:239] Iteration 31310 (2.38179 iter/s, 4.19852s/10 iters), loss = 8.97282
I0523 02:03:59.731071 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97282 (* 1 = 8.97282 loss)
I0523 02:03:59.802534 34682 sgd_solver.cpp:112] Iteration 31310, lr = 0.01
I0523 02:04:04.546231 34682 solver.cpp:239] Iteration 31320 (2.07686 iter/s, 4.81496s/10 iters), loss = 8.74278
I0523 02:04:04.546270 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74278 (* 1 = 8.74278 loss)
I0523 02:04:04.620170 34682 sgd_solver.cpp:112] Iteration 31320, lr = 0.01
I0523 02:04:11.636667 34682 solver.cpp:239] Iteration 31330 (1.41042 iter/s, 7.0901s/10 iters), loss = 8.22471
I0523 02:04:11.636716 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22471 (* 1 = 8.22471 loss)
I0523 02:04:11.709718 34682 sgd_solver.cpp:112] Iteration 31330, lr = 0.01
I0523 02:04:15.491327 34682 solver.cpp:239] Iteration 31340 (2.5944 iter/s, 3.85445s/10 iters), loss = 8.86895
I0523 02:04:15.491370 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86895 (* 1 = 8.86895 loss)
I0523 02:04:15.555371 34682 sgd_solver.cpp:112] Iteration 31340, lr = 0.01
I0523 02:04:18.800972 34682 solver.cpp:239] Iteration 31350 (3.02165 iter/s, 3.30945s/10 iters), loss = 8.83816
I0523 02:04:18.801026 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83816 (* 1 = 8.83816 loss)
I0523 02:04:19.540119 34682 sgd_solver.cpp:112] Iteration 31350, lr = 0.01
I0523 02:04:23.624142 34682 solver.cpp:239] Iteration 31360 (2.07343 iter/s, 4.82292s/10 iters), loss = 8.09931
I0523 02:04:23.624188 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09931 (* 1 = 8.09931 loss)
I0523 02:04:23.702399 34682 sgd_solver.cpp:112] Iteration 31360, lr = 0.01
I0523 02:04:30.382783 34682 solver.cpp:239] Iteration 31370 (1.47966 iter/s, 6.75832s/10 iters), loss = 8.64265
I0523 02:04:30.383066 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64265 (* 1 = 8.64265 loss)
I0523 02:04:30.452105 34682 sgd_solver.cpp:112] Iteration 31370, lr = 0.01
I0523 02:04:33.963788 34682 solver.cpp:239] Iteration 31380 (2.79283 iter/s, 3.58059s/10 iters), loss = 8.31708
I0523 02:04:33.963846 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31708 (* 1 = 8.31708 loss)
I0523 02:04:34.781414 34682 sgd_solver.cpp:112] Iteration 31380, lr = 0.01
I0523 02:04:40.063989 34682 solver.cpp:239] Iteration 31390 (1.63937 iter/s, 6.0999s/10 iters), loss = 8.99355
I0523 02:04:40.064034 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99355 (* 1 = 8.99355 loss)
I0523 02:04:40.918910 34682 sgd_solver.cpp:112] Iteration 31390, lr = 0.01
I0523 02:04:45.195374 34682 solver.cpp:239] Iteration 31400 (1.94889 iter/s, 5.13112s/10 iters), loss = 9.21086
I0523 02:04:45.195427 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21086 (* 1 = 9.21086 loss)
I0523 02:04:45.263252 34682 sgd_solver.cpp:112] Iteration 31400, lr = 0.01
I0523 02:04:49.852306 34682 solver.cpp:239] Iteration 31410 (2.14745 iter/s, 4.65669s/10 iters), loss = 8.9095
I0523 02:04:49.852358 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9095 (* 1 = 8.9095 loss)
I0523 02:04:49.918203 34682 sgd_solver.cpp:112] Iteration 31410, lr = 0.01
I0523 02:04:53.143350 34682 solver.cpp:239] Iteration 31420 (3.03873 iter/s, 3.29085s/10 iters), loss = 10.0242
I0523 02:04:53.143399 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0242 (* 1 = 10.0242 loss)
I0523 02:04:53.207660 34682 sgd_solver.cpp:112] Iteration 31420, lr = 0.01
I0523 02:04:59.025653 34682 solver.cpp:239] Iteration 31430 (1.7001 iter/s, 5.882s/10 iters), loss = 8.55505
I0523 02:04:59.025718 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55505 (* 1 = 8.55505 loss)
I0523 02:04:59.611052 34682 sgd_solver.cpp:112] Iteration 31430, lr = 0.01
I0523 02:05:04.556541 34682 solver.cpp:239] Iteration 31440 (1.80813 iter/s, 5.53059s/10 iters), loss = 9.35677
I0523 02:05:04.556783 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35677 (* 1 = 9.35677 loss)
I0523 02:05:04.655933 34682 sgd_solver.cpp:112] Iteration 31440, lr = 0.01
I0523 02:05:07.535004 34682 solver.cpp:239] Iteration 31450 (3.35782 iter/s, 2.97812s/10 iters), loss = 9.14628
I0523 02:05:07.535065 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14628 (* 1 = 9.14628 loss)
I0523 02:05:08.328899 34682 sgd_solver.cpp:112] Iteration 31450, lr = 0.01
I0523 02:05:14.059640 34682 solver.cpp:239] Iteration 31460 (1.53273 iter/s, 6.52431s/10 iters), loss = 8.49196
I0523 02:05:14.059695 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49196 (* 1 = 8.49196 loss)
I0523 02:05:14.140707 34682 sgd_solver.cpp:112] Iteration 31460, lr = 0.01
I0523 02:05:17.939045 34682 solver.cpp:239] Iteration 31470 (2.57786 iter/s, 3.87918s/10 iters), loss = 8.72477
I0523 02:05:17.939092 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72477 (* 1 = 8.72477 loss)
I0523 02:05:18.725116 34682 sgd_solver.cpp:112] Iteration 31470, lr = 0.01
I0523 02:05:23.697986 34682 solver.cpp:239] Iteration 31480 (1.73651 iter/s, 5.75866s/10 iters), loss = 8.64763
I0523 02:05:23.698032 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64763 (* 1 = 8.64763 loss)
I0523 02:05:23.765767 34682 sgd_solver.cpp:112] Iteration 31480, lr = 0.01
I0523 02:05:27.768297 34682 solver.cpp:239] Iteration 31490 (2.45695 iter/s, 4.07009s/10 iters), loss = 9.22913
I0523 02:05:27.768357 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22913 (* 1 = 9.22913 loss)
I0523 02:05:28.643378 34682 sgd_solver.cpp:112] Iteration 31490, lr = 0.01
I0523 02:05:35.343361 34682 solver.cpp:239] Iteration 31500 (1.32018 iter/s, 7.5747s/10 iters), loss = 8.21996
I0523 02:05:35.343631 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21996 (* 1 = 8.21996 loss)
I0523 02:05:36.088150 34682 sgd_solver.cpp:112] Iteration 31500, lr = 0.01
I0523 02:05:40.075938 34682 solver.cpp:239] Iteration 31510 (2.11321 iter/s, 4.73214s/10 iters), loss = 8.84256
I0523 02:05:40.075989 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84256 (* 1 = 8.84256 loss)
I0523 02:05:40.143623 34682 sgd_solver.cpp:112] Iteration 31510, lr = 0.01
I0523 02:05:44.191900 34682 solver.cpp:239] Iteration 31520 (2.4297 iter/s, 4.11574s/10 iters), loss = 8.47598
I0523 02:05:44.191951 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47598 (* 1 = 8.47598 loss)
I0523 02:05:44.272217 34682 sgd_solver.cpp:112] Iteration 31520, lr = 0.01
I0523 02:05:49.206007 34682 solver.cpp:239] Iteration 31530 (1.99448 iter/s, 5.01385s/10 iters), loss = 8.86618
I0523 02:05:49.206054 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86618 (* 1 = 8.86618 loss)
I0523 02:05:49.273146 34682 sgd_solver.cpp:112] Iteration 31530, lr = 0.01
I0523 02:05:53.628254 34682 solver.cpp:239] Iteration 31540 (2.26142 iter/s, 4.422s/10 iters), loss = 9.17295
I0523 02:05:53.628327 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17295 (* 1 = 9.17295 loss)
I0523 02:05:54.364601 34682 sgd_solver.cpp:112] Iteration 31540, lr = 0.01
I0523 02:06:00.996258 34682 solver.cpp:239] Iteration 31550 (1.35729 iter/s, 7.36764s/10 iters), loss = 8.78481
I0523 02:06:00.996300 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78481 (* 1 = 8.78481 loss)
I0523 02:06:01.066591 34682 sgd_solver.cpp:112] Iteration 31550, lr = 0.01
I0523 02:06:03.682293 34682 solver.cpp:239] Iteration 31560 (3.72317 iter/s, 2.68588s/10 iters), loss = 8.55989
I0523 02:06:03.682338 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55989 (* 1 = 8.55989 loss)
I0523 02:06:03.747576 34682 sgd_solver.cpp:112] Iteration 31560, lr = 0.01
I0523 02:06:09.136363 34682 solver.cpp:239] Iteration 31570 (1.83358 iter/s, 5.4538s/10 iters), loss = 9.41001
I0523 02:06:09.136499 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.41001 (* 1 = 9.41001 loss)
I0523 02:06:09.200475 34682 sgd_solver.cpp:112] Iteration 31570, lr = 0.01
I0523 02:06:14.091713 34682 solver.cpp:239] Iteration 31580 (2.01816 iter/s, 4.955s/10 iters), loss = 8.8861
I0523 02:06:14.091775 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8861 (* 1 = 8.8861 loss)
I0523 02:06:14.916693 34682 sgd_solver.cpp:112] Iteration 31580, lr = 0.01
I0523 02:06:20.460196 34682 solver.cpp:239] Iteration 31590 (1.57031 iter/s, 6.36816s/10 iters), loss = 8.53821
I0523 02:06:20.460245 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53821 (* 1 = 8.53821 loss)
I0523 02:06:21.298456 34682 sgd_solver.cpp:112] Iteration 31590, lr = 0.01
I0523 02:06:25.123028 34682 solver.cpp:239] Iteration 31600 (2.14473 iter/s, 4.66259s/10 iters), loss = 8.32092
I0523 02:06:25.123081 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32092 (* 1 = 8.32092 loss)
I0523 02:06:25.866591 34682 sgd_solver.cpp:112] Iteration 31600, lr = 0.01
I0523 02:06:28.979393 34682 solver.cpp:239] Iteration 31610 (2.59326 iter/s, 3.85615s/10 iters), loss = 8.29696
I0523 02:06:28.979447 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29696 (* 1 = 8.29696 loss)
I0523 02:06:29.606144 34682 sgd_solver.cpp:112] Iteration 31610, lr = 0.01
I0523 02:06:35.573022 34682 solver.cpp:239] Iteration 31620 (1.51669 iter/s, 6.5933s/10 iters), loss = 8.62108
I0523 02:06:35.573091 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62108 (* 1 = 8.62108 loss)
I0523 02:06:36.427992 34682 sgd_solver.cpp:112] Iteration 31620, lr = 0.01
I0523 02:06:39.876305 34682 solver.cpp:239] Iteration 31630 (2.32394 iter/s, 4.30304s/10 iters), loss = 8.73541
I0523 02:06:39.876531 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73541 (* 1 = 8.73541 loss)
I0523 02:06:40.589252 34682 sgd_solver.cpp:112] Iteration 31630, lr = 0.01
I0523 02:06:45.157515 34682 solver.cpp:239] Iteration 31640 (1.89366 iter/s, 5.28078s/10 iters), loss = 7.81528
I0523 02:06:45.157562 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81528 (* 1 = 7.81528 loss)
I0523 02:06:45.217715 34682 sgd_solver.cpp:112] Iteration 31640, lr = 0.01
I0523 02:06:49.256588 34682 solver.cpp:239] Iteration 31650 (2.4397 iter/s, 4.09886s/10 iters), loss = 8.43489
I0523 02:06:49.256631 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43489 (* 1 = 8.43489 loss)
I0523 02:06:49.316524 34682 sgd_solver.cpp:112] Iteration 31650, lr = 0.01
I0523 02:06:54.159425 34682 solver.cpp:239] Iteration 31660 (2.03974 iter/s, 4.90258s/10 iters), loss = 8.64241
I0523 02:06:54.159481 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64241 (* 1 = 8.64241 loss)
I0523 02:06:54.991503 34682 sgd_solver.cpp:112] Iteration 31660, lr = 0.01
I0523 02:06:59.073156 34682 solver.cpp:239] Iteration 31670 (2.03522 iter/s, 4.91347s/10 iters), loss = 9.17866
I0523 02:06:59.073207 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17866 (* 1 = 9.17866 loss)
I0523 02:06:59.135280 34682 sgd_solver.cpp:112] Iteration 31670, lr = 0.01
I0523 02:07:03.120002 34682 solver.cpp:239] Iteration 31680 (2.47119 iter/s, 4.04663s/10 iters), loss = 9.60781
I0523 02:07:03.120055 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60781 (* 1 = 9.60781 loss)
I0523 02:07:03.185405 34682 sgd_solver.cpp:112] Iteration 31680, lr = 0.01
I0523 02:07:08.557032 34682 solver.cpp:239] Iteration 31690 (1.83934 iter/s, 5.43674s/10 iters), loss = 8.83121
I0523 02:07:08.557090 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83121 (* 1 = 8.83121 loss)
I0523 02:07:09.393599 34682 sgd_solver.cpp:112] Iteration 31690, lr = 0.01
I0523 02:07:13.197041 34682 solver.cpp:239] Iteration 31700 (2.15528 iter/s, 4.63976s/10 iters), loss = 8.86863
I0523 02:07:13.197259 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86863 (* 1 = 8.86863 loss)
I0523 02:07:13.275264 34682 sgd_solver.cpp:112] Iteration 31700, lr = 0.01
I0523 02:07:18.701556 34682 solver.cpp:239] Iteration 31710 (1.81683 iter/s, 5.5041s/10 iters), loss = 9.16277
I0523 02:07:18.701611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16277 (* 1 = 9.16277 loss)
I0523 02:07:18.773957 34682 sgd_solver.cpp:112] Iteration 31710, lr = 0.01
I0523 02:07:21.982300 34682 solver.cpp:239] Iteration 31720 (3.04827 iter/s, 3.28055s/10 iters), loss = 10.0391
I0523 02:07:21.982349 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0391 (* 1 = 10.0391 loss)
I0523 02:07:22.045817 34682 sgd_solver.cpp:112] Iteration 31720, lr = 0.01
I0523 02:07:28.954592 34682 solver.cpp:239] Iteration 31730 (1.43432 iter/s, 6.97195s/10 iters), loss = 9.19127
I0523 02:07:28.954666 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19127 (* 1 = 9.19127 loss)
I0523 02:07:29.016940 34682 sgd_solver.cpp:112] Iteration 31730, lr = 0.01
I0523 02:07:34.000365 34682 solver.cpp:239] Iteration 31740 (1.98197 iter/s, 5.0455s/10 iters), loss = 8.29873
I0523 02:07:34.000427 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29873 (* 1 = 8.29873 loss)
I0523 02:07:34.060417 34682 sgd_solver.cpp:112] Iteration 31740, lr = 0.01
I0523 02:07:37.543090 34682 solver.cpp:239] Iteration 31750 (2.82285 iter/s, 3.54252s/10 iters), loss = 8.33988
I0523 02:07:37.543130 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33988 (* 1 = 8.33988 loss)
I0523 02:07:37.600294 34682 sgd_solver.cpp:112] Iteration 31750, lr = 0.01
I0523 02:07:40.566351 34682 solver.cpp:239] Iteration 31760 (3.30788 iter/s, 3.02309s/10 iters), loss = 8.96782
I0523 02:07:40.566395 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96782 (* 1 = 8.96782 loss)
I0523 02:07:40.644371 34682 sgd_solver.cpp:112] Iteration 31760, lr = 0.01
I0523 02:07:44.793675 34682 solver.cpp:239] Iteration 31770 (2.36569 iter/s, 4.2271s/10 iters), loss = 8.35244
I0523 02:07:44.793876 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35244 (* 1 = 8.35244 loss)
I0523 02:07:44.856145 34682 sgd_solver.cpp:112] Iteration 31770, lr = 0.01
I0523 02:07:49.799422 34682 solver.cpp:239] Iteration 31780 (1.99787 iter/s, 5.00534s/10 iters), loss = 8.78229
I0523 02:07:49.799475 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78229 (* 1 = 8.78229 loss)
I0523 02:07:50.420577 34682 sgd_solver.cpp:112] Iteration 31780, lr = 0.01
I0523 02:07:57.512502 34682 solver.cpp:239] Iteration 31790 (1.29656 iter/s, 7.71272s/10 iters), loss = 9.44511
I0523 02:07:57.512545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44511 (* 1 = 9.44511 loss)
I0523 02:07:58.244279 34682 sgd_solver.cpp:112] Iteration 31790, lr = 0.01
I0523 02:08:01.541889 34682 solver.cpp:239] Iteration 31800 (2.48189 iter/s, 4.02919s/10 iters), loss = 9.28653
I0523 02:08:01.541929 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28653 (* 1 = 9.28653 loss)
I0523 02:08:01.620493 34682 sgd_solver.cpp:112] Iteration 31800, lr = 0.01
I0523 02:08:05.357626 34682 solver.cpp:239] Iteration 31810 (2.62086 iter/s, 3.81554s/10 iters), loss = 7.82433
I0523 02:08:05.357679 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82433 (* 1 = 7.82433 loss)
I0523 02:08:05.429358 34682 sgd_solver.cpp:112] Iteration 31810, lr = 0.01
I0523 02:08:10.388741 34682 solver.cpp:239] Iteration 31820 (1.98774 iter/s, 5.03085s/10 iters), loss = 9.36073
I0523 02:08:10.388794 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36073 (* 1 = 9.36073 loss)
I0523 02:08:10.463865 34682 sgd_solver.cpp:112] Iteration 31820, lr = 0.01
I0523 02:08:16.559447 34682 solver.cpp:239] Iteration 31830 (1.62064 iter/s, 6.17039s/10 iters), loss = 9.11475
I0523 02:08:16.559684 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11475 (* 1 = 9.11475 loss)
I0523 02:08:16.630800 34682 sgd_solver.cpp:112] Iteration 31830, lr = 0.01
I0523 02:08:23.400480 34682 solver.cpp:239] Iteration 31840 (1.46188 iter/s, 6.84053s/10 iters), loss = 8.49372
I0523 02:08:23.400543 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49372 (* 1 = 8.49372 loss)
I0523 02:08:23.723772 34682 sgd_solver.cpp:112] Iteration 31840, lr = 0.01
I0523 02:08:27.888806 34682 solver.cpp:239] Iteration 31850 (2.22813 iter/s, 4.48806s/10 iters), loss = 9.04852
I0523 02:08:27.888859 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04852 (* 1 = 9.04852 loss)
I0523 02:08:27.946842 34682 sgd_solver.cpp:112] Iteration 31850, lr = 0.01
I0523 02:08:33.716486 34682 solver.cpp:239] Iteration 31860 (1.71604 iter/s, 5.82738s/10 iters), loss = 9.81362
I0523 02:08:33.716549 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.81362 (* 1 = 9.81362 loss)
I0523 02:08:34.523160 34682 sgd_solver.cpp:112] Iteration 31860, lr = 0.01
I0523 02:08:37.889223 34682 solver.cpp:239] Iteration 31870 (2.39664 iter/s, 4.1725s/10 iters), loss = 8.3497
I0523 02:08:37.889271 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3497 (* 1 = 8.3497 loss)
I0523 02:08:38.568912 34682 sgd_solver.cpp:112] Iteration 31870, lr = 0.01
I0523 02:08:43.081266 34682 solver.cpp:239] Iteration 31880 (1.92612 iter/s, 5.19178s/10 iters), loss = 9.11124
I0523 02:08:43.081331 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11124 (* 1 = 9.11124 loss)
I0523 02:08:43.142781 34682 sgd_solver.cpp:112] Iteration 31880, lr = 0.01
I0523 02:08:47.590690 34682 solver.cpp:239] Iteration 31890 (2.2177 iter/s, 4.50918s/10 iters), loss = 9.12028
I0523 02:08:47.592429 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12028 (* 1 = 9.12028 loss)
I0523 02:08:47.657761 34682 sgd_solver.cpp:112] Iteration 31890, lr = 0.01
I0523 02:08:52.570658 34682 solver.cpp:239] Iteration 31900 (2.00883 iter/s, 4.97803s/10 iters), loss = 8.67254
I0523 02:08:52.570721 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67254 (* 1 = 8.67254 loss)
I0523 02:08:52.630558 34682 sgd_solver.cpp:112] Iteration 31900, lr = 0.01
I0523 02:08:58.070999 34682 solver.cpp:239] Iteration 31910 (1.81816 iter/s, 5.50005s/10 iters), loss = 8.6164
I0523 02:08:58.071048 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6164 (* 1 = 8.6164 loss)
I0523 02:08:58.151345 34682 sgd_solver.cpp:112] Iteration 31910, lr = 0.01
I0523 02:09:01.997860 34682 solver.cpp:239] Iteration 31920 (2.5467 iter/s, 3.92665s/10 iters), loss = 9.21271
I0523 02:09:01.997917 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21271 (* 1 = 9.21271 loss)
I0523 02:09:02.203817 34682 sgd_solver.cpp:112] Iteration 31920, lr = 0.01
I0523 02:09:04.725143 34682 solver.cpp:239] Iteration 31930 (3.66689 iter/s, 2.72711s/10 iters), loss = 8.01054
I0523 02:09:04.725193 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01054 (* 1 = 8.01054 loss)
I0523 02:09:04.805238 34682 sgd_solver.cpp:112] Iteration 31930, lr = 0.01
I0523 02:09:09.926738 34682 solver.cpp:239] Iteration 31940 (1.92259 iter/s, 5.20132s/10 iters), loss = 8.86929
I0523 02:09:09.926795 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86929 (* 1 = 8.86929 loss)
I0523 02:09:10.752148 34682 sgd_solver.cpp:112] Iteration 31940, lr = 0.01
I0523 02:09:14.334520 34682 solver.cpp:239] Iteration 31950 (2.26884 iter/s, 4.40754s/10 iters), loss = 10.0372
I0523 02:09:14.334581 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0372 (* 1 = 10.0372 loss)
I0523 02:09:15.229033 34682 sgd_solver.cpp:112] Iteration 31950, lr = 0.01
I0523 02:09:19.250820 34682 solver.cpp:239] Iteration 31960 (2.03416 iter/s, 4.91603s/10 iters), loss = 8.22457
I0523 02:09:19.251121 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22457 (* 1 = 8.22457 loss)
I0523 02:09:19.909561 34682 sgd_solver.cpp:112] Iteration 31960, lr = 0.01
I0523 02:09:25.266763 34682 solver.cpp:239] Iteration 31970 (1.6624 iter/s, 6.01541s/10 iters), loss = 8.48513
I0523 02:09:25.266824 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48513 (* 1 = 8.48513 loss)
I0523 02:09:25.324072 34682 sgd_solver.cpp:112] Iteration 31970, lr = 0.01
I0523 02:09:32.205741 34682 solver.cpp:239] Iteration 31980 (1.4412 iter/s, 6.93864s/10 iters), loss = 7.8056
I0523 02:09:32.205780 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8056 (* 1 = 7.8056 loss)
I0523 02:09:32.278307 34682 sgd_solver.cpp:112] Iteration 31980, lr = 0.01
I0523 02:09:36.695859 34682 solver.cpp:239] Iteration 31990 (2.22723 iter/s, 4.48988s/10 iters), loss = 9.66793
I0523 02:09:36.695911 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.66793 (* 1 = 9.66793 loss)
I0523 02:09:36.766512 34682 sgd_solver.cpp:112] Iteration 31990, lr = 0.01
I0523 02:09:42.332267 34682 solver.cpp:239] Iteration 32000 (1.77427 iter/s, 5.63613s/10 iters), loss = 8.30344
I0523 02:09:42.332312 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30344 (* 1 = 8.30344 loss)
I0523 02:09:42.401943 34682 sgd_solver.cpp:112] Iteration 32000, lr = 0.01
I0523 02:09:47.254012 34682 solver.cpp:239] Iteration 32010 (2.0319 iter/s, 4.92149s/10 iters), loss = 8.44976
I0523 02:09:47.254058 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44976 (* 1 = 8.44976 loss)
I0523 02:09:47.332685 34682 sgd_solver.cpp:112] Iteration 32010, lr = 0.01
I0523 02:09:51.349375 34682 solver.cpp:239] Iteration 32020 (2.44191 iter/s, 4.09515s/10 iters), loss = 8.96225
I0523 02:09:51.349505 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96225 (* 1 = 8.96225 loss)
I0523 02:09:52.118726 34682 sgd_solver.cpp:112] Iteration 32020, lr = 0.01
I0523 02:09:56.996495 34682 solver.cpp:239] Iteration 32030 (1.77092 iter/s, 5.64677s/10 iters), loss = 8.41757
I0523 02:09:56.996541 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41757 (* 1 = 8.41757 loss)
I0523 02:09:57.059922 34682 sgd_solver.cpp:112] Iteration 32030, lr = 0.01
I0523 02:10:01.613477 34682 solver.cpp:239] Iteration 32040 (2.16603 iter/s, 4.61674s/10 iters), loss = 7.59009
I0523 02:10:01.613529 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.59009 (* 1 = 7.59009 loss)
I0523 02:10:01.697285 34682 sgd_solver.cpp:112] Iteration 32040, lr = 0.01
I0523 02:10:06.524230 34682 solver.cpp:239] Iteration 32050 (2.03645 iter/s, 4.9105s/10 iters), loss = 8.86124
I0523 02:10:06.524276 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86124 (* 1 = 8.86124 loss)
I0523 02:10:06.596830 34682 sgd_solver.cpp:112] Iteration 32050, lr = 0.01
I0523 02:10:10.678640 34682 solver.cpp:239] Iteration 32060 (2.40721 iter/s, 4.15419s/10 iters), loss = 7.93144
I0523 02:10:10.678679 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93144 (* 1 = 7.93144 loss)
I0523 02:10:10.755743 34682 sgd_solver.cpp:112] Iteration 32060, lr = 0.01
I0523 02:10:15.337891 34682 solver.cpp:239] Iteration 32070 (2.14638 iter/s, 4.65901s/10 iters), loss = 8.91372
I0523 02:10:15.337949 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91372 (* 1 = 8.91372 loss)
I0523 02:10:15.406025 34682 sgd_solver.cpp:112] Iteration 32070, lr = 0.01
I0523 02:10:21.305359 34682 solver.cpp:239] Iteration 32080 (1.67584 iter/s, 5.96717s/10 iters), loss = 9.16559
I0523 02:10:21.305403 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16559 (* 1 = 9.16559 loss)
I0523 02:10:21.367239 34682 sgd_solver.cpp:112] Iteration 32080, lr = 0.01
I0523 02:10:26.356377 34682 solver.cpp:239] Iteration 32090 (1.9799 iter/s, 5.05076s/10 iters), loss = 8.01392
I0523 02:10:26.356436 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01392 (* 1 = 8.01392 loss)
I0523 02:10:27.072249 34682 sgd_solver.cpp:112] Iteration 32090, lr = 0.01
I0523 02:10:33.450477 34682 solver.cpp:239] Iteration 32100 (1.40969 iter/s, 7.09376s/10 iters), loss = 9.49236
I0523 02:10:33.450532 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.49236 (* 1 = 9.49236 loss)
I0523 02:10:34.120892 34682 sgd_solver.cpp:112] Iteration 32100, lr = 0.01
I0523 02:10:39.015265 34682 solver.cpp:239] Iteration 32110 (1.7971 iter/s, 5.56451s/10 iters), loss = 8.84591
I0523 02:10:39.015305 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84591 (* 1 = 8.84591 loss)
I0523 02:10:39.083003 34682 sgd_solver.cpp:112] Iteration 32110, lr = 0.01
I0523 02:10:44.657472 34682 solver.cpp:239] Iteration 32120 (1.77244 iter/s, 5.64193s/10 iters), loss = 8.64161
I0523 02:10:44.657524 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64161 (* 1 = 8.64161 loss)
I0523 02:10:44.983695 34682 sgd_solver.cpp:112] Iteration 32120, lr = 0.01
I0523 02:10:49.558310 34682 solver.cpp:239] Iteration 32130 (2.04057 iter/s, 4.90059s/10 iters), loss = 10.0774
I0523 02:10:49.558351 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0774 (* 1 = 10.0774 loss)
I0523 02:10:49.626555 34682 sgd_solver.cpp:112] Iteration 32130, lr = 0.01
I0523 02:10:52.998350 34682 solver.cpp:239] Iteration 32140 (2.9071 iter/s, 3.43986s/10 iters), loss = 9.11425
I0523 02:10:52.998584 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11425 (* 1 = 9.11425 loss)
I0523 02:10:53.070466 34682 sgd_solver.cpp:112] Iteration 32140, lr = 0.01
I0523 02:10:58.527273 34682 solver.cpp:239] Iteration 32150 (1.80881 iter/s, 5.52848s/10 iters), loss = 8.42272
I0523 02:10:58.527320 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42272 (* 1 = 8.42272 loss)
I0523 02:10:58.588429 34682 sgd_solver.cpp:112] Iteration 32150, lr = 0.01
I0523 02:11:03.463593 34682 solver.cpp:239] Iteration 32160 (2.0259 iter/s, 4.93607s/10 iters), loss = 9.38306
I0523 02:11:03.463644 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.38306 (* 1 = 9.38306 loss)
I0523 02:11:04.248797 34682 sgd_solver.cpp:112] Iteration 32160, lr = 0.01
I0523 02:11:08.251497 34682 solver.cpp:239] Iteration 32170 (2.08871 iter/s, 4.78764s/10 iters), loss = 8.01389
I0523 02:11:08.251564 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01389 (* 1 = 8.01389 loss)
I0523 02:11:09.092437 34682 sgd_solver.cpp:112] Iteration 32170, lr = 0.01
I0523 02:11:13.956684 34682 solver.cpp:239] Iteration 32180 (1.75288 iter/s, 5.70489s/10 iters), loss = 8.47727
I0523 02:11:13.956742 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47727 (* 1 = 8.47727 loss)
I0523 02:11:14.029997 34682 sgd_solver.cpp:112] Iteration 32180, lr = 0.01
I0523 02:11:19.274624 34682 solver.cpp:239] Iteration 32190 (1.88052 iter/s, 5.31767s/10 iters), loss = 7.97925
I0523 02:11:19.274668 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97925 (* 1 = 7.97925 loss)
I0523 02:11:19.354984 34682 sgd_solver.cpp:112] Iteration 32190, lr = 0.01
I0523 02:11:23.656575 34682 solver.cpp:239] Iteration 32200 (2.28221 iter/s, 4.38172s/10 iters), loss = 9.4466
I0523 02:11:23.656924 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4466 (* 1 = 9.4466 loss)
I0523 02:11:24.514125 34682 sgd_solver.cpp:112] Iteration 32200, lr = 0.01
I0523 02:11:28.440239 34682 solver.cpp:239] Iteration 32210 (2.09067 iter/s, 4.78315s/10 iters), loss = 9.4495
I0523 02:11:28.440295 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4495 (* 1 = 9.4495 loss)
I0523 02:11:28.509534 34682 sgd_solver.cpp:112] Iteration 32210, lr = 0.01
I0523 02:11:33.382431 34682 solver.cpp:239] Iteration 32220 (2.02349 iter/s, 4.94195s/10 iters), loss = 8.4108
I0523 02:11:33.382468 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4108 (* 1 = 8.4108 loss)
I0523 02:11:33.457958 34682 sgd_solver.cpp:112] Iteration 32220, lr = 0.01
I0523 02:11:37.166193 34682 solver.cpp:239] Iteration 32230 (2.64302 iter/s, 3.78355s/10 iters), loss = 9.04864
I0523 02:11:37.166250 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04864 (* 1 = 9.04864 loss)
I0523 02:11:37.266253 34682 sgd_solver.cpp:112] Iteration 32230, lr = 0.01
I0523 02:11:43.114580 34682 solver.cpp:239] Iteration 32240 (1.68121 iter/s, 5.94809s/10 iters), loss = 9.9687
I0523 02:11:43.114639 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.9687 (* 1 = 9.9687 loss)
I0523 02:11:43.936991 34682 sgd_solver.cpp:112] Iteration 32240, lr = 0.01
I0523 02:11:47.870784 34682 solver.cpp:239] Iteration 32250 (2.10263 iter/s, 4.75595s/10 iters), loss = 9.43935
I0523 02:11:47.870829 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43935 (* 1 = 9.43935 loss)
I0523 02:11:47.946337 34682 sgd_solver.cpp:112] Iteration 32250, lr = 0.01
I0523 02:11:51.960606 34682 solver.cpp:239] Iteration 32260 (2.44522 iter/s, 4.08961s/10 iters), loss = 9.01379
I0523 02:11:51.960645 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01379 (* 1 = 9.01379 loss)
I0523 02:11:52.021392 34682 sgd_solver.cpp:112] Iteration 32260, lr = 0.01
I0523 02:11:55.276836 34682 solver.cpp:239] Iteration 32270 (3.01564 iter/s, 3.31605s/10 iters), loss = 8.75032
I0523 02:11:55.276981 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75032 (* 1 = 8.75032 loss)
I0523 02:11:56.029903 34682 sgd_solver.cpp:112] Iteration 32270, lr = 0.01
I0523 02:12:02.148316 34682 solver.cpp:239] Iteration 32280 (1.45538 iter/s, 6.87106s/10 iters), loss = 9.0446
I0523 02:12:02.148375 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0446 (* 1 = 9.0446 loss)
I0523 02:12:02.682061 34682 sgd_solver.cpp:112] Iteration 32280, lr = 0.01
I0523 02:12:07.734691 34682 solver.cpp:239] Iteration 32290 (1.79016 iter/s, 5.58608s/10 iters), loss = 9.55514
I0523 02:12:07.734769 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.55514 (* 1 = 9.55514 loss)
I0523 02:12:07.822062 34682 sgd_solver.cpp:112] Iteration 32290, lr = 0.01
I0523 02:12:11.784039 34682 solver.cpp:239] Iteration 32300 (2.46968 iter/s, 4.04911s/10 iters), loss = 8.20471
I0523 02:12:11.784091 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20471 (* 1 = 8.20471 loss)
I0523 02:12:11.855515 34682 sgd_solver.cpp:112] Iteration 32300, lr = 0.01
I0523 02:12:15.858788 34682 solver.cpp:239] Iteration 32310 (2.45428 iter/s, 4.07452s/10 iters), loss = 8.39875
I0523 02:12:15.858842 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39875 (* 1 = 8.39875 loss)
I0523 02:12:16.698851 34682 sgd_solver.cpp:112] Iteration 32310, lr = 0.01
I0523 02:12:22.413851 34682 solver.cpp:239] Iteration 32320 (1.52561 iter/s, 6.55475s/10 iters), loss = 9.05732
I0523 02:12:22.413895 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05732 (* 1 = 9.05732 loss)
I0523 02:12:22.476444 34682 sgd_solver.cpp:112] Iteration 32320, lr = 0.01
I0523 02:12:28.186771 34682 solver.cpp:239] Iteration 32330 (1.73231 iter/s, 5.77264s/10 iters), loss = 8.91793
I0523 02:12:28.186935 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91793 (* 1 = 8.91793 loss)
I0523 02:12:28.255662 34682 sgd_solver.cpp:112] Iteration 32330, lr = 0.01
I0523 02:12:32.649154 34682 solver.cpp:239] Iteration 32340 (2.24113 iter/s, 4.46203s/10 iters), loss = 8.51003
I0523 02:12:32.649214 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51003 (* 1 = 8.51003 loss)
I0523 02:12:32.721560 34682 sgd_solver.cpp:112] Iteration 32340, lr = 0.01
I0523 02:12:37.418768 34682 solver.cpp:239] Iteration 32350 (2.09672 iter/s, 4.76936s/10 iters), loss = 8.65413
I0523 02:12:37.418817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65413 (* 1 = 8.65413 loss)
I0523 02:12:38.273648 34682 sgd_solver.cpp:112] Iteration 32350, lr = 0.01
I0523 02:12:43.236297 34682 solver.cpp:239] Iteration 32360 (1.71903 iter/s, 5.81723s/10 iters), loss = 8.4864
I0523 02:12:43.236376 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4864 (* 1 = 8.4864 loss)
I0523 02:12:43.978075 34682 sgd_solver.cpp:112] Iteration 32360, lr = 0.01
I0523 02:12:48.969481 34682 solver.cpp:239] Iteration 32370 (1.74433 iter/s, 5.73288s/10 iters), loss = 8.29043
I0523 02:12:48.969537 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29043 (* 1 = 8.29043 loss)
I0523 02:12:49.831894 34682 sgd_solver.cpp:112] Iteration 32370, lr = 0.01
I0523 02:12:54.713490 34682 solver.cpp:239] Iteration 32380 (1.74103 iter/s, 5.74372s/10 iters), loss = 7.97793
I0523 02:12:54.713532 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97793 (* 1 = 7.97793 loss)
I0523 02:12:54.774562 34682 sgd_solver.cpp:112] Iteration 32380, lr = 0.01
I0523 02:12:59.174002 34682 solver.cpp:239] Iteration 32390 (2.24201 iter/s, 4.46028s/10 iters), loss = 8.29943
I0523 02:12:59.174211 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29943 (* 1 = 8.29943 loss)
I0523 02:12:59.245482 34682 sgd_solver.cpp:112] Iteration 32390, lr = 0.01
I0523 02:13:02.642184 34682 solver.cpp:239] Iteration 32400 (2.88362 iter/s, 3.46786s/10 iters), loss = 8.05797
I0523 02:13:02.642220 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05797 (* 1 = 8.05797 loss)
I0523 02:13:02.720916 34682 sgd_solver.cpp:112] Iteration 32400, lr = 0.01
I0523 02:13:06.434634 34682 solver.cpp:239] Iteration 32410 (2.63696 iter/s, 3.79225s/10 iters), loss = 8.34871
I0523 02:13:06.434685 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34871 (* 1 = 8.34871 loss)
I0523 02:13:07.258981 34682 sgd_solver.cpp:112] Iteration 32410, lr = 0.01
I0523 02:13:11.394807 34682 solver.cpp:239] Iteration 32420 (2.01617 iter/s, 4.95989s/10 iters), loss = 8.70954
I0523 02:13:11.394870 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70954 (* 1 = 8.70954 loss)
I0523 02:13:12.246314 34682 sgd_solver.cpp:112] Iteration 32420, lr = 0.01
I0523 02:13:16.275547 34682 solver.cpp:239] Iteration 32430 (2.04898 iter/s, 4.88048s/10 iters), loss = 9.12652
I0523 02:13:16.275593 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12652 (* 1 = 9.12652 loss)
I0523 02:13:16.350100 34682 sgd_solver.cpp:112] Iteration 32430, lr = 0.01
I0523 02:13:21.132383 34682 solver.cpp:239] Iteration 32440 (2.05906 iter/s, 4.85659s/10 iters), loss = 8.30555
I0523 02:13:21.132436 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30555 (* 1 = 8.30555 loss)
I0523 02:13:21.208043 34682 sgd_solver.cpp:112] Iteration 32440, lr = 0.01
I0523 02:13:26.686045 34682 solver.cpp:239] Iteration 32450 (1.8007 iter/s, 5.55338s/10 iters), loss = 8.50526
I0523 02:13:26.686096 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50526 (* 1 = 8.50526 loss)
I0523 02:13:26.750166 34682 sgd_solver.cpp:112] Iteration 32450, lr = 0.01
I0523 02:13:29.598573 34682 solver.cpp:239] Iteration 32460 (3.43364 iter/s, 2.91236s/10 iters), loss = 9.70157
I0523 02:13:29.598806 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.70157 (* 1 = 9.70157 loss)
I0523 02:13:29.666383 34682 sgd_solver.cpp:112] Iteration 32460, lr = 0.01
I0523 02:13:33.098114 34682 solver.cpp:239] Iteration 32470 (2.85783 iter/s, 3.49916s/10 iters), loss = 9.56878
I0523 02:13:33.098168 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56878 (* 1 = 9.56878 loss)
I0523 02:13:33.165643 34682 sgd_solver.cpp:112] Iteration 32470, lr = 0.01
I0523 02:13:38.873553 34682 solver.cpp:239] Iteration 32480 (1.73156 iter/s, 5.77515s/10 iters), loss = 8.17566
I0523 02:13:38.873617 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17566 (* 1 = 8.17566 loss)
I0523 02:13:38.948567 34682 sgd_solver.cpp:112] Iteration 32480, lr = 0.01
I0523 02:13:43.586405 34682 solver.cpp:239] Iteration 32490 (2.12198 iter/s, 4.71259s/10 iters), loss = 9.95304
I0523 02:13:43.586465 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.95304 (* 1 = 9.95304 loss)
I0523 02:13:43.703768 34682 sgd_solver.cpp:112] Iteration 32490, lr = 0.01
I0523 02:13:48.374804 34682 solver.cpp:239] Iteration 32500 (2.08849 iter/s, 4.78815s/10 iters), loss = 9.25883
I0523 02:13:48.374847 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25883 (* 1 = 9.25883 loss)
I0523 02:13:48.439594 34682 sgd_solver.cpp:112] Iteration 32500, lr = 0.01
I0523 02:13:53.297371 34682 solver.cpp:239] Iteration 32510 (2.03156 iter/s, 4.92232s/10 iters), loss = 8.37949
I0523 02:13:53.297420 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37949 (* 1 = 8.37949 loss)
I0523 02:13:53.367206 34682 sgd_solver.cpp:112] Iteration 32510, lr = 0.01
I0523 02:13:58.328039 34682 solver.cpp:239] Iteration 32520 (1.98791 iter/s, 5.03042s/10 iters), loss = 9.50725
I0523 02:13:58.328083 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.50725 (* 1 = 9.50725 loss)
I0523 02:13:59.131871 34682 sgd_solver.cpp:112] Iteration 32520, lr = 0.01
I0523 02:14:02.621229 34682 solver.cpp:239] Iteration 32530 (2.32939 iter/s, 4.29297s/10 iters), loss = 8.9983
I0523 02:14:02.621423 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9983 (* 1 = 8.9983 loss)
I0523 02:14:03.491380 34682 sgd_solver.cpp:112] Iteration 32530, lr = 0.01
I0523 02:14:05.776057 34682 solver.cpp:239] Iteration 32540 (3.17007 iter/s, 3.15451s/10 iters), loss = 8.65876
I0523 02:14:05.776105 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65876 (* 1 = 8.65876 loss)
I0523 02:14:06.416218 34682 sgd_solver.cpp:112] Iteration 32540, lr = 0.01
I0523 02:14:09.045331 34682 solver.cpp:239] Iteration 32550 (3.05897 iter/s, 3.26908s/10 iters), loss = 8.67731
I0523 02:14:09.045388 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67731 (* 1 = 8.67731 loss)
I0523 02:14:09.740711 34682 sgd_solver.cpp:112] Iteration 32550, lr = 0.01
I0523 02:14:14.494784 34682 solver.cpp:239] Iteration 32560 (1.83514 iter/s, 5.44918s/10 iters), loss = 8.67114
I0523 02:14:14.494823 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67114 (* 1 = 8.67114 loss)
I0523 02:14:14.558698 34682 sgd_solver.cpp:112] Iteration 32560, lr = 0.01
I0523 02:14:20.791802 34682 solver.cpp:239] Iteration 32570 (1.58813 iter/s, 6.29672s/10 iters), loss = 8.79253
I0523 02:14:20.791858 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79253 (* 1 = 8.79253 loss)
I0523 02:14:20.845477 34682 sgd_solver.cpp:112] Iteration 32570, lr = 0.01
I0523 02:14:24.228107 34682 solver.cpp:239] Iteration 32580 (2.9134 iter/s, 3.43242s/10 iters), loss = 8.2175
I0523 02:14:24.228142 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2175 (* 1 = 8.2175 loss)
I0523 02:14:24.299865 34682 sgd_solver.cpp:112] Iteration 32580, lr = 0.01
I0523 02:14:29.683771 34682 solver.cpp:239] Iteration 32590 (1.83304 iter/s, 5.45541s/10 iters), loss = 8.4356
I0523 02:14:29.683821 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4356 (* 1 = 8.4356 loss)
I0523 02:14:29.749814 34682 sgd_solver.cpp:112] Iteration 32590, lr = 0.01
I0523 02:14:34.288108 34682 solver.cpp:239] Iteration 32600 (2.17198 iter/s, 4.6041s/10 iters), loss = 8.8496
I0523 02:14:34.288331 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8496 (* 1 = 8.8496 loss)
I0523 02:14:34.366952 34682 sgd_solver.cpp:112] Iteration 32600, lr = 0.01
I0523 02:14:39.441030 34682 solver.cpp:239] Iteration 32610 (1.94081 iter/s, 5.15249s/10 iters), loss = 8.49738
I0523 02:14:39.441114 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49738 (* 1 = 8.49738 loss)
I0523 02:14:40.099792 34682 sgd_solver.cpp:112] Iteration 32610, lr = 0.01
I0523 02:14:45.723444 34682 solver.cpp:239] Iteration 32620 (1.59183 iter/s, 6.28207s/10 iters), loss = 9.69672
I0523 02:14:45.723515 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.69672 (* 1 = 9.69672 loss)
I0523 02:14:45.941566 34682 sgd_solver.cpp:112] Iteration 32620, lr = 0.01
I0523 02:14:50.165853 34682 solver.cpp:239] Iteration 32630 (2.25337 iter/s, 4.43779s/10 iters), loss = 8.37765
I0523 02:14:50.165894 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37765 (* 1 = 8.37765 loss)
I0523 02:14:50.965454 34682 sgd_solver.cpp:112] Iteration 32630, lr = 0.01
I0523 02:14:57.133908 34682 solver.cpp:239] Iteration 32640 (1.43519 iter/s, 6.96773s/10 iters), loss = 9.83799
I0523 02:14:57.133975 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.83799 (* 1 = 9.83799 loss)
I0523 02:14:57.842437 34682 sgd_solver.cpp:112] Iteration 32640, lr = 0.01
I0523 02:15:02.031261 34682 solver.cpp:239] Iteration 32650 (2.04203 iter/s, 4.89709s/10 iters), loss = 8.1974
I0523 02:15:02.031314 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1974 (* 1 = 8.1974 loss)
I0523 02:15:02.861436 34682 sgd_solver.cpp:112] Iteration 32650, lr = 0.01
I0523 02:15:07.719956 34682 solver.cpp:239] Iteration 32660 (1.75796 iter/s, 5.68841s/10 iters), loss = 9.00156
I0523 02:15:07.720150 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00156 (* 1 = 9.00156 loss)
I0523 02:15:07.792690 34682 sgd_solver.cpp:112] Iteration 32660, lr = 0.01
I0523 02:15:10.990609 34682 solver.cpp:239] Iteration 32670 (3.06176 iter/s, 3.2661s/10 iters), loss = 8.52701
I0523 02:15:10.990664 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52701 (* 1 = 8.52701 loss)
I0523 02:15:11.073155 34682 sgd_solver.cpp:112] Iteration 32670, lr = 0.01
I0523 02:15:15.453696 34682 solver.cpp:239] Iteration 32680 (2.24073 iter/s, 4.46283s/10 iters), loss = 9.90857
I0523 02:15:15.453742 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.90857 (* 1 = 9.90857 loss)
I0523 02:15:16.189679 34682 sgd_solver.cpp:112] Iteration 32680, lr = 0.01
I0523 02:15:21.370049 34682 solver.cpp:239] Iteration 32690 (1.69032 iter/s, 5.91605s/10 iters), loss = 9.07828
I0523 02:15:21.370124 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07828 (* 1 = 9.07828 loss)
I0523 02:15:21.915042 34682 sgd_solver.cpp:112] Iteration 32690, lr = 0.01
I0523 02:15:27.489634 34682 solver.cpp:239] Iteration 32700 (1.63418 iter/s, 6.11926s/10 iters), loss = 9.13276
I0523 02:15:27.489691 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13276 (* 1 = 9.13276 loss)
I0523 02:15:28.216058 34682 sgd_solver.cpp:112] Iteration 32700, lr = 0.01
I0523 02:15:32.205538 34682 solver.cpp:239] Iteration 32710 (2.1206 iter/s, 4.71566s/10 iters), loss = 8.86889
I0523 02:15:32.205585 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86889 (* 1 = 8.86889 loss)
I0523 02:15:32.991828 34682 sgd_solver.cpp:112] Iteration 32710, lr = 0.01
I0523 02:15:36.272120 34682 solver.cpp:239] Iteration 32720 (2.45921 iter/s, 4.06635s/10 iters), loss = 7.70001
I0523 02:15:36.272181 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.70001 (* 1 = 7.70001 loss)
I0523 02:15:36.334607 34682 sgd_solver.cpp:112] Iteration 32720, lr = 0.01
I0523 02:15:42.454962 34682 solver.cpp:239] Iteration 32730 (1.61746 iter/s, 6.18253s/10 iters), loss = 9.15586
I0523 02:15:42.455255 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15586 (* 1 = 9.15586 loss)
I0523 02:15:43.299546 34682 sgd_solver.cpp:112] Iteration 32730, lr = 0.01
I0523 02:15:47.176892 34682 solver.cpp:239] Iteration 32740 (2.11798 iter/s, 4.72148s/10 iters), loss = 8.58322
I0523 02:15:47.176930 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58322 (* 1 = 8.58322 loss)
I0523 02:15:47.247195 34682 sgd_solver.cpp:112] Iteration 32740, lr = 0.01
I0523 02:15:51.472718 34682 solver.cpp:239] Iteration 32750 (2.32796 iter/s, 4.29561s/10 iters), loss = 7.4701
I0523 02:15:51.472764 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4701 (* 1 = 7.4701 loss)
I0523 02:15:52.327836 34682 sgd_solver.cpp:112] Iteration 32750, lr = 0.01
I0523 02:15:55.721345 34682 solver.cpp:239] Iteration 32760 (2.35383 iter/s, 4.2484s/10 iters), loss = 9.30959
I0523 02:15:55.721391 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30959 (* 1 = 9.30959 loss)
I0523 02:15:55.790311 34682 sgd_solver.cpp:112] Iteration 32760, lr = 0.01
I0523 02:16:00.422957 34682 solver.cpp:239] Iteration 32770 (2.12704 iter/s, 4.70137s/10 iters), loss = 8.71173
I0523 02:16:00.423013 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71173 (* 1 = 8.71173 loss)
I0523 02:16:01.017710 34682 sgd_solver.cpp:112] Iteration 32770, lr = 0.01
I0523 02:16:05.262315 34682 solver.cpp:239] Iteration 32780 (2.0665 iter/s, 4.83911s/10 iters), loss = 8.4042
I0523 02:16:05.262361 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4042 (* 1 = 8.4042 loss)
I0523 02:16:05.333626 34682 sgd_solver.cpp:112] Iteration 32780, lr = 0.01
I0523 02:16:10.062784 34682 solver.cpp:239] Iteration 32790 (2.08324 iter/s, 4.80022s/10 iters), loss = 9.50928
I0523 02:16:10.062840 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.50928 (* 1 = 9.50928 loss)
I0523 02:16:10.547642 34682 sgd_solver.cpp:112] Iteration 32790, lr = 0.01
I0523 02:16:16.132515 34682 solver.cpp:239] Iteration 32800 (1.6476 iter/s, 6.06942s/10 iters), loss = 8.91504
I0523 02:16:16.132663 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91504 (* 1 = 8.91504 loss)
I0523 02:16:16.191546 34682 sgd_solver.cpp:112] Iteration 32800, lr = 0.01
I0523 02:16:21.073796 34682 solver.cpp:239] Iteration 32810 (2.02391 iter/s, 4.94094s/10 iters), loss = 8.78279
I0523 02:16:21.073839 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78279 (* 1 = 8.78279 loss)
I0523 02:16:21.886117 34682 sgd_solver.cpp:112] Iteration 32810, lr = 0.01
I0523 02:16:25.179608 34682 solver.cpp:239] Iteration 32820 (2.4357 iter/s, 4.1056s/10 iters), loss = 8.97692
I0523 02:16:25.179653 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97692 (* 1 = 8.97692 loss)
I0523 02:16:25.257598 34682 sgd_solver.cpp:112] Iteration 32820, lr = 0.01
I0523 02:16:29.735044 34682 solver.cpp:239] Iteration 32830 (2.19529 iter/s, 4.5552s/10 iters), loss = 8.68477
I0523 02:16:29.735095 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68477 (* 1 = 8.68477 loss)
I0523 02:16:30.617066 34682 sgd_solver.cpp:112] Iteration 32830, lr = 0.01
I0523 02:16:35.558658 34682 solver.cpp:239] Iteration 32840 (1.71723 iter/s, 5.82333s/10 iters), loss = 9.44703
I0523 02:16:35.558732 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44703 (* 1 = 9.44703 loss)
I0523 02:16:36.222546 34682 sgd_solver.cpp:112] Iteration 32840, lr = 0.01
I0523 02:16:40.974283 34682 solver.cpp:239] Iteration 32850 (1.84661 iter/s, 5.41533s/10 iters), loss = 7.84729
I0523 02:16:40.974326 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84729 (* 1 = 7.84729 loss)
I0523 02:16:41.046437 34682 sgd_solver.cpp:112] Iteration 32850, lr = 0.01
I0523 02:16:46.613937 34682 solver.cpp:239] Iteration 32860 (1.77324 iter/s, 5.63938s/10 iters), loss = 9.37056
I0523 02:16:46.614092 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.37056 (* 1 = 9.37056 loss)
I0523 02:16:46.678907 34682 sgd_solver.cpp:112] Iteration 32860, lr = 0.01
I0523 02:16:51.291788 34682 solver.cpp:239] Iteration 32870 (2.13789 iter/s, 4.6775s/10 iters), loss = 8.84082
I0523 02:16:51.291839 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84082 (* 1 = 8.84082 loss)
I0523 02:16:51.351337 34682 sgd_solver.cpp:112] Iteration 32870, lr = 0.01
I0523 02:16:57.123878 34682 solver.cpp:239] Iteration 32880 (1.71474 iter/s, 5.83178s/10 iters), loss = 8.63422
I0523 02:16:57.123955 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63422 (* 1 = 8.63422 loss)
I0523 02:16:57.868062 34682 sgd_solver.cpp:112] Iteration 32880, lr = 0.01
I0523 02:17:03.429880 34682 solver.cpp:239] Iteration 32890 (1.58587 iter/s, 6.30568s/10 iters), loss = 8.84247
I0523 02:17:03.429921 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84247 (* 1 = 8.84247 loss)
I0523 02:17:03.505301 34682 sgd_solver.cpp:112] Iteration 32890, lr = 0.01
I0523 02:17:07.522527 34682 solver.cpp:239] Iteration 32900 (2.44353 iter/s, 4.09244s/10 iters), loss = 8.77806
I0523 02:17:07.522572 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77806 (* 1 = 8.77806 loss)
I0523 02:17:07.591897 34682 sgd_solver.cpp:112] Iteration 32900, lr = 0.01
I0523 02:17:12.386127 34682 solver.cpp:239] Iteration 32910 (2.0562 iter/s, 4.86335s/10 iters), loss = 8.28798
I0523 02:17:12.386190 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28798 (* 1 = 8.28798 loss)
I0523 02:17:13.244578 34682 sgd_solver.cpp:112] Iteration 32910, lr = 0.01
I0523 02:17:17.644548 34682 solver.cpp:239] Iteration 32920 (1.90181 iter/s, 5.25815s/10 iters), loss = 9.43313
I0523 02:17:17.644729 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43313 (* 1 = 9.43313 loss)
I0523 02:17:17.717970 34682 sgd_solver.cpp:112] Iteration 32920, lr = 0.01
I0523 02:17:24.140590 34682 solver.cpp:239] Iteration 32930 (1.53951 iter/s, 6.49558s/10 iters), loss = 8.54116
I0523 02:17:24.140655 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54116 (* 1 = 8.54116 loss)
I0523 02:17:24.195899 34682 sgd_solver.cpp:112] Iteration 32930, lr = 0.01
I0523 02:17:27.259295 34682 solver.cpp:239] Iteration 32940 (3.20665 iter/s, 3.11852s/10 iters), loss = 10.0058
I0523 02:17:27.259340 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0058 (* 1 = 10.0058 loss)
I0523 02:17:28.083288 34682 sgd_solver.cpp:112] Iteration 32940, lr = 0.01
I0523 02:17:32.855779 34682 solver.cpp:239] Iteration 32950 (1.78692 iter/s, 5.59621s/10 iters), loss = 9.34845
I0523 02:17:32.855834 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34845 (* 1 = 9.34845 loss)
I0523 02:17:33.649493 34682 sgd_solver.cpp:112] Iteration 32950, lr = 0.01
I0523 02:17:38.713838 34682 solver.cpp:239] Iteration 32960 (1.70714 iter/s, 5.85774s/10 iters), loss = 9.07914
I0523 02:17:38.713903 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07914 (* 1 = 9.07914 loss)
I0523 02:17:39.494729 34682 sgd_solver.cpp:112] Iteration 32960, lr = 0.01
I0523 02:17:43.631338 34682 solver.cpp:239] Iteration 32970 (2.03366 iter/s, 4.91724s/10 iters), loss = 8.59942
I0523 02:17:43.631391 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59942 (* 1 = 8.59942 loss)
I0523 02:17:43.695864 34682 sgd_solver.cpp:112] Iteration 32970, lr = 0.01
I0523 02:17:48.424741 34682 solver.cpp:239] Iteration 32980 (2.08631 iter/s, 4.79316s/10 iters), loss = 8.86379
I0523 02:17:48.424927 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86379 (* 1 = 8.86379 loss)
I0523 02:17:48.485608 34682 sgd_solver.cpp:112] Iteration 32980, lr = 0.01
I0523 02:17:52.686290 34682 solver.cpp:239] Iteration 32990 (2.34675 iter/s, 4.26121s/10 iters), loss = 9.29361
I0523 02:17:52.686334 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29361 (* 1 = 9.29361 loss)
I0523 02:17:52.761970 34682 sgd_solver.cpp:112] Iteration 32990, lr = 0.01
I0523 02:17:56.881927 34682 solver.cpp:239] Iteration 33000 (2.38355 iter/s, 4.19542s/10 iters), loss = 9.27193
I0523 02:17:56.881970 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27193 (* 1 = 9.27193 loss)
I0523 02:17:57.749195 34682 sgd_solver.cpp:112] Iteration 33000, lr = 0.01
I0523 02:18:01.338577 34682 solver.cpp:239] Iteration 33010 (2.24395 iter/s, 4.45642s/10 iters), loss = 9.03144
I0523 02:18:01.338624 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03144 (* 1 = 9.03144 loss)
I0523 02:18:01.414275 34682 sgd_solver.cpp:112] Iteration 33010, lr = 0.01
I0523 02:18:05.552949 34682 solver.cpp:239] Iteration 33020 (2.37296 iter/s, 4.21415s/10 iters), loss = 8.89439
I0523 02:18:05.553005 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89439 (* 1 = 8.89439 loss)
I0523 02:18:05.648500 34682 sgd_solver.cpp:112] Iteration 33020, lr = 0.01
I0523 02:18:09.805896 34682 solver.cpp:239] Iteration 33030 (2.35143 iter/s, 4.25272s/10 iters), loss = 9.63039
I0523 02:18:09.805941 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.63039 (* 1 = 9.63039 loss)
I0523 02:18:09.885695 34682 sgd_solver.cpp:112] Iteration 33030, lr = 0.01
I0523 02:18:15.462255 34682 solver.cpp:239] Iteration 33040 (1.76801 iter/s, 5.65608s/10 iters), loss = 9.04786
I0523 02:18:15.462326 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04786 (* 1 = 9.04786 loss)
I0523 02:18:15.531289 34682 sgd_solver.cpp:112] Iteration 33040, lr = 0.01
I0523 02:18:18.985291 34682 solver.cpp:239] Iteration 33050 (2.83863 iter/s, 3.52283s/10 iters), loss = 8.9594
I0523 02:18:18.985543 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9594 (* 1 = 8.9594 loss)
I0523 02:18:19.756748 34682 sgd_solver.cpp:112] Iteration 33050, lr = 0.01
I0523 02:18:23.931362 34682 solver.cpp:239] Iteration 33060 (2.02198 iter/s, 4.94564s/10 iters), loss = 9.19145
I0523 02:18:23.931407 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19145 (* 1 = 9.19145 loss)
I0523 02:18:24.767601 34682 sgd_solver.cpp:112] Iteration 33060, lr = 0.01
I0523 02:18:28.240213 34682 solver.cpp:239] Iteration 33070 (2.32093 iter/s, 4.30862s/10 iters), loss = 9.28876
I0523 02:18:28.240272 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28876 (* 1 = 9.28876 loss)
I0523 02:18:28.309376 34682 sgd_solver.cpp:112] Iteration 33070, lr = 0.01
I0523 02:18:33.134690 34682 solver.cpp:239] Iteration 33080 (2.04323 iter/s, 4.89422s/10 iters), loss = 8.49018
I0523 02:18:33.134753 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49018 (* 1 = 8.49018 loss)
I0523 02:18:33.194135 34682 sgd_solver.cpp:112] Iteration 33080, lr = 0.01
I0523 02:18:38.110623 34682 solver.cpp:239] Iteration 33090 (2.00978 iter/s, 4.97566s/10 iters), loss = 8.22563
I0523 02:18:38.110675 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22563 (* 1 = 8.22563 loss)
I0523 02:18:38.187791 34682 sgd_solver.cpp:112] Iteration 33090, lr = 0.01
I0523 02:18:43.219043 34682 solver.cpp:239] Iteration 33100 (1.95765 iter/s, 5.10815s/10 iters), loss = 9.62729
I0523 02:18:43.219099 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.62729 (* 1 = 9.62729 loss)
I0523 02:18:43.295194 34682 sgd_solver.cpp:112] Iteration 33100, lr = 0.01
I0523 02:18:48.186188 34682 solver.cpp:239] Iteration 33110 (2.01333 iter/s, 4.96689s/10 iters), loss = 8.97035
I0523 02:18:48.186236 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97035 (* 1 = 8.97035 loss)
I0523 02:18:48.242434 34682 sgd_solver.cpp:112] Iteration 33110, lr = 0.01
I0523 02:18:52.966928 34682 solver.cpp:239] Iteration 33120 (2.09184 iter/s, 4.78049s/10 iters), loss = 9.22595
I0523 02:18:52.967183 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22595 (* 1 = 9.22595 loss)
I0523 02:18:53.789830 34682 sgd_solver.cpp:112] Iteration 33120, lr = 0.01
I0523 02:18:59.856658 34682 solver.cpp:239] Iteration 33130 (1.45154 iter/s, 6.88923s/10 iters), loss = 9.42885
I0523 02:18:59.856719 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42885 (* 1 = 9.42885 loss)
I0523 02:19:00.502332 34682 sgd_solver.cpp:112] Iteration 33130, lr = 0.01
I0523 02:19:02.845233 34682 solver.cpp:239] Iteration 33140 (3.34629 iter/s, 2.98838s/10 iters), loss = 8.96912
I0523 02:19:02.845299 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96912 (* 1 = 8.96912 loss)
I0523 02:19:02.913981 34682 sgd_solver.cpp:112] Iteration 33140, lr = 0.01
I0523 02:19:07.674031 34682 solver.cpp:239] Iteration 33150 (2.07102 iter/s, 4.82853s/10 iters), loss = 8.93602
I0523 02:19:07.674088 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93602 (* 1 = 8.93602 loss)
I0523 02:19:08.562716 34682 sgd_solver.cpp:112] Iteration 33150, lr = 0.01
I0523 02:19:12.544174 34682 solver.cpp:239] Iteration 33160 (2.05344 iter/s, 4.86989s/10 iters), loss = 9.32354
I0523 02:19:12.544224 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32354 (* 1 = 9.32354 loss)
I0523 02:19:13.005548 34682 sgd_solver.cpp:112] Iteration 33160, lr = 0.01
I0523 02:19:18.613582 34682 solver.cpp:239] Iteration 33170 (1.64769 iter/s, 6.06911s/10 iters), loss = 8.64547
I0523 02:19:18.613627 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64547 (* 1 = 8.64547 loss)
I0523 02:19:19.424060 34682 sgd_solver.cpp:112] Iteration 33170, lr = 0.01
I0523 02:19:24.207451 34682 solver.cpp:239] Iteration 33180 (1.78776 iter/s, 5.59359s/10 iters), loss = 8.73889
I0523 02:19:24.207690 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73889 (* 1 = 8.73889 loss)
I0523 02:19:24.269893 34682 sgd_solver.cpp:112] Iteration 33180, lr = 0.01
I0523 02:19:27.482398 34682 solver.cpp:239] Iteration 33190 (3.05384 iter/s, 3.27457s/10 iters), loss = 9.35159
I0523 02:19:27.482451 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35159 (* 1 = 9.35159 loss)
I0523 02:19:27.550364 34682 sgd_solver.cpp:112] Iteration 33190, lr = 0.01
I0523 02:19:33.055768 34682 solver.cpp:239] Iteration 33200 (1.79433 iter/s, 5.5731s/10 iters), loss = 9.40467
I0523 02:19:33.055814 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40467 (* 1 = 9.40467 loss)
I0523 02:19:33.132905 34682 sgd_solver.cpp:112] Iteration 33200, lr = 0.01
I0523 02:19:37.644547 34682 solver.cpp:239] Iteration 33210 (2.17934 iter/s, 4.58854s/10 iters), loss = 7.94763
I0523 02:19:37.644609 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94763 (* 1 = 7.94763 loss)
I0523 02:19:37.712393 34682 sgd_solver.cpp:112] Iteration 33210, lr = 0.01
I0523 02:19:43.266486 34682 solver.cpp:239] Iteration 33220 (1.77884 iter/s, 5.62166s/10 iters), loss = 8.85729
I0523 02:19:43.266530 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85729 (* 1 = 8.85729 loss)
I0523 02:19:43.346472 34682 sgd_solver.cpp:112] Iteration 33220, lr = 0.01
I0523 02:19:48.762534 34682 solver.cpp:239] Iteration 33230 (1.81958 iter/s, 5.49578s/10 iters), loss = 8.75768
I0523 02:19:48.762583 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75768 (* 1 = 8.75768 loss)
I0523 02:19:48.827200 34682 sgd_solver.cpp:112] Iteration 33230, lr = 0.01
I0523 02:19:52.713918 34682 solver.cpp:239] Iteration 33240 (2.5309 iter/s, 3.95117s/10 iters), loss = 9.29804
I0523 02:19:52.713966 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29804 (* 1 = 9.29804 loss)
I0523 02:19:53.511775 34682 sgd_solver.cpp:112] Iteration 33240, lr = 0.01
I0523 02:19:56.956924 34682 solver.cpp:239] Iteration 33250 (2.35694 iter/s, 4.24279s/10 iters), loss = 8.34386
I0523 02:19:56.957165 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34386 (* 1 = 8.34386 loss)
I0523 02:19:57.630162 34682 sgd_solver.cpp:112] Iteration 33250, lr = 0.01
I0523 02:20:02.601665 34682 solver.cpp:239] Iteration 33260 (1.7717 iter/s, 5.64429s/10 iters), loss = 8.51105
I0523 02:20:02.601712 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51105 (* 1 = 8.51105 loss)
I0523 02:20:03.431571 34682 sgd_solver.cpp:112] Iteration 33260, lr = 0.01
I0523 02:20:07.634702 34682 solver.cpp:239] Iteration 33270 (1.98697 iter/s, 5.03279s/10 iters), loss = 9.24242
I0523 02:20:07.634752 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24242 (* 1 = 9.24242 loss)
I0523 02:20:07.692819 34682 sgd_solver.cpp:112] Iteration 33270, lr = 0.01
I0523 02:20:11.491613 34682 solver.cpp:239] Iteration 33280 (2.59289 iter/s, 3.8567s/10 iters), loss = 9.90002
I0523 02:20:11.491664 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.90002 (* 1 = 9.90002 loss)
I0523 02:20:11.566555 34682 sgd_solver.cpp:112] Iteration 33280, lr = 0.01
I0523 02:20:16.486438 34682 solver.cpp:239] Iteration 33290 (2.00218 iter/s, 4.99456s/10 iters), loss = 8.82621
I0523 02:20:16.486497 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82621 (* 1 = 8.82621 loss)
I0523 02:20:17.175704 34682 sgd_solver.cpp:112] Iteration 33290, lr = 0.01
I0523 02:20:21.547523 34682 solver.cpp:239] Iteration 33300 (1.97597 iter/s, 5.06081s/10 iters), loss = 8.70326
I0523 02:20:21.547602 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70326 (* 1 = 8.70326 loss)
I0523 02:20:22.188539 34682 sgd_solver.cpp:112] Iteration 33300, lr = 0.01
I0523 02:20:26.224932 34682 solver.cpp:239] Iteration 33310 (2.13806 iter/s, 4.67715s/10 iters), loss = 9.24435
I0523 02:20:26.224974 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24435 (* 1 = 9.24435 loss)
I0523 02:20:26.284245 34682 sgd_solver.cpp:112] Iteration 33310, lr = 0.01
I0523 02:20:31.151527 34682 solver.cpp:239] Iteration 33320 (2.02991 iter/s, 4.92634s/10 iters), loss = 8.61304
I0523 02:20:31.151800 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61304 (* 1 = 8.61304 loss)
I0523 02:20:31.221560 34682 sgd_solver.cpp:112] Iteration 33320, lr = 0.01
I0523 02:20:36.063102 34682 solver.cpp:239] Iteration 33330 (2.03619 iter/s, 4.91114s/10 iters), loss = 7.73169
I0523 02:20:36.063144 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.73169 (* 1 = 7.73169 loss)
I0523 02:20:36.136780 34682 sgd_solver.cpp:112] Iteration 33330, lr = 0.01
I0523 02:20:41.902161 34682 solver.cpp:239] Iteration 33340 (1.71269 iter/s, 5.83876s/10 iters), loss = 8.01801
I0523 02:20:41.902248 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01801 (* 1 = 8.01801 loss)
I0523 02:20:41.977378 34682 sgd_solver.cpp:112] Iteration 33340, lr = 0.01
I0523 02:20:48.135984 34682 solver.cpp:239] Iteration 33350 (1.60423 iter/s, 6.23351s/10 iters), loss = 8.46865
I0523 02:20:48.136029 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46865 (* 1 = 8.46865 loss)
I0523 02:20:48.202914 34682 sgd_solver.cpp:112] Iteration 33350, lr = 0.01
I0523 02:20:51.067204 34682 solver.cpp:239] Iteration 33360 (3.41175 iter/s, 2.93104s/10 iters), loss = 8.80034
I0523 02:20:51.067252 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80034 (* 1 = 8.80034 loss)
I0523 02:20:51.893589 34682 sgd_solver.cpp:112] Iteration 33360, lr = 0.01
I0523 02:20:58.087734 34682 solver.cpp:239] Iteration 33370 (1.42446 iter/s, 7.0202s/10 iters), loss = 8.54594
I0523 02:20:58.087779 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54594 (* 1 = 8.54594 loss)
I0523 02:20:58.963232 34682 sgd_solver.cpp:112] Iteration 33370, lr = 0.01
I0523 02:21:03.650954 34682 solver.cpp:239] Iteration 33380 (1.79761 iter/s, 5.56293s/10 iters), loss = 9.08323
I0523 02:21:03.651100 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08323 (* 1 = 9.08323 loss)
I0523 02:21:03.717658 34682 sgd_solver.cpp:112] Iteration 33380, lr = 0.01
I0523 02:21:09.017611 34682 solver.cpp:239] Iteration 33390 (1.86348 iter/s, 5.3663s/10 iters), loss = 9.22239
I0523 02:21:09.017653 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22239 (* 1 = 9.22239 loss)
I0523 02:21:09.090966 34682 sgd_solver.cpp:112] Iteration 33390, lr = 0.01
I0523 02:21:12.522006 34682 solver.cpp:239] Iteration 33400 (2.85372 iter/s, 3.5042s/10 iters), loss = 8.55407
I0523 02:21:12.522054 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55407 (* 1 = 8.55407 loss)
I0523 02:21:12.604676 34682 sgd_solver.cpp:112] Iteration 33400, lr = 0.01
I0523 02:21:17.343283 34682 solver.cpp:239] Iteration 33410 (2.07424 iter/s, 4.82103s/10 iters), loss = 8.81973
I0523 02:21:17.343324 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81973 (* 1 = 8.81973 loss)
I0523 02:21:17.418476 34682 sgd_solver.cpp:112] Iteration 33410, lr = 0.01
I0523 02:21:21.475803 34682 solver.cpp:239] Iteration 33420 (2.41996 iter/s, 4.13231s/10 iters), loss = 7.4799
I0523 02:21:21.475847 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4799 (* 1 = 7.4799 loss)
I0523 02:21:21.549360 34682 sgd_solver.cpp:112] Iteration 33420, lr = 0.01
I0523 02:21:25.823737 34682 solver.cpp:239] Iteration 33430 (2.30006 iter/s, 4.34771s/10 iters), loss = 8.75857
I0523 02:21:25.823778 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75857 (* 1 = 8.75857 loss)
I0523 02:21:25.885371 34682 sgd_solver.cpp:112] Iteration 33430, lr = 0.01
I0523 02:21:28.411243 34682 solver.cpp:239] Iteration 33440 (3.86496 iter/s, 2.58735s/10 iters), loss = 8.66595
I0523 02:21:28.411291 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66595 (* 1 = 8.66595 loss)
I0523 02:21:28.475587 34682 sgd_solver.cpp:112] Iteration 33440, lr = 0.01
I0523 02:21:34.377275 34682 solver.cpp:239] Iteration 33450 (1.67624 iter/s, 5.96574s/10 iters), loss = 9.40526
I0523 02:21:34.377524 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40526 (* 1 = 9.40526 loss)
I0523 02:21:35.231819 34682 sgd_solver.cpp:112] Iteration 33450, lr = 0.01
I0523 02:21:39.257148 34682 solver.cpp:239] Iteration 33460 (2.04941 iter/s, 4.87945s/10 iters), loss = 8.96622
I0523 02:21:39.257195 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96622 (* 1 = 8.96622 loss)
I0523 02:21:40.089596 34682 sgd_solver.cpp:112] Iteration 33460, lr = 0.01
I0523 02:21:44.085830 34682 solver.cpp:239] Iteration 33470 (2.07106 iter/s, 4.82844s/10 iters), loss = 8.94231
I0523 02:21:44.085880 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94231 (* 1 = 8.94231 loss)
I0523 02:21:44.240284 34682 sgd_solver.cpp:112] Iteration 33470, lr = 0.01
I0523 02:21:47.679060 34682 solver.cpp:239] Iteration 33480 (2.78316 iter/s, 3.59303s/10 iters), loss = 8.91618
I0523 02:21:47.679101 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91618 (* 1 = 8.91618 loss)
I0523 02:21:47.747050 34682 sgd_solver.cpp:112] Iteration 33480, lr = 0.01
I0523 02:21:51.013550 34682 blocking_queue.cpp:49] Waiting for data
I0523 02:21:51.528232 34682 solver.cpp:239] Iteration 33490 (2.5981 iter/s, 3.84897s/10 iters), loss = 9.07256
I0523 02:21:51.528281 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07256 (* 1 = 9.07256 loss)
I0523 02:21:52.222537 34682 sgd_solver.cpp:112] Iteration 33490, lr = 0.01
I0523 02:21:55.419775 34682 solver.cpp:239] Iteration 33500 (2.56981 iter/s, 3.89134s/10 iters), loss = 8.69067
I0523 02:21:55.419817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69067 (* 1 = 8.69067 loss)
I0523 02:21:55.493391 34682 sgd_solver.cpp:112] Iteration 33500, lr = 0.01
I0523 02:22:01.131934 34682 solver.cpp:239] Iteration 33510 (1.75074 iter/s, 5.71187s/10 iters), loss = 8.54344
I0523 02:22:01.132007 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54344 (* 1 = 8.54344 loss)
I0523 02:22:01.202136 34682 sgd_solver.cpp:112] Iteration 33510, lr = 0.01
I0523 02:22:05.186121 34682 solver.cpp:239] Iteration 33520 (2.46673 iter/s, 4.05395s/10 iters), loss = 8.1875
I0523 02:22:05.186313 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1875 (* 1 = 8.1875 loss)
I0523 02:22:05.899525 34682 sgd_solver.cpp:112] Iteration 33520, lr = 0.01
I0523 02:22:10.100744 34682 solver.cpp:239] Iteration 33530 (2.0349 iter/s, 4.91426s/10 iters), loss = 9.23411
I0523 02:22:10.100786 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23411 (* 1 = 9.23411 loss)
I0523 02:22:10.385993 34682 sgd_solver.cpp:112] Iteration 33530, lr = 0.01
I0523 02:22:14.477634 34682 solver.cpp:239] Iteration 33540 (2.28484 iter/s, 4.37667s/10 iters), loss = 8.48959
I0523 02:22:14.477684 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48959 (* 1 = 8.48959 loss)
I0523 02:22:15.268620 34682 sgd_solver.cpp:112] Iteration 33540, lr = 0.01
I0523 02:22:20.610211 34682 solver.cpp:239] Iteration 33550 (1.63072 iter/s, 6.13228s/10 iters), loss = 8.41535
I0523 02:22:20.610272 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41535 (* 1 = 8.41535 loss)
I0523 02:22:20.678597 34682 sgd_solver.cpp:112] Iteration 33550, lr = 0.01
I0523 02:22:25.774161 34682 solver.cpp:239] Iteration 33560 (1.9366 iter/s, 5.16368s/10 iters), loss = 8.22142
I0523 02:22:25.774219 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22142 (* 1 = 8.22142 loss)
I0523 02:22:26.540784 34682 sgd_solver.cpp:112] Iteration 33560, lr = 0.01
I0523 02:22:30.194478 34682 solver.cpp:239] Iteration 33570 (2.26241 iter/s, 4.42007s/10 iters), loss = 8.5522
I0523 02:22:30.194528 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5522 (* 1 = 8.5522 loss)
I0523 02:22:31.094683 34682 sgd_solver.cpp:112] Iteration 33570, lr = 0.01
I0523 02:22:35.921720 34682 solver.cpp:239] Iteration 33580 (1.74613 iter/s, 5.72695s/10 iters), loss = 8.78237
I0523 02:22:35.922016 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78237 (* 1 = 8.78237 loss)
I0523 02:22:36.672166 34682 sgd_solver.cpp:112] Iteration 33580, lr = 0.01
I0523 02:22:40.473901 34682 solver.cpp:239] Iteration 33590 (2.19697 iter/s, 4.55173s/10 iters), loss = 9.89388
I0523 02:22:40.473953 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.89388 (* 1 = 9.89388 loss)
I0523 02:22:40.553144 34682 sgd_solver.cpp:112] Iteration 33590, lr = 0.01
I0523 02:22:45.285161 34682 solver.cpp:239] Iteration 33600 (2.07857 iter/s, 4.81101s/10 iters), loss = 9.10166
I0523 02:22:45.285212 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10166 (* 1 = 9.10166 loss)
I0523 02:22:45.345897 34682 sgd_solver.cpp:112] Iteration 33600, lr = 0.01
I0523 02:22:49.681046 34682 solver.cpp:239] Iteration 33610 (2.27498 iter/s, 4.39564s/10 iters), loss = 9.19763
I0523 02:22:49.681107 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19763 (* 1 = 9.19763 loss)
I0523 02:22:49.737675 34682 sgd_solver.cpp:112] Iteration 33610, lr = 0.01
I0523 02:22:54.265858 34682 solver.cpp:239] Iteration 33620 (2.18123 iter/s, 4.58456s/10 iters), loss = 8.61473
I0523 02:22:54.265924 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61473 (* 1 = 8.61473 loss)
I0523 02:22:55.093122 34682 sgd_solver.cpp:112] Iteration 33620, lr = 0.01
I0523 02:22:59.796159 34682 solver.cpp:239] Iteration 33630 (1.80831 iter/s, 5.53001s/10 iters), loss = 8.32138
I0523 02:22:59.796205 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32138 (* 1 = 8.32138 loss)
I0523 02:23:00.474062 34682 sgd_solver.cpp:112] Iteration 33630, lr = 0.01
I0523 02:23:04.694646 34682 solver.cpp:239] Iteration 33640 (2.04155 iter/s, 4.89823s/10 iters), loss = 8.8879
I0523 02:23:04.694741 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8879 (* 1 = 8.8879 loss)
I0523 02:23:05.506358 34682 sgd_solver.cpp:112] Iteration 33640, lr = 0.01
I0523 02:23:10.768851 34682 solver.cpp:239] Iteration 33650 (1.6464 iter/s, 6.07386s/10 iters), loss = 9.28086
I0523 02:23:10.769145 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28086 (* 1 = 9.28086 loss)
I0523 02:23:11.637900 34682 sgd_solver.cpp:112] Iteration 33650, lr = 0.01
I0523 02:23:14.289043 34682 solver.cpp:239] Iteration 33660 (2.8411 iter/s, 3.51976s/10 iters), loss = 9.62822
I0523 02:23:14.289103 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.62822 (* 1 = 9.62822 loss)
I0523 02:23:15.119000 34682 sgd_solver.cpp:112] Iteration 33660, lr = 0.01
I0523 02:23:19.581434 34682 solver.cpp:239] Iteration 33670 (1.8896 iter/s, 5.29212s/10 iters), loss = 8.82893
I0523 02:23:19.581477 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82893 (* 1 = 8.82893 loss)
I0523 02:23:19.653321 34682 sgd_solver.cpp:112] Iteration 33670, lr = 0.01
I0523 02:23:22.274829 34682 solver.cpp:239] Iteration 33680 (3.71301 iter/s, 2.69324s/10 iters), loss = 9.4943
I0523 02:23:22.274873 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4943 (* 1 = 9.4943 loss)
I0523 02:23:23.122798 34682 sgd_solver.cpp:112] Iteration 33680, lr = 0.01
I0523 02:23:28.660279 34682 solver.cpp:239] Iteration 33690 (1.56614 iter/s, 6.38514s/10 iters), loss = 9.08461
I0523 02:23:28.660327 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08461 (* 1 = 9.08461 loss)
I0523 02:23:28.723510 34682 sgd_solver.cpp:112] Iteration 33690, lr = 0.01
I0523 02:23:31.935228 34682 solver.cpp:239] Iteration 33700 (3.05366 iter/s, 3.27476s/10 iters), loss = 8.58104
I0523 02:23:31.935278 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58104 (* 1 = 8.58104 loss)
I0523 02:23:31.993042 34682 sgd_solver.cpp:112] Iteration 33700, lr = 0.01
I0523 02:23:36.076145 34682 solver.cpp:239] Iteration 33710 (2.41505 iter/s, 4.1407s/10 iters), loss = 8.64094
I0523 02:23:36.076203 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64094 (* 1 = 8.64094 loss)
I0523 02:23:36.842988 34682 sgd_solver.cpp:112] Iteration 33710, lr = 0.01
I0523 02:23:41.725607 34682 solver.cpp:239] Iteration 33720 (1.77017 iter/s, 5.64917s/10 iters), loss = 9.34538
I0523 02:23:41.725864 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34538 (* 1 = 9.34538 loss)
I0523 02:23:41.796689 34682 sgd_solver.cpp:112] Iteration 33720, lr = 0.01
I0523 02:23:46.060590 34682 solver.cpp:239] Iteration 33730 (2.30705 iter/s, 4.33455s/10 iters), loss = 8.97355
I0523 02:23:46.060647 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97355 (* 1 = 8.97355 loss)
I0523 02:23:46.794225 34682 sgd_solver.cpp:112] Iteration 33730, lr = 0.01
I0523 02:23:50.521688 34682 solver.cpp:239] Iteration 33740 (2.24172 iter/s, 4.46086s/10 iters), loss = 8.93563
I0523 02:23:50.521733 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93563 (* 1 = 8.93563 loss)
I0523 02:23:50.602329 34682 sgd_solver.cpp:112] Iteration 33740, lr = 0.01
I0523 02:23:54.258388 34682 solver.cpp:239] Iteration 33750 (2.6763 iter/s, 3.7365s/10 iters), loss = 8.82706
I0523 02:23:54.258448 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82706 (* 1 = 8.82706 loss)
I0523 02:23:54.921700 34682 sgd_solver.cpp:112] Iteration 33750, lr = 0.01
I0523 02:23:59.157112 34682 solver.cpp:239] Iteration 33760 (2.04146 iter/s, 4.89846s/10 iters), loss = 8.90767
I0523 02:23:59.157162 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90767 (* 1 = 8.90767 loss)
I0523 02:23:59.713881 34682 sgd_solver.cpp:112] Iteration 33760, lr = 0.01
I0523 02:24:04.440069 34682 solver.cpp:239] Iteration 33770 (1.89298 iter/s, 5.28268s/10 iters), loss = 8.7339
I0523 02:24:04.440134 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7339 (* 1 = 8.7339 loss)
I0523 02:24:05.274227 34682 sgd_solver.cpp:112] Iteration 33770, lr = 0.01
I0523 02:24:09.261659 34682 solver.cpp:239] Iteration 33780 (2.07412 iter/s, 4.82132s/10 iters), loss = 8.94444
I0523 02:24:09.261711 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94444 (* 1 = 8.94444 loss)
I0523 02:24:09.724803 34682 sgd_solver.cpp:112] Iteration 33780, lr = 0.01
I0523 02:24:14.356935 34682 solver.cpp:239] Iteration 33790 (1.9627 iter/s, 5.09502s/10 iters), loss = 8.97272
I0523 02:24:14.357031 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97272 (* 1 = 8.97272 loss)
I0523 02:24:14.429911 34682 sgd_solver.cpp:112] Iteration 33790, lr = 0.01
I0523 02:24:18.443308 34682 solver.cpp:239] Iteration 33800 (2.44732 iter/s, 4.08609s/10 iters), loss = 9.21006
I0523 02:24:18.443368 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21006 (* 1 = 9.21006 loss)
I0523 02:24:18.504174 34682 sgd_solver.cpp:112] Iteration 33800, lr = 0.01
I0523 02:24:24.789844 34682 solver.cpp:239] Iteration 33810 (1.57574 iter/s, 6.34623s/10 iters), loss = 9.95566
I0523 02:24:24.789882 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.95566 (* 1 = 9.95566 loss)
I0523 02:24:24.861410 34682 sgd_solver.cpp:112] Iteration 33810, lr = 0.01
I0523 02:24:26.941591 34682 solver.cpp:239] Iteration 33820 (4.64769 iter/s, 2.15161s/10 iters), loss = 9.33601
I0523 02:24:26.941642 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33601 (* 1 = 9.33601 loss)
I0523 02:24:27.760383 34682 sgd_solver.cpp:112] Iteration 33820, lr = 0.01
I0523 02:24:31.736922 34682 solver.cpp:239] Iteration 33830 (2.08547 iter/s, 4.79508s/10 iters), loss = 8.87211
I0523 02:24:31.736969 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87211 (* 1 = 8.87211 loss)
I0523 02:24:31.806090 34682 sgd_solver.cpp:112] Iteration 33830, lr = 0.01
I0523 02:24:36.696688 34682 solver.cpp:239] Iteration 33840 (2.01633 iter/s, 4.95951s/10 iters), loss = 9.67073
I0523 02:24:36.696738 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.67073 (* 1 = 9.67073 loss)
I0523 02:24:36.755427 34682 sgd_solver.cpp:112] Iteration 33840, lr = 0.01
I0523 02:24:43.011665 34682 solver.cpp:239] Iteration 33850 (1.58362 iter/s, 6.31465s/10 iters), loss = 9.27934
I0523 02:24:43.011723 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27934 (* 1 = 9.27934 loss)
I0523 02:24:43.718677 34682 sgd_solver.cpp:112] Iteration 33850, lr = 0.01
I0523 02:24:48.708302 34682 solver.cpp:239] Iteration 33860 (1.75551 iter/s, 5.69634s/10 iters), loss = 8.68044
I0523 02:24:48.708591 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68044 (* 1 = 8.68044 loss)
I0523 02:24:49.497395 34682 sgd_solver.cpp:112] Iteration 33860, lr = 0.01
I0523 02:24:53.457434 34682 solver.cpp:239] Iteration 33870 (2.10585 iter/s, 4.74868s/10 iters), loss = 8.85854
I0523 02:24:53.457489 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85854 (* 1 = 8.85854 loss)
I0523 02:24:54.308523 34682 sgd_solver.cpp:112] Iteration 33870, lr = 0.01
I0523 02:24:58.783957 34682 solver.cpp:239] Iteration 33880 (1.87749 iter/s, 5.32625s/10 iters), loss = 9.00947
I0523 02:24:58.784024 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00947 (* 1 = 9.00947 loss)
I0523 02:24:58.852941 34682 sgd_solver.cpp:112] Iteration 33880, lr = 0.01
I0523 02:25:02.406615 34682 solver.cpp:239] Iteration 33890 (2.76057 iter/s, 3.62244s/10 iters), loss = 9.19387
I0523 02:25:02.406688 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19387 (* 1 = 9.19387 loss)
I0523 02:25:03.198721 34682 sgd_solver.cpp:112] Iteration 33890, lr = 0.01
I0523 02:25:09.340342 34682 solver.cpp:239] Iteration 33900 (1.4423 iter/s, 6.93336s/10 iters), loss = 8.2245
I0523 02:25:09.340406 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2245 (* 1 = 8.2245 loss)
I0523 02:25:10.176609 34682 sgd_solver.cpp:112] Iteration 33900, lr = 0.01
I0523 02:25:14.952709 34682 solver.cpp:239] Iteration 33910 (1.78187 iter/s, 5.61208s/10 iters), loss = 8.62021
I0523 02:25:14.952757 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62021 (* 1 = 8.62021 loss)
I0523 02:25:15.479882 34682 sgd_solver.cpp:112] Iteration 33910, lr = 0.01
I0523 02:25:21.468230 34682 solver.cpp:239] Iteration 33920 (1.53487 iter/s, 6.51521s/10 iters), loss = 8.34325
I0523 02:25:21.468338 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34325 (* 1 = 8.34325 loss)
I0523 02:25:21.533946 34682 sgd_solver.cpp:112] Iteration 33920, lr = 0.01
I0523 02:25:24.292889 34682 solver.cpp:239] Iteration 33930 (3.54054 iter/s, 2.82443s/10 iters), loss = 9.14394
I0523 02:25:24.292923 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14394 (* 1 = 9.14394 loss)
I0523 02:25:24.359771 34682 sgd_solver.cpp:112] Iteration 33930, lr = 0.01
I0523 02:25:29.435868 34682 solver.cpp:239] Iteration 33940 (1.94449 iter/s, 5.14274s/10 iters), loss = 9.01998
I0523 02:25:29.435912 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01998 (* 1 = 9.01998 loss)
I0523 02:25:29.787751 34682 sgd_solver.cpp:112] Iteration 33940, lr = 0.01
I0523 02:25:34.547230 34682 solver.cpp:239] Iteration 33950 (1.95653 iter/s, 5.11109s/10 iters), loss = 9.31357
I0523 02:25:34.547281 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31357 (* 1 = 9.31357 loss)
I0523 02:25:34.611503 34682 sgd_solver.cpp:112] Iteration 33950, lr = 0.01
I0523 02:25:38.344910 34682 solver.cpp:239] Iteration 33960 (2.63333 iter/s, 3.79747s/10 iters), loss = 9.84945
I0523 02:25:38.344976 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.84945 (* 1 = 9.84945 loss)
I0523 02:25:38.419651 34682 sgd_solver.cpp:112] Iteration 33960, lr = 0.01
I0523 02:25:41.659301 34682 solver.cpp:239] Iteration 33970 (3.01733 iter/s, 3.31419s/10 iters), loss = 8.37322
I0523 02:25:41.659348 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37322 (* 1 = 8.37322 loss)
I0523 02:25:41.714854 34682 sgd_solver.cpp:112] Iteration 33970, lr = 0.01
I0523 02:25:47.953213 34682 solver.cpp:239] Iteration 33980 (1.58891 iter/s, 6.29361s/10 iters), loss = 9.29615
I0523 02:25:47.953265 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29615 (* 1 = 9.29615 loss)
I0523 02:25:48.016044 34682 sgd_solver.cpp:112] Iteration 33980, lr = 0.01
I0523 02:25:53.585398 34682 solver.cpp:239] Iteration 33990 (1.7756 iter/s, 5.6319s/10 iters), loss = 9.30486
I0523 02:25:53.585625 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30486 (* 1 = 9.30486 loss)
I0523 02:25:53.651091 34682 sgd_solver.cpp:112] Iteration 33990, lr = 0.01
I0523 02:25:59.599931 34682 solver.cpp:239] Iteration 34000 (1.66277 iter/s, 6.01406s/10 iters), loss = 8.68897
I0523 02:25:59.599997 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68897 (* 1 = 8.68897 loss)
I0523 02:26:00.381178 34682 sgd_solver.cpp:112] Iteration 34000, lr = 0.01
I0523 02:26:06.154175 34682 solver.cpp:239] Iteration 34010 (1.52581 iter/s, 6.55392s/10 iters), loss = 9.1385
I0523 02:26:06.154233 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1385 (* 1 = 9.1385 loss)
I0523 02:26:06.233705 34682 sgd_solver.cpp:112] Iteration 34010, lr = 0.01
I0523 02:26:11.012758 34682 solver.cpp:239] Iteration 34020 (2.05832 iter/s, 4.85833s/10 iters), loss = 8.73734
I0523 02:26:11.012807 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73734 (* 1 = 8.73734 loss)
I0523 02:26:11.781071 34682 sgd_solver.cpp:112] Iteration 34020, lr = 0.01
I0523 02:26:15.687083 34682 solver.cpp:239] Iteration 34030 (2.13946 iter/s, 4.67408s/10 iters), loss = 8.81755
I0523 02:26:15.687139 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81755 (* 1 = 8.81755 loss)
I0523 02:26:16.289408 34682 sgd_solver.cpp:112] Iteration 34030, lr = 0.01
I0523 02:26:21.178797 34682 solver.cpp:239] Iteration 34040 (1.82102 iter/s, 5.49144s/10 iters), loss = 7.82278
I0523 02:26:21.178840 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82278 (* 1 = 7.82278 loss)
I0523 02:26:21.256566 34682 sgd_solver.cpp:112] Iteration 34040, lr = 0.01
I0523 02:26:23.885926 34682 solver.cpp:239] Iteration 34050 (3.69418 iter/s, 2.70696s/10 iters), loss = 8.00899
I0523 02:26:23.886132 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00899 (* 1 = 8.00899 loss)
I0523 02:26:24.642099 34682 sgd_solver.cpp:112] Iteration 34050, lr = 0.01
I0523 02:26:29.506716 34682 solver.cpp:239] Iteration 34060 (1.77924 iter/s, 5.62037s/10 iters), loss = 8.50529
I0523 02:26:29.506760 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50529 (* 1 = 8.50529 loss)
I0523 02:26:30.346074 34682 sgd_solver.cpp:112] Iteration 34060, lr = 0.01
I0523 02:26:33.557425 34682 solver.cpp:239] Iteration 34070 (2.46884 iter/s, 4.05048s/10 iters), loss = 9.42017
I0523 02:26:33.557487 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42017 (* 1 = 9.42017 loss)
I0523 02:26:34.371320 34682 sgd_solver.cpp:112] Iteration 34070, lr = 0.01
I0523 02:26:38.450007 34682 solver.cpp:239] Iteration 34080 (2.04402 iter/s, 4.89232s/10 iters), loss = 8.5755
I0523 02:26:38.450052 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5755 (* 1 = 8.5755 loss)
I0523 02:26:38.518316 34682 sgd_solver.cpp:112] Iteration 34080, lr = 0.01
I0523 02:26:42.677515 34682 solver.cpp:239] Iteration 34090 (2.36559 iter/s, 4.22728s/10 iters), loss = 8.57383
I0523 02:26:42.677561 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57383 (* 1 = 8.57383 loss)
I0523 02:26:43.398545 34682 sgd_solver.cpp:112] Iteration 34090, lr = 0.01
I0523 02:26:46.072460 34682 solver.cpp:239] Iteration 34100 (2.94573 iter/s, 3.39475s/10 iters), loss = 8.36245
I0523 02:26:46.072505 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36245 (* 1 = 8.36245 loss)
I0523 02:26:46.151826 34682 sgd_solver.cpp:112] Iteration 34100, lr = 0.01
I0523 02:26:51.381626 34682 solver.cpp:239] Iteration 34110 (1.88363 iter/s, 5.3089s/10 iters), loss = 9.01135
I0523 02:26:51.381678 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01135 (* 1 = 9.01135 loss)
I0523 02:26:51.455413 34682 sgd_solver.cpp:112] Iteration 34110, lr = 0.01
I0523 02:26:55.487347 34682 solver.cpp:239] Iteration 34120 (2.43576 iter/s, 4.10549s/10 iters), loss = 8.80008
I0523 02:26:55.487578 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80008 (* 1 = 8.80008 loss)
I0523 02:26:55.549937 34682 sgd_solver.cpp:112] Iteration 34120, lr = 0.01
I0523 02:27:00.497339 34682 solver.cpp:239] Iteration 34130 (1.99617 iter/s, 5.00959s/10 iters), loss = 8.52975
I0523 02:27:00.497390 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52975 (* 1 = 8.52975 loss)
I0523 02:27:00.556679 34682 sgd_solver.cpp:112] Iteration 34130, lr = 0.01
I0523 02:27:04.289980 34682 solver.cpp:239] Iteration 34140 (2.63683 iter/s, 3.79243s/10 iters), loss = 9.72076
I0523 02:27:04.290020 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.72076 (* 1 = 9.72076 loss)
I0523 02:27:05.109459 34682 sgd_solver.cpp:112] Iteration 34140, lr = 0.01
I0523 02:27:08.260174 34682 solver.cpp:239] Iteration 34150 (2.5189 iter/s, 3.96998s/10 iters), loss = 8.95798
I0523 02:27:08.260231 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95798 (* 1 = 8.95798 loss)
I0523 02:27:09.010507 34682 sgd_solver.cpp:112] Iteration 34150, lr = 0.01
I0523 02:27:12.372268 34682 solver.cpp:239] Iteration 34160 (2.43199 iter/s, 4.11187s/10 iters), loss = 9.06955
I0523 02:27:12.372318 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06955 (* 1 = 9.06955 loss)
I0523 02:27:12.449844 34682 sgd_solver.cpp:112] Iteration 34160, lr = 0.01
I0523 02:27:16.929026 34682 solver.cpp:239] Iteration 34170 (2.19466 iter/s, 4.55652s/10 iters), loss = 8.18557
I0523 02:27:16.929076 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18557 (* 1 = 8.18557 loss)
I0523 02:27:17.725848 34682 sgd_solver.cpp:112] Iteration 34170, lr = 0.01
I0523 02:27:22.433889 34682 solver.cpp:239] Iteration 34180 (1.81667 iter/s, 5.50458s/10 iters), loss = 8.41382
I0523 02:27:22.433948 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41382 (* 1 = 8.41382 loss)
I0523 02:27:22.500291 34682 sgd_solver.cpp:112] Iteration 34180, lr = 0.01
I0523 02:27:28.919134 34682 solver.cpp:239] Iteration 34190 (1.54204 iter/s, 6.48493s/10 iters), loss = 8.60066
I0523 02:27:28.919348 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60066 (* 1 = 8.60066 loss)
I0523 02:27:29.760910 34682 sgd_solver.cpp:112] Iteration 34190, lr = 0.01
I0523 02:27:34.075664 34682 solver.cpp:239] Iteration 34200 (1.93945 iter/s, 5.15611s/10 iters), loss = 9.07347
I0523 02:27:34.075744 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07347 (* 1 = 9.07347 loss)
I0523 02:27:34.752897 34682 sgd_solver.cpp:112] Iteration 34200, lr = 0.01
I0523 02:27:41.295539 34682 solver.cpp:239] Iteration 34210 (1.38514 iter/s, 7.2195s/10 iters), loss = 8.85822
I0523 02:27:41.295603 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85822 (* 1 = 8.85822 loss)
I0523 02:27:41.974388 34682 sgd_solver.cpp:112] Iteration 34210, lr = 0.01
I0523 02:27:45.057520 34682 solver.cpp:239] Iteration 34220 (2.65833 iter/s, 3.76176s/10 iters), loss = 9.41571
I0523 02:27:45.057564 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.41571 (* 1 = 9.41571 loss)
I0523 02:27:45.884160 34682 sgd_solver.cpp:112] Iteration 34220, lr = 0.01
I0523 02:27:50.643829 34682 solver.cpp:239] Iteration 34230 (1.79018 iter/s, 5.58604s/10 iters), loss = 8.12776
I0523 02:27:50.643893 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12776 (* 1 = 8.12776 loss)
I0523 02:27:51.520998 34682 sgd_solver.cpp:112] Iteration 34230, lr = 0.01
I0523 02:27:56.241818 34682 solver.cpp:239] Iteration 34240 (1.78645 iter/s, 5.5977s/10 iters), loss = 9.45198
I0523 02:27:56.241861 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.45198 (* 1 = 9.45198 loss)
I0523 02:27:57.040786 34682 sgd_solver.cpp:112] Iteration 34240, lr = 0.01
I0523 02:27:59.665834 34682 solver.cpp:239] Iteration 34250 (2.92072 iter/s, 3.42382s/10 iters), loss = 9.43425
I0523 02:27:59.666012 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43425 (* 1 = 9.43425 loss)
I0523 02:27:59.753720 34682 sgd_solver.cpp:112] Iteration 34250, lr = 0.01
I0523 02:28:04.523648 34682 solver.cpp:239] Iteration 34260 (2.05869 iter/s, 4.85746s/10 iters), loss = 9.39827
I0523 02:28:04.523708 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.39827 (* 1 = 9.39827 loss)
I0523 02:28:04.732923 34682 sgd_solver.cpp:112] Iteration 34260, lr = 0.01
I0523 02:28:10.509784 34682 solver.cpp:239] Iteration 34270 (1.67061 iter/s, 5.98583s/10 iters), loss = 8.74909
I0523 02:28:10.509837 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74909 (* 1 = 8.74909 loss)
I0523 02:28:11.323510 34682 sgd_solver.cpp:112] Iteration 34270, lr = 0.01
I0523 02:28:16.543767 34682 solver.cpp:239] Iteration 34280 (1.65736 iter/s, 6.03368s/10 iters), loss = 8.57359
I0523 02:28:16.543818 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57359 (* 1 = 8.57359 loss)
I0523 02:28:16.599820 34682 sgd_solver.cpp:112] Iteration 34280, lr = 0.01
I0523 02:28:20.244796 34682 solver.cpp:239] Iteration 34290 (2.70211 iter/s, 3.70082s/10 iters), loss = 8.74891
I0523 02:28:20.244853 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74891 (* 1 = 8.74891 loss)
I0523 02:28:20.322458 34682 sgd_solver.cpp:112] Iteration 34290, lr = 0.01
I0523 02:28:24.324049 34682 solver.cpp:239] Iteration 34300 (2.45156 iter/s, 4.07903s/10 iters), loss = 8.51912
I0523 02:28:24.324105 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51912 (* 1 = 8.51912 loss)
I0523 02:28:24.731276 34682 sgd_solver.cpp:112] Iteration 34300, lr = 0.01
I0523 02:28:29.725215 34682 solver.cpp:239] Iteration 34310 (1.85155 iter/s, 5.40089s/10 iters), loss = 8.06108
I0523 02:28:29.725468 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06108 (* 1 = 8.06108 loss)
I0523 02:28:30.507094 34682 sgd_solver.cpp:112] Iteration 34310, lr = 0.01
I0523 02:28:35.756335 34682 solver.cpp:239] Iteration 34320 (1.65819 iter/s, 6.03066s/10 iters), loss = 8.93734
I0523 02:28:35.756399 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93734 (* 1 = 8.93734 loss)
I0523 02:28:36.497807 34682 sgd_solver.cpp:112] Iteration 34320, lr = 0.01
I0523 02:28:40.668421 34682 solver.cpp:239] Iteration 34330 (2.03591 iter/s, 4.91181s/10 iters), loss = 9.52378
I0523 02:28:40.668507 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.52378 (* 1 = 9.52378 loss)
I0523 02:28:40.715622 34682 sgd_solver.cpp:112] Iteration 34330, lr = 0.01
I0523 02:28:45.453436 34682 solver.cpp:239] Iteration 34340 (2.08998 iter/s, 4.78473s/10 iters), loss = 8.7335
I0523 02:28:45.453488 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7335 (* 1 = 8.7335 loss)
I0523 02:28:45.522966 34682 sgd_solver.cpp:112] Iteration 34340, lr = 0.01
I0523 02:28:51.036218 34682 solver.cpp:239] Iteration 34350 (1.79131 iter/s, 5.5825s/10 iters), loss = 8.62625
I0523 02:28:51.036267 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62625 (* 1 = 8.62625 loss)
I0523 02:28:51.896128 34682 sgd_solver.cpp:112] Iteration 34350, lr = 0.01
I0523 02:28:56.225762 34682 solver.cpp:239] Iteration 34360 (1.92705 iter/s, 5.18928s/10 iters), loss = 9.4865
I0523 02:28:56.225816 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4865 (* 1 = 9.4865 loss)
I0523 02:28:57.080349 34682 sgd_solver.cpp:112] Iteration 34360, lr = 0.01
I0523 02:29:01.502537 34682 solver.cpp:239] Iteration 34370 (1.89519 iter/s, 5.2765s/10 iters), loss = 9.02724
I0523 02:29:01.502843 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02724 (* 1 = 9.02724 loss)
I0523 02:29:01.568755 34682 sgd_solver.cpp:112] Iteration 34370, lr = 0.01
I0523 02:29:06.501145 34682 solver.cpp:239] Iteration 34380 (2.00075 iter/s, 4.99813s/10 iters), loss = 9.10429
I0523 02:29:06.501209 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10429 (* 1 = 9.10429 loss)
I0523 02:29:06.588930 34682 sgd_solver.cpp:112] Iteration 34380, lr = 0.01
I0523 02:29:09.730067 34682 solver.cpp:239] Iteration 34390 (3.0972 iter/s, 3.22872s/10 iters), loss = 8.83926
I0523 02:29:09.730116 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83926 (* 1 = 8.83926 loss)
I0523 02:29:09.810747 34682 sgd_solver.cpp:112] Iteration 34390, lr = 0.01
I0523 02:29:15.522531 34682 solver.cpp:239] Iteration 34400 (1.72647 iter/s, 5.79218s/10 iters), loss = 8.52448
I0523 02:29:15.522578 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52448 (* 1 = 8.52448 loss)
I0523 02:29:15.586486 34682 sgd_solver.cpp:112] Iteration 34400, lr = 0.01
I0523 02:29:18.740639 34682 solver.cpp:239] Iteration 34410 (3.10759 iter/s, 3.21793s/10 iters), loss = 8.66973
I0523 02:29:18.740684 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66973 (* 1 = 8.66973 loss)
I0523 02:29:18.820024 34682 sgd_solver.cpp:112] Iteration 34410, lr = 0.01
I0523 02:29:23.411335 34682 solver.cpp:239] Iteration 34420 (2.14112 iter/s, 4.67046s/10 iters), loss = 8.61351
I0523 02:29:23.411388 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61351 (* 1 = 8.61351 loss)
I0523 02:29:23.480346 34682 sgd_solver.cpp:112] Iteration 34420, lr = 0.01
I0523 02:29:28.600150 34682 solver.cpp:239] Iteration 34430 (1.92732 iter/s, 5.18854s/10 iters), loss = 8.04573
I0523 02:29:28.600210 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04573 (* 1 = 8.04573 loss)
I0523 02:29:29.464727 34682 sgd_solver.cpp:112] Iteration 34430, lr = 0.01
I0523 02:29:34.585634 34682 solver.cpp:239] Iteration 34440 (1.67079 iter/s, 5.98518s/10 iters), loss = 9.13667
I0523 02:29:34.585830 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13667 (* 1 = 9.13667 loss)
I0523 02:29:34.662102 34682 sgd_solver.cpp:112] Iteration 34440, lr = 0.01
I0523 02:29:40.785311 34682 solver.cpp:239] Iteration 34450 (1.6131 iter/s, 6.19926s/10 iters), loss = 8.15296
I0523 02:29:40.785392 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15296 (* 1 = 8.15296 loss)
I0523 02:29:41.625543 34682 sgd_solver.cpp:112] Iteration 34450, lr = 0.01
I0523 02:29:47.223603 34682 solver.cpp:239] Iteration 34460 (1.55329 iter/s, 6.43796s/10 iters), loss = 8.0642
I0523 02:29:47.223670 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0642 (* 1 = 8.0642 loss)
I0523 02:29:48.018867 34682 sgd_solver.cpp:112] Iteration 34460, lr = 0.01
I0523 02:29:51.907970 34682 solver.cpp:239] Iteration 34470 (2.13488 iter/s, 4.68411s/10 iters), loss = 9.63088
I0523 02:29:51.908025 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.63088 (* 1 = 9.63088 loss)
I0523 02:29:51.977672 34682 sgd_solver.cpp:112] Iteration 34470, lr = 0.01
I0523 02:29:56.840138 34682 solver.cpp:239] Iteration 34480 (2.02761 iter/s, 4.93191s/10 iters), loss = 8.30516
I0523 02:29:56.840201 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30516 (* 1 = 8.30516 loss)
I0523 02:29:56.918582 34682 sgd_solver.cpp:112] Iteration 34480, lr = 0.01
I0523 02:30:04.003259 34682 solver.cpp:239] Iteration 34490 (1.3961 iter/s, 7.16279s/10 iters), loss = 9.80138
I0523 02:30:04.003301 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.80138 (* 1 = 9.80138 loss)
I0523 02:30:04.067781 34682 sgd_solver.cpp:112] Iteration 34490, lr = 0.01
I0523 02:30:08.197048 34682 solver.cpp:239] Iteration 34500 (2.3846 iter/s, 4.19357s/10 iters), loss = 8.47556
I0523 02:30:08.197299 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47556 (* 1 = 8.47556 loss)
I0523 02:30:08.272943 34682 sgd_solver.cpp:112] Iteration 34500, lr = 0.01
I0523 02:30:11.590981 34682 solver.cpp:239] Iteration 34510 (2.94674 iter/s, 3.39357s/10 iters), loss = 8.20176
I0523 02:30:11.591028 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20176 (* 1 = 8.20176 loss)
I0523 02:30:12.304041 34682 sgd_solver.cpp:112] Iteration 34510, lr = 0.01
I0523 02:30:16.893453 34682 solver.cpp:239] Iteration 34520 (1.88601 iter/s, 5.30221s/10 iters), loss = 8.19897
I0523 02:30:16.893503 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19897 (* 1 = 8.19897 loss)
I0523 02:30:16.950647 34682 sgd_solver.cpp:112] Iteration 34520, lr = 0.01
I0523 02:30:22.002342 34682 solver.cpp:239] Iteration 34530 (1.95747 iter/s, 5.10863s/10 iters), loss = 9.54168
I0523 02:30:22.002391 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.54168 (* 1 = 9.54168 loss)
I0523 02:30:22.757508 34682 sgd_solver.cpp:112] Iteration 34530, lr = 0.01
I0523 02:30:26.371578 34682 solver.cpp:239] Iteration 34540 (2.28885 iter/s, 4.36901s/10 iters), loss = 8.54857
I0523 02:30:26.371620 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54857 (* 1 = 8.54857 loss)
I0523 02:30:26.434013 34682 sgd_solver.cpp:112] Iteration 34540, lr = 0.01
I0523 02:30:30.653039 34682 solver.cpp:239] Iteration 34550 (2.33577 iter/s, 4.28124s/10 iters), loss = 7.95886
I0523 02:30:30.653089 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95886 (* 1 = 7.95886 loss)
I0523 02:30:30.723490 34682 sgd_solver.cpp:112] Iteration 34550, lr = 0.01
I0523 02:30:35.588897 34682 solver.cpp:239] Iteration 34560 (2.02609 iter/s, 4.93561s/10 iters), loss = 8.10667
I0523 02:30:35.588939 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10667 (* 1 = 8.10667 loss)
I0523 02:30:35.661522 34682 sgd_solver.cpp:112] Iteration 34560, lr = 0.01
I0523 02:30:37.469262 34682 solver.cpp:239] Iteration 34570 (5.3185 iter/s, 1.88023s/10 iters), loss = 8.17748
I0523 02:30:37.469314 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17748 (* 1 = 8.17748 loss)
I0523 02:30:37.530488 34682 sgd_solver.cpp:112] Iteration 34570, lr = 0.01
I0523 02:30:40.746222 34682 solver.cpp:239] Iteration 34580 (3.05178 iter/s, 3.27677s/10 iters), loss = 8.58133
I0523 02:30:40.746445 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58133 (* 1 = 8.58133 loss)
I0523 02:30:41.490057 34682 sgd_solver.cpp:112] Iteration 34580, lr = 0.01
I0523 02:30:45.831498 34682 solver.cpp:239] Iteration 34590 (1.96662 iter/s, 5.08487s/10 iters), loss = 9.18757
I0523 02:30:45.831557 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18757 (* 1 = 9.18757 loss)
I0523 02:30:45.906376 34682 sgd_solver.cpp:112] Iteration 34590, lr = 0.01
I0523 02:30:51.926038 34682 solver.cpp:239] Iteration 34600 (1.6409 iter/s, 6.09423s/10 iters), loss = 7.67373
I0523 02:30:51.926095 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.67373 (* 1 = 7.67373 loss)
I0523 02:30:51.983552 34682 sgd_solver.cpp:112] Iteration 34600, lr = 0.01
I0523 02:30:56.530769 34682 solver.cpp:239] Iteration 34610 (2.1718 iter/s, 4.60449s/10 iters), loss = 8.12871
I0523 02:30:56.530819 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12871 (* 1 = 8.12871 loss)
I0523 02:30:56.600442 34682 sgd_solver.cpp:112] Iteration 34610, lr = 0.01
I0523 02:31:01.605906 34682 solver.cpp:239] Iteration 34620 (1.97049 iter/s, 5.07487s/10 iters), loss = 9.1055
I0523 02:31:01.605955 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1055 (* 1 = 9.1055 loss)
I0523 02:31:01.669898 34682 sgd_solver.cpp:112] Iteration 34620, lr = 0.01
I0523 02:31:04.978420 34682 solver.cpp:239] Iteration 34630 (2.96531 iter/s, 3.37233s/10 iters), loss = 8.41233
I0523 02:31:04.978461 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41233 (* 1 = 8.41233 loss)
I0523 02:31:05.055852 34682 sgd_solver.cpp:112] Iteration 34630, lr = 0.01
I0523 02:31:08.892858 34682 solver.cpp:239] Iteration 34640 (2.55478 iter/s, 3.91423s/10 iters), loss = 8.3624
I0523 02:31:08.892922 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3624 (* 1 = 8.3624 loss)
I0523 02:31:09.661347 34682 sgd_solver.cpp:112] Iteration 34640, lr = 0.01
I0523 02:31:12.259366 34682 solver.cpp:239] Iteration 34650 (2.97062 iter/s, 3.3663s/10 iters), loss = 9.17422
I0523 02:31:12.259539 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17422 (* 1 = 9.17422 loss)
I0523 02:31:12.325480 34682 sgd_solver.cpp:112] Iteration 34650, lr = 0.01
I0523 02:31:17.076195 34682 solver.cpp:239] Iteration 34660 (2.07621 iter/s, 4.81647s/10 iters), loss = 9.01305
I0523 02:31:17.076264 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01305 (* 1 = 9.01305 loss)
I0523 02:31:17.940976 34682 sgd_solver.cpp:112] Iteration 34660, lr = 0.01
I0523 02:31:23.109297 34682 solver.cpp:239] Iteration 34670 (1.65761 iter/s, 6.03279s/10 iters), loss = 8.20248
I0523 02:31:23.109362 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20248 (* 1 = 8.20248 loss)
I0523 02:31:23.860606 34682 sgd_solver.cpp:112] Iteration 34670, lr = 0.01
I0523 02:31:29.869252 34682 solver.cpp:239] Iteration 34680 (1.47937 iter/s, 6.75961s/10 iters), loss = 9.14694
I0523 02:31:29.869298 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14694 (* 1 = 9.14694 loss)
I0523 02:31:29.940094 34682 sgd_solver.cpp:112] Iteration 34680, lr = 0.01
I0523 02:31:33.299245 34682 solver.cpp:239] Iteration 34690 (2.91562 iter/s, 3.4298s/10 iters), loss = 8.39998
I0523 02:31:33.299288 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39998 (* 1 = 8.39998 loss)
I0523 02:31:33.370426 34682 sgd_solver.cpp:112] Iteration 34690, lr = 0.01
I0523 02:31:38.791734 34682 solver.cpp:239] Iteration 34700 (1.82076 iter/s, 5.49222s/10 iters), loss = 8.819
I0523 02:31:38.791775 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.819 (* 1 = 8.819 loss)
I0523 02:31:38.869472 34682 sgd_solver.cpp:112] Iteration 34700, lr = 0.01
I0523 02:31:44.351420 34682 solver.cpp:239] Iteration 34710 (1.79875 iter/s, 5.5594s/10 iters), loss = 9.24655
I0523 02:31:44.351581 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24655 (* 1 = 9.24655 loss)
I0523 02:31:45.213310 34682 sgd_solver.cpp:112] Iteration 34710, lr = 0.01
I0523 02:31:50.668021 34682 solver.cpp:239] Iteration 34720 (1.58324 iter/s, 6.31617s/10 iters), loss = 8.92332
I0523 02:31:50.668090 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92332 (* 1 = 8.92332 loss)
I0523 02:31:51.495579 34682 sgd_solver.cpp:112] Iteration 34720, lr = 0.01
I0523 02:31:57.158268 34682 solver.cpp:239] Iteration 34730 (1.54085 iter/s, 6.48992s/10 iters), loss = 8.26646
I0523 02:31:57.158318 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26646 (* 1 = 8.26646 loss)
I0523 02:31:57.221503 34682 sgd_solver.cpp:112] Iteration 34730, lr = 0.01
I0523 02:32:01.309556 34682 solver.cpp:239] Iteration 34740 (2.40903 iter/s, 4.15106s/10 iters), loss = 8.40126
I0523 02:32:01.309622 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40126 (* 1 = 8.40126 loss)
I0523 02:32:01.879321 34682 sgd_solver.cpp:112] Iteration 34740, lr = 0.01
I0523 02:32:05.989850 34682 solver.cpp:239] Iteration 34750 (2.13673 iter/s, 4.68004s/10 iters), loss = 8.54424
I0523 02:32:05.989908 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54424 (* 1 = 8.54424 loss)
I0523 02:32:06.714032 34682 sgd_solver.cpp:112] Iteration 34750, lr = 0.01
I0523 02:32:09.259840 34682 solver.cpp:239] Iteration 34760 (3.06241 iter/s, 3.2654s/10 iters), loss = 9.59862
I0523 02:32:09.259883 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59862 (* 1 = 9.59862 loss)
I0523 02:32:09.320564 34682 sgd_solver.cpp:112] Iteration 34760, lr = 0.01
I0523 02:32:13.287407 34682 solver.cpp:239] Iteration 34770 (2.48302 iter/s, 4.02735s/10 iters), loss = 9.27282
I0523 02:32:13.287464 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27282 (* 1 = 9.27282 loss)
I0523 02:32:13.343446 34682 sgd_solver.cpp:112] Iteration 34770, lr = 0.01
I0523 02:32:18.169368 34682 solver.cpp:239] Iteration 34780 (2.04846 iter/s, 4.88171s/10 iters), loss = 9.40952
I0523 02:32:18.169641 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40952 (* 1 = 9.40952 loss)
I0523 02:32:18.224895 34682 sgd_solver.cpp:112] Iteration 34780, lr = 0.01
I0523 02:32:22.802769 34682 solver.cpp:239] Iteration 34790 (2.15844 iter/s, 4.63298s/10 iters), loss = 9.61332
I0523 02:32:22.802824 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.61332 (* 1 = 9.61332 loss)
I0523 02:32:22.881592 34682 sgd_solver.cpp:112] Iteration 34790, lr = 0.01
I0523 02:32:28.245275 34682 solver.cpp:239] Iteration 34800 (1.83884 iter/s, 5.4382s/10 iters), loss = 8.11186
I0523 02:32:28.245317 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11186 (* 1 = 8.11186 loss)
I0523 02:32:28.305344 34682 sgd_solver.cpp:112] Iteration 34800, lr = 0.01
I0523 02:32:31.222064 34682 solver.cpp:239] Iteration 34810 (3.35954 iter/s, 2.97659s/10 iters), loss = 8.37788
I0523 02:32:31.222120 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37788 (* 1 = 8.37788 loss)
I0523 02:32:31.283390 34682 sgd_solver.cpp:112] Iteration 34810, lr = 0.01
I0523 02:32:36.728866 34682 solver.cpp:239] Iteration 34820 (1.81606 iter/s, 5.50642s/10 iters), loss = 9.19989
I0523 02:32:36.728971 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19989 (* 1 = 9.19989 loss)
I0523 02:32:37.558781 34682 sgd_solver.cpp:112] Iteration 34820, lr = 0.01
I0523 02:32:42.122897 34682 solver.cpp:239] Iteration 34830 (1.85401 iter/s, 5.39371s/10 iters), loss = 8.23674
I0523 02:32:42.122941 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23674 (* 1 = 8.23674 loss)
I0523 02:32:42.194504 34682 sgd_solver.cpp:112] Iteration 34830, lr = 0.01
I0523 02:32:47.941573 34682 solver.cpp:239] Iteration 34840 (1.71869 iter/s, 5.81838s/10 iters), loss = 8.88509
I0523 02:32:47.941632 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88509 (* 1 = 8.88509 loss)
I0523 02:32:48.781414 34682 sgd_solver.cpp:112] Iteration 34840, lr = 0.01
I0523 02:32:55.324417 34682 solver.cpp:239] Iteration 34850 (1.35456 iter/s, 7.38249s/10 iters), loss = 8.58091
I0523 02:32:55.324470 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58091 (* 1 = 8.58091 loss)
I0523 02:32:55.528933 34682 sgd_solver.cpp:112] Iteration 34850, lr = 0.01
I0523 02:33:00.603849 34682 solver.cpp:239] Iteration 34860 (1.89424 iter/s, 5.27916s/10 iters), loss = 9.38667
I0523 02:33:00.603904 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.38667 (* 1 = 9.38667 loss)
I0523 02:33:01.406781 34682 sgd_solver.cpp:112] Iteration 34860, lr = 0.01
I0523 02:33:06.336715 34682 solver.cpp:239] Iteration 34870 (1.74442 iter/s, 5.73258s/10 iters), loss = 9.6519
I0523 02:33:06.336769 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.6519 (* 1 = 9.6519 loss)
I0523 02:33:07.094591 34682 sgd_solver.cpp:112] Iteration 34870, lr = 0.01
I0523 02:33:11.596220 34682 solver.cpp:239] Iteration 34880 (1.90142 iter/s, 5.25923s/10 iters), loss = 8.89135
I0523 02:33:11.596266 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89135 (* 1 = 8.89135 loss)
I0523 02:33:11.683508 34682 sgd_solver.cpp:112] Iteration 34880, lr = 0.01
I0523 02:33:15.563737 34682 solver.cpp:239] Iteration 34890 (2.5206 iter/s, 3.96731s/10 iters), loss = 9.22534
I0523 02:33:15.563781 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22534 (* 1 = 9.22534 loss)
I0523 02:33:16.423544 34682 sgd_solver.cpp:112] Iteration 34890, lr = 0.01
I0523 02:33:19.695840 34682 solver.cpp:239] Iteration 34900 (2.42021 iter/s, 4.13186s/10 iters), loss = 8.64935
I0523 02:33:19.696015 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64935 (* 1 = 8.64935 loss)
I0523 02:33:20.556268 34682 sgd_solver.cpp:112] Iteration 34900, lr = 0.01
I0523 02:33:26.571954 34682 solver.cpp:239] Iteration 34910 (1.4544 iter/s, 6.87567s/10 iters), loss = 8.87405
I0523 02:33:26.572005 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87405 (* 1 = 8.87405 loss)
I0523 02:33:26.637814 34682 sgd_solver.cpp:112] Iteration 34910, lr = 0.01
I0523 02:33:32.468487 34682 solver.cpp:239] Iteration 34920 (1.696 iter/s, 5.89624s/10 iters), loss = 8.51595
I0523 02:33:32.468538 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51595 (* 1 = 8.51595 loss)
I0523 02:33:33.287621 34682 sgd_solver.cpp:112] Iteration 34920, lr = 0.01
I0523 02:33:38.001850 34682 solver.cpp:239] Iteration 34930 (1.80731 iter/s, 5.53308s/10 iters), loss = 9.61816
I0523 02:33:38.001899 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.61816 (* 1 = 9.61816 loss)
I0523 02:33:38.070884 34682 sgd_solver.cpp:112] Iteration 34930, lr = 0.01
I0523 02:33:42.463732 34682 solver.cpp:239] Iteration 34940 (2.24132 iter/s, 4.46165s/10 iters), loss = 8.72424
I0523 02:33:42.463776 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72424 (* 1 = 8.72424 loss)
I0523 02:33:43.281278 34682 sgd_solver.cpp:112] Iteration 34940, lr = 0.01
I0523 02:33:48.208782 34682 solver.cpp:239] Iteration 34950 (1.74071 iter/s, 5.74477s/10 iters), loss = 8.71462
I0523 02:33:48.208837 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71462 (* 1 = 8.71462 loss)
I0523 02:33:48.281631 34682 sgd_solver.cpp:112] Iteration 34950, lr = 0.01
I0523 02:33:52.287943 34682 solver.cpp:239] Iteration 34960 (2.45161 iter/s, 4.07894s/10 iters), loss = 8.49209
I0523 02:33:52.288220 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49209 (* 1 = 8.49209 loss)
I0523 02:33:52.357547 34682 sgd_solver.cpp:112] Iteration 34960, lr = 0.01
I0523 02:33:57.865124 34682 solver.cpp:239] Iteration 34970 (1.79318 iter/s, 5.57669s/10 iters), loss = 9.61587
I0523 02:33:57.865177 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.61587 (* 1 = 9.61587 loss)
I0523 02:33:58.402781 34682 sgd_solver.cpp:112] Iteration 34970, lr = 0.01
I0523 02:34:02.409672 34682 solver.cpp:239] Iteration 34980 (2.20057 iter/s, 4.54428s/10 iters), loss = 8.3497
I0523 02:34:02.409716 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3497 (* 1 = 8.3497 loss)
I0523 02:34:02.488710 34682 sgd_solver.cpp:112] Iteration 34980, lr = 0.01
I0523 02:34:09.885509 34682 solver.cpp:239] Iteration 34990 (1.3377 iter/s, 7.4755s/10 iters), loss = 8.609
I0523 02:34:09.885548 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.609 (* 1 = 8.609 loss)
I0523 02:34:09.964637 34682 sgd_solver.cpp:112] Iteration 34990, lr = 0.01
I0523 02:34:16.003922 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_35000.caffemodel
I0523 02:34:17.188885 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_35000.solverstate
I0523 02:34:17.434288 34682 solver.cpp:239] Iteration 35000 (1.32478 iter/s, 7.54844s/10 iters), loss = 7.9534
I0523 02:34:17.434334 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9534 (* 1 = 7.9534 loss)
I0523 02:34:18.242599 34682 sgd_solver.cpp:112] Iteration 35000, lr = 0.01
I0523 02:34:21.853252 34682 solver.cpp:239] Iteration 35010 (2.26309 iter/s, 4.41874s/10 iters), loss = 8.56231
I0523 02:34:21.853309 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56231 (* 1 = 8.56231 loss)
I0523 02:34:22.112255 34682 sgd_solver.cpp:112] Iteration 35010, lr = 0.01
I0523 02:34:25.540026 34682 solver.cpp:239] Iteration 35020 (2.71256 iter/s, 3.68656s/10 iters), loss = 8.7469
I0523 02:34:25.540261 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7469 (* 1 = 8.7469 loss)
I0523 02:34:26.424358 34682 sgd_solver.cpp:112] Iteration 35020, lr = 0.01
I0523 02:34:32.022110 34682 solver.cpp:239] Iteration 35030 (1.54283 iter/s, 6.48161s/10 iters), loss = 8.80231
I0523 02:34:32.022171 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80231 (* 1 = 8.80231 loss)
I0523 02:34:32.097692 34682 sgd_solver.cpp:112] Iteration 35030, lr = 0.01
I0523 02:34:35.461980 34682 solver.cpp:239] Iteration 35040 (2.90726 iter/s, 3.43967s/10 iters), loss = 8.91742
I0523 02:34:35.462023 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91742 (* 1 = 8.91742 loss)
I0523 02:34:35.527799 34682 sgd_solver.cpp:112] Iteration 35040, lr = 0.01
I0523 02:34:39.003839 34682 solver.cpp:239] Iteration 35050 (2.82353 iter/s, 3.54167s/10 iters), loss = 8.53743
I0523 02:34:39.003891 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53743 (* 1 = 8.53743 loss)
I0523 02:34:39.066875 34682 sgd_solver.cpp:112] Iteration 35050, lr = 0.01
I0523 02:34:44.029456 34682 solver.cpp:239] Iteration 35060 (1.98991 iter/s, 5.02535s/10 iters), loss = 7.57958
I0523 02:34:44.029508 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.57958 (* 1 = 7.57958 loss)
I0523 02:34:44.106359 34682 sgd_solver.cpp:112] Iteration 35060, lr = 0.01
I0523 02:34:51.324496 34682 solver.cpp:239] Iteration 35070 (1.37086 iter/s, 7.2947s/10 iters), loss = 8.51112
I0523 02:34:51.324554 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51112 (* 1 = 8.51112 loss)
I0523 02:34:51.386091 34682 sgd_solver.cpp:112] Iteration 35070, lr = 0.01
I0523 02:34:55.561822 34682 solver.cpp:239] Iteration 35080 (2.36011 iter/s, 4.2371s/10 iters), loss = 8.36058
I0523 02:34:55.562000 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36058 (* 1 = 8.36058 loss)
I0523 02:34:55.627665 34682 sgd_solver.cpp:112] Iteration 35080, lr = 0.01
I0523 02:35:01.804632 34682 solver.cpp:239] Iteration 35090 (1.60195 iter/s, 6.24237s/10 iters), loss = 9.34259
I0523 02:35:01.804688 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34259 (* 1 = 9.34259 loss)
I0523 02:35:02.589728 34682 sgd_solver.cpp:112] Iteration 35090, lr = 0.01
I0523 02:35:06.141986 34682 solver.cpp:239] Iteration 35100 (2.30568 iter/s, 4.33712s/10 iters), loss = 9.1214
I0523 02:35:06.142041 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1214 (* 1 = 9.1214 loss)
I0523 02:35:06.957106 34682 sgd_solver.cpp:112] Iteration 35100, lr = 0.01
I0523 02:35:10.456862 34682 solver.cpp:239] Iteration 35110 (2.32007 iter/s, 4.31022s/10 iters), loss = 9.01016
I0523 02:35:10.456921 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01016 (* 1 = 9.01016 loss)
I0523 02:35:11.061911 34682 sgd_solver.cpp:112] Iteration 35110, lr = 0.01
I0523 02:35:16.492401 34682 solver.cpp:239] Iteration 35120 (1.65693 iter/s, 6.03524s/10 iters), loss = 9.65979
I0523 02:35:16.492441 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.65979 (* 1 = 9.65979 loss)
I0523 02:35:16.567098 34682 sgd_solver.cpp:112] Iteration 35120, lr = 0.01
I0523 02:35:19.756858 34682 solver.cpp:239] Iteration 35130 (3.06347 iter/s, 3.26428s/10 iters), loss = 8.43428
I0523 02:35:19.756904 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43428 (* 1 = 8.43428 loss)
I0523 02:35:20.565568 34682 sgd_solver.cpp:112] Iteration 35130, lr = 0.01
I0523 02:35:24.998536 34682 solver.cpp:239] Iteration 35140 (1.90788 iter/s, 5.24142s/10 iters), loss = 8.71848
I0523 02:35:24.998586 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71848 (* 1 = 8.71848 loss)
I0523 02:35:25.834848 34682 sgd_solver.cpp:112] Iteration 35140, lr = 0.01
I0523 02:35:28.190667 34682 solver.cpp:239] Iteration 35150 (3.13289 iter/s, 3.19194s/10 iters), loss = 7.80034
I0523 02:35:28.190744 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80034 (* 1 = 7.80034 loss)
I0523 02:35:28.270629 34682 sgd_solver.cpp:112] Iteration 35150, lr = 0.01
I0523 02:35:30.989243 34682 solver.cpp:239] Iteration 35160 (3.57349 iter/s, 2.79838s/10 iters), loss = 9.68028
I0523 02:35:30.989286 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.68028 (* 1 = 9.68028 loss)
I0523 02:35:31.062748 34682 sgd_solver.cpp:112] Iteration 35160, lr = 0.01
I0523 02:35:34.140053 34682 solver.cpp:239] Iteration 35170 (3.17398 iter/s, 3.15062s/10 iters), loss = 9.61314
I0523 02:35:34.140099 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.61314 (* 1 = 9.61314 loss)
I0523 02:35:34.204058 34682 sgd_solver.cpp:112] Iteration 35170, lr = 0.01
I0523 02:35:40.007755 34682 solver.cpp:239] Iteration 35180 (1.70433 iter/s, 5.86742s/10 iters), loss = 8.80432
I0523 02:35:40.007802 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80432 (* 1 = 8.80432 loss)
I0523 02:35:40.063284 34682 sgd_solver.cpp:112] Iteration 35180, lr = 0.01
I0523 02:35:46.791960 34682 solver.cpp:239] Iteration 35190 (1.47408 iter/s, 6.78388s/10 iters), loss = 8.67278
I0523 02:35:46.792006 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67278 (* 1 = 8.67278 loss)
I0523 02:35:46.867882 34682 sgd_solver.cpp:112] Iteration 35190, lr = 0.01
I0523 02:35:50.929735 34682 solver.cpp:239] Iteration 35200 (2.41689 iter/s, 4.13755s/10 iters), loss = 8.96513
I0523 02:35:50.929780 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96513 (* 1 = 8.96513 loss)
I0523 02:35:50.994413 34682 sgd_solver.cpp:112] Iteration 35200, lr = 0.01
I0523 02:35:56.687543 34682 solver.cpp:239] Iteration 35210 (1.73686 iter/s, 5.75753s/10 iters), loss = 8.80797
I0523 02:35:56.687680 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80797 (* 1 = 8.80797 loss)
I0523 02:35:56.735139 34682 sgd_solver.cpp:112] Iteration 35210, lr = 0.01
I0523 02:35:59.172456 34682 solver.cpp:239] Iteration 35220 (4.02468 iter/s, 2.48467s/10 iters), loss = 8.50048
I0523 02:35:59.172508 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50048 (* 1 = 8.50048 loss)
I0523 02:35:59.694182 34682 sgd_solver.cpp:112] Iteration 35220, lr = 0.01
I0523 02:36:02.373438 34682 solver.cpp:239] Iteration 35230 (3.12422 iter/s, 3.20079s/10 iters), loss = 9.82548
I0523 02:36:02.373486 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.82548 (* 1 = 9.82548 loss)
I0523 02:36:02.437513 34682 sgd_solver.cpp:112] Iteration 35230, lr = 0.01
I0523 02:36:07.719362 34682 solver.cpp:239] Iteration 35240 (1.87068 iter/s, 5.34565s/10 iters), loss = 9.31066
I0523 02:36:07.719408 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31066 (* 1 = 9.31066 loss)
I0523 02:36:08.471154 34682 sgd_solver.cpp:112] Iteration 35240, lr = 0.01
I0523 02:36:13.107961 34682 solver.cpp:239] Iteration 35250 (1.85586 iter/s, 5.38832s/10 iters), loss = 8.54608
I0523 02:36:13.108018 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54608 (* 1 = 8.54608 loss)
I0523 02:36:13.174624 34682 sgd_solver.cpp:112] Iteration 35250, lr = 0.01
I0523 02:36:17.320188 34682 solver.cpp:239] Iteration 35260 (2.37417 iter/s, 4.212s/10 iters), loss = 8.24822
I0523 02:36:17.320237 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24822 (* 1 = 8.24822 loss)
I0523 02:36:18.120954 34682 sgd_solver.cpp:112] Iteration 35260, lr = 0.01
I0523 02:36:23.261358 34682 solver.cpp:239] Iteration 35270 (1.68325 iter/s, 5.94088s/10 iters), loss = 7.80733
I0523 02:36:23.261411 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80733 (* 1 = 7.80733 loss)
I0523 02:36:23.325840 34682 sgd_solver.cpp:112] Iteration 35270, lr = 0.01
I0523 02:36:27.595216 34682 solver.cpp:239] Iteration 35280 (2.30754 iter/s, 4.33362s/10 iters), loss = 8.66316
I0523 02:36:27.595309 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66316 (* 1 = 8.66316 loss)
I0523 02:36:28.417250 34682 sgd_solver.cpp:112] Iteration 35280, lr = 0.01
I0523 02:36:32.305016 34682 solver.cpp:239] Iteration 35290 (2.12336 iter/s, 4.70951s/10 iters), loss = 8.70886
I0523 02:36:32.305065 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70886 (* 1 = 8.70886 loss)
I0523 02:36:32.374665 34682 sgd_solver.cpp:112] Iteration 35290, lr = 0.01
I0523 02:36:36.007784 34682 solver.cpp:239] Iteration 35300 (2.70083 iter/s, 3.70257s/10 iters), loss = 8.7195
I0523 02:36:36.007836 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7195 (* 1 = 8.7195 loss)
I0523 02:36:36.080643 34682 sgd_solver.cpp:112] Iteration 35300, lr = 0.01
I0523 02:36:40.898447 34682 solver.cpp:239] Iteration 35310 (2.04482 iter/s, 4.89041s/10 iters), loss = 9.08091
I0523 02:36:40.898502 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08091 (* 1 = 9.08091 loss)
I0523 02:36:41.417884 34682 sgd_solver.cpp:112] Iteration 35310, lr = 0.01
I0523 02:36:45.498098 34682 solver.cpp:239] Iteration 35320 (2.17419 iter/s, 4.59941s/10 iters), loss = 9.55397
I0523 02:36:45.498152 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.55397 (* 1 = 9.55397 loss)
I0523 02:36:46.083168 34682 sgd_solver.cpp:112] Iteration 35320, lr = 0.01
I0523 02:36:51.118438 34682 solver.cpp:239] Iteration 35330 (1.77934 iter/s, 5.62005s/10 iters), loss = 9.73148
I0523 02:36:51.118491 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.73148 (* 1 = 9.73148 loss)
I0523 02:36:51.182294 34682 sgd_solver.cpp:112] Iteration 35330, lr = 0.01
I0523 02:36:54.584900 34682 solver.cpp:239] Iteration 35340 (2.88495 iter/s, 3.46626s/10 iters), loss = 8.18936
I0523 02:36:54.584942 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18936 (* 1 = 8.18936 loss)
I0523 02:36:54.657768 34682 sgd_solver.cpp:112] Iteration 35340, lr = 0.01
I0523 02:36:59.380062 34682 solver.cpp:239] Iteration 35350 (2.08554 iter/s, 4.79492s/10 iters), loss = 9.18983
I0523 02:36:59.380247 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18983 (* 1 = 9.18983 loss)
I0523 02:36:59.926435 34682 sgd_solver.cpp:112] Iteration 35350, lr = 0.01
I0523 02:37:03.709470 34682 solver.cpp:239] Iteration 35360 (2.30998 iter/s, 4.32904s/10 iters), loss = 9.09794
I0523 02:37:03.709523 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09794 (* 1 = 9.09794 loss)
I0523 02:37:03.777364 34682 sgd_solver.cpp:112] Iteration 35360, lr = 0.01
I0523 02:37:09.830541 34682 solver.cpp:239] Iteration 35370 (1.63378 iter/s, 6.12077s/10 iters), loss = 9.13623
I0523 02:37:09.830597 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13623 (* 1 = 9.13623 loss)
I0523 02:37:09.909807 34682 sgd_solver.cpp:112] Iteration 35370, lr = 0.01
I0523 02:37:15.028487 34682 solver.cpp:239] Iteration 35380 (1.92394 iter/s, 5.19767s/10 iters), loss = 8.78949
I0523 02:37:15.028548 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78949 (* 1 = 8.78949 loss)
I0523 02:37:15.107631 34682 sgd_solver.cpp:112] Iteration 35380, lr = 0.01
I0523 02:37:20.794504 34682 solver.cpp:239] Iteration 35390 (1.73439 iter/s, 5.76573s/10 iters), loss = 9.52612
I0523 02:37:20.794565 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.52612 (* 1 = 9.52612 loss)
I0523 02:37:21.367179 34682 sgd_solver.cpp:112] Iteration 35390, lr = 0.01
I0523 02:37:28.343498 34682 solver.cpp:239] Iteration 35400 (1.32474 iter/s, 7.54863s/10 iters), loss = 8.95792
I0523 02:37:28.343555 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95792 (* 1 = 8.95792 loss)
I0523 02:37:28.416529 34682 sgd_solver.cpp:112] Iteration 35400, lr = 0.01
I0523 02:37:33.346509 34682 solver.cpp:239] Iteration 35410 (1.9989 iter/s, 5.00275s/10 iters), loss = 8.98287
I0523 02:37:33.346604 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98287 (* 1 = 8.98287 loss)
I0523 02:37:33.604504 34682 sgd_solver.cpp:112] Iteration 35410, lr = 0.01
I0523 02:37:39.105654 34682 solver.cpp:239] Iteration 35420 (1.73647 iter/s, 5.75881s/10 iters), loss = 8.68431
I0523 02:37:39.105707 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68431 (* 1 = 8.68431 loss)
I0523 02:37:39.949497 34682 sgd_solver.cpp:112] Iteration 35420, lr = 0.01
I0523 02:37:45.033130 34682 solver.cpp:239] Iteration 35430 (1.68714 iter/s, 5.92718s/10 iters), loss = 9.78087
I0523 02:37:45.033185 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.78087 (* 1 = 9.78087 loss)
I0523 02:37:45.847818 34682 sgd_solver.cpp:112] Iteration 35430, lr = 0.01
I0523 02:37:50.344231 34682 solver.cpp:239] Iteration 35440 (1.88295 iter/s, 5.31082s/10 iters), loss = 8.84648
I0523 02:37:50.344292 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84648 (* 1 = 8.84648 loss)
I0523 02:37:50.988417 34682 sgd_solver.cpp:112] Iteration 35440, lr = 0.01
I0523 02:37:53.561818 34682 solver.cpp:239] Iteration 35450 (3.10812 iter/s, 3.21738s/10 iters), loss = 9.15885
I0523 02:37:53.561882 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15885 (* 1 = 9.15885 loss)
I0523 02:37:53.635802 34682 sgd_solver.cpp:112] Iteration 35450, lr = 0.01
I0523 02:37:58.032554 34682 solver.cpp:239] Iteration 35460 (2.23689 iter/s, 4.47049s/10 iters), loss = 8.28383
I0523 02:37:58.032603 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28383 (* 1 = 8.28383 loss)
I0523 02:37:58.591166 34682 sgd_solver.cpp:112] Iteration 35460, lr = 0.01
I0523 02:38:02.700327 34682 solver.cpp:239] Iteration 35470 (2.14246 iter/s, 4.66753s/10 iters), loss = 8.38707
I0523 02:38:02.700384 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38707 (* 1 = 8.38707 loss)
I0523 02:38:03.566766 34682 sgd_solver.cpp:112] Iteration 35470, lr = 0.01
I0523 02:38:08.968152 34682 solver.cpp:239] Iteration 35480 (1.59553 iter/s, 6.2675s/10 iters), loss = 9.18622
I0523 02:38:08.968199 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18622 (* 1 = 9.18622 loss)
I0523 02:38:09.035126 34682 sgd_solver.cpp:112] Iteration 35480, lr = 0.01
I0523 02:38:12.903846 34682 solver.cpp:239] Iteration 35490 (2.54099 iter/s, 3.93547s/10 iters), loss = 8.50944
I0523 02:38:12.903903 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50944 (* 1 = 8.50944 loss)
I0523 02:38:13.623230 34682 sgd_solver.cpp:112] Iteration 35490, lr = 0.01
I0523 02:38:17.552004 34682 solver.cpp:239] Iteration 35500 (2.15151 iter/s, 4.6479s/10 iters), loss = 9.4237
I0523 02:38:17.552064 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4237 (* 1 = 9.4237 loss)
I0523 02:38:17.614338 34682 sgd_solver.cpp:112] Iteration 35500, lr = 0.01
I0523 02:38:21.480705 34682 solver.cpp:239] Iteration 35510 (2.54551 iter/s, 3.92848s/10 iters), loss = 8.97813
I0523 02:38:21.480762 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97813 (* 1 = 8.97813 loss)
I0523 02:38:22.090020 34682 sgd_solver.cpp:112] Iteration 35510, lr = 0.01
I0523 02:38:25.438576 34682 solver.cpp:239] Iteration 35520 (2.52675 iter/s, 3.95765s/10 iters), loss = 9.40724
I0523 02:38:25.438627 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40724 (* 1 = 9.40724 loss)
I0523 02:38:25.519699 34682 sgd_solver.cpp:112] Iteration 35520, lr = 0.01
I0523 02:38:31.213646 34682 solver.cpp:239] Iteration 35530 (1.73166 iter/s, 5.77479s/10 iters), loss = 7.71158
I0523 02:38:31.213691 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.71158 (* 1 = 7.71158 loss)
I0523 02:38:31.916386 34682 sgd_solver.cpp:112] Iteration 35530, lr = 0.01
I0523 02:38:36.098222 34682 solver.cpp:239] Iteration 35540 (2.04736 iter/s, 4.88433s/10 iters), loss = 8.46099
I0523 02:38:36.098369 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46099 (* 1 = 8.46099 loss)
I0523 02:38:36.167222 34682 sgd_solver.cpp:112] Iteration 35540, lr = 0.01
I0523 02:38:40.532222 34682 solver.cpp:239] Iteration 35550 (2.25547 iter/s, 4.43367s/10 iters), loss = 8.69541
I0523 02:38:40.532275 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69541 (* 1 = 8.69541 loss)
I0523 02:38:40.594187 34682 sgd_solver.cpp:112] Iteration 35550, lr = 0.01
I0523 02:38:46.815261 34682 solver.cpp:239] Iteration 35560 (1.59166 iter/s, 6.28273s/10 iters), loss = 9.01726
I0523 02:38:46.815310 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01726 (* 1 = 9.01726 loss)
I0523 02:38:46.871575 34682 sgd_solver.cpp:112] Iteration 35560, lr = 0.01
I0523 02:38:51.443892 34682 solver.cpp:239] Iteration 35570 (2.16058 iter/s, 4.62839s/10 iters), loss = 7.92941
I0523 02:38:51.443939 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92941 (* 1 = 7.92941 loss)
I0523 02:38:52.260318 34682 sgd_solver.cpp:112] Iteration 35570, lr = 0.01
I0523 02:38:57.580374 34682 solver.cpp:239] Iteration 35580 (1.62968 iter/s, 6.13617s/10 iters), loss = 9.26849
I0523 02:38:57.580436 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.26849 (* 1 = 9.26849 loss)
I0523 02:38:58.416790 34682 sgd_solver.cpp:112] Iteration 35580, lr = 0.01
I0523 02:39:04.088769 34682 solver.cpp:239] Iteration 35590 (1.53655 iter/s, 6.50807s/10 iters), loss = 8.84394
I0523 02:39:04.088819 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84394 (* 1 = 8.84394 loss)
I0523 02:39:04.825969 34682 sgd_solver.cpp:112] Iteration 35590, lr = 0.01
I0523 02:39:09.219895 34682 solver.cpp:239] Iteration 35600 (1.94899 iter/s, 5.13087s/10 iters), loss = 9.42218
I0523 02:39:09.220141 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42218 (* 1 = 9.42218 loss)
I0523 02:39:10.046703 34682 sgd_solver.cpp:112] Iteration 35600, lr = 0.01
I0523 02:39:14.765192 34682 solver.cpp:239] Iteration 35610 (1.80348 iter/s, 5.54484s/10 iters), loss = 8.88327
I0523 02:39:14.765247 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88327 (* 1 = 8.88327 loss)
I0523 02:39:15.129779 34682 sgd_solver.cpp:112] Iteration 35610, lr = 0.01
I0523 02:39:20.529034 34682 solver.cpp:239] Iteration 35620 (1.73504 iter/s, 5.76354s/10 iters), loss = 8.64222
I0523 02:39:20.529100 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64222 (* 1 = 8.64222 loss)
I0523 02:39:20.590616 34682 sgd_solver.cpp:112] Iteration 35620, lr = 0.01
I0523 02:39:23.218574 34682 solver.cpp:239] Iteration 35630 (3.71835 iter/s, 2.68936s/10 iters), loss = 9.2512
I0523 02:39:23.218629 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2512 (* 1 = 9.2512 loss)
I0523 02:39:24.086001 34682 sgd_solver.cpp:112] Iteration 35630, lr = 0.01
I0523 02:39:30.587766 34682 solver.cpp:239] Iteration 35640 (1.35707 iter/s, 7.36881s/10 iters), loss = 9.6484
I0523 02:39:30.587836 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.6484 (* 1 = 9.6484 loss)
I0523 02:39:30.660797 34682 sgd_solver.cpp:112] Iteration 35640, lr = 0.01
I0523 02:39:35.028980 34682 solver.cpp:239] Iteration 35650 (2.25176 iter/s, 4.44097s/10 iters), loss = 8.7561
I0523 02:39:35.029023 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7561 (* 1 = 8.7561 loss)
I0523 02:39:35.100708 34682 sgd_solver.cpp:112] Iteration 35650, lr = 0.01
I0523 02:39:39.390389 34682 solver.cpp:239] Iteration 35660 (2.29296 iter/s, 4.36118s/10 iters), loss = 8.77937
I0523 02:39:39.390465 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77937 (* 1 = 8.77937 loss)
I0523 02:39:40.206084 34682 sgd_solver.cpp:112] Iteration 35660, lr = 0.01
I0523 02:39:44.132561 34682 solver.cpp:239] Iteration 35670 (2.11079 iter/s, 4.73756s/10 iters), loss = 9.44346
I0523 02:39:44.132606 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44346 (* 1 = 9.44346 loss)
I0523 02:39:44.201570 34682 sgd_solver.cpp:112] Iteration 35670, lr = 0.01
I0523 02:39:48.572656 34682 solver.cpp:239] Iteration 35680 (2.25232 iter/s, 4.43986s/10 iters), loss = 9.20299
I0523 02:39:48.572718 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20299 (* 1 = 9.20299 loss)
I0523 02:39:48.628070 34682 sgd_solver.cpp:112] Iteration 35680, lr = 0.01
I0523 02:39:52.173485 34682 solver.cpp:239] Iteration 35690 (2.77733 iter/s, 3.60059s/10 iters), loss = 8.83612
I0523 02:39:52.173543 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83612 (* 1 = 8.83612 loss)
I0523 02:39:52.916442 34682 sgd_solver.cpp:112] Iteration 35690, lr = 0.01
I0523 02:39:55.905630 34682 solver.cpp:239] Iteration 35700 (2.67958 iter/s, 3.73193s/10 iters), loss = 8.88589
I0523 02:39:55.905680 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88589 (* 1 = 8.88589 loss)
I0523 02:39:55.969578 34682 sgd_solver.cpp:112] Iteration 35700, lr = 0.01
I0523 02:40:00.069675 34682 solver.cpp:239] Iteration 35710 (2.40164 iter/s, 4.16382s/10 iters), loss = 8.84401
I0523 02:40:00.069730 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84401 (* 1 = 8.84401 loss)
I0523 02:40:00.850292 34682 sgd_solver.cpp:112] Iteration 35710, lr = 0.01
I0523 02:40:02.908710 34682 solver.cpp:239] Iteration 35720 (3.52254 iter/s, 2.83886s/10 iters), loss = 8.25117
I0523 02:40:02.908754 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25117 (* 1 = 8.25117 loss)
I0523 02:40:03.755645 34682 sgd_solver.cpp:112] Iteration 35720, lr = 0.01
I0523 02:40:08.610961 34682 solver.cpp:239] Iteration 35730 (1.75378 iter/s, 5.70197s/10 iters), loss = 8.66178
I0523 02:40:08.611007 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66178 (* 1 = 8.66178 loss)
I0523 02:40:08.678361 34682 sgd_solver.cpp:112] Iteration 35730, lr = 0.01
I0523 02:40:12.707998 34682 solver.cpp:239] Iteration 35740 (2.44092 iter/s, 4.09682s/10 iters), loss = 8.34502
I0523 02:40:12.708217 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34502 (* 1 = 8.34502 loss)
I0523 02:40:13.402808 34682 sgd_solver.cpp:112] Iteration 35740, lr = 0.01
I0523 02:40:18.339062 34682 solver.cpp:239] Iteration 35750 (1.776 iter/s, 5.63063s/10 iters), loss = 9.36818
I0523 02:40:18.339110 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36818 (* 1 = 9.36818 loss)
I0523 02:40:19.159405 34682 sgd_solver.cpp:112] Iteration 35750, lr = 0.01
I0523 02:40:22.418216 34682 solver.cpp:239] Iteration 35760 (2.45162 iter/s, 4.07893s/10 iters), loss = 8.92662
I0523 02:40:22.418268 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92662 (* 1 = 8.92662 loss)
I0523 02:40:22.490104 34682 sgd_solver.cpp:112] Iteration 35760, lr = 0.01
I0523 02:40:26.700543 34682 solver.cpp:239] Iteration 35770 (2.3353 iter/s, 4.2821s/10 iters), loss = 8.36798
I0523 02:40:26.700598 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36798 (* 1 = 8.36798 loss)
I0523 02:40:27.540416 34682 sgd_solver.cpp:112] Iteration 35770, lr = 0.01
I0523 02:40:31.688832 34682 solver.cpp:239] Iteration 35780 (2.0048 iter/s, 4.98802s/10 iters), loss = 8.27203
I0523 02:40:31.688884 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27203 (* 1 = 8.27203 loss)
I0523 02:40:31.762845 34682 sgd_solver.cpp:112] Iteration 35780, lr = 0.01
I0523 02:40:35.873719 34682 solver.cpp:239] Iteration 35790 (2.38968 iter/s, 4.18466s/10 iters), loss = 8.53408
I0523 02:40:35.873783 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53408 (* 1 = 8.53408 loss)
I0523 02:40:35.953358 34682 sgd_solver.cpp:112] Iteration 35790, lr = 0.01
I0523 02:40:40.601773 34682 solver.cpp:239] Iteration 35800 (2.11515 iter/s, 4.7278s/10 iters), loss = 9.09431
I0523 02:40:40.601830 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09431 (* 1 = 9.09431 loss)
I0523 02:40:40.674592 34682 sgd_solver.cpp:112] Iteration 35800, lr = 0.01
I0523 02:40:45.956676 34682 solver.cpp:239] Iteration 35810 (1.86754 iter/s, 5.35463s/10 iters), loss = 8.4428
I0523 02:40:45.956833 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4428 (* 1 = 8.4428 loss)
I0523 02:40:46.022387 34682 sgd_solver.cpp:112] Iteration 35810, lr = 0.01
I0523 02:40:53.538638 34682 solver.cpp:239] Iteration 35820 (1.319 iter/s, 7.58151s/10 iters), loss = 8.89318
I0523 02:40:53.538692 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89318 (* 1 = 8.89318 loss)
I0523 02:40:54.438156 34682 sgd_solver.cpp:112] Iteration 35820, lr = 0.01
I0523 02:40:59.746608 34682 solver.cpp:239] Iteration 35830 (1.61091 iter/s, 6.20766s/10 iters), loss = 8.37763
I0523 02:40:59.746656 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37763 (* 1 = 8.37763 loss)
I0523 02:40:59.824751 34682 sgd_solver.cpp:112] Iteration 35830, lr = 0.01
I0523 02:41:03.873006 34682 solver.cpp:239] Iteration 35840 (2.42355 iter/s, 4.12618s/10 iters), loss = 8.98245
I0523 02:41:03.873055 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98245 (* 1 = 8.98245 loss)
I0523 02:41:04.410735 34682 sgd_solver.cpp:112] Iteration 35840, lr = 0.01
I0523 02:41:07.803822 34682 solver.cpp:239] Iteration 35850 (2.54414 iter/s, 3.9306s/10 iters), loss = 9.17662
I0523 02:41:07.803869 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17662 (* 1 = 9.17662 loss)
I0523 02:41:07.869765 34682 sgd_solver.cpp:112] Iteration 35850, lr = 0.01
I0523 02:41:11.819515 34682 solver.cpp:239] Iteration 35860 (2.49036 iter/s, 4.01548s/10 iters), loss = 9.23525
I0523 02:41:11.819558 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23525 (* 1 = 9.23525 loss)
I0523 02:41:12.598026 34682 sgd_solver.cpp:112] Iteration 35860, lr = 0.01
I0523 02:41:18.288473 34682 solver.cpp:239] Iteration 35870 (1.54592 iter/s, 6.46865s/10 iters), loss = 8.23256
I0523 02:41:18.288604 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23256 (* 1 = 8.23256 loss)
I0523 02:41:18.362349 34682 sgd_solver.cpp:112] Iteration 35870, lr = 0.01
I0523 02:41:21.678961 34682 solver.cpp:239] Iteration 35880 (2.94966 iter/s, 3.39022s/10 iters), loss = 9.01714
I0523 02:41:21.679018 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01714 (* 1 = 9.01714 loss)
I0523 02:41:21.743829 34682 sgd_solver.cpp:112] Iteration 35880, lr = 0.01
I0523 02:41:25.637069 34682 solver.cpp:239] Iteration 35890 (2.5266 iter/s, 3.95789s/10 iters), loss = 9.65346
I0523 02:41:25.637121 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.65346 (* 1 = 9.65346 loss)
I0523 02:41:26.447031 34682 sgd_solver.cpp:112] Iteration 35890, lr = 0.01
I0523 02:41:30.418730 34682 solver.cpp:239] Iteration 35900 (2.09143 iter/s, 4.78141s/10 iters), loss = 8.1845
I0523 02:41:30.418783 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1845 (* 1 = 8.1845 loss)
I0523 02:41:31.187101 34682 sgd_solver.cpp:112] Iteration 35900, lr = 0.01
I0523 02:41:35.962997 34682 solver.cpp:239] Iteration 35910 (1.80376 iter/s, 5.54398s/10 iters), loss = 9.78186
I0523 02:41:35.963049 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.78186 (* 1 = 9.78186 loss)
I0523 02:41:36.823881 34682 sgd_solver.cpp:112] Iteration 35910, lr = 0.01
I0523 02:41:41.656965 34682 solver.cpp:239] Iteration 35920 (1.75633 iter/s, 5.69368s/10 iters), loss = 8.64856
I0523 02:41:41.657032 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64856 (* 1 = 8.64856 loss)
I0523 02:41:41.735096 34682 sgd_solver.cpp:112] Iteration 35920, lr = 0.01
I0523 02:41:45.074550 34682 solver.cpp:239] Iteration 35930 (2.92958 iter/s, 3.41346s/10 iters), loss = 9.55256
I0523 02:41:45.074611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.55256 (* 1 = 9.55256 loss)
I0523 02:41:45.374308 34682 sgd_solver.cpp:112] Iteration 35930, lr = 0.01
I0523 02:41:48.435456 34682 solver.cpp:239] Iteration 35940 (2.97556 iter/s, 3.36071s/10 iters), loss = 9.23597
I0523 02:41:48.435539 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23597 (* 1 = 9.23597 loss)
I0523 02:41:49.279044 34682 sgd_solver.cpp:112] Iteration 35940, lr = 0.01
I0523 02:41:51.725507 34682 solver.cpp:239] Iteration 35950 (3.03968 iter/s, 3.28983s/10 iters), loss = 8.57476
I0523 02:41:51.725554 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57476 (* 1 = 8.57476 loss)
I0523 02:41:52.552253 34682 sgd_solver.cpp:112] Iteration 35950, lr = 0.01
I0523 02:41:58.098652 34682 solver.cpp:239] Iteration 35960 (1.56916 iter/s, 6.37283s/10 iters), loss = 9.10312
I0523 02:41:58.098718 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10312 (* 1 = 9.10312 loss)
I0523 02:41:58.162451 34682 sgd_solver.cpp:112] Iteration 35960, lr = 0.01
I0523 02:42:02.164129 34682 solver.cpp:239] Iteration 35970 (2.45988 iter/s, 4.06525s/10 iters), loss = 8.99303
I0523 02:42:02.164176 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99303 (* 1 = 8.99303 loss)
I0523 02:42:02.229904 34682 sgd_solver.cpp:112] Iteration 35970, lr = 0.01
I0523 02:42:06.176669 34682 solver.cpp:239] Iteration 35980 (2.49232 iter/s, 4.01232s/10 iters), loss = 9.3343
I0523 02:42:06.176731 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3343 (* 1 = 9.3343 loss)
I0523 02:42:06.243232 34682 sgd_solver.cpp:112] Iteration 35980, lr = 0.01
I0523 02:42:11.074904 34682 solver.cpp:239] Iteration 35990 (2.04166 iter/s, 4.89797s/10 iters), loss = 8.64846
I0523 02:42:11.074966 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64846 (* 1 = 8.64846 loss)
I0523 02:42:11.131640 34682 sgd_solver.cpp:112] Iteration 35990, lr = 0.01
I0523 02:42:16.079208 34682 solver.cpp:239] Iteration 36000 (1.99838 iter/s, 5.00405s/10 iters), loss = 8.45243
I0523 02:42:16.079254 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45243 (* 1 = 8.45243 loss)
I0523 02:42:16.145438 34682 sgd_solver.cpp:112] Iteration 36000, lr = 0.01
I0523 02:42:21.627660 34682 solver.cpp:239] Iteration 36010 (1.80239 iter/s, 5.54817s/10 iters), loss = 8.34477
I0523 02:42:21.627825 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34477 (* 1 = 8.34477 loss)
I0523 02:42:21.705832 34682 sgd_solver.cpp:112] Iteration 36010, lr = 0.01
I0523 02:42:26.196858 34682 solver.cpp:239] Iteration 36020 (2.18874 iter/s, 4.56885s/10 iters), loss = 9.59104
I0523 02:42:26.196913 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59104 (* 1 = 9.59104 loss)
I0523 02:42:26.421488 34682 sgd_solver.cpp:112] Iteration 36020, lr = 0.01
I0523 02:42:29.752120 34682 solver.cpp:239] Iteration 36030 (2.81289 iter/s, 3.55506s/10 iters), loss = 9.43161
I0523 02:42:29.752176 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43161 (* 1 = 9.43161 loss)
I0523 02:42:29.834939 34682 sgd_solver.cpp:112] Iteration 36030, lr = 0.01
I0523 02:42:35.063383 34682 solver.cpp:239] Iteration 36040 (1.88289 iter/s, 5.31099s/10 iters), loss = 8.65969
I0523 02:42:35.063426 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65969 (* 1 = 8.65969 loss)
I0523 02:42:35.136260 34682 sgd_solver.cpp:112] Iteration 36040, lr = 0.01
I0523 02:42:41.262833 34682 solver.cpp:239] Iteration 36050 (1.61313 iter/s, 6.19915s/10 iters), loss = 7.93947
I0523 02:42:41.262897 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93947 (* 1 = 7.93947 loss)
I0523 02:42:41.308182 34682 sgd_solver.cpp:112] Iteration 36050, lr = 0.01
I0523 02:42:42.485123 34682 solver.cpp:239] Iteration 36060 (8.18216 iter/s, 1.22217s/10 iters), loss = 9.78355
I0523 02:42:42.485162 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.78355 (* 1 = 9.78355 loss)
I0523 02:42:42.529170 34682 sgd_solver.cpp:112] Iteration 36060, lr = 0.01
I0523 02:42:45.321316 34682 solver.cpp:239] Iteration 36070 (3.52606 iter/s, 2.83603s/10 iters), loss = 8.97699
I0523 02:42:45.321360 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97699 (* 1 = 8.97699 loss)
I0523 02:42:45.376874 34682 sgd_solver.cpp:112] Iteration 36070, lr = 0.01
I0523 02:42:52.675295 34682 solver.cpp:239] Iteration 36080 (1.35987 iter/s, 7.35363s/10 iters), loss = 9.31378
I0523 02:42:52.675510 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31378 (* 1 = 9.31378 loss)
I0523 02:42:53.518198 34682 sgd_solver.cpp:112] Iteration 36080, lr = 0.01
I0523 02:42:58.100769 34682 solver.cpp:239] Iteration 36090 (1.8433 iter/s, 5.42506s/10 iters), loss = 7.7072
I0523 02:42:58.100808 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7072 (* 1 = 7.7072 loss)
I0523 02:42:58.174233 34682 sgd_solver.cpp:112] Iteration 36090, lr = 0.01
I0523 02:43:02.421363 34682 solver.cpp:239] Iteration 36100 (2.31462 iter/s, 4.32037s/10 iters), loss = 9.18691
I0523 02:43:02.421418 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18691 (* 1 = 9.18691 loss)
I0523 02:43:03.250048 34682 sgd_solver.cpp:112] Iteration 36100, lr = 0.01
I0523 02:43:09.065170 34682 solver.cpp:239] Iteration 36110 (1.50523 iter/s, 6.64349s/10 iters), loss = 8.30901
I0523 02:43:09.065215 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30901 (* 1 = 8.30901 loss)
I0523 02:43:09.863438 34682 sgd_solver.cpp:112] Iteration 36110, lr = 0.01
I0523 02:43:14.552968 34682 solver.cpp:239] Iteration 36120 (1.82231 iter/s, 5.48753s/10 iters), loss = 8.7931
I0523 02:43:14.553012 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7931 (* 1 = 8.7931 loss)
I0523 02:43:15.264701 34682 sgd_solver.cpp:112] Iteration 36120, lr = 0.01
I0523 02:43:20.182901 34682 solver.cpp:239] Iteration 36130 (1.77631 iter/s, 5.62966s/10 iters), loss = 8.59447
I0523 02:43:20.182953 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59447 (* 1 = 8.59447 loss)
I0523 02:43:20.980402 34682 sgd_solver.cpp:112] Iteration 36130, lr = 0.01
I0523 02:43:25.018308 34682 solver.cpp:239] Iteration 36140 (2.06818 iter/s, 4.83516s/10 iters), loss = 9.86257
I0523 02:43:25.018595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.86257 (* 1 = 9.86257 loss)
I0523 02:43:25.097702 34682 sgd_solver.cpp:112] Iteration 36140, lr = 0.01
I0523 02:43:28.116475 34682 solver.cpp:239] Iteration 36150 (3.22812 iter/s, 3.09778s/10 iters), loss = 8.4988
I0523 02:43:28.116541 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4988 (* 1 = 8.4988 loss)
I0523 02:43:28.814386 34682 sgd_solver.cpp:112] Iteration 36150, lr = 0.01
I0523 02:43:34.464184 34682 solver.cpp:239] Iteration 36160 (1.57545 iter/s, 6.34739s/10 iters), loss = 8.13745
I0523 02:43:34.464236 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13745 (* 1 = 8.13745 loss)
I0523 02:43:35.299094 34682 sgd_solver.cpp:112] Iteration 36160, lr = 0.01
I0523 02:43:39.314052 34682 solver.cpp:239] Iteration 36170 (2.06202 iter/s, 4.84962s/10 iters), loss = 8.81898
I0523 02:43:39.314105 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81898 (* 1 = 8.81898 loss)
I0523 02:43:39.383244 34682 sgd_solver.cpp:112] Iteration 36170, lr = 0.01
I0523 02:43:43.174072 34682 solver.cpp:239] Iteration 36180 (2.5908 iter/s, 3.85981s/10 iters), loss = 8.79665
I0523 02:43:43.174124 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79665 (* 1 = 8.79665 loss)
I0523 02:43:44.013954 34682 sgd_solver.cpp:112] Iteration 36180, lr = 0.01
I0523 02:43:49.222482 34682 solver.cpp:239] Iteration 36190 (1.65341 iter/s, 6.04811s/10 iters), loss = 9.00959
I0523 02:43:49.222535 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00959 (* 1 = 9.00959 loss)
I0523 02:43:50.022074 34682 sgd_solver.cpp:112] Iteration 36190, lr = 0.01
I0523 02:43:54.010354 34682 solver.cpp:239] Iteration 36200 (2.08872 iter/s, 4.78762s/10 iters), loss = 9.03332
I0523 02:43:54.010407 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03332 (* 1 = 9.03332 loss)
I0523 02:43:54.901334 34682 sgd_solver.cpp:112] Iteration 36200, lr = 0.01
I0523 02:43:58.364703 34682 solver.cpp:239] Iteration 36210 (2.29668 iter/s, 4.35411s/10 iters), loss = 8.56299
I0523 02:43:58.364939 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56299 (* 1 = 8.56299 loss)
I0523 02:43:58.443281 34682 sgd_solver.cpp:112] Iteration 36210, lr = 0.01
I0523 02:44:03.905025 34682 solver.cpp:239] Iteration 36220 (1.80509 iter/s, 5.53989s/10 iters), loss = 8.58263
I0523 02:44:03.905067 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58263 (* 1 = 8.58263 loss)
I0523 02:44:03.979223 34682 sgd_solver.cpp:112] Iteration 36220, lr = 0.01
I0523 02:44:09.425902 34682 solver.cpp:239] Iteration 36230 (1.8114 iter/s, 5.5206s/10 iters), loss = 8.993
I0523 02:44:09.425966 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.993 (* 1 = 8.993 loss)
I0523 02:44:10.083922 34682 sgd_solver.cpp:112] Iteration 36230, lr = 0.01
I0523 02:44:15.362134 34682 solver.cpp:239] Iteration 36240 (1.68465 iter/s, 5.93593s/10 iters), loss = 8.34079
I0523 02:44:15.362175 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34079 (* 1 = 8.34079 loss)
I0523 02:44:15.434571 34682 sgd_solver.cpp:112] Iteration 36240, lr = 0.01
I0523 02:44:20.291137 34682 solver.cpp:239] Iteration 36250 (2.02891 iter/s, 4.92875s/10 iters), loss = 8.78099
I0523 02:44:20.291203 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78099 (* 1 = 8.78099 loss)
I0523 02:44:20.358310 34682 sgd_solver.cpp:112] Iteration 36250, lr = 0.01
I0523 02:44:23.905619 34682 solver.cpp:239] Iteration 36260 (2.76681 iter/s, 3.61427s/10 iters), loss = 9.27763
I0523 02:44:23.905663 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27763 (* 1 = 9.27763 loss)
I0523 02:44:24.745805 34682 sgd_solver.cpp:112] Iteration 36260, lr = 0.01
I0523 02:44:28.630654 34682 solver.cpp:239] Iteration 36270 (2.11649 iter/s, 4.72479s/10 iters), loss = 8.09437
I0523 02:44:28.630889 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09437 (* 1 = 8.09437 loss)
I0523 02:44:28.715891 34682 sgd_solver.cpp:112] Iteration 36270, lr = 0.01
I0523 02:44:33.877470 34682 solver.cpp:239] Iteration 36280 (1.90608 iter/s, 5.24637s/10 iters), loss = 8.31373
I0523 02:44:33.877526 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31373 (* 1 = 8.31373 loss)
I0523 02:44:33.961410 34682 sgd_solver.cpp:112] Iteration 36280, lr = 0.01
I0523 02:44:39.010529 34682 solver.cpp:239] Iteration 36290 (1.94826 iter/s, 5.13279s/10 iters), loss = 9.37933
I0523 02:44:39.010596 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.37933 (* 1 = 9.37933 loss)
I0523 02:44:39.752370 34682 sgd_solver.cpp:112] Iteration 36290, lr = 0.01
I0523 02:44:43.409996 34682 solver.cpp:239] Iteration 36300 (2.27314 iter/s, 4.39919s/10 iters), loss = 9.62574
I0523 02:44:43.410043 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.62574 (* 1 = 9.62574 loss)
I0523 02:44:43.483055 34682 sgd_solver.cpp:112] Iteration 36300, lr = 0.01
I0523 02:44:49.145700 34682 solver.cpp:239] Iteration 36310 (1.74355 iter/s, 5.73542s/10 iters), loss = 9.07572
I0523 02:44:49.145745 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07572 (* 1 = 9.07572 loss)
I0523 02:44:49.207542 34682 sgd_solver.cpp:112] Iteration 36310, lr = 0.01
I0523 02:44:52.646401 34682 solver.cpp:239] Iteration 36320 (2.85673 iter/s, 3.5005s/10 iters), loss = 8.6973
I0523 02:44:52.646456 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6973 (* 1 = 8.6973 loss)
I0523 02:44:53.394737 34682 sgd_solver.cpp:112] Iteration 36320, lr = 0.01
I0523 02:44:57.280251 34682 solver.cpp:239] Iteration 36330 (2.15815 iter/s, 4.63361s/10 iters), loss = 9.04784
I0523 02:44:57.280308 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04784 (* 1 = 9.04784 loss)
I0523 02:44:58.131433 34682 sgd_solver.cpp:112] Iteration 36330, lr = 0.01
I0523 02:45:02.597765 34682 solver.cpp:239] Iteration 36340 (1.88068 iter/s, 5.31724s/10 iters), loss = 8.40916
I0523 02:45:02.597916 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40916 (* 1 = 8.40916 loss)
I0523 02:45:02.661062 34682 sgd_solver.cpp:112] Iteration 36340, lr = 0.01
I0523 02:45:08.674886 34682 solver.cpp:239] Iteration 36350 (1.64562 iter/s, 6.07672s/10 iters), loss = 8.24376
I0523 02:45:08.674947 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24376 (* 1 = 8.24376 loss)
I0523 02:45:08.751956 34682 sgd_solver.cpp:112] Iteration 36350, lr = 0.01
I0523 02:45:13.042490 34682 solver.cpp:239] Iteration 36360 (2.28971 iter/s, 4.36737s/10 iters), loss = 8.89995
I0523 02:45:13.042532 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89995 (* 1 = 8.89995 loss)
I0523 02:45:13.116396 34682 sgd_solver.cpp:112] Iteration 36360, lr = 0.01
I0523 02:45:17.481021 34682 solver.cpp:239] Iteration 36370 (2.25313 iter/s, 4.43827s/10 iters), loss = 8.59196
I0523 02:45:17.481101 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59196 (* 1 = 8.59196 loss)
I0523 02:45:18.300127 34682 sgd_solver.cpp:112] Iteration 36370, lr = 0.01
I0523 02:45:24.675864 34682 solver.cpp:239] Iteration 36380 (1.38995 iter/s, 7.19448s/10 iters), loss = 8.65387
I0523 02:45:24.675921 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65387 (* 1 = 8.65387 loss)
I0523 02:45:24.761185 34682 sgd_solver.cpp:112] Iteration 36380, lr = 0.01
I0523 02:45:28.890758 34682 solver.cpp:239] Iteration 36390 (2.37267 iter/s, 4.21466s/10 iters), loss = 9.33228
I0523 02:45:28.890810 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33228 (* 1 = 9.33228 loss)
I0523 02:45:29.645819 34682 sgd_solver.cpp:112] Iteration 36390, lr = 0.01
I0523 02:45:34.095698 34682 solver.cpp:239] Iteration 36400 (1.92135 iter/s, 5.20467s/10 iters), loss = 9.11239
I0523 02:45:34.095851 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11239 (* 1 = 9.11239 loss)
I0523 02:45:34.162941 34682 sgd_solver.cpp:112] Iteration 36400, lr = 0.01
I0523 02:45:38.367162 34682 solver.cpp:239] Iteration 36410 (2.34129 iter/s, 4.27115s/10 iters), loss = 8.55489
I0523 02:45:38.367204 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55489 (* 1 = 8.55489 loss)
I0523 02:45:38.442644 34682 sgd_solver.cpp:112] Iteration 36410, lr = 0.01
I0523 02:45:43.292309 34682 solver.cpp:239] Iteration 36420 (2.0305 iter/s, 4.9249s/10 iters), loss = 9.5116
I0523 02:45:43.292348 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.5116 (* 1 = 9.5116 loss)
I0523 02:45:43.351188 34682 sgd_solver.cpp:112] Iteration 36420, lr = 0.01
I0523 02:45:47.863677 34682 solver.cpp:239] Iteration 36430 (2.18764 iter/s, 4.57114s/10 iters), loss = 8.53012
I0523 02:45:47.863723 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53012 (* 1 = 8.53012 loss)
I0523 02:45:47.940789 34682 sgd_solver.cpp:112] Iteration 36430, lr = 0.01
I0523 02:45:52.704443 34682 solver.cpp:239] Iteration 36440 (2.06589 iter/s, 4.84052s/10 iters), loss = 9.16921
I0523 02:45:52.704493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16921 (* 1 = 9.16921 loss)
I0523 02:45:52.772133 34682 sgd_solver.cpp:112] Iteration 36440, lr = 0.01
I0523 02:45:58.523495 34682 solver.cpp:239] Iteration 36450 (1.71858 iter/s, 5.81876s/10 iters), loss = 7.98192
I0523 02:45:58.523541 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98192 (* 1 = 7.98192 loss)
I0523 02:45:58.596897 34682 sgd_solver.cpp:112] Iteration 36450, lr = 0.01
I0523 02:46:03.327579 34682 solver.cpp:239] Iteration 36460 (2.08167 iter/s, 4.80383s/10 iters), loss = 8.79457
I0523 02:46:03.327630 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79457 (* 1 = 8.79457 loss)
I0523 02:46:04.192703 34682 sgd_solver.cpp:112] Iteration 36460, lr = 0.01
I0523 02:46:10.640261 34682 solver.cpp:239] Iteration 36470 (1.36755 iter/s, 7.31234s/10 iters), loss = 8.26497
I0523 02:46:10.640317 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26497 (* 1 = 8.26497 loss)
I0523 02:46:11.468258 34682 sgd_solver.cpp:112] Iteration 36470, lr = 0.01
I0523 02:46:16.432776 34682 solver.cpp:239] Iteration 36480 (1.72645 iter/s, 5.79222s/10 iters), loss = 8.59235
I0523 02:46:16.432817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59235 (* 1 = 8.59235 loss)
I0523 02:46:16.508302 34682 sgd_solver.cpp:112] Iteration 36480, lr = 0.01
I0523 02:46:19.937885 34682 solver.cpp:239] Iteration 36490 (2.85313 iter/s, 3.50492s/10 iters), loss = 7.93242
I0523 02:46:19.937929 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93242 (* 1 = 7.93242 loss)
I0523 02:46:20.744921 34682 sgd_solver.cpp:112] Iteration 36490, lr = 0.01
I0523 02:46:27.706483 34682 solver.cpp:239] Iteration 36500 (1.28729 iter/s, 7.76824s/10 iters), loss = 8.41883
I0523 02:46:27.706529 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41883 (* 1 = 8.41883 loss)
I0523 02:46:27.769975 34682 sgd_solver.cpp:112] Iteration 36500, lr = 0.01
I0523 02:46:33.345765 34682 solver.cpp:239] Iteration 36510 (1.77336 iter/s, 5.639s/10 iters), loss = 8.54291
I0523 02:46:33.345813 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54291 (* 1 = 8.54291 loss)
I0523 02:46:33.407644 34682 sgd_solver.cpp:112] Iteration 36510, lr = 0.01
I0523 02:46:35.760560 34682 solver.cpp:239] Iteration 36520 (4.14141 iter/s, 2.41463s/10 iters), loss = 9.0967
I0523 02:46:35.760797 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0967 (* 1 = 9.0967 loss)
I0523 02:46:36.092396 34682 sgd_solver.cpp:112] Iteration 36520, lr = 0.01
I0523 02:46:41.151999 34682 solver.cpp:239] Iteration 36530 (1.85494 iter/s, 5.39102s/10 iters), loss = 8.13524
I0523 02:46:41.152043 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13524 (* 1 = 8.13524 loss)
I0523 02:46:41.206483 34682 sgd_solver.cpp:112] Iteration 36530, lr = 0.01
I0523 02:46:45.409191 34682 solver.cpp:239] Iteration 36540 (2.34909 iter/s, 4.25697s/10 iters), loss = 7.83231
I0523 02:46:45.409246 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83231 (* 1 = 7.83231 loss)
I0523 02:46:46.131445 34682 sgd_solver.cpp:112] Iteration 36540, lr = 0.01
I0523 02:46:49.571830 34682 solver.cpp:239] Iteration 36550 (2.40245 iter/s, 4.16241s/10 iters), loss = 9.90701
I0523 02:46:49.571874 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.90701 (* 1 = 9.90701 loss)
I0523 02:46:50.436012 34682 sgd_solver.cpp:112] Iteration 36550, lr = 0.01
I0523 02:46:54.622540 34682 solver.cpp:239] Iteration 36560 (1.98002 iter/s, 5.05046s/10 iters), loss = 7.4391
I0523 02:46:54.622594 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4391 (* 1 = 7.4391 loss)
I0523 02:46:54.683272 34682 sgd_solver.cpp:112] Iteration 36560, lr = 0.01
I0523 02:46:59.837807 34682 solver.cpp:239] Iteration 36570 (1.91754 iter/s, 5.215s/10 iters), loss = 8.55561
I0523 02:46:59.837859 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55561 (* 1 = 8.55561 loss)
I0523 02:47:00.711483 34682 sgd_solver.cpp:112] Iteration 36570, lr = 0.01
I0523 02:47:07.010797 34682 solver.cpp:239] Iteration 36580 (1.39419 iter/s, 7.17264s/10 iters), loss = 9.19834
I0523 02:47:07.011087 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19834 (* 1 = 9.19834 loss)
I0523 02:47:07.833010 34682 sgd_solver.cpp:112] Iteration 36580, lr = 0.01
I0523 02:47:11.052376 34682 solver.cpp:239] Iteration 36590 (2.47454 iter/s, 4.04115s/10 iters), loss = 8.72182
I0523 02:47:11.052426 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72182 (* 1 = 8.72182 loss)
I0523 02:47:11.129261 34682 sgd_solver.cpp:112] Iteration 36590, lr = 0.01
I0523 02:47:16.020026 34682 solver.cpp:239] Iteration 36600 (2.01313 iter/s, 4.96738s/10 iters), loss = 8.19572
I0523 02:47:16.020092 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19572 (* 1 = 8.19572 loss)
I0523 02:47:16.086602 34682 sgd_solver.cpp:112] Iteration 36600, lr = 0.01
I0523 02:47:20.820704 34682 solver.cpp:239] Iteration 36610 (2.08315 iter/s, 4.80042s/10 iters), loss = 8.39048
I0523 02:47:20.820755 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39048 (* 1 = 8.39048 loss)
I0523 02:47:20.876694 34682 sgd_solver.cpp:112] Iteration 36610, lr = 0.01
I0523 02:47:25.250963 34682 solver.cpp:239] Iteration 36620 (2.25733 iter/s, 4.43002s/10 iters), loss = 8.61601
I0523 02:47:25.251015 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61601 (* 1 = 8.61601 loss)
I0523 02:47:26.111049 34682 sgd_solver.cpp:112] Iteration 36620, lr = 0.01
I0523 02:47:31.578174 34682 solver.cpp:239] Iteration 36630 (1.58055 iter/s, 6.32691s/10 iters), loss = 8.95137
I0523 02:47:31.578218 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95137 (* 1 = 8.95137 loss)
I0523 02:47:31.658138 34682 sgd_solver.cpp:112] Iteration 36630, lr = 0.01
I0523 02:47:36.388515 34682 solver.cpp:239] Iteration 36640 (2.07896 iter/s, 4.81009s/10 iters), loss = 9.13457
I0523 02:47:36.388566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13457 (* 1 = 9.13457 loss)
I0523 02:47:36.451655 34682 sgd_solver.cpp:112] Iteration 36640, lr = 0.01
I0523 02:47:42.085975 34682 solver.cpp:239] Iteration 36650 (1.75526 iter/s, 5.69718s/10 iters), loss = 9.84047
I0523 02:47:42.086226 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.84047 (* 1 = 9.84047 loss)
I0523 02:47:42.947032 34682 sgd_solver.cpp:112] Iteration 36650, lr = 0.01
I0523 02:47:47.254648 34682 solver.cpp:239] Iteration 36660 (1.93489 iter/s, 5.16825s/10 iters), loss = 8.78391
I0523 02:47:47.254703 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78391 (* 1 = 8.78391 loss)
I0523 02:47:47.314965 34682 sgd_solver.cpp:112] Iteration 36660, lr = 0.01
I0523 02:47:51.222285 34682 solver.cpp:239] Iteration 36670 (2.52053 iter/s, 3.96741s/10 iters), loss = 9.66044
I0523 02:47:51.222340 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.66044 (* 1 = 9.66044 loss)
I0523 02:47:52.058068 34682 sgd_solver.cpp:112] Iteration 36670, lr = 0.01
I0523 02:47:56.219367 34682 solver.cpp:239] Iteration 36680 (2.00127 iter/s, 4.99683s/10 iters), loss = 8.58662
I0523 02:47:56.219413 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58662 (* 1 = 8.58662 loss)
I0523 02:47:56.282160 34682 sgd_solver.cpp:112] Iteration 36680, lr = 0.01
I0523 02:48:01.012681 34682 solver.cpp:239] Iteration 36690 (2.08635 iter/s, 4.79307s/10 iters), loss = 9.66769
I0523 02:48:01.012730 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.66769 (* 1 = 9.66769 loss)
I0523 02:48:01.843297 34682 sgd_solver.cpp:112] Iteration 36690, lr = 0.01
I0523 02:48:03.656054 34682 solver.cpp:239] Iteration 36700 (3.78328 iter/s, 2.64321s/10 iters), loss = 9.71635
I0523 02:48:03.656111 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.71635 (* 1 = 9.71635 loss)
I0523 02:48:04.387265 34682 sgd_solver.cpp:112] Iteration 36700, lr = 0.01
I0523 02:48:08.600664 34682 solver.cpp:239] Iteration 36710 (2.02251 iter/s, 4.94435s/10 iters), loss = 9.02811
I0523 02:48:08.600709 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02811 (* 1 = 9.02811 loss)
I0523 02:48:08.671152 34682 sgd_solver.cpp:112] Iteration 36710, lr = 0.01
I0523 02:48:11.075703 34682 solver.cpp:239] Iteration 36720 (4.04059 iter/s, 2.47489s/10 iters), loss = 9.16891
I0523 02:48:11.075758 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16891 (* 1 = 9.16891 loss)
I0523 02:48:11.732359 34682 sgd_solver.cpp:112] Iteration 36720, lr = 0.01
I0523 02:48:17.229641 34682 solver.cpp:239] Iteration 36730 (1.62506 iter/s, 6.15363s/10 iters), loss = 8.58795
I0523 02:48:17.229991 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58795 (* 1 = 8.58795 loss)
I0523 02:48:17.428604 34682 sgd_solver.cpp:112] Iteration 36730, lr = 0.01
I0523 02:48:19.984431 34682 solver.cpp:239] Iteration 36740 (3.6306 iter/s, 2.75437s/10 iters), loss = 8.87271
I0523 02:48:19.984493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87271 (* 1 = 8.87271 loss)
I0523 02:48:20.821327 34682 sgd_solver.cpp:112] Iteration 36740, lr = 0.01
I0523 02:48:24.868104 34682 solver.cpp:239] Iteration 36750 (2.04958 iter/s, 4.87906s/10 iters), loss = 9.14076
I0523 02:48:24.868149 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14076 (* 1 = 9.14076 loss)
I0523 02:48:24.941586 34682 sgd_solver.cpp:112] Iteration 36750, lr = 0.01
I0523 02:48:29.363607 34682 solver.cpp:239] Iteration 36760 (2.22456 iter/s, 4.49527s/10 iters), loss = 8.77057
I0523 02:48:29.363654 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77057 (* 1 = 8.77057 loss)
I0523 02:48:29.431805 34682 sgd_solver.cpp:112] Iteration 36760, lr = 0.01
I0523 02:48:33.635807 34682 solver.cpp:239] Iteration 36770 (2.34084 iter/s, 4.27197s/10 iters), loss = 9.47502
I0523 02:48:33.635857 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47502 (* 1 = 9.47502 loss)
I0523 02:48:33.795984 34682 sgd_solver.cpp:112] Iteration 36770, lr = 0.01
I0523 02:48:37.743423 34682 solver.cpp:239] Iteration 36780 (2.43463 iter/s, 4.10739s/10 iters), loss = 9.07196
I0523 02:48:37.743471 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07196 (* 1 = 9.07196 loss)
I0523 02:48:37.817864 34682 sgd_solver.cpp:112] Iteration 36780, lr = 0.01
I0523 02:48:41.503824 34682 solver.cpp:239] Iteration 36790 (2.65944 iter/s, 3.76019s/10 iters), loss = 8.78506
I0523 02:48:41.503877 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78506 (* 1 = 8.78506 loss)
I0523 02:48:41.583828 34682 sgd_solver.cpp:112] Iteration 36790, lr = 0.01
I0523 02:48:46.939141 34682 solver.cpp:239] Iteration 36800 (1.83991 iter/s, 5.43504s/10 iters), loss = 8.73506
I0523 02:48:46.939190 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73506 (* 1 = 8.73506 loss)
I0523 02:48:47.012498 34682 sgd_solver.cpp:112] Iteration 36800, lr = 0.01
I0523 02:48:51.914988 34682 solver.cpp:239] Iteration 36810 (2.00981 iter/s, 4.97559s/10 iters), loss = 9.05664
I0523 02:48:51.915241 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05664 (* 1 = 9.05664 loss)
I0523 02:48:52.794036 34682 sgd_solver.cpp:112] Iteration 36810, lr = 0.01
I0523 02:48:56.766016 34682 solver.cpp:239] Iteration 36820 (2.0616 iter/s, 4.85059s/10 iters), loss = 8.8435
I0523 02:48:56.766069 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8435 (* 1 = 8.8435 loss)
I0523 02:48:56.833379 34682 sgd_solver.cpp:112] Iteration 36820, lr = 0.01
I0523 02:49:03.222712 34682 solver.cpp:239] Iteration 36830 (1.54886 iter/s, 6.45637s/10 iters), loss = 9.86125
I0523 02:49:03.222759 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.86125 (* 1 = 9.86125 loss)
I0523 02:49:03.294703 34682 sgd_solver.cpp:112] Iteration 36830, lr = 0.01
I0523 02:49:08.869832 34682 solver.cpp:239] Iteration 36840 (1.7709 iter/s, 5.64685s/10 iters), loss = 8.61619
I0523 02:49:08.869874 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61619 (* 1 = 8.61619 loss)
I0523 02:49:08.939374 34682 sgd_solver.cpp:112] Iteration 36840, lr = 0.01
I0523 02:49:14.578378 34682 solver.cpp:239] Iteration 36850 (1.75185 iter/s, 5.70827s/10 iters), loss = 8.61116
I0523 02:49:14.578441 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61116 (* 1 = 8.61116 loss)
I0523 02:49:15.416308 34682 sgd_solver.cpp:112] Iteration 36850, lr = 0.01
I0523 02:49:20.966919 34682 solver.cpp:239] Iteration 36860 (1.56538 iter/s, 6.38822s/10 iters), loss = 9.04806
I0523 02:49:20.966977 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04806 (* 1 = 9.04806 loss)
I0523 02:49:21.782416 34682 sgd_solver.cpp:112] Iteration 36860, lr = 0.01
I0523 02:49:26.646513 34682 solver.cpp:239] Iteration 36870 (1.76078 iter/s, 5.6793s/10 iters), loss = 8.39319
I0523 02:49:26.646778 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39319 (* 1 = 8.39319 loss)
I0523 02:49:26.694991 34682 sgd_solver.cpp:112] Iteration 36870, lr = 0.01
I0523 02:49:32.267022 34682 solver.cpp:239] Iteration 36880 (1.77935 iter/s, 5.62004s/10 iters), loss = 9.3118
I0523 02:49:32.267112 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3118 (* 1 = 9.3118 loss)
I0523 02:49:32.321902 34682 sgd_solver.cpp:112] Iteration 36880, lr = 0.01
I0523 02:49:39.306306 34682 solver.cpp:239] Iteration 36890 (1.42068 iter/s, 7.0389s/10 iters), loss = 9.03595
I0523 02:49:39.306375 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03595 (* 1 = 9.03595 loss)
I0523 02:49:40.051487 34682 sgd_solver.cpp:112] Iteration 36890, lr = 0.01
I0523 02:49:44.333765 34682 solver.cpp:239] Iteration 36900 (1.98919 iter/s, 5.02716s/10 iters), loss = 7.02858
I0523 02:49:44.333815 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.02858 (* 1 = 7.02858 loss)
I0523 02:49:44.397240 34682 sgd_solver.cpp:112] Iteration 36900, lr = 0.01
I0523 02:49:50.679160 34682 solver.cpp:239] Iteration 36910 (1.57602 iter/s, 6.34508s/10 iters), loss = 9.36909
I0523 02:49:50.679229 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36909 (* 1 = 9.36909 loss)
I0523 02:49:50.736001 34682 sgd_solver.cpp:112] Iteration 36910, lr = 0.01
I0523 02:49:54.395900 34682 solver.cpp:239] Iteration 36920 (2.69069 iter/s, 3.71652s/10 iters), loss = 8.79633
I0523 02:49:54.395943 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79633 (* 1 = 8.79633 loss)
I0523 02:49:54.457612 34682 sgd_solver.cpp:112] Iteration 36920, lr = 0.01
I0523 02:49:58.668264 34682 solver.cpp:239] Iteration 36930 (2.34074 iter/s, 4.27215s/10 iters), loss = 8.9664
I0523 02:49:58.668447 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9664 (* 1 = 8.9664 loss)
I0523 02:49:59.498811 34682 sgd_solver.cpp:112] Iteration 36930, lr = 0.01
I0523 02:50:04.770627 34682 solver.cpp:239] Iteration 36940 (1.63882 iter/s, 6.10196s/10 iters), loss = 9.59035
I0523 02:50:04.770674 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59035 (* 1 = 9.59035 loss)
I0523 02:50:04.833429 34682 sgd_solver.cpp:112] Iteration 36940, lr = 0.01
I0523 02:50:09.101850 34682 solver.cpp:239] Iteration 36950 (2.30895 iter/s, 4.33098s/10 iters), loss = 8.41916
I0523 02:50:09.101897 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41916 (* 1 = 8.41916 loss)
I0523 02:50:09.161703 34682 sgd_solver.cpp:112] Iteration 36950, lr = 0.01
I0523 02:50:13.767521 34682 solver.cpp:239] Iteration 36960 (2.14345 iter/s, 4.66538s/10 iters), loss = 9.46196
I0523 02:50:13.767570 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.46196 (* 1 = 9.46196 loss)
I0523 02:50:13.842418 34682 sgd_solver.cpp:112] Iteration 36960, lr = 0.01
I0523 02:50:19.548193 34682 solver.cpp:239] Iteration 36970 (1.72999 iter/s, 5.78038s/10 iters), loss = 8.91142
I0523 02:50:19.548244 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91142 (* 1 = 8.91142 loss)
I0523 02:50:19.616541 34682 sgd_solver.cpp:112] Iteration 36970, lr = 0.01
I0523 02:50:23.660508 34682 solver.cpp:239] Iteration 36980 (2.43185 iter/s, 4.11209s/10 iters), loss = 9.07631
I0523 02:50:23.660557 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07631 (* 1 = 9.07631 loss)
I0523 02:50:23.730221 34682 sgd_solver.cpp:112] Iteration 36980, lr = 0.01
I0523 02:50:29.473170 34682 solver.cpp:239] Iteration 36990 (1.72047 iter/s, 5.81238s/10 iters), loss = 9.13818
I0523 02:50:29.473384 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13818 (* 1 = 9.13818 loss)
I0523 02:50:29.536599 34682 sgd_solver.cpp:112] Iteration 36990, lr = 0.01
I0523 02:50:33.632066 34682 solver.cpp:239] Iteration 37000 (2.40469 iter/s, 4.15853s/10 iters), loss = 9.24794
I0523 02:50:33.632108 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24794 (* 1 = 9.24794 loss)
I0523 02:50:33.708398 34682 sgd_solver.cpp:112] Iteration 37000, lr = 0.01
I0523 02:50:39.755722 34682 solver.cpp:239] Iteration 37010 (1.63309 iter/s, 6.12335s/10 iters), loss = 9.11407
I0523 02:50:39.755784 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11407 (* 1 = 9.11407 loss)
I0523 02:50:40.619185 34682 sgd_solver.cpp:112] Iteration 37010, lr = 0.01
I0523 02:50:44.787086 34682 solver.cpp:239] Iteration 37020 (1.98764 iter/s, 5.0311s/10 iters), loss = 8.7997
I0523 02:50:44.787132 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7997 (* 1 = 8.7997 loss)
I0523 02:50:44.845265 34682 sgd_solver.cpp:112] Iteration 37020, lr = 0.01
I0523 02:50:49.552400 34682 solver.cpp:239] Iteration 37030 (2.09861 iter/s, 4.76507s/10 iters), loss = 9.10298
I0523 02:50:49.552454 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10298 (* 1 = 9.10298 loss)
I0523 02:50:50.383929 34682 sgd_solver.cpp:112] Iteration 37030, lr = 0.01
I0523 02:50:56.039819 34682 solver.cpp:239] Iteration 37040 (1.54152 iter/s, 6.48709s/10 iters), loss = 8.77619
I0523 02:50:56.039876 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77619 (* 1 = 8.77619 loss)
I0523 02:50:56.096418 34682 sgd_solver.cpp:112] Iteration 37040, lr = 0.01
I0523 02:51:01.618109 34682 solver.cpp:239] Iteration 37050 (1.79275 iter/s, 5.57801s/10 iters), loss = 9.14241
I0523 02:51:01.618340 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14241 (* 1 = 9.14241 loss)
I0523 02:51:01.693477 34682 sgd_solver.cpp:112] Iteration 37050, lr = 0.01
I0523 02:51:05.658675 34682 solver.cpp:239] Iteration 37060 (2.47513 iter/s, 4.0402s/10 iters), loss = 9.59798
I0523 02:51:05.658766 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59798 (* 1 = 9.59798 loss)
I0523 02:51:05.714742 34682 sgd_solver.cpp:112] Iteration 37060, lr = 0.01
I0523 02:51:11.205689 34682 solver.cpp:239] Iteration 37070 (1.80287 iter/s, 5.5467s/10 iters), loss = 9.62627
I0523 02:51:11.205746 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.62627 (* 1 = 9.62627 loss)
I0523 02:51:12.032209 34682 sgd_solver.cpp:112] Iteration 37070, lr = 0.01
I0523 02:51:17.932086 34682 solver.cpp:239] Iteration 37080 (1.48675 iter/s, 6.72607s/10 iters), loss = 8.81314
I0523 02:51:17.932142 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81314 (* 1 = 8.81314 loss)
I0523 02:51:18.404045 34682 sgd_solver.cpp:112] Iteration 37080, lr = 0.01
I0523 02:51:22.457813 34682 solver.cpp:239] Iteration 37090 (2.20971 iter/s, 4.52548s/10 iters), loss = 9.25341
I0523 02:51:22.457871 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25341 (* 1 = 9.25341 loss)
I0523 02:51:22.522635 34682 sgd_solver.cpp:112] Iteration 37090, lr = 0.01
I0523 02:51:27.758427 34682 solver.cpp:239] Iteration 37100 (1.88667 iter/s, 5.30034s/10 iters), loss = 9.57598
I0523 02:51:27.758484 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.57598 (* 1 = 9.57598 loss)
I0523 02:51:27.979159 34682 sgd_solver.cpp:112] Iteration 37100, lr = 0.01
I0523 02:51:32.914894 34682 solver.cpp:239] Iteration 37110 (1.93941 iter/s, 5.1562s/10 iters), loss = 8.47834
I0523 02:51:32.915141 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47834 (* 1 = 8.47834 loss)
I0523 02:51:33.653889 34682 sgd_solver.cpp:112] Iteration 37110, lr = 0.01
I0523 02:51:37.986022 34682 solver.cpp:239] Iteration 37120 (1.97211 iter/s, 5.0707s/10 iters), loss = 8.72911
I0523 02:51:37.986068 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72911 (* 1 = 8.72911 loss)
I0523 02:51:38.834110 34682 sgd_solver.cpp:112] Iteration 37120, lr = 0.01
I0523 02:51:44.819458 34682 solver.cpp:239] Iteration 37130 (1.46346 iter/s, 6.83311s/10 iters), loss = 9.48207
I0523 02:51:44.819530 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.48207 (* 1 = 9.48207 loss)
I0523 02:51:44.880937 34682 sgd_solver.cpp:112] Iteration 37130, lr = 0.01
I0523 02:51:50.430884 34682 solver.cpp:239] Iteration 37140 (1.78218 iter/s, 5.61111s/10 iters), loss = 9.09182
I0523 02:51:50.430974 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09182 (* 1 = 9.09182 loss)
I0523 02:51:51.252576 34682 sgd_solver.cpp:112] Iteration 37140, lr = 0.01
I0523 02:51:54.462456 34682 solver.cpp:239] Iteration 37150 (2.48057 iter/s, 4.03133s/10 iters), loss = 8.08017
I0523 02:51:54.462503 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08017 (* 1 = 8.08017 loss)
I0523 02:51:55.329471 34682 sgd_solver.cpp:112] Iteration 37150, lr = 0.01
I0523 02:52:01.453653 34682 solver.cpp:239] Iteration 37160 (1.43044 iter/s, 6.99087s/10 iters), loss = 9.06692
I0523 02:52:01.453699 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06692 (* 1 = 9.06692 loss)
I0523 02:52:01.522253 34682 sgd_solver.cpp:112] Iteration 37160, lr = 0.01
I0523 02:52:06.035497 34682 solver.cpp:239] Iteration 37170 (2.18264 iter/s, 4.58161s/10 iters), loss = 7.74343
I0523 02:52:06.035717 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74343 (* 1 = 7.74343 loss)
I0523 02:52:06.101145 34682 sgd_solver.cpp:112] Iteration 37170, lr = 0.01
I0523 02:52:11.731988 34682 solver.cpp:239] Iteration 37180 (1.75561 iter/s, 5.69603s/10 iters), loss = 8.85756
I0523 02:52:11.732077 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85756 (* 1 = 8.85756 loss)
I0523 02:52:12.581424 34682 sgd_solver.cpp:112] Iteration 37180, lr = 0.01
I0523 02:52:19.969733 34682 solver.cpp:239] Iteration 37190 (1.21399 iter/s, 8.23733s/10 iters), loss = 10.1332
I0523 02:52:19.969790 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1332 (* 1 = 10.1332 loss)
I0523 02:52:20.043524 34682 sgd_solver.cpp:112] Iteration 37190, lr = 0.01
I0523 02:52:24.846328 34682 solver.cpp:239] Iteration 37200 (2.05072 iter/s, 4.87635s/10 iters), loss = 9.9124
I0523 02:52:24.846379 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.9124 (* 1 = 9.9124 loss)
I0523 02:52:25.347122 34682 sgd_solver.cpp:112] Iteration 37200, lr = 0.01
I0523 02:52:31.169279 34682 solver.cpp:239] Iteration 37210 (1.58162 iter/s, 6.32264s/10 iters), loss = 9.34456
I0523 02:52:31.169350 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34456 (* 1 = 9.34456 loss)
I0523 02:52:31.767588 34682 sgd_solver.cpp:112] Iteration 37210, lr = 0.01
I0523 02:52:36.571362 34682 solver.cpp:239] Iteration 37220 (1.85148 iter/s, 5.40109s/10 iters), loss = 8.71103
I0523 02:52:36.571589 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71103 (* 1 = 8.71103 loss)
I0523 02:52:37.429994 34682 sgd_solver.cpp:112] Iteration 37220, lr = 0.01
I0523 02:52:42.354023 34682 solver.cpp:239] Iteration 37230 (1.72944 iter/s, 5.78222s/10 iters), loss = 9.13239
I0523 02:52:42.354074 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13239 (* 1 = 9.13239 loss)
I0523 02:52:42.426126 34682 sgd_solver.cpp:112] Iteration 37230, lr = 0.01
I0523 02:52:48.870265 34682 solver.cpp:239] Iteration 37240 (1.5347 iter/s, 6.51593s/10 iters), loss = 8.39694
I0523 02:52:48.870326 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39694 (* 1 = 8.39694 loss)
I0523 02:52:48.953675 34682 sgd_solver.cpp:112] Iteration 37240, lr = 0.01
I0523 02:52:51.912510 34682 solver.cpp:239] Iteration 37250 (3.28726 iter/s, 3.04205s/10 iters), loss = 9.22541
I0523 02:52:51.912559 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22541 (* 1 = 9.22541 loss)
I0523 02:52:51.994529 34682 sgd_solver.cpp:112] Iteration 37250, lr = 0.01
I0523 02:52:57.534747 34682 solver.cpp:239] Iteration 37260 (1.77874 iter/s, 5.62196s/10 iters), loss = 8.84278
I0523 02:52:57.534808 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84278 (* 1 = 8.84278 loss)
I0523 02:52:58.215351 34682 sgd_solver.cpp:112] Iteration 37260, lr = 0.01
I0523 02:53:03.129020 34682 solver.cpp:239] Iteration 37270 (1.78763 iter/s, 5.59399s/10 iters), loss = 8.7218
I0523 02:53:03.129065 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7218 (* 1 = 8.7218 loss)
I0523 02:53:03.194025 34682 sgd_solver.cpp:112] Iteration 37270, lr = 0.01
I0523 02:53:06.476528 34682 solver.cpp:239] Iteration 37280 (2.98748 iter/s, 3.3473s/10 iters), loss = 8.40862
I0523 02:53:06.476588 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40862 (* 1 = 8.40862 loss)
I0523 02:53:06.903187 34682 sgd_solver.cpp:112] Iteration 37280, lr = 0.01
I0523 02:53:11.197049 34682 solver.cpp:239] Iteration 37290 (2.11853 iter/s, 4.72025s/10 iters), loss = 9.37527
I0523 02:53:11.197129 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.37527 (* 1 = 9.37527 loss)
I0523 02:53:12.009608 34682 sgd_solver.cpp:112] Iteration 37290, lr = 0.01
I0523 02:53:16.154326 34682 solver.cpp:239] Iteration 37300 (2.01735 iter/s, 4.957s/10 iters), loss = 9.0605
I0523 02:53:16.154386 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0605 (* 1 = 9.0605 loss)
I0523 02:53:16.985126 34682 sgd_solver.cpp:112] Iteration 37300, lr = 0.01
I0523 02:53:22.249174 34682 solver.cpp:239] Iteration 37310 (1.64081 iter/s, 6.09454s/10 iters), loss = 8.46951
I0523 02:53:22.249223 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46951 (* 1 = 8.46951 loss)
I0523 02:53:23.013387 34682 sgd_solver.cpp:112] Iteration 37310, lr = 0.01
I0523 02:53:28.660079 34682 solver.cpp:239] Iteration 37320 (1.55992 iter/s, 6.41059s/10 iters), loss = 8.40996
I0523 02:53:28.660140 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40996 (* 1 = 8.40996 loss)
I0523 02:53:29.368857 34682 sgd_solver.cpp:112] Iteration 37320, lr = 0.01
I0523 02:53:32.101141 34682 solver.cpp:239] Iteration 37330 (2.90625 iter/s, 3.44086s/10 iters), loss = 8.68312
I0523 02:53:32.101188 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68312 (* 1 = 8.68312 loss)
I0523 02:53:32.976181 34682 sgd_solver.cpp:112] Iteration 37330, lr = 0.01
I0523 02:53:37.659041 34682 solver.cpp:239] Iteration 37340 (1.79933 iter/s, 5.55763s/10 iters), loss = 8.76263
I0523 02:53:37.659279 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76263 (* 1 = 8.76263 loss)
I0523 02:53:37.721099 34682 sgd_solver.cpp:112] Iteration 37340, lr = 0.01
I0523 02:53:40.604142 34682 solver.cpp:239] Iteration 37350 (3.39586 iter/s, 2.94476s/10 iters), loss = 8.8334
I0523 02:53:40.604208 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8334 (* 1 = 8.8334 loss)
I0523 02:53:41.386054 34682 sgd_solver.cpp:112] Iteration 37350, lr = 0.01
I0523 02:53:47.237459 34682 solver.cpp:239] Iteration 37360 (1.50762 iter/s, 6.63297s/10 iters), loss = 8.77723
I0523 02:53:47.237515 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77723 (* 1 = 8.77723 loss)
I0523 02:53:47.950759 34682 sgd_solver.cpp:112] Iteration 37360, lr = 0.01
I0523 02:53:51.232583 34682 solver.cpp:239] Iteration 37370 (2.5032 iter/s, 3.99489s/10 iters), loss = 8.57026
I0523 02:53:51.232627 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57026 (* 1 = 8.57026 loss)
I0523 02:53:51.302129 34682 sgd_solver.cpp:112] Iteration 37370, lr = 0.01
I0523 02:53:55.540642 34682 solver.cpp:239] Iteration 37380 (2.32136 iter/s, 4.30782s/10 iters), loss = 9.31472
I0523 02:53:55.540697 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31472 (* 1 = 9.31472 loss)
I0523 02:53:56.408684 34682 sgd_solver.cpp:112] Iteration 37380, lr = 0.01
I0523 02:54:00.349962 34682 solver.cpp:239] Iteration 37390 (2.0794 iter/s, 4.80907s/10 iters), loss = 9.46761
I0523 02:54:00.350004 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.46761 (* 1 = 9.46761 loss)
I0523 02:54:00.424971 34682 sgd_solver.cpp:112] Iteration 37390, lr = 0.01
I0523 02:54:04.865732 34682 solver.cpp:239] Iteration 37400 (2.21458 iter/s, 4.51554s/10 iters), loss = 9.20968
I0523 02:54:04.865787 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20968 (* 1 = 9.20968 loss)
I0523 02:54:05.623899 34682 sgd_solver.cpp:112] Iteration 37400, lr = 0.01
I0523 02:54:09.089485 34682 solver.cpp:239] Iteration 37410 (2.36769 iter/s, 4.22353s/10 iters), loss = 8.93782
I0523 02:54:09.089727 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93782 (* 1 = 8.93782 loss)
I0523 02:54:09.162714 34682 sgd_solver.cpp:112] Iteration 37410, lr = 0.01
I0523 02:54:15.812569 34682 solver.cpp:239] Iteration 37420 (1.48752 iter/s, 6.72258s/10 iters), loss = 9.13901
I0523 02:54:15.812642 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13901 (* 1 = 9.13901 loss)
I0523 02:54:15.886457 34682 sgd_solver.cpp:112] Iteration 37420, lr = 0.01
I0523 02:54:18.419543 34682 solver.cpp:239] Iteration 37430 (3.83612 iter/s, 2.6068s/10 iters), loss = 9.83274
I0523 02:54:18.419587 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.83274 (* 1 = 9.83274 loss)
I0523 02:54:18.503412 34682 sgd_solver.cpp:112] Iteration 37430, lr = 0.01
I0523 02:54:23.957551 34682 solver.cpp:239] Iteration 37440 (1.80579 iter/s, 5.53773s/10 iters), loss = 8.94892
I0523 02:54:23.957602 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94892 (* 1 = 8.94892 loss)
I0523 02:54:24.015841 34682 sgd_solver.cpp:112] Iteration 37440, lr = 0.01
I0523 02:54:26.570997 34682 solver.cpp:239] Iteration 37450 (3.82661 iter/s, 2.61328s/10 iters), loss = 9.01538
I0523 02:54:26.571053 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01538 (* 1 = 9.01538 loss)
I0523 02:54:26.721901 34682 sgd_solver.cpp:112] Iteration 37450, lr = 0.01
I0523 02:54:30.121615 34682 solver.cpp:239] Iteration 37460 (2.81657 iter/s, 3.55042s/10 iters), loss = 9.0171
I0523 02:54:30.121672 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0171 (* 1 = 9.0171 loss)
I0523 02:54:30.583490 34682 sgd_solver.cpp:112] Iteration 37460, lr = 0.01
I0523 02:54:34.554950 34682 solver.cpp:239] Iteration 37470 (2.25576 iter/s, 4.43309s/10 iters), loss = 8.59302
I0523 02:54:34.555016 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59302 (* 1 = 8.59302 loss)
I0523 02:54:35.428369 34682 sgd_solver.cpp:112] Iteration 37470, lr = 0.01
I0523 02:54:40.149029 34682 solver.cpp:239] Iteration 37480 (1.7877 iter/s, 5.59378s/10 iters), loss = 8.79809
I0523 02:54:40.149236 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79809 (* 1 = 8.79809 loss)
I0523 02:54:40.976969 34682 sgd_solver.cpp:112] Iteration 37480, lr = 0.01
I0523 02:54:44.290727 34682 solver.cpp:239] Iteration 37490 (2.41469 iter/s, 4.14132s/10 iters), loss = 8.10073
I0523 02:54:44.290776 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10073 (* 1 = 8.10073 loss)
I0523 02:54:45.105947 34682 sgd_solver.cpp:112] Iteration 37490, lr = 0.01
I0523 02:54:50.881155 34682 solver.cpp:239] Iteration 37500 (1.51743 iter/s, 6.59011s/10 iters), loss = 9.28425
I0523 02:54:50.881222 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28425 (* 1 = 9.28425 loss)
I0523 02:54:50.952374 34682 sgd_solver.cpp:112] Iteration 37500, lr = 0.01
I0523 02:54:56.416273 34682 solver.cpp:239] Iteration 37510 (1.80674 iter/s, 5.53483s/10 iters), loss = 9.25139
I0523 02:54:56.416314 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25139 (* 1 = 9.25139 loss)
I0523 02:54:57.259178 34682 sgd_solver.cpp:112] Iteration 37510, lr = 0.01
I0523 02:55:01.471436 34682 solver.cpp:239] Iteration 37520 (1.97828 iter/s, 5.0549s/10 iters), loss = 8.28896
I0523 02:55:01.471485 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28896 (* 1 = 8.28896 loss)
I0523 02:55:01.535121 34682 sgd_solver.cpp:112] Iteration 37520, lr = 0.01
I0523 02:55:06.923763 34682 solver.cpp:239] Iteration 37530 (1.83417 iter/s, 5.45205s/10 iters), loss = 9.47449
I0523 02:55:06.923820 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47449 (* 1 = 9.47449 loss)
I0523 02:55:07.736248 34682 sgd_solver.cpp:112] Iteration 37530, lr = 0.01
I0523 02:55:13.421245 34682 solver.cpp:239] Iteration 37540 (1.53913 iter/s, 6.49716s/10 iters), loss = 8.91603
I0523 02:55:13.421515 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91603 (* 1 = 8.91603 loss)
I0523 02:55:14.205279 34682 sgd_solver.cpp:112] Iteration 37540, lr = 0.01
I0523 02:55:20.114733 34682 solver.cpp:239] Iteration 37550 (1.4941 iter/s, 6.69298s/10 iters), loss = 9.06974
I0523 02:55:20.114790 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06974 (* 1 = 9.06974 loss)
I0523 02:55:20.580060 34682 sgd_solver.cpp:112] Iteration 37550, lr = 0.01
I0523 02:55:25.297674 34682 solver.cpp:239] Iteration 37560 (1.92951 iter/s, 5.18267s/10 iters), loss = 8.22719
I0523 02:55:25.297735 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22719 (* 1 = 8.22719 loss)
I0523 02:55:26.077802 34682 sgd_solver.cpp:112] Iteration 37560, lr = 0.01
I0523 02:55:32.672315 34682 solver.cpp:239] Iteration 37570 (1.35606 iter/s, 7.37428s/10 iters), loss = 8.67359
I0523 02:55:32.672366 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67359 (* 1 = 8.67359 loss)
I0523 02:55:32.728499 34682 sgd_solver.cpp:112] Iteration 37570, lr = 0.01
I0523 02:55:38.204327 34682 solver.cpp:239] Iteration 37580 (1.80775 iter/s, 5.53173s/10 iters), loss = 9.69563
I0523 02:55:38.204375 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.69563 (* 1 = 9.69563 loss)
I0523 02:55:38.276736 34682 sgd_solver.cpp:112] Iteration 37580, lr = 0.01
I0523 02:55:42.981654 34682 solver.cpp:239] Iteration 37590 (2.09333 iter/s, 4.77709s/10 iters), loss = 9.70566
I0523 02:55:42.981708 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.70566 (* 1 = 9.70566 loss)
I0523 02:55:43.067270 34682 sgd_solver.cpp:112] Iteration 37590, lr = 0.01
I0523 02:55:45.912369 34682 solver.cpp:239] Iteration 37600 (3.41235 iter/s, 2.93053s/10 iters), loss = 9.04586
I0523 02:55:45.912569 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04586 (* 1 = 9.04586 loss)
I0523 02:55:46.553123 34682 sgd_solver.cpp:112] Iteration 37600, lr = 0.01
I0523 02:55:52.196362 34682 solver.cpp:239] Iteration 37610 (1.59145 iter/s, 6.28356s/10 iters), loss = 9.11679
I0523 02:55:52.196421 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11679 (* 1 = 9.11679 loss)
I0523 02:55:53.016898 34682 sgd_solver.cpp:112] Iteration 37610, lr = 0.01
I0523 02:55:58.703605 34682 solver.cpp:239] Iteration 37620 (1.53683 iter/s, 6.50692s/10 iters), loss = 8.7005
I0523 02:55:58.703652 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7005 (* 1 = 8.7005 loss)
I0523 02:55:58.779628 34682 sgd_solver.cpp:112] Iteration 37620, lr = 0.01
I0523 02:56:02.426749 34682 solver.cpp:239] Iteration 37630 (2.68608 iter/s, 3.7229s/10 iters), loss = 8.57439
I0523 02:56:02.426808 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57439 (* 1 = 8.57439 loss)
I0523 02:56:02.667141 34682 sgd_solver.cpp:112] Iteration 37630, lr = 0.01
I0523 02:56:06.872299 34682 solver.cpp:239] Iteration 37640 (2.24957 iter/s, 4.4453s/10 iters), loss = 9.14945
I0523 02:56:06.872360 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14945 (* 1 = 9.14945 loss)
I0523 02:56:07.639339 34682 sgd_solver.cpp:112] Iteration 37640, lr = 0.01
I0523 02:56:11.270246 34682 solver.cpp:239] Iteration 37650 (2.27391 iter/s, 4.3977s/10 iters), loss = 8.61671
I0523 02:56:11.270310 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61671 (* 1 = 8.61671 loss)
I0523 02:56:11.803268 34682 sgd_solver.cpp:112] Iteration 37650, lr = 0.01
I0523 02:56:16.117508 34682 solver.cpp:239] Iteration 37660 (2.06314 iter/s, 4.84699s/10 iters), loss = 8.40634
I0523 02:56:16.117758 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40634 (* 1 = 8.40634 loss)
I0523 02:56:16.947929 34682 sgd_solver.cpp:112] Iteration 37660, lr = 0.01
I0523 02:56:20.276351 34682 solver.cpp:239] Iteration 37670 (2.40475 iter/s, 4.15844s/10 iters), loss = 9.09792
I0523 02:56:20.276399 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09792 (* 1 = 9.09792 loss)
I0523 02:56:20.349196 34682 sgd_solver.cpp:112] Iteration 37670, lr = 0.01
I0523 02:56:24.445194 34682 solver.cpp:239] Iteration 37680 (2.39888 iter/s, 4.16862s/10 iters), loss = 8.81569
I0523 02:56:24.445255 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81569 (* 1 = 8.81569 loss)
I0523 02:56:24.934072 34682 sgd_solver.cpp:112] Iteration 37680, lr = 0.01
I0523 02:56:30.510550 34682 solver.cpp:239] Iteration 37690 (1.64879 iter/s, 6.06504s/10 iters), loss = 8.28638
I0523 02:56:30.510615 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28638 (* 1 = 8.28638 loss)
I0523 02:56:30.584321 34682 sgd_solver.cpp:112] Iteration 37690, lr = 0.01
I0523 02:56:33.158320 34682 solver.cpp:239] Iteration 37700 (3.77701 iter/s, 2.6476s/10 iters), loss = 8.85263
I0523 02:56:33.158375 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85263 (* 1 = 8.85263 loss)
I0523 02:56:33.221678 34682 sgd_solver.cpp:112] Iteration 37700, lr = 0.01
I0523 02:56:38.935807 34682 solver.cpp:239] Iteration 37710 (1.73094 iter/s, 5.77719s/10 iters), loss = 8.38085
I0523 02:56:38.935868 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38085 (* 1 = 8.38085 loss)
I0523 02:56:39.129137 34682 sgd_solver.cpp:112] Iteration 37710, lr = 0.01
I0523 02:56:42.241003 34682 solver.cpp:239] Iteration 37720 (3.02572 iter/s, 3.305s/10 iters), loss = 8.51153
I0523 02:56:42.241053 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51153 (* 1 = 8.51153 loss)
I0523 02:56:42.309587 34682 sgd_solver.cpp:112] Iteration 37720, lr = 0.01
I0523 02:56:47.122287 34682 solver.cpp:239] Iteration 37730 (2.04875 iter/s, 4.88102s/10 iters), loss = 8.96357
I0523 02:56:47.122553 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96357 (* 1 = 8.96357 loss)
I0523 02:56:47.206295 34682 sgd_solver.cpp:112] Iteration 37730, lr = 0.01
I0523 02:56:51.340986 34682 solver.cpp:239] Iteration 37740 (2.37063 iter/s, 4.21829s/10 iters), loss = 8.76225
I0523 02:56:51.341033 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76225 (* 1 = 8.76225 loss)
I0523 02:56:52.126099 34682 sgd_solver.cpp:112] Iteration 37740, lr = 0.01
I0523 02:56:54.723141 34682 solver.cpp:239] Iteration 37750 (2.95688 iter/s, 3.38194s/10 iters), loss = 8.10199
I0523 02:56:54.723201 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10199 (* 1 = 8.10199 loss)
I0523 02:56:55.520619 34682 sgd_solver.cpp:112] Iteration 37750, lr = 0.01
I0523 02:57:00.177533 34682 solver.cpp:239] Iteration 37760 (1.83348 iter/s, 5.45411s/10 iters), loss = 8.67896
I0523 02:57:00.177585 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67896 (* 1 = 8.67896 loss)
I0523 02:57:00.941504 34682 sgd_solver.cpp:112] Iteration 37760, lr = 0.01
I0523 02:57:05.520025 34682 solver.cpp:239] Iteration 37770 (1.87188 iter/s, 5.34223s/10 iters), loss = 8.3709
I0523 02:57:05.520059 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3709 (* 1 = 8.3709 loss)
I0523 02:57:05.598549 34682 sgd_solver.cpp:112] Iteration 37770, lr = 0.01
I0523 02:57:09.709748 34682 solver.cpp:239] Iteration 37780 (2.38691 iter/s, 4.18951s/10 iters), loss = 9.53825
I0523 02:57:09.709805 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.53825 (* 1 = 9.53825 loss)
I0523 02:57:10.534050 34682 sgd_solver.cpp:112] Iteration 37780, lr = 0.01
I0523 02:57:16.698302 34682 solver.cpp:239] Iteration 37790 (1.43098 iter/s, 6.9882s/10 iters), loss = 8.95421
I0523 02:57:16.698356 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95421 (* 1 = 8.95421 loss)
I0523 02:57:16.764104 34682 sgd_solver.cpp:112] Iteration 37790, lr = 0.01
I0523 02:57:22.163074 34682 solver.cpp:239] Iteration 37800 (1.82999 iter/s, 5.4645s/10 iters), loss = 9.26113
I0523 02:57:22.163269 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.26113 (* 1 = 9.26113 loss)
I0523 02:57:22.233579 34682 sgd_solver.cpp:112] Iteration 37800, lr = 0.01
I0523 02:57:27.616639 34682 solver.cpp:239] Iteration 37810 (1.8338 iter/s, 5.45317s/10 iters), loss = 8.51939
I0523 02:57:27.616696 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51939 (* 1 = 8.51939 loss)
I0523 02:57:27.923189 34682 sgd_solver.cpp:112] Iteration 37810, lr = 0.01
I0523 02:57:30.624361 34682 solver.cpp:239] Iteration 37820 (3.32497 iter/s, 3.00754s/10 iters), loss = 9.57609
I0523 02:57:30.624410 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.57609 (* 1 = 9.57609 loss)
I0523 02:57:31.395752 34682 sgd_solver.cpp:112] Iteration 37820, lr = 0.01
I0523 02:57:34.837010 34682 solver.cpp:239] Iteration 37830 (2.37393 iter/s, 4.21243s/10 iters), loss = 8.43934
I0523 02:57:34.837054 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43934 (* 1 = 8.43934 loss)
I0523 02:57:34.910015 34682 sgd_solver.cpp:112] Iteration 37830, lr = 0.01
I0523 02:57:40.463112 34682 solver.cpp:239] Iteration 37840 (1.77752 iter/s, 5.62583s/10 iters), loss = 8.01438
I0523 02:57:40.463177 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01438 (* 1 = 8.01438 loss)
I0523 02:57:41.313315 34682 sgd_solver.cpp:112] Iteration 37840, lr = 0.01
I0523 02:57:47.401536 34682 solver.cpp:239] Iteration 37850 (1.44132 iter/s, 6.93808s/10 iters), loss = 8.94907
I0523 02:57:47.401582 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94907 (* 1 = 8.94907 loss)
I0523 02:57:48.224759 34682 sgd_solver.cpp:112] Iteration 37850, lr = 0.01
I0523 02:57:52.720293 34682 solver.cpp:239] Iteration 37860 (1.88023 iter/s, 5.31849s/10 iters), loss = 9.53273
I0523 02:57:52.720427 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.53273 (* 1 = 9.53273 loss)
I0523 02:57:53.574209 34682 sgd_solver.cpp:112] Iteration 37860, lr = 0.01
I0523 02:57:57.678122 34682 solver.cpp:239] Iteration 37870 (2.01715 iter/s, 4.95749s/10 iters), loss = 9.12382
I0523 02:57:57.678169 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12382 (* 1 = 9.12382 loss)
I0523 02:57:57.749939 34682 sgd_solver.cpp:112] Iteration 37870, lr = 0.01
I0523 02:58:01.146569 34682 solver.cpp:239] Iteration 37880 (2.88329 iter/s, 3.46826s/10 iters), loss = 8.10954
I0523 02:58:01.146611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10954 (* 1 = 8.10954 loss)
I0523 02:58:01.203634 34682 sgd_solver.cpp:112] Iteration 37880, lr = 0.01
I0523 02:58:05.638324 34682 solver.cpp:239] Iteration 37890 (2.22642 iter/s, 4.49152s/10 iters), loss = 8.31969
I0523 02:58:05.638375 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31969 (* 1 = 8.31969 loss)
I0523 02:58:05.697700 34682 sgd_solver.cpp:112] Iteration 37890, lr = 0.01
I0523 02:58:12.587347 34682 solver.cpp:239] Iteration 37900 (1.43912 iter/s, 6.94868s/10 iters), loss = 8.48319
I0523 02:58:12.587406 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48319 (* 1 = 8.48319 loss)
I0523 02:58:12.656301 34682 sgd_solver.cpp:112] Iteration 37900, lr = 0.01
I0523 02:58:16.971050 34682 solver.cpp:239] Iteration 37910 (2.2813 iter/s, 4.38347s/10 iters), loss = 9.02797
I0523 02:58:16.971101 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02797 (* 1 = 9.02797 loss)
I0523 02:58:17.778245 34682 sgd_solver.cpp:112] Iteration 37910, lr = 0.01
I0523 02:58:22.283687 34682 solver.cpp:239] Iteration 37920 (1.8824 iter/s, 5.31237s/10 iters), loss = 8.81963
I0523 02:58:22.283756 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81963 (* 1 = 8.81963 loss)
I0523 02:58:23.023387 34682 sgd_solver.cpp:112] Iteration 37920, lr = 0.01
I0523 02:58:27.362063 34682 solver.cpp:239] Iteration 37930 (1.96924 iter/s, 5.0781s/10 iters), loss = 9.39859
I0523 02:58:27.362118 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.39859 (* 1 = 9.39859 loss)
I0523 02:58:28.201586 34682 sgd_solver.cpp:112] Iteration 37930, lr = 0.01
I0523 02:58:32.727218 34682 solver.cpp:239] Iteration 37940 (1.86397 iter/s, 5.36489s/10 iters), loss = 9.77793
I0523 02:58:32.727262 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.77793 (* 1 = 9.77793 loss)
I0523 02:58:32.804379 34682 sgd_solver.cpp:112] Iteration 37940, lr = 0.01
I0523 02:58:38.510766 34682 solver.cpp:239] Iteration 37950 (1.72913 iter/s, 5.78327s/10 iters), loss = 9.51583
I0523 02:58:38.510823 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51583 (* 1 = 9.51583 loss)
I0523 02:58:39.379627 34682 sgd_solver.cpp:112] Iteration 37950, lr = 0.01
I0523 02:58:41.928088 34682 solver.cpp:239] Iteration 37960 (2.92643 iter/s, 3.41713s/10 iters), loss = 8.37013
I0523 02:58:41.928128 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37013 (* 1 = 8.37013 loss)
I0523 02:58:42.001052 34682 sgd_solver.cpp:112] Iteration 37960, lr = 0.01
I0523 02:58:48.114477 34682 solver.cpp:239] Iteration 37970 (1.61653 iter/s, 6.18609s/10 iters), loss = 8.71897
I0523 02:58:48.114544 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71897 (* 1 = 8.71897 loss)
I0523 02:58:48.949122 34682 sgd_solver.cpp:112] Iteration 37970, lr = 0.01
I0523 02:58:54.311784 34682 solver.cpp:239] Iteration 37980 (1.61369 iter/s, 6.19699s/10 iters), loss = 8.18842
I0523 02:58:54.312003 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18842 (* 1 = 8.18842 loss)
I0523 02:58:55.053117 34682 sgd_solver.cpp:112] Iteration 37980, lr = 0.01
I0523 02:58:59.225289 34682 solver.cpp:239] Iteration 37990 (2.03537 iter/s, 4.9131s/10 iters), loss = 9.02354
I0523 02:58:59.225339 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02354 (* 1 = 9.02354 loss)
I0523 02:58:59.305791 34682 sgd_solver.cpp:112] Iteration 37990, lr = 0.01
I0523 02:59:03.195998 34682 solver.cpp:239] Iteration 38000 (2.51858 iter/s, 3.97049s/10 iters), loss = 8.95068
I0523 02:59:03.196053 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95068 (* 1 = 8.95068 loss)
I0523 02:59:03.966809 34682 sgd_solver.cpp:112] Iteration 38000, lr = 0.01
I0523 02:59:07.541720 34682 solver.cpp:239] Iteration 38010 (2.30125 iter/s, 4.34547s/10 iters), loss = 9.03963
I0523 02:59:07.541792 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03963 (* 1 = 9.03963 loss)
I0523 02:59:08.342408 34682 sgd_solver.cpp:112] Iteration 38010, lr = 0.01
I0523 02:59:13.034266 34682 solver.cpp:239] Iteration 38020 (1.82075 iter/s, 5.49225s/10 iters), loss = 8.84681
I0523 02:59:13.034312 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84681 (* 1 = 8.84681 loss)
I0523 02:59:13.743206 34682 sgd_solver.cpp:112] Iteration 38020, lr = 0.01
I0523 02:59:17.602430 34682 solver.cpp:239] Iteration 38030 (2.18918 iter/s, 4.56793s/10 iters), loss = 8.85534
I0523 02:59:17.602486 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85534 (* 1 = 8.85534 loss)
I0523 02:59:18.473139 34682 sgd_solver.cpp:112] Iteration 38030, lr = 0.01
I0523 02:59:24.271353 34682 solver.cpp:239] Iteration 38040 (1.49957 iter/s, 6.6686s/10 iters), loss = 9.01744
I0523 02:59:24.271409 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01744 (* 1 = 9.01744 loss)
I0523 02:59:25.083950 34682 sgd_solver.cpp:112] Iteration 38040, lr = 0.01
I0523 02:59:27.852259 34682 solver.cpp:239] Iteration 38050 (2.79275 iter/s, 3.5807s/10 iters), loss = 8.06366
I0523 02:59:27.852330 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06366 (* 1 = 8.06366 loss)
I0523 02:59:28.584826 34682 sgd_solver.cpp:112] Iteration 38050, lr = 0.01
I0523 02:59:32.716481 34682 solver.cpp:239] Iteration 38060 (2.05594 iter/s, 4.86395s/10 iters), loss = 8.28684
I0523 02:59:32.716531 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28684 (* 1 = 8.28684 loss)
I0523 02:59:32.800797 34682 sgd_solver.cpp:112] Iteration 38060, lr = 0.01
I0523 02:59:36.690585 34682 solver.cpp:239] Iteration 38070 (2.51642 iter/s, 3.9739s/10 iters), loss = 8.69333
I0523 02:59:36.690625 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69333 (* 1 = 8.69333 loss)
I0523 02:59:36.759742 34682 sgd_solver.cpp:112] Iteration 38070, lr = 0.01
I0523 02:59:41.761528 34682 solver.cpp:239] Iteration 38080 (1.97212 iter/s, 5.07068s/10 iters), loss = 9.14751
I0523 02:59:41.761600 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14751 (* 1 = 9.14751 loss)
I0523 02:59:41.816409 34682 sgd_solver.cpp:112] Iteration 38080, lr = 0.01
I0523 02:59:47.567119 34682 solver.cpp:239] Iteration 38090 (1.72257 iter/s, 5.80529s/10 iters), loss = 8.68863
I0523 02:59:47.567171 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68863 (* 1 = 8.68863 loss)
I0523 02:59:48.308558 34682 sgd_solver.cpp:112] Iteration 38090, lr = 0.01
I0523 02:59:52.961526 34682 solver.cpp:239] Iteration 38100 (1.85387 iter/s, 5.39413s/10 iters), loss = 8.41538
I0523 02:59:52.961587 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41538 (* 1 = 8.41538 loss)
I0523 02:59:53.834225 34682 sgd_solver.cpp:112] Iteration 38100, lr = 0.01
I0523 02:59:58.837172 34682 solver.cpp:239] Iteration 38110 (1.70203 iter/s, 5.87535s/10 iters), loss = 8.45893
I0523 02:59:58.837345 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45893 (* 1 = 8.45893 loss)
I0523 02:59:58.903174 34682 sgd_solver.cpp:112] Iteration 38110, lr = 0.01
I0523 03:00:03.121265 34682 solver.cpp:239] Iteration 38120 (2.3344 iter/s, 4.28375s/10 iters), loss = 8.50106
I0523 03:00:03.121327 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50106 (* 1 = 8.50106 loss)
I0523 03:00:03.839670 34682 sgd_solver.cpp:112] Iteration 38120, lr = 0.01
I0523 03:00:08.765771 34682 solver.cpp:239] Iteration 38130 (1.77173 iter/s, 5.6442s/10 iters), loss = 9.66929
I0523 03:00:08.765830 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.66929 (* 1 = 9.66929 loss)
I0523 03:00:09.599678 34682 sgd_solver.cpp:112] Iteration 38130, lr = 0.01
I0523 03:00:14.296918 34682 solver.cpp:239] Iteration 38140 (1.80804 iter/s, 5.53086s/10 iters), loss = 8.30662
I0523 03:00:14.296967 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30662 (* 1 = 8.30662 loss)
I0523 03:00:14.376490 34682 sgd_solver.cpp:112] Iteration 38140, lr = 0.01
I0523 03:00:18.399533 34682 solver.cpp:239] Iteration 38150 (2.43761 iter/s, 4.10239s/10 iters), loss = 9.17499
I0523 03:00:18.399587 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17499 (* 1 = 9.17499 loss)
I0523 03:00:19.059412 34682 sgd_solver.cpp:112] Iteration 38150, lr = 0.01
I0523 03:00:23.476881 34682 solver.cpp:239] Iteration 38160 (1.96963 iter/s, 5.07709s/10 iters), loss = 9.10631
I0523 03:00:23.476927 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10631 (* 1 = 9.10631 loss)
I0523 03:00:23.557974 34682 sgd_solver.cpp:112] Iteration 38160, lr = 0.01
I0523 03:00:27.767267 34682 solver.cpp:239] Iteration 38170 (2.33092 iter/s, 4.29015s/10 iters), loss = 8.85249
I0523 03:00:27.767316 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85249 (* 1 = 8.85249 loss)
I0523 03:00:27.836904 34682 sgd_solver.cpp:112] Iteration 38170, lr = 0.01
I0523 03:00:32.414714 34682 solver.cpp:239] Iteration 38180 (2.15184 iter/s, 4.64718s/10 iters), loss = 8.74166
I0523 03:00:32.415030 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74166 (* 1 = 8.74166 loss)
I0523 03:00:32.483199 34682 sgd_solver.cpp:112] Iteration 38180, lr = 0.01
I0523 03:00:38.032925 34682 solver.cpp:239] Iteration 38190 (1.78009 iter/s, 5.61769s/10 iters), loss = 8.89322
I0523 03:00:38.032989 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89322 (* 1 = 8.89322 loss)
I0523 03:00:38.088807 34682 sgd_solver.cpp:112] Iteration 38190, lr = 0.01
I0523 03:00:43.072229 34682 solver.cpp:239] Iteration 38200 (1.98451 iter/s, 5.03904s/10 iters), loss = 7.88359
I0523 03:00:43.072279 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88359 (* 1 = 7.88359 loss)
I0523 03:00:43.216707 34682 sgd_solver.cpp:112] Iteration 38200, lr = 0.01
I0523 03:00:47.212476 34682 solver.cpp:239] Iteration 38210 (2.41545 iter/s, 4.14002s/10 iters), loss = 9.2244
I0523 03:00:47.212541 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2244 (* 1 = 9.2244 loss)
I0523 03:00:47.475742 34682 sgd_solver.cpp:112] Iteration 38210, lr = 0.01
I0523 03:00:52.232789 34682 solver.cpp:239] Iteration 38220 (1.99202 iter/s, 5.02004s/10 iters), loss = 8.98615
I0523 03:00:52.232846 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98615 (* 1 = 8.98615 loss)
I0523 03:00:53.082314 34682 sgd_solver.cpp:112] Iteration 38220, lr = 0.01
I0523 03:00:58.736148 34682 solver.cpp:239] Iteration 38230 (1.53774 iter/s, 6.50304s/10 iters), loss = 8.9328
I0523 03:00:58.736203 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9328 (* 1 = 8.9328 loss)
I0523 03:00:59.553970 34682 sgd_solver.cpp:112] Iteration 38230, lr = 0.01
I0523 03:01:04.688205 34682 solver.cpp:239] Iteration 38240 (1.68017 iter/s, 5.95177s/10 iters), loss = 8.45715
I0523 03:01:04.688426 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45715 (* 1 = 8.45715 loss)
I0523 03:01:04.744460 34682 sgd_solver.cpp:112] Iteration 38240, lr = 0.01
I0523 03:01:09.136405 34682 solver.cpp:239] Iteration 38250 (2.2483 iter/s, 4.44781s/10 iters), loss = 8.57367
I0523 03:01:09.136453 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57367 (* 1 = 8.57367 loss)
I0523 03:01:09.198523 34682 sgd_solver.cpp:112] Iteration 38250, lr = 0.01
I0523 03:01:13.610870 34682 solver.cpp:239] Iteration 38260 (2.23502 iter/s, 4.47424s/10 iters), loss = 8.75535
I0523 03:01:13.610911 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75535 (* 1 = 8.75535 loss)
I0523 03:01:13.686003 34682 sgd_solver.cpp:112] Iteration 38260, lr = 0.01
I0523 03:01:17.887401 34682 solver.cpp:239] Iteration 38270 (2.33847 iter/s, 4.2763s/10 iters), loss = 8.08014
I0523 03:01:17.887459 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08014 (* 1 = 8.08014 loss)
I0523 03:01:18.336818 34682 sgd_solver.cpp:112] Iteration 38270, lr = 0.01
I0523 03:01:21.584921 34682 solver.cpp:239] Iteration 38280 (2.70467 iter/s, 3.69731s/10 iters), loss = 8.922
I0523 03:01:21.584973 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.922 (* 1 = 8.922 loss)
I0523 03:01:21.643918 34682 sgd_solver.cpp:112] Iteration 38280, lr = 0.01
I0523 03:01:25.683272 34682 solver.cpp:239] Iteration 38290 (2.44014 iter/s, 4.09813s/10 iters), loss = 8.37266
I0523 03:01:25.683316 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37266 (* 1 = 8.37266 loss)
I0523 03:01:26.395238 34682 sgd_solver.cpp:112] Iteration 38290, lr = 0.01
I0523 03:01:30.449203 34682 solver.cpp:239] Iteration 38300 (2.09834 iter/s, 4.76567s/10 iters), loss = 8.19786
I0523 03:01:30.449270 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19786 (* 1 = 8.19786 loss)
I0523 03:01:30.516609 34682 sgd_solver.cpp:112] Iteration 38300, lr = 0.01
I0523 03:01:36.553937 34682 solver.cpp:239] Iteration 38310 (1.63816 iter/s, 6.10442s/10 iters), loss = 9.85794
I0523 03:01:36.554217 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.85794 (* 1 = 9.85794 loss)
I0523 03:01:37.332832 34682 sgd_solver.cpp:112] Iteration 38310, lr = 0.01
I0523 03:01:41.755610 34682 solver.cpp:239] Iteration 38320 (1.92263 iter/s, 5.2012s/10 iters), loss = 9.27658
I0523 03:01:41.755686 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27658 (* 1 = 9.27658 loss)
I0523 03:01:41.811375 34682 sgd_solver.cpp:112] Iteration 38320, lr = 0.01
I0523 03:01:47.368505 34682 solver.cpp:239] Iteration 38330 (1.7817 iter/s, 5.6126s/10 iters), loss = 8.83308
I0523 03:01:47.368561 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83308 (* 1 = 8.83308 loss)
I0523 03:01:48.164152 34682 sgd_solver.cpp:112] Iteration 38330, lr = 0.01
I0523 03:01:52.267192 34682 solver.cpp:239] Iteration 38340 (2.04147 iter/s, 4.89843s/10 iters), loss = 8.8237
I0523 03:01:52.267253 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8237 (* 1 = 8.8237 loss)
I0523 03:01:52.865998 34682 sgd_solver.cpp:112] Iteration 38340, lr = 0.01
I0523 03:01:57.138116 34682 solver.cpp:239] Iteration 38350 (2.0531 iter/s, 4.87067s/10 iters), loss = 8.74599
I0523 03:01:57.138159 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74599 (* 1 = 8.74599 loss)
I0523 03:01:57.823137 34682 sgd_solver.cpp:112] Iteration 38350, lr = 0.01
I0523 03:02:00.567543 34682 solver.cpp:239] Iteration 38360 (2.9161 iter/s, 3.42923s/10 iters), loss = 8.30638
I0523 03:02:00.567595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30638 (* 1 = 8.30638 loss)
I0523 03:02:00.627171 34682 sgd_solver.cpp:112] Iteration 38360, lr = 0.01
I0523 03:02:05.440336 34682 solver.cpp:239] Iteration 38370 (2.05231 iter/s, 4.87255s/10 iters), loss = 8.23426
I0523 03:02:05.440377 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23426 (* 1 = 8.23426 loss)
I0523 03:02:05.504591 34682 sgd_solver.cpp:112] Iteration 38370, lr = 0.01
I0523 03:02:10.886080 34682 solver.cpp:239] Iteration 38380 (1.83639 iter/s, 5.44547s/10 iters), loss = 8.11071
I0523 03:02:10.886224 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11071 (* 1 = 8.11071 loss)
I0523 03:02:10.948751 34682 sgd_solver.cpp:112] Iteration 38380, lr = 0.01
I0523 03:02:14.196187 34682 solver.cpp:239] Iteration 38390 (3.0213 iter/s, 3.30983s/10 iters), loss = 8.41691
I0523 03:02:14.196228 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41691 (* 1 = 8.41691 loss)
I0523 03:02:15.066440 34682 sgd_solver.cpp:112] Iteration 38390, lr = 0.01
I0523 03:02:18.812083 34682 solver.cpp:239] Iteration 38400 (2.16654 iter/s, 4.61566s/10 iters), loss = 7.99491
I0523 03:02:18.812127 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99491 (* 1 = 7.99491 loss)
I0523 03:02:18.892629 34682 sgd_solver.cpp:112] Iteration 38400, lr = 0.01
I0523 03:02:23.646931 34682 solver.cpp:239] Iteration 38410 (2.06843 iter/s, 4.8346s/10 iters), loss = 8.8826
I0523 03:02:23.646998 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8826 (* 1 = 8.8826 loss)
I0523 03:02:24.427619 34682 sgd_solver.cpp:112] Iteration 38410, lr = 0.01
I0523 03:02:27.618233 34682 solver.cpp:239] Iteration 38420 (2.51821 iter/s, 3.97107s/10 iters), loss = 9.22026
I0523 03:02:27.618279 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22026 (* 1 = 9.22026 loss)
I0523 03:02:27.685298 34682 sgd_solver.cpp:112] Iteration 38420, lr = 0.01
I0523 03:02:32.867482 34682 solver.cpp:239] Iteration 38430 (1.90513 iter/s, 5.24898s/10 iters), loss = 8.8228
I0523 03:02:32.867528 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8228 (* 1 = 8.8228 loss)
I0523 03:02:32.939817 34682 sgd_solver.cpp:112] Iteration 38430, lr = 0.01
I0523 03:02:38.620226 34682 solver.cpp:239] Iteration 38440 (1.73839 iter/s, 5.75247s/10 iters), loss = 8.85079
I0523 03:02:38.620275 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85079 (* 1 = 8.85079 loss)
I0523 03:02:39.449978 34682 sgd_solver.cpp:112] Iteration 38440, lr = 0.01
I0523 03:02:45.655671 34682 solver.cpp:239] Iteration 38450 (1.42144 iter/s, 7.0351s/10 iters), loss = 9.10317
I0523 03:02:45.655861 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10317 (* 1 = 9.10317 loss)
I0523 03:02:46.485652 34682 sgd_solver.cpp:112] Iteration 38450, lr = 0.01
I0523 03:02:50.306180 34682 solver.cpp:239] Iteration 38460 (2.15048 iter/s, 4.65013s/10 iters), loss = 9.61181
I0523 03:02:50.306232 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.61181 (* 1 = 9.61181 loss)
I0523 03:02:50.379010 34682 sgd_solver.cpp:112] Iteration 38460, lr = 0.01
I0523 03:02:55.439648 34682 solver.cpp:239] Iteration 38470 (1.9481 iter/s, 5.13321s/10 iters), loss = 9.58143
I0523 03:02:55.439692 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.58143 (* 1 = 9.58143 loss)
I0523 03:02:55.517457 34682 sgd_solver.cpp:112] Iteration 38470, lr = 0.01
I0523 03:03:02.511198 34682 solver.cpp:239] Iteration 38480 (1.41419 iter/s, 7.07121s/10 iters), loss = 9.04329
I0523 03:03:02.511242 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04329 (* 1 = 9.04329 loss)
I0523 03:03:02.584213 34682 sgd_solver.cpp:112] Iteration 38480, lr = 0.01
I0523 03:03:09.023264 34682 solver.cpp:239] Iteration 38490 (1.53569 iter/s, 6.51174s/10 iters), loss = 8.4799
I0523 03:03:09.023324 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4799 (* 1 = 8.4799 loss)
I0523 03:03:09.843690 34682 sgd_solver.cpp:112] Iteration 38490, lr = 0.01
I0523 03:03:14.131997 34682 solver.cpp:239] Iteration 38500 (1.95753 iter/s, 5.10847s/10 iters), loss = 8.72396
I0523 03:03:14.132041 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72396 (* 1 = 8.72396 loss)
I0523 03:03:14.194824 34682 sgd_solver.cpp:112] Iteration 38500, lr = 0.01
I0523 03:03:19.278306 34682 solver.cpp:239] Iteration 38510 (1.94324 iter/s, 5.14605s/10 iters), loss = 9.88901
I0523 03:03:19.278502 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.88901 (* 1 = 9.88901 loss)
I0523 03:03:20.125994 34682 sgd_solver.cpp:112] Iteration 38510, lr = 0.01
I0523 03:03:25.040309 34682 solver.cpp:239] Iteration 38520 (1.73563 iter/s, 5.76159s/10 iters), loss = 8.95941
I0523 03:03:25.040360 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95941 (* 1 = 8.95941 loss)
I0523 03:03:25.788938 34682 sgd_solver.cpp:112] Iteration 38520, lr = 0.01
I0523 03:03:31.999426 34682 solver.cpp:239] Iteration 38530 (1.43703 iter/s, 6.95879s/10 iters), loss = 8.49619
I0523 03:03:31.999482 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49619 (* 1 = 8.49619 loss)
I0523 03:03:32.664913 34682 sgd_solver.cpp:112] Iteration 38530, lr = 0.01
I0523 03:03:36.060389 34682 solver.cpp:239] Iteration 38540 (2.4626 iter/s, 4.06075s/10 iters), loss = 8.45034
I0523 03:03:36.060433 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45034 (* 1 = 8.45034 loss)
I0523 03:03:36.879920 34682 sgd_solver.cpp:112] Iteration 38540, lr = 0.01
I0523 03:03:41.880461 34682 solver.cpp:239] Iteration 38550 (1.71827 iter/s, 5.81979s/10 iters), loss = 8.93289
I0523 03:03:41.880519 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93289 (* 1 = 8.93289 loss)
I0523 03:03:42.640194 34682 sgd_solver.cpp:112] Iteration 38550, lr = 0.01
I0523 03:03:46.960739 34682 solver.cpp:239] Iteration 38560 (1.97019 iter/s, 5.07565s/10 iters), loss = 8.26593
I0523 03:03:46.960785 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26593 (* 1 = 8.26593 loss)
I0523 03:03:47.020838 34682 sgd_solver.cpp:112] Iteration 38560, lr = 0.01
I0523 03:03:50.513901 34682 solver.cpp:239] Iteration 38570 (2.81457 iter/s, 3.55294s/10 iters), loss = 8.27072
I0523 03:03:50.514042 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27072 (* 1 = 8.27072 loss)
I0523 03:03:51.321605 34682 sgd_solver.cpp:112] Iteration 38570, lr = 0.01
I0523 03:03:57.456209 34682 solver.cpp:239] Iteration 38580 (1.44053 iter/s, 6.94189s/10 iters), loss = 8.7963
I0523 03:03:57.456255 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7963 (* 1 = 8.7963 loss)
I0523 03:03:57.513748 34682 sgd_solver.cpp:112] Iteration 38580, lr = 0.01
I0523 03:04:02.985817 34682 solver.cpp:239] Iteration 38590 (1.80854 iter/s, 5.52932s/10 iters), loss = 8.34871
I0523 03:04:02.985891 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34871 (* 1 = 8.34871 loss)
I0523 03:04:03.756767 34682 sgd_solver.cpp:112] Iteration 38590, lr = 0.01
I0523 03:04:09.448974 34682 solver.cpp:239] Iteration 38600 (1.54731 iter/s, 6.46283s/10 iters), loss = 9.75015
I0523 03:04:09.449025 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.75015 (* 1 = 9.75015 loss)
I0523 03:04:09.516180 34682 sgd_solver.cpp:112] Iteration 38600, lr = 0.01
I0523 03:04:13.739172 34682 solver.cpp:239] Iteration 38610 (2.33102 iter/s, 4.28997s/10 iters), loss = 8.48863
I0523 03:04:13.739217 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48863 (* 1 = 8.48863 loss)
I0523 03:04:13.813928 34682 sgd_solver.cpp:112] Iteration 38610, lr = 0.01
I0523 03:04:17.079874 34682 solver.cpp:239] Iteration 38620 (2.99356 iter/s, 3.34051s/10 iters), loss = 7.8579
I0523 03:04:17.079926 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8579 (* 1 = 7.8579 loss)
I0523 03:04:17.141047 34682 sgd_solver.cpp:112] Iteration 38620, lr = 0.01
I0523 03:04:22.126842 34682 solver.cpp:239] Iteration 38630 (1.98149 iter/s, 5.04672s/10 iters), loss = 8.32611
I0523 03:04:22.127141 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32611 (* 1 = 8.32611 loss)
I0523 03:04:22.200469 34682 sgd_solver.cpp:112] Iteration 38630, lr = 0.01
I0523 03:04:27.941572 34682 solver.cpp:239] Iteration 38640 (1.71992 iter/s, 5.81421s/10 iters), loss = 8.37774
I0523 03:04:27.941630 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37774 (* 1 = 8.37774 loss)
I0523 03:04:28.000100 34682 sgd_solver.cpp:112] Iteration 38640, lr = 0.01
I0523 03:04:32.061166 34682 solver.cpp:239] Iteration 38650 (2.42756 iter/s, 4.11937s/10 iters), loss = 9.31202
I0523 03:04:32.061208 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31202 (* 1 = 9.31202 loss)
I0523 03:04:32.125493 34682 sgd_solver.cpp:112] Iteration 38650, lr = 0.01
I0523 03:04:36.171458 34682 solver.cpp:239] Iteration 38660 (2.43304 iter/s, 4.11008s/10 iters), loss = 10.1926
I0523 03:04:36.171514 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1926 (* 1 = 10.1926 loss)
I0523 03:04:36.352833 34682 sgd_solver.cpp:112] Iteration 38660, lr = 0.01
I0523 03:04:41.116731 34682 solver.cpp:239] Iteration 38670 (2.02224 iter/s, 4.94501s/10 iters), loss = 9.50066
I0523 03:04:41.116789 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.50066 (* 1 = 9.50066 loss)
I0523 03:04:41.916200 34682 sgd_solver.cpp:112] Iteration 38670, lr = 0.01
I0523 03:04:46.345574 34682 solver.cpp:239] Iteration 38680 (1.91257 iter/s, 5.22858s/10 iters), loss = 9.14373
I0523 03:04:46.345618 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14373 (* 1 = 9.14373 loss)
I0523 03:04:46.415952 34682 sgd_solver.cpp:112] Iteration 38680, lr = 0.01
I0523 03:04:50.387202 34682 solver.cpp:239] Iteration 38690 (2.47439 iter/s, 4.04141s/10 iters), loss = 8.91643
I0523 03:04:50.387253 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91643 (* 1 = 8.91643 loss)
I0523 03:04:50.455205 34682 sgd_solver.cpp:112] Iteration 38690, lr = 0.01
I0523 03:04:55.421581 34682 solver.cpp:239] Iteration 38700 (1.98645 iter/s, 5.03411s/10 iters), loss = 8.47486
I0523 03:04:55.421851 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47486 (* 1 = 8.47486 loss)
I0523 03:04:56.193033 34682 sgd_solver.cpp:112] Iteration 38700, lr = 0.01
I0523 03:05:00.884526 34682 solver.cpp:239] Iteration 38710 (1.83067 iter/s, 5.46247s/10 iters), loss = 8.90751
I0523 03:05:00.884582 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90751 (* 1 = 8.90751 loss)
I0523 03:05:00.953394 34682 sgd_solver.cpp:112] Iteration 38710, lr = 0.01
I0523 03:05:05.071285 34682 solver.cpp:239] Iteration 38720 (2.38861 iter/s, 4.18653s/10 iters), loss = 8.71137
I0523 03:05:05.071337 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71137 (* 1 = 8.71137 loss)
I0523 03:05:05.310910 34682 sgd_solver.cpp:112] Iteration 38720, lr = 0.01
I0523 03:05:10.033998 34682 solver.cpp:239] Iteration 38730 (2.01513 iter/s, 4.96245s/10 iters), loss = 8.87138
I0523 03:05:10.034060 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87138 (* 1 = 8.87138 loss)
I0523 03:05:10.162652 34682 sgd_solver.cpp:112] Iteration 38730, lr = 0.01
I0523 03:05:15.104702 34682 solver.cpp:239] Iteration 38740 (1.97221 iter/s, 5.07044s/10 iters), loss = 8.46441
I0523 03:05:15.104748 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46441 (* 1 = 8.46441 loss)
I0523 03:05:15.950515 34682 sgd_solver.cpp:112] Iteration 38740, lr = 0.01
I0523 03:05:20.363719 34682 solver.cpp:239] Iteration 38750 (1.90159 iter/s, 5.25875s/10 iters), loss = 9.51406
I0523 03:05:20.363772 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51406 (* 1 = 9.51406 loss)
I0523 03:05:20.438318 34682 sgd_solver.cpp:112] Iteration 38750, lr = 0.01
I0523 03:05:23.751917 34682 solver.cpp:239] Iteration 38760 (2.95159 iter/s, 3.388s/10 iters), loss = 8.15142
I0523 03:05:23.751962 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15142 (* 1 = 8.15142 loss)
I0523 03:05:23.817505 34682 sgd_solver.cpp:112] Iteration 38760, lr = 0.01
I0523 03:05:28.097626 34682 solver.cpp:239] Iteration 38770 (2.30124 iter/s, 4.34549s/10 iters), loss = 8.9576
I0523 03:05:28.097868 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9576 (* 1 = 8.9576 loss)
I0523 03:05:28.795933 34682 sgd_solver.cpp:112] Iteration 38770, lr = 0.01
I0523 03:05:32.612540 34682 solver.cpp:239] Iteration 38780 (2.21508 iter/s, 4.51451s/10 iters), loss = 8.86932
I0523 03:05:32.612592 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86932 (* 1 = 8.86932 loss)
I0523 03:05:33.422456 34682 sgd_solver.cpp:112] Iteration 38780, lr = 0.01
I0523 03:05:36.843523 34682 solver.cpp:239] Iteration 38790 (2.36364 iter/s, 4.23076s/10 iters), loss = 9.55441
I0523 03:05:36.843575 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.55441 (* 1 = 9.55441 loss)
I0523 03:05:36.920343 34682 sgd_solver.cpp:112] Iteration 38790, lr = 0.01
I0523 03:05:42.216655 34682 solver.cpp:239] Iteration 38800 (1.86121 iter/s, 5.37285s/10 iters), loss = 9.82274
I0523 03:05:42.216703 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.82274 (* 1 = 9.82274 loss)
I0523 03:05:43.051332 34682 sgd_solver.cpp:112] Iteration 38800, lr = 0.01
I0523 03:05:49.525295 34682 solver.cpp:239] Iteration 38810 (1.36831 iter/s, 7.30829s/10 iters), loss = 8.76069
I0523 03:05:49.525357 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76069 (* 1 = 8.76069 loss)
I0523 03:05:49.993603 34682 sgd_solver.cpp:112] Iteration 38810, lr = 0.01
I0523 03:05:55.473716 34682 solver.cpp:239] Iteration 38820 (1.6812 iter/s, 5.94811s/10 iters), loss = 8.77816
I0523 03:05:55.473764 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77816 (* 1 = 8.77816 loss)
I0523 03:05:55.537776 34682 sgd_solver.cpp:112] Iteration 38820, lr = 0.01
I0523 03:06:00.735229 34682 solver.cpp:239] Iteration 38830 (1.90069 iter/s, 5.26125s/10 iters), loss = 8.90705
I0523 03:06:00.735474 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90705 (* 1 = 8.90705 loss)
I0523 03:06:00.811096 34682 sgd_solver.cpp:112] Iteration 38830, lr = 0.01
I0523 03:06:07.254719 34682 solver.cpp:239] Iteration 38840 (1.53397 iter/s, 6.51902s/10 iters), loss = 9.54696
I0523 03:06:07.254770 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.54696 (* 1 = 9.54696 loss)
I0523 03:06:08.030782 34682 sgd_solver.cpp:112] Iteration 38840, lr = 0.01
I0523 03:06:12.782637 34682 solver.cpp:239] Iteration 38850 (1.80909 iter/s, 5.52764s/10 iters), loss = 8.57419
I0523 03:06:12.782687 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57419 (* 1 = 8.57419 loss)
I0523 03:06:13.453665 34682 sgd_solver.cpp:112] Iteration 38850, lr = 0.01
I0523 03:06:18.323588 34682 solver.cpp:239] Iteration 38860 (1.80483 iter/s, 5.54068s/10 iters), loss = 8.29382
I0523 03:06:18.323633 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29382 (* 1 = 8.29382 loss)
I0523 03:06:19.152310 34682 sgd_solver.cpp:112] Iteration 38860, lr = 0.01
I0523 03:06:23.963099 34682 solver.cpp:239] Iteration 38870 (1.77329 iter/s, 5.63923s/10 iters), loss = 9.57298
I0523 03:06:23.963146 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.57298 (* 1 = 9.57298 loss)
I0523 03:06:24.041167 34682 sgd_solver.cpp:112] Iteration 38870, lr = 0.01
I0523 03:06:26.178542 34682 solver.cpp:239] Iteration 38880 (4.51406 iter/s, 2.2153s/10 iters), loss = 8.68338
I0523 03:06:26.178582 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68338 (* 1 = 8.68338 loss)
I0523 03:06:26.260493 34682 sgd_solver.cpp:112] Iteration 38880, lr = 0.01
I0523 03:06:30.430543 34682 solver.cpp:239] Iteration 38890 (2.35196 iter/s, 4.25178s/10 iters), loss = 8.61511
I0523 03:06:30.430596 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61511 (* 1 = 8.61511 loss)
I0523 03:06:30.504302 34682 sgd_solver.cpp:112] Iteration 38890, lr = 0.01
I0523 03:06:35.314221 34682 solver.cpp:239] Iteration 38900 (2.04774 iter/s, 4.88342s/10 iters), loss = 8.56285
I0523 03:06:35.314486 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56285 (* 1 = 8.56285 loss)
I0523 03:06:36.109527 34682 sgd_solver.cpp:112] Iteration 38900, lr = 0.01
I0523 03:06:39.460789 34682 solver.cpp:239] Iteration 38910 (2.41187 iter/s, 4.14616s/10 iters), loss = 9.33885
I0523 03:06:39.460834 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33885 (* 1 = 9.33885 loss)
I0523 03:06:39.542845 34682 sgd_solver.cpp:112] Iteration 38910, lr = 0.01
I0523 03:06:45.651455 34682 solver.cpp:239] Iteration 38920 (1.61541 iter/s, 6.19037s/10 iters), loss = 8.66234
I0523 03:06:45.651505 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66234 (* 1 = 8.66234 loss)
I0523 03:06:46.329126 34682 sgd_solver.cpp:112] Iteration 38920, lr = 0.01
I0523 03:06:51.391408 34682 solver.cpp:239] Iteration 38930 (1.74226 iter/s, 5.73966s/10 iters), loss = 9.19384
I0523 03:06:51.391470 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19384 (* 1 = 9.19384 loss)
I0523 03:06:51.464594 34682 sgd_solver.cpp:112] Iteration 38930, lr = 0.01
I0523 03:06:55.187948 34682 solver.cpp:239] Iteration 38940 (2.63413 iter/s, 3.79632s/10 iters), loss = 9.06488
I0523 03:06:55.188001 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06488 (* 1 = 9.06488 loss)
I0523 03:06:56.075265 34682 sgd_solver.cpp:112] Iteration 38940, lr = 0.01
I0523 03:07:00.123041 34682 solver.cpp:239] Iteration 38950 (2.02641 iter/s, 4.93483s/10 iters), loss = 9.06686
I0523 03:07:00.123109 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06686 (* 1 = 9.06686 loss)
I0523 03:07:00.304437 34682 sgd_solver.cpp:112] Iteration 38950, lr = 0.01
I0523 03:07:06.216157 34682 solver.cpp:239] Iteration 38960 (1.64128 iter/s, 6.0928s/10 iters), loss = 8.7935
I0523 03:07:06.216307 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7935 (* 1 = 8.7935 loss)
I0523 03:07:06.789707 34682 sgd_solver.cpp:112] Iteration 38960, lr = 0.01
I0523 03:07:12.320513 34682 solver.cpp:239] Iteration 38970 (1.63828 iter/s, 6.10396s/10 iters), loss = 9.15018
I0523 03:07:12.320564 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15018 (* 1 = 9.15018 loss)
I0523 03:07:13.021178 34682 sgd_solver.cpp:112] Iteration 38970, lr = 0.01
I0523 03:07:17.076843 34682 solver.cpp:239] Iteration 38980 (2.10257 iter/s, 4.75608s/10 iters), loss = 8.62018
I0523 03:07:17.076897 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62018 (* 1 = 8.62018 loss)
I0523 03:07:17.784392 34682 sgd_solver.cpp:112] Iteration 38980, lr = 0.01
I0523 03:07:20.425199 34682 solver.cpp:239] Iteration 38990 (2.98671 iter/s, 3.34816s/10 iters), loss = 8.34631
I0523 03:07:20.425243 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34631 (* 1 = 8.34631 loss)
I0523 03:07:20.492251 34682 sgd_solver.cpp:112] Iteration 38990, lr = 0.01
I0523 03:07:26.223938 34682 solver.cpp:239] Iteration 39000 (1.7246 iter/s, 5.79846s/10 iters), loss = 8.75956
I0523 03:07:26.223989 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75956 (* 1 = 8.75956 loss)
I0523 03:07:26.288489 34682 sgd_solver.cpp:112] Iteration 39000, lr = 0.01
I0523 03:07:30.600607 34682 solver.cpp:239] Iteration 39010 (2.28496 iter/s, 4.37644s/10 iters), loss = 10.4804
I0523 03:07:30.600647 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.4804 (* 1 = 10.4804 loss)
I0523 03:07:31.379462 34682 sgd_solver.cpp:112] Iteration 39010, lr = 0.01
I0523 03:07:35.460909 34682 solver.cpp:239] Iteration 39020 (2.05759 iter/s, 4.86007s/10 iters), loss = 9.08558
I0523 03:07:35.460949 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08558 (* 1 = 9.08558 loss)
I0523 03:07:36.278437 34682 sgd_solver.cpp:112] Iteration 39020, lr = 0.01
I0523 03:07:38.467053 34682 solver.cpp:239] Iteration 39030 (3.32672 iter/s, 3.00597s/10 iters), loss = 8.99188
I0523 03:07:38.467118 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99188 (* 1 = 8.99188 loss)
I0523 03:07:39.286064 34682 sgd_solver.cpp:112] Iteration 39030, lr = 0.01
I0523 03:07:44.456936 34682 solver.cpp:239] Iteration 39040 (1.66957 iter/s, 5.98956s/10 iters), loss = 8.67079
I0523 03:07:44.456998 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67079 (* 1 = 8.67079 loss)
I0523 03:07:45.233665 34682 sgd_solver.cpp:112] Iteration 39040, lr = 0.01
I0523 03:07:48.997900 34682 solver.cpp:239] Iteration 39050 (2.2023 iter/s, 4.54071s/10 iters), loss = 8.82293
I0523 03:07:48.997952 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82293 (* 1 = 8.82293 loss)
I0523 03:07:49.060111 34682 sgd_solver.cpp:112] Iteration 39050, lr = 0.01
I0523 03:07:52.968178 34682 solver.cpp:239] Iteration 39060 (2.51886 iter/s, 3.97006s/10 iters), loss = 8.78652
I0523 03:07:52.968230 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78652 (* 1 = 8.78652 loss)
I0523 03:07:53.049144 34682 sgd_solver.cpp:112] Iteration 39060, lr = 0.01
I0523 03:07:57.663592 34682 solver.cpp:239] Iteration 39070 (2.12985 iter/s, 4.69517s/10 iters), loss = 9.56845
I0523 03:07:57.663638 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56845 (* 1 = 9.56845 loss)
I0523 03:07:57.723307 34682 sgd_solver.cpp:112] Iteration 39070, lr = 0.01
I0523 03:08:02.830453 34682 solver.cpp:239] Iteration 39080 (1.93551 iter/s, 5.1666s/10 iters), loss = 8.32908
I0523 03:08:02.830497 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32908 (* 1 = 8.32908 loss)
I0523 03:08:03.674877 34682 sgd_solver.cpp:112] Iteration 39080, lr = 0.01
I0523 03:08:08.985886 34682 solver.cpp:239] Iteration 39090 (1.62466 iter/s, 6.15513s/10 iters), loss = 8.99168
I0523 03:08:08.986121 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99168 (* 1 = 8.99168 loss)
I0523 03:08:09.052852 34682 sgd_solver.cpp:112] Iteration 39090, lr = 0.01
I0523 03:08:15.561321 34682 solver.cpp:239] Iteration 39100 (1.52092 iter/s, 6.57495s/10 iters), loss = 8.74617
I0523 03:08:15.561381 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74617 (* 1 = 8.74617 loss)
I0523 03:08:15.621810 34682 sgd_solver.cpp:112] Iteration 39100, lr = 0.01
I0523 03:08:20.075522 34682 solver.cpp:239] Iteration 39110 (2.21535 iter/s, 4.51396s/10 iters), loss = 8.73602
I0523 03:08:20.075563 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73602 (* 1 = 8.73602 loss)
I0523 03:08:20.138336 34682 sgd_solver.cpp:112] Iteration 39110, lr = 0.01
I0523 03:08:27.260267 34682 solver.cpp:239] Iteration 39120 (1.3919 iter/s, 7.1844s/10 iters), loss = 8.78577
I0523 03:08:27.260315 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78577 (* 1 = 8.78577 loss)
I0523 03:08:27.323426 34682 sgd_solver.cpp:112] Iteration 39120, lr = 0.01
I0523 03:08:33.855330 34682 solver.cpp:239] Iteration 39130 (1.51636 iter/s, 6.59474s/10 iters), loss = 7.68977
I0523 03:08:33.855377 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68977 (* 1 = 7.68977 loss)
I0523 03:08:33.925127 34682 sgd_solver.cpp:112] Iteration 39130, lr = 0.01
I0523 03:08:37.805013 34682 solver.cpp:239] Iteration 39140 (2.53198 iter/s, 3.94948s/10 iters), loss = 8.98276
I0523 03:08:37.805058 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98276 (* 1 = 8.98276 loss)
I0523 03:08:37.863579 34682 sgd_solver.cpp:112] Iteration 39140, lr = 0.01
I0523 03:08:41.936132 34682 solver.cpp:239] Iteration 39150 (2.42078 iter/s, 4.1309s/10 iters), loss = 8.55142
I0523 03:08:41.936422 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55142 (* 1 = 8.55142 loss)
I0523 03:08:41.994683 34682 sgd_solver.cpp:112] Iteration 39150, lr = 0.01
I0523 03:08:45.489357 34682 solver.cpp:239] Iteration 39160 (2.81468 iter/s, 3.55281s/10 iters), loss = 7.95655
I0523 03:08:45.489410 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95655 (* 1 = 7.95655 loss)
I0523 03:08:45.805889 34682 sgd_solver.cpp:112] Iteration 39160, lr = 0.01
I0523 03:08:49.677381 34682 solver.cpp:239] Iteration 39170 (2.38789 iter/s, 4.18779s/10 iters), loss = 8.9103
I0523 03:08:49.677436 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9103 (* 1 = 8.9103 loss)
I0523 03:08:50.361100 34682 sgd_solver.cpp:112] Iteration 39170, lr = 0.01
I0523 03:08:55.279085 34682 solver.cpp:239] Iteration 39180 (1.78526 iter/s, 5.60143s/10 iters), loss = 9.21053
I0523 03:08:55.279126 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21053 (* 1 = 9.21053 loss)
I0523 03:08:55.352809 34682 sgd_solver.cpp:112] Iteration 39180, lr = 0.01
I0523 03:08:59.607597 34682 solver.cpp:239] Iteration 39190 (2.31039 iter/s, 4.32828s/10 iters), loss = 8.44264
I0523 03:08:59.607652 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44264 (* 1 = 8.44264 loss)
I0523 03:09:00.418197 34682 sgd_solver.cpp:112] Iteration 39190, lr = 0.01
I0523 03:09:04.389173 34682 solver.cpp:239] Iteration 39200 (2.09147 iter/s, 4.78133s/10 iters), loss = 10.2602
I0523 03:09:04.389215 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2602 (* 1 = 10.2602 loss)
I0523 03:09:04.454069 34682 sgd_solver.cpp:112] Iteration 39200, lr = 0.01
I0523 03:09:10.054095 34682 solver.cpp:239] Iteration 39210 (1.76534 iter/s, 5.66464s/10 iters), loss = 8.82173
I0523 03:09:10.054163 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82173 (* 1 = 8.82173 loss)
I0523 03:09:10.907090 34682 sgd_solver.cpp:112] Iteration 39210, lr = 0.01
I0523 03:09:15.494400 34682 solver.cpp:239] Iteration 39220 (1.83823 iter/s, 5.44003s/10 iters), loss = 8.7843
I0523 03:09:15.494505 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7843 (* 1 = 8.7843 loss)
I0523 03:09:15.562819 34682 sgd_solver.cpp:112] Iteration 39220, lr = 0.01
I0523 03:09:19.684392 34682 solver.cpp:239] Iteration 39230 (2.3868 iter/s, 4.18971s/10 iters), loss = 8.55508
I0523 03:09:19.684456 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55508 (* 1 = 8.55508 loss)
I0523 03:09:19.738433 34682 sgd_solver.cpp:112] Iteration 39230, lr = 0.01
I0523 03:09:24.727893 34682 solver.cpp:239] Iteration 39240 (1.98286 iter/s, 5.04323s/10 iters), loss = 8.70993
I0523 03:09:24.727951 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70993 (* 1 = 8.70993 loss)
I0523 03:09:25.384850 34682 sgd_solver.cpp:112] Iteration 39240, lr = 0.01
I0523 03:09:27.880545 34682 solver.cpp:239] Iteration 39250 (3.17213 iter/s, 3.15246s/10 iters), loss = 8.60761
I0523 03:09:27.880587 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60761 (* 1 = 8.60761 loss)
I0523 03:09:27.937521 34682 sgd_solver.cpp:112] Iteration 39250, lr = 0.01
I0523 03:09:31.963398 34682 solver.cpp:239] Iteration 39260 (2.4494 iter/s, 4.08264s/10 iters), loss = 8.94088
I0523 03:09:31.963459 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94088 (* 1 = 8.94088 loss)
I0523 03:09:32.771838 34682 sgd_solver.cpp:112] Iteration 39260, lr = 0.01
I0523 03:09:38.734354 34682 solver.cpp:239] Iteration 39270 (1.47697 iter/s, 6.77062s/10 iters), loss = 8.99371
I0523 03:09:38.734411 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99371 (* 1 = 8.99371 loss)
I0523 03:09:38.797315 34682 sgd_solver.cpp:112] Iteration 39270, lr = 0.01
I0523 03:09:42.675137 34682 solver.cpp:239] Iteration 39280 (2.53771 iter/s, 3.94057s/10 iters), loss = 8.37576
I0523 03:09:42.675197 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37576 (* 1 = 8.37576 loss)
I0523 03:09:43.424495 34682 sgd_solver.cpp:112] Iteration 39280, lr = 0.01
I0523 03:09:47.288831 34682 solver.cpp:239] Iteration 39290 (2.16758 iter/s, 4.61345s/10 iters), loss = 8.53027
I0523 03:09:47.289011 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53027 (* 1 = 8.53027 loss)
I0523 03:09:47.351197 34682 sgd_solver.cpp:112] Iteration 39290, lr = 0.01
I0523 03:09:53.694437 34682 solver.cpp:239] Iteration 39300 (1.56124 iter/s, 6.40517s/10 iters), loss = 7.93756
I0523 03:09:53.694484 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93756 (* 1 = 7.93756 loss)
I0523 03:09:53.762962 34682 sgd_solver.cpp:112] Iteration 39300, lr = 0.01
I0523 03:09:58.457484 34682 solver.cpp:239] Iteration 39310 (2.09961 iter/s, 4.76279s/10 iters), loss = 8.8498
I0523 03:09:58.457556 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8498 (* 1 = 8.8498 loss)
I0523 03:09:59.289131 34682 sgd_solver.cpp:112] Iteration 39310, lr = 0.01
I0523 03:10:03.310526 34682 solver.cpp:239] Iteration 39320 (2.06068 iter/s, 4.85277s/10 iters), loss = 8.9684
I0523 03:10:03.310602 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9684 (* 1 = 8.9684 loss)
I0523 03:10:03.388165 34682 sgd_solver.cpp:112] Iteration 39320, lr = 0.01
I0523 03:10:08.236920 34682 solver.cpp:239] Iteration 39330 (2.02999 iter/s, 4.92612s/10 iters), loss = 8.40084
I0523 03:10:08.236960 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40084 (* 1 = 8.40084 loss)
I0523 03:10:08.306685 34682 sgd_solver.cpp:112] Iteration 39330, lr = 0.01
I0523 03:10:11.644224 34682 solver.cpp:239] Iteration 39340 (2.93503 iter/s, 3.40712s/10 iters), loss = 9.21023
I0523 03:10:11.644265 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21023 (* 1 = 9.21023 loss)
I0523 03:10:11.716954 34682 sgd_solver.cpp:112] Iteration 39340, lr = 0.01
I0523 03:10:15.937322 34682 solver.cpp:239] Iteration 39350 (2.32944 iter/s, 4.29288s/10 iters), loss = 9.76977
I0523 03:10:15.937372 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.76977 (* 1 = 9.76977 loss)
I0523 03:10:16.429764 34682 sgd_solver.cpp:112] Iteration 39350, lr = 0.01
I0523 03:10:23.401907 34682 solver.cpp:239] Iteration 39360 (1.33972 iter/s, 7.46423s/10 iters), loss = 8.53637
I0523 03:10:23.402006 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53637 (* 1 = 8.53637 loss)
I0523 03:10:24.242149 34682 sgd_solver.cpp:112] Iteration 39360, lr = 0.01
I0523 03:10:27.561623 34682 solver.cpp:239] Iteration 39370 (2.40416 iter/s, 4.15945s/10 iters), loss = 9.47005
I0523 03:10:27.561669 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47005 (* 1 = 9.47005 loss)
I0523 03:10:27.638334 34682 sgd_solver.cpp:112] Iteration 39370, lr = 0.01
I0523 03:10:33.233045 34682 solver.cpp:239] Iteration 39380 (1.76332 iter/s, 5.67113s/10 iters), loss = 7.92915
I0523 03:10:33.233111 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92915 (* 1 = 7.92915 loss)
I0523 03:10:33.995314 34682 sgd_solver.cpp:112] Iteration 39380, lr = 0.01
I0523 03:10:39.582583 34682 solver.cpp:239] Iteration 39390 (1.575 iter/s, 6.34923s/10 iters), loss = 9.14955
I0523 03:10:39.582628 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14955 (* 1 = 9.14955 loss)
I0523 03:10:39.640364 34682 sgd_solver.cpp:112] Iteration 39390, lr = 0.01
I0523 03:10:42.982805 34682 solver.cpp:239] Iteration 39400 (2.94115 iter/s, 3.40003s/10 iters), loss = 8.12326
I0523 03:10:42.982872 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12326 (* 1 = 8.12326 loss)
I0523 03:10:43.055022 34682 sgd_solver.cpp:112] Iteration 39400, lr = 0.01
I0523 03:10:47.808887 34682 solver.cpp:239] Iteration 39410 (2.07219 iter/s, 4.82581s/10 iters), loss = 9.76996
I0523 03:10:47.808948 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.76996 (* 1 = 9.76996 loss)
I0523 03:10:48.522756 34682 sgd_solver.cpp:112] Iteration 39410, lr = 0.01
I0523 03:10:52.786314 34682 solver.cpp:239] Iteration 39420 (2.00919 iter/s, 4.97714s/10 iters), loss = 8.0434
I0523 03:10:52.786399 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0434 (* 1 = 8.0434 loss)
I0523 03:10:53.626276 34682 sgd_solver.cpp:112] Iteration 39420, lr = 0.01
I0523 03:10:56.882679 34682 solver.cpp:239] Iteration 39430 (2.44133 iter/s, 4.09612s/10 iters), loss = 8.52135
I0523 03:10:56.882740 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52135 (* 1 = 8.52135 loss)
I0523 03:10:56.954646 34682 sgd_solver.cpp:112] Iteration 39430, lr = 0.01
I0523 03:11:01.636204 34682 solver.cpp:239] Iteration 39440 (2.10381 iter/s, 4.75327s/10 iters), loss = 8.95412
I0523 03:11:01.636250 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95412 (* 1 = 8.95412 loss)
I0523 03:11:01.696753 34682 sgd_solver.cpp:112] Iteration 39440, lr = 0.01
I0523 03:11:05.705540 34682 solver.cpp:239] Iteration 39450 (2.45754 iter/s, 4.0691s/10 iters), loss = 9.20794
I0523 03:11:05.705638 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20794 (* 1 = 9.20794 loss)
I0523 03:11:05.759977 34682 sgd_solver.cpp:112] Iteration 39450, lr = 0.01
I0523 03:11:12.796275 34682 solver.cpp:239] Iteration 39460 (1.41037 iter/s, 7.09036s/10 iters), loss = 8.83823
I0523 03:11:12.796331 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83823 (* 1 = 8.83823 loss)
I0523 03:11:12.866108 34682 sgd_solver.cpp:112] Iteration 39460, lr = 0.01
I0523 03:11:18.647475 34682 solver.cpp:239] Iteration 39470 (1.70914 iter/s, 5.85091s/10 iters), loss = 8.37126
I0523 03:11:18.647521 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37126 (* 1 = 8.37126 loss)
I0523 03:11:18.967517 34682 sgd_solver.cpp:112] Iteration 39470, lr = 0.01
I0523 03:11:23.577499 34682 solver.cpp:239] Iteration 39480 (2.02849 iter/s, 4.92976s/10 iters), loss = 7.90773
I0523 03:11:23.577549 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90773 (* 1 = 7.90773 loss)
I0523 03:11:23.650873 34682 sgd_solver.cpp:112] Iteration 39480, lr = 0.01
I0523 03:11:29.119463 34682 solver.cpp:239] Iteration 39490 (1.80451 iter/s, 5.54169s/10 iters), loss = 8.71401
I0523 03:11:29.119527 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71401 (* 1 = 8.71401 loss)
I0523 03:11:29.962582 34682 sgd_solver.cpp:112] Iteration 39490, lr = 0.01
I0523 03:11:36.271041 34682 solver.cpp:239] Iteration 39500 (1.39836 iter/s, 7.15123s/10 iters), loss = 9.38782
I0523 03:11:36.271093 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.38782 (* 1 = 9.38782 loss)
I0523 03:11:36.350006 34682 sgd_solver.cpp:112] Iteration 39500, lr = 0.01
I0523 03:11:41.514776 34682 solver.cpp:239] Iteration 39510 (1.90713 iter/s, 5.24347s/10 iters), loss = 8.45139
I0523 03:11:41.514822 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45139 (* 1 = 8.45139 loss)
I0523 03:11:42.347792 34682 sgd_solver.cpp:112] Iteration 39510, lr = 0.01
I0523 03:11:47.967236 34682 solver.cpp:239] Iteration 39520 (1.54987 iter/s, 6.45215s/10 iters), loss = 9.25262
I0523 03:11:47.967296 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25262 (* 1 = 9.25262 loss)
I0523 03:11:48.046197 34682 sgd_solver.cpp:112] Iteration 39520, lr = 0.01
I0523 03:11:52.238484 34682 solver.cpp:239] Iteration 39530 (2.34137 iter/s, 4.27101s/10 iters), loss = 8.74099
I0523 03:11:52.238538 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74099 (* 1 = 8.74099 loss)
I0523 03:11:52.769713 34682 sgd_solver.cpp:112] Iteration 39530, lr = 0.01
I0523 03:11:56.242331 34682 solver.cpp:239] Iteration 39540 (2.49774 iter/s, 4.00363s/10 iters), loss = 9.03209
I0523 03:11:56.242455 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03209 (* 1 = 9.03209 loss)
I0523 03:11:56.864145 34682 sgd_solver.cpp:112] Iteration 39540, lr = 0.01
I0523 03:12:01.336957 34682 solver.cpp:239] Iteration 39550 (1.96298 iter/s, 5.09428s/10 iters), loss = 9.12677
I0523 03:12:01.337028 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12677 (* 1 = 9.12677 loss)
I0523 03:12:01.399859 34682 sgd_solver.cpp:112] Iteration 39550, lr = 0.01
I0523 03:12:05.642032 34682 solver.cpp:239] Iteration 39560 (2.32297 iter/s, 4.30483s/10 iters), loss = 9.12083
I0523 03:12:05.642093 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12083 (* 1 = 9.12083 loss)
I0523 03:12:06.431403 34682 sgd_solver.cpp:112] Iteration 39560, lr = 0.01
I0523 03:12:09.044931 34682 solver.cpp:239] Iteration 39570 (2.93885 iter/s, 3.4027s/10 iters), loss = 8.1428
I0523 03:12:09.044975 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1428 (* 1 = 8.1428 loss)
I0523 03:12:09.646179 34682 sgd_solver.cpp:112] Iteration 39570, lr = 0.01
I0523 03:12:15.084288 34682 solver.cpp:239] Iteration 39580 (1.65589 iter/s, 6.03906s/10 iters), loss = 8.66041
I0523 03:12:15.084360 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66041 (* 1 = 8.66041 loss)
I0523 03:12:15.810326 34682 sgd_solver.cpp:112] Iteration 39580, lr = 0.01
I0523 03:12:20.843698 34682 solver.cpp:239] Iteration 39590 (1.73638 iter/s, 5.7591s/10 iters), loss = 8.74618
I0523 03:12:20.843742 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74618 (* 1 = 8.74618 loss)
I0523 03:12:20.900774 34682 sgd_solver.cpp:112] Iteration 39590, lr = 0.01
I0523 03:12:24.583767 34682 solver.cpp:239] Iteration 39600 (2.67389 iter/s, 3.73986s/10 iters), loss = 8.48717
I0523 03:12:24.583824 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48717 (* 1 = 8.48717 loss)
I0523 03:12:24.656692 34682 sgd_solver.cpp:112] Iteration 39600, lr = 0.01
I0523 03:12:27.969203 34682 solver.cpp:239] Iteration 39610 (2.954 iter/s, 3.38524s/10 iters), loss = 9.49894
I0523 03:12:27.969372 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.49894 (* 1 = 9.49894 loss)
I0523 03:12:28.052825 34682 sgd_solver.cpp:112] Iteration 39610, lr = 0.01
I0523 03:12:32.319816 34682 solver.cpp:239] Iteration 39620 (2.29872 iter/s, 4.35026s/10 iters), loss = 8.21949
I0523 03:12:32.319867 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21949 (* 1 = 8.21949 loss)
I0523 03:12:32.829555 34682 sgd_solver.cpp:112] Iteration 39620, lr = 0.01
I0523 03:12:36.584621 34682 solver.cpp:239] Iteration 39630 (2.3449 iter/s, 4.26457s/10 iters), loss = 8.59071
I0523 03:12:36.584671 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59071 (* 1 = 8.59071 loss)
I0523 03:12:36.647840 34682 sgd_solver.cpp:112] Iteration 39630, lr = 0.01
I0523 03:12:40.007988 34682 solver.cpp:239] Iteration 39640 (2.92126 iter/s, 3.42318s/10 iters), loss = 9.39705
I0523 03:12:40.008028 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.39705 (* 1 = 9.39705 loss)
I0523 03:12:40.746867 34682 sgd_solver.cpp:112] Iteration 39640, lr = 0.01
I0523 03:12:45.367290 34682 solver.cpp:239] Iteration 39650 (1.86754 iter/s, 5.35464s/10 iters), loss = 8.92935
I0523 03:12:45.367338 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92935 (* 1 = 8.92935 loss)
I0523 03:12:46.197357 34682 sgd_solver.cpp:112] Iteration 39650, lr = 0.01
I0523 03:12:49.181736 34682 solver.cpp:239] Iteration 39660 (2.62175 iter/s, 3.81424s/10 iters), loss = 9.73545
I0523 03:12:49.181782 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.73545 (* 1 = 9.73545 loss)
I0523 03:12:50.020081 34682 sgd_solver.cpp:112] Iteration 39660, lr = 0.01
I0523 03:12:54.330425 34682 solver.cpp:239] Iteration 39670 (1.94234 iter/s, 5.14843s/10 iters), loss = 9.24021
I0523 03:12:54.330492 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24021 (* 1 = 9.24021 loss)
I0523 03:12:54.399152 34682 sgd_solver.cpp:112] Iteration 39670, lr = 0.01
I0523 03:13:00.321897 34682 solver.cpp:239] Iteration 39680 (1.66913 iter/s, 5.99116s/10 iters), loss = 8.30105
I0523 03:13:00.322211 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30105 (* 1 = 8.30105 loss)
I0523 03:13:01.139386 34682 sgd_solver.cpp:112] Iteration 39680, lr = 0.01
I0523 03:13:06.802304 34682 solver.cpp:239] Iteration 39690 (1.54324 iter/s, 6.47986s/10 iters), loss = 8.90983
I0523 03:13:06.802352 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90983 (* 1 = 8.90983 loss)
I0523 03:13:06.853338 34682 sgd_solver.cpp:112] Iteration 39690, lr = 0.01
I0523 03:13:11.734791 34682 solver.cpp:239] Iteration 39700 (2.02748 iter/s, 4.93223s/10 iters), loss = 9.31713
I0523 03:13:11.734839 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31713 (* 1 = 9.31713 loss)
I0523 03:13:11.800997 34682 sgd_solver.cpp:112] Iteration 39700, lr = 0.01
I0523 03:13:15.441133 34682 solver.cpp:239] Iteration 39710 (2.69823 iter/s, 3.70613s/10 iters), loss = 9.10895
I0523 03:13:15.441180 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10895 (* 1 = 9.10895 loss)
I0523 03:13:15.514356 34682 sgd_solver.cpp:112] Iteration 39710, lr = 0.01
I0523 03:13:21.250639 34682 solver.cpp:239] Iteration 39720 (1.7214 iter/s, 5.80921s/10 iters), loss = 9.3517
I0523 03:13:21.250727 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3517 (* 1 = 9.3517 loss)
I0523 03:13:22.121246 34682 sgd_solver.cpp:112] Iteration 39720, lr = 0.01
I0523 03:13:27.718261 34682 solver.cpp:239] Iteration 39730 (1.54624 iter/s, 6.4673s/10 iters), loss = 9.32203
I0523 03:13:27.718312 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32203 (* 1 = 9.32203 loss)
I0523 03:13:28.379856 34682 sgd_solver.cpp:112] Iteration 39730, lr = 0.01
I0523 03:13:32.910250 34682 solver.cpp:239] Iteration 39740 (1.92614 iter/s, 5.19173s/10 iters), loss = 8.21976
I0523 03:13:32.910496 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21976 (* 1 = 8.21976 loss)
I0523 03:13:32.972223 34682 sgd_solver.cpp:112] Iteration 39740, lr = 0.01
I0523 03:13:37.421149 34682 solver.cpp:239] Iteration 39750 (2.21705 iter/s, 4.51049s/10 iters), loss = 9.1785
I0523 03:13:37.421200 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1785 (* 1 = 9.1785 loss)
I0523 03:13:38.136939 34682 sgd_solver.cpp:112] Iteration 39750, lr = 0.01
I0523 03:13:41.634323 34682 solver.cpp:239] Iteration 39760 (2.37363 iter/s, 4.21295s/10 iters), loss = 8.05009
I0523 03:13:41.634373 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05009 (* 1 = 8.05009 loss)
I0523 03:13:42.461990 34682 sgd_solver.cpp:112] Iteration 39760, lr = 0.01
I0523 03:13:46.647830 34682 solver.cpp:239] Iteration 39770 (1.99471 iter/s, 5.01325s/10 iters), loss = 9.34149
I0523 03:13:46.647888 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34149 (* 1 = 9.34149 loss)
I0523 03:13:47.482224 34682 sgd_solver.cpp:112] Iteration 39770, lr = 0.01
I0523 03:13:52.803397 34682 solver.cpp:239] Iteration 39780 (1.62463 iter/s, 6.15526s/10 iters), loss = 8.41902
I0523 03:13:52.803447 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41902 (* 1 = 8.41902 loss)
I0523 03:13:53.616518 34682 sgd_solver.cpp:112] Iteration 39780, lr = 0.01
I0523 03:13:59.199299 34682 solver.cpp:239] Iteration 39790 (1.56358 iter/s, 6.3956s/10 iters), loss = 7.96344
I0523 03:13:59.199352 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96344 (* 1 = 7.96344 loss)
I0523 03:14:00.074405 34682 sgd_solver.cpp:112] Iteration 39790, lr = 0.01
I0523 03:14:04.963569 34682 solver.cpp:239] Iteration 39800 (1.73491 iter/s, 5.76398s/10 iters), loss = 8.32833
I0523 03:14:04.963745 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32833 (* 1 = 8.32833 loss)
I0523 03:14:05.028697 34682 sgd_solver.cpp:112] Iteration 39800, lr = 0.01
I0523 03:14:09.241498 34682 solver.cpp:239] Iteration 39810 (2.33776 iter/s, 4.27759s/10 iters), loss = 8.7061
I0523 03:14:09.241539 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7061 (* 1 = 8.7061 loss)
I0523 03:14:09.303818 34682 sgd_solver.cpp:112] Iteration 39810, lr = 0.01
I0523 03:14:15.784166 34682 solver.cpp:239] Iteration 39820 (1.5285 iter/s, 6.54236s/10 iters), loss = 8.23439
I0523 03:14:15.784214 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23439 (* 1 = 8.23439 loss)
I0523 03:14:15.851459 34682 sgd_solver.cpp:112] Iteration 39820, lr = 0.01
I0523 03:14:20.096891 34682 solver.cpp:239] Iteration 39830 (2.31884 iter/s, 4.3125s/10 iters), loss = 9.2954
I0523 03:14:20.096932 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2954 (* 1 = 9.2954 loss)
I0523 03:14:20.173400 34682 sgd_solver.cpp:112] Iteration 39830, lr = 0.01
I0523 03:14:23.204586 34682 solver.cpp:239] Iteration 39840 (3.218 iter/s, 3.10752s/10 iters), loss = 8.98163
I0523 03:14:23.204636 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98163 (* 1 = 8.98163 loss)
I0523 03:14:24.047299 34682 sgd_solver.cpp:112] Iteration 39840, lr = 0.01
I0523 03:14:30.332428 34682 solver.cpp:239] Iteration 39850 (1.40302 iter/s, 7.1275s/10 iters), loss = 8.83453
I0523 03:14:30.332495 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83453 (* 1 = 8.83453 loss)
I0523 03:14:31.095459 34682 sgd_solver.cpp:112] Iteration 39850, lr = 0.01
I0523 03:14:35.561058 34682 solver.cpp:239] Iteration 39860 (1.91265 iter/s, 5.22835s/10 iters), loss = 9.15937
I0523 03:14:35.561357 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15937 (* 1 = 9.15937 loss)
I0523 03:14:35.639768 34682 sgd_solver.cpp:112] Iteration 39860, lr = 0.01
I0523 03:14:40.995404 34682 solver.cpp:239] Iteration 39870 (1.84032 iter/s, 5.43385s/10 iters), loss = 9.373
I0523 03:14:40.995446 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.373 (* 1 = 9.373 loss)
I0523 03:14:41.066139 34682 sgd_solver.cpp:112] Iteration 39870, lr = 0.01
I0523 03:14:44.279322 34682 solver.cpp:239] Iteration 39880 (3.04531 iter/s, 3.28374s/10 iters), loss = 9.15524
I0523 03:14:44.279368 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15524 (* 1 = 9.15524 loss)
I0523 03:14:45.051396 34682 sgd_solver.cpp:112] Iteration 39880, lr = 0.01
I0523 03:14:49.150852 34682 solver.cpp:239] Iteration 39890 (2.05285 iter/s, 4.87128s/10 iters), loss = 8.54802
I0523 03:14:49.150902 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54802 (* 1 = 8.54802 loss)
I0523 03:14:49.893707 34682 sgd_solver.cpp:112] Iteration 39890, lr = 0.01
I0523 03:14:53.869885 34682 solver.cpp:239] Iteration 39900 (2.11919 iter/s, 4.71879s/10 iters), loss = 9.55236
I0523 03:14:53.869937 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.55236 (* 1 = 9.55236 loss)
I0523 03:14:53.935034 34682 sgd_solver.cpp:112] Iteration 39900, lr = 0.01
I0523 03:14:59.308795 34682 solver.cpp:239] Iteration 39910 (1.8387 iter/s, 5.43863s/10 iters), loss = 9.50679
I0523 03:14:59.308872 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.50679 (* 1 = 9.50679 loss)
I0523 03:14:59.367781 34682 sgd_solver.cpp:112] Iteration 39910, lr = 0.01
I0523 03:15:03.204311 34682 solver.cpp:239] Iteration 39920 (2.56723 iter/s, 3.89526s/10 iters), loss = 8.86578
I0523 03:15:03.204368 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86578 (* 1 = 8.86578 loss)
I0523 03:15:03.262056 34682 sgd_solver.cpp:112] Iteration 39920, lr = 0.01
I0523 03:15:07.208750 34682 solver.cpp:239] Iteration 39930 (2.49737 iter/s, 4.00422s/10 iters), loss = 8.19315
I0523 03:15:07.208917 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19315 (* 1 = 8.19315 loss)
I0523 03:15:07.294751 34682 sgd_solver.cpp:112] Iteration 39930, lr = 0.01
I0523 03:15:10.540051 34682 solver.cpp:239] Iteration 39940 (3.00208 iter/s, 3.33102s/10 iters), loss = 8.67592
I0523 03:15:10.540093 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67592 (* 1 = 8.67592 loss)
I0523 03:15:10.606212 34682 sgd_solver.cpp:112] Iteration 39940, lr = 0.01
I0523 03:15:16.780628 34682 solver.cpp:239] Iteration 39950 (1.6025 iter/s, 6.24027s/10 iters), loss = 8.43708
I0523 03:15:16.780694 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43708 (* 1 = 8.43708 loss)
I0523 03:15:17.353216 34682 sgd_solver.cpp:112] Iteration 39950, lr = 0.01
I0523 03:15:21.663065 34682 solver.cpp:239] Iteration 39960 (2.04827 iter/s, 4.88218s/10 iters), loss = 8.42855
I0523 03:15:21.663107 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42855 (* 1 = 8.42855 loss)
I0523 03:15:21.726351 34682 sgd_solver.cpp:112] Iteration 39960, lr = 0.01
I0523 03:15:26.792547 34682 solver.cpp:239] Iteration 39970 (1.94961 iter/s, 5.12922s/10 iters), loss = 8.22966
I0523 03:15:26.792593 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22966 (* 1 = 8.22966 loss)
I0523 03:15:26.866542 34682 sgd_solver.cpp:112] Iteration 39970, lr = 0.01
I0523 03:15:30.644521 34682 solver.cpp:239] Iteration 39980 (2.59621 iter/s, 3.85177s/10 iters), loss = 9.2894
I0523 03:15:30.644568 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2894 (* 1 = 9.2894 loss)
I0523 03:15:31.468400 34682 sgd_solver.cpp:112] Iteration 39980, lr = 0.01
I0523 03:15:36.510308 34682 solver.cpp:239] Iteration 39990 (1.70489 iter/s, 5.8655s/10 iters), loss = 8.36532
I0523 03:15:36.510359 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36532 (* 1 = 8.36532 loss)
I0523 03:15:36.583492 34682 sgd_solver.cpp:112] Iteration 39990, lr = 0.01
I0523 03:15:42.338845 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_40000.caffemodel
I0523 03:15:43.464938 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_40000.solverstate
I0523 03:15:43.731312 34682 solver.cpp:239] Iteration 40000 (1.38491 iter/s, 7.22066s/10 iters), loss = 7.96206
I0523 03:15:43.731353 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96206 (* 1 = 7.96206 loss)
I0523 03:15:44.514078 34682 sgd_solver.cpp:112] Iteration 40000, lr = 0.01
I0523 03:15:48.645304 34682 solver.cpp:239] Iteration 40010 (2.03511 iter/s, 4.91374s/10 iters), loss = 7.90775
I0523 03:15:48.645368 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90775 (* 1 = 7.90775 loss)
I0523 03:15:49.468698 34682 sgd_solver.cpp:112] Iteration 40010, lr = 0.01
I0523 03:15:52.849227 34682 solver.cpp:239] Iteration 40020 (2.37886 iter/s, 4.20369s/10 iters), loss = 7.98372
I0523 03:15:52.849278 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98372 (* 1 = 7.98372 loss)
I0523 03:15:53.672993 34682 sgd_solver.cpp:112] Iteration 40020, lr = 0.01
I0523 03:16:00.652773 34682 solver.cpp:239] Iteration 40030 (1.28153 iter/s, 7.80318s/10 iters), loss = 8.97642
I0523 03:16:00.652828 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97642 (* 1 = 8.97642 loss)
I0523 03:16:01.409636 34682 sgd_solver.cpp:112] Iteration 40030, lr = 0.01
I0523 03:16:04.950527 34682 solver.cpp:239] Iteration 40040 (2.32692 iter/s, 4.29752s/10 iters), loss = 8.67745
I0523 03:16:04.950582 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67745 (* 1 = 8.67745 loss)
I0523 03:16:05.027187 34682 sgd_solver.cpp:112] Iteration 40040, lr = 0.01
I0523 03:16:09.917176 34682 solver.cpp:239] Iteration 40050 (2.01354 iter/s, 4.96639s/10 iters), loss = 8.76165
I0523 03:16:09.917227 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76165 (* 1 = 8.76165 loss)
I0523 03:16:10.649329 34682 sgd_solver.cpp:112] Iteration 40050, lr = 0.01
I0523 03:16:17.328754 34682 solver.cpp:239] Iteration 40060 (1.3493 iter/s, 7.41123s/10 iters), loss = 8.7027
I0523 03:16:17.328945 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7027 (* 1 = 8.7027 loss)
I0523 03:16:18.214640 34682 sgd_solver.cpp:112] Iteration 40060, lr = 0.01
I0523 03:16:22.998668 34682 solver.cpp:239] Iteration 40070 (1.76382 iter/s, 5.66952s/10 iters), loss = 8.54086
I0523 03:16:22.998749 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54086 (* 1 = 8.54086 loss)
I0523 03:16:23.897666 34682 sgd_solver.cpp:112] Iteration 40070, lr = 0.01
I0523 03:16:30.285606 34682 solver.cpp:239] Iteration 40080 (1.37239 iter/s, 7.28654s/10 iters), loss = 9.1288
I0523 03:16:30.285673 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1288 (* 1 = 9.1288 loss)
I0523 03:16:30.933698 34682 sgd_solver.cpp:112] Iteration 40080, lr = 0.01
I0523 03:16:37.008234 34682 solver.cpp:239] Iteration 40090 (1.48759 iter/s, 6.7223s/10 iters), loss = 8.72903
I0523 03:16:37.008275 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72903 (* 1 = 8.72903 loss)
I0523 03:16:37.082481 34682 sgd_solver.cpp:112] Iteration 40090, lr = 0.01
I0523 03:16:42.595283 34682 solver.cpp:239] Iteration 40100 (1.78994 iter/s, 5.58678s/10 iters), loss = 8.67251
I0523 03:16:42.595338 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67251 (* 1 = 8.67251 loss)
I0523 03:16:43.360179 34682 sgd_solver.cpp:112] Iteration 40100, lr = 0.01
I0523 03:16:48.247794 34682 solver.cpp:239] Iteration 40110 (1.76921 iter/s, 5.65223s/10 iters), loss = 8.73233
I0523 03:16:48.248112 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73233 (* 1 = 8.73233 loss)
I0523 03:16:48.995040 34682 sgd_solver.cpp:112] Iteration 40110, lr = 0.01
I0523 03:16:52.390609 34682 solver.cpp:239] Iteration 40120 (2.41632 iter/s, 4.13852s/10 iters), loss = 9.25732
I0523 03:16:52.390652 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25732 (* 1 = 9.25732 loss)
I0523 03:16:52.464299 34682 sgd_solver.cpp:112] Iteration 40120, lr = 0.01
I0523 03:16:57.109488 34682 solver.cpp:239] Iteration 40130 (2.11926 iter/s, 4.71863s/10 iters), loss = 7.85039
I0523 03:16:57.109568 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.85039 (* 1 = 7.85039 loss)
I0523 03:16:57.179261 34682 sgd_solver.cpp:112] Iteration 40130, lr = 0.01
I0523 03:17:00.593935 34682 solver.cpp:239] Iteration 40140 (2.87008 iter/s, 3.48422s/10 iters), loss = 8.32425
I0523 03:17:00.593989 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32425 (* 1 = 8.32425 loss)
I0523 03:17:01.125099 34682 sgd_solver.cpp:112] Iteration 40140, lr = 0.01
I0523 03:17:06.042013 34682 solver.cpp:239] Iteration 40150 (1.8356 iter/s, 5.4478s/10 iters), loss = 8.29012
I0523 03:17:06.042070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29012 (* 1 = 8.29012 loss)
I0523 03:17:06.785905 34682 sgd_solver.cpp:112] Iteration 40150, lr = 0.01
I0523 03:17:11.027485 34682 solver.cpp:239] Iteration 40160 (2.00593 iter/s, 4.98521s/10 iters), loss = 8.60246
I0523 03:17:11.027534 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60246 (* 1 = 8.60246 loss)
I0523 03:17:11.811718 34682 sgd_solver.cpp:112] Iteration 40160, lr = 0.01
I0523 03:17:16.479372 34682 solver.cpp:239] Iteration 40170 (1.83432 iter/s, 5.45161s/10 iters), loss = 8.35154
I0523 03:17:16.479419 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35154 (* 1 = 8.35154 loss)
I0523 03:17:16.540632 34682 sgd_solver.cpp:112] Iteration 40170, lr = 0.01
I0523 03:17:23.771097 34682 solver.cpp:239] Iteration 40180 (1.37148 iter/s, 7.29139s/10 iters), loss = 8.13472
I0523 03:17:23.771306 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13472 (* 1 = 8.13472 loss)
I0523 03:17:23.841524 34682 sgd_solver.cpp:112] Iteration 40180, lr = 0.01
I0523 03:17:25.933128 34682 solver.cpp:239] Iteration 40190 (4.62589 iter/s, 2.16174s/10 iters), loss = 9.42736
I0523 03:17:25.933182 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42736 (* 1 = 9.42736 loss)
I0523 03:17:26.003028 34682 sgd_solver.cpp:112] Iteration 40190, lr = 0.01
I0523 03:17:31.659036 34682 solver.cpp:239] Iteration 40200 (1.74654 iter/s, 5.72562s/10 iters), loss = 7.30611
I0523 03:17:31.659090 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.30611 (* 1 = 7.30611 loss)
I0523 03:17:32.488265 34682 sgd_solver.cpp:112] Iteration 40200, lr = 0.01
I0523 03:17:37.200557 34682 solver.cpp:239] Iteration 40210 (1.80466 iter/s, 5.54122s/10 iters), loss = 9.02479
I0523 03:17:37.200611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02479 (* 1 = 9.02479 loss)
I0523 03:17:38.051482 34682 sgd_solver.cpp:112] Iteration 40210, lr = 0.01
I0523 03:17:42.438146 34682 solver.cpp:239] Iteration 40220 (1.90937 iter/s, 5.23732s/10 iters), loss = 9.26845
I0523 03:17:42.438200 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.26845 (* 1 = 9.26845 loss)
I0523 03:17:42.493963 34682 sgd_solver.cpp:112] Iteration 40220, lr = 0.01
I0523 03:17:45.839989 34682 solver.cpp:239] Iteration 40230 (2.93976 iter/s, 3.40164s/10 iters), loss = 8.13245
I0523 03:17:45.840042 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13245 (* 1 = 8.13245 loss)
I0523 03:17:45.910238 34682 sgd_solver.cpp:112] Iteration 40230, lr = 0.01
I0523 03:17:49.901665 34682 solver.cpp:239] Iteration 40240 (2.46217 iter/s, 4.06146s/10 iters), loss = 9.37485
I0523 03:17:49.901705 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.37485 (* 1 = 9.37485 loss)
I0523 03:17:49.983451 34682 sgd_solver.cpp:112] Iteration 40240, lr = 0.01
I0523 03:17:54.444950 34682 solver.cpp:239] Iteration 40250 (2.20117 iter/s, 4.54305s/10 iters), loss = 8.76828
I0523 03:17:54.445173 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76828 (* 1 = 8.76828 loss)
I0523 03:17:54.513207 34682 sgd_solver.cpp:112] Iteration 40250, lr = 0.01
I0523 03:18:00.960079 34682 solver.cpp:239] Iteration 40260 (1.535 iter/s, 6.51467s/10 iters), loss = 8.35943
I0523 03:18:00.960120 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35943 (* 1 = 8.35943 loss)
I0523 03:18:01.025403 34682 sgd_solver.cpp:112] Iteration 40260, lr = 0.01
I0523 03:18:05.192030 34682 solver.cpp:239] Iteration 40270 (2.3631 iter/s, 4.23173s/10 iters), loss = 8.92835
I0523 03:18:05.192080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92835 (* 1 = 8.92835 loss)
I0523 03:18:05.250699 34682 sgd_solver.cpp:112] Iteration 40270, lr = 0.01
I0523 03:18:11.186834 34682 solver.cpp:239] Iteration 40280 (1.66819 iter/s, 5.99452s/10 iters), loss = 8.80088
I0523 03:18:11.186869 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80088 (* 1 = 8.80088 loss)
I0523 03:18:11.256911 34682 sgd_solver.cpp:112] Iteration 40280, lr = 0.01
I0523 03:18:15.215505 34682 solver.cpp:239] Iteration 40290 (2.48234 iter/s, 4.02845s/10 iters), loss = 7.93454
I0523 03:18:15.215566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93454 (* 1 = 7.93454 loss)
I0523 03:18:16.032027 34682 sgd_solver.cpp:112] Iteration 40290, lr = 0.01
I0523 03:18:18.794574 34682 solver.cpp:239] Iteration 40300 (2.79419 iter/s, 3.57886s/10 iters), loss = 8.36954
I0523 03:18:18.794623 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36954 (* 1 = 8.36954 loss)
I0523 03:18:19.635025 34682 sgd_solver.cpp:112] Iteration 40300, lr = 0.01
I0523 03:18:26.783215 34682 solver.cpp:239] Iteration 40310 (1.25184 iter/s, 7.98826s/10 iters), loss = 8.82733
I0523 03:18:26.783454 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82733 (* 1 = 8.82733 loss)
I0523 03:18:27.518265 34682 sgd_solver.cpp:112] Iteration 40310, lr = 0.01
I0523 03:18:31.630957 34682 solver.cpp:239] Iteration 40320 (2.06299 iter/s, 4.84733s/10 iters), loss = 8.1812
I0523 03:18:31.631026 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1812 (* 1 = 8.1812 loss)
I0523 03:18:31.716585 34682 sgd_solver.cpp:112] Iteration 40320, lr = 0.01
I0523 03:18:37.130466 34682 solver.cpp:239] Iteration 40330 (1.81844 iter/s, 5.49922s/10 iters), loss = 8.47818
I0523 03:18:37.130506 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47818 (* 1 = 8.47818 loss)
I0523 03:18:37.189726 34682 sgd_solver.cpp:112] Iteration 40330, lr = 0.01
I0523 03:18:39.562108 34682 solver.cpp:239] Iteration 40340 (4.1127 iter/s, 2.43149s/10 iters), loss = 8.2669
I0523 03:18:39.562157 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2669 (* 1 = 8.2669 loss)
I0523 03:18:39.627138 34682 sgd_solver.cpp:112] Iteration 40340, lr = 0.01
I0523 03:18:43.865955 34682 solver.cpp:239] Iteration 40350 (2.32363 iter/s, 4.30362s/10 iters), loss = 8.5776
I0523 03:18:43.866001 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5776 (* 1 = 8.5776 loss)
I0523 03:18:44.691949 34682 sgd_solver.cpp:112] Iteration 40350, lr = 0.01
I0523 03:18:48.809746 34682 solver.cpp:239] Iteration 40360 (2.02285 iter/s, 4.94352s/10 iters), loss = 8.42821
I0523 03:18:48.809818 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42821 (* 1 = 8.42821 loss)
I0523 03:18:48.872319 34682 sgd_solver.cpp:112] Iteration 40360, lr = 0.01
I0523 03:18:52.689813 34682 solver.cpp:239] Iteration 40370 (2.57743 iter/s, 3.87983s/10 iters), loss = 9.003
I0523 03:18:52.689863 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.003 (* 1 = 9.003 loss)
I0523 03:18:52.772606 34682 sgd_solver.cpp:112] Iteration 40370, lr = 0.01
I0523 03:18:58.197784 34682 solver.cpp:239] Iteration 40380 (1.81564 iter/s, 5.50769s/10 iters), loss = 8.71081
I0523 03:18:58.198060 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71081 (* 1 = 8.71081 loss)
I0523 03:18:58.266157 34682 sgd_solver.cpp:112] Iteration 40380, lr = 0.01
I0523 03:19:03.844925 34682 solver.cpp:239] Iteration 40390 (1.77096 iter/s, 5.64666s/10 iters), loss = 9.4712
I0523 03:19:03.844976 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4712 (* 1 = 9.4712 loss)
I0523 03:19:03.914149 34682 sgd_solver.cpp:112] Iteration 40390, lr = 0.01
I0523 03:19:08.671314 34682 solver.cpp:239] Iteration 40400 (2.07205 iter/s, 4.82613s/10 iters), loss = 8.84799
I0523 03:19:08.671383 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84799 (* 1 = 8.84799 loss)
I0523 03:19:09.346289 34682 sgd_solver.cpp:112] Iteration 40400, lr = 0.01
I0523 03:19:14.314767 34682 solver.cpp:239] Iteration 40410 (1.77206 iter/s, 5.64316s/10 iters), loss = 10.1674
I0523 03:19:14.314818 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1674 (* 1 = 10.1674 loss)
I0523 03:19:14.372378 34682 sgd_solver.cpp:112] Iteration 40410, lr = 0.01
I0523 03:19:20.893977 34682 solver.cpp:239] Iteration 40420 (1.52001 iter/s, 6.57889s/10 iters), loss = 9.34659
I0523 03:19:20.894027 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34659 (* 1 = 9.34659 loss)
I0523 03:19:20.947541 34682 sgd_solver.cpp:112] Iteration 40420, lr = 0.01
I0523 03:19:24.106425 34682 solver.cpp:239] Iteration 40430 (3.11311 iter/s, 3.21222s/10 iters), loss = 9.44026
I0523 03:19:24.106482 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44026 (* 1 = 9.44026 loss)
I0523 03:19:24.174274 34682 sgd_solver.cpp:112] Iteration 40430, lr = 0.01
I0523 03:19:28.012162 34682 solver.cpp:239] Iteration 40440 (2.56048 iter/s, 3.90552s/10 iters), loss = 8.86012
I0523 03:19:28.012225 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86012 (* 1 = 8.86012 loss)
I0523 03:19:28.647114 34682 sgd_solver.cpp:112] Iteration 40440, lr = 0.01
I0523 03:19:34.101550 34682 solver.cpp:239] Iteration 40450 (1.64229 iter/s, 6.08907s/10 iters), loss = 9.00542
I0523 03:19:34.101609 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00542 (* 1 = 9.00542 loss)
I0523 03:19:34.169708 34682 sgd_solver.cpp:112] Iteration 40450, lr = 0.01
I0523 03:19:39.499349 34682 solver.cpp:239] Iteration 40460 (1.8527 iter/s, 5.39752s/10 iters), loss = 8.6987
I0523 03:19:39.499411 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6987 (* 1 = 8.6987 loss)
I0523 03:19:39.557646 34682 sgd_solver.cpp:112] Iteration 40460, lr = 0.01
I0523 03:19:42.844625 34682 solver.cpp:239] Iteration 40470 (2.98948 iter/s, 3.34506s/10 iters), loss = 8.35369
I0523 03:19:42.844683 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35369 (* 1 = 8.35369 loss)
I0523 03:19:43.559670 34682 sgd_solver.cpp:112] Iteration 40470, lr = 0.01
I0523 03:19:48.369915 34682 solver.cpp:239] Iteration 40480 (1.80995 iter/s, 5.52501s/10 iters), loss = 8.59008
I0523 03:19:48.369961 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59008 (* 1 = 8.59008 loss)
I0523 03:19:49.207494 34682 sgd_solver.cpp:112] Iteration 40480, lr = 0.01
I0523 03:19:52.560556 34682 solver.cpp:239] Iteration 40490 (2.38768 iter/s, 4.18817s/10 iters), loss = 8.65609
I0523 03:19:52.560595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65609 (* 1 = 8.65609 loss)
I0523 03:19:52.633504 34682 sgd_solver.cpp:112] Iteration 40490, lr = 0.01
I0523 03:19:57.061152 34682 solver.cpp:239] Iteration 40500 (2.22206 iter/s, 4.50033s/10 iters), loss = 8.6071
I0523 03:19:57.061224 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6071 (* 1 = 8.6071 loss)
I0523 03:19:57.800298 34682 sgd_solver.cpp:112] Iteration 40500, lr = 0.01
I0523 03:20:02.612116 34682 solver.cpp:239] Iteration 40510 (1.80159 iter/s, 5.55066s/10 iters), loss = 9.17567
I0523 03:20:02.612323 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17567 (* 1 = 9.17567 loss)
I0523 03:20:02.924547 34682 sgd_solver.cpp:112] Iteration 40510, lr = 0.01
I0523 03:20:08.660277 34682 solver.cpp:239] Iteration 40520 (1.65352 iter/s, 6.04771s/10 iters), loss = 8.45748
I0523 03:20:08.660329 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45748 (* 1 = 8.45748 loss)
I0523 03:20:09.483767 34682 sgd_solver.cpp:112] Iteration 40520, lr = 0.01
I0523 03:20:14.497598 34682 solver.cpp:239] Iteration 40530 (1.71321 iter/s, 5.83701s/10 iters), loss = 9.27838
I0523 03:20:14.497666 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27838 (* 1 = 9.27838 loss)
I0523 03:20:14.573108 34682 sgd_solver.cpp:112] Iteration 40530, lr = 0.01
I0523 03:20:18.491681 34682 solver.cpp:239] Iteration 40540 (2.50384 iter/s, 3.99386s/10 iters), loss = 8.93695
I0523 03:20:18.491722 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93695 (* 1 = 8.93695 loss)
I0523 03:20:19.323858 34682 sgd_solver.cpp:112] Iteration 40540, lr = 0.01
I0523 03:20:21.852056 34682 solver.cpp:239] Iteration 40550 (2.97605 iter/s, 3.36016s/10 iters), loss = 9.64652
I0523 03:20:21.852129 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.64652 (* 1 = 9.64652 loss)
I0523 03:20:21.921339 34682 sgd_solver.cpp:112] Iteration 40550, lr = 0.01
I0523 03:20:27.546327 34682 solver.cpp:239] Iteration 40560 (1.75625 iter/s, 5.69396s/10 iters), loss = 8.00277
I0523 03:20:27.546372 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00277 (* 1 = 8.00277 loss)
I0523 03:20:27.613006 34682 sgd_solver.cpp:112] Iteration 40560, lr = 0.01
I0523 03:20:30.705669 34682 solver.cpp:239] Iteration 40570 (3.1654 iter/s, 3.15916s/10 iters), loss = 9.33442
I0523 03:20:30.705723 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33442 (* 1 = 9.33442 loss)
I0523 03:20:30.769891 34682 sgd_solver.cpp:112] Iteration 40570, lr = 0.01
I0523 03:20:34.764971 34682 solver.cpp:239] Iteration 40580 (2.46363 iter/s, 4.05905s/10 iters), loss = 9.2131
I0523 03:20:34.765287 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2131 (* 1 = 9.2131 loss)
I0523 03:20:35.398092 34682 sgd_solver.cpp:112] Iteration 40580, lr = 0.01
I0523 03:20:39.624935 34682 solver.cpp:239] Iteration 40590 (2.05783 iter/s, 4.85948s/10 iters), loss = 8.4424
I0523 03:20:39.624994 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4424 (* 1 = 8.4424 loss)
I0523 03:20:40.461282 34682 sgd_solver.cpp:112] Iteration 40590, lr = 0.01
I0523 03:20:43.170704 34682 solver.cpp:239] Iteration 40600 (2.82043 iter/s, 3.54556s/10 iters), loss = 8.39859
I0523 03:20:43.170750 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39859 (* 1 = 8.39859 loss)
I0523 03:20:44.010594 34682 sgd_solver.cpp:112] Iteration 40600, lr = 0.01
I0523 03:20:48.004039 34682 solver.cpp:239] Iteration 40610 (2.06907 iter/s, 4.83308s/10 iters), loss = 8.34089
I0523 03:20:48.004101 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34089 (* 1 = 8.34089 loss)
I0523 03:20:48.076731 34682 sgd_solver.cpp:112] Iteration 40610, lr = 0.01
I0523 03:20:53.173305 34682 solver.cpp:239] Iteration 40620 (1.93461 iter/s, 5.169s/10 iters), loss = 9.05626
I0523 03:20:53.173357 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05626 (* 1 = 9.05626 loss)
I0523 03:20:53.235640 34682 sgd_solver.cpp:112] Iteration 40620, lr = 0.01
I0523 03:20:57.469899 34682 solver.cpp:239] Iteration 40630 (2.32755 iter/s, 4.29637s/10 iters), loss = 9.23914
I0523 03:20:57.469961 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23914 (* 1 = 9.23914 loss)
I0523 03:20:57.537413 34682 sgd_solver.cpp:112] Iteration 40630, lr = 0.01
I0523 03:21:01.518383 34682 solver.cpp:239] Iteration 40640 (2.4702 iter/s, 4.04825s/10 iters), loss = 8.94376
I0523 03:21:01.518438 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94376 (* 1 = 8.94376 loss)
I0523 03:21:02.376997 34682 sgd_solver.cpp:112] Iteration 40640, lr = 0.01
I0523 03:21:07.653216 34682 solver.cpp:239] Iteration 40650 (1.63012 iter/s, 6.13453s/10 iters), loss = 9.80332
I0523 03:21:07.653506 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.80332 (* 1 = 9.80332 loss)
I0523 03:21:07.723109 34682 sgd_solver.cpp:112] Iteration 40650, lr = 0.01
I0523 03:21:11.538934 34682 solver.cpp:239] Iteration 40660 (2.57381 iter/s, 3.88528s/10 iters), loss = 8.4983
I0523 03:21:11.538987 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4983 (* 1 = 8.4983 loss)
I0523 03:21:12.302181 34682 sgd_solver.cpp:112] Iteration 40660, lr = 0.01
I0523 03:21:15.709648 34682 solver.cpp:239] Iteration 40670 (2.3978 iter/s, 4.17049s/10 iters), loss = 8.585
I0523 03:21:15.709689 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.585 (* 1 = 8.585 loss)
I0523 03:21:16.543525 34682 sgd_solver.cpp:112] Iteration 40670, lr = 0.01
I0523 03:21:20.664918 34682 solver.cpp:239] Iteration 40680 (2.01816 iter/s, 4.95502s/10 iters), loss = 7.80434
I0523 03:21:20.664990 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80434 (* 1 = 7.80434 loss)
I0523 03:21:20.903590 34682 sgd_solver.cpp:112] Iteration 40680, lr = 0.01
I0523 03:21:24.303856 34682 solver.cpp:239] Iteration 40690 (2.74823 iter/s, 3.63871s/10 iters), loss = 8.83113
I0523 03:21:24.303913 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83113 (* 1 = 8.83113 loss)
I0523 03:21:24.368561 34682 sgd_solver.cpp:112] Iteration 40690, lr = 0.01
I0523 03:21:28.941565 34682 solver.cpp:239] Iteration 40700 (2.15636 iter/s, 4.63744s/10 iters), loss = 10.0827
I0523 03:21:28.941614 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0827 (* 1 = 10.0827 loss)
I0523 03:21:29.014108 34682 sgd_solver.cpp:112] Iteration 40700, lr = 0.01
I0523 03:21:36.044317 34682 solver.cpp:239] Iteration 40710 (1.40797 iter/s, 7.10242s/10 iters), loss = 8.73273
I0523 03:21:36.044369 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73273 (* 1 = 8.73273 loss)
I0523 03:21:36.110445 34682 sgd_solver.cpp:112] Iteration 40710, lr = 0.01
I0523 03:21:42.386504 34682 solver.cpp:239] Iteration 40720 (1.57682 iter/s, 6.34187s/10 iters), loss = 8.04015
I0523 03:21:42.386768 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04015 (* 1 = 8.04015 loss)
I0523 03:21:43.205368 34682 sgd_solver.cpp:112] Iteration 40720, lr = 0.01
I0523 03:21:47.903053 34682 solver.cpp:239] Iteration 40730 (1.81288 iter/s, 5.51609s/10 iters), loss = 8.6911
I0523 03:21:47.903115 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6911 (* 1 = 8.6911 loss)
I0523 03:21:47.962640 34682 sgd_solver.cpp:112] Iteration 40730, lr = 0.01
I0523 03:21:52.492188 34682 solver.cpp:239] Iteration 40740 (2.17917 iter/s, 4.58889s/10 iters), loss = 8.64717
I0523 03:21:52.492234 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64717 (* 1 = 8.64717 loss)
I0523 03:21:52.554425 34682 sgd_solver.cpp:112] Iteration 40740, lr = 0.01
I0523 03:21:55.479269 34682 solver.cpp:239] Iteration 40750 (3.34795 iter/s, 2.9869s/10 iters), loss = 8.46536
I0523 03:21:55.479353 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46536 (* 1 = 8.46536 loss)
I0523 03:21:55.680411 34682 sgd_solver.cpp:112] Iteration 40750, lr = 0.01
I0523 03:22:00.003223 34682 solver.cpp:239] Iteration 40760 (2.21058 iter/s, 4.5237s/10 iters), loss = 9.44136
I0523 03:22:00.003269 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44136 (* 1 = 9.44136 loss)
I0523 03:22:00.079818 34682 sgd_solver.cpp:112] Iteration 40760, lr = 0.01
I0523 03:22:04.912609 34682 solver.cpp:239] Iteration 40770 (2.03702 iter/s, 4.90913s/10 iters), loss = 8.85344
I0523 03:22:04.912664 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85344 (* 1 = 8.85344 loss)
I0523 03:22:05.030361 34682 sgd_solver.cpp:112] Iteration 40770, lr = 0.01
I0523 03:22:08.876768 34682 solver.cpp:239] Iteration 40780 (2.52274 iter/s, 3.96394s/10 iters), loss = 9.35357
I0523 03:22:08.876823 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35357 (* 1 = 9.35357 loss)
I0523 03:22:09.693725 34682 sgd_solver.cpp:112] Iteration 40780, lr = 0.01
I0523 03:22:15.880661 34682 solver.cpp:239] Iteration 40790 (1.42785 iter/s, 7.00354s/10 iters), loss = 9.14105
I0523 03:22:15.880951 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14105 (* 1 = 9.14105 loss)
I0523 03:22:16.727275 34682 sgd_solver.cpp:112] Iteration 40790, lr = 0.01
I0523 03:22:19.151012 34682 solver.cpp:239] Iteration 40800 (3.05815 iter/s, 3.26995s/10 iters), loss = 9.09036
I0523 03:22:19.151053 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09036 (* 1 = 9.09036 loss)
I0523 03:22:19.219308 34682 sgd_solver.cpp:112] Iteration 40800, lr = 0.01
I0523 03:22:22.926677 34682 solver.cpp:239] Iteration 40810 (2.64868 iter/s, 3.77546s/10 iters), loss = 8.87408
I0523 03:22:22.926739 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87408 (* 1 = 8.87408 loss)
I0523 03:22:22.997670 34682 sgd_solver.cpp:112] Iteration 40810, lr = 0.01
I0523 03:22:27.767616 34682 solver.cpp:239] Iteration 40820 (2.06583 iter/s, 4.84067s/10 iters), loss = 8.5606
I0523 03:22:27.767669 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5606 (* 1 = 8.5606 loss)
I0523 03:22:27.825696 34682 sgd_solver.cpp:112] Iteration 40820, lr = 0.01
I0523 03:22:31.794607 34682 solver.cpp:239] Iteration 40830 (2.48338 iter/s, 4.02677s/10 iters), loss = 8.85806
I0523 03:22:31.794664 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85806 (* 1 = 8.85806 loss)
I0523 03:22:31.881165 34682 sgd_solver.cpp:112] Iteration 40830, lr = 0.01
I0523 03:22:34.559610 34682 solver.cpp:239] Iteration 40840 (3.62259 iter/s, 2.76046s/10 iters), loss = 8.17277
I0523 03:22:34.559661 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17277 (* 1 = 8.17277 loss)
I0523 03:22:34.617624 34682 sgd_solver.cpp:112] Iteration 40840, lr = 0.01
I0523 03:22:39.259680 34682 solver.cpp:239] Iteration 40850 (2.12774 iter/s, 4.69982s/10 iters), loss = 9.29172
I0523 03:22:39.259737 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29172 (* 1 = 9.29172 loss)
I0523 03:22:39.985098 34682 sgd_solver.cpp:112] Iteration 40850, lr = 0.01
I0523 03:22:44.297549 34682 solver.cpp:239] Iteration 40860 (1.98507 iter/s, 5.03761s/10 iters), loss = 9.67738
I0523 03:22:44.297608 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.67738 (* 1 = 9.67738 loss)
I0523 03:22:45.168260 34682 sgd_solver.cpp:112] Iteration 40860, lr = 0.01
I0523 03:22:50.923034 34682 solver.cpp:239] Iteration 40870 (1.5094 iter/s, 6.62516s/10 iters), loss = 9.49089
I0523 03:22:50.923297 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.49089 (* 1 = 9.49089 loss)
I0523 03:22:51.005550 34682 sgd_solver.cpp:112] Iteration 40870, lr = 0.01
I0523 03:22:56.362223 34682 solver.cpp:239] Iteration 40880 (1.83866 iter/s, 5.43875s/10 iters), loss = 9.17169
I0523 03:22:56.362272 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17169 (* 1 = 9.17169 loss)
I0523 03:22:56.428411 34682 sgd_solver.cpp:112] Iteration 40880, lr = 0.01
I0523 03:23:01.357998 34682 solver.cpp:239] Iteration 40890 (2.00182 iter/s, 4.99546s/10 iters), loss = 9.39943
I0523 03:23:01.358105 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.39943 (* 1 = 9.39943 loss)
I0523 03:23:02.187500 34682 sgd_solver.cpp:112] Iteration 40890, lr = 0.01
I0523 03:23:05.682989 34682 solver.cpp:239] Iteration 40900 (2.31229 iter/s, 4.32472s/10 iters), loss = 8.2173
I0523 03:23:05.683051 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2173 (* 1 = 8.2173 loss)
I0523 03:23:05.744743 34682 sgd_solver.cpp:112] Iteration 40900, lr = 0.01
I0523 03:23:09.418429 34682 solver.cpp:239] Iteration 40910 (2.67721 iter/s, 3.73523s/10 iters), loss = 9.05067
I0523 03:23:09.418479 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05067 (* 1 = 9.05067 loss)
I0523 03:23:09.981784 34682 sgd_solver.cpp:112] Iteration 40910, lr = 0.01
I0523 03:23:14.242122 34682 solver.cpp:239] Iteration 40920 (2.07322 iter/s, 4.82341s/10 iters), loss = 8.94914
I0523 03:23:14.242235 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94914 (* 1 = 8.94914 loss)
I0523 03:23:14.307790 34682 sgd_solver.cpp:112] Iteration 40920, lr = 0.01
I0523 03:23:18.389106 34682 solver.cpp:239] Iteration 40930 (2.41155 iter/s, 4.14672s/10 iters), loss = 9.13191
I0523 03:23:18.389168 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13191 (* 1 = 9.13191 loss)
I0523 03:23:18.946401 34682 sgd_solver.cpp:112] Iteration 40930, lr = 0.01
I0523 03:23:21.606163 34682 solver.cpp:239] Iteration 40940 (3.10862 iter/s, 3.21686s/10 iters), loss = 8.68795
I0523 03:23:21.606295 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68795 (* 1 = 8.68795 loss)
I0523 03:23:22.453533 34682 sgd_solver.cpp:112] Iteration 40940, lr = 0.01
I0523 03:23:27.797093 34682 solver.cpp:239] Iteration 40950 (1.61537 iter/s, 6.19054s/10 iters), loss = 8.75406
I0523 03:23:27.797149 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75406 (* 1 = 8.75406 loss)
I0523 03:23:28.438457 34682 sgd_solver.cpp:112] Iteration 40950, lr = 0.01
I0523 03:23:33.691407 34682 solver.cpp:239] Iteration 40960 (1.69664 iter/s, 5.894s/10 iters), loss = 9.68459
I0523 03:23:33.691478 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.68459 (* 1 = 9.68459 loss)
I0523 03:23:34.514001 34682 sgd_solver.cpp:112] Iteration 40960, lr = 0.01
I0523 03:23:39.336776 34682 solver.cpp:239] Iteration 40970 (1.77146 iter/s, 5.64507s/10 iters), loss = 9.15299
I0523 03:23:39.336822 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15299 (* 1 = 9.15299 loss)
I0523 03:23:40.131494 34682 sgd_solver.cpp:112] Iteration 40970, lr = 0.01
I0523 03:23:43.797534 34682 solver.cpp:239] Iteration 40980 (2.24189 iter/s, 4.46052s/10 iters), loss = 9.2528
I0523 03:23:43.797586 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2528 (* 1 = 9.2528 loss)
I0523 03:23:43.871136 34682 sgd_solver.cpp:112] Iteration 40980, lr = 0.01
I0523 03:23:48.024155 34682 solver.cpp:239] Iteration 40990 (2.36608 iter/s, 4.22639s/10 iters), loss = 9.66979
I0523 03:23:48.024219 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.66979 (* 1 = 9.66979 loss)
I0523 03:23:48.877799 34682 sgd_solver.cpp:112] Iteration 40990, lr = 0.01
I0523 03:23:53.537168 34682 solver.cpp:239] Iteration 41000 (1.81399 iter/s, 5.51272s/10 iters), loss = 9.00867
I0523 03:23:53.537457 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00867 (* 1 = 9.00867 loss)
I0523 03:23:53.604647 34682 sgd_solver.cpp:112] Iteration 41000, lr = 0.01
I0523 03:23:57.075690 34682 solver.cpp:239] Iteration 41010 (2.82986 iter/s, 3.53374s/10 iters), loss = 8.87782
I0523 03:23:57.075732 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87782 (* 1 = 8.87782 loss)
I0523 03:23:57.149574 34682 sgd_solver.cpp:112] Iteration 41010, lr = 0.01
I0523 03:23:59.922478 34682 solver.cpp:239] Iteration 41020 (3.51293 iter/s, 2.84662s/10 iters), loss = 10.2108
I0523 03:23:59.922533 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2108 (* 1 = 10.2108 loss)
I0523 03:24:00.064713 34682 sgd_solver.cpp:112] Iteration 41020, lr = 0.01
I0523 03:24:05.108824 34682 solver.cpp:239] Iteration 41030 (1.92825 iter/s, 5.18606s/10 iters), loss = 8.18989
I0523 03:24:05.108927 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18989 (* 1 = 8.18989 loss)
I0523 03:24:05.934459 34682 sgd_solver.cpp:112] Iteration 41030, lr = 0.01
I0523 03:24:08.635555 34682 solver.cpp:239] Iteration 41040 (2.83568 iter/s, 3.5265s/10 iters), loss = 8.28793
I0523 03:24:08.635622 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28793 (* 1 = 8.28793 loss)
I0523 03:24:09.413445 34682 sgd_solver.cpp:112] Iteration 41040, lr = 0.01
I0523 03:24:12.561384 34682 solver.cpp:239] Iteration 41050 (2.54738 iter/s, 3.92561s/10 iters), loss = 8.55122
I0523 03:24:12.561431 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55122 (* 1 = 8.55122 loss)
I0523 03:24:12.638664 34682 sgd_solver.cpp:112] Iteration 41050, lr = 0.01
I0523 03:24:16.115741 34682 solver.cpp:239] Iteration 41060 (2.81362 iter/s, 3.55414s/10 iters), loss = 9.23391
I0523 03:24:16.115795 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23391 (* 1 = 9.23391 loss)
I0523 03:24:16.154929 34682 sgd_solver.cpp:112] Iteration 41060, lr = 0.01
I0523 03:24:17.520823 34682 solver.cpp:239] Iteration 41070 (7.11766 iter/s, 1.40496s/10 iters), loss = 8.293
I0523 03:24:17.520903 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.293 (* 1 = 8.293 loss)
I0523 03:24:17.572702 34682 sgd_solver.cpp:112] Iteration 41070, lr = 0.01
I0523 03:24:18.729391 34682 solver.cpp:239] Iteration 41080 (8.27516 iter/s, 1.20844s/10 iters), loss = 8.62525
I0523 03:24:18.729439 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62525 (* 1 = 8.62525 loss)
I0523 03:24:18.779636 34682 sgd_solver.cpp:112] Iteration 41080, lr = 0.01
I0523 03:24:19.952750 34682 solver.cpp:239] Iteration 41090 (8.17497 iter/s, 1.22325s/10 iters), loss = 8.30575
I0523 03:24:19.952817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30575 (* 1 = 8.30575 loss)
I0523 03:24:19.992594 34682 sgd_solver.cpp:112] Iteration 41090, lr = 0.01
I0523 03:24:21.400259 34682 solver.cpp:239] Iteration 41100 (6.90907 iter/s, 1.44737s/10 iters), loss = 8.63598
I0523 03:24:21.400331 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63598 (* 1 = 8.63598 loss)
I0523 03:24:21.440745 34682 sgd_solver.cpp:112] Iteration 41100, lr = 0.01
I0523 03:24:23.154180 34682 solver.cpp:239] Iteration 41110 (5.70213 iter/s, 1.75373s/10 iters), loss = 7.84122
I0523 03:24:23.154232 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84122 (* 1 = 7.84122 loss)
I0523 03:24:23.187423 34682 sgd_solver.cpp:112] Iteration 41110, lr = 0.01
I0523 03:24:24.364377 34682 solver.cpp:239] Iteration 41120 (8.26396 iter/s, 1.21007s/10 iters), loss = 7.72049
I0523 03:24:24.364822 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72049 (* 1 = 7.72049 loss)
I0523 03:24:24.409184 34682 sgd_solver.cpp:112] Iteration 41120, lr = 0.01
I0523 03:24:25.567328 34682 solver.cpp:239] Iteration 41130 (8.31612 iter/s, 1.20248s/10 iters), loss = 8.35683
I0523 03:24:25.567399 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35683 (* 1 = 8.35683 loss)
I0523 03:24:25.604961 34682 sgd_solver.cpp:112] Iteration 41130, lr = 0.01
I0523 03:24:26.787425 34682 solver.cpp:239] Iteration 41140 (8.19694 iter/s, 1.21997s/10 iters), loss = 9.70336
I0523 03:24:26.787485 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.70336 (* 1 = 9.70336 loss)
I0523 03:24:26.827509 34682 sgd_solver.cpp:112] Iteration 41140, lr = 0.01
I0523 03:24:28.003202 34682 solver.cpp:239] Iteration 41150 (8.22601 iter/s, 1.21566s/10 iters), loss = 8.60629
I0523 03:24:28.003257 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60629 (* 1 = 8.60629 loss)
I0523 03:24:28.042750 34682 sgd_solver.cpp:112] Iteration 41150, lr = 0.01
I0523 03:24:29.177163 34682 solver.cpp:239] Iteration 41160 (8.51898 iter/s, 1.17385s/10 iters), loss = 9.72977
I0523 03:24:29.177206 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.72977 (* 1 = 9.72977 loss)
I0523 03:24:29.236086 34682 sgd_solver.cpp:112] Iteration 41160, lr = 0.01
I0523 03:24:30.361876 34682 solver.cpp:239] Iteration 41170 (8.44163 iter/s, 1.18461s/10 iters), loss = 8.85826
I0523 03:24:30.361933 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85826 (* 1 = 8.85826 loss)
I0523 03:24:30.410159 34682 sgd_solver.cpp:112] Iteration 41170, lr = 0.01
I0523 03:24:31.571018 34682 solver.cpp:239] Iteration 41180 (8.27112 iter/s, 1.20903s/10 iters), loss = 8.7995
I0523 03:24:31.571091 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7995 (* 1 = 8.7995 loss)
I0523 03:24:31.610327 34682 sgd_solver.cpp:112] Iteration 41180, lr = 0.01
I0523 03:24:32.788359 34682 solver.cpp:239] Iteration 41190 (8.21549 iter/s, 1.21721s/10 iters), loss = 8.89093
I0523 03:24:32.788424 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89093 (* 1 = 8.89093 loss)
I0523 03:24:32.841593 34682 sgd_solver.cpp:112] Iteration 41190, lr = 0.01
I0523 03:24:34.273018 34682 solver.cpp:239] Iteration 41200 (6.73616 iter/s, 1.48452s/10 iters), loss = 9.23851
I0523 03:24:34.273077 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23851 (* 1 = 9.23851 loss)
I0523 03:24:34.314981 34682 sgd_solver.cpp:112] Iteration 41200, lr = 0.01
I0523 03:24:35.502194 34682 solver.cpp:239] Iteration 41210 (8.13637 iter/s, 1.22905s/10 iters), loss = 8.75472
I0523 03:24:35.502271 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75472 (* 1 = 8.75472 loss)
I0523 03:24:35.538261 34682 sgd_solver.cpp:112] Iteration 41210, lr = 0.01
I0523 03:24:36.713593 34682 solver.cpp:239] Iteration 41220 (8.2558 iter/s, 1.21127s/10 iters), loss = 8.79968
I0523 03:24:36.713634 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79968 (* 1 = 8.79968 loss)
I0523 03:24:36.764102 34682 sgd_solver.cpp:112] Iteration 41220, lr = 0.01
I0523 03:24:37.922669 34682 solver.cpp:239] Iteration 41230 (8.27148 iter/s, 1.20897s/10 iters), loss = 9.59395
I0523 03:24:37.922758 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59395 (* 1 = 9.59395 loss)
I0523 03:24:37.964045 34682 sgd_solver.cpp:112] Iteration 41230, lr = 0.01
I0523 03:24:39.487506 34682 solver.cpp:239] Iteration 41240 (6.39122 iter/s, 1.56465s/10 iters), loss = 9.51558
I0523 03:24:39.487658 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51558 (* 1 = 9.51558 loss)
I0523 03:24:39.523387 34682 sgd_solver.cpp:112] Iteration 41240, lr = 0.01
I0523 03:24:41.676826 34682 solver.cpp:239] Iteration 41250 (4.56813 iter/s, 2.18908s/10 iters), loss = 8.97373
I0523 03:24:41.676885 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97373 (* 1 = 8.97373 loss)
I0523 03:24:41.712862 34682 sgd_solver.cpp:112] Iteration 41250, lr = 0.01
I0523 03:24:42.899976 34682 solver.cpp:239] Iteration 41260 (8.17639 iter/s, 1.22303s/10 iters), loss = 8.87557
I0523 03:24:42.900060 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87557 (* 1 = 8.87557 loss)
I0523 03:24:42.938839 34682 sgd_solver.cpp:112] Iteration 41260, lr = 0.01
I0523 03:24:44.199144 34682 solver.cpp:239] Iteration 41270 (7.69815 iter/s, 1.29901s/10 iters), loss = 8.57725
I0523 03:24:44.199200 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57725 (* 1 = 8.57725 loss)
I0523 03:24:44.242231 34682 sgd_solver.cpp:112] Iteration 41270, lr = 0.01
I0523 03:24:45.832739 34682 solver.cpp:239] Iteration 41280 (6.12198 iter/s, 1.63346s/10 iters), loss = 8.75465
I0523 03:24:45.832800 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75465 (* 1 = 8.75465 loss)
I0523 03:24:45.881314 34682 sgd_solver.cpp:112] Iteration 41280, lr = 0.01
I0523 03:24:47.374537 34682 solver.cpp:239] Iteration 41290 (6.48652 iter/s, 1.54166s/10 iters), loss = 9.23827
I0523 03:24:47.374596 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23827 (* 1 = 9.23827 loss)
I0523 03:24:47.413060 34682 sgd_solver.cpp:112] Iteration 41290, lr = 0.01
I0523 03:24:48.734130 34682 solver.cpp:239] Iteration 41300 (7.35583 iter/s, 1.35947s/10 iters), loss = 7.75702
I0523 03:24:48.734190 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75702 (* 1 = 7.75702 loss)
I0523 03:24:48.783145 34682 sgd_solver.cpp:112] Iteration 41300, lr = 0.01
I0523 03:24:50.397799 34682 solver.cpp:239] Iteration 41310 (6.01132 iter/s, 1.66353s/10 iters), loss = 8.70379
I0523 03:24:50.397859 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70379 (* 1 = 8.70379 loss)
I0523 03:24:50.437891 34682 sgd_solver.cpp:112] Iteration 41310, lr = 0.01
I0523 03:24:52.103047 34682 solver.cpp:239] Iteration 41320 (5.86474 iter/s, 1.7051s/10 iters), loss = 8.4531
I0523 03:24:52.103101 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4531 (* 1 = 8.4531 loss)
I0523 03:24:52.149744 34682 sgd_solver.cpp:112] Iteration 41320, lr = 0.01
I0523 03:24:53.313565 34682 solver.cpp:239] Iteration 41330 (8.27675 iter/s, 1.2082s/10 iters), loss = 9.15852
I0523 03:24:53.313613 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15852 (* 1 = 9.15852 loss)
I0523 03:24:53.365181 34682 sgd_solver.cpp:112] Iteration 41330, lr = 0.01
I0523 03:24:54.519351 34682 solver.cpp:239] Iteration 41340 (8.29405 iter/s, 1.20568s/10 iters), loss = 8.7934
I0523 03:24:54.519670 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7934 (* 1 = 8.7934 loss)
I0523 03:24:54.548967 34682 sgd_solver.cpp:112] Iteration 41340, lr = 0.01
I0523 03:24:56.286979 34682 solver.cpp:239] Iteration 41350 (5.65853 iter/s, 1.76724s/10 iters), loss = 9.13304
I0523 03:24:56.287029 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13304 (* 1 = 9.13304 loss)
I0523 03:24:56.747387 34682 sgd_solver.cpp:112] Iteration 41350, lr = 0.01
I0523 03:24:57.978847 34682 solver.cpp:239] Iteration 41360 (5.91109 iter/s, 1.69174s/10 iters), loss = 8.85718
I0523 03:24:57.978906 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85718 (* 1 = 8.85718 loss)
I0523 03:24:58.017668 34682 sgd_solver.cpp:112] Iteration 41360, lr = 0.01
I0523 03:24:59.315584 34682 solver.cpp:239] Iteration 41370 (7.48159 iter/s, 1.33661s/10 iters), loss = 9.01376
I0523 03:24:59.315654 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01376 (* 1 = 9.01376 loss)
I0523 03:24:59.843611 34682 sgd_solver.cpp:112] Iteration 41370, lr = 0.01
I0523 03:25:01.012625 34682 solver.cpp:239] Iteration 41380 (5.89313 iter/s, 1.69689s/10 iters), loss = 8.25985
I0523 03:25:01.012691 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25985 (* 1 = 8.25985 loss)
I0523 03:25:01.056501 34682 sgd_solver.cpp:112] Iteration 41380, lr = 0.01
I0523 03:25:02.392853 34682 solver.cpp:239] Iteration 41390 (7.24586 iter/s, 1.3801s/10 iters), loss = 8.6995
I0523 03:25:02.392907 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6995 (* 1 = 8.6995 loss)
I0523 03:25:02.436007 34682 sgd_solver.cpp:112] Iteration 41390, lr = 0.01
I0523 03:25:06.219310 34682 solver.cpp:239] Iteration 41400 (2.61353 iter/s, 3.82624s/10 iters), loss = 8.73061
I0523 03:25:06.219357 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73061 (* 1 = 8.73061 loss)
I0523 03:25:07.100803 34682 sgd_solver.cpp:112] Iteration 41400, lr = 0.01
I0523 03:25:11.899046 34682 solver.cpp:239] Iteration 41410 (1.76073 iter/s, 5.67946s/10 iters), loss = 8.61664
I0523 03:25:11.899096 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61664 (* 1 = 8.61664 loss)
I0523 03:25:11.962508 34682 sgd_solver.cpp:112] Iteration 41410, lr = 0.01
I0523 03:25:16.354332 34682 solver.cpp:239] Iteration 41420 (2.24464 iter/s, 4.45505s/10 iters), loss = 7.92551
I0523 03:25:16.354382 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92551 (* 1 = 7.92551 loss)
I0523 03:25:16.418117 34682 sgd_solver.cpp:112] Iteration 41420, lr = 0.01
I0523 03:25:22.921402 34682 solver.cpp:239] Iteration 41430 (1.52282 iter/s, 6.56675s/10 iters), loss = 7.9323
I0523 03:25:22.921447 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9323 (* 1 = 7.9323 loss)
I0523 03:25:22.997547 34682 sgd_solver.cpp:112] Iteration 41430, lr = 0.01
I0523 03:25:27.333333 34682 solver.cpp:239] Iteration 41440 (2.26671 iter/s, 4.41169s/10 iters), loss = 8.88939
I0523 03:25:27.333561 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88939 (* 1 = 8.88939 loss)
I0523 03:25:27.886972 34682 sgd_solver.cpp:112] Iteration 41440, lr = 0.01
I0523 03:25:30.416853 34682 solver.cpp:239] Iteration 41450 (3.2434 iter/s, 3.08318s/10 iters), loss = 8.25644
I0523 03:25:30.416913 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25644 (* 1 = 8.25644 loss)
I0523 03:25:31.259758 34682 sgd_solver.cpp:112] Iteration 41450, lr = 0.01
I0523 03:25:36.580986 34682 solver.cpp:239] Iteration 41460 (1.62237 iter/s, 6.16382s/10 iters), loss = 8.64982
I0523 03:25:36.581048 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64982 (* 1 = 8.64982 loss)
I0523 03:25:36.650890 34682 sgd_solver.cpp:112] Iteration 41460, lr = 0.01
I0523 03:25:41.182023 34682 solver.cpp:239] Iteration 41470 (2.17355 iter/s, 4.60078s/10 iters), loss = 9.71416
I0523 03:25:41.182096 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.71416 (* 1 = 9.71416 loss)
I0523 03:25:41.252616 34682 sgd_solver.cpp:112] Iteration 41470, lr = 0.01
I0523 03:25:44.936308 34682 solver.cpp:239] Iteration 41480 (2.66379 iter/s, 3.75405s/10 iters), loss = 9.13177
I0523 03:25:44.936384 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13177 (* 1 = 9.13177 loss)
I0523 03:25:44.993695 34682 sgd_solver.cpp:112] Iteration 41480, lr = 0.01
I0523 03:25:49.110672 34682 solver.cpp:239] Iteration 41490 (2.39571 iter/s, 4.17413s/10 iters), loss = 8.90588
I0523 03:25:49.110747 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90588 (* 1 = 8.90588 loss)
I0523 03:25:49.249336 34682 sgd_solver.cpp:112] Iteration 41490, lr = 0.01
I0523 03:25:53.234215 34682 solver.cpp:239] Iteration 41500 (2.42524 iter/s, 4.1233s/10 iters), loss = 9.70479
I0523 03:25:53.234261 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.70479 (* 1 = 9.70479 loss)
I0523 03:25:54.075240 34682 sgd_solver.cpp:112] Iteration 41500, lr = 0.01
I0523 03:25:57.601398 34682 solver.cpp:239] Iteration 41510 (2.28993 iter/s, 4.36695s/10 iters), loss = 9.11234
I0523 03:25:57.601531 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11234 (* 1 = 9.11234 loss)
I0523 03:25:58.225811 34682 sgd_solver.cpp:112] Iteration 41510, lr = 0.01
I0523 03:26:01.671229 34682 solver.cpp:239] Iteration 41520 (2.45729 iter/s, 4.06953s/10 iters), loss = 9.05471
I0523 03:26:01.671281 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05471 (* 1 = 9.05471 loss)
I0523 03:26:02.509143 34682 sgd_solver.cpp:112] Iteration 41520, lr = 0.01
I0523 03:26:06.046464 34682 solver.cpp:239] Iteration 41530 (2.28571 iter/s, 4.37501s/10 iters), loss = 8.74009
I0523 03:26:06.046509 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74009 (* 1 = 8.74009 loss)
I0523 03:26:06.143800 34682 sgd_solver.cpp:112] Iteration 41530, lr = 0.01
I0523 03:26:11.202821 34682 solver.cpp:239] Iteration 41540 (1.93945 iter/s, 5.1561s/10 iters), loss = 9.57214
I0523 03:26:11.202864 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.57214 (* 1 = 9.57214 loss)
I0523 03:26:11.280519 34682 sgd_solver.cpp:112] Iteration 41540, lr = 0.01
I0523 03:26:17.386986 34682 solver.cpp:239] Iteration 41550 (1.61711 iter/s, 6.18386s/10 iters), loss = 8.97014
I0523 03:26:17.387037 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97014 (* 1 = 8.97014 loss)
I0523 03:26:17.460791 34682 sgd_solver.cpp:112] Iteration 41550, lr = 0.01
I0523 03:26:22.576723 34682 solver.cpp:239] Iteration 41560 (1.92698 iter/s, 5.18947s/10 iters), loss = 8.78981
I0523 03:26:22.576774 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78981 (* 1 = 8.78981 loss)
I0523 03:26:22.651803 34682 sgd_solver.cpp:112] Iteration 41560, lr = 0.01
I0523 03:26:27.436807 34682 solver.cpp:239] Iteration 41570 (2.05768 iter/s, 4.85983s/10 iters), loss = 8.86794
I0523 03:26:27.436870 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86794 (* 1 = 8.86794 loss)
I0523 03:26:28.255566 34682 sgd_solver.cpp:112] Iteration 41570, lr = 0.01
I0523 03:26:32.994062 34682 solver.cpp:239] Iteration 41580 (1.79954 iter/s, 5.55697s/10 iters), loss = 9.34223
I0523 03:26:32.994115 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34223 (* 1 = 9.34223 loss)
I0523 03:26:33.757593 34682 sgd_solver.cpp:112] Iteration 41580, lr = 0.01
I0523 03:26:37.760505 34682 solver.cpp:239] Iteration 41590 (2.09811 iter/s, 4.76619s/10 iters), loss = 9.42676
I0523 03:26:37.760551 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42676 (* 1 = 9.42676 loss)
I0523 03:26:37.835512 34682 sgd_solver.cpp:112] Iteration 41590, lr = 0.01
I0523 03:26:43.947708 34682 solver.cpp:239] Iteration 41600 (1.61632 iter/s, 6.18688s/10 iters), loss = 8.55903
I0523 03:26:43.947773 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55903 (* 1 = 8.55903 loss)
I0523 03:26:44.019626 34682 sgd_solver.cpp:112] Iteration 41600, lr = 0.01
I0523 03:26:47.681011 34682 solver.cpp:239] Iteration 41610 (2.67875 iter/s, 3.73308s/10 iters), loss = 9.26357
I0523 03:26:47.681062 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.26357 (* 1 = 9.26357 loss)
I0523 03:26:48.521440 34682 sgd_solver.cpp:112] Iteration 41610, lr = 0.01
I0523 03:26:52.614135 34682 solver.cpp:239] Iteration 41620 (2.02722 iter/s, 4.93286s/10 iters), loss = 8.42981
I0523 03:26:52.614195 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42981 (* 1 = 8.42981 loss)
I0523 03:26:52.672754 34682 sgd_solver.cpp:112] Iteration 41620, lr = 0.01
I0523 03:26:56.742269 34682 solver.cpp:239] Iteration 41630 (2.42254 iter/s, 4.1279s/10 iters), loss = 9.30998
I0523 03:26:56.742313 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30998 (* 1 = 9.30998 loss)
I0523 03:26:56.811560 34682 sgd_solver.cpp:112] Iteration 41630, lr = 0.01
I0523 03:27:01.699301 34682 solver.cpp:239] Iteration 41640 (2.01744 iter/s, 4.95678s/10 iters), loss = 9.71127
I0523 03:27:01.699522 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.71127 (* 1 = 9.71127 loss)
I0523 03:27:02.128782 34682 sgd_solver.cpp:112] Iteration 41640, lr = 0.01
I0523 03:27:07.897402 34682 solver.cpp:239] Iteration 41650 (1.61351 iter/s, 6.19765s/10 iters), loss = 8.49203
I0523 03:27:07.897454 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49203 (* 1 = 8.49203 loss)
I0523 03:27:08.613222 34682 sgd_solver.cpp:112] Iteration 41650, lr = 0.01
I0523 03:27:12.857055 34682 solver.cpp:239] Iteration 41660 (2.01638 iter/s, 4.95939s/10 iters), loss = 8.66434
I0523 03:27:12.857118 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66434 (* 1 = 8.66434 loss)
I0523 03:27:12.916261 34682 sgd_solver.cpp:112] Iteration 41660, lr = 0.01
I0523 03:27:18.123401 34682 solver.cpp:239] Iteration 41670 (1.89895 iter/s, 5.26607s/10 iters), loss = 8.29626
I0523 03:27:18.123461 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29626 (* 1 = 8.29626 loss)
I0523 03:27:18.768095 34682 sgd_solver.cpp:112] Iteration 41670, lr = 0.01
I0523 03:27:22.897265 34682 solver.cpp:239] Iteration 41680 (2.09485 iter/s, 4.77361s/10 iters), loss = 9.24702
I0523 03:27:22.897323 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24702 (* 1 = 9.24702 loss)
I0523 03:27:22.953598 34682 sgd_solver.cpp:112] Iteration 41680, lr = 0.01
I0523 03:27:27.854691 34682 solver.cpp:239] Iteration 41690 (2.01728 iter/s, 4.95716s/10 iters), loss = 9.31595
I0523 03:27:27.854770 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31595 (* 1 = 9.31595 loss)
I0523 03:27:27.931807 34682 sgd_solver.cpp:112] Iteration 41690, lr = 0.01
I0523 03:27:30.790012 34682 solver.cpp:239] Iteration 41700 (3.40702 iter/s, 2.93511s/10 iters), loss = 9.15181
I0523 03:27:30.790064 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15181 (* 1 = 9.15181 loss)
I0523 03:27:30.860870 34682 sgd_solver.cpp:112] Iteration 41700, lr = 0.01
I0523 03:27:36.100590 34682 solver.cpp:239] Iteration 41710 (1.88313 iter/s, 5.31031s/10 iters), loss = 9.30926
I0523 03:27:36.100709 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30926 (* 1 = 9.30926 loss)
I0523 03:27:36.885491 34682 sgd_solver.cpp:112] Iteration 41710, lr = 0.01
I0523 03:27:41.669129 34682 solver.cpp:239] Iteration 41720 (1.79591 iter/s, 5.56819s/10 iters), loss = 8.76794
I0523 03:27:41.669181 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76794 (* 1 = 8.76794 loss)
I0523 03:27:41.730262 34682 sgd_solver.cpp:112] Iteration 41720, lr = 0.01
I0523 03:27:47.014387 34682 solver.cpp:239] Iteration 41730 (1.87091 iter/s, 5.34498s/10 iters), loss = 8.3639
I0523 03:27:47.014430 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3639 (* 1 = 8.3639 loss)
I0523 03:27:47.090982 34682 sgd_solver.cpp:112] Iteration 41730, lr = 0.01
I0523 03:27:50.261982 34682 solver.cpp:239] Iteration 41740 (3.07937 iter/s, 3.24742s/10 iters), loss = 8.36463
I0523 03:27:50.262029 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36463 (* 1 = 8.36463 loss)
I0523 03:27:50.324188 34682 sgd_solver.cpp:112] Iteration 41740, lr = 0.01
I0523 03:27:54.600019 34682 solver.cpp:239] Iteration 41750 (2.30531 iter/s, 4.3378s/10 iters), loss = 9.71901
I0523 03:27:54.600083 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.71901 (* 1 = 9.71901 loss)
I0523 03:27:54.670166 34682 sgd_solver.cpp:112] Iteration 41750, lr = 0.01
I0523 03:28:00.232491 34682 solver.cpp:239] Iteration 41760 (1.77551 iter/s, 5.63217s/10 iters), loss = 8.57957
I0523 03:28:00.232538 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57957 (* 1 = 8.57957 loss)
I0523 03:28:00.317991 34682 sgd_solver.cpp:112] Iteration 41760, lr = 0.01
I0523 03:28:05.915364 34682 solver.cpp:239] Iteration 41770 (1.75976 iter/s, 5.68259s/10 iters), loss = 8.33818
I0523 03:28:05.915424 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33818 (* 1 = 8.33818 loss)
I0523 03:28:06.749902 34682 sgd_solver.cpp:112] Iteration 41770, lr = 0.01
I0523 03:28:12.193431 34682 solver.cpp:239] Iteration 41780 (1.59293 iter/s, 6.27775s/10 iters), loss = 8.98617
I0523 03:28:12.193488 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98617 (* 1 = 8.98617 loss)
I0523 03:28:12.952718 34682 sgd_solver.cpp:112] Iteration 41780, lr = 0.01
I0523 03:28:18.417790 34682 solver.cpp:239] Iteration 41790 (1.60667 iter/s, 6.22405s/10 iters), loss = 8.86167
I0523 03:28:18.417845 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86167 (* 1 = 8.86167 loss)
I0523 03:28:19.162019 34682 sgd_solver.cpp:112] Iteration 41790, lr = 0.01
I0523 03:28:23.638736 34682 solver.cpp:239] Iteration 41800 (1.91546 iter/s, 5.22067s/10 iters), loss = 8.78881
I0523 03:28:23.638795 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78881 (* 1 = 8.78881 loss)
I0523 03:28:24.189010 34682 sgd_solver.cpp:112] Iteration 41800, lr = 0.01
I0523 03:28:28.176714 34682 solver.cpp:239] Iteration 41810 (2.20375 iter/s, 4.53772s/10 iters), loss = 8.71584
I0523 03:28:28.176784 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71584 (* 1 = 8.71584 loss)
I0523 03:28:28.244488 34682 sgd_solver.cpp:112] Iteration 41810, lr = 0.01
I0523 03:28:32.653106 34682 solver.cpp:239] Iteration 41820 (2.23407 iter/s, 4.47614s/10 iters), loss = 9.11288
I0523 03:28:32.653165 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11288 (* 1 = 9.11288 loss)
I0523 03:28:33.351723 34682 sgd_solver.cpp:112] Iteration 41820, lr = 0.01
I0523 03:28:36.739765 34682 solver.cpp:239] Iteration 41830 (2.44712 iter/s, 4.08644s/10 iters), loss = 8.72176
I0523 03:28:36.739814 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72176 (* 1 = 8.72176 loss)
I0523 03:28:36.815213 34682 sgd_solver.cpp:112] Iteration 41830, lr = 0.01
I0523 03:28:41.853333 34682 solver.cpp:239] Iteration 41840 (1.95568 iter/s, 5.11331s/10 iters), loss = 8.42169
I0523 03:28:41.853381 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42169 (* 1 = 8.42169 loss)
I0523 03:28:42.571405 34682 sgd_solver.cpp:112] Iteration 41840, lr = 0.01
I0523 03:28:47.282840 34682 solver.cpp:239] Iteration 41850 (1.84189 iter/s, 5.42922s/10 iters), loss = 8.08674
I0523 03:28:47.282904 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08674 (* 1 = 8.08674 loss)
I0523 03:28:47.995906 34682 sgd_solver.cpp:112] Iteration 41850, lr = 0.01
I0523 03:28:51.845373 34682 solver.cpp:239] Iteration 41860 (2.1919 iter/s, 4.56226s/10 iters), loss = 8.90662
I0523 03:28:51.845422 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90662 (* 1 = 8.90662 loss)
I0523 03:28:51.907753 34682 sgd_solver.cpp:112] Iteration 41860, lr = 0.01
I0523 03:28:56.030238 34682 solver.cpp:239] Iteration 41870 (2.38969 iter/s, 4.18465s/10 iters), loss = 9.841
I0523 03:28:56.030283 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.841 (* 1 = 9.841 loss)
I0523 03:28:56.101986 34682 sgd_solver.cpp:112] Iteration 41870, lr = 0.01
I0523 03:29:00.502558 34682 solver.cpp:239] Iteration 41880 (2.23609 iter/s, 4.47209s/10 iters), loss = 8.43337
I0523 03:29:00.502629 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43337 (* 1 = 8.43337 loss)
I0523 03:29:01.230716 34682 sgd_solver.cpp:112] Iteration 41880, lr = 0.01
I0523 03:29:06.761112 34682 solver.cpp:239] Iteration 41890 (1.5979 iter/s, 6.25822s/10 iters), loss = 8.89685
I0523 03:29:06.761165 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89685 (* 1 = 8.89685 loss)
I0523 03:29:07.088805 34682 sgd_solver.cpp:112] Iteration 41890, lr = 0.01
I0523 03:29:11.476354 34682 solver.cpp:239] Iteration 41900 (2.12089 iter/s, 4.71499s/10 iters), loss = 8.7952
I0523 03:29:11.476416 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7952 (* 1 = 8.7952 loss)
I0523 03:29:11.538847 34682 sgd_solver.cpp:112] Iteration 41900, lr = 0.01
I0523 03:29:14.950599 34682 solver.cpp:239] Iteration 41910 (2.87849 iter/s, 3.47405s/10 iters), loss = 8.71866
I0523 03:29:14.950640 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71866 (* 1 = 8.71866 loss)
I0523 03:29:15.019580 34682 sgd_solver.cpp:112] Iteration 41910, lr = 0.01
I0523 03:29:21.571086 34682 solver.cpp:239] Iteration 41920 (1.51053 iter/s, 6.62018s/10 iters), loss = 9.11734
I0523 03:29:21.571141 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11734 (* 1 = 9.11734 loss)
I0523 03:29:21.640650 34682 sgd_solver.cpp:112] Iteration 41920, lr = 0.01
I0523 03:29:27.276392 34682 solver.cpp:239] Iteration 41930 (1.75284 iter/s, 5.70501s/10 iters), loss = 8.73718
I0523 03:29:27.276453 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73718 (* 1 = 8.73718 loss)
I0523 03:29:28.123952 34682 sgd_solver.cpp:112] Iteration 41930, lr = 0.01
I0523 03:29:33.654114 34682 solver.cpp:239] Iteration 41940 (1.56804 iter/s, 6.37741s/10 iters), loss = 8.4916
I0523 03:29:33.654170 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4916 (* 1 = 8.4916 loss)
I0523 03:29:34.473376 34682 sgd_solver.cpp:112] Iteration 41940, lr = 0.01
I0523 03:29:40.847473 34682 solver.cpp:239] Iteration 41950 (1.39024 iter/s, 7.19301s/10 iters), loss = 8.31147
I0523 03:29:40.847611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31147 (* 1 = 8.31147 loss)
I0523 03:29:41.661705 34682 sgd_solver.cpp:112] Iteration 41950, lr = 0.01
I0523 03:29:46.509081 34682 solver.cpp:239] Iteration 41960 (1.7664 iter/s, 5.66125s/10 iters), loss = 8.77241
I0523 03:29:46.509131 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77241 (* 1 = 8.77241 loss)
I0523 03:29:47.069502 34682 sgd_solver.cpp:112] Iteration 41960, lr = 0.01
I0523 03:29:51.418229 34682 solver.cpp:239] Iteration 41970 (2.03713 iter/s, 4.90887s/10 iters), loss = 8.17614
I0523 03:29:51.418293 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17614 (* 1 = 8.17614 loss)
I0523 03:29:52.173832 34682 sgd_solver.cpp:112] Iteration 41970, lr = 0.01
I0523 03:29:57.148365 34682 solver.cpp:239] Iteration 41980 (1.74525 iter/s, 5.72984s/10 iters), loss = 8.73164
I0523 03:29:57.148416 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73164 (* 1 = 8.73164 loss)
I0523 03:29:57.211004 34682 sgd_solver.cpp:112] Iteration 41980, lr = 0.01
I0523 03:30:01.083132 34682 solver.cpp:239] Iteration 41990 (2.54159 iter/s, 3.93455s/10 iters), loss = 8.76907
I0523 03:30:01.083180 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76907 (* 1 = 8.76907 loss)
I0523 03:30:01.783695 34682 sgd_solver.cpp:112] Iteration 41990, lr = 0.01
I0523 03:30:06.005525 34682 solver.cpp:239] Iteration 42000 (2.03164 iter/s, 4.92213s/10 iters), loss = 9.03782
I0523 03:30:06.005576 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03782 (* 1 = 9.03782 loss)
I0523 03:30:06.066750 34682 sgd_solver.cpp:112] Iteration 42000, lr = 0.01
I0523 03:30:10.112920 34682 solver.cpp:239] Iteration 42010 (2.43476 iter/s, 4.10718s/10 iters), loss = 8.04057
I0523 03:30:10.112964 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04057 (* 1 = 8.04057 loss)
I0523 03:30:10.942723 34682 sgd_solver.cpp:112] Iteration 42010, lr = 0.01
I0523 03:30:15.973763 34682 solver.cpp:239] Iteration 42020 (1.70632 iter/s, 5.86055s/10 iters), loss = 8.64672
I0523 03:30:15.973819 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64672 (* 1 = 8.64672 loss)
I0523 03:30:16.054216 34682 sgd_solver.cpp:112] Iteration 42020, lr = 0.01
I0523 03:30:19.624500 34682 solver.cpp:239] Iteration 42030 (2.73933 iter/s, 3.65053s/10 iters), loss = 8.00654
I0523 03:30:19.624547 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00654 (* 1 = 8.00654 loss)
I0523 03:30:19.697185 34682 sgd_solver.cpp:112] Iteration 42030, lr = 0.01
I0523 03:30:25.599898 34682 solver.cpp:239] Iteration 42040 (1.67361 iter/s, 5.97511s/10 iters), loss = 9.40278
I0523 03:30:25.599951 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40278 (* 1 = 9.40278 loss)
I0523 03:30:26.415009 34682 sgd_solver.cpp:112] Iteration 42040, lr = 0.01
I0523 03:30:30.633368 34682 solver.cpp:239] Iteration 42050 (1.9868 iter/s, 5.03321s/10 iters), loss = 9.53085
I0523 03:30:30.633419 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.53085 (* 1 = 9.53085 loss)
I0523 03:30:31.404603 34682 sgd_solver.cpp:112] Iteration 42050, lr = 0.01
I0523 03:30:36.588086 34682 solver.cpp:239] Iteration 42060 (1.67942 iter/s, 5.95442s/10 iters), loss = 8.134
I0523 03:30:36.588143 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.134 (* 1 = 8.134 loss)
I0523 03:30:36.674958 34682 sgd_solver.cpp:112] Iteration 42060, lr = 0.01
I0523 03:30:40.914700 34682 solver.cpp:239] Iteration 42070 (2.31141 iter/s, 4.32636s/10 iters), loss = 8.8798
I0523 03:30:40.914760 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8798 (* 1 = 8.8798 loss)
I0523 03:30:41.575690 34682 sgd_solver.cpp:112] Iteration 42070, lr = 0.01
I0523 03:30:47.599171 34682 solver.cpp:239] Iteration 42080 (1.49608 iter/s, 6.68412s/10 iters), loss = 9.38242
I0523 03:30:47.599252 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.38242 (* 1 = 9.38242 loss)
I0523 03:30:48.440436 34682 sgd_solver.cpp:112] Iteration 42080, lr = 0.01
I0523 03:30:51.160913 34682 solver.cpp:239] Iteration 42090 (2.80779 iter/s, 3.56152s/10 iters), loss = 8.83441
I0523 03:30:51.160969 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83441 (* 1 = 8.83441 loss)
I0523 03:30:52.029247 34682 sgd_solver.cpp:112] Iteration 42090, lr = 0.01
I0523 03:30:57.434288 34682 solver.cpp:239] Iteration 42100 (1.59412 iter/s, 6.27306s/10 iters), loss = 8.22433
I0523 03:30:57.434340 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22433 (* 1 = 8.22433 loss)
I0523 03:30:57.509526 34682 sgd_solver.cpp:112] Iteration 42100, lr = 0.01
I0523 03:31:02.009975 34682 solver.cpp:239] Iteration 42110 (2.18558 iter/s, 4.57544s/10 iters), loss = 9.27066
I0523 03:31:02.010053 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27066 (* 1 = 9.27066 loss)
I0523 03:31:02.080073 34682 sgd_solver.cpp:112] Iteration 42110, lr = 0.01
I0523 03:31:04.745000 34682 solver.cpp:239] Iteration 42120 (3.65655 iter/s, 2.73482s/10 iters), loss = 8.35468
I0523 03:31:04.745048 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35468 (* 1 = 8.35468 loss)
I0523 03:31:05.554662 34682 sgd_solver.cpp:112] Iteration 42120, lr = 0.01
I0523 03:31:11.322325 34682 solver.cpp:239] Iteration 42130 (1.52045 iter/s, 6.57701s/10 iters), loss = 9.71713
I0523 03:31:11.322366 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.71713 (* 1 = 9.71713 loss)
I0523 03:31:11.392894 34682 sgd_solver.cpp:112] Iteration 42130, lr = 0.01
I0523 03:31:17.713925 34682 solver.cpp:239] Iteration 42140 (1.56463 iter/s, 6.3913s/10 iters), loss = 8.15511
I0523 03:31:17.714208 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15511 (* 1 = 8.15511 loss)
I0523 03:31:17.793201 34682 sgd_solver.cpp:112] Iteration 42140, lr = 0.01
I0523 03:31:22.421398 34682 solver.cpp:239] Iteration 42150 (2.12644 iter/s, 4.7027s/10 iters), loss = 8.32107
I0523 03:31:22.421449 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32107 (* 1 = 8.32107 loss)
I0523 03:31:22.520403 34682 sgd_solver.cpp:112] Iteration 42150, lr = 0.01
I0523 03:31:26.527320 34682 solver.cpp:239] Iteration 42160 (2.43564 iter/s, 4.1057s/10 iters), loss = 8.97402
I0523 03:31:26.527374 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97402 (* 1 = 8.97402 loss)
I0523 03:31:27.371253 34682 sgd_solver.cpp:112] Iteration 42160, lr = 0.01
I0523 03:31:32.495138 34682 solver.cpp:239] Iteration 42170 (1.67574 iter/s, 5.96752s/10 iters), loss = 8.48289
I0523 03:31:32.495189 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48289 (* 1 = 8.48289 loss)
I0523 03:31:32.824609 34682 sgd_solver.cpp:112] Iteration 42170, lr = 0.01
I0523 03:31:36.790181 34682 solver.cpp:239] Iteration 42180 (2.3284 iter/s, 4.29479s/10 iters), loss = 8.44519
I0523 03:31:36.790237 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44519 (* 1 = 8.44519 loss)
I0523 03:31:37.609017 34682 sgd_solver.cpp:112] Iteration 42180, lr = 0.01
I0523 03:31:42.531015 34682 solver.cpp:239] Iteration 42190 (1.74199 iter/s, 5.74055s/10 iters), loss = 8.91765
I0523 03:31:42.531059 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91765 (* 1 = 8.91765 loss)
I0523 03:31:43.329252 34682 sgd_solver.cpp:112] Iteration 42190, lr = 0.01
I0523 03:31:48.076726 34682 solver.cpp:239] Iteration 42200 (1.80329 iter/s, 5.54543s/10 iters), loss = 9.0065
I0523 03:31:48.076975 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0065 (* 1 = 9.0065 loss)
I0523 03:31:48.812114 34682 sgd_solver.cpp:112] Iteration 42200, lr = 0.01
I0523 03:31:54.335737 34682 solver.cpp:239] Iteration 42210 (1.59782 iter/s, 6.25854s/10 iters), loss = 8.45918
I0523 03:31:54.335795 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45918 (* 1 = 8.45918 loss)
I0523 03:31:54.414685 34682 sgd_solver.cpp:112] Iteration 42210, lr = 0.01
I0523 03:31:58.646981 34682 solver.cpp:239] Iteration 42220 (2.31964 iter/s, 4.31101s/10 iters), loss = 8.52177
I0523 03:31:58.647024 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52177 (* 1 = 8.52177 loss)
I0523 03:31:58.716100 34682 sgd_solver.cpp:112] Iteration 42220, lr = 0.01
I0523 03:32:02.750674 34682 solver.cpp:239] Iteration 42230 (2.43696 iter/s, 4.10347s/10 iters), loss = 9.00057
I0523 03:32:02.750735 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00057 (* 1 = 9.00057 loss)
I0523 03:32:02.815596 34682 sgd_solver.cpp:112] Iteration 42230, lr = 0.01
I0523 03:32:07.469718 34682 solver.cpp:239] Iteration 42240 (2.1192 iter/s, 4.71877s/10 iters), loss = 9.44736
I0523 03:32:07.469799 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44736 (* 1 = 9.44736 loss)
I0523 03:32:08.287232 34682 sgd_solver.cpp:112] Iteration 42240, lr = 0.01
I0523 03:32:14.486448 34682 solver.cpp:239] Iteration 42250 (1.42524 iter/s, 7.01637s/10 iters), loss = 9.24762
I0523 03:32:14.486502 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.24762 (* 1 = 9.24762 loss)
I0523 03:32:15.268036 34682 sgd_solver.cpp:112] Iteration 42250, lr = 0.01
I0523 03:32:18.436493 34682 solver.cpp:239] Iteration 42260 (2.53178 iter/s, 3.9498s/10 iters), loss = 9.05727
I0523 03:32:18.436766 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05727 (* 1 = 9.05727 loss)
I0523 03:32:18.502810 34682 sgd_solver.cpp:112] Iteration 42260, lr = 0.01
I0523 03:32:24.187654 34682 solver.cpp:239] Iteration 42270 (1.73894 iter/s, 5.75063s/10 iters), loss = 9.47795
I0523 03:32:24.187726 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47795 (* 1 = 9.47795 loss)
I0523 03:32:24.957955 34682 sgd_solver.cpp:112] Iteration 42270, lr = 0.01
I0523 03:32:29.812783 34682 solver.cpp:239] Iteration 42280 (1.77784 iter/s, 5.62481s/10 iters), loss = 8.89252
I0523 03:32:29.812849 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89252 (* 1 = 8.89252 loss)
I0523 03:32:30.084470 34682 sgd_solver.cpp:112] Iteration 42280, lr = 0.01
I0523 03:32:35.560020 34682 solver.cpp:239] Iteration 42290 (1.74006 iter/s, 5.74693s/10 iters), loss = 8.85431
I0523 03:32:35.560075 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85431 (* 1 = 8.85431 loss)
I0523 03:32:35.625272 34682 sgd_solver.cpp:112] Iteration 42290, lr = 0.01
I0523 03:32:39.945613 34682 solver.cpp:239] Iteration 42300 (2.28031 iter/s, 4.38537s/10 iters), loss = 9.67131
I0523 03:32:39.945657 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.67131 (* 1 = 9.67131 loss)
I0523 03:32:40.008293 34682 sgd_solver.cpp:112] Iteration 42300, lr = 0.01
I0523 03:32:44.501345 34682 solver.cpp:239] Iteration 42310 (2.19516 iter/s, 4.55548s/10 iters), loss = 9.27188
I0523 03:32:44.501406 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27188 (* 1 = 9.27188 loss)
I0523 03:32:45.338614 34682 sgd_solver.cpp:112] Iteration 42310, lr = 0.01
I0523 03:32:50.142952 34682 solver.cpp:239] Iteration 42320 (1.77264 iter/s, 5.64131s/10 iters), loss = 7.69772
I0523 03:32:50.143124 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69772 (* 1 = 7.69772 loss)
I0523 03:32:50.209784 34682 sgd_solver.cpp:112] Iteration 42320, lr = 0.01
I0523 03:32:56.368928 34682 solver.cpp:239] Iteration 42330 (1.60628 iter/s, 6.22556s/10 iters), loss = 8.307
I0523 03:32:56.368986 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.307 (* 1 = 8.307 loss)
I0523 03:32:57.127436 34682 sgd_solver.cpp:112] Iteration 42330, lr = 0.01
I0523 03:33:02.275120 34682 solver.cpp:239] Iteration 42340 (1.69322 iter/s, 5.90589s/10 iters), loss = 8.27487
I0523 03:33:02.275167 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27487 (* 1 = 8.27487 loss)
I0523 03:33:02.338589 34682 sgd_solver.cpp:112] Iteration 42340, lr = 0.01
I0523 03:33:06.508554 34682 solver.cpp:239] Iteration 42350 (2.36227 iter/s, 4.23321s/10 iters), loss = 8.47825
I0523 03:33:06.508599 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47825 (* 1 = 8.47825 loss)
I0523 03:33:06.565829 34682 sgd_solver.cpp:112] Iteration 42350, lr = 0.01
I0523 03:33:10.705132 34682 solver.cpp:239] Iteration 42360 (2.38302 iter/s, 4.19635s/10 iters), loss = 9.28856
I0523 03:33:10.705180 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28856 (* 1 = 9.28856 loss)
I0523 03:33:10.761240 34682 sgd_solver.cpp:112] Iteration 42360, lr = 0.01
I0523 03:33:13.873819 34682 solver.cpp:239] Iteration 42370 (3.15606 iter/s, 3.1685s/10 iters), loss = 8.65682
I0523 03:33:13.873872 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65682 (* 1 = 8.65682 loss)
I0523 03:33:14.588625 34682 sgd_solver.cpp:112] Iteration 42370, lr = 0.01
I0523 03:33:19.424579 34682 solver.cpp:239] Iteration 42380 (1.80164 iter/s, 5.55048s/10 iters), loss = 9.35101
I0523 03:33:19.424628 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35101 (* 1 = 9.35101 loss)
I0523 03:33:20.254616 34682 sgd_solver.cpp:112] Iteration 42380, lr = 0.01
I0523 03:33:22.795275 34682 solver.cpp:239] Iteration 42390 (2.96692 iter/s, 3.3705s/10 iters), loss = 8.1446
I0523 03:33:22.795332 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1446 (* 1 = 8.1446 loss)
I0523 03:33:22.863389 34682 sgd_solver.cpp:112] Iteration 42390, lr = 0.01
I0523 03:33:27.284170 34682 solver.cpp:239] Iteration 42400 (2.22784 iter/s, 4.48866s/10 iters), loss = 8.51518
I0523 03:33:27.284229 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51518 (* 1 = 8.51518 loss)
I0523 03:33:27.374873 34682 sgd_solver.cpp:112] Iteration 42400, lr = 0.01
I0523 03:33:31.575814 34682 solver.cpp:239] Iteration 42410 (2.33024 iter/s, 4.29141s/10 iters), loss = 8.55788
I0523 03:33:31.575858 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55788 (* 1 = 8.55788 loss)
I0523 03:33:32.461305 34682 sgd_solver.cpp:112] Iteration 42410, lr = 0.01
I0523 03:33:36.498592 34682 solver.cpp:239] Iteration 42420 (2.03148 iter/s, 4.92252s/10 iters), loss = 8.67038
I0523 03:33:36.498641 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67038 (* 1 = 8.67038 loss)
I0523 03:33:36.564438 34682 sgd_solver.cpp:112] Iteration 42420, lr = 0.01
I0523 03:33:41.669945 34682 solver.cpp:239] Iteration 42430 (1.93383 iter/s, 5.17109s/10 iters), loss = 7.40796
I0523 03:33:41.670006 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.40796 (* 1 = 7.40796 loss)
I0523 03:33:41.750243 34682 sgd_solver.cpp:112] Iteration 42430, lr = 0.01
I0523 03:33:46.875938 34682 solver.cpp:239] Iteration 42440 (1.92096 iter/s, 5.20572s/10 iters), loss = 8.61495
I0523 03:33:46.875993 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61495 (* 1 = 8.61495 loss)
I0523 03:33:47.681979 34682 sgd_solver.cpp:112] Iteration 42440, lr = 0.01
I0523 03:33:52.642976 34682 solver.cpp:239] Iteration 42450 (1.73408 iter/s, 5.76676s/10 iters), loss = 8.98525
I0523 03:33:52.643110 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98525 (* 1 = 8.98525 loss)
I0523 03:33:52.705165 34682 sgd_solver.cpp:112] Iteration 42450, lr = 0.01
I0523 03:33:56.836688 34682 solver.cpp:239] Iteration 42460 (2.3847 iter/s, 4.19339s/10 iters), loss = 8.92565
I0523 03:33:56.836752 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92565 (* 1 = 8.92565 loss)
I0523 03:33:57.439087 34682 sgd_solver.cpp:112] Iteration 42460, lr = 0.01
I0523 03:34:03.647112 34682 solver.cpp:239] Iteration 42470 (1.46841 iter/s, 6.81008s/10 iters), loss = 9.11389
I0523 03:34:03.647176 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11389 (* 1 = 9.11389 loss)
I0523 03:34:03.711524 34682 sgd_solver.cpp:112] Iteration 42470, lr = 0.01
I0523 03:34:08.518637 34682 solver.cpp:239] Iteration 42480 (2.05285 iter/s, 4.87126s/10 iters), loss = 8.95199
I0523 03:34:08.518677 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95199 (* 1 = 8.95199 loss)
I0523 03:34:08.595326 34682 sgd_solver.cpp:112] Iteration 42480, lr = 0.01
I0523 03:34:11.117012 34682 solver.cpp:239] Iteration 42490 (3.84879 iter/s, 2.59822s/10 iters), loss = 9.08802
I0523 03:34:11.117067 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08802 (* 1 = 9.08802 loss)
I0523 03:34:11.840541 34682 sgd_solver.cpp:112] Iteration 42490, lr = 0.01
I0523 03:34:17.460391 34682 solver.cpp:239] Iteration 42500 (1.57653 iter/s, 6.34305s/10 iters), loss = 8.20237
I0523 03:34:17.460463 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20237 (* 1 = 8.20237 loss)
I0523 03:34:18.300731 34682 sgd_solver.cpp:112] Iteration 42500, lr = 0.01
I0523 03:34:25.420325 34682 solver.cpp:239] Iteration 42510 (1.25635 iter/s, 7.95954s/10 iters), loss = 8.47291
I0523 03:34:25.420464 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47291 (* 1 = 8.47291 loss)
I0523 03:34:26.104079 34682 sgd_solver.cpp:112] Iteration 42510, lr = 0.01
I0523 03:34:31.056988 34682 solver.cpp:239] Iteration 42520 (1.77422 iter/s, 5.63629s/10 iters), loss = 9.18224
I0523 03:34:31.057044 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18224 (* 1 = 9.18224 loss)
I0523 03:34:31.118824 34682 sgd_solver.cpp:112] Iteration 42520, lr = 0.01
I0523 03:34:36.036598 34682 solver.cpp:239] Iteration 42530 (2.0083 iter/s, 4.97935s/10 iters), loss = 9.12513
I0523 03:34:36.036666 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12513 (* 1 = 9.12513 loss)
I0523 03:34:36.800870 34682 sgd_solver.cpp:112] Iteration 42530, lr = 0.01
I0523 03:34:41.375396 34682 solver.cpp:239] Iteration 42540 (1.87319 iter/s, 5.33849s/10 iters), loss = 9.35105
I0523 03:34:41.375457 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35105 (* 1 = 9.35105 loss)
I0523 03:34:42.243708 34682 sgd_solver.cpp:112] Iteration 42540, lr = 0.01
I0523 03:34:46.438583 34682 solver.cpp:239] Iteration 42550 (1.97515 iter/s, 5.0629s/10 iters), loss = 9.79733
I0523 03:34:46.438633 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.79733 (* 1 = 9.79733 loss)
I0523 03:34:47.241866 34682 sgd_solver.cpp:112] Iteration 42550, lr = 0.01
I0523 03:34:52.070844 34682 solver.cpp:239] Iteration 42560 (1.77558 iter/s, 5.63197s/10 iters), loss = 9.6945
I0523 03:34:52.070904 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.6945 (* 1 = 9.6945 loss)
I0523 03:34:52.148650 34682 sgd_solver.cpp:112] Iteration 42560, lr = 0.01
I0523 03:34:55.544970 34682 solver.cpp:239] Iteration 42570 (2.87859 iter/s, 3.47392s/10 iters), loss = 8.69328
I0523 03:34:55.545241 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69328 (* 1 = 8.69328 loss)
I0523 03:34:56.385109 34682 sgd_solver.cpp:112] Iteration 42570, lr = 0.01
I0523 03:35:00.541383 34682 solver.cpp:239] Iteration 42580 (2.00163 iter/s, 4.99594s/10 iters), loss = 8.7857
I0523 03:35:00.541429 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7857 (* 1 = 8.7857 loss)
I0523 03:35:00.622829 34682 sgd_solver.cpp:112] Iteration 42580, lr = 0.01
I0523 03:35:04.733217 34682 solver.cpp:239] Iteration 42590 (2.38572 iter/s, 4.19161s/10 iters), loss = 9.96084
I0523 03:35:04.733276 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.96084 (* 1 = 9.96084 loss)
I0523 03:35:05.321985 34682 sgd_solver.cpp:112] Iteration 42590, lr = 0.01
I0523 03:35:10.804903 34682 solver.cpp:239] Iteration 42600 (1.64707 iter/s, 6.07138s/10 iters), loss = 8.39834
I0523 03:35:10.804953 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39834 (* 1 = 8.39834 loss)
I0523 03:35:11.611502 34682 sgd_solver.cpp:112] Iteration 42600, lr = 0.01
I0523 03:35:16.359709 34682 solver.cpp:239] Iteration 42610 (1.80033 iter/s, 5.55454s/10 iters), loss = 9.25883
I0523 03:35:16.359752 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25883 (* 1 = 9.25883 loss)
I0523 03:35:16.417616 34682 sgd_solver.cpp:112] Iteration 42610, lr = 0.01
I0523 03:35:21.834277 34682 solver.cpp:239] Iteration 42620 (1.82672 iter/s, 5.47429s/10 iters), loss = 8.87592
I0523 03:35:21.834347 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87592 (* 1 = 8.87592 loss)
I0523 03:35:21.906281 34682 sgd_solver.cpp:112] Iteration 42620, lr = 0.01
I0523 03:35:25.977291 34682 solver.cpp:239] Iteration 42630 (2.41384 iter/s, 4.14277s/10 iters), loss = 8.76237
I0523 03:35:25.977550 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76237 (* 1 = 8.76237 loss)
I0523 03:35:26.051574 34682 sgd_solver.cpp:112] Iteration 42630, lr = 0.01
I0523 03:35:31.723023 34682 solver.cpp:239] Iteration 42640 (1.74057 iter/s, 5.74525s/10 iters), loss = 8.60129
I0523 03:35:31.723084 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60129 (* 1 = 8.60129 loss)
I0523 03:35:31.784812 34682 sgd_solver.cpp:112] Iteration 42640, lr = 0.01
I0523 03:35:35.414963 34682 solver.cpp:239] Iteration 42650 (2.70876 iter/s, 3.69173s/10 iters), loss = 8.42717
I0523 03:35:35.415035 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42717 (* 1 = 8.42717 loss)
I0523 03:35:36.292632 34682 sgd_solver.cpp:112] Iteration 42650, lr = 0.01
I0523 03:35:41.435428 34682 solver.cpp:239] Iteration 42660 (1.66109 iter/s, 6.02015s/10 iters), loss = 8.89468
I0523 03:35:41.435483 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89468 (* 1 = 8.89468 loss)
I0523 03:35:42.259006 34682 sgd_solver.cpp:112] Iteration 42660, lr = 0.01
I0523 03:35:48.717937 34682 solver.cpp:239] Iteration 42670 (1.37322 iter/s, 7.28216s/10 iters), loss = 9.49
I0523 03:35:48.717993 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.49 (* 1 = 9.49 loss)
I0523 03:35:49.489053 34682 sgd_solver.cpp:112] Iteration 42670, lr = 0.01
I0523 03:35:53.830428 34682 solver.cpp:239] Iteration 42680 (1.95609 iter/s, 5.11223s/10 iters), loss = 8.36418
I0523 03:35:53.830482 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36418 (* 1 = 8.36418 loss)
I0523 03:35:54.363003 34682 sgd_solver.cpp:112] Iteration 42680, lr = 0.01
I0523 03:35:58.645015 34682 solver.cpp:239] Iteration 42690 (2.07713 iter/s, 4.81434s/10 iters), loss = 8.82642
I0523 03:35:58.645270 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82642 (* 1 = 8.82642 loss)
I0523 03:35:58.721981 34682 sgd_solver.cpp:112] Iteration 42690, lr = 0.01
I0523 03:36:02.084081 34682 solver.cpp:239] Iteration 42700 (2.90808 iter/s, 3.4387s/10 iters), loss = 9.07003
I0523 03:36:02.084127 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07003 (* 1 = 9.07003 loss)
I0523 03:36:02.148183 34682 sgd_solver.cpp:112] Iteration 42700, lr = 0.01
I0523 03:36:06.150399 34682 solver.cpp:239] Iteration 42710 (2.45936 iter/s, 4.06609s/10 iters), loss = 8.8046
I0523 03:36:06.150455 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8046 (* 1 = 8.8046 loss)
I0523 03:36:06.777009 34682 sgd_solver.cpp:112] Iteration 42710, lr = 0.01
I0523 03:36:10.895864 34682 solver.cpp:239] Iteration 42720 (2.10738 iter/s, 4.74522s/10 iters), loss = 9.39345
I0523 03:36:10.895908 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.39345 (* 1 = 9.39345 loss)
I0523 03:36:10.965833 34682 sgd_solver.cpp:112] Iteration 42720, lr = 0.01
I0523 03:36:14.854982 34682 solver.cpp:239] Iteration 42730 (2.52595 iter/s, 3.9589s/10 iters), loss = 9.23363
I0523 03:36:14.855049 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23363 (* 1 = 9.23363 loss)
I0523 03:36:15.008196 34682 sgd_solver.cpp:112] Iteration 42730, lr = 0.01
I0523 03:36:18.891952 34682 solver.cpp:239] Iteration 42740 (2.47725 iter/s, 4.03674s/10 iters), loss = 9.33538
I0523 03:36:18.891994 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33538 (* 1 = 9.33538 loss)
I0523 03:36:18.961635 34682 sgd_solver.cpp:112] Iteration 42740, lr = 0.01
I0523 03:36:22.947064 34682 solver.cpp:239] Iteration 42750 (2.46615 iter/s, 4.0549s/10 iters), loss = 8.831
I0523 03:36:22.947110 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.831 (* 1 = 8.831 loss)
I0523 03:36:23.020364 34682 sgd_solver.cpp:112] Iteration 42750, lr = 0.01
I0523 03:36:28.583895 34682 solver.cpp:239] Iteration 42760 (1.77414 iter/s, 5.63654s/10 iters), loss = 9.68592
I0523 03:36:28.583945 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.68592 (* 1 = 9.68592 loss)
I0523 03:36:29.424010 34682 sgd_solver.cpp:112] Iteration 42760, lr = 0.01
I0523 03:36:33.399051 34682 solver.cpp:239] Iteration 42770 (2.07688 iter/s, 4.81491s/10 iters), loss = 8.01402
I0523 03:36:33.399103 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01402 (* 1 = 8.01402 loss)
I0523 03:36:33.463023 34682 sgd_solver.cpp:112] Iteration 42770, lr = 0.01
I0523 03:36:39.041671 34682 solver.cpp:239] Iteration 42780 (1.77231 iter/s, 5.64234s/10 iters), loss = 8.54829
I0523 03:36:39.041712 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54829 (* 1 = 8.54829 loss)
I0523 03:36:39.121958 34682 sgd_solver.cpp:112] Iteration 42780, lr = 0.01
I0523 03:36:45.041193 34682 solver.cpp:239] Iteration 42790 (1.66688 iter/s, 5.99923s/10 iters), loss = 8.29682
I0523 03:36:45.041239 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29682 (* 1 = 8.29682 loss)
I0523 03:36:45.113123 34682 sgd_solver.cpp:112] Iteration 42790, lr = 0.01
I0523 03:36:51.063292 34682 solver.cpp:239] Iteration 42800 (1.66063 iter/s, 6.0218s/10 iters), loss = 8.71461
I0523 03:36:51.063349 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71461 (* 1 = 8.71461 loss)
I0523 03:36:51.141502 34682 sgd_solver.cpp:112] Iteration 42800, lr = 0.01
I0523 03:36:57.210404 34682 solver.cpp:239] Iteration 42810 (1.62686 iter/s, 6.14681s/10 iters), loss = 8.64163
I0523 03:36:57.210475 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64163 (* 1 = 8.64163 loss)
I0523 03:36:57.857851 34682 sgd_solver.cpp:112] Iteration 42810, lr = 0.01
I0523 03:37:02.887212 34682 solver.cpp:239] Iteration 42820 (1.76164 iter/s, 5.67651s/10 iters), loss = 9.26341
I0523 03:37:02.887468 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.26341 (* 1 = 9.26341 loss)
I0523 03:37:03.512200 34682 sgd_solver.cpp:112] Iteration 42820, lr = 0.01
I0523 03:37:09.138487 34682 solver.cpp:239] Iteration 42830 (1.5998 iter/s, 6.25079s/10 iters), loss = 9.26734
I0523 03:37:09.138550 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.26734 (* 1 = 9.26734 loss)
I0523 03:37:09.207134 34682 sgd_solver.cpp:112] Iteration 42830, lr = 0.01
I0523 03:37:12.073253 34682 solver.cpp:239] Iteration 42840 (3.40765 iter/s, 2.93458s/10 iters), loss = 7.75442
I0523 03:37:12.073307 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75442 (* 1 = 7.75442 loss)
I0523 03:37:12.722035 34682 sgd_solver.cpp:112] Iteration 42840, lr = 0.01
I0523 03:37:19.176694 34682 solver.cpp:239] Iteration 42850 (1.40784 iter/s, 7.10309s/10 iters), loss = 8.44734
I0523 03:37:19.176755 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44734 (* 1 = 8.44734 loss)
I0523 03:37:19.234434 34682 sgd_solver.cpp:112] Iteration 42850, lr = 0.01
I0523 03:37:23.348322 34682 solver.cpp:239] Iteration 42860 (2.39727 iter/s, 4.1714s/10 iters), loss = 8.37761
I0523 03:37:23.348368 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37761 (* 1 = 8.37761 loss)
I0523 03:37:24.162377 34682 sgd_solver.cpp:112] Iteration 42860, lr = 0.01
I0523 03:37:28.985880 34682 solver.cpp:239] Iteration 42870 (1.77391 iter/s, 5.63727s/10 iters), loss = 8.54701
I0523 03:37:28.985945 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54701 (* 1 = 8.54701 loss)
I0523 03:37:29.059001 34682 sgd_solver.cpp:112] Iteration 42870, lr = 0.01
I0523 03:37:32.373667 34682 solver.cpp:239] Iteration 42880 (2.95196 iter/s, 3.38758s/10 iters), loss = 8.50907
I0523 03:37:32.373714 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50907 (* 1 = 8.50907 loss)
I0523 03:37:32.440657 34682 sgd_solver.cpp:112] Iteration 42880, lr = 0.01
I0523 03:37:36.329960 34682 solver.cpp:239] Iteration 42890 (2.52775 iter/s, 3.95608s/10 iters), loss = 8.52112
I0523 03:37:36.330098 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52112 (* 1 = 8.52112 loss)
I0523 03:37:36.401904 34682 sgd_solver.cpp:112] Iteration 42890, lr = 0.01
I0523 03:37:42.088310 34682 solver.cpp:239] Iteration 42900 (1.73672 iter/s, 5.75797s/10 iters), loss = 8.51711
I0523 03:37:42.088361 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51711 (* 1 = 8.51711 loss)
I0523 03:37:42.164212 34682 sgd_solver.cpp:112] Iteration 42900, lr = 0.01
I0523 03:37:46.995229 34682 solver.cpp:239] Iteration 42910 (2.03804 iter/s, 4.90667s/10 iters), loss = 8.90686
I0523 03:37:46.995285 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90686 (* 1 = 8.90686 loss)
I0523 03:37:47.060180 34682 sgd_solver.cpp:112] Iteration 42910, lr = 0.01
I0523 03:37:50.965966 34682 solver.cpp:239] Iteration 42920 (2.51857 iter/s, 3.97051s/10 iters), loss = 9.11273
I0523 03:37:50.966019 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11273 (* 1 = 9.11273 loss)
I0523 03:37:51.252887 34682 sgd_solver.cpp:112] Iteration 42920, lr = 0.01
I0523 03:37:55.289404 34682 solver.cpp:239] Iteration 42930 (2.3131 iter/s, 4.32321s/10 iters), loss = 9.1009
I0523 03:37:55.289448 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1009 (* 1 = 9.1009 loss)
I0523 03:37:55.348906 34682 sgd_solver.cpp:112] Iteration 42930, lr = 0.01
I0523 03:38:00.696296 34682 solver.cpp:239] Iteration 42940 (1.84958 iter/s, 5.40663s/10 iters), loss = 9.28896
I0523 03:38:00.696348 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28896 (* 1 = 9.28896 loss)
I0523 03:38:01.502038 34682 sgd_solver.cpp:112] Iteration 42940, lr = 0.01
I0523 03:38:05.497045 34682 solver.cpp:239] Iteration 42950 (2.08312 iter/s, 4.8005s/10 iters), loss = 8.99927
I0523 03:38:05.497095 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99927 (* 1 = 8.99927 loss)
I0523 03:38:05.561208 34682 sgd_solver.cpp:112] Iteration 42950, lr = 0.01
I0523 03:38:10.248872 34682 solver.cpp:239] Iteration 42960 (2.10456 iter/s, 4.75158s/10 iters), loss = 8.04055
I0523 03:38:10.249089 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04055 (* 1 = 8.04055 loss)
I0523 03:38:10.310295 34682 sgd_solver.cpp:112] Iteration 42960, lr = 0.01
I0523 03:38:14.354248 34682 solver.cpp:239] Iteration 42970 (2.43605 iter/s, 4.105s/10 iters), loss = 9.0158
I0523 03:38:14.354291 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0158 (* 1 = 9.0158 loss)
I0523 03:38:15.177829 34682 sgd_solver.cpp:112] Iteration 42970, lr = 0.01
I0523 03:38:20.185518 34682 solver.cpp:239] Iteration 42980 (1.71498 iter/s, 5.83098s/10 iters), loss = 9.03963
I0523 03:38:20.185560 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03963 (* 1 = 9.03963 loss)
I0523 03:38:20.267952 34682 sgd_solver.cpp:112] Iteration 42980, lr = 0.01
I0523 03:38:26.308735 34682 solver.cpp:239] Iteration 42990 (1.63321 iter/s, 6.12293s/10 iters), loss = 8.39363
I0523 03:38:26.308784 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39363 (* 1 = 8.39363 loss)
I0523 03:38:26.898008 34682 sgd_solver.cpp:112] Iteration 42990, lr = 0.01
I0523 03:38:33.154289 34682 solver.cpp:239] Iteration 43000 (1.46087 iter/s, 6.84523s/10 iters), loss = 8.80742
I0523 03:38:33.154346 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80742 (* 1 = 8.80742 loss)
I0523 03:38:33.859318 34682 sgd_solver.cpp:112] Iteration 43000, lr = 0.01
I0523 03:38:37.140398 34682 solver.cpp:239] Iteration 43010 (2.50885 iter/s, 3.98589s/10 iters), loss = 9.34776
I0523 03:38:37.140444 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34776 (* 1 = 9.34776 loss)
I0523 03:38:37.203023 34682 sgd_solver.cpp:112] Iteration 43010, lr = 0.01
I0523 03:38:43.115691 34682 solver.cpp:239] Iteration 43020 (1.67364 iter/s, 5.97499s/10 iters), loss = 8.87384
I0523 03:38:43.115931 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87384 (* 1 = 8.87384 loss)
I0523 03:38:43.861277 34682 sgd_solver.cpp:112] Iteration 43020, lr = 0.01
I0523 03:38:48.041060 34682 solver.cpp:239] Iteration 43030 (2.03048 iter/s, 4.92495s/10 iters), loss = 9.11063
I0523 03:38:48.041112 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11063 (* 1 = 9.11063 loss)
I0523 03:38:48.670294 34682 sgd_solver.cpp:112] Iteration 43030, lr = 0.01
I0523 03:38:53.621163 34682 solver.cpp:239] Iteration 43040 (1.79217 iter/s, 5.57983s/10 iters), loss = 8.57075
I0523 03:38:53.621210 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57075 (* 1 = 8.57075 loss)
I0523 03:38:53.964696 34682 sgd_solver.cpp:112] Iteration 43040, lr = 0.01
I0523 03:38:56.766867 34682 solver.cpp:239] Iteration 43050 (3.17912 iter/s, 3.14552s/10 iters), loss = 8.72868
I0523 03:38:56.766913 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72868 (* 1 = 8.72868 loss)
I0523 03:38:56.825146 34682 sgd_solver.cpp:112] Iteration 43050, lr = 0.01
I0523 03:39:00.022513 34682 solver.cpp:239] Iteration 43060 (3.07176 iter/s, 3.25546s/10 iters), loss = 8.44164
I0523 03:39:00.022564 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44164 (* 1 = 8.44164 loss)
I0523 03:39:00.333286 34682 sgd_solver.cpp:112] Iteration 43060, lr = 0.01
I0523 03:39:06.093329 34682 solver.cpp:239] Iteration 43070 (1.6473 iter/s, 6.07052s/10 iters), loss = 8.4255
I0523 03:39:06.093382 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4255 (* 1 = 8.4255 loss)
I0523 03:39:06.152478 34682 sgd_solver.cpp:112] Iteration 43070, lr = 0.01
I0523 03:39:11.944211 34682 solver.cpp:239] Iteration 43080 (1.70923 iter/s, 5.85058s/10 iters), loss = 8.94795
I0523 03:39:11.944260 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94795 (* 1 = 8.94795 loss)
I0523 03:39:12.010872 34682 sgd_solver.cpp:112] Iteration 43080, lr = 0.01
I0523 03:39:15.823027 34682 solver.cpp:239] Iteration 43090 (2.57825 iter/s, 3.8786s/10 iters), loss = 9.39185
I0523 03:39:15.823359 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.39185 (* 1 = 9.39185 loss)
I0523 03:39:16.693608 34682 sgd_solver.cpp:112] Iteration 43090, lr = 0.01
I0523 03:39:21.500340 34682 solver.cpp:239] Iteration 43100 (1.76156 iter/s, 5.67678s/10 iters), loss = 8.45299
I0523 03:39:21.500399 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45299 (* 1 = 8.45299 loss)
I0523 03:39:21.960608 34682 sgd_solver.cpp:112] Iteration 43100, lr = 0.01
I0523 03:39:26.275432 34682 solver.cpp:239] Iteration 43110 (2.09432 iter/s, 4.77483s/10 iters), loss = 9.42725
I0523 03:39:26.275485 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42725 (* 1 = 9.42725 loss)
I0523 03:39:26.339733 34682 sgd_solver.cpp:112] Iteration 43110, lr = 0.01
I0523 03:39:33.357201 34682 solver.cpp:239] Iteration 43120 (1.41214 iter/s, 7.08143s/10 iters), loss = 8.53958
I0523 03:39:33.357249 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53958 (* 1 = 8.53958 loss)
I0523 03:39:33.429985 34682 sgd_solver.cpp:112] Iteration 43120, lr = 0.01
I0523 03:39:36.792223 34682 solver.cpp:239] Iteration 43130 (2.91135 iter/s, 3.43483s/10 iters), loss = 9.19682
I0523 03:39:36.792279 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19682 (* 1 = 9.19682 loss)
I0523 03:39:36.849416 34682 sgd_solver.cpp:112] Iteration 43130, lr = 0.01
I0523 03:39:41.828935 34682 solver.cpp:239] Iteration 43140 (1.98553 iter/s, 5.03643s/10 iters), loss = 9.52158
I0523 03:39:41.829007 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.52158 (* 1 = 9.52158 loss)
I0523 03:39:42.626574 34682 sgd_solver.cpp:112] Iteration 43140, lr = 0.01
I0523 03:39:47.907061 34682 solver.cpp:239] Iteration 43150 (1.64533 iter/s, 6.07781s/10 iters), loss = 8.76598
I0523 03:39:47.907304 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76598 (* 1 = 8.76598 loss)
I0523 03:39:48.741648 34682 sgd_solver.cpp:112] Iteration 43150, lr = 0.01
I0523 03:39:55.730047 34682 solver.cpp:239] Iteration 43160 (1.27837 iter/s, 7.82244s/10 iters), loss = 9.60937
I0523 03:39:55.730118 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60937 (* 1 = 9.60937 loss)
I0523 03:39:56.401268 34682 sgd_solver.cpp:112] Iteration 43160, lr = 0.01
I0523 03:40:00.536553 34682 solver.cpp:239] Iteration 43170 (2.08062 iter/s, 4.80625s/10 iters), loss = 9.71547
I0523 03:40:00.536605 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.71547 (* 1 = 9.71547 loss)
I0523 03:40:00.608451 34682 sgd_solver.cpp:112] Iteration 43170, lr = 0.01
I0523 03:40:05.537870 34682 solver.cpp:239] Iteration 43180 (1.99958 iter/s, 5.00105s/10 iters), loss = 9.20975
I0523 03:40:05.537919 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20975 (* 1 = 9.20975 loss)
I0523 03:40:05.600191 34682 sgd_solver.cpp:112] Iteration 43180, lr = 0.01
I0523 03:40:09.909932 34682 solver.cpp:239] Iteration 43190 (2.28737 iter/s, 4.37183s/10 iters), loss = 9.16881
I0523 03:40:09.909984 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16881 (* 1 = 9.16881 loss)
I0523 03:40:09.976549 34682 sgd_solver.cpp:112] Iteration 43190, lr = 0.01
I0523 03:40:13.719853 34682 solver.cpp:239] Iteration 43200 (2.62488 iter/s, 3.8097s/10 iters), loss = 8.38841
I0523 03:40:13.719916 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38841 (* 1 = 8.38841 loss)
I0523 03:40:13.781497 34682 sgd_solver.cpp:112] Iteration 43200, lr = 0.01
I0523 03:40:17.403906 34682 solver.cpp:239] Iteration 43210 (2.71457 iter/s, 3.68383s/10 iters), loss = 8.71916
I0523 03:40:17.403986 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71916 (* 1 = 8.71916 loss)
I0523 03:40:18.136634 34682 sgd_solver.cpp:112] Iteration 43210, lr = 0.01
I0523 03:40:21.410857 34682 solver.cpp:239] Iteration 43220 (2.49581 iter/s, 4.00672s/10 iters), loss = 9.27086
I0523 03:40:21.410900 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27086 (* 1 = 9.27086 loss)
I0523 03:40:21.472856 34682 sgd_solver.cpp:112] Iteration 43220, lr = 0.01
I0523 03:40:25.191469 34682 solver.cpp:239] Iteration 43230 (2.64522 iter/s, 3.7804s/10 iters), loss = 9.15731
I0523 03:40:25.191531 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15731 (* 1 = 9.15731 loss)
I0523 03:40:26.008527 34682 sgd_solver.cpp:112] Iteration 43230, lr = 0.01
I0523 03:40:30.375870 34682 solver.cpp:239] Iteration 43240 (1.92896 iter/s, 5.18413s/10 iters), loss = 9.11881
I0523 03:40:30.375924 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11881 (* 1 = 9.11881 loss)
I0523 03:40:31.115252 34682 sgd_solver.cpp:112] Iteration 43240, lr = 0.01
I0523 03:40:38.844228 34682 solver.cpp:239] Iteration 43250 (1.18092 iter/s, 8.46796s/10 iters), loss = 9.32166
I0523 03:40:38.844274 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32166 (* 1 = 9.32166 loss)
I0523 03:40:38.908679 34682 sgd_solver.cpp:112] Iteration 43250, lr = 0.01
I0523 03:40:44.668514 34682 solver.cpp:239] Iteration 43260 (1.71769 iter/s, 5.82176s/10 iters), loss = 7.90797
I0523 03:40:44.668556 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90797 (* 1 = 7.90797 loss)
I0523 03:40:44.745539 34682 sgd_solver.cpp:112] Iteration 43260, lr = 0.01
I0523 03:40:50.308868 34682 solver.cpp:239] Iteration 43270 (1.77302 iter/s, 5.64008s/10 iters), loss = 8.46897
I0523 03:40:50.309104 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46897 (* 1 = 8.46897 loss)
I0523 03:40:50.442124 34682 sgd_solver.cpp:112] Iteration 43270, lr = 0.01
I0523 03:40:54.376235 34682 solver.cpp:239] Iteration 43280 (2.45882 iter/s, 4.067s/10 iters), loss = 9.19905
I0523 03:40:54.376277 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19905 (* 1 = 9.19905 loss)
I0523 03:40:54.452986 34682 sgd_solver.cpp:112] Iteration 43280, lr = 0.01
I0523 03:41:01.039257 34682 solver.cpp:239] Iteration 43290 (1.5009 iter/s, 6.66268s/10 iters), loss = 8.80692
I0523 03:41:01.039324 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80692 (* 1 = 8.80692 loss)
I0523 03:41:01.888737 34682 sgd_solver.cpp:112] Iteration 43290, lr = 0.01
I0523 03:41:07.323276 34682 solver.cpp:239] Iteration 43300 (1.59142 iter/s, 6.2837s/10 iters), loss = 9.3101
I0523 03:41:07.323318 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3101 (* 1 = 9.3101 loss)
I0523 03:41:07.399816 34682 sgd_solver.cpp:112] Iteration 43300, lr = 0.01
I0523 03:41:10.298876 34682 solver.cpp:239] Iteration 43310 (3.36087 iter/s, 2.97542s/10 iters), loss = 9.14826
I0523 03:41:10.298941 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14826 (* 1 = 9.14826 loss)
I0523 03:41:11.171569 34682 sgd_solver.cpp:112] Iteration 43310, lr = 0.01
I0523 03:41:15.229543 34682 solver.cpp:239] Iteration 43320 (2.02823 iter/s, 4.9304s/10 iters), loss = 8.97888
I0523 03:41:15.229590 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97888 (* 1 = 8.97888 loss)
I0523 03:41:15.854692 34682 sgd_solver.cpp:112] Iteration 43320, lr = 0.01
I0523 03:41:18.689069 34682 solver.cpp:239] Iteration 43330 (2.89073 iter/s, 3.45933s/10 iters), loss = 8.29921
I0523 03:41:18.689115 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29921 (* 1 = 8.29921 loss)
I0523 03:41:18.768677 34682 sgd_solver.cpp:112] Iteration 43330, lr = 0.01
I0523 03:41:23.348892 34682 solver.cpp:239] Iteration 43340 (2.14611 iter/s, 4.65959s/10 iters), loss = 8.79372
I0523 03:41:23.350739 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79372 (* 1 = 8.79372 loss)
I0523 03:41:23.426662 34682 sgd_solver.cpp:112] Iteration 43340, lr = 0.01
I0523 03:41:27.809602 34682 solver.cpp:239] Iteration 43350 (2.24408 iter/s, 4.45618s/10 iters), loss = 9.06385
I0523 03:41:27.809657 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06385 (* 1 = 9.06385 loss)
I0523 03:41:27.883874 34682 sgd_solver.cpp:112] Iteration 43350, lr = 0.01
I0523 03:41:32.574676 34682 solver.cpp:239] Iteration 43360 (2.09872 iter/s, 4.76482s/10 iters), loss = 9.33342
I0523 03:41:32.574743 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33342 (* 1 = 9.33342 loss)
I0523 03:41:32.632977 34682 sgd_solver.cpp:112] Iteration 43360, lr = 0.01
I0523 03:41:36.835670 34682 solver.cpp:239] Iteration 43370 (2.347 iter/s, 4.26075s/10 iters), loss = 8.64477
I0523 03:41:36.835719 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64477 (* 1 = 8.64477 loss)
I0523 03:41:37.449785 34682 sgd_solver.cpp:112] Iteration 43370, lr = 0.01
I0523 03:41:40.950522 34682 solver.cpp:239] Iteration 43380 (2.43036 iter/s, 4.11462s/10 iters), loss = 8.60365
I0523 03:41:40.950577 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60365 (* 1 = 8.60365 loss)
I0523 03:41:41.019629 34682 sgd_solver.cpp:112] Iteration 43380, lr = 0.01
I0523 03:41:46.062716 34682 solver.cpp:239] Iteration 43390 (1.95621 iter/s, 5.11192s/10 iters), loss = 8.55463
I0523 03:41:46.062762 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55463 (* 1 = 8.55463 loss)
I0523 03:41:46.119292 34682 sgd_solver.cpp:112] Iteration 43390, lr = 0.01
I0523 03:41:50.274492 34682 solver.cpp:239] Iteration 43400 (2.37444 iter/s, 4.21152s/10 iters), loss = 8.76178
I0523 03:41:50.274564 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76178 (* 1 = 8.76178 loss)
I0523 03:41:50.393709 34682 sgd_solver.cpp:112] Iteration 43400, lr = 0.01
I0523 03:41:54.698293 34682 solver.cpp:239] Iteration 43410 (2.26062 iter/s, 4.42356s/10 iters), loss = 8.7668
I0523 03:41:54.698554 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7668 (* 1 = 8.7668 loss)
I0523 03:41:54.757076 34682 sgd_solver.cpp:112] Iteration 43410, lr = 0.01
I0523 03:41:57.262171 34682 solver.cpp:239] Iteration 43420 (3.90087 iter/s, 2.56353s/10 iters), loss = 7.90545
I0523 03:41:57.262214 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90545 (* 1 = 7.90545 loss)
I0523 03:41:57.343015 34682 sgd_solver.cpp:112] Iteration 43420, lr = 0.01
I0523 03:42:01.465817 34682 solver.cpp:239] Iteration 43430 (2.37901 iter/s, 4.20342s/10 iters), loss = 8.77711
I0523 03:42:01.465873 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77711 (* 1 = 8.77711 loss)
I0523 03:42:01.531936 34682 sgd_solver.cpp:112] Iteration 43430, lr = 0.01
I0523 03:42:06.078060 34682 solver.cpp:239] Iteration 43440 (2.16826 iter/s, 4.61199s/10 iters), loss = 9.00785
I0523 03:42:06.078121 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00785 (* 1 = 9.00785 loss)
I0523 03:42:06.892424 34682 sgd_solver.cpp:112] Iteration 43440, lr = 0.01
I0523 03:42:12.433279 34682 solver.cpp:239] Iteration 43450 (1.57359 iter/s, 6.35491s/10 iters), loss = 9.16289
I0523 03:42:12.433336 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16289 (* 1 = 9.16289 loss)
I0523 03:42:13.272581 34682 sgd_solver.cpp:112] Iteration 43450, lr = 0.01
I0523 03:42:17.326815 34682 solver.cpp:239] Iteration 43460 (2.04364 iter/s, 4.89324s/10 iters), loss = 8.32159
I0523 03:42:17.326859 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32159 (* 1 = 8.32159 loss)
I0523 03:42:17.386534 34682 sgd_solver.cpp:112] Iteration 43460, lr = 0.01
I0523 03:42:21.272739 34682 solver.cpp:239] Iteration 43470 (2.5344 iter/s, 3.94571s/10 iters), loss = 9.3999
I0523 03:42:21.272805 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3999 (* 1 = 9.3999 loss)
I0523 03:42:22.093906 34682 sgd_solver.cpp:112] Iteration 43470, lr = 0.01
I0523 03:42:26.167209 34682 solver.cpp:239] Iteration 43480 (2.04323 iter/s, 4.89421s/10 iters), loss = 8.95091
I0523 03:42:26.167434 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95091 (* 1 = 8.95091 loss)
I0523 03:42:26.240540 34682 sgd_solver.cpp:112] Iteration 43480, lr = 0.01
I0523 03:42:30.188318 34682 solver.cpp:239] Iteration 43490 (2.4871 iter/s, 4.02075s/10 iters), loss = 8.52475
I0523 03:42:30.188359 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52475 (* 1 = 8.52475 loss)
I0523 03:42:30.258188 34682 sgd_solver.cpp:112] Iteration 43490, lr = 0.01
I0523 03:42:35.050591 34682 solver.cpp:239] Iteration 43500 (2.05676 iter/s, 4.86203s/10 iters), loss = 8.91546
I0523 03:42:35.050644 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91546 (* 1 = 8.91546 loss)
I0523 03:42:35.131927 34682 sgd_solver.cpp:112] Iteration 43500, lr = 0.01
I0523 03:42:39.426724 34682 solver.cpp:239] Iteration 43510 (2.28525 iter/s, 4.37589s/10 iters), loss = 9.56866
I0523 03:42:39.426776 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56866 (* 1 = 9.56866 loss)
I0523 03:42:39.487489 34682 sgd_solver.cpp:112] Iteration 43510, lr = 0.01
I0523 03:42:44.915544 34682 solver.cpp:239] Iteration 43520 (1.82198 iter/s, 5.48855s/10 iters), loss = 8.70252
I0523 03:42:44.915585 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70252 (* 1 = 8.70252 loss)
I0523 03:42:44.978088 34682 sgd_solver.cpp:112] Iteration 43520, lr = 0.01
I0523 03:42:48.357000 34682 solver.cpp:239] Iteration 43530 (2.90591 iter/s, 3.44127s/10 iters), loss = 7.90779
I0523 03:42:48.357046 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90779 (* 1 = 7.90779 loss)
I0523 03:42:48.425382 34682 sgd_solver.cpp:112] Iteration 43530, lr = 0.01
I0523 03:42:53.196533 34682 solver.cpp:239] Iteration 43540 (2.06642 iter/s, 4.83929s/10 iters), loss = 7.72797
I0523 03:42:53.196575 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72797 (* 1 = 7.72797 loss)
I0523 03:42:53.269228 34682 sgd_solver.cpp:112] Iteration 43540, lr = 0.01
I0523 03:42:58.466780 34682 solver.cpp:239] Iteration 43550 (1.89754 iter/s, 5.26999s/10 iters), loss = 8.53856
I0523 03:42:58.467047 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53856 (* 1 = 8.53856 loss)
I0523 03:42:58.541479 34682 sgd_solver.cpp:112] Iteration 43550, lr = 0.01
I0523 03:43:01.538230 34682 solver.cpp:239] Iteration 43560 (3.25619 iter/s, 3.07108s/10 iters), loss = 9.44541
I0523 03:43:01.538274 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44541 (* 1 = 9.44541 loss)
I0523 03:43:01.602349 34682 sgd_solver.cpp:112] Iteration 43560, lr = 0.01
I0523 03:43:06.845726 34682 solver.cpp:239] Iteration 43570 (1.88422 iter/s, 5.30723s/10 iters), loss = 8.73269
I0523 03:43:06.845773 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73269 (* 1 = 8.73269 loss)
I0523 03:43:06.906474 34682 sgd_solver.cpp:112] Iteration 43570, lr = 0.01
I0523 03:43:11.816395 34682 solver.cpp:239] Iteration 43580 (2.0119 iter/s, 4.97042s/10 iters), loss = 9.58199
I0523 03:43:11.816448 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.58199 (* 1 = 9.58199 loss)
I0523 03:43:11.880247 34682 sgd_solver.cpp:112] Iteration 43580, lr = 0.01
I0523 03:43:15.945946 34682 solver.cpp:239] Iteration 43590 (2.4217 iter/s, 4.12934s/10 iters), loss = 8.64202
I0523 03:43:15.945988 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64202 (* 1 = 8.64202 loss)
I0523 03:43:16.808218 34682 sgd_solver.cpp:112] Iteration 43590, lr = 0.01
I0523 03:43:19.937168 34682 solver.cpp:239] Iteration 43600 (2.50563 iter/s, 3.99101s/10 iters), loss = 8.11168
I0523 03:43:19.937227 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11168 (* 1 = 8.11168 loss)
I0523 03:43:20.598290 34682 sgd_solver.cpp:112] Iteration 43600, lr = 0.01
I0523 03:43:23.817706 34682 solver.cpp:239] Iteration 43610 (2.57711 iter/s, 3.88031s/10 iters), loss = 8.88861
I0523 03:43:23.817755 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88861 (* 1 = 8.88861 loss)
I0523 03:43:23.878106 34682 sgd_solver.cpp:112] Iteration 43610, lr = 0.01
I0523 03:43:27.238099 34682 solver.cpp:239] Iteration 43620 (2.9238 iter/s, 3.4202s/10 iters), loss = 8.34988
I0523 03:43:27.238142 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34988 (* 1 = 8.34988 loss)
I0523 03:43:27.301903 34682 sgd_solver.cpp:112] Iteration 43620, lr = 0.01
I0523 03:43:30.590414 34682 solver.cpp:239] Iteration 43630 (2.98319 iter/s, 3.35212s/10 iters), loss = 8.60005
I0523 03:43:30.590813 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60005 (* 1 = 8.60005 loss)
I0523 03:43:30.654573 34682 sgd_solver.cpp:112] Iteration 43630, lr = 0.01
I0523 03:43:34.933686 34682 solver.cpp:239] Iteration 43640 (2.30266 iter/s, 4.34281s/10 iters), loss = 8.60853
I0523 03:43:34.933749 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60853 (* 1 = 8.60853 loss)
I0523 03:43:35.010397 34682 sgd_solver.cpp:112] Iteration 43640, lr = 0.01
I0523 03:43:39.100421 34682 solver.cpp:239] Iteration 43650 (2.40009 iter/s, 4.16651s/10 iters), loss = 9.00018
I0523 03:43:39.100471 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00018 (* 1 = 9.00018 loss)
I0523 03:43:39.166522 34682 sgd_solver.cpp:112] Iteration 43650, lr = 0.01
I0523 03:43:44.507213 34682 solver.cpp:239] Iteration 43660 (1.84962 iter/s, 5.40652s/10 iters), loss = 8.43144
I0523 03:43:44.507280 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43144 (* 1 = 8.43144 loss)
I0523 03:43:44.568414 34682 sgd_solver.cpp:112] Iteration 43660, lr = 0.01
I0523 03:43:51.456003 34682 solver.cpp:239] Iteration 43670 (1.43917 iter/s, 6.94844s/10 iters), loss = 8.713
I0523 03:43:51.456058 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.713 (* 1 = 8.713 loss)
I0523 03:43:52.343865 34682 sgd_solver.cpp:112] Iteration 43670, lr = 0.01
I0523 03:43:56.189034 34682 solver.cpp:239] Iteration 43680 (2.11293 iter/s, 4.73275s/10 iters), loss = 8.04575
I0523 03:43:56.189080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04575 (* 1 = 8.04575 loss)
I0523 03:43:56.251822 34682 sgd_solver.cpp:112] Iteration 43680, lr = 0.01
I0523 03:44:00.366708 34682 solver.cpp:239] Iteration 43690 (2.39381 iter/s, 4.17744s/10 iters), loss = 8.92702
I0523 03:44:00.366755 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92702 (* 1 = 8.92702 loss)
I0523 03:44:00.727587 34682 sgd_solver.cpp:112] Iteration 43690, lr = 0.01
I0523 03:44:04.569046 34682 solver.cpp:239] Iteration 43700 (2.37976 iter/s, 4.20211s/10 iters), loss = 8.46758
I0523 03:44:04.569103 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46758 (* 1 = 8.46758 loss)
I0523 03:44:04.637670 34682 sgd_solver.cpp:112] Iteration 43700, lr = 0.01
I0523 03:44:09.477490 34682 solver.cpp:239] Iteration 43710 (2.03741 iter/s, 4.90819s/10 iters), loss = 8.21042
I0523 03:44:09.477535 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21042 (* 1 = 8.21042 loss)
I0523 03:44:10.292523 34682 sgd_solver.cpp:112] Iteration 43710, lr = 0.01
I0523 03:44:16.638993 34682 solver.cpp:239] Iteration 43720 (1.39642 iter/s, 7.16117s/10 iters), loss = 9.67401
I0523 03:44:16.639039 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.67401 (* 1 = 9.67401 loss)
I0523 03:44:16.712575 34682 sgd_solver.cpp:112] Iteration 43720, lr = 0.01
I0523 03:44:22.354677 34682 solver.cpp:239] Iteration 43730 (1.74966 iter/s, 5.71541s/10 iters), loss = 8.01709
I0523 03:44:22.354737 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01709 (* 1 = 8.01709 loss)
I0523 03:44:22.430459 34682 sgd_solver.cpp:112] Iteration 43730, lr = 0.01
I0523 03:44:26.644357 34682 solver.cpp:239] Iteration 43740 (2.3313 iter/s, 4.28944s/10 iters), loss = 9.13461
I0523 03:44:26.644402 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13461 (* 1 = 9.13461 loss)
I0523 03:44:26.701861 34682 sgd_solver.cpp:112] Iteration 43740, lr = 0.01
I0523 03:44:30.009047 34682 solver.cpp:239] Iteration 43750 (2.9722 iter/s, 3.36451s/10 iters), loss = 8.25527
I0523 03:44:30.009089 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25527 (* 1 = 8.25527 loss)
I0523 03:44:30.073487 34682 sgd_solver.cpp:112] Iteration 43750, lr = 0.01
I0523 03:44:33.652833 34682 solver.cpp:239] Iteration 43760 (2.74455 iter/s, 3.64358s/10 iters), loss = 8.2712
I0523 03:44:33.653050 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2712 (* 1 = 8.2712 loss)
I0523 03:44:34.490736 34682 sgd_solver.cpp:112] Iteration 43760, lr = 0.01
I0523 03:44:40.564067 34682 solver.cpp:239] Iteration 43770 (1.44702 iter/s, 6.91076s/10 iters), loss = 9.34836
I0523 03:44:40.564122 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.34836 (* 1 = 9.34836 loss)
I0523 03:44:41.365830 34682 sgd_solver.cpp:112] Iteration 43770, lr = 0.01
I0523 03:44:45.966279 34682 solver.cpp:239] Iteration 43780 (1.85119 iter/s, 5.40194s/10 iters), loss = 9.04395
I0523 03:44:45.966326 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04395 (* 1 = 9.04395 loss)
I0523 03:44:46.028139 34682 sgd_solver.cpp:112] Iteration 43780, lr = 0.01
I0523 03:44:50.214593 34682 solver.cpp:239] Iteration 43790 (2.354 iter/s, 4.24809s/10 iters), loss = 9.18174
I0523 03:44:50.214643 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18174 (* 1 = 9.18174 loss)
I0523 03:44:50.275501 34682 sgd_solver.cpp:112] Iteration 43790, lr = 0.01
I0523 03:44:53.112947 34682 solver.cpp:239] Iteration 43800 (3.45044 iter/s, 2.89818s/10 iters), loss = 8.81441
I0523 03:44:53.112993 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81441 (* 1 = 8.81441 loss)
I0523 03:44:53.185142 34682 sgd_solver.cpp:112] Iteration 43800, lr = 0.01
I0523 03:44:55.473373 34682 solver.cpp:239] Iteration 43810 (4.23683 iter/s, 2.36025s/10 iters), loss = 8.02495
I0523 03:44:55.473462 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02495 (* 1 = 8.02495 loss)
I0523 03:44:55.524715 34682 sgd_solver.cpp:112] Iteration 43810, lr = 0.01
I0523 03:44:57.882082 34682 solver.cpp:239] Iteration 43820 (4.15195 iter/s, 2.40851s/10 iters), loss = 8.21641
I0523 03:44:57.882150 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21641 (* 1 = 8.21641 loss)
I0523 03:44:58.672181 34682 sgd_solver.cpp:112] Iteration 43820, lr = 0.01
I0523 03:45:03.286015 34682 solver.cpp:239] Iteration 43830 (1.8506 iter/s, 5.40365s/10 iters), loss = 9.06312
I0523 03:45:03.286054 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06312 (* 1 = 9.06312 loss)
I0523 03:45:03.358850 34682 sgd_solver.cpp:112] Iteration 43830, lr = 0.01
I0523 03:45:08.878484 34682 solver.cpp:239] Iteration 43840 (1.78821 iter/s, 5.5922s/10 iters), loss = 8.63723
I0523 03:45:08.878824 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63723 (* 1 = 8.63723 loss)
I0523 03:45:08.955510 34682 sgd_solver.cpp:112] Iteration 43840, lr = 0.01
I0523 03:45:13.772394 34682 solver.cpp:239] Iteration 43850 (2.04357 iter/s, 4.8934s/10 iters), loss = 8.26789
I0523 03:45:13.772452 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26789 (* 1 = 8.26789 loss)
I0523 03:45:13.848203 34682 sgd_solver.cpp:112] Iteration 43850, lr = 0.01
I0523 03:45:17.889698 34682 solver.cpp:239] Iteration 43860 (2.42891 iter/s, 4.11707s/10 iters), loss = 9.19126
I0523 03:45:17.889751 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19126 (* 1 = 9.19126 loss)
I0523 03:45:18.631459 34682 sgd_solver.cpp:112] Iteration 43860, lr = 0.01
I0523 03:45:23.223242 34682 solver.cpp:239] Iteration 43870 (1.87502 iter/s, 5.33327s/10 iters), loss = 8.24543
I0523 03:45:23.223290 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24543 (* 1 = 8.24543 loss)
I0523 03:45:23.294183 34682 sgd_solver.cpp:112] Iteration 43870, lr = 0.01
I0523 03:45:29.077903 34682 solver.cpp:239] Iteration 43880 (1.70812 iter/s, 5.85438s/10 iters), loss = 8.74501
I0523 03:45:29.077958 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74501 (* 1 = 8.74501 loss)
I0523 03:45:29.147239 34682 sgd_solver.cpp:112] Iteration 43880, lr = 0.01
I0523 03:45:32.717566 34682 solver.cpp:239] Iteration 43890 (2.74767 iter/s, 3.63945s/10 iters), loss = 8.15234
I0523 03:45:32.717614 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15234 (* 1 = 8.15234 loss)
I0523 03:45:33.553892 34682 sgd_solver.cpp:112] Iteration 43890, lr = 0.01
I0523 03:45:37.576092 34682 solver.cpp:239] Iteration 43900 (2.05834 iter/s, 4.85828s/10 iters), loss = 9.00059
I0523 03:45:37.576145 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00059 (* 1 = 9.00059 loss)
I0523 03:45:38.327814 34682 sgd_solver.cpp:112] Iteration 43900, lr = 0.01
I0523 03:45:43.467494 34682 solver.cpp:239] Iteration 43910 (1.69748 iter/s, 5.8911s/10 iters), loss = 8.79204
I0523 03:45:43.467787 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79204 (* 1 = 8.79204 loss)
I0523 03:45:43.544404 34682 sgd_solver.cpp:112] Iteration 43910, lr = 0.01
I0523 03:45:46.058981 34682 solver.cpp:239] Iteration 43920 (3.86284 iter/s, 2.58877s/10 iters), loss = 8.47946
I0523 03:45:46.059029 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47946 (* 1 = 8.47946 loss)
I0523 03:45:46.842135 34682 sgd_solver.cpp:112] Iteration 43920, lr = 0.01
I0523 03:45:52.625967 34682 solver.cpp:239] Iteration 43930 (1.52284 iter/s, 6.56667s/10 iters), loss = 8.60848
I0523 03:45:52.626013 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60848 (* 1 = 8.60848 loss)
I0523 03:45:53.470166 34682 sgd_solver.cpp:112] Iteration 43930, lr = 0.01
I0523 03:45:57.399693 34682 solver.cpp:239] Iteration 43940 (2.09491 iter/s, 4.77347s/10 iters), loss = 8.80338
I0523 03:45:57.399756 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80338 (* 1 = 8.80338 loss)
I0523 03:45:58.194926 34682 sgd_solver.cpp:112] Iteration 43940, lr = 0.01
I0523 03:46:02.685914 34682 solver.cpp:239] Iteration 43950 (1.89182 iter/s, 5.28593s/10 iters), loss = 8.30907
I0523 03:46:02.685962 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30907 (* 1 = 8.30907 loss)
I0523 03:46:02.743721 34682 sgd_solver.cpp:112] Iteration 43950, lr = 0.01
I0523 03:46:07.428524 34682 solver.cpp:239] Iteration 43960 (2.10865 iter/s, 4.74237s/10 iters), loss = 8.96523
I0523 03:46:07.428570 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96523 (* 1 = 8.96523 loss)
I0523 03:46:08.250172 34682 sgd_solver.cpp:112] Iteration 43960, lr = 0.01
I0523 03:46:13.570168 34682 solver.cpp:239] Iteration 43970 (1.62831 iter/s, 6.14135s/10 iters), loss = 8.8504
I0523 03:46:13.570300 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8504 (* 1 = 8.8504 loss)
I0523 03:46:14.357532 34682 sgd_solver.cpp:112] Iteration 43970, lr = 0.01
I0523 03:46:18.815521 34682 solver.cpp:239] Iteration 43980 (1.90657 iter/s, 5.24501s/10 iters), loss = 8.99481
I0523 03:46:18.815565 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99481 (* 1 = 8.99481 loss)
I0523 03:46:18.879148 34682 sgd_solver.cpp:112] Iteration 43980, lr = 0.01
I0523 03:46:22.313448 34682 solver.cpp:239] Iteration 43990 (2.859 iter/s, 3.49772s/10 iters), loss = 7.87954
I0523 03:46:22.313508 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87954 (* 1 = 7.87954 loss)
I0523 03:46:22.948607 34682 sgd_solver.cpp:112] Iteration 43990, lr = 0.01
I0523 03:46:26.173688 34682 solver.cpp:239] Iteration 44000 (2.59066 iter/s, 3.86002s/10 iters), loss = 8.30801
I0523 03:46:26.173735 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30801 (* 1 = 8.30801 loss)
I0523 03:46:26.988339 34682 sgd_solver.cpp:112] Iteration 44000, lr = 0.01
I0523 03:46:30.397334 34682 solver.cpp:239] Iteration 44010 (2.36775 iter/s, 4.22342s/10 iters), loss = 8.85003
I0523 03:46:30.397373 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85003 (* 1 = 8.85003 loss)
I0523 03:46:30.459110 34682 sgd_solver.cpp:112] Iteration 44010, lr = 0.01
I0523 03:46:34.240350 34682 solver.cpp:239] Iteration 44020 (2.60226 iter/s, 3.84281s/10 iters), loss = 9.14265
I0523 03:46:34.240393 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14265 (* 1 = 9.14265 loss)
I0523 03:46:34.317828 34682 sgd_solver.cpp:112] Iteration 44020, lr = 0.01
I0523 03:46:41.305526 34682 solver.cpp:239] Iteration 44030 (1.41546 iter/s, 7.06485s/10 iters), loss = 8.95213
I0523 03:46:41.305567 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95213 (* 1 = 8.95213 loss)
I0523 03:46:41.373327 34682 sgd_solver.cpp:112] Iteration 44030, lr = 0.01
I0523 03:46:45.352941 34682 solver.cpp:239] Iteration 44040 (2.47085 iter/s, 4.04719s/10 iters), loss = 9.77612
I0523 03:46:45.353153 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.77612 (* 1 = 9.77612 loss)
I0523 03:46:45.851472 34682 sgd_solver.cpp:112] Iteration 44040, lr = 0.01
I0523 03:46:50.939884 34682 solver.cpp:239] Iteration 44050 (1.79003 iter/s, 5.5865s/10 iters), loss = 8.1832
I0523 03:46:50.939932 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1832 (* 1 = 8.1832 loss)
I0523 03:46:51.000309 34682 sgd_solver.cpp:112] Iteration 44050, lr = 0.01
I0523 03:46:54.904784 34682 solver.cpp:239] Iteration 44060 (2.52228 iter/s, 3.96466s/10 iters), loss = 7.63156
I0523 03:46:54.904850 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63156 (* 1 = 7.63156 loss)
I0523 03:46:54.984964 34682 sgd_solver.cpp:112] Iteration 44060, lr = 0.01
I0523 03:46:59.155331 34682 solver.cpp:239] Iteration 44070 (2.35278 iter/s, 4.2503s/10 iters), loss = 8.29762
I0523 03:46:59.155388 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29762 (* 1 = 8.29762 loss)
I0523 03:46:59.996280 34682 sgd_solver.cpp:112] Iteration 44070, lr = 0.01
I0523 03:47:04.161013 34682 solver.cpp:239] Iteration 44080 (1.99783 iter/s, 5.00543s/10 iters), loss = 8.46712
I0523 03:47:04.161059 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46712 (* 1 = 8.46712 loss)
I0523 03:47:04.901394 34682 sgd_solver.cpp:112] Iteration 44080, lr = 0.01
I0523 03:47:08.898092 34682 solver.cpp:239] Iteration 44090 (2.11111 iter/s, 4.73683s/10 iters), loss = 8.75888
I0523 03:47:08.898145 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75888 (* 1 = 8.75888 loss)
I0523 03:47:09.743381 34682 sgd_solver.cpp:112] Iteration 44090, lr = 0.01
I0523 03:47:16.731513 34682 solver.cpp:239] Iteration 44100 (1.27664 iter/s, 7.83305s/10 iters), loss = 8.3332
I0523 03:47:16.731693 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3332 (* 1 = 8.3332 loss)
I0523 03:47:16.800818 34682 sgd_solver.cpp:112] Iteration 44100, lr = 0.01
I0523 03:47:20.030130 34682 solver.cpp:239] Iteration 44110 (3.03187 iter/s, 3.2983s/10 iters), loss = 9.20613
I0523 03:47:20.030203 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20613 (* 1 = 9.20613 loss)
I0523 03:47:20.091241 34682 sgd_solver.cpp:112] Iteration 44110, lr = 0.01
I0523 03:47:24.238900 34682 solver.cpp:239] Iteration 44120 (2.37617 iter/s, 4.20845s/10 iters), loss = 8.51233
I0523 03:47:24.238966 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51233 (* 1 = 8.51233 loss)
I0523 03:47:25.101449 34682 sgd_solver.cpp:112] Iteration 44120, lr = 0.01
I0523 03:47:27.672670 34682 solver.cpp:239] Iteration 44130 (2.91244 iter/s, 3.43355s/10 iters), loss = 7.96769
I0523 03:47:27.672730 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96769 (* 1 = 7.96769 loss)
I0523 03:47:28.512857 34682 sgd_solver.cpp:112] Iteration 44130, lr = 0.01
I0523 03:47:32.926672 34682 solver.cpp:239] Iteration 44140 (1.90341 iter/s, 5.25373s/10 iters), loss = 8.58642
I0523 03:47:32.926738 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58642 (* 1 = 8.58642 loss)
I0523 03:47:32.998165 34682 sgd_solver.cpp:112] Iteration 44140, lr = 0.01
I0523 03:47:36.947755 34682 solver.cpp:239] Iteration 44150 (2.48705 iter/s, 4.02083s/10 iters), loss = 8.07217
I0523 03:47:36.947834 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07217 (* 1 = 8.07217 loss)
I0523 03:47:37.003108 34682 sgd_solver.cpp:112] Iteration 44150, lr = 0.01
I0523 03:47:42.743108 34682 solver.cpp:239] Iteration 44160 (1.72561 iter/s, 5.79505s/10 iters), loss = 9.91332
I0523 03:47:42.743163 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.91332 (* 1 = 9.91332 loss)
I0523 03:47:43.527510 34682 sgd_solver.cpp:112] Iteration 44160, lr = 0.01
I0523 03:47:47.682212 34682 solver.cpp:239] Iteration 44170 (2.02477 iter/s, 4.93884s/10 iters), loss = 8.70976
I0523 03:47:47.682437 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70976 (* 1 = 8.70976 loss)
I0523 03:47:47.753031 34682 sgd_solver.cpp:112] Iteration 44170, lr = 0.01
I0523 03:47:50.477704 34682 solver.cpp:239] Iteration 44180 (3.57758 iter/s, 2.79518s/10 iters), loss = 8.59529
I0523 03:47:50.477748 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59529 (* 1 = 8.59529 loss)
I0523 03:47:50.550920 34682 sgd_solver.cpp:112] Iteration 44180, lr = 0.01
I0523 03:47:56.147562 34682 solver.cpp:239] Iteration 44190 (1.7638 iter/s, 5.66958s/10 iters), loss = 8.83111
I0523 03:47:56.147610 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83111 (* 1 = 8.83111 loss)
I0523 03:47:56.220212 34682 sgd_solver.cpp:112] Iteration 44190, lr = 0.01
I0523 03:48:01.203056 34682 solver.cpp:239] Iteration 44200 (1.97815 iter/s, 5.05524s/10 iters), loss = 7.78416
I0523 03:48:01.203099 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.78416 (* 1 = 7.78416 loss)
I0523 03:48:01.276911 34682 sgd_solver.cpp:112] Iteration 44200, lr = 0.01
I0523 03:48:06.245067 34682 solver.cpp:239] Iteration 44210 (1.98343 iter/s, 5.04176s/10 iters), loss = 8.48252
I0523 03:48:06.245120 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48252 (* 1 = 8.48252 loss)
I0523 03:48:06.304530 34682 sgd_solver.cpp:112] Iteration 44210, lr = 0.01
I0523 03:48:12.516372 34682 solver.cpp:239] Iteration 44220 (1.59465 iter/s, 6.27098s/10 iters), loss = 8.82917
I0523 03:48:12.516430 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82917 (* 1 = 8.82917 loss)
I0523 03:48:12.589058 34682 sgd_solver.cpp:112] Iteration 44220, lr = 0.01
I0523 03:48:18.616474 34682 solver.cpp:239] Iteration 44230 (1.6394 iter/s, 6.09979s/10 iters), loss = 8.91401
I0523 03:48:18.616745 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91401 (* 1 = 8.91401 loss)
I0523 03:48:19.346307 34682 sgd_solver.cpp:112] Iteration 44230, lr = 0.01
I0523 03:48:23.427345 34682 solver.cpp:239] Iteration 44240 (2.07882 iter/s, 4.81041s/10 iters), loss = 8.80084
I0523 03:48:23.427395 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80084 (* 1 = 8.80084 loss)
I0523 03:48:23.496193 34682 sgd_solver.cpp:112] Iteration 44240, lr = 0.01
I0523 03:48:26.014767 34682 solver.cpp:239] Iteration 44250 (3.86509 iter/s, 2.58726s/10 iters), loss = 9.09921
I0523 03:48:26.014833 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09921 (* 1 = 9.09921 loss)
I0523 03:48:26.084040 34682 sgd_solver.cpp:112] Iteration 44250, lr = 0.01
I0523 03:48:29.432628 34682 solver.cpp:239] Iteration 44260 (2.92599 iter/s, 3.41765s/10 iters), loss = 8.91168
I0523 03:48:29.432682 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91168 (* 1 = 8.91168 loss)
I0523 03:48:29.501288 34682 sgd_solver.cpp:112] Iteration 44260, lr = 0.01
I0523 03:48:35.344820 34682 solver.cpp:239] Iteration 44270 (1.69151 iter/s, 5.91189s/10 iters), loss = 8.62845
I0523 03:48:35.344888 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62845 (* 1 = 8.62845 loss)
I0523 03:48:35.720521 34682 sgd_solver.cpp:112] Iteration 44270, lr = 0.01
I0523 03:48:39.761291 34682 solver.cpp:239] Iteration 44280 (2.26438 iter/s, 4.41622s/10 iters), loss = 8.99995
I0523 03:48:39.761342 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99995 (* 1 = 8.99995 loss)
I0523 03:48:39.822266 34682 sgd_solver.cpp:112] Iteration 44280, lr = 0.01
I0523 03:48:42.327461 34682 solver.cpp:239] Iteration 44290 (3.89713 iter/s, 2.56599s/10 iters), loss = 8.23437
I0523 03:48:42.327510 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23437 (* 1 = 8.23437 loss)
I0523 03:48:42.390763 34682 sgd_solver.cpp:112] Iteration 44290, lr = 0.01
I0523 03:48:45.133450 34682 solver.cpp:239] Iteration 44300 (3.56403 iter/s, 2.80581s/10 iters), loss = 8.89533
I0523 03:48:45.133518 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89533 (* 1 = 8.89533 loss)
I0523 03:48:45.905207 34682 sgd_solver.cpp:112] Iteration 44300, lr = 0.01
I0523 03:48:49.593191 34682 solver.cpp:239] Iteration 44310 (2.2424 iter/s, 4.4595s/10 iters), loss = 9.44391
I0523 03:48:49.593402 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44391 (* 1 = 9.44391 loss)
I0523 03:48:49.655884 34682 sgd_solver.cpp:112] Iteration 44310, lr = 0.01
I0523 03:48:55.085340 34682 solver.cpp:239] Iteration 44320 (1.82093 iter/s, 5.49171s/10 iters), loss = 9.31354
I0523 03:48:55.085386 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31354 (* 1 = 9.31354 loss)
I0523 03:48:55.943970 34682 sgd_solver.cpp:112] Iteration 44320, lr = 0.01
I0523 03:49:02.266537 34682 solver.cpp:239] Iteration 44330 (1.39259 iter/s, 7.18086s/10 iters), loss = 9.60375
I0523 03:49:02.266602 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60375 (* 1 = 9.60375 loss)
I0523 03:49:02.340919 34682 sgd_solver.cpp:112] Iteration 44330, lr = 0.01
I0523 03:49:06.388427 34682 solver.cpp:239] Iteration 44340 (2.42621 iter/s, 4.12166s/10 iters), loss = 9.2595
I0523 03:49:06.388471 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2595 (* 1 = 9.2595 loss)
I0523 03:49:06.452620 34682 sgd_solver.cpp:112] Iteration 44340, lr = 0.01
I0523 03:49:09.200392 34682 solver.cpp:239] Iteration 44350 (3.55644 iter/s, 2.8118s/10 iters), loss = 8.31664
I0523 03:49:09.200428 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31664 (* 1 = 8.31664 loss)
I0523 03:49:10.034570 34682 sgd_solver.cpp:112] Iteration 44350, lr = 0.01
I0523 03:49:14.242969 34682 solver.cpp:239] Iteration 44360 (1.98321 iter/s, 5.04234s/10 iters), loss = 7.9136
I0523 03:49:14.243018 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9136 (* 1 = 7.9136 loss)
I0523 03:49:14.323846 34682 sgd_solver.cpp:112] Iteration 44360, lr = 0.01
I0523 03:49:18.979272 34682 solver.cpp:239] Iteration 44370 (2.11146 iter/s, 4.73606s/10 iters), loss = 9.82032
I0523 03:49:18.979326 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.82032 (* 1 = 9.82032 loss)
I0523 03:49:19.778916 34682 sgd_solver.cpp:112] Iteration 44370, lr = 0.01
I0523 03:49:22.322953 34682 solver.cpp:239] Iteration 44380 (2.99089 iter/s, 3.34349s/10 iters), loss = 9.02364
I0523 03:49:22.323006 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02364 (* 1 = 9.02364 loss)
I0523 03:49:22.400461 34682 sgd_solver.cpp:112] Iteration 44380, lr = 0.01
I0523 03:49:26.662545 34682 solver.cpp:239] Iteration 44390 (2.30449 iter/s, 4.33936s/10 iters), loss = 9.03413
I0523 03:49:26.662597 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03413 (* 1 = 9.03413 loss)
I0523 03:49:27.507278 34682 sgd_solver.cpp:112] Iteration 44390, lr = 0.01
I0523 03:49:33.389706 34682 solver.cpp:239] Iteration 44400 (1.48658 iter/s, 6.72683s/10 iters), loss = 8.89605
I0523 03:49:33.389832 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89605 (* 1 = 8.89605 loss)
I0523 03:49:33.448449 34682 sgd_solver.cpp:112] Iteration 44400, lr = 0.01
I0523 03:49:36.771689 34682 solver.cpp:239] Iteration 44410 (2.95709 iter/s, 3.3817s/10 iters), loss = 9.12167
I0523 03:49:36.771739 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12167 (* 1 = 9.12167 loss)
I0523 03:49:36.840680 34682 sgd_solver.cpp:112] Iteration 44410, lr = 0.01
I0523 03:49:40.558014 34682 solver.cpp:239] Iteration 44420 (2.64123 iter/s, 3.78611s/10 iters), loss = 8.99169
I0523 03:49:40.558068 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99169 (* 1 = 8.99169 loss)
I0523 03:49:41.219358 34682 sgd_solver.cpp:112] Iteration 44420, lr = 0.01
I0523 03:49:46.599630 34682 solver.cpp:239] Iteration 44430 (1.65527 iter/s, 6.04131s/10 iters), loss = 8.60806
I0523 03:49:46.599683 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60806 (* 1 = 8.60806 loss)
I0523 03:49:46.669621 34682 sgd_solver.cpp:112] Iteration 44430, lr = 0.01
I0523 03:49:51.525326 34682 solver.cpp:239] Iteration 44440 (2.03027 iter/s, 4.92545s/10 iters), loss = 9.02739
I0523 03:49:51.525555 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02739 (* 1 = 9.02739 loss)
I0523 03:49:52.292419 34682 sgd_solver.cpp:112] Iteration 44440, lr = 0.01
I0523 03:49:56.293277 34682 solver.cpp:239] Iteration 44450 (2.09751 iter/s, 4.76755s/10 iters), loss = 9.1558
I0523 03:49:56.293318 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1558 (* 1 = 9.1558 loss)
I0523 03:49:56.367240 34682 sgd_solver.cpp:112] Iteration 44450, lr = 0.01
I0523 03:50:02.285588 34682 solver.cpp:239] Iteration 44460 (1.66889 iter/s, 5.99202s/10 iters), loss = 8.79103
I0523 03:50:02.285643 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79103 (* 1 = 8.79103 loss)
I0523 03:50:02.347107 34682 sgd_solver.cpp:112] Iteration 44460, lr = 0.01
I0523 03:50:06.895539 34682 solver.cpp:239] Iteration 44470 (2.16934 iter/s, 4.60971s/10 iters), loss = 9.03986
I0523 03:50:06.895591 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03986 (* 1 = 9.03986 loss)
I0523 03:50:06.946723 34682 sgd_solver.cpp:112] Iteration 44470, lr = 0.01
I0523 03:50:11.935683 34682 solver.cpp:239] Iteration 44480 (1.98418 iter/s, 5.03988s/10 iters), loss = 8.26718
I0523 03:50:11.935734 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26718 (* 1 = 8.26718 loss)
I0523 03:50:12.005188 34682 sgd_solver.cpp:112] Iteration 44480, lr = 0.01
I0523 03:50:17.031006 34682 solver.cpp:239] Iteration 44490 (1.96268 iter/s, 5.09506s/10 iters), loss = 8.53947
I0523 03:50:17.031057 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53947 (* 1 = 8.53947 loss)
I0523 03:50:17.878707 34682 sgd_solver.cpp:112] Iteration 44490, lr = 0.01
I0523 03:50:23.444344 34682 solver.cpp:239] Iteration 44500 (1.55933 iter/s, 6.41303s/10 iters), loss = 9.178
I0523 03:50:23.444638 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.178 (* 1 = 9.178 loss)
I0523 03:50:23.518597 34682 sgd_solver.cpp:112] Iteration 44500, lr = 0.01
I0523 03:50:27.097878 34682 solver.cpp:239] Iteration 44510 (2.73739 iter/s, 3.65312s/10 iters), loss = 8.22609
I0523 03:50:27.097929 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22609 (* 1 = 8.22609 loss)
I0523 03:50:27.153005 34682 sgd_solver.cpp:112] Iteration 44510, lr = 0.01
I0523 03:50:30.296942 34682 solver.cpp:239] Iteration 44520 (3.1261 iter/s, 3.19887s/10 iters), loss = 8.62478
I0523 03:50:30.296982 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62478 (* 1 = 8.62478 loss)
I0523 03:50:30.369856 34682 sgd_solver.cpp:112] Iteration 44520, lr = 0.01
I0523 03:50:33.976905 34682 solver.cpp:239] Iteration 44530 (2.71757 iter/s, 3.67976s/10 iters), loss = 9.38895
I0523 03:50:33.976968 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.38895 (* 1 = 9.38895 loss)
I0523 03:50:34.792898 34682 sgd_solver.cpp:112] Iteration 44530, lr = 0.01
I0523 03:50:38.601081 34682 solver.cpp:239] Iteration 44540 (2.16267 iter/s, 4.62392s/10 iters), loss = 8.88702
I0523 03:50:38.601131 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88702 (* 1 = 8.88702 loss)
I0523 03:50:39.282726 34682 sgd_solver.cpp:112] Iteration 44540, lr = 0.01
I0523 03:50:44.719758 34682 solver.cpp:239] Iteration 44550 (1.63442 iter/s, 6.11838s/10 iters), loss = 8.58545
I0523 03:50:44.719797 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58545 (* 1 = 8.58545 loss)
I0523 03:50:44.799266 34682 sgd_solver.cpp:112] Iteration 44550, lr = 0.01
I0523 03:50:48.725558 34682 solver.cpp:239] Iteration 44560 (2.49651 iter/s, 4.00559s/10 iters), loss = 8.95734
I0523 03:50:48.725615 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95734 (* 1 = 8.95734 loss)
I0523 03:50:49.583549 34682 sgd_solver.cpp:112] Iteration 44560, lr = 0.01
I0523 03:50:53.790385 34682 solver.cpp:239] Iteration 44570 (1.97451 iter/s, 5.06456s/10 iters), loss = 8.60723
I0523 03:50:53.790673 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60723 (* 1 = 8.60723 loss)
I0523 03:50:54.605033 34682 sgd_solver.cpp:112] Iteration 44570, lr = 0.01
I0523 03:51:00.155532 34682 solver.cpp:239] Iteration 44580 (1.57118 iter/s, 6.36464s/10 iters), loss = 8.8159
I0523 03:51:00.155581 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8159 (* 1 = 8.8159 loss)
I0523 03:51:00.233666 34682 sgd_solver.cpp:112] Iteration 44580, lr = 0.01
I0523 03:51:05.335981 34682 solver.cpp:239] Iteration 44590 (1.93043 iter/s, 5.18018s/10 iters), loss = 8.62028
I0523 03:51:05.336040 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62028 (* 1 = 8.62028 loss)
I0523 03:51:05.403002 34682 sgd_solver.cpp:112] Iteration 44590, lr = 0.01
I0523 03:51:09.770685 34682 solver.cpp:239] Iteration 44600 (2.25507 iter/s, 4.43446s/10 iters), loss = 9.11333
I0523 03:51:09.770752 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11333 (* 1 = 9.11333 loss)
I0523 03:51:09.847136 34682 sgd_solver.cpp:112] Iteration 44600, lr = 0.01
I0523 03:51:13.227730 34682 solver.cpp:239] Iteration 44610 (2.89282 iter/s, 3.45684s/10 iters), loss = 9.10573
I0523 03:51:13.227773 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10573 (* 1 = 9.10573 loss)
I0523 03:51:13.281411 34682 sgd_solver.cpp:112] Iteration 44610, lr = 0.01
I0523 03:51:16.788758 34682 solver.cpp:239] Iteration 44620 (2.80833 iter/s, 3.56083s/10 iters), loss = 8.31801
I0523 03:51:16.788812 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31801 (* 1 = 8.31801 loss)
I0523 03:51:16.986205 34682 sgd_solver.cpp:112] Iteration 44620, lr = 0.01
I0523 03:51:22.536986 34682 solver.cpp:239] Iteration 44630 (1.73975 iter/s, 5.74794s/10 iters), loss = 8.94826
I0523 03:51:22.537045 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94826 (* 1 = 8.94826 loss)
I0523 03:51:22.618772 34682 sgd_solver.cpp:112] Iteration 44630, lr = 0.01
I0523 03:51:27.311810 34682 solver.cpp:239] Iteration 44640 (2.09443 iter/s, 4.77457s/10 iters), loss = 8.76967
I0523 03:51:27.311997 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76967 (* 1 = 8.76967 loss)
I0523 03:51:27.595839 34682 sgd_solver.cpp:112] Iteration 44640, lr = 0.01
I0523 03:51:30.185550 34682 solver.cpp:239] Iteration 44650 (3.48016 iter/s, 2.87344s/10 iters), loss = 8.88604
I0523 03:51:30.185595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88604 (* 1 = 8.88604 loss)
I0523 03:51:30.974728 34682 sgd_solver.cpp:112] Iteration 44650, lr = 0.01
I0523 03:51:36.038233 34682 solver.cpp:239] Iteration 44660 (1.7087 iter/s, 5.85239s/10 iters), loss = 8.96072
I0523 03:51:36.038286 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96072 (* 1 = 8.96072 loss)
I0523 03:51:36.115104 34682 sgd_solver.cpp:112] Iteration 44660, lr = 0.01
I0523 03:51:39.429342 34682 solver.cpp:239] Iteration 44670 (2.94906 iter/s, 3.39092s/10 iters), loss = 9.19861
I0523 03:51:39.429381 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19861 (* 1 = 9.19861 loss)
I0523 03:51:39.499943 34682 sgd_solver.cpp:112] Iteration 44670, lr = 0.01
I0523 03:51:44.123014 34682 solver.cpp:239] Iteration 44680 (2.13064 iter/s, 4.69343s/10 iters), loss = 8.58235
I0523 03:51:44.123059 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58235 (* 1 = 8.58235 loss)
I0523 03:51:44.196207 34682 sgd_solver.cpp:112] Iteration 44680, lr = 0.01
I0523 03:51:47.665541 34682 solver.cpp:239] Iteration 44690 (2.823 iter/s, 3.54233s/10 iters), loss = 9.00065
I0523 03:51:47.665591 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00065 (* 1 = 9.00065 loss)
I0523 03:51:48.446377 34682 sgd_solver.cpp:112] Iteration 44690, lr = 0.01
I0523 03:51:52.410794 34682 solver.cpp:239] Iteration 44700 (2.10748 iter/s, 4.74501s/10 iters), loss = 8.28421
I0523 03:51:52.410859 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28421 (* 1 = 8.28421 loss)
I0523 03:51:52.483888 34682 sgd_solver.cpp:112] Iteration 44700, lr = 0.01
I0523 03:51:56.018319 34682 solver.cpp:239] Iteration 44710 (2.77216 iter/s, 3.6073s/10 iters), loss = 8.80464
I0523 03:51:56.018388 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80464 (* 1 = 8.80464 loss)
I0523 03:51:56.883710 34682 sgd_solver.cpp:112] Iteration 44710, lr = 0.01
I0523 03:52:01.067618 34682 solver.cpp:239] Iteration 44720 (1.98058 iter/s, 5.04903s/10 iters), loss = 8.60815
I0523 03:52:01.067824 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60815 (* 1 = 8.60815 loss)
I0523 03:52:01.533987 34682 sgd_solver.cpp:112] Iteration 44720, lr = 0.01
I0523 03:52:06.608350 34682 solver.cpp:239] Iteration 44730 (1.80495 iter/s, 5.54032s/10 iters), loss = 7.71267
I0523 03:52:06.608400 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.71267 (* 1 = 7.71267 loss)
I0523 03:52:07.464131 34682 sgd_solver.cpp:112] Iteration 44730, lr = 0.01
I0523 03:52:10.085788 34682 solver.cpp:239] Iteration 44740 (2.87584 iter/s, 3.47724s/10 iters), loss = 9.42254
I0523 03:52:10.085829 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42254 (* 1 = 9.42254 loss)
I0523 03:52:10.915515 34682 sgd_solver.cpp:112] Iteration 44740, lr = 0.01
I0523 03:52:13.455575 34682 solver.cpp:239] Iteration 44750 (2.96771 iter/s, 3.36961s/10 iters), loss = 9.04014
I0523 03:52:13.455621 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04014 (* 1 = 9.04014 loss)
I0523 03:52:13.536916 34682 sgd_solver.cpp:112] Iteration 44750, lr = 0.01
I0523 03:52:19.614950 34682 solver.cpp:239] Iteration 44760 (1.62362 iter/s, 6.15908s/10 iters), loss = 7.90802
I0523 03:52:19.615001 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90802 (* 1 = 7.90802 loss)
I0523 03:52:19.673331 34682 sgd_solver.cpp:112] Iteration 44760, lr = 0.01
I0523 03:52:23.869721 34682 solver.cpp:239] Iteration 44770 (2.35043 iter/s, 4.25454s/10 iters), loss = 8.50866
I0523 03:52:23.869786 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50866 (* 1 = 8.50866 loss)
I0523 03:52:24.611647 34682 sgd_solver.cpp:112] Iteration 44770, lr = 0.01
I0523 03:52:29.329913 34682 solver.cpp:239] Iteration 44780 (1.83154 iter/s, 5.4599s/10 iters), loss = 9.20932
I0523 03:52:29.329959 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20932 (* 1 = 9.20932 loss)
I0523 03:52:29.405967 34682 sgd_solver.cpp:112] Iteration 44780, lr = 0.01
I0523 03:52:33.604018 34682 solver.cpp:239] Iteration 44790 (2.33979 iter/s, 4.27389s/10 iters), loss = 9.20615
I0523 03:52:33.604284 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20615 (* 1 = 9.20615 loss)
I0523 03:52:33.667204 34682 sgd_solver.cpp:112] Iteration 44790, lr = 0.01
I0523 03:52:39.176981 34682 solver.cpp:239] Iteration 44800 (1.79453 iter/s, 5.5725s/10 iters), loss = 7.8407
I0523 03:52:39.177028 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8407 (* 1 = 7.8407 loss)
I0523 03:52:39.293246 34682 sgd_solver.cpp:112] Iteration 44800, lr = 0.01
I0523 03:52:42.755153 34682 solver.cpp:239] Iteration 44810 (2.79489 iter/s, 3.57796s/10 iters), loss = 9.35974
I0523 03:52:42.755208 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35974 (* 1 = 9.35974 loss)
I0523 03:52:42.816063 34682 sgd_solver.cpp:112] Iteration 44810, lr = 0.01
I0523 03:52:47.493628 34682 solver.cpp:239] Iteration 44820 (2.1105 iter/s, 4.73821s/10 iters), loss = 9.5596
I0523 03:52:47.493701 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.5596 (* 1 = 9.5596 loss)
I0523 03:52:48.217378 34682 sgd_solver.cpp:112] Iteration 44820, lr = 0.01
I0523 03:52:53.752789 34682 solver.cpp:239] Iteration 44830 (1.59774 iter/s, 6.25883s/10 iters), loss = 8.20703
I0523 03:52:53.752840 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20703 (* 1 = 8.20703 loss)
I0523 03:52:53.808805 34682 sgd_solver.cpp:112] Iteration 44830, lr = 0.01
I0523 03:52:57.701725 34682 solver.cpp:239] Iteration 44840 (2.53247 iter/s, 3.94872s/10 iters), loss = 8.40732
I0523 03:52:57.701786 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40732 (* 1 = 8.40732 loss)
I0523 03:52:57.775787 34682 sgd_solver.cpp:112] Iteration 44840, lr = 0.01
I0523 03:53:02.398563 34682 solver.cpp:239] Iteration 44850 (2.12921 iter/s, 4.69658s/10 iters), loss = 9.93845
I0523 03:53:02.398619 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.93845 (* 1 = 9.93845 loss)
I0523 03:53:03.189239 34682 sgd_solver.cpp:112] Iteration 44850, lr = 0.01
I0523 03:53:08.199215 34682 solver.cpp:239] Iteration 44860 (1.72403 iter/s, 5.80037s/10 iters), loss = 7.93294
I0523 03:53:08.199496 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93294 (* 1 = 7.93294 loss)
I0523 03:53:08.845825 34682 sgd_solver.cpp:112] Iteration 44860, lr = 0.01
I0523 03:53:14.656225 34682 solver.cpp:239] Iteration 44870 (1.54883 iter/s, 6.4565s/10 iters), loss = 8.14108
I0523 03:53:14.656275 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14108 (* 1 = 8.14108 loss)
I0523 03:53:15.492442 34682 sgd_solver.cpp:112] Iteration 44870, lr = 0.01
I0523 03:53:20.245913 34682 solver.cpp:239] Iteration 44880 (1.7891 iter/s, 5.5894s/10 iters), loss = 8.22687
I0523 03:53:20.245992 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22687 (* 1 = 8.22687 loss)
I0523 03:53:20.798315 34682 sgd_solver.cpp:112] Iteration 44880, lr = 0.01
I0523 03:53:25.037142 34682 solver.cpp:239] Iteration 44890 (2.08727 iter/s, 4.79095s/10 iters), loss = 8.5892
I0523 03:53:25.037197 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5892 (* 1 = 8.5892 loss)
I0523 03:53:25.821094 34682 sgd_solver.cpp:112] Iteration 44890, lr = 0.01
I0523 03:53:30.874634 34682 solver.cpp:239] Iteration 44900 (1.71315 iter/s, 5.8372s/10 iters), loss = 8.54754
I0523 03:53:30.874675 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54754 (* 1 = 8.54754 loss)
I0523 03:53:30.958254 34682 sgd_solver.cpp:112] Iteration 44900, lr = 0.01
I0523 03:53:36.934926 34682 solver.cpp:239] Iteration 44910 (1.65016 iter/s, 6.06s/10 iters), loss = 7.66837
I0523 03:53:36.934980 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66837 (* 1 = 7.66837 loss)
I0523 03:53:37.766729 34682 sgd_solver.cpp:112] Iteration 44910, lr = 0.01
I0523 03:53:41.853916 34682 solver.cpp:239] Iteration 44920 (2.03304 iter/s, 4.91873s/10 iters), loss = 8.50572
I0523 03:53:41.854059 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50572 (* 1 = 8.50572 loss)
I0523 03:53:41.912286 34682 sgd_solver.cpp:112] Iteration 44920, lr = 0.01
I0523 03:53:45.321837 34682 solver.cpp:239] Iteration 44930 (2.88382 iter/s, 3.46762s/10 iters), loss = 9.01849
I0523 03:53:45.321893 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01849 (* 1 = 9.01849 loss)
I0523 03:53:45.397136 34682 sgd_solver.cpp:112] Iteration 44930, lr = 0.01
I0523 03:53:50.116880 34682 solver.cpp:239] Iteration 44940 (2.08559 iter/s, 4.7948s/10 iters), loss = 8.73051
I0523 03:53:50.116921 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73051 (* 1 = 8.73051 loss)
I0523 03:53:50.190469 34682 sgd_solver.cpp:112] Iteration 44940, lr = 0.01
I0523 03:53:55.789463 34682 solver.cpp:239] Iteration 44950 (1.76295 iter/s, 5.6723s/10 iters), loss = 8.16398
I0523 03:53:55.789523 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16398 (* 1 = 8.16398 loss)
I0523 03:53:55.857877 34682 sgd_solver.cpp:112] Iteration 44950, lr = 0.01
I0523 03:54:00.885608 34682 solver.cpp:239] Iteration 44960 (1.96237 iter/s, 5.09587s/10 iters), loss = 9.84531
I0523 03:54:00.885654 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.84531 (* 1 = 9.84531 loss)
I0523 03:54:00.972105 34682 sgd_solver.cpp:112] Iteration 44960, lr = 0.01
I0523 03:54:06.643267 34682 solver.cpp:239] Iteration 44970 (1.7369 iter/s, 5.75737s/10 iters), loss = 8.51851
I0523 03:54:06.643318 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51851 (* 1 = 8.51851 loss)
I0523 03:54:06.716581 34682 sgd_solver.cpp:112] Iteration 44970, lr = 0.01
I0523 03:54:10.159576 34682 solver.cpp:239] Iteration 44980 (2.84405 iter/s, 3.51612s/10 iters), loss = 8.91999
I0523 03:54:10.159616 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91999 (* 1 = 8.91999 loss)
I0523 03:54:10.521016 34682 sgd_solver.cpp:112] Iteration 44980, lr = 0.01
I0523 03:54:16.035475 34682 solver.cpp:239] Iteration 44990 (1.70195 iter/s, 5.87562s/10 iters), loss = 9.04476
I0523 03:54:16.035606 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04476 (* 1 = 9.04476 loss)
I0523 03:54:16.107261 34682 sgd_solver.cpp:112] Iteration 44990, lr = 0.01
I0523 03:54:20.196480 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_45000.caffemodel
I0523 03:54:21.441795 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_45000.solverstate
I0523 03:54:21.645294 34682 solver.cpp:239] Iteration 45000 (1.7827 iter/s, 5.60946s/10 iters), loss = 8.50453
I0523 03:54:21.645339 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50453 (* 1 = 8.50453 loss)
I0523 03:54:21.721370 34682 sgd_solver.cpp:112] Iteration 45000, lr = 0.01
I0523 03:54:27.079193 34682 solver.cpp:239] Iteration 45010 (1.84039 iter/s, 5.43364s/10 iters), loss = 8.55615
I0523 03:54:27.079234 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55615 (* 1 = 8.55615 loss)
I0523 03:54:27.156710 34682 sgd_solver.cpp:112] Iteration 45010, lr = 0.01
I0523 03:54:32.132581 34682 solver.cpp:239] Iteration 45020 (1.97897 iter/s, 5.05314s/10 iters), loss = 8.45679
I0523 03:54:32.132625 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45679 (* 1 = 8.45679 loss)
I0523 03:54:32.192589 34682 sgd_solver.cpp:112] Iteration 45020, lr = 0.01
I0523 03:54:36.336236 34682 solver.cpp:239] Iteration 45030 (2.37901 iter/s, 4.20342s/10 iters), loss = 8.8739
I0523 03:54:36.336292 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8739 (* 1 = 8.8739 loss)
I0523 03:54:36.852149 34682 sgd_solver.cpp:112] Iteration 45030, lr = 0.01
I0523 03:54:41.986659 34682 solver.cpp:239] Iteration 45040 (1.76987 iter/s, 5.65014s/10 iters), loss = 8.9641
I0523 03:54:41.986721 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9641 (* 1 = 8.9641 loss)
I0523 03:54:42.055109 34682 sgd_solver.cpp:112] Iteration 45040, lr = 0.01
I0523 03:54:48.156736 34682 solver.cpp:239] Iteration 45050 (1.62081 iter/s, 6.16976s/10 iters), loss = 8.17121
I0523 03:54:48.157099 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17121 (* 1 = 8.17121 loss)
I0523 03:54:48.238979 34682 sgd_solver.cpp:112] Iteration 45050, lr = 0.01
I0523 03:54:53.118371 34682 solver.cpp:239] Iteration 45060 (2.01568 iter/s, 4.96109s/10 iters), loss = 9.08904
I0523 03:54:53.118427 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08904 (* 1 = 9.08904 loss)
I0523 03:54:53.941761 34682 sgd_solver.cpp:112] Iteration 45060, lr = 0.01
I0523 03:54:58.912643 34682 solver.cpp:239] Iteration 45070 (1.72593 iter/s, 5.79398s/10 iters), loss = 8.79293
I0523 03:54:58.912688 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79293 (* 1 = 8.79293 loss)
I0523 03:54:58.985458 34682 sgd_solver.cpp:112] Iteration 45070, lr = 0.01
I0523 03:55:04.750275 34682 solver.cpp:239] Iteration 45080 (1.71311 iter/s, 5.83735s/10 iters), loss = 8.93429
I0523 03:55:04.750334 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93429 (* 1 = 8.93429 loss)
I0523 03:55:05.417912 34682 sgd_solver.cpp:112] Iteration 45080, lr = 0.01
I0523 03:55:08.750046 34682 solver.cpp:239] Iteration 45090 (2.50029 iter/s, 3.99954s/10 iters), loss = 9.4004
I0523 03:55:08.750088 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4004 (* 1 = 9.4004 loss)
I0523 03:55:09.527710 34682 sgd_solver.cpp:112] Iteration 45090, lr = 0.01
I0523 03:55:13.964857 34682 solver.cpp:239] Iteration 45100 (1.91771 iter/s, 5.21456s/10 iters), loss = 8.29773
I0523 03:55:13.964893 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29773 (* 1 = 8.29773 loss)
I0523 03:55:14.042909 34682 sgd_solver.cpp:112] Iteration 45100, lr = 0.01
I0523 03:55:19.251446 34682 solver.cpp:239] Iteration 45110 (1.89167 iter/s, 5.28633s/10 iters), loss = 9.18176
I0523 03:55:19.251679 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18176 (* 1 = 9.18176 loss)
I0523 03:55:20.124655 34682 sgd_solver.cpp:112] Iteration 45110, lr = 0.01
I0523 03:55:24.911487 34682 solver.cpp:239] Iteration 45120 (1.7669 iter/s, 5.65961s/10 iters), loss = 9.09501
I0523 03:55:24.911546 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09501 (* 1 = 9.09501 loss)
I0523 03:55:25.401852 34682 sgd_solver.cpp:112] Iteration 45120, lr = 0.01
I0523 03:55:30.234683 34682 solver.cpp:239] Iteration 45130 (1.87867 iter/s, 5.32293s/10 iters), loss = 9.60042
I0523 03:55:30.234736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60042 (* 1 = 9.60042 loss)
I0523 03:55:30.312085 34682 sgd_solver.cpp:112] Iteration 45130, lr = 0.01
I0523 03:55:35.026564 34682 solver.cpp:239] Iteration 45140 (2.08698 iter/s, 4.79162s/10 iters), loss = 9.16545
I0523 03:55:35.026618 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16545 (* 1 = 9.16545 loss)
I0523 03:55:35.083766 34682 sgd_solver.cpp:112] Iteration 45140, lr = 0.01
I0523 03:55:37.830950 34682 solver.cpp:239] Iteration 45150 (3.56606 iter/s, 2.80422s/10 iters), loss = 8.04095
I0523 03:55:37.830996 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04095 (* 1 = 8.04095 loss)
I0523 03:55:38.658715 34682 sgd_solver.cpp:112] Iteration 45150, lr = 0.01
I0523 03:55:44.941452 34682 solver.cpp:239] Iteration 45160 (1.40644 iter/s, 7.11016s/10 iters), loss = 8.22413
I0523 03:55:44.941499 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22413 (* 1 = 8.22413 loss)
I0523 03:55:45.010303 34682 sgd_solver.cpp:112] Iteration 45160, lr = 0.01
I0523 03:55:49.836679 34682 solver.cpp:239] Iteration 45170 (2.04291 iter/s, 4.89498s/10 iters), loss = 8.93115
I0523 03:55:49.836870 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93115 (* 1 = 8.93115 loss)
I0523 03:55:49.895351 34682 sgd_solver.cpp:112] Iteration 45170, lr = 0.01
I0523 03:55:54.209437 34682 solver.cpp:239] Iteration 45180 (2.28708 iter/s, 4.3724s/10 iters), loss = 8.25775
I0523 03:55:54.209481 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25775 (* 1 = 8.25775 loss)
I0523 03:55:54.970506 34682 sgd_solver.cpp:112] Iteration 45180, lr = 0.01
I0523 03:55:59.842195 34682 solver.cpp:239] Iteration 45190 (1.77542 iter/s, 5.63248s/10 iters), loss = 8.21095
I0523 03:55:59.842241 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21095 (* 1 = 8.21095 loss)
I0523 03:56:00.673516 34682 sgd_solver.cpp:112] Iteration 45190, lr = 0.01
I0523 03:56:05.377888 34682 solver.cpp:239] Iteration 45200 (1.80655 iter/s, 5.53542s/10 iters), loss = 9.32282
I0523 03:56:05.377935 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32282 (* 1 = 9.32282 loss)
I0523 03:56:05.452669 34682 sgd_solver.cpp:112] Iteration 45200, lr = 0.01
I0523 03:56:09.532353 34682 solver.cpp:239] Iteration 45210 (2.40718 iter/s, 4.15424s/10 iters), loss = 8.74075
I0523 03:56:09.532394 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74075 (* 1 = 8.74075 loss)
I0523 03:56:10.341758 34682 sgd_solver.cpp:112] Iteration 45210, lr = 0.01
I0523 03:56:14.750890 34682 solver.cpp:239] Iteration 45220 (1.91634 iter/s, 5.21828s/10 iters), loss = 8.5366
I0523 03:56:14.750936 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5366 (* 1 = 8.5366 loss)
I0523 03:56:14.833983 34682 sgd_solver.cpp:112] Iteration 45220, lr = 0.01
I0523 03:56:17.990377 34682 solver.cpp:239] Iteration 45230 (3.08709 iter/s, 3.23929s/10 iters), loss = 8.92421
I0523 03:56:17.990447 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92421 (* 1 = 8.92421 loss)
I0523 03:56:18.885525 34682 sgd_solver.cpp:112] Iteration 45230, lr = 0.01
I0523 03:56:22.196028 34682 solver.cpp:239] Iteration 45240 (2.37789 iter/s, 4.20541s/10 iters), loss = 9.36125
I0523 03:56:22.196231 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36125 (* 1 = 9.36125 loss)
I0523 03:56:22.258278 34682 sgd_solver.cpp:112] Iteration 45240, lr = 0.01
I0523 03:56:28.531643 34682 solver.cpp:239] Iteration 45250 (1.57849 iter/s, 6.33518s/10 iters), loss = 9.21763
I0523 03:56:28.531700 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21763 (* 1 = 9.21763 loss)
I0523 03:56:29.166822 34682 sgd_solver.cpp:112] Iteration 45250, lr = 0.01
I0523 03:56:34.696846 34682 solver.cpp:239] Iteration 45260 (1.62209 iter/s, 6.1649s/10 iters), loss = 9.48946
I0523 03:56:34.696894 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.48946 (* 1 = 9.48946 loss)
I0523 03:56:34.773368 34682 sgd_solver.cpp:112] Iteration 45260, lr = 0.01
I0523 03:56:39.458642 34682 solver.cpp:239] Iteration 45270 (2.10016 iter/s, 4.76155s/10 iters), loss = 9.08909
I0523 03:56:39.458685 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08909 (* 1 = 9.08909 loss)
I0523 03:56:39.526737 34682 sgd_solver.cpp:112] Iteration 45270, lr = 0.01
I0523 03:56:42.963560 34682 solver.cpp:239] Iteration 45280 (2.85329 iter/s, 3.50472s/10 iters), loss = 8.94555
I0523 03:56:42.963608 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94555 (* 1 = 8.94555 loss)
I0523 03:56:43.025524 34682 sgd_solver.cpp:112] Iteration 45280, lr = 0.01
I0523 03:56:47.693784 34682 solver.cpp:239] Iteration 45290 (2.11418 iter/s, 4.72998s/10 iters), loss = 8.30001
I0523 03:56:47.693840 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30001 (* 1 = 8.30001 loss)
I0523 03:56:48.517372 34682 sgd_solver.cpp:112] Iteration 45290, lr = 0.01
I0523 03:56:51.995699 34682 solver.cpp:239] Iteration 45300 (2.32467 iter/s, 4.30168s/10 iters), loss = 9.07085
I0523 03:56:51.995838 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07085 (* 1 = 9.07085 loss)
I0523 03:56:52.052207 34682 sgd_solver.cpp:112] Iteration 45300, lr = 0.01
I0523 03:56:56.698309 34682 solver.cpp:239] Iteration 45310 (2.12664 iter/s, 4.70226s/10 iters), loss = 8.01907
I0523 03:56:56.698595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01907 (* 1 = 8.01907 loss)
I0523 03:56:57.517252 34682 sgd_solver.cpp:112] Iteration 45310, lr = 0.01
I0523 03:57:00.890255 34682 solver.cpp:239] Iteration 45320 (2.38576 iter/s, 4.19153s/10 iters), loss = 9.33932
I0523 03:57:00.890298 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33932 (* 1 = 9.33932 loss)
I0523 03:57:00.958518 34682 sgd_solver.cpp:112] Iteration 45320, lr = 0.01
I0523 03:57:05.656392 34682 solver.cpp:239] Iteration 45330 (2.09824 iter/s, 4.76589s/10 iters), loss = 10.1443
I0523 03:57:05.656451 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.1443 (* 1 = 10.1443 loss)
I0523 03:57:05.741353 34682 sgd_solver.cpp:112] Iteration 45330, lr = 0.01
I0523 03:57:11.252934 34682 solver.cpp:239] Iteration 45340 (1.78691 iter/s, 5.59625s/10 iters), loss = 8.41679
I0523 03:57:11.252992 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41679 (* 1 = 8.41679 loss)
I0523 03:57:11.317462 34682 sgd_solver.cpp:112] Iteration 45340, lr = 0.01
I0523 03:57:15.582192 34682 solver.cpp:239] Iteration 45350 (2.30999 iter/s, 4.32902s/10 iters), loss = 8.32325
I0523 03:57:15.582239 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32325 (* 1 = 8.32325 loss)
I0523 03:57:16.444613 34682 sgd_solver.cpp:112] Iteration 45350, lr = 0.01
I0523 03:57:22.602161 34682 solver.cpp:239] Iteration 45360 (1.42458 iter/s, 7.01963s/10 iters), loss = 8.92706
I0523 03:57:22.602219 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92706 (* 1 = 8.92706 loss)
I0523 03:57:23.379889 34682 sgd_solver.cpp:112] Iteration 45360, lr = 0.01
I0523 03:57:27.683481 34682 solver.cpp:239] Iteration 45370 (1.9681 iter/s, 5.08105s/10 iters), loss = 8.94762
I0523 03:57:27.683660 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94762 (* 1 = 8.94762 loss)
I0523 03:57:27.756830 34682 sgd_solver.cpp:112] Iteration 45370, lr = 0.01
I0523 03:57:31.852979 34682 solver.cpp:239] Iteration 45380 (2.39857 iter/s, 4.16915s/10 iters), loss = 8.64789
I0523 03:57:31.853044 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64789 (* 1 = 8.64789 loss)
I0523 03:57:32.679215 34682 sgd_solver.cpp:112] Iteration 45380, lr = 0.01
I0523 03:57:38.251139 34682 solver.cpp:239] Iteration 45390 (1.56303 iter/s, 6.39784s/10 iters), loss = 8.93985
I0523 03:57:38.251197 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93985 (* 1 = 8.93985 loss)
I0523 03:57:38.850049 34682 sgd_solver.cpp:112] Iteration 45390, lr = 0.01
I0523 03:57:44.226524 34682 solver.cpp:239] Iteration 45400 (1.67362 iter/s, 5.97509s/10 iters), loss = 7.95605
I0523 03:57:44.226573 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95605 (* 1 = 7.95605 loss)
I0523 03:57:45.066751 34682 sgd_solver.cpp:112] Iteration 45400, lr = 0.01
I0523 03:57:49.884840 34682 solver.cpp:239] Iteration 45410 (1.7674 iter/s, 5.65803s/10 iters), loss = 8.8838
I0523 03:57:49.884898 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8838 (* 1 = 8.8838 loss)
I0523 03:57:50.682994 34682 sgd_solver.cpp:112] Iteration 45410, lr = 0.01
I0523 03:57:55.189498 34682 solver.cpp:239] Iteration 45420 (1.88524 iter/s, 5.30437s/10 iters), loss = 8.89001
I0523 03:57:55.189553 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89001 (* 1 = 8.89001 loss)
I0523 03:57:55.931783 34682 sgd_solver.cpp:112] Iteration 45420, lr = 0.01
I0523 03:58:00.174734 34682 solver.cpp:239] Iteration 45430 (2.00603 iter/s, 4.98496s/10 iters), loss = 8.50354
I0523 03:58:00.174911 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50354 (* 1 = 8.50354 loss)
I0523 03:58:00.242069 34682 sgd_solver.cpp:112] Iteration 45430, lr = 0.01
I0523 03:58:04.963932 34682 solver.cpp:239] Iteration 45440 (2.08819 iter/s, 4.78883s/10 iters), loss = 8.91772
I0523 03:58:04.963992 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91772 (* 1 = 8.91772 loss)
I0523 03:58:05.033769 34682 sgd_solver.cpp:112] Iteration 45440, lr = 0.01
I0523 03:58:10.074388 34682 solver.cpp:239] Iteration 45450 (1.95688 iter/s, 5.11018s/10 iters), loss = 9.27157
I0523 03:58:10.074450 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27157 (* 1 = 9.27157 loss)
I0523 03:58:10.881045 34682 sgd_solver.cpp:112] Iteration 45450, lr = 0.01
I0523 03:58:15.860348 34682 solver.cpp:239] Iteration 45460 (1.72841 iter/s, 5.78566s/10 iters), loss = 9.04753
I0523 03:58:15.860404 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04753 (* 1 = 9.04753 loss)
I0523 03:58:16.492964 34682 sgd_solver.cpp:112] Iteration 45460, lr = 0.01
I0523 03:58:19.784579 34682 solver.cpp:239] Iteration 45470 (2.54841 iter/s, 3.92401s/10 iters), loss = 8.46775
I0523 03:58:19.784627 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46775 (* 1 = 8.46775 loss)
I0523 03:58:19.840740 34682 sgd_solver.cpp:112] Iteration 45470, lr = 0.01
I0523 03:58:22.476662 34682 solver.cpp:239] Iteration 45480 (3.71483 iter/s, 2.69191s/10 iters), loss = 9.16358
I0523 03:58:22.476712 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16358 (* 1 = 9.16358 loss)
I0523 03:58:23.206099 34682 sgd_solver.cpp:112] Iteration 45480, lr = 0.01
I0523 03:58:28.187830 34682 solver.cpp:239] Iteration 45490 (1.75104 iter/s, 5.71088s/10 iters), loss = 9.04583
I0523 03:58:28.187893 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04583 (* 1 = 9.04583 loss)
I0523 03:58:28.923117 34682 sgd_solver.cpp:112] Iteration 45490, lr = 0.01
I0523 03:58:32.318931 34682 solver.cpp:239] Iteration 45500 (2.4208 iter/s, 4.13087s/10 iters), loss = 9.13441
I0523 03:58:32.319129 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13441 (* 1 = 9.13441 loss)
I0523 03:58:32.383580 34682 sgd_solver.cpp:112] Iteration 45500, lr = 0.01
I0523 03:58:38.820538 34682 solver.cpp:239] Iteration 45510 (1.53818 iter/s, 6.50118s/10 iters), loss = 8.15044
I0523 03:58:38.820590 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15044 (* 1 = 8.15044 loss)
I0523 03:58:38.893980 34682 sgd_solver.cpp:112] Iteration 45510, lr = 0.01
I0523 03:58:44.550518 34682 solver.cpp:239] Iteration 45520 (1.74529 iter/s, 5.72969s/10 iters), loss = 9.42805
I0523 03:58:44.550568 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42805 (* 1 = 9.42805 loss)
I0523 03:58:44.638937 34682 sgd_solver.cpp:112] Iteration 45520, lr = 0.01
I0523 03:58:48.976079 34682 solver.cpp:239] Iteration 45530 (2.25972 iter/s, 4.42533s/10 iters), loss = 8.29132
I0523 03:58:48.976131 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29132 (* 1 = 8.29132 loss)
I0523 03:58:49.049309 34682 sgd_solver.cpp:112] Iteration 45530, lr = 0.01
I0523 03:58:53.247355 34682 solver.cpp:239] Iteration 45540 (2.34135 iter/s, 4.27104s/10 iters), loss = 8.90736
I0523 03:58:53.247416 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90736 (* 1 = 8.90736 loss)
I0523 03:58:54.032474 34682 sgd_solver.cpp:112] Iteration 45540, lr = 0.01
I0523 03:58:58.062788 34682 solver.cpp:239] Iteration 45550 (2.07677 iter/s, 4.81518s/10 iters), loss = 8.96678
I0523 03:58:58.062830 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96678 (* 1 = 8.96678 loss)
I0523 03:58:58.135854 34682 sgd_solver.cpp:112] Iteration 45550, lr = 0.01
I0523 03:59:02.400753 34682 solver.cpp:239] Iteration 45560 (2.30652 iter/s, 4.33554s/10 iters), loss = 9.30125
I0523 03:59:02.400943 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30125 (* 1 = 9.30125 loss)
I0523 03:59:03.217408 34682 sgd_solver.cpp:112] Iteration 45560, lr = 0.01
I0523 03:59:07.561777 34682 solver.cpp:239] Iteration 45570 (1.93775 iter/s, 5.16062s/10 iters), loss = 8.07782
I0523 03:59:07.561830 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07782 (* 1 = 8.07782 loss)
I0523 03:59:07.631624 34682 sgd_solver.cpp:112] Iteration 45570, lr = 0.01
I0523 03:59:12.552511 34682 solver.cpp:239] Iteration 45580 (2.00382 iter/s, 4.99048s/10 iters), loss = 8.61044
I0523 03:59:12.552559 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61044 (* 1 = 8.61044 loss)
I0523 03:59:12.627372 34682 sgd_solver.cpp:112] Iteration 45580, lr = 0.01
I0523 03:59:17.385440 34682 solver.cpp:239] Iteration 45590 (2.06925 iter/s, 4.83268s/10 iters), loss = 8.1625
I0523 03:59:17.385493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1625 (* 1 = 8.1625 loss)
I0523 03:59:18.189481 34682 sgd_solver.cpp:112] Iteration 45590, lr = 0.01
I0523 03:59:23.806046 34682 solver.cpp:239] Iteration 45600 (1.55756 iter/s, 6.42029s/10 iters), loss = 8.18631
I0523 03:59:23.806103 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18631 (* 1 = 8.18631 loss)
I0523 03:59:23.869316 34682 sgd_solver.cpp:112] Iteration 45600, lr = 0.01
I0523 03:59:28.698112 34682 solver.cpp:239] Iteration 45610 (2.04424 iter/s, 4.8918s/10 iters), loss = 9.03955
I0523 03:59:28.698175 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03955 (* 1 = 9.03955 loss)
I0523 03:59:29.294176 34682 sgd_solver.cpp:112] Iteration 45610, lr = 0.01
I0523 03:59:34.113148 34682 solver.cpp:239] Iteration 45620 (1.84681 iter/s, 5.41475s/10 iters), loss = 9.41678
I0523 03:59:34.113394 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.41678 (* 1 = 9.41678 loss)
I0523 03:59:34.475363 34682 sgd_solver.cpp:112] Iteration 45620, lr = 0.01
I0523 03:59:38.433076 34682 solver.cpp:239] Iteration 45630 (2.31506 iter/s, 4.31954s/10 iters), loss = 8.29398
I0523 03:59:38.433117 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29398 (* 1 = 8.29398 loss)
I0523 03:59:38.517671 34682 sgd_solver.cpp:112] Iteration 45630, lr = 0.01
I0523 03:59:44.163204 34682 solver.cpp:239] Iteration 45640 (1.74524 iter/s, 5.72986s/10 iters), loss = 8.78263
I0523 03:59:44.163252 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78263 (* 1 = 8.78263 loss)
I0523 03:59:44.233016 34682 sgd_solver.cpp:112] Iteration 45640, lr = 0.01
I0523 03:59:49.102846 34682 solver.cpp:239] Iteration 45650 (2.02455 iter/s, 4.93938s/10 iters), loss = 9.39822
I0523 03:59:49.102905 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.39822 (* 1 = 9.39822 loss)
I0523 03:59:49.941764 34682 sgd_solver.cpp:112] Iteration 45650, lr = 0.01
I0523 03:59:53.887471 34682 solver.cpp:239] Iteration 45660 (2.09014 iter/s, 4.78438s/10 iters), loss = 8.75258
I0523 03:59:53.887519 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75258 (* 1 = 8.75258 loss)
I0523 03:59:54.730163 34682 sgd_solver.cpp:112] Iteration 45660, lr = 0.01
I0523 03:59:59.547662 34682 solver.cpp:239] Iteration 45670 (1.76682 iter/s, 5.6599s/10 iters), loss = 9.31026
I0523 03:59:59.547729 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31026 (* 1 = 9.31026 loss)
I0523 03:59:59.617987 34682 sgd_solver.cpp:112] Iteration 45670, lr = 0.01
I0523 04:00:04.679755 34682 solver.cpp:239] Iteration 45680 (1.94863 iter/s, 5.13182s/10 iters), loss = 8.66401
I0523 04:00:04.680006 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66401 (* 1 = 8.66401 loss)
I0523 04:00:05.516783 34682 sgd_solver.cpp:112] Iteration 45680, lr = 0.01
I0523 04:00:07.356294 34682 solver.cpp:239] Iteration 45690 (3.73663 iter/s, 2.67621s/10 iters), loss = 8.96423
I0523 04:00:07.356354 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96423 (* 1 = 8.96423 loss)
I0523 04:00:08.175194 34682 sgd_solver.cpp:112] Iteration 45690, lr = 0.01
I0523 04:00:12.425789 34682 solver.cpp:239] Iteration 45700 (1.97269 iter/s, 5.06922s/10 iters), loss = 9.59615
I0523 04:00:12.425851 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59615 (* 1 = 9.59615 loss)
I0523 04:00:13.183440 34682 sgd_solver.cpp:112] Iteration 45700, lr = 0.01
I0523 04:00:19.257845 34682 solver.cpp:239] Iteration 45710 (1.46376 iter/s, 6.8317s/10 iters), loss = 7.86815
I0523 04:00:19.257896 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86815 (* 1 = 7.86815 loss)
I0523 04:00:20.087548 34682 sgd_solver.cpp:112] Iteration 45710, lr = 0.01
I0523 04:00:26.691501 34682 solver.cpp:239] Iteration 45720 (1.3453 iter/s, 7.43331s/10 iters), loss = 8.29977
I0523 04:00:26.691541 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29977 (* 1 = 8.29977 loss)
I0523 04:00:26.757932 34682 sgd_solver.cpp:112] Iteration 45720, lr = 0.01
I0523 04:00:28.549970 34682 solver.cpp:239] Iteration 45730 (5.38113 iter/s, 1.85834s/10 iters), loss = 9.1306
I0523 04:00:28.550010 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1306 (* 1 = 9.1306 loss)
I0523 04:00:28.615686 34682 sgd_solver.cpp:112] Iteration 45730, lr = 0.01
I0523 04:00:33.299870 34682 solver.cpp:239] Iteration 45740 (2.10542 iter/s, 4.74966s/10 iters), loss = 9.03961
I0523 04:00:33.299924 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03961 (* 1 = 9.03961 loss)
I0523 04:00:33.358515 34682 sgd_solver.cpp:112] Iteration 45740, lr = 0.01
I0523 04:00:38.027166 34682 solver.cpp:239] Iteration 45750 (2.11549 iter/s, 4.72705s/10 iters), loss = 9.29422
I0523 04:00:38.027392 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29422 (* 1 = 9.29422 loss)
I0523 04:00:38.095863 34682 sgd_solver.cpp:112] Iteration 45750, lr = 0.01
I0523 04:00:42.940927 34682 solver.cpp:239] Iteration 45760 (2.03526 iter/s, 4.91337s/10 iters), loss = 8.66188
I0523 04:00:42.940984 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66188 (* 1 = 8.66188 loss)
I0523 04:00:43.774551 34682 sgd_solver.cpp:112] Iteration 45760, lr = 0.01
I0523 04:00:47.867353 34682 solver.cpp:239] Iteration 45770 (2.02998 iter/s, 4.92617s/10 iters), loss = 8.45924
I0523 04:00:47.867421 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45924 (* 1 = 8.45924 loss)
I0523 04:00:47.983405 34682 sgd_solver.cpp:112] Iteration 45770, lr = 0.01
I0523 04:00:53.050011 34682 solver.cpp:239] Iteration 45780 (1.92962 iter/s, 5.18237s/10 iters), loss = 8.67313
I0523 04:00:53.050079 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67313 (* 1 = 8.67313 loss)
I0523 04:00:53.907980 34682 sgd_solver.cpp:112] Iteration 45780, lr = 0.01
I0523 04:00:58.521126 34682 solver.cpp:239] Iteration 45790 (1.82788 iter/s, 5.47083s/10 iters), loss = 9.95495
I0523 04:00:58.521180 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.95495 (* 1 = 9.95495 loss)
I0523 04:00:59.317975 34682 sgd_solver.cpp:112] Iteration 45790, lr = 0.01
I0523 04:01:04.146347 34682 solver.cpp:239] Iteration 45800 (1.77781 iter/s, 5.62491s/10 iters), loss = 8.47774
I0523 04:01:04.146389 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47774 (* 1 = 8.47774 loss)
I0523 04:01:04.221724 34682 sgd_solver.cpp:112] Iteration 45800, lr = 0.01
I0523 04:01:08.458878 34682 solver.cpp:239] Iteration 45810 (2.31894 iter/s, 4.31231s/10 iters), loss = 8.42246
I0523 04:01:08.459111 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42246 (* 1 = 8.42246 loss)
I0523 04:01:08.539558 34682 sgd_solver.cpp:112] Iteration 45810, lr = 0.01
I0523 04:01:14.094481 34682 solver.cpp:239] Iteration 45820 (1.77457 iter/s, 5.63516s/10 iters), loss = 8.44573
I0523 04:01:14.094535 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44573 (* 1 = 8.44573 loss)
I0523 04:01:14.916942 34682 sgd_solver.cpp:112] Iteration 45820, lr = 0.01
I0523 04:01:19.111837 34682 solver.cpp:239] Iteration 45830 (1.99319 iter/s, 5.01709s/10 iters), loss = 8.72903
I0523 04:01:19.111899 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72903 (* 1 = 8.72903 loss)
I0523 04:01:19.959228 34682 sgd_solver.cpp:112] Iteration 45830, lr = 0.01
I0523 04:01:25.543550 34682 solver.cpp:239] Iteration 45840 (1.55487 iter/s, 6.43139s/10 iters), loss = 8.96657
I0523 04:01:25.543602 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96657 (* 1 = 8.96657 loss)
I0523 04:01:25.614550 34682 sgd_solver.cpp:112] Iteration 45840, lr = 0.01
I0523 04:01:30.168160 34682 solver.cpp:239] Iteration 45850 (2.16246 iter/s, 4.62437s/10 iters), loss = 9.25421
I0523 04:01:30.168205 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25421 (* 1 = 9.25421 loss)
I0523 04:01:30.910852 34682 sgd_solver.cpp:112] Iteration 45850, lr = 0.01
I0523 04:01:35.662176 34682 solver.cpp:239] Iteration 45860 (1.82026 iter/s, 5.49373s/10 iters), loss = 8.89494
I0523 04:01:35.662232 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89494 (* 1 = 8.89494 loss)
I0523 04:01:35.730475 34682 sgd_solver.cpp:112] Iteration 45860, lr = 0.01
I0523 04:01:40.673440 34682 solver.cpp:239] Iteration 45870 (1.99736 iter/s, 5.00661s/10 iters), loss = 9.00586
I0523 04:01:40.673652 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00586 (* 1 = 9.00586 loss)
I0523 04:01:40.737438 34682 sgd_solver.cpp:112] Iteration 45870, lr = 0.01
I0523 04:01:46.133297 34682 solver.cpp:239] Iteration 45880 (1.83169 iter/s, 5.45945s/10 iters), loss = 8.22528
I0523 04:01:46.133337 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22528 (* 1 = 8.22528 loss)
I0523 04:01:46.215854 34682 sgd_solver.cpp:112] Iteration 45880, lr = 0.01
I0523 04:01:51.994819 34682 solver.cpp:239] Iteration 45890 (1.70612 iter/s, 5.86124s/10 iters), loss = 9.63356
I0523 04:01:51.994863 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.63356 (* 1 = 9.63356 loss)
I0523 04:01:52.067883 34682 sgd_solver.cpp:112] Iteration 45890, lr = 0.01
I0523 04:01:57.866154 34682 solver.cpp:239] Iteration 45900 (1.70327 iter/s, 5.87105s/10 iters), loss = 8.78382
I0523 04:01:57.866204 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78382 (* 1 = 8.78382 loss)
I0523 04:01:57.942366 34682 sgd_solver.cpp:112] Iteration 45900, lr = 0.01
I0523 04:02:03.961453 34682 solver.cpp:239] Iteration 45910 (1.64069 iter/s, 6.095s/10 iters), loss = 8.68355
I0523 04:02:03.961510 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68355 (* 1 = 8.68355 loss)
I0523 04:02:04.681447 34682 sgd_solver.cpp:112] Iteration 45910, lr = 0.01
I0523 04:02:10.989233 34682 solver.cpp:239] Iteration 45920 (1.423 iter/s, 7.02743s/10 iters), loss = 9.694
I0523 04:02:10.989382 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.694 (* 1 = 9.694 loss)
I0523 04:02:11.809319 34682 sgd_solver.cpp:112] Iteration 45920, lr = 0.01
I0523 04:02:17.262420 34682 solver.cpp:239] Iteration 45930 (1.59418 iter/s, 6.2728s/10 iters), loss = 9.23066
I0523 04:02:17.262468 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23066 (* 1 = 9.23066 loss)
I0523 04:02:18.021584 34682 sgd_solver.cpp:112] Iteration 45930, lr = 0.01
I0523 04:02:22.760890 34682 solver.cpp:239] Iteration 45940 (1.81878 iter/s, 5.4982s/10 iters), loss = 8.93683
I0523 04:02:22.760937 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93683 (* 1 = 8.93683 loss)
I0523 04:02:23.587244 34682 sgd_solver.cpp:112] Iteration 45940, lr = 0.01
I0523 04:02:27.265575 34682 solver.cpp:239] Iteration 45950 (2.22003 iter/s, 4.50445s/10 iters), loss = 9.01077
I0523 04:02:27.265635 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01077 (* 1 = 9.01077 loss)
I0523 04:02:27.979216 34682 sgd_solver.cpp:112] Iteration 45950, lr = 0.01
I0523 04:02:30.771374 34682 solver.cpp:239] Iteration 45960 (2.85259 iter/s, 3.50559s/10 iters), loss = 9.32064
I0523 04:02:30.771423 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32064 (* 1 = 9.32064 loss)
I0523 04:02:30.848574 34682 sgd_solver.cpp:112] Iteration 45960, lr = 0.01
I0523 04:02:34.189340 34682 solver.cpp:239] Iteration 45970 (2.92588 iter/s, 3.41778s/10 iters), loss = 8.01917
I0523 04:02:34.189386 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01917 (* 1 = 8.01917 loss)
I0523 04:02:34.838007 34682 sgd_solver.cpp:112] Iteration 45970, lr = 0.01
I0523 04:02:39.829391 34682 solver.cpp:239] Iteration 45980 (1.77312 iter/s, 5.63977s/10 iters), loss = 8.65339
I0523 04:02:39.829452 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65339 (* 1 = 8.65339 loss)
I0523 04:02:40.515573 34682 sgd_solver.cpp:112] Iteration 45980, lr = 0.01
I0523 04:02:44.583487 34682 solver.cpp:239] Iteration 45990 (2.10356 iter/s, 4.75384s/10 iters), loss = 9.20202
I0523 04:02:44.583750 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20202 (* 1 = 9.20202 loss)
I0523 04:02:44.659776 34682 sgd_solver.cpp:112] Iteration 45990, lr = 0.01
I0523 04:02:50.155671 34682 solver.cpp:239] Iteration 46000 (1.79478 iter/s, 5.57172s/10 iters), loss = 9.19098
I0523 04:02:50.155728 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19098 (* 1 = 9.19098 loss)
I0523 04:02:50.218984 34682 sgd_solver.cpp:112] Iteration 46000, lr = 0.01
I0523 04:02:53.950018 34682 solver.cpp:239] Iteration 46010 (2.63565 iter/s, 3.79413s/10 iters), loss = 8.62774
I0523 04:02:53.950074 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62774 (* 1 = 8.62774 loss)
I0523 04:02:54.805037 34682 sgd_solver.cpp:112] Iteration 46010, lr = 0.01
I0523 04:02:57.388528 34682 solver.cpp:239] Iteration 46020 (2.9084 iter/s, 3.43832s/10 iters), loss = 8.8972
I0523 04:02:57.388569 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8972 (* 1 = 8.8972 loss)
I0523 04:02:57.450546 34682 sgd_solver.cpp:112] Iteration 46020, lr = 0.01
I0523 04:03:03.736382 34682 solver.cpp:239] Iteration 46030 (1.57541 iter/s, 6.34755s/10 iters), loss = 8.2914
I0523 04:03:03.736429 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2914 (* 1 = 8.2914 loss)
I0523 04:03:03.795132 34682 sgd_solver.cpp:112] Iteration 46030, lr = 0.01
I0523 04:03:07.196955 34682 solver.cpp:239] Iteration 46040 (2.88986 iter/s, 3.46037s/10 iters), loss = 8.35778
I0523 04:03:07.197006 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35778 (* 1 = 8.35778 loss)
I0523 04:03:07.331960 34682 sgd_solver.cpp:112] Iteration 46040, lr = 0.01
I0523 04:03:14.259035 34682 solver.cpp:239] Iteration 46050 (1.41608 iter/s, 7.06174s/10 iters), loss = 9.46558
I0523 04:03:14.259088 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.46558 (* 1 = 9.46558 loss)
I0523 04:03:14.323096 34682 sgd_solver.cpp:112] Iteration 46050, lr = 0.01
I0523 04:03:18.250336 34682 solver.cpp:239] Iteration 46060 (2.50559 iter/s, 3.99108s/10 iters), loss = 8.2404
I0523 04:03:18.250438 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2404 (* 1 = 8.2404 loss)
I0523 04:03:18.317562 34682 sgd_solver.cpp:112] Iteration 46060, lr = 0.01
I0523 04:03:21.504824 34682 solver.cpp:239] Iteration 46070 (3.07292 iter/s, 3.25424s/10 iters), loss = 7.81124
I0523 04:03:21.504873 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81124 (* 1 = 7.81124 loss)
I0523 04:03:21.566440 34682 sgd_solver.cpp:112] Iteration 46070, lr = 0.01
I0523 04:03:25.819100 34682 solver.cpp:239] Iteration 46080 (2.31801 iter/s, 4.31405s/10 iters), loss = 8.72104
I0523 04:03:25.819144 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72104 (* 1 = 8.72104 loss)
I0523 04:03:25.974297 34682 sgd_solver.cpp:112] Iteration 46080, lr = 0.01
I0523 04:03:28.988179 34682 solver.cpp:239] Iteration 46090 (3.15567 iter/s, 3.1689s/10 iters), loss = 7.95155
I0523 04:03:28.988225 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95155 (* 1 = 7.95155 loss)
I0523 04:03:29.053714 34682 sgd_solver.cpp:112] Iteration 46090, lr = 0.01
I0523 04:03:34.798271 34682 solver.cpp:239] Iteration 46100 (1.72123 iter/s, 5.8098s/10 iters), loss = 9.2304
I0523 04:03:34.798321 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2304 (* 1 = 9.2304 loss)
I0523 04:03:34.869244 34682 sgd_solver.cpp:112] Iteration 46100, lr = 0.01
I0523 04:03:39.028993 34682 solver.cpp:239] Iteration 46110 (2.36379 iter/s, 4.2305s/10 iters), loss = 9.30643
I0523 04:03:39.029033 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30643 (* 1 = 9.30643 loss)
I0523 04:03:39.110513 34682 sgd_solver.cpp:112] Iteration 46110, lr = 0.01
I0523 04:03:43.182826 34682 solver.cpp:239] Iteration 46120 (2.40755 iter/s, 4.1536s/10 iters), loss = 8.70563
I0523 04:03:43.182900 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70563 (* 1 = 8.70563 loss)
I0523 04:03:43.638249 34682 sgd_solver.cpp:112] Iteration 46120, lr = 0.01
I0523 04:03:49.067237 34682 solver.cpp:239] Iteration 46130 (1.69949 iter/s, 5.88411s/10 iters), loss = 9.32367
I0523 04:03:49.067502 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32367 (* 1 = 9.32367 loss)
I0523 04:03:49.142271 34682 sgd_solver.cpp:112] Iteration 46130, lr = 0.01
I0523 04:03:53.316510 34682 solver.cpp:239] Iteration 46140 (2.35357 iter/s, 4.24886s/10 iters), loss = 8.26831
I0523 04:03:53.316558 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26831 (* 1 = 8.26831 loss)
I0523 04:03:53.381646 34682 sgd_solver.cpp:112] Iteration 46140, lr = 0.01
I0523 04:03:58.140880 34682 solver.cpp:239] Iteration 46150 (2.07292 iter/s, 4.82412s/10 iters), loss = 8.82532
I0523 04:03:58.140945 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82532 (* 1 = 8.82532 loss)
I0523 04:03:58.908291 34682 sgd_solver.cpp:112] Iteration 46150, lr = 0.01
I0523 04:04:03.811334 34682 solver.cpp:239] Iteration 46160 (1.76362 iter/s, 5.67016s/10 iters), loss = 8.88965
I0523 04:04:03.811388 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88965 (* 1 = 8.88965 loss)
I0523 04:04:04.702988 34682 sgd_solver.cpp:112] Iteration 46160, lr = 0.01
I0523 04:04:08.782356 34682 solver.cpp:239] Iteration 46170 (2.01176 iter/s, 4.97077s/10 iters), loss = 8.3341
I0523 04:04:08.782409 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3341 (* 1 = 8.3341 loss)
I0523 04:04:08.839429 34682 sgd_solver.cpp:112] Iteration 46170, lr = 0.01
I0523 04:04:14.430411 34682 solver.cpp:239] Iteration 46180 (1.77061 iter/s, 5.64777s/10 iters), loss = 8.64424
I0523 04:04:14.430452 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64424 (* 1 = 8.64424 loss)
I0523 04:04:14.503406 34682 sgd_solver.cpp:112] Iteration 46180, lr = 0.01
I0523 04:04:18.258538 34682 solver.cpp:239] Iteration 46190 (2.61238 iter/s, 3.82793s/10 iters), loss = 8.623
I0523 04:04:18.258584 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.623 (* 1 = 8.623 loss)
I0523 04:04:18.332412 34682 sgd_solver.cpp:112] Iteration 46190, lr = 0.01
I0523 04:04:22.470479 34682 solver.cpp:239] Iteration 46200 (2.37433 iter/s, 4.21172s/10 iters), loss = 9.01985
I0523 04:04:22.470683 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01985 (* 1 = 9.01985 loss)
I0523 04:04:22.547106 34682 sgd_solver.cpp:112] Iteration 46200, lr = 0.01
I0523 04:04:26.709220 34682 solver.cpp:239] Iteration 46210 (2.35939 iter/s, 4.23839s/10 iters), loss = 8.19122
I0523 04:04:26.709266 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19122 (* 1 = 8.19122 loss)
I0523 04:04:27.475332 34682 sgd_solver.cpp:112] Iteration 46210, lr = 0.01
I0523 04:04:32.029359 34682 solver.cpp:239] Iteration 46220 (1.87974 iter/s, 5.31987s/10 iters), loss = 8.30217
I0523 04:04:32.029415 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30217 (* 1 = 8.30217 loss)
I0523 04:04:32.563016 34682 sgd_solver.cpp:112] Iteration 46220, lr = 0.01
I0523 04:04:34.931823 34682 solver.cpp:239] Iteration 46230 (3.44558 iter/s, 2.90227s/10 iters), loss = 8.08714
I0523 04:04:34.931886 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08714 (* 1 = 8.08714 loss)
I0523 04:04:35.296123 34682 sgd_solver.cpp:112] Iteration 46230, lr = 0.01
I0523 04:04:40.898247 34682 solver.cpp:239] Iteration 46240 (1.67613 iter/s, 5.96612s/10 iters), loss = 8.77508
I0523 04:04:40.898303 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77508 (* 1 = 8.77508 loss)
I0523 04:04:41.641841 34682 sgd_solver.cpp:112] Iteration 46240, lr = 0.01
I0523 04:04:46.137830 34682 solver.cpp:239] Iteration 46250 (1.90865 iter/s, 5.2393s/10 iters), loss = 9.9873
I0523 04:04:46.137889 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.9873 (* 1 = 9.9873 loss)
I0523 04:04:46.970340 34682 sgd_solver.cpp:112] Iteration 46250, lr = 0.01
I0523 04:04:50.638314 34682 solver.cpp:239] Iteration 46260 (2.22211 iter/s, 4.50024s/10 iters), loss = 9.16844
I0523 04:04:50.638370 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16844 (* 1 = 9.16844 loss)
I0523 04:04:50.701524 34682 sgd_solver.cpp:112] Iteration 46260, lr = 0.01
I0523 04:04:54.275956 34682 solver.cpp:239] Iteration 46270 (2.74919 iter/s, 3.63744s/10 iters), loss = 8.14362
I0523 04:04:54.276139 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14362 (* 1 = 8.14362 loss)
I0523 04:04:54.737412 34682 sgd_solver.cpp:112] Iteration 46270, lr = 0.01
I0523 04:04:58.708317 34682 solver.cpp:239] Iteration 46280 (2.25632 iter/s, 4.43199s/10 iters), loss = 9.16456
I0523 04:04:58.708380 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16456 (* 1 = 9.16456 loss)
I0523 04:04:59.404635 34682 sgd_solver.cpp:112] Iteration 46280, lr = 0.01
I0523 04:05:03.297185 34682 solver.cpp:239] Iteration 46290 (2.1793 iter/s, 4.58862s/10 iters), loss = 8.8371
I0523 04:05:03.297233 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8371 (* 1 = 8.8371 loss)
I0523 04:05:03.370402 34682 sgd_solver.cpp:112] Iteration 46290, lr = 0.01
I0523 04:05:08.110167 34682 solver.cpp:239] Iteration 46300 (2.07782 iter/s, 4.81273s/10 iters), loss = 7.73355
I0523 04:05:08.110224 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.73355 (* 1 = 7.73355 loss)
I0523 04:05:08.940423 34682 sgd_solver.cpp:112] Iteration 46300, lr = 0.01
I0523 04:05:13.920691 34682 solver.cpp:239] Iteration 46310 (1.7211 iter/s, 5.81023s/10 iters), loss = 8.52832
I0523 04:05:13.920743 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52832 (* 1 = 8.52832 loss)
I0523 04:05:13.984593 34682 sgd_solver.cpp:112] Iteration 46310, lr = 0.01
I0523 04:05:17.721961 34682 solver.cpp:239] Iteration 46320 (2.63086 iter/s, 3.80103s/10 iters), loss = 8.6623
I0523 04:05:17.722024 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6623 (* 1 = 8.6623 loss)
I0523 04:05:18.213634 34682 sgd_solver.cpp:112] Iteration 46320, lr = 0.01
I0523 04:05:23.012893 34682 solver.cpp:239] Iteration 46330 (1.89014 iter/s, 5.29062s/10 iters), loss = 8.37045
I0523 04:05:23.012964 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37045 (* 1 = 8.37045 loss)
I0523 04:05:23.082471 34682 sgd_solver.cpp:112] Iteration 46330, lr = 0.01
I0523 04:05:26.475703 34682 solver.cpp:239] Iteration 46340 (2.88801 iter/s, 3.46259s/10 iters), loss = 8.77304
I0523 04:05:26.475946 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77304 (* 1 = 8.77304 loss)
I0523 04:05:27.115432 34682 sgd_solver.cpp:112] Iteration 46340, lr = 0.01
I0523 04:05:31.189600 34682 solver.cpp:239] Iteration 46350 (2.12158 iter/s, 4.71347s/10 iters), loss = 8.60367
I0523 04:05:31.189677 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60367 (* 1 = 8.60367 loss)
I0523 04:05:32.050635 34682 sgd_solver.cpp:112] Iteration 46350, lr = 0.01
I0523 04:05:34.737995 34682 solver.cpp:239] Iteration 46360 (2.81835 iter/s, 3.54817s/10 iters), loss = 8.64031
I0523 04:05:34.738047 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64031 (* 1 = 8.64031 loss)
I0523 04:05:35.488152 34682 sgd_solver.cpp:112] Iteration 46360, lr = 0.01
I0523 04:05:40.288125 34682 solver.cpp:239] Iteration 46370 (1.80185 iter/s, 5.54985s/10 iters), loss = 8.39311
I0523 04:05:40.288170 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39311 (* 1 = 8.39311 loss)
I0523 04:05:40.366025 34682 sgd_solver.cpp:112] Iteration 46370, lr = 0.01
I0523 04:05:44.787045 34682 solver.cpp:239] Iteration 46380 (2.22287 iter/s, 4.49869s/10 iters), loss = 8.81563
I0523 04:05:44.787093 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81563 (* 1 = 8.81563 loss)
I0523 04:05:44.858659 34682 sgd_solver.cpp:112] Iteration 46380, lr = 0.01
I0523 04:05:46.050787 34682 solver.cpp:239] Iteration 46390 (7.91369 iter/s, 1.26363s/10 iters), loss = 8.95394
I0523 04:05:46.050833 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95394 (* 1 = 8.95394 loss)
I0523 04:05:46.091747 34682 sgd_solver.cpp:112] Iteration 46390, lr = 0.01
I0523 04:05:47.348264 34682 solver.cpp:239] Iteration 46400 (7.70789 iter/s, 1.29737s/10 iters), loss = 9.0482
I0523 04:05:47.348304 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0482 (* 1 = 9.0482 loss)
I0523 04:05:47.386960 34682 sgd_solver.cpp:112] Iteration 46400, lr = 0.01
I0523 04:05:48.743202 34682 solver.cpp:239] Iteration 46410 (7.16933 iter/s, 1.39483s/10 iters), loss = 8.49532
I0523 04:05:48.743245 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49532 (* 1 = 8.49532 loss)
I0523 04:05:48.791735 34682 sgd_solver.cpp:112] Iteration 46410, lr = 0.01
I0523 04:05:49.919162 34682 solver.cpp:239] Iteration 46420 (8.50441 iter/s, 1.17586s/10 iters), loss = 8.13811
I0523 04:05:49.919199 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13811 (* 1 = 8.13811 loss)
I0523 04:05:49.970317 34682 sgd_solver.cpp:112] Iteration 46420, lr = 0.01
I0523 04:05:53.684919 34682 solver.cpp:239] Iteration 46430 (2.65565 iter/s, 3.76555s/10 iters), loss = 9.47634
I0523 04:05:53.684980 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47634 (* 1 = 9.47634 loss)
I0523 04:05:53.756326 34682 sgd_solver.cpp:112] Iteration 46430, lr = 0.01
I0523 04:05:56.992599 34682 solver.cpp:239] Iteration 46440 (3.02345 iter/s, 3.30748s/10 iters), loss = 8.96761
I0523 04:05:56.992784 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96761 (* 1 = 8.96761 loss)
I0523 04:05:57.806212 34682 sgd_solver.cpp:112] Iteration 46440, lr = 0.01
I0523 04:06:04.810353 34682 solver.cpp:239] Iteration 46450 (1.27922 iter/s, 7.81726s/10 iters), loss = 8.70746
I0523 04:06:04.810397 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70746 (* 1 = 8.70746 loss)
I0523 04:06:05.547304 34682 sgd_solver.cpp:112] Iteration 46450, lr = 0.01
I0523 04:06:10.420125 34682 solver.cpp:239] Iteration 46460 (1.78269 iter/s, 5.60949s/10 iters), loss = 7.84578
I0523 04:06:10.420200 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84578 (* 1 = 7.84578 loss)
I0523 04:06:11.282718 34682 sgd_solver.cpp:112] Iteration 46460, lr = 0.01
I0523 04:06:16.487992 34682 solver.cpp:239] Iteration 46470 (1.64811 iter/s, 6.06755s/10 iters), loss = 8.56068
I0523 04:06:16.488044 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56068 (* 1 = 8.56068 loss)
I0523 04:06:17.194442 34682 sgd_solver.cpp:112] Iteration 46470, lr = 0.01
I0523 04:06:21.893790 34682 solver.cpp:239] Iteration 46480 (1.84996 iter/s, 5.40552s/10 iters), loss = 8.89557
I0523 04:06:21.893838 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89557 (* 1 = 8.89557 loss)
I0523 04:06:21.956728 34682 sgd_solver.cpp:112] Iteration 46480, lr = 0.01
I0523 04:06:25.482326 34682 solver.cpp:239] Iteration 46490 (2.78681 iter/s, 3.58833s/10 iters), loss = 8.33747
I0523 04:06:25.482393 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33747 (* 1 = 8.33747 loss)
I0523 04:06:25.550341 34682 sgd_solver.cpp:112] Iteration 46490, lr = 0.01
I0523 04:06:29.346361 34682 solver.cpp:239] Iteration 46500 (2.58812 iter/s, 3.86381s/10 iters), loss = 9.08472
I0523 04:06:29.346583 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08472 (* 1 = 9.08472 loss)
I0523 04:06:29.436031 34682 sgd_solver.cpp:112] Iteration 46500, lr = 0.01
I0523 04:06:32.983379 34682 solver.cpp:239] Iteration 46510 (2.74979 iter/s, 3.63665s/10 iters), loss = 8.33483
I0523 04:06:32.983424 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33483 (* 1 = 8.33483 loss)
I0523 04:06:33.056068 34682 sgd_solver.cpp:112] Iteration 46510, lr = 0.01
I0523 04:06:37.099354 34682 solver.cpp:239] Iteration 46520 (2.42968 iter/s, 4.11576s/10 iters), loss = 8.01349
I0523 04:06:37.099388 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01349 (* 1 = 8.01349 loss)
I0523 04:06:37.173949 34682 sgd_solver.cpp:112] Iteration 46520, lr = 0.01
I0523 04:06:42.583055 34682 solver.cpp:239] Iteration 46530 (1.82367 iter/s, 5.48344s/10 iters), loss = 8.26577
I0523 04:06:42.583107 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26577 (* 1 = 8.26577 loss)
I0523 04:06:42.646374 34682 sgd_solver.cpp:112] Iteration 46530, lr = 0.01
I0523 04:06:46.892242 34682 solver.cpp:239] Iteration 46540 (2.32074 iter/s, 4.30896s/10 iters), loss = 8.86679
I0523 04:06:46.892283 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86679 (* 1 = 8.86679 loss)
I0523 04:06:46.961766 34682 sgd_solver.cpp:112] Iteration 46540, lr = 0.01
I0523 04:06:50.844377 34682 solver.cpp:239] Iteration 46550 (2.53042 iter/s, 3.95192s/10 iters), loss = 9.36485
I0523 04:06:50.844422 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36485 (* 1 = 9.36485 loss)
I0523 04:06:50.918035 34682 sgd_solver.cpp:112] Iteration 46550, lr = 0.01
I0523 04:06:55.772159 34682 solver.cpp:239] Iteration 46560 (2.02941 iter/s, 4.92754s/10 iters), loss = 8.67369
I0523 04:06:55.772204 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67369 (* 1 = 8.67369 loss)
I0523 04:06:56.511446 34682 sgd_solver.cpp:112] Iteration 46560, lr = 0.01
I0523 04:07:01.128576 34682 solver.cpp:239] Iteration 46570 (1.86701 iter/s, 5.35616s/10 iters), loss = 9.02091
I0523 04:07:01.128834 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02091 (* 1 = 9.02091 loss)
I0523 04:07:01.868320 34682 sgd_solver.cpp:112] Iteration 46570, lr = 0.01
I0523 04:07:05.215689 34682 solver.cpp:239] Iteration 46580 (2.44696 iter/s, 4.08671s/10 iters), loss = 9.29747
I0523 04:07:05.215750 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29747 (* 1 = 9.29747 loss)
I0523 04:07:05.932456 34682 sgd_solver.cpp:112] Iteration 46580, lr = 0.01
I0523 04:07:09.023123 34682 solver.cpp:239] Iteration 46590 (2.62659 iter/s, 3.80722s/10 iters), loss = 8.55815
I0523 04:07:09.023170 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55815 (* 1 = 8.55815 loss)
I0523 04:07:09.092409 34682 sgd_solver.cpp:112] Iteration 46590, lr = 0.01
I0523 04:07:13.985230 34682 solver.cpp:239] Iteration 46600 (2.01538 iter/s, 4.96185s/10 iters), loss = 9.32095
I0523 04:07:13.985280 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32095 (* 1 = 9.32095 loss)
I0523 04:07:14.754619 34682 sgd_solver.cpp:112] Iteration 46600, lr = 0.01
I0523 04:07:19.461031 34682 solver.cpp:239] Iteration 46610 (1.82631 iter/s, 5.47553s/10 iters), loss = 8.05928
I0523 04:07:19.461071 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05928 (* 1 = 8.05928 loss)
I0523 04:07:19.528852 34682 sgd_solver.cpp:112] Iteration 46610, lr = 0.01
I0523 04:07:24.526165 34682 solver.cpp:239] Iteration 46620 (1.97438 iter/s, 5.06488s/10 iters), loss = 8.53113
I0523 04:07:24.526208 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53113 (* 1 = 8.53113 loss)
I0523 04:07:24.598201 34682 sgd_solver.cpp:112] Iteration 46620, lr = 0.01
I0523 04:07:28.895850 34682 solver.cpp:239] Iteration 46630 (2.28992 iter/s, 4.36697s/10 iters), loss = 8.93457
I0523 04:07:28.895892 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93457 (* 1 = 8.93457 loss)
I0523 04:07:28.963610 34682 sgd_solver.cpp:112] Iteration 46630, lr = 0.01
I0523 04:07:34.380858 34682 solver.cpp:239] Iteration 46640 (1.82324 iter/s, 5.48474s/10 iters), loss = 8.98835
I0523 04:07:34.381145 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98835 (* 1 = 8.98835 loss)
I0523 04:07:35.178427 34682 sgd_solver.cpp:112] Iteration 46640, lr = 0.01
I0523 04:07:38.634436 34682 solver.cpp:239] Iteration 46650 (2.3512 iter/s, 4.25316s/10 iters), loss = 8.87248
I0523 04:07:38.634476 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87248 (* 1 = 8.87248 loss)
I0523 04:07:38.699126 34682 sgd_solver.cpp:112] Iteration 46650, lr = 0.01
I0523 04:07:41.979666 34682 solver.cpp:239] Iteration 46660 (2.98949 iter/s, 3.34505s/10 iters), loss = 9.0916
I0523 04:07:41.979708 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0916 (* 1 = 9.0916 loss)
I0523 04:07:42.831899 34682 sgd_solver.cpp:112] Iteration 46660, lr = 0.01
I0523 04:07:49.988147 34682 solver.cpp:239] Iteration 46670 (1.24873 iter/s, 8.00811s/10 iters), loss = 8.89727
I0523 04:07:49.988200 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89727 (* 1 = 8.89727 loss)
I0523 04:07:50.046708 34682 sgd_solver.cpp:112] Iteration 46670, lr = 0.01
I0523 04:07:55.537032 34682 solver.cpp:239] Iteration 46680 (1.80225 iter/s, 5.54861s/10 iters), loss = 8.94897
I0523 04:07:55.537080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94897 (* 1 = 8.94897 loss)
I0523 04:07:55.613328 34682 sgd_solver.cpp:112] Iteration 46680, lr = 0.01
I0523 04:07:59.740589 34682 solver.cpp:239] Iteration 46690 (2.37906 iter/s, 4.20334s/10 iters), loss = 8.9269
I0523 04:07:59.740639 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9269 (* 1 = 8.9269 loss)
I0523 04:08:00.581508 34682 sgd_solver.cpp:112] Iteration 46690, lr = 0.01
I0523 04:08:06.966773 34682 solver.cpp:239] Iteration 46700 (1.38392 iter/s, 7.22584s/10 iters), loss = 8.98138
I0523 04:08:06.966980 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98138 (* 1 = 8.98138 loss)
I0523 04:08:07.711689 34682 sgd_solver.cpp:112] Iteration 46700, lr = 0.01
I0523 04:08:11.105170 34682 solver.cpp:239] Iteration 46710 (2.41661 iter/s, 4.13804s/10 iters), loss = 8.72618
I0523 04:08:11.105226 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72618 (* 1 = 8.72618 loss)
I0523 04:08:11.683759 34682 sgd_solver.cpp:112] Iteration 46710, lr = 0.01
I0523 04:08:16.909166 34682 solver.cpp:239] Iteration 46720 (1.72304 iter/s, 5.8037s/10 iters), loss = 8.32113
I0523 04:08:16.909216 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32113 (* 1 = 8.32113 loss)
I0523 04:08:17.468622 34682 sgd_solver.cpp:112] Iteration 46720, lr = 0.01
I0523 04:08:21.431797 34682 solver.cpp:239] Iteration 46730 (2.21122 iter/s, 4.5224s/10 iters), loss = 8.11683
I0523 04:08:21.431843 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11683 (* 1 = 8.11683 loss)
I0523 04:08:22.022907 34682 sgd_solver.cpp:112] Iteration 46730, lr = 0.01
I0523 04:08:27.569617 34682 solver.cpp:239] Iteration 46740 (1.62932 iter/s, 6.13752s/10 iters), loss = 8.59261
I0523 04:08:27.569671 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59261 (* 1 = 8.59261 loss)
I0523 04:08:28.348065 34682 sgd_solver.cpp:112] Iteration 46740, lr = 0.01
I0523 04:08:31.644062 34682 solver.cpp:239] Iteration 46750 (2.45445 iter/s, 4.07422s/10 iters), loss = 8.42498
I0523 04:08:31.644115 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42498 (* 1 = 8.42498 loss)
I0523 04:08:31.726758 34682 sgd_solver.cpp:112] Iteration 46750, lr = 0.01
I0523 04:08:35.167191 34682 solver.cpp:239] Iteration 46760 (2.83856 iter/s, 3.52292s/10 iters), loss = 8.05441
I0523 04:08:35.167237 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05441 (* 1 = 8.05441 loss)
I0523 04:08:35.990556 34682 sgd_solver.cpp:112] Iteration 46760, lr = 0.01
I0523 04:08:40.539638 34682 solver.cpp:239] Iteration 46770 (1.86144 iter/s, 5.37218s/10 iters), loss = 9.32533
I0523 04:08:40.539840 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32533 (* 1 = 9.32533 loss)
I0523 04:08:41.248975 34682 sgd_solver.cpp:112] Iteration 46770, lr = 0.01
I0523 04:08:45.358892 34682 solver.cpp:239] Iteration 46780 (2.07518 iter/s, 4.81885s/10 iters), loss = 8.73426
I0523 04:08:45.358938 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73426 (* 1 = 8.73426 loss)
I0523 04:08:45.446621 34682 sgd_solver.cpp:112] Iteration 46780, lr = 0.01
I0523 04:08:48.071432 34682 solver.cpp:239] Iteration 46790 (3.6868 iter/s, 2.71238s/10 iters), loss = 8.38121
I0523 04:08:48.071483 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38121 (* 1 = 8.38121 loss)
I0523 04:08:48.134250 34682 sgd_solver.cpp:112] Iteration 46790, lr = 0.01
I0523 04:08:52.521600 34682 solver.cpp:239] Iteration 46800 (2.24722 iter/s, 4.44993s/10 iters), loss = 9.79244
I0523 04:08:52.521641 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.79244 (* 1 = 9.79244 loss)
I0523 04:08:52.602259 34682 sgd_solver.cpp:112] Iteration 46800, lr = 0.01
I0523 04:08:56.471974 34682 solver.cpp:239] Iteration 46810 (2.53155 iter/s, 3.95015s/10 iters), loss = 8.88671
I0523 04:08:56.472039 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88671 (* 1 = 8.88671 loss)
I0523 04:08:57.276180 34682 sgd_solver.cpp:112] Iteration 46810, lr = 0.01
I0523 04:09:02.033493 34682 solver.cpp:239] Iteration 46820 (1.79816 iter/s, 5.56123s/10 iters), loss = 8.65208
I0523 04:09:02.033540 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65208 (* 1 = 8.65208 loss)
I0523 04:09:02.090240 34682 sgd_solver.cpp:112] Iteration 46820, lr = 0.01
I0523 04:09:06.715986 34682 solver.cpp:239] Iteration 46830 (2.13572 iter/s, 4.68225s/10 iters), loss = 8.30362
I0523 04:09:06.716042 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30362 (* 1 = 8.30362 loss)
I0523 04:09:07.522264 34682 sgd_solver.cpp:112] Iteration 46830, lr = 0.01
I0523 04:09:14.342042 34682 solver.cpp:239] Iteration 46840 (1.31135 iter/s, 7.6257s/10 iters), loss = 8.73617
I0523 04:09:14.342279 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73617 (* 1 = 8.73617 loss)
I0523 04:09:15.187357 34682 sgd_solver.cpp:112] Iteration 46840, lr = 0.01
I0523 04:09:18.718124 34682 solver.cpp:239] Iteration 46850 (2.28536 iter/s, 4.37568s/10 iters), loss = 8.95033
I0523 04:09:18.718173 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95033 (* 1 = 8.95033 loss)
I0523 04:09:18.792534 34682 sgd_solver.cpp:112] Iteration 46850, lr = 0.01
I0523 04:09:22.798542 34682 solver.cpp:239] Iteration 46860 (2.45086 iter/s, 4.0802s/10 iters), loss = 8.75225
I0523 04:09:22.798597 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75225 (* 1 = 8.75225 loss)
I0523 04:09:23.592386 34682 sgd_solver.cpp:112] Iteration 46860, lr = 0.01
I0523 04:09:26.360491 34682 solver.cpp:239] Iteration 46870 (2.80763 iter/s, 3.56172s/10 iters), loss = 9.35441
I0523 04:09:26.360558 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35441 (* 1 = 9.35441 loss)
I0523 04:09:27.185202 34682 sgd_solver.cpp:112] Iteration 46870, lr = 0.01
I0523 04:09:32.726270 34682 solver.cpp:239] Iteration 46880 (1.57098 iter/s, 6.36546s/10 iters), loss = 9.16
I0523 04:09:32.726311 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16 (* 1 = 9.16 loss)
I0523 04:09:32.798322 34682 sgd_solver.cpp:112] Iteration 46880, lr = 0.01
I0523 04:09:38.059548 34682 solver.cpp:239] Iteration 46890 (1.87511 iter/s, 5.33301s/10 iters), loss = 7.74695
I0523 04:09:38.059607 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74695 (* 1 = 7.74695 loss)
I0523 04:09:38.865650 34682 sgd_solver.cpp:112] Iteration 46890, lr = 0.01
I0523 04:09:44.075265 34682 solver.cpp:239] Iteration 46900 (1.6624 iter/s, 6.01541s/10 iters), loss = 8.11707
I0523 04:09:44.075314 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11707 (* 1 = 8.11707 loss)
I0523 04:09:44.164945 34682 sgd_solver.cpp:112] Iteration 46900, lr = 0.01
I0523 04:09:50.408298 34682 solver.cpp:239] Iteration 46910 (1.5791 iter/s, 6.33273s/10 iters), loss = 7.69232
I0523 04:09:50.408610 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69232 (* 1 = 7.69232 loss)
I0523 04:09:50.481984 34682 sgd_solver.cpp:112] Iteration 46910, lr = 0.01
I0523 04:09:56.778434 34682 solver.cpp:239] Iteration 46920 (1.57104 iter/s, 6.36521s/10 iters), loss = 9.16879
I0523 04:09:56.778488 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16879 (* 1 = 9.16879 loss)
I0523 04:09:56.843127 34682 sgd_solver.cpp:112] Iteration 46920, lr = 0.01
I0523 04:10:00.875749 34682 solver.cpp:239] Iteration 46930 (2.44075 iter/s, 4.0971s/10 iters), loss = 8.55765
I0523 04:10:00.875790 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55765 (* 1 = 8.55765 loss)
I0523 04:10:00.953871 34682 sgd_solver.cpp:112] Iteration 46930, lr = 0.01
I0523 04:10:04.055830 34682 solver.cpp:239] Iteration 46940 (3.14475 iter/s, 3.1799s/10 iters), loss = 8.48491
I0523 04:10:04.055881 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48491 (* 1 = 8.48491 loss)
I0523 04:10:04.126066 34682 sgd_solver.cpp:112] Iteration 46940, lr = 0.01
I0523 04:10:08.862074 34682 solver.cpp:239] Iteration 46950 (2.08073 iter/s, 4.806s/10 iters), loss = 8.87758
I0523 04:10:08.862130 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87758 (* 1 = 8.87758 loss)
I0523 04:10:08.938525 34682 sgd_solver.cpp:112] Iteration 46950, lr = 0.01
I0523 04:10:12.306212 34682 solver.cpp:239] Iteration 46960 (2.90365 iter/s, 3.44394s/10 iters), loss = 9.25821
I0523 04:10:12.306258 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25821 (* 1 = 9.25821 loss)
I0523 04:10:13.078367 34682 sgd_solver.cpp:112] Iteration 46960, lr = 0.01
I0523 04:10:18.675648 34682 solver.cpp:239] Iteration 46970 (1.57007 iter/s, 6.36912s/10 iters), loss = 8.41896
I0523 04:10:18.675709 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41896 (* 1 = 8.41896 loss)
I0523 04:10:19.472113 34682 sgd_solver.cpp:112] Iteration 46970, lr = 0.01
I0523 04:10:23.471236 34682 solver.cpp:239] Iteration 46980 (2.08536 iter/s, 4.79534s/10 iters), loss = 8.55577
I0523 04:10:23.471379 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55577 (* 1 = 8.55577 loss)
I0523 04:10:23.529655 34682 sgd_solver.cpp:112] Iteration 46980, lr = 0.01
I0523 04:10:26.027765 34682 solver.cpp:239] Iteration 46990 (3.91195 iter/s, 2.55627s/10 iters), loss = 8.37037
I0523 04:10:26.027820 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37037 (* 1 = 8.37037 loss)
I0523 04:10:26.086474 34682 sgd_solver.cpp:112] Iteration 46990, lr = 0.01
I0523 04:10:30.267870 34682 solver.cpp:239] Iteration 47000 (2.35856 iter/s, 4.23988s/10 iters), loss = 8.9253
I0523 04:10:30.267910 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9253 (* 1 = 8.9253 loss)
I0523 04:10:31.072892 34682 sgd_solver.cpp:112] Iteration 47000, lr = 0.01
I0523 04:10:35.351323 34682 solver.cpp:239] Iteration 47010 (1.96727 iter/s, 5.08319s/10 iters), loss = 8.66166
I0523 04:10:35.351387 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66166 (* 1 = 8.66166 loss)
I0523 04:10:35.417286 34682 sgd_solver.cpp:112] Iteration 47010, lr = 0.01
I0523 04:10:41.055219 34682 solver.cpp:239] Iteration 47020 (1.75328 iter/s, 5.70361s/10 iters), loss = 9.1301
I0523 04:10:41.055265 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1301 (* 1 = 9.1301 loss)
I0523 04:10:41.114158 34682 sgd_solver.cpp:112] Iteration 47020, lr = 0.01
I0523 04:10:48.216845 34682 solver.cpp:239] Iteration 47030 (1.3964 iter/s, 7.16129s/10 iters), loss = 8.72456
I0523 04:10:48.216900 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72456 (* 1 = 8.72456 loss)
I0523 04:10:48.289526 34682 sgd_solver.cpp:112] Iteration 47030, lr = 0.01
I0523 04:10:53.179450 34682 solver.cpp:239] Iteration 47040 (2.01518 iter/s, 4.96233s/10 iters), loss = 9.22341
I0523 04:10:53.179507 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22341 (* 1 = 9.22341 loss)
I0523 04:10:53.933719 34682 sgd_solver.cpp:112] Iteration 47040, lr = 0.01
I0523 04:10:59.266206 34682 solver.cpp:239] Iteration 47050 (1.643 iter/s, 6.08642s/10 iters), loss = 8.79741
I0523 04:10:59.266273 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79741 (* 1 = 8.79741 loss)
I0523 04:10:59.331571 34682 sgd_solver.cpp:112] Iteration 47050, lr = 0.01
I0523 04:11:02.543824 34682 solver.cpp:239] Iteration 47060 (3.05118 iter/s, 3.27742s/10 iters), loss = 8.02871
I0523 04:11:02.543871 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02871 (* 1 = 8.02871 loss)
I0523 04:11:02.612488 34682 sgd_solver.cpp:112] Iteration 47060, lr = 0.01
I0523 04:11:06.389678 34682 solver.cpp:239] Iteration 47070 (2.60034 iter/s, 3.84565s/10 iters), loss = 8.00947
I0523 04:11:06.389730 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00947 (* 1 = 8.00947 loss)
I0523 04:11:07.035316 34682 sgd_solver.cpp:112] Iteration 47070, lr = 0.01
I0523 04:11:12.236553 34682 solver.cpp:239] Iteration 47080 (1.7104 iter/s, 5.84658s/10 iters), loss = 8.02683
I0523 04:11:12.236606 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02683 (* 1 = 8.02683 loss)
I0523 04:11:12.870273 34682 sgd_solver.cpp:112] Iteration 47080, lr = 0.01
I0523 04:11:17.968333 34682 solver.cpp:239] Iteration 47090 (1.74475 iter/s, 5.73148s/10 iters), loss = 7.99184
I0523 04:11:17.968397 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99184 (* 1 = 7.99184 loss)
I0523 04:11:18.830103 34682 sgd_solver.cpp:112] Iteration 47090, lr = 0.01
I0523 04:11:23.699178 34682 solver.cpp:239] Iteration 47100 (1.74503 iter/s, 5.73055s/10 iters), loss = 9.05154
I0523 04:11:23.699229 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05154 (* 1 = 9.05154 loss)
I0523 04:11:23.767571 34682 sgd_solver.cpp:112] Iteration 47100, lr = 0.01
I0523 04:11:26.944388 34682 solver.cpp:239] Iteration 47110 (3.08164 iter/s, 3.24502s/10 iters), loss = 9.57276
I0523 04:11:26.944519 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.57276 (* 1 = 9.57276 loss)
I0523 04:11:27.000078 34682 sgd_solver.cpp:112] Iteration 47110, lr = 0.01
I0523 04:11:31.125435 34682 solver.cpp:239] Iteration 47120 (2.39191 iter/s, 4.18075s/10 iters), loss = 9.44477
I0523 04:11:31.125479 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44477 (* 1 = 9.44477 loss)
I0523 04:11:31.965365 34682 sgd_solver.cpp:112] Iteration 47120, lr = 0.01
I0523 04:11:35.808692 34682 solver.cpp:239] Iteration 47130 (2.13538 iter/s, 4.68302s/10 iters), loss = 8.68967
I0523 04:11:35.808753 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68967 (* 1 = 8.68967 loss)
I0523 04:11:36.584625 34682 sgd_solver.cpp:112] Iteration 47130, lr = 0.01
I0523 04:11:41.558557 34682 solver.cpp:239] Iteration 47140 (1.73926 iter/s, 5.74958s/10 iters), loss = 7.88834
I0523 04:11:41.558611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88834 (* 1 = 7.88834 loss)
I0523 04:11:41.616071 34682 sgd_solver.cpp:112] Iteration 47140, lr = 0.01
I0523 04:11:46.116665 34682 solver.cpp:239] Iteration 47150 (2.19401 iter/s, 4.55787s/10 iters), loss = 8.75548
I0523 04:11:46.116710 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75548 (* 1 = 8.75548 loss)
I0523 04:11:46.191751 34682 sgd_solver.cpp:112] Iteration 47150, lr = 0.01
I0523 04:11:50.670120 34682 solver.cpp:239] Iteration 47160 (2.19625 iter/s, 4.55322s/10 iters), loss = 9.00197
I0523 04:11:50.670166 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00197 (* 1 = 9.00197 loss)
I0523 04:11:50.748702 34682 sgd_solver.cpp:112] Iteration 47160, lr = 0.01
I0523 04:11:56.241281 34682 solver.cpp:239] Iteration 47170 (1.79504 iter/s, 5.57089s/10 iters), loss = 8.61448
I0523 04:11:56.241322 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61448 (* 1 = 8.61448 loss)
I0523 04:11:57.067322 34682 sgd_solver.cpp:112] Iteration 47170, lr = 0.01
I0523 04:12:03.517874 34682 solver.cpp:239] Iteration 47180 (1.37433 iter/s, 7.27626s/10 iters), loss = 9.15963
I0523 04:12:03.517935 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15963 (* 1 = 9.15963 loss)
I0523 04:12:03.977627 34682 sgd_solver.cpp:112] Iteration 47180, lr = 0.01
I0523 04:12:08.642474 34682 solver.cpp:239] Iteration 47190 (1.95147 iter/s, 5.12434s/10 iters), loss = 8.78933
I0523 04:12:08.642526 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78933 (* 1 = 8.78933 loss)
I0523 04:12:08.953163 34682 sgd_solver.cpp:112] Iteration 47190, lr = 0.01
I0523 04:12:14.759311 34682 solver.cpp:239] Iteration 47200 (1.63491 iter/s, 6.11654s/10 iters), loss = 9.10722
I0523 04:12:14.759357 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10722 (* 1 = 9.10722 loss)
I0523 04:12:14.826813 34682 sgd_solver.cpp:112] Iteration 47200, lr = 0.01
I0523 04:12:19.367579 34682 solver.cpp:239] Iteration 47210 (2.17012 iter/s, 4.60803s/10 iters), loss = 8.8614
I0523 04:12:19.367632 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8614 (* 1 = 8.8614 loss)
I0523 04:12:19.426736 34682 sgd_solver.cpp:112] Iteration 47210, lr = 0.01
I0523 04:12:21.848906 34682 solver.cpp:239] Iteration 47220 (4.03037 iter/s, 2.48116s/10 iters), loss = 9.25322
I0523 04:12:21.848963 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25322 (* 1 = 9.25322 loss)
I0523 04:12:21.931933 34682 sgd_solver.cpp:112] Iteration 47220, lr = 0.01
I0523 04:12:27.548910 34682 solver.cpp:239] Iteration 47230 (1.75448 iter/s, 5.69968s/10 iters), loss = 8.44285
I0523 04:12:27.549060 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44285 (* 1 = 8.44285 loss)
I0523 04:12:27.615247 34682 sgd_solver.cpp:112] Iteration 47230, lr = 0.01
I0523 04:12:33.324064 34682 solver.cpp:239] Iteration 47240 (1.73167 iter/s, 5.77477s/10 iters), loss = 8.77941
I0523 04:12:33.324110 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77941 (* 1 = 8.77941 loss)
I0523 04:12:33.400332 34682 sgd_solver.cpp:112] Iteration 47240, lr = 0.01
I0523 04:12:37.514169 34682 solver.cpp:239] Iteration 47250 (2.38671 iter/s, 4.18987s/10 iters), loss = 8.14614
I0523 04:12:37.514214 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14614 (* 1 = 8.14614 loss)
I0523 04:12:37.578562 34682 sgd_solver.cpp:112] Iteration 47250, lr = 0.01
I0523 04:12:41.398295 34682 solver.cpp:239] Iteration 47260 (2.57472 iter/s, 3.88392s/10 iters), loss = 9.19057
I0523 04:12:41.398339 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19057 (* 1 = 9.19057 loss)
I0523 04:12:41.458240 34682 sgd_solver.cpp:112] Iteration 47260, lr = 0.01
I0523 04:12:44.916712 34682 solver.cpp:239] Iteration 47270 (2.84235 iter/s, 3.51822s/10 iters), loss = 7.94255
I0523 04:12:44.916769 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94255 (* 1 = 7.94255 loss)
I0523 04:12:44.984623 34682 sgd_solver.cpp:112] Iteration 47270, lr = 0.01
I0523 04:12:51.122853 34682 solver.cpp:239] Iteration 47280 (1.61139 iter/s, 6.20583s/10 iters), loss = 8.53459
I0523 04:12:51.122911 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53459 (* 1 = 8.53459 loss)
I0523 04:12:51.181234 34682 sgd_solver.cpp:112] Iteration 47280, lr = 0.01
I0523 04:12:55.209152 34682 solver.cpp:239] Iteration 47290 (2.44735 iter/s, 4.08606s/10 iters), loss = 9.04113
I0523 04:12:55.209215 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04113 (* 1 = 9.04113 loss)
I0523 04:12:55.433265 34682 sgd_solver.cpp:112] Iteration 47290, lr = 0.01
I0523 04:13:00.361543 34682 solver.cpp:239] Iteration 47300 (1.94095 iter/s, 5.15212s/10 iters), loss = 9.19257
I0523 04:13:00.361708 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19257 (* 1 = 9.19257 loss)
I0523 04:13:00.432296 34682 sgd_solver.cpp:112] Iteration 47300, lr = 0.01
I0523 04:13:05.246299 34682 solver.cpp:239] Iteration 47310 (2.04734 iter/s, 4.88439s/10 iters), loss = 9.31449
I0523 04:13:05.246342 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31449 (* 1 = 9.31449 loss)
I0523 04:13:05.327708 34682 sgd_solver.cpp:112] Iteration 47310, lr = 0.01
I0523 04:13:09.425552 34682 solver.cpp:239] Iteration 47320 (2.3929 iter/s, 4.17903s/10 iters), loss = 8.82633
I0523 04:13:09.425637 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82633 (* 1 = 8.82633 loss)
I0523 04:13:10.290663 34682 sgd_solver.cpp:112] Iteration 47320, lr = 0.01
I0523 04:13:16.268623 34682 solver.cpp:239] Iteration 47330 (1.46141 iter/s, 6.84271s/10 iters), loss = 9.3982
I0523 04:13:16.268672 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3982 (* 1 = 9.3982 loss)
I0523 04:13:16.341514 34682 sgd_solver.cpp:112] Iteration 47330, lr = 0.01
I0523 04:13:21.494740 34682 solver.cpp:239] Iteration 47340 (1.91356 iter/s, 5.22585s/10 iters), loss = 8.61364
I0523 04:13:21.494788 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61364 (* 1 = 8.61364 loss)
I0523 04:13:21.571368 34682 sgd_solver.cpp:112] Iteration 47340, lr = 0.01
I0523 04:13:25.543617 34682 solver.cpp:239] Iteration 47350 (2.46995 iter/s, 4.04866s/10 iters), loss = 8.82845
I0523 04:13:25.543670 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82845 (* 1 = 8.82845 loss)
I0523 04:13:25.617249 34682 sgd_solver.cpp:112] Iteration 47350, lr = 0.01
I0523 04:13:30.138944 34682 solver.cpp:239] Iteration 47360 (2.17624 iter/s, 4.59508s/10 iters), loss = 8.09418
I0523 04:13:30.139004 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09418 (* 1 = 8.09418 loss)
I0523 04:13:31.033233 34682 sgd_solver.cpp:112] Iteration 47360, lr = 0.01
I0523 04:13:33.807415 34682 solver.cpp:239] Iteration 47370 (2.72609 iter/s, 3.66825s/10 iters), loss = 7.98152
I0523 04:13:33.807466 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98152 (* 1 = 7.98152 loss)
I0523 04:13:34.462357 34682 sgd_solver.cpp:112] Iteration 47370, lr = 0.01
I0523 04:13:39.445765 34682 solver.cpp:239] Iteration 47380 (1.77366 iter/s, 5.63807s/10 iters), loss = 8.43115
I0523 04:13:39.445823 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43115 (* 1 = 8.43115 loss)
I0523 04:13:39.521011 34682 sgd_solver.cpp:112] Iteration 47380, lr = 0.01
I0523 04:13:45.351198 34682 solver.cpp:239] Iteration 47390 (1.69344 iter/s, 5.90515s/10 iters), loss = 9.42834
I0523 04:13:45.351234 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42834 (* 1 = 9.42834 loss)
I0523 04:13:45.430718 34682 sgd_solver.cpp:112] Iteration 47390, lr = 0.01
I0523 04:13:48.020787 34682 solver.cpp:239] Iteration 47400 (3.74612 iter/s, 2.66943s/10 iters), loss = 9.56666
I0523 04:13:48.020840 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.56666 (* 1 = 9.56666 loss)
I0523 04:13:48.087009 34682 sgd_solver.cpp:112] Iteration 47400, lr = 0.01
I0523 04:13:51.477979 34682 solver.cpp:239] Iteration 47410 (2.89268 iter/s, 3.457s/10 iters), loss = 8.54437
I0523 04:13:51.478019 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54437 (* 1 = 8.54437 loss)
I0523 04:13:51.547309 34682 sgd_solver.cpp:112] Iteration 47410, lr = 0.01
I0523 04:13:56.397953 34682 solver.cpp:239] Iteration 47420 (2.03263 iter/s, 4.91973s/10 iters), loss = 8.34419
I0523 04:13:56.397996 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34419 (* 1 = 8.34419 loss)
I0523 04:13:56.481647 34682 sgd_solver.cpp:112] Iteration 47420, lr = 0.01
I0523 04:14:01.153075 34682 solver.cpp:239] Iteration 47430 (2.1031 iter/s, 4.75488s/10 iters), loss = 8.39865
I0523 04:14:01.153201 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39865 (* 1 = 8.39865 loss)
I0523 04:14:01.232591 34682 sgd_solver.cpp:112] Iteration 47430, lr = 0.01
I0523 04:14:06.654767 34682 solver.cpp:239] Iteration 47440 (1.81774 iter/s, 5.50134s/10 iters), loss = 8.94263
I0523 04:14:06.654821 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94263 (* 1 = 8.94263 loss)
I0523 04:14:07.415832 34682 sgd_solver.cpp:112] Iteration 47440, lr = 0.01
I0523 04:14:13.074522 34682 solver.cpp:239] Iteration 47450 (1.55777 iter/s, 6.41943s/10 iters), loss = 8.92966
I0523 04:14:13.074582 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92966 (* 1 = 8.92966 loss)
I0523 04:14:13.147159 34682 sgd_solver.cpp:112] Iteration 47450, lr = 0.01
I0523 04:14:18.071944 34682 solver.cpp:239] Iteration 47460 (2.00114 iter/s, 4.99716s/10 iters), loss = 8.29184
I0523 04:14:18.071985 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29184 (* 1 = 8.29184 loss)
I0523 04:14:18.130187 34682 sgd_solver.cpp:112] Iteration 47460, lr = 0.01
I0523 04:14:22.407740 34682 solver.cpp:239] Iteration 47470 (2.3065 iter/s, 4.33557s/10 iters), loss = 8.91821
I0523 04:14:22.407796 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91821 (* 1 = 8.91821 loss)
I0523 04:14:22.482996 34682 sgd_solver.cpp:112] Iteration 47470, lr = 0.01
I0523 04:14:28.065798 34682 solver.cpp:239] Iteration 47480 (1.76748 iter/s, 5.65777s/10 iters), loss = 8.54596
I0523 04:14:28.065845 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54596 (* 1 = 8.54596 loss)
I0523 04:14:28.844712 34682 sgd_solver.cpp:112] Iteration 47480, lr = 0.01
I0523 04:14:33.638098 34682 solver.cpp:239] Iteration 47490 (1.79468 iter/s, 5.57203s/10 iters), loss = 8.28811
I0523 04:14:33.638255 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28811 (* 1 = 8.28811 loss)
I0523 04:14:33.711200 34682 sgd_solver.cpp:112] Iteration 47490, lr = 0.01
I0523 04:14:38.509409 34682 solver.cpp:239] Iteration 47500 (2.05299 iter/s, 4.87095s/10 iters), loss = 8.25174
I0523 04:14:38.509469 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25174 (* 1 = 8.25174 loss)
I0523 04:14:39.299923 34682 sgd_solver.cpp:112] Iteration 47500, lr = 0.01
I0523 04:14:43.439573 34682 solver.cpp:239] Iteration 47510 (2.02844 iter/s, 4.9299s/10 iters), loss = 8.86649
I0523 04:14:43.439618 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86649 (* 1 = 8.86649 loss)
I0523 04:14:43.514024 34682 sgd_solver.cpp:112] Iteration 47510, lr = 0.01
I0523 04:14:48.332939 34682 solver.cpp:239] Iteration 47520 (2.04369 iter/s, 4.89312s/10 iters), loss = 8.52728
I0523 04:14:48.332993 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52728 (* 1 = 8.52728 loss)
I0523 04:14:48.389768 34682 sgd_solver.cpp:112] Iteration 47520, lr = 0.01
I0523 04:14:54.190732 34682 solver.cpp:239] Iteration 47530 (1.70722 iter/s, 5.85746s/10 iters), loss = 8.67167
I0523 04:14:54.190790 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67167 (* 1 = 8.67167 loss)
I0523 04:14:54.247117 34682 sgd_solver.cpp:112] Iteration 47530, lr = 0.01
I0523 04:14:59.163404 34682 solver.cpp:239] Iteration 47540 (2.01109 iter/s, 4.97242s/10 iters), loss = 8.15152
I0523 04:14:59.163460 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15152 (* 1 = 8.15152 loss)
I0523 04:14:59.237027 34682 sgd_solver.cpp:112] Iteration 47540, lr = 0.01
I0523 04:15:03.478523 34682 solver.cpp:239] Iteration 47550 (2.31991 iter/s, 4.31052s/10 iters), loss = 8.42857
I0523 04:15:03.478582 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42857 (* 1 = 8.42857 loss)
I0523 04:15:04.241611 34682 sgd_solver.cpp:112] Iteration 47550, lr = 0.01
I0523 04:15:09.371600 34682 solver.cpp:239] Iteration 47560 (1.697 iter/s, 5.89275s/10 iters), loss = 8.60103
I0523 04:15:09.371647 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60103 (* 1 = 8.60103 loss)
I0523 04:15:10.138509 34682 sgd_solver.cpp:112] Iteration 47560, lr = 0.01
I0523 04:15:13.994777 34682 solver.cpp:239] Iteration 47570 (2.16312 iter/s, 4.62294s/10 iters), loss = 8.94241
I0523 04:15:13.994818 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94241 (* 1 = 8.94241 loss)
I0523 04:15:14.062827 34682 sgd_solver.cpp:112] Iteration 47570, lr = 0.01
I0523 04:15:19.037233 34682 solver.cpp:239] Iteration 47580 (1.98326 iter/s, 5.04219s/10 iters), loss = 9.35882
I0523 04:15:19.037303 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35882 (* 1 = 9.35882 loss)
I0523 04:15:19.587975 34682 sgd_solver.cpp:112] Iteration 47580, lr = 0.01
I0523 04:15:24.394223 34682 solver.cpp:239] Iteration 47590 (1.86682 iter/s, 5.35671s/10 iters), loss = 8.73986
I0523 04:15:24.394284 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73986 (* 1 = 8.73986 loss)
I0523 04:15:24.679478 34682 sgd_solver.cpp:112] Iteration 47590, lr = 0.01
I0523 04:15:28.059144 34682 solver.cpp:239] Iteration 47600 (2.72874 iter/s, 3.66469s/10 iters), loss = 8.82593
I0523 04:15:28.059206 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82593 (* 1 = 8.82593 loss)
I0523 04:15:28.880996 34682 sgd_solver.cpp:112] Iteration 47600, lr = 0.01
I0523 04:15:34.934563 34682 solver.cpp:239] Iteration 47610 (1.45453 iter/s, 6.87508s/10 iters), loss = 8.91173
I0523 04:15:34.934868 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91173 (* 1 = 8.91173 loss)
I0523 04:15:35.003221 34682 sgd_solver.cpp:112] Iteration 47610, lr = 0.01
I0523 04:15:39.697515 34682 solver.cpp:239] Iteration 47620 (2.09976 iter/s, 4.76245s/10 iters), loss = 8.06664
I0523 04:15:39.697584 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06664 (* 1 = 8.06664 loss)
I0523 04:15:39.772127 34682 sgd_solver.cpp:112] Iteration 47620, lr = 0.01
I0523 04:15:42.799866 34682 solver.cpp:239] Iteration 47630 (3.22358 iter/s, 3.10215s/10 iters), loss = 8.52658
I0523 04:15:42.799917 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52658 (* 1 = 8.52658 loss)
I0523 04:15:42.864212 34682 sgd_solver.cpp:112] Iteration 47630, lr = 0.01
I0523 04:15:47.386209 34682 solver.cpp:239] Iteration 47640 (2.1805 iter/s, 4.58611s/10 iters), loss = 9.73178
I0523 04:15:47.386252 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.73178 (* 1 = 9.73178 loss)
I0523 04:15:47.449661 34682 sgd_solver.cpp:112] Iteration 47640, lr = 0.01
I0523 04:15:52.364706 34682 solver.cpp:239] Iteration 47650 (2.00874 iter/s, 4.97825s/10 iters), loss = 9.04397
I0523 04:15:52.364748 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04397 (* 1 = 9.04397 loss)
I0523 04:15:52.441604 34682 sgd_solver.cpp:112] Iteration 47650, lr = 0.01
I0523 04:15:55.321975 34682 solver.cpp:239] Iteration 47660 (3.3817 iter/s, 2.95709s/10 iters), loss = 8.4901
I0523 04:15:55.322043 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4901 (* 1 = 8.4901 loss)
I0523 04:15:55.394740 34682 sgd_solver.cpp:112] Iteration 47660, lr = 0.01
I0523 04:15:59.703883 34682 solver.cpp:239] Iteration 47670 (2.28224 iter/s, 4.38166s/10 iters), loss = 7.80964
I0523 04:15:59.703944 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80964 (* 1 = 7.80964 loss)
I0523 04:16:00.583842 34682 sgd_solver.cpp:112] Iteration 47670, lr = 0.01
I0523 04:16:06.508725 34682 solver.cpp:239] Iteration 47680 (1.46962 iter/s, 6.8045s/10 iters), loss = 9.27706
I0523 04:16:06.508955 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27706 (* 1 = 9.27706 loss)
I0523 04:16:07.394696 34682 sgd_solver.cpp:112] Iteration 47680, lr = 0.01
I0523 04:16:12.045635 34682 solver.cpp:239] Iteration 47690 (1.8062 iter/s, 5.53648s/10 iters), loss = 9.23868
I0523 04:16:12.045693 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23868 (* 1 = 9.23868 loss)
I0523 04:16:12.114534 34682 sgd_solver.cpp:112] Iteration 47690, lr = 0.01
I0523 04:16:16.696910 34682 solver.cpp:239] Iteration 47700 (2.15006 iter/s, 4.65103s/10 iters), loss = 8.91419
I0523 04:16:16.696956 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91419 (* 1 = 8.91419 loss)
I0523 04:16:17.558444 34682 sgd_solver.cpp:112] Iteration 47700, lr = 0.01
I0523 04:16:23.884621 34682 solver.cpp:239] Iteration 47710 (1.39133 iter/s, 7.18738s/10 iters), loss = 8.89526
I0523 04:16:23.884667 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89526 (* 1 = 8.89526 loss)
I0523 04:16:23.957599 34682 sgd_solver.cpp:112] Iteration 47710, lr = 0.01
I0523 04:16:28.731823 34682 solver.cpp:239] Iteration 47720 (2.06315 iter/s, 4.84696s/10 iters), loss = 8.94912
I0523 04:16:28.731874 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94912 (* 1 = 8.94912 loss)
I0523 04:16:28.810569 34682 sgd_solver.cpp:112] Iteration 47720, lr = 0.01
I0523 04:16:33.791924 34682 solver.cpp:239] Iteration 47730 (1.97634 iter/s, 5.05985s/10 iters), loss = 9.00363
I0523 04:16:33.791968 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00363 (* 1 = 9.00363 loss)
I0523 04:16:33.849514 34682 sgd_solver.cpp:112] Iteration 47730, lr = 0.01
I0523 04:16:39.782666 34682 solver.cpp:239] Iteration 47740 (1.66932 iter/s, 5.99045s/10 iters), loss = 9.39656
I0523 04:16:39.782969 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.39656 (* 1 = 9.39656 loss)
I0523 04:16:39.849617 34682 sgd_solver.cpp:112] Iteration 47740, lr = 0.01
I0523 04:16:45.610332 34682 solver.cpp:239] Iteration 47750 (1.7161 iter/s, 5.82715s/10 iters), loss = 8.89348
I0523 04:16:45.610380 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89348 (* 1 = 8.89348 loss)
I0523 04:16:46.450601 34682 sgd_solver.cpp:112] Iteration 47750, lr = 0.01
I0523 04:16:50.365031 34682 solver.cpp:239] Iteration 47760 (2.10329 iter/s, 4.75445s/10 iters), loss = 9.29698
I0523 04:16:50.365080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29698 (* 1 = 9.29698 loss)
I0523 04:16:50.434311 34682 sgd_solver.cpp:112] Iteration 47760, lr = 0.01
I0523 04:16:54.981708 34682 solver.cpp:239] Iteration 47770 (2.16617 iter/s, 4.61643s/10 iters), loss = 9.49304
I0523 04:16:54.981762 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.49304 (* 1 = 9.49304 loss)
I0523 04:16:55.041990 34682 sgd_solver.cpp:112] Iteration 47770, lr = 0.01
I0523 04:17:00.518446 34682 solver.cpp:239] Iteration 47780 (1.80621 iter/s, 5.53646s/10 iters), loss = 8.75925
I0523 04:17:00.518502 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75925 (* 1 = 8.75925 loss)
I0523 04:17:00.848124 34682 sgd_solver.cpp:112] Iteration 47780, lr = 0.01
I0523 04:17:04.335237 34682 solver.cpp:239] Iteration 47790 (2.62015 iter/s, 3.81658s/10 iters), loss = 8.43611
I0523 04:17:04.335291 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43611 (* 1 = 8.43611 loss)
I0523 04:17:05.141858 34682 sgd_solver.cpp:112] Iteration 47790, lr = 0.01
I0523 04:17:09.742983 34682 solver.cpp:239] Iteration 47800 (1.84929 iter/s, 5.40748s/10 iters), loss = 8.64334
I0523 04:17:09.743036 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64334 (* 1 = 8.64334 loss)
I0523 04:17:10.601368 34682 sgd_solver.cpp:112] Iteration 47800, lr = 0.01
I0523 04:17:15.293056 34682 solver.cpp:239] Iteration 47810 (1.80187 iter/s, 5.5498s/10 iters), loss = 7.87266
I0523 04:17:15.293098 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87266 (* 1 = 7.87266 loss)
I0523 04:17:16.148592 34682 sgd_solver.cpp:112] Iteration 47810, lr = 0.01
I0523 04:17:21.639917 34682 solver.cpp:239] Iteration 47820 (1.57566 iter/s, 6.34657s/10 iters), loss = 9.09403
I0523 04:17:21.639971 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09403 (* 1 = 9.09403 loss)
I0523 04:17:21.710009 34682 sgd_solver.cpp:112] Iteration 47820, lr = 0.01
I0523 04:17:25.841332 34682 solver.cpp:239] Iteration 47830 (2.38028 iter/s, 4.20118s/10 iters), loss = 8.62295
I0523 04:17:25.841382 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62295 (* 1 = 8.62295 loss)
I0523 04:17:26.645611 34682 sgd_solver.cpp:112] Iteration 47830, lr = 0.01
I0523 04:17:31.267115 34682 solver.cpp:239] Iteration 47840 (1.84314 iter/s, 5.42552s/10 iters), loss = 8.38534
I0523 04:17:31.267158 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38534 (* 1 = 8.38534 loss)
I0523 04:17:31.772727 34682 sgd_solver.cpp:112] Iteration 47840, lr = 0.01
I0523 04:17:36.077518 34682 solver.cpp:239] Iteration 47850 (2.07893 iter/s, 4.81016s/10 iters), loss = 8.81364
I0523 04:17:36.077566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81364 (* 1 = 8.81364 loss)
I0523 04:17:36.152074 34682 sgd_solver.cpp:112] Iteration 47850, lr = 0.01
I0523 04:17:41.112957 34682 solver.cpp:239] Iteration 47860 (1.98603 iter/s, 5.03518s/10 iters), loss = 8.31234
I0523 04:17:41.113200 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31234 (* 1 = 8.31234 loss)
I0523 04:17:41.949453 34682 sgd_solver.cpp:112] Iteration 47860, lr = 0.01
I0523 04:17:45.804090 34682 solver.cpp:239] Iteration 47870 (2.13187 iter/s, 4.69071s/10 iters), loss = 9.4598
I0523 04:17:45.804131 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4598 (* 1 = 9.4598 loss)
I0523 04:17:45.870952 34682 sgd_solver.cpp:112] Iteration 47870, lr = 0.01
I0523 04:17:50.865881 34682 solver.cpp:239] Iteration 47880 (1.97568 iter/s, 5.06154s/10 iters), loss = 8.21957
I0523 04:17:50.865926 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21957 (* 1 = 8.21957 loss)
I0523 04:17:50.942845 34682 sgd_solver.cpp:112] Iteration 47880, lr = 0.01
I0523 04:17:55.541854 34682 solver.cpp:239] Iteration 47890 (2.13871 iter/s, 4.67572s/10 iters), loss = 8.57574
I0523 04:17:55.541915 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57574 (* 1 = 8.57574 loss)
I0523 04:17:56.199826 34682 sgd_solver.cpp:112] Iteration 47890, lr = 0.01
I0523 04:18:03.861155 34682 solver.cpp:239] Iteration 47900 (1.20208 iter/s, 8.31892s/10 iters), loss = 8.095
I0523 04:18:03.861202 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.095 (* 1 = 8.095 loss)
I0523 04:18:03.938407 34682 sgd_solver.cpp:112] Iteration 47900, lr = 0.01
I0523 04:18:07.288018 34682 solver.cpp:239] Iteration 47910 (2.91829 iter/s, 3.42667s/10 iters), loss = 8.01512
I0523 04:18:07.288070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01512 (* 1 = 8.01512 loss)
I0523 04:18:08.139010 34682 sgd_solver.cpp:112] Iteration 47910, lr = 0.01
I0523 04:18:13.725098 34682 solver.cpp:239] Iteration 47920 (1.55357 iter/s, 6.43677s/10 iters), loss = 9.21364
I0523 04:18:13.725332 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21364 (* 1 = 9.21364 loss)
I0523 04:18:13.793387 34682 sgd_solver.cpp:112] Iteration 47920, lr = 0.01
I0523 04:18:17.599071 34682 solver.cpp:239] Iteration 47930 (2.58159 iter/s, 3.87359s/10 iters), loss = 9.73285
I0523 04:18:17.599134 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.73285 (* 1 = 9.73285 loss)
I0523 04:18:18.373997 34682 sgd_solver.cpp:112] Iteration 47930, lr = 0.01
I0523 04:18:24.032438 34682 solver.cpp:239] Iteration 47940 (1.55447 iter/s, 6.43305s/10 iters), loss = 8.47954
I0523 04:18:24.032486 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47954 (* 1 = 8.47954 loss)
I0523 04:18:24.104176 34682 sgd_solver.cpp:112] Iteration 47940, lr = 0.01
I0523 04:18:28.402199 34682 solver.cpp:239] Iteration 47950 (2.28857 iter/s, 4.36953s/10 iters), loss = 8.54263
I0523 04:18:28.402256 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54263 (* 1 = 8.54263 loss)
I0523 04:18:28.463608 34682 sgd_solver.cpp:112] Iteration 47950, lr = 0.01
I0523 04:18:33.599002 34682 solver.cpp:239] Iteration 47960 (1.92436 iter/s, 5.19654s/10 iters), loss = 7.95088
I0523 04:18:33.599057 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95088 (* 1 = 7.95088 loss)
I0523 04:18:33.677728 34682 sgd_solver.cpp:112] Iteration 47960, lr = 0.01
I0523 04:18:39.705993 34682 solver.cpp:239] Iteration 47970 (1.63756 iter/s, 6.10663s/10 iters), loss = 9.30136
I0523 04:18:39.706058 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30136 (* 1 = 9.30136 loss)
I0523 04:18:40.533699 34682 sgd_solver.cpp:112] Iteration 47970, lr = 0.01
I0523 04:18:44.798013 34682 solver.cpp:239] Iteration 47980 (1.96396 iter/s, 5.09176s/10 iters), loss = 8.62146
I0523 04:18:44.798234 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62146 (* 1 = 8.62146 loss)
I0523 04:18:45.651701 34682 sgd_solver.cpp:112] Iteration 47980, lr = 0.01
I0523 04:18:49.819386 34682 solver.cpp:239] Iteration 47990 (1.99164 iter/s, 5.02098s/10 iters), loss = 8.38691
I0523 04:18:49.819427 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38691 (* 1 = 8.38691 loss)
I0523 04:18:50.534116 34682 sgd_solver.cpp:112] Iteration 47990, lr = 0.01
I0523 04:18:55.299013 34682 solver.cpp:239] Iteration 48000 (1.82503 iter/s, 5.47936s/10 iters), loss = 8.00598
I0523 04:18:55.299062 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00598 (* 1 = 8.00598 loss)
I0523 04:18:55.369426 34682 sgd_solver.cpp:112] Iteration 48000, lr = 0.01
I0523 04:19:02.439601 34682 solver.cpp:239] Iteration 48010 (1.40051 iter/s, 7.14024s/10 iters), loss = 8.48178
I0523 04:19:02.439664 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48178 (* 1 = 8.48178 loss)
I0523 04:19:03.152639 34682 sgd_solver.cpp:112] Iteration 48010, lr = 0.01
I0523 04:19:07.422281 34682 solver.cpp:239] Iteration 48020 (2.00706 iter/s, 4.98241s/10 iters), loss = 7.7752
I0523 04:19:07.422341 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7752 (* 1 = 7.7752 loss)
I0523 04:19:07.504618 34682 sgd_solver.cpp:112] Iteration 48020, lr = 0.01
I0523 04:19:12.041581 34682 solver.cpp:239] Iteration 48030 (2.16495 iter/s, 4.61905s/10 iters), loss = 8.10437
I0523 04:19:12.041646 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10437 (* 1 = 8.10437 loss)
I0523 04:19:12.907440 34682 sgd_solver.cpp:112] Iteration 48030, lr = 0.01
I0523 04:19:20.706604 34682 solver.cpp:239] Iteration 48040 (1.15412 iter/s, 8.6646s/10 iters), loss = 9.15482
I0523 04:19:20.706928 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15482 (* 1 = 9.15482 loss)
I0523 04:19:21.415457 34682 sgd_solver.cpp:112] Iteration 48040, lr = 0.01
I0523 04:19:26.397145 34682 solver.cpp:239] Iteration 48050 (1.75746 iter/s, 5.69002s/10 iters), loss = 8.9582
I0523 04:19:26.397197 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9582 (* 1 = 8.9582 loss)
I0523 04:19:26.477556 34682 sgd_solver.cpp:112] Iteration 48050, lr = 0.01
I0523 04:19:30.744169 34682 solver.cpp:239] Iteration 48060 (2.30055 iter/s, 4.34679s/10 iters), loss = 8.34444
I0523 04:19:30.744220 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34444 (* 1 = 8.34444 loss)
I0523 04:19:31.586987 34682 sgd_solver.cpp:112] Iteration 48060, lr = 0.01
I0523 04:19:37.714000 34682 solver.cpp:239] Iteration 48070 (1.43482 iter/s, 6.96949s/10 iters), loss = 7.63288
I0523 04:19:37.714062 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63288 (* 1 = 7.63288 loss)
I0523 04:19:37.779358 34682 sgd_solver.cpp:112] Iteration 48070, lr = 0.01
I0523 04:19:41.082715 34682 solver.cpp:239] Iteration 48080 (2.9687 iter/s, 3.36848s/10 iters), loss = 8.11909
I0523 04:19:41.082764 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11909 (* 1 = 8.11909 loss)
I0523 04:19:41.141320 34682 sgd_solver.cpp:112] Iteration 48080, lr = 0.01
I0523 04:19:46.661769 34682 solver.cpp:239] Iteration 48090 (1.79251 iter/s, 5.57878s/10 iters), loss = 8.65971
I0523 04:19:46.661818 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65971 (* 1 = 8.65971 loss)
I0523 04:19:47.176578 34682 sgd_solver.cpp:112] Iteration 48090, lr = 0.01
I0523 04:19:52.233322 34682 solver.cpp:239] Iteration 48100 (1.79492 iter/s, 5.57127s/10 iters), loss = 8.69351
I0523 04:19:52.233542 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69351 (* 1 = 8.69351 loss)
I0523 04:19:52.313077 34682 sgd_solver.cpp:112] Iteration 48100, lr = 0.01
I0523 04:19:57.822875 34682 solver.cpp:239] Iteration 48110 (1.78919 iter/s, 5.58912s/10 iters), loss = 8.21189
I0523 04:19:57.822923 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21189 (* 1 = 8.21189 loss)
I0523 04:19:57.888134 34682 sgd_solver.cpp:112] Iteration 48110, lr = 0.01
I0523 04:20:01.631835 34682 solver.cpp:239] Iteration 48120 (2.62553 iter/s, 3.80875s/10 iters), loss = 9.36227
I0523 04:20:01.631882 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36227 (* 1 = 9.36227 loss)
I0523 04:20:02.399003 34682 sgd_solver.cpp:112] Iteration 48120, lr = 0.01
I0523 04:20:08.928468 34682 solver.cpp:239] Iteration 48130 (1.37056 iter/s, 7.2963s/10 iters), loss = 9.09239
I0523 04:20:08.928510 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09239 (* 1 = 9.09239 loss)
I0523 04:20:08.994098 34682 sgd_solver.cpp:112] Iteration 48130, lr = 0.01
I0523 04:20:14.088635 34682 solver.cpp:239] Iteration 48140 (1.93802 iter/s, 5.15991s/10 iters), loss = 8.82256
I0523 04:20:14.088682 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82256 (* 1 = 8.82256 loss)
I0523 04:20:14.155576 34682 sgd_solver.cpp:112] Iteration 48140, lr = 0.01
I0523 04:20:20.341087 34682 solver.cpp:239] Iteration 48150 (1.59945 iter/s, 6.25215s/10 iters), loss = 8.90142
I0523 04:20:20.341140 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90142 (* 1 = 8.90142 loss)
I0523 04:20:20.404357 34682 sgd_solver.cpp:112] Iteration 48150, lr = 0.01
I0523 04:20:25.123878 34682 solver.cpp:239] Iteration 48160 (2.09094 iter/s, 4.78254s/10 iters), loss = 8.7132
I0523 04:20:25.124123 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7132 (* 1 = 8.7132 loss)
I0523 04:20:25.193795 34682 sgd_solver.cpp:112] Iteration 48160, lr = 0.01
I0523 04:20:29.888358 34682 solver.cpp:239] Iteration 48170 (2.09905 iter/s, 4.76407s/10 iters), loss = 8.17925
I0523 04:20:29.888399 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17925 (* 1 = 8.17925 loss)
I0523 04:20:29.954141 34682 sgd_solver.cpp:112] Iteration 48170, lr = 0.01
I0523 04:20:36.361225 34682 solver.cpp:239] Iteration 48180 (1.54498 iter/s, 6.47256s/10 iters), loss = 8.20748
I0523 04:20:36.361300 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20748 (* 1 = 8.20748 loss)
I0523 04:20:37.158828 34682 sgd_solver.cpp:112] Iteration 48180, lr = 0.01
I0523 04:20:42.036554 34682 solver.cpp:239] Iteration 48190 (1.76211 iter/s, 5.67502s/10 iters), loss = 8.35909
I0523 04:20:42.036617 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35909 (* 1 = 8.35909 loss)
I0523 04:20:42.094408 34682 sgd_solver.cpp:112] Iteration 48190, lr = 0.01
I0523 04:20:46.531042 34682 solver.cpp:239] Iteration 48200 (2.22507 iter/s, 4.49425s/10 iters), loss = 8.30924
I0523 04:20:46.531085 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30924 (* 1 = 8.30924 loss)
I0523 04:20:46.596235 34682 sgd_solver.cpp:112] Iteration 48200, lr = 0.01
I0523 04:20:49.897683 34682 solver.cpp:239] Iteration 48210 (2.97049 iter/s, 3.36645s/10 iters), loss = 8.81728
I0523 04:20:49.897729 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81728 (* 1 = 8.81728 loss)
I0523 04:20:49.966738 34682 sgd_solver.cpp:112] Iteration 48210, lr = 0.01
I0523 04:20:55.546865 34682 solver.cpp:239] Iteration 48220 (1.77025 iter/s, 5.64891s/10 iters), loss = 8.2643
I0523 04:20:55.547072 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2643 (* 1 = 8.2643 loss)
I0523 04:20:55.623766 34682 sgd_solver.cpp:112] Iteration 48220, lr = 0.01
I0523 04:21:01.302222 34682 solver.cpp:239] Iteration 48230 (1.73895 iter/s, 5.75061s/10 iters), loss = 8.66261
I0523 04:21:01.302275 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66261 (* 1 = 8.66261 loss)
I0523 04:21:01.370669 34682 sgd_solver.cpp:112] Iteration 48230, lr = 0.01
I0523 04:21:05.015151 34682 solver.cpp:239] Iteration 48240 (2.69345 iter/s, 3.71271s/10 iters), loss = 8.44216
I0523 04:21:05.015209 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44216 (* 1 = 8.44216 loss)
I0523 04:21:05.652696 34682 sgd_solver.cpp:112] Iteration 48240, lr = 0.01
I0523 04:21:10.566833 34682 solver.cpp:239] Iteration 48250 (1.80135 iter/s, 5.5514s/10 iters), loss = 8.53525
I0523 04:21:10.566881 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53525 (* 1 = 8.53525 loss)
I0523 04:21:11.078574 34682 sgd_solver.cpp:112] Iteration 48250, lr = 0.01
I0523 04:21:14.984107 34682 solver.cpp:239] Iteration 48260 (2.26396 iter/s, 4.41704s/10 iters), loss = 8.08806
I0523 04:21:14.984158 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08806 (* 1 = 8.08806 loss)
I0523 04:21:15.762707 34682 sgd_solver.cpp:112] Iteration 48260, lr = 0.01
I0523 04:21:18.412183 34682 solver.cpp:239] Iteration 48270 (2.91726 iter/s, 3.42787s/10 iters), loss = 9.35122
I0523 04:21:18.412245 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.35122 (* 1 = 9.35122 loss)
I0523 04:21:18.748440 34682 sgd_solver.cpp:112] Iteration 48270, lr = 0.01
I0523 04:21:21.297617 34682 solver.cpp:239] Iteration 48280 (3.46591 iter/s, 2.88525s/10 iters), loss = 8.4985
I0523 04:21:21.297657 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4985 (* 1 = 8.4985 loss)
I0523 04:21:21.378696 34682 sgd_solver.cpp:112] Iteration 48280, lr = 0.01
I0523 04:21:26.038283 34682 solver.cpp:239] Iteration 48290 (2.10951 iter/s, 4.74043s/10 iters), loss = 8.00017
I0523 04:21:26.038532 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00017 (* 1 = 8.00017 loss)
I0523 04:21:26.098824 34682 sgd_solver.cpp:112] Iteration 48290, lr = 0.01
I0523 04:21:29.517278 34682 solver.cpp:239] Iteration 48300 (2.87469 iter/s, 3.47863s/10 iters), loss = 9.07018
I0523 04:21:29.517326 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07018 (* 1 = 9.07018 loss)
I0523 04:21:30.234735 34682 sgd_solver.cpp:112] Iteration 48300, lr = 0.01
I0523 04:21:35.015588 34682 solver.cpp:239] Iteration 48310 (1.81884 iter/s, 5.49802s/10 iters), loss = 8.4392
I0523 04:21:35.015642 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4392 (* 1 = 8.4392 loss)
I0523 04:21:35.077143 34682 sgd_solver.cpp:112] Iteration 48310, lr = 0.01
I0523 04:21:38.557731 34682 solver.cpp:239] Iteration 48320 (2.82331 iter/s, 3.54194s/10 iters), loss = 8.42159
I0523 04:21:38.557785 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42159 (* 1 = 8.42159 loss)
I0523 04:21:39.231957 34682 sgd_solver.cpp:112] Iteration 48320, lr = 0.01
I0523 04:21:43.720912 34682 solver.cpp:239] Iteration 48330 (1.93689 iter/s, 5.16292s/10 iters), loss = 8.4068
I0523 04:21:43.720955 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4068 (* 1 = 8.4068 loss)
I0523 04:21:44.550966 34682 sgd_solver.cpp:112] Iteration 48330, lr = 0.01
I0523 04:21:46.379210 34682 solver.cpp:239] Iteration 48340 (3.76204 iter/s, 2.65813s/10 iters), loss = 8.32861
I0523 04:21:46.379274 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32861 (* 1 = 8.32861 loss)
I0523 04:21:47.231999 34682 sgd_solver.cpp:112] Iteration 48340, lr = 0.01
I0523 04:21:52.539932 34682 solver.cpp:239] Iteration 48350 (1.62327 iter/s, 6.1604s/10 iters), loss = 9.21901
I0523 04:21:52.539988 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21901 (* 1 = 9.21901 loss)
I0523 04:21:53.367254 34682 sgd_solver.cpp:112] Iteration 48350, lr = 0.01
I0523 04:21:56.746168 34682 solver.cpp:239] Iteration 48360 (2.37755 iter/s, 4.20601s/10 iters), loss = 8.27547
I0523 04:21:56.746404 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27547 (* 1 = 8.27547 loss)
I0523 04:21:57.543860 34682 sgd_solver.cpp:112] Iteration 48360, lr = 0.01
I0523 04:22:01.917244 34682 solver.cpp:239] Iteration 48370 (1.93399 iter/s, 5.17065s/10 iters), loss = 9.15414
I0523 04:22:01.917297 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15414 (* 1 = 9.15414 loss)
I0523 04:22:02.667934 34682 sgd_solver.cpp:112] Iteration 48370, lr = 0.01
I0523 04:22:07.293694 34682 solver.cpp:239] Iteration 48380 (1.86006 iter/s, 5.37617s/10 iters), loss = 8.94716
I0523 04:22:07.293772 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94716 (* 1 = 8.94716 loss)
I0523 04:22:08.010926 34682 sgd_solver.cpp:112] Iteration 48380, lr = 0.01
I0523 04:22:13.635519 34682 solver.cpp:239] Iteration 48390 (1.57692 iter/s, 6.34148s/10 iters), loss = 8.62395
I0523 04:22:13.635583 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62395 (* 1 = 8.62395 loss)
I0523 04:22:14.233155 34682 sgd_solver.cpp:112] Iteration 48390, lr = 0.01
I0523 04:22:20.775987 34682 solver.cpp:239] Iteration 48400 (1.40054 iter/s, 7.14012s/10 iters), loss = 8.88933
I0523 04:22:20.776039 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88933 (* 1 = 8.88933 loss)
I0523 04:22:21.421319 34682 sgd_solver.cpp:112] Iteration 48400, lr = 0.01
I0523 04:22:26.588865 34682 solver.cpp:239] Iteration 48410 (1.7204 iter/s, 5.81259s/10 iters), loss = 8.70165
I0523 04:22:26.588908 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70165 (* 1 = 8.70165 loss)
I0523 04:22:26.666251 34682 sgd_solver.cpp:112] Iteration 48410, lr = 0.01
I0523 04:22:30.826833 34682 solver.cpp:239] Iteration 48420 (2.35975 iter/s, 4.23775s/10 iters), loss = 9.01444
I0523 04:22:30.827082 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01444 (* 1 = 9.01444 loss)
I0523 04:22:31.685050 34682 sgd_solver.cpp:112] Iteration 48420, lr = 0.01
I0523 04:22:35.118835 34682 solver.cpp:239] Iteration 48430 (2.33013 iter/s, 4.2916s/10 iters), loss = 8.36088
I0523 04:22:35.118896 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36088 (* 1 = 8.36088 loss)
I0523 04:22:35.894620 34682 sgd_solver.cpp:112] Iteration 48430, lr = 0.01
I0523 04:22:38.660117 34682 solver.cpp:239] Iteration 48440 (2.82402 iter/s, 3.54105s/10 iters), loss = 8.75542
I0523 04:22:38.660163 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75542 (* 1 = 8.75542 loss)
I0523 04:22:38.734733 34682 sgd_solver.cpp:112] Iteration 48440, lr = 0.01
I0523 04:22:43.850944 34682 solver.cpp:239] Iteration 48450 (1.92657 iter/s, 5.19056s/10 iters), loss = 8.48287
I0523 04:22:43.850994 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48287 (* 1 = 8.48287 loss)
I0523 04:22:44.664474 34682 sgd_solver.cpp:112] Iteration 48450, lr = 0.01
I0523 04:22:48.877774 34682 solver.cpp:239] Iteration 48460 (1.98943 iter/s, 5.02657s/10 iters), loss = 7.68892
I0523 04:22:48.877832 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68892 (* 1 = 7.68892 loss)
I0523 04:22:49.700573 34682 sgd_solver.cpp:112] Iteration 48460, lr = 0.01
I0523 04:22:53.907685 34682 solver.cpp:239] Iteration 48470 (1.98821 iter/s, 5.02965s/10 iters), loss = 8.31894
I0523 04:22:53.907734 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31894 (* 1 = 8.31894 loss)
I0523 04:22:53.980937 34682 sgd_solver.cpp:112] Iteration 48470, lr = 0.01
I0523 04:22:58.012610 34682 solver.cpp:239] Iteration 48480 (2.43623 iter/s, 4.1047s/10 iters), loss = 9.06098
I0523 04:22:58.012657 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06098 (* 1 = 9.06098 loss)
I0523 04:22:58.086099 34682 sgd_solver.cpp:112] Iteration 48480, lr = 0.01
I0523 04:23:02.322212 34682 solver.cpp:239] Iteration 48490 (2.32052 iter/s, 4.30938s/10 iters), loss = 8.58437
I0523 04:23:02.322377 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58437 (* 1 = 8.58437 loss)
I0523 04:23:03.183884 34682 sgd_solver.cpp:112] Iteration 48490, lr = 0.01
I0523 04:23:06.437530 34682 solver.cpp:239] Iteration 48500 (2.43015 iter/s, 4.11497s/10 iters), loss = 8.52599
I0523 04:23:06.437589 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52599 (* 1 = 8.52599 loss)
I0523 04:23:07.287866 34682 sgd_solver.cpp:112] Iteration 48500, lr = 0.01
I0523 04:23:12.544193 34682 solver.cpp:239] Iteration 48510 (1.63764 iter/s, 6.10636s/10 iters), loss = 8.7054
I0523 04:23:12.544243 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7054 (* 1 = 8.7054 loss)
I0523 04:23:13.317116 34682 sgd_solver.cpp:112] Iteration 48510, lr = 0.01
I0523 04:23:16.690779 34682 solver.cpp:239] Iteration 48520 (2.41175 iter/s, 4.14636s/10 iters), loss = 8.567
I0523 04:23:16.690824 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.567 (* 1 = 8.567 loss)
I0523 04:23:17.377902 34682 sgd_solver.cpp:112] Iteration 48520, lr = 0.01
I0523 04:23:22.871166 34682 solver.cpp:239] Iteration 48530 (1.6181 iter/s, 6.18009s/10 iters), loss = 8.35003
I0523 04:23:22.871214 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35003 (* 1 = 8.35003 loss)
I0523 04:23:23.603338 34682 sgd_solver.cpp:112] Iteration 48530, lr = 0.01
I0523 04:23:27.800967 34682 solver.cpp:239] Iteration 48540 (2.02859 iter/s, 4.92954s/10 iters), loss = 9.10385
I0523 04:23:27.801030 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10385 (* 1 = 9.10385 loss)
I0523 04:23:27.860867 34682 sgd_solver.cpp:112] Iteration 48540, lr = 0.01
I0523 04:23:32.658891 34682 solver.cpp:239] Iteration 48550 (2.0586 iter/s, 4.85766s/10 iters), loss = 7.96865
I0523 04:23:32.659112 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96865 (* 1 = 7.96865 loss)
I0523 04:23:33.323318 34682 sgd_solver.cpp:112] Iteration 48550, lr = 0.01
I0523 04:23:35.243014 34682 solver.cpp:239] Iteration 48560 (3.87028 iter/s, 2.58379s/10 iters), loss = 9.48868
I0523 04:23:35.243062 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.48868 (* 1 = 9.48868 loss)
I0523 04:23:36.111574 34682 sgd_solver.cpp:112] Iteration 48560, lr = 0.01
I0523 04:23:39.412652 34682 solver.cpp:239] Iteration 48570 (2.39842 iter/s, 4.16941s/10 iters), loss = 8.32883
I0523 04:23:39.412703 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32883 (* 1 = 8.32883 loss)
I0523 04:23:39.476467 34682 sgd_solver.cpp:112] Iteration 48570, lr = 0.01
I0523 04:23:44.195441 34682 solver.cpp:239] Iteration 48580 (2.09094 iter/s, 4.78254s/10 iters), loss = 8.8148
I0523 04:23:44.195484 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8148 (* 1 = 8.8148 loss)
I0523 04:23:44.264793 34682 sgd_solver.cpp:112] Iteration 48580, lr = 0.01
I0523 04:23:48.443692 34682 solver.cpp:239] Iteration 48590 (2.35403 iter/s, 4.24803s/10 iters), loss = 8.44175
I0523 04:23:48.443739 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44175 (* 1 = 8.44175 loss)
I0523 04:23:48.502475 34682 sgd_solver.cpp:112] Iteration 48590, lr = 0.01
I0523 04:23:53.440994 34682 solver.cpp:239] Iteration 48600 (2.00118 iter/s, 4.99705s/10 iters), loss = 8.11151
I0523 04:23:53.441046 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11151 (* 1 = 8.11151 loss)
I0523 04:23:53.507390 34682 sgd_solver.cpp:112] Iteration 48600, lr = 0.01
I0523 04:23:58.314591 34682 solver.cpp:239] Iteration 48610 (2.05198 iter/s, 4.87334s/10 iters), loss = 8.6484
I0523 04:23:58.314642 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6484 (* 1 = 8.6484 loss)
I0523 04:23:59.146786 34682 sgd_solver.cpp:112] Iteration 48610, lr = 0.01
I0523 04:24:04.098487 34682 solver.cpp:239] Iteration 48620 (1.72902 iter/s, 5.78361s/10 iters), loss = 8.97343
I0523 04:24:04.098656 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97343 (* 1 = 8.97343 loss)
I0523 04:24:04.175694 34682 sgd_solver.cpp:112] Iteration 48620, lr = 0.01
I0523 04:24:09.058197 34682 solver.cpp:239] Iteration 48630 (2.0164 iter/s, 4.95935s/10 iters), loss = 8.6007
I0523 04:24:09.058239 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6007 (* 1 = 8.6007 loss)
I0523 04:24:09.121050 34682 sgd_solver.cpp:112] Iteration 48630, lr = 0.01
I0523 04:24:12.551690 34682 solver.cpp:239] Iteration 48640 (2.86263 iter/s, 3.4933s/10 iters), loss = 7.91406
I0523 04:24:12.551749 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91406 (* 1 = 7.91406 loss)
I0523 04:24:13.366526 34682 sgd_solver.cpp:112] Iteration 48640, lr = 0.01
I0523 04:24:20.305337 34682 solver.cpp:239] Iteration 48650 (1.28978 iter/s, 7.75328s/10 iters), loss = 8.91562
I0523 04:24:20.305378 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91562 (* 1 = 8.91562 loss)
I0523 04:24:20.360877 34682 sgd_solver.cpp:112] Iteration 48650, lr = 0.01
I0523 04:24:26.349895 34682 solver.cpp:239] Iteration 48660 (1.65446 iter/s, 6.04426s/10 iters), loss = 7.63271
I0523 04:24:26.349947 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63271 (* 1 = 7.63271 loss)
I0523 04:24:27.128597 34682 sgd_solver.cpp:112] Iteration 48660, lr = 0.01
I0523 04:24:31.918509 34682 solver.cpp:239] Iteration 48670 (1.79587 iter/s, 5.56834s/10 iters), loss = 8.59215
I0523 04:24:31.918555 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59215 (* 1 = 8.59215 loss)
I0523 04:24:31.991389 34682 sgd_solver.cpp:112] Iteration 48670, lr = 0.01
I0523 04:24:35.997048 34682 solver.cpp:239] Iteration 48680 (2.45199 iter/s, 4.07832s/10 iters), loss = 8.56529
I0523 04:24:35.997278 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56529 (* 1 = 8.56529 loss)
I0523 04:24:36.064909 34682 sgd_solver.cpp:112] Iteration 48680, lr = 0.01
I0523 04:24:40.216509 34682 solver.cpp:239] Iteration 48690 (2.37019 iter/s, 4.21907s/10 iters), loss = 9.13926
I0523 04:24:40.216552 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13926 (* 1 = 9.13926 loss)
I0523 04:24:40.284365 34682 sgd_solver.cpp:112] Iteration 48690, lr = 0.01
I0523 04:24:43.900902 34682 solver.cpp:239] Iteration 48700 (2.7143 iter/s, 3.6842s/10 iters), loss = 8.8134
I0523 04:24:43.900949 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8134 (* 1 = 8.8134 loss)
I0523 04:24:43.973394 34682 sgd_solver.cpp:112] Iteration 48700, lr = 0.01
I0523 04:24:49.048331 34682 solver.cpp:239] Iteration 48710 (1.94281 iter/s, 5.14717s/10 iters), loss = 7.94527
I0523 04:24:49.048401 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94527 (* 1 = 7.94527 loss)
I0523 04:24:49.873678 34682 sgd_solver.cpp:112] Iteration 48710, lr = 0.01
I0523 04:24:54.832793 34682 solver.cpp:239] Iteration 48720 (1.72886 iter/s, 5.78416s/10 iters), loss = 9.19418
I0523 04:24:54.832844 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19418 (* 1 = 9.19418 loss)
I0523 04:24:55.692855 34682 sgd_solver.cpp:112] Iteration 48720, lr = 0.01
I0523 04:25:00.428465 34682 solver.cpp:239] Iteration 48730 (1.78718 iter/s, 5.59539s/10 iters), loss = 9.40059
I0523 04:25:00.428514 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40059 (* 1 = 9.40059 loss)
I0523 04:25:01.209537 34682 sgd_solver.cpp:112] Iteration 48730, lr = 0.01
I0523 04:25:05.498147 34682 solver.cpp:239] Iteration 48740 (1.97261 iter/s, 5.06943s/10 iters), loss = 8.20315
I0523 04:25:05.498208 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20315 (* 1 = 8.20315 loss)
I0523 04:25:05.561187 34682 sgd_solver.cpp:112] Iteration 48740, lr = 0.01
I0523 04:25:11.374621 34682 solver.cpp:239] Iteration 48750 (1.70179 iter/s, 5.87617s/10 iters), loss = 9.83736
I0523 04:25:11.374903 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.83736 (* 1 = 9.83736 loss)
I0523 04:25:12.220888 34682 sgd_solver.cpp:112] Iteration 48750, lr = 0.01
I0523 04:25:18.172294 34682 solver.cpp:239] Iteration 48760 (1.47121 iter/s, 6.79714s/10 iters), loss = 8.96215
I0523 04:25:18.172343 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96215 (* 1 = 8.96215 loss)
I0523 04:25:18.868050 34682 sgd_solver.cpp:112] Iteration 48760, lr = 0.01
I0523 04:25:22.483249 34682 solver.cpp:239] Iteration 48770 (2.31979 iter/s, 4.31073s/10 iters), loss = 9.23022
I0523 04:25:22.483290 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23022 (* 1 = 9.23022 loss)
I0523 04:25:22.564178 34682 sgd_solver.cpp:112] Iteration 48770, lr = 0.01
I0523 04:25:28.755789 34682 solver.cpp:239] Iteration 48780 (1.59433 iter/s, 6.27223s/10 iters), loss = 8.54785
I0523 04:25:28.755869 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54785 (* 1 = 8.54785 loss)
I0523 04:25:29.474530 34682 sgd_solver.cpp:112] Iteration 48780, lr = 0.01
I0523 04:25:33.638393 34682 solver.cpp:239] Iteration 48790 (2.0482 iter/s, 4.88233s/10 iters), loss = 9.15885
I0523 04:25:33.638439 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15885 (* 1 = 9.15885 loss)
I0523 04:25:34.439832 34682 sgd_solver.cpp:112] Iteration 48790, lr = 0.01
I0523 04:25:39.774276 34682 solver.cpp:239] Iteration 48800 (1.62984 iter/s, 6.13559s/10 iters), loss = 8.02659
I0523 04:25:39.774330 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02659 (* 1 = 8.02659 loss)
I0523 04:25:40.011729 34682 sgd_solver.cpp:112] Iteration 48800, lr = 0.01
I0523 04:25:44.011626 34682 solver.cpp:239] Iteration 48810 (2.3601 iter/s, 4.23712s/10 iters), loss = 7.80832
I0523 04:25:44.011812 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80832 (* 1 = 7.80832 loss)
I0523 04:25:44.823073 34682 sgd_solver.cpp:112] Iteration 48810, lr = 0.01
I0523 04:25:50.249997 34682 solver.cpp:239] Iteration 48820 (1.60309 iter/s, 6.23794s/10 iters), loss = 8.4535
I0523 04:25:50.250043 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4535 (* 1 = 8.4535 loss)
I0523 04:25:50.320658 34682 sgd_solver.cpp:112] Iteration 48820, lr = 0.01
I0523 04:25:54.412547 34682 solver.cpp:239] Iteration 48830 (2.4025 iter/s, 4.16233s/10 iters), loss = 9.00378
I0523 04:25:54.412611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00378 (* 1 = 9.00378 loss)
I0523 04:25:55.015235 34682 sgd_solver.cpp:112] Iteration 48830, lr = 0.01
I0523 04:25:59.336912 34682 solver.cpp:239] Iteration 48840 (2.03083 iter/s, 4.9241s/10 iters), loss = 8.88564
I0523 04:25:59.336961 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88564 (* 1 = 8.88564 loss)
I0523 04:26:00.199203 34682 sgd_solver.cpp:112] Iteration 48840, lr = 0.01
I0523 04:26:04.861665 34682 solver.cpp:239] Iteration 48850 (1.81013 iter/s, 5.52448s/10 iters), loss = 10.2608
I0523 04:26:04.861726 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.2608 (* 1 = 10.2608 loss)
I0523 04:26:04.931396 34682 sgd_solver.cpp:112] Iteration 48850, lr = 0.01
I0523 04:26:09.549937 34682 solver.cpp:239] Iteration 48860 (2.1331 iter/s, 4.68802s/10 iters), loss = 9.14765
I0523 04:26:09.549991 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14765 (* 1 = 9.14765 loss)
I0523 04:26:09.622454 34682 sgd_solver.cpp:112] Iteration 48860, lr = 0.01
I0523 04:26:14.503108 34682 solver.cpp:239] Iteration 48870 (2.01901 iter/s, 4.95291s/10 iters), loss = 9.3227
I0523 04:26:14.503221 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3227 (* 1 = 9.3227 loss)
I0523 04:26:14.573441 34682 sgd_solver.cpp:112] Iteration 48870, lr = 0.01
I0523 04:26:18.762859 34682 solver.cpp:239] Iteration 48880 (2.34772 iter/s, 4.25945s/10 iters), loss = 8.58304
I0523 04:26:18.762905 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58304 (* 1 = 8.58304 loss)
I0523 04:26:18.989598 34682 sgd_solver.cpp:112] Iteration 48880, lr = 0.01
I0523 04:26:24.332836 34682 solver.cpp:239] Iteration 48890 (1.79543 iter/s, 5.5697s/10 iters), loss = 8.6452
I0523 04:26:24.332890 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6452 (* 1 = 8.6452 loss)
I0523 04:26:24.407554 34682 sgd_solver.cpp:112] Iteration 48890, lr = 0.01
I0523 04:26:27.874311 34682 solver.cpp:239] Iteration 48900 (2.82384 iter/s, 3.54128s/10 iters), loss = 7.64588
I0523 04:26:27.874357 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64588 (* 1 = 7.64588 loss)
I0523 04:26:28.671917 34682 sgd_solver.cpp:112] Iteration 48900, lr = 0.01
I0523 04:26:33.933073 34682 solver.cpp:239] Iteration 48910 (1.65058 iter/s, 6.05847s/10 iters), loss = 9.3044
I0523 04:26:33.933126 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3044 (* 1 = 9.3044 loss)
I0523 04:26:34.636778 34682 sgd_solver.cpp:112] Iteration 48910, lr = 0.01
I0523 04:26:41.144995 34682 solver.cpp:239] Iteration 48920 (1.38666 iter/s, 7.21158s/10 iters), loss = 8.15115
I0523 04:26:41.145040 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15115 (* 1 = 8.15115 loss)
I0523 04:26:41.301677 34682 sgd_solver.cpp:112] Iteration 48920, lr = 0.01
I0523 04:26:45.170169 34682 solver.cpp:239] Iteration 48930 (2.4845 iter/s, 4.02495s/10 iters), loss = 9.06703
I0523 04:26:45.170336 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06703 (* 1 = 9.06703 loss)
I0523 04:26:45.232767 34682 sgd_solver.cpp:112] Iteration 48930, lr = 0.01
I0523 04:26:50.041709 34682 solver.cpp:239] Iteration 48940 (2.05289 iter/s, 4.87118s/10 iters), loss = 9.31889
I0523 04:26:50.041764 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31889 (* 1 = 9.31889 loss)
I0523 04:26:50.100750 34682 sgd_solver.cpp:112] Iteration 48940, lr = 0.01
I0523 04:26:52.732791 34682 solver.cpp:239] Iteration 48950 (3.7162 iter/s, 2.69092s/10 iters), loss = 8.88798
I0523 04:26:52.732841 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88798 (* 1 = 8.88798 loss)
I0523 04:26:52.809404 34682 sgd_solver.cpp:112] Iteration 48950, lr = 0.01
I0523 04:26:56.161682 34682 solver.cpp:239] Iteration 48960 (2.91656 iter/s, 3.4287s/10 iters), loss = 8.91269
I0523 04:26:56.161736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91269 (* 1 = 8.91269 loss)
I0523 04:26:56.898228 34682 sgd_solver.cpp:112] Iteration 48960, lr = 0.01
I0523 04:27:00.969611 34682 solver.cpp:239] Iteration 48970 (2.08003 iter/s, 4.80762s/10 iters), loss = 9.04309
I0523 04:27:00.969684 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04309 (* 1 = 9.04309 loss)
I0523 04:27:01.789196 34682 sgd_solver.cpp:112] Iteration 48970, lr = 0.01
I0523 04:27:06.707844 34682 solver.cpp:239] Iteration 48980 (1.74279 iter/s, 5.73793s/10 iters), loss = 8.69116
I0523 04:27:06.707896 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69116 (* 1 = 8.69116 loss)
I0523 04:27:06.768136 34682 sgd_solver.cpp:112] Iteration 48980, lr = 0.01
I0523 04:27:12.495623 34682 solver.cpp:239] Iteration 48990 (1.72786 iter/s, 5.7875s/10 iters), loss = 9.59254
I0523 04:27:12.495672 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.59254 (* 1 = 9.59254 loss)
I0523 04:27:12.554529 34682 sgd_solver.cpp:112] Iteration 48990, lr = 0.01
I0523 04:27:18.025887 34682 solver.cpp:239] Iteration 49000 (1.80832 iter/s, 5.52999s/10 iters), loss = 8.42351
I0523 04:27:18.026181 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42351 (* 1 = 8.42351 loss)
I0523 04:27:18.089283 34682 sgd_solver.cpp:112] Iteration 49000, lr = 0.01
I0523 04:27:22.063455 34682 solver.cpp:239] Iteration 49010 (2.47701 iter/s, 4.03713s/10 iters), loss = 8.35451
I0523 04:27:22.063508 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35451 (* 1 = 8.35451 loss)
I0523 04:27:22.949103 34682 sgd_solver.cpp:112] Iteration 49010, lr = 0.01
I0523 04:27:29.346371 34682 solver.cpp:239] Iteration 49020 (1.37314 iter/s, 7.28257s/10 iters), loss = 8.10196
I0523 04:27:29.346421 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10196 (* 1 = 8.10196 loss)
I0523 04:27:29.425132 34682 sgd_solver.cpp:112] Iteration 49020, lr = 0.01
I0523 04:27:33.220002 34682 solver.cpp:239] Iteration 49030 (2.5817 iter/s, 3.87342s/10 iters), loss = 9.08536
I0523 04:27:33.220049 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08536 (* 1 = 9.08536 loss)
I0523 04:27:34.053745 34682 sgd_solver.cpp:112] Iteration 49030, lr = 0.01
I0523 04:27:38.252491 34682 solver.cpp:239] Iteration 49040 (1.98719 iter/s, 5.03223s/10 iters), loss = 8.5378
I0523 04:27:38.252545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5378 (* 1 = 8.5378 loss)
I0523 04:27:38.335247 34682 sgd_solver.cpp:112] Iteration 49040, lr = 0.01
I0523 04:27:43.337122 34682 solver.cpp:239] Iteration 49050 (1.96681 iter/s, 5.08437s/10 iters), loss = 9.04991
I0523 04:27:43.337164 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04991 (* 1 = 9.04991 loss)
I0523 04:27:43.409721 34682 sgd_solver.cpp:112] Iteration 49050, lr = 0.01
I0523 04:27:48.117275 34682 solver.cpp:239] Iteration 49060 (2.09209 iter/s, 4.7799s/10 iters), loss = 7.84013
I0523 04:27:48.117403 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84013 (* 1 = 7.84013 loss)
I0523 04:27:48.872402 34682 sgd_solver.cpp:112] Iteration 49060, lr = 0.01
I0523 04:27:52.738646 34682 solver.cpp:239] Iteration 49070 (2.16401 iter/s, 4.62106s/10 iters), loss = 9.63994
I0523 04:27:52.738718 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.63994 (* 1 = 9.63994 loss)
I0523 04:27:52.807729 34682 sgd_solver.cpp:112] Iteration 49070, lr = 0.01
I0523 04:27:57.532933 34682 solver.cpp:239] Iteration 49080 (2.08592 iter/s, 4.79404s/10 iters), loss = 8.68277
I0523 04:27:57.532979 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68277 (* 1 = 8.68277 loss)
I0523 04:27:57.600226 34682 sgd_solver.cpp:112] Iteration 49080, lr = 0.01
I0523 04:28:01.971554 34682 solver.cpp:239] Iteration 49090 (2.25307 iter/s, 4.4384s/10 iters), loss = 9.02276
I0523 04:28:01.971595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02276 (* 1 = 9.02276 loss)
I0523 04:28:02.032291 34682 sgd_solver.cpp:112] Iteration 49090, lr = 0.01
I0523 04:28:05.433389 34682 solver.cpp:239] Iteration 49100 (2.8888 iter/s, 3.46165s/10 iters), loss = 8.76531
I0523 04:28:05.433441 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76531 (* 1 = 8.76531 loss)
I0523 04:28:06.097322 34682 sgd_solver.cpp:112] Iteration 49100, lr = 0.01
I0523 04:28:09.515430 34682 solver.cpp:239] Iteration 49110 (2.44989 iter/s, 4.08182s/10 iters), loss = 8.4416
I0523 04:28:09.515476 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4416 (* 1 = 8.4416 loss)
I0523 04:28:09.579210 34682 sgd_solver.cpp:112] Iteration 49110, lr = 0.01
I0523 04:28:14.124621 34682 solver.cpp:239] Iteration 49120 (2.16969 iter/s, 4.60896s/10 iters), loss = 9.08609
I0523 04:28:14.124671 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08609 (* 1 = 9.08609 loss)
I0523 04:28:14.389891 34682 sgd_solver.cpp:112] Iteration 49120, lr = 0.01
I0523 04:28:17.796211 34682 solver.cpp:239] Iteration 49130 (2.72376 iter/s, 3.67139s/10 iters), loss = 8.50689
I0523 04:28:17.796255 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50689 (* 1 = 8.50689 loss)
I0523 04:28:17.859215 34682 sgd_solver.cpp:112] Iteration 49130, lr = 0.01
I0523 04:28:20.966282 34682 solver.cpp:239] Iteration 49140 (3.15468 iter/s, 3.16989s/10 iters), loss = 7.89085
I0523 04:28:20.966439 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89085 (* 1 = 7.89085 loss)
I0523 04:28:21.045544 34682 sgd_solver.cpp:112] Iteration 49140, lr = 0.01
I0523 04:28:24.790894 34682 solver.cpp:239] Iteration 49150 (2.61486 iter/s, 3.8243s/10 iters), loss = 8.53874
I0523 04:28:24.790941 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53874 (* 1 = 8.53874 loss)
I0523 04:28:24.860682 34682 sgd_solver.cpp:112] Iteration 49150, lr = 0.01
I0523 04:28:28.151160 34682 solver.cpp:239] Iteration 49160 (2.97612 iter/s, 3.36008s/10 iters), loss = 9.10298
I0523 04:28:28.151206 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10298 (* 1 = 9.10298 loss)
I0523 04:28:28.937623 34682 sgd_solver.cpp:112] Iteration 49160, lr = 0.01
I0523 04:28:33.306191 34682 solver.cpp:239] Iteration 49170 (1.93995 iter/s, 5.15477s/10 iters), loss = 8.55995
I0523 04:28:33.306237 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55995 (* 1 = 8.55995 loss)
I0523 04:28:33.369567 34682 sgd_solver.cpp:112] Iteration 49170, lr = 0.01
I0523 04:28:37.130954 34682 solver.cpp:239] Iteration 49180 (2.61469 iter/s, 3.82455s/10 iters), loss = 8.18204
I0523 04:28:37.131023 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18204 (* 1 = 8.18204 loss)
I0523 04:28:37.863404 34682 sgd_solver.cpp:112] Iteration 49180, lr = 0.01
I0523 04:28:42.860656 34682 solver.cpp:239] Iteration 49190 (1.74538 iter/s, 5.7294s/10 iters), loss = 8.90961
I0523 04:28:42.860707 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90961 (* 1 = 8.90961 loss)
I0523 04:28:42.929427 34682 sgd_solver.cpp:112] Iteration 49190, lr = 0.01
I0523 04:28:47.843415 34682 solver.cpp:239] Iteration 49200 (2.00702 iter/s, 4.98251s/10 iters), loss = 8.15267
I0523 04:28:47.843461 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15267 (* 1 = 8.15267 loss)
I0523 04:28:48.523723 34682 sgd_solver.cpp:112] Iteration 49200, lr = 0.01
I0523 04:28:55.037557 34682 solver.cpp:239] Iteration 49210 (1.39009 iter/s, 7.1938s/10 iters), loss = 9.2623
I0523 04:28:55.037796 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2623 (* 1 = 9.2623 loss)
I0523 04:28:55.682076 34682 sgd_solver.cpp:112] Iteration 49210, lr = 0.01
I0523 04:29:00.589228 34682 solver.cpp:239] Iteration 49220 (1.8014 iter/s, 5.55124s/10 iters), loss = 9.67197
I0523 04:29:00.589274 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.67197 (* 1 = 9.67197 loss)
I0523 04:29:00.650974 34682 sgd_solver.cpp:112] Iteration 49220, lr = 0.01
I0523 04:29:04.544432 34682 solver.cpp:239] Iteration 49230 (2.52845 iter/s, 3.95499s/10 iters), loss = 8.24252
I0523 04:29:04.544483 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24252 (* 1 = 8.24252 loss)
I0523 04:29:04.608392 34682 sgd_solver.cpp:112] Iteration 49230, lr = 0.01
I0523 04:29:08.759234 34682 solver.cpp:239] Iteration 49240 (2.37273 iter/s, 4.21456s/10 iters), loss = 9.16245
I0523 04:29:08.759295 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16245 (* 1 = 9.16245 loss)
I0523 04:29:09.636695 34682 sgd_solver.cpp:112] Iteration 49240, lr = 0.01
I0523 04:29:12.934947 34682 solver.cpp:239] Iteration 49250 (2.39493 iter/s, 4.17548s/10 iters), loss = 7.43758
I0523 04:29:12.934988 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43758 (* 1 = 7.43758 loss)
I0523 04:29:13.795635 34682 sgd_solver.cpp:112] Iteration 49250, lr = 0.01
I0523 04:29:17.757941 34682 solver.cpp:239] Iteration 49260 (2.0735 iter/s, 4.82276s/10 iters), loss = 8.71393
I0523 04:29:17.757990 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71393 (* 1 = 8.71393 loss)
I0523 04:29:18.263622 34682 sgd_solver.cpp:112] Iteration 49260, lr = 0.01
I0523 04:29:22.473119 34682 solver.cpp:239] Iteration 49270 (2.12092 iter/s, 4.71493s/10 iters), loss = 9.04439
I0523 04:29:22.473173 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04439 (* 1 = 9.04439 loss)
I0523 04:29:22.529242 34682 sgd_solver.cpp:112] Iteration 49270, lr = 0.01
I0523 04:29:26.810333 34682 solver.cpp:239] Iteration 49280 (2.30576 iter/s, 4.33697s/10 iters), loss = 8.42101
I0523 04:29:26.810616 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42101 (* 1 = 8.42101 loss)
I0523 04:29:27.648468 34682 sgd_solver.cpp:112] Iteration 49280, lr = 0.01
I0523 04:29:33.384829 34682 solver.cpp:239] Iteration 49290 (1.52115 iter/s, 6.57398s/10 iters), loss = 8.78651
I0523 04:29:33.384865 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78651 (* 1 = 8.78651 loss)
I0523 04:29:34.162330 34682 sgd_solver.cpp:112] Iteration 49290, lr = 0.01
I0523 04:29:38.315865 34682 solver.cpp:239] Iteration 49300 (2.02807 iter/s, 4.9308s/10 iters), loss = 8.48237
I0523 04:29:38.315912 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48237 (* 1 = 8.48237 loss)
I0523 04:29:38.388738 34682 sgd_solver.cpp:112] Iteration 49300, lr = 0.01
I0523 04:29:42.399204 34682 solver.cpp:239] Iteration 49310 (2.44911 iter/s, 4.08312s/10 iters), loss = 8.47065
I0523 04:29:42.399269 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47065 (* 1 = 8.47065 loss)
I0523 04:29:42.478688 34682 sgd_solver.cpp:112] Iteration 49310, lr = 0.01
I0523 04:29:46.600968 34682 solver.cpp:239] Iteration 49320 (2.38009 iter/s, 4.20152s/10 iters), loss = 9.18732
I0523 04:29:46.601016 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18732 (* 1 = 9.18732 loss)
I0523 04:29:46.917033 34682 sgd_solver.cpp:112] Iteration 49320, lr = 0.01
I0523 04:29:50.105626 34682 solver.cpp:239] Iteration 49330 (2.85351 iter/s, 3.50445s/10 iters), loss = 8.61555
I0523 04:29:50.105685 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61555 (* 1 = 8.61555 loss)
I0523 04:29:50.169236 34682 sgd_solver.cpp:112] Iteration 49330, lr = 0.01
I0523 04:29:53.496559 34682 solver.cpp:239] Iteration 49340 (2.95201 iter/s, 3.38753s/10 iters), loss = 8.31735
I0523 04:29:53.496631 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31735 (* 1 = 8.31735 loss)
I0523 04:29:54.316737 34682 sgd_solver.cpp:112] Iteration 49340, lr = 0.01
I0523 04:29:58.256462 34682 solver.cpp:239] Iteration 49350 (2.101 iter/s, 4.75963s/10 iters), loss = 9.07319
I0523 04:29:58.256697 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07319 (* 1 = 9.07319 loss)
I0523 04:29:58.335258 34682 sgd_solver.cpp:112] Iteration 49350, lr = 0.01
I0523 04:30:02.366973 34682 solver.cpp:239] Iteration 49360 (2.433 iter/s, 4.11015s/10 iters), loss = 8.58389
I0523 04:30:02.367029 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58389 (* 1 = 8.58389 loss)
I0523 04:30:02.446563 34682 sgd_solver.cpp:112] Iteration 49360, lr = 0.01
I0523 04:30:05.857750 34682 solver.cpp:239] Iteration 49370 (2.86486 iter/s, 3.49057s/10 iters), loss = 8.44391
I0523 04:30:05.857805 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44391 (* 1 = 8.44391 loss)
I0523 04:30:06.643903 34682 sgd_solver.cpp:112] Iteration 49370, lr = 0.01
I0523 04:30:12.080359 34682 solver.cpp:239] Iteration 49380 (1.60712 iter/s, 6.22231s/10 iters), loss = 8.1433
I0523 04:30:12.080409 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1433 (* 1 = 8.1433 loss)
I0523 04:30:12.154309 34682 sgd_solver.cpp:112] Iteration 49380, lr = 0.01
I0523 04:30:17.380476 34682 solver.cpp:239] Iteration 49390 (1.88685 iter/s, 5.29985s/10 iters), loss = 9.2036
I0523 04:30:17.380527 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2036 (* 1 = 9.2036 loss)
I0523 04:30:17.461205 34682 sgd_solver.cpp:112] Iteration 49390, lr = 0.01
I0523 04:30:23.007386 34682 solver.cpp:239] Iteration 49400 (1.77726 iter/s, 5.62664s/10 iters), loss = 8.49716
I0523 04:30:23.007431 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49716 (* 1 = 8.49716 loss)
I0523 04:30:23.080201 34682 sgd_solver.cpp:112] Iteration 49400, lr = 0.01
I0523 04:30:27.296419 34682 solver.cpp:239] Iteration 49410 (2.33165 iter/s, 4.28881s/10 iters), loss = 7.52248
I0523 04:30:27.296468 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.52248 (* 1 = 7.52248 loss)
I0523 04:30:27.361243 34682 sgd_solver.cpp:112] Iteration 49410, lr = 0.01
I0523 04:30:31.120599 34682 solver.cpp:239] Iteration 49420 (2.61508 iter/s, 3.82397s/10 iters), loss = 9.31896
I0523 04:30:31.120785 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31896 (* 1 = 9.31896 loss)
I0523 04:30:31.899837 34682 sgd_solver.cpp:112] Iteration 49420, lr = 0.01
I0523 04:30:37.661339 34682 solver.cpp:239] Iteration 49430 (1.52898 iter/s, 6.54029s/10 iters), loss = 9.72514
I0523 04:30:37.661384 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.72514 (* 1 = 9.72514 loss)
I0523 04:30:38.451967 34682 sgd_solver.cpp:112] Iteration 49430, lr = 0.01
I0523 04:30:42.538875 34682 solver.cpp:239] Iteration 49440 (2.05032 iter/s, 4.87729s/10 iters), loss = 8.56475
I0523 04:30:42.538926 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56475 (* 1 = 8.56475 loss)
I0523 04:30:42.608322 34682 sgd_solver.cpp:112] Iteration 49440, lr = 0.01
I0523 04:30:47.144266 34682 solver.cpp:239] Iteration 49450 (2.17148 iter/s, 4.60515s/10 iters), loss = 7.85508
I0523 04:30:47.144313 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.85508 (* 1 = 7.85508 loss)
I0523 04:30:47.205437 34682 sgd_solver.cpp:112] Iteration 49450, lr = 0.01
I0523 04:30:52.582587 34682 solver.cpp:239] Iteration 49460 (1.83889 iter/s, 5.43806s/10 iters), loss = 8.24181
I0523 04:30:52.582638 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24181 (* 1 = 8.24181 loss)
I0523 04:30:53.406860 34682 sgd_solver.cpp:112] Iteration 49460, lr = 0.01
I0523 04:30:58.325877 34682 solver.cpp:239] Iteration 49470 (1.74125 iter/s, 5.74301s/10 iters), loss = 9.25864
I0523 04:30:58.325924 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25864 (* 1 = 9.25864 loss)
I0523 04:30:58.406613 34682 sgd_solver.cpp:112] Iteration 49470, lr = 0.01
I0523 04:31:01.825639 34682 solver.cpp:239] Iteration 49480 (2.8575 iter/s, 3.49957s/10 iters), loss = 8.15049
I0523 04:31:01.825855 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15049 (* 1 = 8.15049 loss)
I0523 04:31:02.466764 34682 sgd_solver.cpp:112] Iteration 49480, lr = 0.01
I0523 04:31:07.386595 34682 solver.cpp:239] Iteration 49490 (1.79839 iter/s, 5.56053s/10 iters), loss = 8.34785
I0523 04:31:07.386653 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34785 (* 1 = 8.34785 loss)
I0523 04:31:08.184092 34682 sgd_solver.cpp:112] Iteration 49490, lr = 0.01
I0523 04:31:13.812414 34682 solver.cpp:239] Iteration 49500 (1.5563 iter/s, 6.42551s/10 iters), loss = 7.7366
I0523 04:31:13.812463 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7366 (* 1 = 7.7366 loss)
I0523 04:31:14.599681 34682 sgd_solver.cpp:112] Iteration 49500, lr = 0.01
I0523 04:31:19.163974 34682 solver.cpp:239] Iteration 49510 (1.86871 iter/s, 5.35129s/10 iters), loss = 8.39867
I0523 04:31:19.164021 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39867 (* 1 = 8.39867 loss)
I0523 04:31:19.235468 34682 sgd_solver.cpp:112] Iteration 49510, lr = 0.01
I0523 04:31:21.586894 34682 solver.cpp:239] Iteration 49520 (4.12752 iter/s, 2.42276s/10 iters), loss = 8.70518
I0523 04:31:21.586942 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70518 (* 1 = 8.70518 loss)
I0523 04:31:21.655611 34682 sgd_solver.cpp:112] Iteration 49520, lr = 0.01
I0523 04:31:24.990530 34682 solver.cpp:239] Iteration 49530 (2.9382 iter/s, 3.40344s/10 iters), loss = 9.00165
I0523 04:31:24.990576 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00165 (* 1 = 9.00165 loss)
I0523 04:31:25.063125 34682 sgd_solver.cpp:112] Iteration 49530, lr = 0.01
I0523 04:31:31.528460 34682 solver.cpp:239] Iteration 49540 (1.52961 iter/s, 6.53762s/10 iters), loss = 8.38731
I0523 04:31:31.528512 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38731 (* 1 = 8.38731 loss)
I0523 04:31:32.127916 34682 sgd_solver.cpp:112] Iteration 49540, lr = 0.01
I0523 04:31:36.068773 34682 solver.cpp:239] Iteration 49550 (2.20261 iter/s, 4.54007s/10 iters), loss = 8.37895
I0523 04:31:36.068825 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37895 (* 1 = 8.37895 loss)
I0523 04:31:36.142925 34682 sgd_solver.cpp:112] Iteration 49550, lr = 0.01
I0523 04:31:39.345314 34682 solver.cpp:239] Iteration 49560 (3.05217 iter/s, 3.27635s/10 iters), loss = 9.85774
I0523 04:31:39.345369 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.85774 (* 1 = 9.85774 loss)
I0523 04:31:39.407541 34682 sgd_solver.cpp:112] Iteration 49560, lr = 0.01
I0523 04:31:42.956616 34682 solver.cpp:239] Iteration 49570 (2.76925 iter/s, 3.61109s/10 iters), loss = 8.65022
I0523 04:31:42.956671 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65022 (* 1 = 8.65022 loss)
I0523 04:31:43.594301 34682 sgd_solver.cpp:112] Iteration 49570, lr = 0.01
I0523 04:31:46.923400 34682 solver.cpp:239] Iteration 49580 (2.52108 iter/s, 3.96656s/10 iters), loss = 8.70968
I0523 04:31:46.923449 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70968 (* 1 = 8.70968 loss)
I0523 04:31:46.991283 34682 sgd_solver.cpp:112] Iteration 49580, lr = 0.01
I0523 04:31:50.065613 34682 solver.cpp:239] Iteration 49590 (3.18265 iter/s, 3.14203s/10 iters), loss = 8.57066
I0523 04:31:50.065665 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57066 (* 1 = 8.57066 loss)
I0523 04:31:50.189612 34682 sgd_solver.cpp:112] Iteration 49590, lr = 0.01
I0523 04:31:54.691460 34682 solver.cpp:239] Iteration 49600 (2.16188 iter/s, 4.6256s/10 iters), loss = 8.33675
I0523 04:31:54.691505 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33675 (* 1 = 8.33675 loss)
I0523 04:31:55.310330 34682 sgd_solver.cpp:112] Iteration 49600, lr = 0.01
I0523 04:31:58.156482 34682 solver.cpp:239] Iteration 49610 (2.88615 iter/s, 3.46482s/10 iters), loss = 9.06189
I0523 04:31:58.156527 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06189 (* 1 = 9.06189 loss)
I0523 04:31:58.230068 34682 sgd_solver.cpp:112] Iteration 49610, lr = 0.01
I0523 04:32:03.300820 34682 solver.cpp:239] Iteration 49620 (1.94398 iter/s, 5.14408s/10 iters), loss = 9.37316
I0523 04:32:03.300938 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.37316 (* 1 = 9.37316 loss)
I0523 04:32:04.037314 34682 sgd_solver.cpp:112] Iteration 49620, lr = 0.01
I0523 04:32:09.525563 34682 solver.cpp:239] Iteration 49630 (1.60659 iter/s, 6.22436s/10 iters), loss = 8.44703
I0523 04:32:09.525617 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44703 (* 1 = 8.44703 loss)
I0523 04:32:09.635495 34682 sgd_solver.cpp:112] Iteration 49630, lr = 0.01
I0523 04:32:14.550334 34682 solver.cpp:239] Iteration 49640 (1.99024 iter/s, 5.02451s/10 iters), loss = 8.30882
I0523 04:32:14.550395 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30882 (* 1 = 8.30882 loss)
I0523 04:32:15.380592 34682 sgd_solver.cpp:112] Iteration 49640, lr = 0.01
I0523 04:32:18.048435 34682 solver.cpp:239] Iteration 49650 (2.85886 iter/s, 3.4979s/10 iters), loss = 9.3289
I0523 04:32:18.048487 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3289 (* 1 = 9.3289 loss)
I0523 04:32:18.116181 34682 sgd_solver.cpp:112] Iteration 49650, lr = 0.01
I0523 04:32:21.719020 34682 solver.cpp:239] Iteration 49660 (2.72452 iter/s, 3.67037s/10 iters), loss = 9.01957
I0523 04:32:21.719089 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01957 (* 1 = 9.01957 loss)
I0523 04:32:21.788746 34682 sgd_solver.cpp:112] Iteration 49660, lr = 0.01
I0523 04:32:27.786664 34682 solver.cpp:239] Iteration 49670 (1.64817 iter/s, 6.06734s/10 iters), loss = 9.51167
I0523 04:32:27.786726 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51167 (* 1 = 9.51167 loss)
I0523 04:32:28.641541 34682 sgd_solver.cpp:112] Iteration 49670, lr = 0.01
I0523 04:32:33.243736 34682 solver.cpp:239] Iteration 49680 (1.83258 iter/s, 5.45679s/10 iters), loss = 9.08912
I0523 04:32:33.243783 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08912 (* 1 = 9.08912 loss)
I0523 04:32:34.091608 34682 sgd_solver.cpp:112] Iteration 49680, lr = 0.01
I0523 04:32:38.920742 34682 solver.cpp:239] Iteration 49690 (1.76158 iter/s, 5.67672s/10 iters), loss = 8.54902
I0523 04:32:38.920804 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54902 (* 1 = 8.54902 loss)
I0523 04:32:38.984385 34682 sgd_solver.cpp:112] Iteration 49690, lr = 0.01
I0523 04:32:42.346983 34682 solver.cpp:239] Iteration 49700 (2.91882 iter/s, 3.42604s/10 iters), loss = 8.1729
I0523 04:32:42.347024 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1729 (* 1 = 8.1729 loss)
I0523 04:32:43.202180 34682 sgd_solver.cpp:112] Iteration 49700, lr = 0.01
I0523 04:32:49.489537 34682 solver.cpp:239] Iteration 49710 (1.40012 iter/s, 7.14222s/10 iters), loss = 9.64204
I0523 04:32:49.489591 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.64204 (* 1 = 9.64204 loss)
I0523 04:32:49.565922 34682 sgd_solver.cpp:112] Iteration 49710, lr = 0.01
I0523 04:32:55.157198 34682 solver.cpp:239] Iteration 49720 (1.76448 iter/s, 5.66738s/10 iters), loss = 8.8154
I0523 04:32:55.157240 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8154 (* 1 = 8.8154 loss)
I0523 04:32:55.233310 34682 sgd_solver.cpp:112] Iteration 49720, lr = 0.01
I0523 04:33:00.133476 34682 solver.cpp:239] Iteration 49730 (2.00964 iter/s, 4.97603s/10 iters), loss = 8.36609
I0523 04:33:00.133539 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36609 (* 1 = 8.36609 loss)
I0523 04:33:00.195770 34682 sgd_solver.cpp:112] Iteration 49730, lr = 0.01
I0523 04:33:03.533529 34682 solver.cpp:239] Iteration 49740 (2.94131 iter/s, 3.39984s/10 iters), loss = 8.55766
I0523 04:33:03.533586 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55766 (* 1 = 8.55766 loss)
I0523 04:33:04.292919 34682 sgd_solver.cpp:112] Iteration 49740, lr = 0.01
I0523 04:33:09.928714 34682 solver.cpp:239] Iteration 49750 (1.56375 iter/s, 6.39487s/10 iters), loss = 8.50675
I0523 04:33:09.928761 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50675 (* 1 = 8.50675 loss)
I0523 04:33:10.001477 34682 sgd_solver.cpp:112] Iteration 49750, lr = 0.01
I0523 04:33:15.458487 34682 solver.cpp:239] Iteration 49760 (1.80848 iter/s, 5.5295s/10 iters), loss = 8.56565
I0523 04:33:15.458539 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56565 (* 1 = 8.56565 loss)
I0523 04:33:15.527397 34682 sgd_solver.cpp:112] Iteration 49760, lr = 0.01
I0523 04:33:18.959223 34682 solver.cpp:239] Iteration 49770 (2.85671 iter/s, 3.50053s/10 iters), loss = 8.8443
I0523 04:33:18.959282 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8443 (* 1 = 8.8443 loss)
I0523 04:33:19.782861 34682 sgd_solver.cpp:112] Iteration 49770, lr = 0.01
I0523 04:33:24.779625 34682 solver.cpp:239] Iteration 49780 (1.71818 iter/s, 5.8201s/10 iters), loss = 8.23656
I0523 04:33:24.779690 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23656 (* 1 = 8.23656 loss)
I0523 04:33:24.972213 34682 sgd_solver.cpp:112] Iteration 49780, lr = 0.01
I0523 04:33:27.677296 34682 solver.cpp:239] Iteration 49790 (3.45128 iter/s, 2.89748s/10 iters), loss = 8.86094
I0523 04:33:27.677345 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86094 (* 1 = 8.86094 loss)
I0523 04:33:28.540253 34682 sgd_solver.cpp:112] Iteration 49790, lr = 0.01
I0523 04:33:32.701896 34682 solver.cpp:239] Iteration 49800 (1.99031 iter/s, 5.02434s/10 iters), loss = 8.28925
I0523 04:33:32.701951 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28925 (* 1 = 8.28925 loss)
I0523 04:33:32.755584 34682 sgd_solver.cpp:112] Iteration 49800, lr = 0.01
I0523 04:33:40.594975 34682 solver.cpp:239] Iteration 49810 (1.26699 iter/s, 7.89271s/10 iters), loss = 8.894
I0523 04:33:40.595235 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.894 (* 1 = 8.894 loss)
I0523 04:33:41.388823 34682 sgd_solver.cpp:112] Iteration 49810, lr = 0.01
I0523 04:33:44.577066 34682 solver.cpp:239] Iteration 49820 (2.51149 iter/s, 3.98169s/10 iters), loss = 8.98975
I0523 04:33:44.577111 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98975 (* 1 = 8.98975 loss)
I0523 04:33:44.641180 34682 sgd_solver.cpp:112] Iteration 49820, lr = 0.01
I0523 04:33:48.150243 34682 solver.cpp:239] Iteration 49830 (2.79879 iter/s, 3.57297s/10 iters), loss = 8.38162
I0523 04:33:48.150292 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38162 (* 1 = 8.38162 loss)
I0523 04:33:48.996929 34682 sgd_solver.cpp:112] Iteration 49830, lr = 0.01
I0523 04:33:52.955795 34682 solver.cpp:239] Iteration 49840 (2.08103 iter/s, 4.8053s/10 iters), loss = 9.19095
I0523 04:33:52.955845 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19095 (* 1 = 9.19095 loss)
I0523 04:33:53.023999 34682 sgd_solver.cpp:112] Iteration 49840, lr = 0.01
I0523 04:33:55.618867 34682 solver.cpp:239] Iteration 49850 (3.75529 iter/s, 2.66291s/10 iters), loss = 8.475
I0523 04:33:55.618912 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.475 (* 1 = 8.475 loss)
I0523 04:33:55.679857 34682 sgd_solver.cpp:112] Iteration 49850, lr = 0.01
I0523 04:34:00.532994 34682 solver.cpp:239] Iteration 49860 (2.03505 iter/s, 4.91388s/10 iters), loss = 8.47619
I0523 04:34:00.533035 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47619 (* 1 = 8.47619 loss)
I0523 04:34:00.608052 34682 sgd_solver.cpp:112] Iteration 49860, lr = 0.01
I0523 04:34:06.401827 34682 solver.cpp:239] Iteration 49870 (1.704 iter/s, 5.86855s/10 iters), loss = 8.66391
I0523 04:34:06.401885 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66391 (* 1 = 8.66391 loss)
I0523 04:34:06.462842 34682 sgd_solver.cpp:112] Iteration 49870, lr = 0.01
I0523 04:34:09.982110 34682 solver.cpp:239] Iteration 49880 (2.79324 iter/s, 3.58008s/10 iters), loss = 7.47821
I0523 04:34:09.982158 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.47821 (* 1 = 7.47821 loss)
I0523 04:34:10.193282 34682 sgd_solver.cpp:112] Iteration 49880, lr = 0.01
I0523 04:34:14.811254 34682 solver.cpp:239] Iteration 49890 (2.07086 iter/s, 4.8289s/10 iters), loss = 8.93814
I0523 04:34:14.811511 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93814 (* 1 = 8.93814 loss)
I0523 04:34:14.884158 34682 sgd_solver.cpp:112] Iteration 49890, lr = 0.01
I0523 04:34:18.300849 34682 solver.cpp:239] Iteration 49900 (2.86596 iter/s, 3.48923s/10 iters), loss = 9.21009
I0523 04:34:18.300901 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21009 (* 1 = 9.21009 loss)
I0523 04:34:19.126787 34682 sgd_solver.cpp:112] Iteration 49900, lr = 0.01
I0523 04:34:23.259052 34682 solver.cpp:239] Iteration 49910 (2.01696 iter/s, 4.95795s/10 iters), loss = 8.89608
I0523 04:34:23.259093 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89608 (* 1 = 8.89608 loss)
I0523 04:34:23.322736 34682 sgd_solver.cpp:112] Iteration 49910, lr = 0.01
I0523 04:34:28.690475 34682 solver.cpp:239] Iteration 49920 (1.84123 iter/s, 5.43115s/10 iters), loss = 8.90605
I0523 04:34:28.690526 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90605 (* 1 = 8.90605 loss)
I0523 04:34:29.399956 34682 sgd_solver.cpp:112] Iteration 49920, lr = 0.01
I0523 04:34:33.260411 34682 solver.cpp:239] Iteration 49930 (2.18833 iter/s, 4.56969s/10 iters), loss = 8.06215
I0523 04:34:33.260459 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06215 (* 1 = 8.06215 loss)
I0523 04:34:33.361546 34682 sgd_solver.cpp:112] Iteration 49930, lr = 0.01
I0523 04:34:39.534574 34682 solver.cpp:239] Iteration 49940 (1.59392 iter/s, 6.27385s/10 iters), loss = 8.91525
I0523 04:34:39.534620 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91525 (* 1 = 8.91525 loss)
I0523 04:34:39.600884 34682 sgd_solver.cpp:112] Iteration 49940, lr = 0.01
I0523 04:34:42.474318 34682 solver.cpp:239] Iteration 49950 (3.40186 iter/s, 2.93957s/10 iters), loss = 9.69335
I0523 04:34:42.474355 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.69335 (* 1 = 9.69335 loss)
I0523 04:34:42.551489 34682 sgd_solver.cpp:112] Iteration 49950, lr = 0.01
I0523 04:34:46.659332 34682 solver.cpp:239] Iteration 49960 (2.38962 iter/s, 4.18477s/10 iters), loss = 8.45034
I0523 04:34:46.662271 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45034 (* 1 = 8.45034 loss)
I0523 04:34:46.735273 34682 sgd_solver.cpp:112] Iteration 49960, lr = 0.01
I0523 04:34:50.734623 34682 solver.cpp:239] Iteration 49970 (2.45568 iter/s, 4.0722s/10 iters), loss = 7.70642
I0523 04:34:50.734681 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.70642 (* 1 = 7.70642 loss)
I0523 04:34:50.813910 34682 sgd_solver.cpp:112] Iteration 49970, lr = 0.01
I0523 04:34:55.520189 34682 solver.cpp:239] Iteration 49980 (2.08972 iter/s, 4.78532s/10 iters), loss = 8.15564
I0523 04:34:55.520236 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15564 (* 1 = 8.15564 loss)
I0523 04:34:55.584168 34682 sgd_solver.cpp:112] Iteration 49980, lr = 0.01
I0523 04:35:00.493316 34682 solver.cpp:239] Iteration 49990 (2.01091 iter/s, 4.97287s/10 iters), loss = 8.02632
I0523 04:35:00.493369 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02632 (* 1 = 8.02632 loss)
I0523 04:35:00.553050 34682 sgd_solver.cpp:112] Iteration 49990, lr = 0.01
I0523 04:35:05.841092 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_50000.caffemodel
I0523 04:35:07.450954 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_50000.solverstate
I0523 04:35:07.724135 34682 solver.cpp:239] Iteration 50000 (1.38304 iter/s, 7.23047s/10 iters), loss = 8.58524
I0523 04:35:07.724195 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58524 (* 1 = 8.58524 loss)
I0523 04:35:08.341526 34682 sgd_solver.cpp:112] Iteration 50000, lr = 0.01
I0523 04:35:14.438544 34682 solver.cpp:239] Iteration 50010 (1.48941 iter/s, 6.71407s/10 iters), loss = 7.89248
I0523 04:35:14.438601 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89248 (* 1 = 7.89248 loss)
I0523 04:35:15.275749 34682 sgd_solver.cpp:112] Iteration 50010, lr = 0.01
I0523 04:35:21.027596 34682 solver.cpp:239] Iteration 50020 (1.51774 iter/s, 6.58873s/10 iters), loss = 8.26177
I0523 04:35:21.027729 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26177 (* 1 = 8.26177 loss)
I0523 04:35:21.096964 34682 sgd_solver.cpp:112] Iteration 50020, lr = 0.01
I0523 04:35:26.173574 34682 solver.cpp:239] Iteration 50030 (1.9434 iter/s, 5.14563s/10 iters), loss = 8.72829
I0523 04:35:26.173619 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72829 (* 1 = 8.72829 loss)
I0523 04:35:26.230820 34682 sgd_solver.cpp:112] Iteration 50030, lr = 0.01
I0523 04:35:31.720788 34682 solver.cpp:239] Iteration 50040 (1.80279 iter/s, 5.54694s/10 iters), loss = 8.0213
I0523 04:35:31.720844 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0213 (* 1 = 8.0213 loss)
I0523 04:35:31.793884 34682 sgd_solver.cpp:112] Iteration 50040, lr = 0.01
I0523 04:35:36.194914 34682 solver.cpp:239] Iteration 50050 (2.23519 iter/s, 4.47388s/10 iters), loss = 8.59582
I0523 04:35:36.194974 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59582 (* 1 = 8.59582 loss)
I0523 04:35:36.980753 34682 sgd_solver.cpp:112] Iteration 50050, lr = 0.01
I0523 04:35:41.157618 34682 solver.cpp:239] Iteration 50060 (2.01514 iter/s, 4.96244s/10 iters), loss = 8.33597
I0523 04:35:41.157663 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33597 (* 1 = 8.33597 loss)
I0523 04:35:41.237228 34682 sgd_solver.cpp:112] Iteration 50060, lr = 0.01
I0523 04:35:44.867085 34682 solver.cpp:239] Iteration 50070 (2.69595 iter/s, 3.70927s/10 iters), loss = 8.36821
I0523 04:35:44.867138 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36821 (* 1 = 8.36821 loss)
I0523 04:35:44.928295 34682 sgd_solver.cpp:112] Iteration 50070, lr = 0.01
I0523 04:35:49.665585 34682 solver.cpp:239] Iteration 50080 (2.08409 iter/s, 4.79825s/10 iters), loss = 8.10247
I0523 04:35:49.665624 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10247 (* 1 = 8.10247 loss)
I0523 04:35:49.732437 34682 sgd_solver.cpp:112] Iteration 50080, lr = 0.01
I0523 04:35:52.824841 34682 solver.cpp:239] Iteration 50090 (3.16549 iter/s, 3.15907s/10 iters), loss = 9.51405
I0523 04:35:52.824998 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.51405 (* 1 = 9.51405 loss)
I0523 04:35:52.889832 34682 sgd_solver.cpp:112] Iteration 50090, lr = 0.01
I0523 04:35:58.669487 34682 solver.cpp:239] Iteration 50100 (1.71109 iter/s, 5.84424s/10 iters), loss = 8.9546
I0523 04:35:58.669570 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9546 (* 1 = 8.9546 loss)
I0523 04:35:58.720583 34682 sgd_solver.cpp:112] Iteration 50100, lr = 0.01
I0523 04:36:01.236502 34682 solver.cpp:239] Iteration 50110 (3.89588 iter/s, 2.56681s/10 iters), loss = 8.88661
I0523 04:36:01.236553 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88661 (* 1 = 8.88661 loss)
I0523 04:36:01.293236 34682 sgd_solver.cpp:112] Iteration 50110, lr = 0.01
I0523 04:36:06.178644 34682 solver.cpp:239] Iteration 50120 (2.02352 iter/s, 4.94189s/10 iters), loss = 9.03067
I0523 04:36:06.178689 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03067 (* 1 = 9.03067 loss)
I0523 04:36:06.644779 34682 sgd_solver.cpp:112] Iteration 50120, lr = 0.01
I0523 04:36:10.757436 34682 solver.cpp:239] Iteration 50130 (2.1841 iter/s, 4.57855s/10 iters), loss = 8.30151
I0523 04:36:10.757488 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30151 (* 1 = 8.30151 loss)
I0523 04:36:10.824973 34682 sgd_solver.cpp:112] Iteration 50130, lr = 0.01
I0523 04:36:15.497792 34682 solver.cpp:239] Iteration 50140 (2.10965 iter/s, 4.74011s/10 iters), loss = 8.36781
I0523 04:36:15.497840 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36781 (* 1 = 8.36781 loss)
I0523 04:36:15.950168 34682 sgd_solver.cpp:112] Iteration 50140, lr = 0.01
I0523 04:36:21.818853 34682 solver.cpp:239] Iteration 50150 (1.58209 iter/s, 6.32076s/10 iters), loss = 8.58585
I0523 04:36:21.818907 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58585 (* 1 = 8.58585 loss)
I0523 04:36:21.895622 34682 sgd_solver.cpp:112] Iteration 50150, lr = 0.01
I0523 04:36:25.983479 34682 solver.cpp:239] Iteration 50160 (2.40131 iter/s, 4.16439s/10 iters), loss = 8.85751
I0523 04:36:25.983793 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85751 (* 1 = 8.85751 loss)
I0523 04:36:26.043921 34682 sgd_solver.cpp:112] Iteration 50160, lr = 0.01
I0523 04:36:31.562496 34682 solver.cpp:239] Iteration 50170 (1.79259 iter/s, 5.57852s/10 iters), loss = 8.89838
I0523 04:36:31.562548 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89838 (* 1 = 8.89838 loss)
I0523 04:36:32.450853 34682 sgd_solver.cpp:112] Iteration 50170, lr = 0.01
I0523 04:36:38.732278 34682 solver.cpp:239] Iteration 50180 (1.39481 iter/s, 7.16942s/10 iters), loss = 8.4764
I0523 04:36:38.732348 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4764 (* 1 = 8.4764 loss)
I0523 04:36:39.569857 34682 sgd_solver.cpp:112] Iteration 50180, lr = 0.01
I0523 04:36:46.242552 34682 solver.cpp:239] Iteration 50190 (1.33157 iter/s, 7.5099s/10 iters), loss = 8.58792
I0523 04:36:46.242604 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58792 (* 1 = 8.58792 loss)
I0523 04:36:46.313619 34682 sgd_solver.cpp:112] Iteration 50190, lr = 0.01
I0523 04:36:50.691777 34682 solver.cpp:239] Iteration 50200 (2.2477 iter/s, 4.449s/10 iters), loss = 9.2672
I0523 04:36:50.691817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2672 (* 1 = 9.2672 loss)
I0523 04:36:50.768848 34682 sgd_solver.cpp:112] Iteration 50200, lr = 0.01
I0523 04:36:56.878664 34682 solver.cpp:239] Iteration 50210 (1.6164 iter/s, 6.18659s/10 iters), loss = 8.3634
I0523 04:36:56.878912 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3634 (* 1 = 8.3634 loss)
I0523 04:36:57.510937 34682 sgd_solver.cpp:112] Iteration 50210, lr = 0.01
I0523 04:37:00.755926 34682 solver.cpp:239] Iteration 50220 (2.57939 iter/s, 3.87688s/10 iters), loss = 8.51618
I0523 04:37:00.755973 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51618 (* 1 = 8.51618 loss)
I0523 04:37:00.840292 34682 sgd_solver.cpp:112] Iteration 50220, lr = 0.01
I0523 04:37:06.432548 34682 solver.cpp:239] Iteration 50230 (1.7617 iter/s, 5.67634s/10 iters), loss = 8.24379
I0523 04:37:06.432592 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24379 (* 1 = 8.24379 loss)
I0523 04:37:06.504346 34682 sgd_solver.cpp:112] Iteration 50230, lr = 0.01
I0523 04:37:10.790887 34682 solver.cpp:239] Iteration 50240 (2.29457 iter/s, 4.35811s/10 iters), loss = 7.5978
I0523 04:37:10.790928 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5978 (* 1 = 7.5978 loss)
I0523 04:37:10.865150 34682 sgd_solver.cpp:112] Iteration 50240, lr = 0.01
I0523 04:37:16.226651 34682 solver.cpp:239] Iteration 50250 (1.83976 iter/s, 5.4355s/10 iters), loss = 8.66314
I0523 04:37:16.226716 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66314 (* 1 = 8.66314 loss)
I0523 04:37:16.301565 34682 sgd_solver.cpp:112] Iteration 50250, lr = 0.01
I0523 04:37:20.367900 34682 solver.cpp:239] Iteration 50260 (2.41486 iter/s, 4.14102s/10 iters), loss = 9.48078
I0523 04:37:20.367962 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.48078 (* 1 = 9.48078 loss)
I0523 04:37:21.187306 34682 sgd_solver.cpp:112] Iteration 50260, lr = 0.01
I0523 04:37:24.246302 34682 solver.cpp:239] Iteration 50270 (2.57853 iter/s, 3.87818s/10 iters), loss = 8.07589
I0523 04:37:24.246342 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07589 (* 1 = 8.07589 loss)
I0523 04:37:24.308413 34682 sgd_solver.cpp:112] Iteration 50270, lr = 0.01
I0523 04:37:28.376628 34682 solver.cpp:239] Iteration 50280 (2.42124 iter/s, 4.13011s/10 iters), loss = 8.24308
I0523 04:37:28.376859 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24308 (* 1 = 8.24308 loss)
I0523 04:37:28.443862 34682 sgd_solver.cpp:112] Iteration 50280, lr = 0.01
I0523 04:37:34.000309 34682 solver.cpp:239] Iteration 50290 (1.77833 iter/s, 5.62324s/10 iters), loss = 8.34265
I0523 04:37:34.000351 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34265 (* 1 = 8.34265 loss)
I0523 04:37:34.075410 34682 sgd_solver.cpp:112] Iteration 50290, lr = 0.01
I0523 04:37:39.932160 34682 solver.cpp:239] Iteration 50300 (1.6859 iter/s, 5.93156s/10 iters), loss = 8.92808
I0523 04:37:39.932226 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92808 (* 1 = 8.92808 loss)
I0523 04:37:40.780697 34682 sgd_solver.cpp:112] Iteration 50300, lr = 0.01
I0523 04:37:45.572051 34682 solver.cpp:239] Iteration 50310 (1.77318 iter/s, 5.6396s/10 iters), loss = 7.77351
I0523 04:37:45.572095 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77351 (* 1 = 7.77351 loss)
I0523 04:37:45.636512 34682 sgd_solver.cpp:112] Iteration 50310, lr = 0.01
I0523 04:37:49.450903 34682 solver.cpp:239] Iteration 50320 (2.57823 iter/s, 3.87864s/10 iters), loss = 8.45317
I0523 04:37:49.450959 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45317 (* 1 = 8.45317 loss)
I0523 04:37:49.513768 34682 sgd_solver.cpp:112] Iteration 50320, lr = 0.01
I0523 04:37:54.616729 34682 solver.cpp:239] Iteration 50330 (1.9359 iter/s, 5.16556s/10 iters), loss = 8.29243
I0523 04:37:54.616770 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29243 (* 1 = 8.29243 loss)
I0523 04:37:54.682690 34682 sgd_solver.cpp:112] Iteration 50330, lr = 0.01
I0523 04:37:58.639895 34682 solver.cpp:239] Iteration 50340 (2.48574 iter/s, 4.02295s/10 iters), loss = 9.39487
I0523 04:37:58.640178 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.39487 (* 1 = 9.39487 loss)
I0523 04:37:58.713273 34682 sgd_solver.cpp:112] Iteration 50340, lr = 0.01
I0523 04:38:01.365528 34682 solver.cpp:239] Iteration 50350 (3.67292 iter/s, 2.72263s/10 iters), loss = 7.4203
I0523 04:38:01.365581 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4203 (* 1 = 7.4203 loss)
I0523 04:38:01.439383 34682 sgd_solver.cpp:112] Iteration 50350, lr = 0.01
I0523 04:38:06.777794 34682 solver.cpp:239] Iteration 50360 (1.84775 iter/s, 5.41199s/10 iters), loss = 9.15419
I0523 04:38:06.777849 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15419 (* 1 = 9.15419 loss)
I0523 04:38:07.589928 34682 sgd_solver.cpp:112] Iteration 50360, lr = 0.01
I0523 04:38:12.556710 34682 solver.cpp:239] Iteration 50370 (1.73052 iter/s, 5.77862s/10 iters), loss = 8.2226
I0523 04:38:12.556766 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2226 (* 1 = 8.2226 loss)
I0523 04:38:13.324005 34682 sgd_solver.cpp:112] Iteration 50370, lr = 0.01
I0523 04:38:15.965605 34682 solver.cpp:239] Iteration 50380 (2.93367 iter/s, 3.4087s/10 iters), loss = 8.43406
I0523 04:38:15.965641 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43406 (* 1 = 8.43406 loss)
I0523 04:38:16.038570 34682 sgd_solver.cpp:112] Iteration 50380, lr = 0.01
I0523 04:38:20.980386 34682 solver.cpp:239] Iteration 50390 (1.9942 iter/s, 5.01454s/10 iters), loss = 8.85021
I0523 04:38:20.980432 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85021 (* 1 = 8.85021 loss)
I0523 04:38:21.065554 34682 sgd_solver.cpp:112] Iteration 50390, lr = 0.01
I0523 04:38:27.547098 34682 solver.cpp:239] Iteration 50400 (1.52291 iter/s, 6.56639s/10 iters), loss = 8.2226
I0523 04:38:27.547164 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2226 (* 1 = 8.2226 loss)
I0523 04:38:28.292192 34682 sgd_solver.cpp:112] Iteration 50400, lr = 0.01
I0523 04:38:31.226205 34682 solver.cpp:239] Iteration 50410 (2.71823 iter/s, 3.67887s/10 iters), loss = 9.19023
I0523 04:38:31.226371 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19023 (* 1 = 9.19023 loss)
I0523 04:38:31.305840 34682 sgd_solver.cpp:112] Iteration 50410, lr = 0.01
I0523 04:38:33.941087 34682 solver.cpp:239] Iteration 50420 (3.68378 iter/s, 2.7146s/10 iters), loss = 8.90318
I0523 04:38:33.941138 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90318 (* 1 = 8.90318 loss)
I0523 04:38:34.005393 34682 sgd_solver.cpp:112] Iteration 50420, lr = 0.01
I0523 04:38:38.647447 34682 solver.cpp:239] Iteration 50430 (2.12489 iter/s, 4.70612s/10 iters), loss = 8.39481
I0523 04:38:38.647500 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39481 (* 1 = 8.39481 loss)
I0523 04:38:39.409835 34682 sgd_solver.cpp:112] Iteration 50430, lr = 0.01
I0523 04:38:46.268067 34682 solver.cpp:239] Iteration 50440 (1.31229 iter/s, 7.62026s/10 iters), loss = 8.61498
I0523 04:38:46.268124 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61498 (* 1 = 8.61498 loss)
I0523 04:38:46.572597 34682 sgd_solver.cpp:112] Iteration 50440, lr = 0.01
I0523 04:38:49.893951 34682 solver.cpp:239] Iteration 50450 (2.75812 iter/s, 3.62566s/10 iters), loss = 8.96813
I0523 04:38:49.893996 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96813 (* 1 = 8.96813 loss)
I0523 04:38:49.958395 34682 sgd_solver.cpp:112] Iteration 50450, lr = 0.01
I0523 04:38:54.596839 34682 solver.cpp:239] Iteration 50460 (2.12646 iter/s, 4.70265s/10 iters), loss = 8.66693
I0523 04:38:54.596886 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66693 (* 1 = 8.66693 loss)
I0523 04:38:55.198652 34682 sgd_solver.cpp:112] Iteration 50460, lr = 0.01
I0523 04:38:58.969322 34682 solver.cpp:239] Iteration 50470 (2.28715 iter/s, 4.37225s/10 iters), loss = 8.35701
I0523 04:38:58.969379 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35701 (* 1 = 8.35701 loss)
I0523 04:38:59.046787 34682 sgd_solver.cpp:112] Iteration 50470, lr = 0.01
I0523 04:39:03.614329 34682 solver.cpp:239] Iteration 50480 (2.15297 iter/s, 4.64476s/10 iters), loss = 8.72073
I0523 04:39:03.614537 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72073 (* 1 = 8.72073 loss)
I0523 04:39:04.343477 34682 sgd_solver.cpp:112] Iteration 50480, lr = 0.01
I0523 04:39:08.885502 34682 solver.cpp:239] Iteration 50490 (1.89726 iter/s, 5.27075s/10 iters), loss = 8.15217
I0523 04:39:08.885565 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15217 (* 1 = 8.15217 loss)
I0523 04:39:08.966774 34682 sgd_solver.cpp:112] Iteration 50490, lr = 0.01
I0523 04:39:10.870474 34682 solver.cpp:239] Iteration 50500 (5.03823 iter/s, 1.98482s/10 iters), loss = 8.71395
I0523 04:39:10.870533 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71395 (* 1 = 8.71395 loss)
I0523 04:39:11.723866 34682 sgd_solver.cpp:112] Iteration 50500, lr = 0.01
I0523 04:39:14.945329 34682 solver.cpp:239] Iteration 50510 (2.45421 iter/s, 4.07462s/10 iters), loss = 8.57107
I0523 04:39:14.945384 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57107 (* 1 = 8.57107 loss)
I0523 04:39:15.819737 34682 sgd_solver.cpp:112] Iteration 50510, lr = 0.01
I0523 04:39:19.686368 34682 solver.cpp:239] Iteration 50520 (2.10935 iter/s, 4.74079s/10 iters), loss = 9.03425
I0523 04:39:19.686415 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03425 (* 1 = 9.03425 loss)
I0523 04:39:19.759156 34682 sgd_solver.cpp:112] Iteration 50520, lr = 0.01
I0523 04:39:24.712635 34682 solver.cpp:239] Iteration 50530 (1.98965 iter/s, 5.02602s/10 iters), loss = 7.97691
I0523 04:39:24.712682 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97691 (* 1 = 7.97691 loss)
I0523 04:39:24.767974 34682 sgd_solver.cpp:112] Iteration 50530, lr = 0.01
I0523 04:39:28.845307 34682 solver.cpp:239] Iteration 50540 (2.41987 iter/s, 4.13245s/10 iters), loss = 8.44235
I0523 04:39:28.845356 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44235 (* 1 = 8.44235 loss)
I0523 04:39:29.686298 34682 sgd_solver.cpp:112] Iteration 50540, lr = 0.01
I0523 04:39:35.924530 34682 solver.cpp:239] Iteration 50550 (1.41265 iter/s, 7.07889s/10 iters), loss = 8.76367
I0523 04:39:35.924742 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76367 (* 1 = 8.76367 loss)
I0523 04:39:35.994256 34682 sgd_solver.cpp:112] Iteration 50550, lr = 0.01
I0523 04:39:39.109251 34682 solver.cpp:239] Iteration 50560 (3.14031 iter/s, 3.1844s/10 iters), loss = 8.35629
I0523 04:39:39.109303 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35629 (* 1 = 8.35629 loss)
I0523 04:39:39.168056 34682 sgd_solver.cpp:112] Iteration 50560, lr = 0.01
I0523 04:39:41.803000 34682 solver.cpp:239] Iteration 50570 (3.71253 iter/s, 2.69358s/10 iters), loss = 8.78835
I0523 04:39:41.803048 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78835 (* 1 = 8.78835 loss)
I0523 04:39:42.529918 34682 sgd_solver.cpp:112] Iteration 50570, lr = 0.01
I0523 04:39:46.424121 34682 solver.cpp:239] Iteration 50580 (2.16409 iter/s, 4.62088s/10 iters), loss = 7.66963
I0523 04:39:46.424160 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66963 (* 1 = 7.66963 loss)
I0523 04:39:46.503228 34682 sgd_solver.cpp:112] Iteration 50580, lr = 0.01
I0523 04:39:50.662575 34682 solver.cpp:239] Iteration 50590 (2.35947 iter/s, 4.23824s/10 iters), loss = 8.14761
I0523 04:39:50.662626 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14761 (* 1 = 8.14761 loss)
I0523 04:39:51.492667 34682 sgd_solver.cpp:112] Iteration 50590, lr = 0.01
I0523 04:39:56.004585 34682 solver.cpp:239] Iteration 50600 (1.87205 iter/s, 5.34174s/10 iters), loss = 8.79588
I0523 04:39:56.004631 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79588 (* 1 = 8.79588 loss)
I0523 04:39:56.072368 34682 sgd_solver.cpp:112] Iteration 50600, lr = 0.01
I0523 04:39:59.367763 34682 solver.cpp:239] Iteration 50610 (2.97354 iter/s, 3.36299s/10 iters), loss = 9.54385
I0523 04:39:59.367807 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.54385 (* 1 = 9.54385 loss)
I0523 04:40:00.206688 34682 sgd_solver.cpp:112] Iteration 50610, lr = 0.01
I0523 04:40:05.265758 34682 solver.cpp:239] Iteration 50620 (1.69558 iter/s, 5.8977s/10 iters), loss = 9.39742
I0523 04:40:05.265820 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.39742 (* 1 = 9.39742 loss)
I0523 04:40:05.961452 34682 sgd_solver.cpp:112] Iteration 50620, lr = 0.01
I0523 04:40:09.999630 34682 solver.cpp:239] Iteration 50630 (2.11255 iter/s, 4.73362s/10 iters), loss = 8.1386
I0523 04:40:09.999675 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1386 (* 1 = 8.1386 loss)
I0523 04:40:10.078809 34682 sgd_solver.cpp:112] Iteration 50630, lr = 0.01
I0523 04:40:13.471554 34682 solver.cpp:239] Iteration 50640 (2.88041 iter/s, 3.47173s/10 iters), loss = 8.8425
I0523 04:40:13.471602 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8425 (* 1 = 8.8425 loss)
I0523 04:40:13.530683 34682 sgd_solver.cpp:112] Iteration 50640, lr = 0.01
I0523 04:40:17.722657 34682 solver.cpp:239] Iteration 50650 (2.35246 iter/s, 4.25087s/10 iters), loss = 9.36319
I0523 04:40:17.722759 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36319 (* 1 = 9.36319 loss)
I0523 04:40:17.783406 34682 sgd_solver.cpp:112] Iteration 50650, lr = 0.01
I0523 04:40:21.857357 34682 solver.cpp:239] Iteration 50660 (2.41871 iter/s, 4.13444s/10 iters), loss = 8.33092
I0523 04:40:21.857398 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33092 (* 1 = 8.33092 loss)
I0523 04:40:21.930272 34682 sgd_solver.cpp:112] Iteration 50660, lr = 0.01
I0523 04:40:25.892920 34682 solver.cpp:239] Iteration 50670 (2.4781 iter/s, 4.03534s/10 iters), loss = 8.72346
I0523 04:40:25.892982 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72346 (* 1 = 8.72346 loss)
I0523 04:40:25.969637 34682 sgd_solver.cpp:112] Iteration 50670, lr = 0.01
I0523 04:40:31.607544 34682 solver.cpp:239] Iteration 50680 (1.74999 iter/s, 5.71433s/10 iters), loss = 8.41139
I0523 04:40:31.607620 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41139 (* 1 = 8.41139 loss)
I0523 04:40:31.677517 34682 sgd_solver.cpp:112] Iteration 50680, lr = 0.01
I0523 04:40:36.210945 34682 solver.cpp:239] Iteration 50690 (2.17243 iter/s, 4.60314s/10 iters), loss = 8.29319
I0523 04:40:36.211163 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29319 (* 1 = 8.29319 loss)
I0523 04:40:36.275779 34682 sgd_solver.cpp:112] Iteration 50690, lr = 0.01
I0523 04:40:39.848248 34682 solver.cpp:239] Iteration 50700 (2.74958 iter/s, 3.63692s/10 iters), loss = 8.53405
I0523 04:40:39.848301 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53405 (* 1 = 8.53405 loss)
I0523 04:40:39.911656 34682 sgd_solver.cpp:112] Iteration 50700, lr = 0.01
I0523 04:40:42.994443 34682 solver.cpp:239] Iteration 50710 (3.17864 iter/s, 3.146s/10 iters), loss = 7.57387
I0523 04:40:42.994493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.57387 (* 1 = 7.57387 loss)
I0523 04:40:43.774318 34682 sgd_solver.cpp:112] Iteration 50710, lr = 0.01
I0523 04:40:47.766448 34682 solver.cpp:239] Iteration 50720 (2.09566 iter/s, 4.77176s/10 iters), loss = 9.60218
I0523 04:40:47.766496 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60218 (* 1 = 9.60218 loss)
I0523 04:40:47.903244 34682 sgd_solver.cpp:112] Iteration 50720, lr = 0.01
I0523 04:40:52.254896 34682 solver.cpp:239] Iteration 50730 (2.22806 iter/s, 4.48821s/10 iters), loss = 7.35381
I0523 04:40:52.254940 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.35381 (* 1 = 7.35381 loss)
I0523 04:40:52.328250 34682 sgd_solver.cpp:112] Iteration 50730, lr = 0.01
I0523 04:40:57.337375 34682 solver.cpp:239] Iteration 50740 (1.96764 iter/s, 5.08223s/10 iters), loss = 8.97962
I0523 04:40:57.337417 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97962 (* 1 = 8.97962 loss)
I0523 04:40:57.400120 34682 sgd_solver.cpp:112] Iteration 50740, lr = 0.01
I0523 04:41:02.205759 34682 solver.cpp:239] Iteration 50750 (2.05417 iter/s, 4.86814s/10 iters), loss = 9.16494
I0523 04:41:02.205808 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16494 (* 1 = 9.16494 loss)
I0523 04:41:02.276157 34682 sgd_solver.cpp:112] Iteration 50750, lr = 0.01
I0523 04:41:06.164666 34682 solver.cpp:239] Iteration 50760 (2.52609 iter/s, 3.95869s/10 iters), loss = 9.29267
I0523 04:41:06.164721 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29267 (* 1 = 9.29267 loss)
I0523 04:41:06.986395 34682 sgd_solver.cpp:112] Iteration 50760, lr = 0.01
I0523 04:41:12.276835 34682 solver.cpp:239] Iteration 50770 (1.63616 iter/s, 6.11186s/10 iters), loss = 8.25226
I0523 04:41:12.276885 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25226 (* 1 = 8.25226 loss)
I0523 04:41:13.123632 34682 sgd_solver.cpp:112] Iteration 50770, lr = 0.01
I0523 04:41:18.469096 34682 solver.cpp:239] Iteration 50780 (1.615 iter/s, 6.19196s/10 iters), loss = 9.4103
I0523 04:41:18.469141 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.4103 (* 1 = 9.4103 loss)
I0523 04:41:18.533339 34682 sgd_solver.cpp:112] Iteration 50780, lr = 0.01
I0523 04:41:21.871378 34682 solver.cpp:239] Iteration 50790 (2.93936 iter/s, 3.4021s/10 iters), loss = 8.41859
I0523 04:41:21.871428 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41859 (* 1 = 8.41859 loss)
I0523 04:41:21.951542 34682 sgd_solver.cpp:112] Iteration 50790, lr = 0.01
I0523 04:41:26.315234 34682 solver.cpp:239] Iteration 50800 (2.25042 iter/s, 4.44361s/10 iters), loss = 8.6492
I0523 04:41:26.315287 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6492 (* 1 = 8.6492 loss)
I0523 04:41:26.387970 34682 sgd_solver.cpp:112] Iteration 50800, lr = 0.01
I0523 04:41:32.545006 34682 solver.cpp:239] Iteration 50810 (1.60527 iter/s, 6.22947s/10 iters), loss = 8.96172
I0523 04:41:32.545053 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96172 (* 1 = 8.96172 loss)
I0523 04:41:33.426097 34682 sgd_solver.cpp:112] Iteration 50810, lr = 0.01
I0523 04:41:38.413496 34682 solver.cpp:239] Iteration 50820 (1.7041 iter/s, 5.8682s/10 iters), loss = 8.70492
I0523 04:41:38.413730 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70492 (* 1 = 8.70492 loss)
I0523 04:41:38.487289 34682 sgd_solver.cpp:112] Iteration 50820, lr = 0.01
I0523 04:41:41.822679 34682 solver.cpp:239] Iteration 50830 (2.93356 iter/s, 3.40883s/10 iters), loss = 8.07275
I0523 04:41:41.822782 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07275 (* 1 = 8.07275 loss)
I0523 04:41:42.570371 34682 sgd_solver.cpp:112] Iteration 50830, lr = 0.01
I0523 04:41:48.548895 34682 solver.cpp:239] Iteration 50840 (1.4868 iter/s, 6.72585s/10 iters), loss = 7.98951
I0523 04:41:48.548949 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98951 (* 1 = 7.98951 loss)
I0523 04:41:48.631544 34682 sgd_solver.cpp:112] Iteration 50840, lr = 0.01
I0523 04:41:53.510040 34682 solver.cpp:239] Iteration 50850 (2.01577 iter/s, 4.96089s/10 iters), loss = 8.52052
I0523 04:41:53.510083 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52052 (* 1 = 8.52052 loss)
I0523 04:41:54.356025 34682 sgd_solver.cpp:112] Iteration 50850, lr = 0.01
I0523 04:41:59.040887 34682 solver.cpp:239] Iteration 50860 (1.80813 iter/s, 5.53057s/10 iters), loss = 8.80262
I0523 04:41:59.040946 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80262 (* 1 = 8.80262 loss)
I0523 04:41:59.116267 34682 sgd_solver.cpp:112] Iteration 50860, lr = 0.01
I0523 04:42:03.618402 34682 solver.cpp:239] Iteration 50870 (2.18472 iter/s, 4.57725s/10 iters), loss = 8.22442
I0523 04:42:03.618463 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22442 (* 1 = 8.22442 loss)
I0523 04:42:04.383370 34682 sgd_solver.cpp:112] Iteration 50870, lr = 0.01
I0523 04:42:09.082998 34682 solver.cpp:239] Iteration 50880 (1.83005 iter/s, 5.46432s/10 iters), loss = 7.68903
I0523 04:42:09.083258 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68903 (* 1 = 7.68903 loss)
I0523 04:42:09.156924 34682 sgd_solver.cpp:112] Iteration 50880, lr = 0.01
I0523 04:42:13.394860 34682 solver.cpp:239] Iteration 50890 (2.31941 iter/s, 4.31145s/10 iters), loss = 9.03785
I0523 04:42:13.394910 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03785 (* 1 = 9.03785 loss)
I0523 04:42:13.467597 34682 sgd_solver.cpp:112] Iteration 50890, lr = 0.01
I0523 04:42:18.855218 34682 solver.cpp:239] Iteration 50900 (1.83147 iter/s, 5.46009s/10 iters), loss = 8.71574
I0523 04:42:18.855264 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71574 (* 1 = 8.71574 loss)
I0523 04:42:19.734791 34682 sgd_solver.cpp:112] Iteration 50900, lr = 0.01
I0523 04:42:24.704996 34682 solver.cpp:239] Iteration 50910 (1.70955 iter/s, 5.8495s/10 iters), loss = 7.39333
I0523 04:42:24.705049 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.39333 (* 1 = 7.39333 loss)
I0523 04:42:24.767503 34682 sgd_solver.cpp:112] Iteration 50910, lr = 0.01
I0523 04:42:29.378113 34682 solver.cpp:239] Iteration 50920 (2.14001 iter/s, 4.67287s/10 iters), loss = 8.39081
I0523 04:42:29.378149 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39081 (* 1 = 8.39081 loss)
I0523 04:42:29.450556 34682 sgd_solver.cpp:112] Iteration 50920, lr = 0.01
I0523 04:42:34.761149 34682 solver.cpp:239] Iteration 50930 (1.85778 iter/s, 5.38277s/10 iters), loss = 8.2367
I0523 04:42:34.761207 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2367 (* 1 = 8.2367 loss)
I0523 04:42:34.818645 34682 sgd_solver.cpp:112] Iteration 50930, lr = 0.01
I0523 04:42:39.768153 34682 solver.cpp:239] Iteration 50940 (1.9973 iter/s, 5.00675s/10 iters), loss = 8.18718
I0523 04:42:39.768414 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18718 (* 1 = 8.18718 loss)
I0523 04:42:39.842043 34682 sgd_solver.cpp:112] Iteration 50940, lr = 0.01
I0523 04:42:44.705420 34682 solver.cpp:239] Iteration 50950 (2.02559 iter/s, 4.93683s/10 iters), loss = 8.72039
I0523 04:42:44.705476 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72039 (* 1 = 8.72039 loss)
I0523 04:42:45.336612 34682 sgd_solver.cpp:112] Iteration 50950, lr = 0.01
I0523 04:42:50.230069 34682 solver.cpp:239] Iteration 50960 (1.81016 iter/s, 5.52437s/10 iters), loss = 8.44936
I0523 04:42:50.230121 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44936 (* 1 = 8.44936 loss)
I0523 04:42:50.410151 34682 sgd_solver.cpp:112] Iteration 50960, lr = 0.01
I0523 04:42:55.504763 34682 solver.cpp:239] Iteration 50970 (1.89594 iter/s, 5.27443s/10 iters), loss = 9.12234
I0523 04:42:55.504807 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12234 (* 1 = 9.12234 loss)
I0523 04:42:55.584784 34682 sgd_solver.cpp:112] Iteration 50970, lr = 0.01
I0523 04:42:59.315419 34682 solver.cpp:239] Iteration 50980 (2.62436 iter/s, 3.81045s/10 iters), loss = 9.7135
I0523 04:42:59.315471 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.7135 (* 1 = 9.7135 loss)
I0523 04:42:59.386293 34682 sgd_solver.cpp:112] Iteration 50980, lr = 0.01
I0523 04:43:02.797323 34682 solver.cpp:239] Iteration 50990 (2.87215 iter/s, 3.48171s/10 iters), loss = 8.20426
I0523 04:43:02.797365 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20426 (* 1 = 8.20426 loss)
I0523 04:43:02.861760 34682 sgd_solver.cpp:112] Iteration 50990, lr = 0.01
I0523 04:43:06.301268 34682 solver.cpp:239] Iteration 51000 (2.85409 iter/s, 3.50374s/10 iters), loss = 7.72102
I0523 04:43:06.301339 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72102 (* 1 = 7.72102 loss)
I0523 04:43:07.005158 34682 sgd_solver.cpp:112] Iteration 51000, lr = 0.01
I0523 04:43:10.735544 34682 solver.cpp:239] Iteration 51010 (2.25529 iter/s, 4.43402s/10 iters), loss = 7.66818
I0523 04:43:10.735815 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66818 (* 1 = 7.66818 loss)
I0523 04:43:10.794548 34682 sgd_solver.cpp:112] Iteration 51010, lr = 0.01
I0523 04:43:17.472920 34682 solver.cpp:239] Iteration 51020 (1.48437 iter/s, 6.73686s/10 iters), loss = 8.80449
I0523 04:43:17.472975 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80449 (* 1 = 8.80449 loss)
I0523 04:43:18.247387 34682 sgd_solver.cpp:112] Iteration 51020, lr = 0.01
I0523 04:43:23.089149 34682 solver.cpp:239] Iteration 51030 (1.78065 iter/s, 5.61593s/10 iters), loss = 8.46448
I0523 04:43:23.089212 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46448 (* 1 = 8.46448 loss)
I0523 04:43:23.161341 34682 sgd_solver.cpp:112] Iteration 51030, lr = 0.01
I0523 04:43:25.183042 34682 solver.cpp:239] Iteration 51040 (4.77614 iter/s, 2.09374s/10 iters), loss = 9.38768
I0523 04:43:25.183086 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.38768 (* 1 = 9.38768 loss)
I0523 04:43:25.260462 34682 sgd_solver.cpp:112] Iteration 51040, lr = 0.01
I0523 04:43:30.519085 34682 solver.cpp:239] Iteration 51050 (1.87414 iter/s, 5.33577s/10 iters), loss = 7.89065
I0523 04:43:30.519136 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89065 (* 1 = 7.89065 loss)
I0523 04:43:30.586843 34682 sgd_solver.cpp:112] Iteration 51050, lr = 0.01
I0523 04:43:36.188324 34682 solver.cpp:239] Iteration 51060 (1.76399 iter/s, 5.66896s/10 iters), loss = 8.27237
I0523 04:43:36.188381 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27237 (* 1 = 8.27237 loss)
I0523 04:43:36.258611 34682 sgd_solver.cpp:112] Iteration 51060, lr = 0.01
I0523 04:43:40.304354 34682 solver.cpp:239] Iteration 51070 (2.42966 iter/s, 4.1158s/10 iters), loss = 8.97841
I0523 04:43:40.304397 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97841 (* 1 = 8.97841 loss)
I0523 04:43:40.381435 34682 sgd_solver.cpp:112] Iteration 51070, lr = 0.01
I0523 04:43:46.750401 34682 solver.cpp:239] Iteration 51080 (1.55141 iter/s, 6.44574s/10 iters), loss = 9.44333
I0523 04:43:46.750560 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.44333 (* 1 = 9.44333 loss)
I0523 04:43:46.819727 34682 sgd_solver.cpp:112] Iteration 51080, lr = 0.01
I0523 04:43:52.769361 34682 solver.cpp:239] Iteration 51090 (1.66153 iter/s, 6.01856s/10 iters), loss = 7.83078
I0523 04:43:52.769412 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83078 (* 1 = 7.83078 loss)
I0523 04:43:52.829536 34682 sgd_solver.cpp:112] Iteration 51090, lr = 0.01
I0523 04:43:56.937849 34682 solver.cpp:239] Iteration 51100 (2.39908 iter/s, 4.16826s/10 iters), loss = 8.11312
I0523 04:43:56.937925 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11312 (* 1 = 8.11312 loss)
I0523 04:43:57.777942 34682 sgd_solver.cpp:112] Iteration 51100, lr = 0.01
I0523 04:44:01.832470 34682 solver.cpp:239] Iteration 51110 (2.04317 iter/s, 4.89435s/10 iters), loss = 8.49433
I0523 04:44:01.832512 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49433 (* 1 = 8.49433 loss)
I0523 04:44:02.688566 34682 sgd_solver.cpp:112] Iteration 51110, lr = 0.01
I0523 04:44:07.061802 34682 solver.cpp:239] Iteration 51120 (1.91239 iter/s, 5.22907s/10 iters), loss = 7.06908
I0523 04:44:07.061866 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.06908 (* 1 = 7.06908 loss)
I0523 04:44:07.121955 34682 sgd_solver.cpp:112] Iteration 51120, lr = 0.01
I0523 04:44:12.736846 34682 solver.cpp:239] Iteration 51130 (1.76219 iter/s, 5.67475s/10 iters), loss = 9.36986
I0523 04:44:12.736899 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36986 (* 1 = 9.36986 loss)
I0523 04:44:12.834488 34682 sgd_solver.cpp:112] Iteration 51130, lr = 0.01
I0523 04:44:18.664139 34682 solver.cpp:239] Iteration 51140 (1.68719 iter/s, 5.927s/10 iters), loss = 8.84404
I0523 04:44:18.664417 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84404 (* 1 = 8.84404 loss)
I0523 04:44:18.743913 34682 sgd_solver.cpp:112] Iteration 51140, lr = 0.01
I0523 04:44:22.801090 34682 solver.cpp:239] Iteration 51150 (2.42002 iter/s, 4.1322s/10 iters), loss = 8.56849
I0523 04:44:22.801151 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56849 (* 1 = 8.56849 loss)
I0523 04:44:23.650890 34682 sgd_solver.cpp:112] Iteration 51150, lr = 0.01
I0523 04:44:28.316185 34682 solver.cpp:239] Iteration 51160 (1.8133 iter/s, 5.51481s/10 iters), loss = 8.53417
I0523 04:44:28.316236 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53417 (* 1 = 8.53417 loss)
I0523 04:44:29.112377 34682 sgd_solver.cpp:112] Iteration 51160, lr = 0.01
I0523 04:44:34.899621 34682 solver.cpp:239] Iteration 51170 (1.51904 iter/s, 6.58312s/10 iters), loss = 8.71266
I0523 04:44:34.899663 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71266 (* 1 = 8.71266 loss)
I0523 04:44:34.963098 34682 sgd_solver.cpp:112] Iteration 51170, lr = 0.01
I0523 04:44:40.532871 34682 solver.cpp:239] Iteration 51180 (1.77526 iter/s, 5.63297s/10 iters), loss = 9.02193
I0523 04:44:40.532933 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02193 (* 1 = 9.02193 loss)
I0523 04:44:41.037869 34682 sgd_solver.cpp:112] Iteration 51180, lr = 0.01
I0523 04:44:45.082389 34682 solver.cpp:239] Iteration 51190 (2.19815 iter/s, 4.54927s/10 iters), loss = 9.02131
I0523 04:44:45.082440 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02131 (* 1 = 9.02131 loss)
I0523 04:44:45.861340 34682 sgd_solver.cpp:112] Iteration 51190, lr = 0.01
I0523 04:44:51.212311 34682 solver.cpp:239] Iteration 51200 (1.63142 iter/s, 6.12962s/10 iters), loss = 8.73741
I0523 04:44:51.212452 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73741 (* 1 = 8.73741 loss)
I0523 04:44:51.278560 34682 sgd_solver.cpp:112] Iteration 51200, lr = 0.01
I0523 04:44:56.007158 34682 solver.cpp:239] Iteration 51210 (2.08572 iter/s, 4.79451s/10 iters), loss = 8.11154
I0523 04:44:56.007221 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11154 (* 1 = 8.11154 loss)
I0523 04:44:56.661718 34682 sgd_solver.cpp:112] Iteration 51210, lr = 0.01
I0523 04:44:58.612535 34682 solver.cpp:239] Iteration 51220 (3.83848 iter/s, 2.6052s/10 iters), loss = 7.64432
I0523 04:44:58.612579 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64432 (* 1 = 7.64432 loss)
I0523 04:44:58.687862 34682 sgd_solver.cpp:112] Iteration 51220, lr = 0.01
I0523 04:45:04.032240 34682 solver.cpp:239] Iteration 51230 (1.84521 iter/s, 5.41944s/10 iters), loss = 8.19384
I0523 04:45:04.032285 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19384 (* 1 = 8.19384 loss)
I0523 04:45:04.104898 34682 sgd_solver.cpp:112] Iteration 51230, lr = 0.01
I0523 04:45:08.608817 34682 solver.cpp:239] Iteration 51240 (2.18515 iter/s, 4.57634s/10 iters), loss = 7.76352
I0523 04:45:08.608867 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.76352 (* 1 = 7.76352 loss)
I0523 04:45:08.687508 34682 sgd_solver.cpp:112] Iteration 51240, lr = 0.01
I0523 04:45:13.636765 34682 solver.cpp:239] Iteration 51250 (1.98898 iter/s, 5.0277s/10 iters), loss = 8.23431
I0523 04:45:13.636808 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23431 (* 1 = 8.23431 loss)
I0523 04:45:13.699507 34682 sgd_solver.cpp:112] Iteration 51250, lr = 0.01
I0523 04:45:17.939029 34682 solver.cpp:239] Iteration 51260 (2.32448 iter/s, 4.30204s/10 iters), loss = 9.23739
I0523 04:45:17.939074 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23739 (* 1 = 9.23739 loss)
I0523 04:45:18.015607 34682 sgd_solver.cpp:112] Iteration 51260, lr = 0.01
I0523 04:45:22.304651 34682 solver.cpp:239] Iteration 51270 (2.29074 iter/s, 4.3654s/10 iters), loss = 9.07491
I0523 04:45:22.304908 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07491 (* 1 = 9.07491 loss)
I0523 04:45:22.374624 34682 sgd_solver.cpp:112] Iteration 51270, lr = 0.01
I0523 04:45:26.611574 34682 solver.cpp:239] Iteration 51280 (2.32207 iter/s, 4.30651s/10 iters), loss = 8.60372
I0523 04:45:26.611629 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60372 (* 1 = 8.60372 loss)
I0523 04:45:27.376541 34682 sgd_solver.cpp:112] Iteration 51280, lr = 0.01
I0523 04:45:32.190389 34682 solver.cpp:239] Iteration 51290 (1.79259 iter/s, 5.57853s/10 iters), loss = 8.80138
I0523 04:45:32.190448 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80138 (* 1 = 8.80138 loss)
I0523 04:45:32.253232 34682 sgd_solver.cpp:112] Iteration 51290, lr = 0.01
I0523 04:45:36.846489 34682 solver.cpp:239] Iteration 51300 (2.14784 iter/s, 4.65585s/10 iters), loss = 7.37436
I0523 04:45:36.846529 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.37436 (* 1 = 7.37436 loss)
I0523 04:45:36.910462 34682 sgd_solver.cpp:112] Iteration 51300, lr = 0.01
I0523 04:45:43.582625 34682 solver.cpp:239] Iteration 51310 (1.4846 iter/s, 6.73582s/10 iters), loss = 8.57641
I0523 04:45:43.582680 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57641 (* 1 = 8.57641 loss)
I0523 04:45:43.656221 34682 sgd_solver.cpp:112] Iteration 51310, lr = 0.01
I0523 04:45:48.339920 34682 solver.cpp:239] Iteration 51320 (2.10215 iter/s, 4.75704s/10 iters), loss = 8.18089
I0523 04:45:48.339985 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18089 (* 1 = 8.18089 loss)
I0523 04:45:48.923892 34682 sgd_solver.cpp:112] Iteration 51320, lr = 0.01
I0523 04:45:53.389416 34682 solver.cpp:239] Iteration 51330 (1.9805 iter/s, 5.04923s/10 iters), loss = 8.73808
I0523 04:45:53.389642 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73808 (* 1 = 8.73808 loss)
I0523 04:45:53.467133 34682 sgd_solver.cpp:112] Iteration 51330, lr = 0.01
I0523 04:45:57.542485 34682 solver.cpp:239] Iteration 51340 (2.40807 iter/s, 4.1527s/10 iters), loss = 8.33926
I0523 04:45:57.542531 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33926 (* 1 = 8.33926 loss)
I0523 04:45:57.608177 34682 sgd_solver.cpp:112] Iteration 51340, lr = 0.01
I0523 04:46:04.449095 34682 solver.cpp:239] Iteration 51350 (1.44796 iter/s, 6.90629s/10 iters), loss = 9.2551
I0523 04:46:04.449139 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2551 (* 1 = 9.2551 loss)
I0523 04:46:05.201335 34682 sgd_solver.cpp:112] Iteration 51350, lr = 0.01
I0523 04:46:10.056257 34682 solver.cpp:239] Iteration 51360 (1.78352 iter/s, 5.60688s/10 iters), loss = 8.77988
I0523 04:46:10.056331 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77988 (* 1 = 8.77988 loss)
I0523 04:46:10.296717 34682 sgd_solver.cpp:112] Iteration 51360, lr = 0.01
I0523 04:46:15.303830 34682 solver.cpp:239] Iteration 51370 (1.90575 iter/s, 5.24729s/10 iters), loss = 8.92244
I0523 04:46:15.303887 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92244 (* 1 = 8.92244 loss)
I0523 04:46:16.177433 34682 sgd_solver.cpp:112] Iteration 51370, lr = 0.01
I0523 04:46:20.568590 34682 solver.cpp:239] Iteration 51380 (1.89952 iter/s, 5.26449s/10 iters), loss = 8.5767
I0523 04:46:20.568640 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5767 (* 1 = 8.5767 loss)
I0523 04:46:20.648299 34682 sgd_solver.cpp:112] Iteration 51380, lr = 0.01
I0523 04:46:24.963986 34682 solver.cpp:239] Iteration 51390 (2.27523 iter/s, 4.39517s/10 iters), loss = 9.19533
I0523 04:46:24.964238 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19533 (* 1 = 9.19533 loss)
I0523 04:46:25.778870 34682 sgd_solver.cpp:112] Iteration 51390, lr = 0.01
I0523 04:46:31.490562 34682 solver.cpp:239] Iteration 51400 (1.53231 iter/s, 6.52608s/10 iters), loss = 8.29618
I0523 04:46:31.490623 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29618 (* 1 = 8.29618 loss)
I0523 04:46:32.322065 34682 sgd_solver.cpp:112] Iteration 51400, lr = 0.01
I0523 04:46:38.563625 34682 solver.cpp:239] Iteration 51410 (1.41388 iter/s, 7.07272s/10 iters), loss = 8.8864
I0523 04:46:38.563678 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8864 (* 1 = 8.8864 loss)
I0523 04:46:38.628310 34682 sgd_solver.cpp:112] Iteration 51410, lr = 0.01
I0523 04:46:40.606981 34682 solver.cpp:239] Iteration 51420 (4.89427 iter/s, 2.04321s/10 iters), loss = 8.44743
I0523 04:46:40.607053 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44743 (* 1 = 8.44743 loss)
I0523 04:46:40.659615 34682 sgd_solver.cpp:112] Iteration 51420, lr = 0.01
I0523 04:46:41.848652 34682 solver.cpp:239] Iteration 51430 (8.0545 iter/s, 1.24154s/10 iters), loss = 9.27884
I0523 04:46:41.848703 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27884 (* 1 = 9.27884 loss)
I0523 04:46:41.887369 34682 sgd_solver.cpp:112] Iteration 51430, lr = 0.01
I0523 04:46:43.049394 34682 solver.cpp:239] Iteration 51440 (8.32896 iter/s, 1.20063s/10 iters), loss = 8.88938
I0523 04:46:43.049439 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88938 (* 1 = 8.88938 loss)
I0523 04:46:43.097251 34682 sgd_solver.cpp:112] Iteration 51440, lr = 0.01
I0523 04:46:44.252903 34682 solver.cpp:239] Iteration 51450 (8.30973 iter/s, 1.20341s/10 iters), loss = 8.96677
I0523 04:46:44.252943 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96677 (* 1 = 8.96677 loss)
I0523 04:46:44.301872 34682 sgd_solver.cpp:112] Iteration 51450, lr = 0.01
I0523 04:46:45.664280 34682 solver.cpp:239] Iteration 51460 (7.08581 iter/s, 1.41127s/10 iters), loss = 8.37956
I0523 04:46:45.664350 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37956 (* 1 = 8.37956 loss)
I0523 04:46:45.696049 34682 sgd_solver.cpp:112] Iteration 51460, lr = 0.01
I0523 04:46:46.847789 34682 solver.cpp:239] Iteration 51470 (8.45032 iter/s, 1.18339s/10 iters), loss = 8.98574
I0523 04:46:46.847836 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98574 (* 1 = 8.98574 loss)
I0523 04:46:46.882658 34682 sgd_solver.cpp:112] Iteration 51470, lr = 0.01
I0523 04:46:48.260540 34682 solver.cpp:239] Iteration 51480 (7.07897 iter/s, 1.41263s/10 iters), loss = 8.74679
I0523 04:46:48.260599 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74679 (* 1 = 8.74679 loss)
I0523 04:46:48.302865 34682 sgd_solver.cpp:112] Iteration 51480, lr = 0.01
I0523 04:46:49.875905 34682 solver.cpp:239] Iteration 51490 (6.19107 iter/s, 1.61523s/10 iters), loss = 7.34503
I0523 04:46:49.875952 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.34503 (* 1 = 7.34503 loss)
I0523 04:46:49.943742 34682 sgd_solver.cpp:112] Iteration 51490, lr = 0.01
I0523 04:46:53.144732 34682 solver.cpp:239] Iteration 51500 (3.05937 iter/s, 3.26865s/10 iters), loss = 8.56979
I0523 04:46:53.144773 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56979 (* 1 = 8.56979 loss)
I0523 04:46:53.213358 34682 sgd_solver.cpp:112] Iteration 51500, lr = 0.01
I0523 04:46:57.994685 34682 solver.cpp:239] Iteration 51510 (2.06198 iter/s, 4.84971s/10 iters), loss = 8.4863
I0523 04:46:57.998842 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4863 (* 1 = 8.4863 loss)
I0523 04:46:58.824517 34682 sgd_solver.cpp:112] Iteration 51510, lr = 0.01
I0523 04:47:03.029827 34682 solver.cpp:239] Iteration 51520 (1.98843 iter/s, 5.02909s/10 iters), loss = 9.48731
I0523 04:47:03.029886 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.48731 (* 1 = 9.48731 loss)
I0523 04:47:03.639739 34682 sgd_solver.cpp:112] Iteration 51520, lr = 0.01
I0523 04:47:07.868902 34682 solver.cpp:239] Iteration 51530 (2.06662 iter/s, 4.83883s/10 iters), loss = 9.19859
I0523 04:47:07.868952 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19859 (* 1 = 9.19859 loss)
I0523 04:47:07.937263 34682 sgd_solver.cpp:112] Iteration 51530, lr = 0.01
I0523 04:47:13.751504 34682 solver.cpp:239] Iteration 51540 (1.70001 iter/s, 5.88231s/10 iters), loss = 7.96547
I0523 04:47:13.751566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96547 (* 1 = 7.96547 loss)
I0523 04:47:14.356600 34682 sgd_solver.cpp:112] Iteration 51540, lr = 0.01
I0523 04:47:17.757638 34682 solver.cpp:239] Iteration 51550 (2.49632 iter/s, 4.0059s/10 iters), loss = 8.65487
I0523 04:47:17.757688 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65487 (* 1 = 8.65487 loss)
I0523 04:47:17.837101 34682 sgd_solver.cpp:112] Iteration 51550, lr = 0.01
I0523 04:47:23.653074 34682 solver.cpp:239] Iteration 51560 (1.69631 iter/s, 5.89515s/10 iters), loss = 8.75154
I0523 04:47:23.653122 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75154 (* 1 = 8.75154 loss)
I0523 04:47:23.714705 34682 sgd_solver.cpp:112] Iteration 51560, lr = 0.01
I0523 04:47:29.262715 34682 solver.cpp:239] Iteration 51570 (1.78274 iter/s, 5.60935s/10 iters), loss = 9.725
I0523 04:47:29.262964 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.725 (* 1 = 9.725 loss)
I0523 04:47:29.335572 34682 sgd_solver.cpp:112] Iteration 51570, lr = 0.01
I0523 04:47:34.996507 34682 solver.cpp:239] Iteration 51580 (1.74418 iter/s, 5.73334s/10 iters), loss = 8.22799
I0523 04:47:34.996551 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22799 (* 1 = 8.22799 loss)
I0523 04:47:35.451515 34682 sgd_solver.cpp:112] Iteration 51580, lr = 0.01
I0523 04:47:39.784298 34682 solver.cpp:239] Iteration 51590 (2.08875 iter/s, 4.78755s/10 iters), loss = 8.638
I0523 04:47:39.784343 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.638 (* 1 = 8.638 loss)
I0523 04:47:39.854564 34682 sgd_solver.cpp:112] Iteration 51590, lr = 0.01
I0523 04:47:44.151554 34682 solver.cpp:239] Iteration 51600 (2.28988 iter/s, 4.36703s/10 iters), loss = 8.76874
I0523 04:47:44.151602 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76874 (* 1 = 8.76874 loss)
I0523 04:47:44.457778 34682 sgd_solver.cpp:112] Iteration 51600, lr = 0.01
I0523 04:47:50.372723 34682 solver.cpp:239] Iteration 51610 (1.60749 iter/s, 6.22087s/10 iters), loss = 9.22281
I0523 04:47:50.372771 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22281 (* 1 = 9.22281 loss)
I0523 04:47:50.435725 34682 sgd_solver.cpp:112] Iteration 51610, lr = 0.01
I0523 04:47:56.526119 34682 solver.cpp:239] Iteration 51620 (1.6252 iter/s, 6.1531s/10 iters), loss = 9.13499
I0523 04:47:56.526163 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13499 (* 1 = 9.13499 loss)
I0523 04:47:56.590618 34682 sgd_solver.cpp:112] Iteration 51620, lr = 0.01
I0523 04:48:01.329723 34682 solver.cpp:239] Iteration 51630 (2.08187 iter/s, 4.80336s/10 iters), loss = 8.53592
I0523 04:48:01.329865 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53592 (* 1 = 8.53592 loss)
I0523 04:48:02.143450 34682 sgd_solver.cpp:112] Iteration 51630, lr = 0.01
I0523 04:48:09.262255 34682 solver.cpp:239] Iteration 51640 (1.2607 iter/s, 7.93208s/10 iters), loss = 9.05388
I0523 04:48:09.262295 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05388 (* 1 = 9.05388 loss)
I0523 04:48:09.339287 34682 sgd_solver.cpp:112] Iteration 51640, lr = 0.01
I0523 04:48:13.859750 34682 solver.cpp:239] Iteration 51650 (2.17521 iter/s, 4.59726s/10 iters), loss = 9.08432
I0523 04:48:13.859817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08432 (* 1 = 9.08432 loss)
I0523 04:48:14.689512 34682 sgd_solver.cpp:112] Iteration 51650, lr = 0.01
I0523 04:48:18.670342 34682 solver.cpp:239] Iteration 51660 (2.07886 iter/s, 4.81033s/10 iters), loss = 7.59826
I0523 04:48:18.670389 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.59826 (* 1 = 7.59826 loss)
I0523 04:48:19.488711 34682 sgd_solver.cpp:112] Iteration 51660, lr = 0.01
I0523 04:48:25.647802 34682 solver.cpp:239] Iteration 51670 (1.43325 iter/s, 6.97713s/10 iters), loss = 8.98351
I0523 04:48:25.647855 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98351 (* 1 = 8.98351 loss)
I0523 04:48:26.376883 34682 sgd_solver.cpp:112] Iteration 51670, lr = 0.01
I0523 04:48:31.298454 34682 solver.cpp:239] Iteration 51680 (1.76979 iter/s, 5.65037s/10 iters), loss = 8.4167
I0523 04:48:31.298502 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4167 (* 1 = 8.4167 loss)
I0523 04:48:31.367341 34682 sgd_solver.cpp:112] Iteration 51680, lr = 0.01
I0523 04:48:35.223480 34682 solver.cpp:239] Iteration 51690 (2.54789 iter/s, 3.92481s/10 iters), loss = 8.66798
I0523 04:48:35.223529 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66798 (* 1 = 8.66798 loss)
I0523 04:48:35.287581 34682 sgd_solver.cpp:112] Iteration 51690, lr = 0.01
I0523 04:48:38.821530 34682 solver.cpp:239] Iteration 51700 (2.77944 iter/s, 3.59785s/10 iters), loss = 8.23082
I0523 04:48:38.821576 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23082 (* 1 = 8.23082 loss)
I0523 04:48:39.553786 34682 sgd_solver.cpp:112] Iteration 51700, lr = 0.01
I0523 04:48:43.023922 34682 solver.cpp:239] Iteration 51710 (2.37972 iter/s, 4.20218s/10 iters), loss = 7.9304
I0523 04:48:43.023968 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9304 (* 1 = 7.9304 loss)
I0523 04:48:43.873034 34682 sgd_solver.cpp:112] Iteration 51710, lr = 0.01
I0523 04:48:47.309896 34682 solver.cpp:239] Iteration 51720 (2.33332 iter/s, 4.28574s/10 iters), loss = 9.45408
I0523 04:48:47.309947 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.45408 (* 1 = 9.45408 loss)
I0523 04:48:47.367274 34682 sgd_solver.cpp:112] Iteration 51720, lr = 0.01
I0523 04:48:52.441274 34682 solver.cpp:239] Iteration 51730 (1.9489 iter/s, 5.13111s/10 iters), loss = 8.70694
I0523 04:48:52.441334 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70694 (* 1 = 8.70694 loss)
I0523 04:48:53.301076 34682 sgd_solver.cpp:112] Iteration 51730, lr = 0.01
I0523 04:48:56.420022 34682 solver.cpp:239] Iteration 51740 (2.5135 iter/s, 3.97852s/10 iters), loss = 8.42511
I0523 04:48:56.420090 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42511 (* 1 = 8.42511 loss)
I0523 04:48:57.194990 34682 sgd_solver.cpp:112] Iteration 51740, lr = 0.01
I0523 04:49:05.165642 34682 solver.cpp:239] Iteration 51750 (1.14348 iter/s, 8.74521s/10 iters), loss = 8.51789
I0523 04:49:05.165776 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51789 (* 1 = 8.51789 loss)
I0523 04:49:05.227830 34682 sgd_solver.cpp:112] Iteration 51750, lr = 0.01
I0523 04:49:09.867290 34682 solver.cpp:239] Iteration 51760 (2.12706 iter/s, 4.70133s/10 iters), loss = 8.78576
I0523 04:49:09.867341 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78576 (* 1 = 8.78576 loss)
I0523 04:49:10.299150 34682 sgd_solver.cpp:112] Iteration 51760, lr = 0.01
I0523 04:49:14.957098 34682 solver.cpp:239] Iteration 51770 (1.96481 iter/s, 5.08954s/10 iters), loss = 7.99438
I0523 04:49:14.957182 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99438 (* 1 = 7.99438 loss)
I0523 04:49:15.017190 34682 sgd_solver.cpp:112] Iteration 51770, lr = 0.01
I0523 04:49:18.869174 34682 solver.cpp:239] Iteration 51780 (2.55634 iter/s, 3.91185s/10 iters), loss = 8.4841
I0523 04:49:18.869230 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4841 (* 1 = 8.4841 loss)
I0523 04:49:18.953510 34682 sgd_solver.cpp:112] Iteration 51780, lr = 0.01
I0523 04:49:23.792732 34682 solver.cpp:239] Iteration 51790 (2.03116 iter/s, 4.9233s/10 iters), loss = 8.57807
I0523 04:49:23.792783 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57807 (* 1 = 8.57807 loss)
I0523 04:49:23.866008 34682 sgd_solver.cpp:112] Iteration 51790, lr = 0.01
I0523 04:49:28.867508 34682 solver.cpp:239] Iteration 51800 (1.97063 iter/s, 5.07452s/10 iters), loss = 8.40241
I0523 04:49:28.867552 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40241 (* 1 = 8.40241 loss)
I0523 04:49:28.942406 34682 sgd_solver.cpp:112] Iteration 51800, lr = 0.01
I0523 04:49:33.892418 34682 solver.cpp:239] Iteration 51810 (1.99019 iter/s, 5.02466s/10 iters), loss = 8.40396
I0523 04:49:33.892473 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40396 (* 1 = 8.40396 loss)
I0523 04:49:34.732408 34682 sgd_solver.cpp:112] Iteration 51810, lr = 0.01
I0523 04:49:37.306485 34682 solver.cpp:239] Iteration 51820 (2.92923 iter/s, 3.41387s/10 iters), loss = 8.67079
I0523 04:49:37.306680 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67079 (* 1 = 8.67079 loss)
I0523 04:49:37.847360 34682 sgd_solver.cpp:112] Iteration 51820, lr = 0.01
I0523 04:49:42.636366 34682 solver.cpp:239] Iteration 51830 (1.87636 iter/s, 5.32946s/10 iters), loss = 8.32759
I0523 04:49:42.636420 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32759 (* 1 = 8.32759 loss)
I0523 04:49:42.702067 34682 sgd_solver.cpp:112] Iteration 51830, lr = 0.01
I0523 04:49:46.321919 34682 solver.cpp:239] Iteration 51840 (2.71345 iter/s, 3.68535s/10 iters), loss = 8.90342
I0523 04:49:46.321955 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90342 (* 1 = 8.90342 loss)
I0523 04:49:46.390115 34682 sgd_solver.cpp:112] Iteration 51840, lr = 0.01
I0523 04:49:52.425302 34682 solver.cpp:239] Iteration 51850 (1.63851 iter/s, 6.1031s/10 iters), loss = 8.64369
I0523 04:49:52.425350 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64369 (* 1 = 8.64369 loss)
I0523 04:49:52.492745 34682 sgd_solver.cpp:112] Iteration 51850, lr = 0.01
I0523 04:49:58.306982 34682 solver.cpp:239] Iteration 51860 (1.70028 iter/s, 5.8814s/10 iters), loss = 9.15425
I0523 04:49:58.307025 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15425 (* 1 = 9.15425 loss)
I0523 04:49:58.376111 34682 sgd_solver.cpp:112] Iteration 51860, lr = 0.01
I0523 04:50:03.826570 34682 solver.cpp:239] Iteration 51870 (1.81182 iter/s, 5.51932s/10 iters), loss = 8.54792
I0523 04:50:03.826622 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54792 (* 1 = 8.54792 loss)
I0523 04:50:04.556027 34682 sgd_solver.cpp:112] Iteration 51870, lr = 0.01
I0523 04:50:08.238396 34682 solver.cpp:239] Iteration 51880 (2.26676 iter/s, 4.41159s/10 iters), loss = 8.79746
I0523 04:50:08.238538 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79746 (* 1 = 8.79746 loss)
I0523 04:50:09.091008 34682 sgd_solver.cpp:112] Iteration 51880, lr = 0.01
I0523 04:50:14.401918 34682 solver.cpp:239] Iteration 51890 (1.62255 iter/s, 6.16313s/10 iters), loss = 9.12256
I0523 04:50:14.401967 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12256 (* 1 = 9.12256 loss)
I0523 04:50:15.204504 34682 sgd_solver.cpp:112] Iteration 51890, lr = 0.01
I0523 04:50:18.297014 34682 solver.cpp:239] Iteration 51900 (2.56747 iter/s, 3.89488s/10 iters), loss = 9.61296
I0523 04:50:18.297080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.61296 (* 1 = 9.61296 loss)
I0523 04:50:19.132917 34682 sgd_solver.cpp:112] Iteration 51900, lr = 0.01
I0523 04:50:26.346289 34682 solver.cpp:239] Iteration 51910 (1.24241 iter/s, 8.04889s/10 iters), loss = 8.74107
I0523 04:50:26.346329 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74107 (* 1 = 8.74107 loss)
I0523 04:50:26.423539 34682 sgd_solver.cpp:112] Iteration 51910, lr = 0.01
I0523 04:50:29.784963 34682 solver.cpp:239] Iteration 51920 (2.90826 iter/s, 3.43849s/10 iters), loss = 8.70335
I0523 04:50:29.785009 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70335 (* 1 = 8.70335 loss)
I0523 04:50:30.574462 34682 sgd_solver.cpp:112] Iteration 51920, lr = 0.01
I0523 04:50:34.358376 34682 solver.cpp:239] Iteration 51930 (2.18666 iter/s, 4.57317s/10 iters), loss = 8.00087
I0523 04:50:34.358422 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00087 (* 1 = 8.00087 loss)
I0523 04:50:34.419507 34682 sgd_solver.cpp:112] Iteration 51930, lr = 0.01
I0523 04:50:37.024111 34682 solver.cpp:239] Iteration 51940 (3.75153 iter/s, 2.66558s/10 iters), loss = 7.82247
I0523 04:50:37.024157 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82247 (* 1 = 7.82247 loss)
I0523 04:50:37.090453 34682 sgd_solver.cpp:112] Iteration 51940, lr = 0.01
I0523 04:50:41.876448 34682 solver.cpp:239] Iteration 51950 (2.06097 iter/s, 4.85209s/10 iters), loss = 8.40281
I0523 04:50:41.876685 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40281 (* 1 = 8.40281 loss)
I0523 04:50:41.956655 34682 sgd_solver.cpp:112] Iteration 51950, lr = 0.01
I0523 04:50:46.240860 34682 solver.cpp:239] Iteration 51960 (2.29147 iter/s, 4.36402s/10 iters), loss = 8.5235
I0523 04:50:46.240900 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5235 (* 1 = 8.5235 loss)
I0523 04:50:46.506515 34682 sgd_solver.cpp:112] Iteration 51960, lr = 0.01
I0523 04:50:51.694422 34682 solver.cpp:239] Iteration 51970 (1.83375 iter/s, 5.4533s/10 iters), loss = 8.90839
I0523 04:50:51.694471 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90839 (* 1 = 8.90839 loss)
I0523 04:50:51.773031 34682 sgd_solver.cpp:112] Iteration 51970, lr = 0.01
I0523 04:50:55.094127 34682 solver.cpp:239] Iteration 51980 (2.94161 iter/s, 3.3995s/10 iters), loss = 8.74578
I0523 04:50:55.094193 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74578 (* 1 = 8.74578 loss)
I0523 04:50:55.953903 34682 sgd_solver.cpp:112] Iteration 51980, lr = 0.01
I0523 04:50:59.981691 34682 solver.cpp:239] Iteration 51990 (2.04612 iter/s, 4.8873s/10 iters), loss = 8.72974
I0523 04:50:59.981737 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72974 (* 1 = 8.72974 loss)
I0523 04:51:00.058213 34682 sgd_solver.cpp:112] Iteration 51990, lr = 0.01
I0523 04:51:03.605516 34682 solver.cpp:239] Iteration 52000 (2.75966 iter/s, 3.62363s/10 iters), loss = 8.3629
I0523 04:51:03.605566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3629 (* 1 = 8.3629 loss)
I0523 04:51:04.117839 34682 sgd_solver.cpp:112] Iteration 52000, lr = 0.01
I0523 04:51:06.769408 34682 solver.cpp:239] Iteration 52010 (3.16086 iter/s, 3.1637s/10 iters), loss = 8.9632
I0523 04:51:06.769457 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9632 (* 1 = 8.9632 loss)
I0523 04:51:07.629945 34682 sgd_solver.cpp:112] Iteration 52010, lr = 0.01
I0523 04:51:09.485463 34682 solver.cpp:239] Iteration 52020 (3.68203 iter/s, 2.71589s/10 iters), loss = 8.38344
I0523 04:51:09.485512 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38344 (* 1 = 8.38344 loss)
I0523 04:51:09.546113 34682 sgd_solver.cpp:112] Iteration 52020, lr = 0.01
I0523 04:51:14.360204 34682 solver.cpp:239] Iteration 52030 (2.05149 iter/s, 4.8745s/10 iters), loss = 8.1545
I0523 04:51:14.360425 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1545 (* 1 = 8.1545 loss)
I0523 04:51:14.433378 34682 sgd_solver.cpp:112] Iteration 52030, lr = 0.01
I0523 04:51:19.550388 34682 solver.cpp:239] Iteration 52040 (1.92686 iter/s, 5.18979s/10 iters), loss = 8.61937
I0523 04:51:19.550426 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61937 (* 1 = 8.61937 loss)
I0523 04:51:20.269476 34682 sgd_solver.cpp:112] Iteration 52040, lr = 0.01
I0523 04:51:22.921700 34682 solver.cpp:239] Iteration 52050 (2.96636 iter/s, 3.37113s/10 iters), loss = 9.17171
I0523 04:51:22.921751 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17171 (* 1 = 9.17171 loss)
I0523 04:51:22.999233 34682 sgd_solver.cpp:112] Iteration 52050, lr = 0.01
I0523 04:51:26.248167 34682 solver.cpp:239] Iteration 52060 (3.00636 iter/s, 3.32628s/10 iters), loss = 8.13821
I0523 04:51:26.248217 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13821 (* 1 = 8.13821 loss)
I0523 04:51:26.320768 34682 sgd_solver.cpp:112] Iteration 52060, lr = 0.01
I0523 04:51:30.877212 34682 solver.cpp:239] Iteration 52070 (2.16038 iter/s, 4.6288s/10 iters), loss = 8.67599
I0523 04:51:30.877250 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67599 (* 1 = 8.67599 loss)
I0523 04:51:30.956558 34682 sgd_solver.cpp:112] Iteration 52070, lr = 0.01
I0523 04:51:36.182786 34682 solver.cpp:239] Iteration 52080 (1.8849 iter/s, 5.30532s/10 iters), loss = 8.22308
I0523 04:51:36.182827 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22308 (* 1 = 8.22308 loss)
I0523 04:51:36.246613 34682 sgd_solver.cpp:112] Iteration 52080, lr = 0.01
I0523 04:51:42.839900 34682 solver.cpp:239] Iteration 52090 (1.50222 iter/s, 6.6568s/10 iters), loss = 8.02259
I0523 04:51:42.839952 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02259 (* 1 = 8.02259 loss)
I0523 04:51:43.582135 34682 sgd_solver.cpp:112] Iteration 52090, lr = 0.01
I0523 04:51:50.562855 34682 solver.cpp:239] Iteration 52100 (1.2949 iter/s, 7.72258s/10 iters), loss = 8.83652
I0523 04:51:50.563127 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83652 (* 1 = 8.83652 loss)
I0523 04:51:51.239346 34682 sgd_solver.cpp:112] Iteration 52100, lr = 0.01
I0523 04:51:55.542778 34682 solver.cpp:239] Iteration 52110 (2.00825 iter/s, 4.97946s/10 iters), loss = 7.79955
I0523 04:51:55.542858 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79955 (* 1 = 7.79955 loss)
I0523 04:51:55.828377 34682 sgd_solver.cpp:112] Iteration 52110, lr = 0.01
I0523 04:52:02.262107 34682 solver.cpp:239] Iteration 52120 (1.48832 iter/s, 6.71898s/10 iters), loss = 8.80476
I0523 04:52:02.262156 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80476 (* 1 = 8.80476 loss)
I0523 04:52:02.335613 34682 sgd_solver.cpp:112] Iteration 52120, lr = 0.01
I0523 04:52:05.732903 34682 solver.cpp:239] Iteration 52130 (2.88135 iter/s, 3.4706s/10 iters), loss = 8.27777
I0523 04:52:05.732950 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27777 (* 1 = 8.27777 loss)
I0523 04:52:06.601704 34682 sgd_solver.cpp:112] Iteration 52130, lr = 0.01
I0523 04:52:10.661047 34682 solver.cpp:239] Iteration 52140 (2.02927 iter/s, 4.92789s/10 iters), loss = 8.46694
I0523 04:52:10.661109 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46694 (* 1 = 8.46694 loss)
I0523 04:52:11.480077 34682 sgd_solver.cpp:112] Iteration 52140, lr = 0.01
I0523 04:52:16.168635 34682 solver.cpp:239] Iteration 52150 (1.81577 iter/s, 5.5073s/10 iters), loss = 8.36813
I0523 04:52:16.168687 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36813 (* 1 = 8.36813 loss)
I0523 04:52:16.832036 34682 sgd_solver.cpp:112] Iteration 52150, lr = 0.01
I0523 04:52:22.527927 34682 solver.cpp:239] Iteration 52160 (1.57258 iter/s, 6.35899s/10 iters), loss = 8.7729
I0523 04:52:22.528121 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7729 (* 1 = 8.7729 loss)
I0523 04:52:22.605636 34682 sgd_solver.cpp:112] Iteration 52160, lr = 0.01
I0523 04:52:27.349248 34682 solver.cpp:239] Iteration 52170 (2.07428 iter/s, 4.82096s/10 iters), loss = 9.37319
I0523 04:52:27.349295 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.37319 (* 1 = 9.37319 loss)
I0523 04:52:27.427671 34682 sgd_solver.cpp:112] Iteration 52170, lr = 0.01
I0523 04:52:33.220576 34682 solver.cpp:239] Iteration 52180 (1.70327 iter/s, 5.87104s/10 iters), loss = 8.66893
I0523 04:52:33.220620 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66893 (* 1 = 8.66893 loss)
I0523 04:52:34.023051 34682 sgd_solver.cpp:112] Iteration 52180, lr = 0.01
I0523 04:52:39.683440 34682 solver.cpp:239] Iteration 52190 (1.54737 iter/s, 6.46256s/10 iters), loss = 8.2907
I0523 04:52:39.683482 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2907 (* 1 = 8.2907 loss)
I0523 04:52:39.752701 34682 sgd_solver.cpp:112] Iteration 52190, lr = 0.01
I0523 04:52:44.501927 34682 solver.cpp:239] Iteration 52200 (2.07544 iter/s, 4.81824s/10 iters), loss = 9.17138
I0523 04:52:44.501971 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17138 (* 1 = 9.17138 loss)
I0523 04:52:44.578974 34682 sgd_solver.cpp:112] Iteration 52200, lr = 0.01
I0523 04:52:50.126030 34682 solver.cpp:239] Iteration 52210 (1.77815 iter/s, 5.62383s/10 iters), loss = 8.65744
I0523 04:52:50.126075 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65744 (* 1 = 8.65744 loss)
I0523 04:52:50.913158 34682 sgd_solver.cpp:112] Iteration 52210, lr = 0.01
I0523 04:52:56.385184 34682 solver.cpp:239] Iteration 52220 (1.59774 iter/s, 6.25884s/10 iters), loss = 9.42082
I0523 04:52:56.385390 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.42082 (* 1 = 9.42082 loss)
I0523 04:52:56.443936 34682 sgd_solver.cpp:112] Iteration 52220, lr = 0.01
I0523 04:53:02.300242 34682 solver.cpp:239] Iteration 52230 (1.69073 iter/s, 5.91461s/10 iters), loss = 8.42916
I0523 04:53:02.300285 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42916 (* 1 = 8.42916 loss)
I0523 04:53:02.681222 34682 sgd_solver.cpp:112] Iteration 52230, lr = 0.01
I0523 04:53:07.799332 34682 solver.cpp:239] Iteration 52240 (1.81857 iter/s, 5.49882s/10 iters), loss = 9.02235
I0523 04:53:07.799389 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02235 (* 1 = 9.02235 loss)
I0523 04:53:07.970533 34682 sgd_solver.cpp:112] Iteration 52240, lr = 0.01
I0523 04:53:13.311195 34682 solver.cpp:239] Iteration 52250 (1.81436 iter/s, 5.51159s/10 iters), loss = 7.84407
I0523 04:53:13.311244 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84407 (* 1 = 7.84407 loss)
I0523 04:53:13.385282 34682 sgd_solver.cpp:112] Iteration 52250, lr = 0.01
I0523 04:53:19.005193 34682 solver.cpp:239] Iteration 52260 (1.75632 iter/s, 5.69371s/10 iters), loss = 9.09464
I0523 04:53:19.005254 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09464 (* 1 = 9.09464 loss)
I0523 04:53:19.695003 34682 sgd_solver.cpp:112] Iteration 52260, lr = 0.01
I0523 04:53:22.918043 34682 solver.cpp:239] Iteration 52270 (2.55582 iter/s, 3.91263s/10 iters), loss = 8.74503
I0523 04:53:22.918085 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74503 (* 1 = 8.74503 loss)
I0523 04:53:22.982388 34682 sgd_solver.cpp:112] Iteration 52270, lr = 0.01
I0523 04:53:28.475453 34682 solver.cpp:239] Iteration 52280 (1.79949 iter/s, 5.55714s/10 iters), loss = 9.25406
I0523 04:53:28.475549 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25406 (* 1 = 9.25406 loss)
I0523 04:53:29.322127 34682 sgd_solver.cpp:112] Iteration 52280, lr = 0.01
I0523 04:53:33.415266 34682 solver.cpp:239] Iteration 52290 (2.02449 iter/s, 4.93951s/10 iters), loss = 9.41577
I0523 04:53:33.415333 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.41577 (* 1 = 9.41577 loss)
I0523 04:53:33.488101 34682 sgd_solver.cpp:112] Iteration 52290, lr = 0.01
I0523 04:53:38.692029 34682 solver.cpp:239] Iteration 52300 (1.8952 iter/s, 5.27649s/10 iters), loss = 8.56506
I0523 04:53:38.692088 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56506 (* 1 = 8.56506 loss)
I0523 04:53:39.329965 34682 sgd_solver.cpp:112] Iteration 52300, lr = 0.01
I0523 04:53:43.475502 34682 solver.cpp:239] Iteration 52310 (2.09064 iter/s, 4.78322s/10 iters), loss = 8.37435
I0523 04:53:43.475543 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37435 (* 1 = 8.37435 loss)
I0523 04:53:43.561480 34682 sgd_solver.cpp:112] Iteration 52310, lr = 0.01
I0523 04:53:48.439502 34682 solver.cpp:239] Iteration 52320 (2.0146 iter/s, 4.96375s/10 iters), loss = 8.54907
I0523 04:53:48.439553 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54907 (* 1 = 8.54907 loss)
I0523 04:53:48.511435 34682 sgd_solver.cpp:112] Iteration 52320, lr = 0.01
I0523 04:53:51.796077 34682 solver.cpp:239] Iteration 52330 (2.9794 iter/s, 3.35638s/10 iters), loss = 8.67871
I0523 04:53:51.796135 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67871 (* 1 = 8.67871 loss)
I0523 04:53:52.524758 34682 sgd_solver.cpp:112] Iteration 52330, lr = 0.01
I0523 04:53:55.880313 34682 solver.cpp:239] Iteration 52340 (2.44857 iter/s, 4.08401s/10 iters), loss = 7.88843
I0523 04:53:55.880358 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88843 (* 1 = 7.88843 loss)
I0523 04:53:55.954928 34682 sgd_solver.cpp:112] Iteration 52340, lr = 0.01
I0523 04:54:00.933322 34682 solver.cpp:239] Iteration 52350 (1.97912 iter/s, 5.05275s/10 iters), loss = 8.56849
I0523 04:54:00.933593 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56849 (* 1 = 8.56849 loss)
I0523 04:54:01.008600 34682 sgd_solver.cpp:112] Iteration 52350, lr = 0.01
I0523 04:54:04.808295 34682 solver.cpp:239] Iteration 52360 (2.58095 iter/s, 3.87455s/10 iters), loss = 9.7091
I0523 04:54:04.808351 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.7091 (* 1 = 9.7091 loss)
I0523 04:54:04.888798 34682 sgd_solver.cpp:112] Iteration 52360, lr = 0.01
I0523 04:54:09.299872 34682 solver.cpp:239] Iteration 52370 (2.22652 iter/s, 4.49132s/10 iters), loss = 7.91111
I0523 04:54:09.299935 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91111 (* 1 = 7.91111 loss)
I0523 04:54:10.110328 34682 sgd_solver.cpp:112] Iteration 52370, lr = 0.01
I0523 04:54:14.081218 34682 solver.cpp:239] Iteration 52380 (2.09158 iter/s, 4.78108s/10 iters), loss = 9.11964
I0523 04:54:14.081276 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11964 (* 1 = 9.11964 loss)
I0523 04:54:14.852741 34682 sgd_solver.cpp:112] Iteration 52380, lr = 0.01
I0523 04:54:20.276319 34682 solver.cpp:239] Iteration 52390 (1.61426 iter/s, 6.19479s/10 iters), loss = 8.31687
I0523 04:54:20.276370 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31687 (* 1 = 8.31687 loss)
I0523 04:54:20.333148 34682 sgd_solver.cpp:112] Iteration 52390, lr = 0.01
I0523 04:54:24.529721 34682 solver.cpp:239] Iteration 52400 (2.35118 iter/s, 4.25318s/10 iters), loss = 8.45147
I0523 04:54:24.529773 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45147 (* 1 = 8.45147 loss)
I0523 04:54:25.183540 34682 sgd_solver.cpp:112] Iteration 52400, lr = 0.01
I0523 04:54:30.573369 34682 solver.cpp:239] Iteration 52410 (1.65471 iter/s, 6.04335s/10 iters), loss = 8.35039
I0523 04:54:30.573423 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35039 (* 1 = 8.35039 loss)
I0523 04:54:31.338690 34682 sgd_solver.cpp:112] Iteration 52410, lr = 0.01
I0523 04:54:36.257259 34682 solver.cpp:239] Iteration 52420 (1.75945 iter/s, 5.6836s/10 iters), loss = 8.36841
I0523 04:54:36.257313 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36841 (* 1 = 8.36841 loss)
I0523 04:54:37.074373 34682 sgd_solver.cpp:112] Iteration 52420, lr = 0.01
I0523 04:54:42.632130 34682 solver.cpp:239] Iteration 52430 (1.56873 iter/s, 6.37456s/10 iters), loss = 8.28339
I0523 04:54:42.632174 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28339 (* 1 = 8.28339 loss)
I0523 04:54:43.480836 34682 sgd_solver.cpp:112] Iteration 52430, lr = 0.01
I0523 04:54:48.411300 34682 solver.cpp:239] Iteration 52440 (1.73044 iter/s, 5.77889s/10 iters), loss = 8.81522
I0523 04:54:48.411367 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81522 (* 1 = 8.81522 loss)
I0523 04:54:48.474400 34682 sgd_solver.cpp:112] Iteration 52440, lr = 0.01
I0523 04:54:52.591774 34682 solver.cpp:239] Iteration 52450 (2.39221 iter/s, 4.18024s/10 iters), loss = 9.47135
I0523 04:54:52.591819 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47135 (* 1 = 9.47135 loss)
I0523 04:54:52.654933 34682 sgd_solver.cpp:112] Iteration 52450, lr = 0.01
I0523 04:54:56.382104 34682 solver.cpp:239] Iteration 52460 (2.63844 iter/s, 3.79012s/10 iters), loss = 8.15182
I0523 04:54:56.382151 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15182 (* 1 = 8.15182 loss)
I0523 04:54:56.448595 34682 sgd_solver.cpp:112] Iteration 52460, lr = 0.01
I0523 04:54:59.774559 34682 solver.cpp:239] Iteration 52470 (2.94788 iter/s, 3.39226s/10 iters), loss = 9.21386
I0523 04:54:59.774632 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21386 (* 1 = 9.21386 loss)
I0523 04:55:00.629650 34682 sgd_solver.cpp:112] Iteration 52470, lr = 0.01
I0523 04:55:05.488896 34682 solver.cpp:239] Iteration 52480 (1.75008 iter/s, 5.71404s/10 iters), loss = 9.21337
I0523 04:55:05.489152 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21337 (* 1 = 9.21337 loss)
I0523 04:55:05.558913 34682 sgd_solver.cpp:112] Iteration 52480, lr = 0.01
I0523 04:55:09.684944 34682 solver.cpp:239] Iteration 52490 (2.38342 iter/s, 4.19564s/10 iters), loss = 9.00984
I0523 04:55:09.684985 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00984 (* 1 = 9.00984 loss)
I0523 04:55:09.755614 34682 sgd_solver.cpp:112] Iteration 52490, lr = 0.01
I0523 04:55:13.905699 34682 solver.cpp:239] Iteration 52500 (2.36937 iter/s, 4.22053s/10 iters), loss = 8.33751
I0523 04:55:13.905752 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33751 (* 1 = 8.33751 loss)
I0523 04:55:13.974704 34682 sgd_solver.cpp:112] Iteration 52500, lr = 0.01
I0523 04:55:17.375411 34682 solver.cpp:239] Iteration 52510 (2.88225 iter/s, 3.46951s/10 iters), loss = 8.33798
I0523 04:55:17.375468 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33798 (* 1 = 8.33798 loss)
I0523 04:55:17.775202 34682 sgd_solver.cpp:112] Iteration 52510, lr = 0.01
I0523 04:55:21.642750 34682 solver.cpp:239] Iteration 52520 (2.34351 iter/s, 4.26711s/10 iters), loss = 7.36511
I0523 04:55:21.642805 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.36511 (* 1 = 7.36511 loss)
I0523 04:55:22.433742 34682 sgd_solver.cpp:112] Iteration 52520, lr = 0.01
I0523 04:55:26.471448 34682 solver.cpp:239] Iteration 52530 (2.07106 iter/s, 4.82844s/10 iters), loss = 8.96082
I0523 04:55:26.471508 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96082 (* 1 = 8.96082 loss)
I0523 04:55:26.533773 34682 sgd_solver.cpp:112] Iteration 52530, lr = 0.01
I0523 04:55:30.110903 34682 solver.cpp:239] Iteration 52540 (2.74782 iter/s, 3.63925s/10 iters), loss = 8.17675
I0523 04:55:30.110950 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17675 (* 1 = 8.17675 loss)
I0523 04:55:30.173249 34682 sgd_solver.cpp:112] Iteration 52540, lr = 0.01
I0523 04:55:33.391468 34682 solver.cpp:239] Iteration 52550 (3.04843 iter/s, 3.28038s/10 iters), loss = 7.72034
I0523 04:55:33.391510 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72034 (* 1 = 7.72034 loss)
I0523 04:55:33.468945 34682 sgd_solver.cpp:112] Iteration 52550, lr = 0.01
I0523 04:55:38.732946 34682 solver.cpp:239] Iteration 52560 (1.87223 iter/s, 5.34121s/10 iters), loss = 8.43377
I0523 04:55:38.733153 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43377 (* 1 = 8.43377 loss)
I0523 04:55:38.796696 34682 sgd_solver.cpp:112] Iteration 52560, lr = 0.01
I0523 04:55:44.351482 34682 solver.cpp:239] Iteration 52570 (1.77996 iter/s, 5.61812s/10 iters), loss = 9.57916
I0523 04:55:44.351531 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.57916 (* 1 = 9.57916 loss)
I0523 04:55:44.423121 34682 sgd_solver.cpp:112] Iteration 52570, lr = 0.01
I0523 04:55:49.691062 34682 solver.cpp:239] Iteration 52580 (1.8729 iter/s, 5.33931s/10 iters), loss = 8.44478
I0523 04:55:49.691105 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44478 (* 1 = 8.44478 loss)
I0523 04:55:49.767488 34682 sgd_solver.cpp:112] Iteration 52580, lr = 0.01
I0523 04:55:54.559727 34682 solver.cpp:239] Iteration 52590 (2.05406 iter/s, 4.86841s/10 iters), loss = 8.82003
I0523 04:55:54.559778 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82003 (* 1 = 8.82003 loss)
I0523 04:55:55.327046 34682 sgd_solver.cpp:112] Iteration 52590, lr = 0.01
I0523 04:55:59.842943 34682 solver.cpp:239] Iteration 52600 (1.89288 iter/s, 5.28294s/10 iters), loss = 8.5308
I0523 04:55:59.843006 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5308 (* 1 = 8.5308 loss)
I0523 04:56:00.628883 34682 sgd_solver.cpp:112] Iteration 52600, lr = 0.01
I0523 04:56:05.120416 34682 solver.cpp:239] Iteration 52610 (1.89495 iter/s, 5.2772s/10 iters), loss = 8.60892
I0523 04:56:05.120456 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60892 (* 1 = 8.60892 loss)
I0523 04:56:05.198397 34682 sgd_solver.cpp:112] Iteration 52610, lr = 0.01
I0523 04:56:10.293750 34682 solver.cpp:239] Iteration 52620 (1.93308 iter/s, 5.17308s/10 iters), loss = 8.17625
I0523 04:56:10.293982 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17625 (* 1 = 8.17625 loss)
I0523 04:56:10.958489 34682 sgd_solver.cpp:112] Iteration 52620, lr = 0.01
I0523 04:56:14.157582 34682 solver.cpp:239] Iteration 52630 (2.58836 iter/s, 3.86345s/10 iters), loss = 8.56088
I0523 04:56:14.157634 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56088 (* 1 = 8.56088 loss)
I0523 04:56:14.963672 34682 sgd_solver.cpp:112] Iteration 52630, lr = 0.01
I0523 04:56:19.519748 34682 solver.cpp:239] Iteration 52640 (1.86582 iter/s, 5.35956s/10 iters), loss = 9.00944
I0523 04:56:19.519791 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00944 (* 1 = 9.00944 loss)
I0523 04:56:19.583158 34682 sgd_solver.cpp:112] Iteration 52640, lr = 0.01
I0523 04:56:25.240677 34682 solver.cpp:239] Iteration 52650 (1.74805 iter/s, 5.72065s/10 iters), loss = 9.10786
I0523 04:56:25.240737 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10786 (* 1 = 9.10786 loss)
I0523 04:56:25.314548 34682 sgd_solver.cpp:112] Iteration 52650, lr = 0.01
I0523 04:56:29.464157 34682 solver.cpp:239] Iteration 52660 (2.36785 iter/s, 4.22324s/10 iters), loss = 9.2403
I0523 04:56:29.464207 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2403 (* 1 = 9.2403 loss)
I0523 04:56:29.533573 34682 sgd_solver.cpp:112] Iteration 52660, lr = 0.01
I0523 04:56:32.180529 34682 solver.cpp:239] Iteration 52670 (3.6816 iter/s, 2.71621s/10 iters), loss = 7.95192
I0523 04:56:32.180578 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95192 (* 1 = 7.95192 loss)
I0523 04:56:32.994529 34682 sgd_solver.cpp:112] Iteration 52670, lr = 0.01
I0523 04:56:38.946985 34682 solver.cpp:239] Iteration 52680 (1.47795 iter/s, 6.76613s/10 iters), loss = 8.42632
I0523 04:56:38.947038 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42632 (* 1 = 8.42632 loss)
I0523 04:56:39.702997 34682 sgd_solver.cpp:112] Iteration 52680, lr = 0.01
I0523 04:56:43.992724 34682 solver.cpp:239] Iteration 52690 (1.98197 iter/s, 5.04548s/10 iters), loss = 8.9006
I0523 04:56:43.992926 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9006 (* 1 = 8.9006 loss)
I0523 04:56:44.052496 34682 sgd_solver.cpp:112] Iteration 52690, lr = 0.01
I0523 04:56:49.040200 34682 solver.cpp:239] Iteration 52700 (1.98134 iter/s, 5.0471s/10 iters), loss = 8.02615
I0523 04:56:49.040244 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02615 (* 1 = 8.02615 loss)
I0523 04:56:49.102438 34682 sgd_solver.cpp:112] Iteration 52700, lr = 0.01
I0523 04:56:53.713698 34682 solver.cpp:239] Iteration 52710 (2.13983 iter/s, 4.67327s/10 iters), loss = 8.03617
I0523 04:56:53.713747 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03617 (* 1 = 8.03617 loss)
I0523 04:56:54.532475 34682 sgd_solver.cpp:112] Iteration 52710, lr = 0.01
I0523 04:56:58.518579 34682 solver.cpp:239] Iteration 52720 (2.08133 iter/s, 4.80463s/10 iters), loss = 8.68269
I0523 04:56:58.518630 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68269 (* 1 = 8.68269 loss)
I0523 04:56:58.589671 34682 sgd_solver.cpp:112] Iteration 52720, lr = 0.01
I0523 04:57:03.963189 34682 solver.cpp:239] Iteration 52730 (1.83677 iter/s, 5.44434s/10 iters), loss = 8.62979
I0523 04:57:03.963235 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62979 (* 1 = 8.62979 loss)
I0523 04:57:04.023015 34682 sgd_solver.cpp:112] Iteration 52730, lr = 0.01
I0523 04:57:08.611368 34682 solver.cpp:239] Iteration 52740 (2.15149 iter/s, 4.64794s/10 iters), loss = 8.85395
I0523 04:57:08.611420 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85395 (* 1 = 8.85395 loss)
I0523 04:57:09.363931 34682 sgd_solver.cpp:112] Iteration 52740, lr = 0.01
I0523 04:57:13.520279 34682 solver.cpp:239] Iteration 52750 (2.03722 iter/s, 4.90866s/10 iters), loss = 9.69677
I0523 04:57:13.520340 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.69677 (* 1 = 9.69677 loss)
I0523 04:57:13.597008 34682 sgd_solver.cpp:112] Iteration 52750, lr = 0.01
I0523 04:57:16.975961 34682 solver.cpp:239] Iteration 52760 (2.89396 iter/s, 3.45548s/10 iters), loss = 9.0653
I0523 04:57:16.976238 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0653 (* 1 = 9.0653 loss)
I0523 04:57:17.044953 34682 sgd_solver.cpp:112] Iteration 52760, lr = 0.01
I0523 04:57:24.380328 34682 solver.cpp:239] Iteration 52770 (1.35065 iter/s, 7.40382s/10 iters), loss = 7.91391
I0523 04:57:24.380367 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91391 (* 1 = 7.91391 loss)
I0523 04:57:24.443379 34682 sgd_solver.cpp:112] Iteration 52770, lr = 0.01
I0523 04:57:29.322327 34682 solver.cpp:239] Iteration 52780 (2.02359 iter/s, 4.94172s/10 iters), loss = 8.94732
I0523 04:57:29.322412 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94732 (* 1 = 8.94732 loss)
I0523 04:57:30.039048 34682 sgd_solver.cpp:112] Iteration 52780, lr = 0.01
I0523 04:57:34.023984 34682 solver.cpp:239] Iteration 52790 (2.12704 iter/s, 4.70138s/10 iters), loss = 9.08601
I0523 04:57:34.024058 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08601 (* 1 = 9.08601 loss)
I0523 04:57:34.673458 34682 sgd_solver.cpp:112] Iteration 52790, lr = 0.01
I0523 04:57:38.916280 34682 solver.cpp:239] Iteration 52800 (2.04414 iter/s, 4.89202s/10 iters), loss = 9.61523
I0523 04:57:38.916330 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.61523 (* 1 = 9.61523 loss)
I0523 04:57:38.979152 34682 sgd_solver.cpp:112] Iteration 52800, lr = 0.01
I0523 04:57:42.084455 34682 solver.cpp:239] Iteration 52810 (3.15657 iter/s, 3.16799s/10 iters), loss = 7.66725
I0523 04:57:42.084496 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66725 (* 1 = 7.66725 loss)
I0523 04:57:42.158798 34682 sgd_solver.cpp:112] Iteration 52810, lr = 0.01
I0523 04:57:47.105696 34682 solver.cpp:239] Iteration 52820 (1.99164 iter/s, 5.02099s/10 iters), loss = 7.95144
I0523 04:57:47.105949 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95144 (* 1 = 7.95144 loss)
I0523 04:57:47.890832 34682 sgd_solver.cpp:112] Iteration 52820, lr = 0.01
I0523 04:57:52.702072 34682 solver.cpp:239] Iteration 52830 (1.78702 iter/s, 5.5959s/10 iters), loss = 8.58686
I0523 04:57:52.702117 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58686 (* 1 = 8.58686 loss)
I0523 04:57:52.770870 34682 sgd_solver.cpp:112] Iteration 52830, lr = 0.01
I0523 04:57:56.917243 34682 solver.cpp:239] Iteration 52840 (2.37251 iter/s, 4.21494s/10 iters), loss = 8.08903
I0523 04:57:56.917290 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08903 (* 1 = 8.08903 loss)
I0523 04:57:56.991971 34682 sgd_solver.cpp:112] Iteration 52840, lr = 0.01
I0523 04:58:01.589725 34682 solver.cpp:239] Iteration 52850 (2.1403 iter/s, 4.67224s/10 iters), loss = 8.4081
I0523 04:58:01.589771 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4081 (* 1 = 8.4081 loss)
I0523 04:58:02.380697 34682 sgd_solver.cpp:112] Iteration 52850, lr = 0.01
I0523 04:58:05.806213 34682 solver.cpp:239] Iteration 52860 (2.37177 iter/s, 4.21625s/10 iters), loss = 8.47991
I0523 04:58:05.806296 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47991 (* 1 = 8.47991 loss)
I0523 04:58:06.596680 34682 sgd_solver.cpp:112] Iteration 52860, lr = 0.01
I0523 04:58:11.241578 34682 solver.cpp:239] Iteration 52870 (1.83991 iter/s, 5.43504s/10 iters), loss = 8.5685
I0523 04:58:11.241621 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5685 (* 1 = 8.5685 loss)
I0523 04:58:11.303375 34682 sgd_solver.cpp:112] Iteration 52870, lr = 0.01
I0523 04:58:15.184381 34682 solver.cpp:239] Iteration 52880 (2.5364 iter/s, 3.94259s/10 iters), loss = 8.81719
I0523 04:58:15.184442 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81719 (* 1 = 8.81719 loss)
I0523 04:58:16.001379 34682 sgd_solver.cpp:112] Iteration 52880, lr = 0.01
I0523 04:58:19.770839 34682 solver.cpp:239] Iteration 52890 (2.18045 iter/s, 4.58621s/10 iters), loss = 7.93045
I0523 04:58:19.771001 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93045 (* 1 = 7.93045 loss)
I0523 04:58:19.847980 34682 sgd_solver.cpp:112] Iteration 52890, lr = 0.01
I0523 04:58:24.096086 34682 solver.cpp:239] Iteration 52900 (2.31219 iter/s, 4.3249s/10 iters), loss = 8.80066
I0523 04:58:24.096184 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80066 (* 1 = 8.80066 loss)
I0523 04:58:24.158999 34682 sgd_solver.cpp:112] Iteration 52900, lr = 0.01
I0523 04:58:28.835492 34682 solver.cpp:239] Iteration 52910 (2.1101 iter/s, 4.73912s/10 iters), loss = 7.87604
I0523 04:58:28.835546 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87604 (* 1 = 7.87604 loss)
I0523 04:58:28.900857 34682 sgd_solver.cpp:112] Iteration 52910, lr = 0.01
I0523 04:58:34.465351 34682 solver.cpp:239] Iteration 52920 (1.77633 iter/s, 5.62958s/10 iters), loss = 8.29269
I0523 04:58:34.465396 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29269 (* 1 = 8.29269 loss)
I0523 04:58:34.538800 34682 sgd_solver.cpp:112] Iteration 52920, lr = 0.01
I0523 04:58:39.067986 34682 solver.cpp:239] Iteration 52930 (2.17278 iter/s, 4.6024s/10 iters), loss = 7.05801
I0523 04:58:39.068033 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.05801 (* 1 = 7.05801 loss)
I0523 04:58:39.936079 34682 sgd_solver.cpp:112] Iteration 52930, lr = 0.01
I0523 04:58:45.585324 34682 solver.cpp:239] Iteration 52940 (1.53444 iter/s, 6.51702s/10 iters), loss = 8.31224
I0523 04:58:45.585371 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31224 (* 1 = 8.31224 loss)
I0523 04:58:46.424253 34682 sgd_solver.cpp:112] Iteration 52940, lr = 0.01
I0523 04:58:51.346122 34682 solver.cpp:239] Iteration 52950 (1.73597 iter/s, 5.76047s/10 iters), loss = 7.56008
I0523 04:58:51.346287 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56008 (* 1 = 7.56008 loss)
I0523 04:58:51.425348 34682 sgd_solver.cpp:112] Iteration 52950, lr = 0.01
I0523 04:58:56.590675 34682 solver.cpp:239] Iteration 52960 (1.90688 iter/s, 5.24418s/10 iters), loss = 8.78727
I0523 04:58:56.590744 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78727 (* 1 = 8.78727 loss)
I0523 04:58:57.245368 34682 sgd_solver.cpp:112] Iteration 52960, lr = 0.01
I0523 04:59:01.240936 34682 solver.cpp:239] Iteration 52970 (2.15054 iter/s, 4.65s/10 iters), loss = 8.68317
I0523 04:59:01.240981 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68317 (* 1 = 8.68317 loss)
I0523 04:59:01.316085 34682 sgd_solver.cpp:112] Iteration 52970, lr = 0.01
I0523 04:59:05.332904 34682 solver.cpp:239] Iteration 52980 (2.44394 iter/s, 4.09175s/10 iters), loss = 7.99168
I0523 04:59:05.332952 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99168 (* 1 = 7.99168 loss)
I0523 04:59:05.405750 34682 sgd_solver.cpp:112] Iteration 52980, lr = 0.01
I0523 04:59:11.302628 34682 solver.cpp:239] Iteration 52990 (1.6752 iter/s, 5.96943s/10 iters), loss = 9.20669
I0523 04:59:11.302687 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20669 (* 1 = 9.20669 loss)
I0523 04:59:12.119567 34682 sgd_solver.cpp:112] Iteration 52990, lr = 0.01
I0523 04:59:16.835599 34682 solver.cpp:239] Iteration 53000 (1.80744 iter/s, 5.53269s/10 iters), loss = 7.90199
I0523 04:59:16.835654 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90199 (* 1 = 7.90199 loss)
I0523 04:59:17.188377 34682 sgd_solver.cpp:112] Iteration 53000, lr = 0.01
I0523 04:59:20.370728 34682 solver.cpp:239] Iteration 53010 (2.82893 iter/s, 3.53491s/10 iters), loss = 7.89892
I0523 04:59:20.370779 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89892 (* 1 = 7.89892 loss)
I0523 04:59:20.429404 34682 sgd_solver.cpp:112] Iteration 53010, lr = 0.01
I0523 04:59:25.011270 34682 solver.cpp:239] Iteration 53020 (2.15504 iter/s, 4.6403s/10 iters), loss = 8.66339
I0523 04:59:25.011541 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66339 (* 1 = 8.66339 loss)
I0523 04:59:25.077044 34682 sgd_solver.cpp:112] Iteration 53020, lr = 0.01
I0523 04:59:27.997793 34682 solver.cpp:239] Iteration 53030 (3.3488 iter/s, 2.98615s/10 iters), loss = 8.62931
I0523 04:59:27.997850 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62931 (* 1 = 8.62931 loss)
I0523 04:59:28.720803 34682 sgd_solver.cpp:112] Iteration 53030, lr = 0.01
I0523 04:59:32.725546 34682 solver.cpp:239] Iteration 53040 (2.11528 iter/s, 4.7275s/10 iters), loss = 8.25495
I0523 04:59:32.725602 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25495 (* 1 = 8.25495 loss)
I0523 04:59:32.788878 34682 sgd_solver.cpp:112] Iteration 53040, lr = 0.01
I0523 04:59:35.264881 34682 solver.cpp:239] Iteration 53050 (3.93828 iter/s, 2.53918s/10 iters), loss = 7.81004
I0523 04:59:35.264925 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81004 (* 1 = 7.81004 loss)
I0523 04:59:36.066234 34682 sgd_solver.cpp:112] Iteration 53050, lr = 0.01
I0523 04:59:39.234735 34682 solver.cpp:239] Iteration 53060 (2.51912 iter/s, 3.96965s/10 iters), loss = 9.53996
I0523 04:59:39.234786 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.53996 (* 1 = 9.53996 loss)
I0523 04:59:39.846673 34682 sgd_solver.cpp:112] Iteration 53060, lr = 0.01
I0523 04:59:44.764075 34682 solver.cpp:239] Iteration 53070 (1.80863 iter/s, 5.52906s/10 iters), loss = 7.86703
I0523 04:59:44.764130 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86703 (* 1 = 7.86703 loss)
I0523 04:59:45.580746 34682 sgd_solver.cpp:112] Iteration 53070, lr = 0.01
I0523 04:59:49.403792 34682 solver.cpp:239] Iteration 53080 (2.15542 iter/s, 4.63947s/10 iters), loss = 7.38627
I0523 04:59:49.403839 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.38627 (* 1 = 7.38627 loss)
I0523 04:59:49.471949 34682 sgd_solver.cpp:112] Iteration 53080, lr = 0.01
I0523 04:59:52.538982 34682 solver.cpp:239] Iteration 53090 (3.18979 iter/s, 3.135s/10 iters), loss = 7.29108
I0523 04:59:52.539043 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.29108 (* 1 = 7.29108 loss)
I0523 04:59:53.307973 34682 sgd_solver.cpp:112] Iteration 53090, lr = 0.01
I0523 04:59:58.266590 34682 solver.cpp:239] Iteration 53100 (1.74602 iter/s, 5.72731s/10 iters), loss = 8.96912
I0523 04:59:58.266724 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96912 (* 1 = 8.96912 loss)
I0523 04:59:58.340329 34682 sgd_solver.cpp:112] Iteration 53100, lr = 0.01
I0523 05:00:01.313355 34682 solver.cpp:239] Iteration 53110 (3.28245 iter/s, 3.04651s/10 iters), loss = 7.88341
I0523 05:00:01.313405 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88341 (* 1 = 7.88341 loss)
I0523 05:00:02.127133 34682 sgd_solver.cpp:112] Iteration 53110, lr = 0.01
I0523 05:00:08.122407 34682 solver.cpp:239] Iteration 53120 (1.4687 iter/s, 6.80872s/10 iters), loss = 7.8754
I0523 05:00:08.122463 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8754 (* 1 = 7.8754 loss)
I0523 05:00:08.196480 34682 sgd_solver.cpp:112] Iteration 53120, lr = 0.01
I0523 05:00:10.869906 34682 solver.cpp:239] Iteration 53130 (3.6399 iter/s, 2.74733s/10 iters), loss = 8.66983
I0523 05:00:10.869949 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66983 (* 1 = 8.66983 loss)
I0523 05:00:10.932744 34682 sgd_solver.cpp:112] Iteration 53130, lr = 0.01
I0523 05:00:16.632885 34682 solver.cpp:239] Iteration 53140 (1.7353 iter/s, 5.76269s/10 iters), loss = 7.93401
I0523 05:00:16.632961 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93401 (* 1 = 7.93401 loss)
I0523 05:00:16.689189 34682 sgd_solver.cpp:112] Iteration 53140, lr = 0.01
I0523 05:00:20.221643 34682 solver.cpp:239] Iteration 53150 (2.78665 iter/s, 3.58854s/10 iters), loss = 8.85758
I0523 05:00:20.221695 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85758 (* 1 = 8.85758 loss)
I0523 05:00:20.289562 34682 sgd_solver.cpp:112] Iteration 53150, lr = 0.01
I0523 05:00:25.045629 34682 solver.cpp:239] Iteration 53160 (2.07308 iter/s, 4.82373s/10 iters), loss = 8.81686
I0523 05:00:25.045670 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81686 (* 1 = 8.81686 loss)
I0523 05:00:25.126973 34682 sgd_solver.cpp:112] Iteration 53160, lr = 0.01
I0523 05:00:30.279650 34682 solver.cpp:239] Iteration 53170 (1.91067 iter/s, 5.23376s/10 iters), loss = 7.69662
I0523 05:00:30.279772 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69662 (* 1 = 7.69662 loss)
I0523 05:00:30.346216 34682 sgd_solver.cpp:112] Iteration 53170, lr = 0.01
I0523 05:00:35.515655 34682 solver.cpp:239] Iteration 53180 (1.90997 iter/s, 5.23568s/10 iters), loss = 8.34608
I0523 05:00:35.515705 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34608 (* 1 = 8.34608 loss)
I0523 05:00:35.682768 34682 sgd_solver.cpp:112] Iteration 53180, lr = 0.01
I0523 05:00:39.539038 34682 solver.cpp:239] Iteration 53190 (2.48561 iter/s, 4.02317s/10 iters), loss = 8.70393
I0523 05:00:39.539084 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70393 (* 1 = 8.70393 loss)
I0523 05:00:39.616322 34682 sgd_solver.cpp:112] Iteration 53190, lr = 0.01
I0523 05:00:44.419773 34682 solver.cpp:239] Iteration 53200 (2.04897 iter/s, 4.88049s/10 iters), loss = 8.20616
I0523 05:00:44.419814 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20616 (* 1 = 8.20616 loss)
I0523 05:00:44.490286 34682 sgd_solver.cpp:112] Iteration 53200, lr = 0.01
I0523 05:00:49.213696 34682 solver.cpp:239] Iteration 53210 (2.08608 iter/s, 4.79368s/10 iters), loss = 8.82297
I0523 05:00:49.213766 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82297 (* 1 = 8.82297 loss)
I0523 05:00:49.732859 34682 sgd_solver.cpp:112] Iteration 53210, lr = 0.01
I0523 05:00:53.728636 34682 solver.cpp:239] Iteration 53220 (2.21499 iter/s, 4.51469s/10 iters), loss = 8.39935
I0523 05:00:53.728685 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39935 (* 1 = 8.39935 loss)
I0523 05:00:54.552333 34682 sgd_solver.cpp:112] Iteration 53220, lr = 0.01
I0523 05:01:00.055779 34682 solver.cpp:239] Iteration 53230 (1.58057 iter/s, 6.32684s/10 iters), loss = 8.19027
I0523 05:01:00.055821 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19027 (* 1 = 8.19027 loss)
I0523 05:01:00.816351 34682 sgd_solver.cpp:112] Iteration 53230, lr = 0.01
I0523 05:01:05.443730 34682 solver.cpp:239] Iteration 53240 (1.85759 iter/s, 5.38333s/10 iters), loss = 8.47165
I0523 05:01:05.443775 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47165 (* 1 = 8.47165 loss)
I0523 05:01:05.506990 34682 sgd_solver.cpp:112] Iteration 53240, lr = 0.01
I0523 05:01:10.550894 34682 solver.cpp:239] Iteration 53250 (1.95813 iter/s, 5.10691s/10 iters), loss = 9.49346
I0523 05:01:10.550937 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.49346 (* 1 = 9.49346 loss)
I0523 05:01:11.410816 34682 sgd_solver.cpp:112] Iteration 53250, lr = 0.01
I0523 05:01:16.497355 34682 solver.cpp:239] Iteration 53260 (1.68175 iter/s, 5.94617s/10 iters), loss = 8.91532
I0523 05:01:16.497400 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91532 (* 1 = 8.91532 loss)
I0523 05:01:16.569885 34682 sgd_solver.cpp:112] Iteration 53260, lr = 0.01
I0523 05:01:20.527051 34682 solver.cpp:239] Iteration 53270 (2.48171 iter/s, 4.02949s/10 iters), loss = 8.21982
I0523 05:01:20.527096 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21982 (* 1 = 8.21982 loss)
I0523 05:01:20.599550 34682 sgd_solver.cpp:112] Iteration 53270, lr = 0.01
I0523 05:01:26.520557 34682 solver.cpp:239] Iteration 53280 (1.66855 iter/s, 5.99322s/10 iters), loss = 9.47734
I0523 05:01:26.520601 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47734 (* 1 = 9.47734 loss)
I0523 05:01:26.585189 34682 sgd_solver.cpp:112] Iteration 53280, lr = 0.01
I0523 05:01:31.729535 34682 solver.cpp:239] Iteration 53290 (1.91986 iter/s, 5.20872s/10 iters), loss = 8.6452
I0523 05:01:31.729722 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6452 (* 1 = 8.6452 loss)
I0523 05:01:31.815146 34682 sgd_solver.cpp:112] Iteration 53290, lr = 0.01
I0523 05:01:37.364610 34682 solver.cpp:239] Iteration 53300 (1.77473 iter/s, 5.63466s/10 iters), loss = 8.28827
I0523 05:01:37.364678 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28827 (* 1 = 8.28827 loss)
I0523 05:01:37.430248 34682 sgd_solver.cpp:112] Iteration 53300, lr = 0.01
I0523 05:01:41.047852 34682 solver.cpp:239] Iteration 53310 (2.71676 iter/s, 3.68086s/10 iters), loss = 8.90468
I0523 05:01:41.047902 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90468 (* 1 = 8.90468 loss)
I0523 05:01:41.129056 34682 sgd_solver.cpp:112] Iteration 53310, lr = 0.01
I0523 05:01:46.322249 34682 solver.cpp:239] Iteration 53320 (1.89605 iter/s, 5.27413s/10 iters), loss = 8.55126
I0523 05:01:46.322296 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55126 (* 1 = 8.55126 loss)
I0523 05:01:46.375234 34682 sgd_solver.cpp:112] Iteration 53320, lr = 0.01
I0523 05:01:52.164726 34682 solver.cpp:239] Iteration 53330 (1.71169 iter/s, 5.84219s/10 iters), loss = 8.6163
I0523 05:01:52.164780 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6163 (* 1 = 8.6163 loss)
I0523 05:01:52.219635 34682 sgd_solver.cpp:112] Iteration 53330, lr = 0.01
I0523 05:01:54.850651 34682 solver.cpp:239] Iteration 53340 (3.72335 iter/s, 2.68575s/10 iters), loss = 8.40024
I0523 05:01:54.850729 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40024 (* 1 = 8.40024 loss)
I0523 05:01:55.693717 34682 sgd_solver.cpp:112] Iteration 53340, lr = 0.01
I0523 05:01:59.324800 34682 solver.cpp:239] Iteration 53350 (2.23519 iter/s, 4.47389s/10 iters), loss = 9.37538
I0523 05:01:59.324841 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.37538 (* 1 = 9.37538 loss)
I0523 05:01:59.389158 34682 sgd_solver.cpp:112] Iteration 53350, lr = 0.01
I0523 05:02:03.401697 34682 solver.cpp:239] Iteration 53360 (2.45297 iter/s, 4.07668s/10 iters), loss = 7.80645
I0523 05:02:03.401978 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80645 (* 1 = 7.80645 loss)
I0523 05:02:04.250862 34682 sgd_solver.cpp:112] Iteration 53360, lr = 0.01
I0523 05:02:09.382339 34682 solver.cpp:239] Iteration 53370 (1.6722 iter/s, 5.98014s/10 iters), loss = 8.63091
I0523 05:02:09.382393 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63091 (* 1 = 8.63091 loss)
I0523 05:02:09.446319 34682 sgd_solver.cpp:112] Iteration 53370, lr = 0.01
I0523 05:02:14.161451 34682 solver.cpp:239] Iteration 53380 (2.09255 iter/s, 4.77886s/10 iters), loss = 8.35801
I0523 05:02:14.161499 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35801 (* 1 = 8.35801 loss)
I0523 05:02:14.358498 34682 sgd_solver.cpp:112] Iteration 53380, lr = 0.01
I0523 05:02:19.653964 34682 solver.cpp:239] Iteration 53390 (1.82075 iter/s, 5.49224s/10 iters), loss = 7.45883
I0523 05:02:19.654011 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.45883 (* 1 = 7.45883 loss)
I0523 05:02:19.719365 34682 sgd_solver.cpp:112] Iteration 53390, lr = 0.01
I0523 05:02:25.333016 34682 solver.cpp:239] Iteration 53400 (1.76094 iter/s, 5.67878s/10 iters), loss = 8.69558
I0523 05:02:25.333065 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69558 (* 1 = 8.69558 loss)
I0523 05:02:25.393903 34682 sgd_solver.cpp:112] Iteration 53400, lr = 0.01
I0523 05:02:30.647948 34682 solver.cpp:239] Iteration 53410 (1.88159 iter/s, 5.31467s/10 iters), loss = 8.54102
I0523 05:02:30.647999 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54102 (* 1 = 8.54102 loss)
I0523 05:02:30.717605 34682 sgd_solver.cpp:112] Iteration 53410, lr = 0.01
I0523 05:02:35.625380 34682 solver.cpp:239] Iteration 53420 (2.00917 iter/s, 4.97718s/10 iters), loss = 7.85364
I0523 05:02:35.625587 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.85364 (* 1 = 7.85364 loss)
I0523 05:02:36.331805 34682 sgd_solver.cpp:112] Iteration 53420, lr = 0.01
I0523 05:02:42.087136 34682 solver.cpp:239] Iteration 53430 (1.54768 iter/s, 6.46129s/10 iters), loss = 8.90073
I0523 05:02:42.087188 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90073 (* 1 = 8.90073 loss)
I0523 05:02:42.154753 34682 sgd_solver.cpp:112] Iteration 53430, lr = 0.01
I0523 05:02:46.671804 34682 solver.cpp:239] Iteration 53440 (2.1813 iter/s, 4.58443s/10 iters), loss = 8.28355
I0523 05:02:46.671854 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28355 (* 1 = 8.28355 loss)
I0523 05:02:47.457715 34682 sgd_solver.cpp:112] Iteration 53440, lr = 0.01
I0523 05:02:51.714412 34682 solver.cpp:239] Iteration 53450 (1.98321 iter/s, 5.04234s/10 iters), loss = 8.67513
I0523 05:02:51.714493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67513 (* 1 = 8.67513 loss)
I0523 05:02:52.493044 34682 sgd_solver.cpp:112] Iteration 53450, lr = 0.01
I0523 05:02:55.797236 34682 solver.cpp:239] Iteration 53460 (2.44943 iter/s, 4.08258s/10 iters), loss = 9.58517
I0523 05:02:55.797288 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.58517 (* 1 = 9.58517 loss)
I0523 05:02:55.854832 34682 sgd_solver.cpp:112] Iteration 53460, lr = 0.01
I0523 05:02:59.512560 34682 solver.cpp:239] Iteration 53470 (2.6917 iter/s, 3.71512s/10 iters), loss = 8.90246
I0523 05:02:59.512609 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90246 (* 1 = 8.90246 loss)
I0523 05:02:59.587313 34682 sgd_solver.cpp:112] Iteration 53470, lr = 0.01
I0523 05:03:03.467945 34682 solver.cpp:239] Iteration 53480 (2.52834 iter/s, 3.95517s/10 iters), loss = 9.11827
I0523 05:03:03.467996 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11827 (* 1 = 9.11827 loss)
I0523 05:03:04.276270 34682 sgd_solver.cpp:112] Iteration 53480, lr = 0.01
I0523 05:03:07.523430 34682 solver.cpp:239] Iteration 53490 (2.46593 iter/s, 4.05526s/10 iters), loss = 8.27705
I0523 05:03:07.523669 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27705 (* 1 = 8.27705 loss)
I0523 05:03:07.602818 34682 sgd_solver.cpp:112] Iteration 53490, lr = 0.01
I0523 05:03:13.673228 34682 solver.cpp:239] Iteration 53500 (1.62734 iter/s, 6.14501s/10 iters), loss = 8.50126
I0523 05:03:13.673295 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50126 (* 1 = 8.50126 loss)
I0523 05:03:14.452628 34682 sgd_solver.cpp:112] Iteration 53500, lr = 0.01
I0523 05:03:18.002504 34682 solver.cpp:239] Iteration 53510 (2.30998 iter/s, 4.32903s/10 iters), loss = 8.47524
I0523 05:03:18.002549 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47524 (* 1 = 8.47524 loss)
I0523 05:03:18.058976 34682 sgd_solver.cpp:112] Iteration 53510, lr = 0.01
I0523 05:03:21.966305 34682 solver.cpp:239] Iteration 53520 (2.52297 iter/s, 3.96359s/10 iters), loss = 8.53222
I0523 05:03:21.966364 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53222 (* 1 = 8.53222 loss)
I0523 05:03:22.027844 34682 sgd_solver.cpp:112] Iteration 53520, lr = 0.01
I0523 05:03:27.177322 34682 solver.cpp:239] Iteration 53530 (1.91912 iter/s, 5.21073s/10 iters), loss = 8.6569
I0523 05:03:27.177371 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6569 (* 1 = 8.6569 loss)
I0523 05:03:27.896889 34682 sgd_solver.cpp:112] Iteration 53530, lr = 0.01
I0523 05:03:34.496239 34682 solver.cpp:239] Iteration 53540 (1.36639 iter/s, 7.31855s/10 iters), loss = 8.50525
I0523 05:03:34.496296 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50525 (* 1 = 8.50525 loss)
I0523 05:03:34.571874 34682 sgd_solver.cpp:112] Iteration 53540, lr = 0.01
I0523 05:03:39.774893 34682 solver.cpp:239] Iteration 53550 (1.89541 iter/s, 5.27589s/10 iters), loss = 8.219
I0523 05:03:39.775108 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.219 (* 1 = 8.219 loss)
I0523 05:03:40.672544 34682 sgd_solver.cpp:112] Iteration 53550, lr = 0.01
I0523 05:03:44.774772 34682 solver.cpp:239] Iteration 53560 (2.00021 iter/s, 4.99948s/10 iters), loss = 9.1572
I0523 05:03:44.774847 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1572 (* 1 = 9.1572 loss)
I0523 05:03:45.584878 34682 sgd_solver.cpp:112] Iteration 53560, lr = 0.01
I0523 05:03:51.176836 34682 solver.cpp:239] Iteration 53570 (1.56207 iter/s, 6.40174s/10 iters), loss = 8.29274
I0523 05:03:51.176877 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29274 (* 1 = 8.29274 loss)
I0523 05:03:51.739915 34682 sgd_solver.cpp:112] Iteration 53570, lr = 0.01
I0523 05:03:56.604161 34682 solver.cpp:239] Iteration 53580 (1.84262 iter/s, 5.42706s/10 iters), loss = 8.44727
I0523 05:03:56.604207 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44727 (* 1 = 8.44727 loss)
I0523 05:03:57.336560 34682 sgd_solver.cpp:112] Iteration 53580, lr = 0.01
I0523 05:04:01.827762 34682 solver.cpp:239] Iteration 53590 (1.91448 iter/s, 5.22335s/10 iters), loss = 8.46318
I0523 05:04:01.827813 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46318 (* 1 = 8.46318 loss)
I0523 05:04:01.903571 34682 sgd_solver.cpp:112] Iteration 53590, lr = 0.01
I0523 05:04:07.161660 34682 solver.cpp:239] Iteration 53600 (1.8749 iter/s, 5.33363s/10 iters), loss = 8.63631
I0523 05:04:07.161700 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63631 (* 1 = 8.63631 loss)
I0523 05:04:07.234139 34682 sgd_solver.cpp:112] Iteration 53600, lr = 0.01
I0523 05:04:10.682263 34682 solver.cpp:239] Iteration 53610 (2.84058 iter/s, 3.52041s/10 iters), loss = 7.42013
I0523 05:04:10.682515 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42013 (* 1 = 7.42013 loss)
I0523 05:04:10.760049 34682 sgd_solver.cpp:112] Iteration 53610, lr = 0.01
I0523 05:04:15.974445 34682 solver.cpp:239] Iteration 53620 (1.88974 iter/s, 5.29173s/10 iters), loss = 8.56614
I0523 05:04:15.974524 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56614 (* 1 = 8.56614 loss)
I0523 05:04:16.803488 34682 sgd_solver.cpp:112] Iteration 53620, lr = 0.01
I0523 05:04:22.321774 34682 solver.cpp:239] Iteration 53630 (1.57555 iter/s, 6.347s/10 iters), loss = 7.82444
I0523 05:04:22.321836 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82444 (* 1 = 7.82444 loss)
I0523 05:04:23.121346 34682 sgd_solver.cpp:112] Iteration 53630, lr = 0.01
I0523 05:04:27.126603 34682 solver.cpp:239] Iteration 53640 (2.08233 iter/s, 4.80232s/10 iters), loss = 8.58902
I0523 05:04:27.126652 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58902 (* 1 = 8.58902 loss)
I0523 05:04:27.183042 34682 sgd_solver.cpp:112] Iteration 53640, lr = 0.01
I0523 05:04:31.267117 34682 solver.cpp:239] Iteration 53650 (2.41747 iter/s, 4.13655s/10 iters), loss = 8.44318
I0523 05:04:31.267180 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44318 (* 1 = 8.44318 loss)
I0523 05:04:31.980756 34682 sgd_solver.cpp:112] Iteration 53650, lr = 0.01
I0523 05:04:34.581058 34682 solver.cpp:239] Iteration 53660 (3.01775 iter/s, 3.31373s/10 iters), loss = 8.20395
I0523 05:04:34.581111 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20395 (* 1 = 8.20395 loss)
I0523 05:04:35.387818 34682 sgd_solver.cpp:112] Iteration 53660, lr = 0.01
I0523 05:04:39.414862 34682 solver.cpp:239] Iteration 53670 (2.06887 iter/s, 4.83355s/10 iters), loss = 8.86162
I0523 05:04:39.414914 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86162 (* 1 = 8.86162 loss)
I0523 05:04:40.236697 34682 sgd_solver.cpp:112] Iteration 53670, lr = 0.01
I0523 05:04:45.536370 34682 solver.cpp:239] Iteration 53680 (1.63366 iter/s, 6.12121s/10 iters), loss = 8.85557
I0523 05:04:45.536609 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85557 (* 1 = 8.85557 loss)
I0523 05:04:45.611114 34682 sgd_solver.cpp:112] Iteration 53680, lr = 0.01
I0523 05:04:50.424721 34682 solver.cpp:239] Iteration 53690 (2.04685 iter/s, 4.88556s/10 iters), loss = 8.2599
I0523 05:04:50.424767 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2599 (* 1 = 8.2599 loss)
I0523 05:04:50.489164 34682 sgd_solver.cpp:112] Iteration 53690, lr = 0.01
I0523 05:04:57.151468 34682 solver.cpp:239] Iteration 53700 (1.48667 iter/s, 6.72643s/10 iters), loss = 9.07017
I0523 05:04:57.151522 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07017 (* 1 = 9.07017 loss)
I0523 05:04:57.806303 34682 sgd_solver.cpp:112] Iteration 53700, lr = 0.01
I0523 05:05:02.814225 34682 solver.cpp:239] Iteration 53710 (1.76601 iter/s, 5.66248s/10 iters), loss = 7.48001
I0523 05:05:02.814278 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.48001 (* 1 = 7.48001 loss)
I0523 05:05:02.884025 34682 sgd_solver.cpp:112] Iteration 53710, lr = 0.01
I0523 05:05:06.334134 34682 solver.cpp:239] Iteration 53720 (2.84114 iter/s, 3.51971s/10 iters), loss = 8.93748
I0523 05:05:06.334178 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93748 (* 1 = 8.93748 loss)
I0523 05:05:06.406774 34682 sgd_solver.cpp:112] Iteration 53720, lr = 0.01
I0523 05:05:12.143182 34682 solver.cpp:239] Iteration 53730 (1.72154 iter/s, 5.80877s/10 iters), loss = 8.61046
I0523 05:05:12.143224 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61046 (* 1 = 8.61046 loss)
I0523 05:05:12.207444 34682 sgd_solver.cpp:112] Iteration 53730, lr = 0.01
I0523 05:05:17.159986 34682 solver.cpp:239] Iteration 53740 (1.9934 iter/s, 5.01656s/10 iters), loss = 8.37912
I0523 05:05:17.160256 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37912 (* 1 = 8.37912 loss)
I0523 05:05:17.230459 34682 sgd_solver.cpp:112] Iteration 53740, lr = 0.01
I0523 05:05:21.508934 34682 solver.cpp:239] Iteration 53750 (2.29963 iter/s, 4.34852s/10 iters), loss = 7.05354
I0523 05:05:21.508982 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.05354 (* 1 = 7.05354 loss)
I0523 05:05:22.251581 34682 sgd_solver.cpp:112] Iteration 53750, lr = 0.01
I0523 05:05:27.876282 34682 solver.cpp:239] Iteration 53760 (1.57059 iter/s, 6.36704s/10 iters), loss = 7.92563
I0523 05:05:27.876327 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92563 (* 1 = 7.92563 loss)
I0523 05:05:27.941537 34682 sgd_solver.cpp:112] Iteration 53760, lr = 0.01
I0523 05:05:31.344312 34682 solver.cpp:239] Iteration 53770 (2.88364 iter/s, 3.46784s/10 iters), loss = 7.72257
I0523 05:05:31.344359 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72257 (* 1 = 7.72257 loss)
I0523 05:05:31.417335 34682 sgd_solver.cpp:112] Iteration 53770, lr = 0.01
I0523 05:05:35.326238 34682 solver.cpp:239] Iteration 53780 (2.51148 iter/s, 3.98171s/10 iters), loss = 8.60478
I0523 05:05:35.326293 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60478 (* 1 = 8.60478 loss)
I0523 05:05:36.042531 34682 sgd_solver.cpp:112] Iteration 53780, lr = 0.01
I0523 05:05:40.254519 34682 solver.cpp:239] Iteration 53790 (2.02921 iter/s, 4.92802s/10 iters), loss = 8.16298
I0523 05:05:40.254588 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16298 (* 1 = 8.16298 loss)
I0523 05:05:41.097525 34682 sgd_solver.cpp:112] Iteration 53790, lr = 0.01
I0523 05:05:45.356350 34682 solver.cpp:239] Iteration 53800 (1.96019 iter/s, 5.10155s/10 iters), loss = 9.0063
I0523 05:05:45.356405 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0063 (* 1 = 9.0063 loss)
I0523 05:05:45.418875 34682 sgd_solver.cpp:112] Iteration 53800, lr = 0.01
I0523 05:05:49.306815 34682 solver.cpp:239] Iteration 53810 (2.53149 iter/s, 3.95025s/10 iters), loss = 8.10793
I0523 05:05:49.306952 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10793 (* 1 = 8.10793 loss)
I0523 05:05:49.387775 34682 sgd_solver.cpp:112] Iteration 53810, lr = 0.01
I0523 05:05:55.047029 34682 solver.cpp:239] Iteration 53820 (1.74221 iter/s, 5.73984s/10 iters), loss = 8.59264
I0523 05:05:55.047075 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59264 (* 1 = 8.59264 loss)
I0523 05:05:55.108762 34682 sgd_solver.cpp:112] Iteration 53820, lr = 0.01
I0523 05:06:00.902458 34682 solver.cpp:239] Iteration 53830 (1.7079 iter/s, 5.85514s/10 iters), loss = 8.54844
I0523 05:06:00.902515 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54844 (* 1 = 8.54844 loss)
I0523 05:06:00.962802 34682 sgd_solver.cpp:112] Iteration 53830, lr = 0.01
I0523 05:06:05.834306 34682 solver.cpp:239] Iteration 53840 (2.02774 iter/s, 4.93159s/10 iters), loss = 8.03351
I0523 05:06:05.834352 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03351 (* 1 = 8.03351 loss)
I0523 05:06:05.891777 34682 sgd_solver.cpp:112] Iteration 53840, lr = 0.01
I0523 05:06:12.030014 34682 solver.cpp:239] Iteration 53850 (1.6141 iter/s, 6.1954s/10 iters), loss = 8.73318
I0523 05:06:12.030061 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73318 (* 1 = 8.73318 loss)
I0523 05:06:12.098147 34682 sgd_solver.cpp:112] Iteration 53850, lr = 0.01
I0523 05:06:19.040328 34682 solver.cpp:239] Iteration 53860 (1.42654 iter/s, 7.00998s/10 iters), loss = 7.74106
I0523 05:06:19.040375 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74106 (* 1 = 7.74106 loss)
I0523 05:06:19.116909 34682 sgd_solver.cpp:112] Iteration 53860, lr = 0.01
I0523 05:06:23.965586 34682 solver.cpp:239] Iteration 53870 (2.03045 iter/s, 4.92501s/10 iters), loss = 8.70344
I0523 05:06:23.965754 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70344 (* 1 = 8.70344 loss)
I0523 05:06:24.024616 34682 sgd_solver.cpp:112] Iteration 53870, lr = 0.01
I0523 05:06:27.086163 34682 solver.cpp:239] Iteration 53880 (3.20484 iter/s, 3.12028s/10 iters), loss = 7.56777
I0523 05:06:27.086206 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56777 (* 1 = 7.56777 loss)
I0523 05:06:27.157383 34682 sgd_solver.cpp:112] Iteration 53880, lr = 0.01
I0523 05:06:30.374805 34682 solver.cpp:239] Iteration 53890 (3.04094 iter/s, 3.28846s/10 iters), loss = 8.7011
I0523 05:06:30.374850 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7011 (* 1 = 8.7011 loss)
I0523 05:06:30.454100 34682 sgd_solver.cpp:112] Iteration 53890, lr = 0.01
I0523 05:06:35.644034 34682 solver.cpp:239] Iteration 53900 (1.89791 iter/s, 5.26897s/10 iters), loss = 8.92526
I0523 05:06:35.644081 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92526 (* 1 = 8.92526 loss)
I0523 05:06:35.707943 34682 sgd_solver.cpp:112] Iteration 53900, lr = 0.01
I0523 05:06:40.548723 34682 solver.cpp:239] Iteration 53910 (2.03897 iter/s, 4.90444s/10 iters), loss = 9.07021
I0523 05:06:40.548774 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07021 (* 1 = 9.07021 loss)
I0523 05:06:40.611145 34682 sgd_solver.cpp:112] Iteration 53910, lr = 0.01
I0523 05:06:45.364970 34682 solver.cpp:239] Iteration 53920 (2.07641 iter/s, 4.816s/10 iters), loss = 8.21479
I0523 05:06:45.365015 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21479 (* 1 = 8.21479 loss)
I0523 05:06:45.438026 34682 sgd_solver.cpp:112] Iteration 53920, lr = 0.01
I0523 05:06:48.961787 34682 solver.cpp:239] Iteration 53930 (2.78039 iter/s, 3.59661s/10 iters), loss = 8.81835
I0523 05:06:48.961836 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81835 (* 1 = 8.81835 loss)
I0523 05:06:49.813256 34682 sgd_solver.cpp:112] Iteration 53930, lr = 0.01
I0523 05:06:54.057948 34682 solver.cpp:239] Iteration 53940 (1.96236 iter/s, 5.09591s/10 iters), loss = 9.18943
I0523 05:06:54.058080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18943 (* 1 = 9.18943 loss)
I0523 05:06:54.881042 34682 sgd_solver.cpp:112] Iteration 53940, lr = 0.01
I0523 05:06:59.773946 34682 solver.cpp:239] Iteration 53950 (1.74959 iter/s, 5.71562s/10 iters), loss = 7.64312
I0523 05:06:59.774008 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64312 (* 1 = 7.64312 loss)
I0523 05:07:00.562925 34682 sgd_solver.cpp:112] Iteration 53950, lr = 0.01
I0523 05:07:04.359375 34682 solver.cpp:239] Iteration 53960 (2.18094 iter/s, 4.58519s/10 iters), loss = 8.64621
I0523 05:07:04.359417 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64621 (* 1 = 8.64621 loss)
I0523 05:07:04.420758 34682 sgd_solver.cpp:112] Iteration 53960, lr = 0.01
I0523 05:07:09.933791 34682 solver.cpp:239] Iteration 53970 (1.794 iter/s, 5.57414s/10 iters), loss = 8.15818
I0523 05:07:09.933837 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15818 (* 1 = 8.15818 loss)
I0523 05:07:09.997066 34682 sgd_solver.cpp:112] Iteration 53970, lr = 0.01
I0523 05:07:14.244415 34682 solver.cpp:239] Iteration 53980 (2.31997 iter/s, 4.3104s/10 iters), loss = 8.15034
I0523 05:07:14.244462 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15034 (* 1 = 8.15034 loss)
I0523 05:07:15.130875 34682 sgd_solver.cpp:112] Iteration 53980, lr = 0.01
I0523 05:07:19.098139 34682 solver.cpp:239] Iteration 53990 (2.06039 iter/s, 4.85346s/10 iters), loss = 7.96822
I0523 05:07:19.098206 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96822 (* 1 = 7.96822 loss)
I0523 05:07:19.928079 34682 sgd_solver.cpp:112] Iteration 53990, lr = 0.01
I0523 05:07:25.970975 34682 solver.cpp:239] Iteration 54000 (1.45508 iter/s, 6.8725s/10 iters), loss = 9.31068
I0523 05:07:25.971168 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31068 (* 1 = 9.31068 loss)
I0523 05:07:26.051265 34682 sgd_solver.cpp:112] Iteration 54000, lr = 0.01
I0523 05:07:30.501670 34682 solver.cpp:239] Iteration 54010 (2.20735 iter/s, 4.53031s/10 iters), loss = 8.1035
I0523 05:07:30.501727 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1035 (* 1 = 8.1035 loss)
I0523 05:07:31.342646 34682 sgd_solver.cpp:112] Iteration 54010, lr = 0.01
I0523 05:07:37.058542 34682 solver.cpp:239] Iteration 54020 (1.52519 iter/s, 6.55656s/10 iters), loss = 8.24924
I0523 05:07:37.058595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24924 (* 1 = 8.24924 loss)
I0523 05:07:37.891916 34682 sgd_solver.cpp:112] Iteration 54020, lr = 0.01
I0523 05:07:41.956784 34682 solver.cpp:239] Iteration 54030 (2.04165 iter/s, 4.89799s/10 iters), loss = 8.27522
I0523 05:07:41.956833 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27522 (* 1 = 8.27522 loss)
I0523 05:07:42.708916 34682 sgd_solver.cpp:112] Iteration 54030, lr = 0.01
I0523 05:07:46.752511 34682 solver.cpp:239] Iteration 54040 (2.0853 iter/s, 4.79548s/10 iters), loss = 7.74967
I0523 05:07:46.752564 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74967 (* 1 = 7.74967 loss)
I0523 05:07:46.816633 34682 sgd_solver.cpp:112] Iteration 54040, lr = 0.01
I0523 05:07:54.727079 34682 solver.cpp:239] Iteration 54050 (1.25405 iter/s, 7.97417s/10 iters), loss = 8.37466
I0523 05:07:54.727149 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37466 (* 1 = 8.37466 loss)
I0523 05:07:55.518026 34682 sgd_solver.cpp:112] Iteration 54050, lr = 0.01
I0523 05:08:02.151145 34682 solver.cpp:239] Iteration 54060 (1.34704 iter/s, 7.4237s/10 iters), loss = 9.25976
I0523 05:08:02.151279 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25976 (* 1 = 9.25976 loss)
I0523 05:08:02.233839 34682 sgd_solver.cpp:112] Iteration 54060, lr = 0.01
I0523 05:08:04.600832 34682 solver.cpp:239] Iteration 54070 (4.08256 iter/s, 2.44945s/10 iters), loss = 8.92333
I0523 05:08:04.600891 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92333 (* 1 = 8.92333 loss)
I0523 05:08:04.668342 34682 sgd_solver.cpp:112] Iteration 54070, lr = 0.01
I0523 05:08:09.456879 34682 solver.cpp:239] Iteration 54080 (2.0594 iter/s, 4.85579s/10 iters), loss = 8.32442
I0523 05:08:09.456930 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32442 (* 1 = 8.32442 loss)
I0523 05:08:09.519907 34682 sgd_solver.cpp:112] Iteration 54080, lr = 0.01
I0523 05:08:12.519192 34682 solver.cpp:239] Iteration 54090 (3.2657 iter/s, 3.06213s/10 iters), loss = 9.00582
I0523 05:08:12.519235 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00582 (* 1 = 9.00582 loss)
I0523 05:08:12.587157 34682 sgd_solver.cpp:112] Iteration 54090, lr = 0.01
I0523 05:08:16.617341 34682 solver.cpp:239] Iteration 54100 (2.44026 iter/s, 4.09793s/10 iters), loss = 7.68663
I0523 05:08:16.617396 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68663 (* 1 = 7.68663 loss)
I0523 05:08:16.690052 34682 sgd_solver.cpp:112] Iteration 54100, lr = 0.01
I0523 05:08:22.239472 34682 solver.cpp:239] Iteration 54110 (1.77877 iter/s, 5.62185s/10 iters), loss = 8.40189
I0523 05:08:22.239526 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40189 (* 1 = 8.40189 loss)
I0523 05:08:22.953346 34682 sgd_solver.cpp:112] Iteration 54110, lr = 0.01
I0523 05:08:27.439471 34682 solver.cpp:239] Iteration 54120 (1.92317 iter/s, 5.19974s/10 iters), loss = 8.50108
I0523 05:08:27.439517 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50108 (* 1 = 8.50108 loss)
I0523 05:08:27.497901 34682 sgd_solver.cpp:112] Iteration 54120, lr = 0.01
I0523 05:08:30.974076 34682 solver.cpp:239] Iteration 54130 (2.82933 iter/s, 3.53441s/10 iters), loss = 8.32647
I0523 05:08:30.974118 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32647 (* 1 = 8.32647 loss)
I0523 05:08:31.819108 34682 sgd_solver.cpp:112] Iteration 54130, lr = 0.01
I0523 05:08:36.065922 34682 solver.cpp:239] Iteration 54140 (1.96402 iter/s, 5.0916s/10 iters), loss = 8.10423
I0523 05:08:36.066193 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10423 (* 1 = 8.10423 loss)
I0523 05:08:36.136216 34682 sgd_solver.cpp:112] Iteration 54140, lr = 0.01
I0523 05:08:39.425022 34682 solver.cpp:239] Iteration 54150 (2.97733 iter/s, 3.35871s/10 iters), loss = 8.85038
I0523 05:08:39.425061 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85038 (* 1 = 8.85038 loss)
I0523 05:08:39.511860 34682 sgd_solver.cpp:112] Iteration 54150, lr = 0.01
I0523 05:08:45.078675 34682 solver.cpp:239] Iteration 54160 (1.76886 iter/s, 5.65337s/10 iters), loss = 9.69702
I0523 05:08:45.078757 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.69702 (* 1 = 9.69702 loss)
I0523 05:08:45.137183 34682 sgd_solver.cpp:112] Iteration 54160, lr = 0.01
I0523 05:08:50.969173 34682 solver.cpp:239] Iteration 54170 (1.69774 iter/s, 5.89018s/10 iters), loss = 8.93943
I0523 05:08:50.969228 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93943 (* 1 = 8.93943 loss)
I0523 05:08:51.695279 34682 sgd_solver.cpp:112] Iteration 54170, lr = 0.01
I0523 05:08:55.399475 34682 solver.cpp:239] Iteration 54180 (2.25731 iter/s, 4.43005s/10 iters), loss = 8.17871
I0523 05:08:55.399533 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17871 (* 1 = 8.17871 loss)
I0523 05:08:56.223291 34682 sgd_solver.cpp:112] Iteration 54180, lr = 0.01
I0523 05:09:01.817617 34682 solver.cpp:239] Iteration 54190 (1.55816 iter/s, 6.41783s/10 iters), loss = 8.60889
I0523 05:09:01.817667 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60889 (* 1 = 8.60889 loss)
I0523 05:09:01.894286 34682 sgd_solver.cpp:112] Iteration 54190, lr = 0.01
I0523 05:09:04.444428 34682 solver.cpp:239] Iteration 54200 (3.80713 iter/s, 2.62665s/10 iters), loss = 8.55824
I0523 05:09:04.444481 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55824 (* 1 = 8.55824 loss)
I0523 05:09:04.516770 34682 sgd_solver.cpp:112] Iteration 54200, lr = 0.01
I0523 05:09:11.161201 34682 solver.cpp:239] Iteration 54210 (1.48888 iter/s, 6.71645s/10 iters), loss = 8.64038
I0523 05:09:11.161330 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64038 (* 1 = 8.64038 loss)
I0523 05:09:12.027770 34682 sgd_solver.cpp:112] Iteration 54210, lr = 0.01
I0523 05:09:16.366456 34682 solver.cpp:239] Iteration 54220 (1.92126 iter/s, 5.20492s/10 iters), loss = 7.94078
I0523 05:09:16.366508 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94078 (* 1 = 7.94078 loss)
I0523 05:09:17.142066 34682 sgd_solver.cpp:112] Iteration 54220, lr = 0.01
I0523 05:09:18.964864 34682 solver.cpp:239] Iteration 54230 (3.84875 iter/s, 2.59825s/10 iters), loss = 8.38174
I0523 05:09:18.964907 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38174 (* 1 = 8.38174 loss)
I0523 05:09:19.230772 34682 sgd_solver.cpp:112] Iteration 54230, lr = 0.01
I0523 05:09:23.842914 34682 solver.cpp:239] Iteration 54240 (2.0501 iter/s, 4.87781s/10 iters), loss = 8.69028
I0523 05:09:23.842957 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69028 (* 1 = 8.69028 loss)
I0523 05:09:23.909132 34682 sgd_solver.cpp:112] Iteration 54240, lr = 0.01
I0523 05:09:28.124022 34682 solver.cpp:239] Iteration 54250 (2.33597 iter/s, 4.28088s/10 iters), loss = 8.4699
I0523 05:09:28.124073 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4699 (* 1 = 8.4699 loss)
I0523 05:09:28.186395 34682 sgd_solver.cpp:112] Iteration 54250, lr = 0.01
I0523 05:09:33.903975 34682 solver.cpp:239] Iteration 54260 (1.73021 iter/s, 5.77966s/10 iters), loss = 8.84027
I0523 05:09:33.904026 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84027 (* 1 = 8.84027 loss)
I0523 05:09:34.617578 34682 sgd_solver.cpp:112] Iteration 54260, lr = 0.01
I0523 05:09:38.861557 34682 solver.cpp:239] Iteration 54270 (2.01722 iter/s, 4.95731s/10 iters), loss = 8.00655
I0523 05:09:38.861608 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00655 (* 1 = 8.00655 loss)
I0523 05:09:39.609763 34682 sgd_solver.cpp:112] Iteration 54270, lr = 0.01
I0523 05:09:44.517904 34682 solver.cpp:239] Iteration 54280 (1.76801 iter/s, 5.65606s/10 iters), loss = 8.68921
I0523 05:09:44.518090 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68921 (* 1 = 8.68921 loss)
I0523 05:09:45.291378 34682 sgd_solver.cpp:112] Iteration 54280, lr = 0.01
I0523 05:09:49.857954 34682 solver.cpp:239] Iteration 54290 (1.87278 iter/s, 5.33965s/10 iters), loss = 9.14402
I0523 05:09:49.858009 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14402 (* 1 = 9.14402 loss)
I0523 05:09:49.936756 34682 sgd_solver.cpp:112] Iteration 54290, lr = 0.01
I0523 05:09:53.197070 34682 solver.cpp:239] Iteration 54300 (2.99498 iter/s, 3.33892s/10 iters), loss = 7.79895
I0523 05:09:53.197122 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79895 (* 1 = 7.79895 loss)
I0523 05:09:53.253033 34682 sgd_solver.cpp:112] Iteration 54300, lr = 0.01
I0523 05:09:59.113937 34682 solver.cpp:239] Iteration 54310 (1.69018 iter/s, 5.91652s/10 iters), loss = 7.72904
I0523 05:09:59.114049 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72904 (* 1 = 7.72904 loss)
I0523 05:09:59.184187 34682 sgd_solver.cpp:112] Iteration 54310, lr = 0.01
I0523 05:10:03.190855 34682 solver.cpp:239] Iteration 54320 (2.453 iter/s, 4.07665s/10 iters), loss = 9.03104
I0523 05:10:03.190902 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03104 (* 1 = 9.03104 loss)
I0523 05:10:03.263880 34682 sgd_solver.cpp:112] Iteration 54320, lr = 0.01
I0523 05:10:07.904289 34682 solver.cpp:239] Iteration 54330 (2.1217 iter/s, 4.71319s/10 iters), loss = 8.87042
I0523 05:10:07.904345 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87042 (* 1 = 8.87042 loss)
I0523 05:10:07.964248 34682 sgd_solver.cpp:112] Iteration 54330, lr = 0.01
I0523 05:10:12.580528 34682 solver.cpp:239] Iteration 54340 (2.13858 iter/s, 4.67599s/10 iters), loss = 9.33088
I0523 05:10:12.580574 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33088 (* 1 = 9.33088 loss)
I0523 05:10:12.641299 34682 sgd_solver.cpp:112] Iteration 54340, lr = 0.01
I0523 05:10:17.243230 34682 solver.cpp:239] Iteration 54350 (2.14479 iter/s, 4.66245s/10 iters), loss = 9.07001
I0523 05:10:17.243453 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07001 (* 1 = 9.07001 loss)
I0523 05:10:17.306850 34682 sgd_solver.cpp:112] Iteration 54350, lr = 0.01
I0523 05:10:20.522332 34682 solver.cpp:239] Iteration 54360 (3.04993 iter/s, 3.27877s/10 iters), loss = 8.32012
I0523 05:10:20.522377 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32012 (* 1 = 8.32012 loss)
I0523 05:10:20.604779 34682 sgd_solver.cpp:112] Iteration 54360, lr = 0.01
I0523 05:10:25.488240 34682 solver.cpp:239] Iteration 54370 (2.01383 iter/s, 4.96566s/10 iters), loss = 8.25283
I0523 05:10:25.488291 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25283 (* 1 = 8.25283 loss)
I0523 05:10:25.552042 34682 sgd_solver.cpp:112] Iteration 54370, lr = 0.01
I0523 05:10:29.034397 34682 solver.cpp:239] Iteration 54380 (2.82012 iter/s, 3.54595s/10 iters), loss = 8.31441
I0523 05:10:29.034457 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31441 (* 1 = 8.31441 loss)
I0523 05:10:29.826186 34682 sgd_solver.cpp:112] Iteration 54380, lr = 0.01
I0523 05:10:35.479490 34682 solver.cpp:239] Iteration 54390 (1.55165 iter/s, 6.44477s/10 iters), loss = 8.3722
I0523 05:10:35.479545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3722 (* 1 = 8.3722 loss)
I0523 05:10:35.553532 34682 sgd_solver.cpp:112] Iteration 54390, lr = 0.01
I0523 05:10:38.955566 34682 solver.cpp:239] Iteration 54400 (2.87697 iter/s, 3.47588s/10 iters), loss = 8.18627
I0523 05:10:38.955607 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18627 (* 1 = 8.18627 loss)
I0523 05:10:39.409525 34682 sgd_solver.cpp:112] Iteration 54400, lr = 0.01
I0523 05:10:45.068331 34682 solver.cpp:239] Iteration 54410 (1.636 iter/s, 6.11247s/10 iters), loss = 8.21542
I0523 05:10:45.068380 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21542 (* 1 = 8.21542 loss)
I0523 05:10:45.141288 34682 sgd_solver.cpp:112] Iteration 54410, lr = 0.01
I0523 05:10:49.087370 34682 solver.cpp:239] Iteration 54420 (2.4883 iter/s, 4.01881s/10 iters), loss = 8.62853
I0523 05:10:49.087625 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62853 (* 1 = 8.62853 loss)
I0523 05:10:49.169575 34682 sgd_solver.cpp:112] Iteration 54420, lr = 0.01
I0523 05:10:54.127020 34682 solver.cpp:239] Iteration 54430 (1.98444 iter/s, 5.03921s/10 iters), loss = 8.66003
I0523 05:10:54.127071 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66003 (* 1 = 8.66003 loss)
I0523 05:10:54.414841 34682 sgd_solver.cpp:112] Iteration 54430, lr = 0.01
I0523 05:10:58.559569 34682 solver.cpp:239] Iteration 54440 (2.25616 iter/s, 4.43231s/10 iters), loss = 8.04385
I0523 05:10:58.559633 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04385 (* 1 = 8.04385 loss)
I0523 05:10:58.627326 34682 sgd_solver.cpp:112] Iteration 54440, lr = 0.01
I0523 05:11:03.915515 34682 solver.cpp:239] Iteration 54450 (1.86718 iter/s, 5.35566s/10 iters), loss = 8.57853
I0523 05:11:03.915580 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57853 (* 1 = 8.57853 loss)
I0523 05:11:03.984153 34682 sgd_solver.cpp:112] Iteration 54450, lr = 0.01
I0523 05:11:09.748023 34682 solver.cpp:239] Iteration 54460 (1.71462 iter/s, 5.8322s/10 iters), loss = 8.69173
I0523 05:11:09.748090 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69173 (* 1 = 8.69173 loss)
I0523 05:11:10.596603 34682 sgd_solver.cpp:112] Iteration 54460, lr = 0.01
I0523 05:11:15.332217 34682 solver.cpp:239] Iteration 54470 (1.79086 iter/s, 5.58391s/10 iters), loss = 8.33625
I0523 05:11:15.332260 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33625 (* 1 = 8.33625 loss)
I0523 05:11:16.158524 34682 sgd_solver.cpp:112] Iteration 54470, lr = 0.01
I0523 05:11:20.938021 34682 solver.cpp:239] Iteration 54480 (1.78395 iter/s, 5.60553s/10 iters), loss = 8.52713
I0523 05:11:20.938179 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52713 (* 1 = 8.52713 loss)
I0523 05:11:21.380681 34682 sgd_solver.cpp:112] Iteration 54480, lr = 0.01
I0523 05:11:25.449472 34682 solver.cpp:239] Iteration 54490 (2.21675 iter/s, 4.51112s/10 iters), loss = 8.7592
I0523 05:11:25.449520 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7592 (* 1 = 8.7592 loss)
I0523 05:11:25.533790 34682 sgd_solver.cpp:112] Iteration 54490, lr = 0.01
I0523 05:11:30.052004 34682 solver.cpp:239] Iteration 54500 (2.17283 iter/s, 4.6023s/10 iters), loss = 8.23219
I0523 05:11:30.052045 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23219 (* 1 = 8.23219 loss)
I0523 05:11:30.126056 34682 sgd_solver.cpp:112] Iteration 54500, lr = 0.01
I0523 05:11:34.391412 34682 solver.cpp:239] Iteration 54510 (2.30458 iter/s, 4.33918s/10 iters), loss = 7.48597
I0523 05:11:34.391465 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.48597 (* 1 = 7.48597 loss)
I0523 05:11:34.445595 34682 sgd_solver.cpp:112] Iteration 54510, lr = 0.01
I0523 05:11:38.483793 34682 solver.cpp:239] Iteration 54520 (2.44371 iter/s, 4.09215s/10 iters), loss = 8.37008
I0523 05:11:38.483857 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37008 (* 1 = 8.37008 loss)
I0523 05:11:39.219436 34682 sgd_solver.cpp:112] Iteration 54520, lr = 0.01
I0523 05:11:43.396878 34682 solver.cpp:239] Iteration 54530 (2.03549 iter/s, 4.91282s/10 iters), loss = 8.16114
I0523 05:11:43.396924 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16114 (* 1 = 8.16114 loss)
I0523 05:11:43.462685 34682 sgd_solver.cpp:112] Iteration 54530, lr = 0.01
I0523 05:11:46.734565 34682 solver.cpp:239] Iteration 54540 (2.99626 iter/s, 3.3375s/10 iters), loss = 7.8622
I0523 05:11:46.734616 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8622 (* 1 = 7.8622 loss)
I0523 05:11:47.521984 34682 sgd_solver.cpp:112] Iteration 54540, lr = 0.01
I0523 05:11:53.111747 34682 solver.cpp:239] Iteration 54550 (1.56817 iter/s, 6.37687s/10 iters), loss = 8.14681
I0523 05:11:53.111973 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14681 (* 1 = 8.14681 loss)
I0523 05:11:53.178704 34682 sgd_solver.cpp:112] Iteration 54550, lr = 0.01
I0523 05:11:58.682548 34682 solver.cpp:239] Iteration 54560 (1.79521 iter/s, 5.57038s/10 iters), loss = 8.29355
I0523 05:11:58.682593 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29355 (* 1 = 8.29355 loss)
I0523 05:11:59.536624 34682 sgd_solver.cpp:112] Iteration 54560, lr = 0.01
I0523 05:12:02.841061 34682 solver.cpp:239] Iteration 54570 (2.40483 iter/s, 4.1583s/10 iters), loss = 9.18432
I0523 05:12:02.841106 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18432 (* 1 = 9.18432 loss)
I0523 05:12:03.695767 34682 sgd_solver.cpp:112] Iteration 54570, lr = 0.01
I0523 05:12:08.059062 34682 solver.cpp:239] Iteration 54580 (1.91654 iter/s, 5.21774s/10 iters), loss = 8.17536
I0523 05:12:08.059113 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17536 (* 1 = 8.17536 loss)
I0523 05:12:08.125313 34682 sgd_solver.cpp:112] Iteration 54580, lr = 0.01
I0523 05:12:12.385175 34682 solver.cpp:239] Iteration 54590 (2.31167 iter/s, 4.32588s/10 iters), loss = 9.1943
I0523 05:12:12.385223 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1943 (* 1 = 9.1943 loss)
I0523 05:12:13.010993 34682 sgd_solver.cpp:112] Iteration 54590, lr = 0.01
I0523 05:12:16.484103 34682 solver.cpp:239] Iteration 54600 (2.4398 iter/s, 4.0987s/10 iters), loss = 8.97759
I0523 05:12:16.484149 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97759 (* 1 = 8.97759 loss)
I0523 05:12:16.543246 34682 sgd_solver.cpp:112] Iteration 54600, lr = 0.01
I0523 05:12:20.392840 34682 solver.cpp:239] Iteration 54610 (2.55851 iter/s, 3.90852s/10 iters), loss = 8.77468
I0523 05:12:20.392891 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77468 (* 1 = 8.77468 loss)
I0523 05:12:20.456604 34682 sgd_solver.cpp:112] Iteration 54610, lr = 0.01
I0523 05:12:25.021366 34682 solver.cpp:239] Iteration 54620 (2.16063 iter/s, 4.62828s/10 iters), loss = 9.60064
I0523 05:12:25.021522 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.60064 (* 1 = 9.60064 loss)
I0523 05:12:25.096380 34682 sgd_solver.cpp:112] Iteration 54620, lr = 0.01
I0523 05:12:30.677242 34682 solver.cpp:239] Iteration 54630 (1.76819 iter/s, 5.6555s/10 iters), loss = 9.06046
I0523 05:12:30.677287 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06046 (* 1 = 9.06046 loss)
I0523 05:12:30.735425 34682 sgd_solver.cpp:112] Iteration 54630, lr = 0.01
I0523 05:12:35.521019 34682 solver.cpp:239] Iteration 54640 (2.06461 iter/s, 4.84353s/10 iters), loss = 8.84704
I0523 05:12:35.521071 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84704 (* 1 = 8.84704 loss)
I0523 05:12:36.344645 34682 sgd_solver.cpp:112] Iteration 54640, lr = 0.01
I0523 05:12:41.527293 34682 solver.cpp:239] Iteration 54650 (1.66501 iter/s, 6.00598s/10 iters), loss = 10.0465
I0523 05:12:41.527343 34682 solver.cpp:258]     Train net output #0: softmax_loss = 10.0465 (* 1 = 10.0465 loss)
I0523 05:12:41.616917 34682 sgd_solver.cpp:112] Iteration 54650, lr = 0.01
I0523 05:12:48.373948 34682 solver.cpp:239] Iteration 54660 (1.46064 iter/s, 6.84633s/10 iters), loss = 8.10931
I0523 05:12:48.373994 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10931 (* 1 = 8.10931 loss)
I0523 05:12:48.443051 34682 sgd_solver.cpp:112] Iteration 54660, lr = 0.01
I0523 05:12:53.344601 34682 solver.cpp:239] Iteration 54670 (2.01191 iter/s, 4.9704s/10 iters), loss = 8.71507
I0523 05:12:53.344657 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71507 (* 1 = 8.71507 loss)
I0523 05:12:53.864854 34682 sgd_solver.cpp:112] Iteration 54670, lr = 0.01
I0523 05:12:59.812516 34682 solver.cpp:239] Iteration 54680 (1.54617 iter/s, 6.4676s/10 iters), loss = 8.71711
I0523 05:12:59.812779 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71711 (* 1 = 8.71711 loss)
I0523 05:13:00.562825 34682 sgd_solver.cpp:112] Iteration 54680, lr = 0.01
I0523 05:13:04.604140 34682 solver.cpp:239] Iteration 54690 (2.08717 iter/s, 4.79119s/10 iters), loss = 8.51567
I0523 05:13:04.604182 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51567 (* 1 = 8.51567 loss)
I0523 05:13:04.665799 34682 sgd_solver.cpp:112] Iteration 54690, lr = 0.01
I0523 05:13:08.823766 34682 solver.cpp:239] Iteration 54700 (2.37001 iter/s, 4.2194s/10 iters), loss = 8.43596
I0523 05:13:08.823829 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43596 (* 1 = 8.43596 loss)
I0523 05:13:09.266763 34682 sgd_solver.cpp:112] Iteration 54700, lr = 0.01
I0523 05:13:13.779204 34682 solver.cpp:239] Iteration 54710 (2.01809 iter/s, 4.95517s/10 iters), loss = 9.81872
I0523 05:13:13.779243 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.81872 (* 1 = 9.81872 loss)
I0523 05:13:13.852671 34682 sgd_solver.cpp:112] Iteration 54710, lr = 0.01
I0523 05:13:18.746201 34682 solver.cpp:239] Iteration 54720 (2.01339 iter/s, 4.96675s/10 iters), loss = 8.73691
I0523 05:13:18.746256 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73691 (* 1 = 8.73691 loss)
I0523 05:13:19.588704 34682 sgd_solver.cpp:112] Iteration 54720, lr = 0.01
I0523 05:13:22.310336 34682 solver.cpp:239] Iteration 54730 (2.80589 iter/s, 3.56393s/10 iters), loss = 8.26694
I0523 05:13:22.310384 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26694 (* 1 = 8.26694 loss)
I0523 05:13:22.380901 34682 sgd_solver.cpp:112] Iteration 54730, lr = 0.01
I0523 05:13:26.389827 34682 solver.cpp:239] Iteration 54740 (2.45142 iter/s, 4.07927s/10 iters), loss = 9.28209
I0523 05:13:26.389873 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28209 (* 1 = 9.28209 loss)
I0523 05:13:27.063542 34682 sgd_solver.cpp:112] Iteration 54740, lr = 0.01
I0523 05:13:32.558290 34682 solver.cpp:239] Iteration 54750 (1.62123 iter/s, 6.16816s/10 iters), loss = 8.85496
I0523 05:13:32.558493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85496 (* 1 = 8.85496 loss)
I0523 05:13:32.633792 34682 sgd_solver.cpp:112] Iteration 54750, lr = 0.01
I0523 05:13:36.196521 34682 solver.cpp:239] Iteration 54760 (2.75062 iter/s, 3.63555s/10 iters), loss = 8.41033
I0523 05:13:36.196591 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41033 (* 1 = 8.41033 loss)
I0523 05:13:36.496428 34682 sgd_solver.cpp:112] Iteration 54760, lr = 0.01
I0523 05:13:40.664544 34682 solver.cpp:239] Iteration 54770 (2.23825 iter/s, 4.46777s/10 iters), loss = 8.22767
I0523 05:13:40.664590 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22767 (* 1 = 8.22767 loss)
I0523 05:13:40.738106 34682 sgd_solver.cpp:112] Iteration 54770, lr = 0.01
I0523 05:13:44.519062 34682 solver.cpp:239] Iteration 54780 (2.5945 iter/s, 3.85431s/10 iters), loss = 8.79125
I0523 05:13:44.519107 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79125 (* 1 = 8.79125 loss)
I0523 05:13:44.582592 34682 sgd_solver.cpp:112] Iteration 54780, lr = 0.01
I0523 05:13:48.739248 34682 solver.cpp:239] Iteration 54790 (2.36969 iter/s, 4.21997s/10 iters), loss = 8.74093
I0523 05:13:48.739289 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74093 (* 1 = 8.74093 loss)
I0523 05:13:48.816063 34682 sgd_solver.cpp:112] Iteration 54790, lr = 0.01
I0523 05:13:52.583317 34682 solver.cpp:239] Iteration 54800 (2.60155 iter/s, 3.84387s/10 iters), loss = 8.54023
I0523 05:13:52.583364 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54023 (* 1 = 8.54023 loss)
I0523 05:13:53.160082 34682 sgd_solver.cpp:112] Iteration 54800, lr = 0.01
I0523 05:13:56.918413 34682 solver.cpp:239] Iteration 54810 (2.30688 iter/s, 4.33486s/10 iters), loss = 8.13926
I0523 05:13:56.918474 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13926 (* 1 = 8.13926 loss)
I0523 05:13:57.751713 34682 sgd_solver.cpp:112] Iteration 54810, lr = 0.01
I0523 05:14:02.459040 34682 solver.cpp:239] Iteration 54820 (1.80494 iter/s, 5.54034s/10 iters), loss = 8.12236
I0523 05:14:02.459084 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12236 (* 1 = 8.12236 loss)
I0523 05:14:02.524682 34682 sgd_solver.cpp:112] Iteration 54820, lr = 0.01
I0523 05:14:06.810114 34682 solver.cpp:239] Iteration 54830 (2.29841 iter/s, 4.35083s/10 iters), loss = 7.94285
I0523 05:14:06.810289 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94285 (* 1 = 7.94285 loss)
I0523 05:14:07.610348 34682 sgd_solver.cpp:112] Iteration 54830, lr = 0.01
I0523 05:14:11.904347 34682 solver.cpp:239] Iteration 54840 (1.96315 iter/s, 5.09386s/10 iters), loss = 8.67323
I0523 05:14:11.904393 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67323 (* 1 = 8.67323 loss)
I0523 05:14:12.493300 34682 sgd_solver.cpp:112] Iteration 54840, lr = 0.01
I0523 05:14:17.646172 34682 solver.cpp:239] Iteration 54850 (1.74169 iter/s, 5.74155s/10 iters), loss = 7.89972
I0523 05:14:17.646219 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89972 (* 1 = 7.89972 loss)
I0523 05:14:18.514037 34682 sgd_solver.cpp:112] Iteration 54850, lr = 0.01
I0523 05:14:24.217201 34682 solver.cpp:239] Iteration 54860 (1.5219 iter/s, 6.57072s/10 iters), loss = 8.32821
I0523 05:14:24.217253 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32821 (* 1 = 8.32821 loss)
I0523 05:14:24.291579 34682 sgd_solver.cpp:112] Iteration 54860, lr = 0.01
I0523 05:14:28.436347 34682 solver.cpp:239] Iteration 54870 (2.37027 iter/s, 4.21892s/10 iters), loss = 8.27931
I0523 05:14:28.436395 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27931 (* 1 = 8.27931 loss)
I0523 05:14:29.293820 34682 sgd_solver.cpp:112] Iteration 54870, lr = 0.01
I0523 05:14:32.383853 34682 solver.cpp:239] Iteration 54880 (2.5334 iter/s, 3.94726s/10 iters), loss = 9.08966
I0523 05:14:32.383908 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08966 (* 1 = 9.08966 loss)
I0523 05:14:33.206120 34682 sgd_solver.cpp:112] Iteration 54880, lr = 0.01
I0523 05:14:39.241400 34682 solver.cpp:239] Iteration 54890 (1.45832 iter/s, 6.85721s/10 iters), loss = 7.4583
I0523 05:14:39.241647 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4583 (* 1 = 7.4583 loss)
I0523 05:14:40.088451 34682 sgd_solver.cpp:112] Iteration 54890, lr = 0.01
I0523 05:14:44.880318 34682 solver.cpp:239] Iteration 54900 (1.77353 iter/s, 5.63846s/10 iters), loss = 8.5862
I0523 05:14:44.880373 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5862 (* 1 = 8.5862 loss)
I0523 05:14:45.666673 34682 sgd_solver.cpp:112] Iteration 54900, lr = 0.01
I0523 05:14:52.403997 34682 solver.cpp:239] Iteration 54910 (1.3292 iter/s, 7.52332s/10 iters), loss = 8.74215
I0523 05:14:52.404047 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74215 (* 1 = 8.74215 loss)
I0523 05:14:53.247100 34682 sgd_solver.cpp:112] Iteration 54910, lr = 0.01
I0523 05:14:57.526775 34682 solver.cpp:239] Iteration 54920 (1.95217 iter/s, 5.12252s/10 iters), loss = 9.09886
I0523 05:14:57.526825 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09886 (* 1 = 9.09886 loss)
I0523 05:14:58.284752 34682 sgd_solver.cpp:112] Iteration 54920, lr = 0.01
I0523 05:15:03.182018 34682 solver.cpp:239] Iteration 54930 (1.76836 iter/s, 5.65496s/10 iters), loss = 8.6338
I0523 05:15:03.182065 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6338 (* 1 = 8.6338 loss)
I0523 05:15:03.484330 34682 sgd_solver.cpp:112] Iteration 54930, lr = 0.01
I0523 05:15:07.454473 34682 solver.cpp:239] Iteration 54940 (2.3407 iter/s, 4.27223s/10 iters), loss = 8.46129
I0523 05:15:07.454514 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46129 (* 1 = 8.46129 loss)
I0523 05:15:07.529140 34682 sgd_solver.cpp:112] Iteration 54940, lr = 0.01
I0523 05:15:11.604713 34682 solver.cpp:239] Iteration 54950 (2.40962 iter/s, 4.15003s/10 iters), loss = 8.72021
I0523 05:15:11.604884 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72021 (* 1 = 8.72021 loss)
I0523 05:15:11.681497 34682 sgd_solver.cpp:112] Iteration 54950, lr = 0.01
I0523 05:15:14.178513 34682 solver.cpp:239] Iteration 54960 (3.8857 iter/s, 2.57354s/10 iters), loss = 7.92059
I0523 05:15:14.178567 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92059 (* 1 = 7.92059 loss)
I0523 05:15:14.848954 34682 sgd_solver.cpp:112] Iteration 54960, lr = 0.01
I0523 05:15:20.565212 34682 solver.cpp:239] Iteration 54970 (1.56583 iter/s, 6.38638s/10 iters), loss = 7.41791
I0523 05:15:20.565271 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.41791 (* 1 = 7.41791 loss)
I0523 05:15:21.389390 34682 sgd_solver.cpp:112] Iteration 54970, lr = 0.01
I0523 05:15:26.844655 34682 solver.cpp:239] Iteration 54980 (1.59258 iter/s, 6.27914s/10 iters), loss = 8.21818
I0523 05:15:26.844697 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21818 (* 1 = 8.21818 loss)
I0523 05:15:27.037744 34682 sgd_solver.cpp:112] Iteration 54980, lr = 0.01
I0523 05:15:29.824668 34682 solver.cpp:239] Iteration 54990 (3.35588 iter/s, 2.97985s/10 iters), loss = 7.61525
I0523 05:15:29.824708 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.61525 (* 1 = 7.61525 loss)
I0523 05:15:29.912456 34682 sgd_solver.cpp:112] Iteration 54990, lr = 0.01
I0523 05:15:34.679762 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_55000.caffemodel
I0523 05:15:38.457177 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_55000.solverstate
I0523 05:15:38.758777 34682 solver.cpp:239] Iteration 55000 (1.11936 iter/s, 8.93371s/10 iters), loss = 8.88712
I0523 05:15:38.758831 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88712 (* 1 = 8.88712 loss)
I0523 05:15:38.911875 34682 sgd_solver.cpp:112] Iteration 55000, lr = 0.01
I0523 05:15:44.401262 34682 solver.cpp:239] Iteration 55010 (1.77236 iter/s, 5.6422s/10 iters), loss = 7.80247
I0523 05:15:44.401484 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80247 (* 1 = 7.80247 loss)
I0523 05:15:44.472151 34682 sgd_solver.cpp:112] Iteration 55010, lr = 0.01
I0523 05:15:48.531867 34682 solver.cpp:239] Iteration 55020 (2.42117 iter/s, 4.13024s/10 iters), loss = 7.91022
I0523 05:15:48.531916 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91022 (* 1 = 7.91022 loss)
I0523 05:15:49.134537 34682 sgd_solver.cpp:112] Iteration 55020, lr = 0.01
I0523 05:15:53.154371 34682 solver.cpp:239] Iteration 55030 (2.16344 iter/s, 4.62226s/10 iters), loss = 8.37095
I0523 05:15:53.154426 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37095 (* 1 = 8.37095 loss)
I0523 05:15:53.967542 34682 sgd_solver.cpp:112] Iteration 55030, lr = 0.01
I0523 05:15:57.944994 34682 solver.cpp:239] Iteration 55040 (2.08752 iter/s, 4.79036s/10 iters), loss = 8.48944
I0523 05:15:57.945073 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48944 (* 1 = 8.48944 loss)
I0523 05:15:58.024679 34682 sgd_solver.cpp:112] Iteration 55040, lr = 0.01
I0523 05:16:02.590786 34682 solver.cpp:239] Iteration 55050 (2.15261 iter/s, 4.64551s/10 iters), loss = 8.801
I0523 05:16:02.590854 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.801 (* 1 = 8.801 loss)
I0523 05:16:03.228612 34682 sgd_solver.cpp:112] Iteration 55050, lr = 0.01
I0523 05:16:08.498426 34682 solver.cpp:239] Iteration 55060 (1.69281 iter/s, 5.90733s/10 iters), loss = 7.77143
I0523 05:16:08.498477 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77143 (* 1 = 7.77143 loss)
I0523 05:16:09.292932 34682 sgd_solver.cpp:112] Iteration 55060, lr = 0.01
I0523 05:16:14.219337 34682 solver.cpp:239] Iteration 55070 (1.74806 iter/s, 5.72061s/10 iters), loss = 7.62919
I0523 05:16:14.219391 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62919 (* 1 = 7.62919 loss)
I0523 05:16:14.296203 34682 sgd_solver.cpp:112] Iteration 55070, lr = 0.01
I0523 05:16:18.900600 34682 solver.cpp:239] Iteration 55080 (2.13629 iter/s, 4.68102s/10 iters), loss = 8.94823
I0523 05:16:18.900759 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94823 (* 1 = 8.94823 loss)
I0523 05:16:18.964215 34682 sgd_solver.cpp:112] Iteration 55080, lr = 0.01
I0523 05:16:23.932394 34682 solver.cpp:239] Iteration 55090 (1.9875 iter/s, 5.03145s/10 iters), loss = 7.84282
I0523 05:16:23.932432 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84282 (* 1 = 7.84282 loss)
I0523 05:16:24.013995 34682 sgd_solver.cpp:112] Iteration 55090, lr = 0.01
I0523 05:16:29.019729 34682 solver.cpp:239] Iteration 55100 (1.96576 iter/s, 5.08708s/10 iters), loss = 9.22623
I0523 05:16:29.019788 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22623 (* 1 = 9.22623 loss)
I0523 05:16:29.807755 34682 sgd_solver.cpp:112] Iteration 55100, lr = 0.01
I0523 05:16:33.877082 34682 solver.cpp:239] Iteration 55110 (2.05885 iter/s, 4.85709s/10 iters), loss = 8.39711
I0523 05:16:33.877132 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39711 (* 1 = 8.39711 loss)
I0523 05:16:33.945350 34682 sgd_solver.cpp:112] Iteration 55110, lr = 0.01
I0523 05:16:38.684129 34682 solver.cpp:239] Iteration 55120 (2.08038 iter/s, 4.8068s/10 iters), loss = 9.20939
I0523 05:16:38.684181 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.20939 (* 1 = 9.20939 loss)
I0523 05:16:38.765427 34682 sgd_solver.cpp:112] Iteration 55120, lr = 0.01
I0523 05:16:43.588991 34682 solver.cpp:239] Iteration 55130 (2.0389 iter/s, 4.90461s/10 iters), loss = 8.4297
I0523 05:16:43.589052 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4297 (* 1 = 8.4297 loss)
I0523 05:16:43.744094 34682 sgd_solver.cpp:112] Iteration 55130, lr = 0.01
I0523 05:16:48.400707 34682 solver.cpp:239] Iteration 55140 (2.07837 iter/s, 4.81147s/10 iters), loss = 9.11099
I0523 05:16:48.400749 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11099 (* 1 = 9.11099 loss)
I0523 05:16:48.463088 34682 sgd_solver.cpp:112] Iteration 55140, lr = 0.01
I0523 05:16:54.281563 34682 solver.cpp:239] Iteration 55150 (1.70051 iter/s, 5.88058s/10 iters), loss = 8.70006
I0523 05:16:54.281800 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70006 (* 1 = 8.70006 loss)
I0523 05:16:54.346254 34682 sgd_solver.cpp:112] Iteration 55150, lr = 0.01
I0523 05:16:58.999290 34682 solver.cpp:239] Iteration 55160 (2.11985 iter/s, 4.71732s/10 iters), loss = 8.92575
I0523 05:16:58.999337 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92575 (* 1 = 8.92575 loss)
I0523 05:16:59.683255 34682 sgd_solver.cpp:112] Iteration 55160, lr = 0.01
I0523 05:17:04.491322 34682 solver.cpp:239] Iteration 55170 (1.82091 iter/s, 5.49176s/10 iters), loss = 8.88239
I0523 05:17:04.491379 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88239 (* 1 = 8.88239 loss)
I0523 05:17:05.367132 34682 sgd_solver.cpp:112] Iteration 55170, lr = 0.01
I0523 05:17:10.350353 34682 solver.cpp:239] Iteration 55180 (1.70686 iter/s, 5.85872s/10 iters), loss = 7.12842
I0523 05:17:10.350396 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.12842 (* 1 = 7.12842 loss)
I0523 05:17:10.436237 34682 sgd_solver.cpp:112] Iteration 55180, lr = 0.01
I0523 05:17:15.066582 34682 solver.cpp:239] Iteration 55190 (2.12044 iter/s, 4.71599s/10 iters), loss = 8.61714
I0523 05:17:15.066625 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61714 (* 1 = 8.61714 loss)
I0523 05:17:15.140095 34682 sgd_solver.cpp:112] Iteration 55190, lr = 0.01
I0523 05:17:19.131912 34682 solver.cpp:239] Iteration 55200 (2.45995 iter/s, 4.06512s/10 iters), loss = 8.19092
I0523 05:17:19.131956 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19092 (* 1 = 8.19092 loss)
I0523 05:17:19.945323 34682 sgd_solver.cpp:112] Iteration 55200, lr = 0.01
I0523 05:17:26.515630 34682 solver.cpp:239] Iteration 55210 (1.3544 iter/s, 7.38336s/10 iters), loss = 8.94279
I0523 05:17:26.515920 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94279 (* 1 = 8.94279 loss)
I0523 05:17:27.312261 34682 sgd_solver.cpp:112] Iteration 55210, lr = 0.01
I0523 05:17:31.257149 34682 solver.cpp:239] Iteration 55220 (2.10923 iter/s, 4.74106s/10 iters), loss = 9.3725
I0523 05:17:31.257195 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3725 (* 1 = 9.3725 loss)
I0523 05:17:32.065719 34682 sgd_solver.cpp:112] Iteration 55220, lr = 0.01
I0523 05:17:36.827711 34682 solver.cpp:239] Iteration 55230 (1.79524 iter/s, 5.57029s/10 iters), loss = 8.19732
I0523 05:17:36.827756 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19732 (* 1 = 8.19732 loss)
I0523 05:17:37.527621 34682 sgd_solver.cpp:112] Iteration 55230, lr = 0.01
I0523 05:17:41.710167 34682 solver.cpp:239] Iteration 55240 (2.04825 iter/s, 4.88222s/10 iters), loss = 9.65001
I0523 05:17:41.710214 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.65001 (* 1 = 9.65001 loss)
I0523 05:17:41.777922 34682 sgd_solver.cpp:112] Iteration 55240, lr = 0.01
I0523 05:17:45.021456 34682 solver.cpp:239] Iteration 55250 (3.02014 iter/s, 3.31111s/10 iters), loss = 8.27444
I0523 05:17:45.021502 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27444 (* 1 = 8.27444 loss)
I0523 05:17:45.096386 34682 sgd_solver.cpp:112] Iteration 55250, lr = 0.01
I0523 05:17:47.945435 34682 solver.cpp:239] Iteration 55260 (3.4202 iter/s, 2.92381s/10 iters), loss = 8.72835
I0523 05:17:47.945480 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72835 (* 1 = 8.72835 loss)
I0523 05:17:48.721544 34682 sgd_solver.cpp:112] Iteration 55260, lr = 0.01
I0523 05:17:52.836892 34682 solver.cpp:239] Iteration 55270 (2.04448 iter/s, 4.89121s/10 iters), loss = 7.82958
I0523 05:17:52.836936 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82958 (* 1 = 7.82958 loss)
I0523 05:17:53.707525 34682 sgd_solver.cpp:112] Iteration 55270, lr = 0.01
I0523 05:17:57.879001 34682 solver.cpp:239] Iteration 55280 (1.9834 iter/s, 5.04185s/10 iters), loss = 8.22367
I0523 05:17:57.879271 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22367 (* 1 = 8.22367 loss)
I0523 05:17:58.059106 34682 sgd_solver.cpp:112] Iteration 55280, lr = 0.01
I0523 05:18:04.100329 34682 solver.cpp:239] Iteration 55290 (1.6075 iter/s, 6.22082s/10 iters), loss = 7.83277
I0523 05:18:04.100380 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83277 (* 1 = 7.83277 loss)
I0523 05:18:04.171001 34682 sgd_solver.cpp:112] Iteration 55290, lr = 0.01
I0523 05:18:09.547796 34682 solver.cpp:239] Iteration 55300 (1.83581 iter/s, 5.44718s/10 iters), loss = 7.91521
I0523 05:18:09.547855 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91521 (* 1 = 7.91521 loss)
I0523 05:18:09.619191 34682 sgd_solver.cpp:112] Iteration 55300, lr = 0.01
I0523 05:18:14.654906 34682 solver.cpp:239] Iteration 55310 (1.95816 iter/s, 5.10685s/10 iters), loss = 7.67224
I0523 05:18:14.654953 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.67224 (* 1 = 7.67224 loss)
I0523 05:18:15.372519 34682 sgd_solver.cpp:112] Iteration 55310, lr = 0.01
I0523 05:18:19.625216 34682 solver.cpp:239] Iteration 55320 (2.01205 iter/s, 4.97006s/10 iters), loss = 8.54767
I0523 05:18:19.625270 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54767 (* 1 = 8.54767 loss)
I0523 05:18:19.688789 34682 sgd_solver.cpp:112] Iteration 55320, lr = 0.01
I0523 05:18:24.519210 34682 solver.cpp:239] Iteration 55330 (2.04343 iter/s, 4.89374s/10 iters), loss = 8.7937
I0523 05:18:24.519261 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7937 (* 1 = 8.7937 loss)
I0523 05:18:24.595630 34682 sgd_solver.cpp:112] Iteration 55330, lr = 0.01
I0523 05:18:29.223002 34682 solver.cpp:239] Iteration 55340 (2.12605 iter/s, 4.70355s/10 iters), loss = 8.35689
I0523 05:18:29.223311 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35689 (* 1 = 8.35689 loss)
I0523 05:18:29.290241 34682 sgd_solver.cpp:112] Iteration 55340, lr = 0.01
I0523 05:18:34.236212 34682 solver.cpp:239] Iteration 55350 (1.99578 iter/s, 5.01057s/10 iters), loss = 8.08761
I0523 05:18:34.236259 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08761 (* 1 = 8.08761 loss)
I0523 05:18:34.295888 34682 sgd_solver.cpp:112] Iteration 55350, lr = 0.01
I0523 05:18:37.725211 34682 solver.cpp:239] Iteration 55360 (2.86631 iter/s, 3.4888s/10 iters), loss = 8.51507
I0523 05:18:37.725255 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51507 (* 1 = 8.51507 loss)
I0523 05:18:37.787005 34682 sgd_solver.cpp:112] Iteration 55360, lr = 0.01
I0523 05:18:42.572958 34682 solver.cpp:239] Iteration 55370 (2.06292 iter/s, 4.84751s/10 iters), loss = 8.65531
I0523 05:18:42.573000 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65531 (* 1 = 8.65531 loss)
I0523 05:18:42.646471 34682 sgd_solver.cpp:112] Iteration 55370, lr = 0.01
I0523 05:18:47.242781 34682 solver.cpp:239] Iteration 55380 (2.14152 iter/s, 4.66959s/10 iters), loss = 8.27805
I0523 05:18:47.242823 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27805 (* 1 = 8.27805 loss)
I0523 05:18:47.306967 34682 sgd_solver.cpp:112] Iteration 55380, lr = 0.01
I0523 05:18:50.612310 34682 solver.cpp:239] Iteration 55390 (2.96794 iter/s, 3.36934s/10 iters), loss = 9.07629
I0523 05:18:50.612359 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07629 (* 1 = 9.07629 loss)
I0523 05:18:50.673941 34682 sgd_solver.cpp:112] Iteration 55390, lr = 0.01
I0523 05:18:54.579723 34682 solver.cpp:239] Iteration 55400 (2.52068 iter/s, 3.96719s/10 iters), loss = 9.01199
I0523 05:18:54.579795 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01199 (* 1 = 9.01199 loss)
I0523 05:18:55.311496 34682 sgd_solver.cpp:112] Iteration 55400, lr = 0.01
I0523 05:19:00.979398 34682 solver.cpp:239] Iteration 55410 (1.56266 iter/s, 6.39934s/10 iters), loss = 8.45041
I0523 05:19:00.979620 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45041 (* 1 = 8.45041 loss)
I0523 05:19:01.046622 34682 sgd_solver.cpp:112] Iteration 55410, lr = 0.01
I0523 05:19:06.435997 34682 solver.cpp:239] Iteration 55420 (1.83278 iter/s, 5.45618s/10 iters), loss = 8.27582
I0523 05:19:06.436048 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27582 (* 1 = 8.27582 loss)
I0523 05:19:07.027488 34682 sgd_solver.cpp:112] Iteration 55420, lr = 0.01
I0523 05:19:12.311791 34682 solver.cpp:239] Iteration 55430 (1.70199 iter/s, 5.87548s/10 iters), loss = 8.17921
I0523 05:19:12.311833 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17921 (* 1 = 8.17921 loss)
I0523 05:19:12.962177 34682 sgd_solver.cpp:112] Iteration 55430, lr = 0.01
I0523 05:19:17.589107 34682 solver.cpp:239] Iteration 55440 (1.895 iter/s, 5.27706s/10 iters), loss = 8.13952
I0523 05:19:17.589167 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13952 (* 1 = 8.13952 loss)
I0523 05:19:17.651036 34682 sgd_solver.cpp:112] Iteration 55440, lr = 0.01
I0523 05:19:21.620918 34682 solver.cpp:239] Iteration 55450 (2.48041 iter/s, 4.03159s/10 iters), loss = 8.4468
I0523 05:19:21.620967 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4468 (* 1 = 8.4468 loss)
I0523 05:19:21.696756 34682 sgd_solver.cpp:112] Iteration 55450, lr = 0.01
I0523 05:19:25.738700 34682 solver.cpp:239] Iteration 55460 (2.42863 iter/s, 4.11754s/10 iters), loss = 8.60573
I0523 05:19:25.738756 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60573 (* 1 = 8.60573 loss)
I0523 05:19:26.600119 34682 sgd_solver.cpp:112] Iteration 55460, lr = 0.01
I0523 05:19:29.262701 34682 solver.cpp:239] Iteration 55470 (2.83785 iter/s, 3.52379s/10 iters), loss = 9.13526
I0523 05:19:29.262753 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13526 (* 1 = 9.13526 loss)
I0523 05:19:30.071277 34682 sgd_solver.cpp:112] Iteration 55470, lr = 0.01
I0523 05:19:33.286173 34682 solver.cpp:239] Iteration 55480 (2.48555 iter/s, 4.02325s/10 iters), loss = 8.11646
I0523 05:19:33.286360 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11646 (* 1 = 8.11646 loss)
I0523 05:19:34.164002 34682 sgd_solver.cpp:112] Iteration 55480, lr = 0.01
I0523 05:19:38.713906 34682 solver.cpp:239] Iteration 55490 (1.84252 iter/s, 5.42735s/10 iters), loss = 8.3844
I0523 05:19:38.713954 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3844 (* 1 = 8.3844 loss)
I0523 05:19:38.776497 34682 sgd_solver.cpp:112] Iteration 55490, lr = 0.01
I0523 05:19:43.385219 34682 solver.cpp:239] Iteration 55500 (2.14083 iter/s, 4.67108s/10 iters), loss = 7.59932
I0523 05:19:43.385260 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.59932 (* 1 = 7.59932 loss)
I0523 05:19:43.452381 34682 sgd_solver.cpp:112] Iteration 55500, lr = 0.01
I0523 05:19:49.884634 34682 solver.cpp:239] Iteration 55510 (1.53868 iter/s, 6.4991s/10 iters), loss = 8.26157
I0523 05:19:49.884696 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26157 (* 1 = 8.26157 loss)
I0523 05:19:50.636526 34682 sgd_solver.cpp:112] Iteration 55510, lr = 0.01
I0523 05:19:55.216948 34682 solver.cpp:239] Iteration 55520 (1.87546 iter/s, 5.33204s/10 iters), loss = 8.00882
I0523 05:19:55.216992 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00882 (* 1 = 8.00882 loss)
I0523 05:19:56.086501 34682 sgd_solver.cpp:112] Iteration 55520, lr = 0.01
I0523 05:20:00.772614 34682 solver.cpp:239] Iteration 55530 (1.80005 iter/s, 5.5554s/10 iters), loss = 8.58418
I0523 05:20:00.772660 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58418 (* 1 = 8.58418 loss)
I0523 05:20:00.839092 34682 sgd_solver.cpp:112] Iteration 55530, lr = 0.01
I0523 05:20:05.549126 34682 solver.cpp:239] Iteration 55540 (2.09368 iter/s, 4.77627s/10 iters), loss = 8.20221
I0523 05:20:05.549345 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20221 (* 1 = 8.20221 loss)
I0523 05:20:06.395943 34682 sgd_solver.cpp:112] Iteration 55540, lr = 0.01
I0523 05:20:10.639569 34682 solver.cpp:239] Iteration 55550 (1.96462 iter/s, 5.09003s/10 iters), loss = 8.65712
I0523 05:20:10.639633 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65712 (* 1 = 8.65712 loss)
I0523 05:20:11.446270 34682 sgd_solver.cpp:112] Iteration 55550, lr = 0.01
I0523 05:20:15.503589 34682 solver.cpp:239] Iteration 55560 (2.05602 iter/s, 4.86376s/10 iters), loss = 8.5683
I0523 05:20:15.503648 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5683 (* 1 = 8.5683 loss)
I0523 05:20:15.580512 34682 sgd_solver.cpp:112] Iteration 55560, lr = 0.01
I0523 05:20:19.069191 34682 solver.cpp:239] Iteration 55570 (2.80474 iter/s, 3.5654s/10 iters), loss = 8.04032
I0523 05:20:19.069247 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04032 (* 1 = 8.04032 loss)
I0523 05:20:19.144040 34682 sgd_solver.cpp:112] Iteration 55570, lr = 0.01
I0523 05:20:23.493716 34682 solver.cpp:239] Iteration 55580 (2.26025 iter/s, 4.42429s/10 iters), loss = 9.1145
I0523 05:20:23.493767 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1145 (* 1 = 9.1145 loss)
I0523 05:20:23.564750 34682 sgd_solver.cpp:112] Iteration 55580, lr = 0.01
I0523 05:20:26.292865 34682 solver.cpp:239] Iteration 55590 (3.57276 iter/s, 2.79896s/10 iters), loss = 8.78332
I0523 05:20:26.292922 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78332 (* 1 = 8.78332 loss)
I0523 05:20:27.121989 34682 sgd_solver.cpp:112] Iteration 55590, lr = 0.01
I0523 05:20:31.864502 34682 solver.cpp:239] Iteration 55600 (1.79489 iter/s, 5.57136s/10 iters), loss = 8.0695
I0523 05:20:31.864547 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0695 (* 1 = 8.0695 loss)
I0523 05:20:31.934263 34682 sgd_solver.cpp:112] Iteration 55600, lr = 0.01
I0523 05:20:37.542898 34682 solver.cpp:239] Iteration 55610 (1.76115 iter/s, 5.67811s/10 iters), loss = 8.24657
I0523 05:20:37.543108 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24657 (* 1 = 8.24657 loss)
I0523 05:20:38.426465 34682 sgd_solver.cpp:112] Iteration 55610, lr = 0.01
I0523 05:20:44.343014 34682 solver.cpp:239] Iteration 55620 (1.47066 iter/s, 6.79965s/10 iters), loss = 7.58655
I0523 05:20:44.343055 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58655 (* 1 = 7.58655 loss)
I0523 05:20:44.410688 34682 sgd_solver.cpp:112] Iteration 55620, lr = 0.01
I0523 05:20:48.976405 34682 solver.cpp:239] Iteration 55630 (2.15836 iter/s, 4.63315s/10 iters), loss = 8.69724
I0523 05:20:48.976469 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69724 (* 1 = 8.69724 loss)
I0523 05:20:49.047730 34682 sgd_solver.cpp:112] Iteration 55630, lr = 0.01
I0523 05:20:52.494444 34682 solver.cpp:239] Iteration 55640 (2.84266 iter/s, 3.51784s/10 iters), loss = 8.09285
I0523 05:20:52.494488 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09285 (* 1 = 8.09285 loss)
I0523 05:20:52.570945 34682 sgd_solver.cpp:112] Iteration 55640, lr = 0.01
I0523 05:20:56.705178 34682 solver.cpp:239] Iteration 55650 (2.375 iter/s, 4.21052s/10 iters), loss = 8.76395
I0523 05:20:56.705216 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76395 (* 1 = 8.76395 loss)
I0523 05:20:56.782698 34682 sgd_solver.cpp:112] Iteration 55650, lr = 0.01
I0523 05:20:59.669924 34682 solver.cpp:239] Iteration 55660 (3.37317 iter/s, 2.96457s/10 iters), loss = 8.33861
I0523 05:20:59.669973 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33861 (* 1 = 8.33861 loss)
I0523 05:20:59.749274 34682 sgd_solver.cpp:112] Iteration 55660, lr = 0.01
I0523 05:21:03.577826 34682 solver.cpp:239] Iteration 55670 (2.55906 iter/s, 3.90769s/10 iters), loss = 8.57846
I0523 05:21:03.577880 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57846 (* 1 = 8.57846 loss)
I0523 05:21:03.654680 34682 sgd_solver.cpp:112] Iteration 55670, lr = 0.01
I0523 05:21:10.466116 34682 solver.cpp:239] Iteration 55680 (1.45181 iter/s, 6.88796s/10 iters), loss = 8.30882
I0523 05:21:10.466339 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30882 (* 1 = 8.30882 loss)
I0523 05:21:10.533923 34682 sgd_solver.cpp:112] Iteration 55680, lr = 0.01
I0523 05:21:15.370525 34682 solver.cpp:239] Iteration 55690 (2.03915 iter/s, 4.90401s/10 iters), loss = 7.4117
I0523 05:21:15.370569 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4117 (* 1 = 7.4117 loss)
I0523 05:21:15.435565 34682 sgd_solver.cpp:112] Iteration 55690, lr = 0.01
I0523 05:21:20.526082 34682 solver.cpp:239] Iteration 55700 (1.93975 iter/s, 5.1553s/10 iters), loss = 7.6908
I0523 05:21:20.526126 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6908 (* 1 = 7.6908 loss)
I0523 05:21:20.593659 34682 sgd_solver.cpp:112] Iteration 55700, lr = 0.01
I0523 05:21:24.350006 34682 solver.cpp:239] Iteration 55710 (2.61526 iter/s, 3.82372s/10 iters), loss = 7.71937
I0523 05:21:24.350060 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.71937 (* 1 = 7.71937 loss)
I0523 05:21:24.409219 34682 sgd_solver.cpp:112] Iteration 55710, lr = 0.01
I0523 05:21:30.213708 34682 solver.cpp:239] Iteration 55720 (1.70549 iter/s, 5.86341s/10 iters), loss = 7.79748
I0523 05:21:30.213757 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79748 (* 1 = 7.79748 loss)
I0523 05:21:31.028059 34682 sgd_solver.cpp:112] Iteration 55720, lr = 0.01
I0523 05:21:34.417865 34682 solver.cpp:239] Iteration 55730 (2.37873 iter/s, 4.20393s/10 iters), loss = 7.50836
I0523 05:21:34.417929 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50836 (* 1 = 7.50836 loss)
I0523 05:21:35.274961 34682 sgd_solver.cpp:112] Iteration 55730, lr = 0.01
I0523 05:21:38.529958 34682 solver.cpp:239] Iteration 55740 (2.43199 iter/s, 4.11186s/10 iters), loss = 8.47308
I0523 05:21:38.530002 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47308 (* 1 = 8.47308 loss)
I0523 05:21:38.597764 34682 sgd_solver.cpp:112] Iteration 55740, lr = 0.01
I0523 05:21:42.781461 34682 solver.cpp:239] Iteration 55750 (2.35223 iter/s, 4.25128s/10 iters), loss = 7.94456
I0523 05:21:42.781708 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94456 (* 1 = 7.94456 loss)
I0523 05:21:42.838070 34682 sgd_solver.cpp:112] Iteration 55750, lr = 0.01
I0523 05:21:47.345506 34682 solver.cpp:239] Iteration 55760 (2.19123 iter/s, 4.56364s/10 iters), loss = 7.60722
I0523 05:21:47.345554 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.60722 (* 1 = 7.60722 loss)
I0523 05:21:47.409672 34682 sgd_solver.cpp:112] Iteration 55760, lr = 0.01
I0523 05:21:51.618999 34682 solver.cpp:239] Iteration 55770 (2.34013 iter/s, 4.27327s/10 iters), loss = 8.17943
I0523 05:21:51.619040 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17943 (* 1 = 8.17943 loss)
I0523 05:21:51.689095 34682 sgd_solver.cpp:112] Iteration 55770, lr = 0.01
I0523 05:21:56.539947 34682 solver.cpp:239] Iteration 55780 (2.03223 iter/s, 4.92069s/10 iters), loss = 7.45342
I0523 05:21:56.539989 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.45342 (* 1 = 7.45342 loss)
I0523 05:21:56.614724 34682 sgd_solver.cpp:112] Iteration 55780, lr = 0.01
I0523 05:22:01.685226 34682 solver.cpp:239] Iteration 55790 (1.94362 iter/s, 5.14503s/10 iters), loss = 8.53498
I0523 05:22:01.685267 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53498 (* 1 = 8.53498 loss)
I0523 05:22:01.748816 34682 sgd_solver.cpp:112] Iteration 55790, lr = 0.01
I0523 05:22:06.940335 34682 solver.cpp:239] Iteration 55800 (1.903 iter/s, 5.25485s/10 iters), loss = 7.7077
I0523 05:22:06.940379 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7077 (* 1 = 7.7077 loss)
I0523 05:22:07.009745 34682 sgd_solver.cpp:112] Iteration 55800, lr = 0.01
I0523 05:22:10.271833 34682 solver.cpp:239] Iteration 55810 (3.00182 iter/s, 3.33131s/10 iters), loss = 8.42977
I0523 05:22:10.271878 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42977 (* 1 = 8.42977 loss)
I0523 05:22:10.336901 34682 sgd_solver.cpp:112] Iteration 55810, lr = 0.01
I0523 05:22:14.530777 34682 solver.cpp:239] Iteration 55820 (2.34812 iter/s, 4.25872s/10 iters), loss = 9.47017
I0523 05:22:14.530975 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.47017 (* 1 = 9.47017 loss)
I0523 05:22:15.348984 34682 sgd_solver.cpp:112] Iteration 55820, lr = 0.01
I0523 05:22:19.299019 34682 solver.cpp:239] Iteration 55830 (2.09738 iter/s, 4.76786s/10 iters), loss = 8.05349
I0523 05:22:19.299062 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05349 (* 1 = 8.05349 loss)
I0523 05:22:19.368048 34682 sgd_solver.cpp:112] Iteration 55830, lr = 0.01
I0523 05:22:23.963721 34682 solver.cpp:239] Iteration 55840 (2.14387 iter/s, 4.66447s/10 iters), loss = 8.5442
I0523 05:22:23.963768 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5442 (* 1 = 8.5442 loss)
I0523 05:22:24.036070 34682 sgd_solver.cpp:112] Iteration 55840, lr = 0.01
I0523 05:22:28.303098 34682 solver.cpp:239] Iteration 55850 (2.3046 iter/s, 4.33914s/10 iters), loss = 8.61612
I0523 05:22:28.303149 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61612 (* 1 = 8.61612 loss)
I0523 05:22:28.926246 34682 sgd_solver.cpp:112] Iteration 55850, lr = 0.01
I0523 05:22:32.898665 34682 solver.cpp:239] Iteration 55860 (2.17612 iter/s, 4.59533s/10 iters), loss = 8.5121
I0523 05:22:32.898721 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5121 (* 1 = 8.5121 loss)
I0523 05:22:32.955962 34682 sgd_solver.cpp:112] Iteration 55860, lr = 0.01
I0523 05:22:36.350543 34682 solver.cpp:239] Iteration 55870 (2.89714 iter/s, 3.45168s/10 iters), loss = 8.50129
I0523 05:22:36.350582 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50129 (* 1 = 8.50129 loss)
I0523 05:22:37.166052 34682 sgd_solver.cpp:112] Iteration 55870, lr = 0.01
I0523 05:22:41.928962 34682 solver.cpp:239] Iteration 55880 (1.79271 iter/s, 5.57815s/10 iters), loss = 8.13367
I0523 05:22:41.929013 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13367 (* 1 = 8.13367 loss)
I0523 05:22:42.007926 34682 sgd_solver.cpp:112] Iteration 55880, lr = 0.01
I0523 05:22:44.651715 34682 solver.cpp:239] Iteration 55890 (3.67299 iter/s, 2.72258s/10 iters), loss = 8.5503
I0523 05:22:44.651991 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5503 (* 1 = 8.5503 loss)
I0523 05:22:45.339992 34682 sgd_solver.cpp:112] Iteration 55890, lr = 0.01
I0523 05:22:49.259644 34682 solver.cpp:239] Iteration 55900 (2.17037 iter/s, 4.6075s/10 iters), loss = 8.01771
I0523 05:22:49.259696 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01771 (* 1 = 8.01771 loss)
I0523 05:22:50.089432 34682 sgd_solver.cpp:112] Iteration 55900, lr = 0.01
I0523 05:22:55.842947 34682 solver.cpp:239] Iteration 55910 (1.51907 iter/s, 6.58298s/10 iters), loss = 7.01811
I0523 05:22:55.843016 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.01811 (* 1 = 7.01811 loss)
I0523 05:22:55.906702 34682 sgd_solver.cpp:112] Iteration 55910, lr = 0.01
I0523 05:23:00.833441 34682 solver.cpp:239] Iteration 55920 (2.00392 iter/s, 4.99021s/10 iters), loss = 8.8439
I0523 05:23:00.833503 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8439 (* 1 = 8.8439 loss)
I0523 05:23:00.905902 34682 sgd_solver.cpp:112] Iteration 55920, lr = 0.01
I0523 05:23:05.641752 34682 solver.cpp:239] Iteration 55930 (2.07986 iter/s, 4.80801s/10 iters), loss = 8.21863
I0523 05:23:05.641805 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21863 (* 1 = 8.21863 loss)
I0523 05:23:05.703577 34682 sgd_solver.cpp:112] Iteration 55930, lr = 0.01
I0523 05:23:10.518429 34682 solver.cpp:239] Iteration 55940 (2.05068 iter/s, 4.87642s/10 iters), loss = 7.82502
I0523 05:23:10.518477 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82502 (* 1 = 7.82502 loss)
I0523 05:23:10.575799 34682 sgd_solver.cpp:112] Iteration 55940, lr = 0.01
I0523 05:23:16.291837 34682 solver.cpp:239] Iteration 55950 (1.73217 iter/s, 5.77312s/10 iters), loss = 8.696
I0523 05:23:16.292040 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.696 (* 1 = 8.696 loss)
I0523 05:23:17.118055 34682 sgd_solver.cpp:112] Iteration 55950, lr = 0.01
I0523 05:23:21.746557 34682 solver.cpp:239] Iteration 55960 (1.83341 iter/s, 5.4543s/10 iters), loss = 8.70212
I0523 05:23:21.746598 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70212 (* 1 = 8.70212 loss)
I0523 05:23:22.437983 34682 sgd_solver.cpp:112] Iteration 55960, lr = 0.01
I0523 05:23:27.111793 34682 solver.cpp:239] Iteration 55970 (1.86394 iter/s, 5.36497s/10 iters), loss = 8.0808
I0523 05:23:27.111840 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0808 (* 1 = 8.0808 loss)
I0523 05:23:27.178591 34682 sgd_solver.cpp:112] Iteration 55970, lr = 0.01
I0523 05:23:30.560708 34682 solver.cpp:239] Iteration 55980 (2.89962 iter/s, 3.44872s/10 iters), loss = 8.26613
I0523 05:23:30.560750 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26613 (* 1 = 8.26613 loss)
I0523 05:23:30.617950 34682 sgd_solver.cpp:112] Iteration 55980, lr = 0.01
I0523 05:23:34.877910 34682 solver.cpp:239] Iteration 55990 (2.31643 iter/s, 4.31698s/10 iters), loss = 8.12788
I0523 05:23:34.877960 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12788 (* 1 = 8.12788 loss)
I0523 05:23:35.693233 34682 sgd_solver.cpp:112] Iteration 55990, lr = 0.01
I0523 05:23:40.218086 34682 solver.cpp:239] Iteration 56000 (1.87269 iter/s, 5.3399s/10 iters), loss = 8.6649
I0523 05:23:40.218165 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6649 (* 1 = 8.6649 loss)
I0523 05:23:41.060864 34682 sgd_solver.cpp:112] Iteration 56000, lr = 0.01
I0523 05:23:46.316181 34682 solver.cpp:239] Iteration 56010 (1.63994 iter/s, 6.09777s/10 iters), loss = 8.34428
I0523 05:23:46.316401 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34428 (* 1 = 8.34428 loss)
I0523 05:23:46.715355 34682 sgd_solver.cpp:112] Iteration 56010, lr = 0.01
I0523 05:23:50.798835 34682 solver.cpp:239] Iteration 56020 (2.23101 iter/s, 4.48227s/10 iters), loss = 8.17971
I0523 05:23:50.798887 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17971 (* 1 = 8.17971 loss)
I0523 05:23:51.584185 34682 sgd_solver.cpp:112] Iteration 56020, lr = 0.01
I0523 05:23:56.393908 34682 solver.cpp:239] Iteration 56030 (1.78738 iter/s, 5.59479s/10 iters), loss = 8.02382
I0523 05:23:56.393945 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02382 (* 1 = 8.02382 loss)
I0523 05:23:56.469236 34682 sgd_solver.cpp:112] Iteration 56030, lr = 0.01
I0523 05:24:00.461768 34682 solver.cpp:239] Iteration 56040 (2.45843 iter/s, 4.06764s/10 iters), loss = 8.76714
I0523 05:24:00.461817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76714 (* 1 = 8.76714 loss)
I0523 05:24:00.514950 34682 sgd_solver.cpp:112] Iteration 56040, lr = 0.01
I0523 05:24:05.405834 34682 solver.cpp:239] Iteration 56050 (2.02273 iter/s, 4.94382s/10 iters), loss = 8.74214
I0523 05:24:05.405879 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74214 (* 1 = 8.74214 loss)
I0523 05:24:05.468626 34682 sgd_solver.cpp:112] Iteration 56050, lr = 0.01
I0523 05:24:09.564909 34682 solver.cpp:239] Iteration 56060 (2.40451 iter/s, 4.15885s/10 iters), loss = 9.11414
I0523 05:24:09.564960 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11414 (* 1 = 9.11414 loss)
I0523 05:24:10.424731 34682 sgd_solver.cpp:112] Iteration 56060, lr = 0.01
I0523 05:24:15.895632 34682 solver.cpp:239] Iteration 56070 (1.57967 iter/s, 6.33042s/10 iters), loss = 8.54552
I0523 05:24:15.895678 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54552 (* 1 = 8.54552 loss)
I0523 05:24:16.044342 34682 sgd_solver.cpp:112] Iteration 56070, lr = 0.01
I0523 05:24:20.119705 34682 solver.cpp:239] Iteration 56080 (2.36751 iter/s, 4.22385s/10 iters), loss = 8.03858
I0523 05:24:20.120008 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03858 (* 1 = 8.03858 loss)
I0523 05:24:20.639230 34682 sgd_solver.cpp:112] Iteration 56080, lr = 0.01
I0523 05:24:25.260804 34682 solver.cpp:239] Iteration 56090 (1.94529 iter/s, 5.14061s/10 iters), loss = 7.84799
I0523 05:24:25.260854 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84799 (* 1 = 7.84799 loss)
I0523 05:24:26.102388 34682 sgd_solver.cpp:112] Iteration 56090, lr = 0.01
I0523 05:24:33.051373 34682 solver.cpp:239] Iteration 56100 (1.28366 iter/s, 7.79021s/10 iters), loss = 8.56554
I0523 05:24:33.051412 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56554 (* 1 = 8.56554 loss)
I0523 05:24:33.124564 34682 sgd_solver.cpp:112] Iteration 56100, lr = 0.01
I0523 05:24:35.562114 34682 solver.cpp:239] Iteration 56110 (3.98313 iter/s, 2.51059s/10 iters), loss = 7.77604
I0523 05:24:35.562153 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77604 (* 1 = 7.77604 loss)
I0523 05:24:35.636056 34682 sgd_solver.cpp:112] Iteration 56110, lr = 0.01
I0523 05:24:40.418053 34682 solver.cpp:239] Iteration 56120 (2.05944 iter/s, 4.8557s/10 iters), loss = 8.89096
I0523 05:24:40.418097 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89096 (* 1 = 8.89096 loss)
I0523 05:24:40.495254 34682 sgd_solver.cpp:112] Iteration 56120, lr = 0.01
I0523 05:24:44.998324 34682 solver.cpp:239] Iteration 56130 (2.18339 iter/s, 4.58004s/10 iters), loss = 7.97536
I0523 05:24:44.998373 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97536 (* 1 = 7.97536 loss)
I0523 05:24:45.073061 34682 sgd_solver.cpp:112] Iteration 56130, lr = 0.01
I0523 05:24:49.112515 34682 solver.cpp:239] Iteration 56140 (2.43074 iter/s, 4.11397s/10 iters), loss = 7.74946
I0523 05:24:49.112574 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74946 (* 1 = 7.74946 loss)
I0523 05:24:49.993260 34682 sgd_solver.cpp:112] Iteration 56140, lr = 0.01
I0523 05:24:54.975040 34682 solver.cpp:239] Iteration 56150 (1.70584 iter/s, 5.86222s/10 iters), loss = 9.62503
I0523 05:24:54.975258 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.62503 (* 1 = 9.62503 loss)
I0523 05:24:55.862040 34682 sgd_solver.cpp:112] Iteration 56150, lr = 0.01
I0523 05:24:59.864248 34682 solver.cpp:239] Iteration 56160 (2.04548 iter/s, 4.88882s/10 iters), loss = 8.13636
I0523 05:24:59.864307 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13636 (* 1 = 8.13636 loss)
I0523 05:25:00.310336 34682 sgd_solver.cpp:112] Iteration 56160, lr = 0.01
I0523 05:25:04.264616 34682 solver.cpp:239] Iteration 56170 (2.27266 iter/s, 4.40013s/10 iters), loss = 9.11827
I0523 05:25:04.264663 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11827 (* 1 = 9.11827 loss)
I0523 05:25:04.322018 34682 sgd_solver.cpp:112] Iteration 56170, lr = 0.01
I0523 05:25:07.694782 34682 solver.cpp:239] Iteration 56180 (2.91547 iter/s, 3.42998s/10 iters), loss = 8.28573
I0523 05:25:07.694833 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28573 (* 1 = 8.28573 loss)
I0523 05:25:07.999513 34682 sgd_solver.cpp:112] Iteration 56180, lr = 0.01
I0523 05:25:12.255431 34682 solver.cpp:239] Iteration 56190 (2.19279 iter/s, 4.56041s/10 iters), loss = 8.32652
I0523 05:25:12.255483 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32652 (* 1 = 8.32652 loss)
I0523 05:25:12.325556 34682 sgd_solver.cpp:112] Iteration 56190, lr = 0.01
I0523 05:25:19.060250 34682 solver.cpp:239] Iteration 56200 (1.46962 iter/s, 6.8045s/10 iters), loss = 8.18283
I0523 05:25:19.060299 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18283 (* 1 = 8.18283 loss)
I0523 05:25:19.763245 34682 sgd_solver.cpp:112] Iteration 56200, lr = 0.01
I0523 05:25:23.068229 34682 solver.cpp:239] Iteration 56210 (2.49516 iter/s, 4.00777s/10 iters), loss = 8.11813
I0523 05:25:23.068275 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11813 (* 1 = 8.11813 loss)
I0523 05:25:23.894948 34682 sgd_solver.cpp:112] Iteration 56210, lr = 0.01
I0523 05:25:28.395009 34682 solver.cpp:239] Iteration 56220 (1.8774 iter/s, 5.32651s/10 iters), loss = 8.22059
I0523 05:25:28.395301 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22059 (* 1 = 8.22059 loss)
I0523 05:25:29.201063 34682 sgd_solver.cpp:112] Iteration 56220, lr = 0.01
I0523 05:25:34.875699 34682 solver.cpp:239] Iteration 56230 (1.54319 iter/s, 6.4801s/10 iters), loss = 9.04881
I0523 05:25:34.875766 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04881 (* 1 = 9.04881 loss)
I0523 05:25:35.589622 34682 sgd_solver.cpp:112] Iteration 56230, lr = 0.01
I0523 05:25:40.168090 34682 solver.cpp:239] Iteration 56240 (1.88961 iter/s, 5.29211s/10 iters), loss = 8.45922
I0523 05:25:40.168133 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45922 (* 1 = 8.45922 loss)
I0523 05:25:40.241297 34682 sgd_solver.cpp:112] Iteration 56240, lr = 0.01
I0523 05:25:46.185487 34682 solver.cpp:239] Iteration 56250 (1.66193 iter/s, 6.01711s/10 iters), loss = 7.41932
I0523 05:25:46.185551 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.41932 (* 1 = 7.41932 loss)
I0523 05:25:47.054383 34682 sgd_solver.cpp:112] Iteration 56250, lr = 0.01
I0523 05:25:52.599726 34682 solver.cpp:239] Iteration 56260 (1.55911 iter/s, 6.41392s/10 iters), loss = 8.26628
I0523 05:25:52.599776 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26628 (* 1 = 8.26628 loss)
I0523 05:25:52.664978 34682 sgd_solver.cpp:112] Iteration 56260, lr = 0.01
I0523 05:25:57.236502 34682 solver.cpp:239] Iteration 56270 (2.15678 iter/s, 4.63653s/10 iters), loss = 8.21459
I0523 05:25:57.236542 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21459 (* 1 = 8.21459 loss)
I0523 05:25:57.313369 34682 sgd_solver.cpp:112] Iteration 56270, lr = 0.01
I0523 05:26:02.816751 34682 solver.cpp:239] Iteration 56280 (1.79212 iter/s, 5.57997s/10 iters), loss = 8.93265
I0523 05:26:02.816951 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93265 (* 1 = 8.93265 loss)
I0523 05:26:02.870141 34682 sgd_solver.cpp:112] Iteration 56280, lr = 0.01
I0523 05:26:08.646800 34682 solver.cpp:239] Iteration 56290 (1.71538 iter/s, 5.82963s/10 iters), loss = 8.77601
I0523 05:26:08.646857 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77601 (* 1 = 8.77601 loss)
I0523 05:26:08.713939 34682 sgd_solver.cpp:112] Iteration 56290, lr = 0.01
I0523 05:26:12.491896 34682 solver.cpp:239] Iteration 56300 (2.60088 iter/s, 3.84486s/10 iters), loss = 8.01165
I0523 05:26:12.491956 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01165 (* 1 = 8.01165 loss)
I0523 05:26:13.345175 34682 sgd_solver.cpp:112] Iteration 56300, lr = 0.01
I0523 05:26:17.524559 34682 solver.cpp:239] Iteration 56310 (1.98713 iter/s, 5.03237s/10 iters), loss = 7.56256
I0523 05:26:17.524601 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56256 (* 1 = 7.56256 loss)
I0523 05:26:17.590936 34682 sgd_solver.cpp:112] Iteration 56310, lr = 0.01
I0523 05:26:22.890650 34682 solver.cpp:239] Iteration 56320 (1.86365 iter/s, 5.36581s/10 iters), loss = 8.0246
I0523 05:26:22.890741 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0246 (* 1 = 8.0246 loss)
I0523 05:26:23.699916 34682 sgd_solver.cpp:112] Iteration 56320, lr = 0.01
I0523 05:26:27.221202 34682 solver.cpp:239] Iteration 56330 (2.30932 iter/s, 4.33029s/10 iters), loss = 9.45174
I0523 05:26:27.221246 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.45174 (* 1 = 9.45174 loss)
I0523 05:26:27.727980 34682 sgd_solver.cpp:112] Iteration 56330, lr = 0.01
I0523 05:26:33.715566 34682 solver.cpp:239] Iteration 56340 (1.53987 iter/s, 6.49405s/10 iters), loss = 9.13158
I0523 05:26:33.715859 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13158 (* 1 = 9.13158 loss)
I0523 05:26:33.780720 34682 sgd_solver.cpp:112] Iteration 56340, lr = 0.01
I0523 05:26:38.172179 34682 solver.cpp:239] Iteration 56350 (2.24408 iter/s, 4.45617s/10 iters), loss = 8.42779
I0523 05:26:38.172228 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42779 (* 1 = 8.42779 loss)
I0523 05:26:38.263396 34682 sgd_solver.cpp:112] Iteration 56350, lr = 0.01
I0523 05:26:44.666411 34682 solver.cpp:239] Iteration 56360 (1.5399 iter/s, 6.49392s/10 iters), loss = 8.95879
I0523 05:26:44.666453 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95879 (* 1 = 8.95879 loss)
I0523 05:26:44.743151 34682 sgd_solver.cpp:112] Iteration 56360, lr = 0.01
I0523 05:26:50.018903 34682 solver.cpp:239] Iteration 56370 (1.86838 iter/s, 5.35222s/10 iters), loss = 8.5204
I0523 05:26:50.018967 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5204 (* 1 = 8.5204 loss)
I0523 05:26:50.786181 34682 sgd_solver.cpp:112] Iteration 56370, lr = 0.01
I0523 05:26:55.487262 34682 solver.cpp:239] Iteration 56380 (1.8288 iter/s, 5.46808s/10 iters), loss = 8.41212
I0523 05:26:55.487326 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41212 (* 1 = 8.41212 loss)
I0523 05:26:55.563127 34682 sgd_solver.cpp:112] Iteration 56380, lr = 0.01
I0523 05:26:59.596139 34682 solver.cpp:239] Iteration 56390 (2.4339 iter/s, 4.10864s/10 iters), loss = 8.43167
I0523 05:26:59.596190 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43167 (* 1 = 8.43167 loss)
I0523 05:26:59.672494 34682 sgd_solver.cpp:112] Iteration 56390, lr = 0.01
I0523 05:27:04.641810 34682 solver.cpp:239] Iteration 56400 (1.982 iter/s, 5.04542s/10 iters), loss = 8.65723
I0523 05:27:04.641968 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65723 (* 1 = 8.65723 loss)
I0523 05:27:05.208468 34682 sgd_solver.cpp:112] Iteration 56400, lr = 0.01
I0523 05:27:12.200922 34682 solver.cpp:239] Iteration 56410 (1.32299 iter/s, 7.55864s/10 iters), loss = 7.97509
I0523 05:27:12.200978 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97509 (* 1 = 7.97509 loss)
I0523 05:27:12.260248 34682 sgd_solver.cpp:112] Iteration 56410, lr = 0.01
I0523 05:27:20.144729 34682 solver.cpp:239] Iteration 56420 (1.2589 iter/s, 7.94344s/10 iters), loss = 8.61147
I0523 05:27:20.144778 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61147 (* 1 = 8.61147 loss)
I0523 05:27:20.200209 34682 sgd_solver.cpp:112] Iteration 56420, lr = 0.01
I0523 05:27:25.028295 34682 solver.cpp:239] Iteration 56430 (2.04779 iter/s, 4.88332s/10 iters), loss = 8.29595
I0523 05:27:25.028337 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29595 (* 1 = 8.29595 loss)
I0523 05:27:25.098106 34682 sgd_solver.cpp:112] Iteration 56430, lr = 0.01
I0523 05:27:29.091320 34682 solver.cpp:239] Iteration 56440 (2.46135 iter/s, 4.06281s/10 iters), loss = 8.12728
I0523 05:27:29.091372 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12728 (* 1 = 8.12728 loss)
I0523 05:27:29.897583 34682 sgd_solver.cpp:112] Iteration 56440, lr = 0.01
I0523 05:27:35.367060 34682 solver.cpp:239] Iteration 56450 (1.59352 iter/s, 6.27543s/10 iters), loss = 8.14933
I0523 05:27:35.367308 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14933 (* 1 = 8.14933 loss)
I0523 05:27:35.428422 34682 sgd_solver.cpp:112] Iteration 56450, lr = 0.01
I0523 05:27:40.251443 34682 solver.cpp:239] Iteration 56460 (2.04752 iter/s, 4.88396s/10 iters), loss = 7.68452
I0523 05:27:40.251492 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68452 (* 1 = 7.68452 loss)
I0523 05:27:40.330803 34682 sgd_solver.cpp:112] Iteration 56460, lr = 0.01
I0523 05:27:42.733453 34682 solver.cpp:239] Iteration 56470 (4.02925 iter/s, 2.48185s/10 iters), loss = 8.29954
I0523 05:27:42.733501 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29954 (* 1 = 8.29954 loss)
I0523 05:27:42.779235 34682 sgd_solver.cpp:112] Iteration 56470, lr = 0.01
I0523 05:27:44.629097 34682 solver.cpp:239] Iteration 56480 (5.27561 iter/s, 1.89552s/10 iters), loss = 8.36094
I0523 05:27:44.629138 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36094 (* 1 = 8.36094 loss)
I0523 05:27:44.670907 34682 sgd_solver.cpp:112] Iteration 56480, lr = 0.01
I0523 05:27:45.859097 34682 solver.cpp:239] Iteration 56490 (8.13078 iter/s, 1.22989s/10 iters), loss = 7.63743
I0523 05:27:45.859136 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63743 (* 1 = 7.63743 loss)
I0523 05:27:45.905102 34682 sgd_solver.cpp:112] Iteration 56490, lr = 0.01
I0523 05:27:47.264919 34682 solver.cpp:239] Iteration 56500 (7.1138 iter/s, 1.40572s/10 iters), loss = 7.4463
I0523 05:27:47.264962 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4463 (* 1 = 7.4463 loss)
I0523 05:27:47.933327 34682 sgd_solver.cpp:112] Iteration 56500, lr = 0.01
I0523 05:27:52.531774 34682 solver.cpp:239] Iteration 56510 (1.89876 iter/s, 5.2666s/10 iters), loss = 7.4554
I0523 05:27:52.531817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4554 (* 1 = 7.4554 loss)
I0523 05:27:53.368990 34682 sgd_solver.cpp:112] Iteration 56510, lr = 0.01
I0523 05:27:58.191177 34682 solver.cpp:239] Iteration 56520 (1.76706 iter/s, 5.65912s/10 iters), loss = 8.08901
I0523 05:27:58.191236 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08901 (* 1 = 8.08901 loss)
I0523 05:27:59.031599 34682 sgd_solver.cpp:112] Iteration 56520, lr = 0.01
I0523 05:28:04.454288 34682 solver.cpp:239] Iteration 56530 (1.59673 iter/s, 6.2628s/10 iters), loss = 8.23953
I0523 05:28:04.454337 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23953 (* 1 = 8.23953 loss)
I0523 05:28:05.336164 34682 sgd_solver.cpp:112] Iteration 56530, lr = 0.01
I0523 05:28:07.882385 34682 solver.cpp:239] Iteration 56540 (2.91724 iter/s, 3.4279s/10 iters), loss = 8.88637
I0523 05:28:07.882575 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88637 (* 1 = 8.88637 loss)
I0523 05:28:08.740099 34682 sgd_solver.cpp:112] Iteration 56540, lr = 0.01
I0523 05:28:13.114087 34682 solver.cpp:239] Iteration 56550 (1.91157 iter/s, 5.2313s/10 iters), loss = 8.81593
I0523 05:28:13.114132 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81593 (* 1 = 8.81593 loss)
I0523 05:28:13.179057 34682 sgd_solver.cpp:112] Iteration 56550, lr = 0.01
I0523 05:28:17.667194 34682 solver.cpp:239] Iteration 56560 (2.19642 iter/s, 4.55286s/10 iters), loss = 7.84807
I0523 05:28:17.667258 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84807 (* 1 = 7.84807 loss)
I0523 05:28:18.474918 34682 sgd_solver.cpp:112] Iteration 56560, lr = 0.01
I0523 05:28:25.731425 34682 solver.cpp:239] Iteration 56570 (1.2401 iter/s, 8.06385s/10 iters), loss = 7.85819
I0523 05:28:25.731467 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.85819 (* 1 = 7.85819 loss)
I0523 05:28:25.807873 34682 sgd_solver.cpp:112] Iteration 56570, lr = 0.01
I0523 05:28:29.823312 34682 solver.cpp:239] Iteration 56580 (2.44399 iter/s, 4.09168s/10 iters), loss = 8.22303
I0523 05:28:29.823351 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22303 (* 1 = 8.22303 loss)
I0523 05:28:29.911209 34682 sgd_solver.cpp:112] Iteration 56580, lr = 0.01
I0523 05:28:33.033985 34682 solver.cpp:239] Iteration 56590 (3.11479 iter/s, 3.21049s/10 iters), loss = 9.25105
I0523 05:28:33.034027 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25105 (* 1 = 9.25105 loss)
I0523 05:28:33.113196 34682 sgd_solver.cpp:112] Iteration 56590, lr = 0.01
I0523 05:28:37.852502 34682 solver.cpp:239] Iteration 56600 (2.07544 iter/s, 4.81827s/10 iters), loss = 8.10998
I0523 05:28:37.852558 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10998 (* 1 = 8.10998 loss)
I0523 05:28:38.706323 34682 sgd_solver.cpp:112] Iteration 56600, lr = 0.01
I0523 05:28:43.387833 34682 solver.cpp:239] Iteration 56610 (1.80667 iter/s, 5.53504s/10 iters), loss = 8.17983
I0523 05:28:43.387878 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17983 (* 1 = 8.17983 loss)
I0523 05:28:44.250396 34682 sgd_solver.cpp:112] Iteration 56610, lr = 0.01
I0523 05:28:49.140555 34682 solver.cpp:239] Iteration 56620 (1.73839 iter/s, 5.75244s/10 iters), loss = 8.28797
I0523 05:28:49.140600 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28797 (* 1 = 8.28797 loss)
I0523 05:28:49.207144 34682 sgd_solver.cpp:112] Iteration 56620, lr = 0.01
I0523 05:28:54.716123 34682 solver.cpp:239] Iteration 56630 (1.79363 iter/s, 5.57529s/10 iters), loss = 8.73357
I0523 05:28:54.716176 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73357 (* 1 = 8.73357 loss)
I0523 05:28:54.792695 34682 sgd_solver.cpp:112] Iteration 56630, lr = 0.01
I0523 05:28:59.440711 34682 solver.cpp:239] Iteration 56640 (2.1167 iter/s, 4.72434s/10 iters), loss = 9.70083
I0523 05:28:59.440762 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.70083 (* 1 = 9.70083 loss)
I0523 05:28:59.504515 34682 sgd_solver.cpp:112] Iteration 56640, lr = 0.01
I0523 05:29:04.237668 34682 solver.cpp:239] Iteration 56650 (2.08476 iter/s, 4.79672s/10 iters), loss = 8.93027
I0523 05:29:04.237706 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93027 (* 1 = 8.93027 loss)
I0523 05:29:04.313478 34682 sgd_solver.cpp:112] Iteration 56650, lr = 0.01
I0523 05:29:09.264432 34682 solver.cpp:239] Iteration 56660 (1.98945 iter/s, 5.02652s/10 iters), loss = 7.92596
I0523 05:29:09.264663 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92596 (* 1 = 7.92596 loss)
I0523 05:29:09.328006 34682 sgd_solver.cpp:112] Iteration 56660, lr = 0.01
I0523 05:29:14.183966 34682 solver.cpp:239] Iteration 56670 (2.03288 iter/s, 4.91913s/10 iters), loss = 8.97193
I0523 05:29:14.184029 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97193 (* 1 = 8.97193 loss)
I0523 05:29:14.249442 34682 sgd_solver.cpp:112] Iteration 56670, lr = 0.01
I0523 05:29:17.515359 34682 solver.cpp:239] Iteration 56680 (3.00192 iter/s, 3.3312s/10 iters), loss = 8.29208
I0523 05:29:17.515403 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29208 (* 1 = 8.29208 loss)
I0523 05:29:17.578341 34682 sgd_solver.cpp:112] Iteration 56680, lr = 0.01
I0523 05:29:21.993357 34682 solver.cpp:239] Iteration 56690 (2.23326 iter/s, 4.47777s/10 iters), loss = 8.98299
I0523 05:29:21.993402 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98299 (* 1 = 8.98299 loss)
I0523 05:29:22.061555 34682 sgd_solver.cpp:112] Iteration 56690, lr = 0.01
I0523 05:29:27.432458 34682 solver.cpp:239] Iteration 56700 (1.83937 iter/s, 5.43666s/10 iters), loss = 7.53255
I0523 05:29:27.432512 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.53255 (* 1 = 7.53255 loss)
I0523 05:29:27.831023 34682 sgd_solver.cpp:112] Iteration 56700, lr = 0.01
I0523 05:29:33.497730 34682 solver.cpp:239] Iteration 56710 (1.64881 iter/s, 6.06497s/10 iters), loss = 9.32327
I0523 05:29:33.497777 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.32327 (* 1 = 9.32327 loss)
I0523 05:29:34.331813 34682 sgd_solver.cpp:112] Iteration 56710, lr = 0.01
I0523 05:29:37.921854 34682 solver.cpp:239] Iteration 56720 (2.26046 iter/s, 4.42388s/10 iters), loss = 7.56506
I0523 05:29:37.921921 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56506 (* 1 = 7.56506 loss)
I0523 05:29:37.981822 34682 sgd_solver.cpp:112] Iteration 56720, lr = 0.01
I0523 05:29:43.388229 34682 solver.cpp:239] Iteration 56730 (1.82946 iter/s, 5.46609s/10 iters), loss = 8.36601
I0523 05:29:43.388434 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36601 (* 1 = 8.36601 loss)
I0523 05:29:43.464519 34682 sgd_solver.cpp:112] Iteration 56730, lr = 0.01
I0523 05:29:48.316396 34682 solver.cpp:239] Iteration 56740 (2.02931 iter/s, 4.92777s/10 iters), loss = 8.00488
I0523 05:29:48.316454 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00488 (* 1 = 8.00488 loss)
I0523 05:29:49.085999 34682 sgd_solver.cpp:112] Iteration 56740, lr = 0.01
I0523 05:29:54.138597 34682 solver.cpp:239] Iteration 56750 (1.71765 iter/s, 5.82191s/10 iters), loss = 8.97086
I0523 05:29:54.138643 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97086 (* 1 = 8.97086 loss)
I0523 05:29:54.215327 34682 sgd_solver.cpp:112] Iteration 56750, lr = 0.01
I0523 05:29:57.427232 34682 solver.cpp:239] Iteration 56760 (3.04094 iter/s, 3.28845s/10 iters), loss = 8.91138
I0523 05:29:57.427273 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91138 (* 1 = 8.91138 loss)
I0523 05:29:57.501276 34682 sgd_solver.cpp:112] Iteration 56760, lr = 0.01
I0523 05:30:00.118393 34682 solver.cpp:239] Iteration 56770 (3.71609 iter/s, 2.691s/10 iters), loss = 8.5861
I0523 05:30:00.118461 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5861 (* 1 = 8.5861 loss)
I0523 05:30:00.802359 34682 sgd_solver.cpp:112] Iteration 56770, lr = 0.01
I0523 05:30:04.722157 34682 solver.cpp:239] Iteration 56780 (2.17226 iter/s, 4.6035s/10 iters), loss = 8.54255
I0523 05:30:04.722226 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54255 (* 1 = 8.54255 loss)
I0523 05:30:05.424551 34682 sgd_solver.cpp:112] Iteration 56780, lr = 0.01
I0523 05:30:09.773495 34682 solver.cpp:239] Iteration 56790 (1.97978 iter/s, 5.05107s/10 iters), loss = 8.72542
I0523 05:30:09.773547 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72542 (* 1 = 8.72542 loss)
I0523 05:30:10.478301 34682 sgd_solver.cpp:112] Iteration 56790, lr = 0.01
I0523 05:30:15.582617 34682 solver.cpp:239] Iteration 56800 (1.72152 iter/s, 5.80883s/10 iters), loss = 8.9274
I0523 05:30:15.583675 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9274 (* 1 = 8.9274 loss)
I0523 05:30:15.661314 34682 sgd_solver.cpp:112] Iteration 56800, lr = 0.01
I0523 05:30:21.195235 34682 solver.cpp:239] Iteration 56810 (1.78338 iter/s, 5.60732s/10 iters), loss = 8.88672
I0523 05:30:21.195281 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88672 (* 1 = 8.88672 loss)
I0523 05:30:21.270769 34682 sgd_solver.cpp:112] Iteration 56810, lr = 0.01
I0523 05:30:24.433727 34682 solver.cpp:239] Iteration 56820 (3.08805 iter/s, 3.23829s/10 iters), loss = 7.64445
I0523 05:30:24.433778 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64445 (* 1 = 7.64445 loss)
I0523 05:30:25.276767 34682 sgd_solver.cpp:112] Iteration 56820, lr = 0.01
I0523 05:30:28.825161 34682 solver.cpp:239] Iteration 56830 (2.27728 iter/s, 4.3912s/10 iters), loss = 8.30841
I0523 05:30:28.825202 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30841 (* 1 = 8.30841 loss)
I0523 05:30:28.889245 34682 sgd_solver.cpp:112] Iteration 56830, lr = 0.01
I0523 05:30:31.715693 34682 solver.cpp:239] Iteration 56840 (3.45978 iter/s, 2.89036s/10 iters), loss = 8.76162
I0523 05:30:31.715744 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76162 (* 1 = 8.76162 loss)
I0523 05:30:31.774096 34682 sgd_solver.cpp:112] Iteration 56840, lr = 0.01
I0523 05:30:37.110370 34682 solver.cpp:239] Iteration 56850 (1.85377 iter/s, 5.39441s/10 iters), loss = 9.17409
I0523 05:30:37.110416 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17409 (* 1 = 9.17409 loss)
I0523 05:30:37.203568 34682 sgd_solver.cpp:112] Iteration 56850, lr = 0.01
I0523 05:30:42.997843 34682 solver.cpp:239] Iteration 56860 (1.69861 iter/s, 5.88717s/10 iters), loss = 7.97222
I0523 05:30:42.997930 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97222 (* 1 = 7.97222 loss)
I0523 05:30:43.548044 34682 sgd_solver.cpp:112] Iteration 56860, lr = 0.01
I0523 05:30:48.313328 34682 solver.cpp:239] Iteration 56870 (1.88141 iter/s, 5.31518s/10 iters), loss = 8.90988
I0523 05:30:48.313499 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90988 (* 1 = 8.90988 loss)
I0523 05:30:49.103166 34682 sgd_solver.cpp:112] Iteration 56870, lr = 0.01
I0523 05:30:55.071508 34682 solver.cpp:239] Iteration 56880 (1.47979 iter/s, 6.75773s/10 iters), loss = 7.82251
I0523 05:30:55.071574 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82251 (* 1 = 7.82251 loss)
I0523 05:30:55.151901 34682 sgd_solver.cpp:112] Iteration 56880, lr = 0.01
I0523 05:31:00.185833 34682 solver.cpp:239] Iteration 56890 (1.95708 iter/s, 5.10966s/10 iters), loss = 8.51917
I0523 05:31:00.185878 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51917 (* 1 = 8.51917 loss)
I0523 05:31:00.249275 34682 sgd_solver.cpp:112] Iteration 56890, lr = 0.01
I0523 05:31:04.452822 34682 solver.cpp:239] Iteration 56900 (2.34369 iter/s, 4.26677s/10 iters), loss = 8.34527
I0523 05:31:04.452865 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34527 (* 1 = 8.34527 loss)
I0523 05:31:04.526199 34682 sgd_solver.cpp:112] Iteration 56900, lr = 0.01
I0523 05:31:09.310397 34682 solver.cpp:239] Iteration 56910 (2.05874 iter/s, 4.85733s/10 iters), loss = 9.43067
I0523 05:31:09.310446 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43067 (* 1 = 9.43067 loss)
I0523 05:31:09.396035 34682 sgd_solver.cpp:112] Iteration 56910, lr = 0.01
I0523 05:31:14.256104 34682 solver.cpp:239] Iteration 56920 (2.02207 iter/s, 4.94543s/10 iters), loss = 8.34799
I0523 05:31:14.256165 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34799 (* 1 = 8.34799 loss)
I0523 05:31:14.331755 34682 sgd_solver.cpp:112] Iteration 56920, lr = 0.01
I0523 05:31:19.259471 34682 solver.cpp:239] Iteration 56930 (1.99876 iter/s, 5.0031s/10 iters), loss = 9.21221
I0523 05:31:19.259690 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21221 (* 1 = 9.21221 loss)
I0523 05:31:20.124912 34682 sgd_solver.cpp:112] Iteration 56930, lr = 0.01
I0523 05:31:23.435806 34682 solver.cpp:239] Iteration 56940 (2.39465 iter/s, 4.17597s/10 iters), loss = 7.97953
I0523 05:31:23.435853 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97953 (* 1 = 7.97953 loss)
I0523 05:31:24.174234 34682 sgd_solver.cpp:112] Iteration 56940, lr = 0.01
I0523 05:31:28.510031 34682 solver.cpp:239] Iteration 56950 (1.97084 iter/s, 5.07397s/10 iters), loss = 9.22775
I0523 05:31:28.510078 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22775 (* 1 = 9.22775 loss)
I0523 05:31:28.597532 34682 sgd_solver.cpp:112] Iteration 56950, lr = 0.01
I0523 05:31:33.326108 34682 solver.cpp:239] Iteration 56960 (2.07649 iter/s, 4.81582s/10 iters), loss = 8.24122
I0523 05:31:33.326171 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24122 (* 1 = 8.24122 loss)
I0523 05:31:34.179332 34682 sgd_solver.cpp:112] Iteration 56960, lr = 0.01
I0523 05:31:40.674944 34682 solver.cpp:239] Iteration 56970 (1.36083 iter/s, 7.34848s/10 iters), loss = 8.19957
I0523 05:31:40.674990 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19957 (* 1 = 8.19957 loss)
I0523 05:31:40.737404 34682 sgd_solver.cpp:112] Iteration 56970, lr = 0.01
I0523 05:31:45.498728 34682 solver.cpp:239] Iteration 56980 (2.07318 iter/s, 4.82352s/10 iters), loss = 9.00578
I0523 05:31:45.498772 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00578 (* 1 = 9.00578 loss)
I0523 05:31:45.813715 34682 sgd_solver.cpp:112] Iteration 56980, lr = 0.01
I0523 05:31:48.695232 34682 solver.cpp:239] Iteration 56990 (3.12859 iter/s, 3.19633s/10 iters), loss = 7.99397
I0523 05:31:48.695271 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99397 (* 1 = 7.99397 loss)
I0523 05:31:48.762250 34682 sgd_solver.cpp:112] Iteration 56990, lr = 0.01
I0523 05:31:53.808177 34682 solver.cpp:239] Iteration 57000 (1.95592 iter/s, 5.11269s/10 iters), loss = 7.53113
I0523 05:31:53.808476 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.53113 (* 1 = 7.53113 loss)
I0523 05:31:54.404880 34682 sgd_solver.cpp:112] Iteration 57000, lr = 0.01
I0523 05:32:00.044064 34682 solver.cpp:239] Iteration 57010 (1.60376 iter/s, 6.23535s/10 iters), loss = 7.97421
I0523 05:32:00.044104 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97421 (* 1 = 7.97421 loss)
I0523 05:32:00.116901 34682 sgd_solver.cpp:112] Iteration 57010, lr = 0.01
I0523 05:32:03.443848 34682 solver.cpp:239] Iteration 57020 (2.94152 iter/s, 3.3996s/10 iters), loss = 9.12118
I0523 05:32:03.443892 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12118 (* 1 = 9.12118 loss)
I0523 05:32:03.508402 34682 sgd_solver.cpp:112] Iteration 57020, lr = 0.01
I0523 05:32:08.278337 34682 solver.cpp:239] Iteration 57030 (2.06857 iter/s, 4.83425s/10 iters), loss = 8.16612
I0523 05:32:08.278378 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16612 (* 1 = 8.16612 loss)
I0523 05:32:08.348878 34682 sgd_solver.cpp:112] Iteration 57030, lr = 0.01
I0523 05:32:13.992733 34682 solver.cpp:239] Iteration 57040 (1.75005 iter/s, 5.71412s/10 iters), loss = 8.55492
I0523 05:32:13.992782 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55492 (* 1 = 8.55492 loss)
I0523 05:32:14.852793 34682 sgd_solver.cpp:112] Iteration 57040, lr = 0.01
I0523 05:32:20.890954 34682 solver.cpp:239] Iteration 57050 (1.44972 iter/s, 6.89789s/10 iters), loss = 8.88065
I0523 05:32:20.891016 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88065 (* 1 = 8.88065 loss)
I0523 05:32:20.957159 34682 sgd_solver.cpp:112] Iteration 57050, lr = 0.01
I0523 05:32:25.523597 34682 solver.cpp:239] Iteration 57060 (2.15872 iter/s, 4.63238s/10 iters), loss = 8.05265
I0523 05:32:25.523754 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05265 (* 1 = 8.05265 loss)
I0523 05:32:25.580401 34682 sgd_solver.cpp:112] Iteration 57060, lr = 0.01
I0523 05:32:29.947113 34682 solver.cpp:239] Iteration 57070 (2.26082 iter/s, 4.42318s/10 iters), loss = 7.80704
I0523 05:32:29.947181 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80704 (* 1 = 7.80704 loss)
I0523 05:32:30.795565 34682 sgd_solver.cpp:112] Iteration 57070, lr = 0.01
I0523 05:32:34.418318 34682 solver.cpp:239] Iteration 57080 (2.23666 iter/s, 4.47096s/10 iters), loss = 8.56411
I0523 05:32:34.418370 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56411 (* 1 = 8.56411 loss)
I0523 05:32:35.249678 34682 sgd_solver.cpp:112] Iteration 57080, lr = 0.01
I0523 05:32:40.131189 34682 solver.cpp:239] Iteration 57090 (1.75053 iter/s, 5.71256s/10 iters), loss = 8.66134
I0523 05:32:40.131273 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66134 (* 1 = 8.66134 loss)
I0523 05:32:40.865373 34682 sgd_solver.cpp:112] Iteration 57090, lr = 0.01
I0523 05:32:46.665969 34682 solver.cpp:239] Iteration 57100 (1.53035 iter/s, 6.53444s/10 iters), loss = 8.39695
I0523 05:32:46.666015 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39695 (* 1 = 8.39695 loss)
I0523 05:32:46.734758 34682 sgd_solver.cpp:112] Iteration 57100, lr = 0.01
I0523 05:32:50.483731 34682 solver.cpp:239] Iteration 57110 (2.61947 iter/s, 3.81756s/10 iters), loss = 7.44403
I0523 05:32:50.483777 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44403 (* 1 = 7.44403 loss)
I0523 05:32:50.544065 34682 sgd_solver.cpp:112] Iteration 57110, lr = 0.01
I0523 05:32:57.066285 34682 solver.cpp:239] Iteration 57120 (1.51924 iter/s, 6.58223s/10 iters), loss = 8.28409
I0523 05:32:57.066582 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28409 (* 1 = 8.28409 loss)
I0523 05:32:57.773224 34682 sgd_solver.cpp:112] Iteration 57120, lr = 0.01
I0523 05:33:02.320201 34682 solver.cpp:239] Iteration 57130 (1.90352 iter/s, 5.25343s/10 iters), loss = 7.8762
I0523 05:33:02.320243 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8762 (* 1 = 7.8762 loss)
I0523 05:33:02.389802 34682 sgd_solver.cpp:112] Iteration 57130, lr = 0.01
I0523 05:33:08.251463 34682 solver.cpp:239] Iteration 57140 (1.68606 iter/s, 5.93097s/10 iters), loss = 8.71086
I0523 05:33:08.251518 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71086 (* 1 = 8.71086 loss)
I0523 05:33:08.324728 34682 sgd_solver.cpp:112] Iteration 57140, lr = 0.01
I0523 05:33:12.774688 34682 solver.cpp:239] Iteration 57150 (2.21094 iter/s, 4.52296s/10 iters), loss = 9.21881
I0523 05:33:12.774788 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21881 (* 1 = 9.21881 loss)
I0523 05:33:13.582309 34682 sgd_solver.cpp:112] Iteration 57150, lr = 0.01
I0523 05:33:16.131855 34682 solver.cpp:239] Iteration 57160 (2.97892 iter/s, 3.35693s/10 iters), loss = 8.47563
I0523 05:33:16.131911 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47563 (* 1 = 8.47563 loss)
I0523 05:33:16.201864 34682 sgd_solver.cpp:112] Iteration 57160, lr = 0.01
I0523 05:33:18.887542 34682 solver.cpp:239] Iteration 57170 (3.62909 iter/s, 2.75551s/10 iters), loss = 7.77365
I0523 05:33:18.887595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77365 (* 1 = 7.77365 loss)
I0523 05:33:19.615703 34682 sgd_solver.cpp:112] Iteration 57170, lr = 0.01
I0523 05:33:22.996232 34682 solver.cpp:239] Iteration 57180 (2.434 iter/s, 4.10846s/10 iters), loss = 8.10463
I0523 05:33:22.996286 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10463 (* 1 = 8.10463 loss)
I0523 05:33:23.065451 34682 sgd_solver.cpp:112] Iteration 57180, lr = 0.01
I0523 05:33:27.387460 34682 solver.cpp:239] Iteration 57190 (2.27739 iter/s, 4.39099s/10 iters), loss = 8.75367
I0523 05:33:27.387609 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75367 (* 1 = 8.75367 loss)
I0523 05:33:27.451787 34682 sgd_solver.cpp:112] Iteration 57190, lr = 0.01
I0523 05:33:33.297744 34682 solver.cpp:239] Iteration 57200 (1.69208 iter/s, 5.9099s/10 iters), loss = 7.46599
I0523 05:33:33.297796 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.46599 (* 1 = 7.46599 loss)
I0523 05:33:34.174636 34682 sgd_solver.cpp:112] Iteration 57200, lr = 0.01
I0523 05:33:38.062249 34682 solver.cpp:239] Iteration 57210 (2.09897 iter/s, 4.76425s/10 iters), loss = 8.18412
I0523 05:33:38.062315 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18412 (* 1 = 8.18412 loss)
I0523 05:33:38.125521 34682 sgd_solver.cpp:112] Iteration 57210, lr = 0.01
I0523 05:33:42.245419 34682 solver.cpp:239] Iteration 57220 (2.39067 iter/s, 4.18293s/10 iters), loss = 7.9443
I0523 05:33:42.245461 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9443 (* 1 = 7.9443 loss)
I0523 05:33:43.069535 34682 sgd_solver.cpp:112] Iteration 57220, lr = 0.01
I0523 05:33:47.596457 34682 solver.cpp:239] Iteration 57230 (1.86889 iter/s, 5.35078s/10 iters), loss = 8.6637
I0523 05:33:47.596508 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6637 (* 1 = 8.6637 loss)
I0523 05:33:47.656720 34682 sgd_solver.cpp:112] Iteration 57230, lr = 0.01
I0523 05:33:51.972240 34682 solver.cpp:239] Iteration 57240 (2.28542 iter/s, 4.37556s/10 iters), loss = 7.71285
I0523 05:33:51.972283 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.71285 (* 1 = 7.71285 loss)
I0523 05:33:52.856360 34682 sgd_solver.cpp:112] Iteration 57240, lr = 0.01
I0523 05:33:58.261996 34682 solver.cpp:239] Iteration 57250 (1.58996 iter/s, 6.28945s/10 iters), loss = 6.80074
I0523 05:33:58.262322 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.80074 (* 1 = 6.80074 loss)
I0523 05:33:58.320693 34682 sgd_solver.cpp:112] Iteration 57250, lr = 0.01
I0523 05:34:02.505733 34682 solver.cpp:239] Iteration 57260 (2.35668 iter/s, 4.24327s/10 iters), loss = 8.22731
I0523 05:34:02.505772 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22731 (* 1 = 8.22731 loss)
I0523 05:34:02.578951 34682 sgd_solver.cpp:112] Iteration 57260, lr = 0.01
I0523 05:34:09.799386 34682 solver.cpp:239] Iteration 57270 (1.37112 iter/s, 7.29332s/10 iters), loss = 8.84539
I0523 05:34:09.799432 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84539 (* 1 = 8.84539 loss)
I0523 05:34:09.872858 34682 sgd_solver.cpp:112] Iteration 57270, lr = 0.01
I0523 05:34:12.574326 34682 solver.cpp:239] Iteration 57280 (3.60389 iter/s, 2.77478s/10 iters), loss = 8.84929
I0523 05:34:12.574371 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84929 (* 1 = 8.84929 loss)
I0523 05:34:12.652915 34682 sgd_solver.cpp:112] Iteration 57280, lr = 0.01
I0523 05:34:18.748420 34682 solver.cpp:239] Iteration 57290 (1.61975 iter/s, 6.17378s/10 iters), loss = 8.42494
I0523 05:34:18.748482 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42494 (* 1 = 8.42494 loss)
I0523 05:34:19.553457 34682 sgd_solver.cpp:112] Iteration 57290, lr = 0.01
I0523 05:34:23.885273 34682 solver.cpp:239] Iteration 57300 (1.94682 iter/s, 5.13659s/10 iters), loss = 9.72963
I0523 05:34:23.885319 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.72963 (* 1 = 9.72963 loss)
I0523 05:34:24.611681 34682 sgd_solver.cpp:112] Iteration 57300, lr = 0.01
I0523 05:34:27.637666 34682 solver.cpp:239] Iteration 57310 (2.66511 iter/s, 3.75219s/10 iters), loss = 8.49966
I0523 05:34:27.637719 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49966 (* 1 = 8.49966 loss)
I0523 05:34:27.694941 34682 sgd_solver.cpp:112] Iteration 57310, lr = 0.01
I0523 05:34:32.650498 34682 solver.cpp:239] Iteration 57320 (1.99498 iter/s, 5.01258s/10 iters), loss = 9.09923
I0523 05:34:32.650786 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09923 (* 1 = 9.09923 loss)
I0523 05:34:32.711071 34682 sgd_solver.cpp:112] Iteration 57320, lr = 0.01
I0523 05:34:36.722442 34682 solver.cpp:239] Iteration 57330 (2.45608 iter/s, 4.07152s/10 iters), loss = 7.1401
I0523 05:34:36.722484 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.1401 (* 1 = 7.1401 loss)
I0523 05:34:36.788211 34682 sgd_solver.cpp:112] Iteration 57330, lr = 0.01
I0523 05:34:42.615959 34682 solver.cpp:239] Iteration 57340 (1.69686 iter/s, 5.89324s/10 iters), loss = 7.80766
I0523 05:34:42.616003 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80766 (* 1 = 7.80766 loss)
I0523 05:34:42.679718 34682 sgd_solver.cpp:112] Iteration 57340, lr = 0.01
I0523 05:34:46.566496 34682 solver.cpp:239] Iteration 57350 (2.53144 iter/s, 3.95032s/10 iters), loss = 8.13551
I0523 05:34:46.566561 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13551 (* 1 = 8.13551 loss)
I0523 05:34:46.630003 34682 sgd_solver.cpp:112] Iteration 57350, lr = 0.01
I0523 05:34:51.361948 34682 solver.cpp:239] Iteration 57360 (2.08542 iter/s, 4.79519s/10 iters), loss = 7.94358
I0523 05:34:51.361992 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94358 (* 1 = 7.94358 loss)
I0523 05:34:51.427698 34682 sgd_solver.cpp:112] Iteration 57360, lr = 0.01
I0523 05:34:54.793182 34682 solver.cpp:239] Iteration 57370 (2.91456 iter/s, 3.43105s/10 iters), loss = 8.97956
I0523 05:34:54.793232 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97956 (* 1 = 8.97956 loss)
I0523 05:34:55.011880 34682 sgd_solver.cpp:112] Iteration 57370, lr = 0.01
I0523 05:35:00.004544 34682 solver.cpp:239] Iteration 57380 (1.91898 iter/s, 5.2111s/10 iters), loss = 7.5036
I0523 05:35:00.004586 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5036 (* 1 = 7.5036 loss)
I0523 05:35:00.062377 34682 sgd_solver.cpp:112] Iteration 57380, lr = 0.01
I0523 05:35:03.501375 34682 solver.cpp:239] Iteration 57390 (2.85989 iter/s, 3.49664s/10 iters), loss = 7.63599
I0523 05:35:03.501572 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63599 (* 1 = 7.63599 loss)
I0523 05:35:03.566574 34682 sgd_solver.cpp:112] Iteration 57390, lr = 0.01
I0523 05:35:09.405748 34682 solver.cpp:239] Iteration 57400 (1.69379 iter/s, 5.90394s/10 iters), loss = 8.90907
I0523 05:35:09.405808 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90907 (* 1 = 8.90907 loss)
I0523 05:35:10.193439 34682 sgd_solver.cpp:112] Iteration 57400, lr = 0.01
I0523 05:35:14.899422 34682 solver.cpp:239] Iteration 57410 (1.82037 iter/s, 5.49338s/10 iters), loss = 8.26929
I0523 05:35:14.899466 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26929 (* 1 = 8.26929 loss)
I0523 05:35:14.964146 34682 sgd_solver.cpp:112] Iteration 57410, lr = 0.01
I0523 05:35:18.885581 34682 solver.cpp:239] Iteration 57420 (2.50882 iter/s, 3.98594s/10 iters), loss = 8.37746
I0523 05:35:18.885644 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37746 (* 1 = 8.37746 loss)
I0523 05:35:18.956835 34682 sgd_solver.cpp:112] Iteration 57420, lr = 0.01
I0523 05:35:25.368666 34682 solver.cpp:239] Iteration 57430 (1.54255 iter/s, 6.48276s/10 iters), loss = 7.29452
I0523 05:35:25.368726 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.29452 (* 1 = 7.29452 loss)
I0523 05:35:26.059382 34682 sgd_solver.cpp:112] Iteration 57430, lr = 0.01
I0523 05:35:32.397249 34682 solver.cpp:239] Iteration 57440 (1.42283 iter/s, 7.02823s/10 iters), loss = 7.97278
I0523 05:35:32.397298 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97278 (* 1 = 7.97278 loss)
I0523 05:35:32.457336 34682 sgd_solver.cpp:112] Iteration 57440, lr = 0.01
I0523 05:35:38.081997 34682 solver.cpp:239] Iteration 57450 (1.75918 iter/s, 5.68446s/10 iters), loss = 8.70828
I0523 05:35:38.082242 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70828 (* 1 = 8.70828 loss)
I0523 05:35:38.730108 34682 sgd_solver.cpp:112] Iteration 57450, lr = 0.01
I0523 05:35:43.599321 34682 solver.cpp:239] Iteration 57460 (1.81262 iter/s, 5.51689s/10 iters), loss = 8.97584
I0523 05:35:43.599361 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97584 (* 1 = 8.97584 loss)
I0523 05:35:43.675822 34682 sgd_solver.cpp:112] Iteration 57460, lr = 0.01
I0523 05:35:47.189064 34682 solver.cpp:239] Iteration 57470 (2.78587 iter/s, 3.58955s/10 iters), loss = 8.32176
I0523 05:35:47.189107 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32176 (* 1 = 8.32176 loss)
I0523 05:35:47.938163 34682 sgd_solver.cpp:112] Iteration 57470, lr = 0.01
I0523 05:35:53.484629 34682 solver.cpp:239] Iteration 57480 (1.5885 iter/s, 6.29527s/10 iters), loss = 7.40439
I0523 05:35:53.484679 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.40439 (* 1 = 7.40439 loss)
I0523 05:35:54.171485 34682 sgd_solver.cpp:112] Iteration 57480, lr = 0.01
I0523 05:35:59.166136 34682 solver.cpp:239] Iteration 57490 (1.76019 iter/s, 5.68122s/10 iters), loss = 8.10911
I0523 05:35:59.166185 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10911 (* 1 = 8.10911 loss)
I0523 05:35:59.235915 34682 sgd_solver.cpp:112] Iteration 57490, lr = 0.01
I0523 05:36:03.465454 34682 solver.cpp:239] Iteration 57500 (2.32608 iter/s, 4.29908s/10 iters), loss = 7.52708
I0523 05:36:03.465513 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.52708 (* 1 = 7.52708 loss)
I0523 05:36:04.288301 34682 sgd_solver.cpp:112] Iteration 57500, lr = 0.01
I0523 05:36:09.846763 34682 solver.cpp:239] Iteration 57510 (1.56715 iter/s, 6.38099s/10 iters), loss = 8.98986
I0523 05:36:09.846854 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98986 (* 1 = 8.98986 loss)
I0523 05:36:09.924213 34682 sgd_solver.cpp:112] Iteration 57510, lr = 0.01
I0523 05:36:14.893431 34682 solver.cpp:239] Iteration 57520 (1.98162 iter/s, 5.04637s/10 iters), loss = 8.65464
I0523 05:36:14.893483 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65464 (* 1 = 8.65464 loss)
I0523 05:36:14.963238 34682 sgd_solver.cpp:112] Iteration 57520, lr = 0.01
I0523 05:36:18.859738 34682 solver.cpp:239] Iteration 57530 (2.52137 iter/s, 3.96609s/10 iters), loss = 8.04152
I0523 05:36:18.859788 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04152 (* 1 = 8.04152 loss)
I0523 05:36:19.530773 34682 sgd_solver.cpp:112] Iteration 57530, lr = 0.01
I0523 05:36:23.308262 34682 solver.cpp:239] Iteration 57540 (2.24806 iter/s, 4.44828s/10 iters), loss = 8.56277
I0523 05:36:23.308328 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56277 (* 1 = 8.56277 loss)
I0523 05:36:24.147680 34682 sgd_solver.cpp:112] Iteration 57540, lr = 0.01
I0523 05:36:28.312841 34682 solver.cpp:239] Iteration 57550 (1.99828 iter/s, 5.00431s/10 iters), loss = 9.07805
I0523 05:36:28.312901 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07805 (* 1 = 9.07805 loss)
I0523 05:36:28.999682 34682 sgd_solver.cpp:112] Iteration 57550, lr = 0.01
I0523 05:36:32.864734 34682 solver.cpp:239] Iteration 57560 (2.197 iter/s, 4.55165s/10 iters), loss = 8.3107
I0523 05:36:32.864779 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3107 (* 1 = 8.3107 loss)
I0523 05:36:32.936205 34682 sgd_solver.cpp:112] Iteration 57560, lr = 0.01
I0523 05:36:38.789289 34682 solver.cpp:239] Iteration 57570 (1.68797 iter/s, 5.92427s/10 iters), loss = 8.36842
I0523 05:36:38.789338 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36842 (* 1 = 8.36842 loss)
I0523 05:36:38.857676 34682 sgd_solver.cpp:112] Iteration 57570, lr = 0.01
I0523 05:36:42.198241 34682 solver.cpp:239] Iteration 57580 (2.93362 iter/s, 3.40876s/10 iters), loss = 8.19793
I0523 05:36:42.198519 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19793 (* 1 = 8.19793 loss)
I0523 05:36:42.974767 34682 sgd_solver.cpp:112] Iteration 57580, lr = 0.01
I0523 05:36:45.667984 34682 solver.cpp:239] Iteration 57590 (2.8824 iter/s, 3.46933s/10 iters), loss = 8.47445
I0523 05:36:45.668041 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47445 (* 1 = 8.47445 loss)
I0523 05:36:45.736217 34682 sgd_solver.cpp:112] Iteration 57590, lr = 0.01
I0523 05:36:50.368638 34682 solver.cpp:239] Iteration 57600 (2.12748 iter/s, 4.7004s/10 iters), loss = 8.1148
I0523 05:36:50.368688 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1148 (* 1 = 8.1148 loss)
I0523 05:36:51.224541 34682 sgd_solver.cpp:112] Iteration 57600, lr = 0.01
I0523 05:36:56.752393 34682 solver.cpp:239] Iteration 57610 (1.56655 iter/s, 6.38344s/10 iters), loss = 8.60731
I0523 05:36:56.752444 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60731 (* 1 = 8.60731 loss)
I0523 05:36:56.830195 34682 sgd_solver.cpp:112] Iteration 57610, lr = 0.01
I0523 05:37:01.146759 34682 solver.cpp:239] Iteration 57620 (2.27576 iter/s, 4.39413s/10 iters), loss = 7.40164
I0523 05:37:01.146803 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.40164 (* 1 = 7.40164 loss)
I0523 05:37:01.961627 34682 sgd_solver.cpp:112] Iteration 57620, lr = 0.01
I0523 05:37:05.348652 34682 solver.cpp:239] Iteration 57630 (2.38001 iter/s, 4.20167s/10 iters), loss = 8.07508
I0523 05:37:05.348711 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07508 (* 1 = 8.07508 loss)
I0523 05:37:05.413650 34682 sgd_solver.cpp:112] Iteration 57630, lr = 0.01
I0523 05:37:08.676661 34682 solver.cpp:239] Iteration 57640 (3.00498 iter/s, 3.32781s/10 iters), loss = 8.10071
I0523 05:37:08.676705 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10071 (* 1 = 8.10071 loss)
I0523 05:37:08.743880 34682 sgd_solver.cpp:112] Iteration 57640, lr = 0.01
I0523 05:37:13.075574 34682 solver.cpp:239] Iteration 57650 (2.27341 iter/s, 4.39868s/10 iters), loss = 7.61713
I0523 05:37:13.075788 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.61713 (* 1 = 7.61713 loss)
I0523 05:37:13.151108 34682 sgd_solver.cpp:112] Iteration 57650, lr = 0.01
I0523 05:37:19.664633 34682 solver.cpp:239] Iteration 57660 (1.51777 iter/s, 6.58861s/10 iters), loss = 8.21655
I0523 05:37:19.664678 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21655 (* 1 = 8.21655 loss)
I0523 05:37:20.351516 34682 sgd_solver.cpp:112] Iteration 57660, lr = 0.01
I0523 05:37:23.643496 34682 solver.cpp:239] Iteration 57670 (2.51342 iter/s, 3.97865s/10 iters), loss = 8.47105
I0523 05:37:23.643540 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47105 (* 1 = 8.47105 loss)
I0523 05:37:23.710716 34682 sgd_solver.cpp:112] Iteration 57670, lr = 0.01
I0523 05:37:26.815695 34682 solver.cpp:239] Iteration 57680 (3.15264 iter/s, 3.17194s/10 iters), loss = 8.83967
I0523 05:37:26.815786 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83967 (* 1 = 8.83967 loss)
I0523 05:37:26.874254 34682 sgd_solver.cpp:112] Iteration 57680, lr = 0.01
I0523 05:37:31.881456 34682 solver.cpp:239] Iteration 57690 (1.97415 iter/s, 5.06548s/10 iters), loss = 7.81412
I0523 05:37:31.881507 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81412 (* 1 = 7.81412 loss)
I0523 05:37:31.945334 34682 sgd_solver.cpp:112] Iteration 57690, lr = 0.01
I0523 05:37:35.223783 34682 solver.cpp:239] Iteration 57700 (2.9921 iter/s, 3.34214s/10 iters), loss = 8.05805
I0523 05:37:35.223835 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05805 (* 1 = 8.05805 loss)
I0523 05:37:35.304108 34682 sgd_solver.cpp:112] Iteration 57700, lr = 0.01
I0523 05:37:38.758093 34682 solver.cpp:239] Iteration 57710 (2.82957 iter/s, 3.53411s/10 iters), loss = 8.88268
I0523 05:37:38.758137 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88268 (* 1 = 8.88268 loss)
I0523 05:37:38.830014 34682 sgd_solver.cpp:112] Iteration 57710, lr = 0.01
I0523 05:37:44.452296 34682 solver.cpp:239] Iteration 57720 (1.75626 iter/s, 5.69393s/10 iters), loss = 8.50395
I0523 05:37:44.452556 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50395 (* 1 = 8.50395 loss)
I0523 05:37:44.513824 34682 sgd_solver.cpp:112] Iteration 57720, lr = 0.01
I0523 05:37:49.493188 34682 solver.cpp:239] Iteration 57730 (1.98395 iter/s, 5.04045s/10 iters), loss = 8.94693
I0523 05:37:49.493247 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94693 (* 1 = 8.94693 loss)
I0523 05:37:50.206372 34682 sgd_solver.cpp:112] Iteration 57730, lr = 0.01
I0523 05:37:53.575057 34682 solver.cpp:239] Iteration 57740 (2.44999 iter/s, 4.08164s/10 iters), loss = 8.31631
I0523 05:37:53.575111 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31631 (* 1 = 8.31631 loss)
I0523 05:37:54.416294 34682 sgd_solver.cpp:112] Iteration 57740, lr = 0.01
I0523 05:37:59.323338 34682 solver.cpp:239] Iteration 57750 (1.73974 iter/s, 5.74798s/10 iters), loss = 8.44561
I0523 05:37:59.323405 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44561 (* 1 = 8.44561 loss)
I0523 05:37:59.374573 34682 sgd_solver.cpp:112] Iteration 57750, lr = 0.01
I0523 05:38:03.185817 34682 solver.cpp:239] Iteration 57760 (2.58916 iter/s, 3.86225s/10 iters), loss = 8.6214
I0523 05:38:03.185868 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6214 (* 1 = 8.6214 loss)
I0523 05:38:03.834378 34682 sgd_solver.cpp:112] Iteration 57760, lr = 0.01
I0523 05:38:07.224452 34682 solver.cpp:239] Iteration 57770 (2.47622 iter/s, 4.03841s/10 iters), loss = 8.57537
I0523 05:38:07.224498 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57537 (* 1 = 8.57537 loss)
I0523 05:38:07.279755 34682 sgd_solver.cpp:112] Iteration 57770, lr = 0.01
I0523 05:38:13.085650 34682 solver.cpp:239] Iteration 57780 (1.70622 iter/s, 5.86091s/10 iters), loss = 9.25045
I0523 05:38:13.085698 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25045 (* 1 = 9.25045 loss)
I0523 05:38:13.146680 34682 sgd_solver.cpp:112] Iteration 57780, lr = 0.01
I0523 05:38:17.097921 34682 solver.cpp:239] Iteration 57790 (2.49248 iter/s, 4.01206s/10 iters), loss = 8.84543
I0523 05:38:17.098104 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84543 (* 1 = 8.84543 loss)
I0523 05:38:17.172197 34682 sgd_solver.cpp:112] Iteration 57790, lr = 0.01
I0523 05:38:22.051934 34682 solver.cpp:239] Iteration 57800 (2.01872 iter/s, 4.95363s/10 iters), loss = 8.47994
I0523 05:38:22.051980 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47994 (* 1 = 8.47994 loss)
I0523 05:38:22.110868 34682 sgd_solver.cpp:112] Iteration 57800, lr = 0.01
I0523 05:38:28.605402 34682 solver.cpp:239] Iteration 57810 (1.52598 iter/s, 6.55315s/10 iters), loss = 7.98108
I0523 05:38:28.605450 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98108 (* 1 = 7.98108 loss)
I0523 05:38:29.406852 34682 sgd_solver.cpp:112] Iteration 57810, lr = 0.01
I0523 05:38:34.755554 34682 solver.cpp:239] Iteration 57820 (1.62606 iter/s, 6.14985s/10 iters), loss = 7.9344
I0523 05:38:34.755615 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9344 (* 1 = 7.9344 loss)
I0523 05:38:35.286665 34682 sgd_solver.cpp:112] Iteration 57820, lr = 0.01
I0523 05:38:41.166878 34682 solver.cpp:239] Iteration 57830 (1.55982 iter/s, 6.41101s/10 iters), loss = 7.77011
I0523 05:38:41.166919 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77011 (* 1 = 7.77011 loss)
I0523 05:38:41.244066 34682 sgd_solver.cpp:112] Iteration 57830, lr = 0.01
I0523 05:38:44.598378 34682 solver.cpp:239] Iteration 57840 (2.91433 iter/s, 3.43132s/10 iters), loss = 7.86128
I0523 05:38:44.598420 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86128 (* 1 = 7.86128 loss)
I0523 05:38:44.668231 34682 sgd_solver.cpp:112] Iteration 57840, lr = 0.01
I0523 05:38:47.960085 34682 solver.cpp:239] Iteration 57850 (2.97484 iter/s, 3.36152s/10 iters), loss = 8.95428
I0523 05:38:47.960295 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95428 (* 1 = 8.95428 loss)
I0523 05:38:48.049224 34682 sgd_solver.cpp:112] Iteration 57850, lr = 0.01
I0523 05:38:51.280014 34682 solver.cpp:239] Iteration 57860 (3.01242 iter/s, 3.31959s/10 iters), loss = 7.21971
I0523 05:38:51.280059 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.21971 (* 1 = 7.21971 loss)
I0523 05:38:51.341135 34682 sgd_solver.cpp:112] Iteration 57860, lr = 0.01
I0523 05:38:56.432641 34682 solver.cpp:239] Iteration 57870 (1.94086 iter/s, 5.15237s/10 iters), loss = 7.39829
I0523 05:38:56.432693 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.39829 (* 1 = 7.39829 loss)
I0523 05:38:57.250932 34682 sgd_solver.cpp:112] Iteration 57870, lr = 0.01
I0523 05:39:03.425196 34682 solver.cpp:239] Iteration 57880 (1.43016 iter/s, 6.99222s/10 iters), loss = 7.96836
I0523 05:39:03.425253 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96836 (* 1 = 7.96836 loss)
I0523 05:39:03.490097 34682 sgd_solver.cpp:112] Iteration 57880, lr = 0.01
I0523 05:39:08.055614 34682 solver.cpp:239] Iteration 57890 (2.15975 iter/s, 4.63017s/10 iters), loss = 9.11779
I0523 05:39:08.055660 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11779 (* 1 = 9.11779 loss)
I0523 05:39:08.147070 34682 sgd_solver.cpp:112] Iteration 57890, lr = 0.01
I0523 05:39:13.999775 34682 solver.cpp:239] Iteration 57900 (1.68241 iter/s, 5.94385s/10 iters), loss = 7.61093
I0523 05:39:13.999845 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.61093 (* 1 = 7.61093 loss)
I0523 05:39:14.801041 34682 sgd_solver.cpp:112] Iteration 57900, lr = 0.01
I0523 05:39:18.689503 34682 solver.cpp:239] Iteration 57910 (2.13243 iter/s, 4.68948s/10 iters), loss = 9.17104
I0523 05:39:18.689688 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17104 (* 1 = 9.17104 loss)
I0523 05:39:19.555964 34682 sgd_solver.cpp:112] Iteration 57910, lr = 0.01
I0523 05:39:24.398561 34682 solver.cpp:239] Iteration 57920 (1.75173 iter/s, 5.70866s/10 iters), loss = 8.53467
I0523 05:39:24.398612 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53467 (* 1 = 8.53467 loss)
I0523 05:39:24.460582 34682 sgd_solver.cpp:112] Iteration 57920, lr = 0.01
I0523 05:39:29.800302 34682 solver.cpp:239] Iteration 57930 (1.85135 iter/s, 5.40146s/10 iters), loss = 8.21494
I0523 05:39:29.800367 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21494 (* 1 = 8.21494 loss)
I0523 05:39:29.973201 34682 sgd_solver.cpp:112] Iteration 57930, lr = 0.01
I0523 05:39:33.375859 34682 solver.cpp:239] Iteration 57940 (2.79694 iter/s, 3.57534s/10 iters), loss = 8.37934
I0523 05:39:33.375919 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37934 (* 1 = 8.37934 loss)
I0523 05:39:33.433899 34682 sgd_solver.cpp:112] Iteration 57940, lr = 0.01
I0523 05:39:40.129912 34682 solver.cpp:239] Iteration 57950 (1.48066 iter/s, 6.75372s/10 iters), loss = 8.59874
I0523 05:39:40.129957 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59874 (* 1 = 8.59874 loss)
I0523 05:39:40.911439 34682 sgd_solver.cpp:112] Iteration 57950, lr = 0.01
I0523 05:39:44.511694 34682 solver.cpp:239] Iteration 57960 (2.28229 iter/s, 4.38155s/10 iters), loss = 8.00635
I0523 05:39:44.511749 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00635 (* 1 = 8.00635 loss)
I0523 05:39:45.337707 34682 sgd_solver.cpp:112] Iteration 57960, lr = 0.01
I0523 05:39:49.419288 34682 solver.cpp:239] Iteration 57970 (2.03776 iter/s, 4.90734s/10 iters), loss = 8.30317
I0523 05:39:49.419595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30317 (* 1 = 8.30317 loss)
I0523 05:39:49.484118 34682 sgd_solver.cpp:112] Iteration 57970, lr = 0.01
I0523 05:39:53.459827 34682 solver.cpp:239] Iteration 57980 (2.47644 iter/s, 4.03805s/10 iters), loss = 7.92124
I0523 05:39:53.459900 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92124 (* 1 = 7.92124 loss)
I0523 05:39:53.516734 34682 sgd_solver.cpp:112] Iteration 57980, lr = 0.01
I0523 05:39:57.663552 34682 solver.cpp:239] Iteration 57990 (2.37898 iter/s, 4.20348s/10 iters), loss = 7.86827
I0523 05:39:57.663600 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86827 (* 1 = 7.86827 loss)
I0523 05:39:57.724158 34682 sgd_solver.cpp:112] Iteration 57990, lr = 0.01
I0523 05:40:01.686344 34682 solver.cpp:239] Iteration 58000 (2.48598 iter/s, 4.02256s/10 iters), loss = 7.91614
I0523 05:40:01.686404 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91614 (* 1 = 7.91614 loss)
I0523 05:40:01.745698 34682 sgd_solver.cpp:112] Iteration 58000, lr = 0.01
I0523 05:40:04.268739 34682 solver.cpp:239] Iteration 58010 (3.87263 iter/s, 2.58223s/10 iters), loss = 6.74887
I0523 05:40:04.268800 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.74887 (* 1 = 6.74887 loss)
I0523 05:40:04.323693 34682 sgd_solver.cpp:112] Iteration 58010, lr = 0.01
I0523 05:40:08.607702 34682 solver.cpp:239] Iteration 58020 (2.30482 iter/s, 4.33873s/10 iters), loss = 8.28438
I0523 05:40:08.607748 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28438 (* 1 = 8.28438 loss)
I0523 05:40:09.476464 34682 sgd_solver.cpp:112] Iteration 58020, lr = 0.01
I0523 05:40:12.190506 34682 solver.cpp:239] Iteration 58030 (2.79128 iter/s, 3.58259s/10 iters), loss = 8.42455
I0523 05:40:12.190568 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42455 (* 1 = 8.42455 loss)
I0523 05:40:13.020294 34682 sgd_solver.cpp:112] Iteration 58030, lr = 0.01
I0523 05:40:16.151273 34682 solver.cpp:239] Iteration 58040 (2.52491 iter/s, 3.96054s/10 iters), loss = 8.40296
I0523 05:40:16.151324 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40296 (* 1 = 8.40296 loss)
I0523 05:40:16.192358 34682 sgd_solver.cpp:112] Iteration 58040, lr = 0.01
I0523 05:40:19.913961 34682 solver.cpp:239] Iteration 58050 (2.65782 iter/s, 3.76248s/10 iters), loss = 9.16735
I0523 05:40:19.914093 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.16735 (* 1 = 9.16735 loss)
I0523 05:40:19.981954 34682 sgd_solver.cpp:112] Iteration 58050, lr = 0.01
I0523 05:40:25.695806 34682 solver.cpp:239] Iteration 58060 (1.72966 iter/s, 5.78148s/10 iters), loss = 8.51719
I0523 05:40:25.695853 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51719 (* 1 = 8.51719 loss)
I0523 05:40:25.772727 34682 sgd_solver.cpp:112] Iteration 58060, lr = 0.01
I0523 05:40:30.674405 34682 solver.cpp:239] Iteration 58070 (2.0087 iter/s, 4.97835s/10 iters), loss = 7.66716
I0523 05:40:30.674450 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66716 (* 1 = 7.66716 loss)
I0523 05:40:31.415073 34682 sgd_solver.cpp:112] Iteration 58070, lr = 0.01
I0523 05:40:36.054324 34682 solver.cpp:239] Iteration 58080 (1.85886 iter/s, 5.37965s/10 iters), loss = 8.06231
I0523 05:40:36.054366 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06231 (* 1 = 8.06231 loss)
I0523 05:40:36.126019 34682 sgd_solver.cpp:112] Iteration 58080, lr = 0.01
I0523 05:40:38.454670 34682 solver.cpp:239] Iteration 58090 (4.16631 iter/s, 2.4002s/10 iters), loss = 8.96221
I0523 05:40:38.454725 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96221 (* 1 = 8.96221 loss)
I0523 05:40:39.328038 34682 sgd_solver.cpp:112] Iteration 58090, lr = 0.01
I0523 05:40:44.105381 34682 solver.cpp:239] Iteration 58100 (1.76978 iter/s, 5.65042s/10 iters), loss = 7.25774
I0523 05:40:44.105434 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.25774 (* 1 = 7.25774 loss)
I0523 05:40:44.181071 34682 sgd_solver.cpp:112] Iteration 58100, lr = 0.01
I0523 05:40:48.349467 34682 solver.cpp:239] Iteration 58110 (2.35635 iter/s, 4.24386s/10 iters), loss = 7.87558
I0523 05:40:48.349519 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87558 (* 1 = 7.87558 loss)
I0523 05:40:49.134886 34682 sgd_solver.cpp:112] Iteration 58110, lr = 0.01
I0523 05:40:53.013371 34682 solver.cpp:239] Iteration 58120 (2.14424 iter/s, 4.66366s/10 iters), loss = 7.6717
I0523 05:40:53.013655 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6717 (* 1 = 7.6717 loss)
I0523 05:40:53.855829 34682 sgd_solver.cpp:112] Iteration 58120, lr = 0.01
I0523 05:40:58.637048 34682 solver.cpp:239] Iteration 58130 (1.77836 iter/s, 5.62317s/10 iters), loss = 7.7393
I0523 05:40:58.637120 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7393 (* 1 = 7.7393 loss)
I0523 05:40:59.462378 34682 sgd_solver.cpp:112] Iteration 58130, lr = 0.01
I0523 05:41:04.369362 34682 solver.cpp:239] Iteration 58140 (1.74459 iter/s, 5.73201s/10 iters), loss = 8.68395
I0523 05:41:04.369422 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68395 (* 1 = 8.68395 loss)
I0523 05:41:05.242110 34682 sgd_solver.cpp:112] Iteration 58140, lr = 0.01
I0523 05:41:09.381031 34682 solver.cpp:239] Iteration 58150 (1.99545 iter/s, 5.01141s/10 iters), loss = 9.57097
I0523 05:41:09.381081 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.57097 (* 1 = 9.57097 loss)
I0523 05:41:09.443627 34682 sgd_solver.cpp:112] Iteration 58150, lr = 0.01
I0523 05:41:14.783131 34682 solver.cpp:239] Iteration 58160 (1.85123 iter/s, 5.40183s/10 iters), loss = 7.7216
I0523 05:41:14.783190 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7216 (* 1 = 7.7216 loss)
I0523 05:41:14.858609 34682 sgd_solver.cpp:112] Iteration 58160, lr = 0.01
I0523 05:41:18.210177 34682 solver.cpp:239] Iteration 58170 (2.91814 iter/s, 3.42684s/10 iters), loss = 8.33295
I0523 05:41:18.210249 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33295 (* 1 = 8.33295 loss)
I0523 05:41:18.278885 34682 sgd_solver.cpp:112] Iteration 58170, lr = 0.01
I0523 05:41:23.247478 34682 solver.cpp:239] Iteration 58180 (1.9853 iter/s, 5.03702s/10 iters), loss = 8.08624
I0523 05:41:23.247702 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08624 (* 1 = 8.08624 loss)
I0523 05:41:24.104697 34682 sgd_solver.cpp:112] Iteration 58180, lr = 0.01
I0523 05:41:28.232924 34682 solver.cpp:239] Iteration 58190 (2.006 iter/s, 4.98505s/10 iters), loss = 9.1124
I0523 05:41:28.232967 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1124 (* 1 = 9.1124 loss)
I0523 05:41:28.308302 34682 sgd_solver.cpp:112] Iteration 58190, lr = 0.01
I0523 05:41:33.202365 34682 solver.cpp:239] Iteration 58200 (2.01241 iter/s, 4.96916s/10 iters), loss = 8.95287
I0523 05:41:33.202436 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95287 (* 1 = 8.95287 loss)
I0523 05:41:34.017004 34682 sgd_solver.cpp:112] Iteration 58200, lr = 0.01
I0523 05:41:38.148496 34682 solver.cpp:239] Iteration 58210 (2.02189 iter/s, 4.94586s/10 iters), loss = 9.29952
I0523 05:41:38.148540 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.29952 (* 1 = 9.29952 loss)
I0523 05:41:38.231495 34682 sgd_solver.cpp:112] Iteration 58210, lr = 0.01
I0523 05:41:42.605921 34682 solver.cpp:239] Iteration 58220 (2.24357 iter/s, 4.45718s/10 iters), loss = 9.00683
I0523 05:41:42.605973 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00683 (* 1 = 9.00683 loss)
I0523 05:41:43.394431 34682 sgd_solver.cpp:112] Iteration 58220, lr = 0.01
I0523 05:41:46.892066 34682 solver.cpp:239] Iteration 58230 (2.33325 iter/s, 4.28588s/10 iters), loss = 8.43253
I0523 05:41:46.892124 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43253 (* 1 = 8.43253 loss)
I0523 05:41:46.966038 34682 sgd_solver.cpp:112] Iteration 58230, lr = 0.01
I0523 05:41:52.617040 34682 solver.cpp:239] Iteration 58240 (1.74682 iter/s, 5.72468s/10 iters), loss = 7.49424
I0523 05:41:52.617094 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.49424 (* 1 = 7.49424 loss)
I0523 05:41:53.438081 34682 sgd_solver.cpp:112] Iteration 58240, lr = 0.01
I0523 05:41:58.063264 34682 solver.cpp:239] Iteration 58250 (1.83623 iter/s, 5.44595s/10 iters), loss = 7.63488
I0523 05:41:58.063308 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63488 (* 1 = 7.63488 loss)
I0523 05:41:58.136015 34682 sgd_solver.cpp:112] Iteration 58250, lr = 0.01
I0523 05:42:03.627826 34682 solver.cpp:239] Iteration 58260 (1.7986 iter/s, 5.55988s/10 iters), loss = 8.04708
I0523 05:42:03.627869 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04708 (* 1 = 8.04708 loss)
I0523 05:42:04.505704 34682 sgd_solver.cpp:112] Iteration 58260, lr = 0.01
I0523 05:42:09.694267 34682 solver.cpp:239] Iteration 58270 (1.64849 iter/s, 6.06615s/10 iters), loss = 7.8139
I0523 05:42:09.694317 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8139 (* 1 = 7.8139 loss)
I0523 05:42:10.026891 34682 sgd_solver.cpp:112] Iteration 58270, lr = 0.01
I0523 05:42:15.525892 34682 solver.cpp:239] Iteration 58280 (1.71488 iter/s, 5.83132s/10 iters), loss = 7.86359
I0523 05:42:15.525938 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86359 (* 1 = 7.86359 loss)
I0523 05:42:15.585748 34682 sgd_solver.cpp:112] Iteration 58280, lr = 0.01
I0523 05:42:19.644786 34682 solver.cpp:239] Iteration 58290 (2.42796 iter/s, 4.11868s/10 iters), loss = 8.22686
I0523 05:42:19.644827 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22686 (* 1 = 8.22686 loss)
I0523 05:42:19.704130 34682 sgd_solver.cpp:112] Iteration 58290, lr = 0.01
I0523 05:42:25.809578 34682 solver.cpp:239] Iteration 58300 (1.62219 iter/s, 6.1645s/10 iters), loss = 8.66316
I0523 05:42:25.809751 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66316 (* 1 = 8.66316 loss)
I0523 05:42:25.886813 34682 sgd_solver.cpp:112] Iteration 58300, lr = 0.01
I0523 05:42:30.104193 34682 solver.cpp:239] Iteration 58310 (2.32867 iter/s, 4.29429s/10 iters), loss = 8.06679
I0523 05:42:30.104259 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06679 (* 1 = 8.06679 loss)
I0523 05:42:30.317459 34682 sgd_solver.cpp:112] Iteration 58310, lr = 0.01
I0523 05:42:34.249311 34682 solver.cpp:239] Iteration 58320 (2.41262 iter/s, 4.14488s/10 iters), loss = 7.9496
I0523 05:42:34.249374 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9496 (* 1 = 7.9496 loss)
I0523 05:42:35.064258 34682 sgd_solver.cpp:112] Iteration 58320, lr = 0.01
I0523 05:42:38.316963 34682 solver.cpp:239] Iteration 58330 (2.45856 iter/s, 4.06742s/10 iters), loss = 8.25968
I0523 05:42:38.317009 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25968 (* 1 = 8.25968 loss)
I0523 05:42:38.379531 34682 sgd_solver.cpp:112] Iteration 58330, lr = 0.01
I0523 05:42:41.914124 34682 solver.cpp:239] Iteration 58340 (2.78012 iter/s, 3.59696s/10 iters), loss = 7.81818
I0523 05:42:41.914163 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81818 (* 1 = 7.81818 loss)
I0523 05:42:41.991438 34682 sgd_solver.cpp:112] Iteration 58340, lr = 0.01
I0523 05:42:45.891707 34682 solver.cpp:239] Iteration 58350 (2.51422 iter/s, 3.97737s/10 iters), loss = 7.8159
I0523 05:42:45.891753 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8159 (* 1 = 7.8159 loss)
I0523 05:42:45.968608 34682 sgd_solver.cpp:112] Iteration 58350, lr = 0.01
I0523 05:42:49.442354 34682 solver.cpp:239] Iteration 58360 (2.81655 iter/s, 3.55044s/10 iters), loss = 7.64764
I0523 05:42:49.442418 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64764 (* 1 = 7.64764 loss)
I0523 05:42:49.506209 34682 sgd_solver.cpp:112] Iteration 58360, lr = 0.01
I0523 05:42:54.900303 34682 solver.cpp:239] Iteration 58370 (1.83228 iter/s, 5.45767s/10 iters), loss = 8.18483
I0523 05:42:54.900346 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18483 (* 1 = 8.18483 loss)
I0523 05:42:54.968900 34682 sgd_solver.cpp:112] Iteration 58370, lr = 0.01
I0523 05:42:59.075583 34682 solver.cpp:239] Iteration 58380 (2.39517 iter/s, 4.17506s/10 iters), loss = 7.55661
I0523 05:42:59.075853 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.55661 (* 1 = 7.55661 loss)
I0523 05:42:59.945387 34682 sgd_solver.cpp:112] Iteration 58380, lr = 0.01
I0523 05:43:05.439281 34682 solver.cpp:239] Iteration 58390 (1.57154 iter/s, 6.36319s/10 iters), loss = 8.81685
I0523 05:43:05.439327 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81685 (* 1 = 8.81685 loss)
I0523 05:43:05.515863 34682 sgd_solver.cpp:112] Iteration 58390, lr = 0.01
I0523 05:43:09.664839 34682 solver.cpp:239] Iteration 58400 (2.36667 iter/s, 4.22534s/10 iters), loss = 7.84732
I0523 05:43:09.664885 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84732 (* 1 = 7.84732 loss)
I0523 05:43:09.728940 34682 sgd_solver.cpp:112] Iteration 58400, lr = 0.01
I0523 05:43:15.936115 34682 solver.cpp:239] Iteration 58410 (1.59465 iter/s, 6.27098s/10 iters), loss = 8.13481
I0523 05:43:15.936164 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13481 (* 1 = 8.13481 loss)
I0523 05:43:16.655645 34682 sgd_solver.cpp:112] Iteration 58410, lr = 0.01
I0523 05:43:20.320600 34682 solver.cpp:239] Iteration 58420 (2.2809 iter/s, 4.38424s/10 iters), loss = 7.95252
I0523 05:43:20.320659 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95252 (* 1 = 7.95252 loss)
I0523 05:43:20.382223 34682 sgd_solver.cpp:112] Iteration 58420, lr = 0.01
I0523 05:43:25.193189 34682 solver.cpp:239] Iteration 58430 (2.0524 iter/s, 4.87233s/10 iters), loss = 8.69096
I0523 05:43:25.193230 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69096 (* 1 = 8.69096 loss)
I0523 05:43:25.954181 34682 sgd_solver.cpp:112] Iteration 58430, lr = 0.01
I0523 05:43:28.928812 34682 solver.cpp:239] Iteration 58440 (2.67707 iter/s, 3.73542s/10 iters), loss = 8.42337
I0523 05:43:28.928860 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42337 (* 1 = 8.42337 loss)
I0523 05:43:29.785117 34682 sgd_solver.cpp:112] Iteration 58440, lr = 0.01
I0523 05:43:33.765205 34682 solver.cpp:239] Iteration 58450 (2.06776 iter/s, 4.83615s/10 iters), loss = 8.97879
I0523 05:43:33.765264 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97879 (* 1 = 8.97879 loss)
I0523 05:43:33.831226 34682 sgd_solver.cpp:112] Iteration 58450, lr = 0.01
I0523 05:43:37.891320 34682 solver.cpp:239] Iteration 58460 (2.42372 iter/s, 4.12589s/10 iters), loss = 8.09187
I0523 05:43:37.891391 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09187 (* 1 = 8.09187 loss)
I0523 05:43:38.690570 34682 sgd_solver.cpp:112] Iteration 58460, lr = 0.01
I0523 05:43:42.005421 34682 solver.cpp:239] Iteration 58470 (2.43081 iter/s, 4.11386s/10 iters), loss = 7.27159
I0523 05:43:42.005471 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.27159 (* 1 = 7.27159 loss)
I0523 05:43:42.858721 34682 sgd_solver.cpp:112] Iteration 58470, lr = 0.01
I0523 05:43:47.060504 34682 solver.cpp:239] Iteration 58480 (1.97831 iter/s, 5.05482s/10 iters), loss = 9.07774
I0523 05:43:47.060586 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07774 (* 1 = 9.07774 loss)
I0523 05:43:47.844964 34682 sgd_solver.cpp:112] Iteration 58480, lr = 0.01
I0523 05:43:52.856010 34682 solver.cpp:239] Iteration 58490 (1.72558 iter/s, 5.79517s/10 iters), loss = 8.63894
I0523 05:43:52.856096 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63894 (* 1 = 8.63894 loss)
I0523 05:43:53.598628 34682 sgd_solver.cpp:112] Iteration 58490, lr = 0.01
I0523 05:43:58.489581 34682 solver.cpp:239] Iteration 58500 (1.77517 iter/s, 5.63326s/10 iters), loss = 8.02532
I0523 05:43:58.489625 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02532 (* 1 = 8.02532 loss)
I0523 05:43:58.558218 34682 sgd_solver.cpp:112] Iteration 58500, lr = 0.01
I0523 05:44:02.692513 34682 solver.cpp:239] Iteration 58510 (2.37942 iter/s, 4.2027s/10 iters), loss = 8.35979
I0523 05:44:02.692812 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35979 (* 1 = 8.35979 loss)
I0523 05:44:03.282130 34682 sgd_solver.cpp:112] Iteration 58510, lr = 0.01
I0523 05:44:07.385716 34682 solver.cpp:239] Iteration 58520 (2.13095 iter/s, 4.69274s/10 iters), loss = 9.11837
I0523 05:44:07.385769 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11837 (* 1 = 9.11837 loss)
I0523 05:44:08.066107 34682 sgd_solver.cpp:112] Iteration 58520, lr = 0.01
I0523 05:44:12.713910 34682 solver.cpp:239] Iteration 58530 (1.87691 iter/s, 5.32792s/10 iters), loss = 7.74623
I0523 05:44:12.713997 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74623 (* 1 = 7.74623 loss)
I0523 05:44:13.452808 34682 sgd_solver.cpp:112] Iteration 58530, lr = 0.01
I0523 05:44:17.818987 34682 solver.cpp:239] Iteration 58540 (1.95895 iter/s, 5.10479s/10 iters), loss = 8.388
I0523 05:44:17.819032 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.388 (* 1 = 8.388 loss)
I0523 05:44:17.893728 34682 sgd_solver.cpp:112] Iteration 58540, lr = 0.01
I0523 05:44:22.673655 34682 solver.cpp:239] Iteration 58550 (2.05998 iter/s, 4.85442s/10 iters), loss = 9.01115
I0523 05:44:22.673701 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01115 (* 1 = 9.01115 loss)
I0523 05:44:22.735162 34682 sgd_solver.cpp:112] Iteration 58550, lr = 0.01
I0523 05:44:28.933131 34682 solver.cpp:239] Iteration 58560 (1.59765 iter/s, 6.25918s/10 iters), loss = 8.4267
I0523 05:44:28.933177 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4267 (* 1 = 8.4267 loss)
I0523 05:44:28.998752 34682 sgd_solver.cpp:112] Iteration 58560, lr = 0.01
I0523 05:44:33.780014 34682 solver.cpp:239] Iteration 58570 (2.06329 iter/s, 4.84663s/10 iters), loss = 8.41039
I0523 05:44:33.780200 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41039 (* 1 = 8.41039 loss)
I0523 05:44:34.636265 34682 sgd_solver.cpp:112] Iteration 58570, lr = 0.01
I0523 05:44:37.725430 34682 solver.cpp:239] Iteration 58580 (2.53481 iter/s, 3.94506s/10 iters), loss = 8.95911
I0523 05:44:37.725499 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95911 (* 1 = 8.95911 loss)
I0523 05:44:37.808307 34682 sgd_solver.cpp:112] Iteration 58580, lr = 0.01
I0523 05:44:43.018388 34682 solver.cpp:239] Iteration 58590 (1.8894 iter/s, 5.29267s/10 iters), loss = 8.65845
I0523 05:44:43.018430 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65845 (* 1 = 8.65845 loss)
I0523 05:44:43.098166 34682 sgd_solver.cpp:112] Iteration 58590, lr = 0.01
I0523 05:44:47.014242 34682 solver.cpp:239] Iteration 58600 (2.50273 iter/s, 3.99564s/10 iters), loss = 8.21456
I0523 05:44:47.014310 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21456 (* 1 = 8.21456 loss)
I0523 05:44:47.078589 34682 sgd_solver.cpp:112] Iteration 58600, lr = 0.01
I0523 05:44:51.551967 34682 solver.cpp:239] Iteration 58610 (2.20387 iter/s, 4.53747s/10 iters), loss = 8.20973
I0523 05:44:51.552008 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20973 (* 1 = 8.20973 loss)
I0523 05:44:51.625169 34682 sgd_solver.cpp:112] Iteration 58610, lr = 0.01
I0523 05:44:56.343821 34682 solver.cpp:239] Iteration 58620 (2.08698 iter/s, 4.79161s/10 iters), loss = 9.73181
I0523 05:44:56.343864 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.73181 (* 1 = 9.73181 loss)
I0523 05:44:56.412055 34682 sgd_solver.cpp:112] Iteration 58620, lr = 0.01
I0523 05:45:01.880767 34682 solver.cpp:239] Iteration 58630 (1.80614 iter/s, 5.53668s/10 iters), loss = 8.60426
I0523 05:45:01.880817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60426 (* 1 = 8.60426 loss)
I0523 05:45:02.736155 34682 sgd_solver.cpp:112] Iteration 58630, lr = 0.01
I0523 05:45:06.933187 34682 solver.cpp:239] Iteration 58640 (1.97935 iter/s, 5.05216s/10 iters), loss = 8.33041
I0523 05:45:06.933495 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33041 (* 1 = 8.33041 loss)
I0523 05:45:06.995592 34682 sgd_solver.cpp:112] Iteration 58640, lr = 0.01
I0523 05:45:09.281044 34682 solver.cpp:239] Iteration 58650 (4.25988 iter/s, 2.34749s/10 iters), loss = 7.70155
I0523 05:45:09.281101 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.70155 (* 1 = 7.70155 loss)
I0523 05:45:10.135864 34682 sgd_solver.cpp:112] Iteration 58650, lr = 0.01
I0523 05:45:14.547317 34682 solver.cpp:239] Iteration 58660 (1.89898 iter/s, 5.26599s/10 iters), loss = 7.38411
I0523 05:45:14.547375 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.38411 (* 1 = 7.38411 loss)
I0523 05:45:15.407027 34682 sgd_solver.cpp:112] Iteration 58660, lr = 0.01
I0523 05:45:19.294390 34682 solver.cpp:239] Iteration 58670 (2.10667 iter/s, 4.74682s/10 iters), loss = 8.34323
I0523 05:45:19.294430 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34323 (* 1 = 8.34323 loss)
I0523 05:45:19.371577 34682 sgd_solver.cpp:112] Iteration 58670, lr = 0.01
I0523 05:45:25.100230 34682 solver.cpp:239] Iteration 58680 (1.72249 iter/s, 5.80556s/10 iters), loss = 8.5179
I0523 05:45:25.100284 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5179 (* 1 = 8.5179 loss)
I0523 05:45:25.951182 34682 sgd_solver.cpp:112] Iteration 58680, lr = 0.01
I0523 05:45:31.310572 34682 solver.cpp:239] Iteration 58690 (1.6103 iter/s, 6.21004s/10 iters), loss = 8.17032
I0523 05:45:31.310622 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17032 (* 1 = 8.17032 loss)
I0523 05:45:32.017120 34682 sgd_solver.cpp:112] Iteration 58690, lr = 0.01
I0523 05:45:37.393034 34682 solver.cpp:239] Iteration 58700 (1.64415 iter/s, 6.08216s/10 iters), loss = 7.86631
I0523 05:45:37.393252 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86631 (* 1 = 7.86631 loss)
I0523 05:45:37.467911 34682 sgd_solver.cpp:112] Iteration 58700, lr = 0.01
I0523 05:45:41.612774 34682 solver.cpp:239] Iteration 58710 (2.37002 iter/s, 4.21937s/10 iters), loss = 8.48798
I0523 05:45:41.612823 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48798 (* 1 = 8.48798 loss)
I0523 05:45:42.365077 34682 sgd_solver.cpp:112] Iteration 58710, lr = 0.01
I0523 05:45:46.300598 34682 solver.cpp:239] Iteration 58720 (2.1333 iter/s, 4.68758s/10 iters), loss = 8.92956
I0523 05:45:46.300655 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92956 (* 1 = 8.92956 loss)
I0523 05:45:47.193429 34682 sgd_solver.cpp:112] Iteration 58720, lr = 0.01
I0523 05:45:51.374958 34682 solver.cpp:239] Iteration 58730 (1.97079 iter/s, 5.0741s/10 iters), loss = 8.73722
I0523 05:45:51.375013 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73722 (* 1 = 8.73722 loss)
I0523 05:45:51.451375 34682 sgd_solver.cpp:112] Iteration 58730, lr = 0.01
I0523 05:45:56.022276 34682 solver.cpp:239] Iteration 58740 (2.1519 iter/s, 4.64706s/10 iters), loss = 8.45361
I0523 05:45:56.022326 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45361 (* 1 = 8.45361 loss)
I0523 05:45:56.095721 34682 sgd_solver.cpp:112] Iteration 58740, lr = 0.01
I0523 05:45:59.698468 34682 solver.cpp:239] Iteration 58750 (2.72036 iter/s, 3.67598s/10 iters), loss = 7.5065
I0523 05:45:59.698532 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5065 (* 1 = 7.5065 loss)
I0523 05:46:00.553495 34682 sgd_solver.cpp:112] Iteration 58750, lr = 0.01
I0523 05:46:05.715832 34682 solver.cpp:239] Iteration 58760 (1.66194 iter/s, 6.01705s/10 iters), loss = 8.64648
I0523 05:46:05.715876 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64648 (* 1 = 8.64648 loss)
I0523 05:46:05.786178 34682 sgd_solver.cpp:112] Iteration 58760, lr = 0.01
I0523 05:46:10.691857 34682 solver.cpp:239] Iteration 58770 (2.00974 iter/s, 4.97577s/10 iters), loss = 7.94525
I0523 05:46:10.692162 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94525 (* 1 = 7.94525 loss)
I0523 05:46:11.535962 34682 sgd_solver.cpp:112] Iteration 58770, lr = 0.01
I0523 05:46:15.743762 34682 solver.cpp:239] Iteration 58780 (1.97964 iter/s, 5.05142s/10 iters), loss = 7.54162
I0523 05:46:15.743825 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54162 (* 1 = 7.54162 loss)
I0523 05:46:15.822427 34682 sgd_solver.cpp:112] Iteration 58780, lr = 0.01
I0523 05:46:20.879654 34682 solver.cpp:239] Iteration 58790 (1.94718 iter/s, 5.13562s/10 iters), loss = 8.0592
I0523 05:46:20.879700 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0592 (* 1 = 8.0592 loss)
I0523 05:46:21.696549 34682 sgd_solver.cpp:112] Iteration 58790, lr = 0.01
I0523 05:46:27.297677 34682 solver.cpp:239] Iteration 58800 (1.55819 iter/s, 6.41772s/10 iters), loss = 8.18046
I0523 05:46:27.297732 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18046 (* 1 = 8.18046 loss)
I0523 05:46:27.375803 34682 sgd_solver.cpp:112] Iteration 58800, lr = 0.01
I0523 05:46:34.401283 34682 solver.cpp:239] Iteration 58810 (1.4078 iter/s, 7.10326s/10 iters), loss = 8.4079
I0523 05:46:34.401350 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4079 (* 1 = 8.4079 loss)
I0523 05:46:34.455483 34682 sgd_solver.cpp:112] Iteration 58810, lr = 0.01
I0523 05:46:37.841823 34682 solver.cpp:239] Iteration 58820 (2.9067 iter/s, 3.44033s/10 iters), loss = 8.45417
I0523 05:46:37.841886 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45417 (* 1 = 8.45417 loss)
I0523 05:46:37.920686 34682 sgd_solver.cpp:112] Iteration 58820, lr = 0.01
I0523 05:46:43.434419 34682 solver.cpp:239] Iteration 58830 (1.78817 iter/s, 5.5923s/10 iters), loss = 8.33256
I0523 05:46:43.434535 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33256 (* 1 = 8.33256 loss)
I0523 05:46:43.502892 34682 sgd_solver.cpp:112] Iteration 58830, lr = 0.01
I0523 05:46:48.096417 34682 solver.cpp:239] Iteration 58840 (2.14515 iter/s, 4.66169s/10 iters), loss = 8.23542
I0523 05:46:48.096493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23542 (* 1 = 8.23542 loss)
I0523 05:46:48.699578 34682 sgd_solver.cpp:112] Iteration 58840, lr = 0.01
I0523 05:46:52.267416 34682 solver.cpp:239] Iteration 58850 (2.39765 iter/s, 4.17075s/10 iters), loss = 9.54447
I0523 05:46:52.267469 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.54447 (* 1 = 9.54447 loss)
I0523 05:46:53.067967 34682 sgd_solver.cpp:112] Iteration 58850, lr = 0.01
I0523 05:46:58.270133 34682 solver.cpp:239] Iteration 58860 (1.666 iter/s, 6.00242s/10 iters), loss = 7.79947
I0523 05:46:58.270184 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79947 (* 1 = 7.79947 loss)
I0523 05:46:59.038424 34682 sgd_solver.cpp:112] Iteration 58860, lr = 0.01
I0523 05:47:03.076772 34682 solver.cpp:239] Iteration 58870 (2.08056 iter/s, 4.8064s/10 iters), loss = 8.40771
I0523 05:47:03.076817 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40771 (* 1 = 8.40771 loss)
I0523 05:47:03.907142 34682 sgd_solver.cpp:112] Iteration 58870, lr = 0.01
I0523 05:47:08.540467 34682 solver.cpp:239] Iteration 58880 (1.83035 iter/s, 5.46342s/10 iters), loss = 8.12861
I0523 05:47:08.540519 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12861 (* 1 = 8.12861 loss)
I0523 05:47:08.615555 34682 sgd_solver.cpp:112] Iteration 58880, lr = 0.01
I0523 05:47:12.641125 34682 solver.cpp:239] Iteration 58890 (2.43876 iter/s, 4.10044s/10 iters), loss = 9.23679
I0523 05:47:12.641173 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.23679 (* 1 = 9.23679 loss)
I0523 05:47:12.704105 34682 sgd_solver.cpp:112] Iteration 58890, lr = 0.01
I0523 05:47:16.623015 34682 solver.cpp:239] Iteration 58900 (2.51151 iter/s, 3.98166s/10 iters), loss = 8.05794
I0523 05:47:16.623160 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05794 (* 1 = 8.05794 loss)
I0523 05:47:16.692193 34682 sgd_solver.cpp:112] Iteration 58900, lr = 0.01
I0523 05:47:21.594715 34682 solver.cpp:239] Iteration 58910 (2.01154 iter/s, 4.97132s/10 iters), loss = 8.679
I0523 05:47:21.594789 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.679 (* 1 = 8.679 loss)
I0523 05:47:22.436229 34682 sgd_solver.cpp:112] Iteration 58910, lr = 0.01
I0523 05:47:28.348397 34682 solver.cpp:239] Iteration 58920 (1.48075 iter/s, 6.75334s/10 iters), loss = 9.22216
I0523 05:47:28.348440 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.22216 (* 1 = 9.22216 loss)
I0523 05:47:28.425565 34682 sgd_solver.cpp:112] Iteration 58920, lr = 0.01
I0523 05:47:34.061532 34682 solver.cpp:239] Iteration 58930 (1.75044 iter/s, 5.71286s/10 iters), loss = 7.69959
I0523 05:47:34.061589 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69959 (* 1 = 7.69959 loss)
I0523 05:47:34.122859 34682 sgd_solver.cpp:112] Iteration 58930, lr = 0.01
I0523 05:47:39.154989 34682 solver.cpp:239] Iteration 58940 (1.96341 iter/s, 5.09319s/10 iters), loss = 7.69847
I0523 05:47:39.155046 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69847 (* 1 = 7.69847 loss)
I0523 05:47:40.038216 34682 sgd_solver.cpp:112] Iteration 58940, lr = 0.01
I0523 05:47:45.928074 34682 solver.cpp:239] Iteration 58950 (1.47651 iter/s, 6.77274s/10 iters), loss = 8.53194
I0523 05:47:45.928160 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53194 (* 1 = 8.53194 loss)
I0523 05:47:46.686004 34682 sgd_solver.cpp:112] Iteration 58950, lr = 0.01
I0523 05:47:50.544795 34682 solver.cpp:239] Iteration 58960 (2.16616 iter/s, 4.61646s/10 iters), loss = 8.35199
I0523 05:47:50.544836 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35199 (* 1 = 8.35199 loss)
I0523 05:47:50.626806 34682 sgd_solver.cpp:112] Iteration 58960, lr = 0.01
I0523 05:47:55.333220 34682 solver.cpp:239] Iteration 58970 (2.08848 iter/s, 4.78818s/10 iters), loss = 8.81303
I0523 05:47:55.333281 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81303 (* 1 = 8.81303 loss)
I0523 05:47:55.393934 34682 sgd_solver.cpp:112] Iteration 58970, lr = 0.01
I0523 05:48:01.206562 34682 solver.cpp:239] Iteration 58980 (1.70269 iter/s, 5.87305s/10 iters), loss = 8.3434
I0523 05:48:01.206609 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3434 (* 1 = 8.3434 loss)
I0523 05:48:01.276298 34682 sgd_solver.cpp:112] Iteration 58980, lr = 0.01
I0523 05:48:06.300415 34682 solver.cpp:239] Iteration 58990 (1.96325 iter/s, 5.0936s/10 iters), loss = 8.67356
I0523 05:48:06.300462 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67356 (* 1 = 8.67356 loss)
I0523 05:48:07.170367 34682 sgd_solver.cpp:112] Iteration 58990, lr = 0.01
I0523 05:48:11.144012 34682 solver.cpp:239] Iteration 59000 (2.06469 iter/s, 4.84335s/10 iters), loss = 7.46196
I0523 05:48:11.144068 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.46196 (* 1 = 7.46196 loss)
I0523 05:48:11.222481 34682 sgd_solver.cpp:112] Iteration 59000, lr = 0.01
I0523 05:48:16.787183 34682 solver.cpp:239] Iteration 59010 (1.77215 iter/s, 5.64287s/10 iters), loss = 8.09479
I0523 05:48:16.787358 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09479 (* 1 = 8.09479 loss)
I0523 05:48:16.845402 34682 sgd_solver.cpp:112] Iteration 59010, lr = 0.01
I0523 05:48:20.182806 34682 solver.cpp:239] Iteration 59020 (2.94709 iter/s, 3.39318s/10 iters), loss = 8.38113
I0523 05:48:20.182848 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38113 (* 1 = 8.38113 loss)
I0523 05:48:21.052884 34682 sgd_solver.cpp:112] Iteration 59020, lr = 0.01
I0523 05:48:25.294044 34682 solver.cpp:239] Iteration 59030 (1.95657 iter/s, 5.11098s/10 iters), loss = 8.01055
I0523 05:48:25.294095 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01055 (* 1 = 8.01055 loss)
I0523 05:48:25.344625 34682 sgd_solver.cpp:112] Iteration 59030, lr = 0.01
I0523 05:48:31.266319 34682 solver.cpp:239] Iteration 59040 (1.67449 iter/s, 5.97198s/10 iters), loss = 8.37208
I0523 05:48:31.266374 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37208 (* 1 = 8.37208 loss)
I0523 05:48:31.344671 34682 sgd_solver.cpp:112] Iteration 59040, lr = 0.01
I0523 05:48:35.435648 34682 solver.cpp:239] Iteration 59050 (2.40103 iter/s, 4.16487s/10 iters), loss = 8.18342
I0523 05:48:35.435691 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18342 (* 1 = 8.18342 loss)
I0523 05:48:35.502279 34682 sgd_solver.cpp:112] Iteration 59050, lr = 0.01
I0523 05:48:38.964278 34682 solver.cpp:239] Iteration 59060 (2.83412 iter/s, 3.52843s/10 iters), loss = 7.86333
I0523 05:48:38.964339 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86333 (* 1 = 7.86333 loss)
I0523 05:48:39.738515 34682 sgd_solver.cpp:112] Iteration 59060, lr = 0.01
I0523 05:48:43.523036 34682 solver.cpp:239] Iteration 59070 (2.1937 iter/s, 4.55851s/10 iters), loss = 7.23408
I0523 05:48:43.523082 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.23408 (* 1 = 7.23408 loss)
I0523 05:48:43.599290 34682 sgd_solver.cpp:112] Iteration 59070, lr = 0.01
I0523 05:48:48.888012 34682 solver.cpp:239] Iteration 59080 (1.86403 iter/s, 5.36471s/10 iters), loss = 8.19844
I0523 05:48:48.888259 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19844 (* 1 = 8.19844 loss)
I0523 05:48:48.951814 34682 sgd_solver.cpp:112] Iteration 59080, lr = 0.01
I0523 05:48:52.202641 34682 solver.cpp:239] Iteration 59090 (3.01726 iter/s, 3.31427s/10 iters), loss = 8.88211
I0523 05:48:52.202725 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88211 (* 1 = 8.88211 loss)
I0523 05:48:52.986963 34682 sgd_solver.cpp:112] Iteration 59090, lr = 0.01
I0523 05:48:54.887354 34682 solver.cpp:239] Iteration 59100 (3.72502 iter/s, 2.68455s/10 iters), loss = 8.02316
I0523 05:48:54.887405 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02316 (* 1 = 8.02316 loss)
I0523 05:48:54.950279 34682 sgd_solver.cpp:112] Iteration 59100, lr = 0.01
I0523 05:49:00.155957 34682 solver.cpp:239] Iteration 59110 (1.89813 iter/s, 5.26833s/10 iters), loss = 8.30724
I0523 05:49:00.156008 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30724 (* 1 = 8.30724 loss)
I0523 05:49:01.051930 34682 sgd_solver.cpp:112] Iteration 59110, lr = 0.01
I0523 05:49:05.158293 34682 solver.cpp:239] Iteration 59120 (1.99917 iter/s, 5.00208s/10 iters), loss = 9.2339
I0523 05:49:05.158342 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.2339 (* 1 = 9.2339 loss)
I0523 05:49:05.403496 34682 sgd_solver.cpp:112] Iteration 59120, lr = 0.01
I0523 05:49:11.286940 34682 solver.cpp:239] Iteration 59130 (1.63176 iter/s, 6.12835s/10 iters), loss = 7.91679
I0523 05:49:11.286990 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91679 (* 1 = 7.91679 loss)
I0523 05:49:11.360059 34682 sgd_solver.cpp:112] Iteration 59130, lr = 0.01
I0523 05:49:16.193008 34682 solver.cpp:239] Iteration 59140 (2.0384 iter/s, 4.90582s/10 iters), loss = 8.32875
I0523 05:49:16.193065 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32875 (* 1 = 8.32875 loss)
I0523 05:49:17.003157 34682 sgd_solver.cpp:112] Iteration 59140, lr = 0.01
I0523 05:49:21.265599 34682 solver.cpp:239] Iteration 59150 (1.97148 iter/s, 5.07233s/10 iters), loss = 8.07622
I0523 05:49:21.265822 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07622 (* 1 = 8.07622 loss)
I0523 05:49:21.316918 34682 sgd_solver.cpp:112] Iteration 59150, lr = 0.01
I0523 05:49:26.962481 34682 solver.cpp:239] Iteration 59160 (1.75548 iter/s, 5.69643s/10 iters), loss = 8.64761
I0523 05:49:26.962524 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64761 (* 1 = 8.64761 loss)
I0523 05:49:27.802752 34682 sgd_solver.cpp:112] Iteration 59160, lr = 0.01
I0523 05:49:32.631366 34682 solver.cpp:239] Iteration 59170 (1.7641 iter/s, 5.66861s/10 iters), loss = 7.50128
I0523 05:49:32.631420 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50128 (* 1 = 7.50128 loss)
I0523 05:49:32.701206 34682 sgd_solver.cpp:112] Iteration 59170, lr = 0.01
I0523 05:49:37.570865 34682 solver.cpp:239] Iteration 59180 (2.0246 iter/s, 4.93924s/10 iters), loss = 8.65231
I0523 05:49:37.570914 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65231 (* 1 = 8.65231 loss)
I0523 05:49:38.375577 34682 sgd_solver.cpp:112] Iteration 59180, lr = 0.01
I0523 05:49:43.178447 34682 solver.cpp:239] Iteration 59190 (1.78339 iter/s, 5.60731s/10 iters), loss = 8.75126
I0523 05:49:43.178493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75126 (* 1 = 8.75126 loss)
I0523 05:49:43.251948 34682 sgd_solver.cpp:112] Iteration 59190, lr = 0.01
I0523 05:49:47.291396 34682 solver.cpp:239] Iteration 59200 (2.43147 iter/s, 4.11273s/10 iters), loss = 9.27417
I0523 05:49:47.291440 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.27417 (* 1 = 9.27417 loss)
I0523 05:49:47.368611 34682 sgd_solver.cpp:112] Iteration 59200, lr = 0.01
I0523 05:49:52.921423 34682 solver.cpp:239] Iteration 59210 (1.77628 iter/s, 5.62975s/10 iters), loss = 8.80462
I0523 05:49:52.921722 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80462 (* 1 = 8.80462 loss)
I0523 05:49:53.777756 34682 sgd_solver.cpp:112] Iteration 59210, lr = 0.01
I0523 05:50:00.021139 34682 solver.cpp:239] Iteration 59220 (1.40905 iter/s, 7.09697s/10 iters), loss = 8.34659
I0523 05:50:00.021180 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34659 (* 1 = 8.34659 loss)
I0523 05:50:00.080802 34682 sgd_solver.cpp:112] Iteration 59220, lr = 0.01
I0523 05:50:05.789541 34682 solver.cpp:239] Iteration 59230 (1.73366 iter/s, 5.76813s/10 iters), loss = 6.53898
I0523 05:50:05.789583 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.53898 (* 1 = 6.53898 loss)
I0523 05:50:05.870683 34682 sgd_solver.cpp:112] Iteration 59230, lr = 0.01
I0523 05:50:10.771327 34682 solver.cpp:239] Iteration 59240 (2.00741 iter/s, 4.98153s/10 iters), loss = 9.02822
I0523 05:50:10.771386 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02822 (* 1 = 9.02822 loss)
I0523 05:50:10.841346 34682 sgd_solver.cpp:112] Iteration 59240, lr = 0.01
I0523 05:50:14.681010 34682 solver.cpp:239] Iteration 59250 (2.5579 iter/s, 3.90946s/10 iters), loss = 8.07662
I0523 05:50:14.681066 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07662 (* 1 = 8.07662 loss)
I0523 05:50:14.789222 34682 sgd_solver.cpp:112] Iteration 59250, lr = 0.01
I0523 05:50:19.117403 34682 solver.cpp:239] Iteration 59260 (2.25421 iter/s, 4.43615s/10 iters), loss = 8.92827
I0523 05:50:19.117466 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92827 (* 1 = 8.92827 loss)
I0523 05:50:19.170557 34682 sgd_solver.cpp:112] Iteration 59260, lr = 0.01
I0523 05:50:22.398692 34682 solver.cpp:239] Iteration 59270 (3.04777 iter/s, 3.28109s/10 iters), loss = 9.11969
I0523 05:50:22.398780 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11969 (* 1 = 9.11969 loss)
I0523 05:50:22.472141 34682 sgd_solver.cpp:112] Iteration 59270, lr = 0.01
I0523 05:50:26.396428 34682 solver.cpp:239] Iteration 59280 (2.50157 iter/s, 3.99748s/10 iters), loss = 7.5736
I0523 05:50:26.396672 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5736 (* 1 = 7.5736 loss)
I0523 05:50:27.187152 34682 sgd_solver.cpp:112] Iteration 59280, lr = 0.01
I0523 05:50:31.092232 34682 solver.cpp:239] Iteration 59290 (2.12974 iter/s, 4.6954s/10 iters), loss = 7.75225
I0523 05:50:31.092278 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75225 (* 1 = 7.75225 loss)
I0523 05:50:31.962014 34682 sgd_solver.cpp:112] Iteration 59290, lr = 0.01
I0523 05:50:35.272591 34682 solver.cpp:239] Iteration 59300 (2.39226 iter/s, 4.18014s/10 iters), loss = 8.3713
I0523 05:50:35.272640 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3713 (* 1 = 8.3713 loss)
I0523 05:50:36.125550 34682 sgd_solver.cpp:112] Iteration 59300, lr = 0.01
I0523 05:50:41.697834 34682 solver.cpp:239] Iteration 59310 (1.55644 iter/s, 6.42493s/10 iters), loss = 8.8309
I0523 05:50:41.697875 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8309 (* 1 = 8.8309 loss)
I0523 05:50:41.765478 34682 sgd_solver.cpp:112] Iteration 59310, lr = 0.01
I0523 05:50:47.377745 34682 solver.cpp:239] Iteration 59320 (1.76068 iter/s, 5.67963s/10 iters), loss = 7.61497
I0523 05:50:47.377812 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.61497 (* 1 = 7.61497 loss)
I0523 05:50:48.235564 34682 sgd_solver.cpp:112] Iteration 59320, lr = 0.01
I0523 05:50:53.428175 34682 solver.cpp:239] Iteration 59330 (1.65286 iter/s, 6.05013s/10 iters), loss = 8.03114
I0523 05:50:53.428217 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03114 (* 1 = 8.03114 loss)
I0523 05:50:53.512099 34682 sgd_solver.cpp:112] Iteration 59330, lr = 0.01
I0523 05:50:59.381117 34682 solver.cpp:239] Iteration 59340 (1.67993 iter/s, 5.95265s/10 iters), loss = 8.25606
I0523 05:50:59.381410 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25606 (* 1 = 8.25606 loss)
I0523 05:50:59.438241 34682 sgd_solver.cpp:112] Iteration 59340, lr = 0.01
I0523 05:51:05.451413 34682 solver.cpp:239] Iteration 59350 (1.6475 iter/s, 6.06979s/10 iters), loss = 7.10049
I0523 05:51:05.451447 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.10049 (* 1 = 7.10049 loss)
I0523 05:51:05.524134 34682 sgd_solver.cpp:112] Iteration 59350, lr = 0.01
I0523 05:51:09.573034 34682 solver.cpp:239] Iteration 59360 (2.42635 iter/s, 4.12141s/10 iters), loss = 9.12704
I0523 05:51:09.573083 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12704 (* 1 = 9.12704 loss)
I0523 05:51:10.305953 34682 sgd_solver.cpp:112] Iteration 59360, lr = 0.01
I0523 05:51:13.609652 34682 solver.cpp:239] Iteration 59370 (2.47745 iter/s, 4.0364s/10 iters), loss = 6.6079
I0523 05:51:13.609704 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.6079 (* 1 = 6.6079 loss)
I0523 05:51:13.669113 34682 sgd_solver.cpp:112] Iteration 59370, lr = 0.01
I0523 05:51:19.890111 34682 solver.cpp:239] Iteration 59380 (1.59232 iter/s, 6.28015s/10 iters), loss = 9.07197
I0523 05:51:19.890168 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.07197 (* 1 = 9.07197 loss)
I0523 05:51:20.621516 34682 sgd_solver.cpp:112] Iteration 59380, lr = 0.01
I0523 05:51:25.938426 34682 solver.cpp:239] Iteration 59390 (1.65344 iter/s, 6.048s/10 iters), loss = 7.89539
I0523 05:51:25.938488 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89539 (* 1 = 7.89539 loss)
I0523 05:51:26.672262 34682 sgd_solver.cpp:112] Iteration 59390, lr = 0.01
I0523 05:51:31.233302 34682 solver.cpp:239] Iteration 59400 (1.88871 iter/s, 5.29461s/10 iters), loss = 8.52196
I0523 05:51:31.233418 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52196 (* 1 = 8.52196 loss)
I0523 05:51:32.063669 34682 sgd_solver.cpp:112] Iteration 59400, lr = 0.01
I0523 05:51:35.661592 34682 solver.cpp:239] Iteration 59410 (2.25836 iter/s, 4.42799s/10 iters), loss = 7.97214
I0523 05:51:35.661648 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97214 (* 1 = 7.97214 loss)
I0523 05:51:35.738281 34682 sgd_solver.cpp:112] Iteration 59410, lr = 0.01
I0523 05:51:40.567900 34682 solver.cpp:239] Iteration 59420 (2.0383 iter/s, 4.90606s/10 iters), loss = 8.56189
I0523 05:51:40.567945 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56189 (* 1 = 8.56189 loss)
I0523 05:51:41.385617 34682 sgd_solver.cpp:112] Iteration 59420, lr = 0.01
I0523 05:51:44.651527 34682 solver.cpp:239] Iteration 59430 (2.44894 iter/s, 4.0834s/10 iters), loss = 8.04368
I0523 05:51:44.651590 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04368 (* 1 = 8.04368 loss)
I0523 05:51:45.287878 34682 sgd_solver.cpp:112] Iteration 59430, lr = 0.01
I0523 05:51:50.059702 34682 solver.cpp:239] Iteration 59440 (1.84915 iter/s, 5.40789s/10 iters), loss = 8.03113
I0523 05:51:50.059767 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03113 (* 1 = 8.03113 loss)
I0523 05:51:50.950292 34682 sgd_solver.cpp:112] Iteration 59440, lr = 0.01
I0523 05:51:55.369098 34682 solver.cpp:239] Iteration 59450 (1.88355 iter/s, 5.30912s/10 iters), loss = 8.61845
I0523 05:51:55.369153 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61845 (* 1 = 8.61845 loss)
I0523 05:51:55.439122 34682 sgd_solver.cpp:112] Iteration 59450, lr = 0.01
I0523 05:52:00.443727 34682 solver.cpp:239] Iteration 59460 (1.97069 iter/s, 5.07437s/10 iters), loss = 8.50639
I0523 05:52:00.443773 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50639 (* 1 = 8.50639 loss)
I0523 05:52:00.514870 34682 sgd_solver.cpp:112] Iteration 59460, lr = 0.01
I0523 05:52:05.106575 34682 solver.cpp:239] Iteration 59470 (2.14473 iter/s, 4.6626s/10 iters), loss = 8.1842
I0523 05:52:05.106887 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1842 (* 1 = 8.1842 loss)
I0523 05:52:05.347942 34682 sgd_solver.cpp:112] Iteration 59470, lr = 0.01
I0523 05:52:09.206889 34682 solver.cpp:239] Iteration 59480 (2.4391 iter/s, 4.09987s/10 iters), loss = 8.22638
I0523 05:52:09.206933 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22638 (* 1 = 8.22638 loss)
I0523 05:52:09.270913 34682 sgd_solver.cpp:112] Iteration 59480, lr = 0.01
I0523 05:52:14.397605 34682 solver.cpp:239] Iteration 59490 (1.92661 iter/s, 5.19046s/10 iters), loss = 9.28453
I0523 05:52:14.397655 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.28453 (* 1 = 9.28453 loss)
I0523 05:52:14.462121 34682 sgd_solver.cpp:112] Iteration 59490, lr = 0.01
I0523 05:52:18.582496 34682 solver.cpp:239] Iteration 59500 (2.38968 iter/s, 4.18467s/10 iters), loss = 8.373
I0523 05:52:18.582557 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.373 (* 1 = 8.373 loss)
I0523 05:52:18.665163 34682 sgd_solver.cpp:112] Iteration 59500, lr = 0.01
I0523 05:52:23.819919 34682 solver.cpp:239] Iteration 59510 (1.90943 iter/s, 5.23715s/10 iters), loss = 8.69069
I0523 05:52:23.819964 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69069 (* 1 = 8.69069 loss)
I0523 05:52:23.881368 34682 sgd_solver.cpp:112] Iteration 59510, lr = 0.01
I0523 05:52:27.747614 34682 solver.cpp:239] Iteration 59520 (2.54617 iter/s, 3.92747s/10 iters), loss = 7.82969
I0523 05:52:27.747678 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82969 (* 1 = 7.82969 loss)
I0523 05:52:28.503159 34682 sgd_solver.cpp:112] Iteration 59520, lr = 0.01
I0523 05:52:32.101411 34682 solver.cpp:239] Iteration 59530 (2.29698 iter/s, 4.35354s/10 iters), loss = 8.54819
I0523 05:52:32.101480 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54819 (* 1 = 8.54819 loss)
I0523 05:52:32.161773 34682 sgd_solver.cpp:112] Iteration 59530, lr = 0.01
I0523 05:52:36.869128 34682 solver.cpp:239] Iteration 59540 (2.09755 iter/s, 4.76746s/10 iters), loss = 8.00726
I0523 05:52:36.869350 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00726 (* 1 = 8.00726 loss)
I0523 05:52:36.930210 34682 sgd_solver.cpp:112] Iteration 59540, lr = 0.01
I0523 05:52:40.454795 34682 solver.cpp:239] Iteration 59550 (2.78916 iter/s, 3.58531s/10 iters), loss = 7.21614
I0523 05:52:40.454849 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.21614 (* 1 = 7.21614 loss)
I0523 05:52:40.516650 34682 sgd_solver.cpp:112] Iteration 59550, lr = 0.01
I0523 05:52:45.390632 34682 solver.cpp:239] Iteration 59560 (2.0261 iter/s, 4.93559s/10 iters), loss = 7.23175
I0523 05:52:45.390681 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.23175 (* 1 = 7.23175 loss)
I0523 05:52:45.452834 34682 sgd_solver.cpp:112] Iteration 59560, lr = 0.01
I0523 05:52:50.398954 34682 solver.cpp:239] Iteration 59570 (1.99678 iter/s, 5.00807s/10 iters), loss = 8.16317
I0523 05:52:50.399020 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16317 (* 1 = 8.16317 loss)
I0523 05:52:51.186236 34682 sgd_solver.cpp:112] Iteration 59570, lr = 0.01
I0523 05:52:54.453099 34682 solver.cpp:239] Iteration 59580 (2.46675 iter/s, 4.05391s/10 iters), loss = 8.03246
I0523 05:52:54.453140 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03246 (* 1 = 8.03246 loss)
I0523 05:52:54.534176 34682 sgd_solver.cpp:112] Iteration 59580, lr = 0.01
I0523 05:52:59.523545 34682 solver.cpp:239] Iteration 59590 (1.97231 iter/s, 5.0702s/10 iters), loss = 8.35743
I0523 05:52:59.523597 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35743 (* 1 = 8.35743 loss)
I0523 05:52:59.601752 34682 sgd_solver.cpp:112] Iteration 59590, lr = 0.01
I0523 05:53:05.246830 34682 solver.cpp:239] Iteration 59600 (1.74734 iter/s, 5.723s/10 iters), loss = 7.54962
I0523 05:53:05.246879 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54962 (* 1 = 7.54962 loss)
I0523 05:53:06.102912 34682 sgd_solver.cpp:112] Iteration 59600, lr = 0.01
I0523 05:53:10.804651 34682 solver.cpp:239] Iteration 59610 (1.79936 iter/s, 5.55752s/10 iters), loss = 8.98325
I0523 05:53:10.805094 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98325 (* 1 = 8.98325 loss)
I0523 05:53:10.857573 34682 sgd_solver.cpp:112] Iteration 59610, lr = 0.01
I0523 05:53:15.496659 34682 solver.cpp:239] Iteration 59620 (2.13153 iter/s, 4.69147s/10 iters), loss = 8.86431
I0523 05:53:15.496697 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86431 (* 1 = 8.86431 loss)
I0523 05:53:15.572687 34682 sgd_solver.cpp:112] Iteration 59620, lr = 0.01
I0523 05:53:20.551887 34682 solver.cpp:239] Iteration 59630 (1.97825 iter/s, 5.05498s/10 iters), loss = 8.33316
I0523 05:53:20.551941 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33316 (* 1 = 8.33316 loss)
I0523 05:53:20.628060 34682 sgd_solver.cpp:112] Iteration 59630, lr = 0.01
I0523 05:53:26.192869 34682 solver.cpp:239] Iteration 59640 (1.77283 iter/s, 5.6407s/10 iters), loss = 8.33566
I0523 05:53:26.192920 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33566 (* 1 = 8.33566 loss)
I0523 05:53:26.939729 34682 sgd_solver.cpp:112] Iteration 59640, lr = 0.01
I0523 05:53:30.333164 34682 solver.cpp:239] Iteration 59650 (2.41541 iter/s, 4.14008s/10 iters), loss = 8.15064
I0523 05:53:30.333209 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15064 (* 1 = 8.15064 loss)
I0523 05:53:31.047017 34682 sgd_solver.cpp:112] Iteration 59650, lr = 0.01
I0523 05:53:33.754931 34682 solver.cpp:239] Iteration 59660 (2.92263 iter/s, 3.42157s/10 iters), loss = 8.43363
I0523 05:53:33.754972 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43363 (* 1 = 8.43363 loss)
I0523 05:53:33.830765 34682 sgd_solver.cpp:112] Iteration 59660, lr = 0.01
I0523 05:53:40.100092 34682 solver.cpp:239] Iteration 59670 (1.57719 iter/s, 6.34039s/10 iters), loss = 8.26677
I0523 05:53:40.100143 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26677 (* 1 = 8.26677 loss)
I0523 05:53:40.173261 34682 sgd_solver.cpp:112] Iteration 59670, lr = 0.01
I0523 05:53:46.427942 34682 solver.cpp:239] Iteration 59680 (1.58039 iter/s, 6.32754s/10 iters), loss = 8.26068
I0523 05:53:46.428158 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26068 (* 1 = 8.26068 loss)
I0523 05:53:46.500864 34682 sgd_solver.cpp:112] Iteration 59680, lr = 0.01
I0523 05:53:50.367995 34682 solver.cpp:239] Iteration 59690 (2.53827 iter/s, 3.93969s/10 iters), loss = 7.81275
I0523 05:53:50.368044 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81275 (* 1 = 7.81275 loss)
I0523 05:53:50.429908 34682 sgd_solver.cpp:112] Iteration 59690, lr = 0.01
I0523 05:53:53.859987 34682 solver.cpp:239] Iteration 59700 (2.86386 iter/s, 3.49179s/10 iters), loss = 8.00301
I0523 05:53:53.860047 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00301 (* 1 = 8.00301 loss)
I0523 05:53:54.719583 34682 sgd_solver.cpp:112] Iteration 59700, lr = 0.01
I0523 05:54:00.387025 34682 solver.cpp:239] Iteration 59710 (1.53217 iter/s, 6.52671s/10 iters), loss = 8.20728
I0523 05:54:00.387070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20728 (* 1 = 8.20728 loss)
I0523 05:54:00.450825 34682 sgd_solver.cpp:112] Iteration 59710, lr = 0.01
I0523 05:54:05.426837 34682 solver.cpp:239] Iteration 59720 (1.9843 iter/s, 5.03955s/10 iters), loss = 8.91363
I0523 05:54:05.426892 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91363 (* 1 = 8.91363 loss)
I0523 05:54:05.510442 34682 sgd_solver.cpp:112] Iteration 59720, lr = 0.01
I0523 05:54:09.111376 34682 solver.cpp:239] Iteration 59730 (2.7142 iter/s, 3.68433s/10 iters), loss = 8.25363
I0523 05:54:09.111429 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25363 (* 1 = 8.25363 loss)
I0523 05:54:09.198777 34682 sgd_solver.cpp:112] Iteration 59730, lr = 0.01
I0523 05:54:15.090931 34682 solver.cpp:239] Iteration 59740 (1.67245 iter/s, 5.97926s/10 iters), loss = 8.99297
I0523 05:54:15.090979 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.99297 (* 1 = 8.99297 loss)
I0523 05:54:15.159608 34682 sgd_solver.cpp:112] Iteration 59740, lr = 0.01
I0523 05:54:20.970834 34682 solver.cpp:239] Iteration 59750 (1.70079 iter/s, 5.87961s/10 iters), loss = 8.16804
I0523 05:54:20.971122 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16804 (* 1 = 8.16804 loss)
I0523 05:54:21.040462 34682 sgd_solver.cpp:112] Iteration 59750, lr = 0.01
I0523 05:54:24.268997 34682 solver.cpp:239] Iteration 59760 (3.03237 iter/s, 3.29775s/10 iters), loss = 9.11411
I0523 05:54:24.269093 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.11411 (* 1 = 9.11411 loss)
I0523 05:54:24.353572 34682 sgd_solver.cpp:112] Iteration 59760, lr = 0.01
I0523 05:54:28.791426 34682 solver.cpp:239] Iteration 59770 (2.21134 iter/s, 4.52215s/10 iters), loss = 9.40177
I0523 05:54:28.791474 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.40177 (* 1 = 9.40177 loss)
I0523 05:54:28.872058 34682 sgd_solver.cpp:112] Iteration 59770, lr = 0.01
I0523 05:54:34.657634 34682 solver.cpp:239] Iteration 59780 (1.70477 iter/s, 5.86591s/10 iters), loss = 7.31118
I0523 05:54:34.657706 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.31118 (* 1 = 7.31118 loss)
I0523 05:54:35.417724 34682 sgd_solver.cpp:112] Iteration 59780, lr = 0.01
I0523 05:54:39.638478 34682 solver.cpp:239] Iteration 59790 (2.0078 iter/s, 4.98057s/10 iters), loss = 8.47153
I0523 05:54:39.638555 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47153 (* 1 = 8.47153 loss)
I0523 05:54:40.467890 34682 sgd_solver.cpp:112] Iteration 59790, lr = 0.01
I0523 05:54:44.543810 34682 solver.cpp:239] Iteration 59800 (2.03871 iter/s, 4.90506s/10 iters), loss = 8.45917
I0523 05:54:44.543857 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45917 (* 1 = 8.45917 loss)
I0523 05:54:44.620824 34682 sgd_solver.cpp:112] Iteration 59800, lr = 0.01
I0523 05:54:48.561177 34682 solver.cpp:239] Iteration 59810 (2.48932 iter/s, 4.01716s/10 iters), loss = 7.44906
I0523 05:54:48.561218 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44906 (* 1 = 7.44906 loss)
I0523 05:54:48.635852 34682 sgd_solver.cpp:112] Iteration 59810, lr = 0.01
I0523 05:54:52.519433 34682 solver.cpp:239] Iteration 59820 (2.5265 iter/s, 3.95804s/10 iters), loss = 7.83007
I0523 05:54:52.519556 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83007 (* 1 = 7.83007 loss)
I0523 05:54:52.588408 34682 sgd_solver.cpp:112] Iteration 59820, lr = 0.01
I0523 05:54:55.269023 34682 solver.cpp:239] Iteration 59830 (3.63724 iter/s, 2.74934s/10 iters), loss = 7.68341
I0523 05:54:55.269074 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68341 (* 1 = 7.68341 loss)
I0523 05:54:55.329197 34682 sgd_solver.cpp:112] Iteration 59830, lr = 0.01
I0523 05:55:00.067124 34682 solver.cpp:239] Iteration 59840 (2.08427 iter/s, 4.79785s/10 iters), loss = 8.4034
I0523 05:55:00.067179 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4034 (* 1 = 8.4034 loss)
I0523 05:55:00.126540 34682 sgd_solver.cpp:112] Iteration 59840, lr = 0.01
I0523 05:55:04.677342 34682 solver.cpp:239] Iteration 59850 (2.16921 iter/s, 4.60997s/10 iters), loss = 7.64322
I0523 05:55:04.677399 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64322 (* 1 = 7.64322 loss)
I0523 05:55:05.445705 34682 sgd_solver.cpp:112] Iteration 59850, lr = 0.01
I0523 05:55:09.681087 34682 solver.cpp:239] Iteration 59860 (1.99861 iter/s, 5.00348s/10 iters), loss = 8.60315
I0523 05:55:09.681133 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60315 (* 1 = 8.60315 loss)
I0523 05:55:10.262791 34682 sgd_solver.cpp:112] Iteration 59860, lr = 0.01
I0523 05:55:15.376199 34682 solver.cpp:239] Iteration 59870 (1.75598 iter/s, 5.69483s/10 iters), loss = 7.83704
I0523 05:55:15.376242 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83704 (* 1 = 7.83704 loss)
I0523 05:55:16.209353 34682 sgd_solver.cpp:112] Iteration 59870, lr = 0.01
I0523 05:55:21.532851 34682 solver.cpp:239] Iteration 59880 (1.62434 iter/s, 6.15636s/10 iters), loss = 8.31531
I0523 05:55:21.532904 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31531 (* 1 = 8.31531 loss)
I0523 05:55:21.608711 34682 sgd_solver.cpp:112] Iteration 59880, lr = 0.01
I0523 05:55:26.105464 34682 solver.cpp:239] Iteration 59890 (2.18706 iter/s, 4.57235s/10 iters), loss = 8.73993
I0523 05:55:26.105707 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73993 (* 1 = 8.73993 loss)
I0523 05:55:26.951952 34682 sgd_solver.cpp:112] Iteration 59890, lr = 0.01
I0523 05:55:32.495685 34682 solver.cpp:239] Iteration 59900 (1.56501 iter/s, 6.38973s/10 iters), loss = 8.46159
I0523 05:55:32.495734 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46159 (* 1 = 8.46159 loss)
I0523 05:55:32.567623 34682 sgd_solver.cpp:112] Iteration 59900, lr = 0.01
I0523 05:55:37.897516 34682 solver.cpp:239] Iteration 59910 (1.85132 iter/s, 5.40155s/10 iters), loss = 8.91077
I0523 05:55:37.897574 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91077 (* 1 = 8.91077 loss)
I0523 05:55:38.682672 34682 sgd_solver.cpp:112] Iteration 59910, lr = 0.01
I0523 05:55:43.512841 34682 solver.cpp:239] Iteration 59920 (1.78094 iter/s, 5.615s/10 iters), loss = 8.45106
I0523 05:55:43.512902 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45106 (* 1 = 8.45106 loss)
I0523 05:55:43.584707 34682 sgd_solver.cpp:112] Iteration 59920, lr = 0.01
I0523 05:55:48.261716 34682 solver.cpp:239] Iteration 59930 (2.10587 iter/s, 4.74862s/10 iters), loss = 8.47865
I0523 05:55:48.261759 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47865 (* 1 = 8.47865 loss)
I0523 05:55:48.324362 34682 sgd_solver.cpp:112] Iteration 59930, lr = 0.01
I0523 05:55:52.972231 34682 solver.cpp:239] Iteration 59940 (2.12302 iter/s, 4.71027s/10 iters), loss = 8.94555
I0523 05:55:52.972280 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.94555 (* 1 = 8.94555 loss)
I0523 05:55:53.838168 34682 sgd_solver.cpp:112] Iteration 59940, lr = 0.01
I0523 05:55:59.478472 34682 solver.cpp:239] Iteration 59950 (1.53706 iter/s, 6.50593s/10 iters), loss = 7.94871
I0523 05:55:59.478555 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94871 (* 1 = 7.94871 loss)
I0523 05:55:59.549087 34682 sgd_solver.cpp:112] Iteration 59950, lr = 0.01
I0523 05:56:03.491742 34682 solver.cpp:239] Iteration 59960 (2.49189 iter/s, 4.01301s/10 iters), loss = 9.13312
I0523 05:56:03.491793 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13312 (* 1 = 9.13312 loss)
I0523 05:56:03.570102 34682 sgd_solver.cpp:112] Iteration 59960, lr = 0.01
I0523 05:56:06.882469 34682 solver.cpp:239] Iteration 59970 (2.94939 iter/s, 3.39053s/10 iters), loss = 8.28543
I0523 05:56:06.882515 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28543 (* 1 = 8.28543 loss)
I0523 05:56:06.951661 34682 sgd_solver.cpp:112] Iteration 59970, lr = 0.01
I0523 05:56:09.513983 34682 solver.cpp:239] Iteration 59980 (3.80032 iter/s, 2.63135s/10 iters), loss = 7.83123
I0523 05:56:09.514039 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83123 (* 1 = 7.83123 loss)
I0523 05:56:10.309437 34682 sgd_solver.cpp:112] Iteration 59980, lr = 0.01
I0523 05:56:14.480098 34682 solver.cpp:239] Iteration 59990 (2.01375 iter/s, 4.96586s/10 iters), loss = 8.78267
I0523 05:56:14.480136 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78267 (* 1 = 8.78267 loss)
I0523 05:56:14.556685 34682 sgd_solver.cpp:112] Iteration 59990, lr = 0.01
I0523 05:56:19.991817 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_60000.caffemodel
I0523 05:56:21.933207 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_60000.solverstate
I0523 05:56:22.140090 34682 solver.cpp:239] Iteration 60000 (1.30554 iter/s, 7.65965s/10 iters), loss = 8.75993
I0523 05:56:22.140137 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75993 (* 1 = 8.75993 loss)
I0523 05:56:22.213594 34682 sgd_solver.cpp:112] Iteration 60000, lr = 0.01
I0523 05:56:28.424767 34682 solver.cpp:239] Iteration 60010 (1.59125 iter/s, 6.28437s/10 iters), loss = 9.08528
I0523 05:56:28.424813 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08528 (* 1 = 9.08528 loss)
I0523 05:56:28.515146 34682 sgd_solver.cpp:112] Iteration 60010, lr = 0.01
I0523 05:56:33.987368 34682 solver.cpp:239] Iteration 60020 (1.79781 iter/s, 5.56232s/10 iters), loss = 7.51063
I0523 05:56:33.987648 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51063 (* 1 = 7.51063 loss)
I0523 05:56:34.756067 34682 sgd_solver.cpp:112] Iteration 60020, lr = 0.01
I0523 05:56:38.911067 34682 solver.cpp:239] Iteration 60030 (2.03118 iter/s, 4.92324s/10 iters), loss = 9.01901
I0523 05:56:38.911113 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.01901 (* 1 = 9.01901 loss)
I0523 05:56:38.968852 34682 sgd_solver.cpp:112] Iteration 60030, lr = 0.01
I0523 05:56:44.372498 34682 solver.cpp:239] Iteration 60040 (1.83111 iter/s, 5.46117s/10 iters), loss = 8.25016
I0523 05:56:44.372545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25016 (* 1 = 8.25016 loss)
I0523 05:56:44.439733 34682 sgd_solver.cpp:112] Iteration 60040, lr = 0.01
I0523 05:56:47.692173 34682 solver.cpp:239] Iteration 60050 (3.01251 iter/s, 3.31949s/10 iters), loss = 8.23825
I0523 05:56:47.692234 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23825 (* 1 = 8.23825 loss)
I0523 05:56:47.760840 34682 sgd_solver.cpp:112] Iteration 60050, lr = 0.01
I0523 05:56:51.062206 34682 solver.cpp:239] Iteration 60060 (2.96751 iter/s, 3.36983s/10 iters), loss = 7.55196
I0523 05:56:51.062261 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.55196 (* 1 = 7.55196 loss)
I0523 05:56:51.140622 34682 sgd_solver.cpp:112] Iteration 60060, lr = 0.01
I0523 05:56:56.088104 34682 solver.cpp:239] Iteration 60070 (1.9898 iter/s, 5.02563s/10 iters), loss = 7.51951
I0523 05:56:56.088150 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51951 (* 1 = 7.51951 loss)
I0523 05:56:56.167670 34682 sgd_solver.cpp:112] Iteration 60070, lr = 0.01
I0523 05:57:00.625948 34682 solver.cpp:239] Iteration 60080 (2.2038 iter/s, 4.53761s/10 iters), loss = 7.65915
I0523 05:57:00.626008 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.65915 (* 1 = 7.65915 loss)
I0523 05:57:00.703943 34682 sgd_solver.cpp:112] Iteration 60080, lr = 0.01
I0523 05:57:03.943449 34682 solver.cpp:239] Iteration 60090 (3.0145 iter/s, 3.3173s/10 iters), loss = 8.19329
I0523 05:57:03.943490 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19329 (* 1 = 8.19329 loss)
I0523 05:57:04.007427 34682 sgd_solver.cpp:112] Iteration 60090, lr = 0.01
I0523 05:57:06.970104 34682 solver.cpp:239] Iteration 60100 (3.30417 iter/s, 3.02648s/10 iters), loss = 8.32977
I0523 05:57:06.970142 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32977 (* 1 = 8.32977 loss)
I0523 05:57:07.037766 34682 sgd_solver.cpp:112] Iteration 60100, lr = 0.01
I0523 05:57:11.173323 34682 solver.cpp:239] Iteration 60110 (2.37925 iter/s, 4.203s/10 iters), loss = 8.93005
I0523 05:57:11.173368 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93005 (* 1 = 8.93005 loss)
I0523 05:57:11.237215 34682 sgd_solver.cpp:112] Iteration 60110, lr = 0.01
I0523 05:57:13.988049 34682 solver.cpp:239] Iteration 60120 (3.55295 iter/s, 2.81456s/10 iters), loss = 8.54577
I0523 05:57:13.988090 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54577 (* 1 = 8.54577 loss)
I0523 05:57:14.060623 34682 sgd_solver.cpp:112] Iteration 60120, lr = 0.01
I0523 05:57:18.146791 34682 solver.cpp:239] Iteration 60130 (2.4047 iter/s, 4.15853s/10 iters), loss = 9.12201
I0523 05:57:18.146836 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12201 (* 1 = 9.12201 loss)
I0523 05:57:18.991283 34682 sgd_solver.cpp:112] Iteration 60130, lr = 0.01
I0523 05:57:23.782866 34682 solver.cpp:239] Iteration 60140 (1.77437 iter/s, 5.6358s/10 iters), loss = 8.06004
I0523 05:57:23.782930 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06004 (* 1 = 8.06004 loss)
I0523 05:57:23.854640 34682 sgd_solver.cpp:112] Iteration 60140, lr = 0.01
I0523 05:57:28.252831 34682 solver.cpp:239] Iteration 60150 (2.23728 iter/s, 4.46971s/10 iters), loss = 8.21522
I0523 05:57:28.252904 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21522 (* 1 = 8.21522 loss)
I0523 05:57:28.902567 34682 sgd_solver.cpp:112] Iteration 60150, lr = 0.01
I0523 05:57:33.659183 34682 solver.cpp:239] Iteration 60160 (1.84978 iter/s, 5.40606s/10 iters), loss = 9.38201
I0523 05:57:33.659237 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.38201 (* 1 = 9.38201 loss)
I0523 05:57:34.251950 34682 sgd_solver.cpp:112] Iteration 60160, lr = 0.01
I0523 05:57:40.024324 34682 solver.cpp:239] Iteration 60170 (1.57113 iter/s, 6.36483s/10 iters), loss = 7.22227
I0523 05:57:40.024385 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.22227 (* 1 = 7.22227 loss)
I0523 05:57:40.092586 34682 sgd_solver.cpp:112] Iteration 60170, lr = 0.01
I0523 05:57:44.091652 34682 solver.cpp:239] Iteration 60180 (2.45876 iter/s, 4.06709s/10 iters), loss = 8.12381
I0523 05:57:44.091711 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12381 (* 1 = 8.12381 loss)
I0523 05:57:44.933707 34682 sgd_solver.cpp:112] Iteration 60180, lr = 0.01
I0523 05:57:51.775471 34682 solver.cpp:239] Iteration 60190 (1.3015 iter/s, 7.68346s/10 iters), loss = 7.27578
I0523 05:57:51.775517 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.27578 (* 1 = 7.27578 loss)
I0523 05:57:52.559948 34682 sgd_solver.cpp:112] Iteration 60190, lr = 0.01
I0523 05:57:58.482231 34682 solver.cpp:239] Iteration 60200 (1.49111 iter/s, 6.70643s/10 iters), loss = 8.05504
I0523 05:57:58.482286 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05504 (* 1 = 8.05504 loss)
I0523 05:57:59.016165 34682 sgd_solver.cpp:112] Iteration 60200, lr = 0.01
I0523 05:58:03.913820 34682 solver.cpp:239] Iteration 60210 (1.84118 iter/s, 5.43131s/10 iters), loss = 7.88115
I0523 05:58:03.913863 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88115 (* 1 = 7.88115 loss)
I0523 05:58:03.983398 34682 sgd_solver.cpp:112] Iteration 60210, lr = 0.01
I0523 05:58:09.417775 34682 solver.cpp:239] Iteration 60220 (1.81696 iter/s, 5.50369s/10 iters), loss = 8.73912
I0523 05:58:09.417904 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73912 (* 1 = 8.73912 loss)
I0523 05:58:10.222398 34682 sgd_solver.cpp:112] Iteration 60220, lr = 0.01
I0523 05:58:14.436377 34682 solver.cpp:239] Iteration 60230 (1.99272 iter/s, 5.01827s/10 iters), loss = 8.71998
I0523 05:58:14.436419 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71998 (* 1 = 8.71998 loss)
I0523 05:58:15.297166 34682 sgd_solver.cpp:112] Iteration 60230, lr = 0.01
I0523 05:58:18.579308 34682 solver.cpp:239] Iteration 60240 (2.41387 iter/s, 4.14272s/10 iters), loss = 8.50241
I0523 05:58:18.579352 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50241 (* 1 = 8.50241 loss)
I0523 05:58:18.641922 34682 sgd_solver.cpp:112] Iteration 60240, lr = 0.01
I0523 05:58:23.193563 34682 solver.cpp:239] Iteration 60250 (2.16731 iter/s, 4.61401s/10 iters), loss = 7.75715
I0523 05:58:23.193627 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75715 (* 1 = 7.75715 loss)
I0523 05:58:23.954764 34682 sgd_solver.cpp:112] Iteration 60250, lr = 0.01
I0523 05:58:27.479441 34682 solver.cpp:239] Iteration 60260 (2.33337 iter/s, 4.28565s/10 iters), loss = 8.12667
I0523 05:58:27.479483 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12667 (* 1 = 8.12667 loss)
I0523 05:58:27.533939 34682 sgd_solver.cpp:112] Iteration 60260, lr = 0.01
I0523 05:58:32.951498 34682 solver.cpp:239] Iteration 60270 (1.82756 iter/s, 5.47179s/10 iters), loss = 8.72219
I0523 05:58:32.951545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72219 (* 1 = 8.72219 loss)
I0523 05:58:33.018502 34682 sgd_solver.cpp:112] Iteration 60270, lr = 0.01
I0523 05:58:38.837765 34682 solver.cpp:239] Iteration 60280 (1.69895 iter/s, 5.88598s/10 iters), loss = 7.93979
I0523 05:58:38.837816 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93979 (* 1 = 7.93979 loss)
I0523 05:58:39.653201 34682 sgd_solver.cpp:112] Iteration 60280, lr = 0.01
I0523 05:58:42.944223 34682 solver.cpp:239] Iteration 60290 (2.43532 iter/s, 4.10624s/10 iters), loss = 9.12803
I0523 05:58:42.944270 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12803 (* 1 = 9.12803 loss)
I0523 05:58:43.735144 34682 sgd_solver.cpp:112] Iteration 60290, lr = 0.01
I0523 05:58:49.041730 34682 solver.cpp:239] Iteration 60300 (1.64009 iter/s, 6.09722s/10 iters), loss = 7.79896
I0523 05:58:49.041785 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79896 (* 1 = 7.79896 loss)
I0523 05:58:49.110755 34682 sgd_solver.cpp:112] Iteration 60300, lr = 0.01
I0523 05:58:54.200979 34682 solver.cpp:239] Iteration 60310 (1.93837 iter/s, 5.15897s/10 iters), loss = 8.49224
I0523 05:58:54.201052 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49224 (* 1 = 8.49224 loss)
I0523 05:58:54.929039 34682 sgd_solver.cpp:112] Iteration 60310, lr = 0.01
I0523 05:58:59.249687 34682 solver.cpp:239] Iteration 60320 (1.98082 iter/s, 5.04842s/10 iters), loss = 8.31196
I0523 05:58:59.249747 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31196 (* 1 = 8.31196 loss)
I0523 05:58:59.991976 34682 sgd_solver.cpp:112] Iteration 60320, lr = 0.01
I0523 05:59:03.455231 34682 solver.cpp:239] Iteration 60330 (2.37796 iter/s, 4.20528s/10 iters), loss = 8.42252
I0523 05:59:03.455288 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42252 (* 1 = 8.42252 loss)
I0523 05:59:04.275985 34682 sgd_solver.cpp:112] Iteration 60330, lr = 0.01
I0523 05:59:08.361140 34682 solver.cpp:239] Iteration 60340 (2.03847 iter/s, 4.90565s/10 iters), loss = 8.07933
I0523 05:59:08.361194 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07933 (* 1 = 8.07933 loss)
I0523 05:59:08.434485 34682 sgd_solver.cpp:112] Iteration 60340, lr = 0.01
I0523 05:59:15.246954 34682 solver.cpp:239] Iteration 60350 (1.45233 iter/s, 6.88549s/10 iters), loss = 7.80927
I0523 05:59:15.247093 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80927 (* 1 = 7.80927 loss)
I0523 05:59:15.309969 34682 sgd_solver.cpp:112] Iteration 60350, lr = 0.01
I0523 05:59:21.614588 34682 solver.cpp:239] Iteration 60360 (1.57054 iter/s, 6.36722s/10 iters), loss = 8.26492
I0523 05:59:21.614662 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26492 (* 1 = 8.26492 loss)
I0523 05:59:22.489332 34682 sgd_solver.cpp:112] Iteration 60360, lr = 0.01
I0523 05:59:26.773479 34682 solver.cpp:239] Iteration 60370 (1.93851 iter/s, 5.15861s/10 iters), loss = 8.44743
I0523 05:59:26.773526 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44743 (* 1 = 8.44743 loss)
I0523 05:59:26.835729 34682 sgd_solver.cpp:112] Iteration 60370, lr = 0.01
I0523 05:59:32.526355 34682 solver.cpp:239] Iteration 60380 (1.73835 iter/s, 5.7526s/10 iters), loss = 8.143
I0523 05:59:32.526406 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.143 (* 1 = 8.143 loss)
I0523 05:59:33.278282 34682 sgd_solver.cpp:112] Iteration 60380, lr = 0.01
I0523 05:59:40.319608 34682 solver.cpp:239] Iteration 60390 (1.28322 iter/s, 7.79289s/10 iters), loss = 8.71457
I0523 05:59:40.319654 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71457 (* 1 = 8.71457 loss)
I0523 05:59:41.173401 34682 sgd_solver.cpp:112] Iteration 60390, lr = 0.01
I0523 05:59:44.377167 34682 solver.cpp:239] Iteration 60400 (2.46466 iter/s, 4.05735s/10 iters), loss = 7.89898
I0523 05:59:44.377208 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89898 (* 1 = 7.89898 loss)
I0523 05:59:44.443753 34682 sgd_solver.cpp:112] Iteration 60400, lr = 0.01
I0523 05:59:50.310241 34682 solver.cpp:239] Iteration 60410 (1.68555 iter/s, 5.93279s/10 iters), loss = 8.04038
I0523 05:59:50.310431 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04038 (* 1 = 8.04038 loss)
I0523 05:59:50.366480 34682 sgd_solver.cpp:112] Iteration 60410, lr = 0.01
I0523 05:59:56.635259 34682 solver.cpp:239] Iteration 60420 (1.58113 iter/s, 6.32458s/10 iters), loss = 8.28462
I0523 05:59:56.635303 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28462 (* 1 = 8.28462 loss)
I0523 05:59:56.707556 34682 sgd_solver.cpp:112] Iteration 60420, lr = 0.01
I0523 06:00:00.586027 34682 solver.cpp:239] Iteration 60430 (2.53129 iter/s, 3.95056s/10 iters), loss = 7.84764
I0523 06:00:00.586076 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84764 (* 1 = 7.84764 loss)
I0523 06:00:01.424731 34682 sgd_solver.cpp:112] Iteration 60430, lr = 0.01
I0523 06:00:06.495658 34682 solver.cpp:239] Iteration 60440 (1.69224 iter/s, 5.90934s/10 iters), loss = 9.41462
I0523 06:00:06.495707 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.41462 (* 1 = 9.41462 loss)
I0523 06:00:07.319180 34682 sgd_solver.cpp:112] Iteration 60440, lr = 0.01
I0523 06:00:11.349387 34682 solver.cpp:239] Iteration 60450 (2.06038 iter/s, 4.85348s/10 iters), loss = 7.31293
I0523 06:00:11.349433 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.31293 (* 1 = 7.31293 loss)
I0523 06:00:11.418090 34682 sgd_solver.cpp:112] Iteration 60450, lr = 0.01
I0523 06:00:17.717793 34682 solver.cpp:239] Iteration 60460 (1.57033 iter/s, 6.36811s/10 iters), loss = 7.54075
I0523 06:00:17.717842 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54075 (* 1 = 7.54075 loss)
I0523 06:00:18.350150 34682 sgd_solver.cpp:112] Iteration 60460, lr = 0.01
I0523 06:00:23.643714 34682 solver.cpp:239] Iteration 60470 (1.68758 iter/s, 5.92563s/10 iters), loss = 8.58781
I0523 06:00:23.643945 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58781 (* 1 = 8.58781 loss)
I0523 06:00:23.716346 34682 sgd_solver.cpp:112] Iteration 60470, lr = 0.01
I0523 06:00:27.935483 34682 solver.cpp:239] Iteration 60480 (2.33025 iter/s, 4.29139s/10 iters), loss = 8.45309
I0523 06:00:27.935530 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45309 (* 1 = 8.45309 loss)
I0523 06:00:28.719781 34682 sgd_solver.cpp:112] Iteration 60480, lr = 0.01
I0523 06:00:32.468772 34682 solver.cpp:239] Iteration 60490 (2.20602 iter/s, 4.53305s/10 iters), loss = 7.50047
I0523 06:00:32.468816 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50047 (* 1 = 7.50047 loss)
I0523 06:00:32.543499 34682 sgd_solver.cpp:112] Iteration 60490, lr = 0.01
I0523 06:00:37.428246 34682 solver.cpp:239] Iteration 60500 (2.01645 iter/s, 4.95922s/10 iters), loss = 9.19424
I0523 06:00:37.428290 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.19424 (* 1 = 9.19424 loss)
I0523 06:00:37.495903 34682 sgd_solver.cpp:112] Iteration 60500, lr = 0.01
I0523 06:00:40.622082 34682 solver.cpp:239] Iteration 60510 (3.1312 iter/s, 3.19366s/10 iters), loss = 7.38898
I0523 06:00:40.622126 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.38898 (* 1 = 7.38898 loss)
I0523 06:00:41.348052 34682 sgd_solver.cpp:112] Iteration 60510, lr = 0.01
I0523 06:00:44.824056 34682 solver.cpp:239] Iteration 60520 (2.37996 iter/s, 4.20175s/10 iters), loss = 8.33363
I0523 06:00:44.824118 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33363 (* 1 = 8.33363 loss)
I0523 06:00:45.594944 34682 sgd_solver.cpp:112] Iteration 60520, lr = 0.01
I0523 06:00:48.785955 34682 solver.cpp:239] Iteration 60530 (2.5242 iter/s, 3.96165s/10 iters), loss = 7.72696
I0523 06:00:48.786005 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72696 (* 1 = 7.72696 loss)
I0523 06:00:49.564898 34682 sgd_solver.cpp:112] Iteration 60530, lr = 0.01
I0523 06:00:52.929095 34682 solver.cpp:239] Iteration 60540 (2.41376 iter/s, 4.14292s/10 iters), loss = 8.28976
I0523 06:00:52.929143 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28976 (* 1 = 8.28976 loss)
I0523 06:00:52.997736 34682 sgd_solver.cpp:112] Iteration 60540, lr = 0.01
I0523 06:00:59.051221 34682 solver.cpp:239] Iteration 60550 (1.6335 iter/s, 6.12182s/10 iters), loss = 8.27199
I0523 06:00:59.051476 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27199 (* 1 = 8.27199 loss)
I0523 06:00:59.128376 34682 sgd_solver.cpp:112] Iteration 60550, lr = 0.01
I0523 06:01:04.938381 34682 solver.cpp:239] Iteration 60560 (1.69874 iter/s, 5.8867s/10 iters), loss = 8.68671
I0523 06:01:04.938426 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68671 (* 1 = 8.68671 loss)
I0523 06:01:05.016402 34682 sgd_solver.cpp:112] Iteration 60560, lr = 0.01
I0523 06:01:09.913182 34682 solver.cpp:239] Iteration 60570 (2.01023 iter/s, 4.97455s/10 iters), loss = 8.00045
I0523 06:01:09.913224 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00045 (* 1 = 8.00045 loss)
I0523 06:01:09.985785 34682 sgd_solver.cpp:112] Iteration 60570, lr = 0.01
I0523 06:01:12.534240 34682 solver.cpp:239] Iteration 60580 (3.81549 iter/s, 2.6209s/10 iters), loss = 8.26053
I0523 06:01:12.534284 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26053 (* 1 = 8.26053 loss)
I0523 06:01:12.594422 34682 sgd_solver.cpp:112] Iteration 60580, lr = 0.01
I0523 06:01:17.342362 34682 solver.cpp:239] Iteration 60590 (2.07992 iter/s, 4.80788s/10 iters), loss = 7.16662
I0523 06:01:17.342413 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.16662 (* 1 = 7.16662 loss)
I0523 06:01:17.418597 34682 sgd_solver.cpp:112] Iteration 60590, lr = 0.01
I0523 06:01:22.477531 34682 solver.cpp:239] Iteration 60600 (1.94745 iter/s, 5.13491s/10 iters), loss = 7.54374
I0523 06:01:22.477594 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54374 (* 1 = 7.54374 loss)
I0523 06:01:23.301898 34682 sgd_solver.cpp:112] Iteration 60600, lr = 0.01
I0523 06:01:27.985584 34682 solver.cpp:239] Iteration 60610 (1.81562 iter/s, 5.50777s/10 iters), loss = 8.64793
I0523 06:01:27.985638 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64793 (* 1 = 8.64793 loss)
I0523 06:01:28.336767 34682 sgd_solver.cpp:112] Iteration 60610, lr = 0.01
I0523 06:01:30.696204 34682 solver.cpp:239] Iteration 60620 (3.68943 iter/s, 2.71045s/10 iters), loss = 8.6793
I0523 06:01:30.696444 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6793 (* 1 = 8.6793 loss)
I0523 06:01:31.569591 34682 sgd_solver.cpp:112] Iteration 60620, lr = 0.01
I0523 06:01:37.340235 34682 solver.cpp:239] Iteration 60630 (1.50522 iter/s, 6.64356s/10 iters), loss = 8.3205
I0523 06:01:37.340287 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3205 (* 1 = 8.3205 loss)
I0523 06:01:37.409425 34682 sgd_solver.cpp:112] Iteration 60630, lr = 0.01
I0523 06:01:42.832909 34682 solver.cpp:239] Iteration 60640 (1.8207 iter/s, 5.4924s/10 iters), loss = 8.56432
I0523 06:01:42.832948 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56432 (* 1 = 8.56432 loss)
I0523 06:01:42.897567 34682 sgd_solver.cpp:112] Iteration 60640, lr = 0.01
I0523 06:01:47.810340 34682 solver.cpp:239] Iteration 60650 (2.00917 iter/s, 4.97719s/10 iters), loss = 8.46314
I0523 06:01:47.810382 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46314 (* 1 = 8.46314 loss)
I0523 06:01:48.600087 34682 sgd_solver.cpp:112] Iteration 60650, lr = 0.01
I0523 06:01:52.832057 34682 solver.cpp:239] Iteration 60660 (1.99145 iter/s, 5.02146s/10 iters), loss = 7.48231
I0523 06:01:52.832111 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.48231 (* 1 = 7.48231 loss)
I0523 06:01:52.902027 34682 sgd_solver.cpp:112] Iteration 60660, lr = 0.01
I0523 06:01:58.537600 34682 solver.cpp:239] Iteration 60670 (1.75277 iter/s, 5.70526s/10 iters), loss = 7.56752
I0523 06:01:58.537647 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56752 (* 1 = 7.56752 loss)
I0523 06:01:59.387229 34682 sgd_solver.cpp:112] Iteration 60670, lr = 0.01
I0523 06:02:03.553067 34682 solver.cpp:239] Iteration 60680 (1.99393 iter/s, 5.01521s/10 iters), loss = 7.84774
I0523 06:02:03.553256 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84774 (* 1 = 7.84774 loss)
I0523 06:02:04.429268 34682 sgd_solver.cpp:112] Iteration 60680, lr = 0.01
I0523 06:02:08.641386 34682 solver.cpp:239] Iteration 60690 (1.96544 iter/s, 5.08792s/10 iters), loss = 9.17257
I0523 06:02:08.641436 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17257 (* 1 = 9.17257 loss)
I0523 06:02:08.699077 34682 sgd_solver.cpp:112] Iteration 60690, lr = 0.01
I0523 06:02:13.461452 34682 solver.cpp:239] Iteration 60700 (2.07477 iter/s, 4.81981s/10 iters), loss = 8.54336
I0523 06:02:13.461498 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54336 (* 1 = 8.54336 loss)
I0523 06:02:13.528075 34682 sgd_solver.cpp:112] Iteration 60700, lr = 0.01
I0523 06:02:18.401042 34682 solver.cpp:239] Iteration 60710 (2.02456 iter/s, 4.93934s/10 iters), loss = 8.20672
I0523 06:02:18.401087 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20672 (* 1 = 8.20672 loss)
I0523 06:02:18.485396 34682 sgd_solver.cpp:112] Iteration 60710, lr = 0.01
I0523 06:02:21.138753 34682 solver.cpp:239] Iteration 60720 (3.6529 iter/s, 2.73755s/10 iters), loss = 8.58047
I0523 06:02:21.138808 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58047 (* 1 = 8.58047 loss)
I0523 06:02:21.856346 34682 sgd_solver.cpp:112] Iteration 60720, lr = 0.01
I0523 06:02:26.577517 34682 solver.cpp:239] Iteration 60730 (1.83875 iter/s, 5.43849s/10 iters), loss = 8.54431
I0523 06:02:26.577561 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54431 (* 1 = 8.54431 loss)
I0523 06:02:26.644068 34682 sgd_solver.cpp:112] Iteration 60730, lr = 0.01
I0523 06:02:31.586839 34682 solver.cpp:239] Iteration 60740 (1.99638 iter/s, 5.00908s/10 iters), loss = 8.53329
I0523 06:02:31.586884 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53329 (* 1 = 8.53329 loss)
I0523 06:02:31.655624 34682 sgd_solver.cpp:112] Iteration 60740, lr = 0.01
I0523 06:02:35.711585 34682 solver.cpp:239] Iteration 60750 (2.42452 iter/s, 4.12453s/10 iters), loss = 8.38106
I0523 06:02:35.711690 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38106 (* 1 = 8.38106 loss)
I0523 06:02:35.781646 34682 sgd_solver.cpp:112] Iteration 60750, lr = 0.01
I0523 06:02:41.376858 34682 solver.cpp:239] Iteration 60760 (1.76525 iter/s, 5.66493s/10 iters), loss = 8.17836
I0523 06:02:41.376910 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17836 (* 1 = 8.17836 loss)
I0523 06:02:41.441649 34682 sgd_solver.cpp:112] Iteration 60760, lr = 0.01
I0523 06:02:45.240116 34682 solver.cpp:239] Iteration 60770 (2.58863 iter/s, 3.86304s/10 iters), loss = 7.80928
I0523 06:02:45.240170 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80928 (* 1 = 7.80928 loss)
I0523 06:02:46.089423 34682 sgd_solver.cpp:112] Iteration 60770, lr = 0.01
I0523 06:02:50.401098 34682 solver.cpp:239] Iteration 60780 (1.93771 iter/s, 5.16072s/10 iters), loss = 8.25645
I0523 06:02:50.401149 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25645 (* 1 = 8.25645 loss)
I0523 06:02:51.231410 34682 sgd_solver.cpp:112] Iteration 60780, lr = 0.01
I0523 06:02:56.532421 34682 solver.cpp:239] Iteration 60790 (1.63105 iter/s, 6.13103s/10 iters), loss = 8.32888
I0523 06:02:56.532465 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32888 (* 1 = 8.32888 loss)
I0523 06:02:56.601346 34682 sgd_solver.cpp:112] Iteration 60790, lr = 0.01
I0523 06:03:00.161505 34682 solver.cpp:239] Iteration 60800 (2.75568 iter/s, 3.62887s/10 iters), loss = 7.11184
I0523 06:03:00.161581 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.11184 (* 1 = 7.11184 loss)
I0523 06:03:01.037593 34682 sgd_solver.cpp:112] Iteration 60800, lr = 0.01
I0523 06:03:05.826441 34682 solver.cpp:239] Iteration 60810 (1.76534 iter/s, 5.66463s/10 iters), loss = 8.03691
I0523 06:03:05.826603 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03691 (* 1 = 8.03691 loss)
I0523 06:03:05.911988 34682 sgd_solver.cpp:112] Iteration 60810, lr = 0.01
I0523 06:03:11.978569 34682 solver.cpp:239] Iteration 60820 (1.62556 iter/s, 6.15173s/10 iters), loss = 8.18537
I0523 06:03:11.978621 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18537 (* 1 = 8.18537 loss)
I0523 06:03:12.649193 34682 sgd_solver.cpp:112] Iteration 60820, lr = 0.01
I0523 06:03:18.572646 34682 solver.cpp:239] Iteration 60830 (1.51658 iter/s, 6.59376s/10 iters), loss = 8.7377
I0523 06:03:18.572690 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7377 (* 1 = 8.7377 loss)
I0523 06:03:19.354451 34682 sgd_solver.cpp:112] Iteration 60830, lr = 0.01
I0523 06:03:24.370417 34682 solver.cpp:239] Iteration 60840 (1.72488 iter/s, 5.79749s/10 iters), loss = 8.61857
I0523 06:03:24.370462 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61857 (* 1 = 8.61857 loss)
I0523 06:03:24.444090 34682 sgd_solver.cpp:112] Iteration 60840, lr = 0.01
I0523 06:03:29.333314 34682 solver.cpp:239] Iteration 60850 (2.01506 iter/s, 4.96264s/10 iters), loss = 8.4273
I0523 06:03:29.333374 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4273 (* 1 = 8.4273 loss)
I0523 06:03:29.978255 34682 sgd_solver.cpp:112] Iteration 60850, lr = 0.01
I0523 06:03:34.879925 34682 solver.cpp:239] Iteration 60860 (1.80299 iter/s, 5.54633s/10 iters), loss = 8.46676
I0523 06:03:34.879966 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46676 (* 1 = 8.46676 loss)
I0523 06:03:34.957396 34682 sgd_solver.cpp:112] Iteration 60860, lr = 0.01
I0523 06:03:41.735301 34682 solver.cpp:239] Iteration 60870 (1.45878 iter/s, 6.85505s/10 iters), loss = 7.84604
I0523 06:03:41.735465 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84604 (* 1 = 7.84604 loss)
I0523 06:03:41.812530 34682 sgd_solver.cpp:112] Iteration 60870, lr = 0.01
I0523 06:03:46.320430 34682 solver.cpp:239] Iteration 60880 (2.18113 iter/s, 4.58478s/10 iters), loss = 8.26193
I0523 06:03:46.320492 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26193 (* 1 = 8.26193 loss)
I0523 06:03:47.027006 34682 sgd_solver.cpp:112] Iteration 60880, lr = 0.01
I0523 06:03:51.332211 34682 solver.cpp:239] Iteration 60890 (1.99541 iter/s, 5.01151s/10 iters), loss = 8.19814
I0523 06:03:51.332254 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19814 (* 1 = 8.19814 loss)
I0523 06:03:51.401443 34682 sgd_solver.cpp:112] Iteration 60890, lr = 0.01
I0523 06:03:56.386309 34682 solver.cpp:239] Iteration 60900 (1.97869 iter/s, 5.05384s/10 iters), loss = 8.485
I0523 06:03:56.386366 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.485 (* 1 = 8.485 loss)
I0523 06:03:57.251448 34682 sgd_solver.cpp:112] Iteration 60900, lr = 0.01
I0523 06:04:01.413319 34682 solver.cpp:239] Iteration 60910 (1.98937 iter/s, 5.02673s/10 iters), loss = 8.00854
I0523 06:04:01.413408 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00854 (* 1 = 8.00854 loss)
I0523 06:04:02.180936 34682 sgd_solver.cpp:112] Iteration 60910, lr = 0.01
I0523 06:04:07.239416 34682 solver.cpp:239] Iteration 60920 (1.71651 iter/s, 5.82578s/10 iters), loss = 8.25027
I0523 06:04:07.239457 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25027 (* 1 = 8.25027 loss)
I0523 06:04:07.309168 34682 sgd_solver.cpp:112] Iteration 60920, lr = 0.01
I0523 06:04:12.255211 34682 solver.cpp:239] Iteration 60930 (1.9938 iter/s, 5.01554s/10 iters), loss = 9.21048
I0523 06:04:12.255460 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.21048 (* 1 = 9.21048 loss)
I0523 06:04:13.074359 34682 sgd_solver.cpp:112] Iteration 60930, lr = 0.01
I0523 06:04:17.190954 34682 solver.cpp:239] Iteration 60940 (2.02621 iter/s, 4.93532s/10 iters), loss = 8.64587
I0523 06:04:17.191011 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64587 (* 1 = 8.64587 loss)
I0523 06:04:17.714124 34682 sgd_solver.cpp:112] Iteration 60940, lr = 0.01
I0523 06:04:20.963482 34682 solver.cpp:239] Iteration 60950 (2.65089 iter/s, 3.77231s/10 iters), loss = 7.49705
I0523 06:04:20.963527 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.49705 (* 1 = 7.49705 loss)
I0523 06:04:21.826169 34682 sgd_solver.cpp:112] Iteration 60950, lr = 0.01
I0523 06:04:26.913929 34682 solver.cpp:239] Iteration 60960 (1.68063 iter/s, 5.95016s/10 iters), loss = 8.44578
I0523 06:04:26.913980 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44578 (* 1 = 8.44578 loss)
I0523 06:04:26.995092 34682 sgd_solver.cpp:112] Iteration 60960, lr = 0.01
I0523 06:04:31.201596 34682 solver.cpp:239] Iteration 60970 (2.3324 iter/s, 4.28743s/10 iters), loss = 7.91605
I0523 06:04:31.201650 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91605 (* 1 = 7.91605 loss)
I0523 06:04:31.263134 34682 sgd_solver.cpp:112] Iteration 60970, lr = 0.01
I0523 06:04:33.939239 34682 solver.cpp:239] Iteration 60980 (3.65301 iter/s, 2.73747s/10 iters), loss = 8.62378
I0523 06:04:33.939282 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62378 (* 1 = 8.62378 loss)
I0523 06:04:34.003053 34682 sgd_solver.cpp:112] Iteration 60980, lr = 0.01
I0523 06:04:39.294034 34682 solver.cpp:239] Iteration 60990 (1.86758 iter/s, 5.35453s/10 iters), loss = 8.08592
I0523 06:04:39.294080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08592 (* 1 = 8.08592 loss)
I0523 06:04:39.360646 34682 sgd_solver.cpp:112] Iteration 60990, lr = 0.01
I0523 06:04:44.440083 34682 solver.cpp:239] Iteration 61000 (1.94333 iter/s, 5.1458s/10 iters), loss = 7.53927
I0523 06:04:44.440281 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.53927 (* 1 = 7.53927 loss)
I0523 06:04:45.202656 34682 sgd_solver.cpp:112] Iteration 61000, lr = 0.01
I0523 06:04:47.771059 34682 solver.cpp:239] Iteration 61010 (3.00241 iter/s, 3.33066s/10 iters), loss = 7.63326
I0523 06:04:47.771111 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63326 (* 1 = 7.63326 loss)
I0523 06:04:47.909368 34682 sgd_solver.cpp:112] Iteration 61010, lr = 0.01
I0523 06:04:50.521972 34682 solver.cpp:239] Iteration 61020 (3.63538 iter/s, 2.75074s/10 iters), loss = 8.50721
I0523 06:04:50.522017 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50721 (* 1 = 8.50721 loss)
I0523 06:04:50.586171 34682 sgd_solver.cpp:112] Iteration 61020, lr = 0.01
I0523 06:04:54.349853 34682 solver.cpp:239] Iteration 61030 (2.61256 iter/s, 3.82767s/10 iters), loss = 8.29513
I0523 06:04:54.349912 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29513 (* 1 = 8.29513 loss)
I0523 06:04:54.412947 34682 sgd_solver.cpp:112] Iteration 61030, lr = 0.01
I0523 06:04:58.624121 34682 solver.cpp:239] Iteration 61040 (2.33972 iter/s, 4.27402s/10 iters), loss = 8.6733
I0523 06:04:58.624182 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6733 (* 1 = 8.6733 loss)
I0523 06:04:58.785409 34682 sgd_solver.cpp:112] Iteration 61040, lr = 0.01
I0523 06:05:02.924304 34682 solver.cpp:239] Iteration 61050 (2.32561 iter/s, 4.29994s/10 iters), loss = 8.63323
I0523 06:05:02.924351 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63323 (* 1 = 8.63323 loss)
I0523 06:05:02.986973 34682 sgd_solver.cpp:112] Iteration 61050, lr = 0.01
I0523 06:05:09.401567 34682 solver.cpp:239] Iteration 61060 (1.54394 iter/s, 6.47695s/10 iters), loss = 8.11311
I0523 06:05:09.401612 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11311 (* 1 = 8.11311 loss)
I0523 06:05:10.245952 34682 sgd_solver.cpp:112] Iteration 61060, lr = 0.01
I0523 06:05:13.339999 34682 solver.cpp:239] Iteration 61070 (2.53922 iter/s, 3.93822s/10 iters), loss = 6.88141
I0523 06:05:13.340059 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.88141 (* 1 = 6.88141 loss)
I0523 06:05:14.053066 34682 sgd_solver.cpp:112] Iteration 61070, lr = 0.01
I0523 06:05:18.586688 34682 solver.cpp:239] Iteration 61080 (1.90606 iter/s, 5.24642s/10 iters), loss = 8.56746
I0523 06:05:18.586846 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56746 (* 1 = 8.56746 loss)
I0523 06:05:18.674549 34682 sgd_solver.cpp:112] Iteration 61080, lr = 0.01
I0523 06:05:22.031370 34682 solver.cpp:239] Iteration 61090 (2.90328 iter/s, 3.44438s/10 iters), loss = 8.71952
I0523 06:05:22.031416 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71952 (* 1 = 8.71952 loss)
I0523 06:05:22.106775 34682 sgd_solver.cpp:112] Iteration 61090, lr = 0.01
I0523 06:05:27.135097 34682 solver.cpp:239] Iteration 61100 (1.95945 iter/s, 5.10347s/10 iters), loss = 8.01681
I0523 06:05:27.135148 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01681 (* 1 = 8.01681 loss)
I0523 06:05:27.969434 34682 sgd_solver.cpp:112] Iteration 61100, lr = 0.01
I0523 06:05:32.698658 34682 solver.cpp:239] Iteration 61110 (1.79751 iter/s, 5.56327s/10 iters), loss = 8.7873
I0523 06:05:32.698709 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7873 (* 1 = 8.7873 loss)
I0523 06:05:32.771623 34682 sgd_solver.cpp:112] Iteration 61110, lr = 0.01
I0523 06:05:37.446030 34682 solver.cpp:239] Iteration 61120 (2.10653 iter/s, 4.74713s/10 iters), loss = 7.77882
I0523 06:05:37.446084 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77882 (* 1 = 7.77882 loss)
I0523 06:05:37.518463 34682 sgd_solver.cpp:112] Iteration 61120, lr = 0.01
I0523 06:05:40.539218 34682 solver.cpp:239] Iteration 61130 (3.2331 iter/s, 3.093s/10 iters), loss = 7.25278
I0523 06:05:40.539260 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.25278 (* 1 = 7.25278 loss)
I0523 06:05:41.383472 34682 sgd_solver.cpp:112] Iteration 61130, lr = 0.01
I0523 06:05:45.490869 34682 solver.cpp:239] Iteration 61140 (2.01963 iter/s, 4.95141s/10 iters), loss = 8.08752
I0523 06:05:45.490919 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08752 (* 1 = 8.08752 loss)
I0523 06:05:46.261840 34682 sgd_solver.cpp:112] Iteration 61140, lr = 0.01
I0523 06:05:51.545615 34682 solver.cpp:239] Iteration 61150 (1.65168 iter/s, 6.05446s/10 iters), loss = 7.65475
I0523 06:05:51.545718 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.65475 (* 1 = 7.65475 loss)
I0523 06:05:51.615576 34682 sgd_solver.cpp:112] Iteration 61150, lr = 0.01
I0523 06:05:56.410643 34682 solver.cpp:239] Iteration 61160 (2.05561 iter/s, 4.86473s/10 iters), loss = 8.04947
I0523 06:05:56.410687 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04947 (* 1 = 8.04947 loss)
I0523 06:05:56.811642 34682 sgd_solver.cpp:112] Iteration 61160, lr = 0.01
I0523 06:06:02.321812 34682 solver.cpp:239] Iteration 61170 (1.69179 iter/s, 5.91089s/10 iters), loss = 8.78819
I0523 06:06:02.321862 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78819 (* 1 = 8.78819 loss)
I0523 06:06:02.570161 34682 sgd_solver.cpp:112] Iteration 61170, lr = 0.01
I0523 06:06:07.377964 34682 solver.cpp:239] Iteration 61180 (1.97789 iter/s, 5.05589s/10 iters), loss = 7.22105
I0523 06:06:07.378010 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.22105 (* 1 = 7.22105 loss)
I0523 06:06:07.437253 34682 sgd_solver.cpp:112] Iteration 61180, lr = 0.01
I0523 06:06:10.915339 34682 solver.cpp:239] Iteration 61190 (2.82711 iter/s, 3.53718s/10 iters), loss = 8.70676
I0523 06:06:10.915382 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70676 (* 1 = 8.70676 loss)
I0523 06:06:10.973991 34682 sgd_solver.cpp:112] Iteration 61190, lr = 0.01
I0523 06:06:14.370152 34682 solver.cpp:239] Iteration 61200 (2.89467 iter/s, 3.45462s/10 iters), loss = 7.46488
I0523 06:06:14.370200 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.46488 (* 1 = 7.46488 loss)
I0523 06:06:14.430984 34682 sgd_solver.cpp:112] Iteration 61200, lr = 0.01
I0523 06:06:19.771373 34682 solver.cpp:239] Iteration 61210 (1.85153 iter/s, 5.40093s/10 iters), loss = 8.66611
I0523 06:06:19.771437 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66611 (* 1 = 8.66611 loss)
I0523 06:06:20.385594 34682 sgd_solver.cpp:112] Iteration 61210, lr = 0.01
I0523 06:06:25.911013 34682 solver.cpp:239] Iteration 61220 (1.62884 iter/s, 6.13933s/10 iters), loss = 8.85606
I0523 06:06:25.911244 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85606 (* 1 = 8.85606 loss)
I0523 06:06:25.980778 34682 sgd_solver.cpp:112] Iteration 61220, lr = 0.01
I0523 06:06:30.894804 34682 solver.cpp:239] Iteration 61230 (2.00668 iter/s, 4.98337s/10 iters), loss = 7.91864
I0523 06:06:30.894865 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91864 (* 1 = 7.91864 loss)
I0523 06:06:31.681265 34682 sgd_solver.cpp:112] Iteration 61230, lr = 0.01
I0523 06:06:36.812309 34682 solver.cpp:239] Iteration 61240 (1.68999 iter/s, 5.91721s/10 iters), loss = 8.11691
I0523 06:06:36.812363 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11691 (* 1 = 8.11691 loss)
I0523 06:06:36.876371 34682 sgd_solver.cpp:112] Iteration 61240, lr = 0.01
I0523 06:06:39.495775 34682 solver.cpp:239] Iteration 61250 (3.72677 iter/s, 2.68329s/10 iters), loss = 8.67585
I0523 06:06:39.495843 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67585 (* 1 = 8.67585 loss)
I0523 06:06:40.277519 34682 sgd_solver.cpp:112] Iteration 61250, lr = 0.01
I0523 06:06:44.643023 34682 solver.cpp:239] Iteration 61260 (1.94289 iter/s, 5.14698s/10 iters), loss = 8.33915
I0523 06:06:44.643074 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33915 (* 1 = 8.33915 loss)
I0523 06:06:44.733268 34682 sgd_solver.cpp:112] Iteration 61260, lr = 0.01
I0523 06:06:48.857421 34682 solver.cpp:239] Iteration 61270 (2.37295 iter/s, 4.21417s/10 iters), loss = 8.42567
I0523 06:06:48.857481 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42567 (* 1 = 8.42567 loss)
I0523 06:06:48.926141 34682 sgd_solver.cpp:112] Iteration 61270, lr = 0.01
I0523 06:06:55.403156 34682 solver.cpp:239] Iteration 61280 (1.52779 iter/s, 6.54541s/10 iters), loss = 8.57683
I0523 06:06:55.403206 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57683 (* 1 = 8.57683 loss)
I0523 06:06:55.465155 34682 sgd_solver.cpp:112] Iteration 61280, lr = 0.01
I0523 06:06:59.600766 34682 solver.cpp:239] Iteration 61290 (2.38243 iter/s, 4.19739s/10 iters), loss = 7.42945
I0523 06:06:59.600873 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42945 (* 1 = 7.42945 loss)
I0523 06:06:59.671833 34682 sgd_solver.cpp:112] Iteration 61290, lr = 0.01
I0523 06:07:04.558730 34682 solver.cpp:239] Iteration 61300 (2.01709 iter/s, 4.95763s/10 iters), loss = 7.58747
I0523 06:07:04.558787 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58747 (* 1 = 7.58747 loss)
I0523 06:07:05.401825 34682 sgd_solver.cpp:112] Iteration 61300, lr = 0.01
I0523 06:07:11.661231 34682 solver.cpp:239] Iteration 61310 (1.40802 iter/s, 7.10216s/10 iters), loss = 7.94685
I0523 06:07:11.661280 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94685 (* 1 = 7.94685 loss)
I0523 06:07:11.729550 34682 sgd_solver.cpp:112] Iteration 61310, lr = 0.01
I0523 06:07:16.528406 34682 solver.cpp:239] Iteration 61320 (2.05469 iter/s, 4.86692s/10 iters), loss = 7.87615
I0523 06:07:16.528450 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87615 (* 1 = 7.87615 loss)
I0523 06:07:17.382674 34682 sgd_solver.cpp:112] Iteration 61320, lr = 0.01
I0523 06:07:21.611165 34682 solver.cpp:239] Iteration 61330 (1.96753 iter/s, 5.08251s/10 iters), loss = 7.63381
I0523 06:07:21.611205 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63381 (* 1 = 7.63381 loss)
I0523 06:07:21.679978 34682 sgd_solver.cpp:112] Iteration 61330, lr = 0.01
I0523 06:07:28.638651 34682 solver.cpp:239] Iteration 61340 (1.42305 iter/s, 7.02716s/10 iters), loss = 8.41871
I0523 06:07:28.638725 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41871 (* 1 = 8.41871 loss)
I0523 06:07:28.710080 34682 sgd_solver.cpp:112] Iteration 61340, lr = 0.01
I0523 06:07:34.346747 34682 solver.cpp:239] Iteration 61350 (1.75199 iter/s, 5.70779s/10 iters), loss = 7.83536
I0523 06:07:34.347052 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83536 (* 1 = 7.83536 loss)
I0523 06:07:34.412711 34682 sgd_solver.cpp:112] Iteration 61350, lr = 0.01
I0523 06:07:39.904122 34682 solver.cpp:239] Iteration 61360 (1.79957 iter/s, 5.55688s/10 iters), loss = 8.72679
I0523 06:07:39.904186 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72679 (* 1 = 8.72679 loss)
I0523 06:07:39.987339 34682 sgd_solver.cpp:112] Iteration 61360, lr = 0.01
I0523 06:07:46.520045 34682 solver.cpp:239] Iteration 61370 (1.51158 iter/s, 6.61559s/10 iters), loss = 7.05849
I0523 06:07:46.520110 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.05849 (* 1 = 7.05849 loss)
I0523 06:07:46.578150 34682 sgd_solver.cpp:112] Iteration 61370, lr = 0.01
I0523 06:07:52.131430 34682 solver.cpp:239] Iteration 61380 (1.78219 iter/s, 5.61109s/10 iters), loss = 8.00791
I0523 06:07:52.131491 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00791 (* 1 = 8.00791 loss)
I0523 06:07:52.695354 34682 sgd_solver.cpp:112] Iteration 61380, lr = 0.01
I0523 06:07:58.075145 34682 solver.cpp:239] Iteration 61390 (1.68254 iter/s, 5.94341s/10 iters), loss = 8.59448
I0523 06:07:58.075217 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59448 (* 1 = 8.59448 loss)
I0523 06:07:58.863919 34682 sgd_solver.cpp:112] Iteration 61390, lr = 0.01
I0523 06:08:03.450562 34682 solver.cpp:239] Iteration 61400 (1.86042 iter/s, 5.37512s/10 iters), loss = 8.30466
I0523 06:08:03.450609 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30466 (* 1 = 8.30466 loss)
I0523 06:08:04.299553 34682 sgd_solver.cpp:112] Iteration 61400, lr = 0.01
I0523 06:08:09.938953 34682 solver.cpp:239] Iteration 61410 (1.54129 iter/s, 6.48808s/10 iters), loss = 8.11678
I0523 06:08:09.939085 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11678 (* 1 = 8.11678 loss)
I0523 06:08:10.001906 34682 sgd_solver.cpp:112] Iteration 61410, lr = 0.01
I0523 06:08:13.936705 34682 solver.cpp:239] Iteration 61420 (2.50159 iter/s, 3.99745s/10 iters), loss = 8.82502
I0523 06:08:13.936754 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82502 (* 1 = 8.82502 loss)
I0523 06:08:14.010072 34682 sgd_solver.cpp:112] Iteration 61420, lr = 0.01
I0523 06:08:18.759011 34682 solver.cpp:239] Iteration 61430 (2.07381 iter/s, 4.82205s/10 iters), loss = 9.26663
I0523 06:08:18.759054 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.26663 (* 1 = 9.26663 loss)
I0523 06:08:18.819109 34682 sgd_solver.cpp:112] Iteration 61430, lr = 0.01
I0523 06:08:23.242862 34682 solver.cpp:239] Iteration 61440 (2.23034 iter/s, 4.48362s/10 iters), loss = 7.67641
I0523 06:08:23.242921 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.67641 (* 1 = 7.67641 loss)
I0523 06:08:23.962920 34682 sgd_solver.cpp:112] Iteration 61440, lr = 0.01
I0523 06:08:28.688848 34682 solver.cpp:239] Iteration 61450 (1.83631 iter/s, 5.44571s/10 iters), loss = 8.42057
I0523 06:08:28.688905 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42057 (* 1 = 8.42057 loss)
I0523 06:08:28.769345 34682 sgd_solver.cpp:112] Iteration 61450, lr = 0.01
I0523 06:08:34.072960 34682 solver.cpp:239] Iteration 61460 (1.85741 iter/s, 5.38384s/10 iters), loss = 8.40808
I0523 06:08:34.073012 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40808 (* 1 = 8.40808 loss)
I0523 06:08:34.922863 34682 sgd_solver.cpp:112] Iteration 61460, lr = 0.01
I0523 06:08:38.336092 34682 solver.cpp:239] Iteration 61470 (2.34582 iter/s, 4.26289s/10 iters), loss = 7.85782
I0523 06:08:38.336149 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.85782 (* 1 = 7.85782 loss)
I0523 06:08:38.713037 34682 sgd_solver.cpp:112] Iteration 61470, lr = 0.01
I0523 06:08:42.105659 34682 solver.cpp:239] Iteration 61480 (2.65298 iter/s, 3.76935s/10 iters), loss = 8.33217
I0523 06:08:42.105913 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33217 (* 1 = 8.33217 loss)
I0523 06:08:42.174268 34682 sgd_solver.cpp:112] Iteration 61480, lr = 0.01
I0523 06:08:45.258958 34682 solver.cpp:239] Iteration 61490 (3.17164 iter/s, 3.15294s/10 iters), loss = 8.29825
I0523 06:08:45.259001 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29825 (* 1 = 8.29825 loss)
I0523 06:08:45.304603 34682 sgd_solver.cpp:112] Iteration 61490, lr = 0.01
I0523 06:08:46.484447 34682 solver.cpp:239] Iteration 61500 (8.16071 iter/s, 1.22538s/10 iters), loss = 7.17301
I0523 06:08:46.484501 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.17301 (* 1 = 7.17301 loss)
I0523 06:08:46.657282 34682 sgd_solver.cpp:112] Iteration 61500, lr = 0.01
I0523 06:08:47.802490 34682 solver.cpp:239] Iteration 61510 (7.5877 iter/s, 1.31792s/10 iters), loss = 8.02622
I0523 06:08:47.802542 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02622 (* 1 = 8.02622 loss)
I0523 06:08:47.851717 34682 sgd_solver.cpp:112] Iteration 61510, lr = 0.01
I0523 06:08:49.104197 34682 solver.cpp:239] Iteration 61520 (7.68292 iter/s, 1.30159s/10 iters), loss = 8.42955
I0523 06:08:49.104254 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42955 (* 1 = 8.42955 loss)
I0523 06:08:49.151216 34682 sgd_solver.cpp:112] Iteration 61520, lr = 0.01
I0523 06:08:50.725790 34682 solver.cpp:239] Iteration 61530 (6.17571 iter/s, 1.61925s/10 iters), loss = 8.54912
I0523 06:08:50.725831 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54912 (* 1 = 8.54912 loss)
I0523 06:08:50.764598 34682 sgd_solver.cpp:112] Iteration 61530, lr = 0.01
I0523 06:08:53.842824 34682 solver.cpp:239] Iteration 61540 (3.20836 iter/s, 3.11686s/10 iters), loss = 8.90863
I0523 06:08:53.842871 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90863 (* 1 = 8.90863 loss)
I0523 06:08:54.691450 34682 sgd_solver.cpp:112] Iteration 61540, lr = 0.01
I0523 06:08:59.444394 34682 solver.cpp:239] Iteration 61550 (1.7853 iter/s, 5.60129s/10 iters), loss = 8.06571
I0523 06:08:59.444449 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06571 (* 1 = 8.06571 loss)
I0523 06:09:00.306537 34682 sgd_solver.cpp:112] Iteration 61550, lr = 0.01
I0523 06:09:04.816828 34682 solver.cpp:239] Iteration 61560 (1.86145 iter/s, 5.37216s/10 iters), loss = 8.17918
I0523 06:09:04.816882 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17918 (* 1 = 8.17918 loss)
I0523 06:09:04.896224 34682 sgd_solver.cpp:112] Iteration 61560, lr = 0.01
I0523 06:09:11.421000 34682 solver.cpp:239] Iteration 61570 (1.51427 iter/s, 6.60385s/10 iters), loss = 8.35783
I0523 06:09:11.421056 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35783 (* 1 = 8.35783 loss)
I0523 06:09:12.293143 34682 sgd_solver.cpp:112] Iteration 61570, lr = 0.01
I0523 06:09:17.207092 34682 solver.cpp:239] Iteration 61580 (1.72837 iter/s, 5.78581s/10 iters), loss = 7.74551
I0523 06:09:17.207141 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74551 (* 1 = 7.74551 loss)
I0523 06:09:18.041441 34682 sgd_solver.cpp:112] Iteration 61580, lr = 0.01
I0523 06:09:21.124573 34682 solver.cpp:239] Iteration 61590 (2.5528 iter/s, 3.91726s/10 iters), loss = 8.28091
I0523 06:09:21.124631 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28091 (* 1 = 8.28091 loss)
I0523 06:09:21.183130 34682 sgd_solver.cpp:112] Iteration 61590, lr = 0.01
I0523 06:09:23.251735 34682 solver.cpp:239] Iteration 61600 (4.70144 iter/s, 2.12701s/10 iters), loss = 7.93705
I0523 06:09:23.251797 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93705 (* 1 = 7.93705 loss)
I0523 06:09:23.303844 34682 sgd_solver.cpp:112] Iteration 61600, lr = 0.01
I0523 06:09:27.035872 34682 solver.cpp:239] Iteration 61610 (2.64277 iter/s, 3.78391s/10 iters), loss = 7.38163
I0523 06:09:27.035926 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.38163 (* 1 = 7.38163 loss)
I0523 06:09:27.122074 34682 sgd_solver.cpp:112] Iteration 61610, lr = 0.01
I0523 06:09:29.820150 34682 solver.cpp:239] Iteration 61620 (3.59749 iter/s, 2.77972s/10 iters), loss = 8.11835
I0523 06:09:29.820197 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11835 (* 1 = 8.11835 loss)
I0523 06:09:29.891782 34682 sgd_solver.cpp:112] Iteration 61620, lr = 0.01
I0523 06:09:35.448748 34682 solver.cpp:239] Iteration 61630 (1.77673 iter/s, 5.62832s/10 iters), loss = 7.93334
I0523 06:09:35.448793 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93334 (* 1 = 7.93334 loss)
I0523 06:09:35.510082 34682 sgd_solver.cpp:112] Iteration 61630, lr = 0.01
I0523 06:09:38.636642 34682 solver.cpp:239] Iteration 61640 (3.13705 iter/s, 3.18771s/10 iters), loss = 8.37508
I0523 06:09:38.636700 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37508 (* 1 = 8.37508 loss)
I0523 06:09:38.711215 34682 sgd_solver.cpp:112] Iteration 61640, lr = 0.01
I0523 06:09:46.011015 34682 solver.cpp:239] Iteration 61650 (1.35611 iter/s, 7.37402s/10 iters), loss = 8.92975
I0523 06:09:46.011229 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92975 (* 1 = 8.92975 loss)
I0523 06:09:46.125600 34682 sgd_solver.cpp:112] Iteration 61650, lr = 0.01
I0523 06:09:49.546836 34682 solver.cpp:239] Iteration 61660 (2.82848 iter/s, 3.53546s/10 iters), loss = 8.8638
I0523 06:09:49.546886 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.8638 (* 1 = 8.8638 loss)
I0523 06:09:50.340760 34682 sgd_solver.cpp:112] Iteration 61660, lr = 0.01
I0523 06:09:56.587415 34682 solver.cpp:239] Iteration 61670 (1.42041 iter/s, 7.04024s/10 iters), loss = 8.96212
I0523 06:09:56.587462 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96212 (* 1 = 8.96212 loss)
I0523 06:09:56.668488 34682 sgd_solver.cpp:112] Iteration 61670, lr = 0.01
I0523 06:09:59.448145 34682 solver.cpp:239] Iteration 61680 (3.49582 iter/s, 2.86056s/10 iters), loss = 7.8375
I0523 06:09:59.448209 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8375 (* 1 = 7.8375 loss)
I0523 06:09:59.510246 34682 sgd_solver.cpp:112] Iteration 61680, lr = 0.01
I0523 06:10:03.573330 34682 solver.cpp:239] Iteration 61690 (2.42427 iter/s, 4.12495s/10 iters), loss = 9.06524
I0523 06:10:03.573385 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.06524 (* 1 = 9.06524 loss)
I0523 06:10:04.372830 34682 sgd_solver.cpp:112] Iteration 61690, lr = 0.01
I0523 06:10:08.522083 34682 solver.cpp:239] Iteration 61700 (2.02082 iter/s, 4.94849s/10 iters), loss = 8.37902
I0523 06:10:08.522132 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37902 (* 1 = 8.37902 loss)
I0523 06:10:08.955874 34682 sgd_solver.cpp:112] Iteration 61700, lr = 0.01
I0523 06:10:14.795377 34682 solver.cpp:239] Iteration 61710 (1.59414 iter/s, 6.27298s/10 iters), loss = 8.6179
I0523 06:10:14.795430 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6179 (* 1 = 8.6179 loss)
I0523 06:10:14.870427 34682 sgd_solver.cpp:112] Iteration 61710, lr = 0.01
I0523 06:10:19.471894 34682 solver.cpp:239] Iteration 61720 (2.13846 iter/s, 4.67627s/10 iters), loss = 7.94492
I0523 06:10:19.472065 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94492 (* 1 = 7.94492 loss)
I0523 06:10:19.539526 34682 sgd_solver.cpp:112] Iteration 61720, lr = 0.01
I0523 06:10:26.004115 34682 solver.cpp:239] Iteration 61730 (1.53097 iter/s, 6.53179s/10 iters), loss = 8.38438
I0523 06:10:26.004181 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38438 (* 1 = 8.38438 loss)
I0523 06:10:26.518352 34682 sgd_solver.cpp:112] Iteration 61730, lr = 0.01
I0523 06:10:30.107147 34682 solver.cpp:239] Iteration 61740 (2.43736 iter/s, 4.1028s/10 iters), loss = 6.95965
I0523 06:10:30.107192 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.95965 (* 1 = 6.95965 loss)
I0523 06:10:30.182394 34682 sgd_solver.cpp:112] Iteration 61740, lr = 0.01
I0523 06:10:35.186060 34682 solver.cpp:239] Iteration 61750 (1.96902 iter/s, 5.07866s/10 iters), loss = 8.23407
I0523 06:10:35.186100 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23407 (* 1 = 8.23407 loss)
I0523 06:10:35.258833 34682 sgd_solver.cpp:112] Iteration 61750, lr = 0.01
I0523 06:10:39.678483 34682 solver.cpp:239] Iteration 61760 (2.22608 iter/s, 4.4922s/10 iters), loss = 8.52906
I0523 06:10:39.678525 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52906 (* 1 = 8.52906 loss)
I0523 06:10:39.752831 34682 sgd_solver.cpp:112] Iteration 61760, lr = 0.01
I0523 06:10:44.568717 34682 solver.cpp:239] Iteration 61770 (2.045 iter/s, 4.88998s/10 iters), loss = 8.50751
I0523 06:10:44.568783 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50751 (* 1 = 8.50751 loss)
I0523 06:10:44.647068 34682 sgd_solver.cpp:112] Iteration 61770, lr = 0.01
I0523 06:10:48.988243 34682 solver.cpp:239] Iteration 61780 (2.26281 iter/s, 4.41929s/10 iters), loss = 7.95633
I0523 06:10:48.988288 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95633 (* 1 = 7.95633 loss)
I0523 06:10:49.822023 34682 sgd_solver.cpp:112] Iteration 61780, lr = 0.01
I0523 06:10:56.044541 34682 solver.cpp:239] Iteration 61790 (1.41724 iter/s, 7.05597s/10 iters), loss = 8.50405
I0523 06:10:56.044589 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50405 (* 1 = 8.50405 loss)
I0523 06:10:56.125757 34682 sgd_solver.cpp:112] Iteration 61790, lr = 0.01
I0523 06:11:01.747815 34682 solver.cpp:239] Iteration 61800 (1.75346 iter/s, 5.703s/10 iters), loss = 7.83318
I0523 06:11:01.747853 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83318 (* 1 = 7.83318 loss)
I0523 06:11:01.820986 34682 sgd_solver.cpp:112] Iteration 61800, lr = 0.01
I0523 06:11:07.340188 34682 solver.cpp:239] Iteration 61810 (1.78823 iter/s, 5.59211s/10 iters), loss = 8.29842
I0523 06:11:07.340227 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29842 (* 1 = 8.29842 loss)
I0523 06:11:07.411494 34682 sgd_solver.cpp:112] Iteration 61810, lr = 0.01
I0523 06:11:11.680822 34682 solver.cpp:239] Iteration 61820 (2.30393 iter/s, 4.34041s/10 iters), loss = 8.38066
I0523 06:11:11.680881 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38066 (* 1 = 8.38066 loss)
I0523 06:11:12.378866 34682 sgd_solver.cpp:112] Iteration 61820, lr = 0.01
I0523 06:11:15.705637 34682 solver.cpp:239] Iteration 61830 (2.48473 iter/s, 4.02459s/10 iters), loss = 7.11974
I0523 06:11:15.705685 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.11974 (* 1 = 7.11974 loss)
I0523 06:11:15.769182 34682 sgd_solver.cpp:112] Iteration 61830, lr = 0.01
I0523 06:11:20.696857 34682 solver.cpp:239] Iteration 61840 (2.00365 iter/s, 4.99088s/10 iters), loss = 8.86821
I0523 06:11:20.697062 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86821 (* 1 = 8.86821 loss)
I0523 06:11:20.913405 34682 sgd_solver.cpp:112] Iteration 61840, lr = 0.01
I0523 06:11:24.233531 34682 solver.cpp:239] Iteration 61850 (2.82779 iter/s, 3.53633s/10 iters), loss = 8.05058
I0523 06:11:24.233585 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05058 (* 1 = 8.05058 loss)
I0523 06:11:24.960243 34682 sgd_solver.cpp:112] Iteration 61850, lr = 0.01
I0523 06:11:29.080986 34682 solver.cpp:239] Iteration 61860 (2.06305 iter/s, 4.8472s/10 iters), loss = 8.58225
I0523 06:11:29.081037 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58225 (* 1 = 8.58225 loss)
I0523 06:11:29.152998 34682 sgd_solver.cpp:112] Iteration 61860, lr = 0.01
I0523 06:11:33.493643 34682 solver.cpp:239] Iteration 61870 (2.26633 iter/s, 4.41242s/10 iters), loss = 8.26445
I0523 06:11:33.493680 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26445 (* 1 = 8.26445 loss)
I0523 06:11:33.568138 34682 sgd_solver.cpp:112] Iteration 61870, lr = 0.01
I0523 06:11:38.173215 34682 solver.cpp:239] Iteration 61880 (2.13705 iter/s, 4.67934s/10 iters), loss = 8.15872
I0523 06:11:38.173275 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15872 (* 1 = 8.15872 loss)
I0523 06:11:39.037597 34682 sgd_solver.cpp:112] Iteration 61880, lr = 0.01
I0523 06:11:43.240310 34682 solver.cpp:239] Iteration 61890 (1.97362 iter/s, 5.06683s/10 iters), loss = 7.36325
I0523 06:11:43.240366 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.36325 (* 1 = 7.36325 loss)
I0523 06:11:43.302613 34682 sgd_solver.cpp:112] Iteration 61890, lr = 0.01
I0523 06:11:46.257475 34682 solver.cpp:239] Iteration 61900 (3.31457 iter/s, 3.01699s/10 iters), loss = 7.79003
I0523 06:11:46.257522 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79003 (* 1 = 7.79003 loss)
I0523 06:11:47.088063 34682 sgd_solver.cpp:112] Iteration 61900, lr = 0.01
I0523 06:11:51.960522 34682 solver.cpp:239] Iteration 61910 (1.75353 iter/s, 5.70277s/10 iters), loss = 8.53642
I0523 06:11:51.960700 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53642 (* 1 = 8.53642 loss)
I0523 06:11:52.042497 34682 sgd_solver.cpp:112] Iteration 61910, lr = 0.01
I0523 06:11:57.240756 34682 solver.cpp:239] Iteration 61920 (1.894 iter/s, 5.27983s/10 iters), loss = 8.66417
I0523 06:11:57.240803 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66417 (* 1 = 8.66417 loss)
I0523 06:11:57.308465 34682 sgd_solver.cpp:112] Iteration 61920, lr = 0.01
I0523 06:11:59.876370 34682 solver.cpp:239] Iteration 61930 (3.79441 iter/s, 2.63545s/10 iters), loss = 8.2783
I0523 06:11:59.876412 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2783 (* 1 = 8.2783 loss)
I0523 06:11:59.953336 34682 sgd_solver.cpp:112] Iteration 61930, lr = 0.01
I0523 06:12:04.615182 34682 solver.cpp:239] Iteration 61940 (2.11034 iter/s, 4.73857s/10 iters), loss = 8.89192
I0523 06:12:04.615229 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89192 (* 1 = 8.89192 loss)
I0523 06:12:05.445181 34682 sgd_solver.cpp:112] Iteration 61940, lr = 0.01
I0523 06:12:08.830464 34682 solver.cpp:239] Iteration 61950 (2.37245 iter/s, 4.21506s/10 iters), loss = 8.34908
I0523 06:12:08.830514 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34908 (* 1 = 8.34908 loss)
I0523 06:12:09.658547 34682 sgd_solver.cpp:112] Iteration 61950, lr = 0.01
I0523 06:12:14.574796 34682 solver.cpp:239] Iteration 61960 (1.74093 iter/s, 5.74405s/10 iters), loss = 7.83434
I0523 06:12:14.574848 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83434 (* 1 = 7.83434 loss)
I0523 06:12:14.633544 34682 sgd_solver.cpp:112] Iteration 61960, lr = 0.01
I0523 06:12:19.628362 34682 solver.cpp:239] Iteration 61970 (1.9789 iter/s, 5.0533s/10 iters), loss = 8.29282
I0523 06:12:19.628417 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29282 (* 1 = 8.29282 loss)
I0523 06:12:19.709197 34682 sgd_solver.cpp:112] Iteration 61970, lr = 0.01
I0523 06:12:24.620187 34682 solver.cpp:239] Iteration 61980 (2.00339 iter/s, 4.99155s/10 iters), loss = 7.88031
I0523 06:12:24.620414 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88031 (* 1 = 7.88031 loss)
I0523 06:12:24.684957 34682 sgd_solver.cpp:112] Iteration 61980, lr = 0.01
I0523 06:12:28.719382 34682 solver.cpp:239] Iteration 61990 (2.43973 iter/s, 4.09882s/10 iters), loss = 8.28962
I0523 06:12:28.719434 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28962 (* 1 = 8.28962 loss)
I0523 06:12:28.788719 34682 sgd_solver.cpp:112] Iteration 61990, lr = 0.01
I0523 06:12:35.178138 34682 solver.cpp:239] Iteration 62000 (1.54836 iter/s, 6.45844s/10 iters), loss = 7.9822
I0523 06:12:35.178189 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9822 (* 1 = 7.9822 loss)
I0523 06:12:35.243508 34682 sgd_solver.cpp:112] Iteration 62000, lr = 0.01
I0523 06:12:39.276558 34682 solver.cpp:239] Iteration 62010 (2.4401 iter/s, 4.0982s/10 iters), loss = 9.12991
I0523 06:12:39.276602 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12991 (* 1 = 9.12991 loss)
I0523 06:12:39.347267 34682 sgd_solver.cpp:112] Iteration 62010, lr = 0.01
I0523 06:12:43.437716 34682 solver.cpp:239] Iteration 62020 (2.40331 iter/s, 4.16093s/10 iters), loss = 7.96658
I0523 06:12:43.437783 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96658 (* 1 = 7.96658 loss)
I0523 06:12:44.193019 34682 sgd_solver.cpp:112] Iteration 62020, lr = 0.01
I0523 06:12:46.653275 34682 solver.cpp:239] Iteration 62030 (3.11007 iter/s, 3.21536s/10 iters), loss = 7.9259
I0523 06:12:46.653328 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9259 (* 1 = 7.9259 loss)
I0523 06:12:47.547508 34682 sgd_solver.cpp:112] Iteration 62030, lr = 0.01
I0523 06:12:53.088963 34682 solver.cpp:239] Iteration 62040 (1.55391 iter/s, 6.43538s/10 iters), loss = 8.49523
I0523 06:12:53.089006 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49523 (* 1 = 8.49523 loss)
I0523 06:12:53.156049 34682 sgd_solver.cpp:112] Iteration 62040, lr = 0.01
I0523 06:12:57.619119 34682 solver.cpp:239] Iteration 62050 (2.20754 iter/s, 4.52993s/10 iters), loss = 8.1354
I0523 06:12:57.619407 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1354 (* 1 = 8.1354 loss)
I0523 06:12:57.690924 34682 sgd_solver.cpp:112] Iteration 62050, lr = 0.01
I0523 06:13:02.537662 34682 solver.cpp:239] Iteration 62060 (2.03331 iter/s, 4.91808s/10 iters), loss = 8.59823
I0523 06:13:02.537709 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59823 (* 1 = 8.59823 loss)
I0523 06:13:02.605366 34682 sgd_solver.cpp:112] Iteration 62060, lr = 0.01
I0523 06:13:05.975522 34682 solver.cpp:239] Iteration 62070 (2.90895 iter/s, 3.43767s/10 iters), loss = 8.35667
I0523 06:13:05.975564 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35667 (* 1 = 8.35667 loss)
I0523 06:13:06.057427 34682 sgd_solver.cpp:112] Iteration 62070, lr = 0.01
I0523 06:13:10.925416 34682 solver.cpp:239] Iteration 62080 (2.02035 iter/s, 4.94965s/10 iters), loss = 7.34837
I0523 06:13:10.925462 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.34837 (* 1 = 7.34837 loss)
I0523 06:13:10.982391 34682 sgd_solver.cpp:112] Iteration 62080, lr = 0.01
I0523 06:13:16.652161 34682 solver.cpp:239] Iteration 62090 (1.74628 iter/s, 5.72646s/10 iters), loss = 8.2469
I0523 06:13:16.652227 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2469 (* 1 = 8.2469 loss)
I0523 06:13:17.489125 34682 sgd_solver.cpp:112] Iteration 62090, lr = 0.01
I0523 06:13:23.072656 34682 solver.cpp:239] Iteration 62100 (1.55759 iter/s, 6.42016s/10 iters), loss = 8.74371
I0523 06:13:23.072707 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74371 (* 1 = 8.74371 loss)
I0523 06:13:23.858485 34682 sgd_solver.cpp:112] Iteration 62100, lr = 0.01
I0523 06:13:27.130795 34682 solver.cpp:239] Iteration 62110 (2.46432 iter/s, 4.05791s/10 iters), loss = 7.33826
I0523 06:13:27.130864 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.33826 (* 1 = 7.33826 loss)
I0523 06:13:27.967931 34682 sgd_solver.cpp:112] Iteration 62110, lr = 0.01
I0523 06:13:30.679376 34682 solver.cpp:239] Iteration 62120 (2.8182 iter/s, 3.54836s/10 iters), loss = 7.73985
I0523 06:13:30.679428 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.73985 (* 1 = 7.73985 loss)
I0523 06:13:30.763939 34682 sgd_solver.cpp:112] Iteration 62120, lr = 0.01
I0523 06:13:36.049553 34682 solver.cpp:239] Iteration 62130 (1.86223 iter/s, 5.36991s/10 iters), loss = 8.20765
I0523 06:13:36.049594 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20765 (* 1 = 8.20765 loss)
I0523 06:13:36.842733 34682 sgd_solver.cpp:112] Iteration 62130, lr = 0.01
I0523 06:13:42.827677 34682 solver.cpp:239] Iteration 62140 (1.4754 iter/s, 6.77781s/10 iters), loss = 7.83511
I0523 06:13:42.827731 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83511 (* 1 = 7.83511 loss)
I0523 06:13:42.903954 34682 sgd_solver.cpp:112] Iteration 62140, lr = 0.01
I0523 06:13:46.925196 34682 solver.cpp:239] Iteration 62150 (2.44063 iter/s, 4.0973s/10 iters), loss = 8.18025
I0523 06:13:46.925236 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18025 (* 1 = 8.18025 loss)
I0523 06:13:47.002885 34682 sgd_solver.cpp:112] Iteration 62150, lr = 0.01
I0523 06:13:53.582504 34682 solver.cpp:239] Iteration 62160 (1.50218 iter/s, 6.65699s/10 iters), loss = 7.61197
I0523 06:13:53.582556 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.61197 (* 1 = 7.61197 loss)
I0523 06:13:53.657850 34682 sgd_solver.cpp:112] Iteration 62160, lr = 0.01
I0523 06:14:00.013667 34682 solver.cpp:239] Iteration 62170 (1.55501 iter/s, 6.43085s/10 iters), loss = 8.32906
I0523 06:14:00.013978 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32906 (* 1 = 8.32906 loss)
I0523 06:14:00.593374 34682 sgd_solver.cpp:112] Iteration 62170, lr = 0.01
I0523 06:14:04.519896 34682 solver.cpp:239] Iteration 62180 (2.21938 iter/s, 4.50576s/10 iters), loss = 8.48544
I0523 06:14:04.519948 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48544 (* 1 = 8.48544 loss)
I0523 06:14:05.152278 34682 sgd_solver.cpp:112] Iteration 62180, lr = 0.01
I0523 06:14:10.689796 34682 solver.cpp:239] Iteration 62190 (1.62085 iter/s, 6.16959s/10 iters), loss = 7.57212
I0523 06:14:10.689853 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.57212 (* 1 = 7.57212 loss)
I0523 06:14:10.772810 34682 sgd_solver.cpp:112] Iteration 62190, lr = 0.01
I0523 06:14:17.995802 34682 solver.cpp:239] Iteration 62200 (1.3688 iter/s, 7.30565s/10 iters), loss = 7.59076
I0523 06:14:17.995858 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.59076 (* 1 = 7.59076 loss)
I0523 06:14:18.763291 34682 sgd_solver.cpp:112] Iteration 62200, lr = 0.01
I0523 06:14:23.512730 34682 solver.cpp:239] Iteration 62210 (1.8127 iter/s, 5.51663s/10 iters), loss = 8.81451
I0523 06:14:23.512780 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81451 (* 1 = 8.81451 loss)
I0523 06:14:23.583329 34682 sgd_solver.cpp:112] Iteration 62210, lr = 0.01
I0523 06:14:27.595072 34682 solver.cpp:239] Iteration 62220 (2.44971 iter/s, 4.08212s/10 iters), loss = 7.70442
I0523 06:14:27.595124 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.70442 (* 1 = 7.70442 loss)
I0523 06:14:27.659307 34682 sgd_solver.cpp:112] Iteration 62220, lr = 0.01
I0523 06:14:34.073475 34682 solver.cpp:239] Iteration 62230 (1.54366 iter/s, 6.47809s/10 iters), loss = 8.77171
I0523 06:14:34.073675 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77171 (* 1 = 8.77171 loss)
I0523 06:14:34.144812 34682 sgd_solver.cpp:112] Iteration 62230, lr = 0.01
I0523 06:14:40.050678 34682 solver.cpp:239] Iteration 62240 (1.67314 iter/s, 5.97678s/10 iters), loss = 7.94646
I0523 06:14:40.050736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94646 (* 1 = 7.94646 loss)
I0523 06:14:40.567991 34682 sgd_solver.cpp:112] Iteration 62240, lr = 0.01
I0523 06:14:44.906059 34682 solver.cpp:239] Iteration 62250 (2.05968 iter/s, 4.85512s/10 iters), loss = 8.04844
I0523 06:14:44.906105 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04844 (* 1 = 8.04844 loss)
I0523 06:14:44.962990 34682 sgd_solver.cpp:112] Iteration 62250, lr = 0.01
I0523 06:14:49.998766 34682 solver.cpp:239] Iteration 62260 (1.96369 iter/s, 5.09245s/10 iters), loss = 8.89011
I0523 06:14:49.998816 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89011 (* 1 = 8.89011 loss)
I0523 06:14:50.059849 34682 sgd_solver.cpp:112] Iteration 62260, lr = 0.01
I0523 06:14:54.885388 34682 solver.cpp:239] Iteration 62270 (2.04651 iter/s, 4.88637s/10 iters), loss = 7.13814
I0523 06:14:54.885442 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.13814 (* 1 = 7.13814 loss)
I0523 06:14:54.959415 34682 sgd_solver.cpp:112] Iteration 62270, lr = 0.01
I0523 06:14:58.875723 34682 solver.cpp:239] Iteration 62280 (2.50619 iter/s, 3.99011s/10 iters), loss = 8.11484
I0523 06:14:58.875773 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11484 (* 1 = 8.11484 loss)
I0523 06:14:58.934548 34682 sgd_solver.cpp:112] Iteration 62280, lr = 0.01
I0523 06:15:04.126488 34682 solver.cpp:239] Iteration 62290 (1.90458 iter/s, 5.2505s/10 iters), loss = 7.35125
I0523 06:15:04.126806 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.35125 (* 1 = 7.35125 loss)
I0523 06:15:04.819244 34682 sgd_solver.cpp:112] Iteration 62290, lr = 0.01
I0523 06:15:09.426334 34682 solver.cpp:239] Iteration 62300 (1.88703 iter/s, 5.29934s/10 iters), loss = 9.10499
I0523 06:15:09.426388 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10499 (* 1 = 9.10499 loss)
I0523 06:15:09.483691 34682 sgd_solver.cpp:112] Iteration 62300, lr = 0.01
I0523 06:15:13.877882 34682 solver.cpp:239] Iteration 62310 (2.24653 iter/s, 4.45131s/10 iters), loss = 8.07771
I0523 06:15:13.877933 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07771 (* 1 = 8.07771 loss)
I0523 06:15:13.943064 34682 sgd_solver.cpp:112] Iteration 62310, lr = 0.01
I0523 06:15:19.402875 34682 solver.cpp:239] Iteration 62320 (1.81005 iter/s, 5.52471s/10 iters), loss = 7.6107
I0523 06:15:19.402950 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6107 (* 1 = 7.6107 loss)
I0523 06:15:19.577510 34682 sgd_solver.cpp:112] Iteration 62320, lr = 0.01
I0523 06:15:23.504745 34682 solver.cpp:239] Iteration 62330 (2.43806 iter/s, 4.10162s/10 iters), loss = 7.99481
I0523 06:15:23.504791 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99481 (* 1 = 7.99481 loss)
I0523 06:15:24.341339 34682 sgd_solver.cpp:112] Iteration 62330, lr = 0.01
I0523 06:15:28.980350 34682 solver.cpp:239] Iteration 62340 (1.82637 iter/s, 5.47534s/10 iters), loss = 8.25991
I0523 06:15:28.980397 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25991 (* 1 = 8.25991 loss)
I0523 06:15:29.832845 34682 sgd_solver.cpp:112] Iteration 62340, lr = 0.01
I0523 06:15:35.010932 34682 solver.cpp:239] Iteration 62350 (1.6583 iter/s, 6.03028s/10 iters), loss = 7.84431
I0523 06:15:35.011070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84431 (* 1 = 7.84431 loss)
I0523 06:15:35.658263 34682 sgd_solver.cpp:112] Iteration 62350, lr = 0.01
I0523 06:15:39.644649 34682 solver.cpp:239] Iteration 62360 (2.15825 iter/s, 4.63339s/10 iters), loss = 8.30222
I0523 06:15:39.644696 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30222 (* 1 = 8.30222 loss)
I0523 06:15:39.700666 34682 sgd_solver.cpp:112] Iteration 62360, lr = 0.01
I0523 06:15:44.062376 34682 solver.cpp:239] Iteration 62370 (2.26373 iter/s, 4.4175s/10 iters), loss = 8.53091
I0523 06:15:44.062433 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53091 (* 1 = 8.53091 loss)
I0523 06:15:44.730482 34682 sgd_solver.cpp:112] Iteration 62370, lr = 0.01
I0523 06:15:49.196399 34682 solver.cpp:239] Iteration 62380 (1.94789 iter/s, 5.13375s/10 iters), loss = 7.27975
I0523 06:15:49.196444 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.27975 (* 1 = 7.27975 loss)
I0523 06:15:50.038724 34682 sgd_solver.cpp:112] Iteration 62380, lr = 0.01
I0523 06:15:55.620352 34682 solver.cpp:239] Iteration 62390 (1.55675 iter/s, 6.42364s/10 iters), loss = 8.15074
I0523 06:15:55.620396 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15074 (* 1 = 8.15074 loss)
I0523 06:15:55.680989 34682 sgd_solver.cpp:112] Iteration 62390, lr = 0.01
I0523 06:15:59.914419 34682 solver.cpp:239] Iteration 62400 (2.32892 iter/s, 4.29383s/10 iters), loss = 8.22583
I0523 06:15:59.914474 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22583 (* 1 = 8.22583 loss)
I0523 06:16:00.676914 34682 sgd_solver.cpp:112] Iteration 62400, lr = 0.01
I0523 06:16:04.604182 34682 solver.cpp:239] Iteration 62410 (2.13242 iter/s, 4.68951s/10 iters), loss = 7.75897
I0523 06:16:04.604228 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75897 (* 1 = 7.75897 loss)
I0523 06:16:05.483868 34682 sgd_solver.cpp:112] Iteration 62410, lr = 0.01
I0523 06:16:10.406219 34682 solver.cpp:239] Iteration 62420 (1.72362 iter/s, 5.80176s/10 iters), loss = 7.90645
I0523 06:16:10.406268 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90645 (* 1 = 7.90645 loss)
I0523 06:16:11.234299 34682 sgd_solver.cpp:112] Iteration 62420, lr = 0.01
I0523 06:16:15.756726 34682 solver.cpp:239] Iteration 62430 (1.86907 iter/s, 5.35024s/10 iters), loss = 8.03911
I0523 06:16:15.756767 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03911 (* 1 = 8.03911 loss)
I0523 06:16:15.825811 34682 sgd_solver.cpp:112] Iteration 62430, lr = 0.01
I0523 06:16:20.827060 34682 solver.cpp:239] Iteration 62440 (1.97235 iter/s, 5.07009s/10 iters), loss = 8.66092
I0523 06:16:20.827111 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66092 (* 1 = 8.66092 loss)
I0523 06:16:21.597790 34682 sgd_solver.cpp:112] Iteration 62440, lr = 0.01
I0523 06:16:26.338958 34682 solver.cpp:239] Iteration 62450 (1.81435 iter/s, 5.51161s/10 iters), loss = 8.87862
I0523 06:16:26.339005 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87862 (* 1 = 8.87862 loss)
I0523 06:16:26.406827 34682 sgd_solver.cpp:112] Iteration 62450, lr = 0.01
I0523 06:16:30.807878 34682 solver.cpp:239] Iteration 62460 (2.2378 iter/s, 4.46868s/10 iters), loss = 8.54194
I0523 06:16:30.807924 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54194 (* 1 = 8.54194 loss)
I0523 06:16:31.427641 34682 sgd_solver.cpp:112] Iteration 62460, lr = 0.01
I0523 06:16:36.279167 34682 solver.cpp:239] Iteration 62470 (1.82782 iter/s, 5.47101s/10 iters), loss = 8.13462
I0523 06:16:36.279417 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13462 (* 1 = 8.13462 loss)
I0523 06:16:37.089630 34682 sgd_solver.cpp:112] Iteration 62470, lr = 0.01
I0523 06:16:42.322922 34682 solver.cpp:239] Iteration 62480 (1.65473 iter/s, 6.04327s/10 iters), loss = 8.79807
I0523 06:16:42.322973 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79807 (* 1 = 8.79807 loss)
I0523 06:16:42.379753 34682 sgd_solver.cpp:112] Iteration 62480, lr = 0.01
I0523 06:16:48.025405 34682 solver.cpp:239] Iteration 62490 (1.75371 iter/s, 5.7022s/10 iters), loss = 9.09275
I0523 06:16:48.025460 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.09275 (* 1 = 9.09275 loss)
I0523 06:16:48.088809 34682 sgd_solver.cpp:112] Iteration 62490, lr = 0.01
I0523 06:16:52.529227 34682 solver.cpp:239] Iteration 62500 (2.22045 iter/s, 4.50358s/10 iters), loss = 8.42342
I0523 06:16:52.529271 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42342 (* 1 = 8.42342 loss)
I0523 06:16:53.409003 34682 sgd_solver.cpp:112] Iteration 62500, lr = 0.01
I0523 06:16:56.924984 34682 solver.cpp:239] Iteration 62510 (2.27505 iter/s, 4.39551s/10 iters), loss = 8.62923
I0523 06:16:56.925048 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62923 (* 1 = 8.62923 loss)
I0523 06:16:57.734560 34682 sgd_solver.cpp:112] Iteration 62510, lr = 0.01
I0523 06:17:04.247088 34682 solver.cpp:239] Iteration 62520 (1.36579 iter/s, 7.32175s/10 iters), loss = 8.26867
I0523 06:17:04.247164 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26867 (* 1 = 8.26867 loss)
I0523 06:17:04.315706 34682 sgd_solver.cpp:112] Iteration 62520, lr = 0.01
I0523 06:17:08.997859 34682 solver.cpp:239] Iteration 62530 (2.10504 iter/s, 4.7505s/10 iters), loss = 7.29584
I0523 06:17:08.997978 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.29584 (* 1 = 7.29584 loss)
I0523 06:17:09.059053 34682 sgd_solver.cpp:112] Iteration 62530, lr = 0.01
I0523 06:17:13.781780 34682 solver.cpp:239] Iteration 62540 (2.09047 iter/s, 4.78361s/10 iters), loss = 7.91928
I0523 06:17:13.781826 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91928 (* 1 = 7.91928 loss)
I0523 06:17:13.845072 34682 sgd_solver.cpp:112] Iteration 62540, lr = 0.01
I0523 06:17:17.696229 34682 solver.cpp:239] Iteration 62550 (2.55478 iter/s, 3.91424s/10 iters), loss = 7.92506
I0523 06:17:17.696288 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92506 (* 1 = 7.92506 loss)
I0523 06:17:17.767165 34682 sgd_solver.cpp:112] Iteration 62550, lr = 0.01
I0523 06:17:23.120220 34682 solver.cpp:239] Iteration 62560 (1.84376 iter/s, 5.42371s/10 iters), loss = 8.17918
I0523 06:17:23.120272 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17918 (* 1 = 8.17918 loss)
I0523 06:17:23.930356 34682 sgd_solver.cpp:112] Iteration 62560, lr = 0.01
I0523 06:17:26.775544 34682 solver.cpp:239] Iteration 62570 (2.73589 iter/s, 3.65512s/10 iters), loss = 7.51551
I0523 06:17:26.775589 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51551 (* 1 = 7.51551 loss)
I0523 06:17:26.851019 34682 sgd_solver.cpp:112] Iteration 62570, lr = 0.01
I0523 06:17:30.204512 34682 solver.cpp:239] Iteration 62580 (2.9165 iter/s, 3.42877s/10 iters), loss = 7.98946
I0523 06:17:30.204550 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98946 (* 1 = 7.98946 loss)
I0523 06:17:31.027837 34682 sgd_solver.cpp:112] Iteration 62580, lr = 0.01
I0523 06:17:34.139710 34682 solver.cpp:239] Iteration 62590 (2.5413 iter/s, 3.935s/10 iters), loss = 7.09409
I0523 06:17:34.139765 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.09409 (* 1 = 7.09409 loss)
I0523 06:17:34.909952 34682 sgd_solver.cpp:112] Iteration 62590, lr = 0.01
I0523 06:17:39.637939 34682 solver.cpp:239] Iteration 62600 (1.81887 iter/s, 5.49793s/10 iters), loss = 7.65102
I0523 06:17:39.638195 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.65102 (* 1 = 7.65102 loss)
I0523 06:17:39.707069 34682 sgd_solver.cpp:112] Iteration 62600, lr = 0.01
I0523 06:17:43.721273 34682 solver.cpp:239] Iteration 62610 (2.44922 iter/s, 4.08293s/10 iters), loss = 7.75934
I0523 06:17:43.721326 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75934 (* 1 = 7.75934 loss)
I0523 06:17:43.794706 34682 sgd_solver.cpp:112] Iteration 62610, lr = 0.01
I0523 06:17:49.105712 34682 solver.cpp:239] Iteration 62620 (1.8573 iter/s, 5.38417s/10 iters), loss = 9.05732
I0523 06:17:49.105767 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05732 (* 1 = 9.05732 loss)
I0523 06:17:49.947142 34682 sgd_solver.cpp:112] Iteration 62620, lr = 0.01
I0523 06:17:54.092536 34682 solver.cpp:239] Iteration 62630 (2.00539 iter/s, 4.98656s/10 iters), loss = 7.78785
I0523 06:17:54.092587 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.78785 (* 1 = 7.78785 loss)
I0523 06:17:54.166199 34682 sgd_solver.cpp:112] Iteration 62630, lr = 0.01
I0523 06:17:59.852733 34682 solver.cpp:239] Iteration 62640 (1.73614 iter/s, 5.75991s/10 iters), loss = 8.07619
I0523 06:17:59.852787 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07619 (* 1 = 8.07619 loss)
I0523 06:18:00.668720 34682 sgd_solver.cpp:112] Iteration 62640, lr = 0.01
I0523 06:18:04.762198 34682 solver.cpp:239] Iteration 62650 (2.037 iter/s, 4.90919s/10 iters), loss = 7.98326
I0523 06:18:04.762254 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98326 (* 1 = 7.98326 loss)
I0523 06:18:04.820828 34682 sgd_solver.cpp:112] Iteration 62650, lr = 0.01
I0523 06:18:11.204882 34682 solver.cpp:239] Iteration 62660 (1.55223 iter/s, 6.44236s/10 iters), loss = 8.34629
I0523 06:18:11.205011 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34629 (* 1 = 8.34629 loss)
I0523 06:18:11.264729 34682 sgd_solver.cpp:112] Iteration 62660, lr = 0.01
I0523 06:18:16.150864 34682 solver.cpp:239] Iteration 62670 (2.02198 iter/s, 4.94565s/10 iters), loss = 8.52421
I0523 06:18:16.150925 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52421 (* 1 = 8.52421 loss)
I0523 06:18:16.642587 34682 sgd_solver.cpp:112] Iteration 62670, lr = 0.01
I0523 06:18:23.059814 34682 solver.cpp:239] Iteration 62680 (1.44747 iter/s, 6.90861s/10 iters), loss = 8.27556
I0523 06:18:23.059866 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27556 (* 1 = 8.27556 loss)
I0523 06:18:23.128932 34682 sgd_solver.cpp:112] Iteration 62680, lr = 0.01
I0523 06:18:26.493139 34682 solver.cpp:239] Iteration 62690 (2.9128 iter/s, 3.43313s/10 iters), loss = 8.57444
I0523 06:18:26.493196 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57444 (* 1 = 8.57444 loss)
I0523 06:18:26.565302 34682 sgd_solver.cpp:112] Iteration 62690, lr = 0.01
I0523 06:18:30.891963 34682 solver.cpp:239] Iteration 62700 (2.27346 iter/s, 4.39859s/10 iters), loss = 7.63522
I0523 06:18:30.892004 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63522 (* 1 = 7.63522 loss)
I0523 06:18:30.953845 34682 sgd_solver.cpp:112] Iteration 62700, lr = 0.01
I0523 06:18:36.193395 34682 solver.cpp:239] Iteration 62710 (1.88638 iter/s, 5.30117s/10 iters), loss = 8.37652
I0523 06:18:36.193449 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37652 (* 1 = 8.37652 loss)
I0523 06:18:36.266940 34682 sgd_solver.cpp:112] Iteration 62710, lr = 0.01
I0523 06:18:41.258323 34682 solver.cpp:239] Iteration 62720 (1.97446 iter/s, 5.06467s/10 iters), loss = 7.68218
I0523 06:18:41.258601 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68218 (* 1 = 7.68218 loss)
I0523 06:18:41.320806 34682 sgd_solver.cpp:112] Iteration 62720, lr = 0.01
I0523 06:18:45.286518 34682 solver.cpp:239] Iteration 62730 (2.48276 iter/s, 4.02778s/10 iters), loss = 7.77978
I0523 06:18:45.286566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77978 (* 1 = 7.77978 loss)
I0523 06:18:45.348320 34682 sgd_solver.cpp:112] Iteration 62730, lr = 0.01
I0523 06:18:49.814980 34682 solver.cpp:239] Iteration 62740 (2.20837 iter/s, 4.52823s/10 iters), loss = 7.83147
I0523 06:18:49.815021 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83147 (* 1 = 7.83147 loss)
I0523 06:18:49.880590 34682 sgd_solver.cpp:112] Iteration 62740, lr = 0.01
I0523 06:18:53.756989 34682 solver.cpp:239] Iteration 62750 (2.53692 iter/s, 3.94179s/10 iters), loss = 8.34483
I0523 06:18:53.757045 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34483 (* 1 = 8.34483 loss)
I0523 06:18:54.593417 34682 sgd_solver.cpp:112] Iteration 62750, lr = 0.01
I0523 06:18:57.799942 34682 solver.cpp:239] Iteration 62760 (2.47358 iter/s, 4.04273s/10 iters), loss = 9.49122
I0523 06:18:57.799985 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.49122 (* 1 = 9.49122 loss)
I0523 06:18:57.880095 34682 sgd_solver.cpp:112] Iteration 62760, lr = 0.01
I0523 06:19:01.256777 34682 solver.cpp:239] Iteration 62770 (2.89298 iter/s, 3.45664s/10 iters), loss = 7.30155
I0523 06:19:01.256829 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.30155 (* 1 = 7.30155 loss)
I0523 06:19:01.337157 34682 sgd_solver.cpp:112] Iteration 62770, lr = 0.01
I0523 06:19:07.043396 34682 solver.cpp:239] Iteration 62780 (1.72821 iter/s, 5.78634s/10 iters), loss = 8.41633
I0523 06:19:07.043440 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41633 (* 1 = 8.41633 loss)
I0523 06:19:07.106315 34682 sgd_solver.cpp:112] Iteration 62780, lr = 0.01
I0523 06:19:10.416864 34682 solver.cpp:239] Iteration 62790 (2.96448 iter/s, 3.37328s/10 iters), loss = 7.87471
I0523 06:19:10.416910 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87471 (* 1 = 7.87471 loss)
I0523 06:19:10.488278 34682 sgd_solver.cpp:112] Iteration 62790, lr = 0.01
I0523 06:19:14.450562 34682 solver.cpp:239] Iteration 62800 (2.47925 iter/s, 4.03349s/10 iters), loss = 7.45817
I0523 06:19:14.450736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.45817 (* 1 = 7.45817 loss)
I0523 06:19:14.527096 34682 sgd_solver.cpp:112] Iteration 62800, lr = 0.01
I0523 06:19:18.734781 34682 solver.cpp:239] Iteration 62810 (2.33432 iter/s, 4.28391s/10 iters), loss = 7.39342
I0523 06:19:18.734833 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.39342 (* 1 = 7.39342 loss)
I0523 06:19:19.273010 34682 sgd_solver.cpp:112] Iteration 62810, lr = 0.01
I0523 06:19:25.730305 34682 solver.cpp:239] Iteration 62820 (1.42956 iter/s, 6.99518s/10 iters), loss = 8.30856
I0523 06:19:25.730370 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30856 (* 1 = 8.30856 loss)
I0523 06:19:26.485050 34682 sgd_solver.cpp:112] Iteration 62820, lr = 0.01
I0523 06:19:30.758276 34682 solver.cpp:239] Iteration 62830 (1.98898 iter/s, 5.0277s/10 iters), loss = 8.49377
I0523 06:19:30.758316 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49377 (* 1 = 8.49377 loss)
I0523 06:19:30.835446 34682 sgd_solver.cpp:112] Iteration 62830, lr = 0.01
I0523 06:19:36.196434 34682 solver.cpp:239] Iteration 62840 (1.83895 iter/s, 5.43789s/10 iters), loss = 7.99005
I0523 06:19:36.196470 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99005 (* 1 = 7.99005 loss)
I0523 06:19:36.273345 34682 sgd_solver.cpp:112] Iteration 62840, lr = 0.01
I0523 06:19:41.012782 34682 solver.cpp:239] Iteration 62850 (2.07636 iter/s, 4.81611s/10 iters), loss = 8.34485
I0523 06:19:41.012838 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34485 (* 1 = 8.34485 loss)
I0523 06:19:41.088995 34682 sgd_solver.cpp:112] Iteration 62850, lr = 0.01
I0523 06:19:46.197640 34682 solver.cpp:239] Iteration 62860 (1.92879 iter/s, 5.1846s/10 iters), loss = 8.82174
I0523 06:19:46.197850 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82174 (* 1 = 8.82174 loss)
I0523 06:19:46.795621 34682 sgd_solver.cpp:112] Iteration 62860, lr = 0.01
I0523 06:19:51.378096 34682 solver.cpp:239] Iteration 62870 (1.93049 iter/s, 5.18003s/10 iters), loss = 7.67506
I0523 06:19:51.378155 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.67506 (* 1 = 7.67506 loss)
I0523 06:19:52.126523 34682 sgd_solver.cpp:112] Iteration 62870, lr = 0.01
I0523 06:19:58.577036 34682 solver.cpp:239] Iteration 62880 (1.38916 iter/s, 7.1986s/10 iters), loss = 7.90401
I0523 06:19:58.577093 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90401 (* 1 = 7.90401 loss)
I0523 06:19:58.654603 34682 sgd_solver.cpp:112] Iteration 62880, lr = 0.01
I0523 06:20:02.923701 34682 solver.cpp:239] Iteration 62890 (2.30074 iter/s, 4.34643s/10 iters), loss = 7.81693
I0523 06:20:02.923751 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81693 (* 1 = 7.81693 loss)
I0523 06:20:02.996173 34682 sgd_solver.cpp:112] Iteration 62890, lr = 0.01
I0523 06:20:06.268956 34682 solver.cpp:239] Iteration 62900 (2.98948 iter/s, 3.34506s/10 iters), loss = 8.43723
I0523 06:20:06.269011 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43723 (* 1 = 8.43723 loss)
I0523 06:20:06.328992 34682 sgd_solver.cpp:112] Iteration 62900, lr = 0.01
I0523 06:20:11.112179 34682 solver.cpp:239] Iteration 62910 (2.06485 iter/s, 4.84297s/10 iters), loss = 8.01444
I0523 06:20:11.112221 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01444 (* 1 = 8.01444 loss)
I0523 06:20:11.906502 34682 sgd_solver.cpp:112] Iteration 62910, lr = 0.01
I0523 06:20:16.752674 34682 solver.cpp:239] Iteration 62920 (1.77298 iter/s, 5.64022s/10 iters), loss = 7.90964
I0523 06:20:16.752810 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90964 (* 1 = 7.90964 loss)
I0523 06:20:17.219089 34682 sgd_solver.cpp:112] Iteration 62920, lr = 0.01
I0523 06:20:21.851665 34682 solver.cpp:239] Iteration 62930 (1.96131 iter/s, 5.09864s/10 iters), loss = 8.88129
I0523 06:20:21.851732 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88129 (* 1 = 8.88129 loss)
I0523 06:20:22.665040 34682 sgd_solver.cpp:112] Iteration 62930, lr = 0.01
I0523 06:20:27.367859 34682 solver.cpp:239] Iteration 62940 (1.81294 iter/s, 5.5159s/10 iters), loss = 8.67758
I0523 06:20:27.367897 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67758 (* 1 = 8.67758 loss)
I0523 06:20:27.448460 34682 sgd_solver.cpp:112] Iteration 62940, lr = 0.01
I0523 06:20:32.990690 34682 solver.cpp:239] Iteration 62950 (1.77855 iter/s, 5.62256s/10 iters), loss = 7.51686
I0523 06:20:32.990743 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51686 (* 1 = 7.51686 loss)
I0523 06:20:33.050703 34682 sgd_solver.cpp:112] Iteration 62950, lr = 0.01
I0523 06:20:36.642375 34682 solver.cpp:239] Iteration 62960 (2.73862 iter/s, 3.65147s/10 iters), loss = 8.46306
I0523 06:20:36.642431 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46306 (* 1 = 8.46306 loss)
I0523 06:20:36.713160 34682 sgd_solver.cpp:112] Iteration 62960, lr = 0.01
I0523 06:20:40.598958 34682 solver.cpp:239] Iteration 62970 (2.52757 iter/s, 3.95637s/10 iters), loss = 7.28004
I0523 06:20:40.598996 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.28004 (* 1 = 7.28004 loss)
I0523 06:20:40.657946 34682 sgd_solver.cpp:112] Iteration 62970, lr = 0.01
I0523 06:20:44.015269 34682 solver.cpp:239] Iteration 62980 (2.92729 iter/s, 3.41613s/10 iters), loss = 7.86772
I0523 06:20:44.015311 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86772 (* 1 = 7.86772 loss)
I0523 06:20:44.077473 34682 sgd_solver.cpp:112] Iteration 62980, lr = 0.01
I0523 06:20:48.325608 34682 solver.cpp:239] Iteration 62990 (2.32012 iter/s, 4.31012s/10 iters), loss = 8.5044
I0523 06:20:48.325851 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5044 (* 1 = 8.5044 loss)
I0523 06:20:49.078330 34682 sgd_solver.cpp:112] Iteration 62990, lr = 0.01
I0523 06:20:54.095893 34682 solver.cpp:239] Iteration 63000 (1.73315 iter/s, 5.76983s/10 iters), loss = 6.88444
I0523 06:20:54.095954 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.88444 (* 1 = 6.88444 loss)
I0523 06:20:54.165932 34682 sgd_solver.cpp:112] Iteration 63000, lr = 0.01
I0523 06:20:59.005429 34682 solver.cpp:239] Iteration 63010 (2.03696 iter/s, 4.90928s/10 iters), loss = 8.68877
I0523 06:20:59.005471 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68877 (* 1 = 8.68877 loss)
I0523 06:20:59.069680 34682 sgd_solver.cpp:112] Iteration 63010, lr = 0.01
I0523 06:21:04.346415 34682 solver.cpp:239] Iteration 63020 (1.8724 iter/s, 5.34073s/10 iters), loss = 7.8722
I0523 06:21:04.346457 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8722 (* 1 = 7.8722 loss)
I0523 06:21:04.424875 34682 sgd_solver.cpp:112] Iteration 63020, lr = 0.01
I0523 06:21:09.993269 34682 solver.cpp:239] Iteration 63030 (1.77098 iter/s, 5.64658s/10 iters), loss = 7.4463
I0523 06:21:09.993335 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4463 (* 1 = 7.4463 loss)
I0523 06:21:10.065665 34682 sgd_solver.cpp:112] Iteration 63030, lr = 0.01
I0523 06:21:13.511168 34682 solver.cpp:239] Iteration 63040 (2.84278 iter/s, 3.51769s/10 iters), loss = 7.98837
I0523 06:21:13.511210 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98837 (* 1 = 7.98837 loss)
I0523 06:21:13.572697 34682 sgd_solver.cpp:112] Iteration 63040, lr = 0.01
I0523 06:21:18.085467 34682 solver.cpp:239] Iteration 63050 (2.18624 iter/s, 4.57407s/10 iters), loss = 8.45318
I0523 06:21:18.085506 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45318 (* 1 = 8.45318 loss)
I0523 06:21:18.156378 34682 sgd_solver.cpp:112] Iteration 63050, lr = 0.01
I0523 06:21:23.055162 34682 solver.cpp:239] Iteration 63060 (2.01229 iter/s, 4.96945s/10 iters), loss = 8.29731
I0523 06:21:23.055402 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29731 (* 1 = 8.29731 loss)
I0523 06:21:23.852402 34682 sgd_solver.cpp:112] Iteration 63060, lr = 0.01
I0523 06:21:29.636384 34682 solver.cpp:239] Iteration 63070 (1.51959 iter/s, 6.58074s/10 iters), loss = 8.72589
I0523 06:21:29.636448 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72589 (* 1 = 8.72589 loss)
I0523 06:21:30.466722 34682 sgd_solver.cpp:112] Iteration 63070, lr = 0.01
I0523 06:21:34.777979 34682 solver.cpp:239] Iteration 63080 (1.94503 iter/s, 5.14132s/10 iters), loss = 8.47817
I0523 06:21:34.778028 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47817 (* 1 = 8.47817 loss)
I0523 06:21:34.844782 34682 sgd_solver.cpp:112] Iteration 63080, lr = 0.01
I0523 06:21:39.323266 34682 solver.cpp:239] Iteration 63090 (2.2002 iter/s, 4.54505s/10 iters), loss = 9.00386
I0523 06:21:39.323307 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00386 (* 1 = 9.00386 loss)
I0523 06:21:39.399353 34682 sgd_solver.cpp:112] Iteration 63090, lr = 0.01
I0523 06:21:42.658504 34682 solver.cpp:239] Iteration 63100 (2.99846 iter/s, 3.33504s/10 iters), loss = 6.91257
I0523 06:21:42.658579 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.91257 (* 1 = 6.91257 loss)
I0523 06:21:43.373150 34682 sgd_solver.cpp:112] Iteration 63100, lr = 0.01
I0523 06:21:47.575927 34682 solver.cpp:239] Iteration 63110 (2.0337 iter/s, 4.91715s/10 iters), loss = 8.13063
I0523 06:21:47.575976 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13063 (* 1 = 8.13063 loss)
I0523 06:21:47.653162 34682 sgd_solver.cpp:112] Iteration 63110, lr = 0.01
I0523 06:21:52.001889 34682 solver.cpp:239] Iteration 63120 (2.25952 iter/s, 4.42573s/10 iters), loss = 7.74329
I0523 06:21:52.001931 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74329 (* 1 = 7.74329 loss)
I0523 06:21:52.067173 34682 sgd_solver.cpp:112] Iteration 63120, lr = 0.01
I0523 06:21:57.198720 34682 solver.cpp:239] Iteration 63130 (1.92435 iter/s, 5.19655s/10 iters), loss = 8.14698
I0523 06:21:57.198998 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14698 (* 1 = 8.14698 loss)
I0523 06:21:57.465755 34682 sgd_solver.cpp:112] Iteration 63130, lr = 0.01
I0523 06:22:02.424337 34682 solver.cpp:239] Iteration 63140 (1.91382 iter/s, 5.22515s/10 iters), loss = 8.27666
I0523 06:22:02.424381 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27666 (* 1 = 8.27666 loss)
I0523 06:22:03.292366 34682 sgd_solver.cpp:112] Iteration 63140, lr = 0.01
I0523 06:22:08.172550 34682 solver.cpp:239] Iteration 63150 (1.73975 iter/s, 5.74794s/10 iters), loss = 8.67492
I0523 06:22:08.172601 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67492 (* 1 = 8.67492 loss)
I0523 06:22:08.807591 34682 sgd_solver.cpp:112] Iteration 63150, lr = 0.01
I0523 06:22:15.228904 34682 solver.cpp:239] Iteration 63160 (1.41723 iter/s, 7.05602s/10 iters), loss = 8.65296
I0523 06:22:15.228956 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65296 (* 1 = 8.65296 loss)
I0523 06:22:15.292973 34682 sgd_solver.cpp:112] Iteration 63160, lr = 0.01
I0523 06:22:20.880070 34682 solver.cpp:239] Iteration 63170 (1.76964 iter/s, 5.65088s/10 iters), loss = 9.93599
I0523 06:22:20.880127 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.93599 (* 1 = 9.93599 loss)
I0523 06:22:20.941352 34682 sgd_solver.cpp:112] Iteration 63170, lr = 0.01
I0523 06:22:27.487166 34682 solver.cpp:239] Iteration 63180 (1.5136 iter/s, 6.60678s/10 iters), loss = 8.11436
I0523 06:22:27.487349 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11436 (* 1 = 8.11436 loss)
I0523 06:22:27.560947 34682 sgd_solver.cpp:112] Iteration 63180, lr = 0.01
I0523 06:22:32.226485 34682 solver.cpp:239] Iteration 63190 (2.11016 iter/s, 4.73897s/10 iters), loss = 8.06193
I0523 06:22:32.226533 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06193 (* 1 = 8.06193 loss)
I0523 06:22:32.403441 34682 sgd_solver.cpp:112] Iteration 63190, lr = 0.01
I0523 06:22:38.068353 34682 solver.cpp:239] Iteration 63200 (1.71187 iter/s, 5.84158s/10 iters), loss = 8.41939
I0523 06:22:38.068395 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41939 (* 1 = 8.41939 loss)
I0523 06:22:38.146584 34682 sgd_solver.cpp:112] Iteration 63200, lr = 0.01
I0523 06:22:43.141072 34682 solver.cpp:239] Iteration 63210 (1.97143 iter/s, 5.07246s/10 iters), loss = 7.88098
I0523 06:22:43.141124 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88098 (* 1 = 7.88098 loss)
I0523 06:22:43.205046 34682 sgd_solver.cpp:112] Iteration 63210, lr = 0.01
I0523 06:22:48.156258 34682 solver.cpp:239] Iteration 63220 (1.99405 iter/s, 5.01493s/10 iters), loss = 8.13724
I0523 06:22:48.156306 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13724 (* 1 = 8.13724 loss)
I0523 06:22:48.237215 34682 sgd_solver.cpp:112] Iteration 63220, lr = 0.01
I0523 06:22:52.607205 34682 solver.cpp:239] Iteration 63230 (2.24683 iter/s, 4.45072s/10 iters), loss = 8.34799
I0523 06:22:52.607249 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34799 (* 1 = 8.34799 loss)
I0523 06:22:52.676378 34682 sgd_solver.cpp:112] Iteration 63230, lr = 0.01
I0523 06:22:56.963207 34682 solver.cpp:239] Iteration 63240 (2.2958 iter/s, 4.35577s/10 iters), loss = 7.47233
I0523 06:22:56.963264 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.47233 (* 1 = 7.47233 loss)
I0523 06:22:57.046862 34682 sgd_solver.cpp:112] Iteration 63240, lr = 0.01
I0523 06:23:00.719321 34682 solver.cpp:239] Iteration 63250 (2.66248 iter/s, 3.7559s/10 iters), loss = 6.60623
I0523 06:23:00.719612 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.60623 (* 1 = 6.60623 loss)
I0523 06:23:00.782505 34682 sgd_solver.cpp:112] Iteration 63250, lr = 0.01
I0523 06:23:04.357408 34682 solver.cpp:239] Iteration 63260 (2.74901 iter/s, 3.63768s/10 iters), loss = 8.01912
I0523 06:23:04.357460 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01912 (* 1 = 8.01912 loss)
I0523 06:23:04.433538 34682 sgd_solver.cpp:112] Iteration 63260, lr = 0.01
I0523 06:23:09.277251 34682 solver.cpp:239] Iteration 63270 (2.03269 iter/s, 4.91959s/10 iters), loss = 8.38368
I0523 06:23:09.277295 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38368 (* 1 = 8.38368 loss)
I0523 06:23:09.896392 34682 sgd_solver.cpp:112] Iteration 63270, lr = 0.01
I0523 06:23:15.084200 34682 solver.cpp:239] Iteration 63280 (1.72216 iter/s, 5.80667s/10 iters), loss = 7.9242
I0523 06:23:15.084251 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9242 (* 1 = 7.9242 loss)
I0523 06:23:15.153466 34682 sgd_solver.cpp:112] Iteration 63280, lr = 0.01
I0523 06:23:18.903664 34682 solver.cpp:239] Iteration 63290 (2.61832 iter/s, 3.81925s/10 iters), loss = 8.70533
I0523 06:23:18.903712 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70533 (* 1 = 8.70533 loss)
I0523 06:23:18.970836 34682 sgd_solver.cpp:112] Iteration 63290, lr = 0.01
I0523 06:23:23.666878 34682 solver.cpp:239] Iteration 63300 (2.09953 iter/s, 4.76297s/10 iters), loss = 8.27912
I0523 06:23:23.666919 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27912 (* 1 = 8.27912 loss)
I0523 06:23:23.739058 34682 sgd_solver.cpp:112] Iteration 63300, lr = 0.01
I0523 06:23:28.663050 34682 solver.cpp:239] Iteration 63310 (2.00163 iter/s, 4.99592s/10 iters), loss = 7.95883
I0523 06:23:28.663092 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95883 (* 1 = 7.95883 loss)
I0523 06:23:28.732269 34682 sgd_solver.cpp:112] Iteration 63310, lr = 0.01
I0523 06:23:32.183941 34682 solver.cpp:239] Iteration 63320 (2.84035 iter/s, 3.5207s/10 iters), loss = 8.26512
I0523 06:23:32.184087 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26512 (* 1 = 8.26512 loss)
I0523 06:23:32.245287 34682 sgd_solver.cpp:112] Iteration 63320, lr = 0.01
I0523 06:23:34.787338 34682 solver.cpp:239] Iteration 63330 (3.84151 iter/s, 2.60314s/10 iters), loss = 8.08042
I0523 06:23:34.787385 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08042 (* 1 = 8.08042 loss)
I0523 06:23:34.844326 34682 sgd_solver.cpp:112] Iteration 63330, lr = 0.01
I0523 06:23:38.282831 34682 solver.cpp:239] Iteration 63340 (2.86098 iter/s, 3.49531s/10 iters), loss = 7.96147
I0523 06:23:38.282883 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96147 (* 1 = 7.96147 loss)
I0523 06:23:38.341821 34682 sgd_solver.cpp:112] Iteration 63340, lr = 0.01
I0523 06:23:43.040606 34682 solver.cpp:239] Iteration 63350 (2.10193 iter/s, 4.75752s/10 iters), loss = 7.05195
I0523 06:23:43.040647 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.05195 (* 1 = 7.05195 loss)
I0523 06:23:43.108377 34682 sgd_solver.cpp:112] Iteration 63350, lr = 0.01
I0523 06:23:47.291692 34682 solver.cpp:239] Iteration 63360 (2.35246 iter/s, 4.25086s/10 iters), loss = 7.61447
I0523 06:23:47.291735 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.61447 (* 1 = 7.61447 loss)
I0523 06:23:47.361325 34682 sgd_solver.cpp:112] Iteration 63360, lr = 0.01
I0523 06:23:52.102896 34682 solver.cpp:239] Iteration 63370 (2.07859 iter/s, 4.81096s/10 iters), loss = 8.43616
I0523 06:23:52.102941 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43616 (* 1 = 8.43616 loss)
I0523 06:23:52.186820 34682 sgd_solver.cpp:112] Iteration 63370, lr = 0.01
I0523 06:23:57.723657 34682 solver.cpp:239] Iteration 63380 (1.77921 iter/s, 5.62049s/10 iters), loss = 8.453
I0523 06:23:57.723711 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.453 (* 1 = 8.453 loss)
I0523 06:23:58.242429 34682 sgd_solver.cpp:112] Iteration 63380, lr = 0.01
I0523 06:24:04.438323 34682 solver.cpp:239] Iteration 63390 (1.48935 iter/s, 6.71433s/10 iters), loss = 9.10351
I0523 06:24:04.438459 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.10351 (* 1 = 9.10351 loss)
I0523 06:24:04.512117 34682 sgd_solver.cpp:112] Iteration 63390, lr = 0.01
I0523 06:24:09.494622 34682 solver.cpp:239] Iteration 63400 (1.97787 iter/s, 5.05595s/10 iters), loss = 8.37028
I0523 06:24:09.494673 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37028 (* 1 = 8.37028 loss)
I0523 06:24:10.346941 34682 sgd_solver.cpp:112] Iteration 63400, lr = 0.01
I0523 06:24:13.776588 34682 solver.cpp:239] Iteration 63410 (2.3355 iter/s, 4.28174s/10 iters), loss = 8.22402
I0523 06:24:13.776628 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22402 (* 1 = 8.22402 loss)
I0523 06:24:13.836138 34682 sgd_solver.cpp:112] Iteration 63410, lr = 0.01
I0523 06:24:18.840150 34682 solver.cpp:239] Iteration 63420 (1.975 iter/s, 5.06328s/10 iters), loss = 7.77637
I0523 06:24:18.840211 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77637 (* 1 = 7.77637 loss)
I0523 06:24:18.907467 34682 sgd_solver.cpp:112] Iteration 63420, lr = 0.01
I0523 06:24:24.952406 34682 solver.cpp:239] Iteration 63430 (1.63614 iter/s, 6.11195s/10 iters), loss = 7.5764
I0523 06:24:24.952459 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5764 (* 1 = 7.5764 loss)
I0523 06:24:25.010478 34682 sgd_solver.cpp:112] Iteration 63430, lr = 0.01
I0523 06:24:28.372452 34682 solver.cpp:239] Iteration 63440 (2.92411 iter/s, 3.41985s/10 iters), loss = 7.88256
I0523 06:24:28.372498 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88256 (* 1 = 7.88256 loss)
I0523 06:24:29.234863 34682 sgd_solver.cpp:112] Iteration 63440, lr = 0.01
I0523 06:24:33.299545 34682 solver.cpp:239] Iteration 63450 (2.0297 iter/s, 4.92684s/10 iters), loss = 8.36908
I0523 06:24:33.299592 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36908 (* 1 = 8.36908 loss)
I0523 06:24:33.381683 34682 sgd_solver.cpp:112] Iteration 63450, lr = 0.01
I0523 06:24:38.802124 34682 solver.cpp:239] Iteration 63460 (1.81742 iter/s, 5.50232s/10 iters), loss = 7.7742
I0523 06:24:38.802366 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7742 (* 1 = 7.7742 loss)
I0523 06:24:39.658094 34682 sgd_solver.cpp:112] Iteration 63460, lr = 0.01
I0523 06:24:44.838497 34682 solver.cpp:239] Iteration 63470 (1.65675 iter/s, 6.03592s/10 iters), loss = 8.44073
I0523 06:24:44.838546 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44073 (* 1 = 8.44073 loss)
I0523 06:24:45.016904 34682 sgd_solver.cpp:112] Iteration 63470, lr = 0.01
I0523 06:24:50.055507 34682 solver.cpp:239] Iteration 63480 (1.9169 iter/s, 5.21675s/10 iters), loss = 7.86756
I0523 06:24:50.055577 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86756 (* 1 = 7.86756 loss)
I0523 06:24:50.767360 34682 sgd_solver.cpp:112] Iteration 63480, lr = 0.01
I0523 06:24:55.135223 34682 solver.cpp:239] Iteration 63490 (1.96872 iter/s, 5.07944s/10 iters), loss = 8.17929
I0523 06:24:55.135272 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17929 (* 1 = 8.17929 loss)
I0523 06:24:55.199604 34682 sgd_solver.cpp:112] Iteration 63490, lr = 0.01
I0523 06:25:01.664978 34682 solver.cpp:239] Iteration 63500 (1.53153 iter/s, 6.52944s/10 iters), loss = 8.49798
I0523 06:25:01.665033 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49798 (* 1 = 8.49798 loss)
I0523 06:25:02.533288 34682 sgd_solver.cpp:112] Iteration 63500, lr = 0.01
I0523 06:25:05.984064 34682 solver.cpp:239] Iteration 63510 (2.31543 iter/s, 4.31885s/10 iters), loss = 8.37383
I0523 06:25:05.984114 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37383 (* 1 = 8.37383 loss)
I0523 06:25:06.787253 34682 sgd_solver.cpp:112] Iteration 63510, lr = 0.01
I0523 06:25:09.916108 34682 solver.cpp:239] Iteration 63520 (2.54334 iter/s, 3.93183s/10 iters), loss = 8.16301
I0523 06:25:09.916354 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16301 (* 1 = 8.16301 loss)
I0523 06:25:09.980747 34682 sgd_solver.cpp:112] Iteration 63520, lr = 0.01
I0523 06:25:13.958676 34682 solver.cpp:239] Iteration 63530 (2.47393 iter/s, 4.04216s/10 iters), loss = 7.33786
I0523 06:25:13.958770 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.33786 (* 1 = 7.33786 loss)
I0523 06:25:14.560964 34682 sgd_solver.cpp:112] Iteration 63530, lr = 0.01
I0523 06:25:17.669926 34682 solver.cpp:239] Iteration 63540 (2.69469 iter/s, 3.711s/10 iters), loss = 7.96025
I0523 06:25:17.669968 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96025 (* 1 = 7.96025 loss)
I0523 06:25:17.733077 34682 sgd_solver.cpp:112] Iteration 63540, lr = 0.01
I0523 06:25:21.157986 34682 solver.cpp:239] Iteration 63550 (2.86708 iter/s, 3.48787s/10 iters), loss = 8.36279
I0523 06:25:21.158037 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36279 (* 1 = 8.36279 loss)
I0523 06:25:21.225762 34682 sgd_solver.cpp:112] Iteration 63550, lr = 0.01
I0523 06:25:25.876237 34682 solver.cpp:239] Iteration 63560 (2.11954 iter/s, 4.71801s/10 iters), loss = 7.05083
I0523 06:25:25.876279 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.05083 (* 1 = 7.05083 loss)
I0523 06:25:25.954475 34682 sgd_solver.cpp:112] Iteration 63560, lr = 0.01
I0523 06:25:30.229897 34682 solver.cpp:239] Iteration 63570 (2.29703 iter/s, 4.35344s/10 iters), loss = 8.05068
I0523 06:25:30.229941 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05068 (* 1 = 8.05068 loss)
I0523 06:25:30.290740 34682 sgd_solver.cpp:112] Iteration 63570, lr = 0.01
I0523 06:25:35.198525 34682 solver.cpp:239] Iteration 63580 (2.01273 iter/s, 4.96838s/10 iters), loss = 7.79625
I0523 06:25:35.198581 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79625 (* 1 = 7.79625 loss)
I0523 06:25:35.865870 34682 sgd_solver.cpp:112] Iteration 63580, lr = 0.01
I0523 06:25:41.526809 34682 solver.cpp:239] Iteration 63590 (1.58028 iter/s, 6.32798s/10 iters), loss = 7.53901
I0523 06:25:41.528054 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.53901 (* 1 = 7.53901 loss)
I0523 06:25:41.602738 34682 sgd_solver.cpp:112] Iteration 63590, lr = 0.01
I0523 06:25:44.278682 34682 solver.cpp:239] Iteration 63600 (3.63566 iter/s, 2.75053s/10 iters), loss = 7.48544
I0523 06:25:44.278735 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.48544 (* 1 = 7.48544 loss)
I0523 06:25:44.346453 34682 sgd_solver.cpp:112] Iteration 63600, lr = 0.01
I0523 06:25:49.221683 34682 solver.cpp:239] Iteration 63610 (2.02317 iter/s, 4.94274s/10 iters), loss = 7.73528
I0523 06:25:49.221731 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.73528 (* 1 = 7.73528 loss)
I0523 06:25:50.076334 34682 sgd_solver.cpp:112] Iteration 63610, lr = 0.01
I0523 06:25:54.973985 34682 solver.cpp:239] Iteration 63620 (1.73852 iter/s, 5.75202s/10 iters), loss = 8.44341
I0523 06:25:54.974030 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44341 (* 1 = 8.44341 loss)
I0523 06:25:55.030952 34682 sgd_solver.cpp:112] Iteration 63620, lr = 0.01
I0523 06:25:58.446131 34682 solver.cpp:239] Iteration 63630 (2.88023 iter/s, 3.47195s/10 iters), loss = 8.61787
I0523 06:25:58.446177 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61787 (* 1 = 8.61787 loss)
I0523 06:25:59.249617 34682 sgd_solver.cpp:112] Iteration 63630, lr = 0.01
I0523 06:26:05.236588 34682 solver.cpp:239] Iteration 63640 (1.47273 iter/s, 6.79013s/10 iters), loss = 8.47893
I0523 06:26:05.236645 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47893 (* 1 = 8.47893 loss)
I0523 06:26:05.974301 34682 sgd_solver.cpp:112] Iteration 63640, lr = 0.01
I0523 06:26:10.751631 34682 solver.cpp:239] Iteration 63650 (1.81332 iter/s, 5.51474s/10 iters), loss = 8.50974
I0523 06:26:10.751699 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50974 (* 1 = 8.50974 loss)
I0523 06:26:11.590459 34682 sgd_solver.cpp:112] Iteration 63650, lr = 0.01
I0523 06:26:17.278115 34682 solver.cpp:239] Iteration 63660 (1.5323 iter/s, 6.52616s/10 iters), loss = 7.65413
I0523 06:26:17.278167 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.65413 (* 1 = 7.65413 loss)
I0523 06:26:17.334197 34682 sgd_solver.cpp:112] Iteration 63660, lr = 0.01
I0523 06:26:22.552397 34682 solver.cpp:239] Iteration 63670 (1.89609 iter/s, 5.27401s/10 iters), loss = 7.31706
I0523 06:26:22.552438 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.31706 (* 1 = 7.31706 loss)
I0523 06:26:22.624971 34682 sgd_solver.cpp:112] Iteration 63670, lr = 0.01
I0523 06:26:25.134174 34682 solver.cpp:239] Iteration 63680 (3.87355 iter/s, 2.58161s/10 iters), loss = 7.92145
I0523 06:26:25.134250 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92145 (* 1 = 7.92145 loss)
I0523 06:26:25.976295 34682 sgd_solver.cpp:112] Iteration 63680, lr = 0.01
I0523 06:26:30.796253 34682 solver.cpp:239] Iteration 63690 (1.76623 iter/s, 5.66178s/10 iters), loss = 7.42697
I0523 06:26:30.796293 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42697 (* 1 = 7.42697 loss)
I0523 06:26:31.556972 34682 sgd_solver.cpp:112] Iteration 63690, lr = 0.01
I0523 06:26:35.947888 34682 solver.cpp:239] Iteration 63700 (1.94123 iter/s, 5.15138s/10 iters), loss = 8.50028
I0523 06:26:35.947935 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50028 (* 1 = 8.50028 loss)
I0523 06:26:36.013159 34682 sgd_solver.cpp:112] Iteration 63700, lr = 0.01
I0523 06:26:40.092016 34682 solver.cpp:239] Iteration 63710 (2.41318 iter/s, 4.14391s/10 iters), loss = 8.1335
I0523 06:26:40.092063 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1335 (* 1 = 8.1335 loss)
I0523 06:26:40.166152 34682 sgd_solver.cpp:112] Iteration 63710, lr = 0.01
I0523 06:26:44.468605 34682 solver.cpp:239] Iteration 63720 (2.285 iter/s, 4.37636s/10 iters), loss = 7.36608
I0523 06:26:44.468689 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.36608 (* 1 = 7.36608 loss)
I0523 06:26:44.537145 34682 sgd_solver.cpp:112] Iteration 63720, lr = 0.01
I0523 06:26:50.582792 34682 solver.cpp:239] Iteration 63730 (1.63563 iter/s, 6.11386s/10 iters), loss = 7.96801
I0523 06:26:50.582839 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96801 (* 1 = 7.96801 loss)
I0523 06:26:50.655894 34682 sgd_solver.cpp:112] Iteration 63730, lr = 0.01
I0523 06:26:58.342016 34682 solver.cpp:239] Iteration 63740 (1.28885 iter/s, 7.75886s/10 iters), loss = 7.72283
I0523 06:26:58.342063 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72283 (* 1 = 7.72283 loss)
I0523 06:26:58.405655 34682 sgd_solver.cpp:112] Iteration 63740, lr = 0.01
I0523 06:27:02.846542 34682 solver.cpp:239] Iteration 63750 (2.2201 iter/s, 4.50429s/10 iters), loss = 8.27969
I0523 06:27:02.846585 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27969 (* 1 = 8.27969 loss)
I0523 06:27:02.912255 34682 sgd_solver.cpp:112] Iteration 63750, lr = 0.01
I0523 06:27:05.658390 34682 solver.cpp:239] Iteration 63760 (3.55659 iter/s, 2.81168s/10 iters), loss = 7.34043
I0523 06:27:05.658433 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.34043 (* 1 = 7.34043 loss)
I0523 06:27:05.720126 34682 sgd_solver.cpp:112] Iteration 63760, lr = 0.01
I0523 06:27:12.243073 34682 solver.cpp:239] Iteration 63770 (1.51875 iter/s, 6.58437s/10 iters), loss = 8.2902
I0523 06:27:12.243119 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2902 (* 1 = 8.2902 loss)
I0523 06:27:12.318480 34682 sgd_solver.cpp:112] Iteration 63770, lr = 0.01
I0523 06:27:17.209378 34682 solver.cpp:239] Iteration 63780 (2.01367 iter/s, 4.96605s/10 iters), loss = 8.82116
I0523 06:27:17.210789 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82116 (* 1 = 8.82116 loss)
I0523 06:27:17.286748 34682 sgd_solver.cpp:112] Iteration 63780, lr = 0.01
I0523 06:27:21.533838 34682 solver.cpp:239] Iteration 63790 (2.31484 iter/s, 4.31995s/10 iters), loss = 7.99891
I0523 06:27:21.533893 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99891 (* 1 = 7.99891 loss)
I0523 06:27:22.400457 34682 sgd_solver.cpp:112] Iteration 63790, lr = 0.01
I0523 06:27:25.807574 34682 solver.cpp:239] Iteration 63800 (2.34 iter/s, 4.27351s/10 iters), loss = 8.72711
I0523 06:27:25.807618 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72711 (* 1 = 8.72711 loss)
I0523 06:27:26.347367 34682 sgd_solver.cpp:112] Iteration 63800, lr = 0.01
I0523 06:27:30.984346 34682 solver.cpp:239] Iteration 63810 (1.93181 iter/s, 5.1765s/10 iters), loss = 7.99131
I0523 06:27:30.984400 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99131 (* 1 = 7.99131 loss)
I0523 06:27:31.755316 34682 sgd_solver.cpp:112] Iteration 63810, lr = 0.01
I0523 06:27:35.890384 34682 solver.cpp:239] Iteration 63820 (2.03842 iter/s, 4.90576s/10 iters), loss = 8.3695
I0523 06:27:35.890426 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3695 (* 1 = 8.3695 loss)
I0523 06:27:35.967252 34682 sgd_solver.cpp:112] Iteration 63820, lr = 0.01
I0523 06:27:41.839850 34682 solver.cpp:239] Iteration 63830 (1.6809 iter/s, 5.94918s/10 iters), loss = 8.5201
I0523 06:27:41.839910 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5201 (* 1 = 8.5201 loss)
I0523 06:27:42.494577 34682 sgd_solver.cpp:112] Iteration 63830, lr = 0.01
I0523 06:27:48.101507 34682 solver.cpp:239] Iteration 63840 (1.5971 iter/s, 6.26135s/10 iters), loss = 8.26996
I0523 06:27:48.101676 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26996 (* 1 = 8.26996 loss)
I0523 06:27:48.164542 34682 sgd_solver.cpp:112] Iteration 63840, lr = 0.01
I0523 06:27:53.698750 34682 solver.cpp:239] Iteration 63850 (1.78672 iter/s, 5.59685s/10 iters), loss = 7.21946
I0523 06:27:53.698801 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.21946 (* 1 = 7.21946 loss)
I0523 06:27:54.183562 34682 sgd_solver.cpp:112] Iteration 63850, lr = 0.01
I0523 06:27:59.944180 34682 solver.cpp:239] Iteration 63860 (1.60125 iter/s, 6.24513s/10 iters), loss = 8.81596
I0523 06:27:59.944227 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81596 (* 1 = 8.81596 loss)
I0523 06:28:00.178089 34682 sgd_solver.cpp:112] Iteration 63860, lr = 0.01
I0523 06:28:04.655040 34682 solver.cpp:239] Iteration 63870 (2.12286 iter/s, 4.71062s/10 iters), loss = 8.68683
I0523 06:28:04.655083 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68683 (* 1 = 8.68683 loss)
I0523 06:28:04.741291 34682 sgd_solver.cpp:112] Iteration 63870, lr = 0.01
I0523 06:28:11.242425 34682 solver.cpp:239] Iteration 63880 (1.51812 iter/s, 6.58707s/10 iters), loss = 8.06179
I0523 06:28:11.242471 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06179 (* 1 = 8.06179 loss)
I0523 06:28:11.659210 34682 sgd_solver.cpp:112] Iteration 63880, lr = 0.01
I0523 06:28:15.641491 34682 solver.cpp:239] Iteration 63890 (2.27333 iter/s, 4.39883s/10 iters), loss = 8.10591
I0523 06:28:15.641539 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10591 (* 1 = 8.10591 loss)
I0523 06:28:15.703747 34682 sgd_solver.cpp:112] Iteration 63890, lr = 0.01
I0523 06:28:19.241159 34682 solver.cpp:239] Iteration 63900 (2.77819 iter/s, 3.59947s/10 iters), loss = 8.33206
I0523 06:28:19.241350 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33206 (* 1 = 8.33206 loss)
I0523 06:28:19.303344 34682 sgd_solver.cpp:112] Iteration 63900, lr = 0.01
I0523 06:28:24.146733 34682 solver.cpp:239] Iteration 63910 (2.03867 iter/s, 4.90517s/10 iters), loss = 7.8393
I0523 06:28:24.146791 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8393 (* 1 = 7.8393 loss)
I0523 06:28:24.496472 34682 sgd_solver.cpp:112] Iteration 63910, lr = 0.01
I0523 06:28:28.541996 34682 solver.cpp:239] Iteration 63920 (2.2753 iter/s, 4.39502s/10 iters), loss = 8.06289
I0523 06:28:28.542049 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06289 (* 1 = 8.06289 loss)
I0523 06:28:29.373643 34682 sgd_solver.cpp:112] Iteration 63920, lr = 0.01
I0523 06:28:33.427963 34682 solver.cpp:239] Iteration 63930 (2.04678 iter/s, 4.88572s/10 iters), loss = 8.33405
I0523 06:28:33.428009 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33405 (* 1 = 8.33405 loss)
I0523 06:28:33.498108 34682 sgd_solver.cpp:112] Iteration 63930, lr = 0.01
I0523 06:28:37.130321 34682 solver.cpp:239] Iteration 63940 (2.70113 iter/s, 3.70215s/10 iters), loss = 8.58883
I0523 06:28:37.130378 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58883 (* 1 = 8.58883 loss)
I0523 06:28:37.664320 34682 sgd_solver.cpp:112] Iteration 63940, lr = 0.01
I0523 06:28:41.853240 34682 solver.cpp:239] Iteration 63950 (2.11745 iter/s, 4.72266s/10 iters), loss = 7.03144
I0523 06:28:41.853286 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.03144 (* 1 = 7.03144 loss)
I0523 06:28:41.926651 34682 sgd_solver.cpp:112] Iteration 63950, lr = 0.01
I0523 06:28:47.018637 34682 solver.cpp:239] Iteration 63960 (1.93605 iter/s, 5.16514s/10 iters), loss = 8.32003
I0523 06:28:47.018676 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32003 (* 1 = 8.32003 loss)
I0523 06:28:47.092733 34682 sgd_solver.cpp:112] Iteration 63960, lr = 0.01
I0523 06:28:52.735275 34682 solver.cpp:239] Iteration 63970 (1.74936 iter/s, 5.71637s/10 iters), loss = 9.0761
I0523 06:28:52.735527 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.0761 (* 1 = 9.0761 loss)
I0523 06:28:53.374541 34682 sgd_solver.cpp:112] Iteration 63970, lr = 0.01
I0523 06:28:58.307210 34682 solver.cpp:239] Iteration 63980 (1.79486 iter/s, 5.57145s/10 iters), loss = 7.49471
I0523 06:28:58.307271 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.49471 (* 1 = 7.49471 loss)
I0523 06:28:59.157474 34682 sgd_solver.cpp:112] Iteration 63980, lr = 0.01
I0523 06:29:04.226140 34682 solver.cpp:239] Iteration 63990 (1.68958 iter/s, 5.91863s/10 iters), loss = 8.50048
I0523 06:29:04.226263 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50048 (* 1 = 8.50048 loss)
I0523 06:29:04.297508 34682 sgd_solver.cpp:112] Iteration 63990, lr = 0.01
I0523 06:29:07.567131 34682 solver.cpp:239] Iteration 64000 (2.99336 iter/s, 3.34073s/10 iters), loss = 7.17264
I0523 06:29:07.567180 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.17264 (* 1 = 7.17264 loss)
I0523 06:29:08.331799 34682 sgd_solver.cpp:112] Iteration 64000, lr = 0.01
I0523 06:29:11.488037 34682 solver.cpp:239] Iteration 64010 (2.55057 iter/s, 3.92069s/10 iters), loss = 8.35689
I0523 06:29:11.488080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35689 (* 1 = 8.35689 loss)
I0523 06:29:11.563588 34682 sgd_solver.cpp:112] Iteration 64010, lr = 0.01
I0523 06:29:16.564486 34682 solver.cpp:239] Iteration 64020 (1.96998 iter/s, 5.0762s/10 iters), loss = 8.11545
I0523 06:29:16.564537 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11545 (* 1 = 8.11545 loss)
I0523 06:29:16.648190 34682 sgd_solver.cpp:112] Iteration 64020, lr = 0.01
I0523 06:29:20.366111 34682 solver.cpp:239] Iteration 64030 (2.6306 iter/s, 3.80142s/10 iters), loss = 8.11582
I0523 06:29:20.366161 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11582 (* 1 = 8.11582 loss)
I0523 06:29:20.438452 34682 sgd_solver.cpp:112] Iteration 64030, lr = 0.01
I0523 06:29:26.874933 34682 solver.cpp:239] Iteration 64040 (1.53645 iter/s, 6.5085s/10 iters), loss = 8.10854
I0523 06:29:26.875126 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10854 (* 1 = 8.10854 loss)
I0523 06:29:27.617149 34682 sgd_solver.cpp:112] Iteration 64040, lr = 0.01
I0523 06:29:32.412776 34682 solver.cpp:239] Iteration 64050 (1.8059 iter/s, 5.53742s/10 iters), loss = 8.37595
I0523 06:29:32.412840 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37595 (* 1 = 8.37595 loss)
I0523 06:29:33.232923 34682 sgd_solver.cpp:112] Iteration 64050, lr = 0.01
I0523 06:29:37.277873 34682 solver.cpp:239] Iteration 64060 (2.05557 iter/s, 4.86483s/10 iters), loss = 8.18292
I0523 06:29:37.277920 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18292 (* 1 = 8.18292 loss)
I0523 06:29:37.342417 34682 sgd_solver.cpp:112] Iteration 64060, lr = 0.01
I0523 06:29:41.532757 34682 solver.cpp:239] Iteration 64070 (2.35036 iter/s, 4.25466s/10 iters), loss = 7.87403
I0523 06:29:41.532797 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87403 (* 1 = 7.87403 loss)
I0523 06:29:42.352694 34682 sgd_solver.cpp:112] Iteration 64070, lr = 0.01
I0523 06:29:46.628384 34682 solver.cpp:239] Iteration 64080 (1.96256 iter/s, 5.09537s/10 iters), loss = 7.43117
I0523 06:29:46.628434 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43117 (* 1 = 7.43117 loss)
I0523 06:29:46.698556 34682 sgd_solver.cpp:112] Iteration 64080, lr = 0.01
I0523 06:29:50.934402 34682 solver.cpp:239] Iteration 64090 (2.32246 iter/s, 4.30578s/10 iters), loss = 8.66624
I0523 06:29:50.934481 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66624 (* 1 = 8.66624 loss)
I0523 06:29:51.643185 34682 sgd_solver.cpp:112] Iteration 64090, lr = 0.01
I0523 06:29:57.194579 34682 solver.cpp:239] Iteration 64100 (1.59748 iter/s, 6.25985s/10 iters), loss = 7.69096
I0523 06:29:57.194857 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69096 (* 1 = 7.69096 loss)
I0523 06:29:57.267396 34682 sgd_solver.cpp:112] Iteration 64100, lr = 0.01
I0523 06:30:02.321456 34682 solver.cpp:239] Iteration 64110 (1.95146 iter/s, 5.12437s/10 iters), loss = 7.58649
I0523 06:30:02.321508 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58649 (* 1 = 7.58649 loss)
I0523 06:30:03.182255 34682 sgd_solver.cpp:112] Iteration 64110, lr = 0.01
I0523 06:30:08.246309 34682 solver.cpp:239] Iteration 64120 (1.68789 iter/s, 5.92455s/10 iters), loss = 7.5795
I0523 06:30:08.246366 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5795 (* 1 = 7.5795 loss)
I0523 06:30:09.045775 34682 sgd_solver.cpp:112] Iteration 64120, lr = 0.01
I0523 06:30:14.271467 34682 solver.cpp:239] Iteration 64130 (1.65979 iter/s, 6.02486s/10 iters), loss = 8.3642
I0523 06:30:14.271517 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3642 (* 1 = 8.3642 loss)
I0523 06:30:14.341388 34682 sgd_solver.cpp:112] Iteration 64130, lr = 0.01
I0523 06:30:19.008738 34682 solver.cpp:239] Iteration 64140 (2.11103 iter/s, 4.73702s/10 iters), loss = 8.08982
I0523 06:30:19.008801 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08982 (* 1 = 8.08982 loss)
I0523 06:30:19.848359 34682 sgd_solver.cpp:112] Iteration 64140, lr = 0.01
I0523 06:30:24.523597 34682 solver.cpp:239] Iteration 64150 (1.81338 iter/s, 5.51457s/10 iters), loss = 7.58875
I0523 06:30:24.523638 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58875 (* 1 = 7.58875 loss)
I0523 06:30:24.611670 34682 sgd_solver.cpp:112] Iteration 64150, lr = 0.01
I0523 06:30:28.084110 34682 solver.cpp:239] Iteration 64160 (2.80874 iter/s, 3.56032s/10 iters), loss = 8.19258
I0523 06:30:28.084293 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19258 (* 1 = 8.19258 loss)
I0523 06:30:28.137770 34682 sgd_solver.cpp:112] Iteration 64160, lr = 0.01
I0523 06:30:32.868404 34682 solver.cpp:239] Iteration 64170 (2.09034 iter/s, 4.78392s/10 iters), loss = 8.33195
I0523 06:30:32.868456 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33195 (* 1 = 8.33195 loss)
I0523 06:30:33.023277 34682 sgd_solver.cpp:112] Iteration 64170, lr = 0.01
I0523 06:30:37.603502 34682 solver.cpp:239] Iteration 64180 (2.112 iter/s, 4.73485s/10 iters), loss = 8.84227
I0523 06:30:37.603555 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84227 (* 1 = 8.84227 loss)
I0523 06:30:38.471977 34682 sgd_solver.cpp:112] Iteration 64180, lr = 0.01
I0523 06:30:41.899838 34682 solver.cpp:239] Iteration 64190 (2.3277 iter/s, 4.29609s/10 iters), loss = 8.00891
I0523 06:30:41.899907 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00891 (* 1 = 8.00891 loss)
I0523 06:30:41.972872 34682 sgd_solver.cpp:112] Iteration 64190, lr = 0.01
I0523 06:30:48.382243 34682 solver.cpp:239] Iteration 64200 (1.54272 iter/s, 6.48207s/10 iters), loss = 7.0897
I0523 06:30:48.382318 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.0897 (* 1 = 7.0897 loss)
I0523 06:30:48.459666 34682 sgd_solver.cpp:112] Iteration 64200, lr = 0.01
I0523 06:30:52.280540 34682 solver.cpp:239] Iteration 64210 (2.56538 iter/s, 3.89806s/10 iters), loss = 7.45165
I0523 06:30:52.280596 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.45165 (* 1 = 7.45165 loss)
I0523 06:30:52.350571 34682 sgd_solver.cpp:112] Iteration 64210, lr = 0.01
I0523 06:30:56.608722 34682 solver.cpp:239] Iteration 64220 (2.31056 iter/s, 4.32795s/10 iters), loss = 8.00544
I0523 06:30:56.608774 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00544 (* 1 = 8.00544 loss)
I0523 06:30:56.682822 34682 sgd_solver.cpp:112] Iteration 64220, lr = 0.01
I0523 06:31:03.228297 34682 solver.cpp:239] Iteration 64230 (1.51075 iter/s, 6.61925s/10 iters), loss = 7.50614
I0523 06:31:03.228545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50614 (* 1 = 7.50614 loss)
I0523 06:31:04.126780 34682 sgd_solver.cpp:112] Iteration 64230, lr = 0.01
I0523 06:31:09.358912 34682 solver.cpp:239] Iteration 64240 (1.63129 iter/s, 6.13013s/10 iters), loss = 8.66738
I0523 06:31:09.358986 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66738 (* 1 = 8.66738 loss)
I0523 06:31:09.425135 34682 sgd_solver.cpp:112] Iteration 64240, lr = 0.01
I0523 06:31:12.683907 34682 solver.cpp:239] Iteration 64250 (3.00772 iter/s, 3.32477s/10 iters), loss = 8.67434
I0523 06:31:12.683954 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67434 (* 1 = 8.67434 loss)
I0523 06:31:12.757738 34682 sgd_solver.cpp:112] Iteration 64250, lr = 0.01
I0523 06:31:18.297500 34682 solver.cpp:239] Iteration 64260 (1.78148 iter/s, 5.61332s/10 iters), loss = 7.44833
I0523 06:31:18.297555 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44833 (* 1 = 7.44833 loss)
I0523 06:31:19.012167 34682 sgd_solver.cpp:112] Iteration 64260, lr = 0.01
I0523 06:31:24.733162 34682 solver.cpp:239] Iteration 64270 (1.55392 iter/s, 6.43534s/10 iters), loss = 7.94506
I0523 06:31:24.733211 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94506 (* 1 = 7.94506 loss)
I0523 06:31:25.609856 34682 sgd_solver.cpp:112] Iteration 64270, lr = 0.01
I0523 06:31:32.614008 34682 solver.cpp:239] Iteration 64280 (1.26896 iter/s, 7.88047s/10 iters), loss = 7.62465
I0523 06:31:32.614069 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62465 (* 1 = 7.62465 loss)
I0523 06:31:33.438572 34682 sgd_solver.cpp:112] Iteration 64280, lr = 0.01
I0523 06:31:37.402359 34682 solver.cpp:239] Iteration 64290 (2.08851 iter/s, 4.78809s/10 iters), loss = 8.19638
I0523 06:31:37.402400 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19638 (* 1 = 8.19638 loss)
I0523 06:31:37.457084 34682 sgd_solver.cpp:112] Iteration 64290, lr = 0.01
I0523 06:31:41.694574 34682 solver.cpp:239] Iteration 64300 (2.32992 iter/s, 4.29199s/10 iters), loss = 7.8232
I0523 06:31:41.694628 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8232 (* 1 = 7.8232 loss)
I0523 06:31:42.293679 34682 sgd_solver.cpp:112] Iteration 64300, lr = 0.01
I0523 06:31:47.660895 34682 solver.cpp:239] Iteration 64310 (1.67616 iter/s, 5.96602s/10 iters), loss = 8.09128
I0523 06:31:47.660960 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09128 (* 1 = 8.09128 loss)
I0523 06:31:48.395895 34682 sgd_solver.cpp:112] Iteration 64310, lr = 0.01
I0523 06:31:53.217540 34682 solver.cpp:239] Iteration 64320 (1.79974 iter/s, 5.55636s/10 iters), loss = 7.81935
I0523 06:31:53.217586 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81935 (* 1 = 7.81935 loss)
I0523 06:31:53.290997 34682 sgd_solver.cpp:112] Iteration 64320, lr = 0.01
I0523 06:31:58.641037 34682 solver.cpp:239] Iteration 64330 (1.84392 iter/s, 5.42323s/10 iters), loss = 8.81997
I0523 06:31:58.641093 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81997 (* 1 = 8.81997 loss)
I0523 06:31:58.702038 34682 sgd_solver.cpp:112] Iteration 64330, lr = 0.01
I0523 06:32:05.308100 34682 solver.cpp:239] Iteration 64340 (1.49998 iter/s, 6.66674s/10 iters), loss = 8.04767
I0523 06:32:05.308337 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04767 (* 1 = 8.04767 loss)
I0523 06:32:05.373642 34682 sgd_solver.cpp:112] Iteration 64340, lr = 0.01
I0523 06:32:10.368821 34682 solver.cpp:239] Iteration 64350 (1.97617 iter/s, 5.06031s/10 iters), loss = 8.20044
I0523 06:32:10.368872 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20044 (* 1 = 8.20044 loss)
I0523 06:32:11.123935 34682 sgd_solver.cpp:112] Iteration 64350, lr = 0.01
I0523 06:32:15.133666 34682 solver.cpp:239] Iteration 64360 (2.09881 iter/s, 4.76459s/10 iters), loss = 7.2645
I0523 06:32:15.133719 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.2645 (* 1 = 7.2645 loss)
I0523 06:32:15.903961 34682 sgd_solver.cpp:112] Iteration 64360, lr = 0.01
I0523 06:32:20.095724 34682 solver.cpp:239] Iteration 64370 (2.0154 iter/s, 4.9618s/10 iters), loss = 7.05443
I0523 06:32:20.095770 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.05443 (* 1 = 7.05443 loss)
I0523 06:32:20.155879 34682 sgd_solver.cpp:112] Iteration 64370, lr = 0.01
I0523 06:32:24.192842 34682 solver.cpp:239] Iteration 64380 (2.44087 iter/s, 4.0969s/10 iters), loss = 8.26353
I0523 06:32:24.192883 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26353 (* 1 = 8.26353 loss)
I0523 06:32:24.271657 34682 sgd_solver.cpp:112] Iteration 64380, lr = 0.01
I0523 06:32:26.904458 34682 solver.cpp:239] Iteration 64390 (3.68805 iter/s, 2.71146s/10 iters), loss = 7.75111
I0523 06:32:26.904511 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75111 (* 1 = 7.75111 loss)
I0523 06:32:27.745054 34682 sgd_solver.cpp:112] Iteration 64390, lr = 0.01
I0523 06:32:34.867012 34682 solver.cpp:239] Iteration 64400 (1.25594 iter/s, 7.96218s/10 iters), loss = 9.18964
I0523 06:32:34.867066 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.18964 (* 1 = 9.18964 loss)
I0523 06:32:35.677455 34682 sgd_solver.cpp:112] Iteration 64400, lr = 0.01
I0523 06:32:39.829030 34682 solver.cpp:239] Iteration 64410 (2.01541 iter/s, 4.96176s/10 iters), loss = 8.08453
I0523 06:32:39.829087 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08453 (* 1 = 8.08453 loss)
I0523 06:32:39.900873 34682 sgd_solver.cpp:112] Iteration 64410, lr = 0.01
I0523 06:32:43.985180 34682 solver.cpp:239] Iteration 64420 (2.40621 iter/s, 4.15592s/10 iters), loss = 7.20062
I0523 06:32:43.985229 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.20062 (* 1 = 7.20062 loss)
I0523 06:32:44.038832 34682 sgd_solver.cpp:112] Iteration 64420, lr = 0.01
I0523 06:32:47.318712 34682 solver.cpp:239] Iteration 64430 (3.00001 iter/s, 3.33333s/10 iters), loss = 7.76389
I0523 06:32:47.318758 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.76389 (* 1 = 7.76389 loss)
I0523 06:32:47.382570 34682 sgd_solver.cpp:112] Iteration 64430, lr = 0.01
I0523 06:32:52.296391 34682 solver.cpp:239] Iteration 64440 (2.00907 iter/s, 4.97742s/10 iters), loss = 9.3828
I0523 06:32:52.296437 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.3828 (* 1 = 9.3828 loss)
I0523 06:32:52.360781 34682 sgd_solver.cpp:112] Iteration 64440, lr = 0.01
I0523 06:32:56.277081 34682 solver.cpp:239] Iteration 64450 (2.51227 iter/s, 3.98047s/10 iters), loss = 7.72955
I0523 06:32:56.277142 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72955 (* 1 = 7.72955 loss)
I0523 06:32:56.355654 34682 sgd_solver.cpp:112] Iteration 64450, lr = 0.01
I0523 06:32:59.565953 34682 solver.cpp:239] Iteration 64460 (3.04074 iter/s, 3.28868s/10 iters), loss = 8.85002
I0523 06:32:59.565994 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85002 (* 1 = 8.85002 loss)
I0523 06:33:00.329087 34682 sgd_solver.cpp:112] Iteration 64460, lr = 0.01
I0523 06:33:04.354895 34682 solver.cpp:239] Iteration 64470 (2.08825 iter/s, 4.7887s/10 iters), loss = 8.37932
I0523 06:33:04.354956 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37932 (* 1 = 8.37932 loss)
I0523 06:33:05.188468 34682 sgd_solver.cpp:112] Iteration 64470, lr = 0.01
I0523 06:33:09.024438 34682 solver.cpp:239] Iteration 64480 (2.14165 iter/s, 4.66929s/10 iters), loss = 7.98492
I0523 06:33:09.024739 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98492 (* 1 = 7.98492 loss)
I0523 06:33:09.081912 34682 sgd_solver.cpp:112] Iteration 64480, lr = 0.01
I0523 06:33:14.596750 34682 solver.cpp:239] Iteration 64490 (1.79475 iter/s, 5.57181s/10 iters), loss = 7.95835
I0523 06:33:14.596802 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95835 (* 1 = 7.95835 loss)
I0523 06:33:14.661829 34682 sgd_solver.cpp:112] Iteration 64490, lr = 0.01
I0523 06:33:19.241061 34682 solver.cpp:239] Iteration 64500 (2.15328 iter/s, 4.64407s/10 iters), loss = 7.94286
I0523 06:33:19.241106 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94286 (* 1 = 7.94286 loss)
I0523 06:33:19.318475 34682 sgd_solver.cpp:112] Iteration 64500, lr = 0.01
I0523 06:33:23.615272 34682 solver.cpp:239] Iteration 64510 (2.28625 iter/s, 4.37398s/10 iters), loss = 7.74568
I0523 06:33:23.615332 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74568 (* 1 = 7.74568 loss)
I0523 06:33:24.420541 34682 sgd_solver.cpp:112] Iteration 64510, lr = 0.01
I0523 06:33:29.065589 34682 solver.cpp:239] Iteration 64520 (1.83485 iter/s, 5.45004s/10 iters), loss = 6.7515
I0523 06:33:29.065636 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.7515 (* 1 = 6.7515 loss)
I0523 06:33:29.898645 34682 sgd_solver.cpp:112] Iteration 64520, lr = 0.01
I0523 06:33:34.666188 34682 solver.cpp:239] Iteration 64530 (1.78561 iter/s, 5.60032s/10 iters), loss = 8.00818
I0523 06:33:34.666234 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00818 (* 1 = 8.00818 loss)
I0523 06:33:35.473685 34682 sgd_solver.cpp:112] Iteration 64530, lr = 0.01
I0523 06:33:41.820420 34682 solver.cpp:239] Iteration 64540 (1.39784 iter/s, 7.15388s/10 iters), loss = 7.82497
I0523 06:33:41.820648 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82497 (* 1 = 7.82497 loss)
I0523 06:33:42.643718 34682 sgd_solver.cpp:112] Iteration 64540, lr = 0.01
I0523 06:33:45.173190 34682 solver.cpp:239] Iteration 64550 (2.98291 iter/s, 3.35243s/10 iters), loss = 8.01649
I0523 06:33:45.173255 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01649 (* 1 = 8.01649 loss)
I0523 06:33:45.745220 34682 sgd_solver.cpp:112] Iteration 64550, lr = 0.01
I0523 06:33:49.968760 34682 solver.cpp:239] Iteration 64560 (2.08537 iter/s, 4.79531s/10 iters), loss = 9.05305
I0523 06:33:49.968889 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05305 (* 1 = 9.05305 loss)
I0523 06:33:50.038908 34682 sgd_solver.cpp:112] Iteration 64560, lr = 0.01
I0523 06:33:52.724869 34682 solver.cpp:239] Iteration 64570 (3.62863 iter/s, 2.75586s/10 iters), loss = 8.16285
I0523 06:33:52.724913 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16285 (* 1 = 8.16285 loss)
I0523 06:33:53.566581 34682 sgd_solver.cpp:112] Iteration 64570, lr = 0.01
I0523 06:33:58.130848 34682 solver.cpp:239] Iteration 64580 (1.8499 iter/s, 5.40571s/10 iters), loss = 7.83927
I0523 06:33:58.130913 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83927 (* 1 = 7.83927 loss)
I0523 06:33:58.837437 34682 sgd_solver.cpp:112] Iteration 64580, lr = 0.01
I0523 06:34:02.745553 34682 solver.cpp:239] Iteration 64590 (2.1671 iter/s, 4.61445s/10 iters), loss = 8.02088
I0523 06:34:02.745609 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02088 (* 1 = 8.02088 loss)
I0523 06:34:02.819356 34682 sgd_solver.cpp:112] Iteration 64590, lr = 0.01
I0523 06:34:08.063565 34682 solver.cpp:239] Iteration 64600 (1.8805 iter/s, 5.31773s/10 iters), loss = 8.51996
I0523 06:34:08.063625 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51996 (* 1 = 8.51996 loss)
I0523 06:34:08.210170 34682 sgd_solver.cpp:112] Iteration 64600, lr = 0.01
I0523 06:34:12.980398 34682 solver.cpp:239] Iteration 64610 (2.03394 iter/s, 4.91658s/10 iters), loss = 6.8289
I0523 06:34:12.980620 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.8289 (* 1 = 6.8289 loss)
I0523 06:34:13.812024 34682 sgd_solver.cpp:112] Iteration 64610, lr = 0.01
I0523 06:34:18.387181 34682 solver.cpp:239] Iteration 64620 (1.84967 iter/s, 5.40637s/10 iters), loss = 8.07913
I0523 06:34:18.387228 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07913 (* 1 = 8.07913 loss)
I0523 06:34:18.444434 34682 sgd_solver.cpp:112] Iteration 64620, lr = 0.01
I0523 06:34:23.222509 34682 solver.cpp:239] Iteration 64630 (2.06822 iter/s, 4.83508s/10 iters), loss = 8.90502
I0523 06:34:23.222551 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90502 (* 1 = 8.90502 loss)
I0523 06:34:23.303267 34682 sgd_solver.cpp:112] Iteration 64630, lr = 0.01
I0523 06:34:30.477525 34682 solver.cpp:239] Iteration 64640 (1.37925 iter/s, 7.2503s/10 iters), loss = 8.15477
I0523 06:34:30.477573 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15477 (* 1 = 8.15477 loss)
I0523 06:34:31.340873 34682 sgd_solver.cpp:112] Iteration 64640, lr = 0.01
I0523 06:34:35.762261 34682 solver.cpp:239] Iteration 64650 (1.89234 iter/s, 5.28447s/10 iters), loss = 8.11416
I0523 06:34:35.762312 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11416 (* 1 = 8.11416 loss)
I0523 06:34:35.838639 34682 sgd_solver.cpp:112] Iteration 64650, lr = 0.01
I0523 06:34:39.126651 34682 solver.cpp:239] Iteration 64660 (2.97248 iter/s, 3.3642s/10 iters), loss = 7.95111
I0523 06:34:39.126736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95111 (* 1 = 7.95111 loss)
I0523 06:34:39.623651 34682 sgd_solver.cpp:112] Iteration 64660, lr = 0.01
I0523 06:34:44.724512 34682 solver.cpp:239] Iteration 64670 (1.78649 iter/s, 5.59755s/10 iters), loss = 7.69738
I0523 06:34:44.724779 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69738 (* 1 = 7.69738 loss)
I0523 06:34:44.799253 34682 sgd_solver.cpp:112] Iteration 64670, lr = 0.01
I0523 06:34:50.223647 34682 solver.cpp:239] Iteration 64680 (1.81863 iter/s, 5.49866s/10 iters), loss = 7.42352
I0523 06:34:50.223693 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42352 (* 1 = 7.42352 loss)
I0523 06:34:50.291637 34682 sgd_solver.cpp:112] Iteration 64680, lr = 0.01
I0523 06:34:54.204282 34682 solver.cpp:239] Iteration 64690 (2.5123 iter/s, 3.98042s/10 iters), loss = 7.97892
I0523 06:34:54.204336 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97892 (* 1 = 7.97892 loss)
I0523 06:34:55.012099 34682 sgd_solver.cpp:112] Iteration 64690, lr = 0.01
I0523 06:34:59.978113 34682 solver.cpp:239] Iteration 64700 (1.73204 iter/s, 5.77355s/10 iters), loss = 7.47124
I0523 06:34:59.978159 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.47124 (* 1 = 7.47124 loss)
I0523 06:35:00.035951 34682 sgd_solver.cpp:112] Iteration 64700, lr = 0.01
I0523 06:35:03.979133 34682 solver.cpp:239] Iteration 64710 (2.4995 iter/s, 4.0008s/10 iters), loss = 8.24704
I0523 06:35:03.979171 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24704 (* 1 = 8.24704 loss)
I0523 06:35:04.055281 34682 sgd_solver.cpp:112] Iteration 64710, lr = 0.01
I0523 06:35:08.971747 34682 solver.cpp:239] Iteration 64720 (2.00306 iter/s, 4.99237s/10 iters), loss = 6.66872
I0523 06:35:08.971804 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.66872 (* 1 = 6.66872 loss)
I0523 06:35:09.617427 34682 sgd_solver.cpp:112] Iteration 64720, lr = 0.01
I0523 06:35:15.903049 34682 solver.cpp:239] Iteration 64730 (1.4428 iter/s, 6.93096s/10 iters), loss = 8.55947
I0523 06:35:15.903203 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55947 (* 1 = 8.55947 loss)
I0523 06:35:16.751243 34682 sgd_solver.cpp:112] Iteration 64730, lr = 0.01
I0523 06:35:20.010211 34682 solver.cpp:239] Iteration 64740 (2.43496 iter/s, 4.10684s/10 iters), loss = 8.43685
I0523 06:35:20.010251 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43685 (* 1 = 8.43685 loss)
I0523 06:35:20.085377 34682 sgd_solver.cpp:112] Iteration 64740, lr = 0.01
I0523 06:35:23.254588 34682 solver.cpp:239] Iteration 64750 (3.08242 iter/s, 3.2442s/10 iters), loss = 7.50549
I0523 06:35:23.254632 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50549 (* 1 = 7.50549 loss)
I0523 06:35:23.915418 34682 sgd_solver.cpp:112] Iteration 64750, lr = 0.01
I0523 06:35:27.258134 34682 solver.cpp:239] Iteration 64760 (2.49792 iter/s, 4.00333s/10 iters), loss = 7.74183
I0523 06:35:27.258186 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74183 (* 1 = 7.74183 loss)
I0523 06:35:27.334307 34682 sgd_solver.cpp:112] Iteration 64760, lr = 0.01
I0523 06:35:31.552251 34682 solver.cpp:239] Iteration 64770 (2.32889 iter/s, 4.29389s/10 iters), loss = 8.45802
I0523 06:35:31.552299 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45802 (* 1 = 8.45802 loss)
I0523 06:35:31.614780 34682 sgd_solver.cpp:112] Iteration 64770, lr = 0.01
I0523 06:35:36.943424 34682 solver.cpp:239] Iteration 64780 (1.85497 iter/s, 5.39091s/10 iters), loss = 8.08396
I0523 06:35:36.943467 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08396 (* 1 = 8.08396 loss)
I0523 06:35:37.025292 34682 sgd_solver.cpp:112] Iteration 64780, lr = 0.01
I0523 06:35:40.699229 34682 solver.cpp:239] Iteration 64790 (2.66269 iter/s, 3.7556s/10 iters), loss = 8.3024
I0523 06:35:40.699272 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3024 (* 1 = 8.3024 loss)
I0523 06:35:40.759397 34682 sgd_solver.cpp:112] Iteration 64790, lr = 0.01
I0523 06:35:45.741986 34682 solver.cpp:239] Iteration 64800 (1.98314 iter/s, 5.0425s/10 iters), loss = 7.44059
I0523 06:35:45.742040 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44059 (* 1 = 7.44059 loss)
I0523 06:35:45.810343 34682 sgd_solver.cpp:112] Iteration 64800, lr = 0.01
I0523 06:35:50.509227 34682 solver.cpp:239] Iteration 64810 (2.09885 iter/s, 4.76451s/10 iters), loss = 8.21413
I0523 06:35:50.509373 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21413 (* 1 = 8.21413 loss)
I0523 06:35:50.578436 34682 sgd_solver.cpp:112] Iteration 64810, lr = 0.01
I0523 06:35:56.351552 34682 solver.cpp:239] Iteration 64820 (1.71304 iter/s, 5.83759s/10 iters), loss = 7.26995
I0523 06:35:56.351604 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.26995 (* 1 = 7.26995 loss)
I0523 06:35:56.416457 34682 sgd_solver.cpp:112] Iteration 64820, lr = 0.01
I0523 06:36:00.537791 34682 solver.cpp:239] Iteration 64830 (2.38891 iter/s, 4.18601s/10 iters), loss = 6.60541
I0523 06:36:00.537854 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.60541 (* 1 = 6.60541 loss)
I0523 06:36:01.358248 34682 sgd_solver.cpp:112] Iteration 64830, lr = 0.01
I0523 06:36:04.877058 34682 solver.cpp:239] Iteration 64840 (2.30467 iter/s, 4.33901s/10 iters), loss = 9.33311
I0523 06:36:04.877125 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.33311 (* 1 = 9.33311 loss)
I0523 06:36:05.764997 34682 sgd_solver.cpp:112] Iteration 64840, lr = 0.01
I0523 06:36:09.945230 34682 solver.cpp:239] Iteration 64850 (1.9732 iter/s, 5.0679s/10 iters), loss = 8.33561
I0523 06:36:09.945273 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33561 (* 1 = 8.33561 loss)
I0523 06:36:10.025476 34682 sgd_solver.cpp:112] Iteration 64850, lr = 0.01
I0523 06:36:13.385774 34682 solver.cpp:239] Iteration 64860 (2.90667 iter/s, 3.44036s/10 iters), loss = 8.90599
I0523 06:36:13.385818 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90599 (* 1 = 8.90599 loss)
I0523 06:36:14.166869 34682 sgd_solver.cpp:112] Iteration 64860, lr = 0.01
I0523 06:36:16.765187 34682 solver.cpp:239] Iteration 64870 (2.95926 iter/s, 3.37923s/10 iters), loss = 7.28711
I0523 06:36:16.765234 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.28711 (* 1 = 7.28711 loss)
I0523 06:36:16.837718 34682 sgd_solver.cpp:112] Iteration 64870, lr = 0.01
I0523 06:36:22.473768 34682 solver.cpp:239] Iteration 64880 (1.75184 iter/s, 5.7083s/10 iters), loss = 8.04636
I0523 06:36:22.473999 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04636 (* 1 = 8.04636 loss)
I0523 06:36:22.553238 34682 sgd_solver.cpp:112] Iteration 64880, lr = 0.01
I0523 06:36:26.053625 34682 solver.cpp:239] Iteration 64890 (2.79369 iter/s, 3.5795s/10 iters), loss = 7.68339
I0523 06:36:26.053681 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68339 (* 1 = 7.68339 loss)
I0523 06:36:26.875013 34682 sgd_solver.cpp:112] Iteration 64890, lr = 0.01
I0523 06:36:30.340853 34682 solver.cpp:239] Iteration 64900 (2.33264 iter/s, 4.287s/10 iters), loss = 7.85972
I0523 06:36:30.340898 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.85972 (* 1 = 7.85972 loss)
I0523 06:36:30.403662 34682 sgd_solver.cpp:112] Iteration 64900, lr = 0.01
I0523 06:36:34.289041 34682 solver.cpp:239] Iteration 64910 (2.53294 iter/s, 3.94798s/10 iters), loss = 8.45762
I0523 06:36:34.289086 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45762 (* 1 = 8.45762 loss)
I0523 06:36:34.358875 34682 sgd_solver.cpp:112] Iteration 64910, lr = 0.01
I0523 06:36:38.361624 34682 solver.cpp:239] Iteration 64920 (2.45558 iter/s, 4.07237s/10 iters), loss = 7.16504
I0523 06:36:38.361666 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.16504 (* 1 = 7.16504 loss)
I0523 06:36:38.434597 34682 sgd_solver.cpp:112] Iteration 64920, lr = 0.01
I0523 06:36:41.202601 34682 solver.cpp:239] Iteration 64930 (3.52012 iter/s, 2.84081s/10 iters), loss = 8.40817
I0523 06:36:41.202662 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40817 (* 1 = 8.40817 loss)
I0523 06:36:41.283646 34682 sgd_solver.cpp:112] Iteration 64930, lr = 0.01
I0523 06:36:47.366410 34682 solver.cpp:239] Iteration 64940 (1.62245 iter/s, 6.1635s/10 iters), loss = 8.60573
I0523 06:36:47.366458 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60573 (* 1 = 8.60573 loss)
I0523 06:36:47.833222 34682 sgd_solver.cpp:112] Iteration 64940, lr = 0.01
I0523 06:36:52.646500 34682 solver.cpp:239] Iteration 64950 (1.89401 iter/s, 5.27982s/10 iters), loss = 7.85049
I0523 06:36:52.646811 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.85049 (* 1 = 7.85049 loss)
I0523 06:36:53.496451 34682 sgd_solver.cpp:112] Iteration 64950, lr = 0.01
I0523 06:36:57.283263 34682 solver.cpp:239] Iteration 64960 (2.15689 iter/s, 4.6363s/10 iters), loss = 7.65748
I0523 06:36:57.283313 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.65748 (* 1 = 7.65748 loss)
I0523 06:36:58.029459 34682 sgd_solver.cpp:112] Iteration 64960, lr = 0.01
I0523 06:37:02.068222 34682 solver.cpp:239] Iteration 64970 (2.08999 iter/s, 4.78471s/10 iters), loss = 8.66811
I0523 06:37:02.068284 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66811 (* 1 = 8.66811 loss)
I0523 06:37:02.739470 34682 sgd_solver.cpp:112] Iteration 64970, lr = 0.01
I0523 06:37:07.210603 34682 solver.cpp:239] Iteration 64980 (1.94473 iter/s, 5.14211s/10 iters), loss = 8.05744
I0523 06:37:07.210661 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05744 (* 1 = 8.05744 loss)
I0523 06:37:07.291566 34682 sgd_solver.cpp:112] Iteration 64980, lr = 0.01
I0523 06:37:11.293421 34682 solver.cpp:239] Iteration 64990 (2.44942 iter/s, 4.0826s/10 iters), loss = 8.22109
I0523 06:37:11.293462 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22109 (* 1 = 8.22109 loss)
I0523 06:37:11.366217 34682 sgd_solver.cpp:112] Iteration 64990, lr = 0.01
I0523 06:37:17.534970 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_65000.caffemodel
I0523 06:37:22.220942 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_65000.solverstate
I0523 06:37:22.427011 34682 solver.cpp:239] Iteration 65000 (0.898222 iter/s, 11.1331s/10 iters), loss = 7.48557
I0523 06:37:22.427054 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.48557 (* 1 = 7.48557 loss)
I0523 06:37:22.489944 34682 sgd_solver.cpp:112] Iteration 65000, lr = 0.01
I0523 06:37:26.532786 34682 solver.cpp:239] Iteration 65010 (2.43573 iter/s, 4.10555s/10 iters), loss = 8.06351
I0523 06:37:26.532912 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06351 (* 1 = 8.06351 loss)
I0523 06:37:26.613395 34682 sgd_solver.cpp:112] Iteration 65010, lr = 0.01
I0523 06:37:30.731370 34682 solver.cpp:239] Iteration 65020 (2.38193 iter/s, 4.19828s/10 iters), loss = 7.94296
I0523 06:37:30.731410 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94296 (* 1 = 7.94296 loss)
I0523 06:37:30.808061 34682 sgd_solver.cpp:112] Iteration 65020, lr = 0.01
I0523 06:37:34.255897 34682 solver.cpp:239] Iteration 65030 (2.83741 iter/s, 3.52434s/10 iters), loss = 8.50133
I0523 06:37:34.255944 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50133 (* 1 = 8.50133 loss)
I0523 06:37:35.100564 34682 sgd_solver.cpp:112] Iteration 65030, lr = 0.01
I0523 06:37:41.188431 34682 solver.cpp:239] Iteration 65040 (1.44254 iter/s, 6.93221s/10 iters), loss = 8.16808
I0523 06:37:41.188488 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16808 (* 1 = 8.16808 loss)
I0523 06:37:41.251451 34682 sgd_solver.cpp:112] Iteration 65040, lr = 0.01
I0523 06:37:45.898121 34682 solver.cpp:239] Iteration 65050 (2.1234 iter/s, 4.70944s/10 iters), loss = 8.3006
I0523 06:37:45.898174 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3006 (* 1 = 8.3006 loss)
I0523 06:37:45.966322 34682 sgd_solver.cpp:112] Iteration 65050, lr = 0.01
I0523 06:37:51.610826 34682 solver.cpp:239] Iteration 65060 (1.75057 iter/s, 5.71241s/10 iters), loss = 7.71735
I0523 06:37:51.610889 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.71735 (* 1 = 7.71735 loss)
I0523 06:37:51.680500 34682 sgd_solver.cpp:112] Iteration 65060, lr = 0.01
I0523 06:37:55.727357 34682 solver.cpp:239] Iteration 65070 (2.42936 iter/s, 4.1163s/10 iters), loss = 7.77312
I0523 06:37:55.727403 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77312 (* 1 = 7.77312 loss)
I0523 06:37:55.779328 34682 sgd_solver.cpp:112] Iteration 65070, lr = 0.01
I0523 06:38:00.866205 34682 solver.cpp:239] Iteration 65080 (1.94606 iter/s, 5.13859s/10 iters), loss = 8.98564
I0523 06:38:00.866466 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98564 (* 1 = 8.98564 loss)
I0523 06:38:00.947229 34682 sgd_solver.cpp:112] Iteration 65080, lr = 0.01
I0523 06:38:05.230087 34682 solver.cpp:239] Iteration 65090 (2.29175 iter/s, 4.36347s/10 iters), loss = 8.01071
I0523 06:38:05.230137 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01071 (* 1 = 8.01071 loss)
I0523 06:38:06.048709 34682 sgd_solver.cpp:112] Iteration 65090, lr = 0.01
I0523 06:38:10.053531 34682 solver.cpp:239] Iteration 65100 (2.07331 iter/s, 4.8232s/10 iters), loss = 9.14509
I0523 06:38:10.053571 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14509 (* 1 = 9.14509 loss)
I0523 06:38:10.127420 34682 sgd_solver.cpp:112] Iteration 65100, lr = 0.01
I0523 06:38:16.167234 34682 solver.cpp:239] Iteration 65110 (1.63575 iter/s, 6.11341s/10 iters), loss = 8.07411
I0523 06:38:16.167282 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07411 (* 1 = 8.07411 loss)
I0523 06:38:16.990420 34682 sgd_solver.cpp:112] Iteration 65110, lr = 0.01
I0523 06:38:22.517043 34682 solver.cpp:239] Iteration 65120 (1.57493 iter/s, 6.3495s/10 iters), loss = 6.97527
I0523 06:38:22.517092 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.97527 (* 1 = 6.97527 loss)
I0523 06:38:22.579210 34682 sgd_solver.cpp:112] Iteration 65120, lr = 0.01
I0523 06:38:28.184753 34682 solver.cpp:239] Iteration 65130 (1.76447 iter/s, 5.66742s/10 iters), loss = 7.62424
I0523 06:38:28.184818 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62424 (* 1 = 7.62424 loss)
I0523 06:38:28.860112 34682 sgd_solver.cpp:112] Iteration 65130, lr = 0.01
I0523 06:38:33.114959 34682 solver.cpp:239] Iteration 65140 (2.02842 iter/s, 4.92994s/10 iters), loss = 6.9867
I0523 06:38:33.115073 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.9867 (* 1 = 6.9867 loss)
I0523 06:38:33.184146 34682 sgd_solver.cpp:112] Iteration 65140, lr = 0.01
I0523 06:38:37.246299 34682 solver.cpp:239] Iteration 65150 (2.4207 iter/s, 4.13104s/10 iters), loss = 7.67657
I0523 06:38:37.246356 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.67657 (* 1 = 7.67657 loss)
I0523 06:38:38.066192 34682 sgd_solver.cpp:112] Iteration 65150, lr = 0.01
I0523 06:38:41.392735 34682 solver.cpp:239] Iteration 65160 (2.41184 iter/s, 4.14621s/10 iters), loss = 8.34956
I0523 06:38:41.392786 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34956 (* 1 = 8.34956 loss)
I0523 06:38:42.216794 34682 sgd_solver.cpp:112] Iteration 65160, lr = 0.01
I0523 06:38:48.271028 34682 solver.cpp:239] Iteration 65170 (1.45392 iter/s, 6.87796s/10 iters), loss = 8.36008
I0523 06:38:48.271091 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36008 (* 1 = 8.36008 loss)
I0523 06:38:49.092830 34682 sgd_solver.cpp:112] Iteration 65170, lr = 0.01
I0523 06:38:54.730551 34682 solver.cpp:239] Iteration 65180 (1.54818 iter/s, 6.4592s/10 iters), loss = 9.30456
I0523 06:38:54.730590 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.30456 (* 1 = 9.30456 loss)
I0523 06:38:54.801206 34682 sgd_solver.cpp:112] Iteration 65180, lr = 0.01
I0523 06:38:59.079246 34682 solver.cpp:239] Iteration 65190 (2.29966 iter/s, 4.34847s/10 iters), loss = 8.9695
I0523 06:38:59.079298 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.9695 (* 1 = 8.9695 loss)
I0523 06:38:59.132203 34682 sgd_solver.cpp:112] Iteration 65190, lr = 0.01
I0523 06:39:03.197840 34682 solver.cpp:239] Iteration 65200 (2.42814 iter/s, 4.11838s/10 iters), loss = 8.30612
I0523 06:39:03.198055 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30612 (* 1 = 8.30612 loss)
I0523 06:39:03.860452 34682 sgd_solver.cpp:112] Iteration 65200, lr = 0.01
I0523 06:39:09.300133 34682 solver.cpp:239] Iteration 65210 (1.63885 iter/s, 6.10184s/10 iters), loss = 8.03652
I0523 06:39:09.300179 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03652 (* 1 = 8.03652 loss)
I0523 06:39:10.139561 34682 sgd_solver.cpp:112] Iteration 65210, lr = 0.01
I0523 06:39:14.488162 34682 solver.cpp:239] Iteration 65220 (1.92761 iter/s, 5.18776s/10 iters), loss = 8.65084
I0523 06:39:14.488219 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.65084 (* 1 = 8.65084 loss)
I0523 06:39:14.550741 34682 sgd_solver.cpp:112] Iteration 65220, lr = 0.01
I0523 06:39:19.478289 34682 solver.cpp:239] Iteration 65230 (2.00407 iter/s, 4.98985s/10 iters), loss = 7.96194
I0523 06:39:19.478335 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96194 (* 1 = 7.96194 loss)
I0523 06:39:19.558183 34682 sgd_solver.cpp:112] Iteration 65230, lr = 0.01
I0523 06:39:24.221740 34682 solver.cpp:239] Iteration 65240 (2.1083 iter/s, 4.74316s/10 iters), loss = 8.03939
I0523 06:39:24.221804 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03939 (* 1 = 8.03939 loss)
I0523 06:39:24.289301 34682 sgd_solver.cpp:112] Iteration 65240, lr = 0.01
I0523 06:39:29.141183 34682 solver.cpp:239] Iteration 65250 (2.03286 iter/s, 4.91918s/10 iters), loss = 8.71656
I0523 06:39:29.141234 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71656 (* 1 = 8.71656 loss)
I0523 06:39:29.209532 34682 sgd_solver.cpp:112] Iteration 65250, lr = 0.01
I0523 06:39:32.598978 34682 solver.cpp:239] Iteration 65260 (2.89219 iter/s, 3.45759s/10 iters), loss = 8.28834
I0523 06:39:32.599036 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28834 (* 1 = 8.28834 loss)
I0523 06:39:33.468235 34682 sgd_solver.cpp:112] Iteration 65260, lr = 0.01
I0523 06:39:37.677376 34682 solver.cpp:239] Iteration 65270 (1.96923 iter/s, 5.07812s/10 iters), loss = 7.56878
I0523 06:39:37.677433 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56878 (* 1 = 7.56878 loss)
I0523 06:39:38.478693 34682 sgd_solver.cpp:112] Iteration 65270, lr = 0.01
I0523 06:39:42.898792 34682 solver.cpp:239] Iteration 65280 (1.91529 iter/s, 5.22115s/10 iters), loss = 8.46625
I0523 06:39:42.898838 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46625 (* 1 = 8.46625 loss)
I0523 06:39:43.611903 34682 sgd_solver.cpp:112] Iteration 65280, lr = 0.01
I0523 06:39:49.083272 34682 solver.cpp:239] Iteration 65290 (1.61703 iter/s, 6.18418s/10 iters), loss = 8.59756
I0523 06:39:49.083339 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59756 (* 1 = 8.59756 loss)
I0523 06:39:49.923537 34682 sgd_solver.cpp:112] Iteration 65290, lr = 0.01
I0523 06:39:53.599664 34682 solver.cpp:239] Iteration 65300 (2.21428 iter/s, 4.51614s/10 iters), loss = 8.07813
I0523 06:39:53.599716 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07813 (* 1 = 8.07813 loss)
I0523 06:39:54.385937 34682 sgd_solver.cpp:112] Iteration 65300, lr = 0.01
I0523 06:39:57.764683 34682 solver.cpp:239] Iteration 65310 (2.40108 iter/s, 4.16479s/10 iters), loss = 8.66071
I0523 06:39:57.764752 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66071 (* 1 = 8.66071 loss)
I0523 06:39:57.944023 34682 sgd_solver.cpp:112] Iteration 65310, lr = 0.01
I0523 06:40:00.375036 34682 solver.cpp:239] Iteration 65320 (3.83117 iter/s, 2.61017s/10 iters), loss = 8.42624
I0523 06:40:00.375085 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42624 (* 1 = 8.42624 loss)
I0523 06:40:00.983744 34682 sgd_solver.cpp:112] Iteration 65320, lr = 0.01
I0523 06:40:05.298207 34682 solver.cpp:239] Iteration 65330 (2.03131 iter/s, 4.92292s/10 iters), loss = 8.58651
I0523 06:40:05.298480 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58651 (* 1 = 8.58651 loss)
I0523 06:40:05.366739 34682 sgd_solver.cpp:112] Iteration 65330, lr = 0.01
I0523 06:40:08.676379 34682 solver.cpp:239] Iteration 65340 (2.96052 iter/s, 3.37778s/10 iters), loss = 8.85596
I0523 06:40:08.676427 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85596 (* 1 = 8.85596 loss)
I0523 06:40:09.319753 34682 sgd_solver.cpp:112] Iteration 65340, lr = 0.01
I0523 06:40:15.729336 34682 solver.cpp:239] Iteration 65350 (1.41791 iter/s, 7.05263s/10 iters), loss = 7.71032
I0523 06:40:15.729398 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.71032 (* 1 = 7.71032 loss)
I0523 06:40:15.785326 34682 sgd_solver.cpp:112] Iteration 65350, lr = 0.01
I0523 06:40:19.947093 34682 solver.cpp:239] Iteration 65360 (2.37106 iter/s, 4.21753s/10 iters), loss = 7.91123
I0523 06:40:19.947131 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91123 (* 1 = 7.91123 loss)
I0523 06:40:20.020175 34682 sgd_solver.cpp:112] Iteration 65360, lr = 0.01
I0523 06:40:24.008451 34682 solver.cpp:239] Iteration 65370 (2.46236 iter/s, 4.06115s/10 iters), loss = 8.00676
I0523 06:40:24.008500 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00676 (* 1 = 8.00676 loss)
I0523 06:40:24.075256 34682 sgd_solver.cpp:112] Iteration 65370, lr = 0.01
I0523 06:40:28.920186 34682 solver.cpp:239] Iteration 65380 (2.03605 iter/s, 4.91148s/10 iters), loss = 7.95525
I0523 06:40:28.920243 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95525 (* 1 = 7.95525 loss)
I0523 06:40:29.333163 34682 sgd_solver.cpp:112] Iteration 65380, lr = 0.01
I0523 06:40:32.792698 34682 solver.cpp:239] Iteration 65390 (2.58245 iter/s, 3.87229s/10 iters), loss = 8.0915
I0523 06:40:32.792765 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0915 (* 1 = 8.0915 loss)
I0523 06:40:32.849619 34682 sgd_solver.cpp:112] Iteration 65390, lr = 0.01
I0523 06:40:38.149685 34682 solver.cpp:239] Iteration 65400 (1.86682 iter/s, 5.3567s/10 iters), loss = 7.97791
I0523 06:40:38.149886 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97791 (* 1 = 7.97791 loss)
I0523 06:40:38.223502 34682 sgd_solver.cpp:112] Iteration 65400, lr = 0.01
I0523 06:40:42.462229 34682 solver.cpp:239] Iteration 65410 (2.31901 iter/s, 4.31219s/10 iters), loss = 8.78059
I0523 06:40:42.462294 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78059 (* 1 = 8.78059 loss)
I0523 06:40:43.233830 34682 sgd_solver.cpp:112] Iteration 65410, lr = 0.01
I0523 06:40:48.988247 34682 solver.cpp:239] Iteration 65420 (1.53241 iter/s, 6.52569s/10 iters), loss = 7.7231
I0523 06:40:48.988298 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7231 (* 1 = 7.7231 loss)
I0523 06:40:49.771421 34682 sgd_solver.cpp:112] Iteration 65420, lr = 0.01
I0523 06:40:54.757298 34682 solver.cpp:239] Iteration 65430 (1.73347 iter/s, 5.76877s/10 iters), loss = 8.55231
I0523 06:40:54.757354 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55231 (* 1 = 8.55231 loss)
I0523 06:40:55.364156 34682 sgd_solver.cpp:112] Iteration 65430, lr = 0.01
I0523 06:40:58.539746 34682 solver.cpp:239] Iteration 65440 (2.64394 iter/s, 3.78224s/10 iters), loss = 8.56831
I0523 06:40:58.539788 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56831 (* 1 = 8.56831 loss)
I0523 06:40:58.609320 34682 sgd_solver.cpp:112] Iteration 65440, lr = 0.01
I0523 06:41:02.832293 34682 solver.cpp:239] Iteration 65450 (2.32974 iter/s, 4.29232s/10 iters), loss = 9.54396
I0523 06:41:02.832339 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.54396 (* 1 = 9.54396 loss)
I0523 06:41:02.905238 34682 sgd_solver.cpp:112] Iteration 65450, lr = 0.01
I0523 06:41:06.474537 34682 solver.cpp:239] Iteration 65460 (2.74571 iter/s, 3.64204s/10 iters), loss = 8.19603
I0523 06:41:06.474608 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19603 (* 1 = 8.19603 loss)
I0523 06:41:07.311995 34682 sgd_solver.cpp:112] Iteration 65460, lr = 0.01
I0523 06:41:12.846771 34682 solver.cpp:239] Iteration 65470 (1.56939 iter/s, 6.37191s/10 iters), loss = 8.07731
I0523 06:41:12.847066 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07731 (* 1 = 8.07731 loss)
I0523 06:41:12.933185 34682 sgd_solver.cpp:112] Iteration 65470, lr = 0.01
I0523 06:41:17.903650 34682 solver.cpp:239] Iteration 65480 (1.97769 iter/s, 5.05641s/10 iters), loss = 8.16865
I0523 06:41:17.903697 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16865 (* 1 = 8.16865 loss)
I0523 06:41:17.972117 34682 sgd_solver.cpp:112] Iteration 65480, lr = 0.01
I0523 06:41:20.598038 34682 solver.cpp:239] Iteration 65490 (3.71165 iter/s, 2.69422s/10 iters), loss = 7.29856
I0523 06:41:20.598088 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.29856 (* 1 = 7.29856 loss)
I0523 06:41:20.673033 34682 sgd_solver.cpp:112] Iteration 65490, lr = 0.01
I0523 06:41:25.381829 34682 solver.cpp:239] Iteration 65500 (2.0905 iter/s, 4.78355s/10 iters), loss = 7.61423
I0523 06:41:25.381872 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.61423 (* 1 = 7.61423 loss)
I0523 06:41:25.459868 34682 sgd_solver.cpp:112] Iteration 65500, lr = 0.01
I0523 06:41:31.781510 34682 solver.cpp:239] Iteration 65510 (1.56265 iter/s, 6.39938s/10 iters), loss = 7.60379
I0523 06:41:31.781564 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.60379 (* 1 = 7.60379 loss)
I0523 06:41:31.834753 34682 sgd_solver.cpp:112] Iteration 65510, lr = 0.01
I0523 06:41:36.652760 34682 solver.cpp:239] Iteration 65520 (2.05297 iter/s, 4.87099s/10 iters), loss = 8.80484
I0523 06:41:36.652810 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80484 (* 1 = 8.80484 loss)
I0523 06:41:36.727376 34682 sgd_solver.cpp:112] Iteration 65520, lr = 0.01
I0523 06:41:42.496493 34682 solver.cpp:239] Iteration 65530 (1.71132 iter/s, 5.84345s/10 iters), loss = 8.98346
I0523 06:41:42.496536 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98346 (* 1 = 8.98346 loss)
I0523 06:41:43.325822 34682 sgd_solver.cpp:112] Iteration 65530, lr = 0.01
I0523 06:41:46.243404 34682 solver.cpp:239] Iteration 65540 (2.66902 iter/s, 3.7467s/10 iters), loss = 8.14427
I0523 06:41:46.243456 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14427 (* 1 = 8.14427 loss)
I0523 06:41:46.312794 34682 sgd_solver.cpp:112] Iteration 65540, lr = 0.01
I0523 06:41:50.872295 34682 solver.cpp:239] Iteration 65550 (2.16046 iter/s, 4.62865s/10 iters), loss = 7.59537
I0523 06:41:50.872347 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.59537 (* 1 = 7.59537 loss)
I0523 06:41:51.695264 34682 sgd_solver.cpp:112] Iteration 65550, lr = 0.01
I0523 06:41:55.772163 34682 solver.cpp:239] Iteration 65560 (2.04098 iter/s, 4.89961s/10 iters), loss = 7.61166
I0523 06:41:55.772233 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.61166 (* 1 = 7.61166 loss)
I0523 06:41:56.002923 34682 sgd_solver.cpp:112] Iteration 65560, lr = 0.01
I0523 06:41:59.842473 34682 solver.cpp:239] Iteration 65570 (2.45696 iter/s, 4.07007s/10 iters), loss = 7.42567
I0523 06:41:59.842521 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42567 (* 1 = 7.42567 loss)
I0523 06:42:00.376843 34682 sgd_solver.cpp:112] Iteration 65570, lr = 0.01
I0523 06:42:05.066759 34682 solver.cpp:239] Iteration 65580 (1.91425 iter/s, 5.22398s/10 iters), loss = 8.1822
I0523 06:42:05.066828 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1822 (* 1 = 8.1822 loss)
I0523 06:42:05.916752 34682 sgd_solver.cpp:112] Iteration 65580, lr = 0.01
I0523 06:42:11.375087 34682 solver.cpp:239] Iteration 65590 (1.58529 iter/s, 6.30801s/10 iters), loss = 7.97185
I0523 06:42:11.375139 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97185 (* 1 = 7.97185 loss)
I0523 06:42:11.916623 34682 sgd_solver.cpp:112] Iteration 65590, lr = 0.01
I0523 06:42:15.829391 34682 solver.cpp:239] Iteration 65600 (2.24515 iter/s, 4.45405s/10 iters), loss = 7.74123
I0523 06:42:15.829849 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74123 (* 1 = 7.74123 loss)
I0523 06:42:15.902926 34682 sgd_solver.cpp:112] Iteration 65600, lr = 0.01
I0523 06:42:22.361555 34682 solver.cpp:239] Iteration 65610 (1.53104 iter/s, 6.53152s/10 iters), loss = 9.41139
I0523 06:42:22.361616 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.41139 (* 1 = 9.41139 loss)
I0523 06:42:23.062958 34682 sgd_solver.cpp:112] Iteration 65610, lr = 0.01
I0523 06:42:27.078636 34682 solver.cpp:239] Iteration 65620 (2.12007 iter/s, 4.71683s/10 iters), loss = 7.82955
I0523 06:42:27.078682 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82955 (* 1 = 7.82955 loss)
I0523 06:42:27.137198 34682 sgd_solver.cpp:112] Iteration 65620, lr = 0.01
I0523 06:42:31.151299 34682 solver.cpp:239] Iteration 65630 (2.45553 iter/s, 4.07245s/10 iters), loss = 8.1546
I0523 06:42:31.151357 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1546 (* 1 = 8.1546 loss)
I0523 06:42:31.230124 34682 sgd_solver.cpp:112] Iteration 65630, lr = 0.01
I0523 06:42:36.048730 34682 solver.cpp:239] Iteration 65640 (2.04199 iter/s, 4.89717s/10 iters), loss = 8.95356
I0523 06:42:36.048785 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95356 (* 1 = 8.95356 loss)
I0523 06:42:36.229102 34682 sgd_solver.cpp:112] Iteration 65640, lr = 0.01
I0523 06:42:40.497474 34682 solver.cpp:239] Iteration 65650 (2.24795 iter/s, 4.4485s/10 iters), loss = 8.35341
I0523 06:42:40.497531 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35341 (* 1 = 8.35341 loss)
I0523 06:42:41.353487 34682 sgd_solver.cpp:112] Iteration 65650, lr = 0.01
I0523 06:42:44.656540 34682 solver.cpp:239] Iteration 65660 (2.40452 iter/s, 4.15884s/10 iters), loss = 7.6598
I0523 06:42:44.656586 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6598 (* 1 = 7.6598 loss)
I0523 06:42:44.711467 34682 sgd_solver.cpp:112] Iteration 65660, lr = 0.01
I0523 06:42:49.657359 34682 solver.cpp:239] Iteration 65670 (1.99977 iter/s, 5.00056s/10 iters), loss = 8.39402
I0523 06:42:49.657591 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39402 (* 1 = 8.39402 loss)
I0523 06:42:50.392647 34682 sgd_solver.cpp:112] Iteration 65670, lr = 0.01
I0523 06:42:55.686643 34682 solver.cpp:239] Iteration 65680 (1.6587 iter/s, 6.02882s/10 iters), loss = 8.97065
I0523 06:42:55.686719 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97065 (* 1 = 8.97065 loss)
I0523 06:42:55.765703 34682 sgd_solver.cpp:112] Iteration 65680, lr = 0.01
I0523 06:43:01.814064 34682 solver.cpp:239] Iteration 65690 (1.63209 iter/s, 6.12712s/10 iters), loss = 8.22076
I0523 06:43:01.814116 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22076 (* 1 = 8.22076 loss)
I0523 06:43:02.491871 34682 sgd_solver.cpp:112] Iteration 65690, lr = 0.01
I0523 06:43:05.645687 34682 solver.cpp:239] Iteration 65700 (2.61 iter/s, 3.83141s/10 iters), loss = 7.83073
I0523 06:43:05.645730 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83073 (* 1 = 7.83073 loss)
I0523 06:43:05.713562 34682 sgd_solver.cpp:112] Iteration 65700, lr = 0.01
I0523 06:43:09.706240 34682 solver.cpp:239] Iteration 65710 (2.46286 iter/s, 4.06032s/10 iters), loss = 7.90854
I0523 06:43:09.706301 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90854 (* 1 = 7.90854 loss)
I0523 06:43:10.515887 34682 sgd_solver.cpp:112] Iteration 65710, lr = 0.01
I0523 06:43:15.480110 34682 solver.cpp:239] Iteration 65720 (1.73203 iter/s, 5.77358s/10 iters), loss = 8.22141
I0523 06:43:15.480154 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22141 (* 1 = 8.22141 loss)
I0523 06:43:16.260411 34682 sgd_solver.cpp:112] Iteration 65720, lr = 0.01
I0523 06:43:21.036736 34682 solver.cpp:239] Iteration 65730 (1.79974 iter/s, 5.55635s/10 iters), loss = 8.43043
I0523 06:43:21.037021 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43043 (* 1 = 8.43043 loss)
I0523 06:43:21.090185 34682 sgd_solver.cpp:112] Iteration 65730, lr = 0.01
I0523 06:43:26.731904 34682 solver.cpp:239] Iteration 65740 (1.75603 iter/s, 5.69468s/10 iters), loss = 7.98276
I0523 06:43:26.731954 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98276 (* 1 = 7.98276 loss)
I0523 06:43:26.802666 34682 sgd_solver.cpp:112] Iteration 65740, lr = 0.01
I0523 06:43:30.959767 34682 solver.cpp:239] Iteration 65750 (2.3654 iter/s, 4.22762s/10 iters), loss = 7.06736
I0523 06:43:30.959828 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.06736 (* 1 = 7.06736 loss)
I0523 06:43:31.791107 34682 sgd_solver.cpp:112] Iteration 65750, lr = 0.01
I0523 06:43:36.931846 34682 solver.cpp:239] Iteration 65760 (1.67454 iter/s, 5.97177s/10 iters), loss = 7.17522
I0523 06:43:36.931895 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.17522 (* 1 = 7.17522 loss)
I0523 06:43:37.673693 34682 sgd_solver.cpp:112] Iteration 65760, lr = 0.01
I0523 06:43:42.496773 34682 solver.cpp:239] Iteration 65770 (1.79706 iter/s, 5.56465s/10 iters), loss = 8.24826
I0523 06:43:42.496836 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24826 (* 1 = 8.24826 loss)
I0523 06:43:43.367153 34682 sgd_solver.cpp:112] Iteration 65770, lr = 0.01
I0523 06:43:47.766620 34682 solver.cpp:239] Iteration 65780 (1.89769 iter/s, 5.26957s/10 iters), loss = 8.55925
I0523 06:43:47.766685 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55925 (* 1 = 8.55925 loss)
I0523 06:43:48.551312 34682 sgd_solver.cpp:112] Iteration 65780, lr = 0.01
I0523 06:43:52.416345 34682 solver.cpp:239] Iteration 65790 (2.15078 iter/s, 4.64947s/10 iters), loss = 8.82092
I0523 06:43:52.416602 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82092 (* 1 = 8.82092 loss)
I0523 06:43:52.493300 34682 sgd_solver.cpp:112] Iteration 65790, lr = 0.01
I0523 06:43:57.023602 34682 solver.cpp:239] Iteration 65800 (2.17068 iter/s, 4.60684s/10 iters), loss = 8.5299
I0523 06:43:57.023649 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5299 (* 1 = 8.5299 loss)
I0523 06:43:57.093027 34682 sgd_solver.cpp:112] Iteration 65800, lr = 0.01
I0523 06:44:03.724345 34682 solver.cpp:239] Iteration 65810 (1.49244 iter/s, 6.70042s/10 iters), loss = 8.67462
I0523 06:44:03.724391 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67462 (* 1 = 8.67462 loss)
I0523 06:44:03.787905 34682 sgd_solver.cpp:112] Iteration 65810, lr = 0.01
I0523 06:44:07.942884 34682 solver.cpp:239] Iteration 65820 (2.37061 iter/s, 4.21832s/10 iters), loss = 8.90973
I0523 06:44:07.942946 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90973 (* 1 = 8.90973 loss)
I0523 06:44:08.603854 34682 sgd_solver.cpp:112] Iteration 65820, lr = 0.01
I0523 06:44:12.800710 34682 solver.cpp:239] Iteration 65830 (2.05865 iter/s, 4.85756s/10 iters), loss = 8.87865
I0523 06:44:12.800770 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87865 (* 1 = 8.87865 loss)
I0523 06:44:13.612319 34682 sgd_solver.cpp:112] Iteration 65830, lr = 0.01
I0523 06:44:17.594801 34682 solver.cpp:239] Iteration 65840 (2.08601 iter/s, 4.79383s/10 iters), loss = 7.73896
I0523 06:44:17.594859 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.73896 (* 1 = 7.73896 loss)
I0523 06:44:18.433495 34682 sgd_solver.cpp:112] Iteration 65840, lr = 0.01
I0523 06:44:25.492974 34682 solver.cpp:239] Iteration 65850 (1.26617 iter/s, 7.8978s/10 iters), loss = 8.21148
I0523 06:44:25.493113 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21148 (* 1 = 8.21148 loss)
I0523 06:44:25.567806 34682 sgd_solver.cpp:112] Iteration 65850, lr = 0.01
I0523 06:44:30.623972 34682 solver.cpp:239] Iteration 65860 (1.94907 iter/s, 5.13065s/10 iters), loss = 7.60549
I0523 06:44:30.624022 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.60549 (* 1 = 7.60549 loss)
I0523 06:44:31.369503 34682 sgd_solver.cpp:112] Iteration 65860, lr = 0.01
I0523 06:44:35.269320 34682 solver.cpp:239] Iteration 65870 (2.1528 iter/s, 4.64511s/10 iters), loss = 8.17484
I0523 06:44:35.269366 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17484 (* 1 = 8.17484 loss)
I0523 06:44:36.105983 34682 sgd_solver.cpp:112] Iteration 65870, lr = 0.01
I0523 06:44:41.833518 34682 solver.cpp:239] Iteration 65880 (1.52349 iter/s, 6.56388s/10 iters), loss = 8.00598
I0523 06:44:41.833580 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00598 (* 1 = 8.00598 loss)
I0523 06:44:42.029403 34682 sgd_solver.cpp:112] Iteration 65880, lr = 0.01
I0523 06:44:44.823962 34682 solver.cpp:239] Iteration 65890 (3.3442 iter/s, 2.99026s/10 iters), loss = 7.99831
I0523 06:44:44.824017 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99831 (* 1 = 7.99831 loss)
I0523 06:44:44.882083 34682 sgd_solver.cpp:112] Iteration 65890, lr = 0.01
I0523 06:44:50.058166 34682 solver.cpp:239] Iteration 65900 (1.91061 iter/s, 5.23393s/10 iters), loss = 7.60838
I0523 06:44:50.058218 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.60838 (* 1 = 7.60838 loss)
I0523 06:44:50.897734 34682 sgd_solver.cpp:112] Iteration 65900, lr = 0.01
I0523 06:44:54.805541 34682 solver.cpp:239] Iteration 65910 (2.10654 iter/s, 4.74713s/10 iters), loss = 8.04666
I0523 06:44:54.805582 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04666 (* 1 = 8.04666 loss)
I0523 06:44:55.650205 34682 sgd_solver.cpp:112] Iteration 65910, lr = 0.01
I0523 06:45:01.902310 34682 solver.cpp:239] Iteration 65920 (1.40916 iter/s, 7.09643s/10 iters), loss = 7.37506
I0523 06:45:01.902389 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.37506 (* 1 = 7.37506 loss)
I0523 06:45:01.975805 34682 sgd_solver.cpp:112] Iteration 65920, lr = 0.01
I0523 06:45:08.162220 34682 solver.cpp:239] Iteration 65930 (1.59755 iter/s, 6.25958s/10 iters), loss = 8.49084
I0523 06:45:08.162266 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49084 (* 1 = 8.49084 loss)
I0523 06:45:08.222618 34682 sgd_solver.cpp:112] Iteration 65930, lr = 0.01
I0523 06:45:13.348022 34682 solver.cpp:239] Iteration 65940 (1.92844 iter/s, 5.18553s/10 iters), loss = 7.56836
I0523 06:45:13.348073 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56836 (* 1 = 7.56836 loss)
I0523 06:45:13.416297 34682 sgd_solver.cpp:112] Iteration 65940, lr = 0.01
I0523 06:45:17.410145 34682 solver.cpp:239] Iteration 65950 (2.4619 iter/s, 4.0619s/10 iters), loss = 7.43165
I0523 06:45:17.410192 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43165 (* 1 = 7.43165 loss)
I0523 06:45:18.285858 34682 sgd_solver.cpp:112] Iteration 65950, lr = 0.01
I0523 06:45:24.371851 34682 solver.cpp:239] Iteration 65960 (1.4365 iter/s, 6.96138s/10 iters), loss = 7.23911
I0523 06:45:24.371902 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.23911 (* 1 = 7.23911 loss)
I0523 06:45:24.446738 34682 sgd_solver.cpp:112] Iteration 65960, lr = 0.01
I0523 06:45:30.113557 34682 solver.cpp:239] Iteration 65970 (1.74173 iter/s, 5.74141s/10 iters), loss = 9.6395
I0523 06:45:30.113839 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.6395 (* 1 = 9.6395 loss)
I0523 06:45:30.765313 34682 sgd_solver.cpp:112] Iteration 65970, lr = 0.01
I0523 06:45:36.369750 34682 solver.cpp:239] Iteration 65980 (1.59855 iter/s, 6.25568s/10 iters), loss = 7.9498
I0523 06:45:36.369793 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9498 (* 1 = 7.9498 loss)
I0523 06:45:36.443704 34682 sgd_solver.cpp:112] Iteration 65980, lr = 0.01
I0523 06:45:40.689642 34682 solver.cpp:239] Iteration 65990 (2.31734 iter/s, 4.31529s/10 iters), loss = 8.63028
I0523 06:45:40.689688 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63028 (* 1 = 8.63028 loss)
I0523 06:45:40.759418 34682 sgd_solver.cpp:112] Iteration 65990, lr = 0.01
I0523 06:45:46.505563 34682 solver.cpp:239] Iteration 66000 (1.71951 iter/s, 5.81563s/10 iters), loss = 9.43433
I0523 06:45:46.505616 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.43433 (* 1 = 9.43433 loss)
I0523 06:45:47.103036 34682 sgd_solver.cpp:112] Iteration 66000, lr = 0.01
I0523 06:45:52.047976 34682 solver.cpp:239] Iteration 66010 (1.80436 iter/s, 5.54213s/10 iters), loss = 8.10676
I0523 06:45:52.048023 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10676 (* 1 = 8.10676 loss)
I0523 06:45:52.854117 34682 sgd_solver.cpp:112] Iteration 66010, lr = 0.01
I0523 06:45:56.210269 34682 solver.cpp:239] Iteration 66020 (2.40265 iter/s, 4.16207s/10 iters), loss = 7.67426
I0523 06:45:56.210309 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.67426 (* 1 = 7.67426 loss)
I0523 06:45:56.290283 34682 sgd_solver.cpp:112] Iteration 66020, lr = 0.01
I0523 06:46:01.469975 34682 solver.cpp:239] Iteration 66030 (1.90134 iter/s, 5.25944s/10 iters), loss = 8.23866
I0523 06:46:01.470154 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23866 (* 1 = 8.23866 loss)
I0523 06:46:01.537025 34682 sgd_solver.cpp:112] Iteration 66030, lr = 0.01
I0523 06:46:06.694836 34682 solver.cpp:239] Iteration 66040 (1.91406 iter/s, 5.22449s/10 iters), loss = 7.70776
I0523 06:46:06.694878 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.70776 (* 1 = 7.70776 loss)
I0523 06:46:06.760990 34682 sgd_solver.cpp:112] Iteration 66040, lr = 0.01
I0523 06:46:11.861023 34682 solver.cpp:239] Iteration 66050 (1.93576 iter/s, 5.16593s/10 iters), loss = 6.75972
I0523 06:46:11.861078 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.75972 (* 1 = 6.75972 loss)
I0523 06:46:12.593601 34682 sgd_solver.cpp:112] Iteration 66050, lr = 0.01
I0523 06:46:14.409651 34682 solver.cpp:239] Iteration 66060 (3.92394 iter/s, 2.54846s/10 iters), loss = 8.21589
I0523 06:46:14.409694 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21589 (* 1 = 8.21589 loss)
I0523 06:46:15.165619 34682 sgd_solver.cpp:112] Iteration 66060, lr = 0.01
I0523 06:46:19.541786 34682 solver.cpp:239] Iteration 66070 (1.94861 iter/s, 5.13187s/10 iters), loss = 7.98889
I0523 06:46:19.541842 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98889 (* 1 = 7.98889 loss)
I0523 06:46:19.774307 34682 sgd_solver.cpp:112] Iteration 66070, lr = 0.01
I0523 06:46:22.392586 34682 solver.cpp:239] Iteration 66080 (3.50803 iter/s, 2.8506s/10 iters), loss = 8.48799
I0523 06:46:22.392635 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48799 (* 1 = 8.48799 loss)
I0523 06:46:23.191109 34682 sgd_solver.cpp:112] Iteration 66080, lr = 0.01
I0523 06:46:27.567445 34682 solver.cpp:239] Iteration 66090 (1.93252 iter/s, 5.17459s/10 iters), loss = 8.44933
I0523 06:46:27.567494 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44933 (* 1 = 8.44933 loss)
I0523 06:46:27.631702 34682 sgd_solver.cpp:112] Iteration 66090, lr = 0.01
I0523 06:46:33.247714 34682 solver.cpp:239] Iteration 66100 (1.76057 iter/s, 5.67999s/10 iters), loss = 8.23988
I0523 06:46:33.247970 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23988 (* 1 = 8.23988 loss)
I0523 06:46:33.324332 34682 sgd_solver.cpp:112] Iteration 66100, lr = 0.01
I0523 06:46:36.578512 34682 solver.cpp:239] Iteration 66110 (3.00261 iter/s, 3.33043s/10 iters), loss = 7.88833
I0523 06:46:36.578563 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88833 (* 1 = 7.88833 loss)
I0523 06:46:37.397387 34682 sgd_solver.cpp:112] Iteration 66110, lr = 0.01
I0523 06:46:42.923241 34682 solver.cpp:239] Iteration 66120 (1.57619 iter/s, 6.34442s/10 iters), loss = 8.81252
I0523 06:46:42.923280 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81252 (* 1 = 8.81252 loss)
I0523 06:46:43.001047 34682 sgd_solver.cpp:112] Iteration 66120, lr = 0.01
I0523 06:46:48.628639 34682 solver.cpp:239] Iteration 66130 (1.75281 iter/s, 5.70512s/10 iters), loss = 8.32203
I0523 06:46:48.628685 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32203 (* 1 = 8.32203 loss)
I0523 06:46:49.257675 34682 sgd_solver.cpp:112] Iteration 66130, lr = 0.01
I0523 06:46:54.908381 34682 solver.cpp:239] Iteration 66140 (1.5925 iter/s, 6.27944s/10 iters), loss = 7.78456
I0523 06:46:54.908438 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.78456 (* 1 = 7.78456 loss)
I0523 06:46:55.621275 34682 sgd_solver.cpp:112] Iteration 66140, lr = 0.01
I0523 06:47:01.047569 34682 solver.cpp:239] Iteration 66150 (1.62896 iter/s, 6.13888s/10 iters), loss = 8.96729
I0523 06:47:01.047610 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96729 (* 1 = 8.96729 loss)
I0523 06:47:01.134860 34682 sgd_solver.cpp:112] Iteration 66150, lr = 0.01
I0523 06:47:06.112540 34682 solver.cpp:239] Iteration 66160 (1.97444 iter/s, 5.06472s/10 iters), loss = 7.96181
I0523 06:47:06.112768 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96181 (* 1 = 7.96181 loss)
I0523 06:47:06.945430 34682 sgd_solver.cpp:112] Iteration 66160, lr = 0.01
I0523 06:47:11.164574 34682 solver.cpp:239] Iteration 66170 (1.97956 iter/s, 5.05162s/10 iters), loss = 6.7022
I0523 06:47:11.164619 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.7022 (* 1 = 6.7022 loss)
I0523 06:47:11.227339 34682 sgd_solver.cpp:112] Iteration 66170, lr = 0.01
I0523 06:47:15.597806 34682 solver.cpp:239] Iteration 66180 (2.25581 iter/s, 4.433s/10 iters), loss = 7.97395
I0523 06:47:15.597862 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97395 (* 1 = 7.97395 loss)
I0523 06:47:16.421386 34682 sgd_solver.cpp:112] Iteration 66180, lr = 0.01
I0523 06:47:20.533584 34682 solver.cpp:239] Iteration 66190 (2.02613 iter/s, 4.93552s/10 iters), loss = 8.04236
I0523 06:47:20.533639 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04236 (* 1 = 8.04236 loss)
I0523 06:47:20.601186 34682 sgd_solver.cpp:112] Iteration 66190, lr = 0.01
I0523 06:47:25.376787 34682 solver.cpp:239] Iteration 66200 (2.06486 iter/s, 4.84295s/10 iters), loss = 7.29734
I0523 06:47:25.376852 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.29734 (* 1 = 7.29734 loss)
I0523 06:47:25.442037 34682 sgd_solver.cpp:112] Iteration 66200, lr = 0.01
I0523 06:47:29.954589 34682 solver.cpp:239] Iteration 66210 (2.18458 iter/s, 4.57754s/10 iters), loss = 8.01437
I0523 06:47:29.954653 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01437 (* 1 = 8.01437 loss)
I0523 06:47:30.021325 34682 sgd_solver.cpp:112] Iteration 66210, lr = 0.01
I0523 06:47:34.403110 34682 solver.cpp:239] Iteration 66220 (2.24806 iter/s, 4.44827s/10 iters), loss = 7.42074
I0523 06:47:34.403163 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42074 (* 1 = 7.42074 loss)
I0523 06:47:35.101073 34682 sgd_solver.cpp:112] Iteration 66220, lr = 0.01
I0523 06:47:39.011191 34682 solver.cpp:239] Iteration 66230 (2.17022 iter/s, 4.60784s/10 iters), loss = 8.03297
I0523 06:47:39.011279 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03297 (* 1 = 8.03297 loss)
I0523 06:47:39.090106 34682 sgd_solver.cpp:112] Iteration 66230, lr = 0.01
I0523 06:47:43.318260 34682 solver.cpp:239] Iteration 66240 (2.32191 iter/s, 4.30681s/10 iters), loss = 7.66479
I0523 06:47:43.318320 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66479 (* 1 = 7.66479 loss)
I0523 06:47:43.380751 34682 sgd_solver.cpp:112] Iteration 66240, lr = 0.01
I0523 06:47:49.083878 34682 solver.cpp:239] Iteration 66250 (1.73451 iter/s, 5.76531s/10 iters), loss = 7.7045
I0523 06:47:49.083940 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7045 (* 1 = 7.7045 loss)
I0523 06:47:49.154973 34682 sgd_solver.cpp:112] Iteration 66250, lr = 0.01
I0523 06:47:56.371047 34682 solver.cpp:239] Iteration 66260 (1.37234 iter/s, 7.28681s/10 iters), loss = 8.51762
I0523 06:47:56.371104 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51762 (* 1 = 8.51762 loss)
I0523 06:47:56.428705 34682 sgd_solver.cpp:112] Iteration 66260, lr = 0.01
I0523 06:48:02.852871 34682 solver.cpp:239] Iteration 66270 (1.54285 iter/s, 6.48151s/10 iters), loss = 7.1032
I0523 06:48:02.852917 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.1032 (* 1 = 7.1032 loss)
I0523 06:48:03.681563 34682 sgd_solver.cpp:112] Iteration 66270, lr = 0.01
I0523 06:48:06.864065 34682 solver.cpp:239] Iteration 66280 (2.49456 iter/s, 4.00872s/10 iters), loss = 8.89628
I0523 06:48:06.864106 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89628 (* 1 = 8.89628 loss)
I0523 06:48:06.937032 34682 sgd_solver.cpp:112] Iteration 66280, lr = 0.01
I0523 06:48:12.413620 34682 solver.cpp:239] Iteration 66290 (1.80203 iter/s, 5.54928s/10 iters), loss = 8.85377
I0523 06:48:12.413818 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85377 (* 1 = 8.85377 loss)
I0523 06:48:12.486551 34682 sgd_solver.cpp:112] Iteration 66290, lr = 0.01
I0523 06:48:18.513730 34682 solver.cpp:239] Iteration 66300 (1.6401 iter/s, 6.09719s/10 iters), loss = 7.84095
I0523 06:48:18.513765 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84095 (* 1 = 7.84095 loss)
I0523 06:48:18.583235 34682 sgd_solver.cpp:112] Iteration 66300, lr = 0.01
I0523 06:48:24.594898 34682 solver.cpp:239] Iteration 66310 (1.6445 iter/s, 6.08089s/10 iters), loss = 8.2208
I0523 06:48:24.594944 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2208 (* 1 = 8.2208 loss)
I0523 06:48:24.662622 34682 sgd_solver.cpp:112] Iteration 66310, lr = 0.01
I0523 06:48:30.305443 34682 solver.cpp:239] Iteration 66320 (1.75123 iter/s, 5.71027s/10 iters), loss = 8.19122
I0523 06:48:30.305491 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19122 (* 1 = 8.19122 loss)
I0523 06:48:31.161238 34682 sgd_solver.cpp:112] Iteration 66320, lr = 0.01
I0523 06:48:35.337833 34682 solver.cpp:239] Iteration 66330 (1.98723 iter/s, 5.03214s/10 iters), loss = 8.06239
I0523 06:48:35.337877 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06239 (* 1 = 8.06239 loss)
I0523 06:48:35.399817 34682 sgd_solver.cpp:112] Iteration 66330, lr = 0.01
I0523 06:48:39.992823 34682 solver.cpp:239] Iteration 66340 (2.14834 iter/s, 4.65475s/10 iters), loss = 8.20561
I0523 06:48:39.992874 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20561 (* 1 = 8.20561 loss)
I0523 06:48:40.055531 34682 sgd_solver.cpp:112] Iteration 66340, lr = 0.01
I0523 06:48:45.839795 34682 solver.cpp:239] Iteration 66350 (1.71037 iter/s, 5.84668s/10 iters), loss = 7.28255
I0523 06:48:45.840030 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.28255 (* 1 = 7.28255 loss)
I0523 06:48:45.919070 34682 sgd_solver.cpp:112] Iteration 66350, lr = 0.01
I0523 06:48:50.864218 34682 solver.cpp:239] Iteration 66360 (1.99212 iter/s, 5.01979s/10 iters), loss = 8.14033
I0523 06:48:50.864284 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14033 (* 1 = 8.14033 loss)
I0523 06:48:51.642045 34682 sgd_solver.cpp:112] Iteration 66360, lr = 0.01
I0523 06:48:56.070943 34682 solver.cpp:239] Iteration 66370 (1.9207 iter/s, 5.20644s/10 iters), loss = 7.45258
I0523 06:48:56.071009 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.45258 (* 1 = 7.45258 loss)
I0523 06:48:56.861510 34682 sgd_solver.cpp:112] Iteration 66370, lr = 0.01
I0523 06:49:00.275151 34682 solver.cpp:239] Iteration 66380 (2.3787 iter/s, 4.20397s/10 iters), loss = 7.20154
I0523 06:49:00.275199 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.20154 (* 1 = 7.20154 loss)
I0523 06:49:01.134191 34682 sgd_solver.cpp:112] Iteration 66380, lr = 0.01
I0523 06:49:03.556408 34682 solver.cpp:239] Iteration 66390 (3.04779 iter/s, 3.28106s/10 iters), loss = 8.44734
I0523 06:49:03.556473 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44734 (* 1 = 8.44734 loss)
I0523 06:49:04.324012 34682 sgd_solver.cpp:112] Iteration 66390, lr = 0.01
I0523 06:49:08.913583 34682 solver.cpp:239] Iteration 66400 (1.86675 iter/s, 5.3569s/10 iters), loss = 8.10893
I0523 06:49:08.913625 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10893 (* 1 = 8.10893 loss)
I0523 06:49:08.994475 34682 sgd_solver.cpp:112] Iteration 66400, lr = 0.01
I0523 06:49:11.908646 34682 solver.cpp:239] Iteration 66410 (3.33902 iter/s, 2.99489s/10 iters), loss = 8.33707
I0523 06:49:11.908687 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33707 (* 1 = 8.33707 loss)
I0523 06:49:11.987506 34682 sgd_solver.cpp:112] Iteration 66410, lr = 0.01
I0523 06:49:16.313758 34682 solver.cpp:239] Iteration 66420 (2.2702 iter/s, 4.40489s/10 iters), loss = 9.08691
I0523 06:49:16.313937 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.08691 (* 1 = 9.08691 loss)
I0523 06:49:16.368688 34682 sgd_solver.cpp:112] Iteration 66420, lr = 0.01
I0523 06:49:19.037856 34682 solver.cpp:239] Iteration 66430 (3.67133 iter/s, 2.72381s/10 iters), loss = 7.19456
I0523 06:49:19.037920 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.19456 (* 1 = 7.19456 loss)
I0523 06:49:19.558594 34682 sgd_solver.cpp:112] Iteration 66430, lr = 0.01
I0523 06:49:23.341658 34682 solver.cpp:239] Iteration 66440 (2.32366 iter/s, 4.30356s/10 iters), loss = 7.90647
I0523 06:49:23.341706 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90647 (* 1 = 7.90647 loss)
I0523 06:49:24.006227 34682 sgd_solver.cpp:112] Iteration 66440, lr = 0.01
I0523 06:49:28.888478 34682 solver.cpp:239] Iteration 66450 (1.80293 iter/s, 5.54654s/10 iters), loss = 7.90984
I0523 06:49:28.888527 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90984 (* 1 = 7.90984 loss)
I0523 06:49:28.958883 34682 sgd_solver.cpp:112] Iteration 66450, lr = 0.01
I0523 06:49:32.086300 34682 solver.cpp:239] Iteration 66460 (3.12731 iter/s, 3.19764s/10 iters), loss = 8.61255
I0523 06:49:32.086349 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61255 (* 1 = 8.61255 loss)
I0523 06:49:32.154050 34682 sgd_solver.cpp:112] Iteration 66460, lr = 0.01
I0523 06:49:36.747463 34682 solver.cpp:239] Iteration 66470 (2.1455 iter/s, 4.66091s/10 iters), loss = 8.11689
I0523 06:49:36.747529 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11689 (* 1 = 8.11689 loss)
I0523 06:49:37.482635 34682 sgd_solver.cpp:112] Iteration 66470, lr = 0.01
I0523 06:49:43.916401 34682 solver.cpp:239] Iteration 66480 (1.39498 iter/s, 7.16859s/10 iters), loss = 8.46361
I0523 06:49:43.916450 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46361 (* 1 = 8.46361 loss)
I0523 06:49:44.760512 34682 sgd_solver.cpp:112] Iteration 66480, lr = 0.01
I0523 06:49:49.369922 34682 solver.cpp:239] Iteration 66490 (1.83377 iter/s, 5.45325s/10 iters), loss = 7.4798
I0523 06:49:49.370025 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4798 (* 1 = 7.4798 loss)
I0523 06:49:50.162765 34682 sgd_solver.cpp:112] Iteration 66490, lr = 0.01
I0523 06:49:55.217772 34682 solver.cpp:239] Iteration 66500 (1.71013 iter/s, 5.84751s/10 iters), loss = 7.67641
I0523 06:49:55.217845 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.67641 (* 1 = 7.67641 loss)
I0523 06:49:55.253522 34682 sgd_solver.cpp:112] Iteration 66500, lr = 0.01
I0523 06:49:56.420188 34682 solver.cpp:239] Iteration 66510 (8.31748 iter/s, 1.20229s/10 iters), loss = 7.81602
I0523 06:49:56.420240 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81602 (* 1 = 7.81602 loss)
I0523 06:49:56.466445 34682 sgd_solver.cpp:112] Iteration 66510, lr = 0.01
I0523 06:49:57.908540 34682 solver.cpp:239] Iteration 66520 (6.71939 iter/s, 1.48823s/10 iters), loss = 7.79713
I0523 06:49:57.908581 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79713 (* 1 = 7.79713 loss)
I0523 06:49:57.952744 34682 sgd_solver.cpp:112] Iteration 66520, lr = 0.01
I0523 06:49:59.123169 34682 solver.cpp:239] Iteration 66530 (8.23375 iter/s, 1.21451s/10 iters), loss = 8.24104
I0523 06:49:59.123245 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24104 (* 1 = 8.24104 loss)
I0523 06:49:59.158916 34682 sgd_solver.cpp:112] Iteration 66530, lr = 0.01
I0523 06:50:00.325054 34682 solver.cpp:239] Iteration 66540 (8.32111 iter/s, 1.20176s/10 iters), loss = 7.82163
I0523 06:50:00.325095 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82163 (* 1 = 7.82163 loss)
I0523 06:50:00.362972 34682 sgd_solver.cpp:112] Iteration 66540, lr = 0.01
I0523 06:50:01.575634 34682 solver.cpp:239] Iteration 66550 (7.99693 iter/s, 1.25048s/10 iters), loss = 7.42844
I0523 06:50:01.575690 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42844 (* 1 = 7.42844 loss)
I0523 06:50:01.623770 34682 sgd_solver.cpp:112] Iteration 66550, lr = 0.01
I0523 06:50:02.770617 34682 solver.cpp:239] Iteration 66560 (8.36911 iter/s, 1.19487s/10 iters), loss = 7.76489
I0523 06:50:02.770656 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.76489 (* 1 = 7.76489 loss)
I0523 06:50:02.812155 34682 sgd_solver.cpp:112] Iteration 66560, lr = 0.01
I0523 06:50:05.015841 34682 solver.cpp:239] Iteration 66570 (4.45418 iter/s, 2.24508s/10 iters), loss = 8.23076
I0523 06:50:05.015889 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23076 (* 1 = 8.23076 loss)
I0523 06:50:05.675303 34682 sgd_solver.cpp:112] Iteration 66570, lr = 0.01
I0523 06:50:09.123759 34682 solver.cpp:239] Iteration 66580 (2.43445 iter/s, 4.1077s/10 iters), loss = 7.81376
I0523 06:50:09.123801 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81376 (* 1 = 7.81376 loss)
I0523 06:50:09.197417 34682 sgd_solver.cpp:112] Iteration 66580, lr = 0.01
I0523 06:50:13.983788 34682 solver.cpp:239] Iteration 66590 (2.0577 iter/s, 4.85978s/10 iters), loss = 8.54272
I0523 06:50:13.983842 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54272 (* 1 = 8.54272 loss)
I0523 06:50:14.049911 34682 sgd_solver.cpp:112] Iteration 66590, lr = 0.01
I0523 06:50:20.252758 34682 solver.cpp:239] Iteration 66600 (1.59524 iter/s, 6.26866s/10 iters), loss = 7.58916
I0523 06:50:20.252974 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58916 (* 1 = 7.58916 loss)
I0523 06:50:21.100041 34682 sgd_solver.cpp:112] Iteration 66600, lr = 0.01
I0523 06:50:24.350903 34682 solver.cpp:239] Iteration 66610 (2.44035 iter/s, 4.09778s/10 iters), loss = 7.98297
I0523 06:50:24.350950 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98297 (* 1 = 7.98297 loss)
I0523 06:50:24.416183 34682 sgd_solver.cpp:112] Iteration 66610, lr = 0.01
I0523 06:50:28.780105 34682 solver.cpp:239] Iteration 66620 (2.25786 iter/s, 4.42897s/10 iters), loss = 8.00733
I0523 06:50:28.780145 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00733 (* 1 = 8.00733 loss)
I0523 06:50:28.850409 34682 sgd_solver.cpp:112] Iteration 66620, lr = 0.01
I0523 06:50:36.695261 34682 solver.cpp:239] Iteration 66630 (1.26346 iter/s, 7.9148s/10 iters), loss = 7.46962
I0523 06:50:36.695299 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.46962 (* 1 = 7.46962 loss)
I0523 06:50:36.763790 34682 sgd_solver.cpp:112] Iteration 66630, lr = 0.01
I0523 06:50:41.519503 34682 solver.cpp:239] Iteration 66640 (2.07487 iter/s, 4.81959s/10 iters), loss = 7.60085
I0523 06:50:41.519544 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.60085 (* 1 = 7.60085 loss)
I0523 06:50:41.596249 34682 sgd_solver.cpp:112] Iteration 66640, lr = 0.01
I0523 06:50:44.215530 34682 solver.cpp:239] Iteration 66650 (3.70938 iter/s, 2.69587s/10 iters), loss = 7.67972
I0523 06:50:44.215575 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.67972 (* 1 = 7.67972 loss)
I0523 06:50:44.283052 34682 sgd_solver.cpp:112] Iteration 66650, lr = 0.01
I0523 06:50:48.376907 34682 solver.cpp:239] Iteration 66660 (2.40317 iter/s, 4.16116s/10 iters), loss = 7.44951
I0523 06:50:48.376948 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44951 (* 1 = 7.44951 loss)
I0523 06:50:48.457653 34682 sgd_solver.cpp:112] Iteration 66660, lr = 0.01
I0523 06:50:53.025856 34682 solver.cpp:239] Iteration 66670 (2.15113 iter/s, 4.64871s/10 iters), loss = 8.10829
I0523 06:50:53.026079 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10829 (* 1 = 8.10829 loss)
I0523 06:50:53.083815 34682 sgd_solver.cpp:112] Iteration 66670, lr = 0.01
I0523 06:50:57.837361 34682 solver.cpp:239] Iteration 66680 (2.07853 iter/s, 4.81109s/10 iters), loss = 7.88495
I0523 06:50:57.837412 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88495 (* 1 = 7.88495 loss)
I0523 06:50:58.520860 34682 sgd_solver.cpp:112] Iteration 66680, lr = 0.01
I0523 06:51:01.794600 34682 solver.cpp:239] Iteration 66690 (2.52716 iter/s, 3.95702s/10 iters), loss = 7.61068
I0523 06:51:01.794644 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.61068 (* 1 = 7.61068 loss)
I0523 06:51:02.638733 34682 sgd_solver.cpp:112] Iteration 66690, lr = 0.01
I0523 06:51:08.743084 34682 solver.cpp:239] Iteration 66700 (1.43923 iter/s, 6.94816s/10 iters), loss = 8.35647
I0523 06:51:08.743124 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35647 (* 1 = 8.35647 loss)
I0523 06:51:08.806601 34682 sgd_solver.cpp:112] Iteration 66700, lr = 0.01
I0523 06:51:13.562888 34682 solver.cpp:239] Iteration 66710 (2.07488 iter/s, 4.81956s/10 iters), loss = 7.34071
I0523 06:51:13.562938 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.34071 (* 1 = 7.34071 loss)
I0523 06:51:14.352757 34682 sgd_solver.cpp:112] Iteration 66710, lr = 0.01
I0523 06:51:17.708210 34682 solver.cpp:239] Iteration 66720 (2.41249 iter/s, 4.14509s/10 iters), loss = 8.30199
I0523 06:51:17.708277 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30199 (* 1 = 8.30199 loss)
I0523 06:51:17.778075 34682 sgd_solver.cpp:112] Iteration 66720, lr = 0.01
I0523 06:51:21.742772 34682 solver.cpp:239] Iteration 66730 (2.47873 iter/s, 4.03433s/10 iters), loss = 7.93801
I0523 06:51:21.742818 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93801 (* 1 = 7.93801 loss)
I0523 06:51:22.533927 34682 sgd_solver.cpp:112] Iteration 66730, lr = 0.01
I0523 06:51:28.820382 34682 solver.cpp:239] Iteration 66740 (1.41297 iter/s, 7.07727s/10 iters), loss = 7.68414
I0523 06:51:28.820523 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68414 (* 1 = 7.68414 loss)
I0523 06:51:29.684594 34682 sgd_solver.cpp:112] Iteration 66740, lr = 0.01
I0523 06:51:34.319242 34682 solver.cpp:239] Iteration 66750 (1.81868 iter/s, 5.4985s/10 iters), loss = 7.38643
I0523 06:51:34.319288 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.38643 (* 1 = 7.38643 loss)
I0523 06:51:34.397964 34682 sgd_solver.cpp:112] Iteration 66750, lr = 0.01
I0523 06:51:38.779358 34682 solver.cpp:239] Iteration 66760 (2.24221 iter/s, 4.45989s/10 iters), loss = 7.57407
I0523 06:51:38.779404 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.57407 (* 1 = 7.57407 loss)
I0523 06:51:38.847308 34682 sgd_solver.cpp:112] Iteration 66760, lr = 0.01
I0523 06:51:43.915999 34682 solver.cpp:239] Iteration 66770 (1.9469 iter/s, 5.13636s/10 iters), loss = 7.57027
I0523 06:51:43.916072 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.57027 (* 1 = 7.57027 loss)
I0523 06:51:43.986279 34682 sgd_solver.cpp:112] Iteration 66770, lr = 0.01
I0523 06:51:49.581013 34682 solver.cpp:239] Iteration 66780 (1.76531 iter/s, 5.66472s/10 iters), loss = 7.70335
I0523 06:51:49.581063 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.70335 (* 1 = 7.70335 loss)
I0523 06:51:49.772249 34682 sgd_solver.cpp:112] Iteration 66780, lr = 0.01
I0523 06:51:54.841045 34682 solver.cpp:239] Iteration 66790 (1.90123 iter/s, 5.25977s/10 iters), loss = 7.88905
I0523 06:51:54.841094 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88905 (* 1 = 7.88905 loss)
I0523 06:51:54.898665 34682 sgd_solver.cpp:112] Iteration 66790, lr = 0.01
I0523 06:51:59.802893 34682 solver.cpp:239] Iteration 66800 (2.01548 iter/s, 4.9616s/10 iters), loss = 8.2437
I0523 06:51:59.803083 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2437 (* 1 = 8.2437 loss)
I0523 06:51:59.866578 34682 sgd_solver.cpp:112] Iteration 66800, lr = 0.01
I0523 06:52:04.644160 34682 solver.cpp:239] Iteration 66810 (2.06574 iter/s, 4.84088s/10 iters), loss = 8.12594
I0523 06:52:04.644224 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12594 (* 1 = 8.12594 loss)
I0523 06:52:05.295915 34682 sgd_solver.cpp:112] Iteration 66810, lr = 0.01
I0523 06:52:09.514310 34682 solver.cpp:239] Iteration 66820 (2.05344 iter/s, 4.86989s/10 iters), loss = 7.97704
I0523 06:52:09.514367 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97704 (* 1 = 7.97704 loss)
I0523 06:52:09.766389 34682 sgd_solver.cpp:112] Iteration 66820, lr = 0.01
I0523 06:52:13.890907 34682 solver.cpp:239] Iteration 66830 (2.28501 iter/s, 4.37635s/10 iters), loss = 7.9859
I0523 06:52:13.890966 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9859 (* 1 = 7.9859 loss)
I0523 06:52:14.725538 34682 sgd_solver.cpp:112] Iteration 66830, lr = 0.01
I0523 06:52:19.006886 34682 solver.cpp:239] Iteration 66840 (1.95477 iter/s, 5.1157s/10 iters), loss = 8.4694
I0523 06:52:19.006952 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4694 (* 1 = 8.4694 loss)
I0523 06:52:19.810997 34682 sgd_solver.cpp:112] Iteration 66840, lr = 0.01
I0523 06:52:23.907763 34682 solver.cpp:239] Iteration 66850 (2.04056 iter/s, 4.90061s/10 iters), loss = 7.92695
I0523 06:52:23.907814 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92695 (* 1 = 7.92695 loss)
I0523 06:52:23.977126 34682 sgd_solver.cpp:112] Iteration 66850, lr = 0.01
I0523 06:52:29.824476 34682 solver.cpp:239] Iteration 66860 (1.69021 iter/s, 5.91643s/10 iters), loss = 8.72319
I0523 06:52:29.824611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72319 (* 1 = 8.72319 loss)
I0523 06:52:30.469774 34682 sgd_solver.cpp:112] Iteration 66860, lr = 0.01
I0523 06:52:33.740792 34682 solver.cpp:239] Iteration 66870 (2.55361 iter/s, 3.91602s/10 iters), loss = 7.35072
I0523 06:52:33.740834 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.35072 (* 1 = 7.35072 loss)
I0523 06:52:33.802243 34682 sgd_solver.cpp:112] Iteration 66870, lr = 0.01
I0523 06:52:37.547879 34682 solver.cpp:239] Iteration 66880 (2.62682 iter/s, 3.80689s/10 iters), loss = 8.20428
I0523 06:52:37.547920 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20428 (* 1 = 8.20428 loss)
I0523 06:52:37.618188 34682 sgd_solver.cpp:112] Iteration 66880, lr = 0.01
I0523 06:52:41.454493 34682 solver.cpp:239] Iteration 66890 (2.5599 iter/s, 3.90641s/10 iters), loss = 8.24734
I0523 06:52:41.454535 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24734 (* 1 = 8.24734 loss)
I0523 06:52:41.523269 34682 sgd_solver.cpp:112] Iteration 66890, lr = 0.01
I0523 06:52:44.122320 34682 solver.cpp:239] Iteration 66900 (3.74859 iter/s, 2.66767s/10 iters), loss = 7.8324
I0523 06:52:44.122364 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8324 (* 1 = 7.8324 loss)
I0523 06:52:44.184005 34682 sgd_solver.cpp:112] Iteration 66900, lr = 0.01
I0523 06:52:49.430595 34682 solver.cpp:239] Iteration 66910 (1.88394 iter/s, 5.30801s/10 iters), loss = 7.77505
I0523 06:52:49.430644 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77505 (* 1 = 7.77505 loss)
I0523 06:52:49.508265 34682 sgd_solver.cpp:112] Iteration 66910, lr = 0.01
I0523 06:52:53.470353 34682 solver.cpp:239] Iteration 66920 (2.47553 iter/s, 4.03954s/10 iters), loss = 7.69016
I0523 06:52:53.470393 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69016 (* 1 = 7.69016 loss)
I0523 06:52:53.531265 34682 sgd_solver.cpp:112] Iteration 66920, lr = 0.01
I0523 06:52:59.069278 34682 solver.cpp:239] Iteration 66930 (1.78614 iter/s, 5.59865s/10 iters), loss = 8.12323
I0523 06:52:59.069316 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12323 (* 1 = 8.12323 loss)
I0523 06:52:59.140583 34682 sgd_solver.cpp:112] Iteration 66930, lr = 0.01
I0523 06:53:04.480031 34682 solver.cpp:239] Iteration 66940 (1.84826 iter/s, 5.41049s/10 iters), loss = 8.78326
I0523 06:53:04.480293 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78326 (* 1 = 8.78326 loss)
I0523 06:53:04.851548 34682 sgd_solver.cpp:112] Iteration 66940, lr = 0.01
I0523 06:53:09.362336 34682 solver.cpp:239] Iteration 66950 (2.04839 iter/s, 4.88187s/10 iters), loss = 7.9459
I0523 06:53:09.362387 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9459 (* 1 = 7.9459 loss)
I0523 06:53:10.108603 34682 sgd_solver.cpp:112] Iteration 66950, lr = 0.01
I0523 06:53:15.442791 34682 solver.cpp:239] Iteration 66960 (1.6447 iter/s, 6.08015s/10 iters), loss = 8.03962
I0523 06:53:15.442852 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03962 (* 1 = 8.03962 loss)
I0523 06:53:16.156667 34682 sgd_solver.cpp:112] Iteration 66960, lr = 0.01
I0523 06:53:21.781424 34682 solver.cpp:239] Iteration 66970 (1.57771 iter/s, 6.33832s/10 iters), loss = 8.97165
I0523 06:53:21.781476 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.97165 (* 1 = 8.97165 loss)
I0523 06:53:22.618187 34682 sgd_solver.cpp:112] Iteration 66970, lr = 0.01
I0523 06:53:26.599262 34682 solver.cpp:239] Iteration 66980 (2.07573 iter/s, 4.81758s/10 iters), loss = 8.21337
I0523 06:53:26.599315 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21337 (* 1 = 8.21337 loss)
I0523 06:53:26.665910 34682 sgd_solver.cpp:112] Iteration 66980, lr = 0.01
I0523 06:53:30.859776 34682 solver.cpp:239] Iteration 66990 (2.34726 iter/s, 4.26028s/10 iters), loss = 7.44699
I0523 06:53:30.859818 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44699 (* 1 = 7.44699 loss)
I0523 06:53:30.934725 34682 sgd_solver.cpp:112] Iteration 66990, lr = 0.01
I0523 06:53:35.896687 34682 solver.cpp:239] Iteration 67000 (1.98545 iter/s, 5.03665s/10 iters), loss = 8.02443
I0523 06:53:35.896921 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02443 (* 1 = 8.02443 loss)
I0523 06:53:36.610280 34682 sgd_solver.cpp:112] Iteration 67000, lr = 0.01
I0523 06:53:40.491755 34682 solver.cpp:239] Iteration 67010 (2.17643 iter/s, 4.59467s/10 iters), loss = 7.84238
I0523 06:53:40.491808 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84238 (* 1 = 7.84238 loss)
I0523 06:53:40.548727 34682 sgd_solver.cpp:112] Iteration 67010, lr = 0.01
I0523 06:53:46.637523 34682 solver.cpp:239] Iteration 67020 (1.62722 iter/s, 6.14547s/10 iters), loss = 7.78194
I0523 06:53:46.637578 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.78194 (* 1 = 7.78194 loss)
I0523 06:53:46.699620 34682 sgd_solver.cpp:112] Iteration 67020, lr = 0.01
I0523 06:53:52.490592 34682 solver.cpp:239] Iteration 67030 (1.70859 iter/s, 5.85278s/10 iters), loss = 8.41609
I0523 06:53:52.490655 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41609 (* 1 = 8.41609 loss)
I0523 06:53:52.555063 34682 sgd_solver.cpp:112] Iteration 67030, lr = 0.01
I0523 06:53:56.689965 34682 solver.cpp:239] Iteration 67040 (2.38144 iter/s, 4.19913s/10 iters), loss = 8.06988
I0523 06:53:56.690011 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06988 (* 1 = 8.06988 loss)
I0523 06:53:56.768841 34682 sgd_solver.cpp:112] Iteration 67040, lr = 0.01
I0523 06:54:01.770114 34682 solver.cpp:239] Iteration 67050 (1.96854 iter/s, 5.0799s/10 iters), loss = 8.45757
I0523 06:54:01.770167 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45757 (* 1 = 8.45757 loss)
I0523 06:54:02.573143 34682 sgd_solver.cpp:112] Iteration 67050, lr = 0.01
I0523 06:54:08.079460 34682 solver.cpp:239] Iteration 67060 (1.58503 iter/s, 6.30903s/10 iters), loss = 8.23127
I0523 06:54:08.079658 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23127 (* 1 = 8.23127 loss)
I0523 06:54:08.910326 34682 sgd_solver.cpp:112] Iteration 67060, lr = 0.01
I0523 06:54:12.505940 34682 solver.cpp:239] Iteration 67070 (2.25932 iter/s, 4.42611s/10 iters), loss = 7.77627
I0523 06:54:12.505981 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77627 (* 1 = 7.77627 loss)
I0523 06:54:12.569979 34682 sgd_solver.cpp:112] Iteration 67070, lr = 0.01
I0523 06:54:16.818835 34682 solver.cpp:239] Iteration 67080 (2.31875 iter/s, 4.31267s/10 iters), loss = 8.07964
I0523 06:54:16.818881 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07964 (* 1 = 8.07964 loss)
I0523 06:54:16.880408 34682 sgd_solver.cpp:112] Iteration 67080, lr = 0.01
I0523 06:54:21.044275 34682 solver.cpp:239] Iteration 67090 (2.36674 iter/s, 4.22522s/10 iters), loss = 8.67043
I0523 06:54:21.044337 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67043 (* 1 = 8.67043 loss)
I0523 06:54:21.825340 34682 sgd_solver.cpp:112] Iteration 67090, lr = 0.01
I0523 06:54:27.481426 34682 solver.cpp:239] Iteration 67100 (1.55356 iter/s, 6.43683s/10 iters), loss = 7.66845
I0523 06:54:27.481482 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66845 (* 1 = 7.66845 loss)
I0523 06:54:28.220490 34682 sgd_solver.cpp:112] Iteration 67100, lr = 0.01
I0523 06:54:31.310060 34682 solver.cpp:239] Iteration 67110 (2.61204 iter/s, 3.82842s/10 iters), loss = 7.6068
I0523 06:54:31.310104 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6068 (* 1 = 7.6068 loss)
I0523 06:54:31.378386 34682 sgd_solver.cpp:112] Iteration 67110, lr = 0.01
I0523 06:54:35.578044 34682 solver.cpp:239] Iteration 67120 (2.34315 iter/s, 4.26776s/10 iters), loss = 8.41694
I0523 06:54:35.578112 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41694 (* 1 = 8.41694 loss)
I0523 06:54:35.646963 34682 sgd_solver.cpp:112] Iteration 67120, lr = 0.01
I0523 06:54:41.266556 34682 solver.cpp:239] Iteration 67130 (1.75802 iter/s, 5.68821s/10 iters), loss = 8.854
I0523 06:54:41.266738 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.854 (* 1 = 8.854 loss)
I0523 06:54:41.588428 34682 sgd_solver.cpp:112] Iteration 67130, lr = 0.01
I0523 06:54:45.872422 34682 solver.cpp:239] Iteration 67140 (2.17132 iter/s, 4.6055s/10 iters), loss = 8.05114
I0523 06:54:45.872481 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05114 (* 1 = 8.05114 loss)
I0523 06:54:46.714148 34682 sgd_solver.cpp:112] Iteration 67140, lr = 0.01
I0523 06:54:50.705199 34682 solver.cpp:239] Iteration 67150 (2.06931 iter/s, 4.83253s/10 iters), loss = 8.5514
I0523 06:54:50.705252 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5514 (* 1 = 8.5514 loss)
I0523 06:54:51.524142 34682 sgd_solver.cpp:112] Iteration 67150, lr = 0.01
I0523 06:54:55.753998 34682 solver.cpp:239] Iteration 67160 (1.98077 iter/s, 5.04854s/10 iters), loss = 7.58277
I0523 06:54:55.754040 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58277 (* 1 = 7.58277 loss)
I0523 06:54:55.812292 34682 sgd_solver.cpp:112] Iteration 67160, lr = 0.01
I0523 06:54:58.297219 34682 solver.cpp:239] Iteration 67170 (3.93225 iter/s, 2.54307s/10 iters), loss = 8.59435
I0523 06:54:58.297268 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59435 (* 1 = 8.59435 loss)
I0523 06:54:58.938784 34682 sgd_solver.cpp:112] Iteration 67170, lr = 0.01
I0523 06:55:02.347506 34682 solver.cpp:239] Iteration 67180 (2.4691 iter/s, 4.05006s/10 iters), loss = 8.5263
I0523 06:55:02.347568 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5263 (* 1 = 8.5263 loss)
I0523 06:55:02.405630 34682 sgd_solver.cpp:112] Iteration 67180, lr = 0.01
I0523 06:55:05.600412 34682 solver.cpp:239] Iteration 67190 (3.07436 iter/s, 3.2527s/10 iters), loss = 8.49116
I0523 06:55:05.600462 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49116 (* 1 = 8.49116 loss)
I0523 06:55:05.676941 34682 sgd_solver.cpp:112] Iteration 67190, lr = 0.01
I0523 06:55:11.864531 34682 solver.cpp:239] Iteration 67200 (1.59647 iter/s, 6.26381s/10 iters), loss = 7.83908
I0523 06:55:11.864739 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83908 (* 1 = 7.83908 loss)
I0523 06:55:11.937651 34682 sgd_solver.cpp:112] Iteration 67200, lr = 0.01
I0523 06:55:17.293879 34682 solver.cpp:239] Iteration 67210 (1.84198 iter/s, 5.42895s/10 iters), loss = 8.51593
I0523 06:55:17.293932 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51593 (* 1 = 8.51593 loss)
I0523 06:55:18.133051 34682 sgd_solver.cpp:112] Iteration 67210, lr = 0.01
I0523 06:55:21.576645 34682 solver.cpp:239] Iteration 67220 (2.3363 iter/s, 4.28027s/10 iters), loss = 8.82129
I0523 06:55:21.576685 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.82129 (* 1 = 8.82129 loss)
I0523 06:55:21.647056 34682 sgd_solver.cpp:112] Iteration 67220, lr = 0.01
I0523 06:55:26.271124 34682 solver.cpp:239] Iteration 67230 (2.13027 iter/s, 4.69424s/10 iters), loss = 7.77735
I0523 06:55:26.271181 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77735 (* 1 = 7.77735 loss)
I0523 06:55:26.335536 34682 sgd_solver.cpp:112] Iteration 67230, lr = 0.01
I0523 06:55:29.025019 34682 solver.cpp:239] Iteration 67240 (3.63145 iter/s, 2.75372s/10 iters), loss = 7.64382
I0523 06:55:29.025070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64382 (* 1 = 7.64382 loss)
I0523 06:55:29.865929 34682 sgd_solver.cpp:112] Iteration 67240, lr = 0.01
I0523 06:55:33.190788 34682 solver.cpp:239] Iteration 67250 (2.40065 iter/s, 4.16555s/10 iters), loss = 7.37526
I0523 06:55:33.190830 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.37526 (* 1 = 7.37526 loss)
I0523 06:55:34.039676 34682 sgd_solver.cpp:112] Iteration 67250, lr = 0.01
I0523 06:55:38.224536 34682 solver.cpp:239] Iteration 67260 (1.98669 iter/s, 5.03349s/10 iters), loss = 7.62112
I0523 06:55:38.224596 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62112 (* 1 = 7.62112 loss)
I0523 06:55:38.283061 34682 sgd_solver.cpp:112] Iteration 67260, lr = 0.01
I0523 06:55:42.071995 34682 solver.cpp:239] Iteration 67270 (2.59926 iter/s, 3.84724s/10 iters), loss = 8.60107
I0523 06:55:42.072180 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60107 (* 1 = 8.60107 loss)
I0523 06:55:42.147176 34682 sgd_solver.cpp:112] Iteration 67270, lr = 0.01
I0523 06:55:46.679497 34682 solver.cpp:239] Iteration 67280 (2.17054 iter/s, 4.60714s/10 iters), loss = 8.60583
I0523 06:55:46.679558 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.60583 (* 1 = 8.60583 loss)
I0523 06:55:47.522828 34682 sgd_solver.cpp:112] Iteration 67280, lr = 0.01
I0523 06:55:50.694478 34682 solver.cpp:239] Iteration 67290 (2.49081 iter/s, 4.01476s/10 iters), loss = 8.52015
I0523 06:55:50.694521 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52015 (* 1 = 8.52015 loss)
I0523 06:55:51.549088 34682 sgd_solver.cpp:112] Iteration 67290, lr = 0.01
I0523 06:55:54.938958 34682 solver.cpp:239] Iteration 67300 (2.35613 iter/s, 4.24424s/10 iters), loss = 8.96958
I0523 06:55:54.939007 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.96958 (* 1 = 8.96958 loss)
I0523 06:55:55.004986 34682 sgd_solver.cpp:112] Iteration 67300, lr = 0.01
I0523 06:56:00.199235 34682 solver.cpp:239] Iteration 67310 (1.90114 iter/s, 5.26001s/10 iters), loss = 8.1315
I0523 06:56:00.199290 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1315 (* 1 = 8.1315 loss)
I0523 06:56:00.895464 34682 sgd_solver.cpp:112] Iteration 67310, lr = 0.01
I0523 06:56:04.185827 34682 solver.cpp:239] Iteration 67320 (2.50855 iter/s, 3.98637s/10 iters), loss = 7.72018
I0523 06:56:04.185869 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72018 (* 1 = 7.72018 loss)
I0523 06:56:04.255432 34682 sgd_solver.cpp:112] Iteration 67320, lr = 0.01
I0523 06:56:10.059886 34682 solver.cpp:239] Iteration 67330 (1.70248 iter/s, 5.87377s/10 iters), loss = 6.53758
I0523 06:56:10.059938 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.53758 (* 1 = 6.53758 loss)
I0523 06:56:10.892297 34682 sgd_solver.cpp:112] Iteration 67330, lr = 0.01
I0523 06:56:18.066877 34682 solver.cpp:239] Iteration 67340 (1.24897 iter/s, 8.00661s/10 iters), loss = 8.71154
I0523 06:56:18.067070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71154 (* 1 = 8.71154 loss)
I0523 06:56:18.245332 34682 sgd_solver.cpp:112] Iteration 67340, lr = 0.01
I0523 06:56:22.211086 34682 solver.cpp:239] Iteration 67350 (2.41321 iter/s, 4.14385s/10 iters), loss = 7.6734
I0523 06:56:22.211127 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6734 (* 1 = 7.6734 loss)
I0523 06:56:23.051393 34682 sgd_solver.cpp:112] Iteration 67350, lr = 0.01
I0523 06:56:28.162811 34682 solver.cpp:239] Iteration 67360 (1.68027 iter/s, 5.95144s/10 iters), loss = 7.44033
I0523 06:56:28.162858 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44033 (* 1 = 7.44033 loss)
I0523 06:56:28.321949 34682 sgd_solver.cpp:112] Iteration 67360, lr = 0.01
I0523 06:56:31.434911 34682 solver.cpp:239] Iteration 67370 (3.05632 iter/s, 3.27191s/10 iters), loss = 7.54873
I0523 06:56:31.434962 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54873 (* 1 = 7.54873 loss)
I0523 06:56:31.509083 34682 sgd_solver.cpp:112] Iteration 67370, lr = 0.01
I0523 06:56:36.298315 34682 solver.cpp:239] Iteration 67380 (2.05628 iter/s, 4.86315s/10 iters), loss = 7.458
I0523 06:56:36.298357 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.458 (* 1 = 7.458 loss)
I0523 06:56:36.375663 34682 sgd_solver.cpp:112] Iteration 67380, lr = 0.01
I0523 06:56:39.893399 34682 solver.cpp:239] Iteration 67390 (2.78174 iter/s, 3.59488s/10 iters), loss = 7.78602
I0523 06:56:39.893463 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.78602 (* 1 = 7.78602 loss)
I0523 06:56:40.210374 34682 sgd_solver.cpp:112] Iteration 67390, lr = 0.01
I0523 06:56:46.178253 34682 solver.cpp:239] Iteration 67400 (1.59121 iter/s, 6.28454s/10 iters), loss = 7.60873
I0523 06:56:46.178319 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.60873 (* 1 = 7.60873 loss)
I0523 06:56:46.242643 34682 sgd_solver.cpp:112] Iteration 67400, lr = 0.01
I0523 06:56:51.216994 34682 solver.cpp:239] Iteration 67410 (1.98473 iter/s, 5.03847s/10 iters), loss = 8.39245
I0523 06:56:51.217123 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39245 (* 1 = 8.39245 loss)
I0523 06:56:51.284584 34682 sgd_solver.cpp:112] Iteration 67410, lr = 0.01
I0523 06:56:55.203843 34682 solver.cpp:239] Iteration 67420 (2.50844 iter/s, 3.98654s/10 iters), loss = 6.52044
I0523 06:56:55.203893 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.52044 (* 1 = 6.52044 loss)
I0523 06:56:55.268364 34682 sgd_solver.cpp:112] Iteration 67420, lr = 0.01
I0523 06:56:59.940737 34682 solver.cpp:239] Iteration 67430 (2.1112 iter/s, 4.73665s/10 iters), loss = 7.71807
I0523 06:56:59.940783 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.71807 (* 1 = 7.71807 loss)
I0523 06:57:00.002663 34682 sgd_solver.cpp:112] Iteration 67430, lr = 0.01
I0523 06:57:04.694427 34682 solver.cpp:239] Iteration 67440 (2.10374 iter/s, 4.75345s/10 iters), loss = 7.64328
I0523 06:57:04.694478 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64328 (* 1 = 7.64328 loss)
I0523 06:57:04.764808 34682 sgd_solver.cpp:112] Iteration 67440, lr = 0.01
I0523 06:57:09.406561 34682 solver.cpp:239] Iteration 67450 (2.12229 iter/s, 4.71189s/10 iters), loss = 6.21861
I0523 06:57:09.406616 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.21861 (* 1 = 6.21861 loss)
I0523 06:57:10.076149 34682 sgd_solver.cpp:112] Iteration 67450, lr = 0.01
I0523 06:57:14.011757 34682 solver.cpp:239] Iteration 67460 (2.17157 iter/s, 4.60495s/10 iters), loss = 8.83908
I0523 06:57:14.011799 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83908 (* 1 = 8.83908 loss)
I0523 06:57:14.075522 34682 sgd_solver.cpp:112] Iteration 67460, lr = 0.01
I0523 06:57:20.021787 34682 solver.cpp:239] Iteration 67470 (1.66396 iter/s, 6.00974s/10 iters), loss = 8.14687
I0523 06:57:20.021834 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14687 (* 1 = 8.14687 loss)
I0523 06:57:20.865329 34682 sgd_solver.cpp:112] Iteration 67470, lr = 0.01
I0523 06:57:26.320438 34682 solver.cpp:239] Iteration 67480 (1.58772 iter/s, 6.29835s/10 iters), loss = 7.82199
I0523 06:57:26.320564 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82199 (* 1 = 7.82199 loss)
I0523 06:57:26.391867 34682 sgd_solver.cpp:112] Iteration 67480, lr = 0.01
I0523 06:57:30.587395 34682 solver.cpp:239] Iteration 67490 (2.34376 iter/s, 4.26665s/10 iters), loss = 8.36531
I0523 06:57:30.587451 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36531 (* 1 = 8.36531 loss)
I0523 06:57:30.840927 34682 sgd_solver.cpp:112] Iteration 67490, lr = 0.01
I0523 06:57:35.978471 34682 solver.cpp:239] Iteration 67500 (1.85502 iter/s, 5.39078s/10 iters), loss = 7.46649
I0523 06:57:35.978523 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.46649 (* 1 = 7.46649 loss)
I0523 06:57:36.775964 34682 sgd_solver.cpp:112] Iteration 67500, lr = 0.01
I0523 06:57:40.695031 34682 solver.cpp:239] Iteration 67510 (2.1203 iter/s, 4.71632s/10 iters), loss = 8.89121
I0523 06:57:40.695083 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89121 (* 1 = 8.89121 loss)
I0523 06:57:40.764132 34682 sgd_solver.cpp:112] Iteration 67510, lr = 0.01
I0523 06:57:46.166860 34682 solver.cpp:239] Iteration 67520 (1.82764 iter/s, 5.47153s/10 iters), loss = 8.10789
I0523 06:57:46.166934 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10789 (* 1 = 8.10789 loss)
I0523 06:57:46.228000 34682 sgd_solver.cpp:112] Iteration 67520, lr = 0.01
I0523 06:57:50.751440 34682 solver.cpp:239] Iteration 67530 (2.18135 iter/s, 4.58432s/10 iters), loss = 7.60544
I0523 06:57:50.751513 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.60544 (* 1 = 7.60544 loss)
I0523 06:57:50.826697 34682 sgd_solver.cpp:112] Iteration 67530, lr = 0.01
I0523 06:57:54.999398 34682 solver.cpp:239] Iteration 67540 (2.35421 iter/s, 4.24771s/10 iters), loss = 9.00366
I0523 06:57:54.999447 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.00366 (* 1 = 9.00366 loss)
I0523 06:57:55.074254 34682 sgd_solver.cpp:112] Iteration 67540, lr = 0.01
I0523 06:57:59.485196 34682 solver.cpp:239] Iteration 67550 (2.22938 iter/s, 4.48556s/10 iters), loss = 7.72931
I0523 06:57:59.485385 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72931 (* 1 = 7.72931 loss)
I0523 06:58:00.112681 34682 sgd_solver.cpp:112] Iteration 67550, lr = 0.01
I0523 06:58:05.000705 34682 solver.cpp:239] Iteration 67560 (1.8132 iter/s, 5.5151s/10 iters), loss = 7.3723
I0523 06:58:05.000758 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.3723 (* 1 = 7.3723 loss)
I0523 06:58:05.076656 34682 sgd_solver.cpp:112] Iteration 67560, lr = 0.01
I0523 06:58:10.003818 34682 solver.cpp:239] Iteration 67570 (1.99886 iter/s, 5.00285s/10 iters), loss = 7.72202
I0523 06:58:10.003875 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72202 (* 1 = 7.72202 loss)
I0523 06:58:10.077812 34682 sgd_solver.cpp:112] Iteration 67570, lr = 0.01
I0523 06:58:14.107044 34682 solver.cpp:239] Iteration 67580 (2.43725 iter/s, 4.10298s/10 iters), loss = 7.63024
I0523 06:58:14.107101 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63024 (* 1 = 7.63024 loss)
I0523 06:58:15.009840 34682 sgd_solver.cpp:112] Iteration 67580, lr = 0.01
I0523 06:58:19.144469 34682 solver.cpp:239] Iteration 67590 (1.98525 iter/s, 5.03716s/10 iters), loss = 8.10612
I0523 06:58:19.144523 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10612 (* 1 = 8.10612 loss)
I0523 06:58:19.217574 34682 sgd_solver.cpp:112] Iteration 67590, lr = 0.01
I0523 06:58:24.949396 34682 solver.cpp:239] Iteration 67600 (1.72276 iter/s, 5.80463s/10 iters), loss = 7.11265
I0523 06:58:24.949445 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.11265 (* 1 = 7.11265 loss)
I0523 06:58:25.013556 34682 sgd_solver.cpp:112] Iteration 67600, lr = 0.01
I0523 06:58:28.412652 34682 solver.cpp:239] Iteration 67610 (2.88765 iter/s, 3.46302s/10 iters), loss = 8.61782
I0523 06:58:28.412706 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61782 (* 1 = 8.61782 loss)
I0523 06:58:29.151662 34682 sgd_solver.cpp:112] Iteration 67610, lr = 0.01
I0523 06:58:34.268692 34682 solver.cpp:239] Iteration 67620 (1.70773 iter/s, 5.85572s/10 iters), loss = 8.25746
I0523 06:58:34.268900 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25746 (* 1 = 8.25746 loss)
I0523 06:58:34.338050 34682 sgd_solver.cpp:112] Iteration 67620, lr = 0.01
I0523 06:58:39.078410 34682 solver.cpp:239] Iteration 67630 (2.0793 iter/s, 4.80931s/10 iters), loss = 7.26256
I0523 06:58:39.078464 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.26256 (* 1 = 7.26256 loss)
I0523 06:58:39.382899 34682 sgd_solver.cpp:112] Iteration 67630, lr = 0.01
I0523 06:58:44.021898 34682 solver.cpp:239] Iteration 67640 (2.02297 iter/s, 4.94323s/10 iters), loss = 8.39291
I0523 06:58:44.021955 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39291 (* 1 = 8.39291 loss)
I0523 06:58:44.082877 34682 sgd_solver.cpp:112] Iteration 67640, lr = 0.01
I0523 06:58:48.851286 34682 solver.cpp:239] Iteration 67650 (2.07076 iter/s, 4.82913s/10 iters), loss = 8.04798
I0523 06:58:48.851333 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04798 (* 1 = 8.04798 loss)
I0523 06:58:48.911628 34682 sgd_solver.cpp:112] Iteration 67650, lr = 0.01
I0523 06:58:53.707018 34682 solver.cpp:239] Iteration 67660 (2.05953 iter/s, 4.85549s/10 iters), loss = 7.82348
I0523 06:58:53.707072 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82348 (* 1 = 7.82348 loss)
I0523 06:58:53.783064 34682 sgd_solver.cpp:112] Iteration 67660, lr = 0.01
I0523 06:58:58.321286 34682 solver.cpp:239] Iteration 67670 (2.1673 iter/s, 4.61403s/10 iters), loss = 7.76196
I0523 06:58:58.321348 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.76196 (* 1 = 7.76196 loss)
I0523 06:58:58.391472 34682 sgd_solver.cpp:112] Iteration 67670, lr = 0.01
I0523 06:59:02.356168 34682 solver.cpp:239] Iteration 67680 (2.47853 iter/s, 4.03465s/10 iters), loss = 8.58249
I0523 06:59:02.356226 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58249 (* 1 = 8.58249 loss)
I0523 06:59:02.427067 34682 sgd_solver.cpp:112] Iteration 67680, lr = 0.01
I0523 06:59:06.057050 34682 solver.cpp:239] Iteration 67690 (2.70221 iter/s, 3.70068s/10 iters), loss = 8.03106
I0523 06:59:06.057283 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03106 (* 1 = 8.03106 loss)
I0523 06:59:06.821873 34682 sgd_solver.cpp:112] Iteration 67690, lr = 0.01
I0523 06:59:12.447932 34682 solver.cpp:239] Iteration 67700 (1.56484 iter/s, 6.39042s/10 iters), loss = 7.71209
I0523 06:59:12.447978 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.71209 (* 1 = 7.71209 loss)
I0523 06:59:12.514618 34682 sgd_solver.cpp:112] Iteration 67700, lr = 0.01
I0523 06:59:17.199139 34682 solver.cpp:239] Iteration 67710 (2.10483 iter/s, 4.75097s/10 iters), loss = 8.14908
I0523 06:59:17.199193 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14908 (* 1 = 8.14908 loss)
I0523 06:59:17.268755 34682 sgd_solver.cpp:112] Iteration 67710, lr = 0.01
I0523 06:59:22.153338 34682 solver.cpp:239] Iteration 67720 (2.01859 iter/s, 4.95395s/10 iters), loss = 8.33856
I0523 06:59:22.153388 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33856 (* 1 = 8.33856 loss)
I0523 06:59:22.561754 34682 sgd_solver.cpp:112] Iteration 67720, lr = 0.01
I0523 06:59:28.133086 34682 solver.cpp:239] Iteration 67730 (1.6724 iter/s, 5.97944s/10 iters), loss = 7.92314
I0523 06:59:28.133132 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92314 (* 1 = 7.92314 loss)
I0523 06:59:28.185533 34682 sgd_solver.cpp:112] Iteration 67730, lr = 0.01
I0523 06:59:32.383965 34682 solver.cpp:239] Iteration 67740 (2.35258 iter/s, 4.25066s/10 iters), loss = 8.37889
I0523 06:59:32.384017 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37889 (* 1 = 8.37889 loss)
I0523 06:59:32.454147 34682 sgd_solver.cpp:112] Iteration 67740, lr = 0.01
I0523 06:59:36.236877 34682 solver.cpp:239] Iteration 67750 (2.59558 iter/s, 3.8527s/10 iters), loss = 7.29639
I0523 06:59:36.237174 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.29639 (* 1 = 7.29639 loss)
I0523 06:59:37.001710 34682 sgd_solver.cpp:112] Iteration 67750, lr = 0.01
I0523 06:59:40.306118 34682 solver.cpp:239] Iteration 67760 (2.45772 iter/s, 4.0688s/10 iters), loss = 8.31079
I0523 06:59:40.306170 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31079 (* 1 = 8.31079 loss)
I0523 06:59:40.374651 34682 sgd_solver.cpp:112] Iteration 67760, lr = 0.01
I0523 06:59:43.784678 34682 solver.cpp:239] Iteration 67770 (2.87491 iter/s, 3.47837s/10 iters), loss = 8.06003
I0523 06:59:43.784723 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06003 (* 1 = 8.06003 loss)
I0523 06:59:43.844216 34682 sgd_solver.cpp:112] Iteration 67770, lr = 0.01
I0523 06:59:47.760730 34682 solver.cpp:239] Iteration 67780 (2.51519 iter/s, 3.97584s/10 iters), loss = 7.73239
I0523 06:59:47.760774 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.73239 (* 1 = 7.73239 loss)
I0523 06:59:47.835517 34682 sgd_solver.cpp:112] Iteration 67780, lr = 0.01
I0523 06:59:53.051195 34682 solver.cpp:239] Iteration 67790 (1.89029 iter/s, 5.2902s/10 iters), loss = 7.99325
I0523 06:59:53.051259 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99325 (* 1 = 7.99325 loss)
I0523 06:59:53.127393 34682 sgd_solver.cpp:112] Iteration 67790, lr = 0.01
I0523 07:00:00.337867 34682 solver.cpp:239] Iteration 67800 (1.37244 iter/s, 7.28632s/10 iters), loss = 8.47728
I0523 07:00:00.337921 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47728 (* 1 = 8.47728 loss)
I0523 07:00:01.176517 34682 sgd_solver.cpp:112] Iteration 67800, lr = 0.01
I0523 07:00:05.753738 34682 solver.cpp:239] Iteration 67810 (1.84652 iter/s, 5.4156s/10 iters), loss = 7.37925
I0523 07:00:05.753778 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.37925 (* 1 = 7.37925 loss)
I0523 07:00:05.830830 34682 sgd_solver.cpp:112] Iteration 67810, lr = 0.01
I0523 07:00:09.903460 34682 solver.cpp:239] Iteration 67820 (2.40993 iter/s, 4.14951s/10 iters), loss = 8.37265
I0523 07:00:09.903560 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37265 (* 1 = 8.37265 loss)
I0523 07:00:09.970212 34682 sgd_solver.cpp:112] Iteration 67820, lr = 0.01
I0523 07:00:15.520143 34682 solver.cpp:239] Iteration 67830 (1.78051 iter/s, 5.61636s/10 iters), loss = 7.88491
I0523 07:00:15.520195 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88491 (* 1 = 7.88491 loss)
I0523 07:00:15.933985 34682 sgd_solver.cpp:112] Iteration 67830, lr = 0.01
I0523 07:00:21.381381 34682 solver.cpp:239] Iteration 67840 (1.70621 iter/s, 5.86095s/10 iters), loss = 7.83839
I0523 07:00:21.381422 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83839 (* 1 = 7.83839 loss)
I0523 07:00:21.451723 34682 sgd_solver.cpp:112] Iteration 67840, lr = 0.01
I0523 07:00:24.817770 34682 solver.cpp:239] Iteration 67850 (2.91019 iter/s, 3.4362s/10 iters), loss = 8.20342
I0523 07:00:24.817819 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20342 (* 1 = 8.20342 loss)
I0523 07:00:25.682873 34682 sgd_solver.cpp:112] Iteration 67850, lr = 0.01
I0523 07:00:30.611650 34682 solver.cpp:239] Iteration 67860 (1.72604 iter/s, 5.7936s/10 iters), loss = 8.62268
I0523 07:00:30.611694 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62268 (* 1 = 8.62268 loss)
I0523 07:00:30.675845 34682 sgd_solver.cpp:112] Iteration 67860, lr = 0.01
I0523 07:00:34.842733 34682 solver.cpp:239] Iteration 67870 (2.3636 iter/s, 4.23083s/10 iters), loss = 7.82749
I0523 07:00:34.842789 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82749 (* 1 = 7.82749 loss)
I0523 07:00:35.570370 34682 sgd_solver.cpp:112] Iteration 67870, lr = 0.01
I0523 07:00:40.275243 34682 solver.cpp:239] Iteration 67880 (1.84086 iter/s, 5.43223s/10 iters), loss = 7.65943
I0523 07:00:40.275439 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.65943 (* 1 = 7.65943 loss)
I0523 07:00:40.344985 34682 sgd_solver.cpp:112] Iteration 67880, lr = 0.01
I0523 07:00:45.315598 34682 solver.cpp:239] Iteration 67890 (1.98414 iter/s, 5.03998s/10 iters), loss = 7.41839
I0523 07:00:45.315641 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.41839 (* 1 = 7.41839 loss)
I0523 07:00:45.389329 34682 sgd_solver.cpp:112] Iteration 67890, lr = 0.01
I0523 07:00:50.295269 34682 solver.cpp:239] Iteration 67900 (2.00827 iter/s, 4.97941s/10 iters), loss = 7.51336
I0523 07:00:50.295325 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51336 (* 1 = 7.51336 loss)
I0523 07:00:50.361097 34682 sgd_solver.cpp:112] Iteration 67900, lr = 0.01
I0523 07:00:56.042762 34682 solver.cpp:239] Iteration 67910 (1.73998 iter/s, 5.74719s/10 iters), loss = 8.84221
I0523 07:00:56.042824 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84221 (* 1 = 8.84221 loss)
I0523 07:00:56.889838 34682 sgd_solver.cpp:112] Iteration 67910, lr = 0.01
I0523 07:01:00.756103 34682 solver.cpp:239] Iteration 67920 (2.12175 iter/s, 4.71309s/10 iters), loss = 7.36738
I0523 07:01:00.756163 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.36738 (* 1 = 7.36738 loss)
I0523 07:01:00.833503 34682 sgd_solver.cpp:112] Iteration 67920, lr = 0.01
I0523 07:01:07.124701 34682 solver.cpp:239] Iteration 67930 (1.57028 iter/s, 6.36828s/10 iters), loss = 7.4557
I0523 07:01:07.124758 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4557 (* 1 = 7.4557 loss)
I0523 07:01:07.919473 34682 sgd_solver.cpp:112] Iteration 67930, lr = 0.01
I0523 07:01:12.294384 34682 solver.cpp:239] Iteration 67940 (1.93445 iter/s, 5.16942s/10 iters), loss = 8.44065
I0523 07:01:12.294603 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44065 (* 1 = 8.44065 loss)
I0523 07:01:12.367251 34682 sgd_solver.cpp:112] Iteration 67940, lr = 0.01
I0523 07:01:16.398299 34682 solver.cpp:239] Iteration 67950 (2.43691 iter/s, 4.10355s/10 iters), loss = 8.20421
I0523 07:01:16.398347 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20421 (* 1 = 8.20421 loss)
I0523 07:01:16.468405 34682 sgd_solver.cpp:112] Iteration 67950, lr = 0.01
I0523 07:01:23.467473 34682 solver.cpp:239] Iteration 67960 (1.41466 iter/s, 7.06885s/10 iters), loss = 8.54795
I0523 07:01:23.467515 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54795 (* 1 = 8.54795 loss)
I0523 07:01:23.549155 34682 sgd_solver.cpp:112] Iteration 67960, lr = 0.01
I0523 07:01:27.747745 34682 solver.cpp:239] Iteration 67970 (2.33643 iter/s, 4.28004s/10 iters), loss = 7.52529
I0523 07:01:27.747802 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.52529 (* 1 = 7.52529 loss)
I0523 07:01:28.591435 34682 sgd_solver.cpp:112] Iteration 67970, lr = 0.01
I0523 07:01:32.799592 34682 solver.cpp:239] Iteration 67980 (1.97958 iter/s, 5.05158s/10 iters), loss = 7.97966
I0523 07:01:32.799664 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97966 (* 1 = 7.97966 loss)
I0523 07:01:33.594460 34682 sgd_solver.cpp:112] Iteration 67980, lr = 0.01
I0523 07:01:37.896800 34682 solver.cpp:239] Iteration 67990 (1.96197 iter/s, 5.09693s/10 iters), loss = 7.47295
I0523 07:01:37.896849 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.47295 (* 1 = 7.47295 loss)
I0523 07:01:37.978140 34682 sgd_solver.cpp:112] Iteration 67990, lr = 0.01
I0523 07:01:41.244597 34682 solver.cpp:239] Iteration 68000 (2.98721 iter/s, 3.3476s/10 iters), loss = 8.70797
I0523 07:01:41.244645 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70797 (* 1 = 8.70797 loss)
I0523 07:01:42.058017 34682 sgd_solver.cpp:112] Iteration 68000, lr = 0.01
I0523 07:01:47.695123 34682 solver.cpp:239] Iteration 68010 (1.55033 iter/s, 6.45022s/10 iters), loss = 7.7809
I0523 07:01:47.695266 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7809 (* 1 = 7.7809 loss)
I0523 07:01:47.771072 34682 sgd_solver.cpp:112] Iteration 68010, lr = 0.01
I0523 07:01:51.819435 34682 solver.cpp:239] Iteration 68020 (2.42482 iter/s, 4.12401s/10 iters), loss = 8.16505
I0523 07:01:51.819484 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16505 (* 1 = 8.16505 loss)
I0523 07:01:51.896379 34682 sgd_solver.cpp:112] Iteration 68020, lr = 0.01
I0523 07:01:57.412034 34682 solver.cpp:239] Iteration 68030 (1.78817 iter/s, 5.5923s/10 iters), loss = 7.30167
I0523 07:01:57.412101 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.30167 (* 1 = 7.30167 loss)
I0523 07:01:57.985457 34682 sgd_solver.cpp:112] Iteration 68030, lr = 0.01
I0523 07:02:03.153740 34682 solver.cpp:239] Iteration 68040 (1.74174 iter/s, 5.74138s/10 iters), loss = 8.19176
I0523 07:02:03.153816 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19176 (* 1 = 8.19176 loss)
I0523 07:02:03.995646 34682 sgd_solver.cpp:112] Iteration 68040, lr = 0.01
I0523 07:02:07.244165 34682 solver.cpp:239] Iteration 68050 (2.44488 iter/s, 4.09018s/10 iters), loss = 7.78637
I0523 07:02:07.244222 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.78637 (* 1 = 7.78637 loss)
I0523 07:02:07.305934 34682 sgd_solver.cpp:112] Iteration 68050, lr = 0.01
I0523 07:02:12.169046 34682 solver.cpp:239] Iteration 68060 (2.03061 iter/s, 4.92462s/10 iters), loss = 7.8235
I0523 07:02:12.169098 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8235 (* 1 = 7.8235 loss)
I0523 07:02:12.816056 34682 sgd_solver.cpp:112] Iteration 68060, lr = 0.01
I0523 07:02:20.065426 34682 solver.cpp:239] Iteration 68070 (1.26646 iter/s, 7.89602s/10 iters), loss = 7.63953
I0523 07:02:20.065559 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63953 (* 1 = 7.63953 loss)
I0523 07:02:20.824131 34682 sgd_solver.cpp:112] Iteration 68070, lr = 0.01
I0523 07:02:24.943800 34682 solver.cpp:239] Iteration 68080 (2.05001 iter/s, 4.87804s/10 iters), loss = 8.07806
I0523 07:02:24.943841 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07806 (* 1 = 8.07806 loss)
I0523 07:02:25.018923 34682 sgd_solver.cpp:112] Iteration 68080, lr = 0.01
I0523 07:02:28.374390 34682 solver.cpp:239] Iteration 68090 (2.91511 iter/s, 3.4304s/10 iters), loss = 7.69438
I0523 07:02:28.374444 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69438 (* 1 = 7.69438 loss)
I0523 07:02:29.163689 34682 sgd_solver.cpp:112] Iteration 68090, lr = 0.01
I0523 07:02:35.402679 34682 solver.cpp:239] Iteration 68100 (1.42289 iter/s, 7.02796s/10 iters), loss = 7.6548
I0523 07:02:35.402731 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6548 (* 1 = 7.6548 loss)
I0523 07:02:35.473246 34682 sgd_solver.cpp:112] Iteration 68100, lr = 0.01
I0523 07:02:41.091303 34682 solver.cpp:239] Iteration 68110 (1.75798 iter/s, 5.68834s/10 iters), loss = 8.33125
I0523 07:02:41.091361 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33125 (* 1 = 8.33125 loss)
I0523 07:02:41.149305 34682 sgd_solver.cpp:112] Iteration 68110, lr = 0.01
I0523 07:02:46.840943 34682 solver.cpp:239] Iteration 68120 (1.73933 iter/s, 5.74935s/10 iters), loss = 8.42465
I0523 07:02:46.840987 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42465 (* 1 = 8.42465 loss)
I0523 07:02:46.919078 34682 sgd_solver.cpp:112] Iteration 68120, lr = 0.01
I0523 07:02:50.726372 34682 solver.cpp:239] Iteration 68130 (2.57386 iter/s, 3.88522s/10 iters), loss = 8.39685
I0523 07:02:50.726564 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39685 (* 1 = 8.39685 loss)
I0523 07:02:51.584192 34682 sgd_solver.cpp:112] Iteration 68130, lr = 0.01
I0523 07:02:57.183404 34682 solver.cpp:239] Iteration 68140 (1.54984 iter/s, 6.45229s/10 iters), loss = 8.71987
I0523 07:02:57.183450 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.71987 (* 1 = 8.71987 loss)
I0523 07:02:57.245517 34682 sgd_solver.cpp:112] Iteration 68140, lr = 0.01
I0523 07:03:02.866991 34682 solver.cpp:239] Iteration 68150 (1.75954 iter/s, 5.68331s/10 iters), loss = 8.0086
I0523 07:03:02.867038 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0086 (* 1 = 8.0086 loss)
I0523 07:03:03.605676 34682 sgd_solver.cpp:112] Iteration 68150, lr = 0.01
I0523 07:03:08.507845 34682 solver.cpp:239] Iteration 68160 (1.77287 iter/s, 5.64056s/10 iters), loss = 7.4816
I0523 07:03:08.507917 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4816 (* 1 = 7.4816 loss)
I0523 07:03:09.317937 34682 sgd_solver.cpp:112] Iteration 68160, lr = 0.01
I0523 07:03:15.156038 34682 solver.cpp:239] Iteration 68170 (1.50424 iter/s, 6.64786s/10 iters), loss = 7.32559
I0523 07:03:15.156081 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.32559 (* 1 = 7.32559 loss)
I0523 07:03:15.229871 34682 sgd_solver.cpp:112] Iteration 68170, lr = 0.01
I0523 07:03:18.960561 34682 solver.cpp:239] Iteration 68180 (2.62859 iter/s, 3.80432s/10 iters), loss = 8.23874
I0523 07:03:18.960605 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23874 (* 1 = 8.23874 loss)
I0523 07:03:19.715795 34682 sgd_solver.cpp:112] Iteration 68180, lr = 0.01
I0523 07:03:23.346561 34682 solver.cpp:239] Iteration 68190 (2.2801 iter/s, 4.38577s/10 iters), loss = 8.25725
I0523 07:03:23.346762 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25725 (* 1 = 8.25725 loss)
I0523 07:03:23.425607 34682 sgd_solver.cpp:112] Iteration 68190, lr = 0.01
I0523 07:03:28.290319 34682 solver.cpp:239] Iteration 68200 (2.02291 iter/s, 4.94336s/10 iters), loss = 7.21326
I0523 07:03:28.290370 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.21326 (* 1 = 7.21326 loss)
I0523 07:03:28.346915 34682 sgd_solver.cpp:112] Iteration 68200, lr = 0.01
I0523 07:03:32.328145 34682 solver.cpp:239] Iteration 68210 (2.47672 iter/s, 4.0376s/10 iters), loss = 7.74545
I0523 07:03:32.328191 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74545 (* 1 = 7.74545 loss)
I0523 07:03:32.392208 34682 sgd_solver.cpp:112] Iteration 68210, lr = 0.01
I0523 07:03:37.096554 34682 solver.cpp:239] Iteration 68220 (2.09724 iter/s, 4.76816s/10 iters), loss = 7.83422
I0523 07:03:37.096616 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83422 (* 1 = 7.83422 loss)
I0523 07:03:37.159291 34682 sgd_solver.cpp:112] Iteration 68220, lr = 0.01
I0523 07:03:39.417827 34682 solver.cpp:239] Iteration 68230 (4.31501 iter/s, 2.31749s/10 iters), loss = 7.773
I0523 07:03:39.417866 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.773 (* 1 = 7.773 loss)
I0523 07:03:39.488236 34682 sgd_solver.cpp:112] Iteration 68230, lr = 0.01
I0523 07:03:46.330852 34682 solver.cpp:239] Iteration 68240 (1.44661 iter/s, 6.9127s/10 iters), loss = 8.79193
I0523 07:03:46.330909 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79193 (* 1 = 8.79193 loss)
I0523 07:03:46.388950 34682 sgd_solver.cpp:112] Iteration 68240, lr = 0.01
I0523 07:03:51.798372 34682 solver.cpp:239] Iteration 68250 (1.82908 iter/s, 5.46724s/10 iters), loss = 8.25173
I0523 07:03:51.798425 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25173 (* 1 = 8.25173 loss)
I0523 07:03:52.633906 34682 sgd_solver.cpp:112] Iteration 68250, lr = 0.01
I0523 07:03:56.754973 34682 solver.cpp:239] Iteration 68260 (2.01762 iter/s, 4.95632s/10 iters), loss = 7.4413
I0523 07:03:56.755115 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4413 (* 1 = 7.4413 loss)
I0523 07:03:57.519811 34682 sgd_solver.cpp:112] Iteration 68260, lr = 0.01
I0523 07:03:59.323485 34682 solver.cpp:239] Iteration 68270 (3.89368 iter/s, 2.56826s/10 iters), loss = 8.14586
I0523 07:03:59.323529 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14586 (* 1 = 8.14586 loss)
I0523 07:03:59.386802 34682 sgd_solver.cpp:112] Iteration 68270, lr = 0.01
I0523 07:04:03.538352 34682 solver.cpp:239] Iteration 68280 (2.37268 iter/s, 4.21465s/10 iters), loss = 7.96304
I0523 07:04:03.538398 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96304 (* 1 = 7.96304 loss)
I0523 07:04:04.326146 34682 sgd_solver.cpp:112] Iteration 68280, lr = 0.01
I0523 07:04:10.104471 34682 solver.cpp:239] Iteration 68290 (1.52304 iter/s, 6.56581s/10 iters), loss = 7.70372
I0523 07:04:10.104521 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.70372 (* 1 = 7.70372 loss)
I0523 07:04:10.171995 34682 sgd_solver.cpp:112] Iteration 68290, lr = 0.01
I0523 07:04:16.284381 34682 solver.cpp:239] Iteration 68300 (1.61822 iter/s, 6.17961s/10 iters), loss = 7.81247
I0523 07:04:16.284428 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81247 (* 1 = 7.81247 loss)
I0523 07:04:16.364869 34682 sgd_solver.cpp:112] Iteration 68300, lr = 0.01
I0523 07:04:18.984500 34682 solver.cpp:239] Iteration 68310 (3.70377 iter/s, 2.69995s/10 iters), loss = 8.35979
I0523 07:04:18.984557 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35979 (* 1 = 8.35979 loss)
I0523 07:04:19.048789 34682 sgd_solver.cpp:112] Iteration 68310, lr = 0.01
I0523 07:04:22.995636 34682 solver.cpp:239] Iteration 68320 (2.4932 iter/s, 4.01092s/10 iters), loss = 7.584
I0523 07:04:22.995679 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.584 (* 1 = 7.584 loss)
I0523 07:04:23.779701 34682 sgd_solver.cpp:112] Iteration 68320, lr = 0.01
I0523 07:04:28.457669 34682 solver.cpp:239] Iteration 68330 (1.83091 iter/s, 5.46177s/10 iters), loss = 7.20054
I0523 07:04:28.457792 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.20054 (* 1 = 7.20054 loss)
I0523 07:04:28.524361 34682 sgd_solver.cpp:112] Iteration 68330, lr = 0.01
I0523 07:04:32.715312 34682 solver.cpp:239] Iteration 68340 (2.34888 iter/s, 4.25734s/10 iters), loss = 8.80884
I0523 07:04:32.715363 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80884 (* 1 = 8.80884 loss)
I0523 07:04:33.475708 34682 sgd_solver.cpp:112] Iteration 68340, lr = 0.01
I0523 07:04:41.610407 34682 solver.cpp:239] Iteration 68350 (1.12427 iter/s, 8.89469s/10 iters), loss = 7.9633
I0523 07:04:41.610458 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9633 (* 1 = 7.9633 loss)
I0523 07:04:41.678884 34682 sgd_solver.cpp:112] Iteration 68350, lr = 0.01
I0523 07:04:46.026002 34682 solver.cpp:239] Iteration 68360 (2.26482 iter/s, 4.41535s/10 iters), loss = 7.72629
I0523 07:04:46.026048 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72629 (* 1 = 7.72629 loss)
I0523 07:04:46.093834 34682 sgd_solver.cpp:112] Iteration 68360, lr = 0.01
I0523 07:04:49.526983 34682 solver.cpp:239] Iteration 68370 (2.8565 iter/s, 3.50079s/10 iters), loss = 7.08782
I0523 07:04:49.527025 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.08782 (* 1 = 7.08782 loss)
I0523 07:04:49.617480 34682 sgd_solver.cpp:112] Iteration 68370, lr = 0.01
I0523 07:04:53.817000 34682 solver.cpp:239] Iteration 68380 (2.33111 iter/s, 4.2898s/10 iters), loss = 8.06411
I0523 07:04:53.817049 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06411 (* 1 = 8.06411 loss)
I0523 07:04:54.029132 34682 sgd_solver.cpp:112] Iteration 68380, lr = 0.01
I0523 07:04:57.386502 34682 solver.cpp:239] Iteration 68390 (2.80167 iter/s, 3.5693s/10 iters), loss = 7.45683
I0523 07:04:57.386543 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.45683 (* 1 = 7.45683 loss)
I0523 07:04:58.070758 34682 sgd_solver.cpp:112] Iteration 68390, lr = 0.01
I0523 07:05:01.955688 34682 solver.cpp:239] Iteration 68400 (2.18868 iter/s, 4.56896s/10 iters), loss = 8.08793
I0523 07:05:01.955941 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08793 (* 1 = 8.08793 loss)
I0523 07:05:02.024616 34682 sgd_solver.cpp:112] Iteration 68400, lr = 0.01
I0523 07:05:06.313853 34682 solver.cpp:239] Iteration 68410 (2.29476 iter/s, 4.35776s/10 iters), loss = 8.35403
I0523 07:05:06.313905 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35403 (* 1 = 8.35403 loss)
I0523 07:05:06.380561 34682 sgd_solver.cpp:112] Iteration 68410, lr = 0.01
I0523 07:05:11.458081 34682 solver.cpp:239] Iteration 68420 (1.94403 iter/s, 5.14395s/10 iters), loss = 8.76577
I0523 07:05:11.458142 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76577 (* 1 = 8.76577 loss)
I0523 07:05:12.358922 34682 sgd_solver.cpp:112] Iteration 68420, lr = 0.01
I0523 07:05:15.952363 34682 solver.cpp:239] Iteration 68430 (2.22517 iter/s, 4.49404s/10 iters), loss = 7.7862
I0523 07:05:15.952404 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7862 (* 1 = 7.7862 loss)
I0523 07:05:16.026196 34682 sgd_solver.cpp:112] Iteration 68430, lr = 0.01
I0523 07:05:21.793231 34682 solver.cpp:239] Iteration 68440 (1.71216 iter/s, 5.84058s/10 iters), loss = 8.15875
I0523 07:05:21.793309 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15875 (* 1 = 8.15875 loss)
I0523 07:05:22.552680 34682 sgd_solver.cpp:112] Iteration 68440, lr = 0.01
I0523 07:05:26.136822 34682 solver.cpp:239] Iteration 68450 (2.30238 iter/s, 4.34334s/10 iters), loss = 6.40605
I0523 07:05:26.136868 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.40605 (* 1 = 6.40605 loss)
I0523 07:05:26.965828 34682 sgd_solver.cpp:112] Iteration 68450, lr = 0.01
I0523 07:05:31.834352 34682 solver.cpp:239] Iteration 68460 (1.75523 iter/s, 5.69724s/10 iters), loss = 8.89067
I0523 07:05:31.834404 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89067 (* 1 = 8.89067 loss)
I0523 07:05:31.909827 34682 sgd_solver.cpp:112] Iteration 68460, lr = 0.01
I0523 07:05:38.250252 34682 solver.cpp:239] Iteration 68470 (1.5587 iter/s, 6.41559s/10 iters), loss = 7.45621
I0523 07:05:38.250490 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.45621 (* 1 = 7.45621 loss)
I0523 07:05:38.315999 34682 sgd_solver.cpp:112] Iteration 68470, lr = 0.01
I0523 07:05:41.478997 34682 solver.cpp:239] Iteration 68480 (3.09753 iter/s, 3.22838s/10 iters), loss = 7.62389
I0523 07:05:41.479038 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62389 (* 1 = 7.62389 loss)
I0523 07:05:41.557227 34682 sgd_solver.cpp:112] Iteration 68480, lr = 0.01
I0523 07:05:45.967557 34682 solver.cpp:239] Iteration 68490 (2.228 iter/s, 4.48834s/10 iters), loss = 8.66133
I0523 07:05:45.967594 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66133 (* 1 = 8.66133 loss)
I0523 07:05:46.040706 34682 sgd_solver.cpp:112] Iteration 68490, lr = 0.01
I0523 07:05:52.305174 34682 solver.cpp:239] Iteration 68500 (1.57795 iter/s, 6.33732s/10 iters), loss = 7.47194
I0523 07:05:52.305225 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.47194 (* 1 = 7.47194 loss)
I0523 07:05:52.370633 34682 sgd_solver.cpp:112] Iteration 68500, lr = 0.01
I0523 07:05:57.619726 34682 solver.cpp:239] Iteration 68510 (1.88172 iter/s, 5.31428s/10 iters), loss = 8.39374
I0523 07:05:57.619779 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39374 (* 1 = 8.39374 loss)
I0523 07:05:58.474287 34682 sgd_solver.cpp:112] Iteration 68510, lr = 0.01
I0523 07:06:03.701211 34682 solver.cpp:239] Iteration 68520 (1.64442 iter/s, 6.08119s/10 iters), loss = 8.02277
I0523 07:06:03.701257 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02277 (* 1 = 8.02277 loss)
I0523 07:06:04.007468 34682 sgd_solver.cpp:112] Iteration 68520, lr = 0.01
I0523 07:06:09.112469 34682 solver.cpp:239] Iteration 68530 (1.84809 iter/s, 5.41099s/10 iters), loss = 8.79702
I0523 07:06:09.112596 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79702 (* 1 = 8.79702 loss)
I0523 07:06:09.182209 34682 sgd_solver.cpp:112] Iteration 68530, lr = 0.01
I0523 07:06:14.548247 34682 solver.cpp:239] Iteration 68540 (1.83978 iter/s, 5.43544s/10 iters), loss = 7.50195
I0523 07:06:14.548292 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50195 (* 1 = 7.50195 loss)
I0523 07:06:15.144875 34682 sgd_solver.cpp:112] Iteration 68540, lr = 0.01
I0523 07:06:20.486510 34682 solver.cpp:239] Iteration 68550 (1.68408 iter/s, 5.93797s/10 iters), loss = 9.1917
I0523 07:06:20.486573 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1917 (* 1 = 9.1917 loss)
I0523 07:06:21.170315 34682 sgd_solver.cpp:112] Iteration 68550, lr = 0.01
I0523 07:06:24.273699 34682 solver.cpp:239] Iteration 68560 (2.64064 iter/s, 3.78696s/10 iters), loss = 8.20398
I0523 07:06:24.273763 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20398 (* 1 = 8.20398 loss)
I0523 07:06:24.990751 34682 sgd_solver.cpp:112] Iteration 68560, lr = 0.01
I0523 07:06:29.106155 34682 solver.cpp:239] Iteration 68570 (2.06946 iter/s, 4.83219s/10 iters), loss = 8.48794
I0523 07:06:29.106230 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48794 (* 1 = 8.48794 loss)
I0523 07:06:29.348942 34682 sgd_solver.cpp:112] Iteration 68570, lr = 0.01
I0523 07:06:33.149039 34682 solver.cpp:239] Iteration 68580 (2.47363 iter/s, 4.04264s/10 iters), loss = 8.39269
I0523 07:06:33.149083 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39269 (* 1 = 8.39269 loss)
I0523 07:06:33.975589 34682 sgd_solver.cpp:112] Iteration 68580, lr = 0.01
I0523 07:06:38.776576 34682 solver.cpp:239] Iteration 68590 (1.77706 iter/s, 5.62726s/10 iters), loss = 8.63677
I0523 07:06:38.776621 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.63677 (* 1 = 8.63677 loss)
I0523 07:06:38.851011 34682 sgd_solver.cpp:112] Iteration 68590, lr = 0.01
I0523 07:06:45.047691 34682 solver.cpp:239] Iteration 68600 (1.59469 iter/s, 6.27081s/10 iters), loss = 9.36534
I0523 07:06:45.047960 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.36534 (* 1 = 9.36534 loss)
I0523 07:06:45.621758 34682 sgd_solver.cpp:112] Iteration 68600, lr = 0.01
I0523 07:06:48.923808 34682 solver.cpp:239] Iteration 68610 (2.58018 iter/s, 3.8757s/10 iters), loss = 7.95919
I0523 07:06:48.923856 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95919 (* 1 = 7.95919 loss)
I0523 07:06:49.724632 34682 sgd_solver.cpp:112] Iteration 68610, lr = 0.01
I0523 07:06:54.924036 34682 solver.cpp:239] Iteration 68620 (1.66669 iter/s, 5.99993s/10 iters), loss = 8.05186
I0523 07:06:54.924103 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05186 (* 1 = 8.05186 loss)
I0523 07:06:54.981011 34682 sgd_solver.cpp:112] Iteration 68620, lr = 0.01
I0523 07:06:59.657660 34682 solver.cpp:239] Iteration 68630 (2.11266 iter/s, 4.73337s/10 iters), loss = 8.09935
I0523 07:06:59.657707 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09935 (* 1 = 8.09935 loss)
I0523 07:06:59.716615 34682 sgd_solver.cpp:112] Iteration 68630, lr = 0.01
I0523 07:07:04.212304 34682 solver.cpp:239] Iteration 68640 (2.19568 iter/s, 4.5544s/10 iters), loss = 8.54765
I0523 07:07:04.212369 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54765 (* 1 = 8.54765 loss)
I0523 07:07:04.905906 34682 sgd_solver.cpp:112] Iteration 68640, lr = 0.01
I0523 07:07:09.725214 34682 solver.cpp:239] Iteration 68650 (1.81402 iter/s, 5.51261s/10 iters), loss = 7.65657
I0523 07:07:09.725296 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.65657 (* 1 = 7.65657 loss)
I0523 07:07:09.788754 34682 sgd_solver.cpp:112] Iteration 68650, lr = 0.01
I0523 07:07:15.743656 34682 solver.cpp:239] Iteration 68660 (1.66164 iter/s, 6.01814s/10 iters), loss = 8.51371
I0523 07:07:15.743873 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51371 (* 1 = 8.51371 loss)
I0523 07:07:16.403611 34682 sgd_solver.cpp:112] Iteration 68660, lr = 0.01
I0523 07:07:21.826230 34682 solver.cpp:239] Iteration 68670 (1.64416 iter/s, 6.08213s/10 iters), loss = 7.60995
I0523 07:07:21.826272 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.60995 (* 1 = 7.60995 loss)
I0523 07:07:21.895799 34682 sgd_solver.cpp:112] Iteration 68670, lr = 0.01
I0523 07:07:25.221942 34682 solver.cpp:239] Iteration 68680 (2.94505 iter/s, 3.39553s/10 iters), loss = 7.47516
I0523 07:07:25.221983 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.47516 (* 1 = 7.47516 loss)
I0523 07:07:25.298856 34682 sgd_solver.cpp:112] Iteration 68680, lr = 0.01
I0523 07:07:31.768643 34682 solver.cpp:239] Iteration 68690 (1.52756 iter/s, 6.54639s/10 iters), loss = 8.295
I0523 07:07:31.768695 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.295 (* 1 = 8.295 loss)
I0523 07:07:31.865047 34682 sgd_solver.cpp:112] Iteration 68690, lr = 0.01
I0523 07:07:33.160596 34682 solver.cpp:239] Iteration 68700 (7.1848 iter/s, 1.39183s/10 iters), loss = 8.11092
I0523 07:07:33.160673 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11092 (* 1 = 8.11092 loss)
I0523 07:07:33.202095 34682 sgd_solver.cpp:112] Iteration 68700, lr = 0.01
I0523 07:07:36.388962 34682 solver.cpp:239] Iteration 68710 (3.09774 iter/s, 3.22815s/10 iters), loss = 8.35179
I0523 07:07:36.389010 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35179 (* 1 = 8.35179 loss)
I0523 07:07:36.450382 34682 sgd_solver.cpp:112] Iteration 68710, lr = 0.01
I0523 07:07:40.765864 34682 solver.cpp:239] Iteration 68720 (2.28486 iter/s, 4.37664s/10 iters), loss = 8.42147
I0523 07:07:40.765954 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42147 (* 1 = 8.42147 loss)
I0523 07:07:40.827405 34682 sgd_solver.cpp:112] Iteration 68720, lr = 0.01
I0523 07:07:44.911396 34682 solver.cpp:239] Iteration 68730 (2.4137 iter/s, 4.14302s/10 iters), loss = 8.66277
I0523 07:07:44.911448 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66277 (* 1 = 8.66277 loss)
I0523 07:07:45.754703 34682 sgd_solver.cpp:112] Iteration 68730, lr = 0.01
I0523 07:07:51.156816 34682 solver.cpp:239] Iteration 68740 (1.60125 iter/s, 6.24511s/10 iters), loss = 8.17458
I0523 07:07:51.156867 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17458 (* 1 = 8.17458 loss)
I0523 07:07:51.223441 34682 sgd_solver.cpp:112] Iteration 68740, lr = 0.01
I0523 07:07:55.450052 34682 solver.cpp:239] Iteration 68750 (2.32937 iter/s, 4.29301s/10 iters), loss = 8.44586
I0523 07:07:55.450093 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44586 (* 1 = 8.44586 loss)
I0523 07:07:56.274003 34682 sgd_solver.cpp:112] Iteration 68750, lr = 0.01
I0523 07:08:01.810786 34682 solver.cpp:239] Iteration 68760 (1.57222 iter/s, 6.36043s/10 iters), loss = 6.99356
I0523 07:08:01.810833 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.99356 (* 1 = 6.99356 loss)
I0523 07:08:02.595921 34682 sgd_solver.cpp:112] Iteration 68760, lr = 0.01
I0523 07:08:05.837903 34682 solver.cpp:239] Iteration 68770 (2.4833 iter/s, 4.02689s/10 iters), loss = 7.6629
I0523 07:08:05.837958 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6629 (* 1 = 7.6629 loss)
I0523 07:08:06.196178 34682 sgd_solver.cpp:112] Iteration 68770, lr = 0.01
I0523 07:08:11.597896 34682 solver.cpp:239] Iteration 68780 (1.7362 iter/s, 5.75969s/10 iters), loss = 7.74514
I0523 07:08:11.597957 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74514 (* 1 = 7.74514 loss)
I0523 07:08:12.291101 34682 sgd_solver.cpp:112] Iteration 68780, lr = 0.01
I0523 07:08:18.005164 34682 solver.cpp:239] Iteration 68790 (1.56081 iter/s, 6.40695s/10 iters), loss = 7.6369
I0523 07:08:18.005403 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6369 (* 1 = 7.6369 loss)
I0523 07:08:18.279534 34682 sgd_solver.cpp:112] Iteration 68790, lr = 0.01
I0523 07:08:24.007519 34682 solver.cpp:239] Iteration 68800 (1.66614 iter/s, 6.00189s/10 iters), loss = 8.2232
I0523 07:08:24.007573 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2232 (* 1 = 8.2232 loss)
I0523 07:08:24.773841 34682 sgd_solver.cpp:112] Iteration 68800, lr = 0.01
I0523 07:08:28.013676 34682 solver.cpp:239] Iteration 68810 (2.4963 iter/s, 4.00593s/10 iters), loss = 7.66475
I0523 07:08:28.013725 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66475 (* 1 = 7.66475 loss)
I0523 07:08:28.097415 34682 sgd_solver.cpp:112] Iteration 68810, lr = 0.01
I0523 07:08:32.223029 34682 solver.cpp:239] Iteration 68820 (2.37579 iter/s, 4.20912s/10 iters), loss = 8.14955
I0523 07:08:32.223083 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14955 (* 1 = 8.14955 loss)
I0523 07:08:32.285034 34682 sgd_solver.cpp:112] Iteration 68820, lr = 0.01
I0523 07:08:35.652027 34682 solver.cpp:239] Iteration 68830 (2.91648 iter/s, 3.42879s/10 iters), loss = 7.7858
I0523 07:08:35.652084 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7858 (* 1 = 7.7858 loss)
I0523 07:08:35.825829 34682 sgd_solver.cpp:112] Iteration 68830, lr = 0.01
I0523 07:08:40.457300 34682 solver.cpp:239] Iteration 68840 (2.08116 iter/s, 4.80502s/10 iters), loss = 7.84122
I0523 07:08:40.457346 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84122 (* 1 = 7.84122 loss)
I0523 07:08:40.519903 34682 sgd_solver.cpp:112] Iteration 68840, lr = 0.01
I0523 07:08:44.080832 34682 solver.cpp:239] Iteration 68850 (2.75989 iter/s, 3.62333s/10 iters), loss = 8.48792
I0523 07:08:44.080883 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48792 (* 1 = 8.48792 loss)
I0523 07:08:44.145073 34682 sgd_solver.cpp:112] Iteration 68850, lr = 0.01
I0523 07:08:48.175325 34682 solver.cpp:239] Iteration 68860 (2.44244 iter/s, 4.09427s/10 iters), loss = 6.11338
I0523 07:08:48.175511 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.11338 (* 1 = 6.11338 loss)
I0523 07:08:48.237488 34682 sgd_solver.cpp:112] Iteration 68860, lr = 0.01
I0523 07:08:53.270943 34682 solver.cpp:239] Iteration 68870 (1.96262 iter/s, 5.09523s/10 iters), loss = 7.87452
I0523 07:08:53.270992 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87452 (* 1 = 7.87452 loss)
I0523 07:08:53.342813 34682 sgd_solver.cpp:112] Iteration 68870, lr = 0.01
I0523 07:08:58.384294 34682 solver.cpp:239] Iteration 68880 (1.95745 iter/s, 5.10868s/10 iters), loss = 8.0533
I0523 07:08:58.384349 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0533 (* 1 = 8.0533 loss)
I0523 07:08:59.245415 34682 sgd_solver.cpp:112] Iteration 68880, lr = 0.01
I0523 07:09:03.657579 34682 solver.cpp:239] Iteration 68890 (1.89645 iter/s, 5.27302s/10 iters), loss = 8.13243
I0523 07:09:03.657629 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13243 (* 1 = 8.13243 loss)
I0523 07:09:04.220085 34682 sgd_solver.cpp:112] Iteration 68890, lr = 0.01
I0523 07:09:08.263538 34682 solver.cpp:239] Iteration 68900 (2.17122 iter/s, 4.60571s/10 iters), loss = 7.87362
I0523 07:09:08.263590 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87362 (* 1 = 7.87362 loss)
I0523 07:09:09.070608 34682 sgd_solver.cpp:112] Iteration 68900, lr = 0.01
I0523 07:09:12.060571 34682 solver.cpp:239] Iteration 68910 (2.63378 iter/s, 3.79682s/10 iters), loss = 8.46419
I0523 07:09:12.060616 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46419 (* 1 = 8.46419 loss)
I0523 07:09:12.117054 34682 sgd_solver.cpp:112] Iteration 68910, lr = 0.01
I0523 07:09:16.750643 34682 solver.cpp:239] Iteration 68920 (2.13227 iter/s, 4.68983s/10 iters), loss = 7.80854
I0523 07:09:16.750689 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80854 (* 1 = 7.80854 loss)
I0523 07:09:16.838038 34682 sgd_solver.cpp:112] Iteration 68920, lr = 0.01
I0523 07:09:21.079102 34682 solver.cpp:239] Iteration 68930 (2.31041 iter/s, 4.32823s/10 iters), loss = 7.38741
I0523 07:09:21.079303 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.38741 (* 1 = 7.38741 loss)
I0523 07:09:21.149871 34682 sgd_solver.cpp:112] Iteration 68930, lr = 0.01
I0523 07:09:22.981081 34682 solver.cpp:239] Iteration 68940 (5.2584 iter/s, 1.90172s/10 iters), loss = 8.56915
I0523 07:09:22.981128 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56915 (* 1 = 8.56915 loss)
I0523 07:09:23.049407 34682 sgd_solver.cpp:112] Iteration 68940, lr = 0.01
I0523 07:09:25.785894 34682 solver.cpp:239] Iteration 68950 (3.56552 iter/s, 2.80464s/10 iters), loss = 7.69857
I0523 07:09:25.785946 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69857 (* 1 = 7.69857 loss)
I0523 07:09:26.603030 34682 sgd_solver.cpp:112] Iteration 68950, lr = 0.01
I0523 07:09:28.422173 34682 solver.cpp:239] Iteration 68960 (3.79347 iter/s, 2.63611s/10 iters), loss = 8.47528
I0523 07:09:28.422212 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47528 (* 1 = 8.47528 loss)
I0523 07:09:28.498478 34682 sgd_solver.cpp:112] Iteration 68960, lr = 0.01
I0523 07:09:33.581732 34682 solver.cpp:239] Iteration 68970 (1.93825 iter/s, 5.15931s/10 iters), loss = 7.35269
I0523 07:09:33.581779 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.35269 (* 1 = 7.35269 loss)
I0523 07:09:33.643368 34682 sgd_solver.cpp:112] Iteration 68970, lr = 0.01
I0523 07:09:37.555160 34682 solver.cpp:239] Iteration 68980 (2.51686 iter/s, 3.97321s/10 iters), loss = 7.98265
I0523 07:09:37.555208 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98265 (* 1 = 7.98265 loss)
I0523 07:09:37.616968 34682 sgd_solver.cpp:112] Iteration 68980, lr = 0.01
I0523 07:09:43.138460 34682 solver.cpp:239] Iteration 68990 (1.79115 iter/s, 5.58302s/10 iters), loss = 6.99168
I0523 07:09:43.138512 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.99168 (* 1 = 6.99168 loss)
I0523 07:09:43.205269 34682 sgd_solver.cpp:112] Iteration 68990, lr = 0.01
I0523 07:09:48.015619 34682 solver.cpp:239] Iteration 69000 (2.05048 iter/s, 4.8769s/10 iters), loss = 7.29999
I0523 07:09:48.015669 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.29999 (* 1 = 7.29999 loss)
I0523 07:09:48.832769 34682 sgd_solver.cpp:112] Iteration 69000, lr = 0.01
I0523 07:09:54.221701 34682 solver.cpp:239] Iteration 69010 (1.6114 iter/s, 6.20578s/10 iters), loss = 8.62726
I0523 07:09:54.221928 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62726 (* 1 = 8.62726 loss)
I0523 07:09:55.068130 34682 sgd_solver.cpp:112] Iteration 69010, lr = 0.01
I0523 07:09:59.976615 34682 solver.cpp:239] Iteration 69020 (1.73778 iter/s, 5.75446s/10 iters), loss = 7.40417
I0523 07:09:59.976670 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.40417 (* 1 = 7.40417 loss)
I0523 07:10:00.637918 34682 sgd_solver.cpp:112] Iteration 69020, lr = 0.01
I0523 07:10:03.458361 34682 solver.cpp:239] Iteration 69030 (2.8723 iter/s, 3.48154s/10 iters), loss = 8.47024
I0523 07:10:03.458425 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47024 (* 1 = 8.47024 loss)
I0523 07:10:04.185837 34682 sgd_solver.cpp:112] Iteration 69030, lr = 0.01
I0523 07:10:09.115501 34682 solver.cpp:239] Iteration 69040 (1.76777 iter/s, 5.65685s/10 iters), loss = 8.57493
I0523 07:10:09.115552 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57493 (* 1 = 8.57493 loss)
I0523 07:10:09.722133 34682 sgd_solver.cpp:112] Iteration 69040, lr = 0.01
I0523 07:10:13.134024 34682 solver.cpp:239] Iteration 69050 (2.48862 iter/s, 4.0183s/10 iters), loss = 6.99306
I0523 07:10:13.134075 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.99306 (* 1 = 6.99306 loss)
I0523 07:10:13.989661 34682 sgd_solver.cpp:112] Iteration 69050, lr = 0.01
I0523 07:10:17.326994 34682 solver.cpp:239] Iteration 69060 (2.38508 iter/s, 4.19274s/10 iters), loss = 7.5183
I0523 07:10:17.327039 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5183 (* 1 = 7.5183 loss)
I0523 07:10:17.390976 34682 sgd_solver.cpp:112] Iteration 69060, lr = 0.01
I0523 07:10:21.826861 34682 solver.cpp:239] Iteration 69070 (2.2224 iter/s, 4.49963s/10 iters), loss = 7.62046
I0523 07:10:21.826921 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62046 (* 1 = 7.62046 loss)
I0523 07:10:22.568810 34682 sgd_solver.cpp:112] Iteration 69070, lr = 0.01
I0523 07:10:27.566867 34682 solver.cpp:239] Iteration 69080 (1.74225 iter/s, 5.7397s/10 iters), loss = 8.66628
I0523 07:10:27.567102 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66628 (* 1 = 8.66628 loss)
I0523 07:10:27.627012 34682 sgd_solver.cpp:112] Iteration 69080, lr = 0.01
I0523 07:10:33.823566 34682 solver.cpp:239] Iteration 69090 (1.5984 iter/s, 6.25624s/10 iters), loss = 8.80293
I0523 07:10:33.823611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80293 (* 1 = 8.80293 loss)
I0523 07:10:34.591882 34682 sgd_solver.cpp:112] Iteration 69090, lr = 0.01
I0523 07:10:38.811950 34682 solver.cpp:239] Iteration 69100 (2.00476 iter/s, 4.98814s/10 iters), loss = 8.09898
I0523 07:10:38.811995 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09898 (* 1 = 8.09898 loss)
I0523 07:10:39.641603 34682 sgd_solver.cpp:112] Iteration 69100, lr = 0.01
I0523 07:10:43.026005 34682 solver.cpp:239] Iteration 69110 (2.37314 iter/s, 4.21383s/10 iters), loss = 8.24921
I0523 07:10:43.026072 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24921 (* 1 = 8.24921 loss)
I0523 07:10:43.897372 34682 sgd_solver.cpp:112] Iteration 69110, lr = 0.01
I0523 07:10:47.987208 34682 solver.cpp:239] Iteration 69120 (2.01575 iter/s, 4.96094s/10 iters), loss = 7.79868
I0523 07:10:47.987247 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79868 (* 1 = 7.79868 loss)
I0523 07:10:48.760792 34682 sgd_solver.cpp:112] Iteration 69120, lr = 0.01
I0523 07:10:54.792080 34682 solver.cpp:239] Iteration 69130 (1.4696 iter/s, 6.80456s/10 iters), loss = 8.75222
I0523 07:10:54.792119 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75222 (* 1 = 8.75222 loss)
I0523 07:10:55.526767 34682 sgd_solver.cpp:112] Iteration 69130, lr = 0.01
I0523 07:10:59.317426 34682 solver.cpp:239] Iteration 69140 (2.20989 iter/s, 4.52511s/10 iters), loss = 8.01276
I0523 07:10:59.317692 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01276 (* 1 = 8.01276 loss)
I0523 07:10:59.975747 34682 sgd_solver.cpp:112] Iteration 69140, lr = 0.01
I0523 07:11:05.240198 34682 solver.cpp:239] Iteration 69150 (1.68854 iter/s, 5.92228s/10 iters), loss = 8.09183
I0523 07:11:05.240283 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09183 (* 1 = 8.09183 loss)
I0523 07:11:05.296491 34682 sgd_solver.cpp:112] Iteration 69150, lr = 0.01
I0523 07:11:09.124054 34682 solver.cpp:239] Iteration 69160 (2.57492 iter/s, 3.88361s/10 iters), loss = 7.88025
I0523 07:11:09.124116 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88025 (* 1 = 7.88025 loss)
I0523 07:11:09.184833 34682 sgd_solver.cpp:112] Iteration 69160, lr = 0.01
I0523 07:11:13.832865 34682 solver.cpp:239] Iteration 69170 (2.12379 iter/s, 4.70856s/10 iters), loss = 8.24516
I0523 07:11:13.832911 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24516 (* 1 = 8.24516 loss)
I0523 07:11:13.896239 34682 sgd_solver.cpp:112] Iteration 69170, lr = 0.01
I0523 07:11:16.923234 34682 solver.cpp:239] Iteration 69180 (3.23606 iter/s, 3.09018s/10 iters), loss = 7.68663
I0523 07:11:16.923295 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68663 (* 1 = 7.68663 loss)
I0523 07:11:17.503753 34682 sgd_solver.cpp:112] Iteration 69180, lr = 0.01
I0523 07:11:22.656822 34682 solver.cpp:239] Iteration 69190 (1.7442 iter/s, 5.7333s/10 iters), loss = 8.95813
I0523 07:11:22.656870 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95813 (* 1 = 8.95813 loss)
I0523 07:11:22.930452 34682 sgd_solver.cpp:112] Iteration 69190, lr = 0.01
I0523 07:11:26.534085 34682 solver.cpp:239] Iteration 69200 (2.57928 iter/s, 3.87706s/10 iters), loss = 7.74693
I0523 07:11:26.534128 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74693 (* 1 = 7.74693 loss)
I0523 07:11:27.386909 34682 sgd_solver.cpp:112] Iteration 69200, lr = 0.01
I0523 07:11:32.246915 34682 solver.cpp:239] Iteration 69210 (1.75053 iter/s, 5.71255s/10 iters), loss = 7.40661
I0523 07:11:32.247064 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.40661 (* 1 = 7.40661 loss)
I0523 07:11:32.883513 34682 sgd_solver.cpp:112] Iteration 69210, lr = 0.01
I0523 07:11:38.554924 34682 solver.cpp:239] Iteration 69220 (1.58539 iter/s, 6.30761s/10 iters), loss = 7.78357
I0523 07:11:38.554970 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.78357 (* 1 = 7.78357 loss)
I0523 07:11:38.631083 34682 sgd_solver.cpp:112] Iteration 69220, lr = 0.01
I0523 07:11:44.212445 34682 solver.cpp:239] Iteration 69230 (1.76765 iter/s, 5.65724s/10 iters), loss = 8.52825
I0523 07:11:44.212499 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52825 (* 1 = 8.52825 loss)
I0523 07:11:44.444173 34682 sgd_solver.cpp:112] Iteration 69230, lr = 0.01
I0523 07:11:49.856595 34682 solver.cpp:239] Iteration 69240 (1.77183 iter/s, 5.64387s/10 iters), loss = 8.47603
I0523 07:11:49.856644 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47603 (* 1 = 8.47603 loss)
I0523 07:11:50.690973 34682 sgd_solver.cpp:112] Iteration 69240, lr = 0.01
I0523 07:11:53.116143 34682 solver.cpp:239] Iteration 69250 (3.06809 iter/s, 3.25936s/10 iters), loss = 7.19277
I0523 07:11:53.116189 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.19277 (* 1 = 7.19277 loss)
I0523 07:11:53.989670 34682 sgd_solver.cpp:112] Iteration 69250, lr = 0.01
I0523 07:11:57.452731 34682 solver.cpp:239] Iteration 69260 (2.30725 iter/s, 4.33416s/10 iters), loss = 7.49319
I0523 07:11:57.452776 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.49319 (* 1 = 7.49319 loss)
I0523 07:11:58.154222 34682 sgd_solver.cpp:112] Iteration 69260, lr = 0.01
I0523 07:12:02.970888 34682 solver.cpp:239] Iteration 69270 (1.81229 iter/s, 5.51788s/10 iters), loss = 8.04789
I0523 07:12:02.971134 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04789 (* 1 = 8.04789 loss)
I0523 07:12:03.098536 34682 sgd_solver.cpp:112] Iteration 69270, lr = 0.01
I0523 07:12:07.985018 34682 solver.cpp:239] Iteration 69280 (1.99454 iter/s, 5.01369s/10 iters), loss = 7.82648
I0523 07:12:07.985069 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82648 (* 1 = 7.82648 loss)
I0523 07:12:08.542901 34682 sgd_solver.cpp:112] Iteration 69280, lr = 0.01
I0523 07:12:13.378440 34682 solver.cpp:239] Iteration 69290 (1.8542 iter/s, 5.39315s/10 iters), loss = 7.50338
I0523 07:12:13.378482 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50338 (* 1 = 7.50338 loss)
I0523 07:12:13.442368 34682 sgd_solver.cpp:112] Iteration 69290, lr = 0.01
I0523 07:12:17.407546 34682 solver.cpp:239] Iteration 69300 (2.48207 iter/s, 4.0289s/10 iters), loss = 7.59671
I0523 07:12:17.407593 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.59671 (* 1 = 7.59671 loss)
I0523 07:12:18.220194 34682 sgd_solver.cpp:112] Iteration 69300, lr = 0.01
I0523 07:12:20.684448 34682 solver.cpp:239] Iteration 69310 (3.05184 iter/s, 3.27671s/10 iters), loss = 7.53171
I0523 07:12:20.684514 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.53171 (* 1 = 7.53171 loss)
I0523 07:12:21.532781 34682 sgd_solver.cpp:112] Iteration 69310, lr = 0.01
I0523 07:12:25.190214 34682 solver.cpp:239] Iteration 69320 (2.2195 iter/s, 4.50552s/10 iters), loss = 8.46135
I0523 07:12:25.190292 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46135 (* 1 = 8.46135 loss)
I0523 07:12:25.249234 34682 sgd_solver.cpp:112] Iteration 69320, lr = 0.01
I0523 07:12:31.417480 34682 solver.cpp:239] Iteration 69330 (1.60592 iter/s, 6.22696s/10 iters), loss = 7.73506
I0523 07:12:31.417529 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.73506 (* 1 = 7.73506 loss)
I0523 07:12:31.722187 34682 sgd_solver.cpp:112] Iteration 69330, lr = 0.01
I0523 07:12:35.640039 34682 solver.cpp:239] Iteration 69340 (2.36837 iter/s, 4.22231s/10 iters), loss = 7.91503
I0523 07:12:35.640259 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91503 (* 1 = 7.91503 loss)
I0523 07:12:36.122252 34682 sgd_solver.cpp:112] Iteration 69340, lr = 0.01
I0523 07:12:40.601466 34682 solver.cpp:239] Iteration 69350 (2.0157 iter/s, 4.96105s/10 iters), loss = 8.30438
I0523 07:12:40.601511 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30438 (* 1 = 8.30438 loss)
I0523 07:12:40.656327 34682 sgd_solver.cpp:112] Iteration 69350, lr = 0.01
I0523 07:12:47.714428 34682 solver.cpp:239] Iteration 69360 (1.40595 iter/s, 7.11262s/10 iters), loss = 8.23773
I0523 07:12:47.714504 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23773 (* 1 = 8.23773 loss)
I0523 07:12:48.357676 34682 sgd_solver.cpp:112] Iteration 69360, lr = 0.01
I0523 07:12:52.050460 34682 solver.cpp:239] Iteration 69370 (2.30639 iter/s, 4.33579s/10 iters), loss = 8.35587
I0523 07:12:52.050509 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35587 (* 1 = 8.35587 loss)
I0523 07:12:52.874212 34682 sgd_solver.cpp:112] Iteration 69370, lr = 0.01
I0523 07:12:56.244537 34682 solver.cpp:239] Iteration 69380 (2.38444 iter/s, 4.19385s/10 iters), loss = 8.4355
I0523 07:12:56.244586 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4355 (* 1 = 8.4355 loss)
I0523 07:12:57.100096 34682 sgd_solver.cpp:112] Iteration 69380, lr = 0.01
I0523 07:13:00.138795 34682 solver.cpp:239] Iteration 69390 (2.56802 iter/s, 3.89405s/10 iters), loss = 8.44786
I0523 07:13:00.138839 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44786 (* 1 = 8.44786 loss)
I0523 07:13:00.196342 34682 sgd_solver.cpp:112] Iteration 69390, lr = 0.01
I0523 07:13:05.116037 34682 solver.cpp:239] Iteration 69400 (2.00925 iter/s, 4.97698s/10 iters), loss = 7.9575
I0523 07:13:05.116089 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9575 (* 1 = 7.9575 loss)
I0523 07:13:05.940083 34682 sgd_solver.cpp:112] Iteration 69400, lr = 0.01
I0523 07:13:11.384045 34682 solver.cpp:239] Iteration 69410 (1.59548 iter/s, 6.2677s/10 iters), loss = 8.21054
I0523 07:13:11.384107 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21054 (* 1 = 8.21054 loss)
I0523 07:13:12.249456 34682 sgd_solver.cpp:112] Iteration 69410, lr = 0.01
I0523 07:13:16.230002 34682 solver.cpp:239] Iteration 69420 (2.06369 iter/s, 4.8457s/10 iters), loss = 8.23528
I0523 07:13:16.230049 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23528 (* 1 = 8.23528 loss)
I0523 07:13:16.311118 34682 sgd_solver.cpp:112] Iteration 69420, lr = 0.01
I0523 07:13:23.311588 34682 solver.cpp:239] Iteration 69430 (1.41218 iter/s, 7.08124s/10 iters), loss = 7.31672
I0523 07:13:23.311641 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.31672 (* 1 = 7.31672 loss)
I0523 07:13:23.973003 34682 sgd_solver.cpp:112] Iteration 69430, lr = 0.01
I0523 07:13:28.904923 34682 solver.cpp:239] Iteration 69440 (1.78793 iter/s, 5.59306s/10 iters), loss = 7.61188
I0523 07:13:28.904968 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.61188 (* 1 = 7.61188 loss)
I0523 07:13:28.965847 34682 sgd_solver.cpp:112] Iteration 69440, lr = 0.01
I0523 07:13:31.671982 34682 solver.cpp:239] Iteration 69450 (3.61416 iter/s, 2.76689s/10 iters), loss = 7.49018
I0523 07:13:31.672040 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.49018 (* 1 = 7.49018 loss)
I0523 07:13:32.196806 34682 sgd_solver.cpp:112] Iteration 69450, lr = 0.01
I0523 07:13:36.956702 34682 solver.cpp:239] Iteration 69460 (1.89235 iter/s, 5.28444s/10 iters), loss = 8.86103
I0523 07:13:36.956917 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86103 (* 1 = 8.86103 loss)
I0523 07:13:37.017072 34682 sgd_solver.cpp:112] Iteration 69460, lr = 0.01
I0523 07:13:40.890691 34682 solver.cpp:239] Iteration 69470 (2.54218 iter/s, 3.93363s/10 iters), loss = 8.22513
I0523 07:13:40.890795 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22513 (* 1 = 8.22513 loss)
I0523 07:13:41.415436 34682 sgd_solver.cpp:112] Iteration 69470, lr = 0.01
I0523 07:13:47.449312 34682 solver.cpp:239] Iteration 69480 (1.5248 iter/s, 6.55824s/10 iters), loss = 7.90316
I0523 07:13:47.449352 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90316 (* 1 = 7.90316 loss)
I0523 07:13:47.644093 34682 sgd_solver.cpp:112] Iteration 69480, lr = 0.01
I0523 07:13:52.901779 34682 solver.cpp:239] Iteration 69490 (1.83412 iter/s, 5.4522s/10 iters), loss = 8.91923
I0523 07:13:52.901841 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91923 (* 1 = 8.91923 loss)
I0523 07:13:52.965724 34682 sgd_solver.cpp:112] Iteration 69490, lr = 0.01
I0523 07:13:56.504735 34682 solver.cpp:239] Iteration 69500 (2.77568 iter/s, 3.60273s/10 iters), loss = 8.07363
I0523 07:13:56.504811 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07363 (* 1 = 8.07363 loss)
I0523 07:13:56.562265 34682 sgd_solver.cpp:112] Iteration 69500, lr = 0.01
I0523 07:14:00.686942 34682 solver.cpp:239] Iteration 69510 (2.39122 iter/s, 4.18196s/10 iters), loss = 8.58097
I0523 07:14:00.686982 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58097 (* 1 = 8.58097 loss)
I0523 07:14:00.749112 34682 sgd_solver.cpp:112] Iteration 69510, lr = 0.01
I0523 07:14:05.416635 34682 solver.cpp:239] Iteration 69520 (2.11441 iter/s, 4.72945s/10 iters), loss = 8.57838
I0523 07:14:05.416689 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57838 (* 1 = 8.57838 loss)
I0523 07:14:05.492920 34682 sgd_solver.cpp:112] Iteration 69520, lr = 0.01
I0523 07:14:10.277190 34682 solver.cpp:239] Iteration 69530 (2.05748 iter/s, 4.86031s/10 iters), loss = 7.65364
I0523 07:14:10.277376 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.65364 (* 1 = 7.65364 loss)
I0523 07:14:10.346065 34682 sgd_solver.cpp:112] Iteration 69530, lr = 0.01
I0523 07:14:14.155256 34682 solver.cpp:239] Iteration 69540 (2.57884 iter/s, 3.87771s/10 iters), loss = 8.27801
I0523 07:14:14.155304 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27801 (* 1 = 8.27801 loss)
I0523 07:14:14.216876 34682 sgd_solver.cpp:112] Iteration 69540, lr = 0.01
I0523 07:14:18.370837 34682 solver.cpp:239] Iteration 69550 (2.37228 iter/s, 4.21536s/10 iters), loss = 7.87755
I0523 07:14:18.370879 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87755 (* 1 = 7.87755 loss)
I0523 07:14:18.439880 34682 sgd_solver.cpp:112] Iteration 69550, lr = 0.01
I0523 07:14:22.696460 34682 solver.cpp:239] Iteration 69560 (2.31193 iter/s, 4.3254s/10 iters), loss = 8.72236
I0523 07:14:22.696509 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72236 (* 1 = 8.72236 loss)
I0523 07:14:23.002418 34682 sgd_solver.cpp:112] Iteration 69560, lr = 0.01
I0523 07:14:29.691704 34682 solver.cpp:239] Iteration 69570 (1.42961 iter/s, 6.99491s/10 iters), loss = 7.54825
I0523 07:14:29.691748 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54825 (* 1 = 7.54825 loss)
I0523 07:14:30.551617 34682 sgd_solver.cpp:112] Iteration 69570, lr = 0.01
I0523 07:14:35.411650 34682 solver.cpp:239] Iteration 69580 (1.74835 iter/s, 5.71966s/10 iters), loss = 7.98751
I0523 07:14:35.411705 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98751 (* 1 = 7.98751 loss)
I0523 07:14:36.169183 34682 sgd_solver.cpp:112] Iteration 69580, lr = 0.01
I0523 07:14:41.191500 34682 solver.cpp:239] Iteration 69590 (1.73023 iter/s, 5.77957s/10 iters), loss = 8.08438
I0523 07:14:41.191690 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08438 (* 1 = 8.08438 loss)
I0523 07:14:41.270928 34682 sgd_solver.cpp:112] Iteration 69590, lr = 0.01
I0523 07:14:46.682570 34682 solver.cpp:239] Iteration 69600 (1.82128 iter/s, 5.49066s/10 iters), loss = 8.24815
I0523 07:14:46.682633 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24815 (* 1 = 8.24815 loss)
I0523 07:14:46.751761 34682 sgd_solver.cpp:112] Iteration 69600, lr = 0.01
I0523 07:14:50.240525 34682 solver.cpp:239] Iteration 69610 (2.81077 iter/s, 3.55775s/10 iters), loss = 7.41045
I0523 07:14:50.240566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.41045 (* 1 = 7.41045 loss)
I0523 07:14:50.988256 34682 sgd_solver.cpp:112] Iteration 69610, lr = 0.01
I0523 07:14:55.312705 34682 solver.cpp:239] Iteration 69620 (1.97164 iter/s, 5.07193s/10 iters), loss = 7.62063
I0523 07:14:55.312765 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62063 (* 1 = 7.62063 loss)
I0523 07:14:56.109843 34682 sgd_solver.cpp:112] Iteration 69620, lr = 0.01
I0523 07:15:00.658006 34682 solver.cpp:239] Iteration 69630 (1.87091 iter/s, 5.345s/10 iters), loss = 8.34288
I0523 07:15:00.658067 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34288 (* 1 = 8.34288 loss)
I0523 07:15:01.084695 34682 sgd_solver.cpp:112] Iteration 69630, lr = 0.01
I0523 07:15:05.155886 34682 solver.cpp:239] Iteration 69640 (2.22339 iter/s, 4.49764s/10 iters), loss = 8.69719
I0523 07:15:05.155930 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69719 (* 1 = 8.69719 loss)
I0523 07:15:05.265596 34682 sgd_solver.cpp:112] Iteration 69640, lr = 0.01
I0523 07:15:09.937582 34682 solver.cpp:239] Iteration 69650 (2.09142 iter/s, 4.78145s/10 iters), loss = 7.62189
I0523 07:15:09.937628 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62189 (* 1 = 7.62189 loss)
I0523 07:15:10.005893 34682 sgd_solver.cpp:112] Iteration 69650, lr = 0.01
I0523 07:15:13.004676 34682 solver.cpp:239] Iteration 69660 (3.2606 iter/s, 3.06692s/10 iters), loss = 7.4527
I0523 07:15:13.004936 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4527 (* 1 = 7.4527 loss)
I0523 07:15:13.767704 34682 sgd_solver.cpp:112] Iteration 69660, lr = 0.01
I0523 07:15:18.195703 34682 solver.cpp:239] Iteration 69670 (1.92657 iter/s, 5.19057s/10 iters), loss = 8.12923
I0523 07:15:18.195751 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12923 (* 1 = 8.12923 loss)
I0523 07:15:19.003541 34682 sgd_solver.cpp:112] Iteration 69670, lr = 0.01
I0523 07:15:25.315109 34682 solver.cpp:239] Iteration 69680 (1.40468 iter/s, 7.11907s/10 iters), loss = 7.96306
I0523 07:15:25.315151 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96306 (* 1 = 7.96306 loss)
I0523 07:15:25.390585 34682 sgd_solver.cpp:112] Iteration 69680, lr = 0.01
I0523 07:15:31.005707 34682 solver.cpp:239] Iteration 69690 (1.75737 iter/s, 5.69032s/10 iters), loss = 7.33836
I0523 07:15:31.005759 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.33836 (* 1 = 7.33836 loss)
I0523 07:15:31.076433 34682 sgd_solver.cpp:112] Iteration 69690, lr = 0.01
I0523 07:15:36.784713 34682 solver.cpp:239] Iteration 69700 (1.73049 iter/s, 5.77872s/10 iters), loss = 7.86677
I0523 07:15:36.784759 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86677 (* 1 = 7.86677 loss)
I0523 07:15:37.563083 34682 sgd_solver.cpp:112] Iteration 69700, lr = 0.01
I0523 07:15:39.391571 34682 solver.cpp:239] Iteration 69710 (3.83627 iter/s, 2.6067s/10 iters), loss = 7.42033
I0523 07:15:39.391624 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42033 (* 1 = 7.42033 loss)
I0523 07:15:40.212797 34682 sgd_solver.cpp:112] Iteration 69710, lr = 0.01
I0523 07:15:44.031752 34682 solver.cpp:239] Iteration 69720 (2.1552 iter/s, 4.63994s/10 iters), loss = 8.33494
I0523 07:15:44.032006 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33494 (* 1 = 8.33494 loss)
I0523 07:15:44.104084 34682 sgd_solver.cpp:112] Iteration 69720, lr = 0.01
I0523 07:15:49.078920 34682 solver.cpp:239] Iteration 69730 (1.98146 iter/s, 5.04677s/10 iters), loss = 7.59704
I0523 07:15:49.078970 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.59704 (* 1 = 7.59704 loss)
I0523 07:15:49.139924 34682 sgd_solver.cpp:112] Iteration 69730, lr = 0.01
I0523 07:15:55.697154 34682 solver.cpp:239] Iteration 69740 (1.51105 iter/s, 6.61792s/10 iters), loss = 8.21875
I0523 07:15:55.697193 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21875 (* 1 = 8.21875 loss)
I0523 07:15:55.769196 34682 sgd_solver.cpp:112] Iteration 69740, lr = 0.01
I0523 07:16:01.362884 34682 solver.cpp:239] Iteration 69750 (1.76508 iter/s, 5.66546s/10 iters), loss = 8.26426
I0523 07:16:01.362927 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26426 (* 1 = 8.26426 loss)
I0523 07:16:01.437752 34682 sgd_solver.cpp:112] Iteration 69750, lr = 0.01
I0523 07:16:06.483834 34682 solver.cpp:239] Iteration 69760 (1.95286 iter/s, 5.12069s/10 iters), loss = 7.51926
I0523 07:16:06.483887 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51926 (* 1 = 7.51926 loss)
I0523 07:16:07.362272 34682 sgd_solver.cpp:112] Iteration 69760, lr = 0.01
I0523 07:16:12.139672 34682 solver.cpp:239] Iteration 69770 (1.76817 iter/s, 5.65555s/10 iters), loss = 7.18461
I0523 07:16:12.139726 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.18461 (* 1 = 7.18461 loss)
I0523 07:16:12.903373 34682 sgd_solver.cpp:112] Iteration 69770, lr = 0.01
I0523 07:16:15.511617 34682 solver.cpp:239] Iteration 69780 (2.96582 iter/s, 3.37175s/10 iters), loss = 6.95078
I0523 07:16:15.511744 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.95078 (* 1 = 6.95078 loss)
I0523 07:16:16.039103 34682 sgd_solver.cpp:112] Iteration 69780, lr = 0.01
I0523 07:16:19.150418 34682 solver.cpp:239] Iteration 69790 (2.74838 iter/s, 3.63851s/10 iters), loss = 8.22955
I0523 07:16:19.150501 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22955 (* 1 = 8.22955 loss)
I0523 07:16:19.616418 34682 sgd_solver.cpp:112] Iteration 69790, lr = 0.01
I0523 07:16:25.144937 34682 solver.cpp:239] Iteration 69800 (1.66828 iter/s, 5.99419s/10 iters), loss = 8.00792
I0523 07:16:25.144986 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00792 (* 1 = 8.00792 loss)
I0523 07:16:25.904208 34682 sgd_solver.cpp:112] Iteration 69800, lr = 0.01
I0523 07:16:32.035646 34682 solver.cpp:239] Iteration 69810 (1.4513 iter/s, 6.89038s/10 iters), loss = 8.10433
I0523 07:16:32.035698 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10433 (* 1 = 8.10433 loss)
I0523 07:16:32.111508 34682 sgd_solver.cpp:112] Iteration 69810, lr = 0.01
I0523 07:16:36.416688 34682 solver.cpp:239] Iteration 69820 (2.28268 iter/s, 4.38081s/10 iters), loss = 9.04669
I0523 07:16:36.416729 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.04669 (* 1 = 9.04669 loss)
I0523 07:16:36.489776 34682 sgd_solver.cpp:112] Iteration 69820, lr = 0.01
I0523 07:16:42.894199 34682 solver.cpp:239] Iteration 69830 (1.54387 iter/s, 6.47721s/10 iters), loss = 7.15223
I0523 07:16:42.894243 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.15223 (* 1 = 7.15223 loss)
I0523 07:16:42.962412 34682 sgd_solver.cpp:112] Iteration 69830, lr = 0.01
I0523 07:16:48.287581 34682 solver.cpp:239] Iteration 69840 (1.85421 iter/s, 5.39312s/10 iters), loss = 7.88196
I0523 07:16:48.287818 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88196 (* 1 = 7.88196 loss)
I0523 07:16:48.356042 34682 sgd_solver.cpp:112] Iteration 69840, lr = 0.01
I0523 07:16:52.776091 34682 solver.cpp:239] Iteration 69850 (2.22812 iter/s, 4.48808s/10 iters), loss = 7.74086
I0523 07:16:52.776152 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74086 (* 1 = 7.74086 loss)
I0523 07:16:53.564383 34682 sgd_solver.cpp:112] Iteration 69850, lr = 0.01
I0523 07:16:57.125504 34682 solver.cpp:239] Iteration 69860 (2.29929 iter/s, 4.34917s/10 iters), loss = 7.70975
I0523 07:16:57.125571 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.70975 (* 1 = 7.70975 loss)
I0523 07:16:57.972394 34682 sgd_solver.cpp:112] Iteration 69860, lr = 0.01
I0523 07:17:03.197809 34682 solver.cpp:239] Iteration 69870 (1.64691 iter/s, 6.07199s/10 iters), loss = 8.18615
I0523 07:17:03.197855 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18615 (* 1 = 8.18615 loss)
I0523 07:17:04.065696 34682 sgd_solver.cpp:112] Iteration 69870, lr = 0.01
I0523 07:17:10.654345 34682 solver.cpp:239] Iteration 69880 (1.34117 iter/s, 7.4562s/10 iters), loss = 8.87107
I0523 07:17:10.654382 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87107 (* 1 = 8.87107 loss)
I0523 07:17:10.723354 34682 sgd_solver.cpp:112] Iteration 69880, lr = 0.01
I0523 07:17:14.856999 34682 solver.cpp:239] Iteration 69890 (2.37957 iter/s, 4.20244s/10 iters), loss = 8.44551
I0523 07:17:14.857053 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44551 (* 1 = 8.44551 loss)
I0523 07:17:14.920131 34682 sgd_solver.cpp:112] Iteration 69890, lr = 0.01
I0523 07:17:20.408964 34682 solver.cpp:239] Iteration 69900 (1.80126 iter/s, 5.55168s/10 iters), loss = 8.12848
I0523 07:17:20.409193 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12848 (* 1 = 8.12848 loss)
I0523 07:17:21.042546 34682 sgd_solver.cpp:112] Iteration 69900, lr = 0.01
I0523 07:17:24.602061 34682 solver.cpp:239] Iteration 69910 (2.38509 iter/s, 4.19272s/10 iters), loss = 7.76226
I0523 07:17:24.602113 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.76226 (* 1 = 7.76226 loss)
I0523 07:17:25.424196 34682 sgd_solver.cpp:112] Iteration 69910, lr = 0.01
I0523 07:17:28.791527 34682 solver.cpp:239] Iteration 69920 (2.38707 iter/s, 4.18923s/10 iters), loss = 8.3531
I0523 07:17:28.791592 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3531 (* 1 = 8.3531 loss)
I0523 07:17:28.870755 34682 sgd_solver.cpp:112] Iteration 69920, lr = 0.01
I0523 07:17:34.886114 34682 solver.cpp:239] Iteration 69930 (1.64088 iter/s, 6.09428s/10 iters), loss = 8.02604
I0523 07:17:34.886152 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02604 (* 1 = 8.02604 loss)
I0523 07:17:34.960064 34682 sgd_solver.cpp:112] Iteration 69930, lr = 0.01
I0523 07:17:40.745731 34682 solver.cpp:239] Iteration 69940 (1.70668 iter/s, 5.85934s/10 iters), loss = 7.29063
I0523 07:17:40.745772 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.29063 (* 1 = 7.29063 loss)
I0523 07:17:40.823201 34682 sgd_solver.cpp:112] Iteration 69940, lr = 0.01
I0523 07:17:45.509826 34682 solver.cpp:239] Iteration 69950 (2.09914 iter/s, 4.76386s/10 iters), loss = 6.91816
I0523 07:17:45.509873 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.91816 (* 1 = 6.91816 loss)
I0523 07:17:45.586130 34682 sgd_solver.cpp:112] Iteration 69950, lr = 0.01
I0523 07:17:51.055754 34682 solver.cpp:239] Iteration 69960 (1.80322 iter/s, 5.54565s/10 iters), loss = 7.78743
I0523 07:17:51.055941 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.78743 (* 1 = 7.78743 loss)
I0523 07:17:51.917062 34682 sgd_solver.cpp:112] Iteration 69960, lr = 0.01
I0523 07:17:55.357126 34682 solver.cpp:239] Iteration 69970 (2.32503 iter/s, 4.30101s/10 iters), loss = 8.38267
I0523 07:17:55.357178 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38267 (* 1 = 8.38267 loss)
I0523 07:17:55.422518 34682 sgd_solver.cpp:112] Iteration 69970, lr = 0.01
I0523 07:17:58.723176 34682 solver.cpp:239] Iteration 69980 (2.97101 iter/s, 3.36585s/10 iters), loss = 8.15055
I0523 07:17:58.723222 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15055 (* 1 = 8.15055 loss)
I0523 07:17:59.534675 34682 sgd_solver.cpp:112] Iteration 69980, lr = 0.01
I0523 07:18:02.088421 34682 solver.cpp:239] Iteration 69990 (2.97172 iter/s, 3.36506s/10 iters), loss = 7.55311
I0523 07:18:02.088461 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.55311 (* 1 = 7.55311 loss)
I0523 07:18:02.158558 34682 sgd_solver.cpp:112] Iteration 69990, lr = 0.01
I0523 07:18:05.884613 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_70000.caffemodel
I0523 07:18:07.861024 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_70000.solverstate
I0523 07:18:08.121054 34682 solver.cpp:239] Iteration 70000 (1.65773 iter/s, 6.03235s/10 iters), loss = 7.95998
I0523 07:18:08.121103 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95998 (* 1 = 7.95998 loss)
I0523 07:18:08.744040 34682 sgd_solver.cpp:112] Iteration 70000, lr = 0.01
I0523 07:18:13.701135 34682 solver.cpp:239] Iteration 70010 (1.79218 iter/s, 5.57981s/10 iters), loss = 7.17046
I0523 07:18:13.701180 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.17046 (* 1 = 7.17046 loss)
I0523 07:18:13.761895 34682 sgd_solver.cpp:112] Iteration 70010, lr = 0.01
I0523 07:18:19.485783 34682 solver.cpp:239] Iteration 70020 (1.7288 iter/s, 5.78437s/10 iters), loss = 8.00744
I0523 07:18:19.485822 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00744 (* 1 = 8.00744 loss)
I0523 07:18:19.557425 34682 sgd_solver.cpp:112] Iteration 70020, lr = 0.01
I0523 07:18:24.720223 34682 solver.cpp:239] Iteration 70030 (1.91052 iter/s, 5.23418s/10 iters), loss = 7.30252
I0523 07:18:24.720391 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.30252 (* 1 = 7.30252 loss)
I0523 07:18:24.796293 34682 sgd_solver.cpp:112] Iteration 70030, lr = 0.01
I0523 07:18:29.605015 34682 solver.cpp:239] Iteration 70040 (2.04832 iter/s, 4.88204s/10 iters), loss = 7.34331
I0523 07:18:29.605060 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.34331 (* 1 = 7.34331 loss)
I0523 07:18:30.433027 34682 sgd_solver.cpp:112] Iteration 70040, lr = 0.01
I0523 07:18:36.177827 34682 solver.cpp:239] Iteration 70050 (1.52149 iter/s, 6.5725s/10 iters), loss = 7.64784
I0523 07:18:36.177884 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64784 (* 1 = 7.64784 loss)
I0523 07:18:36.248347 34682 sgd_solver.cpp:112] Iteration 70050, lr = 0.01
I0523 07:18:40.461336 34682 solver.cpp:239] Iteration 70060 (2.33466 iter/s, 4.28328s/10 iters), loss = 7.64239
I0523 07:18:40.461380 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64239 (* 1 = 7.64239 loss)
I0523 07:18:40.526199 34682 sgd_solver.cpp:112] Iteration 70060, lr = 0.01
I0523 07:18:44.986734 34682 solver.cpp:239] Iteration 70070 (2.20986 iter/s, 4.52517s/10 iters), loss = 7.94844
I0523 07:18:44.986773 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94844 (* 1 = 7.94844 loss)
I0523 07:18:45.742939 34682 sgd_solver.cpp:112] Iteration 70070, lr = 0.01
I0523 07:18:51.356837 34682 solver.cpp:239] Iteration 70080 (1.56991 iter/s, 6.36978s/10 iters), loss = 8.20311
I0523 07:18:51.356905 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20311 (* 1 = 8.20311 loss)
I0523 07:18:52.222865 34682 sgd_solver.cpp:112] Iteration 70080, lr = 0.01
I0523 07:18:56.447860 34682 solver.cpp:239] Iteration 70090 (1.96435 iter/s, 5.09073s/10 iters), loss = 7.98208
I0523 07:18:56.448026 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98208 (* 1 = 7.98208 loss)
I0523 07:18:56.528625 34682 sgd_solver.cpp:112] Iteration 70090, lr = 0.01
I0523 07:19:01.953150 34682 solver.cpp:239] Iteration 70100 (1.81657 iter/s, 5.50489s/10 iters), loss = 7.76019
I0523 07:19:01.953192 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.76019 (* 1 = 7.76019 loss)
I0523 07:19:02.015008 34682 sgd_solver.cpp:112] Iteration 70100, lr = 0.01
I0523 07:19:06.876040 34682 solver.cpp:239] Iteration 70110 (2.03143 iter/s, 4.92264s/10 iters), loss = 8.58949
I0523 07:19:06.876097 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58949 (* 1 = 8.58949 loss)
I0523 07:19:06.937955 34682 sgd_solver.cpp:112] Iteration 70110, lr = 0.01
I0523 07:19:12.210101 34682 solver.cpp:239] Iteration 70120 (1.87484 iter/s, 5.33379s/10 iters), loss = 7.37034
I0523 07:19:12.210147 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.37034 (* 1 = 7.37034 loss)
I0523 07:19:12.856169 34682 sgd_solver.cpp:112] Iteration 70120, lr = 0.01
I0523 07:19:19.329432 34682 solver.cpp:239] Iteration 70130 (1.40469 iter/s, 7.119s/10 iters), loss = 7.54447
I0523 07:19:19.329478 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54447 (* 1 = 7.54447 loss)
I0523 07:19:19.401079 34682 sgd_solver.cpp:112] Iteration 70130, lr = 0.01
I0523 07:19:24.067639 34682 solver.cpp:239] Iteration 70140 (2.11061 iter/s, 4.73796s/10 iters), loss = 7.69226
I0523 07:19:24.067697 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69226 (* 1 = 7.69226 loss)
I0523 07:19:24.915158 34682 sgd_solver.cpp:112] Iteration 70140, lr = 0.01
I0523 07:19:28.111632 34682 solver.cpp:239] Iteration 70150 (2.47294 iter/s, 4.04377s/10 iters), loss = 6.83449
I0523 07:19:28.111878 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.83449 (* 1 = 6.83449 loss)
I0523 07:19:28.175987 34682 sgd_solver.cpp:112] Iteration 70150, lr = 0.01
I0523 07:19:32.878433 34682 solver.cpp:239] Iteration 70160 (2.09802 iter/s, 4.76639s/10 iters), loss = 7.95321
I0523 07:19:32.878473 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95321 (* 1 = 7.95321 loss)
I0523 07:19:32.942394 34682 sgd_solver.cpp:112] Iteration 70160, lr = 0.01
I0523 07:19:36.248289 34682 solver.cpp:239] Iteration 70170 (2.96766 iter/s, 3.36965s/10 iters), loss = 7.60154
I0523 07:19:36.248360 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.60154 (* 1 = 7.60154 loss)
I0523 07:19:36.306382 34682 sgd_solver.cpp:112] Iteration 70170, lr = 0.01
I0523 07:19:40.309602 34682 solver.cpp:239] Iteration 70180 (2.4624 iter/s, 4.06108s/10 iters), loss = 8.36605
I0523 07:19:40.309643 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36605 (* 1 = 8.36605 loss)
I0523 07:19:40.384680 34682 sgd_solver.cpp:112] Iteration 70180, lr = 0.01
I0523 07:19:45.374212 34682 solver.cpp:239] Iteration 70190 (1.97459 iter/s, 5.06435s/10 iters), loss = 7.89177
I0523 07:19:45.374258 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89177 (* 1 = 7.89177 loss)
I0523 07:19:45.445422 34682 sgd_solver.cpp:112] Iteration 70190, lr = 0.01
I0523 07:19:48.972093 34682 solver.cpp:239] Iteration 70200 (2.77957 iter/s, 3.59767s/10 iters), loss = 8.09604
I0523 07:19:48.972149 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09604 (* 1 = 8.09604 loss)
I0523 07:19:49.051476 34682 sgd_solver.cpp:112] Iteration 70200, lr = 0.01
I0523 07:19:51.968350 34682 solver.cpp:239] Iteration 70210 (3.3377 iter/s, 2.99608s/10 iters), loss = 7.54658
I0523 07:19:51.968394 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54658 (* 1 = 7.54658 loss)
I0523 07:19:52.844045 34682 sgd_solver.cpp:112] Iteration 70210, lr = 0.01
I0523 07:19:56.218219 34682 solver.cpp:239] Iteration 70220 (2.35314 iter/s, 4.24965s/10 iters), loss = 7.25337
I0523 07:19:56.218256 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.25337 (* 1 = 7.25337 loss)
I0523 07:19:56.300711 34682 sgd_solver.cpp:112] Iteration 70220, lr = 0.01
I0523 07:20:02.375798 34682 solver.cpp:239] Iteration 70230 (1.62409 iter/s, 6.15729s/10 iters), loss = 8.03424
I0523 07:20:02.376111 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03424 (* 1 = 8.03424 loss)
I0523 07:20:02.442823 34682 sgd_solver.cpp:112] Iteration 70230, lr = 0.01
I0523 07:20:08.470155 34682 solver.cpp:239] Iteration 70240 (1.64158 iter/s, 6.09168s/10 iters), loss = 8.32365
I0523 07:20:08.470229 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32365 (* 1 = 8.32365 loss)
I0523 07:20:09.255648 34682 sgd_solver.cpp:112] Iteration 70240, lr = 0.01
I0523 07:20:14.887359 34682 solver.cpp:239] Iteration 70250 (1.55839 iter/s, 6.41687s/10 iters), loss = 7.76013
I0523 07:20:14.887413 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.76013 (* 1 = 7.76013 loss)
I0523 07:20:14.963126 34682 sgd_solver.cpp:112] Iteration 70250, lr = 0.01
I0523 07:20:20.288863 34682 solver.cpp:239] Iteration 70260 (1.85143 iter/s, 5.40122s/10 iters), loss = 8.31068
I0523 07:20:20.288918 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31068 (* 1 = 8.31068 loss)
I0523 07:20:20.346092 34682 sgd_solver.cpp:112] Iteration 70260, lr = 0.01
I0523 07:20:25.358764 34682 solver.cpp:239] Iteration 70270 (1.97254 iter/s, 5.06959s/10 iters), loss = 8.00403
I0523 07:20:25.358826 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00403 (* 1 = 8.00403 loss)
I0523 07:20:25.558068 34682 sgd_solver.cpp:112] Iteration 70270, lr = 0.01
I0523 07:20:30.872059 34682 solver.cpp:239] Iteration 70280 (1.81389 iter/s, 5.51301s/10 iters), loss = 7.94785
I0523 07:20:30.872117 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94785 (* 1 = 7.94785 loss)
I0523 07:20:31.649915 34682 sgd_solver.cpp:112] Iteration 70280, lr = 0.01
I0523 07:20:37.438026 34682 solver.cpp:239] Iteration 70290 (1.52308 iter/s, 6.56564s/10 iters), loss = 6.94746
I0523 07:20:37.438242 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.94746 (* 1 = 6.94746 loss)
I0523 07:20:37.845844 34682 sgd_solver.cpp:112] Iteration 70290, lr = 0.01
I0523 07:20:42.817214 34682 solver.cpp:239] Iteration 70300 (1.85916 iter/s, 5.37877s/10 iters), loss = 8.89768
I0523 07:20:42.817260 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89768 (* 1 = 8.89768 loss)
I0523 07:20:43.581111 34682 sgd_solver.cpp:112] Iteration 70300, lr = 0.01
I0523 07:20:47.860532 34682 solver.cpp:239] Iteration 70310 (1.98292 iter/s, 5.04306s/10 iters), loss = 8.03588
I0523 07:20:47.860577 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03588 (* 1 = 8.03588 loss)
I0523 07:20:47.927683 34682 sgd_solver.cpp:112] Iteration 70310, lr = 0.01
I0523 07:20:51.951263 34682 solver.cpp:239] Iteration 70320 (2.44468 iter/s, 4.09051s/10 iters), loss = 8.52338
I0523 07:20:51.951309 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52338 (* 1 = 8.52338 loss)
I0523 07:20:52.807066 34682 sgd_solver.cpp:112] Iteration 70320, lr = 0.01
I0523 07:20:56.199023 34682 solver.cpp:239] Iteration 70330 (2.35431 iter/s, 4.24754s/10 iters), loss = 7.55263
I0523 07:20:56.199070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.55263 (* 1 = 7.55263 loss)
I0523 07:20:56.827258 34682 sgd_solver.cpp:112] Iteration 70330, lr = 0.01
I0523 07:21:02.456652 34682 solver.cpp:239] Iteration 70340 (1.59926 iter/s, 6.2529s/10 iters), loss = 7.79969
I0523 07:21:02.456698 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79969 (* 1 = 7.79969 loss)
I0523 07:21:03.254170 34682 sgd_solver.cpp:112] Iteration 70340, lr = 0.01
I0523 07:21:07.901091 34682 solver.cpp:239] Iteration 70350 (1.83683 iter/s, 5.44417s/10 iters), loss = 7.61919
I0523 07:21:07.901329 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.61919 (* 1 = 7.61919 loss)
I0523 07:21:07.976197 34682 sgd_solver.cpp:112] Iteration 70350, lr = 0.01
I0523 07:21:11.226750 34682 solver.cpp:239] Iteration 70360 (3.01124 iter/s, 3.32089s/10 iters), loss = 8.39874
I0523 07:21:11.226804 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39874 (* 1 = 8.39874 loss)
I0523 07:21:12.076591 34682 sgd_solver.cpp:112] Iteration 70360, lr = 0.01
I0523 07:21:16.868400 34682 solver.cpp:239] Iteration 70370 (1.77262 iter/s, 5.64136s/10 iters), loss = 8.30547
I0523 07:21:16.868468 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30547 (* 1 = 8.30547 loss)
I0523 07:21:17.652726 34682 sgd_solver.cpp:112] Iteration 70370, lr = 0.01
I0523 07:21:22.521515 34682 solver.cpp:239] Iteration 70380 (1.76903 iter/s, 5.65282s/10 iters), loss = 8.06048
I0523 07:21:22.521560 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06048 (* 1 = 8.06048 loss)
I0523 07:21:22.598407 34682 sgd_solver.cpp:112] Iteration 70380, lr = 0.01
I0523 07:21:26.770947 34682 solver.cpp:239] Iteration 70390 (2.35338 iter/s, 4.2492s/10 iters), loss = 8.4717
I0523 07:21:26.770995 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4717 (* 1 = 8.4717 loss)
I0523 07:21:26.834899 34682 sgd_solver.cpp:112] Iteration 70390, lr = 0.01
I0523 07:21:32.169256 34682 solver.cpp:239] Iteration 70400 (1.85252 iter/s, 5.39804s/10 iters), loss = 8.26976
I0523 07:21:32.169299 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26976 (* 1 = 8.26976 loss)
I0523 07:21:32.248172 34682 sgd_solver.cpp:112] Iteration 70400, lr = 0.01
I0523 07:21:37.843070 34682 solver.cpp:239] Iteration 70410 (1.76257 iter/s, 5.67354s/10 iters), loss = 8.87079
I0523 07:21:37.843127 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.87079 (* 1 = 8.87079 loss)
I0523 07:21:37.903164 34682 sgd_solver.cpp:112] Iteration 70410, lr = 0.01
I0523 07:21:43.407352 34682 solver.cpp:239] Iteration 70420 (1.79727 iter/s, 5.564s/10 iters), loss = 8.24968
I0523 07:21:43.407402 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24968 (* 1 = 8.24968 loss)
I0523 07:21:43.476060 34682 sgd_solver.cpp:112] Iteration 70420, lr = 0.01
I0523 07:21:47.103411 34682 solver.cpp:239] Iteration 70430 (2.70573 iter/s, 3.69586s/10 iters), loss = 8.03144
I0523 07:21:47.103474 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03144 (* 1 = 8.03144 loss)
I0523 07:21:47.777448 34682 sgd_solver.cpp:112] Iteration 70430, lr = 0.01
I0523 07:21:51.721374 34682 solver.cpp:239] Iteration 70440 (2.16558 iter/s, 4.61771s/10 iters), loss = 7.96447
I0523 07:21:51.721415 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96447 (* 1 = 7.96447 loss)
I0523 07:21:52.587976 34682 sgd_solver.cpp:112] Iteration 70440, lr = 0.01
I0523 07:21:55.802901 34682 solver.cpp:239] Iteration 70450 (2.45019 iter/s, 4.08132s/10 iters), loss = 8.05551
I0523 07:21:55.802954 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05551 (* 1 = 8.05551 loss)
I0523 07:21:55.875771 34682 sgd_solver.cpp:112] Iteration 70450, lr = 0.01
I0523 07:22:01.486860 34682 solver.cpp:239] Iteration 70460 (1.75943 iter/s, 5.68367s/10 iters), loss = 8.0592
I0523 07:22:01.486910 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0592 (* 1 = 8.0592 loss)
I0523 07:22:01.686138 34682 sgd_solver.cpp:112] Iteration 70460, lr = 0.01
I0523 07:22:06.066690 34682 solver.cpp:239] Iteration 70470 (2.1836 iter/s, 4.57958s/10 iters), loss = 8.72503
I0523 07:22:06.066766 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72503 (* 1 = 8.72503 loss)
I0523 07:22:06.706025 34682 sgd_solver.cpp:112] Iteration 70470, lr = 0.01
I0523 07:22:10.512640 34682 solver.cpp:239] Iteration 70480 (2.24937 iter/s, 4.44569s/10 iters), loss = 7.62859
I0523 07:22:10.512884 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62859 (* 1 = 7.62859 loss)
I0523 07:22:10.771415 34682 sgd_solver.cpp:112] Iteration 70480, lr = 0.01
I0523 07:22:14.186331 34682 solver.cpp:239] Iteration 70490 (2.72233 iter/s, 3.67332s/10 iters), loss = 8.78367
I0523 07:22:14.186372 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78367 (* 1 = 8.78367 loss)
I0523 07:22:14.267966 34682 sgd_solver.cpp:112] Iteration 70490, lr = 0.01
I0523 07:22:18.436730 34682 solver.cpp:239] Iteration 70500 (2.35284 iter/s, 4.25018s/10 iters), loss = 8.03834
I0523 07:22:18.436779 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03834 (* 1 = 8.03834 loss)
I0523 07:22:18.505564 34682 sgd_solver.cpp:112] Iteration 70500, lr = 0.01
I0523 07:22:21.842731 34682 solver.cpp:239] Iteration 70510 (2.93616 iter/s, 3.40581s/10 iters), loss = 8.09134
I0523 07:22:21.842774 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09134 (* 1 = 8.09134 loss)
I0523 07:22:22.120610 34682 sgd_solver.cpp:112] Iteration 70510, lr = 0.01
I0523 07:22:26.976388 34682 solver.cpp:239] Iteration 70520 (1.94803 iter/s, 5.1334s/10 iters), loss = 9.03058
I0523 07:22:26.976440 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03058 (* 1 = 9.03058 loss)
I0523 07:22:27.048800 34682 sgd_solver.cpp:112] Iteration 70520, lr = 0.01
I0523 07:22:30.620820 34682 solver.cpp:239] Iteration 70530 (2.74408 iter/s, 3.64421s/10 iters), loss = 7.66625
I0523 07:22:30.620884 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66625 (* 1 = 7.66625 loss)
I0523 07:22:31.385699 34682 sgd_solver.cpp:112] Iteration 70530, lr = 0.01
I0523 07:22:35.496426 34682 solver.cpp:239] Iteration 70540 (2.05114 iter/s, 4.87535s/10 iters), loss = 7.9561
I0523 07:22:35.496481 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9561 (* 1 = 7.9561 loss)
I0523 07:22:35.570724 34682 sgd_solver.cpp:112] Iteration 70540, lr = 0.01
I0523 07:22:39.178517 34682 solver.cpp:239] Iteration 70550 (2.716 iter/s, 3.68188s/10 iters), loss = 8.04047
I0523 07:22:39.178562 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04047 (* 1 = 8.04047 loss)
I0523 07:22:39.250958 34682 sgd_solver.cpp:112] Iteration 70550, lr = 0.01
I0523 07:22:44.840189 34682 solver.cpp:239] Iteration 70560 (1.76635 iter/s, 5.6614s/10 iters), loss = 7.62906
I0523 07:22:44.840386 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62906 (* 1 = 7.62906 loss)
I0523 07:22:44.904603 34682 sgd_solver.cpp:112] Iteration 70560, lr = 0.01
I0523 07:22:49.981230 34682 solver.cpp:239] Iteration 70570 (1.94694 iter/s, 5.13627s/10 iters), loss = 7.79093
I0523 07:22:49.981281 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79093 (* 1 = 7.79093 loss)
I0523 07:22:50.808883 34682 sgd_solver.cpp:112] Iteration 70570, lr = 0.01
I0523 07:22:53.988997 34682 solver.cpp:239] Iteration 70580 (2.49529 iter/s, 4.00755s/10 iters), loss = 7.86766
I0523 07:22:53.989044 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86766 (* 1 = 7.86766 loss)
I0523 07:22:54.049507 34682 sgd_solver.cpp:112] Iteration 70580, lr = 0.01
I0523 07:22:58.812561 34682 solver.cpp:239] Iteration 70590 (2.07326 iter/s, 4.82331s/10 iters), loss = 7.36463
I0523 07:22:58.812608 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.36463 (* 1 = 7.36463 loss)
I0523 07:22:59.605283 34682 sgd_solver.cpp:112] Iteration 70590, lr = 0.01
I0523 07:23:03.470191 34682 solver.cpp:239] Iteration 70600 (2.14714 iter/s, 4.65735s/10 iters), loss = 8.3298
I0523 07:23:03.470252 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3298 (* 1 = 8.3298 loss)
I0523 07:23:03.543151 34682 sgd_solver.cpp:112] Iteration 70600, lr = 0.01
I0523 07:23:08.573336 34682 solver.cpp:239] Iteration 70610 (1.95968 iter/s, 5.10286s/10 iters), loss = 7.71854
I0523 07:23:08.573390 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.71854 (* 1 = 7.71854 loss)
I0523 07:23:08.637758 34682 sgd_solver.cpp:112] Iteration 70610, lr = 0.01
I0523 07:23:13.190387 34682 solver.cpp:239] Iteration 70620 (2.166 iter/s, 4.6168s/10 iters), loss = 7.89901
I0523 07:23:13.190429 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89901 (* 1 = 7.89901 loss)
I0523 07:23:13.270459 34682 sgd_solver.cpp:112] Iteration 70620, lr = 0.01
I0523 07:23:18.645133 34682 solver.cpp:239] Iteration 70630 (1.83336 iter/s, 5.45448s/10 iters), loss = 7.83149
I0523 07:23:18.645352 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83149 (* 1 = 7.83149 loss)
I0523 07:23:19.319885 34682 sgd_solver.cpp:112] Iteration 70630, lr = 0.01
I0523 07:23:24.855407 34682 solver.cpp:239] Iteration 70640 (1.61035 iter/s, 6.20981s/10 iters), loss = 6.90493
I0523 07:23:24.855481 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.90493 (* 1 = 6.90493 loss)
I0523 07:23:25.674170 34682 sgd_solver.cpp:112] Iteration 70640, lr = 0.01
I0523 07:23:31.394619 34682 solver.cpp:239] Iteration 70650 (1.52931 iter/s, 6.53888s/10 iters), loss = 7.96687
I0523 07:23:31.394676 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96687 (* 1 = 7.96687 loss)
I0523 07:23:32.150826 34682 sgd_solver.cpp:112] Iteration 70650, lr = 0.01
I0523 07:23:38.551672 34682 solver.cpp:239] Iteration 70660 (1.39729 iter/s, 7.15671s/10 iters), loss = 7.63852
I0523 07:23:38.551733 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63852 (* 1 = 7.63852 loss)
I0523 07:23:38.623355 34682 sgd_solver.cpp:112] Iteration 70660, lr = 0.01
I0523 07:23:43.355365 34682 solver.cpp:239] Iteration 70670 (2.08184 iter/s, 4.80344s/10 iters), loss = 8.80755
I0523 07:23:43.355412 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80755 (* 1 = 8.80755 loss)
I0523 07:23:43.422127 34682 sgd_solver.cpp:112] Iteration 70670, lr = 0.01
I0523 07:23:48.341269 34682 solver.cpp:239] Iteration 70680 (2.00576 iter/s, 4.98565s/10 iters), loss = 7.88247
I0523 07:23:48.341305 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88247 (* 1 = 7.88247 loss)
I0523 07:23:48.410620 34682 sgd_solver.cpp:112] Iteration 70680, lr = 0.01
I0523 07:23:52.667619 34682 solver.cpp:239] Iteration 70690 (2.31153 iter/s, 4.32613s/10 iters), loss = 7.88295
I0523 07:23:52.667809 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88295 (* 1 = 7.88295 loss)
I0523 07:23:52.727636 34682 sgd_solver.cpp:112] Iteration 70690, lr = 0.01
I0523 07:23:56.288151 34682 solver.cpp:239] Iteration 70700 (2.76228 iter/s, 3.6202s/10 iters), loss = 7.20538
I0523 07:23:56.288197 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.20538 (* 1 = 7.20538 loss)
I0523 07:23:57.109437 34682 sgd_solver.cpp:112] Iteration 70700, lr = 0.01
I0523 07:24:01.907852 34682 solver.cpp:239] Iteration 70710 (1.77955 iter/s, 5.61941s/10 iters), loss = 8.3012
I0523 07:24:01.907908 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3012 (* 1 = 8.3012 loss)
I0523 07:24:01.966544 34682 sgd_solver.cpp:112] Iteration 70710, lr = 0.01
I0523 07:24:06.896955 34682 solver.cpp:239] Iteration 70720 (2.00447 iter/s, 4.98884s/10 iters), loss = 7.82087
I0523 07:24:06.897004 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82087 (* 1 = 7.82087 loss)
I0523 07:24:07.614466 34682 sgd_solver.cpp:112] Iteration 70720, lr = 0.01
I0523 07:24:13.293028 34682 solver.cpp:239] Iteration 70730 (1.56354 iter/s, 6.39575s/10 iters), loss = 8.13707
I0523 07:24:13.293105 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13707 (* 1 = 8.13707 loss)
I0523 07:24:13.442384 34682 sgd_solver.cpp:112] Iteration 70730, lr = 0.01
I0523 07:24:18.774060 34682 solver.cpp:239] Iteration 70740 (1.82457 iter/s, 5.48074s/10 iters), loss = 8.62076
I0523 07:24:18.774108 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.62076 (* 1 = 8.62076 loss)
I0523 07:24:18.855161 34682 sgd_solver.cpp:112] Iteration 70740, lr = 0.01
I0523 07:24:23.116770 34682 solver.cpp:239] Iteration 70750 (2.30283 iter/s, 4.34248s/10 iters), loss = 7.55158
I0523 07:24:23.117034 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.55158 (* 1 = 7.55158 loss)
I0523 07:24:23.179762 34682 sgd_solver.cpp:112] Iteration 70750, lr = 0.01
I0523 07:24:27.982602 34682 solver.cpp:239] Iteration 70760 (2.05534 iter/s, 4.86537s/10 iters), loss = 8.06452
I0523 07:24:27.982683 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06452 (* 1 = 8.06452 loss)
I0523 07:24:28.725474 34682 sgd_solver.cpp:112] Iteration 70760, lr = 0.01
I0523 07:24:33.910441 34682 solver.cpp:239] Iteration 70770 (1.68705 iter/s, 5.92752s/10 iters), loss = 7.50811
I0523 07:24:33.910493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50811 (* 1 = 7.50811 loss)
I0523 07:24:33.978452 34682 sgd_solver.cpp:112] Iteration 70770, lr = 0.01
I0523 07:24:38.842144 34682 solver.cpp:239] Iteration 70780 (2.0278 iter/s, 4.93144s/10 iters), loss = 8.05296
I0523 07:24:38.842190 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05296 (* 1 = 8.05296 loss)
I0523 07:24:39.589906 34682 sgd_solver.cpp:112] Iteration 70780, lr = 0.01
I0523 07:24:45.229961 34682 solver.cpp:239] Iteration 70790 (1.56555 iter/s, 6.38752s/10 iters), loss = 7.38381
I0523 07:24:45.230005 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.38381 (* 1 = 7.38381 loss)
I0523 07:24:45.298224 34682 sgd_solver.cpp:112] Iteration 70790, lr = 0.01
I0523 07:24:49.967573 34682 solver.cpp:239] Iteration 70800 (2.11088 iter/s, 4.73737s/10 iters), loss = 7.60459
I0523 07:24:49.967631 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.60459 (* 1 = 7.60459 loss)
I0523 07:24:50.361793 34682 sgd_solver.cpp:112] Iteration 70800, lr = 0.01
I0523 07:24:53.722424 34682 solver.cpp:239] Iteration 70810 (2.66338 iter/s, 3.75463s/10 iters), loss = 7.62107
I0523 07:24:53.722645 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62107 (* 1 = 7.62107 loss)
I0523 07:24:53.797569 34682 sgd_solver.cpp:112] Iteration 70810, lr = 0.01
I0523 07:24:58.249135 34682 solver.cpp:239] Iteration 70820 (2.2093 iter/s, 4.52632s/10 iters), loss = 7.96852
I0523 07:24:58.249181 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96852 (* 1 = 7.96852 loss)
I0523 07:24:58.941810 34682 sgd_solver.cpp:112] Iteration 70820, lr = 0.01
I0523 07:25:02.737001 34682 solver.cpp:239] Iteration 70830 (2.22835 iter/s, 4.48763s/10 iters), loss = 7.38235
I0523 07:25:02.737056 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.38235 (* 1 = 7.38235 loss)
I0523 07:25:03.569262 34682 sgd_solver.cpp:112] Iteration 70830, lr = 0.01
I0523 07:25:06.665774 34682 solver.cpp:239] Iteration 70840 (2.54546 iter/s, 3.92856s/10 iters), loss = 8.13755
I0523 07:25:06.665818 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13755 (* 1 = 8.13755 loss)
I0523 07:25:06.737635 34682 sgd_solver.cpp:112] Iteration 70840, lr = 0.01
I0523 07:25:12.683229 34682 solver.cpp:239] Iteration 70850 (1.66191 iter/s, 6.01716s/10 iters), loss = 7.66638
I0523 07:25:12.683313 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66638 (* 1 = 7.66638 loss)
I0523 07:25:13.476783 34682 sgd_solver.cpp:112] Iteration 70850, lr = 0.01
I0523 07:25:17.705834 34682 solver.cpp:239] Iteration 70860 (1.99111 iter/s, 5.02232s/10 iters), loss = 8.38259
I0523 07:25:17.705870 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38259 (* 1 = 8.38259 loss)
I0523 07:25:17.777664 34682 sgd_solver.cpp:112] Iteration 70860, lr = 0.01
I0523 07:25:22.560427 34682 solver.cpp:239] Iteration 70870 (2.06001 iter/s, 4.85435s/10 iters), loss = 7.99293
I0523 07:25:22.560472 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99293 (* 1 = 7.99293 loss)
I0523 07:25:22.628449 34682 sgd_solver.cpp:112] Iteration 70870, lr = 0.01
I0523 07:25:26.096076 34682 solver.cpp:239] Iteration 70880 (2.8285 iter/s, 3.53544s/10 iters), loss = 7.57834
I0523 07:25:26.096371 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.57834 (* 1 = 7.57834 loss)
I0523 07:25:26.858039 34682 sgd_solver.cpp:112] Iteration 70880, lr = 0.01
I0523 07:25:30.238775 34682 solver.cpp:239] Iteration 70890 (2.41414 iter/s, 4.14226s/10 iters), loss = 8.61967
I0523 07:25:30.238824 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61967 (* 1 = 8.61967 loss)
I0523 07:25:30.300698 34682 sgd_solver.cpp:112] Iteration 70890, lr = 0.01
I0523 07:25:34.294945 34682 solver.cpp:239] Iteration 70900 (2.46551 iter/s, 4.05595s/10 iters), loss = 7.27786
I0523 07:25:34.294989 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.27786 (* 1 = 7.27786 loss)
I0523 07:25:34.366647 34682 sgd_solver.cpp:112] Iteration 70900, lr = 0.01
I0523 07:25:38.422062 34682 solver.cpp:239] Iteration 70910 (2.42313 iter/s, 4.1269s/10 iters), loss = 8.05261
I0523 07:25:38.422106 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05261 (* 1 = 8.05261 loss)
I0523 07:25:38.485404 34682 sgd_solver.cpp:112] Iteration 70910, lr = 0.01
I0523 07:25:44.920336 34682 solver.cpp:239] Iteration 70920 (1.53894 iter/s, 6.49797s/10 iters), loss = 8.2092
I0523 07:25:44.920385 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2092 (* 1 = 8.2092 loss)
I0523 07:25:45.767027 34682 sgd_solver.cpp:112] Iteration 70920, lr = 0.01
I0523 07:25:51.402905 34682 solver.cpp:239] Iteration 70930 (1.54267 iter/s, 6.48226s/10 iters), loss = 6.97773
I0523 07:25:51.402954 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.97773 (* 1 = 6.97773 loss)
I0523 07:25:52.087560 34682 sgd_solver.cpp:112] Iteration 70930, lr = 0.01
I0523 07:25:55.267179 34682 solver.cpp:239] Iteration 70940 (2.58795 iter/s, 3.86406s/10 iters), loss = 8.36346
I0523 07:25:55.267231 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36346 (* 1 = 8.36346 loss)
I0523 07:25:55.339208 34682 sgd_solver.cpp:112] Iteration 70940, lr = 0.01
I0523 07:26:01.032595 34682 solver.cpp:239] Iteration 70950 (1.73457 iter/s, 5.76513s/10 iters), loss = 8.80449
I0523 07:26:01.032723 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80449 (* 1 = 8.80449 loss)
I0523 07:26:01.752724 34682 sgd_solver.cpp:112] Iteration 70950, lr = 0.01
I0523 07:26:05.820264 34682 solver.cpp:239] Iteration 70960 (2.08884 iter/s, 4.78735s/10 iters), loss = 8.11879
I0523 07:26:05.820317 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11879 (* 1 = 8.11879 loss)
I0523 07:26:05.891621 34682 sgd_solver.cpp:112] Iteration 70960, lr = 0.01
I0523 07:26:10.005386 34682 solver.cpp:239] Iteration 70970 (2.38955 iter/s, 4.18489s/10 iters), loss = 7.43562
I0523 07:26:10.005430 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43562 (* 1 = 7.43562 loss)
I0523 07:26:10.079128 34682 sgd_solver.cpp:112] Iteration 70970, lr = 0.01
I0523 07:26:14.698493 34682 solver.cpp:239] Iteration 70980 (2.13089 iter/s, 4.69287s/10 iters), loss = 7.99344
I0523 07:26:14.698546 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99344 (* 1 = 7.99344 loss)
I0523 07:26:14.756907 34682 sgd_solver.cpp:112] Iteration 70980, lr = 0.01
I0523 07:26:20.570982 34682 solver.cpp:239] Iteration 70990 (1.70295 iter/s, 5.87218s/10 iters), loss = 6.90357
I0523 07:26:20.571048 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.90357 (* 1 = 6.90357 loss)
I0523 07:26:21.218677 34682 sgd_solver.cpp:112] Iteration 70990, lr = 0.01
I0523 07:26:25.394600 34682 solver.cpp:239] Iteration 71000 (2.07325 iter/s, 4.82335s/10 iters), loss = 8.23561
I0523 07:26:25.394660 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23561 (* 1 = 8.23561 loss)
I0523 07:26:26.089946 34682 sgd_solver.cpp:112] Iteration 71000, lr = 0.01
I0523 07:26:30.220965 34682 solver.cpp:239] Iteration 71010 (2.07206 iter/s, 4.82611s/10 iters), loss = 7.54263
I0523 07:26:30.221025 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54263 (* 1 = 7.54263 loss)
I0523 07:26:30.290977 34682 sgd_solver.cpp:112] Iteration 71010, lr = 0.01
I0523 07:26:34.415019 34682 solver.cpp:239] Iteration 71020 (2.38445 iter/s, 4.19383s/10 iters), loss = 8.04978
I0523 07:26:34.415261 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04978 (* 1 = 8.04978 loss)
I0523 07:26:34.484886 34682 sgd_solver.cpp:112] Iteration 71020, lr = 0.01
I0523 07:26:38.127842 34682 solver.cpp:239] Iteration 71030 (2.6969 iter/s, 3.70796s/10 iters), loss = 7.09689
I0523 07:26:38.127902 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.09689 (* 1 = 7.09689 loss)
I0523 07:26:38.979504 34682 sgd_solver.cpp:112] Iteration 71030, lr = 0.01
I0523 07:26:44.840517 34682 solver.cpp:239] Iteration 71040 (1.48979 iter/s, 6.71234s/10 iters), loss = 7.88101
I0523 07:26:44.840569 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88101 (* 1 = 7.88101 loss)
I0523 07:26:44.917138 34682 sgd_solver.cpp:112] Iteration 71040, lr = 0.01
I0523 07:26:48.447664 34682 solver.cpp:239] Iteration 71050 (2.77243 iter/s, 3.60694s/10 iters), loss = 7.51474
I0523 07:26:48.447711 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51474 (* 1 = 7.51474 loss)
I0523 07:26:48.529253 34682 sgd_solver.cpp:112] Iteration 71050, lr = 0.01
I0523 07:26:55.093647 34682 solver.cpp:239] Iteration 71060 (1.50474 iter/s, 6.64566s/10 iters), loss = 7.26169
I0523 07:26:55.093703 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.26169 (* 1 = 7.26169 loss)
I0523 07:26:55.874680 34682 sgd_solver.cpp:112] Iteration 71060, lr = 0.01
I0523 07:27:01.459868 34682 solver.cpp:239] Iteration 71070 (1.57087 iter/s, 6.36591s/10 iters), loss = 8.07307
I0523 07:27:01.459908 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07307 (* 1 = 8.07307 loss)
I0523 07:27:01.541880 34682 sgd_solver.cpp:112] Iteration 71070, lr = 0.01
I0523 07:27:06.813814 34682 solver.cpp:239] Iteration 71080 (1.86787 iter/s, 5.35369s/10 iters), loss = 7.1033
I0523 07:27:06.814061 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.1033 (* 1 = 7.1033 loss)
I0523 07:27:06.887786 34682 sgd_solver.cpp:112] Iteration 71080, lr = 0.01
I0523 07:27:10.202397 34682 solver.cpp:239] Iteration 71090 (2.95139 iter/s, 3.38824s/10 iters), loss = 7.80588
I0523 07:27:10.202440 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80588 (* 1 = 7.80588 loss)
I0523 07:27:10.277817 34682 sgd_solver.cpp:112] Iteration 71090, lr = 0.01
I0523 07:27:15.149085 34682 solver.cpp:239] Iteration 71100 (2.02166 iter/s, 4.94643s/10 iters), loss = 8.67449
I0523 07:27:15.149139 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67449 (* 1 = 8.67449 loss)
I0523 07:27:15.222098 34682 sgd_solver.cpp:112] Iteration 71100, lr = 0.01
I0523 07:27:19.578096 34682 solver.cpp:239] Iteration 71110 (2.25796 iter/s, 4.42878s/10 iters), loss = 7.51641
I0523 07:27:19.578137 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51641 (* 1 = 7.51641 loss)
I0523 07:27:19.787789 34682 sgd_solver.cpp:112] Iteration 71110, lr = 0.01
I0523 07:27:26.092331 34682 solver.cpp:239] Iteration 71120 (1.53517 iter/s, 6.51392s/10 iters), loss = 7.56447
I0523 07:27:26.092386 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56447 (* 1 = 7.56447 loss)
I0523 07:27:26.953912 34682 sgd_solver.cpp:112] Iteration 71120, lr = 0.01
I0523 07:27:31.284610 34682 solver.cpp:239] Iteration 71130 (1.92604 iter/s, 5.19201s/10 iters), loss = 7.63134
I0523 07:27:31.284649 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63134 (* 1 = 7.63134 loss)
I0523 07:27:31.350770 34682 sgd_solver.cpp:112] Iteration 71130, lr = 0.01
I0523 07:27:36.007689 34682 solver.cpp:239] Iteration 71140 (2.11737 iter/s, 4.72285s/10 iters), loss = 8.06007
I0523 07:27:36.007732 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06007 (* 1 = 8.06007 loss)
I0523 07:27:36.078668 34682 sgd_solver.cpp:112] Iteration 71140, lr = 0.01
I0523 07:27:41.822921 34682 solver.cpp:239] Iteration 71150 (1.72074 iter/s, 5.81144s/10 iters), loss = 7.84002
I0523 07:27:41.823110 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84002 (* 1 = 7.84002 loss)
I0523 07:27:42.678968 34682 sgd_solver.cpp:112] Iteration 71150, lr = 0.01
I0523 07:27:47.418298 34682 solver.cpp:239] Iteration 71160 (1.78732 iter/s, 5.59496s/10 iters), loss = 8.37234
I0523 07:27:47.418349 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37234 (* 1 = 8.37234 loss)
I0523 07:27:47.484639 34682 sgd_solver.cpp:112] Iteration 71160, lr = 0.01
I0523 07:27:51.658581 34682 solver.cpp:239] Iteration 71170 (2.35846 iter/s, 4.24005s/10 iters), loss = 7.90167
I0523 07:27:51.658658 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90167 (* 1 = 7.90167 loss)
I0523 07:27:51.728806 34682 sgd_solver.cpp:112] Iteration 71170, lr = 0.01
I0523 07:27:56.442771 34682 solver.cpp:239] Iteration 71180 (2.09034 iter/s, 4.78392s/10 iters), loss = 8.38321
I0523 07:27:56.442819 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38321 (* 1 = 8.38321 loss)
I0523 07:27:56.505995 34682 sgd_solver.cpp:112] Iteration 71180, lr = 0.01
I0523 07:28:01.904625 34682 solver.cpp:239] Iteration 71190 (1.83097 iter/s, 5.46159s/10 iters), loss = 8.34361
I0523 07:28:01.904670 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34361 (* 1 = 8.34361 loss)
I0523 07:28:01.965492 34682 sgd_solver.cpp:112] Iteration 71190, lr = 0.01
I0523 07:28:07.742877 34682 solver.cpp:239] Iteration 71200 (1.71292 iter/s, 5.83797s/10 iters), loss = 7.4512
I0523 07:28:07.742920 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4512 (* 1 = 7.4512 loss)
I0523 07:28:08.612452 34682 sgd_solver.cpp:112] Iteration 71200, lr = 0.01
I0523 07:28:14.339098 34682 solver.cpp:239] Iteration 71210 (1.51609 iter/s, 6.59591s/10 iters), loss = 8.29489
I0523 07:28:14.339318 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29489 (* 1 = 8.29489 loss)
I0523 07:28:14.402405 34682 sgd_solver.cpp:112] Iteration 71210, lr = 0.01
I0523 07:28:19.107432 34682 solver.cpp:239] Iteration 71220 (2.09735 iter/s, 4.76793s/10 iters), loss = 8.19434
I0523 07:28:19.107481 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19434 (* 1 = 8.19434 loss)
I0523 07:28:19.190315 34682 sgd_solver.cpp:112] Iteration 71220, lr = 0.01
I0523 07:28:24.676221 34682 solver.cpp:239] Iteration 71230 (1.79581 iter/s, 5.56851s/10 iters), loss = 8.00343
I0523 07:28:24.676278 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00343 (* 1 = 8.00343 loss)
I0523 07:28:25.529379 34682 sgd_solver.cpp:112] Iteration 71230, lr = 0.01
I0523 07:28:30.407022 34682 solver.cpp:239] Iteration 71240 (1.74504 iter/s, 5.73051s/10 iters), loss = 7.50401
I0523 07:28:30.407060 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50401 (* 1 = 7.50401 loss)
I0523 07:28:30.483064 34682 sgd_solver.cpp:112] Iteration 71240, lr = 0.01
I0523 07:28:35.408098 34682 solver.cpp:239] Iteration 71250 (1.99967 iter/s, 5.00083s/10 iters), loss = 7.12295
I0523 07:28:35.408152 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.12295 (* 1 = 7.12295 loss)
I0523 07:28:35.463973 34682 sgd_solver.cpp:112] Iteration 71250, lr = 0.01
I0523 07:28:39.417387 34682 solver.cpp:239] Iteration 71260 (2.49435 iter/s, 4.00907s/10 iters), loss = 7.56255
I0523 07:28:39.417428 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56255 (* 1 = 7.56255 loss)
I0523 07:28:39.490892 34682 sgd_solver.cpp:112] Iteration 71260, lr = 0.01
I0523 07:28:45.132688 34682 solver.cpp:239] Iteration 71270 (1.74978 iter/s, 5.71502s/10 iters), loss = 8.52502
I0523 07:28:45.132812 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52502 (* 1 = 8.52502 loss)
I0523 07:28:45.966979 34682 sgd_solver.cpp:112] Iteration 71270, lr = 0.01
I0523 07:28:49.975774 34682 solver.cpp:239] Iteration 71280 (2.06493 iter/s, 4.84277s/10 iters), loss = 8.17189
I0523 07:28:49.975841 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17189 (* 1 = 8.17189 loss)
I0523 07:28:50.036998 34682 sgd_solver.cpp:112] Iteration 71280, lr = 0.01
I0523 07:28:53.926815 34682 solver.cpp:239] Iteration 71290 (2.53112 iter/s, 3.95082s/10 iters), loss = 7.39821
I0523 07:28:53.926857 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.39821 (* 1 = 7.39821 loss)
I0523 07:28:54.003468 34682 sgd_solver.cpp:112] Iteration 71290, lr = 0.01
I0523 07:28:57.443207 34682 solver.cpp:239] Iteration 71300 (2.84398 iter/s, 3.5162s/10 iters), loss = 7.66269
I0523 07:28:57.443259 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66269 (* 1 = 7.66269 loss)
I0523 07:28:58.273005 34682 sgd_solver.cpp:112] Iteration 71300, lr = 0.01
I0523 07:29:02.275506 34682 solver.cpp:239] Iteration 71310 (2.06952 iter/s, 4.83204s/10 iters), loss = 8.18799
I0523 07:29:02.275566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18799 (* 1 = 8.18799 loss)
I0523 07:29:02.634254 34682 sgd_solver.cpp:112] Iteration 71310, lr = 0.01
I0523 07:29:06.804476 34682 solver.cpp:239] Iteration 71320 (2.20813 iter/s, 4.52872s/10 iters), loss = 7.37881
I0523 07:29:06.804543 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.37881 (* 1 = 7.37881 loss)
I0523 07:29:06.869619 34682 sgd_solver.cpp:112] Iteration 71320, lr = 0.01
I0523 07:29:10.461861 34682 solver.cpp:239] Iteration 71330 (2.73436 iter/s, 3.65716s/10 iters), loss = 8.52562
I0523 07:29:10.461910 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52562 (* 1 = 8.52562 loss)
I0523 07:29:10.521354 34682 sgd_solver.cpp:112] Iteration 71330, lr = 0.01
I0523 07:29:15.153482 34682 solver.cpp:239] Iteration 71340 (2.13157 iter/s, 4.69138s/10 iters), loss = 7.47062
I0523 07:29:15.153743 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.47062 (* 1 = 7.47062 loss)
I0523 07:29:15.229063 34682 sgd_solver.cpp:112] Iteration 71340, lr = 0.01
I0523 07:29:19.586565 34682 solver.cpp:239] Iteration 71350 (2.25599 iter/s, 4.43265s/10 iters), loss = 7.08192
I0523 07:29:19.586622 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.08192 (* 1 = 7.08192 loss)
I0523 07:29:20.469419 34682 sgd_solver.cpp:112] Iteration 71350, lr = 0.01
I0523 07:29:25.131283 34682 solver.cpp:239] Iteration 71360 (1.80361 iter/s, 5.54444s/10 iters), loss = 7.44574
I0523 07:29:25.131332 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44574 (* 1 = 7.44574 loss)
I0523 07:29:25.182322 34682 sgd_solver.cpp:112] Iteration 71360, lr = 0.01
I0523 07:29:30.024319 34682 solver.cpp:239] Iteration 71370 (2.04549 iter/s, 4.88882s/10 iters), loss = 7.83633
I0523 07:29:30.024376 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83633 (* 1 = 7.83633 loss)
I0523 07:29:30.905539 34682 sgd_solver.cpp:112] Iteration 71370, lr = 0.01
I0523 07:29:35.726447 34682 solver.cpp:239] Iteration 71380 (1.75382 iter/s, 5.70184s/10 iters), loss = 7.69007
I0523 07:29:35.726491 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69007 (* 1 = 7.69007 loss)
I0523 07:29:35.779669 34682 sgd_solver.cpp:112] Iteration 71380, lr = 0.01
I0523 07:29:39.092744 34682 solver.cpp:239] Iteration 71390 (2.97079 iter/s, 3.36611s/10 iters), loss = 8.83606
I0523 07:29:39.092804 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.83606 (* 1 = 8.83606 loss)
I0523 07:29:39.163714 34682 sgd_solver.cpp:112] Iteration 71390, lr = 0.01
I0523 07:29:43.191932 34682 solver.cpp:239] Iteration 71400 (2.43964 iter/s, 4.09896s/10 iters), loss = 7.20461
I0523 07:29:43.191980 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.20461 (* 1 = 7.20461 loss)
I0523 07:29:43.953670 34682 sgd_solver.cpp:112] Iteration 71400, lr = 0.01
I0523 07:29:47.775348 34682 solver.cpp:239] Iteration 71410 (2.18189 iter/s, 4.58318s/10 iters), loss = 8.12881
I0523 07:29:47.775527 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12881 (* 1 = 8.12881 loss)
I0523 07:29:47.851219 34682 sgd_solver.cpp:112] Iteration 71410, lr = 0.01
I0523 07:29:54.387121 34682 solver.cpp:239] Iteration 71420 (1.51256 iter/s, 6.61133s/10 iters), loss = 7.51417
I0523 07:29:54.387177 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51417 (* 1 = 7.51417 loss)
I0523 07:29:54.463914 34682 sgd_solver.cpp:112] Iteration 71420, lr = 0.01
I0523 07:29:58.423655 34682 solver.cpp:239] Iteration 71430 (2.47751 iter/s, 4.03631s/10 iters), loss = 7.36607
I0523 07:29:58.423709 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.36607 (* 1 = 7.36607 loss)
I0523 07:29:59.240020 34682 sgd_solver.cpp:112] Iteration 71430, lr = 0.01
I0523 07:30:05.133337 34682 solver.cpp:239] Iteration 71440 (1.49046 iter/s, 6.70935s/10 iters), loss = 8.1119
I0523 07:30:05.133744 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1119 (* 1 = 8.1119 loss)
I0523 07:30:05.196463 34682 sgd_solver.cpp:112] Iteration 71440, lr = 0.01
I0523 07:30:10.599256 34682 solver.cpp:239] Iteration 71450 (1.83048 iter/s, 5.46304s/10 iters), loss = 7.85367
I0523 07:30:10.599306 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.85367 (* 1 = 7.85367 loss)
I0523 07:30:10.675643 34682 sgd_solver.cpp:112] Iteration 71450, lr = 0.01
I0523 07:30:13.211890 34682 solver.cpp:239] Iteration 71460 (3.82781 iter/s, 2.61246s/10 iters), loss = 7.51591
I0523 07:30:13.211948 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51591 (* 1 = 7.51591 loss)
I0523 07:30:13.296125 34682 sgd_solver.cpp:112] Iteration 71460, lr = 0.01
I0523 07:30:17.799046 34682 solver.cpp:239] Iteration 71470 (2.18012 iter/s, 4.58691s/10 iters), loss = 7.42328
I0523 07:30:17.799248 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42328 (* 1 = 7.42328 loss)
I0523 07:30:17.866724 34682 sgd_solver.cpp:112] Iteration 71470, lr = 0.01
I0523 07:30:24.364430 34682 solver.cpp:239] Iteration 71480 (1.52325 iter/s, 6.56493s/10 iters), loss = 7.3742
I0523 07:30:24.364476 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.3742 (* 1 = 7.3742 loss)
I0523 07:30:24.444308 34682 sgd_solver.cpp:112] Iteration 71480, lr = 0.01
I0523 07:30:27.256851 34682 solver.cpp:239] Iteration 71490 (3.45751 iter/s, 2.89225s/10 iters), loss = 7.8725
I0523 07:30:27.256902 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8725 (* 1 = 7.8725 loss)
I0523 07:30:27.326737 34682 sgd_solver.cpp:112] Iteration 71490, lr = 0.01
I0523 07:30:32.673887 34682 solver.cpp:239] Iteration 71500 (1.84612 iter/s, 5.41676s/10 iters), loss = 8.35242
I0523 07:30:32.673941 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35242 (* 1 = 8.35242 loss)
I0523 07:30:33.496793 34682 sgd_solver.cpp:112] Iteration 71500, lr = 0.01
I0523 07:30:39.056550 34682 solver.cpp:239] Iteration 71510 (1.56682 iter/s, 6.38234s/10 iters), loss = 7.89997
I0523 07:30:39.056601 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89997 (* 1 = 7.89997 loss)
I0523 07:30:39.115218 34682 sgd_solver.cpp:112] Iteration 71510, lr = 0.01
I0523 07:30:42.896488 34682 solver.cpp:239] Iteration 71520 (2.60435 iter/s, 3.83973s/10 iters), loss = 7.44373
I0523 07:30:42.896535 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44373 (* 1 = 7.44373 loss)
I0523 07:30:42.959765 34682 sgd_solver.cpp:112] Iteration 71520, lr = 0.01
I0523 07:30:47.527333 34682 solver.cpp:239] Iteration 71530 (2.15955 iter/s, 4.6306s/10 iters), loss = 7.74189
I0523 07:30:47.527372 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74189 (* 1 = 7.74189 loss)
I0523 07:30:47.600543 34682 sgd_solver.cpp:112] Iteration 71530, lr = 0.01
I0523 07:30:52.331873 34682 solver.cpp:239] Iteration 71540 (2.08147 iter/s, 4.8043s/10 iters), loss = 7.35831
I0523 07:30:52.332080 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.35831 (* 1 = 7.35831 loss)
I0523 07:30:53.116960 34682 sgd_solver.cpp:112] Iteration 71540, lr = 0.01
I0523 07:30:57.681578 34682 solver.cpp:239] Iteration 71550 (1.8694 iter/s, 5.3493s/10 iters), loss = 7.06943
I0523 07:30:57.681623 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.06943 (* 1 = 7.06943 loss)
I0523 07:30:57.767899 34682 sgd_solver.cpp:112] Iteration 71550, lr = 0.01
I0523 07:30:59.959607 34682 solver.cpp:239] Iteration 71560 (4.39003 iter/s, 2.27789s/10 iters), loss = 8.53271
I0523 07:30:59.959651 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53271 (* 1 = 8.53271 loss)
I0523 07:31:00.084856 34682 sgd_solver.cpp:112] Iteration 71560, lr = 0.01
I0523 07:31:01.231021 34682 solver.cpp:239] Iteration 71570 (7.86589 iter/s, 1.27131s/10 iters), loss = 8.98176
I0523 07:31:01.231057 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.98176 (* 1 = 8.98176 loss)
I0523 07:31:01.276783 34682 sgd_solver.cpp:112] Iteration 71570, lr = 0.01
I0523 07:31:02.588610 34682 solver.cpp:239] Iteration 71580 (7.36659 iter/s, 1.35748s/10 iters), loss = 7.95673
I0523 07:31:02.588687 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95673 (* 1 = 7.95673 loss)
I0523 07:31:02.628744 34682 sgd_solver.cpp:112] Iteration 71580, lr = 0.01
I0523 07:31:04.125685 34682 solver.cpp:239] Iteration 71590 (6.50649 iter/s, 1.53693s/10 iters), loss = 8.18311
I0523 07:31:04.125736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18311 (* 1 = 8.18311 loss)
I0523 07:31:04.563381 34682 sgd_solver.cpp:112] Iteration 71590, lr = 0.01
I0523 07:31:06.121675 34682 solver.cpp:239] Iteration 71600 (5.01043 iter/s, 1.99584s/10 iters), loss = 7.25386
I0523 07:31:06.121745 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.25386 (* 1 = 7.25386 loss)
I0523 07:31:06.158713 34682 sgd_solver.cpp:112] Iteration 71600, lr = 0.01
I0523 07:31:07.698001 34682 solver.cpp:239] Iteration 71610 (6.34469 iter/s, 1.57612s/10 iters), loss = 7.49906
I0523 07:31:07.698060 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.49906 (* 1 = 7.49906 loss)
I0523 07:31:07.742713 34682 sgd_solver.cpp:112] Iteration 71610, lr = 0.01
I0523 07:31:08.897595 34682 solver.cpp:239] Iteration 71620 (8.33698 iter/s, 1.19947s/10 iters), loss = 8.7482
I0523 07:31:08.897635 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.7482 (* 1 = 8.7482 loss)
I0523 07:31:08.946043 34682 sgd_solver.cpp:112] Iteration 71620, lr = 0.01
I0523 07:31:10.511746 34682 solver.cpp:239] Iteration 71630 (6.19565 iter/s, 1.61404s/10 iters), loss = 8.01869
I0523 07:31:10.511795 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01869 (* 1 = 8.01869 loss)
I0523 07:31:10.557380 34682 sgd_solver.cpp:112] Iteration 71630, lr = 0.01
I0523 07:31:14.877928 34682 solver.cpp:239] Iteration 71640 (2.29045 iter/s, 4.36595s/10 iters), loss = 7.55336
I0523 07:31:14.877970 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.55336 (* 1 = 7.55336 loss)
I0523 07:31:14.950031 34682 sgd_solver.cpp:112] Iteration 71640, lr = 0.01
I0523 07:31:19.897718 34682 solver.cpp:239] Iteration 71650 (1.99395 iter/s, 5.01518s/10 iters), loss = 8.5064
I0523 07:31:19.897766 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5064 (* 1 = 8.5064 loss)
I0523 07:31:19.967190 34682 sgd_solver.cpp:112] Iteration 71650, lr = 0.01
I0523 07:31:24.387684 34682 solver.cpp:239] Iteration 71660 (2.2273 iter/s, 4.48973s/10 iters), loss = 6.46445
I0523 07:31:24.387964 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.46445 (* 1 = 6.46445 loss)
I0523 07:31:24.467325 34682 sgd_solver.cpp:112] Iteration 71660, lr = 0.01
I0523 07:31:28.447154 34682 solver.cpp:239] Iteration 71670 (2.46363 iter/s, 4.05905s/10 iters), loss = 7.82844
I0523 07:31:28.447216 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82844 (* 1 = 7.82844 loss)
I0523 07:31:28.521688 34682 sgd_solver.cpp:112] Iteration 71670, lr = 0.01
I0523 07:31:33.084594 34682 solver.cpp:239] Iteration 71680 (2.15648 iter/s, 4.63718s/10 iters), loss = 9.13261
I0523 07:31:33.084664 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.13261 (* 1 = 9.13261 loss)
I0523 07:31:33.898993 34682 sgd_solver.cpp:112] Iteration 71680, lr = 0.01
I0523 07:31:38.671263 34682 solver.cpp:239] Iteration 71690 (1.79007 iter/s, 5.58638s/10 iters), loss = 7.23341
I0523 07:31:38.671319 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.23341 (* 1 = 7.23341 loss)
I0523 07:31:39.468243 34682 sgd_solver.cpp:112] Iteration 71690, lr = 0.01
I0523 07:31:43.581043 34682 solver.cpp:239] Iteration 71700 (2.03686 iter/s, 4.90952s/10 iters), loss = 7.99031
I0523 07:31:43.581084 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99031 (* 1 = 7.99031 loss)
I0523 07:31:43.653327 34682 sgd_solver.cpp:112] Iteration 71700, lr = 0.01
I0523 07:31:48.552690 34682 solver.cpp:239] Iteration 71710 (2.01151 iter/s, 4.9714s/10 iters), loss = 8.40617
I0523 07:31:48.552738 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40617 (* 1 = 8.40617 loss)
I0523 07:31:49.117008 34682 sgd_solver.cpp:112] Iteration 71710, lr = 0.01
I0523 07:31:54.757616 34682 solver.cpp:239] Iteration 71720 (1.61173 iter/s, 6.20453s/10 iters), loss = 8.01517
I0523 07:31:54.758056 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01517 (* 1 = 8.01517 loss)
I0523 07:31:55.036320 34682 sgd_solver.cpp:112] Iteration 71720, lr = 0.01
I0523 07:31:59.648942 34682 solver.cpp:239] Iteration 71730 (2.04468 iter/s, 4.89074s/10 iters), loss = 7.28144
I0523 07:31:59.649003 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.28144 (* 1 = 7.28144 loss)
I0523 07:31:59.710197 34682 sgd_solver.cpp:112] Iteration 71730, lr = 0.01
I0523 07:32:02.902010 34682 solver.cpp:239] Iteration 71740 (3.07422 iter/s, 3.25285s/10 iters), loss = 8.01764
I0523 07:32:02.902097 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01764 (* 1 = 8.01764 loss)
I0523 07:32:02.975239 34682 sgd_solver.cpp:112] Iteration 71740, lr = 0.01
I0523 07:32:09.928510 34682 solver.cpp:239] Iteration 71750 (1.42326 iter/s, 7.02614s/10 iters), loss = 7.87285
I0523 07:32:09.928556 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87285 (* 1 = 7.87285 loss)
I0523 07:32:10.773571 34682 sgd_solver.cpp:112] Iteration 71750, lr = 0.01
I0523 07:32:13.427947 34682 solver.cpp:239] Iteration 71760 (2.85776 iter/s, 3.49924s/10 iters), loss = 8.24656
I0523 07:32:13.428000 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24656 (* 1 = 8.24656 loss)
I0523 07:32:14.088155 34682 sgd_solver.cpp:112] Iteration 71760, lr = 0.01
I0523 07:32:18.109341 34682 solver.cpp:239] Iteration 71770 (2.13623 iter/s, 4.68114s/10 iters), loss = 8.55675
I0523 07:32:18.109398 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55675 (* 1 = 8.55675 loss)
I0523 07:32:18.184077 34682 sgd_solver.cpp:112] Iteration 71770, lr = 0.01
I0523 07:32:22.367846 34682 solver.cpp:239] Iteration 71780 (2.34837 iter/s, 4.25827s/10 iters), loss = 7.84052
I0523 07:32:22.367893 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84052 (* 1 = 7.84052 loss)
I0523 07:32:22.573192 34682 sgd_solver.cpp:112] Iteration 71780, lr = 0.01
I0523 07:32:28.106117 34682 solver.cpp:239] Iteration 71790 (1.74277 iter/s, 5.73798s/10 iters), loss = 7.02189
I0523 07:32:28.106307 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.02189 (* 1 = 7.02189 loss)
I0523 07:32:28.177968 34682 sgd_solver.cpp:112] Iteration 71790, lr = 0.01
I0523 07:32:33.058133 34682 solver.cpp:239] Iteration 71800 (2.01953 iter/s, 4.95165s/10 iters), loss = 6.64186
I0523 07:32:33.058178 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.64186 (* 1 = 6.64186 loss)
I0523 07:32:33.908776 34682 sgd_solver.cpp:112] Iteration 71800, lr = 0.01
I0523 07:32:40.781095 34682 solver.cpp:239] Iteration 71810 (1.2949 iter/s, 7.7226s/10 iters), loss = 6.99251
I0523 07:32:40.781148 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.99251 (* 1 = 6.99251 loss)
I0523 07:32:41.049552 34682 sgd_solver.cpp:112] Iteration 71810, lr = 0.01
I0523 07:32:43.632463 34682 solver.cpp:239] Iteration 71820 (3.50731 iter/s, 2.85119s/10 iters), loss = 6.76375
I0523 07:32:43.632529 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.76375 (* 1 = 6.76375 loss)
I0523 07:32:44.419744 34682 sgd_solver.cpp:112] Iteration 71820, lr = 0.01
I0523 07:32:49.711771 34682 solver.cpp:239] Iteration 71830 (1.64501 iter/s, 6.079s/10 iters), loss = 6.89211
I0523 07:32:49.711832 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.89211 (* 1 = 6.89211 loss)
I0523 07:32:50.589015 34682 sgd_solver.cpp:112] Iteration 71830, lr = 0.01
I0523 07:32:55.580240 34682 solver.cpp:239] Iteration 71840 (1.70411 iter/s, 5.86817s/10 iters), loss = 8.10225
I0523 07:32:55.580283 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10225 (* 1 = 8.10225 loss)
I0523 07:32:55.635570 34682 sgd_solver.cpp:112] Iteration 71840, lr = 0.01
I0523 07:33:01.236143 34682 solver.cpp:239] Iteration 71850 (1.76815 iter/s, 5.65563s/10 iters), loss = 7.88113
I0523 07:33:01.236419 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88113 (* 1 = 7.88113 loss)
I0523 07:33:01.300832 34682 sgd_solver.cpp:112] Iteration 71850, lr = 0.01
I0523 07:33:05.265029 34682 solver.cpp:239] Iteration 71860 (2.48233 iter/s, 4.02847s/10 iters), loss = 8.3448
I0523 07:33:05.265075 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3448 (* 1 = 8.3448 loss)
I0523 07:33:05.336913 34682 sgd_solver.cpp:112] Iteration 71860, lr = 0.01
I0523 07:33:09.062489 34682 solver.cpp:239] Iteration 71870 (2.63348 iter/s, 3.79726s/10 iters), loss = 7.75091
I0523 07:33:09.062527 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75091 (* 1 = 7.75091 loss)
I0523 07:33:09.935484 34682 sgd_solver.cpp:112] Iteration 71870, lr = 0.01
I0523 07:33:12.944460 34682 solver.cpp:239] Iteration 71880 (2.57614 iter/s, 3.88177s/10 iters), loss = 7.6175
I0523 07:33:12.944517 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6175 (* 1 = 7.6175 loss)
I0523 07:33:13.741924 34682 sgd_solver.cpp:112] Iteration 71880, lr = 0.01
I0523 07:33:18.108420 34682 solver.cpp:239] Iteration 71890 (1.93662 iter/s, 5.16364s/10 iters), loss = 7.82685
I0523 07:33:18.108481 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82685 (* 1 = 7.82685 loss)
I0523 07:33:18.796967 34682 sgd_solver.cpp:112] Iteration 71890, lr = 0.01
I0523 07:33:23.915082 34682 solver.cpp:239] Iteration 71900 (1.72225 iter/s, 5.80636s/10 iters), loss = 7.85187
I0523 07:33:23.915132 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.85187 (* 1 = 7.85187 loss)
I0523 07:33:23.980234 34682 sgd_solver.cpp:112] Iteration 71900, lr = 0.01
I0523 07:33:29.393543 34682 solver.cpp:239] Iteration 71910 (1.82542 iter/s, 5.47819s/10 iters), loss = 7.19863
I0523 07:33:29.393604 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.19863 (* 1 = 7.19863 loss)
I0523 07:33:30.130197 34682 sgd_solver.cpp:112] Iteration 71910, lr = 0.01
I0523 07:33:34.962815 34682 solver.cpp:239] Iteration 71920 (1.79566 iter/s, 5.56898s/10 iters), loss = 6.94473
I0523 07:33:34.962939 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.94473 (* 1 = 6.94473 loss)
I0523 07:33:35.046795 34682 sgd_solver.cpp:112] Iteration 71920, lr = 0.01
I0523 07:33:39.860411 34682 solver.cpp:239] Iteration 71930 (2.04195 iter/s, 4.89727s/10 iters), loss = 7.44526
I0523 07:33:39.860473 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44526 (* 1 = 7.44526 loss)
I0523 07:33:39.936761 34682 sgd_solver.cpp:112] Iteration 71930, lr = 0.01
I0523 07:33:45.275913 34682 solver.cpp:239] Iteration 71940 (1.84665 iter/s, 5.41522s/10 iters), loss = 8.10908
I0523 07:33:45.275974 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10908 (* 1 = 8.10908 loss)
I0523 07:33:46.020414 34682 sgd_solver.cpp:112] Iteration 71940, lr = 0.01
I0523 07:33:49.666254 34682 solver.cpp:239] Iteration 71950 (2.27785 iter/s, 4.3901s/10 iters), loss = 7.36261
I0523 07:33:49.666302 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.36261 (* 1 = 7.36261 loss)
I0523 07:33:50.418517 34682 sgd_solver.cpp:112] Iteration 71950, lr = 0.01
I0523 07:33:54.074597 34682 solver.cpp:239] Iteration 71960 (2.26856 iter/s, 4.40809s/10 iters), loss = 7.18649
I0523 07:33:54.074656 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.18649 (* 1 = 7.18649 loss)
I0523 07:33:54.135782 34682 sgd_solver.cpp:112] Iteration 71960, lr = 0.01
I0523 07:33:57.227442 34682 solver.cpp:239] Iteration 71970 (3.17193 iter/s, 3.15265s/10 iters), loss = 8.31056
I0523 07:33:57.227491 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31056 (* 1 = 8.31056 loss)
I0523 07:33:57.297633 34682 sgd_solver.cpp:112] Iteration 71970, lr = 0.01
I0523 07:33:59.965653 34682 solver.cpp:239] Iteration 71980 (3.65224 iter/s, 2.73805s/10 iters), loss = 7.78038
I0523 07:33:59.965704 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.78038 (* 1 = 7.78038 loss)
I0523 07:34:00.181412 34682 sgd_solver.cpp:112] Iteration 71980, lr = 0.01
I0523 07:34:04.563885 34682 solver.cpp:239] Iteration 71990 (2.17486 iter/s, 4.59799s/10 iters), loss = 7.66247
I0523 07:34:04.563935 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66247 (* 1 = 7.66247 loss)
I0523 07:34:05.432543 34682 sgd_solver.cpp:112] Iteration 71990, lr = 0.01
I0523 07:34:10.529049 34682 solver.cpp:239] Iteration 72000 (1.67648 iter/s, 5.96487s/10 iters), loss = 8.33665
I0523 07:34:10.529095 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33665 (* 1 = 8.33665 loss)
I0523 07:34:10.585525 34682 sgd_solver.cpp:112] Iteration 72000, lr = 0.01
I0523 07:34:14.999791 34682 solver.cpp:239] Iteration 72010 (2.23688 iter/s, 4.4705s/10 iters), loss = 6.65029
I0523 07:34:14.999847 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.65029 (* 1 = 6.65029 loss)
I0523 07:34:15.833575 34682 sgd_solver.cpp:112] Iteration 72010, lr = 0.01
I0523 07:34:21.924615 34682 solver.cpp:239] Iteration 72020 (1.44415 iter/s, 6.92449s/10 iters), loss = 7.94895
I0523 07:34:21.924667 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94895 (* 1 = 7.94895 loss)
I0523 07:34:22.675118 34682 sgd_solver.cpp:112] Iteration 72020, lr = 0.01
I0523 07:34:27.436882 34682 solver.cpp:239] Iteration 72030 (1.81423 iter/s, 5.51199s/10 iters), loss = 8.57195
I0523 07:34:27.436936 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57195 (* 1 = 8.57195 loss)
I0523 07:34:27.515746 34682 sgd_solver.cpp:112] Iteration 72030, lr = 0.01
I0523 07:34:32.419248 34682 solver.cpp:239] Iteration 72040 (2.00719 iter/s, 4.9821s/10 iters), loss = 7.65016
I0523 07:34:32.419318 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.65016 (* 1 = 7.65016 loss)
I0523 07:34:32.497133 34682 sgd_solver.cpp:112] Iteration 72040, lr = 0.01
I0523 07:34:38.055579 34682 solver.cpp:239] Iteration 72050 (1.7743 iter/s, 5.63603s/10 iters), loss = 7.0442
I0523 07:34:38.055801 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.0442 (* 1 = 7.0442 loss)
I0523 07:34:38.111707 34682 sgd_solver.cpp:112] Iteration 72050, lr = 0.01
I0523 07:34:41.937664 34682 solver.cpp:239] Iteration 72060 (2.57618 iter/s, 3.88172s/10 iters), loss = 7.71783
I0523 07:34:41.937700 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.71783 (* 1 = 7.71783 loss)
I0523 07:34:41.999716 34682 sgd_solver.cpp:112] Iteration 72060, lr = 0.01
I0523 07:34:48.618625 34682 solver.cpp:239] Iteration 72070 (1.49686 iter/s, 6.68066s/10 iters), loss = 7.06545
I0523 07:34:48.618672 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.06545 (* 1 = 7.06545 loss)
I0523 07:34:48.682883 34682 sgd_solver.cpp:112] Iteration 72070, lr = 0.01
I0523 07:34:52.987246 34682 solver.cpp:239] Iteration 72080 (2.28917 iter/s, 4.36839s/10 iters), loss = 8.78248
I0523 07:34:52.987296 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78248 (* 1 = 8.78248 loss)
I0523 07:34:53.819741 34682 sgd_solver.cpp:112] Iteration 72080, lr = 0.01
I0523 07:34:57.786265 34682 solver.cpp:239] Iteration 72090 (2.08387 iter/s, 4.79877s/10 iters), loss = 8.6323
I0523 07:34:57.786322 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6323 (* 1 = 8.6323 loss)
I0523 07:34:58.631783 34682 sgd_solver.cpp:112] Iteration 72090, lr = 0.01
I0523 07:35:03.967960 34682 solver.cpp:239] Iteration 72100 (1.61777 iter/s, 6.18136s/10 iters), loss = 7.68287
I0523 07:35:03.968050 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68287 (* 1 = 7.68287 loss)
I0523 07:35:04.670120 34682 sgd_solver.cpp:112] Iteration 72100, lr = 0.01
I0523 07:35:08.099444 34682 solver.cpp:239] Iteration 72110 (2.42059 iter/s, 4.13122s/10 iters), loss = 7.63566
I0523 07:35:08.099730 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63566 (* 1 = 7.63566 loss)
I0523 07:35:08.176750 34682 sgd_solver.cpp:112] Iteration 72110, lr = 0.01
I0523 07:35:11.733368 34682 solver.cpp:239] Iteration 72120 (2.75215 iter/s, 3.63352s/10 iters), loss = 7.91951
I0523 07:35:11.733413 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91951 (* 1 = 7.91951 loss)
I0523 07:35:11.796066 34682 sgd_solver.cpp:112] Iteration 72120, lr = 0.01
I0523 07:35:16.865473 34682 solver.cpp:239] Iteration 72130 (1.94862 iter/s, 5.13185s/10 iters), loss = 7.34495
I0523 07:35:16.865517 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.34495 (* 1 = 7.34495 loss)
I0523 07:35:16.923099 34682 sgd_solver.cpp:112] Iteration 72130, lr = 0.01
I0523 07:35:20.904222 34682 solver.cpp:239] Iteration 72140 (2.47615 iter/s, 4.03854s/10 iters), loss = 7.92809
I0523 07:35:20.904276 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92809 (* 1 = 7.92809 loss)
I0523 07:35:21.726622 34682 sgd_solver.cpp:112] Iteration 72140, lr = 0.01
I0523 07:35:26.584450 34682 solver.cpp:239] Iteration 72150 (1.76058 iter/s, 5.67995s/10 iters), loss = 7.87666
I0523 07:35:26.584493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87666 (* 1 = 7.87666 loss)
I0523 07:35:26.646867 34682 sgd_solver.cpp:112] Iteration 72150, lr = 0.01
I0523 07:35:29.839913 34682 solver.cpp:239] Iteration 72160 (3.07412 iter/s, 3.25297s/10 iters), loss = 7.42295
I0523 07:35:29.839973 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42295 (* 1 = 7.42295 loss)
I0523 07:35:30.140040 34682 sgd_solver.cpp:112] Iteration 72160, lr = 0.01
I0523 07:35:34.577916 34682 solver.cpp:239] Iteration 72170 (2.11071 iter/s, 4.73775s/10 iters), loss = 7.99676
I0523 07:35:34.577962 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99676 (* 1 = 7.99676 loss)
I0523 07:35:34.650442 34682 sgd_solver.cpp:112] Iteration 72170, lr = 0.01
I0523 07:35:38.672529 34682 solver.cpp:239] Iteration 72180 (2.44236 iter/s, 4.0944s/10 iters), loss = 8.48631
I0523 07:35:38.672744 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48631 (* 1 = 8.48631 loss)
I0523 07:35:38.732416 34682 sgd_solver.cpp:112] Iteration 72180, lr = 0.01
I0523 07:35:41.506618 34682 solver.cpp:239] Iteration 72190 (3.52887 iter/s, 2.83377s/10 iters), loss = 8.35034
I0523 07:35:41.506659 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35034 (* 1 = 8.35034 loss)
I0523 07:35:41.574949 34682 sgd_solver.cpp:112] Iteration 72190, lr = 0.01
I0523 07:35:46.582144 34682 solver.cpp:239] Iteration 72200 (1.97034 iter/s, 5.07527s/10 iters), loss = 6.24735
I0523 07:35:46.582206 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.24735 (* 1 = 6.24735 loss)
I0523 07:35:47.292600 34682 sgd_solver.cpp:112] Iteration 72200, lr = 0.01
I0523 07:35:49.846427 34682 solver.cpp:239] Iteration 72210 (3.06365 iter/s, 3.26409s/10 iters), loss = 7.13251
I0523 07:35:49.846468 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.13251 (* 1 = 7.13251 loss)
I0523 07:35:49.934511 34682 sgd_solver.cpp:112] Iteration 72210, lr = 0.01
I0523 07:35:53.920501 34682 solver.cpp:239] Iteration 72220 (2.45467 iter/s, 4.07386s/10 iters), loss = 8.17884
I0523 07:35:53.920547 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17884 (* 1 = 8.17884 loss)
I0523 07:35:54.006867 34682 sgd_solver.cpp:112] Iteration 72220, lr = 0.01
I0523 07:35:58.490460 34682 solver.cpp:239] Iteration 72230 (2.18831 iter/s, 4.56973s/10 iters), loss = 7.73202
I0523 07:35:58.490501 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.73202 (* 1 = 7.73202 loss)
I0523 07:35:58.547683 34682 sgd_solver.cpp:112] Iteration 72230, lr = 0.01
I0523 07:36:02.431207 34682 solver.cpp:239] Iteration 72240 (2.53772 iter/s, 3.94054s/10 iters), loss = 7.84654
I0523 07:36:02.431248 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84654 (* 1 = 7.84654 loss)
I0523 07:36:02.514915 34682 sgd_solver.cpp:112] Iteration 72240, lr = 0.01
I0523 07:36:08.004180 34682 solver.cpp:239] Iteration 72250 (1.79446 iter/s, 5.5727s/10 iters), loss = 7.23861
I0523 07:36:08.004235 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.23861 (* 1 = 7.23861 loss)
I0523 07:36:08.073766 34682 sgd_solver.cpp:112] Iteration 72250, lr = 0.01
I0523 07:36:11.033077 34682 solver.cpp:239] Iteration 72260 (3.30173 iter/s, 3.02872s/10 iters), loss = 6.94437
I0523 07:36:11.033301 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.94437 (* 1 = 6.94437 loss)
I0523 07:36:11.110776 34682 sgd_solver.cpp:112] Iteration 72260, lr = 0.01
I0523 07:36:14.174567 34682 solver.cpp:239] Iteration 72270 (3.18357 iter/s, 3.14113s/10 iters), loss = 8.23948
I0523 07:36:14.174619 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23948 (* 1 = 8.23948 loss)
I0523 07:36:14.233819 34682 sgd_solver.cpp:112] Iteration 72270, lr = 0.01
I0523 07:36:18.043079 34682 solver.cpp:239] Iteration 72280 (2.58512 iter/s, 3.86829s/10 iters), loss = 8.80735
I0523 07:36:18.043138 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80735 (* 1 = 8.80735 loss)
I0523 07:36:18.863798 34682 sgd_solver.cpp:112] Iteration 72280, lr = 0.01
I0523 07:36:23.642717 34682 solver.cpp:239] Iteration 72290 (1.78593 iter/s, 5.59933s/10 iters), loss = 7.77019
I0523 07:36:23.642766 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77019 (* 1 = 7.77019 loss)
I0523 07:36:23.714979 34682 sgd_solver.cpp:112] Iteration 72290, lr = 0.01
I0523 07:36:27.171440 34682 solver.cpp:239] Iteration 72300 (2.83602 iter/s, 3.52607s/10 iters), loss = 7.88942
I0523 07:36:27.171483 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88942 (* 1 = 7.88942 loss)
I0523 07:36:27.252351 34682 sgd_solver.cpp:112] Iteration 72300, lr = 0.01
I0523 07:36:32.015445 34682 solver.cpp:239] Iteration 72310 (2.06451 iter/s, 4.84376s/10 iters), loss = 7.7282
I0523 07:36:32.015499 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7282 (* 1 = 7.7282 loss)
I0523 07:36:32.851799 34682 sgd_solver.cpp:112] Iteration 72310, lr = 0.01
I0523 07:36:38.475492 34682 solver.cpp:239] Iteration 72320 (1.54805 iter/s, 6.45974s/10 iters), loss = 6.86812
I0523 07:36:38.475536 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.86812 (* 1 = 6.86812 loss)
I0523 07:36:38.713997 34682 sgd_solver.cpp:112] Iteration 72320, lr = 0.01
I0523 07:36:42.954012 34682 solver.cpp:239] Iteration 72330 (2.23299 iter/s, 4.47829s/10 iters), loss = 7.48013
I0523 07:36:42.954226 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.48013 (* 1 = 7.48013 loss)
I0523 07:36:43.735257 34682 sgd_solver.cpp:112] Iteration 72330, lr = 0.01
I0523 07:36:46.439805 34682 solver.cpp:239] Iteration 72340 (2.86906 iter/s, 3.48546s/10 iters), loss = 6.81862
I0523 07:36:46.439862 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.81862 (* 1 = 6.81862 loss)
I0523 07:36:47.014276 34682 sgd_solver.cpp:112] Iteration 72340, lr = 0.01
I0523 07:36:51.047778 34682 solver.cpp:239] Iteration 72350 (2.17027 iter/s, 4.60773s/10 iters), loss = 8.84259
I0523 07:36:51.047827 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.84259 (* 1 = 8.84259 loss)
I0523 07:36:51.124830 34682 sgd_solver.cpp:112] Iteration 72350, lr = 0.01
I0523 07:36:54.080368 34682 solver.cpp:239] Iteration 72360 (3.29772 iter/s, 3.0324s/10 iters), loss = 8.3259
I0523 07:36:54.080420 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3259 (* 1 = 8.3259 loss)
I0523 07:36:54.152904 34682 sgd_solver.cpp:112] Iteration 72360, lr = 0.01
I0523 07:36:58.262840 34682 solver.cpp:239] Iteration 72370 (2.39106 iter/s, 4.18224s/10 iters), loss = 8.46905
I0523 07:36:58.262915 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46905 (* 1 = 8.46905 loss)
I0523 07:36:58.643527 34682 sgd_solver.cpp:112] Iteration 72370, lr = 0.01
I0523 07:37:04.193239 34682 solver.cpp:239] Iteration 72380 (1.68632 iter/s, 5.93009s/10 iters), loss = 7.59558
I0523 07:37:04.193289 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.59558 (* 1 = 7.59558 loss)
I0523 07:37:04.985096 34682 sgd_solver.cpp:112] Iteration 72380, lr = 0.01
I0523 07:37:09.606488 34682 solver.cpp:239] Iteration 72390 (1.84741 iter/s, 5.41298s/10 iters), loss = 8.85337
I0523 07:37:09.606535 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85337 (* 1 = 8.85337 loss)
I0523 07:37:10.416445 34682 sgd_solver.cpp:112] Iteration 72390, lr = 0.01
I0523 07:37:17.171605 34682 solver.cpp:239] Iteration 72400 (1.32192 iter/s, 7.56476s/10 iters), loss = 7.56257
I0523 07:37:17.171808 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56257 (* 1 = 7.56257 loss)
I0523 07:37:17.752724 34682 sgd_solver.cpp:112] Iteration 72400, lr = 0.01
I0523 07:37:23.486387 34682 solver.cpp:239] Iteration 72410 (1.5837 iter/s, 6.31433s/10 iters), loss = 8.27725
I0523 07:37:23.486435 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27725 (* 1 = 8.27725 loss)
I0523 07:37:23.565768 34682 sgd_solver.cpp:112] Iteration 72410, lr = 0.01
I0523 07:37:27.836280 34682 solver.cpp:239] Iteration 72420 (2.29904 iter/s, 4.34965s/10 iters), loss = 8.40915
I0523 07:37:27.836345 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40915 (* 1 = 8.40915 loss)
I0523 07:37:28.673120 34682 sgd_solver.cpp:112] Iteration 72420, lr = 0.01
I0523 07:37:34.813722 34682 solver.cpp:239] Iteration 72430 (1.43326 iter/s, 6.9771s/10 iters), loss = 7.68495
I0523 07:37:34.813784 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68495 (* 1 = 7.68495 loss)
I0523 07:37:34.899320 34682 sgd_solver.cpp:112] Iteration 72430, lr = 0.01
I0523 07:37:40.881175 34682 solver.cpp:239] Iteration 72440 (1.64822 iter/s, 6.06715s/10 iters), loss = 7.90182
I0523 07:37:40.881233 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90182 (* 1 = 7.90182 loss)
I0523 07:37:41.742493 34682 sgd_solver.cpp:112] Iteration 72440, lr = 0.01
I0523 07:37:46.559587 34682 solver.cpp:239] Iteration 72450 (1.76115 iter/s, 5.67812s/10 iters), loss = 8.20685
I0523 07:37:46.559648 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20685 (* 1 = 8.20685 loss)
I0523 07:37:47.123241 34682 sgd_solver.cpp:112] Iteration 72450, lr = 0.01
I0523 07:37:51.044261 34682 solver.cpp:239] Iteration 72460 (2.22994 iter/s, 4.48442s/10 iters), loss = 7.55649
I0523 07:37:51.044515 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.55649 (* 1 = 7.55649 loss)
I0523 07:37:51.117692 34682 sgd_solver.cpp:112] Iteration 72460, lr = 0.01
I0523 07:37:56.751806 34682 solver.cpp:239] Iteration 72470 (1.75221 iter/s, 5.70708s/10 iters), loss = 8.70283
I0523 07:37:56.751857 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70283 (* 1 = 8.70283 loss)
I0523 07:37:56.811333 34682 sgd_solver.cpp:112] Iteration 72470, lr = 0.01
I0523 07:38:02.211647 34682 solver.cpp:239] Iteration 72480 (1.83165 iter/s, 5.45956s/10 iters), loss = 7.26756
I0523 07:38:02.211704 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.26756 (* 1 = 7.26756 loss)
I0523 07:38:02.435094 34682 sgd_solver.cpp:112] Iteration 72480, lr = 0.01
I0523 07:38:06.551851 34682 solver.cpp:239] Iteration 72490 (2.30417 iter/s, 4.33996s/10 iters), loss = 7.48141
I0523 07:38:06.551913 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.48141 (* 1 = 7.48141 loss)
I0523 07:38:06.975123 34682 sgd_solver.cpp:112] Iteration 72490, lr = 0.01
I0523 07:38:10.816457 34682 solver.cpp:239] Iteration 72500 (2.34501 iter/s, 4.26437s/10 iters), loss = 6.90935
I0523 07:38:10.816506 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.90935 (* 1 = 6.90935 loss)
I0523 07:38:10.882738 34682 sgd_solver.cpp:112] Iteration 72500, lr = 0.01
I0523 07:38:14.039852 34682 solver.cpp:239] Iteration 72510 (3.1025 iter/s, 3.22321s/10 iters), loss = 8.48733
I0523 07:38:14.039898 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48733 (* 1 = 8.48733 loss)
I0523 07:38:14.802647 34682 sgd_solver.cpp:112] Iteration 72510, lr = 0.01
I0523 07:38:19.075124 34682 solver.cpp:239] Iteration 72520 (1.98609 iter/s, 5.03502s/10 iters), loss = 7.08206
I0523 07:38:19.075172 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.08206 (* 1 = 7.08206 loss)
I0523 07:38:19.150626 34682 sgd_solver.cpp:112] Iteration 72520, lr = 0.01
I0523 07:38:23.892110 34682 solver.cpp:239] Iteration 72530 (2.07609 iter/s, 4.81674s/10 iters), loss = 7.99937
I0523 07:38:23.892360 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99937 (* 1 = 7.99937 loss)
I0523 07:38:23.966152 34682 sgd_solver.cpp:112] Iteration 72530, lr = 0.01
I0523 07:38:28.829495 34682 solver.cpp:239] Iteration 72540 (2.02554 iter/s, 4.93695s/10 iters), loss = 8.91986
I0523 07:38:28.829546 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91986 (* 1 = 8.91986 loss)
I0523 07:38:28.886602 34682 sgd_solver.cpp:112] Iteration 72540, lr = 0.01
I0523 07:38:35.319106 34682 solver.cpp:239] Iteration 72550 (1.541 iter/s, 6.48929s/10 iters), loss = 7.41729
I0523 07:38:35.319165 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.41729 (* 1 = 7.41729 loss)
I0523 07:38:36.054446 34682 sgd_solver.cpp:112] Iteration 72550, lr = 0.01
I0523 07:38:42.317854 34682 solver.cpp:239] Iteration 72560 (1.4289 iter/s, 6.99841s/10 iters), loss = 7.98288
I0523 07:38:42.317910 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98288 (* 1 = 7.98288 loss)
I0523 07:38:42.846738 34682 sgd_solver.cpp:112] Iteration 72560, lr = 0.01
I0523 07:38:49.221954 34682 solver.cpp:239] Iteration 72570 (1.44849 iter/s, 6.90376s/10 iters), loss = 8.25196
I0523 07:38:49.221997 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25196 (* 1 = 8.25196 loss)
I0523 07:38:49.304419 34682 sgd_solver.cpp:112] Iteration 72570, lr = 0.01
I0523 07:38:52.411888 34682 solver.cpp:239] Iteration 72580 (3.13504 iter/s, 3.18975s/10 iters), loss = 6.60217
I0523 07:38:52.411937 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.60217 (* 1 = 6.60217 loss)
I0523 07:38:52.472719 34682 sgd_solver.cpp:112] Iteration 72580, lr = 0.01
I0523 07:38:56.462952 34682 solver.cpp:239] Iteration 72590 (2.46862 iter/s, 4.05084s/10 iters), loss = 7.67331
I0523 07:38:56.463169 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.67331 (* 1 = 7.67331 loss)
I0523 07:38:57.292431 34682 sgd_solver.cpp:112] Iteration 72590, lr = 0.01
I0523 07:39:01.412757 34682 solver.cpp:239] Iteration 72600 (2.02044 iter/s, 4.94941s/10 iters), loss = 7.93488
I0523 07:39:01.412816 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93488 (* 1 = 7.93488 loss)
I0523 07:39:02.223148 34682 sgd_solver.cpp:112] Iteration 72600, lr = 0.01
I0523 07:39:06.384438 34682 solver.cpp:239] Iteration 72610 (2.0115 iter/s, 4.97142s/10 iters), loss = 7.58537
I0523 07:39:06.384487 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58537 (* 1 = 7.58537 loss)
I0523 07:39:07.155597 34682 sgd_solver.cpp:112] Iteration 72610, lr = 0.01
I0523 07:39:11.539796 34682 solver.cpp:239] Iteration 72620 (1.93983 iter/s, 5.1551s/10 iters), loss = 8.00742
I0523 07:39:11.539855 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00742 (* 1 = 8.00742 loss)
I0523 07:39:11.606122 34682 sgd_solver.cpp:112] Iteration 72620, lr = 0.01
I0523 07:39:14.758426 34682 solver.cpp:239] Iteration 72630 (3.10709 iter/s, 3.21844s/10 iters), loss = 8.29988
I0523 07:39:14.758474 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29988 (* 1 = 8.29988 loss)
I0523 07:39:14.822230 34682 sgd_solver.cpp:112] Iteration 72630, lr = 0.01
I0523 07:39:19.706564 34682 solver.cpp:239] Iteration 72640 (2.02107 iter/s, 4.94788s/10 iters), loss = 7.77203
I0523 07:39:19.706607 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77203 (* 1 = 7.77203 loss)
I0523 07:39:19.777276 34682 sgd_solver.cpp:112] Iteration 72640, lr = 0.01
I0523 07:39:23.630884 34682 solver.cpp:239] Iteration 72650 (2.54835 iter/s, 3.92411s/10 iters), loss = 8.75302
I0523 07:39:23.630928 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75302 (* 1 = 8.75302 loss)
I0523 07:39:24.447913 34682 sgd_solver.cpp:112] Iteration 72650, lr = 0.01
I0523 07:39:29.178215 34682 solver.cpp:239] Iteration 72660 (1.80276 iter/s, 5.54706s/10 iters), loss = 7.72048
I0523 07:39:29.178449 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72048 (* 1 = 7.72048 loss)
I0523 07:39:29.893700 34682 sgd_solver.cpp:112] Iteration 72660, lr = 0.01
I0523 07:39:33.637316 34682 solver.cpp:239] Iteration 72670 (2.2428 iter/s, 4.45872s/10 iters), loss = 7.16411
I0523 07:39:33.637363 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.16411 (* 1 = 7.16411 loss)
I0523 07:39:33.712769 34682 sgd_solver.cpp:112] Iteration 72670, lr = 0.01
I0523 07:39:38.890339 34682 solver.cpp:239] Iteration 72680 (1.90376 iter/s, 5.25275s/10 iters), loss = 7.58243
I0523 07:39:38.890399 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58243 (* 1 = 7.58243 loss)
I0523 07:39:39.755560 34682 sgd_solver.cpp:112] Iteration 72680, lr = 0.01
I0523 07:39:43.009376 34682 solver.cpp:239] Iteration 72690 (2.42789 iter/s, 4.1188s/10 iters), loss = 8.90115
I0523 07:39:43.009423 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90115 (* 1 = 8.90115 loss)
I0523 07:39:43.079319 34682 sgd_solver.cpp:112] Iteration 72690, lr = 0.01
I0523 07:39:46.766506 34682 solver.cpp:239] Iteration 72700 (2.66175 iter/s, 3.75693s/10 iters), loss = 7.66993
I0523 07:39:46.766552 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66993 (* 1 = 7.66993 loss)
I0523 07:39:46.830668 34682 sgd_solver.cpp:112] Iteration 72700, lr = 0.01
I0523 07:39:51.641369 34682 solver.cpp:239] Iteration 72710 (2.05145 iter/s, 4.87461s/10 iters), loss = 6.85564
I0523 07:39:51.641420 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.85564 (* 1 = 6.85564 loss)
I0523 07:39:52.471364 34682 sgd_solver.cpp:112] Iteration 72710, lr = 0.01
I0523 07:39:58.271248 34682 solver.cpp:239] Iteration 72720 (1.5084 iter/s, 6.62956s/10 iters), loss = 8.06213
I0523 07:39:58.271291 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06213 (* 1 = 8.06213 loss)
I0523 07:39:58.344103 34682 sgd_solver.cpp:112] Iteration 72720, lr = 0.01
I0523 07:40:01.751865 34682 solver.cpp:239] Iteration 72730 (2.87322 iter/s, 3.48042s/10 iters), loss = 8.06784
I0523 07:40:01.752113 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06784 (* 1 = 8.06784 loss)
I0523 07:40:01.824470 34682 sgd_solver.cpp:112] Iteration 72730, lr = 0.01
I0523 07:40:05.394176 34682 solver.cpp:239] Iteration 72740 (2.7458 iter/s, 3.64193s/10 iters), loss = 7.26762
I0523 07:40:05.394227 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.26762 (* 1 = 7.26762 loss)
I0523 07:40:06.249007 34682 sgd_solver.cpp:112] Iteration 72740, lr = 0.01
I0523 07:40:09.686996 34682 solver.cpp:239] Iteration 72750 (2.3296 iter/s, 4.29259s/10 iters), loss = 7.42132
I0523 07:40:09.687049 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42132 (* 1 = 7.42132 loss)
I0523 07:40:09.761385 34682 sgd_solver.cpp:112] Iteration 72750, lr = 0.01
I0523 07:40:13.423043 34682 solver.cpp:239] Iteration 72760 (2.67678 iter/s, 3.73584s/10 iters), loss = 8.28021
I0523 07:40:13.423085 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28021 (* 1 = 8.28021 loss)
I0523 07:40:13.493043 34682 sgd_solver.cpp:112] Iteration 72760, lr = 0.01
I0523 07:40:16.509454 34682 solver.cpp:239] Iteration 72770 (3.2402 iter/s, 3.08623s/10 iters), loss = 7.9761
I0523 07:40:16.509510 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9761 (* 1 = 7.9761 loss)
I0523 07:40:17.309160 34682 sgd_solver.cpp:112] Iteration 72770, lr = 0.01
I0523 07:40:21.246250 34682 solver.cpp:239] Iteration 72780 (2.11124 iter/s, 4.73655s/10 iters), loss = 6.6721
I0523 07:40:21.246299 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.6721 (* 1 = 6.6721 loss)
I0523 07:40:21.312981 34682 sgd_solver.cpp:112] Iteration 72780, lr = 0.01
I0523 07:40:28.088104 34682 solver.cpp:239] Iteration 72790 (1.46166 iter/s, 6.84152s/10 iters), loss = 6.71314
I0523 07:40:28.088162 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.71314 (* 1 = 6.71314 loss)
I0523 07:40:28.162531 34682 sgd_solver.cpp:112] Iteration 72790, lr = 0.01
I0523 07:40:33.607369 34682 solver.cpp:239] Iteration 72800 (1.81193 iter/s, 5.51899s/10 iters), loss = 8.00543
I0523 07:40:33.607668 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00543 (* 1 = 8.00543 loss)
I0523 07:40:34.403596 34682 sgd_solver.cpp:112] Iteration 72800, lr = 0.01
I0523 07:40:39.332083 34682 solver.cpp:239] Iteration 72810 (1.74697 iter/s, 5.72421s/10 iters), loss = 7.74317
I0523 07:40:39.332130 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.74317 (* 1 = 7.74317 loss)
I0523 07:40:40.173483 34682 sgd_solver.cpp:112] Iteration 72810, lr = 0.01
I0523 07:40:44.123836 34682 solver.cpp:239] Iteration 72820 (2.08703 iter/s, 4.79151s/10 iters), loss = 8.42523
I0523 07:40:44.123878 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42523 (* 1 = 8.42523 loss)
I0523 07:40:44.196007 34682 sgd_solver.cpp:112] Iteration 72820, lr = 0.01
I0523 07:40:47.452497 34682 solver.cpp:239] Iteration 72830 (3.00438 iter/s, 3.32848s/10 iters), loss = 8.31054
I0523 07:40:47.452540 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31054 (* 1 = 8.31054 loss)
I0523 07:40:48.252128 34682 sgd_solver.cpp:112] Iteration 72830, lr = 0.01
I0523 07:40:51.467558 34682 solver.cpp:239] Iteration 72840 (2.49075 iter/s, 4.01485s/10 iters), loss = 8.14914
I0523 07:40:51.467613 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14914 (* 1 = 8.14914 loss)
I0523 07:40:51.520212 34682 sgd_solver.cpp:112] Iteration 72840, lr = 0.01
I0523 07:40:56.274091 34682 solver.cpp:239] Iteration 72850 (2.08061 iter/s, 4.80627s/10 iters), loss = 7.50405
I0523 07:40:56.274148 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50405 (* 1 = 7.50405 loss)
I0523 07:40:57.121243 34682 sgd_solver.cpp:112] Iteration 72850, lr = 0.01
I0523 07:41:01.802139 34682 solver.cpp:239] Iteration 72860 (1.80905 iter/s, 5.52776s/10 iters), loss = 8.09213
I0523 07:41:01.802206 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09213 (* 1 = 8.09213 loss)
I0523 07:41:02.212494 34682 sgd_solver.cpp:112] Iteration 72860, lr = 0.01
I0523 07:41:07.560433 34682 solver.cpp:239] Iteration 72870 (1.73671 iter/s, 5.758s/10 iters), loss = 7.4283
I0523 07:41:07.560668 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4283 (* 1 = 7.4283 loss)
I0523 07:41:07.637127 34682 sgd_solver.cpp:112] Iteration 72870, lr = 0.01
I0523 07:41:13.200330 34682 solver.cpp:239] Iteration 72880 (1.77322 iter/s, 5.63946s/10 iters), loss = 6.64523
I0523 07:41:13.200388 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.64523 (* 1 = 6.64523 loss)
I0523 07:41:13.258036 34682 sgd_solver.cpp:112] Iteration 72880, lr = 0.01
I0523 07:41:17.399907 34682 solver.cpp:239] Iteration 72890 (2.38132 iter/s, 4.19935s/10 iters), loss = 7.50132
I0523 07:41:17.399952 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50132 (* 1 = 7.50132 loss)
I0523 07:41:17.470388 34682 sgd_solver.cpp:112] Iteration 72890, lr = 0.01
I0523 07:41:23.046026 34682 solver.cpp:239] Iteration 72900 (1.77122 iter/s, 5.64583s/10 iters), loss = 7.36181
I0523 07:41:23.046089 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.36181 (* 1 = 7.36181 loss)
I0523 07:41:23.119065 34682 sgd_solver.cpp:112] Iteration 72900, lr = 0.01
I0523 07:41:26.576202 34682 solver.cpp:239] Iteration 72910 (2.83289 iter/s, 3.52997s/10 iters), loss = 7.16336
I0523 07:41:26.576251 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.16336 (* 1 = 7.16336 loss)
I0523 07:41:26.652936 34682 sgd_solver.cpp:112] Iteration 72910, lr = 0.01
I0523 07:41:31.516683 34682 solver.cpp:239] Iteration 72920 (2.0242 iter/s, 4.94023s/10 iters), loss = 7.12235
I0523 07:41:31.516736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.12235 (* 1 = 7.12235 loss)
I0523 07:41:31.603747 34682 sgd_solver.cpp:112] Iteration 72920, lr = 0.01
I0523 07:41:37.242676 34682 solver.cpp:239] Iteration 72930 (1.74651 iter/s, 5.72571s/10 iters), loss = 6.84161
I0523 07:41:37.242741 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.84161 (* 1 = 6.84161 loss)
I0523 07:41:37.308341 34682 sgd_solver.cpp:112] Iteration 72930, lr = 0.01
I0523 07:41:41.290448 34682 solver.cpp:239] Iteration 72940 (2.47064 iter/s, 4.04754s/10 iters), loss = 7.91887
I0523 07:41:41.290735 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91887 (* 1 = 7.91887 loss)
I0523 07:41:41.353969 34682 sgd_solver.cpp:112] Iteration 72940, lr = 0.01
I0523 07:41:47.891049 34682 solver.cpp:239] Iteration 72950 (1.51513 iter/s, 6.6001s/10 iters), loss = 8.23206
I0523 07:41:47.891096 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23206 (* 1 = 8.23206 loss)
I0523 07:41:48.702190 34682 sgd_solver.cpp:112] Iteration 72950, lr = 0.01
I0523 07:41:53.843858 34682 solver.cpp:239] Iteration 72960 (1.67996 iter/s, 5.95252s/10 iters), loss = 7.77924
I0523 07:41:53.843899 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77924 (* 1 = 7.77924 loss)
I0523 07:41:53.920907 34682 sgd_solver.cpp:112] Iteration 72960, lr = 0.01
I0523 07:41:58.852553 34682 solver.cpp:239] Iteration 72970 (1.99663 iter/s, 5.00844s/10 iters), loss = 8.05101
I0523 07:41:58.852608 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05101 (* 1 = 8.05101 loss)
I0523 07:41:58.914214 34682 sgd_solver.cpp:112] Iteration 72970, lr = 0.01
I0523 07:42:03.354414 34682 solver.cpp:239] Iteration 72980 (2.22142 iter/s, 4.50162s/10 iters), loss = 7.86244
I0523 07:42:03.354471 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86244 (* 1 = 7.86244 loss)
I0523 07:42:04.128304 34682 sgd_solver.cpp:112] Iteration 72980, lr = 0.01
I0523 07:42:07.333926 34682 solver.cpp:239] Iteration 72990 (2.51302 iter/s, 3.97928s/10 iters), loss = 8.32125
I0523 07:42:07.333984 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32125 (* 1 = 8.32125 loss)
I0523 07:42:07.384397 34682 sgd_solver.cpp:112] Iteration 72990, lr = 0.01
I0523 07:42:13.394867 34682 solver.cpp:239] Iteration 73000 (1.64999 iter/s, 6.06063s/10 iters), loss = 8.00896
I0523 07:42:13.395045 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00896 (* 1 = 8.00896 loss)
I0523 07:42:13.466913 34682 sgd_solver.cpp:112] Iteration 73000, lr = 0.01
I0523 07:42:16.715798 34682 solver.cpp:239] Iteration 73010 (3.01532 iter/s, 3.3164s/10 iters), loss = 8.17325
I0523 07:42:16.715852 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17325 (* 1 = 8.17325 loss)
I0523 07:42:17.091626 34682 sgd_solver.cpp:112] Iteration 73010, lr = 0.01
I0523 07:42:20.904381 34682 solver.cpp:239] Iteration 73020 (2.38757 iter/s, 4.18836s/10 iters), loss = 7.57974
I0523 07:42:20.904428 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.57974 (* 1 = 7.57974 loss)
I0523 07:42:21.400633 34682 sgd_solver.cpp:112] Iteration 73020, lr = 0.01
I0523 07:42:25.591898 34682 solver.cpp:239] Iteration 73030 (2.13343 iter/s, 4.68728s/10 iters), loss = 8.16094
I0523 07:42:25.591953 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16094 (* 1 = 8.16094 loss)
I0523 07:42:25.662310 34682 sgd_solver.cpp:112] Iteration 73030, lr = 0.01
I0523 07:42:29.201936 34682 solver.cpp:239] Iteration 73040 (2.77023 iter/s, 3.60981s/10 iters), loss = 6.80922
I0523 07:42:29.201994 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.80922 (* 1 = 6.80922 loss)
I0523 07:42:29.856343 34682 sgd_solver.cpp:112] Iteration 73040, lr = 0.01
I0523 07:42:33.143095 34682 solver.cpp:239] Iteration 73050 (2.53747 iter/s, 3.94094s/10 iters), loss = 7.78017
I0523 07:42:33.143144 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.78017 (* 1 = 7.78017 loss)
I0523 07:42:33.201781 34682 sgd_solver.cpp:112] Iteration 73050, lr = 0.01
I0523 07:42:37.186372 34682 solver.cpp:239] Iteration 73060 (2.47337 iter/s, 4.04306s/10 iters), loss = 7.96167
I0523 07:42:37.186414 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96167 (* 1 = 7.96167 loss)
I0523 07:42:38.029086 34682 sgd_solver.cpp:112] Iteration 73060, lr = 0.01
I0523 07:42:41.105911 34682 solver.cpp:239] Iteration 73070 (2.55146 iter/s, 3.91933s/10 iters), loss = 7.25699
I0523 07:42:41.105969 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.25699 (* 1 = 7.25699 loss)
I0523 07:42:41.169154 34682 sgd_solver.cpp:112] Iteration 73070, lr = 0.01
I0523 07:42:44.286317 34682 solver.cpp:239] Iteration 73080 (3.14444 iter/s, 3.18022s/10 iters), loss = 8.42285
I0523 07:42:44.286496 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42285 (* 1 = 8.42285 loss)
I0523 07:42:44.363154 34682 sgd_solver.cpp:112] Iteration 73080, lr = 0.01
I0523 07:42:49.570452 34682 solver.cpp:239] Iteration 73090 (1.8926 iter/s, 5.28373s/10 iters), loss = 7.8289
I0523 07:42:49.570502 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8289 (* 1 = 7.8289 loss)
I0523 07:42:49.640148 34682 sgd_solver.cpp:112] Iteration 73090, lr = 0.01
I0523 07:42:52.260874 34682 solver.cpp:239] Iteration 73100 (3.71712 iter/s, 2.69026s/10 iters), loss = 7.49381
I0523 07:42:52.260917 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.49381 (* 1 = 7.49381 loss)
I0523 07:42:53.061261 34682 sgd_solver.cpp:112] Iteration 73100, lr = 0.01
I0523 07:42:58.693720 34682 solver.cpp:239] Iteration 73110 (1.5546 iter/s, 6.43253s/10 iters), loss = 8.19473
I0523 07:42:58.693769 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19473 (* 1 = 8.19473 loss)
I0523 07:42:59.510838 34682 sgd_solver.cpp:112] Iteration 73110, lr = 0.01
I0523 07:43:03.715682 34682 solver.cpp:239] Iteration 73120 (1.99136 iter/s, 5.0217s/10 iters), loss = 7.72022
I0523 07:43:03.715737 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72022 (* 1 = 7.72022 loss)
I0523 07:43:04.559201 34682 sgd_solver.cpp:112] Iteration 73120, lr = 0.01
I0523 07:43:10.225479 34682 solver.cpp:239] Iteration 73130 (1.53622 iter/s, 6.50948s/10 iters), loss = 7.17301
I0523 07:43:10.225518 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.17301 (* 1 = 7.17301 loss)
I0523 07:43:10.296888 34682 sgd_solver.cpp:112] Iteration 73130, lr = 0.01
I0523 07:43:15.121263 34682 solver.cpp:239] Iteration 73140 (2.04268 iter/s, 4.89553s/10 iters), loss = 8.18888
I0523 07:43:15.121544 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18888 (* 1 = 8.18888 loss)
I0523 07:43:15.353044 34682 sgd_solver.cpp:112] Iteration 73140, lr = 0.01
I0523 07:43:21.018864 34682 solver.cpp:239] Iteration 73150 (1.69575 iter/s, 5.89711s/10 iters), loss = 7.58246
I0523 07:43:21.018906 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58246 (* 1 = 7.58246 loss)
I0523 07:43:21.092859 34682 sgd_solver.cpp:112] Iteration 73150, lr = 0.01
I0523 07:43:25.987107 34682 solver.cpp:239] Iteration 73160 (2.01288 iter/s, 4.968s/10 iters), loss = 7.76282
I0523 07:43:25.987152 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.76282 (* 1 = 7.76282 loss)
I0523 07:43:26.049154 34682 sgd_solver.cpp:112] Iteration 73160, lr = 0.01
I0523 07:43:32.465382 34682 solver.cpp:239] Iteration 73170 (1.5437 iter/s, 6.47795s/10 iters), loss = 8.29257
I0523 07:43:32.465453 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29257 (* 1 = 8.29257 loss)
I0523 07:43:33.288650 34682 sgd_solver.cpp:112] Iteration 73170, lr = 0.01
I0523 07:43:38.023687 34682 solver.cpp:239] Iteration 73180 (1.7992 iter/s, 5.55802s/10 iters), loss = 8.4415
I0523 07:43:38.023736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4415 (* 1 = 8.4415 loss)
I0523 07:43:38.087822 34682 sgd_solver.cpp:112] Iteration 73180, lr = 0.01
I0523 07:43:44.282522 34682 solver.cpp:239] Iteration 73190 (1.59782 iter/s, 6.25852s/10 iters), loss = 7.78719
I0523 07:43:44.282579 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.78719 (* 1 = 7.78719 loss)
I0523 07:43:44.365237 34682 sgd_solver.cpp:112] Iteration 73190, lr = 0.01
I0523 07:43:48.124621 34682 solver.cpp:239] Iteration 73200 (2.60289 iter/s, 3.84188s/10 iters), loss = 7.94984
I0523 07:43:48.124940 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94984 (* 1 = 7.94984 loss)
I0523 07:43:48.918782 34682 sgd_solver.cpp:112] Iteration 73200, lr = 0.01
I0523 07:43:53.516194 34682 solver.cpp:239] Iteration 73210 (1.85491 iter/s, 5.39108s/10 iters), loss = 7.89449
I0523 07:43:53.516238 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89449 (* 1 = 7.89449 loss)
I0523 07:43:53.585237 34682 sgd_solver.cpp:112] Iteration 73210, lr = 0.01
I0523 07:43:56.344445 34682 solver.cpp:239] Iteration 73220 (3.53596 iter/s, 2.82809s/10 iters), loss = 7.64573
I0523 07:43:56.344489 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64573 (* 1 = 7.64573 loss)
I0523 07:43:56.413661 34682 sgd_solver.cpp:112] Iteration 73220, lr = 0.01
I0523 07:43:59.039834 34682 solver.cpp:239] Iteration 73230 (3.71026 iter/s, 2.69523s/10 iters), loss = 8.32528
I0523 07:43:59.039878 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32528 (* 1 = 8.32528 loss)
I0523 07:43:59.117702 34682 sgd_solver.cpp:112] Iteration 73230, lr = 0.01
I0523 07:44:03.247882 34682 solver.cpp:239] Iteration 73240 (2.37652 iter/s, 4.20783s/10 iters), loss = 8.35969
I0523 07:44:03.247931 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35969 (* 1 = 8.35969 loss)
I0523 07:44:03.311352 34682 sgd_solver.cpp:112] Iteration 73240, lr = 0.01
I0523 07:44:07.388310 34682 solver.cpp:239] Iteration 73250 (2.41534 iter/s, 4.1402s/10 iters), loss = 8.6211
I0523 07:44:07.388360 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6211 (* 1 = 8.6211 loss)
I0523 07:44:07.462898 34682 sgd_solver.cpp:112] Iteration 73250, lr = 0.01
I0523 07:44:11.103314 34682 solver.cpp:239] Iteration 73260 (2.69194 iter/s, 3.7148s/10 iters), loss = 7.47171
I0523 07:44:11.103372 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.47171 (* 1 = 7.47171 loss)
I0523 07:44:11.167104 34682 sgd_solver.cpp:112] Iteration 73260, lr = 0.01
I0523 07:44:14.862776 34682 solver.cpp:239] Iteration 73270 (2.6601 iter/s, 3.75925s/10 iters), loss = 8.69744
I0523 07:44:14.862835 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69744 (* 1 = 8.69744 loss)
I0523 07:44:15.727140 34682 sgd_solver.cpp:112] Iteration 73270, lr = 0.01
I0523 07:44:20.339254 34682 solver.cpp:239] Iteration 73280 (1.82608 iter/s, 5.4762s/10 iters), loss = 8.57388
I0523 07:44:20.339509 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57388 (* 1 = 8.57388 loss)
I0523 07:44:21.154256 34682 sgd_solver.cpp:112] Iteration 73280, lr = 0.01
I0523 07:44:25.688042 34682 solver.cpp:239] Iteration 73290 (1.86974 iter/s, 5.34835s/10 iters), loss = 8.38008
I0523 07:44:25.688093 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38008 (* 1 = 8.38008 loss)
I0523 07:44:25.760375 34682 sgd_solver.cpp:112] Iteration 73290, lr = 0.01
I0523 07:44:29.907852 34682 solver.cpp:239] Iteration 73300 (2.3699 iter/s, 4.21959s/10 iters), loss = 7.88679
I0523 07:44:29.907915 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88679 (* 1 = 7.88679 loss)
I0523 07:44:29.985405 34682 sgd_solver.cpp:112] Iteration 73300, lr = 0.01
I0523 07:44:34.876081 34682 solver.cpp:239] Iteration 73310 (2.0129 iter/s, 4.96797s/10 iters), loss = 9.1846
I0523 07:44:34.876132 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.1846 (* 1 = 9.1846 loss)
I0523 07:44:34.934576 34682 sgd_solver.cpp:112] Iteration 73310, lr = 0.01
I0523 07:44:38.340339 34682 solver.cpp:239] Iteration 73320 (2.8868 iter/s, 3.46405s/10 iters), loss = 6.53131
I0523 07:44:38.340399 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.53131 (* 1 = 6.53131 loss)
I0523 07:44:39.130534 34682 sgd_solver.cpp:112] Iteration 73320, lr = 0.01
I0523 07:44:43.165745 34682 solver.cpp:239] Iteration 73330 (2.07248 iter/s, 4.82513s/10 iters), loss = 8.05475
I0523 07:44:43.165787 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05475 (* 1 = 8.05475 loss)
I0523 07:44:43.237098 34682 sgd_solver.cpp:112] Iteration 73330, lr = 0.01
I0523 07:44:45.837924 34682 solver.cpp:239] Iteration 73340 (3.74249 iter/s, 2.67202s/10 iters), loss = 7.56
I0523 07:44:45.837970 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56 (* 1 = 7.56 loss)
I0523 07:44:46.654688 34682 sgd_solver.cpp:112] Iteration 73340, lr = 0.01
I0523 07:44:51.245263 34682 solver.cpp:239] Iteration 73350 (1.84943 iter/s, 5.40707s/10 iters), loss = 7.89299
I0523 07:44:51.245513 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89299 (* 1 = 7.89299 loss)
I0523 07:44:51.338735 34682 sgd_solver.cpp:112] Iteration 73350, lr = 0.01
I0523 07:44:54.076797 34682 solver.cpp:239] Iteration 73360 (3.53209 iter/s, 2.83119s/10 iters), loss = 7.25986
I0523 07:44:54.076850 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.25986 (* 1 = 7.25986 loss)
I0523 07:44:54.619678 34682 sgd_solver.cpp:112] Iteration 73360, lr = 0.01
I0523 07:44:59.870945 34682 solver.cpp:239] Iteration 73370 (1.72596 iter/s, 5.79386s/10 iters), loss = 7.55646
I0523 07:44:59.870990 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.55646 (* 1 = 7.55646 loss)
I0523 07:44:59.945298 34682 sgd_solver.cpp:112] Iteration 73370, lr = 0.01
I0523 07:45:04.661352 34682 solver.cpp:239] Iteration 73380 (2.08762 iter/s, 4.79015s/10 iters), loss = 7.56265
I0523 07:45:04.661408 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56265 (* 1 = 7.56265 loss)
I0523 07:45:05.446130 34682 sgd_solver.cpp:112] Iteration 73380, lr = 0.01
I0523 07:45:10.567014 34682 solver.cpp:239] Iteration 73390 (1.69337 iter/s, 5.90537s/10 iters), loss = 9.14372
I0523 07:45:10.567070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.14372 (* 1 = 9.14372 loss)
I0523 07:45:11.273571 34682 sgd_solver.cpp:112] Iteration 73390, lr = 0.01
I0523 07:45:15.929018 34682 solver.cpp:239] Iteration 73400 (1.86507 iter/s, 5.36173s/10 iters), loss = 7.77797
I0523 07:45:15.929066 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77797 (* 1 = 7.77797 loss)
I0523 07:45:16.518946 34682 sgd_solver.cpp:112] Iteration 73400, lr = 0.01
I0523 07:45:21.286854 34682 solver.cpp:239] Iteration 73410 (1.86652 iter/s, 5.35757s/10 iters), loss = 7.40703
I0523 07:45:21.287045 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.40703 (* 1 = 7.40703 loss)
I0523 07:45:21.367189 34682 sgd_solver.cpp:112] Iteration 73410, lr = 0.01
I0523 07:45:25.497083 34682 solver.cpp:239] Iteration 73420 (2.37536 iter/s, 4.20988s/10 iters), loss = 7.62279
I0523 07:45:25.497134 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62279 (* 1 = 7.62279 loss)
I0523 07:45:25.569615 34682 sgd_solver.cpp:112] Iteration 73420, lr = 0.01
I0523 07:45:28.981801 34682 solver.cpp:239] Iteration 73430 (2.86983 iter/s, 3.48452s/10 iters), loss = 7.34953
I0523 07:45:28.981848 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.34953 (* 1 = 7.34953 loss)
I0523 07:45:29.051257 34682 sgd_solver.cpp:112] Iteration 73430, lr = 0.01
I0523 07:45:35.291327 34682 solver.cpp:239] Iteration 73440 (1.58498 iter/s, 6.30922s/10 iters), loss = 7.3837
I0523 07:45:35.291380 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.3837 (* 1 = 7.3837 loss)
I0523 07:45:35.350121 34682 sgd_solver.cpp:112] Iteration 73440, lr = 0.01
I0523 07:45:40.033116 34682 solver.cpp:239] Iteration 73450 (2.10902 iter/s, 4.74154s/10 iters), loss = 7.25466
I0523 07:45:40.033161 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.25466 (* 1 = 7.25466 loss)
I0523 07:45:40.094378 34682 sgd_solver.cpp:112] Iteration 73450, lr = 0.01
I0523 07:45:44.378069 34682 solver.cpp:239] Iteration 73460 (2.30164 iter/s, 4.34473s/10 iters), loss = 8.05879
I0523 07:45:44.378114 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05879 (* 1 = 8.05879 loss)
I0523 07:45:44.438572 34682 sgd_solver.cpp:112] Iteration 73460, lr = 0.01
I0523 07:45:49.775542 34682 solver.cpp:239] Iteration 73470 (1.85281 iter/s, 5.3972s/10 iters), loss = 8.27635
I0523 07:45:49.775586 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27635 (* 1 = 8.27635 loss)
I0523 07:45:49.840250 34682 sgd_solver.cpp:112] Iteration 73470, lr = 0.01
I0523 07:45:53.810242 34682 solver.cpp:239] Iteration 73480 (2.47864 iter/s, 4.03448s/10 iters), loss = 7.60091
I0523 07:45:53.810534 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.60091 (* 1 = 7.60091 loss)
I0523 07:45:54.676594 34682 sgd_solver.cpp:112] Iteration 73480, lr = 0.01
I0523 07:46:00.904902 34682 solver.cpp:239] Iteration 73490 (1.40962 iter/s, 7.09411s/10 iters), loss = 7.8484
I0523 07:46:00.904944 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8484 (* 1 = 7.8484 loss)
I0523 07:46:00.974778 34682 sgd_solver.cpp:112] Iteration 73490, lr = 0.01
I0523 07:46:06.732383 34682 solver.cpp:239] Iteration 73500 (1.71609 iter/s, 5.8272s/10 iters), loss = 8.15095
I0523 07:46:06.732434 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15095 (* 1 = 8.15095 loss)
I0523 07:46:06.803184 34682 sgd_solver.cpp:112] Iteration 73500, lr = 0.01
I0523 07:46:10.645948 34682 solver.cpp:239] Iteration 73510 (2.55535 iter/s, 3.91335s/10 iters), loss = 7.70678
I0523 07:46:10.646003 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.70678 (* 1 = 7.70678 loss)
I0523 07:46:11.407408 34682 sgd_solver.cpp:112] Iteration 73510, lr = 0.01
I0523 07:46:16.389564 34682 solver.cpp:239] Iteration 73520 (1.74115 iter/s, 5.74333s/10 iters), loss = 7.8406
I0523 07:46:16.389611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8406 (* 1 = 7.8406 loss)
I0523 07:46:16.446823 34682 sgd_solver.cpp:112] Iteration 73520, lr = 0.01
I0523 07:46:21.602021 34682 solver.cpp:239] Iteration 73530 (1.91858 iter/s, 5.2122s/10 iters), loss = 8.42811
I0523 07:46:21.602082 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42811 (* 1 = 8.42811 loss)
I0523 07:46:21.670138 34682 sgd_solver.cpp:112] Iteration 73530, lr = 0.01
I0523 07:46:28.233793 34682 solver.cpp:239] Iteration 73540 (1.50797 iter/s, 6.63144s/10 iters), loss = 7.44365
I0523 07:46:28.234024 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44365 (* 1 = 7.44365 loss)
I0523 07:46:28.812782 34682 sgd_solver.cpp:112] Iteration 73540, lr = 0.01
I0523 07:46:35.353195 34682 solver.cpp:239] Iteration 73550 (1.40471 iter/s, 7.11891s/10 iters), loss = 8.24799
I0523 07:46:35.353263 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24799 (* 1 = 8.24799 loss)
I0523 07:46:36.048313 34682 sgd_solver.cpp:112] Iteration 73550, lr = 0.01
I0523 07:46:39.223590 34682 solver.cpp:239] Iteration 73560 (2.58387 iter/s, 3.87017s/10 iters), loss = 7.96261
I0523 07:46:39.223639 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96261 (* 1 = 7.96261 loss)
I0523 07:46:39.281323 34682 sgd_solver.cpp:112] Iteration 73560, lr = 0.01
I0523 07:46:44.912056 34682 solver.cpp:239] Iteration 73570 (1.75803 iter/s, 5.68818s/10 iters), loss = 8.00049
I0523 07:46:44.912117 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00049 (* 1 = 8.00049 loss)
I0523 07:46:44.990335 34682 sgd_solver.cpp:112] Iteration 73570, lr = 0.01
I0523 07:46:49.254055 34682 solver.cpp:239] Iteration 73580 (2.30321 iter/s, 4.34176s/10 iters), loss = 8.26226
I0523 07:46:49.254096 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26226 (* 1 = 8.26226 loss)
I0523 07:46:49.317669 34682 sgd_solver.cpp:112] Iteration 73580, lr = 0.01
I0523 07:46:53.337776 34682 solver.cpp:239] Iteration 73590 (2.44887 iter/s, 4.08351s/10 iters), loss = 8.27664
I0523 07:46:53.337828 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27664 (* 1 = 8.27664 loss)
I0523 07:46:53.768576 34682 sgd_solver.cpp:112] Iteration 73590, lr = 0.01
I0523 07:46:58.418064 34682 solver.cpp:239] Iteration 73600 (1.96849 iter/s, 5.08003s/10 iters), loss = 8.35294
I0523 07:46:58.418298 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35294 (* 1 = 8.35294 loss)
I0523 07:46:59.126541 34682 sgd_solver.cpp:112] Iteration 73600, lr = 0.01
I0523 07:47:05.135304 34682 solver.cpp:239] Iteration 73610 (1.48881 iter/s, 6.71676s/10 iters), loss = 7.42485
I0523 07:47:05.135365 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42485 (* 1 = 7.42485 loss)
I0523 07:47:05.944202 34682 sgd_solver.cpp:112] Iteration 73610, lr = 0.01
I0523 07:47:10.974334 34682 solver.cpp:239] Iteration 73620 (1.7127 iter/s, 5.83873s/10 iters), loss = 7.14643
I0523 07:47:10.974395 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.14643 (* 1 = 7.14643 loss)
I0523 07:47:11.804325 34682 sgd_solver.cpp:112] Iteration 73620, lr = 0.01
I0523 07:47:16.007002 34682 solver.cpp:239] Iteration 73630 (1.98713 iter/s, 5.03239s/10 iters), loss = 8.59219
I0523 07:47:16.007066 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59219 (* 1 = 8.59219 loss)
I0523 07:47:16.836827 34682 sgd_solver.cpp:112] Iteration 73630, lr = 0.01
I0523 07:47:20.997012 34682 solver.cpp:239] Iteration 73640 (2.00411 iter/s, 4.98974s/10 iters), loss = 8.12007
I0523 07:47:20.997058 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12007 (* 1 = 8.12007 loss)
I0523 07:47:21.070003 34682 sgd_solver.cpp:112] Iteration 73640, lr = 0.01
I0523 07:47:25.958302 34682 solver.cpp:239] Iteration 73650 (2.0157 iter/s, 4.96104s/10 iters), loss = 7.72968
I0523 07:47:25.958346 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72968 (* 1 = 7.72968 loss)
I0523 07:47:26.025849 34682 sgd_solver.cpp:112] Iteration 73650, lr = 0.01
I0523 07:47:31.678812 34682 solver.cpp:239] Iteration 73660 (1.74818 iter/s, 5.72023s/10 iters), loss = 8.04949
I0523 07:47:31.679038 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04949 (* 1 = 8.04949 loss)
I0523 07:47:32.481992 34682 sgd_solver.cpp:112] Iteration 73660, lr = 0.01
I0523 07:47:35.500591 34682 solver.cpp:239] Iteration 73670 (2.61684 iter/s, 3.8214s/10 iters), loss = 7.84553
I0523 07:47:35.500648 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84553 (* 1 = 7.84553 loss)
I0523 07:47:35.582973 34682 sgd_solver.cpp:112] Iteration 73670, lr = 0.01
I0523 07:47:40.723134 34682 solver.cpp:239] Iteration 73680 (1.91487 iter/s, 5.22228s/10 iters), loss = 7.67378
I0523 07:47:40.723178 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.67378 (* 1 = 7.67378 loss)
I0523 07:47:40.797513 34682 sgd_solver.cpp:112] Iteration 73680, lr = 0.01
I0523 07:47:45.748584 34682 solver.cpp:239] Iteration 73690 (1.98997 iter/s, 5.02519s/10 iters), loss = 8.42457
I0523 07:47:45.748636 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42457 (* 1 = 8.42457 loss)
I0523 07:47:45.821091 34682 sgd_solver.cpp:112] Iteration 73690, lr = 0.01
I0523 07:47:50.302247 34682 solver.cpp:239] Iteration 73700 (2.19615 iter/s, 4.55343s/10 iters), loss = 8.31791
I0523 07:47:50.302292 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31791 (* 1 = 8.31791 loss)
I0523 07:47:50.364519 34682 sgd_solver.cpp:112] Iteration 73700, lr = 0.01
I0523 07:47:52.956130 34682 solver.cpp:239] Iteration 73710 (3.7683 iter/s, 2.65372s/10 iters), loss = 7.57012
I0523 07:47:52.956182 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.57012 (* 1 = 7.57012 loss)
I0523 07:47:53.022588 34682 sgd_solver.cpp:112] Iteration 73710, lr = 0.01
I0523 07:47:57.520504 34682 solver.cpp:239] Iteration 73720 (2.191 iter/s, 4.56413s/10 iters), loss = 7.97935
I0523 07:47:57.520567 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97935 (* 1 = 7.97935 loss)
I0523 07:47:57.584651 34682 sgd_solver.cpp:112] Iteration 73720, lr = 0.01
I0523 07:48:03.028117 34682 solver.cpp:239] Iteration 73730 (1.81576 iter/s, 5.50733s/10 iters), loss = 7.09431
I0523 07:48:03.028334 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.09431 (* 1 = 7.09431 loss)
I0523 07:48:03.091243 34682 sgd_solver.cpp:112] Iteration 73730, lr = 0.01
I0523 07:48:08.170323 34682 solver.cpp:239] Iteration 73740 (1.94484 iter/s, 5.14181s/10 iters), loss = 8.0427
I0523 07:48:08.170375 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0427 (* 1 = 8.0427 loss)
I0523 07:48:08.970361 34682 sgd_solver.cpp:112] Iteration 73740, lr = 0.01
I0523 07:48:14.949892 34682 solver.cpp:239] Iteration 73750 (1.47509 iter/s, 6.77924s/10 iters), loss = 7.26639
I0523 07:48:14.949936 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.26639 (* 1 = 7.26639 loss)
I0523 07:48:15.011873 34682 sgd_solver.cpp:112] Iteration 73750, lr = 0.01
I0523 07:48:21.188995 34682 solver.cpp:239] Iteration 73760 (1.60287 iter/s, 6.23881s/10 iters), loss = 7.47469
I0523 07:48:21.189052 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.47469 (* 1 = 7.47469 loss)
I0523 07:48:21.937824 34682 sgd_solver.cpp:112] Iteration 73760, lr = 0.01
I0523 07:48:26.764820 34682 solver.cpp:239] Iteration 73770 (1.79355 iter/s, 5.57554s/10 iters), loss = 8.31051
I0523 07:48:26.764873 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31051 (* 1 = 8.31051 loss)
I0523 07:48:26.832329 34682 sgd_solver.cpp:112] Iteration 73770, lr = 0.01
I0523 07:48:29.467154 34682 solver.cpp:239] Iteration 73780 (3.70073 iter/s, 2.70217s/10 iters), loss = 7.89385
I0523 07:48:29.467193 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89385 (* 1 = 7.89385 loss)
I0523 07:48:29.529202 34682 sgd_solver.cpp:112] Iteration 73780, lr = 0.01
I0523 07:48:33.453138 34682 solver.cpp:239] Iteration 73790 (2.50892 iter/s, 3.98578s/10 iters), loss = 7.08325
I0523 07:48:33.453342 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.08325 (* 1 = 7.08325 loss)
I0523 07:48:33.521302 34682 sgd_solver.cpp:112] Iteration 73790, lr = 0.01
I0523 07:48:40.782472 34682 solver.cpp:239] Iteration 73800 (1.36447 iter/s, 7.32884s/10 iters), loss = 7.22506
I0523 07:48:40.782515 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.22506 (* 1 = 7.22506 loss)
I0523 07:48:41.381863 34682 sgd_solver.cpp:112] Iteration 73800, lr = 0.01
I0523 07:48:46.165438 34682 solver.cpp:239] Iteration 73810 (1.8578 iter/s, 5.3827s/10 iters), loss = 8.17164
I0523 07:48:46.165490 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17164 (* 1 = 8.17164 loss)
I0523 07:48:46.966166 34682 sgd_solver.cpp:112] Iteration 73810, lr = 0.01
I0523 07:48:51.728778 34682 solver.cpp:239] Iteration 73820 (1.79757 iter/s, 5.56306s/10 iters), loss = 6.9868
I0523 07:48:51.728827 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.9868 (* 1 = 6.9868 loss)
I0523 07:48:51.786484 34682 sgd_solver.cpp:112] Iteration 73820, lr = 0.01
I0523 07:48:56.633966 34682 solver.cpp:239] Iteration 73830 (2.03876 iter/s, 4.90494s/10 iters), loss = 8.28025
I0523 07:48:56.634013 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28025 (* 1 = 8.28025 loss)
I0523 07:48:56.710759 34682 sgd_solver.cpp:112] Iteration 73830, lr = 0.01
I0523 07:49:02.727521 34682 solver.cpp:239] Iteration 73840 (1.64116 iter/s, 6.09326s/10 iters), loss = 8.54767
I0523 07:49:02.727566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54767 (* 1 = 8.54767 loss)
I0523 07:49:02.799940 34682 sgd_solver.cpp:112] Iteration 73840, lr = 0.01
I0523 07:49:08.929113 34682 solver.cpp:239] Iteration 73850 (1.61257 iter/s, 6.2013s/10 iters), loss = 8.74272
I0523 07:49:08.929363 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74272 (* 1 = 8.74272 loss)
I0523 07:49:09.051326 34682 sgd_solver.cpp:112] Iteration 73850, lr = 0.01
I0523 07:49:12.467830 34682 solver.cpp:239] Iteration 73860 (2.82617 iter/s, 3.53836s/10 iters), loss = 8.4755
I0523 07:49:12.467875 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4755 (* 1 = 8.4755 loss)
I0523 07:49:13.293424 34682 sgd_solver.cpp:112] Iteration 73860, lr = 0.01
I0523 07:49:17.596452 34682 solver.cpp:239] Iteration 73870 (1.94994 iter/s, 5.12836s/10 iters), loss = 7.88535
I0523 07:49:17.596509 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88535 (* 1 = 7.88535 loss)
I0523 07:49:17.657582 34682 sgd_solver.cpp:112] Iteration 73870, lr = 0.01
I0523 07:49:21.645604 34682 solver.cpp:239] Iteration 73880 (2.46979 iter/s, 4.04893s/10 iters), loss = 7.50262
I0523 07:49:21.645647 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50262 (* 1 = 7.50262 loss)
I0523 07:49:21.706692 34682 sgd_solver.cpp:112] Iteration 73880, lr = 0.01
I0523 07:49:25.467340 34682 solver.cpp:239] Iteration 73890 (2.61675 iter/s, 3.82154s/10 iters), loss = 7.99139
I0523 07:49:25.467381 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99139 (* 1 = 7.99139 loss)
I0523 07:49:25.537811 34682 sgd_solver.cpp:112] Iteration 73890, lr = 0.01
I0523 07:49:29.313623 34682 solver.cpp:239] Iteration 73900 (2.60005 iter/s, 3.84608s/10 iters), loss = 7.14702
I0523 07:49:29.313666 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.14702 (* 1 = 7.14702 loss)
I0523 07:49:29.386140 34682 sgd_solver.cpp:112] Iteration 73900, lr = 0.01
I0523 07:49:33.408887 34682 solver.cpp:239] Iteration 73910 (2.44197 iter/s, 4.09505s/10 iters), loss = 8.80057
I0523 07:49:33.408923 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.80057 (* 1 = 8.80057 loss)
I0523 07:49:33.483085 34682 sgd_solver.cpp:112] Iteration 73910, lr = 0.01
I0523 07:49:38.340674 34682 solver.cpp:239] Iteration 73920 (2.02776 iter/s, 4.93155s/10 iters), loss = 7.96102
I0523 07:49:38.340716 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96102 (* 1 = 7.96102 loss)
I0523 07:49:38.405207 34682 sgd_solver.cpp:112] Iteration 73920, lr = 0.01
I0523 07:49:41.713946 34682 solver.cpp:239] Iteration 73930 (2.96465 iter/s, 3.37307s/10 iters), loss = 7.45915
I0523 07:49:41.714188 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.45915 (* 1 = 7.45915 loss)
I0523 07:49:42.476367 34682 sgd_solver.cpp:112] Iteration 73930, lr = 0.01
I0523 07:49:47.585778 34682 solver.cpp:239] Iteration 73940 (1.70318 iter/s, 5.87138s/10 iters), loss = 7.69216
I0523 07:49:47.585826 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69216 (* 1 = 7.69216 loss)
I0523 07:49:48.453457 34682 sgd_solver.cpp:112] Iteration 73940, lr = 0.01
I0523 07:49:53.049739 34682 solver.cpp:239] Iteration 73950 (1.83027 iter/s, 5.46368s/10 iters), loss = 8.17214
I0523 07:49:53.049813 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17214 (* 1 = 8.17214 loss)
I0523 07:49:53.113080 34682 sgd_solver.cpp:112] Iteration 73950, lr = 0.01
I0523 07:49:56.023739 34682 solver.cpp:239] Iteration 73960 (3.3627 iter/s, 2.9738s/10 iters), loss = 8.5766
I0523 07:49:56.023798 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.5766 (* 1 = 8.5766 loss)
I0523 07:49:56.778349 34682 sgd_solver.cpp:112] Iteration 73960, lr = 0.01
I0523 07:50:01.016129 34682 solver.cpp:239] Iteration 73970 (2.00315 iter/s, 4.99213s/10 iters), loss = 8.43165
I0523 07:50:01.016189 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43165 (* 1 = 8.43165 loss)
I0523 07:50:01.807034 34682 sgd_solver.cpp:112] Iteration 73970, lr = 0.01
I0523 07:50:07.554069 34682 solver.cpp:239] Iteration 73980 (1.52961 iter/s, 6.53761s/10 iters), loss = 7.82992
I0523 07:50:07.554116 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82992 (* 1 = 7.82992 loss)
I0523 07:50:07.632262 34682 sgd_solver.cpp:112] Iteration 73980, lr = 0.01
I0523 07:50:13.465674 34682 solver.cpp:239] Iteration 73990 (1.69167 iter/s, 5.91132s/10 iters), loss = 7.9892
I0523 07:50:13.465816 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9892 (* 1 = 7.9892 loss)
I0523 07:50:13.521989 34682 sgd_solver.cpp:112] Iteration 73990, lr = 0.01
I0523 07:50:18.600611 34682 solver.cpp:239] Iteration 74000 (1.94758 iter/s, 5.13458s/10 iters), loss = 7.46015
I0523 07:50:18.600659 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.46015 (* 1 = 7.46015 loss)
I0523 07:50:19.417017 34682 sgd_solver.cpp:112] Iteration 74000, lr = 0.01
I0523 07:50:23.864337 34682 solver.cpp:239] Iteration 74010 (1.89989 iter/s, 5.26346s/10 iters), loss = 7.42072
I0523 07:50:23.864393 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42072 (* 1 = 7.42072 loss)
I0523 07:50:23.934345 34682 sgd_solver.cpp:112] Iteration 74010, lr = 0.01
I0523 07:50:27.779709 34682 solver.cpp:239] Iteration 74020 (2.55418 iter/s, 3.91515s/10 iters), loss = 6.89639
I0523 07:50:27.779750 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.89639 (* 1 = 6.89639 loss)
I0523 07:50:27.858861 34682 sgd_solver.cpp:112] Iteration 74020, lr = 0.01
I0523 07:50:33.282232 34682 solver.cpp:239] Iteration 74030 (1.81744 iter/s, 5.50225s/10 iters), loss = 7.44046
I0523 07:50:33.282277 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44046 (* 1 = 7.44046 loss)
I0523 07:50:33.347103 34682 sgd_solver.cpp:112] Iteration 74030, lr = 0.01
I0523 07:50:38.212436 34682 solver.cpp:239] Iteration 74040 (2.02841 iter/s, 4.92996s/10 iters), loss = 8.49358
I0523 07:50:38.212486 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49358 (* 1 = 8.49358 loss)
I0523 07:50:39.042346 34682 sgd_solver.cpp:112] Iteration 74040, lr = 0.01
I0523 07:50:43.098382 34682 solver.cpp:239] Iteration 74050 (2.04679 iter/s, 4.88569s/10 iters), loss = 7.40561
I0523 07:50:43.098428 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.40561 (* 1 = 7.40561 loss)
I0523 07:50:43.173137 34682 sgd_solver.cpp:112] Iteration 74050, lr = 0.01
I0523 07:50:47.099258 34682 solver.cpp:239] Iteration 74060 (2.49959 iter/s, 4.00065s/10 iters), loss = 8.02612
I0523 07:50:47.099545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02612 (* 1 = 8.02612 loss)
I0523 07:50:47.928421 34682 sgd_solver.cpp:112] Iteration 74060, lr = 0.01
I0523 07:50:52.446636 34682 solver.cpp:239] Iteration 74070 (1.87024 iter/s, 5.3469s/10 iters), loss = 8.53615
I0523 07:50:52.446684 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53615 (* 1 = 8.53615 loss)
I0523 07:50:53.276437 34682 sgd_solver.cpp:112] Iteration 74070, lr = 0.01
I0523 07:50:57.255584 34682 solver.cpp:239] Iteration 74080 (2.07956 iter/s, 4.80871s/10 iters), loss = 8.54776
I0523 07:50:57.255635 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54776 (* 1 = 8.54776 loss)
I0523 07:50:58.069450 34682 sgd_solver.cpp:112] Iteration 74080, lr = 0.01
I0523 07:51:02.745770 34682 solver.cpp:239] Iteration 74090 (1.82152 iter/s, 5.48991s/10 iters), loss = 7.76204
I0523 07:51:02.745826 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.76204 (* 1 = 7.76204 loss)
I0523 07:51:03.519526 34682 sgd_solver.cpp:112] Iteration 74090, lr = 0.01
I0523 07:51:08.829217 34682 solver.cpp:239] Iteration 74100 (1.64389 iter/s, 6.08315s/10 iters), loss = 8.35615
I0523 07:51:08.829262 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35615 (* 1 = 8.35615 loss)
I0523 07:51:08.901865 34682 sgd_solver.cpp:112] Iteration 74100, lr = 0.01
I0523 07:51:12.757056 34682 solver.cpp:239] Iteration 74110 (2.54607 iter/s, 3.92763s/10 iters), loss = 7.03459
I0523 07:51:12.757097 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.03459 (* 1 = 7.03459 loss)
I0523 07:51:12.822710 34682 sgd_solver.cpp:112] Iteration 74110, lr = 0.01
I0523 07:51:18.900208 34682 solver.cpp:239] Iteration 74120 (1.62791 iter/s, 6.14286s/10 iters), loss = 7.04932
I0523 07:51:18.900427 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.04932 (* 1 = 7.04932 loss)
I0523 07:51:19.701553 34682 sgd_solver.cpp:112] Iteration 74120, lr = 0.01
I0523 07:51:23.894862 34682 solver.cpp:239] Iteration 74130 (2.0023 iter/s, 4.99425s/10 iters), loss = 6.77543
I0523 07:51:23.894908 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.77543 (* 1 = 6.77543 loss)
I0523 07:51:23.959214 34682 sgd_solver.cpp:112] Iteration 74130, lr = 0.01
I0523 07:51:28.073932 34682 solver.cpp:239] Iteration 74140 (2.393 iter/s, 4.17885s/10 iters), loss = 8.37679
I0523 07:51:28.073977 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37679 (* 1 = 8.37679 loss)
I0523 07:51:28.154850 34682 sgd_solver.cpp:112] Iteration 74140, lr = 0.01
I0523 07:51:31.562899 34682 solver.cpp:239] Iteration 74150 (2.86635 iter/s, 3.48876s/10 iters), loss = 7.86993
I0523 07:51:31.562958 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86993 (* 1 = 7.86993 loss)
I0523 07:51:31.636270 34682 sgd_solver.cpp:112] Iteration 74150, lr = 0.01
I0523 07:51:35.821933 34682 solver.cpp:239] Iteration 74160 (2.34808 iter/s, 4.2588s/10 iters), loss = 8.3005
I0523 07:51:35.821981 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3005 (* 1 = 8.3005 loss)
I0523 07:51:35.888465 34682 sgd_solver.cpp:112] Iteration 74160, lr = 0.01
I0523 07:51:40.439591 34682 solver.cpp:239] Iteration 74170 (2.16779 iter/s, 4.613s/10 iters), loss = 8.81657
I0523 07:51:40.439653 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81657 (* 1 = 8.81657 loss)
I0523 07:51:41.277047 34682 sgd_solver.cpp:112] Iteration 74170, lr = 0.01
I0523 07:51:46.433018 34682 solver.cpp:239] Iteration 74180 (1.66858 iter/s, 5.99313s/10 iters), loss = 8.21753
I0523 07:51:46.433061 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21753 (* 1 = 8.21753 loss)
I0523 07:51:46.501035 34682 sgd_solver.cpp:112] Iteration 74180, lr = 0.01
I0523 07:51:52.175596 34682 solver.cpp:239] Iteration 74190 (1.74146 iter/s, 5.7423s/10 iters), loss = 7.15441
I0523 07:51:52.175885 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.15441 (* 1 = 7.15441 loss)
I0523 07:51:52.229131 34682 sgd_solver.cpp:112] Iteration 74190, lr = 0.01
I0523 07:51:57.770853 34682 solver.cpp:239] Iteration 74200 (1.78738 iter/s, 5.59477s/10 iters), loss = 7.81314
I0523 07:51:57.770911 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81314 (* 1 = 7.81314 loss)
I0523 07:51:58.626410 34682 sgd_solver.cpp:112] Iteration 74200, lr = 0.01
I0523 07:52:03.529235 34682 solver.cpp:239] Iteration 74210 (1.73669 iter/s, 5.75809s/10 iters), loss = 8.25913
I0523 07:52:03.529284 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25913 (* 1 = 8.25913 loss)
I0523 07:52:03.596760 34682 sgd_solver.cpp:112] Iteration 74210, lr = 0.01
I0523 07:52:08.090816 34682 solver.cpp:239] Iteration 74220 (2.19234 iter/s, 4.56133s/10 iters), loss = 7.0852
I0523 07:52:08.090883 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.0852 (* 1 = 7.0852 loss)
I0523 07:52:08.337077 34682 sgd_solver.cpp:112] Iteration 74220, lr = 0.01
I0523 07:52:12.565930 34682 solver.cpp:239] Iteration 74230 (2.23471 iter/s, 4.47486s/10 iters), loss = 7.33584
I0523 07:52:12.566004 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.33584 (* 1 = 7.33584 loss)
I0523 07:52:13.352361 34682 sgd_solver.cpp:112] Iteration 74230, lr = 0.01
I0523 07:52:17.310047 34682 solver.cpp:239] Iteration 74240 (2.10799 iter/s, 4.74385s/10 iters), loss = 8.58345
I0523 07:52:17.310087 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58345 (* 1 = 8.58345 loss)
I0523 07:52:17.389621 34682 sgd_solver.cpp:112] Iteration 74240, lr = 0.01
I0523 07:52:21.462503 34682 solver.cpp:239] Iteration 74250 (2.40834 iter/s, 4.15224s/10 iters), loss = 7.64712
I0523 07:52:21.462549 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64712 (* 1 = 7.64712 loss)
I0523 07:52:22.329813 34682 sgd_solver.cpp:112] Iteration 74250, lr = 0.01
I0523 07:52:27.108558 34682 solver.cpp:239] Iteration 74260 (1.77124 iter/s, 5.64577s/10 iters), loss = 6.77116
I0523 07:52:27.108613 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.77116 (* 1 = 6.77116 loss)
I0523 07:52:27.178215 34682 sgd_solver.cpp:112] Iteration 74260, lr = 0.01
I0523 07:52:30.734552 34682 solver.cpp:239] Iteration 74270 (2.75802 iter/s, 3.62579s/10 iters), loss = 7.60391
I0523 07:52:30.734599 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.60391 (* 1 = 7.60391 loss)
I0523 07:52:30.805104 34682 sgd_solver.cpp:112] Iteration 74270, lr = 0.01
I0523 07:52:34.912509 34682 solver.cpp:239] Iteration 74280 (2.39365 iter/s, 4.17772s/10 iters), loss = 7.08146
I0523 07:52:34.912587 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.08146 (* 1 = 7.08146 loss)
I0523 07:52:35.276513 34682 sgd_solver.cpp:112] Iteration 74280, lr = 0.01
I0523 07:52:38.711992 34682 solver.cpp:239] Iteration 74290 (2.6321 iter/s, 3.79925s/10 iters), loss = 7.92568
I0523 07:52:38.712045 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92568 (* 1 = 7.92568 loss)
I0523 07:52:39.349398 34682 sgd_solver.cpp:112] Iteration 74290, lr = 0.01
I0523 07:52:42.007498 34682 solver.cpp:239] Iteration 74300 (3.03462 iter/s, 3.29531s/10 iters), loss = 8.32731
I0523 07:52:42.007560 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32731 (* 1 = 8.32731 loss)
I0523 07:52:42.522673 34682 sgd_solver.cpp:112] Iteration 74300, lr = 0.01
I0523 07:52:48.251677 34682 solver.cpp:239] Iteration 74310 (1.60157 iter/s, 6.24387s/10 iters), loss = 7.99679
I0523 07:52:48.251724 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99679 (* 1 = 7.99679 loss)
I0523 07:52:48.324482 34682 sgd_solver.cpp:112] Iteration 74310, lr = 0.01
I0523 07:52:54.178175 34682 solver.cpp:239] Iteration 74320 (1.68742 iter/s, 5.92621s/10 iters), loss = 7.93826
I0523 07:52:54.178370 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93826 (* 1 = 7.93826 loss)
I0523 07:52:54.255723 34682 sgd_solver.cpp:112] Iteration 74320, lr = 0.01
I0523 07:52:58.489714 34682 solver.cpp:239] Iteration 74330 (2.31957 iter/s, 4.31115s/10 iters), loss = 8.19385
I0523 07:52:58.489801 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19385 (* 1 = 8.19385 loss)
I0523 07:52:59.266306 34682 sgd_solver.cpp:112] Iteration 74330, lr = 0.01
I0523 07:53:02.562520 34682 solver.cpp:239] Iteration 74340 (2.45546 iter/s, 4.07255s/10 iters), loss = 6.95184
I0523 07:53:02.562582 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.95184 (* 1 = 6.95184 loss)
I0523 07:53:02.629376 34682 sgd_solver.cpp:112] Iteration 74340, lr = 0.01
I0523 07:53:07.490942 34682 solver.cpp:239] Iteration 74350 (2.02916 iter/s, 4.92815s/10 iters), loss = 7.33789
I0523 07:53:07.491009 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.33789 (* 1 = 7.33789 loss)
I0523 07:53:07.556310 34682 sgd_solver.cpp:112] Iteration 74350, lr = 0.01
I0523 07:53:12.095904 34682 solver.cpp:239] Iteration 74360 (2.17169 iter/s, 4.60471s/10 iters), loss = 7.82359
I0523 07:53:12.095952 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82359 (* 1 = 7.82359 loss)
I0523 07:53:12.164484 34682 sgd_solver.cpp:112] Iteration 74360, lr = 0.01
I0523 07:53:15.639075 34682 solver.cpp:239] Iteration 74370 (2.82249 iter/s, 3.54297s/10 iters), loss = 7.64216
I0523 07:53:15.639125 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64216 (* 1 = 7.64216 loss)
I0523 07:53:16.451290 34682 sgd_solver.cpp:112] Iteration 74370, lr = 0.01
I0523 07:53:23.404227 34682 solver.cpp:239] Iteration 74380 (1.28787 iter/s, 7.76476s/10 iters), loss = 7.1078
I0523 07:53:23.404274 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.1078 (* 1 = 7.1078 loss)
I0523 07:53:24.132393 34682 sgd_solver.cpp:112] Iteration 74380, lr = 0.01
I0523 07:53:27.398110 34682 solver.cpp:239] Iteration 74390 (2.50396 iter/s, 3.99367s/10 iters), loss = 7.04533
I0523 07:53:27.398319 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.04533 (* 1 = 7.04533 loss)
I0523 07:53:28.259869 34682 sgd_solver.cpp:112] Iteration 74390, lr = 0.01
I0523 07:53:33.443442 34682 solver.cpp:239] Iteration 74400 (1.65429 iter/s, 6.04487s/10 iters), loss = 7.1908
I0523 07:53:33.443503 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.1908 (* 1 = 7.1908 loss)
I0523 07:53:33.500094 34682 sgd_solver.cpp:112] Iteration 74400, lr = 0.01
I0523 07:53:35.587949 34682 solver.cpp:239] Iteration 74410 (4.66341 iter/s, 2.14435s/10 iters), loss = 7.89706
I0523 07:53:35.587996 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89706 (* 1 = 7.89706 loss)
I0523 07:53:35.667131 34682 sgd_solver.cpp:112] Iteration 74410, lr = 0.01
I0523 07:53:39.535959 34682 solver.cpp:239] Iteration 74420 (2.53306 iter/s, 3.94779s/10 iters), loss = 7.47259
I0523 07:53:39.536017 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.47259 (* 1 = 7.47259 loss)
I0523 07:53:39.604593 34682 sgd_solver.cpp:112] Iteration 74420, lr = 0.01
I0523 07:53:44.437996 34682 solver.cpp:239] Iteration 74430 (2.04008 iter/s, 4.90178s/10 iters), loss = 7.63976
I0523 07:53:44.438060 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63976 (* 1 = 7.63976 loss)
I0523 07:53:45.262142 34682 sgd_solver.cpp:112] Iteration 74430, lr = 0.01
I0523 07:53:51.179036 34682 solver.cpp:239] Iteration 74440 (1.48352 iter/s, 6.74071s/10 iters), loss = 8.95515
I0523 07:53:51.179088 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95515 (* 1 = 8.95515 loss)
I0523 07:53:52.012064 34682 sgd_solver.cpp:112] Iteration 74440, lr = 0.01
I0523 07:53:56.516413 34682 solver.cpp:239] Iteration 74450 (1.87367 iter/s, 5.33711s/10 iters), loss = 7.54207
I0523 07:53:56.516468 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54207 (* 1 = 7.54207 loss)
I0523 07:53:56.581936 34682 sgd_solver.cpp:112] Iteration 74450, lr = 0.01
I0523 07:54:01.008646 34682 solver.cpp:239] Iteration 74460 (2.22618 iter/s, 4.49199s/10 iters), loss = 7.21566
I0523 07:54:01.008924 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.21566 (* 1 = 7.21566 loss)
I0523 07:54:01.072166 34682 sgd_solver.cpp:112] Iteration 74460, lr = 0.01
I0523 07:54:05.956063 34682 solver.cpp:239] Iteration 74470 (2.02144 iter/s, 4.94696s/10 iters), loss = 7.25613
I0523 07:54:05.956101 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.25613 (* 1 = 7.25613 loss)
I0523 07:54:06.029943 34682 sgd_solver.cpp:112] Iteration 74470, lr = 0.01
I0523 07:54:11.034421 34682 solver.cpp:239] Iteration 74480 (1.96924 iter/s, 5.07811s/10 iters), loss = 8.76702
I0523 07:54:11.034466 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76702 (* 1 = 8.76702 loss)
I0523 07:54:11.103626 34682 sgd_solver.cpp:112] Iteration 74480, lr = 0.01
I0523 07:54:16.830987 34682 solver.cpp:239] Iteration 74490 (1.72524 iter/s, 5.79628s/10 iters), loss = 8.34087
I0523 07:54:16.831030 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34087 (* 1 = 8.34087 loss)
I0523 07:54:16.892009 34682 sgd_solver.cpp:112] Iteration 74490, lr = 0.01
I0523 07:54:20.720626 34682 solver.cpp:239] Iteration 74500 (2.57107 iter/s, 3.88944s/10 iters), loss = 8.43372
I0523 07:54:20.720676 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43372 (* 1 = 8.43372 loss)
I0523 07:54:20.783396 34682 sgd_solver.cpp:112] Iteration 74500, lr = 0.01
I0523 07:54:23.567045 34682 solver.cpp:239] Iteration 74510 (3.5134 iter/s, 2.84625s/10 iters), loss = 8.26775
I0523 07:54:23.567090 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26775 (* 1 = 8.26775 loss)
I0523 07:54:24.371225 34682 sgd_solver.cpp:112] Iteration 74510, lr = 0.01
I0523 07:54:27.103780 34682 solver.cpp:239] Iteration 74520 (2.82762 iter/s, 3.53654s/10 iters), loss = 7.1047
I0523 07:54:27.103823 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.1047 (* 1 = 7.1047 loss)
I0523 07:54:27.167882 34682 sgd_solver.cpp:112] Iteration 74520, lr = 0.01
I0523 07:54:33.649375 34682 solver.cpp:239] Iteration 74530 (1.52782 iter/s, 6.54528s/10 iters), loss = 7.73853
I0523 07:54:33.649643 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.73853 (* 1 = 7.73853 loss)
I0523 07:54:33.709764 34682 sgd_solver.cpp:112] Iteration 74530, lr = 0.01
I0523 07:54:40.261366 34682 solver.cpp:239] Iteration 74540 (1.51252 iter/s, 6.6115s/10 iters), loss = 8.54389
I0523 07:54:40.261407 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54389 (* 1 = 8.54389 loss)
I0523 07:54:40.330031 34682 sgd_solver.cpp:112] Iteration 74540, lr = 0.01
I0523 07:54:45.367046 34682 solver.cpp:239] Iteration 74550 (1.9587 iter/s, 5.10542s/10 iters), loss = 8.09406
I0523 07:54:45.367115 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09406 (* 1 = 8.09406 loss)
I0523 07:54:45.981801 34682 sgd_solver.cpp:112] Iteration 74550, lr = 0.01
I0523 07:54:50.023780 34682 solver.cpp:239] Iteration 74560 (2.14755 iter/s, 4.65647s/10 iters), loss = 7.01032
I0523 07:54:50.023849 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.01032 (* 1 = 7.01032 loss)
I0523 07:54:50.098632 34682 sgd_solver.cpp:112] Iteration 74560, lr = 0.01
I0523 07:54:55.476119 34682 solver.cpp:239] Iteration 74570 (1.83417 iter/s, 5.45205s/10 iters), loss = 8.37999
I0523 07:54:55.476181 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37999 (* 1 = 8.37999 loss)
I0523 07:54:55.531764 34682 sgd_solver.cpp:112] Iteration 74570, lr = 0.01
I0523 07:55:01.296660 34682 solver.cpp:239] Iteration 74580 (1.71814 iter/s, 5.82024s/10 iters), loss = 7.64508
I0523 07:55:01.296711 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64508 (* 1 = 7.64508 loss)
I0523 07:55:01.363883 34682 sgd_solver.cpp:112] Iteration 74580, lr = 0.01
I0523 07:55:05.309774 34682 solver.cpp:239] Iteration 74590 (2.49197 iter/s, 4.0129s/10 iters), loss = 8.3946
I0523 07:55:05.310024 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3946 (* 1 = 8.3946 loss)
I0523 07:55:05.387646 34682 sgd_solver.cpp:112] Iteration 74590, lr = 0.01
I0523 07:55:11.072016 34682 solver.cpp:239] Iteration 74600 (1.73557 iter/s, 5.7618s/10 iters), loss = 8.91034
I0523 07:55:11.072062 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91034 (* 1 = 8.91034 loss)
I0523 07:55:11.850718 34682 sgd_solver.cpp:112] Iteration 74600, lr = 0.01
I0523 07:55:15.343219 34682 solver.cpp:239] Iteration 74610 (2.34138 iter/s, 4.27098s/10 iters), loss = 8.73048
I0523 07:55:15.343271 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.73048 (* 1 = 8.73048 loss)
I0523 07:55:15.433601 34682 sgd_solver.cpp:112] Iteration 74610, lr = 0.01
I0523 07:55:18.207512 34682 solver.cpp:239] Iteration 74620 (3.49147 iter/s, 2.86412s/10 iters), loss = 8.2909
I0523 07:55:18.207559 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2909 (* 1 = 8.2909 loss)
I0523 07:55:18.822871 34682 sgd_solver.cpp:112] Iteration 74620, lr = 0.01
I0523 07:55:21.322095 34682 solver.cpp:239] Iteration 74630 (3.21089 iter/s, 3.1144s/10 iters), loss = 7.06346
I0523 07:55:21.322140 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.06346 (* 1 = 7.06346 loss)
I0523 07:55:22.194059 34682 sgd_solver.cpp:112] Iteration 74630, lr = 0.01
I0523 07:55:27.767537 34682 solver.cpp:239] Iteration 74640 (1.55156 iter/s, 6.44514s/10 iters), loss = 8.85406
I0523 07:55:27.767576 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.85406 (* 1 = 8.85406 loss)
I0523 07:55:27.840276 34682 sgd_solver.cpp:112] Iteration 74640, lr = 0.01
I0523 07:55:33.467550 34682 solver.cpp:239] Iteration 74650 (1.75447 iter/s, 5.69974s/10 iters), loss = 8.24133
I0523 07:55:33.467597 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24133 (* 1 = 8.24133 loss)
I0523 07:55:34.183065 34682 sgd_solver.cpp:112] Iteration 74650, lr = 0.01
I0523 07:55:41.363131 34682 solver.cpp:239] Iteration 74660 (1.26659 iter/s, 7.89522s/10 iters), loss = 7.6627
I0523 07:55:41.363337 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6627 (* 1 = 7.6627 loss)
I0523 07:55:41.431349 34682 sgd_solver.cpp:112] Iteration 74660, lr = 0.01
I0523 07:55:45.520957 34682 solver.cpp:239] Iteration 74670 (2.40531 iter/s, 4.15746s/10 iters), loss = 7.29315
I0523 07:55:45.521013 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.29315 (* 1 = 7.29315 loss)
I0523 07:55:46.119190 34682 sgd_solver.cpp:112] Iteration 74670, lr = 0.01
I0523 07:55:48.921550 34682 solver.cpp:239] Iteration 74680 (2.94084 iter/s, 3.40039s/10 iters), loss = 7.5608
I0523 07:55:48.921599 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5608 (* 1 = 7.5608 loss)
I0523 07:55:49.003772 34682 sgd_solver.cpp:112] Iteration 74680, lr = 0.01
I0523 07:55:53.168525 34682 solver.cpp:239] Iteration 74690 (2.35474 iter/s, 4.24675s/10 iters), loss = 8.0537
I0523 07:55:53.168568 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0537 (* 1 = 8.0537 loss)
I0523 07:55:53.246176 34682 sgd_solver.cpp:112] Iteration 74690, lr = 0.01
I0523 07:55:59.353637 34682 solver.cpp:239] Iteration 74700 (1.61686 iter/s, 6.18482s/10 iters), loss = 7.57111
I0523 07:55:59.353673 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.57111 (* 1 = 7.57111 loss)
I0523 07:55:59.974267 34682 sgd_solver.cpp:112] Iteration 74700, lr = 0.01
I0523 07:56:05.608141 34682 solver.cpp:239] Iteration 74710 (1.59892 iter/s, 6.25421s/10 iters), loss = 8.53987
I0523 07:56:05.608191 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53987 (* 1 = 8.53987 loss)
I0523 07:56:05.688066 34682 sgd_solver.cpp:112] Iteration 74710, lr = 0.01
I0523 07:56:12.885673 34682 solver.cpp:239] Iteration 74720 (1.37416 iter/s, 7.27719s/10 iters), loss = 7.32616
I0523 07:56:12.885980 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.32616 (* 1 = 7.32616 loss)
I0523 07:56:13.723239 34682 sgd_solver.cpp:112] Iteration 74720, lr = 0.01
I0523 07:56:18.668187 34682 solver.cpp:239] Iteration 74730 (1.7295 iter/s, 5.78201s/10 iters), loss = 7.11496
I0523 07:56:18.668241 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.11496 (* 1 = 7.11496 loss)
I0523 07:56:19.492391 34682 sgd_solver.cpp:112] Iteration 74730, lr = 0.01
I0523 07:56:25.781785 34682 solver.cpp:239] Iteration 74740 (1.40583 iter/s, 7.11324s/10 iters), loss = 7.32753
I0523 07:56:25.781847 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.32753 (* 1 = 7.32753 loss)
I0523 07:56:26.571012 34682 sgd_solver.cpp:112] Iteration 74740, lr = 0.01
I0523 07:56:30.989547 34682 solver.cpp:239] Iteration 74750 (1.92031 iter/s, 5.20748s/10 iters), loss = 8.32977
I0523 07:56:30.989608 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32977 (* 1 = 8.32977 loss)
I0523 07:56:31.046542 34682 sgd_solver.cpp:112] Iteration 74750, lr = 0.01
I0523 07:56:35.250600 34682 solver.cpp:239] Iteration 74760 (2.34697 iter/s, 4.26082s/10 iters), loss = 8.18222
I0523 07:56:35.250651 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18222 (* 1 = 8.18222 loss)
I0523 07:56:35.312826 34682 sgd_solver.cpp:112] Iteration 74760, lr = 0.01
I0523 07:56:39.559914 34682 solver.cpp:239] Iteration 74770 (2.32068 iter/s, 4.30908s/10 iters), loss = 8.1199
I0523 07:56:39.559973 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1199 (* 1 = 8.1199 loss)
I0523 07:56:40.382690 34682 sgd_solver.cpp:112] Iteration 74770, lr = 0.01
I0523 07:56:44.529407 34682 solver.cpp:239] Iteration 74780 (2.01238 iter/s, 4.96924s/10 iters), loss = 8.55961
I0523 07:56:44.529614 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.55961 (* 1 = 8.55961 loss)
I0523 07:56:44.593786 34682 sgd_solver.cpp:112] Iteration 74780, lr = 0.01
I0523 07:56:50.232376 34682 solver.cpp:239] Iteration 74790 (1.7536 iter/s, 5.70256s/10 iters), loss = 8.47788
I0523 07:56:50.232419 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47788 (* 1 = 8.47788 loss)
I0523 07:56:50.308750 34682 sgd_solver.cpp:112] Iteration 74790, lr = 0.01
I0523 07:56:54.427117 34682 solver.cpp:239] Iteration 74800 (2.38406 iter/s, 4.19453s/10 iters), loss = 7.49422
I0523 07:56:54.427168 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.49422 (* 1 = 7.49422 loss)
I0523 07:56:55.150384 34682 sgd_solver.cpp:112] Iteration 74800, lr = 0.01
I0523 07:56:58.284809 34682 solver.cpp:239] Iteration 74810 (2.59237 iter/s, 3.85747s/10 iters), loss = 6.94153
I0523 07:56:58.284859 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.94153 (* 1 = 6.94153 loss)
I0523 07:56:59.086681 34682 sgd_solver.cpp:112] Iteration 74810, lr = 0.01
I0523 07:57:01.761441 34682 solver.cpp:239] Iteration 74820 (2.87651 iter/s, 3.47644s/10 iters), loss = 7.64041
I0523 07:57:01.761487 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64041 (* 1 = 7.64041 loss)
I0523 07:57:01.821523 34682 sgd_solver.cpp:112] Iteration 74820, lr = 0.01
I0523 07:57:05.123373 34682 solver.cpp:239] Iteration 74830 (2.97465 iter/s, 3.36174s/10 iters), loss = 7.77886
I0523 07:57:05.123412 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77886 (* 1 = 7.77886 loss)
I0523 07:57:05.197657 34682 sgd_solver.cpp:112] Iteration 74830, lr = 0.01
I0523 07:57:09.364018 34682 solver.cpp:239] Iteration 74840 (2.35825 iter/s, 4.24043s/10 iters), loss = 8.15359
I0523 07:57:09.364068 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15359 (* 1 = 8.15359 loss)
I0523 07:57:10.213544 34682 sgd_solver.cpp:112] Iteration 74840, lr = 0.01
I0523 07:57:12.731425 34682 solver.cpp:239] Iteration 74850 (2.96981 iter/s, 3.36722s/10 iters), loss = 8.54121
I0523 07:57:12.731473 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54121 (* 1 = 8.54121 loss)
I0523 07:57:12.792171 34682 sgd_solver.cpp:112] Iteration 74850, lr = 0.01
I0523 07:57:17.418459 34682 solver.cpp:239] Iteration 74860 (2.13366 iter/s, 4.68679s/10 iters), loss = 7.27211
I0523 07:57:17.418748 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.27211 (* 1 = 7.27211 loss)
I0523 07:57:17.483047 34682 sgd_solver.cpp:112] Iteration 74860, lr = 0.01
I0523 07:57:21.280805 34682 solver.cpp:239] Iteration 74870 (2.58937 iter/s, 3.86195s/10 iters), loss = 8.02602
I0523 07:57:21.280848 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02602 (* 1 = 8.02602 loss)
I0523 07:57:21.345829 34682 sgd_solver.cpp:112] Iteration 74870, lr = 0.01
I0523 07:57:25.616888 34682 solver.cpp:239] Iteration 74880 (2.30635 iter/s, 4.33586s/10 iters), loss = 8.02662
I0523 07:57:25.616937 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02662 (* 1 = 8.02662 loss)
I0523 07:57:25.688379 34682 sgd_solver.cpp:112] Iteration 74880, lr = 0.01
I0523 07:57:29.523526 34682 solver.cpp:239] Iteration 74890 (2.56176 iter/s, 3.90356s/10 iters), loss = 7.87213
I0523 07:57:29.523584 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87213 (* 1 = 7.87213 loss)
I0523 07:57:29.584162 34682 sgd_solver.cpp:112] Iteration 74890, lr = 0.01
I0523 07:57:32.838670 34682 solver.cpp:239] Iteration 74900 (3.01664 iter/s, 3.31495s/10 iters), loss = 8.70086
I0523 07:57:32.838742 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70086 (* 1 = 8.70086 loss)
I0523 07:57:33.636037 34682 sgd_solver.cpp:112] Iteration 74900, lr = 0.01
I0523 07:57:37.576179 34682 solver.cpp:239] Iteration 74910 (2.11093 iter/s, 4.73724s/10 iters), loss = 8.69466
I0523 07:57:37.576222 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.69466 (* 1 = 8.69466 loss)
I0523 07:57:38.438436 34682 sgd_solver.cpp:112] Iteration 74910, lr = 0.01
I0523 07:57:41.874224 34682 solver.cpp:239] Iteration 74920 (2.32676 iter/s, 4.29782s/10 iters), loss = 8.41975
I0523 07:57:41.874269 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41975 (* 1 = 8.41975 loss)
I0523 07:57:41.934902 34682 sgd_solver.cpp:112] Iteration 74920, lr = 0.01
I0523 07:57:47.841176 34682 solver.cpp:239] Iteration 74930 (1.67598 iter/s, 5.96666s/10 iters), loss = 7.89037
I0523 07:57:47.841420 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89037 (* 1 = 7.89037 loss)
I0523 07:57:48.550554 34682 sgd_solver.cpp:112] Iteration 74930, lr = 0.01
I0523 07:57:53.295711 34682 solver.cpp:239] Iteration 74940 (1.83348 iter/s, 5.4541s/10 iters), loss = 7.89682
I0523 07:57:53.295759 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89682 (* 1 = 7.89682 loss)
I0523 07:57:54.137357 34682 sgd_solver.cpp:112] Iteration 74940, lr = 0.01
I0523 07:57:59.995868 34682 solver.cpp:239] Iteration 74950 (1.49257 iter/s, 6.69984s/10 iters), loss = 7.07225
I0523 07:57:59.995908 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.07225 (* 1 = 7.07225 loss)
I0523 07:58:00.749414 34682 sgd_solver.cpp:112] Iteration 74950, lr = 0.01
I0523 07:58:05.073570 34682 solver.cpp:239] Iteration 74960 (1.96949 iter/s, 5.07746s/10 iters), loss = 7.95322
I0523 07:58:05.073621 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95322 (* 1 = 7.95322 loss)
I0523 07:58:05.905872 34682 sgd_solver.cpp:112] Iteration 74960, lr = 0.01
I0523 07:58:11.899539 34682 solver.cpp:239] Iteration 74970 (1.46506 iter/s, 6.82564s/10 iters), loss = 6.57795
I0523 07:58:11.899585 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.57795 (* 1 = 6.57795 loss)
I0523 07:58:11.959511 34682 sgd_solver.cpp:112] Iteration 74970, lr = 0.01
I0523 07:58:16.997040 34682 solver.cpp:239] Iteration 74980 (1.96184 iter/s, 5.09725s/10 iters), loss = 7.72044
I0523 07:58:16.997099 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72044 (* 1 = 7.72044 loss)
I0523 07:58:17.054828 34682 sgd_solver.cpp:112] Iteration 74980, lr = 0.01
I0523 07:58:21.645514 34682 solver.cpp:239] Iteration 74990 (2.15136 iter/s, 4.64822s/10 iters), loss = 7.55893
I0523 07:58:21.645772 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.55893 (* 1 = 7.55893 loss)
I0523 07:58:21.705701 34682 sgd_solver.cpp:112] Iteration 74990, lr = 0.01
I0523 07:58:24.346526 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_75000.caffemodel
I0523 07:58:25.579808 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_75000.solverstate
I0523 07:58:25.856364 34682 solver.cpp:239] Iteration 75000 (2.37504 iter/s, 4.21045s/10 iters), loss = 8.00085
I0523 07:58:25.856408 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00085 (* 1 = 8.00085 loss)
I0523 07:58:26.443318 34682 sgd_solver.cpp:112] Iteration 75000, lr = 0.01
I0523 07:58:29.693838 34682 solver.cpp:239] Iteration 75010 (2.60602 iter/s, 3.83727s/10 iters), loss = 7.80182
I0523 07:58:29.693881 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80182 (* 1 = 7.80182 loss)
I0523 07:58:29.758707 34682 sgd_solver.cpp:112] Iteration 75010, lr = 0.01
I0523 07:58:33.845029 34682 solver.cpp:239] Iteration 75020 (2.40908 iter/s, 4.15097s/10 iters), loss = 8.37534
I0523 07:58:33.845096 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37534 (* 1 = 8.37534 loss)
I0523 07:58:34.704638 34682 sgd_solver.cpp:112] Iteration 75020, lr = 0.01
I0523 07:58:40.949271 34682 solver.cpp:239] Iteration 75030 (1.40768 iter/s, 7.10389s/10 iters), loss = 7.31209
I0523 07:58:40.949327 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.31209 (* 1 = 7.31209 loss)
I0523 07:58:41.697847 34682 sgd_solver.cpp:112] Iteration 75030, lr = 0.01
I0523 07:58:45.880436 34682 solver.cpp:239] Iteration 75040 (2.02803 iter/s, 4.93091s/10 iters), loss = 7.17789
I0523 07:58:45.880486 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.17789 (* 1 = 7.17789 loss)
I0523 07:58:46.710258 34682 sgd_solver.cpp:112] Iteration 75040, lr = 0.01
I0523 07:58:52.441164 34682 solver.cpp:239] Iteration 75050 (1.5243 iter/s, 6.5604s/10 iters), loss = 7.80946
I0523 07:58:52.441324 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80946 (* 1 = 7.80946 loss)
I0523 07:58:52.511984 34682 sgd_solver.cpp:112] Iteration 75050, lr = 0.01
I0523 07:58:57.213443 34682 solver.cpp:239] Iteration 75060 (2.09559 iter/s, 4.77193s/10 iters), loss = 7.10043
I0523 07:58:57.213492 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.10043 (* 1 = 7.10043 loss)
I0523 07:58:58.109206 34682 sgd_solver.cpp:112] Iteration 75060, lr = 0.01
I0523 07:59:03.168717 34682 solver.cpp:239] Iteration 75070 (1.67927 iter/s, 5.95498s/10 iters), loss = 8.12694
I0523 07:59:03.168766 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12694 (* 1 = 8.12694 loss)
I0523 07:59:03.245308 34682 sgd_solver.cpp:112] Iteration 75070, lr = 0.01
I0523 07:59:07.435340 34682 solver.cpp:239] Iteration 75080 (2.3439 iter/s, 4.2664s/10 iters), loss = 8.59456
I0523 07:59:07.435390 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59456 (* 1 = 8.59456 loss)
I0523 07:59:07.490113 34682 sgd_solver.cpp:112] Iteration 75080, lr = 0.01
I0523 07:59:13.193230 34682 solver.cpp:239] Iteration 75090 (1.73683 iter/s, 5.7576s/10 iters), loss = 8.28165
I0523 07:59:13.193296 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28165 (* 1 = 8.28165 loss)
I0523 07:59:13.256603 34682 sgd_solver.cpp:112] Iteration 75090, lr = 0.01
I0523 07:59:17.770453 34682 solver.cpp:239] Iteration 75100 (2.18485 iter/s, 4.57697s/10 iters), loss = 6.81072
I0523 07:59:17.770504 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.81072 (* 1 = 6.81072 loss)
I0523 07:59:18.585371 34682 sgd_solver.cpp:112] Iteration 75100, lr = 0.01
I0523 07:59:22.314569 34682 solver.cpp:239] Iteration 75110 (2.20076 iter/s, 4.54388s/10 iters), loss = 7.71669
I0523 07:59:22.314621 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.71669 (* 1 = 7.71669 loss)
I0523 07:59:23.136240 34682 sgd_solver.cpp:112] Iteration 75110, lr = 0.01
I0523 07:59:28.697352 34682 solver.cpp:239] Iteration 75120 (1.56679 iter/s, 6.38248s/10 iters), loss = 7.59589
I0523 07:59:28.697396 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.59589 (* 1 = 7.59589 loss)
I0523 07:59:29.531597 34682 sgd_solver.cpp:112] Iteration 75120, lr = 0.01
I0523 07:59:34.648689 34682 solver.cpp:239] Iteration 75130 (1.68037 iter/s, 5.95105s/10 iters), loss = 7.82359
I0523 07:59:34.648737 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82359 (* 1 = 7.82359 loss)
I0523 07:59:34.911171 34682 sgd_solver.cpp:112] Iteration 75130, lr = 0.01
I0523 07:59:39.001657 34682 solver.cpp:239] Iteration 75140 (2.29741 iter/s, 4.35273s/10 iters), loss = 7.34747
I0523 07:59:39.001703 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.34747 (* 1 = 7.34747 loss)
I0523 07:59:39.065156 34682 sgd_solver.cpp:112] Iteration 75140, lr = 0.01
I0523 07:59:44.370012 34682 solver.cpp:239] Iteration 75150 (1.86286 iter/s, 5.36809s/10 iters), loss = 7.992
I0523 07:59:44.370057 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.992 (* 1 = 7.992 loss)
I0523 07:59:44.443756 34682 sgd_solver.cpp:112] Iteration 75150, lr = 0.01
I0523 07:59:47.266803 34682 solver.cpp:239] Iteration 75160 (3.45231 iter/s, 2.89661s/10 iters), loss = 8.16876
I0523 07:59:47.266886 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16876 (* 1 = 8.16876 loss)
I0523 07:59:47.421145 34682 sgd_solver.cpp:112] Iteration 75160, lr = 0.01
I0523 07:59:52.305750 34682 solver.cpp:239] Iteration 75170 (1.98466 iter/s, 5.03866s/10 iters), loss = 8.09598
I0523 07:59:52.305795 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09598 (* 1 = 8.09598 loss)
I0523 07:59:53.103057 34682 sgd_solver.cpp:112] Iteration 75170, lr = 0.01
I0523 07:59:58.859521 34682 solver.cpp:239] Iteration 75180 (1.52592 iter/s, 6.55344s/10 iters), loss = 7.59667
I0523 07:59:58.859705 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.59667 (* 1 = 7.59667 loss)
I0523 07:59:58.945550 34682 sgd_solver.cpp:112] Iteration 75180, lr = 0.01
I0523 08:00:03.199610 34682 solver.cpp:239] Iteration 75190 (2.30428 iter/s, 4.33974s/10 iters), loss = 8.05111
I0523 08:00:03.199661 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05111 (* 1 = 8.05111 loss)
I0523 08:00:03.272788 34682 sgd_solver.cpp:112] Iteration 75190, lr = 0.01
I0523 08:00:06.898805 34682 solver.cpp:239] Iteration 75200 (2.70345 iter/s, 3.69898s/10 iters), loss = 8.34275
I0523 08:00:06.898869 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34275 (* 1 = 8.34275 loss)
I0523 08:00:07.734922 34682 sgd_solver.cpp:112] Iteration 75200, lr = 0.01
I0523 08:00:12.304198 34682 solver.cpp:239] Iteration 75210 (1.85011 iter/s, 5.40507s/10 iters), loss = 7.63935
I0523 08:00:12.304245 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63935 (* 1 = 7.63935 loss)
I0523 08:00:12.371920 34682 sgd_solver.cpp:112] Iteration 75210, lr = 0.01
I0523 08:00:17.213737 34682 solver.cpp:239] Iteration 75220 (2.03696 iter/s, 4.90929s/10 iters), loss = 7.50466
I0523 08:00:17.213788 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50466 (* 1 = 7.50466 loss)
I0523 08:00:17.286340 34682 sgd_solver.cpp:112] Iteration 75220, lr = 0.01
I0523 08:00:21.306246 34682 solver.cpp:239] Iteration 75230 (2.44363 iter/s, 4.09228s/10 iters), loss = 7.95862
I0523 08:00:21.306303 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95862 (* 1 = 7.95862 loss)
I0523 08:00:22.165412 34682 sgd_solver.cpp:112] Iteration 75230, lr = 0.01
I0523 08:00:26.859329 34682 solver.cpp:239] Iteration 75240 (1.80089 iter/s, 5.5528s/10 iters), loss = 7.71519
I0523 08:00:26.859380 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.71519 (* 1 = 7.71519 loss)
I0523 08:00:27.679528 34682 sgd_solver.cpp:112] Iteration 75240, lr = 0.01
I0523 08:00:32.452881 34682 solver.cpp:239] Iteration 75250 (1.78786 iter/s, 5.59327s/10 iters), loss = 8.27089
I0523 08:00:32.453065 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27089 (* 1 = 8.27089 loss)
I0523 08:00:32.515899 34682 sgd_solver.cpp:112] Iteration 75250, lr = 0.01
I0523 08:00:37.455566 34682 solver.cpp:239] Iteration 75260 (1.99908 iter/s, 5.0023s/10 iters), loss = 8.15413
I0523 08:00:37.455621 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15413 (* 1 = 8.15413 loss)
I0523 08:00:38.237960 34682 sgd_solver.cpp:112] Iteration 75260, lr = 0.01
I0523 08:00:43.906491 34682 solver.cpp:239] Iteration 75270 (1.55025 iter/s, 6.45059s/10 iters), loss = 8.76056
I0523 08:00:43.906558 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76056 (* 1 = 8.76056 loss)
I0523 08:00:44.790055 34682 sgd_solver.cpp:112] Iteration 75270, lr = 0.01
I0523 08:00:51.494277 34682 solver.cpp:239] Iteration 75280 (1.31797 iter/s, 7.58742s/10 iters), loss = 7.39237
I0523 08:00:51.494333 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.39237 (* 1 = 7.39237 loss)
I0523 08:00:51.567728 34682 sgd_solver.cpp:112] Iteration 75280, lr = 0.01
I0523 08:00:56.289280 34682 solver.cpp:239] Iteration 75290 (2.08562 iter/s, 4.79475s/10 iters), loss = 7.37933
I0523 08:00:56.289335 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.37933 (* 1 = 7.37933 loss)
I0523 08:00:56.366024 34682 sgd_solver.cpp:112] Iteration 75290, lr = 0.01
I0523 08:00:59.492887 34682 solver.cpp:239] Iteration 75300 (3.12167 iter/s, 3.20341s/10 iters), loss = 7.66298
I0523 08:00:59.492936 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66298 (* 1 = 7.66298 loss)
I0523 08:01:00.323125 34682 sgd_solver.cpp:112] Iteration 75300, lr = 0.01
I0523 08:01:03.681076 34682 solver.cpp:239] Iteration 75310 (2.38779 iter/s, 4.18796s/10 iters), loss = 8.20855
I0523 08:01:03.681278 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20855 (* 1 = 8.20855 loss)
I0523 08:01:04.546973 34682 sgd_solver.cpp:112] Iteration 75310, lr = 0.01
I0523 08:01:07.155086 34682 solver.cpp:239] Iteration 75320 (2.87878 iter/s, 3.47369s/10 iters), loss = 7.02909
I0523 08:01:07.155149 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.02909 (* 1 = 7.02909 loss)
I0523 08:01:07.968226 34682 sgd_solver.cpp:112] Iteration 75320, lr = 0.01
I0523 08:01:11.965968 34682 solver.cpp:239] Iteration 75330 (2.07873 iter/s, 4.81062s/10 iters), loss = 7.87151
I0523 08:01:11.966017 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87151 (* 1 = 7.87151 loss)
I0523 08:01:12.042531 34682 sgd_solver.cpp:112] Iteration 75330, lr = 0.01
I0523 08:01:17.495702 34682 solver.cpp:239] Iteration 75340 (1.8085 iter/s, 5.52945s/10 iters), loss = 8.06602
I0523 08:01:17.495749 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06602 (* 1 = 8.06602 loss)
I0523 08:01:18.165380 34682 sgd_solver.cpp:112] Iteration 75340, lr = 0.01
I0523 08:01:24.283907 34682 solver.cpp:239] Iteration 75350 (1.47321 iter/s, 6.78788s/10 iters), loss = 7.62884
I0523 08:01:24.283960 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62884 (* 1 = 7.62884 loss)
I0523 08:01:24.347178 34682 sgd_solver.cpp:112] Iteration 75350, lr = 0.01
I0523 08:01:27.741578 34682 solver.cpp:239] Iteration 75360 (2.89229 iter/s, 3.45747s/10 iters), loss = 7.66838
I0523 08:01:27.741619 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66838 (* 1 = 7.66838 loss)
I0523 08:01:27.811870 34682 sgd_solver.cpp:112] Iteration 75360, lr = 0.01
I0523 08:01:31.321707 34682 solver.cpp:239] Iteration 75370 (2.79335 iter/s, 3.57994s/10 iters), loss = 7.64398
I0523 08:01:31.321748 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64398 (* 1 = 7.64398 loss)
I0523 08:01:31.400040 34682 sgd_solver.cpp:112] Iteration 75370, lr = 0.01
I0523 08:01:36.431180 34682 solver.cpp:239] Iteration 75380 (1.95725 iter/s, 5.10922s/10 iters), loss = 7.42121
I0523 08:01:36.431402 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42121 (* 1 = 7.42121 loss)
I0523 08:01:36.506342 34682 sgd_solver.cpp:112] Iteration 75380, lr = 0.01
I0523 08:01:40.860394 34682 solver.cpp:239] Iteration 75390 (2.25794 iter/s, 4.42881s/10 iters), loss = 8.14247
I0523 08:01:40.860452 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14247 (* 1 = 8.14247 loss)
I0523 08:01:41.645773 34682 sgd_solver.cpp:112] Iteration 75390, lr = 0.01
I0523 08:01:46.216719 34682 solver.cpp:239] Iteration 75400 (1.86705 iter/s, 5.35605s/10 iters), loss = 8.15603
I0523 08:01:46.216759 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15603 (* 1 = 8.15603 loss)
I0523 08:01:46.288265 34682 sgd_solver.cpp:112] Iteration 75400, lr = 0.01
I0523 08:01:50.178092 34682 solver.cpp:239] Iteration 75410 (2.52451 iter/s, 3.96116s/10 iters), loss = 8.31292
I0523 08:01:50.178139 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31292 (* 1 = 8.31292 loss)
I0523 08:01:50.242007 34682 sgd_solver.cpp:112] Iteration 75410, lr = 0.01
I0523 08:01:55.981536 34682 solver.cpp:239] Iteration 75420 (1.7232 iter/s, 5.80316s/10 iters), loss = 8.51477
I0523 08:01:55.981588 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51477 (* 1 = 8.51477 loss)
I0523 08:01:56.793326 34682 sgd_solver.cpp:112] Iteration 75420, lr = 0.01
I0523 08:02:02.137761 34682 solver.cpp:239] Iteration 75430 (1.62445 iter/s, 6.15592s/10 iters), loss = 7.43443
I0523 08:02:02.137804 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43443 (* 1 = 7.43443 loss)
I0523 08:02:02.984360 34682 sgd_solver.cpp:112] Iteration 75430, lr = 0.01
I0523 08:02:07.607445 34682 solver.cpp:239] Iteration 75440 (1.82835 iter/s, 5.46942s/10 iters), loss = 6.95195
I0523 08:02:07.607717 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.95195 (* 1 = 6.95195 loss)
I0523 08:02:07.680030 34682 sgd_solver.cpp:112] Iteration 75440, lr = 0.01
I0523 08:02:11.844386 34682 solver.cpp:239] Iteration 75450 (2.36042 iter/s, 4.23654s/10 iters), loss = 7.37832
I0523 08:02:11.844440 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.37832 (* 1 = 7.37832 loss)
I0523 08:02:12.561125 34682 sgd_solver.cpp:112] Iteration 75450, lr = 0.01
I0523 08:02:16.624487 34682 solver.cpp:239] Iteration 75460 (2.09212 iter/s, 4.77984s/10 iters), loss = 7.55031
I0523 08:02:16.624547 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.55031 (* 1 = 7.55031 loss)
I0523 08:02:16.710258 34682 sgd_solver.cpp:112] Iteration 75460, lr = 0.01
I0523 08:02:20.857704 34682 solver.cpp:239] Iteration 75470 (2.3624 iter/s, 4.23299s/10 iters), loss = 7.83641
I0523 08:02:20.857746 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83641 (* 1 = 7.83641 loss)
I0523 08:02:20.935051 34682 sgd_solver.cpp:112] Iteration 75470, lr = 0.01
I0523 08:02:26.616688 34682 solver.cpp:239] Iteration 75480 (1.7365 iter/s, 5.75871s/10 iters), loss = 7.90416
I0523 08:02:26.616734 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90416 (* 1 = 7.90416 loss)
I0523 08:02:26.692875 34682 sgd_solver.cpp:112] Iteration 75480, lr = 0.01
I0523 08:02:30.803704 34682 solver.cpp:239] Iteration 75490 (2.38846 iter/s, 4.18679s/10 iters), loss = 7.43374
I0523 08:02:30.803752 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43374 (* 1 = 7.43374 loss)
I0523 08:02:31.564514 34682 sgd_solver.cpp:112] Iteration 75490, lr = 0.01
I0523 08:02:35.365492 34682 solver.cpp:239] Iteration 75500 (2.19224 iter/s, 4.56155s/10 iters), loss = 7.22572
I0523 08:02:35.365535 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.22572 (* 1 = 7.22572 loss)
I0523 08:02:35.436904 34682 sgd_solver.cpp:112] Iteration 75500, lr = 0.01
I0523 08:02:41.794628 34682 solver.cpp:239] Iteration 75510 (1.55549 iter/s, 6.42883s/10 iters), loss = 8.28546
I0523 08:02:41.794925 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.28546 (* 1 = 8.28546 loss)
I0523 08:02:41.862726 34682 sgd_solver.cpp:112] Iteration 75510, lr = 0.01
I0523 08:02:45.456554 34682 solver.cpp:239] Iteration 75520 (2.73112 iter/s, 3.6615s/10 iters), loss = 8.86971
I0523 08:02:45.456611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86971 (* 1 = 8.86971 loss)
I0523 08:02:45.520517 34682 sgd_solver.cpp:112] Iteration 75520, lr = 0.01
I0523 08:02:50.968017 34682 solver.cpp:239] Iteration 75530 (1.81449 iter/s, 5.51119s/10 iters), loss = 7.9631
I0523 08:02:50.968073 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9631 (* 1 = 7.9631 loss)
I0523 08:02:51.042174 34682 sgd_solver.cpp:112] Iteration 75530, lr = 0.01
I0523 08:02:54.426324 34682 solver.cpp:239] Iteration 75540 (2.89175 iter/s, 3.45811s/10 iters), loss = 8.36871
I0523 08:02:54.426362 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36871 (* 1 = 8.36871 loss)
I0523 08:02:54.511550 34682 sgd_solver.cpp:112] Iteration 75540, lr = 0.01
I0523 08:02:58.693385 34682 solver.cpp:239] Iteration 75550 (2.34365 iter/s, 4.26685s/10 iters), loss = 8.89742
I0523 08:02:58.693434 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.89742 (* 1 = 8.89742 loss)
I0523 08:02:59.549458 34682 sgd_solver.cpp:112] Iteration 75550, lr = 0.01
I0523 08:03:05.638397 34682 solver.cpp:239] Iteration 75560 (1.43995 iter/s, 6.94468s/10 iters), loss = 8.13204
I0523 08:03:05.638455 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13204 (* 1 = 8.13204 loss)
I0523 08:03:06.445013 34682 sgd_solver.cpp:112] Iteration 75560, lr = 0.01
I0523 08:03:12.086683 34682 solver.cpp:239] Iteration 75570 (1.55088 iter/s, 6.44797s/10 iters), loss = 7.69574
I0523 08:03:12.086942 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69574 (* 1 = 7.69574 loss)
I0523 08:03:12.851301 34682 sgd_solver.cpp:112] Iteration 75570, lr = 0.01
I0523 08:03:18.124116 34682 solver.cpp:239] Iteration 75580 (1.65646 iter/s, 6.03696s/10 iters), loss = 7.56152
I0523 08:03:18.124171 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56152 (* 1 = 7.56152 loss)
I0523 08:03:18.182523 34682 sgd_solver.cpp:112] Iteration 75580, lr = 0.01
I0523 08:03:23.229341 34682 solver.cpp:239] Iteration 75590 (1.95888 iter/s, 5.10496s/10 iters), loss = 7.40373
I0523 08:03:23.229393 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.40373 (* 1 = 7.40373 loss)
I0523 08:03:23.924384 34682 sgd_solver.cpp:112] Iteration 75590, lr = 0.01
I0523 08:03:29.677697 34682 solver.cpp:239] Iteration 75600 (1.55086 iter/s, 6.44804s/10 iters), loss = 7.54598
I0523 08:03:29.677752 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54598 (* 1 = 7.54598 loss)
I0523 08:03:30.391242 34682 sgd_solver.cpp:112] Iteration 75600, lr = 0.01
I0523 08:03:36.076921 34682 solver.cpp:239] Iteration 75610 (1.56277 iter/s, 6.39891s/10 iters), loss = 7.16855
I0523 08:03:36.076967 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.16855 (* 1 = 7.16855 loss)
I0523 08:03:36.786334 34682 sgd_solver.cpp:112] Iteration 75610, lr = 0.01
I0523 08:03:40.112913 34682 solver.cpp:239] Iteration 75620 (2.47784 iter/s, 4.03578s/10 iters), loss = 7.5568
I0523 08:03:40.112958 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5568 (* 1 = 7.5568 loss)
I0523 08:03:40.174114 34682 sgd_solver.cpp:112] Iteration 75620, lr = 0.01
I0523 08:03:43.956742 34682 solver.cpp:239] Iteration 75630 (2.60171 iter/s, 3.84363s/10 iters), loss = 7.13924
I0523 08:03:43.956976 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.13924 (* 1 = 7.13924 loss)
I0523 08:03:44.035567 34682 sgd_solver.cpp:112] Iteration 75630, lr = 0.01
I0523 08:03:48.823388 34682 solver.cpp:239] Iteration 75640 (2.05497 iter/s, 4.86625s/10 iters), loss = 7.70895
I0523 08:03:48.823429 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.70895 (* 1 = 7.70895 loss)
I0523 08:03:48.900598 34682 sgd_solver.cpp:112] Iteration 75640, lr = 0.01
I0523 08:03:52.187894 34682 solver.cpp:239] Iteration 75650 (2.97237 iter/s, 3.36432s/10 iters), loss = 7.60327
I0523 08:03:52.187942 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.60327 (* 1 = 7.60327 loss)
I0523 08:03:52.979312 34682 sgd_solver.cpp:112] Iteration 75650, lr = 0.01
I0523 08:03:58.170076 34682 solver.cpp:239] Iteration 75660 (1.67171 iter/s, 5.98189s/10 iters), loss = 8.27718
I0523 08:03:58.170137 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27718 (* 1 = 8.27718 loss)
I0523 08:03:58.758519 34682 sgd_solver.cpp:112] Iteration 75660, lr = 0.01
I0523 08:04:03.589269 34682 solver.cpp:239] Iteration 75670 (1.84539 iter/s, 5.41891s/10 iters), loss = 8.30184
I0523 08:04:03.589311 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.30184 (* 1 = 8.30184 loss)
I0523 08:04:03.659919 34682 sgd_solver.cpp:112] Iteration 75670, lr = 0.01
I0523 08:04:07.930786 34682 solver.cpp:239] Iteration 75680 (2.30346 iter/s, 4.34129s/10 iters), loss = 7.75864
I0523 08:04:07.930842 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75864 (* 1 = 7.75864 loss)
I0523 08:04:08.531577 34682 sgd_solver.cpp:112] Iteration 75680, lr = 0.01
I0523 08:04:12.714567 34682 solver.cpp:239] Iteration 75690 (2.09051 iter/s, 4.78352s/10 iters), loss = 8.11769
I0523 08:04:12.714617 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11769 (* 1 = 8.11769 loss)
I0523 08:04:12.770913 34682 sgd_solver.cpp:112] Iteration 75690, lr = 0.01
I0523 08:04:17.380107 34682 solver.cpp:239] Iteration 75700 (2.14348 iter/s, 4.66531s/10 iters), loss = 7.52478
I0523 08:04:17.380353 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.52478 (* 1 = 7.52478 loss)
I0523 08:04:17.815810 34682 sgd_solver.cpp:112] Iteration 75700, lr = 0.01
I0523 08:04:22.540498 34682 solver.cpp:239] Iteration 75710 (1.938 iter/s, 5.15996s/10 iters), loss = 8.11628
I0523 08:04:22.540539 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11628 (* 1 = 8.11628 loss)
I0523 08:04:23.346426 34682 sgd_solver.cpp:112] Iteration 75710, lr = 0.01
I0523 08:04:25.441206 34682 solver.cpp:239] Iteration 75720 (3.44763 iter/s, 2.90055s/10 iters), loss = 8.05995
I0523 08:04:25.441262 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05995 (* 1 = 8.05995 loss)
I0523 08:04:26.050390 34682 sgd_solver.cpp:112] Iteration 75720, lr = 0.01
I0523 08:04:32.018278 34682 solver.cpp:239] Iteration 75730 (1.52051 iter/s, 6.57676s/10 iters), loss = 7.53822
I0523 08:04:32.018326 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.53822 (* 1 = 7.53822 loss)
I0523 08:04:32.872303 34682 sgd_solver.cpp:112] Iteration 75730, lr = 0.01
I0523 08:04:38.235177 34682 solver.cpp:239] Iteration 75740 (1.6086 iter/s, 6.21659s/10 iters), loss = 7.34692
I0523 08:04:38.235234 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.34692 (* 1 = 7.34692 loss)
I0523 08:04:38.997747 34682 sgd_solver.cpp:112] Iteration 75740, lr = 0.01
I0523 08:04:43.859653 34682 solver.cpp:239] Iteration 75750 (1.77803 iter/s, 5.62419s/10 iters), loss = 7.86044
I0523 08:04:43.859700 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86044 (* 1 = 7.86044 loss)
I0523 08:04:43.927947 34682 sgd_solver.cpp:112] Iteration 75750, lr = 0.01
I0523 08:04:48.076679 34682 solver.cpp:239] Iteration 75760 (2.37146 iter/s, 4.21681s/10 iters), loss = 7.75985
I0523 08:04:48.076920 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75985 (* 1 = 7.75985 loss)
I0523 08:04:48.258278 34682 sgd_solver.cpp:112] Iteration 75760, lr = 0.01
I0523 08:04:51.440182 34682 solver.cpp:239] Iteration 75770 (2.97342 iter/s, 3.36313s/10 iters), loss = 7.60452
I0523 08:04:51.440225 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.60452 (* 1 = 7.60452 loss)
I0523 08:04:51.513103 34682 sgd_solver.cpp:112] Iteration 75770, lr = 0.01
I0523 08:04:55.550225 34682 solver.cpp:239] Iteration 75780 (2.4332 iter/s, 4.10982s/10 iters), loss = 9.17861
I0523 08:04:55.550278 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.17861 (* 1 = 9.17861 loss)
I0523 08:04:55.601130 34682 sgd_solver.cpp:112] Iteration 75780, lr = 0.01
I0523 08:05:00.393617 34682 solver.cpp:239] Iteration 75790 (2.06478 iter/s, 4.84314s/10 iters), loss = 7.26982
I0523 08:05:00.393681 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.26982 (* 1 = 7.26982 loss)
I0523 08:05:00.467226 34682 sgd_solver.cpp:112] Iteration 75790, lr = 0.01
I0523 08:05:06.188846 34682 solver.cpp:239] Iteration 75800 (1.72564 iter/s, 5.79494s/10 iters), loss = 7.82513
I0523 08:05:06.188891 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82513 (* 1 = 7.82513 loss)
I0523 08:05:06.258746 34682 sgd_solver.cpp:112] Iteration 75800, lr = 0.01
I0523 08:05:08.681388 34682 solver.cpp:239] Iteration 75810 (4.01222 iter/s, 2.49239s/10 iters), loss = 8.24117
I0523 08:05:08.681427 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24117 (* 1 = 8.24117 loss)
I0523 08:05:08.756332 34682 sgd_solver.cpp:112] Iteration 75810, lr = 0.01
I0523 08:05:13.365691 34682 solver.cpp:239] Iteration 75820 (2.1349 iter/s, 4.68407s/10 iters), loss = 8.66326
I0523 08:05:13.365734 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.66326 (* 1 = 8.66326 loss)
I0523 08:05:13.427853 34682 sgd_solver.cpp:112] Iteration 75820, lr = 0.01
I0523 08:05:16.941799 34682 solver.cpp:239] Iteration 75830 (2.79649 iter/s, 3.57592s/10 iters), loss = 8.16745
I0523 08:05:16.941843 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16745 (* 1 = 8.16745 loss)
I0523 08:05:17.774636 34682 sgd_solver.cpp:112] Iteration 75830, lr = 0.01
I0523 08:05:19.975636 34682 solver.cpp:239] Iteration 75840 (3.29635 iter/s, 3.03366s/10 iters), loss = 8.03514
I0523 08:05:19.975919 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03514 (* 1 = 8.03514 loss)
I0523 08:05:20.755652 34682 sgd_solver.cpp:112] Iteration 75840, lr = 0.01
I0523 08:05:26.695785 34682 solver.cpp:239] Iteration 75850 (1.48818 iter/s, 6.71963s/10 iters), loss = 7.40044
I0523 08:05:26.695825 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.40044 (* 1 = 7.40044 loss)
I0523 08:05:26.767596 34682 sgd_solver.cpp:112] Iteration 75850, lr = 0.01
I0523 08:05:30.136771 34682 solver.cpp:239] Iteration 75860 (2.90631 iter/s, 3.44079s/10 iters), loss = 8.00173
I0523 08:05:30.136822 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00173 (* 1 = 8.00173 loss)
I0523 08:05:30.532083 34682 sgd_solver.cpp:112] Iteration 75860, lr = 0.01
I0523 08:05:36.124357 34682 solver.cpp:239] Iteration 75870 (1.6702 iter/s, 5.98729s/10 iters), loss = 7.00481
I0523 08:05:36.124410 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.00481 (* 1 = 7.00481 loss)
I0523 08:05:36.182950 34682 sgd_solver.cpp:112] Iteration 75870, lr = 0.01
I0523 08:05:40.933295 34682 solver.cpp:239] Iteration 75880 (2.07957 iter/s, 4.80868s/10 iters), loss = 7.40845
I0523 08:05:40.933353 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.40845 (* 1 = 7.40845 loss)
I0523 08:05:40.995939 34682 sgd_solver.cpp:112] Iteration 75880, lr = 0.01
I0523 08:05:46.484921 34682 solver.cpp:239] Iteration 75890 (1.80136 iter/s, 5.55135s/10 iters), loss = 7.56525
I0523 08:05:46.484966 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56525 (* 1 = 7.56525 loss)
I0523 08:05:46.549105 34682 sgd_solver.cpp:112] Iteration 75890, lr = 0.01
I0523 08:05:49.153126 34682 solver.cpp:239] Iteration 75900 (3.74806 iter/s, 2.66805s/10 iters), loss = 7.16651
I0523 08:05:49.153167 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.16651 (* 1 = 7.16651 loss)
I0523 08:05:49.974858 34682 sgd_solver.cpp:112] Iteration 75900, lr = 0.01
I0523 08:05:55.024099 34682 solver.cpp:239] Iteration 75910 (1.70338 iter/s, 5.87069s/10 iters), loss = 8.24608
I0523 08:05:55.024299 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24608 (* 1 = 8.24608 loss)
I0523 08:05:55.878950 34682 sgd_solver.cpp:112] Iteration 75910, lr = 0.01
I0523 08:06:00.209983 34682 solver.cpp:239] Iteration 75920 (1.92846 iter/s, 5.18548s/10 iters), loss = 7.68432
I0523 08:06:00.210031 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68432 (* 1 = 7.68432 loss)
I0523 08:06:00.942739 34682 sgd_solver.cpp:112] Iteration 75920, lr = 0.01
I0523 08:06:04.867489 34682 solver.cpp:239] Iteration 75930 (2.14718 iter/s, 4.65727s/10 iters), loss = 7.43134
I0523 08:06:04.867539 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43134 (* 1 = 7.43134 loss)
I0523 08:06:05.698283 34682 sgd_solver.cpp:112] Iteration 75930, lr = 0.01
I0523 08:06:10.681613 34682 solver.cpp:239] Iteration 75940 (1.72003 iter/s, 5.81384s/10 iters), loss = 7.80942
I0523 08:06:10.681660 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80942 (* 1 = 7.80942 loss)
I0523 08:06:11.582293 34682 sgd_solver.cpp:112] Iteration 75940, lr = 0.01
I0523 08:06:16.204608 34682 solver.cpp:239] Iteration 75950 (1.8107 iter/s, 5.52272s/10 iters), loss = 7.66968
I0523 08:06:16.204658 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66968 (* 1 = 7.66968 loss)
I0523 08:06:16.281770 34682 sgd_solver.cpp:112] Iteration 75950, lr = 0.01
I0523 08:06:21.868482 34682 solver.cpp:239] Iteration 75960 (1.76566 iter/s, 5.66359s/10 iters), loss = 8.32898
I0523 08:06:21.868532 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32898 (* 1 = 8.32898 loss)
I0523 08:06:22.676700 34682 sgd_solver.cpp:112] Iteration 75960, lr = 0.01
I0523 08:06:26.142943 34682 solver.cpp:239] Iteration 75970 (2.3396 iter/s, 4.27424s/10 iters), loss = 7.54176
I0523 08:06:26.143084 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54176 (* 1 = 7.54176 loss)
I0523 08:06:26.972723 34682 sgd_solver.cpp:112] Iteration 75970, lr = 0.01
I0523 08:06:30.165637 34682 solver.cpp:239] Iteration 75980 (2.48609 iter/s, 4.02238s/10 iters), loss = 7.1616
I0523 08:06:30.165702 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.1616 (* 1 = 7.1616 loss)
I0523 08:06:30.956442 34682 sgd_solver.cpp:112] Iteration 75980, lr = 0.01
I0523 08:06:34.187539 34682 solver.cpp:239] Iteration 75990 (2.48653 iter/s, 4.02167s/10 iters), loss = 7.30175
I0523 08:06:34.187595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.30175 (* 1 = 7.30175 loss)
I0523 08:06:35.062973 34682 sgd_solver.cpp:112] Iteration 75990, lr = 0.01
I0523 08:06:39.858032 34682 solver.cpp:239] Iteration 76000 (1.76361 iter/s, 5.6702s/10 iters), loss = 8.25752
I0523 08:06:39.858083 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25752 (* 1 = 8.25752 loss)
I0523 08:06:39.909003 34682 sgd_solver.cpp:112] Iteration 76000, lr = 0.01
I0523 08:06:45.240985 34682 solver.cpp:239] Iteration 76010 (1.85781 iter/s, 5.38268s/10 iters), loss = 8.31533
I0523 08:06:45.241039 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31533 (* 1 = 8.31533 loss)
I0523 08:06:45.326746 34682 sgd_solver.cpp:112] Iteration 76010, lr = 0.01
I0523 08:06:50.038975 34682 solver.cpp:239] Iteration 76020 (2.08432 iter/s, 4.79773s/10 iters), loss = 8.01586
I0523 08:06:50.039028 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01586 (* 1 = 8.01586 loss)
I0523 08:06:50.865106 34682 sgd_solver.cpp:112] Iteration 76020, lr = 0.01
I0523 08:06:55.605787 34682 solver.cpp:239] Iteration 76030 (1.79645 iter/s, 5.56653s/10 iters), loss = 7.13459
I0523 08:06:55.605830 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.13459 (* 1 = 7.13459 loss)
I0523 08:06:56.446349 34682 sgd_solver.cpp:112] Iteration 76030, lr = 0.01
I0523 08:06:59.948967 34682 solver.cpp:239] Iteration 76040 (2.30258 iter/s, 4.34296s/10 iters), loss = 8.4525
I0523 08:06:59.949007 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4525 (* 1 = 8.4525 loss)
I0523 08:07:00.289979 34682 sgd_solver.cpp:112] Iteration 76040, lr = 0.01
I0523 08:07:05.150892 34682 solver.cpp:239] Iteration 76050 (1.92246 iter/s, 5.20167s/10 iters), loss = 7.54995
I0523 08:07:05.150938 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54995 (* 1 = 7.54995 loss)
I0523 08:07:05.952879 34682 sgd_solver.cpp:112] Iteration 76050, lr = 0.01
I0523 08:07:09.570451 34682 solver.cpp:239] Iteration 76060 (2.26279 iter/s, 4.41933s/10 iters), loss = 7.65803
I0523 08:07:09.570493 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.65803 (* 1 = 7.65803 loss)
I0523 08:07:09.643167 34682 sgd_solver.cpp:112] Iteration 76060, lr = 0.01
I0523 08:07:12.309222 34682 solver.cpp:239] Iteration 76070 (3.65149 iter/s, 2.73861s/10 iters), loss = 6.89905
I0523 08:07:12.309273 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.89905 (* 1 = 6.89905 loss)
I0523 08:07:12.870580 34682 sgd_solver.cpp:112] Iteration 76070, lr = 0.01
I0523 08:07:15.767549 34682 solver.cpp:239] Iteration 76080 (2.89173 iter/s, 3.45814s/10 iters), loss = 8.93078
I0523 08:07:15.767593 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.93078 (* 1 = 8.93078 loss)
I0523 08:07:15.841084 34682 sgd_solver.cpp:112] Iteration 76080, lr = 0.01
I0523 08:07:19.273921 34682 solver.cpp:239] Iteration 76090 (2.85532 iter/s, 3.50223s/10 iters), loss = 7.5619
I0523 08:07:19.273967 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5619 (* 1 = 7.5619 loss)
I0523 08:07:19.348906 34682 sgd_solver.cpp:112] Iteration 76090, lr = 0.01
I0523 08:07:23.550266 34682 solver.cpp:239] Iteration 76100 (2.33857 iter/s, 4.27612s/10 iters), loss = 6.69233
I0523 08:07:23.550310 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.69233 (* 1 = 6.69233 loss)
I0523 08:07:23.633432 34682 sgd_solver.cpp:112] Iteration 76100, lr = 0.01
I0523 08:07:26.904567 34682 solver.cpp:239] Iteration 76110 (2.98141 iter/s, 3.35412s/10 iters), loss = 8.32408
I0523 08:07:26.904803 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32408 (* 1 = 8.32408 loss)
I0523 08:07:26.968744 34682 sgd_solver.cpp:112] Iteration 76110, lr = 0.01
I0523 08:07:31.157337 34682 solver.cpp:239] Iteration 76120 (2.35162 iter/s, 4.25238s/10 iters), loss = 7.50326
I0523 08:07:31.157395 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50326 (* 1 = 7.50326 loss)
I0523 08:07:31.926455 34682 sgd_solver.cpp:112] Iteration 76120, lr = 0.01
I0523 08:07:35.132326 34682 solver.cpp:239] Iteration 76130 (2.51587 iter/s, 3.97476s/10 iters), loss = 7.50845
I0523 08:07:35.132380 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50845 (* 1 = 7.50845 loss)
I0523 08:07:35.951074 34682 sgd_solver.cpp:112] Iteration 76130, lr = 0.01
I0523 08:07:41.373935 34682 solver.cpp:239] Iteration 76140 (1.60223 iter/s, 6.2413s/10 iters), loss = 7.38663
I0523 08:07:41.373989 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.38663 (* 1 = 7.38663 loss)
I0523 08:07:41.989346 34682 sgd_solver.cpp:112] Iteration 76140, lr = 0.01
I0523 08:07:47.032171 34682 solver.cpp:239] Iteration 76150 (1.76742 iter/s, 5.65795s/10 iters), loss = 8.26964
I0523 08:07:47.032213 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26964 (* 1 = 8.26964 loss)
I0523 08:07:47.102742 34682 sgd_solver.cpp:112] Iteration 76150, lr = 0.01
I0523 08:07:52.016362 34682 solver.cpp:239] Iteration 76160 (2.00644 iter/s, 4.98394s/10 iters), loss = 8.42792
I0523 08:07:52.016402 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42792 (* 1 = 8.42792 loss)
I0523 08:07:52.095832 34682 sgd_solver.cpp:112] Iteration 76160, lr = 0.01
I0523 08:07:56.333279 34682 solver.cpp:239] Iteration 76170 (2.31659 iter/s, 4.3167s/10 iters), loss = 7.87868
I0523 08:07:56.333322 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87868 (* 1 = 7.87868 loss)
I0523 08:07:56.415356 34682 sgd_solver.cpp:112] Iteration 76170, lr = 0.01
I0523 08:08:02.222112 34682 solver.cpp:239] Iteration 76180 (1.69821 iter/s, 5.88855s/10 iters), loss = 8.41259
I0523 08:08:02.222304 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41259 (* 1 = 8.41259 loss)
I0523 08:08:03.045209 34682 sgd_solver.cpp:112] Iteration 76180, lr = 0.01
I0523 08:08:07.929776 34682 solver.cpp:239] Iteration 76190 (1.75216 iter/s, 5.70725s/10 iters), loss = 7.73879
I0523 08:08:07.929818 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.73879 (* 1 = 7.73879 loss)
I0523 08:08:08.001950 34682 sgd_solver.cpp:112] Iteration 76190, lr = 0.01
I0523 08:08:12.775161 34682 solver.cpp:239] Iteration 76200 (2.06392 iter/s, 4.84514s/10 iters), loss = 7.93503
I0523 08:08:12.775200 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93503 (* 1 = 7.93503 loss)
I0523 08:08:12.848666 34682 sgd_solver.cpp:112] Iteration 76200, lr = 0.01
I0523 08:08:16.207090 34682 solver.cpp:239] Iteration 76210 (2.91396 iter/s, 3.43175s/10 iters), loss = 8.53149
I0523 08:08:16.207132 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.53149 (* 1 = 8.53149 loss)
I0523 08:08:16.267846 34682 sgd_solver.cpp:112] Iteration 76210, lr = 0.01
I0523 08:08:22.378792 34682 solver.cpp:239] Iteration 76220 (1.62038 iter/s, 6.1714s/10 iters), loss = 7.58728
I0523 08:08:22.378850 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58728 (* 1 = 7.58728 loss)
I0523 08:08:23.227941 34682 sgd_solver.cpp:112] Iteration 76220, lr = 0.01
I0523 08:08:28.227171 34682 solver.cpp:239] Iteration 76230 (1.70996 iter/s, 5.84809s/10 iters), loss = 8.00258
I0523 08:08:28.227212 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00258 (* 1 = 8.00258 loss)
I0523 08:08:28.301524 34682 sgd_solver.cpp:112] Iteration 76230, lr = 0.01
I0523 08:08:33.105398 34682 solver.cpp:239] Iteration 76240 (2.05003 iter/s, 4.87798s/10 iters), loss = 7.86249
I0523 08:08:33.105588 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86249 (* 1 = 7.86249 loss)
I0523 08:08:33.174628 34682 sgd_solver.cpp:112] Iteration 76240, lr = 0.01
I0523 08:08:38.369868 34682 solver.cpp:239] Iteration 76250 (1.89966 iter/s, 5.26409s/10 iters), loss = 7.09389
I0523 08:08:38.369915 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.09389 (* 1 = 7.09389 loss)
I0523 08:08:38.440240 34682 sgd_solver.cpp:112] Iteration 76250, lr = 0.01
I0523 08:08:41.090104 34682 solver.cpp:239] Iteration 76260 (3.67637 iter/s, 2.72007s/10 iters), loss = 8.08205
I0523 08:08:41.090155 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08205 (* 1 = 8.08205 loss)
I0523 08:08:41.614356 34682 sgd_solver.cpp:112] Iteration 76260, lr = 0.01
I0523 08:08:46.522836 34682 solver.cpp:239] Iteration 76270 (1.84079 iter/s, 5.43246s/10 iters), loss = 7.25488
I0523 08:08:46.522902 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.25488 (* 1 = 7.25488 loss)
I0523 08:08:46.591558 34682 sgd_solver.cpp:112] Iteration 76270, lr = 0.01
I0523 08:08:51.348351 34682 solver.cpp:239] Iteration 76280 (2.07243 iter/s, 4.82525s/10 iters), loss = 7.90894
I0523 08:08:51.348410 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90894 (* 1 = 7.90894 loss)
I0523 08:08:51.428977 34682 sgd_solver.cpp:112] Iteration 76280, lr = 0.01
I0523 08:08:55.265873 34682 solver.cpp:239] Iteration 76290 (2.55279 iter/s, 3.91728s/10 iters), loss = 9.31637
I0523 08:08:55.265935 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.31637 (* 1 = 9.31637 loss)
I0523 08:08:56.095544 34682 sgd_solver.cpp:112] Iteration 76290, lr = 0.01
I0523 08:09:00.179704 34682 solver.cpp:239] Iteration 76300 (2.03518 iter/s, 4.91357s/10 iters), loss = 6.89555
I0523 08:09:00.179749 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.89555 (* 1 = 6.89555 loss)
I0523 08:09:00.248636 34682 sgd_solver.cpp:112] Iteration 76300, lr = 0.01
I0523 08:09:06.695667 34682 solver.cpp:239] Iteration 76310 (1.53476 iter/s, 6.51566s/10 iters), loss = 8.45269
I0523 08:09:06.695935 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45269 (* 1 = 8.45269 loss)
I0523 08:09:07.506925 34682 sgd_solver.cpp:112] Iteration 76310, lr = 0.01
I0523 08:09:11.783421 34682 solver.cpp:239] Iteration 76320 (1.96567 iter/s, 5.08733s/10 iters), loss = 8.21428
I0523 08:09:11.783465 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21428 (* 1 = 8.21428 loss)
I0523 08:09:12.595547 34682 sgd_solver.cpp:112] Iteration 76320, lr = 0.01
I0523 08:09:16.950685 34682 solver.cpp:239] Iteration 76330 (1.93536 iter/s, 5.167s/10 iters), loss = 7.53902
I0523 08:09:16.950774 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.53902 (* 1 = 7.53902 loss)
I0523 08:09:17.715519 34682 sgd_solver.cpp:112] Iteration 76330, lr = 0.01
I0523 08:09:22.638761 34682 solver.cpp:239] Iteration 76340 (1.75816 iter/s, 5.68776s/10 iters), loss = 7.92857
I0523 08:09:22.638814 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92857 (* 1 = 7.92857 loss)
I0523 08:09:22.715648 34682 sgd_solver.cpp:112] Iteration 76340, lr = 0.01
I0523 08:09:27.070686 34682 solver.cpp:239] Iteration 76350 (2.25647 iter/s, 4.43169s/10 iters), loss = 8.09259
I0523 08:09:27.070750 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09259 (* 1 = 8.09259 loss)
I0523 08:09:27.147442 34682 sgd_solver.cpp:112] Iteration 76350, lr = 0.01
I0523 08:09:32.380770 34682 solver.cpp:239] Iteration 76360 (1.88331 iter/s, 5.3098s/10 iters), loss = 7.73032
I0523 08:09:32.380838 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.73032 (* 1 = 7.73032 loss)
I0523 08:09:32.457651 34682 sgd_solver.cpp:112] Iteration 76360, lr = 0.01
I0523 08:09:35.667640 34682 solver.cpp:239] Iteration 76370 (3.0426 iter/s, 3.28667s/10 iters), loss = 7.79423
I0523 08:09:35.667680 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79423 (* 1 = 7.79423 loss)
I0523 08:09:35.748504 34682 sgd_solver.cpp:112] Iteration 76370, lr = 0.01
I0523 08:09:41.609154 34682 solver.cpp:239] Iteration 76380 (1.68315 iter/s, 5.94123s/10 iters), loss = 9.05054
I0523 08:09:41.609334 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.05054 (* 1 = 9.05054 loss)
I0523 08:09:41.669857 34682 sgd_solver.cpp:112] Iteration 76380, lr = 0.01
I0523 08:09:44.242568 34682 solver.cpp:239] Iteration 76390 (3.79776 iter/s, 2.63313s/10 iters), loss = 7.57222
I0523 08:09:44.242609 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.57222 (* 1 = 7.57222 loss)
I0523 08:09:44.307109 34682 sgd_solver.cpp:112] Iteration 76390, lr = 0.01
I0523 08:09:48.184407 34682 solver.cpp:239] Iteration 76400 (2.53703 iter/s, 3.94162s/10 iters), loss = 6.59319
I0523 08:09:48.184454 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.59319 (* 1 = 6.59319 loss)
I0523 08:09:48.253553 34682 sgd_solver.cpp:112] Iteration 76400, lr = 0.01
I0523 08:09:53.932608 34682 solver.cpp:239] Iteration 76410 (1.73976 iter/s, 5.74792s/10 iters), loss = 7.34455
I0523 08:09:53.932651 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.34455 (* 1 = 7.34455 loss)
I0523 08:09:54.680312 34682 sgd_solver.cpp:112] Iteration 76410, lr = 0.01
I0523 08:10:00.594286 34682 solver.cpp:239] Iteration 76420 (1.50119 iter/s, 6.66136s/10 iters), loss = 6.83716
I0523 08:10:00.594338 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.83716 (* 1 = 6.83716 loss)
I0523 08:10:01.465760 34682 sgd_solver.cpp:112] Iteration 76420, lr = 0.01
I0523 08:10:06.194711 34682 solver.cpp:239] Iteration 76430 (1.78567 iter/s, 5.60013s/10 iters), loss = 8.11704
I0523 08:10:06.194761 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11704 (* 1 = 8.11704 loss)
I0523 08:10:06.254659 34682 sgd_solver.cpp:112] Iteration 76430, lr = 0.01
I0523 08:10:11.250813 34682 solver.cpp:239] Iteration 76440 (1.97791 iter/s, 5.05585s/10 iters), loss = 7.4368
I0523 08:10:11.250856 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4368 (* 1 = 7.4368 loss)
I0523 08:10:11.310585 34682 sgd_solver.cpp:112] Iteration 76440, lr = 0.01
I0523 08:10:15.908830 34682 solver.cpp:239] Iteration 76450 (2.14695 iter/s, 4.65776s/10 iters), loss = 8.19061
I0523 08:10:15.909008 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19061 (* 1 = 8.19061 loss)
I0523 08:10:16.764822 34682 sgd_solver.cpp:112] Iteration 76450, lr = 0.01
I0523 08:10:22.112399 34682 solver.cpp:239] Iteration 76460 (1.61208 iter/s, 6.20315s/10 iters), loss = 7.35502
I0523 08:10:22.112442 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.35502 (* 1 = 7.35502 loss)
I0523 08:10:22.529031 34682 sgd_solver.cpp:112] Iteration 76460, lr = 0.01
I0523 08:10:26.813500 34682 solver.cpp:239] Iteration 76470 (2.12727 iter/s, 4.70086s/10 iters), loss = 7.86948
I0523 08:10:26.813542 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86948 (* 1 = 7.86948 loss)
I0523 08:10:27.563696 34682 sgd_solver.cpp:112] Iteration 76470, lr = 0.01
I0523 08:10:31.968003 34682 solver.cpp:239] Iteration 76480 (1.94015 iter/s, 5.15425s/10 iters), loss = 7.90609
I0523 08:10:31.968060 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90609 (* 1 = 7.90609 loss)
I0523 08:10:32.795294 34682 sgd_solver.cpp:112] Iteration 76480, lr = 0.01
I0523 08:10:37.223558 34682 solver.cpp:239] Iteration 76490 (1.90285 iter/s, 5.25528s/10 iters), loss = 7.53806
I0523 08:10:37.223616 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.53806 (* 1 = 7.53806 loss)
I0523 08:10:37.310469 34682 sgd_solver.cpp:112] Iteration 76490, lr = 0.01
I0523 08:10:42.792570 34682 solver.cpp:239] Iteration 76500 (1.79574 iter/s, 5.56872s/10 iters), loss = 7.35572
I0523 08:10:42.792625 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.35572 (* 1 = 7.35572 loss)
I0523 08:10:43.588691 34682 sgd_solver.cpp:112] Iteration 76500, lr = 0.01
I0523 08:10:49.225602 34682 solver.cpp:239] Iteration 76510 (1.55455 iter/s, 6.43272s/10 iters), loss = 7.25699
I0523 08:10:49.225877 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.25699 (* 1 = 7.25699 loss)
I0523 08:10:50.030472 34682 sgd_solver.cpp:112] Iteration 76510, lr = 0.01
I0523 08:10:54.585846 34682 solver.cpp:239] Iteration 76520 (1.86575 iter/s, 5.35978s/10 iters), loss = 7.88459
I0523 08:10:54.585886 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88459 (* 1 = 7.88459 loss)
I0523 08:10:54.660449 34682 sgd_solver.cpp:112] Iteration 76520, lr = 0.01
I0523 08:10:59.334276 34682 solver.cpp:239] Iteration 76530 (2.10607 iter/s, 4.74818s/10 iters), loss = 7.47605
I0523 08:10:59.334347 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.47605 (* 1 = 7.47605 loss)
I0523 08:10:59.396873 34682 sgd_solver.cpp:112] Iteration 76530, lr = 0.01
I0523 08:11:04.081840 34682 solver.cpp:239] Iteration 76540 (2.10646 iter/s, 4.7473s/10 iters), loss = 6.98044
I0523 08:11:04.081894 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.98044 (* 1 = 6.98044 loss)
I0523 08:11:04.143870 34682 sgd_solver.cpp:112] Iteration 76540, lr = 0.01
I0523 08:11:07.609087 34682 solver.cpp:239] Iteration 76550 (2.83524 iter/s, 3.52704s/10 iters), loss = 8.72235
I0523 08:11:07.609131 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72235 (* 1 = 8.72235 loss)
I0523 08:11:08.324340 34682 sgd_solver.cpp:112] Iteration 76550, lr = 0.01
I0523 08:11:14.653609 34682 solver.cpp:239] Iteration 76560 (1.41961 iter/s, 7.0442s/10 iters), loss = 7.6505
I0523 08:11:14.653663 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6505 (* 1 = 7.6505 loss)
I0523 08:11:15.522794 34682 sgd_solver.cpp:112] Iteration 76560, lr = 0.01
I0523 08:11:21.263770 34682 solver.cpp:239] Iteration 76570 (1.5129 iter/s, 6.60984s/10 iters), loss = 8.32827
I0523 08:11:21.263880 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32827 (* 1 = 8.32827 loss)
I0523 08:11:21.323896 34682 sgd_solver.cpp:112] Iteration 76570, lr = 0.01
I0523 08:11:25.477666 34682 solver.cpp:239] Iteration 76580 (2.37326 iter/s, 4.21361s/10 iters), loss = 8.18371
I0523 08:11:25.477710 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.18371 (* 1 = 8.18371 loss)
I0523 08:11:25.546805 34682 sgd_solver.cpp:112] Iteration 76580, lr = 0.01
I0523 08:11:30.390978 34682 solver.cpp:239] Iteration 76590 (2.03539 iter/s, 4.91306s/10 iters), loss = 7.51959
I0523 08:11:30.391026 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51959 (* 1 = 7.51959 loss)
I0523 08:11:30.455220 34682 sgd_solver.cpp:112] Iteration 76590, lr = 0.01
I0523 08:11:33.695811 34682 solver.cpp:239] Iteration 76600 (3.02604 iter/s, 3.30465s/10 iters), loss = 7.62605
I0523 08:11:33.695852 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62605 (* 1 = 7.62605 loss)
I0523 08:11:33.753757 34682 sgd_solver.cpp:112] Iteration 76600, lr = 0.01
I0523 08:11:37.027626 34682 solver.cpp:239] Iteration 76610 (3.00153 iter/s, 3.33163s/10 iters), loss = 8.2377
I0523 08:11:37.027668 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2377 (* 1 = 8.2377 loss)
I0523 08:11:37.104895 34682 sgd_solver.cpp:112] Iteration 76610, lr = 0.01
I0523 08:11:43.184603 34682 solver.cpp:239] Iteration 76620 (1.62425 iter/s, 6.15667s/10 iters), loss = 6.91752
I0523 08:11:43.184655 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.91752 (* 1 = 6.91752 loss)
I0523 08:11:43.249773 34682 sgd_solver.cpp:112] Iteration 76620, lr = 0.01
I0523 08:11:47.053478 34682 solver.cpp:239] Iteration 76630 (2.58487 iter/s, 3.86867s/10 iters), loss = 8.03109
I0523 08:11:47.053517 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03109 (* 1 = 8.03109 loss)
I0523 08:11:47.091755 34682 sgd_solver.cpp:112] Iteration 76630, lr = 0.01
I0523 08:11:48.258435 34682 solver.cpp:239] Iteration 76640 (8.29975 iter/s, 1.20486s/10 iters), loss = 7.30634
I0523 08:11:48.258478 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.30634 (* 1 = 7.30634 loss)
I0523 08:11:48.306859 34682 sgd_solver.cpp:112] Iteration 76640, lr = 0.01
I0523 08:11:49.689105 34682 solver.cpp:239] Iteration 76650 (6.99026 iter/s, 1.43056s/10 iters), loss = 8.09169
I0523 08:11:49.689142 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09169 (* 1 = 8.09169 loss)
I0523 08:11:49.740615 34682 sgd_solver.cpp:112] Iteration 76650, lr = 0.01
I0523 08:11:53.738736 34682 solver.cpp:239] Iteration 76660 (2.4695 iter/s, 4.0494s/10 iters), loss = 6.76403
I0523 08:11:53.739007 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.76403 (* 1 = 6.76403 loss)
I0523 08:11:53.808884 34682 sgd_solver.cpp:112] Iteration 76660, lr = 0.01
I0523 08:11:58.999665 34682 solver.cpp:239] Iteration 76670 (1.90256 iter/s, 5.25609s/10 iters), loss = 7.40734
I0523 08:11:58.999716 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.40734 (* 1 = 7.40734 loss)
I0523 08:11:59.752125 34682 sgd_solver.cpp:112] Iteration 76670, lr = 0.01
I0523 08:12:04.209964 34682 solver.cpp:239] Iteration 76680 (1.91937 iter/s, 5.21004s/10 iters), loss = 8.10326
I0523 08:12:04.210011 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10326 (* 1 = 8.10326 loss)
I0523 08:12:04.268481 34682 sgd_solver.cpp:112] Iteration 76680, lr = 0.01
I0523 08:12:07.493675 34682 solver.cpp:239] Iteration 76690 (3.04551 iter/s, 3.28352s/10 iters), loss = 7.29162
I0523 08:12:07.493719 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.29162 (* 1 = 7.29162 loss)
I0523 08:12:07.558673 34682 sgd_solver.cpp:112] Iteration 76690, lr = 0.01
I0523 08:12:13.354866 34682 solver.cpp:239] Iteration 76700 (1.70622 iter/s, 5.8609s/10 iters), loss = 7.29086
I0523 08:12:13.354928 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.29086 (* 1 = 7.29086 loss)
I0523 08:12:13.426198 34682 sgd_solver.cpp:112] Iteration 76700, lr = 0.01
I0523 08:12:16.570812 34682 solver.cpp:239] Iteration 76710 (3.1097 iter/s, 3.21575s/10 iters), loss = 7.49528
I0523 08:12:16.570858 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.49528 (* 1 = 7.49528 loss)
I0523 08:12:16.652292 34682 sgd_solver.cpp:112] Iteration 76710, lr = 0.01
I0523 08:12:20.839512 34682 solver.cpp:239] Iteration 76720 (2.34276 iter/s, 4.26847s/10 iters), loss = 7.79613
I0523 08:12:20.839558 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79613 (* 1 = 7.79613 loss)
I0523 08:12:20.911653 34682 sgd_solver.cpp:112] Iteration 76720, lr = 0.01
I0523 08:12:26.263691 34682 solver.cpp:239] Iteration 76730 (1.84519 iter/s, 5.4195s/10 iters), loss = 7.17096
I0523 08:12:26.264048 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.17096 (* 1 = 7.17096 loss)
I0523 08:12:26.333600 34682 sgd_solver.cpp:112] Iteration 76730, lr = 0.01
I0523 08:12:29.951242 34682 solver.cpp:239] Iteration 76740 (2.71218 iter/s, 3.68707s/10 iters), loss = 6.34207
I0523 08:12:29.951292 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.34207 (* 1 = 6.34207 loss)
I0523 08:12:30.012624 34682 sgd_solver.cpp:112] Iteration 76740, lr = 0.01
I0523 08:12:36.451915 34682 solver.cpp:239] Iteration 76750 (1.53838 iter/s, 6.50035s/10 iters), loss = 7.45773
I0523 08:12:36.451995 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.45773 (* 1 = 7.45773 loss)
I0523 08:12:36.522156 34682 sgd_solver.cpp:112] Iteration 76750, lr = 0.01
I0523 08:12:41.403439 34682 solver.cpp:239] Iteration 76760 (2.01969 iter/s, 4.95125s/10 iters), loss = 8.14233
I0523 08:12:41.403482 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14233 (* 1 = 8.14233 loss)
I0523 08:12:41.478271 34682 sgd_solver.cpp:112] Iteration 76760, lr = 0.01
I0523 08:12:46.770640 34682 solver.cpp:239] Iteration 76770 (1.86326 iter/s, 5.36693s/10 iters), loss = 8.24716
I0523 08:12:46.770736 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24716 (* 1 = 8.24716 loss)
I0523 08:12:47.493831 34682 sgd_solver.cpp:112] Iteration 76770, lr = 0.01
I0523 08:12:52.675029 34682 solver.cpp:239] Iteration 76780 (1.69374 iter/s, 5.90408s/10 iters), loss = 7.35989
I0523 08:12:52.675099 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.35989 (* 1 = 7.35989 loss)
I0523 08:12:53.303666 34682 sgd_solver.cpp:112] Iteration 76780, lr = 0.01
I0523 08:12:56.626399 34682 solver.cpp:239] Iteration 76790 (2.53092 iter/s, 3.95113s/10 iters), loss = 8.45462
I0523 08:12:56.626524 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45462 (* 1 = 8.45462 loss)
I0523 08:12:56.703207 34682 sgd_solver.cpp:112] Iteration 76790, lr = 0.01
I0523 08:13:00.256700 34682 solver.cpp:239] Iteration 76800 (2.7548 iter/s, 3.63003s/10 iters), loss = 7.64799
I0523 08:13:00.256742 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64799 (* 1 = 7.64799 loss)
I0523 08:13:00.324309 34682 sgd_solver.cpp:112] Iteration 76800, lr = 0.01
I0523 08:13:05.179899 34682 solver.cpp:239] Iteration 76810 (2.03131 iter/s, 4.92294s/10 iters), loss = 8.49026
I0523 08:13:05.179973 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49026 (* 1 = 8.49026 loss)
I0523 08:13:05.237251 34682 sgd_solver.cpp:112] Iteration 76810, lr = 0.01
I0523 08:13:11.458513 34682 solver.cpp:239] Iteration 76820 (1.59279 iter/s, 6.27829s/10 iters), loss = 7.39811
I0523 08:13:11.458576 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.39811 (* 1 = 7.39811 loss)
I0523 08:13:11.532140 34682 sgd_solver.cpp:112] Iteration 76820, lr = 0.01
I0523 08:13:16.887894 34682 solver.cpp:239] Iteration 76830 (1.84193 iter/s, 5.42909s/10 iters), loss = 6.94224
I0523 08:13:16.887939 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.94224 (* 1 = 6.94224 loss)
I0523 08:13:16.960551 34682 sgd_solver.cpp:112] Iteration 76830, lr = 0.01
I0523 08:13:21.174939 34682 solver.cpp:239] Iteration 76840 (2.33274 iter/s, 4.28681s/10 iters), loss = 8.20552
I0523 08:13:21.175002 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20552 (* 1 = 8.20552 loss)
I0523 08:13:21.975322 34682 sgd_solver.cpp:112] Iteration 76840, lr = 0.01
I0523 08:13:24.892278 34682 solver.cpp:239] Iteration 76850 (2.69026 iter/s, 3.71712s/10 iters), loss = 7.93666
I0523 08:13:24.892335 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93666 (* 1 = 7.93666 loss)
I0523 08:13:25.765087 34682 sgd_solver.cpp:112] Iteration 76850, lr = 0.01
I0523 08:13:30.833600 34682 solver.cpp:239] Iteration 76860 (1.68321 iter/s, 5.94103s/10 iters), loss = 7.26618
I0523 08:13:30.833825 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.26618 (* 1 = 7.26618 loss)
I0523 08:13:30.888969 34682 sgd_solver.cpp:112] Iteration 76860, lr = 0.01
I0523 08:13:35.945389 34682 solver.cpp:239] Iteration 76870 (1.95642 iter/s, 5.11139s/10 iters), loss = 8.09928
I0523 08:13:35.945439 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09928 (* 1 = 8.09928 loss)
I0523 08:13:36.766053 34682 sgd_solver.cpp:112] Iteration 76870, lr = 0.01
I0523 08:13:41.807924 34682 solver.cpp:239] Iteration 76880 (1.70583 iter/s, 5.86225s/10 iters), loss = 7.54692
I0523 08:13:41.807976 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54692 (* 1 = 7.54692 loss)
I0523 08:13:42.644538 34682 sgd_solver.cpp:112] Iteration 76880, lr = 0.01
I0523 08:13:46.515368 34682 solver.cpp:239] Iteration 76890 (2.1244 iter/s, 4.7072s/10 iters), loss = 8.11209
I0523 08:13:46.515411 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11209 (* 1 = 8.11209 loss)
I0523 08:13:46.590924 34682 sgd_solver.cpp:112] Iteration 76890, lr = 0.01
I0523 08:13:50.998664 34682 solver.cpp:239] Iteration 76900 (2.23062 iter/s, 4.48307s/10 iters), loss = 8.36056
I0523 08:13:50.998739 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36056 (* 1 = 8.36056 loss)
I0523 08:13:51.064800 34682 sgd_solver.cpp:112] Iteration 76900, lr = 0.01
I0523 08:13:55.268453 34682 solver.cpp:239] Iteration 76910 (2.34218 iter/s, 4.26953s/10 iters), loss = 8.12325
I0523 08:13:55.268514 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12325 (* 1 = 8.12325 loss)
I0523 08:13:56.071653 34682 sgd_solver.cpp:112] Iteration 76910, lr = 0.01
I0523 08:14:01.112006 34682 solver.cpp:239] Iteration 76920 (1.71137 iter/s, 5.84326s/10 iters), loss = 7.8097
I0523 08:14:01.112227 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8097 (* 1 = 7.8097 loss)
I0523 08:14:01.174243 34682 sgd_solver.cpp:112] Iteration 76920, lr = 0.01
I0523 08:14:07.100493 34682 solver.cpp:239] Iteration 76930 (1.67 iter/s, 5.98803s/10 iters), loss = 6.47856
I0523 08:14:07.100543 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.47856 (* 1 = 6.47856 loss)
I0523 08:14:07.180440 34682 sgd_solver.cpp:112] Iteration 76930, lr = 0.01
I0523 08:14:12.271318 34682 solver.cpp:239] Iteration 76940 (1.93402 iter/s, 5.17057s/10 iters), loss = 6.96239
I0523 08:14:12.271374 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.96239 (* 1 = 6.96239 loss)
I0523 08:14:13.059792 34682 sgd_solver.cpp:112] Iteration 76940, lr = 0.01
I0523 08:14:17.345924 34682 solver.cpp:239] Iteration 76950 (1.9707 iter/s, 5.07434s/10 iters), loss = 6.66714
I0523 08:14:17.345968 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.66714 (* 1 = 6.66714 loss)
I0523 08:14:17.404070 34682 sgd_solver.cpp:112] Iteration 76950, lr = 0.01
I0523 08:14:20.132954 34682 solver.cpp:239] Iteration 76960 (3.58826 iter/s, 2.78687s/10 iters), loss = 7.98053
I0523 08:14:20.133007 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98053 (* 1 = 7.98053 loss)
I0523 08:14:20.982484 34682 sgd_solver.cpp:112] Iteration 76960, lr = 0.01
I0523 08:14:24.921686 34682 solver.cpp:239] Iteration 76970 (2.08834 iter/s, 4.78849s/10 iters), loss = 7.93876
I0523 08:14:24.921730 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93876 (* 1 = 7.93876 loss)
I0523 08:14:24.990450 34682 sgd_solver.cpp:112] Iteration 76970, lr = 0.01
I0523 08:14:30.435417 34682 solver.cpp:239] Iteration 76980 (1.81374 iter/s, 5.51346s/10 iters), loss = 8.67145
I0523 08:14:30.435462 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.67145 (* 1 = 8.67145 loss)
I0523 08:14:31.241478 34682 sgd_solver.cpp:112] Iteration 76980, lr = 0.01
I0523 08:14:36.860344 34682 solver.cpp:239] Iteration 76990 (1.55651 iter/s, 6.42462s/10 iters), loss = 8.81099
I0523 08:14:36.860390 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81099 (* 1 = 8.81099 loss)
I0523 08:14:37.732952 34682 sgd_solver.cpp:112] Iteration 76990, lr = 0.01
I0523 08:14:41.897548 34682 solver.cpp:239] Iteration 77000 (1.98533 iter/s, 5.03695s/10 iters), loss = 7.1793
I0523 08:14:41.897603 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.1793 (* 1 = 7.1793 loss)
I0523 08:14:41.965915 34682 sgd_solver.cpp:112] Iteration 77000, lr = 0.01
I0523 08:14:46.143031 34682 solver.cpp:239] Iteration 77010 (2.35557 iter/s, 4.24526s/10 iters), loss = 8.16013
I0523 08:14:46.143092 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16013 (* 1 = 8.16013 loss)
I0523 08:14:46.211838 34682 sgd_solver.cpp:112] Iteration 77010, lr = 0.01
I0523 08:14:51.894181 34682 solver.cpp:239] Iteration 77020 (1.73887 iter/s, 5.75085s/10 iters), loss = 7.11697
I0523 08:14:51.894232 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.11697 (* 1 = 7.11697 loss)
I0523 08:14:51.954136 34682 sgd_solver.cpp:112] Iteration 77020, lr = 0.01
I0523 08:14:58.243041 34682 solver.cpp:239] Iteration 77030 (1.57516 iter/s, 6.34855s/10 iters), loss = 8.57757
I0523 08:14:58.243095 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57757 (* 1 = 8.57757 loss)
I0523 08:14:58.306723 34682 sgd_solver.cpp:112] Iteration 77030, lr = 0.01
I0523 08:15:02.514267 34682 solver.cpp:239] Iteration 77040 (2.34137 iter/s, 4.271s/10 iters), loss = 8.15722
I0523 08:15:02.514420 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15722 (* 1 = 8.15722 loss)
I0523 08:15:02.594854 34682 sgd_solver.cpp:112] Iteration 77040, lr = 0.01
I0523 08:15:06.457583 34682 solver.cpp:239] Iteration 77050 (2.53614 iter/s, 3.94301s/10 iters), loss = 8.78518
I0523 08:15:06.457633 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78518 (* 1 = 8.78518 loss)
I0523 08:15:06.531504 34682 sgd_solver.cpp:112] Iteration 77050, lr = 0.01
I0523 08:15:11.685278 34682 solver.cpp:239] Iteration 77060 (1.91299 iter/s, 5.22743s/10 iters), loss = 8.19252
I0523 08:15:11.685330 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.19252 (* 1 = 8.19252 loss)
I0523 08:15:12.429481 34682 sgd_solver.cpp:112] Iteration 77060, lr = 0.01
I0523 08:15:16.508278 34682 solver.cpp:239] Iteration 77070 (2.07351 iter/s, 4.82275s/10 iters), loss = 7.43217
I0523 08:15:16.508345 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43217 (* 1 = 7.43217 loss)
I0523 08:15:16.813829 34682 sgd_solver.cpp:112] Iteration 77070, lr = 0.01
I0523 08:15:20.436866 34682 solver.cpp:239] Iteration 77080 (2.54559 iter/s, 3.92835s/10 iters), loss = 8.90676
I0523 08:15:20.436916 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90676 (* 1 = 8.90676 loss)
I0523 08:15:21.243376 34682 sgd_solver.cpp:112] Iteration 77080, lr = 0.01
I0523 08:15:26.840530 34682 solver.cpp:239] Iteration 77090 (1.56168 iter/s, 6.40335s/10 iters), loss = 8.25136
I0523 08:15:26.840575 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25136 (* 1 = 8.25136 loss)
I0523 08:15:26.914258 34682 sgd_solver.cpp:112] Iteration 77090, lr = 0.01
I0523 08:15:31.931216 34682 solver.cpp:239] Iteration 77100 (1.96447 iter/s, 5.09044s/10 iters), loss = 7.90859
I0523 08:15:31.931254 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90859 (* 1 = 7.90859 loss)
I0523 08:15:32.008393 34682 sgd_solver.cpp:112] Iteration 77100, lr = 0.01
I0523 08:15:36.073429 34682 solver.cpp:239] Iteration 77110 (2.41429 iter/s, 4.142s/10 iters), loss = 7.43756
I0523 08:15:36.073606 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43756 (* 1 = 7.43756 loss)
I0523 08:15:36.153720 34682 sgd_solver.cpp:112] Iteration 77110, lr = 0.01
I0523 08:15:39.491745 34682 solver.cpp:239] Iteration 77120 (2.92567 iter/s, 3.41802s/10 iters), loss = 8.0036
I0523 08:15:39.491799 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0036 (* 1 = 8.0036 loss)
I0523 08:15:40.257381 34682 sgd_solver.cpp:112] Iteration 77120, lr = 0.01
I0523 08:15:44.193898 34682 solver.cpp:239] Iteration 77130 (2.1268 iter/s, 4.7019s/10 iters), loss = 7.99544
I0523 08:15:44.193949 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99544 (* 1 = 7.99544 loss)
I0523 08:15:44.256075 34682 sgd_solver.cpp:112] Iteration 77130, lr = 0.01
I0523 08:15:49.789559 34682 solver.cpp:239] Iteration 77140 (1.78719 iter/s, 5.59538s/10 iters), loss = 7.43748
I0523 08:15:49.789615 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43748 (* 1 = 7.43748 loss)
I0523 08:15:50.403852 34682 sgd_solver.cpp:112] Iteration 77140, lr = 0.01
I0523 08:15:56.044278 34682 solver.cpp:239] Iteration 77150 (1.59887 iter/s, 6.25441s/10 iters), loss = 7.71033
I0523 08:15:56.044328 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.71033 (* 1 = 7.71033 loss)
I0523 08:15:56.808241 34682 sgd_solver.cpp:112] Iteration 77150, lr = 0.01
I0523 08:16:01.325824 34682 solver.cpp:239] Iteration 77160 (1.89349 iter/s, 5.28125s/10 iters), loss = 7.82464
I0523 08:16:01.325861 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82464 (* 1 = 7.82464 loss)
I0523 08:16:01.401351 34682 sgd_solver.cpp:112] Iteration 77160, lr = 0.01
I0523 08:16:06.387087 34682 solver.cpp:239] Iteration 77170 (1.97589 iter/s, 5.06102s/10 iters), loss = 7.93623
I0523 08:16:06.387358 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93623 (* 1 = 7.93623 loss)
I0523 08:16:06.447840 34682 sgd_solver.cpp:112] Iteration 77170, lr = 0.01
I0523 08:16:10.559150 34682 solver.cpp:239] Iteration 77180 (2.39713 iter/s, 4.17166s/10 iters), loss = 7.27296
I0523 08:16:10.559200 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.27296 (* 1 = 7.27296 loss)
I0523 08:16:11.402827 34682 sgd_solver.cpp:112] Iteration 77180, lr = 0.01
I0523 08:16:16.386440 34682 solver.cpp:239] Iteration 77190 (1.71615 iter/s, 5.827s/10 iters), loss = 6.90487
I0523 08:16:16.386482 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.90487 (* 1 = 6.90487 loss)
I0523 08:16:16.467411 34682 sgd_solver.cpp:112] Iteration 77190, lr = 0.01
I0523 08:16:22.877303 34682 solver.cpp:239] Iteration 77200 (1.5407 iter/s, 6.49056s/10 iters), loss = 7.42866
I0523 08:16:22.877346 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42866 (* 1 = 7.42866 loss)
I0523 08:16:22.954807 34682 sgd_solver.cpp:112] Iteration 77200, lr = 0.01
I0523 08:16:28.503890 34682 solver.cpp:239] Iteration 77210 (1.77736 iter/s, 5.62631s/10 iters), loss = 7.27845
I0523 08:16:28.503942 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.27845 (* 1 = 7.27845 loss)
I0523 08:16:28.564815 34682 sgd_solver.cpp:112] Iteration 77210, lr = 0.01
I0523 08:16:31.251653 34682 solver.cpp:239] Iteration 77220 (3.63956 iter/s, 2.74759s/10 iters), loss = 7.88623
I0523 08:16:31.251703 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88623 (* 1 = 7.88623 loss)
I0523 08:16:31.328459 34682 sgd_solver.cpp:112] Iteration 77220, lr = 0.01
I0523 08:16:36.156129 34682 solver.cpp:239] Iteration 77230 (2.03906 iter/s, 4.90422s/10 iters), loss = 7.97375
I0523 08:16:36.156177 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97375 (* 1 = 7.97375 loss)
I0523 08:16:36.216830 34682 sgd_solver.cpp:112] Iteration 77230, lr = 0.01
I0523 08:16:41.523157 34682 solver.cpp:239] Iteration 77240 (1.86332 iter/s, 5.36676s/10 iters), loss = 7.44562
I0523 08:16:41.523301 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44562 (* 1 = 7.44562 loss)
I0523 08:16:42.340317 34682 sgd_solver.cpp:112] Iteration 77240, lr = 0.01
I0523 08:16:47.851433 34682 solver.cpp:239] Iteration 77250 (1.58031 iter/s, 6.32788s/10 iters), loss = 7.28377
I0523 08:16:47.851481 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.28377 (* 1 = 7.28377 loss)
I0523 08:16:47.916287 34682 sgd_solver.cpp:112] Iteration 77250, lr = 0.01
I0523 08:16:51.714048 34682 solver.cpp:239] Iteration 77260 (2.58906 iter/s, 3.8624s/10 iters), loss = 7.58539
I0523 08:16:51.714108 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58539 (* 1 = 7.58539 loss)
I0523 08:16:52.507295 34682 sgd_solver.cpp:112] Iteration 77260, lr = 0.01
I0523 08:16:56.433924 34682 solver.cpp:239] Iteration 77270 (2.11881 iter/s, 4.71962s/10 iters), loss = 6.96638
I0523 08:16:56.433972 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.96638 (* 1 = 6.96638 loss)
I0523 08:16:56.496559 34682 sgd_solver.cpp:112] Iteration 77270, lr = 0.01
I0523 08:17:00.555907 34682 solver.cpp:239] Iteration 77280 (2.42615 iter/s, 4.12176s/10 iters), loss = 7.09479
I0523 08:17:00.555960 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.09479 (* 1 = 7.09479 loss)
I0523 08:17:01.353345 34682 sgd_solver.cpp:112] Iteration 77280, lr = 0.01
I0523 08:17:05.706146 34682 solver.cpp:239] Iteration 77290 (1.94176 iter/s, 5.14997s/10 iters), loss = 7.41756
I0523 08:17:05.706200 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.41756 (* 1 = 7.41756 loss)
I0523 08:17:06.535764 34682 sgd_solver.cpp:112] Iteration 77290, lr = 0.01
I0523 08:17:11.959561 34682 solver.cpp:239] Iteration 77300 (1.5992 iter/s, 6.25311s/10 iters), loss = 8.04755
I0523 08:17:11.959749 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04755 (* 1 = 8.04755 loss)
I0523 08:17:12.036053 34682 sgd_solver.cpp:112] Iteration 77300, lr = 0.01
I0523 08:17:17.699543 34682 solver.cpp:239] Iteration 77310 (1.74229 iter/s, 5.73956s/10 iters), loss = 6.75058
I0523 08:17:17.699587 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.75058 (* 1 = 6.75058 loss)
I0523 08:17:17.775756 34682 sgd_solver.cpp:112] Iteration 77310, lr = 0.01
I0523 08:17:21.887470 34682 solver.cpp:239] Iteration 77320 (2.38794 iter/s, 4.1877s/10 iters), loss = 8.56744
I0523 08:17:21.887513 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56744 (* 1 = 8.56744 loss)
I0523 08:17:21.964148 34682 sgd_solver.cpp:112] Iteration 77320, lr = 0.01
I0523 08:17:25.735913 34682 solver.cpp:239] Iteration 77330 (2.5986 iter/s, 3.84823s/10 iters), loss = 6.77944
I0523 08:17:25.735970 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.77944 (* 1 = 6.77944 loss)
I0523 08:17:25.797112 34682 sgd_solver.cpp:112] Iteration 77330, lr = 0.01
I0523 08:17:29.083986 34682 solver.cpp:239] Iteration 77340 (2.98696 iter/s, 3.34788s/10 iters), loss = 7.18672
I0523 08:17:29.084025 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.18672 (* 1 = 7.18672 loss)
I0523 08:17:29.142302 34682 sgd_solver.cpp:112] Iteration 77340, lr = 0.01
I0523 08:17:34.097878 34682 solver.cpp:239] Iteration 77350 (1.99456 iter/s, 5.01364s/10 iters), loss = 8.34977
I0523 08:17:34.097929 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34977 (* 1 = 8.34977 loss)
I0523 08:17:34.170543 34682 sgd_solver.cpp:112] Iteration 77350, lr = 0.01
I0523 08:17:39.449764 34682 solver.cpp:239] Iteration 77360 (1.8686 iter/s, 5.35161s/10 iters), loss = 7.38413
I0523 08:17:39.449825 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.38413 (* 1 = 7.38413 loss)
I0523 08:17:40.068598 34682 sgd_solver.cpp:112] Iteration 77360, lr = 0.01
I0523 08:17:44.871759 34682 solver.cpp:239] Iteration 77370 (1.84443 iter/s, 5.42172s/10 iters), loss = 7.47628
I0523 08:17:44.871981 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.47628 (* 1 = 7.47628 loss)
I0523 08:17:45.694814 34682 sgd_solver.cpp:112] Iteration 77370, lr = 0.01
I0523 08:17:50.940672 34682 solver.cpp:239] Iteration 77380 (1.64786 iter/s, 6.06847s/10 iters), loss = 7.83285
I0523 08:17:50.940717 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83285 (* 1 = 7.83285 loss)
I0523 08:17:50.992308 34682 sgd_solver.cpp:112] Iteration 77380, lr = 0.01
I0523 08:17:55.185597 34682 solver.cpp:239] Iteration 77390 (2.35587 iter/s, 4.24471s/10 iters), loss = 7.36173
I0523 08:17:55.185638 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.36173 (* 1 = 7.36173 loss)
I0523 08:17:55.243052 34682 sgd_solver.cpp:112] Iteration 77390, lr = 0.01
I0523 08:18:00.152616 34682 solver.cpp:239] Iteration 77400 (2.01338 iter/s, 4.96677s/10 iters), loss = 7.17176
I0523 08:18:00.152659 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.17176 (* 1 = 7.17176 loss)
I0523 08:18:01.022500 34682 sgd_solver.cpp:112] Iteration 77400, lr = 0.01
I0523 08:18:04.314476 34682 solver.cpp:239] Iteration 77410 (2.40291 iter/s, 4.16163s/10 iters), loss = 7.19446
I0523 08:18:04.314530 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.19446 (* 1 = 7.19446 loss)
I0523 08:18:04.573107 34682 sgd_solver.cpp:112] Iteration 77410, lr = 0.01
I0523 08:18:08.729807 34682 solver.cpp:239] Iteration 77420 (2.26496 iter/s, 4.4151s/10 iters), loss = 8.29836
I0523 08:18:08.729851 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29836 (* 1 = 8.29836 loss)
I0523 08:18:08.797904 34682 sgd_solver.cpp:112] Iteration 77420, lr = 0.01
I0523 08:18:15.312124 34682 solver.cpp:239] Iteration 77430 (1.5193 iter/s, 6.58199s/10 iters), loss = 8.2248
I0523 08:18:15.312324 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2248 (* 1 = 8.2248 loss)
I0523 08:18:15.953578 34682 sgd_solver.cpp:112] Iteration 77430, lr = 0.01
I0523 08:18:20.756614 34682 solver.cpp:239] Iteration 77440 (1.83686 iter/s, 5.44407s/10 iters), loss = 8.09253
I0523 08:18:20.756675 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09253 (* 1 = 8.09253 loss)
I0523 08:18:20.830224 34682 sgd_solver.cpp:112] Iteration 77440, lr = 0.01
I0523 08:18:25.761883 34682 solver.cpp:239] Iteration 77450 (1.998 iter/s, 5.00501s/10 iters), loss = 8.46943
I0523 08:18:25.761960 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46943 (* 1 = 8.46943 loss)
I0523 08:18:25.837699 34682 sgd_solver.cpp:112] Iteration 77450, lr = 0.01
I0523 08:18:29.995878 34682 solver.cpp:239] Iteration 77460 (2.36197 iter/s, 4.23375s/10 iters), loss = 8.26468
I0523 08:18:29.995923 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26468 (* 1 = 8.26468 loss)
I0523 08:18:30.874452 34682 sgd_solver.cpp:112] Iteration 77460, lr = 0.01
I0523 08:18:34.875146 34682 solver.cpp:239] Iteration 77470 (2.04959 iter/s, 4.87902s/10 iters), loss = 8.17252
I0523 08:18:34.875191 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17252 (* 1 = 8.17252 loss)
I0523 08:18:34.943037 34682 sgd_solver.cpp:112] Iteration 77470, lr = 0.01
I0523 08:18:39.029804 34682 solver.cpp:239] Iteration 77480 (2.40707 iter/s, 4.15443s/10 iters), loss = 7.51126
I0523 08:18:39.029851 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51126 (* 1 = 7.51126 loss)
I0523 08:18:39.097367 34682 sgd_solver.cpp:112] Iteration 77480, lr = 0.01
I0523 08:18:44.556632 34682 solver.cpp:239] Iteration 77490 (1.80944 iter/s, 5.52656s/10 iters), loss = 7.43594
I0523 08:18:44.556671 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43594 (* 1 = 7.43594 loss)
I0523 08:18:44.625574 34682 sgd_solver.cpp:112] Iteration 77490, lr = 0.01
I0523 08:18:47.932823 34682 solver.cpp:239] Iteration 77500 (2.96208 iter/s, 3.37601s/10 iters), loss = 7.8355
I0523 08:18:47.933029 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8355 (* 1 = 7.8355 loss)
I0523 08:18:48.655334 34682 sgd_solver.cpp:112] Iteration 77500, lr = 0.01
I0523 08:18:52.871166 34682 solver.cpp:239] Iteration 77510 (2.02513 iter/s, 4.93795s/10 iters), loss = 9.03853
I0523 08:18:52.871207 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03853 (* 1 = 9.03853 loss)
I0523 08:18:53.676061 34682 sgd_solver.cpp:112] Iteration 77510, lr = 0.01
I0523 08:18:58.784229 34682 solver.cpp:239] Iteration 77520 (1.69125 iter/s, 5.91278s/10 iters), loss = 8.02066
I0523 08:18:58.784286 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02066 (* 1 = 8.02066 loss)
I0523 08:18:58.861824 34682 sgd_solver.cpp:112] Iteration 77520, lr = 0.01
I0523 08:19:04.438414 34682 solver.cpp:239] Iteration 77530 (1.76869 iter/s, 5.65389s/10 iters), loss = 8.75143
I0523 08:19:04.438483 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75143 (* 1 = 8.75143 loss)
I0523 08:19:05.313951 34682 sgd_solver.cpp:112] Iteration 77530, lr = 0.01
I0523 08:19:09.927896 34682 solver.cpp:239] Iteration 77540 (1.82176 iter/s, 5.48918s/10 iters), loss = 7.36613
I0523 08:19:09.927965 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.36613 (* 1 = 7.36613 loss)
I0523 08:19:10.752792 34682 sgd_solver.cpp:112] Iteration 77540, lr = 0.01
I0523 08:19:16.324584 34682 solver.cpp:239] Iteration 77550 (1.56339 iter/s, 6.39635s/10 iters), loss = 7.05907
I0523 08:19:16.324636 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.05907 (* 1 = 7.05907 loss)
I0523 08:19:16.391676 34682 sgd_solver.cpp:112] Iteration 77550, lr = 0.01
I0523 08:19:21.166465 34682 solver.cpp:239] Iteration 77560 (2.06542 iter/s, 4.84162s/10 iters), loss = 7.64901
I0523 08:19:21.166769 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64901 (* 1 = 7.64901 loss)
I0523 08:19:21.996879 34682 sgd_solver.cpp:112] Iteration 77560, lr = 0.01
I0523 08:19:24.678417 34682 solver.cpp:239] Iteration 77570 (2.84776 iter/s, 3.51153s/10 iters), loss = 8.14826
I0523 08:19:24.678491 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14826 (* 1 = 8.14826 loss)
I0523 08:19:25.551030 34682 sgd_solver.cpp:112] Iteration 77570, lr = 0.01
I0523 08:19:30.470003 34682 solver.cpp:239] Iteration 77580 (1.72673 iter/s, 5.79129s/10 iters), loss = 7.92553
I0523 08:19:30.470046 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92553 (* 1 = 7.92553 loss)
I0523 08:19:30.539139 34682 sgd_solver.cpp:112] Iteration 77580, lr = 0.01
I0523 08:19:35.218955 34682 solver.cpp:239] Iteration 77590 (2.10583 iter/s, 4.74871s/10 iters), loss = 7.96573
I0523 08:19:35.219008 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96573 (* 1 = 7.96573 loss)
I0523 08:19:35.294584 34682 sgd_solver.cpp:112] Iteration 77590, lr = 0.01
I0523 08:19:42.406777 34682 solver.cpp:239] Iteration 77600 (1.39131 iter/s, 7.18748s/10 iters), loss = 8.24494
I0523 08:19:42.406821 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24494 (* 1 = 8.24494 loss)
I0523 08:19:42.473661 34682 sgd_solver.cpp:112] Iteration 77600, lr = 0.01
I0523 08:19:45.116313 34682 solver.cpp:239] Iteration 77610 (3.6909 iter/s, 2.70937s/10 iters), loss = 8.07005
I0523 08:19:45.116371 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07005 (* 1 = 8.07005 loss)
I0523 08:19:45.940474 34682 sgd_solver.cpp:112] Iteration 77610, lr = 0.01
I0523 08:19:49.471518 34682 solver.cpp:239] Iteration 77620 (2.29623 iter/s, 4.35497s/10 iters), loss = 7.79924
I0523 08:19:49.471576 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79924 (* 1 = 7.79924 loss)
I0523 08:19:49.532541 34682 sgd_solver.cpp:112] Iteration 77620, lr = 0.01
I0523 08:19:53.144071 34682 solver.cpp:239] Iteration 77630 (2.72305 iter/s, 3.67235s/10 iters), loss = 8.09738
I0523 08:19:53.144312 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09738 (* 1 = 8.09738 loss)
I0523 08:19:54.002411 34682 sgd_solver.cpp:112] Iteration 77630, lr = 0.01
I0523 08:19:58.264297 34682 solver.cpp:239] Iteration 77640 (1.9532 iter/s, 5.1198s/10 iters), loss = 7.68661
I0523 08:19:58.264344 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68661 (* 1 = 7.68661 loss)
I0523 08:19:58.324959 34682 sgd_solver.cpp:112] Iteration 77640, lr = 0.01
I0523 08:20:02.532244 34682 solver.cpp:239] Iteration 77650 (2.34317 iter/s, 4.26773s/10 iters), loss = 6.68995
I0523 08:20:02.532289 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.68995 (* 1 = 6.68995 loss)
I0523 08:20:02.589921 34682 sgd_solver.cpp:112] Iteration 77650, lr = 0.01
I0523 08:20:07.347002 34682 solver.cpp:239] Iteration 77660 (2.07705 iter/s, 4.81451s/10 iters), loss = 8.08584
I0523 08:20:07.347054 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.08584 (* 1 = 8.08584 loss)
I0523 08:20:07.423332 34682 sgd_solver.cpp:112] Iteration 77660, lr = 0.01
I0523 08:20:10.705896 34682 solver.cpp:239] Iteration 77670 (2.97734 iter/s, 3.3587s/10 iters), loss = 7.67803
I0523 08:20:10.705945 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.67803 (* 1 = 7.67803 loss)
I0523 08:20:11.516129 34682 sgd_solver.cpp:112] Iteration 77670, lr = 0.01
I0523 08:20:15.937688 34682 solver.cpp:239] Iteration 77680 (1.91149 iter/s, 5.23153s/10 iters), loss = 6.53084
I0523 08:20:15.937741 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.53084 (* 1 = 6.53084 loss)
I0523 08:20:16.004999 34682 sgd_solver.cpp:112] Iteration 77680, lr = 0.01
I0523 08:20:20.965214 34682 solver.cpp:239] Iteration 77690 (1.98915 iter/s, 5.02727s/10 iters), loss = 7.0407
I0523 08:20:20.965258 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.0407 (* 1 = 7.0407 loss)
I0523 08:20:21.030815 34682 sgd_solver.cpp:112] Iteration 77690, lr = 0.01
I0523 08:20:25.218797 34682 solver.cpp:239] Iteration 77700 (2.35108 iter/s, 4.25336s/10 iters), loss = 8.29782
I0523 08:20:25.219077 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29782 (* 1 = 8.29782 loss)
I0523 08:20:26.037452 34682 sgd_solver.cpp:112] Iteration 77700, lr = 0.01
I0523 08:20:30.160670 34682 solver.cpp:239] Iteration 77710 (2.02371 iter/s, 4.94142s/10 iters), loss = 7.79686
I0523 08:20:30.160719 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79686 (* 1 = 7.79686 loss)
I0523 08:20:31.001390 34682 sgd_solver.cpp:112] Iteration 77710, lr = 0.01
I0523 08:20:34.180399 34682 solver.cpp:239] Iteration 77720 (2.48786 iter/s, 4.01951s/10 iters), loss = 8.25472
I0523 08:20:34.180449 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25472 (* 1 = 8.25472 loss)
I0523 08:20:34.256680 34682 sgd_solver.cpp:112] Iteration 77720, lr = 0.01
I0523 08:20:37.465435 34682 solver.cpp:239] Iteration 77730 (3.0443 iter/s, 3.28483s/10 iters), loss = 8.44608
I0523 08:20:37.465502 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44608 (* 1 = 8.44608 loss)
I0523 08:20:37.758715 34682 sgd_solver.cpp:112] Iteration 77730, lr = 0.01
I0523 08:20:40.297394 34682 solver.cpp:239] Iteration 77740 (3.53137 iter/s, 2.83177s/10 iters), loss = 8.59913
I0523 08:20:40.297462 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.59913 (* 1 = 8.59913 loss)
I0523 08:20:40.369794 34682 sgd_solver.cpp:112] Iteration 77740, lr = 0.01
I0523 08:20:42.992566 34682 solver.cpp:239] Iteration 77750 (3.71058 iter/s, 2.695s/10 iters), loss = 8.12095
I0523 08:20:42.992607 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12095 (* 1 = 8.12095 loss)
I0523 08:20:43.853000 34682 sgd_solver.cpp:112] Iteration 77750, lr = 0.01
I0523 08:20:48.231241 34682 solver.cpp:239] Iteration 77760 (1.90897 iter/s, 5.23842s/10 iters), loss = 7.78273
I0523 08:20:48.231294 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.78273 (* 1 = 7.78273 loss)
I0523 08:20:49.050815 34682 sgd_solver.cpp:112] Iteration 77760, lr = 0.01
I0523 08:20:54.890396 34682 solver.cpp:239] Iteration 77770 (1.50176 iter/s, 6.65883s/10 iters), loss = 7.86518
I0523 08:20:54.890445 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.86518 (* 1 = 7.86518 loss)
I0523 08:20:55.713623 34682 sgd_solver.cpp:112] Iteration 77770, lr = 0.01
I0523 08:21:00.889470 34682 solver.cpp:239] Iteration 77780 (1.66701 iter/s, 5.99878s/10 iters), loss = 7.23703
I0523 08:21:00.889520 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.23703 (* 1 = 7.23703 loss)
I0523 08:21:00.964308 34682 sgd_solver.cpp:112] Iteration 77780, lr = 0.01
I0523 08:21:04.657941 34682 solver.cpp:239] Iteration 77790 (2.65374 iter/s, 3.76826s/10 iters), loss = 7.14811
I0523 08:21:04.657982 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.14811 (* 1 = 7.14811 loss)
I0523 08:21:04.728008 34682 sgd_solver.cpp:112] Iteration 77790, lr = 0.01
I0523 08:21:08.953722 34682 solver.cpp:239] Iteration 77800 (2.32798 iter/s, 4.29556s/10 iters), loss = 6.88699
I0523 08:21:08.953775 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.88699 (* 1 = 6.88699 loss)
I0523 08:21:09.028074 34682 sgd_solver.cpp:112] Iteration 77800, lr = 0.01
I0523 08:21:15.327090 34682 solver.cpp:239] Iteration 77810 (1.56911 iter/s, 6.37306s/10 iters), loss = 7.51079
I0523 08:21:15.327145 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51079 (* 1 = 7.51079 loss)
I0523 08:21:15.395613 34682 sgd_solver.cpp:112] Iteration 77810, lr = 0.01
I0523 08:21:21.122292 34682 solver.cpp:239] Iteration 77820 (1.72565 iter/s, 5.7949s/10 iters), loss = 8.20532
I0523 08:21:21.122362 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20532 (* 1 = 8.20532 loss)
I0523 08:21:22.001421 34682 sgd_solver.cpp:112] Iteration 77820, lr = 0.01
I0523 08:21:26.166656 34682 solver.cpp:239] Iteration 77830 (1.98252 iter/s, 5.04409s/10 iters), loss = 8.54459
I0523 08:21:26.166996 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.54459 (* 1 = 8.54459 loss)
I0523 08:21:26.229049 34682 sgd_solver.cpp:112] Iteration 77830, lr = 0.01
I0523 08:21:31.002717 34682 solver.cpp:239] Iteration 77840 (2.06802 iter/s, 4.83553s/10 iters), loss = 7.82299
I0523 08:21:31.002773 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82299 (* 1 = 7.82299 loss)
I0523 08:21:31.061988 34682 sgd_solver.cpp:112] Iteration 77840, lr = 0.01
I0523 08:21:33.795286 34682 solver.cpp:239] Iteration 77850 (3.58413 iter/s, 2.79008s/10 iters), loss = 7.42418
I0523 08:21:33.795363 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42418 (* 1 = 7.42418 loss)
I0523 08:21:33.860958 34682 sgd_solver.cpp:112] Iteration 77850, lr = 0.01
I0523 08:21:39.308281 34682 solver.cpp:239] Iteration 77860 (1.81399 iter/s, 5.5127s/10 iters), loss = 7.27616
I0523 08:21:39.308321 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.27616 (* 1 = 7.27616 loss)
I0523 08:21:40.041828 34682 sgd_solver.cpp:112] Iteration 77860, lr = 0.01
I0523 08:21:44.759047 34682 solver.cpp:239] Iteration 77870 (1.83469 iter/s, 5.4505s/10 iters), loss = 7.9702
I0523 08:21:44.759095 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9702 (* 1 = 7.9702 loss)
I0523 08:21:44.832356 34682 sgd_solver.cpp:112] Iteration 77870, lr = 0.01
I0523 08:21:48.268702 34682 solver.cpp:239] Iteration 77880 (2.84945 iter/s, 3.50945s/10 iters), loss = 7.82043
I0523 08:21:48.268770 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82043 (* 1 = 7.82043 loss)
I0523 08:21:48.965109 34682 sgd_solver.cpp:112] Iteration 77880, lr = 0.01
I0523 08:21:54.354513 34682 solver.cpp:239] Iteration 77890 (1.64325 iter/s, 6.0855s/10 iters), loss = 8.49228
I0523 08:21:54.354568 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.49228 (* 1 = 8.49228 loss)
I0523 08:21:54.987175 34682 sgd_solver.cpp:112] Iteration 77890, lr = 0.01
I0523 08:21:58.940827 34682 solver.cpp:239] Iteration 77900 (2.18051 iter/s, 4.58608s/10 iters), loss = 7.68523
I0523 08:21:58.940948 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68523 (* 1 = 7.68523 loss)
I0523 08:21:59.013721 34682 sgd_solver.cpp:112] Iteration 77900, lr = 0.01
I0523 08:22:03.049978 34682 solver.cpp:239] Iteration 77910 (2.43377 iter/s, 4.10886s/10 iters), loss = 8.72522
I0523 08:22:03.050022 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.72522 (* 1 = 8.72522 loss)
I0523 08:22:03.128774 34682 sgd_solver.cpp:112] Iteration 77910, lr = 0.01
I0523 08:22:08.450739 34682 solver.cpp:239] Iteration 77920 (1.85169 iter/s, 5.40048s/10 iters), loss = 7.09649
I0523 08:22:08.450793 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.09649 (* 1 = 7.09649 loss)
I0523 08:22:09.167572 34682 sgd_solver.cpp:112] Iteration 77920, lr = 0.01
I0523 08:22:14.502516 34682 solver.cpp:239] Iteration 77930 (1.6525 iter/s, 6.05143s/10 iters), loss = 8.4608
I0523 08:22:14.502595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4608 (* 1 = 8.4608 loss)
I0523 08:22:14.562319 34682 sgd_solver.cpp:112] Iteration 77930, lr = 0.01
I0523 08:22:19.062911 34682 solver.cpp:239] Iteration 77940 (2.19292 iter/s, 4.56013s/10 iters), loss = 7.64832
I0523 08:22:19.062963 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.64832 (* 1 = 7.64832 loss)
I0523 08:22:19.841964 34682 sgd_solver.cpp:112] Iteration 77940, lr = 0.01
I0523 08:22:22.796686 34682 solver.cpp:239] Iteration 77950 (2.6784 iter/s, 3.73357s/10 iters), loss = 7.9173
I0523 08:22:22.796732 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9173 (* 1 = 7.9173 loss)
I0523 08:22:22.862660 34682 sgd_solver.cpp:112] Iteration 77950, lr = 0.01
I0523 08:22:26.758304 34682 solver.cpp:239] Iteration 77960 (2.52436 iter/s, 3.96141s/10 iters), loss = 6.93859
I0523 08:22:26.758358 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.93859 (* 1 = 6.93859 loss)
I0523 08:22:26.823510 34682 sgd_solver.cpp:112] Iteration 77960, lr = 0.01
I0523 08:22:31.446735 34682 solver.cpp:239] Iteration 77970 (2.13304 iter/s, 4.68815s/10 iters), loss = 7.56869
I0523 08:22:31.446975 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56869 (* 1 = 7.56869 loss)
I0523 08:22:31.508203 34682 sgd_solver.cpp:112] Iteration 77970, lr = 0.01
I0523 08:22:36.085841 34682 solver.cpp:239] Iteration 77980 (2.15578 iter/s, 4.6387s/10 iters), loss = 8.95092
I0523 08:22:36.085889 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.95092 (* 1 = 8.95092 loss)
I0523 08:22:36.134286 34682 sgd_solver.cpp:112] Iteration 77980, lr = 0.01
I0523 08:22:41.130800 34682 solver.cpp:239] Iteration 77990 (1.98228 iter/s, 5.04469s/10 iters), loss = 8.23641
I0523 08:22:41.130861 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23641 (* 1 = 8.23641 loss)
I0523 08:22:41.927016 34682 sgd_solver.cpp:112] Iteration 77990, lr = 0.01
I0523 08:22:47.835391 34682 solver.cpp:239] Iteration 78000 (1.49159 iter/s, 6.70426s/10 iters), loss = 7.9904
I0523 08:22:47.835436 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9904 (* 1 = 7.9904 loss)
I0523 08:22:48.670156 34682 sgd_solver.cpp:112] Iteration 78000, lr = 0.01
I0523 08:22:52.317266 34682 solver.cpp:239] Iteration 78010 (2.23132 iter/s, 4.48164s/10 iters), loss = 7.85261
I0523 08:22:52.317314 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.85261 (* 1 = 7.85261 loss)
I0523 08:22:52.393857 34682 sgd_solver.cpp:112] Iteration 78010, lr = 0.01
I0523 08:22:55.511998 34682 solver.cpp:239] Iteration 78020 (3.13033 iter/s, 3.19455s/10 iters), loss = 8.78607
I0523 08:22:55.512044 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78607 (* 1 = 8.78607 loss)
I0523 08:22:56.000403 34682 sgd_solver.cpp:112] Iteration 78020, lr = 0.01
I0523 08:23:01.731320 34682 solver.cpp:239] Iteration 78030 (1.60797 iter/s, 6.21902s/10 iters), loss = 7.50099
I0523 08:23:01.731590 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50099 (* 1 = 7.50099 loss)
I0523 08:23:02.569159 34682 sgd_solver.cpp:112] Iteration 78030, lr = 0.01
I0523 08:23:06.070823 34682 solver.cpp:239] Iteration 78040 (2.30464 iter/s, 4.33907s/10 iters), loss = 7.23556
I0523 08:23:06.070878 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.23556 (* 1 = 7.23556 loss)
I0523 08:23:06.847666 34682 sgd_solver.cpp:112] Iteration 78040, lr = 0.01
I0523 08:23:13.597370 34682 solver.cpp:239] Iteration 78050 (1.32869 iter/s, 7.52619s/10 iters), loss = 7.90639
I0523 08:23:13.597409 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90639 (* 1 = 7.90639 loss)
I0523 08:23:13.669800 34682 sgd_solver.cpp:112] Iteration 78050, lr = 0.01
I0523 08:23:18.241279 34682 solver.cpp:239] Iteration 78060 (2.15347 iter/s, 4.64367s/10 iters), loss = 7.37807
I0523 08:23:18.241325 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.37807 (* 1 = 7.37807 loss)
I0523 08:23:18.297682 34682 sgd_solver.cpp:112] Iteration 78060, lr = 0.01
I0523 08:23:20.129022 34682 solver.cpp:239] Iteration 78070 (5.29769 iter/s, 1.88761s/10 iters), loss = 7.87983
I0523 08:23:20.129066 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87983 (* 1 = 7.87983 loss)
I0523 08:23:20.255123 34682 sgd_solver.cpp:112] Iteration 78070, lr = 0.01
I0523 08:23:25.933543 34682 solver.cpp:239] Iteration 78080 (1.72289 iter/s, 5.80419s/10 iters), loss = 8.40386
I0523 08:23:25.933599 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40386 (* 1 = 8.40386 loss)
I0523 08:23:26.664867 34682 sgd_solver.cpp:112] Iteration 78080, lr = 0.01
I0523 08:23:29.933454 34682 solver.cpp:239] Iteration 78090 (2.50019 iter/s, 3.99969s/10 iters), loss = 8.57618
I0523 08:23:29.933506 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.57618 (* 1 = 8.57618 loss)
I0523 08:23:30.727833 34682 sgd_solver.cpp:112] Iteration 78090, lr = 0.01
I0523 08:23:37.040868 34682 solver.cpp:239] Iteration 78100 (1.40705 iter/s, 7.10707s/10 iters), loss = 8.01547
I0523 08:23:37.041076 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01547 (* 1 = 8.01547 loss)
I0523 08:23:37.673817 34682 sgd_solver.cpp:112] Iteration 78100, lr = 0.01
I0523 08:23:41.993623 34682 solver.cpp:239] Iteration 78110 (2.01924 iter/s, 4.95236s/10 iters), loss = 7.76388
I0523 08:23:41.993665 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.76388 (* 1 = 7.76388 loss)
I0523 08:23:42.066493 34682 sgd_solver.cpp:112] Iteration 78110, lr = 0.01
I0523 08:23:46.739255 34682 solver.cpp:239] Iteration 78120 (2.10857 iter/s, 4.74255s/10 iters), loss = 8.47535
I0523 08:23:46.739317 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47535 (* 1 = 8.47535 loss)
I0523 08:23:47.091449 34682 sgd_solver.cpp:112] Iteration 78120, lr = 0.01
I0523 08:23:53.185901 34682 solver.cpp:239] Iteration 78130 (1.55127 iter/s, 6.44632s/10 iters), loss = 7.23218
I0523 08:23:53.185947 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.23218 (* 1 = 7.23218 loss)
I0523 08:23:53.254169 34682 sgd_solver.cpp:112] Iteration 78130, lr = 0.01
I0523 08:23:58.427096 34682 solver.cpp:239] Iteration 78140 (1.90806 iter/s, 5.24093s/10 iters), loss = 8.75119
I0523 08:23:58.427168 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75119 (* 1 = 8.75119 loss)
I0523 08:23:59.256762 34682 sgd_solver.cpp:112] Iteration 78140, lr = 0.01
I0523 08:24:01.922850 34682 solver.cpp:239] Iteration 78150 (2.86079 iter/s, 3.49554s/10 iters), loss = 7.66233
I0523 08:24:01.922919 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66233 (* 1 = 7.66233 loss)
I0523 08:24:02.501840 34682 sgd_solver.cpp:112] Iteration 78150, lr = 0.01
I0523 08:24:08.074939 34682 solver.cpp:239] Iteration 78160 (1.62555 iter/s, 6.15177s/10 iters), loss = 7.65859
I0523 08:24:08.075116 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.65859 (* 1 = 7.65859 loss)
I0523 08:24:08.313241 34682 sgd_solver.cpp:112] Iteration 78160, lr = 0.01
I0523 08:24:12.487242 34682 solver.cpp:239] Iteration 78170 (2.26657 iter/s, 4.41194s/10 iters), loss = 7.24281
I0523 08:24:12.487303 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.24281 (* 1 = 7.24281 loss)
I0523 08:24:12.550628 34682 sgd_solver.cpp:112] Iteration 78170, lr = 0.01
I0523 08:24:16.660109 34682 solver.cpp:239] Iteration 78180 (2.39656 iter/s, 4.17264s/10 iters), loss = 8.40771
I0523 08:24:16.660166 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40771 (* 1 = 8.40771 loss)
I0523 08:24:17.407408 34682 sgd_solver.cpp:112] Iteration 78180, lr = 0.01
I0523 08:24:22.416901 34682 solver.cpp:239] Iteration 78190 (1.73717 iter/s, 5.7565s/10 iters), loss = 7.04262
I0523 08:24:22.416968 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.04262 (* 1 = 7.04262 loss)
I0523 08:24:23.220577 34682 sgd_solver.cpp:112] Iteration 78190, lr = 0.01
I0523 08:24:27.849388 34682 solver.cpp:239] Iteration 78200 (1.84088 iter/s, 5.43218s/10 iters), loss = 8.48823
I0523 08:24:27.849472 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48823 (* 1 = 8.48823 loss)
I0523 08:24:27.905148 34682 sgd_solver.cpp:112] Iteration 78200, lr = 0.01
I0523 08:24:32.799032 34682 solver.cpp:239] Iteration 78210 (2.02047 iter/s, 4.94936s/10 iters), loss = 7.20158
I0523 08:24:32.799074 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.20158 (* 1 = 7.20158 loss)
I0523 08:24:32.877123 34682 sgd_solver.cpp:112] Iteration 78210, lr = 0.01
I0523 08:24:36.246299 34682 solver.cpp:239] Iteration 78220 (2.90101 iter/s, 3.44708s/10 iters), loss = 8.0263
I0523 08:24:36.246358 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0263 (* 1 = 8.0263 loss)
I0523 08:24:37.060371 34682 sgd_solver.cpp:112] Iteration 78220, lr = 0.01
I0523 08:24:42.103837 34682 solver.cpp:239] Iteration 78230 (1.70729 iter/s, 5.85725s/10 iters), loss = 7.16822
I0523 08:24:42.103947 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.16822 (* 1 = 7.16822 loss)
I0523 08:24:42.176973 34682 sgd_solver.cpp:112] Iteration 78230, lr = 0.01
I0523 08:24:48.170775 34682 solver.cpp:239] Iteration 78240 (1.64837 iter/s, 6.06658s/10 iters), loss = 6.35518
I0523 08:24:48.170825 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.35518 (* 1 = 6.35518 loss)
I0523 08:24:48.242687 34682 sgd_solver.cpp:112] Iteration 78240, lr = 0.01
I0523 08:24:52.068073 34682 solver.cpp:239] Iteration 78250 (2.56604 iter/s, 3.89706s/10 iters), loss = 7.31424
I0523 08:24:52.068210 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.31424 (* 1 = 7.31424 loss)
I0523 08:24:52.842618 34682 sgd_solver.cpp:112] Iteration 78250, lr = 0.01
I0523 08:24:57.540591 34682 solver.cpp:239] Iteration 78260 (1.82743 iter/s, 5.47217s/10 iters), loss = 7.90148
I0523 08:24:57.540632 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90148 (* 1 = 7.90148 loss)
I0523 08:24:57.611493 34682 sgd_solver.cpp:112] Iteration 78260, lr = 0.01
I0523 08:25:02.470197 34682 solver.cpp:239] Iteration 78270 (2.02866 iter/s, 4.92935s/10 iters), loss = 8.52154
I0523 08:25:02.470252 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52154 (* 1 = 8.52154 loss)
I0523 08:25:02.539847 34682 sgd_solver.cpp:112] Iteration 78270, lr = 0.01
I0523 08:25:09.131127 34682 solver.cpp:239] Iteration 78280 (1.50136 iter/s, 6.66061s/10 iters), loss = 7.16616
I0523 08:25:09.131186 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.16616 (* 1 = 7.16616 loss)
I0523 08:25:09.218799 34682 sgd_solver.cpp:112] Iteration 78280, lr = 0.01
I0523 08:25:14.149924 34682 solver.cpp:239] Iteration 78290 (1.99261 iter/s, 5.01853s/10 iters), loss = 8.26143
I0523 08:25:14.150164 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26143 (* 1 = 8.26143 loss)
I0523 08:25:14.984275 34682 sgd_solver.cpp:112] Iteration 78290, lr = 0.01
I0523 08:25:19.994537 34682 solver.cpp:239] Iteration 78300 (1.71111 iter/s, 5.84415s/10 iters), loss = 7.22978
I0523 08:25:19.994590 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.22978 (* 1 = 7.22978 loss)
I0523 08:25:20.056762 34682 sgd_solver.cpp:112] Iteration 78300, lr = 0.01
I0523 08:25:27.832162 34682 solver.cpp:239] Iteration 78310 (1.27596 iter/s, 7.83724s/10 iters), loss = 7.76434
I0523 08:25:27.832228 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.76434 (* 1 = 7.76434 loss)
I0523 08:25:27.891387 34682 sgd_solver.cpp:112] Iteration 78310, lr = 0.01
I0523 08:25:33.536810 34682 solver.cpp:239] Iteration 78320 (1.75305 iter/s, 5.70435s/10 iters), loss = 6.73918
I0523 08:25:33.536851 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.73918 (* 1 = 6.73918 loss)
I0523 08:25:33.610771 34682 sgd_solver.cpp:112] Iteration 78320, lr = 0.01
I0523 08:25:39.256089 34682 solver.cpp:239] Iteration 78330 (1.74855 iter/s, 5.71901s/10 iters), loss = 7.798
I0523 08:25:39.256129 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.798 (* 1 = 7.798 loss)
I0523 08:25:40.019136 34682 sgd_solver.cpp:112] Iteration 78330, lr = 0.01
I0523 08:25:43.369683 34682 solver.cpp:239] Iteration 78340 (2.43109 iter/s, 4.11338s/10 iters), loss = 7.90954
I0523 08:25:43.369732 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90954 (* 1 = 7.90954 loss)
I0523 08:25:43.445382 34682 sgd_solver.cpp:112] Iteration 78340, lr = 0.01
I0523 08:25:50.010620 34682 solver.cpp:239] Iteration 78350 (1.50589 iter/s, 6.64061s/10 iters), loss = 7.88281
I0523 08:25:50.010725 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88281 (* 1 = 7.88281 loss)
I0523 08:25:50.718686 34682 sgd_solver.cpp:112] Iteration 78350, lr = 0.01
I0523 08:25:54.115252 34682 solver.cpp:239] Iteration 78360 (2.43644 iter/s, 4.10435s/10 iters), loss = 7.91754
I0523 08:25:54.115300 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91754 (* 1 = 7.91754 loss)
I0523 08:25:54.178645 34682 sgd_solver.cpp:112] Iteration 78360, lr = 0.01
I0523 08:25:57.499889 34682 solver.cpp:239] Iteration 78370 (2.9547 iter/s, 3.38444s/10 iters), loss = 8.35758
I0523 08:25:57.499944 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35758 (* 1 = 8.35758 loss)
I0523 08:25:57.565939 34682 sgd_solver.cpp:112] Iteration 78370, lr = 0.01
I0523 08:26:01.935389 34682 solver.cpp:239] Iteration 78380 (2.25466 iter/s, 4.43527s/10 iters), loss = 9.12675
I0523 08:26:01.935434 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.12675 (* 1 = 9.12675 loss)
I0523 08:26:02.788765 34682 sgd_solver.cpp:112] Iteration 78380, lr = 0.01
I0523 08:26:06.974128 34682 solver.cpp:239] Iteration 78390 (1.98473 iter/s, 5.03847s/10 iters), loss = 6.99316
I0523 08:26:06.974189 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.99316 (* 1 = 6.99316 loss)
I0523 08:26:07.798173 34682 sgd_solver.cpp:112] Iteration 78390, lr = 0.01
I0523 08:26:12.084134 34682 solver.cpp:239] Iteration 78400 (1.95705 iter/s, 5.10974s/10 iters), loss = 7.07831
I0523 08:26:12.084178 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.07831 (* 1 = 7.07831 loss)
I0523 08:26:12.773855 34682 sgd_solver.cpp:112] Iteration 78400, lr = 0.01
I0523 08:26:16.010782 34682 solver.cpp:239] Iteration 78410 (2.54684 iter/s, 3.92644s/10 iters), loss = 8.13852
I0523 08:26:16.010821 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13852 (* 1 = 8.13852 loss)
I0523 08:26:16.088155 34682 sgd_solver.cpp:112] Iteration 78410, lr = 0.01
I0523 08:26:21.142021 34682 solver.cpp:239] Iteration 78420 (1.94895 iter/s, 5.13098s/10 iters), loss = 8.17275
I0523 08:26:21.142449 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17275 (* 1 = 8.17275 loss)
I0523 08:26:21.200848 34682 sgd_solver.cpp:112] Iteration 78420, lr = 0.01
I0523 08:26:27.677916 34682 solver.cpp:239] Iteration 78430 (1.53015 iter/s, 6.5353s/10 iters), loss = 7.52064
I0523 08:26:27.677958 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.52064 (* 1 = 7.52064 loss)
I0523 08:26:27.741673 34682 sgd_solver.cpp:112] Iteration 78430, lr = 0.01
I0523 08:26:31.029964 34682 solver.cpp:239] Iteration 78440 (2.98343 iter/s, 3.35184s/10 iters), loss = 8.41108
I0523 08:26:31.030023 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41108 (* 1 = 8.41108 loss)
I0523 08:26:31.706953 34682 sgd_solver.cpp:112] Iteration 78440, lr = 0.01
I0523 08:26:36.473063 34682 solver.cpp:239] Iteration 78450 (1.83728 iter/s, 5.44281s/10 iters), loss = 7.21119
I0523 08:26:36.473122 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.21119 (* 1 = 7.21119 loss)
I0523 08:26:36.547096 34682 sgd_solver.cpp:112] Iteration 78450, lr = 0.01
I0523 08:26:42.186795 34682 solver.cpp:239] Iteration 78460 (1.751 iter/s, 5.71101s/10 iters), loss = 7.18333
I0523 08:26:42.186853 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.18333 (* 1 = 7.18333 loss)
I0523 08:26:42.268246 34682 sgd_solver.cpp:112] Iteration 78460, lr = 0.01
I0523 08:26:45.440806 34682 solver.cpp:239] Iteration 78470 (3.07333 iter/s, 3.2538s/10 iters), loss = 8.6707
I0523 08:26:45.440866 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6707 (* 1 = 8.6707 loss)
I0523 08:26:45.513384 34682 sgd_solver.cpp:112] Iteration 78470, lr = 0.01
I0523 08:26:49.026991 34682 solver.cpp:239] Iteration 78480 (2.79165 iter/s, 3.58211s/10 iters), loss = 9.25387
I0523 08:26:49.027045 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.25387 (* 1 = 9.25387 loss)
I0523 08:26:49.840059 34682 sgd_solver.cpp:112] Iteration 78480, lr = 0.01
I0523 08:26:54.893573 34682 solver.cpp:239] Iteration 78490 (1.70466 iter/s, 5.86627s/10 iters), loss = 7.93442
I0523 08:26:54.893762 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93442 (* 1 = 7.93442 loss)
I0523 08:26:55.777710 34682 sgd_solver.cpp:112] Iteration 78490, lr = 0.01
I0523 08:27:00.472597 34682 solver.cpp:239] Iteration 78500 (1.79256 iter/s, 5.57862s/10 iters), loss = 7.92682
I0523 08:27:00.472635 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.92682 (* 1 = 7.92682 loss)
I0523 08:27:00.556445 34682 sgd_solver.cpp:112] Iteration 78500, lr = 0.01
I0523 08:27:05.249613 34682 solver.cpp:239] Iteration 78510 (2.09346 iter/s, 4.77677s/10 iters), loss = 7.44971
I0523 08:27:05.249677 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44971 (* 1 = 7.44971 loss)
I0523 08:27:05.321115 34682 sgd_solver.cpp:112] Iteration 78510, lr = 0.01
I0523 08:27:08.562903 34682 solver.cpp:239] Iteration 78520 (3.01833 iter/s, 3.3131s/10 iters), loss = 8.45351
I0523 08:27:08.562950 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45351 (* 1 = 8.45351 loss)
I0523 08:27:08.632202 34682 sgd_solver.cpp:112] Iteration 78520, lr = 0.01
I0523 08:27:13.663385 34682 solver.cpp:239] Iteration 78530 (1.9607 iter/s, 5.10022s/10 iters), loss = 7.09123
I0523 08:27:13.663450 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.09123 (* 1 = 7.09123 loss)
I0523 08:27:14.515923 34682 sgd_solver.cpp:112] Iteration 78530, lr = 0.01
I0523 08:27:17.172948 34682 solver.cpp:239] Iteration 78540 (2.84952 iter/s, 3.50936s/10 iters), loss = 7.23182
I0523 08:27:17.172999 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.23182 (* 1 = 7.23182 loss)
I0523 08:27:17.231993 34682 sgd_solver.cpp:112] Iteration 78540, lr = 0.01
I0523 08:27:20.822962 34682 solver.cpp:239] Iteration 78550 (2.73987 iter/s, 3.64981s/10 iters), loss = 8.26983
I0523 08:27:20.823046 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26983 (* 1 = 8.26983 loss)
I0523 08:27:20.883807 34682 sgd_solver.cpp:112] Iteration 78550, lr = 0.01
I0523 08:27:25.850255 34682 solver.cpp:239] Iteration 78560 (1.98925 iter/s, 5.02701s/10 iters), loss = 7.88402
I0523 08:27:25.850512 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88402 (* 1 = 7.88402 loss)
I0523 08:27:26.682391 34682 sgd_solver.cpp:112] Iteration 78560, lr = 0.01
I0523 08:27:32.163982 34682 solver.cpp:239] Iteration 78570 (1.58397 iter/s, 6.31324s/10 iters), loss = 8.06009
I0523 08:27:32.164031 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06009 (* 1 = 8.06009 loss)
I0523 08:27:32.220619 34682 sgd_solver.cpp:112] Iteration 78570, lr = 0.01
I0523 08:27:36.156805 34682 solver.cpp:239] Iteration 78580 (2.50463 iter/s, 3.99261s/10 iters), loss = 8.70337
I0523 08:27:36.156857 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.70337 (* 1 = 8.70337 loss)
I0523 08:27:36.234939 34682 sgd_solver.cpp:112] Iteration 78580, lr = 0.01
I0523 08:27:42.342291 34682 solver.cpp:239] Iteration 78590 (1.61782 iter/s, 6.18117s/10 iters), loss = 8.90697
I0523 08:27:42.342342 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.90697 (* 1 = 8.90697 loss)
I0523 08:27:42.419692 34682 sgd_solver.cpp:112] Iteration 78590, lr = 0.01
I0523 08:27:47.998330 34682 solver.cpp:239] Iteration 78600 (1.76811 iter/s, 5.65576s/10 iters), loss = 7.24939
I0523 08:27:47.998374 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.24939 (* 1 = 7.24939 loss)
I0523 08:27:48.062757 34682 sgd_solver.cpp:112] Iteration 78600, lr = 0.01
I0523 08:27:51.583693 34682 solver.cpp:239] Iteration 78610 (2.78927 iter/s, 3.58516s/10 iters), loss = 7.46492
I0523 08:27:51.583737 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.46492 (* 1 = 7.46492 loss)
I0523 08:27:51.660291 34682 sgd_solver.cpp:112] Iteration 78610, lr = 0.01
I0523 08:27:56.404484 34682 solver.cpp:239] Iteration 78620 (2.07446 iter/s, 4.82054s/10 iters), loss = 8.988
I0523 08:27:56.404582 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.988 (* 1 = 8.988 loss)
I0523 08:27:56.470935 34682 sgd_solver.cpp:112] Iteration 78620, lr = 0.01
I0523 08:28:00.936074 34682 solver.cpp:239] Iteration 78630 (2.20687 iter/s, 4.53131s/10 iters), loss = 7.46495
I0523 08:28:00.936125 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.46495 (* 1 = 7.46495 loss)
I0523 08:28:01.013751 34682 sgd_solver.cpp:112] Iteration 78630, lr = 0.01
I0523 08:28:05.098429 34682 solver.cpp:239] Iteration 78640 (2.40262 iter/s, 4.16213s/10 iters), loss = 7.58763
I0523 08:28:05.098490 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58763 (* 1 = 7.58763 loss)
I0523 08:28:05.467402 34682 sgd_solver.cpp:112] Iteration 78640, lr = 0.01
I0523 08:28:08.068460 34682 solver.cpp:239] Iteration 78650 (3.36717 iter/s, 2.96985s/10 iters), loss = 7.82336
I0523 08:28:08.068513 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82336 (* 1 = 7.82336 loss)
I0523 08:28:08.896783 34682 sgd_solver.cpp:112] Iteration 78650, lr = 0.01
I0523 08:28:11.523562 34682 solver.cpp:239] Iteration 78660 (2.89445 iter/s, 3.45489s/10 iters), loss = 7.84171
I0523 08:28:11.523620 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84171 (* 1 = 7.84171 loss)
I0523 08:28:11.590121 34682 sgd_solver.cpp:112] Iteration 78660, lr = 0.01
I0523 08:28:16.664810 34682 solver.cpp:239] Iteration 78670 (1.94516 iter/s, 5.14098s/10 iters), loss = 7.28787
I0523 08:28:16.664866 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.28787 (* 1 = 7.28787 loss)
I0523 08:28:17.511869 34682 sgd_solver.cpp:112] Iteration 78670, lr = 0.01
I0523 08:28:23.031954 34682 solver.cpp:239] Iteration 78680 (1.57064 iter/s, 6.36681s/10 iters), loss = 7.72849
I0523 08:28:23.032033 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72849 (* 1 = 7.72849 loss)
I0523 08:28:23.101078 34682 sgd_solver.cpp:112] Iteration 78680, lr = 0.01
I0523 08:28:28.485958 34682 solver.cpp:239] Iteration 78690 (1.83362 iter/s, 5.4537s/10 iters), loss = 7.80759
I0523 08:28:28.486178 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80759 (* 1 = 7.80759 loss)
I0523 08:28:28.559851 34682 sgd_solver.cpp:112] Iteration 78690, lr = 0.01
I0523 08:28:33.174921 34682 solver.cpp:239] Iteration 78700 (2.13286 iter/s, 4.68855s/10 iters), loss = 7.46334
I0523 08:28:33.174984 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.46334 (* 1 = 7.46334 loss)
I0523 08:28:34.034528 34682 sgd_solver.cpp:112] Iteration 78700, lr = 0.01
I0523 08:28:38.057201 34682 solver.cpp:239] Iteration 78710 (2.04834 iter/s, 4.88201s/10 iters), loss = 8.75569
I0523 08:28:38.057258 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.75569 (* 1 = 8.75569 loss)
I0523 08:28:38.126433 34682 sgd_solver.cpp:112] Iteration 78710, lr = 0.01
I0523 08:28:43.648757 34682 solver.cpp:239] Iteration 78720 (1.7885 iter/s, 5.59127s/10 iters), loss = 8.74079
I0523 08:28:43.648807 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.74079 (* 1 = 8.74079 loss)
I0523 08:28:44.481139 34682 sgd_solver.cpp:112] Iteration 78720, lr = 0.01
I0523 08:28:48.674589 34682 solver.cpp:239] Iteration 78730 (1.98982 iter/s, 5.02558s/10 iters), loss = 7.66412
I0523 08:28:48.674641 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66412 (* 1 = 7.66412 loss)
I0523 08:28:49.097784 34682 sgd_solver.cpp:112] Iteration 78730, lr = 0.01
I0523 08:28:53.819464 34682 solver.cpp:239] Iteration 78740 (1.9438 iter/s, 5.14457s/10 iters), loss = 7.27489
I0523 08:28:53.819545 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.27489 (* 1 = 7.27489 loss)
I0523 08:28:54.250025 34682 sgd_solver.cpp:112] Iteration 78740, lr = 0.01
I0523 08:29:00.844502 34682 solver.cpp:239] Iteration 78750 (1.42355 iter/s, 7.02468s/10 iters), loss = 7.25349
I0523 08:29:00.844599 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.25349 (* 1 = 7.25349 loss)
I0523 08:29:00.915212 34682 sgd_solver.cpp:112] Iteration 78750, lr = 0.01
I0523 08:29:04.196817 34682 solver.cpp:239] Iteration 78760 (2.98323 iter/s, 3.35207s/10 iters), loss = 7.91075
I0523 08:29:04.196864 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91075 (* 1 = 7.91075 loss)
I0523 08:29:05.050879 34682 sgd_solver.cpp:112] Iteration 78760, lr = 0.01
I0523 08:29:10.800654 34682 solver.cpp:239] Iteration 78770 (1.51434 iter/s, 6.60353s/10 iters), loss = 7.66618
I0523 08:29:10.800698 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66618 (* 1 = 7.66618 loss)
I0523 08:29:10.869413 34682 sgd_solver.cpp:112] Iteration 78770, lr = 0.01
I0523 08:29:16.605237 34682 solver.cpp:239] Iteration 78780 (1.72286 iter/s, 5.80431s/10 iters), loss = 7.4804
I0523 08:29:16.605285 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4804 (* 1 = 7.4804 loss)
I0523 08:29:17.432596 34682 sgd_solver.cpp:112] Iteration 78780, lr = 0.01
I0523 08:29:20.204977 34682 solver.cpp:239] Iteration 78790 (2.77814 iter/s, 3.59954s/10 iters), loss = 7.51042
I0523 08:29:20.205031 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51042 (* 1 = 7.51042 loss)
I0523 08:29:21.034843 34682 sgd_solver.cpp:112] Iteration 78790, lr = 0.01
I0523 08:29:25.738734 34682 solver.cpp:239] Iteration 78800 (1.80719 iter/s, 5.53346s/10 iters), loss = 7.79249
I0523 08:29:25.738781 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79249 (* 1 = 7.79249 loss)
I0523 08:29:25.808810 34682 sgd_solver.cpp:112] Iteration 78800, lr = 0.01
I0523 08:29:32.531440 34682 solver.cpp:239] Iteration 78810 (1.47224 iter/s, 6.79238s/10 iters), loss = 7.87642
I0523 08:29:32.531625 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87642 (* 1 = 7.87642 loss)
I0523 08:29:33.370762 34682 sgd_solver.cpp:112] Iteration 78810, lr = 0.01
I0523 08:29:37.652038 34682 solver.cpp:239] Iteration 78820 (1.95305 iter/s, 5.1202s/10 iters), loss = 7.57764
I0523 08:29:37.652086 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.57764 (* 1 = 7.57764 loss)
I0523 08:29:38.432436 34682 sgd_solver.cpp:112] Iteration 78820, lr = 0.01
I0523 08:29:43.963074 34682 solver.cpp:239] Iteration 78830 (1.5846 iter/s, 6.31072s/10 iters), loss = 8.13268
I0523 08:29:43.963166 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13268 (* 1 = 8.13268 loss)
I0523 08:29:44.807916 34682 sgd_solver.cpp:112] Iteration 78830, lr = 0.01
I0523 08:29:50.449946 34682 solver.cpp:239] Iteration 78840 (1.54166 iter/s, 6.48652s/10 iters), loss = 8.31485
I0523 08:29:50.450008 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31485 (* 1 = 8.31485 loss)
I0523 08:29:51.052346 34682 sgd_solver.cpp:112] Iteration 78840, lr = 0.01
I0523 08:29:56.910825 34682 solver.cpp:239] Iteration 78850 (1.54786 iter/s, 6.46055s/10 iters), loss = 7.75289
I0523 08:29:56.910881 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75289 (* 1 = 7.75289 loss)
I0523 08:29:57.743841 34682 sgd_solver.cpp:112] Iteration 78850, lr = 0.01
I0523 08:30:04.405591 34682 solver.cpp:239] Iteration 78860 (1.33433 iter/s, 7.49441s/10 iters), loss = 7.79017
I0523 08:30:04.405763 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79017 (* 1 = 7.79017 loss)
I0523 08:30:04.465412 34682 sgd_solver.cpp:112] Iteration 78860, lr = 0.01
I0523 08:30:09.319996 34682 solver.cpp:239] Iteration 78870 (2.03499 iter/s, 4.91403s/10 iters), loss = 7.84361
I0523 08:30:09.320056 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84361 (* 1 = 7.84361 loss)
I0523 08:30:09.951094 34682 sgd_solver.cpp:112] Iteration 78870, lr = 0.01
I0523 08:30:14.451169 34682 solver.cpp:239] Iteration 78880 (1.94898 iter/s, 5.1309s/10 iters), loss = 8.27749
I0523 08:30:14.451223 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27749 (* 1 = 8.27749 loss)
I0523 08:30:14.520048 34682 sgd_solver.cpp:112] Iteration 78880, lr = 0.01
I0523 08:30:20.420258 34682 solver.cpp:239] Iteration 78890 (1.67538 iter/s, 5.9688s/10 iters), loss = 7.44101
I0523 08:30:20.420301 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44101 (* 1 = 7.44101 loss)
I0523 08:30:21.199286 34682 sgd_solver.cpp:112] Iteration 78890, lr = 0.01
I0523 08:30:27.502634 34682 solver.cpp:239] Iteration 78900 (1.41202 iter/s, 7.08205s/10 iters), loss = 8.2254
I0523 08:30:27.502681 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.2254 (* 1 = 8.2254 loss)
I0523 08:30:27.569279 34682 sgd_solver.cpp:112] Iteration 78900, lr = 0.01
I0523 08:30:35.492658 34682 solver.cpp:239] Iteration 78910 (1.25162 iter/s, 7.98965s/10 iters), loss = 7.30822
I0523 08:30:35.492774 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.30822 (* 1 = 7.30822 loss)
I0523 08:30:35.565299 34682 sgd_solver.cpp:112] Iteration 78910, lr = 0.01
I0523 08:30:39.811350 34682 solver.cpp:239] Iteration 78920 (2.31567 iter/s, 4.3184s/10 iters), loss = 7.30966
I0523 08:30:39.811403 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.30966 (* 1 = 7.30966 loss)
I0523 08:30:40.620648 34682 sgd_solver.cpp:112] Iteration 78920, lr = 0.01
I0523 08:30:44.600492 34682 solver.cpp:239] Iteration 78930 (2.08817 iter/s, 4.78889s/10 iters), loss = 7.19794
I0523 08:30:44.600541 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.19794 (* 1 = 7.19794 loss)
I0523 08:30:44.669975 34682 sgd_solver.cpp:112] Iteration 78930, lr = 0.01
I0523 08:30:47.939329 34682 solver.cpp:239] Iteration 78940 (2.99522 iter/s, 3.33865s/10 iters), loss = 8.6236
I0523 08:30:47.939371 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.6236 (* 1 = 8.6236 loss)
I0523 08:30:48.517429 34682 sgd_solver.cpp:112] Iteration 78940, lr = 0.01
I0523 08:30:52.999888 34682 solver.cpp:239] Iteration 78950 (1.97617 iter/s, 5.06029s/10 iters), loss = 7.47412
I0523 08:30:52.999946 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.47412 (* 1 = 7.47412 loss)
I0523 08:30:53.702611 34682 sgd_solver.cpp:112] Iteration 78950, lr = 0.01
I0523 08:30:57.916576 34682 solver.cpp:239] Iteration 78960 (2.034 iter/s, 4.91641s/10 iters), loss = 8.52033
I0523 08:30:57.916654 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52033 (* 1 = 8.52033 loss)
I0523 08:30:57.979286 34682 sgd_solver.cpp:112] Iteration 78960, lr = 0.01
I0523 08:31:02.255869 34682 solver.cpp:239] Iteration 78970 (2.30466 iter/s, 4.33903s/10 iters), loss = 8.56291
I0523 08:31:02.255928 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56291 (* 1 = 8.56291 loss)
I0523 08:31:02.488144 34682 sgd_solver.cpp:112] Iteration 78970, lr = 0.01
I0523 08:31:06.708906 34682 solver.cpp:239] Iteration 78980 (2.24578 iter/s, 4.4528s/10 iters), loss = 8.04417
I0523 08:31:06.709100 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04417 (* 1 = 8.04417 loss)
I0523 08:31:07.501412 34682 sgd_solver.cpp:112] Iteration 78980, lr = 0.01
I0523 08:31:11.601617 34682 solver.cpp:239] Iteration 78990 (2.04402 iter/s, 4.89232s/10 iters), loss = 8.48215
I0523 08:31:11.601660 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.48215 (* 1 = 8.48215 loss)
I0523 08:31:11.657586 34682 sgd_solver.cpp:112] Iteration 78990, lr = 0.01
I0523 08:31:18.213207 34682 solver.cpp:239] Iteration 79000 (1.51257 iter/s, 6.61128s/10 iters), loss = 7.10504
I0523 08:31:18.213268 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.10504 (* 1 = 7.10504 loss)
I0523 08:31:18.284929 34682 sgd_solver.cpp:112] Iteration 79000, lr = 0.01
I0523 08:31:22.287426 34682 solver.cpp:239] Iteration 79010 (2.4546 iter/s, 4.07399s/10 iters), loss = 8.40874
I0523 08:31:22.287475 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40874 (* 1 = 8.40874 loss)
I0523 08:31:22.361191 34682 sgd_solver.cpp:112] Iteration 79010, lr = 0.01
I0523 08:31:26.589036 34682 solver.cpp:239] Iteration 79020 (2.32483 iter/s, 4.30138s/10 iters), loss = 8.68705
I0523 08:31:26.589095 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68705 (* 1 = 8.68705 loss)
I0523 08:31:26.669919 34682 sgd_solver.cpp:112] Iteration 79020, lr = 0.01
I0523 08:31:31.492907 34682 solver.cpp:239] Iteration 79030 (2.03931 iter/s, 4.90361s/10 iters), loss = 7.42363
I0523 08:31:31.492957 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42363 (* 1 = 7.42363 loss)
I0523 08:31:31.556032 34682 sgd_solver.cpp:112] Iteration 79030, lr = 0.01
I0523 08:31:37.378278 34682 solver.cpp:239] Iteration 79040 (1.69921 iter/s, 5.88508s/10 iters), loss = 8.04616
I0523 08:31:37.378451 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04616 (* 1 = 8.04616 loss)
I0523 08:31:38.207988 34682 sgd_solver.cpp:112] Iteration 79040, lr = 0.01
I0523 08:31:43.210256 34682 solver.cpp:239] Iteration 79050 (1.7148 iter/s, 5.83157s/10 iters), loss = 7.63792
I0523 08:31:43.210309 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63792 (* 1 = 7.63792 loss)
I0523 08:31:44.051868 34682 sgd_solver.cpp:112] Iteration 79050, lr = 0.01
I0523 08:31:47.927409 34682 solver.cpp:239] Iteration 79060 (2.12004 iter/s, 4.7169s/10 iters), loss = 7.62113
I0523 08:31:47.927458 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62113 (* 1 = 7.62113 loss)
I0523 08:31:48.762143 34682 sgd_solver.cpp:112] Iteration 79060, lr = 0.01
I0523 08:31:53.555407 34682 solver.cpp:239] Iteration 79070 (1.77692 iter/s, 5.62772s/10 iters), loss = 7.50121
I0523 08:31:53.555449 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50121 (* 1 = 7.50121 loss)
I0523 08:31:53.626649 34682 sgd_solver.cpp:112] Iteration 79070, lr = 0.01
I0523 08:31:58.357327 34682 solver.cpp:239] Iteration 79080 (2.08261 iter/s, 4.80166s/10 iters), loss = 7.90585
I0523 08:31:58.357415 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90585 (* 1 = 7.90585 loss)
I0523 08:31:59.179500 34682 sgd_solver.cpp:112] Iteration 79080, lr = 0.01
I0523 08:32:01.783387 34682 solver.cpp:239] Iteration 79090 (2.91901 iter/s, 3.42582s/10 iters), loss = 8.09636
I0523 08:32:01.783463 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09636 (* 1 = 8.09636 loss)
I0523 08:32:02.598486 34682 sgd_solver.cpp:112] Iteration 79090, lr = 0.01
I0523 08:32:08.192643 34682 solver.cpp:239] Iteration 79100 (1.56032 iter/s, 6.40892s/10 iters), loss = 8.68561
I0523 08:32:08.192914 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.68561 (* 1 = 8.68561 loss)
I0523 08:32:09.039686 34682 sgd_solver.cpp:112] Iteration 79100, lr = 0.01
I0523 08:32:13.927616 34682 solver.cpp:239] Iteration 79110 (1.74383 iter/s, 5.7345s/10 iters), loss = 7.95309
I0523 08:32:13.927661 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95309 (* 1 = 7.95309 loss)
I0523 08:32:14.036581 34682 sgd_solver.cpp:112] Iteration 79110, lr = 0.01
I0523 08:32:17.315186 34682 solver.cpp:239] Iteration 79120 (2.95213 iter/s, 3.38738s/10 iters), loss = 8.42846
I0523 08:32:17.315232 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.42846 (* 1 = 8.42846 loss)
I0523 08:32:17.374245 34682 sgd_solver.cpp:112] Iteration 79120, lr = 0.01
I0523 08:32:21.445745 34682 solver.cpp:239] Iteration 79130 (2.42111 iter/s, 4.13033s/10 iters), loss = 8.3836
I0523 08:32:21.445796 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3836 (* 1 = 8.3836 loss)
I0523 08:32:22.295333 34682 sgd_solver.cpp:112] Iteration 79130, lr = 0.01
I0523 08:32:28.592934 34682 solver.cpp:239] Iteration 79140 (1.39922 iter/s, 7.14685s/10 iters), loss = 7.48085
I0523 08:32:28.592979 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.48085 (* 1 = 7.48085 loss)
I0523 08:32:29.431916 34682 sgd_solver.cpp:112] Iteration 79140, lr = 0.01
I0523 08:32:33.354506 34682 solver.cpp:239] Iteration 79150 (2.10025 iter/s, 4.76133s/10 iters), loss = 7.87931
I0523 08:32:33.354552 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87931 (* 1 = 7.87931 loss)
I0523 08:32:33.431725 34682 sgd_solver.cpp:112] Iteration 79150, lr = 0.01
I0523 08:32:37.966756 34682 solver.cpp:239] Iteration 79160 (2.16825 iter/s, 4.61202s/10 iters), loss = 6.49189
I0523 08:32:37.966805 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.49189 (* 1 = 6.49189 loss)
I0523 08:32:38.744477 34682 sgd_solver.cpp:112] Iteration 79160, lr = 0.01
I0523 08:32:42.771631 34682 solver.cpp:239] Iteration 79170 (2.08133 iter/s, 4.80463s/10 iters), loss = 7.33183
I0523 08:32:42.771670 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.33183 (* 1 = 7.33183 loss)
I0523 08:32:42.838251 34682 sgd_solver.cpp:112] Iteration 79170, lr = 0.01
I0523 08:32:47.641738 34682 solver.cpp:239] Iteration 79180 (2.05345 iter/s, 4.86985s/10 iters), loss = 7.65054
I0523 08:32:47.641803 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.65054 (* 1 = 7.65054 loss)
I0523 08:32:47.703528 34682 sgd_solver.cpp:112] Iteration 79180, lr = 0.01
I0523 08:32:54.379541 34682 solver.cpp:239] Iteration 79190 (1.48424 iter/s, 6.73746s/10 iters), loss = 7.07853
I0523 08:32:54.379621 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.07853 (* 1 = 7.07853 loss)
I0523 08:32:55.253958 34682 sgd_solver.cpp:112] Iteration 79190, lr = 0.01
I0523 08:33:00.102725 34682 solver.cpp:239] Iteration 79200 (1.74737 iter/s, 5.72288s/10 iters), loss = 9.15487
I0523 08:33:00.102780 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.15487 (* 1 = 9.15487 loss)
I0523 08:33:00.163182 34682 sgd_solver.cpp:112] Iteration 79200, lr = 0.01
I0523 08:33:05.154886 34682 solver.cpp:239] Iteration 79210 (1.97945 iter/s, 5.0519s/10 iters), loss = 7.82765
I0523 08:33:05.154944 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82765 (* 1 = 7.82765 loss)
I0523 08:33:05.923403 34682 sgd_solver.cpp:112] Iteration 79210, lr = 0.01
I0523 08:33:08.470232 34682 solver.cpp:239] Iteration 79220 (3.01646 iter/s, 3.31515s/10 iters), loss = 8.0359
I0523 08:33:08.470273 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0359 (* 1 = 8.0359 loss)
I0523 08:33:08.551851 34682 sgd_solver.cpp:112] Iteration 79220, lr = 0.01
I0523 08:33:13.390472 34682 solver.cpp:239] Iteration 79230 (2.03252 iter/s, 4.91999s/10 iters), loss = 8.14176
I0523 08:33:13.390691 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14176 (* 1 = 8.14176 loss)
I0523 08:33:13.462030 34682 sgd_solver.cpp:112] Iteration 79230, lr = 0.01
I0523 08:33:19.898654 34682 solver.cpp:239] Iteration 79240 (1.53664 iter/s, 6.50771s/10 iters), loss = 8.07364
I0523 08:33:19.898725 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07364 (* 1 = 8.07364 loss)
I0523 08:33:20.636482 34682 sgd_solver.cpp:112] Iteration 79240, lr = 0.01
I0523 08:33:25.231235 34682 solver.cpp:239] Iteration 79250 (1.87536 iter/s, 5.3323s/10 iters), loss = 8.04941
I0523 08:33:25.231313 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04941 (* 1 = 8.04941 loss)
I0523 08:33:25.868947 34682 sgd_solver.cpp:112] Iteration 79250, lr = 0.01
I0523 08:33:30.086588 34682 solver.cpp:239] Iteration 79260 (2.0597 iter/s, 4.85508s/10 iters), loss = 6.80554
I0523 08:33:30.086668 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.80554 (* 1 = 6.80554 loss)
I0523 08:33:30.147086 34682 sgd_solver.cpp:112] Iteration 79260, lr = 0.01
I0523 08:33:35.761881 34682 solver.cpp:239] Iteration 79270 (1.76212 iter/s, 5.67499s/10 iters), loss = 7.88845
I0523 08:33:35.761924 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.88845 (* 1 = 7.88845 loss)
I0523 08:33:35.831465 34682 sgd_solver.cpp:112] Iteration 79270, lr = 0.01
I0523 08:33:42.530305 34682 solver.cpp:239] Iteration 79280 (1.47752 iter/s, 6.76811s/10 iters), loss = 7.58275
I0523 08:33:42.530349 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58275 (* 1 = 7.58275 loss)
I0523 08:33:42.590860 34682 sgd_solver.cpp:112] Iteration 79280, lr = 0.01
I0523 08:33:46.845113 34682 solver.cpp:239] Iteration 79290 (2.31772 iter/s, 4.31459s/10 iters), loss = 8.10488
I0523 08:33:46.845335 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10488 (* 1 = 8.10488 loss)
I0523 08:33:47.642459 34682 sgd_solver.cpp:112] Iteration 79290, lr = 0.01
I0523 08:33:53.097440 34682 solver.cpp:239] Iteration 79300 (1.59952 iter/s, 6.25188s/10 iters), loss = 7.54097
I0523 08:33:53.097478 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54097 (* 1 = 7.54097 loss)
I0523 08:33:53.170867 34682 sgd_solver.cpp:112] Iteration 79300, lr = 0.01
I0523 08:33:57.873890 34682 solver.cpp:239] Iteration 79310 (2.09371 iter/s, 4.77622s/10 iters), loss = 7.01255
I0523 08:33:57.873935 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.01255 (* 1 = 7.01255 loss)
I0523 08:33:58.574322 34682 sgd_solver.cpp:112] Iteration 79310, lr = 0.01
I0523 08:34:01.271244 34682 solver.cpp:239] Iteration 79320 (2.94363 iter/s, 3.39717s/10 iters), loss = 6.44044
I0523 08:34:01.271291 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.44044 (* 1 = 6.44044 loss)
I0523 08:34:01.345387 34682 sgd_solver.cpp:112] Iteration 79320, lr = 0.01
I0523 08:34:03.986835 34682 solver.cpp:239] Iteration 79330 (3.68266 iter/s, 2.71543s/10 iters), loss = 7.12535
I0523 08:34:03.986876 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.12535 (* 1 = 7.12535 loss)
I0523 08:34:04.052028 34682 sgd_solver.cpp:112] Iteration 79330, lr = 0.01
I0523 08:34:08.022800 34682 solver.cpp:239] Iteration 79340 (2.47785 iter/s, 4.03576s/10 iters), loss = 7.22857
I0523 08:34:08.022850 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.22857 (* 1 = 7.22857 loss)
I0523 08:34:08.095351 34682 sgd_solver.cpp:112] Iteration 79340, lr = 0.01
I0523 08:34:12.339900 34682 solver.cpp:239] Iteration 79350 (2.3165 iter/s, 4.31686s/10 iters), loss = 7.75344
I0523 08:34:12.339982 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75344 (* 1 = 7.75344 loss)
I0523 08:34:12.403570 34682 sgd_solver.cpp:112] Iteration 79350, lr = 0.01
I0523 08:34:16.873682 34682 solver.cpp:239] Iteration 79360 (2.20579 iter/s, 4.53352s/10 iters), loss = 7.63732
I0523 08:34:16.873956 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63732 (* 1 = 7.63732 loss)
I0523 08:34:16.943670 34682 sgd_solver.cpp:112] Iteration 79360, lr = 0.01
I0523 08:34:20.383785 34682 solver.cpp:239] Iteration 79370 (2.84923 iter/s, 3.50972s/10 iters), loss = 8.01825
I0523 08:34:20.383828 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01825 (* 1 = 8.01825 loss)
I0523 08:34:20.447348 34682 sgd_solver.cpp:112] Iteration 79370, lr = 0.01
I0523 08:34:23.048943 34682 solver.cpp:239] Iteration 79380 (3.75235 iter/s, 2.665s/10 iters), loss = 7.56256
I0523 08:34:23.048985 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56256 (* 1 = 7.56256 loss)
I0523 08:34:23.115231 34682 sgd_solver.cpp:112] Iteration 79380, lr = 0.01
I0523 08:34:27.581228 34682 solver.cpp:239] Iteration 79390 (2.20651 iter/s, 4.53204s/10 iters), loss = 7.70032
I0523 08:34:27.581297 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.70032 (* 1 = 7.70032 loss)
I0523 08:34:28.434954 34682 sgd_solver.cpp:112] Iteration 79390, lr = 0.01
I0523 08:34:34.929543 34682 solver.cpp:239] Iteration 79400 (1.36092 iter/s, 7.34796s/10 iters), loss = 6.2772
I0523 08:34:34.929584 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.2772 (* 1 = 6.2772 loss)
I0523 08:34:35.008893 34682 sgd_solver.cpp:112] Iteration 79400, lr = 0.01
I0523 08:34:39.916653 34682 solver.cpp:239] Iteration 79410 (2.00527 iter/s, 4.98687s/10 iters), loss = 8.51919
I0523 08:34:39.916716 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51919 (* 1 = 8.51919 loss)
I0523 08:34:40.696264 34682 sgd_solver.cpp:112] Iteration 79410, lr = 0.01
I0523 08:34:44.000949 34682 solver.cpp:239] Iteration 79420 (2.44855 iter/s, 4.08406s/10 iters), loss = 7.90114
I0523 08:34:44.001010 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90114 (* 1 = 7.90114 loss)
I0523 08:34:44.072700 34682 sgd_solver.cpp:112] Iteration 79420, lr = 0.01
I0523 08:34:48.817140 34682 solver.cpp:239] Iteration 79430 (2.07644 iter/s, 4.81594s/10 iters), loss = 8.44421
I0523 08:34:48.817250 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.44421 (* 1 = 8.44421 loss)
I0523 08:34:48.880935 34682 sgd_solver.cpp:112] Iteration 79430, lr = 0.01
I0523 08:34:53.182494 34682 solver.cpp:239] Iteration 79440 (2.29287 iter/s, 4.36134s/10 iters), loss = 8.29481
I0523 08:34:53.182538 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29481 (* 1 = 8.29481 loss)
I0523 08:34:53.244971 34682 sgd_solver.cpp:112] Iteration 79440, lr = 0.01
I0523 08:34:59.725471 34682 solver.cpp:239] Iteration 79450 (1.52843 iter/s, 6.54267s/10 iters), loss = 7.5642
I0523 08:34:59.725519 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5642 (* 1 = 7.5642 loss)
I0523 08:35:00.551141 34682 sgd_solver.cpp:112] Iteration 79450, lr = 0.01
I0523 08:35:07.054976 34682 solver.cpp:239] Iteration 79460 (1.36441 iter/s, 7.32916s/10 iters), loss = 7.40765
I0523 08:35:07.055023 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.40765 (* 1 = 7.40765 loss)
I0523 08:35:07.115425 34682 sgd_solver.cpp:112] Iteration 79460, lr = 0.01
I0523 08:35:11.389449 34682 solver.cpp:239] Iteration 79470 (2.3072 iter/s, 4.33425s/10 iters), loss = 8.09575
I0523 08:35:11.389489 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09575 (* 1 = 8.09575 loss)
I0523 08:35:11.446842 34682 sgd_solver.cpp:112] Iteration 79470, lr = 0.01
I0523 08:35:16.527354 34682 solver.cpp:239] Iteration 79480 (1.94642 iter/s, 5.13765s/10 iters), loss = 7.37815
I0523 08:35:16.527395 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.37815 (* 1 = 7.37815 loss)
I0523 08:35:16.606736 34682 sgd_solver.cpp:112] Iteration 79480, lr = 0.01
I0523 08:35:20.365731 34682 solver.cpp:239] Iteration 79490 (2.6054 iter/s, 3.83818s/10 iters), loss = 7.80967
I0523 08:35:20.365990 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80967 (* 1 = 7.80967 loss)
I0523 08:35:21.232751 34682 sgd_solver.cpp:112] Iteration 79490, lr = 0.01
I0523 08:35:25.850497 34682 solver.cpp:239] Iteration 79500 (1.82339 iter/s, 5.4843s/10 iters), loss = 7.91176
I0523 08:35:25.850543 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.91176 (* 1 = 7.91176 loss)
I0523 08:35:25.921650 34682 sgd_solver.cpp:112] Iteration 79500, lr = 0.01
I0523 08:35:29.736399 34682 solver.cpp:239] Iteration 79510 (2.57354 iter/s, 3.8857s/10 iters), loss = 7.44474
I0523 08:35:29.736443 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44474 (* 1 = 7.44474 loss)
I0523 08:35:29.797940 34682 sgd_solver.cpp:112] Iteration 79510, lr = 0.01
I0523 08:35:33.700230 34682 solver.cpp:239] Iteration 79520 (2.52294 iter/s, 3.96362s/10 iters), loss = 7.62273
I0523 08:35:33.700274 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62273 (* 1 = 7.62273 loss)
I0523 08:35:33.783262 34682 sgd_solver.cpp:112] Iteration 79520, lr = 0.01
I0523 08:35:39.123577 34682 solver.cpp:239] Iteration 79530 (1.84397 iter/s, 5.42309s/10 iters), loss = 8.78934
I0523 08:35:39.123622 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.78934 (* 1 = 8.78934 loss)
I0523 08:35:39.202152 34682 sgd_solver.cpp:112] Iteration 79530, lr = 0.01
I0523 08:35:45.493489 34682 solver.cpp:239] Iteration 79540 (1.56995 iter/s, 6.36961s/10 iters), loss = 7.25909
I0523 08:35:45.493530 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.25909 (* 1 = 7.25909 loss)
I0523 08:35:45.564815 34682 sgd_solver.cpp:112] Iteration 79540, lr = 0.01
I0523 08:35:50.312644 34682 solver.cpp:239] Iteration 79550 (2.07515 iter/s, 4.81892s/10 iters), loss = 6.83815
I0523 08:35:50.312690 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.83815 (* 1 = 6.83815 loss)
I0523 08:35:50.376894 34682 sgd_solver.cpp:112] Iteration 79550, lr = 0.01
I0523 08:35:56.001642 34682 solver.cpp:239] Iteration 79560 (1.75786 iter/s, 5.68872s/10 iters), loss = 7.725
I0523 08:35:56.001689 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.725 (* 1 = 7.725 loss)
I0523 08:35:56.853457 34682 sgd_solver.cpp:112] Iteration 79560, lr = 0.01
I0523 08:36:01.616660 34682 solver.cpp:239] Iteration 79570 (1.78103 iter/s, 5.61473s/10 iters), loss = 7.63409
I0523 08:36:01.616719 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.63409 (* 1 = 7.63409 loss)
I0523 08:36:01.687985 34682 sgd_solver.cpp:112] Iteration 79570, lr = 0.01
I0523 08:36:06.671761 34682 solver.cpp:239] Iteration 79580 (1.97831 iter/s, 5.05483s/10 iters), loss = 8.0749
I0523 08:36:06.671847 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0749 (* 1 = 8.0749 loss)
I0523 08:36:06.733773 34682 sgd_solver.cpp:112] Iteration 79580, lr = 0.01
I0523 08:36:12.270809 34682 solver.cpp:239] Iteration 79590 (1.78612 iter/s, 5.59873s/10 iters), loss = 7.87634
I0523 08:36:12.270877 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87634 (* 1 = 7.87634 loss)
I0523 08:36:12.917188 34682 sgd_solver.cpp:112] Iteration 79590, lr = 0.01
I0523 08:36:18.650015 34682 solver.cpp:239] Iteration 79600 (1.56767 iter/s, 6.37888s/10 iters), loss = 8.22182
I0523 08:36:18.650079 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22182 (* 1 = 8.22182 loss)
I0523 08:36:19.477649 34682 sgd_solver.cpp:112] Iteration 79600, lr = 0.01
I0523 08:36:24.436185 34682 solver.cpp:239] Iteration 79610 (1.72834 iter/s, 5.78588s/10 iters), loss = 6.84192
I0523 08:36:24.436465 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.84192 (* 1 = 6.84192 loss)
I0523 08:36:24.509977 34682 sgd_solver.cpp:112] Iteration 79610, lr = 0.01
I0523 08:36:29.877497 34682 solver.cpp:239] Iteration 79620 (1.83796 iter/s, 5.44083s/10 iters), loss = 7.50686
I0523 08:36:29.877542 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50686 (* 1 = 7.50686 loss)
I0523 08:36:29.942163 34682 sgd_solver.cpp:112] Iteration 79620, lr = 0.01
I0523 08:36:35.715272 34682 solver.cpp:239] Iteration 79630 (1.71306 iter/s, 5.83749s/10 iters), loss = 7.73556
I0523 08:36:35.715318 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.73556 (* 1 = 7.73556 loss)
I0523 08:36:35.778508 34682 sgd_solver.cpp:112] Iteration 79630, lr = 0.01
I0523 08:36:39.091408 34682 solver.cpp:239] Iteration 79640 (2.96214 iter/s, 3.37593s/10 iters), loss = 7.19797
I0523 08:36:39.091476 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.19797 (* 1 = 7.19797 loss)
I0523 08:36:39.936249 34682 sgd_solver.cpp:112] Iteration 79640, lr = 0.01
I0523 08:36:44.817385 34682 solver.cpp:239] Iteration 79650 (1.74652 iter/s, 5.72567s/10 iters), loss = 7.82789
I0523 08:36:44.817458 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82789 (* 1 = 7.82789 loss)
I0523 08:36:45.437849 34682 sgd_solver.cpp:112] Iteration 79650, lr = 0.01
I0523 08:36:48.696295 34682 solver.cpp:239] Iteration 79660 (2.5782 iter/s, 3.87867s/10 iters), loss = 8.20644
I0523 08:36:48.696355 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20644 (* 1 = 8.20644 loss)
I0523 08:36:49.357414 34682 sgd_solver.cpp:112] Iteration 79660, lr = 0.01
I0523 08:36:52.505861 34682 solver.cpp:239] Iteration 79670 (2.62512 iter/s, 3.80934s/10 iters), loss = 7.50699
I0523 08:36:52.505913 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50699 (* 1 = 7.50699 loss)
I0523 08:36:52.761713 34682 sgd_solver.cpp:112] Iteration 79670, lr = 0.01
I0523 08:36:56.529211 34682 solver.cpp:239] Iteration 79680 (2.48563 iter/s, 4.02312s/10 iters), loss = 7.36591
I0523 08:36:56.529496 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.36591 (* 1 = 7.36591 loss)
I0523 08:36:57.352851 34682 sgd_solver.cpp:112] Iteration 79680, lr = 0.01
I0523 08:37:04.192885 34682 solver.cpp:239] Iteration 79690 (1.30495 iter/s, 7.66311s/10 iters), loss = 7.38413
I0523 08:37:04.192937 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.38413 (* 1 = 7.38413 loss)
I0523 08:37:04.261101 34682 sgd_solver.cpp:112] Iteration 79690, lr = 0.01
I0523 08:37:07.562037 34682 solver.cpp:239] Iteration 79700 (2.96828 iter/s, 3.36896s/10 iters), loss = 8.06852
I0523 08:37:07.562083 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06852 (* 1 = 8.06852 loss)
I0523 08:37:07.629504 34682 sgd_solver.cpp:112] Iteration 79700, lr = 0.01
I0523 08:37:11.920560 34682 solver.cpp:239] Iteration 79710 (2.29448 iter/s, 4.35829s/10 iters), loss = 8.33988
I0523 08:37:11.920621 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33988 (* 1 = 8.33988 loss)
I0523 08:37:12.796171 34682 sgd_solver.cpp:112] Iteration 79710, lr = 0.01
I0523 08:37:16.227267 34682 solver.cpp:239] Iteration 79720 (2.32209 iter/s, 4.30646s/10 iters), loss = 7.38736
I0523 08:37:16.227322 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.38736 (* 1 = 7.38736 loss)
I0523 08:37:17.046355 34682 sgd_solver.cpp:112] Iteration 79720, lr = 0.01
I0523 08:37:20.692518 34682 solver.cpp:239] Iteration 79730 (2.23964 iter/s, 4.46501s/10 iters), loss = 7.58753
I0523 08:37:20.692577 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58753 (* 1 = 7.58753 loss)
I0523 08:37:20.912374 34682 sgd_solver.cpp:112] Iteration 79730, lr = 0.01
I0523 08:37:25.696197 34682 solver.cpp:239] Iteration 79740 (1.99864 iter/s, 5.00341s/10 iters), loss = 7.98275
I0523 08:37:25.696265 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98275 (* 1 = 7.98275 loss)
I0523 08:37:25.757381 34682 sgd_solver.cpp:112] Iteration 79740, lr = 0.01
I0523 08:37:30.863512 34682 solver.cpp:239] Iteration 79750 (1.93535 iter/s, 5.16703s/10 iters), loss = 6.18118
I0523 08:37:30.863790 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.18118 (* 1 = 6.18118 loss)
I0523 08:37:30.938882 34682 sgd_solver.cpp:112] Iteration 79750, lr = 0.01
I0523 08:37:35.438220 34682 solver.cpp:239] Iteration 79760 (2.18615 iter/s, 4.57425s/10 iters), loss = 8.04634
I0523 08:37:35.438269 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04634 (* 1 = 8.04634 loss)
I0523 08:37:36.244300 34682 sgd_solver.cpp:112] Iteration 79760, lr = 0.01
I0523 08:37:39.718358 34682 solver.cpp:239] Iteration 79770 (2.3365 iter/s, 4.27991s/10 iters), loss = 7.61619
I0523 08:37:39.718408 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.61619 (* 1 = 7.61619 loss)
I0523 08:37:40.524183 34682 sgd_solver.cpp:112] Iteration 79770, lr = 0.01
I0523 08:37:46.261817 34682 solver.cpp:239] Iteration 79780 (1.52832 iter/s, 6.54314s/10 iters), loss = 6.81973
I0523 08:37:46.261868 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.81973 (* 1 = 6.81973 loss)
I0523 08:37:46.322778 34682 sgd_solver.cpp:112] Iteration 79780, lr = 0.01
I0523 08:37:49.576500 34682 solver.cpp:239] Iteration 79790 (3.01705 iter/s, 3.31449s/10 iters), loss = 8.26542
I0523 08:37:49.576541 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26542 (* 1 = 8.26542 loss)
I0523 08:37:49.638594 34682 sgd_solver.cpp:112] Iteration 79790, lr = 0.01
I0523 08:37:55.424511 34682 solver.cpp:239] Iteration 79800 (1.71007 iter/s, 5.84773s/10 iters), loss = 7.96296
I0523 08:37:55.424552 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96296 (* 1 = 7.96296 loss)
I0523 08:37:55.496053 34682 sgd_solver.cpp:112] Iteration 79800, lr = 0.01
I0523 08:38:00.631148 34682 solver.cpp:239] Iteration 79810 (1.92072 iter/s, 5.20638s/10 iters), loss = 8.34373
I0523 08:38:00.631201 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34373 (* 1 = 8.34373 loss)
I0523 08:38:01.505929 34682 sgd_solver.cpp:112] Iteration 79810, lr = 0.01
I0523 08:38:06.160290 34682 solver.cpp:239] Iteration 79820 (1.80869 iter/s, 5.52887s/10 iters), loss = 7.01058
I0523 08:38:06.160346 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.01058 (* 1 = 7.01058 loss)
I0523 08:38:06.230352 34682 sgd_solver.cpp:112] Iteration 79820, lr = 0.01
I0523 08:38:10.247184 34682 solver.cpp:239] Iteration 79830 (2.44698 iter/s, 4.08666s/10 iters), loss = 8.32204
I0523 08:38:10.247237 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32204 (* 1 = 8.32204 loss)
I0523 08:38:11.054944 34682 sgd_solver.cpp:112] Iteration 79830, lr = 0.01
I0523 08:38:16.371136 34682 solver.cpp:239] Iteration 79840 (1.63301 iter/s, 6.12366s/10 iters), loss = 8.47798
I0523 08:38:16.371186 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.47798 (* 1 = 8.47798 loss)
I0523 08:38:17.064419 34682 sgd_solver.cpp:112] Iteration 79840, lr = 0.01
I0523 08:38:21.808693 34682 solver.cpp:239] Iteration 79850 (1.83916 iter/s, 5.43727s/10 iters), loss = 8.52053
I0523 08:38:21.808775 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.52053 (* 1 = 8.52053 loss)
I0523 08:38:22.586514 34682 sgd_solver.cpp:112] Iteration 79850, lr = 0.01
I0523 08:38:28.153385 34682 solver.cpp:239] Iteration 79860 (1.5762 iter/s, 6.34437s/10 iters), loss = 7.4126
I0523 08:38:28.153435 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4126 (* 1 = 7.4126 loss)
I0523 08:38:28.968292 34682 sgd_solver.cpp:112] Iteration 79860, lr = 0.01
I0523 08:38:35.258435 34682 solver.cpp:239] Iteration 79870 (1.40752 iter/s, 7.10472s/10 iters), loss = 7.78559
I0523 08:38:35.258565 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.78559 (* 1 = 7.78559 loss)
I0523 08:38:35.316380 34682 sgd_solver.cpp:112] Iteration 79870, lr = 0.01
I0523 08:38:39.707932 34682 solver.cpp:239] Iteration 79880 (2.2476 iter/s, 4.44919s/10 iters), loss = 8.36169
I0523 08:38:39.707973 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.36169 (* 1 = 8.36169 loss)
I0523 08:38:39.780663 34682 sgd_solver.cpp:112] Iteration 79880, lr = 0.01
I0523 08:38:44.240840 34682 solver.cpp:239] Iteration 79890 (2.2062 iter/s, 4.53268s/10 iters), loss = 7.26515
I0523 08:38:44.240882 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.26515 (* 1 = 7.26515 loss)
I0523 08:38:44.315018 34682 sgd_solver.cpp:112] Iteration 79890, lr = 0.01
I0523 08:38:48.261508 34682 solver.cpp:239] Iteration 79900 (2.48728 iter/s, 4.02046s/10 iters), loss = 8.45263
I0523 08:38:48.261569 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.45263 (* 1 = 8.45263 loss)
I0523 08:38:49.082451 34682 sgd_solver.cpp:112] Iteration 79900, lr = 0.01
I0523 08:38:53.185468 34682 solver.cpp:239] Iteration 79910 (2.03099 iter/s, 4.9237s/10 iters), loss = 7.73326
I0523 08:38:53.185511 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.73326 (* 1 = 7.73326 loss)
I0523 08:38:53.986295 34682 sgd_solver.cpp:112] Iteration 79910, lr = 0.01
I0523 08:38:59.965036 34682 solver.cpp:239] Iteration 79920 (1.47509 iter/s, 6.77923s/10 iters), loss = 7.77165
I0523 08:38:59.965092 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.77165 (* 1 = 7.77165 loss)
I0523 08:39:00.839864 34682 sgd_solver.cpp:112] Iteration 79920, lr = 0.01
I0523 08:39:05.514142 34682 solver.cpp:239] Iteration 79930 (1.80219 iter/s, 5.5488s/10 iters), loss = 7.69026
I0523 08:39:05.514495 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69026 (* 1 = 7.69026 loss)
I0523 08:39:06.226894 34682 sgd_solver.cpp:112] Iteration 79930, lr = 0.01
I0523 08:39:10.070331 34682 solver.cpp:239] Iteration 79940 (2.19506 iter/s, 4.55569s/10 iters), loss = 7.73824
I0523 08:39:10.070376 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.73824 (* 1 = 7.73824 loss)
I0523 08:39:10.906019 34682 sgd_solver.cpp:112] Iteration 79940, lr = 0.01
I0523 08:39:15.932096 34682 solver.cpp:239] Iteration 79950 (1.70606 iter/s, 5.86147s/10 iters), loss = 8.11948
I0523 08:39:15.932186 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11948 (* 1 = 8.11948 loss)
I0523 08:39:15.999306 34682 sgd_solver.cpp:112] Iteration 79950, lr = 0.01
I0523 08:39:20.866266 34682 solver.cpp:239] Iteration 79960 (2.0268 iter/s, 4.93388s/10 iters), loss = 6.95755
I0523 08:39:20.866348 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.95755 (* 1 = 6.95755 loss)
I0523 08:39:20.928759 34682 sgd_solver.cpp:112] Iteration 79960, lr = 0.01
I0523 08:39:25.150972 34682 solver.cpp:239] Iteration 79970 (2.33402 iter/s, 4.28445s/10 iters), loss = 7.7379
I0523 08:39:25.151026 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7379 (* 1 = 7.7379 loss)
I0523 08:39:25.233793 34682 sgd_solver.cpp:112] Iteration 79970, lr = 0.01
I0523 08:39:31.427340 34682 solver.cpp:239] Iteration 79980 (1.59336 iter/s, 6.27606s/10 iters), loss = 7.72771
I0523 08:39:31.427392 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72771 (* 1 = 7.72771 loss)
I0523 08:39:31.525436 34682 sgd_solver.cpp:112] Iteration 79980, lr = 0.01
I0523 08:39:36.285506 34682 solver.cpp:239] Iteration 79990 (2.0585 iter/s, 4.85792s/10 iters), loss = 8.00192
I0523 08:39:36.285614 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00192 (* 1 = 8.00192 loss)
I0523 08:39:36.350253 34682 sgd_solver.cpp:112] Iteration 79990, lr = 0.01
I0523 08:39:42.497975 34682 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_80000.caffemodel
I0523 08:39:43.762393 34682 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageCdata/AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn/2018-05-22_AMImageCdata-b0.35s30_fc_0.35_112x96_b+FaceAdd_faceNet-20-light2s4-bn_zkx_iter_80000.solverstate
I0523 08:39:44.115198 34682 solver.cpp:239] Iteration 80000 (1.27726 iter/s, 7.82928s/10 iters), loss = 7.95596
I0523 08:39:44.115244 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95596 (* 1 = 7.95596 loss)
I0523 08:39:44.681885 34682 sgd_solver.cpp:112] Iteration 80000, lr = 0.01
I0523 08:39:49.566182 34682 solver.cpp:239] Iteration 80010 (1.83462 iter/s, 5.45072s/10 iters), loss = 7.43458
I0523 08:39:49.566231 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43458 (* 1 = 7.43458 loss)
I0523 08:39:49.629313 34682 sgd_solver.cpp:112] Iteration 80010, lr = 0.01
I0523 08:39:54.287719 34682 solver.cpp:239] Iteration 80020 (2.11806 iter/s, 4.72129s/10 iters), loss = 8.81441
I0523 08:39:54.287772 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.81441 (* 1 = 8.81441 loss)
I0523 08:39:54.356338 34682 sgd_solver.cpp:112] Iteration 80020, lr = 0.01
I0523 08:39:59.496106 34682 solver.cpp:239] Iteration 80030 (1.92008 iter/s, 5.20813s/10 iters), loss = 7.94436
I0523 08:39:59.496150 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94436 (* 1 = 7.94436 loss)
I0523 08:40:00.344468 34682 sgd_solver.cpp:112] Iteration 80030, lr = 0.01
I0523 08:40:05.232926 34682 solver.cpp:239] Iteration 80040 (1.74321 iter/s, 5.73654s/10 iters), loss = 7.85083
I0523 08:40:05.232972 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.85083 (* 1 = 7.85083 loss)
I0523 08:40:05.820040 34682 sgd_solver.cpp:112] Iteration 80040, lr = 0.01
I0523 08:40:09.593391 34682 solver.cpp:239] Iteration 80050 (2.29345 iter/s, 4.36024s/10 iters), loss = 7.02304
I0523 08:40:09.593601 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.02304 (* 1 = 7.02304 loss)
I0523 08:40:09.654386 34682 sgd_solver.cpp:112] Iteration 80050, lr = 0.01
I0523 08:40:12.286001 34682 solver.cpp:239] Iteration 80060 (3.71431 iter/s, 2.69229s/10 iters), loss = 8.09938
I0523 08:40:12.286053 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.09938 (* 1 = 8.09938 loss)
I0523 08:40:13.058149 34682 sgd_solver.cpp:112] Iteration 80060, lr = 0.01
I0523 08:40:17.405916 34682 solver.cpp:239] Iteration 80070 (1.95326 iter/s, 5.11965s/10 iters), loss = 7.98432
I0523 08:40:17.405979 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98432 (* 1 = 7.98432 loss)
I0523 08:40:18.216395 34682 sgd_solver.cpp:112] Iteration 80070, lr = 0.01
I0523 08:40:23.236191 34682 solver.cpp:239] Iteration 80080 (1.71527 iter/s, 5.82998s/10 iters), loss = 7.19176
I0523 08:40:23.236258 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.19176 (* 1 = 7.19176 loss)
I0523 08:40:23.306156 34682 sgd_solver.cpp:112] Iteration 80080, lr = 0.01
I0523 08:40:27.845198 34682 solver.cpp:239] Iteration 80090 (2.16978 iter/s, 4.60876s/10 iters), loss = 7.8236
I0523 08:40:27.845244 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.8236 (* 1 = 7.8236 loss)
I0523 08:40:27.923807 34682 sgd_solver.cpp:112] Iteration 80090, lr = 0.01
I0523 08:40:34.754834 34682 solver.cpp:239] Iteration 80100 (1.44732 iter/s, 6.90931s/10 iters), loss = 6.87583
I0523 08:40:34.754882 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.87583 (* 1 = 6.87583 loss)
I0523 08:40:34.822090 34682 sgd_solver.cpp:112] Iteration 80100, lr = 0.01
I0523 08:40:42.119261 34682 solver.cpp:239] Iteration 80110 (1.35794 iter/s, 7.36409s/10 iters), loss = 6.54531
I0523 08:40:42.119608 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.54531 (* 1 = 6.54531 loss)
I0523 08:40:42.181113 34682 sgd_solver.cpp:112] Iteration 80110, lr = 0.01
I0523 08:40:47.691323 34682 solver.cpp:239] Iteration 80120 (1.79483 iter/s, 5.57156s/10 iters), loss = 7.98677
I0523 08:40:47.691367 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98677 (* 1 = 7.98677 loss)
I0523 08:40:47.752890 34682 sgd_solver.cpp:112] Iteration 80120, lr = 0.01
I0523 08:40:53.036732 34682 solver.cpp:239] Iteration 80130 (1.87086 iter/s, 5.34515s/10 iters), loss = 7.81608
I0523 08:40:53.036772 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81608 (* 1 = 7.81608 loss)
I0523 08:40:53.771015 34682 sgd_solver.cpp:112] Iteration 80130, lr = 0.01
I0523 08:40:59.168340 34682 solver.cpp:239] Iteration 80140 (1.63216 iter/s, 6.12685s/10 iters), loss = 7.9464
I0523 08:40:59.168421 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9464 (* 1 = 7.9464 loss)
I0523 08:40:59.566581 34682 sgd_solver.cpp:112] Iteration 80140, lr = 0.01
I0523 08:41:03.569392 34682 solver.cpp:239] Iteration 80150 (2.27232 iter/s, 4.40079s/10 iters), loss = 7.19448
I0523 08:41:03.569439 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.19448 (* 1 = 7.19448 loss)
I0523 08:41:03.640333 34682 sgd_solver.cpp:112] Iteration 80150, lr = 0.01
I0523 08:41:06.843766 34682 solver.cpp:239] Iteration 80160 (3.05419 iter/s, 3.27419s/10 iters), loss = 7.51831
I0523 08:41:06.843806 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51831 (* 1 = 7.51831 loss)
I0523 08:41:06.905683 34682 sgd_solver.cpp:112] Iteration 80160, lr = 0.01
I0523 08:41:10.106639 34682 solver.cpp:239] Iteration 80170 (3.06495 iter/s, 3.26269s/10 iters), loss = 8.0457
I0523 08:41:10.106683 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0457 (* 1 = 8.0457 loss)
I0523 08:41:10.171095 34682 sgd_solver.cpp:112] Iteration 80170, lr = 0.01
I0523 08:41:14.022711 34682 solver.cpp:239] Iteration 80180 (2.55372 iter/s, 3.91586s/10 iters), loss = 7.31026
I0523 08:41:14.022864 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.31026 (* 1 = 7.31026 loss)
I0523 08:41:14.106506 34682 sgd_solver.cpp:112] Iteration 80180, lr = 0.01
I0523 08:41:19.562983 34682 solver.cpp:239] Iteration 80190 (1.80508 iter/s, 5.53991s/10 iters), loss = 7.39004
I0523 08:41:19.563035 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.39004 (* 1 = 7.39004 loss)
I0523 08:41:20.411418 34682 sgd_solver.cpp:112] Iteration 80190, lr = 0.01
I0523 08:41:25.926219 34682 solver.cpp:239] Iteration 80200 (1.5716 iter/s, 6.36293s/10 iters), loss = 7.47371
I0523 08:41:25.926265 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.47371 (* 1 = 7.47371 loss)
I0523 08:41:26.671103 34682 sgd_solver.cpp:112] Iteration 80200, lr = 0.01
I0523 08:41:29.258904 34682 solver.cpp:239] Iteration 80210 (3.00075 iter/s, 3.3325s/10 iters), loss = 7.99568
I0523 08:41:29.258954 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.99568 (* 1 = 7.99568 loss)
I0523 08:41:29.849799 34682 sgd_solver.cpp:112] Iteration 80210, lr = 0.01
I0523 08:41:33.323951 34682 solver.cpp:239] Iteration 80220 (2.46013 iter/s, 4.06483s/10 iters), loss = 8.00404
I0523 08:41:33.324007 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00404 (* 1 = 8.00404 loss)
I0523 08:41:34.157194 34682 sgd_solver.cpp:112] Iteration 80220, lr = 0.01
I0523 08:41:37.672467 34682 solver.cpp:239] Iteration 80230 (2.29976 iter/s, 4.34829s/10 iters), loss = 7.9311
I0523 08:41:37.672524 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9311 (* 1 = 7.9311 loss)
I0523 08:41:38.241272 34682 sgd_solver.cpp:112] Iteration 80230, lr = 0.01
I0523 08:41:41.987157 34682 solver.cpp:239] Iteration 80240 (2.31779 iter/s, 4.31446s/10 iters), loss = 7.53718
I0523 08:41:41.987207 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.53718 (* 1 = 7.53718 loss)
I0523 08:41:42.468792 34682 sgd_solver.cpp:112] Iteration 80240, lr = 0.01
I0523 08:41:49.085443 34682 solver.cpp:239] Iteration 80250 (1.40886 iter/s, 7.09795s/10 iters), loss = 7.27387
I0523 08:41:49.085611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.27387 (* 1 = 7.27387 loss)
I0523 08:41:49.958823 34682 sgd_solver.cpp:112] Iteration 80250, lr = 0.01
I0523 08:41:56.310200 34682 solver.cpp:239] Iteration 80260 (1.38422 iter/s, 7.2243s/10 iters), loss = 7.06727
I0523 08:41:56.310297 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.06727 (* 1 = 7.06727 loss)
I0523 08:41:57.089352 34682 sgd_solver.cpp:112] Iteration 80260, lr = 0.01
I0523 08:42:01.922204 34682 solver.cpp:239] Iteration 80270 (1.782 iter/s, 5.61168s/10 iters), loss = 7.66572
I0523 08:42:01.922260 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.66572 (* 1 = 7.66572 loss)
I0523 08:42:02.772156 34682 sgd_solver.cpp:112] Iteration 80270, lr = 0.01
I0523 08:42:08.073485 34682 solver.cpp:239] Iteration 80280 (1.62576 iter/s, 6.15097s/10 iters), loss = 7.51285
I0523 08:42:08.073559 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.51285 (* 1 = 7.51285 loss)
I0523 08:42:08.135166 34682 sgd_solver.cpp:112] Iteration 80280, lr = 0.01
I0523 08:42:15.282102 34682 solver.cpp:239] Iteration 80290 (1.3873 iter/s, 7.20826s/10 iters), loss = 7.5027
I0523 08:42:15.282150 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5027 (* 1 = 7.5027 loss)
I0523 08:42:15.359680 34682 sgd_solver.cpp:112] Iteration 80290, lr = 0.01
I0523 08:42:20.883507 34682 solver.cpp:239] Iteration 80300 (1.78536 iter/s, 5.60113s/10 iters), loss = 7.7017
I0523 08:42:20.883682 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7017 (* 1 = 7.7017 loss)
I0523 08:42:21.694777 34682 sgd_solver.cpp:112] Iteration 80300, lr = 0.01
I0523 08:42:25.309036 34682 solver.cpp:239] Iteration 80310 (2.2598 iter/s, 4.42517s/10 iters), loss = 6.76246
I0523 08:42:25.309088 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.76246 (* 1 = 6.76246 loss)
I0523 08:42:25.377970 34682 sgd_solver.cpp:112] Iteration 80310, lr = 0.01
I0523 08:42:29.297260 34682 solver.cpp:239] Iteration 80320 (2.50752 iter/s, 3.988s/10 iters), loss = 7.18629
I0523 08:42:29.297314 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.18629 (* 1 = 7.18629 loss)
I0523 08:42:29.365378 34682 sgd_solver.cpp:112] Iteration 80320, lr = 0.01
I0523 08:42:34.050417 34682 solver.cpp:239] Iteration 80330 (2.10397 iter/s, 4.75291s/10 iters), loss = 7.14967
I0523 08:42:34.050458 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.14967 (* 1 = 7.14967 loss)
I0523 08:42:34.883275 34682 sgd_solver.cpp:112] Iteration 80330, lr = 0.01
I0523 08:42:39.057364 34682 solver.cpp:239] Iteration 80340 (1.99732 iter/s, 5.0067s/10 iters), loss = 7.6972
I0523 08:42:39.057410 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.6972 (* 1 = 7.6972 loss)
I0523 08:42:39.495214 34682 sgd_solver.cpp:112] Iteration 80340, lr = 0.01
I0523 08:42:42.979331 34682 solver.cpp:239] Iteration 80350 (2.54987 iter/s, 3.92176s/10 iters), loss = 8.1741
I0523 08:42:42.979377 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1741 (* 1 = 8.1741 loss)
I0523 08:42:43.038238 34682 sgd_solver.cpp:112] Iteration 80350, lr = 0.01
I0523 08:42:47.100741 34682 solver.cpp:239] Iteration 80360 (2.42648 iter/s, 4.12119s/10 iters), loss = 7.5236
I0523 08:42:47.100793 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5236 (* 1 = 7.5236 loss)
I0523 08:42:47.169970 34682 sgd_solver.cpp:112] Iteration 80360, lr = 0.01
I0523 08:42:51.556464 34682 solver.cpp:239] Iteration 80370 (2.24443 iter/s, 4.45548s/10 iters), loss = 8.21401
I0523 08:42:51.556713 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.21401 (* 1 = 8.21401 loss)
I0523 08:42:51.629164 34682 sgd_solver.cpp:112] Iteration 80370, lr = 0.01
I0523 08:42:54.193998 34682 solver.cpp:239] Iteration 80380 (3.7919 iter/s, 2.6372s/10 iters), loss = 7.57186
I0523 08:42:54.194069 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.57186 (* 1 = 7.57186 loss)
I0523 08:42:55.053619 34682 sgd_solver.cpp:112] Iteration 80380, lr = 0.01
I0523 08:42:58.187309 34682 solver.cpp:239] Iteration 80390 (2.50433 iter/s, 3.99308s/10 iters), loss = 8.40323
I0523 08:42:58.187355 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40323 (* 1 = 8.40323 loss)
I0523 08:42:58.262981 34682 sgd_solver.cpp:112] Iteration 80390, lr = 0.01
I0523 08:43:03.440625 34682 solver.cpp:239] Iteration 80400 (1.90366 iter/s, 5.25305s/10 iters), loss = 7.4361
I0523 08:43:03.440683 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4361 (* 1 = 7.4361 loss)
I0523 08:43:03.503931 34682 sgd_solver.cpp:112] Iteration 80400, lr = 0.01
I0523 08:43:11.074676 34682 solver.cpp:239] Iteration 80410 (1.30998 iter/s, 7.63368s/10 iters), loss = 7.7894
I0523 08:43:11.074750 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7894 (* 1 = 7.7894 loss)
I0523 08:43:11.948710 34682 sgd_solver.cpp:112] Iteration 80410, lr = 0.01
I0523 08:43:18.573468 34682 solver.cpp:239] Iteration 80420 (1.33362 iter/s, 7.49841s/10 iters), loss = 6.83925
I0523 08:43:18.573525 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.83925 (* 1 = 6.83925 loss)
I0523 08:43:19.437814 34682 sgd_solver.cpp:112] Iteration 80420, lr = 0.01
I0523 08:43:24.748286 34682 solver.cpp:239] Iteration 80430 (1.61956 iter/s, 6.17451s/10 iters), loss = 7.30594
I0523 08:43:24.748522 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.30594 (* 1 = 7.30594 loss)
I0523 08:43:24.827628 34682 sgd_solver.cpp:112] Iteration 80430, lr = 0.01
I0523 08:43:28.117322 34682 solver.cpp:239] Iteration 80440 (2.96852 iter/s, 3.36868s/10 iters), loss = 7.97429
I0523 08:43:28.117377 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.97429 (* 1 = 7.97429 loss)
I0523 08:43:28.884454 34682 sgd_solver.cpp:112] Iteration 80440, lr = 0.01
I0523 08:43:33.778759 34682 solver.cpp:239] Iteration 80450 (1.76643 iter/s, 5.66115s/10 iters), loss = 7.53791
I0523 08:43:33.778811 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.53791 (* 1 = 7.53791 loss)
I0523 08:43:34.565704 34682 sgd_solver.cpp:112] Iteration 80450, lr = 0.01
I0523 08:43:40.585077 34682 solver.cpp:239] Iteration 80460 (1.46929 iter/s, 6.80599s/10 iters), loss = 8.61127
I0523 08:43:40.585135 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.61127 (* 1 = 8.61127 loss)
I0523 08:43:40.662004 34682 sgd_solver.cpp:112] Iteration 80460, lr = 0.01
I0523 08:43:45.022402 34682 solver.cpp:239] Iteration 80470 (2.25373 iter/s, 4.43709s/10 iters), loss = 7.7023
I0523 08:43:45.022445 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.7023 (* 1 = 7.7023 loss)
I0523 08:43:45.086390 34682 sgd_solver.cpp:112] Iteration 80470, lr = 0.01
I0523 08:43:50.471928 34682 solver.cpp:239] Iteration 80480 (1.83511 iter/s, 5.44925s/10 iters), loss = 8.46766
I0523 08:43:50.471993 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46766 (* 1 = 8.46766 loss)
I0523 08:43:51.327435 34682 sgd_solver.cpp:112] Iteration 80480, lr = 0.01
I0523 08:43:55.622799 34682 solver.cpp:239] Iteration 80490 (1.94152 iter/s, 5.15059s/10 iters), loss = 8.06064
I0523 08:43:55.622920 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.06064 (* 1 = 8.06064 loss)
I0523 08:43:56.424962 34682 sgd_solver.cpp:112] Iteration 80490, lr = 0.01
I0523 08:44:02.783607 34682 solver.cpp:239] Iteration 80500 (1.39657 iter/s, 7.1604s/10 iters), loss = 8.15286
I0523 08:44:02.783669 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.15286 (* 1 = 8.15286 loss)
I0523 08:44:03.429744 34682 sgd_solver.cpp:112] Iteration 80500, lr = 0.01
I0523 08:44:08.188300 34682 solver.cpp:239] Iteration 80510 (1.85035 iter/s, 5.40439s/10 iters), loss = 7.98495
I0523 08:44:08.188387 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.98495 (* 1 = 7.98495 loss)
I0523 08:44:09.061441 34682 sgd_solver.cpp:112] Iteration 80510, lr = 0.01
I0523 08:44:12.313962 34682 solver.cpp:239] Iteration 80520 (2.424 iter/s, 4.12541s/10 iters), loss = 6.84198
I0523 08:44:12.314007 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.84198 (* 1 = 6.84198 loss)
I0523 08:44:12.385488 34682 sgd_solver.cpp:112] Iteration 80520, lr = 0.01
I0523 08:44:15.495168 34682 solver.cpp:239] Iteration 80530 (3.14365 iter/s, 3.18102s/10 iters), loss = 7.18796
I0523 08:44:15.495231 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.18796 (* 1 = 7.18796 loss)
I0523 08:44:16.295828 34682 sgd_solver.cpp:112] Iteration 80530, lr = 0.01
I0523 08:44:21.025532 34682 solver.cpp:239] Iteration 80540 (1.8083 iter/s, 5.53007s/10 iters), loss = 7.83974
I0523 08:44:21.025672 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83974 (* 1 = 7.83974 loss)
I0523 08:44:21.820199 34682 sgd_solver.cpp:112] Iteration 80540, lr = 0.01
I0523 08:44:25.628419 34682 solver.cpp:239] Iteration 80550 (2.17271 iter/s, 4.60256s/10 iters), loss = 7.76649
I0523 08:44:25.628670 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.76649 (* 1 = 7.76649 loss)
I0523 08:44:25.695116 34682 sgd_solver.cpp:112] Iteration 80550, lr = 0.01
I0523 08:44:29.809587 34682 solver.cpp:239] Iteration 80560 (2.39192 iter/s, 4.18075s/10 iters), loss = 7.20966
I0523 08:44:29.809630 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.20966 (* 1 = 7.20966 loss)
I0523 08:44:29.889300 34682 sgd_solver.cpp:112] Iteration 80560, lr = 0.01
I0523 08:44:34.075161 34682 solver.cpp:239] Iteration 80570 (2.34447 iter/s, 4.26535s/10 iters), loss = 8.05917
I0523 08:44:34.075201 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05917 (* 1 = 8.05917 loss)
I0523 08:44:34.152755 34682 sgd_solver.cpp:112] Iteration 80570, lr = 0.01
I0523 08:44:39.083571 34682 solver.cpp:239] Iteration 80580 (1.99674 iter/s, 5.00816s/10 iters), loss = 7.30585
I0523 08:44:39.083611 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.30585 (* 1 = 7.30585 loss)
I0523 08:44:39.144135 34682 sgd_solver.cpp:112] Iteration 80580, lr = 0.01
I0523 08:44:42.485576 34682 solver.cpp:239] Iteration 80590 (2.93961 iter/s, 3.40182s/10 iters), loss = 7.37197
I0523 08:44:42.485617 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.37197 (* 1 = 7.37197 loss)
I0523 08:44:42.566398 34682 sgd_solver.cpp:112] Iteration 80590, lr = 0.01
I0523 08:44:45.745378 34682 solver.cpp:239] Iteration 80600 (3.06784 iter/s, 3.25962s/10 iters), loss = 7.89368
I0523 08:44:45.745434 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.89368 (* 1 = 7.89368 loss)
I0523 08:44:46.523030 34682 sgd_solver.cpp:112] Iteration 80600, lr = 0.01
I0523 08:44:50.555383 34682 solver.cpp:239] Iteration 80610 (2.07911 iter/s, 4.80976s/10 iters), loss = 7.04456
I0523 08:44:50.555423 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.04456 (* 1 = 7.04456 loss)
I0523 08:44:50.617700 34682 sgd_solver.cpp:112] Iteration 80610, lr = 0.01
I0523 08:44:54.720405 34682 solver.cpp:239] Iteration 80620 (2.40107 iter/s, 4.16481s/10 iters), loss = 7.61181
I0523 08:44:54.720445 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.61181 (* 1 = 7.61181 loss)
I0523 08:44:55.541934 34682 sgd_solver.cpp:112] Iteration 80620, lr = 0.01
I0523 08:44:59.316155 34682 solver.cpp:239] Iteration 80630 (2.17603 iter/s, 4.59552s/10 iters), loss = 6.93526
I0523 08:44:59.316431 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.93526 (* 1 = 6.93526 loss)
I0523 08:44:59.373633 34682 sgd_solver.cpp:112] Iteration 80630, lr = 0.01
I0523 08:45:04.605751 34682 solver.cpp:239] Iteration 80640 (1.89066 iter/s, 5.28915s/10 iters), loss = 8.91636
I0523 08:45:04.605796 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.91636 (* 1 = 8.91636 loss)
I0523 08:45:04.669108 34682 sgd_solver.cpp:112] Iteration 80640, lr = 0.01
I0523 08:45:08.831467 34682 solver.cpp:239] Iteration 80650 (2.36658 iter/s, 4.2255s/10 iters), loss = 6.66809
I0523 08:45:08.831512 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.66809 (* 1 = 6.66809 loss)
I0523 08:45:09.695365 34682 sgd_solver.cpp:112] Iteration 80650, lr = 0.01
I0523 08:45:13.970634 34682 solver.cpp:239] Iteration 80660 (1.94594 iter/s, 5.13891s/10 iters), loss = 6.86827
I0523 08:45:13.970692 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.86827 (* 1 = 6.86827 loss)
I0523 08:45:14.028810 34682 sgd_solver.cpp:112] Iteration 80660, lr = 0.01
I0523 08:45:17.958955 34682 solver.cpp:239] Iteration 80670 (2.50746 iter/s, 3.98809s/10 iters), loss = 7.437
I0523 08:45:17.959022 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.437 (* 1 = 7.437 loss)
I0523 08:45:18.032446 34682 sgd_solver.cpp:112] Iteration 80670, lr = 0.01
I0523 08:45:22.870898 34682 solver.cpp:239] Iteration 80680 (2.03597 iter/s, 4.91167s/10 iters), loss = 7.42963
I0523 08:45:22.870942 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42963 (* 1 = 7.42963 loss)
I0523 08:45:22.936296 34682 sgd_solver.cpp:112] Iteration 80680, lr = 0.01
I0523 08:45:25.419406 34682 solver.cpp:239] Iteration 80690 (3.9241 iter/s, 2.54835s/10 iters), loss = 8.12934
I0523 08:45:25.419452 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.12934 (* 1 = 8.12934 loss)
I0523 08:45:26.276345 34682 sgd_solver.cpp:112] Iteration 80690, lr = 0.01
I0523 08:45:31.067872 34682 solver.cpp:239] Iteration 80700 (1.77048 iter/s, 5.64819s/10 iters), loss = 7.33634
I0523 08:45:31.068055 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.33634 (* 1 = 7.33634 loss)
I0523 08:45:31.134889 34682 sgd_solver.cpp:112] Iteration 80700, lr = 0.01
I0523 08:45:35.801825 34682 solver.cpp:239] Iteration 80710 (2.11257 iter/s, 4.73358s/10 iters), loss = 7.11983
I0523 08:45:35.801863 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.11983 (* 1 = 7.11983 loss)
I0523 08:45:35.878305 34682 sgd_solver.cpp:112] Iteration 80710, lr = 0.01
I0523 08:45:40.094723 34682 solver.cpp:239] Iteration 80720 (2.32956 iter/s, 4.29266s/10 iters), loss = 7.65961
I0523 08:45:40.094777 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.65961 (* 1 = 7.65961 loss)
I0523 08:45:40.845450 34682 sgd_solver.cpp:112] Iteration 80720, lr = 0.01
I0523 08:45:45.063410 34682 solver.cpp:239] Iteration 80730 (2.01271 iter/s, 4.96843s/10 iters), loss = 7.57252
I0523 08:45:45.063459 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.57252 (* 1 = 7.57252 loss)
I0523 08:45:45.126816 34682 sgd_solver.cpp:112] Iteration 80730, lr = 0.01
I0523 08:45:49.343921 34682 solver.cpp:239] Iteration 80740 (2.33629 iter/s, 4.28029s/10 iters), loss = 6.92265
I0523 08:45:49.343966 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.92265 (* 1 = 6.92265 loss)
I0523 08:45:50.204774 34682 sgd_solver.cpp:112] Iteration 80740, lr = 0.01
I0523 08:45:55.338584 34682 solver.cpp:239] Iteration 80750 (1.66823 iter/s, 5.99436s/10 iters), loss = 7.55105
I0523 08:45:55.338645 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.55105 (* 1 = 7.55105 loss)
I0523 08:45:55.418815 34682 sgd_solver.cpp:112] Iteration 80750, lr = 0.01
I0523 08:46:00.194417 34682 solver.cpp:239] Iteration 80760 (2.05949 iter/s, 4.85557s/10 iters), loss = 8.11853
I0523 08:46:00.194475 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.11853 (* 1 = 8.11853 loss)
I0523 08:46:01.009024 34682 sgd_solver.cpp:112] Iteration 80760, lr = 0.01
I0523 08:46:05.155719 34682 solver.cpp:239] Iteration 80770 (2.01571 iter/s, 4.96103s/10 iters), loss = 8.10479
I0523 08:46:05.155949 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10479 (* 1 = 8.10479 loss)
I0523 08:46:05.787117 34682 sgd_solver.cpp:112] Iteration 80770, lr = 0.01
I0523 08:46:08.383496 34682 solver.cpp:239] Iteration 80780 (3.09843 iter/s, 3.22744s/10 iters), loss = 8.01812
I0523 08:46:08.383549 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.01812 (* 1 = 8.01812 loss)
I0523 08:46:08.633855 34682 sgd_solver.cpp:112] Iteration 80780, lr = 0.01
I0523 08:46:14.131919 34682 solver.cpp:239] Iteration 80790 (1.73971 iter/s, 5.7481s/10 iters), loss = 7.87463
I0523 08:46:14.132011 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87463 (* 1 = 7.87463 loss)
I0523 08:46:14.911576 34682 sgd_solver.cpp:112] Iteration 80790, lr = 0.01
I0523 08:46:22.016571 34682 solver.cpp:239] Iteration 80800 (1.26835 iter/s, 7.88424s/10 iters), loss = 8.31084
I0523 08:46:22.016631 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31084 (* 1 = 8.31084 loss)
I0523 08:46:22.651563 34682 sgd_solver.cpp:112] Iteration 80800, lr = 0.01
I0523 08:46:25.943037 34682 solver.cpp:239] Iteration 80810 (2.54697 iter/s, 3.92623s/10 iters), loss = 8.4243
I0523 08:46:25.943100 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.4243 (* 1 = 8.4243 loss)
I0523 08:46:26.720600 34682 sgd_solver.cpp:112] Iteration 80810, lr = 0.01
I0523 08:46:30.735478 34682 solver.cpp:239] Iteration 80820 (2.08674 iter/s, 4.79216s/10 iters), loss = 7.95479
I0523 08:46:30.735535 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.95479 (* 1 = 7.95479 loss)
I0523 08:46:31.610965 34682 sgd_solver.cpp:112] Iteration 80820, lr = 0.01
I0523 08:46:35.861156 34682 solver.cpp:239] Iteration 80830 (1.95106 iter/s, 5.12541s/10 iters), loss = 8.40655
I0523 08:46:35.861402 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40655 (* 1 = 8.40655 loss)
I0523 08:46:35.918184 34682 sgd_solver.cpp:112] Iteration 80830, lr = 0.01
I0523 08:46:39.631203 34682 solver.cpp:239] Iteration 80840 (2.65276 iter/s, 3.76966s/10 iters), loss = 7.43136
I0523 08:46:39.631286 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43136 (* 1 = 7.43136 loss)
I0523 08:46:39.694561 34682 sgd_solver.cpp:112] Iteration 80840, lr = 0.01
I0523 08:46:43.024416 34682 solver.cpp:239] Iteration 80850 (2.94726 iter/s, 3.39298s/10 iters), loss = 7.68914
I0523 08:46:43.024471 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68914 (* 1 = 7.68914 loss)
I0523 08:46:43.498380 34682 sgd_solver.cpp:112] Iteration 80850, lr = 0.01
I0523 08:46:46.747095 34682 solver.cpp:239] Iteration 80860 (2.6864 iter/s, 3.72246s/10 iters), loss = 8.39526
I0523 08:46:46.747143 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39526 (* 1 = 8.39526 loss)
I0523 08:46:46.810341 34682 sgd_solver.cpp:112] Iteration 80860, lr = 0.01
I0523 08:46:51.460211 34682 solver.cpp:239] Iteration 80870 (2.12185 iter/s, 4.71286s/10 iters), loss = 7.62358
I0523 08:46:51.460248 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.62358 (* 1 = 7.62358 loss)
I0523 08:46:51.531729 34682 sgd_solver.cpp:112] Iteration 80870, lr = 0.01
I0523 08:46:55.860523 34682 solver.cpp:239] Iteration 80880 (2.27268 iter/s, 4.40009s/10 iters), loss = 7.5478
I0523 08:46:55.860575 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5478 (* 1 = 7.5478 loss)
I0523 08:46:56.714568 34682 sgd_solver.cpp:112] Iteration 80880, lr = 0.01
I0523 08:47:01.004537 34682 solver.cpp:239] Iteration 80890 (1.94411 iter/s, 5.14375s/10 iters), loss = 7.32312
I0523 08:47:01.004606 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.32312 (* 1 = 7.32312 loss)
I0523 08:47:01.825131 34682 sgd_solver.cpp:112] Iteration 80890, lr = 0.01
I0523 08:47:05.688503 34682 solver.cpp:239] Iteration 80900 (2.13506 iter/s, 4.68371s/10 iters), loss = 7.44518
I0523 08:47:05.688552 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44518 (* 1 = 7.44518 loss)
I0523 08:47:05.757258 34682 sgd_solver.cpp:112] Iteration 80900, lr = 0.01
I0523 08:47:09.022517 34682 solver.cpp:239] Iteration 80910 (2.99955 iter/s, 3.33383s/10 iters), loss = 8.35846
I0523 08:47:09.022770 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.35846 (* 1 = 8.35846 loss)
I0523 08:47:09.855859 34682 sgd_solver.cpp:112] Iteration 80910, lr = 0.01
I0523 08:47:14.854683 34682 solver.cpp:239] Iteration 80920 (1.71477 iter/s, 5.8317s/10 iters), loss = 7.07459
I0523 08:47:14.855172 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.07459 (* 1 = 7.07459 loss)
I0523 08:47:14.912973 34682 sgd_solver.cpp:112] Iteration 80920, lr = 0.01
I0523 08:47:20.282235 34682 solver.cpp:239] Iteration 80930 (1.84269 iter/s, 5.42685s/10 iters), loss = 6.98662
I0523 08:47:20.282284 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.98662 (* 1 = 6.98662 loss)
I0523 08:47:20.344317 34682 sgd_solver.cpp:112] Iteration 80930, lr = 0.01
I0523 08:47:25.377992 34682 solver.cpp:239] Iteration 80940 (1.96252 iter/s, 5.0955s/10 iters), loss = 7.47709
I0523 08:47:25.378046 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.47709 (* 1 = 7.47709 loss)
I0523 08:47:26.118827 34682 sgd_solver.cpp:112] Iteration 80940, lr = 0.01
I0523 08:47:31.406488 34682 solver.cpp:239] Iteration 80950 (1.65887 iter/s, 6.0282s/10 iters), loss = 7.85853
I0523 08:47:31.406527 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.85853 (* 1 = 7.85853 loss)
I0523 08:47:31.485260 34682 sgd_solver.cpp:112] Iteration 80950, lr = 0.01
I0523 08:47:36.665150 34682 solver.cpp:239] Iteration 80960 (1.90172 iter/s, 5.25841s/10 iters), loss = 7.54874
I0523 08:47:36.665202 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54874 (* 1 = 7.54874 loss)
I0523 08:47:36.730020 34682 sgd_solver.cpp:112] Iteration 80960, lr = 0.01
I0523 08:47:41.424418 34682 solver.cpp:239] Iteration 80970 (2.10127 iter/s, 4.75902s/10 iters), loss = 8.02953
I0523 08:47:41.424618 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02953 (* 1 = 8.02953 loss)
I0523 08:47:42.259923 34682 sgd_solver.cpp:112] Iteration 80970, lr = 0.01
I0523 08:47:47.138499 34682 solver.cpp:239] Iteration 80980 (1.7502 iter/s, 5.71364s/10 iters), loss = 7.37714
I0523 08:47:47.138578 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.37714 (* 1 = 7.37714 loss)
I0523 08:47:47.201926 34682 sgd_solver.cpp:112] Iteration 80980, lr = 0.01
I0523 08:47:51.714041 34682 solver.cpp:239] Iteration 80990 (2.18566 iter/s, 4.57528s/10 iters), loss = 8.37603
I0523 08:47:51.714081 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37603 (* 1 = 8.37603 loss)
I0523 08:47:51.787248 34682 sgd_solver.cpp:112] Iteration 80990, lr = 0.01
I0523 08:47:57.366112 34682 solver.cpp:239] Iteration 81000 (1.76935 iter/s, 5.65179s/10 iters), loss = 8.00391
I0523 08:47:57.366169 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00391 (* 1 = 8.00391 loss)
I0523 08:47:58.083083 34682 sgd_solver.cpp:112] Iteration 81000, lr = 0.01
I0523 08:48:03.117156 34682 solver.cpp:239] Iteration 81010 (1.7389 iter/s, 5.75076s/10 iters), loss = 7.23862
I0523 08:48:03.117202 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.23862 (* 1 = 7.23862 loss)
I0523 08:48:03.827904 34682 sgd_solver.cpp:112] Iteration 81010, lr = 0.01
I0523 08:48:08.178287 34682 solver.cpp:239] Iteration 81020 (1.97594 iter/s, 5.06088s/10 iters), loss = 6.97797
I0523 08:48:08.178329 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.97797 (* 1 = 6.97797 loss)
I0523 08:48:08.248801 34682 sgd_solver.cpp:112] Iteration 81020, lr = 0.01
I0523 08:48:12.408136 34682 solver.cpp:239] Iteration 81030 (2.36427 iter/s, 4.22963s/10 iters), loss = 8.76159
I0523 08:48:12.408272 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.76159 (* 1 = 8.76159 loss)
I0523 08:48:13.210651 34682 sgd_solver.cpp:112] Iteration 81030, lr = 0.01
I0523 08:48:17.228085 34682 solver.cpp:239] Iteration 81040 (2.07485 iter/s, 4.81962s/10 iters), loss = 7.80498
I0523 08:48:17.228128 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80498 (* 1 = 7.80498 loss)
I0523 08:48:17.304889 34682 sgd_solver.cpp:112] Iteration 81040, lr = 0.01
I0523 08:48:21.253770 34682 solver.cpp:239] Iteration 81050 (2.48418 iter/s, 4.02547s/10 iters), loss = 8.43241
I0523 08:48:21.253820 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.43241 (* 1 = 8.43241 loss)
I0523 08:48:22.097358 34682 sgd_solver.cpp:112] Iteration 81050, lr = 0.01
I0523 08:48:28.491214 34682 solver.cpp:239] Iteration 81060 (1.38177 iter/s, 7.2371s/10 iters), loss = 7.09611
I0523 08:48:28.491291 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.09611 (* 1 = 7.09611 loss)
I0523 08:48:28.579900 34682 sgd_solver.cpp:112] Iteration 81060, lr = 0.01
I0523 08:48:32.752586 34682 solver.cpp:239] Iteration 81070 (2.3468 iter/s, 4.26112s/10 iters), loss = 7.08263
I0523 08:48:32.752629 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.08263 (* 1 = 7.08263 loss)
I0523 08:48:32.818928 34682 sgd_solver.cpp:112] Iteration 81070, lr = 0.01
I0523 08:48:37.681888 34682 solver.cpp:239] Iteration 81080 (2.02879 iter/s, 4.92906s/10 iters), loss = 7.84419
I0523 08:48:37.681937 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84419 (* 1 = 7.84419 loss)
I0523 08:48:37.755287 34682 sgd_solver.cpp:112] Iteration 81080, lr = 0.01
I0523 08:48:42.260866 34682 solver.cpp:239] Iteration 81090 (2.18401 iter/s, 4.57874s/10 iters), loss = 8.10528
I0523 08:48:42.260936 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.10528 (* 1 = 8.10528 loss)
I0523 08:48:43.106078 34682 sgd_solver.cpp:112] Iteration 81090, lr = 0.01
I0523 08:48:47.985883 34682 solver.cpp:239] Iteration 81100 (1.74681 iter/s, 5.72471s/10 iters), loss = 7.60916
I0523 08:48:47.985944 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.60916 (* 1 = 7.60916 loss)
I0523 08:48:48.772442 34682 sgd_solver.cpp:112] Iteration 81100, lr = 0.01
I0523 08:48:51.698457 34682 solver.cpp:239] Iteration 81110 (2.6937 iter/s, 3.71236s/10 iters), loss = 8.39159
I0523 08:48:51.698501 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39159 (* 1 = 8.39159 loss)
I0523 08:48:51.755664 34682 sgd_solver.cpp:112] Iteration 81110, lr = 0.01
I0523 08:48:55.787180 34682 solver.cpp:239] Iteration 81120 (2.44588 iter/s, 4.08851s/10 iters), loss = 7.43053
I0523 08:48:55.787230 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43053 (* 1 = 7.43053 loss)
I0523 08:48:56.641394 34682 sgd_solver.cpp:112] Iteration 81120, lr = 0.01
I0523 08:49:00.557718 34682 solver.cpp:239] Iteration 81130 (2.09631 iter/s, 4.77029s/10 iters), loss = 8.32792
I0523 08:49:00.557765 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.32792 (* 1 = 8.32792 loss)
I0523 08:49:01.323053 34682 sgd_solver.cpp:112] Iteration 81130, lr = 0.01
I0523 08:49:06.689741 34682 solver.cpp:239] Iteration 81140 (1.63086 iter/s, 6.13173s/10 iters), loss = 6.89419
I0523 08:49:06.689791 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.89419 (* 1 = 6.89419 loss)
I0523 08:49:07.397073 34682 sgd_solver.cpp:112] Iteration 81140, lr = 0.01
I0523 08:49:12.303901 34682 solver.cpp:239] Iteration 81150 (1.7813 iter/s, 5.61388s/10 iters), loss = 7.81476
I0523 08:49:12.303946 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81476 (* 1 = 7.81476 loss)
I0523 08:49:12.387148 34682 sgd_solver.cpp:112] Iteration 81150, lr = 0.01
I0523 08:49:16.385520 34682 solver.cpp:239] Iteration 81160 (2.45014 iter/s, 4.08141s/10 iters), loss = 7.00389
I0523 08:49:16.385701 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.00389 (* 1 = 7.00389 loss)
I0523 08:49:16.437081 34682 sgd_solver.cpp:112] Iteration 81160, lr = 0.01
I0523 08:49:21.396138 34682 solver.cpp:239] Iteration 81170 (1.99591 iter/s, 5.01023s/10 iters), loss = 7.75278
I0523 08:49:21.396198 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75278 (* 1 = 7.75278 loss)
I0523 08:49:21.462785 34682 sgd_solver.cpp:112] Iteration 81170, lr = 0.01
I0523 08:49:26.184893 34682 solver.cpp:239] Iteration 81180 (2.08834 iter/s, 4.7885s/10 iters), loss = 7.56354
I0523 08:49:26.184947 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.56354 (* 1 = 7.56354 loss)
I0523 08:49:26.243261 34682 sgd_solver.cpp:112] Iteration 81180, lr = 0.01
I0523 08:49:29.705637 34682 solver.cpp:239] Iteration 81190 (2.84048 iter/s, 3.52053s/10 iters), loss = 8.33044
I0523 08:49:29.705701 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.33044 (* 1 = 8.33044 loss)
I0523 08:49:29.765662 34682 sgd_solver.cpp:112] Iteration 81190, lr = 0.01
I0523 08:49:33.793241 34682 solver.cpp:239] Iteration 81200 (2.44656 iter/s, 4.08737s/10 iters), loss = 7.82038
I0523 08:49:33.793292 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.82038 (* 1 = 7.82038 loss)
I0523 08:49:33.870625 34682 sgd_solver.cpp:112] Iteration 81200, lr = 0.01
I0523 08:49:37.235383 34682 solver.cpp:239] Iteration 81210 (2.90533 iter/s, 3.44195s/10 iters), loss = 7.13522
I0523 08:49:37.235430 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.13522 (* 1 = 7.13522 loss)
I0523 08:49:37.293874 34682 sgd_solver.cpp:112] Iteration 81210, lr = 0.01
I0523 08:49:43.215104 34682 solver.cpp:239] Iteration 81220 (1.6724 iter/s, 5.97943s/10 iters), loss = 7.69336
I0523 08:49:43.215157 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69336 (* 1 = 7.69336 loss)
I0523 08:49:43.291049 34682 sgd_solver.cpp:112] Iteration 81220, lr = 0.01
I0523 08:49:48.039824 34682 solver.cpp:239] Iteration 81230 (2.07277 iter/s, 4.82447s/10 iters), loss = 8.40166
I0523 08:49:48.040109 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.40166 (* 1 = 8.40166 loss)
I0523 08:49:48.102298 34682 sgd_solver.cpp:112] Iteration 81230, lr = 0.01
I0523 08:49:51.909072 34682 solver.cpp:239] Iteration 81240 (2.58475 iter/s, 3.86885s/10 iters), loss = 8.31414
I0523 08:49:51.909112 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31414 (* 1 = 8.31414 loss)
I0523 08:49:52.668179 34682 sgd_solver.cpp:112] Iteration 81240, lr = 0.01
I0523 08:49:55.718086 34682 solver.cpp:239] Iteration 81250 (2.62549 iter/s, 3.80881s/10 iters), loss = 7.40018
I0523 08:49:55.718127 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.40018 (* 1 = 7.40018 loss)
I0523 08:49:55.797230 34682 sgd_solver.cpp:112] Iteration 81250, lr = 0.01
I0523 08:50:01.795414 34682 solver.cpp:239] Iteration 81260 (1.64554 iter/s, 6.07704s/10 iters), loss = 7.55257
I0523 08:50:01.795462 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.55257 (* 1 = 7.55257 loss)
I0523 08:50:01.872263 34682 sgd_solver.cpp:112] Iteration 81260, lr = 0.01
I0523 08:50:06.424515 34682 solver.cpp:239] Iteration 81270 (2.16036 iter/s, 4.62886s/10 iters), loss = 7.57938
I0523 08:50:06.424566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.57938 (* 1 = 7.57938 loss)
I0523 08:50:06.487565 34682 sgd_solver.cpp:112] Iteration 81270, lr = 0.01
I0523 08:50:09.791756 34682 solver.cpp:239] Iteration 81280 (2.96996 iter/s, 3.36704s/10 iters), loss = 7.76673
I0523 08:50:09.791822 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.76673 (* 1 = 7.76673 loss)
I0523 08:50:09.851294 34682 sgd_solver.cpp:112] Iteration 81280, lr = 0.01
I0523 08:50:15.211642 34682 solver.cpp:239] Iteration 81290 (1.84517 iter/s, 5.41957s/10 iters), loss = 7.90875
I0523 08:50:15.211729 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90875 (* 1 = 7.90875 loss)
I0523 08:50:16.080315 34682 sgd_solver.cpp:112] Iteration 81290, lr = 0.01
I0523 08:50:21.593302 34682 solver.cpp:239] Iteration 81300 (1.56707 iter/s, 6.38132s/10 iters), loss = 6.43388
I0523 08:50:21.593513 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.43388 (* 1 = 6.43388 loss)
I0523 08:50:21.656555 34682 sgd_solver.cpp:112] Iteration 81300, lr = 0.01
I0523 08:50:24.218765 34682 solver.cpp:239] Iteration 81310 (3.80928 iter/s, 2.62517s/10 iters), loss = 7.48187
I0523 08:50:24.218814 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.48187 (* 1 = 7.48187 loss)
I0523 08:50:24.275040 34682 sgd_solver.cpp:112] Iteration 81310, lr = 0.01
I0523 08:50:28.258478 34682 solver.cpp:239] Iteration 81320 (2.47556 iter/s, 4.03949s/10 iters), loss = 7.75
I0523 08:50:28.258543 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75 (* 1 = 7.75 loss)
I0523 08:50:28.867324 34682 sgd_solver.cpp:112] Iteration 81320, lr = 0.01
I0523 08:50:32.636615 34682 solver.cpp:239] Iteration 81330 (2.2842 iter/s, 4.3779s/10 iters), loss = 7.79037
I0523 08:50:32.636665 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.79037 (* 1 = 7.79037 loss)
I0523 08:50:33.228472 34682 sgd_solver.cpp:112] Iteration 81330, lr = 0.01
I0523 08:50:38.475579 34682 solver.cpp:239] Iteration 81340 (1.71272 iter/s, 5.83867s/10 iters), loss = 6.92887
I0523 08:50:38.475623 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.92887 (* 1 = 6.92887 loss)
I0523 08:50:38.551515 34682 sgd_solver.cpp:112] Iteration 81340, lr = 0.01
I0523 08:50:42.080628 34682 solver.cpp:239] Iteration 81350 (2.77403 iter/s, 3.60486s/10 iters), loss = 9.02847
I0523 08:50:42.080675 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.02847 (* 1 = 9.02847 loss)
I0523 08:50:42.142323 34682 sgd_solver.cpp:112] Iteration 81350, lr = 0.01
I0523 08:50:49.256161 34682 solver.cpp:239] Iteration 81360 (1.39369 iter/s, 7.17519s/10 iters), loss = 7.73891
I0523 08:50:49.256217 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.73891 (* 1 = 7.73891 loss)
I0523 08:50:50.006054 34682 sgd_solver.cpp:112] Iteration 81360, lr = 0.01
I0523 08:50:55.198426 34682 solver.cpp:239] Iteration 81370 (1.68295 iter/s, 5.94195s/10 iters), loss = 8.03197
I0523 08:50:55.198740 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.03197 (* 1 = 8.03197 loss)
I0523 08:50:56.060066 34682 sgd_solver.cpp:112] Iteration 81370, lr = 0.01
I0523 08:51:01.043606 34682 solver.cpp:239] Iteration 81380 (1.71095 iter/s, 5.8447s/10 iters), loss = 7.68786
I0523 08:51:01.043658 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68786 (* 1 = 7.68786 loss)
I0523 08:51:01.290673 34682 sgd_solver.cpp:112] Iteration 81380, lr = 0.01
I0523 08:51:07.468415 34682 solver.cpp:239] Iteration 81390 (1.55654 iter/s, 6.4245s/10 iters), loss = 7.33726
I0523 08:51:07.468462 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.33726 (* 1 = 7.33726 loss)
I0523 08:51:07.541138 34682 sgd_solver.cpp:112] Iteration 81390, lr = 0.01
I0523 08:51:12.346241 34682 solver.cpp:239] Iteration 81400 (2.0502 iter/s, 4.87758s/10 iters), loss = 7.87409
I0523 08:51:12.346308 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.87409 (* 1 = 7.87409 loss)
I0523 08:51:12.426023 34682 sgd_solver.cpp:112] Iteration 81400, lr = 0.01
I0523 08:51:17.406306 34682 solver.cpp:239] Iteration 81410 (1.97637 iter/s, 5.05979s/10 iters), loss = 7.5974
I0523 08:51:17.406361 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5974 (* 1 = 7.5974 loss)
I0523 08:51:17.466416 34682 sgd_solver.cpp:112] Iteration 81410, lr = 0.01
I0523 08:51:20.955343 34682 solver.cpp:239] Iteration 81420 (2.81784 iter/s, 3.54882s/10 iters), loss = 7.39195
I0523 08:51:20.955405 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.39195 (* 1 = 7.39195 loss)
I0523 08:51:21.443775 34682 sgd_solver.cpp:112] Iteration 81420, lr = 0.01
I0523 08:51:25.465400 34682 solver.cpp:239] Iteration 81430 (2.21739 iter/s, 4.50981s/10 iters), loss = 7.11482
I0523 08:51:25.465531 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.11482 (* 1 = 7.11482 loss)
I0523 08:51:26.293800 34682 sgd_solver.cpp:112] Iteration 81430, lr = 0.01
I0523 08:51:30.317615 34682 solver.cpp:239] Iteration 81440 (2.06105 iter/s, 4.85189s/10 iters), loss = 7.96345
I0523 08:51:30.317661 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.96345 (* 1 = 7.96345 loss)
I0523 08:51:30.389683 34682 sgd_solver.cpp:112] Iteration 81440, lr = 0.01
I0523 08:51:36.665586 34682 solver.cpp:239] Iteration 81450 (1.57538 iter/s, 6.34767s/10 iters), loss = 7.29565
I0523 08:51:36.665635 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.29565 (* 1 = 7.29565 loss)
I0523 08:51:36.728974 34682 sgd_solver.cpp:112] Iteration 81450, lr = 0.01
I0523 08:51:39.505694 34682 solver.cpp:239] Iteration 81460 (3.5212 iter/s, 2.83994s/10 iters), loss = 8.04973
I0523 08:51:39.505744 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04973 (* 1 = 8.04973 loss)
I0523 08:51:40.357758 34682 sgd_solver.cpp:112] Iteration 81460, lr = 0.01
I0523 08:51:46.673166 34682 solver.cpp:239] Iteration 81470 (1.39526 iter/s, 7.16714s/10 iters), loss = 8.20858
I0523 08:51:46.673213 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20858 (* 1 = 8.20858 loss)
I0523 08:51:46.758347 34682 sgd_solver.cpp:112] Iteration 81470, lr = 0.01
I0523 08:51:52.242244 34682 solver.cpp:239] Iteration 81480 (1.79572 iter/s, 5.56879s/10 iters), loss = 8.1767
I0523 08:51:52.242303 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1767 (* 1 = 8.1767 loss)
I0523 08:51:53.079648 34682 sgd_solver.cpp:112] Iteration 81480, lr = 0.01
I0523 08:51:55.714733 34682 solver.cpp:239] Iteration 81490 (2.87998 iter/s, 3.47224s/10 iters), loss = 8.1866
I0523 08:51:55.714983 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1866 (* 1 = 8.1866 loss)
I0523 08:51:56.541656 34682 sgd_solver.cpp:112] Iteration 81490, lr = 0.01
I0523 08:52:01.832413 34682 solver.cpp:239] Iteration 81500 (1.63474 iter/s, 6.1172s/10 iters), loss = 7.18613
I0523 08:52:01.832486 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.18613 (* 1 = 7.18613 loss)
I0523 08:52:02.668915 34682 sgd_solver.cpp:112] Iteration 81500, lr = 0.01
I0523 08:52:06.945617 34682 solver.cpp:239] Iteration 81510 (1.95583 iter/s, 5.11293s/10 iters), loss = 7.28426
I0523 08:52:06.945663 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.28426 (* 1 = 7.28426 loss)
I0523 08:52:07.009428 34682 sgd_solver.cpp:112] Iteration 81510, lr = 0.01
I0523 08:52:10.893674 34682 solver.cpp:239] Iteration 81520 (2.53303 iter/s, 3.94785s/10 iters), loss = 7.4589
I0523 08:52:10.893715 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.4589 (* 1 = 7.4589 loss)
I0523 08:52:10.960821 34682 sgd_solver.cpp:112] Iteration 81520, lr = 0.01
I0523 08:52:15.613261 34682 solver.cpp:239] Iteration 81530 (2.11893 iter/s, 4.71935s/10 iters), loss = 7.41555
I0523 08:52:15.613307 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.41555 (* 1 = 7.41555 loss)
I0523 08:52:15.693116 34682 sgd_solver.cpp:112] Iteration 81530, lr = 0.01
I0523 08:52:21.888553 34682 solver.cpp:239] Iteration 81540 (1.59363 iter/s, 6.27499s/10 iters), loss = 7.50966
I0523 08:52:21.888597 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50966 (* 1 = 7.50966 loss)
I0523 08:52:22.595043 34682 sgd_solver.cpp:112] Iteration 81540, lr = 0.01
I0523 08:52:27.412034 34682 solver.cpp:239] Iteration 81550 (1.81054 iter/s, 5.52321s/10 iters), loss = 7.05862
I0523 08:52:27.412227 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.05862 (* 1 = 7.05862 loss)
I0523 08:52:27.481400 34682 sgd_solver.cpp:112] Iteration 81550, lr = 0.01
I0523 08:52:32.275632 34682 solver.cpp:239] Iteration 81560 (2.05625 iter/s, 4.86321s/10 iters), loss = 8.02776
I0523 08:52:32.275672 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02776 (* 1 = 8.02776 loss)
I0523 08:52:32.349378 34682 sgd_solver.cpp:112] Iteration 81560, lr = 0.01
I0523 08:52:38.103178 34682 solver.cpp:239] Iteration 81570 (1.71608 iter/s, 5.82724s/10 iters), loss = 7.50388
I0523 08:52:38.103283 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50388 (* 1 = 7.50388 loss)
I0523 08:52:38.585575 34682 sgd_solver.cpp:112] Iteration 81570, lr = 0.01
I0523 08:52:41.842733 34682 solver.cpp:239] Iteration 81580 (2.67431 iter/s, 3.73929s/10 iters), loss = 7.54071
I0523 08:52:41.842779 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54071 (* 1 = 7.54071 loss)
I0523 08:52:42.658823 34682 sgd_solver.cpp:112] Iteration 81580, lr = 0.01
I0523 08:52:48.350536 34682 solver.cpp:239] Iteration 81590 (1.53669 iter/s, 6.50749s/10 iters), loss = 7.37412
I0523 08:52:48.350605 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.37412 (* 1 = 7.37412 loss)
I0523 08:52:49.175441 34682 sgd_solver.cpp:112] Iteration 81590, lr = 0.01
I0523 08:52:54.200312 34682 solver.cpp:239] Iteration 81600 (1.70956 iter/s, 5.84946s/10 iters), loss = 8.29747
I0523 08:52:54.200373 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.29747 (* 1 = 8.29747 loss)
I0523 08:52:54.997617 34682 sgd_solver.cpp:112] Iteration 81600, lr = 0.01
I0523 08:52:58.440310 34682 solver.cpp:239] Iteration 81610 (2.35862 iter/s, 4.23977s/10 iters), loss = 7.1607
I0523 08:52:58.440490 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.1607 (* 1 = 7.1607 loss)
I0523 08:52:58.498097 34682 sgd_solver.cpp:112] Iteration 81610, lr = 0.01
I0523 08:53:01.180730 34682 solver.cpp:239] Iteration 81620 (3.64945 iter/s, 2.74014s/10 iters), loss = 8.38614
I0523 08:53:01.180773 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.38614 (* 1 = 8.38614 loss)
I0523 08:53:01.249080 34682 sgd_solver.cpp:112] Iteration 81620, lr = 0.01
I0523 08:53:04.551165 34682 solver.cpp:239] Iteration 81630 (2.96715 iter/s, 3.37024s/10 iters), loss = 7.58927
I0523 08:53:04.551236 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58927 (* 1 = 7.58927 loss)
I0523 08:53:04.606019 34682 sgd_solver.cpp:112] Iteration 81630, lr = 0.01
I0523 08:53:08.179824 34682 solver.cpp:239] Iteration 81640 (2.75601 iter/s, 3.62844s/10 iters), loss = 7.15366
I0523 08:53:08.179872 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.15366 (* 1 = 7.15366 loss)
I0523 08:53:08.239208 34682 sgd_solver.cpp:112] Iteration 81640, lr = 0.01
I0523 08:53:12.983778 34682 solver.cpp:239] Iteration 81650 (2.08173 iter/s, 4.8037s/10 iters), loss = 7.2165
I0523 08:53:12.983834 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.2165 (* 1 = 7.2165 loss)
I0523 08:53:13.807881 34682 sgd_solver.cpp:112] Iteration 81650, lr = 0.01
I0523 08:53:14.982372 34682 solver.cpp:239] Iteration 81660 (5.00389 iter/s, 1.99845s/10 iters), loss = 7.37433
I0523 08:53:14.982432 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.37433 (* 1 = 7.37433 loss)
I0523 08:53:15.023867 34682 sgd_solver.cpp:112] Iteration 81660, lr = 0.01
I0523 08:53:16.590073 34682 solver.cpp:239] Iteration 81670 (6.22063 iter/s, 1.60756s/10 iters), loss = 8.58231
I0523 08:53:16.590128 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.58231 (* 1 = 8.58231 loss)
I0523 08:53:16.644385 34682 sgd_solver.cpp:112] Iteration 81670, lr = 0.01
I0523 08:53:17.806776 34682 solver.cpp:239] Iteration 81680 (8.21973 iter/s, 1.21659s/10 iters), loss = 6.85648
I0523 08:53:17.806846 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.85648 (* 1 = 6.85648 loss)
I0523 08:53:17.851882 34682 sgd_solver.cpp:112] Iteration 81680, lr = 0.01
I0523 08:53:19.026595 34682 solver.cpp:239] Iteration 81690 (8.19879 iter/s, 1.21969s/10 iters), loss = 7.93362
I0523 08:53:19.026661 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.93362 (* 1 = 7.93362 loss)
I0523 08:53:19.070149 34682 sgd_solver.cpp:112] Iteration 81690, lr = 0.01
I0523 08:53:20.738283 34682 solver.cpp:239] Iteration 81700 (5.84268 iter/s, 1.71154s/10 iters), loss = 7.33854
I0523 08:53:20.738339 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.33854 (* 1 = 7.33854 loss)
I0523 08:53:20.780907 34682 sgd_solver.cpp:112] Iteration 81700, lr = 0.01
I0523 08:53:21.947531 34682 solver.cpp:239] Iteration 81710 (8.27039 iter/s, 1.20913s/10 iters), loss = 8.26752
I0523 08:53:21.947583 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.26752 (* 1 = 8.26752 loss)
I0523 08:53:21.985232 34682 sgd_solver.cpp:112] Iteration 81710, lr = 0.01
I0523 08:53:23.446332 34682 solver.cpp:239] Iteration 81720 (6.67255 iter/s, 1.49868s/10 iters), loss = 7.43889
I0523 08:53:23.446405 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43889 (* 1 = 7.43889 loss)
I0523 08:53:23.497069 34682 sgd_solver.cpp:112] Iteration 81720, lr = 0.01
I0523 08:53:27.104135 34682 solver.cpp:239] Iteration 81730 (2.73405 iter/s, 3.65758s/10 iters), loss = 8.17929
I0523 08:53:27.104182 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17929 (* 1 = 8.17929 loss)
I0523 08:53:27.933892 34682 sgd_solver.cpp:112] Iteration 81730, lr = 0.01
I0523 08:53:30.585350 34682 solver.cpp:239] Iteration 81740 (2.87272 iter/s, 3.48102s/10 iters), loss = 7.81154
I0523 08:53:30.585532 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.81154 (* 1 = 7.81154 loss)
I0523 08:53:31.299772 34682 sgd_solver.cpp:112] Iteration 81740, lr = 0.01
I0523 08:53:34.686185 34682 solver.cpp:239] Iteration 81750 (2.43873 iter/s, 4.10049s/10 iters), loss = 7.395
I0523 08:53:34.686230 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.395 (* 1 = 7.395 loss)
I0523 08:53:35.117305 34682 sgd_solver.cpp:112] Iteration 81750, lr = 0.01
I0523 08:53:39.832207 34682 solver.cpp:239] Iteration 81760 (1.94335 iter/s, 5.14576s/10 iters), loss = 8.14034
I0523 08:53:39.832257 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.14034 (* 1 = 8.14034 loss)
I0523 08:53:39.902586 34682 sgd_solver.cpp:112] Iteration 81760, lr = 0.01
I0523 08:53:44.655418 34682 solver.cpp:239] Iteration 81770 (2.07341 iter/s, 4.82296s/10 iters), loss = 8.41893
I0523 08:53:44.655467 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.41893 (* 1 = 8.41893 loss)
I0523 08:53:44.854612 34682 sgd_solver.cpp:112] Iteration 81770, lr = 0.01
I0523 08:53:49.417981 34682 solver.cpp:239] Iteration 81780 (2.09982 iter/s, 4.76232s/10 iters), loss = 8.24702
I0523 08:53:49.418025 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.24702 (* 1 = 8.24702 loss)
I0523 08:53:49.487387 34682 sgd_solver.cpp:112] Iteration 81780, lr = 0.01
I0523 08:53:54.886405 34682 solver.cpp:239] Iteration 81790 (1.82877 iter/s, 5.46814s/10 iters), loss = 8.92636
I0523 08:53:54.886472 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.92636 (* 1 = 8.92636 loss)
I0523 08:53:55.341078 34682 sgd_solver.cpp:112] Iteration 81790, lr = 0.01
I0523 08:53:59.246019 34682 solver.cpp:239] Iteration 81800 (2.29391 iter/s, 4.35937s/10 iters), loss = 6.532
I0523 08:53:59.246068 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.532 (* 1 = 6.532 loss)
I0523 08:53:59.545076 34682 sgd_solver.cpp:112] Iteration 81800, lr = 0.01
I0523 08:54:02.076117 34682 solver.cpp:239] Iteration 81810 (3.53366 iter/s, 2.82993s/10 iters), loss = 7.90264
I0523 08:54:02.076359 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.90264 (* 1 = 7.90264 loss)
I0523 08:54:02.138962 34682 sgd_solver.cpp:112] Iteration 81810, lr = 0.01
I0523 08:54:08.142182 34682 solver.cpp:239] Iteration 81820 (1.64865 iter/s, 6.06558s/10 iters), loss = 8.23224
I0523 08:54:08.142225 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.23224 (* 1 = 8.23224 loss)
I0523 08:54:08.220861 34682 sgd_solver.cpp:112] Iteration 81820, lr = 0.01
I0523 08:54:13.210737 34682 solver.cpp:239] Iteration 81830 (1.97305 iter/s, 5.0683s/10 iters), loss = 7.94808
I0523 08:54:13.210777 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94808 (* 1 = 7.94808 loss)
I0523 08:54:14.034909 34682 sgd_solver.cpp:112] Iteration 81830, lr = 0.01
I0523 08:54:19.626914 34682 solver.cpp:239] Iteration 81840 (1.55863 iter/s, 6.41588s/10 iters), loss = 7.3025
I0523 08:54:19.626956 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.3025 (* 1 = 7.3025 loss)
I0523 08:54:19.696583 34682 sgd_solver.cpp:112] Iteration 81840, lr = 0.01
I0523 08:54:22.207968 34682 solver.cpp:239] Iteration 81850 (3.87463 iter/s, 2.58089s/10 iters), loss = 7.48154
I0523 08:54:22.208009 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.48154 (* 1 = 7.48154 loss)
I0523 08:54:22.278754 34682 sgd_solver.cpp:112] Iteration 81850, lr = 0.01
I0523 08:54:27.604595 34682 solver.cpp:239] Iteration 81860 (1.8531 iter/s, 5.39637s/10 iters), loss = 8.20818
I0523 08:54:27.604647 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.20818 (* 1 = 8.20818 loss)
I0523 08:54:27.680691 34682 sgd_solver.cpp:112] Iteration 81860, lr = 0.01
I0523 08:54:33.425633 34682 solver.cpp:239] Iteration 81870 (1.71799 iter/s, 5.82076s/10 iters), loss = 8.88999
I0523 08:54:33.425726 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.88999 (* 1 = 8.88999 loss)
I0523 08:54:33.501708 34682 sgd_solver.cpp:112] Iteration 81870, lr = 0.01
I0523 08:54:37.036744 34682 solver.cpp:239] Iteration 81880 (2.76942 iter/s, 3.61086s/10 iters), loss = 7.54552
I0523 08:54:37.036803 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.54552 (* 1 = 7.54552 loss)
I0523 08:54:37.817891 34682 sgd_solver.cpp:112] Iteration 81880, lr = 0.01
I0523 08:54:41.564198 34682 solver.cpp:239] Iteration 81890 (2.20887 iter/s, 4.52721s/10 iters), loss = 8.46848
I0523 08:54:41.564242 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.46848 (* 1 = 8.46848 loss)
I0523 08:54:41.638986 34682 sgd_solver.cpp:112] Iteration 81890, lr = 0.01
I0523 08:54:47.344143 34682 solver.cpp:239] Iteration 81900 (1.73021 iter/s, 5.77966s/10 iters), loss = 7.42498
I0523 08:54:47.344210 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42498 (* 1 = 7.42498 loss)
I0523 08:54:47.407738 34682 sgd_solver.cpp:112] Iteration 81900, lr = 0.01
I0523 08:54:51.567454 34682 solver.cpp:239] Iteration 81910 (2.36795 iter/s, 4.22307s/10 iters), loss = 7.1816
I0523 08:54:51.567517 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.1816 (* 1 = 7.1816 loss)
I0523 08:54:52.007586 34682 sgd_solver.cpp:112] Iteration 81910, lr = 0.01
I0523 08:54:55.354327 34682 solver.cpp:239] Iteration 81920 (2.64085 iter/s, 3.78666s/10 iters), loss = 7.28744
I0523 08:54:55.354372 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.28744 (* 1 = 7.28744 loss)
I0523 08:54:56.154373 34682 sgd_solver.cpp:112] Iteration 81920, lr = 0.01
I0523 08:54:59.523438 34682 solver.cpp:239] Iteration 81930 (2.39873 iter/s, 4.16887s/10 iters), loss = 7.53147
I0523 08:54:59.523546 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.53147 (* 1 = 7.53147 loss)
I0523 08:55:00.373060 34682 sgd_solver.cpp:112] Iteration 81930, lr = 0.01
I0523 08:55:06.839013 34682 solver.cpp:239] Iteration 81940 (1.36702 iter/s, 7.31519s/10 iters), loss = 7.41991
I0523 08:55:06.839196 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.41991 (* 1 = 7.41991 loss)
I0523 08:55:07.705729 34682 sgd_solver.cpp:112] Iteration 81940, lr = 0.01
I0523 08:55:13.050372 34682 solver.cpp:239] Iteration 81950 (1.61007 iter/s, 6.21092s/10 iters), loss = 8.13352
I0523 08:55:13.050429 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.13352 (* 1 = 8.13352 loss)
I0523 08:55:13.880395 34682 sgd_solver.cpp:112] Iteration 81950, lr = 0.01
I0523 08:55:17.263980 34682 solver.cpp:239] Iteration 81960 (2.37339 iter/s, 4.21338s/10 iters), loss = 8.0766
I0523 08:55:17.264029 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0766 (* 1 = 8.0766 loss)
I0523 08:55:17.944444 34682 sgd_solver.cpp:112] Iteration 81960, lr = 0.01
I0523 08:55:25.032519 34682 solver.cpp:239] Iteration 81970 (1.28731 iter/s, 7.76816s/10 iters), loss = 8.31406
I0523 08:55:25.032577 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31406 (* 1 = 8.31406 loss)
I0523 08:55:25.854985 34682 sgd_solver.cpp:112] Iteration 81970, lr = 0.01
I0523 08:55:31.583724 34682 solver.cpp:239] Iteration 81980 (1.52652 iter/s, 6.55086s/10 iters), loss = 7.3131
I0523 08:55:31.583771 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.3131 (* 1 = 7.3131 loss)
I0523 08:55:32.359642 34682 sgd_solver.cpp:112] Iteration 81980, lr = 0.01
I0523 08:55:37.685721 34682 solver.cpp:239] Iteration 81990 (1.63889 iter/s, 6.1017s/10 iters), loss = 8.3629
I0523 08:55:37.685951 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3629 (* 1 = 8.3629 loss)
I0523 08:55:38.307804 34682 sgd_solver.cpp:112] Iteration 81990, lr = 0.01
I0523 08:55:44.532454 34682 solver.cpp:239] Iteration 82000 (1.46066 iter/s, 6.84622s/10 iters), loss = 7.42032
I0523 08:55:44.532568 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42032 (* 1 = 7.42032 loss)
I0523 08:55:45.368688 34682 sgd_solver.cpp:112] Iteration 82000, lr = 0.01
I0523 08:55:50.740433 34682 solver.cpp:239] Iteration 82010 (1.61092 iter/s, 6.20762s/10 iters), loss = 8.1103
I0523 08:55:50.740500 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.1103 (* 1 = 8.1103 loss)
I0523 08:55:50.821951 34682 sgd_solver.cpp:112] Iteration 82010, lr = 0.01
I0523 08:55:57.118422 34682 solver.cpp:239] Iteration 82020 (1.56797 iter/s, 6.37767s/10 iters), loss = 8.04784
I0523 08:55:57.118481 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.04784 (* 1 = 8.04784 loss)
I0523 08:55:57.196444 34682 sgd_solver.cpp:112] Iteration 82020, lr = 0.01
I0523 08:56:02.997143 34682 solver.cpp:239] Iteration 82030 (1.70114 iter/s, 5.87843s/10 iters), loss = 7.38992
I0523 08:56:02.997196 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.38992 (* 1 = 7.38992 loss)
I0523 08:56:03.065373 34682 sgd_solver.cpp:112] Iteration 82030, lr = 0.01
I0523 08:56:09.609863 34682 solver.cpp:239] Iteration 82040 (1.51231 iter/s, 6.6124s/10 iters), loss = 7.17309
I0523 08:56:09.610101 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.17309 (* 1 = 7.17309 loss)
I0523 08:56:10.464236 34682 sgd_solver.cpp:112] Iteration 82040, lr = 0.01
I0523 08:56:15.027320 34682 solver.cpp:239] Iteration 82050 (1.84603 iter/s, 5.41703s/10 iters), loss = 7.80257
I0523 08:56:15.027390 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.80257 (* 1 = 7.80257 loss)
I0523 08:56:15.100095 34682 sgd_solver.cpp:112] Iteration 82050, lr = 0.01
I0523 08:56:21.196779 34682 solver.cpp:239] Iteration 82060 (1.62097 iter/s, 6.16915s/10 iters), loss = 8.64793
I0523 08:56:21.196825 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.64793 (* 1 = 8.64793 loss)
I0523 08:56:21.265630 34682 sgd_solver.cpp:112] Iteration 82060, lr = 0.01
I0523 08:56:25.304488 34682 solver.cpp:239] Iteration 82070 (2.43458 iter/s, 4.10749s/10 iters), loss = 6.79262
I0523 08:56:25.304546 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.79262 (* 1 = 6.79262 loss)
I0523 08:56:25.379248 34682 sgd_solver.cpp:112] Iteration 82070, lr = 0.01
I0523 08:56:29.941594 34682 solver.cpp:239] Iteration 82080 (2.15663 iter/s, 4.63686s/10 iters), loss = 6.19273
I0523 08:56:29.941644 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.19273 (* 1 = 6.19273 loss)
I0523 08:56:30.021909 34682 sgd_solver.cpp:112] Iteration 82080, lr = 0.01
I0523 08:56:35.646536 34682 solver.cpp:239] Iteration 82090 (1.75295 iter/s, 5.70466s/10 iters), loss = 7.43872
I0523 08:56:35.646584 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.43872 (* 1 = 7.43872 loss)
I0523 08:56:36.511987 34682 sgd_solver.cpp:112] Iteration 82090, lr = 0.01
I0523 08:56:41.609298 34682 solver.cpp:239] Iteration 82100 (1.67716 iter/s, 5.96247s/10 iters), loss = 6.87232
I0523 08:56:41.609566 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.87232 (* 1 = 6.87232 loss)
I0523 08:56:42.462908 34682 sgd_solver.cpp:112] Iteration 82100, lr = 0.01
I0523 08:56:47.383571 34682 solver.cpp:239] Iteration 82110 (1.73196 iter/s, 5.7738s/10 iters), loss = 9.03245
I0523 08:56:47.383622 34682 solver.cpp:258]     Train net output #0: softmax_loss = 9.03245 (* 1 = 9.03245 loss)
I0523 08:56:47.441359 34682 sgd_solver.cpp:112] Iteration 82110, lr = 0.01
I0523 08:56:53.791573 34682 solver.cpp:239] Iteration 82120 (1.56063 iter/s, 6.40768s/10 iters), loss = 8.22511
I0523 08:56:53.791631 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.22511 (* 1 = 8.22511 loss)
I0523 08:56:54.629601 34682 sgd_solver.cpp:112] Iteration 82120, lr = 0.01
I0523 08:56:59.614130 34682 solver.cpp:239] Iteration 82130 (1.71755 iter/s, 5.82226s/10 iters), loss = 7.08224
I0523 08:56:59.614207 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.08224 (* 1 = 7.08224 loss)
I0523 08:56:59.681879 34682 sgd_solver.cpp:112] Iteration 82130, lr = 0.01
I0523 08:57:02.916645 34682 solver.cpp:239] Iteration 82140 (3.02826 iter/s, 3.30223s/10 iters), loss = 7.94002
I0523 08:57:02.916764 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.94002 (* 1 = 7.94002 loss)
I0523 08:57:02.983963 34682 sgd_solver.cpp:112] Iteration 82140, lr = 0.01
I0523 08:57:08.237093 34682 solver.cpp:239] Iteration 82150 (1.87965 iter/s, 5.32013s/10 iters), loss = 7.2112
I0523 08:57:08.237155 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.2112 (* 1 = 7.2112 loss)
I0523 08:57:09.036975 34682 sgd_solver.cpp:112] Iteration 82150, lr = 0.01
I0523 08:57:14.363730 34682 solver.cpp:239] Iteration 82160 (1.6323 iter/s, 6.12633s/10 iters), loss = 7.50223
I0523 08:57:14.363847 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.50223 (* 1 = 7.50223 loss)
I0523 08:57:14.432265 34682 sgd_solver.cpp:112] Iteration 82160, lr = 0.01
I0523 08:57:17.868192 34682 solver.cpp:239] Iteration 82170 (2.85373 iter/s, 3.50419s/10 iters), loss = 7.37939
I0523 08:57:17.868253 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.37939 (* 1 = 7.37939 loss)
I0523 08:57:17.936015 34682 sgd_solver.cpp:112] Iteration 82170, lr = 0.01
I0523 08:57:19.744205 34682 solver.cpp:239] Iteration 82180 (5.33085 iter/s, 1.87587s/10 iters), loss = 6.64968
I0523 08:57:19.744254 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.64968 (* 1 = 6.64968 loss)
I0523 08:57:19.799962 34682 sgd_solver.cpp:112] Iteration 82180, lr = 0.01
I0523 08:57:25.513186 34682 solver.cpp:239] Iteration 82190 (1.73349 iter/s, 5.76869s/10 iters), loss = 8.39844
I0523 08:57:25.513238 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.39844 (* 1 = 8.39844 loss)
I0523 08:57:25.590801 34682 sgd_solver.cpp:112] Iteration 82190, lr = 0.01
I0523 08:57:29.651005 34682 solver.cpp:239] Iteration 82200 (2.41686 iter/s, 4.1376s/10 iters), loss = 7.01038
I0523 08:57:29.651049 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.01038 (* 1 = 7.01038 loss)
I0523 08:57:30.357619 34682 sgd_solver.cpp:112] Iteration 82200, lr = 0.01
I0523 08:57:34.582427 34682 solver.cpp:239] Iteration 82210 (2.02792 iter/s, 4.93117s/10 iters), loss = 7.32997
I0523 08:57:34.582484 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.32997 (* 1 = 7.32997 loss)
I0523 08:57:35.223781 34682 sgd_solver.cpp:112] Iteration 82210, lr = 0.01
I0523 08:57:39.172847 34682 solver.cpp:239] Iteration 82220 (2.17857 iter/s, 4.59016s/10 iters), loss = 8.51561
I0523 08:57:39.172920 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51561 (* 1 = 8.51561 loss)
I0523 08:57:39.238056 34682 sgd_solver.cpp:112] Iteration 82220, lr = 0.01
I0523 08:57:43.876590 34682 solver.cpp:239] Iteration 82230 (2.12608 iter/s, 4.70348s/10 iters), loss = 8.34496
I0523 08:57:43.876641 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34496 (* 1 = 8.34496 loss)
I0523 08:57:43.940054 34682 sgd_solver.cpp:112] Iteration 82230, lr = 0.01
I0523 08:57:48.278178 34682 solver.cpp:239] Iteration 82240 (2.27203 iter/s, 4.40136s/10 iters), loss = 8.37597
I0523 08:57:48.278367 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.37597 (* 1 = 8.37597 loss)
I0523 08:57:48.358774 34682 sgd_solver.cpp:112] Iteration 82240, lr = 0.01
I0523 08:57:52.481863 34682 solver.cpp:239] Iteration 82250 (2.37907 iter/s, 4.20332s/10 iters), loss = 8.3484
I0523 08:57:52.481905 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.3484 (* 1 = 8.3484 loss)
I0523 08:57:53.354300 34682 sgd_solver.cpp:112] Iteration 82250, lr = 0.01
I0523 08:57:59.575513 34682 solver.cpp:239] Iteration 82260 (1.40978 iter/s, 7.09333s/10 iters), loss = 8.16876
I0523 08:57:59.575563 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.16876 (* 1 = 8.16876 loss)
I0523 08:57:59.638471 34682 sgd_solver.cpp:112] Iteration 82260, lr = 0.01
I0523 08:58:04.239398 34682 solver.cpp:239] Iteration 82270 (2.14425 iter/s, 4.66364s/10 iters), loss = 7.12136
I0523 08:58:04.239440 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.12136 (* 1 = 7.12136 loss)
I0523 08:58:04.300755 34682 sgd_solver.cpp:112] Iteration 82270, lr = 0.01
I0523 08:58:08.655195 34682 solver.cpp:239] Iteration 82280 (2.26472 iter/s, 4.41556s/10 iters), loss = 8.50965
I0523 08:58:08.655256 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.50965 (* 1 = 8.50965 loss)
I0523 08:58:09.450032 34682 sgd_solver.cpp:112] Iteration 82280, lr = 0.01
I0523 08:58:14.188258 34682 solver.cpp:239] Iteration 82290 (1.80741 iter/s, 5.53278s/10 iters), loss = 8.79355
I0523 08:58:14.188313 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.79355 (* 1 = 8.79355 loss)
I0523 08:58:14.262619 34682 sgd_solver.cpp:112] Iteration 82290, lr = 0.01
I0523 08:58:18.943744 34682 solver.cpp:239] Iteration 82300 (2.10295 iter/s, 4.75523s/10 iters), loss = 7.42959
I0523 08:58:18.944162 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.42959 (* 1 = 7.42959 loss)
I0523 08:58:19.511654 34682 sgd_solver.cpp:112] Iteration 82300, lr = 0.01
I0523 08:58:23.754889 34682 solver.cpp:239] Iteration 82310 (2.07872 iter/s, 4.81064s/10 iters), loss = 8.31135
I0523 08:58:23.754936 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.31135 (* 1 = 8.31135 loss)
I0523 08:58:23.821039 34682 sgd_solver.cpp:112] Iteration 82310, lr = 0.01
I0523 08:58:27.544939 34682 solver.cpp:239] Iteration 82320 (2.63863 iter/s, 3.78984s/10 iters), loss = 6.85696
I0523 08:58:27.544994 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.85696 (* 1 = 6.85696 loss)
I0523 08:58:27.601861 34682 sgd_solver.cpp:112] Iteration 82320, lr = 0.01
I0523 08:58:32.780522 34682 solver.cpp:239] Iteration 82330 (1.91011 iter/s, 5.23531s/10 iters), loss = 7.28012
I0523 08:58:32.780580 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.28012 (* 1 = 7.28012 loss)
I0523 08:58:32.850355 34682 sgd_solver.cpp:112] Iteration 82330, lr = 0.01
I0523 08:58:39.214350 34682 solver.cpp:239] Iteration 82340 (1.55436 iter/s, 6.43352s/10 iters), loss = 8.17669
I0523 08:58:39.214406 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.17669 (* 1 = 8.17669 loss)
I0523 08:58:39.952718 34682 sgd_solver.cpp:112] Iteration 82340, lr = 0.01
I0523 08:58:45.124776 34682 solver.cpp:239] Iteration 82350 (1.69201 iter/s, 5.91012s/10 iters), loss = 7.69848
I0523 08:58:45.124866 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.69848 (* 1 = 7.69848 loss)
I0523 08:58:45.187752 34682 sgd_solver.cpp:112] Iteration 82350, lr = 0.01
I0523 08:58:49.591644 34682 solver.cpp:239] Iteration 82360 (2.23885 iter/s, 4.46659s/10 iters), loss = 7.03539
I0523 08:58:49.591925 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.03539 (* 1 = 7.03539 loss)
I0523 08:58:49.666115 34682 sgd_solver.cpp:112] Iteration 82360, lr = 0.01
I0523 08:58:53.837007 34682 solver.cpp:239] Iteration 82370 (2.35709 iter/s, 4.24252s/10 iters), loss = 7.10431
I0523 08:58:53.837070 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.10431 (* 1 = 7.10431 loss)
I0523 08:58:53.904050 34682 sgd_solver.cpp:112] Iteration 82370, lr = 0.01
I0523 08:58:57.077247 34682 solver.cpp:239] Iteration 82380 (3.08638 iter/s, 3.24004s/10 iters), loss = 7.20188
I0523 08:58:57.077291 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.20188 (* 1 = 7.20188 loss)
I0523 08:58:57.140668 34682 sgd_solver.cpp:112] Iteration 82380, lr = 0.01
I0523 08:59:02.736973 34682 solver.cpp:239] Iteration 82390 (1.76696 iter/s, 5.65945s/10 iters), loss = 7.83615
I0523 08:59:02.737025 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.83615 (* 1 = 7.83615 loss)
I0523 08:59:02.806807 34682 sgd_solver.cpp:112] Iteration 82390, lr = 0.01
I0523 08:59:07.563885 34682 solver.cpp:239] Iteration 82400 (2.07183 iter/s, 4.82666s/10 iters), loss = 7.5251
I0523 08:59:07.563954 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.5251 (* 1 = 7.5251 loss)
I0523 08:59:07.705554 34682 sgd_solver.cpp:112] Iteration 82400, lr = 0.01
I0523 08:59:11.841228 34682 solver.cpp:239] Iteration 82410 (2.33804 iter/s, 4.27709s/10 iters), loss = 7.44668
I0523 08:59:11.841290 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44668 (* 1 = 7.44668 loss)
I0523 08:59:11.918315 34682 sgd_solver.cpp:112] Iteration 82410, lr = 0.01
I0523 08:59:16.561807 34682 solver.cpp:239] Iteration 82420 (2.1185 iter/s, 4.72032s/10 iters), loss = 6.76698
I0523 08:59:16.561877 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.76698 (* 1 = 6.76698 loss)
I0523 08:59:17.411264 34682 sgd_solver.cpp:112] Iteration 82420, lr = 0.01
I0523 08:59:21.574982 34682 solver.cpp:239] Iteration 82430 (1.99485 iter/s, 5.0129s/10 iters), loss = 7.05752
I0523 08:59:21.575129 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.05752 (* 1 = 7.05752 loss)
I0523 08:59:22.320183 34682 sgd_solver.cpp:112] Iteration 82430, lr = 0.01
I0523 08:59:27.210289 34682 solver.cpp:239] Iteration 82440 (1.77465 iter/s, 5.63492s/10 iters), loss = 7.26727
I0523 08:59:27.210350 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.26727 (* 1 = 7.26727 loss)
I0523 08:59:27.271134 34682 sgd_solver.cpp:112] Iteration 82440, lr = 0.01
I0523 08:59:34.226037 34682 solver.cpp:239] Iteration 82450 (1.42543 iter/s, 7.01541s/10 iters), loss = 7.72579
I0523 08:59:34.226073 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72579 (* 1 = 7.72579 loss)
I0523 08:59:34.300107 34682 sgd_solver.cpp:112] Iteration 82450, lr = 0.01
I0523 08:59:39.320240 34682 solver.cpp:239] Iteration 82460 (1.96312 iter/s, 5.09393s/10 iters), loss = 7.72999
I0523 08:59:39.320331 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.72999 (* 1 = 7.72999 loss)
I0523 08:59:39.922781 34682 sgd_solver.cpp:112] Iteration 82460, lr = 0.01
I0523 08:59:45.093919 34682 solver.cpp:239] Iteration 82470 (1.73209 iter/s, 5.77336s/10 iters), loss = 7.25782
I0523 08:59:45.093961 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.25782 (* 1 = 7.25782 loss)
I0523 08:59:45.167754 34682 sgd_solver.cpp:112] Iteration 82470, lr = 0.01
I0523 08:59:49.827527 34682 solver.cpp:239] Iteration 82480 (2.11266 iter/s, 4.73336s/10 iters), loss = 7.22508
I0523 08:59:49.827595 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.22508 (* 1 = 7.22508 loss)
I0523 08:59:49.896371 34682 sgd_solver.cpp:112] Iteration 82480, lr = 0.01
I0523 08:59:53.164909 34682 solver.cpp:239] Iteration 82490 (2.99656 iter/s, 3.33716s/10 iters), loss = 7.59142
I0523 08:59:53.165254 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.59142 (* 1 = 7.59142 loss)
I0523 08:59:53.764748 34682 sgd_solver.cpp:112] Iteration 82490, lr = 0.01
I0523 08:59:57.347877 34682 solver.cpp:239] Iteration 82500 (2.39092 iter/s, 4.18249s/10 iters), loss = 7.27546
I0523 08:59:57.347920 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.27546 (* 1 = 7.27546 loss)
I0523 08:59:57.418367 34682 sgd_solver.cpp:112] Iteration 82500, lr = 0.01
I0523 09:00:01.518607 34682 solver.cpp:239] Iteration 82510 (2.39778 iter/s, 4.17052s/10 iters), loss = 8.02554
I0523 09:00:01.518654 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.02554 (* 1 = 8.02554 loss)
I0523 09:00:01.593581 34682 sgd_solver.cpp:112] Iteration 82510, lr = 0.01
I0523 09:00:05.436480 34682 solver.cpp:239] Iteration 82520 (2.55254 iter/s, 3.91766s/10 iters), loss = 7.75063
I0523 09:00:05.436525 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75063 (* 1 = 7.75063 loss)
I0523 09:00:05.516724 34682 sgd_solver.cpp:112] Iteration 82520, lr = 0.01
I0523 09:00:11.113139 34682 solver.cpp:239] Iteration 82530 (1.76168 iter/s, 5.67639s/10 iters), loss = 7.33014
I0523 09:00:11.113175 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.33014 (* 1 = 7.33014 loss)
I0523 09:00:11.182762 34682 sgd_solver.cpp:112] Iteration 82530, lr = 0.01
I0523 09:00:14.556653 34682 solver.cpp:239] Iteration 82540 (2.90417 iter/s, 3.44333s/10 iters), loss = 7.27971
I0523 09:00:14.556696 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.27971 (* 1 = 7.27971 loss)
I0523 09:00:14.618275 34682 sgd_solver.cpp:112] Iteration 82540, lr = 0.01
I0523 09:00:18.288235 34682 solver.cpp:239] Iteration 82550 (2.67997 iter/s, 3.73138s/10 iters), loss = 7.9357
I0523 09:00:18.288281 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.9357 (* 1 = 7.9357 loss)
I0523 09:00:19.146775 34682 sgd_solver.cpp:112] Iteration 82550, lr = 0.01
I0523 09:00:23.993114 34682 solver.cpp:239] Iteration 82560 (1.75297 iter/s, 5.7046s/10 iters), loss = 7.58127
I0523 09:00:23.993346 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.58127 (* 1 = 7.58127 loss)
I0523 09:00:24.833259 34682 sgd_solver.cpp:112] Iteration 82560, lr = 0.01
I0523 09:00:29.699774 34682 solver.cpp:239] Iteration 82570 (1.75248 iter/s, 5.7062s/10 iters), loss = 8.00249
I0523 09:00:29.699873 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.00249 (* 1 = 8.00249 loss)
I0523 09:00:30.482745 34682 sgd_solver.cpp:112] Iteration 82570, lr = 0.01
I0523 09:00:35.970942 34682 solver.cpp:239] Iteration 82580 (1.59469 iter/s, 6.27082s/10 iters), loss = 6.87774
I0523 09:00:35.970999 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.87774 (* 1 = 6.87774 loss)
I0523 09:00:36.785481 34682 sgd_solver.cpp:112] Iteration 82580, lr = 0.01
I0523 09:00:42.028321 34682 solver.cpp:239] Iteration 82590 (1.65096 iter/s, 6.05708s/10 iters), loss = 7.44584
I0523 09:00:42.028383 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.44584 (* 1 = 7.44584 loss)
I0523 09:00:42.108981 34682 sgd_solver.cpp:112] Iteration 82590, lr = 0.01
I0523 09:00:47.499759 34682 solver.cpp:239] Iteration 82600 (1.82777 iter/s, 5.47116s/10 iters), loss = 7.75694
I0523 09:00:47.499809 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.75694 (* 1 = 7.75694 loss)
I0523 09:00:47.578894 34682 sgd_solver.cpp:112] Iteration 82600, lr = 0.01
I0523 09:00:53.185019 34682 solver.cpp:239] Iteration 82610 (1.75902 iter/s, 5.68498s/10 iters), loss = 8.07467
I0523 09:00:53.185062 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.07467 (* 1 = 8.07467 loss)
I0523 09:00:53.262786 34682 sgd_solver.cpp:112] Iteration 82610, lr = 0.01
I0523 09:00:57.382344 34682 solver.cpp:239] Iteration 82620 (2.38259 iter/s, 4.19711s/10 iters), loss = 8.51663
I0523 09:00:57.382632 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.51663 (* 1 = 8.51663 loss)
I0523 09:00:57.447811 34682 sgd_solver.cpp:112] Iteration 82620, lr = 0.01
I0523 09:01:03.250494 34682 solver.cpp:239] Iteration 82630 (1.70426 iter/s, 5.86763s/10 iters), loss = 8.77076
I0523 09:01:03.250586 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.77076 (* 1 = 8.77076 loss)
I0523 09:01:03.321225 34682 sgd_solver.cpp:112] Iteration 82630, lr = 0.01
I0523 09:01:06.744448 34682 solver.cpp:239] Iteration 82640 (2.86228 iter/s, 3.49372s/10 iters), loss = 7.13477
I0523 09:01:06.744496 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.13477 (* 1 = 7.13477 loss)
I0523 09:01:06.808250 34682 sgd_solver.cpp:112] Iteration 82640, lr = 0.01
I0523 09:01:10.062829 34682 solver.cpp:239] Iteration 82650 (3.01369 iter/s, 3.31819s/10 iters), loss = 8.25491
I0523 09:01:10.062883 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.25491 (* 1 = 8.25491 loss)
I0523 09:01:10.929471 34682 sgd_solver.cpp:112] Iteration 82650, lr = 0.01
I0523 09:01:17.325696 34682 solver.cpp:239] Iteration 82660 (1.37693 iter/s, 7.26252s/10 iters), loss = 7.85283
I0523 09:01:17.325763 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.85283 (* 1 = 7.85283 loss)
I0523 09:01:18.140120 34682 sgd_solver.cpp:112] Iteration 82660, lr = 0.01
I0523 09:01:23.101933 34682 solver.cpp:239] Iteration 82670 (1.73132 iter/s, 5.77593s/10 iters), loss = 7.14918
I0523 09:01:23.101979 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.14918 (* 1 = 7.14918 loss)
I0523 09:01:23.159423 34682 sgd_solver.cpp:112] Iteration 82670, lr = 0.01
I0523 09:01:27.571565 34682 solver.cpp:239] Iteration 82680 (2.23744 iter/s, 4.4694s/10 iters), loss = 7.68625
I0523 09:01:27.571792 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68625 (* 1 = 7.68625 loss)
I0523 09:01:27.641676 34682 sgd_solver.cpp:112] Iteration 82680, lr = 0.01
I0523 09:01:31.319082 34682 solver.cpp:239] Iteration 82690 (2.66869 iter/s, 3.74716s/10 iters), loss = 7.17372
I0523 09:01:31.319124 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.17372 (* 1 = 7.17372 loss)
I0523 09:01:31.395709 34682 sgd_solver.cpp:112] Iteration 82690, lr = 0.01
I0523 09:01:35.373821 34682 solver.cpp:239] Iteration 82700 (2.46638 iter/s, 4.05453s/10 iters), loss = 8.56035
I0523 09:01:35.373877 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.56035 (* 1 = 8.56035 loss)
I0523 09:01:35.459187 34682 sgd_solver.cpp:112] Iteration 82700, lr = 0.01
I0523 09:01:39.242985 34682 solver.cpp:239] Iteration 82710 (2.58469 iter/s, 3.86894s/10 iters), loss = 8.34995
I0523 09:01:39.243026 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.34995 (* 1 = 8.34995 loss)
I0523 09:01:39.305801 34682 sgd_solver.cpp:112] Iteration 82710, lr = 0.01
I0523 09:01:42.606858 34682 solver.cpp:239] Iteration 82720 (2.97292 iter/s, 3.36369s/10 iters), loss = 8.0527
I0523 09:01:42.606907 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.0527 (* 1 = 8.0527 loss)
I0523 09:01:43.365340 34682 sgd_solver.cpp:112] Iteration 82720, lr = 0.01
I0523 09:01:46.847460 34682 solver.cpp:239] Iteration 82730 (2.35828 iter/s, 4.24037s/10 iters), loss = 8.27743
I0523 09:01:46.847525 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.27743 (* 1 = 8.27743 loss)
I0523 09:01:46.920197 34682 sgd_solver.cpp:112] Iteration 82730, lr = 0.01
I0523 09:01:50.295518 34682 solver.cpp:239] Iteration 82740 (2.90035 iter/s, 3.44786s/10 iters), loss = 7.68623
I0523 09:01:50.295565 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.68623 (* 1 = 7.68623 loss)
I0523 09:01:50.362912 34682 sgd_solver.cpp:112] Iteration 82740, lr = 0.01
I0523 09:01:56.782240 34682 solver.cpp:239] Iteration 82750 (1.54169 iter/s, 6.4864s/10 iters), loss = 8.86064
I0523 09:01:56.782312 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.86064 (* 1 = 8.86064 loss)
I0523 09:01:57.214471 34682 sgd_solver.cpp:112] Iteration 82750, lr = 0.01
I0523 09:02:03.061028 34682 solver.cpp:239] Iteration 82760 (1.59275 iter/s, 6.27845s/10 iters), loss = 8.05566
I0523 09:02:03.061353 34682 solver.cpp:258]     Train net output #0: softmax_loss = 8.05566 (* 1 = 8.05566 loss)
I0523 09:02:03.905714 34682 sgd_solver.cpp:112] Iteration 82760, lr = 0.01
I0523 09:02:08.116318 34682 solver.cpp:239] Iteration 82770 (1.97832 iter/s, 5.05479s/10 iters), loss = 7.84533
I0523 09:02:08.116379 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.84533 (* 1 = 7.84533 loss)
I0523 09:02:08.184125 34682 sgd_solver.cpp:112] Iteration 82770, lr = 0.01
I0523 09:02:12.615913 34682 solver.cpp:239] Iteration 82780 (2.22254 iter/s, 4.49935s/10 iters), loss = 6.73844
I0523 09:02:12.615958 34682 solver.cpp:258]     Train net output #0: softmax_loss = 6.73844 (* 1 = 6.73844 loss)
I0523 09:02:12.677583 34682 sgd_solver.cpp:112] Iteration 82780, lr = 0.01
I0523 09:02:16.051475 34682 solver.cpp:239] Iteration 82790 (2.9109 iter/s, 3.43537s/10 iters), loss = 7.15325
I0523 09:02:16.051528 34682 solver.cpp:258]     Train net output #0: softmax_loss = 7.15325 (* 1 = 7.15325 loss)
I0523 09:02:16.110033 34682 sgd_solver.cpp:112] Iteration 82790, lr = 0.01
*** Aborted at 1527037340 (unix time) try "date -d @1527037340" if you are using GNU date ***
PC: @     0x7f798f777f1c __lll_lock_wait
*** SIGTERM (@0x3e90000441d) received by PID 34682 (TID 0x7f79a0d1b780) from PID 17437; stack trace: ***
    @     0x7f799e8cdcb0 (unknown)
    @     0x7f798f777f1c __lll_lock_wait
    @     0x7f798f773664 _L_lock_952
    @     0x7f798f7734c6 __GI___pthread_mutex_lock
    @     0x7f796805cff6 (unknown)
    @     0x7f796819238b (unknown)
    @     0x7f7992cf4d04 (unknown)
    @     0x7f7992cf5805 (unknown)
    @     0x7f7992ceb360 (unknown)
    @     0x7f7992d05e6d (unknown)
    @     0x7f79929b8cbb (unknown)
    @     0x7f79929bd777 (unknown)
    @     0x7f79929b9c7a (unknown)
    @     0x7f7992977d3f (unknown)
    @     0x7f799297858e (unknown)
    @     0x7f799290c7c9 (unknown)
    @     0x7f799ffc9cf7 caffe::CuDNNConvolutionLayer<>::Forward_gpu()
    @     0x7f799ff6a1d3 caffe::Net<>::ForwardFromTo()
    @     0x7f799ff6a587 caffe::Net<>::Forward()
    @     0x7f799fe0fcc7 caffe::Solver<>::Step()
    @     0x7f799fe104bf caffe::Solver<>::Solve()
    @     0x7f799ff4c4e6 caffe::NCCL<>::Run()
    @           0x40c1e3 train()
    @           0x40982c main
    @     0x7f799e8b8f45 (unknown)
    @           0x40a111 (unknown)
