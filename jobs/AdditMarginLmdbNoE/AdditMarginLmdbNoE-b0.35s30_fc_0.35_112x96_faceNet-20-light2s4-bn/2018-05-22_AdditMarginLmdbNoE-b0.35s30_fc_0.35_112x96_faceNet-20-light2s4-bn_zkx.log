./build/tools/caffe: /home/zkx/anaconda2/lib/libtiff.so.5: no version information available (required by /home/zkx/env/opencv/lib/libopencv_highgui.so.2.4)
I0522 21:41:39.513461 26558 upgrade_proto.cpp:1084] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': models/AdditMarginLmdbNoE/AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/solver.prototxt
I0522 21:41:39.513795 26558 upgrade_proto.cpp:1091] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W0522 21:41:39.513808 26558 upgrade_proto.cpp:1093] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I0522 21:41:39.513972 26558 caffe.cpp:204] Using GPUs 0, 1, 2, 3
I0522 21:41:39.540912 26558 caffe.cpp:209] GPU 0: TITAN Xp
I0522 21:41:39.541595 26558 caffe.cpp:209] GPU 1: TITAN Xp
I0522 21:41:39.542268 26558 caffe.cpp:209] GPU 2: TITAN Xp
I0522 21:41:39.542935 26558 caffe.cpp:209] GPU 3: TITAN Xp
I0522 21:41:41.372268 26558 solver.cpp:45] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 500000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "../asset/snapshot/AdditMarginLmdbNoE/AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx"
solver_mode: GPU
device_id: 0
net: "models/AdditMarginLmdbNoE/AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/train.prototxt"
train_state {
  level: 0
  stage: ""
}
snapshot_after_train: true
stepvalue: 150000
stepvalue: 300000
iter_size: 1
type: "SGD"
I0522 21:41:41.372522 26558 solver.cpp:102] Creating training net from net file: models/AdditMarginLmdbNoE/AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/train.prototxt
I0522 21:41:41.378705 26558 net.cpp:51] Initializing net from parameters: 
name: "2018-05-22_AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_train"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  data_param {
    source: "/home/zkx/Data_sdb/TrainData/Data_sdc/Patches/lmdb/fc_0.35_112x96_base-noencode-shuffle_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_1"
  type: "PReLU"
  bottom: "conv1_1"
  top: "conv1_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "conv1_3"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_3"
  type: "PReLU"
  bottom: "conv1_3"
  top: "conv1_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res1_3"
  type: "Eltwise"
  bottom: "conv1_1"
  bottom: "conv1_3"
  top: "res1_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res1_3_reduce"
  type: "Convolution"
  bottom: "res1_3"
  top: "res1_3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "res1_3_reduce"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "res1_3"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1"
  type: "PReLU"
  bottom: "conv2_1"
  top: "conv2_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res1_3_p"
  type: "Eltwise"
  bottom: "pool1"
  bottom: "conv2_1"
  top: "res1_3_p"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_3"
  type: "Convolution"
  bottom: "res1_3_p"
  top: "conv2_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_3"
  type: "PReLU"
  bottom: "conv2_3"
  top: "conv2_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res2_3"
  type: "Eltwise"
  bottom: "res1_3_p"
  bottom: "conv2_3"
  top: "res2_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_5"
  type: "Convolution"
  bottom: "res2_3"
  top: "conv2_5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_5"
  type: "PReLU"
  bottom: "conv2_5"
  top: "conv2_5"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res2_5"
  type: "Eltwise"
  bottom: "res2_3"
  bottom: "conv2_5"
  top: "res2_5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res2_5_reduce"
  type: "Convolution"
  bottom: "res2_5"
  top: "res2_5_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2_5_reduce"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "res2_5"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1"
  type: "PReLU"
  bottom: "conv3_1"
  top: "conv3_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res2_5_p"
  type: "Eltwise"
  bottom: "pool2"
  bottom: "conv3_1"
  top: "res2_5_p"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "res2_5_p"
  top: "conv3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_3"
  type: "PReLU"
  bottom: "conv3_3"
  top: "conv3_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res3_3"
  type: "Eltwise"
  bottom: "res2_5_p"
  bottom: "conv3_3"
  top: "res3_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_5"
  type: "Convolution"
  bottom: "res3_3"
  top: "conv3_5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_5"
  type: "PReLU"
  bottom: "conv3_5"
  top: "conv3_5"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res3_5"
  type: "Eltwise"
  bottom: "res3_3"
  bottom: "conv3_5"
  top: "res3_5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res3_5_reduce"
  type: "Convolution"
  bottom: "res3_5"
  top: "res3_5_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 144
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3_5_reduce"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "res3_5"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 144
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1"
  type: "PReLU"
  bottom: "conv4_1"
  top: "conv4_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res3_5_p"
  type: "Eltwise"
  bottom: "pool3"
  bottom: "conv4_1"
  top: "res3_5_p"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "res3_5_p"
  top: "conv4_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 144
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_3"
  type: "PReLU"
  bottom: "conv4_3"
  top: "conv4_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res4_3"
  type: "Eltwise"
  bottom: "res3_5_p"
  bottom: "conv4_3"
  top: "res4_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "fc5"
  type: "InnerProduct"
  bottom: "res4_3"
  top: "fc5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc5_bn"
  type: "BatchNorm"
  bottom: "fc5"
  top: "fc5"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "norm1"
  type: "Normalize"
  bottom: "fc5"
  top: "norm1"
}
layer {
  name: "fc-6_l2"
  type: "InnerProduct"
  bottom: "norm1"
  top: "fc-6_l2"
  param {
    lr_mult: 1
  }
  inner_product_param {
    num_output: 21331
    bias_term: false
    weight_filler {
      type: "xavier"
    }
    normalize: true
  }
}
layer {
  name: "fc-6_margin"
  type: "LabelSpecificAdd"
  bottom: "fc-6_l2"
  bottom: "label"
  top: "fc-6_margin"
  label_specific_add_param {
    bias: -0.35
  }
}
layer {
  name: "fc-6_margin_scale"
  type: "Scale"
  bottom: "fc-6_margin"
  top: "fc-6_margin_scale"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      type: "constant"
      value: 30
    }
  }
}
layer {
  name: "softmax_loss"
  type: "SoftmaxWithLoss"
  bottom: "fc-6_margin_scale"
  bottom: "label"
  top: "softmax_loss"
}
I0522 21:41:41.379346 26558 layer_factory.hpp:77] Creating layer data
I0522 21:41:41.379614 26558 db_lmdb.cpp:35] Opened lmdb /home/zkx/Data_sdb/TrainData/Data_sdc/Patches/lmdb/fc_0.35_112x96_base-noencode-shuffle_lmdb
I0522 21:41:41.379717 26558 net.cpp:84] Creating Layer data
I0522 21:41:41.382859 26558 net.cpp:380] data -> data
I0522 21:41:41.382937 26558 net.cpp:380] data -> label
I0522 21:41:41.385440 26558 data_layer.cpp:45] output data size: 64,3,112,96
I0522 21:41:41.485353 26558 net.cpp:122] Setting up data
I0522 21:41:41.485404 26558 net.cpp:129] Top shape: 64 3 112 96 (2064384)
I0522 21:41:41.485414 26558 net.cpp:129] Top shape: 64 (64)
I0522 21:41:41.485446 26558 net.cpp:137] Memory required for data: 8257792
I0522 21:41:41.485461 26558 layer_factory.hpp:77] Creating layer label_data_1_split
I0522 21:41:41.485487 26558 net.cpp:84] Creating Layer label_data_1_split
I0522 21:41:41.485496 26558 net.cpp:406] label_data_1_split <- label
I0522 21:41:41.485512 26558 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0522 21:41:41.485527 26558 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0522 21:41:41.485641 26558 net.cpp:122] Setting up label_data_1_split
I0522 21:41:41.485652 26558 net.cpp:129] Top shape: 64 (64)
I0522 21:41:41.485659 26558 net.cpp:129] Top shape: 64 (64)
I0522 21:41:41.485663 26558 net.cpp:137] Memory required for data: 8258304
I0522 21:41:41.485667 26558 layer_factory.hpp:77] Creating layer conv1_1
I0522 21:41:41.485699 26558 net.cpp:84] Creating Layer conv1_1
I0522 21:41:41.485707 26558 net.cpp:406] conv1_1 <- data
I0522 21:41:41.485716 26558 net.cpp:380] conv1_1 -> conv1_1
I0522 21:41:43.084406 26558 net.cpp:122] Setting up conv1_1
I0522 21:41:43.084473 26558 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.084481 26558 net.cpp:137] Memory required for data: 24773376
I0522 21:41:43.084527 26558 layer_factory.hpp:77] Creating layer relu1_1
I0522 21:41:43.084589 26558 net.cpp:84] Creating Layer relu1_1
I0522 21:41:43.084599 26558 net.cpp:406] relu1_1 <- conv1_1
I0522 21:41:43.084609 26558 net.cpp:367] relu1_1 -> conv1_1 (in-place)
I0522 21:41:43.086009 26558 net.cpp:122] Setting up relu1_1
I0522 21:41:43.086025 26558 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.086045 26558 net.cpp:137] Memory required for data: 41288448
I0522 21:41:43.086063 26558 layer_factory.hpp:77] Creating layer conv1_1_relu1_1_0_split
I0522 21:41:43.086114 26558 net.cpp:84] Creating Layer conv1_1_relu1_1_0_split
I0522 21:41:43.086123 26558 net.cpp:406] conv1_1_relu1_1_0_split <- conv1_1
I0522 21:41:43.086145 26558 net.cpp:380] conv1_1_relu1_1_0_split -> conv1_1_relu1_1_0_split_0
I0522 21:41:43.086159 26558 net.cpp:380] conv1_1_relu1_1_0_split -> conv1_1_relu1_1_0_split_1
I0522 21:41:43.086216 26558 net.cpp:122] Setting up conv1_1_relu1_1_0_split
I0522 21:41:43.086225 26558 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.086239 26558 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.086251 26558 net.cpp:137] Memory required for data: 74318592
I0522 21:41:43.086263 26558 layer_factory.hpp:77] Creating layer conv1_3
I0522 21:41:43.086297 26558 net.cpp:84] Creating Layer conv1_3
I0522 21:41:43.086310 26558 net.cpp:406] conv1_3 <- conv1_1_relu1_1_0_split_0
I0522 21:41:43.086324 26558 net.cpp:380] conv1_3 -> conv1_3
I0522 21:41:43.089092 26558 net.cpp:122] Setting up conv1_3
I0522 21:41:43.089110 26558 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.089116 26558 net.cpp:137] Memory required for data: 90833664
I0522 21:41:43.089146 26558 layer_factory.hpp:77] Creating layer relu1_3
I0522 21:41:43.089159 26558 net.cpp:84] Creating Layer relu1_3
I0522 21:41:43.089164 26558 net.cpp:406] relu1_3 <- conv1_3
I0522 21:41:43.089172 26558 net.cpp:367] relu1_3 -> conv1_3 (in-place)
I0522 21:41:43.089318 26558 net.cpp:122] Setting up relu1_3
I0522 21:41:43.089326 26558 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.089330 26558 net.cpp:137] Memory required for data: 107348736
I0522 21:41:43.089335 26558 layer_factory.hpp:77] Creating layer res1_3
I0522 21:41:43.089345 26558 net.cpp:84] Creating Layer res1_3
I0522 21:41:43.089350 26558 net.cpp:406] res1_3 <- conv1_1_relu1_1_0_split_1
I0522 21:41:43.089354 26558 net.cpp:406] res1_3 <- conv1_3
I0522 21:41:43.089360 26558 net.cpp:380] res1_3 -> res1_3
I0522 21:41:43.089390 26558 net.cpp:122] Setting up res1_3
I0522 21:41:43.089396 26558 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.089401 26558 net.cpp:137] Memory required for data: 123863808
I0522 21:41:43.089403 26558 layer_factory.hpp:77] Creating layer res1_3_res1_3_0_split
I0522 21:41:43.089411 26558 net.cpp:84] Creating Layer res1_3_res1_3_0_split
I0522 21:41:43.089435 26558 net.cpp:406] res1_3_res1_3_0_split <- res1_3
I0522 21:41:43.089443 26558 net.cpp:380] res1_3_res1_3_0_split -> res1_3_res1_3_0_split_0
I0522 21:41:43.089450 26558 net.cpp:380] res1_3_res1_3_0_split -> res1_3_res1_3_0_split_1
I0522 21:41:43.089486 26558 net.cpp:122] Setting up res1_3_res1_3_0_split
I0522 21:41:43.089493 26558 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.089498 26558 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.089500 26558 net.cpp:137] Memory required for data: 156893952
I0522 21:41:43.089505 26558 layer_factory.hpp:77] Creating layer res1_3_reduce
I0522 21:41:43.089519 26558 net.cpp:84] Creating Layer res1_3_reduce
I0522 21:41:43.089524 26558 net.cpp:406] res1_3_reduce <- res1_3_res1_3_0_split_0
I0522 21:41:43.089532 26558 net.cpp:380] res1_3_reduce -> res1_3_reduce
I0522 21:41:43.093427 26558 net.cpp:122] Setting up res1_3_reduce
I0522 21:41:43.093443 26558 net.cpp:129] Top shape: 64 48 56 48 (8257536)
I0522 21:41:43.093449 26558 net.cpp:137] Memory required for data: 189924096
I0522 21:41:43.093457 26558 layer_factory.hpp:77] Creating layer pool1
I0522 21:41:43.093473 26558 net.cpp:84] Creating Layer pool1
I0522 21:41:43.093479 26558 net.cpp:406] pool1 <- res1_3_reduce
I0522 21:41:43.093487 26558 net.cpp:380] pool1 -> pool1
I0522 21:41:43.093544 26558 net.cpp:122] Setting up pool1
I0522 21:41:43.093551 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.093555 26558 net.cpp:137] Memory required for data: 198181632
I0522 21:41:43.093559 26558 layer_factory.hpp:77] Creating layer conv2_1
I0522 21:41:43.093572 26558 net.cpp:84] Creating Layer conv2_1
I0522 21:41:43.093578 26558 net.cpp:406] conv2_1 <- res1_3_res1_3_0_split_1
I0522 21:41:43.093586 26558 net.cpp:380] conv2_1 -> conv2_1
I0522 21:41:43.100497 26558 net.cpp:122] Setting up conv2_1
I0522 21:41:43.100513 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.100518 26558 net.cpp:137] Memory required for data: 206439168
I0522 21:41:43.100533 26558 layer_factory.hpp:77] Creating layer relu2_1
I0522 21:41:43.100545 26558 net.cpp:84] Creating Layer relu2_1
I0522 21:41:43.100553 26558 net.cpp:406] relu2_1 <- conv2_1
I0522 21:41:43.100563 26558 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I0522 21:41:43.100678 26558 net.cpp:122] Setting up relu2_1
I0522 21:41:43.100687 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.100693 26558 net.cpp:137] Memory required for data: 214696704
I0522 21:41:43.100702 26558 layer_factory.hpp:77] Creating layer res1_3_p
I0522 21:41:43.100710 26558 net.cpp:84] Creating Layer res1_3_p
I0522 21:41:43.100718 26558 net.cpp:406] res1_3_p <- pool1
I0522 21:41:43.100724 26558 net.cpp:406] res1_3_p <- conv2_1
I0522 21:41:43.100731 26558 net.cpp:380] res1_3_p -> res1_3_p
I0522 21:41:43.100757 26558 net.cpp:122] Setting up res1_3_p
I0522 21:41:43.100764 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.100767 26558 net.cpp:137] Memory required for data: 222954240
I0522 21:41:43.100770 26558 layer_factory.hpp:77] Creating layer res1_3_p_res1_3_p_0_split
I0522 21:41:43.100780 26558 net.cpp:84] Creating Layer res1_3_p_res1_3_p_0_split
I0522 21:41:43.100785 26558 net.cpp:406] res1_3_p_res1_3_p_0_split <- res1_3_p
I0522 21:41:43.100790 26558 net.cpp:380] res1_3_p_res1_3_p_0_split -> res1_3_p_res1_3_p_0_split_0
I0522 21:41:43.100800 26558 net.cpp:380] res1_3_p_res1_3_p_0_split -> res1_3_p_res1_3_p_0_split_1
I0522 21:41:43.100836 26558 net.cpp:122] Setting up res1_3_p_res1_3_p_0_split
I0522 21:41:43.100842 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.100850 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.100857 26558 net.cpp:137] Memory required for data: 239469312
I0522 21:41:43.100860 26558 layer_factory.hpp:77] Creating layer conv2_3
I0522 21:41:43.100877 26558 net.cpp:84] Creating Layer conv2_3
I0522 21:41:43.100881 26558 net.cpp:406] conv2_3 <- res1_3_p_res1_3_p_0_split_0
I0522 21:41:43.100888 26558 net.cpp:380] conv2_3 -> conv2_3
I0522 21:41:43.107148 26558 net.cpp:122] Setting up conv2_3
I0522 21:41:43.107178 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.107188 26558 net.cpp:137] Memory required for data: 247726848
I0522 21:41:43.107203 26558 layer_factory.hpp:77] Creating layer relu2_3
I0522 21:41:43.107218 26558 net.cpp:84] Creating Layer relu2_3
I0522 21:41:43.107223 26558 net.cpp:406] relu2_3 <- conv2_3
I0522 21:41:43.107231 26558 net.cpp:367] relu2_3 -> conv2_3 (in-place)
I0522 21:41:43.107355 26558 net.cpp:122] Setting up relu2_3
I0522 21:41:43.107364 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.107367 26558 net.cpp:137] Memory required for data: 255984384
I0522 21:41:43.107373 26558 layer_factory.hpp:77] Creating layer res2_3
I0522 21:41:43.107380 26558 net.cpp:84] Creating Layer res2_3
I0522 21:41:43.107385 26558 net.cpp:406] res2_3 <- res1_3_p_res1_3_p_0_split_1
I0522 21:41:43.107390 26558 net.cpp:406] res2_3 <- conv2_3
I0522 21:41:43.107398 26558 net.cpp:380] res2_3 -> res2_3
I0522 21:41:43.107424 26558 net.cpp:122] Setting up res2_3
I0522 21:41:43.107430 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.107434 26558 net.cpp:137] Memory required for data: 264241920
I0522 21:41:43.107436 26558 layer_factory.hpp:77] Creating layer res2_3_res2_3_0_split
I0522 21:41:43.107442 26558 net.cpp:84] Creating Layer res2_3_res2_3_0_split
I0522 21:41:43.107446 26558 net.cpp:406] res2_3_res2_3_0_split <- res2_3
I0522 21:41:43.107451 26558 net.cpp:380] res2_3_res2_3_0_split -> res2_3_res2_3_0_split_0
I0522 21:41:43.107457 26558 net.cpp:380] res2_3_res2_3_0_split -> res2_3_res2_3_0_split_1
I0522 21:41:43.107491 26558 net.cpp:122] Setting up res2_3_res2_3_0_split
I0522 21:41:43.107497 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.107501 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.107504 26558 net.cpp:137] Memory required for data: 280756992
I0522 21:41:43.107511 26558 layer_factory.hpp:77] Creating layer conv2_5
I0522 21:41:43.107528 26558 net.cpp:84] Creating Layer conv2_5
I0522 21:41:43.107533 26558 net.cpp:406] conv2_5 <- res2_3_res2_3_0_split_0
I0522 21:41:43.107542 26558 net.cpp:380] conv2_5 -> conv2_5
I0522 21:41:43.110673 26558 net.cpp:122] Setting up conv2_5
I0522 21:41:43.110692 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.110708 26558 net.cpp:137] Memory required for data: 289014528
I0522 21:41:43.110716 26558 layer_factory.hpp:77] Creating layer relu2_5
I0522 21:41:43.110729 26558 net.cpp:84] Creating Layer relu2_5
I0522 21:41:43.110738 26558 net.cpp:406] relu2_5 <- conv2_5
I0522 21:41:43.110750 26558 net.cpp:367] relu2_5 -> conv2_5 (in-place)
I0522 21:41:43.110890 26558 net.cpp:122] Setting up relu2_5
I0522 21:41:43.110906 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.110911 26558 net.cpp:137] Memory required for data: 297272064
I0522 21:41:43.110924 26558 layer_factory.hpp:77] Creating layer res2_5
I0522 21:41:43.110932 26558 net.cpp:84] Creating Layer res2_5
I0522 21:41:43.110937 26558 net.cpp:406] res2_5 <- res2_3_res2_3_0_split_1
I0522 21:41:43.110942 26558 net.cpp:406] res2_5 <- conv2_5
I0522 21:41:43.110949 26558 net.cpp:380] res2_5 -> res2_5
I0522 21:41:43.110975 26558 net.cpp:122] Setting up res2_5
I0522 21:41:43.110982 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.110985 26558 net.cpp:137] Memory required for data: 305529600
I0522 21:41:43.110992 26558 layer_factory.hpp:77] Creating layer res2_5_res2_5_0_split
I0522 21:41:43.111002 26558 net.cpp:84] Creating Layer res2_5_res2_5_0_split
I0522 21:41:43.111006 26558 net.cpp:406] res2_5_res2_5_0_split <- res2_5
I0522 21:41:43.111012 26558 net.cpp:380] res2_5_res2_5_0_split -> res2_5_res2_5_0_split_0
I0522 21:41:43.111018 26558 net.cpp:380] res2_5_res2_5_0_split -> res2_5_res2_5_0_split_1
I0522 21:41:43.111050 26558 net.cpp:122] Setting up res2_5_res2_5_0_split
I0522 21:41:43.111062 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.111068 26558 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.111070 26558 net.cpp:137] Memory required for data: 322044672
I0522 21:41:43.111088 26558 layer_factory.hpp:77] Creating layer res2_5_reduce
I0522 21:41:43.111100 26558 net.cpp:84] Creating Layer res2_5_reduce
I0522 21:41:43.111106 26558 net.cpp:406] res2_5_reduce <- res2_5_res2_5_0_split_0
I0522 21:41:43.111114 26558 net.cpp:380] res2_5_reduce -> res2_5_reduce
I0522 21:41:43.115350 26558 net.cpp:122] Setting up res2_5_reduce
I0522 21:41:43.115377 26558 net.cpp:129] Top shape: 64 72 28 24 (3096576)
I0522 21:41:43.115383 26558 net.cpp:137] Memory required for data: 334430976
I0522 21:41:43.115391 26558 layer_factory.hpp:77] Creating layer pool2
I0522 21:41:43.115402 26558 net.cpp:84] Creating Layer pool2
I0522 21:41:43.115408 26558 net.cpp:406] pool2 <- res2_5_reduce
I0522 21:41:43.115414 26558 net.cpp:380] pool2 -> pool2
I0522 21:41:43.115473 26558 net.cpp:122] Setting up pool2
I0522 21:41:43.115479 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.115484 26558 net.cpp:137] Memory required for data: 337527552
I0522 21:41:43.115489 26558 layer_factory.hpp:77] Creating layer conv3_1
I0522 21:41:43.115504 26558 net.cpp:84] Creating Layer conv3_1
I0522 21:41:43.115509 26558 net.cpp:406] conv3_1 <- res2_5_res2_5_0_split_1
I0522 21:41:43.115519 26558 net.cpp:380] conv3_1 -> conv3_1
I0522 21:41:43.120978 26558 net.cpp:122] Setting up conv3_1
I0522 21:41:43.121001 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.121006 26558 net.cpp:137] Memory required for data: 340624128
I0522 21:41:43.121016 26558 layer_factory.hpp:77] Creating layer relu3_1
I0522 21:41:43.121024 26558 net.cpp:84] Creating Layer relu3_1
I0522 21:41:43.121029 26558 net.cpp:406] relu3_1 <- conv3_1
I0522 21:41:43.121035 26558 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I0522 21:41:43.121157 26558 net.cpp:122] Setting up relu3_1
I0522 21:41:43.121166 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.121171 26558 net.cpp:137] Memory required for data: 343720704
I0522 21:41:43.121176 26558 layer_factory.hpp:77] Creating layer res2_5_p
I0522 21:41:43.121186 26558 net.cpp:84] Creating Layer res2_5_p
I0522 21:41:43.121193 26558 net.cpp:406] res2_5_p <- pool2
I0522 21:41:43.121203 26558 net.cpp:406] res2_5_p <- conv3_1
I0522 21:41:43.121213 26558 net.cpp:380] res2_5_p -> res2_5_p
I0522 21:41:43.121239 26558 net.cpp:122] Setting up res2_5_p
I0522 21:41:43.121246 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.121249 26558 net.cpp:137] Memory required for data: 346817280
I0522 21:41:43.121253 26558 layer_factory.hpp:77] Creating layer res2_5_p_res2_5_p_0_split
I0522 21:41:43.121259 26558 net.cpp:84] Creating Layer res2_5_p_res2_5_p_0_split
I0522 21:41:43.121263 26558 net.cpp:406] res2_5_p_res2_5_p_0_split <- res2_5_p
I0522 21:41:43.121269 26558 net.cpp:380] res2_5_p_res2_5_p_0_split -> res2_5_p_res2_5_p_0_split_0
I0522 21:41:43.121276 26558 net.cpp:380] res2_5_p_res2_5_p_0_split -> res2_5_p_res2_5_p_0_split_1
I0522 21:41:43.121315 26558 net.cpp:122] Setting up res2_5_p_res2_5_p_0_split
I0522 21:41:43.121322 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.121326 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.121330 26558 net.cpp:137] Memory required for data: 353010432
I0522 21:41:43.121333 26558 layer_factory.hpp:77] Creating layer conv3_3
I0522 21:41:43.121348 26558 net.cpp:84] Creating Layer conv3_3
I0522 21:41:43.121356 26558 net.cpp:406] conv3_3 <- res2_5_p_res2_5_p_0_split_0
I0522 21:41:43.121369 26558 net.cpp:380] conv3_3 -> conv3_3
I0522 21:41:43.123994 26558 net.cpp:122] Setting up conv3_3
I0522 21:41:43.124012 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.124027 26558 net.cpp:137] Memory required for data: 356107008
I0522 21:41:43.124043 26558 layer_factory.hpp:77] Creating layer relu3_3
I0522 21:41:43.124058 26558 net.cpp:84] Creating Layer relu3_3
I0522 21:41:43.124065 26558 net.cpp:406] relu3_3 <- conv3_3
I0522 21:41:43.124076 26558 net.cpp:367] relu3_3 -> conv3_3 (in-place)
I0522 21:41:43.124217 26558 net.cpp:122] Setting up relu3_3
I0522 21:41:43.124225 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.124249 26558 net.cpp:137] Memory required for data: 359203584
I0522 21:41:43.124261 26558 layer_factory.hpp:77] Creating layer res3_3
I0522 21:41:43.124276 26558 net.cpp:84] Creating Layer res3_3
I0522 21:41:43.124282 26558 net.cpp:406] res3_3 <- res2_5_p_res2_5_p_0_split_1
I0522 21:41:43.124289 26558 net.cpp:406] res3_3 <- conv3_3
I0522 21:41:43.124297 26558 net.cpp:380] res3_3 -> res3_3
I0522 21:41:43.124343 26558 net.cpp:122] Setting up res3_3
I0522 21:41:43.124353 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.124357 26558 net.cpp:137] Memory required for data: 362300160
I0522 21:41:43.124361 26558 layer_factory.hpp:77] Creating layer res3_3_res3_3_0_split
I0522 21:41:43.124372 26558 net.cpp:84] Creating Layer res3_3_res3_3_0_split
I0522 21:41:43.124375 26558 net.cpp:406] res3_3_res3_3_0_split <- res3_3
I0522 21:41:43.124382 26558 net.cpp:380] res3_3_res3_3_0_split -> res3_3_res3_3_0_split_0
I0522 21:41:43.124392 26558 net.cpp:380] res3_3_res3_3_0_split -> res3_3_res3_3_0_split_1
I0522 21:41:43.124445 26558 net.cpp:122] Setting up res3_3_res3_3_0_split
I0522 21:41:43.124452 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.124460 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.124466 26558 net.cpp:137] Memory required for data: 368493312
I0522 21:41:43.124471 26558 layer_factory.hpp:77] Creating layer conv3_5
I0522 21:41:43.124487 26558 net.cpp:84] Creating Layer conv3_5
I0522 21:41:43.124495 26558 net.cpp:406] conv3_5 <- res3_3_res3_3_0_split_0
I0522 21:41:43.124506 26558 net.cpp:380] conv3_5 -> conv3_5
I0522 21:41:43.126991 26558 net.cpp:122] Setting up conv3_5
I0522 21:41:43.127013 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.127019 26558 net.cpp:137] Memory required for data: 371589888
I0522 21:41:43.127027 26558 layer_factory.hpp:77] Creating layer relu3_5
I0522 21:41:43.127037 26558 net.cpp:84] Creating Layer relu3_5
I0522 21:41:43.127043 26558 net.cpp:406] relu3_5 <- conv3_5
I0522 21:41:43.127050 26558 net.cpp:367] relu3_5 -> conv3_5 (in-place)
I0522 21:41:43.127173 26558 net.cpp:122] Setting up relu3_5
I0522 21:41:43.127182 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.127185 26558 net.cpp:137] Memory required for data: 374686464
I0522 21:41:43.127190 26558 layer_factory.hpp:77] Creating layer res3_5
I0522 21:41:43.127198 26558 net.cpp:84] Creating Layer res3_5
I0522 21:41:43.127203 26558 net.cpp:406] res3_5 <- res3_3_res3_3_0_split_1
I0522 21:41:43.127207 26558 net.cpp:406] res3_5 <- conv3_5
I0522 21:41:43.127213 26558 net.cpp:380] res3_5 -> res3_5
I0522 21:41:43.127244 26558 net.cpp:122] Setting up res3_5
I0522 21:41:43.127251 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.127255 26558 net.cpp:137] Memory required for data: 377783040
I0522 21:41:43.127259 26558 layer_factory.hpp:77] Creating layer res3_5_res3_5_0_split
I0522 21:41:43.127264 26558 net.cpp:84] Creating Layer res3_5_res3_5_0_split
I0522 21:41:43.127269 26558 net.cpp:406] res3_5_res3_5_0_split <- res3_5
I0522 21:41:43.127274 26558 net.cpp:380] res3_5_res3_5_0_split -> res3_5_res3_5_0_split_0
I0522 21:41:43.127281 26558 net.cpp:380] res3_5_res3_5_0_split -> res3_5_res3_5_0_split_1
I0522 21:41:43.127313 26558 net.cpp:122] Setting up res3_5_res3_5_0_split
I0522 21:41:43.127321 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.127326 26558 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.127333 26558 net.cpp:137] Memory required for data: 383976192
I0522 21:41:43.127341 26558 layer_factory.hpp:77] Creating layer res3_5_reduce
I0522 21:41:43.127353 26558 net.cpp:84] Creating Layer res3_5_reduce
I0522 21:41:43.127358 26558 net.cpp:406] res3_5_reduce <- res3_5_res3_5_0_split_0
I0522 21:41:43.127365 26558 net.cpp:380] res3_5_reduce -> res3_5_reduce
I0522 21:41:43.129088 26558 net.cpp:122] Setting up res3_5_reduce
I0522 21:41:43.129117 26558 net.cpp:129] Top shape: 64 144 14 12 (1548288)
I0522 21:41:43.129127 26558 net.cpp:137] Memory required for data: 390169344
I0522 21:41:43.129142 26558 layer_factory.hpp:77] Creating layer pool3
I0522 21:41:43.129169 26558 net.cpp:84] Creating Layer pool3
I0522 21:41:43.129182 26558 net.cpp:406] pool3 <- res3_5_reduce
I0522 21:41:43.129199 26558 net.cpp:380] pool3 -> pool3
I0522 21:41:43.129273 26558 net.cpp:122] Setting up pool3
I0522 21:41:43.129281 26558 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.129285 26558 net.cpp:137] Memory required for data: 391717632
I0522 21:41:43.129297 26558 layer_factory.hpp:77] Creating layer conv4_1
I0522 21:41:43.129320 26558 net.cpp:84] Creating Layer conv4_1
I0522 21:41:43.129330 26558 net.cpp:406] conv4_1 <- res3_5_res3_5_0_split_1
I0522 21:41:43.129343 26558 net.cpp:380] conv4_1 -> conv4_1
I0522 21:41:43.136622 26558 net.cpp:122] Setting up conv4_1
I0522 21:41:43.136667 26558 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.136675 26558 net.cpp:137] Memory required for data: 393265920
I0522 21:41:43.136688 26558 layer_factory.hpp:77] Creating layer relu4_1
I0522 21:41:43.136704 26558 net.cpp:84] Creating Layer relu4_1
I0522 21:41:43.136714 26558 net.cpp:406] relu4_1 <- conv4_1
I0522 21:41:43.136732 26558 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I0522 21:41:43.136878 26558 net.cpp:122] Setting up relu4_1
I0522 21:41:43.136900 26558 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.136906 26558 net.cpp:137] Memory required for data: 394814208
I0522 21:41:43.136920 26558 layer_factory.hpp:77] Creating layer res3_5_p
I0522 21:41:43.136929 26558 net.cpp:84] Creating Layer res3_5_p
I0522 21:41:43.136934 26558 net.cpp:406] res3_5_p <- pool3
I0522 21:41:43.136945 26558 net.cpp:406] res3_5_p <- conv4_1
I0522 21:41:43.136958 26558 net.cpp:380] res3_5_p -> res3_5_p
I0522 21:41:43.136993 26558 net.cpp:122] Setting up res3_5_p
I0522 21:41:43.137001 26558 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.137003 26558 net.cpp:137] Memory required for data: 396362496
I0522 21:41:43.137013 26558 layer_factory.hpp:77] Creating layer res3_5_p_res3_5_p_0_split
I0522 21:41:43.137025 26558 net.cpp:84] Creating Layer res3_5_p_res3_5_p_0_split
I0522 21:41:43.137034 26558 net.cpp:406] res3_5_p_res3_5_p_0_split <- res3_5_p
I0522 21:41:43.137051 26558 net.cpp:380] res3_5_p_res3_5_p_0_split -> res3_5_p_res3_5_p_0_split_0
I0522 21:41:43.137068 26558 net.cpp:380] res3_5_p_res3_5_p_0_split -> res3_5_p_res3_5_p_0_split_1
I0522 21:41:43.137145 26558 net.cpp:122] Setting up res3_5_p_res3_5_p_0_split
I0522 21:41:43.137157 26558 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.137166 26558 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.137173 26558 net.cpp:137] Memory required for data: 399459072
I0522 21:41:43.137177 26558 layer_factory.hpp:77] Creating layer conv4_3
I0522 21:41:43.137192 26558 net.cpp:84] Creating Layer conv4_3
I0522 21:41:43.137202 26558 net.cpp:406] conv4_3 <- res3_5_p_res3_5_p_0_split_0
I0522 21:41:43.137212 26558 net.cpp:380] conv4_3 -> conv4_3
I0522 21:41:43.143632 26558 net.cpp:122] Setting up conv4_3
I0522 21:41:43.143648 26558 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.143683 26558 net.cpp:137] Memory required for data: 401007360
I0522 21:41:43.143694 26558 layer_factory.hpp:77] Creating layer relu4_3
I0522 21:41:43.143705 26558 net.cpp:84] Creating Layer relu4_3
I0522 21:41:43.143712 26558 net.cpp:406] relu4_3 <- conv4_3
I0522 21:41:43.143718 26558 net.cpp:367] relu4_3 -> conv4_3 (in-place)
I0522 21:41:43.143834 26558 net.cpp:122] Setting up relu4_3
I0522 21:41:43.143843 26558 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.143847 26558 net.cpp:137] Memory required for data: 402555648
I0522 21:41:43.143853 26558 layer_factory.hpp:77] Creating layer res4_3
I0522 21:41:43.143862 26558 net.cpp:84] Creating Layer res4_3
I0522 21:41:43.143867 26558 net.cpp:406] res4_3 <- res3_5_p_res3_5_p_0_split_1
I0522 21:41:43.143872 26558 net.cpp:406] res4_3 <- conv4_3
I0522 21:41:43.143878 26558 net.cpp:380] res4_3 -> res4_3
I0522 21:41:43.143916 26558 net.cpp:122] Setting up res4_3
I0522 21:41:43.143923 26558 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.143939 26558 net.cpp:137] Memory required for data: 404103936
I0522 21:41:43.143942 26558 layer_factory.hpp:77] Creating layer fc5
I0522 21:41:43.143961 26558 net.cpp:84] Creating Layer fc5
I0522 21:41:43.143967 26558 net.cpp:406] fc5 <- res4_3
I0522 21:41:43.143976 26558 net.cpp:380] fc5 -> fc5
I0522 21:41:43.157896 26558 net.cpp:122] Setting up fc5
I0522 21:41:43.157913 26558 net.cpp:129] Top shape: 64 256 (16384)
I0522 21:41:43.157917 26558 net.cpp:137] Memory required for data: 404169472
I0522 21:41:43.157928 26558 layer_factory.hpp:77] Creating layer fc5_bn
I0522 21:41:43.157948 26558 net.cpp:84] Creating Layer fc5_bn
I0522 21:41:43.157954 26558 net.cpp:406] fc5_bn <- fc5
I0522 21:41:43.157963 26558 net.cpp:367] fc5_bn -> fc5 (in-place)
I0522 21:41:43.158165 26558 net.cpp:122] Setting up fc5_bn
I0522 21:41:43.158174 26558 net.cpp:129] Top shape: 64 256 (16384)
I0522 21:41:43.158179 26558 net.cpp:137] Memory required for data: 404235008
I0522 21:41:43.158186 26558 layer_factory.hpp:77] Creating layer norm1
I0522 21:41:43.158200 26558 net.cpp:84] Creating Layer norm1
I0522 21:41:43.158205 26558 net.cpp:406] norm1 <- fc5
I0522 21:41:43.158211 26558 net.cpp:380] norm1 -> norm1
I0522 21:41:43.158278 26558 net.cpp:122] Setting up norm1
I0522 21:41:43.158285 26558 net.cpp:129] Top shape: 64 256 1 1 (16384)
I0522 21:41:43.158293 26558 net.cpp:137] Memory required for data: 404300544
I0522 21:41:43.158299 26558 layer_factory.hpp:77] Creating layer fc-6_l2
I0522 21:41:43.158310 26558 net.cpp:84] Creating Layer fc-6_l2
I0522 21:41:43.158316 26558 net.cpp:406] fc-6_l2 <- norm1
I0522 21:41:43.158327 26558 net.cpp:380] fc-6_l2 -> fc-6_l2
I0522 21:41:43.208570 26558 net.cpp:122] Setting up fc-6_l2
I0522 21:41:43.208602 26558 net.cpp:129] Top shape: 64 21331 (1365184)
I0522 21:41:43.208607 26558 net.cpp:137] Memory required for data: 409761280
I0522 21:41:43.208616 26558 layer_factory.hpp:77] Creating layer fc-6_margin
I0522 21:41:43.208642 26558 net.cpp:84] Creating Layer fc-6_margin
I0522 21:41:43.208649 26558 net.cpp:406] fc-6_margin <- fc-6_l2
I0522 21:41:43.208657 26558 net.cpp:406] fc-6_margin <- label_data_1_split_0
I0522 21:41:43.208665 26558 net.cpp:380] fc-6_margin -> fc-6_margin
I0522 21:41:43.208708 26558 net.cpp:122] Setting up fc-6_margin
I0522 21:41:43.208714 26558 net.cpp:129] Top shape: 64 21331 (1365184)
I0522 21:41:43.208719 26558 net.cpp:137] Memory required for data: 415222016
I0522 21:41:43.208722 26558 layer_factory.hpp:77] Creating layer fc-6_margin_scale
I0522 21:41:43.208748 26558 net.cpp:84] Creating Layer fc-6_margin_scale
I0522 21:41:43.208755 26558 net.cpp:406] fc-6_margin_scale <- fc-6_margin
I0522 21:41:43.208765 26558 net.cpp:380] fc-6_margin_scale -> fc-6_margin_scale
I0522 21:41:43.208948 26558 net.cpp:122] Setting up fc-6_margin_scale
I0522 21:41:43.208958 26558 net.cpp:129] Top shape: 64 21331 (1365184)
I0522 21:41:43.208964 26558 net.cpp:137] Memory required for data: 420682752
I0522 21:41:43.208973 26558 layer_factory.hpp:77] Creating layer softmax_loss
I0522 21:41:43.208984 26558 net.cpp:84] Creating Layer softmax_loss
I0522 21:41:43.208992 26558 net.cpp:406] softmax_loss <- fc-6_margin_scale
I0522 21:41:43.208997 26558 net.cpp:406] softmax_loss <- label_data_1_split_1
I0522 21:41:43.209007 26558 net.cpp:380] softmax_loss -> softmax_loss
I0522 21:41:43.209022 26558 layer_factory.hpp:77] Creating layer softmax_loss
I0522 21:41:43.214026 26558 net.cpp:122] Setting up softmax_loss
I0522 21:41:43.214047 26558 net.cpp:129] Top shape: (1)
I0522 21:41:43.214051 26558 net.cpp:132]     with loss weight 1
I0522 21:41:43.214073 26558 net.cpp:137] Memory required for data: 420682756
I0522 21:41:43.214078 26558 net.cpp:198] softmax_loss needs backward computation.
I0522 21:41:43.214085 26558 net.cpp:198] fc-6_margin_scale needs backward computation.
I0522 21:41:43.214092 26558 net.cpp:198] fc-6_margin needs backward computation.
I0522 21:41:43.214098 26558 net.cpp:198] fc-6_l2 needs backward computation.
I0522 21:41:43.214103 26558 net.cpp:198] norm1 needs backward computation.
I0522 21:41:43.214107 26558 net.cpp:198] fc5_bn needs backward computation.
I0522 21:41:43.214131 26558 net.cpp:198] fc5 needs backward computation.
I0522 21:41:43.214136 26558 net.cpp:198] res4_3 needs backward computation.
I0522 21:41:43.214141 26558 net.cpp:198] relu4_3 needs backward computation.
I0522 21:41:43.214145 26558 net.cpp:198] conv4_3 needs backward computation.
I0522 21:41:43.214150 26558 net.cpp:198] res3_5_p_res3_5_p_0_split needs backward computation.
I0522 21:41:43.214154 26558 net.cpp:198] res3_5_p needs backward computation.
I0522 21:41:43.214159 26558 net.cpp:198] relu4_1 needs backward computation.
I0522 21:41:43.214164 26558 net.cpp:198] conv4_1 needs backward computation.
I0522 21:41:43.214167 26558 net.cpp:198] pool3 needs backward computation.
I0522 21:41:43.214171 26558 net.cpp:198] res3_5_reduce needs backward computation.
I0522 21:41:43.214175 26558 net.cpp:198] res3_5_res3_5_0_split needs backward computation.
I0522 21:41:43.214181 26558 net.cpp:198] res3_5 needs backward computation.
I0522 21:41:43.214185 26558 net.cpp:198] relu3_5 needs backward computation.
I0522 21:41:43.214190 26558 net.cpp:198] conv3_5 needs backward computation.
I0522 21:41:43.214195 26558 net.cpp:198] res3_3_res3_3_0_split needs backward computation.
I0522 21:41:43.214198 26558 net.cpp:198] res3_3 needs backward computation.
I0522 21:41:43.214206 26558 net.cpp:198] relu3_3 needs backward computation.
I0522 21:41:43.214213 26558 net.cpp:198] conv3_3 needs backward computation.
I0522 21:41:43.214220 26558 net.cpp:198] res2_5_p_res2_5_p_0_split needs backward computation.
I0522 21:41:43.214226 26558 net.cpp:198] res2_5_p needs backward computation.
I0522 21:41:43.214231 26558 net.cpp:198] relu3_1 needs backward computation.
I0522 21:41:43.214233 26558 net.cpp:198] conv3_1 needs backward computation.
I0522 21:41:43.214237 26558 net.cpp:198] pool2 needs backward computation.
I0522 21:41:43.214242 26558 net.cpp:198] res2_5_reduce needs backward computation.
I0522 21:41:43.214246 26558 net.cpp:198] res2_5_res2_5_0_split needs backward computation.
I0522 21:41:43.214251 26558 net.cpp:198] res2_5 needs backward computation.
I0522 21:41:43.214257 26558 net.cpp:198] relu2_5 needs backward computation.
I0522 21:41:43.214262 26558 net.cpp:198] conv2_5 needs backward computation.
I0522 21:41:43.214265 26558 net.cpp:198] res2_3_res2_3_0_split needs backward computation.
I0522 21:41:43.214269 26558 net.cpp:198] res2_3 needs backward computation.
I0522 21:41:43.214273 26558 net.cpp:198] relu2_3 needs backward computation.
I0522 21:41:43.214278 26558 net.cpp:198] conv2_3 needs backward computation.
I0522 21:41:43.214282 26558 net.cpp:198] res1_3_p_res1_3_p_0_split needs backward computation.
I0522 21:41:43.214287 26558 net.cpp:198] res1_3_p needs backward computation.
I0522 21:41:43.214303 26558 net.cpp:198] relu2_1 needs backward computation.
I0522 21:41:43.214308 26558 net.cpp:198] conv2_1 needs backward computation.
I0522 21:41:43.214311 26558 net.cpp:198] pool1 needs backward computation.
I0522 21:41:43.214318 26558 net.cpp:198] res1_3_reduce needs backward computation.
I0522 21:41:43.214321 26558 net.cpp:198] res1_3_res1_3_0_split needs backward computation.
I0522 21:41:43.214326 26558 net.cpp:198] res1_3 needs backward computation.
I0522 21:41:43.214335 26558 net.cpp:198] relu1_3 needs backward computation.
I0522 21:41:43.214342 26558 net.cpp:198] conv1_3 needs backward computation.
I0522 21:41:43.214347 26558 net.cpp:198] conv1_1_relu1_1_0_split needs backward computation.
I0522 21:41:43.214354 26558 net.cpp:198] relu1_1 needs backward computation.
I0522 21:41:43.214359 26558 net.cpp:198] conv1_1 needs backward computation.
I0522 21:41:43.214365 26558 net.cpp:200] label_data_1_split does not need backward computation.
I0522 21:41:43.214372 26558 net.cpp:200] data does not need backward computation.
I0522 21:41:43.214377 26558 net.cpp:242] This network produces output softmax_loss
I0522 21:41:43.214418 26558 net.cpp:255] Network initialization done.
I0522 21:41:43.214612 26558 solver.cpp:57] Solver scaffolding done.
I0522 21:41:43.216141 26558 caffe.cpp:235] Resuming from ../asset/snapshot/AdditMarginLmdbNoE/AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_400000.solverstate
I0522 21:41:44.990236 26558 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AdditMarginLmdbNoE/AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_400000.caffemodel
I0522 21:41:44.990301 26558 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:41:45.003283 26558 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:41:45.049460 26558 caffe.cpp:239] Starting Optimization
I0522 21:41:54.914288 26723 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AdditMarginLmdbNoE/AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_400000.caffemodel
I0522 21:41:54.914330 26723 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:41:54.923648 26723 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:41:54.998565 26721 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AdditMarginLmdbNoE/AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_400000.caffemodel
I0522 21:41:54.998565 26722 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AdditMarginLmdbNoE/AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_400000.caffemodel
I0522 21:41:54.998605 26721 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:41:54.998610 26722 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:41:55.008358 26721 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:41:55.008976 26722 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:41:55.403455 26558 solver.cpp:293] Solving 2018-05-22_AdditMarginLmdbNoE-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_train
I0522 21:41:55.403545 26558 solver.cpp:294] Learning Rate Policy: multistep
I0522 21:41:55.791678 26558 solver.cpp:239] Iteration 400000 (1.21914e+06 iter/s, 0.3281s/10 iters), loss = 6.12259
I0522 21:41:55.791728 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.12259 (* 1 = 6.12259 loss)
I0522 21:41:55.791822 26558 sgd_solver.cpp:112] Iteration 400000, lr = 0.0001
I0522 21:42:00.788189 26558 solver.cpp:239] Iteration 400010 (2.00149 iter/s, 4.99627s/10 iters), loss = 6.41391
I0522 21:42:00.788224 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.41391 (* 1 = 6.41391 loss)
I0522 21:42:01.720230 26558 sgd_solver.cpp:112] Iteration 400010, lr = 0.0001
I0522 21:42:07.041486 26558 solver.cpp:239] Iteration 400020 (1.59923 iter/s, 6.25302s/10 iters), loss = 5.92067
I0522 21:42:07.041527 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.92067 (* 1 = 5.92067 loss)
I0522 21:42:07.059474 26558 sgd_solver.cpp:112] Iteration 400020, lr = 0.0001
I0522 21:42:11.922889 26558 solver.cpp:239] Iteration 400030 (2.04869 iter/s, 4.88117s/10 iters), loss = 6.62
I0522 21:42:11.923195 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.62 (* 1 = 6.62 loss)
I0522 21:42:11.940521 26558 sgd_solver.cpp:112] Iteration 400030, lr = 0.0001
I0522 21:42:16.314599 26558 solver.cpp:239] Iteration 400040 (2.27726 iter/s, 4.39125s/10 iters), loss = 6.30242
I0522 21:42:16.314640 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.30242 (* 1 = 6.30242 loss)
I0522 21:42:17.247391 26558 sgd_solver.cpp:112] Iteration 400040, lr = 0.0001
I0522 21:42:25.926020 26558 solver.cpp:239] Iteration 400050 (1.04047 iter/s, 9.61101s/10 iters), loss = 5.87989
I0522 21:42:25.926061 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.87989 (* 1 = 5.87989 loss)
I0522 21:42:26.883853 26558 sgd_solver.cpp:112] Iteration 400050, lr = 0.0001
I0522 21:42:36.335860 26558 solver.cpp:239] Iteration 400060 (0.960671 iter/s, 10.4094s/10 iters), loss = 6.70004
I0522 21:42:36.335901 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.70004 (* 1 = 6.70004 loss)
I0522 21:42:36.822422 26558 sgd_solver.cpp:112] Iteration 400060, lr = 0.0001
I0522 21:42:43.247084 26558 solver.cpp:239] Iteration 400070 (1.44699 iter/s, 6.91091s/10 iters), loss = 5.6095
I0522 21:42:43.247385 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.6095 (* 1 = 5.6095 loss)
I0522 21:42:43.264927 26558 sgd_solver.cpp:112] Iteration 400070, lr = 0.0001
I0522 21:42:48.543469 26558 solver.cpp:239] Iteration 400080 (1.88825 iter/s, 5.2959s/10 iters), loss = 7.25284
I0522 21:42:48.543514 26558 solver.cpp:258]     Train net output #0: softmax_loss = 7.25284 (* 1 = 7.25284 loss)
I0522 21:42:48.796517 26558 sgd_solver.cpp:112] Iteration 400080, lr = 0.0001
I0522 21:42:52.738474 26558 solver.cpp:239] Iteration 400090 (2.38391 iter/s, 4.19479s/10 iters), loss = 6.48309
I0522 21:42:52.738524 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.48309 (* 1 = 6.48309 loss)
I0522 21:42:52.756191 26558 sgd_solver.cpp:112] Iteration 400090, lr = 0.0001
I0522 21:43:01.147591 26558 solver.cpp:239] Iteration 400100 (1.18924 iter/s, 8.40873s/10 iters), loss = 6.13706
I0522 21:43:01.147642 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.13706 (* 1 = 6.13706 loss)
I0522 21:43:01.995569 26558 sgd_solver.cpp:112] Iteration 400100, lr = 0.0001
I0522 21:43:10.198623 26558 solver.cpp:239] Iteration 400110 (1.1049 iter/s, 9.05063s/10 iters), loss = 6.10525
I0522 21:43:10.198664 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.10525 (* 1 = 6.10525 loss)
I0522 21:43:11.119094 26558 sgd_solver.cpp:112] Iteration 400110, lr = 0.0001
I0522 21:43:18.186655 26558 solver.cpp:239] Iteration 400120 (1.25193 iter/s, 7.98768s/10 iters), loss = 5.60443
I0522 21:43:18.186867 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.60443 (* 1 = 5.60443 loss)
I0522 21:43:18.204059 26558 sgd_solver.cpp:112] Iteration 400120, lr = 0.0001
I0522 21:43:22.366649 26558 solver.cpp:239] Iteration 400130 (2.39256 iter/s, 4.17963s/10 iters), loss = 6.22182
I0522 21:43:22.366689 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.22182 (* 1 = 6.22182 loss)
I0522 21:43:23.279517 26558 sgd_solver.cpp:112] Iteration 400130, lr = 0.0001
I0522 21:43:27.236837 26558 solver.cpp:239] Iteration 400140 (2.05341 iter/s, 4.86995s/10 iters), loss = 6.7647
I0522 21:43:27.236873 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.7647 (* 1 = 6.7647 loss)
I0522 21:43:27.254662 26558 sgd_solver.cpp:112] Iteration 400140, lr = 0.0001
I0522 21:43:36.034796 26558 solver.cpp:239] Iteration 400150 (1.13668 iter/s, 8.79757s/10 iters), loss = 6.79781
I0522 21:43:36.034848 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.79781 (* 1 = 6.79781 loss)
I0522 21:43:36.560823 26558 sgd_solver.cpp:112] Iteration 400150, lr = 0.0001
I0522 21:43:46.211722 26558 solver.cpp:239] Iteration 400160 (0.982659 iter/s, 10.1765s/10 iters), loss = 5.08745
I0522 21:43:46.211799 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.08745 (* 1 = 5.08745 loss)
I0522 21:43:46.393702 26558 sgd_solver.cpp:112] Iteration 400160, lr = 0.0001
I0522 21:43:51.829797 26558 solver.cpp:239] Iteration 400170 (1.78006 iter/s, 5.61778s/10 iters), loss = 6.26928
I0522 21:43:51.830098 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.26928 (* 1 = 6.26928 loss)
I0522 21:43:51.847800 26558 sgd_solver.cpp:112] Iteration 400170, lr = 0.0001
I0522 21:43:56.420119 26558 solver.cpp:239] Iteration 400180 (2.17871 iter/s, 4.58987s/10 iters), loss = 5.6641
I0522 21:43:56.420164 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.6641 (* 1 = 5.6641 loss)
I0522 21:43:57.328142 26558 sgd_solver.cpp:112] Iteration 400180, lr = 0.0001
I0522 21:44:01.278131 26558 solver.cpp:239] Iteration 400190 (2.05857 iter/s, 4.85775s/10 iters), loss = 6.37125
I0522 21:44:01.278168 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.37125 (* 1 = 6.37125 loss)
I0522 21:44:02.168718 26558 sgd_solver.cpp:112] Iteration 400190, lr = 0.0001
I0522 21:44:09.451328 26558 solver.cpp:239] Iteration 400200 (1.22356 iter/s, 8.17284s/10 iters), loss = 6.42421
I0522 21:44:09.451369 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.42421 (* 1 = 6.42421 loss)
I0522 21:44:10.272969 26558 sgd_solver.cpp:112] Iteration 400200, lr = 0.0001
I0522 21:44:17.428174 26558 solver.cpp:239] Iteration 400210 (1.25369 iter/s, 7.97648s/10 iters), loss = 6.7074
I0522 21:44:17.428226 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.7074 (* 1 = 6.7074 loss)
I0522 21:44:17.608644 26558 sgd_solver.cpp:112] Iteration 400210, lr = 0.0001
I0522 21:44:22.293619 26558 solver.cpp:239] Iteration 400220 (2.05541 iter/s, 4.8652s/10 iters), loss = 7.18982
I0522 21:44:22.293831 26558 solver.cpp:258]     Train net output #0: softmax_loss = 7.18982 (* 1 = 7.18982 loss)
I0522 21:44:22.310735 26558 sgd_solver.cpp:112] Iteration 400220, lr = 0.0001
I0522 21:44:29.511759 26558 solver.cpp:239] Iteration 400230 (1.38549 iter/s, 7.21766s/10 iters), loss = 6.29664
I0522 21:44:29.511795 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.29664 (* 1 = 6.29664 loss)
I0522 21:44:29.531683 26558 sgd_solver.cpp:112] Iteration 400230, lr = 0.0001
I0522 21:44:35.523669 26558 solver.cpp:239] Iteration 400240 (1.66344 iter/s, 6.01163s/10 iters), loss = 5.86232
I0522 21:44:35.523711 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.86232 (* 1 = 5.86232 loss)
I0522 21:44:35.563891 26558 sgd_solver.cpp:112] Iteration 400240, lr = 0.0001
I0522 21:44:41.612535 26558 solver.cpp:239] Iteration 400250 (1.64242 iter/s, 6.08858s/10 iters), loss = 5.96989
I0522 21:44:41.612584 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.96989 (* 1 = 5.96989 loss)
I0522 21:44:42.442286 26558 sgd_solver.cpp:112] Iteration 400250, lr = 0.0001
I0522 21:44:47.134069 26558 solver.cpp:239] Iteration 400260 (1.81118 iter/s, 5.52127s/10 iters), loss = 6.49515
I0522 21:44:47.134105 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.49515 (* 1 = 6.49515 loss)
I0522 21:44:47.999181 26558 sgd_solver.cpp:112] Iteration 400260, lr = 0.0001
I0522 21:44:52.142396 26558 solver.cpp:239] Iteration 400270 (1.99677 iter/s, 5.00808s/10 iters), loss = 6.28734
I0522 21:44:52.142441 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.28734 (* 1 = 6.28734 loss)
I0522 21:44:52.771548 26558 sgd_solver.cpp:112] Iteration 400270, lr = 0.0001
I0522 21:44:57.735245 26558 solver.cpp:239] Iteration 400280 (1.78809 iter/s, 5.59256s/10 iters), loss = 6.07265
I0522 21:44:57.735283 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.07265 (* 1 = 6.07265 loss)
I0522 21:44:58.623401 26558 sgd_solver.cpp:112] Iteration 400280, lr = 0.0001
I0522 21:45:04.526226 26558 solver.cpp:239] Iteration 400290 (1.47261 iter/s, 6.79068s/10 iters), loss = 5.96187
I0522 21:45:04.526264 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.96187 (* 1 = 5.96187 loss)
I0522 21:45:04.543951 26558 sgd_solver.cpp:112] Iteration 400290, lr = 0.0001
I0522 21:45:11.712569 26558 solver.cpp:239] Iteration 400300 (1.39159 iter/s, 7.18601s/10 iters), loss = 6.50838
I0522 21:45:11.712618 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.50838 (* 1 = 6.50838 loss)
I0522 21:45:11.864440 26558 sgd_solver.cpp:112] Iteration 400300, lr = 0.0001
I0522 21:45:18.106417 26558 solver.cpp:239] Iteration 400310 (1.56408 iter/s, 6.39354s/10 iters), loss = 6.27906
I0522 21:45:18.106462 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.27906 (* 1 = 6.27906 loss)
I0522 21:45:18.113442 26558 sgd_solver.cpp:112] Iteration 400310, lr = 0.0001
I0522 21:45:25.626452 26558 solver.cpp:239] Iteration 400320 (1.32984 iter/s, 7.51969s/10 iters), loss = 5.21321
I0522 21:45:25.626616 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.21321 (* 1 = 5.21321 loss)
I0522 21:45:26.241473 26558 sgd_solver.cpp:112] Iteration 400320, lr = 0.0001
I0522 21:45:29.781648 26558 solver.cpp:239] Iteration 400330 (2.40682 iter/s, 4.15487s/10 iters), loss = 5.94591
I0522 21:45:29.781684 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.94591 (* 1 = 5.94591 loss)
I0522 21:45:30.639264 26558 sgd_solver.cpp:112] Iteration 400330, lr = 0.0001
I0522 21:45:37.173250 26558 solver.cpp:239] Iteration 400340 (1.35295 iter/s, 7.39127s/10 iters), loss = 6.3549
I0522 21:45:37.173286 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.3549 (* 1 = 6.3549 loss)
I0522 21:45:37.191287 26558 sgd_solver.cpp:112] Iteration 400340, lr = 0.0001
I0522 21:45:44.512279 26558 solver.cpp:239] Iteration 400350 (1.36264 iter/s, 7.3387s/10 iters), loss = 5.82762
I0522 21:45:44.512320 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.82762 (* 1 = 5.82762 loss)
I0522 21:45:44.916031 26558 sgd_solver.cpp:112] Iteration 400350, lr = 0.0001
I0522 21:45:48.980648 26558 solver.cpp:239] Iteration 400360 (2.23806 iter/s, 4.46815s/10 iters), loss = 5.17499
I0522 21:45:48.980687 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.17499 (* 1 = 5.17499 loss)
I0522 21:45:48.998409 26558 sgd_solver.cpp:112] Iteration 400360, lr = 0.0001
I0522 21:45:54.559434 26558 solver.cpp:239] Iteration 400370 (1.79259 iter/s, 5.57852s/10 iters), loss = 6.90292
I0522 21:45:54.559473 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.90292 (* 1 = 6.90292 loss)
I0522 21:45:55.503747 26558 sgd_solver.cpp:112] Iteration 400370, lr = 0.0001
I0522 21:46:02.369465 26558 solver.cpp:239] Iteration 400380 (1.28046 iter/s, 7.80969s/10 iters), loss = 5.74357
I0522 21:46:02.369586 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.74357 (* 1 = 5.74357 loss)
I0522 21:46:02.387346 26558 sgd_solver.cpp:112] Iteration 400380, lr = 0.0001
I0522 21:46:06.253836 26558 solver.cpp:239] Iteration 400390 (2.57461 iter/s, 3.88409s/10 iters), loss = 6.68967
I0522 21:46:06.253875 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.68967 (* 1 = 6.68967 loss)
I0522 21:46:07.159005 26558 sgd_solver.cpp:112] Iteration 400390, lr = 0.0001
I0522 21:46:13.745596 26558 solver.cpp:239] Iteration 400400 (1.33486 iter/s, 7.49142s/10 iters), loss = 5.97753
I0522 21:46:13.745638 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.97753 (* 1 = 5.97753 loss)
I0522 21:46:13.759135 26558 sgd_solver.cpp:112] Iteration 400400, lr = 0.0001
I0522 21:46:16.912294 26558 solver.cpp:239] Iteration 400410 (3.15804 iter/s, 3.16652s/10 iters), loss = 7.32717
I0522 21:46:16.912333 26558 solver.cpp:258]     Train net output #0: softmax_loss = 7.32717 (* 1 = 7.32717 loss)
I0522 21:46:16.959800 26558 sgd_solver.cpp:112] Iteration 400410, lr = 0.0001
I0522 21:46:21.119335 26558 solver.cpp:239] Iteration 400420 (2.37709 iter/s, 4.20682s/10 iters), loss = 6.0386
I0522 21:46:21.119375 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.0386 (* 1 = 6.0386 loss)
I0522 21:46:21.137228 26558 sgd_solver.cpp:112] Iteration 400420, lr = 0.0001
I0522 21:46:27.738390 26558 solver.cpp:239] Iteration 400430 (1.51086 iter/s, 6.61875s/10 iters), loss = 5.54272
I0522 21:46:27.738443 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.54272 (* 1 = 5.54272 loss)
I0522 21:46:28.475167 26558 sgd_solver.cpp:112] Iteration 400430, lr = 0.0001
I0522 21:46:37.006803 26558 solver.cpp:239] Iteration 400440 (1.07898 iter/s, 9.268s/10 iters), loss = 7.65369
I0522 21:46:37.007058 26558 solver.cpp:258]     Train net output #0: softmax_loss = 7.65369 (* 1 = 7.65369 loss)
I0522 21:46:37.024736 26558 sgd_solver.cpp:112] Iteration 400440, lr = 0.0001
I0522 21:46:43.095618 26558 solver.cpp:239] Iteration 400450 (1.64248 iter/s, 6.08834s/10 iters), loss = 7.3669
I0522 21:46:43.095654 26558 solver.cpp:258]     Train net output #0: softmax_loss = 7.3669 (* 1 = 7.3669 loss)
I0522 21:46:43.292112 26558 sgd_solver.cpp:112] Iteration 400450, lr = 0.0001
I0522 21:46:47.178148 26558 solver.cpp:239] Iteration 400460 (2.44959 iter/s, 4.08232s/10 iters), loss = 5.60198
I0522 21:46:47.178189 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.60198 (* 1 = 5.60198 loss)
I0522 21:46:47.432966 26558 sgd_solver.cpp:112] Iteration 400460, lr = 0.0001
I0522 21:46:55.425789 26558 solver.cpp:239] Iteration 400470 (1.21252 iter/s, 8.24726s/10 iters), loss = 6.10855
I0522 21:46:55.425853 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.10855 (* 1 = 6.10855 loss)
I0522 21:46:56.066952 26558 sgd_solver.cpp:112] Iteration 400470, lr = 0.0001
I0522 21:47:01.772006 26558 solver.cpp:239] Iteration 400480 (1.57582 iter/s, 6.34591s/10 iters), loss = 5.18017
I0522 21:47:01.772047 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.18017 (* 1 = 5.18017 loss)
I0522 21:47:02.708377 26558 sgd_solver.cpp:112] Iteration 400480, lr = 0.0001
I0522 21:47:10.346786 26558 solver.cpp:239] Iteration 400490 (1.16626 iter/s, 8.5744s/10 iters), loss = 6.04779
I0522 21:47:10.346990 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.04779 (* 1 = 6.04779 loss)
I0522 21:47:11.274641 26558 sgd_solver.cpp:112] Iteration 400490, lr = 0.0001
I0522 21:47:17.331558 26558 solver.cpp:239] Iteration 400500 (1.43178 iter/s, 6.98432s/10 iters), loss = 6.57468
I0522 21:47:17.331590 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.57468 (* 1 = 6.57468 loss)
I0522 21:47:17.425748 26558 sgd_solver.cpp:112] Iteration 400500, lr = 0.0001
I0522 21:47:23.970754 26558 solver.cpp:239] Iteration 400510 (1.50627 iter/s, 6.6389s/10 iters), loss = 6.43784
I0522 21:47:23.970794 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.43784 (* 1 = 6.43784 loss)
I0522 21:47:24.893203 26558 sgd_solver.cpp:112] Iteration 400510, lr = 0.0001
I0522 21:47:34.138231 26558 solver.cpp:239] Iteration 400520 (0.983572 iter/s, 10.167s/10 iters), loss = 5.75896
I0522 21:47:34.138298 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.75896 (* 1 = 5.75896 loss)
I0522 21:47:34.154846 26558 sgd_solver.cpp:112] Iteration 400520, lr = 0.0001
I0522 21:47:37.084188 26558 solver.cpp:239] Iteration 400530 (3.3947 iter/s, 2.94577s/10 iters), loss = 5.83295
I0522 21:47:37.084223 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.83295 (* 1 = 5.83295 loss)
I0522 21:47:37.101881 26558 sgd_solver.cpp:112] Iteration 400530, lr = 0.0001
I0522 21:47:42.768816 26558 solver.cpp:239] Iteration 400540 (1.75921 iter/s, 5.68435s/10 iters), loss = 7.00397
I0522 21:47:42.769037 26558 solver.cpp:258]     Train net output #0: softmax_loss = 7.00397 (* 1 = 7.00397 loss)
I0522 21:47:43.472614 26558 sgd_solver.cpp:112] Iteration 400540, lr = 0.0001
I0522 21:47:49.748126 26558 solver.cpp:239] Iteration 400550 (1.43291 iter/s, 6.97883s/10 iters), loss = 6.79748
I0522 21:47:49.748176 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.79748 (* 1 = 6.79748 loss)
I0522 21:47:50.589972 26558 sgd_solver.cpp:112] Iteration 400550, lr = 0.0001
I0522 21:47:57.893915 26558 solver.cpp:239] Iteration 400560 (1.22768 iter/s, 8.14542s/10 iters), loss = 6.11625
I0522 21:47:57.893957 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.11625 (* 1 = 6.11625 loss)
I0522 21:47:58.599778 26558 sgd_solver.cpp:112] Iteration 400560, lr = 0.0001
I0522 21:48:02.744058 26558 solver.cpp:239] Iteration 400570 (2.0619 iter/s, 4.84989s/10 iters), loss = 6.52488
I0522 21:48:02.744119 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.52488 (* 1 = 6.52488 loss)
I0522 21:48:03.544406 26558 sgd_solver.cpp:112] Iteration 400570, lr = 0.0001
I0522 21:48:12.902925 26558 solver.cpp:239] Iteration 400580 (0.984407 iter/s, 10.1584s/10 iters), loss = 6.77736
I0522 21:48:12.903132 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.77736 (* 1 = 6.77736 loss)
I0522 21:48:13.152515 26558 sgd_solver.cpp:112] Iteration 400580, lr = 0.0001
I0522 21:48:19.159569 26558 solver.cpp:239] Iteration 400590 (1.59842 iter/s, 6.25619s/10 iters), loss = 6.72879
I0522 21:48:19.159612 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.72879 (* 1 = 6.72879 loss)
I0522 21:48:20.105651 26558 sgd_solver.cpp:112] Iteration 400590, lr = 0.0001
I0522 21:48:22.893908 26558 solver.cpp:239] Iteration 400600 (2.67799 iter/s, 3.73414s/10 iters), loss = 6.05459
I0522 21:48:22.893949 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.05459 (* 1 = 6.05459 loss)
I0522 21:48:22.947446 26558 sgd_solver.cpp:112] Iteration 400600, lr = 0.0001
I0522 21:48:26.167053 26558 solver.cpp:239] Iteration 400610 (3.05534 iter/s, 3.27296s/10 iters), loss = 6.16444
I0522 21:48:26.167104 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.16444 (* 1 = 6.16444 loss)
I0522 21:48:26.380924 26558 sgd_solver.cpp:112] Iteration 400610, lr = 0.0001
I0522 21:48:33.790493 26558 solver.cpp:239] Iteration 400620 (1.3118 iter/s, 7.62309s/10 iters), loss = 5.55212
I0522 21:48:33.790529 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.55212 (* 1 = 5.55212 loss)
I0522 21:48:34.197818 26558 sgd_solver.cpp:112] Iteration 400620, lr = 0.0001
I0522 21:48:36.185727 26558 solver.cpp:239] Iteration 400630 (4.1752 iter/s, 2.39509s/10 iters), loss = 7.72085
I0522 21:48:36.185762 26558 solver.cpp:258]     Train net output #0: softmax_loss = 7.72085 (* 1 = 7.72085 loss)
I0522 21:48:36.238512 26558 sgd_solver.cpp:112] Iteration 400630, lr = 0.0001
I0522 21:48:45.542748 26558 solver.cpp:239] Iteration 400640 (1.06876 iter/s, 9.35661s/10 iters), loss = 6.18509
I0522 21:48:45.542968 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.18509 (* 1 = 6.18509 loss)
I0522 21:48:46.464448 26558 sgd_solver.cpp:112] Iteration 400640, lr = 0.0001
I0522 21:48:52.740185 26558 solver.cpp:239] Iteration 400650 (1.38948 iter/s, 7.19695s/10 iters), loss = 6.63486
I0522 21:48:52.740226 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.63486 (* 1 = 6.63486 loss)
I0522 21:48:52.758194 26558 sgd_solver.cpp:112] Iteration 400650, lr = 0.0001
I0522 21:49:01.464586 26558 solver.cpp:239] Iteration 400660 (1.14626 iter/s, 8.72401s/10 iters), loss = 6.51235
I0522 21:49:01.464627 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.51235 (* 1 = 6.51235 loss)
I0522 21:49:01.482301 26558 sgd_solver.cpp:112] Iteration 400660, lr = 0.0001
I0522 21:49:04.434494 26558 solver.cpp:239] Iteration 400670 (3.3673 iter/s, 2.96973s/10 iters), loss = 5.16986
I0522 21:49:04.434548 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.16986 (* 1 = 5.16986 loss)
I0522 21:49:05.340857 26558 sgd_solver.cpp:112] Iteration 400670, lr = 0.0001
I0522 21:49:15.128073 26558 solver.cpp:239] Iteration 400680 (0.935182 iter/s, 10.6931s/10 iters), loss = 5.94197
I0522 21:49:15.128115 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.94197 (* 1 = 5.94197 loss)
I0522 21:49:15.837739 26558 sgd_solver.cpp:112] Iteration 400680, lr = 0.0001
I0522 21:49:20.556638 26558 solver.cpp:239] Iteration 400690 (1.8422 iter/s, 5.4283s/10 iters), loss = 6.48646
I0522 21:49:20.556682 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.48646 (* 1 = 6.48646 loss)
I0522 21:49:21.518759 26558 sgd_solver.cpp:112] Iteration 400690, lr = 0.0001
I0522 21:49:26.759927 26558 solver.cpp:239] Iteration 400700 (1.61212 iter/s, 6.203s/10 iters), loss = 6.00873
I0522 21:49:26.759965 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.00873 (* 1 = 6.00873 loss)
I0522 21:49:27.009879 26558 sgd_solver.cpp:112] Iteration 400700, lr = 0.0001
I0522 21:49:34.005507 26558 solver.cpp:239] Iteration 400710 (1.38021 iter/s, 7.24525s/10 iters), loss = 6.96207
I0522 21:49:34.005548 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.96207 (* 1 = 6.96207 loss)
I0522 21:49:34.931162 26558 sgd_solver.cpp:112] Iteration 400710, lr = 0.0001
I0522 21:49:39.624260 26558 solver.cpp:239] Iteration 400720 (1.77984 iter/s, 5.61849s/10 iters), loss = 5.96886
I0522 21:49:39.624299 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.96886 (* 1 = 5.96886 loss)
I0522 21:49:39.976508 26558 sgd_solver.cpp:112] Iteration 400720, lr = 0.0001
I0522 21:49:46.443116 26558 solver.cpp:239] Iteration 400730 (1.46659 iter/s, 6.81853s/10 iters), loss = 6.89083
I0522 21:49:46.443393 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.89083 (* 1 = 6.89083 loss)
I0522 21:49:47.351485 26558 sgd_solver.cpp:112] Iteration 400730, lr = 0.0001
I0522 21:49:56.804275 26558 solver.cpp:239] Iteration 400740 (0.965204 iter/s, 10.3605s/10 iters), loss = 6.94317
I0522 21:49:56.804320 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.94317 (* 1 = 6.94317 loss)
I0522 21:49:56.821811 26558 sgd_solver.cpp:112] Iteration 400740, lr = 0.0001
I0522 21:50:01.286224 26558 solver.cpp:239] Iteration 400750 (2.23129 iter/s, 4.48172s/10 iters), loss = 6.47699
I0522 21:50:01.286262 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.47699 (* 1 = 6.47699 loss)
I0522 21:50:02.179848 26558 sgd_solver.cpp:112] Iteration 400750, lr = 0.0001
I0522 21:50:06.956574 26558 solver.cpp:239] Iteration 400760 (1.76364 iter/s, 5.67008s/10 iters), loss = 7.18834
I0522 21:50:06.956614 26558 solver.cpp:258]     Train net output #0: softmax_loss = 7.18834 (* 1 = 7.18834 loss)
I0522 21:50:06.974440 26558 sgd_solver.cpp:112] Iteration 400760, lr = 0.0001
I0522 21:50:10.738893 26558 solver.cpp:239] Iteration 400770 (2.64402 iter/s, 3.78212s/10 iters), loss = 6.37739
I0522 21:50:10.738941 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.37739 (* 1 = 6.37739 loss)
I0522 21:50:11.691987 26558 sgd_solver.cpp:112] Iteration 400770, lr = 0.0001
I0522 21:50:20.280498 26558 solver.cpp:239] Iteration 400780 (1.04809 iter/s, 9.54118s/10 iters), loss = 5.57836
I0522 21:50:20.280665 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.57836 (* 1 = 5.57836 loss)
I0522 21:50:21.186841 26558 sgd_solver.cpp:112] Iteration 400780, lr = 0.0001
I0522 21:50:27.248530 26558 solver.cpp:239] Iteration 400790 (1.43521 iter/s, 6.96761s/10 iters), loss = 6.18785
I0522 21:50:27.248572 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.18785 (* 1 = 6.18785 loss)
I0522 21:50:27.563593 26558 sgd_solver.cpp:112] Iteration 400790, lr = 0.0001
I0522 21:50:31.762756 26558 solver.cpp:239] Iteration 400800 (2.21533 iter/s, 4.514s/10 iters), loss = 6.9559
I0522 21:50:31.762792 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.9559 (* 1 = 6.9559 loss)
I0522 21:50:32.385072 26558 sgd_solver.cpp:112] Iteration 400800, lr = 0.0001
I0522 21:50:41.235337 26558 solver.cpp:239] Iteration 400810 (1.05573 iter/s, 9.47215s/10 iters), loss = 7.00512
I0522 21:50:41.235394 26558 solver.cpp:258]     Train net output #0: softmax_loss = 7.00512 (* 1 = 7.00512 loss)
I0522 21:50:42.168148 26558 sgd_solver.cpp:112] Iteration 400810, lr = 0.0001
I0522 21:50:49.121801 26558 solver.cpp:239] Iteration 400820 (1.26805 iter/s, 7.8861s/10 iters), loss = 7.19968
I0522 21:50:49.121843 26558 solver.cpp:258]     Train net output #0: softmax_loss = 7.19968 (* 1 = 7.19968 loss)
I0522 21:50:49.139696 26558 sgd_solver.cpp:112] Iteration 400820, lr = 0.0001
I0522 21:50:52.866211 26558 solver.cpp:239] Iteration 400830 (2.67079 iter/s, 3.74421s/10 iters), loss = 6.92415
I0522 21:50:52.866466 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.92415 (* 1 = 6.92415 loss)
I0522 21:50:53.794275 26558 sgd_solver.cpp:112] Iteration 400830, lr = 0.0001
I0522 21:51:03.180347 26558 solver.cpp:239] Iteration 400840 (0.969604 iter/s, 10.3135s/10 iters), loss = 6.70723
I0522 21:51:03.180403 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.70723 (* 1 = 6.70723 loss)
I0522 21:51:04.140326 26558 sgd_solver.cpp:112] Iteration 400840, lr = 0.0001
I0522 21:51:09.279803 26558 solver.cpp:239] Iteration 400850 (1.63957 iter/s, 6.09914s/10 iters), loss = 5.91299
I0522 21:51:09.279862 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.91299 (* 1 = 5.91299 loss)
I0522 21:51:10.201170 26558 sgd_solver.cpp:112] Iteration 400850, lr = 0.0001
I0522 21:51:19.245064 26558 solver.cpp:239] Iteration 400860 (1.00353 iter/s, 9.96481s/10 iters), loss = 6.73339
I0522 21:51:19.245106 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.73339 (* 1 = 6.73339 loss)
I0522 21:51:20.198940 26558 sgd_solver.cpp:112] Iteration 400860, lr = 0.0001
I0522 21:51:24.449124 26558 solver.cpp:239] Iteration 400870 (1.92167 iter/s, 5.20381s/10 iters), loss = 5.70079
I0522 21:51:24.449378 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.70079 (* 1 = 5.70079 loss)
I0522 21:51:24.467267 26558 sgd_solver.cpp:112] Iteration 400870, lr = 0.0001
I0522 21:51:30.927597 26558 solver.cpp:239] Iteration 400880 (1.54369 iter/s, 6.47798s/10 iters), loss = 6.89259
I0522 21:51:30.927639 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.89259 (* 1 = 6.89259 loss)
I0522 21:51:31.874647 26558 sgd_solver.cpp:112] Iteration 400880, lr = 0.0001
I0522 21:51:39.571851 26558 solver.cpp:239] Iteration 400890 (1.15689 iter/s, 8.64387s/10 iters), loss = 7.47079
I0522 21:51:39.571892 26558 solver.cpp:258]     Train net output #0: softmax_loss = 7.47079 (* 1 = 7.47079 loss)
I0522 21:51:40.465651 26558 sgd_solver.cpp:112] Iteration 400890, lr = 0.0001
I0522 21:51:48.753948 26558 solver.cpp:239] Iteration 400900 (1.08912 iter/s, 9.18169s/10 iters), loss = 5.95391
I0522 21:51:48.753993 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.95391 (* 1 = 5.95391 loss)
I0522 21:51:48.845094 26558 sgd_solver.cpp:112] Iteration 400900, lr = 0.0001
I0522 21:51:54.317375 26558 solver.cpp:239] Iteration 400910 (1.79754 iter/s, 5.56316s/10 iters), loss = 6.68317
I0522 21:51:54.317415 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.68317 (* 1 = 6.68317 loss)
I0522 21:51:54.335317 26558 sgd_solver.cpp:112] Iteration 400910, lr = 0.0001
I0522 21:51:56.536303 26558 solver.cpp:239] Iteration 400920 (4.50697 iter/s, 2.21879s/10 iters), loss = 7.26016
I0522 21:51:56.536530 26558 solver.cpp:258]     Train net output #0: softmax_loss = 7.26016 (* 1 = 7.26016 loss)
I0522 21:51:56.554318 26558 sgd_solver.cpp:112] Iteration 400920, lr = 0.0001
I0522 21:52:02.207877 26558 solver.cpp:239] Iteration 400930 (1.76331 iter/s, 5.67114s/10 iters), loss = 6.60087
I0522 21:52:02.207921 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.60087 (* 1 = 6.60087 loss)
I0522 21:52:02.456187 26558 sgd_solver.cpp:112] Iteration 400930, lr = 0.0001
I0522 21:52:09.855960 26558 solver.cpp:239] Iteration 400940 (1.30758 iter/s, 7.64772s/10 iters), loss = 6.08984
I0522 21:52:09.856011 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.08984 (* 1 = 6.08984 loss)
I0522 21:52:10.612205 26558 sgd_solver.cpp:112] Iteration 400940, lr = 0.0001
I0522 21:52:14.846252 26558 solver.cpp:239] Iteration 400950 (2.004 iter/s, 4.99001s/10 iters), loss = 6.79614
I0522 21:52:14.846303 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.79614 (* 1 = 6.79614 loss)
I0522 21:52:15.808468 26558 sgd_solver.cpp:112] Iteration 400950, lr = 0.0001
I0522 21:52:21.646235 26558 solver.cpp:239] Iteration 400960 (1.47066 iter/s, 6.79966s/10 iters), loss = 6.44251
I0522 21:52:21.646275 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.44251 (* 1 = 6.44251 loss)
I0522 21:52:21.664157 26558 sgd_solver.cpp:112] Iteration 400960, lr = 0.0001
I0522 21:52:27.604509 26558 solver.cpp:239] Iteration 400970 (1.67842 iter/s, 5.95799s/10 iters), loss = 7.18121
I0522 21:52:27.604709 26558 solver.cpp:258]     Train net output #0: softmax_loss = 7.18121 (* 1 = 7.18121 loss)
I0522 21:52:28.360785 26558 sgd_solver.cpp:112] Iteration 400970, lr = 0.0001
I0522 21:52:37.741379 26558 solver.cpp:239] Iteration 400980 (0.986555 iter/s, 10.1363s/10 iters), loss = 5.81407
I0522 21:52:37.741433 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.81407 (* 1 = 5.81407 loss)
I0522 21:52:38.668668 26558 sgd_solver.cpp:112] Iteration 400980, lr = 0.0001
I0522 21:52:46.065241 26558 solver.cpp:239] Iteration 400990 (1.20142 iter/s, 8.32347s/10 iters), loss = 6.09916
I0522 21:52:46.065296 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.09916 (* 1 = 6.09916 loss)
I0522 21:52:46.956377 26558 sgd_solver.cpp:112] Iteration 400990, lr = 0.0001
I0522 21:52:59.089388 26558 solver.cpp:239] Iteration 401000 (0.767837 iter/s, 13.0236s/10 iters), loss = 7.73893
I0522 21:52:59.089561 26558 solver.cpp:258]     Train net output #0: softmax_loss = 7.73893 (* 1 = 7.73893 loss)
I0522 21:53:00.148918 26558 sgd_solver.cpp:112] Iteration 401000, lr = 0.0001
I0522 21:53:10.344379 26558 solver.cpp:239] Iteration 401010 (0.888543 iter/s, 11.2544s/10 iters), loss = 7.03168
I0522 21:53:10.344424 26558 solver.cpp:258]     Train net output #0: softmax_loss = 7.03168 (* 1 = 7.03168 loss)
I0522 21:53:10.825211 26558 sgd_solver.cpp:112] Iteration 401010, lr = 0.0001
I0522 21:53:19.804664 26558 solver.cpp:239] Iteration 401020 (1.0571 iter/s, 9.45987s/10 iters), loss = 6.49289
I0522 21:53:19.804705 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.49289 (* 1 = 6.49289 loss)
I0522 21:53:21.012456 26558 sgd_solver.cpp:112] Iteration 401020, lr = 0.0001
I0522 21:53:34.608407 26558 solver.cpp:239] Iteration 401030 (0.675534 iter/s, 14.8031s/10 iters), loss = 5.77789
I0522 21:53:34.608583 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.77789 (* 1 = 5.77789 loss)
I0522 21:53:35.740531 26558 sgd_solver.cpp:112] Iteration 401030, lr = 0.0001
I0522 21:53:48.735741 26558 solver.cpp:239] Iteration 401040 (0.707884 iter/s, 14.1266s/10 iters), loss = 6.12236
I0522 21:53:48.735790 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.12236 (* 1 = 6.12236 loss)
I0522 21:53:49.738802 26558 sgd_solver.cpp:112] Iteration 401040, lr = 0.0001
I0522 21:53:56.883038 26558 solver.cpp:239] Iteration 401050 (1.22746 iter/s, 8.14693s/10 iters), loss = 6.44732
I0522 21:53:56.883080 26558 solver.cpp:258]     Train net output #0: softmax_loss = 6.44732 (* 1 = 6.44732 loss)
I0522 21:53:58.038877 26558 sgd_solver.cpp:112] Iteration 401050, lr = 0.0001
I0522 21:54:06.862923 26558 solver.cpp:239] Iteration 401060 (1.00206 iter/s, 9.97945s/10 iters), loss = 5.88862
I0522 21:54:06.863093 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.88862 (* 1 = 5.88862 loss)
I0522 21:54:06.885289 26558 sgd_solver.cpp:112] Iteration 401060, lr = 0.0001
I0522 21:54:19.026814 26558 solver.cpp:239] Iteration 401070 (0.822147 iter/s, 12.1633s/10 iters), loss = 5.66153
I0522 21:54:19.026857 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.66153 (* 1 = 5.66153 loss)
I0522 21:54:20.076553 26558 sgd_solver.cpp:112] Iteration 401070, lr = 0.0001
I0522 21:54:28.395581 26558 solver.cpp:239] Iteration 401080 (1.06742 iter/s, 9.36835s/10 iters), loss = 5.87798
I0522 21:54:28.395630 26558 solver.cpp:258]     Train net output #0: softmax_loss = 5.87798 (* 1 = 5.87798 loss)
I0522 21:54:29.471331 26558 sgd_solver.cpp:112] Iteration 401080, lr = 0.0001
