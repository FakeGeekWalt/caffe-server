./build/tools/caffe: /home/zkx/anaconda2/lib/libtiff.so.5: no version information available (required by /home/zkx/env/opencv/lib/libopencv_highgui.so.2.4)
I0523 21:53:45.017961 11835 upgrade_proto.cpp:1084] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': models/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/solver.prototxt
I0523 21:53:45.018290 11835 upgrade_proto.cpp:1091] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W0523 21:53:45.018306 11835 upgrade_proto.cpp:1093] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I0523 21:53:45.018474 11835 caffe.cpp:204] Using GPUs 0, 1, 2, 3
I0523 21:53:45.040755 11835 caffe.cpp:209] GPU 0: TITAN Xp
I0523 21:53:45.041476 11835 caffe.cpp:209] GPU 1: TITAN Xp
I0523 21:53:45.042140 11835 caffe.cpp:209] GPU 2: TITAN Xp
I0523 21:53:45.042799 11835 caffe.cpp:209] GPU 3: TITAN Xp
I0523 21:53:46.084122 11835 solver.cpp:45] Initializing solver from parameters: 
base_lr: 0.1
display: 10
max_iter: 160000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx"
solver_mode: GPU
device_id: 0
net: "models/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/train.prototxt"
train_state {
  level: 0
  stage: ""
}
snapshot_after_train: true
stepvalue: 80000
stepvalue: 120000
stepvalue: 140000
iter_size: 1
type: "SGD"
weights: "/home/zkx/Project/O2N/DepthwiseConvolution-master/MobileFaceNet-master/face_snapshot/MobileFaceNet.caffemodel"
I0523 21:53:46.084302 11835 solver.cpp:102] Creating training net from net file: models/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/train.prototxt
I0523 21:53:46.089758 11835 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: models/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/train.prototxt
I0523 21:53:46.089781 11835 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0523 21:53:46.092275 11835 net.cpp:51] Initializing net from parameters: 
name: "2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_train"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  transform_param {
    scale: 0.0078125
    mean_value: 127.5
    mean_value: 127.5
    mean_value: 127.5
  }
  image_data_param {
    source: "/home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/image_list/train_list/Combine_base_Cmul-Asia-beid-cap10-ZheD_list.txt_label.txt"
    batch_size: 64
    shuffle: true
    root_folder: "/home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/fc_0.35_112x96/"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1/bn"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv1/scale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "PReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_dw"
  type: "DepthwiseConvolution"
  bottom: "conv1"
  top: "conv1_dw"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv1_dw/bn"
  type: "BatchNorm"
  bottom: "conv1_dw"
  top: "conv1_dw"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv1_dw/scale"
  type: "Scale"
  bottom: "conv1_dw"
  top: "conv1_dw"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_dw"
  type: "PReLU"
  bottom: "conv1_dw"
  top: "conv1_dw"
}
layer {
  name: "conv2_ex"
  type: "Convolution"
  bottom: "conv1_dw"
  top: "conv2_ex"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_ex/bn"
  type: "BatchNorm"
  bottom: "conv2_ex"
  top: "conv2_ex"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_ex/scale"
  type: "Scale"
  bottom: "conv2_ex"
  top: "conv2_ex"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_ex"
  type: "PReLU"
  bottom: "conv2_ex"
  top: "conv2_ex"
}
layer {
  name: "conv2_dw"
  type: "DepthwiseConvolution"
  bottom: "conv2_ex"
  top: "conv2_dw"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 128
    stride: 2
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "conv2_dw/bn"
  type: "BatchNorm"
  bottom: "conv2_dw"
  top: "conv2_dw"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_dw/scale"
  type: "Scale"
  bottom: "conv2_dw"
  top: "conv2_dw"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_dw"
  type: "PReLU"
  bottom: "conv2_dw"
  top: "conv2_dw"
}
layer {
  name: "conv2_em"
  type: "Convolution"
  bottom: "conv2_dw"
  top: "conv2_em"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_em/bn"
  type: "BatchNorm"
  bottom: "conv2_em"
  top: "conv2_em"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_em/scale"
  type: "Scale"
  bottom: "conv2_em"
  top: "conv2_em"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_1_ex"
  type: "Convolution"
  bottom: "conv2_em"
  top: "conv2_1_ex"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_1_ex/bn"
  type: "BatchNorm"
  bottom: "conv2_1_ex"
  top: "conv2_1_ex"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_1_ex/scale"
  type: "Scale"
  bottom: "conv2_1_ex"
  top: "conv2_1_ex"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1_ex"
  type: "PReLU"
  bottom: "conv2_1_ex"
  top: "conv2_1_ex"
}
layer {
  name: "conv2_1_dw"
  type: "DepthwiseConvolution"
  bottom: "conv2_1_ex"
  top: "conv2_1_dw"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 128
    stride: 1
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "conv2_1_dw/bn"
  type: "BatchNorm"
  bottom: "conv2_1_dw"
  top: "conv2_1_dw"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_1_dw/scale"
  type: "Scale"
  bottom: "conv2_1_dw"
  top: "conv2_1_dw"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1_dw"
  type: "PReLU"
  bottom: "conv2_1_dw"
  top: "conv2_1_dw"
}
layer {
  name: "conv2_1_em"
  type: "Convolution"
  bottom: "conv2_1_dw"
  top: "conv2_1_em"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_1_em/bn"
  type: "BatchNorm"
  bottom: "conv2_1_em"
  top: "conv2_1_em"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_1_em/scale"
  type: "Scale"
  bottom: "conv2_1_em"
  top: "conv2_1_em"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2_1"
  type: "Eltwise"
  bottom: "conv2_em"
  bottom: "conv2_1_em"
  top: "res2_1"
}
layer {
  name: "conv2_2_ex"
  type: "Convolution"
  bottom: "res2_1"
  top: "conv2_2_ex"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_2_ex/bn"
  type: "BatchNorm"
  bottom: "conv2_2_ex"
  top: "conv2_2_ex"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_2_ex/scale"
  type: "Scale"
  bottom: "conv2_2_ex"
  top: "conv2_2_ex"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2_ex"
  type: "PReLU"
  bottom: "conv2_2_ex"
  top: "conv2_2_ex"
}
layer {
  name: "conv2_2_dw"
  type: "DepthwiseConvolution"
  bottom: "conv2_2_ex"
  top: "conv2_2_dw"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 128
    stride: 1
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "conv2_2_dw/bn"
  type: "BatchNorm"
  bottom: "conv2_2_dw"
  top: "conv2_2_dw"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_2_dw/scale"
  type: "Scale"
  bottom: "conv2_2_dw"
  top: "conv2_2_dw"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2_dw"
  type: "PReLU"
  bottom: "conv2_2_dw"
  top: "conv2_2_dw"
}
layer {
  name: "conv2_2_em"
  type: "Convolution"
  bottom: "conv2_2_dw"
  top: "conv2_2_em"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_2_em/bn"
  type: "BatchNorm"
  bottom: "conv2_2_em"
  top: "conv2_2_em"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_2_em/scale"
  type: "Scale"
  bottom: "conv2_2_em"
  top: "conv2_2_em"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2_2"
  type: "Eltwise"
  bottom: "res2_1"
  bottom: "conv2_2_em"
  top: "res2_2"
}
layer {
  name: "conv2_3_ex"
  type: "Convolution"
  bottom: "res2_2"
  top: "conv2_3_ex"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_3_ex/bn"
  type: "BatchNorm"
  bottom: "conv2_3_ex"
  top: "conv2_3_ex"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_3_ex/scale"
  type: "Scale"
  bottom: "conv2_3_ex"
  top: "conv2_3_ex"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_3_ex"
  type: "PReLU"
  bottom: "conv2_3_ex"
  top: "conv2_3_ex"
}
layer {
  name: "conv2_3_dw"
  type: "DepthwiseConvolution"
  bottom: "conv2_3_ex"
  top: "conv2_3_dw"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 128
    stride: 1
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "conv2_3_dw/bn"
  type: "BatchNorm"
  bottom: "conv2_3_dw"
  top: "conv2_3_dw"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_3_dw/scale"
  type: "Scale"
  bottom: "conv2_3_dw"
  top: "conv2_3_dw"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_3_dw"
  type: "PReLU"
  bottom: "conv2_3_dw"
  top: "conv2_3_dw"
}
layer {
  name: "conv2_3_em"
  type: "Convolution"
  bottom: "conv2_3_dw"
  top: "conv2_3_em"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_3_em/bn"
  type: "BatchNorm"
  bottom: "conv2_3_em"
  top: "conv2_3_em"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_3_em/scale"
  type: "Scale"
  bottom: "conv2_3_em"
  top: "conv2_3_em"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2_3"
  type: "Eltwise"
  bottom: "res2_2"
  bottom: "conv2_3_em"
  top: "res2_3"
}
layer {
  name: "conv2_4_ex"
  type: "Convolution"
  bottom: "res2_3"
  top: "conv2_4_ex"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_4_ex/bn"
  type: "BatchNorm"
  bottom: "conv2_4_ex"
  top: "conv2_4_ex"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_4_ex/scale"
  type: "Scale"
  bottom: "conv2_4_ex"
  top: "conv2_4_ex"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_4_ex"
  type: "PReLU"
  bottom: "conv2_4_ex"
  top: "conv2_4_ex"
}
layer {
  name: "conv2_4_dw"
  type: "DepthwiseConvolution"
  bottom: "conv2_4_ex"
  top: "conv2_4_dw"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 128
    stride: 1
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "conv2_4_dw/bn"
  type: "BatchNorm"
  bottom: "conv2_4_dw"
  top: "conv2_4_dw"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_4_dw/scale"
  type: "Scale"
  bottom: "conv2_4_dw"
  top: "conv2_4_dw"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_4_dw"
  type: "PReLU"
  bottom: "conv2_4_dw"
  top: "conv2_4_dw"
}
layer {
  name: "conv2_4_em"
  type: "Convolution"
  bottom: "conv2_4_dw"
  top: "conv2_4_em"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_4_em/bn"
  type: "BatchNorm"
  bottom: "conv2_4_em"
  top: "conv2_4_em"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_4_em/scale"
  type: "Scale"
  bottom: "conv2_4_em"
  top: "conv2_4_em"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2_4"
  type: "Eltwise"
  bottom: "res2_3"
  bottom: "conv2_4_em"
  top: "res2_4"
}
layer {
  name: "conv3_ex"
  type: "Convolution"
  bottom: "res2_4"
  top: "conv3_ex"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_ex/bn"
  type: "BatchNorm"
  bottom: "conv3_ex"
  top: "conv3_ex"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_ex/scale"
  type: "Scale"
  bottom: "conv3_ex"
  top: "conv3_ex"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_ex"
  type: "PReLU"
  bottom: "conv3_ex"
  top: "conv3_ex"
}
layer {
  name: "conv3_dw"
  type: "DepthwiseConvolution"
  bottom: "conv3_ex"
  top: "conv3_dw"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 256
    stride: 2
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "conv3_dw/bn"
  type: "BatchNorm"
  bottom: "conv3_dw"
  top: "conv3_dw"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_dw/scale"
  type: "Scale"
  bottom: "conv3_dw"
  top: "conv3_dw"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_dw"
  type: "PReLU"
  bottom: "conv3_dw"
  top: "conv3_dw"
}
layer {
  name: "conv3_em"
  type: "Convolution"
  bottom: "conv3_dw"
  top: "conv3_em"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_em/bn"
  type: "BatchNorm"
  bottom: "conv3_em"
  top: "conv3_em"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_em/scale"
  type: "Scale"
  bottom: "conv3_em"
  top: "conv3_em"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_1_ex"
  type: "Convolution"
  bottom: "conv3_em"
  top: "conv3_1_ex"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_1_ex/bn"
  type: "BatchNorm"
  bottom: "conv3_1_ex"
  top: "conv3_1_ex"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_1_ex/scale"
  type: "Scale"
  bottom: "conv3_1_ex"
  top: "conv3_1_ex"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1_ex"
  type: "PReLU"
  bottom: "conv3_1_ex"
  top: "conv3_1_ex"
}
layer {
  name: "conv3_1_dw"
  type: "DepthwiseConvolution"
  bottom: "conv3_1_ex"
  top: "conv3_1_dw"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 256
    stride: 1
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "conv3_1_dw/bn"
  type: "BatchNorm"
  bottom: "conv3_1_dw"
  top: "conv3_1_dw"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_1_dw/scale"
  type: "Scale"
  bottom: "conv3_1_dw"
  top: "conv3_1_dw"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1_dw"
  type: "PReLU"
  bottom: "conv3_1_dw"
  top: "conv3_1_dw"
}
layer {
  name: "conv3_1_em"
  type: "Convolution"
  bottom: "conv3_1_dw"
  top: "conv3_1_em"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_1_em/bn"
  type: "BatchNorm"
  bottom: "conv3_1_em"
  top: "conv3_1_em"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_1_em/scale"
  type: "Scale"
  bottom: "conv3_1_em"
  top: "conv3_1_em"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3_1"
  type: "Eltwise"
  bottom: "conv3_em"
  bottom: "conv3_1_em"
  top: "res3_1"
}
layer {
  name: "conv3_2_ex"
  type: "Convolution"
  bottom: "res3_1"
  top: "conv3_2_ex"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_2_ex/bn"
  type: "BatchNorm"
  bottom: "conv3_2_ex"
  top: "conv3_2_ex"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_2_ex/scale"
  type: "Scale"
  bottom: "conv3_2_ex"
  top: "conv3_2_ex"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_2_ex"
  type: "PReLU"
  bottom: "conv3_2_ex"
  top: "conv3_2_ex"
}
layer {
  name: "conv3_2_dw"
  type: "DepthwiseConvolution"
  bottom: "conv3_2_ex"
  top: "conv3_2_dw"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 256
    stride: 1
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "conv3_2_dw/bn"
  type: "BatchNorm"
  bottom: "conv3_2_dw"
  top: "conv3_2_dw"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_2_dw/scale"
  type: "Scale"
  bottom: "conv3_2_dw"
  top: "conv3_2_dw"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_2_dw"
  type: "PReLU"
  bottom: "conv3_2_dw"
  top: "conv3_2_dw"
}
layer {
  name: "conv3_2_em"
  type: "Convolution"
  bottom: "conv3_2_dw"
  top: "conv3_2_em"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_2_em/bn"
  type: "BatchNorm"
  bottom: "conv3_2_em"
  top: "conv3_2_em"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_2_em/scale"
  type: "Scale"
  bottom: "conv3_2_em"
  top: "conv3_2_em"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3_2"
  type: "Eltwise"
  bottom: "res3_1"
  bottom: "conv3_2_em"
  top: "res3_2"
}
layer {
  name: "conv3_3_ex"
  type: "Convolution"
  bottom: "res3_2"
  top: "conv3_3_ex"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_3_ex/bn"
  type: "BatchNorm"
  bottom: "conv3_3_ex"
  top: "conv3_3_ex"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_3_ex/scale"
  type: "Scale"
  bottom: "conv3_3_ex"
  top: "conv3_3_ex"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_3_ex"
  type: "PReLU"
  bottom: "conv3_3_ex"
  top: "conv3_3_ex"
}
layer {
  name: "conv3_3_dw"
  type: "DepthwiseConvolution"
  bottom: "conv3_3_ex"
  top: "conv3_3_dw"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 256
    stride: 1
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "conv3_3_dw/bn"
  type: "BatchNorm"
  bottom: "conv3_3_dw"
  top: "conv3_3_dw"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_3_dw/scale"
  type: "Scale"
  bottom: "conv3_3_dw"
  top: "conv3_3_dw"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_3_dw"
  type: "PReLU"
  bottom: "conv3_3_dw"
  top: "conv3_3_dw"
}
layer {
  name: "conv3_3_em"
  type: "Convolution"
  bottom: "conv3_3_dw"
  top: "conv3_3_em"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_3_em/bn"
  type: "BatchNorm"
  bottom: "conv3_3_em"
  top: "conv3_3_em"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_3_em/scale"
  type: "Scale"
  bottom: "conv3_3_em"
  top: "conv3_3_em"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3_3"
  type: "Eltwise"
  bottom: "res3_2"
  bottom: "conv3_3_em"
  top: "res3_3"
}
layer {
  name: "conv3_4_ex"
  type: "Convolution"
  bottom: "res3_3"
  top: "conv3_4_ex"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_4_ex/bn"
  type: "BatchNorm"
  bottom: "conv3_4_ex"
  top: "conv3_4_ex"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_4_ex/scale"
  type: "Scale"
  bottom: "conv3_4_ex"
  top: "conv3_4_ex"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_4_ex"
  type: "PReLU"
  bottom: "conv3_4_ex"
  top: "conv3_4_ex"
}
layer {
  name: "conv3_4_dw"
  type: "DepthwiseConvolution"
  bottom: "conv3_4_ex"
  top: "conv3_4_dw"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 256
    stride: 1
    weight_filler {
      type: "xavier"
    }
    engine: CAFFE
  }
}
layer {
  name: "conv3_4_dw/bn"
  type: "BatchNorm"
  bottom: "conv3_4_dw"
  top: "conv3_4_dw"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_4_dw/scale"
  type: "Scale"
  bottom: "conv3_4_dw"
  top: "conv3_4_dw"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_4_dw"
  type: "PReLU"
  bottom: "conv3_4_dw"
  top: "conv3_4_dw"
}
layer {
  name: "conv3_4_em"
  type: "Convolution"
  bottom: "conv3_4_dw"
  top: "conv3_4_em"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3_4_em/bn"
  type: "BatchNorm"
  bottom: "conv3_4_em"
  top: "conv3_4_em"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_4_em/scale"
  type: "Scale"
  bottom: "conv3_4_em"
  top: "conv3_4_em"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3_4"
  type: "Eltwise"
  bottom: "res3_3"
  bottom: "conv3_4_em"
  top: "
I0523 21:53:46.093322 11835 layer_factory.hpp:77] Creating layer data
I0523 21:53:46.093382 11835 net.cpp:84] Creating Layer data
I0523 21:53:46.093391 11835 net.cpp:380] data -> data
I0523 21:53:46.093422 11835 net.cpp:380] data -> label
I0523 21:53:46.093442 11835 image_data_layer.cpp:38] Opening file /home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/image_list/train_list/Combine_base_Cmul-Asia-beid-cap10-ZheD_list.txt_label.txt
I0523 21:53:46.926336 11835 image_data_layer.cpp:53] Shuffling data
I0523 21:53:47.494565 11835 image_data_layer.cpp:63] A total of 2075438 images.
I0523 21:53:47.497694 11835 image_data_layer.cpp:90] output data size: 64,3,112,96
I0523 21:53:47.531455 11835 net.cpp:122] Setting up data
I0523 21:53:47.531508 11835 net.cpp:129] Top shape: 64 3 112 96 (2064384)
I0523 21:53:47.531518 11835 net.cpp:129] Top shape: 64 (64)
I0523 21:53:47.531549 11835 net.cpp:137] Memory required for data: 8257792
I0523 21:53:47.531596 11835 layer_factory.hpp:77] Creating layer label_data_1_split
I0523 21:53:47.531713 11835 net.cpp:84] Creating Layer label_data_1_split
I0523 21:53:47.531723 11835 net.cpp:406] label_data_1_split <- label
I0523 21:53:47.531772 11835 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0523 21:53:47.531790 11835 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0523 21:53:47.531874 11835 net.cpp:122] Setting up label_data_1_split
I0523 21:53:47.531884 11835 net.cpp:129] Top shape: 64 (64)
I0523 21:53:47.531890 11835 net.cpp:129] Top shape: 64 (64)
I0523 21:53:47.531893 11835 net.cpp:137] Memory required for data: 8258304
I0523 21:53:47.531898 11835 layer_factory.hpp:77] Creating layer conv1
I0523 21:53:47.531981 11835 net.cpp:84] Creating Layer conv1
I0523 21:53:47.531988 11835 net.cpp:406] conv1 <- data
I0523 21:53:47.532004 11835 net.cpp:380] conv1 -> conv1
I0523 21:53:48.256599 11835 net.cpp:122] Setting up conv1
I0523 21:53:48.256646 11835 net.cpp:129] Top shape: 64 64 56 48 (11010048)
I0523 21:53:48.256652 11835 net.cpp:137] Memory required for data: 52298496
I0523 21:53:48.256687 11835 layer_factory.hpp:77] Creating layer conv1/bn
I0523 21:53:48.256741 11835 net.cpp:84] Creating Layer conv1/bn
I0523 21:53:48.256748 11835 net.cpp:406] conv1/bn <- conv1
I0523 21:53:48.256758 11835 net.cpp:367] conv1/bn -> conv1 (in-place)
I0523 21:53:48.256974 11835 net.cpp:122] Setting up conv1/bn
I0523 21:53:48.256983 11835 net.cpp:129] Top shape: 64 64 56 48 (11010048)
I0523 21:53:48.256988 11835 net.cpp:137] Memory required for data: 96338688
I0523 21:53:48.257005 11835 layer_factory.hpp:77] Creating layer conv1/scale
I0523 21:53:48.257024 11835 net.cpp:84] Creating Layer conv1/scale
I0523 21:53:48.257028 11835 net.cpp:406] conv1/scale <- conv1
I0523 21:53:48.257035 11835 net.cpp:367] conv1/scale -> conv1 (in-place)
I0523 21:53:48.257084 11835 layer_factory.hpp:77] Creating layer conv1/scale
I0523 21:53:48.257205 11835 net.cpp:122] Setting up conv1/scale
I0523 21:53:48.257215 11835 net.cpp:129] Top shape: 64 64 56 48 (11010048)
I0523 21:53:48.257218 11835 net.cpp:137] Memory required for data: 140378880
I0523 21:53:48.257226 11835 layer_factory.hpp:77] Creating layer relu1
I0523 21:53:48.257238 11835 net.cpp:84] Creating Layer relu1
I0523 21:53:48.257244 11835 net.cpp:406] relu1 <- conv1
I0523 21:53:48.257249 11835 net.cpp:367] relu1 -> conv1 (in-place)
I0523 21:53:48.259124 11835 net.cpp:122] Setting up relu1
I0523 21:53:48.259142 11835 net.cpp:129] Top shape: 64 64 56 48 (11010048)
I0523 21:53:48.259147 11835 net.cpp:137] Memory required for data: 184419072
I0523 21:53:48.259153 11835 layer_factory.hpp:77] Creating layer conv1_dw
I0523 21:53:48.259179 11835 net.cpp:84] Creating Layer conv1_dw
I0523 21:53:48.259184 11835 net.cpp:406] conv1_dw <- conv1
I0523 21:53:48.259193 11835 net.cpp:380] conv1_dw -> conv1_dw
I0523 21:53:48.261234 11835 net.cpp:122] Setting up conv1_dw
I0523 21:53:48.261250 11835 net.cpp:129] Top shape: 64 64 56 48 (11010048)
I0523 21:53:48.261255 11835 net.cpp:137] Memory required for data: 228459264
I0523 21:53:48.261265 11835 layer_factory.hpp:77] Creating layer conv1_dw/bn
I0523 21:53:48.261273 11835 net.cpp:84] Creating Layer conv1_dw/bn
I0523 21:53:48.261278 11835 net.cpp:406] conv1_dw/bn <- conv1_dw
I0523 21:53:48.261284 11835 net.cpp:367] conv1_dw/bn -> conv1_dw (in-place)
I0523 21:53:48.261497 11835 net.cpp:122] Setting up conv1_dw/bn
I0523 21:53:48.261505 11835 net.cpp:129] Top shape: 64 64 56 48 (11010048)
I0523 21:53:48.261509 11835 net.cpp:137] Memory required for data: 272499456
I0523 21:53:48.261521 11835 layer_factory.hpp:77] Creating layer conv1_dw/scale
I0523 21:53:48.261531 11835 net.cpp:84] Creating Layer conv1_dw/scale
I0523 21:53:48.261536 11835 net.cpp:406] conv1_dw/scale <- conv1_dw
I0523 21:53:48.261541 11835 net.cpp:367] conv1_dw/scale -> conv1_dw (in-place)
I0523 21:53:48.261581 11835 layer_factory.hpp:77] Creating layer conv1_dw/scale
I0523 21:53:48.261698 11835 net.cpp:122] Setting up conv1_dw/scale
I0523 21:53:48.261724 11835 net.cpp:129] Top shape: 64 64 56 48 (11010048)
I0523 21:53:48.261729 11835 net.cpp:137] Memory required for data: 316539648
I0523 21:53:48.261735 11835 layer_factory.hpp:77] Creating layer relu1_dw
I0523 21:53:48.261745 11835 net.cpp:84] Creating Layer relu1_dw
I0523 21:53:48.261751 11835 net.cpp:406] relu1_dw <- conv1_dw
I0523 21:53:48.261756 11835 net.cpp:367] relu1_dw -> conv1_dw (in-place)
I0523 21:53:48.261981 11835 net.cpp:122] Setting up relu1_dw
I0523 21:53:48.261991 11835 net.cpp:129] Top shape: 64 64 56 48 (11010048)
I0523 21:53:48.261994 11835 net.cpp:137] Memory required for data: 360579840
I0523 21:53:48.262001 11835 layer_factory.hpp:77] Creating layer conv2_ex
I0523 21:53:48.262014 11835 net.cpp:84] Creating Layer conv2_ex
I0523 21:53:48.262019 11835 net.cpp:406] conv2_ex <- conv1_dw
I0523 21:53:48.262027 11835 net.cpp:380] conv2_ex -> conv2_ex
I0523 21:53:48.265230 11835 net.cpp:122] Setting up conv2_ex
I0523 21:53:48.265249 11835 net.cpp:129] Top shape: 64 128 56 48 (22020096)
I0523 21:53:48.265254 11835 net.cpp:137] Memory required for data: 448660224
I0523 21:53:48.265260 11835 layer_factory.hpp:77] Creating layer conv2_ex/bn
I0523 21:53:48.265270 11835 net.cpp:84] Creating Layer conv2_ex/bn
I0523 21:53:48.265275 11835 net.cpp:406] conv2_ex/bn <- conv2_ex
I0523 21:53:48.265281 11835 net.cpp:367] conv2_ex/bn -> conv2_ex (in-place)
I0523 21:53:48.265476 11835 net.cpp:122] Setting up conv2_ex/bn
I0523 21:53:48.265485 11835 net.cpp:129] Top shape: 64 128 56 48 (22020096)
I0523 21:53:48.265488 11835 net.cpp:137] Memory required for data: 536740608
I0523 21:53:48.265501 11835 layer_factory.hpp:77] Creating layer conv2_ex/scale
I0523 21:53:48.265516 11835 net.cpp:84] Creating Layer conv2_ex/scale
I0523 21:53:48.265522 11835 net.cpp:406] conv2_ex/scale <- conv2_ex
I0523 21:53:48.265527 11835 net.cpp:367] conv2_ex/scale -> conv2_ex (in-place)
I0523 21:53:48.265569 11835 layer_factory.hpp:77] Creating layer conv2_ex/scale
I0523 21:53:48.265679 11835 net.cpp:122] Setting up conv2_ex/scale
I0523 21:53:48.265688 11835 net.cpp:129] Top shape: 64 128 56 48 (22020096)
I0523 21:53:48.265692 11835 net.cpp:137] Memory required for data: 624820992
I0523 21:53:48.265698 11835 layer_factory.hpp:77] Creating layer relu2_ex
I0523 21:53:48.265705 11835 net.cpp:84] Creating Layer relu2_ex
I0523 21:53:48.265709 11835 net.cpp:406] relu2_ex <- conv2_ex
I0523 21:53:48.265714 11835 net.cpp:367] relu2_ex -> conv2_ex (in-place)
I0523 21:53:48.267628 11835 net.cpp:122] Setting up relu2_ex
I0523 21:53:48.267647 11835 net.cpp:129] Top shape: 64 128 56 48 (22020096)
I0523 21:53:48.267650 11835 net.cpp:137] Memory required for data: 712901376
I0523 21:53:48.267666 11835 layer_factory.hpp:77] Creating layer conv2_dw
I0523 21:53:48.267678 11835 net.cpp:84] Creating Layer conv2_dw
I0523 21:53:48.267683 11835 net.cpp:406] conv2_dw <- conv2_ex
I0523 21:53:48.267691 11835 net.cpp:380] conv2_dw -> conv2_dw
I0523 21:53:48.267897 11835 net.cpp:122] Setting up conv2_dw
I0523 21:53:48.267907 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.267911 11835 net.cpp:137] Memory required for data: 734921472
I0523 21:53:48.267917 11835 layer_factory.hpp:77] Creating layer conv2_dw/bn
I0523 21:53:48.267925 11835 net.cpp:84] Creating Layer conv2_dw/bn
I0523 21:53:48.267928 11835 net.cpp:406] conv2_dw/bn <- conv2_dw
I0523 21:53:48.267935 11835 net.cpp:367] conv2_dw/bn -> conv2_dw (in-place)
I0523 21:53:48.268124 11835 net.cpp:122] Setting up conv2_dw/bn
I0523 21:53:48.268132 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.268142 11835 net.cpp:137] Memory required for data: 756941568
I0523 21:53:48.268151 11835 layer_factory.hpp:77] Creating layer conv2_dw/scale
I0523 21:53:48.268163 11835 net.cpp:84] Creating Layer conv2_dw/scale
I0523 21:53:48.268167 11835 net.cpp:406] conv2_dw/scale <- conv2_dw
I0523 21:53:48.268172 11835 net.cpp:367] conv2_dw/scale -> conv2_dw (in-place)
I0523 21:53:48.268213 11835 layer_factory.hpp:77] Creating layer conv2_dw/scale
I0523 21:53:48.268344 11835 net.cpp:122] Setting up conv2_dw/scale
I0523 21:53:48.268353 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.268357 11835 net.cpp:137] Memory required for data: 778961664
I0523 21:53:48.268363 11835 layer_factory.hpp:77] Creating layer relu2_dw
I0523 21:53:48.268373 11835 net.cpp:84] Creating Layer relu2_dw
I0523 21:53:48.268378 11835 net.cpp:406] relu2_dw <- conv2_dw
I0523 21:53:48.268383 11835 net.cpp:367] relu2_dw -> conv2_dw (in-place)
I0523 21:53:48.268534 11835 net.cpp:122] Setting up relu2_dw
I0523 21:53:48.268543 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.268548 11835 net.cpp:137] Memory required for data: 800981760
I0523 21:53:48.268553 11835 layer_factory.hpp:77] Creating layer conv2_em
I0523 21:53:48.268566 11835 net.cpp:84] Creating Layer conv2_em
I0523 21:53:48.268571 11835 net.cpp:406] conv2_em <- conv2_dw
I0523 21:53:48.268580 11835 net.cpp:380] conv2_em -> conv2_em
I0523 21:53:48.270025 11835 net.cpp:122] Setting up conv2_em
I0523 21:53:48.270042 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.270047 11835 net.cpp:137] Memory required for data: 811991808
I0523 21:53:48.270053 11835 layer_factory.hpp:77] Creating layer conv2_em/bn
I0523 21:53:48.270062 11835 net.cpp:84] Creating Layer conv2_em/bn
I0523 21:53:48.270066 11835 net.cpp:406] conv2_em/bn <- conv2_em
I0523 21:53:48.270073 11835 net.cpp:367] conv2_em/bn -> conv2_em (in-place)
I0523 21:53:48.270283 11835 net.cpp:122] Setting up conv2_em/bn
I0523 21:53:48.270292 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.270296 11835 net.cpp:137] Memory required for data: 823001856
I0523 21:53:48.270304 11835 layer_factory.hpp:77] Creating layer conv2_em/scale
I0523 21:53:48.270314 11835 net.cpp:84] Creating Layer conv2_em/scale
I0523 21:53:48.270318 11835 net.cpp:406] conv2_em/scale <- conv2_em
I0523 21:53:48.270324 11835 net.cpp:367] conv2_em/scale -> conv2_em (in-place)
I0523 21:53:48.270364 11835 layer_factory.hpp:77] Creating layer conv2_em/scale
I0523 21:53:48.270486 11835 net.cpp:122] Setting up conv2_em/scale
I0523 21:53:48.270494 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.270498 11835 net.cpp:137] Memory required for data: 834011904
I0523 21:53:48.270515 11835 layer_factory.hpp:77] Creating layer conv2_em_conv2_em/scale_0_split
I0523 21:53:48.270524 11835 net.cpp:84] Creating Layer conv2_em_conv2_em/scale_0_split
I0523 21:53:48.270529 11835 net.cpp:406] conv2_em_conv2_em/scale_0_split <- conv2_em
I0523 21:53:48.270534 11835 net.cpp:380] conv2_em_conv2_em/scale_0_split -> conv2_em_conv2_em/scale_0_split_0
I0523 21:53:48.270541 11835 net.cpp:380] conv2_em_conv2_em/scale_0_split -> conv2_em_conv2_em/scale_0_split_1
I0523 21:53:48.270582 11835 net.cpp:122] Setting up conv2_em_conv2_em/scale_0_split
I0523 21:53:48.270591 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.270596 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.270598 11835 net.cpp:137] Memory required for data: 856032000
I0523 21:53:48.270602 11835 layer_factory.hpp:77] Creating layer conv2_1_ex
I0523 21:53:48.270614 11835 net.cpp:84] Creating Layer conv2_1_ex
I0523 21:53:48.270620 11835 net.cpp:406] conv2_1_ex <- conv2_em_conv2_em/scale_0_split_0
I0523 21:53:48.270629 11835 net.cpp:380] conv2_1_ex -> conv2_1_ex
I0523 21:53:48.271921 11835 net.cpp:122] Setting up conv2_1_ex
I0523 21:53:48.271939 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.271944 11835 net.cpp:137] Memory required for data: 878052096
I0523 21:53:48.271950 11835 layer_factory.hpp:77] Creating layer conv2_1_ex/bn
I0523 21:53:48.271970 11835 net.cpp:84] Creating Layer conv2_1_ex/bn
I0523 21:53:48.271976 11835 net.cpp:406] conv2_1_ex/bn <- conv2_1_ex
I0523 21:53:48.271983 11835 net.cpp:367] conv2_1_ex/bn -> conv2_1_ex (in-place)
I0523 21:53:48.272183 11835 net.cpp:122] Setting up conv2_1_ex/bn
I0523 21:53:48.272192 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.272195 11835 net.cpp:137] Memory required for data: 900072192
I0523 21:53:48.272218 11835 layer_factory.hpp:77] Creating layer conv2_1_ex/scale
I0523 21:53:48.272228 11835 net.cpp:84] Creating Layer conv2_1_ex/scale
I0523 21:53:48.272234 11835 net.cpp:406] conv2_1_ex/scale <- conv2_1_ex
I0523 21:53:48.272244 11835 net.cpp:367] conv2_1_ex/scale -> conv2_1_ex (in-place)
I0523 21:53:48.272292 11835 layer_factory.hpp:77] Creating layer conv2_1_ex/scale
I0523 21:53:48.272408 11835 net.cpp:122] Setting up conv2_1_ex/scale
I0523 21:53:48.272415 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.272419 11835 net.cpp:137] Memory required for data: 922092288
I0523 21:53:48.272429 11835 layer_factory.hpp:77] Creating layer relu2_1_ex
I0523 21:53:48.272439 11835 net.cpp:84] Creating Layer relu2_1_ex
I0523 21:53:48.272442 11835 net.cpp:406] relu2_1_ex <- conv2_1_ex
I0523 21:53:48.272449 11835 net.cpp:367] relu2_1_ex -> conv2_1_ex (in-place)
I0523 21:53:48.272601 11835 net.cpp:122] Setting up relu2_1_ex
I0523 21:53:48.272610 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.272614 11835 net.cpp:137] Memory required for data: 944112384
I0523 21:53:48.272619 11835 layer_factory.hpp:77] Creating layer conv2_1_dw
I0523 21:53:48.272629 11835 net.cpp:84] Creating Layer conv2_1_dw
I0523 21:53:48.272634 11835 net.cpp:406] conv2_1_dw <- conv2_1_ex
I0523 21:53:48.272640 11835 net.cpp:380] conv2_1_dw -> conv2_1_dw
I0523 21:53:48.272846 11835 net.cpp:122] Setting up conv2_1_dw
I0523 21:53:48.272855 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.272858 11835 net.cpp:137] Memory required for data: 966132480
I0523 21:53:48.272863 11835 layer_factory.hpp:77] Creating layer conv2_1_dw/bn
I0523 21:53:48.272871 11835 net.cpp:84] Creating Layer conv2_1_dw/bn
I0523 21:53:48.272876 11835 net.cpp:406] conv2_1_dw/bn <- conv2_1_dw
I0523 21:53:48.272881 11835 net.cpp:367] conv2_1_dw/bn -> conv2_1_dw (in-place)
I0523 21:53:48.273073 11835 net.cpp:122] Setting up conv2_1_dw/bn
I0523 21:53:48.273080 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.273084 11835 net.cpp:137] Memory required for data: 988152576
I0523 21:53:48.273092 11835 layer_factory.hpp:77] Creating layer conv2_1_dw/scale
I0523 21:53:48.273104 11835 net.cpp:84] Creating Layer conv2_1_dw/scale
I0523 21:53:48.273108 11835 net.cpp:406] conv2_1_dw/scale <- conv2_1_dw
I0523 21:53:48.273114 11835 net.cpp:367] conv2_1_dw/scale -> conv2_1_dw (in-place)
I0523 21:53:48.273154 11835 layer_factory.hpp:77] Creating layer conv2_1_dw/scale
I0523 21:53:48.273265 11835 net.cpp:122] Setting up conv2_1_dw/scale
I0523 21:53:48.273273 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.273277 11835 net.cpp:137] Memory required for data: 1010172672
I0523 21:53:48.273283 11835 layer_factory.hpp:77] Creating layer relu2_1_dw
I0523 21:53:48.273289 11835 net.cpp:84] Creating Layer relu2_1_dw
I0523 21:53:48.273293 11835 net.cpp:406] relu2_1_dw <- conv2_1_dw
I0523 21:53:48.273298 11835 net.cpp:367] relu2_1_dw -> conv2_1_dw (in-place)
I0523 21:53:48.273458 11835 net.cpp:122] Setting up relu2_1_dw
I0523 21:53:48.273466 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.273469 11835 net.cpp:137] Memory required for data: 1032192768
I0523 21:53:48.273474 11835 layer_factory.hpp:77] Creating layer conv2_1_em
I0523 21:53:48.273484 11835 net.cpp:84] Creating Layer conv2_1_em
I0523 21:53:48.273490 11835 net.cpp:406] conv2_1_em <- conv2_1_dw
I0523 21:53:48.273496 11835 net.cpp:380] conv2_1_em -> conv2_1_em
I0523 21:53:48.274794 11835 net.cpp:122] Setting up conv2_1_em
I0523 21:53:48.274813 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.274817 11835 net.cpp:137] Memory required for data: 1043202816
I0523 21:53:48.274823 11835 layer_factory.hpp:77] Creating layer conv2_1_em/bn
I0523 21:53:48.274838 11835 net.cpp:84] Creating Layer conv2_1_em/bn
I0523 21:53:48.274843 11835 net.cpp:406] conv2_1_em/bn <- conv2_1_em
I0523 21:53:48.274849 11835 net.cpp:367] conv2_1_em/bn -> conv2_1_em (in-place)
I0523 21:53:48.275058 11835 net.cpp:122] Setting up conv2_1_em/bn
I0523 21:53:48.275068 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.275085 11835 net.cpp:137] Memory required for data: 1054212864
I0523 21:53:48.275096 11835 layer_factory.hpp:77] Creating layer conv2_1_em/scale
I0523 21:53:48.275110 11835 net.cpp:84] Creating Layer conv2_1_em/scale
I0523 21:53:48.275116 11835 net.cpp:406] conv2_1_em/scale <- conv2_1_em
I0523 21:53:48.275122 11835 net.cpp:367] conv2_1_em/scale -> conv2_1_em (in-place)
I0523 21:53:48.275166 11835 layer_factory.hpp:77] Creating layer conv2_1_em/scale
I0523 21:53:48.275286 11835 net.cpp:122] Setting up conv2_1_em/scale
I0523 21:53:48.275295 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.275297 11835 net.cpp:137] Memory required for data: 1065222912
I0523 21:53:48.275305 11835 layer_factory.hpp:77] Creating layer res2_1
I0523 21:53:48.275311 11835 net.cpp:84] Creating Layer res2_1
I0523 21:53:48.275316 11835 net.cpp:406] res2_1 <- conv2_em_conv2_em/scale_0_split_1
I0523 21:53:48.275321 11835 net.cpp:406] res2_1 <- conv2_1_em
I0523 21:53:48.275326 11835 net.cpp:380] res2_1 -> res2_1
I0523 21:53:48.275355 11835 net.cpp:122] Setting up res2_1
I0523 21:53:48.275362 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.275365 11835 net.cpp:137] Memory required for data: 1076232960
I0523 21:53:48.275369 11835 layer_factory.hpp:77] Creating layer res2_1_res2_1_0_split
I0523 21:53:48.275377 11835 net.cpp:84] Creating Layer res2_1_res2_1_0_split
I0523 21:53:48.275379 11835 net.cpp:406] res2_1_res2_1_0_split <- res2_1
I0523 21:53:48.275388 11835 net.cpp:380] res2_1_res2_1_0_split -> res2_1_res2_1_0_split_0
I0523 21:53:48.275396 11835 net.cpp:380] res2_1_res2_1_0_split -> res2_1_res2_1_0_split_1
I0523 21:53:48.275434 11835 net.cpp:122] Setting up res2_1_res2_1_0_split
I0523 21:53:48.275440 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.275446 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.275449 11835 net.cpp:137] Memory required for data: 1098253056
I0523 21:53:48.275454 11835 layer_factory.hpp:77] Creating layer conv2_2_ex
I0523 21:53:48.275467 11835 net.cpp:84] Creating Layer conv2_2_ex
I0523 21:53:48.275473 11835 net.cpp:406] conv2_2_ex <- res2_1_res2_1_0_split_0
I0523 21:53:48.275480 11835 net.cpp:380] conv2_2_ex -> conv2_2_ex
I0523 21:53:48.276803 11835 net.cpp:122] Setting up conv2_2_ex
I0523 21:53:48.276821 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.276827 11835 net.cpp:137] Memory required for data: 1120273152
I0523 21:53:48.276834 11835 layer_factory.hpp:77] Creating layer conv2_2_ex/bn
I0523 21:53:48.276844 11835 net.cpp:84] Creating Layer conv2_2_ex/bn
I0523 21:53:48.276849 11835 net.cpp:406] conv2_2_ex/bn <- conv2_2_ex
I0523 21:53:48.276856 11835 net.cpp:367] conv2_2_ex/bn -> conv2_2_ex (in-place)
I0523 21:53:48.277056 11835 net.cpp:122] Setting up conv2_2_ex/bn
I0523 21:53:48.277065 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.277068 11835 net.cpp:137] Memory required for data: 1142293248
I0523 21:53:48.277076 11835 layer_factory.hpp:77] Creating layer conv2_2_ex/scale
I0523 21:53:48.277086 11835 net.cpp:84] Creating Layer conv2_2_ex/scale
I0523 21:53:48.277091 11835 net.cpp:406] conv2_2_ex/scale <- conv2_2_ex
I0523 21:53:48.277096 11835 net.cpp:367] conv2_2_ex/scale -> conv2_2_ex (in-place)
I0523 21:53:48.277137 11835 layer_factory.hpp:77] Creating layer conv2_2_ex/scale
I0523 21:53:48.277251 11835 net.cpp:122] Setting up conv2_2_ex/scale
I0523 21:53:48.277259 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.277262 11835 net.cpp:137] Memory required for data: 1164313344
I0523 21:53:48.277269 11835 layer_factory.hpp:77] Creating layer relu2_2_ex
I0523 21:53:48.277281 11835 net.cpp:84] Creating Layer relu2_2_ex
I0523 21:53:48.277284 11835 net.cpp:406] relu2_2_ex <- conv2_2_ex
I0523 21:53:48.277289 11835 net.cpp:367] relu2_2_ex -> conv2_2_ex (in-place)
I0523 21:53:48.277452 11835 net.cpp:122] Setting up relu2_2_ex
I0523 21:53:48.277462 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.277465 11835 net.cpp:137] Memory required for data: 1186333440
I0523 21:53:48.277487 11835 layer_factory.hpp:77] Creating layer conv2_2_dw
I0523 21:53:48.277498 11835 net.cpp:84] Creating Layer conv2_2_dw
I0523 21:53:48.277503 11835 net.cpp:406] conv2_2_dw <- conv2_2_ex
I0523 21:53:48.277510 11835 net.cpp:380] conv2_2_dw -> conv2_2_dw
I0523 21:53:48.277719 11835 net.cpp:122] Setting up conv2_2_dw
I0523 21:53:48.277727 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.277731 11835 net.cpp:137] Memory required for data: 1208353536
I0523 21:53:48.277736 11835 layer_factory.hpp:77] Creating layer conv2_2_dw/bn
I0523 21:53:48.277745 11835 net.cpp:84] Creating Layer conv2_2_dw/bn
I0523 21:53:48.277748 11835 net.cpp:406] conv2_2_dw/bn <- conv2_2_dw
I0523 21:53:48.277755 11835 net.cpp:367] conv2_2_dw/bn -> conv2_2_dw (in-place)
I0523 21:53:48.277947 11835 net.cpp:122] Setting up conv2_2_dw/bn
I0523 21:53:48.277956 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.277959 11835 net.cpp:137] Memory required for data: 1230373632
I0523 21:53:48.277974 11835 layer_factory.hpp:77] Creating layer conv2_2_dw/scale
I0523 21:53:48.277987 11835 net.cpp:84] Creating Layer conv2_2_dw/scale
I0523 21:53:48.277992 11835 net.cpp:406] conv2_2_dw/scale <- conv2_2_dw
I0523 21:53:48.277998 11835 net.cpp:367] conv2_2_dw/scale -> conv2_2_dw (in-place)
I0523 21:53:48.278038 11835 layer_factory.hpp:77] Creating layer conv2_2_dw/scale
I0523 21:53:48.278151 11835 net.cpp:122] Setting up conv2_2_dw/scale
I0523 21:53:48.278158 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.278162 11835 net.cpp:137] Memory required for data: 1252393728
I0523 21:53:48.278168 11835 layer_factory.hpp:77] Creating layer relu2_2_dw
I0523 21:53:48.278174 11835 net.cpp:84] Creating Layer relu2_2_dw
I0523 21:53:48.278178 11835 net.cpp:406] relu2_2_dw <- conv2_2_dw
I0523 21:53:48.278183 11835 net.cpp:367] relu2_2_dw -> conv2_2_dw (in-place)
I0523 21:53:48.280110 11835 net.cpp:122] Setting up relu2_2_dw
I0523 21:53:48.280128 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.280134 11835 net.cpp:137] Memory required for data: 1274413824
I0523 21:53:48.280144 11835 layer_factory.hpp:77] Creating layer conv2_2_em
I0523 21:53:48.280159 11835 net.cpp:84] Creating Layer conv2_2_em
I0523 21:53:48.280166 11835 net.cpp:406] conv2_2_em <- conv2_2_dw
I0523 21:53:48.280174 11835 net.cpp:380] conv2_2_em -> conv2_2_em
I0523 21:53:48.281668 11835 net.cpp:122] Setting up conv2_2_em
I0523 21:53:48.281687 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.281690 11835 net.cpp:137] Memory required for data: 1285423872
I0523 21:53:48.281697 11835 layer_factory.hpp:77] Creating layer conv2_2_em/bn
I0523 21:53:48.281713 11835 net.cpp:84] Creating Layer conv2_2_em/bn
I0523 21:53:48.281718 11835 net.cpp:406] conv2_2_em/bn <- conv2_2_em
I0523 21:53:48.281726 11835 net.cpp:367] conv2_2_em/bn -> conv2_2_em (in-place)
I0523 21:53:48.281952 11835 net.cpp:122] Setting up conv2_2_em/bn
I0523 21:53:48.281961 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.281965 11835 net.cpp:137] Memory required for data: 1296433920
I0523 21:53:48.281973 11835 layer_factory.hpp:77] Creating layer conv2_2_em/scale
I0523 21:53:48.281981 11835 net.cpp:84] Creating Layer conv2_2_em/scale
I0523 21:53:48.281987 11835 net.cpp:406] conv2_2_em/scale <- conv2_2_em
I0523 21:53:48.281993 11835 net.cpp:367] conv2_2_em/scale -> conv2_2_em (in-place)
I0523 21:53:48.282035 11835 layer_factory.hpp:77] Creating layer conv2_2_em/scale
I0523 21:53:48.282161 11835 net.cpp:122] Setting up conv2_2_em/scale
I0523 21:53:48.282171 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.282173 11835 net.cpp:137] Memory required for data: 1307443968
I0523 21:53:48.282179 11835 layer_factory.hpp:77] Creating layer res2_2
I0523 21:53:48.282189 11835 net.cpp:84] Creating Layer res2_2
I0523 21:53:48.282193 11835 net.cpp:406] res2_2 <- res2_1_res2_1_0_split_1
I0523 21:53:48.282199 11835 net.cpp:406] res2_2 <- conv2_2_em
I0523 21:53:48.282204 11835 net.cpp:380] res2_2 -> res2_2
I0523 21:53:48.282248 11835 net.cpp:122] Setting up res2_2
I0523 21:53:48.282256 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.282260 11835 net.cpp:137] Memory required for data: 1318454016
I0523 21:53:48.282264 11835 layer_factory.hpp:77] Creating layer res2_2_res2_2_0_split
I0523 21:53:48.282271 11835 net.cpp:84] Creating Layer res2_2_res2_2_0_split
I0523 21:53:48.282276 11835 net.cpp:406] res2_2_res2_2_0_split <- res2_2
I0523 21:53:48.282284 11835 net.cpp:380] res2_2_res2_2_0_split -> res2_2_res2_2_0_split_0
I0523 21:53:48.282289 11835 net.cpp:380] res2_2_res2_2_0_split -> res2_2_res2_2_0_split_1
I0523 21:53:48.282327 11835 net.cpp:122] Setting up res2_2_res2_2_0_split
I0523 21:53:48.282333 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.282338 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.282341 11835 net.cpp:137] Memory required for data: 1340474112
I0523 21:53:48.282346 11835 layer_factory.hpp:77] Creating layer conv2_3_ex
I0523 21:53:48.282358 11835 net.cpp:84] Creating Layer conv2_3_ex
I0523 21:53:48.282363 11835 net.cpp:406] conv2_3_ex <- res2_2_res2_2_0_split_0
I0523 21:53:48.282371 11835 net.cpp:380] conv2_3_ex -> conv2_3_ex
I0523 21:53:48.283814 11835 net.cpp:122] Setting up conv2_3_ex
I0523 21:53:48.283833 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.283838 11835 net.cpp:137] Memory required for data: 1362494208
I0523 21:53:48.283844 11835 layer_factory.hpp:77] Creating layer conv2_3_ex/bn
I0523 21:53:48.283854 11835 net.cpp:84] Creating Layer conv2_3_ex/bn
I0523 21:53:48.283860 11835 net.cpp:406] conv2_3_ex/bn <- conv2_3_ex
I0523 21:53:48.283867 11835 net.cpp:367] conv2_3_ex/bn -> conv2_3_ex (in-place)
I0523 21:53:48.284080 11835 net.cpp:122] Setting up conv2_3_ex/bn
I0523 21:53:48.284090 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.284093 11835 net.cpp:137] Memory required for data: 1384514304
I0523 21:53:48.284106 11835 layer_factory.hpp:77] Creating layer conv2_3_ex/scale
I0523 21:53:48.284116 11835 net.cpp:84] Creating Layer conv2_3_ex/scale
I0523 21:53:48.284119 11835 net.cpp:406] conv2_3_ex/scale <- conv2_3_ex
I0523 21:53:48.284126 11835 net.cpp:367] conv2_3_ex/scale -> conv2_3_ex (in-place)
I0523 21:53:48.284169 11835 layer_factory.hpp:77] Creating layer conv2_3_ex/scale
I0523 21:53:48.284289 11835 net.cpp:122] Setting up conv2_3_ex/scale
I0523 21:53:48.284297 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.284301 11835 net.cpp:137] Memory required for data: 1406534400
I0523 21:53:48.284307 11835 layer_factory.hpp:77] Creating layer relu2_3_ex
I0523 21:53:48.284314 11835 net.cpp:84] Creating Layer relu2_3_ex
I0523 21:53:48.284318 11835 net.cpp:406] relu2_3_ex <- conv2_3_ex
I0523 21:53:48.284323 11835 net.cpp:367] relu2_3_ex -> conv2_3_ex (in-place)
I0523 21:53:48.284482 11835 net.cpp:122] Setting up relu2_3_ex
I0523 21:53:48.284490 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.284493 11835 net.cpp:137] Memory required for data: 1428554496
I0523 21:53:48.284499 11835 layer_factory.hpp:77] Creating layer conv2_3_dw
I0523 21:53:48.284512 11835 net.cpp:84] Creating Layer conv2_3_dw
I0523 21:53:48.284518 11835 net.cpp:406] conv2_3_dw <- conv2_3_ex
I0523 21:53:48.284524 11835 net.cpp:380] conv2_3_dw -> conv2_3_dw
I0523 21:53:48.284736 11835 net.cpp:122] Setting up conv2_3_dw
I0523 21:53:48.284744 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.284749 11835 net.cpp:137] Memory required for data: 1450574592
I0523 21:53:48.284754 11835 layer_factory.hpp:77] Creating layer conv2_3_dw/bn
I0523 21:53:48.284762 11835 net.cpp:84] Creating Layer conv2_3_dw/bn
I0523 21:53:48.284766 11835 net.cpp:406] conv2_3_dw/bn <- conv2_3_dw
I0523 21:53:48.284772 11835 net.cpp:367] conv2_3_dw/bn -> conv2_3_dw (in-place)
I0523 21:53:48.284970 11835 net.cpp:122] Setting up conv2_3_dw/bn
I0523 21:53:48.284978 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.284982 11835 net.cpp:137] Memory required for data: 1472594688
I0523 21:53:48.285010 11835 layer_factory.hpp:77] Creating layer conv2_3_dw/scale
I0523 21:53:48.285020 11835 net.cpp:84] Creating Layer conv2_3_dw/scale
I0523 21:53:48.285027 11835 net.cpp:406] conv2_3_dw/scale <- conv2_3_dw
I0523 21:53:48.285033 11835 net.cpp:367] conv2_3_dw/scale -> conv2_3_dw (in-place)
I0523 21:53:48.285076 11835 layer_factory.hpp:77] Creating layer conv2_3_dw/scale
I0523 21:53:48.285197 11835 net.cpp:122] Setting up conv2_3_dw/scale
I0523 21:53:48.285204 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.285208 11835 net.cpp:137] Memory required for data: 1494614784
I0523 21:53:48.285214 11835 layer_factory.hpp:77] Creating layer relu2_3_dw
I0523 21:53:48.285223 11835 net.cpp:84] Creating Layer relu2_3_dw
I0523 21:53:48.285228 11835 net.cpp:406] relu2_3_dw <- conv2_3_dw
I0523 21:53:48.285233 11835 net.cpp:367] relu2_3_dw -> conv2_3_dw (in-place)
I0523 21:53:48.285388 11835 net.cpp:122] Setting up relu2_3_dw
I0523 21:53:48.285396 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.285400 11835 net.cpp:137] Memory required for data: 1516634880
I0523 21:53:48.285405 11835 layer_factory.hpp:77] Creating layer conv2_3_em
I0523 21:53:48.285416 11835 net.cpp:84] Creating Layer conv2_3_em
I0523 21:53:48.285423 11835 net.cpp:406] conv2_3_em <- conv2_3_dw
I0523 21:53:48.285429 11835 net.cpp:380] conv2_3_em -> conv2_3_em
I0523 21:53:48.286839 11835 net.cpp:122] Setting up conv2_3_em
I0523 21:53:48.286857 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.286864 11835 net.cpp:137] Memory required for data: 1527644928
I0523 21:53:48.286870 11835 layer_factory.hpp:77] Creating layer conv2_3_em/bn
I0523 21:53:48.286886 11835 net.cpp:84] Creating Layer conv2_3_em/bn
I0523 21:53:48.286892 11835 net.cpp:406] conv2_3_em/bn <- conv2_3_em
I0523 21:53:48.286900 11835 net.cpp:367] conv2_3_em/bn -> conv2_3_em (in-place)
I0523 21:53:48.287132 11835 net.cpp:122] Setting up conv2_3_em/bn
I0523 21:53:48.287140 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.287144 11835 net.cpp:137] Memory required for data: 1538654976
I0523 21:53:48.287153 11835 layer_factory.hpp:77] Creating layer conv2_3_em/scale
I0523 21:53:48.287164 11835 net.cpp:84] Creating Layer conv2_3_em/scale
I0523 21:53:48.287171 11835 net.cpp:406] conv2_3_em/scale <- conv2_3_em
I0523 21:53:48.287178 11835 net.cpp:367] conv2_3_em/scale -> conv2_3_em (in-place)
I0523 21:53:48.287222 11835 layer_factory.hpp:77] Creating layer conv2_3_em/scale
I0523 21:53:48.287358 11835 net.cpp:122] Setting up conv2_3_em/scale
I0523 21:53:48.287367 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.287370 11835 net.cpp:137] Memory required for data: 1549665024
I0523 21:53:48.287376 11835 layer_factory.hpp:77] Creating layer res2_3
I0523 21:53:48.287384 11835 net.cpp:84] Creating Layer res2_3
I0523 21:53:48.287387 11835 net.cpp:406] res2_3 <- res2_2_res2_2_0_split_1
I0523 21:53:48.287392 11835 net.cpp:406] res2_3 <- conv2_3_em
I0523 21:53:48.287398 11835 net.cpp:380] res2_3 -> res2_3
I0523 21:53:48.287423 11835 net.cpp:122] Setting up res2_3
I0523 21:53:48.287431 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.287433 11835 net.cpp:137] Memory required for data: 1560675072
I0523 21:53:48.287437 11835 layer_factory.hpp:77] Creating layer res2_3_res2_3_0_split
I0523 21:53:48.287443 11835 net.cpp:84] Creating Layer res2_3_res2_3_0_split
I0523 21:53:48.287447 11835 net.cpp:406] res2_3_res2_3_0_split <- res2_3
I0523 21:53:48.287452 11835 net.cpp:380] res2_3_res2_3_0_split -> res2_3_res2_3_0_split_0
I0523 21:53:48.287458 11835 net.cpp:380] res2_3_res2_3_0_split -> res2_3_res2_3_0_split_1
I0523 21:53:48.287493 11835 net.cpp:122] Setting up res2_3_res2_3_0_split
I0523 21:53:48.287500 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.287504 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.287508 11835 net.cpp:137] Memory required for data: 1582695168
I0523 21:53:48.287514 11835 layer_factory.hpp:77] Creating layer conv2_4_ex
I0523 21:53:48.287528 11835 net.cpp:84] Creating Layer conv2_4_ex
I0523 21:53:48.287546 11835 net.cpp:406] conv2_4_ex <- res2_3_res2_3_0_split_0
I0523 21:53:48.287554 11835 net.cpp:380] conv2_4_ex -> conv2_4_ex
I0523 21:53:48.289036 11835 net.cpp:122] Setting up conv2_4_ex
I0523 21:53:48.289053 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.289059 11835 net.cpp:137] Memory required for data: 1604715264
I0523 21:53:48.289067 11835 layer_factory.hpp:77] Creating layer conv2_4_ex/bn
I0523 21:53:48.289075 11835 net.cpp:84] Creating Layer conv2_4_ex/bn
I0523 21:53:48.289081 11835 net.cpp:406] conv2_4_ex/bn <- conv2_4_ex
I0523 21:53:48.289088 11835 net.cpp:367] conv2_4_ex/bn -> conv2_4_ex (in-place)
I0523 21:53:48.289296 11835 net.cpp:122] Setting up conv2_4_ex/bn
I0523 21:53:48.289304 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.289309 11835 net.cpp:137] Memory required for data: 1626735360
I0523 21:53:48.289316 11835 layer_factory.hpp:77] Creating layer conv2_4_ex/scale
I0523 21:53:48.289324 11835 net.cpp:84] Creating Layer conv2_4_ex/scale
I0523 21:53:48.289330 11835 net.cpp:406] conv2_4_ex/scale <- conv2_4_ex
I0523 21:53:48.289336 11835 net.cpp:367] conv2_4_ex/scale -> conv2_4_ex (in-place)
I0523 21:53:48.289377 11835 layer_factory.hpp:77] Creating layer conv2_4_ex/scale
I0523 21:53:48.289500 11835 net.cpp:122] Setting up conv2_4_ex/scale
I0523 21:53:48.289507 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.289511 11835 net.cpp:137] Memory required for data: 1648755456
I0523 21:53:48.289517 11835 layer_factory.hpp:77] Creating layer relu2_4_ex
I0523 21:53:48.289537 11835 net.cpp:84] Creating Layer relu2_4_ex
I0523 21:53:48.289543 11835 net.cpp:406] relu2_4_ex <- conv2_4_ex
I0523 21:53:48.289548 11835 net.cpp:367] relu2_4_ex -> conv2_4_ex (in-place)
I0523 21:53:48.289705 11835 net.cpp:122] Setting up relu2_4_ex
I0523 21:53:48.289713 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.289716 11835 net.cpp:137] Memory required for data: 1670775552
I0523 21:53:48.289727 11835 layer_factory.hpp:77] Creating layer conv2_4_dw
I0523 21:53:48.289741 11835 net.cpp:84] Creating Layer conv2_4_dw
I0523 21:53:48.289746 11835 net.cpp:406] conv2_4_dw <- conv2_4_ex
I0523 21:53:48.289752 11835 net.cpp:380] conv2_4_dw -> conv2_4_dw
I0523 21:53:48.289965 11835 net.cpp:122] Setting up conv2_4_dw
I0523 21:53:48.289973 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.289976 11835 net.cpp:137] Memory required for data: 1692795648
I0523 21:53:48.289981 11835 layer_factory.hpp:77] Creating layer conv2_4_dw/bn
I0523 21:53:48.289989 11835 net.cpp:84] Creating Layer conv2_4_dw/bn
I0523 21:53:48.289994 11835 net.cpp:406] conv2_4_dw/bn <- conv2_4_dw
I0523 21:53:48.290002 11835 net.cpp:367] conv2_4_dw/bn -> conv2_4_dw (in-place)
I0523 21:53:48.290199 11835 net.cpp:122] Setting up conv2_4_dw/bn
I0523 21:53:48.290207 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.290211 11835 net.cpp:137] Memory required for data: 1714815744
I0523 21:53:48.290220 11835 layer_factory.hpp:77] Creating layer conv2_4_dw/scale
I0523 21:53:48.290227 11835 net.cpp:84] Creating Layer conv2_4_dw/scale
I0523 21:53:48.290232 11835 net.cpp:406] conv2_4_dw/scale <- conv2_4_dw
I0523 21:53:48.290237 11835 net.cpp:367] conv2_4_dw/scale -> conv2_4_dw (in-place)
I0523 21:53:48.290275 11835 layer_factory.hpp:77] Creating layer conv2_4_dw/scale
I0523 21:53:48.290390 11835 net.cpp:122] Setting up conv2_4_dw/scale
I0523 21:53:48.290397 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.290401 11835 net.cpp:137] Memory required for data: 1736835840
I0523 21:53:48.290407 11835 layer_factory.hpp:77] Creating layer relu2_4_dw
I0523 21:53:48.290416 11835 net.cpp:84] Creating Layer relu2_4_dw
I0523 21:53:48.290419 11835 net.cpp:406] relu2_4_dw <- conv2_4_dw
I0523 21:53:48.290424 11835 net.cpp:367] relu2_4_dw -> conv2_4_dw (in-place)
I0523 21:53:48.290580 11835 net.cpp:122] Setting up relu2_4_dw
I0523 21:53:48.290587 11835 net.cpp:129] Top shape: 64 128 28 24 (5505024)
I0523 21:53:48.290591 11835 net.cpp:137] Memory required for data: 1758855936
I0523 21:53:48.290612 11835 layer_factory.hpp:77] Creating layer conv2_4_em
I0523 21:53:48.290624 11835 net.cpp:84] Creating Layer conv2_4_em
I0523 21:53:48.290630 11835 net.cpp:406] conv2_4_em <- conv2_4_dw
I0523 21:53:48.290637 11835 net.cpp:380] conv2_4_em -> conv2_4_em
I0523 21:53:48.291970 11835 net.cpp:122] Setting up conv2_4_em
I0523 21:53:48.291987 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.291993 11835 net.cpp:137] Memory required for data: 1769865984
I0523 21:53:48.292009 11835 layer_factory.hpp:77] Creating layer conv2_4_em/bn
I0523 21:53:48.292019 11835 net.cpp:84] Creating Layer conv2_4_em/bn
I0523 21:53:48.292024 11835 net.cpp:406] conv2_4_em/bn <- conv2_4_em
I0523 21:53:48.292032 11835 net.cpp:367] conv2_4_em/bn -> conv2_4_em (in-place)
I0523 21:53:48.292253 11835 net.cpp:122] Setting up conv2_4_em/bn
I0523 21:53:48.292260 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.292264 11835 net.cpp:137] Memory required for data: 1780876032
I0523 21:53:48.292273 11835 layer_factory.hpp:77] Creating layer conv2_4_em/scale
I0523 21:53:48.292286 11835 net.cpp:84] Creating Layer conv2_4_em/scale
I0523 21:53:48.292291 11835 net.cpp:406] conv2_4_em/scale <- conv2_4_em
I0523 21:53:48.292299 11835 net.cpp:367] conv2_4_em/scale -> conv2_4_em (in-place)
I0523 21:53:48.292342 11835 layer_factory.hpp:77] Creating layer conv2_4_em/scale
I0523 21:53:48.292471 11835 net.cpp:122] Setting up conv2_4_em/scale
I0523 21:53:48.292479 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.292482 11835 net.cpp:137] Memory required for data: 1791886080
I0523 21:53:48.292490 11835 layer_factory.hpp:77] Creating layer res2_4
I0523 21:53:48.292497 11835 net.cpp:84] Creating Layer res2_4
I0523 21:53:48.292503 11835 net.cpp:406] res2_4 <- res2_3_res2_3_0_split_1
I0523 21:53:48.292508 11835 net.cpp:406] res2_4 <- conv2_4_em
I0523 21:53:48.292515 11835 net.cpp:380] res2_4 -> res2_4
I0523 21:53:48.292542 11835 net.cpp:122] Setting up res2_4
I0523 21:53:48.292551 11835 net.cpp:129] Top shape: 64 64 28 24 (2752512)
I0523 21:53:48.292553 11835 net.cpp:137] Memory required for data: 1802896128
I0523 21:53:48.292557 11835 layer_factory.hpp:77] Creating layer conv3_ex
I0523 21:53:48.292569 11835 net.cpp:84] Creating Layer conv3_ex
I0523 21:53:48.292575 11835 net.cpp:406] conv3_ex <- res2_4
I0523 21:53:48.292582 11835 net.cpp:380] conv3_ex -> conv3_ex
I0523 21:53:48.294059 11835 net.cpp:122] Setting up conv3_ex
I0523 21:53:48.294076 11835 net.cpp:129] Top shape: 64 256 28 24 (11010048)
I0523 21:53:48.294081 11835 net.cpp:137] Memory required for data: 1846936320
I0523 21:53:48.294088 11835 layer_factory.hpp:77] Creating layer conv3_ex/bn
I0523 21:53:48.294097 11835 net.cpp:84] Creating Layer conv3_ex/bn
I0523 21:53:48.294102 11835 net.cpp:406] conv3_ex/bn <- conv3_ex
I0523 21:53:48.294109 11835 net.cpp:367] conv3_ex/bn -> conv3_ex (in-place)
I0523 21:53:48.294322 11835 net.cpp:122] Setting up conv3_ex/bn
I0523 21:53:48.294329 11835 net.cpp:129] Top shape: 64 256 28 24 (11010048)
I0523 21:53:48.294333 11835 net.cpp:137] Memory required for data: 1890976512
I0523 21:53:48.294342 11835 layer_factory.hpp:77] Creating layer conv3_ex/scale
I0523 21:53:48.294349 11835 net.cpp:84] Creating Layer conv3_ex/scale
I0523 21:53:48.294354 11835 net.cpp:406] conv3_ex/scale <- conv3_ex
I0523 21:53:48.294360 11835 net.cpp:367] conv3_ex/scale -> conv3_ex (in-place)
I0523 21:53:48.294401 11835 layer_factory.hpp:77] Creating layer conv3_ex/scale
I0523 21:53:48.294522 11835 net.cpp:122] Setting up conv3_ex/scale
I0523 21:53:48.294528 11835 net.cpp:129] Top shape: 64 256 28 24 (11010048)
I0523 21:53:48.294533 11835 net.cpp:137] Memory required for data: 1935016704
I0523 21:53:48.294543 11835 layer_factory.hpp:77] Creating layer relu3_ex
I0523 21:53:48.294549 11835 net.cpp:84] Creating Layer relu3_ex
I0523 21:53:48.294553 11835 net.cpp:406] relu3_ex <- conv3_ex
I0523 21:53:48.294559 11835 net.cpp:367] relu3_ex -> conv3_ex (in-place)
I0523 21:53:48.296789 11835 net.cpp:122] Setting up relu3_ex
I0523 21:53:48.296821 11835 net.cpp:129] Top shape: 64 256 28 24 (11010048)
I0523 21:53:48.296825 11835 net.cpp:137] Memory required for data: 1979056896
I0523 21:53:48.296833 11835 layer_factory.hpp:77] Creating layer conv3_dw
I0523 21:53:48.296847 11835 net.cpp:84] Creating Layer conv3_dw
I0523 21:53:48.296852 11835 net.cpp:406] conv3_dw <- conv3_ex
I0523 21:53:48.296871 11835 net.cpp:380] conv3_dw -> conv3_dw
I0523 21:53:48.297107 11835 net.cpp:122] Setting up conv3_dw
I0523 21:53:48.297116 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.297121 11835 net.cpp:137] Memory required for data: 1990066944
I0523 21:53:48.297125 11835 layer_factory.hpp:77] Creating layer conv3_dw/bn
I0523 21:53:48.297133 11835 net.cpp:84] Creating Layer conv3_dw/bn
I0523 21:53:48.297137 11835 net.cpp:406] conv3_dw/bn <- conv3_dw
I0523 21:53:48.297143 11835 net.cpp:367] conv3_dw/bn -> conv3_dw (in-place)
I0523 21:53:48.297344 11835 net.cpp:122] Setting up conv3_dw/bn
I0523 21:53:48.297351 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.297355 11835 net.cpp:137] Memory required for data: 2001076992
I0523 21:53:48.297363 11835 layer_factory.hpp:77] Creating layer conv3_dw/scale
I0523 21:53:48.297376 11835 net.cpp:84] Creating Layer conv3_dw/scale
I0523 21:53:48.297381 11835 net.cpp:406] conv3_dw/scale <- conv3_dw
I0523 21:53:48.297387 11835 net.cpp:367] conv3_dw/scale -> conv3_dw (in-place)
I0523 21:53:48.297430 11835 layer_factory.hpp:77] Creating layer conv3_dw/scale
I0523 21:53:48.297545 11835 net.cpp:122] Setting up conv3_dw/scale
I0523 21:53:48.297552 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.297556 11835 net.cpp:137] Memory required for data: 2012087040
I0523 21:53:48.297562 11835 layer_factory.hpp:77] Creating layer relu3_dw
I0523 21:53:48.297571 11835 net.cpp:84] Creating Layer relu3_dw
I0523 21:53:48.297576 11835 net.cpp:406] relu3_dw <- conv3_dw
I0523 21:53:48.297582 11835 net.cpp:367] relu3_dw -> conv3_dw (in-place)
I0523 21:53:48.299299 11835 net.cpp:122] Setting up relu3_dw
I0523 21:53:48.299315 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.299320 11835 net.cpp:137] Memory required for data: 2023097088
I0523 21:53:48.299326 11835 layer_factory.hpp:77] Creating layer conv3_em
I0523 21:53:48.299340 11835 net.cpp:84] Creating Layer conv3_em
I0523 21:53:48.299345 11835 net.cpp:406] conv3_em <- conv3_dw
I0523 21:53:48.299353 11835 net.cpp:380] conv3_em -> conv3_em
I0523 21:53:48.300843 11835 net.cpp:122] Setting up conv3_em
I0523 21:53:48.300856 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.300860 11835 net.cpp:137] Memory required for data: 2028602112
I0523 21:53:48.300884 11835 layer_factory.hpp:77] Creating layer conv3_em/bn
I0523 21:53:48.300901 11835 net.cpp:84] Creating Layer conv3_em/bn
I0523 21:53:48.300906 11835 net.cpp:406] conv3_em/bn <- conv3_em
I0523 21:53:48.300920 11835 net.cpp:367] conv3_em/bn -> conv3_em (in-place)
I0523 21:53:48.301134 11835 net.cpp:122] Setting up conv3_em/bn
I0523 21:53:48.301142 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.301146 11835 net.cpp:137] Memory required for data: 2034107136
I0523 21:53:48.301158 11835 layer_factory.hpp:77] Creating layer conv3_em/scale
I0523 21:53:48.301170 11835 net.cpp:84] Creating Layer conv3_em/scale
I0523 21:53:48.301177 11835 net.cpp:406] conv3_em/scale <- conv3_em
I0523 21:53:48.301182 11835 net.cpp:367] conv3_em/scale -> conv3_em (in-place)
I0523 21:53:48.301226 11835 layer_factory.hpp:77] Creating layer conv3_em/scale
I0523 21:53:48.301349 11835 net.cpp:122] Setting up conv3_em/scale
I0523 21:53:48.301357 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.301360 11835 net.cpp:137] Memory required for data: 2039612160
I0523 21:53:48.301367 11835 layer_factory.hpp:77] Creating layer conv3_em_conv3_em/scale_0_split
I0523 21:53:48.301375 11835 net.cpp:84] Creating Layer conv3_em_conv3_em/scale_0_split
I0523 21:53:48.301379 11835 net.cpp:406] conv3_em_conv3_em/scale_0_split <- conv3_em
I0523 21:53:48.301385 11835 net.cpp:380] conv3_em_conv3_em/scale_0_split -> conv3_em_conv3_em/scale_0_split_0
I0523 21:53:48.301407 11835 net.cpp:380] conv3_em_conv3_em/scale_0_split -> conv3_em_conv3_em/scale_0_split_1
I0523 21:53:48.301456 11835 net.cpp:122] Setting up conv3_em_conv3_em/scale_0_split
I0523 21:53:48.301465 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.301468 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.301472 11835 net.cpp:137] Memory required for data: 2050622208
I0523 21:53:48.301476 11835 layer_factory.hpp:77] Creating layer conv3_1_ex
I0523 21:53:48.301486 11835 net.cpp:84] Creating Layer conv3_1_ex
I0523 21:53:48.301491 11835 net.cpp:406] conv3_1_ex <- conv3_em_conv3_em/scale_0_split_0
I0523 21:53:48.301501 11835 net.cpp:380] conv3_1_ex -> conv3_1_ex
I0523 21:53:48.303071 11835 net.cpp:122] Setting up conv3_1_ex
I0523 21:53:48.303089 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.303093 11835 net.cpp:137] Memory required for data: 2061632256
I0523 21:53:48.303099 11835 layer_factory.hpp:77] Creating layer conv3_1_ex/bn
I0523 21:53:48.303110 11835 net.cpp:84] Creating Layer conv3_1_ex/bn
I0523 21:53:48.303117 11835 net.cpp:406] conv3_1_ex/bn <- conv3_1_ex
I0523 21:53:48.303125 11835 net.cpp:367] conv3_1_ex/bn -> conv3_1_ex (in-place)
I0523 21:53:48.303359 11835 net.cpp:122] Setting up conv3_1_ex/bn
I0523 21:53:48.303366 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.303370 11835 net.cpp:137] Memory required for data: 2072642304
I0523 21:53:48.303380 11835 layer_factory.hpp:77] Creating layer conv3_1_ex/scale
I0523 21:53:48.303393 11835 net.cpp:84] Creating Layer conv3_1_ex/scale
I0523 21:53:48.303400 11835 net.cpp:406] conv3_1_ex/scale <- conv3_1_ex
I0523 21:53:48.303405 11835 net.cpp:367] conv3_1_ex/scale -> conv3_1_ex (in-place)
I0523 21:53:48.303452 11835 layer_factory.hpp:77] Creating layer conv3_1_ex/scale
I0523 21:53:48.303586 11835 net.cpp:122] Setting up conv3_1_ex/scale
I0523 21:53:48.303594 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.303597 11835 net.cpp:137] Memory required for data: 2083652352
I0523 21:53:48.303604 11835 layer_factory.hpp:77] Creating layer relu3_1_ex
I0523 21:53:48.303611 11835 net.cpp:84] Creating Layer relu3_1_ex
I0523 21:53:48.303614 11835 net.cpp:406] relu3_1_ex <- conv3_1_ex
I0523 21:53:48.303622 11835 net.cpp:367] relu3_1_ex -> conv3_1_ex (in-place)
I0523 21:53:48.303747 11835 net.cpp:122] Setting up relu3_1_ex
I0523 21:53:48.303755 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.303759 11835 net.cpp:137] Memory required for data: 2094662400
I0523 21:53:48.303764 11835 layer_factory.hpp:77] Creating layer conv3_1_dw
I0523 21:53:48.303774 11835 net.cpp:84] Creating Layer conv3_1_dw
I0523 21:53:48.303781 11835 net.cpp:406] conv3_1_dw <- conv3_1_ex
I0523 21:53:48.303788 11835 net.cpp:380] conv3_1_dw -> conv3_1_dw
I0523 21:53:48.304025 11835 net.cpp:122] Setting up conv3_1_dw
I0523 21:53:48.304034 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.304038 11835 net.cpp:137] Memory required for data: 2105672448
I0523 21:53:48.304045 11835 layer_factory.hpp:77] Creating layer conv3_1_dw/bn
I0523 21:53:48.304054 11835 net.cpp:84] Creating Layer conv3_1_dw/bn
I0523 21:53:48.304059 11835 net.cpp:406] conv3_1_dw/bn <- conv3_1_dw
I0523 21:53:48.304064 11835 net.cpp:367] conv3_1_dw/bn -> conv3_1_dw (in-place)
I0523 21:53:48.304281 11835 net.cpp:122] Setting up conv3_1_dw/bn
I0523 21:53:48.304289 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.304293 11835 net.cpp:137] Memory required for data: 2116682496
I0523 21:53:48.304302 11835 layer_factory.hpp:77] Creating layer conv3_1_dw/scale
I0523 21:53:48.304311 11835 net.cpp:84] Creating Layer conv3_1_dw/scale
I0523 21:53:48.304316 11835 net.cpp:406] conv3_1_dw/scale <- conv3_1_dw
I0523 21:53:48.304320 11835 net.cpp:367] conv3_1_dw/scale -> conv3_1_dw (in-place)
I0523 21:53:48.304365 11835 layer_factory.hpp:77] Creating layer conv3_1_dw/scale
I0523 21:53:48.304492 11835 net.cpp:122] Setting up conv3_1_dw/scale
I0523 21:53:48.304514 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.304518 11835 net.cpp:137] Memory required for data: 2127692544
I0523 21:53:48.304524 11835 layer_factory.hpp:77] Creating layer relu3_1_dw
I0523 21:53:48.304531 11835 net.cpp:84] Creating Layer relu3_1_dw
I0523 21:53:48.304535 11835 net.cpp:406] relu3_1_dw <- conv3_1_dw
I0523 21:53:48.304543 11835 net.cpp:367] relu3_1_dw -> conv3_1_dw (in-place)
I0523 21:53:48.304669 11835 net.cpp:122] Setting up relu3_1_dw
I0523 21:53:48.304677 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.304682 11835 net.cpp:137] Memory required for data: 2138702592
I0523 21:53:48.304687 11835 layer_factory.hpp:77] Creating layer conv3_1_em
I0523 21:53:48.304700 11835 net.cpp:84] Creating Layer conv3_1_em
I0523 21:53:48.304706 11835 net.cpp:406] conv3_1_em <- conv3_1_dw
I0523 21:53:48.304713 11835 net.cpp:380] conv3_1_em -> conv3_1_em
I0523 21:53:48.306411 11835 net.cpp:122] Setting up conv3_1_em
I0523 21:53:48.306432 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.306435 11835 net.cpp:137] Memory required for data: 2144207616
I0523 21:53:48.306444 11835 layer_factory.hpp:77] Creating layer conv3_1_em/bn
I0523 21:53:48.306454 11835 net.cpp:84] Creating Layer conv3_1_em/bn
I0523 21:53:48.306459 11835 net.cpp:406] conv3_1_em/bn <- conv3_1_em
I0523 21:53:48.306473 11835 net.cpp:367] conv3_1_em/bn -> conv3_1_em (in-place)
I0523 21:53:48.306726 11835 net.cpp:122] Setting up conv3_1_em/bn
I0523 21:53:48.306747 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.306751 11835 net.cpp:137] Memory required for data: 2149712640
I0523 21:53:48.306758 11835 layer_factory.hpp:77] Creating layer conv3_1_em/scale
I0523 21:53:48.306766 11835 net.cpp:84] Creating Layer conv3_1_em/scale
I0523 21:53:48.306771 11835 net.cpp:406] conv3_1_em/scale <- conv3_1_em
I0523 21:53:48.306777 11835 net.cpp:367] conv3_1_em/scale -> conv3_1_em (in-place)
I0523 21:53:48.306830 11835 layer_factory.hpp:77] Creating layer conv3_1_em/scale
I0523 21:53:48.306962 11835 net.cpp:122] Setting up conv3_1_em/scale
I0523 21:53:48.306972 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.306977 11835 net.cpp:137] Memory required for data: 2155217664
I0523 21:53:48.306982 11835 layer_factory.hpp:77] Creating layer res3_1
I0523 21:53:48.306991 11835 net.cpp:84] Creating Layer res3_1
I0523 21:53:48.306998 11835 net.cpp:406] res3_1 <- conv3_em_conv3_em/scale_0_split_1
I0523 21:53:48.307003 11835 net.cpp:406] res3_1 <- conv3_1_em
I0523 21:53:48.307008 11835 net.cpp:380] res3_1 -> res3_1
I0523 21:53:48.307036 11835 net.cpp:122] Setting up res3_1
I0523 21:53:48.307044 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.307047 11835 net.cpp:137] Memory required for data: 2160722688
I0523 21:53:48.307051 11835 layer_factory.hpp:77] Creating layer res3_1_res3_1_0_split
I0523 21:53:48.307060 11835 net.cpp:84] Creating Layer res3_1_res3_1_0_split
I0523 21:53:48.307063 11835 net.cpp:406] res3_1_res3_1_0_split <- res3_1
I0523 21:53:48.307071 11835 net.cpp:380] res3_1_res3_1_0_split -> res3_1_res3_1_0_split_0
I0523 21:53:48.307077 11835 net.cpp:380] res3_1_res3_1_0_split -> res3_1_res3_1_0_split_1
I0523 21:53:48.307116 11835 net.cpp:122] Setting up res3_1_res3_1_0_split
I0523 21:53:48.307122 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.307127 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.307130 11835 net.cpp:137] Memory required for data: 2171732736
I0523 21:53:48.307133 11835 layer_factory.hpp:77] Creating layer conv3_2_ex
I0523 21:53:48.307145 11835 net.cpp:84] Creating Layer conv3_2_ex
I0523 21:53:48.307152 11835 net.cpp:406] conv3_2_ex <- res3_1_res3_1_0_split_0
I0523 21:53:48.307158 11835 net.cpp:380] conv3_2_ex -> conv3_2_ex
I0523 21:53:48.309022 11835 net.cpp:122] Setting up conv3_2_ex
I0523 21:53:48.309041 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.309046 11835 net.cpp:137] Memory required for data: 2182742784
I0523 21:53:48.309052 11835 layer_factory.hpp:77] Creating layer conv3_2_ex/bn
I0523 21:53:48.309080 11835 net.cpp:84] Creating Layer conv3_2_ex/bn
I0523 21:53:48.309087 11835 net.cpp:406] conv3_2_ex/bn <- conv3_2_ex
I0523 21:53:48.309093 11835 net.cpp:367] conv3_2_ex/bn -> conv3_2_ex (in-place)
I0523 21:53:48.309325 11835 net.cpp:122] Setting up conv3_2_ex/bn
I0523 21:53:48.309334 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.309337 11835 net.cpp:137] Memory required for data: 2193752832
I0523 21:53:48.309346 11835 layer_factory.hpp:77] Creating layer conv3_2_ex/scale
I0523 21:53:48.309360 11835 net.cpp:84] Creating Layer conv3_2_ex/scale
I0523 21:53:48.309365 11835 net.cpp:406] conv3_2_ex/scale <- conv3_2_ex
I0523 21:53:48.309370 11835 net.cpp:367] conv3_2_ex/scale -> conv3_2_ex (in-place)
I0523 21:53:48.309419 11835 layer_factory.hpp:77] Creating layer conv3_2_ex/scale
I0523 21:53:48.309554 11835 net.cpp:122] Setting up conv3_2_ex/scale
I0523 21:53:48.309562 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.309566 11835 net.cpp:137] Memory required for data: 2204762880
I0523 21:53:48.309574 11835 layer_factory.hpp:77] Creating layer relu3_2_ex
I0523 21:53:48.309582 11835 net.cpp:84] Creating Layer relu3_2_ex
I0523 21:53:48.309587 11835 net.cpp:406] relu3_2_ex <- conv3_2_ex
I0523 21:53:48.309594 11835 net.cpp:367] relu3_2_ex -> conv3_2_ex (in-place)
I0523 21:53:48.309720 11835 net.cpp:122] Setting up relu3_2_ex
I0523 21:53:48.309728 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.309732 11835 net.cpp:137] Memory required for data: 2215772928
I0523 21:53:48.309737 11835 layer_factory.hpp:77] Creating layer conv3_2_dw
I0523 21:53:48.309748 11835 net.cpp:84] Creating Layer conv3_2_dw
I0523 21:53:48.309754 11835 net.cpp:406] conv3_2_dw <- conv3_2_ex
I0523 21:53:48.309761 11835 net.cpp:380] conv3_2_dw -> conv3_2_dw
I0523 21:53:48.309999 11835 net.cpp:122] Setting up conv3_2_dw
I0523 21:53:48.310009 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.310011 11835 net.cpp:137] Memory required for data: 2226782976
I0523 21:53:48.310016 11835 layer_factory.hpp:77] Creating layer conv3_2_dw/bn
I0523 21:53:48.310024 11835 net.cpp:84] Creating Layer conv3_2_dw/bn
I0523 21:53:48.310029 11835 net.cpp:406] conv3_2_dw/bn <- conv3_2_dw
I0523 21:53:48.310035 11835 net.cpp:367] conv3_2_dw/bn -> conv3_2_dw (in-place)
I0523 21:53:48.310253 11835 net.cpp:122] Setting up conv3_2_dw/bn
I0523 21:53:48.310261 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.310266 11835 net.cpp:137] Memory required for data: 2237793024
I0523 21:53:48.310273 11835 layer_factory.hpp:77] Creating layer conv3_2_dw/scale
I0523 21:53:48.310284 11835 net.cpp:84] Creating Layer conv3_2_dw/scale
I0523 21:53:48.310290 11835 net.cpp:406] conv3_2_dw/scale <- conv3_2_dw
I0523 21:53:48.310298 11835 net.cpp:367] conv3_2_dw/scale -> conv3_2_dw (in-place)
I0523 21:53:48.310339 11835 layer_factory.hpp:77] Creating layer conv3_2_dw/scale
I0523 21:53:48.310465 11835 net.cpp:122] Setting up conv3_2_dw/scale
I0523 21:53:48.310475 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.310482 11835 net.cpp:137] Memory required for data: 2248803072
I0523 21:53:48.310489 11835 layer_factory.hpp:77] Creating layer relu3_2_dw
I0523 21:53:48.310497 11835 net.cpp:84] Creating Layer relu3_2_dw
I0523 21:53:48.310501 11835 net.cpp:406] relu3_2_dw <- conv3_2_dw
I0523 21:53:48.310506 11835 net.cpp:367] relu3_2_dw -> conv3_2_dw (in-place)
I0523 21:53:48.310632 11835 net.cpp:122] Setting up relu3_2_dw
I0523 21:53:48.310642 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.310644 11835 net.cpp:137] Memory required for data: 2259813120
I0523 21:53:48.310649 11835 layer_factory.hpp:77] Creating layer conv3_2_em
I0523 21:53:48.310662 11835 net.cpp:84] Creating Layer conv3_2_em
I0523 21:53:48.310668 11835 net.cpp:406] conv3_2_em <- conv3_2_dw
I0523 21:53:48.310678 11835 net.cpp:380] conv3_2_em -> conv3_2_em
I0523 21:53:48.312343 11835 net.cpp:122] Setting up conv3_2_em
I0523 21:53:48.312361 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.312381 11835 net.cpp:137] Memory required for data: 2265318144
I0523 21:53:48.312388 11835 layer_factory.hpp:77] Creating layer conv3_2_em/bn
I0523 21:53:48.312402 11835 net.cpp:84] Creating Layer conv3_2_em/bn
I0523 21:53:48.312407 11835 net.cpp:406] conv3_2_em/bn <- conv3_2_em
I0523 21:53:48.312417 11835 net.cpp:367] conv3_2_em/bn -> conv3_2_em (in-place)
I0523 21:53:48.312651 11835 net.cpp:122] Setting up conv3_2_em/bn
I0523 21:53:48.312660 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.312664 11835 net.cpp:137] Memory required for data: 2270823168
I0523 21:53:48.312671 11835 layer_factory.hpp:77] Creating layer conv3_2_em/scale
I0523 21:53:48.312687 11835 net.cpp:84] Creating Layer conv3_2_em/scale
I0523 21:53:48.312693 11835 net.cpp:406] conv3_2_em/scale <- conv3_2_em
I0523 21:53:48.312700 11835 net.cpp:367] conv3_2_em/scale -> conv3_2_em (in-place)
I0523 21:53:48.312747 11835 layer_factory.hpp:77] Creating layer conv3_2_em/scale
I0523 21:53:48.312882 11835 net.cpp:122] Setting up conv3_2_em/scale
I0523 21:53:48.312891 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.312894 11835 net.cpp:137] Memory required for data: 2276328192
I0523 21:53:48.312901 11835 layer_factory.hpp:77] Creating layer res3_2
I0523 21:53:48.312909 11835 net.cpp:84] Creating Layer res3_2
I0523 21:53:48.312916 11835 net.cpp:406] res3_2 <- res3_1_res3_1_0_split_1
I0523 21:53:48.312921 11835 net.cpp:406] res3_2 <- conv3_2_em
I0523 21:53:48.312927 11835 net.cpp:380] res3_2 -> res3_2
I0523 21:53:48.312958 11835 net.cpp:122] Setting up res3_2
I0523 21:53:48.312965 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.312968 11835 net.cpp:137] Memory required for data: 2281833216
I0523 21:53:48.312973 11835 layer_factory.hpp:77] Creating layer res3_2_res3_2_0_split
I0523 21:53:48.312978 11835 net.cpp:84] Creating Layer res3_2_res3_2_0_split
I0523 21:53:48.312981 11835 net.cpp:406] res3_2_res3_2_0_split <- res3_2
I0523 21:53:48.312988 11835 net.cpp:380] res3_2_res3_2_0_split -> res3_2_res3_2_0_split_0
I0523 21:53:48.312994 11835 net.cpp:380] res3_2_res3_2_0_split -> res3_2_res3_2_0_split_1
I0523 21:53:48.313036 11835 net.cpp:122] Setting up res3_2_res3_2_0_split
I0523 21:53:48.313043 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.313048 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.313051 11835 net.cpp:137] Memory required for data: 2292843264
I0523 21:53:48.313055 11835 layer_factory.hpp:77] Creating layer conv3_3_ex
I0523 21:53:48.313067 11835 net.cpp:84] Creating Layer conv3_3_ex
I0523 21:53:48.313072 11835 net.cpp:406] conv3_3_ex <- res3_2_res3_2_0_split_0
I0523 21:53:48.313079 11835 net.cpp:380] conv3_3_ex -> conv3_3_ex
I0523 21:53:48.314826 11835 net.cpp:122] Setting up conv3_3_ex
I0523 21:53:48.314846 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.314852 11835 net.cpp:137] Memory required for data: 2303853312
I0523 21:53:48.314862 11835 layer_factory.hpp:77] Creating layer conv3_3_ex/bn
I0523 21:53:48.314872 11835 net.cpp:84] Creating Layer conv3_3_ex/bn
I0523 21:53:48.314878 11835 net.cpp:406] conv3_3_ex/bn <- conv3_3_ex
I0523 21:53:48.314896 11835 net.cpp:367] conv3_3_ex/bn -> conv3_3_ex (in-place)
I0523 21:53:48.315136 11835 net.cpp:122] Setting up conv3_3_ex/bn
I0523 21:53:48.315145 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.315150 11835 net.cpp:137] Memory required for data: 2314863360
I0523 21:53:48.315157 11835 layer_factory.hpp:77] Creating layer conv3_3_ex/scale
I0523 21:53:48.315166 11835 net.cpp:84] Creating Layer conv3_3_ex/scale
I0523 21:53:48.315172 11835 net.cpp:406] conv3_3_ex/scale <- conv3_3_ex
I0523 21:53:48.315178 11835 net.cpp:367] conv3_3_ex/scale -> conv3_3_ex (in-place)
I0523 21:53:48.315223 11835 layer_factory.hpp:77] Creating layer conv3_3_ex/scale
I0523 21:53:48.315362 11835 net.cpp:122] Setting up conv3_3_ex/scale
I0523 21:53:48.315372 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.315377 11835 net.cpp:137] Memory required for data: 2325873408
I0523 21:53:48.315398 11835 layer_factory.hpp:77] Creating layer relu3_3_ex
I0523 21:53:48.315410 11835 net.cpp:84] Creating Layer relu3_3_ex
I0523 21:53:48.315415 11835 net.cpp:406] relu3_3_ex <- conv3_3_ex
I0523 21:53:48.315420 11835 net.cpp:367] relu3_3_ex -> conv3_3_ex (in-place)
I0523 21:53:48.315559 11835 net.cpp:122] Setting up relu3_3_ex
I0523 21:53:48.315567 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.315572 11835 net.cpp:137] Memory required for data: 2336883456
I0523 21:53:48.315577 11835 layer_factory.hpp:77] Creating layer conv3_3_dw
I0523 21:53:48.315585 11835 net.cpp:84] Creating Layer conv3_3_dw
I0523 21:53:48.315591 11835 net.cpp:406] conv3_3_dw <- conv3_3_ex
I0523 21:53:48.315598 11835 net.cpp:380] conv3_3_dw -> conv3_3_dw
I0523 21:53:48.315843 11835 net.cpp:122] Setting up conv3_3_dw
I0523 21:53:48.315852 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.315855 11835 net.cpp:137] Memory required for data: 2347893504
I0523 21:53:48.315863 11835 layer_factory.hpp:77] Creating layer conv3_3_dw/bn
I0523 21:53:48.315872 11835 net.cpp:84] Creating Layer conv3_3_dw/bn
I0523 21:53:48.315877 11835 net.cpp:406] conv3_3_dw/bn <- conv3_3_dw
I0523 21:53:48.315883 11835 net.cpp:367] conv3_3_dw/bn -> conv3_3_dw (in-place)
I0523 21:53:48.316109 11835 net.cpp:122] Setting up conv3_3_dw/bn
I0523 21:53:48.316117 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.316120 11835 net.cpp:137] Memory required for data: 2358903552
I0523 21:53:48.316128 11835 layer_factory.hpp:77] Creating layer conv3_3_dw/scale
I0523 21:53:48.316141 11835 net.cpp:84] Creating Layer conv3_3_dw/scale
I0523 21:53:48.316146 11835 net.cpp:406] conv3_3_dw/scale <- conv3_3_dw
I0523 21:53:48.316153 11835 net.cpp:367] conv3_3_dw/scale -> conv3_3_dw (in-place)
I0523 21:53:48.316197 11835 layer_factory.hpp:77] Creating layer conv3_3_dw/scale
I0523 21:53:48.316329 11835 net.cpp:122] Setting up conv3_3_dw/scale
I0523 21:53:48.316336 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.316339 11835 net.cpp:137] Memory required for data: 2369913600
I0523 21:53:48.316345 11835 layer_factory.hpp:77] Creating layer relu3_3_dw
I0523 21:53:48.316351 11835 net.cpp:84] Creating Layer relu3_3_dw
I0523 21:53:48.316355 11835 net.cpp:406] relu3_3_dw <- conv3_3_dw
I0523 21:53:48.316362 11835 net.cpp:367] relu3_3_dw -> conv3_3_dw (in-place)
I0523 21:53:48.316493 11835 net.cpp:122] Setting up relu3_3_dw
I0523 21:53:48.316501 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.316504 11835 net.cpp:137] Memory required for data: 2380923648
I0523 21:53:48.316509 11835 layer_factory.hpp:77] Creating layer conv3_3_em
I0523 21:53:48.316529 11835 net.cpp:84] Creating Layer conv3_3_em
I0523 21:53:48.316534 11835 net.cpp:406] conv3_3_em <- conv3_3_dw
I0523 21:53:48.316541 11835 net.cpp:380] conv3_3_em -> conv3_3_em
I0523 21:53:48.319730 11835 net.cpp:122] Setting up conv3_3_em
I0523 21:53:48.319749 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.319756 11835 net.cpp:137] Memory required for data: 2386428672
I0523 21:53:48.319762 11835 layer_factory.hpp:77] Creating layer conv3_3_em/bn
I0523 21:53:48.319775 11835 net.cpp:84] Creating Layer conv3_3_em/bn
I0523 21:53:48.319782 11835 net.cpp:406] conv3_3_em/bn <- conv3_3_em
I0523 21:53:48.319787 11835 net.cpp:367] conv3_3_em/bn -> conv3_3_em (in-place)
I0523 21:53:48.320034 11835 net.cpp:122] Setting up conv3_3_em/bn
I0523 21:53:48.320041 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.320045 11835 net.cpp:137] Memory required for data: 2391933696
I0523 21:53:48.320052 11835 layer_factory.hpp:77] Creating layer conv3_3_em/scale
I0523 21:53:48.320061 11835 net.cpp:84] Creating Layer conv3_3_em/scale
I0523 21:53:48.320065 11835 net.cpp:406] conv3_3_em/scale <- conv3_3_em
I0523 21:53:48.320073 11835 net.cpp:367] conv3_3_em/scale -> conv3_3_em (in-place)
I0523 21:53:48.320121 11835 layer_factory.hpp:77] Creating layer conv3_3_em/scale
I0523 21:53:48.320256 11835 net.cpp:122] Setting up conv3_3_em/scale
I0523 21:53:48.320278 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.320282 11835 net.cpp:137] Memory required for data: 2397438720
I0523 21:53:48.320288 11835 layer_factory.hpp:77] Creating layer res3_3
I0523 21:53:48.320298 11835 net.cpp:84] Creating Layer res3_3
I0523 21:53:48.320304 11835 net.cpp:406] res3_3 <- res3_2_res3_2_0_split_1
I0523 21:53:48.320309 11835 net.cpp:406] res3_3 <- conv3_3_em
I0523 21:53:48.320318 11835 net.cpp:380] res3_3 -> res3_3
I0523 21:53:48.320348 11835 net.cpp:122] Setting up res3_3
I0523 21:53:48.320358 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.320363 11835 net.cpp:137] Memory required for data: 2402943744
I0523 21:53:48.320367 11835 layer_factory.hpp:77] Creating layer res3_3_res3_3_0_split
I0523 21:53:48.320376 11835 net.cpp:84] Creating Layer res3_3_res3_3_0_split
I0523 21:53:48.320380 11835 net.cpp:406] res3_3_res3_3_0_split <- res3_3
I0523 21:53:48.320385 11835 net.cpp:380] res3_3_res3_3_0_split -> res3_3_res3_3_0_split_0
I0523 21:53:48.320392 11835 net.cpp:380] res3_3_res3_3_0_split -> res3_3_res3_3_0_split_1
I0523 21:53:48.320436 11835 net.cpp:122] Setting up res3_3_res3_3_0_split
I0523 21:53:48.320442 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.320446 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.320451 11835 net.cpp:137] Memory required for data: 2413953792
I0523 21:53:48.320454 11835 layer_factory.hpp:77] Creating layer conv3_4_ex
I0523 21:53:48.320466 11835 net.cpp:84] Creating Layer conv3_4_ex
I0523 21:53:48.320472 11835 net.cpp:406] conv3_4_ex <- res3_3_res3_3_0_split_0
I0523 21:53:48.320478 11835 net.cpp:380] conv3_4_ex -> conv3_4_ex
I0523 21:53:48.322069 11835 net.cpp:122] Setting up conv3_4_ex
I0523 21:53:48.322088 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.322093 11835 net.cpp:137] Memory required for data: 2424963840
I0523 21:53:48.322099 11835 layer_factory.hpp:77] Creating layer conv3_4_ex/bn
I0523 21:53:48.322111 11835 net.cpp:84] Creating Layer conv3_4_ex/bn
I0523 21:53:48.322118 11835 net.cpp:406] conv3_4_ex/bn <- conv3_4_ex
I0523 21:53:48.322125 11835 net.cpp:367] conv3_4_ex/bn -> conv3_4_ex (in-place)
I0523 21:53:48.322366 11835 net.cpp:122] Setting up conv3_4_ex/bn
I0523 21:53:48.322376 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.322378 11835 net.cpp:137] Memory required for data: 2435973888
I0523 21:53:48.322387 11835 layer_factory.hpp:77] Creating layer conv3_4_ex/scale
I0523 21:53:48.322396 11835 net.cpp:84] Creating Layer conv3_4_ex/scale
I0523 21:53:48.322402 11835 net.cpp:406] conv3_4_ex/scale <- conv3_4_ex
I0523 21:53:48.322407 11835 net.cpp:367] conv3_4_ex/scale -> conv3_4_ex (in-place)
I0523 21:53:48.322453 11835 layer_factory.hpp:77] Creating layer conv3_4_ex/scale
I0523 21:53:48.322585 11835 net.cpp:122] Setting up conv3_4_ex/scale
I0523 21:53:48.322593 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.322597 11835 net.cpp:137] Memory required for data: 2446983936
I0523 21:53:48.322607 11835 layer_factory.hpp:77] Creating layer relu3_4_ex
I0523 21:53:48.322619 11835 net.cpp:84] Creating Layer relu3_4_ex
I0523 21:53:48.322624 11835 net.cpp:406] relu3_4_ex <- conv3_4_ex
I0523 21:53:48.322629 11835 net.cpp:367] relu3_4_ex -> conv3_4_ex (in-place)
I0523 21:53:48.322770 11835 net.cpp:122] Setting up relu3_4_ex
I0523 21:53:48.322779 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.322783 11835 net.cpp:137] Memory required for data: 2457993984
I0523 21:53:48.322788 11835 layer_factory.hpp:77] Creating layer conv3_4_dw
I0523 21:53:48.322818 11835 net.cpp:84] Creating Layer conv3_4_dw
I0523 21:53:48.322824 11835 net.cpp:406] conv3_4_dw <- conv3_4_ex
I0523 21:53:48.322839 11835 net.cpp:380] conv3_4_dw -> conv3_4_dw
I0523 21:53:48.323086 11835 net.cpp:122] Setting up conv3_4_dw
I0523 21:53:48.323093 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.323097 11835 net.cpp:137] Memory required for data: 2469004032
I0523 21:53:48.323102 11835 layer_factory.hpp:77] Creating layer conv3_4_dw/bn
I0523 21:53:48.323124 11835 net.cpp:84] Creating Layer conv3_4_dw/bn
I0523 21:53:48.323132 11835 net.cpp:406] conv3_4_dw/bn <- conv3_4_dw
I0523 21:53:48.323139 11835 net.cpp:367] conv3_4_dw/bn -> conv3_4_dw (in-place)
I0523 21:53:48.323367 11835 net.cpp:122] Setting up conv3_4_dw/bn
I0523 21:53:48.323376 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.323379 11835 net.cpp:137] Memory required for data: 2480014080
I0523 21:53:48.323387 11835 layer_factory.hpp:77] Creating layer conv3_4_dw/scale
I0523 21:53:48.323400 11835 net.cpp:84] Creating Layer conv3_4_dw/scale
I0523 21:53:48.323405 11835 net.cpp:406] conv3_4_dw/scale <- conv3_4_dw
I0523 21:53:48.323410 11835 net.cpp:367] conv3_4_dw/scale -> conv3_4_dw (in-place)
I0523 21:53:48.323457 11835 layer_factory.hpp:77] Creating layer conv3_4_dw/scale
I0523 21:53:48.323588 11835 net.cpp:122] Setting up conv3_4_dw/scale
I0523 21:53:48.323598 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.323602 11835 net.cpp:137] Memory required for data: 2491024128
I0523 21:53:48.323608 11835 layer_factory.hpp:77] Creating layer relu3_4_dw
I0523 21:53:48.323616 11835 net.cpp:84] Creating Layer relu3_4_dw
I0523 21:53:48.323621 11835 net.cpp:406] relu3_4_dw <- conv3_4_dw
I0523 21:53:48.323626 11835 net.cpp:367] relu3_4_dw -> conv3_4_dw (in-place)
I0523 21:53:48.323757 11835 net.cpp:122] Setting up relu3_4_dw
I0523 21:53:48.323766 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.323770 11835 net.cpp:137] Memory required for data: 2502034176
I0523 21:53:48.323774 11835 layer_factory.hpp:77] Creating layer conv3_4_em
I0523 21:53:48.323784 11835 net.cpp:84] Creating Layer conv3_4_em
I0523 21:53:48.323789 11835 net.cpp:406] conv3_4_em <- conv3_4_dw
I0523 21:53:48.323796 11835 net.cpp:380] conv3_4_em -> conv3_4_em
I0523 21:53:48.325822 11835 net.cpp:122] Setting up conv3_4_em
I0523 21:53:48.325840 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.325846 11835 net.cpp:137] Memory required for data: 2507539200
I0523 21:53:48.325852 11835 layer_factory.hpp:77] Creating layer conv3_4_em/bn
I0523 21:53:48.325878 11835 net.cpp:84] Creating Layer conv3_4_em/bn
I0523 21:53:48.325886 11835 net.cpp:406] conv3_4_em/bn <- conv3_4_em
I0523 21:53:48.325902 11835 net.cpp:367] conv3_4_em/bn -> conv3_4_em (in-place)
I0523 21:53:48.326151 11835 net.cpp:122] Setting up conv3_4_em/bn
I0523 21:53:48.326160 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.326164 11835 net.cpp:137] Memory required for data: 2513044224
I0523 21:53:48.326185 11835 layer_factory.hpp:77] Creating layer conv3_4_em/scale
I0523 21:53:48.326195 11835 net.cpp:84] Creating Layer conv3_4_em/scale
I0523 21:53:48.326200 11835 net.cpp:406] conv3_4_em/scale <- conv3_4_em
I0523 21:53:48.326206 11835 net.cpp:367] conv3_4_em/scale -> conv3_4_em (in-place)
I0523 21:53:48.326258 11835 layer_factory.hpp:77] Creating layer conv3_4_em/scale
I0523 21:53:48.326393 11835 net.cpp:122] Setting up conv3_4_em/scale
I0523 21:53:48.326401 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.326405 11835 net.cpp:137] Memory required for data: 2518549248
I0523 21:53:48.326411 11835 layer_factory.hpp:77] Creating layer res3_4
I0523 21:53:48.326418 11835 net.cpp:84] Creating Layer res3_4
I0523 21:53:48.326422 11835 net.cpp:406] res3_4 <- res3_3_res3_3_0_split_1
I0523 21:53:48.326427 11835 net.cpp:406] res3_4 <- conv3_4_em
I0523 21:53:48.326436 11835 net.cpp:380] res3_4 -> res3_4
I0523 21:53:48.326462 11835 net.cpp:122] Setting up res3_4
I0523 21:53:48.326469 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.326473 11835 net.cpp:137] Memory required for data: 2524054272
I0523 21:53:48.326476 11835 layer_factory.hpp:77] Creating layer res3_4_res3_4_0_split
I0523 21:53:48.326484 11835 net.cpp:84] Creating Layer res3_4_res3_4_0_split
I0523 21:53:48.326488 11835 net.cpp:406] res3_4_res3_4_0_split <- res3_4
I0523 21:53:48.326493 11835 net.cpp:380] res3_4_res3_4_0_split -> res3_4_res3_4_0_split_0
I0523 21:53:48.326514 11835 net.cpp:380] res3_4_res3_4_0_split -> res3_4_res3_4_0_split_1
I0523 21:53:48.326561 11835 net.cpp:122] Setting up res3_4_res3_4_0_split
I0523 21:53:48.326568 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.326573 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.326576 11835 net.cpp:137] Memory required for data: 2535064320
I0523 21:53:48.326580 11835 layer_factory.hpp:77] Creating layer conv3_5_ex
I0523 21:53:48.326592 11835 net.cpp:84] Creating Layer conv3_5_ex
I0523 21:53:48.326597 11835 net.cpp:406] conv3_5_ex <- res3_4_res3_4_0_split_0
I0523 21:53:48.326607 11835 net.cpp:380] conv3_5_ex -> conv3_5_ex
I0523 21:53:48.328655 11835 net.cpp:122] Setting up conv3_5_ex
I0523 21:53:48.328673 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.328678 11835 net.cpp:137] Memory required for data: 2546074368
I0523 21:53:48.328689 11835 layer_factory.hpp:77] Creating layer conv3_5_ex/bn
I0523 21:53:48.328701 11835 net.cpp:84] Creating Layer conv3_5_ex/bn
I0523 21:53:48.328708 11835 net.cpp:406] conv3_5_ex/bn <- conv3_5_ex
I0523 21:53:48.328716 11835 net.cpp:367] conv3_5_ex/bn -> conv3_5_ex (in-place)
I0523 21:53:48.328968 11835 net.cpp:122] Setting up conv3_5_ex/bn
I0523 21:53:48.328976 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.328980 11835 net.cpp:137] Memory required for data: 2557084416
I0523 21:53:48.328989 11835 layer_factory.hpp:77] Creating layer conv3_5_ex/scale
I0523 21:53:48.328999 11835 net.cpp:84] Creating Layer conv3_5_ex/scale
I0523 21:53:48.329005 11835 net.cpp:406] conv3_5_ex/scale <- conv3_5_ex
I0523 21:53:48.329010 11835 net.cpp:367] conv3_5_ex/scale -> conv3_5_ex (in-place)
I0523 21:53:48.329056 11835 layer_factory.hpp:77] Creating layer conv3_5_ex/scale
I0523 21:53:48.329190 11835 net.cpp:122] Setting up conv3_5_ex/scale
I0523 21:53:48.329197 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.329201 11835 net.cpp:137] Memory required for data: 2568094464
I0523 21:53:48.329207 11835 layer_factory.hpp:77] Creating layer relu3_5_ex
I0523 21:53:48.329218 11835 net.cpp:84] Creating Layer relu3_5_ex
I0523 21:53:48.329224 11835 net.cpp:406] relu3_5_ex <- conv3_5_ex
I0523 21:53:48.329229 11835 net.cpp:367] relu3_5_ex -> conv3_5_ex (in-place)
I0523 21:53:48.329370 11835 net.cpp:122] Setting up relu3_5_ex
I0523 21:53:48.329378 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.329382 11835 net.cpp:137] Memory required for data: 2579104512
I0523 21:53:48.329387 11835 layer_factory.hpp:77] Creating layer conv3_5_dw
I0523 21:53:48.329401 11835 net.cpp:84] Creating Layer conv3_5_dw
I0523 21:53:48.329407 11835 net.cpp:406] conv3_5_dw <- conv3_5_ex
I0523 21:53:48.329413 11835 net.cpp:380] conv3_5_dw -> conv3_5_dw
I0523 21:53:48.329663 11835 net.cpp:122] Setting up conv3_5_dw
I0523 21:53:48.329670 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.329674 11835 net.cpp:137] Memory required for data: 2590114560
I0523 21:53:48.329679 11835 layer_factory.hpp:77] Creating layer conv3_5_dw/bn
I0523 21:53:48.329686 11835 net.cpp:84] Creating Layer conv3_5_dw/bn
I0523 21:53:48.329690 11835 net.cpp:406] conv3_5_dw/bn <- conv3_5_dw
I0523 21:53:48.329695 11835 net.cpp:367] conv3_5_dw/bn -> conv3_5_dw (in-place)
I0523 21:53:48.329926 11835 net.cpp:122] Setting up conv3_5_dw/bn
I0523 21:53:48.329934 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.329937 11835 net.cpp:137] Memory required for data: 2601124608
I0523 21:53:48.329946 11835 layer_factory.hpp:77] Creating layer conv3_5_dw/scale
I0523 21:53:48.329955 11835 net.cpp:84] Creating Layer conv3_5_dw/scale
I0523 21:53:48.329960 11835 net.cpp:406] conv3_5_dw/scale <- conv3_5_dw
I0523 21:53:48.329965 11835 net.cpp:367] conv3_5_dw/scale -> conv3_5_dw (in-place)
I0523 21:53:48.330014 11835 layer_factory.hpp:77] Creating layer conv3_5_dw/scale
I0523 21:53:48.330145 11835 net.cpp:122] Setting up conv3_5_dw/scale
I0523 21:53:48.330153 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.330157 11835 net.cpp:137] Memory required for data: 2612134656
I0523 21:53:48.330178 11835 layer_factory.hpp:77] Creating layer relu3_5_dw
I0523 21:53:48.330184 11835 net.cpp:84] Creating Layer relu3_5_dw
I0523 21:53:48.330190 11835 net.cpp:406] relu3_5_dw <- conv3_5_dw
I0523 21:53:48.330198 11835 net.cpp:367] relu3_5_dw -> conv3_5_dw (in-place)
I0523 21:53:48.330327 11835 net.cpp:122] Setting up relu3_5_dw
I0523 21:53:48.330335 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.330338 11835 net.cpp:137] Memory required for data: 2623144704
I0523 21:53:48.330345 11835 layer_factory.hpp:77] Creating layer conv3_5_em
I0523 21:53:48.330359 11835 net.cpp:84] Creating Layer conv3_5_em
I0523 21:53:48.330364 11835 net.cpp:406] conv3_5_em <- conv3_5_dw
I0523 21:53:48.330371 11835 net.cpp:380] conv3_5_em -> conv3_5_em
I0523 21:53:48.331538 11835 net.cpp:122] Setting up conv3_5_em
I0523 21:53:48.331557 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.331562 11835 net.cpp:137] Memory required for data: 2628649728
I0523 21:53:48.331568 11835 layer_factory.hpp:77] Creating layer conv3_5_em/bn
I0523 21:53:48.331576 11835 net.cpp:84] Creating Layer conv3_5_em/bn
I0523 21:53:48.331583 11835 net.cpp:406] conv3_5_em/bn <- conv3_5_em
I0523 21:53:48.331590 11835 net.cpp:367] conv3_5_em/bn -> conv3_5_em (in-place)
I0523 21:53:48.331835 11835 net.cpp:122] Setting up conv3_5_em/bn
I0523 21:53:48.331843 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.331847 11835 net.cpp:137] Memory required for data: 2634154752
I0523 21:53:48.331856 11835 layer_factory.hpp:77] Creating layer conv3_5_em/scale
I0523 21:53:48.331862 11835 net.cpp:84] Creating Layer conv3_5_em/scale
I0523 21:53:48.331868 11835 net.cpp:406] conv3_5_em/scale <- conv3_5_em
I0523 21:53:48.331876 11835 net.cpp:367] conv3_5_em/scale -> conv3_5_em (in-place)
I0523 21:53:48.331919 11835 layer_factory.hpp:77] Creating layer conv3_5_em/scale
I0523 21:53:48.332051 11835 net.cpp:122] Setting up conv3_5_em/scale
I0523 21:53:48.332060 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.332064 11835 net.cpp:137] Memory required for data: 2639659776
I0523 21:53:48.332070 11835 layer_factory.hpp:77] Creating layer res3_5
I0523 21:53:48.332080 11835 net.cpp:84] Creating Layer res3_5
I0523 21:53:48.332087 11835 net.cpp:406] res3_5 <- res3_4_res3_4_0_split_1
I0523 21:53:48.332092 11835 net.cpp:406] res3_5 <- conv3_5_em
I0523 21:53:48.332096 11835 net.cpp:380] res3_5 -> res3_5
I0523 21:53:48.332125 11835 net.cpp:122] Setting up res3_5
I0523 21:53:48.332132 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.332136 11835 net.cpp:137] Memory required for data: 2645164800
I0523 21:53:48.332141 11835 layer_factory.hpp:77] Creating layer res3_5_res3_5_0_split
I0523 21:53:48.332150 11835 net.cpp:84] Creating Layer res3_5_res3_5_0_split
I0523 21:53:48.332154 11835 net.cpp:406] res3_5_res3_5_0_split <- res3_5
I0523 21:53:48.332159 11835 net.cpp:380] res3_5_res3_5_0_split -> res3_5_res3_5_0_split_0
I0523 21:53:48.332166 11835 net.cpp:380] res3_5_res3_5_0_split -> res3_5_res3_5_0_split_1
I0523 21:53:48.332206 11835 net.cpp:122] Setting up res3_5_res3_5_0_split
I0523 21:53:48.332213 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.332218 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.332221 11835 net.cpp:137] Memory required for data: 2656174848
I0523 21:53:48.332224 11835 layer_factory.hpp:77] Creating layer conv3_6_ex
I0523 21:53:48.332237 11835 net.cpp:84] Creating Layer conv3_6_ex
I0523 21:53:48.332242 11835 net.cpp:406] conv3_6_ex <- res3_5_res3_5_0_split_0
I0523 21:53:48.332248 11835 net.cpp:380] conv3_6_ex -> conv3_6_ex
I0523 21:53:48.335661 11835 net.cpp:122] Setting up conv3_6_ex
I0523 21:53:48.335697 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.335705 11835 net.cpp:137] Memory required for data: 2667184896
I0523 21:53:48.335717 11835 layer_factory.hpp:77] Creating layer conv3_6_ex/bn
I0523 21:53:48.335738 11835 net.cpp:84] Creating Layer conv3_6_ex/bn
I0523 21:53:48.335748 11835 net.cpp:406] conv3_6_ex/bn <- conv3_6_ex
I0523 21:53:48.335783 11835 net.cpp:367] conv3_6_ex/bn -> conv3_6_ex (in-place)
I0523 21:53:48.336058 11835 net.cpp:122] Setting up conv3_6_ex/bn
I0523 21:53:48.336068 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.336072 11835 net.cpp:137] Memory required for data: 2678194944
I0523 21:53:48.336083 11835 layer_factory.hpp:77] Creating layer conv3_6_ex/scale
I0523 21:53:48.336098 11835 net.cpp:84] Creating Layer conv3_6_ex/scale
I0523 21:53:48.336104 11835 net.cpp:406] conv3_6_ex/scale <- conv3_6_ex
I0523 21:53:48.336112 11835 net.cpp:367] conv3_6_ex/scale -> conv3_6_ex (in-place)
I0523 21:53:48.336170 11835 layer_factory.hpp:77] Creating layer conv3_6_ex/scale
I0523 21:53:48.336310 11835 net.cpp:122] Setting up conv3_6_ex/scale
I0523 21:53:48.336320 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.336324 11835 net.cpp:137] Memory required for data: 2689204992
I0523 21:53:48.336331 11835 layer_factory.hpp:77] Creating layer relu3_6_ex
I0523 21:53:48.336341 11835 net.cpp:84] Creating Layer relu3_6_ex
I0523 21:53:48.336347 11835 net.cpp:406] relu3_6_ex <- conv3_6_ex
I0523 21:53:48.336355 11835 net.cpp:367] relu3_6_ex -> conv3_6_ex (in-place)
I0523 21:53:48.336518 11835 net.cpp:122] Setting up relu3_6_ex
I0523 21:53:48.336529 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.336532 11835 net.cpp:137] Memory required for data: 2700215040
I0523 21:53:48.336539 11835 layer_factory.hpp:77] Creating layer conv3_6_dw
I0523 21:53:48.336552 11835 net.cpp:84] Creating Layer conv3_6_dw
I0523 21:53:48.336558 11835 net.cpp:406] conv3_6_dw <- conv3_6_ex
I0523 21:53:48.336566 11835 net.cpp:380] conv3_6_dw -> conv3_6_dw
I0523 21:53:48.336835 11835 net.cpp:122] Setting up conv3_6_dw
I0523 21:53:48.336845 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.336849 11835 net.cpp:137] Memory required for data: 2711225088
I0523 21:53:48.336855 11835 layer_factory.hpp:77] Creating layer conv3_6_dw/bn
I0523 21:53:48.336863 11835 net.cpp:84] Creating Layer conv3_6_dw/bn
I0523 21:53:48.336868 11835 net.cpp:406] conv3_6_dw/bn <- conv3_6_dw
I0523 21:53:48.336875 11835 net.cpp:367] conv3_6_dw/bn -> conv3_6_dw (in-place)
I0523 21:53:48.337107 11835 net.cpp:122] Setting up conv3_6_dw/bn
I0523 21:53:48.337116 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.337121 11835 net.cpp:137] Memory required for data: 2722235136
I0523 21:53:48.337131 11835 layer_factory.hpp:77] Creating layer conv3_6_dw/scale
I0523 21:53:48.337141 11835 net.cpp:84] Creating Layer conv3_6_dw/scale
I0523 21:53:48.337146 11835 net.cpp:406] conv3_6_dw/scale <- conv3_6_dw
I0523 21:53:48.337153 11835 net.cpp:367] conv3_6_dw/scale -> conv3_6_dw (in-place)
I0523 21:53:48.337203 11835 layer_factory.hpp:77] Creating layer conv3_6_dw/scale
I0523 21:53:48.337340 11835 net.cpp:122] Setting up conv3_6_dw/scale
I0523 21:53:48.337350 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.337354 11835 net.cpp:137] Memory required for data: 2733245184
I0523 21:53:48.337361 11835 layer_factory.hpp:77] Creating layer relu3_6_dw
I0523 21:53:48.337369 11835 net.cpp:84] Creating Layer relu3_6_dw
I0523 21:53:48.337374 11835 net.cpp:406] relu3_6_dw <- conv3_6_dw
I0523 21:53:48.337379 11835 net.cpp:367] relu3_6_dw -> conv3_6_dw (in-place)
I0523 21:53:48.337529 11835 net.cpp:122] Setting up relu3_6_dw
I0523 21:53:48.337540 11835 net.cpp:129] Top shape: 64 256 14 12 (2752512)
I0523 21:53:48.337543 11835 net.cpp:137] Memory required for data: 2744255232
I0523 21:53:48.337550 11835 layer_factory.hpp:77] Creating layer conv3_6_em
I0523 21:53:48.337563 11835 net.cpp:84] Creating Layer conv3_6_em
I0523 21:53:48.337568 11835 net.cpp:406] conv3_6_em <- conv3_6_dw
I0523 21:53:48.337577 11835 net.cpp:380] conv3_6_em -> conv3_6_em
I0523 21:53:48.339308 11835 net.cpp:122] Setting up conv3_6_em
I0523 21:53:48.339329 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.339334 11835 net.cpp:137] Memory required for data: 2749760256
I0523 21:53:48.339339 11835 layer_factory.hpp:77] Creating layer conv3_6_em/bn
I0523 21:53:48.339370 11835 net.cpp:84] Creating Layer conv3_6_em/bn
I0523 21:53:48.339380 11835 net.cpp:406] conv3_6_em/bn <- conv3_6_em
I0523 21:53:48.339388 11835 net.cpp:367] conv3_6_em/bn -> conv3_6_em (in-place)
I0523 21:53:48.339632 11835 net.cpp:122] Setting up conv3_6_em/bn
I0523 21:53:48.339643 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.339646 11835 net.cpp:137] Memory required for data: 2755265280
I0523 21:53:48.339656 11835 layer_factory.hpp:77] Creating layer conv3_6_em/scale
I0523 21:53:48.339666 11835 net.cpp:84] Creating Layer conv3_6_em/scale
I0523 21:53:48.339675 11835 net.cpp:406] conv3_6_em/scale <- conv3_6_em
I0523 21:53:48.339681 11835 net.cpp:367] conv3_6_em/scale -> conv3_6_em (in-place)
I0523 21:53:48.339733 11835 layer_factory.hpp:77] Creating layer conv3_6_em/scale
I0523 21:53:48.339877 11835 net.cpp:122] Setting up conv3_6_em/scale
I0523 21:53:48.339886 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.339890 11835 net.cpp:137] Memory required for data: 2760770304
I0523 21:53:48.339897 11835 layer_factory.hpp:77] Creating layer res3_6
I0523 21:53:48.339908 11835 net.cpp:84] Creating Layer res3_6
I0523 21:53:48.339915 11835 net.cpp:406] res3_6 <- res3_5_res3_5_0_split_1
I0523 21:53:48.339920 11835 net.cpp:406] res3_6 <- conv3_6_em
I0523 21:53:48.339927 11835 net.cpp:380] res3_6 -> res3_6
I0523 21:53:48.339962 11835 net.cpp:122] Setting up res3_6
I0523 21:53:48.339970 11835 net.cpp:129] Top shape: 64 128 14 12 (1376256)
I0523 21:53:48.339974 11835 net.cpp:137] Memory required for data: 2766275328
I0523 21:53:48.339978 11835 layer_factory.hpp:77] Creating layer conv4_ex
I0523 21:53:48.339990 11835 net.cpp:84] Creating Layer conv4_ex
I0523 21:53:48.339996 11835 net.cpp:406] conv4_ex <- res3_6
I0523 21:53:48.340003 11835 net.cpp:380] conv4_ex -> conv4_ex
I0523 21:53:48.343652 11835 net.cpp:122] Setting up conv4_ex
I0523 21:53:48.343672 11835 net.cpp:129] Top shape: 64 512 14 12 (5505024)
I0523 21:53:48.343677 11835 net.cpp:137] Memory required for data: 2788295424
I0523 21:53:48.343685 11835 layer_factory.hpp:77] Creating layer conv4_ex/bn
I0523 21:53:48.343698 11835 net.cpp:84] Creating Layer conv4_ex/bn
I0523 21:53:48.343704 11835 net.cpp:406] conv4_ex/bn <- conv4_ex
I0523 21:53:48.343714 11835 net.cpp:367] conv4_ex/bn -> conv4_ex (in-place)
I0523 21:53:48.343978 11835 net.cpp:122] Setting up conv4_ex/bn
I0523 21:53:48.343988 11835 net.cpp:129] Top shape: 64 512 14 12 (5505024)
I0523 21:53:48.343991 11835 net.cpp:137] Memory required for data: 2810315520
I0523 21:53:48.344048 11835 layer_factory.hpp:77] Creating layer conv4_ex/scale
I0523 21:53:48.344069 11835 net.cpp:84] Creating Layer conv4_ex/scale
I0523 21:53:48.344075 11835 net.cpp:406] conv4_ex/scale <- conv4_ex
I0523 21:53:48.344082 11835 net.cpp:367] conv4_ex/scale -> conv4_ex (in-place)
I0523 21:53:48.344135 11835 layer_factory.hpp:77] Creating layer conv4_ex/scale
I0523 21:53:48.344290 11835 net.cpp:122] Setting up conv4_ex/scale
I0523 21:53:48.344301 11835 net.cpp:129] Top shape: 64 512 14 12 (5505024)
I0523 21:53:48.344305 11835 net.cpp:137] Memory required for data: 2832335616
I0523 21:53:48.344312 11835 layer_factory.hpp:77] Creating layer relu4_ex
I0523 21:53:48.344321 11835 net.cpp:84] Creating Layer relu4_ex
I0523 21:53:48.344327 11835 net.cpp:406] relu4_ex <- conv4_ex
I0523 21:53:48.344333 11835 net.cpp:367] relu4_ex -> conv4_ex (in-place)
I0523 21:53:48.344534 11835 net.cpp:122] Setting up relu4_ex
I0523 21:53:48.344544 11835 net.cpp:129] Top shape: 64 512 14 12 (5505024)
I0523 21:53:48.344548 11835 net.cpp:137] Memory required for data: 2854355712
I0523 21:53:48.344554 11835 layer_factory.hpp:77] Creating layer conv4_dw
I0523 21:53:48.344564 11835 net.cpp:84] Creating Layer conv4_dw
I0523 21:53:48.344568 11835 net.cpp:406] conv4_dw <- conv4_ex
I0523 21:53:48.344575 11835 net.cpp:380] conv4_dw -> conv4_dw
I0523 21:53:48.344863 11835 net.cpp:122] Setting up conv4_dw
I0523 21:53:48.344874 11835 net.cpp:129] Top shape: 64 512 7 6 (1376256)
I0523 21:53:48.344892 11835 net.cpp:137] Memory required for data: 2859860736
I0523 21:53:48.344898 11835 layer_factory.hpp:77] Creating layer conv4_dw/bn
I0523 21:53:48.344908 11835 net.cpp:84] Creating Layer conv4_dw/bn
I0523 21:53:48.344913 11835 net.cpp:406] conv4_dw/bn <- conv4_dw
I0523 21:53:48.344918 11835 net.cpp:367] conv4_dw/bn -> conv4_dw (in-place)
I0523 21:53:48.345180 11835 net.cpp:122] Setting up conv4_dw/bn
I0523 21:53:48.345190 11835 net.cpp:129] Top shape: 64 512 7 6 (1376256)
I0523 21:53:48.345193 11835 net.cpp:137] Memory required for data: 2865365760
I0523 21:53:48.345201 11835 layer_factory.hpp:77] Creating layer conv4_dw/scale
I0523 21:53:48.345211 11835 net.cpp:84] Creating Layer conv4_dw/scale
I0523 21:53:48.345216 11835 net.cpp:406] conv4_dw/scale <- conv4_dw
I0523 21:53:48.345222 11835 net.cpp:367] conv4_dw/scale -> conv4_dw (in-place)
I0523 21:53:48.345268 11835 layer_factory.hpp:77] Creating layer conv4_dw/scale
I0523 21:53:48.345419 11835 net.cpp:122] Setting up conv4_dw/scale
I0523 21:53:48.345428 11835 net.cpp:129] Top shape: 64 512 7 6 (1376256)
I0523 21:53:48.345432 11835 net.cpp:137] Memory required for data: 2870870784
I0523 21:53:48.345439 11835 layer_factory.hpp:77] Creating layer relu4_dw
I0523 21:53:48.345446 11835 net.cpp:84] Creating Layer relu4_dw
I0523 21:53:48.345453 11835 net.cpp:406] relu4_dw <- conv4_dw
I0523 21:53:48.345459 11835 net.cpp:367] relu4_dw -> conv4_dw (in-place)
I0523 21:53:48.345577 11835 net.cpp:122] Setting up relu4_dw
I0523 21:53:48.345587 11835 net.cpp:129] Top shape: 64 512 7 6 (1376256)
I0523 21:53:48.345590 11835 net.cpp:137] Memory required for data: 2876375808
I0523 21:53:48.345597 11835 layer_factory.hpp:77] Creating layer conv4_em
I0523 21:53:48.345612 11835 net.cpp:84] Creating Layer conv4_em
I0523 21:53:48.345618 11835 net.cpp:406] conv4_em <- conv4_dw
I0523 21:53:48.345624 11835 net.cpp:380] conv4_em -> conv4_em
I0523 21:53:48.348383 11835 net.cpp:122] Setting up conv4_em
I0523 21:53:48.348402 11835 net.cpp:129] Top shape: 64 128 7 6 (344064)
I0523 21:53:48.348407 11835 net.cpp:137] Memory required for data: 2877752064
I0523 21:53:48.348414 11835 layer_factory.hpp:77] Creating layer conv4_em/bn
I0523 21:53:48.348428 11835 net.cpp:84] Creating Layer conv4_em/bn
I0523 21:53:48.348433 11835 net.cpp:406] conv4_em/bn <- conv4_em
I0523 21:53:48.348440 11835 net.cpp:367] conv4_em/bn -> conv4_em (in-place)
I0523 21:53:48.348692 11835 net.cpp:122] Setting up conv4_em/bn
I0523 21:53:48.348702 11835 net.cpp:129] Top shape: 64 128 7 6 (344064)
I0523 21:53:48.348706 11835 net.cpp:137] Memory required for data: 2879128320
I0523 21:53:48.348714 11835 layer_factory.hpp:77] Creating layer conv4_em/scale
I0523 21:53:48.348723 11835 net.cpp:84] Creating Layer conv4_em/scale
I0523 21:53:48.348727 11835 net.cpp:406] conv4_em/scale <- conv4_em
I0523 21:53:48.348736 11835 net.cpp:367] conv4_em/scale -> conv4_em (in-place)
I0523 21:53:48.348785 11835 layer_factory.hpp:77] Creating layer conv4_em/scale
I0523 21:53:48.348928 11835 net.cpp:122] Setting up conv4_em/scale
I0523 21:53:48.348938 11835 net.cpp:129] Top shape: 64 128 7 6 (344064)
I0523 21:53:48.348942 11835 net.cpp:137] Memory required for data: 2880504576
I0523 21:53:48.348950 11835 layer_factory.hpp:77] Creating layer conv4_em_conv4_em/scale_0_split
I0523 21:53:48.348958 11835 net.cpp:84] Creating Layer conv4_em_conv4_em/scale_0_split
I0523 21:53:48.348963 11835 net.cpp:406] conv4_em_conv4_em/scale_0_split <- conv4_em
I0523 21:53:48.348971 11835 net.cpp:380] conv4_em_conv4_em/scale_0_split -> conv4_em_conv4_em/scale_0_split_0
I0523 21:53:48.348981 11835 net.cpp:380] conv4_em_conv4_em/scale_0_split -> conv4_em_conv4_em/scale_0_split_1
I0523 21:53:48.349027 11835 net.cpp:122] Setting up conv4_em_conv4_em/scale_0_split
I0523 21:53:48.349036 11835 net.cpp:129] Top shape: 64 128 7 6 (344064)
I0523 21:53:48.349041 11835 net.cpp:129] Top shape: 64 128 7 6 (344064)
I0523 21:53:48.349045 11835 net.cpp:137] Memory required for data: 2883257088
I0523 21:53:48.349048 11835 layer_factory.hpp:77] Creating layer conv4_1_ex
I0523 21:53:48.349076 11835 net.cpp:84] Creating Layer conv4_1_ex
I0523 21:53:48.349084 11835 net.cpp:406] conv4_1_ex <- conv4_em_conv4_em/scale_0_split_0
I0523 21:53:48.349092 11835 net.cpp:380] conv4_1_ex -> conv4_1_ex
I0523 21:53:48.351022 11835 net.cpp:122] Setting up conv4_1_ex
I0523 21:53:48.351040 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.351045 11835 net.cpp:137] Memory required for data: 2886009600
I0523 21:53:48.351052 11835 layer_factory.hpp:77] Creating layer conv4_1_ex/bn
I0523 21:53:48.351066 11835 net.cpp:84] Creating Layer conv4_1_ex/bn
I0523 21:53:48.351071 11835 net.cpp:406] conv4_1_ex/bn <- conv4_1_ex
I0523 21:53:48.351078 11835 net.cpp:367] conv4_1_ex/bn -> conv4_1_ex (in-place)
I0523 21:53:48.351330 11835 net.cpp:122] Setting up conv4_1_ex/bn
I0523 21:53:48.351341 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.351344 11835 net.cpp:137] Memory required for data: 2888762112
I0523 21:53:48.351352 11835 layer_factory.hpp:77] Creating layer conv4_1_ex/scale
I0523 21:53:48.351361 11835 net.cpp:84] Creating Layer conv4_1_ex/scale
I0523 21:53:48.351366 11835 net.cpp:406] conv4_1_ex/scale <- conv4_1_ex
I0523 21:53:48.351372 11835 net.cpp:367] conv4_1_ex/scale -> conv4_1_ex (in-place)
I0523 21:53:48.351428 11835 layer_factory.hpp:77] Creating layer conv4_1_ex/scale
I0523 21:53:48.351570 11835 net.cpp:122] Setting up conv4_1_ex/scale
I0523 21:53:48.351580 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.351584 11835 net.cpp:137] Memory required for data: 2891514624
I0523 21:53:48.351590 11835 layer_factory.hpp:77] Creating layer relu4_1_ex
I0523 21:53:48.351603 11835 net.cpp:84] Creating Layer relu4_1_ex
I0523 21:53:48.351606 11835 net.cpp:406] relu4_1_ex <- conv4_1_ex
I0523 21:53:48.351613 11835 net.cpp:367] relu4_1_ex -> conv4_1_ex (in-place)
I0523 21:53:48.351727 11835 net.cpp:122] Setting up relu4_1_ex
I0523 21:53:48.351737 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.351740 11835 net.cpp:137] Memory required for data: 2894267136
I0523 21:53:48.351747 11835 layer_factory.hpp:77] Creating layer conv4_1_dw
I0523 21:53:48.351758 11835 net.cpp:84] Creating Layer conv4_1_dw
I0523 21:53:48.351764 11835 net.cpp:406] conv4_1_dw <- conv4_1_ex
I0523 21:53:48.351771 11835 net.cpp:380] conv4_1_dw -> conv4_1_dw
I0523 21:53:48.352032 11835 net.cpp:122] Setting up conv4_1_dw
I0523 21:53:48.352041 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.352046 11835 net.cpp:137] Memory required for data: 2897019648
I0523 21:53:48.352051 11835 layer_factory.hpp:77] Creating layer conv4_1_dw/bn
I0523 21:53:48.352061 11835 net.cpp:84] Creating Layer conv4_1_dw/bn
I0523 21:53:48.352066 11835 net.cpp:406] conv4_1_dw/bn <- conv4_1_dw
I0523 21:53:48.352071 11835 net.cpp:367] conv4_1_dw/bn -> conv4_1_dw (in-place)
I0523 21:53:48.352306 11835 net.cpp:122] Setting up conv4_1_dw/bn
I0523 21:53:48.352315 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.352319 11835 net.cpp:137] Memory required for data: 2899772160
I0523 21:53:48.352327 11835 layer_factory.hpp:77] Creating layer conv4_1_dw/scale
I0523 21:53:48.352335 11835 net.cpp:84] Creating Layer conv4_1_dw/scale
I0523 21:53:48.352339 11835 net.cpp:406] conv4_1_dw/scale <- conv4_1_dw
I0523 21:53:48.352347 11835 net.cpp:367] conv4_1_dw/scale -> conv4_1_dw (in-place)
I0523 21:53:48.352394 11835 layer_factory.hpp:77] Creating layer conv4_1_dw/scale
I0523 21:53:48.352537 11835 net.cpp:122] Setting up conv4_1_dw/scale
I0523 21:53:48.352547 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.352551 11835 net.cpp:137] Memory required for data: 2902524672
I0523 21:53:48.352557 11835 layer_factory.hpp:77] Creating layer relu4_1_dw
I0523 21:53:48.352566 11835 net.cpp:84] Creating Layer relu4_1_dw
I0523 21:53:48.352571 11835 net.cpp:406] relu4_1_dw <- conv4_1_dw
I0523 21:53:48.352578 11835 net.cpp:367] relu4_1_dw -> conv4_1_dw (in-place)
I0523 21:53:48.352694 11835 net.cpp:122] Setting up relu4_1_dw
I0523 21:53:48.352704 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.352721 11835 net.cpp:137] Memory required for data: 2905277184
I0523 21:53:48.352728 11835 layer_factory.hpp:77] Creating layer conv4_1_em
I0523 21:53:48.352740 11835 net.cpp:84] Creating Layer conv4_1_em
I0523 21:53:48.352746 11835 net.cpp:406] conv4_1_em <- conv4_1_dw
I0523 21:53:48.352754 11835 net.cpp:380] conv4_1_em -> conv4_1_em
I0523 21:53:48.354571 11835 net.cpp:122] Setting up conv4_1_em
I0523 21:53:48.354590 11835 net.cpp:129] Top shape: 64 128 7 6 (344064)
I0523 21:53:48.354595 11835 net.cpp:137] Memory required for data: 2906653440
I0523 21:53:48.354601 11835 layer_factory.hpp:77] Creating layer conv4_1_em/bn
I0523 21:53:48.354612 11835 net.cpp:84] Creating Layer conv4_1_em/bn
I0523 21:53:48.354617 11835 net.cpp:406] conv4_1_em/bn <- conv4_1_em
I0523 21:53:48.354626 11835 net.cpp:367] conv4_1_em/bn -> conv4_1_em (in-place)
I0523 21:53:48.354912 11835 net.cpp:122] Setting up conv4_1_em/bn
I0523 21:53:48.354924 11835 net.cpp:129] Top shape: 64 128 7 6 (344064)
I0523 21:53:48.354928 11835 net.cpp:137] Memory required for data: 2908029696
I0523 21:53:48.354938 11835 layer_factory.hpp:77] Creating layer conv4_1_em/scale
I0523 21:53:48.354949 11835 net.cpp:84] Creating Layer conv4_1_em/scale
I0523 21:53:48.354954 11835 net.cpp:406] conv4_1_em/scale <- conv4_1_em
I0523 21:53:48.354960 11835 net.cpp:367] conv4_1_em/scale -> conv4_1_em (in-place)
I0523 21:53:48.355013 11835 layer_factory.hpp:77] Creating layer conv4_1_em/scale
I0523 21:53:48.355159 11835 net.cpp:122] Setting up conv4_1_em/scale
I0523 21:53:48.355170 11835 net.cpp:129] Top shape: 64 128 7 6 (344064)
I0523 21:53:48.355173 11835 net.cpp:137] Memory required for data: 2909405952
I0523 21:53:48.355181 11835 layer_factory.hpp:77] Creating layer res4_1
I0523 21:53:48.355192 11835 net.cpp:84] Creating Layer res4_1
I0523 21:53:48.355199 11835 net.cpp:406] res4_1 <- conv4_em_conv4_em/scale_0_split_1
I0523 21:53:48.355204 11835 net.cpp:406] res4_1 <- conv4_1_em
I0523 21:53:48.355211 11835 net.cpp:380] res4_1 -> res4_1
I0523 21:53:48.355242 11835 net.cpp:122] Setting up res4_1
I0523 21:53:48.355250 11835 net.cpp:129] Top shape: 64 128 7 6 (344064)
I0523 21:53:48.355254 11835 net.cpp:137] Memory required for data: 2910782208
I0523 21:53:48.355258 11835 layer_factory.hpp:77] Creating layer res4_1_res4_1_0_split
I0523 21:53:48.355264 11835 net.cpp:84] Creating Layer res4_1_res4_1_0_split
I0523 21:53:48.355269 11835 net.cpp:406] res4_1_res4_1_0_split <- res4_1
I0523 21:53:48.355276 11835 net.cpp:380] res4_1_res4_1_0_split -> res4_1_res4_1_0_split_0
I0523 21:53:48.355284 11835 net.cpp:380] res4_1_res4_1_0_split -> res4_1_res4_1_0_split_1
I0523 21:53:48.355326 11835 net.cpp:122] Setting up res4_1_res4_1_0_split
I0523 21:53:48.355334 11835 net.cpp:129] Top shape: 64 128 7 6 (344064)
I0523 21:53:48.355340 11835 net.cpp:129] Top shape: 64 128 7 6 (344064)
I0523 21:53:48.355343 11835 net.cpp:137] Memory required for data: 2913534720
I0523 21:53:48.355350 11835 layer_factory.hpp:77] Creating layer conv4_2_ex
I0523 21:53:48.355363 11835 net.cpp:84] Creating Layer conv4_2_ex
I0523 21:53:48.355370 11835 net.cpp:406] conv4_2_ex <- res4_1_res4_1_0_split_0
I0523 21:53:48.355377 11835 net.cpp:380] conv4_2_ex -> conv4_2_ex
I0523 21:53:48.356992 11835 net.cpp:122] Setting up conv4_2_ex
I0523 21:53:48.357010 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.357017 11835 net.cpp:137] Memory required for data: 2916287232
I0523 21:53:48.357024 11835 layer_factory.hpp:77] Creating layer conv4_2_ex/bn
I0523 21:53:48.357035 11835 net.cpp:84] Creating Layer conv4_2_ex/bn
I0523 21:53:48.357043 11835 net.cpp:406] conv4_2_ex/bn <- conv4_2_ex
I0523 21:53:48.357054 11835 net.cpp:367] conv4_2_ex/bn -> conv4_2_ex (in-place)
I0523 21:53:48.357321 11835 net.cpp:122] Setting up conv4_2_ex/bn
I0523 21:53:48.357329 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.357333 11835 net.cpp:137] Memory required for data: 2919039744
I0523 21:53:48.357343 11835 layer_factory.hpp:77] Creating layer conv4_2_ex/scale
I0523 21:53:48.357352 11835 net.cpp:84] Creating Layer conv4_2_ex/scale
I0523 21:53:48.357372 11835 net.cpp:406] conv4_2_ex/scale <- conv4_2_ex
I0523 21:53:48.357378 11835 net.cpp:367] conv4_2_ex/scale -> conv4_2_ex (in-place)
I0523 21:53:48.357435 11835 layer_factory.hpp:77] Creating layer conv4_2_ex/scale
I0523 21:53:48.357589 11835 net.cpp:122] Setting up conv4_2_ex/scale
I0523 21:53:48.357599 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.357602 11835 net.cpp:137] Memory required for data: 2921792256
I0523 21:53:48.357609 11835 layer_factory.hpp:77] Creating layer relu4_2_ex
I0523 21:53:48.357617 11835 net.cpp:84] Creating Layer relu4_2_ex
I0523 21:53:48.357622 11835 net.cpp:406] relu4_2_ex <- conv4_2_ex
I0523 21:53:48.357628 11835 net.cpp:367] relu4_2_ex -> conv4_2_ex (in-place)
I0523 21:53:48.357744 11835 net.cpp:122] Setting up relu4_2_ex
I0523 21:53:48.357754 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.357758 11835 net.cpp:137] Memory required for data: 2924544768
I0523 21:53:48.357764 11835 layer_factory.hpp:77] Creating layer conv4_2_dw
I0523 21:53:48.357774 11835 net.cpp:84] Creating Layer conv4_2_dw
I0523 21:53:48.357779 11835 net.cpp:406] conv4_2_dw <- conv4_2_ex
I0523 21:53:48.357786 11835 net.cpp:380] conv4_2_dw -> conv4_2_dw
I0523 21:53:48.358057 11835 net.cpp:122] Setting up conv4_2_dw
I0523 21:53:48.358068 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.358072 11835 net.cpp:137] Memory required for data: 2927297280
I0523 21:53:48.358078 11835 layer_factory.hpp:77] Creating layer conv4_2_dw/bn
I0523 21:53:48.358086 11835 net.cpp:84] Creating Layer conv4_2_dw/bn
I0523 21:53:48.358091 11835 net.cpp:406] conv4_2_dw/bn <- conv4_2_dw
I0523 21:53:48.358098 11835 net.cpp:367] conv4_2_dw/bn -> conv4_2_dw (in-place)
I0523 21:53:48.358340 11835 net.cpp:122] Setting up conv4_2_dw/bn
I0523 21:53:48.358348 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.358351 11835 net.cpp:137] Memory required for data: 2930049792
I0523 21:53:48.358361 11835 layer_factory.hpp:77] Creating layer conv4_2_dw/scale
I0523 21:53:48.358371 11835 net.cpp:84] Creating Layer conv4_2_dw/scale
I0523 21:53:48.358374 11835 net.cpp:406] conv4_2_dw/scale <- conv4_2_dw
I0523 21:53:48.358381 11835 net.cpp:367] conv4_2_dw/scale -> conv4_2_dw (in-place)
I0523 21:53:48.358436 11835 layer_factory.hpp:77] Creating layer conv4_2_dw/scale
I0523 21:53:48.358575 11835 net.cpp:122] Setting up conv4_2_dw/scale
I0523 21:53:48.358584 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.358589 11835 net.cpp:137] Memory required for data: 2932802304
I0523 21:53:48.358606 11835 layer_factory.hpp:77] Creating layer relu4_2_dw
I0523 21:53:48.358613 11835 net.cpp:84] Creating Layer relu4_2_dw
I0523 21:53:48.358618 11835 net.cpp:406] relu4_2_dw <- conv4_2_dw
I0523 21:53:48.358624 11835 net.cpp:367] relu4_2_dw -> conv4_2_dw (in-place)
I0523 21:53:48.358752 11835 net.cpp:122] Setting up relu4_2_dw
I0523 21:53:48.358762 11835 net.cpp:129] Top shape: 64 256 7 6 (688128)
I0523 21:53:48.358765 11835 net.cpp:137] Memory required for data: 2935554816
I0523 21:53:48.358770 11835 layer_factory.hpp:77] Creating layer conv4_2_em
I0523 21:53:48.358783 11835 net.cpp:84] Creating Layer conv4_2_em
I0523 21:53:48.358788 11835 net.cpp:406] conv4_2_em <- conv4_2_dw
I0523 21:53:48.358798 11835 net.cpp:380] conv4_2_em -> conv4_2_em
I0523 21:53:48.360435 11835 net.cpp:122] Setting up conv4_2_em
I0523 21:53:48.360455 11835 net.cpp:129] Top shape: 64 128 7 6 (344064)
I0523 21:53:48.360460 11835 net.cpp:137] Memory required for data: 2936931072
I0523 21:53:48.360466 11835 layer_factory.hpp:77] Creating layer conv4_2_em/bn
I0523 21:53:48.360477 11835 net.cpp:84] Creating Layer conv4_2_em/bn
I0523 21:53:48.360486 11835 net.cpp:406] conv4_2_em/bn <- conv4_2_em
I0523 21:53:48.360493 11835 net.cpp:367] conv4_2_em/bn -> conv4_2_em (in-place)
I0523 21:53:48.360740 11835 net.cpp:122] Setting up conv4_2_em/bn
I0523 21:53:48.360750 11835 net.cpp:129] Top shape: 64 128 7 6 (344064)
I0523 21:53:48.360754 11835 net.cpp:137] Memory required for data: 2938307328
I0523 21:53:48.360762 11835 layer_factory.hpp:77] Creating layer conv4_2_em/scale
I0523 21:53:48.360787 11835 net.cpp:84] Creating Layer conv4_2_em/scale
I0523 21:53:48.360795 11835 net.cpp:406] conv4_2_em/scale <- conv4_2_em
I0523 21:53:48.360801 11835 net.cpp:367] conv4_2_em/scale -> conv4_2_em (in-place)
I0523 21:53:48.360867 11835 layer_factory.hpp:77] Creating layer conv4_2_em/scale
I0523 21:53:48.361009 11835 net.cpp:122] Setting up conv4_2_em/scale
I0523 21:53:48.361019 11835 net.cpp:129] Top shape: 64 128 7 6 (344064)
I0523 21:53:48.361023 11835 net.cpp:137] Memory required for data: 2939683584
I0523 21:53:48.361030 11835 layer_factory.hpp:77] Creating layer res4_2
I0523 21:53:48.361042 11835 net.cpp:84] Creating Layer res4_2
I0523 21:53:48.361048 11835 net.cpp:406] res4_2 <- res4_1_res4_1_0_split_1
I0523 21:53:48.361054 11835 net.cpp:406] res4_2 <- conv4_2_em
I0523 21:53:48.361060 11835 net.cpp:380] res4_2 -> res4_2
I0523 21:53:48.361093 11835 net.cpp:122] Setting up res4_2
I0523 21:53:48.361100 11835 net.cpp:129] Top shape: 64 128 7 6 (344064)
I0523 21:53:48.361104 11835 net.cpp:137] Memory required for data: 2941059840
I0523 21:53:48.361107 11835 layer_factory.hpp:77] Creating layer conv5_ex
I0523 21:53:48.361120 11835 net.cpp:84] Creating Layer conv5_ex
I0523 21:53:48.361126 11835 net.cpp:406] conv5_ex <- res4_2
I0523 21:53:48.361133 11835 net.cpp:380] conv5_ex -> conv5_ex
I0523 21:53:48.363124 11835 net.cpp:122] Setting up conv5_ex
I0523 21:53:48.363145 11835 net.cpp:129] Top shape: 64 512 7 6 (1376256)
I0523 21:53:48.363152 11835 net.cpp:137] Memory required for data: 2946564864
I0523 21:53:48.363160 11835 layer_factory.hpp:77] Creating layer conv5_ex/bn
I0523 21:53:48.363171 11835 net.cpp:84] Creating Layer conv5_ex/bn
I0523 21:53:48.363178 11835 net.cpp:406] conv5_ex/bn <- conv5_ex
I0523 21:53:48.363190 11835 net.cpp:367] conv5_ex/bn -> conv5_ex (in-place)
I0523 21:53:48.363447 11835 net.cpp:122] Setting up conv5_ex/bn
I0523 21:53:48.363457 11835 net.cpp:129] Top shape: 64 512 7 6 (1376256)
I0523 21:53:48.363461 11835 net.cpp:137] Memory required for data: 2952069888
I0523 21:53:48.363471 11835 layer_factory.hpp:77] Creating layer conv5_ex/scale
I0523 21:53:48.363481 11835 net.cpp:84] Creating Layer conv5_ex/scale
I0523 21:53:48.363487 11835 net.cpp:406] conv5_ex/scale <- conv5_ex
I0523 21:53:48.363497 11835 net.cpp:367] conv5_ex/scale -> conv5_ex (in-place)
I0523 21:53:48.363546 11835 layer_factory.hpp:77] Creating layer conv5_ex/scale
I0523 21:53:48.363696 11835 net.cpp:122] Setting up conv5_ex/scale
I0523 21:53:48.363708 11835 net.cpp:129] Top shape: 64 512 7 6 (1376256)
I0523 21:53:48.363713 11835 net.cpp:137] Memory required for data: 2957574912
I0523 21:53:48.363719 11835 layer_factory.hpp:77] Creating layer relu5_ex
I0523 21:53:48.363726 11835 net.cpp:84] Creating Layer relu5_ex
I0523 21:53:48.363730 11835 net.cpp:406] relu5_ex <- conv5_ex
I0523 21:53:48.363736 11835 net.cpp:367] relu5_ex -> conv5_ex (in-place)
I0523 21:53:48.363859 11835 net.cpp:122] Setting up relu5_ex
I0523 21:53:48.363869 11835 net.cpp:129] Top shape: 64 512 7 6 (1376256)
I0523 21:53:48.363873 11835 net.cpp:137] Memory required for data: 2963079936
I0523 21:53:48.363878 11835 layer_factory.hpp:77] Creating layer conv5_dw
I0523 21:53:48.363888 11835 net.cpp:84] Creating Layer conv5_dw
I0523 21:53:48.363894 11835 net.cpp:406] conv5_dw <- conv5_ex
I0523 21:53:48.363901 11835 net.cpp:380] conv5_dw -> conv5_dw
I0523 21:53:48.364298 11835 net.cpp:122] Setting up conv5_dw
I0523 21:53:48.364308 11835 net.cpp:129] Top shape: 64 512 1 1 (32768)
I0523 21:53:48.364312 11835 net.cpp:137] Memory required for data: 2963211008
I0523 21:53:48.364317 11835 layer_factory.hpp:77] Creating layer conv5_dw/bn
I0523 21:53:48.364327 11835 net.cpp:84] Creating Layer conv5_dw/bn
I0523 21:53:48.364331 11835 net.cpp:406] conv5_dw/bn <- conv5_dw
I0523 21:53:48.364337 11835 net.cpp:367] conv5_dw/bn -> conv5_dw (in-place)
I0523 21:53:48.364570 11835 net.cpp:122] Setting up conv5_dw/bn
I0523 21:53:48.364579 11835 net.cpp:129] Top shape: 64 512 1 1 (32768)
I0523 21:53:48.364583 11835 net.cpp:137] Memory required for data: 2963342080
I0523 21:53:48.364604 11835 layer_factory.hpp:77] Creating layer conv5_dw/scale
I0523 21:53:48.364615 11835 net.cpp:84] Creating Layer conv5_dw/scale
I0523 21:53:48.364619 11835 net.cpp:406] conv5_dw/scale <- conv5_dw
I0523 21:53:48.364625 11835 net.cpp:367] conv5_dw/scale -> conv5_dw (in-place)
I0523 21:53:48.364675 11835 layer_factory.hpp:77] Creating layer conv5_dw/scale
I0523 21:53:48.364825 11835 net.cpp:122] Setting up conv5_dw/scale
I0523 21:53:48.364835 11835 net.cpp:129] Top shape: 64 512 1 1 (32768)
I0523 21:53:48.364838 11835 net.cpp:137] Memory required for data: 2963473152
I0523 21:53:48.364847 11835 layer_factory.hpp:77] Creating layer fc5
I0523 21:53:48.364857 11835 net.cpp:84] Creating Layer fc5
I0523 21:53:48.364863 11835 net.cpp:406] fc5 <- conv5_dw
I0523 21:53:48.364871 11835 net.cpp:380] fc5 -> fc5
I0523 21:53:48.365408 11835 net.cpp:122] Setting up fc5
I0523 21:53:48.365418 11835 net.cpp:129] Top shape: 64 128 (8192)
I0523 21:53:48.365422 11835 net.cpp:137] Memory required for data: 2963505920
I0523 21:53:48.365427 11835 layer_factory.hpp:77] Creating layer norm1
I0523 21:53:48.365434 11835 net.cpp:84] Creating Layer norm1
I0523 21:53:48.365439 11835 net.cpp:406] norm1 <- fc5
I0523 21:53:48.365445 11835 net.cpp:380] norm1 -> norm1
I0523 21:53:48.365514 11835 net.cpp:122] Setting up norm1
I0523 21:53:48.365521 11835 net.cpp:129] Top shape: 64 128 1 1 (8192)
I0523 21:53:48.365525 11835 net.cpp:137] Memory required for data: 2963538688
I0523 21:53:48.365530 11835 layer_factory.hpp:77] Creating layer fc-6_l2
I0523 21:53:48.365536 11835 net.cpp:84] Creating Layer fc-6_l2
I0523 21:53:48.365545 11835 net.cpp:406] fc-6_l2 <- norm1
I0523 21:53:48.365550 11835 net.cpp:380] fc-6_l2 -> fc-6_l2
I0523 21:53:48.428081 11835 net.cpp:122] Setting up fc-6_l2
I0523 21:53:48.428115 11835 net.cpp:129] Top shape: 64 54547 (3491008)
I0523 21:53:48.428120 11835 net.cpp:137] Memory required for data: 2977502720
I0523 21:53:48.428130 11835 layer_factory.hpp:77] Creating layer fc-6_margin
I0523 21:53:48.428165 11835 net.cpp:84] Creating Layer fc-6_margin
I0523 21:53:48.428175 11835 net.cpp:406] fc-6_margin <- fc-6_l2
I0523 21:53:48.428185 11835 net.cpp:406] fc-6_margin <- label_data_1_split_0
I0523 21:53:48.428194 11835 net.cpp:380] fc-6_margin -> fc-6_margin
I0523 21:53:48.428243 11835 net.cpp:122] Setting up fc-6_margin
I0523 21:53:48.428253 11835 net.cpp:129] Top shape: 64 54547 (3491008)
I0523 21:53:48.428257 11835 net.cpp:137] Memory required for data: 2991466752
I0523 21:53:48.428261 11835 layer_factory.hpp:77] Creating layer fc-6_margin_scale
I0523 21:53:48.428275 11835 net.cpp:84] Creating Layer fc-6_margin_scale
I0523 21:53:48.428282 11835 net.cpp:406] fc-6_margin_scale <- fc-6_margin
I0523 21:53:48.428289 11835 net.cpp:380] fc-6_margin_scale -> fc-6_margin_scale
I0523 21:53:48.433593 11835 net.cpp:122] Setting up fc-6_margin_scale
I0523 21:53:48.433620 11835 net.cpp:129] Top shape: 64 54547 (3491008)
I0523 21:53:48.433626 11835 net.cpp:137] Memory required for data: 3005430784
I0523 21:53:48.433636 11835 layer_factory.hpp:77] Creating layer softmax_loss
I0523 21:53:48.433650 11835 net.cpp:84] Creating Layer softmax_loss
I0523 21:53:48.433657 11835 net.cpp:406] softmax_loss <- fc-6_margin_scale
I0523 21:53:48.433670 11835 net.cpp:406] softmax_loss <- label_data_1_split_1
I0523 21:53:48.433689 11835 net.cpp:380] softmax_loss -> softmax_loss
I0523 21:53:48.433708 11835 layer_factory.hpp:77] Creating layer softmax_loss
I0523 21:53:48.450506 11835 net.cpp:122] Setting up softmax_loss
I0523 21:53:48.450541 11835 net.cpp:129] Top shape: (1)
I0523 21:53:48.450548 11835 net.cpp:132]     with loss weight 1
I0523 21:53:48.450578 11835 net.cpp:137] Memory required for data: 3005430788
I0523 21:53:48.450585 11835 net.cpp:198] softmax_loss needs backward computation.
I0523 21:53:48.450594 11835 net.cpp:198] fc-6_margin_scale needs backward computation.
I0523 21:53:48.450604 11835 net.cpp:198] fc-6_margin needs backward computation.
I0523 21:53:48.450614 11835 net.cpp:198] fc-6_l2 needs backward computation.
I0523 21:53:48.450649 11835 net.cpp:198] norm1 needs backward computation.
I0523 21:53:48.450659 11835 net.cpp:198] fc5 needs backward computation.
I0523 21:53:48.450666 11835 net.cpp:198] conv5_dw/scale needs backward computation.
I0523 21:53:48.450675 11835 net.cpp:198] conv5_dw/bn needs backward computation.
I0523 21:53:48.450681 11835 net.cpp:198] conv5_dw needs backward computation.
I0523 21:53:48.450690 11835 net.cpp:198] relu5_ex needs backward computation.
I0523 21:53:48.450714 11835 net.cpp:198] conv5_ex/scale needs backward computation.
I0523 21:53:48.450721 11835 net.cpp:198] conv5_ex/bn needs backward computation.
I0523 21:53:48.450726 11835 net.cpp:198] conv5_ex needs backward computation.
I0523 21:53:48.450733 11835 net.cpp:198] res4_2 needs backward computation.
I0523 21:53:48.450742 11835 net.cpp:198] conv4_2_em/scale needs backward computation.
I0523 21:53:48.450750 11835 net.cpp:198] conv4_2_em/bn needs backward computation.
I0523 21:53:48.450758 11835 net.cpp:198] conv4_2_em needs backward computation.
I0523 21:53:48.450767 11835 net.cpp:198] relu4_2_dw needs backward computation.
I0523 21:53:48.450773 11835 net.cpp:198] conv4_2_dw/scale needs backward computation.
I0523 21:53:48.450781 11835 net.cpp:198] conv4_2_dw/bn needs backward computation.
I0523 21:53:48.450788 11835 net.cpp:198] conv4_2_dw needs backward computation.
I0523 21:53:48.450795 11835 net.cpp:198] relu4_2_ex needs backward computation.
I0523 21:53:48.450803 11835 net.cpp:198] conv4_2_ex/scale needs backward computation.
I0523 21:53:48.450810 11835 net.cpp:198] conv4_2_ex/bn needs backward computation.
I0523 21:53:48.450819 11835 net.cpp:198] conv4_2_ex needs backward computation.
I0523 21:53:48.450827 11835 net.cpp:198] res4_1_res4_1_0_split needs backward computation.
I0523 21:53:48.450835 11835 net.cpp:198] res4_1 needs backward computation.
I0523 21:53:48.450845 11835 net.cpp:198] conv4_1_em/scale needs backward computation.
I0523 21:53:48.450852 11835 net.cpp:198] conv4_1_em/bn needs backward computation.
I0523 21:53:48.450860 11835 net.cpp:198] conv4_1_em needs backward computation.
I0523 21:53:48.450867 11835 net.cpp:198] relu4_1_dw needs backward computation.
I0523 21:53:48.450875 11835 net.cpp:198] conv4_1_dw/scale needs backward computation.
I0523 21:53:48.450882 11835 net.cpp:198] conv4_1_dw/bn needs backward computation.
I0523 21:53:48.450888 11835 net.cpp:198] conv4_1_dw needs backward computation.
I0523 21:53:48.450896 11835 net.cpp:198] relu4_1_ex needs backward computation.
I0523 21:53:48.450903 11835 net.cpp:198] conv4_1_ex/scale needs backward computation.
I0523 21:53:48.450922 11835 net.cpp:198] conv4_1_ex/bn needs backward computation.
I0523 21:53:48.450930 11835 net.cpp:198] conv4_1_ex needs backward computation.
I0523 21:53:48.450938 11835 net.cpp:198] conv4_em_conv4_em/scale_0_split needs backward computation.
I0523 21:53:48.450947 11835 net.cpp:198] conv4_em/scale needs backward computation.
I0523 21:53:48.450954 11835 net.cpp:198] conv4_em/bn needs backward computation.
I0523 21:53:48.450968 11835 net.cpp:198] conv4_em needs backward computation.
I0523 21:53:48.450975 11835 net.cpp:198] relu4_dw needs backward computation.
I0523 21:53:48.450984 11835 net.cpp:198] conv4_dw/scale needs backward computation.
I0523 21:53:48.450990 11835 net.cpp:198] conv4_dw/bn needs backward computation.
I0523 21:53:48.450999 11835 net.cpp:198] conv4_dw needs backward computation.
I0523 21:53:48.451007 11835 net.cpp:198] relu4_ex needs backward computation.
I0523 21:53:48.451015 11835 net.cpp:198] conv4_ex/scale needs backward computation.
I0523 21:53:48.451022 11835 net.cpp:198] conv4_ex/bn needs backward computation.
I0523 21:53:48.451030 11835 net.cpp:198] conv4_ex needs backward computation.
I0523 21:53:48.451036 11835 net.cpp:198] res3_6 needs backward computation.
I0523 21:53:48.451045 11835 net.cpp:198] conv3_6_em/scale needs backward computation.
I0523 21:53:48.451052 11835 net.cpp:198] conv3_6_em/bn needs backward computation.
I0523 21:53:48.451059 11835 net.cpp:198] conv3_6_em needs backward computation.
I0523 21:53:48.451092 11835 net.cpp:198] relu3_6_dw needs backward computation.
I0523 21:53:48.451098 11835 net.cpp:198] conv3_6_dw/scale needs backward computation.
I0523 21:53:48.451104 11835 net.cpp:198] conv3_6_dw/bn needs backward computation.
I0523 21:53:48.451110 11835 net.cpp:198] conv3_6_dw needs backward computation.
I0523 21:53:48.451117 11835 net.cpp:198] relu3_6_ex needs backward computation.
I0523 21:53:48.451125 11835 net.cpp:198] conv3_6_ex/scale needs backward computation.
I0523 21:53:48.451131 11835 net.cpp:198] conv3_6_ex/bn needs backward computation.
I0523 21:53:48.451138 11835 net.cpp:198] conv3_6_ex needs backward computation.
I0523 21:53:48.451144 11835 net.cpp:198] res3_5_res3_5_0_split needs backward computation.
I0523 21:53:48.451151 11835 net.cpp:198] res3_5 needs backward computation.
I0523 21:53:48.451160 11835 net.cpp:198] conv3_5_em/scale needs backward computation.
I0523 21:53:48.451166 11835 net.cpp:198] conv3_5_em/bn needs backward computation.
I0523 21:53:48.451171 11835 net.cpp:198] conv3_5_em needs backward computation.
I0523 21:53:48.451179 11835 net.cpp:198] relu3_5_dw needs backward computation.
I0523 21:53:48.451184 11835 net.cpp:198] conv3_5_dw/scale needs backward computation.
I0523 21:53:48.451190 11835 net.cpp:198] conv3_5_dw/bn needs backward computation.
I0523 21:53:48.451196 11835 net.cpp:198] conv3_5_dw needs backward computation.
I0523 21:53:48.451202 11835 net.cpp:198] relu3_5_ex needs backward computation.
I0523 21:53:48.451208 11835 net.cpp:198] conv3_5_ex/scale needs backward computation.
I0523 21:53:48.451215 11835 net.cpp:198] conv3_5_ex/bn needs backward computation.
I0523 21:53:48.451220 11835 net.cpp:198] conv3_5_ex needs backward computation.
I0523 21:53:48.451227 11835 net.cpp:198] res3_4_res3_4_0_split needs backward computation.
I0523 21:53:48.451236 11835 net.cpp:198] res3_4 needs backward computation.
I0523 21:53:48.451243 11835 net.cpp:198] conv3_4_em/scale needs backward computation.
I0523 21:53:48.451249 11835 net.cpp:198] conv3_4_em/bn needs backward computation.
I0523 21:53:48.451256 11835 net.cpp:198] conv3_4_em needs backward computation.
I0523 21:53:48.451262 11835 net.cpp:198] relu3_4_dw needs backward computation.
I0523 21:53:48.451269 11835 net.cpp:198] conv3_4_dw/scale needs backward computation.
I0523 21:53:48.451277 11835 net.cpp:198] conv3_4_dw/bn needs backward computation.
I0523 21:53:48.451282 11835 net.cpp:198] conv3_4_dw needs backward computation.
I0523 21:53:48.451292 11835 net.cpp:198] relu3_4_ex needs backward computation.
I0523 21:53:48.451299 11835 net.cpp:198] conv3_4_ex/scale needs backward computation.
I0523 21:53:48.451305 11835 net.cpp:198] conv3_4_ex/bn needs backward computation.
I0523 21:53:48.451310 11835 net.cpp:198] conv3_4_ex needs backward computation.
I0523 21:53:48.451318 11835 net.cpp:198] res3_3_res3_3_0_split needs backward computation.
I0523 21:53:48.451324 11835 net.cpp:198] res3_3 needs backward computation.
I0523 21:53:48.451331 11835 net.cpp:198] conv3_3_em/scale needs backward computation.
I0523 21:53:48.451339 11835 net.cpp:198] conv3_3_em/bn needs backward computation.
I0523 21:53:48.451344 11835 net.cpp:198] conv3_3_em needs backward computation.
I0523 21:53:48.451351 11835 net.cpp:198] relu3_3_dw needs backward computation.
I0523 21:53:48.451356 11835 net.cpp:198] conv3_3_dw/scale needs backward computation.
I0523 21:53:48.451362 11835 net.cpp:198] conv3_3_dw/bn needs backward computation.
I0523 21:53:48.451369 11835 net.cpp:198] conv3_3_dw needs backward computation.
I0523 21:53:48.451375 11835 net.cpp:198] relu3_3_ex needs backward computation.
I0523 21:53:48.451380 11835 net.cpp:198] conv3_3_ex/scale needs backward computation.
I0523 21:53:48.451386 11835 net.cpp:198] conv3_3_ex/bn needs backward computation.
I0523 21:53:48.451391 11835 net.cpp:198] conv3_3_ex needs backward computation.
I0523 21:53:48.451398 11835 net.cpp:198] res3_2_res3_2_0_split needs backward computation.
I0523 21:53:48.451405 11835 net.cpp:198] res3_2 needs backward computation.
I0523 21:53:48.451413 11835 net.cpp:198] conv3_2_em/scale needs backward computation.
I0523 21:53:48.451431 11835 net.cpp:198] conv3_2_em/bn needs backward computation.
I0523 21:53:48.451436 11835 net.cpp:198] conv3_2_em needs backward computation.
I0523 21:53:48.451442 11835 net.cpp:198] relu3_2_dw needs backward computation.
I0523 21:53:48.451448 11835 net.cpp:198] conv3_2_dw/scale needs backward computation.
I0523 21:53:48.451453 11835 net.cpp:198] conv3_2_dw/bn needs backward computation.
I0523 21:53:48.451459 11835 net.cpp:198] conv3_2_dw needs backward computation.
I0523 21:53:48.451465 11835 net.cpp:198] relu3_2_ex needs backward computation.
I0523 21:53:48.451472 11835 net.cpp:198] conv3_2_ex/scale needs backward computation.
I0523 21:53:48.451478 11835 net.cpp:198] conv3_2_ex/bn needs backward computation.
I0523 21:53:48.451483 11835 net.cpp:198] conv3_2_ex needs backward computation.
I0523 21:53:48.451489 11835 net.cpp:198] res3_1_res3_1_0_split needs backward computation.
I0523 21:53:48.451496 11835 net.cpp:198] res3_1 needs backward computation.
I0523 21:53:48.451503 11835 net.cpp:198] conv3_1_em/scale needs backward computation.
I0523 21:53:48.451511 11835 net.cpp:198] conv3_1_em/bn needs backward computation.
I0523 21:53:48.451517 11835 net.cpp:198] conv3_1_em needs backward computation.
I0523 21:53:48.451524 11835 net.cpp:198] relu3_1_dw needs backward computation.
I0523 21:53:48.451529 11835 net.cpp:198] conv3_1_dw/scale needs backward computation.
I0523 21:53:48.451535 11835 net.cpp:198] conv3_1_dw/bn needs backward computation.
I0523 21:53:48.451540 11835 net.cpp:198] conv3_1_dw needs backward computation.
I0523 21:53:48.451547 11835 net.cpp:198] relu3_1_ex needs backward computation.
I0523 21:53:48.451553 11835 net.cpp:198] conv3_1_ex/scale needs backward computation.
I0523 21:53:48.451560 11835 net.cpp:198] conv3_1_ex/bn needs backward computation.
I0523 21:53:48.451565 11835 net.cpp:198] conv3_1_ex needs backward computation.
I0523 21:53:48.451571 11835 net.cpp:198] conv3_em_conv3_em/scale_0_split needs backward computation.
I0523 21:53:48.451581 11835 net.cpp:198] conv3_em/scale needs backward computation.
I0523 21:53:48.451587 11835 net.cpp:198] conv3_em/bn needs backward computation.
I0523 21:53:48.451593 11835 net.cpp:198] conv3_em needs backward computation.
I0523 21:53:48.451601 11835 net.cpp:198] relu3_dw needs backward computation.
I0523 21:53:48.451608 11835 net.cpp:198] conv3_dw/scale needs backward computation.
I0523 21:53:48.451614 11835 net.cpp:198] conv3_dw/bn needs backward computation.
I0523 21:53:48.451619 11835 net.cpp:198] conv3_dw needs backward computation.
I0523 21:53:48.451627 11835 net.cpp:198] relu3_ex needs backward computation.
I0523 21:53:48.451632 11835 net.cpp:198] conv3_ex/scale needs backward computation.
I0523 21:53:48.451638 11835 net.cpp:198] conv3_ex/bn needs backward computation.
I0523 21:53:48.451644 11835 net.cpp:198] conv3_ex needs backward computation.
I0523 21:53:48.451650 11835 net.cpp:198] res2_4 needs backward computation.
I0523 21:53:48.451658 11835 net.cpp:198] conv2_4_em/scale needs backward computation.
I0523 21:53:48.451666 11835 net.cpp:198] conv2_4_em/bn needs backward computation.
I0523 21:53:48.451673 11835 net.cpp:198] conv2_4_em needs backward computation.
I0523 21:53:48.451678 11835 net.cpp:198] relu2_4_dw needs backward computation.
I0523 21:53:48.451684 11835 net.cpp:198] conv2_4_dw/scale needs backward computation.
I0523 21:53:48.451690 11835 net.cpp:198] conv2_4_dw/bn needs backward computation.
I0523 21:53:48.451695 11835 net.cpp:198] conv2_4_dw needs backward computation.
I0523 21:53:48.451702 11835 net.cpp:198] relu2_4_ex needs backward computation.
I0523 21:53:48.451709 11835 net.cpp:198] conv2_4_ex/scale needs backward computation.
I0523 21:53:48.451716 11835 net.cpp:198] conv2_4_ex/bn needs backward computation.
I0523 21:53:48.451722 11835 net.cpp:198] conv2_4_ex needs backward computation.
I0523 21:53:48.451730 11835 net.cpp:198] res2_3_res2_3_0_split needs backward computation.
I0523 21:53:48.451735 11835 net.cpp:198] res2_3 needs backward computation.
I0523 21:53:48.451743 11835 net.cpp:198] conv2_3_em/scale needs backward computation.
I0523 21:53:48.451761 11835 net.cpp:198] conv2_3_em/bn needs backward computation.
I0523 21:53:48.451768 11835 net.cpp:198] conv2_3_em needs backward computation.
I0523 21:53:48.451774 11835 net.cpp:198] relu2_3_dw needs backward computation.
I0523 21:53:48.451781 11835 net.cpp:198] conv2_3_dw/scale needs backward computation.
I0523 21:53:48.451786 11835 net.cpp:198] conv2_3_dw/bn needs backward computation.
I0523 21:53:48.451792 11835 net.cpp:198] conv2_3_dw needs backward computation.
I0523 21:53:48.451799 11835 net.cpp:198] relu2_3_ex needs backward computation.
I0523 21:53:48.451807 11835 net.cpp:198] conv2_3_ex/scale needs backward computation.
I0523 21:53:48.451812 11835 net.cpp:198] conv2_3_ex/bn needs backward computation.
I0523 21:53:48.451819 11835 net.cpp:198] conv2_3_ex needs backward computation.
I0523 21:53:48.451825 11835 net.cpp:198] res2_2_res2_2_0_split needs backward computation.
I0523 21:53:48.451838 11835 net.cpp:198] res2_2 needs backward computation.
I0523 21:53:48.451844 11835 net.cpp:198] conv2_2_em/scale needs backward computation.
I0523 21:53:48.451853 11835 net.cpp:198] conv2_2_em/bn needs backward computation.
I0523 21:53:48.451858 11835 net.cpp:198] conv2_2_em needs backward computation.
I0523 21:53:48.451864 11835 net.cpp:198] relu2_2_dw needs backward computation.
I0523 21:53:48.451870 11835 net.cpp:198] conv2_2_dw/scale needs backward computation.
I0523 21:53:48.451875 11835 net.cpp:198] conv2_2_dw/bn needs backward computation.
I0523 21:53:48.451881 11835 net.cpp:198] conv2_2_dw needs backward computation.
I0523 21:53:48.451889 11835 net.cpp:198] relu2_2_ex needs backward computation.
I0523 21:53:48.451894 11835 net.cpp:198] conv2_2_ex/scale needs backward computation.
I0523 21:53:48.451900 11835 net.cpp:198] conv2_2_ex/bn needs backward computation.
I0523 21:53:48.451907 11835 net.cpp:198] conv2_2_ex needs backward computation.
I0523 21:53:48.451913 11835 net.cpp:198] res2_1_res2_1_0_split needs backward computation.
I0523 21:53:48.451922 11835 net.cpp:198] res2_1 needs backward computation.
I0523 21:53:48.451930 11835 net.cpp:198] conv2_1_em/scale needs backward computation.
I0523 21:53:48.451936 11835 net.cpp:198] conv2_1_em/bn needs backward computation.
I0523 21:53:48.451942 11835 net.cpp:198] conv2_1_em needs backward computation.
I0523 21:53:48.451948 11835 net.cpp:198] relu2_1_dw needs backward computation.
I0523 21:53:48.451957 11835 net.cpp:198] conv2_1_dw/scale needs backward computation.
I0523 21:53:48.451963 11835 net.cpp:198] conv2_1_dw/bn needs backward computation.
I0523 21:53:48.451969 11835 net.cpp:198] conv2_1_dw needs backward computation.
I0523 21:53:48.451975 11835 net.cpp:198] relu2_1_ex needs backward computation.
I0523 21:53:48.451982 11835 net.cpp:198] conv2_1_ex/scale needs backward computation.
I0523 21:53:48.451987 11835 net.cpp:198] conv2_1_ex/bn needs backward computation.
I0523 21:53:48.451993 11835 net.cpp:198] conv2_1_ex needs backward computation.
I0523 21:53:48.452000 11835 net.cpp:198] conv2_em_conv2_em/scale_0_split needs backward computation.
I0523 21:53:48.452009 11835 net.cpp:198] conv2_em/scale needs backward computation.
I0523 21:53:48.452016 11835 net.cpp:198] conv2_em/bn needs backward computation.
I0523 21:53:48.452023 11835 net.cpp:198] conv2_em needs backward computation.
I0523 21:53:48.452028 11835 net.cpp:198] relu2_dw needs backward computation.
I0523 21:53:48.452035 11835 net.cpp:198] conv2_dw/scale needs backward computation.
I0523 21:53:48.452040 11835 net.cpp:198] conv2_dw/bn needs backward computation.
I0523 21:53:48.452047 11835 net.cpp:198] conv2_dw needs backward computation.
I0523 21:53:48.452054 11835 net.cpp:198] relu2_ex needs backward computation.
I0523 21:53:48.452059 11835 net.cpp:198] conv2_ex/scale needs backward computation.
I0523 21:53:48.452065 11835 net.cpp:198] conv2_ex/bn needs backward computation.
I0523 21:53:48.452071 11835 net.cpp:198] conv2_ex needs backward computation.
I0523 21:53:48.452077 11835 net.cpp:198] relu1_dw needs backward computation.
I0523 21:53:48.452095 11835 net.cpp:198] conv1_dw/scale needs backward computation.
I0523 21:53:48.452100 11835 net.cpp:198] conv1_dw/bn needs backward computation.
I0523 21:53:48.452106 11835 net.cpp:198] conv1_dw needs backward computation.
I0523 21:53:48.452113 11835 net.cpp:198] relu1 needs backward computation.
I0523 21:53:48.452119 11835 net.cpp:198] conv1/scale needs backward computation.
I0523 21:53:48.452126 11835 net.cpp:198] conv1/bn needs backward computation.
I0523 21:53:48.452131 11835 net.cpp:198] conv1 needs backward computation.
I0523 21:53:48.452141 11835 net.cpp:200] label_data_1_split does not need backward computation.
I0523 21:53:48.452150 11835 net.cpp:200] data does not need backward computation.
I0523 21:53:48.452155 11835 net.cpp:242] This network produces output softmax_loss
I0523 21:53:48.452371 11835 net.cpp:255] Network initialization done.
I0523 21:53:48.453259 11835 solver.cpp:72] Finetuning from /home/zkx/Project/O2N/DepthwiseConvolution-master/MobileFaceNet-master/face_snapshot/MobileFaceNet.caffemodel
I0523 21:53:48.461589 11835 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: /home/zkx/Project/O2N/DepthwiseConvolution-master/MobileFaceNet-master/face_snapshot/MobileFaceNet.caffemodel
I0523 21:53:48.461628 11835 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0523 21:53:48.461642 11835 net.cpp:744] Ignoring source layer input
I0523 21:53:48.464341 11835 solver.cpp:57] Solver scaffolding done.
I0523 21:53:48.485483 11835 caffe.cpp:239] Starting Optimization
I0523 21:53:50.397825 11892 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: models/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/train.prototxt
I0523 21:53:50.397871 11892 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0523 21:53:50.400288 11892 image_data_layer.cpp:38] Opening file /home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/image_list/train_list/Combine_base_Cmul-Asia-beid-cap10-ZheD_list.txt_label.txt
I0523 21:53:50.511448 11890 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: models/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/train.prototxt
I0523 21:53:50.511487 11890 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0523 21:53:50.513082 11890 image_data_layer.cpp:38] Opening file /home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/image_list/train_list/Combine_base_Cmul-Asia-beid-cap10-ZheD_list.txt_label.txt
I0523 21:53:50.626294 11891 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: models/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/train.prototxt
I0523 21:53:50.626332 11891 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0523 21:53:50.632575 11891 image_data_layer.cpp:38] Opening file /home/zkx/Data_sdb/TrainData/Data_sdc/Patches/MultiPatches/image_list/train_list/Combine_base_Cmul-Asia-beid-cap10-ZheD_list.txt_label.txt
I0523 21:53:51.686748 11892 image_data_layer.cpp:53] Shuffling data
I0523 21:53:51.815367 11890 image_data_layer.cpp:53] Shuffling data
I0523 21:53:51.880730 11891 image_data_layer.cpp:53] Shuffling data
I0523 21:53:52.309234 11892 image_data_layer.cpp:63] A total of 2075438 images.
I0523 21:53:52.315037 11892 image_data_layer.cpp:90] output data size: 64,3,112,96
I0523 21:53:52.448355 11890 image_data_layer.cpp:63] A total of 2075438 images.
I0523 21:53:52.449151 11891 image_data_layer.cpp:63] A total of 2075438 images.
I0523 21:53:52.522322 11890 image_data_layer.cpp:90] output data size: 64,3,112,96
I0523 21:53:52.522393 11891 image_data_layer.cpp:90] output data size: 64,3,112,96
I0523 21:53:54.963318 11892 solver.cpp:72] Finetuning from /home/zkx/Project/O2N/DepthwiseConvolution-master/MobileFaceNet-master/face_snapshot/MobileFaceNet.caffemodel
I0523 21:53:54.967871 11892 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: /home/zkx/Project/O2N/DepthwiseConvolution-master/MobileFaceNet-master/face_snapshot/MobileFaceNet.caffemodel
I0523 21:53:54.967926 11892 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0523 21:53:54.967932 11892 net.cpp:744] Ignoring source layer input
I0523 21:53:54.986191 11891 solver.cpp:72] Finetuning from /home/zkx/Project/O2N/DepthwiseConvolution-master/MobileFaceNet-master/face_snapshot/MobileFaceNet.caffemodel
I0523 21:53:54.996261 11891 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: /home/zkx/Project/O2N/DepthwiseConvolution-master/MobileFaceNet-master/face_snapshot/MobileFaceNet.caffemodel
I0523 21:53:54.996299 11891 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0523 21:53:54.996307 11891 net.cpp:744] Ignoring source layer input
I0523 21:53:54.997841 11890 solver.cpp:72] Finetuning from /home/zkx/Project/O2N/DepthwiseConvolution-master/MobileFaceNet-master/face_snapshot/MobileFaceNet.caffemodel
I0523 21:53:55.001787 11890 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: /home/zkx/Project/O2N/DepthwiseConvolution-master/MobileFaceNet-master/face_snapshot/MobileFaceNet.caffemodel
I0523 21:53:55.001807 11890 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0523 21:53:55.001812 11890 net.cpp:744] Ignoring source layer input
I0523 21:53:55.747880 11835 solver.cpp:293] Solving 2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_train
I0523 21:53:55.747905 11835 solver.cpp:294] Learning Rate Policy: multistep
I0523 21:53:57.659512 11835 solver.cpp:239] Iteration 0 (-nan iter/s, 1.90686s/10 iters), loss = 14.0323
I0523 21:53:57.659564 11835 solver.cpp:258]     Train net output #0: softmax_loss = 14.0323 (* 1 = 14.0323 loss)
I0523 21:53:57.659833 11835 sgd_solver.cpp:112] Iteration 0, lr = 0.1
I0523 21:54:05.184964 11835 solver.cpp:239] Iteration 10 (1.32888 iter/s, 7.52515s/10 iters), loss = 14.1303
I0523 21:54:05.185014 11835 solver.cpp:258]     Train net output #0: softmax_loss = 14.1303 (* 1 = 14.1303 loss)
I0523 21:54:05.185262 11835 sgd_solver.cpp:112] Iteration 10, lr = 0.1
I0523 21:54:11.083012 11835 solver.cpp:239] Iteration 20 (1.69555 iter/s, 5.89779s/10 iters), loss = 14.0805
I0523 21:54:11.083065 11835 solver.cpp:258]     Train net output #0: softmax_loss = 14.0805 (* 1 = 14.0805 loss)
I0523 21:54:11.083148 11835 sgd_solver.cpp:112] Iteration 20, lr = 0.1
I0523 21:54:17.766723 11835 solver.cpp:239] Iteration 30 (1.49624 iter/s, 6.68343s/10 iters), loss = 13.4746
I0523 21:54:17.766844 11835 solver.cpp:258]     Train net output #0: softmax_loss = 13.4746 (* 1 = 13.4746 loss)
I0523 21:54:17.766875 11835 sgd_solver.cpp:112] Iteration 30, lr = 0.1
I0523 21:54:24.192859 11835 solver.cpp:239] Iteration 40 (1.55623 iter/s, 6.42578s/10 iters), loss = 13.3615
I0523 21:54:24.192911 11835 solver.cpp:258]     Train net output #0: softmax_loss = 13.3615 (* 1 = 13.3615 loss)
I0523 21:54:24.193091 11835 sgd_solver.cpp:112] Iteration 40, lr = 0.1
I0523 21:54:29.936434 11835 solver.cpp:239] Iteration 50 (1.74115 iter/s, 5.74332s/10 iters), loss = 13.0905
I0523 21:54:29.936473 11835 solver.cpp:258]     Train net output #0: softmax_loss = 13.0905 (* 1 = 13.0905 loss)
I0523 21:54:29.936484 11835 sgd_solver.cpp:112] Iteration 50, lr = 0.1
I0523 21:54:36.294644 11835 solver.cpp:239] Iteration 60 (1.57285 iter/s, 6.35787s/10 iters), loss = 12.7156
I0523 21:54:36.294689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 12.7156 (* 1 = 12.7156 loss)
I0523 21:54:36.294823 11835 sgd_solver.cpp:112] Iteration 60, lr = 0.1
I0523 21:54:44.138247 11835 solver.cpp:239] Iteration 70 (1.27498 iter/s, 7.84327s/10 iters), loss = 12.4154
I0523 21:54:44.138290 11835 solver.cpp:258]     Train net output #0: softmax_loss = 12.4154 (* 1 = 12.4154 loss)
I0523 21:54:44.558857 11835 sgd_solver.cpp:112] Iteration 70, lr = 0.1
I0523 21:54:54.529822 11835 solver.cpp:239] Iteration 80 (0.962357 iter/s, 10.3912s/10 iters), loss = 12.207
I0523 21:54:54.530022 11835 solver.cpp:258]     Train net output #0: softmax_loss = 12.207 (* 1 = 12.207 loss)
I0523 21:54:54.530038 11835 sgd_solver.cpp:112] Iteration 80, lr = 0.1
I0523 21:55:00.179337 11835 solver.cpp:239] Iteration 90 (1.77053 iter/s, 5.64804s/10 iters), loss = 12.5596
I0523 21:55:00.179386 11835 solver.cpp:258]     Train net output #0: softmax_loss = 12.5596 (* 1 = 12.5596 loss)
I0523 21:55:00.179399 11835 sgd_solver.cpp:112] Iteration 90, lr = 0.1
I0523 21:55:07.262462 11835 solver.cpp:239] Iteration 100 (1.41188 iter/s, 7.08275s/10 iters), loss = 12.1782
I0523 21:55:07.262500 11835 solver.cpp:258]     Train net output #0: softmax_loss = 12.1782 (* 1 = 12.1782 loss)
I0523 21:55:07.452504 11835 sgd_solver.cpp:112] Iteration 100, lr = 0.1
I0523 21:55:15.476157 11835 solver.cpp:239] Iteration 110 (1.21753 iter/s, 8.21334s/10 iters), loss = 11.6191
I0523 21:55:15.476205 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.6191 (* 1 = 11.6191 loss)
I0523 21:55:15.476229 11835 sgd_solver.cpp:112] Iteration 110, lr = 0.1
I0523 21:55:21.121150 11835 solver.cpp:239] Iteration 120 (1.77156 iter/s, 5.64473s/10 iters), loss = 12.0794
I0523 21:55:21.121189 11835 solver.cpp:258]     Train net output #0: softmax_loss = 12.0794 (* 1 = 12.0794 loss)
I0523 21:55:21.121495 11835 sgd_solver.cpp:112] Iteration 120, lr = 0.1
I0523 21:55:26.940045 11835 solver.cpp:239] Iteration 130 (1.71862 iter/s, 5.81863s/10 iters), loss = 12.441
I0523 21:55:26.940282 11835 solver.cpp:258]     Train net output #0: softmax_loss = 12.441 (* 1 = 12.441 loss)
I0523 21:55:27.293066 11835 sgd_solver.cpp:112] Iteration 130, lr = 0.1
I0523 21:55:34.058594 11835 solver.cpp:239] Iteration 140 (1.40488 iter/s, 7.11807s/10 iters), loss = 11.7091
I0523 21:55:34.058647 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.7091 (* 1 = 11.7091 loss)
I0523 21:55:34.058665 11835 sgd_solver.cpp:112] Iteration 140, lr = 0.1
I0523 21:55:40.017637 11835 solver.cpp:239] Iteration 150 (1.67822 iter/s, 5.95871s/10 iters), loss = 12.0126
I0523 21:55:40.017683 11835 solver.cpp:258]     Train net output #0: softmax_loss = 12.0126 (* 1 = 12.0126 loss)
I0523 21:55:40.017696 11835 sgd_solver.cpp:112] Iteration 150, lr = 0.1
I0523 21:55:45.751914 11835 solver.cpp:239] Iteration 160 (1.74398 iter/s, 5.73401s/10 iters), loss = 12.4524
I0523 21:55:45.751956 11835 solver.cpp:258]     Train net output #0: softmax_loss = 12.4524 (* 1 = 12.4524 loss)
I0523 21:55:45.756521 11835 sgd_solver.cpp:112] Iteration 160, lr = 0.1
I0523 21:55:52.782099 11835 solver.cpp:239] Iteration 170 (1.4225 iter/s, 7.02986s/10 iters), loss = 11.4646
I0523 21:55:52.782155 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4646 (* 1 = 11.4646 loss)
I0523 21:55:52.782428 11835 sgd_solver.cpp:112] Iteration 170, lr = 0.1
I0523 21:55:59.571849 11835 solver.cpp:239] Iteration 180 (1.47288 iter/s, 6.78943s/10 iters), loss = 11.7902
I0523 21:55:59.572048 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.7902 (* 1 = 11.7902 loss)
I0523 21:55:59.572067 11835 sgd_solver.cpp:112] Iteration 180, lr = 0.1
I0523 21:56:05.551425 11835 solver.cpp:239] Iteration 190 (1.67248 iter/s, 5.97915s/10 iters), loss = 11.5961
I0523 21:56:05.551473 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.5961 (* 1 = 11.5961 loss)
I0523 21:56:05.551492 11835 sgd_solver.cpp:112] Iteration 190, lr = 0.1
I0523 21:56:14.428870 11835 solver.cpp:239] Iteration 200 (1.1265 iter/s, 8.87705s/10 iters), loss = 11.6991
I0523 21:56:14.428920 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.6991 (* 1 = 11.6991 loss)
I0523 21:56:14.429170 11835 sgd_solver.cpp:112] Iteration 200, lr = 0.1
I0523 21:56:20.736729 11835 solver.cpp:239] Iteration 210 (1.5854 iter/s, 6.30756s/10 iters), loss = 11.4643
I0523 21:56:20.736781 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4643 (* 1 = 11.4643 loss)
I0523 21:56:20.736968 11835 sgd_solver.cpp:112] Iteration 210, lr = 0.1
I0523 21:56:27.467408 11835 solver.cpp:239] Iteration 220 (1.4858 iter/s, 6.73037s/10 iters), loss = 11.6932
I0523 21:56:27.467453 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.6932 (* 1 = 11.6932 loss)
I0523 21:56:27.467635 11835 sgd_solver.cpp:112] Iteration 220, lr = 0.1
I0523 21:56:37.367221 11835 solver.cpp:239] Iteration 230 (1.01017 iter/s, 9.89937s/10 iters), loss = 11.6763
I0523 21:56:37.367502 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.6763 (* 1 = 11.6763 loss)
I0523 21:56:37.367609 11835 sgd_solver.cpp:112] Iteration 230, lr = 0.1
I0523 21:56:44.278379 11835 solver.cpp:239] Iteration 240 (1.44704 iter/s, 6.91064s/10 iters), loss = 11.9452
I0523 21:56:44.278415 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.9452 (* 1 = 11.9452 loss)
I0523 21:56:44.278426 11835 sgd_solver.cpp:112] Iteration 240, lr = 0.1
I0523 21:56:52.036729 11835 solver.cpp:239] Iteration 250 (1.28905 iter/s, 7.75762s/10 iters), loss = 11.8404
I0523 21:56:52.036772 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.8404 (* 1 = 11.8404 loss)
I0523 21:56:52.036983 11835 sgd_solver.cpp:112] Iteration 250, lr = 0.1
I0523 21:56:57.971555 11835 solver.cpp:239] Iteration 260 (1.68505 iter/s, 5.93455s/10 iters), loss = 11.5623
I0523 21:56:57.971599 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.5623 (* 1 = 11.5623 loss)
I0523 21:56:58.380219 11835 sgd_solver.cpp:112] Iteration 260, lr = 0.1
I0523 21:57:04.756075 11835 solver.cpp:239] Iteration 270 (1.47401 iter/s, 6.78421s/10 iters), loss = 11.0043
I0523 21:57:04.756129 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0043 (* 1 = 11.0043 loss)
I0523 21:57:04.756239 11835 sgd_solver.cpp:112] Iteration 270, lr = 0.1
I0523 21:57:10.982020 11835 solver.cpp:239] Iteration 280 (1.60626 iter/s, 6.22565s/10 iters), loss = 11.4722
I0523 21:57:10.982282 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4722 (* 1 = 11.4722 loss)
I0523 21:57:10.982333 11835 sgd_solver.cpp:112] Iteration 280, lr = 0.1
I0523 21:57:16.641588 11835 solver.cpp:239] Iteration 290 (1.76706 iter/s, 5.65913s/10 iters), loss = 11.2209
I0523 21:57:16.641628 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2209 (* 1 = 11.2209 loss)
I0523 21:57:16.641638 11835 sgd_solver.cpp:112] Iteration 290, lr = 0.1
I0523 21:57:24.743118 11835 solver.cpp:239] Iteration 300 (1.23439 iter/s, 8.10117s/10 iters), loss = 11.3552
I0523 21:57:24.743160 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3552 (* 1 = 11.3552 loss)
I0523 21:57:25.468317 11835 sgd_solver.cpp:112] Iteration 300, lr = 0.1
I0523 21:57:31.982466 11835 solver.cpp:239] Iteration 310 (1.3814 iter/s, 7.23903s/10 iters), loss = 11.1912
I0523 21:57:31.982501 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1912 (* 1 = 11.1912 loss)
I0523 21:57:32.468200 11835 sgd_solver.cpp:112] Iteration 310, lr = 0.1
I0523 21:57:38.856956 11835 solver.cpp:239] Iteration 320 (1.45472 iter/s, 6.87418s/10 iters), loss = 11.3846
I0523 21:57:38.857000 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3846 (* 1 = 11.3846 loss)
I0523 21:57:38.858948 11835 sgd_solver.cpp:112] Iteration 320, lr = 0.1
I0523 21:57:44.779417 11835 solver.cpp:239] Iteration 330 (1.68857 iter/s, 5.92218s/10 iters), loss = 11.4874
I0523 21:57:44.779655 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4874 (* 1 = 11.4874 loss)
I0523 21:57:44.779695 11835 sgd_solver.cpp:112] Iteration 330, lr = 0.1
I0523 21:57:50.781424 11835 solver.cpp:239] Iteration 340 (1.66623 iter/s, 6.00157s/10 iters), loss = 11.5376
I0523 21:57:50.781468 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.5376 (* 1 = 11.5376 loss)
I0523 21:57:50.928452 11835 sgd_solver.cpp:112] Iteration 340, lr = 0.1
I0523 21:57:56.819510 11835 solver.cpp:239] Iteration 350 (1.65623 iter/s, 6.0378s/10 iters), loss = 11.4686
I0523 21:57:56.819552 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4686 (* 1 = 11.4686 loss)
I0523 21:57:56.819759 11835 sgd_solver.cpp:112] Iteration 350, lr = 0.1
I0523 21:58:03.235841 11835 solver.cpp:239] Iteration 360 (1.55859 iter/s, 6.41604s/10 iters), loss = 11.286
I0523 21:58:03.235879 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.286 (* 1 = 11.286 loss)
I0523 21:58:03.235958 11835 sgd_solver.cpp:112] Iteration 360, lr = 0.1
I0523 21:58:09.026899 11835 solver.cpp:239] Iteration 370 (1.72688 iter/s, 5.79079s/10 iters), loss = 11.4609
I0523 21:58:09.026950 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4609 (* 1 = 11.4609 loss)
I0523 21:58:09.027240 11835 sgd_solver.cpp:112] Iteration 370, lr = 0.1
I0523 21:58:15.132045 11835 solver.cpp:239] Iteration 380 (1.63804 iter/s, 6.10485s/10 iters), loss = 11.375
I0523 21:58:15.132329 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.375 (* 1 = 11.375 loss)
I0523 21:58:15.132388 11835 sgd_solver.cpp:112] Iteration 380, lr = 0.1
I0523 21:58:21.654141 11835 solver.cpp:239] Iteration 390 (1.53343 iter/s, 6.52132s/10 iters), loss = 11.3033
I0523 21:58:21.654188 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3033 (* 1 = 11.3033 loss)
I0523 21:58:21.654203 11835 sgd_solver.cpp:112] Iteration 390, lr = 0.1
I0523 21:58:31.727771 11835 solver.cpp:239] Iteration 400 (0.992734 iter/s, 10.0732s/10 iters), loss = 11.3118
I0523 21:58:31.727813 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3118 (* 1 = 11.3118 loss)
I0523 21:58:31.727826 11835 sgd_solver.cpp:112] Iteration 400, lr = 0.1
I0523 21:58:39.594286 11835 solver.cpp:239] Iteration 410 (1.27141 iter/s, 7.86531s/10 iters), loss = 11.5776
I0523 21:58:39.594331 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.5776 (* 1 = 11.5776 loss)
I0523 21:58:40.077366 11835 sgd_solver.cpp:112] Iteration 410, lr = 0.1
I0523 21:58:47.022320 11835 solver.cpp:239] Iteration 420 (1.34631 iter/s, 7.4277s/10 iters), loss = 11.6604
I0523 21:58:47.022578 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.6604 (* 1 = 11.6604 loss)
I0523 21:58:47.022629 11835 sgd_solver.cpp:112] Iteration 420, lr = 0.1
I0523 21:58:53.708204 11835 solver.cpp:239] Iteration 430 (1.49591 iter/s, 6.68491s/10 iters), loss = 11.3166
I0523 21:58:53.708256 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3166 (* 1 = 11.3166 loss)
I0523 21:58:53.709476 11835 sgd_solver.cpp:112] Iteration 430, lr = 0.1
I0523 21:59:00.688740 11835 solver.cpp:239] Iteration 440 (1.43262 iter/s, 6.98022s/10 iters), loss = 11.2967
I0523 21:59:00.688782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2967 (* 1 = 11.2967 loss)
I0523 21:59:01.069505 11835 sgd_solver.cpp:112] Iteration 440, lr = 0.1
I0523 21:59:07.586761 11835 solver.cpp:239] Iteration 450 (1.44976 iter/s, 6.89771s/10 iters), loss = 11.4539
I0523 21:59:07.586805 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4539 (* 1 = 11.4539 loss)
I0523 21:59:07.586920 11835 sgd_solver.cpp:112] Iteration 450, lr = 0.1
I0523 21:59:15.643631 11835 solver.cpp:239] Iteration 460 (1.24123 iter/s, 8.05651s/10 iters), loss = 11.0231
I0523 21:59:15.643679 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0231 (* 1 = 11.0231 loss)
I0523 21:59:15.643858 11835 sgd_solver.cpp:112] Iteration 460, lr = 0.1
I0523 21:59:21.407582 11835 solver.cpp:239] Iteration 470 (1.735 iter/s, 5.76368s/10 iters), loss = 11.2494
I0523 21:59:21.407833 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2494 (* 1 = 11.2494 loss)
I0523 21:59:21.407887 11835 sgd_solver.cpp:112] Iteration 470, lr = 0.1
I0523 21:59:28.214233 11835 solver.cpp:239] Iteration 480 (1.46926 iter/s, 6.80613s/10 iters), loss = 11.0416
I0523 21:59:28.214280 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0416 (* 1 = 11.0416 loss)
I0523 21:59:28.214529 11835 sgd_solver.cpp:112] Iteration 480, lr = 0.1
I0523 21:59:36.034081 11835 solver.cpp:239] Iteration 490 (1.27885 iter/s, 7.81951s/10 iters), loss = 11.4731
I0523 21:59:36.034122 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4731 (* 1 = 11.4731 loss)
I0523 21:59:36.554584 11835 sgd_solver.cpp:112] Iteration 490, lr = 0.1
I0523 21:59:43.826864 11835 solver.cpp:239] Iteration 500 (1.2833 iter/s, 7.79243s/10 iters), loss = 11.2622
I0523 21:59:43.826912 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2622 (* 1 = 11.2622 loss)
I0523 21:59:43.827283 11835 sgd_solver.cpp:112] Iteration 500, lr = 0.1
I0523 21:59:51.214434 11835 solver.cpp:239] Iteration 510 (1.35369 iter/s, 7.38724s/10 iters), loss = 11.1107
I0523 21:59:51.214475 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1107 (* 1 = 11.1107 loss)
I0523 21:59:52.233129 11835 sgd_solver.cpp:112] Iteration 510, lr = 0.1
I0523 21:59:57.795543 11835 solver.cpp:239] Iteration 520 (1.51957 iter/s, 6.58081s/10 iters), loss = 11.1925
I0523 21:59:57.795594 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1925 (* 1 = 11.1925 loss)
I0523 21:59:57.795608 11835 sgd_solver.cpp:112] Iteration 520, lr = 0.1
I0523 22:00:03.887959 11835 solver.cpp:239] Iteration 530 (1.64148 iter/s, 6.09207s/10 iters), loss = 10.8099
I0523 22:00:03.888002 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8099 (* 1 = 10.8099 loss)
I0523 22:00:03.888057 11835 sgd_solver.cpp:112] Iteration 530, lr = 0.1
I0523 22:00:11.846922 11835 solver.cpp:239] Iteration 540 (1.2565 iter/s, 7.95862s/10 iters), loss = 11.8216
I0523 22:00:11.846967 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.8216 (* 1 = 11.8216 loss)
I0523 22:00:11.847075 11835 sgd_solver.cpp:112] Iteration 540, lr = 0.1
I0523 22:00:18.108680 11835 solver.cpp:239] Iteration 550 (1.59707 iter/s, 6.26147s/10 iters), loss = 11.0428
I0523 22:00:18.108721 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0428 (* 1 = 11.0428 loss)
I0523 22:00:18.108736 11835 sgd_solver.cpp:112] Iteration 550, lr = 0.1
I0523 22:00:23.976640 11835 solver.cpp:239] Iteration 560 (1.7043 iter/s, 5.86751s/10 iters), loss = 11.0184
I0523 22:00:23.976871 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0184 (* 1 = 11.0184 loss)
I0523 22:00:24.013394 11835 sgd_solver.cpp:112] Iteration 560, lr = 0.1
I0523 22:00:31.243867 11835 solver.cpp:239] Iteration 570 (1.37613 iter/s, 7.26675s/10 iters), loss = 11.3435
I0523 22:00:31.243914 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3435 (* 1 = 11.3435 loss)
I0523 22:00:31.244225 11835 sgd_solver.cpp:112] Iteration 570, lr = 0.1
I0523 22:00:37.006884 11835 solver.cpp:239] Iteration 580 (1.73528 iter/s, 5.76275s/10 iters), loss = 10.8975
I0523 22:00:37.006927 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8975 (* 1 = 10.8975 loss)
I0523 22:00:37.006953 11835 sgd_solver.cpp:112] Iteration 580, lr = 0.1
I0523 22:00:42.857863 11835 solver.cpp:239] Iteration 590 (1.70919 iter/s, 5.85071s/10 iters), loss = 10.7571
I0523 22:00:42.857902 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7571 (* 1 = 10.7571 loss)
I0523 22:00:43.040068 11835 sgd_solver.cpp:112] Iteration 590, lr = 0.1
I0523 22:00:51.768381 11835 solver.cpp:239] Iteration 600 (1.12232 iter/s, 8.91014s/10 iters), loss = 11.2559
I0523 22:00:51.768424 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2559 (* 1 = 11.2559 loss)
I0523 22:00:51.815366 11835 sgd_solver.cpp:112] Iteration 600, lr = 0.1
I0523 22:00:57.557096 11835 solver.cpp:239] Iteration 610 (1.72758 iter/s, 5.78845s/10 iters), loss = 10.8829
I0523 22:00:57.557346 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8829 (* 1 = 10.8829 loss)
I0523 22:00:57.557396 11835 sgd_solver.cpp:112] Iteration 610, lr = 0.1
I0523 22:01:06.472136 11835 solver.cpp:239] Iteration 620 (1.12177 iter/s, 8.91446s/10 iters), loss = 11.3781
I0523 22:01:06.472177 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3781 (* 1 = 11.3781 loss)
I0523 22:01:06.472257 11835 sgd_solver.cpp:112] Iteration 620, lr = 0.1
I0523 22:01:12.984114 11835 solver.cpp:239] Iteration 630 (1.5357 iter/s, 6.51168s/10 iters), loss = 10.7731
I0523 22:01:12.984158 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7731 (* 1 = 10.7731 loss)
I0523 22:01:12.984294 11835 sgd_solver.cpp:112] Iteration 630, lr = 0.1
I0523 22:01:19.580138 11835 solver.cpp:239] Iteration 640 (1.51613 iter/s, 6.59573s/10 iters), loss = 11.3721
I0523 22:01:19.580183 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3721 (* 1 = 11.3721 loss)
I0523 22:01:19.580299 11835 sgd_solver.cpp:112] Iteration 640, lr = 0.1
I0523 22:01:25.513943 11835 solver.cpp:239] Iteration 650 (1.68534 iter/s, 5.93354s/10 iters), loss = 11.1847
I0523 22:01:25.513984 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1847 (* 1 = 11.1847 loss)
I0523 22:01:25.517747 11835 sgd_solver.cpp:112] Iteration 650, lr = 0.1
I0523 22:01:31.256307 11835 solver.cpp:239] Iteration 660 (1.74152 iter/s, 5.74211s/10 iters), loss = 11.0794
I0523 22:01:31.256584 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0794 (* 1 = 11.0794 loss)
I0523 22:01:31.256636 11835 sgd_solver.cpp:112] Iteration 660, lr = 0.1
I0523 22:01:40.354725 11835 solver.cpp:239] Iteration 670 (1.09917 iter/s, 9.09777s/10 iters), loss = 10.472
I0523 22:01:40.354768 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.472 (* 1 = 10.472 loss)
I0523 22:01:40.354859 11835 sgd_solver.cpp:112] Iteration 670, lr = 0.1
I0523 22:01:46.631903 11835 solver.cpp:239] Iteration 680 (1.59314 iter/s, 6.27689s/10 iters), loss = 10.82
I0523 22:01:46.631947 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.82 (* 1 = 10.82 loss)
I0523 22:01:46.631958 11835 sgd_solver.cpp:112] Iteration 680, lr = 0.1
I0523 22:01:52.852744 11835 solver.cpp:239] Iteration 690 (1.60774 iter/s, 6.21992s/10 iters), loss = 10.9796
I0523 22:01:52.852785 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9796 (* 1 = 10.9796 loss)
I0523 22:01:52.906138 11835 sgd_solver.cpp:112] Iteration 690, lr = 0.1
I0523 22:01:59.620520 11835 solver.cpp:239] Iteration 700 (1.47766 iter/s, 6.76747s/10 iters), loss = 11.0405
I0523 22:01:59.620568 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0405 (* 1 = 11.0405 loss)
I0523 22:01:59.620697 11835 sgd_solver.cpp:112] Iteration 700, lr = 0.1
I0523 22:02:05.748520 11835 solver.cpp:239] Iteration 710 (1.63193 iter/s, 6.12772s/10 iters), loss = 10.8756
I0523 22:02:05.748759 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8756 (* 1 = 10.8756 loss)
I0523 22:02:05.748805 11835 sgd_solver.cpp:112] Iteration 710, lr = 0.1
I0523 22:02:12.111836 11835 solver.cpp:239] Iteration 720 (1.57163 iter/s, 6.36284s/10 iters), loss = 10.9312
I0523 22:02:12.111876 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9312 (* 1 = 10.9312 loss)
I0523 22:02:13.326376 11835 sgd_solver.cpp:112] Iteration 720, lr = 0.1
I0523 22:02:22.138099 11835 solver.cpp:239] Iteration 730 (0.997423 iter/s, 10.0258s/10 iters), loss = 10.9177
I0523 22:02:22.138161 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9177 (* 1 = 10.9177 loss)
I0523 22:02:22.155498 11835 sgd_solver.cpp:112] Iteration 730, lr = 0.1
I0523 22:02:28.708834 11835 solver.cpp:239] Iteration 740 (1.52197 iter/s, 6.57042s/10 iters), loss = 10.8866
I0523 22:02:28.708884 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8866 (* 1 = 10.8866 loss)
I0523 22:02:28.709062 11835 sgd_solver.cpp:112] Iteration 740, lr = 0.1
I0523 22:02:36.476692 11835 solver.cpp:239] Iteration 750 (1.28741 iter/s, 7.76751s/10 iters), loss = 11.3587
I0523 22:02:36.476786 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3587 (* 1 = 11.3587 loss)
I0523 22:02:36.799484 11835 sgd_solver.cpp:112] Iteration 750, lr = 0.1
I0523 22:02:44.460018 11835 solver.cpp:239] Iteration 760 (1.25267 iter/s, 7.98293s/10 iters), loss = 10.9748
I0523 22:02:44.460063 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9748 (* 1 = 10.9748 loss)
I0523 22:02:44.476729 11835 sgd_solver.cpp:112] Iteration 760, lr = 0.1
I0523 22:02:52.245527 11835 solver.cpp:239] Iteration 770 (1.28449 iter/s, 7.78516s/10 iters), loss = 11.3965
I0523 22:02:52.245574 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3965 (* 1 = 11.3965 loss)
I0523 22:02:52.245843 11835 sgd_solver.cpp:112] Iteration 770, lr = 0.1
I0523 22:02:58.225184 11835 solver.cpp:239] Iteration 780 (1.67241 iter/s, 5.97938s/10 iters), loss = 10.6664
I0523 22:02:58.225222 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6664 (* 1 = 10.6664 loss)
I0523 22:02:59.154417 11835 sgd_solver.cpp:112] Iteration 780, lr = 0.1
I0523 22:03:06.083344 11835 solver.cpp:239] Iteration 790 (1.27262 iter/s, 7.85782s/10 iters), loss = 11.225
I0523 22:03:06.083381 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.225 (* 1 = 11.225 loss)
I0523 22:03:06.083518 11835 sgd_solver.cpp:112] Iteration 790, lr = 0.1
I0523 22:03:12.383100 11835 solver.cpp:239] Iteration 800 (1.58744 iter/s, 6.29947s/10 iters), loss = 10.6594
I0523 22:03:12.383258 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6594 (* 1 = 10.6594 loss)
I0523 22:03:12.383427 11835 sgd_solver.cpp:112] Iteration 800, lr = 0.1
I0523 22:03:19.426883 11835 solver.cpp:239] Iteration 810 (1.41977 iter/s, 7.04337s/10 iters), loss = 10.7067
I0523 22:03:19.426934 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7067 (* 1 = 10.7067 loss)
I0523 22:03:19.427397 11835 sgd_solver.cpp:112] Iteration 810, lr = 0.1
I0523 22:03:27.714607 11835 solver.cpp:239] Iteration 820 (1.20666 iter/s, 8.28736s/10 iters), loss = 10.5244
I0523 22:03:27.714653 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5244 (* 1 = 10.5244 loss)
I0523 22:03:27.714939 11835 sgd_solver.cpp:112] Iteration 820, lr = 0.1
I0523 22:03:33.872601 11835 solver.cpp:239] Iteration 830 (1.62398 iter/s, 6.15771s/10 iters), loss = 10.8397
I0523 22:03:33.872645 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8397 (* 1 = 10.8397 loss)
I0523 22:03:33.872769 11835 sgd_solver.cpp:112] Iteration 830, lr = 0.1
I0523 22:03:41.049183 11835 solver.cpp:239] Iteration 840 (1.39348 iter/s, 7.17626s/10 iters), loss = 11.2525
I0523 22:03:41.049219 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2525 (* 1 = 11.2525 loss)
I0523 22:03:41.049257 11835 sgd_solver.cpp:112] Iteration 840, lr = 0.1
I0523 22:03:48.117089 11835 solver.cpp:239] Iteration 850 (1.41491 iter/s, 7.06759s/10 iters), loss = 10.6646
I0523 22:03:48.117312 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6646 (* 1 = 10.6646 loss)
I0523 22:03:48.118173 11835 sgd_solver.cpp:112] Iteration 850, lr = 0.1
I0523 22:03:55.672372 11835 solver.cpp:239] Iteration 860 (1.32366 iter/s, 7.55481s/10 iters), loss = 10.7921
I0523 22:03:55.672420 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7921 (* 1 = 10.7921 loss)
I0523 22:03:56.126929 11835 sgd_solver.cpp:112] Iteration 860, lr = 0.1
I0523 22:04:04.179489 11835 solver.cpp:239] Iteration 870 (1.17554 iter/s, 8.50674s/10 iters), loss = 10.8103
I0523 22:04:04.179533 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8103 (* 1 = 10.8103 loss)
I0523 22:04:04.179647 11835 sgd_solver.cpp:112] Iteration 870, lr = 0.1
I0523 22:04:10.156308 11835 solver.cpp:239] Iteration 880 (1.67321 iter/s, 5.97655s/10 iters), loss = 10.7507
I0523 22:04:10.156342 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7507 (* 1 = 10.7507 loss)
I0523 22:04:10.156363 11835 sgd_solver.cpp:112] Iteration 880, lr = 0.1
I0523 22:04:18.330531 11835 solver.cpp:239] Iteration 890 (1.22341 iter/s, 8.17388s/10 iters), loss = 10.8255
I0523 22:04:18.330821 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8255 (* 1 = 10.8255 loss)
I0523 22:04:18.330916 11835 sgd_solver.cpp:112] Iteration 890, lr = 0.1
I0523 22:04:25.493829 11835 solver.cpp:239] Iteration 900 (1.39611 iter/s, 7.16277s/10 iters), loss = 10.8212
I0523 22:04:25.493880 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8212 (* 1 = 10.8212 loss)
I0523 22:04:25.494410 11835 sgd_solver.cpp:112] Iteration 900, lr = 0.1
I0523 22:04:31.292697 11835 solver.cpp:239] Iteration 910 (1.72455 iter/s, 5.7986s/10 iters), loss = 10.4656
I0523 22:04:31.292732 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4656 (* 1 = 10.4656 loss)
I0523 22:04:31.292742 11835 sgd_solver.cpp:112] Iteration 910, lr = 0.1
I0523 22:04:38.873803 11835 solver.cpp:239] Iteration 920 (1.31913 iter/s, 7.58078s/10 iters), loss = 10.7824
I0523 22:04:38.873852 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7824 (* 1 = 10.7824 loss)
I0523 22:04:38.874064 11835 sgd_solver.cpp:112] Iteration 920, lr = 0.1
I0523 22:04:47.353453 11835 solver.cpp:239] Iteration 930 (1.17935 iter/s, 8.47928s/10 iters), loss = 10.3604
I0523 22:04:47.353497 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3604 (* 1 = 10.3604 loss)
I0523 22:04:47.474107 11835 sgd_solver.cpp:112] Iteration 930, lr = 0.1
I0523 22:04:53.561547 11835 solver.cpp:239] Iteration 940 (1.61087 iter/s, 6.20781s/10 iters), loss = 11.0296
I0523 22:04:53.561805 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0296 (* 1 = 11.0296 loss)
I0523 22:04:53.561846 11835 sgd_solver.cpp:112] Iteration 940, lr = 0.1
I0523 22:04:59.858183 11835 solver.cpp:239] Iteration 950 (1.58834 iter/s, 6.29587s/10 iters), loss = 10.7387
I0523 22:04:59.858220 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7387 (* 1 = 10.7387 loss)
I0523 22:04:59.858232 11835 sgd_solver.cpp:112] Iteration 950, lr = 0.1
I0523 22:05:08.376894 11835 solver.cpp:239] Iteration 960 (1.17424 iter/s, 8.51612s/10 iters), loss = 10.7059
I0523 22:05:08.376945 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7059 (* 1 = 10.7059 loss)
I0523 22:05:08.377135 11835 sgd_solver.cpp:112] Iteration 960, lr = 0.1
I0523 22:05:14.184103 11835 solver.cpp:239] Iteration 970 (1.72208 iter/s, 5.80694s/10 iters), loss = 11.1792
I0523 22:05:14.184139 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1792 (* 1 = 11.1792 loss)
I0523 22:05:14.441602 11835 sgd_solver.cpp:112] Iteration 970, lr = 0.1
I0523 22:05:21.954365 11835 solver.cpp:239] Iteration 980 (1.28701 iter/s, 7.76992s/10 iters), loss = 10.4736
I0523 22:05:21.954411 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4736 (* 1 = 10.4736 loss)
I0523 22:05:22.866921 11835 sgd_solver.cpp:112] Iteration 980, lr = 0.1
I0523 22:05:33.021154 11835 solver.cpp:239] Iteration 990 (0.903642 iter/s, 11.0663s/10 iters), loss = 10.3573
I0523 22:05:33.021383 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3573 (* 1 = 10.3573 loss)
I0523 22:05:33.082736 11835 sgd_solver.cpp:112] Iteration 990, lr = 0.1
I0523 22:05:39.262667 11835 solver.cpp:239] Iteration 1000 (1.60229 iter/s, 6.24108s/10 iters), loss = 11.3398
I0523 22:05:39.262740 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3398 (* 1 = 11.3398 loss)
I0523 22:05:39.262753 11835 sgd_solver.cpp:112] Iteration 1000, lr = 0.1
I0523 22:05:46.527169 11835 solver.cpp:239] Iteration 1010 (1.37699 iter/s, 7.26224s/10 iters), loss = 10.8056
I0523 22:05:46.527220 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8056 (* 1 = 10.8056 loss)
I0523 22:05:47.664649 11835 sgd_solver.cpp:112] Iteration 1010, lr = 0.1
I0523 22:05:54.757885 11835 solver.cpp:239] Iteration 1020 (1.21502 iter/s, 8.23035s/10 iters), loss = 11.0309
I0523 22:05:54.757935 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0309 (* 1 = 11.0309 loss)
I0523 22:05:54.758195 11835 sgd_solver.cpp:112] Iteration 1020, lr = 0.1
I0523 22:06:01.326064 11835 solver.cpp:239] Iteration 1030 (1.52256 iter/s, 6.56787s/10 iters), loss = 10.9099
I0523 22:06:01.326108 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9099 (* 1 = 10.9099 loss)
I0523 22:06:01.326321 11835 sgd_solver.cpp:112] Iteration 1030, lr = 0.1
I0523 22:06:07.162987 11835 solver.cpp:239] Iteration 1040 (1.71332 iter/s, 5.83663s/10 iters), loss = 10.5267
I0523 22:06:07.163162 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5267 (* 1 = 10.5267 loss)
I0523 22:06:07.163341 11835 sgd_solver.cpp:112] Iteration 1040, lr = 0.1
I0523 22:06:13.463508 11835 solver.cpp:239] Iteration 1050 (1.58727 iter/s, 6.30012s/10 iters), loss = 11.1434
I0523 22:06:13.463546 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1434 (* 1 = 11.1434 loss)
I0523 22:06:13.463688 11835 sgd_solver.cpp:112] Iteration 1050, lr = 0.1
I0523 22:06:19.873203 11835 solver.cpp:239] Iteration 1060 (1.56021 iter/s, 6.40941s/10 iters), loss = 10.8742
I0523 22:06:19.873246 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8742 (* 1 = 10.8742 loss)
I0523 22:06:20.092078 11835 sgd_solver.cpp:112] Iteration 1060, lr = 0.1
I0523 22:06:25.757721 11835 solver.cpp:239] Iteration 1070 (1.69945 iter/s, 5.88425s/10 iters), loss = 10.9621
I0523 22:06:25.757760 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9621 (* 1 = 10.9621 loss)
I0523 22:06:25.757771 11835 sgd_solver.cpp:112] Iteration 1070, lr = 0.1
I0523 22:06:32.825371 11835 solver.cpp:239] Iteration 1080 (1.41519 iter/s, 7.06618s/10 iters), loss = 10.7287
I0523 22:06:32.825414 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7287 (* 1 = 10.7287 loss)
I0523 22:06:32.843766 11835 sgd_solver.cpp:112] Iteration 1080, lr = 0.1
I0523 22:06:38.426853 11835 solver.cpp:239] Iteration 1090 (1.78532 iter/s, 5.60122s/10 iters), loss = 10.8406
I0523 22:06:38.427110 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8406 (* 1 = 10.8406 loss)
I0523 22:06:38.427177 11835 sgd_solver.cpp:112] Iteration 1090, lr = 0.1
I0523 22:06:45.338146 11835 solver.cpp:239] Iteration 1100 (1.44701 iter/s, 6.91078s/10 iters), loss = 11.1767
I0523 22:06:45.338181 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1767 (* 1 = 11.1767 loss)
I0523 22:06:45.338237 11835 sgd_solver.cpp:112] Iteration 1100, lr = 0.1
I0523 22:06:51.696496 11835 solver.cpp:239] Iteration 1110 (1.57281 iter/s, 6.35806s/10 iters), loss = 10.4744
I0523 22:06:51.696544 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4744 (* 1 = 10.4744 loss)
I0523 22:06:51.696588 11835 sgd_solver.cpp:112] Iteration 1110, lr = 0.1
I0523 22:06:57.636395 11835 solver.cpp:239] Iteration 1120 (1.68361 iter/s, 5.93962s/10 iters), loss = 11.0535
I0523 22:06:57.636430 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0535 (* 1 = 11.0535 loss)
I0523 22:06:57.636440 11835 sgd_solver.cpp:112] Iteration 1120, lr = 0.1
I0523 22:07:04.950992 11835 solver.cpp:239] Iteration 1130 (1.36719 iter/s, 7.31427s/10 iters), loss = 11.2343
I0523 22:07:04.951048 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2343 (* 1 = 11.2343 loss)
I0523 22:07:04.951175 11835 sgd_solver.cpp:112] Iteration 1130, lr = 0.1
I0523 22:07:12.384423 11835 solver.cpp:239] Iteration 1140 (1.34534 iter/s, 7.43308s/10 iters), loss = 10.6313
I0523 22:07:12.384670 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6313 (* 1 = 10.6313 loss)
I0523 22:07:12.384771 11835 sgd_solver.cpp:112] Iteration 1140, lr = 0.1
I0523 22:07:19.071279 11835 solver.cpp:239] Iteration 1150 (1.49558 iter/s, 6.68637s/10 iters), loss = 10.8266
I0523 22:07:19.071326 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8266 (* 1 = 10.8266 loss)
I0523 22:07:19.071712 11835 sgd_solver.cpp:112] Iteration 1150, lr = 0.1
I0523 22:07:26.964701 11835 solver.cpp:239] Iteration 1160 (1.26693 iter/s, 7.89307s/10 iters), loss = 10.9132
I0523 22:07:26.964751 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9132 (* 1 = 10.9132 loss)
I0523 22:07:26.965018 11835 sgd_solver.cpp:112] Iteration 1160, lr = 0.1
I0523 22:07:33.110361 11835 solver.cpp:239] Iteration 1170 (1.62724 iter/s, 6.14538s/10 iters), loss = 10.7426
I0523 22:07:33.110401 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7426 (* 1 = 10.7426 loss)
I0523 22:07:33.246594 11835 sgd_solver.cpp:112] Iteration 1170, lr = 0.1
I0523 22:07:40.587359 11835 solver.cpp:239] Iteration 1180 (1.33749 iter/s, 7.47667s/10 iters), loss = 10.265
I0523 22:07:40.587404 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.265 (* 1 = 10.265 loss)
I0523 22:07:41.754127 11835 sgd_solver.cpp:112] Iteration 1180, lr = 0.1
I0523 22:07:49.519780 11835 solver.cpp:239] Iteration 1190 (1.11957 iter/s, 8.93204s/10 iters), loss = 10.6409
I0523 22:07:49.519939 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6409 (* 1 = 10.6409 loss)
I0523 22:07:50.335476 11835 sgd_solver.cpp:112] Iteration 1190, lr = 0.1
I0523 22:07:57.369906 11835 solver.cpp:239] Iteration 1200 (1.27394 iter/s, 7.84967s/10 iters), loss = 9.81063
I0523 22:07:57.369948 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.81063 (* 1 = 9.81063 loss)
I0523 22:07:57.370164 11835 sgd_solver.cpp:112] Iteration 1200, lr = 0.1
I0523 22:08:03.117365 11835 solver.cpp:239] Iteration 1210 (1.73998 iter/s, 5.7472s/10 iters), loss = 10.31
I0523 22:08:03.117403 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.31 (* 1 = 10.31 loss)
I0523 22:08:03.117413 11835 sgd_solver.cpp:112] Iteration 1210, lr = 0.1
I0523 22:08:09.034157 11835 solver.cpp:239] Iteration 1220 (1.69018 iter/s, 5.91653s/10 iters), loss = 10.3654
I0523 22:08:09.034199 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3654 (* 1 = 10.3654 loss)
I0523 22:08:09.034339 11835 sgd_solver.cpp:112] Iteration 1220, lr = 0.1
I0523 22:08:15.291489 11835 solver.cpp:239] Iteration 1230 (1.5982 iter/s, 6.25705s/10 iters), loss = 10.8293
I0523 22:08:15.291532 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8293 (* 1 = 10.8293 loss)
I0523 22:08:15.369549 11835 sgd_solver.cpp:112] Iteration 1230, lr = 0.1
I0523 22:08:21.071382 11835 solver.cpp:239] Iteration 1240 (1.73021 iter/s, 5.77963s/10 iters), loss = 10.2777
I0523 22:08:21.071641 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2777 (* 1 = 10.2777 loss)
I0523 22:08:21.071692 11835 sgd_solver.cpp:112] Iteration 1240, lr = 0.1
I0523 22:08:27.954671 11835 solver.cpp:239] Iteration 1250 (1.45312 iter/s, 6.88174s/10 iters), loss = 10.6212
I0523 22:08:27.954767 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6212 (* 1 = 10.6212 loss)
I0523 22:08:27.954916 11835 sgd_solver.cpp:112] Iteration 1250, lr = 0.1
I0523 22:08:34.259357 11835 solver.cpp:239] Iteration 1260 (1.58621 iter/s, 6.30435s/10 iters), loss = 11.4519
I0523 22:08:34.259392 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4519 (* 1 = 11.4519 loss)
I0523 22:08:34.259542 11835 sgd_solver.cpp:112] Iteration 1260, lr = 0.1
I0523 22:08:39.897709 11835 solver.cpp:239] Iteration 1270 (1.77365 iter/s, 5.6381s/10 iters), loss = 10.3951
I0523 22:08:39.897752 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3951 (* 1 = 10.3951 loss)
I0523 22:08:39.897763 11835 sgd_solver.cpp:112] Iteration 1270, lr = 0.1
I0523 22:08:46.196651 11835 solver.cpp:239] Iteration 1280 (1.58764 iter/s, 6.29866s/10 iters), loss = 10.4637
I0523 22:08:46.196694 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4637 (* 1 = 10.4637 loss)
I0523 22:08:46.196847 11835 sgd_solver.cpp:112] Iteration 1280, lr = 0.1
I0523 22:08:52.766988 11835 solver.cpp:239] Iteration 1290 (1.52206 iter/s, 6.57003s/10 iters), loss = 10.9155
I0523 22:08:52.767246 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9155 (* 1 = 10.9155 loss)
I0523 22:08:52.767328 11835 sgd_solver.cpp:112] Iteration 1290, lr = 0.1
I0523 22:08:58.587450 11835 solver.cpp:239] Iteration 1300 (1.71821 iter/s, 5.82002s/10 iters), loss = 10.7194
I0523 22:08:58.587486 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7194 (* 1 = 10.7194 loss)
I0523 22:08:58.587628 11835 sgd_solver.cpp:112] Iteration 1300, lr = 0.1
I0523 22:09:06.188751 11835 solver.cpp:239] Iteration 1310 (1.31562 iter/s, 7.60097s/10 iters), loss = 10.3192
I0523 22:09:06.188794 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3192 (* 1 = 10.3192 loss)
I0523 22:09:06.188932 11835 sgd_solver.cpp:112] Iteration 1310, lr = 0.1
I0523 22:09:15.048445 11835 solver.cpp:239] Iteration 1320 (1.12875 iter/s, 8.85932s/10 iters), loss = 10.558
I0523 22:09:15.048483 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.558 (* 1 = 10.558 loss)
I0523 22:09:15.524780 11835 sgd_solver.cpp:112] Iteration 1320, lr = 0.1
I0523 22:09:21.599633 11835 solver.cpp:239] Iteration 1330 (1.52651 iter/s, 6.55089s/10 iters), loss = 10.4764
I0523 22:09:21.599685 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4764 (* 1 = 10.4764 loss)
I0523 22:09:21.599915 11835 sgd_solver.cpp:112] Iteration 1330, lr = 0.1
I0523 22:09:29.724843 11835 solver.cpp:239] Iteration 1340 (1.23079 iter/s, 8.12485s/10 iters), loss = 10.6138
I0523 22:09:29.725129 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6138 (* 1 = 10.6138 loss)
I0523 22:09:29.761584 11835 sgd_solver.cpp:112] Iteration 1340, lr = 0.1
I0523 22:09:38.600028 11835 solver.cpp:239] Iteration 1350 (1.12681 iter/s, 8.8746s/10 iters), loss = 10.7223
I0523 22:09:38.600070 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7223 (* 1 = 10.7223 loss)
I0523 22:09:38.985239 11835 sgd_solver.cpp:112] Iteration 1350, lr = 0.1
I0523 22:09:46.658313 11835 solver.cpp:239] Iteration 1360 (1.24101 iter/s, 8.05794s/10 iters), loss = 10.7787
I0523 22:09:46.658354 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7787 (* 1 = 10.7787 loss)
I0523 22:09:46.658366 11835 sgd_solver.cpp:112] Iteration 1360, lr = 0.1
I0523 22:09:52.528599 11835 solver.cpp:239] Iteration 1370 (1.70362 iter/s, 5.86984s/10 iters), loss = 10.8074
I0523 22:09:52.528637 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8074 (* 1 = 10.8074 loss)
I0523 22:09:52.528868 11835 sgd_solver.cpp:112] Iteration 1370, lr = 0.1
I0523 22:09:58.962218 11835 solver.cpp:239] Iteration 1380 (1.55441 iter/s, 6.43333s/10 iters), loss = 10.8859
I0523 22:09:58.962271 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8859 (* 1 = 10.8859 loss)
I0523 22:09:59.048261 11835 sgd_solver.cpp:112] Iteration 1380, lr = 0.1
I0523 22:10:07.598791 11835 solver.cpp:239] Iteration 1390 (1.15792 iter/s, 8.63619s/10 iters), loss = 10.3473
I0523 22:10:07.599014 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3473 (* 1 = 10.3473 loss)
I0523 22:10:07.599068 11835 sgd_solver.cpp:112] Iteration 1390, lr = 0.1
I0523 22:10:13.481341 11835 solver.cpp:239] Iteration 1400 (1.70012 iter/s, 5.88195s/10 iters), loss = 10.5946
I0523 22:10:13.481376 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5946 (* 1 = 10.5946 loss)
I0523 22:10:13.481387 11835 sgd_solver.cpp:112] Iteration 1400, lr = 0.1
I0523 22:10:21.077503 11835 solver.cpp:239] Iteration 1410 (1.31651 iter/s, 7.59583s/10 iters), loss = 9.98264
I0523 22:10:21.077544 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.98264 (* 1 = 9.98264 loss)
I0523 22:10:21.077682 11835 sgd_solver.cpp:112] Iteration 1410, lr = 0.1
I0523 22:10:29.164832 11835 solver.cpp:239] Iteration 1420 (1.23655 iter/s, 8.08699s/10 iters), loss = 11.4389
I0523 22:10:29.164873 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4389 (* 1 = 11.4389 loss)
I0523 22:10:29.384938 11835 sgd_solver.cpp:112] Iteration 1420, lr = 0.1
I0523 22:10:37.276413 11835 solver.cpp:239] Iteration 1430 (1.23286 iter/s, 8.11122s/10 iters), loss = 10.5031
I0523 22:10:37.276458 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5031 (* 1 = 10.5031 loss)
I0523 22:10:38.023684 11835 sgd_solver.cpp:112] Iteration 1430, lr = 0.1
I0523 22:10:43.622733 11835 solver.cpp:239] Iteration 1440 (1.57579 iter/s, 6.34604s/10 iters), loss = 11.2228
I0523 22:10:43.622768 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2228 (* 1 = 11.2228 loss)
I0523 22:10:43.631454 11835 sgd_solver.cpp:112] Iteration 1440, lr = 0.1
I0523 22:10:51.453943 11835 solver.cpp:239] Iteration 1450 (1.277 iter/s, 7.83088s/10 iters), loss = 10.1261
I0523 22:10:51.453984 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1261 (* 1 = 10.1261 loss)
I0523 22:10:51.818142 11835 sgd_solver.cpp:112] Iteration 1450, lr = 0.1
I0523 22:11:01.992897 11835 solver.cpp:239] Iteration 1460 (0.9489 iter/s, 10.5385s/10 iters), loss = 10.2714
I0523 22:11:01.992945 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2714 (* 1 = 10.2714 loss)
I0523 22:11:01.993300 11835 sgd_solver.cpp:112] Iteration 1460, lr = 0.1
I0523 22:11:09.024188 11835 solver.cpp:239] Iteration 1470 (1.42228 iter/s, 7.03098s/10 iters), loss = 10.2209
I0523 22:11:09.024443 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2209 (* 1 = 10.2209 loss)
I0523 22:11:09.024490 11835 sgd_solver.cpp:112] Iteration 1470, lr = 0.1
I0523 22:11:15.827291 11835 solver.cpp:239] Iteration 1480 (1.4701 iter/s, 6.80225s/10 iters), loss = 10.7119
I0523 22:11:15.827337 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7119 (* 1 = 10.7119 loss)
I0523 22:11:15.827350 11835 sgd_solver.cpp:112] Iteration 1480, lr = 0.1
I0523 22:11:21.593581 11835 solver.cpp:239] Iteration 1490 (1.73434 iter/s, 5.76589s/10 iters), loss = 10.675
I0523 22:11:21.593623 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.675 (* 1 = 10.675 loss)
I0523 22:11:21.594066 11835 sgd_solver.cpp:112] Iteration 1490, lr = 0.1
I0523 22:11:28.214877 11835 solver.cpp:239] Iteration 1500 (1.51035 iter/s, 6.621s/10 iters), loss = 10.8323
I0523 22:11:28.214920 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8323 (* 1 = 10.8323 loss)
I0523 22:11:28.609448 11835 sgd_solver.cpp:112] Iteration 1500, lr = 0.1
I0523 22:11:35.519398 11835 solver.cpp:239] Iteration 1510 (1.36908 iter/s, 7.30419s/10 iters), loss = 10.4379
I0523 22:11:35.519445 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4379 (* 1 = 10.4379 loss)
I0523 22:11:35.747428 11835 sgd_solver.cpp:112] Iteration 1510, lr = 0.1
I0523 22:11:41.301559 11835 solver.cpp:239] Iteration 1520 (1.72954 iter/s, 5.78188s/10 iters), loss = 10.568
I0523 22:11:41.301817 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.568 (* 1 = 10.568 loss)
I0523 22:11:41.301872 11835 sgd_solver.cpp:112] Iteration 1520, lr = 0.1
I0523 22:11:49.251432 11835 solver.cpp:239] Iteration 1530 (1.25801 iter/s, 7.94905s/10 iters), loss = 9.99454
I0523 22:11:49.251471 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.99454 (* 1 = 9.99454 loss)
I0523 22:11:49.251682 11835 sgd_solver.cpp:112] Iteration 1530, lr = 0.1
I0523 22:11:55.125342 11835 solver.cpp:239] Iteration 1540 (1.70252 iter/s, 5.87363s/10 iters), loss = 10.9209
I0523 22:11:55.125391 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9209 (* 1 = 10.9209 loss)
I0523 22:11:55.125507 11835 sgd_solver.cpp:112] Iteration 1540, lr = 0.1
I0523 22:12:01.444139 11835 solver.cpp:239] Iteration 1550 (1.58265 iter/s, 6.3185s/10 iters), loss = 11.1785
I0523 22:12:01.444175 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1785 (* 1 = 11.1785 loss)
I0523 22:12:01.444387 11835 sgd_solver.cpp:112] Iteration 1550, lr = 0.1
I0523 22:12:07.147119 11835 solver.cpp:239] Iteration 1560 (1.75355 iter/s, 5.7027s/10 iters), loss = 11.0823
I0523 22:12:07.147174 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0823 (* 1 = 11.0823 loss)
I0523 22:12:07.147409 11835 sgd_solver.cpp:112] Iteration 1560, lr = 0.1
I0523 22:12:13.007300 11835 solver.cpp:239] Iteration 1570 (1.70651 iter/s, 5.8599s/10 iters), loss = 10.2242
I0523 22:12:13.007537 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2242 (* 1 = 10.2242 loss)
I0523 22:12:13.007664 11835 sgd_solver.cpp:112] Iteration 1570, lr = 0.1
I0523 22:12:20.192809 11835 solver.cpp:239] Iteration 1580 (1.39178 iter/s, 7.18503s/10 iters), loss = 10.5605
I0523 22:12:20.192852 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5605 (* 1 = 10.5605 loss)
I0523 22:12:20.487360 11835 sgd_solver.cpp:112] Iteration 1580, lr = 0.1
I0523 22:12:27.481616 11835 solver.cpp:239] Iteration 1590 (1.37203 iter/s, 7.28848s/10 iters), loss = 11.1232
I0523 22:12:27.481658 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1232 (* 1 = 11.1232 loss)
I0523 22:12:27.481775 11835 sgd_solver.cpp:112] Iteration 1590, lr = 0.1
I0523 22:12:34.176793 11835 solver.cpp:239] Iteration 1600 (1.49368 iter/s, 6.69487s/10 iters), loss = 10.8342
I0523 22:12:34.176836 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8342 (* 1 = 10.8342 loss)
I0523 22:12:34.262679 11835 sgd_solver.cpp:112] Iteration 1600, lr = 0.1
I0523 22:12:41.451298 11835 solver.cpp:239] Iteration 1610 (1.37473 iter/s, 7.27418s/10 iters), loss = 10.722
I0523 22:12:41.451345 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.722 (* 1 = 10.722 loss)
I0523 22:12:41.451571 11835 sgd_solver.cpp:112] Iteration 1610, lr = 0.1
I0523 22:12:48.385704 11835 solver.cpp:239] Iteration 1620 (1.44215 iter/s, 6.93409s/10 iters), loss = 10.8709
I0523 22:12:48.385840 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8709 (* 1 = 10.8709 loss)
I0523 22:12:48.385871 11835 sgd_solver.cpp:112] Iteration 1620, lr = 0.1
I0523 22:12:55.162825 11835 solver.cpp:239] Iteration 1630 (1.47576 iter/s, 6.77616s/10 iters), loss = 10.5199
I0523 22:12:55.162864 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5199 (* 1 = 10.5199 loss)
I0523 22:12:55.435969 11835 sgd_solver.cpp:112] Iteration 1630, lr = 0.1
I0523 22:13:01.245194 11835 solver.cpp:239] Iteration 1640 (1.64417 iter/s, 6.0821s/10 iters), loss = 10.3161
I0523 22:13:01.245232 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3161 (* 1 = 10.3161 loss)
I0523 22:13:01.245268 11835 sgd_solver.cpp:112] Iteration 1640, lr = 0.1
I0523 22:13:09.890193 11835 solver.cpp:239] Iteration 1650 (1.15679 iter/s, 8.64462s/10 iters), loss = 10.7603
I0523 22:13:09.890241 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7603 (* 1 = 10.7603 loss)
I0523 22:13:09.890259 11835 sgd_solver.cpp:112] Iteration 1650, lr = 0.1
I0523 22:13:18.769524 11835 solver.cpp:239] Iteration 1660 (1.12626 iter/s, 8.87894s/10 iters), loss = 10.8985
I0523 22:13:18.769708 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8985 (* 1 = 10.8985 loss)
I0523 22:13:18.769773 11835 sgd_solver.cpp:112] Iteration 1660, lr = 0.1
I0523 22:13:27.080513 11835 solver.cpp:239] Iteration 1670 (1.20329 iter/s, 8.31052s/10 iters), loss = 11.1479
I0523 22:13:27.080549 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1479 (* 1 = 11.1479 loss)
I0523 22:13:27.080561 11835 sgd_solver.cpp:112] Iteration 1670, lr = 0.1
I0523 22:13:34.622251 11835 solver.cpp:239] Iteration 1680 (1.32621 iter/s, 7.5403s/10 iters), loss = 10.8365
I0523 22:13:34.622297 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8365 (* 1 = 10.8365 loss)
I0523 22:13:34.717465 11835 sgd_solver.cpp:112] Iteration 1680, lr = 0.1
I0523 22:13:43.513667 11835 solver.cpp:239] Iteration 1690 (1.12473 iter/s, 8.89103s/10 iters), loss = 10.9188
I0523 22:13:43.513716 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9188 (* 1 = 10.9188 loss)
I0523 22:13:43.513978 11835 sgd_solver.cpp:112] Iteration 1690, lr = 0.1
I0523 22:13:50.697405 11835 solver.cpp:239] Iteration 1700 (1.3921 iter/s, 7.18342s/10 iters), loss = 10.5807
I0523 22:13:50.697625 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5807 (* 1 = 10.5807 loss)
I0523 22:13:51.297834 11835 sgd_solver.cpp:112] Iteration 1700, lr = 0.1
I0523 22:13:58.444406 11835 solver.cpp:239] Iteration 1710 (1.29091 iter/s, 7.7465s/10 iters), loss = 11.1277
I0523 22:13:58.444454 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1277 (* 1 = 11.1277 loss)
I0523 22:13:58.444700 11835 sgd_solver.cpp:112] Iteration 1710, lr = 0.1
I0523 22:14:07.505328 11835 solver.cpp:239] Iteration 1720 (1.10369 iter/s, 9.06053s/10 iters), loss = 11.2308
I0523 22:14:07.505372 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2308 (* 1 = 11.2308 loss)
I0523 22:14:07.505491 11835 sgd_solver.cpp:112] Iteration 1720, lr = 0.1
I0523 22:14:14.274935 11835 solver.cpp:239] Iteration 1730 (1.47726 iter/s, 6.76931s/10 iters), loss = 10.8168
I0523 22:14:14.274974 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8168 (* 1 = 10.8168 loss)
I0523 22:14:14.600097 11835 sgd_solver.cpp:112] Iteration 1730, lr = 0.1
I0523 22:14:21.550951 11835 solver.cpp:239] Iteration 1740 (1.37444 iter/s, 7.2757s/10 iters), loss = 10.4597
I0523 22:14:21.551213 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4597 (* 1 = 10.4597 loss)
I0523 22:14:21.551282 11835 sgd_solver.cpp:112] Iteration 1740, lr = 0.1
I0523 22:14:27.435648 11835 solver.cpp:239] Iteration 1750 (1.69946 iter/s, 5.88424s/10 iters), loss = 10.8112
I0523 22:14:27.435688 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8112 (* 1 = 10.8112 loss)
I0523 22:14:27.929132 11835 sgd_solver.cpp:112] Iteration 1750, lr = 0.1
I0523 22:14:34.100718 11835 solver.cpp:239] Iteration 1760 (1.50043 iter/s, 6.66477s/10 iters), loss = 10.7073
I0523 22:14:34.100752 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7073 (* 1 = 10.7073 loss)
I0523 22:14:34.100817 11835 sgd_solver.cpp:112] Iteration 1760, lr = 0.1
I0523 22:14:41.573781 11835 solver.cpp:239] Iteration 1770 (1.3382 iter/s, 7.47274s/10 iters), loss = 10.9481
I0523 22:14:41.573822 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9481 (* 1 = 10.9481 loss)
I0523 22:14:41.573954 11835 sgd_solver.cpp:112] Iteration 1770, lr = 0.1
I0523 22:14:47.292315 11835 solver.cpp:239] Iteration 1780 (1.74878 iter/s, 5.71827s/10 iters), loss = 10.1882
I0523 22:14:47.292352 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1882 (* 1 = 10.1882 loss)
I0523 22:14:47.327364 11835 sgd_solver.cpp:112] Iteration 1780, lr = 0.1
I0523 22:14:55.007777 11835 solver.cpp:239] Iteration 1790 (1.29616 iter/s, 7.71513s/10 iters), loss = 10.6124
I0523 22:14:55.007983 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6124 (* 1 = 10.6124 loss)
I0523 22:14:55.254959 11835 sgd_solver.cpp:112] Iteration 1790, lr = 0.1
I0523 22:15:02.476831 11835 solver.cpp:239] Iteration 1800 (1.33894 iter/s, 7.46858s/10 iters), loss = 10.7943
I0523 22:15:02.476873 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7943 (* 1 = 10.7943 loss)
I0523 22:15:02.476974 11835 sgd_solver.cpp:112] Iteration 1800, lr = 0.1
I0523 22:15:08.843116 11835 solver.cpp:239] Iteration 1810 (1.57085 iter/s, 6.366s/10 iters), loss = 10.7256
I0523 22:15:08.843160 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7256 (* 1 = 10.7256 loss)
I0523 22:15:08.843318 11835 sgd_solver.cpp:112] Iteration 1810, lr = 0.1
I0523 22:15:17.045722 11835 solver.cpp:239] Iteration 1820 (1.21918 iter/s, 8.20225s/10 iters), loss = 10.8477
I0523 22:15:17.045765 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8477 (* 1 = 10.8477 loss)
I0523 22:15:17.045908 11835 sgd_solver.cpp:112] Iteration 1820, lr = 0.1
I0523 22:15:22.767562 11835 solver.cpp:239] Iteration 1830 (1.74777 iter/s, 5.72156s/10 iters), loss = 10.5513
I0523 22:15:22.767603 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5513 (* 1 = 10.5513 loss)
I0523 22:15:23.687041 11835 sgd_solver.cpp:112] Iteration 1830, lr = 0.1
I0523 22:15:30.248044 11835 solver.cpp:239] Iteration 1840 (1.33687 iter/s, 7.48015s/10 iters), loss = 10.4639
I0523 22:15:30.248229 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4639 (* 1 = 10.4639 loss)
I0523 22:15:30.248266 11835 sgd_solver.cpp:112] Iteration 1840, lr = 0.1
I0523 22:15:36.607769 11835 solver.cpp:239] Iteration 1850 (1.57258 iter/s, 6.35898s/10 iters), loss = 10.3587
I0523 22:15:36.607806 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3587 (* 1 = 10.3587 loss)
I0523 22:15:36.607923 11835 sgd_solver.cpp:112] Iteration 1850, lr = 0.1
I0523 22:15:43.258371 11835 solver.cpp:239] Iteration 1860 (1.50369 iter/s, 6.6503s/10 iters), loss = 10.1913
I0523 22:15:43.258430 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1913 (* 1 = 10.1913 loss)
I0523 22:15:43.258744 11835 sgd_solver.cpp:112] Iteration 1860, lr = 0.1
I0523 22:15:48.956415 11835 solver.cpp:239] Iteration 1870 (1.75507 iter/s, 5.69777s/10 iters), loss = 11.2183
I0523 22:15:48.956454 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2183 (* 1 = 11.2183 loss)
I0523 22:15:48.956465 11835 sgd_solver.cpp:112] Iteration 1870, lr = 0.1
I0523 22:15:58.676873 11835 solver.cpp:239] Iteration 1880 (1.0288 iter/s, 9.72005s/10 iters), loss = 10.4525
I0523 22:15:58.676918 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4525 (* 1 = 10.4525 loss)
I0523 22:15:58.778484 11835 sgd_solver.cpp:112] Iteration 1880, lr = 0.1
I0523 22:16:07.622215 11835 solver.cpp:239] Iteration 1890 (1.11795 iter/s, 8.94496s/10 iters), loss = 9.87437
I0523 22:16:07.622375 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.87437 (* 1 = 9.87437 loss)
I0523 22:16:07.747143 11835 sgd_solver.cpp:112] Iteration 1890, lr = 0.1
I0523 22:16:14.300197 11835 solver.cpp:239] Iteration 1900 (1.49755 iter/s, 6.67757s/10 iters), loss = 11.3546
I0523 22:16:14.300232 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3546 (* 1 = 11.3546 loss)
I0523 22:16:14.401326 11835 sgd_solver.cpp:112] Iteration 1900, lr = 0.1
I0523 22:16:21.365540 11835 solver.cpp:239] Iteration 1910 (1.41542 iter/s, 7.06503s/10 iters), loss = 11.3395
I0523 22:16:21.365588 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3395 (* 1 = 11.3395 loss)
I0523 22:16:21.365639 11835 sgd_solver.cpp:112] Iteration 1910, lr = 0.1
I0523 22:16:27.337939 11835 solver.cpp:239] Iteration 1920 (1.67445 iter/s, 5.97212s/10 iters), loss = 10.1542
I0523 22:16:27.337980 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1542 (* 1 = 10.1542 loss)
I0523 22:16:27.365711 11835 sgd_solver.cpp:112] Iteration 1920, lr = 0.1
I0523 22:16:35.404464 11835 solver.cpp:239] Iteration 1930 (1.23974 iter/s, 8.06618s/10 iters), loss = 10.419
I0523 22:16:35.404503 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.419 (* 1 = 10.419 loss)
I0523 22:16:35.510442 11835 sgd_solver.cpp:112] Iteration 1930, lr = 0.1
I0523 22:16:42.575680 11835 solver.cpp:239] Iteration 1940 (1.39453 iter/s, 7.1709s/10 iters), loss = 11.5542
I0523 22:16:42.575796 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.5542 (* 1 = 11.5542 loss)
I0523 22:16:42.575858 11835 sgd_solver.cpp:112] Iteration 1940, lr = 0.1
I0523 22:16:49.590056 11835 solver.cpp:239] Iteration 1950 (1.42572 iter/s, 7.01399s/10 iters), loss = 10.8199
I0523 22:16:49.590101 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8199 (* 1 = 10.8199 loss)
I0523 22:16:50.244204 11835 sgd_solver.cpp:112] Iteration 1950, lr = 0.1
I0523 22:16:56.823108 11835 solver.cpp:239] Iteration 1960 (1.3826 iter/s, 7.23273s/10 iters), loss = 10.9896
I0523 22:16:56.823153 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9896 (* 1 = 10.9896 loss)
I0523 22:16:56.823227 11835 sgd_solver.cpp:112] Iteration 1960, lr = 0.1
I0523 22:17:04.823798 11835 solver.cpp:239] Iteration 1970 (1.24995 iter/s, 8.00035s/10 iters), loss = 11.5966
I0523 22:17:04.823838 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.5966 (* 1 = 11.5966 loss)
I0523 22:17:04.824054 11835 sgd_solver.cpp:112] Iteration 1970, lr = 0.1
I0523 22:17:12.257988 11835 solver.cpp:239] Iteration 1980 (1.3452 iter/s, 7.43386s/10 iters), loss = 10.7061
I0523 22:17:12.258038 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7061 (* 1 = 10.7061 loss)
I0523 22:17:12.258311 11835 sgd_solver.cpp:112] Iteration 1980, lr = 0.1
I0523 22:17:18.577531 11835 solver.cpp:239] Iteration 1990 (1.58247 iter/s, 6.31925s/10 iters), loss = 10.8189
I0523 22:17:18.577761 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8189 (* 1 = 10.8189 loss)
I0523 22:17:18.577919 11835 sgd_solver.cpp:112] Iteration 1990, lr = 0.1
I0523 22:17:24.618475 11835 solver.cpp:239] Iteration 2000 (1.65549 iter/s, 6.0405s/10 iters), loss = 10.9096
I0523 22:17:24.618518 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9096 (* 1 = 10.9096 loss)
I0523 22:17:24.618530 11835 sgd_solver.cpp:112] Iteration 2000, lr = 0.1
I0523 22:17:32.717172 11835 solver.cpp:239] Iteration 2010 (1.23483 iter/s, 8.09826s/10 iters), loss = 11.0689
I0523 22:17:32.717226 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0689 (* 1 = 11.0689 loss)
I0523 22:17:32.717629 11835 sgd_solver.cpp:112] Iteration 2010, lr = 0.1
I0523 22:17:40.787029 11835 solver.cpp:239] Iteration 2020 (1.23924 iter/s, 8.06949s/10 iters), loss = 10.1421
I0523 22:17:40.787077 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1421 (* 1 = 10.1421 loss)
I0523 22:17:40.787621 11835 sgd_solver.cpp:112] Iteration 2020, lr = 0.1
I0523 22:17:47.375470 11835 solver.cpp:239] Iteration 2030 (1.51788 iter/s, 6.58814s/10 iters), loss = 10.6806
I0523 22:17:47.375507 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6806 (* 1 = 10.6806 loss)
I0523 22:17:47.375643 11835 sgd_solver.cpp:112] Iteration 2030, lr = 0.1
I0523 22:17:54.326617 11835 solver.cpp:239] Iteration 2040 (1.43868 iter/s, 6.95082s/10 iters), loss = 11.7046
I0523 22:17:54.326889 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.7046 (* 1 = 11.7046 loss)
I0523 22:17:54.326941 11835 sgd_solver.cpp:112] Iteration 2040, lr = 0.1
I0523 22:18:01.779444 11835 solver.cpp:239] Iteration 2050 (1.34198 iter/s, 7.4517s/10 iters), loss = 10.8353
I0523 22:18:01.779484 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8353 (* 1 = 10.8353 loss)
I0523 22:18:02.841137 11835 sgd_solver.cpp:112] Iteration 2050, lr = 0.1
I0523 22:18:11.613548 11835 solver.cpp:239] Iteration 2060 (1.01691 iter/s, 9.83368s/10 iters), loss = 11.1068
I0523 22:18:11.613590 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1068 (* 1 = 11.1068 loss)
I0523 22:18:11.738652 11835 sgd_solver.cpp:112] Iteration 2060, lr = 0.1
I0523 22:18:20.956240 11835 solver.cpp:239] Iteration 2070 (1.0704 iter/s, 9.34229s/10 iters), loss = 11.1022
I0523 22:18:20.956285 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1022 (* 1 = 11.1022 loss)
I0523 22:18:20.956671 11835 sgd_solver.cpp:112] Iteration 2070, lr = 0.1
I0523 22:18:26.591838 11835 solver.cpp:239] Iteration 2080 (1.77452 iter/s, 5.63532s/10 iters), loss = 10.6363
I0523 22:18:26.592113 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6363 (* 1 = 10.6363 loss)
I0523 22:18:26.592167 11835 sgd_solver.cpp:112] Iteration 2080, lr = 0.1
I0523 22:18:34.780872 11835 solver.cpp:239] Iteration 2090 (1.22155 iter/s, 8.18629s/10 iters), loss = 10.0999
I0523 22:18:34.780916 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0999 (* 1 = 10.0999 loss)
I0523 22:18:34.780928 11835 sgd_solver.cpp:112] Iteration 2090, lr = 0.1
I0523 22:18:40.539453 11835 solver.cpp:239] Iteration 2100 (1.73667 iter/s, 5.75813s/10 iters), loss = 10.7674
I0523 22:18:40.539486 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7674 (* 1 = 10.7674 loss)
I0523 22:18:40.539497 11835 sgd_solver.cpp:112] Iteration 2100, lr = 0.1
I0523 22:18:48.264876 11835 solver.cpp:239] Iteration 2110 (1.29448 iter/s, 7.72509s/10 iters), loss = 11.4438
I0523 22:18:48.264925 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4438 (* 1 = 11.4438 loss)
I0523 22:18:48.265147 11835 sgd_solver.cpp:112] Iteration 2110, lr = 0.1
I0523 22:18:54.207057 11835 solver.cpp:239] Iteration 2120 (1.68296 iter/s, 5.94191s/10 iters), loss = 11.0111
I0523 22:18:54.207092 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0111 (* 1 = 11.0111 loss)
I0523 22:18:54.207103 11835 sgd_solver.cpp:112] Iteration 2120, lr = 0.1
I0523 22:19:01.760392 11835 solver.cpp:239] Iteration 2130 (1.32398 iter/s, 7.55301s/10 iters), loss = 10.9135
I0523 22:19:01.760627 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9135 (* 1 = 10.9135 loss)
I0523 22:19:01.804157 11835 sgd_solver.cpp:112] Iteration 2130, lr = 0.1
I0523 22:19:10.977991 11835 solver.cpp:239] Iteration 2140 (1.08495 iter/s, 9.21704s/10 iters), loss = 10.8391
I0523 22:19:10.978036 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8391 (* 1 = 10.8391 loss)
I0523 22:19:11.589231 11835 sgd_solver.cpp:112] Iteration 2140, lr = 0.1
I0523 22:19:18.799847 11835 solver.cpp:239] Iteration 2150 (1.27853 iter/s, 7.82151s/10 iters), loss = 11.0821
I0523 22:19:18.799883 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0821 (* 1 = 11.0821 loss)
I0523 22:19:18.800026 11835 sgd_solver.cpp:112] Iteration 2150, lr = 0.1
I0523 22:19:24.988546 11835 solver.cpp:239] Iteration 2160 (1.61592 iter/s, 6.18841s/10 iters), loss = 11.1226
I0523 22:19:24.988598 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1226 (* 1 = 11.1226 loss)
I0523 22:19:24.989194 11835 sgd_solver.cpp:112] Iteration 2160, lr = 0.1
I0523 22:19:32.678755 11835 solver.cpp:239] Iteration 2170 (1.30041 iter/s, 7.68986s/10 iters), loss = 10.974
I0523 22:19:32.678993 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.974 (* 1 = 10.974 loss)
I0523 22:19:33.425169 11835 sgd_solver.cpp:112] Iteration 2170, lr = 0.1
I0523 22:19:40.108057 11835 solver.cpp:239] Iteration 2180 (1.34611 iter/s, 7.42881s/10 iters), loss = 10.8601
I0523 22:19:40.108103 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8601 (* 1 = 10.8601 loss)
I0523 22:19:40.108116 11835 sgd_solver.cpp:112] Iteration 2180, lr = 0.1
I0523 22:19:48.173413 11835 solver.cpp:239] Iteration 2190 (1.24026 iter/s, 8.06283s/10 iters), loss = 10.933
I0523 22:19:48.173454 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.933 (* 1 = 10.933 loss)
I0523 22:19:48.173660 11835 sgd_solver.cpp:112] Iteration 2190, lr = 0.1
I0523 22:19:54.984740 11835 solver.cpp:239] Iteration 2200 (1.46821 iter/s, 6.81102s/10 iters), loss = 11.2149
I0523 22:19:54.984782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2149 (* 1 = 11.2149 loss)
I0523 22:19:55.665622 11835 sgd_solver.cpp:112] Iteration 2200, lr = 0.1
I0523 22:20:01.699962 11835 solver.cpp:239] Iteration 2210 (1.48922 iter/s, 6.71492s/10 iters), loss = 10.5188
I0523 22:20:01.700003 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5188 (* 1 = 10.5188 loss)
I0523 22:20:01.700016 11835 sgd_solver.cpp:112] Iteration 2210, lr = 0.1
I0523 22:20:08.533560 11835 solver.cpp:239] Iteration 2220 (1.46348 iter/s, 6.83303s/10 iters), loss = 11.1135
I0523 22:20:08.533804 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1135 (* 1 = 11.1135 loss)
I0523 22:20:08.533860 11835 sgd_solver.cpp:112] Iteration 2220, lr = 0.1
I0523 22:20:16.515913 11835 solver.cpp:239] Iteration 2230 (1.25285 iter/s, 7.98183s/10 iters), loss = 10.9356
I0523 22:20:16.515955 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9356 (* 1 = 10.9356 loss)
I0523 22:20:16.516093 11835 sgd_solver.cpp:112] Iteration 2230, lr = 0.1
I0523 22:20:22.602033 11835 solver.cpp:239] Iteration 2240 (1.64316 iter/s, 6.08584s/10 iters), loss = 11.16
I0523 22:20:22.602073 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.16 (* 1 = 11.16 loss)
I0523 22:20:22.905005 11835 sgd_solver.cpp:112] Iteration 2240, lr = 0.1
I0523 22:20:28.741541 11835 solver.cpp:239] Iteration 2250 (1.62887 iter/s, 6.13923s/10 iters), loss = 11.2576
I0523 22:20:28.741582 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2576 (* 1 = 11.2576 loss)
I0523 22:20:28.742144 11835 sgd_solver.cpp:112] Iteration 2250, lr = 0.1
I0523 22:20:36.115248 11835 solver.cpp:239] Iteration 2260 (1.35623 iter/s, 7.37338s/10 iters), loss = 11.4651
I0523 22:20:36.115293 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4651 (* 1 = 11.4651 loss)
I0523 22:20:36.115546 11835 sgd_solver.cpp:112] Iteration 2260, lr = 0.1
I0523 22:20:41.972631 11835 solver.cpp:239] Iteration 2270 (1.70733 iter/s, 5.8571s/10 iters), loss = 11.1804
I0523 22:20:41.972887 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1804 (* 1 = 11.1804 loss)
I0523 22:20:41.972954 11835 sgd_solver.cpp:112] Iteration 2270, lr = 0.1
I0523 22:20:48.833462 11835 solver.cpp:239] Iteration 2280 (1.45765 iter/s, 6.86037s/10 iters), loss = 10.994
I0523 22:20:48.833504 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.994 (* 1 = 10.994 loss)
I0523 22:20:48.833608 11835 sgd_solver.cpp:112] Iteration 2280, lr = 0.1
I0523 22:20:56.251567 11835 solver.cpp:239] Iteration 2290 (1.34811 iter/s, 7.41778s/10 iters), loss = 11.0989
I0523 22:20:56.251610 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0989 (* 1 = 11.0989 loss)
I0523 22:20:56.251885 11835 sgd_solver.cpp:112] Iteration 2290, lr = 0.1
I0523 22:21:05.058634 11835 solver.cpp:239] Iteration 2300 (1.1355 iter/s, 8.80668s/10 iters), loss = 10.7118
I0523 22:21:05.058724 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7118 (* 1 = 10.7118 loss)
I0523 22:21:05.058926 11835 sgd_solver.cpp:112] Iteration 2300, lr = 0.1
I0523 22:21:12.942804 11835 solver.cpp:239] Iteration 2310 (1.26842 iter/s, 7.88381s/10 iters), loss = 11.0515
I0523 22:21:12.943073 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0515 (* 1 = 11.0515 loss)
I0523 22:21:12.991814 11835 sgd_solver.cpp:112] Iteration 2310, lr = 0.1
I0523 22:21:21.536646 11835 solver.cpp:239] Iteration 2320 (1.1637 iter/s, 8.59328s/10 iters), loss = 10.2524
I0523 22:21:21.536689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2524 (* 1 = 10.2524 loss)
I0523 22:21:22.151698 11835 sgd_solver.cpp:112] Iteration 2320, lr = 0.1
I0523 22:21:30.074627 11835 solver.cpp:239] Iteration 2330 (1.17129 iter/s, 8.53761s/10 iters), loss = 10.9355
I0523 22:21:30.074663 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9355 (* 1 = 10.9355 loss)
I0523 22:21:30.074915 11835 sgd_solver.cpp:112] Iteration 2330, lr = 0.1
I0523 22:21:36.843878 11835 solver.cpp:239] Iteration 2340 (1.47733 iter/s, 6.76895s/10 iters), loss = 11.8425
I0523 22:21:36.843920 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.8425 (* 1 = 11.8425 loss)
I0523 22:21:36.844048 11835 sgd_solver.cpp:112] Iteration 2340, lr = 0.1
I0523 22:21:42.695874 11835 solver.cpp:239] Iteration 2350 (1.7089 iter/s, 5.85173s/10 iters), loss = 11.5735
I0523 22:21:42.695919 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.5735 (* 1 = 11.5735 loss)
I0523 22:21:42.696202 11835 sgd_solver.cpp:112] Iteration 2350, lr = 0.1
I0523 22:21:49.207862 11835 solver.cpp:239] Iteration 2360 (1.5357 iter/s, 6.51169s/10 iters), loss = 10.6056
I0523 22:21:49.208108 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6056 (* 1 = 10.6056 loss)
I0523 22:21:49.208156 11835 sgd_solver.cpp:112] Iteration 2360, lr = 0.1
I0523 22:21:55.464865 11835 solver.cpp:239] Iteration 2370 (1.59833 iter/s, 6.25652s/10 iters), loss = 11.4475
I0523 22:21:55.464901 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4475 (* 1 = 11.4475 loss)
I0523 22:21:55.746793 11835 sgd_solver.cpp:112] Iteration 2370, lr = 0.1
I0523 22:22:01.479570 11835 solver.cpp:239] Iteration 2380 (1.66267 iter/s, 6.01443s/10 iters), loss = 10.9007
I0523 22:22:01.479620 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9007 (* 1 = 10.9007 loss)
I0523 22:22:01.479689 11835 sgd_solver.cpp:112] Iteration 2380, lr = 0.1
I0523 22:22:07.181329 11835 solver.cpp:239] Iteration 2390 (1.75393 iter/s, 5.70149s/10 iters), loss = 10.5558
I0523 22:22:07.181366 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5558 (* 1 = 10.5558 loss)
I0523 22:22:07.181484 11835 sgd_solver.cpp:112] Iteration 2390, lr = 0.1
I0523 22:22:12.803395 11835 solver.cpp:239] Iteration 2400 (1.77879 iter/s, 5.62181s/10 iters), loss = 11.0247
I0523 22:22:12.803437 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0247 (* 1 = 11.0247 loss)
I0523 22:22:12.803448 11835 sgd_solver.cpp:112] Iteration 2400, lr = 0.1
I0523 22:22:20.193197 11835 solver.cpp:239] Iteration 2410 (1.35329 iter/s, 7.38939s/10 iters), loss = 11.0535
I0523 22:22:20.193275 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0535 (* 1 = 11.0535 loss)
I0523 22:22:20.193346 11835 sgd_solver.cpp:112] Iteration 2410, lr = 0.1
I0523 22:22:27.311259 11835 solver.cpp:239] Iteration 2420 (1.40495 iter/s, 7.11769s/10 iters), loss = 10.9194
I0523 22:22:27.311309 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9194 (* 1 = 10.9194 loss)
I0523 22:22:28.583572 11835 sgd_solver.cpp:112] Iteration 2420, lr = 0.1
I0523 22:22:37.320013 11835 solver.cpp:239] Iteration 2430 (0.999168 iter/s, 10.0083s/10 iters), loss = 11.3017
I0523 22:22:37.320062 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3017 (* 1 = 11.3017 loss)
I0523 22:22:37.320258 11835 sgd_solver.cpp:112] Iteration 2430, lr = 0.1
I0523 22:22:43.071539 11835 solver.cpp:239] Iteration 2440 (1.73875 iter/s, 5.75126s/10 iters), loss = 11.5046
I0523 22:22:43.071575 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.5046 (* 1 = 11.5046 loss)
I0523 22:22:43.071596 11835 sgd_solver.cpp:112] Iteration 2440, lr = 0.1
I0523 22:22:49.720489 11835 solver.cpp:239] Iteration 2450 (1.50406 iter/s, 6.64865s/10 iters), loss = 10.9782
I0523 22:22:49.720535 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9782 (* 1 = 10.9782 loss)
I0523 22:22:49.837504 11835 sgd_solver.cpp:112] Iteration 2450, lr = 0.1
I0523 22:22:55.838248 11835 solver.cpp:239] Iteration 2460 (1.63466 iter/s, 6.11748s/10 iters), loss = 11.0633
I0523 22:22:55.838410 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0633 (* 1 = 11.0633 loss)
I0523 22:22:56.360826 11835 sgd_solver.cpp:112] Iteration 2460, lr = 0.1
I0523 22:23:02.239811 11835 solver.cpp:239] Iteration 2470 (1.56222 iter/s, 6.40115s/10 iters), loss = 10.4846
I0523 22:23:02.239854 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4846 (* 1 = 10.4846 loss)
I0523 22:23:02.239868 11835 sgd_solver.cpp:112] Iteration 2470, lr = 0.1
I0523 22:23:08.291981 11835 solver.cpp:239] Iteration 2480 (1.65238 iter/s, 6.05189s/10 iters), loss = 10.8495
I0523 22:23:08.292021 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8495 (* 1 = 10.8495 loss)
I0523 22:23:08.292284 11835 sgd_solver.cpp:112] Iteration 2480, lr = 0.1
I0523 22:23:15.004885 11835 solver.cpp:239] Iteration 2490 (1.48974 iter/s, 6.7126s/10 iters), loss = 11.0813
I0523 22:23:15.004931 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0813 (* 1 = 11.0813 loss)
I0523 22:23:15.680933 11835 sgd_solver.cpp:112] Iteration 2490, lr = 0.1
I0523 22:23:23.611747 11835 solver.cpp:239] Iteration 2500 (1.16191 iter/s, 8.60649s/10 iters), loss = 11.152
I0523 22:23:23.611788 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.152 (* 1 = 11.152 loss)
I0523 22:23:23.611809 11835 sgd_solver.cpp:112] Iteration 2500, lr = 0.1
I0523 22:23:30.393610 11835 solver.cpp:239] Iteration 2510 (1.47459 iter/s, 6.78156s/10 iters), loss = 11.1271
I0523 22:23:30.393736 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1271 (* 1 = 11.1271 loss)
I0523 22:23:30.393910 11835 sgd_solver.cpp:112] Iteration 2510, lr = 0.1
I0523 22:23:37.272377 11835 solver.cpp:239] Iteration 2520 (1.45383 iter/s, 6.87838s/10 iters), loss = 10.3296
I0523 22:23:37.272419 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3296 (* 1 = 10.3296 loss)
I0523 22:23:37.443588 11835 sgd_solver.cpp:112] Iteration 2520, lr = 0.1
I0523 22:23:43.090123 11835 solver.cpp:239] Iteration 2530 (1.71896 iter/s, 5.81748s/10 iters), loss = 10.9698
I0523 22:23:43.090163 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9698 (* 1 = 10.9698 loss)
I0523 22:23:43.090574 11835 sgd_solver.cpp:112] Iteration 2530, lr = 0.1
I0523 22:23:50.961460 11835 solver.cpp:239] Iteration 2540 (1.27049 iter/s, 7.87099s/10 iters), loss = 10.8228
I0523 22:23:50.961519 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8228 (* 1 = 10.8228 loss)
I0523 22:23:50.961705 11835 sgd_solver.cpp:112] Iteration 2540, lr = 0.1
I0523 22:23:56.540616 11835 solver.cpp:239] Iteration 2550 (1.79247 iter/s, 5.57889s/10 iters), loss = 11.0935
I0523 22:23:56.540654 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0935 (* 1 = 11.0935 loss)
I0523 22:23:56.540668 11835 sgd_solver.cpp:112] Iteration 2550, lr = 0.1
I0523 22:24:04.786201 11835 solver.cpp:239] Iteration 2560 (1.21283 iter/s, 8.24516s/10 iters), loss = 11.0167
I0523 22:24:04.786476 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0167 (* 1 = 11.0167 loss)
I0523 22:24:04.786517 11835 sgd_solver.cpp:112] Iteration 2560, lr = 0.1
I0523 22:24:11.258981 11835 solver.cpp:239] Iteration 2570 (1.54505 iter/s, 6.47229s/10 iters), loss = 11.5544
I0523 22:24:11.259024 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.5544 (* 1 = 11.5544 loss)
I0523 22:24:11.514010 11835 sgd_solver.cpp:112] Iteration 2570, lr = 0.1
I0523 22:24:17.879060 11835 solver.cpp:239] Iteration 2580 (1.51063 iter/s, 6.61977s/10 iters), loss = 10.86
I0523 22:24:17.879109 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.86 (* 1 = 10.86 loss)
I0523 22:24:17.879144 11835 sgd_solver.cpp:112] Iteration 2580, lr = 0.1
I0523 22:24:25.651341 11835 solver.cpp:239] Iteration 2590 (1.28668 iter/s, 7.77194s/10 iters), loss = 11.3136
I0523 22:24:25.651384 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3136 (* 1 = 11.3136 loss)
I0523 22:24:25.651497 11835 sgd_solver.cpp:112] Iteration 2590, lr = 0.1
I0523 22:24:33.319847 11835 solver.cpp:239] Iteration 2600 (1.30409 iter/s, 7.66817s/10 iters), loss = 11.0496
I0523 22:24:33.319893 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0496 (* 1 = 11.0496 loss)
I0523 22:24:33.691949 11835 sgd_solver.cpp:112] Iteration 2600, lr = 0.1
I0523 22:24:42.275414 11835 solver.cpp:239] Iteration 2610 (1.11667 iter/s, 8.95518s/10 iters), loss = 11.3756
I0523 22:24:42.275629 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3756 (* 1 = 11.3756 loss)
I0523 22:24:42.275671 11835 sgd_solver.cpp:112] Iteration 2610, lr = 0.1
I0523 22:24:50.919708 11835 solver.cpp:239] Iteration 2620 (1.157 iter/s, 8.64303s/10 iters), loss = 11.0616
I0523 22:24:50.919757 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0616 (* 1 = 11.0616 loss)
I0523 22:24:50.919890 11835 sgd_solver.cpp:112] Iteration 2620, lr = 0.1
I0523 22:24:56.803730 11835 solver.cpp:239] Iteration 2630 (1.6996 iter/s, 5.88375s/10 iters), loss = 10.4419
I0523 22:24:56.803771 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4419 (* 1 = 10.4419 loss)
I0523 22:24:56.985440 11835 sgd_solver.cpp:112] Iteration 2630, lr = 0.1
I0523 22:25:05.460829 11835 solver.cpp:239] Iteration 2640 (1.15517 iter/s, 8.65672s/10 iters), loss = 10.5278
I0523 22:25:05.460880 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5278 (* 1 = 10.5278 loss)
I0523 22:25:05.461022 11835 sgd_solver.cpp:112] Iteration 2640, lr = 0.1
I0523 22:25:11.514603 11835 solver.cpp:239] Iteration 2650 (1.65194 iter/s, 6.05349s/10 iters), loss = 10.725
I0523 22:25:11.514639 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.725 (* 1 = 10.725 loss)
I0523 22:25:11.514652 11835 sgd_solver.cpp:112] Iteration 2650, lr = 0.1
I0523 22:25:21.219171 11835 solver.cpp:239] Iteration 2660 (1.03072 iter/s, 9.70194s/10 iters), loss = 10.3664
I0523 22:25:21.219310 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3664 (* 1 = 10.3664 loss)
I0523 22:25:21.378852 11835 sgd_solver.cpp:112] Iteration 2660, lr = 0.1
I0523 22:25:28.665134 11835 solver.cpp:239] Iteration 2670 (1.34309 iter/s, 7.44554s/10 iters), loss = 10.6117
I0523 22:25:28.665176 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6117 (* 1 = 10.6117 loss)
I0523 22:25:28.803803 11835 sgd_solver.cpp:112] Iteration 2670, lr = 0.1
I0523 22:25:34.695183 11835 solver.cpp:239] Iteration 2680 (1.65844 iter/s, 6.02977s/10 iters), loss = 11.3186
I0523 22:25:34.695225 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3186 (* 1 = 11.3186 loss)
I0523 22:25:34.695420 11835 sgd_solver.cpp:112] Iteration 2680, lr = 0.1
I0523 22:25:41.233481 11835 solver.cpp:239] Iteration 2690 (1.52952 iter/s, 6.538s/10 iters), loss = 10.7294
I0523 22:25:41.233528 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7294 (* 1 = 10.7294 loss)
I0523 22:25:41.234074 11835 sgd_solver.cpp:112] Iteration 2690, lr = 0.1
I0523 22:25:46.959038 11835 solver.cpp:239] Iteration 2700 (1.74664 iter/s, 5.72529s/10 iters), loss = 10.6905
I0523 22:25:46.959082 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6905 (* 1 = 10.6905 loss)
I0523 22:25:46.959098 11835 sgd_solver.cpp:112] Iteration 2700, lr = 0.1
I0523 22:25:56.445628 11835 solver.cpp:239] Iteration 2710 (1.05416 iter/s, 9.48619s/10 iters), loss = 10.669
I0523 22:25:56.445775 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.669 (* 1 = 10.669 loss)
I0523 22:25:56.445904 11835 sgd_solver.cpp:112] Iteration 2710, lr = 0.1
I0523 22:26:03.994189 11835 solver.cpp:239] Iteration 2720 (1.32483 iter/s, 7.54813s/10 iters), loss = 10.4363
I0523 22:26:03.994235 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4363 (* 1 = 10.4363 loss)
I0523 22:26:03.994516 11835 sgd_solver.cpp:112] Iteration 2720, lr = 0.1
I0523 22:26:10.935681 11835 solver.cpp:239] Iteration 2730 (1.44068 iter/s, 6.94117s/10 iters), loss = 10.7574
I0523 22:26:10.935725 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7574 (* 1 = 10.7574 loss)
I0523 22:26:10.935781 11835 sgd_solver.cpp:112] Iteration 2730, lr = 0.1
I0523 22:26:16.653812 11835 solver.cpp:239] Iteration 2740 (1.7489 iter/s, 5.71786s/10 iters), loss = 11.0843
I0523 22:26:16.653856 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0843 (* 1 = 11.0843 loss)
I0523 22:26:16.653867 11835 sgd_solver.cpp:112] Iteration 2740, lr = 0.1
I0523 22:26:23.859612 11835 solver.cpp:239] Iteration 2750 (1.38786 iter/s, 7.20534s/10 iters), loss = 10.8148
I0523 22:26:23.859654 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8148 (* 1 = 10.8148 loss)
I0523 22:26:24.542155 11835 sgd_solver.cpp:112] Iteration 2750, lr = 0.1
I0523 22:26:33.012320 11835 solver.cpp:239] Iteration 2760 (1.09262 iter/s, 9.15232s/10 iters), loss = 11.4131
I0523 22:26:33.012539 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4131 (* 1 = 11.4131 loss)
I0523 22:26:33.012617 11835 sgd_solver.cpp:112] Iteration 2760, lr = 0.1
I0523 22:26:38.909328 11835 solver.cpp:239] Iteration 2770 (1.6959 iter/s, 5.89659s/10 iters), loss = 10.9225
I0523 22:26:38.909371 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9225 (* 1 = 10.9225 loss)
I0523 22:26:38.909382 11835 sgd_solver.cpp:112] Iteration 2770, lr = 0.1
I0523 22:26:45.659096 11835 solver.cpp:239] Iteration 2780 (1.4816 iter/s, 6.74947s/10 iters), loss = 10.6576
I0523 22:26:45.659138 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6576 (* 1 = 10.6576 loss)
I0523 22:26:45.659421 11835 sgd_solver.cpp:112] Iteration 2780, lr = 0.1
I0523 22:26:51.625259 11835 solver.cpp:239] Iteration 2790 (1.6762 iter/s, 5.96589s/10 iters), loss = 10.9139
I0523 22:26:51.625306 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9139 (* 1 = 10.9139 loss)
I0523 22:26:51.625542 11835 sgd_solver.cpp:112] Iteration 2790, lr = 0.1
I0523 22:26:59.027652 11835 solver.cpp:239] Iteration 2800 (1.35098 iter/s, 7.40206s/10 iters), loss = 10.8651
I0523 22:26:59.027701 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8651 (* 1 = 10.8651 loss)
I0523 22:26:59.027900 11835 sgd_solver.cpp:112] Iteration 2800, lr = 0.1
I0523 22:27:05.435088 11835 solver.cpp:239] Iteration 2810 (1.56076 iter/s, 6.40714s/10 iters), loss = 10.6928
I0523 22:27:05.435180 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6928 (* 1 = 10.6928 loss)
I0523 22:27:05.444865 11835 sgd_solver.cpp:112] Iteration 2810, lr = 0.1
I0523 22:27:12.008076 11835 solver.cpp:239] Iteration 2820 (1.52146 iter/s, 6.57265s/10 iters), loss = 10.3326
I0523 22:27:12.008122 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3326 (* 1 = 10.3326 loss)
I0523 22:27:12.271934 11835 sgd_solver.cpp:112] Iteration 2820, lr = 0.1
I0523 22:27:19.103860 11835 solver.cpp:239] Iteration 2830 (1.40935 iter/s, 7.09546s/10 iters), loss = 10.4268
I0523 22:27:19.103899 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4268 (* 1 = 10.4268 loss)
I0523 22:27:19.603763 11835 sgd_solver.cpp:112] Iteration 2830, lr = 0.1
I0523 22:27:26.386111 11835 solver.cpp:239] Iteration 2840 (1.37326 iter/s, 7.28193s/10 iters), loss = 10.3997
I0523 22:27:26.386158 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3997 (* 1 = 10.3997 loss)
I0523 22:27:26.420034 11835 sgd_solver.cpp:112] Iteration 2840, lr = 0.1
I0523 22:27:33.478528 11835 solver.cpp:239] Iteration 2850 (1.41002 iter/s, 7.0921s/10 iters), loss = 10.4514
I0523 22:27:33.478570 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4514 (* 1 = 10.4514 loss)
I0523 22:27:33.478651 11835 sgd_solver.cpp:112] Iteration 2850, lr = 0.1
I0523 22:27:41.510244 11835 solver.cpp:239] Iteration 2860 (1.24512 iter/s, 8.03137s/10 iters), loss = 11.6543
I0523 22:27:41.510517 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.6543 (* 1 = 11.6543 loss)
I0523 22:27:41.553086 11835 sgd_solver.cpp:112] Iteration 2860, lr = 0.1
I0523 22:27:48.976788 11835 solver.cpp:239] Iteration 2870 (1.33977 iter/s, 7.46396s/10 iters), loss = 10.4638
I0523 22:27:48.976840 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4638 (* 1 = 10.4638 loss)
I0523 22:27:48.976891 11835 sgd_solver.cpp:112] Iteration 2870, lr = 0.1
I0523 22:27:54.810082 11835 solver.cpp:239] Iteration 2880 (1.71438 iter/s, 5.83302s/10 iters), loss = 10.4062
I0523 22:27:54.810125 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4062 (* 1 = 10.4062 loss)
I0523 22:27:54.810137 11835 sgd_solver.cpp:112] Iteration 2880, lr = 0.1
I0523 22:28:00.561183 11835 solver.cpp:239] Iteration 2890 (1.73906 iter/s, 5.75023s/10 iters), loss = 10.9755
I0523 22:28:00.561221 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9755 (* 1 = 10.9755 loss)
I0523 22:28:00.561234 11835 sgd_solver.cpp:112] Iteration 2890, lr = 0.1
I0523 22:28:06.488601 11835 solver.cpp:239] Iteration 2900 (1.68715 iter/s, 5.92715s/10 iters), loss = 10.1686
I0523 22:28:06.488642 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1686 (* 1 = 10.1686 loss)
I0523 22:28:06.488654 11835 sgd_solver.cpp:112] Iteration 2900, lr = 0.1
I0523 22:28:14.795624 11835 solver.cpp:239] Iteration 2910 (1.20391 iter/s, 8.30629s/10 iters), loss = 10.4119
I0523 22:28:14.795843 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4119 (* 1 = 10.4119 loss)
I0523 22:28:15.604414 11835 sgd_solver.cpp:112] Iteration 2910, lr = 0.1
I0523 22:28:22.461797 11835 solver.cpp:239] Iteration 2920 (1.30452 iter/s, 7.66568s/10 iters), loss = 10.8943
I0523 22:28:22.461869 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8943 (* 1 = 10.8943 loss)
I0523 22:28:22.462049 11835 sgd_solver.cpp:112] Iteration 2920, lr = 0.1
I0523 22:28:28.686643 11835 solver.cpp:239] Iteration 2930 (1.60654 iter/s, 6.22455s/10 iters), loss = 11.0517
I0523 22:28:28.686681 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0517 (* 1 = 11.0517 loss)
I0523 22:28:28.686692 11835 sgd_solver.cpp:112] Iteration 2930, lr = 0.1
I0523 22:28:35.357513 11835 solver.cpp:239] Iteration 2940 (1.49912 iter/s, 6.67057s/10 iters), loss = 11.2986
I0523 22:28:35.357559 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2986 (* 1 = 11.2986 loss)
I0523 22:28:35.357571 11835 sgd_solver.cpp:112] Iteration 2940, lr = 0.1
I0523 22:28:41.412215 11835 solver.cpp:239] Iteration 2950 (1.65199 iter/s, 6.05331s/10 iters), loss = 10.6683
I0523 22:28:41.412256 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6683 (* 1 = 10.6683 loss)
I0523 22:28:41.513134 11835 sgd_solver.cpp:112] Iteration 2950, lr = 0.1
I0523 22:28:48.393972 11835 solver.cpp:239] Iteration 2960 (1.43237 iter/s, 6.98145s/10 iters), loss = 10.3814
I0523 22:28:48.394215 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3814 (* 1 = 10.3814 loss)
I0523 22:28:48.394263 11835 sgd_solver.cpp:112] Iteration 2960, lr = 0.1
I0523 22:28:54.328469 11835 solver.cpp:239] Iteration 2970 (1.68545 iter/s, 5.93312s/10 iters), loss = 10.8331
I0523 22:28:54.328506 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8331 (* 1 = 10.8331 loss)
I0523 22:28:54.360220 11835 sgd_solver.cpp:112] Iteration 2970, lr = 0.1
I0523 22:29:01.927620 11835 solver.cpp:239] Iteration 2980 (1.31599 iter/s, 7.59882s/10 iters), loss = 10.6983
I0523 22:29:01.927661 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6983 (* 1 = 10.6983 loss)
I0523 22:29:01.953622 11835 sgd_solver.cpp:112] Iteration 2980, lr = 0.1
I0523 22:29:09.813628 11835 solver.cpp:239] Iteration 2990 (1.26812 iter/s, 7.88567s/10 iters), loss = 11.1232
I0523 22:29:09.813673 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1232 (* 1 = 11.1232 loss)
I0523 22:29:09.813920 11835 sgd_solver.cpp:112] Iteration 2990, lr = 0.1
I0523 22:29:15.697895 11835 solver.cpp:239] Iteration 3000 (1.69952 iter/s, 5.884s/10 iters), loss = 10.5945
I0523 22:29:15.697935 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5945 (* 1 = 10.5945 loss)
I0523 22:29:15.697947 11835 sgd_solver.cpp:112] Iteration 3000, lr = 0.1
I0523 22:29:22.262045 11835 solver.cpp:239] Iteration 3010 (1.52349 iter/s, 6.56386s/10 iters), loss = 10.68
I0523 22:29:22.262192 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.68 (* 1 = 10.68 loss)
I0523 22:29:22.262207 11835 sgd_solver.cpp:112] Iteration 3010, lr = 0.1
I0523 22:29:29.547889 11835 solver.cpp:239] Iteration 3020 (1.37279 iter/s, 7.28441s/10 iters), loss = 10.6553
I0523 22:29:29.547925 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6553 (* 1 = 10.6553 loss)
I0523 22:29:29.548069 11835 sgd_solver.cpp:112] Iteration 3020, lr = 0.1
I0523 22:29:38.169572 11835 solver.cpp:239] Iteration 3030 (1.15992 iter/s, 8.62131s/10 iters), loss = 10.1807
I0523 22:29:38.169617 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1807 (* 1 = 10.1807 loss)
I0523 22:29:38.169751 11835 sgd_solver.cpp:112] Iteration 3030, lr = 0.1
I0523 22:29:44.445765 11835 solver.cpp:239] Iteration 3040 (1.5934 iter/s, 6.2759s/10 iters), loss = 10.1884
I0523 22:29:44.445807 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1884 (* 1 = 10.1884 loss)
I0523 22:29:44.445821 11835 sgd_solver.cpp:112] Iteration 3040, lr = 0.1
I0523 22:29:51.459597 11835 solver.cpp:239] Iteration 3050 (1.42582 iter/s, 7.01352s/10 iters), loss = 10.4415
I0523 22:29:51.459637 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4415 (* 1 = 10.4415 loss)
I0523 22:29:51.459648 11835 sgd_solver.cpp:112] Iteration 3050, lr = 0.1
I0523 22:29:58.010414 11835 solver.cpp:239] Iteration 3060 (1.5266 iter/s, 6.55052s/10 iters), loss = 10.5239
I0523 22:29:58.010511 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5239 (* 1 = 10.5239 loss)
I0523 22:29:58.010718 11835 sgd_solver.cpp:112] Iteration 3060, lr = 0.1
I0523 22:30:06.756345 11835 solver.cpp:239] Iteration 3070 (1.14345 iter/s, 8.7455s/10 iters), loss = 10.7954
I0523 22:30:06.756383 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7954 (* 1 = 10.7954 loss)
I0523 22:30:07.524278 11835 sgd_solver.cpp:112] Iteration 3070, lr = 0.1
I0523 22:30:17.029261 11835 solver.cpp:239] Iteration 3080 (0.973474 iter/s, 10.2725s/10 iters), loss = 10.957
I0523 22:30:17.029299 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.957 (* 1 = 10.957 loss)
I0523 22:30:17.072552 11835 sgd_solver.cpp:112] Iteration 3080, lr = 0.1
I0523 22:30:26.098991 11835 solver.cpp:239] Iteration 3090 (1.10262 iter/s, 9.06934s/10 iters), loss = 10.5838
I0523 22:30:26.099035 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5838 (* 1 = 10.5838 loss)
I0523 22:30:26.099139 11835 sgd_solver.cpp:112] Iteration 3090, lr = 0.1
I0523 22:30:32.037569 11835 solver.cpp:239] Iteration 3100 (1.68398 iter/s, 5.93831s/10 iters), loss = 11.0783
I0523 22:30:32.037818 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0783 (* 1 = 11.0783 loss)
I0523 22:30:32.037876 11835 sgd_solver.cpp:112] Iteration 3100, lr = 0.1
I0523 22:30:39.056097 11835 solver.cpp:239] Iteration 3110 (1.42492 iter/s, 7.01794s/10 iters), loss = 10.1058
I0523 22:30:39.056139 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1058 (* 1 = 10.1058 loss)
I0523 22:30:39.056185 11835 sgd_solver.cpp:112] Iteration 3110, lr = 0.1
I0523 22:30:47.282248 11835 solver.cpp:239] Iteration 3120 (1.21569 iter/s, 8.22579s/10 iters), loss = 10.8034
I0523 22:30:47.282284 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8034 (* 1 = 10.8034 loss)
I0523 22:30:47.311782 11835 sgd_solver.cpp:112] Iteration 3120, lr = 0.1
I0523 22:30:54.598194 11835 solver.cpp:239] Iteration 3130 (1.36694 iter/s, 7.31562s/10 iters), loss = 10.4629
I0523 22:30:54.598250 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4629 (* 1 = 10.4629 loss)
I0523 22:30:54.858376 11835 sgd_solver.cpp:112] Iteration 3130, lr = 0.1
I0523 22:31:01.582056 11835 solver.cpp:239] Iteration 3140 (1.43194 iter/s, 6.98353s/10 iters), loss = 10.8076
I0523 22:31:01.582103 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8076 (* 1 = 10.8076 loss)
I0523 22:31:01.633364 11835 sgd_solver.cpp:112] Iteration 3140, lr = 0.1
I0523 22:31:09.639086 11835 solver.cpp:239] Iteration 3150 (1.24121 iter/s, 8.05667s/10 iters), loss = 11.0845
I0523 22:31:09.639329 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0845 (* 1 = 11.0845 loss)
I0523 22:31:09.639359 11835 sgd_solver.cpp:112] Iteration 3150, lr = 0.1
I0523 22:31:17.545892 11835 solver.cpp:239] Iteration 3160 (1.26482 iter/s, 7.90627s/10 iters), loss = 10.5809
I0523 22:31:17.545939 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5809 (* 1 = 10.5809 loss)
I0523 22:31:17.546207 11835 sgd_solver.cpp:112] Iteration 3160, lr = 0.1
I0523 22:31:27.447147 11835 solver.cpp:239] Iteration 3170 (1.01002 iter/s, 9.90083s/10 iters), loss = 10.2065
I0523 22:31:27.447190 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2065 (* 1 = 10.2065 loss)
I0523 22:31:27.706521 11835 sgd_solver.cpp:112] Iteration 3170, lr = 0.1
I0523 22:31:35.607084 11835 solver.cpp:239] Iteration 3180 (1.22555 iter/s, 8.15958s/10 iters), loss = 10.8962
I0523 22:31:35.607123 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8962 (* 1 = 10.8962 loss)
I0523 22:31:35.754716 11835 sgd_solver.cpp:112] Iteration 3180, lr = 0.1
I0523 22:31:43.174101 11835 solver.cpp:239] Iteration 3190 (1.32158 iter/s, 7.56669s/10 iters), loss = 10.6103
I0523 22:31:43.174219 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6103 (* 1 = 10.6103 loss)
I0523 22:31:43.200261 11835 sgd_solver.cpp:112] Iteration 3190, lr = 0.1
I0523 22:31:53.185896 11835 solver.cpp:239] Iteration 3200 (0.998872 iter/s, 10.0113s/10 iters), loss = 11.0821
I0523 22:31:53.185940 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0821 (* 1 = 11.0821 loss)
I0523 22:31:53.323875 11835 sgd_solver.cpp:112] Iteration 3200, lr = 0.1
I0523 22:31:59.147581 11835 solver.cpp:239] Iteration 3210 (1.67745 iter/s, 5.96141s/10 iters), loss = 10.3848
I0523 22:31:59.147615 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3848 (* 1 = 10.3848 loss)
I0523 22:31:59.147627 11835 sgd_solver.cpp:112] Iteration 3210, lr = 0.1
I0523 22:32:06.105684 11835 solver.cpp:239] Iteration 3220 (1.43724 iter/s, 6.9578s/10 iters), loss = 10.4138
I0523 22:32:06.105725 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4138 (* 1 = 10.4138 loss)
I0523 22:32:06.105736 11835 sgd_solver.cpp:112] Iteration 3220, lr = 0.1
I0523 22:32:14.029423 11835 solver.cpp:239] Iteration 3230 (1.26212 iter/s, 7.92321s/10 iters), loss = 10.2582
I0523 22:32:14.029646 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2582 (* 1 = 10.2582 loss)
I0523 22:32:14.228780 11835 sgd_solver.cpp:112] Iteration 3230, lr = 0.1
I0523 22:32:19.924603 11835 solver.cpp:239] Iteration 3240 (1.69642 iter/s, 5.89477s/10 iters), loss = 10.8783
I0523 22:32:19.924638 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8783 (* 1 = 10.8783 loss)
I0523 22:32:19.925705 11835 sgd_solver.cpp:112] Iteration 3240, lr = 0.1
I0523 22:32:25.741014 11835 solver.cpp:239] Iteration 3250 (1.71935 iter/s, 5.81614s/10 iters), loss = 10.9811
I0523 22:32:25.741055 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9811 (* 1 = 10.9811 loss)
I0523 22:32:25.741067 11835 sgd_solver.cpp:112] Iteration 3250, lr = 0.1
I0523 22:32:32.692139 11835 solver.cpp:239] Iteration 3260 (1.43891 iter/s, 6.9497s/10 iters), loss = 10.2587
I0523 22:32:32.692176 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2587 (* 1 = 10.2587 loss)
I0523 22:32:32.692322 11835 sgd_solver.cpp:112] Iteration 3260, lr = 0.1
I0523 22:32:39.475495 11835 solver.cpp:239] Iteration 3270 (1.47426 iter/s, 6.78306s/10 iters), loss = 10.5943
I0523 22:32:39.475538 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5943 (* 1 = 10.5943 loss)
I0523 22:32:39.533124 11835 sgd_solver.cpp:112] Iteration 3270, lr = 0.1
I0523 22:32:47.696748 11835 solver.cpp:239] Iteration 3280 (1.21641 iter/s, 8.2209s/10 iters), loss = 10.7347
I0523 22:32:47.696983 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7347 (* 1 = 10.7347 loss)
I0523 22:32:47.697021 11835 sgd_solver.cpp:112] Iteration 3280, lr = 0.1
I0523 22:32:55.502677 11835 solver.cpp:239] Iteration 3290 (1.2812 iter/s, 7.80517s/10 iters), loss = 10.5277
I0523 22:32:55.502758 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5277 (* 1 = 10.5277 loss)
I0523 22:32:55.502965 11835 sgd_solver.cpp:112] Iteration 3290, lr = 0.1
I0523 22:33:01.308040 11835 solver.cpp:239] Iteration 3300 (1.72264 iter/s, 5.80506s/10 iters), loss = 10.387
I0523 22:33:01.308084 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.387 (* 1 = 10.387 loss)
I0523 22:33:01.308099 11835 sgd_solver.cpp:112] Iteration 3300, lr = 0.1
I0523 22:33:07.362851 11835 solver.cpp:239] Iteration 3310 (1.65196 iter/s, 6.05342s/10 iters), loss = 10.5997
I0523 22:33:07.362901 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5997 (* 1 = 10.5997 loss)
I0523 22:33:08.021317 11835 sgd_solver.cpp:112] Iteration 3310, lr = 0.1
I0523 22:33:14.636071 11835 solver.cpp:239] Iteration 3320 (1.37497 iter/s, 7.27289s/10 iters), loss = 10.1726
I0523 22:33:14.636107 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1726 (* 1 = 10.1726 loss)
I0523 22:33:14.943841 11835 sgd_solver.cpp:112] Iteration 3320, lr = 0.1
I0523 22:33:22.280889 11835 solver.cpp:239] Iteration 3330 (1.30813 iter/s, 7.64448s/10 iters), loss = 10.4329
I0523 22:33:22.281136 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4329 (* 1 = 10.4329 loss)
I0523 22:33:22.281184 11835 sgd_solver.cpp:112] Iteration 3330, lr = 0.1
I0523 22:33:28.213506 11835 solver.cpp:239] Iteration 3340 (1.68635 iter/s, 5.92997s/10 iters), loss = 9.92237
I0523 22:33:28.213541 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.92237 (* 1 = 9.92237 loss)
I0523 22:33:28.360795 11835 sgd_solver.cpp:112] Iteration 3340, lr = 0.1
I0523 22:33:35.675227 11835 solver.cpp:239] Iteration 3350 (1.34023 iter/s, 7.4614s/10 iters), loss = 10.3034
I0523 22:33:35.675272 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3034 (* 1 = 10.3034 loss)
I0523 22:33:35.677129 11835 sgd_solver.cpp:112] Iteration 3350, lr = 0.1
I0523 22:33:41.747344 11835 solver.cpp:239] Iteration 3360 (1.64695 iter/s, 6.07183s/10 iters), loss = 11.1025
I0523 22:33:41.747388 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1025 (* 1 = 11.1025 loss)
I0523 22:33:41.747462 11835 sgd_solver.cpp:112] Iteration 3360, lr = 0.1
I0523 22:33:48.521054 11835 solver.cpp:239] Iteration 3370 (1.47637 iter/s, 6.77339s/10 iters), loss = 10.2689
I0523 22:33:48.521103 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2689 (* 1 = 10.2689 loss)
I0523 22:33:48.521117 11835 sgd_solver.cpp:112] Iteration 3370, lr = 0.1
I0523 22:33:57.352711 11835 solver.cpp:239] Iteration 3380 (1.13236 iter/s, 8.83114s/10 iters), loss = 11.1586
I0523 22:33:57.352794 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1586 (* 1 = 11.1586 loss)
I0523 22:33:57.677364 11835 sgd_solver.cpp:112] Iteration 3380, lr = 0.1
I0523 22:34:05.740039 11835 solver.cpp:239] Iteration 3390 (1.19233 iter/s, 8.38692s/10 iters), loss = 11.4421
I0523 22:34:05.740084 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4421 (* 1 = 11.4421 loss)
I0523 22:34:05.740097 11835 sgd_solver.cpp:112] Iteration 3390, lr = 0.1
I0523 22:34:12.460973 11835 solver.cpp:239] Iteration 3400 (1.4882 iter/s, 6.71952s/10 iters), loss = 10.9255
I0523 22:34:12.461012 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9255 (* 1 = 10.9255 loss)
I0523 22:34:12.570672 11835 sgd_solver.cpp:112] Iteration 3400, lr = 0.1
I0523 22:34:18.767283 11835 solver.cpp:239] Iteration 3410 (1.58578 iter/s, 6.30603s/10 iters), loss = 9.95963
I0523 22:34:18.767323 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.95963 (* 1 = 9.95963 loss)
I0523 22:34:18.767333 11835 sgd_solver.cpp:112] Iteration 3410, lr = 0.1
I0523 22:34:27.883885 11835 solver.cpp:239] Iteration 3420 (1.09695 iter/s, 9.11621s/10 iters), loss = 10.7426
I0523 22:34:27.884143 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7426 (* 1 = 10.7426 loss)
I0523 22:34:27.884191 11835 sgd_solver.cpp:112] Iteration 3420, lr = 0.1
I0523 22:34:33.624514 11835 solver.cpp:239] Iteration 3430 (1.74211 iter/s, 5.74017s/10 iters), loss = 10.6453
I0523 22:34:33.624562 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6453 (* 1 = 10.6453 loss)
I0523 22:34:33.624573 11835 sgd_solver.cpp:112] Iteration 3430, lr = 0.1
I0523 22:34:39.490602 11835 solver.cpp:239] Iteration 3440 (1.70498 iter/s, 5.86518s/10 iters), loss = 10.8585
I0523 22:34:39.490640 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8585 (* 1 = 10.8585 loss)
I0523 22:34:40.744336 11835 sgd_solver.cpp:112] Iteration 3440, lr = 0.1
I0523 22:34:48.218132 11835 solver.cpp:239] Iteration 3450 (1.14585 iter/s, 8.72715s/10 iters), loss = 10.5183
I0523 22:34:48.218180 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5183 (* 1 = 10.5183 loss)
I0523 22:34:48.218444 11835 sgd_solver.cpp:112] Iteration 3450, lr = 0.1
I0523 22:34:54.076447 11835 solver.cpp:239] Iteration 3460 (1.70706 iter/s, 5.85804s/10 iters), loss = 11.1077
I0523 22:34:54.076478 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1077 (* 1 = 11.1077 loss)
I0523 22:34:54.793273 11835 sgd_solver.cpp:112] Iteration 3460, lr = 0.1
I0523 22:35:01.477679 11835 solver.cpp:239] Iteration 3470 (1.35118 iter/s, 7.40091s/10 iters), loss = 10.9793
I0523 22:35:01.477771 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9793 (* 1 = 10.9793 loss)
I0523 22:35:01.477785 11835 sgd_solver.cpp:112] Iteration 3470, lr = 0.1
I0523 22:35:07.540825 11835 solver.cpp:239] Iteration 3480 (1.64942 iter/s, 6.06274s/10 iters), loss = 10.1526
I0523 22:35:07.540863 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1526 (* 1 = 10.1526 loss)
I0523 22:35:07.540987 11835 sgd_solver.cpp:112] Iteration 3480, lr = 0.1
I0523 22:35:14.025737 11835 solver.cpp:239] Iteration 3490 (1.54211 iter/s, 6.48462s/10 iters), loss = 10.4815
I0523 22:35:14.025780 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4815 (* 1 = 10.4815 loss)
I0523 22:35:14.511570 11835 sgd_solver.cpp:112] Iteration 3490, lr = 0.1
I0523 22:35:20.311635 11835 solver.cpp:239] Iteration 3500 (1.59093 iter/s, 6.28561s/10 iters), loss = 10.2406
I0523 22:35:20.311674 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2406 (* 1 = 10.2406 loss)
I0523 22:35:20.311686 11835 sgd_solver.cpp:112] Iteration 3500, lr = 0.1
I0523 22:35:26.836114 11835 solver.cpp:239] Iteration 3510 (1.53276 iter/s, 6.52419s/10 iters), loss = 10.6513
I0523 22:35:26.836158 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6513 (* 1 = 10.6513 loss)
I0523 22:35:26.836423 11835 sgd_solver.cpp:112] Iteration 3510, lr = 0.1
I0523 22:35:35.350062 11835 solver.cpp:239] Iteration 3520 (1.17459 iter/s, 8.51357s/10 iters), loss = 10.3522
I0523 22:35:35.350289 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3522 (* 1 = 10.3522 loss)
I0523 22:35:35.909068 11835 sgd_solver.cpp:112] Iteration 3520, lr = 0.1
I0523 22:35:42.237123 11835 solver.cpp:239] Iteration 3530 (1.4521 iter/s, 6.8866s/10 iters), loss = 11.2002
I0523 22:35:42.237169 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2002 (* 1 = 11.2002 loss)
I0523 22:35:42.237584 11835 sgd_solver.cpp:112] Iteration 3530, lr = 0.1
I0523 22:35:49.790944 11835 solver.cpp:239] Iteration 3540 (1.32389 iter/s, 7.55349s/10 iters), loss = 10.6674
I0523 22:35:49.790985 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6674 (* 1 = 10.6674 loss)
I0523 22:35:49.791050 11835 sgd_solver.cpp:112] Iteration 3540, lr = 0.1
I0523 22:35:58.723642 11835 solver.cpp:239] Iteration 3550 (1.11953 iter/s, 8.93231s/10 iters), loss = 10.1987
I0523 22:35:58.723693 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1987 (* 1 = 10.1987 loss)
I0523 22:36:00.093629 11835 sgd_solver.cpp:112] Iteration 3550, lr = 0.1
I0523 22:36:06.334647 11835 solver.cpp:239] Iteration 3560 (1.31395 iter/s, 7.61067s/10 iters), loss = 10.9902
I0523 22:36:06.334807 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9902 (* 1 = 10.9902 loss)
I0523 22:36:06.356407 11835 sgd_solver.cpp:112] Iteration 3560, lr = 0.1
I0523 22:36:14.300320 11835 solver.cpp:239] Iteration 3570 (1.25546 iter/s, 7.96521s/10 iters), loss = 10.209
I0523 22:36:14.300370 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.209 (* 1 = 10.209 loss)
I0523 22:36:14.300570 11835 sgd_solver.cpp:112] Iteration 3570, lr = 0.1
I0523 22:36:20.804050 11835 solver.cpp:239] Iteration 3580 (1.53765 iter/s, 6.50342s/10 iters), loss = 10.3954
I0523 22:36:20.804101 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3954 (* 1 = 10.3954 loss)
I0523 22:36:20.804309 11835 sgd_solver.cpp:112] Iteration 3580, lr = 0.1
I0523 22:36:26.621497 11835 solver.cpp:239] Iteration 3590 (1.71905 iter/s, 5.81717s/10 iters), loss = 11.2435
I0523 22:36:26.621543 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2435 (* 1 = 11.2435 loss)
I0523 22:36:26.621556 11835 sgd_solver.cpp:112] Iteration 3590, lr = 0.1
I0523 22:36:33.513422 11835 solver.cpp:239] Iteration 3600 (1.45105 iter/s, 6.89155s/10 iters), loss = 10.8909
I0523 22:36:33.513463 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8909 (* 1 = 10.8909 loss)
I0523 22:36:33.513695 11835 sgd_solver.cpp:112] Iteration 3600, lr = 0.1
I0523 22:36:39.202339 11835 solver.cpp:239] Iteration 3610 (1.75788 iter/s, 5.68866s/10 iters), loss = 10.1008
I0523 22:36:39.202553 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1008 (* 1 = 10.1008 loss)
I0523 22:36:39.202599 11835 sgd_solver.cpp:112] Iteration 3610, lr = 0.1
I0523 22:36:47.565392 11835 solver.cpp:239] Iteration 3620 (1.19581 iter/s, 8.36252s/10 iters), loss = 11.2443
I0523 22:36:47.565433 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2443 (* 1 = 11.2443 loss)
I0523 22:36:47.756274 11835 sgd_solver.cpp:112] Iteration 3620, lr = 0.1
I0523 22:36:53.283571 11835 solver.cpp:239] Iteration 3630 (1.74889 iter/s, 5.71792s/10 iters), loss = 11.0015
I0523 22:36:53.283613 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0015 (* 1 = 11.0015 loss)
I0523 22:36:54.116158 11835 sgd_solver.cpp:112] Iteration 3630, lr = 0.1
I0523 22:37:01.037477 11835 solver.cpp:239] Iteration 3640 (1.28973 iter/s, 7.75356s/10 iters), loss = 10.7296
I0523 22:37:01.037528 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7296 (* 1 = 10.7296 loss)
I0523 22:37:01.037820 11835 sgd_solver.cpp:112] Iteration 3640, lr = 0.1
I0523 22:37:08.826742 11835 solver.cpp:239] Iteration 3650 (1.28388 iter/s, 7.78892s/10 iters), loss = 11.0365
I0523 22:37:08.826782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0365 (* 1 = 11.0365 loss)
I0523 22:37:08.844178 11835 sgd_solver.cpp:112] Iteration 3650, lr = 0.1
I0523 22:37:14.819833 11835 solver.cpp:239] Iteration 3660 (1.66867 iter/s, 5.99281s/10 iters), loss = 10.4922
I0523 22:37:14.820083 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4922 (* 1 = 10.4922 loss)
I0523 22:37:15.223273 11835 sgd_solver.cpp:112] Iteration 3660, lr = 0.1
I0523 22:37:21.564560 11835 solver.cpp:239] Iteration 3670 (1.48274 iter/s, 6.74426s/10 iters), loss = 10.8742
I0523 22:37:21.564610 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8742 (* 1 = 10.8742 loss)
I0523 22:37:21.564649 11835 sgd_solver.cpp:112] Iteration 3670, lr = 0.1
I0523 22:37:27.520449 11835 solver.cpp:239] Iteration 3680 (1.67909 iter/s, 5.95561s/10 iters), loss = 11.0256
I0523 22:37:27.520494 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0256 (* 1 = 11.0256 loss)
I0523 22:37:27.520613 11835 sgd_solver.cpp:112] Iteration 3680, lr = 0.1
I0523 22:37:33.551463 11835 solver.cpp:239] Iteration 3690 (1.65817 iter/s, 6.03073s/10 iters), loss = 10.3071
I0523 22:37:33.551511 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3071 (* 1 = 10.3071 loss)
I0523 22:37:33.551575 11835 sgd_solver.cpp:112] Iteration 3690, lr = 0.1
I0523 22:37:40.450333 11835 solver.cpp:239] Iteration 3700 (1.44958 iter/s, 6.89856s/10 iters), loss = 11.2912
I0523 22:37:40.450372 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2912 (* 1 = 11.2912 loss)
I0523 22:37:40.450384 11835 sgd_solver.cpp:112] Iteration 3700, lr = 0.1
I0523 22:37:47.876917 11835 solver.cpp:239] Iteration 3710 (1.34678 iter/s, 7.42514s/10 iters), loss = 10.7933
I0523 22:37:47.877118 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7933 (* 1 = 10.7933 loss)
I0523 22:37:48.668324 11835 sgd_solver.cpp:112] Iteration 3710, lr = 0.1
I0523 22:37:56.859378 11835 solver.cpp:239] Iteration 3720 (1.11335 iter/s, 8.98193s/10 iters), loss = 10.7122
I0523 22:37:56.859422 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7122 (* 1 = 10.7122 loss)
I0523 22:37:56.859557 11835 sgd_solver.cpp:112] Iteration 3720, lr = 0.1
I0523 22:38:03.835319 11835 solver.cpp:239] Iteration 3730 (1.43356 iter/s, 6.97563s/10 iters), loss = 9.81023
I0523 22:38:03.835355 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.81023 (* 1 = 9.81023 loss)
I0523 22:38:03.835563 11835 sgd_solver.cpp:112] Iteration 3730, lr = 0.1
I0523 22:38:10.315130 11835 solver.cpp:239] Iteration 3740 (1.54333 iter/s, 6.47951s/10 iters), loss = 11.1572
I0523 22:38:10.315172 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1572 (* 1 = 11.1572 loss)
I0523 22:38:10.315183 11835 sgd_solver.cpp:112] Iteration 3740, lr = 0.1
I0523 22:38:17.911814 11835 solver.cpp:239] Iteration 3750 (1.31644 iter/s, 7.59627s/10 iters), loss = 11.2599
I0523 22:38:17.911955 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2599 (* 1 = 11.2599 loss)
I0523 22:38:17.911993 11835 sgd_solver.cpp:112] Iteration 3750, lr = 0.1
I0523 22:38:23.801959 11835 solver.cpp:239] Iteration 3760 (1.69786 iter/s, 5.88978s/10 iters), loss = 10.866
I0523 22:38:23.802000 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.866 (* 1 = 10.866 loss)
I0523 22:38:23.888801 11835 sgd_solver.cpp:112] Iteration 3760, lr = 0.1
I0523 22:38:32.678261 11835 solver.cpp:239] Iteration 3770 (1.12664 iter/s, 8.87592s/10 iters), loss = 10.3557
I0523 22:38:32.678308 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3557 (* 1 = 10.3557 loss)
I0523 22:38:33.406973 11835 sgd_solver.cpp:112] Iteration 3770, lr = 0.1
I0523 22:38:39.327355 11835 solver.cpp:239] Iteration 3780 (1.50403 iter/s, 6.64879s/10 iters), loss = 10.8672
I0523 22:38:39.327401 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8672 (* 1 = 10.8672 loss)
I0523 22:38:39.327507 11835 sgd_solver.cpp:112] Iteration 3780, lr = 0.1
I0523 22:38:45.233741 11835 solver.cpp:239] Iteration 3790 (1.69316 iter/s, 5.90611s/10 iters), loss = 10.5597
I0523 22:38:45.233783 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5597 (* 1 = 10.5597 loss)
I0523 22:38:45.234232 11835 sgd_solver.cpp:112] Iteration 3790, lr = 0.1
I0523 22:38:53.106551 11835 solver.cpp:239] Iteration 3800 (1.27025 iter/s, 7.87247s/10 iters), loss = 10.8069
I0523 22:38:53.106752 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8069 (* 1 = 10.8069 loss)
I0523 22:38:53.106961 11835 sgd_solver.cpp:112] Iteration 3800, lr = 0.1
I0523 22:38:59.959614 11835 solver.cpp:239] Iteration 3810 (1.4593 iter/s, 6.85259s/10 iters), loss = 10.6109
I0523 22:38:59.959650 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6109 (* 1 = 10.6109 loss)
I0523 22:39:00.704814 11835 sgd_solver.cpp:112] Iteration 3810, lr = 0.1
I0523 22:39:08.505511 11835 solver.cpp:239] Iteration 3820 (1.1702 iter/s, 8.54553s/10 iters), loss = 10.5403
I0523 22:39:08.505561 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5403 (* 1 = 10.5403 loss)
I0523 22:39:08.505686 11835 sgd_solver.cpp:112] Iteration 3820, lr = 0.1
I0523 22:39:14.351791 11835 solver.cpp:239] Iteration 3830 (1.71057 iter/s, 5.84601s/10 iters), loss = 10.5712
I0523 22:39:14.351827 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5712 (* 1 = 10.5712 loss)
I0523 22:39:14.351907 11835 sgd_solver.cpp:112] Iteration 3830, lr = 0.1
I0523 22:39:21.122473 11835 solver.cpp:239] Iteration 3840 (1.47702 iter/s, 6.77037s/10 iters), loss = 11.0383
I0523 22:39:21.122522 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0383 (* 1 = 11.0383 loss)
I0523 22:39:21.122588 11835 sgd_solver.cpp:112] Iteration 3840, lr = 0.1
I0523 22:39:31.100785 11835 solver.cpp:239] Iteration 3850 (1.00222 iter/s, 9.97788s/10 iters), loss = 10.2755
I0523 22:39:31.101030 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2755 (* 1 = 10.2755 loss)
I0523 22:39:31.101248 11835 sgd_solver.cpp:112] Iteration 3850, lr = 0.1
I0523 22:39:38.182423 11835 solver.cpp:239] Iteration 3860 (1.4122 iter/s, 7.08116s/10 iters), loss = 11.0965
I0523 22:39:38.182461 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0965 (* 1 = 11.0965 loss)
I0523 22:39:38.182471 11835 sgd_solver.cpp:112] Iteration 3860, lr = 0.1
I0523 22:39:45.281553 11835 solver.cpp:239] Iteration 3870 (1.40871 iter/s, 7.09871s/10 iters), loss = 10.6578
I0523 22:39:45.281601 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6578 (* 1 = 10.6578 loss)
I0523 22:39:46.237144 11835 sgd_solver.cpp:112] Iteration 3870, lr = 0.1
I0523 22:39:53.155005 11835 solver.cpp:239] Iteration 3880 (1.27015 iter/s, 7.8731s/10 iters), loss = 10.9131
I0523 22:39:53.155046 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9131 (* 1 = 10.9131 loss)
I0523 22:39:53.155355 11835 sgd_solver.cpp:112] Iteration 3880, lr = 0.1
I0523 22:39:59.062479 11835 solver.cpp:239] Iteration 3890 (1.69285 iter/s, 5.9072s/10 iters), loss = 10.614
I0523 22:39:59.062515 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.614 (* 1 = 10.614 loss)
I0523 22:39:59.528013 11835 sgd_solver.cpp:112] Iteration 3890, lr = 0.1
I0523 22:40:06.156280 11835 solver.cpp:239] Iteration 3900 (1.40974 iter/s, 7.09349s/10 iters), loss = 10.3438
I0523 22:40:06.156522 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3438 (* 1 = 10.3438 loss)
I0523 22:40:06.691715 11835 sgd_solver.cpp:112] Iteration 3900, lr = 0.1
I0523 22:40:13.077942 11835 solver.cpp:239] Iteration 3910 (1.44484 iter/s, 6.92119s/10 iters), loss = 10.7242
I0523 22:40:13.077993 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7242 (* 1 = 10.7242 loss)
I0523 22:40:13.079936 11835 sgd_solver.cpp:112] Iteration 3910, lr = 0.1
I0523 22:40:19.443789 11835 solver.cpp:239] Iteration 3920 (1.57096 iter/s, 6.36555s/10 iters), loss = 10.2725
I0523 22:40:19.443840 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2725 (* 1 = 10.2725 loss)
I0523 22:40:19.444181 11835 sgd_solver.cpp:112] Iteration 3920, lr = 0.1
I0523 22:40:27.524194 11835 solver.cpp:239] Iteration 3930 (1.23762 iter/s, 8.08005s/10 iters), loss = 10.5149
I0523 22:40:27.524240 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5149 (* 1 = 10.5149 loss)
I0523 22:40:27.524420 11835 sgd_solver.cpp:112] Iteration 3930, lr = 0.1
I0523 22:40:33.460950 11835 solver.cpp:239] Iteration 3940 (1.6845 iter/s, 5.93648s/10 iters), loss = 11.3043
I0523 22:40:33.461002 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3043 (* 1 = 11.3043 loss)
I0523 22:40:33.461251 11835 sgd_solver.cpp:112] Iteration 3940, lr = 0.1
I0523 22:40:39.767632 11835 solver.cpp:239] Iteration 3950 (1.58569 iter/s, 6.30638s/10 iters), loss = 11.3405
I0523 22:40:39.767875 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3405 (* 1 = 11.3405 loss)
I0523 22:40:39.767957 11835 sgd_solver.cpp:112] Iteration 3950, lr = 0.1
I0523 22:40:45.565692 11835 solver.cpp:239] Iteration 3960 (1.72485 iter/s, 5.79762s/10 iters), loss = 10.7899
I0523 22:40:45.565737 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7899 (* 1 = 10.7899 loss)
I0523 22:40:45.565979 11835 sgd_solver.cpp:112] Iteration 3960, lr = 0.1
I0523 22:40:51.470841 11835 solver.cpp:239] Iteration 3970 (1.69352 iter/s, 5.90487s/10 iters), loss = 10.9227
I0523 22:40:51.470883 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9227 (* 1 = 10.9227 loss)
I0523 22:40:51.470989 11835 sgd_solver.cpp:112] Iteration 3970, lr = 0.1
I0523 22:40:57.274158 11835 solver.cpp:239] Iteration 3980 (1.72323 iter/s, 5.80305s/10 iters), loss = 10.6764
I0523 22:40:57.274194 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6764 (* 1 = 10.6764 loss)
I0523 22:40:57.274206 11835 sgd_solver.cpp:112] Iteration 3980, lr = 0.1
I0523 22:41:06.314743 11835 solver.cpp:239] Iteration 3990 (1.10618 iter/s, 9.04013s/10 iters), loss = 11.0247
I0523 22:41:06.314785 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0247 (* 1 = 11.0247 loss)
I0523 22:41:06.315018 11835 sgd_solver.cpp:112] Iteration 3990, lr = 0.1
I0523 22:41:14.896914 11835 solver.cpp:239] Iteration 4000 (1.16526 iter/s, 8.58179s/10 iters), loss = 11.163
I0523 22:41:14.897168 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.163 (* 1 = 11.163 loss)
I0523 22:41:15.496867 11835 sgd_solver.cpp:112] Iteration 4000, lr = 0.1
I0523 22:41:22.870291 11835 solver.cpp:239] Iteration 4010 (1.25426 iter/s, 7.97286s/10 iters), loss = 11.0855
I0523 22:41:22.870333 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0855 (* 1 = 11.0855 loss)
I0523 22:41:22.870345 11835 sgd_solver.cpp:112] Iteration 4010, lr = 0.1
I0523 22:41:29.998989 11835 solver.cpp:239] Iteration 4020 (1.40287 iter/s, 7.12825s/10 iters), loss = 11.0057
I0523 22:41:29.999027 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0057 (* 1 = 11.0057 loss)
I0523 22:41:29.999171 11835 sgd_solver.cpp:112] Iteration 4020, lr = 0.1
I0523 22:41:38.556334 11835 solver.cpp:239] Iteration 4030 (1.16864 iter/s, 8.55697s/10 iters), loss = 10.4503
I0523 22:41:38.556378 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4503 (* 1 = 10.4503 loss)
I0523 22:41:38.590369 11835 sgd_solver.cpp:112] Iteration 4030, lr = 0.1
I0523 22:41:46.449144 11835 solver.cpp:239] Iteration 4040 (1.26703 iter/s, 7.89246s/10 iters), loss = 11.0203
I0523 22:41:46.449395 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0203 (* 1 = 11.0203 loss)
I0523 22:41:47.086653 11835 sgd_solver.cpp:112] Iteration 4040, lr = 0.1
I0523 22:41:52.791662 11835 solver.cpp:239] Iteration 4050 (1.57678 iter/s, 6.34206s/10 iters), loss = 10.9105
I0523 22:41:52.791715 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9105 (* 1 = 10.9105 loss)
I0523 22:41:52.791726 11835 sgd_solver.cpp:112] Iteration 4050, lr = 0.1
I0523 22:41:59.943051 11835 solver.cpp:239] Iteration 4060 (1.39882 iter/s, 7.14886s/10 iters), loss = 9.8689
I0523 22:41:59.943097 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.8689 (* 1 = 9.8689 loss)
I0523 22:41:59.943176 11835 sgd_solver.cpp:112] Iteration 4060, lr = 0.1
I0523 22:42:07.289067 11835 solver.cpp:239] Iteration 4070 (1.36134 iter/s, 7.34568s/10 iters), loss = 10.8018
I0523 22:42:07.289116 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8018 (* 1 = 10.8018 loss)
I0523 22:42:07.289386 11835 sgd_solver.cpp:112] Iteration 4070, lr = 0.1
I0523 22:42:13.576390 11835 solver.cpp:239] Iteration 4080 (1.59058 iter/s, 6.28702s/10 iters), loss = 10.5758
I0523 22:42:13.576434 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5758 (* 1 = 10.5758 loss)
I0523 22:42:13.576563 11835 sgd_solver.cpp:112] Iteration 4080, lr = 0.1
I0523 22:42:21.867451 11835 solver.cpp:239] Iteration 4090 (1.20617 iter/s, 8.2907s/10 iters), loss = 11.1242
I0523 22:42:21.867755 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1242 (* 1 = 11.1242 loss)
I0523 22:42:21.867810 11835 sgd_solver.cpp:112] Iteration 4090, lr = 0.1
I0523 22:42:28.225389 11835 solver.cpp:239] Iteration 4100 (1.57296 iter/s, 6.35744s/10 iters), loss = 10.2952
I0523 22:42:28.225426 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2952 (* 1 = 10.2952 loss)
I0523 22:42:28.231796 11835 sgd_solver.cpp:112] Iteration 4100, lr = 0.1
I0523 22:42:35.341456 11835 solver.cpp:239] Iteration 4110 (1.40533 iter/s, 7.11574s/10 iters), loss = 11.1707
I0523 22:42:35.341500 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1707 (* 1 = 11.1707 loss)
I0523 22:42:35.371145 11835 sgd_solver.cpp:112] Iteration 4110, lr = 0.1
I0523 22:42:41.640521 11835 solver.cpp:239] Iteration 4120 (1.58761 iter/s, 6.29878s/10 iters), loss = 10.4322
I0523 22:42:41.640563 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4322 (* 1 = 10.4322 loss)
I0523 22:42:41.960675 11835 sgd_solver.cpp:112] Iteration 4120, lr = 0.1
I0523 22:42:48.671604 11835 solver.cpp:239] Iteration 4130 (1.42232 iter/s, 7.03077s/10 iters), loss = 10.5617
I0523 22:42:48.671648 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5617 (* 1 = 10.5617 loss)
I0523 22:42:48.671916 11835 sgd_solver.cpp:112] Iteration 4130, lr = 0.1
I0523 22:42:57.214649 11835 solver.cpp:239] Iteration 4140 (1.17059 iter/s, 8.54268s/10 iters), loss = 10.0636
I0523 22:42:57.214905 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0636 (* 1 = 10.0636 loss)
I0523 22:42:57.227648 11835 sgd_solver.cpp:112] Iteration 4140, lr = 0.1
I0523 22:43:03.983626 11835 solver.cpp:239] Iteration 4150 (1.47743 iter/s, 6.7685s/10 iters), loss = 11.1234
I0523 22:43:03.983669 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1234 (* 1 = 11.1234 loss)
I0523 22:43:04.618434 11835 sgd_solver.cpp:112] Iteration 4150, lr = 0.1
I0523 22:43:10.578778 11835 solver.cpp:239] Iteration 4160 (1.51634 iter/s, 6.59484s/10 iters), loss = 10.666
I0523 22:43:10.578835 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.666 (* 1 = 10.666 loss)
I0523 22:43:10.579035 11835 sgd_solver.cpp:112] Iteration 4160, lr = 0.1
I0523 22:43:16.503376 11835 solver.cpp:239] Iteration 4170 (1.68796 iter/s, 5.92432s/10 iters), loss = 10.9149
I0523 22:43:16.503412 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9149 (* 1 = 10.9149 loss)
I0523 22:43:16.515560 11835 sgd_solver.cpp:112] Iteration 4170, lr = 0.1
I0523 22:43:23.513211 11835 solver.cpp:239] Iteration 4180 (1.42663 iter/s, 7.00953s/10 iters), loss = 10.971
I0523 22:43:23.513254 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.971 (* 1 = 10.971 loss)
I0523 22:43:23.513267 11835 sgd_solver.cpp:112] Iteration 4180, lr = 0.1
I0523 22:43:29.958590 11835 solver.cpp:239] Iteration 4190 (1.55159 iter/s, 6.44502s/10 iters), loss = 10.3804
I0523 22:43:29.958886 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3804 (* 1 = 10.3804 loss)
I0523 22:43:29.958936 11835 sgd_solver.cpp:112] Iteration 4190, lr = 0.1
I0523 22:43:38.011809 11835 solver.cpp:239] Iteration 4200 (1.24184 iter/s, 8.05259s/10 iters), loss = 10.7989
I0523 22:43:38.011852 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7989 (* 1 = 10.7989 loss)
I0523 22:43:38.011955 11835 sgd_solver.cpp:112] Iteration 4200, lr = 0.1
I0523 22:43:44.870970 11835 solver.cpp:239] Iteration 4210 (1.45797 iter/s, 6.85886s/10 iters), loss = 10.6062
I0523 22:43:44.871011 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6062 (* 1 = 10.6062 loss)
I0523 22:43:44.871024 11835 sgd_solver.cpp:112] Iteration 4210, lr = 0.1
I0523 22:43:51.308943 11835 solver.cpp:239] Iteration 4220 (1.55336 iter/s, 6.43766s/10 iters), loss = 10.2468
I0523 22:43:51.308986 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2468 (* 1 = 10.2468 loss)
I0523 22:43:51.309000 11835 sgd_solver.cpp:112] Iteration 4220, lr = 0.1
I0523 22:43:58.492882 11835 solver.cpp:239] Iteration 4230 (1.39206 iter/s, 7.18362s/10 iters), loss = 10.2799
I0523 22:43:58.492926 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2799 (* 1 = 10.2799 loss)
I0523 22:43:58.492952 11835 sgd_solver.cpp:112] Iteration 4230, lr = 0.1
I0523 22:44:06.604624 11835 solver.cpp:239] Iteration 4240 (1.23284 iter/s, 8.11138s/10 iters), loss = 9.53759
I0523 22:44:06.604810 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.53759 (* 1 = 9.53759 loss)
I0523 22:44:06.604832 11835 sgd_solver.cpp:112] Iteration 4240, lr = 0.1
I0523 22:44:14.126803 11835 solver.cpp:239] Iteration 4250 (1.32968 iter/s, 7.5206s/10 iters), loss = 10.668
I0523 22:44:14.126844 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.668 (* 1 = 10.668 loss)
I0523 22:44:14.256055 11835 sgd_solver.cpp:112] Iteration 4250, lr = 0.1
I0523 22:44:20.815279 11835 solver.cpp:239] Iteration 4260 (1.49518 iter/s, 6.68818s/10 iters), loss = 10.9622
I0523 22:44:20.815323 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9622 (* 1 = 10.9622 loss)
I0523 22:44:21.153369 11835 sgd_solver.cpp:112] Iteration 4260, lr = 0.1
I0523 22:44:27.743124 11835 solver.cpp:239] Iteration 4270 (1.44351 iter/s, 6.92754s/10 iters), loss = 10.8021
I0523 22:44:27.743162 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8021 (* 1 = 10.8021 loss)
I0523 22:44:27.954632 11835 sgd_solver.cpp:112] Iteration 4270, lr = 0.1
I0523 22:44:34.462790 11835 solver.cpp:239] Iteration 4280 (1.48824 iter/s, 6.71936s/10 iters), loss = 10.1978
I0523 22:44:34.462832 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1978 (* 1 = 10.1978 loss)
I0523 22:44:34.462965 11835 sgd_solver.cpp:112] Iteration 4280, lr = 0.1
I0523 22:44:40.715528 11835 solver.cpp:239] Iteration 4290 (1.59937 iter/s, 6.25245s/10 iters), loss = 11.1231
I0523 22:44:40.715652 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1231 (* 1 = 11.1231 loss)
I0523 22:44:40.717533 11835 sgd_solver.cpp:112] Iteration 4290, lr = 0.1
I0523 22:44:49.020879 11835 solver.cpp:239] Iteration 4300 (1.20411 iter/s, 8.30491s/10 iters), loss = 11.0292
I0523 22:44:49.020918 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0292 (* 1 = 11.0292 loss)
I0523 22:44:49.992384 11835 sgd_solver.cpp:112] Iteration 4300, lr = 0.1
I0523 22:44:56.951702 11835 solver.cpp:239] Iteration 4310 (1.26096 iter/s, 7.93048s/10 iters), loss = 10.4966
I0523 22:44:56.951736 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4966 (* 1 = 10.4966 loss)
I0523 22:44:56.951861 11835 sgd_solver.cpp:112] Iteration 4310, lr = 0.1
I0523 22:45:04.021128 11835 solver.cpp:239] Iteration 4320 (1.4146 iter/s, 7.06912s/10 iters), loss = 11.0828
I0523 22:45:04.021173 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0828 (* 1 = 11.0828 loss)
I0523 22:45:04.021201 11835 sgd_solver.cpp:112] Iteration 4320, lr = 0.1
I0523 22:45:11.909340 11835 solver.cpp:239] Iteration 4330 (1.26777 iter/s, 7.88786s/10 iters), loss = 10.4562
I0523 22:45:11.909577 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4562 (* 1 = 10.4562 loss)
I0523 22:45:11.934649 11835 sgd_solver.cpp:112] Iteration 4330, lr = 0.1
I0523 22:45:20.172006 11835 solver.cpp:239] Iteration 4340 (1.21034 iter/s, 8.26216s/10 iters), loss = 9.94094
I0523 22:45:20.172041 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.94094 (* 1 = 9.94094 loss)
I0523 22:45:20.172286 11835 sgd_solver.cpp:112] Iteration 4340, lr = 0.1
I0523 22:45:27.016640 11835 solver.cpp:239] Iteration 4350 (1.46106 iter/s, 6.84433s/10 iters), loss = 10.6726
I0523 22:45:27.016687 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6726 (* 1 = 10.6726 loss)
I0523 22:45:27.260748 11835 sgd_solver.cpp:112] Iteration 4350, lr = 0.1
I0523 22:45:35.850570 11835 solver.cpp:239] Iteration 4360 (1.13205 iter/s, 8.83354s/10 iters), loss = 11.3467
I0523 22:45:35.850621 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3467 (* 1 = 11.3467 loss)
I0523 22:45:35.850843 11835 sgd_solver.cpp:112] Iteration 4360, lr = 0.1
I0523 22:45:42.224942 11835 solver.cpp:239] Iteration 4370 (1.56886 iter/s, 6.37407s/10 iters), loss = 11.3276
I0523 22:45:42.225107 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3276 (* 1 = 11.3276 loss)
I0523 22:45:42.225122 11835 sgd_solver.cpp:112] Iteration 4370, lr = 0.1
I0523 22:45:48.326138 11835 solver.cpp:239] Iteration 4380 (1.63933 iter/s, 6.10007s/10 iters), loss = 11.3029
I0523 22:45:48.326179 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3029 (* 1 = 11.3029 loss)
I0523 22:45:48.326303 11835 sgd_solver.cpp:112] Iteration 4380, lr = 0.1
I0523 22:45:54.850823 11835 solver.cpp:239] Iteration 4390 (1.53271 iter/s, 6.52439s/10 iters), loss = 10.9026
I0523 22:45:54.850862 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9026 (* 1 = 10.9026 loss)
I0523 22:45:54.850873 11835 sgd_solver.cpp:112] Iteration 4390, lr = 0.1
I0523 22:46:00.670976 11835 solver.cpp:239] Iteration 4400 (1.71887 iter/s, 5.81777s/10 iters), loss = 10.5431
I0523 22:46:00.671018 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5431 (* 1 = 10.5431 loss)
I0523 22:46:01.160126 11835 sgd_solver.cpp:112] Iteration 4400, lr = 0.1
I0523 22:46:09.104560 11835 solver.cpp:239] Iteration 4410 (1.18579 iter/s, 8.43322s/10 iters), loss = 10.0456
I0523 22:46:09.104596 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0456 (* 1 = 10.0456 loss)
I0523 22:46:09.104809 11835 sgd_solver.cpp:112] Iteration 4410, lr = 0.1
I0523 22:46:14.878406 11835 solver.cpp:239] Iteration 4420 (1.73203 iter/s, 5.77359s/10 iters), loss = 10.7282
I0523 22:46:14.878665 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7282 (* 1 = 10.7282 loss)
I0523 22:46:14.878746 11835 sgd_solver.cpp:112] Iteration 4420, lr = 0.1
I0523 22:46:22.478847 11835 solver.cpp:239] Iteration 4430 (1.31581 iter/s, 7.5999s/10 iters), loss = 11.406
I0523 22:46:22.478883 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.406 (* 1 = 11.406 loss)
I0523 22:46:23.879878 11835 sgd_solver.cpp:112] Iteration 4430, lr = 0.1
I0523 22:46:30.554425 11835 solver.cpp:239] Iteration 4440 (1.23836 iter/s, 8.07523s/10 iters), loss = 10.7966
I0523 22:46:30.554476 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7966 (* 1 = 10.7966 loss)
I0523 22:46:30.554710 11835 sgd_solver.cpp:112] Iteration 4440, lr = 0.1
I0523 22:46:36.939602 11835 solver.cpp:239] Iteration 4450 (1.5662 iter/s, 6.38489s/10 iters), loss = 11.0881
I0523 22:46:36.939638 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0881 (* 1 = 11.0881 loss)
I0523 22:46:37.168268 11835 sgd_solver.cpp:112] Iteration 4450, lr = 0.1
I0523 22:46:43.970551 11835 solver.cpp:239] Iteration 4460 (1.42235 iter/s, 7.03063s/10 iters), loss = 9.72009
I0523 22:46:43.970595 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.72009 (* 1 = 9.72009 loss)
I0523 22:46:43.970679 11835 sgd_solver.cpp:112] Iteration 4460, lr = 0.1
I0523 22:46:50.062808 11835 solver.cpp:239] Iteration 4470 (1.6415 iter/s, 6.09197s/10 iters), loss = 10.8519
I0523 22:46:50.063051 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8519 (* 1 = 10.8519 loss)
I0523 22:46:50.063103 11835 sgd_solver.cpp:112] Iteration 4470, lr = 0.1
I0523 22:46:57.847839 11835 solver.cpp:239] Iteration 4480 (1.28461 iter/s, 7.78448s/10 iters), loss = 11.3563
I0523 22:46:57.847882 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3563 (* 1 = 11.3563 loss)
I0523 22:46:58.684798 11835 sgd_solver.cpp:112] Iteration 4480, lr = 0.1
I0523 22:47:05.948243 11835 solver.cpp:239] Iteration 4490 (1.23456 iter/s, 8.10006s/10 iters), loss = 11.3192
I0523 22:47:05.948284 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3192 (* 1 = 11.3192 loss)
I0523 22:47:05.948295 11835 sgd_solver.cpp:112] Iteration 4490, lr = 0.1
I0523 22:47:12.589643 11835 solver.cpp:239] Iteration 4500 (1.50578 iter/s, 6.64109s/10 iters), loss = 9.9852
I0523 22:47:12.589689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.9852 (* 1 = 9.9852 loss)
I0523 22:47:13.013811 11835 sgd_solver.cpp:112] Iteration 4500, lr = 0.1
I0523 22:47:20.062363 11835 solver.cpp:239] Iteration 4510 (1.33826 iter/s, 7.47239s/10 iters), loss = 11.1825
I0523 22:47:20.062403 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1825 (* 1 = 11.1825 loss)
I0523 22:47:20.062511 11835 sgd_solver.cpp:112] Iteration 4510, lr = 0.1
I0523 22:47:27.328372 11835 solver.cpp:239] Iteration 4520 (1.37633 iter/s, 7.26569s/10 iters), loss = 10.7167
I0523 22:47:27.328645 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7167 (* 1 = 10.7167 loss)
I0523 22:47:27.347746 11835 sgd_solver.cpp:112] Iteration 4520, lr = 0.1
I0523 22:47:33.542371 11835 solver.cpp:239] Iteration 4530 (1.60939 iter/s, 6.21352s/10 iters), loss = 10.8398
I0523 22:47:33.542413 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8398 (* 1 = 10.8398 loss)
I0523 22:47:33.542551 11835 sgd_solver.cpp:112] Iteration 4530, lr = 0.1
I0523 22:47:41.302172 11835 solver.cpp:239] Iteration 4540 (1.28875 iter/s, 7.75946s/10 iters), loss = 10.4175
I0523 22:47:41.302214 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4175 (* 1 = 10.4175 loss)
I0523 22:47:41.302260 11835 sgd_solver.cpp:112] Iteration 4540, lr = 0.1
I0523 22:47:48.308504 11835 solver.cpp:239] Iteration 4550 (1.42734 iter/s, 7.00602s/10 iters), loss = 10.6341
I0523 22:47:48.308552 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6341 (* 1 = 10.6341 loss)
I0523 22:47:48.737813 11835 sgd_solver.cpp:112] Iteration 4550, lr = 0.1
I0523 22:47:55.770051 11835 solver.cpp:239] Iteration 4560 (1.34026 iter/s, 7.46121s/10 iters), loss = 10.6347
I0523 22:47:55.770092 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6347 (* 1 = 10.6347 loss)
I0523 22:47:55.770258 11835 sgd_solver.cpp:112] Iteration 4560, lr = 0.1
I0523 22:48:02.140732 11835 solver.cpp:239] Iteration 4570 (1.56976 iter/s, 6.37039s/10 iters), loss = 11.0653
I0523 22:48:02.140969 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0653 (* 1 = 11.0653 loss)
I0523 22:48:02.335819 11835 sgd_solver.cpp:112] Iteration 4570, lr = 0.1
I0523 22:48:11.404225 11835 solver.cpp:239] Iteration 4580 (1.07957 iter/s, 9.26293s/10 iters), loss = 10.9812
I0523 22:48:11.404273 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9812 (* 1 = 10.9812 loss)
I0523 22:48:11.404475 11835 sgd_solver.cpp:112] Iteration 4580, lr = 0.1
I0523 22:48:17.705819 11835 solver.cpp:239] Iteration 4590 (1.58697 iter/s, 6.30131s/10 iters), loss = 11.5844
I0523 22:48:17.705857 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.5844 (* 1 = 11.5844 loss)
I0523 22:48:17.995635 11835 sgd_solver.cpp:112] Iteration 4590, lr = 0.1
I0523 22:48:23.847066 11835 solver.cpp:239] Iteration 4600 (1.62841 iter/s, 6.14096s/10 iters), loss = 10.692
I0523 22:48:23.847107 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.692 (* 1 = 10.692 loss)
I0523 22:48:24.630353 11835 sgd_solver.cpp:112] Iteration 4600, lr = 0.1
I0523 22:48:30.655607 11835 solver.cpp:239] Iteration 4610 (1.46881 iter/s, 6.80824s/10 iters), loss = 10.0101
I0523 22:48:30.655643 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0101 (* 1 = 10.0101 loss)
I0523 22:48:30.655654 11835 sgd_solver.cpp:112] Iteration 4610, lr = 0.1
I0523 22:48:37.030943 11835 solver.cpp:239] Iteration 4620 (1.56871 iter/s, 6.37468s/10 iters), loss = 10.7717
I0523 22:48:37.031139 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7717 (* 1 = 10.7717 loss)
I0523 22:48:37.031185 11835 sgd_solver.cpp:112] Iteration 4620, lr = 0.1
I0523 22:48:45.241489 11835 solver.cpp:239] Iteration 4630 (1.21818 iter/s, 8.20895s/10 iters), loss = 11.1226
I0523 22:48:45.241533 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1226 (* 1 = 11.1226 loss)
I0523 22:48:45.241763 11835 sgd_solver.cpp:112] Iteration 4630, lr = 0.1
I0523 22:48:51.954555 11835 solver.cpp:239] Iteration 4640 (1.4897 iter/s, 6.71277s/10 iters), loss = 11.2635
I0523 22:48:51.954592 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2635 (* 1 = 11.2635 loss)
I0523 22:48:51.977838 11835 sgd_solver.cpp:112] Iteration 4640, lr = 0.1
I0523 22:48:57.830277 11835 solver.cpp:239] Iteration 4650 (1.70199 iter/s, 5.87546s/10 iters), loss = 9.81835
I0523 22:48:57.830314 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.81835 (* 1 = 9.81835 loss)
I0523 22:48:58.432749 11835 sgd_solver.cpp:112] Iteration 4650, lr = 0.1
I0523 22:49:04.792979 11835 solver.cpp:239] Iteration 4660 (1.43629 iter/s, 6.9624s/10 iters), loss = 10.4643
I0523 22:49:04.793012 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4643 (* 1 = 10.4643 loss)
I0523 22:49:04.793098 11835 sgd_solver.cpp:112] Iteration 4660, lr = 0.1
I0523 22:49:11.229157 11835 solver.cpp:239] Iteration 4670 (1.55379 iter/s, 6.43589s/10 iters), loss = 11.7828
I0523 22:49:11.229420 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.7828 (* 1 = 11.7828 loss)
I0523 22:49:11.229462 11835 sgd_solver.cpp:112] Iteration 4670, lr = 0.1
I0523 22:49:17.328341 11835 solver.cpp:239] Iteration 4680 (1.64028 iter/s, 6.09652s/10 iters), loss = 10.1636
I0523 22:49:17.328375 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1636 (* 1 = 10.1636 loss)
I0523 22:49:17.353276 11835 sgd_solver.cpp:112] Iteration 4680, lr = 0.1
I0523 22:49:24.509610 11835 solver.cpp:239] Iteration 4690 (1.39257 iter/s, 7.18096s/10 iters), loss = 11.1747
I0523 22:49:24.509652 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1747 (* 1 = 11.1747 loss)
I0523 22:49:24.509667 11835 sgd_solver.cpp:112] Iteration 4690, lr = 0.1
I0523 22:49:31.929280 11835 solver.cpp:239] Iteration 4700 (1.34786 iter/s, 7.41915s/10 iters), loss = 10.0456
I0523 22:49:31.929322 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0456 (* 1 = 10.0456 loss)
I0523 22:49:31.929450 11835 sgd_solver.cpp:112] Iteration 4700, lr = 0.1
I0523 22:49:39.477452 11835 solver.cpp:239] Iteration 4710 (1.32488 iter/s, 7.54784s/10 iters), loss = 11.4378
I0523 22:49:39.477491 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4378 (* 1 = 11.4378 loss)
I0523 22:49:39.477634 11835 sgd_solver.cpp:112] Iteration 4710, lr = 0.1
I0523 22:49:49.390796 11835 solver.cpp:239] Iteration 4720 (1.00878 iter/s, 9.91293s/10 iters), loss = 10.8336
I0523 22:49:49.391026 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8336 (* 1 = 10.8336 loss)
I0523 22:49:49.391244 11835 sgd_solver.cpp:112] Iteration 4720, lr = 0.1
I0523 22:49:57.059876 11835 solver.cpp:239] Iteration 4730 (1.30402 iter/s, 7.66858s/10 iters), loss = 10.1712
I0523 22:49:57.059921 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1712 (* 1 = 10.1712 loss)
I0523 22:49:57.063575 11835 sgd_solver.cpp:112] Iteration 4730, lr = 0.1
I0523 22:50:05.954082 11835 solver.cpp:239] Iteration 4740 (1.12438 iter/s, 8.89382s/10 iters), loss = 10.7631
I0523 22:50:05.954123 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7631 (* 1 = 10.7631 loss)
I0523 22:50:07.087203 11835 sgd_solver.cpp:112] Iteration 4740, lr = 0.1
I0523 22:50:14.698226 11835 solver.cpp:239] Iteration 4750 (1.14367 iter/s, 8.74377s/10 iters), loss = 10.5889
I0523 22:50:14.698262 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5889 (* 1 = 10.5889 loss)
I0523 22:50:14.698274 11835 sgd_solver.cpp:112] Iteration 4750, lr = 0.1
I0523 22:50:20.738951 11835 solver.cpp:239] Iteration 4760 (1.65568 iter/s, 6.03983s/10 iters), loss = 11.0885
I0523 22:50:20.739225 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0885 (* 1 = 11.0885 loss)
I0523 22:50:20.739363 11835 sgd_solver.cpp:112] Iteration 4760, lr = 0.1
I0523 22:50:27.394876 11835 solver.cpp:239] Iteration 4770 (1.50253 iter/s, 6.65544s/10 iters), loss = 10.8456
I0523 22:50:27.394922 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8456 (* 1 = 10.8456 loss)
I0523 22:50:27.395166 11835 sgd_solver.cpp:112] Iteration 4770, lr = 0.1
I0523 22:50:36.314424 11835 solver.cpp:239] Iteration 4780 (1.12118 iter/s, 8.91916s/10 iters), loss = 10.7378
I0523 22:50:36.314466 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7378 (* 1 = 10.7378 loss)
I0523 22:50:36.947314 11835 sgd_solver.cpp:112] Iteration 4780, lr = 0.1
I0523 22:50:44.182448 11835 solver.cpp:239] Iteration 4790 (1.27102 iter/s, 7.86768s/10 iters), loss = 10.9038
I0523 22:50:44.182494 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9038 (* 1 = 10.9038 loss)
I0523 22:50:44.182775 11835 sgd_solver.cpp:112] Iteration 4790, lr = 0.1
I0523 22:50:51.698418 11835 solver.cpp:239] Iteration 4800 (1.33056 iter/s, 7.51564s/10 iters), loss = 10.5705
I0523 22:50:51.698603 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5705 (* 1 = 10.5705 loss)
I0523 22:50:51.698619 11835 sgd_solver.cpp:112] Iteration 4800, lr = 0.1
I0523 22:50:58.539047 11835 solver.cpp:239] Iteration 4810 (1.46198 iter/s, 6.84003s/10 iters), loss = 10.9876
I0523 22:50:58.539088 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9876 (* 1 = 10.9876 loss)
I0523 22:50:58.539103 11835 sgd_solver.cpp:112] Iteration 4810, lr = 0.1
I0523 22:51:07.260057 11835 solver.cpp:239] Iteration 4820 (1.14675 iter/s, 8.72027s/10 iters), loss = 11.1155
I0523 22:51:07.260097 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1155 (* 1 = 11.1155 loss)
I0523 22:51:07.260309 11835 sgd_solver.cpp:112] Iteration 4820, lr = 0.1
I0523 22:51:13.308332 11835 solver.cpp:239] Iteration 4830 (1.65344 iter/s, 6.048s/10 iters), loss = 10.0299
I0523 22:51:13.308370 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0299 (* 1 = 10.0299 loss)
I0523 22:51:13.836334 11835 sgd_solver.cpp:112] Iteration 4830, lr = 0.1
I0523 22:51:22.272917 11835 solver.cpp:239] Iteration 4840 (1.11555 iter/s, 8.96421s/10 iters), loss = 10.2383
I0523 22:51:22.273149 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2383 (* 1 = 10.2383 loss)
I0523 22:51:22.412763 11835 sgd_solver.cpp:112] Iteration 4840, lr = 0.1
I0523 22:51:32.226578 11835 solver.cpp:239] Iteration 4850 (1.00471 iter/s, 9.95308s/10 iters), loss = 11.7351
I0523 22:51:32.226626 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.7351 (* 1 = 11.7351 loss)
I0523 22:51:32.241014 11835 sgd_solver.cpp:112] Iteration 4850, lr = 0.1
I0523 22:51:41.088001 11835 solver.cpp:239] Iteration 4860 (1.12854 iter/s, 8.86103s/10 iters), loss = 11.1131
I0523 22:51:41.088047 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1131 (* 1 = 11.1131 loss)
I0523 22:51:41.088096 11835 sgd_solver.cpp:112] Iteration 4860, lr = 0.1
I0523 22:51:48.616309 11835 solver.cpp:239] Iteration 4870 (1.32838 iter/s, 7.52797s/10 iters), loss = 10.7205
I0523 22:51:48.616358 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7205 (* 1 = 10.7205 loss)
I0523 22:51:48.995487 11835 sgd_solver.cpp:112] Iteration 4870, lr = 0.1
I0523 22:51:56.649422 11835 solver.cpp:239] Iteration 4880 (1.2449 iter/s, 8.03276s/10 iters), loss = 10.9262
I0523 22:51:56.649624 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9262 (* 1 = 10.9262 loss)
I0523 22:51:57.430348 11835 sgd_solver.cpp:112] Iteration 4880, lr = 0.1
I0523 22:52:04.621302 11835 solver.cpp:239] Iteration 4890 (1.25448 iter/s, 7.9714s/10 iters), loss = 11.8526
I0523 22:52:04.621345 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.8526 (* 1 = 11.8526 loss)
I0523 22:52:04.792178 11835 sgd_solver.cpp:112] Iteration 4890, lr = 0.1
I0523 22:52:12.613018 11835 solver.cpp:239] Iteration 4900 (1.25135 iter/s, 7.99136s/10 iters), loss = 10.6501
I0523 22:52:12.613067 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6501 (* 1 = 10.6501 loss)
I0523 22:52:12.613303 11835 sgd_solver.cpp:112] Iteration 4900, lr = 0.1
I0523 22:52:19.218150 11835 solver.cpp:239] Iteration 4910 (1.51404 iter/s, 6.60483s/10 iters), loss = 10.9838
I0523 22:52:19.218197 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9838 (* 1 = 10.9838 loss)
I0523 22:52:19.218297 11835 sgd_solver.cpp:112] Iteration 4910, lr = 0.1
I0523 22:52:26.840061 11835 solver.cpp:239] Iteration 4920 (1.31207 iter/s, 7.62156s/10 iters), loss = 10.4501
I0523 22:52:26.840348 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4501 (* 1 = 10.4501 loss)
I0523 22:52:27.589951 11835 sgd_solver.cpp:112] Iteration 4920, lr = 0.1
I0523 22:52:34.248105 11835 solver.cpp:239] Iteration 4930 (1.34998 iter/s, 7.4075s/10 iters), loss = 11.1165
I0523 22:52:34.248144 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1165 (* 1 = 11.1165 loss)
I0523 22:52:34.248157 11835 sgd_solver.cpp:112] Iteration 4930, lr = 0.1
I0523 22:52:39.891875 11835 solver.cpp:239] Iteration 4940 (1.77197 iter/s, 5.64343s/10 iters), loss = 10.42
I0523 22:52:39.891927 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.42 (* 1 = 10.42 loss)
I0523 22:52:39.891944 11835 sgd_solver.cpp:112] Iteration 4940, lr = 0.1
I0523 22:52:47.090394 11835 solver.cpp:239] Iteration 4950 (1.38966 iter/s, 7.19599s/10 iters), loss = 11.1735
I0523 22:52:47.090447 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1735 (* 1 = 11.1735 loss)
I0523 22:52:47.458282 11835 sgd_solver.cpp:112] Iteration 4950, lr = 0.1
I0523 22:52:55.122519 11835 solver.cpp:239] Iteration 4960 (1.24506 iter/s, 8.03176s/10 iters), loss = 10.9779
I0523 22:52:55.122571 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9779 (* 1 = 10.9779 loss)
I0523 22:52:55.122678 11835 sgd_solver.cpp:112] Iteration 4960, lr = 0.1
I0523 22:53:03.375756 11835 solver.cpp:239] Iteration 4970 (1.2117 iter/s, 8.25287s/10 iters), loss = 10.0876
I0523 22:53:03.376001 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0876 (* 1 = 10.0876 loss)
I0523 22:53:03.392437 11835 sgd_solver.cpp:112] Iteration 4970, lr = 0.1
I0523 22:53:09.174376 11835 solver.cpp:239] Iteration 4980 (1.72468 iter/s, 5.79819s/10 iters), loss = 10.4637
I0523 22:53:09.174412 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4637 (* 1 = 10.4637 loss)
I0523 22:53:09.174424 11835 sgd_solver.cpp:112] Iteration 4980, lr = 0.1
I0523 22:53:17.170634 11835 solver.cpp:239] Iteration 4990 (1.25064 iter/s, 7.99591s/10 iters), loss = 11.5674
I0523 22:53:17.170677 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.5674 (* 1 = 11.5674 loss)
I0523 22:53:17.170800 11835 sgd_solver.cpp:112] Iteration 4990, lr = 0.1
I0523 22:53:22.361150 11835 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_5000.caffemodel
I0523 22:53:22.804582 11835 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_5000.solverstate
I0523 22:53:23.435626 11835 solver.cpp:239] Iteration 5000 (1.59624 iter/s, 6.26471s/10 iters), loss = 11.7471
I0523 22:53:23.435667 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.7471 (* 1 = 11.7471 loss)
I0523 22:53:23.569183 11835 sgd_solver.cpp:112] Iteration 5000, lr = 0.1
I0523 22:53:31.718451 11835 solver.cpp:239] Iteration 5010 (1.20737 iter/s, 8.28246s/10 iters), loss = 10.7541
I0523 22:53:31.718511 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7541 (* 1 = 10.7541 loss)
I0523 22:53:31.737522 11835 sgd_solver.cpp:112] Iteration 5010, lr = 0.1
I0523 22:53:37.339051 11835 solver.cpp:239] Iteration 5020 (1.77926 iter/s, 5.62032s/10 iters), loss = 10.5751
I0523 22:53:37.339342 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5751 (* 1 = 10.5751 loss)
I0523 22:53:37.339417 11835 sgd_solver.cpp:112] Iteration 5020, lr = 0.1
I0523 22:53:44.156216 11835 solver.cpp:239] Iteration 5030 (1.46699 iter/s, 6.81668s/10 iters), loss = 11.3106
I0523 22:53:44.156267 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3106 (* 1 = 11.3106 loss)
I0523 22:53:44.164278 11835 sgd_solver.cpp:112] Iteration 5030, lr = 0.1
I0523 22:53:49.811106 11835 solver.cpp:239] Iteration 5040 (1.76847 iter/s, 5.65462s/10 iters), loss = 10.5469
I0523 22:53:49.811156 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5469 (* 1 = 10.5469 loss)
I0523 22:53:49.811170 11835 sgd_solver.cpp:112] Iteration 5040, lr = 0.1
I0523 22:53:57.348312 11835 solver.cpp:239] Iteration 5050 (1.32682 iter/s, 7.5368s/10 iters), loss = 10.7959
I0523 22:53:57.348363 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7959 (* 1 = 10.7959 loss)
I0523 22:53:57.348459 11835 sgd_solver.cpp:112] Iteration 5050, lr = 0.1
I0523 22:54:03.507117 11835 solver.cpp:239] Iteration 5060 (1.62376 iter/s, 6.15853s/10 iters), loss = 9.78997
I0523 22:54:03.507156 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.78997 (* 1 = 9.78997 loss)
I0523 22:54:03.507354 11835 sgd_solver.cpp:112] Iteration 5060, lr = 0.1
I0523 22:54:09.667232 11835 solver.cpp:239] Iteration 5070 (1.62342 iter/s, 6.15984s/10 iters), loss = 10.4863
I0523 22:54:09.667402 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4863 (* 1 = 10.4863 loss)
I0523 22:54:09.667428 11835 sgd_solver.cpp:112] Iteration 5070, lr = 0.1
I0523 22:54:15.516942 11835 solver.cpp:239] Iteration 5080 (1.70964 iter/s, 5.84918s/10 iters), loss = 11.248
I0523 22:54:15.516989 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.248 (* 1 = 11.248 loss)
I0523 22:54:15.862774 11835 sgd_solver.cpp:112] Iteration 5080, lr = 0.1
I0523 22:54:21.792179 11835 solver.cpp:239] Iteration 5090 (1.59364 iter/s, 6.27494s/10 iters), loss = 10.6862
I0523 22:54:21.792239 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6862 (* 1 = 10.6862 loss)
I0523 22:54:21.793232 11835 sgd_solver.cpp:112] Iteration 5090, lr = 0.1
I0523 22:54:30.663573 11835 solver.cpp:239] Iteration 5100 (1.12727 iter/s, 8.871s/10 iters), loss = 10.8824
I0523 22:54:30.663630 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8824 (* 1 = 10.8824 loss)
I0523 22:54:30.663866 11835 sgd_solver.cpp:112] Iteration 5100, lr = 0.1
I0523 22:54:38.109395 11835 solver.cpp:239] Iteration 5110 (1.34309 iter/s, 7.44549s/10 iters), loss = 10.6703
I0523 22:54:38.109447 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6703 (* 1 = 10.6703 loss)
I0523 22:54:38.109463 11835 sgd_solver.cpp:112] Iteration 5110, lr = 0.1
I0523 22:54:45.098115 11835 solver.cpp:239] Iteration 5120 (1.43107 iter/s, 6.98778s/10 iters), loss = 10.923
I0523 22:54:45.098223 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.923 (* 1 = 10.923 loss)
I0523 22:54:45.098289 11835 sgd_solver.cpp:112] Iteration 5120, lr = 0.1
I0523 22:54:53.643821 11835 solver.cpp:239] Iteration 5130 (1.17024 iter/s, 8.54529s/10 iters), loss = 11.1534
I0523 22:54:53.643875 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1534 (* 1 = 11.1534 loss)
I0523 22:54:53.643959 11835 sgd_solver.cpp:112] Iteration 5130, lr = 0.1
I0523 22:54:59.581080 11835 solver.cpp:239] Iteration 5140 (1.68436 iter/s, 5.93698s/10 iters), loss = 10.4941
I0523 22:54:59.581128 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4941 (* 1 = 10.4941 loss)
I0523 22:54:59.581143 11835 sgd_solver.cpp:112] Iteration 5140, lr = 0.1
I0523 22:55:06.816812 11835 solver.cpp:239] Iteration 5150 (1.38209 iter/s, 7.23541s/10 iters), loss = 10.8462
I0523 22:55:06.816864 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8462 (* 1 = 10.8462 loss)
I0523 22:55:06.816879 11835 sgd_solver.cpp:112] Iteration 5150, lr = 0.1
I0523 22:55:13.779438 11835 solver.cpp:239] Iteration 5160 (1.43643 iter/s, 6.96169s/10 iters), loss = 11.1386
I0523 22:55:13.779485 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1386 (* 1 = 11.1386 loss)
I0523 22:55:14.164670 11835 sgd_solver.cpp:112] Iteration 5160, lr = 0.1
I0523 22:55:20.942715 11835 solver.cpp:239] Iteration 5170 (1.39607 iter/s, 7.16294s/10 iters), loss = 11.2476
I0523 22:55:20.942999 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2476 (* 1 = 11.2476 loss)
I0523 22:55:20.943040 11835 sgd_solver.cpp:112] Iteration 5170, lr = 0.1
I0523 22:55:30.107369 11835 solver.cpp:239] Iteration 5180 (1.09147 iter/s, 9.16195s/10 iters), loss = 10.6219
I0523 22:55:30.107421 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6219 (* 1 = 10.6219 loss)
I0523 22:55:31.063545 11835 sgd_solver.cpp:112] Iteration 5180, lr = 0.1
I0523 22:55:38.309247 11835 solver.cpp:239] Iteration 5190 (1.21929 iter/s, 8.20152s/10 iters), loss = 10.4562
I0523 22:55:38.309298 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4562 (* 1 = 10.4562 loss)
I0523 22:55:38.864617 11835 sgd_solver.cpp:112] Iteration 5190, lr = 0.1
I0523 22:55:46.578287 11835 solver.cpp:239] Iteration 5200 (1.20938 iter/s, 8.26867s/10 iters), loss = 11.2401
I0523 22:55:46.578346 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2401 (* 1 = 11.2401 loss)
I0523 22:55:46.578579 11835 sgd_solver.cpp:112] Iteration 5200, lr = 0.1
I0523 22:55:52.326479 11835 solver.cpp:239] Iteration 5210 (1.73976 iter/s, 5.74791s/10 iters), loss = 10.6131
I0523 22:55:52.326803 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6131 (* 1 = 10.6131 loss)
I0523 22:55:52.327934 11835 sgd_solver.cpp:112] Iteration 5210, lr = 0.1
I0523 22:55:58.611649 11835 solver.cpp:239] Iteration 5220 (1.59116 iter/s, 6.28471s/10 iters), loss = 11.4769
I0523 22:55:58.611690 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4769 (* 1 = 11.4769 loss)
I0523 22:55:58.611809 11835 sgd_solver.cpp:112] Iteration 5220, lr = 0.1
I0523 22:56:07.195116 11835 solver.cpp:239] Iteration 5230 (1.16508 iter/s, 8.58309s/10 iters), loss = 10.7945
I0523 22:56:07.195170 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7945 (* 1 = 10.7945 loss)
I0523 22:56:07.248595 11835 sgd_solver.cpp:112] Iteration 5230, lr = 0.1
I0523 22:56:15.168129 11835 solver.cpp:239] Iteration 5240 (1.25429 iter/s, 7.97266s/10 iters), loss = 10.8273
I0523 22:56:15.168186 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8273 (* 1 = 10.8273 loss)
I0523 22:56:15.177994 11835 sgd_solver.cpp:112] Iteration 5240, lr = 0.1
I0523 22:56:21.887706 11835 solver.cpp:239] Iteration 5250 (1.48826 iter/s, 6.71926s/10 iters), loss = 10.9038
I0523 22:56:21.887763 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9038 (* 1 = 10.9038 loss)
I0523 22:56:21.887840 11835 sgd_solver.cpp:112] Iteration 5250, lr = 0.1
I0523 22:56:29.777920 11835 solver.cpp:239] Iteration 5260 (1.26745 iter/s, 7.88986s/10 iters), loss = 10.4284
I0523 22:56:29.778175 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4284 (* 1 = 10.4284 loss)
I0523 22:56:29.778419 11835 sgd_solver.cpp:112] Iteration 5260, lr = 0.1
I0523 22:56:37.157379 11835 solver.cpp:239] Iteration 5270 (1.3552 iter/s, 7.37897s/10 iters), loss = 10.4496
I0523 22:56:37.157433 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4496 (* 1 = 10.4496 loss)
I0523 22:56:37.157657 11835 sgd_solver.cpp:112] Iteration 5270, lr = 0.1
I0523 22:56:43.293329 11835 solver.cpp:239] Iteration 5280 (1.62981 iter/s, 6.13567s/10 iters), loss = 11.0982
I0523 22:56:43.293371 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0982 (* 1 = 11.0982 loss)
I0523 22:56:43.293550 11835 sgd_solver.cpp:112] Iteration 5280, lr = 0.1
I0523 22:56:50.940435 11835 solver.cpp:239] Iteration 5290 (1.30774 iter/s, 7.64677s/10 iters), loss = 10.4044
I0523 22:56:50.940487 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4044 (* 1 = 10.4044 loss)
I0523 22:56:51.009158 11835 sgd_solver.cpp:112] Iteration 5290, lr = 0.1
I0523 22:56:59.418738 11835 solver.cpp:239] Iteration 5300 (1.17953 iter/s, 8.47793s/10 iters), loss = 10.992
I0523 22:56:59.418793 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.992 (* 1 = 10.992 loss)
I0523 22:56:59.418969 11835 sgd_solver.cpp:112] Iteration 5300, lr = 0.1
I0523 22:57:06.379616 11835 solver.cpp:239] Iteration 5310 (1.43667 iter/s, 6.96056s/10 iters), loss = 10.6319
I0523 22:57:06.379892 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6319 (* 1 = 10.6319 loss)
I0523 22:57:06.380108 11835 sgd_solver.cpp:112] Iteration 5310, lr = 0.1
I0523 22:57:12.972769 11835 solver.cpp:239] Iteration 5320 (1.51684 iter/s, 6.59265s/10 iters), loss = 10.638
I0523 22:57:12.972820 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.638 (* 1 = 10.638 loss)
I0523 22:57:13.426187 11835 sgd_solver.cpp:112] Iteration 5320, lr = 0.1
I0523 22:57:22.005738 11835 solver.cpp:239] Iteration 5330 (1.1071 iter/s, 9.03258s/10 iters), loss = 10.1258
I0523 22:57:22.005785 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1258 (* 1 = 10.1258 loss)
I0523 22:57:22.005923 11835 sgd_solver.cpp:112] Iteration 5330, lr = 0.1
I0523 22:57:27.602869 11835 solver.cpp:239] Iteration 5340 (1.78671 iter/s, 5.59687s/10 iters), loss = 10.9534
I0523 22:57:27.602906 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9534 (* 1 = 10.9534 loss)
I0523 22:57:27.603082 11835 sgd_solver.cpp:112] Iteration 5340, lr = 0.1
I0523 22:57:35.510301 11835 solver.cpp:239] Iteration 5350 (1.26469 iter/s, 7.90709s/10 iters), loss = 11.0156
I0523 22:57:35.510359 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0156 (* 1 = 11.0156 loss)
I0523 22:57:36.258460 11835 sgd_solver.cpp:112] Iteration 5350, lr = 0.1
I0523 22:57:43.124550 11835 solver.cpp:239] Iteration 5360 (1.31338 iter/s, 7.61391s/10 iters), loss = 10.9285
I0523 22:57:43.124783 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9285 (* 1 = 10.9285 loss)
I0523 22:57:43.147240 11835 sgd_solver.cpp:112] Iteration 5360, lr = 0.1
I0523 22:57:50.013209 11835 solver.cpp:239] Iteration 5370 (1.45176 iter/s, 6.8882s/10 iters), loss = 10.4684
I0523 22:57:50.013267 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4684 (* 1 = 10.4684 loss)
I0523 22:57:50.013283 11835 sgd_solver.cpp:112] Iteration 5370, lr = 0.1
I0523 22:57:56.285948 11835 solver.cpp:239] Iteration 5380 (1.59427 iter/s, 6.27245s/10 iters), loss = 10.6677
I0523 22:57:56.285995 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6677 (* 1 = 10.6677 loss)
I0523 22:57:56.286242 11835 sgd_solver.cpp:112] Iteration 5380, lr = 0.1
I0523 22:58:03.176542 11835 solver.cpp:239] Iteration 5390 (1.45132 iter/s, 6.89028s/10 iters), loss = 10.2183
I0523 22:58:03.176599 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2183 (* 1 = 10.2183 loss)
I0523 22:58:03.176867 11835 sgd_solver.cpp:112] Iteration 5390, lr = 0.1
I0523 22:58:10.417238 11835 solver.cpp:239] Iteration 5400 (1.38115 iter/s, 7.24036s/10 iters), loss = 10.9526
I0523 22:58:10.417302 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9526 (* 1 = 10.9526 loss)
I0523 22:58:10.464251 11835 sgd_solver.cpp:112] Iteration 5400, lr = 0.1
I0523 22:58:17.200135 11835 solver.cpp:239] Iteration 5410 (1.47436 iter/s, 6.78258s/10 iters), loss = 11.2997
I0523 22:58:17.200250 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2997 (* 1 = 11.2997 loss)
I0523 22:58:17.200269 11835 sgd_solver.cpp:112] Iteration 5410, lr = 0.1
I0523 22:58:23.286612 11835 solver.cpp:239] Iteration 5420 (1.64308 iter/s, 6.08613s/10 iters), loss = 10.9614
I0523 22:58:23.286660 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9614 (* 1 = 10.9614 loss)
I0523 22:58:23.330899 11835 sgd_solver.cpp:112] Iteration 5420, lr = 0.1
I0523 22:58:30.904165 11835 solver.cpp:239] Iteration 5430 (1.31282 iter/s, 7.61721s/10 iters), loss = 11.3153
I0523 22:58:30.904235 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3153 (* 1 = 11.3153 loss)
I0523 22:58:30.904482 11835 sgd_solver.cpp:112] Iteration 5430, lr = 0.1
I0523 22:58:37.674458 11835 solver.cpp:239] Iteration 5440 (1.47711 iter/s, 6.76998s/10 iters), loss = 10.6881
I0523 22:58:37.674509 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6881 (* 1 = 10.6881 loss)
I0523 22:58:38.322080 11835 sgd_solver.cpp:112] Iteration 5440, lr = 0.1
I0523 22:58:44.179756 11835 solver.cpp:239] Iteration 5450 (1.53728 iter/s, 6.505s/10 iters), loss = 10.8571
I0523 22:58:44.179805 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8571 (* 1 = 10.8571 loss)
I0523 22:58:44.179819 11835 sgd_solver.cpp:112] Iteration 5450, lr = 0.1
I0523 22:58:50.439579 11835 solver.cpp:239] Iteration 5460 (1.59756 iter/s, 6.25954s/10 iters), loss = 9.64694
I0523 22:58:50.439903 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.64694 (* 1 = 9.64694 loss)
I0523 22:58:50.522008 11835 sgd_solver.cpp:112] Iteration 5460, lr = 0.1
I0523 22:58:56.917923 11835 solver.cpp:239] Iteration 5470 (1.54373 iter/s, 6.47782s/10 iters), loss = 11.4606
I0523 22:58:56.917975 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4606 (* 1 = 11.4606 loss)
I0523 22:58:57.548646 11835 sgd_solver.cpp:112] Iteration 5470, lr = 0.1
I0523 22:59:03.316084 11835 solver.cpp:239] Iteration 5480 (1.56302 iter/s, 6.39787s/10 iters), loss = 10.5607
I0523 22:59:03.316123 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5607 (* 1 = 10.5607 loss)
I0523 22:59:03.316135 11835 sgd_solver.cpp:112] Iteration 5480, lr = 0.1
I0523 22:59:10.936098 11835 solver.cpp:239] Iteration 5490 (1.31239 iter/s, 7.61968s/10 iters), loss = 10.9655
I0523 22:59:10.936153 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9655 (* 1 = 10.9655 loss)
I0523 22:59:11.269904 11835 sgd_solver.cpp:112] Iteration 5490, lr = 0.1
I0523 22:59:17.074357 11835 solver.cpp:239] Iteration 5500 (1.6292 iter/s, 6.13797s/10 iters), loss = 10.0993
I0523 22:59:17.074415 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0993 (* 1 = 10.0993 loss)
I0523 22:59:17.074523 11835 sgd_solver.cpp:112] Iteration 5500, lr = 0.1
I0523 22:59:22.916857 11835 solver.cpp:239] Iteration 5510 (1.71168 iter/s, 5.84223s/10 iters), loss = 10.5168
I0523 22:59:22.917120 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5168 (* 1 = 10.5168 loss)
I0523 22:59:22.917171 11835 sgd_solver.cpp:112] Iteration 5510, lr = 0.1
I0523 22:59:29.489171 11835 solver.cpp:239] Iteration 5520 (1.5218 iter/s, 6.57115s/10 iters), loss = 11.1152
I0523 22:59:29.489233 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1152 (* 1 = 11.1152 loss)
I0523 22:59:29.489408 11835 sgd_solver.cpp:112] Iteration 5520, lr = 0.1
I0523 22:59:35.927340 11835 solver.cpp:239] Iteration 5530 (1.55331 iter/s, 6.43786s/10 iters), loss = 10.8642
I0523 22:59:35.927393 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8642 (* 1 = 10.8642 loss)
I0523 22:59:35.927431 11835 sgd_solver.cpp:112] Iteration 5530, lr = 0.1
I0523 22:59:42.319350 11835 solver.cpp:239] Iteration 5540 (1.56452 iter/s, 6.39172s/10 iters), loss = 10.6582
I0523 22:59:42.319391 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6582 (* 1 = 10.6582 loss)
I0523 22:59:42.343042 11835 sgd_solver.cpp:112] Iteration 5540, lr = 0.1
I0523 22:59:48.839321 11835 solver.cpp:239] Iteration 5550 (1.53382 iter/s, 6.51968s/10 iters), loss = 10.4644
I0523 22:59:48.839373 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4644 (* 1 = 10.4644 loss)
I0523 22:59:48.839488 11835 sgd_solver.cpp:112] Iteration 5550, lr = 0.1
I0523 22:59:54.702391 11835 solver.cpp:239] Iteration 5560 (1.70567 iter/s, 5.8628s/10 iters), loss = 10.5415
I0523 22:59:54.702612 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5415 (* 1 = 10.5415 loss)
I0523 22:59:54.702667 11835 sgd_solver.cpp:112] Iteration 5560, lr = 0.1
I0523 23:00:00.709105 11835 solver.cpp:239] Iteration 5570 (1.66493 iter/s, 6.00626s/10 iters), loss = 11.1352
I0523 23:00:00.709166 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1352 (* 1 = 11.1352 loss)
I0523 23:00:00.709396 11835 sgd_solver.cpp:112] Iteration 5570, lr = 0.1
I0523 23:00:07.873723 11835 solver.cpp:239] Iteration 5580 (1.39581 iter/s, 7.16429s/10 iters), loss = 9.93455
I0523 23:00:07.873776 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.93455 (* 1 = 9.93455 loss)
I0523 23:00:07.874049 11835 sgd_solver.cpp:112] Iteration 5580, lr = 0.1
I0523 23:00:16.292140 11835 solver.cpp:239] Iteration 5590 (1.18792 iter/s, 8.41805s/10 iters), loss = 11.4747
I0523 23:00:16.292197 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4747 (* 1 = 11.4747 loss)
I0523 23:00:16.292418 11835 sgd_solver.cpp:112] Iteration 5590, lr = 0.1
I0523 23:00:22.600997 11835 solver.cpp:239] Iteration 5600 (1.58514 iter/s, 6.30857s/10 iters), loss = 10.3639
I0523 23:00:22.601040 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3639 (* 1 = 10.3639 loss)
I0523 23:00:22.601052 11835 sgd_solver.cpp:112] Iteration 5600, lr = 0.1
I0523 23:00:29.261149 11835 solver.cpp:239] Iteration 5610 (1.50162 iter/s, 6.65947s/10 iters), loss = 10.3686
I0523 23:00:29.261303 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3686 (* 1 = 10.3686 loss)
I0523 23:00:30.241462 11835 sgd_solver.cpp:112] Iteration 5610, lr = 0.1
I0523 23:00:36.539840 11835 solver.cpp:239] Iteration 5620 (1.37395 iter/s, 7.27827s/10 iters), loss = 11.2214
I0523 23:00:36.539891 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2214 (* 1 = 11.2214 loss)
I0523 23:00:36.540053 11835 sgd_solver.cpp:112] Iteration 5620, lr = 0.1
I0523 23:00:43.215906 11835 solver.cpp:239] Iteration 5630 (1.49796 iter/s, 6.67576s/10 iters), loss = 10.7681
I0523 23:00:43.215966 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7681 (* 1 = 10.7681 loss)
I0523 23:00:43.216181 11835 sgd_solver.cpp:112] Iteration 5630, lr = 0.1
I0523 23:00:49.127144 11835 solver.cpp:239] Iteration 5640 (1.69177 iter/s, 5.91096s/10 iters), loss = 10.1493
I0523 23:00:49.127185 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1493 (* 1 = 10.1493 loss)
I0523 23:00:49.127197 11835 sgd_solver.cpp:112] Iteration 5640, lr = 0.1
I0523 23:00:56.117843 11835 solver.cpp:239] Iteration 5650 (1.43054 iter/s, 6.99039s/10 iters), loss = 10.6601
I0523 23:00:56.117894 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6601 (* 1 = 10.6601 loss)
I0523 23:00:56.118077 11835 sgd_solver.cpp:112] Iteration 5650, lr = 0.1
I0523 23:01:03.426792 11835 solver.cpp:239] Iteration 5660 (1.36825 iter/s, 7.30863s/10 iters), loss = 9.65203
I0523 23:01:03.427052 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.65203 (* 1 = 9.65203 loss)
I0523 23:01:03.432021 11835 sgd_solver.cpp:112] Iteration 5660, lr = 0.1
I0523 23:01:11.126364 11835 solver.cpp:239] Iteration 5670 (1.29886 iter/s, 7.69906s/10 iters), loss = 11.2185
I0523 23:01:11.126441 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2185 (* 1 = 11.2185 loss)
I0523 23:01:11.213217 11835 sgd_solver.cpp:112] Iteration 5670, lr = 0.1
I0523 23:01:17.260387 11835 solver.cpp:239] Iteration 5680 (1.63033 iter/s, 6.13373s/10 iters), loss = 10.5255
I0523 23:01:17.260442 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5255 (* 1 = 10.5255 loss)
I0523 23:01:17.350642 11835 sgd_solver.cpp:112] Iteration 5680, lr = 0.1
I0523 23:01:23.384999 11835 solver.cpp:239] Iteration 5690 (1.63283 iter/s, 6.12433s/10 iters), loss = 10.9973
I0523 23:01:23.385042 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9973 (* 1 = 10.9973 loss)
I0523 23:01:23.385056 11835 sgd_solver.cpp:112] Iteration 5690, lr = 0.1
I0523 23:01:29.862630 11835 solver.cpp:239] Iteration 5700 (1.54386 iter/s, 6.47727s/10 iters), loss = 10.4183
I0523 23:01:29.862680 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4183 (* 1 = 10.4183 loss)
I0523 23:01:29.864632 11835 sgd_solver.cpp:112] Iteration 5700, lr = 0.1
I0523 23:01:36.039921 11835 solver.cpp:239] Iteration 5710 (1.61891 iter/s, 6.177s/10 iters), loss = 10.2971
I0523 23:01:36.040187 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2971 (* 1 = 10.2971 loss)
I0523 23:01:36.040247 11835 sgd_solver.cpp:112] Iteration 5710, lr = 0.1
I0523 23:01:42.259261 11835 solver.cpp:239] Iteration 5720 (1.60825 iter/s, 6.21794s/10 iters), loss = 11.0176
I0523 23:01:42.259325 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0176 (* 1 = 11.0176 loss)
I0523 23:01:42.259415 11835 sgd_solver.cpp:112] Iteration 5720, lr = 0.1
I0523 23:01:48.249581 11835 solver.cpp:239] Iteration 5730 (1.66944 iter/s, 5.99004s/10 iters), loss = 11.403
I0523 23:01:48.249616 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.403 (* 1 = 11.403 loss)
I0523 23:01:48.249655 11835 sgd_solver.cpp:112] Iteration 5730, lr = 0.1
I0523 23:01:55.921134 11835 solver.cpp:239] Iteration 5740 (1.30357 iter/s, 7.67122s/10 iters), loss = 11.3093
I0523 23:01:55.921190 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3093 (* 1 = 11.3093 loss)
I0523 23:01:55.921316 11835 sgd_solver.cpp:112] Iteration 5740, lr = 0.1
I0523 23:02:04.096938 11835 solver.cpp:239] Iteration 5750 (1.22317 iter/s, 8.17545s/10 iters), loss = 11.1393
I0523 23:02:04.096988 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1393 (* 1 = 11.1393 loss)
I0523 23:02:04.097092 11835 sgd_solver.cpp:112] Iteration 5750, lr = 0.1
I0523 23:02:11.026887 11835 solver.cpp:239] Iteration 5760 (1.44307 iter/s, 6.92965s/10 iters), loss = 11.0075
I0523 23:02:11.027144 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0075 (* 1 = 11.0075 loss)
I0523 23:02:11.027196 11835 sgd_solver.cpp:112] Iteration 5760, lr = 0.1
I0523 23:02:18.755125 11835 solver.cpp:239] Iteration 5770 (1.29405 iter/s, 7.72768s/10 iters), loss = 11.3879
I0523 23:02:18.755174 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3879 (* 1 = 11.3879 loss)
I0523 23:02:18.854863 11835 sgd_solver.cpp:112] Iteration 5770, lr = 0.1
I0523 23:02:25.257817 11835 solver.cpp:239] Iteration 5780 (1.53789 iter/s, 6.50241s/10 iters), loss = 9.69558
I0523 23:02:25.257855 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.69558 (* 1 = 9.69558 loss)
I0523 23:02:25.257977 11835 sgd_solver.cpp:112] Iteration 5780, lr = 0.1
I0523 23:02:32.567364 11835 solver.cpp:239] Iteration 5790 (1.36813 iter/s, 7.30922s/10 iters), loss = 10.6185
I0523 23:02:32.567425 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6185 (* 1 = 10.6185 loss)
I0523 23:02:32.569360 11835 sgd_solver.cpp:112] Iteration 5790, lr = 0.1
I0523 23:02:38.616600 11835 solver.cpp:239] Iteration 5800 (1.65318 iter/s, 6.04894s/10 iters), loss = 11.2623
I0523 23:02:38.616663 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2623 (* 1 = 11.2623 loss)
I0523 23:02:38.616722 11835 sgd_solver.cpp:112] Iteration 5800, lr = 0.1
I0523 23:02:45.094303 11835 solver.cpp:239] Iteration 5810 (1.54383 iter/s, 6.47741s/10 iters), loss = 10.9236
I0523 23:02:45.094446 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9236 (* 1 = 10.9236 loss)
I0523 23:02:45.094460 11835 sgd_solver.cpp:112] Iteration 5810, lr = 0.1
I0523 23:02:51.262487 11835 solver.cpp:239] Iteration 5820 (1.62136 iter/s, 6.16766s/10 iters), loss = 10.7549
I0523 23:02:51.262543 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7549 (* 1 = 10.7549 loss)
I0523 23:02:51.262802 11835 sgd_solver.cpp:112] Iteration 5820, lr = 0.1
I0523 23:02:57.172260 11835 solver.cpp:239] Iteration 5830 (1.69219 iter/s, 5.9095s/10 iters), loss = 11.4517
I0523 23:02:57.172302 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4517 (* 1 = 11.4517 loss)
I0523 23:02:57.172361 11835 sgd_solver.cpp:112] Iteration 5830, lr = 0.1
I0523 23:03:03.560808 11835 solver.cpp:239] Iteration 5840 (1.56537 iter/s, 6.38826s/10 iters), loss = 10.407
I0523 23:03:03.560860 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.407 (* 1 = 10.407 loss)
I0523 23:03:03.560993 11835 sgd_solver.cpp:112] Iteration 5840, lr = 0.1
I0523 23:03:09.952399 11835 solver.cpp:239] Iteration 5850 (1.56463 iter/s, 6.3913s/10 iters), loss = 11.2915
I0523 23:03:09.952440 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2915 (* 1 = 11.2915 loss)
I0523 23:03:09.952581 11835 sgd_solver.cpp:112] Iteration 5850, lr = 0.1
I0523 23:03:16.616644 11835 solver.cpp:239] Iteration 5860 (1.50061 iter/s, 6.66395s/10 iters), loss = 10.4386
I0523 23:03:16.616921 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4386 (* 1 = 10.4386 loss)
I0523 23:03:16.616979 11835 sgd_solver.cpp:112] Iteration 5860, lr = 0.1
I0523 23:03:22.483086 11835 solver.cpp:239] Iteration 5870 (1.70488 iter/s, 5.86553s/10 iters), loss = 10.4098
I0523 23:03:22.483131 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4098 (* 1 = 10.4098 loss)
I0523 23:03:22.483245 11835 sgd_solver.cpp:112] Iteration 5870, lr = 0.1
I0523 23:03:28.421211 11835 solver.cpp:239] Iteration 5880 (1.68411 iter/s, 5.93784s/10 iters), loss = 10.8516
I0523 23:03:28.421270 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8516 (* 1 = 10.8516 loss)
I0523 23:03:28.421533 11835 sgd_solver.cpp:112] Iteration 5880, lr = 0.1
I0523 23:03:35.741323 11835 solver.cpp:239] Iteration 5890 (1.36616 iter/s, 7.31978s/10 iters), loss = 11.0575
I0523 23:03:35.741364 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0575 (* 1 = 11.0575 loss)
I0523 23:03:35.741426 11835 sgd_solver.cpp:112] Iteration 5890, lr = 0.1
I0523 23:03:41.692876 11835 solver.cpp:239] Iteration 5900 (1.68031 iter/s, 5.95128s/10 iters), loss = 10.8414
I0523 23:03:41.692922 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8414 (* 1 = 10.8414 loss)
I0523 23:03:41.692936 11835 sgd_solver.cpp:112] Iteration 5900, lr = 0.1
I0523 23:03:48.672379 11835 solver.cpp:239] Iteration 5910 (1.43283 iter/s, 6.97919s/10 iters), loss = 9.62216
I0523 23:03:48.672664 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.62216 (* 1 = 9.62216 loss)
I0523 23:03:48.672724 11835 sgd_solver.cpp:112] Iteration 5910, lr = 0.1
I0523 23:03:56.570768 11835 solver.cpp:239] Iteration 5920 (1.26622 iter/s, 7.89755s/10 iters), loss = 10.4917
I0523 23:03:56.570809 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4917 (* 1 = 10.4917 loss)
I0523 23:03:56.616020 11835 sgd_solver.cpp:112] Iteration 5920, lr = 0.1
I0523 23:04:02.977375 11835 solver.cpp:239] Iteration 5930 (1.56096 iter/s, 6.40631s/10 iters), loss = 10.2704
I0523 23:04:02.977427 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2704 (* 1 = 10.2704 loss)
I0523 23:04:02.977442 11835 sgd_solver.cpp:112] Iteration 5930, lr = 0.1
I0523 23:04:10.754174 11835 solver.cpp:239] Iteration 5940 (1.28593 iter/s, 7.77646s/10 iters), loss = 11.0188
I0523 23:04:10.754230 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0188 (* 1 = 11.0188 loss)
I0523 23:04:10.754397 11835 sgd_solver.cpp:112] Iteration 5940, lr = 0.1
I0523 23:04:18.588793 11835 solver.cpp:239] Iteration 5950 (1.27644 iter/s, 7.83428s/10 iters), loss = 10.1597
I0523 23:04:18.588838 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1597 (* 1 = 10.1597 loss)
I0523 23:04:18.588980 11835 sgd_solver.cpp:112] Iteration 5950, lr = 0.1
I0523 23:04:28.140995 11835 solver.cpp:239] Iteration 5960 (1.04692 iter/s, 9.5518s/10 iters), loss = 10.6051
I0523 23:04:28.141355 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6051 (* 1 = 10.6051 loss)
I0523 23:04:28.173303 11835 sgd_solver.cpp:112] Iteration 5960, lr = 0.1
I0523 23:04:36.753006 11835 solver.cpp:239] Iteration 5970 (1.16125 iter/s, 8.61142s/10 iters), loss = 10.4344
I0523 23:04:36.753060 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4344 (* 1 = 10.4344 loss)
I0523 23:04:36.753227 11835 sgd_solver.cpp:112] Iteration 5970, lr = 0.1
I0523 23:04:42.390525 11835 solver.cpp:239] Iteration 5980 (1.77391 iter/s, 5.63726s/10 iters), loss = 10.4271
I0523 23:04:42.390575 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4271 (* 1 = 10.4271 loss)
I0523 23:04:42.390790 11835 sgd_solver.cpp:112] Iteration 5980, lr = 0.1
I0523 23:04:48.742727 11835 solver.cpp:239] Iteration 5990 (1.57433 iter/s, 6.3519s/10 iters), loss = 11.1965
I0523 23:04:48.742780 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1965 (* 1 = 11.1965 loss)
I0523 23:04:48.742846 11835 sgd_solver.cpp:112] Iteration 5990, lr = 0.1
I0523 23:04:56.694501 11835 solver.cpp:239] Iteration 6000 (1.25764 iter/s, 7.95142s/10 iters), loss = 10.5029
I0523 23:04:56.694576 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5029 (* 1 = 10.5029 loss)
I0523 23:04:56.694850 11835 sgd_solver.cpp:112] Iteration 6000, lr = 0.1
I0523 23:05:03.480603 11835 solver.cpp:239] Iteration 6010 (1.47367 iter/s, 6.78579s/10 iters), loss = 10.081
I0523 23:05:03.480885 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.081 (* 1 = 10.081 loss)
I0523 23:05:03.966516 11835 sgd_solver.cpp:112] Iteration 6010, lr = 0.1
I0523 23:05:09.575685 11835 solver.cpp:239] Iteration 6020 (1.6408 iter/s, 6.0946s/10 iters), loss = 10.8068
I0523 23:05:09.575742 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8068 (* 1 = 10.8068 loss)
I0523 23:05:09.575879 11835 sgd_solver.cpp:112] Iteration 6020, lr = 0.1
I0523 23:05:15.385772 11835 solver.cpp:239] Iteration 6030 (1.72122 iter/s, 5.80982s/10 iters), loss = 9.24547
I0523 23:05:15.385812 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.24547 (* 1 = 9.24547 loss)
I0523 23:05:15.385956 11835 sgd_solver.cpp:112] Iteration 6030, lr = 0.1
I0523 23:05:23.254817 11835 solver.cpp:239] Iteration 6040 (1.27086 iter/s, 7.8687s/10 iters), loss = 10.9431
I0523 23:05:23.254873 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9431 (* 1 = 10.9431 loss)
I0523 23:05:23.255002 11835 sgd_solver.cpp:112] Iteration 6040, lr = 0.1
I0523 23:05:29.131515 11835 solver.cpp:239] Iteration 6050 (1.70172 iter/s, 5.87641s/10 iters), loss = 10.8782
I0523 23:05:29.131574 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8782 (* 1 = 10.8782 loss)
I0523 23:05:29.131772 11835 sgd_solver.cpp:112] Iteration 6050, lr = 0.1
I0523 23:05:37.251626 11835 solver.cpp:239] Iteration 6060 (1.23156 iter/s, 8.11975s/10 iters), loss = 10.5195
I0523 23:05:37.251914 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5195 (* 1 = 10.5195 loss)
I0523 23:05:37.251968 11835 sgd_solver.cpp:112] Iteration 6060, lr = 0.1
I0523 23:05:44.587292 11835 solver.cpp:239] Iteration 6070 (1.36351 iter/s, 7.334s/10 iters), loss = 11.2968
I0523 23:05:44.587339 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2968 (* 1 = 11.2968 loss)
I0523 23:05:44.587574 11835 sgd_solver.cpp:112] Iteration 6070, lr = 0.1
I0523 23:05:51.615701 11835 solver.cpp:239] Iteration 6080 (1.42286 iter/s, 7.02809s/10 iters), loss = 10.4073
I0523 23:05:51.615759 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4073 (* 1 = 10.4073 loss)
I0523 23:05:52.278681 11835 sgd_solver.cpp:112] Iteration 6080, lr = 0.1
I0523 23:06:00.834620 11835 solver.cpp:239] Iteration 6090 (1.08477 iter/s, 9.21852s/10 iters), loss = 11.303
I0523 23:06:00.834681 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.303 (* 1 = 11.303 loss)
I0523 23:06:00.834918 11835 sgd_solver.cpp:112] Iteration 6090, lr = 0.1
I0523 23:06:06.839905 11835 solver.cpp:239] Iteration 6100 (1.66528 iter/s, 6.00501s/10 iters), loss = 11.4714
I0523 23:06:06.839946 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.4714 (* 1 = 11.4714 loss)
I0523 23:06:06.839958 11835 sgd_solver.cpp:112] Iteration 6100, lr = 0.1
I0523 23:06:14.679654 11835 solver.cpp:239] Iteration 6110 (1.27561 iter/s, 7.83941s/10 iters), loss = 9.78606
I0523 23:06:14.679894 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.78606 (* 1 = 9.78606 loss)
I0523 23:06:14.679942 11835 sgd_solver.cpp:112] Iteration 6110, lr = 0.1
I0523 23:06:22.006832 11835 solver.cpp:239] Iteration 6120 (1.36487 iter/s, 7.32671s/10 iters), loss = 10.6169
I0523 23:06:22.006888 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6169 (* 1 = 10.6169 loss)
I0523 23:06:22.530809 11835 sgd_solver.cpp:112] Iteration 6120, lr = 0.1
I0523 23:06:28.457902 11835 solver.cpp:239] Iteration 6130 (1.5502 iter/s, 6.45077s/10 iters), loss = 9.94753
I0523 23:06:28.457947 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.94753 (* 1 = 9.94753 loss)
I0523 23:06:28.458201 11835 sgd_solver.cpp:112] Iteration 6130, lr = 0.1
I0523 23:06:34.174607 11835 solver.cpp:239] Iteration 6140 (1.74934 iter/s, 5.71643s/10 iters), loss = 10.2205
I0523 23:06:34.174666 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2205 (* 1 = 10.2205 loss)
I0523 23:06:34.356799 11835 sgd_solver.cpp:112] Iteration 6140, lr = 0.1
I0523 23:06:42.255939 11835 solver.cpp:239] Iteration 6150 (1.23747 iter/s, 8.08097s/10 iters), loss = 10.4426
I0523 23:06:42.255990 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4426 (* 1 = 10.4426 loss)
I0523 23:06:42.256228 11835 sgd_solver.cpp:112] Iteration 6150, lr = 0.1
I0523 23:06:48.879501 11835 solver.cpp:239] Iteration 6160 (1.50983 iter/s, 6.62326s/10 iters), loss = 9.93969
I0523 23:06:48.879756 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.93969 (* 1 = 9.93969 loss)
I0523 23:06:48.879803 11835 sgd_solver.cpp:112] Iteration 6160, lr = 0.1
I0523 23:06:54.613592 11835 solver.cpp:239] Iteration 6170 (1.7441 iter/s, 5.73361s/10 iters), loss = 11.1138
I0523 23:06:54.613647 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1138 (* 1 = 11.1138 loss)
I0523 23:06:54.614001 11835 sgd_solver.cpp:112] Iteration 6170, lr = 0.1
I0523 23:07:01.471838 11835 solver.cpp:239] Iteration 6180 (1.45817 iter/s, 6.85792s/10 iters), loss = 11.27
I0523 23:07:01.471936 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.27 (* 1 = 11.27 loss)
I0523 23:07:01.472074 11835 sgd_solver.cpp:112] Iteration 6180, lr = 0.1
I0523 23:07:08.639341 11835 solver.cpp:239] Iteration 6190 (1.39525 iter/s, 7.16716s/10 iters), loss = 11.264
I0523 23:07:08.639394 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.264 (* 1 = 11.264 loss)
I0523 23:07:08.639408 11835 sgd_solver.cpp:112] Iteration 6190, lr = 0.1
I0523 23:07:17.091323 11835 solver.cpp:239] Iteration 6200 (1.1833 iter/s, 8.45097s/10 iters), loss = 10.4856
I0523 23:07:17.091372 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4856 (* 1 = 10.4856 loss)
I0523 23:07:17.097692 11835 sgd_solver.cpp:112] Iteration 6200, lr = 0.1
I0523 23:07:24.587889 11835 solver.cpp:239] Iteration 6210 (1.334 iter/s, 7.49624s/10 iters), loss = 9.78341
I0523 23:07:24.587960 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.78341 (* 1 = 9.78341 loss)
I0523 23:07:24.588253 11835 sgd_solver.cpp:112] Iteration 6210, lr = 0.1
I0523 23:07:31.020614 11835 solver.cpp:239] Iteration 6220 (1.55463 iter/s, 6.43239s/10 iters), loss = 10.2621
I0523 23:07:31.020676 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2621 (* 1 = 10.2621 loss)
I0523 23:07:31.020732 11835 sgd_solver.cpp:112] Iteration 6220, lr = 0.1
I0523 23:07:36.816855 11835 solver.cpp:239] Iteration 6230 (1.72534 iter/s, 5.79596s/10 iters), loss = 11.1096
I0523 23:07:36.816911 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1096 (* 1 = 11.1096 loss)
I0523 23:07:36.817553 11835 sgd_solver.cpp:112] Iteration 6230, lr = 0.1
I0523 23:07:43.291422 11835 solver.cpp:239] Iteration 6240 (1.54457 iter/s, 6.47428s/10 iters), loss = 10.2139
I0523 23:07:43.291463 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2139 (* 1 = 10.2139 loss)
I0523 23:07:43.291579 11835 sgd_solver.cpp:112] Iteration 6240, lr = 0.1
I0523 23:07:49.076920 11835 solver.cpp:239] Iteration 6250 (1.72854 iter/s, 5.78523s/10 iters), loss = 10.8456
I0523 23:07:49.076978 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8456 (* 1 = 10.8456 loss)
I0523 23:07:50.241957 11835 sgd_solver.cpp:112] Iteration 6250, lr = 0.1
I0523 23:07:56.181988 11835 solver.cpp:239] Iteration 6260 (1.40752 iter/s, 7.10471s/10 iters), loss = 11.0686
I0523 23:07:56.182155 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0686 (* 1 = 11.0686 loss)
I0523 23:07:57.117517 11835 sgd_solver.cpp:112] Iteration 6260, lr = 0.1
I0523 23:08:04.123684 11835 solver.cpp:239] Iteration 6270 (1.25925 iter/s, 7.94124s/10 iters), loss = 11.3569
I0523 23:08:04.123734 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3569 (* 1 = 11.3569 loss)
I0523 23:08:04.711364 11835 sgd_solver.cpp:112] Iteration 6270, lr = 0.1
I0523 23:08:10.493386 11835 solver.cpp:239] Iteration 6280 (1.57 iter/s, 6.36942s/10 iters), loss = 10.1665
I0523 23:08:10.493428 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1665 (* 1 = 10.1665 loss)
I0523 23:08:10.493440 11835 sgd_solver.cpp:112] Iteration 6280, lr = 0.1
I0523 23:08:16.489328 11835 solver.cpp:239] Iteration 6290 (1.66805 iter/s, 5.99503s/10 iters), loss = 10.8489
I0523 23:08:16.489375 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8489 (* 1 = 10.8489 loss)
I0523 23:08:17.036053 11835 sgd_solver.cpp:112] Iteration 6290, lr = 0.1
I0523 23:08:23.389353 11835 solver.cpp:239] Iteration 6300 (1.44933 iter/s, 6.89972s/10 iters), loss = 10.0678
I0523 23:08:23.389394 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0678 (* 1 = 10.0678 loss)
I0523 23:08:23.389511 11835 sgd_solver.cpp:112] Iteration 6300, lr = 0.1
I0523 23:08:28.986358 11835 solver.cpp:239] Iteration 6310 (1.78675 iter/s, 5.59674s/10 iters), loss = 10.0865
I0523 23:08:28.986665 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0865 (* 1 = 10.0865 loss)
I0523 23:08:28.986766 11835 sgd_solver.cpp:112] Iteration 6310, lr = 0.1
I0523 23:08:36.349282 11835 solver.cpp:239] Iteration 6320 (1.35826 iter/s, 7.36235s/10 iters), loss = 10.3484
I0523 23:08:36.349339 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3484 (* 1 = 10.3484 loss)
I0523 23:08:36.349584 11835 sgd_solver.cpp:112] Iteration 6320, lr = 0.1
I0523 23:08:44.124037 11835 solver.cpp:239] Iteration 6330 (1.28627 iter/s, 7.77441s/10 iters), loss = 10.7715
I0523 23:08:44.124085 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7715 (* 1 = 10.7715 loss)
I0523 23:08:44.366951 11835 sgd_solver.cpp:112] Iteration 6330, lr = 0.1
I0523 23:08:52.274742 11835 solver.cpp:239] Iteration 6340 (1.22694 iter/s, 8.15033s/10 iters), loss = 9.98249
I0523 23:08:52.274801 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.98249 (* 1 = 9.98249 loss)
I0523 23:08:52.274819 11835 sgd_solver.cpp:112] Iteration 6340, lr = 0.1
I0523 23:08:59.178390 11835 solver.cpp:239] Iteration 6350 (1.44862 iter/s, 6.90313s/10 iters), loss = 9.76077
I0523 23:08:59.178505 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.76077 (* 1 = 9.76077 loss)
I0523 23:08:59.178629 11835 sgd_solver.cpp:112] Iteration 6350, lr = 0.1
I0523 23:09:08.347401 11835 solver.cpp:239] Iteration 6360 (1.09068 iter/s, 9.16855s/10 iters), loss = 10.4656
I0523 23:09:08.347460 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4656 (* 1 = 10.4656 loss)
I0523 23:09:08.347925 11835 sgd_solver.cpp:112] Iteration 6360, lr = 0.1
I0523 23:09:14.214201 11835 solver.cpp:239] Iteration 6370 (1.70459 iter/s, 5.86653s/10 iters), loss = 10.698
I0523 23:09:14.214239 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.698 (* 1 = 10.698 loss)
I0523 23:09:14.214251 11835 sgd_solver.cpp:112] Iteration 6370, lr = 0.1
I0523 23:09:22.869381 11835 solver.cpp:239] Iteration 6380 (1.15543 iter/s, 8.6548s/10 iters), loss = 10.8117
I0523 23:09:22.869434 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8117 (* 1 = 10.8117 loss)
I0523 23:09:22.869735 11835 sgd_solver.cpp:112] Iteration 6380, lr = 0.1
I0523 23:09:29.928936 11835 solver.cpp:239] Iteration 6390 (1.41659 iter/s, 7.05923s/10 iters), loss = 10.7094
I0523 23:09:29.929077 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7094 (* 1 = 10.7094 loss)
I0523 23:09:30.558928 11835 sgd_solver.cpp:112] Iteration 6390, lr = 0.1
I0523 23:09:36.392274 11835 solver.cpp:239] Iteration 6400 (1.54728 iter/s, 6.46295s/10 iters), loss = 10.3604
I0523 23:09:36.392328 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3604 (* 1 = 10.3604 loss)
I0523 23:09:36.392343 11835 sgd_solver.cpp:112] Iteration 6400, lr = 0.1
I0523 23:09:48.567404 11835 solver.cpp:239] Iteration 6410 (0.82153 iter/s, 12.1724s/10 iters), loss = 11.0581
I0523 23:09:48.567456 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0581 (* 1 = 11.0581 loss)
I0523 23:09:48.567584 11835 sgd_solver.cpp:112] Iteration 6410, lr = 0.1
I0523 23:09:55.170754 11835 solver.cpp:239] Iteration 6420 (1.51445 iter/s, 6.60305s/10 iters), loss = 10.6092
I0523 23:09:55.170805 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6092 (* 1 = 10.6092 loss)
I0523 23:09:55.170825 11835 sgd_solver.cpp:112] Iteration 6420, lr = 0.1
I0523 23:10:02.880050 11835 solver.cpp:239] Iteration 6430 (1.29719 iter/s, 7.70894s/10 iters), loss = 10.712
I0523 23:10:02.880333 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.712 (* 1 = 10.712 loss)
I0523 23:10:03.573482 11835 sgd_solver.cpp:112] Iteration 6430, lr = 0.1
I0523 23:10:11.695893 11835 solver.cpp:239] Iteration 6440 (1.13439 iter/s, 8.81528s/10 iters), loss = 10.3168
I0523 23:10:11.695932 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3168 (* 1 = 10.3168 loss)
I0523 23:10:11.696076 11835 sgd_solver.cpp:112] Iteration 6440, lr = 0.1
I0523 23:10:18.808315 11835 solver.cpp:239] Iteration 6450 (1.40606 iter/s, 7.1121s/10 iters), loss = 11.2501
I0523 23:10:18.808372 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.2501 (* 1 = 11.2501 loss)
I0523 23:10:18.808873 11835 sgd_solver.cpp:112] Iteration 6450, lr = 0.1
I0523 23:10:25.831233 11835 solver.cpp:239] Iteration 6460 (1.42398 iter/s, 7.02259s/10 iters), loss = 9.8364
I0523 23:10:25.831290 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.8364 (* 1 = 9.8364 loss)
I0523 23:10:25.831377 11835 sgd_solver.cpp:112] Iteration 6460, lr = 0.1
I0523 23:10:32.094452 11835 solver.cpp:239] Iteration 6470 (1.5967 iter/s, 6.26293s/10 iters), loss = 10.3405
I0523 23:10:32.094499 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3405 (* 1 = 10.3405 loss)
I0523 23:10:32.094715 11835 sgd_solver.cpp:112] Iteration 6470, lr = 0.1
I0523 23:10:40.053146 11835 solver.cpp:239] Iteration 6480 (1.25654 iter/s, 7.95834s/10 iters), loss = 10.9892
I0523 23:10:40.053284 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9892 (* 1 = 10.9892 loss)
I0523 23:10:40.072569 11835 sgd_solver.cpp:112] Iteration 6480, lr = 0.1
I0523 23:10:47.802328 11835 solver.cpp:239] Iteration 6490 (1.29053 iter/s, 7.74876s/10 iters), loss = 10.5551
I0523 23:10:47.802371 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5551 (* 1 = 10.5551 loss)
I0523 23:10:47.802388 11835 sgd_solver.cpp:112] Iteration 6490, lr = 0.1
I0523 23:10:54.057775 11835 solver.cpp:239] Iteration 6500 (1.59868 iter/s, 6.25515s/10 iters), loss = 10.2177
I0523 23:10:54.057827 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2177 (* 1 = 10.2177 loss)
I0523 23:10:54.787506 11835 sgd_solver.cpp:112] Iteration 6500, lr = 0.1
I0523 23:11:05.481639 11835 solver.cpp:239] Iteration 6510 (0.875397 iter/s, 11.4234s/10 iters), loss = 10.336
I0523 23:11:05.481690 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.336 (* 1 = 10.336 loss)
I0523 23:11:05.481817 11835 sgd_solver.cpp:112] Iteration 6510, lr = 0.1
I0523 23:11:11.233382 11835 solver.cpp:239] Iteration 6520 (1.73868 iter/s, 5.75148s/10 iters), loss = 10.8411
I0523 23:11:11.233631 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8411 (* 1 = 10.8411 loss)
I0523 23:11:11.233682 11835 sgd_solver.cpp:112] Iteration 6520, lr = 0.1
I0523 23:11:18.964977 11835 solver.cpp:239] Iteration 6530 (1.29348 iter/s, 7.7311s/10 iters), loss = 9.72212
I0523 23:11:18.965029 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.72212 (* 1 = 9.72212 loss)
I0523 23:11:18.965536 11835 sgd_solver.cpp:112] Iteration 6530, lr = 0.1
I0523 23:11:25.100895 11835 solver.cpp:239] Iteration 6540 (1.62982 iter/s, 6.13563s/10 iters), loss = 10.2403
I0523 23:11:25.100941 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2403 (* 1 = 10.2403 loss)
I0523 23:11:25.100958 11835 sgd_solver.cpp:112] Iteration 6540, lr = 0.1
I0523 23:11:34.199234 11835 solver.cpp:239] Iteration 6550 (1.09916 iter/s, 9.09786s/10 iters), loss = 10.7251
I0523 23:11:34.199296 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7251 (* 1 = 10.7251 loss)
I0523 23:11:34.199337 11835 sgd_solver.cpp:112] Iteration 6550, lr = 0.1
I0523 23:11:43.476536 11835 solver.cpp:239] Iteration 6560 (1.07795 iter/s, 9.2769s/10 iters), loss = 9.64075
I0523 23:11:43.476665 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.64075 (* 1 = 9.64075 loss)
I0523 23:11:43.757658 11835 sgd_solver.cpp:112] Iteration 6560, lr = 0.1
I0523 23:11:50.149900 11835 solver.cpp:239] Iteration 6570 (1.49858 iter/s, 6.67297s/10 iters), loss = 9.60945
I0523 23:11:50.149955 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.60945 (* 1 = 9.60945 loss)
I0523 23:11:50.566130 11835 sgd_solver.cpp:112] Iteration 6570, lr = 0.1
I0523 23:11:56.627737 11835 solver.cpp:239] Iteration 6580 (1.5438 iter/s, 6.47754s/10 iters), loss = 10.4042
I0523 23:11:56.627789 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4042 (* 1 = 10.4042 loss)
I0523 23:11:56.837106 11835 sgd_solver.cpp:112] Iteration 6580, lr = 0.1
I0523 23:12:04.081897 11835 solver.cpp:239] Iteration 6590 (1.3416 iter/s, 7.45381s/10 iters), loss = 10.2041
I0523 23:12:04.081951 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2041 (* 1 = 10.2041 loss)
I0523 23:12:04.082043 11835 sgd_solver.cpp:112] Iteration 6590, lr = 0.1
I0523 23:12:12.617399 11835 solver.cpp:239] Iteration 6600 (1.17163 iter/s, 8.53512s/10 iters), loss = 10.0683
I0523 23:12:12.617461 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0683 (* 1 = 10.0683 loss)
I0523 23:12:12.617887 11835 sgd_solver.cpp:112] Iteration 6600, lr = 0.1
I0523 23:12:18.954344 11835 solver.cpp:239] Iteration 6610 (1.57812 iter/s, 6.33665s/10 iters), loss = 10.0007
I0523 23:12:18.954584 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0007 (* 1 = 10.0007 loss)
I0523 23:12:19.070567 11835 sgd_solver.cpp:112] Iteration 6610, lr = 0.1
I0523 23:12:26.229317 11835 solver.cpp:239] Iteration 6620 (1.37467 iter/s, 7.27447s/10 iters), loss = 10.4161
I0523 23:12:26.229378 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4161 (* 1 = 10.4161 loss)
I0523 23:12:26.229472 11835 sgd_solver.cpp:112] Iteration 6620, lr = 0.1
I0523 23:12:32.904085 11835 solver.cpp:239] Iteration 6630 (1.49825 iter/s, 6.67446s/10 iters), loss = 10.6155
I0523 23:12:32.904134 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6155 (* 1 = 10.6155 loss)
I0523 23:12:32.904353 11835 sgd_solver.cpp:112] Iteration 6630, lr = 0.1
I0523 23:12:39.192767 11835 solver.cpp:239] Iteration 6640 (1.59023 iter/s, 6.28838s/10 iters), loss = 10.7311
I0523 23:12:39.192826 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7311 (* 1 = 10.7311 loss)
I0523 23:12:39.192876 11835 sgd_solver.cpp:112] Iteration 6640, lr = 0.1
I0523 23:12:44.932384 11835 solver.cpp:239] Iteration 6650 (1.74236 iter/s, 5.73935s/10 iters), loss = 10.3625
I0523 23:12:44.932432 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3625 (* 1 = 10.3625 loss)
I0523 23:12:44.932521 11835 sgd_solver.cpp:112] Iteration 6650, lr = 0.1
I0523 23:12:51.040657 11835 solver.cpp:239] Iteration 6660 (1.6372 iter/s, 6.10799s/10 iters), loss = 10.9393
I0523 23:12:51.040936 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9393 (* 1 = 10.9393 loss)
I0523 23:12:51.040997 11835 sgd_solver.cpp:112] Iteration 6660, lr = 0.1
I0523 23:12:56.998781 11835 solver.cpp:239] Iteration 6670 (1.67857 iter/s, 5.95745s/10 iters), loss = 10.3942
I0523 23:12:56.998872 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3942 (* 1 = 10.3942 loss)
I0523 23:12:56.999199 11835 sgd_solver.cpp:112] Iteration 6670, lr = 0.1
I0523 23:13:03.665931 11835 solver.cpp:239] Iteration 6680 (1.49997 iter/s, 6.66681s/10 iters), loss = 11.255
I0523 23:13:03.665985 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.255 (* 1 = 11.255 loss)
I0523 23:13:03.765651 11835 sgd_solver.cpp:112] Iteration 6680, lr = 0.1
I0523 23:13:09.850731 11835 solver.cpp:239] Iteration 6690 (1.61694 iter/s, 6.18451s/10 iters), loss = 10.0116
I0523 23:13:09.850782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0116 (* 1 = 10.0116 loss)
I0523 23:13:09.850905 11835 sgd_solver.cpp:112] Iteration 6690, lr = 0.1
I0523 23:13:17.352696 11835 solver.cpp:239] Iteration 6700 (1.33304 iter/s, 7.50164s/10 iters), loss = 10.7338
I0523 23:13:17.352738 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7338 (* 1 = 10.7338 loss)
I0523 23:13:17.561002 11835 sgd_solver.cpp:112] Iteration 6700, lr = 0.1
I0523 23:13:24.002784 11835 solver.cpp:239] Iteration 6710 (1.50381 iter/s, 6.64979s/10 iters), loss = 10.8826
I0523 23:13:24.003072 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.8826 (* 1 = 10.8826 loss)
I0523 23:13:24.003125 11835 sgd_solver.cpp:112] Iteration 6710, lr = 0.1
I0523 23:13:30.077571 11835 solver.cpp:239] Iteration 6720 (1.64629 iter/s, 6.07428s/10 iters), loss = 10.51
I0523 23:13:30.077632 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.51 (* 1 = 10.51 loss)
I0523 23:13:30.077791 11835 sgd_solver.cpp:112] Iteration 6720, lr = 0.1
I0523 23:13:36.567600 11835 solver.cpp:239] Iteration 6730 (1.5409 iter/s, 6.48973s/10 iters), loss = 10.4442
I0523 23:13:36.567646 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4442 (* 1 = 10.4442 loss)
I0523 23:13:36.976347 11835 sgd_solver.cpp:112] Iteration 6730, lr = 0.1
I0523 23:13:44.369362 11835 solver.cpp:239] Iteration 6740 (1.28182 iter/s, 7.80141s/10 iters), loss = 9.20333
I0523 23:13:44.369411 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.20333 (* 1 = 9.20333 loss)
I0523 23:13:44.457350 11835 sgd_solver.cpp:112] Iteration 6740, lr = 0.1
I0523 23:13:53.066885 11835 solver.cpp:239] Iteration 6750 (1.1498 iter/s, 8.69714s/10 iters), loss = 9.39269
I0523 23:13:53.066953 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.39269 (* 1 = 9.39269 loss)
I0523 23:13:53.067154 11835 sgd_solver.cpp:112] Iteration 6750, lr = 0.1
I0523 23:13:59.816866 11835 solver.cpp:239] Iteration 6760 (1.48155 iter/s, 6.74967s/10 iters), loss = 10.5693
I0523 23:13:59.817109 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5693 (* 1 = 10.5693 loss)
I0523 23:13:59.817273 11835 sgd_solver.cpp:112] Iteration 6760, lr = 0.1
I0523 23:14:07.098445 11835 solver.cpp:239] Iteration 6770 (1.37342 iter/s, 7.2811s/10 iters), loss = 11.3963
I0523 23:14:07.098487 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.3963 (* 1 = 11.3963 loss)
I0523 23:14:07.098886 11835 sgd_solver.cpp:112] Iteration 6770, lr = 0.1
I0523 23:14:14.564517 11835 solver.cpp:239] Iteration 6780 (1.33945 iter/s, 7.46573s/10 iters), loss = 10.784
I0523 23:14:14.564579 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.784 (* 1 = 10.784 loss)
I0523 23:14:14.564723 11835 sgd_solver.cpp:112] Iteration 6780, lr = 0.1
I0523 23:14:20.742797 11835 solver.cpp:239] Iteration 6790 (1.61865 iter/s, 6.17798s/10 iters), loss = 10.2329
I0523 23:14:20.742857 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2329 (* 1 = 10.2329 loss)
I0523 23:14:20.743068 11835 sgd_solver.cpp:112] Iteration 6790, lr = 0.1
I0523 23:14:26.495533 11835 solver.cpp:239] Iteration 6800 (1.73839 iter/s, 5.75246s/10 iters), loss = 10.4654
I0523 23:14:26.495581 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4654 (* 1 = 10.4654 loss)
I0523 23:14:26.495597 11835 sgd_solver.cpp:112] Iteration 6800, lr = 0.1
I0523 23:14:33.082640 11835 solver.cpp:239] Iteration 6810 (1.5182 iter/s, 6.58675s/10 iters), loss = 10.3679
I0523 23:14:33.082911 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3679 (* 1 = 10.3679 loss)
I0523 23:14:33.167119 11835 sgd_solver.cpp:112] Iteration 6810, lr = 0.1
I0523 23:14:40.245539 11835 solver.cpp:239] Iteration 6820 (1.39618 iter/s, 7.16241s/10 iters), loss = 10.3384
I0523 23:14:40.245589 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3384 (* 1 = 10.3384 loss)
I0523 23:14:40.245684 11835 sgd_solver.cpp:112] Iteration 6820, lr = 0.1
I0523 23:14:47.552356 11835 solver.cpp:239] Iteration 6830 (1.36865 iter/s, 7.30649s/10 iters), loss = 10.3445
I0523 23:14:47.552417 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3445 (* 1 = 10.3445 loss)
I0523 23:14:47.552667 11835 sgd_solver.cpp:112] Iteration 6830, lr = 0.1
I0523 23:14:53.419682 11835 solver.cpp:239] Iteration 6840 (1.70444 iter/s, 5.86704s/10 iters), loss = 10.1
I0523 23:14:53.419740 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1 (* 1 = 10.1 loss)
I0523 23:14:53.419785 11835 sgd_solver.cpp:112] Iteration 6840, lr = 0.1
I0523 23:14:59.541213 11835 solver.cpp:239] Iteration 6850 (1.63365 iter/s, 6.12125s/10 iters), loss = 10.5217
I0523 23:14:59.541250 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5217 (* 1 = 10.5217 loss)
I0523 23:14:59.541265 11835 sgd_solver.cpp:112] Iteration 6850, lr = 0.1
I0523 23:15:05.603397 11835 solver.cpp:239] Iteration 6860 (1.64967 iter/s, 6.06183s/10 iters), loss = 10.4637
I0523 23:15:05.603642 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4637 (* 1 = 10.4637 loss)
I0523 23:15:05.603691 11835 sgd_solver.cpp:112] Iteration 6860, lr = 0.1
I0523 23:15:12.404839 11835 solver.cpp:239] Iteration 6870 (1.47038 iter/s, 6.80098s/10 iters), loss = 11.0512
I0523 23:15:12.404892 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0512 (* 1 = 11.0512 loss)
I0523 23:15:12.519459 11835 sgd_solver.cpp:112] Iteration 6870, lr = 0.1
I0523 23:15:19.343858 11835 solver.cpp:239] Iteration 6880 (1.44119 iter/s, 6.9387s/10 iters), loss = 10.2046
I0523 23:15:19.343922 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2046 (* 1 = 10.2046 loss)
I0523 23:15:20.110918 11835 sgd_solver.cpp:112] Iteration 6880, lr = 0.1
I0523 23:15:26.341506 11835 solver.cpp:239] Iteration 6890 (1.42912 iter/s, 6.99733s/10 iters), loss = 9.42126
I0523 23:15:26.341545 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.42126 (* 1 = 9.42126 loss)
I0523 23:15:26.341557 11835 sgd_solver.cpp:112] Iteration 6890, lr = 0.1
I0523 23:15:32.898455 11835 solver.cpp:239] Iteration 6900 (1.52519 iter/s, 6.55655s/10 iters), loss = 11.0282
I0523 23:15:32.898514 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0282 (* 1 = 11.0282 loss)
I0523 23:15:32.898530 11835 sgd_solver.cpp:112] Iteration 6900, lr = 0.1
I0523 23:15:39.072449 11835 solver.cpp:239] Iteration 6910 (1.61994 iter/s, 6.17307s/10 iters), loss = 10.459
I0523 23:15:39.072583 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.459 (* 1 = 10.459 loss)
I0523 23:15:39.072747 11835 sgd_solver.cpp:112] Iteration 6910, lr = 0.1
I0523 23:15:46.231719 11835 solver.cpp:239] Iteration 6920 (1.39687 iter/s, 7.15887s/10 iters), loss = 10.4761
I0523 23:15:46.231767 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4761 (* 1 = 10.4761 loss)
I0523 23:15:46.231863 11835 sgd_solver.cpp:112] Iteration 6920, lr = 0.1
I0523 23:15:53.720360 11835 solver.cpp:239] Iteration 6930 (1.33541 iter/s, 7.48832s/10 iters), loss = 10.2832
I0523 23:15:53.720402 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2832 (* 1 = 10.2832 loss)
I0523 23:15:53.720530 11835 sgd_solver.cpp:112] Iteration 6930, lr = 0.1
I0523 23:15:59.712303 11835 solver.cpp:239] Iteration 6940 (1.66899 iter/s, 5.99166s/10 iters), loss = 10.1782
I0523 23:15:59.712360 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1782 (* 1 = 10.1782 loss)
I0523 23:15:59.712409 11835 sgd_solver.cpp:112] Iteration 6940, lr = 0.1
I0523 23:16:07.674373 11835 solver.cpp:239] Iteration 6950 (1.25601 iter/s, 7.96172s/10 iters), loss = 9.93063
I0523 23:16:07.674429 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.93063 (* 1 = 9.93063 loss)
I0523 23:16:07.674444 11835 sgd_solver.cpp:112] Iteration 6950, lr = 0.1
I0523 23:16:14.070387 11835 solver.cpp:239] Iteration 6960 (1.56362 iter/s, 6.39542s/10 iters), loss = 10.0182
I0523 23:16:14.070597 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0182 (* 1 = 10.0182 loss)
I0523 23:16:14.070791 11835 sgd_solver.cpp:112] Iteration 6960, lr = 0.1
I0523 23:16:19.699787 11835 solver.cpp:239] Iteration 6970 (1.77652 iter/s, 5.62897s/10 iters), loss = 10.3684
I0523 23:16:19.699841 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3684 (* 1 = 10.3684 loss)
I0523 23:16:19.700181 11835 sgd_solver.cpp:112] Iteration 6970, lr = 0.1
I0523 23:16:30.927309 11835 solver.cpp:239] Iteration 6980 (0.890706 iter/s, 11.227s/10 iters), loss = 10.0465
I0523 23:16:30.927367 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0465 (* 1 = 10.0465 loss)
I0523 23:16:30.927713 11835 sgd_solver.cpp:112] Iteration 6980, lr = 0.1
I0523 23:16:38.826093 11835 solver.cpp:239] Iteration 6990 (1.26608 iter/s, 7.89843s/10 iters), loss = 10.6282
I0523 23:16:38.826155 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6282 (* 1 = 10.6282 loss)
I0523 23:16:38.826442 11835 sgd_solver.cpp:112] Iteration 6990, lr = 0.1
I0523 23:16:45.371300 11835 solver.cpp:239] Iteration 7000 (1.52791 iter/s, 6.54491s/10 iters), loss = 10.1135
I0523 23:16:45.371501 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1135 (* 1 = 10.1135 loss)
I0523 23:16:45.371598 11835 sgd_solver.cpp:112] Iteration 7000, lr = 0.1
I0523 23:16:51.480309 11835 solver.cpp:239] Iteration 7010 (1.63704 iter/s, 6.1086s/10 iters), loss = 10.4164
I0523 23:16:51.480363 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4164 (* 1 = 10.4164 loss)
I0523 23:16:51.480511 11835 sgd_solver.cpp:112] Iteration 7010, lr = 0.1
I0523 23:16:57.923564 11835 solver.cpp:239] Iteration 7020 (1.55208 iter/s, 6.44297s/10 iters), loss = 10.5858
I0523 23:16:57.923604 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5858 (* 1 = 10.5858 loss)
I0523 23:16:58.337285 11835 sgd_solver.cpp:112] Iteration 7020, lr = 0.1
I0523 23:17:05.142743 11835 solver.cpp:239] Iteration 7030 (1.38526 iter/s, 7.21886s/10 iters), loss = 10.3069
I0523 23:17:05.142794 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3069 (* 1 = 10.3069 loss)
I0523 23:17:05.142808 11835 sgd_solver.cpp:112] Iteration 7030, lr = 0.1
I0523 23:17:12.732950 11835 solver.cpp:239] Iteration 7040 (1.31766 iter/s, 7.58923s/10 iters), loss = 10.7051
I0523 23:17:12.733003 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7051 (* 1 = 10.7051 loss)
I0523 23:17:13.105576 11835 sgd_solver.cpp:112] Iteration 7040, lr = 0.1
I0523 23:17:20.025197 11835 solver.cpp:239] Iteration 7050 (1.37138 iter/s, 7.29193s/10 iters), loss = 9.8537
I0523 23:17:20.025329 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.8537 (* 1 = 9.8537 loss)
I0523 23:17:20.025343 11835 sgd_solver.cpp:112] Iteration 7050, lr = 0.1
I0523 23:17:25.675335 11835 solver.cpp:239] Iteration 7060 (1.76998 iter/s, 5.64978s/10 iters), loss = 10.0456
I0523 23:17:25.675388 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0456 (* 1 = 10.0456 loss)
I0523 23:17:26.071312 11835 sgd_solver.cpp:112] Iteration 7060, lr = 0.1
I0523 23:17:32.669510 11835 solver.cpp:239] Iteration 7070 (1.42982 iter/s, 6.99386s/10 iters), loss = 11.0279
I0523 23:17:32.669559 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.0279 (* 1 = 11.0279 loss)
I0523 23:17:32.669805 11835 sgd_solver.cpp:112] Iteration 7070, lr = 0.1
I0523 23:17:39.016376 11835 solver.cpp:239] Iteration 7080 (1.57566 iter/s, 6.34657s/10 iters), loss = 9.86841
I0523 23:17:39.016432 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.86841 (* 1 = 9.86841 loss)
I0523 23:17:39.016845 11835 sgd_solver.cpp:112] Iteration 7080, lr = 0.1
I0523 23:17:46.118733 11835 solver.cpp:239] Iteration 7090 (1.40805 iter/s, 7.10201s/10 iters), loss = 11.1436
I0523 23:17:46.118782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.1436 (* 1 = 11.1436 loss)
I0523 23:17:46.118962 11835 sgd_solver.cpp:112] Iteration 7090, lr = 0.1
I0523 23:17:54.169806 11835 solver.cpp:239] Iteration 7100 (1.24212 iter/s, 8.05073s/10 iters), loss = 10.5041
I0523 23:17:54.170138 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5041 (* 1 = 10.5041 loss)
I0523 23:17:54.469139 11835 sgd_solver.cpp:112] Iteration 7100, lr = 0.1
I0523 23:18:02.063611 11835 solver.cpp:239] Iteration 7110 (1.26691 iter/s, 7.89323s/10 iters), loss = 9.87557
I0523 23:18:02.063666 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.87557 (* 1 = 9.87557 loss)
I0523 23:18:02.201548 11835 sgd_solver.cpp:112] Iteration 7110, lr = 0.1
I0523 23:18:08.673003 11835 solver.cpp:239] Iteration 7120 (1.51307 iter/s, 6.60908s/10 iters), loss = 10.6269
I0523 23:18:08.673054 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6269 (* 1 = 10.6269 loss)
I0523 23:18:08.673167 11835 sgd_solver.cpp:112] Iteration 7120, lr = 0.1
I0523 23:18:15.940141 11835 solver.cpp:239] Iteration 7130 (1.37612 iter/s, 7.26682s/10 iters), loss = 10.5716
I0523 23:18:15.940186 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5716 (* 1 = 10.5716 loss)
I0523 23:18:15.940574 11835 sgd_solver.cpp:112] Iteration 7130, lr = 0.1
I0523 23:18:22.776242 11835 solver.cpp:239] Iteration 7140 (1.46289 iter/s, 6.83578s/10 iters), loss = 10.1106
I0523 23:18:22.776298 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1106 (* 1 = 10.1106 loss)
I0523 23:18:22.776408 11835 sgd_solver.cpp:112] Iteration 7140, lr = 0.1
I0523 23:18:29.071755 11835 solver.cpp:239] Iteration 7150 (1.58851 iter/s, 6.29522s/10 iters), loss = 10.5548
I0523 23:18:29.072007 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5548 (* 1 = 10.5548 loss)
I0523 23:18:29.072064 11835 sgd_solver.cpp:112] Iteration 7150, lr = 0.1
I0523 23:18:36.955384 11835 solver.cpp:239] Iteration 7160 (1.26853 iter/s, 7.88311s/10 iters), loss = 10.1878
I0523 23:18:36.955441 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1878 (* 1 = 10.1878 loss)
I0523 23:18:36.955687 11835 sgd_solver.cpp:112] Iteration 7160, lr = 0.1
I0523 23:18:46.902040 11835 solver.cpp:239] Iteration 7170 (1.00541 iter/s, 9.94622s/10 iters), loss = 10.971
I0523 23:18:46.902096 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.971 (* 1 = 10.971 loss)
I0523 23:18:46.902333 11835 sgd_solver.cpp:112] Iteration 7170, lr = 0.1
I0523 23:18:55.014820 11835 solver.cpp:239] Iteration 7180 (1.23268 iter/s, 8.11242s/10 iters), loss = 10.2719
I0523 23:18:55.014876 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2719 (* 1 = 10.2719 loss)
I0523 23:18:55.578541 11835 sgd_solver.cpp:112] Iteration 7180, lr = 0.1
I0523 23:19:03.916095 11835 solver.cpp:239] Iteration 7190 (1.12348 iter/s, 8.90089s/10 iters), loss = 10.1075
I0523 23:19:03.916343 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1075 (* 1 = 10.1075 loss)
I0523 23:19:04.568761 11835 sgd_solver.cpp:112] Iteration 7190, lr = 0.1
I0523 23:19:13.302899 11835 solver.cpp:239] Iteration 7200 (1.06539 iter/s, 9.38624s/10 iters), loss = 9.99347
I0523 23:19:13.302948 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.99347 (* 1 = 9.99347 loss)
I0523 23:19:13.303189 11835 sgd_solver.cpp:112] Iteration 7200, lr = 0.1
I0523 23:19:19.487030 11835 solver.cpp:239] Iteration 7210 (1.61712 iter/s, 6.18385s/10 iters), loss = 10.5491
I0523 23:19:19.487082 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5491 (* 1 = 10.5491 loss)
I0523 23:19:19.487097 11835 sgd_solver.cpp:112] Iteration 7210, lr = 0.1
I0523 23:19:26.377991 11835 solver.cpp:239] Iteration 7220 (1.45125 iter/s, 6.89063s/10 iters), loss = 9.95875
I0523 23:19:26.378043 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.95875 (* 1 = 9.95875 loss)
I0523 23:19:26.663905 11835 sgd_solver.cpp:112] Iteration 7220, lr = 0.1
I0523 23:19:33.706308 11835 solver.cpp:239] Iteration 7230 (1.36463 iter/s, 7.32799s/10 iters), loss = 10.4082
I0523 23:19:33.706359 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4082 (* 1 = 10.4082 loss)
I0523 23:19:33.706614 11835 sgd_solver.cpp:112] Iteration 7230, lr = 0.1
I0523 23:19:40.230415 11835 solver.cpp:239] Iteration 7240 (1.53285 iter/s, 6.5238s/10 iters), loss = 10.0244
I0523 23:19:40.230582 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0244 (* 1 = 10.0244 loss)
I0523 23:19:40.231071 11835 sgd_solver.cpp:112] Iteration 7240, lr = 0.1
I0523 23:19:47.241586 11835 solver.cpp:239] Iteration 7250 (1.42638 iter/s, 7.01075s/10 iters), loss = 10.4323
I0523 23:19:47.241643 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4323 (* 1 = 10.4323 loss)
I0523 23:19:47.679602 11835 sgd_solver.cpp:112] Iteration 7250, lr = 0.1
I0523 23:19:53.925976 11835 solver.cpp:239] Iteration 7260 (1.4961 iter/s, 6.68406s/10 iters), loss = 9.30779
I0523 23:19:53.926029 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.30779 (* 1 = 9.30779 loss)
I0523 23:19:53.927966 11835 sgd_solver.cpp:112] Iteration 7260, lr = 0.1
I0523 23:20:00.581634 11835 solver.cpp:239] Iteration 7270 (1.50255 iter/s, 6.65536s/10 iters), loss = 10.2634
I0523 23:20:00.581679 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2634 (* 1 = 10.2634 loss)
I0523 23:20:00.897171 11835 sgd_solver.cpp:112] Iteration 7270, lr = 0.1
I0523 23:20:07.962468 11835 solver.cpp:239] Iteration 7280 (1.35492 iter/s, 7.38051s/10 iters), loss = 9.72503
I0523 23:20:07.962519 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.72503 (* 1 = 9.72503 loss)
I0523 23:20:08.526372 11835 sgd_solver.cpp:112] Iteration 7280, lr = 0.1
I0523 23:20:18.268607 11835 solver.cpp:239] Iteration 7290 (0.970337 iter/s, 10.3057s/10 iters), loss = 10.0455
I0523 23:20:18.268874 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0455 (* 1 = 10.0455 loss)
I0523 23:20:18.291488 11835 sgd_solver.cpp:112] Iteration 7290, lr = 0.1
I0523 23:20:26.928422 11835 solver.cpp:239] Iteration 7300 (1.15483 iter/s, 8.65926s/10 iters), loss = 9.96215
I0523 23:20:26.928477 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.96215 (* 1 = 9.96215 loss)
I0523 23:20:26.964967 11835 sgd_solver.cpp:112] Iteration 7300, lr = 0.1
I0523 23:20:33.858489 11835 solver.cpp:239] Iteration 7310 (1.44305 iter/s, 6.92975s/10 iters), loss = 10.7686
I0523 23:20:33.858537 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7686 (* 1 = 10.7686 loss)
I0523 23:20:34.100612 11835 sgd_solver.cpp:112] Iteration 7310, lr = 0.1
I0523 23:20:42.023124 11835 solver.cpp:239] Iteration 7320 (1.22485 iter/s, 8.16427s/10 iters), loss = 10.5084
I0523 23:20:42.023182 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5084 (* 1 = 10.5084 loss)
I0523 23:20:42.023424 11835 sgd_solver.cpp:112] Iteration 7320, lr = 0.1
I0523 23:20:48.179530 11835 solver.cpp:239] Iteration 7330 (1.6244 iter/s, 6.15612s/10 iters), loss = 10.3491
I0523 23:20:48.179575 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3491 (* 1 = 10.3491 loss)
I0523 23:20:48.644635 11835 sgd_solver.cpp:112] Iteration 7330, lr = 0.1
I0523 23:20:54.474566 11835 solver.cpp:239] Iteration 7340 (1.58863 iter/s, 6.29473s/10 iters), loss = 9.65828
I0523 23:20:54.474622 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.65828 (* 1 = 9.65828 loss)
I0523 23:20:54.474675 11835 sgd_solver.cpp:112] Iteration 7340, lr = 0.1
I0523 23:21:02.532311 11835 solver.cpp:239] Iteration 7350 (1.2411 iter/s, 8.05738s/10 iters), loss = 9.97629
I0523 23:21:02.532359 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.97629 (* 1 = 9.97629 loss)
I0523 23:21:02.532445 11835 sgd_solver.cpp:112] Iteration 7350, lr = 0.1
I0523 23:21:10.651183 11835 solver.cpp:239] Iteration 7360 (1.23175 iter/s, 8.11852s/10 iters), loss = 9.34129
I0523 23:21:10.651234 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.34129 (* 1 = 9.34129 loss)
I0523 23:21:10.651473 11835 sgd_solver.cpp:112] Iteration 7360, lr = 0.1
I0523 23:21:17.228932 11835 solver.cpp:239] Iteration 7370 (1.52035 iter/s, 6.57744s/10 iters), loss = 10.0203
I0523 23:21:17.228987 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0203 (* 1 = 10.0203 loss)
I0523 23:21:17.974334 11835 sgd_solver.cpp:112] Iteration 7370, lr = 0.1
I0523 23:21:26.266961 11835 solver.cpp:239] Iteration 7380 (1.10649 iter/s, 9.03763s/10 iters), loss = 9.51229
I0523 23:21:26.267249 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.51229 (* 1 = 9.51229 loss)
I0523 23:21:26.268986 11835 sgd_solver.cpp:112] Iteration 7380, lr = 0.1
I0523 23:21:33.597360 11835 solver.cpp:239] Iteration 7390 (1.36428 iter/s, 7.32987s/10 iters), loss = 9.31734
I0523 23:21:33.597420 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.31734 (* 1 = 9.31734 loss)
I0523 23:21:33.597568 11835 sgd_solver.cpp:112] Iteration 7390, lr = 0.1
I0523 23:21:41.548298 11835 solver.cpp:239] Iteration 7400 (1.25777 iter/s, 7.95058s/10 iters), loss = 9.83808
I0523 23:21:41.548347 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.83808 (* 1 = 9.83808 loss)
I0523 23:21:41.580513 11835 sgd_solver.cpp:112] Iteration 7400, lr = 0.1
I0523 23:21:47.315672 11835 solver.cpp:239] Iteration 7410 (1.73397 iter/s, 5.76711s/10 iters), loss = 10.4877
I0523 23:21:47.315718 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4877 (* 1 = 10.4877 loss)
I0523 23:21:47.333147 11835 sgd_solver.cpp:112] Iteration 7410, lr = 0.1
I0523 23:21:54.419782 11835 solver.cpp:239] Iteration 7420 (1.4077 iter/s, 7.10378s/10 iters), loss = 9.44623
I0523 23:21:54.419837 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.44623 (* 1 = 9.44623 loss)
I0523 23:21:54.419874 11835 sgd_solver.cpp:112] Iteration 7420, lr = 0.1
I0523 23:22:01.074615 11835 solver.cpp:239] Iteration 7430 (1.50274 iter/s, 6.65453s/10 iters), loss = 9.71002
I0523 23:22:01.074921 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.71002 (* 1 = 9.71002 loss)
I0523 23:22:01.074975 11835 sgd_solver.cpp:112] Iteration 7430, lr = 0.1
I0523 23:22:07.441197 11835 solver.cpp:239] Iteration 7440 (1.57082 iter/s, 6.3661s/10 iters), loss = 9.72419
I0523 23:22:07.441241 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.72419 (* 1 = 9.72419 loss)
I0523 23:22:07.487272 11835 sgd_solver.cpp:112] Iteration 7440, lr = 0.1
I0523 23:22:14.437134 11835 solver.cpp:239] Iteration 7450 (1.42947 iter/s, 6.99562s/10 iters), loss = 10.3713
I0523 23:22:14.437186 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3713 (* 1 = 10.3713 loss)
I0523 23:22:14.437315 11835 sgd_solver.cpp:112] Iteration 7450, lr = 0.1
I0523 23:22:20.625602 11835 solver.cpp:239] Iteration 7460 (1.61598 iter/s, 6.18818s/10 iters), loss = 10.4766
I0523 23:22:20.625655 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4766 (* 1 = 10.4766 loss)
I0523 23:22:20.625671 11835 sgd_solver.cpp:112] Iteration 7460, lr = 0.1
I0523 23:22:26.491776 11835 solver.cpp:239] Iteration 7470 (1.7048 iter/s, 5.86579s/10 iters), loss = 9.96369
I0523 23:22:26.491817 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.96369 (* 1 = 9.96369 loss)
I0523 23:22:26.569203 11835 sgd_solver.cpp:112] Iteration 7470, lr = 0.1
I0523 23:22:32.212837 11835 solver.cpp:239] Iteration 7480 (1.74801 iter/s, 5.7208s/10 iters), loss = 10.4232
I0523 23:22:32.212934 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4232 (* 1 = 10.4232 loss)
I0523 23:22:32.213255 11835 sgd_solver.cpp:112] Iteration 7480, lr = 0.1
I0523 23:22:38.498312 11835 solver.cpp:239] Iteration 7490 (1.59105 iter/s, 6.28515s/10 iters), loss = 10.298
I0523 23:22:38.498356 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.298 (* 1 = 10.298 loss)
I0523 23:22:38.498369 11835 sgd_solver.cpp:112] Iteration 7490, lr = 0.1
I0523 23:22:46.138211 11835 solver.cpp:239] Iteration 7500 (1.30898 iter/s, 7.63955s/10 iters), loss = 10.7361
I0523 23:22:46.138267 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7361 (* 1 = 10.7361 loss)
I0523 23:22:46.138489 11835 sgd_solver.cpp:112] Iteration 7500, lr = 0.1
I0523 23:22:52.199177 11835 solver.cpp:239] Iteration 7510 (1.64998 iter/s, 6.06069s/10 iters), loss = 10.4741
I0523 23:22:52.199240 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4741 (* 1 = 10.4741 loss)
I0523 23:22:52.317637 11835 sgd_solver.cpp:112] Iteration 7510, lr = 0.1
I0523 23:22:58.433737 11835 solver.cpp:239] Iteration 7520 (1.60404 iter/s, 6.23425s/10 iters), loss = 9.76003
I0523 23:22:58.433795 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.76003 (* 1 = 9.76003 loss)
I0523 23:22:58.434303 11835 sgd_solver.cpp:112] Iteration 7520, lr = 0.1
I0523 23:23:04.366892 11835 solver.cpp:239] Iteration 7530 (1.68552 iter/s, 5.93288s/10 iters), loss = 10.438
I0523 23:23:04.367221 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.438 (* 1 = 10.438 loss)
I0523 23:23:04.367274 11835 sgd_solver.cpp:112] Iteration 7530, lr = 0.1
I0523 23:23:10.375385 11835 solver.cpp:239] Iteration 7540 (1.66471 iter/s, 6.00705s/10 iters), loss = 9.78342
I0523 23:23:10.375430 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.78342 (* 1 = 9.78342 loss)
I0523 23:23:10.375455 11835 sgd_solver.cpp:112] Iteration 7540, lr = 0.1
I0523 23:23:16.195650 11835 solver.cpp:239] Iteration 7550 (1.71822 iter/s, 5.81999s/10 iters), loss = 10.1165
I0523 23:23:16.195699 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1165 (* 1 = 10.1165 loss)
I0523 23:23:16.195828 11835 sgd_solver.cpp:112] Iteration 7550, lr = 0.1
I0523 23:23:22.476214 11835 solver.cpp:239] Iteration 7560 (1.59229 iter/s, 6.28027s/10 iters), loss = 9.82799
I0523 23:23:22.476265 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.82799 (* 1 = 9.82799 loss)
I0523 23:23:22.476511 11835 sgd_solver.cpp:112] Iteration 7560, lr = 0.1
I0523 23:23:29.771121 11835 solver.cpp:239] Iteration 7570 (1.37088 iter/s, 7.29457s/10 iters), loss = 10.3719
I0523 23:23:29.771178 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3719 (* 1 = 10.3719 loss)
I0523 23:23:29.771414 11835 sgd_solver.cpp:112] Iteration 7570, lr = 0.1
I0523 23:23:36.036756 11835 solver.cpp:239] Iteration 7580 (1.59608 iter/s, 6.26535s/10 iters), loss = 10.3591
I0523 23:23:36.036969 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3591 (* 1 = 10.3591 loss)
I0523 23:23:36.037021 11835 sgd_solver.cpp:112] Iteration 7580, lr = 0.1
I0523 23:23:41.781858 11835 solver.cpp:239] Iteration 7590 (1.74107 iter/s, 5.74359s/10 iters), loss = 10.6144
I0523 23:23:41.781906 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6144 (* 1 = 10.6144 loss)
I0523 23:23:41.781920 11835 sgd_solver.cpp:112] Iteration 7590, lr = 0.1
I0523 23:23:49.244894 11835 solver.cpp:239] Iteration 7600 (1.34 iter/s, 7.46271s/10 iters), loss = 10.2917
I0523 23:23:49.244941 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2917 (* 1 = 10.2917 loss)
I0523 23:23:49.524276 11835 sgd_solver.cpp:112] Iteration 7600, lr = 0.1
I0523 23:23:57.884115 11835 solver.cpp:239] Iteration 7610 (1.15756 iter/s, 8.63884s/10 iters), loss = 10.6808
I0523 23:23:57.884174 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.6808 (* 1 = 10.6808 loss)
I0523 23:23:57.884614 11835 sgd_solver.cpp:112] Iteration 7610, lr = 0.1
I0523 23:24:03.716636 11835 solver.cpp:239] Iteration 7620 (1.71461 iter/s, 5.83224s/10 iters), loss = 9.94801
I0523 23:24:03.716697 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.94801 (* 1 = 9.94801 loss)
I0523 23:24:03.716775 11835 sgd_solver.cpp:112] Iteration 7620, lr = 0.1
I0523 23:24:10.378490 11835 solver.cpp:239] Iteration 7630 (1.50115 iter/s, 6.66155s/10 iters), loss = 10.2211
I0523 23:24:10.378628 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2211 (* 1 = 10.2211 loss)
I0523 23:24:10.378672 11835 sgd_solver.cpp:112] Iteration 7630, lr = 0.1
I0523 23:24:16.899531 11835 solver.cpp:239] Iteration 7640 (1.53359 iter/s, 6.52067s/10 iters), loss = 10.4255
I0523 23:24:16.899577 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4255 (* 1 = 10.4255 loss)
I0523 23:24:16.924867 11835 sgd_solver.cpp:112] Iteration 7640, lr = 0.1
I0523 23:24:24.679173 11835 solver.cpp:239] Iteration 7650 (1.28546 iter/s, 7.77929s/10 iters), loss = 10.3877
I0523 23:24:24.679235 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3877 (* 1 = 10.3877 loss)
I0523 23:24:24.679461 11835 sgd_solver.cpp:112] Iteration 7650, lr = 0.1
I0523 23:24:31.276546 11835 solver.cpp:239] Iteration 7660 (1.51583 iter/s, 6.59706s/10 iters), loss = 10.5196
I0523 23:24:31.276597 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5196 (* 1 = 10.5196 loss)
I0523 23:24:31.313247 11835 sgd_solver.cpp:112] Iteration 7660, lr = 0.1
I0523 23:24:36.926901 11835 solver.cpp:239] Iteration 7670 (1.76988 iter/s, 5.6501s/10 iters), loss = 10.0531
I0523 23:24:36.926942 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0531 (* 1 = 10.0531 loss)
I0523 23:24:36.927084 11835 sgd_solver.cpp:112] Iteration 7670, lr = 0.1
I0523 23:24:43.573562 11835 solver.cpp:239] Iteration 7680 (1.50458 iter/s, 6.64636s/10 iters), loss = 10.0805
I0523 23:24:43.573869 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0805 (* 1 = 10.0805 loss)
I0523 23:24:43.573909 11835 sgd_solver.cpp:112] Iteration 7680, lr = 0.1
I0523 23:24:50.881980 11835 solver.cpp:239] Iteration 7690 (1.36838 iter/s, 7.30789s/10 iters), loss = 9.7251
I0523 23:24:50.882026 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.7251 (* 1 = 9.7251 loss)
I0523 23:24:50.882086 11835 sgd_solver.cpp:112] Iteration 7690, lr = 0.1
I0523 23:24:57.045716 11835 solver.cpp:239] Iteration 7700 (1.62247 iter/s, 6.16345s/10 iters), loss = 9.71319
I0523 23:24:57.045764 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.71319 (* 1 = 9.71319 loss)
I0523 23:24:57.045835 11835 sgd_solver.cpp:112] Iteration 7700, lr = 0.1
I0523 23:25:02.806967 11835 solver.cpp:239] Iteration 7710 (1.73582 iter/s, 5.76098s/10 iters), loss = 9.49377
I0523 23:25:02.807026 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.49377 (* 1 = 9.49377 loss)
I0523 23:25:02.807242 11835 sgd_solver.cpp:112] Iteration 7710, lr = 0.1
I0523 23:25:09.091200 11835 solver.cpp:239] Iteration 7720 (1.59136 iter/s, 6.28394s/10 iters), loss = 10.4397
I0523 23:25:09.091253 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4397 (* 1 = 10.4397 loss)
I0523 23:25:09.091527 11835 sgd_solver.cpp:112] Iteration 7720, lr = 0.1
I0523 23:25:15.010586 11835 solver.cpp:239] Iteration 7730 (1.68944 iter/s, 5.91911s/10 iters), loss = 10.4372
I0523 23:25:15.010855 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4372 (* 1 = 10.4372 loss)
I0523 23:25:15.010895 11835 sgd_solver.cpp:112] Iteration 7730, lr = 0.1
I0523 23:25:21.922416 11835 solver.cpp:239] Iteration 7740 (1.44691 iter/s, 6.9113s/10 iters), loss = 8.98163
I0523 23:25:21.922458 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.98163 (* 1 = 8.98163 loss)
I0523 23:25:21.922472 11835 sgd_solver.cpp:112] Iteration 7740, lr = 0.1
I0523 23:25:30.403759 11835 solver.cpp:239] Iteration 7750 (1.17927 iter/s, 8.47985s/10 iters), loss = 9.608
I0523 23:25:30.403818 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.608 (* 1 = 9.608 loss)
I0523 23:25:30.404058 11835 sgd_solver.cpp:112] Iteration 7750, lr = 0.1
I0523 23:25:36.265556 11835 solver.cpp:239] Iteration 7760 (1.70604 iter/s, 5.86153s/10 iters), loss = 10.2115
I0523 23:25:36.265599 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2115 (* 1 = 10.2115 loss)
I0523 23:25:36.601526 11835 sgd_solver.cpp:112] Iteration 7760, lr = 0.1
I0523 23:25:42.493746 11835 solver.cpp:239] Iteration 7770 (1.60568 iter/s, 6.2279s/10 iters), loss = 9.24594
I0523 23:25:42.493803 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.24594 (* 1 = 9.24594 loss)
I0523 23:25:42.493851 11835 sgd_solver.cpp:112] Iteration 7770, lr = 0.1
I0523 23:25:48.786307 11835 solver.cpp:239] Iteration 7780 (1.58927 iter/s, 6.29222s/10 iters), loss = 9.68155
I0523 23:25:48.786556 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.68155 (* 1 = 9.68155 loss)
I0523 23:25:48.786603 11835 sgd_solver.cpp:112] Iteration 7780, lr = 0.1
I0523 23:25:55.503752 11835 solver.cpp:239] Iteration 7790 (1.48925 iter/s, 6.71479s/10 iters), loss = 9.89825
I0523 23:25:55.503792 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.89825 (* 1 = 9.89825 loss)
I0523 23:25:55.503943 11835 sgd_solver.cpp:112] Iteration 7790, lr = 0.1
I0523 23:26:01.740865 11835 solver.cpp:239] Iteration 7800 (1.60338 iter/s, 6.23682s/10 iters), loss = 9.64967
I0523 23:26:01.740924 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.64967 (* 1 = 9.64967 loss)
I0523 23:26:01.741142 11835 sgd_solver.cpp:112] Iteration 7800, lr = 0.1
I0523 23:26:07.620786 11835 solver.cpp:239] Iteration 7810 (1.70078 iter/s, 5.87965s/10 iters), loss = 8.68152
I0523 23:26:07.620823 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.68152 (* 1 = 8.68152 loss)
I0523 23:26:07.632810 11835 sgd_solver.cpp:112] Iteration 7810, lr = 0.1
I0523 23:26:14.109416 11835 solver.cpp:239] Iteration 7820 (1.54123 iter/s, 6.48833s/10 iters), loss = 9.59389
I0523 23:26:14.109474 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.59389 (* 1 = 9.59389 loss)
I0523 23:26:14.109583 11835 sgd_solver.cpp:112] Iteration 7820, lr = 0.1
I0523 23:26:21.669973 11835 solver.cpp:239] Iteration 7830 (1.32272 iter/s, 7.56021s/10 iters), loss = 9.81019
I0523 23:26:21.670300 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.81019 (* 1 = 9.81019 loss)
I0523 23:26:21.670397 11835 sgd_solver.cpp:112] Iteration 7830, lr = 0.1
I0523 23:26:27.760524 11835 solver.cpp:239] Iteration 7840 (1.64202 iter/s, 6.09004s/10 iters), loss = 9.068
I0523 23:26:27.760581 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.068 (* 1 = 9.068 loss)
I0523 23:26:27.762526 11835 sgd_solver.cpp:112] Iteration 7840, lr = 0.1
I0523 23:26:36.037269 11835 solver.cpp:239] Iteration 7850 (1.20826 iter/s, 8.27639s/10 iters), loss = 9.1748
I0523 23:26:36.037309 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.1748 (* 1 = 9.1748 loss)
I0523 23:26:36.037436 11835 sgd_solver.cpp:112] Iteration 7850, lr = 0.1
I0523 23:26:42.277361 11835 solver.cpp:239] Iteration 7860 (1.60261 iter/s, 6.23981s/10 iters), loss = 9.29391
I0523 23:26:42.277417 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.29391 (* 1 = 9.29391 loss)
I0523 23:26:42.277750 11835 sgd_solver.cpp:112] Iteration 7860, lr = 0.1
I0523 23:26:49.289944 11835 solver.cpp:239] Iteration 7870 (1.42607 iter/s, 7.01227s/10 iters), loss = 9.82602
I0523 23:26:49.289994 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.82602 (* 1 = 9.82602 loss)
I0523 23:26:49.640271 11835 sgd_solver.cpp:112] Iteration 7870, lr = 0.1
I0523 23:26:56.097784 11835 solver.cpp:239] Iteration 7880 (1.46896 iter/s, 6.80754s/10 iters), loss = 9.48942
I0523 23:26:56.098044 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.48942 (* 1 = 9.48942 loss)
I0523 23:26:56.098091 11835 sgd_solver.cpp:112] Iteration 7880, lr = 0.1
I0523 23:27:03.574110 11835 solver.cpp:239] Iteration 7890 (1.33765 iter/s, 7.47582s/10 iters), loss = 10.3514
I0523 23:27:03.574159 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3514 (* 1 = 10.3514 loss)
I0523 23:27:03.574287 11835 sgd_solver.cpp:112] Iteration 7890, lr = 0.1
I0523 23:27:09.773057 11835 solver.cpp:239] Iteration 7900 (1.61325 iter/s, 6.19867s/10 iters), loss = 10.4527
I0523 23:27:09.773103 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4527 (* 1 = 10.4527 loss)
I0523 23:27:09.773351 11835 sgd_solver.cpp:112] Iteration 7900, lr = 0.1
I0523 23:27:15.770756 11835 solver.cpp:239] Iteration 7910 (1.66739 iter/s, 5.99741s/10 iters), loss = 9.5173
I0523 23:27:15.770812 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.5173 (* 1 = 9.5173 loss)
I0523 23:27:15.771011 11835 sgd_solver.cpp:112] Iteration 7910, lr = 0.1
I0523 23:27:21.262826 11835 solver.cpp:239] Iteration 7920 (1.82089 iter/s, 5.49181s/10 iters), loss = 9.00544
I0523 23:27:21.262876 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.00544 (* 1 = 9.00544 loss)
I0523 23:27:21.262967 11835 sgd_solver.cpp:112] Iteration 7920, lr = 0.1
I0523 23:27:28.057456 11835 solver.cpp:239] Iteration 7930 (1.47182 iter/s, 6.79433s/10 iters), loss = 9.46317
I0523 23:27:28.057750 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.46317 (* 1 = 9.46317 loss)
I0523 23:27:28.057914 11835 sgd_solver.cpp:112] Iteration 7930, lr = 0.1
I0523 23:27:34.128942 11835 solver.cpp:239] Iteration 7940 (1.64718 iter/s, 6.07099s/10 iters), loss = 10.2531
I0523 23:27:34.128999 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2531 (* 1 = 10.2531 loss)
I0523 23:27:34.129098 11835 sgd_solver.cpp:112] Iteration 7940, lr = 0.1
I0523 23:27:40.403373 11835 solver.cpp:239] Iteration 7950 (1.59384 iter/s, 6.27414s/10 iters), loss = 9.37453
I0523 23:27:40.403417 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.37453 (* 1 = 9.37453 loss)
I0523 23:27:40.403547 11835 sgd_solver.cpp:112] Iteration 7950, lr = 0.1
I0523 23:27:48.295831 11835 solver.cpp:239] Iteration 7960 (1.26709 iter/s, 7.8921s/10 iters), loss = 9.26493
I0523 23:27:48.295892 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.26493 (* 1 = 9.26493 loss)
I0523 23:27:48.296041 11835 sgd_solver.cpp:112] Iteration 7960, lr = 0.1
I0523 23:27:55.026080 11835 solver.cpp:239] Iteration 7970 (1.4859 iter/s, 6.72994s/10 iters), loss = 10.1361
I0523 23:27:55.026126 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1361 (* 1 = 10.1361 loss)
I0523 23:27:55.026346 11835 sgd_solver.cpp:112] Iteration 7970, lr = 0.1
I0523 23:28:02.393998 11835 solver.cpp:239] Iteration 7980 (1.35729 iter/s, 7.3676s/10 iters), loss = 9.46132
I0523 23:28:02.394111 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.46132 (* 1 = 9.46132 loss)
I0523 23:28:02.464828 11835 sgd_solver.cpp:112] Iteration 7980, lr = 0.1
I0523 23:28:09.054527 11835 solver.cpp:239] Iteration 7990 (1.50146 iter/s, 6.66016s/10 iters), loss = 9.8753
I0523 23:28:09.054579 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.8753 (* 1 = 9.8753 loss)
I0523 23:28:09.054677 11835 sgd_solver.cpp:112] Iteration 7990, lr = 0.1
I0523 23:28:14.879681 11835 solver.cpp:239] Iteration 8000 (1.71677 iter/s, 5.82489s/10 iters), loss = 10.7683
I0523 23:28:14.879724 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.7683 (* 1 = 10.7683 loss)
I0523 23:28:14.879935 11835 sgd_solver.cpp:112] Iteration 8000, lr = 0.1
I0523 23:28:22.180549 11835 solver.cpp:239] Iteration 8010 (1.36976 iter/s, 7.30055s/10 iters), loss = 9.19857
I0523 23:28:22.180601 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.19857 (* 1 = 9.19857 loss)
I0523 23:28:22.180732 11835 sgd_solver.cpp:112] Iteration 8010, lr = 0.1
I0523 23:28:30.151420 11835 solver.cpp:239] Iteration 8020 (1.25462 iter/s, 7.97052s/10 iters), loss = 9.74326
I0523 23:28:30.151476 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.74326 (* 1 = 9.74326 loss)
I0523 23:28:30.151700 11835 sgd_solver.cpp:112] Iteration 8020, lr = 0.1
I0523 23:28:35.968024 11835 solver.cpp:239] Iteration 8030 (1.7193 iter/s, 5.81633s/10 iters), loss = 9.77125
I0523 23:28:35.968291 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.77125 (* 1 = 9.77125 loss)
I0523 23:28:35.968346 11835 sgd_solver.cpp:112] Iteration 8030, lr = 0.1
I0523 23:28:42.066792 11835 solver.cpp:239] Iteration 8040 (1.6399 iter/s, 6.09794s/10 iters), loss = 10.1155
I0523 23:28:42.066844 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1155 (* 1 = 10.1155 loss)
I0523 23:28:42.656102 11835 sgd_solver.cpp:112] Iteration 8040, lr = 0.1
I0523 23:28:49.992208 11835 solver.cpp:239] Iteration 8050 (1.26182 iter/s, 7.92507s/10 iters), loss = 9.79558
I0523 23:28:49.992255 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.79558 (* 1 = 9.79558 loss)
I0523 23:28:49.992269 11835 sgd_solver.cpp:112] Iteration 8050, lr = 0.1
I0523 23:28:55.957151 11835 solver.cpp:239] Iteration 8060 (1.67666 iter/s, 5.96425s/10 iters), loss = 9.24972
I0523 23:28:55.957192 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.24972 (* 1 = 9.24972 loss)
I0523 23:28:55.957572 11835 sgd_solver.cpp:112] Iteration 8060, lr = 0.1
I0523 23:29:01.578233 11835 solver.cpp:239] Iteration 8070 (1.7791 iter/s, 5.62082s/10 iters), loss = 10.3953
I0523 23:29:01.578282 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3953 (* 1 = 10.3953 loss)
I0523 23:29:01.578299 11835 sgd_solver.cpp:112] Iteration 8070, lr = 0.1
I0523 23:29:07.980885 11835 solver.cpp:239] Iteration 8080 (1.56192 iter/s, 6.40236s/10 iters), loss = 10.1936
I0523 23:29:07.981132 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1936 (* 1 = 10.1936 loss)
I0523 23:29:07.981174 11835 sgd_solver.cpp:112] Iteration 8080, lr = 0.1
I0523 23:29:13.744032 11835 solver.cpp:239] Iteration 8090 (1.73529 iter/s, 5.76271s/10 iters), loss = 10.9763
I0523 23:29:13.744079 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.9763 (* 1 = 10.9763 loss)
I0523 23:29:13.744093 11835 sgd_solver.cpp:112] Iteration 8090, lr = 0.1
I0523 23:29:20.118842 11835 solver.cpp:239] Iteration 8100 (1.56874 iter/s, 6.37452s/10 iters), loss = 10.0275
I0523 23:29:20.118891 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0275 (* 1 = 10.0275 loss)
I0523 23:29:20.119123 11835 sgd_solver.cpp:112] Iteration 8100, lr = 0.1
I0523 23:29:26.132810 11835 solver.cpp:239] Iteration 8110 (1.66287 iter/s, 6.01369s/10 iters), loss = 10.1915
I0523 23:29:26.132859 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1915 (* 1 = 10.1915 loss)
I0523 23:29:26.234643 11835 sgd_solver.cpp:112] Iteration 8110, lr = 0.1
I0523 23:29:34.336575 11835 solver.cpp:239] Iteration 8120 (1.21901 iter/s, 8.20341s/10 iters), loss = 10.2712
I0523 23:29:34.336625 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2712 (* 1 = 10.2712 loss)
I0523 23:29:34.337168 11835 sgd_solver.cpp:112] Iteration 8120, lr = 0.1
I0523 23:29:40.500851 11835 solver.cpp:239] Iteration 8130 (1.62233 iter/s, 6.16398s/10 iters), loss = 9.97468
I0523 23:29:40.500957 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.97468 (* 1 = 9.97468 loss)
I0523 23:29:40.502851 11835 sgd_solver.cpp:112] Iteration 8130, lr = 0.1
I0523 23:29:47.175657 11835 solver.cpp:239] Iteration 8140 (1.49825 iter/s, 6.67446s/10 iters), loss = 9.72246
I0523 23:29:47.175696 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.72246 (* 1 = 9.72246 loss)
I0523 23:29:47.175842 11835 sgd_solver.cpp:112] Iteration 8140, lr = 0.1
I0523 23:29:54.719677 11835 solver.cpp:239] Iteration 8150 (1.32561 iter/s, 7.54369s/10 iters), loss = 9.12027
I0523 23:29:54.719732 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.12027 (* 1 = 9.12027 loss)
I0523 23:29:55.328819 11835 sgd_solver.cpp:112] Iteration 8150, lr = 0.1
I0523 23:30:03.145475 11835 solver.cpp:239] Iteration 8160 (1.18688 iter/s, 8.42542s/10 iters), loss = 10.5087
I0523 23:30:03.145531 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5087 (* 1 = 10.5087 loss)
I0523 23:30:03.222708 11835 sgd_solver.cpp:112] Iteration 8160, lr = 0.1
I0523 23:30:10.921361 11835 solver.cpp:239] Iteration 8170 (1.28609 iter/s, 7.77553s/10 iters), loss = 10.2976
I0523 23:30:10.921622 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2976 (* 1 = 10.2976 loss)
I0523 23:30:11.345877 11835 sgd_solver.cpp:112] Iteration 8170, lr = 0.1
I0523 23:30:19.703413 11835 solver.cpp:239] Iteration 8180 (1.13876 iter/s, 8.7815s/10 iters), loss = 9.35563
I0523 23:30:19.703464 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.35563 (* 1 = 9.35563 loss)
I0523 23:30:19.703826 11835 sgd_solver.cpp:112] Iteration 8180, lr = 0.1
I0523 23:30:26.254495 11835 solver.cpp:239] Iteration 8190 (1.52653 iter/s, 6.55079s/10 iters), loss = 10.5533
I0523 23:30:26.254536 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5533 (* 1 = 10.5533 loss)
I0523 23:30:26.466873 11835 sgd_solver.cpp:112] Iteration 8190, lr = 0.1
I0523 23:30:33.677433 11835 solver.cpp:239] Iteration 8200 (1.34724 iter/s, 7.42261s/10 iters), loss = 9.85754
I0523 23:30:33.677484 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.85754 (* 1 = 9.85754 loss)
I0523 23:30:33.677500 11835 sgd_solver.cpp:112] Iteration 8200, lr = 0.1
I0523 23:30:41.116734 11835 solver.cpp:239] Iteration 8210 (1.34428 iter/s, 7.43895s/10 iters), loss = 10.4751
I0523 23:30:41.117027 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4751 (* 1 = 10.4751 loss)
I0523 23:30:41.410909 11835 sgd_solver.cpp:112] Iteration 8210, lr = 0.1
I0523 23:30:47.104418 11835 solver.cpp:239] Iteration 8220 (1.67022 iter/s, 5.98722s/10 iters), loss = 8.90585
I0523 23:30:47.104457 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.90585 (* 1 = 8.90585 loss)
I0523 23:30:47.104470 11835 sgd_solver.cpp:112] Iteration 8220, lr = 0.1
I0523 23:30:54.499084 11835 solver.cpp:239] Iteration 8230 (1.35279 iter/s, 7.39211s/10 iters), loss = 9.43722
I0523 23:30:54.499135 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.43722 (* 1 = 9.43722 loss)
I0523 23:30:55.419661 11835 sgd_solver.cpp:112] Iteration 8230, lr = 0.1
I0523 23:31:01.980545 11835 solver.cpp:239] Iteration 8240 (1.3367 iter/s, 7.48114s/10 iters), loss = 10.4796
I0523 23:31:01.980585 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4796 (* 1 = 10.4796 loss)
I0523 23:31:01.992849 11835 sgd_solver.cpp:112] Iteration 8240, lr = 0.1
I0523 23:31:08.434780 11835 solver.cpp:239] Iteration 8250 (1.54944 iter/s, 6.45394s/10 iters), loss = 9.98267
I0523 23:31:08.434836 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.98267 (* 1 = 9.98267 loss)
I0523 23:31:08.435065 11835 sgd_solver.cpp:112] Iteration 8250, lr = 0.1
I0523 23:31:16.283594 11835 solver.cpp:239] Iteration 8260 (1.27413 iter/s, 7.84847s/10 iters), loss = 9.32478
I0523 23:31:16.283876 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.32478 (* 1 = 9.32478 loss)
I0523 23:31:16.283932 11835 sgd_solver.cpp:112] Iteration 8260, lr = 0.1
I0523 23:31:22.941532 11835 solver.cpp:239] Iteration 8270 (1.50214 iter/s, 6.65719s/10 iters), loss = 9.76529
I0523 23:31:22.941582 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.76529 (* 1 = 9.76529 loss)
I0523 23:31:23.187595 11835 sgd_solver.cpp:112] Iteration 8270, lr = 0.1
I0523 23:31:29.961367 11835 solver.cpp:239] Iteration 8280 (1.4246 iter/s, 7.01953s/10 iters), loss = 10.3793
I0523 23:31:29.961407 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3793 (* 1 = 10.3793 loss)
I0523 23:31:29.961419 11835 sgd_solver.cpp:112] Iteration 8280, lr = 0.1
I0523 23:31:36.531080 11835 solver.cpp:239] Iteration 8290 (1.52227 iter/s, 6.56913s/10 iters), loss = 10.0164
I0523 23:31:36.531128 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0164 (* 1 = 10.0164 loss)
I0523 23:31:36.531242 11835 sgd_solver.cpp:112] Iteration 8290, lr = 0.1
I0523 23:31:42.913722 11835 solver.cpp:239] Iteration 8300 (1.56682 iter/s, 6.38234s/10 iters), loss = 9.92045
I0523 23:31:42.913784 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.92045 (* 1 = 9.92045 loss)
I0523 23:31:42.913878 11835 sgd_solver.cpp:112] Iteration 8300, lr = 0.1
I0523 23:31:48.991751 11835 solver.cpp:239] Iteration 8310 (1.64535 iter/s, 6.07775s/10 iters), loss = 10.0604
I0523 23:31:48.992022 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0604 (* 1 = 10.0604 loss)
I0523 23:31:48.992080 11835 sgd_solver.cpp:112] Iteration 8310, lr = 0.1
I0523 23:31:58.431452 11835 solver.cpp:239] Iteration 8320 (1.05942 iter/s, 9.43911s/10 iters), loss = 9.4487
I0523 23:31:58.431512 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.4487 (* 1 = 9.4487 loss)
I0523 23:31:58.431705 11835 sgd_solver.cpp:112] Iteration 8320, lr = 0.1
I0523 23:32:05.618060 11835 solver.cpp:239] Iteration 8330 (1.39154 iter/s, 7.18628s/10 iters), loss = 8.5244
I0523 23:32:05.618122 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.5244 (* 1 = 8.5244 loss)
I0523 23:32:05.618140 11835 sgd_solver.cpp:112] Iteration 8330, lr = 0.1
I0523 23:32:11.454612 11835 solver.cpp:239] Iteration 8340 (1.71404 iter/s, 5.83418s/10 iters), loss = 9.75035
I0523 23:32:11.454649 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.75035 (* 1 = 9.75035 loss)
I0523 23:32:11.454663 11835 sgd_solver.cpp:112] Iteration 8340, lr = 0.1
I0523 23:32:17.120898 11835 solver.cpp:239] Iteration 8350 (1.76491 iter/s, 5.66603s/10 iters), loss = 9.63699
I0523 23:32:17.120950 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.63699 (* 1 = 9.63699 loss)
I0523 23:32:17.120966 11835 sgd_solver.cpp:112] Iteration 8350, lr = 0.1
I0523 23:32:24.880929 11835 solver.cpp:239] Iteration 8360 (1.28872 iter/s, 7.75962s/10 iters), loss = 9.26633
I0523 23:32:24.881198 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.26633 (* 1 = 9.26633 loss)
I0523 23:32:24.881245 11835 sgd_solver.cpp:112] Iteration 8360, lr = 0.1
I0523 23:32:31.747748 11835 solver.cpp:239] Iteration 8370 (1.4564 iter/s, 6.86626s/10 iters), loss = 9.59894
I0523 23:32:31.747787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.59894 (* 1 = 9.59894 loss)
I0523 23:32:31.851124 11835 sgd_solver.cpp:112] Iteration 8370, lr = 0.1
I0523 23:32:40.776569 11835 solver.cpp:239] Iteration 8380 (1.10761 iter/s, 9.02843s/10 iters), loss = 10.1123
I0523 23:32:40.776628 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1123 (* 1 = 10.1123 loss)
I0523 23:32:40.776878 11835 sgd_solver.cpp:112] Iteration 8380, lr = 0.1
I0523 23:32:48.004155 11835 solver.cpp:239] Iteration 8390 (1.38365 iter/s, 7.22726s/10 iters), loss = 9.65469
I0523 23:32:48.004205 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.65469 (* 1 = 9.65469 loss)
I0523 23:32:48.004221 11835 sgd_solver.cpp:112] Iteration 8390, lr = 0.1
I0523 23:32:54.027446 11835 solver.cpp:239] Iteration 8400 (1.6603 iter/s, 6.02301s/10 iters), loss = 11.446
I0523 23:32:54.027500 11835 solver.cpp:258]     Train net output #0: softmax_loss = 11.446 (* 1 = 11.446 loss)
I0523 23:32:54.027858 11835 sgd_solver.cpp:112] Iteration 8400, lr = 0.1
I0523 23:33:00.033381 11835 solver.cpp:239] Iteration 8410 (1.6651 iter/s, 6.00565s/10 iters), loss = 9.08875
I0523 23:33:00.033506 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.08875 (* 1 = 9.08875 loss)
I0523 23:33:00.033522 11835 sgd_solver.cpp:112] Iteration 8410, lr = 0.1
I0523 23:33:05.581084 11835 solver.cpp:239] Iteration 8420 (1.80266 iter/s, 5.54734s/10 iters), loss = 9.56658
I0523 23:33:05.581122 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.56658 (* 1 = 9.56658 loss)
I0523 23:33:05.581188 11835 sgd_solver.cpp:112] Iteration 8420, lr = 0.1
I0523 23:33:12.188064 11835 solver.cpp:239] Iteration 8430 (1.51362 iter/s, 6.60668s/10 iters), loss = 9.94531
I0523 23:33:12.188122 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.94531 (* 1 = 9.94531 loss)
I0523 23:33:12.646935 11835 sgd_solver.cpp:112] Iteration 8430, lr = 0.1
I0523 23:33:20.026326 11835 solver.cpp:239] Iteration 8440 (1.27585 iter/s, 7.8379s/10 iters), loss = 10.226
I0523 23:33:20.026376 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.226 (* 1 = 10.226 loss)
I0523 23:33:20.033471 11835 sgd_solver.cpp:112] Iteration 8440, lr = 0.1
I0523 23:33:26.567152 11835 solver.cpp:239] Iteration 8450 (1.52893 iter/s, 6.54053s/10 iters), loss = 9.939
I0523 23:33:26.567196 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.939 (* 1 = 9.939 loss)
I0523 23:33:26.949986 11835 sgd_solver.cpp:112] Iteration 8450, lr = 0.1
I0523 23:33:33.358289 11835 solver.cpp:239] Iteration 8460 (1.47257 iter/s, 6.79083s/10 iters), loss = 9.71393
I0523 23:33:33.358526 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.71393 (* 1 = 9.71393 loss)
I0523 23:33:33.444979 11835 sgd_solver.cpp:112] Iteration 8460, lr = 0.1
I0523 23:33:40.316205 11835 solver.cpp:239] Iteration 8470 (1.43731 iter/s, 6.95746s/10 iters), loss = 10.1052
I0523 23:33:40.316246 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1052 (* 1 = 10.1052 loss)
I0523 23:33:40.316259 11835 sgd_solver.cpp:112] Iteration 8470, lr = 0.1
I0523 23:33:46.950546 11835 solver.cpp:239] Iteration 8480 (1.50741 iter/s, 6.63389s/10 iters), loss = 9.00781
I0523 23:33:46.950614 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.00781 (* 1 = 9.00781 loss)
I0523 23:33:46.967257 11835 sgd_solver.cpp:112] Iteration 8480, lr = 0.1
I0523 23:33:56.229339 11835 solver.cpp:239] Iteration 8490 (1.07777 iter/s, 9.27839s/10 iters), loss = 9.76791
I0523 23:33:56.229389 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.76791 (* 1 = 9.76791 loss)
I0523 23:33:56.864265 11835 sgd_solver.cpp:112] Iteration 8490, lr = 0.1
I0523 23:34:02.769235 11835 solver.cpp:239] Iteration 8500 (1.52915 iter/s, 6.53959s/10 iters), loss = 9.86846
I0523 23:34:02.769287 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.86846 (* 1 = 9.86846 loss)
I0523 23:34:03.221807 11835 sgd_solver.cpp:112] Iteration 8500, lr = 0.1
I0523 23:34:12.325947 11835 solver.cpp:239] Iteration 8510 (1.04643 iter/s, 9.5563s/10 iters), loss = 9.75834
I0523 23:34:12.326102 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.75834 (* 1 = 9.75834 loss)
I0523 23:34:12.326126 11835 sgd_solver.cpp:112] Iteration 8510, lr = 0.1
I0523 23:34:18.132953 11835 solver.cpp:239] Iteration 8520 (1.72217 iter/s, 5.80663s/10 iters), loss = 10.1871
I0523 23:34:18.133009 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1871 (* 1 = 10.1871 loss)
I0523 23:34:18.133581 11835 sgd_solver.cpp:112] Iteration 8520, lr = 0.1
I0523 23:34:25.410068 11835 solver.cpp:239] Iteration 8530 (1.37423 iter/s, 7.27679s/10 iters), loss = 9.79033
I0523 23:34:25.410105 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.79033 (* 1 = 9.79033 loss)
I0523 23:34:25.410348 11835 sgd_solver.cpp:112] Iteration 8530, lr = 0.1
I0523 23:34:32.615063 11835 solver.cpp:239] Iteration 8540 (1.38799 iter/s, 7.20468s/10 iters), loss = 8.91671
I0523 23:34:32.615113 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.91671 (* 1 = 8.91671 loss)
I0523 23:34:32.654101 11835 sgd_solver.cpp:112] Iteration 8540, lr = 0.1
I0523 23:34:38.818084 11835 solver.cpp:239] Iteration 8550 (1.61219 iter/s, 6.20273s/10 iters), loss = 9.74726
I0523 23:34:38.818131 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.74726 (* 1 = 9.74726 loss)
I0523 23:34:38.884163 11835 sgd_solver.cpp:112] Iteration 8550, lr = 0.1
I0523 23:34:45.642709 11835 solver.cpp:239] Iteration 8560 (1.46535 iter/s, 6.82431s/10 iters), loss = 10.0875
I0523 23:34:45.642841 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0875 (* 1 = 10.0875 loss)
I0523 23:34:45.643013 11835 sgd_solver.cpp:112] Iteration 8560, lr = 0.1
I0523 23:34:52.032747 11835 solver.cpp:239] Iteration 8570 (1.56503 iter/s, 6.38966s/10 iters), loss = 8.73392
I0523 23:34:52.032801 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.73392 (* 1 = 8.73392 loss)
I0523 23:34:52.498024 11835 sgd_solver.cpp:112] Iteration 8570, lr = 0.1
I0523 23:34:58.642153 11835 solver.cpp:239] Iteration 8580 (1.51306 iter/s, 6.60911s/10 iters), loss = 9.39679
I0523 23:34:58.642196 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.39679 (* 1 = 9.39679 loss)
I0523 23:34:58.642210 11835 sgd_solver.cpp:112] Iteration 8580, lr = 0.1
I0523 23:35:05.902904 11835 solver.cpp:239] Iteration 8590 (1.37735 iter/s, 7.26031s/10 iters), loss = 9.28323
I0523 23:35:05.902953 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.28323 (* 1 = 9.28323 loss)
I0523 23:35:06.385973 11835 sgd_solver.cpp:112] Iteration 8590, lr = 0.1
I0523 23:35:12.167788 11835 solver.cpp:239] Iteration 8600 (1.59627 iter/s, 6.2646s/10 iters), loss = 9.92945
I0523 23:35:12.167829 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.92945 (* 1 = 9.92945 loss)
I0523 23:35:12.167856 11835 sgd_solver.cpp:112] Iteration 8600, lr = 0.1
I0523 23:35:17.972052 11835 solver.cpp:239] Iteration 8610 (1.72295 iter/s, 5.80399s/10 iters), loss = 9.14397
I0523 23:35:17.972401 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.14397 (* 1 = 9.14397 loss)
I0523 23:35:17.972461 11835 sgd_solver.cpp:112] Iteration 8610, lr = 0.1
I0523 23:35:24.018172 11835 solver.cpp:239] Iteration 8620 (1.65428 iter/s, 6.04493s/10 iters), loss = 9.75347
I0523 23:35:24.018230 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.75347 (* 1 = 9.75347 loss)
I0523 23:35:24.018298 11835 sgd_solver.cpp:112] Iteration 8620, lr = 0.1
I0523 23:35:32.461493 11835 solver.cpp:239] Iteration 8630 (1.18442 iter/s, 8.44295s/10 iters), loss = 9.92003
I0523 23:35:32.461540 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.92003 (* 1 = 9.92003 loss)
I0523 23:35:32.461778 11835 sgd_solver.cpp:112] Iteration 8630, lr = 0.1
I0523 23:35:38.611856 11835 solver.cpp:239] Iteration 8640 (1.62599 iter/s, 6.15008s/10 iters), loss = 9.45325
I0523 23:35:38.611938 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.45325 (* 1 = 9.45325 loss)
I0523 23:35:39.723626 11835 sgd_solver.cpp:112] Iteration 8640, lr = 0.1
I0523 23:35:45.401043 11835 solver.cpp:239] Iteration 8650 (1.473 iter/s, 6.78889s/10 iters), loss = 9.0062
I0523 23:35:45.401091 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.0062 (* 1 = 9.0062 loss)
I0523 23:35:45.401105 11835 sgd_solver.cpp:112] Iteration 8650, lr = 0.1
I0523 23:35:51.457756 11835 solver.cpp:239] Iteration 8660 (1.65114 iter/s, 6.05643s/10 iters), loss = 9.99355
I0523 23:35:51.457916 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.99355 (* 1 = 9.99355 loss)
I0523 23:35:51.458007 11835 sgd_solver.cpp:112] Iteration 8660, lr = 0.1
I0523 23:35:59.227707 11835 solver.cpp:239] Iteration 8670 (1.28708 iter/s, 7.76952s/10 iters), loss = 9.58059
I0523 23:35:59.227746 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.58059 (* 1 = 9.58059 loss)
I0523 23:35:59.560901 11835 sgd_solver.cpp:112] Iteration 8670, lr = 0.1
I0523 23:36:07.174870 11835 solver.cpp:239] Iteration 8680 (1.25837 iter/s, 7.94681s/10 iters), loss = 9.28233
I0523 23:36:07.174928 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.28233 (* 1 = 9.28233 loss)
I0523 23:36:07.175029 11835 sgd_solver.cpp:112] Iteration 8680, lr = 0.1
I0523 23:36:15.468762 11835 solver.cpp:239] Iteration 8690 (1.20576 iter/s, 8.29354s/10 iters), loss = 9.95806
I0523 23:36:15.468802 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.95806 (* 1 = 9.95806 loss)
I0523 23:36:15.468817 11835 sgd_solver.cpp:112] Iteration 8690, lr = 0.1
I0523 23:36:21.646200 11835 solver.cpp:239] Iteration 8700 (1.61916 iter/s, 6.17604s/10 iters), loss = 10.0748
I0523 23:36:21.646343 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0748 (* 1 = 10.0748 loss)
I0523 23:36:21.646384 11835 sgd_solver.cpp:112] Iteration 8700, lr = 0.1
I0523 23:36:29.179513 11835 solver.cpp:239] Iteration 8710 (1.32751 iter/s, 7.53287s/10 iters), loss = 9.22369
I0523 23:36:29.179569 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.22369 (* 1 = 9.22369 loss)
I0523 23:36:29.179827 11835 sgd_solver.cpp:112] Iteration 8710, lr = 0.1
I0523 23:36:37.084702 11835 solver.cpp:239] Iteration 8720 (1.26505 iter/s, 7.90484s/10 iters), loss = 9.33478
I0523 23:36:37.084753 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.33478 (* 1 = 9.33478 loss)
I0523 23:36:37.084941 11835 sgd_solver.cpp:112] Iteration 8720, lr = 0.1
I0523 23:36:43.282615 11835 solver.cpp:239] Iteration 8730 (1.61352 iter/s, 6.19763s/10 iters), loss = 9.48386
I0523 23:36:43.282661 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.48386 (* 1 = 9.48386 loss)
I0523 23:36:43.282676 11835 sgd_solver.cpp:112] Iteration 8730, lr = 0.1
I0523 23:36:49.794275 11835 solver.cpp:239] Iteration 8740 (1.5363 iter/s, 6.50914s/10 iters), loss = 10.2343
I0523 23:36:49.794323 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.2343 (* 1 = 10.2343 loss)
I0523 23:36:49.794490 11835 sgd_solver.cpp:112] Iteration 8740, lr = 0.1
I0523 23:36:55.682060 11835 solver.cpp:239] Iteration 8750 (1.69851 iter/s, 5.88751s/10 iters), loss = 9.52064
I0523 23:36:55.682346 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.52064 (* 1 = 9.52064 loss)
I0523 23:36:55.682411 11835 sgd_solver.cpp:112] Iteration 8750, lr = 0.1
I0523 23:37:01.838274 11835 solver.cpp:239] Iteration 8760 (1.6245 iter/s, 6.15573s/10 iters), loss = 8.11717
I0523 23:37:01.838310 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.11717 (* 1 = 8.11717 loss)
I0523 23:37:01.936173 11835 sgd_solver.cpp:112] Iteration 8760, lr = 0.1
I0523 23:37:07.631116 11835 solver.cpp:239] Iteration 8770 (1.72635 iter/s, 5.79257s/10 iters), loss = 10.006
I0523 23:37:07.631180 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.006 (* 1 = 10.006 loss)
I0523 23:37:07.631208 11835 sgd_solver.cpp:112] Iteration 8770, lr = 0.1
I0523 23:37:16.020332 11835 solver.cpp:239] Iteration 8780 (1.19206 iter/s, 8.38884s/10 iters), loss = 9.63251
I0523 23:37:16.020381 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.63251 (* 1 = 9.63251 loss)
I0523 23:37:16.692178 11835 sgd_solver.cpp:112] Iteration 8780, lr = 0.1
I0523 23:37:26.222017 11835 solver.cpp:239] Iteration 8790 (0.980272 iter/s, 10.2013s/10 iters), loss = 9.44949
I0523 23:37:26.222172 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.44949 (* 1 = 9.44949 loss)
I0523 23:37:26.222275 11835 sgd_solver.cpp:112] Iteration 8790, lr = 0.1
I0523 23:37:31.956183 11835 solver.cpp:239] Iteration 8800 (1.74404 iter/s, 5.73382s/10 iters), loss = 8.86129
I0523 23:37:31.956219 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.86129 (* 1 = 8.86129 loss)
I0523 23:37:32.609514 11835 sgd_solver.cpp:112] Iteration 8800, lr = 0.1
I0523 23:37:38.970113 11835 solver.cpp:239] Iteration 8810 (1.4258 iter/s, 7.01361s/10 iters), loss = 8.78714
I0523 23:37:38.970168 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.78714 (* 1 = 8.78714 loss)
I0523 23:37:38.970252 11835 sgd_solver.cpp:112] Iteration 8810, lr = 0.1
I0523 23:37:45.376634 11835 solver.cpp:239] Iteration 8820 (1.56098 iter/s, 6.40623s/10 iters), loss = 9.41965
I0523 23:37:45.376682 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.41965 (* 1 = 9.41965 loss)
I0523 23:37:45.543483 11835 sgd_solver.cpp:112] Iteration 8820, lr = 0.1
I0523 23:37:54.667515 11835 solver.cpp:239] Iteration 8830 (1.07637 iter/s, 9.29049s/10 iters), loss = 10.0673
I0523 23:37:54.667556 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0673 (* 1 = 10.0673 loss)
I0523 23:37:55.573911 11835 sgd_solver.cpp:112] Iteration 8830, lr = 0.1
I0523 23:38:01.279947 11835 solver.cpp:239] Iteration 8840 (1.51237 iter/s, 6.61213s/10 iters), loss = 9.54256
I0523 23:38:01.280194 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.54256 (* 1 = 9.54256 loss)
I0523 23:38:01.280242 11835 sgd_solver.cpp:112] Iteration 8840, lr = 0.1
I0523 23:38:08.247473 11835 solver.cpp:239] Iteration 8850 (1.43533 iter/s, 6.96704s/10 iters), loss = 9.10674
I0523 23:38:08.247530 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.10674 (* 1 = 9.10674 loss)
I0523 23:38:08.247812 11835 sgd_solver.cpp:112] Iteration 8850, lr = 0.1
I0523 23:38:15.753458 11835 solver.cpp:239] Iteration 8860 (1.33233 iter/s, 7.50564s/10 iters), loss = 10.5709
I0523 23:38:15.753520 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5709 (* 1 = 10.5709 loss)
I0523 23:38:15.753597 11835 sgd_solver.cpp:112] Iteration 8860, lr = 0.1
I0523 23:38:22.770902 11835 solver.cpp:239] Iteration 8870 (1.42509 iter/s, 7.01712s/10 iters), loss = 9.7016
I0523 23:38:22.770951 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.7016 (* 1 = 9.7016 loss)
I0523 23:38:22.771071 11835 sgd_solver.cpp:112] Iteration 8870, lr = 0.1
I0523 23:38:28.830655 11835 solver.cpp:239] Iteration 8880 (1.65031 iter/s, 6.05948s/10 iters), loss = 9.0326
I0523 23:38:28.830715 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.0326 (* 1 = 9.0326 loss)
I0523 23:38:28.830729 11835 sgd_solver.cpp:112] Iteration 8880, lr = 0.1
I0523 23:38:36.055575 11835 solver.cpp:239] Iteration 8890 (1.38428 iter/s, 7.22396s/10 iters), loss = 9.65004
I0523 23:38:36.055825 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.65004 (* 1 = 9.65004 loss)
I0523 23:38:36.298548 11835 sgd_solver.cpp:112] Iteration 8890, lr = 0.1
I0523 23:38:43.146981 11835 solver.cpp:239] Iteration 8900 (1.41025 iter/s, 7.09093s/10 iters), loss = 9.55099
I0523 23:38:43.147027 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.55099 (* 1 = 9.55099 loss)
I0523 23:38:43.147575 11835 sgd_solver.cpp:112] Iteration 8900, lr = 0.1
I0523 23:38:50.041590 11835 solver.cpp:239] Iteration 8910 (1.45047 iter/s, 6.8943s/10 iters), loss = 9.47625
I0523 23:38:50.041641 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.47625 (* 1 = 9.47625 loss)
I0523 23:38:50.376294 11835 sgd_solver.cpp:112] Iteration 8910, lr = 0.1
I0523 23:38:56.310067 11835 solver.cpp:239] Iteration 8920 (1.59536 iter/s, 6.26817s/10 iters), loss = 9.57058
I0523 23:38:56.310128 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.57058 (* 1 = 9.57058 loss)
I0523 23:38:56.310164 11835 sgd_solver.cpp:112] Iteration 8920, lr = 0.1
I0523 23:39:02.226037 11835 solver.cpp:239] Iteration 8930 (1.69042 iter/s, 5.9157s/10 iters), loss = 9.37085
I0523 23:39:02.226074 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.37085 (* 1 = 9.37085 loss)
I0523 23:39:02.226167 11835 sgd_solver.cpp:112] Iteration 8930, lr = 0.1
I0523 23:39:07.979749 11835 solver.cpp:239] Iteration 8940 (1.73809 iter/s, 5.75345s/10 iters), loss = 10.4195
I0523 23:39:07.980026 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.4195 (* 1 = 10.4195 loss)
I0523 23:39:07.980075 11835 sgd_solver.cpp:112] Iteration 8940, lr = 0.1
I0523 23:39:13.822113 11835 solver.cpp:239] Iteration 8950 (1.71178 iter/s, 5.84188s/10 iters), loss = 8.89027
I0523 23:39:13.822150 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.89027 (* 1 = 8.89027 loss)
I0523 23:39:14.286360 11835 sgd_solver.cpp:112] Iteration 8950, lr = 0.1
I0523 23:39:21.414361 11835 solver.cpp:239] Iteration 8960 (1.31719 iter/s, 7.59191s/10 iters), loss = 9.58214
I0523 23:39:21.414415 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.58214 (* 1 = 9.58214 loss)
I0523 23:39:21.414455 11835 sgd_solver.cpp:112] Iteration 8960, lr = 0.1
I0523 23:39:29.453346 11835 solver.cpp:239] Iteration 8970 (1.24399 iter/s, 8.03863s/10 iters), loss = 8.99786
I0523 23:39:29.453394 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.99786 (* 1 = 8.99786 loss)
I0523 23:39:29.457929 11835 sgd_solver.cpp:112] Iteration 8970, lr = 0.1
I0523 23:39:37.521922 11835 solver.cpp:239] Iteration 8980 (1.23943 iter/s, 8.06822s/10 iters), loss = 9.94622
I0523 23:39:37.521976 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.94622 (* 1 = 9.94622 loss)
I0523 23:39:37.522161 11835 sgd_solver.cpp:112] Iteration 8980, lr = 0.1
I0523 23:39:44.702780 11835 solver.cpp:239] Iteration 8990 (1.39265 iter/s, 7.18054s/10 iters), loss = 9.25106
I0523 23:39:44.703001 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.25106 (* 1 = 9.25106 loss)
I0523 23:39:45.346828 11835 sgd_solver.cpp:112] Iteration 8990, lr = 0.1
I0523 23:39:51.369488 11835 solver.cpp:239] Iteration 9000 (1.50009 iter/s, 6.66625s/10 iters), loss = 8.95026
I0523 23:39:51.369539 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.95026 (* 1 = 8.95026 loss)
I0523 23:39:51.369554 11835 sgd_solver.cpp:112] Iteration 9000, lr = 0.1
I0523 23:39:57.433302 11835 solver.cpp:239] Iteration 9010 (1.64923 iter/s, 6.06343s/10 iters), loss = 10.1129
I0523 23:39:57.433341 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.1129 (* 1 = 10.1129 loss)
I0523 23:39:57.433529 11835 sgd_solver.cpp:112] Iteration 9010, lr = 0.1
I0523 23:40:04.412132 11835 solver.cpp:239] Iteration 9020 (1.43297 iter/s, 6.97852s/10 iters), loss = 8.85805
I0523 23:40:04.412181 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.85805 (* 1 = 8.85805 loss)
I0523 23:40:04.412442 11835 sgd_solver.cpp:112] Iteration 9020, lr = 0.1
I0523 23:40:11.209379 11835 solver.cpp:239] Iteration 9030 (1.47125 iter/s, 6.79693s/10 iters), loss = 8.63773
I0523 23:40:11.209434 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.63773 (* 1 = 8.63773 loss)
I0523 23:40:11.209482 11835 sgd_solver.cpp:112] Iteration 9030, lr = 0.1
I0523 23:40:17.185310 11835 solver.cpp:239] Iteration 9040 (1.67346 iter/s, 5.97565s/10 iters), loss = 9.77309
I0523 23:40:17.185493 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.77309 (* 1 = 9.77309 loss)
I0523 23:40:17.185624 11835 sgd_solver.cpp:112] Iteration 9040, lr = 0.1
I0523 23:40:22.826346 11835 solver.cpp:239] Iteration 9050 (1.77285 iter/s, 5.64063s/10 iters), loss = 9.75899
I0523 23:40:22.826403 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.75899 (* 1 = 9.75899 loss)
I0523 23:40:22.826687 11835 sgd_solver.cpp:112] Iteration 9050, lr = 0.1
I0523 23:40:28.696997 11835 solver.cpp:239] Iteration 9060 (1.70347 iter/s, 5.87038s/10 iters), loss = 8.95152
I0523 23:40:28.697041 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.95152 (* 1 = 8.95152 loss)
I0523 23:40:28.697057 11835 sgd_solver.cpp:112] Iteration 9060, lr = 0.1
I0523 23:40:36.035193 11835 solver.cpp:239] Iteration 9070 (1.36279 iter/s, 7.33788s/10 iters), loss = 9.06409
I0523 23:40:36.035234 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.06409 (* 1 = 9.06409 loss)
I0523 23:40:36.069202 11835 sgd_solver.cpp:112] Iteration 9070, lr = 0.1
I0523 23:40:41.791599 11835 solver.cpp:239] Iteration 9080 (1.73727 iter/s, 5.75614s/10 iters), loss = 9.70634
I0523 23:40:41.791646 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.70634 (* 1 = 9.70634 loss)
I0523 23:40:41.961800 11835 sgd_solver.cpp:112] Iteration 9080, lr = 0.1
I0523 23:40:47.927084 11835 solver.cpp:239] Iteration 9090 (1.62994 iter/s, 6.1352s/10 iters), loss = 9.10209
I0523 23:40:47.927274 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.10209 (* 1 = 9.10209 loss)
I0523 23:40:47.929163 11835 sgd_solver.cpp:112] Iteration 9090, lr = 0.1
I0523 23:40:55.037854 11835 solver.cpp:239] Iteration 9100 (1.40641 iter/s, 7.11032s/10 iters), loss = 8.94159
I0523 23:40:55.037899 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.94159 (* 1 = 8.94159 loss)
I0523 23:40:55.038118 11835 sgd_solver.cpp:112] Iteration 9100, lr = 0.1
I0523 23:41:01.540562 11835 solver.cpp:239] Iteration 9110 (1.53789 iter/s, 6.50241s/10 iters), loss = 9.8485
I0523 23:41:01.540617 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.8485 (* 1 = 9.8485 loss)
I0523 23:41:01.540843 11835 sgd_solver.cpp:112] Iteration 9110, lr = 0.1
I0523 23:41:09.533735 11835 solver.cpp:239] Iteration 9120 (1.25112 iter/s, 7.99281s/10 iters), loss = 9.72031
I0523 23:41:09.533783 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.72031 (* 1 = 9.72031 loss)
I0523 23:41:09.534024 11835 sgd_solver.cpp:112] Iteration 9120, lr = 0.1
I0523 23:41:15.310246 11835 solver.cpp:239] Iteration 9130 (1.73123 iter/s, 5.77625s/10 iters), loss = 9.76963
I0523 23:41:15.310286 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.76963 (* 1 = 9.76963 loss)
I0523 23:41:15.310299 11835 sgd_solver.cpp:112] Iteration 9130, lr = 0.1
I0523 23:41:21.685467 11835 solver.cpp:239] Iteration 9140 (1.56864 iter/s, 6.37494s/10 iters), loss = 9.16064
I0523 23:41:21.685798 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.16064 (* 1 = 9.16064 loss)
I0523 23:41:21.953667 11835 sgd_solver.cpp:112] Iteration 9140, lr = 0.1
I0523 23:41:27.671898 11835 solver.cpp:239] Iteration 9150 (1.67057 iter/s, 5.98597s/10 iters), loss = 10.3251
I0523 23:41:27.671954 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3251 (* 1 = 10.3251 loss)
I0523 23:41:27.671972 11835 sgd_solver.cpp:112] Iteration 9150, lr = 0.1
I0523 23:41:34.904047 11835 solver.cpp:239] Iteration 9160 (1.38279 iter/s, 7.23176s/10 iters), loss = 9.22505
I0523 23:41:34.904083 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.22505 (* 1 = 9.22505 loss)
I0523 23:41:34.904096 11835 sgd_solver.cpp:112] Iteration 9160, lr = 0.1
I0523 23:41:43.219700 11835 solver.cpp:239] Iteration 9170 (1.2026 iter/s, 8.31529s/10 iters), loss = 9.04752
I0523 23:41:43.219761 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.04752 (* 1 = 9.04752 loss)
I0523 23:41:43.219985 11835 sgd_solver.cpp:112] Iteration 9170, lr = 0.1
I0523 23:41:49.412679 11835 solver.cpp:239] Iteration 9180 (1.61481 iter/s, 6.19269s/10 iters), loss = 9.94892
I0523 23:41:49.412725 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.94892 (* 1 = 9.94892 loss)
I0523 23:41:49.412739 11835 sgd_solver.cpp:112] Iteration 9180, lr = 0.1
I0523 23:41:55.951318 11835 solver.cpp:239] Iteration 9190 (1.52944 iter/s, 6.53834s/10 iters), loss = 9.31211
I0523 23:41:55.951634 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.31211 (* 1 = 9.31211 loss)
I0523 23:41:55.951689 11835 sgd_solver.cpp:112] Iteration 9190, lr = 0.1
I0523 23:42:01.599392 11835 solver.cpp:239] Iteration 9200 (1.7707 iter/s, 5.64748s/10 iters), loss = 9.0143
I0523 23:42:01.599434 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.0143 (* 1 = 9.0143 loss)
I0523 23:42:01.915968 11835 sgd_solver.cpp:112] Iteration 9200, lr = 0.1
I0523 23:42:07.639636 11835 solver.cpp:239] Iteration 9210 (1.65564 iter/s, 6.03996s/10 iters), loss = 8.85285
I0523 23:42:07.639698 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.85285 (* 1 = 8.85285 loss)
I0523 23:42:07.639717 11835 sgd_solver.cpp:112] Iteration 9210, lr = 0.1
I0523 23:42:15.483016 11835 solver.cpp:239] Iteration 9220 (1.27532 iter/s, 7.84116s/10 iters), loss = 9.1668
I0523 23:42:15.483062 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.1668 (* 1 = 9.1668 loss)
I0523 23:42:15.483077 11835 sgd_solver.cpp:112] Iteration 9220, lr = 0.1
I0523 23:42:21.811735 11835 solver.cpp:239] Iteration 9230 (1.58033 iter/s, 6.3278s/10 iters), loss = 10.3108
I0523 23:42:21.811786 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.3108 (* 1 = 10.3108 loss)
I0523 23:42:21.832954 11835 sgd_solver.cpp:112] Iteration 9230, lr = 0.1
I0523 23:42:28.369663 11835 solver.cpp:239] Iteration 9240 (1.52494 iter/s, 6.55761s/10 iters), loss = 8.70423
I0523 23:42:28.369794 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.70423 (* 1 = 8.70423 loss)
I0523 23:42:28.421576 11835 sgd_solver.cpp:112] Iteration 9240, lr = 0.1
I0523 23:42:34.112807 11835 solver.cpp:239] Iteration 9250 (1.74131 iter/s, 5.7428s/10 iters), loss = 9.13056
I0523 23:42:34.112850 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.13056 (* 1 = 9.13056 loss)
I0523 23:42:34.112980 11835 sgd_solver.cpp:112] Iteration 9250, lr = 0.1
I0523 23:42:40.556324 11835 solver.cpp:239] Iteration 9260 (1.55202 iter/s, 6.44322s/10 iters), loss = 8.84579
I0523 23:42:40.556375 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.84579 (* 1 = 8.84579 loss)
I0523 23:42:40.556475 11835 sgd_solver.cpp:112] Iteration 9260, lr = 0.1
I0523 23:42:47.090452 11835 solver.cpp:239] Iteration 9270 (1.53049 iter/s, 6.53384s/10 iters), loss = 9.66626
I0523 23:42:47.090492 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.66626 (* 1 = 9.66626 loss)
I0523 23:42:47.131325 11835 sgd_solver.cpp:112] Iteration 9270, lr = 0.1
I0523 23:42:55.087685 11835 solver.cpp:239] Iteration 9280 (1.25049 iter/s, 7.99689s/10 iters), loss = 9.12434
I0523 23:42:55.087734 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.12434 (* 1 = 9.12434 loss)
I0523 23:42:55.406436 11835 sgd_solver.cpp:112] Iteration 9280, lr = 0.1
I0523 23:43:03.814924 11835 solver.cpp:239] Iteration 9290 (1.14589 iter/s, 8.72685s/10 iters), loss = 9.96701
I0523 23:43:03.815076 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.96701 (* 1 = 9.96701 loss)
I0523 23:43:04.100594 11835 sgd_solver.cpp:112] Iteration 9290, lr = 0.1
I0523 23:43:09.932200 11835 solver.cpp:239] Iteration 9300 (1.63482 iter/s, 6.11689s/10 iters), loss = 9.57782
I0523 23:43:09.932255 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.57782 (* 1 = 9.57782 loss)
I0523 23:43:09.932530 11835 sgd_solver.cpp:112] Iteration 9300, lr = 0.1
I0523 23:43:17.158318 11835 solver.cpp:239] Iteration 9310 (1.38393 iter/s, 7.2258s/10 iters), loss = 9.45215
I0523 23:43:17.158365 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.45215 (* 1 = 9.45215 loss)
I0523 23:43:17.158612 11835 sgd_solver.cpp:112] Iteration 9310, lr = 0.1
I0523 23:43:24.959151 11835 solver.cpp:239] Iteration 9320 (1.28197 iter/s, 7.8005s/10 iters), loss = 9.30041
I0523 23:43:24.959193 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.30041 (* 1 = 9.30041 loss)
I0523 23:43:25.062430 11835 sgd_solver.cpp:112] Iteration 9320, lr = 0.1
I0523 23:43:31.914978 11835 solver.cpp:239] Iteration 9330 (1.43771 iter/s, 6.95551s/10 iters), loss = 9.11264
I0523 23:43:31.915031 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.11264 (* 1 = 9.11264 loss)
I0523 23:43:32.022610 11835 sgd_solver.cpp:112] Iteration 9330, lr = 0.1
I0523 23:43:38.256199 11835 solver.cpp:239] Iteration 9340 (1.57705 iter/s, 6.34093s/10 iters), loss = 9.94127
I0523 23:43:38.256331 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.94127 (* 1 = 9.94127 loss)
I0523 23:43:38.256505 11835 sgd_solver.cpp:112] Iteration 9340, lr = 0.1
I0523 23:43:45.185379 11835 solver.cpp:239] Iteration 9350 (1.44325 iter/s, 6.92879s/10 iters), loss = 9.52855
I0523 23:43:45.185432 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.52855 (* 1 = 9.52855 loss)
I0523 23:43:45.185541 11835 sgd_solver.cpp:112] Iteration 9350, lr = 0.1
I0523 23:43:52.834794 11835 solver.cpp:239] Iteration 9360 (1.30735 iter/s, 7.64907s/10 iters), loss = 8.84663
I0523 23:43:52.834851 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.84663 (* 1 = 8.84663 loss)
I0523 23:43:52.835124 11835 sgd_solver.cpp:112] Iteration 9360, lr = 0.1
I0523 23:43:58.653282 11835 solver.cpp:239] Iteration 9370 (1.71875 iter/s, 5.8182s/10 iters), loss = 8.68051
I0523 23:43:58.653331 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.68051 (* 1 = 8.68051 loss)
I0523 23:43:58.722048 11835 sgd_solver.cpp:112] Iteration 9370, lr = 0.1
I0523 23:44:04.405522 11835 solver.cpp:239] Iteration 9380 (1.73853 iter/s, 5.75198s/10 iters), loss = 9.70569
I0523 23:44:04.405568 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.70569 (* 1 = 9.70569 loss)
I0523 23:44:04.406322 11835 sgd_solver.cpp:112] Iteration 9380, lr = 0.1
I0523 23:44:12.534005 11835 solver.cpp:239] Iteration 9390 (1.2303 iter/s, 8.12813s/10 iters), loss = 10.5268
I0523 23:44:12.534266 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.5268 (* 1 = 10.5268 loss)
I0523 23:44:13.504935 11835 sgd_solver.cpp:112] Iteration 9390, lr = 0.1
I0523 23:44:19.575060 11835 solver.cpp:239] Iteration 9400 (1.42034 iter/s, 7.04058s/10 iters), loss = 8.35356
I0523 23:44:19.575114 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.35356 (* 1 = 8.35356 loss)
I0523 23:44:20.354771 11835 sgd_solver.cpp:112] Iteration 9400, lr = 0.1
I0523 23:44:26.468752 11835 solver.cpp:239] Iteration 9410 (1.45067 iter/s, 6.89338s/10 iters), loss = 9.07011
I0523 23:44:26.468799 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.07011 (* 1 = 9.07011 loss)
I0523 23:44:26.469074 11835 sgd_solver.cpp:112] Iteration 9410, lr = 0.1
I0523 23:44:33.614485 11835 solver.cpp:239] Iteration 9420 (1.3995 iter/s, 7.14542s/10 iters), loss = 8.83053
I0523 23:44:33.614531 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.83053 (* 1 = 8.83053 loss)
I0523 23:44:33.614665 11835 sgd_solver.cpp:112] Iteration 9420, lr = 0.1
I0523 23:44:41.953042 11835 solver.cpp:239] Iteration 9430 (1.1993 iter/s, 8.3382s/10 iters), loss = 8.99297
I0523 23:44:41.953083 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.99297 (* 1 = 8.99297 loss)
I0523 23:44:41.953194 11835 sgd_solver.cpp:112] Iteration 9430, lr = 0.1
I0523 23:44:48.497486 11835 solver.cpp:239] Iteration 9440 (1.52808 iter/s, 6.54415s/10 iters), loss = 9.38594
I0523 23:44:48.497720 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.38594 (* 1 = 9.38594 loss)
I0523 23:44:48.497767 11835 sgd_solver.cpp:112] Iteration 9440, lr = 0.1
I0523 23:44:54.607944 11835 solver.cpp:239] Iteration 9450 (1.63694 iter/s, 6.10895s/10 iters), loss = 9.16979
I0523 23:44:54.607985 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.16979 (* 1 = 9.16979 loss)
I0523 23:44:54.951774 11835 sgd_solver.cpp:112] Iteration 9450, lr = 0.1
I0523 23:45:01.391254 11835 solver.cpp:239] Iteration 9460 (1.47427 iter/s, 6.783s/10 iters), loss = 9.4179
I0523 23:45:01.391304 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.4179 (* 1 = 9.4179 loss)
I0523 23:45:01.391386 11835 sgd_solver.cpp:112] Iteration 9460, lr = 0.1
I0523 23:45:07.590800 11835 solver.cpp:239] Iteration 9470 (1.61309 iter/s, 6.19927s/10 iters), loss = 9.65932
I0523 23:45:07.590850 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.65932 (* 1 = 9.65932 loss)
I0523 23:45:07.820042 11835 sgd_solver.cpp:112] Iteration 9470, lr = 0.1
I0523 23:45:14.609880 11835 solver.cpp:239] Iteration 9480 (1.42476 iter/s, 7.01875s/10 iters), loss = 9.94477
I0523 23:45:14.609948 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.94477 (* 1 = 9.94477 loss)
I0523 23:45:14.609994 11835 sgd_solver.cpp:112] Iteration 9480, lr = 0.1
I0523 23:45:20.712996 11835 solver.cpp:239] Iteration 9490 (1.63858 iter/s, 6.10283s/10 iters), loss = 9.78271
I0523 23:45:20.713228 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.78271 (* 1 = 9.78271 loss)
I0523 23:45:20.713276 11835 sgd_solver.cpp:112] Iteration 9490, lr = 0.1
I0523 23:45:26.670958 11835 solver.cpp:239] Iteration 9500 (1.67858 iter/s, 5.9574s/10 iters), loss = 8.91138
I0523 23:45:26.671020 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.91138 (* 1 = 8.91138 loss)
I0523 23:45:26.671041 11835 sgd_solver.cpp:112] Iteration 9500, lr = 0.1
I0523 23:45:33.441648 11835 solver.cpp:239] Iteration 9510 (1.47705 iter/s, 6.77024s/10 iters), loss = 9.24571
I0523 23:45:33.441694 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.24571 (* 1 = 9.24571 loss)
I0523 23:45:33.441839 11835 sgd_solver.cpp:112] Iteration 9510, lr = 0.1
I0523 23:45:39.167479 11835 solver.cpp:239] Iteration 9520 (1.74655 iter/s, 5.72556s/10 iters), loss = 9.65016
I0523 23:45:39.167529 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.65016 (* 1 = 9.65016 loss)
I0523 23:45:39.167714 11835 sgd_solver.cpp:112] Iteration 9520, lr = 0.1
I0523 23:45:44.958828 11835 solver.cpp:239] Iteration 9530 (1.72679 iter/s, 5.79109s/10 iters), loss = 8.53481
I0523 23:45:44.958865 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.53481 (* 1 = 8.53481 loss)
I0523 23:45:44.959107 11835 sgd_solver.cpp:112] Iteration 9530, lr = 0.1
I0523 23:45:52.787672 11835 solver.cpp:239] Iteration 9540 (1.27738 iter/s, 7.8285s/10 iters), loss = 8.74799
I0523 23:45:52.787845 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.74799 (* 1 = 8.74799 loss)
I0523 23:45:52.787992 11835 sgd_solver.cpp:112] Iteration 9540, lr = 0.1
I0523 23:46:00.099817 11835 solver.cpp:239] Iteration 9550 (1.36767 iter/s, 7.3117s/10 iters), loss = 8.75835
I0523 23:46:00.099866 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.75835 (* 1 = 8.75835 loss)
I0523 23:46:00.100195 11835 sgd_solver.cpp:112] Iteration 9550, lr = 0.1
I0523 23:46:06.092942 11835 solver.cpp:239] Iteration 9560 (1.66865 iter/s, 5.99285s/10 iters), loss = 8.92001
I0523 23:46:06.092990 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.92001 (* 1 = 8.92001 loss)
I0523 23:46:06.093004 11835 sgd_solver.cpp:112] Iteration 9560, lr = 0.1
I0523 23:46:15.735560 11835 solver.cpp:239] Iteration 9570 (1.03711 iter/s, 9.64222s/10 iters), loss = 8.59577
I0523 23:46:15.735597 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.59577 (* 1 = 8.59577 loss)
I0523 23:46:15.735721 11835 sgd_solver.cpp:112] Iteration 9570, lr = 0.1
I0523 23:46:21.902659 11835 solver.cpp:239] Iteration 9580 (1.62158 iter/s, 6.16682s/10 iters), loss = 8.85223
I0523 23:46:21.902745 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.85223 (* 1 = 8.85223 loss)
I0523 23:46:21.902765 11835 sgd_solver.cpp:112] Iteration 9580, lr = 0.1
I0523 23:46:29.142834 11835 solver.cpp:239] Iteration 9590 (1.38137 iter/s, 7.2392s/10 iters), loss = 8.43788
I0523 23:46:29.143007 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.43788 (* 1 = 8.43788 loss)
I0523 23:46:29.823045 11835 sgd_solver.cpp:112] Iteration 9590, lr = 0.1
I0523 23:46:37.858908 11835 solver.cpp:239] Iteration 9600 (1.14737 iter/s, 8.71558s/10 iters), loss = 9.88713
I0523 23:46:37.858961 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.88713 (* 1 = 9.88713 loss)
I0523 23:46:38.311090 11835 sgd_solver.cpp:112] Iteration 9600, lr = 0.1
I0523 23:46:46.194722 11835 solver.cpp:239] Iteration 9610 (1.1997 iter/s, 8.33543s/10 iters), loss = 8.31375
I0523 23:46:46.194784 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.31375 (* 1 = 8.31375 loss)
I0523 23:46:46.554720 11835 sgd_solver.cpp:112] Iteration 9610, lr = 0.1
I0523 23:46:54.370816 11835 solver.cpp:239] Iteration 9620 (1.22313 iter/s, 8.17572s/10 iters), loss = 9.47587
I0523 23:46:54.370873 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.47587 (* 1 = 9.47587 loss)
I0523 23:46:54.370961 11835 sgd_solver.cpp:112] Iteration 9620, lr = 0.1
I0523 23:47:01.360879 11835 solver.cpp:239] Iteration 9630 (1.43067 iter/s, 6.98975s/10 iters), loss = 9.30914
I0523 23:47:01.361143 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.30914 (* 1 = 9.30914 loss)
I0523 23:47:01.361201 11835 sgd_solver.cpp:112] Iteration 9630, lr = 0.1
I0523 23:47:08.494534 11835 solver.cpp:239] Iteration 9640 (1.40232 iter/s, 7.13102s/10 iters), loss = 9.28392
I0523 23:47:08.494592 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.28392 (* 1 = 9.28392 loss)
I0523 23:47:08.494819 11835 sgd_solver.cpp:112] Iteration 9640, lr = 0.1
I0523 23:47:14.696250 11835 solver.cpp:239] Iteration 9650 (1.61253 iter/s, 6.20142s/10 iters), loss = 10.0437
I0523 23:47:14.696300 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0437 (* 1 = 10.0437 loss)
I0523 23:47:14.696411 11835 sgd_solver.cpp:112] Iteration 9650, lr = 0.1
I0523 23:47:20.839279 11835 solver.cpp:239] Iteration 9660 (1.62794 iter/s, 6.14274s/10 iters), loss = 9.17135
I0523 23:47:20.839327 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.17135 (* 1 = 9.17135 loss)
I0523 23:47:20.839342 11835 sgd_solver.cpp:112] Iteration 9660, lr = 0.1
I0523 23:47:27.273006 11835 solver.cpp:239] Iteration 9670 (1.55441 iter/s, 6.4333s/10 iters), loss = 8.60757
I0523 23:47:27.273058 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.60757 (* 1 = 8.60757 loss)
I0523 23:47:27.302354 11835 sgd_solver.cpp:112] Iteration 9670, lr = 0.1
I0523 23:47:34.662621 11835 solver.cpp:239] Iteration 9680 (1.35331 iter/s, 7.38927s/10 iters), loss = 8.74921
I0523 23:47:34.662863 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.74921 (* 1 = 8.74921 loss)
I0523 23:47:34.662958 11835 sgd_solver.cpp:112] Iteration 9680, lr = 0.1
I0523 23:47:40.750726 11835 solver.cpp:239] Iteration 9690 (1.64274 iter/s, 6.0874s/10 iters), loss = 9.18281
I0523 23:47:40.750783 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.18281 (* 1 = 9.18281 loss)
I0523 23:47:40.750967 11835 sgd_solver.cpp:112] Iteration 9690, lr = 0.1
I0523 23:47:47.956334 11835 solver.cpp:239] Iteration 9700 (1.38787 iter/s, 7.20527s/10 iters), loss = 8.9218
I0523 23:47:47.956403 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.9218 (* 1 = 8.9218 loss)
I0523 23:47:47.956579 11835 sgd_solver.cpp:112] Iteration 9700, lr = 0.1
I0523 23:47:53.885601 11835 solver.cpp:239] Iteration 9710 (1.68663 iter/s, 5.92899s/10 iters), loss = 9.24126
I0523 23:47:53.885640 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.24126 (* 1 = 9.24126 loss)
I0523 23:47:54.446359 11835 sgd_solver.cpp:112] Iteration 9710, lr = 0.1
I0523 23:48:02.017340 11835 solver.cpp:239] Iteration 9720 (1.2298 iter/s, 8.13138s/10 iters), loss = 8.47195
I0523 23:48:02.017393 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.47195 (* 1 = 8.47195 loss)
I0523 23:48:02.097425 11835 sgd_solver.cpp:112] Iteration 9720, lr = 0.1
I0523 23:48:07.723723 11835 solver.cpp:239] Iteration 9730 (1.75251 iter/s, 5.70611s/10 iters), loss = 9.52142
I0523 23:48:07.723984 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.52142 (* 1 = 9.52142 loss)
I0523 23:48:07.724030 11835 sgd_solver.cpp:112] Iteration 9730, lr = 0.1
I0523 23:48:14.631414 11835 solver.cpp:239] Iteration 9740 (1.44821 iter/s, 6.90509s/10 iters), loss = 8.97295
I0523 23:48:14.631464 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.97295 (* 1 = 8.97295 loss)
I0523 23:48:14.631479 11835 sgd_solver.cpp:112] Iteration 9740, lr = 0.1
I0523 23:48:21.152719 11835 solver.cpp:239] Iteration 9750 (1.53402 iter/s, 6.51882s/10 iters), loss = 9.41817
I0523 23:48:21.152765 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.41817 (* 1 = 9.41817 loss)
I0523 23:48:21.152957 11835 sgd_solver.cpp:112] Iteration 9750, lr = 0.1
I0523 23:48:27.029379 11835 solver.cpp:239] Iteration 9760 (1.70173 iter/s, 5.87638s/10 iters), loss = 9.01786
I0523 23:48:27.029430 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.01786 (* 1 = 9.01786 loss)
I0523 23:48:27.072692 11835 sgd_solver.cpp:112] Iteration 9760, lr = 0.1
I0523 23:48:34.615829 11835 solver.cpp:239] Iteration 9770 (1.3182 iter/s, 7.58612s/10 iters), loss = 8.86965
I0523 23:48:34.615869 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.86965 (* 1 = 8.86965 loss)
I0523 23:48:34.616067 11835 sgd_solver.cpp:112] Iteration 9770, lr = 0.1
I0523 23:48:41.938434 11835 solver.cpp:239] Iteration 9780 (1.36569 iter/s, 7.32228s/10 iters), loss = 8.20333
I0523 23:48:41.938537 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.20333 (* 1 = 8.20333 loss)
I0523 23:48:41.952363 11835 sgd_solver.cpp:112] Iteration 9780, lr = 0.1
I0523 23:48:48.109043 11835 solver.cpp:239] Iteration 9790 (1.62067 iter/s, 6.17027s/10 iters), loss = 10.0221
I0523 23:48:48.109107 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0221 (* 1 = 10.0221 loss)
I0523 23:48:48.109253 11835 sgd_solver.cpp:112] Iteration 9790, lr = 0.1
I0523 23:48:54.446482 11835 solver.cpp:239] Iteration 9800 (1.578 iter/s, 6.33715s/10 iters), loss = 8.78001
I0523 23:48:54.446522 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.78001 (* 1 = 8.78001 loss)
I0523 23:48:54.650632 11835 sgd_solver.cpp:112] Iteration 9800, lr = 0.1
I0523 23:49:04.036686 11835 solver.cpp:239] Iteration 9810 (1.04278 iter/s, 9.58979s/10 iters), loss = 9.59194
I0523 23:49:04.036737 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.59194 (* 1 = 9.59194 loss)
I0523 23:49:04.109649 11835 sgd_solver.cpp:112] Iteration 9810, lr = 0.1
I0523 23:49:10.212771 11835 solver.cpp:239] Iteration 9820 (1.61923 iter/s, 6.17579s/10 iters), loss = 9.31182
I0523 23:49:10.212822 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.31182 (* 1 = 9.31182 loss)
I0523 23:49:11.189643 11835 sgd_solver.cpp:112] Iteration 9820, lr = 0.1
I0523 23:49:17.215116 11835 solver.cpp:239] Iteration 9830 (1.42816 iter/s, 7.00204s/10 iters), loss = 9.47241
I0523 23:49:17.215215 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.47241 (* 1 = 9.47241 loss)
I0523 23:49:17.215456 11835 sgd_solver.cpp:112] Iteration 9830, lr = 0.1
I0523 23:49:23.754842 11835 solver.cpp:239] Iteration 9840 (1.5292 iter/s, 6.53937s/10 iters), loss = 9.60152
I0523 23:49:23.754899 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.60152 (* 1 = 9.60152 loss)
I0523 23:49:23.755117 11835 sgd_solver.cpp:112] Iteration 9840, lr = 0.1
I0523 23:49:29.615350 11835 solver.cpp:239] Iteration 9850 (1.70642 iter/s, 5.86023s/10 iters), loss = 8.35373
I0523 23:49:29.615401 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.35373 (* 1 = 8.35373 loss)
I0523 23:49:29.615514 11835 sgd_solver.cpp:112] Iteration 9850, lr = 0.1
I0523 23:49:37.363507 11835 solver.cpp:239] Iteration 9860 (1.29069 iter/s, 7.74782s/10 iters), loss = 9.2147
I0523 23:49:37.363553 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.2147 (* 1 = 9.2147 loss)
I0523 23:49:37.363808 11835 sgd_solver.cpp:112] Iteration 9860, lr = 0.1
I0523 23:49:45.677328 11835 solver.cpp:239] Iteration 9870 (1.20287 iter/s, 8.31346s/10 iters), loss = 9.28659
I0523 23:49:45.677381 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.28659 (* 1 = 9.28659 loss)
I0523 23:49:46.182390 11835 sgd_solver.cpp:112] Iteration 9870, lr = 0.1
I0523 23:49:54.870878 11835 solver.cpp:239] Iteration 9880 (1.08777 iter/s, 9.19315s/10 iters), loss = 8.23352
I0523 23:49:54.871026 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.23352 (* 1 = 8.23352 loss)
I0523 23:49:54.883208 11835 sgd_solver.cpp:112] Iteration 9880, lr = 0.1
I0523 23:50:02.004060 11835 solver.cpp:239] Iteration 9890 (1.40198 iter/s, 7.13278s/10 iters), loss = 8.03495
I0523 23:50:02.004104 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.03495 (* 1 = 8.03495 loss)
I0523 23:50:02.004194 11835 sgd_solver.cpp:112] Iteration 9890, lr = 0.1
I0523 23:50:10.819996 11835 solver.cpp:239] Iteration 9900 (1.13436 iter/s, 8.81555s/10 iters), loss = 8.59495
I0523 23:50:10.820052 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.59495 (* 1 = 8.59495 loss)
I0523 23:50:10.820303 11835 sgd_solver.cpp:112] Iteration 9900, lr = 0.1
I0523 23:50:19.827950 11835 solver.cpp:239] Iteration 9910 (1.11018 iter/s, 9.00756s/10 iters), loss = 8.72163
I0523 23:50:19.828004 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.72163 (* 1 = 8.72163 loss)
I0523 23:50:19.828258 11835 sgd_solver.cpp:112] Iteration 9910, lr = 0.1
I0523 23:50:25.919265 11835 solver.cpp:239] Iteration 9920 (1.64176 iter/s, 6.09104s/10 iters), loss = 8.48909
I0523 23:50:25.919536 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.48909 (* 1 = 8.48909 loss)
I0523 23:50:25.919592 11835 sgd_solver.cpp:112] Iteration 9920, lr = 0.1
I0523 23:50:31.860065 11835 solver.cpp:239] Iteration 9930 (1.68341 iter/s, 5.94033s/10 iters), loss = 8.78818
I0523 23:50:31.860121 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.78818 (* 1 = 8.78818 loss)
I0523 23:50:31.862061 11835 sgd_solver.cpp:112] Iteration 9930, lr = 0.1
I0523 23:50:38.897473 11835 solver.cpp:239] Iteration 9940 (1.42104 iter/s, 7.03709s/10 iters), loss = 9.21255
I0523 23:50:38.897528 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.21255 (* 1 = 9.21255 loss)
I0523 23:50:38.897946 11835 sgd_solver.cpp:112] Iteration 9940, lr = 0.1
I0523 23:50:46.521924 11835 solver.cpp:239] Iteration 9950 (1.31163 iter/s, 7.62411s/10 iters), loss = 9.52799
I0523 23:50:46.521971 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.52799 (* 1 = 9.52799 loss)
I0523 23:50:46.522097 11835 sgd_solver.cpp:112] Iteration 9950, lr = 0.1
I0523 23:50:53.519398 11835 solver.cpp:239] Iteration 9960 (1.42915 iter/s, 6.99716s/10 iters), loss = 9.1922
I0523 23:50:53.519448 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.1922 (* 1 = 9.1922 loss)
I0523 23:50:53.519462 11835 sgd_solver.cpp:112] Iteration 9960, lr = 0.1
I0523 23:51:00.637754 11835 solver.cpp:239] Iteration 9970 (1.40493 iter/s, 7.11779s/10 iters), loss = 9.10498
I0523 23:51:00.637960 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.10498 (* 1 = 9.10498 loss)
I0523 23:51:00.639760 11835 sgd_solver.cpp:112] Iteration 9970, lr = 0.1
I0523 23:51:06.551605 11835 solver.cpp:239] Iteration 9980 (1.69106 iter/s, 5.91344s/10 iters), loss = 8.39799
I0523 23:51:06.551672 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.39799 (* 1 = 8.39799 loss)
I0523 23:51:06.551889 11835 sgd_solver.cpp:112] Iteration 9980, lr = 0.1
I0523 23:51:14.480870 11835 solver.cpp:239] Iteration 9990 (1.26121 iter/s, 7.92891s/10 iters), loss = 8.8691
I0523 23:51:14.480914 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.8691 (* 1 = 8.8691 loss)
I0523 23:51:14.481148 11835 sgd_solver.cpp:112] Iteration 9990, lr = 0.1
I0523 23:51:20.951571 11835 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_10000.caffemodel
I0523 23:51:22.669412 11835 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_10000.solverstate
I0523 23:51:23.233527 11835 solver.cpp:239] Iteration 10000 (1.14256 iter/s, 8.75229s/10 iters), loss = 8.58601
I0523 23:51:23.233574 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.58601 (* 1 = 8.58601 loss)
I0523 23:51:23.233666 11835 sgd_solver.cpp:112] Iteration 10000, lr = 0.1
I0523 23:51:30.602375 11835 solver.cpp:239] Iteration 10010 (1.35713 iter/s, 7.36851s/10 iters), loss = 8.32645
I0523 23:51:30.602434 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.32645 (* 1 = 8.32645 loss)
I0523 23:51:30.602656 11835 sgd_solver.cpp:112] Iteration 10010, lr = 0.1
I0523 23:51:37.503830 11835 solver.cpp:239] Iteration 10020 (1.44904 iter/s, 6.90114s/10 iters), loss = 9.38628
I0523 23:51:37.504109 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.38628 (* 1 = 9.38628 loss)
I0523 23:51:37.504155 11835 sgd_solver.cpp:112] Iteration 10020, lr = 0.1
I0523 23:51:43.904716 11835 solver.cpp:239] Iteration 10030 (1.56248 iter/s, 6.40007s/10 iters), loss = 9.85247
I0523 23:51:43.904767 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.85247 (* 1 = 9.85247 loss)
I0523 23:51:43.904947 11835 sgd_solver.cpp:112] Iteration 10030, lr = 0.1
I0523 23:51:49.814682 11835 solver.cpp:239] Iteration 10040 (1.69214 iter/s, 5.90969s/10 iters), loss = 8.25885
I0523 23:51:49.814756 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.25885 (* 1 = 8.25885 loss)
I0523 23:51:49.814864 11835 sgd_solver.cpp:112] Iteration 10040, lr = 0.1
I0523 23:51:56.866469 11835 solver.cpp:239] Iteration 10050 (1.41815 iter/s, 7.05144s/10 iters), loss = 9.38519
I0523 23:51:56.866525 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.38519 (* 1 = 9.38519 loss)
I0523 23:51:56.866829 11835 sgd_solver.cpp:112] Iteration 10050, lr = 0.1
I0523 23:52:03.975747 11835 solver.cpp:239] Iteration 10060 (1.40668 iter/s, 7.10896s/10 iters), loss = 8.93336
I0523 23:52:03.975800 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.93336 (* 1 = 8.93336 loss)
I0523 23:52:03.976073 11835 sgd_solver.cpp:112] Iteration 10060, lr = 0.1
I0523 23:52:11.017447 11835 solver.cpp:239] Iteration 10070 (1.42018 iter/s, 7.04138s/10 iters), loss = 8.37077
I0523 23:52:11.017740 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.37077 (* 1 = 8.37077 loss)
I0523 23:52:11.017810 11835 sgd_solver.cpp:112] Iteration 10070, lr = 0.1
I0523 23:52:17.584064 11835 solver.cpp:239] Iteration 10080 (1.523 iter/s, 6.56601s/10 iters), loss = 8.56628
I0523 23:52:17.584102 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.56628 (* 1 = 8.56628 loss)
I0523 23:52:17.584247 11835 sgd_solver.cpp:112] Iteration 10080, lr = 0.1
I0523 23:52:24.100632 11835 solver.cpp:239] Iteration 10090 (1.53462 iter/s, 6.51628s/10 iters), loss = 8.92872
I0523 23:52:24.100685 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.92872 (* 1 = 8.92872 loss)
I0523 23:52:24.100702 11835 sgd_solver.cpp:112] Iteration 10090, lr = 0.1
I0523 23:52:30.450793 11835 solver.cpp:239] Iteration 10100 (1.57511 iter/s, 6.34874s/10 iters), loss = 8.76215
I0523 23:52:30.450839 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.76215 (* 1 = 8.76215 loss)
I0523 23:52:30.915602 11835 sgd_solver.cpp:112] Iteration 10100, lr = 0.1
I0523 23:52:38.391824 11835 solver.cpp:239] Iteration 10110 (1.25934 iter/s, 7.94069s/10 iters), loss = 9.20902
I0523 23:52:38.391870 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.20902 (* 1 = 9.20902 loss)
I0523 23:52:38.392091 11835 sgd_solver.cpp:112] Iteration 10110, lr = 0.1
I0523 23:52:45.305421 11835 solver.cpp:239] Iteration 10120 (1.44649 iter/s, 6.91328s/10 iters), loss = 8.6357
I0523 23:52:45.305716 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.6357 (* 1 = 8.6357 loss)
I0523 23:52:45.522179 11835 sgd_solver.cpp:112] Iteration 10120, lr = 0.1
I0523 23:52:52.028338 11835 solver.cpp:239] Iteration 10130 (1.48756 iter/s, 6.72241s/10 iters), loss = 8.82995
I0523 23:52:52.028388 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.82995 (* 1 = 8.82995 loss)
I0523 23:52:52.028401 11835 sgd_solver.cpp:112] Iteration 10130, lr = 0.1
I0523 23:52:58.964504 11835 solver.cpp:239] Iteration 10140 (1.44178 iter/s, 6.93586s/10 iters), loss = 9.64137
I0523 23:52:58.964540 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.64137 (* 1 = 9.64137 loss)
I0523 23:52:59.709890 11835 sgd_solver.cpp:112] Iteration 10140, lr = 0.1
I0523 23:53:07.425956 11835 solver.cpp:239] Iteration 10150 (1.18188 iter/s, 8.46108s/10 iters), loss = 8.80184
I0523 23:53:07.426010 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.80184 (* 1 = 8.80184 loss)
I0523 23:53:07.426265 11835 sgd_solver.cpp:112] Iteration 10150, lr = 0.1
I0523 23:53:13.139474 11835 solver.cpp:239] Iteration 10160 (1.75032 iter/s, 5.71325s/10 iters), loss = 9.14634
I0523 23:53:13.139521 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.14634 (* 1 = 9.14634 loss)
I0523 23:53:13.139534 11835 sgd_solver.cpp:112] Iteration 10160, lr = 0.1
I0523 23:53:18.728971 11835 solver.cpp:239] Iteration 10170 (1.78919 iter/s, 5.58912s/10 iters), loss = 8.77159
I0523 23:53:18.729198 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.77159 (* 1 = 8.77159 loss)
I0523 23:53:18.729233 11835 sgd_solver.cpp:112] Iteration 10170, lr = 0.1
I0523 23:53:25.779706 11835 solver.cpp:239] Iteration 10180 (1.41842 iter/s, 7.05012s/10 iters), loss = 8.73248
I0523 23:53:25.779783 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.73248 (* 1 = 8.73248 loss)
I0523 23:53:25.779899 11835 sgd_solver.cpp:112] Iteration 10180, lr = 0.1
I0523 23:53:34.346467 11835 solver.cpp:239] Iteration 10190 (1.16736 iter/s, 8.56637s/10 iters), loss = 8.46239
I0523 23:53:34.346508 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.46239 (* 1 = 8.46239 loss)
I0523 23:53:35.426584 11835 sgd_solver.cpp:112] Iteration 10190, lr = 0.1
I0523 23:53:42.241670 11835 solver.cpp:239] Iteration 10200 (1.26665 iter/s, 7.89485s/10 iters), loss = 8.55373
I0523 23:53:42.241729 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.55373 (* 1 = 8.55373 loss)
I0523 23:53:42.241915 11835 sgd_solver.cpp:112] Iteration 10200, lr = 0.1
I0523 23:53:49.490660 11835 solver.cpp:239] Iteration 10210 (1.37957 iter/s, 7.24865s/10 iters), loss = 8.98163
I0523 23:53:49.490906 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.98163 (* 1 = 8.98163 loss)
I0523 23:53:49.490958 11835 sgd_solver.cpp:112] Iteration 10210, lr = 0.1
I0523 23:53:59.622078 11835 solver.cpp:239] Iteration 10220 (0.987086 iter/s, 10.1308s/10 iters), loss = 8.25863
I0523 23:53:59.622133 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.25863 (* 1 = 8.25863 loss)
I0523 23:53:59.622349 11835 sgd_solver.cpp:112] Iteration 10220, lr = 0.1
I0523 23:54:06.616839 11835 solver.cpp:239] Iteration 10230 (1.42971 iter/s, 6.99445s/10 iters), loss = 9.56097
I0523 23:54:06.616878 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.56097 (* 1 = 9.56097 loss)
I0523 23:54:06.616888 11835 sgd_solver.cpp:112] Iteration 10230, lr = 0.1
I0523 23:54:12.736399 11835 solver.cpp:239] Iteration 10240 (1.63422 iter/s, 6.11913s/10 iters), loss = 9.13117
I0523 23:54:12.736449 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.13117 (* 1 = 9.13117 loss)
I0523 23:54:13.036798 11835 sgd_solver.cpp:112] Iteration 10240, lr = 0.1
I0523 23:54:20.482187 11835 solver.cpp:239] Iteration 10250 (1.29108 iter/s, 7.74544s/10 iters), loss = 9.65635
I0523 23:54:20.482278 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.65635 (* 1 = 9.65635 loss)
I0523 23:54:20.492280 11835 sgd_solver.cpp:112] Iteration 10250, lr = 0.1
I0523 23:54:27.598878 11835 solver.cpp:239] Iteration 10260 (1.40522 iter/s, 7.11634s/10 iters), loss = 8.32908
I0523 23:54:27.598922 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.32908 (* 1 = 8.32908 loss)
I0523 23:54:27.598944 11835 sgd_solver.cpp:112] Iteration 10260, lr = 0.1
I0523 23:54:34.393259 11835 solver.cpp:239] Iteration 10270 (1.47187 iter/s, 6.79407s/10 iters), loss = 8.32516
I0523 23:54:34.393314 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.32516 (* 1 = 8.32516 loss)
I0523 23:54:34.393330 11835 sgd_solver.cpp:112] Iteration 10270, lr = 0.1
I0523 23:54:41.032708 11835 solver.cpp:239] Iteration 10280 (1.50625 iter/s, 6.63899s/10 iters), loss = 8.25495
I0523 23:54:41.032763 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.25495 (* 1 = 8.25495 loss)
I0523 23:54:41.032872 11835 sgd_solver.cpp:112] Iteration 10280, lr = 0.1
I0523 23:54:48.378958 11835 solver.cpp:239] Iteration 10290 (1.3613 iter/s, 7.34593s/10 iters), loss = 9.13514
I0523 23:54:48.379000 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.13514 (* 1 = 9.13514 loss)
I0523 23:54:48.379123 11835 sgd_solver.cpp:112] Iteration 10290, lr = 0.1
I0523 23:54:55.769049 11835 solver.cpp:239] Iteration 10300 (1.35322 iter/s, 7.38977s/10 iters), loss = 9.76833
I0523 23:54:55.769202 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.76833 (* 1 = 9.76833 loss)
I0523 23:54:56.868827 11835 sgd_solver.cpp:112] Iteration 10300, lr = 0.1
I0523 23:55:04.730610 11835 solver.cpp:239] Iteration 10310 (1.11594 iter/s, 8.96109s/10 iters), loss = 8.9429
I0523 23:55:04.730661 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.9429 (* 1 = 8.9429 loss)
I0523 23:55:04.730885 11835 sgd_solver.cpp:112] Iteration 10310, lr = 0.1
I0523 23:55:10.918278 11835 solver.cpp:239] Iteration 10320 (1.61619 iter/s, 6.18739s/10 iters), loss = 9.37699
I0523 23:55:10.918325 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.37699 (* 1 = 9.37699 loss)
I0523 23:55:11.247180 11835 sgd_solver.cpp:112] Iteration 10320, lr = 0.1
I0523 23:55:18.593194 11835 solver.cpp:239] Iteration 10330 (1.303 iter/s, 7.67457s/10 iters), loss = 9.26119
I0523 23:55:18.593250 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.26119 (* 1 = 9.26119 loss)
I0523 23:55:18.593675 11835 sgd_solver.cpp:112] Iteration 10330, lr = 0.1
I0523 23:55:24.379220 11835 solver.cpp:239] Iteration 10340 (1.72838 iter/s, 5.78575s/10 iters), loss = 8.51108
I0523 23:55:24.379267 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.51108 (* 1 = 8.51108 loss)
I0523 23:55:24.379281 11835 sgd_solver.cpp:112] Iteration 10340, lr = 0.1
I0523 23:55:33.274394 11835 solver.cpp:239] Iteration 10350 (1.12425 iter/s, 8.8948s/10 iters), loss = 8.94762
I0523 23:55:33.274528 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.94762 (* 1 = 8.94762 loss)
I0523 23:55:33.348336 11835 sgd_solver.cpp:112] Iteration 10350, lr = 0.1
I0523 23:55:40.056867 11835 solver.cpp:239] Iteration 10360 (1.47448 iter/s, 6.78207s/10 iters), loss = 8.10679
I0523 23:55:40.056923 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.10679 (* 1 = 8.10679 loss)
I0523 23:55:40.057143 11835 sgd_solver.cpp:112] Iteration 10360, lr = 0.1
I0523 23:55:48.705929 11835 solver.cpp:239] Iteration 10370 (1.15625 iter/s, 8.64868s/10 iters), loss = 8.99019
I0523 23:55:48.705976 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.99019 (* 1 = 8.99019 loss)
I0523 23:55:48.783094 11835 sgd_solver.cpp:112] Iteration 10370, lr = 0.1
I0523 23:55:58.521713 11835 solver.cpp:239] Iteration 10380 (1.01881 iter/s, 9.81536s/10 iters), loss = 9.08819
I0523 23:55:58.521764 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.08819 (* 1 = 9.08819 loss)
I0523 23:55:58.599197 11835 sgd_solver.cpp:112] Iteration 10380, lr = 0.1
I0523 23:56:04.991178 11835 solver.cpp:239] Iteration 10390 (1.5458 iter/s, 6.46916s/10 iters), loss = 9.24416
I0523 23:56:04.991467 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.24416 (* 1 = 9.24416 loss)
I0523 23:56:04.991528 11835 sgd_solver.cpp:112] Iteration 10390, lr = 0.1
I0523 23:56:10.856060 11835 solver.cpp:239] Iteration 10400 (1.70535 iter/s, 5.86391s/10 iters), loss = 8.63831
I0523 23:56:10.856111 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.63831 (* 1 = 8.63831 loss)
I0523 23:56:10.861306 11835 sgd_solver.cpp:112] Iteration 10400, lr = 0.1
I0523 23:56:19.014348 11835 solver.cpp:239] Iteration 10410 (1.2258 iter/s, 8.15793s/10 iters), loss = 8.73892
I0523 23:56:19.014398 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.73892 (* 1 = 8.73892 loss)
I0523 23:56:19.519194 11835 sgd_solver.cpp:112] Iteration 10410, lr = 0.1
I0523 23:56:25.551744 11835 solver.cpp:239] Iteration 10420 (1.52973 iter/s, 6.5371s/10 iters), loss = 7.67324
I0523 23:56:25.551782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.67324 (* 1 = 7.67324 loss)
I0523 23:56:25.552044 11835 sgd_solver.cpp:112] Iteration 10420, lr = 0.1
I0523 23:56:31.558274 11835 solver.cpp:239] Iteration 10430 (1.66493 iter/s, 6.00626s/10 iters), loss = 8.46411
I0523 23:56:31.558320 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.46411 (* 1 = 8.46411 loss)
I0523 23:56:31.842043 11835 sgd_solver.cpp:112] Iteration 10430, lr = 0.1
I0523 23:56:37.441542 11835 solver.cpp:239] Iteration 10440 (1.69981 iter/s, 5.883s/10 iters), loss = 8.60348
I0523 23:56:37.441612 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.60348 (* 1 = 8.60348 loss)
I0523 23:56:37.441628 11835 sgd_solver.cpp:112] Iteration 10440, lr = 0.1
I0523 23:56:43.462536 11835 solver.cpp:239] Iteration 10450 (1.66094 iter/s, 6.02068s/10 iters), loss = 8.91087
I0523 23:56:43.462590 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.91087 (* 1 = 8.91087 loss)
I0523 23:56:43.462692 11835 sgd_solver.cpp:112] Iteration 10450, lr = 0.1
I0523 23:56:50.337520 11835 solver.cpp:239] Iteration 10460 (1.45462 iter/s, 6.87467s/10 iters), loss = 9.08448
I0523 23:56:50.337572 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.08448 (* 1 = 9.08448 loss)
I0523 23:56:50.337707 11835 sgd_solver.cpp:112] Iteration 10460, lr = 0.1
I0523 23:56:58.159958 11835 solver.cpp:239] Iteration 10470 (1.27843 iter/s, 7.8221s/10 iters), loss = 8.83087
I0523 23:56:58.160010 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.83087 (* 1 = 8.83087 loss)
I0523 23:56:58.160184 11835 sgd_solver.cpp:112] Iteration 10470, lr = 0.1
I0523 23:57:04.645372 11835 solver.cpp:239] Iteration 10480 (1.54199 iter/s, 6.48511s/10 iters), loss = 8.66138
I0523 23:57:04.645429 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.66138 (* 1 = 8.66138 loss)
I0523 23:57:04.645687 11835 sgd_solver.cpp:112] Iteration 10480, lr = 0.1
I0523 23:57:12.742424 11835 solver.cpp:239] Iteration 10490 (1.23507 iter/s, 8.09669s/10 iters), loss = 9.29599
I0523 23:57:12.742624 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.29599 (* 1 = 9.29599 loss)
I0523 23:57:13.166649 11835 sgd_solver.cpp:112] Iteration 10490, lr = 0.1
I0523 23:57:22.219724 11835 solver.cpp:239] Iteration 10500 (1.05521 iter/s, 9.47678s/10 iters), loss = 9.03026
I0523 23:57:22.219780 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.03026 (* 1 = 9.03026 loss)
I0523 23:57:22.220018 11835 sgd_solver.cpp:112] Iteration 10500, lr = 0.1
I0523 23:57:28.173282 11835 solver.cpp:239] Iteration 10510 (1.67975 iter/s, 5.95328s/10 iters), loss = 8.95646
I0523 23:57:28.173331 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.95646 (* 1 = 8.95646 loss)
I0523 23:57:28.173765 11835 sgd_solver.cpp:112] Iteration 10510, lr = 0.1
I0523 23:57:34.331498 11835 solver.cpp:239] Iteration 10520 (1.62392 iter/s, 6.15792s/10 iters), loss = 8.18988
I0523 23:57:34.331562 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.18988 (* 1 = 8.18988 loss)
I0523 23:57:34.331609 11835 sgd_solver.cpp:112] Iteration 10520, lr = 0.1
I0523 23:57:41.006053 11835 solver.cpp:239] Iteration 10530 (1.4983 iter/s, 6.67425s/10 iters), loss = 9.95163
I0523 23:57:41.006091 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.95163 (* 1 = 9.95163 loss)
I0523 23:57:41.363673 11835 sgd_solver.cpp:112] Iteration 10530, lr = 0.1
I0523 23:57:49.980748 11835 solver.cpp:239] Iteration 10540 (1.11429 iter/s, 8.97431s/10 iters), loss = 8.25823
I0523 23:57:49.980948 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.25823 (* 1 = 8.25823 loss)
I0523 23:57:49.993069 11835 sgd_solver.cpp:112] Iteration 10540, lr = 0.1
I0523 23:57:56.552321 11835 solver.cpp:239] Iteration 10550 (1.52181 iter/s, 6.57113s/10 iters), loss = 7.73956
I0523 23:57:56.552371 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.73956 (* 1 = 7.73956 loss)
I0523 23:57:56.552386 11835 sgd_solver.cpp:112] Iteration 10550, lr = 0.1
I0523 23:58:04.160984 11835 solver.cpp:239] Iteration 10560 (1.31435 iter/s, 7.60833s/10 iters), loss = 8.46998
I0523 23:58:04.161031 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.46998 (* 1 = 8.46998 loss)
I0523 23:58:04.161352 11835 sgd_solver.cpp:112] Iteration 10560, lr = 0.1
I0523 23:58:10.756191 11835 solver.cpp:239] Iteration 10570 (1.51632 iter/s, 6.5949s/10 iters), loss = 8.02527
I0523 23:58:10.756249 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.02527 (* 1 = 8.02527 loss)
I0523 23:58:12.083395 11835 sgd_solver.cpp:112] Iteration 10570, lr = 0.1
I0523 23:58:18.920086 11835 solver.cpp:239] Iteration 10580 (1.22496 iter/s, 8.16352s/10 iters), loss = 9.56796
I0523 23:58:18.920142 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.56796 (* 1 = 9.56796 loss)
I0523 23:58:18.920769 11835 sgd_solver.cpp:112] Iteration 10580, lr = 0.1
I0523 23:58:25.971125 11835 solver.cpp:239] Iteration 10590 (1.4183 iter/s, 7.05071s/10 iters), loss = 8.87216
I0523 23:58:25.971261 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.87216 (* 1 = 8.87216 loss)
I0523 23:58:26.236960 11835 sgd_solver.cpp:112] Iteration 10590, lr = 0.1
I0523 23:58:32.577711 11835 solver.cpp:239] Iteration 10600 (1.51373 iter/s, 6.6062s/10 iters), loss = 9.58903
I0523 23:58:32.577760 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.58903 (* 1 = 9.58903 loss)
I0523 23:58:32.577879 11835 sgd_solver.cpp:112] Iteration 10600, lr = 0.1
I0523 23:58:39.501123 11835 solver.cpp:239] Iteration 10610 (1.44444 iter/s, 6.92311s/10 iters), loss = 7.89618
I0523 23:58:39.501163 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.89618 (* 1 = 7.89618 loss)
I0523 23:58:39.501175 11835 sgd_solver.cpp:112] Iteration 10610, lr = 0.1
I0523 23:58:46.478503 11835 solver.cpp:239] Iteration 10620 (1.43338 iter/s, 6.97651s/10 iters), loss = 8.53452
I0523 23:58:46.478555 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.53452 (* 1 = 8.53452 loss)
I0523 23:58:46.479081 11835 sgd_solver.cpp:112] Iteration 10620, lr = 0.1
I0523 23:58:52.662865 11835 solver.cpp:239] Iteration 10630 (1.61706 iter/s, 6.18408s/10 iters), loss = 9.79762
I0523 23:58:52.662914 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.79762 (* 1 = 9.79762 loss)
I0523 23:58:52.662930 11835 sgd_solver.cpp:112] Iteration 10630, lr = 0.1
I0523 23:58:59.898020 11835 solver.cpp:239] Iteration 10640 (1.38222 iter/s, 7.23475s/10 iters), loss = 7.69249
I0523 23:58:59.898293 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.69249 (* 1 = 7.69249 loss)
I0523 23:58:59.900035 11835 sgd_solver.cpp:112] Iteration 10640, lr = 0.1
I0523 23:59:06.669823 11835 solver.cpp:239] Iteration 10650 (1.47682 iter/s, 6.77132s/10 iters), loss = 8.21983
I0523 23:59:06.669874 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.21983 (* 1 = 8.21983 loss)
I0523 23:59:06.684774 11835 sgd_solver.cpp:112] Iteration 10650, lr = 0.1
I0523 23:59:13.213322 11835 solver.cpp:239] Iteration 10660 (1.5283 iter/s, 6.54321s/10 iters), loss = 9.479
I0523 23:59:13.213362 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.479 (* 1 = 9.479 loss)
I0523 23:59:13.722842 11835 sgd_solver.cpp:112] Iteration 10660, lr = 0.1
I0523 23:59:20.493883 11835 solver.cpp:239] Iteration 10670 (1.37358 iter/s, 7.28023s/10 iters), loss = 8.41379
I0523 23:59:20.493939 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.41379 (* 1 = 8.41379 loss)
I0523 23:59:20.494145 11835 sgd_solver.cpp:112] Iteration 10670, lr = 0.1
I0523 23:59:27.002782 11835 solver.cpp:239] Iteration 10680 (1.53643 iter/s, 6.50861s/10 iters), loss = 8.66723
I0523 23:59:27.002816 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.66723 (* 1 = 8.66723 loss)
I0523 23:59:27.002830 11835 sgd_solver.cpp:112] Iteration 10680, lr = 0.1
I0523 23:59:34.840517 11835 solver.cpp:239] Iteration 10690 (1.27594 iter/s, 7.83733s/10 iters), loss = 8.77121
I0523 23:59:34.840822 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.77121 (* 1 = 8.77121 loss)
I0523 23:59:34.840883 11835 sgd_solver.cpp:112] Iteration 10690, lr = 0.1
I0523 23:59:41.119841 11835 solver.cpp:239] Iteration 10700 (1.59292 iter/s, 6.2778s/10 iters), loss = 8.67378
I0523 23:59:41.119891 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.67378 (* 1 = 8.67378 loss)
I0523 23:59:41.739871 11835 sgd_solver.cpp:112] Iteration 10700, lr = 0.1
I0523 23:59:49.193401 11835 solver.cpp:239] Iteration 10710 (1.23866 iter/s, 8.07321s/10 iters), loss = 8.85128
I0523 23:59:49.193444 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.85128 (* 1 = 8.85128 loss)
I0523 23:59:49.193711 11835 sgd_solver.cpp:112] Iteration 10710, lr = 0.1
I0523 23:59:56.004097 11835 solver.cpp:239] Iteration 10720 (1.46835 iter/s, 6.81038s/10 iters), loss = 7.93217
I0523 23:59:56.004150 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.93217 (* 1 = 7.93217 loss)
I0523 23:59:56.004214 11835 sgd_solver.cpp:112] Iteration 10720, lr = 0.1
I0524 00:00:03.526302 11835 solver.cpp:239] Iteration 10730 (1.32946 iter/s, 7.52186s/10 iters), loss = 8.14133
I0524 00:00:03.526365 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.14133 (* 1 = 8.14133 loss)
I0524 00:00:03.526557 11835 sgd_solver.cpp:112] Iteration 10730, lr = 0.1
I0524 00:00:09.872707 11835 solver.cpp:239] Iteration 10740 (1.57577 iter/s, 6.3461s/10 iters), loss = 8.55301
I0524 00:00:09.872982 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.55301 (* 1 = 8.55301 loss)
I0524 00:00:10.370699 11835 sgd_solver.cpp:112] Iteration 10740, lr = 0.1
I0524 00:00:18.676376 11835 solver.cpp:239] Iteration 10750 (1.13596 iter/s, 8.80311s/10 iters), loss = 8.04301
I0524 00:00:18.676435 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.04301 (* 1 = 8.04301 loss)
I0524 00:00:18.676653 11835 sgd_solver.cpp:112] Iteration 10750, lr = 0.1
I0524 00:00:24.702879 11835 solver.cpp:239] Iteration 10760 (1.65941 iter/s, 6.02622s/10 iters), loss = 10.0471
I0524 00:00:24.702919 11835 solver.cpp:258]     Train net output #0: softmax_loss = 10.0471 (* 1 = 10.0471 loss)
I0524 00:00:24.702934 11835 sgd_solver.cpp:112] Iteration 10760, lr = 0.1
I0524 00:00:31.766494 11835 solver.cpp:239] Iteration 10770 (1.41577 iter/s, 7.0633s/10 iters), loss = 8.83448
I0524 00:00:31.766548 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.83448 (* 1 = 8.83448 loss)
I0524 00:00:32.728669 11835 sgd_solver.cpp:112] Iteration 10770, lr = 0.1
I0524 00:00:39.059283 11835 solver.cpp:239] Iteration 10780 (1.37128 iter/s, 7.29246s/10 iters), loss = 9.33473
I0524 00:00:39.059329 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.33473 (* 1 = 9.33473 loss)
I0524 00:00:39.059468 11835 sgd_solver.cpp:112] Iteration 10780, lr = 0.1
I0524 00:00:45.689831 11835 solver.cpp:239] Iteration 10790 (1.50824 iter/s, 6.63024s/10 iters), loss = 9.29035
I0524 00:00:45.690066 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.29035 (* 1 = 9.29035 loss)
I0524 00:00:46.397097 11835 sgd_solver.cpp:112] Iteration 10790, lr = 0.1
I0524 00:00:53.487334 11835 solver.cpp:239] Iteration 10800 (1.28255 iter/s, 7.79699s/10 iters), loss = 8.4518
I0524 00:00:53.487392 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.4518 (* 1 = 8.4518 loss)
I0524 00:00:54.200029 11835 sgd_solver.cpp:112] Iteration 10800, lr = 0.1
I0524 00:00:59.809321 11835 solver.cpp:239] Iteration 10810 (1.58185 iter/s, 6.3217s/10 iters), loss = 8.82435
I0524 00:00:59.809360 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.82435 (* 1 = 8.82435 loss)
I0524 00:00:59.809370 11835 sgd_solver.cpp:112] Iteration 10810, lr = 0.1
I0524 00:01:06.879503 11835 solver.cpp:239] Iteration 10820 (1.41447 iter/s, 7.06979s/10 iters), loss = 7.68987
I0524 00:01:06.879549 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.68987 (* 1 = 7.68987 loss)
I0524 00:01:06.879766 11835 sgd_solver.cpp:112] Iteration 10820, lr = 0.1
I0524 00:01:12.422294 11835 solver.cpp:239] Iteration 10830 (1.80423 iter/s, 5.54254s/10 iters), loss = 8.41861
I0524 00:01:12.422333 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.41861 (* 1 = 8.41861 loss)
I0524 00:01:12.422344 11835 sgd_solver.cpp:112] Iteration 10830, lr = 0.1
I0524 00:01:19.765570 11835 solver.cpp:239] Iteration 10840 (1.36185 iter/s, 7.34295s/10 iters), loss = 9.45769
I0524 00:01:19.765856 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.45769 (* 1 = 9.45769 loss)
I0524 00:01:19.765914 11835 sgd_solver.cpp:112] Iteration 10840, lr = 0.1
I0524 00:01:25.586803 11835 solver.cpp:239] Iteration 10850 (1.71807 iter/s, 5.82049s/10 iters), loss = 8.33902
I0524 00:01:25.586858 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.33902 (* 1 = 8.33902 loss)
I0524 00:01:25.586872 11835 sgd_solver.cpp:112] Iteration 10850, lr = 0.1
I0524 00:01:31.914994 11835 solver.cpp:239] Iteration 10860 (1.58032 iter/s, 6.32783s/10 iters), loss = 8.41314
I0524 00:01:31.915040 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.41314 (* 1 = 8.41314 loss)
I0524 00:01:31.915113 11835 sgd_solver.cpp:112] Iteration 10860, lr = 0.1
I0524 00:01:37.979251 11835 solver.cpp:239] Iteration 10870 (1.64908 iter/s, 6.06397s/10 iters), loss = 9.24173
I0524 00:01:37.979318 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.24173 (* 1 = 9.24173 loss)
I0524 00:01:37.979872 11835 sgd_solver.cpp:112] Iteration 10870, lr = 0.1
I0524 00:01:43.849054 11835 solver.cpp:239] Iteration 10880 (1.70371 iter/s, 5.86954s/10 iters), loss = 8.05779
I0524 00:01:43.849097 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.05779 (* 1 = 8.05779 loss)
I0524 00:01:43.849192 11835 sgd_solver.cpp:112] Iteration 10880, lr = 0.1
I0524 00:01:49.889932 11835 solver.cpp:239] Iteration 10890 (1.65547 iter/s, 6.04059s/10 iters), loss = 9.27646
I0524 00:01:49.890224 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.27646 (* 1 = 9.27646 loss)
I0524 00:01:49.890285 11835 sgd_solver.cpp:112] Iteration 10890, lr = 0.1
I0524 00:01:55.522260 11835 solver.cpp:239] Iteration 10900 (1.77563 iter/s, 5.6318s/10 iters), loss = 8.7813
I0524 00:01:55.522301 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.7813 (* 1 = 8.7813 loss)
I0524 00:01:55.522333 11835 sgd_solver.cpp:112] Iteration 10900, lr = 0.1
I0524 00:02:02.347378 11835 solver.cpp:239] Iteration 10910 (1.46524 iter/s, 6.82482s/10 iters), loss = 8.50749
I0524 00:02:02.347426 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.50749 (* 1 = 8.50749 loss)
I0524 00:02:02.347667 11835 sgd_solver.cpp:112] Iteration 10910, lr = 0.1
I0524 00:02:08.915614 11835 solver.cpp:239] Iteration 10920 (1.52255 iter/s, 6.56794s/10 iters), loss = 8.08626
I0524 00:02:08.915668 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.08626 (* 1 = 8.08626 loss)
I0524 00:02:08.915685 11835 sgd_solver.cpp:112] Iteration 10920, lr = 0.1
I0524 00:02:14.832382 11835 solver.cpp:239] Iteration 10930 (1.69037 iter/s, 5.91586s/10 iters), loss = 9.67109
I0524 00:02:14.832434 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.67109 (* 1 = 9.67109 loss)
I0524 00:02:14.832453 11835 sgd_solver.cpp:112] Iteration 10930, lr = 0.1
I0524 00:02:21.357857 11835 solver.cpp:239] Iteration 10940 (1.53254 iter/s, 6.52512s/10 iters), loss = 8.19023
I0524 00:02:21.358042 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.19023 (* 1 = 8.19023 loss)
I0524 00:02:21.358059 11835 sgd_solver.cpp:112] Iteration 10940, lr = 0.1
I0524 00:02:27.901412 11835 solver.cpp:239] Iteration 10950 (1.52833 iter/s, 6.54309s/10 iters), loss = 9.02665
I0524 00:02:27.901466 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.02665 (* 1 = 9.02665 loss)
I0524 00:02:27.901484 11835 sgd_solver.cpp:112] Iteration 10950, lr = 0.1
I0524 00:02:34.676920 11835 solver.cpp:239] Iteration 10960 (1.47597 iter/s, 6.77521s/10 iters), loss = 9.0517
I0524 00:02:34.676965 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.0517 (* 1 = 9.0517 loss)
I0524 00:02:35.324973 11835 sgd_solver.cpp:112] Iteration 10960, lr = 0.1
I0524 00:02:41.123986 11835 solver.cpp:239] Iteration 10970 (1.55116 iter/s, 6.44677s/10 iters), loss = 8.89103
I0524 00:02:41.124030 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.89103 (* 1 = 8.89103 loss)
I0524 00:02:41.124042 11835 sgd_solver.cpp:112] Iteration 10970, lr = 0.1
I0524 00:02:49.630116 11835 solver.cpp:239] Iteration 10980 (1.17567 iter/s, 8.50576s/10 iters), loss = 9.04141
I0524 00:02:49.630172 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.04141 (* 1 = 9.04141 loss)
I0524 00:02:50.171682 11835 sgd_solver.cpp:112] Iteration 10980, lr = 0.1
I0524 00:02:58.855891 11835 solver.cpp:239] Iteration 10990 (1.08397 iter/s, 9.22537s/10 iters), loss = 9.06578
I0524 00:02:58.856168 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.06578 (* 1 = 9.06578 loss)
I0524 00:02:58.856230 11835 sgd_solver.cpp:112] Iteration 10990, lr = 0.1
I0524 00:03:04.815752 11835 solver.cpp:239] Iteration 11000 (1.67804 iter/s, 5.95935s/10 iters), loss = 8.64555
I0524 00:03:04.815824 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.64555 (* 1 = 8.64555 loss)
I0524 00:03:04.816280 11835 sgd_solver.cpp:112] Iteration 11000, lr = 0.1
I0524 00:03:10.888741 11835 solver.cpp:239] Iteration 11010 (1.64672 iter/s, 6.07269s/10 iters), loss = 8.82726
I0524 00:03:10.888787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.82726 (* 1 = 8.82726 loss)
I0524 00:03:10.888801 11835 sgd_solver.cpp:112] Iteration 11010, lr = 0.1
I0524 00:03:16.875466 11835 solver.cpp:239] Iteration 11020 (1.67044 iter/s, 5.98646s/10 iters), loss = 8.78769
I0524 00:03:16.875512 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.78769 (* 1 = 8.78769 loss)
I0524 00:03:16.875658 11835 sgd_solver.cpp:112] Iteration 11020, lr = 0.1
I0524 00:03:22.709707 11835 solver.cpp:239] Iteration 11030 (1.7141 iter/s, 5.83397s/10 iters), loss = 8.33775
I0524 00:03:22.709753 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.33775 (* 1 = 8.33775 loss)
I0524 00:03:22.709998 11835 sgd_solver.cpp:112] Iteration 11030, lr = 0.1
I0524 00:03:30.003171 11835 solver.cpp:239] Iteration 11040 (1.37115 iter/s, 7.29315s/10 iters), loss = 9.23888
I0524 00:03:30.003373 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.23888 (* 1 = 9.23888 loss)
I0524 00:03:30.651360 11835 sgd_solver.cpp:112] Iteration 11040, lr = 0.1
I0524 00:03:39.909787 11835 solver.cpp:239] Iteration 11050 (1.00948 iter/s, 9.90608s/10 iters), loss = 8.62323
I0524 00:03:39.909844 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.62323 (* 1 = 8.62323 loss)
I0524 00:03:39.910104 11835 sgd_solver.cpp:112] Iteration 11050, lr = 0.1
I0524 00:03:46.800314 11835 solver.cpp:239] Iteration 11060 (1.45134 iter/s, 6.8902s/10 iters), loss = 7.71304
I0524 00:03:46.800371 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.71304 (* 1 = 7.71304 loss)
I0524 00:03:46.800472 11835 sgd_solver.cpp:112] Iteration 11060, lr = 0.1
I0524 00:03:52.854462 11835 solver.cpp:239] Iteration 11070 (1.65184 iter/s, 6.05387s/10 iters), loss = 8.4262
I0524 00:03:52.854507 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.4262 (* 1 = 8.4262 loss)
I0524 00:03:52.854580 11835 sgd_solver.cpp:112] Iteration 11070, lr = 0.1
I0524 00:03:58.909605 11835 solver.cpp:239] Iteration 11080 (1.65157 iter/s, 6.05486s/10 iters), loss = 9.31153
I0524 00:03:58.909654 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.31153 (* 1 = 9.31153 loss)
I0524 00:03:58.954627 11835 sgd_solver.cpp:112] Iteration 11080, lr = 0.1
I0524 00:04:06.028790 11835 solver.cpp:239] Iteration 11090 (1.40472 iter/s, 7.11887s/10 iters), loss = 8.93806
I0524 00:04:06.028926 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.93806 (* 1 = 8.93806 loss)
I0524 00:04:06.028966 11835 sgd_solver.cpp:112] Iteration 11090, lr = 0.1
I0524 00:04:12.647905 11835 solver.cpp:239] Iteration 11100 (1.51086 iter/s, 6.61873s/10 iters), loss = 8.13809
I0524 00:04:12.647955 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.13809 (* 1 = 8.13809 loss)
I0524 00:04:12.759479 11835 sgd_solver.cpp:112] Iteration 11100, lr = 0.1
I0524 00:04:18.402560 11835 solver.cpp:239] Iteration 11110 (1.73781 iter/s, 5.75439s/10 iters), loss = 9.08278
I0524 00:04:18.402609 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.08278 (* 1 = 9.08278 loss)
I0524 00:04:18.402809 11835 sgd_solver.cpp:112] Iteration 11110, lr = 0.1
I0524 00:04:24.823803 11835 solver.cpp:239] Iteration 11120 (1.5574 iter/s, 6.42095s/10 iters), loss = 8.74913
I0524 00:04:24.823855 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.74913 (* 1 = 8.74913 loss)
I0524 00:04:24.824050 11835 sgd_solver.cpp:112] Iteration 11120, lr = 0.1
I0524 00:04:31.988806 11835 solver.cpp:239] Iteration 11130 (1.39574 iter/s, 7.16468s/10 iters), loss = 8.23514
I0524 00:04:31.988852 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.23514 (* 1 = 8.23514 loss)
I0524 00:04:32.617568 11835 sgd_solver.cpp:112] Iteration 11130, lr = 0.1
I0524 00:04:39.322533 11835 solver.cpp:239] Iteration 11140 (1.36362 iter/s, 7.3334s/10 iters), loss = 9.12643
I0524 00:04:39.322685 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.12643 (* 1 = 9.12643 loss)
I0524 00:04:39.322840 11835 sgd_solver.cpp:112] Iteration 11140, lr = 0.1
I0524 00:04:45.633025 11835 solver.cpp:239] Iteration 11150 (1.58476 iter/s, 6.31011s/10 iters), loss = 9.11287
I0524 00:04:45.633069 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.11287 (* 1 = 9.11287 loss)
I0524 00:04:45.633188 11835 sgd_solver.cpp:112] Iteration 11150, lr = 0.1
I0524 00:04:51.802148 11835 solver.cpp:239] Iteration 11160 (1.62105 iter/s, 6.16883s/10 iters), loss = 9.32329
I0524 00:04:51.802203 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.32329 (* 1 = 9.32329 loss)
I0524 00:04:51.802350 11835 sgd_solver.cpp:112] Iteration 11160, lr = 0.1
I0524 00:04:58.372473 11835 solver.cpp:239] Iteration 11170 (1.52206 iter/s, 6.57003s/10 iters), loss = 8.55575
I0524 00:04:58.372521 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.55575 (* 1 = 8.55575 loss)
I0524 00:04:58.372705 11835 sgd_solver.cpp:112] Iteration 11170, lr = 0.1
I0524 00:05:04.258827 11835 solver.cpp:239] Iteration 11180 (1.69892 iter/s, 5.88608s/10 iters), loss = 8.62143
I0524 00:05:04.258879 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.62143 (* 1 = 8.62143 loss)
I0524 00:05:04.344790 11835 sgd_solver.cpp:112] Iteration 11180, lr = 0.1
I0524 00:05:09.934151 11835 solver.cpp:239] Iteration 11190 (1.7621 iter/s, 5.67505s/10 iters), loss = 8.42546
I0524 00:05:09.934368 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.42546 (* 1 = 8.42546 loss)
I0524 00:05:09.934597 11835 sgd_solver.cpp:112] Iteration 11190, lr = 0.1
I0524 00:05:16.923168 11835 solver.cpp:239] Iteration 11200 (1.43091 iter/s, 6.98857s/10 iters), loss = 8.88238
I0524 00:05:16.923216 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.88238 (* 1 = 8.88238 loss)
I0524 00:05:16.923230 11835 sgd_solver.cpp:112] Iteration 11200, lr = 0.1
I0524 00:05:26.253386 11835 solver.cpp:239] Iteration 11210 (1.07183 iter/s, 9.32982s/10 iters), loss = 9.50095
I0524 00:05:26.253433 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.50095 (* 1 = 9.50095 loss)
I0524 00:05:26.259280 11835 sgd_solver.cpp:112] Iteration 11210, lr = 0.1
I0524 00:05:33.620482 11835 solver.cpp:239] Iteration 11220 (1.35745 iter/s, 7.36678s/10 iters), loss = 9.27162
I0524 00:05:33.620529 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.27162 (* 1 = 9.27162 loss)
I0524 00:05:33.724701 11835 sgd_solver.cpp:112] Iteration 11220, lr = 0.1
I0524 00:05:39.833636 11835 solver.cpp:239] Iteration 11230 (1.60956 iter/s, 6.21286s/10 iters), loss = 8.32068
I0524 00:05:39.833685 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.32068 (* 1 = 8.32068 loss)
I0524 00:05:39.833699 11835 sgd_solver.cpp:112] Iteration 11230, lr = 0.1
I0524 00:05:47.417346 11835 solver.cpp:239] Iteration 11240 (1.31869 iter/s, 7.58326s/10 iters), loss = 8.57669
I0524 00:05:47.417657 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.57669 (* 1 = 8.57669 loss)
I0524 00:05:47.558220 11835 sgd_solver.cpp:112] Iteration 11240, lr = 0.1
I0524 00:05:54.597995 11835 solver.cpp:239] Iteration 11250 (1.39273 iter/s, 7.18014s/10 iters), loss = 7.85776
I0524 00:05:54.598035 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.85776 (* 1 = 7.85776 loss)
I0524 00:05:54.727984 11835 sgd_solver.cpp:112] Iteration 11250, lr = 0.1
I0524 00:06:02.056017 11835 solver.cpp:239] Iteration 11260 (1.3409 iter/s, 7.45768s/10 iters), loss = 7.97239
I0524 00:06:02.056072 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.97239 (* 1 = 7.97239 loss)
I0524 00:06:02.056118 11835 sgd_solver.cpp:112] Iteration 11260, lr = 0.1
I0524 00:06:08.131392 11835 solver.cpp:239] Iteration 11270 (1.64606 iter/s, 6.0751s/10 iters), loss = 8.19612
I0524 00:06:08.131436 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.19612 (* 1 = 8.19612 loss)
I0524 00:06:08.174897 11835 sgd_solver.cpp:112] Iteration 11270, lr = 0.1
I0524 00:06:14.015090 11835 solver.cpp:239] Iteration 11280 (1.69969 iter/s, 5.88342s/10 iters), loss = 8.22062
I0524 00:06:14.015151 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.22062 (* 1 = 8.22062 loss)
I0524 00:06:14.017288 11835 sgd_solver.cpp:112] Iteration 11280, lr = 0.1
I0524 00:06:21.770792 11835 solver.cpp:239] Iteration 11290 (1.28943 iter/s, 7.75534s/10 iters), loss = 8.19712
I0524 00:06:21.770948 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.19712 (* 1 = 8.19712 loss)
I0524 00:06:21.784780 11835 sgd_solver.cpp:112] Iteration 11290, lr = 0.1
I0524 00:06:28.440042 11835 solver.cpp:239] Iteration 11300 (1.49951 iter/s, 6.66886s/10 iters), loss = 8.99198
I0524 00:06:28.440080 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.99198 (* 1 = 8.99198 loss)
I0524 00:06:28.440093 11835 sgd_solver.cpp:112] Iteration 11300, lr = 0.1
I0524 00:06:34.870216 11835 solver.cpp:239] Iteration 11310 (1.55531 iter/s, 6.42957s/10 iters), loss = 8.19405
I0524 00:06:34.870263 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.19405 (* 1 = 8.19405 loss)
I0524 00:06:34.870352 11835 sgd_solver.cpp:112] Iteration 11310, lr = 0.1
I0524 00:06:40.778384 11835 solver.cpp:239] Iteration 11320 (1.69265 iter/s, 5.90789s/10 iters), loss = 8.73448
I0524 00:06:40.778429 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.73448 (* 1 = 8.73448 loss)
I0524 00:06:40.778443 11835 sgd_solver.cpp:112] Iteration 11320, lr = 0.1
I0524 00:06:47.903262 11835 solver.cpp:239] Iteration 11330 (1.40361 iter/s, 7.1245s/10 iters), loss = 8.423
I0524 00:06:47.903306 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.423 (* 1 = 8.423 loss)
I0524 00:06:47.903318 11835 sgd_solver.cpp:112] Iteration 11330, lr = 0.1
I0524 00:06:54.093611 11835 solver.cpp:239] Iteration 11340 (1.61578 iter/s, 6.18894s/10 iters), loss = 8.46189
I0524 00:06:54.093892 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.46189 (* 1 = 8.46189 loss)
I0524 00:06:54.093955 11835 sgd_solver.cpp:112] Iteration 11340, lr = 0.1
I0524 00:07:00.997810 11835 solver.cpp:239] Iteration 11350 (1.4485 iter/s, 6.90371s/10 iters), loss = 8.85214
I0524 00:07:00.997858 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.85214 (* 1 = 8.85214 loss)
I0524 00:07:00.997961 11835 sgd_solver.cpp:112] Iteration 11350, lr = 0.1
I0524 00:07:06.662044 11835 solver.cpp:239] Iteration 11360 (1.76555 iter/s, 5.66397s/10 iters), loss = 8.47202
I0524 00:07:06.662091 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.47202 (* 1 = 8.47202 loss)
I0524 00:07:06.662106 11835 sgd_solver.cpp:112] Iteration 11360, lr = 0.1
I0524 00:07:13.727608 11835 solver.cpp:239] Iteration 11370 (1.41539 iter/s, 7.06517s/10 iters), loss = 8.21103
I0524 00:07:13.727658 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.21103 (* 1 = 8.21103 loss)
I0524 00:07:13.727948 11835 sgd_solver.cpp:112] Iteration 11370, lr = 0.1
I0524 00:07:20.433657 11835 solver.cpp:239] Iteration 11380 (1.49126 iter/s, 6.70574s/10 iters), loss = 8.81123
I0524 00:07:20.433712 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.81123 (* 1 = 8.81123 loss)
I0524 00:07:20.433729 11835 sgd_solver.cpp:112] Iteration 11380, lr = 0.1
I0524 00:07:28.064944 11835 solver.cpp:239] Iteration 11390 (1.31045 iter/s, 7.63096s/10 iters), loss = 8.23977
I0524 00:07:28.065220 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.23977 (* 1 = 8.23977 loss)
I0524 00:07:28.065273 11835 sgd_solver.cpp:112] Iteration 11390, lr = 0.1
I0524 00:07:36.625159 11835 solver.cpp:239] Iteration 11400 (1.16827 iter/s, 8.55964s/10 iters), loss = 8.45363
I0524 00:07:36.625233 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.45363 (* 1 = 8.45363 loss)
I0524 00:07:36.625490 11835 sgd_solver.cpp:112] Iteration 11400, lr = 0.1
I0524 00:07:45.179554 11835 solver.cpp:239] Iteration 11410 (1.16904 iter/s, 8.554s/10 iters), loss = 8.26753
I0524 00:07:45.179605 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.26753 (* 1 = 8.26753 loss)
I0524 00:07:45.179951 11835 sgd_solver.cpp:112] Iteration 11410, lr = 0.1
I0524 00:07:51.351949 11835 solver.cpp:239] Iteration 11420 (1.62019 iter/s, 6.1721s/10 iters), loss = 8.16215
I0524 00:07:51.352026 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.16215 (* 1 = 8.16215 loss)
I0524 00:07:51.370342 11835 sgd_solver.cpp:112] Iteration 11420, lr = 0.1
I0524 00:07:59.629047 11835 solver.cpp:239] Iteration 11430 (1.20821 iter/s, 8.27671s/10 iters), loss = 7.81258
I0524 00:07:59.629191 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.81258 (* 1 = 7.81258 loss)
I0524 00:07:59.629211 11835 sgd_solver.cpp:112] Iteration 11430, lr = 0.1
I0524 00:08:05.630139 11835 solver.cpp:239] Iteration 11440 (1.66647 iter/s, 6.00071s/10 iters), loss = 9.17548
I0524 00:08:05.630194 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.17548 (* 1 = 9.17548 loss)
I0524 00:08:05.630465 11835 sgd_solver.cpp:112] Iteration 11440, lr = 0.1
I0524 00:08:12.778391 11835 solver.cpp:239] Iteration 11450 (1.39901 iter/s, 7.14793s/10 iters), loss = 8.30135
I0524 00:08:12.778437 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.30135 (* 1 = 8.30135 loss)
I0524 00:08:12.778585 11835 sgd_solver.cpp:112] Iteration 11450, lr = 0.1
I0524 00:08:18.786474 11835 solver.cpp:239] Iteration 11460 (1.6645 iter/s, 6.0078s/10 iters), loss = 7.69594
I0524 00:08:18.786533 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.69594 (* 1 = 7.69594 loss)
I0524 00:08:18.786579 11835 sgd_solver.cpp:112] Iteration 11460, lr = 0.1
I0524 00:08:25.097502 11835 solver.cpp:239] Iteration 11470 (1.5846 iter/s, 6.31074s/10 iters), loss = 7.72726
I0524 00:08:25.097549 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.72726 (* 1 = 7.72726 loss)
I0524 00:08:25.097787 11835 sgd_solver.cpp:112] Iteration 11470, lr = 0.1
I0524 00:08:32.606138 11835 solver.cpp:239] Iteration 11480 (1.33186 iter/s, 7.5083s/10 iters), loss = 9.01207
I0524 00:08:32.606235 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.01207 (* 1 = 9.01207 loss)
I0524 00:08:32.606281 11835 sgd_solver.cpp:112] Iteration 11480, lr = 0.1
I0524 00:08:38.147755 11835 solver.cpp:239] Iteration 11490 (1.80462 iter/s, 5.54132s/10 iters), loss = 8.13317
I0524 00:08:38.147801 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.13317 (* 1 = 8.13317 loss)
I0524 00:08:38.147912 11835 sgd_solver.cpp:112] Iteration 11490, lr = 0.1
I0524 00:08:44.311914 11835 solver.cpp:239] Iteration 11500 (1.62236 iter/s, 6.16387s/10 iters), loss = 9.14611
I0524 00:08:44.311964 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.14611 (* 1 = 9.14611 loss)
I0524 00:08:44.312075 11835 sgd_solver.cpp:112] Iteration 11500, lr = 0.1
I0524 00:08:50.062475 11835 solver.cpp:239] Iteration 11510 (1.73904 iter/s, 5.7503s/10 iters), loss = 8.17567
I0524 00:08:50.062525 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.17567 (* 1 = 8.17567 loss)
I0524 00:08:50.062538 11835 sgd_solver.cpp:112] Iteration 11510, lr = 0.1
I0524 00:08:56.060961 11835 solver.cpp:239] Iteration 11520 (1.66716 iter/s, 5.99822s/10 iters), loss = 7.93429
I0524 00:08:56.061002 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.93429 (* 1 = 7.93429 loss)
I0524 00:08:56.492122 11835 sgd_solver.cpp:112] Iteration 11520, lr = 0.1
I0524 00:09:03.787547 11835 solver.cpp:239] Iteration 11530 (1.29429 iter/s, 7.72625s/10 iters), loss = 8.79737
I0524 00:09:03.787746 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.79737 (* 1 = 8.79737 loss)
I0524 00:09:04.620260 11835 sgd_solver.cpp:112] Iteration 11530, lr = 0.1
I0524 00:09:12.582798 11835 solver.cpp:239] Iteration 11540 (1.13705 iter/s, 8.79472s/10 iters), loss = 8.35452
I0524 00:09:12.582852 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.35452 (* 1 = 8.35452 loss)
I0524 00:09:13.354501 11835 sgd_solver.cpp:112] Iteration 11540, lr = 0.1
I0524 00:09:21.196071 11835 solver.cpp:239] Iteration 11550 (1.16105 iter/s, 8.61289s/10 iters), loss = 8.32218
I0524 00:09:21.196122 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.32218 (* 1 = 8.32218 loss)
I0524 00:09:21.899713 11835 sgd_solver.cpp:112] Iteration 11550, lr = 0.1
I0524 00:09:29.482820 11835 solver.cpp:239] Iteration 11560 (1.2068 iter/s, 8.28638s/10 iters), loss = 8.88697
I0524 00:09:29.482870 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.88697 (* 1 = 8.88697 loss)
I0524 00:09:29.483069 11835 sgd_solver.cpp:112] Iteration 11560, lr = 0.1
I0524 00:09:37.246588 11835 solver.cpp:239] Iteration 11570 (1.28809 iter/s, 7.76343s/10 iters), loss = 9.13416
I0524 00:09:37.246822 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.13416 (* 1 = 9.13416 loss)
I0524 00:09:37.393327 11835 sgd_solver.cpp:112] Iteration 11570, lr = 0.1
I0524 00:09:43.895257 11835 solver.cpp:239] Iteration 11580 (1.50417 iter/s, 6.64819s/10 iters), loss = 7.75279
I0524 00:09:43.895305 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.75279 (* 1 = 7.75279 loss)
I0524 00:09:43.895422 11835 sgd_solver.cpp:112] Iteration 11580, lr = 0.1
I0524 00:09:52.503163 11835 solver.cpp:239] Iteration 11590 (1.16177 iter/s, 8.60753s/10 iters), loss = 8.41924
I0524 00:09:52.503221 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.41924 (* 1 = 8.41924 loss)
I0524 00:09:52.503468 11835 sgd_solver.cpp:112] Iteration 11590, lr = 0.1
I0524 00:09:58.888859 11835 solver.cpp:239] Iteration 11600 (1.56607 iter/s, 6.3854s/10 iters), loss = 8.21307
I0524 00:09:58.888911 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.21307 (* 1 = 8.21307 loss)
I0524 00:09:58.888936 11835 sgd_solver.cpp:112] Iteration 11600, lr = 0.1
I0524 00:10:04.845437 11835 solver.cpp:239] Iteration 11610 (1.67892 iter/s, 5.95619s/10 iters), loss = 8.04947
I0524 00:10:04.845476 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.04947 (* 1 = 8.04947 loss)
I0524 00:10:04.845583 11835 sgd_solver.cpp:112] Iteration 11610, lr = 0.1
I0524 00:10:12.106158 11835 solver.cpp:239] Iteration 11620 (1.37733 iter/s, 7.2604s/10 iters), loss = 9.21346
I0524 00:10:12.106415 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.21346 (* 1 = 9.21346 loss)
I0524 00:10:12.247776 11835 sgd_solver.cpp:112] Iteration 11620, lr = 0.1
I0524 00:10:18.346166 11835 solver.cpp:239] Iteration 11630 (1.60268 iter/s, 6.23955s/10 iters), loss = 8.1241
I0524 00:10:18.346222 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.1241 (* 1 = 8.1241 loss)
I0524 00:10:18.346386 11835 sgd_solver.cpp:112] Iteration 11630, lr = 0.1
I0524 00:10:26.580729 11835 solver.cpp:239] Iteration 11640 (1.21445 iter/s, 8.2342s/10 iters), loss = 8.55121
I0524 00:10:26.580776 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.55121 (* 1 = 8.55121 loss)
I0524 00:10:26.581001 11835 sgd_solver.cpp:112] Iteration 11640, lr = 0.1
I0524 00:10:34.444633 11835 solver.cpp:239] Iteration 11650 (1.27169 iter/s, 7.86356s/10 iters), loss = 8.27571
I0524 00:10:34.444680 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.27571 (* 1 = 8.27571 loss)
I0524 00:10:34.444803 11835 sgd_solver.cpp:112] Iteration 11650, lr = 0.1
I0524 00:10:43.173231 11835 solver.cpp:239] Iteration 11660 (1.14571 iter/s, 8.72822s/10 iters), loss = 8.22827
I0524 00:10:43.173514 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.22827 (* 1 = 8.22827 loss)
I0524 00:10:43.183611 11835 sgd_solver.cpp:112] Iteration 11660, lr = 0.1
I0524 00:10:49.236541 11835 solver.cpp:239] Iteration 11670 (1.64939 iter/s, 6.06285s/10 iters), loss = 8.81793
I0524 00:10:49.236574 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.81793 (* 1 = 8.81793 loss)
I0524 00:10:49.236588 11835 sgd_solver.cpp:112] Iteration 11670, lr = 0.1
I0524 00:10:57.331310 11835 solver.cpp:239] Iteration 11680 (1.23542 iter/s, 8.0944s/10 iters), loss = 8.5079
I0524 00:10:57.331364 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.5079 (* 1 = 8.5079 loss)
I0524 00:10:58.012730 11835 sgd_solver.cpp:112] Iteration 11680, lr = 0.1
I0524 00:11:04.202559 11835 solver.cpp:239] Iteration 11690 (1.4554 iter/s, 6.87094s/10 iters), loss = 8.60942
I0524 00:11:04.202611 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.60942 (* 1 = 8.60942 loss)
I0524 00:11:04.202862 11835 sgd_solver.cpp:112] Iteration 11690, lr = 0.1
I0524 00:11:11.631042 11835 solver.cpp:239] Iteration 11700 (1.34623 iter/s, 7.42814s/10 iters), loss = 8.73705
I0524 00:11:11.631095 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.73705 (* 1 = 8.73705 loss)
I0524 00:11:12.181936 11835 sgd_solver.cpp:112] Iteration 11700, lr = 0.1
I0524 00:11:18.643733 11835 solver.cpp:239] Iteration 11710 (1.42605 iter/s, 7.01238s/10 iters), loss = 9.25168
I0524 00:11:18.643944 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.25168 (* 1 = 9.25168 loss)
I0524 00:11:18.827672 11835 sgd_solver.cpp:112] Iteration 11710, lr = 0.1
I0524 00:11:25.948318 11835 solver.cpp:239] Iteration 11720 (1.36909 iter/s, 7.30412s/10 iters), loss = 7.25257
I0524 00:11:25.948369 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.25257 (* 1 = 7.25257 loss)
I0524 00:11:25.948590 11835 sgd_solver.cpp:112] Iteration 11720, lr = 0.1
I0524 00:11:31.758723 11835 solver.cpp:239] Iteration 11730 (1.72113 iter/s, 5.81012s/10 iters), loss = 9.16983
I0524 00:11:31.758771 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.16983 (* 1 = 9.16983 loss)
I0524 00:11:32.456928 11835 sgd_solver.cpp:112] Iteration 11730, lr = 0.1
I0524 00:11:39.470216 11835 solver.cpp:239] Iteration 11740 (1.29682 iter/s, 7.71116s/10 iters), loss = 8.28892
I0524 00:11:39.470257 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.28892 (* 1 = 8.28892 loss)
I0524 00:11:39.625516 11835 sgd_solver.cpp:112] Iteration 11740, lr = 0.1
I0524 00:11:48.102838 11835 solver.cpp:239] Iteration 11750 (1.15845 iter/s, 8.63225s/10 iters), loss = 7.61632
I0524 00:11:48.102885 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.61632 (* 1 = 7.61632 loss)
I0524 00:11:48.103101 11835 sgd_solver.cpp:112] Iteration 11750, lr = 0.1
I0524 00:11:53.736918 11835 solver.cpp:239] Iteration 11760 (1.775 iter/s, 5.63381s/10 iters), loss = 9.09738
I0524 00:11:53.737203 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.09738 (* 1 = 9.09738 loss)
I0524 00:11:53.737277 11835 sgd_solver.cpp:112] Iteration 11760, lr = 0.1
I0524 00:11:59.368304 11835 solver.cpp:239] Iteration 11770 (1.7759 iter/s, 5.63096s/10 iters), loss = 7.23549
I0524 00:11:59.368352 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.23549 (* 1 = 7.23549 loss)
I0524 00:11:59.368366 11835 sgd_solver.cpp:112] Iteration 11770, lr = 0.1
I0524 00:12:06.528411 11835 solver.cpp:239] Iteration 11780 (1.3967 iter/s, 7.15972s/10 iters), loss = 8.77496
I0524 00:12:06.528460 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.77496 (* 1 = 8.77496 loss)
I0524 00:12:06.528493 11835 sgd_solver.cpp:112] Iteration 11780, lr = 0.1
I0524 00:12:14.137642 11835 solver.cpp:239] Iteration 11790 (1.31425 iter/s, 7.6089s/10 iters), loss = 8.73533
I0524 00:12:14.137689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.73533 (* 1 = 8.73533 loss)
I0524 00:12:14.137744 11835 sgd_solver.cpp:112] Iteration 11790, lr = 0.1
I0524 00:12:20.446913 11835 solver.cpp:239] Iteration 11800 (1.58504 iter/s, 6.30898s/10 iters), loss = 7.67518
I0524 00:12:20.446966 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.67518 (* 1 = 7.67518 loss)
I0524 00:12:20.446982 11835 sgd_solver.cpp:112] Iteration 11800, lr = 0.1
I0524 00:12:26.564357 11835 solver.cpp:239] Iteration 11810 (1.63504 iter/s, 6.11604s/10 iters), loss = 9.06689
I0524 00:12:26.564481 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.06689 (* 1 = 9.06689 loss)
I0524 00:12:26.564525 11835 sgd_solver.cpp:112] Iteration 11810, lr = 0.1
I0524 00:12:33.110425 11835 solver.cpp:239] Iteration 11820 (1.52772 iter/s, 6.54569s/10 iters), loss = 7.7964
I0524 00:12:33.110472 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.7964 (* 1 = 7.7964 loss)
I0524 00:12:33.110601 11835 sgd_solver.cpp:112] Iteration 11820, lr = 0.1
I0524 00:12:39.261616 11835 solver.cpp:239] Iteration 11830 (1.62578 iter/s, 6.15091s/10 iters), loss = 8.24425
I0524 00:12:39.261672 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.24425 (* 1 = 8.24425 loss)
I0524 00:12:39.261929 11835 sgd_solver.cpp:112] Iteration 11830, lr = 0.1
I0524 00:12:45.691079 11835 solver.cpp:239] Iteration 11840 (1.55541 iter/s, 6.42916s/10 iters), loss = 8.47012
I0524 00:12:45.691126 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.47012 (* 1 = 8.47012 loss)
I0524 00:12:45.691408 11835 sgd_solver.cpp:112] Iteration 11840, lr = 0.1
I0524 00:12:52.688771 11835 solver.cpp:239] Iteration 11850 (1.42911 iter/s, 6.99738s/10 iters), loss = 8.00748
I0524 00:12:52.688825 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.00748 (* 1 = 8.00748 loss)
I0524 00:12:53.078353 11835 sgd_solver.cpp:112] Iteration 11850, lr = 0.1
I0524 00:13:01.033638 11835 solver.cpp:239] Iteration 11860 (1.1984 iter/s, 8.34448s/10 iters), loss = 7.90865
I0524 00:13:01.033949 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.90865 (* 1 = 7.90865 loss)
I0524 00:13:01.034009 11835 sgd_solver.cpp:112] Iteration 11860, lr = 0.1
I0524 00:13:07.208961 11835 solver.cpp:239] Iteration 11870 (1.61949 iter/s, 6.17479s/10 iters), loss = 8.63322
I0524 00:13:07.209013 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.63322 (* 1 = 8.63322 loss)
I0524 00:13:07.295071 11835 sgd_solver.cpp:112] Iteration 11870, lr = 0.1
I0524 00:13:14.218591 11835 solver.cpp:239] Iteration 11880 (1.42667 iter/s, 7.00932s/10 iters), loss = 7.93504
I0524 00:13:14.218632 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.93504 (* 1 = 7.93504 loss)
I0524 00:13:14.218669 11835 sgd_solver.cpp:112] Iteration 11880, lr = 0.1
I0524 00:13:20.737169 11835 solver.cpp:239] Iteration 11890 (1.53415 iter/s, 6.51827s/10 iters), loss = 8.54564
I0524 00:13:20.737257 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.54564 (* 1 = 8.54564 loss)
I0524 00:13:20.737289 11835 sgd_solver.cpp:112] Iteration 11890, lr = 0.1
I0524 00:13:27.343037 11835 solver.cpp:239] Iteration 11900 (1.51388 iter/s, 6.60554s/10 iters), loss = 8.71334
I0524 00:13:27.343091 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.71334 (* 1 = 8.71334 loss)
I0524 00:13:27.343345 11835 sgd_solver.cpp:112] Iteration 11900, lr = 0.1
I0524 00:13:35.163303 11835 solver.cpp:239] Iteration 11910 (1.27878 iter/s, 7.81993s/10 iters), loss = 8.06164
I0524 00:13:35.163488 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.06164 (* 1 = 8.06164 loss)
I0524 00:13:35.403616 11835 sgd_solver.cpp:112] Iteration 11910, lr = 0.1
I0524 00:13:42.421350 11835 solver.cpp:239] Iteration 11920 (1.37787 iter/s, 7.25759s/10 iters), loss = 9.26951
I0524 00:13:42.421396 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.26951 (* 1 = 9.26951 loss)
I0524 00:13:42.421411 11835 sgd_solver.cpp:112] Iteration 11920, lr = 0.1
I0524 00:13:49.522805 11835 solver.cpp:239] Iteration 11930 (1.40822 iter/s, 7.10114s/10 iters), loss = 7.65401
I0524 00:13:49.522856 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.65401 (* 1 = 7.65401 loss)
I0524 00:13:49.522871 11835 sgd_solver.cpp:112] Iteration 11930, lr = 0.1
I0524 00:13:57.852147 11835 solver.cpp:239] Iteration 11940 (1.20068 iter/s, 8.32861s/10 iters), loss = 7.29074
I0524 00:13:57.852186 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.29074 (* 1 = 7.29074 loss)
I0524 00:13:57.852334 11835 sgd_solver.cpp:112] Iteration 11940, lr = 0.1
I0524 00:14:03.885572 11835 solver.cpp:239] Iteration 11950 (1.65751 iter/s, 6.03315s/10 iters), loss = 8.08581
I0524 00:14:03.885618 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.08581 (* 1 = 8.08581 loss)
I0524 00:14:03.885632 11835 sgd_solver.cpp:112] Iteration 11950, lr = 0.1
I0524 00:14:10.867038 11835 solver.cpp:239] Iteration 11960 (1.43288 iter/s, 6.97894s/10 iters), loss = 8.32787
I0524 00:14:10.867271 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.32787 (* 1 = 8.32787 loss)
I0524 00:14:10.867332 11835 sgd_solver.cpp:112] Iteration 11960, lr = 0.1
I0524 00:14:16.580417 11835 solver.cpp:239] Iteration 11970 (1.75041 iter/s, 5.71295s/10 iters), loss = 7.98904
I0524 00:14:16.580463 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.98904 (* 1 = 7.98904 loss)
I0524 00:14:16.580739 11835 sgd_solver.cpp:112] Iteration 11970, lr = 0.1
I0524 00:14:23.948654 11835 solver.cpp:239] Iteration 11980 (1.35724 iter/s, 7.3679s/10 iters), loss = 8.7987
I0524 00:14:23.948719 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.7987 (* 1 = 8.7987 loss)
I0524 00:14:23.948943 11835 sgd_solver.cpp:112] Iteration 11980, lr = 0.1
I0524 00:14:32.243150 11835 solver.cpp:239] Iteration 11990 (1.20567 iter/s, 8.29413s/10 iters), loss = 8.62147
I0524 00:14:32.243186 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.62147 (* 1 = 8.62147 loss)
I0524 00:14:32.533294 11835 sgd_solver.cpp:112] Iteration 11990, lr = 0.1
I0524 00:14:39.568617 11835 solver.cpp:239] Iteration 12000 (1.36516 iter/s, 7.32514s/10 iters), loss = 8.49039
I0524 00:14:39.568668 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.49039 (* 1 = 8.49039 loss)
I0524 00:14:39.568683 11835 sgd_solver.cpp:112] Iteration 12000, lr = 0.1
I0524 00:14:45.306238 11835 solver.cpp:239] Iteration 12010 (1.743 iter/s, 5.73723s/10 iters), loss = 7.44628
I0524 00:14:45.306521 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.44628 (* 1 = 7.44628 loss)
I0524 00:14:45.306579 11835 sgd_solver.cpp:112] Iteration 12010, lr = 0.1
I0524 00:14:52.428433 11835 solver.cpp:239] Iteration 12020 (1.40454 iter/s, 7.11977s/10 iters), loss = 8.14051
I0524 00:14:52.428477 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.14051 (* 1 = 8.14051 loss)
I0524 00:14:52.784121 11835 sgd_solver.cpp:112] Iteration 12020, lr = 0.1
I0524 00:14:58.720268 11835 solver.cpp:239] Iteration 12030 (1.58944 iter/s, 6.29154s/10 iters), loss = 7.92876
I0524 00:14:58.720324 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.92876 (* 1 = 7.92876 loss)
I0524 00:14:58.762887 11835 sgd_solver.cpp:112] Iteration 12030, lr = 0.1
I0524 00:15:05.893877 11835 solver.cpp:239] Iteration 12040 (1.39406 iter/s, 7.17329s/10 iters), loss = 8.59191
I0524 00:15:05.893915 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.59191 (* 1 = 8.59191 loss)
I0524 00:15:06.079660 11835 sgd_solver.cpp:112] Iteration 12040, lr = 0.1
I0524 00:15:12.488878 11835 solver.cpp:239] Iteration 12050 (1.51637 iter/s, 6.5947s/10 iters), loss = 8.01993
I0524 00:15:12.488925 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.01993 (* 1 = 8.01993 loss)
I0524 00:15:12.488937 11835 sgd_solver.cpp:112] Iteration 12050, lr = 0.1
I0524 00:15:20.675612 11835 solver.cpp:239] Iteration 12060 (1.22158 iter/s, 8.18614s/10 iters), loss = 8.40892
I0524 00:15:20.675890 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.40892 (* 1 = 8.40892 loss)
I0524 00:15:20.702658 11835 sgd_solver.cpp:112] Iteration 12060, lr = 0.1
I0524 00:15:27.502635 11835 solver.cpp:239] Iteration 12070 (1.46488 iter/s, 6.82651s/10 iters), loss = 8.88551
I0524 00:15:27.502674 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.88551 (* 1 = 8.88551 loss)
I0524 00:15:27.503037 11835 sgd_solver.cpp:112] Iteration 12070, lr = 0.1
I0524 00:15:34.712417 11835 solver.cpp:239] Iteration 12080 (1.38707 iter/s, 7.20946s/10 iters), loss = 9.75979
I0524 00:15:34.712471 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.75979 (* 1 = 9.75979 loss)
I0524 00:15:34.712831 11835 sgd_solver.cpp:112] Iteration 12080, lr = 0.1
I0524 00:15:42.231910 11835 solver.cpp:239] Iteration 12090 (1.32994 iter/s, 7.51916s/10 iters), loss = 8.4736
I0524 00:15:42.231959 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.4736 (* 1 = 8.4736 loss)
I0524 00:15:42.762298 11835 sgd_solver.cpp:112] Iteration 12090, lr = 0.1
I0524 00:15:49.720039 11835 solver.cpp:239] Iteration 12100 (1.33551 iter/s, 7.4878s/10 iters), loss = 8.05339
I0524 00:15:49.720091 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.05339 (* 1 = 8.05339 loss)
I0524 00:15:49.720355 11835 sgd_solver.cpp:112] Iteration 12100, lr = 0.1
I0524 00:15:55.686900 11835 solver.cpp:239] Iteration 12110 (1.676 iter/s, 5.96658s/10 iters), loss = 8.6512
I0524 00:15:55.687021 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.6512 (* 1 = 8.6512 loss)
I0524 00:15:55.687041 11835 sgd_solver.cpp:112] Iteration 12110, lr = 0.1
I0524 00:16:01.723788 11835 solver.cpp:239] Iteration 12120 (1.65667 iter/s, 6.03621s/10 iters), loss = 8.04879
I0524 00:16:01.723837 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.04879 (* 1 = 8.04879 loss)
I0524 00:16:01.723976 11835 sgd_solver.cpp:112] Iteration 12120, lr = 0.1
I0524 00:16:08.340677 11835 solver.cpp:239] Iteration 12130 (1.51135 iter/s, 6.61659s/10 iters), loss = 8.63355
I0524 00:16:08.340728 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.63355 (* 1 = 8.63355 loss)
I0524 00:16:08.408087 11835 sgd_solver.cpp:112] Iteration 12130, lr = 0.1
I0524 00:16:16.304770 11835 solver.cpp:239] Iteration 12140 (1.25569 iter/s, 7.96375s/10 iters), loss = 7.51826
I0524 00:16:16.304821 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.51826 (* 1 = 7.51826 loss)
I0524 00:16:16.304951 11835 sgd_solver.cpp:112] Iteration 12140, lr = 0.1
I0524 00:16:22.263980 11835 solver.cpp:239] Iteration 12150 (1.67815 iter/s, 5.95894s/10 iters), loss = 7.94355
I0524 00:16:22.264019 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.94355 (* 1 = 7.94355 loss)
I0524 00:16:22.280562 11835 sgd_solver.cpp:112] Iteration 12150, lr = 0.1
I0524 00:16:28.998209 11835 solver.cpp:239] Iteration 12160 (1.48502 iter/s, 6.73392s/10 iters), loss = 8.08853
I0524 00:16:28.998423 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.08853 (* 1 = 8.08853 loss)
I0524 00:16:28.998471 11835 sgd_solver.cpp:112] Iteration 12160, lr = 0.1
I0524 00:16:35.664999 11835 solver.cpp:239] Iteration 12170 (1.50011 iter/s, 6.66619s/10 iters), loss = 8.41179
I0524 00:16:35.665058 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.41179 (* 1 = 8.41179 loss)
I0524 00:16:35.665293 11835 sgd_solver.cpp:112] Iteration 12170, lr = 0.1
I0524 00:16:42.265691 11835 solver.cpp:239] Iteration 12180 (1.51506 iter/s, 6.60039s/10 iters), loss = 7.98054
I0524 00:16:42.265735 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.98054 (* 1 = 7.98054 loss)
I0524 00:16:42.266067 11835 sgd_solver.cpp:112] Iteration 12180, lr = 0.1
I0524 00:16:50.350044 11835 solver.cpp:239] Iteration 12190 (1.23701 iter/s, 8.084s/10 iters), loss = 8.57535
I0524 00:16:50.350097 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.57535 (* 1 = 8.57535 loss)
I0524 00:16:50.404110 11835 sgd_solver.cpp:112] Iteration 12190, lr = 0.1
I0524 00:16:56.888726 11835 solver.cpp:239] Iteration 12200 (1.52943 iter/s, 6.53839s/10 iters), loss = 9.13939
I0524 00:16:56.888763 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.13939 (* 1 = 9.13939 loss)
I0524 00:16:56.888967 11835 sgd_solver.cpp:112] Iteration 12200, lr = 0.1
I0524 00:17:03.544862 11835 solver.cpp:239] Iteration 12210 (1.50244 iter/s, 6.65583s/10 iters), loss = 9.13679
I0524 00:17:03.545176 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.13679 (* 1 = 9.13679 loss)
I0524 00:17:03.545241 11835 sgd_solver.cpp:112] Iteration 12210, lr = 0.1
I0524 00:17:11.230409 11835 solver.cpp:239] Iteration 12220 (1.30124 iter/s, 7.68499s/10 iters), loss = 8.17371
I0524 00:17:11.230463 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.17371 (* 1 = 8.17371 loss)
I0524 00:17:11.338011 11835 sgd_solver.cpp:112] Iteration 12220, lr = 0.1
I0524 00:17:17.170729 11835 solver.cpp:239] Iteration 12230 (1.68349 iter/s, 5.94003s/10 iters), loss = 7.2614
I0524 00:17:17.170770 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2614 (* 1 = 7.2614 loss)
I0524 00:17:17.171010 11835 sgd_solver.cpp:112] Iteration 12230, lr = 0.1
I0524 00:17:25.724436 11835 solver.cpp:239] Iteration 12240 (1.16914 iter/s, 8.55333s/10 iters), loss = 7.82479
I0524 00:17:25.724498 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.82479 (* 1 = 7.82479 loss)
I0524 00:17:25.724705 11835 sgd_solver.cpp:112] Iteration 12240, lr = 0.1
I0524 00:17:33.932209 11835 solver.cpp:239] Iteration 12250 (1.21841 iter/s, 8.20741s/10 iters), loss = 8.49601
I0524 00:17:33.932502 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.49601 (* 1 = 8.49601 loss)
I0524 00:17:34.499670 11835 sgd_solver.cpp:112] Iteration 12250, lr = 0.1
I0524 00:17:41.128304 11835 solver.cpp:239] Iteration 12260 (1.38974 iter/s, 7.19557s/10 iters), loss = 9.17892
I0524 00:17:41.128355 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.17892 (* 1 = 9.17892 loss)
I0524 00:17:41.902293 11835 sgd_solver.cpp:112] Iteration 12260, lr = 0.1
I0524 00:17:47.788121 11835 solver.cpp:239] Iteration 12270 (1.50161 iter/s, 6.65952s/10 iters), loss = 8.51122
I0524 00:17:47.788167 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.51122 (* 1 = 8.51122 loss)
I0524 00:17:47.788195 11835 sgd_solver.cpp:112] Iteration 12270, lr = 0.1
I0524 00:17:54.429570 11835 solver.cpp:239] Iteration 12280 (1.50577 iter/s, 6.64113s/10 iters), loss = 8.12226
I0524 00:17:54.429625 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.12226 (* 1 = 8.12226 loss)
I0524 00:17:54.429728 11835 sgd_solver.cpp:112] Iteration 12280, lr = 0.1
I0524 00:18:01.627652 11835 solver.cpp:239] Iteration 12290 (1.38932 iter/s, 7.19776s/10 iters), loss = 7.65043
I0524 00:18:01.627704 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.65043 (* 1 = 7.65043 loss)
I0524 00:18:01.627934 11835 sgd_solver.cpp:112] Iteration 12290, lr = 0.1
I0524 00:18:09.093569 11835 solver.cpp:239] Iteration 12300 (1.33948 iter/s, 7.46558s/10 iters), loss = 9.12986
I0524 00:18:09.093672 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.12986 (* 1 = 9.12986 loss)
I0524 00:18:10.199571 11835 sgd_solver.cpp:112] Iteration 12300, lr = 0.1
I0524 00:18:16.483395 11835 solver.cpp:239] Iteration 12310 (1.35328 iter/s, 7.38945s/10 iters), loss = 7.54082
I0524 00:18:16.483439 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.54082 (* 1 = 7.54082 loss)
I0524 00:18:16.553151 11835 sgd_solver.cpp:112] Iteration 12310, lr = 0.1
I0524 00:18:22.491253 11835 solver.cpp:239] Iteration 12320 (1.66457 iter/s, 6.00757s/10 iters), loss = 9.15988
I0524 00:18:22.491313 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.15988 (* 1 = 9.15988 loss)
I0524 00:18:22.491791 11835 sgd_solver.cpp:112] Iteration 12320, lr = 0.1
I0524 00:18:30.041343 11835 solver.cpp:239] Iteration 12330 (1.32455 iter/s, 7.54976s/10 iters), loss = 8.04638
I0524 00:18:30.041383 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.04638 (* 1 = 8.04638 loss)
I0524 00:18:30.704676 11835 sgd_solver.cpp:112] Iteration 12330, lr = 0.1
I0524 00:18:40.809295 11835 solver.cpp:239] Iteration 12340 (0.928721 iter/s, 10.7675s/10 iters), loss = 8.56896
I0524 00:18:40.809598 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.56896 (* 1 = 8.56896 loss)
I0524 00:18:42.083736 11835 sgd_solver.cpp:112] Iteration 12340, lr = 0.1
I0524 00:18:48.356132 11835 solver.cpp:239] Iteration 12350 (1.32515 iter/s, 7.54629s/10 iters), loss = 8.94909
I0524 00:18:48.356184 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.94909 (* 1 = 8.94909 loss)
I0524 00:18:48.356287 11835 sgd_solver.cpp:112] Iteration 12350, lr = 0.1
I0524 00:18:53.976523 11835 solver.cpp:239] Iteration 12360 (1.77932 iter/s, 5.62013s/10 iters), loss = 7.69933
I0524 00:18:53.976559 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.69933 (* 1 = 7.69933 loss)
I0524 00:18:53.976687 11835 sgd_solver.cpp:112] Iteration 12360, lr = 0.1
I0524 00:18:59.869006 11835 solver.cpp:239] Iteration 12370 (1.69716 iter/s, 5.89221s/10 iters), loss = 8.50651
I0524 00:18:59.869063 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.50651 (* 1 = 8.50651 loss)
I0524 00:18:59.879657 11835 sgd_solver.cpp:112] Iteration 12370, lr = 0.1
I0524 00:19:06.300065 11835 solver.cpp:239] Iteration 12380 (1.55503 iter/s, 6.43074s/10 iters), loss = 8.02502
I0524 00:19:06.300122 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.02502 (* 1 = 8.02502 loss)
I0524 00:19:06.300180 11835 sgd_solver.cpp:112] Iteration 12380, lr = 0.1
I0524 00:19:13.906965 11835 solver.cpp:239] Iteration 12390 (1.31465 iter/s, 7.60657s/10 iters), loss = 8.39706
I0524 00:19:13.907101 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.39706 (* 1 = 8.39706 loss)
I0524 00:19:14.056457 11835 sgd_solver.cpp:112] Iteration 12390, lr = 0.1
I0524 00:19:21.184329 11835 solver.cpp:239] Iteration 12400 (1.3742 iter/s, 7.27694s/10 iters), loss = 7.40763
I0524 00:19:21.184382 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.40763 (* 1 = 7.40763 loss)
I0524 00:19:21.184500 11835 sgd_solver.cpp:112] Iteration 12400, lr = 0.1
I0524 00:19:27.305668 11835 solver.cpp:239] Iteration 12410 (1.6337 iter/s, 6.12106s/10 iters), loss = 7.91985
I0524 00:19:27.305716 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.91985 (* 1 = 7.91985 loss)
I0524 00:19:27.306227 11835 sgd_solver.cpp:112] Iteration 12410, lr = 0.1
I0524 00:19:38.254021 11835 solver.cpp:239] Iteration 12420 (0.913418 iter/s, 10.9479s/10 iters), loss = 8.08742
I0524 00:19:38.254077 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.08742 (* 1 = 8.08742 loss)
I0524 00:19:38.254246 11835 sgd_solver.cpp:112] Iteration 12420, lr = 0.1
I0524 00:19:45.383394 11835 solver.cpp:239] Iteration 12430 (1.40271 iter/s, 7.12905s/10 iters), loss = 7.66973
I0524 00:19:45.383489 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.66973 (* 1 = 7.66973 loss)
I0524 00:19:45.816586 11835 sgd_solver.cpp:112] Iteration 12430, lr = 0.1
I0524 00:19:52.966557 11835 solver.cpp:239] Iteration 12440 (1.31878 iter/s, 7.58277s/10 iters), loss = 8.57913
I0524 00:19:52.966604 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.57913 (* 1 = 8.57913 loss)
I0524 00:19:52.966619 11835 sgd_solver.cpp:112] Iteration 12440, lr = 0.1
I0524 00:19:58.538218 11835 solver.cpp:239] Iteration 12450 (1.79508 iter/s, 5.57077s/10 iters), loss = 8.95026
I0524 00:19:58.538265 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.95026 (* 1 = 8.95026 loss)
I0524 00:19:58.538394 11835 sgd_solver.cpp:112] Iteration 12450, lr = 0.1
I0524 00:20:04.649410 11835 solver.cpp:239] Iteration 12460 (1.63642 iter/s, 6.11092s/10 iters), loss = 7.75958
I0524 00:20:04.649451 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.75958 (* 1 = 7.75958 loss)
I0524 00:20:06.258728 11835 sgd_solver.cpp:112] Iteration 12460, lr = 0.1
I0524 00:20:12.498961 11835 solver.cpp:239] Iteration 12470 (1.27402 iter/s, 7.8492s/10 iters), loss = 8.63252
I0524 00:20:12.499020 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.63252 (* 1 = 8.63252 loss)
I0524 00:20:12.499038 11835 sgd_solver.cpp:112] Iteration 12470, lr = 0.1
I0524 00:20:18.823431 11835 solver.cpp:239] Iteration 12480 (1.58129 iter/s, 6.32396s/10 iters), loss = 7.51875
I0524 00:20:18.823709 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.51875 (* 1 = 7.51875 loss)
I0524 00:20:18.823818 11835 sgd_solver.cpp:112] Iteration 12480, lr = 0.1
I0524 00:20:25.112311 11835 solver.cpp:239] Iteration 12490 (1.59023 iter/s, 6.28839s/10 iters), loss = 7.11574
I0524 00:20:25.112363 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11574 (* 1 = 7.11574 loss)
I0524 00:20:25.112378 11835 sgd_solver.cpp:112] Iteration 12490, lr = 0.1
I0524 00:20:30.675357 11835 solver.cpp:239] Iteration 12500 (1.79766 iter/s, 5.56279s/10 iters), loss = 8.719
I0524 00:20:30.675405 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.719 (* 1 = 8.719 loss)
I0524 00:20:30.675781 11835 sgd_solver.cpp:112] Iteration 12500, lr = 0.1
I0524 00:20:36.775416 11835 solver.cpp:239] Iteration 12510 (1.6394 iter/s, 6.09978s/10 iters), loss = 8.06825
I0524 00:20:36.775467 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.06825 (* 1 = 8.06825 loss)
I0524 00:20:36.775599 11835 sgd_solver.cpp:112] Iteration 12510, lr = 0.1
I0524 00:20:42.258280 11835 solver.cpp:239] Iteration 12520 (1.82395 iter/s, 5.48261s/10 iters), loss = 7.65605
I0524 00:20:42.258312 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.65605 (* 1 = 7.65605 loss)
I0524 00:20:42.258553 11835 sgd_solver.cpp:112] Iteration 12520, lr = 0.1
I0524 00:20:49.155242 11835 solver.cpp:239] Iteration 12530 (1.44998 iter/s, 6.89666s/10 iters), loss = 8.03356
I0524 00:20:49.155510 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.03356 (* 1 = 8.03356 loss)
I0524 00:20:49.155557 11835 sgd_solver.cpp:112] Iteration 12530, lr = 0.1
I0524 00:20:57.947010 11835 solver.cpp:239] Iteration 12540 (1.1375 iter/s, 8.79119s/10 iters), loss = 8.50544
I0524 00:20:57.947063 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.50544 (* 1 = 8.50544 loss)
I0524 00:20:57.947144 11835 sgd_solver.cpp:112] Iteration 12540, lr = 0.1
I0524 00:21:05.126024 11835 solver.cpp:239] Iteration 12550 (1.39301 iter/s, 7.17869s/10 iters), loss = 8.15821
I0524 00:21:05.126080 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.15821 (* 1 = 8.15821 loss)
I0524 00:21:05.126309 11835 sgd_solver.cpp:112] Iteration 12550, lr = 0.1
I0524 00:21:12.933663 11835 solver.cpp:239] Iteration 12560 (1.28085 iter/s, 7.8073s/10 iters), loss = 8.34782
I0524 00:21:12.933708 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.34782 (* 1 = 8.34782 loss)
I0524 00:21:12.933959 11835 sgd_solver.cpp:112] Iteration 12560, lr = 0.1
I0524 00:21:19.722865 11835 solver.cpp:239] Iteration 12570 (1.473 iter/s, 6.78889s/10 iters), loss = 7.15187
I0524 00:21:19.723119 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.15187 (* 1 = 7.15187 loss)
I0524 00:21:19.723176 11835 sgd_solver.cpp:112] Iteration 12570, lr = 0.1
I0524 00:21:25.487565 11835 solver.cpp:239] Iteration 12580 (1.73492 iter/s, 5.76396s/10 iters), loss = 8.51153
I0524 00:21:25.487606 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.51153 (* 1 = 8.51153 loss)
I0524 00:21:25.487747 11835 sgd_solver.cpp:112] Iteration 12580, lr = 0.1
I0524 00:21:32.084699 11835 solver.cpp:239] Iteration 12590 (1.51588 iter/s, 6.59684s/10 iters), loss = 7.02468
I0524 00:21:32.084750 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.02468 (* 1 = 7.02468 loss)
I0524 00:21:32.084861 11835 sgd_solver.cpp:112] Iteration 12590, lr = 0.1
I0524 00:21:40.719575 11835 solver.cpp:239] Iteration 12600 (1.15814 iter/s, 8.6345s/10 iters), loss = 7.88119
I0524 00:21:40.719624 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.88119 (* 1 = 7.88119 loss)
I0524 00:21:40.719837 11835 sgd_solver.cpp:112] Iteration 12600, lr = 0.1
I0524 00:21:47.769461 11835 solver.cpp:239] Iteration 12610 (1.41853 iter/s, 7.04957s/10 iters), loss = 9.20715
I0524 00:21:47.769523 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.20715 (* 1 = 9.20715 loss)
I0524 00:21:47.769805 11835 sgd_solver.cpp:112] Iteration 12610, lr = 0.1
I0524 00:21:54.361260 11835 solver.cpp:239] Iteration 12620 (1.51711 iter/s, 6.59148s/10 iters), loss = 8.32121
I0524 00:21:54.361549 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.32121 (* 1 = 8.32121 loss)
I0524 00:21:54.956590 11835 sgd_solver.cpp:112] Iteration 12620, lr = 0.1
I0524 00:22:02.219166 11835 solver.cpp:239] Iteration 12630 (1.27269 iter/s, 7.85735s/10 iters), loss = 7.68743
I0524 00:22:02.219224 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.68743 (* 1 = 7.68743 loss)
I0524 00:22:02.219482 11835 sgd_solver.cpp:112] Iteration 12630, lr = 0.1
I0524 00:22:08.100172 11835 solver.cpp:239] Iteration 12640 (1.70047 iter/s, 5.88073s/10 iters), loss = 7.15923
I0524 00:22:08.100219 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.15923 (* 1 = 7.15923 loss)
I0524 00:22:08.100380 11835 sgd_solver.cpp:112] Iteration 12640, lr = 0.1
I0524 00:22:14.160334 11835 solver.cpp:239] Iteration 12650 (1.6502 iter/s, 6.05988s/10 iters), loss = 8.17564
I0524 00:22:14.160388 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.17564 (* 1 = 8.17564 loss)
I0524 00:22:14.160406 11835 sgd_solver.cpp:112] Iteration 12650, lr = 0.1
I0524 00:22:20.619972 11835 solver.cpp:239] Iteration 12660 (1.54816 iter/s, 6.45927s/10 iters), loss = 7.60036
I0524 00:22:20.620028 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.60036 (* 1 = 7.60036 loss)
I0524 00:22:20.620265 11835 sgd_solver.cpp:112] Iteration 12660, lr = 0.1
I0524 00:22:27.030916 11835 solver.cpp:239] Iteration 12670 (1.5599 iter/s, 6.41065s/10 iters), loss = 8.22206
I0524 00:22:27.031132 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.22206 (* 1 = 8.22206 loss)
I0524 00:22:27.031184 11835 sgd_solver.cpp:112] Iteration 12670, lr = 0.1
I0524 00:22:32.851330 11835 solver.cpp:239] Iteration 12680 (1.71854 iter/s, 5.81891s/10 iters), loss = 7.87273
I0524 00:22:32.851377 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.87273 (* 1 = 7.87273 loss)
I0524 00:22:32.851392 11835 sgd_solver.cpp:112] Iteration 12680, lr = 0.1
I0524 00:22:39.333060 11835 solver.cpp:239] Iteration 12690 (1.54287 iter/s, 6.48143s/10 iters), loss = 8.51396
I0524 00:22:39.333120 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.51396 (* 1 = 8.51396 loss)
I0524 00:22:39.333318 11835 sgd_solver.cpp:112] Iteration 12690, lr = 0.1
I0524 00:22:46.445344 11835 solver.cpp:239] Iteration 12700 (1.40608 iter/s, 7.11197s/10 iters), loss = 7.9276
I0524 00:22:46.445384 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.9276 (* 1 = 7.9276 loss)
I0524 00:22:47.099288 11835 sgd_solver.cpp:112] Iteration 12700, lr = 0.1
I0524 00:22:55.584925 11835 solver.cpp:239] Iteration 12710 (1.09419 iter/s, 9.13919s/10 iters), loss = 8.6985
I0524 00:22:55.584983 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.6985 (* 1 = 8.6985 loss)
I0524 00:22:55.585216 11835 sgd_solver.cpp:112] Iteration 12710, lr = 0.1
I0524 00:23:03.594501 11835 solver.cpp:239] Iteration 12720 (1.24856 iter/s, 8.00921s/10 iters), loss = 8.37068
I0524 00:23:03.594631 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.37068 (* 1 = 8.37068 loss)
I0524 00:23:03.595062 11835 sgd_solver.cpp:112] Iteration 12720, lr = 0.1
I0524 00:23:12.536048 11835 solver.cpp:239] Iteration 12730 (1.11843 iter/s, 8.94109s/10 iters), loss = 8.51414
I0524 00:23:12.536094 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.51414 (* 1 = 8.51414 loss)
I0524 00:23:12.536516 11835 sgd_solver.cpp:112] Iteration 12730, lr = 0.1
I0524 00:23:18.396886 11835 solver.cpp:239] Iteration 12740 (1.70632 iter/s, 5.86057s/10 iters), loss = 8.60429
I0524 00:23:18.396934 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.60429 (* 1 = 8.60429 loss)
I0524 00:23:18.396947 11835 sgd_solver.cpp:112] Iteration 12740, lr = 0.1
I0524 00:23:24.417738 11835 solver.cpp:239] Iteration 12750 (1.66097 iter/s, 6.02058s/10 iters), loss = 8.49535
I0524 00:23:24.417784 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.49535 (* 1 = 8.49535 loss)
I0524 00:23:24.417991 11835 sgd_solver.cpp:112] Iteration 12750, lr = 0.1
I0524 00:23:31.086771 11835 solver.cpp:239] Iteration 12760 (1.49954 iter/s, 6.66872s/10 iters), loss = 8.09127
I0524 00:23:31.086828 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.09127 (* 1 = 8.09127 loss)
I0524 00:23:31.087070 11835 sgd_solver.cpp:112] Iteration 12760, lr = 0.1
I0524 00:23:37.815824 11835 solver.cpp:239] Iteration 12770 (1.48616 iter/s, 6.72875s/10 iters), loss = 8.95875
I0524 00:23:37.816107 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.95875 (* 1 = 8.95875 loss)
I0524 00:23:37.816149 11835 sgd_solver.cpp:112] Iteration 12770, lr = 0.1
I0524 00:23:44.367717 11835 solver.cpp:239] Iteration 12780 (1.52639 iter/s, 6.5514s/10 iters), loss = 8.49176
I0524 00:23:44.367770 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.49176 (* 1 = 8.49176 loss)
I0524 00:23:44.367784 11835 sgd_solver.cpp:112] Iteration 12780, lr = 0.1
I0524 00:23:52.114711 11835 solver.cpp:239] Iteration 12790 (1.2909 iter/s, 7.74656s/10 iters), loss = 7.77912
I0524 00:23:52.114750 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.77912 (* 1 = 7.77912 loss)
I0524 00:23:52.463532 11835 sgd_solver.cpp:112] Iteration 12790, lr = 0.1
I0524 00:23:58.776676 11835 solver.cpp:239] Iteration 12800 (1.50113 iter/s, 6.66167s/10 iters), loss = 8.04881
I0524 00:23:58.776729 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.04881 (* 1 = 8.04881 loss)
I0524 00:23:58.776743 11835 sgd_solver.cpp:112] Iteration 12800, lr = 0.1
I0524 00:24:05.375944 11835 solver.cpp:239] Iteration 12810 (1.51543 iter/s, 6.59878s/10 iters), loss = 8.82006
I0524 00:24:05.375982 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.82006 (* 1 = 8.82006 loss)
I0524 00:24:05.538202 11835 sgd_solver.cpp:112] Iteration 12810, lr = 0.1
I0524 00:24:12.282516 11835 solver.cpp:239] Iteration 12820 (1.44796 iter/s, 6.90626s/10 iters), loss = 8.83293
I0524 00:24:12.282660 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.83293 (* 1 = 8.83293 loss)
I0524 00:24:12.282714 11835 sgd_solver.cpp:112] Iteration 12820, lr = 0.1
I0524 00:24:18.842214 11835 solver.cpp:239] Iteration 12830 (1.52455 iter/s, 6.55931s/10 iters), loss = 6.97291
I0524 00:24:18.842263 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.97291 (* 1 = 6.97291 loss)
I0524 00:24:18.842278 11835 sgd_solver.cpp:112] Iteration 12830, lr = 0.1
I0524 00:24:26.847437 11835 solver.cpp:239] Iteration 12840 (1.24926 iter/s, 8.00477s/10 iters), loss = 7.95873
I0524 00:24:26.847492 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.95873 (* 1 = 7.95873 loss)
I0524 00:24:26.847726 11835 sgd_solver.cpp:112] Iteration 12840, lr = 0.1
I0524 00:24:32.846900 11835 solver.cpp:239] Iteration 12850 (1.6669 iter/s, 5.99917s/10 iters), loss = 7.43896
I0524 00:24:32.846951 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.43896 (* 1 = 7.43896 loss)
I0524 00:24:32.847316 11835 sgd_solver.cpp:112] Iteration 12850, lr = 0.1
I0524 00:24:39.161931 11835 solver.cpp:239] Iteration 12860 (1.5836 iter/s, 6.31474s/10 iters), loss = 7.61926
I0524 00:24:39.161979 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.61926 (* 1 = 7.61926 loss)
I0524 00:24:39.162240 11835 sgd_solver.cpp:112] Iteration 12860, lr = 0.1
I0524 00:24:48.174751 11835 solver.cpp:239] Iteration 12870 (1.10958 iter/s, 9.01244s/10 iters), loss = 8.42482
I0524 00:24:48.174981 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.42482 (* 1 = 8.42482 loss)
I0524 00:24:48.803329 11835 sgd_solver.cpp:112] Iteration 12870, lr = 0.1
I0524 00:24:57.041689 11835 solver.cpp:239] Iteration 12880 (1.12785 iter/s, 8.86642s/10 iters), loss = 7.96253
I0524 00:24:57.041733 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.96253 (* 1 = 7.96253 loss)
I0524 00:24:57.145720 11835 sgd_solver.cpp:112] Iteration 12880, lr = 0.1
I0524 00:25:06.120769 11835 solver.cpp:239] Iteration 12890 (1.10148 iter/s, 9.07869s/10 iters), loss = 7.5196
I0524 00:25:06.120822 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.5196 (* 1 = 7.5196 loss)
I0524 00:25:06.121045 11835 sgd_solver.cpp:112] Iteration 12890, lr = 0.1
I0524 00:25:13.263543 11835 solver.cpp:239] Iteration 12900 (1.40008 iter/s, 7.14244s/10 iters), loss = 8.15892
I0524 00:25:13.263605 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.15892 (* 1 = 8.15892 loss)
I0524 00:25:13.280215 11835 sgd_solver.cpp:112] Iteration 12900, lr = 0.1
I0524 00:25:19.520659 11835 solver.cpp:239] Iteration 12910 (1.59826 iter/s, 6.2568s/10 iters), loss = 7.52223
I0524 00:25:19.520768 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.52223 (* 1 = 7.52223 loss)
I0524 00:25:20.582573 11835 sgd_solver.cpp:112] Iteration 12910, lr = 0.1
I0524 00:25:27.325124 11835 solver.cpp:239] Iteration 12920 (1.28138 iter/s, 7.80407s/10 iters), loss = 7.54012
I0524 00:25:27.325160 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.54012 (* 1 = 7.54012 loss)
I0524 00:25:27.325289 11835 sgd_solver.cpp:112] Iteration 12920, lr = 0.1
I0524 00:25:34.000766 11835 solver.cpp:239] Iteration 12930 (1.49805 iter/s, 6.67534s/10 iters), loss = 7.79154
I0524 00:25:34.000818 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.79154 (* 1 = 7.79154 loss)
I0524 00:25:34.001003 11835 sgd_solver.cpp:112] Iteration 12930, lr = 0.1
I0524 00:25:40.299211 11835 solver.cpp:239] Iteration 12940 (1.58777 iter/s, 6.29816s/10 iters), loss = 7.95654
I0524 00:25:40.299257 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.95654 (* 1 = 7.95654 loss)
I0524 00:25:40.299381 11835 sgd_solver.cpp:112] Iteration 12940, lr = 0.1
I0524 00:25:46.975396 11835 solver.cpp:239] Iteration 12950 (1.49793 iter/s, 6.67588s/10 iters), loss = 8.1752
I0524 00:25:46.975445 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.1752 (* 1 = 8.1752 loss)
I0524 00:25:46.975509 11835 sgd_solver.cpp:112] Iteration 12950, lr = 0.1
I0524 00:25:52.627254 11835 solver.cpp:239] Iteration 12960 (1.76941 iter/s, 5.65159s/10 iters), loss = 8.59968
I0524 00:25:52.627406 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.59968 (* 1 = 8.59968 loss)
I0524 00:25:52.627533 11835 sgd_solver.cpp:112] Iteration 12960, lr = 0.1
I0524 00:26:01.441818 11835 solver.cpp:239] Iteration 12970 (1.13455 iter/s, 8.81408s/10 iters), loss = 8.88446
I0524 00:26:01.441880 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.88446 (* 1 = 8.88446 loss)
I0524 00:26:01.442320 11835 sgd_solver.cpp:112] Iteration 12970, lr = 0.1
I0524 00:26:09.228935 11835 solver.cpp:239] Iteration 12980 (1.28423 iter/s, 7.78677s/10 iters), loss = 6.99927
I0524 00:26:09.228983 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.99927 (* 1 = 6.99927 loss)
I0524 00:26:09.233336 11835 sgd_solver.cpp:112] Iteration 12980, lr = 0.1
I0524 00:26:15.537832 11835 solver.cpp:239] Iteration 12990 (1.58514 iter/s, 6.30861s/10 iters), loss = 7.71435
I0524 00:26:15.537875 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.71435 (* 1 = 7.71435 loss)
I0524 00:26:15.537994 11835 sgd_solver.cpp:112] Iteration 12990, lr = 0.1
I0524 00:26:25.346650 11835 solver.cpp:239] Iteration 13000 (1.01953 iter/s, 9.80842s/10 iters), loss = 8.45984
I0524 00:26:25.346801 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.45984 (* 1 = 8.45984 loss)
I0524 00:26:25.363956 11835 sgd_solver.cpp:112] Iteration 13000, lr = 0.1
I0524 00:26:31.560612 11835 solver.cpp:239] Iteration 13010 (1.60938 iter/s, 6.21357s/10 iters), loss = 7.84903
I0524 00:26:31.560660 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.84903 (* 1 = 7.84903 loss)
I0524 00:26:31.593015 11835 sgd_solver.cpp:112] Iteration 13010, lr = 0.1
I0524 00:26:38.211408 11835 solver.cpp:239] Iteration 13020 (1.50365 iter/s, 6.6505s/10 iters), loss = 8.58879
I0524 00:26:38.211457 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.58879 (* 1 = 8.58879 loss)
I0524 00:26:38.211670 11835 sgd_solver.cpp:112] Iteration 13020, lr = 0.1
I0524 00:26:45.154351 11835 solver.cpp:239] Iteration 13030 (1.44038 iter/s, 6.94263s/10 iters), loss = 7.61598
I0524 00:26:45.154398 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.61598 (* 1 = 7.61598 loss)
I0524 00:26:46.251705 11835 sgd_solver.cpp:112] Iteration 13030, lr = 0.1
I0524 00:26:54.032196 11835 solver.cpp:239] Iteration 13040 (1.12645 iter/s, 8.87747s/10 iters), loss = 8.00707
I0524 00:26:54.032244 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.00707 (* 1 = 8.00707 loss)
I0524 00:26:54.032492 11835 sgd_solver.cpp:112] Iteration 13040, lr = 0.1
I0524 00:26:59.916182 11835 solver.cpp:239] Iteration 13050 (1.69961 iter/s, 5.88371s/10 iters), loss = 7.47698
I0524 00:26:59.916422 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.47698 (* 1 = 7.47698 loss)
I0524 00:26:59.916486 11835 sgd_solver.cpp:112] Iteration 13050, lr = 0.1
I0524 00:27:05.624361 11835 solver.cpp:239] Iteration 13060 (1.75266 iter/s, 5.70561s/10 iters), loss = 7.75625
I0524 00:27:05.624409 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.75625 (* 1 = 7.75625 loss)
I0524 00:27:05.624698 11835 sgd_solver.cpp:112] Iteration 13060, lr = 0.1
I0524 00:27:13.739926 11835 solver.cpp:239] Iteration 13070 (1.23225 iter/s, 8.11521s/10 iters), loss = 8.06297
I0524 00:27:13.739979 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.06297 (* 1 = 8.06297 loss)
I0524 00:27:13.740105 11835 sgd_solver.cpp:112] Iteration 13070, lr = 0.1
I0524 00:27:19.497510 11835 solver.cpp:239] Iteration 13080 (1.73692 iter/s, 5.75732s/10 iters), loss = 7.14679
I0524 00:27:19.497555 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.14679 (* 1 = 7.14679 loss)
I0524 00:27:19.497570 11835 sgd_solver.cpp:112] Iteration 13080, lr = 0.1
I0524 00:27:25.948918 11835 solver.cpp:239] Iteration 13090 (1.55012 iter/s, 6.45113s/10 iters), loss = 8.00948
I0524 00:27:25.948961 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.00948 (* 1 = 8.00948 loss)
I0524 00:27:25.948973 11835 sgd_solver.cpp:112] Iteration 13090, lr = 0.1
I0524 00:27:32.668161 11835 solver.cpp:239] Iteration 13100 (1.48837 iter/s, 6.71875s/10 iters), loss = 9.02645
I0524 00:27:32.668408 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.02645 (* 1 = 9.02645 loss)
I0524 00:27:32.668627 11835 sgd_solver.cpp:112] Iteration 13100, lr = 0.1
I0524 00:27:41.543951 11835 solver.cpp:239] Iteration 13110 (1.12673 iter/s, 8.87524s/10 iters), loss = 8.31233
I0524 00:27:41.544005 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.31233 (* 1 = 8.31233 loss)
I0524 00:27:42.887946 11835 sgd_solver.cpp:112] Iteration 13110, lr = 0.1
I0524 00:27:53.720180 11835 solver.cpp:239] Iteration 13120 (0.821307 iter/s, 12.1757s/10 iters), loss = 8.13795
I0524 00:27:53.720250 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.13795 (* 1 = 8.13795 loss)
I0524 00:27:53.996747 11835 sgd_solver.cpp:112] Iteration 13120, lr = 0.1
I0524 00:28:00.543787 11835 solver.cpp:239] Iteration 13130 (1.46557 iter/s, 6.82329s/10 iters), loss = 7.28626
I0524 00:28:00.543833 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.28626 (* 1 = 7.28626 loss)
I0524 00:28:00.543998 11835 sgd_solver.cpp:112] Iteration 13130, lr = 0.1
I0524 00:28:08.376057 11835 solver.cpp:239] Iteration 13140 (1.27683 iter/s, 7.83192s/10 iters), loss = 8.99421
I0524 00:28:08.376237 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.99421 (* 1 = 8.99421 loss)
I0524 00:28:08.399076 11835 sgd_solver.cpp:112] Iteration 13140, lr = 0.1
I0524 00:28:14.744308 11835 solver.cpp:239] Iteration 13150 (1.57039 iter/s, 6.36783s/10 iters), loss = 7.7221
I0524 00:28:14.744357 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.7221 (* 1 = 7.7221 loss)
I0524 00:28:14.744490 11835 sgd_solver.cpp:112] Iteration 13150, lr = 0.1
I0524 00:28:20.381449 11835 solver.cpp:239] Iteration 13160 (1.77403 iter/s, 5.63688s/10 iters), loss = 7.62661
I0524 00:28:20.381490 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.62661 (* 1 = 7.62661 loss)
I0524 00:28:20.381800 11835 sgd_solver.cpp:112] Iteration 13160, lr = 0.1
I0524 00:28:27.002578 11835 solver.cpp:239] Iteration 13170 (1.51038 iter/s, 6.62083s/10 iters), loss = 8.72761
I0524 00:28:27.002626 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.72761 (* 1 = 8.72761 loss)
I0524 00:28:27.501220 11835 sgd_solver.cpp:112] Iteration 13170, lr = 0.1
I0524 00:28:34.534626 11835 solver.cpp:239] Iteration 13180 (1.32772 iter/s, 7.53172s/10 iters), loss = 7.36196
I0524 00:28:34.534672 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.36196 (* 1 = 7.36196 loss)
I0524 00:28:34.534943 11835 sgd_solver.cpp:112] Iteration 13180, lr = 0.1
I0524 00:28:41.143580 11835 solver.cpp:239] Iteration 13190 (1.51317 iter/s, 6.60865s/10 iters), loss = 8.05511
I0524 00:28:41.143858 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.05511 (* 1 = 8.05511 loss)
I0524 00:28:42.470293 11835 sgd_solver.cpp:112] Iteration 13190, lr = 0.1
I0524 00:28:48.248621 11835 solver.cpp:239] Iteration 13200 (1.40755 iter/s, 7.10455s/10 iters), loss = 7.78849
I0524 00:28:48.248667 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.78849 (* 1 = 7.78849 loss)
I0524 00:28:48.249044 11835 sgd_solver.cpp:112] Iteration 13200, lr = 0.1
I0524 00:28:57.783274 11835 solver.cpp:239] Iteration 13210 (1.04885 iter/s, 9.53425s/10 iters), loss = 7.98698
I0524 00:28:57.783330 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.98698 (* 1 = 7.98698 loss)
I0524 00:28:57.982655 11835 sgd_solver.cpp:112] Iteration 13210, lr = 0.1
I0524 00:29:04.401327 11835 solver.cpp:239] Iteration 13220 (1.51109 iter/s, 6.61775s/10 iters), loss = 7.95109
I0524 00:29:04.401372 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.95109 (* 1 = 7.95109 loss)
I0524 00:29:04.401463 11835 sgd_solver.cpp:112] Iteration 13220, lr = 0.1
I0524 00:29:10.914777 11835 solver.cpp:239] Iteration 13230 (1.53535 iter/s, 6.51316s/10 iters), loss = 7.66382
I0524 00:29:10.914824 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.66382 (* 1 = 7.66382 loss)
I0524 00:29:10.914836 11835 sgd_solver.cpp:112] Iteration 13230, lr = 0.1
I0524 00:29:18.084645 11835 solver.cpp:239] Iteration 13240 (1.39483 iter/s, 7.16935s/10 iters), loss = 7.05357
I0524 00:29:18.084885 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.05357 (* 1 = 7.05357 loss)
I0524 00:29:18.084928 11835 sgd_solver.cpp:112] Iteration 13240, lr = 0.1
I0524 00:29:27.839699 11835 solver.cpp:239] Iteration 13250 (1.02525 iter/s, 9.75371s/10 iters), loss = 8.16987
I0524 00:29:27.839756 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.16987 (* 1 = 8.16987 loss)
I0524 00:29:27.839965 11835 sgd_solver.cpp:112] Iteration 13250, lr = 0.1
I0524 00:29:33.614133 11835 solver.cpp:239] Iteration 13260 (1.73185 iter/s, 5.77417s/10 iters), loss = 8.23561
I0524 00:29:33.614178 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.23561 (* 1 = 8.23561 loss)
I0524 00:29:33.614423 11835 sgd_solver.cpp:112] Iteration 13260, lr = 0.1
I0524 00:29:41.735237 11835 solver.cpp:239] Iteration 13270 (1.23141 iter/s, 8.12074s/10 iters), loss = 7.26976
I0524 00:29:41.735291 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.26976 (* 1 = 7.26976 loss)
I0524 00:29:41.735574 11835 sgd_solver.cpp:112] Iteration 13270, lr = 0.1
I0524 00:29:47.739094 11835 solver.cpp:239] Iteration 13280 (1.66567 iter/s, 6.00358s/10 iters), loss = 7.76343
I0524 00:29:47.739140 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.76343 (* 1 = 7.76343 loss)
I0524 00:29:47.739413 11835 sgd_solver.cpp:112] Iteration 13280, lr = 0.1
I0524 00:29:54.729941 11835 solver.cpp:239] Iteration 13290 (1.43051 iter/s, 6.99052s/10 iters), loss = 8.06142
I0524 00:29:54.730231 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.06142 (* 1 = 8.06142 loss)
I0524 00:29:55.336428 11835 sgd_solver.cpp:112] Iteration 13290, lr = 0.1
I0524 00:30:01.825976 11835 solver.cpp:239] Iteration 13300 (1.40934 iter/s, 7.09552s/10 iters), loss = 7.2903
I0524 00:30:01.826026 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2903 (* 1 = 7.2903 loss)
I0524 00:30:01.826161 11835 sgd_solver.cpp:112] Iteration 13300, lr = 0.1
I0524 00:30:07.723769 11835 solver.cpp:239] Iteration 13310 (1.69563 iter/s, 5.89752s/10 iters), loss = 7.77028
I0524 00:30:07.723817 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.77028 (* 1 = 7.77028 loss)
I0524 00:30:07.723925 11835 sgd_solver.cpp:112] Iteration 13310, lr = 0.1
I0524 00:30:14.477056 11835 solver.cpp:239] Iteration 13320 (1.48083 iter/s, 6.75299s/10 iters), loss = 8.28481
I0524 00:30:14.477099 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.28481 (* 1 = 8.28481 loss)
I0524 00:30:14.477205 11835 sgd_solver.cpp:112] Iteration 13320, lr = 0.1
I0524 00:30:20.735075 11835 solver.cpp:239] Iteration 13330 (1.59803 iter/s, 6.25772s/10 iters), loss = 6.81953
I0524 00:30:20.735129 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81953 (* 1 = 6.81953 loss)
I0524 00:30:20.735369 11835 sgd_solver.cpp:112] Iteration 13330, lr = 0.1
I0524 00:30:28.977080 11835 solver.cpp:239] Iteration 13340 (1.21335 iter/s, 8.24165s/10 iters), loss = 8.6171
I0524 00:30:28.977326 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.6171 (* 1 = 8.6171 loss)
I0524 00:30:29.065780 11835 sgd_solver.cpp:112] Iteration 13340, lr = 0.1
I0524 00:30:35.719830 11835 solver.cpp:239] Iteration 13350 (1.48318 iter/s, 6.74228s/10 iters), loss = 8.16855
I0524 00:30:35.719882 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.16855 (* 1 = 8.16855 loss)
I0524 00:30:35.966995 11835 sgd_solver.cpp:112] Iteration 13350, lr = 0.1
I0524 00:30:41.877389 11835 solver.cpp:239] Iteration 13360 (1.62409 iter/s, 6.15728s/10 iters), loss = 7.72939
I0524 00:30:41.877441 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.72939 (* 1 = 7.72939 loss)
I0524 00:30:41.877456 11835 sgd_solver.cpp:112] Iteration 13360, lr = 0.1
I0524 00:30:48.389681 11835 solver.cpp:239] Iteration 13370 (1.53571 iter/s, 6.51163s/10 iters), loss = 7.21368
I0524 00:30:48.389719 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.21368 (* 1 = 7.21368 loss)
I0524 00:30:48.389731 11835 sgd_solver.cpp:112] Iteration 13370, lr = 0.1
I0524 00:30:54.463470 11835 solver.cpp:239] Iteration 13380 (1.64709 iter/s, 6.07132s/10 iters), loss = 7.46261
I0524 00:30:54.463522 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.46261 (* 1 = 7.46261 loss)
I0524 00:30:54.463543 11835 sgd_solver.cpp:112] Iteration 13380, lr = 0.1
I0524 00:31:01.471215 11835 solver.cpp:239] Iteration 13390 (1.42706 iter/s, 7.00742s/10 iters), loss = 8.50804
I0524 00:31:01.471479 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.50804 (* 1 = 8.50804 loss)
I0524 00:31:01.471529 11835 sgd_solver.cpp:112] Iteration 13390, lr = 0.1
I0524 00:31:07.139806 11835 solver.cpp:239] Iteration 13400 (1.76424 iter/s, 5.66816s/10 iters), loss = 8.13993
I0524 00:31:07.139861 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.13993 (* 1 = 8.13993 loss)
I0524 00:31:07.139957 11835 sgd_solver.cpp:112] Iteration 13400, lr = 0.1
I0524 00:31:15.492226 11835 solver.cpp:239] Iteration 13410 (1.19731 iter/s, 8.35206s/10 iters), loss = 8.22648
I0524 00:31:15.492269 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.22648 (* 1 = 8.22648 loss)
I0524 00:31:15.620025 11835 sgd_solver.cpp:112] Iteration 13410, lr = 0.1
I0524 00:31:23.072850 11835 solver.cpp:239] Iteration 13420 (1.31921 iter/s, 7.58029s/10 iters), loss = 7.63733
I0524 00:31:23.072899 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.63733 (* 1 = 7.63733 loss)
I0524 00:31:23.073029 11835 sgd_solver.cpp:112] Iteration 13420, lr = 0.1
I0524 00:31:28.847077 11835 solver.cpp:239] Iteration 13430 (1.73192 iter/s, 5.77395s/10 iters), loss = 7.37065
I0524 00:31:28.847132 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.37065 (* 1 = 7.37065 loss)
I0524 00:31:28.847518 11835 sgd_solver.cpp:112] Iteration 13430, lr = 0.1
I0524 00:31:36.700172 11835 solver.cpp:239] Iteration 13440 (1.27344 iter/s, 7.85274s/10 iters), loss = 7.90727
I0524 00:31:36.700381 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.90727 (* 1 = 7.90727 loss)
I0524 00:31:36.730689 11835 sgd_solver.cpp:112] Iteration 13440, lr = 0.1
I0524 00:31:46.747251 11835 solver.cpp:239] Iteration 13450 (0.995372 iter/s, 10.0465s/10 iters), loss = 8.956
I0524 00:31:46.747306 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.956 (* 1 = 8.956 loss)
I0524 00:31:46.751637 11835 sgd_solver.cpp:112] Iteration 13450, lr = 0.1
I0524 00:31:52.573081 11835 solver.cpp:239] Iteration 13460 (1.71657 iter/s, 5.82556s/10 iters), loss = 8.19969
I0524 00:31:52.573117 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.19969 (* 1 = 8.19969 loss)
I0524 00:31:52.573128 11835 sgd_solver.cpp:112] Iteration 13460, lr = 0.1
I0524 00:31:58.326566 11835 solver.cpp:239] Iteration 13470 (1.73816 iter/s, 5.75322s/10 iters), loss = 8.44587
I0524 00:31:58.326616 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.44587 (* 1 = 8.44587 loss)
I0524 00:31:58.729687 11835 sgd_solver.cpp:112] Iteration 13470, lr = 0.1
I0524 00:32:05.503427 11835 solver.cpp:239] Iteration 13480 (1.39343 iter/s, 7.17654s/10 iters), loss = 6.817
I0524 00:32:05.503479 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.817 (* 1 = 6.817 loss)
I0524 00:32:05.503944 11835 sgd_solver.cpp:112] Iteration 13480, lr = 0.1
I0524 00:32:11.877158 11835 solver.cpp:239] Iteration 13490 (1.56901 iter/s, 6.37345s/10 iters), loss = 7.40275
I0524 00:32:11.877426 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.40275 (* 1 = 7.40275 loss)
I0524 00:32:11.877473 11835 sgd_solver.cpp:112] Iteration 13490, lr = 0.1
I0524 00:32:18.614557 11835 solver.cpp:239] Iteration 13500 (1.48437 iter/s, 6.73687s/10 iters), loss = 7.60497
I0524 00:32:18.614608 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.60497 (* 1 = 7.60497 loss)
I0524 00:32:18.614799 11835 sgd_solver.cpp:112] Iteration 13500, lr = 0.1
I0524 00:32:26.029631 11835 solver.cpp:239] Iteration 13510 (1.34866 iter/s, 7.41474s/10 iters), loss = 7.67547
I0524 00:32:26.029681 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.67547 (* 1 = 7.67547 loss)
I0524 00:32:26.029952 11835 sgd_solver.cpp:112] Iteration 13510, lr = 0.1
I0524 00:32:32.839309 11835 solver.cpp:239] Iteration 13520 (1.46857 iter/s, 6.80937s/10 iters), loss = 8.79053
I0524 00:32:32.839365 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.79053 (* 1 = 8.79053 loss)
I0524 00:32:32.839473 11835 sgd_solver.cpp:112] Iteration 13520, lr = 0.1
I0524 00:32:40.463249 11835 solver.cpp:239] Iteration 13530 (1.31172 iter/s, 7.62359s/10 iters), loss = 7.5767
I0524 00:32:40.463305 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.5767 (* 1 = 7.5767 loss)
I0524 00:32:40.463721 11835 sgd_solver.cpp:112] Iteration 13530, lr = 0.1
I0524 00:32:49.204205 11835 solver.cpp:239] Iteration 13540 (1.14409 iter/s, 8.74057s/10 iters), loss = 7.42195
I0524 00:32:49.204483 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.42195 (* 1 = 7.42195 loss)
I0524 00:32:49.283502 11835 sgd_solver.cpp:112] Iteration 13540, lr = 0.1
I0524 00:32:57.292592 11835 solver.cpp:239] Iteration 13550 (1.23642 iter/s, 8.08785s/10 iters), loss = 7.79381
I0524 00:32:57.292647 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.79381 (* 1 = 7.79381 loss)
I0524 00:32:57.292896 11835 sgd_solver.cpp:112] Iteration 13550, lr = 0.1
I0524 00:33:04.964264 11835 solver.cpp:239] Iteration 13560 (1.30356 iter/s, 7.67133s/10 iters), loss = 8.71322
I0524 00:33:04.964316 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.71322 (* 1 = 8.71322 loss)
I0524 00:33:04.964417 11835 sgd_solver.cpp:112] Iteration 13560, lr = 0.1
I0524 00:33:12.847110 11835 solver.cpp:239] Iteration 13570 (1.26863 iter/s, 7.88249s/10 iters), loss = 7.16791
I0524 00:33:12.847170 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.16791 (* 1 = 7.16791 loss)
I0524 00:33:12.847357 11835 sgd_solver.cpp:112] Iteration 13570, lr = 0.1
I0524 00:33:20.618479 11835 solver.cpp:239] Iteration 13580 (1.28683 iter/s, 7.77103s/10 iters), loss = 8.34225
I0524 00:33:20.618805 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.34225 (* 1 = 8.34225 loss)
I0524 00:33:21.220708 11835 sgd_solver.cpp:112] Iteration 13580, lr = 0.1
I0524 00:33:28.109380 11835 solver.cpp:239] Iteration 13590 (1.33506 iter/s, 7.49032s/10 iters), loss = 8.67865
I0524 00:33:28.109439 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.67865 (* 1 = 8.67865 loss)
I0524 00:33:28.109562 11835 sgd_solver.cpp:112] Iteration 13590, lr = 0.1
I0524 00:33:35.096340 11835 solver.cpp:239] Iteration 13600 (1.4313 iter/s, 6.98664s/10 iters), loss = 8.23638
I0524 00:33:35.096392 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.23638 (* 1 = 8.23638 loss)
I0524 00:33:35.096501 11835 sgd_solver.cpp:112] Iteration 13600, lr = 0.1
I0524 00:33:43.906920 11835 solver.cpp:239] Iteration 13610 (1.13505 iter/s, 8.8102s/10 iters), loss = 8.68167
I0524 00:33:43.906970 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.68167 (* 1 = 8.68167 loss)
I0524 00:33:43.907220 11835 sgd_solver.cpp:112] Iteration 13610, lr = 0.1
I0524 00:33:49.575316 11835 solver.cpp:239] Iteration 13620 (1.76425 iter/s, 5.66813s/10 iters), loss = 7.48879
I0524 00:33:49.575363 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.48879 (* 1 = 7.48879 loss)
I0524 00:33:49.575377 11835 sgd_solver.cpp:112] Iteration 13620, lr = 0.1
I0524 00:33:56.595708 11835 solver.cpp:239] Iteration 13630 (1.42449 iter/s, 7.02007s/10 iters), loss = 8.465
I0524 00:33:56.595841 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.465 (* 1 = 8.465 loss)
I0524 00:33:56.595984 11835 sgd_solver.cpp:112] Iteration 13630, lr = 0.1
I0524 00:34:02.407528 11835 solver.cpp:239] Iteration 13640 (1.72073 iter/s, 5.81148s/10 iters), loss = 7.8677
I0524 00:34:02.407567 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.8677 (* 1 = 7.8677 loss)
I0524 00:34:02.652395 11835 sgd_solver.cpp:112] Iteration 13640, lr = 0.1
I0524 00:34:10.535326 11835 solver.cpp:239] Iteration 13650 (1.2304 iter/s, 8.12743s/10 iters), loss = 8.16865
I0524 00:34:10.535377 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.16865 (* 1 = 8.16865 loss)
I0524 00:34:10.579954 11835 sgd_solver.cpp:112] Iteration 13650, lr = 0.1
I0524 00:34:19.445683 11835 solver.cpp:239] Iteration 13660 (1.12234 iter/s, 8.90996s/10 iters), loss = 6.89568
I0524 00:34:19.445747 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89568 (* 1 = 6.89568 loss)
I0524 00:34:19.445961 11835 sgd_solver.cpp:112] Iteration 13660, lr = 0.1
I0524 00:34:27.343706 11835 solver.cpp:239] Iteration 13670 (1.2662 iter/s, 7.89767s/10 iters), loss = 7.97356
I0524 00:34:27.343991 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.97356 (* 1 = 7.97356 loss)
I0524 00:34:27.344043 11835 sgd_solver.cpp:112] Iteration 13670, lr = 0.1
I0524 00:34:33.927300 11835 solver.cpp:239] Iteration 13680 (1.51914 iter/s, 6.58268s/10 iters), loss = 8.38272
I0524 00:34:33.927350 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.38272 (* 1 = 8.38272 loss)
I0524 00:34:33.928337 11835 sgd_solver.cpp:112] Iteration 13680, lr = 0.1
I0524 00:34:40.288132 11835 solver.cpp:239] Iteration 13690 (1.5722 iter/s, 6.36053s/10 iters), loss = 8.09972
I0524 00:34:40.288189 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.09972 (* 1 = 8.09972 loss)
I0524 00:34:40.288247 11835 sgd_solver.cpp:112] Iteration 13690, lr = 0.1
I0524 00:34:46.461748 11835 solver.cpp:239] Iteration 13700 (1.61987 iter/s, 6.17333s/10 iters), loss = 7.89591
I0524 00:34:46.461792 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.89591 (* 1 = 7.89591 loss)
I0524 00:34:46.461956 11835 sgd_solver.cpp:112] Iteration 13700, lr = 0.1
I0524 00:34:52.286012 11835 solver.cpp:239] Iteration 13710 (1.71703 iter/s, 5.82399s/10 iters), loss = 7.87841
I0524 00:34:52.286062 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.87841 (* 1 = 7.87841 loss)
I0524 00:34:52.286290 11835 sgd_solver.cpp:112] Iteration 13710, lr = 0.1
I0524 00:34:58.765810 11835 solver.cpp:239] Iteration 13720 (1.54333 iter/s, 6.47951s/10 iters), loss = 8.56963
I0524 00:34:58.766044 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.56963 (* 1 = 8.56963 loss)
I0524 00:34:58.766083 11835 sgd_solver.cpp:112] Iteration 13720, lr = 0.1
I0524 00:35:05.284458 11835 solver.cpp:239] Iteration 13730 (1.5342 iter/s, 6.51805s/10 iters), loss = 7.69874
I0524 00:35:05.284509 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.69874 (* 1 = 7.69874 loss)
I0524 00:35:05.284703 11835 sgd_solver.cpp:112] Iteration 13730, lr = 0.1
I0524 00:35:12.622735 11835 solver.cpp:239] Iteration 13740 (1.36279 iter/s, 7.3379s/10 iters), loss = 7.36748
I0524 00:35:12.622788 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.36748 (* 1 = 7.36748 loss)
I0524 00:35:13.100723 11835 sgd_solver.cpp:112] Iteration 13740, lr = 0.1
I0524 00:35:21.382787 11835 solver.cpp:239] Iteration 13750 (1.1416 iter/s, 8.75967s/10 iters), loss = 7.61162
I0524 00:35:21.382839 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.61162 (* 1 = 7.61162 loss)
I0524 00:35:21.382922 11835 sgd_solver.cpp:112] Iteration 13750, lr = 0.1
I0524 00:35:27.908414 11835 solver.cpp:239] Iteration 13760 (1.53249 iter/s, 6.52533s/10 iters), loss = 8.47986
I0524 00:35:27.908464 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.47986 (* 1 = 8.47986 loss)
I0524 00:35:27.908768 11835 sgd_solver.cpp:112] Iteration 13760, lr = 0.1
I0524 00:35:37.345417 11835 solver.cpp:239] Iteration 13770 (1.0597 iter/s, 9.4366s/10 iters), loss = 8.16004
I0524 00:35:37.345654 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.16004 (* 1 = 8.16004 loss)
I0524 00:35:37.345696 11835 sgd_solver.cpp:112] Iteration 13770, lr = 0.1
I0524 00:35:44.657430 11835 solver.cpp:239] Iteration 13780 (1.36811 iter/s, 7.30937s/10 iters), loss = 8.23956
I0524 00:35:44.657472 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.23956 (* 1 = 8.23956 loss)
I0524 00:35:44.657485 11835 sgd_solver.cpp:112] Iteration 13780, lr = 0.1
I0524 00:35:51.094899 11835 solver.cpp:239] Iteration 13790 (1.55354 iter/s, 6.43693s/10 iters), loss = 8.08978
I0524 00:35:51.094962 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.08978 (* 1 = 8.08978 loss)
I0524 00:35:51.095085 11835 sgd_solver.cpp:112] Iteration 13790, lr = 0.1
I0524 00:35:57.096652 11835 solver.cpp:239] Iteration 13800 (1.66626 iter/s, 6.00146s/10 iters), loss = 7.18258
I0524 00:35:57.096704 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.18258 (* 1 = 7.18258 loss)
I0524 00:35:57.097110 11835 sgd_solver.cpp:112] Iteration 13800, lr = 0.1
I0524 00:36:02.959352 11835 solver.cpp:239] Iteration 13810 (1.70578 iter/s, 5.86243s/10 iters), loss = 8.43546
I0524 00:36:02.959391 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.43546 (* 1 = 8.43546 loss)
I0524 00:36:02.959404 11835 sgd_solver.cpp:112] Iteration 13810, lr = 0.1
I0524 00:36:09.872196 11835 solver.cpp:239] Iteration 13820 (1.44665 iter/s, 6.91253s/10 iters), loss = 7.89823
I0524 00:36:09.872545 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.89823 (* 1 = 7.89823 loss)
I0524 00:36:09.872611 11835 sgd_solver.cpp:112] Iteration 13820, lr = 0.1
I0524 00:36:16.643458 11835 solver.cpp:239] Iteration 13830 (1.47717 iter/s, 6.7697s/10 iters), loss = 7.79595
I0524 00:36:16.643496 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.79595 (* 1 = 7.79595 loss)
I0524 00:36:17.046939 11835 sgd_solver.cpp:112] Iteration 13830, lr = 0.1
I0524 00:36:25.061619 11835 solver.cpp:239] Iteration 13840 (1.18796 iter/s, 8.41779s/10 iters), loss = 8.40335
I0524 00:36:25.061676 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.40335 (* 1 = 8.40335 loss)
I0524 00:36:25.062207 11835 sgd_solver.cpp:112] Iteration 13840, lr = 0.1
I0524 00:36:32.187503 11835 solver.cpp:239] Iteration 13850 (1.4034 iter/s, 7.12556s/10 iters), loss = 8.37832
I0524 00:36:32.187553 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.37832 (* 1 = 8.37832 loss)
I0524 00:36:32.370769 11835 sgd_solver.cpp:112] Iteration 13850, lr = 0.1
I0524 00:36:39.337512 11835 solver.cpp:239] Iteration 13860 (1.39866 iter/s, 7.14968s/10 iters), loss = 7.83316
I0524 00:36:39.337574 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.83316 (* 1 = 7.83316 loss)
I0524 00:36:39.337736 11835 sgd_solver.cpp:112] Iteration 13860, lr = 0.1
I0524 00:36:47.603078 11835 solver.cpp:239] Iteration 13870 (1.20989 iter/s, 8.2652s/10 iters), loss = 7.71439
I0524 00:36:47.603175 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.71439 (* 1 = 7.71439 loss)
I0524 00:36:48.654202 11835 sgd_solver.cpp:112] Iteration 13870, lr = 0.1
I0524 00:36:55.723311 11835 solver.cpp:239] Iteration 13880 (1.23155 iter/s, 8.11982s/10 iters), loss = 8.0056
I0524 00:36:55.723362 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.0056 (* 1 = 8.0056 loss)
I0524 00:36:55.723487 11835 sgd_solver.cpp:112] Iteration 13880, lr = 0.1
I0524 00:37:02.863240 11835 solver.cpp:239] Iteration 13890 (1.40064 iter/s, 7.1396s/10 iters), loss = 7.81877
I0524 00:37:02.863293 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.81877 (* 1 = 7.81877 loss)
I0524 00:37:02.863665 11835 sgd_solver.cpp:112] Iteration 13890, lr = 0.1
I0524 00:37:11.167002 11835 solver.cpp:239] Iteration 13900 (1.20433 iter/s, 8.30339s/10 iters), loss = 7.96168
I0524 00:37:11.167058 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.96168 (* 1 = 7.96168 loss)
I0524 00:37:11.167309 11835 sgd_solver.cpp:112] Iteration 13900, lr = 0.1
I0524 00:37:17.800711 11835 solver.cpp:239] Iteration 13910 (1.50752 iter/s, 6.6334s/10 iters), loss = 7.68052
I0524 00:37:17.800992 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.68052 (* 1 = 7.68052 loss)
I0524 00:37:17.866816 11835 sgd_solver.cpp:112] Iteration 13910, lr = 0.1
I0524 00:37:24.453127 11835 solver.cpp:239] Iteration 13920 (1.50332 iter/s, 6.65195s/10 iters), loss = 7.56144
I0524 00:37:24.453176 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.56144 (* 1 = 7.56144 loss)
I0524 00:37:25.489923 11835 sgd_solver.cpp:112] Iteration 13920, lr = 0.1
I0524 00:37:32.063426 11835 solver.cpp:239] Iteration 13930 (1.31407 iter/s, 7.60997s/10 iters), loss = 7.80471
I0524 00:37:32.063472 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.80471 (* 1 = 7.80471 loss)
I0524 00:37:32.063488 11835 sgd_solver.cpp:112] Iteration 13930, lr = 0.1
I0524 00:37:39.544150 11835 solver.cpp:239] Iteration 13940 (1.33683 iter/s, 7.48039s/10 iters), loss = 6.8661
I0524 00:37:39.544204 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8661 (* 1 = 6.8661 loss)
I0524 00:37:39.544358 11835 sgd_solver.cpp:112] Iteration 13940, lr = 0.1
I0524 00:37:46.625337 11835 solver.cpp:239] Iteration 13950 (1.41226 iter/s, 7.08087s/10 iters), loss = 8.05196
I0524 00:37:46.625385 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.05196 (* 1 = 8.05196 loss)
I0524 00:37:46.625499 11835 sgd_solver.cpp:112] Iteration 13950, lr = 0.1
I0524 00:37:52.943188 11835 solver.cpp:239] Iteration 13960 (1.58289 iter/s, 6.31757s/10 iters), loss = 7.8765
I0524 00:37:52.943444 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.8765 (* 1 = 7.8765 loss)
I0524 00:37:52.943498 11835 sgd_solver.cpp:112] Iteration 13960, lr = 0.1
I0524 00:37:58.682303 11835 solver.cpp:239] Iteration 13970 (1.74259 iter/s, 5.73859s/10 iters), loss = 7.5587
I0524 00:37:58.682348 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.5587 (* 1 = 7.5587 loss)
I0524 00:37:58.682477 11835 sgd_solver.cpp:112] Iteration 13970, lr = 0.1
I0524 00:38:05.606462 11835 solver.cpp:239] Iteration 13980 (1.44428 iter/s, 6.92385s/10 iters), loss = 7.98584
I0524 00:38:05.606513 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.98584 (* 1 = 7.98584 loss)
I0524 00:38:05.606529 11835 sgd_solver.cpp:112] Iteration 13980, lr = 0.1
I0524 00:38:11.722631 11835 solver.cpp:239] Iteration 13990 (1.63538 iter/s, 6.11478s/10 iters), loss = 8.11755
I0524 00:38:11.722719 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.11755 (* 1 = 8.11755 loss)
I0524 00:38:11.722736 11835 sgd_solver.cpp:112] Iteration 13990, lr = 0.1
I0524 00:38:17.983566 11835 solver.cpp:239] Iteration 14000 (1.59728 iter/s, 6.26066s/10 iters), loss = 7.473
I0524 00:38:17.983603 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.473 (* 1 = 7.473 loss)
I0524 00:38:17.983616 11835 sgd_solver.cpp:112] Iteration 14000, lr = 0.1
I0524 00:38:24.143127 11835 solver.cpp:239] Iteration 14010 (1.62356 iter/s, 6.15929s/10 iters), loss = 7.98593
I0524 00:38:24.143332 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.98593 (* 1 = 7.98593 loss)
I0524 00:38:24.187916 11835 sgd_solver.cpp:112] Iteration 14010, lr = 0.1
I0524 00:38:30.532688 11835 solver.cpp:239] Iteration 14020 (1.56515 iter/s, 6.38915s/10 iters), loss = 7.92352
I0524 00:38:30.532735 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.92352 (* 1 = 7.92352 loss)
I0524 00:38:30.532835 11835 sgd_solver.cpp:112] Iteration 14020, lr = 0.1
I0524 00:38:38.439872 11835 solver.cpp:239] Iteration 14030 (1.26473 iter/s, 7.90684s/10 iters), loss = 8.46911
I0524 00:38:38.439920 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.46911 (* 1 = 8.46911 loss)
I0524 00:38:38.440292 11835 sgd_solver.cpp:112] Iteration 14030, lr = 0.1
I0524 00:38:45.002622 11835 solver.cpp:239] Iteration 14040 (1.52382 iter/s, 6.56245s/10 iters), loss = 8.54688
I0524 00:38:45.002674 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.54688 (* 1 = 8.54688 loss)
I0524 00:38:45.002775 11835 sgd_solver.cpp:112] Iteration 14040, lr = 0.1
I0524 00:38:51.140898 11835 solver.cpp:239] Iteration 14050 (1.6292 iter/s, 6.13799s/10 iters), loss = 7.49802
I0524 00:38:51.140950 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.49802 (* 1 = 7.49802 loss)
I0524 00:38:51.213835 11835 sgd_solver.cpp:112] Iteration 14050, lr = 0.1
I0524 00:38:57.726274 11835 solver.cpp:239] Iteration 14060 (1.51859 iter/s, 6.58506s/10 iters), loss = 7.66547
I0524 00:38:57.726538 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.66547 (* 1 = 7.66547 loss)
I0524 00:38:57.726590 11835 sgd_solver.cpp:112] Iteration 14060, lr = 0.1
I0524 00:39:05.048015 11835 solver.cpp:239] Iteration 14070 (1.36589 iter/s, 7.32124s/10 iters), loss = 7.36989
I0524 00:39:05.048064 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.36989 (* 1 = 7.36989 loss)
I0524 00:39:05.048198 11835 sgd_solver.cpp:112] Iteration 14070, lr = 0.1
I0524 00:39:10.613884 11835 solver.cpp:239] Iteration 14080 (1.79675 iter/s, 5.56562s/10 iters), loss = 8.29258
I0524 00:39:10.613922 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.29258 (* 1 = 8.29258 loss)
I0524 00:39:10.614003 11835 sgd_solver.cpp:112] Iteration 14080, lr = 0.1
I0524 00:39:17.669939 11835 solver.cpp:239] Iteration 14090 (1.41729 iter/s, 7.05574s/10 iters), loss = 7.50904
I0524 00:39:17.669989 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.50904 (* 1 = 7.50904 loss)
I0524 00:39:18.103727 11835 sgd_solver.cpp:112] Iteration 14090, lr = 0.1
I0524 00:39:26.776477 11835 solver.cpp:239] Iteration 14100 (1.09816 iter/s, 9.10615s/10 iters), loss = 7.85376
I0524 00:39:26.776525 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.85376 (* 1 = 7.85376 loss)
I0524 00:39:26.776659 11835 sgd_solver.cpp:112] Iteration 14100, lr = 0.1
I0524 00:39:33.812328 11835 solver.cpp:239] Iteration 14110 (1.42136 iter/s, 7.03553s/10 iters), loss = 7.8136
I0524 00:39:33.812611 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.8136 (* 1 = 7.8136 loss)
I0524 00:39:33.812665 11835 sgd_solver.cpp:112] Iteration 14110, lr = 0.1
I0524 00:39:39.619268 11835 solver.cpp:239] Iteration 14120 (1.72221 iter/s, 5.80649s/10 iters), loss = 7.21129
I0524 00:39:39.619310 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.21129 (* 1 = 7.21129 loss)
I0524 00:39:39.619324 11835 sgd_solver.cpp:112] Iteration 14120, lr = 0.1
I0524 00:39:47.011761 11835 solver.cpp:239] Iteration 14130 (1.3528 iter/s, 7.39209s/10 iters), loss = 6.74591
I0524 00:39:47.011807 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74591 (* 1 = 6.74591 loss)
I0524 00:39:47.011821 11835 sgd_solver.cpp:112] Iteration 14130, lr = 0.1
I0524 00:39:53.408843 11835 solver.cpp:239] Iteration 14140 (1.56328 iter/s, 6.3968s/10 iters), loss = 7.37373
I0524 00:39:53.408880 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.37373 (* 1 = 7.37373 loss)
I0524 00:39:53.409026 11835 sgd_solver.cpp:112] Iteration 14140, lr = 0.1
I0524 00:40:01.689775 11835 solver.cpp:239] Iteration 14150 (1.20765 iter/s, 8.28057s/10 iters), loss = 8.45579
I0524 00:40:01.689829 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.45579 (* 1 = 8.45579 loss)
I0524 00:40:01.690034 11835 sgd_solver.cpp:112] Iteration 14150, lr = 0.1
I0524 00:40:11.573895 11835 solver.cpp:239] Iteration 14160 (1.01177 iter/s, 9.8837s/10 iters), loss = 8.30002
I0524 00:40:11.574158 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.30002 (* 1 = 8.30002 loss)
I0524 00:40:13.247619 11835 sgd_solver.cpp:112] Iteration 14160, lr = 0.1
I0524 00:40:20.991380 11835 solver.cpp:239] Iteration 14170 (1.06192 iter/s, 9.41692s/10 iters), loss = 8.18295
I0524 00:40:20.991426 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.18295 (* 1 = 8.18295 loss)
I0524 00:40:20.991592 11835 sgd_solver.cpp:112] Iteration 14170, lr = 0.1
I0524 00:40:27.860895 11835 solver.cpp:239] Iteration 14180 (1.45577 iter/s, 6.86921s/10 iters), loss = 7.75839
I0524 00:40:27.860942 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.75839 (* 1 = 7.75839 loss)
I0524 00:40:27.861155 11835 sgd_solver.cpp:112] Iteration 14180, lr = 0.1
I0524 00:40:33.943639 11835 solver.cpp:239] Iteration 14190 (1.64407 iter/s, 6.08246s/10 iters), loss = 7.47874
I0524 00:40:33.943696 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.47874 (* 1 = 7.47874 loss)
I0524 00:40:33.943830 11835 sgd_solver.cpp:112] Iteration 14190, lr = 0.1
I0524 00:40:40.068933 11835 solver.cpp:239] Iteration 14200 (1.63265 iter/s, 6.12501s/10 iters), loss = 8.45502
I0524 00:40:40.068976 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.45502 (* 1 = 8.45502 loss)
I0524 00:40:40.069103 11835 sgd_solver.cpp:112] Iteration 14200, lr = 0.1
I0524 00:40:46.212862 11835 solver.cpp:239] Iteration 14210 (1.6277 iter/s, 6.14364s/10 iters), loss = 7.91108
I0524 00:40:46.213083 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.91108 (* 1 = 7.91108 loss)
I0524 00:40:46.213136 11835 sgd_solver.cpp:112] Iteration 14210, lr = 0.1
I0524 00:40:53.680397 11835 solver.cpp:239] Iteration 14220 (1.33921 iter/s, 7.46708s/10 iters), loss = 7.89294
I0524 00:40:53.680436 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.89294 (* 1 = 7.89294 loss)
I0524 00:40:53.680459 11835 sgd_solver.cpp:112] Iteration 14220, lr = 0.1
I0524 00:41:00.552901 11835 solver.cpp:239] Iteration 14230 (1.45514 iter/s, 6.8722s/10 iters), loss = 8.76496
I0524 00:41:00.552953 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.76496 (* 1 = 8.76496 loss)
I0524 00:41:00.552965 11835 sgd_solver.cpp:112] Iteration 14230, lr = 0.1
I0524 00:41:07.939075 11835 solver.cpp:239] Iteration 14240 (1.35399 iter/s, 7.3856s/10 iters), loss = 7.47581
I0524 00:41:07.939127 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.47581 (* 1 = 7.47581 loss)
I0524 00:41:07.939164 11835 sgd_solver.cpp:112] Iteration 14240, lr = 0.1
I0524 00:41:15.020757 11835 solver.cpp:239] Iteration 14250 (1.41216 iter/s, 7.08137s/10 iters), loss = 8.63492
I0524 00:41:15.020805 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.63492 (* 1 = 8.63492 loss)
I0524 00:41:15.021070 11835 sgd_solver.cpp:112] Iteration 14250, lr = 0.1
I0524 00:41:22.206336 11835 solver.cpp:239] Iteration 14260 (1.39174 iter/s, 7.18524s/10 iters), loss = 7.56753
I0524 00:41:22.206635 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.56753 (* 1 = 7.56753 loss)
I0524 00:41:22.206683 11835 sgd_solver.cpp:112] Iteration 14260, lr = 0.1
I0524 00:41:28.894960 11835 solver.cpp:239] Iteration 14270 (1.49536 iter/s, 6.68734s/10 iters), loss = 8.05528
I0524 00:41:28.894999 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.05528 (* 1 = 8.05528 loss)
I0524 00:41:28.895011 11835 sgd_solver.cpp:112] Iteration 14270, lr = 0.1
I0524 00:41:36.518185 11835 solver.cpp:239] Iteration 14280 (1.31188 iter/s, 7.62262s/10 iters), loss = 7.56128
I0524 00:41:36.518236 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.56128 (* 1 = 7.56128 loss)
I0524 00:41:37.385608 11835 sgd_solver.cpp:112] Iteration 14280, lr = 0.1
I0524 00:41:45.530736 11835 solver.cpp:239] Iteration 14290 (1.10962 iter/s, 9.01213s/10 iters), loss = 8.06822
I0524 00:41:45.530787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.06822 (* 1 = 8.06822 loss)
I0524 00:41:46.320282 11835 sgd_solver.cpp:112] Iteration 14290, lr = 0.1
I0524 00:41:53.258258 11835 solver.cpp:239] Iteration 14300 (1.29413 iter/s, 7.72718s/10 iters), loss = 7.15143
I0524 00:41:53.258509 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.15143 (* 1 = 7.15143 loss)
I0524 00:41:53.258590 11835 sgd_solver.cpp:112] Iteration 14300, lr = 0.1
I0524 00:41:59.682037 11835 solver.cpp:239] Iteration 14310 (1.55683 iter/s, 6.42332s/10 iters), loss = 7.81274
I0524 00:41:59.682096 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.81274 (* 1 = 7.81274 loss)
I0524 00:41:59.682113 11835 sgd_solver.cpp:112] Iteration 14310, lr = 0.1
I0524 00:42:08.703382 11835 solver.cpp:239] Iteration 14320 (1.10854 iter/s, 9.0209s/10 iters), loss = 7.31168
I0524 00:42:08.703431 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.31168 (* 1 = 7.31168 loss)
I0524 00:42:08.773126 11835 sgd_solver.cpp:112] Iteration 14320, lr = 0.1
I0524 00:42:14.998718 11835 solver.cpp:239] Iteration 14330 (1.58855 iter/s, 6.29505s/10 iters), loss = 8.06666
I0524 00:42:14.998769 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.06666 (* 1 = 8.06666 loss)
I0524 00:42:14.998904 11835 sgd_solver.cpp:112] Iteration 14330, lr = 0.1
I0524 00:42:20.788734 11835 solver.cpp:239] Iteration 14340 (1.72719 iter/s, 5.78975s/10 iters), loss = 8.00467
I0524 00:42:20.788774 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.00467 (* 1 = 8.00467 loss)
I0524 00:42:20.788786 11835 sgd_solver.cpp:112] Iteration 14340, lr = 0.1
I0524 00:42:29.320116 11835 solver.cpp:239] Iteration 14350 (1.17219 iter/s, 8.53101s/10 iters), loss = 7.04544
I0524 00:42:29.320377 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04544 (* 1 = 7.04544 loss)
I0524 00:42:29.320436 11835 sgd_solver.cpp:112] Iteration 14350, lr = 0.1
I0524 00:42:35.191923 11835 solver.cpp:239] Iteration 14360 (1.70325 iter/s, 5.87112s/10 iters), loss = 7.51576
I0524 00:42:35.191962 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.51576 (* 1 = 7.51576 loss)
I0524 00:42:35.193928 11835 sgd_solver.cpp:112] Iteration 14360, lr = 0.1
I0524 00:42:41.110637 11835 solver.cpp:239] Iteration 14370 (1.68963 iter/s, 5.91844s/10 iters), loss = 8.13673
I0524 00:42:41.110690 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.13673 (* 1 = 8.13673 loss)
I0524 00:42:41.917652 11835 sgd_solver.cpp:112] Iteration 14370, lr = 0.1
I0524 00:42:51.199419 11835 solver.cpp:239] Iteration 14380 (0.991242 iter/s, 10.0884s/10 iters), loss = 8.49168
I0524 00:42:51.199473 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.49168 (* 1 = 8.49168 loss)
I0524 00:42:51.309942 11835 sgd_solver.cpp:112] Iteration 14380, lr = 0.1
I0524 00:42:57.105448 11835 solver.cpp:239] Iteration 14390 (1.69327 iter/s, 5.90574s/10 iters), loss = 8.33828
I0524 00:42:57.105499 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.33828 (* 1 = 8.33828 loss)
I0524 00:42:57.134742 11835 sgd_solver.cpp:112] Iteration 14390, lr = 0.1
I0524 00:43:02.978235 11835 solver.cpp:239] Iteration 14400 (1.70285 iter/s, 5.87251s/10 iters), loss = 7.21207
I0524 00:43:02.978523 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.21207 (* 1 = 7.21207 loss)
I0524 00:43:02.978570 11835 sgd_solver.cpp:112] Iteration 14400, lr = 0.1
I0524 00:43:08.977526 11835 solver.cpp:239] Iteration 14410 (1.66707 iter/s, 5.99856s/10 iters), loss = 8.33451
I0524 00:43:08.977568 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.33451 (* 1 = 8.33451 loss)
I0524 00:43:08.977629 11835 sgd_solver.cpp:112] Iteration 14410, lr = 0.1
I0524 00:43:15.902396 11835 solver.cpp:239] Iteration 14420 (1.44413 iter/s, 6.92456s/10 iters), loss = 7.88457
I0524 00:43:15.902444 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.88457 (* 1 = 7.88457 loss)
I0524 00:43:15.902787 11835 sgd_solver.cpp:112] Iteration 14420, lr = 0.1
I0524 00:43:21.657219 11835 solver.cpp:239] Iteration 14430 (1.73776 iter/s, 5.75455s/10 iters), loss = 7.89152
I0524 00:43:21.657268 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.89152 (* 1 = 7.89152 loss)
I0524 00:43:21.657512 11835 sgd_solver.cpp:112] Iteration 14430, lr = 0.1
I0524 00:43:31.224812 11835 solver.cpp:239] Iteration 14440 (1.04524 iter/s, 9.56719s/10 iters), loss = 7.91974
I0524 00:43:31.224875 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.91974 (* 1 = 7.91974 loss)
I0524 00:43:31.998572 11835 sgd_solver.cpp:112] Iteration 14440, lr = 0.1
I0524 00:43:41.200301 11835 solver.cpp:239] Iteration 14450 (1.0025 iter/s, 9.97506s/10 iters), loss = 6.92273
I0524 00:43:41.200405 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92273 (* 1 = 6.92273 loss)
I0524 00:43:41.200600 11835 sgd_solver.cpp:112] Iteration 14450, lr = 0.1
I0524 00:43:47.840415 11835 solver.cpp:239] Iteration 14460 (1.50608 iter/s, 6.63977s/10 iters), loss = 7.37476
I0524 00:43:47.840459 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.37476 (* 1 = 7.37476 loss)
I0524 00:43:47.840471 11835 sgd_solver.cpp:112] Iteration 14460, lr = 0.1
I0524 00:43:56.050768 11835 solver.cpp:239] Iteration 14470 (1.21803 iter/s, 8.20999s/10 iters), loss = 8.15562
I0524 00:43:56.050827 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.15562 (* 1 = 8.15562 loss)
I0524 00:43:56.069975 11835 sgd_solver.cpp:112] Iteration 14470, lr = 0.1
I0524 00:44:03.925753 11835 solver.cpp:239] Iteration 14480 (1.2699 iter/s, 7.87463s/10 iters), loss = 7.63684
I0524 00:44:03.925806 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.63684 (* 1 = 7.63684 loss)
I0524 00:44:03.925819 11835 sgd_solver.cpp:112] Iteration 14480, lr = 0.1
I0524 00:44:11.209844 11835 solver.cpp:239] Iteration 14490 (1.37296 iter/s, 7.28353s/10 iters), loss = 7.7619
I0524 00:44:11.210127 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.7619 (* 1 = 7.7619 loss)
I0524 00:44:11.210183 11835 sgd_solver.cpp:112] Iteration 14490, lr = 0.1
I0524 00:44:18.303310 11835 solver.cpp:239] Iteration 14500 (1.40987 iter/s, 7.09286s/10 iters), loss = 8.71609
I0524 00:44:18.303369 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.71609 (* 1 = 8.71609 loss)
I0524 00:44:18.303588 11835 sgd_solver.cpp:112] Iteration 14500, lr = 0.1
I0524 00:44:24.672828 11835 solver.cpp:239] Iteration 14510 (1.57005 iter/s, 6.36921s/10 iters), loss = 7.26878
I0524 00:44:24.672885 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.26878 (* 1 = 7.26878 loss)
I0524 00:44:24.673017 11835 sgd_solver.cpp:112] Iteration 14510, lr = 0.1
I0524 00:44:31.356715 11835 solver.cpp:239] Iteration 14520 (1.4962 iter/s, 6.68359s/10 iters), loss = 8.02969
I0524 00:44:31.356755 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.02969 (* 1 = 8.02969 loss)
I0524 00:44:32.041306 11835 sgd_solver.cpp:112] Iteration 14520, lr = 0.1
I0524 00:44:38.568356 11835 solver.cpp:239] Iteration 14530 (1.38671 iter/s, 7.21132s/10 iters), loss = 7.9593
I0524 00:44:38.568399 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.9593 (* 1 = 7.9593 loss)
I0524 00:44:38.568413 11835 sgd_solver.cpp:112] Iteration 14530, lr = 0.1
I0524 00:44:44.357074 11835 solver.cpp:239] Iteration 14540 (1.72823 iter/s, 5.78627s/10 iters), loss = 7.04628
I0524 00:44:44.357233 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04628 (* 1 = 7.04628 loss)
I0524 00:44:44.829376 11835 sgd_solver.cpp:112] Iteration 14540, lr = 0.1
I0524 00:44:54.167891 11835 solver.cpp:239] Iteration 14550 (1.01934 iter/s, 9.81027s/10 iters), loss = 8.099
I0524 00:44:54.167946 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.099 (* 1 = 8.099 loss)
I0524 00:44:54.168243 11835 sgd_solver.cpp:112] Iteration 14550, lr = 0.1
I0524 00:45:00.246845 11835 solver.cpp:239] Iteration 14560 (1.6451 iter/s, 6.07866s/10 iters), loss = 8.33876
I0524 00:45:00.246893 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.33876 (* 1 = 8.33876 loss)
I0524 00:45:00.247136 11835 sgd_solver.cpp:112] Iteration 14560, lr = 0.1
I0524 00:45:06.551517 11835 solver.cpp:239] Iteration 14570 (1.5862 iter/s, 6.30437s/10 iters), loss = 7.42709
I0524 00:45:06.551573 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.42709 (* 1 = 7.42709 loss)
I0524 00:45:06.551772 11835 sgd_solver.cpp:112] Iteration 14570, lr = 0.1
I0524 00:45:12.753294 11835 solver.cpp:239] Iteration 14580 (1.61251 iter/s, 6.20149s/10 iters), loss = 6.76275
I0524 00:45:12.753335 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76275 (* 1 = 6.76275 loss)
I0524 00:45:12.753736 11835 sgd_solver.cpp:112] Iteration 14580, lr = 0.1
I0524 00:45:19.648980 11835 solver.cpp:239] Iteration 14590 (1.45025 iter/s, 6.89536s/10 iters), loss = 7.25675
I0524 00:45:19.649099 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.25675 (* 1 = 7.25675 loss)
I0524 00:45:19.661305 11835 sgd_solver.cpp:112] Iteration 14590, lr = 0.1
I0524 00:45:25.598558 11835 solver.cpp:239] Iteration 14600 (1.68089 iter/s, 5.94924s/10 iters), loss = 8.55635
I0524 00:45:25.598597 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.55635 (* 1 = 8.55635 loss)
I0524 00:45:25.958138 11835 sgd_solver.cpp:112] Iteration 14600, lr = 0.1
I0524 00:45:32.159687 11835 solver.cpp:239] Iteration 14610 (1.5242 iter/s, 6.56083s/10 iters), loss = 8.79692
I0524 00:45:32.159737 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.79692 (* 1 = 8.79692 loss)
I0524 00:45:32.341289 11835 sgd_solver.cpp:112] Iteration 14610, lr = 0.1
I0524 00:45:37.843520 11835 solver.cpp:239] Iteration 14620 (1.75946 iter/s, 5.68356s/10 iters), loss = 8.48136
I0524 00:45:37.843577 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.48136 (* 1 = 8.48136 loss)
I0524 00:45:37.844121 11835 sgd_solver.cpp:112] Iteration 14620, lr = 0.1
I0524 00:45:47.069355 11835 solver.cpp:239] Iteration 14630 (1.08396 iter/s, 9.22544s/10 iters), loss = 9.24397
I0524 00:45:47.069401 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.24397 (* 1 = 9.24397 loss)
I0524 00:45:47.470533 11835 sgd_solver.cpp:112] Iteration 14630, lr = 0.1
I0524 00:45:52.956936 11835 solver.cpp:239] Iteration 14640 (1.69857 iter/s, 5.88732s/10 iters), loss = 7.4808
I0524 00:45:52.957140 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.4808 (* 1 = 7.4808 loss)
I0524 00:45:52.957190 11835 sgd_solver.cpp:112] Iteration 14640, lr = 0.1
I0524 00:46:00.425083 11835 solver.cpp:239] Iteration 14650 (1.33911 iter/s, 7.46765s/10 iters), loss = 7.78665
I0524 00:46:00.425137 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.78665 (* 1 = 7.78665 loss)
I0524 00:46:00.425643 11835 sgd_solver.cpp:112] Iteration 14650, lr = 0.1
I0524 00:46:07.097321 11835 solver.cpp:239] Iteration 14660 (1.49881 iter/s, 6.67194s/10 iters), loss = 7.27306
I0524 00:46:07.097363 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.27306 (* 1 = 7.27306 loss)
I0524 00:46:07.097487 11835 sgd_solver.cpp:112] Iteration 14660, lr = 0.1
I0524 00:46:14.335333 11835 solver.cpp:239] Iteration 14670 (1.38166 iter/s, 7.23769s/10 iters), loss = 8.545
I0524 00:46:14.335389 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.545 (* 1 = 8.545 loss)
I0524 00:46:14.608537 11835 sgd_solver.cpp:112] Iteration 14670, lr = 0.1
I0524 00:46:22.466413 11835 solver.cpp:239] Iteration 14680 (1.2299 iter/s, 8.13072s/10 iters), loss = 7.71275
I0524 00:46:22.466460 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.71275 (* 1 = 7.71275 loss)
I0524 00:46:22.466481 11835 sgd_solver.cpp:112] Iteration 14680, lr = 0.1
I0524 00:46:29.321589 11835 solver.cpp:239] Iteration 14690 (1.45882 iter/s, 6.85487s/10 iters), loss = 8.40543
I0524 00:46:29.321812 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.40543 (* 1 = 8.40543 loss)
I0524 00:46:29.340306 11835 sgd_solver.cpp:112] Iteration 14690, lr = 0.1
I0524 00:46:37.986759 11835 solver.cpp:239] Iteration 14700 (1.15412 iter/s, 8.66464s/10 iters), loss = 7.64896
I0524 00:46:37.986815 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.64896 (* 1 = 7.64896 loss)
I0524 00:46:37.987066 11835 sgd_solver.cpp:112] Iteration 14700, lr = 0.1
I0524 00:46:46.161590 11835 solver.cpp:239] Iteration 14710 (1.22332 iter/s, 8.17447s/10 iters), loss = 7.81247
I0524 00:46:46.161650 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.81247 (* 1 = 7.81247 loss)
I0524 00:46:46.161887 11835 sgd_solver.cpp:112] Iteration 14710, lr = 0.1
I0524 00:46:53.018875 11835 solver.cpp:239] Iteration 14720 (1.45837 iter/s, 6.85695s/10 iters), loss = 7.86961
I0524 00:46:53.018934 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.86961 (* 1 = 7.86961 loss)
I0524 00:46:53.019290 11835 sgd_solver.cpp:112] Iteration 14720, lr = 0.1
I0524 00:46:59.395099 11835 solver.cpp:239] Iteration 14730 (1.5684 iter/s, 6.37593s/10 iters), loss = 7.03317
I0524 00:46:59.395347 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03317 (* 1 = 7.03317 loss)
I0524 00:46:59.395390 11835 sgd_solver.cpp:112] Iteration 14730, lr = 0.1
I0524 00:47:05.950184 11835 solver.cpp:239] Iteration 14740 (1.52567 iter/s, 6.5545s/10 iters), loss = 7.04834
I0524 00:47:05.950233 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04834 (* 1 = 7.04834 loss)
I0524 00:47:05.950331 11835 sgd_solver.cpp:112] Iteration 14740, lr = 0.1
I0524 00:47:12.720656 11835 solver.cpp:239] Iteration 14750 (1.47707 iter/s, 6.77017s/10 iters), loss = 7.15898
I0524 00:47:12.720700 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.15898 (* 1 = 7.15898 loss)
I0524 00:47:12.720952 11835 sgd_solver.cpp:112] Iteration 14750, lr = 0.1
I0524 00:47:18.435503 11835 solver.cpp:239] Iteration 14760 (1.74991 iter/s, 5.71458s/10 iters), loss = 8.20079
I0524 00:47:18.435554 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.20079 (* 1 = 8.20079 loss)
I0524 00:47:18.451720 11835 sgd_solver.cpp:112] Iteration 14760, lr = 0.1
I0524 00:47:24.948388 11835 solver.cpp:239] Iteration 14770 (1.53549 iter/s, 6.51259s/10 iters), loss = 8.47751
I0524 00:47:24.948433 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.47751 (* 1 = 8.47751 loss)
I0524 00:47:24.948616 11835 sgd_solver.cpp:112] Iteration 14770, lr = 0.1
I0524 00:47:31.607199 11835 solver.cpp:239] Iteration 14780 (1.50184 iter/s, 6.6585s/10 iters), loss = 7.90848
I0524 00:47:31.607389 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.90848 (* 1 = 7.90848 loss)
I0524 00:47:32.037842 11835 sgd_solver.cpp:112] Iteration 14780, lr = 0.1
I0524 00:47:38.187547 11835 solver.cpp:239] Iteration 14790 (1.51978 iter/s, 6.57991s/10 iters), loss = 7.55558
I0524 00:47:38.187599 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.55558 (* 1 = 7.55558 loss)
I0524 00:47:39.352510 11835 sgd_solver.cpp:112] Iteration 14790, lr = 0.1
I0524 00:47:45.259837 11835 solver.cpp:239] Iteration 14800 (1.41403 iter/s, 7.07198s/10 iters), loss = 7.65051
I0524 00:47:45.259882 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.65051 (* 1 = 7.65051 loss)
I0524 00:47:45.260030 11835 sgd_solver.cpp:112] Iteration 14800, lr = 0.1
I0524 00:47:51.049000 11835 solver.cpp:239] Iteration 14810 (1.72745 iter/s, 5.7889s/10 iters), loss = 7.35243
I0524 00:47:51.049043 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.35243 (* 1 = 7.35243 loss)
I0524 00:47:51.049057 11835 sgd_solver.cpp:112] Iteration 14810, lr = 0.1
I0524 00:47:57.886845 11835 solver.cpp:239] Iteration 14820 (1.46253 iter/s, 6.83747s/10 iters), loss = 7.46461
I0524 00:47:57.886880 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.46461 (* 1 = 7.46461 loss)
I0524 00:47:57.887465 11835 sgd_solver.cpp:112] Iteration 14820, lr = 0.1
I0524 00:48:04.623361 11835 solver.cpp:239] Iteration 14830 (1.48452 iter/s, 6.7362s/10 iters), loss = 7.30481
I0524 00:48:04.623554 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.30481 (* 1 = 7.30481 loss)
I0524 00:48:04.623631 11835 sgd_solver.cpp:112] Iteration 14830, lr = 0.1
I0524 00:48:13.708680 11835 solver.cpp:239] Iteration 14840 (1.10074 iter/s, 9.08479s/10 iters), loss = 9.15166
I0524 00:48:13.708727 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.15166 (* 1 = 9.15166 loss)
I0524 00:48:13.708940 11835 sgd_solver.cpp:112] Iteration 14840, lr = 0.1
I0524 00:48:21.303395 11835 solver.cpp:239] Iteration 14850 (1.31676 iter/s, 7.59439s/10 iters), loss = 7.78911
I0524 00:48:21.303448 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.78911 (* 1 = 7.78911 loss)
I0524 00:48:21.303603 11835 sgd_solver.cpp:112] Iteration 14850, lr = 0.1
I0524 00:48:26.800863 11835 solver.cpp:239] Iteration 14860 (1.81911 iter/s, 5.4972s/10 iters), loss = 8.27985
I0524 00:48:26.800917 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.27985 (* 1 = 8.27985 loss)
I0524 00:48:26.800964 11835 sgd_solver.cpp:112] Iteration 14860, lr = 0.1
I0524 00:48:32.836843 11835 solver.cpp:239] Iteration 14870 (1.65681 iter/s, 6.03571s/10 iters), loss = 7.42649
I0524 00:48:32.836884 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.42649 (* 1 = 7.42649 loss)
I0524 00:48:32.880421 11835 sgd_solver.cpp:112] Iteration 14870, lr = 0.1
I0524 00:48:39.242346 11835 solver.cpp:239] Iteration 14880 (1.56123 iter/s, 6.40521s/10 iters), loss = 7.20621
I0524 00:48:39.242605 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.20621 (* 1 = 7.20621 loss)
I0524 00:48:39.261692 11835 sgd_solver.cpp:112] Iteration 14880, lr = 0.1
I0524 00:48:46.834956 11835 solver.cpp:239] Iteration 14890 (1.31716 iter/s, 7.59211s/10 iters), loss = 6.3153
I0524 00:48:46.835005 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3153 (* 1 = 6.3153 loss)
I0524 00:48:46.835137 11835 sgd_solver.cpp:112] Iteration 14890, lr = 0.1
I0524 00:48:53.585876 11835 solver.cpp:239] Iteration 14900 (1.48135 iter/s, 6.75061s/10 iters), loss = 7.36592
I0524 00:48:53.585932 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.36592 (* 1 = 7.36592 loss)
I0524 00:48:53.585948 11835 sgd_solver.cpp:112] Iteration 14900, lr = 0.1
I0524 00:48:59.529665 11835 solver.cpp:239] Iteration 14910 (1.68251 iter/s, 5.94351s/10 iters), loss = 7.2555
I0524 00:48:59.529714 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2555 (* 1 = 7.2555 loss)
I0524 00:48:59.529729 11835 sgd_solver.cpp:112] Iteration 14910, lr = 0.1
I0524 00:49:06.104030 11835 solver.cpp:239] Iteration 14920 (1.52114 iter/s, 6.574s/10 iters), loss = 8.75199
I0524 00:49:06.104071 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.75199 (* 1 = 8.75199 loss)
I0524 00:49:06.731521 11835 sgd_solver.cpp:112] Iteration 14920, lr = 0.1
I0524 00:49:14.551730 11835 solver.cpp:239] Iteration 14930 (1.18381 iter/s, 8.44733s/10 iters), loss = 7.60938
I0524 00:49:14.552021 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.60938 (* 1 = 7.60938 loss)
I0524 00:49:14.599426 11835 sgd_solver.cpp:112] Iteration 14930, lr = 0.1
I0524 00:49:20.517771 11835 solver.cpp:239] Iteration 14940 (1.67629 iter/s, 5.96557s/10 iters), loss = 7.49446
I0524 00:49:20.517824 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.49446 (* 1 = 7.49446 loss)
I0524 00:49:20.517840 11835 sgd_solver.cpp:112] Iteration 14940, lr = 0.1
I0524 00:49:27.296485 11835 solver.cpp:239] Iteration 14950 (1.47529 iter/s, 6.77831s/10 iters), loss = 7.91573
I0524 00:49:27.296545 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.91573 (* 1 = 7.91573 loss)
I0524 00:49:27.662178 11835 sgd_solver.cpp:112] Iteration 14950, lr = 0.1
I0524 00:49:34.112829 11835 solver.cpp:239] Iteration 14960 (1.46713 iter/s, 6.81603s/10 iters), loss = 7.69657
I0524 00:49:34.112869 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.69657 (* 1 = 7.69657 loss)
I0524 00:49:34.120852 11835 sgd_solver.cpp:112] Iteration 14960, lr = 0.1
I0524 00:49:40.546990 11835 solver.cpp:239] Iteration 14970 (1.55428 iter/s, 6.43387s/10 iters), loss = 8.15716
I0524 00:49:40.547047 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.15716 (* 1 = 8.15716 loss)
I0524 00:49:41.515774 11835 sgd_solver.cpp:112] Iteration 14970, lr = 0.1
I0524 00:49:49.300171 11835 solver.cpp:239] Iteration 14980 (1.14249 iter/s, 8.75279s/10 iters), loss = 6.81011
I0524 00:49:49.300467 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81011 (* 1 = 6.81011 loss)
I0524 00:49:49.300523 11835 sgd_solver.cpp:112] Iteration 14980, lr = 0.1
I0524 00:49:55.054301 11835 solver.cpp:239] Iteration 14990 (1.73802 iter/s, 5.75366s/10 iters), loss = 7.87961
I0524 00:49:55.054355 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.87961 (* 1 = 7.87961 loss)
I0524 00:49:55.054371 11835 sgd_solver.cpp:112] Iteration 14990, lr = 0.1
I0524 00:50:01.197746 11835 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_15000.caffemodel
I0524 00:50:04.777099 11835 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_15000.solverstate
I0524 00:50:05.491890 11835 solver.cpp:239] Iteration 15000 (0.958128 iter/s, 10.437s/10 iters), loss = 7.57693
I0524 00:50:05.491963 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.57693 (* 1 = 7.57693 loss)
I0524 00:50:05.812989 11835 sgd_solver.cpp:112] Iteration 15000, lr = 0.1
I0524 00:50:13.374876 11835 solver.cpp:239] Iteration 15010 (1.26861 iter/s, 7.88263s/10 iters), loss = 8.42275
I0524 00:50:13.374920 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.42275 (* 1 = 8.42275 loss)
I0524 00:50:13.374933 11835 sgd_solver.cpp:112] Iteration 15010, lr = 0.1
I0524 00:50:18.941232 11835 solver.cpp:239] Iteration 15020 (1.79659 iter/s, 5.56609s/10 iters), loss = 6.93623
I0524 00:50:18.941285 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93623 (* 1 = 6.93623 loss)
I0524 00:50:18.941504 11835 sgd_solver.cpp:112] Iteration 15020, lr = 0.1
I0524 00:50:24.799594 11835 solver.cpp:239] Iteration 15030 (1.70704 iter/s, 5.85809s/10 iters), loss = 7.64883
I0524 00:50:24.799824 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.64883 (* 1 = 7.64883 loss)
I0524 00:50:24.799926 11835 sgd_solver.cpp:112] Iteration 15030, lr = 0.1
I0524 00:50:30.699332 11835 solver.cpp:239] Iteration 15040 (1.69511 iter/s, 5.89931s/10 iters), loss = 7.49026
I0524 00:50:30.699388 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.49026 (* 1 = 7.49026 loss)
I0524 00:50:30.699663 11835 sgd_solver.cpp:112] Iteration 15040, lr = 0.1
I0524 00:50:37.137290 11835 solver.cpp:239] Iteration 15050 (1.55336 iter/s, 6.43765s/10 iters), loss = 7.77926
I0524 00:50:37.137346 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.77926 (* 1 = 7.77926 loss)
I0524 00:50:37.904851 11835 sgd_solver.cpp:112] Iteration 15050, lr = 0.1
I0524 00:50:43.930768 11835 solver.cpp:239] Iteration 15060 (1.47207 iter/s, 6.79317s/10 iters), loss = 7.68487
I0524 00:50:43.930810 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.68487 (* 1 = 7.68487 loss)
I0524 00:50:43.930838 11835 sgd_solver.cpp:112] Iteration 15060, lr = 0.1
I0524 00:50:50.322001 11835 solver.cpp:239] Iteration 15070 (1.56471 iter/s, 6.39094s/10 iters), loss = 8.08533
I0524 00:50:50.322052 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.08533 (* 1 = 8.08533 loss)
I0524 00:50:50.322165 11835 sgd_solver.cpp:112] Iteration 15070, lr = 0.1
I0524 00:50:58.611465 11835 solver.cpp:239] Iteration 15080 (1.2064 iter/s, 8.2891s/10 iters), loss = 7.90243
I0524 00:50:58.611732 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.90243 (* 1 = 7.90243 loss)
I0524 00:50:59.175149 11835 sgd_solver.cpp:112] Iteration 15080, lr = 0.1
I0524 00:51:07.731210 11835 solver.cpp:239] Iteration 15090 (1.09659 iter/s, 9.11918s/10 iters), loss = 7.73589
I0524 00:51:07.731263 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.73589 (* 1 = 7.73589 loss)
I0524 00:51:07.773845 11835 sgd_solver.cpp:112] Iteration 15090, lr = 0.1
I0524 00:51:15.337460 11835 solver.cpp:239] Iteration 15100 (1.31477 iter/s, 7.60591s/10 iters), loss = 7.89278
I0524 00:51:15.337515 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.89278 (* 1 = 7.89278 loss)
I0524 00:51:15.337761 11835 sgd_solver.cpp:112] Iteration 15100, lr = 0.1
I0524 00:51:25.082451 11835 solver.cpp:239] Iteration 15110 (1.02621 iter/s, 9.74458s/10 iters), loss = 8.43011
I0524 00:51:25.082489 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.43011 (* 1 = 8.43011 loss)
I0524 00:51:25.082837 11835 sgd_solver.cpp:112] Iteration 15110, lr = 0.1
I0524 00:51:33.301813 11835 solver.cpp:239] Iteration 15120 (1.21669 iter/s, 8.21899s/10 iters), loss = 7.40226
I0524 00:51:33.302067 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.40226 (* 1 = 7.40226 loss)
I0524 00:51:33.302197 11835 sgd_solver.cpp:112] Iteration 15120, lr = 0.1
I0524 00:51:43.723460 11835 solver.cpp:239] Iteration 15130 (0.959595 iter/s, 10.4211s/10 iters), loss = 6.73602
I0524 00:51:43.723505 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73602 (* 1 = 6.73602 loss)
I0524 00:51:44.613857 11835 sgd_solver.cpp:112] Iteration 15130, lr = 0.1
I0524 00:51:50.853652 11835 solver.cpp:239] Iteration 15140 (1.40255 iter/s, 7.12986s/10 iters), loss = 8.67331
I0524 00:51:50.853699 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.67331 (* 1 = 8.67331 loss)
I0524 00:51:50.957206 11835 sgd_solver.cpp:112] Iteration 15140, lr = 0.1
I0524 00:51:58.329545 11835 solver.cpp:239] Iteration 15150 (1.33769 iter/s, 7.47556s/10 iters), loss = 7.61563
I0524 00:51:58.329592 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.61563 (* 1 = 7.61563 loss)
I0524 00:51:58.329807 11835 sgd_solver.cpp:112] Iteration 15150, lr = 0.1
I0524 00:52:05.029192 11835 solver.cpp:239] Iteration 15160 (1.49268 iter/s, 6.69935s/10 iters), loss = 8.16951
I0524 00:52:05.029399 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.16951 (* 1 = 8.16951 loss)
I0524 00:52:05.029440 11835 sgd_solver.cpp:112] Iteration 15160, lr = 0.1
I0524 00:52:10.985604 11835 solver.cpp:239] Iteration 15170 (1.67898 iter/s, 5.956s/10 iters), loss = 7.66755
I0524 00:52:10.985653 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.66755 (* 1 = 7.66755 loss)
I0524 00:52:11.020319 11835 sgd_solver.cpp:112] Iteration 15170, lr = 0.1
I0524 00:52:17.020709 11835 solver.cpp:239] Iteration 15180 (1.65705 iter/s, 6.03482s/10 iters), loss = 8.40178
I0524 00:52:17.020759 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.40178 (* 1 = 8.40178 loss)
I0524 00:52:17.072737 11835 sgd_solver.cpp:112] Iteration 15180, lr = 0.1
I0524 00:52:25.732903 11835 solver.cpp:239] Iteration 15190 (1.14787 iter/s, 8.71182s/10 iters), loss = 7.54634
I0524 00:52:25.732959 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.54634 (* 1 = 7.54634 loss)
I0524 00:52:25.733333 11835 sgd_solver.cpp:112] Iteration 15190, lr = 0.1
I0524 00:52:32.342995 11835 solver.cpp:239] Iteration 15200 (1.51291 iter/s, 6.60977s/10 iters), loss = 7.35908
I0524 00:52:32.343055 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.35908 (* 1 = 7.35908 loss)
I0524 00:52:32.402863 11835 sgd_solver.cpp:112] Iteration 15200, lr = 0.1
I0524 00:52:39.358269 11835 solver.cpp:239] Iteration 15210 (1.42553 iter/s, 7.01495s/10 iters), loss = 7.39546
I0524 00:52:39.358530 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.39546 (* 1 = 7.39546 loss)
I0524 00:52:39.439972 11835 sgd_solver.cpp:112] Iteration 15210, lr = 0.1
I0524 00:52:45.534291 11835 solver.cpp:239] Iteration 15220 (1.61929 iter/s, 6.17555s/10 iters), loss = 7.80899
I0524 00:52:45.534340 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.80899 (* 1 = 7.80899 loss)
I0524 00:52:45.534409 11835 sgd_solver.cpp:112] Iteration 15220, lr = 0.1
I0524 00:52:51.075459 11835 solver.cpp:239] Iteration 15230 (1.80476 iter/s, 5.54091s/10 iters), loss = 7.27261
I0524 00:52:51.075508 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.27261 (* 1 = 7.27261 loss)
I0524 00:52:51.075642 11835 sgd_solver.cpp:112] Iteration 15230, lr = 0.1
I0524 00:52:57.692703 11835 solver.cpp:239] Iteration 15240 (1.51127 iter/s, 6.61695s/10 iters), loss = 7.34553
I0524 00:52:57.692740 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.34553 (* 1 = 7.34553 loss)
I0524 00:52:57.928977 11835 sgd_solver.cpp:112] Iteration 15240, lr = 0.1
I0524 00:53:04.537487 11835 solver.cpp:239] Iteration 15250 (1.46103 iter/s, 6.84447s/10 iters), loss = 7.81887
I0524 00:53:04.537545 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.81887 (* 1 = 7.81887 loss)
I0524 00:53:05.162382 11835 sgd_solver.cpp:112] Iteration 15250, lr = 0.1
I0524 00:53:12.516993 11835 solver.cpp:239] Iteration 15260 (1.25327 iter/s, 7.97915s/10 iters), loss = 8.47298
I0524 00:53:12.517236 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.47298 (* 1 = 8.47298 loss)
I0524 00:53:12.517284 11835 sgd_solver.cpp:112] Iteration 15260, lr = 0.1
I0524 00:53:19.525027 11835 solver.cpp:239] Iteration 15270 (1.42709 iter/s, 7.00728s/10 iters), loss = 7.7834
I0524 00:53:19.525072 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.7834 (* 1 = 7.7834 loss)
I0524 00:53:19.525176 11835 sgd_solver.cpp:112] Iteration 15270, lr = 0.1
I0524 00:53:27.006602 11835 solver.cpp:239] Iteration 15280 (1.33668 iter/s, 7.48123s/10 iters), loss = 6.34634
I0524 00:53:27.006659 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34634 (* 1 = 6.34634 loss)
I0524 00:53:27.006902 11835 sgd_solver.cpp:112] Iteration 15280, lr = 0.1
I0524 00:53:32.892575 11835 solver.cpp:239] Iteration 15290 (1.69903 iter/s, 5.88569s/10 iters), loss = 8.34354
I0524 00:53:32.892629 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.34354 (* 1 = 8.34354 loss)
I0524 00:53:32.892758 11835 sgd_solver.cpp:112] Iteration 15290, lr = 0.1
I0524 00:53:38.848348 11835 solver.cpp:239] Iteration 15300 (1.67912 iter/s, 5.9555s/10 iters), loss = 7.60509
I0524 00:53:38.848392 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.60509 (* 1 = 7.60509 loss)
I0524 00:53:38.848572 11835 sgd_solver.cpp:112] Iteration 15300, lr = 0.1
I0524 00:53:44.387876 11835 solver.cpp:239] Iteration 15310 (1.80529 iter/s, 5.53927s/10 iters), loss = 6.5897
I0524 00:53:44.388178 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5897 (* 1 = 6.5897 loss)
I0524 00:53:44.388240 11835 sgd_solver.cpp:112] Iteration 15310, lr = 0.1
I0524 00:53:50.600481 11835 solver.cpp:239] Iteration 15320 (1.60976 iter/s, 6.21212s/10 iters), loss = 7.52774
I0524 00:53:50.600522 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.52774 (* 1 = 7.52774 loss)
I0524 00:53:50.600705 11835 sgd_solver.cpp:112] Iteration 15320, lr = 0.1
I0524 00:53:56.979445 11835 solver.cpp:239] Iteration 15330 (1.56772 iter/s, 6.37867s/10 iters), loss = 6.83198
I0524 00:53:56.979495 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83198 (* 1 = 6.83198 loss)
I0524 00:53:56.979625 11835 sgd_solver.cpp:112] Iteration 15330, lr = 0.1
I0524 00:54:05.713696 11835 solver.cpp:239] Iteration 15340 (1.14497 iter/s, 8.73387s/10 iters), loss = 7.08173
I0524 00:54:05.713748 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.08173 (* 1 = 7.08173 loss)
I0524 00:54:05.713871 11835 sgd_solver.cpp:112] Iteration 15340, lr = 0.1
I0524 00:54:13.393149 11835 solver.cpp:239] Iteration 15350 (1.30223 iter/s, 7.67911s/10 iters), loss = 8.13677
I0524 00:54:13.393196 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.13677 (* 1 = 8.13677 loss)
I0524 00:54:13.425580 11835 sgd_solver.cpp:112] Iteration 15350, lr = 0.1
I0524 00:54:23.513417 11835 solver.cpp:239] Iteration 15360 (0.988157 iter/s, 10.1198s/10 iters), loss = 6.87738
I0524 00:54:23.513622 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87738 (* 1 = 6.87738 loss)
I0524 00:54:23.513659 11835 sgd_solver.cpp:112] Iteration 15360, lr = 0.1
I0524 00:54:32.405176 11835 solver.cpp:239] Iteration 15370 (1.12484 iter/s, 8.89014s/10 iters), loss = 7.4616
I0524 00:54:32.405230 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.4616 (* 1 = 7.4616 loss)
I0524 00:54:32.725949 11835 sgd_solver.cpp:112] Iteration 15370, lr = 0.1
I0524 00:54:40.533588 11835 solver.cpp:239] Iteration 15380 (1.23031 iter/s, 8.12805s/10 iters), loss = 7.53568
I0524 00:54:40.533645 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.53568 (* 1 = 7.53568 loss)
I0524 00:54:40.540001 11835 sgd_solver.cpp:112] Iteration 15380, lr = 0.1
I0524 00:54:46.829373 11835 solver.cpp:239] Iteration 15390 (1.58844 iter/s, 6.2955s/10 iters), loss = 7.82377
I0524 00:54:46.829416 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.82377 (* 1 = 7.82377 loss)
I0524 00:54:46.829490 11835 sgd_solver.cpp:112] Iteration 15390, lr = 0.1
I0524 00:54:53.465847 11835 solver.cpp:239] Iteration 15400 (1.50689 iter/s, 6.63617s/10 iters), loss = 7.51259
I0524 00:54:53.465903 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.51259 (* 1 = 7.51259 loss)
I0524 00:54:53.506937 11835 sgd_solver.cpp:112] Iteration 15400, lr = 0.1
I0524 00:54:59.331243 11835 solver.cpp:239] Iteration 15410 (1.705 iter/s, 5.86511s/10 iters), loss = 7.16092
I0524 00:54:59.331387 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.16092 (* 1 = 7.16092 loss)
I0524 00:54:59.347882 11835 sgd_solver.cpp:112] Iteration 15410, lr = 0.1
I0524 00:55:05.383891 11835 solver.cpp:239] Iteration 15420 (1.65227 iter/s, 6.05228s/10 iters), loss = 8.60577
I0524 00:55:05.383949 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.60577 (* 1 = 8.60577 loss)
I0524 00:55:05.384119 11835 sgd_solver.cpp:112] Iteration 15420, lr = 0.1
I0524 00:55:11.426762 11835 solver.cpp:239] Iteration 15430 (1.65492 iter/s, 6.04258s/10 iters), loss = 7.74956
I0524 00:55:11.426815 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.74956 (* 1 = 7.74956 loss)
I0524 00:55:12.579582 11835 sgd_solver.cpp:112] Iteration 15430, lr = 0.1
I0524 00:55:20.092905 11835 solver.cpp:239] Iteration 15440 (1.15397 iter/s, 8.66577s/10 iters), loss = 7.69349
I0524 00:55:20.092952 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.69349 (* 1 = 7.69349 loss)
I0524 00:55:20.126917 11835 sgd_solver.cpp:112] Iteration 15440, lr = 0.1
I0524 00:55:26.413240 11835 solver.cpp:239] Iteration 15450 (1.58227 iter/s, 6.32003s/10 iters), loss = 7.75597
I0524 00:55:26.413296 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.75597 (* 1 = 7.75597 loss)
I0524 00:55:26.413444 11835 sgd_solver.cpp:112] Iteration 15450, lr = 0.1
I0524 00:55:32.067415 11835 solver.cpp:239] Iteration 15460 (1.76869 iter/s, 5.65391s/10 iters), loss = 7.2334
I0524 00:55:32.067739 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2334 (* 1 = 7.2334 loss)
I0524 00:55:32.067795 11835 sgd_solver.cpp:112] Iteration 15460, lr = 0.1
I0524 00:55:39.247925 11835 solver.cpp:239] Iteration 15470 (1.3929 iter/s, 7.17927s/10 iters), loss = 6.85277
I0524 00:55:39.247979 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.85277 (* 1 = 6.85277 loss)
I0524 00:55:39.248323 11835 sgd_solver.cpp:112] Iteration 15470, lr = 0.1
I0524 00:55:44.879789 11835 solver.cpp:239] Iteration 15480 (1.7757 iter/s, 5.6316s/10 iters), loss = 7.56409
I0524 00:55:44.879837 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.56409 (* 1 = 7.56409 loss)
I0524 00:55:44.879851 11835 sgd_solver.cpp:112] Iteration 15480, lr = 0.1
I0524 00:55:52.034788 11835 solver.cpp:239] Iteration 15490 (1.39812 iter/s, 7.15246s/10 iters), loss = 7.74411
I0524 00:55:52.034838 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.74411 (* 1 = 7.74411 loss)
I0524 00:55:52.969173 11835 sgd_solver.cpp:112] Iteration 15490, lr = 0.1
I0524 00:56:00.795652 11835 solver.cpp:239] Iteration 15500 (1.14149 iter/s, 8.76049s/10 iters), loss = 7.23589
I0524 00:56:00.795689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.23589 (* 1 = 7.23589 loss)
I0524 00:56:02.235654 11835 sgd_solver.cpp:112] Iteration 15500, lr = 0.1
I0524 00:56:09.106091 11835 solver.cpp:239] Iteration 15510 (1.20336 iter/s, 8.31007s/10 iters), loss = 7.50548
I0524 00:56:09.106137 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.50548 (* 1 = 7.50548 loss)
I0524 00:56:09.124491 11835 sgd_solver.cpp:112] Iteration 15510, lr = 0.1
I0524 00:56:17.339673 11835 solver.cpp:239] Iteration 15520 (1.21459 iter/s, 8.23322s/10 iters), loss = 7.37414
I0524 00:56:17.339725 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.37414 (* 1 = 7.37414 loss)
I0524 00:56:17.339740 11835 sgd_solver.cpp:112] Iteration 15520, lr = 0.1
I0524 00:56:22.966820 11835 solver.cpp:239] Iteration 15530 (1.77723 iter/s, 5.62674s/10 iters), loss = 7.51981
I0524 00:56:22.966862 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.51981 (* 1 = 7.51981 loss)
I0524 00:56:22.966876 11835 sgd_solver.cpp:112] Iteration 15530, lr = 0.1
I0524 00:56:28.920717 11835 solver.cpp:239] Iteration 15540 (1.67996 iter/s, 5.95251s/10 iters), loss = 8.14909
I0524 00:56:28.920774 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.14909 (* 1 = 8.14909 loss)
I0524 00:56:28.920819 11835 sgd_solver.cpp:112] Iteration 15540, lr = 0.1
I0524 00:56:37.182972 11835 solver.cpp:239] Iteration 15550 (1.21069 iter/s, 8.25976s/10 iters), loss = 7.78769
I0524 00:56:37.183254 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.78769 (* 1 = 7.78769 loss)
I0524 00:56:37.183305 11835 sgd_solver.cpp:112] Iteration 15550, lr = 0.1
I0524 00:56:44.244487 11835 solver.cpp:239] Iteration 15560 (1.4163 iter/s, 7.06063s/10 iters), loss = 6.8174
I0524 00:56:44.244527 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8174 (* 1 = 6.8174 loss)
I0524 00:56:44.244761 11835 sgd_solver.cpp:112] Iteration 15560, lr = 0.1
I0524 00:56:52.000568 11835 solver.cpp:239] Iteration 15570 (1.28937 iter/s, 7.75573s/10 iters), loss = 6.77892
I0524 00:56:52.000622 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.77892 (* 1 = 6.77892 loss)
I0524 00:56:52.601212 11835 sgd_solver.cpp:112] Iteration 15570, lr = 0.1
I0524 00:56:59.803174 11835 solver.cpp:239] Iteration 15580 (1.28168 iter/s, 7.80225s/10 iters), loss = 7.5891
I0524 00:56:59.803225 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.5891 (* 1 = 7.5891 loss)
I0524 00:56:59.803484 11835 sgd_solver.cpp:112] Iteration 15580, lr = 0.1
I0524 00:57:05.837102 11835 solver.cpp:239] Iteration 15590 (1.65737 iter/s, 6.03365s/10 iters), loss = 7.69046
I0524 00:57:05.837153 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.69046 (* 1 = 7.69046 loss)
I0524 00:57:05.837174 11835 sgd_solver.cpp:112] Iteration 15590, lr = 0.1
I0524 00:57:12.743253 11835 solver.cpp:239] Iteration 15600 (1.44805 iter/s, 6.90585s/10 iters), loss = 7.89542
I0524 00:57:12.743423 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.89542 (* 1 = 7.89542 loss)
I0524 00:57:13.528077 11835 sgd_solver.cpp:112] Iteration 15600, lr = 0.1
I0524 00:57:19.555510 11835 solver.cpp:239] Iteration 15610 (1.46804 iter/s, 6.81182s/10 iters), loss = 7.70816
I0524 00:57:19.555553 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.70816 (* 1 = 7.70816 loss)
I0524 00:57:19.555565 11835 sgd_solver.cpp:112] Iteration 15610, lr = 0.1
I0524 00:57:27.597251 11835 solver.cpp:239] Iteration 15620 (1.24357 iter/s, 8.04139s/10 iters), loss = 7.21261
I0524 00:57:27.597301 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.21261 (* 1 = 7.21261 loss)
I0524 00:57:27.597515 11835 sgd_solver.cpp:112] Iteration 15620, lr = 0.1
I0524 00:57:34.895892 11835 solver.cpp:239] Iteration 15630 (1.37018 iter/s, 7.29832s/10 iters), loss = 9.1541
I0524 00:57:34.895956 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.1541 (* 1 = 9.1541 loss)
I0524 00:57:34.896075 11835 sgd_solver.cpp:112] Iteration 15630, lr = 0.1
I0524 00:57:41.040374 11835 solver.cpp:239] Iteration 15640 (1.62755 iter/s, 6.1442s/10 iters), loss = 7.18609
I0524 00:57:41.040424 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.18609 (* 1 = 7.18609 loss)
I0524 00:57:41.244741 11835 sgd_solver.cpp:112] Iteration 15640, lr = 0.1
I0524 00:57:47.367034 11835 solver.cpp:239] Iteration 15650 (1.58069 iter/s, 6.32637s/10 iters), loss = 8.35435
I0524 00:57:47.367270 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.35435 (* 1 = 8.35435 loss)
I0524 00:57:47.367312 11835 sgd_solver.cpp:112] Iteration 15650, lr = 0.1
I0524 00:57:52.219676 11835 solver.cpp:239] Iteration 15660 (2.06092 iter/s, 4.85221s/10 iters), loss = 7.03158
I0524 00:57:52.219722 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03158 (* 1 = 7.03158 loss)
I0524 00:57:52.219782 11835 sgd_solver.cpp:112] Iteration 15660, lr = 0.1
I0524 00:57:59.825896 11835 solver.cpp:239] Iteration 15670 (1.31477 iter/s, 7.60588s/10 iters), loss = 8.56511
I0524 00:57:59.825949 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.56511 (* 1 = 8.56511 loss)
I0524 00:57:59.826108 11835 sgd_solver.cpp:112] Iteration 15670, lr = 0.1
I0524 00:58:05.631108 11835 solver.cpp:239] Iteration 15680 (1.72267 iter/s, 5.80495s/10 iters), loss = 7.50258
I0524 00:58:05.631147 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.50258 (* 1 = 7.50258 loss)
I0524 00:58:05.631160 11835 sgd_solver.cpp:112] Iteration 15680, lr = 0.1
I0524 00:58:13.478588 11835 solver.cpp:239] Iteration 15690 (1.27436 iter/s, 7.84706s/10 iters), loss = 7.04872
I0524 00:58:13.478637 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04872 (* 1 = 7.04872 loss)
I0524 00:58:13.478736 11835 sgd_solver.cpp:112] Iteration 15690, lr = 0.1
I0524 00:58:20.811417 11835 solver.cpp:239] Iteration 15700 (1.36379 iter/s, 7.33251s/10 iters), loss = 8.19176
I0524 00:58:20.811614 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.19176 (* 1 = 8.19176 loss)
I0524 00:58:21.036936 11835 sgd_solver.cpp:112] Iteration 15700, lr = 0.1
I0524 00:58:26.846290 11835 solver.cpp:239] Iteration 15710 (1.65714 iter/s, 6.03448s/10 iters), loss = 7.67956
I0524 00:58:26.846331 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.67956 (* 1 = 7.67956 loss)
I0524 00:58:27.370549 11835 sgd_solver.cpp:112] Iteration 15710, lr = 0.1
I0524 00:58:34.255542 11835 solver.cpp:239] Iteration 15720 (1.34972 iter/s, 7.40893s/10 iters), loss = 7.50725
I0524 00:58:34.255592 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.50725 (* 1 = 7.50725 loss)
I0524 00:58:34.751279 11835 sgd_solver.cpp:112] Iteration 15720, lr = 0.1
I0524 00:58:41.157901 11835 solver.cpp:239] Iteration 15730 (1.44885 iter/s, 6.90205s/10 iters), loss = 6.30819
I0524 00:58:41.157950 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30819 (* 1 = 6.30819 loss)
I0524 00:58:41.157965 11835 sgd_solver.cpp:112] Iteration 15730, lr = 0.1
I0524 00:58:47.922842 11835 solver.cpp:239] Iteration 15740 (1.47852 iter/s, 6.76352s/10 iters), loss = 7.46248
I0524 00:58:47.922895 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.46248 (* 1 = 7.46248 loss)
I0524 00:58:47.923053 11835 sgd_solver.cpp:112] Iteration 15740, lr = 0.1
I0524 00:58:55.273639 11835 solver.cpp:239] Iteration 15750 (1.36046 iter/s, 7.35047s/10 iters), loss = 7.66804
I0524 00:58:55.273793 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.66804 (* 1 = 7.66804 loss)
I0524 00:58:55.273813 11835 sgd_solver.cpp:112] Iteration 15750, lr = 0.1
I0524 00:59:02.148339 11835 solver.cpp:239] Iteration 15760 (1.4547 iter/s, 6.87426s/10 iters), loss = 7.22797
I0524 00:59:02.148380 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.22797 (* 1 = 7.22797 loss)
I0524 00:59:02.607522 11835 sgd_solver.cpp:112] Iteration 15760, lr = 0.1
I0524 00:59:10.577412 11835 solver.cpp:239] Iteration 15770 (1.18642 iter/s, 8.4287s/10 iters), loss = 7.30451
I0524 00:59:10.577468 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.30451 (* 1 = 7.30451 loss)
I0524 00:59:10.577575 11835 sgd_solver.cpp:112] Iteration 15770, lr = 0.1
I0524 00:59:18.265566 11835 solver.cpp:239] Iteration 15780 (1.30076 iter/s, 7.6878s/10 iters), loss = 7.94829
I0524 00:59:18.265626 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.94829 (* 1 = 7.94829 loss)
I0524 00:59:18.266021 11835 sgd_solver.cpp:112] Iteration 15780, lr = 0.1
I0524 00:59:25.503285 11835 solver.cpp:239] Iteration 15790 (1.38171 iter/s, 7.23739s/10 iters), loss = 6.48704
I0524 00:59:25.503527 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48704 (* 1 = 6.48704 loss)
I0524 00:59:25.503584 11835 sgd_solver.cpp:112] Iteration 15790, lr = 0.1
I0524 00:59:34.027936 11835 solver.cpp:239] Iteration 15800 (1.17316 iter/s, 8.524s/10 iters), loss = 8.0374
I0524 00:59:34.027986 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.0374 (* 1 = 8.0374 loss)
I0524 00:59:34.028120 11835 sgd_solver.cpp:112] Iteration 15800, lr = 0.1
I0524 00:59:39.690913 11835 solver.cpp:239] Iteration 15810 (1.76594 iter/s, 5.66272s/10 iters), loss = 6.97549
I0524 00:59:39.690955 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.97549 (* 1 = 6.97549 loss)
I0524 00:59:39.690969 11835 sgd_solver.cpp:112] Iteration 15810, lr = 0.1
I0524 00:59:47.044957 11835 solver.cpp:239] Iteration 15820 (1.35986 iter/s, 7.3537s/10 iters), loss = 7.43408
I0524 00:59:47.045011 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.43408 (* 1 = 7.43408 loss)
I0524 00:59:47.838642 11835 sgd_solver.cpp:112] Iteration 15820, lr = 0.1
I0524 00:59:54.145334 11835 solver.cpp:239] Iteration 15830 (1.40844 iter/s, 7.10006s/10 iters), loss = 7.73776
I0524 00:59:54.145383 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.73776 (* 1 = 7.73776 loss)
I0524 00:59:54.145570 11835 sgd_solver.cpp:112] Iteration 15830, lr = 0.1
I0524 01:00:01.159415 11835 solver.cpp:239] Iteration 15840 (1.42577 iter/s, 7.01376s/10 iters), loss = 7.83444
I0524 01:00:01.159651 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.83444 (* 1 = 7.83444 loss)
I0524 01:00:01.159701 11835 sgd_solver.cpp:112] Iteration 15840, lr = 0.1
I0524 01:00:07.773504 11835 solver.cpp:239] Iteration 15850 (1.51202 iter/s, 6.61365s/10 iters), loss = 7.61913
I0524 01:00:07.773556 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.61913 (* 1 = 7.61913 loss)
I0524 01:00:07.883978 11835 sgd_solver.cpp:112] Iteration 15850, lr = 0.1
I0524 01:00:16.536258 11835 solver.cpp:239] Iteration 15860 (1.14124 iter/s, 8.76237s/10 iters), loss = 8.02215
I0524 01:00:16.536309 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.02215 (* 1 = 8.02215 loss)
I0524 01:00:17.435308 11835 sgd_solver.cpp:112] Iteration 15860, lr = 0.1
I0524 01:00:24.989342 11835 solver.cpp:239] Iteration 15870 (1.18305 iter/s, 8.45271s/10 iters), loss = 7.84002
I0524 01:00:24.989393 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.84002 (* 1 = 7.84002 loss)
I0524 01:00:24.989925 11835 sgd_solver.cpp:112] Iteration 15870, lr = 0.1
I0524 01:00:31.511080 11835 solver.cpp:239] Iteration 15880 (1.5334 iter/s, 6.52144s/10 iters), loss = 8.08837
I0524 01:00:31.511292 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.08837 (* 1 = 8.08837 loss)
I0524 01:00:31.628049 11835 sgd_solver.cpp:112] Iteration 15880, lr = 0.1
I0524 01:00:38.739490 11835 solver.cpp:239] Iteration 15890 (1.38353 iter/s, 7.22791s/10 iters), loss = 7.80459
I0524 01:00:38.739548 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.80459 (* 1 = 7.80459 loss)
I0524 01:00:38.739711 11835 sgd_solver.cpp:112] Iteration 15890, lr = 0.1
I0524 01:00:44.874029 11835 solver.cpp:239] Iteration 15900 (1.63019 iter/s, 6.13425s/10 iters), loss = 7.54738
I0524 01:00:44.874076 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.54738 (* 1 = 7.54738 loss)
I0524 01:00:44.874305 11835 sgd_solver.cpp:112] Iteration 15900, lr = 0.1
I0524 01:00:53.566540 11835 solver.cpp:239] Iteration 15910 (1.15047 iter/s, 8.69213s/10 iters), loss = 6.1694
I0524 01:00:53.566593 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1694 (* 1 = 6.1694 loss)
I0524 01:00:54.080634 11835 sgd_solver.cpp:112] Iteration 15910, lr = 0.1
I0524 01:01:00.177634 11835 solver.cpp:239] Iteration 15920 (1.51268 iter/s, 6.61079s/10 iters), loss = 7.74573
I0524 01:01:00.177673 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.74573 (* 1 = 7.74573 loss)
I0524 01:01:00.177685 11835 sgd_solver.cpp:112] Iteration 15920, lr = 0.1
I0524 01:01:07.857664 11835 solver.cpp:239] Iteration 15930 (1.30215 iter/s, 7.67959s/10 iters), loss = 9.36621
I0524 01:01:07.857797 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.36621 (* 1 = 9.36621 loss)
I0524 01:01:07.863005 11835 sgd_solver.cpp:112] Iteration 15930, lr = 0.1
I0524 01:01:14.059697 11835 solver.cpp:239] Iteration 15940 (1.61247 iter/s, 6.20167s/10 iters), loss = 6.91395
I0524 01:01:14.059743 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.91395 (* 1 = 6.91395 loss)
I0524 01:01:14.059759 11835 sgd_solver.cpp:112] Iteration 15940, lr = 0.1
I0524 01:01:22.523496 11835 solver.cpp:239] Iteration 15950 (1.18187 iter/s, 8.46119s/10 iters), loss = 6.81518
I0524 01:01:22.523552 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81518 (* 1 = 6.81518 loss)
I0524 01:01:22.523876 11835 sgd_solver.cpp:112] Iteration 15950, lr = 0.1
I0524 01:01:30.195596 11835 solver.cpp:239] Iteration 15960 (1.30348 iter/s, 7.67175s/10 iters), loss = 7.88732
I0524 01:01:30.195646 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.88732 (* 1 = 7.88732 loss)
I0524 01:01:30.261541 11835 sgd_solver.cpp:112] Iteration 15960, lr = 0.1
I0524 01:01:37.555166 11835 solver.cpp:239] Iteration 15970 (1.35884 iter/s, 7.35924s/10 iters), loss = 7.98306
I0524 01:01:37.555217 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.98306 (* 1 = 7.98306 loss)
I0524 01:01:37.555234 11835 sgd_solver.cpp:112] Iteration 15970, lr = 0.1
I0524 01:01:44.618028 11835 solver.cpp:239] Iteration 15980 (1.41614 iter/s, 7.06144s/10 iters), loss = 7.38171
I0524 01:01:44.618296 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.38171 (* 1 = 7.38171 loss)
I0524 01:01:45.665416 11835 sgd_solver.cpp:112] Iteration 15980, lr = 0.1
I0524 01:01:53.193634 11835 solver.cpp:239] Iteration 15990 (1.16618 iter/s, 8.57504s/10 iters), loss = 7.0946
I0524 01:01:53.193688 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.0946 (* 1 = 7.0946 loss)
I0524 01:01:53.194119 11835 sgd_solver.cpp:112] Iteration 15990, lr = 0.1
I0524 01:01:59.961213 11835 solver.cpp:239] Iteration 16000 (1.4777 iter/s, 6.76727s/10 iters), loss = 7.15832
I0524 01:01:59.961267 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.15832 (* 1 = 7.15832 loss)
I0524 01:01:59.961470 11835 sgd_solver.cpp:112] Iteration 16000, lr = 0.1
I0524 01:02:07.693161 11835 solver.cpp:239] Iteration 16010 (1.29339 iter/s, 7.73159s/10 iters), loss = 7.17381
I0524 01:02:07.693214 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.17381 (* 1 = 7.17381 loss)
I0524 01:02:08.134671 11835 sgd_solver.cpp:112] Iteration 16010, lr = 0.1
I0524 01:02:15.268312 11835 solver.cpp:239] Iteration 16020 (1.32017 iter/s, 7.57481s/10 iters), loss = 7.56724
I0524 01:02:15.268565 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.56724 (* 1 = 7.56724 loss)
I0524 01:02:15.268609 11835 sgd_solver.cpp:112] Iteration 16020, lr = 0.1
I0524 01:02:22.714128 11835 solver.cpp:239] Iteration 16030 (1.34332 iter/s, 7.44425s/10 iters), loss = 6.73331
I0524 01:02:22.714179 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73331 (* 1 = 6.73331 loss)
I0524 01:02:22.714195 11835 sgd_solver.cpp:112] Iteration 16030, lr = 0.1
I0524 01:02:28.857131 11835 solver.cpp:239] Iteration 16040 (1.62794 iter/s, 6.14272s/10 iters), loss = 7.56046
I0524 01:02:28.857168 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.56046 (* 1 = 7.56046 loss)
I0524 01:02:29.824071 11835 sgd_solver.cpp:112] Iteration 16040, lr = 0.1
I0524 01:02:36.766196 11835 solver.cpp:239] Iteration 16050 (1.26443 iter/s, 7.90871s/10 iters), loss = 7.04296
I0524 01:02:36.766255 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04296 (* 1 = 7.04296 loss)
I0524 01:02:36.766772 11835 sgd_solver.cpp:112] Iteration 16050, lr = 0.1
I0524 01:02:43.157788 11835 solver.cpp:239] Iteration 16060 (1.56463 iter/s, 6.39128s/10 iters), loss = 7.76943
I0524 01:02:43.157835 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.76943 (* 1 = 7.76943 loss)
I0524 01:02:43.289216 11835 sgd_solver.cpp:112] Iteration 16060, lr = 0.1
I0524 01:02:51.918613 11835 solver.cpp:239] Iteration 16070 (1.14149 iter/s, 8.76045s/10 iters), loss = 7.29751
I0524 01:02:51.918869 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.29751 (* 1 = 7.29751 loss)
I0524 01:02:53.017421 11835 sgd_solver.cpp:112] Iteration 16070, lr = 0.1
I0524 01:03:01.020967 11835 solver.cpp:239] Iteration 16080 (1.09869 iter/s, 9.10179s/10 iters), loss = 7.19278
I0524 01:03:01.021024 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.19278 (* 1 = 7.19278 loss)
I0524 01:03:01.021265 11835 sgd_solver.cpp:112] Iteration 16080, lr = 0.1
I0524 01:03:06.662518 11835 solver.cpp:239] Iteration 16090 (1.77265 iter/s, 5.64128s/10 iters), loss = 6.81295
I0524 01:03:06.662564 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81295 (* 1 = 6.81295 loss)
I0524 01:03:06.662577 11835 sgd_solver.cpp:112] Iteration 16090, lr = 0.1
I0524 01:03:14.209154 11835 solver.cpp:239] Iteration 16100 (1.32516 iter/s, 7.54624s/10 iters), loss = 7.89537
I0524 01:03:14.209201 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.89537 (* 1 = 7.89537 loss)
I0524 01:03:14.209327 11835 sgd_solver.cpp:112] Iteration 16100, lr = 0.1
I0524 01:03:21.419358 11835 solver.cpp:239] Iteration 16110 (1.38698 iter/s, 7.20989s/10 iters), loss = 7.21593
I0524 01:03:21.419399 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.21593 (* 1 = 7.21593 loss)
I0524 01:03:21.419987 11835 sgd_solver.cpp:112] Iteration 16110, lr = 0.1
I0524 01:03:27.614928 11835 solver.cpp:239] Iteration 16120 (1.61413 iter/s, 6.19528s/10 iters), loss = 7.77089
I0524 01:03:27.615164 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.77089 (* 1 = 7.77089 loss)
I0524 01:03:27.619616 11835 sgd_solver.cpp:112] Iteration 16120, lr = 0.1
I0524 01:03:34.570940 11835 solver.cpp:239] Iteration 16130 (1.4377 iter/s, 6.95556s/10 iters), loss = 6.62352
I0524 01:03:34.570991 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62352 (* 1 = 6.62352 loss)
I0524 01:03:34.571149 11835 sgd_solver.cpp:112] Iteration 16130, lr = 0.1
I0524 01:03:42.799137 11835 solver.cpp:239] Iteration 16140 (1.21539 iter/s, 8.22784s/10 iters), loss = 7.00829
I0524 01:03:42.799173 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00829 (* 1 = 7.00829 loss)
I0524 01:03:42.799325 11835 sgd_solver.cpp:112] Iteration 16140, lr = 0.1
I0524 01:03:48.869587 11835 solver.cpp:239] Iteration 16150 (1.6474 iter/s, 6.07018s/10 iters), loss = 7.52249
I0524 01:03:48.869624 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.52249 (* 1 = 7.52249 loss)
I0524 01:03:49.471303 11835 sgd_solver.cpp:112] Iteration 16150, lr = 0.1
I0524 01:03:56.806166 11835 solver.cpp:239] Iteration 16160 (1.26004 iter/s, 7.93624s/10 iters), loss = 7.28455
I0524 01:03:56.806215 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.28455 (* 1 = 7.28455 loss)
I0524 01:03:56.806227 11835 sgd_solver.cpp:112] Iteration 16160, lr = 0.1
I0524 01:04:03.536610 11835 solver.cpp:239] Iteration 16170 (1.4861 iter/s, 6.72903s/10 iters), loss = 7.08264
I0524 01:04:03.536762 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.08264 (* 1 = 7.08264 loss)
I0524 01:04:03.536881 11835 sgd_solver.cpp:112] Iteration 16170, lr = 0.1
I0524 01:04:09.300348 11835 solver.cpp:239] Iteration 16180 (1.73509 iter/s, 5.76338s/10 iters), loss = 6.94982
I0524 01:04:09.300390 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.94982 (* 1 = 6.94982 loss)
I0524 01:04:09.300529 11835 sgd_solver.cpp:112] Iteration 16180, lr = 0.1
I0524 01:04:17.239447 11835 solver.cpp:239] Iteration 16190 (1.25965 iter/s, 7.93874s/10 iters), loss = 8.47144
I0524 01:04:17.239502 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.47144 (* 1 = 8.47144 loss)
I0524 01:04:17.239703 11835 sgd_solver.cpp:112] Iteration 16190, lr = 0.1
I0524 01:04:23.114679 11835 solver.cpp:239] Iteration 16200 (1.70214 iter/s, 5.87495s/10 iters), loss = 6.72508
I0524 01:04:23.114764 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72508 (* 1 = 6.72508 loss)
I0524 01:04:23.311116 11835 sgd_solver.cpp:112] Iteration 16200, lr = 0.1
I0524 01:04:31.596081 11835 solver.cpp:239] Iteration 16210 (1.17911 iter/s, 8.481s/10 iters), loss = 7.1769
I0524 01:04:31.596132 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.1769 (* 1 = 7.1769 loss)
I0524 01:04:31.596488 11835 sgd_solver.cpp:112] Iteration 16210, lr = 0.1
I0524 01:04:38.723111 11835 solver.cpp:239] Iteration 16220 (1.40317 iter/s, 7.12671s/10 iters), loss = 6.48687
I0524 01:04:38.723325 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48687 (* 1 = 6.48687 loss)
I0524 01:04:38.723392 11835 sgd_solver.cpp:112] Iteration 16220, lr = 0.1
I0524 01:04:44.642154 11835 solver.cpp:239] Iteration 16230 (1.68958 iter/s, 5.91862s/10 iters), loss = 8.0914
I0524 01:04:44.642192 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.0914 (* 1 = 8.0914 loss)
I0524 01:04:44.642207 11835 sgd_solver.cpp:112] Iteration 16230, lr = 0.1
I0524 01:04:51.752167 11835 solver.cpp:239] Iteration 16240 (1.40653 iter/s, 7.10969s/10 iters), loss = 7.23455
I0524 01:04:51.752218 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.23455 (* 1 = 7.23455 loss)
I0524 01:04:51.808111 11835 sgd_solver.cpp:112] Iteration 16240, lr = 0.1
I0524 01:05:00.037349 11835 solver.cpp:239] Iteration 16250 (1.20703 iter/s, 8.28483s/10 iters), loss = 7.28564
I0524 01:05:00.037394 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.28564 (* 1 = 7.28564 loss)
I0524 01:05:00.037657 11835 sgd_solver.cpp:112] Iteration 16250, lr = 0.1
I0524 01:05:06.974385 11835 solver.cpp:239] Iteration 16260 (1.4416 iter/s, 6.93672s/10 iters), loss = 6.53644
I0524 01:05:06.974438 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53644 (* 1 = 6.53644 loss)
I0524 01:05:06.974531 11835 sgd_solver.cpp:112] Iteration 16260, lr = 0.1
I0524 01:05:13.063870 11835 solver.cpp:239] Iteration 16270 (1.64225 iter/s, 6.0892s/10 iters), loss = 7.87278
I0524 01:05:13.064128 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.87278 (* 1 = 7.87278 loss)
I0524 01:05:13.108582 11835 sgd_solver.cpp:112] Iteration 16270, lr = 0.1
I0524 01:05:19.772241 11835 solver.cpp:239] Iteration 16280 (1.49078 iter/s, 6.7079s/10 iters), loss = 6.91825
I0524 01:05:19.772276 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.91825 (* 1 = 6.91825 loss)
I0524 01:05:19.772289 11835 sgd_solver.cpp:112] Iteration 16280, lr = 0.1
I0524 01:05:26.019821 11835 solver.cpp:239] Iteration 16290 (1.60072 iter/s, 6.24719s/10 iters), loss = 7.09771
I0524 01:05:26.019876 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09771 (* 1 = 7.09771 loss)
I0524 01:05:26.020089 11835 sgd_solver.cpp:112] Iteration 16290, lr = 0.1
I0524 01:05:32.871925 11835 solver.cpp:239] Iteration 16300 (1.45947 iter/s, 6.85179s/10 iters), loss = 8.77242
I0524 01:05:32.871978 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.77242 (* 1 = 8.77242 loss)
I0524 01:05:32.872122 11835 sgd_solver.cpp:112] Iteration 16300, lr = 0.1
I0524 01:05:40.161290 11835 solver.cpp:239] Iteration 16310 (1.37192 iter/s, 7.28903s/10 iters), loss = 7.10541
I0524 01:05:40.161345 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10541 (* 1 = 7.10541 loss)
I0524 01:05:40.226228 11835 sgd_solver.cpp:112] Iteration 16310, lr = 0.1
I0524 01:05:46.505785 11835 solver.cpp:239] Iteration 16320 (1.57624 iter/s, 6.3442s/10 iters), loss = 6.35605
I0524 01:05:46.506024 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35605 (* 1 = 6.35605 loss)
I0524 01:05:46.596801 11835 sgd_solver.cpp:112] Iteration 16320, lr = 0.1
I0524 01:05:52.596247 11835 solver.cpp:239] Iteration 16330 (1.64203 iter/s, 6.09003s/10 iters), loss = 6.3023
I0524 01:05:52.596299 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3023 (* 1 = 6.3023 loss)
I0524 01:05:52.600656 11835 sgd_solver.cpp:112] Iteration 16330, lr = 0.1
I0524 01:05:58.332965 11835 solver.cpp:239] Iteration 16340 (1.74324 iter/s, 5.73645s/10 iters), loss = 7.54851
I0524 01:05:58.333014 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.54851 (* 1 = 7.54851 loss)
I0524 01:05:58.862269 11835 sgd_solver.cpp:112] Iteration 16340, lr = 0.1
I0524 01:06:06.256772 11835 solver.cpp:239] Iteration 16350 (1.26208 iter/s, 7.92346s/10 iters), loss = 7.91364
I0524 01:06:06.256819 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.91364 (* 1 = 7.91364 loss)
I0524 01:06:06.256940 11835 sgd_solver.cpp:112] Iteration 16350, lr = 0.1
I0524 01:06:14.169781 11835 solver.cpp:239] Iteration 16360 (1.2638 iter/s, 7.91266s/10 iters), loss = 7.63227
I0524 01:06:14.169836 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.63227 (* 1 = 7.63227 loss)
I0524 01:06:14.170209 11835 sgd_solver.cpp:112] Iteration 16360, lr = 0.1
I0524 01:06:20.650100 11835 solver.cpp:239] Iteration 16370 (1.5432 iter/s, 6.48003s/10 iters), loss = 7.50924
I0524 01:06:20.650235 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.50924 (* 1 = 7.50924 loss)
I0524 01:06:20.758471 11835 sgd_solver.cpp:112] Iteration 16370, lr = 0.1
I0524 01:06:26.618306 11835 solver.cpp:239] Iteration 16380 (1.67565 iter/s, 5.96782s/10 iters), loss = 7.23028
I0524 01:06:26.618369 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.23028 (* 1 = 7.23028 loss)
I0524 01:06:26.618738 11835 sgd_solver.cpp:112] Iteration 16380, lr = 0.1
I0524 01:06:34.089506 11835 solver.cpp:239] Iteration 16390 (1.33853 iter/s, 7.47086s/10 iters), loss = 7.91096
I0524 01:06:34.089547 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.91096 (* 1 = 7.91096 loss)
I0524 01:06:34.089669 11835 sgd_solver.cpp:112] Iteration 16390, lr = 0.1
I0524 01:06:41.894167 11835 solver.cpp:239] Iteration 16400 (1.28134 iter/s, 7.80432s/10 iters), loss = 6.92129
I0524 01:06:41.894219 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92129 (* 1 = 6.92129 loss)
I0524 01:06:41.894330 11835 sgd_solver.cpp:112] Iteration 16400, lr = 0.1
I0524 01:06:49.088979 11835 solver.cpp:239] Iteration 16410 (1.38995 iter/s, 7.19448s/10 iters), loss = 7.48378
I0524 01:06:49.089025 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.48378 (* 1 = 7.48378 loss)
I0524 01:06:49.094677 11835 sgd_solver.cpp:112] Iteration 16410, lr = 0.1
I0524 01:06:56.744613 11835 solver.cpp:239] Iteration 16420 (1.30629 iter/s, 7.65529s/10 iters), loss = 7.97751
I0524 01:06:56.744894 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.97751 (* 1 = 7.97751 loss)
I0524 01:06:56.744948 11835 sgd_solver.cpp:112] Iteration 16420, lr = 0.1
I0524 01:07:03.030424 11835 solver.cpp:239] Iteration 16430 (1.591 iter/s, 6.28534s/10 iters), loss = 7.78231
I0524 01:07:03.030462 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.78231 (* 1 = 7.78231 loss)
I0524 01:07:03.030475 11835 sgd_solver.cpp:112] Iteration 16430, lr = 0.1
I0524 01:07:11.037621 11835 solver.cpp:239] Iteration 16440 (1.24894 iter/s, 8.00677s/10 iters), loss = 7.92206
I0524 01:07:11.037681 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.92206 (* 1 = 7.92206 loss)
I0524 01:07:11.037963 11835 sgd_solver.cpp:112] Iteration 16440, lr = 0.1
I0524 01:07:16.947829 11835 solver.cpp:239] Iteration 16450 (1.69207 iter/s, 5.90992s/10 iters), loss = 7.05383
I0524 01:07:16.947880 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.05383 (* 1 = 7.05383 loss)
I0524 01:07:16.948132 11835 sgd_solver.cpp:112] Iteration 16450, lr = 0.1
I0524 01:07:23.570917 11835 solver.cpp:239] Iteration 16460 (1.50994 iter/s, 6.62279s/10 iters), loss = 7.37278
I0524 01:07:23.570966 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.37278 (* 1 = 7.37278 loss)
I0524 01:07:23.594396 11835 sgd_solver.cpp:112] Iteration 16460, lr = 0.1
I0524 01:07:30.403157 11835 solver.cpp:239] Iteration 16470 (1.46371 iter/s, 6.83193s/10 iters), loss = 7.27463
I0524 01:07:30.403373 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.27463 (* 1 = 7.27463 loss)
I0524 01:07:30.403419 11835 sgd_solver.cpp:112] Iteration 16470, lr = 0.1
I0524 01:07:36.751096 11835 solver.cpp:239] Iteration 16480 (1.57543 iter/s, 6.34749s/10 iters), loss = 8.48614
I0524 01:07:36.751157 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.48614 (* 1 = 8.48614 loss)
I0524 01:07:36.751312 11835 sgd_solver.cpp:112] Iteration 16480, lr = 0.1
I0524 01:07:42.491552 11835 solver.cpp:239] Iteration 16490 (1.7421 iter/s, 5.74019s/10 iters), loss = 6.78867
I0524 01:07:42.491595 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78867 (* 1 = 6.78867 loss)
I0524 01:07:42.647064 11835 sgd_solver.cpp:112] Iteration 16490, lr = 0.1
I0524 01:07:50.786973 11835 solver.cpp:239] Iteration 16500 (1.20554 iter/s, 8.29505s/10 iters), loss = 7.73421
I0524 01:07:50.787032 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.73421 (* 1 = 7.73421 loss)
I0524 01:07:50.787418 11835 sgd_solver.cpp:112] Iteration 16500, lr = 0.1
I0524 01:07:57.142572 11835 solver.cpp:239] Iteration 16510 (1.57349 iter/s, 6.35531s/10 iters), loss = 7.41296
I0524 01:07:57.142616 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.41296 (* 1 = 7.41296 loss)
I0524 01:07:57.142629 11835 sgd_solver.cpp:112] Iteration 16510, lr = 0.1
I0524 01:08:04.071811 11835 solver.cpp:239] Iteration 16520 (1.44325 iter/s, 6.92881s/10 iters), loss = 7.39417
I0524 01:08:04.072043 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.39417 (* 1 = 7.39417 loss)
I0524 01:08:04.072100 11835 sgd_solver.cpp:112] Iteration 16520, lr = 0.1
I0524 01:08:09.952404 11835 solver.cpp:239] Iteration 16530 (1.7009 iter/s, 5.87924s/10 iters), loss = 8.24083
I0524 01:08:09.952450 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.24083 (* 1 = 8.24083 loss)
I0524 01:08:11.335176 11835 sgd_solver.cpp:112] Iteration 16530, lr = 0.1
I0524 01:08:18.288837 11835 solver.cpp:239] Iteration 16540 (1.19961 iter/s, 8.33607s/10 iters), loss = 6.85812
I0524 01:08:18.288882 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.85812 (* 1 = 6.85812 loss)
I0524 01:08:18.288978 11835 sgd_solver.cpp:112] Iteration 16540, lr = 0.1
I0524 01:08:24.964802 11835 solver.cpp:239] Iteration 16550 (1.49798 iter/s, 6.67565s/10 iters), loss = 8.28947
I0524 01:08:24.964860 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.28947 (* 1 = 8.28947 loss)
I0524 01:08:24.965059 11835 sgd_solver.cpp:112] Iteration 16550, lr = 0.1
I0524 01:08:31.393885 11835 solver.cpp:239] Iteration 16560 (1.55551 iter/s, 6.42877s/10 iters), loss = 7.29682
I0524 01:08:31.393932 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.29682 (* 1 = 7.29682 loss)
I0524 01:08:31.545730 11835 sgd_solver.cpp:112] Iteration 16560, lr = 0.1
I0524 01:08:37.371657 11835 solver.cpp:239] Iteration 16570 (1.67294 iter/s, 5.9775s/10 iters), loss = 8.65847
I0524 01:08:37.371778 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.65847 (* 1 = 8.65847 loss)
I0524 01:08:37.371793 11835 sgd_solver.cpp:112] Iteration 16570, lr = 0.1
I0524 01:08:44.873518 11835 solver.cpp:239] Iteration 16580 (1.33308 iter/s, 7.50142s/10 iters), loss = 6.80038
I0524 01:08:44.873570 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80038 (* 1 = 6.80038 loss)
I0524 01:08:45.441694 11835 sgd_solver.cpp:112] Iteration 16580, lr = 0.1
I0524 01:08:54.684545 11835 solver.cpp:239] Iteration 16590 (1.01931 iter/s, 9.81061s/10 iters), loss = 8.15627
I0524 01:08:54.684592 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.15627 (* 1 = 8.15627 loss)
I0524 01:08:55.643882 11835 sgd_solver.cpp:112] Iteration 16590, lr = 0.1
I0524 01:09:04.785403 11835 solver.cpp:239] Iteration 16600 (0.990058 iter/s, 10.1004s/10 iters), loss = 8.24088
I0524 01:09:04.785461 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.24088 (* 1 = 8.24088 loss)
I0524 01:09:05.124747 11835 sgd_solver.cpp:112] Iteration 16600, lr = 0.1
I0524 01:09:10.833318 11835 solver.cpp:239] Iteration 16610 (1.65354 iter/s, 6.04763s/10 iters), loss = 6.92254
I0524 01:09:10.833566 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92254 (* 1 = 6.92254 loss)
I0524 01:09:10.833619 11835 sgd_solver.cpp:112] Iteration 16610, lr = 0.1
I0524 01:09:16.837972 11835 solver.cpp:239] Iteration 16620 (1.66556 iter/s, 6.004s/10 iters), loss = 7.69509
I0524 01:09:16.838019 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.69509 (* 1 = 7.69509 loss)
I0524 01:09:16.838194 11835 sgd_solver.cpp:112] Iteration 16620, lr = 0.1
I0524 01:09:25.165064 11835 solver.cpp:239] Iteration 16630 (1.20095 iter/s, 8.32673s/10 iters), loss = 7.49476
I0524 01:09:25.165110 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.49476 (* 1 = 7.49476 loss)
I0524 01:09:25.472098 11835 sgd_solver.cpp:112] Iteration 16630, lr = 0.1
I0524 01:09:31.700693 11835 solver.cpp:239] Iteration 16640 (1.53014 iter/s, 6.53534s/10 iters), loss = 7.42948
I0524 01:09:31.700742 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.42948 (* 1 = 7.42948 loss)
I0524 01:09:32.150961 11835 sgd_solver.cpp:112] Iteration 16640, lr = 0.1
I0524 01:09:40.374100 11835 solver.cpp:239] Iteration 16650 (1.153 iter/s, 8.67302s/10 iters), loss = 6.08919
I0524 01:09:40.374161 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08919 (* 1 = 6.08919 loss)
I0524 01:09:40.374336 11835 sgd_solver.cpp:112] Iteration 16650, lr = 0.1
I0524 01:09:47.190084 11835 solver.cpp:239] Iteration 16660 (1.46721 iter/s, 6.81566s/10 iters), loss = 7.77072
I0524 01:09:47.190232 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.77072 (* 1 = 7.77072 loss)
I0524 01:09:48.385083 11835 sgd_solver.cpp:112] Iteration 16660, lr = 0.1
I0524 01:09:56.818739 11835 solver.cpp:239] Iteration 16670 (1.03863 iter/s, 9.6281s/10 iters), loss = 6.86592
I0524 01:09:56.818802 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86592 (* 1 = 6.86592 loss)
I0524 01:09:56.818825 11835 sgd_solver.cpp:112] Iteration 16670, lr = 0.1
I0524 01:10:04.271064 11835 solver.cpp:239] Iteration 16680 (1.34192 iter/s, 7.45199s/10 iters), loss = 9.08918
I0524 01:10:04.271109 11835 solver.cpp:258]     Train net output #0: softmax_loss = 9.08918 (* 1 = 9.08918 loss)
I0524 01:10:04.271294 11835 sgd_solver.cpp:112] Iteration 16680, lr = 0.1
I0524 01:10:10.040688 11835 solver.cpp:239] Iteration 16690 (1.7333 iter/s, 5.76935s/10 iters), loss = 7.09335
I0524 01:10:10.040737 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09335 (* 1 = 7.09335 loss)
I0524 01:10:10.040753 11835 sgd_solver.cpp:112] Iteration 16690, lr = 0.1
I0524 01:10:18.277804 11835 solver.cpp:239] Iteration 16700 (1.21408 iter/s, 8.23668s/10 iters), loss = 7.58858
I0524 01:10:18.277971 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.58858 (* 1 = 7.58858 loss)
I0524 01:10:18.278319 11835 sgd_solver.cpp:112] Iteration 16700, lr = 0.1
I0524 01:10:23.886737 11835 solver.cpp:239] Iteration 16710 (1.78299 iter/s, 5.60855s/10 iters), loss = 7.60255
I0524 01:10:23.886783 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.60255 (* 1 = 7.60255 loss)
I0524 01:10:23.886797 11835 sgd_solver.cpp:112] Iteration 16710, lr = 0.1
I0524 01:10:31.360572 11835 solver.cpp:239] Iteration 16720 (1.33807 iter/s, 7.47344s/10 iters), loss = 6.89802
I0524 01:10:31.360617 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89802 (* 1 = 6.89802 loss)
I0524 01:10:31.360878 11835 sgd_solver.cpp:112] Iteration 16720, lr = 0.1
I0524 01:10:38.253681 11835 solver.cpp:239] Iteration 16730 (1.45079 iter/s, 6.89278s/10 iters), loss = 7.05401
I0524 01:10:38.253741 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.05401 (* 1 = 7.05401 loss)
I0524 01:10:38.253801 11835 sgd_solver.cpp:112] Iteration 16730, lr = 0.1
I0524 01:10:43.971140 11835 solver.cpp:239] Iteration 16740 (1.74911 iter/s, 5.71718s/10 iters), loss = 8.39596
I0524 01:10:43.971192 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.39596 (* 1 = 8.39596 loss)
I0524 01:10:44.630070 11835 sgd_solver.cpp:112] Iteration 16740, lr = 0.1
I0524 01:10:50.283021 11835 solver.cpp:239] Iteration 16750 (1.58439 iter/s, 6.31159s/10 iters), loss = 7.47532
I0524 01:10:50.283125 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.47532 (* 1 = 7.47532 loss)
I0524 01:10:50.283140 11835 sgd_solver.cpp:112] Iteration 16750, lr = 0.1
I0524 01:10:57.383055 11835 solver.cpp:239] Iteration 16760 (1.40856 iter/s, 7.09947s/10 iters), loss = 7.72157
I0524 01:10:57.383106 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.72157 (* 1 = 7.72157 loss)
I0524 01:10:57.555819 11835 sgd_solver.cpp:112] Iteration 16760, lr = 0.1
I0524 01:11:03.319824 11835 solver.cpp:239] Iteration 16770 (1.6845 iter/s, 5.93649s/10 iters), loss = 6.98607
I0524 01:11:03.319874 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98607 (* 1 = 6.98607 loss)
I0524 01:11:03.319890 11835 sgd_solver.cpp:112] Iteration 16770, lr = 0.1
I0524 01:11:09.871803 11835 solver.cpp:239] Iteration 16780 (1.52634 iter/s, 6.55162s/10 iters), loss = 7.08641
I0524 01:11:09.871841 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.08641 (* 1 = 7.08641 loss)
I0524 01:11:09.871989 11835 sgd_solver.cpp:112] Iteration 16780, lr = 0.1
I0524 01:11:16.916617 11835 solver.cpp:239] Iteration 16790 (1.41955 iter/s, 7.04449s/10 iters), loss = 6.67001
I0524 01:11:16.916680 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67001 (* 1 = 6.67001 loss)
I0524 01:11:16.916910 11835 sgd_solver.cpp:112] Iteration 16790, lr = 0.1
I0524 01:11:25.109395 11835 solver.cpp:239] Iteration 16800 (1.22064 iter/s, 8.19241s/10 iters), loss = 7.22503
I0524 01:11:25.109643 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.22503 (* 1 = 7.22503 loss)
I0524 01:11:26.130043 11835 sgd_solver.cpp:112] Iteration 16800, lr = 0.1
I0524 01:11:34.696382 11835 solver.cpp:239] Iteration 16810 (1.04314 iter/s, 9.58642s/10 iters), loss = 7.60012
I0524 01:11:34.696429 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.60012 (* 1 = 7.60012 loss)
I0524 01:11:35.766574 11835 sgd_solver.cpp:112] Iteration 16810, lr = 0.1
I0524 01:11:41.937994 11835 solver.cpp:239] Iteration 16820 (1.38097 iter/s, 7.24129s/10 iters), loss = 7.16314
I0524 01:11:41.938036 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.16314 (* 1 = 7.16314 loss)
I0524 01:11:41.938066 11835 sgd_solver.cpp:112] Iteration 16820, lr = 0.1
I0524 01:11:49.291285 11835 solver.cpp:239] Iteration 16830 (1.36 iter/s, 7.35296s/10 iters), loss = 7.50865
I0524 01:11:49.291332 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.50865 (* 1 = 7.50865 loss)
I0524 01:11:49.291391 11835 sgd_solver.cpp:112] Iteration 16830, lr = 0.1
I0524 01:11:55.128365 11835 solver.cpp:239] Iteration 16840 (1.71326 iter/s, 5.83682s/10 iters), loss = 7.41719
I0524 01:11:55.128530 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.41719 (* 1 = 7.41719 loss)
I0524 01:11:55.128545 11835 sgd_solver.cpp:112] Iteration 16840, lr = 0.1
I0524 01:12:01.070947 11835 solver.cpp:239] Iteration 16850 (1.68289 iter/s, 5.94214s/10 iters), loss = 7.45419
I0524 01:12:01.071003 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.45419 (* 1 = 7.45419 loss)
I0524 01:12:01.071135 11835 sgd_solver.cpp:112] Iteration 16850, lr = 0.1
I0524 01:12:07.431177 11835 solver.cpp:239] Iteration 16860 (1.57235 iter/s, 6.35991s/10 iters), loss = 6.26013
I0524 01:12:07.431231 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26013 (* 1 = 6.26013 loss)
I0524 01:12:07.431442 11835 sgd_solver.cpp:112] Iteration 16860, lr = 0.1
I0524 01:12:14.920848 11835 solver.cpp:239] Iteration 16870 (1.33523 iter/s, 7.48934s/10 iters), loss = 7.62851
I0524 01:12:14.920889 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.62851 (* 1 = 7.62851 loss)
I0524 01:12:14.920989 11835 sgd_solver.cpp:112] Iteration 16870, lr = 0.1
I0524 01:12:21.705456 11835 solver.cpp:239] Iteration 16880 (1.47399 iter/s, 6.7843s/10 iters), loss = 7.32664
I0524 01:12:21.705505 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.32664 (* 1 = 7.32664 loss)
I0524 01:12:21.705575 11835 sgd_solver.cpp:112] Iteration 16880, lr = 0.1
I0524 01:12:27.287961 11835 solver.cpp:239] Iteration 16890 (1.79139 iter/s, 5.58225s/10 iters), loss = 7.57803
I0524 01:12:27.288065 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.57803 (* 1 = 7.57803 loss)
I0524 01:12:27.288084 11835 sgd_solver.cpp:112] Iteration 16890, lr = 0.1
I0524 01:12:33.330200 11835 solver.cpp:239] Iteration 16900 (1.65512 iter/s, 6.04187s/10 iters), loss = 7.96527
I0524 01:12:33.330248 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.96527 (* 1 = 7.96527 loss)
I0524 01:12:33.330262 11835 sgd_solver.cpp:112] Iteration 16900, lr = 0.1
I0524 01:12:39.951019 11835 solver.cpp:239] Iteration 16910 (1.51054 iter/s, 6.62013s/10 iters), loss = 7.56628
I0524 01:12:39.951071 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.56628 (* 1 = 7.56628 loss)
I0524 01:12:39.953006 11835 sgd_solver.cpp:112] Iteration 16910, lr = 0.1
I0524 01:12:46.227540 11835 solver.cpp:239] Iteration 16920 (1.59331 iter/s, 6.27624s/10 iters), loss = 6.62547
I0524 01:12:46.227576 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62547 (* 1 = 6.62547 loss)
I0524 01:12:46.227710 11835 sgd_solver.cpp:112] Iteration 16920, lr = 0.1
I0524 01:12:53.071127 11835 solver.cpp:239] Iteration 16930 (1.46129 iter/s, 6.84328s/10 iters), loss = 7.40438
I0524 01:12:53.071185 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.40438 (* 1 = 7.40438 loss)
I0524 01:12:53.071419 11835 sgd_solver.cpp:112] Iteration 16930, lr = 0.1
I0524 01:12:59.479382 11835 solver.cpp:239] Iteration 16940 (1.56056 iter/s, 6.40796s/10 iters), loss = 7.32526
I0524 01:12:59.479598 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.32526 (* 1 = 7.32526 loss)
I0524 01:12:59.479646 11835 sgd_solver.cpp:112] Iteration 16940, lr = 0.1
I0524 01:13:05.759943 11835 solver.cpp:239] Iteration 16950 (1.5926 iter/s, 6.27903s/10 iters), loss = 7.33692
I0524 01:13:05.759994 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.33692 (* 1 = 7.33692 loss)
I0524 01:13:05.760088 11835 sgd_solver.cpp:112] Iteration 16950, lr = 0.1
I0524 01:13:13.528313 11835 solver.cpp:239] Iteration 16960 (1.28733 iter/s, 7.76802s/10 iters), loss = 6.35553
I0524 01:13:13.528365 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35553 (* 1 = 6.35553 loss)
I0524 01:13:13.528692 11835 sgd_solver.cpp:112] Iteration 16960, lr = 0.1
I0524 01:13:19.617290 11835 solver.cpp:239] Iteration 16970 (1.64239 iter/s, 6.0887s/10 iters), loss = 6.69834
I0524 01:13:19.617332 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69834 (* 1 = 6.69834 loss)
I0524 01:13:19.719326 11835 sgd_solver.cpp:112] Iteration 16970, lr = 0.1
I0524 01:13:25.608978 11835 solver.cpp:239] Iteration 16980 (1.66906 iter/s, 5.99141s/10 iters), loss = 6.4736
I0524 01:13:25.609028 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4736 (* 1 = 6.4736 loss)
I0524 01:13:25.609156 11835 sgd_solver.cpp:112] Iteration 16980, lr = 0.1
I0524 01:13:31.466943 11835 solver.cpp:239] Iteration 16990 (1.70715 iter/s, 5.8577s/10 iters), loss = 6.76908
I0524 01:13:31.467227 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76908 (* 1 = 6.76908 loss)
I0524 01:13:31.467278 11835 sgd_solver.cpp:112] Iteration 16990, lr = 0.1
I0524 01:13:38.060123 11835 solver.cpp:239] Iteration 17000 (1.51732 iter/s, 6.59058s/10 iters), loss = 6.46648
I0524 01:13:38.060173 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46648 (* 1 = 6.46648 loss)
I0524 01:13:38.077615 11835 sgd_solver.cpp:112] Iteration 17000, lr = 0.1
I0524 01:13:44.124022 11835 solver.cpp:239] Iteration 17010 (1.64918 iter/s, 6.06361s/10 iters), loss = 7.09208
I0524 01:13:44.124073 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09208 (* 1 = 7.09208 loss)
I0524 01:13:44.124195 11835 sgd_solver.cpp:112] Iteration 17010, lr = 0.1
I0524 01:13:53.242867 11835 solver.cpp:239] Iteration 17020 (1.09668 iter/s, 9.11845s/10 iters), loss = 8.1607
I0524 01:13:53.242918 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.1607 (* 1 = 8.1607 loss)
I0524 01:13:53.243254 11835 sgd_solver.cpp:112] Iteration 17020, lr = 0.1
I0524 01:13:59.206578 11835 solver.cpp:239] Iteration 17030 (1.67689 iter/s, 5.96342s/10 iters), loss = 6.97163
I0524 01:13:59.206635 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.97163 (* 1 = 6.97163 loss)
I0524 01:13:59.207065 11835 sgd_solver.cpp:112] Iteration 17030, lr = 0.1
I0524 01:14:05.546676 11835 solver.cpp:239] Iteration 17040 (1.57734 iter/s, 6.33981s/10 iters), loss = 7.08829
I0524 01:14:05.546898 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.08829 (* 1 = 7.08829 loss)
I0524 01:14:05.546946 11835 sgd_solver.cpp:112] Iteration 17040, lr = 0.1
I0524 01:14:11.991492 11835 solver.cpp:239] Iteration 17050 (1.55174 iter/s, 6.44438s/10 iters), loss = 7.80671
I0524 01:14:11.991542 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.80671 (* 1 = 7.80671 loss)
I0524 01:14:11.991672 11835 sgd_solver.cpp:112] Iteration 17050, lr = 0.1
I0524 01:14:18.285778 11835 solver.cpp:239] Iteration 17060 (1.58881 iter/s, 6.294s/10 iters), loss = 7.2252
I0524 01:14:18.285816 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2252 (* 1 = 7.2252 loss)
I0524 01:14:18.285940 11835 sgd_solver.cpp:112] Iteration 17060, lr = 0.1
I0524 01:14:24.800843 11835 solver.cpp:239] Iteration 17070 (1.53497 iter/s, 6.51477s/10 iters), loss = 7.27895
I0524 01:14:24.800892 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.27895 (* 1 = 7.27895 loss)
I0524 01:14:24.832267 11835 sgd_solver.cpp:112] Iteration 17070, lr = 0.1
I0524 01:14:30.886297 11835 solver.cpp:239] Iteration 17080 (1.64334 iter/s, 6.08518s/10 iters), loss = 8.12999
I0524 01:14:30.886345 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.12999 (* 1 = 8.12999 loss)
I0524 01:14:30.930749 11835 sgd_solver.cpp:112] Iteration 17080, lr = 0.1
I0524 01:14:37.568945 11835 solver.cpp:239] Iteration 17090 (1.49648 iter/s, 6.68234s/10 iters), loss = 6.82788
I0524 01:14:37.569133 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.82788 (* 1 = 6.82788 loss)
I0524 01:14:37.569181 11835 sgd_solver.cpp:112] Iteration 17090, lr = 0.1
I0524 01:14:44.658466 11835 solver.cpp:239] Iteration 17100 (1.41062 iter/s, 7.08906s/10 iters), loss = 6.78575
I0524 01:14:44.658504 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78575 (* 1 = 6.78575 loss)
I0524 01:14:44.658633 11835 sgd_solver.cpp:112] Iteration 17100, lr = 0.1
I0524 01:14:51.265041 11835 solver.cpp:239] Iteration 17110 (1.51371 iter/s, 6.60627s/10 iters), loss = 7.2651
I0524 01:14:51.265090 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2651 (* 1 = 7.2651 loss)
I0524 01:14:51.265261 11835 sgd_solver.cpp:112] Iteration 17110, lr = 0.1
I0524 01:14:57.978571 11835 solver.cpp:239] Iteration 17120 (1.4896 iter/s, 6.71323s/10 iters), loss = 8.06329
I0524 01:14:57.978618 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.06329 (* 1 = 8.06329 loss)
I0524 01:14:57.978863 11835 sgd_solver.cpp:112] Iteration 17120, lr = 0.1
I0524 01:15:04.977677 11835 solver.cpp:239] Iteration 17130 (1.42882 iter/s, 6.99879s/10 iters), loss = 6.94065
I0524 01:15:04.977730 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.94065 (* 1 = 6.94065 loss)
I0524 01:15:04.977951 11835 sgd_solver.cpp:112] Iteration 17130, lr = 0.1
I0524 01:15:12.947819 11835 solver.cpp:239] Iteration 17140 (1.25474 iter/s, 7.96979s/10 iters), loss = 7.07868
I0524 01:15:12.948099 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.07868 (* 1 = 7.07868 loss)
I0524 01:15:12.953171 11835 sgd_solver.cpp:112] Iteration 17140, lr = 0.1
I0524 01:15:20.658634 11835 solver.cpp:239] Iteration 17150 (1.29697 iter/s, 7.71028s/10 iters), loss = 6.66512
I0524 01:15:20.658689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66512 (* 1 = 6.66512 loss)
I0524 01:15:20.658787 11835 sgd_solver.cpp:112] Iteration 17150, lr = 0.1
I0524 01:15:27.174736 11835 solver.cpp:239] Iteration 17160 (1.53474 iter/s, 6.51577s/10 iters), loss = 7.28496
I0524 01:15:27.174785 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.28496 (* 1 = 7.28496 loss)
I0524 01:15:27.217348 11835 sgd_solver.cpp:112] Iteration 17160, lr = 0.1
I0524 01:15:33.988996 11835 solver.cpp:239] Iteration 17170 (1.46758 iter/s, 6.81396s/10 iters), loss = 7.1946
I0524 01:15:33.989046 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.1946 (* 1 = 7.1946 loss)
I0524 01:15:33.989617 11835 sgd_solver.cpp:112] Iteration 17170, lr = 0.1
I0524 01:15:42.714450 11835 solver.cpp:239] Iteration 17180 (1.14612 iter/s, 8.72507s/10 iters), loss = 7.83486
I0524 01:15:42.714501 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.83486 (* 1 = 7.83486 loss)
I0524 01:15:44.102035 11835 sgd_solver.cpp:112] Iteration 17180, lr = 0.1
I0524 01:15:52.677415 11835 solver.cpp:239] Iteration 17190 (1.00376 iter/s, 9.96254s/10 iters), loss = 6.85563
I0524 01:15:52.677460 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.85563 (* 1 = 6.85563 loss)
I0524 01:15:52.684262 11835 sgd_solver.cpp:112] Iteration 17190, lr = 0.1
I0524 01:16:00.770926 11835 solver.cpp:239] Iteration 17200 (1.23561 iter/s, 8.09315s/10 iters), loss = 7.50545
I0524 01:16:00.770983 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.50545 (* 1 = 7.50545 loss)
I0524 01:16:00.771203 11835 sgd_solver.cpp:112] Iteration 17200, lr = 0.1
I0524 01:16:06.868988 11835 solver.cpp:239] Iteration 17210 (1.63994 iter/s, 6.09778s/10 iters), loss = 6.82988
I0524 01:16:06.869030 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.82988 (* 1 = 6.82988 loss)
I0524 01:16:06.869042 11835 sgd_solver.cpp:112] Iteration 17210, lr = 0.1
I0524 01:16:16.645202 11835 solver.cpp:239] Iteration 17220 (1.02317 iter/s, 9.77356s/10 iters), loss = 8.61034
I0524 01:16:16.645426 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.61034 (* 1 = 8.61034 loss)
I0524 01:16:16.645483 11835 sgd_solver.cpp:112] Iteration 17220, lr = 0.1
I0524 01:16:23.715807 11835 solver.cpp:239] Iteration 17230 (1.4144 iter/s, 7.07016s/10 iters), loss = 7.28172
I0524 01:16:23.715852 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.28172 (* 1 = 7.28172 loss)
I0524 01:16:23.716038 11835 sgd_solver.cpp:112] Iteration 17230, lr = 0.1
I0524 01:16:33.257843 11835 solver.cpp:239] Iteration 17240 (1.04804 iter/s, 9.54163s/10 iters), loss = 7.36292
I0524 01:16:33.257894 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.36292 (* 1 = 7.36292 loss)
I0524 01:16:33.257908 11835 sgd_solver.cpp:112] Iteration 17240, lr = 0.1
I0524 01:16:40.670461 11835 solver.cpp:239] Iteration 17250 (1.34931 iter/s, 7.41118s/10 iters), loss = 7.01171
I0524 01:16:40.670511 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01171 (* 1 = 7.01171 loss)
I0524 01:16:40.670619 11835 sgd_solver.cpp:112] Iteration 17250, lr = 0.1
I0524 01:16:47.359798 11835 solver.cpp:239] Iteration 17260 (1.49498 iter/s, 6.68903s/10 iters), loss = 7.39493
I0524 01:16:47.359971 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.39493 (* 1 = 7.39493 loss)
I0524 01:16:47.584435 11835 sgd_solver.cpp:112] Iteration 17260, lr = 0.1
I0524 01:16:55.360718 11835 solver.cpp:239] Iteration 17270 (1.24993 iter/s, 8.00043s/10 iters), loss = 7.98671
I0524 01:16:55.360771 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.98671 (* 1 = 7.98671 loss)
I0524 01:16:55.361014 11835 sgd_solver.cpp:112] Iteration 17270, lr = 0.1
I0524 01:17:01.132056 11835 solver.cpp:239] Iteration 17280 (1.73278 iter/s, 5.77107s/10 iters), loss = 7.31874
I0524 01:17:01.132102 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.31874 (* 1 = 7.31874 loss)
I0524 01:17:01.132380 11835 sgd_solver.cpp:112] Iteration 17280, lr = 0.1
I0524 01:17:09.598983 11835 solver.cpp:239] Iteration 17290 (1.18112 iter/s, 8.46655s/10 iters), loss = 7.07958
I0524 01:17:09.599040 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.07958 (* 1 = 7.07958 loss)
I0524 01:17:09.599308 11835 sgd_solver.cpp:112] Iteration 17290, lr = 0.1
I0524 01:17:16.917315 11835 solver.cpp:239] Iteration 17300 (1.36649 iter/s, 7.31801s/10 iters), loss = 8.09617
I0524 01:17:16.917357 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.09617 (* 1 = 8.09617 loss)
I0524 01:17:16.917666 11835 sgd_solver.cpp:112] Iteration 17300, lr = 0.1
I0524 01:17:25.778678 11835 solver.cpp:239] Iteration 17310 (1.12854 iter/s, 8.86097s/10 iters), loss = 6.74243
I0524 01:17:25.778843 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74243 (* 1 = 6.74243 loss)
I0524 01:17:25.778913 11835 sgd_solver.cpp:112] Iteration 17310, lr = 0.1
I0524 01:17:32.438511 11835 solver.cpp:239] Iteration 17320 (1.50163 iter/s, 6.65941s/10 iters), loss = 8.78288
I0524 01:17:32.438563 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.78288 (* 1 = 8.78288 loss)
I0524 01:17:32.438740 11835 sgd_solver.cpp:112] Iteration 17320, lr = 0.1
I0524 01:17:39.445869 11835 solver.cpp:239] Iteration 17330 (1.42714 iter/s, 7.00704s/10 iters), loss = 6.67387
I0524 01:17:39.445914 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67387 (* 1 = 6.67387 loss)
I0524 01:17:39.446161 11835 sgd_solver.cpp:112] Iteration 17330, lr = 0.1
I0524 01:17:45.774906 11835 solver.cpp:239] Iteration 17340 (1.58009 iter/s, 6.32874s/10 iters), loss = 7.52103
I0524 01:17:45.774955 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.52103 (* 1 = 7.52103 loss)
I0524 01:17:45.774969 11835 sgd_solver.cpp:112] Iteration 17340, lr = 0.1
I0524 01:17:54.223407 11835 solver.cpp:239] Iteration 17350 (1.18371 iter/s, 8.44803s/10 iters), loss = 7.70109
I0524 01:17:54.223469 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.70109 (* 1 = 7.70109 loss)
I0524 01:17:54.895499 11835 sgd_solver.cpp:112] Iteration 17350, lr = 0.1
I0524 01:18:03.746665 11835 solver.cpp:239] Iteration 17360 (1.05011 iter/s, 9.52284s/10 iters), loss = 7.02795
I0524 01:18:03.746889 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.02795 (* 1 = 7.02795 loss)
I0524 01:18:03.747004 11835 sgd_solver.cpp:112] Iteration 17360, lr = 0.1
I0524 01:18:10.811143 11835 solver.cpp:239] Iteration 17370 (1.41563 iter/s, 7.064s/10 iters), loss = 6.78791
I0524 01:18:10.811192 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78791 (* 1 = 6.78791 loss)
I0524 01:18:10.927398 11835 sgd_solver.cpp:112] Iteration 17370, lr = 0.1
I0524 01:18:17.054682 11835 solver.cpp:239] Iteration 17380 (1.60173 iter/s, 6.24325s/10 iters), loss = 6.90953
I0524 01:18:17.054750 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90953 (* 1 = 6.90953 loss)
I0524 01:18:17.992993 11835 sgd_solver.cpp:112] Iteration 17380, lr = 0.1
I0524 01:18:25.118763 11835 solver.cpp:239] Iteration 17390 (1.24013 iter/s, 8.06369s/10 iters), loss = 7.69268
I0524 01:18:25.118818 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.69268 (* 1 = 7.69268 loss)
I0524 01:18:25.118986 11835 sgd_solver.cpp:112] Iteration 17390, lr = 0.1
I0524 01:18:32.951441 11835 solver.cpp:239] Iteration 17400 (1.27676 iter/s, 7.83231s/10 iters), loss = 7.03728
I0524 01:18:32.951499 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03728 (* 1 = 7.03728 loss)
I0524 01:18:32.951645 11835 sgd_solver.cpp:112] Iteration 17400, lr = 0.1
I0524 01:18:39.246170 11835 solver.cpp:239] Iteration 17410 (1.5887 iter/s, 6.29444s/10 iters), loss = 8.16354
I0524 01:18:39.246450 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.16354 (* 1 = 8.16354 loss)
I0524 01:18:39.723652 11835 sgd_solver.cpp:112] Iteration 17410, lr = 0.1
I0524 01:18:46.198755 11835 solver.cpp:239] Iteration 17420 (1.43842 iter/s, 6.95206s/10 iters), loss = 8.14414
I0524 01:18:46.198812 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.14414 (* 1 = 8.14414 loss)
I0524 01:18:46.229034 11835 sgd_solver.cpp:112] Iteration 17420, lr = 0.1
I0524 01:18:52.087219 11835 solver.cpp:239] Iteration 17430 (1.69832 iter/s, 5.88816s/10 iters), loss = 7.54244
I0524 01:18:52.087271 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.54244 (* 1 = 7.54244 loss)
I0524 01:18:52.087291 11835 sgd_solver.cpp:112] Iteration 17430, lr = 0.1
I0524 01:18:58.137030 11835 solver.cpp:239] Iteration 17440 (1.65361 iter/s, 6.04737s/10 iters), loss = 7.50844
I0524 01:18:58.137079 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.50844 (* 1 = 7.50844 loss)
I0524 01:18:58.137171 11835 sgd_solver.cpp:112] Iteration 17440, lr = 0.1
I0524 01:19:04.009850 11835 solver.cpp:239] Iteration 17450 (1.70284 iter/s, 5.87253s/10 iters), loss = 7.95236
I0524 01:19:04.009909 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.95236 (* 1 = 7.95236 loss)
I0524 01:19:04.010208 11835 sgd_solver.cpp:112] Iteration 17450, lr = 0.1
I0524 01:19:10.649555 11835 solver.cpp:239] Iteration 17460 (1.50616 iter/s, 6.6394s/10 iters), loss = 7.11728
I0524 01:19:10.649689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11728 (* 1 = 7.11728 loss)
I0524 01:19:10.649729 11835 sgd_solver.cpp:112] Iteration 17460, lr = 0.1
I0524 01:19:17.510713 11835 solver.cpp:239] Iteration 17470 (1.45757 iter/s, 6.86075s/10 iters), loss = 6.71518
I0524 01:19:17.510764 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71518 (* 1 = 6.71518 loss)
I0524 01:19:17.510874 11835 sgd_solver.cpp:112] Iteration 17470, lr = 0.1
I0524 01:19:23.562362 11835 solver.cpp:239] Iteration 17480 (1.65252 iter/s, 6.05136s/10 iters), loss = 7.73199
I0524 01:19:23.562405 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.73199 (* 1 = 7.73199 loss)
I0524 01:19:23.562760 11835 sgd_solver.cpp:112] Iteration 17480, lr = 0.1
I0524 01:19:29.389143 11835 solver.cpp:239] Iteration 17490 (1.71629 iter/s, 5.82651s/10 iters), loss = 7.27877
I0524 01:19:29.389189 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.27877 (* 1 = 7.27877 loss)
I0524 01:19:29.389204 11835 sgd_solver.cpp:112] Iteration 17490, lr = 0.1
I0524 01:19:36.673831 11835 solver.cpp:239] Iteration 17500 (1.3728 iter/s, 7.28437s/10 iters), loss = 7.74067
I0524 01:19:36.673866 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.74067 (* 1 = 7.74067 loss)
I0524 01:19:36.674103 11835 sgd_solver.cpp:112] Iteration 17500, lr = 0.1
I0524 01:19:43.320798 11835 solver.cpp:239] Iteration 17510 (1.50451 iter/s, 6.64666s/10 iters), loss = 7.60723
I0524 01:19:43.321113 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.60723 (* 1 = 7.60723 loss)
I0524 01:19:43.321173 11835 sgd_solver.cpp:112] Iteration 17510, lr = 0.1
I0524 01:19:49.344338 11835 solver.cpp:239] Iteration 17520 (1.66036 iter/s, 6.02278s/10 iters), loss = 8.08338
I0524 01:19:49.344388 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.08338 (* 1 = 8.08338 loss)
I0524 01:19:49.344454 11835 sgd_solver.cpp:112] Iteration 17520, lr = 0.1
I0524 01:19:56.761533 11835 solver.cpp:239] Iteration 17530 (1.34828 iter/s, 7.41684s/10 iters), loss = 8.61784
I0524 01:19:56.761605 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.61784 (* 1 = 8.61784 loss)
I0524 01:19:56.761672 11835 sgd_solver.cpp:112] Iteration 17530, lr = 0.1
I0524 01:20:03.611932 11835 solver.cpp:239] Iteration 17540 (1.45984 iter/s, 6.85006s/10 iters), loss = 8.56239
I0524 01:20:03.611986 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.56239 (* 1 = 8.56239 loss)
I0524 01:20:03.678395 11835 sgd_solver.cpp:112] Iteration 17540, lr = 0.1
I0524 01:20:10.391786 11835 solver.cpp:239] Iteration 17550 (1.47503 iter/s, 6.77954s/10 iters), loss = 7.34484
I0524 01:20:10.391849 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.34484 (* 1 = 7.34484 loss)
I0524 01:20:10.546788 11835 sgd_solver.cpp:112] Iteration 17550, lr = 0.1
I0524 01:20:18.638116 11835 solver.cpp:239] Iteration 17560 (1.21272 iter/s, 8.24594s/10 iters), loss = 8.00786
I0524 01:20:18.638224 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.00786 (* 1 = 8.00786 loss)
I0524 01:20:18.638366 11835 sgd_solver.cpp:112] Iteration 17560, lr = 0.1
I0524 01:20:25.811499 11835 solver.cpp:239] Iteration 17570 (1.39411 iter/s, 7.17301s/10 iters), loss = 7.25467
I0524 01:20:25.811553 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.25467 (* 1 = 7.25467 loss)
I0524 01:20:25.811916 11835 sgd_solver.cpp:112] Iteration 17570, lr = 0.1
I0524 01:20:31.870334 11835 solver.cpp:239] Iteration 17580 (1.65056 iter/s, 6.05856s/10 iters), loss = 7.46801
I0524 01:20:31.870373 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.46801 (* 1 = 7.46801 loss)
I0524 01:20:31.870384 11835 sgd_solver.cpp:112] Iteration 17580, lr = 0.1
I0524 01:20:38.159111 11835 solver.cpp:239] Iteration 17590 (1.59021 iter/s, 6.28848s/10 iters), loss = 7.61789
I0524 01:20:38.159164 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.61789 (* 1 = 7.61789 loss)
I0524 01:20:38.159227 11835 sgd_solver.cpp:112] Iteration 17590, lr = 0.1
I0524 01:20:44.342156 11835 solver.cpp:239] Iteration 17600 (1.6174 iter/s, 6.18276s/10 iters), loss = 8.37636
I0524 01:20:44.342200 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.37636 (* 1 = 8.37636 loss)
I0524 01:20:44.342320 11835 sgd_solver.cpp:112] Iteration 17600, lr = 0.1
I0524 01:20:49.922821 11835 solver.cpp:239] Iteration 17610 (1.79201 iter/s, 5.58033s/10 iters), loss = 7.20757
I0524 01:20:49.923023 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.20757 (* 1 = 7.20757 loss)
I0524 01:20:49.923068 11835 sgd_solver.cpp:112] Iteration 17610, lr = 0.1
I0524 01:20:55.795451 11835 solver.cpp:239] Iteration 17620 (1.70294 iter/s, 5.8722s/10 iters), loss = 8.04148
I0524 01:20:55.795503 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.04148 (* 1 = 8.04148 loss)
I0524 01:20:55.795712 11835 sgd_solver.cpp:112] Iteration 17620, lr = 0.1
I0524 01:21:04.549679 11835 solver.cpp:239] Iteration 17630 (1.14236 iter/s, 8.75384s/10 iters), loss = 8.06344
I0524 01:21:04.549722 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.06344 (* 1 = 8.06344 loss)
I0524 01:21:05.082381 11835 sgd_solver.cpp:112] Iteration 17630, lr = 0.1
I0524 01:21:15.381739 11835 solver.cpp:239] Iteration 17640 (0.923224 iter/s, 10.8316s/10 iters), loss = 6.76803
I0524 01:21:15.381788 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76803 (* 1 = 6.76803 loss)
I0524 01:21:15.382014 11835 sgd_solver.cpp:112] Iteration 17640, lr = 0.1
I0524 01:21:22.748412 11835 solver.cpp:239] Iteration 17650 (1.35753 iter/s, 7.36634s/10 iters), loss = 6.23557
I0524 01:21:22.748633 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23557 (* 1 = 6.23557 loss)
I0524 01:21:22.892802 11835 sgd_solver.cpp:112] Iteration 17650, lr = 0.1
I0524 01:21:28.738981 11835 solver.cpp:239] Iteration 17660 (1.66941 iter/s, 5.99013s/10 iters), loss = 7.42357
I0524 01:21:28.739034 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.42357 (* 1 = 7.42357 loss)
I0524 01:21:28.739125 11835 sgd_solver.cpp:112] Iteration 17660, lr = 0.1
I0524 01:21:35.694665 11835 solver.cpp:239] Iteration 17670 (1.43774 iter/s, 6.95536s/10 iters), loss = 6.95391
I0524 01:21:35.694742 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.95391 (* 1 = 6.95391 loss)
I0524 01:21:35.694974 11835 sgd_solver.cpp:112] Iteration 17670, lr = 0.1
I0524 01:21:42.614730 11835 solver.cpp:239] Iteration 17680 (1.44515 iter/s, 6.91971s/10 iters), loss = 7.28273
I0524 01:21:42.614787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.28273 (* 1 = 7.28273 loss)
I0524 01:21:42.614981 11835 sgd_solver.cpp:112] Iteration 17680, lr = 0.1
I0524 01:21:50.105965 11835 solver.cpp:239] Iteration 17690 (1.33495 iter/s, 7.4909s/10 iters), loss = 6.7395
I0524 01:21:50.106012 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7395 (* 1 = 6.7395 loss)
I0524 01:21:50.106112 11835 sgd_solver.cpp:112] Iteration 17690, lr = 0.1
I0524 01:21:57.434376 11835 solver.cpp:239] Iteration 17700 (1.36461 iter/s, 7.32808s/10 iters), loss = 7.35929
I0524 01:21:57.434670 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.35929 (* 1 = 7.35929 loss)
I0524 01:21:57.434770 11835 sgd_solver.cpp:112] Iteration 17700, lr = 0.1
I0524 01:22:05.292838 11835 solver.cpp:239] Iteration 17710 (1.27264 iter/s, 7.85767s/10 iters), loss = 7.18426
I0524 01:22:05.292886 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.18426 (* 1 = 7.18426 loss)
I0524 01:22:05.728521 11835 sgd_solver.cpp:112] Iteration 17710, lr = 0.1
I0524 01:22:13.537572 11835 solver.cpp:239] Iteration 17720 (1.21295 iter/s, 8.24437s/10 iters), loss = 8.65692
I0524 01:22:13.537627 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.65692 (* 1 = 8.65692 loss)
I0524 01:22:13.537834 11835 sgd_solver.cpp:112] Iteration 17720, lr = 0.1
I0524 01:22:20.157018 11835 solver.cpp:239] Iteration 17730 (1.51077 iter/s, 6.61915s/10 iters), loss = 7.29864
I0524 01:22:20.157058 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.29864 (* 1 = 7.29864 loss)
I0524 01:22:20.157136 11835 sgd_solver.cpp:112] Iteration 17730, lr = 0.1
I0524 01:22:25.957065 11835 solver.cpp:239] Iteration 17740 (1.72421 iter/s, 5.79977s/10 iters), loss = 7.60173
I0524 01:22:25.957118 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.60173 (* 1 = 7.60173 loss)
I0524 01:22:25.957305 11835 sgd_solver.cpp:112] Iteration 17740, lr = 0.1
I0524 01:22:31.537765 11835 solver.cpp:239] Iteration 17750 (1.79197 iter/s, 5.58044s/10 iters), loss = 7.01113
I0524 01:22:31.538033 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01113 (* 1 = 7.01113 loss)
I0524 01:22:31.538092 11835 sgd_solver.cpp:112] Iteration 17750, lr = 0.1
I0524 01:22:39.947551 11835 solver.cpp:239] Iteration 17760 (1.18917 iter/s, 8.4092s/10 iters), loss = 7.61785
I0524 01:22:39.947607 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.61785 (* 1 = 7.61785 loss)
I0524 01:22:41.157747 11835 sgd_solver.cpp:112] Iteration 17760, lr = 0.1
I0524 01:22:48.223798 11835 solver.cpp:239] Iteration 17770 (1.20833 iter/s, 8.27587s/10 iters), loss = 6.49472
I0524 01:22:48.223845 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49472 (* 1 = 6.49472 loss)
I0524 01:22:48.223948 11835 sgd_solver.cpp:112] Iteration 17770, lr = 0.1
I0524 01:22:55.793270 11835 solver.cpp:239] Iteration 17780 (1.32115 iter/s, 7.56914s/10 iters), loss = 7.91057
I0524 01:22:55.793323 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.91057 (* 1 = 7.91057 loss)
I0524 01:22:55.901746 11835 sgd_solver.cpp:112] Iteration 17780, lr = 0.1
I0524 01:23:01.949817 11835 solver.cpp:239] Iteration 17790 (1.62436 iter/s, 6.15627s/10 iters), loss = 7.14713
I0524 01:23:01.950095 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.14713 (* 1 = 7.14713 loss)
I0524 01:23:01.950145 11835 sgd_solver.cpp:112] Iteration 17790, lr = 0.1
I0524 01:23:08.111045 11835 solver.cpp:239] Iteration 17800 (1.62323 iter/s, 6.16055s/10 iters), loss = 7.30661
I0524 01:23:08.111110 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.30661 (* 1 = 7.30661 loss)
I0524 01:23:08.111225 11835 sgd_solver.cpp:112] Iteration 17800, lr = 0.1
I0524 01:23:14.987267 11835 solver.cpp:239] Iteration 17810 (1.45436 iter/s, 6.87589s/10 iters), loss = 6.80537
I0524 01:23:14.987321 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80537 (* 1 = 6.80537 loss)
I0524 01:23:15.623878 11835 sgd_solver.cpp:112] Iteration 17810, lr = 0.1
I0524 01:23:23.101205 11835 solver.cpp:239] Iteration 17820 (1.2325 iter/s, 8.11359s/10 iters), loss = 7.5005
I0524 01:23:23.101245 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.5005 (* 1 = 7.5005 loss)
I0524 01:23:23.118057 11835 sgd_solver.cpp:112] Iteration 17820, lr = 0.1
I0524 01:23:30.217725 11835 solver.cpp:239] Iteration 17830 (1.40525 iter/s, 7.11619s/10 iters), loss = 7.00806
I0524 01:23:30.217772 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00806 (* 1 = 7.00806 loss)
I0524 01:23:30.217789 11835 sgd_solver.cpp:112] Iteration 17830, lr = 0.1
I0524 01:23:37.718770 11835 solver.cpp:239] Iteration 17840 (1.33359 iter/s, 7.49854s/10 iters), loss = 7.94194
I0524 01:23:37.719017 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.94194 (* 1 = 7.94194 loss)
I0524 01:23:37.719061 11835 sgd_solver.cpp:112] Iteration 17840, lr = 0.1
I0524 01:23:43.615377 11835 solver.cpp:239] Iteration 17850 (1.69629 iter/s, 5.89524s/10 iters), loss = 6.92769
I0524 01:23:43.615418 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92769 (* 1 = 6.92769 loss)
I0524 01:23:43.615700 11835 sgd_solver.cpp:112] Iteration 17850, lr = 0.1
I0524 01:23:51.027328 11835 solver.cpp:239] Iteration 17860 (1.34923 iter/s, 7.41161s/10 iters), loss = 6.67999
I0524 01:23:51.027381 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67999 (* 1 = 6.67999 loss)
I0524 01:23:51.027613 11835 sgd_solver.cpp:112] Iteration 17860, lr = 0.1
I0524 01:23:57.536134 11835 solver.cpp:239] Iteration 17870 (1.53645 iter/s, 6.50851s/10 iters), loss = 7.10549
I0524 01:23:57.536172 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10549 (* 1 = 7.10549 loss)
I0524 01:23:57.536770 11835 sgd_solver.cpp:112] Iteration 17870, lr = 0.1
I0524 01:24:03.913311 11835 solver.cpp:239] Iteration 17880 (1.56817 iter/s, 6.37688s/10 iters), loss = 7.36091
I0524 01:24:03.913364 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.36091 (* 1 = 7.36091 loss)
I0524 01:24:03.913661 11835 sgd_solver.cpp:112] Iteration 17880, lr = 0.1
I0524 01:24:09.677037 11835 solver.cpp:239] Iteration 17890 (1.73507 iter/s, 5.76346s/10 iters), loss = 7.10725
I0524 01:24:09.677167 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10725 (* 1 = 7.10725 loss)
I0524 01:24:10.302729 11835 sgd_solver.cpp:112] Iteration 17890, lr = 0.1
I0524 01:24:17.005192 11835 solver.cpp:239] Iteration 17900 (1.36468 iter/s, 7.32774s/10 iters), loss = 7.721
I0524 01:24:17.005241 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.721 (* 1 = 7.721 loss)
I0524 01:24:17.005255 11835 sgd_solver.cpp:112] Iteration 17900, lr = 0.1
I0524 01:24:23.026736 11835 solver.cpp:239] Iteration 17910 (1.66092 iter/s, 6.02076s/10 iters), loss = 6.76516
I0524 01:24:23.026782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76516 (* 1 = 6.76516 loss)
I0524 01:24:23.026795 11835 sgd_solver.cpp:112] Iteration 17910, lr = 0.1
I0524 01:24:30.522099 11835 solver.cpp:239] Iteration 17920 (1.33423 iter/s, 7.49498s/10 iters), loss = 7.26855
I0524 01:24:30.522142 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.26855 (* 1 = 7.26855 loss)
I0524 01:24:30.522168 11835 sgd_solver.cpp:112] Iteration 17920, lr = 0.1
I0524 01:24:36.459971 11835 solver.cpp:239] Iteration 17930 (1.68418 iter/s, 5.93759s/10 iters), loss = 6.93364
I0524 01:24:36.460026 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93364 (* 1 = 6.93364 loss)
I0524 01:24:36.554725 11835 sgd_solver.cpp:112] Iteration 17930, lr = 0.1
I0524 01:24:45.621050 11835 solver.cpp:239] Iteration 17940 (1.09162 iter/s, 9.16068s/10 iters), loss = 7.44332
I0524 01:24:45.621181 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.44332 (* 1 = 7.44332 loss)
I0524 01:24:45.621212 11835 sgd_solver.cpp:112] Iteration 17940, lr = 0.1
I0524 01:24:52.377015 11835 solver.cpp:239] Iteration 17950 (1.4805 iter/s, 6.75447s/10 iters), loss = 6.97266
I0524 01:24:52.377054 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.97266 (* 1 = 6.97266 loss)
I0524 01:24:52.377178 11835 sgd_solver.cpp:112] Iteration 17950, lr = 0.1
I0524 01:24:59.350728 11835 solver.cpp:239] Iteration 17960 (1.43403 iter/s, 6.97337s/10 iters), loss = 7.1983
I0524 01:24:59.350781 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.1983 (* 1 = 7.1983 loss)
I0524 01:24:59.836365 11835 sgd_solver.cpp:112] Iteration 17960, lr = 0.1
I0524 01:25:05.998085 11835 solver.cpp:239] Iteration 17970 (1.50443 iter/s, 6.64705s/10 iters), loss = 6.9269
I0524 01:25:05.998129 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9269 (* 1 = 6.9269 loss)
I0524 01:25:05.998369 11835 sgd_solver.cpp:112] Iteration 17970, lr = 0.1
I0524 01:25:11.634776 11835 solver.cpp:239] Iteration 17980 (1.77418 iter/s, 5.63642s/10 iters), loss = 7.09284
I0524 01:25:11.634830 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09284 (* 1 = 7.09284 loss)
I0524 01:25:11.635076 11835 sgd_solver.cpp:112] Iteration 17980, lr = 0.1
I0524 01:25:17.638497 11835 solver.cpp:239] Iteration 17990 (1.66571 iter/s, 6.00344s/10 iters), loss = 8.01234
I0524 01:25:17.638784 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.01234 (* 1 = 8.01234 loss)
I0524 01:25:18.301751 11835 sgd_solver.cpp:112] Iteration 17990, lr = 0.1
I0524 01:25:25.369148 11835 solver.cpp:239] Iteration 18000 (1.29364 iter/s, 7.73015s/10 iters), loss = 7.30591
I0524 01:25:25.369194 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.30591 (* 1 = 7.30591 loss)
I0524 01:25:25.369462 11835 sgd_solver.cpp:112] Iteration 18000, lr = 0.1
I0524 01:25:32.885201 11835 solver.cpp:239] Iteration 18010 (1.33054 iter/s, 7.51572s/10 iters), loss = 7.18325
I0524 01:25:32.885249 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.18325 (* 1 = 7.18325 loss)
I0524 01:25:32.885680 11835 sgd_solver.cpp:112] Iteration 18010, lr = 0.1
I0524 01:25:38.957461 11835 solver.cpp:239] Iteration 18020 (1.64691 iter/s, 6.07197s/10 iters), loss = 7.11701
I0524 01:25:38.957511 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11701 (* 1 = 7.11701 loss)
I0524 01:25:38.957540 11835 sgd_solver.cpp:112] Iteration 18020, lr = 0.1
I0524 01:25:49.344517 11835 solver.cpp:239] Iteration 18030 (0.962777 iter/s, 10.3866s/10 iters), loss = 8.37507
I0524 01:25:49.344650 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.37507 (* 1 = 8.37507 loss)
I0524 01:25:50.445221 11835 sgd_solver.cpp:112] Iteration 18030, lr = 0.1
I0524 01:25:57.324683 11835 solver.cpp:239] Iteration 18040 (1.25317 iter/s, 7.97974s/10 iters), loss = 7.7736
I0524 01:25:57.324733 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.7736 (* 1 = 7.7736 loss)
I0524 01:25:57.324967 11835 sgd_solver.cpp:112] Iteration 18040, lr = 0.1
I0524 01:26:03.710783 11835 solver.cpp:239] Iteration 18050 (1.56598 iter/s, 6.3858s/10 iters), loss = 7.13134
I0524 01:26:03.710834 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13134 (* 1 = 7.13134 loss)
I0524 01:26:03.710882 11835 sgd_solver.cpp:112] Iteration 18050, lr = 0.1
I0524 01:26:11.018826 11835 solver.cpp:239] Iteration 18060 (1.36842 iter/s, 7.30772s/10 iters), loss = 5.93266
I0524 01:26:11.018867 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93266 (* 1 = 5.93266 loss)
I0524 01:26:11.019246 11835 sgd_solver.cpp:112] Iteration 18060, lr = 0.1
I0524 01:26:19.503288 11835 solver.cpp:239] Iteration 18070 (1.17868 iter/s, 8.48409s/10 iters), loss = 7.52341
I0524 01:26:19.503587 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.52341 (* 1 = 7.52341 loss)
I0524 01:26:19.503641 11835 sgd_solver.cpp:112] Iteration 18070, lr = 0.1
I0524 01:26:25.454505 11835 solver.cpp:239] Iteration 18080 (1.68047 iter/s, 5.95071s/10 iters), loss = 6.64149
I0524 01:26:25.454545 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64149 (* 1 = 6.64149 loss)
I0524 01:26:25.465143 11835 sgd_solver.cpp:112] Iteration 18080, lr = 0.1
I0524 01:26:33.431134 11835 solver.cpp:239] Iteration 18090 (1.25372 iter/s, 7.97628s/10 iters), loss = 7.64415
I0524 01:26:33.431181 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.64415 (* 1 = 7.64415 loss)
I0524 01:26:33.431411 11835 sgd_solver.cpp:112] Iteration 18090, lr = 0.1
I0524 01:26:40.240536 11835 solver.cpp:239] Iteration 18100 (1.46863 iter/s, 6.80909s/10 iters), loss = 7.71265
I0524 01:26:40.240589 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.71265 (* 1 = 7.71265 loss)
I0524 01:26:40.240658 11835 sgd_solver.cpp:112] Iteration 18100, lr = 0.1
I0524 01:26:46.589386 11835 solver.cpp:239] Iteration 18110 (1.57516 iter/s, 6.34855s/10 iters), loss = 6.6821
I0524 01:26:46.589434 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6821 (* 1 = 6.6821 loss)
I0524 01:26:46.589572 11835 sgd_solver.cpp:112] Iteration 18110, lr = 0.1
I0524 01:26:53.263574 11835 solver.cpp:239] Iteration 18120 (1.49838 iter/s, 6.67389s/10 iters), loss = 6.69969
I0524 01:26:53.263787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69969 (* 1 = 6.69969 loss)
I0524 01:26:53.264214 11835 sgd_solver.cpp:112] Iteration 18120, lr = 0.1
I0524 01:26:58.938766 11835 solver.cpp:239] Iteration 18130 (1.76219 iter/s, 5.67477s/10 iters), loss = 7.82937
I0524 01:26:58.938818 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.82937 (* 1 = 7.82937 loss)
I0524 01:26:59.017594 11835 sgd_solver.cpp:112] Iteration 18130, lr = 0.1
I0524 01:27:05.326692 11835 solver.cpp:239] Iteration 18140 (1.56553 iter/s, 6.38763s/10 iters), loss = 6.6771
I0524 01:27:05.326752 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6771 (* 1 = 6.6771 loss)
I0524 01:27:05.326846 11835 sgd_solver.cpp:112] Iteration 18140, lr = 0.1
I0524 01:27:11.318675 11835 solver.cpp:239] Iteration 18150 (1.66898 iter/s, 5.9917s/10 iters), loss = 5.98818
I0524 01:27:11.318737 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98818 (* 1 = 5.98818 loss)
I0524 01:27:12.493391 11835 sgd_solver.cpp:112] Iteration 18150, lr = 0.1
I0524 01:27:19.380620 11835 solver.cpp:239] Iteration 18160 (1.24045 iter/s, 8.06158s/10 iters), loss = 8.05734
I0524 01:27:19.380661 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.05734 (* 1 = 8.05734 loss)
I0524 01:27:19.402312 11835 sgd_solver.cpp:112] Iteration 18160, lr = 0.1
I0524 01:27:26.816907 11835 solver.cpp:239] Iteration 18170 (1.34482 iter/s, 7.43595s/10 iters), loss = 6.99196
I0524 01:27:26.817168 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.99196 (* 1 = 6.99196 loss)
I0524 01:27:26.817237 11835 sgd_solver.cpp:112] Iteration 18170, lr = 0.1
I0524 01:27:34.069157 11835 solver.cpp:239] Iteration 18180 (1.37898 iter/s, 7.25175s/10 iters), loss = 6.3741
I0524 01:27:34.069218 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3741 (* 1 = 6.3741 loss)
I0524 01:27:34.069464 11835 sgd_solver.cpp:112] Iteration 18180, lr = 0.1
I0524 01:27:41.195677 11835 solver.cpp:239] Iteration 18190 (1.40327 iter/s, 7.12619s/10 iters), loss = 6.36747
I0524 01:27:41.195729 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36747 (* 1 = 6.36747 loss)
I0524 01:27:41.195852 11835 sgd_solver.cpp:112] Iteration 18190, lr = 0.1
I0524 01:27:47.687458 11835 solver.cpp:239] Iteration 18200 (1.54048 iter/s, 6.49149s/10 iters), loss = 7.7066
I0524 01:27:47.687499 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.7066 (* 1 = 7.7066 loss)
I0524 01:27:47.688045 11835 sgd_solver.cpp:112] Iteration 18200, lr = 0.1
I0524 01:27:56.580724 11835 solver.cpp:239] Iteration 18210 (1.1245 iter/s, 8.89287s/10 iters), loss = 6.27977
I0524 01:27:56.580791 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27977 (* 1 = 6.27977 loss)
I0524 01:27:56.581032 11835 sgd_solver.cpp:112] Iteration 18210, lr = 0.1
I0524 01:28:03.130790 11835 solver.cpp:239] Iteration 18220 (1.52678 iter/s, 6.54975s/10 iters), loss = 6.77746
I0524 01:28:03.131073 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.77746 (* 1 = 6.77746 loss)
I0524 01:28:03.260192 11835 sgd_solver.cpp:112] Iteration 18220, lr = 0.1
I0524 01:28:09.841516 11835 solver.cpp:239] Iteration 18230 (1.49026 iter/s, 6.71022s/10 iters), loss = 7.35403
I0524 01:28:09.841572 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.35403 (* 1 = 7.35403 loss)
I0524 01:28:09.841637 11835 sgd_solver.cpp:112] Iteration 18230, lr = 0.1
I0524 01:28:16.641579 11835 solver.cpp:239] Iteration 18240 (1.47064 iter/s, 6.79974s/10 iters), loss = 7.09822
I0524 01:28:16.641646 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09822 (* 1 = 7.09822 loss)
I0524 01:28:16.641863 11835 sgd_solver.cpp:112] Iteration 18240, lr = 0.1
I0524 01:28:22.801424 11835 solver.cpp:239] Iteration 18250 (1.62349 iter/s, 6.15956s/10 iters), loss = 6.43645
I0524 01:28:22.801471 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43645 (* 1 = 6.43645 loss)
I0524 01:28:22.803417 11835 sgd_solver.cpp:112] Iteration 18250, lr = 0.1
I0524 01:28:29.925930 11835 solver.cpp:239] Iteration 18260 (1.40367 iter/s, 7.12417s/10 iters), loss = 7.22322
I0524 01:28:29.926010 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.22322 (* 1 = 7.22322 loss)
I0524 01:28:29.926275 11835 sgd_solver.cpp:112] Iteration 18260, lr = 0.1
I0524 01:28:38.171880 11835 solver.cpp:239] Iteration 18270 (1.21277 iter/s, 8.24556s/10 iters), loss = 6.26576
I0524 01:28:38.171970 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26576 (* 1 = 6.26576 loss)
I0524 01:28:38.171989 11835 sgd_solver.cpp:112] Iteration 18270, lr = 0.1
I0524 01:28:45.841559 11835 solver.cpp:239] Iteration 18280 (1.3039 iter/s, 7.66929s/10 iters), loss = 7.89625
I0524 01:28:45.841622 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.89625 (* 1 = 7.89625 loss)
I0524 01:28:45.841846 11835 sgd_solver.cpp:112] Iteration 18280, lr = 0.1
I0524 01:28:51.567991 11835 solver.cpp:239] Iteration 18290 (1.74637 iter/s, 5.72616s/10 iters), loss = 7.00383
I0524 01:28:51.568025 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00383 (* 1 = 7.00383 loss)
I0524 01:28:51.568038 11835 sgd_solver.cpp:112] Iteration 18290, lr = 0.1
I0524 01:28:58.531417 11835 solver.cpp:239] Iteration 18300 (1.43614 iter/s, 6.96312s/10 iters), loss = 6.8735
I0524 01:28:58.531466 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8735 (* 1 = 6.8735 loss)
I0524 01:28:58.598518 11835 sgd_solver.cpp:112] Iteration 18300, lr = 0.1
I0524 01:29:07.763767 11835 solver.cpp:239] Iteration 18310 (1.08319 iter/s, 9.23195s/10 iters), loss = 6.8516
I0524 01:29:07.763815 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8516 (* 1 = 6.8516 loss)
I0524 01:29:08.886314 11835 sgd_solver.cpp:112] Iteration 18310, lr = 0.1
I0524 01:29:16.429147 11835 solver.cpp:239] Iteration 18320 (1.15407 iter/s, 8.665s/10 iters), loss = 7.11844
I0524 01:29:16.429194 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11844 (* 1 = 7.11844 loss)
I0524 01:29:16.429208 11835 sgd_solver.cpp:112] Iteration 18320, lr = 0.1
I0524 01:29:23.204118 11835 solver.cpp:239] Iteration 18330 (1.47609 iter/s, 6.77466s/10 iters), loss = 5.66861
I0524 01:29:23.204170 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.66861 (* 1 = 5.66861 loss)
I0524 01:29:23.204412 11835 sgd_solver.cpp:112] Iteration 18330, lr = 0.1
I0524 01:29:29.402364 11835 solver.cpp:239] Iteration 18340 (1.61343 iter/s, 6.19796s/10 iters), loss = 6.87758
I0524 01:29:29.402410 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87758 (* 1 = 6.87758 loss)
I0524 01:29:29.402546 11835 sgd_solver.cpp:112] Iteration 18340, lr = 0.1
I0524 01:29:38.549726 11835 solver.cpp:239] Iteration 18350 (1.09326 iter/s, 9.14696s/10 iters), loss = 6.56785
I0524 01:29:38.549788 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56785 (* 1 = 6.56785 loss)
I0524 01:29:38.549816 11835 sgd_solver.cpp:112] Iteration 18350, lr = 0.1
I0524 01:29:45.405443 11835 solver.cpp:239] Iteration 18360 (1.45871 iter/s, 6.85539s/10 iters), loss = 6.96901
I0524 01:29:45.405772 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96901 (* 1 = 6.96901 loss)
I0524 01:29:45.405834 11835 sgd_solver.cpp:112] Iteration 18360, lr = 0.1
I0524 01:29:51.990356 11835 solver.cpp:239] Iteration 18370 (1.51879 iter/s, 6.58418s/10 iters), loss = 6.73683
I0524 01:29:51.990401 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73683 (* 1 = 6.73683 loss)
I0524 01:29:51.990667 11835 sgd_solver.cpp:112] Iteration 18370, lr = 0.1
I0524 01:29:58.818274 11835 solver.cpp:239] Iteration 18380 (1.46464 iter/s, 6.82761s/10 iters), loss = 6.73791
I0524 01:29:58.818321 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73791 (* 1 = 6.73791 loss)
I0524 01:29:58.818452 11835 sgd_solver.cpp:112] Iteration 18380, lr = 0.1
I0524 01:30:07.519189 11835 solver.cpp:239] Iteration 18390 (1.14936 iter/s, 8.70053s/10 iters), loss = 7.32875
I0524 01:30:07.519245 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.32875 (* 1 = 7.32875 loss)
I0524 01:30:07.519443 11835 sgd_solver.cpp:112] Iteration 18390, lr = 0.1
I0524 01:30:13.424093 11835 solver.cpp:239] Iteration 18400 (1.69359 iter/s, 5.90462s/10 iters), loss = 7.38376
I0524 01:30:13.424139 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.38376 (* 1 = 7.38376 loss)
I0524 01:30:14.147245 11835 sgd_solver.cpp:112] Iteration 18400, lr = 0.1
I0524 01:30:21.387915 11835 solver.cpp:239] Iteration 18410 (1.25573 iter/s, 7.96348s/10 iters), loss = 6.83698
I0524 01:30:21.388020 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83698 (* 1 = 6.83698 loss)
I0524 01:30:21.388195 11835 sgd_solver.cpp:112] Iteration 18410, lr = 0.1
I0524 01:30:27.355602 11835 solver.cpp:239] Iteration 18420 (1.67579 iter/s, 5.96735s/10 iters), loss = 7.22377
I0524 01:30:27.355648 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.22377 (* 1 = 7.22377 loss)
I0524 01:30:27.667795 11835 sgd_solver.cpp:112] Iteration 18420, lr = 0.1
I0524 01:30:35.294595 11835 solver.cpp:239] Iteration 18430 (1.25966 iter/s, 7.93864s/10 iters), loss = 7.45181
I0524 01:30:35.294648 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.45181 (* 1 = 7.45181 loss)
I0524 01:30:35.294901 11835 sgd_solver.cpp:112] Iteration 18430, lr = 0.1
I0524 01:30:42.026721 11835 solver.cpp:239] Iteration 18440 (1.48548 iter/s, 6.73181s/10 iters), loss = 8.58412
I0524 01:30:42.026772 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.58412 (* 1 = 8.58412 loss)
I0524 01:30:42.026932 11835 sgd_solver.cpp:112] Iteration 18440, lr = 0.1
I0524 01:30:49.050565 11835 solver.cpp:239] Iteration 18450 (1.42379 iter/s, 7.02351s/10 iters), loss = 7.10533
I0524 01:30:49.050621 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10533 (* 1 = 7.10533 loss)
I0524 01:30:49.050777 11835 sgd_solver.cpp:112] Iteration 18450, lr = 0.1
I0524 01:30:56.783658 11835 solver.cpp:239] Iteration 18460 (1.2932 iter/s, 7.73275s/10 iters), loss = 7.23764
I0524 01:30:56.783913 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.23764 (* 1 = 7.23764 loss)
I0524 01:30:56.784130 11835 sgd_solver.cpp:112] Iteration 18460, lr = 0.1
I0524 01:31:04.367964 11835 solver.cpp:239] Iteration 18470 (1.3186 iter/s, 7.58379s/10 iters), loss = 7.27127
I0524 01:31:04.368010 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.27127 (* 1 = 7.27127 loss)
I0524 01:31:04.401954 11835 sgd_solver.cpp:112] Iteration 18470, lr = 0.1
I0524 01:31:11.979846 11835 solver.cpp:239] Iteration 18480 (1.31379 iter/s, 7.61155s/10 iters), loss = 7.26909
I0524 01:31:11.979883 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.26909 (* 1 = 7.26909 loss)
I0524 01:31:12.192389 11835 sgd_solver.cpp:112] Iteration 18480, lr = 0.1
I0524 01:31:19.202666 11835 solver.cpp:239] Iteration 18490 (1.38456 iter/s, 7.2225s/10 iters), loss = 6.86983
I0524 01:31:19.202733 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86983 (* 1 = 6.86983 loss)
I0524 01:31:19.202913 11835 sgd_solver.cpp:112] Iteration 18490, lr = 0.1
I0524 01:31:26.086758 11835 solver.cpp:239] Iteration 18500 (1.45269 iter/s, 6.88376s/10 iters), loss = 8.00172
I0524 01:31:26.086810 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.00172 (* 1 = 8.00172 loss)
I0524 01:31:26.086825 11835 sgd_solver.cpp:112] Iteration 18500, lr = 0.1
I0524 01:31:35.252528 11835 solver.cpp:239] Iteration 18510 (1.09114 iter/s, 9.16474s/10 iters), loss = 6.98869
I0524 01:31:35.252825 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98869 (* 1 = 6.98869 loss)
I0524 01:31:35.889999 11835 sgd_solver.cpp:112] Iteration 18510, lr = 0.1
I0524 01:31:43.400910 11835 solver.cpp:239] Iteration 18520 (1.22732 iter/s, 8.1478s/10 iters), loss = 7.35843
I0524 01:31:43.400964 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.35843 (* 1 = 7.35843 loss)
I0524 01:31:43.978088 11835 sgd_solver.cpp:112] Iteration 18520, lr = 0.1
I0524 01:31:52.395151 11835 solver.cpp:239] Iteration 18530 (1.11187 iter/s, 8.99384s/10 iters), loss = 7.44968
I0524 01:31:52.395203 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.44968 (* 1 = 7.44968 loss)
I0524 01:31:52.395299 11835 sgd_solver.cpp:112] Iteration 18530, lr = 0.1
I0524 01:31:58.638425 11835 solver.cpp:239] Iteration 18540 (1.6018 iter/s, 6.24299s/10 iters), loss = 6.66961
I0524 01:31:58.638468 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66961 (* 1 = 6.66961 loss)
I0524 01:31:58.638732 11835 sgd_solver.cpp:112] Iteration 18540, lr = 0.1
I0524 01:32:05.342173 11835 solver.cpp:239] Iteration 18550 (1.49177 iter/s, 6.70344s/10 iters), loss = 6.39682
I0524 01:32:05.342453 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39682 (* 1 = 6.39682 loss)
I0524 01:32:05.342505 11835 sgd_solver.cpp:112] Iteration 18550, lr = 0.1
I0524 01:32:11.993515 11835 solver.cpp:239] Iteration 18560 (1.50374 iter/s, 6.65009s/10 iters), loss = 6.5643
I0524 01:32:11.993554 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5643 (* 1 = 6.5643 loss)
I0524 01:32:11.993698 11835 sgd_solver.cpp:112] Iteration 18560, lr = 0.1
I0524 01:32:19.792639 11835 solver.cpp:239] Iteration 18570 (1.28225 iter/s, 7.79877s/10 iters), loss = 7.42998
I0524 01:32:19.792697 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.42998 (* 1 = 7.42998 loss)
I0524 01:32:19.792759 11835 sgd_solver.cpp:112] Iteration 18570, lr = 0.1
I0524 01:32:26.898864 11835 solver.cpp:239] Iteration 18580 (1.40728 iter/s, 7.1059s/10 iters), loss = 7.2829
I0524 01:32:26.898931 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2829 (* 1 = 7.2829 loss)
I0524 01:32:26.898964 11835 sgd_solver.cpp:112] Iteration 18580, lr = 0.1
I0524 01:32:33.441395 11835 solver.cpp:239] Iteration 18590 (1.52853 iter/s, 6.54223s/10 iters), loss = 5.99522
I0524 01:32:33.441434 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99522 (* 1 = 5.99522 loss)
I0524 01:32:33.441447 11835 sgd_solver.cpp:112] Iteration 18590, lr = 0.1
I0524 01:32:39.676838 11835 solver.cpp:239] Iteration 18600 (1.60397 iter/s, 6.23453s/10 iters), loss = 7.37704
I0524 01:32:39.677057 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.37704 (* 1 = 7.37704 loss)
I0524 01:32:39.677096 11835 sgd_solver.cpp:112] Iteration 18600, lr = 0.1
I0524 01:32:47.104413 11835 solver.cpp:239] Iteration 18610 (1.3465 iter/s, 7.42668s/10 iters), loss = 6.91925
I0524 01:32:47.104465 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.91925 (* 1 = 6.91925 loss)
I0524 01:32:47.104571 11835 sgd_solver.cpp:112] Iteration 18610, lr = 0.1
I0524 01:32:53.891898 11835 solver.cpp:239] Iteration 18620 (1.47337 iter/s, 6.78718s/10 iters), loss = 6.56622
I0524 01:32:53.891943 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56622 (* 1 = 6.56622 loss)
I0524 01:32:53.892172 11835 sgd_solver.cpp:112] Iteration 18620, lr = 0.1
I0524 01:33:00.304847 11835 solver.cpp:239] Iteration 18630 (1.55941 iter/s, 6.41266s/10 iters), loss = 7.54544
I0524 01:33:00.304886 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.54544 (* 1 = 7.54544 loss)
I0524 01:33:00.318892 11835 sgd_solver.cpp:112] Iteration 18630, lr = 0.1
I0524 01:33:06.368115 11835 solver.cpp:239] Iteration 18640 (1.64935 iter/s, 6.06299s/10 iters), loss = 6.9931
I0524 01:33:06.368170 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9931 (* 1 = 6.9931 loss)
I0524 01:33:06.368185 11835 sgd_solver.cpp:112] Iteration 18640, lr = 0.1
I0524 01:33:12.506151 11835 solver.cpp:239] Iteration 18650 (1.62929 iter/s, 6.13766s/10 iters), loss = 6.35757
I0524 01:33:12.506312 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35757 (* 1 = 6.35757 loss)
I0524 01:33:12.506359 11835 sgd_solver.cpp:112] Iteration 18650, lr = 0.1
I0524 01:33:18.383898 11835 solver.cpp:239] Iteration 18660 (1.70144 iter/s, 5.87737s/10 iters), loss = 6.61971
I0524 01:33:18.383960 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61971 (* 1 = 6.61971 loss)
I0524 01:33:18.383981 11835 sgd_solver.cpp:112] Iteration 18660, lr = 0.1
I0524 01:33:24.299942 11835 solver.cpp:239] Iteration 18670 (1.6904 iter/s, 5.91576s/10 iters), loss = 6.82569
I0524 01:33:24.299984 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.82569 (* 1 = 6.82569 loss)
I0524 01:33:24.300156 11835 sgd_solver.cpp:112] Iteration 18670, lr = 0.1
I0524 01:33:30.542820 11835 solver.cpp:239] Iteration 18680 (1.6019 iter/s, 6.24259s/10 iters), loss = 7.10043
I0524 01:33:30.542870 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10043 (* 1 = 7.10043 loss)
I0524 01:33:30.572633 11835 sgd_solver.cpp:112] Iteration 18680, lr = 0.1
I0524 01:33:36.185457 11835 solver.cpp:239] Iteration 18690 (1.7723 iter/s, 5.64237s/10 iters), loss = 6.9618
I0524 01:33:36.185498 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9618 (* 1 = 6.9618 loss)
I0524 01:33:36.185513 11835 sgd_solver.cpp:112] Iteration 18690, lr = 0.1
I0524 01:33:42.564460 11835 solver.cpp:239] Iteration 18700 (1.56799 iter/s, 6.3776s/10 iters), loss = 7.26551
I0524 01:33:42.564703 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.26551 (* 1 = 7.26551 loss)
I0524 01:33:42.566112 11835 sgd_solver.cpp:112] Iteration 18700, lr = 0.1
I0524 01:33:49.357446 11835 solver.cpp:239] Iteration 18710 (1.47221 iter/s, 6.79252s/10 iters), loss = 7.52129
I0524 01:33:49.357497 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.52129 (* 1 = 7.52129 loss)
I0524 01:33:49.530946 11835 sgd_solver.cpp:112] Iteration 18710, lr = 0.1
I0524 01:33:57.736589 11835 solver.cpp:239] Iteration 18720 (1.19349 iter/s, 8.37877s/10 iters), loss = 7.02888
I0524 01:33:57.736639 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.02888 (* 1 = 7.02888 loss)
I0524 01:33:58.182627 11835 sgd_solver.cpp:112] Iteration 18720, lr = 0.1
I0524 01:34:05.587548 11835 solver.cpp:239] Iteration 18730 (1.27379 iter/s, 7.85061s/10 iters), loss = 7.2066
I0524 01:34:05.587594 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2066 (* 1 = 7.2066 loss)
I0524 01:34:05.588018 11835 sgd_solver.cpp:112] Iteration 18730, lr = 0.1
I0524 01:34:12.934638 11835 solver.cpp:239] Iteration 18740 (1.36114 iter/s, 7.34676s/10 iters), loss = 7.32503
I0524 01:34:12.934839 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.32503 (* 1 = 7.32503 loss)
I0524 01:34:12.934861 11835 sgd_solver.cpp:112] Iteration 18740, lr = 0.1
I0524 01:34:19.766677 11835 solver.cpp:239] Iteration 18750 (1.46389 iter/s, 6.8311s/10 iters), loss = 7.12043
I0524 01:34:19.766767 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.12043 (* 1 = 7.12043 loss)
I0524 01:34:20.017386 11835 sgd_solver.cpp:112] Iteration 18750, lr = 0.1
I0524 01:34:28.450963 11835 solver.cpp:239] Iteration 18760 (1.15156 iter/s, 8.68386s/10 iters), loss = 7.13453
I0524 01:34:28.451016 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13453 (* 1 = 7.13453 loss)
I0524 01:34:28.543311 11835 sgd_solver.cpp:112] Iteration 18760, lr = 0.1
I0524 01:34:34.761811 11835 solver.cpp:239] Iteration 18770 (1.58464 iter/s, 6.31056s/10 iters), loss = 7.13482
I0524 01:34:34.761850 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13482 (* 1 = 7.13482 loss)
I0524 01:34:34.761862 11835 sgd_solver.cpp:112] Iteration 18770, lr = 0.1
I0524 01:34:41.468914 11835 solver.cpp:239] Iteration 18780 (1.49103 iter/s, 6.70676s/10 iters), loss = 6.23569
I0524 01:34:41.468966 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23569 (* 1 = 6.23569 loss)
I0524 01:34:41.667533 11835 sgd_solver.cpp:112] Iteration 18780, lr = 0.1
I0524 01:34:47.923070 11835 solver.cpp:239] Iteration 18790 (1.54946 iter/s, 6.45386s/10 iters), loss = 7.75945
I0524 01:34:47.923336 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.75945 (* 1 = 7.75945 loss)
I0524 01:34:47.923394 11835 sgd_solver.cpp:112] Iteration 18790, lr = 0.1
I0524 01:34:54.611837 11835 solver.cpp:239] Iteration 18800 (1.49515 iter/s, 6.68827s/10 iters), loss = 7.50736
I0524 01:34:54.611879 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.50736 (* 1 = 7.50736 loss)
I0524 01:34:54.611891 11835 sgd_solver.cpp:112] Iteration 18800, lr = 0.1
I0524 01:35:00.545718 11835 solver.cpp:239] Iteration 18810 (1.68563 iter/s, 5.9325s/10 iters), loss = 7.17085
I0524 01:35:00.545771 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.17085 (* 1 = 7.17085 loss)
I0524 01:35:00.545955 11835 sgd_solver.cpp:112] Iteration 18810, lr = 0.1
I0524 01:35:07.653194 11835 solver.cpp:239] Iteration 18820 (1.40703 iter/s, 7.10716s/10 iters), loss = 6.94554
I0524 01:35:07.653240 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.94554 (* 1 = 6.94554 loss)
I0524 01:35:07.653316 11835 sgd_solver.cpp:112] Iteration 18820, lr = 0.1
I0524 01:35:15.013813 11835 solver.cpp:239] Iteration 18830 (1.35864 iter/s, 7.36029s/10 iters), loss = 7.55898
I0524 01:35:15.013870 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.55898 (* 1 = 7.55898 loss)
I0524 01:35:15.961405 11835 sgd_solver.cpp:112] Iteration 18830, lr = 0.1
I0524 01:35:22.272428 11835 solver.cpp:239] Iteration 18840 (1.37774 iter/s, 7.25828s/10 iters), loss = 6.40297
I0524 01:35:22.272666 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40297 (* 1 = 6.40297 loss)
I0524 01:35:22.272712 11835 sgd_solver.cpp:112] Iteration 18840, lr = 0.1
I0524 01:35:28.778636 11835 solver.cpp:239] Iteration 18850 (1.53716 iter/s, 6.5055s/10 iters), loss = 6.40545
I0524 01:35:28.778676 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40545 (* 1 = 6.40545 loss)
I0524 01:35:29.064136 11835 sgd_solver.cpp:112] Iteration 18850, lr = 0.1
I0524 01:35:36.102849 11835 solver.cpp:239] Iteration 18860 (1.36539 iter/s, 7.32389s/10 iters), loss = 8.19722
I0524 01:35:36.102895 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.19722 (* 1 = 8.19722 loss)
I0524 01:35:36.103130 11835 sgd_solver.cpp:112] Iteration 18860, lr = 0.1
I0524 01:35:42.038820 11835 solver.cpp:239] Iteration 18870 (1.68472 iter/s, 5.93569s/10 iters), loss = 7.52155
I0524 01:35:42.038866 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.52155 (* 1 = 7.52155 loss)
I0524 01:35:43.000860 11835 sgd_solver.cpp:112] Iteration 18870, lr = 0.1
I0524 01:35:50.231736 11835 solver.cpp:239] Iteration 18880 (1.22062 iter/s, 8.19256s/10 iters), loss = 7.28331
I0524 01:35:50.231789 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.28331 (* 1 = 7.28331 loss)
I0524 01:35:50.231804 11835 sgd_solver.cpp:112] Iteration 18880, lr = 0.1
I0524 01:35:55.982715 11835 solver.cpp:239] Iteration 18890 (1.73894 iter/s, 5.75063s/10 iters), loss = 7.72388
I0524 01:35:55.982858 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.72388 (* 1 = 7.72388 loss)
I0524 01:35:56.003756 11835 sgd_solver.cpp:112] Iteration 18890, lr = 0.1
I0524 01:36:02.047420 11835 solver.cpp:239] Iteration 18900 (1.64899 iter/s, 6.06432s/10 iters), loss = 6.23047
I0524 01:36:02.047467 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23047 (* 1 = 6.23047 loss)
I0524 01:36:02.047482 11835 sgd_solver.cpp:112] Iteration 18900, lr = 0.1
I0524 01:36:08.580976 11835 solver.cpp:239] Iteration 18910 (1.53065 iter/s, 6.53316s/10 iters), loss = 7.93536
I0524 01:36:08.581014 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.93536 (* 1 = 7.93536 loss)
I0524 01:36:08.581143 11835 sgd_solver.cpp:112] Iteration 18910, lr = 0.1
I0524 01:36:17.174129 11835 solver.cpp:239] Iteration 18920 (1.16377 iter/s, 8.59278s/10 iters), loss = 6.33685
I0524 01:36:17.174178 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33685 (* 1 = 6.33685 loss)
I0524 01:36:17.174561 11835 sgd_solver.cpp:112] Iteration 18920, lr = 0.1
I0524 01:36:23.864230 11835 solver.cpp:239] Iteration 18930 (1.49481 iter/s, 6.68979s/10 iters), loss = 7.67773
I0524 01:36:23.864279 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.67773 (* 1 = 7.67773 loss)
I0524 01:36:24.394582 11835 sgd_solver.cpp:112] Iteration 18930, lr = 0.1
I0524 01:36:30.540463 11835 solver.cpp:239] Iteration 18940 (1.49792 iter/s, 6.67593s/10 iters), loss = 7.13421
I0524 01:36:30.540707 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13421 (* 1 = 7.13421 loss)
I0524 01:36:30.696683 11835 sgd_solver.cpp:112] Iteration 18940, lr = 0.1
I0524 01:36:37.574612 11835 solver.cpp:239] Iteration 18950 (1.42173 iter/s, 7.03366s/10 iters), loss = 7.87482
I0524 01:36:37.574659 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.87482 (* 1 = 7.87482 loss)
I0524 01:36:37.574749 11835 sgd_solver.cpp:112] Iteration 18950, lr = 0.1
I0524 01:36:45.980909 11835 solver.cpp:239] Iteration 18960 (1.18964 iter/s, 8.40593s/10 iters), loss = 7.15666
I0524 01:36:45.980962 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.15666 (* 1 = 7.15666 loss)
I0524 01:36:46.029785 11835 sgd_solver.cpp:112] Iteration 18960, lr = 0.1
I0524 01:36:53.205541 11835 solver.cpp:239] Iteration 18970 (1.38422 iter/s, 7.22431s/10 iters), loss = 6.36037
I0524 01:36:53.205585 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36037 (* 1 = 6.36037 loss)
I0524 01:36:53.206073 11835 sgd_solver.cpp:112] Iteration 18970, lr = 0.1
I0524 01:36:59.811869 11835 solver.cpp:239] Iteration 18980 (1.51377 iter/s, 6.60602s/10 iters), loss = 7.27694
I0524 01:36:59.811918 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.27694 (* 1 = 7.27694 loss)
I0524 01:36:59.812000 11835 sgd_solver.cpp:112] Iteration 18980, lr = 0.1
I0524 01:37:07.282133 11835 solver.cpp:239] Iteration 18990 (1.3387 iter/s, 7.46994s/10 iters), loss = 7.66947
I0524 01:37:07.282219 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.66947 (* 1 = 7.66947 loss)
I0524 01:37:07.282405 11835 sgd_solver.cpp:112] Iteration 18990, lr = 0.1
I0524 01:37:13.302912 11835 solver.cpp:239] Iteration 19000 (1.66101 iter/s, 6.02045s/10 iters), loss = 6.43196
I0524 01:37:13.302970 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43196 (* 1 = 6.43196 loss)
I0524 01:37:13.303040 11835 sgd_solver.cpp:112] Iteration 19000, lr = 0.1
I0524 01:37:20.583708 11835 solver.cpp:239] Iteration 19010 (1.37354 iter/s, 7.28045s/10 iters), loss = 6.95228
I0524 01:37:20.583763 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.95228 (* 1 = 6.95228 loss)
I0524 01:37:20.583811 11835 sgd_solver.cpp:112] Iteration 19010, lr = 0.1
I0524 01:37:26.257385 11835 solver.cpp:239] Iteration 19020 (1.76261 iter/s, 5.67341s/10 iters), loss = 7.75585
I0524 01:37:26.257426 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.75585 (* 1 = 7.75585 loss)
I0524 01:37:26.257654 11835 sgd_solver.cpp:112] Iteration 19020, lr = 0.1
I0524 01:37:33.449026 11835 solver.cpp:239] Iteration 19030 (1.39057 iter/s, 7.19132s/10 iters), loss = 7.40739
I0524 01:37:33.449082 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.40739 (* 1 = 7.40739 loss)
I0524 01:37:33.919590 11835 sgd_solver.cpp:112] Iteration 19030, lr = 0.1
I0524 01:37:40.356122 11835 solver.cpp:239] Iteration 19040 (1.44785 iter/s, 6.90677s/10 iters), loss = 6.64652
I0524 01:37:40.356420 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64652 (* 1 = 6.64652 loss)
I0524 01:37:40.356500 11835 sgd_solver.cpp:112] Iteration 19040, lr = 0.1
I0524 01:37:47.109275 11835 solver.cpp:239] Iteration 19050 (1.48092 iter/s, 6.75256s/10 iters), loss = 7.50239
I0524 01:37:47.109321 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.50239 (* 1 = 7.50239 loss)
I0524 01:37:47.109344 11835 sgd_solver.cpp:112] Iteration 19050, lr = 0.1
I0524 01:37:53.648638 11835 solver.cpp:239] Iteration 19060 (1.52934 iter/s, 6.53876s/10 iters), loss = 6.97711
I0524 01:37:53.648689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.97711 (* 1 = 6.97711 loss)
I0524 01:37:53.658221 11835 sgd_solver.cpp:112] Iteration 19060, lr = 0.1
I0524 01:37:59.525539 11835 solver.cpp:239] Iteration 19070 (1.70165 iter/s, 5.87664s/10 iters), loss = 7.62216
I0524 01:37:59.525573 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.62216 (* 1 = 7.62216 loss)
I0524 01:37:59.525586 11835 sgd_solver.cpp:112] Iteration 19070, lr = 0.1
I0524 01:38:08.705858 11835 solver.cpp:239] Iteration 19080 (1.08933 iter/s, 9.17993s/10 iters), loss = 6.87998
I0524 01:38:08.705915 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87998 (* 1 = 6.87998 loss)
I0524 01:38:08.706148 11835 sgd_solver.cpp:112] Iteration 19080, lr = 0.1
I0524 01:38:14.564286 11835 solver.cpp:239] Iteration 19090 (1.70702 iter/s, 5.85815s/10 iters), loss = 7.73092
I0524 01:38:14.564519 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.73092 (* 1 = 7.73092 loss)
I0524 01:38:14.564563 11835 sgd_solver.cpp:112] Iteration 19090, lr = 0.1
I0524 01:38:20.383514 11835 solver.cpp:239] Iteration 19100 (1.71857 iter/s, 5.81878s/10 iters), loss = 6.61098
I0524 01:38:20.383554 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61098 (* 1 = 6.61098 loss)
I0524 01:38:20.383622 11835 sgd_solver.cpp:112] Iteration 19100, lr = 0.1
I0524 01:38:26.127032 11835 solver.cpp:239] Iteration 19110 (1.74117 iter/s, 5.74325s/10 iters), loss = 7.47133
I0524 01:38:26.127080 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.47133 (* 1 = 7.47133 loss)
I0524 01:38:26.127192 11835 sgd_solver.cpp:112] Iteration 19110, lr = 0.1
I0524 01:38:34.665396 11835 solver.cpp:239] Iteration 19120 (1.17124 iter/s, 8.53799s/10 iters), loss = 6.32117
I0524 01:38:34.665446 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32117 (* 1 = 6.32117 loss)
I0524 01:38:34.994973 11835 sgd_solver.cpp:112] Iteration 19120, lr = 0.1
I0524 01:38:44.306738 11835 solver.cpp:239] Iteration 19130 (1.03724 iter/s, 9.64093s/10 iters), loss = 6.54389
I0524 01:38:44.306789 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54389 (* 1 = 6.54389 loss)
I0524 01:38:44.306804 11835 sgd_solver.cpp:112] Iteration 19130, lr = 0.1
I0524 01:38:50.616884 11835 solver.cpp:239] Iteration 19140 (1.5851 iter/s, 6.30875s/10 iters), loss = 5.75205
I0524 01:38:50.617116 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75205 (* 1 = 5.75205 loss)
I0524 01:38:50.617172 11835 sgd_solver.cpp:112] Iteration 19140, lr = 0.1
I0524 01:38:56.411391 11835 solver.cpp:239] Iteration 19150 (1.72589 iter/s, 5.7941s/10 iters), loss = 6.93001
I0524 01:38:56.411437 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93001 (* 1 = 6.93001 loss)
I0524 01:38:56.411453 11835 sgd_solver.cpp:112] Iteration 19150, lr = 0.1
I0524 01:39:04.127259 11835 solver.cpp:239] Iteration 19160 (1.2961 iter/s, 7.71545s/10 iters), loss = 6.76428
I0524 01:39:04.127311 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76428 (* 1 = 6.76428 loss)
I0524 01:39:04.127442 11835 sgd_solver.cpp:112] Iteration 19160, lr = 0.1
I0524 01:39:10.218075 11835 solver.cpp:239] Iteration 19170 (1.64189 iter/s, 6.09054s/10 iters), loss = 7.17948
I0524 01:39:10.218116 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.17948 (* 1 = 7.17948 loss)
I0524 01:39:11.832046 11835 sgd_solver.cpp:112] Iteration 19170, lr = 0.1
I0524 01:39:17.646356 11835 solver.cpp:239] Iteration 19180 (1.34627 iter/s, 7.42795s/10 iters), loss = 8.19752
I0524 01:39:17.646409 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.19752 (* 1 = 8.19752 loss)
I0524 01:39:17.646461 11835 sgd_solver.cpp:112] Iteration 19180, lr = 0.1
I0524 01:39:24.879964 11835 solver.cpp:239] Iteration 19190 (1.3825 iter/s, 7.23328s/10 iters), loss = 7.48064
I0524 01:39:24.880697 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.48064 (* 1 = 7.48064 loss)
I0524 01:39:24.880733 11835 sgd_solver.cpp:112] Iteration 19190, lr = 0.1
I0524 01:39:32.012673 11835 solver.cpp:239] Iteration 19200 (1.40254 iter/s, 7.1299s/10 iters), loss = 7.11349
I0524 01:39:32.012725 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11349 (* 1 = 7.11349 loss)
I0524 01:39:32.012805 11835 sgd_solver.cpp:112] Iteration 19200, lr = 0.1
I0524 01:39:38.387668 11835 solver.cpp:239] Iteration 19210 (1.56871 iter/s, 6.37468s/10 iters), loss = 7.0055
I0524 01:39:38.387742 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.0055 (* 1 = 7.0055 loss)
I0524 01:39:38.387908 11835 sgd_solver.cpp:112] Iteration 19210, lr = 0.1
I0524 01:39:44.353794 11835 solver.cpp:239] Iteration 19220 (1.67621 iter/s, 5.96584s/10 iters), loss = 6.80737
I0524 01:39:44.353842 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80737 (* 1 = 6.80737 loss)
I0524 01:39:44.354121 11835 sgd_solver.cpp:112] Iteration 19220, lr = 0.1
I0524 01:39:51.467298 11835 solver.cpp:239] Iteration 19230 (1.40584 iter/s, 7.11318s/10 iters), loss = 8.29579
I0524 01:39:51.467347 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.29579 (* 1 = 8.29579 loss)
I0524 01:39:51.467797 11835 sgd_solver.cpp:112] Iteration 19230, lr = 0.1
I0524 01:39:58.459270 11835 solver.cpp:239] Iteration 19240 (1.43028 iter/s, 6.99165s/10 iters), loss = 7.24068
I0524 01:39:58.459534 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.24068 (* 1 = 7.24068 loss)
I0524 01:39:58.769793 11835 sgd_solver.cpp:112] Iteration 19240, lr = 0.1
I0524 01:40:05.037322 11835 solver.cpp:239] Iteration 19250 (1.52031 iter/s, 6.57761s/10 iters), loss = 7.06862
I0524 01:40:05.037356 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.06862 (* 1 = 7.06862 loss)
I0524 01:40:05.144524 11835 sgd_solver.cpp:112] Iteration 19250, lr = 0.1
I0524 01:40:12.871749 11835 solver.cpp:239] Iteration 19260 (1.27647 iter/s, 7.83409s/10 iters), loss = 7.06435
I0524 01:40:12.871798 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.06435 (* 1 = 7.06435 loss)
I0524 01:40:12.871896 11835 sgd_solver.cpp:112] Iteration 19260, lr = 0.1
I0524 01:40:19.815217 11835 solver.cpp:239] Iteration 19270 (1.44027 iter/s, 6.94315s/10 iters), loss = 7.85446
I0524 01:40:19.815266 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.85446 (* 1 = 7.85446 loss)
I0524 01:40:19.815282 11835 sgd_solver.cpp:112] Iteration 19270, lr = 0.1
I0524 01:40:26.755206 11835 solver.cpp:239] Iteration 19280 (1.44099 iter/s, 6.93966s/10 iters), loss = 7.98819
I0524 01:40:26.755254 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.98819 (* 1 = 7.98819 loss)
I0524 01:40:26.755318 11835 sgd_solver.cpp:112] Iteration 19280, lr = 0.1
I0524 01:40:32.449551 11835 solver.cpp:239] Iteration 19290 (1.75621 iter/s, 5.69407s/10 iters), loss = 7.49926
I0524 01:40:32.449862 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.49926 (* 1 = 7.49926 loss)
I0524 01:40:32.940925 11835 sgd_solver.cpp:112] Iteration 19290, lr = 0.1
I0524 01:40:41.225116 11835 solver.cpp:239] Iteration 19300 (1.13961 iter/s, 8.77496s/10 iters), loss = 7.07885
I0524 01:40:41.225172 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.07885 (* 1 = 7.07885 loss)
I0524 01:40:41.225397 11835 sgd_solver.cpp:112] Iteration 19300, lr = 0.1
I0524 01:40:47.260658 11835 solver.cpp:239] Iteration 19310 (1.65693 iter/s, 6.03527s/10 iters), loss = 6.95565
I0524 01:40:47.260695 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.95565 (* 1 = 6.95565 loss)
I0524 01:40:47.260707 11835 sgd_solver.cpp:112] Iteration 19310, lr = 0.1
I0524 01:40:56.324195 11835 solver.cpp:239] Iteration 19320 (1.10342 iter/s, 9.06271s/10 iters), loss = 7.3667
I0524 01:40:56.324245 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.3667 (* 1 = 7.3667 loss)
I0524 01:40:56.395476 11835 sgd_solver.cpp:112] Iteration 19320, lr = 0.1
I0524 01:41:03.407340 11835 solver.cpp:239] Iteration 19330 (1.41187 iter/s, 7.08281s/10 iters), loss = 7.32277
I0524 01:41:03.407546 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.32277 (* 1 = 7.32277 loss)
I0524 01:41:03.407647 11835 sgd_solver.cpp:112] Iteration 19330, lr = 0.1
I0524 01:41:12.696729 11835 solver.cpp:239] Iteration 19340 (1.07656 iter/s, 9.28887s/10 iters), loss = 7.4715
I0524 01:41:12.696768 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.4715 (* 1 = 7.4715 loss)
I0524 01:41:13.577284 11835 sgd_solver.cpp:112] Iteration 19340, lr = 0.1
I0524 01:41:20.316784 11835 solver.cpp:239] Iteration 19350 (1.31239 iter/s, 7.61971s/10 iters), loss = 7.08523
I0524 01:41:20.316836 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.08523 (* 1 = 7.08523 loss)
I0524 01:41:20.316942 11835 sgd_solver.cpp:112] Iteration 19350, lr = 0.1
I0524 01:41:26.572017 11835 solver.cpp:239] Iteration 19360 (1.59874 iter/s, 6.25494s/10 iters), loss = 8.73246
I0524 01:41:26.572072 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.73246 (* 1 = 8.73246 loss)
I0524 01:41:26.572290 11835 sgd_solver.cpp:112] Iteration 19360, lr = 0.1
I0524 01:41:32.393682 11835 solver.cpp:239] Iteration 19370 (1.71781 iter/s, 5.82138s/10 iters), loss = 8.26672
I0524 01:41:32.393739 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.26672 (* 1 = 8.26672 loss)
I0524 01:41:32.393834 11835 sgd_solver.cpp:112] Iteration 19370, lr = 0.1
I0524 01:41:39.529316 11835 solver.cpp:239] Iteration 19380 (1.40148 iter/s, 7.13531s/10 iters), loss = 6.88911
I0524 01:41:39.529436 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88911 (* 1 = 6.88911 loss)
I0524 01:41:39.529487 11835 sgd_solver.cpp:112] Iteration 19380, lr = 0.1
I0524 01:41:46.931273 11835 solver.cpp:239] Iteration 19390 (1.35107 iter/s, 7.40155s/10 iters), loss = 6.9712
I0524 01:41:46.931329 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9712 (* 1 = 6.9712 loss)
I0524 01:41:47.037240 11835 sgd_solver.cpp:112] Iteration 19390, lr = 0.1
I0524 01:41:53.607245 11835 solver.cpp:239] Iteration 19400 (1.49798 iter/s, 6.67566s/10 iters), loss = 6.43122
I0524 01:41:53.607295 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43122 (* 1 = 6.43122 loss)
I0524 01:41:53.607434 11835 sgd_solver.cpp:112] Iteration 19400, lr = 0.1
I0524 01:42:00.517594 11835 solver.cpp:239] Iteration 19410 (1.44717 iter/s, 6.91004s/10 iters), loss = 6.70806
I0524 01:42:00.517635 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70806 (* 1 = 6.70806 loss)
I0524 01:42:00.517729 11835 sgd_solver.cpp:112] Iteration 19410, lr = 0.1
I0524 01:42:07.699410 11835 solver.cpp:239] Iteration 19420 (1.39247 iter/s, 7.1815s/10 iters), loss = 6.94184
I0524 01:42:07.699453 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.94184 (* 1 = 6.94184 loss)
I0524 01:42:07.756968 11835 sgd_solver.cpp:112] Iteration 19420, lr = 0.1
I0524 01:42:15.048923 11835 solver.cpp:239] Iteration 19430 (1.3607 iter/s, 7.34917s/10 iters), loss = 7.12742
I0524 01:42:15.049113 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.12742 (* 1 = 7.12742 loss)
I0524 01:42:15.117655 11835 sgd_solver.cpp:112] Iteration 19430, lr = 0.1
I0524 01:42:21.609298 11835 solver.cpp:239] Iteration 19440 (1.52441 iter/s, 6.55994s/10 iters), loss = 7.28353
I0524 01:42:21.609345 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.28353 (* 1 = 7.28353 loss)
I0524 01:42:21.609359 11835 sgd_solver.cpp:112] Iteration 19440, lr = 0.1
I0524 01:42:28.172216 11835 solver.cpp:239] Iteration 19450 (1.52378 iter/s, 6.56263s/10 iters), loss = 6.93568
I0524 01:42:28.172255 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93568 (* 1 = 6.93568 loss)
I0524 01:42:28.172403 11835 sgd_solver.cpp:112] Iteration 19450, lr = 0.1
I0524 01:42:34.389782 11835 solver.cpp:239] Iteration 19460 (1.60842 iter/s, 6.21728s/10 iters), loss = 7.44561
I0524 01:42:34.389837 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.44561 (* 1 = 7.44561 loss)
I0524 01:42:34.677625 11835 sgd_solver.cpp:112] Iteration 19460, lr = 0.1
I0524 01:42:41.534766 11835 solver.cpp:239] Iteration 19470 (1.39965 iter/s, 7.14466s/10 iters), loss = 6.26712
I0524 01:42:41.534817 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26712 (* 1 = 6.26712 loss)
I0524 01:42:41.535101 11835 sgd_solver.cpp:112] Iteration 19470, lr = 0.1
I0524 01:42:48.640314 11835 solver.cpp:239] Iteration 19480 (1.40742 iter/s, 7.10522s/10 iters), loss = 6.96375
I0524 01:42:48.640555 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96375 (* 1 = 6.96375 loss)
I0524 01:42:48.640619 11835 sgd_solver.cpp:112] Iteration 19480, lr = 0.1
I0524 01:42:56.217648 11835 solver.cpp:239] Iteration 19490 (1.31981 iter/s, 7.57685s/10 iters), loss = 7.23344
I0524 01:42:56.217700 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.23344 (* 1 = 7.23344 loss)
I0524 01:42:56.329694 11835 sgd_solver.cpp:112] Iteration 19490, lr = 0.1
I0524 01:43:02.753952 11835 solver.cpp:239] Iteration 19500 (1.52999 iter/s, 6.536s/10 iters), loss = 6.88245
I0524 01:43:02.753999 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88245 (* 1 = 6.88245 loss)
I0524 01:43:02.754014 11835 sgd_solver.cpp:112] Iteration 19500, lr = 0.1
I0524 01:43:10.766039 11835 solver.cpp:239] Iteration 19510 (1.24834 iter/s, 8.01064s/10 iters), loss = 7.11404
I0524 01:43:10.766083 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11404 (* 1 = 7.11404 loss)
I0524 01:43:11.362366 11835 sgd_solver.cpp:112] Iteration 19510, lr = 0.1
I0524 01:43:17.111853 11835 solver.cpp:239] Iteration 19520 (1.57591 iter/s, 6.34552s/10 iters), loss = 6.77885
I0524 01:43:17.111901 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.77885 (* 1 = 6.77885 loss)
I0524 01:43:17.315676 11835 sgd_solver.cpp:112] Iteration 19520, lr = 0.1
I0524 01:43:24.346902 11835 solver.cpp:239] Iteration 19530 (1.38222 iter/s, 7.23472s/10 iters), loss = 6.94048
I0524 01:43:24.347168 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.94048 (* 1 = 6.94048 loss)
I0524 01:43:24.348836 11835 sgd_solver.cpp:112] Iteration 19530, lr = 0.1
I0524 01:43:31.481884 11835 solver.cpp:239] Iteration 19540 (1.40164 iter/s, 7.13449s/10 iters), loss = 7.90883
I0524 01:43:31.481932 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.90883 (* 1 = 7.90883 loss)
I0524 01:43:32.069028 11835 sgd_solver.cpp:112] Iteration 19540, lr = 0.1
I0524 01:43:39.813587 11835 solver.cpp:239] Iteration 19550 (1.20029 iter/s, 8.33134s/10 iters), loss = 6.39042
I0524 01:43:39.813632 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39042 (* 1 = 6.39042 loss)
I0524 01:43:39.830400 11835 sgd_solver.cpp:112] Iteration 19550, lr = 0.1
I0524 01:43:48.376711 11835 solver.cpp:239] Iteration 19560 (1.16785 iter/s, 8.56275s/10 iters), loss = 7.89392
I0524 01:43:48.376756 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.89392 (* 1 = 7.89392 loss)
I0524 01:43:48.686267 11835 sgd_solver.cpp:112] Iteration 19560, lr = 0.1
I0524 01:43:56.244173 11835 solver.cpp:239] Iteration 19570 (1.27111 iter/s, 7.86712s/10 iters), loss = 8.18073
I0524 01:43:56.244310 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.18073 (* 1 = 8.18073 loss)
I0524 01:43:56.244326 11835 sgd_solver.cpp:112] Iteration 19570, lr = 0.1
I0524 01:44:02.856369 11835 solver.cpp:239] Iteration 19580 (1.51248 iter/s, 6.61164s/10 iters), loss = 6.72345
I0524 01:44:02.856415 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72345 (* 1 = 6.72345 loss)
I0524 01:44:02.856428 11835 sgd_solver.cpp:112] Iteration 19580, lr = 0.1
I0524 01:44:09.500167 11835 solver.cpp:239] Iteration 19590 (1.50525 iter/s, 6.64341s/10 iters), loss = 6.75849
I0524 01:44:09.500206 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.75849 (* 1 = 6.75849 loss)
I0524 01:44:09.500218 11835 sgd_solver.cpp:112] Iteration 19590, lr = 0.1
I0524 01:44:17.535567 11835 solver.cpp:239] Iteration 19600 (1.24455 iter/s, 8.03504s/10 iters), loss = 8.13002
I0524 01:44:17.535624 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.13002 (* 1 = 8.13002 loss)
I0524 01:44:17.535812 11835 sgd_solver.cpp:112] Iteration 19600, lr = 0.1
I0524 01:44:25.711447 11835 solver.cpp:239] Iteration 19610 (1.22316 iter/s, 8.17552s/10 iters), loss = 7.60598
I0524 01:44:25.711488 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.60598 (* 1 = 7.60598 loss)
I0524 01:44:26.500108 11835 sgd_solver.cpp:112] Iteration 19610, lr = 0.1
I0524 01:44:33.578939 11835 solver.cpp:239] Iteration 19620 (1.27111 iter/s, 7.86714s/10 iters), loss = 7.24728
I0524 01:44:33.578994 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.24728 (* 1 = 7.24728 loss)
I0524 01:44:33.579174 11835 sgd_solver.cpp:112] Iteration 19620, lr = 0.1
I0524 01:44:39.544450 11835 solver.cpp:239] Iteration 19630 (1.67638 iter/s, 5.96524s/10 iters), loss = 7.10684
I0524 01:44:39.544492 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10684 (* 1 = 7.10684 loss)
I0524 01:44:40.030043 11835 sgd_solver.cpp:112] Iteration 19630, lr = 0.1
I0524 01:44:45.774761 11835 solver.cpp:239] Iteration 19640 (1.60513 iter/s, 6.23002s/10 iters), loss = 7.33935
I0524 01:44:45.774806 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.33935 (* 1 = 7.33935 loss)
I0524 01:44:45.774821 11835 sgd_solver.cpp:112] Iteration 19640, lr = 0.1
I0524 01:44:52.728791 11835 solver.cpp:239] Iteration 19650 (1.43809 iter/s, 6.95366s/10 iters), loss = 8.18717
I0524 01:44:52.728842 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.18717 (* 1 = 8.18717 loss)
I0524 01:44:52.729259 11835 sgd_solver.cpp:112] Iteration 19650, lr = 0.1
I0524 01:45:01.512944 11835 solver.cpp:239] Iteration 19660 (1.13847 iter/s, 8.78375s/10 iters), loss = 6.48569
I0524 01:45:01.513068 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48569 (* 1 = 6.48569 loss)
I0524 01:45:01.513087 11835 sgd_solver.cpp:112] Iteration 19660, lr = 0.1
I0524 01:45:08.753355 11835 solver.cpp:239] Iteration 19670 (1.38137 iter/s, 7.23919s/10 iters), loss = 7.18157
I0524 01:45:08.753394 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.18157 (* 1 = 7.18157 loss)
I0524 01:45:08.771952 11835 sgd_solver.cpp:112] Iteration 19670, lr = 0.1
I0524 01:45:15.142823 11835 solver.cpp:239] Iteration 19680 (1.56515 iter/s, 6.38916s/10 iters), loss = 6.95961
I0524 01:45:15.142880 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.95961 (* 1 = 6.95961 loss)
I0524 01:45:15.143049 11835 sgd_solver.cpp:112] Iteration 19680, lr = 0.1
I0524 01:45:21.030360 11835 solver.cpp:239] Iteration 19690 (1.69859 iter/s, 5.88725s/10 iters), loss = 7.68509
I0524 01:45:21.030411 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.68509 (* 1 = 7.68509 loss)
I0524 01:45:21.350419 11835 sgd_solver.cpp:112] Iteration 19690, lr = 0.1
I0524 01:45:27.254885 11835 solver.cpp:239] Iteration 19700 (1.60662 iter/s, 6.22424s/10 iters), loss = 6.78985
I0524 01:45:27.254932 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78985 (* 1 = 6.78985 loss)
I0524 01:45:27.254951 11835 sgd_solver.cpp:112] Iteration 19700, lr = 0.1
I0524 01:45:33.748010 11835 solver.cpp:239] Iteration 19710 (1.54023 iter/s, 6.49256s/10 iters), loss = 7.03092
I0524 01:45:33.748297 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03092 (* 1 = 7.03092 loss)
I0524 01:45:34.693655 11835 sgd_solver.cpp:112] Iteration 19710, lr = 0.1
I0524 01:45:42.039760 11835 solver.cpp:239] Iteration 19720 (1.2061 iter/s, 8.29119s/10 iters), loss = 7.35748
I0524 01:45:42.039800 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.35748 (* 1 = 7.35748 loss)
I0524 01:45:42.188335 11835 sgd_solver.cpp:112] Iteration 19720, lr = 0.1
I0524 01:45:48.989693 11835 solver.cpp:239] Iteration 19730 (1.43893 iter/s, 6.94962s/10 iters), loss = 7.26128
I0524 01:45:48.989745 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.26128 (* 1 = 7.26128 loss)
I0524 01:45:49.020105 11835 sgd_solver.cpp:112] Iteration 19730, lr = 0.1
I0524 01:45:55.442960 11835 solver.cpp:239] Iteration 19740 (1.54968 iter/s, 6.45296s/10 iters), loss = 7.31827
I0524 01:45:55.443017 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.31827 (* 1 = 7.31827 loss)
I0524 01:45:55.443234 11835 sgd_solver.cpp:112] Iteration 19740, lr = 0.1
I0524 01:46:01.952172 11835 solver.cpp:239] Iteration 19750 (1.53635 iter/s, 6.50891s/10 iters), loss = 7.62145
I0524 01:46:01.952209 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.62145 (* 1 = 7.62145 loss)
I0524 01:46:01.952424 11835 sgd_solver.cpp:112] Iteration 19750, lr = 0.1
I0524 01:46:07.580097 11835 solver.cpp:239] Iteration 19760 (1.77694 iter/s, 5.62767s/10 iters), loss = 7.70264
I0524 01:46:07.580173 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.70264 (* 1 = 7.70264 loss)
I0524 01:46:07.580189 11835 sgd_solver.cpp:112] Iteration 19760, lr = 0.1
I0524 01:46:14.100891 11835 solver.cpp:239] Iteration 19770 (1.53369 iter/s, 6.52022s/10 iters), loss = 7.27384
I0524 01:46:14.100946 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.27384 (* 1 = 7.27384 loss)
I0524 01:46:14.101034 11835 sgd_solver.cpp:112] Iteration 19770, lr = 0.1
I0524 01:46:19.828171 11835 solver.cpp:239] Iteration 19780 (1.74611 iter/s, 5.72701s/10 iters), loss = 6.33687
I0524 01:46:19.828224 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33687 (* 1 = 6.33687 loss)
I0524 01:46:20.243698 11835 sgd_solver.cpp:112] Iteration 19780, lr = 0.1
I0524 01:46:27.267551 11835 solver.cpp:239] Iteration 19790 (1.34426 iter/s, 7.43903s/10 iters), loss = 6.41824
I0524 01:46:27.267611 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41824 (* 1 = 6.41824 loss)
I0524 01:46:27.267884 11835 sgd_solver.cpp:112] Iteration 19790, lr = 0.1
I0524 01:46:33.940762 11835 solver.cpp:239] Iteration 19800 (1.4986 iter/s, 6.6729s/10 iters), loss = 7.76931
I0524 01:46:33.940798 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.76931 (* 1 = 7.76931 loss)
I0524 01:46:33.941000 11835 sgd_solver.cpp:112] Iteration 19800, lr = 0.1
I0524 01:46:40.491662 11835 solver.cpp:239] Iteration 19810 (1.52658 iter/s, 6.5506s/10 iters), loss = 6.06868
I0524 01:46:40.491761 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06868 (* 1 = 6.06868 loss)
I0524 01:46:41.086751 11835 sgd_solver.cpp:112] Iteration 19810, lr = 0.1
I0524 01:46:46.820745 11835 solver.cpp:239] Iteration 19820 (1.58009 iter/s, 6.32875s/10 iters), loss = 6.93506
I0524 01:46:46.820782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93506 (* 1 = 6.93506 loss)
I0524 01:46:46.884580 11835 sgd_solver.cpp:112] Iteration 19820, lr = 0.1
I0524 01:46:55.653101 11835 solver.cpp:239] Iteration 19830 (1.13225 iter/s, 8.83197s/10 iters), loss = 6.96124
I0524 01:46:55.653154 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96124 (* 1 = 6.96124 loss)
I0524 01:46:55.653409 11835 sgd_solver.cpp:112] Iteration 19830, lr = 0.1
I0524 01:47:03.068641 11835 solver.cpp:239] Iteration 19840 (1.34858 iter/s, 7.4152s/10 iters), loss = 7.30115
I0524 01:47:03.068693 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.30115 (* 1 = 7.30115 loss)
I0524 01:47:03.512348 11835 sgd_solver.cpp:112] Iteration 19840, lr = 0.1
I0524 01:47:11.545218 11835 solver.cpp:239] Iteration 19850 (1.17977 iter/s, 8.4762s/10 iters), loss = 7.48758
I0524 01:47:11.545400 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.48758 (* 1 = 7.48758 loss)
I0524 01:47:11.545426 11835 sgd_solver.cpp:112] Iteration 19850, lr = 0.1
I0524 01:47:18.617756 11835 solver.cpp:239] Iteration 19860 (1.41401 iter/s, 7.07209s/10 iters), loss = 8.30527
I0524 01:47:18.617815 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.30527 (* 1 = 8.30527 loss)
I0524 01:47:19.015982 11835 sgd_solver.cpp:112] Iteration 19860, lr = 0.1
I0524 01:47:25.762909 11835 solver.cpp:239] Iteration 19870 (1.39961 iter/s, 7.14483s/10 iters), loss = 7.11926
I0524 01:47:25.762955 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11926 (* 1 = 7.11926 loss)
I0524 01:47:25.763052 11835 sgd_solver.cpp:112] Iteration 19870, lr = 0.1
I0524 01:47:32.060175 11835 solver.cpp:239] Iteration 19880 (1.58807 iter/s, 6.29696s/10 iters), loss = 5.84065
I0524 01:47:32.060233 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84065 (* 1 = 5.84065 loss)
I0524 01:47:32.060400 11835 sgd_solver.cpp:112] Iteration 19880, lr = 0.1
I0524 01:47:37.693943 11835 solver.cpp:239] Iteration 19890 (1.7751 iter/s, 5.6335s/10 iters), loss = 7.1802
I0524 01:47:37.693984 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.1802 (* 1 = 7.1802 loss)
I0524 01:47:37.694264 11835 sgd_solver.cpp:112] Iteration 19890, lr = 0.1
I0524 01:47:44.654387 11835 solver.cpp:239] Iteration 19900 (1.43676 iter/s, 6.96013s/10 iters), loss = 7.49891
I0524 01:47:44.654670 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.49891 (* 1 = 7.49891 loss)
I0524 01:47:44.654767 11835 sgd_solver.cpp:112] Iteration 19900, lr = 0.1
I0524 01:47:50.477798 11835 solver.cpp:239] Iteration 19910 (1.71762 iter/s, 5.82202s/10 iters), loss = 7.11391
I0524 01:47:50.477849 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11391 (* 1 = 7.11391 loss)
I0524 01:47:50.477864 11835 sgd_solver.cpp:112] Iteration 19910, lr = 0.1
I0524 01:47:56.729225 11835 solver.cpp:239] Iteration 19920 (1.59999 iter/s, 6.25002s/10 iters), loss = 7.60633
I0524 01:47:56.729274 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.60633 (* 1 = 7.60633 loss)
I0524 01:47:56.729288 11835 sgd_solver.cpp:112] Iteration 19920, lr = 0.1
I0524 01:48:03.529086 11835 solver.cpp:239] Iteration 19930 (1.4707 iter/s, 6.79949s/10 iters), loss = 7.07362
I0524 01:48:03.529136 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.07362 (* 1 = 7.07362 loss)
I0524 01:48:03.529232 11835 sgd_solver.cpp:112] Iteration 19930, lr = 0.1
I0524 01:48:10.362474 11835 solver.cpp:239] Iteration 19940 (1.46347 iter/s, 6.83308s/10 iters), loss = 7.16735
I0524 01:48:10.362512 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.16735 (* 1 = 7.16735 loss)
I0524 01:48:10.362640 11835 sgd_solver.cpp:112] Iteration 19940, lr = 0.1
I0524 01:48:20.332509 11835 solver.cpp:239] Iteration 19950 (1.00305 iter/s, 9.96962s/10 iters), loss = 7.58886
I0524 01:48:20.332607 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.58886 (* 1 = 7.58886 loss)
I0524 01:48:21.206097 11835 sgd_solver.cpp:112] Iteration 19950, lr = 0.1
I0524 01:48:27.896558 11835 solver.cpp:239] Iteration 19960 (1.32211 iter/s, 7.56365s/10 iters), loss = 6.83467
I0524 01:48:27.896610 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83467 (* 1 = 6.83467 loss)
I0524 01:48:27.896708 11835 sgd_solver.cpp:112] Iteration 19960, lr = 0.1
I0524 01:48:34.313247 11835 solver.cpp:239] Iteration 19970 (1.55851 iter/s, 6.41639s/10 iters), loss = 6.54545
I0524 01:48:34.313290 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54545 (* 1 = 6.54545 loss)
I0524 01:48:34.477356 11835 sgd_solver.cpp:112] Iteration 19970, lr = 0.1
I0524 01:48:40.797808 11835 solver.cpp:239] Iteration 19980 (1.5422 iter/s, 6.48425s/10 iters), loss = 6.76275
I0524 01:48:40.797863 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76275 (* 1 = 6.76275 loss)
I0524 01:48:40.798108 11835 sgd_solver.cpp:112] Iteration 19980, lr = 0.1
I0524 01:48:46.916121 11835 solver.cpp:239] Iteration 19990 (1.63451 iter/s, 6.11803s/10 iters), loss = 5.73107
I0524 01:48:46.916162 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73107 (* 1 = 5.73107 loss)
I0524 01:48:46.916193 11835 sgd_solver.cpp:112] Iteration 19990, lr = 0.1
I0524 01:48:54.084025 11835 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_20000.caffemodel
I0524 01:48:54.876823 11835 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_20000.solverstate
I0524 01:48:55.442605 11835 solver.cpp:239] Iteration 20000 (1.17287 iter/s, 8.52612s/10 iters), loss = 7.16354
I0524 01:48:55.442644 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.16354 (* 1 = 7.16354 loss)
I0524 01:48:55.442656 11835 sgd_solver.cpp:112] Iteration 20000, lr = 0.1
I0524 01:49:02.546248 11835 solver.cpp:239] Iteration 20010 (1.40779 iter/s, 7.10332s/10 iters), loss = 7.36287
I0524 01:49:02.546303 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.36287 (* 1 = 7.36287 loss)
I0524 01:49:03.066010 11835 sgd_solver.cpp:112] Iteration 20010, lr = 0.1
I0524 01:49:10.858907 11835 solver.cpp:239] Iteration 20020 (1.20304 iter/s, 8.3123s/10 iters), loss = 7.72024
I0524 01:49:10.858949 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.72024 (* 1 = 7.72024 loss)
I0524 01:49:11.355306 11835 sgd_solver.cpp:112] Iteration 20020, lr = 0.1
I0524 01:49:18.650876 11835 solver.cpp:239] Iteration 20030 (1.28343 iter/s, 7.79162s/10 iters), loss = 6.91001
I0524 01:49:18.650934 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.91001 (* 1 = 6.91001 loss)
I0524 01:49:18.651034 11835 sgd_solver.cpp:112] Iteration 20030, lr = 0.1
I0524 01:49:25.342870 11835 solver.cpp:239] Iteration 20040 (1.49439 iter/s, 6.69169s/10 iters), loss = 6.49791
I0524 01:49:25.342945 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49791 (* 1 = 6.49791 loss)
I0524 01:49:26.479413 11835 sgd_solver.cpp:112] Iteration 20040, lr = 0.1
I0524 01:49:32.727699 11835 solver.cpp:239] Iteration 20050 (1.35419 iter/s, 7.38447s/10 iters), loss = 7.39158
I0524 01:49:32.727747 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.39158 (* 1 = 7.39158 loss)
I0524 01:49:32.755309 11835 sgd_solver.cpp:112] Iteration 20050, lr = 0.1
I0524 01:49:39.371649 11835 solver.cpp:239] Iteration 20060 (1.5052 iter/s, 6.64364s/10 iters), loss = 7.57839
I0524 01:49:39.371704 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.57839 (* 1 = 7.57839 loss)
I0524 01:49:39.719710 11835 sgd_solver.cpp:112] Iteration 20060, lr = 0.1
I0524 01:49:46.411307 11835 solver.cpp:239] Iteration 20070 (1.42059 iter/s, 7.03934s/10 iters), loss = 6.36401
I0524 01:49:46.411355 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36401 (* 1 = 6.36401 loss)
I0524 01:49:46.411481 11835 sgd_solver.cpp:112] Iteration 20070, lr = 0.1
I0524 01:49:52.129406 11835 solver.cpp:239] Iteration 20080 (1.74892 iter/s, 5.71781s/10 iters), loss = 7.27547
I0524 01:49:52.129457 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.27547 (* 1 = 7.27547 loss)
I0524 01:49:52.129473 11835 sgd_solver.cpp:112] Iteration 20080, lr = 0.1
I0524 01:49:58.935626 11835 solver.cpp:239] Iteration 20090 (1.46931 iter/s, 6.80591s/10 iters), loss = 6.93857
I0524 01:49:58.935878 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93857 (* 1 = 6.93857 loss)
I0524 01:49:58.935923 11835 sgd_solver.cpp:112] Iteration 20090, lr = 0.1
I0524 01:50:05.890314 11835 solver.cpp:239] Iteration 20100 (1.43802 iter/s, 6.95401s/10 iters), loss = 7.26056
I0524 01:50:05.890354 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.26056 (* 1 = 7.26056 loss)
I0524 01:50:05.890370 11835 sgd_solver.cpp:112] Iteration 20100, lr = 0.1
I0524 01:50:13.172346 11835 solver.cpp:239] Iteration 20110 (1.37331 iter/s, 7.2817s/10 iters), loss = 6.3529
I0524 01:50:13.172407 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3529 (* 1 = 6.3529 loss)
I0524 01:50:13.172580 11835 sgd_solver.cpp:112] Iteration 20110, lr = 0.1
I0524 01:50:18.711378 11835 solver.cpp:239] Iteration 20120 (1.80546 iter/s, 5.53876s/10 iters), loss = 7.76945
I0524 01:50:18.711433 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.76945 (* 1 = 7.76945 loss)
I0524 01:50:18.711699 11835 sgd_solver.cpp:112] Iteration 20120, lr = 0.1
I0524 01:50:25.512354 11835 solver.cpp:239] Iteration 20130 (1.47044 iter/s, 6.80067s/10 iters), loss = 8.301
I0524 01:50:25.512400 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.301 (* 1 = 8.301 loss)
I0524 01:50:25.512655 11835 sgd_solver.cpp:112] Iteration 20130, lr = 0.1
I0524 01:50:35.495770 11835 solver.cpp:239] Iteration 20140 (1.0017 iter/s, 9.98298s/10 iters), loss = 6.74161
I0524 01:50:35.495934 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74161 (* 1 = 6.74161 loss)
I0524 01:50:35.496330 11835 sgd_solver.cpp:112] Iteration 20140, lr = 0.1
I0524 01:50:42.541155 11835 solver.cpp:239] Iteration 20150 (1.41945 iter/s, 7.04497s/10 iters), loss = 7.74538
I0524 01:50:42.541205 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.74538 (* 1 = 7.74538 loss)
I0524 01:50:42.541435 11835 sgd_solver.cpp:112] Iteration 20150, lr = 0.1
I0524 01:50:51.273809 11835 solver.cpp:239] Iteration 20160 (1.14518 iter/s, 8.73228s/10 iters), loss = 6.84111
I0524 01:50:51.273849 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84111 (* 1 = 6.84111 loss)
I0524 01:50:51.274140 11835 sgd_solver.cpp:112] Iteration 20160, lr = 0.1
I0524 01:50:58.276927 11835 solver.cpp:239] Iteration 20170 (1.428 iter/s, 7.00279s/10 iters), loss = 6.90512
I0524 01:50:58.276986 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90512 (* 1 = 6.90512 loss)
I0524 01:50:58.277148 11835 sgd_solver.cpp:112] Iteration 20170, lr = 0.1
I0524 01:51:04.971428 11835 solver.cpp:239] Iteration 20180 (1.49383 iter/s, 6.69419s/10 iters), loss = 6.92079
I0524 01:51:04.971482 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92079 (* 1 = 6.92079 loss)
I0524 01:51:04.971935 11835 sgd_solver.cpp:112] Iteration 20180, lr = 0.1
I0524 01:51:10.853397 11835 solver.cpp:239] Iteration 20190 (1.70019 iter/s, 5.88169s/10 iters), loss = 6.01999
I0524 01:51:10.853665 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01999 (* 1 = 6.01999 loss)
I0524 01:51:10.853721 11835 sgd_solver.cpp:112] Iteration 20190, lr = 0.1
I0524 01:51:17.584795 11835 solver.cpp:239] Iteration 20200 (1.48569 iter/s, 6.73089s/10 iters), loss = 8.2108
I0524 01:51:17.584856 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.2108 (* 1 = 8.2108 loss)
I0524 01:51:17.584873 11835 sgd_solver.cpp:112] Iteration 20200, lr = 0.1
I0524 01:51:24.855689 11835 solver.cpp:239] Iteration 20210 (1.37541 iter/s, 7.27057s/10 iters), loss = 6.73354
I0524 01:51:24.855726 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73354 (* 1 = 6.73354 loss)
I0524 01:51:24.855844 11835 sgd_solver.cpp:112] Iteration 20210, lr = 0.1
I0524 01:51:31.578730 11835 solver.cpp:239] Iteration 20220 (1.48749 iter/s, 6.72273s/10 iters), loss = 7.5093
I0524 01:51:31.578778 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.5093 (* 1 = 7.5093 loss)
I0524 01:51:31.578888 11835 sgd_solver.cpp:112] Iteration 20220, lr = 0.1
I0524 01:51:39.203497 11835 solver.cpp:239] Iteration 20230 (1.31157 iter/s, 7.62443s/10 iters), loss = 7.37367
I0524 01:51:39.203552 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.37367 (* 1 = 7.37367 loss)
I0524 01:51:39.203768 11835 sgd_solver.cpp:112] Iteration 20230, lr = 0.1
I0524 01:51:45.604846 11835 solver.cpp:239] Iteration 20240 (1.56224 iter/s, 6.40105s/10 iters), loss = 5.97913
I0524 01:51:45.605114 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97913 (* 1 = 5.97913 loss)
I0524 01:51:45.605171 11835 sgd_solver.cpp:112] Iteration 20240, lr = 0.1
I0524 01:51:52.129223 11835 solver.cpp:239] Iteration 20250 (1.53286 iter/s, 6.52375s/10 iters), loss = 6.87581
I0524 01:51:52.129276 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87581 (* 1 = 6.87581 loss)
I0524 01:51:52.129369 11835 sgd_solver.cpp:112] Iteration 20250, lr = 0.1
I0524 01:52:00.149878 11835 solver.cpp:239] Iteration 20260 (1.24684 iter/s, 8.02028s/10 iters), loss = 6.17803
I0524 01:52:00.149932 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17803 (* 1 = 6.17803 loss)
I0524 01:52:00.150172 11835 sgd_solver.cpp:112] Iteration 20260, lr = 0.1
I0524 01:52:07.347112 11835 solver.cpp:239] Iteration 20270 (1.38948 iter/s, 7.19691s/10 iters), loss = 7.01216
I0524 01:52:07.347158 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01216 (* 1 = 7.01216 loss)
I0524 01:52:07.347477 11835 sgd_solver.cpp:112] Iteration 20270, lr = 0.1
I0524 01:52:15.261291 11835 solver.cpp:239] Iteration 20280 (1.26361 iter/s, 7.91383s/10 iters), loss = 6.80794
I0524 01:52:15.261333 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80794 (* 1 = 6.80794 loss)
I0524 01:52:15.261348 11835 sgd_solver.cpp:112] Iteration 20280, lr = 0.1
I0524 01:52:20.977463 11835 solver.cpp:239] Iteration 20290 (1.7495 iter/s, 5.71591s/10 iters), loss = 7.52451
I0524 01:52:20.977718 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.52451 (* 1 = 7.52451 loss)
I0524 01:52:20.979485 11835 sgd_solver.cpp:112] Iteration 20290, lr = 0.1
I0524 01:52:26.722940 11835 solver.cpp:239] Iteration 20300 (1.74064 iter/s, 5.74502s/10 iters), loss = 7.09839
I0524 01:52:26.722987 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09839 (* 1 = 7.09839 loss)
I0524 01:52:26.723239 11835 sgd_solver.cpp:112] Iteration 20300, lr = 0.1
I0524 01:52:32.842532 11835 solver.cpp:239] Iteration 20310 (1.63417 iter/s, 6.11932s/10 iters), loss = 7.1107
I0524 01:52:32.842571 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.1107 (* 1 = 7.1107 loss)
I0524 01:52:32.842582 11835 sgd_solver.cpp:112] Iteration 20310, lr = 0.1
I0524 01:52:40.586727 11835 solver.cpp:239] Iteration 20320 (1.29135 iter/s, 7.74382s/10 iters), loss = 6.85522
I0524 01:52:40.586776 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.85522 (* 1 = 6.85522 loss)
I0524 01:52:40.586946 11835 sgd_solver.cpp:112] Iteration 20320, lr = 0.1
I0524 01:52:48.216567 11835 solver.cpp:239] Iteration 20330 (1.3107 iter/s, 7.6295s/10 iters), loss = 6.25521
I0524 01:52:48.216625 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25521 (* 1 = 6.25521 loss)
I0524 01:52:48.217149 11835 sgd_solver.cpp:112] Iteration 20330, lr = 0.1
I0524 01:52:55.879371 11835 solver.cpp:239] Iteration 20340 (1.30506 iter/s, 7.66246s/10 iters), loss = 8.07352
I0524 01:52:55.879515 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.07352 (* 1 = 8.07352 loss)
I0524 01:52:55.879712 11835 sgd_solver.cpp:112] Iteration 20340, lr = 0.1
I0524 01:53:03.109702 11835 solver.cpp:239] Iteration 20350 (1.38314 iter/s, 7.22991s/10 iters), loss = 6.30898
I0524 01:53:03.109751 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30898 (* 1 = 6.30898 loss)
I0524 01:53:03.558379 11835 sgd_solver.cpp:112] Iteration 20350, lr = 0.1
I0524 01:53:10.831725 11835 solver.cpp:239] Iteration 20360 (1.29506 iter/s, 7.72167s/10 iters), loss = 6.95747
I0524 01:53:10.831780 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.95747 (* 1 = 6.95747 loss)
I0524 01:53:10.832175 11835 sgd_solver.cpp:112] Iteration 20360, lr = 0.1
I0524 01:53:17.470566 11835 solver.cpp:239] Iteration 20370 (1.50636 iter/s, 6.63853s/10 iters), loss = 7.94155
I0524 01:53:17.470618 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.94155 (* 1 = 7.94155 loss)
I0524 01:53:17.867033 11835 sgd_solver.cpp:112] Iteration 20370, lr = 0.1
I0524 01:53:25.811398 11835 solver.cpp:239] Iteration 20380 (1.19898 iter/s, 8.34046s/10 iters), loss = 6.18411
I0524 01:53:25.811448 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18411 (* 1 = 6.18411 loss)
I0524 01:53:25.811986 11835 sgd_solver.cpp:112] Iteration 20380, lr = 0.1
I0524 01:53:32.268046 11835 solver.cpp:239] Iteration 20390 (1.54886 iter/s, 6.45636s/10 iters), loss = 7.12981
I0524 01:53:32.268158 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.12981 (* 1 = 7.12981 loss)
I0524 01:53:32.268517 11835 sgd_solver.cpp:112] Iteration 20390, lr = 0.1
I0524 01:53:39.444663 11835 solver.cpp:239] Iteration 20400 (1.39349 iter/s, 7.17622s/10 iters), loss = 6.71402
I0524 01:53:39.444720 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71402 (* 1 = 6.71402 loss)
I0524 01:53:40.177067 11835 sgd_solver.cpp:112] Iteration 20400, lr = 0.1
I0524 01:53:47.664548 11835 solver.cpp:239] Iteration 20410 (1.21662 iter/s, 8.21951s/10 iters), loss = 5.92795
I0524 01:53:47.664605 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92795 (* 1 = 5.92795 loss)
I0524 01:53:47.666540 11835 sgd_solver.cpp:112] Iteration 20410, lr = 0.1
I0524 01:53:53.604277 11835 solver.cpp:239] Iteration 20420 (1.68366 iter/s, 5.93945s/10 iters), loss = 6.19811
I0524 01:53:53.604316 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19811 (* 1 = 6.19811 loss)
I0524 01:53:53.604328 11835 sgd_solver.cpp:112] Iteration 20420, lr = 0.1
I0524 01:54:00.362457 11835 solver.cpp:239] Iteration 20430 (1.47976 iter/s, 6.75787s/10 iters), loss = 7.4897
I0524 01:54:00.362516 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.4897 (* 1 = 7.4897 loss)
I0524 01:54:00.362735 11835 sgd_solver.cpp:112] Iteration 20430, lr = 0.1
I0524 01:54:07.222607 11835 solver.cpp:239] Iteration 20440 (1.45776 iter/s, 6.85983s/10 iters), loss = 7.39829
I0524 01:54:07.222744 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.39829 (* 1 = 7.39829 loss)
I0524 01:54:07.222770 11835 sgd_solver.cpp:112] Iteration 20440, lr = 0.1
I0524 01:54:13.129557 11835 solver.cpp:239] Iteration 20450 (1.69302 iter/s, 5.90659s/10 iters), loss = 7.26215
I0524 01:54:13.129595 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.26215 (* 1 = 7.26215 loss)
I0524 01:54:13.131036 11835 sgd_solver.cpp:112] Iteration 20450, lr = 0.1
I0524 01:54:19.265923 11835 solver.cpp:239] Iteration 20460 (1.6297 iter/s, 6.13609s/10 iters), loss = 6.9872
I0524 01:54:19.265974 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9872 (* 1 = 6.9872 loss)
I0524 01:54:19.265991 11835 sgd_solver.cpp:112] Iteration 20460, lr = 0.1
I0524 01:54:25.306242 11835 solver.cpp:239] Iteration 20470 (1.65564 iter/s, 6.03997s/10 iters), loss = 5.94649
I0524 01:54:25.306283 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94649 (* 1 = 5.94649 loss)
I0524 01:54:25.306370 11835 sgd_solver.cpp:112] Iteration 20470, lr = 0.1
I0524 01:54:32.431141 11835 solver.cpp:239] Iteration 20480 (1.4036 iter/s, 7.12456s/10 iters), loss = 6.89076
I0524 01:54:32.431221 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89076 (* 1 = 6.89076 loss)
I0524 01:54:33.176157 11835 sgd_solver.cpp:112] Iteration 20480, lr = 0.1
I0524 01:54:38.862597 11835 solver.cpp:239] Iteration 20490 (1.55493 iter/s, 6.43114s/10 iters), loss = 7.15531
I0524 01:54:38.862752 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.15531 (* 1 = 7.15531 loss)
I0524 01:54:38.862767 11835 sgd_solver.cpp:112] Iteration 20490, lr = 0.1
I0524 01:54:47.831342 11835 solver.cpp:239] Iteration 20500 (1.11508 iter/s, 8.96796s/10 iters), loss = 6.60316
I0524 01:54:47.831379 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60316 (* 1 = 6.60316 loss)
I0524 01:54:48.791968 11835 sgd_solver.cpp:112] Iteration 20500, lr = 0.1
I0524 01:54:55.156421 11835 solver.cpp:239] Iteration 20510 (1.36523 iter/s, 7.32476s/10 iters), loss = 6.73542
I0524 01:54:55.156461 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73542 (* 1 = 6.73542 loss)
I0524 01:54:55.156473 11835 sgd_solver.cpp:112] Iteration 20510, lr = 0.1
I0524 01:55:01.067534 11835 solver.cpp:239] Iteration 20520 (1.69186 iter/s, 5.91065s/10 iters), loss = 6.55351
I0524 01:55:01.067584 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55351 (* 1 = 6.55351 loss)
I0524 01:55:01.067826 11835 sgd_solver.cpp:112] Iteration 20520, lr = 0.1
I0524 01:55:06.747398 11835 solver.cpp:239] Iteration 20530 (1.76069 iter/s, 5.6796s/10 iters), loss = 6.98727
I0524 01:55:06.747436 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98727 (* 1 = 6.98727 loss)
I0524 01:55:06.747447 11835 sgd_solver.cpp:112] Iteration 20530, lr = 0.1
I0524 01:55:12.847638 11835 solver.cpp:239] Iteration 20540 (1.63996 iter/s, 6.09771s/10 iters), loss = 7.45822
I0524 01:55:12.847940 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.45822 (* 1 = 7.45822 loss)
I0524 01:55:12.847995 11835 sgd_solver.cpp:112] Iteration 20540, lr = 0.1
I0524 01:55:19.726454 11835 solver.cpp:239] Iteration 20550 (1.4539 iter/s, 6.87804s/10 iters), loss = 6.3372
I0524 01:55:19.726501 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3372 (* 1 = 6.3372 loss)
I0524 01:55:19.726734 11835 sgd_solver.cpp:112] Iteration 20550, lr = 0.1
I0524 01:55:25.679091 11835 solver.cpp:239] Iteration 20560 (1.68001 iter/s, 5.95236s/10 iters), loss = 5.85575
I0524 01:55:25.679138 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85575 (* 1 = 5.85575 loss)
I0524 01:55:26.502498 11835 sgd_solver.cpp:112] Iteration 20560, lr = 0.1
I0524 01:55:33.822890 11835 solver.cpp:239] Iteration 20570 (1.22798 iter/s, 8.14344s/10 iters), loss = 7.00458
I0524 01:55:33.822942 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00458 (* 1 = 7.00458 loss)
I0524 01:55:34.369607 11835 sgd_solver.cpp:112] Iteration 20570, lr = 0.1
I0524 01:55:40.357635 11835 solver.cpp:239] Iteration 20580 (1.53035 iter/s, 6.53445s/10 iters), loss = 5.95573
I0524 01:55:40.357681 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95573 (* 1 = 5.95573 loss)
I0524 01:55:40.359509 11835 sgd_solver.cpp:112] Iteration 20580, lr = 0.1
I0524 01:55:47.842077 11835 solver.cpp:239] Iteration 20590 (1.33617 iter/s, 7.4841s/10 iters), loss = 6.45283
I0524 01:55:47.842293 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45283 (* 1 = 6.45283 loss)
I0524 01:55:47.842341 11835 sgd_solver.cpp:112] Iteration 20590, lr = 0.1
I0524 01:55:55.200554 11835 solver.cpp:239] Iteration 20600 (1.35906 iter/s, 7.35801s/10 iters), loss = 8.16383
I0524 01:55:55.200603 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.16383 (* 1 = 8.16383 loss)
I0524 01:55:55.217983 11835 sgd_solver.cpp:112] Iteration 20600, lr = 0.1
I0524 01:56:01.199699 11835 solver.cpp:239] Iteration 20610 (1.66698 iter/s, 5.99887s/10 iters), loss = 6.44394
I0524 01:56:01.199745 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44394 (* 1 = 6.44394 loss)
I0524 01:56:01.199980 11835 sgd_solver.cpp:112] Iteration 20610, lr = 0.1
I0524 01:56:07.970413 11835 solver.cpp:239] Iteration 20620 (1.47702 iter/s, 6.7704s/10 iters), loss = 6.50185
I0524 01:56:07.970463 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50185 (* 1 = 6.50185 loss)
I0524 01:56:07.970496 11835 sgd_solver.cpp:112] Iteration 20620, lr = 0.1
I0524 01:56:13.740093 11835 solver.cpp:239] Iteration 20630 (1.73328 iter/s, 5.76941s/10 iters), loss = 7.91529
I0524 01:56:13.740160 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.91529 (* 1 = 7.91529 loss)
I0524 01:56:13.740386 11835 sgd_solver.cpp:112] Iteration 20630, lr = 0.1
I0524 01:56:19.615732 11835 solver.cpp:239] Iteration 20640 (1.70202 iter/s, 5.87537s/10 iters), loss = 6.81368
I0524 01:56:19.615912 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81368 (* 1 = 6.81368 loss)
I0524 01:56:20.147192 11835 sgd_solver.cpp:112] Iteration 20640, lr = 0.1
I0524 01:56:26.423354 11835 solver.cpp:239] Iteration 20650 (1.46903 iter/s, 6.80721s/10 iters), loss = 7.478
I0524 01:56:26.423410 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.478 (* 1 = 7.478 loss)
I0524 01:56:27.453544 11835 sgd_solver.cpp:112] Iteration 20650, lr = 0.1
I0524 01:56:34.790539 11835 solver.cpp:239] Iteration 20660 (1.1952 iter/s, 8.36681s/10 iters), loss = 7.66104
I0524 01:56:34.790608 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.66104 (* 1 = 7.66104 loss)
I0524 01:56:34.979648 11835 sgd_solver.cpp:112] Iteration 20660, lr = 0.1
I0524 01:56:41.933003 11835 solver.cpp:239] Iteration 20670 (1.40014 iter/s, 7.14214s/10 iters), loss = 6.74123
I0524 01:56:41.933063 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74123 (* 1 = 6.74123 loss)
I0524 01:56:42.210445 11835 sgd_solver.cpp:112] Iteration 20670, lr = 0.1
I0524 01:56:48.148850 11835 solver.cpp:239] Iteration 20680 (1.60887 iter/s, 6.21556s/10 iters), loss = 6.41864
I0524 01:56:48.148890 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41864 (* 1 = 6.41864 loss)
I0524 01:56:48.148931 11835 sgd_solver.cpp:112] Iteration 20680, lr = 0.1
I0524 01:56:55.413736 11835 solver.cpp:239] Iteration 20690 (1.37655 iter/s, 7.26456s/10 iters), loss = 6.40599
I0524 01:56:55.413934 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40599 (* 1 = 6.40599 loss)
I0524 01:56:56.233326 11835 sgd_solver.cpp:112] Iteration 20690, lr = 0.1
I0524 01:57:03.307693 11835 solver.cpp:239] Iteration 20700 (1.26687 iter/s, 7.89346s/10 iters), loss = 6.55582
I0524 01:57:03.307745 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55582 (* 1 = 6.55582 loss)
I0524 01:57:03.307761 11835 sgd_solver.cpp:112] Iteration 20700, lr = 0.1
I0524 01:57:09.699555 11835 solver.cpp:239] Iteration 20710 (1.56465 iter/s, 6.3912s/10 iters), loss = 7.21939
I0524 01:57:09.699599 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.21939 (* 1 = 7.21939 loss)
I0524 01:57:09.699724 11835 sgd_solver.cpp:112] Iteration 20710, lr = 0.1
I0524 01:57:15.788609 11835 solver.cpp:239] Iteration 20720 (1.64237 iter/s, 6.08876s/10 iters), loss = 7.6061
I0524 01:57:15.788673 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.6061 (* 1 = 7.6061 loss)
I0524 01:57:15.788895 11835 sgd_solver.cpp:112] Iteration 20720, lr = 0.1
I0524 01:57:22.557139 11835 solver.cpp:239] Iteration 20730 (1.4775 iter/s, 6.7682s/10 iters), loss = 7.30127
I0524 01:57:22.557201 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.30127 (* 1 = 7.30127 loss)
I0524 01:57:22.557282 11835 sgd_solver.cpp:112] Iteration 20730, lr = 0.1
I0524 01:57:29.783493 11835 solver.cpp:239] Iteration 20740 (1.38389 iter/s, 7.22601s/10 iters), loss = 7.01679
I0524 01:57:29.783757 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01679 (* 1 = 7.01679 loss)
I0524 01:57:29.785498 11835 sgd_solver.cpp:112] Iteration 20740, lr = 0.1
I0524 01:57:35.771452 11835 solver.cpp:239] Iteration 20750 (1.67014 iter/s, 5.98751s/10 iters), loss = 6.58561
I0524 01:57:35.771500 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58561 (* 1 = 6.58561 loss)
I0524 01:57:35.771515 11835 sgd_solver.cpp:112] Iteration 20750, lr = 0.1
I0524 01:57:41.499608 11835 solver.cpp:239] Iteration 20760 (1.74618 iter/s, 5.72679s/10 iters), loss = 6.39589
I0524 01:57:41.499652 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39589 (* 1 = 6.39589 loss)
I0524 01:57:42.002827 11835 sgd_solver.cpp:112] Iteration 20760, lr = 0.1
I0524 01:57:48.055450 11835 solver.cpp:239] Iteration 20770 (1.52543 iter/s, 6.55554s/10 iters), loss = 8.091
I0524 01:57:48.055500 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.091 (* 1 = 8.091 loss)
I0524 01:57:48.055517 11835 sgd_solver.cpp:112] Iteration 20770, lr = 0.1
I0524 01:57:54.032135 11835 solver.cpp:239] Iteration 20780 (1.67329 iter/s, 5.97623s/10 iters), loss = 6.35319
I0524 01:57:54.032176 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35319 (* 1 = 6.35319 loss)
I0524 01:57:54.129482 11835 sgd_solver.cpp:112] Iteration 20780, lr = 0.1
I0524 01:58:00.754518 11835 solver.cpp:239] Iteration 20790 (1.48764 iter/s, 6.72206s/10 iters), loss = 6.51597
I0524 01:58:00.754863 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51597 (* 1 = 6.51597 loss)
I0524 01:58:00.754894 11835 sgd_solver.cpp:112] Iteration 20790, lr = 0.1
I0524 01:58:09.005415 11835 solver.cpp:239] Iteration 20800 (1.2124 iter/s, 8.24813s/10 iters), loss = 7.34142
I0524 01:58:09.005466 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.34142 (* 1 = 7.34142 loss)
I0524 01:58:09.005481 11835 sgd_solver.cpp:112] Iteration 20800, lr = 0.1
I0524 01:58:15.657627 11835 solver.cpp:239] Iteration 20810 (1.50337 iter/s, 6.65174s/10 iters), loss = 7.53973
I0524 01:58:15.657665 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.53973 (* 1 = 7.53973 loss)
I0524 01:58:16.095568 11835 sgd_solver.cpp:112] Iteration 20810, lr = 0.1
I0524 01:58:22.023979 11835 solver.cpp:239] Iteration 20820 (1.57083 iter/s, 6.36605s/10 iters), loss = 6.23445
I0524 01:58:22.024035 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23445 (* 1 = 6.23445 loss)
I0524 01:58:22.025259 11835 sgd_solver.cpp:112] Iteration 20820, lr = 0.1
I0524 01:58:29.483109 11835 solver.cpp:239] Iteration 20830 (1.3407 iter/s, 7.4588s/10 iters), loss = 7.09989
I0524 01:58:29.483153 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09989 (* 1 = 7.09989 loss)
I0524 01:58:29.483166 11835 sgd_solver.cpp:112] Iteration 20830, lr = 0.1
I0524 01:58:35.445713 11835 solver.cpp:239] Iteration 20840 (1.67751 iter/s, 5.96122s/10 iters), loss = 7.32615
I0524 01:58:35.446003 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.32615 (* 1 = 7.32615 loss)
I0524 01:58:35.446056 11835 sgd_solver.cpp:112] Iteration 20840, lr = 0.1
I0524 01:58:41.591406 11835 solver.cpp:239] Iteration 20850 (1.62752 iter/s, 6.1443s/10 iters), loss = 6.14621
I0524 01:58:41.591452 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14621 (* 1 = 6.14621 loss)
I0524 01:58:41.591658 11835 sgd_solver.cpp:112] Iteration 20850, lr = 0.1
I0524 01:58:48.376685 11835 solver.cpp:239] Iteration 20860 (1.47384 iter/s, 6.78498s/10 iters), loss = 7.31603
I0524 01:58:48.376726 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.31603 (* 1 = 7.31603 loss)
I0524 01:58:48.498042 11835 sgd_solver.cpp:112] Iteration 20860, lr = 0.1
I0524 01:58:54.816606 11835 solver.cpp:239] Iteration 20870 (1.55289 iter/s, 6.43962s/10 iters), loss = 6.77822
I0524 01:58:54.816663 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.77822 (* 1 = 6.77822 loss)
I0524 01:58:54.816678 11835 sgd_solver.cpp:112] Iteration 20870, lr = 0.1
I0524 01:59:03.580979 11835 solver.cpp:239] Iteration 20880 (1.14108 iter/s, 8.76361s/10 iters), loss = 7.05238
I0524 01:59:03.581080 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.05238 (* 1 = 7.05238 loss)
I0524 01:59:03.581120 11835 sgd_solver.cpp:112] Iteration 20880, lr = 0.1
I0524 01:59:09.400404 11835 solver.cpp:239] Iteration 20890 (1.71847 iter/s, 5.81912s/10 iters), loss = 6.86488
I0524 01:59:09.400494 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86488 (* 1 = 6.86488 loss)
I0524 01:59:09.400734 11835 sgd_solver.cpp:112] Iteration 20890, lr = 0.1
I0524 01:59:15.759271 11835 solver.cpp:239] Iteration 20900 (1.57269 iter/s, 6.35853s/10 iters), loss = 6.80061
I0524 01:59:15.759320 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80061 (* 1 = 6.80061 loss)
I0524 01:59:15.759552 11835 sgd_solver.cpp:112] Iteration 20900, lr = 0.1
I0524 01:59:21.914296 11835 solver.cpp:239] Iteration 20910 (1.62476 iter/s, 6.15474s/10 iters), loss = 7.04823
I0524 01:59:21.914341 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04823 (* 1 = 7.04823 loss)
I0524 01:59:22.144011 11835 sgd_solver.cpp:112] Iteration 20910, lr = 0.1
I0524 01:59:28.289548 11835 solver.cpp:239] Iteration 20920 (1.56864 iter/s, 6.37495s/10 iters), loss = 6.42868
I0524 01:59:28.289607 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42868 (* 1 = 6.42868 loss)
I0524 01:59:28.289856 11835 sgd_solver.cpp:112] Iteration 20920, lr = 0.1
I0524 01:59:34.643407 11835 solver.cpp:239] Iteration 20930 (1.57392 iter/s, 6.35355s/10 iters), loss = 7.31005
I0524 01:59:34.643467 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.31005 (* 1 = 7.31005 loss)
I0524 01:59:34.643707 11835 sgd_solver.cpp:112] Iteration 20930, lr = 0.1
I0524 01:59:40.560737 11835 solver.cpp:239] Iteration 20940 (1.69003 iter/s, 5.91705s/10 iters), loss = 7.62219
I0524 01:59:40.560993 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.62219 (* 1 = 7.62219 loss)
I0524 01:59:40.561216 11835 sgd_solver.cpp:112] Iteration 20940, lr = 0.1
I0524 01:59:48.920290 11835 solver.cpp:239] Iteration 20950 (1.19631 iter/s, 8.35902s/10 iters), loss = 6.91382
I0524 01:59:48.920341 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.91382 (* 1 = 6.91382 loss)
I0524 01:59:48.920563 11835 sgd_solver.cpp:112] Iteration 20950, lr = 0.1
I0524 01:59:56.663537 11835 solver.cpp:239] Iteration 20960 (1.2915 iter/s, 7.74291s/10 iters), loss = 6.14214
I0524 01:59:56.663576 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14214 (* 1 = 6.14214 loss)
I0524 01:59:56.663700 11835 sgd_solver.cpp:112] Iteration 20960, lr = 0.1
I0524 02:00:02.586050 11835 solver.cpp:239] Iteration 20970 (1.68855 iter/s, 5.92223s/10 iters), loss = 6.57642
I0524 02:00:02.586102 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57642 (* 1 = 6.57642 loss)
I0524 02:00:02.586123 11835 sgd_solver.cpp:112] Iteration 20970, lr = 0.1
I0524 02:00:11.651219 11835 solver.cpp:239] Iteration 20980 (1.10317 iter/s, 9.06478s/10 iters), loss = 6.45186
I0524 02:00:11.651329 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45186 (* 1 = 6.45186 loss)
I0524 02:00:11.985193 11835 sgd_solver.cpp:112] Iteration 20980, lr = 0.1
I0524 02:00:18.689079 11835 solver.cpp:239] Iteration 20990 (1.42096 iter/s, 7.03748s/10 iters), loss = 6.53501
I0524 02:00:18.689134 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53501 (* 1 = 6.53501 loss)
I0524 02:00:18.699746 11835 sgd_solver.cpp:112] Iteration 20990, lr = 0.1
I0524 02:00:25.957023 11835 solver.cpp:239] Iteration 21000 (1.37597 iter/s, 7.26761s/10 iters), loss = 7.3875
I0524 02:00:25.957084 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.3875 (* 1 = 7.3875 loss)
I0524 02:00:25.957330 11835 sgd_solver.cpp:112] Iteration 21000, lr = 0.1
I0524 02:00:34.930330 11835 solver.cpp:239] Iteration 21010 (1.11447 iter/s, 8.97288s/10 iters), loss = 7.2569
I0524 02:00:34.930419 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2569 (* 1 = 7.2569 loss)
I0524 02:00:34.930723 11835 sgd_solver.cpp:112] Iteration 21010, lr = 0.1
I0524 02:00:42.370338 11835 solver.cpp:239] Iteration 21020 (1.34415 iter/s, 7.43965s/10 iters), loss = 6.93887
I0524 02:00:42.370582 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93887 (* 1 = 6.93887 loss)
I0524 02:00:42.475477 11835 sgd_solver.cpp:112] Iteration 21020, lr = 0.1
I0524 02:00:48.859791 11835 solver.cpp:239] Iteration 21030 (1.54107 iter/s, 6.48899s/10 iters), loss = 6.61856
I0524 02:00:48.859850 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61856 (* 1 = 6.61856 loss)
I0524 02:00:48.859958 11835 sgd_solver.cpp:112] Iteration 21030, lr = 0.1
I0524 02:00:54.714217 11835 solver.cpp:239] Iteration 21040 (1.70819 iter/s, 5.85415s/10 iters), loss = 6.8136
I0524 02:00:54.714258 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8136 (* 1 = 6.8136 loss)
I0524 02:00:54.714399 11835 sgd_solver.cpp:112] Iteration 21040, lr = 0.1
I0524 02:01:00.699496 11835 solver.cpp:239] Iteration 21050 (1.67084 iter/s, 5.985s/10 iters), loss = 6.17803
I0524 02:01:00.699554 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17803 (* 1 = 6.17803 loss)
I0524 02:01:00.699611 11835 sgd_solver.cpp:112] Iteration 21050, lr = 0.1
I0524 02:01:06.865175 11835 solver.cpp:239] Iteration 21060 (1.62196 iter/s, 6.16538s/10 iters), loss = 8.15401
I0524 02:01:06.865233 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.15401 (* 1 = 8.15401 loss)
I0524 02:01:06.865455 11835 sgd_solver.cpp:112] Iteration 21060, lr = 0.1
I0524 02:01:13.427485 11835 solver.cpp:239] Iteration 21070 (1.52393 iter/s, 6.562s/10 iters), loss = 7.21305
I0524 02:01:13.427767 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.21305 (* 1 = 7.21305 loss)
I0524 02:01:13.427820 11835 sgd_solver.cpp:112] Iteration 21070, lr = 0.1
I0524 02:01:19.329910 11835 solver.cpp:239] Iteration 21080 (1.69435 iter/s, 5.90197s/10 iters), loss = 7.7413
I0524 02:01:19.329957 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.7413 (* 1 = 7.7413 loss)
I0524 02:01:19.330039 11835 sgd_solver.cpp:112] Iteration 21080, lr = 0.1
I0524 02:01:25.624394 11835 solver.cpp:239] Iteration 21090 (1.58877 iter/s, 6.29418s/10 iters), loss = 6.43398
I0524 02:01:25.624449 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43398 (* 1 = 6.43398 loss)
I0524 02:01:25.624814 11835 sgd_solver.cpp:112] Iteration 21090, lr = 0.1
I0524 02:01:33.195293 11835 solver.cpp:239] Iteration 21100 (1.32091 iter/s, 7.57055s/10 iters), loss = 6.69746
I0524 02:01:33.195348 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69746 (* 1 = 6.69746 loss)
I0524 02:01:34.038799 11835 sgd_solver.cpp:112] Iteration 21100, lr = 0.1
I0524 02:01:39.849794 11835 solver.cpp:239] Iteration 21110 (1.50281 iter/s, 6.65419s/10 iters), loss = 6.89826
I0524 02:01:39.849840 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89826 (* 1 = 6.89826 loss)
I0524 02:01:39.849851 11835 sgd_solver.cpp:112] Iteration 21110, lr = 0.1
I0524 02:01:45.847044 11835 solver.cpp:239] Iteration 21120 (1.66813 iter/s, 5.99474s/10 iters), loss = 7.82346
I0524 02:01:45.847301 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.82346 (* 1 = 7.82346 loss)
I0524 02:01:45.847342 11835 sgd_solver.cpp:112] Iteration 21120, lr = 0.1
I0524 02:01:53.324750 11835 solver.cpp:239] Iteration 21130 (1.3374 iter/s, 7.47719s/10 iters), loss = 7.33428
I0524 02:01:53.324810 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.33428 (* 1 = 7.33428 loss)
I0524 02:01:53.325076 11835 sgd_solver.cpp:112] Iteration 21130, lr = 0.1
I0524 02:02:00.256989 11835 solver.cpp:239] Iteration 21140 (1.4426 iter/s, 6.93191s/10 iters), loss = 6.78774
I0524 02:02:00.257046 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78774 (* 1 = 6.78774 loss)
I0524 02:02:00.257290 11835 sgd_solver.cpp:112] Iteration 21140, lr = 0.1
I0524 02:02:07.114467 11835 solver.cpp:239] Iteration 21150 (1.45833 iter/s, 6.85716s/10 iters), loss = 6.35962
I0524 02:02:07.114518 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35962 (* 1 = 6.35962 loss)
I0524 02:02:07.511171 11835 sgd_solver.cpp:112] Iteration 21150, lr = 0.1
I0524 02:02:14.664940 11835 solver.cpp:239] Iteration 21160 (1.32448 iter/s, 7.55012s/10 iters), loss = 6.05547
I0524 02:02:14.665004 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05547 (* 1 = 6.05547 loss)
I0524 02:02:14.665225 11835 sgd_solver.cpp:112] Iteration 21160, lr = 0.1
I0524 02:02:22.294168 11835 solver.cpp:239] Iteration 21170 (1.31081 iter/s, 7.62888s/10 iters), loss = 5.67242
I0524 02:02:22.294273 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67242 (* 1 = 5.67242 loss)
I0524 02:02:22.294292 11835 sgd_solver.cpp:112] Iteration 21170, lr = 0.1
I0524 02:02:28.826012 11835 solver.cpp:239] Iteration 21180 (1.53104 iter/s, 6.5315s/10 iters), loss = 6.80323
I0524 02:02:28.826059 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80323 (* 1 = 6.80323 loss)
I0524 02:02:28.826164 11835 sgd_solver.cpp:112] Iteration 21180, lr = 0.1
I0524 02:02:35.499783 11835 solver.cpp:239] Iteration 21190 (1.49847 iter/s, 6.67347s/10 iters), loss = 7.32957
I0524 02:02:35.499830 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.32957 (* 1 = 7.32957 loss)
I0524 02:02:35.500103 11835 sgd_solver.cpp:112] Iteration 21190, lr = 0.1
I0524 02:02:42.099964 11835 solver.cpp:239] Iteration 21200 (1.51518 iter/s, 6.59988s/10 iters), loss = 6.37715
I0524 02:02:42.100018 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37715 (* 1 = 6.37715 loss)
I0524 02:02:42.100044 11835 sgd_solver.cpp:112] Iteration 21200, lr = 0.1
I0524 02:02:48.414070 11835 solver.cpp:239] Iteration 21210 (1.58383 iter/s, 6.31382s/10 iters), loss = 6.10084
I0524 02:02:48.414113 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10084 (* 1 = 6.10084 loss)
I0524 02:02:48.414125 11835 sgd_solver.cpp:112] Iteration 21210, lr = 0.1
I0524 02:02:54.309635 11835 solver.cpp:239] Iteration 21220 (1.69627 iter/s, 5.89528s/10 iters), loss = 5.70066
I0524 02:02:54.309788 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.70066 (* 1 = 5.70066 loss)
I0524 02:02:54.309968 11835 sgd_solver.cpp:112] Iteration 21220, lr = 0.1
I0524 02:02:59.888478 11835 solver.cpp:239] Iteration 21230 (1.7926 iter/s, 5.5785s/10 iters), loss = 6.989
I0524 02:02:59.888515 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.989 (* 1 = 6.989 loss)
I0524 02:02:59.888527 11835 sgd_solver.cpp:112] Iteration 21230, lr = 0.1
I0524 02:03:05.789505 11835 solver.cpp:239] Iteration 21240 (1.6947 iter/s, 5.90075s/10 iters), loss = 6.70843
I0524 02:03:05.789552 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70843 (* 1 = 6.70843 loss)
I0524 02:03:05.794819 11835 sgd_solver.cpp:112] Iteration 21240, lr = 0.1
I0524 02:03:13.050392 11835 solver.cpp:239] Iteration 21250 (1.3773 iter/s, 7.26057s/10 iters), loss = 6.88843
I0524 02:03:13.050436 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88843 (* 1 = 6.88843 loss)
I0524 02:03:13.241149 11835 sgd_solver.cpp:112] Iteration 21250, lr = 0.1
I0524 02:03:19.285835 11835 solver.cpp:239] Iteration 21260 (1.60381 iter/s, 6.23515s/10 iters), loss = 6.59421
I0524 02:03:19.285892 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59421 (* 1 = 6.59421 loss)
I0524 02:03:19.285909 11835 sgd_solver.cpp:112] Iteration 21260, lr = 0.1
I0524 02:03:25.832162 11835 solver.cpp:239] Iteration 21270 (1.52768 iter/s, 6.54589s/10 iters), loss = 6.92251
I0524 02:03:25.832288 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92251 (* 1 = 6.92251 loss)
I0524 02:03:25.832304 11835 sgd_solver.cpp:112] Iteration 21270, lr = 0.1
I0524 02:03:32.919790 11835 solver.cpp:239] Iteration 21280 (1.41099 iter/s, 7.08721s/10 iters), loss = 6.44129
I0524 02:03:32.919831 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44129 (* 1 = 6.44129 loss)
I0524 02:03:33.358060 11835 sgd_solver.cpp:112] Iteration 21280, lr = 0.1
I0524 02:03:40.169106 11835 solver.cpp:239] Iteration 21290 (1.3795 iter/s, 7.24899s/10 iters), loss = 6.32201
I0524 02:03:40.169147 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32201 (* 1 = 6.32201 loss)
I0524 02:03:40.169162 11835 sgd_solver.cpp:112] Iteration 21290, lr = 0.1
I0524 02:03:48.185681 11835 solver.cpp:239] Iteration 21300 (1.24748 iter/s, 8.01614s/10 iters), loss = 6.43007
I0524 02:03:48.185735 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43007 (* 1 = 6.43007 loss)
I0524 02:03:49.465209 11835 sgd_solver.cpp:112] Iteration 21300, lr = 0.1
I0524 02:03:57.614264 11835 solver.cpp:239] Iteration 21310 (1.06065 iter/s, 9.42817s/10 iters), loss = 7.10315
I0524 02:03:57.614486 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10315 (* 1 = 7.10315 loss)
I0524 02:03:57.614570 11835 sgd_solver.cpp:112] Iteration 21310, lr = 0.1
I0524 02:04:06.314780 11835 solver.cpp:239] Iteration 21320 (1.14943 iter/s, 8.7s/10 iters), loss = 7.23993
I0524 02:04:06.314831 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.23993 (* 1 = 7.23993 loss)
I0524 02:04:07.329964 11835 sgd_solver.cpp:112] Iteration 21320, lr = 0.1
I0524 02:04:13.026890 11835 solver.cpp:239] Iteration 21330 (1.48991 iter/s, 6.7118s/10 iters), loss = 7.48623
I0524 02:04:13.026932 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.48623 (* 1 = 7.48623 loss)
I0524 02:04:13.027542 11835 sgd_solver.cpp:112] Iteration 21330, lr = 0.1
I0524 02:04:21.013083 11835 solver.cpp:239] Iteration 21340 (1.25222 iter/s, 7.98584s/10 iters), loss = 6.70882
I0524 02:04:21.013131 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70882 (* 1 = 6.70882 loss)
I0524 02:04:21.013273 11835 sgd_solver.cpp:112] Iteration 21340, lr = 0.1
I0524 02:04:27.994225 11835 solver.cpp:239] Iteration 21350 (1.43249 iter/s, 6.98083s/10 iters), loss = 7.42282
I0524 02:04:27.994402 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.42282 (* 1 = 7.42282 loss)
I0524 02:04:29.113082 11835 sgd_solver.cpp:112] Iteration 21350, lr = 0.1
I0524 02:04:35.266997 11835 solver.cpp:239] Iteration 21360 (1.37508 iter/s, 7.27231s/10 iters), loss = 7.35267
I0524 02:04:35.267047 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.35267 (* 1 = 7.35267 loss)
I0524 02:04:35.267069 11835 sgd_solver.cpp:112] Iteration 21360, lr = 0.1
I0524 02:04:43.187278 11835 solver.cpp:239] Iteration 21370 (1.26264 iter/s, 7.91992s/10 iters), loss = 7.13337
I0524 02:04:43.187333 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13337 (* 1 = 7.13337 loss)
I0524 02:04:43.187734 11835 sgd_solver.cpp:112] Iteration 21370, lr = 0.1
I0524 02:04:52.020208 11835 solver.cpp:239] Iteration 21380 (1.13218 iter/s, 8.83254s/10 iters), loss = 7.22724
I0524 02:04:52.020256 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.22724 (* 1 = 7.22724 loss)
I0524 02:04:52.020632 11835 sgd_solver.cpp:112] Iteration 21380, lr = 0.1
I0524 02:04:58.686668 11835 solver.cpp:239] Iteration 21390 (1.50012 iter/s, 6.66615s/10 iters), loss = 6.78956
I0524 02:04:58.686980 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78956 (* 1 = 6.78956 loss)
I0524 02:04:58.687047 11835 sgd_solver.cpp:112] Iteration 21390, lr = 0.1
I0524 02:05:05.241670 11835 solver.cpp:239] Iteration 21400 (1.52579 iter/s, 6.55399s/10 iters), loss = 7.29467
I0524 02:05:05.241719 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.29467 (* 1 = 7.29467 loss)
I0524 02:05:05.241840 11835 sgd_solver.cpp:112] Iteration 21400, lr = 0.1
I0524 02:05:12.894214 11835 solver.cpp:239] Iteration 21410 (1.30681 iter/s, 7.6522s/10 iters), loss = 7.72055
I0524 02:05:12.894261 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.72055 (* 1 = 7.72055 loss)
I0524 02:05:12.894498 11835 sgd_solver.cpp:112] Iteration 21410, lr = 0.1
I0524 02:05:20.278079 11835 solver.cpp:239] Iteration 21420 (1.35437 iter/s, 7.38352s/10 iters), loss = 7.35652
I0524 02:05:20.278133 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.35652 (* 1 = 7.35652 loss)
I0524 02:05:20.669106 11835 sgd_solver.cpp:112] Iteration 21420, lr = 0.1
I0524 02:05:26.941879 11835 solver.cpp:239] Iteration 21430 (1.50071 iter/s, 6.6635s/10 iters), loss = 7.25149
I0524 02:05:26.941916 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.25149 (* 1 = 7.25149 loss)
I0524 02:05:26.941927 11835 sgd_solver.cpp:112] Iteration 21430, lr = 0.1
I0524 02:05:32.653589 11835 solver.cpp:239] Iteration 21440 (1.75091 iter/s, 5.71132s/10 iters), loss = 7.10703
I0524 02:05:32.653842 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10703 (* 1 = 7.10703 loss)
I0524 02:05:32.689631 11835 sgd_solver.cpp:112] Iteration 21440, lr = 0.1
I0524 02:05:38.837975 11835 solver.cpp:239] Iteration 21450 (1.6171 iter/s, 6.18392s/10 iters), loss = 6.88233
I0524 02:05:38.838032 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88233 (* 1 = 6.88233 loss)
I0524 02:05:38.852160 11835 sgd_solver.cpp:112] Iteration 21450, lr = 0.1
I0524 02:05:46.935511 11835 solver.cpp:239] Iteration 21460 (1.235 iter/s, 8.09717s/10 iters), loss = 6.26517
I0524 02:05:46.935577 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26517 (* 1 = 6.26517 loss)
I0524 02:05:46.935657 11835 sgd_solver.cpp:112] Iteration 21460, lr = 0.1
I0524 02:05:55.809228 11835 solver.cpp:239] Iteration 21470 (1.12697 iter/s, 8.87333s/10 iters), loss = 6.39238
I0524 02:05:55.809280 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39238 (* 1 = 6.39238 loss)
I0524 02:05:55.809865 11835 sgd_solver.cpp:112] Iteration 21470, lr = 0.1
I0524 02:06:01.993410 11835 solver.cpp:239] Iteration 21480 (1.6171 iter/s, 6.1839s/10 iters), loss = 7.29854
I0524 02:06:01.993453 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.29854 (* 1 = 7.29854 loss)
I0524 02:06:01.995424 11835 sgd_solver.cpp:112] Iteration 21480, lr = 0.1
I0524 02:06:08.382966 11835 solver.cpp:239] Iteration 21490 (1.56513 iter/s, 6.38926s/10 iters), loss = 6.08906
I0524 02:06:08.383288 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08906 (* 1 = 6.08906 loss)
I0524 02:06:08.406755 11835 sgd_solver.cpp:112] Iteration 21490, lr = 0.1
I0524 02:06:16.157140 11835 solver.cpp:239] Iteration 21500 (1.28641 iter/s, 7.77358s/10 iters), loss = 8.00704
I0524 02:06:16.157202 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.00704 (* 1 = 8.00704 loss)
I0524 02:06:16.157384 11835 sgd_solver.cpp:112] Iteration 21500, lr = 0.1
I0524 02:06:21.955585 11835 solver.cpp:239] Iteration 21510 (1.72468 iter/s, 5.79817s/10 iters), loss = 7.06967
I0524 02:06:21.955622 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.06967 (* 1 = 7.06967 loss)
I0524 02:06:21.955634 11835 sgd_solver.cpp:112] Iteration 21510, lr = 0.1
I0524 02:06:28.978163 11835 solver.cpp:239] Iteration 21520 (1.42404 iter/s, 7.02225s/10 iters), loss = 6.86322
I0524 02:06:28.978219 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86322 (* 1 = 6.86322 loss)
I0524 02:06:28.978360 11835 sgd_solver.cpp:112] Iteration 21520, lr = 0.1
I0524 02:06:36.461141 11835 solver.cpp:239] Iteration 21530 (1.33643 iter/s, 7.48264s/10 iters), loss = 5.80595
I0524 02:06:36.461181 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80595 (* 1 = 5.80595 loss)
I0524 02:06:36.461724 11835 sgd_solver.cpp:112] Iteration 21530, lr = 0.1
I0524 02:06:44.042526 11835 solver.cpp:239] Iteration 21540 (1.31908 iter/s, 7.58104s/10 iters), loss = 7.15383
I0524 02:06:44.042646 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.15383 (* 1 = 7.15383 loss)
I0524 02:06:44.042719 11835 sgd_solver.cpp:112] Iteration 21540, lr = 0.1
I0524 02:06:50.677109 11835 solver.cpp:239] Iteration 21550 (1.50734 iter/s, 6.6342s/10 iters), loss = 6.6954
I0524 02:06:50.677166 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6954 (* 1 = 6.6954 loss)
I0524 02:06:50.677295 11835 sgd_solver.cpp:112] Iteration 21550, lr = 0.1
I0524 02:06:57.024329 11835 solver.cpp:239] Iteration 21560 (1.57557 iter/s, 6.34693s/10 iters), loss = 6.05044
I0524 02:06:57.024377 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05044 (* 1 = 6.05044 loss)
I0524 02:06:57.024585 11835 sgd_solver.cpp:112] Iteration 21560, lr = 0.1
I0524 02:07:05.150624 11835 solver.cpp:239] Iteration 21570 (1.23063 iter/s, 8.12594s/10 iters), loss = 7.47021
I0524 02:07:05.150673 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.47021 (* 1 = 7.47021 loss)
I0524 02:07:05.150895 11835 sgd_solver.cpp:112] Iteration 21570, lr = 0.1
I0524 02:07:12.916916 11835 solver.cpp:239] Iteration 21580 (1.28767 iter/s, 7.76595s/10 iters), loss = 6.85875
I0524 02:07:12.916970 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.85875 (* 1 = 6.85875 loss)
I0524 02:07:12.927346 11835 sgd_solver.cpp:112] Iteration 21580, lr = 0.1
I0524 02:07:20.700554 11835 solver.cpp:239] Iteration 21590 (1.2848 iter/s, 7.7833s/10 iters), loss = 7.11134
I0524 02:07:20.700687 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11134 (* 1 = 7.11134 loss)
I0524 02:07:20.700781 11835 sgd_solver.cpp:112] Iteration 21590, lr = 0.1
I0524 02:07:28.429653 11835 solver.cpp:239] Iteration 21600 (1.29388 iter/s, 7.72867s/10 iters), loss = 7.45622
I0524 02:07:28.429709 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.45622 (* 1 = 7.45622 loss)
I0524 02:07:28.587747 11835 sgd_solver.cpp:112] Iteration 21600, lr = 0.1
I0524 02:07:35.447106 11835 solver.cpp:239] Iteration 21610 (1.42508 iter/s, 7.01713s/10 iters), loss = 7.7167
I0524 02:07:35.447156 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.7167 (* 1 = 7.7167 loss)
I0524 02:07:35.447170 11835 sgd_solver.cpp:112] Iteration 21610, lr = 0.1
I0524 02:07:42.488065 11835 solver.cpp:239] Iteration 21620 (1.42033 iter/s, 7.04064s/10 iters), loss = 6.46747
I0524 02:07:42.488117 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46747 (* 1 = 6.46747 loss)
I0524 02:07:42.537520 11835 sgd_solver.cpp:112] Iteration 21620, lr = 0.1
I0524 02:07:49.280747 11835 solver.cpp:239] Iteration 21630 (1.47224 iter/s, 6.79236s/10 iters), loss = 6.02044
I0524 02:07:49.280802 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02044 (* 1 = 6.02044 loss)
I0524 02:07:49.281005 11835 sgd_solver.cpp:112] Iteration 21630, lr = 0.1
I0524 02:07:55.527354 11835 solver.cpp:239] Iteration 21640 (1.60094 iter/s, 6.24632s/10 iters), loss = 6.43609
I0524 02:07:55.527634 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43609 (* 1 = 6.43609 loss)
I0524 02:07:55.527681 11835 sgd_solver.cpp:112] Iteration 21640, lr = 0.1
I0524 02:08:03.133893 11835 solver.cpp:239] Iteration 21650 (1.31476 iter/s, 7.60596s/10 iters), loss = 6.26835
I0524 02:08:03.133944 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26835 (* 1 = 6.26835 loss)
I0524 02:08:03.829735 11835 sgd_solver.cpp:112] Iteration 21650, lr = 0.1
I0524 02:08:10.338624 11835 solver.cpp:239] Iteration 21660 (1.38804 iter/s, 7.2044s/10 iters), loss = 7.42634
I0524 02:08:10.338678 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.42634 (* 1 = 7.42634 loss)
I0524 02:08:10.338960 11835 sgd_solver.cpp:112] Iteration 21660, lr = 0.1
I0524 02:08:16.246949 11835 solver.cpp:239] Iteration 21670 (1.69261 iter/s, 5.90805s/10 iters), loss = 7.35453
I0524 02:08:16.246995 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.35453 (* 1 = 7.35453 loss)
I0524 02:08:16.247381 11835 sgd_solver.cpp:112] Iteration 21670, lr = 0.1
I0524 02:08:23.728598 11835 solver.cpp:239] Iteration 21680 (1.33667 iter/s, 7.48131s/10 iters), loss = 7.3005
I0524 02:08:23.728655 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.3005 (* 1 = 7.3005 loss)
I0524 02:08:23.892756 11835 sgd_solver.cpp:112] Iteration 21680, lr = 0.1
I0524 02:08:29.407582 11835 solver.cpp:239] Iteration 21690 (1.76096 iter/s, 5.67871s/10 iters), loss = 6.5061
I0524 02:08:29.407840 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5061 (* 1 = 6.5061 loss)
I0524 02:08:29.407893 11835 sgd_solver.cpp:112] Iteration 21690, lr = 0.1
I0524 02:08:35.810115 11835 solver.cpp:239] Iteration 21700 (1.56203 iter/s, 6.40193s/10 iters), loss = 5.68506
I0524 02:08:35.810156 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68506 (* 1 = 5.68506 loss)
I0524 02:08:35.810503 11835 sgd_solver.cpp:112] Iteration 21700, lr = 0.1
I0524 02:08:42.356940 11835 solver.cpp:239] Iteration 21710 (1.52753 iter/s, 6.5465s/10 iters), loss = 6.6251
I0524 02:08:42.356999 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6251 (* 1 = 6.6251 loss)
I0524 02:08:42.357029 11835 sgd_solver.cpp:112] Iteration 21710, lr = 0.1
I0524 02:08:49.194993 11835 solver.cpp:239] Iteration 21720 (1.46247 iter/s, 6.83774s/10 iters), loss = 7.13282
I0524 02:08:49.195030 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13282 (* 1 = 7.13282 loss)
I0524 02:08:49.235648 11835 sgd_solver.cpp:112] Iteration 21720, lr = 0.1
I0524 02:08:55.011063 11835 solver.cpp:239] Iteration 21730 (1.71945 iter/s, 5.81581s/10 iters), loss = 5.7474
I0524 02:08:55.011099 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7474 (* 1 = 5.7474 loss)
I0524 02:08:55.011111 11835 sgd_solver.cpp:112] Iteration 21730, lr = 0.1
I0524 02:09:00.934989 11835 solver.cpp:239] Iteration 21740 (1.68879 iter/s, 5.92141s/10 iters), loss = 6.75903
I0524 02:09:00.935262 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.75903 (* 1 = 6.75903 loss)
I0524 02:09:00.935304 11835 sgd_solver.cpp:112] Iteration 21740, lr = 0.1
I0524 02:09:07.280220 11835 solver.cpp:239] Iteration 21750 (1.57634 iter/s, 6.34382s/10 iters), loss = 7.41353
I0524 02:09:07.280268 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.41353 (* 1 = 7.41353 loss)
I0524 02:09:07.423276 11835 sgd_solver.cpp:112] Iteration 21750, lr = 0.1
I0524 02:09:14.003551 11835 solver.cpp:239] Iteration 21760 (1.48742 iter/s, 6.72303s/10 iters), loss = 7.24065
I0524 02:09:14.003595 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.24065 (* 1 = 7.24065 loss)
I0524 02:09:14.003610 11835 sgd_solver.cpp:112] Iteration 21760, lr = 0.1
I0524 02:09:20.585681 11835 solver.cpp:239] Iteration 21770 (1.51934 iter/s, 6.58183s/10 iters), loss = 7.11629
I0524 02:09:20.585737 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11629 (* 1 = 7.11629 loss)
I0524 02:09:20.585753 11835 sgd_solver.cpp:112] Iteration 21770, lr = 0.1
I0524 02:09:26.780270 11835 solver.cpp:239] Iteration 21780 (1.61441 iter/s, 6.19421s/10 iters), loss = 7.0578
I0524 02:09:26.780321 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.0578 (* 1 = 7.0578 loss)
I0524 02:09:26.780635 11835 sgd_solver.cpp:112] Iteration 21780, lr = 0.1
I0524 02:09:33.044518 11835 solver.cpp:239] Iteration 21790 (1.59644 iter/s, 6.26394s/10 iters), loss = 7.09534
I0524 02:09:33.044795 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09534 (* 1 = 7.09534 loss)
I0524 02:09:33.044852 11835 sgd_solver.cpp:112] Iteration 21790, lr = 0.1
I0524 02:09:39.785949 11835 solver.cpp:239] Iteration 21800 (1.48352 iter/s, 6.74073s/10 iters), loss = 6.50463
I0524 02:09:39.786002 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50463 (* 1 = 6.50463 loss)
I0524 02:09:39.786100 11835 sgd_solver.cpp:112] Iteration 21800, lr = 0.1
I0524 02:09:46.909118 11835 solver.cpp:239] Iteration 21810 (1.40393 iter/s, 7.12286s/10 iters), loss = 6.36463
I0524 02:09:46.909157 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36463 (* 1 = 6.36463 loss)
I0524 02:09:46.939460 11835 sgd_solver.cpp:112] Iteration 21810, lr = 0.1
I0524 02:09:52.754384 11835 solver.cpp:239] Iteration 21820 (1.71086 iter/s, 5.845s/10 iters), loss = 7.18019
I0524 02:09:52.754429 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.18019 (* 1 = 7.18019 loss)
I0524 02:09:52.754443 11835 sgd_solver.cpp:112] Iteration 21820, lr = 0.1
I0524 02:09:59.948560 11835 solver.cpp:239] Iteration 21830 (1.39007 iter/s, 7.19386s/10 iters), loss = 7.39954
I0524 02:09:59.948611 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.39954 (* 1 = 7.39954 loss)
I0524 02:09:59.948743 11835 sgd_solver.cpp:112] Iteration 21830, lr = 0.1
I0524 02:10:06.414650 11835 solver.cpp:239] Iteration 21840 (1.5466 iter/s, 6.46578s/10 iters), loss = 7.10158
I0524 02:10:06.414804 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10158 (* 1 = 7.10158 loss)
I0524 02:10:06.414944 11835 sgd_solver.cpp:112] Iteration 21840, lr = 0.1
I0524 02:10:13.059563 11835 solver.cpp:239] Iteration 21850 (1.505 iter/s, 6.6445s/10 iters), loss = 7.19748
I0524 02:10:13.059620 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.19748 (* 1 = 7.19748 loss)
I0524 02:10:13.862012 11835 sgd_solver.cpp:112] Iteration 21850, lr = 0.1
I0524 02:10:21.322212 11835 solver.cpp:239] Iteration 21860 (1.21032 iter/s, 8.26228s/10 iters), loss = 6.9029
I0524 02:10:21.322270 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9029 (* 1 = 6.9029 loss)
I0524 02:10:21.322379 11835 sgd_solver.cpp:112] Iteration 21860, lr = 0.1
I0524 02:10:29.511682 11835 solver.cpp:239] Iteration 21870 (1.22113 iter/s, 8.18911s/10 iters), loss = 6.97682
I0524 02:10:29.511732 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.97682 (* 1 = 6.97682 loss)
I0524 02:10:29.511746 11835 sgd_solver.cpp:112] Iteration 21870, lr = 0.1
I0524 02:10:35.753588 11835 solver.cpp:239] Iteration 21880 (1.60215 iter/s, 6.24161s/10 iters), loss = 7.00029
I0524 02:10:35.753640 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00029 (* 1 = 7.00029 loss)
I0524 02:10:35.753885 11835 sgd_solver.cpp:112] Iteration 21880, lr = 0.1
I0524 02:10:42.161371 11835 solver.cpp:239] Iteration 21890 (1.56067 iter/s, 6.40749s/10 iters), loss = 5.63138
I0524 02:10:42.161561 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63138 (* 1 = 5.63138 loss)
I0524 02:10:42.308780 11835 sgd_solver.cpp:112] Iteration 21890, lr = 0.1
I0524 02:10:49.099822 11835 solver.cpp:239] Iteration 21900 (1.44134 iter/s, 6.93799s/10 iters), loss = 7.65064
I0524 02:10:49.099870 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.65064 (* 1 = 7.65064 loss)
I0524 02:10:49.099886 11835 sgd_solver.cpp:112] Iteration 21900, lr = 0.1
I0524 02:10:55.853204 11835 solver.cpp:239] Iteration 21910 (1.48082 iter/s, 6.75301s/10 iters), loss = 6.92756
I0524 02:10:55.853250 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92756 (* 1 = 6.92756 loss)
I0524 02:10:55.853435 11835 sgd_solver.cpp:112] Iteration 21910, lr = 0.1
I0524 02:11:03.702903 11835 solver.cpp:239] Iteration 21920 (1.27399 iter/s, 7.84935s/10 iters), loss = 6.03422
I0524 02:11:03.702961 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03422 (* 1 = 6.03422 loss)
I0524 02:11:03.702976 11835 sgd_solver.cpp:112] Iteration 21920, lr = 0.1
I0524 02:11:10.927546 11835 solver.cpp:239] Iteration 21930 (1.38443 iter/s, 7.22319s/10 iters), loss = 6.67199
I0524 02:11:10.927600 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67199 (* 1 = 6.67199 loss)
I0524 02:11:10.927634 11835 sgd_solver.cpp:112] Iteration 21930, lr = 0.1
I0524 02:11:17.413676 11835 solver.cpp:239] Iteration 21940 (1.54182 iter/s, 6.48583s/10 iters), loss = 6.50332
I0524 02:11:17.413800 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50332 (* 1 = 6.50332 loss)
I0524 02:11:17.662904 11835 sgd_solver.cpp:112] Iteration 21940, lr = 0.1
I0524 02:11:25.034166 11835 solver.cpp:239] Iteration 21950 (1.31232 iter/s, 7.62007s/10 iters), loss = 7.22819
I0524 02:11:25.034220 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.22819 (* 1 = 7.22819 loss)
I0524 02:11:25.094161 11835 sgd_solver.cpp:112] Iteration 21950, lr = 0.1
I0524 02:11:33.037071 11835 solver.cpp:239] Iteration 21960 (1.2496 iter/s, 8.00255s/10 iters), loss = 7.28875
I0524 02:11:33.037122 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.28875 (* 1 = 7.28875 loss)
I0524 02:11:33.037519 11835 sgd_solver.cpp:112] Iteration 21960, lr = 0.1
I0524 02:11:40.187113 11835 solver.cpp:239] Iteration 21970 (1.39866 iter/s, 7.14972s/10 iters), loss = 6.47049
I0524 02:11:40.187152 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.47049 (* 1 = 6.47049 loss)
I0524 02:11:40.188328 11835 sgd_solver.cpp:112] Iteration 21970, lr = 0.1
I0524 02:11:47.430709 11835 solver.cpp:239] Iteration 21980 (1.38059 iter/s, 7.24327s/10 iters), loss = 6.82893
I0524 02:11:47.430944 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.82893 (* 1 = 6.82893 loss)
I0524 02:11:47.430989 11835 sgd_solver.cpp:112] Iteration 21980, lr = 0.1
I0524 02:11:53.984143 11835 solver.cpp:239] Iteration 21990 (1.52625 iter/s, 6.552s/10 iters), loss = 7.29475
I0524 02:11:53.984196 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.29475 (* 1 = 7.29475 loss)
I0524 02:11:53.984300 11835 sgd_solver.cpp:112] Iteration 21990, lr = 0.1
I0524 02:12:01.178635 11835 solver.cpp:239] Iteration 22000 (1.39002 iter/s, 7.19416s/10 iters), loss = 6.87031
I0524 02:12:01.178737 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87031 (* 1 = 6.87031 loss)
I0524 02:12:01.707154 11835 sgd_solver.cpp:112] Iteration 22000, lr = 0.1
I0524 02:12:07.823135 11835 solver.cpp:239] Iteration 22010 (1.50507 iter/s, 6.64419s/10 iters), loss = 6.87641
I0524 02:12:07.823175 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87641 (* 1 = 6.87641 loss)
I0524 02:12:07.943562 11835 sgd_solver.cpp:112] Iteration 22010, lr = 0.1
I0524 02:12:14.226042 11835 solver.cpp:239] Iteration 22020 (1.56186 iter/s, 6.40261s/10 iters), loss = 6.51399
I0524 02:12:14.226090 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51399 (* 1 = 6.51399 loss)
I0524 02:12:14.226105 11835 sgd_solver.cpp:112] Iteration 22020, lr = 0.1
I0524 02:12:21.593798 11835 solver.cpp:239] Iteration 22030 (1.35733 iter/s, 7.36741s/10 iters), loss = 7.62304
I0524 02:12:21.593984 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.62304 (* 1 = 7.62304 loss)
I0524 02:12:21.594043 11835 sgd_solver.cpp:112] Iteration 22030, lr = 0.1
I0524 02:12:29.781991 11835 solver.cpp:239] Iteration 22040 (1.22134 iter/s, 8.18771s/10 iters), loss = 7.23133
I0524 02:12:29.782040 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.23133 (* 1 = 7.23133 loss)
I0524 02:12:29.782052 11835 sgd_solver.cpp:112] Iteration 22040, lr = 0.1
I0524 02:12:38.287206 11835 solver.cpp:239] Iteration 22050 (1.17595 iter/s, 8.50373s/10 iters), loss = 7.17472
I0524 02:12:38.287257 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.17472 (* 1 = 7.17472 loss)
I0524 02:12:38.720957 11835 sgd_solver.cpp:112] Iteration 22050, lr = 0.1
I0524 02:12:46.906257 11835 solver.cpp:239] Iteration 22060 (1.16027 iter/s, 8.61867s/10 iters), loss = 6.37715
I0524 02:12:46.906308 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37715 (* 1 = 6.37715 loss)
I0524 02:12:46.906674 11835 sgd_solver.cpp:112] Iteration 22060, lr = 0.1
I0524 02:12:53.942045 11835 solver.cpp:239] Iteration 22070 (1.42137 iter/s, 7.03547s/10 iters), loss = 6.75249
I0524 02:12:53.942327 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.75249 (* 1 = 6.75249 loss)
I0524 02:12:53.942384 11835 sgd_solver.cpp:112] Iteration 22070, lr = 0.1
I0524 02:13:02.833555 11835 solver.cpp:239] Iteration 22080 (1.12474 iter/s, 8.89093s/10 iters), loss = 7.72301
I0524 02:13:02.833608 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.72301 (* 1 = 7.72301 loss)
I0524 02:13:03.364590 11835 sgd_solver.cpp:112] Iteration 22080, lr = 0.1
I0524 02:13:10.910387 11835 solver.cpp:239] Iteration 22090 (1.23816 iter/s, 8.07647s/10 iters), loss = 7.00701
I0524 02:13:10.910440 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00701 (* 1 = 7.00701 loss)
I0524 02:13:10.923404 11835 sgd_solver.cpp:112] Iteration 22090, lr = 0.1
I0524 02:13:16.490087 11835 solver.cpp:239] Iteration 22100 (1.7923 iter/s, 5.57944s/10 iters), loss = 6.60798
I0524 02:13:16.490133 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60798 (* 1 = 6.60798 loss)
I0524 02:13:16.490382 11835 sgd_solver.cpp:112] Iteration 22100, lr = 0.1
I0524 02:13:23.135824 11835 solver.cpp:239] Iteration 22110 (1.5048 iter/s, 6.64542s/10 iters), loss = 5.86171
I0524 02:13:23.135880 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86171 (* 1 = 5.86171 loss)
I0524 02:13:23.136117 11835 sgd_solver.cpp:112] Iteration 22110, lr = 0.1
I0524 02:13:31.500879 11835 solver.cpp:239] Iteration 22120 (1.1955 iter/s, 8.36468s/10 iters), loss = 6.41399
I0524 02:13:31.501020 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41399 (* 1 = 6.41399 loss)
I0524 02:13:31.501039 11835 sgd_solver.cpp:112] Iteration 22120, lr = 0.1
I0524 02:13:38.308959 11835 solver.cpp:239] Iteration 22130 (1.46939 iter/s, 6.80553s/10 iters), loss = 7.87267
I0524 02:13:38.309008 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.87267 (* 1 = 7.87267 loss)
I0524 02:13:38.309250 11835 sgd_solver.cpp:112] Iteration 22130, lr = 0.1
I0524 02:13:43.983371 11835 solver.cpp:239] Iteration 22140 (1.76238 iter/s, 5.67414s/10 iters), loss = 5.99068
I0524 02:13:43.983417 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99068 (* 1 = 5.99068 loss)
I0524 02:13:43.983431 11835 sgd_solver.cpp:112] Iteration 22140, lr = 0.1
I0524 02:13:49.786645 11835 solver.cpp:239] Iteration 22150 (1.72324 iter/s, 5.80301s/10 iters), loss = 6.93368
I0524 02:13:49.786705 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93368 (* 1 = 6.93368 loss)
I0524 02:13:49.786720 11835 sgd_solver.cpp:112] Iteration 22150, lr = 0.1
I0524 02:13:56.998198 11835 solver.cpp:239] Iteration 22160 (1.38672 iter/s, 7.21124s/10 iters), loss = 6.63912
I0524 02:13:56.998234 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63912 (* 1 = 6.63912 loss)
I0524 02:13:56.998353 11835 sgd_solver.cpp:112] Iteration 22160, lr = 0.1
I0524 02:14:03.636716 11835 solver.cpp:239] Iteration 22170 (1.50643 iter/s, 6.63822s/10 iters), loss = 7.21109
I0524 02:14:03.637051 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.21109 (* 1 = 7.21109 loss)
I0524 02:14:03.819736 11835 sgd_solver.cpp:112] Iteration 22170, lr = 0.1
I0524 02:14:12.077824 11835 solver.cpp:239] Iteration 22180 (1.18476 iter/s, 8.44051s/10 iters), loss = 6.86501
I0524 02:14:12.077864 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86501 (* 1 = 6.86501 loss)
I0524 02:14:12.650388 11835 sgd_solver.cpp:112] Iteration 22180, lr = 0.1
I0524 02:14:19.546433 11835 solver.cpp:239] Iteration 22190 (1.339 iter/s, 7.46828s/10 iters), loss = 7.61056
I0524 02:14:19.546478 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.61056 (* 1 = 7.61056 loss)
I0524 02:14:19.546604 11835 sgd_solver.cpp:112] Iteration 22190, lr = 0.1
I0524 02:14:25.145177 11835 solver.cpp:239] Iteration 22200 (1.7862 iter/s, 5.59849s/10 iters), loss = 6.16496
I0524 02:14:25.145216 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16496 (* 1 = 6.16496 loss)
I0524 02:14:25.145231 11835 sgd_solver.cpp:112] Iteration 22200, lr = 0.1
I0524 02:14:32.532155 11835 solver.cpp:239] Iteration 22210 (1.35381 iter/s, 7.38658s/10 iters), loss = 7.53252
I0524 02:14:32.532199 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.53252 (* 1 = 7.53252 loss)
I0524 02:14:32.532480 11835 sgd_solver.cpp:112] Iteration 22210, lr = 0.1
I0524 02:14:39.105104 11835 solver.cpp:239] Iteration 22220 (1.52146 iter/s, 6.57264s/10 iters), loss = 6.82595
I0524 02:14:39.105211 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.82595 (* 1 = 6.82595 loss)
I0524 02:14:39.105257 11835 sgd_solver.cpp:112] Iteration 22220, lr = 0.1
I0524 02:14:44.652454 11835 solver.cpp:239] Iteration 22230 (1.80276 iter/s, 5.54704s/10 iters), loss = 6.80669
I0524 02:14:44.652499 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80669 (* 1 = 6.80669 loss)
I0524 02:14:44.652712 11835 sgd_solver.cpp:112] Iteration 22230, lr = 0.1
I0524 02:14:51.986115 11835 solver.cpp:239] Iteration 22240 (1.36364 iter/s, 7.33333s/10 iters), loss = 7.47413
I0524 02:14:51.986176 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.47413 (* 1 = 7.47413 loss)
I0524 02:14:51.986253 11835 sgd_solver.cpp:112] Iteration 22240, lr = 0.1
I0524 02:15:00.429280 11835 solver.cpp:239] Iteration 22250 (1.18444 iter/s, 8.44278s/10 iters), loss = 6.43536
I0524 02:15:00.429334 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43536 (* 1 = 6.43536 loss)
I0524 02:15:00.429544 11835 sgd_solver.cpp:112] Iteration 22250, lr = 0.1
I0524 02:15:05.959211 11835 solver.cpp:239] Iteration 22260 (1.80842 iter/s, 5.52968s/10 iters), loss = 6.33609
I0524 02:15:05.959244 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33609 (* 1 = 6.33609 loss)
I0524 02:15:05.959264 11835 sgd_solver.cpp:112] Iteration 22260, lr = 0.1
I0524 02:15:12.528189 11835 solver.cpp:239] Iteration 22270 (1.52237 iter/s, 6.56869s/10 iters), loss = 7.65767
I0524 02:15:12.528295 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.65767 (* 1 = 7.65767 loss)
I0524 02:15:12.528313 11835 sgd_solver.cpp:112] Iteration 22270, lr = 0.1
I0524 02:15:18.307612 11835 solver.cpp:239] Iteration 22280 (1.73069 iter/s, 5.77805s/10 iters), loss = 7.91678
I0524 02:15:18.307651 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.91678 (* 1 = 7.91678 loss)
I0524 02:15:18.307663 11835 sgd_solver.cpp:112] Iteration 22280, lr = 0.1
I0524 02:15:26.387758 11835 solver.cpp:239] Iteration 22290 (1.23767 iter/s, 8.07969s/10 iters), loss = 7.86495
I0524 02:15:26.387820 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.86495 (* 1 = 7.86495 loss)
I0524 02:15:26.388001 11835 sgd_solver.cpp:112] Iteration 22290, lr = 0.1
I0524 02:15:34.133294 11835 solver.cpp:239] Iteration 22300 (1.29112 iter/s, 7.74519s/10 iters), loss = 6.24395
I0524 02:15:34.133342 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24395 (* 1 = 6.24395 loss)
I0524 02:15:34.287154 11835 sgd_solver.cpp:112] Iteration 22300, lr = 0.1
I0524 02:15:40.211094 11835 solver.cpp:239] Iteration 22310 (1.64541 iter/s, 6.07752s/10 iters), loss = 7.63148
I0524 02:15:40.211133 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.63148 (* 1 = 7.63148 loss)
I0524 02:15:40.211194 11835 sgd_solver.cpp:112] Iteration 22310, lr = 0.1
I0524 02:15:47.301551 11835 solver.cpp:239] Iteration 22320 (1.41041 iter/s, 7.09015s/10 iters), loss = 6.42303
I0524 02:15:47.301853 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42303 (* 1 = 6.42303 loss)
I0524 02:15:47.301908 11835 sgd_solver.cpp:112] Iteration 22320, lr = 0.1
I0524 02:15:52.946848 11835 solver.cpp:239] Iteration 22330 (1.77182 iter/s, 5.6439s/10 iters), loss = 6.45231
I0524 02:15:52.946895 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45231 (* 1 = 6.45231 loss)
I0524 02:15:52.946908 11835 sgd_solver.cpp:112] Iteration 22330, lr = 0.1
I0524 02:15:59.386510 11835 solver.cpp:239] Iteration 22340 (1.55296 iter/s, 6.4393s/10 iters), loss = 6.71122
I0524 02:15:59.386554 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71122 (* 1 = 6.71122 loss)
I0524 02:15:59.386826 11835 sgd_solver.cpp:112] Iteration 22340, lr = 0.1
I0524 02:16:06.031700 11835 solver.cpp:239] Iteration 22350 (1.50492 iter/s, 6.64488s/10 iters), loss = 6.95768
I0524 02:16:06.031755 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.95768 (* 1 = 6.95768 loss)
I0524 02:16:06.032004 11835 sgd_solver.cpp:112] Iteration 22350, lr = 0.1
I0524 02:16:13.345213 11835 solver.cpp:239] Iteration 22360 (1.36739 iter/s, 7.31318s/10 iters), loss = 7.46855
I0524 02:16:13.345266 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.46855 (* 1 = 7.46855 loss)
I0524 02:16:13.863180 11835 sgd_solver.cpp:112] Iteration 22360, lr = 0.1
I0524 02:16:19.490308 11835 solver.cpp:239] Iteration 22370 (1.62739 iter/s, 6.14481s/10 iters), loss = 7.17442
I0524 02:16:19.490566 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.17442 (* 1 = 7.17442 loss)
I0524 02:16:19.490654 11835 sgd_solver.cpp:112] Iteration 22370, lr = 0.1
I0524 02:16:25.063174 11835 solver.cpp:239] Iteration 22380 (1.79455 iter/s, 5.57243s/10 iters), loss = 6.42044
I0524 02:16:25.063220 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42044 (* 1 = 6.42044 loss)
I0524 02:16:25.063235 11835 sgd_solver.cpp:112] Iteration 22380, lr = 0.1
I0524 02:16:30.931022 11835 solver.cpp:239] Iteration 22390 (1.70428 iter/s, 5.86758s/10 iters), loss = 6.8895
I0524 02:16:30.931064 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8895 (* 1 = 6.8895 loss)
I0524 02:16:30.931078 11835 sgd_solver.cpp:112] Iteration 22390, lr = 0.1
I0524 02:16:37.020787 11835 solver.cpp:239] Iteration 22400 (1.6422 iter/s, 6.08939s/10 iters), loss = 7.13537
I0524 02:16:37.020828 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13537 (* 1 = 7.13537 loss)
I0524 02:16:37.020853 11835 sgd_solver.cpp:112] Iteration 22400, lr = 0.1
I0524 02:16:42.795074 11835 solver.cpp:239] Iteration 22410 (1.7319 iter/s, 5.77402s/10 iters), loss = 6.81191
I0524 02:16:42.795122 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81191 (* 1 = 6.81191 loss)
I0524 02:16:42.899199 11835 sgd_solver.cpp:112] Iteration 22410, lr = 0.1
I0524 02:16:49.683440 11835 solver.cpp:239] Iteration 22420 (1.45179 iter/s, 6.88806s/10 iters), loss = 6.15763
I0524 02:16:49.683532 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15763 (* 1 = 6.15763 loss)
I0524 02:16:49.683601 11835 sgd_solver.cpp:112] Iteration 22420, lr = 0.1
I0524 02:16:55.404922 11835 solver.cpp:239] Iteration 22430 (1.7479 iter/s, 5.72116s/10 iters), loss = 6.49127
I0524 02:16:55.404968 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49127 (* 1 = 6.49127 loss)
I0524 02:16:55.404984 11835 sgd_solver.cpp:112] Iteration 22430, lr = 0.1
I0524 02:17:01.377970 11835 solver.cpp:239] Iteration 22440 (1.67426 iter/s, 5.97278s/10 iters), loss = 6.61645
I0524 02:17:01.378011 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61645 (* 1 = 6.61645 loss)
I0524 02:17:01.972204 11835 sgd_solver.cpp:112] Iteration 22440, lr = 0.1
I0524 02:17:09.606441 11835 solver.cpp:239] Iteration 22450 (1.21535 iter/s, 8.22811s/10 iters), loss = 6.72953
I0524 02:17:09.606493 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72953 (* 1 = 6.72953 loss)
I0524 02:17:09.823192 11835 sgd_solver.cpp:112] Iteration 22450, lr = 0.1
I0524 02:17:18.226658 11835 solver.cpp:239] Iteration 22460 (1.16011 iter/s, 8.61984s/10 iters), loss = 6.27926
I0524 02:17:18.226732 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27926 (* 1 = 6.27926 loss)
I0524 02:17:18.231981 11835 sgd_solver.cpp:112] Iteration 22460, lr = 0.1
I0524 02:17:25.565842 11835 solver.cpp:239] Iteration 22470 (1.36261 iter/s, 7.33886s/10 iters), loss = 7.6849
I0524 02:17:25.566094 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.6849 (* 1 = 7.6849 loss)
I0524 02:17:25.566148 11835 sgd_solver.cpp:112] Iteration 22470, lr = 0.1
I0524 02:17:32.614023 11835 solver.cpp:239] Iteration 22480 (1.4189 iter/s, 7.0477s/10 iters), loss = 6.4436
I0524 02:17:32.614079 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4436 (* 1 = 6.4436 loss)
I0524 02:17:32.614320 11835 sgd_solver.cpp:112] Iteration 22480, lr = 0.1
I0524 02:17:38.396553 11835 solver.cpp:239] Iteration 22490 (1.72943 iter/s, 5.78226s/10 iters), loss = 6.74004
I0524 02:17:38.396602 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74004 (* 1 = 6.74004 loss)
I0524 02:17:38.396664 11835 sgd_solver.cpp:112] Iteration 22490, lr = 0.1
I0524 02:17:45.170159 11835 solver.cpp:239] Iteration 22500 (1.47639 iter/s, 6.77329s/10 iters), loss = 6.95951
I0524 02:17:45.170217 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.95951 (* 1 = 6.95951 loss)
I0524 02:17:45.170490 11835 sgd_solver.cpp:112] Iteration 22500, lr = 0.1
I0524 02:17:51.330574 11835 solver.cpp:239] Iteration 22510 (1.62334 iter/s, 6.16012s/10 iters), loss = 6.89167
I0524 02:17:51.330623 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89167 (* 1 = 6.89167 loss)
I0524 02:17:51.330834 11835 sgd_solver.cpp:112] Iteration 22510, lr = 0.1
I0524 02:17:57.225848 11835 solver.cpp:239] Iteration 22520 (1.69635 iter/s, 5.895s/10 iters), loss = 6.68769
I0524 02:17:57.226107 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.68769 (* 1 = 6.68769 loss)
I0524 02:17:57.226160 11835 sgd_solver.cpp:112] Iteration 22520, lr = 0.1
I0524 02:18:04.140521 11835 solver.cpp:239] Iteration 22530 (1.44634 iter/s, 6.91402s/10 iters), loss = 7.13603
I0524 02:18:04.140565 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13603 (* 1 = 7.13603 loss)
I0524 02:18:04.206272 11835 sgd_solver.cpp:112] Iteration 22530, lr = 0.1
I0524 02:18:10.694634 11835 solver.cpp:239] Iteration 22540 (1.52583 iter/s, 6.55382s/10 iters), loss = 6.66383
I0524 02:18:10.694681 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66383 (* 1 = 6.66383 loss)
I0524 02:18:11.160454 11835 sgd_solver.cpp:112] Iteration 22540, lr = 0.1
I0524 02:18:17.356998 11835 solver.cpp:239] Iteration 22550 (1.50104 iter/s, 6.66205s/10 iters), loss = 6.45733
I0524 02:18:17.357054 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45733 (* 1 = 6.45733 loss)
I0524 02:18:17.357173 11835 sgd_solver.cpp:112] Iteration 22550, lr = 0.1
I0524 02:18:24.474117 11835 solver.cpp:239] Iteration 22560 (1.40513 iter/s, 7.11679s/10 iters), loss = 6.79128
I0524 02:18:24.474169 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79128 (* 1 = 6.79128 loss)
I0524 02:18:24.474690 11835 sgd_solver.cpp:112] Iteration 22560, lr = 0.1
I0524 02:18:32.597862 11835 solver.cpp:239] Iteration 22570 (1.23101 iter/s, 8.12338s/10 iters), loss = 7.53587
I0524 02:18:32.598016 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.53587 (* 1 = 7.53587 loss)
I0524 02:18:32.608273 11835 sgd_solver.cpp:112] Iteration 22570, lr = 0.1
I0524 02:18:39.784868 11835 solver.cpp:239] Iteration 22580 (1.39148 iter/s, 7.1866s/10 iters), loss = 6.12475
I0524 02:18:39.784914 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12475 (* 1 = 6.12475 loss)
I0524 02:18:39.784998 11835 sgd_solver.cpp:112] Iteration 22580, lr = 0.1
I0524 02:18:46.202808 11835 solver.cpp:239] Iteration 22590 (1.5582 iter/s, 6.41765s/10 iters), loss = 6.26483
I0524 02:18:46.202859 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26483 (* 1 = 6.26483 loss)
I0524 02:18:46.203373 11835 sgd_solver.cpp:112] Iteration 22590, lr = 0.1
I0524 02:18:52.971580 11835 solver.cpp:239] Iteration 22600 (1.47744 iter/s, 6.76845s/10 iters), loss = 8.16177
I0524 02:18:52.971642 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.16177 (* 1 = 8.16177 loss)
I0524 02:18:52.971860 11835 sgd_solver.cpp:112] Iteration 22600, lr = 0.1
I0524 02:18:59.142891 11835 solver.cpp:239] Iteration 22610 (1.62048 iter/s, 6.17102s/10 iters), loss = 6.16274
I0524 02:18:59.142935 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16274 (* 1 = 6.16274 loss)
I0524 02:18:59.143189 11835 sgd_solver.cpp:112] Iteration 22610, lr = 0.1
I0524 02:19:06.815102 11835 solver.cpp:239] Iteration 22620 (1.30346 iter/s, 7.67187s/10 iters), loss = 6.09118
I0524 02:19:06.815368 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09118 (* 1 = 6.09118 loss)
I0524 02:19:06.815424 11835 sgd_solver.cpp:112] Iteration 22620, lr = 0.1
I0524 02:19:12.978932 11835 solver.cpp:239] Iteration 22630 (1.62277 iter/s, 6.1623s/10 iters), loss = 6.50696
I0524 02:19:12.978987 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50696 (* 1 = 6.50696 loss)
I0524 02:19:12.979233 11835 sgd_solver.cpp:112] Iteration 22630, lr = 0.1
I0524 02:19:21.374181 11835 solver.cpp:239] Iteration 22640 (1.1912 iter/s, 8.39488s/10 iters), loss = 7.12511
I0524 02:19:21.374225 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.12511 (* 1 = 7.12511 loss)
I0524 02:19:21.374502 11835 sgd_solver.cpp:112] Iteration 22640, lr = 0.1
I0524 02:19:28.573379 11835 solver.cpp:239] Iteration 22650 (1.38911 iter/s, 7.19887s/10 iters), loss = 7.49069
I0524 02:19:28.573442 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.49069 (* 1 = 7.49069 loss)
I0524 02:19:28.573658 11835 sgd_solver.cpp:112] Iteration 22650, lr = 0.1
I0524 02:19:35.902792 11835 solver.cpp:239] Iteration 22660 (1.36443 iter/s, 7.32907s/10 iters), loss = 7.18647
I0524 02:19:35.902844 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.18647 (* 1 = 7.18647 loss)
I0524 02:19:35.903017 11835 sgd_solver.cpp:112] Iteration 22660, lr = 0.1
I0524 02:19:43.362788 11835 solver.cpp:239] Iteration 22670 (1.34054 iter/s, 7.45966s/10 iters), loss = 7.62592
I0524 02:19:43.362884 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.62592 (* 1 = 7.62592 loss)
I0524 02:19:43.453699 11835 sgd_solver.cpp:112] Iteration 22670, lr = 0.1
I0524 02:19:50.476634 11835 solver.cpp:239] Iteration 22680 (1.40578 iter/s, 7.11347s/10 iters), loss = 7.09787
I0524 02:19:50.476683 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09787 (* 1 = 7.09787 loss)
I0524 02:19:50.476697 11835 sgd_solver.cpp:112] Iteration 22680, lr = 0.1
I0524 02:19:57.277483 11835 solver.cpp:239] Iteration 22690 (1.47071 iter/s, 6.79943s/10 iters), loss = 6.89116
I0524 02:19:57.277531 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89116 (* 1 = 6.89116 loss)
I0524 02:19:57.277806 11835 sgd_solver.cpp:112] Iteration 22690, lr = 0.1
I0524 02:20:05.162541 11835 solver.cpp:239] Iteration 22700 (1.26828 iter/s, 7.88471s/10 iters), loss = 6.52047
I0524 02:20:05.162593 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52047 (* 1 = 6.52047 loss)
I0524 02:20:05.162729 11835 sgd_solver.cpp:112] Iteration 22700, lr = 0.1
I0524 02:20:12.730273 11835 solver.cpp:239] Iteration 22710 (1.32146 iter/s, 7.56739s/10 iters), loss = 6.79259
I0524 02:20:12.730334 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79259 (* 1 = 6.79259 loss)
I0524 02:20:12.730458 11835 sgd_solver.cpp:112] Iteration 22710, lr = 0.1
I0524 02:20:19.489764 11835 solver.cpp:239] Iteration 22720 (1.47947 iter/s, 6.75917s/10 iters), loss = 7.86829
I0524 02:20:19.490072 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.86829 (* 1 = 7.86829 loss)
I0524 02:20:19.490134 11835 sgd_solver.cpp:112] Iteration 22720, lr = 0.1
I0524 02:20:27.102553 11835 solver.cpp:239] Iteration 22730 (1.31403 iter/s, 7.61015s/10 iters), loss = 6.50625
I0524 02:20:27.102602 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50625 (* 1 = 6.50625 loss)
I0524 02:20:27.926415 11835 sgd_solver.cpp:112] Iteration 22730, lr = 0.1
I0524 02:20:34.192330 11835 solver.cpp:239] Iteration 22740 (1.41054 iter/s, 7.08947s/10 iters), loss = 6.75868
I0524 02:20:34.192371 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.75868 (* 1 = 6.75868 loss)
I0524 02:20:34.256644 11835 sgd_solver.cpp:112] Iteration 22740, lr = 0.1
I0524 02:20:40.746843 11835 solver.cpp:239] Iteration 22750 (1.52574 iter/s, 6.55421s/10 iters), loss = 6.34511
I0524 02:20:40.746893 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34511 (* 1 = 6.34511 loss)
I0524 02:20:40.747007 11835 sgd_solver.cpp:112] Iteration 22750, lr = 0.1
I0524 02:20:47.944852 11835 solver.cpp:239] Iteration 22760 (1.38934 iter/s, 7.19769s/10 iters), loss = 7.49586
I0524 02:20:47.944908 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.49586 (* 1 = 7.49586 loss)
I0524 02:20:47.945137 11835 sgd_solver.cpp:112] Iteration 22760, lr = 0.1
I0524 02:20:54.234726 11835 solver.cpp:239] Iteration 22770 (1.58993 iter/s, 6.28957s/10 iters), loss = 6.59897
I0524 02:20:54.234961 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59897 (* 1 = 6.59897 loss)
I0524 02:20:54.957659 11835 sgd_solver.cpp:112] Iteration 22770, lr = 0.1
I0524 02:21:03.087875 11835 solver.cpp:239] Iteration 22780 (1.12961 iter/s, 8.8526s/10 iters), loss = 7.30553
I0524 02:21:03.087931 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.30553 (* 1 = 7.30553 loss)
I0524 02:21:03.088215 11835 sgd_solver.cpp:112] Iteration 22780, lr = 0.1
I0524 02:21:10.772974 11835 solver.cpp:239] Iteration 22790 (1.30128 iter/s, 7.68475s/10 iters), loss = 6.66545
I0524 02:21:10.773031 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66545 (* 1 = 6.66545 loss)
I0524 02:21:10.773205 11835 sgd_solver.cpp:112] Iteration 22790, lr = 0.1
I0524 02:21:17.869124 11835 solver.cpp:239] Iteration 22800 (1.40928 iter/s, 7.09582s/10 iters), loss = 7.12516
I0524 02:21:17.869180 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.12516 (* 1 = 7.12516 loss)
I0524 02:21:18.278870 11835 sgd_solver.cpp:112] Iteration 22800, lr = 0.1
I0524 02:21:24.225316 11835 solver.cpp:239] Iteration 22810 (1.57335 iter/s, 6.35588s/10 iters), loss = 6.46407
I0524 02:21:24.225376 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46407 (* 1 = 6.46407 loss)
I0524 02:21:24.225409 11835 sgd_solver.cpp:112] Iteration 22810, lr = 0.1
I0524 02:21:31.673684 11835 solver.cpp:239] Iteration 22820 (1.34264 iter/s, 7.44802s/10 iters), loss = 6.95738
I0524 02:21:31.673831 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.95738 (* 1 = 6.95738 loss)
I0524 02:21:31.842887 11835 sgd_solver.cpp:112] Iteration 22820, lr = 0.1
I0524 02:21:37.558691 11835 solver.cpp:239] Iteration 22830 (1.69934 iter/s, 5.88464s/10 iters), loss = 6.55807
I0524 02:21:37.558765 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55807 (* 1 = 6.55807 loss)
I0524 02:21:37.558779 11835 sgd_solver.cpp:112] Iteration 22830, lr = 0.1
I0524 02:21:43.381969 11835 solver.cpp:239] Iteration 22840 (1.71734 iter/s, 5.82297s/10 iters), loss = 6.16855
I0524 02:21:43.382019 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16855 (* 1 = 6.16855 loss)
I0524 02:21:43.382081 11835 sgd_solver.cpp:112] Iteration 22840, lr = 0.1
I0524 02:21:49.764258 11835 solver.cpp:239] Iteration 22850 (1.56691 iter/s, 6.38201s/10 iters), loss = 6.41901
I0524 02:21:49.764298 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41901 (* 1 = 6.41901 loss)
I0524 02:21:49.818327 11835 sgd_solver.cpp:112] Iteration 22850, lr = 0.1
I0524 02:21:56.578456 11835 solver.cpp:239] Iteration 22860 (1.46759 iter/s, 6.81389s/10 iters), loss = 6.45267
I0524 02:21:56.578505 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45267 (* 1 = 6.45267 loss)
I0524 02:21:56.578546 11835 sgd_solver.cpp:112] Iteration 22860, lr = 0.1
I0524 02:22:02.973220 11835 solver.cpp:239] Iteration 22870 (1.56385 iter/s, 6.39448s/10 iters), loss = 6.10464
I0524 02:22:02.973400 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10464 (* 1 = 6.10464 loss)
I0524 02:22:03.159812 11835 sgd_solver.cpp:112] Iteration 22870, lr = 0.1
I0524 02:22:11.594394 11835 solver.cpp:239] Iteration 22880 (1.16 iter/s, 8.62066s/10 iters), loss = 7.38679
I0524 02:22:11.594444 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.38679 (* 1 = 7.38679 loss)
I0524 02:22:11.594460 11835 sgd_solver.cpp:112] Iteration 22880, lr = 0.1
I0524 02:22:17.763784 11835 solver.cpp:239] Iteration 22890 (1.62127 iter/s, 6.168s/10 iters), loss = 6.91871
I0524 02:22:17.763833 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.91871 (* 1 = 6.91871 loss)
I0524 02:22:17.813549 11835 sgd_solver.cpp:112] Iteration 22890, lr = 0.1
I0524 02:22:24.884639 11835 solver.cpp:239] Iteration 22900 (1.40439 iter/s, 7.12054s/10 iters), loss = 6.7243
I0524 02:22:24.884681 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7243 (* 1 = 6.7243 loss)
I0524 02:22:25.369707 11835 sgd_solver.cpp:112] Iteration 22900, lr = 0.1
I0524 02:22:32.440663 11835 solver.cpp:239] Iteration 22910 (1.32351 iter/s, 7.55568s/10 iters), loss = 7.40373
I0524 02:22:32.440723 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.40373 (* 1 = 7.40373 loss)
I0524 02:22:32.440975 11835 sgd_solver.cpp:112] Iteration 22910, lr = 0.1
I0524 02:22:39.894938 11835 solver.cpp:239] Iteration 22920 (1.34157 iter/s, 7.45394s/10 iters), loss = 7.16059
I0524 02:22:39.895169 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.16059 (* 1 = 7.16059 loss)
I0524 02:22:39.895223 11835 sgd_solver.cpp:112] Iteration 22920, lr = 0.1
I0524 02:22:45.864408 11835 solver.cpp:239] Iteration 22930 (1.67532 iter/s, 5.96901s/10 iters), loss = 6.83366
I0524 02:22:45.864457 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83366 (* 1 = 6.83366 loss)
I0524 02:22:45.864471 11835 sgd_solver.cpp:112] Iteration 22930, lr = 0.1
I0524 02:22:54.698588 11835 solver.cpp:239] Iteration 22940 (1.13202 iter/s, 8.8338s/10 iters), loss = 7.03665
I0524 02:22:54.698640 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03665 (* 1 = 7.03665 loss)
I0524 02:22:54.860983 11835 sgd_solver.cpp:112] Iteration 22940, lr = 0.1
I0524 02:23:01.684808 11835 solver.cpp:239] Iteration 22950 (1.43145 iter/s, 6.98591s/10 iters), loss = 6.13305
I0524 02:23:01.684854 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13305 (* 1 = 6.13305 loss)
I0524 02:23:01.686825 11835 sgd_solver.cpp:112] Iteration 22950, lr = 0.1
I0524 02:23:09.319896 11835 solver.cpp:239] Iteration 22960 (1.3098 iter/s, 7.63475s/10 iters), loss = 6.36274
I0524 02:23:09.319950 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36274 (* 1 = 6.36274 loss)
I0524 02:23:09.805866 11835 sgd_solver.cpp:112] Iteration 22960, lr = 0.1
I0524 02:23:16.102375 11835 solver.cpp:239] Iteration 22970 (1.47445 iter/s, 6.78217s/10 iters), loss = 7.00727
I0524 02:23:16.102619 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00727 (* 1 = 7.00727 loss)
I0524 02:23:16.102674 11835 sgd_solver.cpp:112] Iteration 22970, lr = 0.1
I0524 02:23:24.650198 11835 solver.cpp:239] Iteration 22980 (1.16996 iter/s, 8.54727s/10 iters), loss = 7.66725
I0524 02:23:24.650259 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.66725 (* 1 = 7.66725 loss)
I0524 02:23:24.650493 11835 sgd_solver.cpp:112] Iteration 22980, lr = 0.1
I0524 02:23:31.427433 11835 solver.cpp:239] Iteration 22990 (1.47559 iter/s, 6.77693s/10 iters), loss = 7.24094
I0524 02:23:31.427472 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.24094 (* 1 = 7.24094 loss)
I0524 02:23:32.004565 11835 sgd_solver.cpp:112] Iteration 22990, lr = 0.1
I0524 02:23:38.627902 11835 solver.cpp:239] Iteration 23000 (1.38886 iter/s, 7.20014s/10 iters), loss = 6.16567
I0524 02:23:38.627949 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16567 (* 1 = 6.16567 loss)
I0524 02:23:38.642000 11835 sgd_solver.cpp:112] Iteration 23000, lr = 0.1
I0524 02:23:45.531941 11835 solver.cpp:239] Iteration 23010 (1.44849 iter/s, 6.90373s/10 iters), loss = 6.4205
I0524 02:23:45.531994 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4205 (* 1 = 6.4205 loss)
I0524 02:23:45.532011 11835 sgd_solver.cpp:112] Iteration 23010, lr = 0.1
I0524 02:23:52.338649 11835 solver.cpp:239] Iteration 23020 (1.46923 iter/s, 6.8063s/10 iters), loss = 6.66161
I0524 02:23:52.338815 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66161 (* 1 = 6.66161 loss)
I0524 02:23:52.338933 11835 sgd_solver.cpp:112] Iteration 23020, lr = 0.1
I0524 02:24:00.241348 11835 solver.cpp:239] Iteration 23030 (1.26546 iter/s, 7.90224s/10 iters), loss = 6.34213
I0524 02:24:00.241387 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34213 (* 1 = 6.34213 loss)
I0524 02:24:00.408080 11835 sgd_solver.cpp:112] Iteration 23030, lr = 0.1
I0524 02:24:06.414944 11835 solver.cpp:239] Iteration 23040 (1.61989 iter/s, 6.17327s/10 iters), loss = 6.98774
I0524 02:24:06.415006 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98774 (* 1 = 6.98774 loss)
I0524 02:24:06.415436 11835 sgd_solver.cpp:112] Iteration 23040, lr = 0.1
I0524 02:24:13.825542 11835 solver.cpp:239] Iteration 23050 (1.34948 iter/s, 7.41025s/10 iters), loss = 7.65344
I0524 02:24:13.825588 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.65344 (* 1 = 7.65344 loss)
I0524 02:24:13.825716 11835 sgd_solver.cpp:112] Iteration 23050, lr = 0.1
I0524 02:24:20.842867 11835 solver.cpp:239] Iteration 23060 (1.42511 iter/s, 7.01702s/10 iters), loss = 6.20113
I0524 02:24:20.842916 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20113 (* 1 = 6.20113 loss)
I0524 02:24:20.904626 11835 sgd_solver.cpp:112] Iteration 23060, lr = 0.1
I0524 02:24:29.316793 11835 solver.cpp:239] Iteration 23070 (1.18014 iter/s, 8.47355s/10 iters), loss = 7.71748
I0524 02:24:29.317071 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.71748 (* 1 = 7.71748 loss)
I0524 02:24:29.585839 11835 sgd_solver.cpp:112] Iteration 23070, lr = 0.1
I0524 02:24:37.135627 11835 solver.cpp:239] Iteration 23080 (1.27905 iter/s, 7.81831s/10 iters), loss = 6.66577
I0524 02:24:37.135676 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66577 (* 1 = 6.66577 loss)
I0524 02:24:37.136688 11835 sgd_solver.cpp:112] Iteration 23080, lr = 0.1
I0524 02:24:43.237859 11835 solver.cpp:239] Iteration 23090 (1.63882 iter/s, 6.10194s/10 iters), loss = 6.88316
I0524 02:24:43.237913 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88316 (* 1 = 6.88316 loss)
I0524 02:24:43.238132 11835 sgd_solver.cpp:112] Iteration 23090, lr = 0.1
I0524 02:24:49.609941 11835 solver.cpp:239] Iteration 23100 (1.56942 iter/s, 6.37178s/10 iters), loss = 7.65575
I0524 02:24:49.609988 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.65575 (* 1 = 7.65575 loss)
I0524 02:24:49.610002 11835 sgd_solver.cpp:112] Iteration 23100, lr = 0.1
I0524 02:24:55.898746 11835 solver.cpp:239] Iteration 23110 (1.59024 iter/s, 6.28835s/10 iters), loss = 7.38762
I0524 02:24:55.898792 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.38762 (* 1 = 7.38762 loss)
I0524 02:24:56.089015 11835 sgd_solver.cpp:112] Iteration 23110, lr = 0.1
I0524 02:25:04.659413 11835 solver.cpp:239] Iteration 23120 (1.14152 iter/s, 8.76025s/10 iters), loss = 5.69837
I0524 02:25:04.659739 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.69837 (* 1 = 5.69837 loss)
I0524 02:25:04.659813 11835 sgd_solver.cpp:112] Iteration 23120, lr = 0.1
I0524 02:25:11.611727 11835 solver.cpp:239] Iteration 23130 (1.43851 iter/s, 6.95162s/10 iters), loss = 6.46723
I0524 02:25:11.611776 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46723 (* 1 = 6.46723 loss)
I0524 02:25:12.653990 11835 sgd_solver.cpp:112] Iteration 23130, lr = 0.1
I0524 02:25:19.885870 11835 solver.cpp:239] Iteration 23140 (1.20864 iter/s, 8.27377s/10 iters), loss = 7.47012
I0524 02:25:19.885921 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.47012 (* 1 = 7.47012 loss)
I0524 02:25:19.886358 11835 sgd_solver.cpp:112] Iteration 23140, lr = 0.1
I0524 02:25:25.735908 11835 solver.cpp:239] Iteration 23150 (1.70947 iter/s, 5.84977s/10 iters), loss = 6.75024
I0524 02:25:25.735946 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.75024 (* 1 = 6.75024 loss)
I0524 02:25:25.735957 11835 sgd_solver.cpp:112] Iteration 23150, lr = 0.1
I0524 02:25:31.396860 11835 solver.cpp:239] Iteration 23160 (1.76657 iter/s, 5.66069s/10 iters), loss = 6.43632
I0524 02:25:31.396903 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43632 (* 1 = 6.43632 loss)
I0524 02:25:31.396917 11835 sgd_solver.cpp:112] Iteration 23160, lr = 0.1
I0524 02:25:40.214638 11835 solver.cpp:239] Iteration 23170 (1.13412 iter/s, 8.81739s/10 iters), loss = 7.23738
I0524 02:25:40.214901 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.23738 (* 1 = 7.23738 loss)
I0524 02:25:40.214957 11835 sgd_solver.cpp:112] Iteration 23170, lr = 0.1
I0524 02:25:47.931082 11835 solver.cpp:239] Iteration 23180 (1.29602 iter/s, 7.71593s/10 iters), loss = 6.38716
I0524 02:25:47.931133 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38716 (* 1 = 6.38716 loss)
I0524 02:25:48.437810 11835 sgd_solver.cpp:112] Iteration 23180, lr = 0.1
I0524 02:25:54.389039 11835 solver.cpp:239] Iteration 23190 (1.54855 iter/s, 6.45765s/10 iters), loss = 6.70588
I0524 02:25:54.389088 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70588 (* 1 = 6.70588 loss)
I0524 02:25:54.389284 11835 sgd_solver.cpp:112] Iteration 23190, lr = 0.1
I0524 02:26:00.812024 11835 solver.cpp:239] Iteration 23200 (1.55698 iter/s, 6.4227s/10 iters), loss = 7.39444
I0524 02:26:00.812067 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.39444 (* 1 = 7.39444 loss)
I0524 02:26:00.853049 11835 sgd_solver.cpp:112] Iteration 23200, lr = 0.1
I0524 02:26:07.173585 11835 solver.cpp:239] Iteration 23210 (1.57201 iter/s, 6.36127s/10 iters), loss = 6.59514
I0524 02:26:07.173638 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59514 (* 1 = 6.59514 loss)
I0524 02:26:07.173657 11835 sgd_solver.cpp:112] Iteration 23210, lr = 0.1
I0524 02:26:14.315063 11835 solver.cpp:239] Iteration 23220 (1.40034 iter/s, 7.1411s/10 iters), loss = 7.3069
I0524 02:26:14.315155 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.3069 (* 1 = 7.3069 loss)
I0524 02:26:14.315240 11835 sgd_solver.cpp:112] Iteration 23220, lr = 0.1
I0524 02:26:21.750455 11835 solver.cpp:239] Iteration 23230 (1.34499 iter/s, 7.435s/10 iters), loss = 7.10393
I0524 02:26:21.750524 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10393 (* 1 = 7.10393 loss)
I0524 02:26:22.561014 11835 sgd_solver.cpp:112] Iteration 23230, lr = 0.1
I0524 02:26:31.338165 11835 solver.cpp:239] Iteration 23240 (1.04305 iter/s, 9.58729s/10 iters), loss = 6.83346
I0524 02:26:31.338207 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83346 (* 1 = 6.83346 loss)
I0524 02:26:31.338346 11835 sgd_solver.cpp:112] Iteration 23240, lr = 0.1
I0524 02:26:38.139320 11835 solver.cpp:239] Iteration 23250 (1.47041 iter/s, 6.80085s/10 iters), loss = 6.78694
I0524 02:26:38.139371 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78694 (* 1 = 6.78694 loss)
I0524 02:26:38.139385 11835 sgd_solver.cpp:112] Iteration 23250, lr = 0.1
I0524 02:26:44.868546 11835 solver.cpp:239] Iteration 23260 (1.48614 iter/s, 6.72885s/10 iters), loss = 5.97014
I0524 02:26:44.868855 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97014 (* 1 = 5.97014 loss)
I0524 02:26:44.868909 11835 sgd_solver.cpp:112] Iteration 23260, lr = 0.1
I0524 02:26:50.678187 11835 solver.cpp:239] Iteration 23270 (1.72148 iter/s, 5.80895s/10 iters), loss = 7.28349
I0524 02:26:50.678236 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.28349 (* 1 = 7.28349 loss)
I0524 02:26:50.678462 11835 sgd_solver.cpp:112] Iteration 23270, lr = 0.1
I0524 02:26:57.483680 11835 solver.cpp:239] Iteration 23280 (1.46947 iter/s, 6.80518s/10 iters), loss = 7.20174
I0524 02:26:57.483731 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.20174 (* 1 = 7.20174 loss)
I0524 02:26:57.483744 11835 sgd_solver.cpp:112] Iteration 23280, lr = 0.1
I0524 02:27:03.867377 11835 solver.cpp:239] Iteration 23290 (1.56659 iter/s, 6.38328s/10 iters), loss = 5.9055
I0524 02:27:03.867415 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9055 (* 1 = 5.9055 loss)
I0524 02:27:03.867558 11835 sgd_solver.cpp:112] Iteration 23290, lr = 0.1
I0524 02:27:10.176986 11835 solver.cpp:239] Iteration 23300 (1.58496 iter/s, 6.30932s/10 iters), loss = 7.09821
I0524 02:27:10.177037 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09821 (* 1 = 7.09821 loss)
I0524 02:27:10.177274 11835 sgd_solver.cpp:112] Iteration 23300, lr = 0.1
I0524 02:27:17.263137 11835 solver.cpp:239] Iteration 23310 (1.41127 iter/s, 7.08582s/10 iters), loss = 5.93721
I0524 02:27:17.263392 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93721 (* 1 = 5.93721 loss)
I0524 02:27:17.263450 11835 sgd_solver.cpp:112] Iteration 23310, lr = 0.1
I0524 02:27:24.544175 11835 solver.cpp:239] Iteration 23320 (1.37353 iter/s, 7.28053s/10 iters), loss = 6.25064
I0524 02:27:24.544214 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25064 (* 1 = 6.25064 loss)
I0524 02:27:24.977263 11835 sgd_solver.cpp:112] Iteration 23320, lr = 0.1
I0524 02:27:31.110347 11835 solver.cpp:239] Iteration 23330 (1.52303 iter/s, 6.56587s/10 iters), loss = 6.37936
I0524 02:27:31.110396 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37936 (* 1 = 6.37936 loss)
I0524 02:27:31.110411 11835 sgd_solver.cpp:112] Iteration 23330, lr = 0.1
I0524 02:27:38.329238 11835 solver.cpp:239] Iteration 23340 (1.38534 iter/s, 7.21842s/10 iters), loss = 6.51339
I0524 02:27:38.329283 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51339 (* 1 = 6.51339 loss)
I0524 02:27:38.329608 11835 sgd_solver.cpp:112] Iteration 23340, lr = 0.1
I0524 02:27:44.885557 11835 solver.cpp:239] Iteration 23350 (1.52532 iter/s, 6.55601s/10 iters), loss = 6.67108
I0524 02:27:44.885617 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67108 (* 1 = 6.67108 loss)
I0524 02:27:44.885805 11835 sgd_solver.cpp:112] Iteration 23350, lr = 0.1
I0524 02:27:51.074887 11835 solver.cpp:239] Iteration 23360 (1.61576 iter/s, 6.18903s/10 iters), loss = 6.83795
I0524 02:27:51.074987 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83795 (* 1 = 6.83795 loss)
I0524 02:27:51.215724 11835 sgd_solver.cpp:112] Iteration 23360, lr = 0.1
I0524 02:27:59.198998 11835 solver.cpp:239] Iteration 23370 (1.23096 iter/s, 8.12371s/10 iters), loss = 5.96052
I0524 02:27:59.199038 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96052 (* 1 = 5.96052 loss)
I0524 02:27:59.199168 11835 sgd_solver.cpp:112] Iteration 23370, lr = 0.1
I0524 02:28:08.792285 11835 solver.cpp:239] Iteration 23380 (1.04244 iter/s, 9.59287s/10 iters), loss = 5.87078
I0524 02:28:08.792348 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87078 (* 1 = 5.87078 loss)
I0524 02:28:08.792549 11835 sgd_solver.cpp:112] Iteration 23380, lr = 0.1
I0524 02:28:14.684051 11835 solver.cpp:239] Iteration 23390 (1.69737 iter/s, 5.89148s/10 iters), loss = 5.97729
I0524 02:28:14.684098 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97729 (* 1 = 5.97729 loss)
I0524 02:28:14.687786 11835 sgd_solver.cpp:112] Iteration 23390, lr = 0.1
I0524 02:28:22.245138 11835 solver.cpp:239] Iteration 23400 (1.32262 iter/s, 7.56076s/10 iters), loss = 6.92372
I0524 02:28:22.245398 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92372 (* 1 = 6.92372 loss)
I0524 02:28:22.245435 11835 sgd_solver.cpp:112] Iteration 23400, lr = 0.1
I0524 02:28:29.620537 11835 solver.cpp:239] Iteration 23410 (1.35596 iter/s, 7.37487s/10 iters), loss = 6.61182
I0524 02:28:29.620589 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61182 (* 1 = 6.61182 loss)
I0524 02:28:30.431516 11835 sgd_solver.cpp:112] Iteration 23410, lr = 0.1
I0524 02:28:39.053182 11835 solver.cpp:239] Iteration 23420 (1.0602 iter/s, 9.43223s/10 iters), loss = 6.14396
I0524 02:28:39.053244 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14396 (* 1 = 6.14396 loss)
I0524 02:28:39.053460 11835 sgd_solver.cpp:112] Iteration 23420, lr = 0.1
I0524 02:28:45.796741 11835 solver.cpp:239] Iteration 23430 (1.48297 iter/s, 6.74324s/10 iters), loss = 7.15946
I0524 02:28:45.796790 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.15946 (* 1 = 7.15946 loss)
I0524 02:28:45.796803 11835 sgd_solver.cpp:112] Iteration 23430, lr = 0.1
I0524 02:28:52.257120 11835 solver.cpp:239] Iteration 23440 (1.54797 iter/s, 6.46008s/10 iters), loss = 6.6165
I0524 02:28:52.257375 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6165 (* 1 = 6.6165 loss)
I0524 02:28:52.257448 11835 sgd_solver.cpp:112] Iteration 23440, lr = 0.1
I0524 02:28:58.081820 11835 solver.cpp:239] Iteration 23450 (1.71695 iter/s, 5.82427s/10 iters), loss = 7.6295
I0524 02:28:58.081871 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.6295 (* 1 = 7.6295 loss)
I0524 02:28:58.081888 11835 sgd_solver.cpp:112] Iteration 23450, lr = 0.1
I0524 02:29:04.839579 11835 solver.cpp:239] Iteration 23460 (1.47985 iter/s, 6.75746s/10 iters), loss = 6.82945
I0524 02:29:04.839619 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.82945 (* 1 = 6.82945 loss)
I0524 02:29:04.839632 11835 sgd_solver.cpp:112] Iteration 23460, lr = 0.1
I0524 02:29:11.296670 11835 solver.cpp:239] Iteration 23470 (1.54878 iter/s, 6.45668s/10 iters), loss = 6.4299
I0524 02:29:11.296720 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4299 (* 1 = 6.4299 loss)
I0524 02:29:11.296850 11835 sgd_solver.cpp:112] Iteration 23470, lr = 0.1
I0524 02:29:18.146951 11835 solver.cpp:239] Iteration 23480 (1.45986 iter/s, 6.84998s/10 iters), loss = 6.11587
I0524 02:29:18.146997 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11587 (* 1 = 6.11587 loss)
I0524 02:29:18.147009 11835 sgd_solver.cpp:112] Iteration 23480, lr = 0.1
I0524 02:29:23.646983 11835 solver.cpp:239] Iteration 23490 (1.8183 iter/s, 5.49964s/10 iters), loss = 7.81656
I0524 02:29:23.647284 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.81656 (* 1 = 7.81656 loss)
I0524 02:29:23.647344 11835 sgd_solver.cpp:112] Iteration 23490, lr = 0.1
I0524 02:29:29.506301 11835 solver.cpp:239] Iteration 23500 (1.70687 iter/s, 5.85867s/10 iters), loss = 6.66379
I0524 02:29:29.506340 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66379 (* 1 = 6.66379 loss)
I0524 02:29:29.506358 11835 sgd_solver.cpp:112] Iteration 23500, lr = 0.1
I0524 02:29:35.208387 11835 solver.cpp:239] Iteration 23510 (1.75383 iter/s, 5.70182s/10 iters), loss = 7.79947
I0524 02:29:35.208437 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.79947 (* 1 = 7.79947 loss)
I0524 02:29:35.208451 11835 sgd_solver.cpp:112] Iteration 23510, lr = 0.1
I0524 02:29:43.190824 11835 solver.cpp:239] Iteration 23520 (1.25281 iter/s, 7.98207s/10 iters), loss = 6.60397
I0524 02:29:43.190877 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60397 (* 1 = 6.60397 loss)
I0524 02:29:43.190937 11835 sgd_solver.cpp:112] Iteration 23520, lr = 0.1
I0524 02:29:49.103636 11835 solver.cpp:239] Iteration 23530 (1.69132 iter/s, 5.91254s/10 iters), loss = 5.63743
I0524 02:29:49.103670 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63743 (* 1 = 5.63743 loss)
I0524 02:29:49.103824 11835 sgd_solver.cpp:112] Iteration 23530, lr = 0.1
I0524 02:29:56.517578 11835 solver.cpp:239] Iteration 23540 (1.34887 iter/s, 7.41362s/10 iters), loss = 7.3378
I0524 02:29:56.517748 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.3378 (* 1 = 7.3378 loss)
I0524 02:29:56.517786 11835 sgd_solver.cpp:112] Iteration 23540, lr = 0.1
I0524 02:30:02.425951 11835 solver.cpp:239] Iteration 23550 (1.69262 iter/s, 5.908s/10 iters), loss = 6.71212
I0524 02:30:02.425995 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71212 (* 1 = 6.71212 loss)
I0524 02:30:02.426108 11835 sgd_solver.cpp:112] Iteration 23550, lr = 0.1
I0524 02:30:08.754359 11835 solver.cpp:239] Iteration 23560 (1.58025 iter/s, 6.32812s/10 iters), loss = 7.03945
I0524 02:30:08.754410 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03945 (* 1 = 7.03945 loss)
I0524 02:30:08.754539 11835 sgd_solver.cpp:112] Iteration 23560, lr = 0.1
I0524 02:30:14.974748 11835 solver.cpp:239] Iteration 23570 (1.60769 iter/s, 6.2201s/10 iters), loss = 6.79505
I0524 02:30:14.974805 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79505 (* 1 = 6.79505 loss)
I0524 02:30:14.975009 11835 sgd_solver.cpp:112] Iteration 23570, lr = 0.1
I0524 02:30:20.744026 11835 solver.cpp:239] Iteration 23580 (1.7334 iter/s, 5.76901s/10 iters), loss = 7.08027
I0524 02:30:20.744060 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.08027 (* 1 = 7.08027 loss)
I0524 02:30:20.744082 11835 sgd_solver.cpp:112] Iteration 23580, lr = 0.1
I0524 02:30:26.553649 11835 solver.cpp:239] Iteration 23590 (1.72136 iter/s, 5.80935s/10 iters), loss = 6.12507
I0524 02:30:26.553949 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12507 (* 1 = 6.12507 loss)
I0524 02:30:26.554009 11835 sgd_solver.cpp:112] Iteration 23590, lr = 0.1
I0524 02:30:35.670168 11835 solver.cpp:239] Iteration 23600 (1.09699 iter/s, 9.11586s/10 iters), loss = 6.61659
I0524 02:30:35.670212 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61659 (* 1 = 6.61659 loss)
I0524 02:30:37.320739 11835 sgd_solver.cpp:112] Iteration 23600, lr = 0.1
I0524 02:30:42.902117 11835 solver.cpp:239] Iteration 23610 (1.38282 iter/s, 7.23162s/10 iters), loss = 7.32967
I0524 02:30:42.902170 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.32967 (* 1 = 7.32967 loss)
I0524 02:30:42.902186 11835 sgd_solver.cpp:112] Iteration 23610, lr = 0.1
I0524 02:30:52.155395 11835 solver.cpp:239] Iteration 23620 (1.08075 iter/s, 9.2528s/10 iters), loss = 7.16156
I0524 02:30:52.155443 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.16156 (* 1 = 7.16156 loss)
I0524 02:30:52.172665 11835 sgd_solver.cpp:112] Iteration 23620, lr = 0.1
I0524 02:30:57.946884 11835 solver.cpp:239] Iteration 23630 (1.72675 iter/s, 5.79122s/10 iters), loss = 6.31813
I0524 02:30:57.947173 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31813 (* 1 = 6.31813 loss)
I0524 02:30:57.947233 11835 sgd_solver.cpp:112] Iteration 23630, lr = 0.1
I0524 02:31:04.387702 11835 solver.cpp:239] Iteration 23640 (1.55272 iter/s, 6.4403s/10 iters), loss = 6.16618
I0524 02:31:04.387765 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16618 (* 1 = 6.16618 loss)
I0524 02:31:04.387823 11835 sgd_solver.cpp:112] Iteration 23640, lr = 0.1
I0524 02:31:11.057457 11835 solver.cpp:239] Iteration 23650 (1.49937 iter/s, 6.66945s/10 iters), loss = 7.34485
I0524 02:31:11.057492 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.34485 (* 1 = 7.34485 loss)
I0524 02:31:11.057632 11835 sgd_solver.cpp:112] Iteration 23650, lr = 0.1
I0524 02:31:19.159341 11835 solver.cpp:239] Iteration 23660 (1.23433 iter/s, 8.10153s/10 iters), loss = 6.8289
I0524 02:31:19.159394 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8289 (* 1 = 6.8289 loss)
I0524 02:31:19.159497 11835 sgd_solver.cpp:112] Iteration 23660, lr = 0.1
I0524 02:31:25.716773 11835 solver.cpp:239] Iteration 23670 (1.52506 iter/s, 6.55712s/10 iters), loss = 6.12558
I0524 02:31:25.716835 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12558 (* 1 = 6.12558 loss)
I0524 02:31:25.716897 11835 sgd_solver.cpp:112] Iteration 23670, lr = 0.1
I0524 02:31:32.123584 11835 solver.cpp:239] Iteration 23680 (1.56091 iter/s, 6.40651s/10 iters), loss = 8.46824
I0524 02:31:32.123886 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.46824 (* 1 = 8.46824 loss)
I0524 02:31:32.125598 11835 sgd_solver.cpp:112] Iteration 23680, lr = 0.1
I0524 02:31:38.534287 11835 solver.cpp:239] Iteration 23690 (1.56002 iter/s, 6.41019s/10 iters), loss = 5.97048
I0524 02:31:38.534334 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97048 (* 1 = 5.97048 loss)
I0524 02:31:38.534539 11835 sgd_solver.cpp:112] Iteration 23690, lr = 0.1
I0524 02:31:45.191562 11835 solver.cpp:239] Iteration 23700 (1.50218 iter/s, 6.65698s/10 iters), loss = 7.71926
I0524 02:31:45.191606 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.71926 (* 1 = 7.71926 loss)
I0524 02:31:45.191907 11835 sgd_solver.cpp:112] Iteration 23700, lr = 0.1
I0524 02:31:52.620630 11835 solver.cpp:239] Iteration 23710 (1.34613 iter/s, 7.42873s/10 iters), loss = 7.17726
I0524 02:31:52.620679 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.17726 (* 1 = 7.17726 loss)
I0524 02:31:52.686614 11835 sgd_solver.cpp:112] Iteration 23710, lr = 0.1
I0524 02:31:59.660149 11835 solver.cpp:239] Iteration 23720 (1.42062 iter/s, 7.0392s/10 iters), loss = 6.14953
I0524 02:31:59.660194 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14953 (* 1 = 6.14953 loss)
I0524 02:31:59.660209 11835 sgd_solver.cpp:112] Iteration 23720, lr = 0.1
I0524 02:32:07.155484 11835 solver.cpp:239] Iteration 23730 (1.33422 iter/s, 7.49501s/10 iters), loss = 6.20505
I0524 02:32:07.155742 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20505 (* 1 = 6.20505 loss)
I0524 02:32:07.155805 11835 sgd_solver.cpp:112] Iteration 23730, lr = 0.1
I0524 02:32:12.660646 11835 solver.cpp:239] Iteration 23740 (1.81696 iter/s, 5.50369s/10 iters), loss = 5.75528
I0524 02:32:12.660691 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75528 (* 1 = 5.75528 loss)
I0524 02:32:12.660921 11835 sgd_solver.cpp:112] Iteration 23740, lr = 0.1
I0524 02:32:18.584750 11835 solver.cpp:239] Iteration 23750 (1.6881 iter/s, 5.92383s/10 iters), loss = 7.1674
I0524 02:32:18.584797 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.1674 (* 1 = 7.1674 loss)
I0524 02:32:18.584851 11835 sgd_solver.cpp:112] Iteration 23750, lr = 0.1
I0524 02:32:24.543311 11835 solver.cpp:239] Iteration 23760 (1.67834 iter/s, 5.95827s/10 iters), loss = 7.38513
I0524 02:32:24.543365 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.38513 (* 1 = 7.38513 loss)
I0524 02:32:24.543810 11835 sgd_solver.cpp:112] Iteration 23760, lr = 0.1
I0524 02:32:30.974596 11835 solver.cpp:239] Iteration 23770 (1.55497 iter/s, 6.43099s/10 iters), loss = 6.75026
I0524 02:32:30.974639 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.75026 (* 1 = 6.75026 loss)
I0524 02:32:30.974654 11835 sgd_solver.cpp:112] Iteration 23770, lr = 0.1
I0524 02:32:37.216337 11835 solver.cpp:239] Iteration 23780 (1.60221 iter/s, 6.24139s/10 iters), loss = 7.57746
I0524 02:32:37.216595 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.57746 (* 1 = 7.57746 loss)
I0524 02:32:37.216675 11835 sgd_solver.cpp:112] Iteration 23780, lr = 0.1
I0524 02:32:44.545411 11835 solver.cpp:239] Iteration 23790 (1.36452 iter/s, 7.32857s/10 iters), loss = 6.97325
I0524 02:32:44.545461 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.97325 (* 1 = 6.97325 loss)
I0524 02:32:44.545655 11835 sgd_solver.cpp:112] Iteration 23790, lr = 0.1
I0524 02:32:53.081274 11835 solver.cpp:239] Iteration 23800 (1.17158 iter/s, 8.53549s/10 iters), loss = 7.38047
I0524 02:32:53.081316 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.38047 (* 1 = 7.38047 loss)
I0524 02:32:53.081456 11835 sgd_solver.cpp:112] Iteration 23800, lr = 0.1
I0524 02:32:59.371417 11835 solver.cpp:239] Iteration 23810 (1.58986 iter/s, 6.28985s/10 iters), loss = 6.96588
I0524 02:32:59.371477 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96588 (* 1 = 6.96588 loss)
I0524 02:32:59.373411 11835 sgd_solver.cpp:112] Iteration 23810, lr = 0.1
I0524 02:33:08.152843 11835 solver.cpp:239] Iteration 23820 (1.13882 iter/s, 8.78103s/10 iters), loss = 7.69392
I0524 02:33:08.153187 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.69392 (* 1 = 7.69392 loss)
I0524 02:33:08.451159 11835 sgd_solver.cpp:112] Iteration 23820, lr = 0.1
I0524 02:33:15.346617 11835 solver.cpp:239] Iteration 23830 (1.3902 iter/s, 7.19322s/10 iters), loss = 6.51323
I0524 02:33:15.346657 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51323 (* 1 = 6.51323 loss)
I0524 02:33:16.268204 11835 sgd_solver.cpp:112] Iteration 23830, lr = 0.1
I0524 02:33:25.615314 11835 solver.cpp:239] Iteration 23840 (0.973875 iter/s, 10.2683s/10 iters), loss = 6.89655
I0524 02:33:25.615387 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89655 (* 1 = 6.89655 loss)
I0524 02:33:26.175669 11835 sgd_solver.cpp:112] Iteration 23840, lr = 0.1
I0524 02:33:33.508615 11835 solver.cpp:239] Iteration 23850 (1.26696 iter/s, 7.89293s/10 iters), loss = 6.88257
I0524 02:33:33.508671 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88257 (* 1 = 6.88257 loss)
I0524 02:33:33.508841 11835 sgd_solver.cpp:112] Iteration 23850, lr = 0.1
I0524 02:33:39.228368 11835 solver.cpp:239] Iteration 23860 (1.74841 iter/s, 5.71948s/10 iters), loss = 7.82039
I0524 02:33:39.228485 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.82039 (* 1 = 7.82039 loss)
I0524 02:33:39.228531 11835 sgd_solver.cpp:112] Iteration 23860, lr = 0.1
I0524 02:33:47.582331 11835 solver.cpp:239] Iteration 23870 (1.1971 iter/s, 8.35352s/10 iters), loss = 6.93147
I0524 02:33:47.582381 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93147 (* 1 = 6.93147 loss)
I0524 02:33:48.243396 11835 sgd_solver.cpp:112] Iteration 23870, lr = 0.1
I0524 02:33:54.755132 11835 solver.cpp:239] Iteration 23880 (1.39422 iter/s, 7.17247s/10 iters), loss = 8.20503
I0524 02:33:54.755187 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.20503 (* 1 = 8.20503 loss)
I0524 02:33:54.755515 11835 sgd_solver.cpp:112] Iteration 23880, lr = 0.1
I0524 02:34:02.137612 11835 solver.cpp:239] Iteration 23890 (1.35462 iter/s, 7.38215s/10 iters), loss = 6.69929
I0524 02:34:02.137655 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69929 (* 1 = 6.69929 loss)
I0524 02:34:02.137897 11835 sgd_solver.cpp:112] Iteration 23890, lr = 0.1
I0524 02:34:09.399719 11835 solver.cpp:239] Iteration 23900 (1.37707 iter/s, 7.26177s/10 iters), loss = 6.21308
I0524 02:34:09.400002 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21308 (* 1 = 6.21308 loss)
I0524 02:34:09.400146 11835 sgd_solver.cpp:112] Iteration 23900, lr = 0.1
I0524 02:34:15.768509 11835 solver.cpp:239] Iteration 23910 (1.57028 iter/s, 6.36831s/10 iters), loss = 5.89836
I0524 02:34:15.768563 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89836 (* 1 = 5.89836 loss)
I0524 02:34:15.768673 11835 sgd_solver.cpp:112] Iteration 23910, lr = 0.1
I0524 02:34:21.883668 11835 solver.cpp:239] Iteration 23920 (1.63536 iter/s, 6.11486s/10 iters), loss = 6.1539
I0524 02:34:21.883750 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1539 (* 1 = 6.1539 loss)
I0524 02:34:22.821420 11835 sgd_solver.cpp:112] Iteration 23920, lr = 0.1
I0524 02:34:30.051993 11835 solver.cpp:239] Iteration 23930 (1.2243 iter/s, 8.16793s/10 iters), loss = 7.25419
I0524 02:34:30.052050 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.25419 (* 1 = 7.25419 loss)
I0524 02:34:30.052258 11835 sgd_solver.cpp:112] Iteration 23930, lr = 0.1
I0524 02:34:38.407280 11835 solver.cpp:239] Iteration 23940 (1.1969 iter/s, 8.35491s/10 iters), loss = 5.3942
I0524 02:34:38.407337 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.3942 (* 1 = 5.3942 loss)
I0524 02:34:38.407552 11835 sgd_solver.cpp:112] Iteration 23940, lr = 0.1
I0524 02:34:44.545346 11835 solver.cpp:239] Iteration 23950 (1.62925 iter/s, 6.13778s/10 iters), loss = 6.65671
I0524 02:34:44.545648 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65671 (* 1 = 6.65671 loss)
I0524 02:34:44.545701 11835 sgd_solver.cpp:112] Iteration 23950, lr = 0.1
I0524 02:34:51.346741 11835 solver.cpp:239] Iteration 23960 (1.47052 iter/s, 6.80033s/10 iters), loss = 5.84643
I0524 02:34:51.346791 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84643 (* 1 = 5.84643 loss)
I0524 02:34:51.346995 11835 sgd_solver.cpp:112] Iteration 23960, lr = 0.1
I0524 02:34:57.402390 11835 solver.cpp:239] Iteration 23970 (1.65143 iter/s, 6.05537s/10 iters), loss = 7.78901
I0524 02:34:57.402428 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.78901 (* 1 = 7.78901 loss)
I0524 02:34:57.402853 11835 sgd_solver.cpp:112] Iteration 23970, lr = 0.1
I0524 02:35:03.790230 11835 solver.cpp:239] Iteration 23980 (1.56555 iter/s, 6.38755s/10 iters), loss = 6.4514
I0524 02:35:03.790292 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4514 (* 1 = 6.4514 loss)
I0524 02:35:04.094797 11835 sgd_solver.cpp:112] Iteration 23980, lr = 0.1
I0524 02:35:11.038220 11835 solver.cpp:239] Iteration 23990 (1.37976 iter/s, 7.24766s/10 iters), loss = 6.72646
I0524 02:35:11.038264 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72646 (* 1 = 6.72646 loss)
I0524 02:35:11.038278 11835 sgd_solver.cpp:112] Iteration 23990, lr = 0.1
I0524 02:35:17.314898 11835 solver.cpp:239] Iteration 24000 (1.59329 iter/s, 6.27632s/10 iters), loss = 7.31744
I0524 02:35:17.315071 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.31744 (* 1 = 7.31744 loss)
I0524 02:35:17.500622 11835 sgd_solver.cpp:112] Iteration 24000, lr = 0.1
I0524 02:35:23.193178 11835 solver.cpp:239] Iteration 24010 (1.70129 iter/s, 5.87789s/10 iters), loss = 7.42552
I0524 02:35:23.193222 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.42552 (* 1 = 7.42552 loss)
I0524 02:35:23.289352 11835 sgd_solver.cpp:112] Iteration 24010, lr = 0.1
I0524 02:35:30.226235 11835 solver.cpp:239] Iteration 24020 (1.42192 iter/s, 7.03274s/10 iters), loss = 7.21075
I0524 02:35:30.226294 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.21075 (* 1 = 7.21075 loss)
I0524 02:35:30.226310 11835 sgd_solver.cpp:112] Iteration 24020, lr = 0.1
I0524 02:35:36.770447 11835 solver.cpp:239] Iteration 24030 (1.52817 iter/s, 6.54379s/10 iters), loss = 6.16793
I0524 02:35:36.770486 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16793 (* 1 = 6.16793 loss)
I0524 02:35:36.770625 11835 sgd_solver.cpp:112] Iteration 24030, lr = 0.1
I0524 02:35:42.724808 11835 solver.cpp:239] Iteration 24040 (1.67952 iter/s, 5.95408s/10 iters), loss = 6.91881
I0524 02:35:42.724854 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.91881 (* 1 = 6.91881 loss)
I0524 02:35:42.724974 11835 sgd_solver.cpp:112] Iteration 24040, lr = 0.1
I0524 02:35:48.338727 11835 solver.cpp:239] Iteration 24050 (1.78137 iter/s, 5.61364s/10 iters), loss = 6.32983
I0524 02:35:48.338980 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32983 (* 1 = 6.32983 loss)
I0524 02:35:48.339046 11835 sgd_solver.cpp:112] Iteration 24050, lr = 0.1
I0524 02:35:54.698115 11835 solver.cpp:239] Iteration 24060 (1.5726 iter/s, 6.35889s/10 iters), loss = 7.11928
I0524 02:35:54.698163 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11928 (* 1 = 7.11928 loss)
I0524 02:35:54.698177 11835 sgd_solver.cpp:112] Iteration 24060, lr = 0.1
I0524 02:36:01.129506 11835 solver.cpp:239] Iteration 24070 (1.55496 iter/s, 6.43103s/10 iters), loss = 7.38843
I0524 02:36:01.129547 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.38843 (* 1 = 7.38843 loss)
I0524 02:36:01.129740 11835 sgd_solver.cpp:112] Iteration 24070, lr = 0.1
I0524 02:36:07.250216 11835 solver.cpp:239] Iteration 24080 (1.63387 iter/s, 6.12042s/10 iters), loss = 6.69145
I0524 02:36:07.250263 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69145 (* 1 = 6.69145 loss)
I0524 02:36:07.250488 11835 sgd_solver.cpp:112] Iteration 24080, lr = 0.1
I0524 02:36:13.811144 11835 solver.cpp:239] Iteration 24090 (1.52424 iter/s, 6.56064s/10 iters), loss = 7.40666
I0524 02:36:13.811182 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.40666 (* 1 = 7.40666 loss)
I0524 02:36:13.811194 11835 sgd_solver.cpp:112] Iteration 24090, lr = 0.1
I0524 02:36:21.847196 11835 solver.cpp:239] Iteration 24100 (1.24446 iter/s, 8.03563s/10 iters), loss = 6.06718
I0524 02:36:21.847359 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06718 (* 1 = 6.06718 loss)
I0524 02:36:21.943737 11835 sgd_solver.cpp:112] Iteration 24100, lr = 0.1
I0524 02:36:27.849902 11835 solver.cpp:239] Iteration 24110 (1.66602 iter/s, 6.00232s/10 iters), loss = 7.16359
I0524 02:36:27.849959 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.16359 (* 1 = 7.16359 loss)
I0524 02:36:27.850208 11835 sgd_solver.cpp:112] Iteration 24110, lr = 0.1
I0524 02:36:33.418386 11835 solver.cpp:239] Iteration 24120 (1.79591 iter/s, 5.56822s/10 iters), loss = 6.50884
I0524 02:36:33.418426 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50884 (* 1 = 6.50884 loss)
I0524 02:36:33.418783 11835 sgd_solver.cpp:112] Iteration 24120, lr = 0.1
I0524 02:36:41.396806 11835 solver.cpp:239] Iteration 24130 (1.25344 iter/s, 7.97806s/10 iters), loss = 6.90792
I0524 02:36:41.396859 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90792 (* 1 = 6.90792 loss)
I0524 02:36:41.396924 11835 sgd_solver.cpp:112] Iteration 24130, lr = 0.1
I0524 02:36:48.664361 11835 solver.cpp:239] Iteration 24140 (1.37604 iter/s, 7.26722s/10 iters), loss = 6.23633
I0524 02:36:48.664415 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23633 (* 1 = 6.23633 loss)
I0524 02:36:48.664527 11835 sgd_solver.cpp:112] Iteration 24140, lr = 0.1
I0524 02:36:54.701181 11835 solver.cpp:239] Iteration 24150 (1.65658 iter/s, 6.03654s/10 iters), loss = 7.39394
I0524 02:36:54.701426 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.39394 (* 1 = 7.39394 loss)
I0524 02:36:55.030733 11835 sgd_solver.cpp:112] Iteration 24150, lr = 0.1
I0524 02:37:02.942997 11835 solver.cpp:239] Iteration 24160 (1.2134 iter/s, 8.24128s/10 iters), loss = 6.43236
I0524 02:37:02.943048 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43236 (* 1 = 6.43236 loss)
I0524 02:37:02.943135 11835 sgd_solver.cpp:112] Iteration 24160, lr = 0.1
I0524 02:37:08.624435 11835 solver.cpp:239] Iteration 24170 (1.7602 iter/s, 5.68118s/10 iters), loss = 5.94934
I0524 02:37:08.624475 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94934 (* 1 = 5.94934 loss)
I0524 02:37:08.624711 11835 sgd_solver.cpp:112] Iteration 24170, lr = 0.1
I0524 02:37:15.770557 11835 solver.cpp:239] Iteration 24180 (1.39942 iter/s, 7.1458s/10 iters), loss = 6.29952
I0524 02:37:15.770609 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29952 (* 1 = 6.29952 loss)
I0524 02:37:15.770634 11835 sgd_solver.cpp:112] Iteration 24180, lr = 0.1
I0524 02:37:22.640684 11835 solver.cpp:239] Iteration 24190 (1.45564 iter/s, 6.86981s/10 iters), loss = 7.25298
I0524 02:37:22.640740 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.25298 (* 1 = 7.25298 loss)
I0524 02:37:22.925487 11835 sgd_solver.cpp:112] Iteration 24190, lr = 0.1
I0524 02:37:28.913079 11835 solver.cpp:239] Iteration 24200 (1.59436 iter/s, 6.27211s/10 iters), loss = 6.82063
I0524 02:37:28.913343 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.82063 (* 1 = 6.82063 loss)
I0524 02:37:28.913395 11835 sgd_solver.cpp:112] Iteration 24200, lr = 0.1
I0524 02:37:34.530834 11835 solver.cpp:239] Iteration 24210 (1.7803 iter/s, 5.61702s/10 iters), loss = 7.03256
I0524 02:37:34.530890 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03256 (* 1 = 7.03256 loss)
I0524 02:37:34.531096 11835 sgd_solver.cpp:112] Iteration 24210, lr = 0.1
I0524 02:37:40.145767 11835 solver.cpp:239] Iteration 24220 (1.78105 iter/s, 5.61467s/10 iters), loss = 7.11569
I0524 02:37:40.145807 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11569 (* 1 = 7.11569 loss)
I0524 02:37:40.145817 11835 sgd_solver.cpp:112] Iteration 24220, lr = 0.1
I0524 02:37:46.039633 11835 solver.cpp:239] Iteration 24230 (1.69676 iter/s, 5.89359s/10 iters), loss = 6.39663
I0524 02:37:46.039683 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39663 (* 1 = 6.39663 loss)
I0524 02:37:46.039759 11835 sgd_solver.cpp:112] Iteration 24230, lr = 0.1
I0524 02:37:52.855994 11835 solver.cpp:239] Iteration 24240 (1.46712 iter/s, 6.81606s/10 iters), loss = 6.33726
I0524 02:37:52.856034 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33726 (* 1 = 6.33726 loss)
I0524 02:37:53.428602 11835 sgd_solver.cpp:112] Iteration 24240, lr = 0.1
I0524 02:38:00.159070 11835 solver.cpp:239] Iteration 24250 (1.36935 iter/s, 7.30275s/10 iters), loss = 7.41251
I0524 02:38:00.159250 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.41251 (* 1 = 7.41251 loss)
I0524 02:38:00.159358 11835 sgd_solver.cpp:112] Iteration 24250, lr = 0.1
I0524 02:38:06.880856 11835 solver.cpp:239] Iteration 24260 (1.4878 iter/s, 6.72135s/10 iters), loss = 6.43137
I0524 02:38:06.880909 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43137 (* 1 = 6.43137 loss)
I0524 02:38:06.881238 11835 sgd_solver.cpp:112] Iteration 24260, lr = 0.1
I0524 02:38:14.869424 11835 solver.cpp:239] Iteration 24270 (1.25185 iter/s, 7.98821s/10 iters), loss = 7.24242
I0524 02:38:14.869478 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.24242 (* 1 = 7.24242 loss)
I0524 02:38:15.273748 11835 sgd_solver.cpp:112] Iteration 24270, lr = 0.1
I0524 02:38:22.424782 11835 solver.cpp:239] Iteration 24280 (1.32362 iter/s, 7.55501s/10 iters), loss = 6.54972
I0524 02:38:22.424834 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54972 (* 1 = 6.54972 loss)
I0524 02:38:22.855690 11835 sgd_solver.cpp:112] Iteration 24280, lr = 0.1
I0524 02:38:30.342782 11835 solver.cpp:239] Iteration 24290 (1.263 iter/s, 7.91764s/10 iters), loss = 6.4933
I0524 02:38:30.343044 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4933 (* 1 = 6.4933 loss)
I0524 02:38:30.343097 11835 sgd_solver.cpp:112] Iteration 24290, lr = 0.1
I0524 02:38:38.148526 11835 solver.cpp:239] Iteration 24300 (1.28123 iter/s, 7.80498s/10 iters), loss = 6.38138
I0524 02:38:38.148564 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38138 (* 1 = 6.38138 loss)
I0524 02:38:38.776772 11835 sgd_solver.cpp:112] Iteration 24300, lr = 0.1
I0524 02:38:47.411351 11835 solver.cpp:239] Iteration 24310 (1.07963 iter/s, 9.26242s/10 iters), loss = 6.88046
I0524 02:38:47.411408 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88046 (* 1 = 6.88046 loss)
I0524 02:38:48.139117 11835 sgd_solver.cpp:112] Iteration 24310, lr = 0.1
I0524 02:38:57.450651 11835 solver.cpp:239] Iteration 24320 (0.996128 iter/s, 10.0389s/10 iters), loss = 7.52454
I0524 02:38:57.450731 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.52454 (* 1 = 7.52454 loss)
I0524 02:38:57.777986 11835 sgd_solver.cpp:112] Iteration 24320, lr = 0.1
I0524 02:39:05.053992 11835 solver.cpp:239] Iteration 24330 (1.31527 iter/s, 7.60299s/10 iters), loss = 7.01861
I0524 02:39:05.054103 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01861 (* 1 = 7.01861 loss)
I0524 02:39:05.054347 11835 sgd_solver.cpp:112] Iteration 24330, lr = 0.1
I0524 02:39:10.869488 11835 solver.cpp:239] Iteration 24340 (1.71964 iter/s, 5.81517s/10 iters), loss = 6.02152
I0524 02:39:10.869540 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02152 (* 1 = 6.02152 loss)
I0524 02:39:10.869557 11835 sgd_solver.cpp:112] Iteration 24340, lr = 0.1
I0524 02:39:17.315984 11835 solver.cpp:239] Iteration 24350 (1.55184 iter/s, 6.44398s/10 iters), loss = 6.64983
I0524 02:39:17.316028 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64983 (* 1 = 6.64983 loss)
I0524 02:39:17.316089 11835 sgd_solver.cpp:112] Iteration 24350, lr = 0.1
I0524 02:39:23.464872 11835 solver.cpp:239] Iteration 24360 (1.62639 iter/s, 6.1486s/10 iters), loss = 7.12342
I0524 02:39:23.464922 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.12342 (* 1 = 7.12342 loss)
I0524 02:39:24.106294 11835 sgd_solver.cpp:112] Iteration 24360, lr = 0.1
I0524 02:39:30.500182 11835 solver.cpp:239] Iteration 24370 (1.42147 iter/s, 7.03499s/10 iters), loss = 7.9899
I0524 02:39:30.500238 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.9899 (* 1 = 7.9899 loss)
I0524 02:39:30.500262 11835 sgd_solver.cpp:112] Iteration 24370, lr = 0.1
I0524 02:39:37.473161 11835 solver.cpp:239] Iteration 24380 (1.43422 iter/s, 6.97244s/10 iters), loss = 6.87827
I0524 02:39:37.473521 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87827 (* 1 = 6.87827 loss)
I0524 02:39:37.473585 11835 sgd_solver.cpp:112] Iteration 24380, lr = 0.1
I0524 02:39:44.362056 11835 solver.cpp:239] Iteration 24390 (1.45193 iter/s, 6.88736s/10 iters), loss = 6.63985
I0524 02:39:44.362109 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63985 (* 1 = 6.63985 loss)
I0524 02:39:44.370771 11835 sgd_solver.cpp:112] Iteration 24390, lr = 0.1
I0524 02:39:51.893299 11835 solver.cpp:239] Iteration 24400 (1.32786 iter/s, 7.5309s/10 iters), loss = 5.59393
I0524 02:39:51.893352 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.59393 (* 1 = 5.59393 loss)
I0524 02:39:52.679642 11835 sgd_solver.cpp:112] Iteration 24400, lr = 0.1
I0524 02:40:00.149837 11835 solver.cpp:239] Iteration 24410 (1.21121 iter/s, 8.25617s/10 iters), loss = 6.13168
I0524 02:40:00.149883 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13168 (* 1 = 6.13168 loss)
I0524 02:40:00.166718 11835 sgd_solver.cpp:112] Iteration 24410, lr = 0.1
I0524 02:40:05.798813 11835 solver.cpp:239] Iteration 24420 (1.77032 iter/s, 5.6487s/10 iters), loss = 7.24227
I0524 02:40:05.798871 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.24227 (* 1 = 7.24227 loss)
I0524 02:40:05.799033 11835 sgd_solver.cpp:112] Iteration 24420, lr = 0.1
I0524 02:40:14.234431 11835 solver.cpp:239] Iteration 24430 (1.1855 iter/s, 8.43523s/10 iters), loss = 7.24163
I0524 02:40:14.234743 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.24163 (* 1 = 7.24163 loss)
I0524 02:40:14.641366 11835 sgd_solver.cpp:112] Iteration 24430, lr = 0.1
I0524 02:40:22.408742 11835 solver.cpp:239] Iteration 24440 (1.22342 iter/s, 8.17378s/10 iters), loss = 6.40082
I0524 02:40:22.408789 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40082 (* 1 = 6.40082 loss)
I0524 02:40:22.409145 11835 sgd_solver.cpp:112] Iteration 24440, lr = 0.1
I0524 02:40:28.332005 11835 solver.cpp:239] Iteration 24450 (1.68834 iter/s, 5.92298s/10 iters), loss = 6.76801
I0524 02:40:28.332057 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76801 (* 1 = 6.76801 loss)
I0524 02:40:28.390329 11835 sgd_solver.cpp:112] Iteration 24450, lr = 0.1
I0524 02:40:35.106106 11835 solver.cpp:239] Iteration 24460 (1.47628 iter/s, 6.77378s/10 iters), loss = 5.46514
I0524 02:40:35.106159 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.46514 (* 1 = 5.46514 loss)
I0524 02:40:35.526234 11835 sgd_solver.cpp:112] Iteration 24460, lr = 0.1
I0524 02:40:42.958884 11835 solver.cpp:239] Iteration 24470 (1.27349 iter/s, 7.85243s/10 iters), loss = 7.0498
I0524 02:40:42.958925 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.0498 (* 1 = 7.0498 loss)
I0524 02:40:42.959053 11835 sgd_solver.cpp:112] Iteration 24470, lr = 0.1
I0524 02:40:49.147429 11835 solver.cpp:239] Iteration 24480 (1.61596 iter/s, 6.18826s/10 iters), loss = 6.80883
I0524 02:40:49.147730 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80883 (* 1 = 6.80883 loss)
I0524 02:40:49.147810 11835 sgd_solver.cpp:112] Iteration 24480, lr = 0.1
I0524 02:40:56.116322 11835 solver.cpp:239] Iteration 24490 (1.43508 iter/s, 6.96826s/10 iters), loss = 6.4887
I0524 02:40:56.116356 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4887 (* 1 = 6.4887 loss)
I0524 02:40:56.408771 11835 sgd_solver.cpp:112] Iteration 24490, lr = 0.1
I0524 02:41:02.460080 11835 solver.cpp:239] Iteration 24500 (1.57642 iter/s, 6.34347s/10 iters), loss = 6.23014
I0524 02:41:02.460135 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23014 (* 1 = 6.23014 loss)
I0524 02:41:02.460152 11835 sgd_solver.cpp:112] Iteration 24500, lr = 0.1
I0524 02:41:09.518002 11835 solver.cpp:239] Iteration 24510 (1.41693 iter/s, 7.05753s/10 iters), loss = 8.07478
I0524 02:41:09.518052 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.07478 (* 1 = 8.07478 loss)
I0524 02:41:09.552731 11835 sgd_solver.cpp:112] Iteration 24510, lr = 0.1
I0524 02:41:15.515696 11835 solver.cpp:239] Iteration 24520 (1.66738 iter/s, 5.99742s/10 iters), loss = 6.3635
I0524 02:41:15.515741 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3635 (* 1 = 6.3635 loss)
I0524 02:41:15.516129 11835 sgd_solver.cpp:112] Iteration 24520, lr = 0.1
I0524 02:41:23.564704 11835 solver.cpp:239] Iteration 24530 (1.24244 iter/s, 8.04865s/10 iters), loss = 6.46348
I0524 02:41:23.564997 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46348 (* 1 = 6.46348 loss)
I0524 02:41:23.715750 11835 sgd_solver.cpp:112] Iteration 24530, lr = 0.1
I0524 02:41:31.139287 11835 solver.cpp:239] Iteration 24540 (1.3203 iter/s, 7.57405s/10 iters), loss = 5.93263
I0524 02:41:31.139339 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93263 (* 1 = 5.93263 loss)
I0524 02:41:31.865716 11835 sgd_solver.cpp:112] Iteration 24540, lr = 0.1
I0524 02:41:40.340322 11835 solver.cpp:239] Iteration 24550 (1.08688 iter/s, 9.20064s/10 iters), loss = 6.15331
I0524 02:41:40.340371 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15331 (* 1 = 6.15331 loss)
I0524 02:41:40.340385 11835 sgd_solver.cpp:112] Iteration 24550, lr = 0.1
I0524 02:41:46.151865 11835 solver.cpp:239] Iteration 24560 (1.72112 iter/s, 5.81015s/10 iters), loss = 7.01177
I0524 02:41:46.151913 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01177 (* 1 = 7.01177 loss)
I0524 02:41:46.151933 11835 sgd_solver.cpp:112] Iteration 24560, lr = 0.1
I0524 02:41:53.273999 11835 solver.cpp:239] Iteration 24570 (1.40415 iter/s, 7.12174s/10 iters), loss = 5.97345
I0524 02:41:53.274055 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97345 (* 1 = 5.97345 loss)
I0524 02:41:53.274072 11835 sgd_solver.cpp:112] Iteration 24570, lr = 0.1
I0524 02:41:59.316658 11835 solver.cpp:239] Iteration 24580 (1.655 iter/s, 6.04229s/10 iters), loss = 7.19792
I0524 02:41:59.316928 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.19792 (* 1 = 7.19792 loss)
I0524 02:41:59.316985 11835 sgd_solver.cpp:112] Iteration 24580, lr = 0.1
I0524 02:42:04.892484 11835 solver.cpp:239] Iteration 24590 (1.7939 iter/s, 5.57444s/10 iters), loss = 6.38519
I0524 02:42:04.892532 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38519 (* 1 = 6.38519 loss)
I0524 02:42:04.943769 11835 sgd_solver.cpp:112] Iteration 24590, lr = 0.1
I0524 02:42:11.558946 11835 solver.cpp:239] Iteration 24600 (1.50011 iter/s, 6.66616s/10 iters), loss = 6.78017
I0524 02:42:11.558997 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78017 (* 1 = 6.78017 loss)
I0524 02:42:11.559062 11835 sgd_solver.cpp:112] Iteration 24600, lr = 0.1
I0524 02:42:18.818681 11835 solver.cpp:239] Iteration 24610 (1.37753 iter/s, 7.25939s/10 iters), loss = 7.79141
I0524 02:42:18.818761 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.79141 (* 1 = 7.79141 loss)
I0524 02:42:19.325062 11835 sgd_solver.cpp:112] Iteration 24610, lr = 0.1
I0524 02:42:25.634269 11835 solver.cpp:239] Iteration 24620 (1.4673 iter/s, 6.81523s/10 iters), loss = 6.0554
I0524 02:42:25.634326 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0554 (* 1 = 6.0554 loss)
I0524 02:42:25.634718 11835 sgd_solver.cpp:112] Iteration 24620, lr = 0.1
I0524 02:42:33.029966 11835 solver.cpp:239] Iteration 24630 (1.3522 iter/s, 7.39537s/10 iters), loss = 7.1005
I0524 02:42:33.030218 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.1005 (* 1 = 7.1005 loss)
I0524 02:42:33.030258 11835 sgd_solver.cpp:112] Iteration 24630, lr = 0.1
I0524 02:42:39.382350 11835 solver.cpp:239] Iteration 24640 (1.57433 iter/s, 6.35191s/10 iters), loss = 6.81985
I0524 02:42:39.382402 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81985 (* 1 = 6.81985 loss)
I0524 02:42:39.382418 11835 sgd_solver.cpp:112] Iteration 24640, lr = 0.1
I0524 02:42:45.128471 11835 solver.cpp:239] Iteration 24650 (1.74039 iter/s, 5.74585s/10 iters), loss = 7.17101
I0524 02:42:45.128518 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.17101 (* 1 = 7.17101 loss)
I0524 02:42:45.128679 11835 sgd_solver.cpp:112] Iteration 24650, lr = 0.1
I0524 02:42:51.615458 11835 solver.cpp:239] Iteration 24660 (1.54162 iter/s, 6.48668s/10 iters), loss = 6.83704
I0524 02:42:51.615523 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83704 (* 1 = 6.83704 loss)
I0524 02:42:51.615667 11835 sgd_solver.cpp:112] Iteration 24660, lr = 0.1
I0524 02:42:57.876888 11835 solver.cpp:239] Iteration 24670 (1.59716 iter/s, 6.26113s/10 iters), loss = 7.41085
I0524 02:42:57.876945 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.41085 (* 1 = 7.41085 loss)
I0524 02:42:58.027534 11835 sgd_solver.cpp:112] Iteration 24670, lr = 0.1
I0524 02:43:04.621333 11835 solver.cpp:239] Iteration 24680 (1.48277 iter/s, 6.74413s/10 iters), loss = 6.09493
I0524 02:43:04.621651 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09493 (* 1 = 6.09493 loss)
I0524 02:43:04.637960 11835 sgd_solver.cpp:112] Iteration 24680, lr = 0.1
I0524 02:43:12.545392 11835 solver.cpp:239] Iteration 24690 (1.26207 iter/s, 7.92352s/10 iters), loss = 5.77645
I0524 02:43:12.545430 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.77645 (* 1 = 5.77645 loss)
I0524 02:43:12.545570 11835 sgd_solver.cpp:112] Iteration 24690, lr = 0.1
I0524 02:43:19.612413 11835 solver.cpp:239] Iteration 24700 (1.41509 iter/s, 7.0667s/10 iters), loss = 7.06035
I0524 02:43:19.612475 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.06035 (* 1 = 7.06035 loss)
I0524 02:43:19.612713 11835 sgd_solver.cpp:112] Iteration 24700, lr = 0.1
I0524 02:43:25.510284 11835 solver.cpp:239] Iteration 24710 (1.69561 iter/s, 5.89759s/10 iters), loss = 6.56897
I0524 02:43:25.510324 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56897 (* 1 = 6.56897 loss)
I0524 02:43:25.861038 11835 sgd_solver.cpp:112] Iteration 24710, lr = 0.1
I0524 02:43:33.139269 11835 solver.cpp:239] Iteration 24720 (1.31085 iter/s, 7.62864s/10 iters), loss = 6.62141
I0524 02:43:33.139319 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62141 (* 1 = 6.62141 loss)
I0524 02:43:33.669522 11835 sgd_solver.cpp:112] Iteration 24720, lr = 0.1
I0524 02:43:40.250628 11835 solver.cpp:239] Iteration 24730 (1.40627 iter/s, 7.11103s/10 iters), loss = 7.42375
I0524 02:43:40.250874 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.42375 (* 1 = 7.42375 loss)
I0524 02:43:40.250918 11835 sgd_solver.cpp:112] Iteration 24730, lr = 0.1
I0524 02:43:47.614132 11835 solver.cpp:239] Iteration 24740 (1.3582 iter/s, 7.36267s/10 iters), loss = 6.45308
I0524 02:43:47.614187 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45308 (* 1 = 6.45308 loss)
I0524 02:43:47.614285 11835 sgd_solver.cpp:112] Iteration 24740, lr = 0.1
I0524 02:43:54.267536 11835 solver.cpp:239] Iteration 24750 (1.50306 iter/s, 6.65309s/10 iters), loss = 6.15094
I0524 02:43:54.267587 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15094 (* 1 = 6.15094 loss)
I0524 02:43:54.804232 11835 sgd_solver.cpp:112] Iteration 24750, lr = 0.1
I0524 02:44:01.524425 11835 solver.cpp:239] Iteration 24760 (1.37806 iter/s, 7.25656s/10 iters), loss = 6.93567
I0524 02:44:01.524472 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93567 (* 1 = 6.93567 loss)
I0524 02:44:01.524487 11835 sgd_solver.cpp:112] Iteration 24760, lr = 0.1
I0524 02:44:10.969264 11835 solver.cpp:239] Iteration 24770 (1.05883 iter/s, 9.44443s/10 iters), loss = 8.03999
I0524 02:44:10.969653 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.03999 (* 1 = 8.03999 loss)
I0524 02:44:11.332499 11835 sgd_solver.cpp:112] Iteration 24770, lr = 0.1
I0524 02:44:20.330934 11835 solver.cpp:239] Iteration 24780 (1.06826 iter/s, 9.36099s/10 iters), loss = 6.90484
I0524 02:44:20.330979 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90484 (* 1 = 6.90484 loss)
I0524 02:44:20.331173 11835 sgd_solver.cpp:112] Iteration 24780, lr = 0.1
I0524 02:44:26.275247 11835 solver.cpp:239] Iteration 24790 (1.68236 iter/s, 5.94403s/10 iters), loss = 6.75596
I0524 02:44:26.275296 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.75596 (* 1 = 6.75596 loss)
I0524 02:44:26.275878 11835 sgd_solver.cpp:112] Iteration 24790, lr = 0.1
I0524 02:44:32.635937 11835 solver.cpp:239] Iteration 24800 (1.57223 iter/s, 6.36039s/10 iters), loss = 6.61636
I0524 02:44:32.635993 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61636 (* 1 = 6.61636 loss)
I0524 02:44:32.636034 11835 sgd_solver.cpp:112] Iteration 24800, lr = 0.1
I0524 02:44:41.509569 11835 solver.cpp:239] Iteration 24810 (1.12698 iter/s, 8.87325s/10 iters), loss = 6.51935
I0524 02:44:41.509697 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51935 (* 1 = 6.51935 loss)
I0524 02:44:41.529028 11835 sgd_solver.cpp:112] Iteration 24810, lr = 0.1
I0524 02:44:48.771872 11835 solver.cpp:239] Iteration 24820 (1.37705 iter/s, 7.26189s/10 iters), loss = 7.15123
I0524 02:44:48.771929 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.15123 (* 1 = 7.15123 loss)
I0524 02:44:48.772009 11835 sgd_solver.cpp:112] Iteration 24820, lr = 0.1
I0524 02:44:54.741258 11835 solver.cpp:239] Iteration 24830 (1.6753 iter/s, 5.96909s/10 iters), loss = 6.28466
I0524 02:44:54.741313 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.28466 (* 1 = 6.28466 loss)
I0524 02:44:54.741533 11835 sgd_solver.cpp:112] Iteration 24830, lr = 0.1
I0524 02:45:00.806090 11835 solver.cpp:239] Iteration 24840 (1.64893 iter/s, 6.06455s/10 iters), loss = 6.88043
I0524 02:45:00.806128 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88043 (* 1 = 6.88043 loss)
I0524 02:45:00.806140 11835 sgd_solver.cpp:112] Iteration 24840, lr = 0.1
I0524 02:45:08.483240 11835 solver.cpp:239] Iteration 24850 (1.30269 iter/s, 7.67641s/10 iters), loss = 6.6912
I0524 02:45:08.483297 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6912 (* 1 = 6.6912 loss)
I0524 02:45:08.483373 11835 sgd_solver.cpp:112] Iteration 24850, lr = 0.1
I0524 02:45:15.505698 11835 solver.cpp:239] Iteration 24860 (1.42407 iter/s, 7.02214s/10 iters), loss = 6.6008
I0524 02:45:15.505833 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6008 (* 1 = 6.6008 loss)
I0524 02:45:15.739167 11835 sgd_solver.cpp:112] Iteration 24860, lr = 0.1
I0524 02:45:22.464023 11835 solver.cpp:239] Iteration 24870 (1.43721 iter/s, 6.95792s/10 iters), loss = 6.4516
I0524 02:45:22.464076 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4516 (* 1 = 6.4516 loss)
I0524 02:45:22.464092 11835 sgd_solver.cpp:112] Iteration 24870, lr = 0.1
I0524 02:45:29.701441 11835 solver.cpp:239] Iteration 24880 (1.38177 iter/s, 7.23708s/10 iters), loss = 7.05637
I0524 02:45:29.701493 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.05637 (* 1 = 7.05637 loss)
I0524 02:45:30.917170 11835 sgd_solver.cpp:112] Iteration 24880, lr = 0.1
I0524 02:45:37.501941 11835 solver.cpp:239] Iteration 24890 (1.28203 iter/s, 7.80015s/10 iters), loss = 6.85647
I0524 02:45:37.501991 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.85647 (* 1 = 6.85647 loss)
I0524 02:45:37.502004 11835 sgd_solver.cpp:112] Iteration 24890, lr = 0.1
I0524 02:45:44.128942 11835 solver.cpp:239] Iteration 24900 (1.50908 iter/s, 6.62655s/10 iters), loss = 6.40495
I0524 02:45:44.128983 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40495 (* 1 = 6.40495 loss)
I0524 02:45:44.128996 11835 sgd_solver.cpp:112] Iteration 24900, lr = 0.1
I0524 02:45:50.718736 11835 solver.cpp:239] Iteration 24910 (1.51783 iter/s, 6.58836s/10 iters), loss = 6.62584
I0524 02:45:50.719055 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62584 (* 1 = 6.62584 loss)
I0524 02:45:51.920166 11835 sgd_solver.cpp:112] Iteration 24910, lr = 0.1
I0524 02:45:59.114389 11835 solver.cpp:239] Iteration 24920 (1.19118 iter/s, 8.39506s/10 iters), loss = 6.91719
I0524 02:45:59.114442 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.91719 (* 1 = 6.91719 loss)
I0524 02:45:59.114557 11835 sgd_solver.cpp:112] Iteration 24920, lr = 0.1
I0524 02:46:06.224030 11835 solver.cpp:239] Iteration 24930 (1.40661 iter/s, 7.10931s/10 iters), loss = 6.21883
I0524 02:46:06.224084 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21883 (* 1 = 6.21883 loss)
I0524 02:46:06.522286 11835 sgd_solver.cpp:112] Iteration 24930, lr = 0.1
I0524 02:46:14.766671 11835 solver.cpp:239] Iteration 24940 (1.17065 iter/s, 8.54226s/10 iters), loss = 6.79195
I0524 02:46:14.766764 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79195 (* 1 = 6.79195 loss)
I0524 02:46:14.830885 11835 sgd_solver.cpp:112] Iteration 24940, lr = 0.1
I0524 02:46:21.200687 11835 solver.cpp:239] Iteration 24950 (1.55432 iter/s, 6.43368s/10 iters), loss = 7.1849
I0524 02:46:21.200937 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.1849 (* 1 = 7.1849 loss)
I0524 02:46:21.218129 11835 sgd_solver.cpp:112] Iteration 24950, lr = 0.1
I0524 02:46:29.522861 11835 solver.cpp:239] Iteration 24960 (1.20169 iter/s, 8.32163s/10 iters), loss = 7.40587
I0524 02:46:29.522929 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.40587 (* 1 = 7.40587 loss)
I0524 02:46:29.523133 11835 sgd_solver.cpp:112] Iteration 24960, lr = 0.1
I0524 02:46:36.194859 11835 solver.cpp:239] Iteration 24970 (1.49887 iter/s, 6.67168s/10 iters), loss = 5.90382
I0524 02:46:36.194907 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90382 (* 1 = 5.90382 loss)
I0524 02:46:36.194922 11835 sgd_solver.cpp:112] Iteration 24970, lr = 0.1
I0524 02:46:43.858925 11835 solver.cpp:239] Iteration 24980 (1.30485 iter/s, 7.66373s/10 iters), loss = 6.98184
I0524 02:46:43.858973 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98184 (* 1 = 6.98184 loss)
I0524 02:46:44.140110 11835 sgd_solver.cpp:112] Iteration 24980, lr = 0.1
I0524 02:46:51.384295 11835 solver.cpp:239] Iteration 24990 (1.3289 iter/s, 7.52504s/10 iters), loss = 6.90159
I0524 02:46:51.384593 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90159 (* 1 = 6.90159 loss)
I0524 02:46:51.384658 11835 sgd_solver.cpp:112] Iteration 24990, lr = 0.1
I0524 02:46:56.757531 11835 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_25000.caffemodel
I0524 02:46:58.241463 11835 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_25000.solverstate
I0524 02:46:58.798724 11835 solver.cpp:239] Iteration 25000 (1.34901 iter/s, 7.41287s/10 iters), loss = 7.11332
I0524 02:46:58.798758 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11332 (* 1 = 7.11332 loss)
I0524 02:46:58.938408 11835 sgd_solver.cpp:112] Iteration 25000, lr = 0.1
I0524 02:47:06.193864 11835 solver.cpp:239] Iteration 25010 (1.3523 iter/s, 7.39481s/10 iters), loss = 6.27761
I0524 02:47:06.193917 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27761 (* 1 = 6.27761 loss)
I0524 02:47:06.193931 11835 sgd_solver.cpp:112] Iteration 25010, lr = 0.1
I0524 02:47:13.513984 11835 solver.cpp:239] Iteration 25020 (1.36637 iter/s, 7.31868s/10 iters), loss = 7.18364
I0524 02:47:13.514034 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.18364 (* 1 = 7.18364 loss)
I0524 02:47:13.514338 11835 sgd_solver.cpp:112] Iteration 25020, lr = 0.1
I0524 02:47:20.527154 11835 solver.cpp:239] Iteration 25030 (1.42595 iter/s, 7.01285s/10 iters), loss = 7.48778
I0524 02:47:20.527201 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.48778 (* 1 = 7.48778 loss)
I0524 02:47:20.883972 11835 sgd_solver.cpp:112] Iteration 25030, lr = 0.1
I0524 02:47:29.017874 11835 solver.cpp:239] Iteration 25040 (1.17781 iter/s, 8.49034s/10 iters), loss = 6.98262
I0524 02:47:29.018054 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98262 (* 1 = 6.98262 loss)
I0524 02:47:30.014873 11835 sgd_solver.cpp:112] Iteration 25040, lr = 0.1
I0524 02:47:37.440822 11835 solver.cpp:239] Iteration 25050 (1.1873 iter/s, 8.42245s/10 iters), loss = 6.26925
I0524 02:47:37.440871 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26925 (* 1 = 6.26925 loss)
I0524 02:47:37.441063 11835 sgd_solver.cpp:112] Iteration 25050, lr = 0.1
I0524 02:47:44.303700 11835 solver.cpp:239] Iteration 25060 (1.45718 iter/s, 6.86256s/10 iters), loss = 6.74942
I0524 02:47:44.303757 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74942 (* 1 = 6.74942 loss)
I0524 02:47:44.303835 11835 sgd_solver.cpp:112] Iteration 25060, lr = 0.1
I0524 02:47:51.102308 11835 solver.cpp:239] Iteration 25070 (1.47096 iter/s, 6.79829s/10 iters), loss = 7.15757
I0524 02:47:51.102363 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.15757 (* 1 = 7.15757 loss)
I0524 02:47:51.102634 11835 sgd_solver.cpp:112] Iteration 25070, lr = 0.1
I0524 02:47:57.357944 11835 solver.cpp:239] Iteration 25080 (1.59863 iter/s, 6.25535s/10 iters), loss = 5.79631
I0524 02:47:57.357991 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79631 (* 1 = 5.79631 loss)
I0524 02:47:57.358108 11835 sgd_solver.cpp:112] Iteration 25080, lr = 0.1
I0524 02:48:03.768818 11835 solver.cpp:239] Iteration 25090 (1.55992 iter/s, 6.41058s/10 iters), loss = 6.90277
I0524 02:48:03.768930 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90277 (* 1 = 6.90277 loss)
I0524 02:48:03.769212 11835 sgd_solver.cpp:112] Iteration 25090, lr = 0.1
I0524 02:48:09.556798 11835 solver.cpp:239] Iteration 25100 (1.72782 iter/s, 5.78764s/10 iters), loss = 6.69743
I0524 02:48:09.556846 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69743 (* 1 = 6.69743 loss)
I0524 02:48:09.556860 11835 sgd_solver.cpp:112] Iteration 25100, lr = 0.1
I0524 02:48:15.522536 11835 solver.cpp:239] Iteration 25110 (1.67649 iter/s, 5.96485s/10 iters), loss = 6.99564
I0524 02:48:15.522578 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.99564 (* 1 = 6.99564 loss)
I0524 02:48:16.298894 11835 sgd_solver.cpp:112] Iteration 25110, lr = 0.1
I0524 02:48:22.377360 11835 solver.cpp:239] Iteration 25120 (1.45889 iter/s, 6.85451s/10 iters), loss = 5.84641
I0524 02:48:22.377411 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84641 (* 1 = 5.84641 loss)
I0524 02:48:23.326297 11835 sgd_solver.cpp:112] Iteration 25120, lr = 0.1
I0524 02:48:30.011862 11835 solver.cpp:239] Iteration 25130 (1.3099 iter/s, 7.63415s/10 iters), loss = 6.93754
I0524 02:48:30.011919 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93754 (* 1 = 6.93754 loss)
I0524 02:48:30.012035 11835 sgd_solver.cpp:112] Iteration 25130, lr = 0.1
I0524 02:48:35.966840 11835 solver.cpp:239] Iteration 25140 (1.67934 iter/s, 5.9547s/10 iters), loss = 6.42302
I0524 02:48:35.966985 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42302 (* 1 = 6.42302 loss)
I0524 02:48:35.967003 11835 sgd_solver.cpp:112] Iteration 25140, lr = 0.1
I0524 02:48:42.283793 11835 solver.cpp:239] Iteration 25150 (1.58315 iter/s, 6.31653s/10 iters), loss = 6.76754
I0524 02:48:42.283843 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76754 (* 1 = 6.76754 loss)
I0524 02:48:42.283859 11835 sgd_solver.cpp:112] Iteration 25150, lr = 0.1
I0524 02:48:49.187041 11835 solver.cpp:239] Iteration 25160 (1.4487 iter/s, 6.90273s/10 iters), loss = 6.47999
I0524 02:48:49.187095 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.47999 (* 1 = 6.47999 loss)
I0524 02:48:49.187163 11835 sgd_solver.cpp:112] Iteration 25160, lr = 0.1
I0524 02:48:55.195909 11835 solver.cpp:239] Iteration 25170 (1.66428 iter/s, 6.00859s/10 iters), loss = 7.35297
I0524 02:48:55.195957 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.35297 (* 1 = 7.35297 loss)
I0524 02:48:55.195978 11835 sgd_solver.cpp:112] Iteration 25170, lr = 0.1
I0524 02:49:03.074607 11835 solver.cpp:239] Iteration 25180 (1.26931 iter/s, 7.87828s/10 iters), loss = 6.86544
I0524 02:49:03.074668 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86544 (* 1 = 6.86544 loss)
I0524 02:49:03.074767 11835 sgd_solver.cpp:112] Iteration 25180, lr = 0.1
I0524 02:49:08.641378 11835 solver.cpp:239] Iteration 25190 (1.79646 iter/s, 5.5665s/10 iters), loss = 5.53542
I0524 02:49:08.641600 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.53542 (* 1 = 5.53542 loss)
I0524 02:49:08.641630 11835 sgd_solver.cpp:112] Iteration 25190, lr = 0.1
I0524 02:49:14.581187 11835 solver.cpp:239] Iteration 25200 (1.68369 iter/s, 5.93935s/10 iters), loss = 6.8236
I0524 02:49:14.581238 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8236 (* 1 = 6.8236 loss)
I0524 02:49:15.708217 11835 sgd_solver.cpp:112] Iteration 25200, lr = 0.1
I0524 02:49:22.457324 11835 solver.cpp:239] Iteration 25210 (1.26971 iter/s, 7.87578s/10 iters), loss = 6.08201
I0524 02:49:22.457394 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08201 (* 1 = 6.08201 loss)
I0524 02:49:22.585621 11835 sgd_solver.cpp:112] Iteration 25210, lr = 0.1
I0524 02:49:29.742092 11835 solver.cpp:239] Iteration 25220 (1.37279 iter/s, 7.28442s/10 iters), loss = 6.9899
I0524 02:49:29.742138 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9899 (* 1 = 6.9899 loss)
I0524 02:49:29.742278 11835 sgd_solver.cpp:112] Iteration 25220, lr = 0.1
I0524 02:49:35.950798 11835 solver.cpp:239] Iteration 25230 (1.61072 iter/s, 6.20841s/10 iters), loss = 6.47813
I0524 02:49:35.950853 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.47813 (* 1 = 6.47813 loss)
I0524 02:49:35.950969 11835 sgd_solver.cpp:112] Iteration 25230, lr = 0.1
I0524 02:49:41.924651 11835 solver.cpp:239] Iteration 25240 (1.67404 iter/s, 5.97358s/10 iters), loss = 7.15245
I0524 02:49:41.924741 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.15245 (* 1 = 7.15245 loss)
I0524 02:49:41.924754 11835 sgd_solver.cpp:112] Iteration 25240, lr = 0.1
I0524 02:49:48.799393 11835 solver.cpp:239] Iteration 25250 (1.45469 iter/s, 6.87432s/10 iters), loss = 6.551
I0524 02:49:48.799455 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.551 (* 1 = 6.551 loss)
I0524 02:49:48.799700 11835 sgd_solver.cpp:112] Iteration 25250, lr = 0.1
I0524 02:49:56.474728 11835 solver.cpp:239] Iteration 25260 (1.30294 iter/s, 7.67496s/10 iters), loss = 7.06925
I0524 02:49:56.474781 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.06925 (* 1 = 7.06925 loss)
I0524 02:49:56.474982 11835 sgd_solver.cpp:112] Iteration 25260, lr = 0.1
I0524 02:50:02.376394 11835 solver.cpp:239] Iteration 25270 (1.69452 iter/s, 5.90138s/10 iters), loss = 7.31945
I0524 02:50:02.376469 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.31945 (* 1 = 7.31945 loss)
I0524 02:50:02.376499 11835 sgd_solver.cpp:112] Iteration 25270, lr = 0.1
I0524 02:50:09.136242 11835 solver.cpp:239] Iteration 25280 (1.47939 iter/s, 6.75954s/10 iters), loss = 6.35209
I0524 02:50:09.136281 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35209 (* 1 = 6.35209 loss)
I0524 02:50:10.261545 11835 sgd_solver.cpp:112] Iteration 25280, lr = 0.1
I0524 02:50:16.856611 11835 solver.cpp:239] Iteration 25290 (1.29533 iter/s, 7.72002s/10 iters), loss = 6.74765
I0524 02:50:16.856928 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74765 (* 1 = 6.74765 loss)
I0524 02:50:16.856992 11835 sgd_solver.cpp:112] Iteration 25290, lr = 0.1
I0524 02:50:23.665783 11835 solver.cpp:239] Iteration 25300 (1.46886 iter/s, 6.80802s/10 iters), loss = 7.25399
I0524 02:50:23.665838 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.25399 (* 1 = 7.25399 loss)
I0524 02:50:23.666244 11835 sgd_solver.cpp:112] Iteration 25300, lr = 0.1
I0524 02:50:30.619117 11835 solver.cpp:239] Iteration 25310 (1.43823 iter/s, 6.95301s/10 iters), loss = 6.19421
I0524 02:50:30.619170 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19421 (* 1 = 6.19421 loss)
I0524 02:50:30.619295 11835 sgd_solver.cpp:112] Iteration 25310, lr = 0.1
I0524 02:50:38.940348 11835 solver.cpp:239] Iteration 25320 (1.2018 iter/s, 8.32086s/10 iters), loss = 7.13879
I0524 02:50:38.940398 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13879 (* 1 = 7.13879 loss)
I0524 02:50:38.940523 11835 sgd_solver.cpp:112] Iteration 25320, lr = 0.1
I0524 02:50:46.771020 11835 solver.cpp:239] Iteration 25330 (1.27709 iter/s, 7.83033s/10 iters), loss = 6.90069
I0524 02:50:46.771065 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90069 (* 1 = 6.90069 loss)
I0524 02:50:46.771284 11835 sgd_solver.cpp:112] Iteration 25330, lr = 0.1
I0524 02:50:52.331149 11835 solver.cpp:239] Iteration 25340 (1.7986 iter/s, 5.55987s/10 iters), loss = 6.21099
I0524 02:50:52.331288 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21099 (* 1 = 6.21099 loss)
I0524 02:50:52.331305 11835 sgd_solver.cpp:112] Iteration 25340, lr = 0.1
I0524 02:50:59.089043 11835 solver.cpp:239] Iteration 25350 (1.47985 iter/s, 6.75746s/10 iters), loss = 5.90282
I0524 02:50:59.089094 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90282 (* 1 = 5.90282 loss)
I0524 02:50:59.089109 11835 sgd_solver.cpp:112] Iteration 25350, lr = 0.1
I0524 02:51:05.275384 11835 solver.cpp:239] Iteration 25360 (1.61654 iter/s, 6.18605s/10 iters), loss = 6.36574
I0524 02:51:05.275432 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36574 (* 1 = 6.36574 loss)
I0524 02:51:05.275446 11835 sgd_solver.cpp:112] Iteration 25360, lr = 0.1
I0524 02:51:12.058387 11835 solver.cpp:239] Iteration 25370 (1.47458 iter/s, 6.78159s/10 iters), loss = 7.30905
I0524 02:51:12.058424 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.30905 (* 1 = 7.30905 loss)
I0524 02:51:12.058732 11835 sgd_solver.cpp:112] Iteration 25370, lr = 0.1
I0524 02:51:18.889981 11835 solver.cpp:239] Iteration 25380 (1.46385 iter/s, 6.83129s/10 iters), loss = 7.45375
I0524 02:51:18.890033 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.45375 (* 1 = 7.45375 loss)
I0524 02:51:19.383399 11835 sgd_solver.cpp:112] Iteration 25380, lr = 0.1
I0524 02:51:26.541676 11835 solver.cpp:239] Iteration 25390 (1.30696 iter/s, 7.65136s/10 iters), loss = 7.06249
I0524 02:51:26.541801 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.06249 (* 1 = 7.06249 loss)
I0524 02:51:26.541949 11835 sgd_solver.cpp:112] Iteration 25390, lr = 0.1
I0524 02:51:32.588449 11835 solver.cpp:239] Iteration 25400 (1.65387 iter/s, 6.04641s/10 iters), loss = 6.69034
I0524 02:51:32.588498 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69034 (* 1 = 6.69034 loss)
I0524 02:51:32.588512 11835 sgd_solver.cpp:112] Iteration 25400, lr = 0.1
I0524 02:51:39.766834 11835 solver.cpp:239] Iteration 25410 (1.39313 iter/s, 7.17806s/10 iters), loss = 6.11045
I0524 02:51:39.766887 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11045 (* 1 = 6.11045 loss)
I0524 02:51:39.766907 11835 sgd_solver.cpp:112] Iteration 25410, lr = 0.1
I0524 02:51:46.186661 11835 solver.cpp:239] Iteration 25420 (1.55784 iter/s, 6.41913s/10 iters), loss = 6.82642
I0524 02:51:46.186733 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.82642 (* 1 = 6.82642 loss)
I0524 02:51:46.186842 11835 sgd_solver.cpp:112] Iteration 25420, lr = 0.1
I0524 02:51:52.134737 11835 solver.cpp:239] Iteration 25430 (1.6813 iter/s, 5.94778s/10 iters), loss = 6.46045
I0524 02:51:52.134780 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46045 (* 1 = 6.46045 loss)
I0524 02:51:52.757097 11835 sgd_solver.cpp:112] Iteration 25430, lr = 0.1
I0524 02:51:59.664319 11835 solver.cpp:239] Iteration 25440 (1.32816 iter/s, 7.52924s/10 iters), loss = 7.21908
I0524 02:51:59.664640 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.21908 (* 1 = 7.21908 loss)
I0524 02:51:59.664700 11835 sgd_solver.cpp:112] Iteration 25440, lr = 0.1
I0524 02:52:06.890172 11835 solver.cpp:239] Iteration 25450 (1.38407 iter/s, 7.22509s/10 iters), loss = 6.67533
I0524 02:52:06.890220 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67533 (* 1 = 6.67533 loss)
I0524 02:52:06.890439 11835 sgd_solver.cpp:112] Iteration 25450, lr = 0.1
I0524 02:52:12.496273 11835 solver.cpp:239] Iteration 25460 (1.78386 iter/s, 5.60583s/10 iters), loss = 6.48357
I0524 02:52:12.496322 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48357 (* 1 = 6.48357 loss)
I0524 02:52:12.496340 11835 sgd_solver.cpp:112] Iteration 25460, lr = 0.1
I0524 02:52:18.978073 11835 solver.cpp:239] Iteration 25470 (1.54287 iter/s, 6.48144s/10 iters), loss = 6.36743
I0524 02:52:18.978126 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36743 (* 1 = 6.36743 loss)
I0524 02:52:18.978349 11835 sgd_solver.cpp:112] Iteration 25470, lr = 0.1
I0524 02:52:24.555240 11835 solver.cpp:239] Iteration 25480 (1.79311 iter/s, 5.57689s/10 iters), loss = 6.26642
I0524 02:52:24.555299 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26642 (* 1 = 6.26642 loss)
I0524 02:52:24.555532 11835 sgd_solver.cpp:112] Iteration 25480, lr = 0.1
I0524 02:52:30.816278 11835 solver.cpp:239] Iteration 25490 (1.59726 iter/s, 6.26073s/10 iters), loss = 7.69486
I0524 02:52:30.816594 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.69486 (* 1 = 7.69486 loss)
I0524 02:52:30.816663 11835 sgd_solver.cpp:112] Iteration 25490, lr = 0.1
I0524 02:52:37.193960 11835 solver.cpp:239] Iteration 25500 (1.56809 iter/s, 6.37719s/10 iters), loss = 7.33534
I0524 02:52:37.194000 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.33534 (* 1 = 7.33534 loss)
I0524 02:52:37.779001 11835 sgd_solver.cpp:112] Iteration 25500, lr = 0.1
I0524 02:52:44.519510 11835 solver.cpp:239] Iteration 25510 (1.36515 iter/s, 7.32523s/10 iters), loss = 5.1185
I0524 02:52:44.519562 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.1185 (* 1 = 5.1185 loss)
I0524 02:52:44.519676 11835 sgd_solver.cpp:112] Iteration 25510, lr = 0.1
I0524 02:52:50.501194 11835 solver.cpp:239] Iteration 25520 (1.67185 iter/s, 5.98141s/10 iters), loss = 6.87164
I0524 02:52:50.501240 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87164 (* 1 = 6.87164 loss)
I0524 02:52:50.501304 11835 sgd_solver.cpp:112] Iteration 25520, lr = 0.1
I0524 02:52:57.970520 11835 solver.cpp:239] Iteration 25530 (1.33887 iter/s, 7.46899s/10 iters), loss = 5.58957
I0524 02:52:57.970576 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.58957 (* 1 = 5.58957 loss)
I0524 02:52:57.970924 11835 sgd_solver.cpp:112] Iteration 25530, lr = 0.1
I0524 02:53:05.800859 11835 solver.cpp:239] Iteration 25540 (1.27714 iter/s, 7.82998s/10 iters), loss = 6.83486
I0524 02:53:05.800995 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83486 (* 1 = 6.83486 loss)
I0524 02:53:05.801012 11835 sgd_solver.cpp:112] Iteration 25540, lr = 0.1
I0524 02:53:12.379334 11835 solver.cpp:239] Iteration 25550 (1.5202 iter/s, 6.57808s/10 iters), loss = 6.56541
I0524 02:53:12.379395 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56541 (* 1 = 6.56541 loss)
I0524 02:53:12.385010 11835 sgd_solver.cpp:112] Iteration 25550, lr = 0.1
I0524 02:53:18.235762 11835 solver.cpp:239] Iteration 25560 (1.70761 iter/s, 5.85615s/10 iters), loss = 6.41953
I0524 02:53:18.235802 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41953 (* 1 = 6.41953 loss)
I0524 02:53:18.235815 11835 sgd_solver.cpp:112] Iteration 25560, lr = 0.1
I0524 02:53:24.602491 11835 solver.cpp:239] Iteration 25570 (1.57074 iter/s, 6.36643s/10 iters), loss = 7.89062
I0524 02:53:24.602552 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.89062 (* 1 = 7.89062 loss)
I0524 02:53:24.603021 11835 sgd_solver.cpp:112] Iteration 25570, lr = 0.1
I0524 02:53:30.824560 11835 solver.cpp:239] Iteration 25580 (1.60726 iter/s, 6.22177s/10 iters), loss = 6.67494
I0524 02:53:30.824618 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67494 (* 1 = 6.67494 loss)
I0524 02:53:30.824726 11835 sgd_solver.cpp:112] Iteration 25580, lr = 0.1
I0524 02:53:37.142058 11835 solver.cpp:239] Iteration 25590 (1.58298 iter/s, 6.31721s/10 iters), loss = 7.41062
I0524 02:53:37.142341 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.41062 (* 1 = 7.41062 loss)
I0524 02:53:37.142390 11835 sgd_solver.cpp:112] Iteration 25590, lr = 0.1
I0524 02:53:43.497699 11835 solver.cpp:239] Iteration 25600 (1.57353 iter/s, 6.35514s/10 iters), loss = 6.63156
I0524 02:53:43.497747 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63156 (* 1 = 6.63156 loss)
I0524 02:53:43.497761 11835 sgd_solver.cpp:112] Iteration 25600, lr = 0.1
I0524 02:53:49.468550 11835 solver.cpp:239] Iteration 25610 (1.67519 iter/s, 5.96947s/10 iters), loss = 7.66383
I0524 02:53:49.468592 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.66383 (* 1 = 7.66383 loss)
I0524 02:53:49.468633 11835 sgd_solver.cpp:112] Iteration 25610, lr = 0.1
I0524 02:53:55.578449 11835 solver.cpp:239] Iteration 25620 (1.63677 iter/s, 6.10961s/10 iters), loss = 7.40919
I0524 02:53:55.578514 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.40919 (* 1 = 7.40919 loss)
I0524 02:53:55.578770 11835 sgd_solver.cpp:112] Iteration 25620, lr = 0.1
I0524 02:54:04.898452 11835 solver.cpp:239] Iteration 25630 (1.07301 iter/s, 9.31958s/10 iters), loss = 7.70402
I0524 02:54:04.898504 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.70402 (* 1 = 7.70402 loss)
I0524 02:54:04.898870 11835 sgd_solver.cpp:112] Iteration 25630, lr = 0.1
I0524 02:54:13.267069 11835 solver.cpp:239] Iteration 25640 (1.19499 iter/s, 8.36824s/10 iters), loss = 6.37178
I0524 02:54:13.267350 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37178 (* 1 = 6.37178 loss)
I0524 02:54:13.817832 11835 sgd_solver.cpp:112] Iteration 25640, lr = 0.1
I0524 02:54:21.704608 11835 solver.cpp:239] Iteration 25650 (1.18526 iter/s, 8.43698s/10 iters), loss = 6.03858
I0524 02:54:21.704663 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03858 (* 1 = 6.03858 loss)
I0524 02:54:21.704789 11835 sgd_solver.cpp:112] Iteration 25650, lr = 0.1
I0524 02:54:29.483733 11835 solver.cpp:239] Iteration 25660 (1.28555 iter/s, 7.77877s/10 iters), loss = 6.7249
I0524 02:54:29.483783 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7249 (* 1 = 6.7249 loss)
I0524 02:54:29.483796 11835 sgd_solver.cpp:112] Iteration 25660, lr = 0.1
I0524 02:54:35.351361 11835 solver.cpp:239] Iteration 25670 (1.70445 iter/s, 5.86699s/10 iters), loss = 6.63307
I0524 02:54:35.351402 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63307 (* 1 = 6.63307 loss)
I0524 02:54:35.351632 11835 sgd_solver.cpp:112] Iteration 25670, lr = 0.1
I0524 02:54:41.770802 11835 solver.cpp:239] Iteration 25680 (1.55784 iter/s, 6.41914s/10 iters), loss = 6.57014
I0524 02:54:41.770864 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57014 (* 1 = 6.57014 loss)
I0524 02:54:41.771083 11835 sgd_solver.cpp:112] Iteration 25680, lr = 0.1
I0524 02:54:50.155409 11835 solver.cpp:239] Iteration 25690 (1.19272 iter/s, 8.38423s/10 iters), loss = 5.95691
I0524 02:54:50.155508 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95691 (* 1 = 5.95691 loss)
I0524 02:54:50.449439 11835 sgd_solver.cpp:112] Iteration 25690, lr = 0.1
I0524 02:54:57.425570 11835 solver.cpp:239] Iteration 25700 (1.37556 iter/s, 7.26978s/10 iters), loss = 7.2343
I0524 02:54:57.425633 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2343 (* 1 = 7.2343 loss)
I0524 02:54:57.425786 11835 sgd_solver.cpp:112] Iteration 25700, lr = 0.1
I0524 02:55:03.277792 11835 solver.cpp:239] Iteration 25710 (1.70883 iter/s, 5.85195s/10 iters), loss = 6.52448
I0524 02:55:03.277832 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52448 (* 1 = 6.52448 loss)
I0524 02:55:03.283143 11835 sgd_solver.cpp:112] Iteration 25710, lr = 0.1
I0524 02:55:11.221560 11835 solver.cpp:239] Iteration 25720 (1.2589 iter/s, 7.94342s/10 iters), loss = 7.10968
I0524 02:55:11.221618 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10968 (* 1 = 7.10968 loss)
I0524 02:55:11.385450 11835 sgd_solver.cpp:112] Iteration 25720, lr = 0.1
I0524 02:55:17.157021 11835 solver.cpp:239] Iteration 25730 (1.68487 iter/s, 5.93518s/10 iters), loss = 6.53457
I0524 02:55:17.157060 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53457 (* 1 = 6.53457 loss)
I0524 02:55:17.157085 11835 sgd_solver.cpp:112] Iteration 25730, lr = 0.1
I0524 02:55:23.678192 11835 solver.cpp:239] Iteration 25740 (1.53354 iter/s, 6.52087s/10 iters), loss = 6.6488
I0524 02:55:23.678448 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6488 (* 1 = 6.6488 loss)
I0524 02:55:23.678503 11835 sgd_solver.cpp:112] Iteration 25740, lr = 0.1
I0524 02:55:30.018072 11835 solver.cpp:239] Iteration 25750 (1.57745 iter/s, 6.33933s/10 iters), loss = 7.00111
I0524 02:55:30.018121 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00111 (* 1 = 7.00111 loss)
I0524 02:55:30.018165 11835 sgd_solver.cpp:112] Iteration 25750, lr = 0.1
I0524 02:55:35.807746 11835 solver.cpp:239] Iteration 25760 (1.72729 iter/s, 5.78941s/10 iters), loss = 7.50261
I0524 02:55:35.807785 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.50261 (* 1 = 7.50261 loss)
I0524 02:55:36.805425 11835 sgd_solver.cpp:112] Iteration 25760, lr = 0.1
I0524 02:55:44.393671 11835 solver.cpp:239] Iteration 25770 (1.16475 iter/s, 8.58555s/10 iters), loss = 6.49376
I0524 02:55:44.393725 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49376 (* 1 = 6.49376 loss)
I0524 02:55:45.208817 11835 sgd_solver.cpp:112] Iteration 25770, lr = 0.1
I0524 02:55:51.243182 11835 solver.cpp:239] Iteration 25780 (1.46002 iter/s, 6.84921s/10 iters), loss = 6.43932
I0524 02:55:51.243218 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43932 (* 1 = 6.43932 loss)
I0524 02:55:51.270225 11835 sgd_solver.cpp:112] Iteration 25780, lr = 0.1
I0524 02:55:57.552634 11835 solver.cpp:239] Iteration 25790 (1.585 iter/s, 6.30917s/10 iters), loss = 6.3447
I0524 02:55:57.552767 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3447 (* 1 = 6.3447 loss)
I0524 02:55:57.588676 11835 sgd_solver.cpp:112] Iteration 25790, lr = 0.1
I0524 02:56:04.250241 11835 solver.cpp:239] Iteration 25800 (1.49316 iter/s, 6.69722s/10 iters), loss = 6.65783
I0524 02:56:04.250291 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65783 (* 1 = 6.65783 loss)
I0524 02:56:04.250408 11835 sgd_solver.cpp:112] Iteration 25800, lr = 0.1
I0524 02:56:13.530163 11835 solver.cpp:239] Iteration 25810 (1.07764 iter/s, 9.27953s/10 iters), loss = 7.05496
I0524 02:56:13.530205 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.05496 (* 1 = 7.05496 loss)
I0524 02:56:13.781743 11835 sgd_solver.cpp:112] Iteration 25810, lr = 0.1
I0524 02:56:19.930928 11835 solver.cpp:239] Iteration 25820 (1.56239 iter/s, 6.40046s/10 iters), loss = 6.52487
I0524 02:56:19.930984 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52487 (* 1 = 6.52487 loss)
I0524 02:56:19.931190 11835 sgd_solver.cpp:112] Iteration 25820, lr = 0.1
I0524 02:56:25.592193 11835 solver.cpp:239] Iteration 25830 (1.76647 iter/s, 5.661s/10 iters), loss = 6.24623
I0524 02:56:25.592231 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24623 (* 1 = 6.24623 loss)
I0524 02:56:25.592250 11835 sgd_solver.cpp:112] Iteration 25830, lr = 0.1
I0524 02:56:31.658994 11835 solver.cpp:239] Iteration 25840 (1.64839 iter/s, 6.06652s/10 iters), loss = 6.10709
I0524 02:56:31.659158 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10709 (* 1 = 6.10709 loss)
I0524 02:56:31.659323 11835 sgd_solver.cpp:112] Iteration 25840, lr = 0.1
I0524 02:56:38.809499 11835 solver.cpp:239] Iteration 25850 (1.39858 iter/s, 7.15009s/10 iters), loss = 6.34356
I0524 02:56:38.809536 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34356 (* 1 = 6.34356 loss)
I0524 02:56:39.407726 11835 sgd_solver.cpp:112] Iteration 25850, lr = 0.1
I0524 02:56:46.837942 11835 solver.cpp:239] Iteration 25860 (1.24563 iter/s, 8.02809s/10 iters), loss = 6.70121
I0524 02:56:46.838003 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70121 (* 1 = 6.70121 loss)
I0524 02:56:47.033102 11835 sgd_solver.cpp:112] Iteration 25860, lr = 0.1
I0524 02:56:54.635102 11835 solver.cpp:239] Iteration 25870 (1.28258 iter/s, 7.7968s/10 iters), loss = 6.67858
I0524 02:56:54.635157 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67858 (* 1 = 6.67858 loss)
I0524 02:56:54.635401 11835 sgd_solver.cpp:112] Iteration 25870, lr = 0.1
I0524 02:57:01.154083 11835 solver.cpp:239] Iteration 25880 (1.53405 iter/s, 6.51868s/10 iters), loss = 6.44857
I0524 02:57:01.154136 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44857 (* 1 = 6.44857 loss)
I0524 02:57:01.154490 11835 sgd_solver.cpp:112] Iteration 25880, lr = 0.1
I0524 02:57:07.897295 11835 solver.cpp:239] Iteration 25890 (1.48304 iter/s, 6.7429s/10 iters), loss = 7.49957
I0524 02:57:07.897558 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.49957 (* 1 = 7.49957 loss)
I0524 02:57:07.897614 11835 sgd_solver.cpp:112] Iteration 25890, lr = 0.1
I0524 02:57:14.606097 11835 solver.cpp:239] Iteration 25900 (1.49074 iter/s, 6.7081s/10 iters), loss = 6.80183
I0524 02:57:14.606158 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80183 (* 1 = 6.80183 loss)
I0524 02:57:14.606277 11835 sgd_solver.cpp:112] Iteration 25900, lr = 0.1
I0524 02:57:20.711860 11835 solver.cpp:239] Iteration 25910 (1.63787 iter/s, 6.10548s/10 iters), loss = 6.18326
I0524 02:57:20.711911 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18326 (* 1 = 6.18326 loss)
I0524 02:57:20.711932 11835 sgd_solver.cpp:112] Iteration 25910, lr = 0.1
I0524 02:57:27.702164 11835 solver.cpp:239] Iteration 25920 (1.43062 iter/s, 6.98998s/10 iters), loss = 6.87622
I0524 02:57:27.702219 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87622 (* 1 = 6.87622 loss)
I0524 02:57:27.702453 11835 sgd_solver.cpp:112] Iteration 25920, lr = 0.1
I0524 02:57:33.502995 11835 solver.cpp:239] Iteration 25930 (1.72397 iter/s, 5.80056s/10 iters), loss = 6.60296
I0524 02:57:33.503032 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60296 (* 1 = 6.60296 loss)
I0524 02:57:33.503046 11835 sgd_solver.cpp:112] Iteration 25930, lr = 0.1
I0524 02:57:39.715009 11835 solver.cpp:239] Iteration 25940 (1.60988 iter/s, 6.21166s/10 iters), loss = 6.50872
I0524 02:57:39.715312 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50872 (* 1 = 6.50872 loss)
I0524 02:57:39.873102 11835 sgd_solver.cpp:112] Iteration 25940, lr = 0.1
I0524 02:57:45.559681 11835 solver.cpp:239] Iteration 25950 (1.7111 iter/s, 5.8442s/10 iters), loss = 7.40787
I0524 02:57:45.559728 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.40787 (* 1 = 7.40787 loss)
I0524 02:57:45.560070 11835 sgd_solver.cpp:112] Iteration 25950, lr = 0.1
I0524 02:57:51.237645 11835 solver.cpp:239] Iteration 25960 (1.76128 iter/s, 5.67769s/10 iters), loss = 7.32209
I0524 02:57:51.237692 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.32209 (* 1 = 7.32209 loss)
I0524 02:57:51.267242 11835 sgd_solver.cpp:112] Iteration 25960, lr = 0.1
I0524 02:58:00.416738 11835 solver.cpp:239] Iteration 25970 (1.08948 iter/s, 9.1787s/10 iters), loss = 6.55246
I0524 02:58:00.416787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55246 (* 1 = 6.55246 loss)
I0524 02:58:00.416898 11835 sgd_solver.cpp:112] Iteration 25970, lr = 0.1
I0524 02:58:06.358785 11835 solver.cpp:239] Iteration 25980 (1.683 iter/s, 5.94177s/10 iters), loss = 7.51631
I0524 02:58:06.358829 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.51631 (* 1 = 7.51631 loss)
I0524 02:58:06.358970 11835 sgd_solver.cpp:112] Iteration 25980, lr = 0.1
I0524 02:58:11.956614 11835 solver.cpp:239] Iteration 25990 (1.78649 iter/s, 5.59756s/10 iters), loss = 6.16551
I0524 02:58:11.956804 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16551 (* 1 = 6.16551 loss)
I0524 02:58:11.956869 11835 sgd_solver.cpp:112] Iteration 25990, lr = 0.1
I0524 02:58:18.848945 11835 solver.cpp:239] Iteration 26000 (1.45098 iter/s, 6.89189s/10 iters), loss = 6.36887
I0524 02:58:18.848994 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36887 (* 1 = 6.36887 loss)
I0524 02:58:18.849105 11835 sgd_solver.cpp:112] Iteration 26000, lr = 0.1
I0524 02:58:25.035861 11835 solver.cpp:239] Iteration 26010 (1.61639 iter/s, 6.18663s/10 iters), loss = 5.95933
I0524 02:58:25.035913 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95933 (* 1 = 5.95933 loss)
I0524 02:58:26.058351 11835 sgd_solver.cpp:112] Iteration 26010, lr = 0.1
I0524 02:58:31.823593 11835 solver.cpp:239] Iteration 26020 (1.47331 iter/s, 6.78743s/10 iters), loss = 6.68728
I0524 02:58:31.823634 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.68728 (* 1 = 6.68728 loss)
I0524 02:58:31.823648 11835 sgd_solver.cpp:112] Iteration 26020, lr = 0.1
I0524 02:58:38.534930 11835 solver.cpp:239] Iteration 26030 (1.4901 iter/s, 6.71095s/10 iters), loss = 7.22001
I0524 02:58:38.534986 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.22001 (* 1 = 7.22001 loss)
I0524 02:58:38.535162 11835 sgd_solver.cpp:112] Iteration 26030, lr = 0.1
I0524 02:58:44.421897 11835 solver.cpp:239] Iteration 26040 (1.69875 iter/s, 5.88668s/10 iters), loss = 7.09095
I0524 02:58:44.422169 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09095 (* 1 = 7.09095 loss)
I0524 02:58:45.026824 11835 sgd_solver.cpp:112] Iteration 26040, lr = 0.1
I0524 02:58:53.024762 11835 solver.cpp:239] Iteration 26050 (1.16248 iter/s, 8.60232s/10 iters), loss = 6.1797
I0524 02:58:53.024808 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1797 (* 1 = 6.1797 loss)
I0524 02:58:53.025017 11835 sgd_solver.cpp:112] Iteration 26050, lr = 0.1
I0524 02:59:00.197831 11835 solver.cpp:239] Iteration 26060 (1.39417 iter/s, 7.17274s/10 iters), loss = 6.37632
I0524 02:59:00.197880 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37632 (* 1 = 6.37632 loss)
I0524 02:59:00.197895 11835 sgd_solver.cpp:112] Iteration 26060, lr = 0.1
I0524 02:59:06.172974 11835 solver.cpp:239] Iteration 26070 (1.67399 iter/s, 5.97376s/10 iters), loss = 6.98882
I0524 02:59:06.173029 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98882 (* 1 = 6.98882 loss)
I0524 02:59:06.178452 11835 sgd_solver.cpp:112] Iteration 26070, lr = 0.1
I0524 02:59:14.023500 11835 solver.cpp:239] Iteration 26080 (1.27386 iter/s, 7.85018s/10 iters), loss = 5.98768
I0524 02:59:14.023538 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98768 (* 1 = 5.98768 loss)
I0524 02:59:14.023876 11835 sgd_solver.cpp:112] Iteration 26080, lr = 0.1
I0524 02:59:19.920262 11835 solver.cpp:239] Iteration 26090 (1.69593 iter/s, 5.89648s/10 iters), loss = 7.56339
I0524 02:59:19.920493 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.56339 (* 1 = 7.56339 loss)
I0524 02:59:19.920552 11835 sgd_solver.cpp:112] Iteration 26090, lr = 0.1
I0524 02:59:28.354365 11835 solver.cpp:239] Iteration 26100 (1.18573 iter/s, 8.43359s/10 iters), loss = 6.40775
I0524 02:59:28.354427 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40775 (* 1 = 6.40775 loss)
I0524 02:59:28.354630 11835 sgd_solver.cpp:112] Iteration 26100, lr = 0.1
I0524 02:59:36.310001 11835 solver.cpp:239] Iteration 26110 (1.25703 iter/s, 7.95528s/10 iters), loss = 6.20713
I0524 02:59:36.310039 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20713 (* 1 = 6.20713 loss)
I0524 02:59:36.310184 11835 sgd_solver.cpp:112] Iteration 26110, lr = 0.1
I0524 02:59:41.477730 11835 solver.cpp:239] Iteration 26120 (1.93519 iter/s, 5.16746s/10 iters), loss = 6.89663
I0524 02:59:41.477815 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89663 (* 1 = 6.89663 loss)
I0524 02:59:41.477834 11835 sgd_solver.cpp:112] Iteration 26120, lr = 0.1
I0524 02:59:47.348718 11835 solver.cpp:239] Iteration 26130 (1.70338 iter/s, 5.87069s/10 iters), loss = 7.10748
I0524 02:59:47.348770 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10748 (* 1 = 7.10748 loss)
I0524 02:59:47.348978 11835 sgd_solver.cpp:112] Iteration 26130, lr = 0.1
I0524 02:59:53.849786 11835 solver.cpp:239] Iteration 26140 (1.53828 iter/s, 6.50076s/10 iters), loss = 6.50296
I0524 02:59:53.850111 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50296 (* 1 = 6.50296 loss)
I0524 02:59:54.213744 11835 sgd_solver.cpp:112] Iteration 26140, lr = 0.1
I0524 03:00:03.168385 11835 solver.cpp:239] Iteration 26150 (1.0732 iter/s, 9.31797s/10 iters), loss = 6.07842
I0524 03:00:03.168440 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07842 (* 1 = 6.07842 loss)
I0524 03:00:03.168726 11835 sgd_solver.cpp:112] Iteration 26150, lr = 0.1
I0524 03:00:09.487061 11835 solver.cpp:239] Iteration 26160 (1.58269 iter/s, 6.31837s/10 iters), loss = 7.25801
I0524 03:00:09.487112 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.25801 (* 1 = 7.25801 loss)
I0524 03:00:09.547777 11835 sgd_solver.cpp:112] Iteration 26160, lr = 0.1
I0524 03:00:15.663175 11835 solver.cpp:239] Iteration 26170 (1.61922 iter/s, 6.17583s/10 iters), loss = 6.72015
I0524 03:00:15.663218 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72015 (* 1 = 6.72015 loss)
I0524 03:00:15.663347 11835 sgd_solver.cpp:112] Iteration 26170, lr = 0.1
I0524 03:00:22.381465 11835 solver.cpp:239] Iteration 26180 (1.48854 iter/s, 6.71799s/10 iters), loss = 7.31839
I0524 03:00:22.381515 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.31839 (* 1 = 7.31839 loss)
I0524 03:00:22.381536 11835 sgd_solver.cpp:112] Iteration 26180, lr = 0.1
I0524 03:00:30.402617 11835 solver.cpp:239] Iteration 26190 (1.24676 iter/s, 8.0208s/10 iters), loss = 7.49959
I0524 03:00:30.402907 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.49959 (* 1 = 7.49959 loss)
I0524 03:00:30.658537 11835 sgd_solver.cpp:112] Iteration 26190, lr = 0.1
I0524 03:00:37.854370 11835 solver.cpp:239] Iteration 26200 (1.34206 iter/s, 7.45123s/10 iters), loss = 6.88242
I0524 03:00:37.854423 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88242 (* 1 = 6.88242 loss)
I0524 03:00:38.283696 11835 sgd_solver.cpp:112] Iteration 26200, lr = 0.1
I0524 03:00:44.541570 11835 solver.cpp:239] Iteration 26210 (1.49546 iter/s, 6.68688s/10 iters), loss = 6.33631
I0524 03:00:44.541630 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33631 (* 1 = 6.33631 loss)
I0524 03:00:44.541687 11835 sgd_solver.cpp:112] Iteration 26210, lr = 0.1
I0524 03:00:50.367802 11835 solver.cpp:239] Iteration 26220 (1.71646 iter/s, 5.82594s/10 iters), loss = 5.80536
I0524 03:00:50.367858 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80536 (* 1 = 5.80536 loss)
I0524 03:00:50.367995 11835 sgd_solver.cpp:112] Iteration 26220, lr = 0.1
I0524 03:00:58.611284 11835 solver.cpp:239] Iteration 26230 (1.21313 iter/s, 8.24312s/10 iters), loss = 6.46176
I0524 03:00:58.611320 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46176 (* 1 = 6.46176 loss)
I0524 03:00:58.611443 11835 sgd_solver.cpp:112] Iteration 26230, lr = 0.1
I0524 03:01:05.465205 11835 solver.cpp:239] Iteration 26240 (1.45908 iter/s, 6.85361s/10 iters), loss = 6.63326
I0524 03:01:05.465343 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63326 (* 1 = 6.63326 loss)
I0524 03:01:06.033185 11835 sgd_solver.cpp:112] Iteration 26240, lr = 0.1
I0524 03:01:12.830054 11835 solver.cpp:239] Iteration 26250 (1.35788 iter/s, 7.36443s/10 iters), loss = 6.81027
I0524 03:01:12.830101 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81027 (* 1 = 6.81027 loss)
I0524 03:01:12.830448 11835 sgd_solver.cpp:112] Iteration 26250, lr = 0.1
I0524 03:01:19.975522 11835 solver.cpp:239] Iteration 26260 (1.39955 iter/s, 7.14515s/10 iters), loss = 6.64768
I0524 03:01:19.975572 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64768 (* 1 = 6.64768 loss)
I0524 03:01:20.716728 11835 sgd_solver.cpp:112] Iteration 26260, lr = 0.1
I0524 03:01:27.671573 11835 solver.cpp:239] Iteration 26270 (1.29943 iter/s, 7.69571s/10 iters), loss = 5.62538
I0524 03:01:27.671627 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.62538 (* 1 = 5.62538 loss)
I0524 03:01:27.671732 11835 sgd_solver.cpp:112] Iteration 26270, lr = 0.1
I0524 03:01:34.010917 11835 solver.cpp:239] Iteration 26280 (1.57752 iter/s, 6.33905s/10 iters), loss = 6.77923
I0524 03:01:34.010957 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.77923 (* 1 = 6.77923 loss)
I0524 03:01:34.015378 11835 sgd_solver.cpp:112] Iteration 26280, lr = 0.1
I0524 03:01:39.616271 11835 solver.cpp:239] Iteration 26290 (1.7841 iter/s, 5.60508s/10 iters), loss = 7.69139
I0524 03:01:39.616626 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.69139 (* 1 = 7.69139 loss)
I0524 03:01:39.616703 11835 sgd_solver.cpp:112] Iteration 26290, lr = 0.1
I0524 03:01:45.736665 11835 solver.cpp:239] Iteration 26300 (1.63403 iter/s, 6.11983s/10 iters), loss = 7.61113
I0524 03:01:45.736703 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.61113 (* 1 = 7.61113 loss)
I0524 03:01:45.736847 11835 sgd_solver.cpp:112] Iteration 26300, lr = 0.1
I0524 03:01:52.481071 11835 solver.cpp:239] Iteration 26310 (1.48278 iter/s, 6.7441s/10 iters), loss = 7.34228
I0524 03:01:52.481127 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.34228 (* 1 = 7.34228 loss)
I0524 03:01:52.481367 11835 sgd_solver.cpp:112] Iteration 26310, lr = 0.1
I0524 03:01:58.544539 11835 solver.cpp:239] Iteration 26320 (1.6493 iter/s, 6.06318s/10 iters), loss = 7.31438
I0524 03:01:58.544601 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.31438 (* 1 = 7.31438 loss)
I0524 03:01:58.709156 11835 sgd_solver.cpp:112] Iteration 26320, lr = 0.1
I0524 03:02:04.589781 11835 solver.cpp:239] Iteration 26330 (1.65428 iter/s, 6.04494s/10 iters), loss = 6.7751
I0524 03:02:04.589828 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7751 (* 1 = 6.7751 loss)
I0524 03:02:04.669565 11835 sgd_solver.cpp:112] Iteration 26330, lr = 0.1
I0524 03:02:11.617882 11835 solver.cpp:239] Iteration 26340 (1.42292 iter/s, 7.02779s/10 iters), loss = 5.77876
I0524 03:02:11.618180 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.77876 (* 1 = 5.77876 loss)
I0524 03:02:11.618981 11835 sgd_solver.cpp:112] Iteration 26340, lr = 0.1
I0524 03:02:17.571048 11835 solver.cpp:239] Iteration 26350 (1.67992 iter/s, 5.95268s/10 iters), loss = 6.08826
I0524 03:02:17.571112 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08826 (* 1 = 6.08826 loss)
I0524 03:02:17.571326 11835 sgd_solver.cpp:112] Iteration 26350, lr = 0.1
I0524 03:02:24.157703 11835 solver.cpp:239] Iteration 26360 (1.51829 iter/s, 6.58634s/10 iters), loss = 6.04521
I0524 03:02:24.157753 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04521 (* 1 = 6.04521 loss)
I0524 03:02:24.157768 11835 sgd_solver.cpp:112] Iteration 26360, lr = 0.1
I0524 03:02:32.918961 11835 solver.cpp:239] Iteration 26370 (1.14144 iter/s, 8.76088s/10 iters), loss = 6.67062
I0524 03:02:32.919024 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67062 (* 1 = 6.67062 loss)
I0524 03:02:32.919364 11835 sgd_solver.cpp:112] Iteration 26370, lr = 0.1
I0524 03:02:39.401628 11835 solver.cpp:239] Iteration 26380 (1.54265 iter/s, 6.48236s/10 iters), loss = 6.17559
I0524 03:02:39.401669 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17559 (* 1 = 6.17559 loss)
I0524 03:02:39.690073 11835 sgd_solver.cpp:112] Iteration 26380, lr = 0.1
I0524 03:02:45.561358 11835 solver.cpp:239] Iteration 26390 (1.62352 iter/s, 6.15944s/10 iters), loss = 6.357
I0524 03:02:45.561682 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.357 (* 1 = 6.357 loss)
I0524 03:02:45.561739 11835 sgd_solver.cpp:112] Iteration 26390, lr = 0.1
I0524 03:02:52.011435 11835 solver.cpp:239] Iteration 26400 (1.5506 iter/s, 6.4491s/10 iters), loss = 6.71186
I0524 03:02:52.011492 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71186 (* 1 = 6.71186 loss)
I0524 03:02:52.011584 11835 sgd_solver.cpp:112] Iteration 26400, lr = 0.1
I0524 03:02:57.862401 11835 solver.cpp:239] Iteration 26410 (1.7092 iter/s, 5.85068s/10 iters), loss = 7.14921
I0524 03:02:57.862449 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.14921 (* 1 = 7.14921 loss)
I0524 03:02:58.508234 11835 sgd_solver.cpp:112] Iteration 26410, lr = 0.1
I0524 03:03:04.405257 11835 solver.cpp:239] Iteration 26420 (1.52845 iter/s, 6.54256s/10 iters), loss = 6.21209
I0524 03:03:04.405297 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21209 (* 1 = 6.21209 loss)
I0524 03:03:05.549753 11835 sgd_solver.cpp:112] Iteration 26420, lr = 0.1
I0524 03:03:13.562250 11835 solver.cpp:239] Iteration 26430 (1.09211 iter/s, 9.15659s/10 iters), loss = 6.76396
I0524 03:03:13.562299 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76396 (* 1 = 6.76396 loss)
I0524 03:03:13.562397 11835 sgd_solver.cpp:112] Iteration 26430, lr = 0.1
I0524 03:03:21.638515 11835 solver.cpp:239] Iteration 26440 (1.23825 iter/s, 8.0759s/10 iters), loss = 5.90435
I0524 03:03:21.638675 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90435 (* 1 = 5.90435 loss)
I0524 03:03:21.640507 11835 sgd_solver.cpp:112] Iteration 26440, lr = 0.1
I0524 03:03:27.570271 11835 solver.cpp:239] Iteration 26450 (1.68595 iter/s, 5.93138s/10 iters), loss = 7.60331
I0524 03:03:27.570313 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.60331 (* 1 = 7.60331 loss)
I0524 03:03:28.082089 11835 sgd_solver.cpp:112] Iteration 26450, lr = 0.1
I0524 03:03:34.571506 11835 solver.cpp:239] Iteration 26460 (1.42838 iter/s, 7.00092s/10 iters), loss = 6.13817
I0524 03:03:34.571554 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13817 (* 1 = 6.13817 loss)
I0524 03:03:34.571568 11835 sgd_solver.cpp:112] Iteration 26460, lr = 0.1
I0524 03:03:40.353545 11835 solver.cpp:239] Iteration 26470 (1.72976 iter/s, 5.78115s/10 iters), loss = 5.94784
I0524 03:03:40.353580 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94784 (* 1 = 5.94784 loss)
I0524 03:03:40.353595 11835 sgd_solver.cpp:112] Iteration 26470, lr = 0.1
I0524 03:03:47.289013 11835 solver.cpp:239] Iteration 26480 (1.44193 iter/s, 6.93516s/10 iters), loss = 6.45213
I0524 03:03:47.289053 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45213 (* 1 = 6.45213 loss)
I0524 03:03:47.289065 11835 sgd_solver.cpp:112] Iteration 26480, lr = 0.1
I0524 03:03:53.283043 11835 solver.cpp:239] Iteration 26490 (1.6684 iter/s, 5.99375s/10 iters), loss = 5.79288
I0524 03:03:53.283324 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79288 (* 1 = 5.79288 loss)
I0524 03:03:53.546751 11835 sgd_solver.cpp:112] Iteration 26490, lr = 0.1
I0524 03:04:00.435242 11835 solver.cpp:239] Iteration 26500 (1.39827 iter/s, 7.15168s/10 iters), loss = 7.34486
I0524 03:04:00.435297 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.34486 (* 1 = 7.34486 loss)
I0524 03:04:00.435624 11835 sgd_solver.cpp:112] Iteration 26500, lr = 0.1
I0524 03:04:08.537395 11835 solver.cpp:239] Iteration 26510 (1.2343 iter/s, 8.10178s/10 iters), loss = 6.46035
I0524 03:04:08.537463 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46035 (* 1 = 6.46035 loss)
I0524 03:04:08.537847 11835 sgd_solver.cpp:112] Iteration 26510, lr = 0.1
I0524 03:04:14.215216 11835 solver.cpp:239] Iteration 26520 (1.76133 iter/s, 5.67753s/10 iters), loss = 5.98948
I0524 03:04:14.215270 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98948 (* 1 = 5.98948 loss)
I0524 03:04:14.253840 11835 sgd_solver.cpp:112] Iteration 26520, lr = 0.1
I0524 03:04:20.653334 11835 solver.cpp:239] Iteration 26530 (1.55332 iter/s, 6.43782s/10 iters), loss = 6.22024
I0524 03:04:20.653379 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22024 (* 1 = 6.22024 loss)
I0524 03:04:20.653453 11835 sgd_solver.cpp:112] Iteration 26530, lr = 0.1
I0524 03:04:27.335762 11835 solver.cpp:239] Iteration 26540 (1.49653 iter/s, 6.68212s/10 iters), loss = 6.15733
I0524 03:04:27.335948 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15733 (* 1 = 6.15733 loss)
I0524 03:04:27.335966 11835 sgd_solver.cpp:112] Iteration 26540, lr = 0.1
I0524 03:04:34.764755 11835 solver.cpp:239] Iteration 26550 (1.34622 iter/s, 7.4282s/10 iters), loss = 6.95446
I0524 03:04:34.764813 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.95446 (* 1 = 6.95446 loss)
I0524 03:04:34.764853 11835 sgd_solver.cpp:112] Iteration 26550, lr = 0.1
I0524 03:04:43.582021 11835 solver.cpp:239] Iteration 26560 (1.13419 iter/s, 8.81687s/10 iters), loss = 6.94885
I0524 03:04:43.582077 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.94885 (* 1 = 6.94885 loss)
I0524 03:04:43.582468 11835 sgd_solver.cpp:112] Iteration 26560, lr = 0.1
I0524 03:04:50.991436 11835 solver.cpp:239] Iteration 26570 (1.34969 iter/s, 7.40908s/10 iters), loss = 7.36425
I0524 03:04:50.991482 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.36425 (* 1 = 7.36425 loss)
I0524 03:04:50.991612 11835 sgd_solver.cpp:112] Iteration 26570, lr = 0.1
I0524 03:04:57.939558 11835 solver.cpp:239] Iteration 26580 (1.4393 iter/s, 6.9478s/10 iters), loss = 6.26986
I0524 03:04:57.939823 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26986 (* 1 = 6.26986 loss)
I0524 03:04:57.939965 11835 sgd_solver.cpp:112] Iteration 26580, lr = 0.1
I0524 03:05:04.702641 11835 solver.cpp:239] Iteration 26590 (1.47872 iter/s, 6.76262s/10 iters), loss = 7.40167
I0524 03:05:04.702679 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.40167 (* 1 = 7.40167 loss)
I0524 03:05:04.910832 11835 sgd_solver.cpp:112] Iteration 26590, lr = 0.1
I0524 03:05:12.061799 11835 solver.cpp:239] Iteration 26600 (1.35891 iter/s, 7.35882s/10 iters), loss = 6.46156
I0524 03:05:12.061856 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46156 (* 1 = 6.46156 loss)
I0524 03:05:12.061921 11835 sgd_solver.cpp:112] Iteration 26600, lr = 0.1
I0524 03:05:18.819985 11835 solver.cpp:239] Iteration 26610 (1.47976 iter/s, 6.75787s/10 iters), loss = 6.15429
I0524 03:05:18.820034 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15429 (* 1 = 6.15429 loss)
I0524 03:05:18.820165 11835 sgd_solver.cpp:112] Iteration 26610, lr = 0.1
I0524 03:05:24.859405 11835 solver.cpp:239] Iteration 26620 (1.65586 iter/s, 6.03915s/10 iters), loss = 6.75216
I0524 03:05:24.859442 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.75216 (* 1 = 6.75216 loss)
I0524 03:05:24.859683 11835 sgd_solver.cpp:112] Iteration 26620, lr = 0.1
I0524 03:05:32.733925 11835 solver.cpp:239] Iteration 26630 (1.26997 iter/s, 7.87418s/10 iters), loss = 6.78202
I0524 03:05:32.734058 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78202 (* 1 = 6.78202 loss)
I0524 03:05:32.804527 11835 sgd_solver.cpp:112] Iteration 26630, lr = 0.1
I0524 03:05:38.480342 11835 solver.cpp:239] Iteration 26640 (1.74032 iter/s, 5.74607s/10 iters), loss = 5.96405
I0524 03:05:38.480384 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96405 (* 1 = 5.96405 loss)
I0524 03:05:38.480396 11835 sgd_solver.cpp:112] Iteration 26640, lr = 0.1
I0524 03:05:45.458360 11835 solver.cpp:239] Iteration 26650 (1.4332 iter/s, 6.97739s/10 iters), loss = 6.86324
I0524 03:05:45.458407 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86324 (* 1 = 6.86324 loss)
I0524 03:05:45.467330 11835 sgd_solver.cpp:112] Iteration 26650, lr = 0.1
I0524 03:05:52.192198 11835 solver.cpp:239] Iteration 26660 (1.4851 iter/s, 6.73353s/10 iters), loss = 5.61017
I0524 03:05:52.192241 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.61017 (* 1 = 5.61017 loss)
I0524 03:05:52.241673 11835 sgd_solver.cpp:112] Iteration 26660, lr = 0.1
I0524 03:06:00.349153 11835 solver.cpp:239] Iteration 26670 (1.226 iter/s, 8.1566s/10 iters), loss = 6.5699
I0524 03:06:00.349202 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5699 (* 1 = 6.5699 loss)
I0524 03:06:00.349262 11835 sgd_solver.cpp:112] Iteration 26670, lr = 0.1
I0524 03:06:09.170253 11835 solver.cpp:239] Iteration 26680 (1.1337 iter/s, 8.82071s/10 iters), loss = 7.01202
I0524 03:06:09.170421 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01202 (* 1 = 7.01202 loss)
I0524 03:06:09.517241 11835 sgd_solver.cpp:112] Iteration 26680, lr = 0.1
I0524 03:06:16.296228 11835 solver.cpp:239] Iteration 26690 (1.4034 iter/s, 7.12553s/10 iters), loss = 7.03775
I0524 03:06:16.296286 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03775 (* 1 = 7.03775 loss)
I0524 03:06:16.296365 11835 sgd_solver.cpp:112] Iteration 26690, lr = 0.1
I0524 03:06:23.460083 11835 solver.cpp:239] Iteration 26700 (1.39596 iter/s, 7.16353s/10 iters), loss = 7.13532
I0524 03:06:23.460136 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13532 (* 1 = 7.13532 loss)
I0524 03:06:24.496578 11835 sgd_solver.cpp:112] Iteration 26700, lr = 0.1
I0524 03:06:32.465890 11835 solver.cpp:239] Iteration 26710 (1.11044 iter/s, 9.00541s/10 iters), loss = 7.04765
I0524 03:06:32.465929 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04765 (* 1 = 7.04765 loss)
I0524 03:06:32.475724 11835 sgd_solver.cpp:112] Iteration 26710, lr = 0.1
I0524 03:06:39.889328 11835 solver.cpp:239] Iteration 26720 (1.34715 iter/s, 7.4231s/10 iters), loss = 6.73625
I0524 03:06:39.889626 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73625 (* 1 = 6.73625 loss)
I0524 03:06:39.894989 11835 sgd_solver.cpp:112] Iteration 26720, lr = 0.1
I0524 03:06:46.940842 11835 solver.cpp:239] Iteration 26730 (1.41824 iter/s, 7.051s/10 iters), loss = 5.62572
I0524 03:06:46.940891 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.62572 (* 1 = 5.62572 loss)
I0524 03:06:47.089567 11835 sgd_solver.cpp:112] Iteration 26730, lr = 0.1
I0524 03:06:54.875836 11835 solver.cpp:239] Iteration 26740 (1.2603 iter/s, 7.93463s/10 iters), loss = 7.47547
I0524 03:06:54.875901 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.47547 (* 1 = 7.47547 loss)
I0524 03:06:54.876260 11835 sgd_solver.cpp:112] Iteration 26740, lr = 0.1
I0524 03:07:01.037106 11835 solver.cpp:239] Iteration 26750 (1.62312 iter/s, 6.16098s/10 iters), loss = 7.43297
I0524 03:07:01.037156 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.43297 (* 1 = 7.43297 loss)
I0524 03:07:01.037186 11835 sgd_solver.cpp:112] Iteration 26750, lr = 0.1
I0524 03:07:06.805544 11835 solver.cpp:239] Iteration 26760 (1.73371 iter/s, 5.76799s/10 iters), loss = 6.72809
I0524 03:07:06.805584 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72809 (* 1 = 6.72809 loss)
I0524 03:07:06.805687 11835 sgd_solver.cpp:112] Iteration 26760, lr = 0.1
I0524 03:07:12.609411 11835 solver.cpp:239] Iteration 26770 (1.72307 iter/s, 5.80359s/10 iters), loss = 6.64632
I0524 03:07:12.609655 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64632 (* 1 = 6.64632 loss)
I0524 03:07:12.609706 11835 sgd_solver.cpp:112] Iteration 26770, lr = 0.1
I0524 03:07:18.451902 11835 solver.cpp:239] Iteration 26780 (1.71172 iter/s, 5.84207s/10 iters), loss = 6.66192
I0524 03:07:18.451942 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66192 (* 1 = 6.66192 loss)
I0524 03:07:18.451954 11835 sgd_solver.cpp:112] Iteration 26780, lr = 0.1
I0524 03:07:25.056895 11835 solver.cpp:239] Iteration 26790 (1.51411 iter/s, 6.60455s/10 iters), loss = 5.83786
I0524 03:07:25.056948 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83786 (* 1 = 5.83786 loss)
I0524 03:07:25.057174 11835 sgd_solver.cpp:112] Iteration 26790, lr = 0.1
I0524 03:07:31.560329 11835 solver.cpp:239] Iteration 26800 (1.53772 iter/s, 6.50313s/10 iters), loss = 5.95711
I0524 03:07:31.560375 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95711 (* 1 = 5.95711 loss)
I0524 03:07:31.560389 11835 sgd_solver.cpp:112] Iteration 26800, lr = 0.1
I0524 03:07:37.738441 11835 solver.cpp:239] Iteration 26810 (1.61879 iter/s, 6.17746s/10 iters), loss = 7.00793
I0524 03:07:37.738492 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00793 (* 1 = 7.00793 loss)
I0524 03:07:37.802728 11835 sgd_solver.cpp:112] Iteration 26810, lr = 0.1
I0524 03:07:43.880501 11835 solver.cpp:239] Iteration 26820 (1.62819 iter/s, 6.14178s/10 iters), loss = 7.04083
I0524 03:07:43.880774 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04083 (* 1 = 7.04083 loss)
I0524 03:07:43.880825 11835 sgd_solver.cpp:112] Iteration 26820, lr = 0.1
I0524 03:07:52.177412 11835 solver.cpp:239] Iteration 26830 (1.20535 iter/s, 8.29633s/10 iters), loss = 7.25222
I0524 03:07:52.177459 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.25222 (* 1 = 7.25222 loss)
I0524 03:07:52.177582 11835 sgd_solver.cpp:112] Iteration 26830, lr = 0.1
I0524 03:07:58.627964 11835 solver.cpp:239] Iteration 26840 (1.55033 iter/s, 6.45025s/10 iters), loss = 6.56281
I0524 03:07:58.628015 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56281 (* 1 = 6.56281 loss)
I0524 03:07:58.628315 11835 sgd_solver.cpp:112] Iteration 26840, lr = 0.1
I0524 03:08:06.632313 11835 solver.cpp:239] Iteration 26850 (1.24938 iter/s, 8.004s/10 iters), loss = 5.97136
I0524 03:08:06.632360 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97136 (* 1 = 5.97136 loss)
I0524 03:08:06.632583 11835 sgd_solver.cpp:112] Iteration 26850, lr = 0.1
I0524 03:08:16.327780 11835 solver.cpp:239] Iteration 26860 (1.03146 iter/s, 9.69504s/10 iters), loss = 7.61322
I0524 03:08:16.327925 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.61322 (* 1 = 7.61322 loss)
I0524 03:08:16.328076 11835 sgd_solver.cpp:112] Iteration 26860, lr = 0.1
I0524 03:08:22.890964 11835 solver.cpp:239] Iteration 26870 (1.52374 iter/s, 6.56279s/10 iters), loss = 5.8919
I0524 03:08:22.891013 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8919 (* 1 = 5.8919 loss)
I0524 03:08:23.655186 11835 sgd_solver.cpp:112] Iteration 26870, lr = 0.1
I0524 03:08:29.984992 11835 solver.cpp:239] Iteration 26880 (1.4097 iter/s, 7.09372s/10 iters), loss = 6.54726
I0524 03:08:29.985034 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54726 (* 1 = 6.54726 loss)
I0524 03:08:29.985049 11835 sgd_solver.cpp:112] Iteration 26880, lr = 0.1
I0524 03:08:37.406638 11835 solver.cpp:239] Iteration 26890 (1.34767 iter/s, 7.4202s/10 iters), loss = 7.34421
I0524 03:08:37.406718 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.34421 (* 1 = 7.34421 loss)
I0524 03:08:37.406805 11835 sgd_solver.cpp:112] Iteration 26890, lr = 0.1
I0524 03:08:43.322088 11835 solver.cpp:239] Iteration 26900 (1.69057 iter/s, 5.91518s/10 iters), loss = 7.47017
I0524 03:08:43.322127 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.47017 (* 1 = 7.47017 loss)
I0524 03:08:43.322139 11835 sgd_solver.cpp:112] Iteration 26900, lr = 0.1
I0524 03:08:48.964220 11835 solver.cpp:239] Iteration 26910 (1.77247 iter/s, 5.64186s/10 iters), loss = 6.54905
I0524 03:08:48.964526 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54905 (* 1 = 6.54905 loss)
I0524 03:08:48.964598 11835 sgd_solver.cpp:112] Iteration 26910, lr = 0.1
I0524 03:08:55.023874 11835 solver.cpp:239] Iteration 26920 (1.65047 iter/s, 6.05887s/10 iters), loss = 5.93192
I0524 03:08:55.023916 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93192 (* 1 = 5.93192 loss)
I0524 03:08:55.024096 11835 sgd_solver.cpp:112] Iteration 26920, lr = 0.1
I0524 03:09:00.778110 11835 solver.cpp:239] Iteration 26930 (1.73793 iter/s, 5.75396s/10 iters), loss = 7.26434
I0524 03:09:00.778161 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.26434 (* 1 = 7.26434 loss)
I0524 03:09:00.778729 11835 sgd_solver.cpp:112] Iteration 26930, lr = 0.1
I0524 03:09:07.931581 11835 solver.cpp:239] Iteration 26940 (1.39799 iter/s, 7.15314s/10 iters), loss = 6.78893
I0524 03:09:07.931638 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78893 (* 1 = 6.78893 loss)
I0524 03:09:07.931808 11835 sgd_solver.cpp:112] Iteration 26940, lr = 0.1
I0524 03:09:14.605674 11835 solver.cpp:239] Iteration 26950 (1.4984 iter/s, 6.67379s/10 iters), loss = 8.09387
I0524 03:09:14.605721 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.09387 (* 1 = 8.09387 loss)
I0524 03:09:14.605851 11835 sgd_solver.cpp:112] Iteration 26950, lr = 0.1
I0524 03:09:20.542809 11835 solver.cpp:239] Iteration 26960 (1.68439 iter/s, 5.93685s/10 iters), loss = 6.02789
I0524 03:09:20.543145 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02789 (* 1 = 6.02789 loss)
I0524 03:09:20.615864 11835 sgd_solver.cpp:112] Iteration 26960, lr = 0.1
I0524 03:09:26.222847 11835 solver.cpp:239] Iteration 26970 (1.7607 iter/s, 5.67955s/10 iters), loss = 6.55583
I0524 03:09:26.222882 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55583 (* 1 = 6.55583 loss)
I0524 03:09:26.222894 11835 sgd_solver.cpp:112] Iteration 26970, lr = 0.1
I0524 03:09:32.851871 11835 solver.cpp:239] Iteration 26980 (1.50859 iter/s, 6.62873s/10 iters), loss = 7.05083
I0524 03:09:32.851927 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.05083 (* 1 = 7.05083 loss)
I0524 03:09:32.852172 11835 sgd_solver.cpp:112] Iteration 26980, lr = 0.1
I0524 03:09:39.698426 11835 solver.cpp:239] Iteration 26990 (1.46065 iter/s, 6.84625s/10 iters), loss = 5.63686
I0524 03:09:39.698470 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63686 (* 1 = 5.63686 loss)
I0524 03:09:39.698628 11835 sgd_solver.cpp:112] Iteration 26990, lr = 0.1
I0524 03:09:46.160796 11835 solver.cpp:239] Iteration 27000 (1.54749 iter/s, 6.46207s/10 iters), loss = 5.84816
I0524 03:09:46.160852 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84816 (* 1 = 5.84816 loss)
I0524 03:09:46.160971 11835 sgd_solver.cpp:112] Iteration 27000, lr = 0.1
I0524 03:09:53.120074 11835 solver.cpp:239] Iteration 27010 (1.43699 iter/s, 6.95897s/10 iters), loss = 6.11936
I0524 03:09:53.120208 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11936 (* 1 = 6.11936 loss)
I0524 03:09:53.120223 11835 sgd_solver.cpp:112] Iteration 27010, lr = 0.1
I0524 03:09:58.617465 11835 solver.cpp:239] Iteration 27020 (1.81985 iter/s, 5.49496s/10 iters), loss = 5.98432
I0524 03:09:58.617516 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98432 (* 1 = 5.98432 loss)
I0524 03:09:58.617530 11835 sgd_solver.cpp:112] Iteration 27020, lr = 0.1
I0524 03:10:04.443644 11835 solver.cpp:239] Iteration 27030 (1.71665 iter/s, 5.82529s/10 iters), loss = 6.60847
I0524 03:10:04.443675 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60847 (* 1 = 6.60847 loss)
I0524 03:10:04.443907 11835 sgd_solver.cpp:112] Iteration 27030, lr = 0.1
I0524 03:10:11.388641 11835 solver.cpp:239] Iteration 27040 (1.43995 iter/s, 6.9447s/10 iters), loss = 7.35693
I0524 03:10:11.388705 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.35693 (* 1 = 7.35693 loss)
I0524 03:10:11.388818 11835 sgd_solver.cpp:112] Iteration 27040, lr = 0.1
I0524 03:10:17.439296 11835 solver.cpp:239] Iteration 27050 (1.65279 iter/s, 6.05037s/10 iters), loss = 6.57893
I0524 03:10:17.439342 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57893 (* 1 = 6.57893 loss)
I0524 03:10:17.439357 11835 sgd_solver.cpp:112] Iteration 27050, lr = 0.1
I0524 03:10:23.268074 11835 solver.cpp:239] Iteration 27060 (1.7157 iter/s, 5.82851s/10 iters), loss = 6.90814
I0524 03:10:23.268352 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90814 (* 1 = 6.90814 loss)
I0524 03:10:23.268407 11835 sgd_solver.cpp:112] Iteration 27060, lr = 0.1
I0524 03:10:30.263906 11835 solver.cpp:239] Iteration 27070 (1.42988 iter/s, 6.99359s/10 iters), loss = 7.86073
I0524 03:10:30.263963 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.86073 (* 1 = 7.86073 loss)
I0524 03:10:30.264109 11835 sgd_solver.cpp:112] Iteration 27070, lr = 0.1
I0524 03:10:36.280097 11835 solver.cpp:239] Iteration 27080 (1.66226 iter/s, 6.01591s/10 iters), loss = 6.87951
I0524 03:10:36.280136 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87951 (* 1 = 6.87951 loss)
I0524 03:10:36.280148 11835 sgd_solver.cpp:112] Iteration 27080, lr = 0.1
I0524 03:10:45.037765 11835 solver.cpp:239] Iteration 27090 (1.14192 iter/s, 8.75716s/10 iters), loss = 5.7259
I0524 03:10:45.037819 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7259 (* 1 = 5.7259 loss)
I0524 03:10:45.042361 11835 sgd_solver.cpp:112] Iteration 27090, lr = 0.1
I0524 03:10:53.773412 11835 solver.cpp:239] Iteration 27100 (1.14479 iter/s, 8.73526s/10 iters), loss = 5.71507
I0524 03:10:53.773600 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71507 (* 1 = 5.71507 loss)
I0524 03:10:54.360002 11835 sgd_solver.cpp:112] Iteration 27100, lr = 0.1
I0524 03:11:02.631814 11835 solver.cpp:239] Iteration 27110 (1.12894 iter/s, 8.85788s/10 iters), loss = 5.93814
I0524 03:11:02.631865 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93814 (* 1 = 5.93814 loss)
I0524 03:11:02.716131 11835 sgd_solver.cpp:112] Iteration 27110, lr = 0.1
I0524 03:11:10.650568 11835 solver.cpp:239] Iteration 27120 (1.24713 iter/s, 8.01839s/10 iters), loss = 6.2406
I0524 03:11:10.650629 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2406 (* 1 = 6.2406 loss)
I0524 03:11:10.650866 11835 sgd_solver.cpp:112] Iteration 27120, lr = 0.1
I0524 03:11:16.385726 11835 solver.cpp:239] Iteration 27130 (1.74371 iter/s, 5.73489s/10 iters), loss = 6.72263
I0524 03:11:16.385761 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72263 (* 1 = 6.72263 loss)
I0524 03:11:16.543762 11835 sgd_solver.cpp:112] Iteration 27130, lr = 0.1
I0524 03:11:22.332558 11835 solver.cpp:239] Iteration 27140 (1.68164 iter/s, 5.94656s/10 iters), loss = 7.57837
I0524 03:11:22.332607 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.57837 (* 1 = 7.57837 loss)
I0524 03:11:22.332620 11835 sgd_solver.cpp:112] Iteration 27140, lr = 0.1
I0524 03:11:29.111944 11835 solver.cpp:239] Iteration 27150 (1.47513 iter/s, 6.77908s/10 iters), loss = 7.49317
I0524 03:11:29.112252 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.49317 (* 1 = 7.49317 loss)
I0524 03:11:29.112313 11835 sgd_solver.cpp:112] Iteration 27150, lr = 0.1
I0524 03:11:35.670341 11835 solver.cpp:239] Iteration 27160 (1.52494 iter/s, 6.55764s/10 iters), loss = 5.64416
I0524 03:11:35.670385 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.64416 (* 1 = 5.64416 loss)
I0524 03:11:36.224601 11835 sgd_solver.cpp:112] Iteration 27160, lr = 0.1
I0524 03:11:41.855265 11835 solver.cpp:239] Iteration 27170 (1.61691 iter/s, 6.18464s/10 iters), loss = 6.60165
I0524 03:11:41.855310 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60165 (* 1 = 6.60165 loss)
I0524 03:11:41.855324 11835 sgd_solver.cpp:112] Iteration 27170, lr = 0.1
I0524 03:11:48.466504 11835 solver.cpp:239] Iteration 27180 (1.51264 iter/s, 6.61095s/10 iters), loss = 6.42984
I0524 03:11:48.466545 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42984 (* 1 = 6.42984 loss)
I0524 03:11:48.868769 11835 sgd_solver.cpp:112] Iteration 27180, lr = 0.1
I0524 03:11:54.623800 11835 solver.cpp:239] Iteration 27190 (1.62417 iter/s, 6.157s/10 iters), loss = 6.42104
I0524 03:11:54.623867 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42104 (* 1 = 6.42104 loss)
I0524 03:11:54.624574 11835 sgd_solver.cpp:112] Iteration 27190, lr = 0.1
I0524 03:12:01.494529 11835 solver.cpp:239] Iteration 27200 (1.45552 iter/s, 6.8704s/10 iters), loss = 6.43889
I0524 03:12:01.494830 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43889 (* 1 = 6.43889 loss)
I0524 03:12:01.494896 11835 sgd_solver.cpp:112] Iteration 27200, lr = 0.1
I0524 03:12:09.511153 11835 solver.cpp:239] Iteration 27210 (1.2475 iter/s, 8.01603s/10 iters), loss = 7.09503
I0524 03:12:09.511212 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09503 (* 1 = 7.09503 loss)
I0524 03:12:09.511478 11835 sgd_solver.cpp:112] Iteration 27210, lr = 0.1
I0524 03:12:15.938184 11835 solver.cpp:239] Iteration 27220 (1.556 iter/s, 6.42674s/10 iters), loss = 6.91817
I0524 03:12:15.938228 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.91817 (* 1 = 6.91817 loss)
I0524 03:12:15.938242 11835 sgd_solver.cpp:112] Iteration 27220, lr = 0.1
I0524 03:12:22.083509 11835 solver.cpp:239] Iteration 27230 (1.62763 iter/s, 6.14392s/10 iters), loss = 6.59707
I0524 03:12:22.083569 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59707 (* 1 = 6.59707 loss)
I0524 03:12:22.084044 11835 sgd_solver.cpp:112] Iteration 27230, lr = 0.1
I0524 03:12:28.147253 11835 solver.cpp:239] Iteration 27240 (1.64923 iter/s, 6.06343s/10 iters), loss = 6.14512
I0524 03:12:28.147310 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14512 (* 1 = 6.14512 loss)
I0524 03:12:28.753347 11835 sgd_solver.cpp:112] Iteration 27240, lr = 0.1
I0524 03:12:34.370383 11835 solver.cpp:239] Iteration 27250 (1.60698 iter/s, 6.22284s/10 iters), loss = 5.98725
I0524 03:12:34.370685 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98725 (* 1 = 5.98725 loss)
I0524 03:12:34.370767 11835 sgd_solver.cpp:112] Iteration 27250, lr = 0.1
I0524 03:12:43.186538 11835 solver.cpp:239] Iteration 27260 (1.13436 iter/s, 8.81552s/10 iters), loss = 5.97835
I0524 03:12:43.186594 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97835 (* 1 = 5.97835 loss)
I0524 03:12:43.186868 11835 sgd_solver.cpp:112] Iteration 27260, lr = 0.1
I0524 03:12:50.136472 11835 solver.cpp:239] Iteration 27270 (1.43893 iter/s, 6.9496s/10 iters), loss = 6.62278
I0524 03:12:50.136519 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62278 (* 1 = 6.62278 loss)
I0524 03:12:50.817471 11835 sgd_solver.cpp:112] Iteration 27270, lr = 0.1
I0524 03:12:58.386363 11835 solver.cpp:239] Iteration 27280 (1.21219 iter/s, 8.24953s/10 iters), loss = 7.48572
I0524 03:12:58.386407 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.48572 (* 1 = 7.48572 loss)
I0524 03:12:58.386633 11835 sgd_solver.cpp:112] Iteration 27280, lr = 0.1
I0524 03:13:05.667425 11835 solver.cpp:239] Iteration 27290 (1.37349 iter/s, 7.28072s/10 iters), loss = 7.14712
I0524 03:13:05.668740 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.14712 (* 1 = 7.14712 loss)
I0524 03:13:05.668783 11835 sgd_solver.cpp:112] Iteration 27290, lr = 0.1
I0524 03:13:11.512513 11835 solver.cpp:239] Iteration 27300 (1.71172 iter/s, 5.84208s/10 iters), loss = 7.07037
I0524 03:13:11.512563 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.07037 (* 1 = 7.07037 loss)
I0524 03:13:11.512773 11835 sgd_solver.cpp:112] Iteration 27300, lr = 0.1
I0524 03:13:18.239195 11835 solver.cpp:239] Iteration 27310 (1.48668 iter/s, 6.72638s/10 iters), loss = 7.37716
I0524 03:13:18.239241 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.37716 (* 1 = 7.37716 loss)
I0524 03:13:18.239364 11835 sgd_solver.cpp:112] Iteration 27310, lr = 0.1
I0524 03:13:25.003825 11835 solver.cpp:239] Iteration 27320 (1.47835 iter/s, 6.76431s/10 iters), loss = 6.7552
I0524 03:13:25.003875 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7552 (* 1 = 6.7552 loss)
I0524 03:13:25.044643 11835 sgd_solver.cpp:112] Iteration 27320, lr = 0.1
I0524 03:13:31.979149 11835 solver.cpp:239] Iteration 27330 (1.43369 iter/s, 6.97501s/10 iters), loss = 6.96908
I0524 03:13:31.979195 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96908 (* 1 = 6.96908 loss)
I0524 03:13:31.979241 11835 sgd_solver.cpp:112] Iteration 27330, lr = 0.1
I0524 03:13:39.397269 11835 solver.cpp:239] Iteration 27340 (1.34811 iter/s, 7.41778s/10 iters), loss = 6.56609
I0524 03:13:39.397433 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56609 (* 1 = 6.56609 loss)
I0524 03:13:39.574107 11835 sgd_solver.cpp:112] Iteration 27340, lr = 0.1
I0524 03:13:46.399256 11835 solver.cpp:239] Iteration 27350 (1.42825 iter/s, 7.00155s/10 iters), loss = 6.3165
I0524 03:13:46.399305 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3165 (* 1 = 6.3165 loss)
I0524 03:13:46.409023 11835 sgd_solver.cpp:112] Iteration 27350, lr = 0.1
I0524 03:13:52.300671 11835 solver.cpp:239] Iteration 27360 (1.69459 iter/s, 5.90114s/10 iters), loss = 7.52612
I0524 03:13:52.300710 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.52612 (* 1 = 7.52612 loss)
I0524 03:13:52.461766 11835 sgd_solver.cpp:112] Iteration 27360, lr = 0.1
I0524 03:13:59.738500 11835 solver.cpp:239] Iteration 27370 (1.34454 iter/s, 7.43749s/10 iters), loss = 6.86115
I0524 03:13:59.738556 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86115 (* 1 = 6.86115 loss)
I0524 03:13:59.738847 11835 sgd_solver.cpp:112] Iteration 27370, lr = 0.1
I0524 03:14:06.633081 11835 solver.cpp:239] Iteration 27380 (1.45048 iter/s, 6.89427s/10 iters), loss = 5.87358
I0524 03:14:06.633126 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87358 (* 1 = 5.87358 loss)
I0524 03:14:06.633265 11835 sgd_solver.cpp:112] Iteration 27380, lr = 0.1
I0524 03:14:14.151566 11835 solver.cpp:239] Iteration 27390 (1.33012 iter/s, 7.51814s/10 iters), loss = 5.83471
I0524 03:14:14.151782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83471 (* 1 = 5.83471 loss)
I0524 03:14:14.151809 11835 sgd_solver.cpp:112] Iteration 27390, lr = 0.1
I0524 03:14:20.496201 11835 solver.cpp:239] Iteration 27400 (1.5763 iter/s, 6.34398s/10 iters), loss = 6.52977
I0524 03:14:20.496248 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52977 (* 1 = 6.52977 loss)
I0524 03:14:20.496376 11835 sgd_solver.cpp:112] Iteration 27400, lr = 0.1
I0524 03:14:26.660496 11835 solver.cpp:239] Iteration 27410 (1.62232 iter/s, 6.16402s/10 iters), loss = 6.81368
I0524 03:14:26.660543 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81368 (* 1 = 6.81368 loss)
I0524 03:14:26.660820 11835 sgd_solver.cpp:112] Iteration 27410, lr = 0.1
I0524 03:14:33.233232 11835 solver.cpp:239] Iteration 27420 (1.52151 iter/s, 6.57243s/10 iters), loss = 6.77968
I0524 03:14:33.233281 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.77968 (* 1 = 6.77968 loss)
I0524 03:14:33.309062 11835 sgd_solver.cpp:112] Iteration 27420, lr = 0.1
I0524 03:14:40.075372 11835 solver.cpp:239] Iteration 27430 (1.4616 iter/s, 6.84183s/10 iters), loss = 7.23622
I0524 03:14:40.075417 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.23622 (* 1 = 7.23622 loss)
I0524 03:14:40.703326 11835 sgd_solver.cpp:112] Iteration 27430, lr = 0.1
I0524 03:14:48.362713 11835 solver.cpp:239] Iteration 27440 (1.20672 iter/s, 8.28695s/10 iters), loss = 6.01244
I0524 03:14:48.363009 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01244 (* 1 = 6.01244 loss)
I0524 03:14:48.945729 11835 sgd_solver.cpp:112] Iteration 27440, lr = 0.1
I0524 03:14:56.207965 11835 solver.cpp:239] Iteration 27450 (1.27474 iter/s, 7.84471s/10 iters), loss = 7.05263
I0524 03:14:56.208017 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.05263 (* 1 = 7.05263 loss)
I0524 03:14:56.744439 11835 sgd_solver.cpp:112] Iteration 27450, lr = 0.1
I0524 03:15:02.261548 11835 solver.cpp:239] Iteration 27460 (1.65199 iter/s, 6.0533s/10 iters), loss = 5.76057
I0524 03:15:02.261596 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.76057 (* 1 = 5.76057 loss)
I0524 03:15:02.261610 11835 sgd_solver.cpp:112] Iteration 27460, lr = 0.1
I0524 03:15:09.125411 11835 solver.cpp:239] Iteration 27470 (1.45697 iter/s, 6.86356s/10 iters), loss = 6.54756
I0524 03:15:09.125455 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54756 (* 1 = 6.54756 loss)
I0524 03:15:09.125756 11835 sgd_solver.cpp:112] Iteration 27470, lr = 0.1
I0524 03:15:15.284301 11835 solver.cpp:239] Iteration 27480 (1.62374 iter/s, 6.1586s/10 iters), loss = 6.03788
I0524 03:15:15.284358 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03788 (* 1 = 6.03788 loss)
I0524 03:15:15.284377 11835 sgd_solver.cpp:112] Iteration 27480, lr = 0.1
I0524 03:15:21.157104 11835 solver.cpp:239] Iteration 27490 (1.70347 iter/s, 5.87038s/10 iters), loss = 6.18595
I0524 03:15:21.157296 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18595 (* 1 = 6.18595 loss)
I0524 03:15:21.159024 11835 sgd_solver.cpp:112] Iteration 27490, lr = 0.1
I0524 03:15:28.338855 11835 solver.cpp:239] Iteration 27500 (1.39251 iter/s, 7.18128s/10 iters), loss = 7.76702
I0524 03:15:28.338907 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.76702 (* 1 = 7.76702 loss)
I0524 03:15:28.339016 11835 sgd_solver.cpp:112] Iteration 27500, lr = 0.1
I0524 03:15:33.976161 11835 solver.cpp:239] Iteration 27510 (1.77398 iter/s, 5.63704s/10 iters), loss = 6.83184
I0524 03:15:33.976195 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83184 (* 1 = 6.83184 loss)
I0524 03:15:33.976472 11835 sgd_solver.cpp:112] Iteration 27510, lr = 0.1
I0524 03:15:39.467134 11835 solver.cpp:239] Iteration 27520 (1.82125 iter/s, 5.49072s/10 iters), loss = 7.50773
I0524 03:15:39.467176 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.50773 (* 1 = 7.50773 loss)
I0524 03:15:39.467504 11835 sgd_solver.cpp:112] Iteration 27520, lr = 0.1
I0524 03:15:46.818414 11835 solver.cpp:239] Iteration 27530 (1.36037 iter/s, 7.35095s/10 iters), loss = 7.62583
I0524 03:15:46.818470 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.62583 (* 1 = 7.62583 loss)
I0524 03:15:46.818655 11835 sgd_solver.cpp:112] Iteration 27530, lr = 0.1
I0524 03:15:52.349089 11835 solver.cpp:239] Iteration 27540 (1.80819 iter/s, 5.5304s/10 iters), loss = 6.52112
I0524 03:15:52.349229 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52112 (* 1 = 6.52112 loss)
I0524 03:15:52.349367 11835 sgd_solver.cpp:112] Iteration 27540, lr = 0.1
I0524 03:15:59.521299 11835 solver.cpp:239] Iteration 27550 (1.39435 iter/s, 7.17181s/10 iters), loss = 6.88248
I0524 03:15:59.521342 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88248 (* 1 = 6.88248 loss)
I0524 03:15:59.521468 11835 sgd_solver.cpp:112] Iteration 27550, lr = 0.1
I0524 03:16:07.669049 11835 solver.cpp:239] Iteration 27560 (1.22739 iter/s, 8.14739s/10 iters), loss = 6.90279
I0524 03:16:07.669102 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90279 (* 1 = 6.90279 loss)
I0524 03:16:07.669252 11835 sgd_solver.cpp:112] Iteration 27560, lr = 0.1
I0524 03:16:15.944545 11835 solver.cpp:239] Iteration 27570 (1.20844 iter/s, 8.27513s/10 iters), loss = 5.47015
I0524 03:16:15.944588 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.47015 (* 1 = 5.47015 loss)
I0524 03:16:16.322971 11835 sgd_solver.cpp:112] Iteration 27570, lr = 0.1
I0524 03:16:22.272217 11835 solver.cpp:239] Iteration 27580 (1.58043 iter/s, 6.32738s/10 iters), loss = 7.24534
I0524 03:16:22.272266 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.24534 (* 1 = 7.24534 loss)
I0524 03:16:22.272385 11835 sgd_solver.cpp:112] Iteration 27580, lr = 0.1
I0524 03:16:30.580348 11835 solver.cpp:239] Iteration 27590 (1.20369 iter/s, 8.30777s/10 iters), loss = 7.65648
I0524 03:16:30.580438 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.65648 (* 1 = 7.65648 loss)
I0524 03:16:30.580452 11835 sgd_solver.cpp:112] Iteration 27590, lr = 0.1
I0524 03:16:36.832854 11835 solver.cpp:239] Iteration 27600 (1.59945 iter/s, 6.25217s/10 iters), loss = 6.07509
I0524 03:16:36.832914 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07509 (* 1 = 6.07509 loss)
I0524 03:16:36.832932 11835 sgd_solver.cpp:112] Iteration 27600, lr = 0.1
I0524 03:16:43.216341 11835 solver.cpp:239] Iteration 27610 (1.56705 iter/s, 6.3814s/10 iters), loss = 6.16878
I0524 03:16:43.216389 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16878 (* 1 = 6.16878 loss)
I0524 03:16:43.216547 11835 sgd_solver.cpp:112] Iteration 27610, lr = 0.1
I0524 03:16:50.946142 11835 solver.cpp:239] Iteration 27620 (1.29375 iter/s, 7.72945s/10 iters), loss = 6.23057
I0524 03:16:50.946202 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23057 (* 1 = 6.23057 loss)
I0524 03:16:50.946462 11835 sgd_solver.cpp:112] Iteration 27620, lr = 0.1
I0524 03:16:57.660393 11835 solver.cpp:239] Iteration 27630 (1.48944 iter/s, 6.71394s/10 iters), loss = 6.8689
I0524 03:16:57.660439 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8689 (* 1 = 6.8689 loss)
I0524 03:16:57.660518 11835 sgd_solver.cpp:112] Iteration 27630, lr = 0.1
I0524 03:17:04.469599 11835 solver.cpp:239] Iteration 27640 (1.46867 iter/s, 6.8089s/10 iters), loss = 7.59391
I0524 03:17:04.469785 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.59391 (* 1 = 7.59391 loss)
I0524 03:17:04.469837 11835 sgd_solver.cpp:112] Iteration 27640, lr = 0.1
I0524 03:17:10.391721 11835 solver.cpp:239] Iteration 27650 (1.6887 iter/s, 5.92171s/10 iters), loss = 5.87485
I0524 03:17:10.391762 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87485 (* 1 = 5.87485 loss)
I0524 03:17:10.473389 11835 sgd_solver.cpp:112] Iteration 27650, lr = 0.1
I0524 03:17:17.483178 11835 solver.cpp:239] Iteration 27660 (1.41021 iter/s, 7.09113s/10 iters), loss = 6.08976
I0524 03:17:17.483237 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08976 (* 1 = 6.08976 loss)
I0524 03:17:17.483319 11835 sgd_solver.cpp:112] Iteration 27660, lr = 0.1
I0524 03:17:23.378069 11835 solver.cpp:239] Iteration 27670 (1.69646 iter/s, 5.89461s/10 iters), loss = 6.22845
I0524 03:17:23.378108 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22845 (* 1 = 6.22845 loss)
I0524 03:17:23.409651 11835 sgd_solver.cpp:112] Iteration 27670, lr = 0.1
I0524 03:17:31.229935 11835 solver.cpp:239] Iteration 27680 (1.27364 iter/s, 7.85151s/10 iters), loss = 6.65273
I0524 03:17:31.229987 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65273 (* 1 = 6.65273 loss)
I0524 03:17:31.230080 11835 sgd_solver.cpp:112] Iteration 27680, lr = 0.1
I0524 03:17:37.504412 11835 solver.cpp:239] Iteration 27690 (1.59383 iter/s, 6.27419s/10 iters), loss = 5.42079
I0524 03:17:37.504623 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.42079 (* 1 = 5.42079 loss)
I0524 03:17:37.605646 11835 sgd_solver.cpp:112] Iteration 27690, lr = 0.1
I0524 03:17:43.167968 11835 solver.cpp:239] Iteration 27700 (1.76581 iter/s, 5.66312s/10 iters), loss = 6.92192
I0524 03:17:43.168020 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92192 (* 1 = 6.92192 loss)
I0524 03:17:43.168037 11835 sgd_solver.cpp:112] Iteration 27700, lr = 0.1
I0524 03:17:48.991545 11835 solver.cpp:239] Iteration 27710 (1.71732 iter/s, 5.82304s/10 iters), loss = 6.58628
I0524 03:17:48.991600 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58628 (* 1 = 6.58628 loss)
I0524 03:17:48.991794 11835 sgd_solver.cpp:112] Iteration 27710, lr = 0.1
I0524 03:17:55.555889 11835 solver.cpp:239] Iteration 27720 (1.52345 iter/s, 6.56404s/10 iters), loss = 6.75746
I0524 03:17:55.555938 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.75746 (* 1 = 6.75746 loss)
I0524 03:17:55.556118 11835 sgd_solver.cpp:112] Iteration 27720, lr = 0.1
I0524 03:18:01.861052 11835 solver.cpp:239] Iteration 27730 (1.58607 iter/s, 6.30488s/10 iters), loss = 6.6159
I0524 03:18:01.861093 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6159 (* 1 = 6.6159 loss)
I0524 03:18:01.861197 11835 sgd_solver.cpp:112] Iteration 27730, lr = 0.1
I0524 03:18:09.222528 11835 solver.cpp:239] Iteration 27740 (1.35848 iter/s, 7.36115s/10 iters), loss = 6.4677
I0524 03:18:09.222805 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4677 (* 1 = 6.4677 loss)
I0524 03:18:09.790701 11835 sgd_solver.cpp:112] Iteration 27740, lr = 0.1
I0524 03:18:16.400003 11835 solver.cpp:239] Iteration 27750 (1.39335 iter/s, 7.17696s/10 iters), loss = 7.27812
I0524 03:18:16.400059 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.27812 (* 1 = 7.27812 loss)
I0524 03:18:16.402004 11835 sgd_solver.cpp:112] Iteration 27750, lr = 0.1
I0524 03:18:22.895671 11835 solver.cpp:239] Iteration 27760 (1.53956 iter/s, 6.49537s/10 iters), loss = 6.91182
I0524 03:18:22.895720 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.91182 (* 1 = 6.91182 loss)
I0524 03:18:22.895804 11835 sgd_solver.cpp:112] Iteration 27760, lr = 0.1
I0524 03:18:29.639824 11835 solver.cpp:239] Iteration 27770 (1.48283 iter/s, 6.74385s/10 iters), loss = 6.60916
I0524 03:18:29.639881 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60916 (* 1 = 6.60916 loss)
I0524 03:18:29.639896 11835 sgd_solver.cpp:112] Iteration 27770, lr = 0.1
I0524 03:18:38.109151 11835 solver.cpp:239] Iteration 27780 (1.18087 iter/s, 8.46834s/10 iters), loss = 6.81313
I0524 03:18:38.109200 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81313 (* 1 = 6.81313 loss)
I0524 03:18:38.109302 11835 sgd_solver.cpp:112] Iteration 27780, lr = 0.1
I0524 03:18:45.168143 11835 solver.cpp:239] Iteration 27790 (1.4167 iter/s, 7.05867s/10 iters), loss = 6.43326
I0524 03:18:45.168315 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43326 (* 1 = 6.43326 loss)
I0524 03:18:45.168344 11835 sgd_solver.cpp:112] Iteration 27790, lr = 0.1
I0524 03:18:51.135856 11835 solver.cpp:239] Iteration 27800 (1.67639 iter/s, 5.96521s/10 iters), loss = 6.18445
I0524 03:18:51.135891 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18445 (* 1 = 6.18445 loss)
I0524 03:18:51.135903 11835 sgd_solver.cpp:112] Iteration 27800, lr = 0.1
I0524 03:18:59.569763 11835 solver.cpp:239] Iteration 27810 (1.18574 iter/s, 8.43354s/10 iters), loss = 7.42869
I0524 03:18:59.569818 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.42869 (* 1 = 7.42869 loss)
I0524 03:18:59.569934 11835 sgd_solver.cpp:112] Iteration 27810, lr = 0.1
I0524 03:19:07.207360 11835 solver.cpp:239] Iteration 27820 (1.30937 iter/s, 7.63725s/10 iters), loss = 7.69033
I0524 03:19:07.207408 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.69033 (* 1 = 7.69033 loss)
I0524 03:19:07.233635 11835 sgd_solver.cpp:112] Iteration 27820, lr = 0.1
I0524 03:19:13.138415 11835 solver.cpp:239] Iteration 27830 (1.68612 iter/s, 5.93078s/10 iters), loss = 7.58173
I0524 03:19:13.138460 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.58173 (* 1 = 7.58173 loss)
I0524 03:19:13.138674 11835 sgd_solver.cpp:112] Iteration 27830, lr = 0.1
I0524 03:19:22.514470 11835 solver.cpp:239] Iteration 27840 (1.06659 iter/s, 9.37565s/10 iters), loss = 6.73814
I0524 03:19:22.514762 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73814 (* 1 = 6.73814 loss)
I0524 03:19:23.459316 11835 sgd_solver.cpp:112] Iteration 27840, lr = 0.1
I0524 03:19:31.140040 11835 solver.cpp:239] Iteration 27850 (1.15942 iter/s, 8.62503s/10 iters), loss = 5.79851
I0524 03:19:31.140092 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79851 (* 1 = 5.79851 loss)
I0524 03:19:31.791751 11835 sgd_solver.cpp:112] Iteration 27850, lr = 0.1
I0524 03:19:38.149634 11835 solver.cpp:239] Iteration 27860 (1.42668 iter/s, 7.00927s/10 iters), loss = 6.44653
I0524 03:19:38.149678 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44653 (* 1 = 6.44653 loss)
I0524 03:19:38.149693 11835 sgd_solver.cpp:112] Iteration 27860, lr = 0.1
I0524 03:19:44.763615 11835 solver.cpp:239] Iteration 27870 (1.51203 iter/s, 6.61362s/10 iters), loss = 7.1028
I0524 03:19:44.763654 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.1028 (* 1 = 7.1028 loss)
I0524 03:19:44.763665 11835 sgd_solver.cpp:112] Iteration 27870, lr = 0.1
I0524 03:19:52.139700 11835 solver.cpp:239] Iteration 27880 (1.356 iter/s, 7.37465s/10 iters), loss = 6.71763
I0524 03:19:52.139746 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71763 (* 1 = 6.71763 loss)
I0524 03:19:52.139880 11835 sgd_solver.cpp:112] Iteration 27880, lr = 0.1
I0524 03:19:59.195283 11835 solver.cpp:239] Iteration 27890 (1.41738 iter/s, 7.05527s/10 iters), loss = 6.41053
I0524 03:19:59.195499 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41053 (* 1 = 6.41053 loss)
I0524 03:19:59.195551 11835 sgd_solver.cpp:112] Iteration 27890, lr = 0.1
I0524 03:20:05.566324 11835 solver.cpp:239] Iteration 27900 (1.56974 iter/s, 6.37049s/10 iters), loss = 6.4926
I0524 03:20:05.566378 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4926 (* 1 = 6.4926 loss)
I0524 03:20:05.570932 11835 sgd_solver.cpp:112] Iteration 27900, lr = 0.1
I0524 03:20:12.015795 11835 solver.cpp:239] Iteration 27910 (1.55059 iter/s, 6.44918s/10 iters), loss = 5.42694
I0524 03:20:12.015836 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.42694 (* 1 = 5.42694 loss)
I0524 03:20:12.017838 11835 sgd_solver.cpp:112] Iteration 27910, lr = 0.1
I0524 03:20:20.504009 11835 solver.cpp:239] Iteration 27920 (1.17816 iter/s, 8.48784s/10 iters), loss = 7.12574
I0524 03:20:20.504058 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.12574 (* 1 = 7.12574 loss)
I0524 03:20:20.618892 11835 sgd_solver.cpp:112] Iteration 27920, lr = 0.1
I0524 03:20:26.191474 11835 solver.cpp:239] Iteration 27930 (1.75833 iter/s, 5.6872s/10 iters), loss = 6.27875
I0524 03:20:26.191514 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27875 (* 1 = 6.27875 loss)
I0524 03:20:26.191613 11835 sgd_solver.cpp:112] Iteration 27930, lr = 0.1
I0524 03:20:35.438442 11835 solver.cpp:239] Iteration 27940 (1.08148 iter/s, 9.24657s/10 iters), loss = 6.56113
I0524 03:20:35.438797 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56113 (* 1 = 6.56113 loss)
I0524 03:20:35.902667 11835 sgd_solver.cpp:112] Iteration 27940, lr = 0.1
I0524 03:20:43.252698 11835 solver.cpp:239] Iteration 27950 (1.27981 iter/s, 7.81369s/10 iters), loss = 6.22161
I0524 03:20:43.252758 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22161 (* 1 = 6.22161 loss)
I0524 03:20:43.267752 11835 sgd_solver.cpp:112] Iteration 27950, lr = 0.1
I0524 03:20:49.310917 11835 solver.cpp:239] Iteration 27960 (1.65073 iter/s, 6.05791s/10 iters), loss = 6.17821
I0524 03:20:49.310969 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17821 (* 1 = 6.17821 loss)
I0524 03:20:49.324863 11835 sgd_solver.cpp:112] Iteration 27960, lr = 0.1
I0524 03:20:55.020921 11835 solver.cpp:239] Iteration 27970 (1.75139 iter/s, 5.70974s/10 iters), loss = 6.69525
I0524 03:20:55.020959 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69525 (* 1 = 6.69525 loss)
I0524 03:20:55.020972 11835 sgd_solver.cpp:112] Iteration 27970, lr = 0.1
I0524 03:21:02.242441 11835 solver.cpp:239] Iteration 27980 (1.38481 iter/s, 7.22121s/10 iters), loss = 7.66042
I0524 03:21:02.242482 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.66042 (* 1 = 7.66042 loss)
I0524 03:21:02.715134 11835 sgd_solver.cpp:112] Iteration 27980, lr = 0.1
I0524 03:21:10.310127 11835 solver.cpp:239] Iteration 27990 (1.23957 iter/s, 8.06733s/10 iters), loss = 5.95636
I0524 03:21:10.310401 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95636 (* 1 = 5.95636 loss)
I0524 03:21:10.310461 11835 sgd_solver.cpp:112] Iteration 27990, lr = 0.1
I0524 03:21:16.187186 11835 solver.cpp:239] Iteration 28000 (1.70167 iter/s, 5.87657s/10 iters), loss = 6.2428
I0524 03:21:16.187237 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2428 (* 1 = 6.2428 loss)
I0524 03:21:16.874518 11835 sgd_solver.cpp:112] Iteration 28000, lr = 0.1
I0524 03:21:22.962926 11835 solver.cpp:239] Iteration 28010 (1.47592 iter/s, 6.77542s/10 iters), loss = 5.87423
I0524 03:21:22.962975 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87423 (* 1 = 5.87423 loss)
I0524 03:21:22.963095 11835 sgd_solver.cpp:112] Iteration 28010, lr = 0.1
I0524 03:21:28.776036 11835 solver.cpp:239] Iteration 28020 (1.72033 iter/s, 5.81284s/10 iters), loss = 6.72336
I0524 03:21:28.776074 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72336 (* 1 = 6.72336 loss)
I0524 03:21:28.776526 11835 sgd_solver.cpp:112] Iteration 28020, lr = 0.1
I0524 03:21:35.207132 11835 solver.cpp:239] Iteration 28030 (1.55502 iter/s, 6.4308s/10 iters), loss = 6.63001
I0524 03:21:35.207188 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63001 (* 1 = 6.63001 loss)
I0524 03:21:35.207612 11835 sgd_solver.cpp:112] Iteration 28030, lr = 0.1
I0524 03:21:43.803905 11835 solver.cpp:239] Iteration 28040 (1.16328 iter/s, 8.59639s/10 iters), loss = 6.67317
I0524 03:21:43.804205 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67317 (* 1 = 6.67317 loss)
I0524 03:21:43.804260 11835 sgd_solver.cpp:112] Iteration 28040, lr = 0.1
I0524 03:21:50.145820 11835 solver.cpp:239] Iteration 28050 (1.57696 iter/s, 6.34133s/10 iters), loss = 7.12776
I0524 03:21:50.145859 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.12776 (* 1 = 7.12776 loss)
I0524 03:21:50.146044 11835 sgd_solver.cpp:112] Iteration 28050, lr = 0.1
I0524 03:21:56.942382 11835 solver.cpp:239] Iteration 28060 (1.4714 iter/s, 6.79626s/10 iters), loss = 6.44814
I0524 03:21:56.942435 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44814 (* 1 = 6.44814 loss)
I0524 03:21:56.942450 11835 sgd_solver.cpp:112] Iteration 28060, lr = 0.1
I0524 03:22:03.833279 11835 solver.cpp:239] Iteration 28070 (1.45149 iter/s, 6.88948s/10 iters), loss = 6.52123
I0524 03:22:03.833317 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52123 (* 1 = 6.52123 loss)
I0524 03:22:03.833446 11835 sgd_solver.cpp:112] Iteration 28070, lr = 0.1
I0524 03:22:10.938036 11835 solver.cpp:239] Iteration 28080 (1.40757 iter/s, 7.10444s/10 iters), loss = 7.90345
I0524 03:22:10.938086 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.90345 (* 1 = 7.90345 loss)
I0524 03:22:10.938194 11835 sgd_solver.cpp:112] Iteration 28080, lr = 0.1
I0524 03:22:16.675709 11835 solver.cpp:239] Iteration 28090 (1.74295 iter/s, 5.73741s/10 iters), loss = 6.97855
I0524 03:22:16.675999 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.97855 (* 1 = 6.97855 loss)
I0524 03:22:16.676057 11835 sgd_solver.cpp:112] Iteration 28090, lr = 0.1
I0524 03:22:26.460614 11835 solver.cpp:239] Iteration 28100 (1.02205 iter/s, 9.78428s/10 iters), loss = 6.56359
I0524 03:22:26.460650 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56359 (* 1 = 6.56359 loss)
I0524 03:22:26.461048 11835 sgd_solver.cpp:112] Iteration 28100, lr = 0.1
I0524 03:22:33.776341 11835 solver.cpp:239] Iteration 28110 (1.36698 iter/s, 7.31541s/10 iters), loss = 5.9423
I0524 03:22:33.776391 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9423 (* 1 = 5.9423 loss)
I0524 03:22:33.986519 11835 sgd_solver.cpp:112] Iteration 28110, lr = 0.1
I0524 03:22:39.664790 11835 solver.cpp:239] Iteration 28120 (1.69832 iter/s, 5.88817s/10 iters), loss = 6.58411
I0524 03:22:39.664836 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58411 (* 1 = 6.58411 loss)
I0524 03:22:39.664917 11835 sgd_solver.cpp:112] Iteration 28120, lr = 0.1
I0524 03:22:45.336556 11835 solver.cpp:239] Iteration 28130 (1.7632 iter/s, 5.6715s/10 iters), loss = 5.84146
I0524 03:22:45.336616 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84146 (* 1 = 5.84146 loss)
I0524 03:22:45.336796 11835 sgd_solver.cpp:112] Iteration 28130, lr = 0.1
I0524 03:22:51.749794 11835 solver.cpp:239] Iteration 28140 (1.55935 iter/s, 6.41293s/10 iters), loss = 6.29584
I0524 03:22:51.750063 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29584 (* 1 = 6.29584 loss)
I0524 03:22:51.750118 11835 sgd_solver.cpp:112] Iteration 28140, lr = 0.1
I0524 03:22:58.509551 11835 solver.cpp:239] Iteration 28150 (1.47951 iter/s, 6.75897s/10 iters), loss = 6.16453
I0524 03:22:58.509588 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16453 (* 1 = 6.16453 loss)
I0524 03:22:58.904335 11835 sgd_solver.cpp:112] Iteration 28150, lr = 0.1
I0524 03:23:05.970727 11835 solver.cpp:239] Iteration 28160 (1.34033 iter/s, 7.46084s/10 iters), loss = 6.4966
I0524 03:23:05.970782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4966 (* 1 = 6.4966 loss)
I0524 03:23:06.491097 11835 sgd_solver.cpp:112] Iteration 28160, lr = 0.1
I0524 03:23:12.970196 11835 solver.cpp:239] Iteration 28170 (1.42874 iter/s, 6.99916s/10 iters), loss = 5.72329
I0524 03:23:12.970240 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72329 (* 1 = 5.72329 loss)
I0524 03:23:12.970255 11835 sgd_solver.cpp:112] Iteration 28170, lr = 0.1
I0524 03:23:19.819555 11835 solver.cpp:239] Iteration 28180 (1.46009 iter/s, 6.84889s/10 iters), loss = 6.79043
I0524 03:23:19.819605 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79043 (* 1 = 6.79043 loss)
I0524 03:23:19.819618 11835 sgd_solver.cpp:112] Iteration 28180, lr = 0.1
I0524 03:23:26.846513 11835 solver.cpp:239] Iteration 28190 (1.42338 iter/s, 7.02552s/10 iters), loss = 6.17785
I0524 03:23:26.846839 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17785 (* 1 = 6.17785 loss)
I0524 03:23:26.846900 11835 sgd_solver.cpp:112] Iteration 28190, lr = 0.1
I0524 03:23:35.985819 11835 solver.cpp:239] Iteration 28200 (1.09448 iter/s, 9.13674s/10 iters), loss = 7.69232
I0524 03:23:35.985867 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.69232 (* 1 = 7.69232 loss)
I0524 03:23:36.791839 11835 sgd_solver.cpp:112] Iteration 28200, lr = 0.1
I0524 03:23:43.868793 11835 solver.cpp:239] Iteration 28210 (1.26862 iter/s, 7.88261s/10 iters), loss = 6.6309
I0524 03:23:43.868849 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6309 (* 1 = 6.6309 loss)
I0524 03:23:43.869096 11835 sgd_solver.cpp:112] Iteration 28210, lr = 0.1
I0524 03:23:52.051973 11835 solver.cpp:239] Iteration 28220 (1.22207 iter/s, 8.18282s/10 iters), loss = 6.14489
I0524 03:23:52.052021 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14489 (* 1 = 6.14489 loss)
I0524 03:23:52.052244 11835 sgd_solver.cpp:112] Iteration 28220, lr = 0.1
I0524 03:23:57.972380 11835 solver.cpp:239] Iteration 28230 (1.68915 iter/s, 5.92014s/10 iters), loss = 7.21077
I0524 03:23:57.972501 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.21077 (* 1 = 7.21077 loss)
I0524 03:23:57.972751 11835 sgd_solver.cpp:112] Iteration 28230, lr = 0.1
I0524 03:24:03.604269 11835 solver.cpp:239] Iteration 28240 (1.77572 iter/s, 5.63153s/10 iters), loss = 5.94245
I0524 03:24:03.604327 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94245 (* 1 = 5.94245 loss)
I0524 03:24:03.604411 11835 sgd_solver.cpp:112] Iteration 28240, lr = 0.1
I0524 03:24:11.032809 11835 solver.cpp:239] Iteration 28250 (1.34622 iter/s, 7.42821s/10 iters), loss = 5.85333
I0524 03:24:11.032852 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85333 (* 1 = 5.85333 loss)
I0524 03:24:11.033381 11835 sgd_solver.cpp:112] Iteration 28250, lr = 0.1
I0524 03:24:18.195982 11835 solver.cpp:239] Iteration 28260 (1.39609 iter/s, 7.16284s/10 iters), loss = 6.74818
I0524 03:24:18.196039 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74818 (* 1 = 6.74818 loss)
I0524 03:24:18.196262 11835 sgd_solver.cpp:112] Iteration 28260, lr = 0.1
I0524 03:24:24.662897 11835 solver.cpp:239] Iteration 28270 (1.5464 iter/s, 6.46661s/10 iters), loss = 6.07939
I0524 03:24:24.662951 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07939 (* 1 = 6.07939 loss)
I0524 03:24:24.663053 11835 sgd_solver.cpp:112] Iteration 28270, lr = 0.1
I0524 03:24:30.514114 11835 solver.cpp:239] Iteration 28280 (1.70912 iter/s, 5.85095s/10 iters), loss = 6.44681
I0524 03:24:30.514223 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44681 (* 1 = 6.44681 loss)
I0524 03:24:30.514243 11835 sgd_solver.cpp:112] Iteration 28280, lr = 0.1
I0524 03:24:37.695019 11835 solver.cpp:239] Iteration 28290 (1.39266 iter/s, 7.18049s/10 iters), loss = 6.57687
I0524 03:24:37.695065 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57687 (* 1 = 6.57687 loss)
I0524 03:24:37.695231 11835 sgd_solver.cpp:112] Iteration 28290, lr = 0.1
I0524 03:24:44.147887 11835 solver.cpp:239] Iteration 28300 (1.54977 iter/s, 6.45256s/10 iters), loss = 6.02629
I0524 03:24:44.147944 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02629 (* 1 = 6.02629 loss)
I0524 03:24:44.148207 11835 sgd_solver.cpp:112] Iteration 28300, lr = 0.1
I0524 03:24:51.295598 11835 solver.cpp:239] Iteration 28310 (1.39911 iter/s, 7.14738s/10 iters), loss = 5.50845
I0524 03:24:51.295650 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.50845 (* 1 = 5.50845 loss)
I0524 03:24:51.403214 11835 sgd_solver.cpp:112] Iteration 28310, lr = 0.1
I0524 03:24:57.135789 11835 solver.cpp:239] Iteration 28320 (1.71235 iter/s, 5.83992s/10 iters), loss = 7.00304
I0524 03:24:57.135836 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00304 (* 1 = 7.00304 loss)
I0524 03:24:57.136101 11835 sgd_solver.cpp:112] Iteration 28320, lr = 0.1
I0524 03:25:02.927399 11835 solver.cpp:239] Iteration 28330 (1.72672 iter/s, 5.79134s/10 iters), loss = 5.92113
I0524 03:25:02.927651 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92113 (* 1 = 5.92113 loss)
I0524 03:25:02.927695 11835 sgd_solver.cpp:112] Iteration 28330, lr = 0.1
I0524 03:25:09.458439 11835 solver.cpp:239] Iteration 28340 (1.53126 iter/s, 6.53055s/10 iters), loss = 6.97903
I0524 03:25:09.458477 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.97903 (* 1 = 6.97903 loss)
I0524 03:25:09.458636 11835 sgd_solver.cpp:112] Iteration 28340, lr = 0.1
I0524 03:25:16.231966 11835 solver.cpp:239] Iteration 28350 (1.4764 iter/s, 6.77322s/10 iters), loss = 6.93395
I0524 03:25:16.232022 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93395 (* 1 = 6.93395 loss)
I0524 03:25:16.232074 11835 sgd_solver.cpp:112] Iteration 28350, lr = 0.1
I0524 03:25:22.107221 11835 solver.cpp:239] Iteration 28360 (1.70214 iter/s, 5.87497s/10 iters), loss = 6.61954
I0524 03:25:22.107267 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61954 (* 1 = 6.61954 loss)
I0524 03:25:22.384382 11835 sgd_solver.cpp:112] Iteration 28360, lr = 0.1
I0524 03:25:30.178783 11835 solver.cpp:239] Iteration 28370 (1.23897 iter/s, 8.07121s/10 iters), loss = 7.67023
I0524 03:25:30.178841 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.67023 (* 1 = 7.67023 loss)
I0524 03:25:30.178966 11835 sgd_solver.cpp:112] Iteration 28370, lr = 0.1
I0524 03:25:38.833433 11835 solver.cpp:239] Iteration 28380 (1.1555 iter/s, 8.65426s/10 iters), loss = 7.49873
I0524 03:25:38.833739 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.49873 (* 1 = 7.49873 loss)
I0524 03:25:38.869983 11835 sgd_solver.cpp:112] Iteration 28380, lr = 0.1
I0524 03:25:46.578361 11835 solver.cpp:239] Iteration 28390 (1.29126 iter/s, 7.74438s/10 iters), loss = 6.34137
I0524 03:25:46.578404 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34137 (* 1 = 6.34137 loss)
I0524 03:25:46.578508 11835 sgd_solver.cpp:112] Iteration 28390, lr = 0.1
I0524 03:25:54.594369 11835 solver.cpp:239] Iteration 28400 (1.24756 iter/s, 8.01566s/10 iters), loss = 6.34644
I0524 03:25:54.594410 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34644 (* 1 = 6.34644 loss)
I0524 03:25:54.916959 11835 sgd_solver.cpp:112] Iteration 28400, lr = 0.1
I0524 03:26:00.742491 11835 solver.cpp:239] Iteration 28410 (1.62659 iter/s, 6.14784s/10 iters), loss = 6.60364
I0524 03:26:00.742538 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60364 (* 1 = 6.60364 loss)
I0524 03:26:00.742557 11835 sgd_solver.cpp:112] Iteration 28410, lr = 0.1
I0524 03:26:09.648330 11835 solver.cpp:239] Iteration 28420 (1.12292 iter/s, 8.90538s/10 iters), loss = 6.14584
I0524 03:26:09.648483 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14584 (* 1 = 6.14584 loss)
I0524 03:26:09.648658 11835 sgd_solver.cpp:112] Iteration 28420, lr = 0.1
I0524 03:26:16.150530 11835 solver.cpp:239] Iteration 28430 (1.53803 iter/s, 6.50181s/10 iters), loss = 5.6432
I0524 03:26:16.150569 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.6432 (* 1 = 5.6432 loss)
I0524 03:26:16.429937 11835 sgd_solver.cpp:112] Iteration 28430, lr = 0.1
I0524 03:26:22.305233 11835 solver.cpp:239] Iteration 28440 (1.62485 iter/s, 6.15442s/10 iters), loss = 7.3393
I0524 03:26:22.305279 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.3393 (* 1 = 7.3393 loss)
I0524 03:26:22.309089 11835 sgd_solver.cpp:112] Iteration 28440, lr = 0.1
I0524 03:26:31.370916 11835 solver.cpp:239] Iteration 28450 (1.10311 iter/s, 9.06529s/10 iters), loss = 5.4566
I0524 03:26:31.370972 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.4566 (* 1 = 5.4566 loss)
I0524 03:26:31.371098 11835 sgd_solver.cpp:112] Iteration 28450, lr = 0.1
I0524 03:26:36.907569 11835 solver.cpp:239] Iteration 28460 (1.80623 iter/s, 5.5364s/10 iters), loss = 6.00305
I0524 03:26:36.907608 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00305 (* 1 = 6.00305 loss)
I0524 03:26:36.907621 11835 sgd_solver.cpp:112] Iteration 28460, lr = 0.1
I0524 03:26:42.800380 11835 solver.cpp:239] Iteration 28470 (1.69706 iter/s, 5.89253s/10 iters), loss = 6.10634
I0524 03:26:42.800689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10634 (* 1 = 6.10634 loss)
I0524 03:26:42.800869 11835 sgd_solver.cpp:112] Iteration 28470, lr = 0.1
I0524 03:26:50.618330 11835 solver.cpp:239] Iteration 28480 (1.2792 iter/s, 7.81739s/10 iters), loss = 7.5458
I0524 03:26:50.618367 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.5458 (* 1 = 7.5458 loss)
I0524 03:26:50.618428 11835 sgd_solver.cpp:112] Iteration 28480, lr = 0.1
I0524 03:26:56.646834 11835 solver.cpp:239] Iteration 28490 (1.65886 iter/s, 6.02823s/10 iters), loss = 6.36246
I0524 03:26:56.646875 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36246 (* 1 = 6.36246 loss)
I0524 03:26:56.646888 11835 sgd_solver.cpp:112] Iteration 28490, lr = 0.1
I0524 03:27:04.450675 11835 solver.cpp:239] Iteration 28500 (1.28148 iter/s, 7.8035s/10 iters), loss = 6.45211
I0524 03:27:04.450745 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45211 (* 1 = 6.45211 loss)
I0524 03:27:04.451109 11835 sgd_solver.cpp:112] Iteration 28500, lr = 0.1
I0524 03:27:12.335845 11835 solver.cpp:239] Iteration 28510 (1.26826 iter/s, 7.8848s/10 iters), loss = 5.37423
I0524 03:27:12.335894 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.37423 (* 1 = 5.37423 loss)
I0524 03:27:12.336004 11835 sgd_solver.cpp:112] Iteration 28510, lr = 0.1
I0524 03:27:21.256510 11835 solver.cpp:239] Iteration 28520 (1.12104 iter/s, 8.92028s/10 iters), loss = 6.45013
I0524 03:27:21.256644 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45013 (* 1 = 6.45013 loss)
I0524 03:27:21.256671 11835 sgd_solver.cpp:112] Iteration 28520, lr = 0.1
I0524 03:27:30.785490 11835 solver.cpp:239] Iteration 28530 (1.04949 iter/s, 9.52848s/10 iters), loss = 7.22838
I0524 03:27:30.785547 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.22838 (* 1 = 7.22838 loss)
I0524 03:27:30.785722 11835 sgd_solver.cpp:112] Iteration 28530, lr = 0.1
I0524 03:27:36.643187 11835 solver.cpp:239] Iteration 28540 (1.70724 iter/s, 5.85741s/10 iters), loss = 6.5667
I0524 03:27:36.643249 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5667 (* 1 = 6.5667 loss)
I0524 03:27:36.664927 11835 sgd_solver.cpp:112] Iteration 28540, lr = 0.1
I0524 03:27:43.069844 11835 solver.cpp:239] Iteration 28550 (1.55609 iter/s, 6.42636s/10 iters), loss = 6.58531
I0524 03:27:43.069886 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58531 (* 1 = 6.58531 loss)
I0524 03:27:43.070138 11835 sgd_solver.cpp:112] Iteration 28550, lr = 0.1
I0524 03:27:51.253929 11835 solver.cpp:239] Iteration 28560 (1.22194 iter/s, 8.18372s/10 iters), loss = 5.9606
I0524 03:27:51.253983 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9606 (* 1 = 5.9606 loss)
I0524 03:27:51.254081 11835 sgd_solver.cpp:112] Iteration 28560, lr = 0.1
I0524 03:27:58.359949 11835 solver.cpp:239] Iteration 28570 (1.40732 iter/s, 7.10569s/10 iters), loss = 6.79369
I0524 03:27:58.360190 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79369 (* 1 = 6.79369 loss)
I0524 03:27:58.360237 11835 sgd_solver.cpp:112] Iteration 28570, lr = 0.1
I0524 03:28:05.557096 11835 solver.cpp:239] Iteration 28580 (1.38953 iter/s, 7.19668s/10 iters), loss = 6.22607
I0524 03:28:05.557149 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22607 (* 1 = 6.22607 loss)
I0524 03:28:05.635161 11835 sgd_solver.cpp:112] Iteration 28580, lr = 0.1
I0524 03:28:12.437731 11835 solver.cpp:239] Iteration 28590 (1.45342 iter/s, 6.88031s/10 iters), loss = 6.14154
I0524 03:28:12.437788 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14154 (* 1 = 6.14154 loss)
I0524 03:28:12.437841 11835 sgd_solver.cpp:112] Iteration 28590, lr = 0.1
I0524 03:28:20.372213 11835 solver.cpp:239] Iteration 28600 (1.26038 iter/s, 7.93413s/10 iters), loss = 6.71954
I0524 03:28:20.372258 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71954 (* 1 = 6.71954 loss)
I0524 03:28:20.372505 11835 sgd_solver.cpp:112] Iteration 28600, lr = 0.1
I0524 03:28:26.355482 11835 solver.cpp:239] Iteration 28610 (1.67141 iter/s, 5.98297s/10 iters), loss = 7.2596
I0524 03:28:26.355531 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2596 (* 1 = 7.2596 loss)
I0524 03:28:26.355754 11835 sgd_solver.cpp:112] Iteration 28610, lr = 0.1
I0524 03:28:34.868368 11835 solver.cpp:239] Iteration 28620 (1.17474 iter/s, 8.51251s/10 iters), loss = 6.37052
I0524 03:28:34.868669 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37052 (* 1 = 6.37052 loss)
I0524 03:28:34.868721 11835 sgd_solver.cpp:112] Iteration 28620, lr = 0.1
I0524 03:28:42.183140 11835 solver.cpp:239] Iteration 28630 (1.36721 iter/s, 7.31415s/10 iters), loss = 6.90922
I0524 03:28:42.183202 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90922 (* 1 = 6.90922 loss)
I0524 03:28:42.183382 11835 sgd_solver.cpp:112] Iteration 28630, lr = 0.1
I0524 03:28:49.226162 11835 solver.cpp:239] Iteration 28640 (1.41991 iter/s, 7.04269s/10 iters), loss = 6.23194
I0524 03:28:49.226220 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23194 (* 1 = 6.23194 loss)
I0524 03:28:49.236265 11835 sgd_solver.cpp:112] Iteration 28640, lr = 0.1
I0524 03:28:57.396203 11835 solver.cpp:239] Iteration 28650 (1.22404 iter/s, 8.16967s/10 iters), loss = 6.94461
I0524 03:28:57.396258 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.94461 (* 1 = 6.94461 loss)
I0524 03:28:58.455988 11835 sgd_solver.cpp:112] Iteration 28650, lr = 0.1
I0524 03:29:04.273442 11835 solver.cpp:239] Iteration 28660 (1.45414 iter/s, 6.87693s/10 iters), loss = 6.52535
I0524 03:29:04.273478 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52535 (* 1 = 6.52535 loss)
I0524 03:29:04.273490 11835 sgd_solver.cpp:112] Iteration 28660, lr = 0.1
I0524 03:29:10.011023 11835 solver.cpp:239] Iteration 28670 (1.74298 iter/s, 5.73731s/10 iters), loss = 6.34369
I0524 03:29:10.011304 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34369 (* 1 = 6.34369 loss)
I0524 03:29:10.011387 11835 sgd_solver.cpp:112] Iteration 28670, lr = 0.1
I0524 03:29:16.195960 11835 solver.cpp:239] Iteration 28680 (1.61701 iter/s, 6.18427s/10 iters), loss = 7.42517
I0524 03:29:16.196013 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.42517 (* 1 = 7.42517 loss)
I0524 03:29:16.196028 11835 sgd_solver.cpp:112] Iteration 28680, lr = 0.1
I0524 03:29:23.629228 11835 solver.cpp:239] Iteration 28690 (1.34536 iter/s, 7.43294s/10 iters), loss = 6.63995
I0524 03:29:23.629274 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63995 (* 1 = 6.63995 loss)
I0524 03:29:23.629302 11835 sgd_solver.cpp:112] Iteration 28690, lr = 0.1
I0524 03:29:29.690951 11835 solver.cpp:239] Iteration 28700 (1.64979 iter/s, 6.06138s/10 iters), loss = 6.57894
I0524 03:29:29.690999 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57894 (* 1 = 6.57894 loss)
I0524 03:29:29.877391 11835 sgd_solver.cpp:112] Iteration 28700, lr = 0.1
I0524 03:29:37.738070 11835 solver.cpp:239] Iteration 28710 (1.24274 iter/s, 8.04674s/10 iters), loss = 6.6664
I0524 03:29:37.738150 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6664 (* 1 = 6.6664 loss)
I0524 03:29:37.738379 11835 sgd_solver.cpp:112] Iteration 28710, lr = 0.1
I0524 03:29:44.737401 11835 solver.cpp:239] Iteration 28720 (1.42878 iter/s, 6.999s/10 iters), loss = 6.35099
I0524 03:29:44.737694 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35099 (* 1 = 6.35099 loss)
I0524 03:29:44.737743 11835 sgd_solver.cpp:112] Iteration 28720, lr = 0.1
I0524 03:29:50.308418 11835 solver.cpp:239] Iteration 28730 (1.79515 iter/s, 5.57058s/10 iters), loss = 6.67429
I0524 03:29:50.308454 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67429 (* 1 = 6.67429 loss)
I0524 03:29:50.308466 11835 sgd_solver.cpp:112] Iteration 28730, lr = 0.1
I0524 03:29:58.335294 11835 solver.cpp:239] Iteration 28740 (1.24587 iter/s, 8.02652s/10 iters), loss = 5.60629
I0524 03:29:58.335341 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.60629 (* 1 = 5.60629 loss)
I0524 03:29:58.432616 11835 sgd_solver.cpp:112] Iteration 28740, lr = 0.1
I0524 03:30:05.593595 11835 solver.cpp:239] Iteration 28750 (1.3778 iter/s, 7.25797s/10 iters), loss = 5.75656
I0524 03:30:05.593648 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75656 (* 1 = 5.75656 loss)
I0524 03:30:05.593768 11835 sgd_solver.cpp:112] Iteration 28750, lr = 0.1
I0524 03:30:12.689919 11835 solver.cpp:239] Iteration 28760 (1.40924 iter/s, 7.09601s/10 iters), loss = 7.00972
I0524 03:30:12.689965 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00972 (* 1 = 7.00972 loss)
I0524 03:30:12.690073 11835 sgd_solver.cpp:112] Iteration 28760, lr = 0.1
I0524 03:30:19.708614 11835 solver.cpp:239] Iteration 28770 (1.42483 iter/s, 7.01837s/10 iters), loss = 6.99246
I0524 03:30:19.708865 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.99246 (* 1 = 6.99246 loss)
I0524 03:30:19.708911 11835 sgd_solver.cpp:112] Iteration 28770, lr = 0.1
I0524 03:30:26.151518 11835 solver.cpp:239] Iteration 28780 (1.55234 iter/s, 6.44188s/10 iters), loss = 6.94417
I0524 03:30:26.151577 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.94417 (* 1 = 6.94417 loss)
I0524 03:30:26.151669 11835 sgd_solver.cpp:112] Iteration 28780, lr = 0.1
I0524 03:30:33.348114 11835 solver.cpp:239] Iteration 28790 (1.38961 iter/s, 7.19627s/10 iters), loss = 6.94853
I0524 03:30:33.348160 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.94853 (* 1 = 6.94853 loss)
I0524 03:30:33.348340 11835 sgd_solver.cpp:112] Iteration 28790, lr = 0.1
I0524 03:30:39.203987 11835 solver.cpp:239] Iteration 28800 (1.70777 iter/s, 5.85559s/10 iters), loss = 6.33466
I0524 03:30:39.204036 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33466 (* 1 = 6.33466 loss)
I0524 03:30:39.204051 11835 sgd_solver.cpp:112] Iteration 28800, lr = 0.1
I0524 03:30:45.695746 11835 solver.cpp:239] Iteration 28810 (1.54049 iter/s, 6.49146s/10 iters), loss = 6.00396
I0524 03:30:45.695791 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00396 (* 1 = 6.00396 loss)
I0524 03:30:46.046320 11835 sgd_solver.cpp:112] Iteration 28810, lr = 0.1
I0524 03:30:52.109652 11835 solver.cpp:239] Iteration 28820 (1.55919 iter/s, 6.4136s/10 iters), loss = 6.40948
I0524 03:30:52.109930 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40948 (* 1 = 6.40948 loss)
I0524 03:30:52.109987 11835 sgd_solver.cpp:112] Iteration 28820, lr = 0.1
I0524 03:30:57.675884 11835 solver.cpp:239] Iteration 28830 (1.79674 iter/s, 5.56565s/10 iters), loss = 5.25166
I0524 03:30:57.675928 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.25166 (* 1 = 5.25166 loss)
I0524 03:30:57.675998 11835 sgd_solver.cpp:112] Iteration 28830, lr = 0.1
I0524 03:31:03.478839 11835 solver.cpp:239] Iteration 28840 (1.72334 iter/s, 5.80268s/10 iters), loss = 5.89317
I0524 03:31:03.478884 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89317 (* 1 = 5.89317 loss)
I0524 03:31:03.478899 11835 sgd_solver.cpp:112] Iteration 28840, lr = 0.1
I0524 03:31:09.217108 11835 solver.cpp:239] Iteration 28850 (1.74279 iter/s, 5.73794s/10 iters), loss = 6.28135
I0524 03:31:09.217151 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.28135 (* 1 = 6.28135 loss)
I0524 03:31:09.217283 11835 sgd_solver.cpp:112] Iteration 28850, lr = 0.1
I0524 03:31:14.982409 11835 solver.cpp:239] Iteration 28860 (1.7346 iter/s, 5.76503s/10 iters), loss = 6.08882
I0524 03:31:14.982460 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08882 (* 1 = 6.08882 loss)
I0524 03:31:14.982475 11835 sgd_solver.cpp:112] Iteration 28860, lr = 0.1
I0524 03:31:23.030632 11835 solver.cpp:239] Iteration 28870 (1.24258 iter/s, 8.04779s/10 iters), loss = 6.26589
I0524 03:31:23.030923 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26589 (* 1 = 6.26589 loss)
I0524 03:31:23.216361 11835 sgd_solver.cpp:112] Iteration 28870, lr = 0.1
I0524 03:31:29.301790 11835 solver.cpp:239] Iteration 28880 (1.59473 iter/s, 6.27067s/10 iters), loss = 7.07963
I0524 03:31:29.301846 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.07963 (* 1 = 7.07963 loss)
I0524 03:31:29.302013 11835 sgd_solver.cpp:112] Iteration 28880, lr = 0.1
I0524 03:31:36.236479 11835 solver.cpp:239] Iteration 28890 (1.44209 iter/s, 6.93437s/10 iters), loss = 6.19817
I0524 03:31:36.236534 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19817 (* 1 = 6.19817 loss)
I0524 03:31:36.236613 11835 sgd_solver.cpp:112] Iteration 28890, lr = 0.1
I0524 03:31:41.914646 11835 solver.cpp:239] Iteration 28900 (1.76121 iter/s, 5.6779s/10 iters), loss = 5.74726
I0524 03:31:41.914716 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74726 (* 1 = 5.74726 loss)
I0524 03:31:41.914732 11835 sgd_solver.cpp:112] Iteration 28900, lr = 0.1
I0524 03:31:48.595551 11835 solver.cpp:239] Iteration 28910 (1.49687 iter/s, 6.6806s/10 iters), loss = 6.0429
I0524 03:31:48.595592 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0429 (* 1 = 6.0429 loss)
I0524 03:31:48.595734 11835 sgd_solver.cpp:112] Iteration 28910, lr = 0.1
I0524 03:31:54.793094 11835 solver.cpp:239] Iteration 28920 (1.61362 iter/s, 6.19726s/10 iters), loss = 6.9353
I0524 03:31:54.793361 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9353 (* 1 = 6.9353 loss)
I0524 03:31:55.629621 11835 sgd_solver.cpp:112] Iteration 28920, lr = 0.1
I0524 03:32:03.531715 11835 solver.cpp:239] Iteration 28930 (1.14442 iter/s, 8.73806s/10 iters), loss = 6.61461
I0524 03:32:03.531765 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61461 (* 1 = 6.61461 loss)
I0524 03:32:03.708750 11835 sgd_solver.cpp:112] Iteration 28930, lr = 0.1
I0524 03:32:10.019822 11835 solver.cpp:239] Iteration 28940 (1.54135 iter/s, 6.48782s/10 iters), loss = 6.39142
I0524 03:32:10.019861 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39142 (* 1 = 6.39142 loss)
I0524 03:32:10.050549 11835 sgd_solver.cpp:112] Iteration 28940, lr = 0.1
I0524 03:32:16.595523 11835 solver.cpp:239] Iteration 28950 (1.52082 iter/s, 6.5754s/10 iters), loss = 5.82315
I0524 03:32:16.595577 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82315 (* 1 = 5.82315 loss)
I0524 03:32:16.595592 11835 sgd_solver.cpp:112] Iteration 28950, lr = 0.1
I0524 03:32:22.624366 11835 solver.cpp:239] Iteration 28960 (1.65908 iter/s, 6.02745s/10 iters), loss = 6.37827
I0524 03:32:22.624421 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37827 (* 1 = 6.37827 loss)
I0524 03:32:22.624438 11835 sgd_solver.cpp:112] Iteration 28960, lr = 0.1
I0524 03:32:28.907260 11835 solver.cpp:239] Iteration 28970 (1.5917 iter/s, 6.28261s/10 iters), loss = 5.82195
I0524 03:32:28.907477 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82195 (* 1 = 5.82195 loss)
I0524 03:32:28.916024 11835 sgd_solver.cpp:112] Iteration 28970, lr = 0.1
I0524 03:32:34.795815 11835 solver.cpp:239] Iteration 28980 (1.69833 iter/s, 5.88814s/10 iters), loss = 7.08187
I0524 03:32:34.795866 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.08187 (* 1 = 7.08187 loss)
I0524 03:32:34.795879 11835 sgd_solver.cpp:112] Iteration 28980, lr = 0.1
I0524 03:32:40.596563 11835 solver.cpp:239] Iteration 28990 (1.72418 iter/s, 5.79986s/10 iters), loss = 6.49884
I0524 03:32:40.596618 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49884 (* 1 = 6.49884 loss)
I0524 03:32:40.596642 11835 sgd_solver.cpp:112] Iteration 28990, lr = 0.1
I0524 03:32:46.652200 11835 solver.cpp:239] Iteration 29000 (1.65148 iter/s, 6.05518s/10 iters), loss = 6.321
I0524 03:32:46.652240 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.321 (* 1 = 6.321 loss)
I0524 03:32:46.652374 11835 sgd_solver.cpp:112] Iteration 29000, lr = 0.1
I0524 03:32:52.865357 11835 solver.cpp:239] Iteration 29010 (1.60956 iter/s, 6.21286s/10 iters), loss = 6.21503
I0524 03:32:52.865411 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21503 (* 1 = 6.21503 loss)
I0524 03:32:52.865514 11835 sgd_solver.cpp:112] Iteration 29010, lr = 0.1
I0524 03:32:59.774190 11835 solver.cpp:239] Iteration 29020 (1.44749 iter/s, 6.90852s/10 iters), loss = 7.49215
I0524 03:32:59.774438 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.49215 (* 1 = 7.49215 loss)
I0524 03:32:59.808913 11835 sgd_solver.cpp:112] Iteration 29020, lr = 0.1
I0524 03:33:06.967440 11835 solver.cpp:239] Iteration 29030 (1.39029 iter/s, 7.19277s/10 iters), loss = 7.96139
I0524 03:33:06.967494 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.96139 (* 1 = 7.96139 loss)
I0524 03:33:06.967509 11835 sgd_solver.cpp:112] Iteration 29030, lr = 0.1
I0524 03:33:12.665632 11835 solver.cpp:239] Iteration 29040 (1.75569 iter/s, 5.69575s/10 iters), loss = 6.79601
I0524 03:33:12.665668 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79601 (* 1 = 6.79601 loss)
I0524 03:33:12.665680 11835 sgd_solver.cpp:112] Iteration 29040, lr = 0.1
I0524 03:33:19.628952 11835 solver.cpp:239] Iteration 29050 (1.43616 iter/s, 6.96301s/10 iters), loss = 7.67215
I0524 03:33:19.629003 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.67215 (* 1 = 7.67215 loss)
I0524 03:33:19.668632 11835 sgd_solver.cpp:112] Iteration 29050, lr = 0.1
I0524 03:33:26.669687 11835 solver.cpp:239] Iteration 29060 (1.42037 iter/s, 7.04041s/10 iters), loss = 7.31826
I0524 03:33:26.669739 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.31826 (* 1 = 7.31826 loss)
I0524 03:33:26.714949 11835 sgd_solver.cpp:112] Iteration 29060, lr = 0.1
I0524 03:33:33.489871 11835 solver.cpp:239] Iteration 29070 (1.4663 iter/s, 6.81987s/10 iters), loss = 7.10293
I0524 03:33:33.489984 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10293 (* 1 = 7.10293 loss)
I0524 03:33:33.489997 11835 sgd_solver.cpp:112] Iteration 29070, lr = 0.1
I0524 03:33:39.204123 11835 solver.cpp:239] Iteration 29080 (1.75023 iter/s, 5.71353s/10 iters), loss = 7.6687
I0524 03:33:39.204174 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.6687 (* 1 = 7.6687 loss)
I0524 03:33:39.284395 11835 sgd_solver.cpp:112] Iteration 29080, lr = 0.1
I0524 03:33:45.661979 11835 solver.cpp:239] Iteration 29090 (1.54858 iter/s, 6.45755s/10 iters), loss = 7.06475
I0524 03:33:45.662032 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.06475 (* 1 = 7.06475 loss)
I0524 03:33:45.662051 11835 sgd_solver.cpp:112] Iteration 29090, lr = 0.1
I0524 03:33:52.991261 11835 solver.cpp:239] Iteration 29100 (1.36445 iter/s, 7.32895s/10 iters), loss = 5.71428
I0524 03:33:52.991312 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71428 (* 1 = 5.71428 loss)
I0524 03:33:52.991425 11835 sgd_solver.cpp:112] Iteration 29100, lr = 0.1
I0524 03:34:00.540427 11835 solver.cpp:239] Iteration 29110 (1.32471 iter/s, 7.54882s/10 iters), loss = 6.54075
I0524 03:34:00.540482 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54075 (* 1 = 6.54075 loss)
I0524 03:34:00.777878 11835 sgd_solver.cpp:112] Iteration 29110, lr = 0.1
I0524 03:34:07.507578 11835 solver.cpp:239] Iteration 29120 (1.43537 iter/s, 6.96683s/10 iters), loss = 5.86229
I0524 03:34:07.507719 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86229 (* 1 = 5.86229 loss)
I0524 03:34:07.507748 11835 sgd_solver.cpp:112] Iteration 29120, lr = 0.1
I0524 03:34:14.051044 11835 solver.cpp:239] Iteration 29130 (1.52833 iter/s, 6.54308s/10 iters), loss = 6.11254
I0524 03:34:14.051090 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11254 (* 1 = 6.11254 loss)
I0524 03:34:14.051103 11835 sgd_solver.cpp:112] Iteration 29130, lr = 0.1
I0524 03:34:20.715049 11835 solver.cpp:239] Iteration 29140 (1.50067 iter/s, 6.66371s/10 iters), loss = 7.32205
I0524 03:34:20.715091 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.32205 (* 1 = 7.32205 loss)
I0524 03:34:20.747356 11835 sgd_solver.cpp:112] Iteration 29140, lr = 0.1
I0524 03:34:28.280532 11835 solver.cpp:239] Iteration 29150 (1.32185 iter/s, 7.56515s/10 iters), loss = 5.05854
I0524 03:34:28.280581 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.05854 (* 1 = 5.05854 loss)
I0524 03:34:28.280597 11835 sgd_solver.cpp:112] Iteration 29150, lr = 0.1
I0524 03:34:33.921797 11835 solver.cpp:239] Iteration 29160 (1.77273 iter/s, 5.641s/10 iters), loss = 6.56314
I0524 03:34:33.921835 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56314 (* 1 = 6.56314 loss)
I0524 03:34:33.921866 11835 sgd_solver.cpp:112] Iteration 29160, lr = 0.1
I0524 03:34:41.099623 11835 solver.cpp:239] Iteration 29170 (1.39324 iter/s, 7.17751s/10 iters), loss = 6.26992
I0524 03:34:41.099946 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26992 (* 1 = 6.26992 loss)
I0524 03:34:41.656651 11835 sgd_solver.cpp:112] Iteration 29170, lr = 0.1
I0524 03:34:49.222506 11835 solver.cpp:239] Iteration 29180 (1.23118 iter/s, 8.1223s/10 iters), loss = 6.10363
I0524 03:34:49.222554 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10363 (* 1 = 6.10363 loss)
I0524 03:34:49.222766 11835 sgd_solver.cpp:112] Iteration 29180, lr = 0.1
I0524 03:34:57.687827 11835 solver.cpp:239] Iteration 29190 (1.18134 iter/s, 8.46495s/10 iters), loss = 7.35026
I0524 03:34:57.687875 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.35026 (* 1 = 7.35026 loss)
I0524 03:34:57.702599 11835 sgd_solver.cpp:112] Iteration 29190, lr = 0.1
I0524 03:35:03.625735 11835 solver.cpp:239] Iteration 29200 (1.68417 iter/s, 5.93763s/10 iters), loss = 6.2485
I0524 03:35:03.625782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2485 (* 1 = 6.2485 loss)
I0524 03:35:03.625872 11835 sgd_solver.cpp:112] Iteration 29200, lr = 0.1
I0524 03:35:09.292781 11835 solver.cpp:239] Iteration 29210 (1.76467 iter/s, 5.66678s/10 iters), loss = 6.35544
I0524 03:35:09.292829 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35544 (* 1 = 6.35544 loss)
I0524 03:35:09.293051 11835 sgd_solver.cpp:112] Iteration 29210, lr = 0.1
I0524 03:35:17.352136 11835 solver.cpp:239] Iteration 29220 (1.24085 iter/s, 8.059s/10 iters), loss = 7.02993
I0524 03:35:17.352383 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.02993 (* 1 = 7.02993 loss)
I0524 03:35:17.364472 11835 sgd_solver.cpp:112] Iteration 29220, lr = 0.1
I0524 03:35:26.028173 11835 solver.cpp:239] Iteration 29230 (1.15267 iter/s, 8.6755s/10 iters), loss = 6.96739
I0524 03:35:26.028228 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96739 (* 1 = 6.96739 loss)
I0524 03:35:26.032719 11835 sgd_solver.cpp:112] Iteration 29230, lr = 0.1
I0524 03:35:33.812440 11835 solver.cpp:239] Iteration 29240 (1.2847 iter/s, 7.78392s/10 iters), loss = 6.03408
I0524 03:35:33.812486 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03408 (* 1 = 6.03408 loss)
I0524 03:35:33.820485 11835 sgd_solver.cpp:112] Iteration 29240, lr = 0.1
I0524 03:35:39.936736 11835 solver.cpp:239] Iteration 29250 (1.63291 iter/s, 6.12402s/10 iters), loss = 7.28618
I0524 03:35:39.936781 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.28618 (* 1 = 7.28618 loss)
I0524 03:35:39.936795 11835 sgd_solver.cpp:112] Iteration 29250, lr = 0.1
I0524 03:35:50.860656 11835 solver.cpp:239] Iteration 29260 (0.915569 iter/s, 10.9222s/10 iters), loss = 6.7472
I0524 03:35:50.860903 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7472 (* 1 = 6.7472 loss)
I0524 03:35:50.860946 11835 sgd_solver.cpp:112] Iteration 29260, lr = 0.1
I0524 03:35:57.074234 11835 solver.cpp:239] Iteration 29270 (1.60949 iter/s, 6.21314s/10 iters), loss = 6.1618
I0524 03:35:57.074268 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1618 (* 1 = 6.1618 loss)
I0524 03:35:57.074280 11835 sgd_solver.cpp:112] Iteration 29270, lr = 0.1
I0524 03:36:03.058462 11835 solver.cpp:239] Iteration 29280 (1.67117 iter/s, 5.98381s/10 iters), loss = 6.5451
I0524 03:36:03.058521 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5451 (* 1 = 6.5451 loss)
I0524 03:36:03.058727 11835 sgd_solver.cpp:112] Iteration 29280, lr = 0.1
I0524 03:36:08.895820 11835 solver.cpp:239] Iteration 29290 (1.71318 iter/s, 5.83708s/10 iters), loss = 6.63642
I0524 03:36:08.895858 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63642 (* 1 = 6.63642 loss)
I0524 03:36:08.895870 11835 sgd_solver.cpp:112] Iteration 29290, lr = 0.1
I0524 03:36:14.793998 11835 solver.cpp:239] Iteration 29300 (1.69552 iter/s, 5.89789s/10 iters), loss = 6.02668
I0524 03:36:14.794055 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02668 (* 1 = 6.02668 loss)
I0524 03:36:14.794229 11835 sgd_solver.cpp:112] Iteration 29300, lr = 0.1
I0524 03:36:20.875669 11835 solver.cpp:239] Iteration 29310 (1.64436 iter/s, 6.08139s/10 iters), loss = 7.94852
I0524 03:36:20.875932 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.94852 (* 1 = 7.94852 loss)
I0524 03:36:20.875985 11835 sgd_solver.cpp:112] Iteration 29310, lr = 0.1
I0524 03:36:28.105594 11835 solver.cpp:239] Iteration 29320 (1.38324 iter/s, 7.22942s/10 iters), loss = 5.92901
I0524 03:36:28.105643 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92901 (* 1 = 5.92901 loss)
I0524 03:36:28.380506 11835 sgd_solver.cpp:112] Iteration 29320, lr = 0.1
I0524 03:36:33.939483 11835 solver.cpp:239] Iteration 29330 (1.7142 iter/s, 5.83361s/10 iters), loss = 7.32709
I0524 03:36:33.939537 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.32709 (* 1 = 7.32709 loss)
I0524 03:36:34.091642 11835 sgd_solver.cpp:112] Iteration 29330, lr = 0.1
I0524 03:36:43.297991 11835 solver.cpp:239] Iteration 29340 (1.06859 iter/s, 9.35811s/10 iters), loss = 7.25999
I0524 03:36:43.298029 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.25999 (* 1 = 7.25999 loss)
I0524 03:36:43.298132 11835 sgd_solver.cpp:112] Iteration 29340, lr = 0.1
I0524 03:36:50.768693 11835 solver.cpp:239] Iteration 29350 (1.33862 iter/s, 7.47037s/10 iters), loss = 6.68505
I0524 03:36:50.768743 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.68505 (* 1 = 6.68505 loss)
I0524 03:36:50.768862 11835 sgd_solver.cpp:112] Iteration 29350, lr = 0.1
I0524 03:36:58.653156 11835 solver.cpp:239] Iteration 29360 (1.26837 iter/s, 7.88411s/10 iters), loss = 5.81419
I0524 03:36:58.653316 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81419 (* 1 = 5.81419 loss)
I0524 03:36:58.653391 11835 sgd_solver.cpp:112] Iteration 29360, lr = 0.1
I0524 03:37:04.632699 11835 solver.cpp:239] Iteration 29370 (1.67248 iter/s, 5.97916s/10 iters), loss = 6.60569
I0524 03:37:04.632742 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60569 (* 1 = 6.60569 loss)
I0524 03:37:04.633116 11835 sgd_solver.cpp:112] Iteration 29370, lr = 0.1
I0524 03:37:10.543674 11835 solver.cpp:239] Iteration 29380 (1.69185 iter/s, 5.91069s/10 iters), loss = 6.00442
I0524 03:37:10.543728 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00442 (* 1 = 6.00442 loss)
I0524 03:37:10.544324 11835 sgd_solver.cpp:112] Iteration 29380, lr = 0.1
I0524 03:37:17.151414 11835 solver.cpp:239] Iteration 29390 (1.51345 iter/s, 6.60744s/10 iters), loss = 6.38854
I0524 03:37:17.151460 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38854 (* 1 = 6.38854 loss)
I0524 03:37:17.151512 11835 sgd_solver.cpp:112] Iteration 29390, lr = 0.1
I0524 03:37:23.245615 11835 solver.cpp:239] Iteration 29400 (1.64098 iter/s, 6.09391s/10 iters), loss = 5.71214
I0524 03:37:23.245671 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71214 (* 1 = 5.71214 loss)
I0524 03:37:23.245759 11835 sgd_solver.cpp:112] Iteration 29400, lr = 0.1
I0524 03:37:29.525158 11835 solver.cpp:239] Iteration 29410 (1.59255 iter/s, 6.27925s/10 iters), loss = 7.68856
I0524 03:37:29.525452 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.68856 (* 1 = 7.68856 loss)
I0524 03:37:30.131573 11835 sgd_solver.cpp:112] Iteration 29410, lr = 0.1
I0524 03:37:35.848748 11835 solver.cpp:239] Iteration 29420 (1.58151 iter/s, 6.32309s/10 iters), loss = 6.79601
I0524 03:37:35.848794 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79601 (* 1 = 6.79601 loss)
I0524 03:37:35.848807 11835 sgd_solver.cpp:112] Iteration 29420, lr = 0.1
I0524 03:37:43.284641 11835 solver.cpp:239] Iteration 29430 (1.34489 iter/s, 7.43556s/10 iters), loss = 6.99225
I0524 03:37:43.284688 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.99225 (* 1 = 6.99225 loss)
I0524 03:37:43.284793 11835 sgd_solver.cpp:112] Iteration 29430, lr = 0.1
I0524 03:37:50.226828 11835 solver.cpp:239] Iteration 29440 (1.44053 iter/s, 6.94188s/10 iters), loss = 6.32442
I0524 03:37:50.226877 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32442 (* 1 = 6.32442 loss)
I0524 03:37:50.227082 11835 sgd_solver.cpp:112] Iteration 29440, lr = 0.1
I0524 03:37:55.816751 11835 solver.cpp:239] Iteration 29450 (1.78902 iter/s, 5.58966s/10 iters), loss = 5.9005
I0524 03:37:55.816797 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9005 (* 1 = 5.9005 loss)
I0524 03:37:55.816810 11835 sgd_solver.cpp:112] Iteration 29450, lr = 0.1
I0524 03:38:03.202286 11835 solver.cpp:239] Iteration 29460 (1.35406 iter/s, 7.38521s/10 iters), loss = 6.46498
I0524 03:38:03.202541 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46498 (* 1 = 6.46498 loss)
I0524 03:38:03.358373 11835 sgd_solver.cpp:112] Iteration 29460, lr = 0.1
I0524 03:38:10.792299 11835 solver.cpp:239] Iteration 29470 (1.31761 iter/s, 7.5895s/10 iters), loss = 6.81756
I0524 03:38:10.792351 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81756 (* 1 = 6.81756 loss)
I0524 03:38:10.792455 11835 sgd_solver.cpp:112] Iteration 29470, lr = 0.1
I0524 03:38:19.237418 11835 solver.cpp:239] Iteration 29480 (1.18417 iter/s, 8.44474s/10 iters), loss = 6.44773
I0524 03:38:19.237468 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44773 (* 1 = 6.44773 loss)
I0524 03:38:19.237583 11835 sgd_solver.cpp:112] Iteration 29480, lr = 0.1
I0524 03:38:25.158517 11835 solver.cpp:239] Iteration 29490 (1.68895 iter/s, 5.92083s/10 iters), loss = 6.6681
I0524 03:38:25.158556 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6681 (* 1 = 6.6681 loss)
I0524 03:38:25.160609 11835 sgd_solver.cpp:112] Iteration 29490, lr = 0.1
I0524 03:38:32.561256 11835 solver.cpp:239] Iteration 29500 (1.35091 iter/s, 7.4024s/10 iters), loss = 6.99527
I0524 03:38:32.561311 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.99527 (* 1 = 6.99527 loss)
I0524 03:38:32.561518 11835 sgd_solver.cpp:112] Iteration 29500, lr = 0.1
I0524 03:38:38.181633 11835 solver.cpp:239] Iteration 29510 (1.77932 iter/s, 5.62011s/10 iters), loss = 6.09881
I0524 03:38:38.181865 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09881 (* 1 = 6.09881 loss)
I0524 03:38:38.181917 11835 sgd_solver.cpp:112] Iteration 29510, lr = 0.1
I0524 03:38:46.078541 11835 solver.cpp:239] Iteration 29520 (1.26641 iter/s, 7.89636s/10 iters), loss = 6.17036
I0524 03:38:46.078595 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17036 (* 1 = 6.17036 loss)
I0524 03:38:46.078788 11835 sgd_solver.cpp:112] Iteration 29520, lr = 0.1
I0524 03:38:52.395402 11835 solver.cpp:239] Iteration 29530 (1.58314 iter/s, 6.31656s/10 iters), loss = 7.10553
I0524 03:38:52.395455 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10553 (* 1 = 7.10553 loss)
I0524 03:38:52.395627 11835 sgd_solver.cpp:112] Iteration 29530, lr = 0.1
I0524 03:38:58.244735 11835 solver.cpp:239] Iteration 29540 (1.70968 iter/s, 5.84906s/10 iters), loss = 7.27107
I0524 03:38:58.244781 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.27107 (* 1 = 7.27107 loss)
I0524 03:38:58.245052 11835 sgd_solver.cpp:112] Iteration 29540, lr = 0.1
I0524 03:39:07.584834 11835 solver.cpp:239] Iteration 29550 (1.0707 iter/s, 9.33968s/10 iters), loss = 6.78221
I0524 03:39:07.584898 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78221 (* 1 = 6.78221 loss)
I0524 03:39:07.644821 11835 sgd_solver.cpp:112] Iteration 29550, lr = 0.1
I0524 03:39:16.337252 11835 solver.cpp:239] Iteration 29560 (1.14259 iter/s, 8.75204s/10 iters), loss = 5.83602
I0524 03:39:16.337530 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83602 (* 1 = 5.83602 loss)
I0524 03:39:16.337576 11835 sgd_solver.cpp:112] Iteration 29560, lr = 0.1
I0524 03:39:22.138555 11835 solver.cpp:239] Iteration 29570 (1.72389 iter/s, 5.80083s/10 iters), loss = 6.27732
I0524 03:39:22.138605 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27732 (* 1 = 6.27732 loss)
I0524 03:39:22.138620 11835 sgd_solver.cpp:112] Iteration 29570, lr = 0.1
I0524 03:39:29.519024 11835 solver.cpp:239] Iteration 29580 (1.35539 iter/s, 7.37793s/10 iters), loss = 6.61928
I0524 03:39:29.519067 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61928 (* 1 = 6.61928 loss)
I0524 03:39:29.519078 11835 sgd_solver.cpp:112] Iteration 29580, lr = 0.1
I0524 03:39:36.080482 11835 solver.cpp:239] Iteration 29590 (1.52412 iter/s, 6.56115s/10 iters), loss = 6.50047
I0524 03:39:36.080535 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50047 (* 1 = 6.50047 loss)
I0524 03:39:36.080847 11835 sgd_solver.cpp:112] Iteration 29590, lr = 0.1
I0524 03:39:42.152432 11835 solver.cpp:239] Iteration 29600 (1.64699 iter/s, 6.07167s/10 iters), loss = 6.70753
I0524 03:39:42.152475 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70753 (* 1 = 6.70753 loss)
I0524 03:39:42.152796 11835 sgd_solver.cpp:112] Iteration 29600, lr = 0.1
I0524 03:39:49.784672 11835 solver.cpp:239] Iteration 29610 (1.31029 iter/s, 7.63188s/10 iters), loss = 6.39298
I0524 03:39:49.784924 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39298 (* 1 = 6.39298 loss)
I0524 03:39:49.784965 11835 sgd_solver.cpp:112] Iteration 29610, lr = 0.1
I0524 03:39:56.343997 11835 solver.cpp:239] Iteration 29620 (1.52514 iter/s, 6.55679s/10 iters), loss = 5.99512
I0524 03:39:56.344040 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99512 (* 1 = 5.99512 loss)
I0524 03:39:56.344161 11835 sgd_solver.cpp:112] Iteration 29620, lr = 0.1
I0524 03:40:02.370558 11835 solver.cpp:239] Iteration 29630 (1.6594 iter/s, 6.02627s/10 iters), loss = 5.46982
I0524 03:40:02.370606 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.46982 (* 1 = 5.46982 loss)
I0524 03:40:02.370621 11835 sgd_solver.cpp:112] Iteration 29630, lr = 0.1
I0524 03:40:10.017551 11835 solver.cpp:239] Iteration 29640 (1.30776 iter/s, 7.64665s/10 iters), loss = 4.04859
I0524 03:40:10.017598 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.04859 (* 1 = 4.04859 loss)
I0524 03:40:10.017719 11835 sgd_solver.cpp:112] Iteration 29640, lr = 0.1
I0524 03:40:15.894551 11835 solver.cpp:239] Iteration 29650 (1.70163 iter/s, 5.87673s/10 iters), loss = 7.93966
I0524 03:40:15.894599 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.93966 (* 1 = 7.93966 loss)
I0524 03:40:15.894616 11835 sgd_solver.cpp:112] Iteration 29650, lr = 0.1
I0524 03:40:23.200134 11835 solver.cpp:239] Iteration 29660 (1.36893 iter/s, 7.30496s/10 iters), loss = 6.70342
I0524 03:40:23.200254 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70342 (* 1 = 6.70342 loss)
I0524 03:40:23.577245 11835 sgd_solver.cpp:112] Iteration 29660, lr = 0.1
I0524 03:40:33.165978 11835 solver.cpp:239] Iteration 29670 (1.00348 iter/s, 9.96534s/10 iters), loss = 5.79638
I0524 03:40:33.166021 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79638 (* 1 = 5.79638 loss)
I0524 03:40:34.156211 11835 sgd_solver.cpp:112] Iteration 29670, lr = 0.1
I0524 03:40:41.037436 11835 solver.cpp:239] Iteration 29680 (1.27047 iter/s, 7.87112s/10 iters), loss = 7.13129
I0524 03:40:41.037483 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13129 (* 1 = 7.13129 loss)
I0524 03:40:41.037678 11835 sgd_solver.cpp:112] Iteration 29680, lr = 0.1
I0524 03:40:46.551012 11835 solver.cpp:239] Iteration 29690 (1.81379 iter/s, 5.51331s/10 iters), loss = 5.54179
I0524 03:40:46.551065 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.54179 (* 1 = 5.54179 loss)
I0524 03:40:46.551363 11835 sgd_solver.cpp:112] Iteration 29690, lr = 0.1
I0524 03:40:53.359020 11835 solver.cpp:239] Iteration 29700 (1.46892 iter/s, 6.8077s/10 iters), loss = 5.86884
I0524 03:40:53.359319 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86884 (* 1 = 5.86884 loss)
I0524 03:40:53.359366 11835 sgd_solver.cpp:112] Iteration 29700, lr = 0.1
I0524 03:41:00.434285 11835 solver.cpp:239] Iteration 29710 (1.4136 iter/s, 7.07412s/10 iters), loss = 6.52102
I0524 03:41:00.434340 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52102 (* 1 = 6.52102 loss)
I0524 03:41:00.434415 11835 sgd_solver.cpp:112] Iteration 29710, lr = 0.1
I0524 03:41:06.945479 11835 solver.cpp:239] Iteration 29720 (1.53589 iter/s, 6.5109s/10 iters), loss = 6.61704
I0524 03:41:06.945519 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61704 (* 1 = 6.61704 loss)
I0524 03:41:06.945729 11835 sgd_solver.cpp:112] Iteration 29720, lr = 0.1
I0524 03:41:12.880424 11835 solver.cpp:239] Iteration 29730 (1.68502 iter/s, 5.93466s/10 iters), loss = 6.40885
I0524 03:41:12.880475 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40885 (* 1 = 6.40885 loss)
I0524 03:41:12.880692 11835 sgd_solver.cpp:112] Iteration 29730, lr = 0.1
I0524 03:41:19.347563 11835 solver.cpp:239] Iteration 29740 (1.54635 iter/s, 6.46683s/10 iters), loss = 6.57649
I0524 03:41:19.347611 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57649 (* 1 = 6.57649 loss)
I0524 03:41:19.347687 11835 sgd_solver.cpp:112] Iteration 29740, lr = 0.1
I0524 03:41:27.068935 11835 solver.cpp:239] Iteration 29750 (1.29516 iter/s, 7.72104s/10 iters), loss = 6.22543
I0524 03:41:27.069169 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22543 (* 1 = 6.22543 loss)
I0524 03:41:27.132586 11835 sgd_solver.cpp:112] Iteration 29750, lr = 0.1
I0524 03:41:33.992868 11835 solver.cpp:239] Iteration 29760 (1.44437 iter/s, 6.92346s/10 iters), loss = 7.08852
I0524 03:41:33.992923 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.08852 (* 1 = 7.08852 loss)
I0524 03:41:33.993041 11835 sgd_solver.cpp:112] Iteration 29760, lr = 0.1
I0524 03:41:41.344501 11835 solver.cpp:239] Iteration 29770 (1.36031 iter/s, 7.35129s/10 iters), loss = 6.71255
I0524 03:41:41.344559 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71255 (* 1 = 6.71255 loss)
I0524 03:41:41.344651 11835 sgd_solver.cpp:112] Iteration 29770, lr = 0.1
I0524 03:41:48.570962 11835 solver.cpp:239] Iteration 29780 (1.38387 iter/s, 7.22613s/10 iters), loss = 6.1329
I0524 03:41:48.571019 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1329 (* 1 = 6.1329 loss)
I0524 03:41:48.571151 11835 sgd_solver.cpp:112] Iteration 29780, lr = 0.1
I0524 03:41:55.315109 11835 solver.cpp:239] Iteration 29790 (1.48284 iter/s, 6.74384s/10 iters), loss = 7.71533
I0524 03:41:55.315152 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.71533 (* 1 = 7.71533 loss)
I0524 03:41:55.316877 11835 sgd_solver.cpp:112] Iteration 29790, lr = 0.1
I0524 03:42:01.597964 11835 solver.cpp:239] Iteration 29800 (1.59171 iter/s, 6.28255s/10 iters), loss = 6.73943
I0524 03:42:01.598206 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73943 (* 1 = 6.73943 loss)
I0524 03:42:01.598261 11835 sgd_solver.cpp:112] Iteration 29800, lr = 0.1
I0524 03:42:07.775838 11835 solver.cpp:239] Iteration 29810 (1.61885 iter/s, 6.17724s/10 iters), loss = 5.56107
I0524 03:42:07.775878 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56107 (* 1 = 5.56107 loss)
I0524 03:42:07.775990 11835 sgd_solver.cpp:112] Iteration 29810, lr = 0.1
I0524 03:42:13.556584 11835 solver.cpp:239] Iteration 29820 (1.72996 iter/s, 5.78048s/10 iters), loss = 6.09156
I0524 03:42:13.556629 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09156 (* 1 = 6.09156 loss)
I0524 03:42:14.428171 11835 sgd_solver.cpp:112] Iteration 29820, lr = 0.1
I0524 03:42:20.603891 11835 solver.cpp:239] Iteration 29830 (1.41904 iter/s, 7.04699s/10 iters), loss = 5.86392
I0524 03:42:20.603936 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86392 (* 1 = 5.86392 loss)
I0524 03:42:20.604182 11835 sgd_solver.cpp:112] Iteration 29830, lr = 0.1
I0524 03:42:26.260409 11835 solver.cpp:239] Iteration 29840 (1.76796 iter/s, 5.65625s/10 iters), loss = 6.55855
I0524 03:42:26.260454 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55855 (* 1 = 6.55855 loss)
I0524 03:42:26.260471 11835 sgd_solver.cpp:112] Iteration 29840, lr = 0.1
I0524 03:42:32.972487 11835 solver.cpp:239] Iteration 29850 (1.48993 iter/s, 6.71171s/10 iters), loss = 6.39942
I0524 03:42:32.972684 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39942 (* 1 = 6.39942 loss)
I0524 03:42:32.972700 11835 sgd_solver.cpp:112] Iteration 29850, lr = 0.1
I0524 03:42:38.984438 11835 solver.cpp:239] Iteration 29860 (1.6635 iter/s, 6.01143s/10 iters), loss = 6.68841
I0524 03:42:38.984477 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.68841 (* 1 = 6.68841 loss)
I0524 03:42:38.984565 11835 sgd_solver.cpp:112] Iteration 29860, lr = 0.1
I0524 03:42:45.880188 11835 solver.cpp:239] Iteration 29870 (1.45023 iter/s, 6.89544s/10 iters), loss = 6.77134
I0524 03:42:45.880239 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.77134 (* 1 = 6.77134 loss)
I0524 03:42:45.880254 11835 sgd_solver.cpp:112] Iteration 29870, lr = 0.1
I0524 03:42:52.134817 11835 solver.cpp:239] Iteration 29880 (1.59889 iter/s, 6.25435s/10 iters), loss = 6.89662
I0524 03:42:52.134858 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89662 (* 1 = 6.89662 loss)
I0524 03:42:52.602592 11835 sgd_solver.cpp:112] Iteration 29880, lr = 0.1
I0524 03:42:59.135185 11835 solver.cpp:239] Iteration 29890 (1.42856 iter/s, 7.00004s/10 iters), loss = 6.67937
I0524 03:42:59.135246 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67937 (* 1 = 6.67937 loss)
I0524 03:42:59.135383 11835 sgd_solver.cpp:112] Iteration 29890, lr = 0.1
I0524 03:43:05.479672 11835 solver.cpp:239] Iteration 29900 (1.57625 iter/s, 6.34419s/10 iters), loss = 6.99302
I0524 03:43:05.479810 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.99302 (* 1 = 6.99302 loss)
I0524 03:43:05.479904 11835 sgd_solver.cpp:112] Iteration 29900, lr = 0.1
I0524 03:43:11.657635 11835 solver.cpp:239] Iteration 29910 (1.61876 iter/s, 6.17758s/10 iters), loss = 6.58999
I0524 03:43:11.657685 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58999 (* 1 = 6.58999 loss)
I0524 03:43:11.657697 11835 sgd_solver.cpp:112] Iteration 29910, lr = 0.1
I0524 03:43:17.540951 11835 solver.cpp:239] Iteration 29920 (1.70043 iter/s, 5.88087s/10 iters), loss = 7.57867
I0524 03:43:17.541007 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.57867 (* 1 = 7.57867 loss)
I0524 03:43:17.541218 11835 sgd_solver.cpp:112] Iteration 29920, lr = 0.1
I0524 03:43:23.461609 11835 solver.cpp:239] Iteration 29930 (1.68908 iter/s, 5.92038s/10 iters), loss = 6.96308
I0524 03:43:23.461652 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96308 (* 1 = 6.96308 loss)
I0524 03:43:23.461915 11835 sgd_solver.cpp:112] Iteration 29930, lr = 0.1
I0524 03:43:30.314862 11835 solver.cpp:239] Iteration 29940 (1.45923 iter/s, 6.85293s/10 iters), loss = 6.96765
I0524 03:43:30.314924 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96765 (* 1 = 6.96765 loss)
I0524 03:43:30.315130 11835 sgd_solver.cpp:112] Iteration 29940, lr = 0.1
I0524 03:43:36.341352 11835 solver.cpp:239] Iteration 29950 (1.65942 iter/s, 6.02621s/10 iters), loss = 6.24953
I0524 03:43:36.341611 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24953 (* 1 = 6.24953 loss)
I0524 03:43:36.341663 11835 sgd_solver.cpp:112] Iteration 29950, lr = 0.1
I0524 03:43:42.214265 11835 solver.cpp:239] Iteration 29960 (1.70287 iter/s, 5.87245s/10 iters), loss = 5.75987
I0524 03:43:42.214321 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75987 (* 1 = 5.75987 loss)
I0524 03:43:42.214587 11835 sgd_solver.cpp:112] Iteration 29960, lr = 0.1
I0524 03:43:48.058033 11835 solver.cpp:239] Iteration 29970 (1.71131 iter/s, 5.84349s/10 iters), loss = 7.68737
I0524 03:43:48.058080 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.68737 (* 1 = 7.68737 loss)
I0524 03:43:48.058321 11835 sgd_solver.cpp:112] Iteration 29970, lr = 0.1
I0524 03:43:55.414623 11835 solver.cpp:239] Iteration 29980 (1.35939 iter/s, 7.35625s/10 iters), loss = 6.87906
I0524 03:43:55.414681 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87906 (* 1 = 6.87906 loss)
I0524 03:43:55.414795 11835 sgd_solver.cpp:112] Iteration 29980, lr = 0.1
I0524 03:44:02.161758 11835 solver.cpp:239] Iteration 29990 (1.48218 iter/s, 6.74682s/10 iters), loss = 6.71992
I0524 03:44:02.161803 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71992 (* 1 = 6.71992 loss)
I0524 03:44:02.254115 11835 sgd_solver.cpp:112] Iteration 29990, lr = 0.1
I0524 03:44:07.869202 11835 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_30000.caffemodel
I0524 03:44:09.063262 11835 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_30000.solverstate
I0524 03:44:09.655016 11835 solver.cpp:239] Iteration 30000 (1.33459 iter/s, 7.49292s/10 iters), loss = 5.73008
I0524 03:44:09.655067 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73008 (* 1 = 5.73008 loss)
I0524 03:44:09.655105 11835 sgd_solver.cpp:112] Iteration 30000, lr = 0.1
I0524 03:44:15.571271 11835 solver.cpp:239] Iteration 30010 (1.69034 iter/s, 5.91597s/10 iters), loss = 5.49787
I0524 03:44:15.571316 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.49787 (* 1 = 5.49787 loss)
I0524 03:44:15.571455 11835 sgd_solver.cpp:112] Iteration 30010, lr = 0.1
I0524 03:44:21.243703 11835 solver.cpp:239] Iteration 30020 (1.763 iter/s, 5.67216s/10 iters), loss = 7.00999
I0524 03:44:21.243759 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00999 (* 1 = 7.00999 loss)
I0524 03:44:21.244169 11835 sgd_solver.cpp:112] Iteration 30020, lr = 0.1
I0524 03:44:27.352331 11835 solver.cpp:239] Iteration 30030 (1.63711 iter/s, 6.10834s/10 iters), loss = 7.16843
I0524 03:44:27.352380 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.16843 (* 1 = 7.16843 loss)
I0524 03:44:27.352396 11835 sgd_solver.cpp:112] Iteration 30030, lr = 0.1
I0524 03:44:33.596266 11835 solver.cpp:239] Iteration 30040 (1.60218 iter/s, 6.2415s/10 iters), loss = 5.49859
I0524 03:44:33.596305 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.49859 (* 1 = 5.49859 loss)
I0524 03:44:34.356773 11835 sgd_solver.cpp:112] Iteration 30040, lr = 0.1
I0524 03:44:42.017897 11835 solver.cpp:239] Iteration 30050 (1.18747 iter/s, 8.42126s/10 iters), loss = 5.52183
I0524 03:44:42.018184 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.52183 (* 1 = 5.52183 loss)
I0524 03:44:42.058204 11835 sgd_solver.cpp:112] Iteration 30050, lr = 0.1
I0524 03:44:48.752785 11835 solver.cpp:239] Iteration 30060 (1.48492 iter/s, 6.73438s/10 iters), loss = 6.42646
I0524 03:44:48.752845 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42646 (* 1 = 6.42646 loss)
I0524 03:44:48.753037 11835 sgd_solver.cpp:112] Iteration 30060, lr = 0.1
I0524 03:44:55.529865 11835 solver.cpp:239] Iteration 30070 (1.47563 iter/s, 6.77674s/10 iters), loss = 5.99734
I0524 03:44:55.529917 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99734 (* 1 = 5.99734 loss)
I0524 03:44:55.530192 11835 sgd_solver.cpp:112] Iteration 30070, lr = 0.1
I0524 03:45:02.579609 11835 solver.cpp:239] Iteration 30080 (1.41856 iter/s, 7.04942s/10 iters), loss = 6.6417
I0524 03:45:02.579656 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6417 (* 1 = 6.6417 loss)
I0524 03:45:02.579785 11835 sgd_solver.cpp:112] Iteration 30080, lr = 0.1
I0524 03:45:10.160013 11835 solver.cpp:239] Iteration 30090 (1.31925 iter/s, 7.58006s/10 iters), loss = 6.47016
I0524 03:45:10.160066 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.47016 (* 1 = 6.47016 loss)
I0524 03:45:10.160189 11835 sgd_solver.cpp:112] Iteration 30090, lr = 0.1
I0524 03:45:16.170606 11835 solver.cpp:239] Iteration 30100 (1.66381 iter/s, 6.01031s/10 iters), loss = 7.0147
I0524 03:45:16.170857 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.0147 (* 1 = 7.0147 loss)
I0524 03:45:16.170915 11835 sgd_solver.cpp:112] Iteration 30100, lr = 0.1
I0524 03:45:23.037431 11835 solver.cpp:239] Iteration 30110 (1.45638 iter/s, 6.86632s/10 iters), loss = 6.72795
I0524 03:45:23.037479 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72795 (* 1 = 6.72795 loss)
I0524 03:45:23.037605 11835 sgd_solver.cpp:112] Iteration 30110, lr = 0.1
I0524 03:45:30.310457 11835 solver.cpp:239] Iteration 30120 (1.37501 iter/s, 7.27269s/10 iters), loss = 5.79005
I0524 03:45:30.310510 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79005 (* 1 = 5.79005 loss)
I0524 03:45:30.310734 11835 sgd_solver.cpp:112] Iteration 30120, lr = 0.1
I0524 03:45:38.116356 11835 solver.cpp:239] Iteration 30130 (1.28114 iter/s, 7.80555s/10 iters), loss = 6.03626
I0524 03:45:38.116405 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03626 (* 1 = 6.03626 loss)
I0524 03:45:38.116956 11835 sgd_solver.cpp:112] Iteration 30130, lr = 0.1
I0524 03:45:45.106545 11835 solver.cpp:239] Iteration 30140 (1.43064 iter/s, 6.98986s/10 iters), loss = 5.52534
I0524 03:45:45.106603 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.52534 (* 1 = 5.52534 loss)
I0524 03:45:45.114542 11835 sgd_solver.cpp:112] Iteration 30140, lr = 0.1
I0524 03:45:52.491456 11835 solver.cpp:239] Iteration 30150 (1.35418 iter/s, 7.38457s/10 iters), loss = 6.05904
I0524 03:45:52.491729 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05904 (* 1 = 6.05904 loss)
I0524 03:45:52.491780 11835 sgd_solver.cpp:112] Iteration 30150, lr = 0.1
I0524 03:45:59.235994 11835 solver.cpp:239] Iteration 30160 (1.48282 iter/s, 6.74392s/10 iters), loss = 7.04055
I0524 03:45:59.236048 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04055 (* 1 = 7.04055 loss)
I0524 03:45:59.236295 11835 sgd_solver.cpp:112] Iteration 30160, lr = 0.1
I0524 03:46:06.756584 11835 solver.cpp:239] Iteration 30170 (1.32974 iter/s, 7.52026s/10 iters), loss = 6.96279
I0524 03:46:06.756623 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96279 (* 1 = 6.96279 loss)
I0524 03:46:06.756749 11835 sgd_solver.cpp:112] Iteration 30170, lr = 0.1
I0524 03:46:14.804692 11835 solver.cpp:239] Iteration 30180 (1.24258 iter/s, 8.04775s/10 iters), loss = 5.99538
I0524 03:46:14.804747 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99538 (* 1 = 5.99538 loss)
I0524 03:46:14.804982 11835 sgd_solver.cpp:112] Iteration 30180, lr = 0.1
I0524 03:46:23.024406 11835 solver.cpp:239] Iteration 30190 (1.21664 iter/s, 8.21933s/10 iters), loss = 6.10645
I0524 03:46:23.024667 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10645 (* 1 = 6.10645 loss)
I0524 03:46:23.024726 11835 sgd_solver.cpp:112] Iteration 30190, lr = 0.1
I0524 03:46:29.901465 11835 solver.cpp:239] Iteration 30200 (1.45421 iter/s, 6.87658s/10 iters), loss = 6.6124
I0524 03:46:29.901517 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6124 (* 1 = 6.6124 loss)
I0524 03:46:29.901532 11835 sgd_solver.cpp:112] Iteration 30200, lr = 0.1
I0524 03:46:36.426980 11835 solver.cpp:239] Iteration 30210 (1.53278 iter/s, 6.5241s/10 iters), loss = 5.71535
I0524 03:46:36.427026 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71535 (* 1 = 5.71535 loss)
I0524 03:46:36.427287 11835 sgd_solver.cpp:112] Iteration 30210, lr = 0.1
I0524 03:46:42.873414 11835 solver.cpp:239] Iteration 30220 (1.55132 iter/s, 6.44613s/10 iters), loss = 6.51933
I0524 03:46:42.873471 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51933 (* 1 = 6.51933 loss)
I0524 03:46:42.873710 11835 sgd_solver.cpp:112] Iteration 30220, lr = 0.1
I0524 03:46:49.639792 11835 solver.cpp:239] Iteration 30230 (1.47796 iter/s, 6.76607s/10 iters), loss = 5.82525
I0524 03:46:49.639835 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82525 (* 1 = 5.82525 loss)
I0524 03:46:49.640111 11835 sgd_solver.cpp:112] Iteration 30230, lr = 0.1
I0524 03:46:57.507279 11835 solver.cpp:239] Iteration 30240 (1.27111 iter/s, 7.86714s/10 iters), loss = 6.81705
I0524 03:46:57.507607 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81705 (* 1 = 6.81705 loss)
I0524 03:46:57.747920 11835 sgd_solver.cpp:112] Iteration 30240, lr = 0.1
I0524 03:47:05.004050 11835 solver.cpp:239] Iteration 30250 (1.33401 iter/s, 7.49622s/10 iters), loss = 6.80802
I0524 03:47:05.004098 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80802 (* 1 = 6.80802 loss)
I0524 03:47:05.004217 11835 sgd_solver.cpp:112] Iteration 30250, lr = 0.1
I0524 03:47:10.874984 11835 solver.cpp:239] Iteration 30260 (1.70338 iter/s, 5.87067s/10 iters), loss = 7.46108
I0524 03:47:10.875023 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.46108 (* 1 = 7.46108 loss)
I0524 03:47:10.875161 11835 sgd_solver.cpp:112] Iteration 30260, lr = 0.1
I0524 03:47:17.596231 11835 solver.cpp:239] Iteration 30270 (1.48789 iter/s, 6.72095s/10 iters), loss = 6.59587
I0524 03:47:17.596278 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59587 (* 1 = 6.59587 loss)
I0524 03:47:17.596451 11835 sgd_solver.cpp:112] Iteration 30270, lr = 0.1
I0524 03:47:26.012900 11835 solver.cpp:239] Iteration 30280 (1.18817 iter/s, 8.4163s/10 iters), loss = 6.87364
I0524 03:47:26.012944 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87364 (* 1 = 6.87364 loss)
I0524 03:47:26.013449 11835 sgd_solver.cpp:112] Iteration 30280, lr = 0.1
I0524 03:47:34.972772 11835 solver.cpp:239] Iteration 30290 (1.11614 iter/s, 8.95947s/10 iters), loss = 6.16621
I0524 03:47:34.973034 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16621 (* 1 = 6.16621 loss)
I0524 03:47:36.042052 11835 sgd_solver.cpp:112] Iteration 30290, lr = 0.1
I0524 03:47:44.817395 11835 solver.cpp:239] Iteration 30300 (1.01585 iter/s, 9.84402s/10 iters), loss = 6.04192
I0524 03:47:44.817447 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04192 (* 1 = 6.04192 loss)
I0524 03:47:44.817564 11835 sgd_solver.cpp:112] Iteration 30300, lr = 0.1
I0524 03:47:51.946818 11835 solver.cpp:239] Iteration 30310 (1.4027 iter/s, 7.1291s/10 iters), loss = 6.1027
I0524 03:47:51.946869 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1027 (* 1 = 6.1027 loss)
I0524 03:47:51.946962 11835 sgd_solver.cpp:112] Iteration 30310, lr = 0.1
I0524 03:47:57.903295 11835 solver.cpp:239] Iteration 30320 (1.67892 iter/s, 5.9562s/10 iters), loss = 5.95643
I0524 03:47:57.903338 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95643 (* 1 = 5.95643 loss)
I0524 03:47:57.903592 11835 sgd_solver.cpp:112] Iteration 30320, lr = 0.1
I0524 03:48:04.761323 11835 solver.cpp:239] Iteration 30330 (1.45821 iter/s, 6.85772s/10 iters), loss = 6.82818
I0524 03:48:04.761373 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.82818 (* 1 = 6.82818 loss)
I0524 03:48:04.761523 11835 sgd_solver.cpp:112] Iteration 30330, lr = 0.1
I0524 03:48:11.626415 11835 solver.cpp:239] Iteration 30340 (1.45671 iter/s, 6.86479s/10 iters), loss = 6.27957
I0524 03:48:11.626607 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27957 (* 1 = 6.27957 loss)
I0524 03:48:12.309572 11835 sgd_solver.cpp:112] Iteration 30340, lr = 0.1
I0524 03:48:18.927012 11835 solver.cpp:239] Iteration 30350 (1.36983 iter/s, 7.30016s/10 iters), loss = 6.77318
I0524 03:48:18.927055 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.77318 (* 1 = 6.77318 loss)
I0524 03:48:18.997902 11835 sgd_solver.cpp:112] Iteration 30350, lr = 0.1
I0524 03:48:27.118940 11835 solver.cpp:239] Iteration 30360 (1.22077 iter/s, 8.19157s/10 iters), loss = 6.86286
I0524 03:48:27.118985 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86286 (* 1 = 6.86286 loss)
I0524 03:48:27.119118 11835 sgd_solver.cpp:112] Iteration 30360, lr = 0.1
I0524 03:48:34.598665 11835 solver.cpp:239] Iteration 30370 (1.33701 iter/s, 7.47938s/10 iters), loss = 5.86446
I0524 03:48:34.598747 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86446 (* 1 = 5.86446 loss)
I0524 03:48:34.598956 11835 sgd_solver.cpp:112] Iteration 30370, lr = 0.1
I0524 03:48:44.005079 11835 solver.cpp:239] Iteration 30380 (1.06315 iter/s, 9.40597s/10 iters), loss = 5.83495
I0524 03:48:44.005309 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83495 (* 1 = 5.83495 loss)
I0524 03:48:44.532100 11835 sgd_solver.cpp:112] Iteration 30380, lr = 0.1
I0524 03:48:52.886739 11835 solver.cpp:239] Iteration 30390 (1.12599 iter/s, 8.88108s/10 iters), loss = 7.16754
I0524 03:48:52.886798 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.16754 (* 1 = 7.16754 loss)
I0524 03:48:52.886888 11835 sgd_solver.cpp:112] Iteration 30390, lr = 0.1
I0524 03:48:59.855007 11835 solver.cpp:239] Iteration 30400 (1.43514 iter/s, 6.96795s/10 iters), loss = 6.32612
I0524 03:48:59.855048 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32612 (* 1 = 6.32612 loss)
I0524 03:49:00.208421 11835 sgd_solver.cpp:112] Iteration 30400, lr = 0.1
I0524 03:49:07.314601 11835 solver.cpp:239] Iteration 30410 (1.34062 iter/s, 7.45926s/10 iters), loss = 6.67296
I0524 03:49:07.314646 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67296 (* 1 = 6.67296 loss)
I0524 03:49:07.314719 11835 sgd_solver.cpp:112] Iteration 30410, lr = 0.1
I0524 03:49:13.156147 11835 solver.cpp:239] Iteration 30420 (1.71195 iter/s, 5.84128s/10 iters), loss = 6.22865
I0524 03:49:13.156185 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22865 (* 1 = 6.22865 loss)
I0524 03:49:13.156196 11835 sgd_solver.cpp:112] Iteration 30420, lr = 0.1
I0524 03:49:19.876672 11835 solver.cpp:239] Iteration 30430 (1.48808 iter/s, 6.72009s/10 iters), loss = 6.15251
I0524 03:49:19.876796 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15251 (* 1 = 6.15251 loss)
I0524 03:49:19.921104 11835 sgd_solver.cpp:112] Iteration 30430, lr = 0.1
I0524 03:49:28.209458 11835 solver.cpp:239] Iteration 30440 (1.20014 iter/s, 8.33234s/10 iters), loss = 6.08896
I0524 03:49:28.209506 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08896 (* 1 = 6.08896 loss)
I0524 03:49:28.240151 11835 sgd_solver.cpp:112] Iteration 30440, lr = 0.1
I0524 03:49:37.246714 11835 solver.cpp:239] Iteration 30450 (1.10658 iter/s, 9.03685s/10 iters), loss = 7.22393
I0524 03:49:37.246763 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.22393 (* 1 = 7.22393 loss)
I0524 03:49:37.246862 11835 sgd_solver.cpp:112] Iteration 30450, lr = 0.1
I0524 03:49:42.807907 11835 solver.cpp:239] Iteration 30460 (1.79826 iter/s, 5.56093s/10 iters), loss = 7.07903
I0524 03:49:42.807955 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.07903 (* 1 = 7.07903 loss)
I0524 03:49:42.808207 11835 sgd_solver.cpp:112] Iteration 30460, lr = 0.1
I0524 03:49:49.490061 11835 solver.cpp:239] Iteration 30470 (1.49659 iter/s, 6.68184s/10 iters), loss = 5.87891
I0524 03:49:49.490116 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87891 (* 1 = 5.87891 loss)
I0524 03:49:49.490439 11835 sgd_solver.cpp:112] Iteration 30470, lr = 0.1
I0524 03:49:56.545516 11835 solver.cpp:239] Iteration 30480 (1.41741 iter/s, 7.05513s/10 iters), loss = 6.68435
I0524 03:49:56.545619 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.68435 (* 1 = 6.68435 loss)
I0524 03:49:57.434875 11835 sgd_solver.cpp:112] Iteration 30480, lr = 0.1
I0524 03:50:04.732276 11835 solver.cpp:239] Iteration 30490 (1.22154 iter/s, 8.18635s/10 iters), loss = 5.70591
I0524 03:50:04.732317 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.70591 (* 1 = 5.70591 loss)
I0524 03:50:04.732419 11835 sgd_solver.cpp:112] Iteration 30490, lr = 0.1
I0524 03:50:11.031533 11835 solver.cpp:239] Iteration 30500 (1.58756 iter/s, 6.29897s/10 iters), loss = 6.55496
I0524 03:50:11.031581 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55496 (* 1 = 6.55496 loss)
I0524 03:50:11.031597 11835 sgd_solver.cpp:112] Iteration 30500, lr = 0.1
I0524 03:50:16.619506 11835 solver.cpp:239] Iteration 30510 (1.78977 iter/s, 5.58731s/10 iters), loss = 5.89974
I0524 03:50:16.619556 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89974 (* 1 = 5.89974 loss)
I0524 03:50:16.852690 11835 sgd_solver.cpp:112] Iteration 30510, lr = 0.1
I0524 03:50:22.817667 11835 solver.cpp:239] Iteration 30520 (1.61345 iter/s, 6.19788s/10 iters), loss = 6.70812
I0524 03:50:22.817706 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70812 (* 1 = 6.70812 loss)
I0524 03:50:22.817726 11835 sgd_solver.cpp:112] Iteration 30520, lr = 0.1
I0524 03:50:30.004335 11835 solver.cpp:239] Iteration 30530 (1.39153 iter/s, 7.18635s/10 iters), loss = 7.04179
I0524 03:50:30.004619 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04179 (* 1 = 7.04179 loss)
I0524 03:50:30.004673 11835 sgd_solver.cpp:112] Iteration 30530, lr = 0.1
I0524 03:50:37.145676 11835 solver.cpp:239] Iteration 30540 (1.40042 iter/s, 7.14071s/10 iters), loss = 6.58571
I0524 03:50:37.145714 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58571 (* 1 = 6.58571 loss)
I0524 03:50:39.089673 11835 sgd_solver.cpp:112] Iteration 30540, lr = 0.1
I0524 03:50:44.941021 11835 solver.cpp:239] Iteration 30550 (1.28287 iter/s, 7.795s/10 iters), loss = 5.51632
I0524 03:50:44.941071 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.51632 (* 1 = 5.51632 loss)
I0524 03:50:44.942562 11835 sgd_solver.cpp:112] Iteration 30550, lr = 0.1
I0524 03:50:53.658367 11835 solver.cpp:239] Iteration 30560 (1.14719 iter/s, 8.71696s/10 iters), loss = 6.25945
I0524 03:50:53.658421 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25945 (* 1 = 6.25945 loss)
I0524 03:50:53.658529 11835 sgd_solver.cpp:112] Iteration 30560, lr = 0.1
I0524 03:50:59.716081 11835 solver.cpp:239] Iteration 30570 (1.65086 iter/s, 6.05744s/10 iters), loss = 6.95507
I0524 03:50:59.716116 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.95507 (* 1 = 6.95507 loss)
I0524 03:51:00.916234 11835 sgd_solver.cpp:112] Iteration 30570, lr = 0.1
I0524 03:51:06.703974 11835 solver.cpp:239] Iteration 30580 (1.43111 iter/s, 6.98757s/10 iters), loss = 6.1976
I0524 03:51:06.704021 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1976 (* 1 = 6.1976 loss)
I0524 03:51:06.704038 11835 sgd_solver.cpp:112] Iteration 30580, lr = 0.1
I0524 03:51:13.609522 11835 solver.cpp:239] Iteration 30590 (1.44818 iter/s, 6.90523s/10 iters), loss = 6.50849
I0524 03:51:13.609576 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50849 (* 1 = 6.50849 loss)
I0524 03:51:13.609774 11835 sgd_solver.cpp:112] Iteration 30590, lr = 0.1
I0524 03:51:19.384341 11835 solver.cpp:239] Iteration 30600 (1.73174 iter/s, 5.77455s/10 iters), loss = 6.42506
I0524 03:51:19.384382 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42506 (* 1 = 6.42506 loss)
I0524 03:51:19.897975 11835 sgd_solver.cpp:112] Iteration 30600, lr = 0.1
I0524 03:51:26.075547 11835 solver.cpp:239] Iteration 30610 (1.49457 iter/s, 6.6909s/10 iters), loss = 7.09939
I0524 03:51:26.075598 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09939 (* 1 = 7.09939 loss)
I0524 03:51:26.075729 11835 sgd_solver.cpp:112] Iteration 30610, lr = 0.1
I0524 03:51:31.554870 11835 solver.cpp:239] Iteration 30620 (1.82513 iter/s, 5.47907s/10 iters), loss = 6.88349
I0524 03:51:31.555111 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88349 (* 1 = 6.88349 loss)
I0524 03:51:31.839880 11835 sgd_solver.cpp:112] Iteration 30620, lr = 0.1
I0524 03:51:38.037780 11835 solver.cpp:239] Iteration 30630 (1.54263 iter/s, 6.48242s/10 iters), loss = 6.33572
I0524 03:51:38.037830 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33572 (* 1 = 6.33572 loss)
I0524 03:51:38.038364 11835 sgd_solver.cpp:112] Iteration 30630, lr = 0.1
I0524 03:51:45.170099 11835 solver.cpp:239] Iteration 30640 (1.40213 iter/s, 7.132s/10 iters), loss = 6.29106
I0524 03:51:45.170141 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29106 (* 1 = 6.29106 loss)
I0524 03:51:45.170393 11835 sgd_solver.cpp:112] Iteration 30640, lr = 0.1
I0524 03:51:51.376238 11835 solver.cpp:239] Iteration 30650 (1.61139 iter/s, 6.20584s/10 iters), loss = 6.43477
I0524 03:51:51.376291 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43477 (* 1 = 6.43477 loss)
I0524 03:51:51.376307 11835 sgd_solver.cpp:112] Iteration 30650, lr = 0.1
I0524 03:51:59.154062 11835 solver.cpp:239] Iteration 30660 (1.28578 iter/s, 7.77737s/10 iters), loss = 7.17723
I0524 03:51:59.154120 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.17723 (* 1 = 7.17723 loss)
I0524 03:51:59.235671 11835 sgd_solver.cpp:112] Iteration 30660, lr = 0.1
I0524 03:52:06.418079 11835 solver.cpp:239] Iteration 30670 (1.37671 iter/s, 7.26368s/10 iters), loss = 6.96435
I0524 03:52:06.418350 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96435 (* 1 = 6.96435 loss)
I0524 03:52:06.418406 11835 sgd_solver.cpp:112] Iteration 30670, lr = 0.1
I0524 03:52:12.860740 11835 solver.cpp:239] Iteration 30680 (1.55227 iter/s, 6.44216s/10 iters), loss = 6.38492
I0524 03:52:12.860787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38492 (* 1 = 6.38492 loss)
I0524 03:52:12.860858 11835 sgd_solver.cpp:112] Iteration 30680, lr = 0.1
I0524 03:52:18.833921 11835 solver.cpp:239] Iteration 30690 (1.67423 iter/s, 5.9729s/10 iters), loss = 6.26816
I0524 03:52:18.833967 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26816 (* 1 = 6.26816 loss)
I0524 03:52:18.833981 11835 sgd_solver.cpp:112] Iteration 30690, lr = 0.1
I0524 03:52:25.741498 11835 solver.cpp:239] Iteration 30700 (1.44775 iter/s, 6.90727s/10 iters), loss = 5.78088
I0524 03:52:25.741545 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78088 (* 1 = 5.78088 loss)
I0524 03:52:25.741806 11835 sgd_solver.cpp:112] Iteration 30700, lr = 0.1
I0524 03:52:34.014838 11835 solver.cpp:239] Iteration 30710 (1.20876 iter/s, 8.27296s/10 iters), loss = 6.68206
I0524 03:52:34.014894 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.68206 (* 1 = 6.68206 loss)
I0524 03:52:34.015100 11835 sgd_solver.cpp:112] Iteration 30710, lr = 0.1
I0524 03:52:39.973978 11835 solver.cpp:239] Iteration 30720 (1.67817 iter/s, 5.95886s/10 iters), loss = 6.69838
I0524 03:52:39.974241 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69838 (* 1 = 6.69838 loss)
I0524 03:52:39.974292 11835 sgd_solver.cpp:112] Iteration 30720, lr = 0.1
I0524 03:52:46.251366 11835 solver.cpp:239] Iteration 30730 (1.59314 iter/s, 6.2769s/10 iters), loss = 5.21408
I0524 03:52:46.251405 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.21408 (* 1 = 5.21408 loss)
I0524 03:52:46.251528 11835 sgd_solver.cpp:112] Iteration 30730, lr = 0.1
I0524 03:52:53.959115 11835 solver.cpp:239] Iteration 30740 (1.29745 iter/s, 7.7074s/10 iters), loss = 6.96209
I0524 03:52:53.959177 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96209 (* 1 = 6.96209 loss)
I0524 03:52:53.959403 11835 sgd_solver.cpp:112] Iteration 30740, lr = 0.1
I0524 03:53:01.817950 11835 solver.cpp:239] Iteration 30750 (1.27251 iter/s, 7.85848s/10 iters), loss = 6.61535
I0524 03:53:01.817988 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61535 (* 1 = 6.61535 loss)
I0524 03:53:02.516057 11835 sgd_solver.cpp:112] Iteration 30750, lr = 0.1
I0524 03:53:11.455476 11835 solver.cpp:239] Iteration 30760 (1.03766 iter/s, 9.63711s/10 iters), loss = 7.13623
I0524 03:53:11.455638 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13623 (* 1 = 7.13623 loss)
I0524 03:53:11.455759 11835 sgd_solver.cpp:112] Iteration 30760, lr = 0.1
I0524 03:53:17.258090 11835 solver.cpp:239] Iteration 30770 (1.72347 iter/s, 5.80225s/10 iters), loss = 7.28618
I0524 03:53:17.258126 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.28618 (* 1 = 7.28618 loss)
I0524 03:53:17.258148 11835 sgd_solver.cpp:112] Iteration 30770, lr = 0.1
I0524 03:53:23.810096 11835 solver.cpp:239] Iteration 30780 (1.52632 iter/s, 6.5517s/10 iters), loss = 5.54716
I0524 03:53:23.810154 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.54716 (* 1 = 5.54716 loss)
I0524 03:53:23.810338 11835 sgd_solver.cpp:112] Iteration 30780, lr = 0.1
I0524 03:53:31.310593 11835 solver.cpp:239] Iteration 30790 (1.33331 iter/s, 7.50015s/10 iters), loss = 6.71107
I0524 03:53:31.310642 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71107 (* 1 = 6.71107 loss)
I0524 03:53:31.310886 11835 sgd_solver.cpp:112] Iteration 30790, lr = 0.1
I0524 03:53:37.664881 11835 solver.cpp:239] Iteration 30800 (1.57381 iter/s, 6.354s/10 iters), loss = 7.5655
I0524 03:53:37.664925 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.5655 (* 1 = 7.5655 loss)
I0524 03:53:37.665177 11835 sgd_solver.cpp:112] Iteration 30800, lr = 0.1
I0524 03:53:44.394127 11835 solver.cpp:239] Iteration 30810 (1.48612 iter/s, 6.72894s/10 iters), loss = 6.85154
I0524 03:53:44.394394 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.85154 (* 1 = 6.85154 loss)
I0524 03:53:44.394459 11835 sgd_solver.cpp:112] Iteration 30810, lr = 0.1
I0524 03:53:51.728502 11835 solver.cpp:239] Iteration 30820 (1.36354 iter/s, 7.33384s/10 iters), loss = 6.32502
I0524 03:53:51.728561 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32502 (* 1 = 6.32502 loss)
I0524 03:53:52.951686 11835 sgd_solver.cpp:112] Iteration 30820, lr = 0.1
I0524 03:53:58.988282 11835 solver.cpp:239] Iteration 30830 (1.37752 iter/s, 7.25944s/10 iters), loss = 5.82409
I0524 03:53:58.988332 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82409 (* 1 = 5.82409 loss)
I0524 03:53:59.870707 11835 sgd_solver.cpp:112] Iteration 30830, lr = 0.1
I0524 03:54:08.041390 11835 solver.cpp:239] Iteration 30840 (1.10464 iter/s, 9.05271s/10 iters), loss = 4.95536
I0524 03:54:08.041436 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.95536 (* 1 = 4.95536 loss)
I0524 03:54:08.041455 11835 sgd_solver.cpp:112] Iteration 30840, lr = 0.1
I0524 03:54:13.751844 11835 solver.cpp:239] Iteration 30850 (1.75159 iter/s, 5.70908s/10 iters), loss = 6.01966
I0524 03:54:13.751886 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01966 (* 1 = 6.01966 loss)
I0524 03:54:13.987037 11835 sgd_solver.cpp:112] Iteration 30850, lr = 0.1
I0524 03:54:20.425040 11835 solver.cpp:239] Iteration 30860 (1.4986 iter/s, 6.67288s/10 iters), loss = 7.36823
I0524 03:54:20.425150 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.36823 (* 1 = 7.36823 loss)
I0524 03:54:20.425235 11835 sgd_solver.cpp:112] Iteration 30860, lr = 0.1
I0524 03:54:26.291561 11835 solver.cpp:239] Iteration 30870 (1.70468 iter/s, 5.8662s/10 iters), loss = 6.55232
I0524 03:54:26.291604 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55232 (* 1 = 6.55232 loss)
I0524 03:54:26.476449 11835 sgd_solver.cpp:112] Iteration 30870, lr = 0.1
I0524 03:54:35.580384 11835 solver.cpp:239] Iteration 30880 (1.07661 iter/s, 9.28842s/10 iters), loss = 6.69978
I0524 03:54:35.580433 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69978 (* 1 = 6.69978 loss)
I0524 03:54:35.581638 11835 sgd_solver.cpp:112] Iteration 30880, lr = 0.1
I0524 03:54:41.167776 11835 solver.cpp:239] Iteration 30890 (1.78983 iter/s, 5.58713s/10 iters), loss = 5.73234
I0524 03:54:41.167815 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73234 (* 1 = 5.73234 loss)
I0524 03:54:41.344632 11835 sgd_solver.cpp:112] Iteration 30890, lr = 0.1
I0524 03:54:48.513001 11835 solver.cpp:239] Iteration 30900 (1.36149 iter/s, 7.34489s/10 iters), loss = 5.80319
I0524 03:54:48.513056 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80319 (* 1 = 5.80319 loss)
I0524 03:54:48.513278 11835 sgd_solver.cpp:112] Iteration 30900, lr = 0.1
I0524 03:54:56.087255 11835 solver.cpp:239] Iteration 30910 (1.32032 iter/s, 7.5739s/10 iters), loss = 6.5145
I0524 03:54:56.087553 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5145 (* 1 = 6.5145 loss)
I0524 03:54:56.087605 11835 sgd_solver.cpp:112] Iteration 30910, lr = 0.1
I0524 03:55:02.383364 11835 solver.cpp:239] Iteration 30920 (1.58858 iter/s, 6.29492s/10 iters), loss = 6.31975
I0524 03:55:02.383414 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31975 (* 1 = 6.31975 loss)
I0524 03:55:02.383513 11835 sgd_solver.cpp:112] Iteration 30920, lr = 0.1
I0524 03:55:08.105274 11835 solver.cpp:239] Iteration 30930 (1.74775 iter/s, 5.72165s/10 iters), loss = 5.59329
I0524 03:55:08.105315 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.59329 (* 1 = 5.59329 loss)
I0524 03:55:08.357962 11835 sgd_solver.cpp:112] Iteration 30930, lr = 0.1
I0524 03:55:14.149619 11835 solver.cpp:239] Iteration 30940 (1.65452 iter/s, 6.04406s/10 iters), loss = 6.67972
I0524 03:55:14.149670 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67972 (* 1 = 6.67972 loss)
I0524 03:55:14.149684 11835 sgd_solver.cpp:112] Iteration 30940, lr = 0.1
I0524 03:55:19.855271 11835 solver.cpp:239] Iteration 30950 (1.75277 iter/s, 5.70527s/10 iters), loss = 5.84025
I0524 03:55:19.855314 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84025 (* 1 = 5.84025 loss)
I0524 03:55:20.201493 11835 sgd_solver.cpp:112] Iteration 30950, lr = 0.1
I0524 03:55:30.333526 11835 solver.cpp:239] Iteration 30960 (0.954398 iter/s, 10.4778s/10 iters), loss = 5.64596
I0524 03:55:30.333770 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.64596 (* 1 = 5.64596 loss)
I0524 03:55:30.711055 11835 sgd_solver.cpp:112] Iteration 30960, lr = 0.1
I0524 03:55:39.178117 11835 solver.cpp:239] Iteration 30970 (1.1307 iter/s, 8.84404s/10 iters), loss = 7.48997
I0524 03:55:39.178171 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.48997 (* 1 = 7.48997 loss)
I0524 03:55:39.178262 11835 sgd_solver.cpp:112] Iteration 30970, lr = 0.1
I0524 03:55:44.750643 11835 solver.cpp:239] Iteration 30980 (1.7946 iter/s, 5.57226s/10 iters), loss = 6.23314
I0524 03:55:44.750685 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23314 (* 1 = 6.23314 loss)
I0524 03:55:44.750723 11835 sgd_solver.cpp:112] Iteration 30980, lr = 0.1
I0524 03:55:50.300945 11835 solver.cpp:239] Iteration 30990 (1.80215 iter/s, 5.54892s/10 iters), loss = 6.94722
I0524 03:55:50.301003 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.94722 (* 1 = 6.94722 loss)
I0524 03:55:50.301196 11835 sgd_solver.cpp:112] Iteration 30990, lr = 0.1
I0524 03:55:56.909652 11835 solver.cpp:239] Iteration 31000 (1.51322 iter/s, 6.6084s/10 iters), loss = 6.02743
I0524 03:55:56.909696 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02743 (* 1 = 6.02743 loss)
I0524 03:55:57.569067 11835 sgd_solver.cpp:112] Iteration 31000, lr = 0.1
I0524 03:56:04.099967 11835 solver.cpp:239] Iteration 31010 (1.39082 iter/s, 7.18999s/10 iters), loss = 6.76647
I0524 03:56:04.100210 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76647 (* 1 = 6.76647 loss)
I0524 03:56:04.100282 11835 sgd_solver.cpp:112] Iteration 31010, lr = 0.1
I0524 03:56:10.548876 11835 solver.cpp:239] Iteration 31020 (1.55076 iter/s, 6.44845s/10 iters), loss = 7.41261
I0524 03:56:10.548929 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.41261 (* 1 = 7.41261 loss)
I0524 03:56:10.578642 11835 sgd_solver.cpp:112] Iteration 31020, lr = 0.1
I0524 03:56:16.431143 11835 solver.cpp:239] Iteration 31030 (1.7001 iter/s, 5.882s/10 iters), loss = 6.06115
I0524 03:56:16.431183 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06115 (* 1 = 6.06115 loss)
I0524 03:56:16.431329 11835 sgd_solver.cpp:112] Iteration 31030, lr = 0.1
I0524 03:56:22.118366 11835 solver.cpp:239] Iteration 31040 (1.75841 iter/s, 5.68696s/10 iters), loss = 6.81025
I0524 03:56:22.118402 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81025 (* 1 = 6.81025 loss)
I0524 03:56:22.118537 11835 sgd_solver.cpp:112] Iteration 31040, lr = 0.1
I0524 03:56:27.668596 11835 solver.cpp:239] Iteration 31050 (1.80181 iter/s, 5.54996s/10 iters), loss = 5.55322
I0524 03:56:27.668649 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.55322 (* 1 = 5.55322 loss)
I0524 03:56:27.668998 11835 sgd_solver.cpp:112] Iteration 31050, lr = 0.1
I0524 03:56:34.650616 11835 solver.cpp:239] Iteration 31060 (1.43231 iter/s, 6.98171s/10 iters), loss = 5.92392
I0524 03:56:34.650913 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92392 (* 1 = 5.92392 loss)
I0524 03:56:34.650964 11835 sgd_solver.cpp:112] Iteration 31060, lr = 0.1
I0524 03:56:41.203949 11835 solver.cpp:239] Iteration 31070 (1.5263 iter/s, 6.55179s/10 iters), loss = 6.99266
I0524 03:56:41.204007 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.99266 (* 1 = 6.99266 loss)
I0524 03:56:41.204047 11835 sgd_solver.cpp:112] Iteration 31070, lr = 0.1
I0524 03:56:47.390074 11835 solver.cpp:239] Iteration 31080 (1.6166 iter/s, 6.18583s/10 iters), loss = 6.36759
I0524 03:56:47.390123 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36759 (* 1 = 6.36759 loss)
I0524 03:56:47.723321 11835 sgd_solver.cpp:112] Iteration 31080, lr = 0.1
I0524 03:56:53.583745 11835 solver.cpp:239] Iteration 31090 (1.61462 iter/s, 6.1934s/10 iters), loss = 6.81961
I0524 03:56:53.583782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81961 (* 1 = 6.81961 loss)
I0524 03:56:53.583798 11835 sgd_solver.cpp:112] Iteration 31090, lr = 0.1
I0524 03:56:59.103438 11835 solver.cpp:239] Iteration 31100 (1.81178 iter/s, 5.51944s/10 iters), loss = 7.57265
I0524 03:56:59.103489 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.57265 (* 1 = 7.57265 loss)
I0524 03:56:59.103688 11835 sgd_solver.cpp:112] Iteration 31100, lr = 0.1
I0524 03:57:06.138531 11835 solver.cpp:239] Iteration 31110 (1.42151 iter/s, 7.03477s/10 iters), loss = 6.46143
I0524 03:57:06.138635 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46143 (* 1 = 6.46143 loss)
I0524 03:57:06.401767 11835 sgd_solver.cpp:112] Iteration 31110, lr = 0.1
I0524 03:57:12.668926 11835 solver.cpp:239] Iteration 31120 (1.53138 iter/s, 6.53004s/10 iters), loss = 6.30891
I0524 03:57:12.668977 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30891 (* 1 = 6.30891 loss)
I0524 03:57:12.669386 11835 sgd_solver.cpp:112] Iteration 31120, lr = 0.1
I0524 03:57:18.593186 11835 solver.cpp:239] Iteration 31130 (1.68805 iter/s, 5.92399s/10 iters), loss = 6.41636
I0524 03:57:18.593226 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41636 (* 1 = 6.41636 loss)
I0524 03:57:18.593241 11835 sgd_solver.cpp:112] Iteration 31130, lr = 0.1
I0524 03:57:25.541731 11835 solver.cpp:239] Iteration 31140 (1.43921 iter/s, 6.94823s/10 iters), loss = 6.95144
I0524 03:57:25.541781 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.95144 (* 1 = 6.95144 loss)
I0524 03:57:25.563428 11835 sgd_solver.cpp:112] Iteration 31140, lr = 0.1
I0524 03:57:31.519984 11835 solver.cpp:239] Iteration 31150 (1.67281 iter/s, 5.97798s/10 iters), loss = 6.54601
I0524 03:57:31.520027 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54601 (* 1 = 6.54601 loss)
I0524 03:57:31.520308 11835 sgd_solver.cpp:112] Iteration 31150, lr = 0.1
I0524 03:57:37.591600 11835 solver.cpp:239] Iteration 31160 (1.64708 iter/s, 6.07133s/10 iters), loss = 5.79123
I0524 03:57:37.591806 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79123 (* 1 = 5.79123 loss)
I0524 03:57:37.591855 11835 sgd_solver.cpp:112] Iteration 31160, lr = 0.1
I0524 03:57:44.063925 11835 solver.cpp:239] Iteration 31170 (1.54526 iter/s, 6.47141s/10 iters), loss = 5.89586
I0524 03:57:44.063968 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89586 (* 1 = 5.89586 loss)
I0524 03:57:44.064369 11835 sgd_solver.cpp:112] Iteration 31170, lr = 0.1
I0524 03:57:50.813935 11835 solver.cpp:239] Iteration 31180 (1.48155 iter/s, 6.74969s/10 iters), loss = 7.42098
I0524 03:57:50.813990 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.42098 (* 1 = 7.42098 loss)
I0524 03:57:50.814515 11835 sgd_solver.cpp:112] Iteration 31180, lr = 0.1
I0524 03:57:59.811967 11835 solver.cpp:239] Iteration 31190 (1.1114 iter/s, 8.99764s/10 iters), loss = 6.05786
I0524 03:57:59.812016 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05786 (* 1 = 6.05786 loss)
I0524 03:57:59.813967 11835 sgd_solver.cpp:112] Iteration 31190, lr = 0.1
I0524 03:58:05.850788 11835 solver.cpp:239] Iteration 31200 (1.65603 iter/s, 6.03855s/10 iters), loss = 6.64755
I0524 03:58:05.850823 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64755 (* 1 = 6.64755 loss)
I0524 03:58:05.850839 11835 sgd_solver.cpp:112] Iteration 31200, lr = 0.1
I0524 03:58:12.205412 11835 solver.cpp:239] Iteration 31210 (1.57373 iter/s, 6.35433s/10 iters), loss = 6.58422
I0524 03:58:12.205574 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58422 (* 1 = 6.58422 loss)
I0524 03:58:12.207482 11835 sgd_solver.cpp:112] Iteration 31210, lr = 0.1
I0524 03:58:18.311806 11835 solver.cpp:239] Iteration 31220 (1.63773 iter/s, 6.10601s/10 iters), loss = 6.51979
I0524 03:58:18.311861 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51979 (* 1 = 6.51979 loss)
I0524 03:58:18.312203 11835 sgd_solver.cpp:112] Iteration 31220, lr = 0.1
I0524 03:58:25.902663 11835 solver.cpp:239] Iteration 31230 (1.31743 iter/s, 7.59052s/10 iters), loss = 7.09808
I0524 03:58:25.902727 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09808 (* 1 = 7.09808 loss)
I0524 03:58:25.902848 11835 sgd_solver.cpp:112] Iteration 31230, lr = 0.1
I0524 03:58:32.419644 11835 solver.cpp:239] Iteration 31240 (1.53453 iter/s, 6.51666s/10 iters), loss = 7.09945
I0524 03:58:32.419694 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09945 (* 1 = 7.09945 loss)
I0524 03:58:32.419937 11835 sgd_solver.cpp:112] Iteration 31240, lr = 0.1
I0524 03:58:40.595237 11835 solver.cpp:239] Iteration 31250 (1.22321 iter/s, 8.17522s/10 iters), loss = 7.631
I0524 03:58:40.595297 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.631 (* 1 = 7.631 loss)
I0524 03:58:40.665977 11835 sgd_solver.cpp:112] Iteration 31250, lr = 0.1
I0524 03:58:48.629705 11835 solver.cpp:239] Iteration 31260 (1.2447 iter/s, 8.03409s/10 iters), loss = 7.65303
I0524 03:58:48.629918 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.65303 (* 1 = 7.65303 loss)
I0524 03:58:48.629948 11835 sgd_solver.cpp:112] Iteration 31260, lr = 0.1
I0524 03:58:56.669950 11835 solver.cpp:239] Iteration 31270 (1.24399 iter/s, 8.03866s/10 iters), loss = 7.44276
I0524 03:58:56.670001 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.44276 (* 1 = 7.44276 loss)
I0524 03:58:56.684746 11835 sgd_solver.cpp:112] Iteration 31270, lr = 0.1
I0524 03:59:03.532219 11835 solver.cpp:239] Iteration 31280 (1.45731 iter/s, 6.86196s/10 iters), loss = 7.0393
I0524 03:59:03.532263 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.0393 (* 1 = 7.0393 loss)
I0524 03:59:03.532464 11835 sgd_solver.cpp:112] Iteration 31280, lr = 0.1
I0524 03:59:09.096642 11835 solver.cpp:239] Iteration 31290 (1.79722 iter/s, 5.56416s/10 iters), loss = 6.42734
I0524 03:59:09.096690 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42734 (* 1 = 6.42734 loss)
I0524 03:59:09.096704 11835 sgd_solver.cpp:112] Iteration 31290, lr = 0.1
I0524 03:59:16.324687 11835 solver.cpp:239] Iteration 31300 (1.38357 iter/s, 7.22765s/10 iters), loss = 7.26283
I0524 03:59:16.324728 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.26283 (* 1 = 7.26283 loss)
I0524 03:59:16.324869 11835 sgd_solver.cpp:112] Iteration 31300, lr = 0.1
I0524 03:59:22.965759 11835 solver.cpp:239] Iteration 31310 (1.50585 iter/s, 6.64077s/10 iters), loss = 6.32702
I0524 03:59:22.966069 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32702 (* 1 = 6.32702 loss)
I0524 03:59:22.966125 11835 sgd_solver.cpp:112] Iteration 31310, lr = 0.1
I0524 03:59:29.207819 11835 solver.cpp:239] Iteration 31320 (1.60227 iter/s, 6.24113s/10 iters), loss = 6.63362
I0524 03:59:29.207862 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63362 (* 1 = 6.63362 loss)
I0524 03:59:29.250494 11835 sgd_solver.cpp:112] Iteration 31320, lr = 0.1
I0524 03:59:36.773699 11835 solver.cpp:239] Iteration 31330 (1.32179 iter/s, 7.56552s/10 iters), loss = 6.53574
I0524 03:59:36.773772 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53574 (* 1 = 6.53574 loss)
I0524 03:59:36.773988 11835 sgd_solver.cpp:112] Iteration 31330, lr = 0.1
I0524 03:59:43.814851 11835 solver.cpp:239] Iteration 31340 (1.42029 iter/s, 7.04081s/10 iters), loss = 5.22852
I0524 03:59:43.814895 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.22852 (* 1 = 5.22852 loss)
I0524 03:59:43.815093 11835 sgd_solver.cpp:112] Iteration 31340, lr = 0.1
I0524 03:59:50.296347 11835 solver.cpp:239] Iteration 31350 (1.54292 iter/s, 6.4812s/10 iters), loss = 6.76527
I0524 03:59:50.296391 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76527 (* 1 = 6.76527 loss)
I0524 03:59:50.296674 11835 sgd_solver.cpp:112] Iteration 31350, lr = 0.1
I0524 03:59:56.895756 11835 solver.cpp:239] Iteration 31360 (1.51536 iter/s, 6.59911s/10 iters), loss = 6.94685
I0524 03:59:56.895884 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.94685 (* 1 = 6.94685 loss)
I0524 03:59:56.895920 11835 sgd_solver.cpp:112] Iteration 31360, lr = 0.1
I0524 04:00:03.357853 11835 solver.cpp:239] Iteration 31370 (1.54757 iter/s, 6.46173s/10 iters), loss = 6.81199
I0524 04:00:03.357902 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81199 (* 1 = 6.81199 loss)
I0524 04:00:03.359865 11835 sgd_solver.cpp:112] Iteration 31370, lr = 0.1
I0524 04:00:12.902801 11835 solver.cpp:239] Iteration 31380 (1.04772 iter/s, 9.54453s/10 iters), loss = 8.07443
I0524 04:00:12.902855 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.07443 (* 1 = 8.07443 loss)
I0524 04:00:13.678587 11835 sgd_solver.cpp:112] Iteration 31380, lr = 0.1
I0524 04:00:21.244679 11835 solver.cpp:239] Iteration 31390 (1.19882 iter/s, 8.3415s/10 iters), loss = 6.73663
I0524 04:00:21.244729 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73663 (* 1 = 6.73663 loss)
I0524 04:00:22.592854 11835 sgd_solver.cpp:112] Iteration 31390, lr = 0.1
I0524 04:00:28.766145 11835 solver.cpp:239] Iteration 31400 (1.32959 iter/s, 7.52112s/10 iters), loss = 5.71391
I0524 04:00:28.766258 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71391 (* 1 = 5.71391 loss)
I0524 04:00:28.768139 11835 sgd_solver.cpp:112] Iteration 31400, lr = 0.1
I0524 04:00:37.074275 11835 solver.cpp:239] Iteration 31410 (1.2037 iter/s, 8.3077s/10 iters), loss = 7.30827
I0524 04:00:37.074335 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.30827 (* 1 = 7.30827 loss)
I0524 04:00:37.088907 11835 sgd_solver.cpp:112] Iteration 31410, lr = 0.1
I0524 04:00:44.038497 11835 solver.cpp:239] Iteration 31420 (1.43597 iter/s, 6.96391s/10 iters), loss = 7.11913
I0524 04:00:44.038537 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11913 (* 1 = 7.11913 loss)
I0524 04:00:44.068861 11835 sgd_solver.cpp:112] Iteration 31420, lr = 0.1
I0524 04:00:50.165339 11835 solver.cpp:239] Iteration 31430 (1.63224 iter/s, 6.12656s/10 iters), loss = 6.86074
I0524 04:00:50.165390 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86074 (* 1 = 6.86074 loss)
I0524 04:00:50.165488 11835 sgd_solver.cpp:112] Iteration 31430, lr = 0.1
I0524 04:00:56.073649 11835 solver.cpp:239] Iteration 31440 (1.69261 iter/s, 5.90804s/10 iters), loss = 6.37756
I0524 04:00:56.073683 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37756 (* 1 = 6.37756 loss)
I0524 04:00:56.332288 11835 sgd_solver.cpp:112] Iteration 31440, lr = 0.1
I0524 04:01:03.135675 11835 solver.cpp:239] Iteration 31450 (1.41609 iter/s, 7.06171s/10 iters), loss = 6.57157
I0524 04:01:03.135850 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57157 (* 1 = 6.57157 loss)
I0524 04:01:03.135869 11835 sgd_solver.cpp:112] Iteration 31450, lr = 0.1
I0524 04:01:09.159179 11835 solver.cpp:239] Iteration 31460 (1.66058 iter/s, 6.02199s/10 iters), loss = 6.32212
I0524 04:01:09.159229 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32212 (* 1 = 6.32212 loss)
I0524 04:01:09.159245 11835 sgd_solver.cpp:112] Iteration 31460, lr = 0.1
I0524 04:01:18.311754 11835 solver.cpp:239] Iteration 31470 (1.09265 iter/s, 9.1521s/10 iters), loss = 6.65229
I0524 04:01:18.311812 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65229 (* 1 = 6.65229 loss)
I0524 04:01:18.311877 11835 sgd_solver.cpp:112] Iteration 31470, lr = 0.1
I0524 04:01:25.085273 11835 solver.cpp:239] Iteration 31480 (1.4764 iter/s, 6.77321s/10 iters), loss = 6.64115
I0524 04:01:25.085314 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64115 (* 1 = 6.64115 loss)
I0524 04:01:25.085456 11835 sgd_solver.cpp:112] Iteration 31480, lr = 0.1
I0524 04:01:33.817325 11835 solver.cpp:239] Iteration 31490 (1.14526 iter/s, 8.73167s/10 iters), loss = 7.03094
I0524 04:01:33.817481 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03094 (* 1 = 7.03094 loss)
I0524 04:01:33.817603 11835 sgd_solver.cpp:112] Iteration 31490, lr = 0.1
I0524 04:01:41.467217 11835 solver.cpp:239] Iteration 31500 (1.30728 iter/s, 7.64945s/10 iters), loss = 7.05691
I0524 04:01:41.467263 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.05691 (* 1 = 7.05691 loss)
I0524 04:01:41.467481 11835 sgd_solver.cpp:112] Iteration 31500, lr = 0.1
I0524 04:01:50.299190 11835 solver.cpp:239] Iteration 31510 (1.1323 iter/s, 8.83159s/10 iters), loss = 7.45041
I0524 04:01:50.299238 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.45041 (* 1 = 7.45041 loss)
I0524 04:01:50.299530 11835 sgd_solver.cpp:112] Iteration 31510, lr = 0.1
I0524 04:01:58.830381 11835 solver.cpp:239] Iteration 31520 (1.17222 iter/s, 8.53081s/10 iters), loss = 7.71335
I0524 04:01:58.830430 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.71335 (* 1 = 7.71335 loss)
I0524 04:01:58.830534 11835 sgd_solver.cpp:112] Iteration 31520, lr = 0.1
I0524 04:02:04.525197 11835 solver.cpp:239] Iteration 31530 (1.75607 iter/s, 5.69454s/10 iters), loss = 6.77126
I0524 04:02:04.525454 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.77126 (* 1 = 6.77126 loss)
I0524 04:02:04.525511 11835 sgd_solver.cpp:112] Iteration 31530, lr = 0.1
I0524 04:02:10.434044 11835 solver.cpp:239] Iteration 31540 (1.69251 iter/s, 5.90838s/10 iters), loss = 7.12884
I0524 04:02:10.434093 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.12884 (* 1 = 7.12884 loss)
I0524 04:02:10.436045 11835 sgd_solver.cpp:112] Iteration 31540, lr = 0.1
I0524 04:02:17.654183 11835 solver.cpp:239] Iteration 31550 (1.38508 iter/s, 7.21981s/10 iters), loss = 6.21205
I0524 04:02:17.654232 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21205 (* 1 = 6.21205 loss)
I0524 04:02:17.654397 11835 sgd_solver.cpp:112] Iteration 31550, lr = 0.1
I0524 04:02:25.398748 11835 solver.cpp:239] Iteration 31560 (1.29129 iter/s, 7.74421s/10 iters), loss = 6.61439
I0524 04:02:25.398804 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61439 (* 1 = 6.61439 loss)
I0524 04:02:25.399013 11835 sgd_solver.cpp:112] Iteration 31560, lr = 0.1
I0524 04:02:32.569401 11835 solver.cpp:239] Iteration 31570 (1.39464 iter/s, 7.17033s/10 iters), loss = 6.39157
I0524 04:02:32.569437 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39157 (* 1 = 6.39157 loss)
I0524 04:02:32.855649 11835 sgd_solver.cpp:112] Iteration 31570, lr = 0.1
I0524 04:02:39.546308 11835 solver.cpp:239] Iteration 31580 (1.43336 iter/s, 6.97659s/10 iters), loss = 6.97403
I0524 04:02:39.546434 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.97403 (* 1 = 6.97403 loss)
I0524 04:02:39.546456 11835 sgd_solver.cpp:112] Iteration 31580, lr = 0.1
I0524 04:02:47.536773 11835 solver.cpp:239] Iteration 31590 (1.25156 iter/s, 7.99003s/10 iters), loss = 6.91595
I0524 04:02:47.536828 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.91595 (* 1 = 6.91595 loss)
I0524 04:02:47.537063 11835 sgd_solver.cpp:112] Iteration 31590, lr = 0.1
I0524 04:02:53.527230 11835 solver.cpp:239] Iteration 31600 (1.6694 iter/s, 5.99017s/10 iters), loss = 6.55222
I0524 04:02:53.527277 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55222 (* 1 = 6.55222 loss)
I0524 04:02:53.527374 11835 sgd_solver.cpp:112] Iteration 31600, lr = 0.1
I0524 04:02:59.383553 11835 solver.cpp:239] Iteration 31610 (1.70764 iter/s, 5.85604s/10 iters), loss = 6.72684
I0524 04:02:59.383602 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72684 (* 1 = 6.72684 loss)
I0524 04:02:59.385502 11835 sgd_solver.cpp:112] Iteration 31610, lr = 0.1
I0524 04:03:05.426743 11835 solver.cpp:239] Iteration 31620 (1.65484 iter/s, 6.04287s/10 iters), loss = 6.29604
I0524 04:03:05.426786 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29604 (* 1 = 6.29604 loss)
I0524 04:03:05.519982 11835 sgd_solver.cpp:112] Iteration 31620, lr = 0.1
I0524 04:03:11.611090 11835 solver.cpp:239] Iteration 31630 (1.61706 iter/s, 6.18406s/10 iters), loss = 6.13331
I0524 04:03:11.611413 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13331 (* 1 = 6.13331 loss)
I0524 04:03:11.611470 11835 sgd_solver.cpp:112] Iteration 31630, lr = 0.1
I0524 04:03:18.187155 11835 solver.cpp:239] Iteration 31640 (1.5208 iter/s, 6.5755s/10 iters), loss = 7.24462
I0524 04:03:18.187209 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.24462 (* 1 = 7.24462 loss)
I0524 04:03:18.187381 11835 sgd_solver.cpp:112] Iteration 31640, lr = 0.1
I0524 04:03:23.992302 11835 solver.cpp:239] Iteration 31650 (1.72269 iter/s, 5.80488s/10 iters), loss = 5.63695
I0524 04:03:23.992341 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63695 (* 1 = 5.63695 loss)
I0524 04:03:23.992352 11835 sgd_solver.cpp:112] Iteration 31650, lr = 0.1
I0524 04:03:31.571573 11835 solver.cpp:239] Iteration 31660 (1.31946 iter/s, 7.57887s/10 iters), loss = 7.6048
I0524 04:03:31.571619 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.6048 (* 1 = 7.6048 loss)
I0524 04:03:31.571840 11835 sgd_solver.cpp:112] Iteration 31660, lr = 0.1
I0524 04:03:37.875567 11835 solver.cpp:239] Iteration 31670 (1.58637 iter/s, 6.30371s/10 iters), loss = 6.49861
I0524 04:03:37.875615 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49861 (* 1 = 6.49861 loss)
I0524 04:03:37.875824 11835 sgd_solver.cpp:112] Iteration 31670, lr = 0.1
I0524 04:03:45.660535 11835 solver.cpp:239] Iteration 31680 (1.28458 iter/s, 7.78462s/10 iters), loss = 6.08656
I0524 04:03:45.660769 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08656 (* 1 = 6.08656 loss)
I0524 04:03:45.660807 11835 sgd_solver.cpp:112] Iteration 31680, lr = 0.1
I0524 04:03:51.316545 11835 solver.cpp:239] Iteration 31690 (1.76846 iter/s, 5.65464s/10 iters), loss = 6.83561
I0524 04:03:51.316591 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83561 (* 1 = 6.83561 loss)
I0524 04:03:51.316606 11835 sgd_solver.cpp:112] Iteration 31690, lr = 0.1
I0524 04:03:57.700994 11835 solver.cpp:239] Iteration 31700 (1.5664 iter/s, 6.38408s/10 iters), loss = 6.40788
I0524 04:03:57.701046 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40788 (* 1 = 6.40788 loss)
I0524 04:03:57.701061 11835 sgd_solver.cpp:112] Iteration 31700, lr = 0.1
I0524 04:04:04.372684 11835 solver.cpp:239] Iteration 31710 (1.49902 iter/s, 6.67101s/10 iters), loss = 6.54931
I0524 04:04:04.372722 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54931 (* 1 = 6.54931 loss)
I0524 04:04:04.372735 11835 sgd_solver.cpp:112] Iteration 31710, lr = 0.1
I0524 04:04:10.489667 11835 solver.cpp:239] Iteration 31720 (1.63546 iter/s, 6.11449s/10 iters), loss = 6.68649
I0524 04:04:10.489715 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.68649 (* 1 = 6.68649 loss)
I0524 04:04:10.489913 11835 sgd_solver.cpp:112] Iteration 31720, lr = 0.1
I0524 04:04:17.535588 11835 solver.cpp:239] Iteration 31730 (1.41932 iter/s, 7.04561s/10 iters), loss = 5.68509
I0524 04:04:17.535820 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68509 (* 1 = 5.68509 loss)
I0524 04:04:17.535840 11835 sgd_solver.cpp:112] Iteration 31730, lr = 0.1
I0524 04:04:24.463518 11835 solver.cpp:239] Iteration 31740 (1.44354 iter/s, 6.9274s/10 iters), loss = 5.97657
I0524 04:04:24.463567 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97657 (* 1 = 5.97657 loss)
I0524 04:04:24.511373 11835 sgd_solver.cpp:112] Iteration 31740, lr = 0.1
I0524 04:04:30.185392 11835 solver.cpp:239] Iteration 31750 (1.74776 iter/s, 5.7216s/10 iters), loss = 6.4485
I0524 04:04:30.185439 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4485 (* 1 = 6.4485 loss)
I0524 04:04:30.185588 11835 sgd_solver.cpp:112] Iteration 31750, lr = 0.1
I0524 04:04:35.768786 11835 solver.cpp:239] Iteration 31760 (1.79111 iter/s, 5.58314s/10 iters), loss = 6.58307
I0524 04:04:35.768823 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58307 (* 1 = 6.58307 loss)
I0524 04:04:35.768837 11835 sgd_solver.cpp:112] Iteration 31760, lr = 0.1
I0524 04:04:43.267241 11835 solver.cpp:239] Iteration 31770 (1.33368 iter/s, 7.49805s/10 iters), loss = 7.61237
I0524 04:04:43.267292 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.61237 (* 1 = 7.61237 loss)
I0524 04:04:43.267309 11835 sgd_solver.cpp:112] Iteration 31770, lr = 0.1
I0524 04:04:50.296440 11835 solver.cpp:239] Iteration 31780 (1.42273 iter/s, 7.02873s/10 iters), loss = 6.34465
I0524 04:04:50.296541 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34465 (* 1 = 6.34465 loss)
I0524 04:04:50.296646 11835 sgd_solver.cpp:112] Iteration 31780, lr = 0.1
I0524 04:04:56.099339 11835 solver.cpp:239] Iteration 31790 (1.72337 iter/s, 5.80258s/10 iters), loss = 6.40573
I0524 04:04:56.099383 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40573 (* 1 = 6.40573 loss)
I0524 04:04:56.099450 11835 sgd_solver.cpp:112] Iteration 31790, lr = 0.1
I0524 04:05:02.968456 11835 solver.cpp:239] Iteration 31800 (1.45586 iter/s, 6.8688s/10 iters), loss = 7.01076
I0524 04:05:02.968508 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01076 (* 1 = 7.01076 loss)
I0524 04:05:03.026160 11835 sgd_solver.cpp:112] Iteration 31800, lr = 0.1
I0524 04:05:11.068599 11835 solver.cpp:239] Iteration 31810 (1.2346 iter/s, 8.09979s/10 iters), loss = 7.31359
I0524 04:05:11.068650 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.31359 (* 1 = 7.31359 loss)
I0524 04:05:11.068871 11835 sgd_solver.cpp:112] Iteration 31810, lr = 0.1
I0524 04:05:17.322829 11835 solver.cpp:239] Iteration 31820 (1.59899 iter/s, 6.25393s/10 iters), loss = 6.62892
I0524 04:05:17.322885 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62892 (* 1 = 6.62892 loss)
I0524 04:05:17.322952 11835 sgd_solver.cpp:112] Iteration 31820, lr = 0.1
I0524 04:05:23.584609 11835 solver.cpp:239] Iteration 31830 (1.59706 iter/s, 6.2615s/10 iters), loss = 6.17928
I0524 04:05:23.584864 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17928 (* 1 = 6.17928 loss)
I0524 04:05:24.168970 11835 sgd_solver.cpp:112] Iteration 31830, lr = 0.1
I0524 04:05:30.253139 11835 solver.cpp:239] Iteration 31840 (1.49969 iter/s, 6.66805s/10 iters), loss = 6.81762
I0524 04:05:30.253187 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81762 (* 1 = 6.81762 loss)
I0524 04:05:30.253265 11835 sgd_solver.cpp:112] Iteration 31840, lr = 0.1
I0524 04:05:36.113360 11835 solver.cpp:239] Iteration 31850 (1.7065 iter/s, 5.85995s/10 iters), loss = 5.41854
I0524 04:05:36.113399 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.41854 (* 1 = 5.41854 loss)
I0524 04:05:36.113517 11835 sgd_solver.cpp:112] Iteration 31850, lr = 0.1
I0524 04:05:42.527101 11835 solver.cpp:239] Iteration 31860 (1.55922 iter/s, 6.41345s/10 iters), loss = 6.68837
I0524 04:05:42.527153 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.68837 (* 1 = 6.68837 loss)
I0524 04:05:42.529088 11835 sgd_solver.cpp:112] Iteration 31860, lr = 0.1
I0524 04:05:48.348222 11835 solver.cpp:239] Iteration 31870 (1.71796 iter/s, 5.82085s/10 iters), loss = 6.58897
I0524 04:05:48.348259 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58897 (* 1 = 6.58897 loss)
I0524 04:05:48.348489 11835 sgd_solver.cpp:112] Iteration 31870, lr = 0.1
I0524 04:05:56.345532 11835 solver.cpp:239] Iteration 31880 (1.25047 iter/s, 7.99696s/10 iters), loss = 6.26071
I0524 04:05:56.345808 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26071 (* 1 = 6.26071 loss)
I0524 04:05:56.345857 11835 sgd_solver.cpp:112] Iteration 31880, lr = 0.1
I0524 04:06:02.365550 11835 solver.cpp:239] Iteration 31890 (1.66154 iter/s, 6.01852s/10 iters), loss = 5.87559
I0524 04:06:02.365588 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87559 (* 1 = 5.87559 loss)
I0524 04:06:02.407969 11835 sgd_solver.cpp:112] Iteration 31890, lr = 0.1
I0524 04:06:08.352824 11835 solver.cpp:239] Iteration 31900 (1.67029 iter/s, 5.98699s/10 iters), loss = 5.99192
I0524 04:06:08.352874 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99192 (* 1 = 5.99192 loss)
I0524 04:06:08.353010 11835 sgd_solver.cpp:112] Iteration 31900, lr = 0.1
I0524 04:06:16.249074 11835 solver.cpp:239] Iteration 31910 (1.26648 iter/s, 7.8959s/10 iters), loss = 7.08745
I0524 04:06:16.249125 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.08745 (* 1 = 7.08745 loss)
I0524 04:06:17.040510 11835 sgd_solver.cpp:112] Iteration 31910, lr = 0.1
I0524 04:06:22.837719 11835 solver.cpp:239] Iteration 31920 (1.51783 iter/s, 6.58835s/10 iters), loss = 5.38099
I0524 04:06:22.837757 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.38099 (* 1 = 5.38099 loss)
I0524 04:06:22.837772 11835 sgd_solver.cpp:112] Iteration 31920, lr = 0.1
I0524 04:06:31.059413 11835 solver.cpp:239] Iteration 31930 (1.21668 iter/s, 8.21909s/10 iters), loss = 7.08007
I0524 04:06:31.059649 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.08007 (* 1 = 7.08007 loss)
I0524 04:06:31.059706 11835 sgd_solver.cpp:112] Iteration 31930, lr = 0.1
I0524 04:06:38.294025 11835 solver.cpp:239] Iteration 31940 (1.38234 iter/s, 7.23411s/10 iters), loss = 6.69777
I0524 04:06:38.294077 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69777 (* 1 = 6.69777 loss)
I0524 04:06:38.294435 11835 sgd_solver.cpp:112] Iteration 31940, lr = 0.1
I0524 04:06:47.335716 11835 solver.cpp:239] Iteration 31950 (1.10604 iter/s, 9.0413s/10 iters), loss = 6.22861
I0524 04:06:47.335762 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22861 (* 1 = 6.22861 loss)
I0524 04:06:47.915340 11835 sgd_solver.cpp:112] Iteration 31950, lr = 0.1
I0524 04:06:54.260375 11835 solver.cpp:239] Iteration 31960 (1.44418 iter/s, 6.92433s/10 iters), loss = 5.81573
I0524 04:06:54.260429 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81573 (* 1 = 5.81573 loss)
I0524 04:06:54.352435 11835 sgd_solver.cpp:112] Iteration 31960, lr = 0.1
I0524 04:07:01.275765 11835 solver.cpp:239] Iteration 31970 (1.4255 iter/s, 7.01508s/10 iters), loss = 6.25484
I0524 04:07:01.275874 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25484 (* 1 = 6.25484 loss)
I0524 04:07:01.275934 11835 sgd_solver.cpp:112] Iteration 31970, lr = 0.1
I0524 04:07:06.996878 11835 solver.cpp:239] Iteration 31980 (1.74801 iter/s, 5.72079s/10 iters), loss = 6.63318
I0524 04:07:06.996922 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63318 (* 1 = 6.63318 loss)
I0524 04:07:07.227843 11835 sgd_solver.cpp:112] Iteration 31980, lr = 0.1
I0524 04:07:13.827477 11835 solver.cpp:239] Iteration 31990 (1.46407 iter/s, 6.83029s/10 iters), loss = 6.57411
I0524 04:07:13.827528 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57411 (* 1 = 6.57411 loss)
I0524 04:07:13.827670 11835 sgd_solver.cpp:112] Iteration 31990, lr = 0.1
I0524 04:07:22.245117 11835 solver.cpp:239] Iteration 32000 (1.18803 iter/s, 8.41726s/10 iters), loss = 6.27514
I0524 04:07:22.245165 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27514 (* 1 = 6.27514 loss)
I0524 04:07:22.245406 11835 sgd_solver.cpp:112] Iteration 32000, lr = 0.1
I0524 04:07:30.808157 11835 solver.cpp:239] Iteration 32010 (1.16786 iter/s, 8.56266s/10 iters), loss = 6.13213
I0524 04:07:30.808205 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13213 (* 1 = 6.13213 loss)
I0524 04:07:30.808285 11835 sgd_solver.cpp:112] Iteration 32010, lr = 0.1
I0524 04:07:36.610518 11835 solver.cpp:239] Iteration 32020 (1.72351 iter/s, 5.8021s/10 iters), loss = 6.35425
I0524 04:07:36.610765 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35425 (* 1 = 6.35425 loss)
I0524 04:07:36.610806 11835 sgd_solver.cpp:112] Iteration 32020, lr = 0.1
I0524 04:07:42.290210 11835 solver.cpp:239] Iteration 32030 (1.76095 iter/s, 5.67874s/10 iters), loss = 5.41288
I0524 04:07:42.290261 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.41288 (* 1 = 5.41288 loss)
I0524 04:07:42.290498 11835 sgd_solver.cpp:112] Iteration 32030, lr = 0.1
I0524 04:07:48.153661 11835 solver.cpp:239] Iteration 32040 (1.70556 iter/s, 5.86318s/10 iters), loss = 6.69007
I0524 04:07:48.153707 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69007 (* 1 = 6.69007 loss)
I0524 04:07:48.616960 11835 sgd_solver.cpp:112] Iteration 32040, lr = 0.1
I0524 04:07:55.422312 11835 solver.cpp:239] Iteration 32050 (1.37583 iter/s, 7.26832s/10 iters), loss = 5.93079
I0524 04:07:55.422361 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93079 (* 1 = 5.93079 loss)
I0524 04:07:55.422446 11835 sgd_solver.cpp:112] Iteration 32050, lr = 0.1
I0524 04:08:01.083235 11835 solver.cpp:239] Iteration 32060 (1.76658 iter/s, 5.66066s/10 iters), loss = 6.312
I0524 04:08:01.083273 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.312 (* 1 = 6.312 loss)
I0524 04:08:01.083286 11835 sgd_solver.cpp:112] Iteration 32060, lr = 0.1
I0524 04:08:06.685405 11835 solver.cpp:239] Iteration 32070 (1.7851 iter/s, 5.60192s/10 iters), loss = 5.97771
I0524 04:08:06.685618 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97771 (* 1 = 5.97771 loss)
I0524 04:08:07.268484 11835 sgd_solver.cpp:112] Iteration 32070, lr = 0.1
I0524 04:08:14.301570 11835 solver.cpp:239] Iteration 32080 (1.31308 iter/s, 7.6157s/10 iters), loss = 6.58454
I0524 04:08:14.301621 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58454 (* 1 = 6.58454 loss)
I0524 04:08:14.301673 11835 sgd_solver.cpp:112] Iteration 32080, lr = 0.1
I0524 04:08:20.959710 11835 solver.cpp:239] Iteration 32090 (1.50199 iter/s, 6.65784s/10 iters), loss = 8.04507
I0524 04:08:20.959759 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.04507 (* 1 = 8.04507 loss)
I0524 04:08:20.959879 11835 sgd_solver.cpp:112] Iteration 32090, lr = 0.1
I0524 04:08:29.542896 11835 solver.cpp:239] Iteration 32100 (1.16512 iter/s, 8.58281s/10 iters), loss = 5.70868
I0524 04:08:29.542939 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.70868 (* 1 = 5.70868 loss)
I0524 04:08:30.593305 11835 sgd_solver.cpp:112] Iteration 32100, lr = 0.1
I0524 04:08:37.194422 11835 solver.cpp:239] Iteration 32110 (1.30699 iter/s, 7.65118s/10 iters), loss = 6.69885
I0524 04:08:37.194671 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69885 (* 1 = 6.69885 loss)
I0524 04:08:37.194751 11835 sgd_solver.cpp:112] Iteration 32110, lr = 0.1
I0524 04:08:44.396092 11835 solver.cpp:239] Iteration 32120 (1.38871 iter/s, 7.20093s/10 iters), loss = 6.83895
I0524 04:08:44.396144 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83895 (* 1 = 6.83895 loss)
I0524 04:08:44.396160 11835 sgd_solver.cpp:112] Iteration 32120, lr = 0.1
I0524 04:08:52.240990 11835 solver.cpp:239] Iteration 32130 (1.27477 iter/s, 7.84455s/10 iters), loss = 7.16722
I0524 04:08:52.241040 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.16722 (* 1 = 7.16722 loss)
I0524 04:08:52.336380 11835 sgd_solver.cpp:112] Iteration 32130, lr = 0.1
I0524 04:09:00.462453 11835 solver.cpp:239] Iteration 32140 (1.21638 iter/s, 8.2211s/10 iters), loss = 6.63515
I0524 04:09:00.462504 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63515 (* 1 = 6.63515 loss)
I0524 04:09:00.462631 11835 sgd_solver.cpp:112] Iteration 32140, lr = 0.1
I0524 04:09:05.914165 11835 solver.cpp:239] Iteration 32150 (1.83437 iter/s, 5.45145s/10 iters), loss = 6.33821
I0524 04:09:05.914213 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33821 (* 1 = 6.33821 loss)
I0524 04:09:05.914228 11835 sgd_solver.cpp:112] Iteration 32150, lr = 0.1
I0524 04:09:12.769460 11835 solver.cpp:239] Iteration 32160 (1.45924 iter/s, 6.85288s/10 iters), loss = 6.73976
I0524 04:09:12.769731 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73976 (* 1 = 6.73976 loss)
I0524 04:09:12.769767 11835 sgd_solver.cpp:112] Iteration 32160, lr = 0.1
I0524 04:09:18.331097 11835 solver.cpp:239] Iteration 32170 (1.79818 iter/s, 5.56117s/10 iters), loss = 5.91898
I0524 04:09:18.331143 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91898 (* 1 = 5.91898 loss)
I0524 04:09:18.331310 11835 sgd_solver.cpp:112] Iteration 32170, lr = 0.1
I0524 04:09:23.831849 11835 solver.cpp:239] Iteration 32180 (1.81802 iter/s, 5.50049s/10 iters), loss = 5.96018
I0524 04:09:23.831888 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96018 (* 1 = 5.96018 loss)
I0524 04:09:23.832027 11835 sgd_solver.cpp:112] Iteration 32180, lr = 0.1
I0524 04:09:29.591140 11835 solver.cpp:239] Iteration 32190 (1.73641 iter/s, 5.75901s/10 iters), loss = 6.99336
I0524 04:09:29.591193 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.99336 (* 1 = 6.99336 loss)
I0524 04:09:29.591392 11835 sgd_solver.cpp:112] Iteration 32190, lr = 0.1
I0524 04:09:36.914415 11835 solver.cpp:239] Iteration 32200 (1.36557 iter/s, 7.32295s/10 iters), loss = 6.10325
I0524 04:09:36.914458 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10325 (* 1 = 6.10325 loss)
I0524 04:09:37.087975 11835 sgd_solver.cpp:112] Iteration 32200, lr = 0.1
I0524 04:09:44.601825 11835 solver.cpp:239] Iteration 32210 (1.30089 iter/s, 7.68707s/10 iters), loss = 5.60498
I0524 04:09:44.601927 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.60498 (* 1 = 5.60498 loss)
I0524 04:09:44.624720 11835 sgd_solver.cpp:112] Iteration 32210, lr = 0.1
I0524 04:09:51.904954 11835 solver.cpp:239] Iteration 32220 (1.36935 iter/s, 7.30275s/10 iters), loss = 5.72903
I0524 04:09:51.905004 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72903 (* 1 = 5.72903 loss)
I0524 04:09:51.905019 11835 sgd_solver.cpp:112] Iteration 32220, lr = 0.1
I0524 04:09:59.801534 11835 solver.cpp:239] Iteration 32230 (1.2666 iter/s, 7.89513s/10 iters), loss = 6.79829
I0524 04:09:59.801573 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79829 (* 1 = 6.79829 loss)
I0524 04:09:59.801787 11835 sgd_solver.cpp:112] Iteration 32230, lr = 0.1
I0524 04:10:05.331225 11835 solver.cpp:239] Iteration 32240 (1.8085 iter/s, 5.52943s/10 iters), loss = 5.96475
I0524 04:10:05.331267 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96475 (* 1 = 5.96475 loss)
I0524 04:10:05.331281 11835 sgd_solver.cpp:112] Iteration 32240, lr = 0.1
I0524 04:10:11.559741 11835 solver.cpp:239] Iteration 32250 (1.60559 iter/s, 6.22823s/10 iters), loss = 6.186
I0524 04:10:11.559787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.186 (* 1 = 6.186 loss)
I0524 04:10:11.559880 11835 sgd_solver.cpp:112] Iteration 32250, lr = 0.1
I0524 04:10:17.453045 11835 solver.cpp:239] Iteration 32260 (1.69692 iter/s, 5.89303s/10 iters), loss = 5.86797
I0524 04:10:17.453320 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86797 (* 1 = 5.86797 loss)
I0524 04:10:17.471691 11835 sgd_solver.cpp:112] Iteration 32260, lr = 0.1
I0524 04:10:23.767447 11835 solver.cpp:239] Iteration 32270 (1.5838 iter/s, 6.31393s/10 iters), loss = 5.87325
I0524 04:10:23.767496 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87325 (* 1 = 5.87325 loss)
I0524 04:10:23.767510 11835 sgd_solver.cpp:112] Iteration 32270, lr = 0.1
I0524 04:10:31.196003 11835 solver.cpp:239] Iteration 32280 (1.34625 iter/s, 7.42806s/10 iters), loss = 6.77253
I0524 04:10:31.196054 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.77253 (* 1 = 6.77253 loss)
I0524 04:10:31.196070 11835 sgd_solver.cpp:112] Iteration 32280, lr = 0.1
I0524 04:10:37.616698 11835 solver.cpp:239] Iteration 32290 (1.55756 iter/s, 6.42029s/10 iters), loss = 6.04092
I0524 04:10:37.616734 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04092 (* 1 = 6.04092 loss)
I0524 04:10:37.628991 11835 sgd_solver.cpp:112] Iteration 32290, lr = 0.1
I0524 04:10:44.106246 11835 solver.cpp:239] Iteration 32300 (1.54101 iter/s, 6.48924s/10 iters), loss = 6.15138
I0524 04:10:44.106304 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15138 (* 1 = 6.15138 loss)
I0524 04:10:44.106362 11835 sgd_solver.cpp:112] Iteration 32300, lr = 0.1
I0524 04:10:51.032954 11835 solver.cpp:239] Iteration 32310 (1.44375 iter/s, 6.9264s/10 iters), loss = 6.77907
I0524 04:10:51.033269 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.77907 (* 1 = 6.77907 loss)
I0524 04:10:51.033321 11835 sgd_solver.cpp:112] Iteration 32310, lr = 0.1
I0524 04:10:57.585497 11835 solver.cpp:239] Iteration 32320 (1.52649 iter/s, 6.55098s/10 iters), loss = 5.95411
I0524 04:10:57.585541 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95411 (* 1 = 5.95411 loss)
I0524 04:10:57.585631 11835 sgd_solver.cpp:112] Iteration 32320, lr = 0.1
I0524 04:11:03.898603 11835 solver.cpp:239] Iteration 32330 (1.58408 iter/s, 6.31282s/10 iters), loss = 6.42345
I0524 04:11:03.898648 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42345 (* 1 = 6.42345 loss)
I0524 04:11:03.899056 11835 sgd_solver.cpp:112] Iteration 32330, lr = 0.1
I0524 04:11:14.083802 11835 solver.cpp:239] Iteration 32340 (0.981859 iter/s, 10.1848s/10 iters), loss = 6.35429
I0524 04:11:14.083854 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35429 (* 1 = 6.35429 loss)
I0524 04:11:14.084033 11835 sgd_solver.cpp:112] Iteration 32340, lr = 0.1
I0524 04:11:20.160528 11835 solver.cpp:239] Iteration 32350 (1.6457 iter/s, 6.07644s/10 iters), loss = 5.11535
I0524 04:11:20.160579 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.11535 (* 1 = 5.11535 loss)
I0524 04:11:21.156918 11835 sgd_solver.cpp:112] Iteration 32350, lr = 0.1
I0524 04:11:31.207366 11835 solver.cpp:239] Iteration 32360 (0.905276 iter/s, 11.0464s/10 iters), loss = 6.19743
I0524 04:11:31.207423 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19743 (* 1 = 6.19743 loss)
I0524 04:11:31.207619 11835 sgd_solver.cpp:112] Iteration 32360, lr = 0.1
I0524 04:11:37.964069 11835 solver.cpp:239] Iteration 32370 (1.48008 iter/s, 6.75638s/10 iters), loss = 6.37912
I0524 04:11:37.964126 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37912 (* 1 = 6.37912 loss)
I0524 04:11:37.964258 11835 sgd_solver.cpp:112] Iteration 32370, lr = 0.1
I0524 04:11:44.141024 11835 solver.cpp:239] Iteration 32380 (1.61899 iter/s, 6.17667s/10 iters), loss = 6.83816
I0524 04:11:44.141059 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83816 (* 1 = 6.83816 loss)
I0524 04:11:44.141072 11835 sgd_solver.cpp:112] Iteration 32380, lr = 0.1
I0524 04:11:50.048674 11835 solver.cpp:239] Iteration 32390 (1.6928 iter/s, 5.90738s/10 iters), loss = 6.05079
I0524 04:11:50.048722 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05079 (* 1 = 6.05079 loss)
I0524 04:11:50.699110 11835 sgd_solver.cpp:112] Iteration 32390, lr = 0.1
I0524 04:11:57.544950 11835 solver.cpp:239] Iteration 32400 (1.33406 iter/s, 7.49592s/10 iters), loss = 5.88478
I0524 04:11:57.545059 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88478 (* 1 = 5.88478 loss)
I0524 04:11:57.545075 11835 sgd_solver.cpp:112] Iteration 32400, lr = 0.1
I0524 04:12:04.581851 11835 solver.cpp:239] Iteration 32410 (1.4212 iter/s, 7.03629s/10 iters), loss = 6.0862
I0524 04:12:04.581895 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0862 (* 1 = 6.0862 loss)
I0524 04:12:04.581907 11835 sgd_solver.cpp:112] Iteration 32410, lr = 0.1
I0524 04:12:10.434623 11835 solver.cpp:239] Iteration 32420 (1.70871 iter/s, 5.85236s/10 iters), loss = 5.68507
I0524 04:12:10.434685 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68507 (* 1 = 5.68507 loss)
I0524 04:12:10.977782 11835 sgd_solver.cpp:112] Iteration 32420, lr = 0.1
I0524 04:12:17.848073 11835 solver.cpp:239] Iteration 32430 (1.34896 iter/s, 7.41312s/10 iters), loss = 6.84072
I0524 04:12:17.848114 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84072 (* 1 = 6.84072 loss)
I0524 04:12:17.848228 11835 sgd_solver.cpp:112] Iteration 32430, lr = 0.1
I0524 04:12:25.122205 11835 solver.cpp:239] Iteration 32440 (1.3748 iter/s, 7.2738s/10 iters), loss = 6.93707
I0524 04:12:25.122258 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93707 (* 1 = 6.93707 loss)
I0524 04:12:25.122491 11835 sgd_solver.cpp:112] Iteration 32440, lr = 0.1
I0524 04:12:32.069483 11835 solver.cpp:239] Iteration 32450 (1.43948 iter/s, 6.94695s/10 iters), loss = 6.11328
I0524 04:12:32.069754 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11328 (* 1 = 6.11328 loss)
I0524 04:12:32.069806 11835 sgd_solver.cpp:112] Iteration 32450, lr = 0.1
I0524 04:12:39.360541 11835 solver.cpp:239] Iteration 32460 (1.37166 iter/s, 7.29041s/10 iters), loss = 6.12823
I0524 04:12:39.360580 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12823 (* 1 = 6.12823 loss)
I0524 04:12:39.374572 11835 sgd_solver.cpp:112] Iteration 32460, lr = 0.1
I0524 04:12:45.960736 11835 solver.cpp:239] Iteration 32470 (1.51518 iter/s, 6.59988s/10 iters), loss = 6.93485
I0524 04:12:45.960790 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93485 (* 1 = 6.93485 loss)
I0524 04:12:45.960990 11835 sgd_solver.cpp:112] Iteration 32470, lr = 0.1
I0524 04:12:51.962182 11835 solver.cpp:239] Iteration 32480 (1.66634 iter/s, 6.00117s/10 iters), loss = 5.91822
I0524 04:12:51.962218 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91822 (* 1 = 5.91822 loss)
I0524 04:12:52.198258 11835 sgd_solver.cpp:112] Iteration 32480, lr = 0.1
I0524 04:12:59.360591 11835 solver.cpp:239] Iteration 32490 (1.3517 iter/s, 7.39807s/10 iters), loss = 5.80174
I0524 04:12:59.360643 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80174 (* 1 = 5.80174 loss)
I0524 04:12:59.360734 11835 sgd_solver.cpp:112] Iteration 32490, lr = 0.1
I0524 04:13:06.228894 11835 solver.cpp:239] Iteration 32500 (1.45603 iter/s, 6.86799s/10 iters), loss = 7.52775
I0524 04:13:06.229128 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.52775 (* 1 = 7.52775 loss)
I0524 04:13:06.229185 11835 sgd_solver.cpp:112] Iteration 32500, lr = 0.1
I0524 04:13:11.824584 11835 solver.cpp:239] Iteration 32510 (1.78722 iter/s, 5.59528s/10 iters), loss = 6.98654
I0524 04:13:11.824635 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98654 (* 1 = 6.98654 loss)
I0524 04:13:11.825975 11835 sgd_solver.cpp:112] Iteration 32510, lr = 0.1
I0524 04:13:19.670608 11835 solver.cpp:239] Iteration 32520 (1.27459 iter/s, 7.84568s/10 iters), loss = 5.91038
I0524 04:13:19.670657 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91038 (* 1 = 5.91038 loss)
I0524 04:13:19.670892 11835 sgd_solver.cpp:112] Iteration 32520, lr = 0.1
I0524 04:13:25.243659 11835 solver.cpp:239] Iteration 32530 (1.79444 iter/s, 5.57278s/10 iters), loss = 5.16647
I0524 04:13:25.243705 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.16647 (* 1 = 5.16647 loss)
I0524 04:13:25.243719 11835 sgd_solver.cpp:112] Iteration 32530, lr = 0.1
I0524 04:13:31.260789 11835 solver.cpp:239] Iteration 32540 (1.66202 iter/s, 6.01678s/10 iters), loss = 5.87652
I0524 04:13:31.260828 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87652 (* 1 = 5.87652 loss)
I0524 04:13:31.373648 11835 sgd_solver.cpp:112] Iteration 32540, lr = 0.1
I0524 04:13:38.214004 11835 solver.cpp:239] Iteration 32550 (1.43825 iter/s, 6.95289s/10 iters), loss = 6.03904
I0524 04:13:38.214304 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03904 (* 1 = 6.03904 loss)
I0524 04:13:38.214359 11835 sgd_solver.cpp:112] Iteration 32550, lr = 0.1
I0524 04:13:44.421730 11835 solver.cpp:239] Iteration 32560 (1.61129 iter/s, 6.20621s/10 iters), loss = 5.74207
I0524 04:13:44.421774 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74207 (* 1 = 5.74207 loss)
I0524 04:13:44.421869 11835 sgd_solver.cpp:112] Iteration 32560, lr = 0.1
I0524 04:13:50.483053 11835 solver.cpp:239] Iteration 32570 (1.64988 iter/s, 6.06103s/10 iters), loss = 6.44354
I0524 04:13:50.483108 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44354 (* 1 = 6.44354 loss)
I0524 04:13:50.483217 11835 sgd_solver.cpp:112] Iteration 32570, lr = 0.1
I0524 04:13:59.661412 11835 solver.cpp:239] Iteration 32580 (1.08957 iter/s, 9.17795s/10 iters), loss = 6.38187
I0524 04:13:59.661460 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38187 (* 1 = 6.38187 loss)
I0524 04:13:59.661571 11835 sgd_solver.cpp:112] Iteration 32580, lr = 0.1
I0524 04:14:05.400396 11835 solver.cpp:239] Iteration 32590 (1.74255 iter/s, 5.73872s/10 iters), loss = 7.34252
I0524 04:14:05.400444 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.34252 (* 1 = 7.34252 loss)
I0524 04:14:05.400669 11835 sgd_solver.cpp:112] Iteration 32590, lr = 0.1
I0524 04:14:10.925963 11835 solver.cpp:239] Iteration 32600 (1.80986 iter/s, 5.5253s/10 iters), loss = 6.41964
I0524 04:14:10.926223 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41964 (* 1 = 6.41964 loss)
I0524 04:14:10.926280 11835 sgd_solver.cpp:112] Iteration 32600, lr = 0.1
I0524 04:14:16.773784 11835 solver.cpp:239] Iteration 32610 (1.71016 iter/s, 5.84739s/10 iters), loss = 5.80819
I0524 04:14:16.773819 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80819 (* 1 = 5.80819 loss)
I0524 04:14:16.773831 11835 sgd_solver.cpp:112] Iteration 32610, lr = 0.1
I0524 04:14:23.251992 11835 solver.cpp:239] Iteration 32620 (1.54371 iter/s, 6.47791s/10 iters), loss = 7.83088
I0524 04:14:23.252048 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.83088 (* 1 = 7.83088 loss)
I0524 04:14:23.252120 11835 sgd_solver.cpp:112] Iteration 32620, lr = 0.1
I0524 04:14:29.644589 11835 solver.cpp:239] Iteration 32630 (1.56438 iter/s, 6.3923s/10 iters), loss = 6.84527
I0524 04:14:29.644634 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84527 (* 1 = 6.84527 loss)
I0524 04:14:29.644912 11835 sgd_solver.cpp:112] Iteration 32630, lr = 0.1
I0524 04:14:36.525991 11835 solver.cpp:239] Iteration 32640 (1.45326 iter/s, 6.88109s/10 iters), loss = 6.09211
I0524 04:14:36.526036 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09211 (* 1 = 6.09211 loss)
I0524 04:14:37.055433 11835 sgd_solver.cpp:112] Iteration 32640, lr = 0.1
I0524 04:14:43.453039 11835 solver.cpp:239] Iteration 32650 (1.44368 iter/s, 6.92673s/10 iters), loss = 6.35346
I0524 04:14:43.453125 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35346 (* 1 = 6.35346 loss)
I0524 04:14:43.453199 11835 sgd_solver.cpp:112] Iteration 32650, lr = 0.1
I0524 04:14:51.063423 11835 solver.cpp:239] Iteration 32660 (1.31406 iter/s, 7.61001s/10 iters), loss = 6.83788
I0524 04:14:51.063467 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83788 (* 1 = 6.83788 loss)
I0524 04:14:51.065423 11835 sgd_solver.cpp:112] Iteration 32660, lr = 0.1
I0524 04:14:58.119410 11835 solver.cpp:239] Iteration 32670 (1.4173 iter/s, 7.05567s/10 iters), loss = 6.87713
I0524 04:14:58.119458 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87713 (* 1 = 6.87713 loss)
I0524 04:14:58.119583 11835 sgd_solver.cpp:112] Iteration 32670, lr = 0.1
I0524 04:15:05.460353 11835 solver.cpp:239] Iteration 32680 (1.36228 iter/s, 7.34062s/10 iters), loss = 5.30436
I0524 04:15:05.460402 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.30436 (* 1 = 5.30436 loss)
I0524 04:15:05.460533 11835 sgd_solver.cpp:112] Iteration 32680, lr = 0.1
I0524 04:15:11.323248 11835 solver.cpp:239] Iteration 32690 (1.70572 iter/s, 5.86263s/10 iters), loss = 5.64731
I0524 04:15:11.323292 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.64731 (* 1 = 5.64731 loss)
I0524 04:15:11.973109 11835 sgd_solver.cpp:112] Iteration 32690, lr = 0.1
I0524 04:15:18.531005 11835 solver.cpp:239] Iteration 32700 (1.38746 iter/s, 7.20743s/10 iters), loss = 6.18845
I0524 04:15:18.531139 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18845 (* 1 = 6.18845 loss)
I0524 04:15:18.531157 11835 sgd_solver.cpp:112] Iteration 32700, lr = 0.1
I0524 04:15:24.476511 11835 solver.cpp:239] Iteration 32710 (1.68214 iter/s, 5.94482s/10 iters), loss = 6.11349
I0524 04:15:24.476552 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11349 (* 1 = 6.11349 loss)
I0524 04:15:24.801232 11835 sgd_solver.cpp:112] Iteration 32710, lr = 0.1
I0524 04:15:30.988399 11835 solver.cpp:239] Iteration 32720 (1.53572 iter/s, 6.51159s/10 iters), loss = 6.0568
I0524 04:15:30.988446 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0568 (* 1 = 6.0568 loss)
I0524 04:15:30.988461 11835 sgd_solver.cpp:112] Iteration 32720, lr = 0.1
I0524 04:15:37.695133 11835 solver.cpp:239] Iteration 32730 (1.49121 iter/s, 6.70595s/10 iters), loss = 6.55523
I0524 04:15:37.695190 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55523 (* 1 = 6.55523 loss)
I0524 04:15:37.695405 11835 sgd_solver.cpp:112] Iteration 32730, lr = 0.1
I0524 04:15:44.465422 11835 solver.cpp:239] Iteration 32740 (1.47711 iter/s, 6.76998s/10 iters), loss = 6.58746
I0524 04:15:44.465464 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58746 (* 1 = 6.58746 loss)
I0524 04:15:44.465478 11835 sgd_solver.cpp:112] Iteration 32740, lr = 0.1
I0524 04:15:51.945632 11835 solver.cpp:239] Iteration 32750 (1.33692 iter/s, 7.47989s/10 iters), loss = 7.33298
I0524 04:15:51.945878 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.33298 (* 1 = 7.33298 loss)
I0524 04:15:52.428069 11835 sgd_solver.cpp:112] Iteration 32750, lr = 0.1
I0524 04:15:59.618749 11835 solver.cpp:239] Iteration 32760 (1.30334 iter/s, 7.67261s/10 iters), loss = 6.73853
I0524 04:15:59.618801 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73853 (* 1 = 6.73853 loss)
I0524 04:15:59.775384 11835 sgd_solver.cpp:112] Iteration 32760, lr = 0.1
I0524 04:16:07.626665 11835 solver.cpp:239] Iteration 32770 (1.24882 iter/s, 8.00755s/10 iters), loss = 5.3216
I0524 04:16:07.626732 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.3216 (* 1 = 5.3216 loss)
I0524 04:16:07.627087 11835 sgd_solver.cpp:112] Iteration 32770, lr = 0.1
I0524 04:16:13.725551 11835 solver.cpp:239] Iteration 32780 (1.63972 iter/s, 6.09859s/10 iters), loss = 5.96328
I0524 04:16:13.725591 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96328 (* 1 = 5.96328 loss)
I0524 04:16:14.779570 11835 sgd_solver.cpp:112] Iteration 32780, lr = 0.1
I0524 04:16:20.588129 11835 solver.cpp:239] Iteration 32790 (1.45724 iter/s, 6.86226s/10 iters), loss = 7.6343
I0524 04:16:20.588177 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.6343 (* 1 = 7.6343 loss)
I0524 04:16:20.588201 11835 sgd_solver.cpp:112] Iteration 32790, lr = 0.1
I0524 04:16:27.033160 11835 solver.cpp:239] Iteration 32800 (1.55219 iter/s, 6.44251s/10 iters), loss = 5.54254
I0524 04:16:27.033416 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.54254 (* 1 = 5.54254 loss)
I0524 04:16:27.037539 11835 sgd_solver.cpp:112] Iteration 32800, lr = 0.1
I0524 04:16:33.007195 11835 solver.cpp:239] Iteration 32810 (1.67404 iter/s, 5.97357s/10 iters), loss = 7.77342
I0524 04:16:33.007247 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.77342 (* 1 = 7.77342 loss)
I0524 04:16:33.007262 11835 sgd_solver.cpp:112] Iteration 32810, lr = 0.1
I0524 04:16:38.733992 11835 solver.cpp:239] Iteration 32820 (1.74628 iter/s, 5.72646s/10 iters), loss = 6.64705
I0524 04:16:38.734031 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64705 (* 1 = 6.64705 loss)
I0524 04:16:38.734043 11835 sgd_solver.cpp:112] Iteration 32820, lr = 0.1
I0524 04:16:44.245332 11835 solver.cpp:239] Iteration 32830 (1.81453 iter/s, 5.51107s/10 iters), loss = 4.89588
I0524 04:16:44.245386 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.89588 (* 1 = 4.89588 loss)
I0524 04:16:44.245487 11835 sgd_solver.cpp:112] Iteration 32830, lr = 0.1
I0524 04:16:50.935600 11835 solver.cpp:239] Iteration 32840 (1.49478 iter/s, 6.68996s/10 iters), loss = 6.06796
I0524 04:16:50.935638 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06796 (* 1 = 6.06796 loss)
I0524 04:16:50.935765 11835 sgd_solver.cpp:112] Iteration 32840, lr = 0.1
I0524 04:16:56.766273 11835 solver.cpp:239] Iteration 32850 (1.71515 iter/s, 5.8304s/10 iters), loss = 6.88484
I0524 04:16:56.766322 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88484 (* 1 = 6.88484 loss)
I0524 04:16:56.766337 11835 sgd_solver.cpp:112] Iteration 32850, lr = 0.1
I0524 04:17:02.379683 11835 solver.cpp:239] Iteration 32860 (1.78156 iter/s, 5.61307s/10 iters), loss = 5.63402
I0524 04:17:02.379956 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63402 (* 1 = 5.63402 loss)
I0524 04:17:02.380002 11835 sgd_solver.cpp:112] Iteration 32860, lr = 0.1
I0524 04:17:09.882448 11835 solver.cpp:239] Iteration 32870 (1.33297 iter/s, 7.50205s/10 iters), loss = 5.48632
I0524 04:17:09.882498 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.48632 (* 1 = 5.48632 loss)
I0524 04:17:09.882511 11835 sgd_solver.cpp:112] Iteration 32870, lr = 0.1
I0524 04:17:16.023985 11835 solver.cpp:239] Iteration 32880 (1.62837 iter/s, 6.14111s/10 iters), loss = 6.46502
I0524 04:17:16.024019 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46502 (* 1 = 6.46502 loss)
I0524 04:17:16.519232 11835 sgd_solver.cpp:112] Iteration 32880, lr = 0.1
I0524 04:17:24.881924 11835 solver.cpp:239] Iteration 32890 (1.12898 iter/s, 8.85756s/10 iters), loss = 6.52587
I0524 04:17:24.881970 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52587 (* 1 = 6.52587 loss)
I0524 04:17:24.882066 11835 sgd_solver.cpp:112] Iteration 32890, lr = 0.1
I0524 04:17:30.384095 11835 solver.cpp:239] Iteration 32900 (1.81755 iter/s, 5.5019s/10 iters), loss = 6.70514
I0524 04:17:30.384150 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70514 (* 1 = 6.70514 loss)
I0524 04:17:30.384389 11835 sgd_solver.cpp:112] Iteration 32900, lr = 0.1
I0524 04:17:37.643420 11835 solver.cpp:239] Iteration 32910 (1.3776 iter/s, 7.259s/10 iters), loss = 6.46313
I0524 04:17:37.643617 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46313 (* 1 = 6.46313 loss)
I0524 04:17:37.643659 11835 sgd_solver.cpp:112] Iteration 32910, lr = 0.1
I0524 04:17:43.544864 11835 solver.cpp:239] Iteration 32920 (1.69478 iter/s, 5.90048s/10 iters), loss = 6.56509
I0524 04:17:43.544914 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56509 (* 1 = 6.56509 loss)
I0524 04:17:43.545038 11835 sgd_solver.cpp:112] Iteration 32920, lr = 0.1
I0524 04:17:49.549062 11835 solver.cpp:239] Iteration 32930 (1.66558 iter/s, 6.00392s/10 iters), loss = 6.99804
I0524 04:17:49.549114 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.99804 (* 1 = 6.99804 loss)
I0524 04:17:49.549628 11835 sgd_solver.cpp:112] Iteration 32930, lr = 0.1
I0524 04:17:55.595526 11835 solver.cpp:239] Iteration 32940 (1.65394 iter/s, 6.04617s/10 iters), loss = 6.27027
I0524 04:17:55.595576 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27027 (* 1 = 6.27027 loss)
I0524 04:17:55.595589 11835 sgd_solver.cpp:112] Iteration 32940, lr = 0.1
I0524 04:18:01.748112 11835 solver.cpp:239] Iteration 32950 (1.62549 iter/s, 6.152s/10 iters), loss = 6.11055
I0524 04:18:01.748159 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11055 (* 1 = 6.11055 loss)
I0524 04:18:01.748280 11835 sgd_solver.cpp:112] Iteration 32950, lr = 0.1
I0524 04:18:07.904847 11835 solver.cpp:239] Iteration 32960 (1.62431 iter/s, 6.15645s/10 iters), loss = 6.61594
I0524 04:18:07.905153 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61594 (* 1 = 6.61594 loss)
I0524 04:18:08.050055 11835 sgd_solver.cpp:112] Iteration 32960, lr = 0.1
I0524 04:18:13.692502 11835 solver.cpp:239] Iteration 32970 (1.72796 iter/s, 5.78717s/10 iters), loss = 6.51239
I0524 04:18:13.692548 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51239 (* 1 = 6.51239 loss)
I0524 04:18:13.692771 11835 sgd_solver.cpp:112] Iteration 32970, lr = 0.1
I0524 04:18:19.620455 11835 solver.cpp:239] Iteration 32980 (1.687 iter/s, 5.92767s/10 iters), loss = 6.48024
I0524 04:18:19.620509 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48024 (* 1 = 6.48024 loss)
I0524 04:18:19.620707 11835 sgd_solver.cpp:112] Iteration 32980, lr = 0.1
I0524 04:18:25.201251 11835 solver.cpp:239] Iteration 32990 (1.79194 iter/s, 5.58053s/10 iters), loss = 6.83327
I0524 04:18:25.201288 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83327 (* 1 = 6.83327 loss)
I0524 04:18:25.201309 11835 sgd_solver.cpp:112] Iteration 32990, lr = 0.1
I0524 04:18:31.378376 11835 solver.cpp:239] Iteration 33000 (1.61895 iter/s, 6.17684s/10 iters), loss = 5.32317
I0524 04:18:31.378437 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.32317 (* 1 = 5.32317 loss)
I0524 04:18:31.380367 11835 sgd_solver.cpp:112] Iteration 33000, lr = 0.1
I0524 04:18:38.477711 11835 solver.cpp:239] Iteration 33010 (1.40865 iter/s, 7.099s/10 iters), loss = 5.75254
I0524 04:18:38.477913 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75254 (* 1 = 5.75254 loss)
I0524 04:18:38.478018 11835 sgd_solver.cpp:112] Iteration 33010, lr = 0.1
I0524 04:18:44.286551 11835 solver.cpp:239] Iteration 33020 (1.72163 iter/s, 5.80845s/10 iters), loss = 6.59269
I0524 04:18:44.286602 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59269 (* 1 = 6.59269 loss)
I0524 04:18:44.286684 11835 sgd_solver.cpp:112] Iteration 33020, lr = 0.1
I0524 04:18:52.245517 11835 solver.cpp:239] Iteration 33030 (1.2565 iter/s, 7.95862s/10 iters), loss = 5.57459
I0524 04:18:52.245568 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.57459 (* 1 = 5.57459 loss)
I0524 04:18:52.245932 11835 sgd_solver.cpp:112] Iteration 33030, lr = 0.1
I0524 04:19:00.998112 11835 solver.cpp:239] Iteration 33040 (1.14257 iter/s, 8.75221s/10 iters), loss = 6.27397
I0524 04:19:00.998158 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27397 (* 1 = 6.27397 loss)
I0524 04:19:01.238654 11835 sgd_solver.cpp:112] Iteration 33040, lr = 0.1
I0524 04:19:09.086076 11835 solver.cpp:239] Iteration 33050 (1.23646 iter/s, 8.08761s/10 iters), loss = 6.21134
I0524 04:19:09.086202 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21134 (* 1 = 6.21134 loss)
I0524 04:19:09.211722 11835 sgd_solver.cpp:112] Iteration 33050, lr = 0.1
I0524 04:19:15.917615 11835 solver.cpp:239] Iteration 33060 (1.46388 iter/s, 6.83116s/10 iters), loss = 6.4242
I0524 04:19:15.917654 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4242 (* 1 = 6.4242 loss)
I0524 04:19:16.763767 11835 sgd_solver.cpp:112] Iteration 33060, lr = 0.1
I0524 04:19:23.079732 11835 solver.cpp:239] Iteration 33070 (1.3963 iter/s, 7.1618s/10 iters), loss = 6.8813
I0524 04:19:23.079783 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8813 (* 1 = 6.8813 loss)
I0524 04:19:23.908633 11835 sgd_solver.cpp:112] Iteration 33070, lr = 0.1
I0524 04:19:30.205940 11835 solver.cpp:239] Iteration 33080 (1.40333 iter/s, 7.12589s/10 iters), loss = 6.66302
I0524 04:19:30.205986 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66302 (* 1 = 6.66302 loss)
I0524 04:19:30.206007 11835 sgd_solver.cpp:112] Iteration 33080, lr = 0.1
I0524 04:19:38.125730 11835 solver.cpp:239] Iteration 33090 (1.26273 iter/s, 7.91934s/10 iters), loss = 6.72446
I0524 04:19:38.125784 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72446 (* 1 = 6.72446 loss)
I0524 04:19:38.125800 11835 sgd_solver.cpp:112] Iteration 33090, lr = 0.1
I0524 04:19:46.861965 11835 solver.cpp:239] Iteration 33100 (1.14479 iter/s, 8.73526s/10 iters), loss = 6.89711
I0524 04:19:46.862229 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89711 (* 1 = 6.89711 loss)
I0524 04:19:47.795773 11835 sgd_solver.cpp:112] Iteration 33100, lr = 0.1
I0524 04:19:54.725193 11835 solver.cpp:239] Iteration 33110 (1.27183 iter/s, 7.86269s/10 iters), loss = 5.73116
I0524 04:19:54.725234 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73116 (* 1 = 5.73116 loss)
I0524 04:19:54.725246 11835 sgd_solver.cpp:112] Iteration 33110, lr = 0.1
I0524 04:20:01.058099 11835 solver.cpp:239] Iteration 33120 (1.57916 iter/s, 6.33248s/10 iters), loss = 6.4253
I0524 04:20:01.058148 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4253 (* 1 = 6.4253 loss)
I0524 04:20:01.070513 11835 sgd_solver.cpp:112] Iteration 33120, lr = 0.1
I0524 04:20:08.325389 11835 solver.cpp:239] Iteration 33130 (1.37609 iter/s, 7.26697s/10 iters), loss = 5.75081
I0524 04:20:08.325433 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75081 (* 1 = 5.75081 loss)
I0524 04:20:08.325651 11835 sgd_solver.cpp:112] Iteration 33130, lr = 0.1
I0524 04:20:15.661860 11835 solver.cpp:239] Iteration 33140 (1.36311 iter/s, 7.33614s/10 iters), loss = 7.21113
I0524 04:20:15.661913 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.21113 (* 1 = 7.21113 loss)
I0524 04:20:15.661939 11835 sgd_solver.cpp:112] Iteration 33140, lr = 0.1
I0524 04:20:22.138651 11835 solver.cpp:239] Iteration 33150 (1.54405 iter/s, 6.47649s/10 iters), loss = 5.97188
I0524 04:20:22.138936 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97188 (* 1 = 5.97188 loss)
I0524 04:20:22.635104 11835 sgd_solver.cpp:112] Iteration 33150, lr = 0.1
I0524 04:20:30.397600 11835 solver.cpp:239] Iteration 33160 (1.21089 iter/s, 8.25841s/10 iters), loss = 5.45521
I0524 04:20:30.397646 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.45521 (* 1 = 5.45521 loss)
I0524 04:20:30.397727 11835 sgd_solver.cpp:112] Iteration 33160, lr = 0.1
I0524 04:20:36.490746 11835 solver.cpp:239] Iteration 33170 (1.64127 iter/s, 6.09285s/10 iters), loss = 6.67414
I0524 04:20:36.490794 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67414 (* 1 = 6.67414 loss)
I0524 04:20:36.491147 11835 sgd_solver.cpp:112] Iteration 33170, lr = 0.1
I0524 04:20:42.679181 11835 solver.cpp:239] Iteration 33180 (1.61599 iter/s, 6.18815s/10 iters), loss = 6.44157
I0524 04:20:42.679229 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44157 (* 1 = 6.44157 loss)
I0524 04:20:42.679457 11835 sgd_solver.cpp:112] Iteration 33180, lr = 0.1
I0524 04:20:49.594101 11835 solver.cpp:239] Iteration 33190 (1.44621 iter/s, 6.91461s/10 iters), loss = 6.9484
I0524 04:20:49.594142 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9484 (* 1 = 6.9484 loss)
I0524 04:20:50.011571 11835 sgd_solver.cpp:112] Iteration 33190, lr = 0.1
I0524 04:20:57.876765 11835 solver.cpp:239] Iteration 33200 (1.20739 iter/s, 8.28231s/10 iters), loss = 6.27989
I0524 04:20:57.877002 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27989 (* 1 = 6.27989 loss)
I0524 04:20:57.889741 11835 sgd_solver.cpp:112] Iteration 33200, lr = 0.1
I0524 04:21:03.783844 11835 solver.cpp:239] Iteration 33210 (1.69301 iter/s, 5.90664s/10 iters), loss = 6.13942
I0524 04:21:03.783892 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13942 (* 1 = 6.13942 loss)
I0524 04:21:03.784104 11835 sgd_solver.cpp:112] Iteration 33210, lr = 0.1
I0524 04:21:11.199980 11835 solver.cpp:239] Iteration 33220 (1.34847 iter/s, 7.41581s/10 iters), loss = 7.00865
I0524 04:21:11.200018 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00865 (* 1 = 7.00865 loss)
I0524 04:21:11.751216 11835 sgd_solver.cpp:112] Iteration 33220, lr = 0.1
I0524 04:21:18.450860 11835 solver.cpp:239] Iteration 33230 (1.3792 iter/s, 7.25055s/10 iters), loss = 6.6761
I0524 04:21:18.450911 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6761 (* 1 = 6.6761 loss)
I0524 04:21:18.966238 11835 sgd_solver.cpp:112] Iteration 33230, lr = 0.1
I0524 04:21:24.988356 11835 solver.cpp:239] Iteration 33240 (1.52971 iter/s, 6.53719s/10 iters), loss = 5.69344
I0524 04:21:24.988404 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.69344 (* 1 = 5.69344 loss)
I0524 04:21:24.988420 11835 sgd_solver.cpp:112] Iteration 33240, lr = 0.1
I0524 04:21:34.460184 11835 solver.cpp:239] Iteration 33250 (1.05606 iter/s, 9.46919s/10 iters), loss = 6.05643
I0524 04:21:34.460328 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05643 (* 1 = 6.05643 loss)
I0524 04:21:35.057214 11835 sgd_solver.cpp:112] Iteration 33250, lr = 0.1
I0524 04:21:41.473861 11835 solver.cpp:239] Iteration 33260 (1.42587 iter/s, 7.01327s/10 iters), loss = 6.49021
I0524 04:21:41.473902 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49021 (* 1 = 6.49021 loss)
I0524 04:21:41.473914 11835 sgd_solver.cpp:112] Iteration 33260, lr = 0.1
I0524 04:21:47.024747 11835 solver.cpp:239] Iteration 33270 (1.80168 iter/s, 5.55038s/10 iters), loss = 6.76586
I0524 04:21:47.024798 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76586 (* 1 = 6.76586 loss)
I0524 04:21:47.024955 11835 sgd_solver.cpp:112] Iteration 33270, lr = 0.1
I0524 04:21:53.928433 11835 solver.cpp:239] Iteration 33280 (1.44857 iter/s, 6.90338s/10 iters), loss = 6.71762
I0524 04:21:53.928483 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71762 (* 1 = 6.71762 loss)
I0524 04:21:53.928743 11835 sgd_solver.cpp:112] Iteration 33280, lr = 0.1
I0524 04:21:59.869832 11835 solver.cpp:239] Iteration 33290 (1.68319 iter/s, 5.94112s/10 iters), loss = 6.99995
I0524 04:21:59.869876 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.99995 (* 1 = 6.99995 loss)
I0524 04:21:59.950151 11835 sgd_solver.cpp:112] Iteration 33290, lr = 0.1
I0524 04:22:07.678158 11835 solver.cpp:239] Iteration 33300 (1.28074 iter/s, 7.80798s/10 iters), loss = 7.40548
I0524 04:22:07.678426 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.40548 (* 1 = 7.40548 loss)
I0524 04:22:07.678483 11835 sgd_solver.cpp:112] Iteration 33300, lr = 0.1
I0524 04:22:13.782541 11835 solver.cpp:239] Iteration 33310 (1.63829 iter/s, 6.10391s/10 iters), loss = 5.62787
I0524 04:22:13.782589 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.62787 (* 1 = 5.62787 loss)
I0524 04:22:13.782608 11835 sgd_solver.cpp:112] Iteration 33310, lr = 0.1
I0524 04:22:21.337965 11835 solver.cpp:239] Iteration 33320 (1.32361 iter/s, 7.55509s/10 iters), loss = 6.69106
I0524 04:22:21.338011 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69106 (* 1 = 6.69106 loss)
I0524 04:22:21.338028 11835 sgd_solver.cpp:112] Iteration 33320, lr = 0.1
I0524 04:22:27.428822 11835 solver.cpp:239] Iteration 33330 (1.64218 iter/s, 6.08948s/10 iters), loss = 7.28373
I0524 04:22:27.428864 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.28373 (* 1 = 7.28373 loss)
I0524 04:22:27.516697 11835 sgd_solver.cpp:112] Iteration 33330, lr = 0.1
I0524 04:22:33.662389 11835 solver.cpp:239] Iteration 33340 (1.60429 iter/s, 6.23327s/10 iters), loss = 6.21188
I0524 04:22:33.662443 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21188 (* 1 = 6.21188 loss)
I0524 04:22:33.662572 11835 sgd_solver.cpp:112] Iteration 33340, lr = 0.1
I0524 04:22:40.241302 11835 solver.cpp:239] Iteration 33350 (1.52008 iter/s, 6.57862s/10 iters), loss = 6.82951
I0524 04:22:40.241381 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.82951 (* 1 = 6.82951 loss)
I0524 04:22:40.241395 11835 sgd_solver.cpp:112] Iteration 33350, lr = 0.1
I0524 04:22:47.776577 11835 solver.cpp:239] Iteration 33360 (1.32755 iter/s, 7.53267s/10 iters), loss = 5.31799
I0524 04:22:47.776628 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.31799 (* 1 = 5.31799 loss)
I0524 04:22:48.249291 11835 sgd_solver.cpp:112] Iteration 33360, lr = 0.1
I0524 04:22:54.392627 11835 solver.cpp:239] Iteration 33370 (1.51155 iter/s, 6.61575s/10 iters), loss = 5.83573
I0524 04:22:54.392674 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83573 (* 1 = 5.83573 loss)
I0524 04:22:54.393259 11835 sgd_solver.cpp:112] Iteration 33370, lr = 0.1
I0524 04:23:01.157671 11835 solver.cpp:239] Iteration 33380 (1.47826 iter/s, 6.76473s/10 iters), loss = 6.8311
I0524 04:23:01.157732 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8311 (* 1 = 6.8311 loss)
I0524 04:23:01.157850 11835 sgd_solver.cpp:112] Iteration 33380, lr = 0.1
I0524 04:23:07.978449 11835 solver.cpp:239] Iteration 33390 (1.46618 iter/s, 6.82046s/10 iters), loss = 5.96892
I0524 04:23:07.978497 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96892 (* 1 = 5.96892 loss)
I0524 04:23:08.441817 11835 sgd_solver.cpp:112] Iteration 33390, lr = 0.1
I0524 04:23:15.757869 11835 solver.cpp:239] Iteration 33400 (1.2855 iter/s, 7.77907s/10 iters), loss = 6.47554
I0524 04:23:15.758167 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.47554 (* 1 = 6.47554 loss)
I0524 04:23:15.758224 11835 sgd_solver.cpp:112] Iteration 33400, lr = 0.1
I0524 04:23:22.244904 11835 solver.cpp:239] Iteration 33410 (1.5419 iter/s, 6.48549s/10 iters), loss = 5.89714
I0524 04:23:22.244947 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89714 (* 1 = 5.89714 loss)
I0524 04:23:22.244962 11835 sgd_solver.cpp:112] Iteration 33410, lr = 0.1
I0524 04:23:31.452368 11835 solver.cpp:239] Iteration 33420 (1.08612 iter/s, 9.20706s/10 iters), loss = 6.11847
I0524 04:23:31.452416 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11847 (* 1 = 6.11847 loss)
I0524 04:23:31.452525 11835 sgd_solver.cpp:112] Iteration 33420, lr = 0.1
I0524 04:23:37.069900 11835 solver.cpp:239] Iteration 33430 (1.78022 iter/s, 5.61727s/10 iters), loss = 5.79228
I0524 04:23:37.069944 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79228 (* 1 = 5.79228 loss)
I0524 04:23:37.069957 11835 sgd_solver.cpp:112] Iteration 33430, lr = 0.1
I0524 04:23:44.698231 11835 solver.cpp:239] Iteration 33440 (1.31096 iter/s, 7.62798s/10 iters), loss = 5.83415
I0524 04:23:44.698279 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83415 (* 1 = 5.83415 loss)
I0524 04:23:44.736534 11835 sgd_solver.cpp:112] Iteration 33440, lr = 0.1
I0524 04:23:50.575713 11835 solver.cpp:239] Iteration 33450 (1.70149 iter/s, 5.87721s/10 iters), loss = 6.05872
I0524 04:23:50.575928 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05872 (* 1 = 6.05872 loss)
I0524 04:23:50.576174 11835 sgd_solver.cpp:112] Iteration 33450, lr = 0.1
I0524 04:23:56.573050 11835 solver.cpp:239] Iteration 33460 (1.66753 iter/s, 5.99691s/10 iters), loss = 6.09037
I0524 04:23:56.573104 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09037 (* 1 = 6.09037 loss)
I0524 04:23:56.573118 11835 sgd_solver.cpp:112] Iteration 33460, lr = 0.1
I0524 04:24:03.335819 11835 solver.cpp:239] Iteration 33470 (1.47875 iter/s, 6.76246s/10 iters), loss = 6.72387
I0524 04:24:03.335858 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72387 (* 1 = 6.72387 loss)
I0524 04:24:03.509232 11835 sgd_solver.cpp:112] Iteration 33470, lr = 0.1
I0524 04:24:11.362125 11835 solver.cpp:239] Iteration 33480 (1.24596 iter/s, 8.02594s/10 iters), loss = 6.57153
I0524 04:24:11.362196 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57153 (* 1 = 6.57153 loss)
I0524 04:24:11.362464 11835 sgd_solver.cpp:112] Iteration 33480, lr = 0.1
I0524 04:24:17.246515 11835 solver.cpp:239] Iteration 33490 (1.6995 iter/s, 5.8841s/10 iters), loss = 6.6426
I0524 04:24:17.246562 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6426 (* 1 = 6.6426 loss)
I0524 04:24:17.246577 11835 sgd_solver.cpp:112] Iteration 33490, lr = 0.1
I0524 04:24:24.636039 11835 solver.cpp:239] Iteration 33500 (1.35334 iter/s, 7.38912s/10 iters), loss = 6.14738
I0524 04:24:24.636119 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14738 (* 1 = 6.14738 loss)
I0524 04:24:24.883342 11835 sgd_solver.cpp:112] Iteration 33500, lr = 0.1
I0524 04:24:30.791863 11835 solver.cpp:239] Iteration 33510 (1.62457 iter/s, 6.15549s/10 iters), loss = 5.39076
I0524 04:24:30.791913 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.39076 (* 1 = 5.39076 loss)
I0524 04:24:30.791929 11835 sgd_solver.cpp:112] Iteration 33510, lr = 0.1
I0524 04:24:36.811684 11835 solver.cpp:239] Iteration 33520 (1.66125 iter/s, 6.01955s/10 iters), loss = 6.98045
I0524 04:24:36.811717 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98045 (* 1 = 6.98045 loss)
I0524 04:24:37.793541 11835 sgd_solver.cpp:112] Iteration 33520, lr = 0.1
I0524 04:24:44.968094 11835 solver.cpp:239] Iteration 33530 (1.22608 iter/s, 8.15606s/10 iters), loss = 5.34974
I0524 04:24:44.968140 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.34974 (* 1 = 5.34974 loss)
I0524 04:24:44.968154 11835 sgd_solver.cpp:112] Iteration 33530, lr = 0.1
I0524 04:24:51.329458 11835 solver.cpp:239] Iteration 33540 (1.57262 iter/s, 6.35883s/10 iters), loss = 6.26011
I0524 04:24:51.329514 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26011 (* 1 = 6.26011 loss)
I0524 04:24:51.329747 11835 sgd_solver.cpp:112] Iteration 33540, lr = 0.1
I0524 04:24:57.876171 11835 solver.cpp:239] Iteration 33550 (1.52755 iter/s, 6.54642s/10 iters), loss = 6.87429
I0524 04:24:57.876413 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87429 (* 1 = 6.87429 loss)
I0524 04:24:57.876546 11835 sgd_solver.cpp:112] Iteration 33550, lr = 0.1
I0524 04:25:05.702571 11835 solver.cpp:239] Iteration 33560 (1.27781 iter/s, 7.82588s/10 iters), loss = 6.26087
I0524 04:25:05.702620 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26087 (* 1 = 6.26087 loss)
I0524 04:25:06.185279 11835 sgd_solver.cpp:112] Iteration 33560, lr = 0.1
I0524 04:25:12.963366 11835 solver.cpp:239] Iteration 33570 (1.37732 iter/s, 7.26047s/10 iters), loss = 7.66542
I0524 04:25:12.963419 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.66542 (* 1 = 7.66542 loss)
I0524 04:25:13.307868 11835 sgd_solver.cpp:112] Iteration 33570, lr = 0.1
I0524 04:25:21.902417 11835 solver.cpp:239] Iteration 33580 (1.11873 iter/s, 8.93867s/10 iters), loss = 6.08172
I0524 04:25:21.902459 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08172 (* 1 = 6.08172 loss)
I0524 04:25:21.902576 11835 sgd_solver.cpp:112] Iteration 33580, lr = 0.1
I0524 04:25:28.053817 11835 solver.cpp:239] Iteration 33590 (1.62572 iter/s, 6.15112s/10 iters), loss = 5.79756
I0524 04:25:28.054067 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79756 (* 1 = 5.79756 loss)
I0524 04:25:28.054118 11835 sgd_solver.cpp:112] Iteration 33590, lr = 0.1
I0524 04:25:35.532464 11835 solver.cpp:239] Iteration 33600 (1.33723 iter/s, 7.47814s/10 iters), loss = 5.1011
I0524 04:25:35.532510 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.1011 (* 1 = 5.1011 loss)
I0524 04:25:35.532742 11835 sgd_solver.cpp:112] Iteration 33600, lr = 0.1
I0524 04:25:41.359073 11835 solver.cpp:239] Iteration 33610 (1.71635 iter/s, 5.82633s/10 iters), loss = 7.59564
I0524 04:25:41.359122 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.59564 (* 1 = 7.59564 loss)
I0524 04:25:41.665561 11835 sgd_solver.cpp:112] Iteration 33610, lr = 0.1
I0524 04:25:47.683990 11835 solver.cpp:239] Iteration 33620 (1.58112 iter/s, 6.32462s/10 iters), loss = 6.61668
I0524 04:25:47.684037 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61668 (* 1 = 6.61668 loss)
I0524 04:25:47.684051 11835 sgd_solver.cpp:112] Iteration 33620, lr = 0.1
I0524 04:25:55.014567 11835 solver.cpp:239] Iteration 33630 (1.36421 iter/s, 7.33025s/10 iters), loss = 6.51217
I0524 04:25:55.014608 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51217 (* 1 = 6.51217 loss)
I0524 04:25:55.014991 11835 sgd_solver.cpp:112] Iteration 33630, lr = 0.1
I0524 04:26:01.420776 11835 solver.cpp:239] Iteration 33640 (1.56106 iter/s, 6.40591s/10 iters), loss = 7.64689
I0524 04:26:01.421089 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.64689 (* 1 = 7.64689 loss)
I0524 04:26:01.421149 11835 sgd_solver.cpp:112] Iteration 33640, lr = 0.1
I0524 04:26:08.274317 11835 solver.cpp:239] Iteration 33650 (1.45943 iter/s, 6.85197s/10 iters), loss = 6.13286
I0524 04:26:08.274371 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13286 (* 1 = 6.13286 loss)
I0524 04:26:08.274618 11835 sgd_solver.cpp:112] Iteration 33650, lr = 0.1
I0524 04:26:14.388885 11835 solver.cpp:239] Iteration 33660 (1.63551 iter/s, 6.11428s/10 iters), loss = 6.06289
I0524 04:26:14.388921 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06289 (* 1 = 6.06289 loss)
I0524 04:26:14.388941 11835 sgd_solver.cpp:112] Iteration 33660, lr = 0.1
I0524 04:26:22.765200 11835 solver.cpp:239] Iteration 33670 (1.19389 iter/s, 8.37595s/10 iters), loss = 5.99602
I0524 04:26:22.765259 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99602 (* 1 = 5.99602 loss)
I0524 04:26:22.765276 11835 sgd_solver.cpp:112] Iteration 33670, lr = 0.1
I0524 04:26:28.562450 11835 solver.cpp:239] Iteration 33680 (1.72513 iter/s, 5.79668s/10 iters), loss = 5.93009
I0524 04:26:28.562513 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93009 (* 1 = 5.93009 loss)
I0524 04:26:28.564450 11835 sgd_solver.cpp:112] Iteration 33680, lr = 0.1
I0524 04:26:34.492101 11835 solver.cpp:239] Iteration 33690 (1.68652 iter/s, 5.92937s/10 iters), loss = 7.46506
I0524 04:26:34.492358 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.46506 (* 1 = 7.46506 loss)
I0524 04:26:34.492414 11835 sgd_solver.cpp:112] Iteration 33690, lr = 0.1
I0524 04:26:40.274832 11835 solver.cpp:239] Iteration 33700 (1.72942 iter/s, 5.78229s/10 iters), loss = 7.47203
I0524 04:26:40.274875 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.47203 (* 1 = 7.47203 loss)
I0524 04:26:40.274893 11835 sgd_solver.cpp:112] Iteration 33700, lr = 0.1
I0524 04:26:47.244073 11835 solver.cpp:239] Iteration 33710 (1.43496 iter/s, 6.96886s/10 iters), loss = 7.41837
I0524 04:26:47.244117 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.41837 (* 1 = 7.41837 loss)
I0524 04:26:47.246083 11835 sgd_solver.cpp:112] Iteration 33710, lr = 0.1
I0524 04:26:54.938567 11835 solver.cpp:239] Iteration 33720 (1.29969 iter/s, 7.69414s/10 iters), loss = 6.59832
I0524 04:26:54.938621 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59832 (* 1 = 6.59832 loss)
I0524 04:26:54.939201 11835 sgd_solver.cpp:112] Iteration 33720, lr = 0.1
I0524 04:27:02.501041 11835 solver.cpp:239] Iteration 33730 (1.32238 iter/s, 7.56213s/10 iters), loss = 6.738
I0524 04:27:02.501098 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.738 (* 1 = 6.738 loss)
I0524 04:27:02.501397 11835 sgd_solver.cpp:112] Iteration 33730, lr = 0.1
I0524 04:27:08.463598 11835 solver.cpp:239] Iteration 33740 (1.67721 iter/s, 5.96227s/10 iters), loss = 6.8576
I0524 04:27:08.463706 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8576 (* 1 = 6.8576 loss)
I0524 04:27:08.465585 11835 sgd_solver.cpp:112] Iteration 33740, lr = 0.1
I0524 04:27:15.152130 11835 solver.cpp:239] Iteration 33750 (1.49518 iter/s, 6.68815s/10 iters), loss = 7.5518
I0524 04:27:15.152186 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.5518 (* 1 = 7.5518 loss)
I0524 04:27:15.152469 11835 sgd_solver.cpp:112] Iteration 33750, lr = 0.1
I0524 04:27:21.212013 11835 solver.cpp:239] Iteration 33760 (1.65027 iter/s, 6.0596s/10 iters), loss = 6.4472
I0524 04:27:21.212047 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4472 (* 1 = 6.4472 loss)
I0524 04:27:21.212059 11835 sgd_solver.cpp:112] Iteration 33760, lr = 0.1
I0524 04:27:29.140311 11835 solver.cpp:239] Iteration 33770 (1.26136 iter/s, 7.92795s/10 iters), loss = 6.7243
I0524 04:27:29.140362 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7243 (* 1 = 6.7243 loss)
I0524 04:27:29.140539 11835 sgd_solver.cpp:112] Iteration 33770, lr = 0.1
I0524 04:27:38.218466 11835 solver.cpp:239] Iteration 33780 (1.10159 iter/s, 9.07776s/10 iters), loss = 6.53124
I0524 04:27:38.218521 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53124 (* 1 = 6.53124 loss)
I0524 04:27:39.098875 11835 sgd_solver.cpp:112] Iteration 33780, lr = 0.1
I0524 04:27:46.966112 11835 solver.cpp:239] Iteration 33790 (1.14322 iter/s, 8.74725s/10 iters), loss = 6.84722
I0524 04:27:46.966169 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84722 (* 1 = 6.84722 loss)
I0524 04:27:46.966424 11835 sgd_solver.cpp:112] Iteration 33790, lr = 0.1
I0524 04:27:53.494494 11835 solver.cpp:239] Iteration 33800 (1.53185 iter/s, 6.52806s/10 iters), loss = 6.43494
I0524 04:27:53.494542 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43494 (* 1 = 6.43494 loss)
I0524 04:27:53.494560 11835 sgd_solver.cpp:112] Iteration 33800, lr = 0.1
I0524 04:28:00.112046 11835 solver.cpp:239] Iteration 33810 (1.51127 iter/s, 6.61696s/10 iters), loss = 6.06745
I0524 04:28:00.112092 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06745 (* 1 = 6.06745 loss)
I0524 04:28:00.206658 11835 sgd_solver.cpp:112] Iteration 33810, lr = 0.1
I0524 04:28:06.305663 11835 solver.cpp:239] Iteration 33820 (1.61464 iter/s, 6.19333s/10 iters), loss = 6.35794
I0524 04:28:06.305714 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35794 (* 1 = 6.35794 loss)
I0524 04:28:06.305960 11835 sgd_solver.cpp:112] Iteration 33820, lr = 0.1
I0524 04:28:12.427637 11835 solver.cpp:239] Iteration 33830 (1.63355 iter/s, 6.12165s/10 iters), loss = 5.36096
I0524 04:28:12.427750 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.36096 (* 1 = 5.36096 loss)
I0524 04:28:12.428179 11835 sgd_solver.cpp:112] Iteration 33830, lr = 0.1
I0524 04:28:21.585270 11835 solver.cpp:239] Iteration 33840 (1.09204 iter/s, 9.15718s/10 iters), loss = 6.39931
I0524 04:28:21.585321 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39931 (* 1 = 6.39931 loss)
I0524 04:28:21.585403 11835 sgd_solver.cpp:112] Iteration 33840, lr = 0.1
I0524 04:28:28.866055 11835 solver.cpp:239] Iteration 33850 (1.37354 iter/s, 7.28046s/10 iters), loss = 6.35647
I0524 04:28:28.866096 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35647 (* 1 = 6.35647 loss)
I0524 04:28:28.866226 11835 sgd_solver.cpp:112] Iteration 33850, lr = 0.1
I0524 04:28:35.519255 11835 solver.cpp:239] Iteration 33860 (1.5031 iter/s, 6.6529s/10 iters), loss = 6.19006
I0524 04:28:35.519307 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19006 (* 1 = 6.19006 loss)
I0524 04:28:35.524863 11835 sgd_solver.cpp:112] Iteration 33860, lr = 0.1
I0524 04:28:41.507457 11835 solver.cpp:239] Iteration 33870 (1.67003 iter/s, 5.98791s/10 iters), loss = 6.09296
I0524 04:28:41.507515 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09296 (* 1 = 6.09296 loss)
I0524 04:28:41.537073 11835 sgd_solver.cpp:112] Iteration 33870, lr = 0.1
I0524 04:28:49.752210 11835 solver.cpp:239] Iteration 33880 (1.21295 iter/s, 8.24439s/10 iters), loss = 6.2664
I0524 04:28:49.752478 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2664 (* 1 = 6.2664 loss)
I0524 04:28:49.752528 11835 sgd_solver.cpp:112] Iteration 33880, lr = 0.1
I0524 04:28:55.493947 11835 solver.cpp:239] Iteration 33890 (1.74177 iter/s, 5.7413s/10 iters), loss = 6.45942
I0524 04:28:55.493993 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45942 (* 1 = 6.45942 loss)
I0524 04:28:55.494009 11835 sgd_solver.cpp:112] Iteration 33890, lr = 0.1
I0524 04:29:02.510650 11835 solver.cpp:239] Iteration 33900 (1.42525 iter/s, 7.0163s/10 iters), loss = 7.21247
I0524 04:29:02.510725 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.21247 (* 1 = 7.21247 loss)
I0524 04:29:02.510752 11835 sgd_solver.cpp:112] Iteration 33900, lr = 0.1
I0524 04:29:09.468504 11835 solver.cpp:239] Iteration 33910 (1.43742 iter/s, 6.9569s/10 iters), loss = 6.96796
I0524 04:29:09.468545 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96796 (* 1 = 6.96796 loss)
I0524 04:29:10.021080 11835 sgd_solver.cpp:112] Iteration 33910, lr = 0.1
I0524 04:29:16.533439 11835 solver.cpp:239] Iteration 33920 (1.41551 iter/s, 7.06461s/10 iters), loss = 7.18549
I0524 04:29:16.533486 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.18549 (* 1 = 7.18549 loss)
I0524 04:29:16.533857 11835 sgd_solver.cpp:112] Iteration 33920, lr = 0.1
I0524 04:29:22.937790 11835 solver.cpp:239] Iteration 33930 (1.56151 iter/s, 6.40406s/10 iters), loss = 6.12741
I0524 04:29:22.938072 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12741 (* 1 = 6.12741 loss)
I0524 04:29:22.938120 11835 sgd_solver.cpp:112] Iteration 33930, lr = 0.1
I0524 04:29:28.927845 11835 solver.cpp:239] Iteration 33940 (1.66963 iter/s, 5.98937s/10 iters), loss = 7.03736
I0524 04:29:28.927891 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03736 (* 1 = 7.03736 loss)
I0524 04:29:28.928261 11835 sgd_solver.cpp:112] Iteration 33940, lr = 0.1
I0524 04:29:34.557137 11835 solver.cpp:239] Iteration 33950 (1.7765 iter/s, 5.62904s/10 iters), loss = 7.10729
I0524 04:29:34.557174 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10729 (* 1 = 7.10729 loss)
I0524 04:29:34.557188 11835 sgd_solver.cpp:112] Iteration 33950, lr = 0.1
I0524 04:29:41.454748 11835 solver.cpp:239] Iteration 33960 (1.44985 iter/s, 6.89727s/10 iters), loss = 5.72657
I0524 04:29:41.454793 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72657 (* 1 = 5.72657 loss)
I0524 04:29:41.696247 11835 sgd_solver.cpp:112] Iteration 33960, lr = 0.1
I0524 04:29:48.062964 11835 solver.cpp:239] Iteration 33970 (1.51333 iter/s, 6.60792s/10 iters), loss = 7.22132
I0524 04:29:48.063004 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.22132 (* 1 = 7.22132 loss)
I0524 04:29:48.065023 11835 sgd_solver.cpp:112] Iteration 33970, lr = 0.1
I0524 04:29:54.429898 11835 solver.cpp:239] Iteration 33980 (1.57069 iter/s, 6.36664s/10 iters), loss = 6.58248
I0524 04:29:54.430146 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58248 (* 1 = 6.58248 loss)
I0524 04:29:54.430200 11835 sgd_solver.cpp:112] Iteration 33980, lr = 0.1
I0524 04:30:01.638535 11835 solver.cpp:239] Iteration 33990 (1.38737 iter/s, 7.20787s/10 iters), loss = 5.8097
I0524 04:30:01.638582 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8097 (* 1 = 5.8097 loss)
I0524 04:30:01.638725 11835 sgd_solver.cpp:112] Iteration 33990, lr = 0.1
I0524 04:30:11.177803 11835 solver.cpp:239] Iteration 34000 (1.04834 iter/s, 9.53886s/10 iters), loss = 7.30486
I0524 04:30:11.177852 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.30486 (* 1 = 7.30486 loss)
I0524 04:30:11.582363 11835 sgd_solver.cpp:112] Iteration 34000, lr = 0.1
I0524 04:30:18.346143 11835 solver.cpp:239] Iteration 34010 (1.39508 iter/s, 7.16802s/10 iters), loss = 6.04558
I0524 04:30:18.346187 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04558 (* 1 = 6.04558 loss)
I0524 04:30:18.346436 11835 sgd_solver.cpp:112] Iteration 34010, lr = 0.1
I0524 04:30:25.132689 11835 solver.cpp:239] Iteration 34020 (1.47357 iter/s, 6.78622s/10 iters), loss = 7.2909
I0524 04:30:25.132797 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2909 (* 1 = 7.2909 loss)
I0524 04:30:25.132879 11835 sgd_solver.cpp:112] Iteration 34020, lr = 0.1
I0524 04:30:33.362972 11835 solver.cpp:239] Iteration 34030 (1.21509 iter/s, 8.22986s/10 iters), loss = 6.92294
I0524 04:30:33.363024 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92294 (* 1 = 6.92294 loss)
I0524 04:30:33.363152 11835 sgd_solver.cpp:112] Iteration 34030, lr = 0.1
I0524 04:30:40.491691 11835 solver.cpp:239] Iteration 34040 (1.40284 iter/s, 7.12839s/10 iters), loss = 6.5163
I0524 04:30:40.491740 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5163 (* 1 = 6.5163 loss)
I0524 04:30:40.573995 11835 sgd_solver.cpp:112] Iteration 34040, lr = 0.1
I0524 04:30:46.332250 11835 solver.cpp:239] Iteration 34050 (1.71224 iter/s, 5.84029s/10 iters), loss = 6.49674
I0524 04:30:46.332288 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49674 (* 1 = 6.49674 loss)
I0524 04:30:46.332301 11835 sgd_solver.cpp:112] Iteration 34050, lr = 0.1
I0524 04:30:52.851418 11835 solver.cpp:239] Iteration 34060 (1.53401 iter/s, 6.51887s/10 iters), loss = 7.0173
I0524 04:30:52.851465 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.0173 (* 1 = 7.0173 loss)
I0524 04:30:53.486179 11835 sgd_solver.cpp:112] Iteration 34060, lr = 0.1
I0524 04:30:59.485105 11835 solver.cpp:239] Iteration 34070 (1.50752 iter/s, 6.63339s/10 iters), loss = 6.58897
I0524 04:30:59.485273 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58897 (* 1 = 6.58897 loss)
I0524 04:30:59.485301 11835 sgd_solver.cpp:112] Iteration 34070, lr = 0.1
I0524 04:31:05.514338 11835 solver.cpp:239] Iteration 34080 (1.65884 iter/s, 6.0283s/10 iters), loss = 6.09207
I0524 04:31:05.514384 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09207 (* 1 = 6.09207 loss)
I0524 04:31:05.930482 11835 sgd_solver.cpp:112] Iteration 34080, lr = 0.1
I0524 04:31:11.582186 11835 solver.cpp:239] Iteration 34090 (1.64811 iter/s, 6.06757s/10 iters), loss = 6.09091
I0524 04:31:11.582234 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09091 (* 1 = 6.09091 loss)
I0524 04:31:11.582247 11835 sgd_solver.cpp:112] Iteration 34090, lr = 0.1
I0524 04:31:20.140010 11835 solver.cpp:239] Iteration 34100 (1.16858 iter/s, 8.55738s/10 iters), loss = 7.2053
I0524 04:31:20.140058 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2053 (* 1 = 7.2053 loss)
I0524 04:31:20.140192 11835 sgd_solver.cpp:112] Iteration 34100, lr = 0.1
I0524 04:31:26.598881 11835 solver.cpp:239] Iteration 34110 (1.54833 iter/s, 6.45857s/10 iters), loss = 4.95373
I0524 04:31:26.598942 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.95373 (* 1 = 4.95373 loss)
I0524 04:31:26.599264 11835 sgd_solver.cpp:112] Iteration 34110, lr = 0.1
I0524 04:31:32.398509 11835 solver.cpp:239] Iteration 34120 (1.72433 iter/s, 5.79934s/10 iters), loss = 5.93727
I0524 04:31:32.398623 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93727 (* 1 = 5.93727 loss)
I0524 04:31:32.398640 11835 sgd_solver.cpp:112] Iteration 34120, lr = 0.1
I0524 04:31:38.689036 11835 solver.cpp:239] Iteration 34130 (1.58978 iter/s, 6.29018s/10 iters), loss = 6.71054
I0524 04:31:38.689079 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71054 (* 1 = 6.71054 loss)
I0524 04:31:38.888592 11835 sgd_solver.cpp:112] Iteration 34130, lr = 0.1
I0524 04:31:45.119570 11835 solver.cpp:239] Iteration 34140 (1.55515 iter/s, 6.43024s/10 iters), loss = 6.30876
I0524 04:31:45.119624 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30876 (* 1 = 6.30876 loss)
I0524 04:31:45.159883 11835 sgd_solver.cpp:112] Iteration 34140, lr = 0.1
I0524 04:31:51.930620 11835 solver.cpp:239] Iteration 34150 (1.46827 iter/s, 6.81074s/10 iters), loss = 5.51631
I0524 04:31:51.930660 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.51631 (* 1 = 5.51631 loss)
I0524 04:31:51.930672 11835 sgd_solver.cpp:112] Iteration 34150, lr = 0.1
I0524 04:31:58.754940 11835 solver.cpp:239] Iteration 34160 (1.46541 iter/s, 6.82401s/10 iters), loss = 5.92874
I0524 04:31:58.754987 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92874 (* 1 = 5.92874 loss)
I0524 04:31:58.886917 11835 sgd_solver.cpp:112] Iteration 34160, lr = 0.1
I0524 04:32:06.972918 11835 solver.cpp:239] Iteration 34170 (1.2169 iter/s, 8.21762s/10 iters), loss = 7.58559
I0524 04:32:06.973165 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.58559 (* 1 = 7.58559 loss)
I0524 04:32:06.973218 11835 sgd_solver.cpp:112] Iteration 34170, lr = 0.1
I0524 04:32:18.460333 11835 solver.cpp:239] Iteration 34180 (0.870649 iter/s, 11.4857s/10 iters), loss = 6.20968
I0524 04:32:18.460388 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20968 (* 1 = 6.20968 loss)
I0524 04:32:18.460512 11835 sgd_solver.cpp:112] Iteration 34180, lr = 0.1
I0524 04:32:26.873392 11835 solver.cpp:239] Iteration 34190 (1.18868 iter/s, 8.41269s/10 iters), loss = 5.71346
I0524 04:32:26.873436 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71346 (* 1 = 5.71346 loss)
I0524 04:32:26.873826 11835 sgd_solver.cpp:112] Iteration 34190, lr = 0.1
I0524 04:32:33.632326 11835 solver.cpp:239] Iteration 34200 (1.47959 iter/s, 6.75861s/10 iters), loss = 5.82774
I0524 04:32:33.632380 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82774 (* 1 = 5.82774 loss)
I0524 04:32:33.632613 11835 sgd_solver.cpp:112] Iteration 34200, lr = 0.1
I0524 04:32:39.957320 11835 solver.cpp:239] Iteration 34210 (1.58111 iter/s, 6.32467s/10 iters), loss = 6.2498
I0524 04:32:39.957629 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2498 (* 1 = 6.2498 loss)
I0524 04:32:39.957685 11835 sgd_solver.cpp:112] Iteration 34210, lr = 0.1
I0524 04:32:49.272169 11835 solver.cpp:239] Iteration 34220 (1.07362 iter/s, 9.31425s/10 iters), loss = 5.28806
I0524 04:32:49.272212 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.28806 (* 1 = 5.28806 loss)
I0524 04:32:49.664469 11835 sgd_solver.cpp:112] Iteration 34220, lr = 0.1
I0524 04:32:58.140269 11835 solver.cpp:239] Iteration 34230 (1.12769 iter/s, 8.86771s/10 iters), loss = 5.35597
I0524 04:32:58.140321 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.35597 (* 1 = 5.35597 loss)
I0524 04:32:58.870826 11835 sgd_solver.cpp:112] Iteration 34230, lr = 0.1
I0524 04:33:06.180169 11835 solver.cpp:239] Iteration 34240 (1.24385 iter/s, 8.03955s/10 iters), loss = 6.7337
I0524 04:33:06.180208 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7337 (* 1 = 6.7337 loss)
I0524 04:33:06.180348 11835 sgd_solver.cpp:112] Iteration 34240, lr = 0.1
I0524 04:33:16.236711 11835 solver.cpp:239] Iteration 34250 (0.99442 iter/s, 10.0561s/10 iters), loss = 7.02615
I0524 04:33:16.236846 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.02615 (* 1 = 7.02615 loss)
I0524 04:33:16.813254 11835 sgd_solver.cpp:112] Iteration 34250, lr = 0.1
I0524 04:33:25.578814 11835 solver.cpp:239] Iteration 34260 (1.07048 iter/s, 9.34161s/10 iters), loss = 5.78774
I0524 04:33:25.578869 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78774 (* 1 = 5.78774 loss)
I0524 04:33:26.650244 11835 sgd_solver.cpp:112] Iteration 34260, lr = 0.1
I0524 04:33:32.393100 11835 solver.cpp:239] Iteration 34270 (1.46757 iter/s, 6.81398s/10 iters), loss = 6.12005
I0524 04:33:32.393136 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12005 (* 1 = 6.12005 loss)
I0524 04:33:32.772547 11835 sgd_solver.cpp:112] Iteration 34270, lr = 0.1
I0524 04:33:39.761333 11835 solver.cpp:239] Iteration 34280 (1.35724 iter/s, 7.3679s/10 iters), loss = 6.22121
I0524 04:33:39.761387 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22121 (* 1 = 6.22121 loss)
I0524 04:33:39.761657 11835 sgd_solver.cpp:112] Iteration 34280, lr = 0.1
I0524 04:33:47.337375 11835 solver.cpp:239] Iteration 34290 (1.32001 iter/s, 7.5757s/10 iters), loss = 5.67432
I0524 04:33:47.337574 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67432 (* 1 = 5.67432 loss)
I0524 04:33:47.337610 11835 sgd_solver.cpp:112] Iteration 34290, lr = 0.1
I0524 04:33:53.001289 11835 solver.cpp:239] Iteration 34300 (1.76568 iter/s, 5.66353s/10 iters), loss = 6.7229
I0524 04:33:53.001332 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7229 (* 1 = 6.7229 loss)
I0524 04:33:53.001365 11835 sgd_solver.cpp:112] Iteration 34300, lr = 0.1
I0524 04:33:59.125313 11835 solver.cpp:239] Iteration 34310 (1.63299 iter/s, 6.12373s/10 iters), loss = 5.82564
I0524 04:33:59.125360 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82564 (* 1 = 5.82564 loss)
I0524 04:33:59.329085 11835 sgd_solver.cpp:112] Iteration 34310, lr = 0.1
I0524 04:34:05.101886 11835 solver.cpp:239] Iteration 34320 (1.67328 iter/s, 5.9763s/10 iters), loss = 5.92418
I0524 04:34:05.101922 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92418 (* 1 = 5.92418 loss)
I0524 04:34:05.101933 11835 sgd_solver.cpp:112] Iteration 34320, lr = 0.1
I0524 04:34:11.393337 11835 solver.cpp:239] Iteration 34330 (1.59082 iter/s, 6.28608s/10 iters), loss = 6.56384
I0524 04:34:11.393415 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56384 (* 1 = 6.56384 loss)
I0524 04:34:11.393440 11835 sgd_solver.cpp:112] Iteration 34330, lr = 0.1
I0524 04:34:18.266798 11835 solver.cpp:239] Iteration 34340 (1.45496 iter/s, 6.87306s/10 iters), loss = 6.20164
I0524 04:34:18.267103 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20164 (* 1 = 6.20164 loss)
I0524 04:34:18.267155 11835 sgd_solver.cpp:112] Iteration 34340, lr = 0.1
I0524 04:34:24.771960 11835 solver.cpp:239] Iteration 34350 (1.5374 iter/s, 6.50447s/10 iters), loss = 6.37981
I0524 04:34:24.772013 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37981 (* 1 = 6.37981 loss)
I0524 04:34:24.781575 11835 sgd_solver.cpp:112] Iteration 34350, lr = 0.1
I0524 04:34:32.799381 11835 solver.cpp:239] Iteration 34360 (1.24579 iter/s, 8.02705s/10 iters), loss = 6.0921
I0524 04:34:32.799439 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0921 (* 1 = 6.0921 loss)
I0524 04:34:32.799645 11835 sgd_solver.cpp:112] Iteration 34360, lr = 0.1
I0524 04:34:39.632361 11835 solver.cpp:239] Iteration 34370 (1.46356 iter/s, 6.83267s/10 iters), loss = 6.41344
I0524 04:34:39.632407 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41344 (* 1 = 6.41344 loss)
I0524 04:34:39.632611 11835 sgd_solver.cpp:112] Iteration 34370, lr = 0.1
I0524 04:34:46.628484 11835 solver.cpp:239] Iteration 34380 (1.42943 iter/s, 6.9958s/10 iters), loss = 5.93369
I0524 04:34:46.628540 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93369 (* 1 = 5.93369 loss)
I0524 04:34:46.668865 11835 sgd_solver.cpp:112] Iteration 34380, lr = 0.1
I0524 04:34:53.653429 11835 solver.cpp:239] Iteration 34390 (1.42356 iter/s, 7.02462s/10 iters), loss = 6.02054
I0524 04:34:53.653517 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02054 (* 1 = 6.02054 loss)
I0524 04:34:53.653533 11835 sgd_solver.cpp:112] Iteration 34390, lr = 0.1
I0524 04:35:00.337262 11835 solver.cpp:239] Iteration 34400 (1.49623 iter/s, 6.68346s/10 iters), loss = 6.13279
I0524 04:35:00.337297 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13279 (* 1 = 6.13279 loss)
I0524 04:35:00.337309 11835 sgd_solver.cpp:112] Iteration 34400, lr = 0.1
I0524 04:35:07.899943 11835 solver.cpp:239] Iteration 34410 (1.32238 iter/s, 7.5621s/10 iters), loss = 5.56054
I0524 04:35:07.899996 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56054 (* 1 = 5.56054 loss)
I0524 04:35:07.940357 11835 sgd_solver.cpp:112] Iteration 34410, lr = 0.1
I0524 04:35:14.188165 11835 solver.cpp:239] Iteration 34420 (1.59035 iter/s, 6.28792s/10 iters), loss = 6.57219
I0524 04:35:14.188220 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57219 (* 1 = 6.57219 loss)
I0524 04:35:14.188465 11835 sgd_solver.cpp:112] Iteration 34420, lr = 0.1
I0524 04:35:20.137370 11835 solver.cpp:239] Iteration 34430 (1.68098 iter/s, 5.94892s/10 iters), loss = 5.89837
I0524 04:35:20.137426 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89837 (* 1 = 5.89837 loss)
I0524 04:35:20.283972 11835 sgd_solver.cpp:112] Iteration 34430, lr = 0.1
I0524 04:35:26.028465 11835 solver.cpp:239] Iteration 34440 (1.69756 iter/s, 5.89082s/10 iters), loss = 4.96063
I0524 04:35:26.028549 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.96063 (* 1 = 4.96063 loss)
I0524 04:35:26.028563 11835 sgd_solver.cpp:112] Iteration 34440, lr = 0.1
I0524 04:35:34.269387 11835 solver.cpp:239] Iteration 34450 (1.21357 iter/s, 8.24018s/10 iters), loss = 6.22536
I0524 04:35:34.269438 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22536 (* 1 = 6.22536 loss)
I0524 04:35:34.269510 11835 sgd_solver.cpp:112] Iteration 34450, lr = 0.1
I0524 04:35:41.559693 11835 solver.cpp:239] Iteration 34460 (1.37175 iter/s, 7.28997s/10 iters), loss = 6.35172
I0524 04:35:41.559751 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35172 (* 1 = 6.35172 loss)
I0524 04:35:41.559819 11835 sgd_solver.cpp:112] Iteration 34460, lr = 0.1
I0524 04:35:47.584774 11835 solver.cpp:239] Iteration 34470 (1.65981 iter/s, 6.0248s/10 iters), loss = 6.41219
I0524 04:35:47.584820 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41219 (* 1 = 6.41219 loss)
I0524 04:35:47.585079 11835 sgd_solver.cpp:112] Iteration 34470, lr = 0.1
I0524 04:35:54.457024 11835 solver.cpp:239] Iteration 34480 (1.45519 iter/s, 6.87193s/10 iters), loss = 5.68544
I0524 04:35:54.457069 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68544 (* 1 = 5.68544 loss)
I0524 04:35:55.371302 11835 sgd_solver.cpp:112] Iteration 34480, lr = 0.1
I0524 04:36:02.295179 11835 solver.cpp:239] Iteration 34490 (1.27587 iter/s, 7.83781s/10 iters), loss = 4.99062
I0524 04:36:02.295308 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.99062 (* 1 = 4.99062 loss)
I0524 04:36:02.295354 11835 sgd_solver.cpp:112] Iteration 34490, lr = 0.1
I0524 04:36:09.403568 11835 solver.cpp:239] Iteration 34500 (1.40687 iter/s, 7.10799s/10 iters), loss = 6.0965
I0524 04:36:09.403617 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0965 (* 1 = 6.0965 loss)
I0524 04:36:09.403717 11835 sgd_solver.cpp:112] Iteration 34500, lr = 0.1
I0524 04:36:15.163990 11835 solver.cpp:239] Iteration 34510 (1.73606 iter/s, 5.76016s/10 iters), loss = 6.69643
I0524 04:36:15.164028 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69643 (* 1 = 6.69643 loss)
I0524 04:36:15.164041 11835 sgd_solver.cpp:112] Iteration 34510, lr = 0.1
I0524 04:36:22.027822 11835 solver.cpp:239] Iteration 34520 (1.45698 iter/s, 6.86351s/10 iters), loss = 7.00699
I0524 04:36:22.027878 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00699 (* 1 = 7.00699 loss)
I0524 04:36:22.028049 11835 sgd_solver.cpp:112] Iteration 34520, lr = 0.1
I0524 04:36:28.156286 11835 solver.cpp:239] Iteration 34530 (1.63181 iter/s, 6.12817s/10 iters), loss = 6.06656
I0524 04:36:28.156337 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06656 (* 1 = 6.06656 loss)
I0524 04:36:28.156663 11835 sgd_solver.cpp:112] Iteration 34530, lr = 0.1
I0524 04:36:35.017223 11835 solver.cpp:239] Iteration 34540 (1.45759 iter/s, 6.86063s/10 iters), loss = 7.11119
I0524 04:36:35.017483 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11119 (* 1 = 7.11119 loss)
I0524 04:36:35.017525 11835 sgd_solver.cpp:112] Iteration 34540, lr = 0.1
I0524 04:36:40.893013 11835 solver.cpp:239] Iteration 34550 (1.70203 iter/s, 5.87535s/10 iters), loss = 6.61896
I0524 04:36:40.893059 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61896 (* 1 = 6.61896 loss)
I0524 04:36:40.893074 11835 sgd_solver.cpp:112] Iteration 34550, lr = 0.1
I0524 04:36:50.591831 11835 solver.cpp:239] Iteration 34560 (1.03133 iter/s, 9.69618s/10 iters), loss = 5.88696
I0524 04:36:50.591884 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88696 (* 1 = 5.88696 loss)
I0524 04:36:50.592036 11835 sgd_solver.cpp:112] Iteration 34560, lr = 0.1
I0524 04:36:56.199357 11835 solver.cpp:239] Iteration 34570 (1.7834 iter/s, 5.60726s/10 iters), loss = 6.5765
I0524 04:36:56.199396 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5765 (* 1 = 6.5765 loss)
I0524 04:36:56.470116 11835 sgd_solver.cpp:112] Iteration 34570, lr = 0.1
I0524 04:37:01.948544 11835 solver.cpp:239] Iteration 34580 (1.73946 iter/s, 5.74892s/10 iters), loss = 5.82667
I0524 04:37:01.948593 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82667 (* 1 = 5.82667 loss)
I0524 04:37:01.948704 11835 sgd_solver.cpp:112] Iteration 34580, lr = 0.1
I0524 04:37:09.461560 11835 solver.cpp:239] Iteration 34590 (1.33108 iter/s, 7.51268s/10 iters), loss = 5.47494
I0524 04:37:09.461828 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.47494 (* 1 = 5.47494 loss)
I0524 04:37:09.461894 11835 sgd_solver.cpp:112] Iteration 34590, lr = 0.1
I0524 04:37:15.391813 11835 solver.cpp:239] Iteration 34600 (1.68641 iter/s, 5.92977s/10 iters), loss = 5.75997
I0524 04:37:15.391851 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75997 (* 1 = 5.75997 loss)
I0524 04:37:15.391863 11835 sgd_solver.cpp:112] Iteration 34600, lr = 0.1
I0524 04:37:22.095018 11835 solver.cpp:239] Iteration 34610 (1.49239 iter/s, 6.70068s/10 iters), loss = 5.89892
I0524 04:37:22.095077 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89892 (* 1 = 5.89892 loss)
I0524 04:37:22.095302 11835 sgd_solver.cpp:112] Iteration 34610, lr = 0.1
I0524 04:37:29.490998 11835 solver.cpp:239] Iteration 34620 (1.35215 iter/s, 7.39565s/10 iters), loss = 6.67633
I0524 04:37:29.491045 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67633 (* 1 = 6.67633 loss)
I0524 04:37:29.491286 11835 sgd_solver.cpp:112] Iteration 34620, lr = 0.1
I0524 04:37:35.188920 11835 solver.cpp:239] Iteration 34630 (1.75511 iter/s, 5.69765s/10 iters), loss = 6.33488
I0524 04:37:35.188966 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33488 (* 1 = 6.33488 loss)
I0524 04:37:36.307927 11835 sgd_solver.cpp:112] Iteration 34630, lr = 0.1
I0524 04:37:41.914301 11835 solver.cpp:239] Iteration 34640 (1.48697 iter/s, 6.72508s/10 iters), loss = 5.73876
I0524 04:37:41.914582 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73876 (* 1 = 5.73876 loss)
I0524 04:37:41.914638 11835 sgd_solver.cpp:112] Iteration 34640, lr = 0.1
I0524 04:37:47.707656 11835 solver.cpp:239] Iteration 34650 (1.72628 iter/s, 5.7928s/10 iters), loss = 5.77324
I0524 04:37:47.707710 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.77324 (* 1 = 5.77324 loss)
I0524 04:37:47.707731 11835 sgd_solver.cpp:112] Iteration 34650, lr = 0.1
I0524 04:37:55.017946 11835 solver.cpp:239] Iteration 34660 (1.368 iter/s, 7.30996s/10 iters), loss = 6.00715
I0524 04:37:55.017994 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00715 (* 1 = 6.00715 loss)
I0524 04:37:55.018266 11835 sgd_solver.cpp:112] Iteration 34660, lr = 0.1
I0524 04:38:02.505343 11835 solver.cpp:239] Iteration 34670 (1.33564 iter/s, 7.48705s/10 iters), loss = 6.76205
I0524 04:38:02.505396 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76205 (* 1 = 6.76205 loss)
I0524 04:38:02.505650 11835 sgd_solver.cpp:112] Iteration 34670, lr = 0.1
I0524 04:38:08.862956 11835 solver.cpp:239] Iteration 34680 (1.57299 iter/s, 6.35731s/10 iters), loss = 6.03168
I0524 04:38:08.863011 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03168 (* 1 = 6.03168 loss)
I0524 04:38:08.863240 11835 sgd_solver.cpp:112] Iteration 34680, lr = 0.1
I0524 04:38:16.481400 11835 solver.cpp:239] Iteration 34690 (1.31267 iter/s, 7.61809s/10 iters), loss = 6.00447
I0524 04:38:16.481542 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00447 (* 1 = 6.00447 loss)
I0524 04:38:16.481654 11835 sgd_solver.cpp:112] Iteration 34690, lr = 0.1
I0524 04:38:23.018836 11835 solver.cpp:239] Iteration 34700 (1.52975 iter/s, 6.53703s/10 iters), loss = 7.07789
I0524 04:38:23.018893 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.07789 (* 1 = 7.07789 loss)
I0524 04:38:23.019172 11835 sgd_solver.cpp:112] Iteration 34700, lr = 0.1
I0524 04:38:29.277657 11835 solver.cpp:239] Iteration 34710 (1.59782 iter/s, 6.25854s/10 iters), loss = 6.11738
I0524 04:38:29.277694 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11738 (* 1 = 6.11738 loss)
I0524 04:38:29.278043 11835 sgd_solver.cpp:112] Iteration 34710, lr = 0.1
I0524 04:38:36.495072 11835 solver.cpp:239] Iteration 34720 (1.3856 iter/s, 7.2171s/10 iters), loss = 6.65776
I0524 04:38:36.495129 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65776 (* 1 = 6.65776 loss)
I0524 04:38:36.495252 11835 sgd_solver.cpp:112] Iteration 34720, lr = 0.1
I0524 04:38:46.166419 11835 solver.cpp:239] Iteration 34730 (1.03403 iter/s, 9.67094s/10 iters), loss = 6.09562
I0524 04:38:46.166458 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09562 (* 1 = 6.09562 loss)
I0524 04:38:46.166715 11835 sgd_solver.cpp:112] Iteration 34730, lr = 0.1
I0524 04:38:52.704435 11835 solver.cpp:239] Iteration 34740 (1.52959 iter/s, 6.53771s/10 iters), loss = 7.32476
I0524 04:38:52.704738 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.32476 (* 1 = 7.32476 loss)
I0524 04:38:52.880054 11835 sgd_solver.cpp:112] Iteration 34740, lr = 0.1
I0524 04:38:58.676231 11835 solver.cpp:239] Iteration 34750 (1.67468 iter/s, 5.97128s/10 iters), loss = 6.33589
I0524 04:38:58.676283 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33589 (* 1 = 6.33589 loss)
I0524 04:38:58.676302 11835 sgd_solver.cpp:112] Iteration 34750, lr = 0.1
I0524 04:39:05.522675 11835 solver.cpp:239] Iteration 34760 (1.46075 iter/s, 6.84578s/10 iters), loss = 6.18555
I0524 04:39:05.522752 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18555 (* 1 = 6.18555 loss)
I0524 04:39:05.523747 11835 sgd_solver.cpp:112] Iteration 34760, lr = 0.1
I0524 04:39:11.690459 11835 solver.cpp:239] Iteration 34770 (1.62141 iter/s, 6.16747s/10 iters), loss = 5.31487
I0524 04:39:11.690505 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.31487 (* 1 = 5.31487 loss)
I0524 04:39:11.690521 11835 sgd_solver.cpp:112] Iteration 34770, lr = 0.1
I0524 04:39:18.652870 11835 solver.cpp:239] Iteration 34780 (1.43637 iter/s, 6.96199s/10 iters), loss = 5.59736
I0524 04:39:18.652912 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.59736 (* 1 = 5.59736 loss)
I0524 04:39:18.652927 11835 sgd_solver.cpp:112] Iteration 34780, lr = 0.1
I0524 04:39:24.272855 11835 solver.cpp:239] Iteration 34790 (1.77945 iter/s, 5.61971s/10 iters), loss = 5.72428
I0524 04:39:24.273058 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72428 (* 1 = 5.72428 loss)
I0524 04:39:24.273110 11835 sgd_solver.cpp:112] Iteration 34790, lr = 0.1
I0524 04:39:30.204247 11835 solver.cpp:239] Iteration 34800 (1.68606 iter/s, 5.931s/10 iters), loss = 6.19064
I0524 04:39:30.204293 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19064 (* 1 = 6.19064 loss)
I0524 04:39:30.204306 11835 sgd_solver.cpp:112] Iteration 34800, lr = 0.1
I0524 04:39:36.336324 11835 solver.cpp:239] Iteration 34810 (1.63114 iter/s, 6.13069s/10 iters), loss = 7.06573
I0524 04:39:36.336371 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.06573 (* 1 = 7.06573 loss)
I0524 04:39:36.336385 11835 sgd_solver.cpp:112] Iteration 34810, lr = 0.1
I0524 04:39:43.630479 11835 solver.cpp:239] Iteration 34820 (1.37104 iter/s, 7.29376s/10 iters), loss = 6.02848
I0524 04:39:43.630525 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02848 (* 1 = 6.02848 loss)
I0524 04:39:43.630539 11835 sgd_solver.cpp:112] Iteration 34820, lr = 0.1
I0524 04:39:49.394271 11835 solver.cpp:239] Iteration 34830 (1.73517 iter/s, 5.76314s/10 iters), loss = 6.76504
I0524 04:39:49.394320 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76504 (* 1 = 6.76504 loss)
I0524 04:39:49.394737 11835 sgd_solver.cpp:112] Iteration 34830, lr = 0.1
I0524 04:39:56.204210 11835 solver.cpp:239] Iteration 34840 (1.46851 iter/s, 6.80964s/10 iters), loss = 5.78817
I0524 04:39:56.204444 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78817 (* 1 = 5.78817 loss)
I0524 04:39:56.812315 11835 sgd_solver.cpp:112] Iteration 34840, lr = 0.1
I0524 04:40:02.713057 11835 solver.cpp:239] Iteration 34850 (1.53648 iter/s, 6.50839s/10 iters), loss = 6.97106
I0524 04:40:02.713116 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.97106 (* 1 = 6.97106 loss)
I0524 04:40:02.713299 11835 sgd_solver.cpp:112] Iteration 34850, lr = 0.1
I0524 04:40:08.538653 11835 solver.cpp:239] Iteration 34860 (1.71665 iter/s, 5.82531s/10 iters), loss = 6.18703
I0524 04:40:08.538719 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18703 (* 1 = 6.18703 loss)
I0524 04:40:08.538776 11835 sgd_solver.cpp:112] Iteration 34860, lr = 0.1
I0524 04:40:14.470475 11835 solver.cpp:239] Iteration 34870 (1.6859 iter/s, 5.93155s/10 iters), loss = 6.61208
I0524 04:40:14.470513 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61208 (* 1 = 6.61208 loss)
I0524 04:40:14.470721 11835 sgd_solver.cpp:112] Iteration 34870, lr = 0.1
I0524 04:40:21.682276 11835 solver.cpp:239] Iteration 34880 (1.38668 iter/s, 7.21148s/10 iters), loss = 6.25404
I0524 04:40:21.682327 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25404 (* 1 = 6.25404 loss)
I0524 04:40:21.682453 11835 sgd_solver.cpp:112] Iteration 34880, lr = 0.1
I0524 04:40:29.298923 11835 solver.cpp:239] Iteration 34890 (1.31297 iter/s, 7.61631s/10 iters), loss = 5.93332
I0524 04:40:29.299101 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93332 (* 1 = 5.93332 loss)
I0524 04:40:29.537291 11835 sgd_solver.cpp:112] Iteration 34890, lr = 0.1
I0524 04:40:36.052729 11835 solver.cpp:239] Iteration 34900 (1.48074 iter/s, 6.75339s/10 iters), loss = 6.55425
I0524 04:40:36.052778 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55425 (* 1 = 6.55425 loss)
I0524 04:40:36.052893 11835 sgd_solver.cpp:112] Iteration 34900, lr = 0.1
I0524 04:40:42.128845 11835 solver.cpp:239] Iteration 34910 (1.64587 iter/s, 6.07583s/10 iters), loss = 7.72791
I0524 04:40:42.128896 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.72791 (* 1 = 7.72791 loss)
I0524 04:40:42.310722 11835 sgd_solver.cpp:112] Iteration 34910, lr = 0.1
I0524 04:40:48.016171 11835 solver.cpp:239] Iteration 34920 (1.69864 iter/s, 5.88706s/10 iters), loss = 5.99844
I0524 04:40:48.016209 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99844 (* 1 = 5.99844 loss)
I0524 04:40:48.016350 11835 sgd_solver.cpp:112] Iteration 34920, lr = 0.1
I0524 04:40:54.819643 11835 solver.cpp:239] Iteration 34930 (1.46991 iter/s, 6.80316s/10 iters), loss = 7.09586
I0524 04:40:54.819697 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09586 (* 1 = 7.09586 loss)
I0524 04:40:54.819712 11835 sgd_solver.cpp:112] Iteration 34930, lr = 0.1
I0524 04:41:01.965436 11835 solver.cpp:239] Iteration 34940 (1.39949 iter/s, 7.14546s/10 iters), loss = 6.44184
I0524 04:41:01.965636 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44184 (* 1 = 6.44184 loss)
I0524 04:41:01.965822 11835 sgd_solver.cpp:112] Iteration 34940, lr = 0.1
I0524 04:41:08.145364 11835 solver.cpp:239] Iteration 34950 (1.61825 iter/s, 6.1795s/10 iters), loss = 7.11289
I0524 04:41:08.145407 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11289 (* 1 = 7.11289 loss)
I0524 04:41:08.145421 11835 sgd_solver.cpp:112] Iteration 34950, lr = 0.1
I0524 04:41:14.687464 11835 solver.cpp:239] Iteration 34960 (1.52863 iter/s, 6.5418s/10 iters), loss = 6.99444
I0524 04:41:14.687511 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.99444 (* 1 = 6.99444 loss)
I0524 04:41:14.687616 11835 sgd_solver.cpp:112] Iteration 34960, lr = 0.1
I0524 04:41:20.837493 11835 solver.cpp:239] Iteration 34970 (1.62608 iter/s, 6.14975s/10 iters), loss = 5.34558
I0524 04:41:20.837532 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.34558 (* 1 = 5.34558 loss)
I0524 04:41:20.837544 11835 sgd_solver.cpp:112] Iteration 34970, lr = 0.1
I0524 04:41:28.304406 11835 solver.cpp:239] Iteration 34980 (1.33934 iter/s, 7.46634s/10 iters), loss = 7.24669
I0524 04:41:28.304458 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.24669 (* 1 = 7.24669 loss)
I0524 04:41:28.909313 11835 sgd_solver.cpp:112] Iteration 34980, lr = 0.1
I0524 04:41:37.975996 11835 solver.cpp:239] Iteration 34990 (1.034 iter/s, 9.67117s/10 iters), loss = 6.08477
I0524 04:41:37.976186 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08477 (* 1 = 6.08477 loss)
I0524 04:41:37.976225 11835 sgd_solver.cpp:112] Iteration 34990, lr = 0.1
I0524 04:41:45.044960 11835 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_35000.caffemodel
I0524 04:41:45.212116 11835 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_35000.solverstate
I0524 04:41:45.790452 11835 solver.cpp:239] Iteration 35000 (1.28011 iter/s, 7.81182s/10 iters), loss = 6.27081
I0524 04:41:45.790488 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27081 (* 1 = 6.27081 loss)
I0524 04:41:45.790510 11835 sgd_solver.cpp:112] Iteration 35000, lr = 0.1
I0524 04:41:51.242514 11835 solver.cpp:239] Iteration 35010 (1.83426 iter/s, 5.4518s/10 iters), loss = 6.413
I0524 04:41:51.242571 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.413 (* 1 = 6.413 loss)
I0524 04:41:51.242802 11835 sgd_solver.cpp:112] Iteration 35010, lr = 0.1
I0524 04:41:57.954334 11835 solver.cpp:239] Iteration 35020 (1.48998 iter/s, 6.7115s/10 iters), loss = 5.86914
I0524 04:41:57.954387 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86914 (* 1 = 5.86914 loss)
I0524 04:41:57.954484 11835 sgd_solver.cpp:112] Iteration 35020, lr = 0.1
I0524 04:42:05.535835 11835 solver.cpp:239] Iteration 35030 (1.31906 iter/s, 7.58116s/10 iters), loss = 6.21875
I0524 04:42:05.535876 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21875 (* 1 = 6.21875 loss)
I0524 04:42:06.003950 11835 sgd_solver.cpp:112] Iteration 35030, lr = 0.1
I0524 04:42:12.454723 11835 solver.cpp:239] Iteration 35040 (1.44539 iter/s, 6.91856s/10 iters), loss = 5.74018
I0524 04:42:12.455030 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74018 (* 1 = 5.74018 loss)
I0524 04:42:12.455090 11835 sgd_solver.cpp:112] Iteration 35040, lr = 0.1
I0524 04:42:20.465940 11835 solver.cpp:239] Iteration 35050 (1.24838 iter/s, 8.01041s/10 iters), loss = 6.5035
I0524 04:42:20.465996 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5035 (* 1 = 6.5035 loss)
I0524 04:42:20.466246 11835 sgd_solver.cpp:112] Iteration 35050, lr = 0.1
I0524 04:42:26.610357 11835 solver.cpp:239] Iteration 35060 (1.62757 iter/s, 6.14413s/10 iters), loss = 6.51261
I0524 04:42:26.610411 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51261 (* 1 = 6.51261 loss)
I0524 04:42:26.701485 11835 sgd_solver.cpp:112] Iteration 35060, lr = 0.1
I0524 04:42:32.375311 11835 solver.cpp:239] Iteration 35070 (1.7347 iter/s, 5.76469s/10 iters), loss = 5.92482
I0524 04:42:32.375346 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92482 (* 1 = 5.92482 loss)
I0524 04:42:32.375358 11835 sgd_solver.cpp:112] Iteration 35070, lr = 0.1
I0524 04:42:38.977602 11835 solver.cpp:239] Iteration 35080 (1.51469 iter/s, 6.602s/10 iters), loss = 6.67488
I0524 04:42:38.977639 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67488 (* 1 = 6.67488 loss)
I0524 04:42:39.307067 11835 sgd_solver.cpp:112] Iteration 35080, lr = 0.1
I0524 04:42:45.437582 11835 solver.cpp:239] Iteration 35090 (1.54806 iter/s, 6.45968s/10 iters), loss = 6.64039
I0524 04:42:45.437856 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64039 (* 1 = 6.64039 loss)
I0524 04:42:45.437923 11835 sgd_solver.cpp:112] Iteration 35090, lr = 0.1
I0524 04:42:52.307189 11835 solver.cpp:239] Iteration 35100 (1.45579 iter/s, 6.86912s/10 iters), loss = 6.49413
I0524 04:42:52.307240 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49413 (* 1 = 6.49413 loss)
I0524 04:42:52.307255 11835 sgd_solver.cpp:112] Iteration 35100, lr = 0.1
I0524 04:43:00.317646 11835 solver.cpp:239] Iteration 35110 (1.24848 iter/s, 8.00973s/10 iters), loss = 6.13718
I0524 04:43:00.317689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13718 (* 1 = 6.13718 loss)
I0524 04:43:00.473902 11835 sgd_solver.cpp:112] Iteration 35110, lr = 0.1
I0524 04:43:06.893092 11835 solver.cpp:239] Iteration 35120 (1.52088 iter/s, 6.57514s/10 iters), loss = 7.3423
I0524 04:43:06.893141 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.3423 (* 1 = 7.3423 loss)
I0524 04:43:07.250798 11835 sgd_solver.cpp:112] Iteration 35120, lr = 0.1
I0524 04:43:15.976795 11835 solver.cpp:239] Iteration 35130 (1.10092 iter/s, 9.08331s/10 iters), loss = 6.78538
I0524 04:43:15.977058 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78538 (* 1 = 6.78538 loss)
I0524 04:43:15.977111 11835 sgd_solver.cpp:112] Iteration 35130, lr = 0.1
I0524 04:43:22.109192 11835 solver.cpp:239] Iteration 35140 (1.63082 iter/s, 6.13188s/10 iters), loss = 5.97583
I0524 04:43:22.109232 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97583 (* 1 = 5.97583 loss)
I0524 04:43:22.109244 11835 sgd_solver.cpp:112] Iteration 35140, lr = 0.1
I0524 04:43:28.033362 11835 solver.cpp:239] Iteration 35150 (1.68812 iter/s, 5.92375s/10 iters), loss = 6.27834
I0524 04:43:28.033417 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27834 (* 1 = 6.27834 loss)
I0524 04:43:28.033432 11835 sgd_solver.cpp:112] Iteration 35150, lr = 0.1
I0524 04:43:37.320780 11835 solver.cpp:239] Iteration 35160 (1.07703 iter/s, 9.28478s/10 iters), loss = 7.20075
I0524 04:43:37.320830 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.20075 (* 1 = 7.20075 loss)
I0524 04:43:39.094162 11835 sgd_solver.cpp:112] Iteration 35160, lr = 0.1
I0524 04:43:46.390785 11835 solver.cpp:239] Iteration 35170 (1.10258 iter/s, 9.06962s/10 iters), loss = 6.42948
I0524 04:43:46.391024 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42948 (* 1 = 6.42948 loss)
I0524 04:43:46.391064 11835 sgd_solver.cpp:112] Iteration 35170, lr = 0.1
I0524 04:43:52.058080 11835 solver.cpp:239] Iteration 35180 (1.76465 iter/s, 5.66686s/10 iters), loss = 6.40007
I0524 04:43:52.058153 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40007 (* 1 = 6.40007 loss)
I0524 04:43:52.058179 11835 sgd_solver.cpp:112] Iteration 35180, lr = 0.1
I0524 04:43:59.073220 11835 solver.cpp:239] Iteration 35190 (1.426 iter/s, 7.01262s/10 iters), loss = 8.08071
I0524 04:43:59.073276 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.08071 (* 1 = 8.08071 loss)
I0524 04:43:59.073292 11835 sgd_solver.cpp:112] Iteration 35190, lr = 0.1
I0524 04:44:04.898895 11835 solver.cpp:239] Iteration 35200 (1.71694 iter/s, 5.8243s/10 iters), loss = 7.12815
I0524 04:44:04.898941 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.12815 (* 1 = 7.12815 loss)
I0524 04:44:04.899091 11835 sgd_solver.cpp:112] Iteration 35200, lr = 0.1
I0524 04:44:11.743574 11835 solver.cpp:239] Iteration 35210 (1.46106 iter/s, 6.84436s/10 iters), loss = 6.80445
I0524 04:44:11.743624 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80445 (* 1 = 6.80445 loss)
I0524 04:44:11.743791 11835 sgd_solver.cpp:112] Iteration 35210, lr = 0.1
I0524 04:44:21.798193 11835 solver.cpp:239] Iteration 35220 (0.994611 iter/s, 10.0542s/10 iters), loss = 6.26967
I0524 04:44:21.798347 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26967 (* 1 = 6.26967 loss)
I0524 04:44:21.798490 11835 sgd_solver.cpp:112] Iteration 35220, lr = 0.1
I0524 04:44:29.753662 11835 solver.cpp:239] Iteration 35230 (1.25707 iter/s, 7.95501s/10 iters), loss = 7.33697
I0524 04:44:29.753717 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.33697 (* 1 = 7.33697 loss)
I0524 04:44:29.753844 11835 sgd_solver.cpp:112] Iteration 35230, lr = 0.1
I0524 04:44:36.193053 11835 solver.cpp:239] Iteration 35240 (1.55301 iter/s, 6.43909s/10 iters), loss = 6.74212
I0524 04:44:36.193100 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74212 (* 1 = 6.74212 loss)
I0524 04:44:36.193115 11835 sgd_solver.cpp:112] Iteration 35240, lr = 0.1
I0524 04:44:41.821669 11835 solver.cpp:239] Iteration 35250 (1.77707 iter/s, 5.62725s/10 iters), loss = 6.88981
I0524 04:44:41.821707 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88981 (* 1 = 6.88981 loss)
I0524 04:44:41.821954 11835 sgd_solver.cpp:112] Iteration 35250, lr = 0.1
I0524 04:44:49.015506 11835 solver.cpp:239] Iteration 35260 (1.39014 iter/s, 7.19351s/10 iters), loss = 6.43158
I0524 04:44:49.015563 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43158 (* 1 = 6.43158 loss)
I0524 04:44:49.015686 11835 sgd_solver.cpp:112] Iteration 35260, lr = 0.1
I0524 04:44:56.614432 11835 solver.cpp:239] Iteration 35270 (1.31604 iter/s, 7.59858s/10 iters), loss = 6.16464
I0524 04:44:56.614778 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16464 (* 1 = 6.16464 loss)
I0524 04:44:57.879210 11835 sgd_solver.cpp:112] Iteration 35270, lr = 0.1
I0524 04:45:03.908917 11835 solver.cpp:239] Iteration 35280 (1.371 iter/s, 7.29395s/10 iters), loss = 6.36641
I0524 04:45:03.908962 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36641 (* 1 = 6.36641 loss)
I0524 04:45:03.909373 11835 sgd_solver.cpp:112] Iteration 35280, lr = 0.1
I0524 04:45:09.558392 11835 solver.cpp:239] Iteration 35290 (1.77016 iter/s, 5.6492s/10 iters), loss = 6.45299
I0524 04:45:09.558444 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45299 (* 1 = 6.45299 loss)
I0524 04:45:09.558462 11835 sgd_solver.cpp:112] Iteration 35290, lr = 0.1
I0524 04:45:16.051779 11835 solver.cpp:239] Iteration 35300 (1.54012 iter/s, 6.493s/10 iters), loss = 5.78652
I0524 04:45:16.051826 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78652 (* 1 = 5.78652 loss)
I0524 04:45:16.359664 11835 sgd_solver.cpp:112] Iteration 35300, lr = 0.1
I0524 04:45:22.355520 11835 solver.cpp:239] Iteration 35310 (1.58643 iter/s, 6.30346s/10 iters), loss = 5.91848
I0524 04:45:22.355561 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91848 (* 1 = 5.91848 loss)
I0524 04:45:22.355573 11835 sgd_solver.cpp:112] Iteration 35310, lr = 0.1
I0524 04:45:29.024154 11835 solver.cpp:239] Iteration 35320 (1.49964 iter/s, 6.66825s/10 iters), loss = 6.09254
I0524 04:45:29.024271 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09254 (* 1 = 6.09254 loss)
I0524 04:45:29.024428 11835 sgd_solver.cpp:112] Iteration 35320, lr = 0.1
I0524 04:45:36.750205 11835 solver.cpp:239] Iteration 35330 (1.29439 iter/s, 7.72563s/10 iters), loss = 6.29917
I0524 04:45:36.750282 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29917 (* 1 = 6.29917 loss)
I0524 04:45:36.750445 11835 sgd_solver.cpp:112] Iteration 35330, lr = 0.1
I0524 04:45:44.120852 11835 solver.cpp:239] Iteration 35340 (1.3568 iter/s, 7.3703s/10 iters), loss = 6.64821
I0524 04:45:44.120901 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64821 (* 1 = 6.64821 loss)
I0524 04:45:44.168597 11835 sgd_solver.cpp:112] Iteration 35340, lr = 0.1
I0524 04:45:51.687021 11835 solver.cpp:239] Iteration 35350 (1.32173 iter/s, 7.56582s/10 iters), loss = 5.84447
I0524 04:45:51.687077 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84447 (* 1 = 5.84447 loss)
I0524 04:45:51.687152 11835 sgd_solver.cpp:112] Iteration 35350, lr = 0.1
I0524 04:45:57.560061 11835 solver.cpp:239] Iteration 35360 (1.70278 iter/s, 5.87277s/10 iters), loss = 6.03314
I0524 04:45:57.560103 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03314 (* 1 = 6.03314 loss)
I0524 04:45:58.468636 11835 sgd_solver.cpp:112] Iteration 35360, lr = 0.1
I0524 04:46:05.641291 11835 solver.cpp:239] Iteration 35370 (1.23749 iter/s, 8.08087s/10 iters), loss = 5.98532
I0524 04:46:05.641537 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98532 (* 1 = 5.98532 loss)
I0524 04:46:05.683919 11835 sgd_solver.cpp:112] Iteration 35370, lr = 0.1
I0524 04:46:12.403244 11835 solver.cpp:239] Iteration 35380 (1.47896 iter/s, 6.76149s/10 iters), loss = 5.98913
I0524 04:46:12.403292 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98913 (* 1 = 5.98913 loss)
I0524 04:46:12.422421 11835 sgd_solver.cpp:112] Iteration 35380, lr = 0.1
I0524 04:46:18.984450 11835 solver.cpp:239] Iteration 35390 (1.51955 iter/s, 6.58091s/10 iters), loss = 6.01511
I0524 04:46:18.984488 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01511 (* 1 = 6.01511 loss)
I0524 04:46:18.984630 11835 sgd_solver.cpp:112] Iteration 35390, lr = 0.1
I0524 04:46:25.030222 11835 solver.cpp:239] Iteration 35400 (1.65413 iter/s, 6.04549s/10 iters), loss = 7.01645
I0524 04:46:25.030270 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01645 (* 1 = 7.01645 loss)
I0524 04:46:25.031244 11835 sgd_solver.cpp:112] Iteration 35400, lr = 0.1
I0524 04:46:31.940469 11835 solver.cpp:239] Iteration 35410 (1.44719 iter/s, 6.90994s/10 iters), loss = 6.0145
I0524 04:46:31.940508 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0145 (* 1 = 6.0145 loss)
I0524 04:46:31.940542 11835 sgd_solver.cpp:112] Iteration 35410, lr = 0.1
I0524 04:46:37.527791 11835 solver.cpp:239] Iteration 35420 (1.78985 iter/s, 5.58705s/10 iters), loss = 6.61813
I0524 04:46:37.528129 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61813 (* 1 = 6.61813 loss)
I0524 04:46:37.528188 11835 sgd_solver.cpp:112] Iteration 35420, lr = 0.1
I0524 04:46:45.658107 11835 solver.cpp:239] Iteration 35430 (1.23008 iter/s, 8.12952s/10 iters), loss = 7.19716
I0524 04:46:45.658159 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.19716 (* 1 = 7.19716 loss)
I0524 04:46:45.658257 11835 sgd_solver.cpp:112] Iteration 35430, lr = 0.1
I0524 04:46:51.537632 11835 solver.cpp:239] Iteration 35440 (1.7009 iter/s, 5.87924s/10 iters), loss = 6.21761
I0524 04:46:51.537674 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21761 (* 1 = 6.21761 loss)
I0524 04:46:51.537689 11835 sgd_solver.cpp:112] Iteration 35440, lr = 0.1
I0524 04:46:58.472555 11835 solver.cpp:239] Iteration 35450 (1.44204 iter/s, 6.9346s/10 iters), loss = 5.70191
I0524 04:46:58.472611 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.70191 (* 1 = 5.70191 loss)
I0524 04:46:58.557418 11835 sgd_solver.cpp:112] Iteration 35450, lr = 0.1
I0524 04:47:05.556021 11835 solver.cpp:239] Iteration 35460 (1.4118 iter/s, 7.08315s/10 iters), loss = 5.7775
I0524 04:47:05.556061 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7775 (* 1 = 5.7775 loss)
I0524 04:47:05.556073 11835 sgd_solver.cpp:112] Iteration 35460, lr = 0.1
I0524 04:47:14.443426 11835 solver.cpp:239] Iteration 35470 (1.12538 iter/s, 8.8859s/10 iters), loss = 6.04886
I0524 04:47:14.443701 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04886 (* 1 = 6.04886 loss)
I0524 04:47:15.069313 11835 sgd_solver.cpp:112] Iteration 35470, lr = 0.1
I0524 04:47:23.810175 11835 solver.cpp:239] Iteration 35480 (1.06767 iter/s, 9.36615s/10 iters), loss = 5.8396
I0524 04:47:23.810235 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8396 (* 1 = 5.8396 loss)
I0524 04:47:23.810475 11835 sgd_solver.cpp:112] Iteration 35480, lr = 0.1
I0524 04:47:30.342512 11835 solver.cpp:239] Iteration 35490 (1.53092 iter/s, 6.53202s/10 iters), loss = 6.18226
I0524 04:47:30.342562 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18226 (* 1 = 6.18226 loss)
I0524 04:47:30.525871 11835 sgd_solver.cpp:112] Iteration 35490, lr = 0.1
I0524 04:47:36.555457 11835 solver.cpp:239] Iteration 35500 (1.60962 iter/s, 6.21266s/10 iters), loss = 6.50843
I0524 04:47:36.555503 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50843 (* 1 = 6.50843 loss)
I0524 04:47:37.147698 11835 sgd_solver.cpp:112] Iteration 35500, lr = 0.1
I0524 04:47:44.591691 11835 solver.cpp:239] Iteration 35510 (1.24442 iter/s, 8.03587s/10 iters), loss = 6.48369
I0524 04:47:44.591948 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48369 (* 1 = 6.48369 loss)
I0524 04:47:44.631819 11835 sgd_solver.cpp:112] Iteration 35510, lr = 0.1
I0524 04:47:51.830380 11835 solver.cpp:239] Iteration 35520 (1.38156 iter/s, 7.2382s/10 iters), loss = 5.67527
I0524 04:47:51.830420 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67527 (* 1 = 5.67527 loss)
I0524 04:47:51.844455 11835 sgd_solver.cpp:112] Iteration 35520, lr = 0.1
I0524 04:47:57.979238 11835 solver.cpp:239] Iteration 35530 (1.62639 iter/s, 6.14857s/10 iters), loss = 5.84698
I0524 04:47:57.979286 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84698 (* 1 = 5.84698 loss)
I0524 04:47:57.979569 11835 sgd_solver.cpp:112] Iteration 35530, lr = 0.1
I0524 04:48:03.780649 11835 solver.cpp:239] Iteration 35540 (1.7238 iter/s, 5.80115s/10 iters), loss = 6.72317
I0524 04:48:03.780686 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72317 (* 1 = 6.72317 loss)
I0524 04:48:03.780697 11835 sgd_solver.cpp:112] Iteration 35540, lr = 0.1
I0524 04:48:10.623623 11835 solver.cpp:239] Iteration 35550 (1.46142 iter/s, 6.84267s/10 iters), loss = 7.5028
I0524 04:48:10.623668 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.5028 (* 1 = 7.5028 loss)
I0524 04:48:10.623865 11835 sgd_solver.cpp:112] Iteration 35550, lr = 0.1
I0524 04:48:17.085065 11835 solver.cpp:239] Iteration 35560 (1.54772 iter/s, 6.46112s/10 iters), loss = 5.1544
I0524 04:48:17.085239 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.1544 (* 1 = 5.1544 loss)
I0524 04:48:17.085259 11835 sgd_solver.cpp:112] Iteration 35560, lr = 0.1
I0524 04:48:23.945266 11835 solver.cpp:239] Iteration 35570 (1.45795 iter/s, 6.85893s/10 iters), loss = 6.53186
I0524 04:48:23.945308 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53186 (* 1 = 6.53186 loss)
I0524 04:48:23.945538 11835 sgd_solver.cpp:112] Iteration 35570, lr = 0.1
I0524 04:48:31.519407 11835 solver.cpp:239] Iteration 35580 (1.32034 iter/s, 7.57379s/10 iters), loss = 6.12156
I0524 04:48:31.519462 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12156 (* 1 = 6.12156 loss)
I0524 04:48:31.683377 11835 sgd_solver.cpp:112] Iteration 35580, lr = 0.1
I0524 04:48:37.235146 11835 solver.cpp:239] Iteration 35590 (1.74964 iter/s, 5.71547s/10 iters), loss = 5.75409
I0524 04:48:37.235183 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75409 (* 1 = 5.75409 loss)
I0524 04:48:37.235195 11835 sgd_solver.cpp:112] Iteration 35590, lr = 0.1
I0524 04:48:43.420079 11835 solver.cpp:239] Iteration 35600 (1.61691 iter/s, 6.18465s/10 iters), loss = 6.93265
I0524 04:48:43.420133 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93265 (* 1 = 6.93265 loss)
I0524 04:48:43.985062 11835 sgd_solver.cpp:112] Iteration 35600, lr = 0.1
I0524 04:48:52.261482 11835 solver.cpp:239] Iteration 35610 (1.13109 iter/s, 8.84102s/10 iters), loss = 6.51288
I0524 04:48:52.261606 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51288 (* 1 = 6.51288 loss)
I0524 04:48:52.725757 11835 sgd_solver.cpp:112] Iteration 35610, lr = 0.1
I0524 04:48:59.617763 11835 solver.cpp:239] Iteration 35620 (1.35946 iter/s, 7.35587s/10 iters), loss = 5.9619
I0524 04:48:59.617812 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9619 (* 1 = 5.9619 loss)
I0524 04:48:59.691432 11835 sgd_solver.cpp:112] Iteration 35620, lr = 0.1
I0524 04:49:06.569455 11835 solver.cpp:239] Iteration 35630 (1.43857 iter/s, 6.95136s/10 iters), loss = 5.36407
I0524 04:49:06.569504 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.36407 (* 1 = 5.36407 loss)
I0524 04:49:06.569614 11835 sgd_solver.cpp:112] Iteration 35630, lr = 0.1
I0524 04:49:12.611469 11835 solver.cpp:239] Iteration 35640 (1.65515 iter/s, 6.04173s/10 iters), loss = 6.20521
I0524 04:49:12.611510 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20521 (* 1 = 6.20521 loss)
I0524 04:49:12.776180 11835 sgd_solver.cpp:112] Iteration 35640, lr = 0.1
I0524 04:49:19.212069 11835 solver.cpp:239] Iteration 35650 (1.51509 iter/s, 6.60029s/10 iters), loss = 7.19865
I0524 04:49:19.212127 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.19865 (* 1 = 7.19865 loss)
I0524 04:49:20.109316 11835 sgd_solver.cpp:112] Iteration 35650, lr = 0.1
I0524 04:49:27.530874 11835 solver.cpp:239] Iteration 35660 (1.20215 iter/s, 8.31843s/10 iters), loss = 6.55111
I0524 04:49:27.531016 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55111 (* 1 = 6.55111 loss)
I0524 04:49:27.531106 11835 sgd_solver.cpp:112] Iteration 35660, lr = 0.1
I0524 04:49:33.918737 11835 solver.cpp:239] Iteration 35670 (1.56556 iter/s, 6.38748s/10 iters), loss = 6.62198
I0524 04:49:33.918782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62198 (* 1 = 6.62198 loss)
I0524 04:49:33.918797 11835 sgd_solver.cpp:112] Iteration 35670, lr = 0.1
I0524 04:49:39.923302 11835 solver.cpp:239] Iteration 35680 (1.66578 iter/s, 6.00318s/10 iters), loss = 5.63143
I0524 04:49:39.923352 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63143 (* 1 = 5.63143 loss)
I0524 04:49:39.923367 11835 sgd_solver.cpp:112] Iteration 35680, lr = 0.1
I0524 04:49:46.675367 11835 solver.cpp:239] Iteration 35690 (1.48111 iter/s, 6.75169s/10 iters), loss = 6.76232
I0524 04:49:46.675406 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76232 (* 1 = 6.76232 loss)
I0524 04:49:46.675534 11835 sgd_solver.cpp:112] Iteration 35690, lr = 0.1
I0524 04:49:52.494403 11835 solver.cpp:239] Iteration 35700 (1.71858 iter/s, 5.81877s/10 iters), loss = 6.63769
I0524 04:49:52.494451 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63769 (* 1 = 6.63769 loss)
I0524 04:49:52.494541 11835 sgd_solver.cpp:112] Iteration 35700, lr = 0.1
I0524 04:49:59.590652 11835 solver.cpp:239] Iteration 35710 (1.40926 iter/s, 7.09593s/10 iters), loss = 6.57019
I0524 04:49:59.590909 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57019 (* 1 = 6.57019 loss)
I0524 04:49:59.590956 11835 sgd_solver.cpp:112] Iteration 35710, lr = 0.1
I0524 04:50:05.552865 11835 solver.cpp:239] Iteration 35720 (1.67744 iter/s, 5.96147s/10 iters), loss = 6.65973
I0524 04:50:05.552906 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65973 (* 1 = 6.65973 loss)
I0524 04:50:05.552917 11835 sgd_solver.cpp:112] Iteration 35720, lr = 0.1
I0524 04:50:11.125336 11835 solver.cpp:239] Iteration 35730 (1.79483 iter/s, 5.57156s/10 iters), loss = 6.34464
I0524 04:50:11.125385 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34464 (* 1 = 6.34464 loss)
I0524 04:50:11.125399 11835 sgd_solver.cpp:112] Iteration 35730, lr = 0.1
I0524 04:50:17.158741 11835 solver.cpp:239] Iteration 35740 (1.65751 iter/s, 6.03313s/10 iters), loss = 6.9015
I0524 04:50:17.158787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9015 (* 1 = 6.9015 loss)
I0524 04:50:17.158799 11835 sgd_solver.cpp:112] Iteration 35740, lr = 0.1
I0524 04:50:24.094736 11835 solver.cpp:239] Iteration 35750 (1.44186 iter/s, 6.9355s/10 iters), loss = 6.67756
I0524 04:50:24.094790 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67756 (* 1 = 6.67756 loss)
I0524 04:50:24.094869 11835 sgd_solver.cpp:112] Iteration 35750, lr = 0.1
I0524 04:50:30.321210 11835 solver.cpp:239] Iteration 35760 (1.60612 iter/s, 6.22619s/10 iters), loss = 5.91146
I0524 04:50:30.321451 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91146 (* 1 = 5.91146 loss)
I0524 04:50:30.557477 11835 sgd_solver.cpp:112] Iteration 35760, lr = 0.1
I0524 04:50:39.415457 11835 solver.cpp:239] Iteration 35770 (1.09966 iter/s, 9.09368s/10 iters), loss = 6.14857
I0524 04:50:39.415504 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14857 (* 1 = 6.14857 loss)
I0524 04:50:40.278712 11835 sgd_solver.cpp:112] Iteration 35770, lr = 0.1
I0524 04:50:46.709384 11835 solver.cpp:239] Iteration 35780 (1.37107 iter/s, 7.2936s/10 iters), loss = 6.05487
I0524 04:50:46.709439 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05487 (* 1 = 6.05487 loss)
I0524 04:50:46.709643 11835 sgd_solver.cpp:112] Iteration 35780, lr = 0.1
I0524 04:50:53.278394 11835 solver.cpp:239] Iteration 35790 (1.52237 iter/s, 6.56871s/10 iters), loss = 5.5467
I0524 04:50:53.278434 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.5467 (* 1 = 5.5467 loss)
I0524 04:50:53.278525 11835 sgd_solver.cpp:112] Iteration 35790, lr = 0.1
I0524 04:50:59.948096 11835 solver.cpp:239] Iteration 35800 (1.49939 iter/s, 6.66939s/10 iters), loss = 6.34624
I0524 04:50:59.948153 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34624 (* 1 = 6.34624 loss)
I0524 04:50:59.948529 11835 sgd_solver.cpp:112] Iteration 35800, lr = 0.1
I0524 04:51:05.611142 11835 solver.cpp:239] Iteration 35810 (1.76592 iter/s, 5.66278s/10 iters), loss = 6.72806
I0524 04:51:05.611238 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72806 (* 1 = 6.72806 loss)
I0524 04:51:05.611251 11835 sgd_solver.cpp:112] Iteration 35810, lr = 0.1
I0524 04:51:15.308526 11835 solver.cpp:239] Iteration 35820 (1.03126 iter/s, 9.69689s/10 iters), loss = 7.09032
I0524 04:51:15.308575 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09032 (* 1 = 7.09032 loss)
I0524 04:51:15.308590 11835 sgd_solver.cpp:112] Iteration 35820, lr = 0.1
I0524 04:51:21.088593 11835 solver.cpp:239] Iteration 35830 (1.73049 iter/s, 5.7787s/10 iters), loss = 6.80387
I0524 04:51:21.088630 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80387 (* 1 = 6.80387 loss)
I0524 04:51:21.088642 11835 sgd_solver.cpp:112] Iteration 35830, lr = 0.1
I0524 04:51:29.463548 11835 solver.cpp:239] Iteration 35840 (1.19409 iter/s, 8.37459s/10 iters), loss = 6.56285
I0524 04:51:29.463598 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56285 (* 1 = 6.56285 loss)
I0524 04:51:29.463613 11835 sgd_solver.cpp:112] Iteration 35840, lr = 0.1
I0524 04:51:35.903955 11835 solver.cpp:239] Iteration 35850 (1.55292 iter/s, 6.43949s/10 iters), loss = 6.81814
I0524 04:51:35.904248 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81814 (* 1 = 6.81814 loss)
I0524 04:51:35.904304 11835 sgd_solver.cpp:112] Iteration 35850, lr = 0.1
I0524 04:51:43.293205 11835 solver.cpp:239] Iteration 35860 (1.35351 iter/s, 7.38822s/10 iters), loss = 6.52242
I0524 04:51:43.293265 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52242 (* 1 = 6.52242 loss)
I0524 04:51:43.293457 11835 sgd_solver.cpp:112] Iteration 35860, lr = 0.1
I0524 04:51:49.014147 11835 solver.cpp:239] Iteration 35870 (1.74805 iter/s, 5.72067s/10 iters), loss = 5.88242
I0524 04:51:49.014199 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88242 (* 1 = 5.88242 loss)
I0524 04:51:49.014212 11835 sgd_solver.cpp:112] Iteration 35870, lr = 0.1
I0524 04:51:56.191876 11835 solver.cpp:239] Iteration 35880 (1.39326 iter/s, 7.17741s/10 iters), loss = 5.93758
I0524 04:51:56.191917 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93758 (* 1 = 5.93758 loss)
I0524 04:51:57.230340 11835 sgd_solver.cpp:112] Iteration 35880, lr = 0.1
I0524 04:52:04.317359 11835 solver.cpp:239] Iteration 35890 (1.23075 iter/s, 8.12513s/10 iters), loss = 6.28717
I0524 04:52:04.317410 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.28717 (* 1 = 6.28717 loss)
I0524 04:52:05.474663 11835 sgd_solver.cpp:112] Iteration 35890, lr = 0.1
I0524 04:52:11.294755 11835 solver.cpp:239] Iteration 35900 (1.43326 iter/s, 6.97708s/10 iters), loss = 5.63891
I0524 04:52:11.295037 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63891 (* 1 = 5.63891 loss)
I0524 04:52:11.295095 11835 sgd_solver.cpp:112] Iteration 35900, lr = 0.1
I0524 04:52:17.428581 11835 solver.cpp:239] Iteration 35910 (1.63056 iter/s, 6.13287s/10 iters), loss = 6.14603
I0524 04:52:17.428633 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14603 (* 1 = 6.14603 loss)
I0524 04:52:17.428884 11835 sgd_solver.cpp:112] Iteration 35910, lr = 0.1
I0524 04:52:23.782820 11835 solver.cpp:239] Iteration 35920 (1.57383 iter/s, 6.35394s/10 iters), loss = 6.58006
I0524 04:52:23.782866 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58006 (* 1 = 6.58006 loss)
I0524 04:52:23.782933 11835 sgd_solver.cpp:112] Iteration 35920, lr = 0.1
I0524 04:52:31.455112 11835 solver.cpp:239] Iteration 35930 (1.30345 iter/s, 7.67194s/10 iters), loss = 6.04583
I0524 04:52:31.455169 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04583 (* 1 = 6.04583 loss)
I0524 04:52:31.455265 11835 sgd_solver.cpp:112] Iteration 35930, lr = 0.1
I0524 04:52:37.012770 11835 solver.cpp:239] Iteration 35940 (1.79941 iter/s, 5.55739s/10 iters), loss = 6.18244
I0524 04:52:37.012816 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18244 (* 1 = 6.18244 loss)
I0524 04:52:37.012830 11835 sgd_solver.cpp:112] Iteration 35940, lr = 0.1
I0524 04:52:42.558627 11835 solver.cpp:239] Iteration 35950 (1.80323 iter/s, 5.5456s/10 iters), loss = 5.97888
I0524 04:52:42.558758 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97888 (* 1 = 5.97888 loss)
I0524 04:52:42.558773 11835 sgd_solver.cpp:112] Iteration 35950, lr = 0.1
I0524 04:52:48.722841 11835 solver.cpp:239] Iteration 35960 (1.62253 iter/s, 6.16321s/10 iters), loss = 5.67477
I0524 04:52:48.722889 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67477 (* 1 = 5.67477 loss)
I0524 04:52:48.729176 11835 sgd_solver.cpp:112] Iteration 35960, lr = 0.1
I0524 04:52:55.122413 11835 solver.cpp:239] Iteration 35970 (1.56268 iter/s, 6.39928s/10 iters), loss = 6.4549
I0524 04:52:55.122467 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4549 (* 1 = 6.4549 loss)
I0524 04:52:55.591915 11835 sgd_solver.cpp:112] Iteration 35970, lr = 0.1
I0524 04:53:01.270805 11835 solver.cpp:239] Iteration 35980 (1.62652 iter/s, 6.14811s/10 iters), loss = 6.18504
I0524 04:53:01.270846 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18504 (* 1 = 6.18504 loss)
I0524 04:53:01.270859 11835 sgd_solver.cpp:112] Iteration 35980, lr = 0.1
I0524 04:53:07.399021 11835 solver.cpp:239] Iteration 35990 (1.63246 iter/s, 6.12571s/10 iters), loss = 7.38431
I0524 04:53:07.399080 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.38431 (* 1 = 7.38431 loss)
I0524 04:53:07.399111 11835 sgd_solver.cpp:112] Iteration 35990, lr = 0.1
I0524 04:53:12.957792 11835 solver.cpp:239] Iteration 36000 (1.79904 iter/s, 5.55851s/10 iters), loss = 6.42141
I0524 04:53:12.957947 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42141 (* 1 = 6.42141 loss)
I0524 04:53:12.958032 11835 sgd_solver.cpp:112] Iteration 36000, lr = 0.1
I0524 04:53:19.703346 11835 solver.cpp:239] Iteration 36010 (1.48255 iter/s, 6.74513s/10 iters), loss = 7.14335
I0524 04:53:19.703398 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.14335 (* 1 = 7.14335 loss)
I0524 04:53:19.703414 11835 sgd_solver.cpp:112] Iteration 36010, lr = 0.1
I0524 04:53:25.596570 11835 solver.cpp:239] Iteration 36020 (1.69712 iter/s, 5.89233s/10 iters), loss = 5.79338
I0524 04:53:25.596618 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79338 (* 1 = 5.79338 loss)
I0524 04:53:26.825634 11835 sgd_solver.cpp:112] Iteration 36020, lr = 0.1
I0524 04:53:32.535308 11835 solver.cpp:239] Iteration 36030 (1.44125 iter/s, 6.93843s/10 iters), loss = 6.70842
I0524 04:53:32.535351 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70842 (* 1 = 6.70842 loss)
I0524 04:53:32.535363 11835 sgd_solver.cpp:112] Iteration 36030, lr = 0.1
I0524 04:53:38.851110 11835 solver.cpp:239] Iteration 36040 (1.58343 iter/s, 6.31541s/10 iters), loss = 6.31906
I0524 04:53:38.851166 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31906 (* 1 = 6.31906 loss)
I0524 04:53:38.851337 11835 sgd_solver.cpp:112] Iteration 36040, lr = 0.1
I0524 04:53:44.590497 11835 solver.cpp:239] Iteration 36050 (1.74243 iter/s, 5.73912s/10 iters), loss = 6.15016
I0524 04:53:44.590752 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15016 (* 1 = 6.15016 loss)
I0524 04:53:44.590808 11835 sgd_solver.cpp:112] Iteration 36050, lr = 0.1
I0524 04:53:51.280059 11835 solver.cpp:239] Iteration 36060 (1.49521 iter/s, 6.68801s/10 iters), loss = 6.2392
I0524 04:53:51.280112 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2392 (* 1 = 6.2392 loss)
I0524 04:53:51.280308 11835 sgd_solver.cpp:112] Iteration 36060, lr = 0.1
I0524 04:53:56.997499 11835 solver.cpp:239] Iteration 36070 (1.74911 iter/s, 5.71718s/10 iters), loss = 6.90694
I0524 04:53:56.997539 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90694 (* 1 = 6.90694 loss)
I0524 04:53:56.997550 11835 sgd_solver.cpp:112] Iteration 36070, lr = 0.1
I0524 04:54:03.847187 11835 solver.cpp:239] Iteration 36080 (1.45999 iter/s, 6.84938s/10 iters), loss = 6.55566
I0524 04:54:03.847232 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55566 (* 1 = 6.55566 loss)
I0524 04:54:03.847326 11835 sgd_solver.cpp:112] Iteration 36080, lr = 0.1
I0524 04:54:10.990957 11835 solver.cpp:239] Iteration 36090 (1.39988 iter/s, 7.14345s/10 iters), loss = 6.66219
I0524 04:54:10.991011 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66219 (* 1 = 6.66219 loss)
I0524 04:54:11.231148 11835 sgd_solver.cpp:112] Iteration 36090, lr = 0.1
I0524 04:54:18.146595 11835 solver.cpp:239] Iteration 36100 (1.39756 iter/s, 7.15532s/10 iters), loss = 6.63773
I0524 04:54:18.146814 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63773 (* 1 = 6.63773 loss)
I0524 04:54:18.146940 11835 sgd_solver.cpp:112] Iteration 36100, lr = 0.1
I0524 04:54:23.695662 11835 solver.cpp:239] Iteration 36110 (1.80224 iter/s, 5.54864s/10 iters), loss = 6.39475
I0524 04:54:23.695708 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39475 (* 1 = 6.39475 loss)
I0524 04:54:23.695721 11835 sgd_solver.cpp:112] Iteration 36110, lr = 0.1
I0524 04:54:32.578032 11835 solver.cpp:239] Iteration 36120 (1.1261 iter/s, 8.88023s/10 iters), loss = 5.99096
I0524 04:54:32.578083 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99096 (* 1 = 5.99096 loss)
I0524 04:54:32.764451 11835 sgd_solver.cpp:112] Iteration 36120, lr = 0.1
I0524 04:54:38.949189 11835 solver.cpp:239] Iteration 36130 (1.56965 iter/s, 6.37086s/10 iters), loss = 5.6645
I0524 04:54:38.949246 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.6645 (* 1 = 5.6645 loss)
I0524 04:54:38.949390 11835 sgd_solver.cpp:112] Iteration 36130, lr = 0.1
I0524 04:54:44.678750 11835 solver.cpp:239] Iteration 36140 (1.74542 iter/s, 5.72929s/10 iters), loss = 5.55151
I0524 04:54:44.678797 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.55151 (* 1 = 5.55151 loss)
I0524 04:54:44.679042 11835 sgd_solver.cpp:112] Iteration 36140, lr = 0.1
I0524 04:54:52.881124 11835 solver.cpp:239] Iteration 36150 (1.21921 iter/s, 8.20201s/10 iters), loss = 7.23918
I0524 04:54:52.881341 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.23918 (* 1 = 7.23918 loss)
I0524 04:54:52.881397 11835 sgd_solver.cpp:112] Iteration 36150, lr = 0.1
I0524 04:54:58.510000 11835 solver.cpp:239] Iteration 36160 (1.77698 iter/s, 5.62751s/10 iters), loss = 4.84078
I0524 04:54:58.510040 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.84078 (* 1 = 4.84078 loss)
I0524 04:54:58.510052 11835 sgd_solver.cpp:112] Iteration 36160, lr = 0.1
I0524 04:55:05.929643 11835 solver.cpp:239] Iteration 36170 (1.34783 iter/s, 7.41931s/10 iters), loss = 7.02459
I0524 04:55:05.929692 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.02459 (* 1 = 7.02459 loss)
I0524 04:55:05.929834 11835 sgd_solver.cpp:112] Iteration 36170, lr = 0.1
I0524 04:55:12.933931 11835 solver.cpp:239] Iteration 36180 (1.42776 iter/s, 7.00398s/10 iters), loss = 6.88218
I0524 04:55:12.933972 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88218 (* 1 = 6.88218 loss)
I0524 04:55:12.933985 11835 sgd_solver.cpp:112] Iteration 36180, lr = 0.1
I0524 04:55:20.840704 11835 solver.cpp:239] Iteration 36190 (1.26482 iter/s, 7.90627s/10 iters), loss = 6.1043
I0524 04:55:20.840757 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1043 (* 1 = 6.1043 loss)
I0524 04:55:20.840772 11835 sgd_solver.cpp:112] Iteration 36190, lr = 0.1
I0524 04:55:27.315616 11835 solver.cpp:239] Iteration 36200 (1.54449 iter/s, 6.47462s/10 iters), loss = 5.76152
I0524 04:55:27.315742 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.76152 (* 1 = 5.76152 loss)
I0524 04:55:27.315757 11835 sgd_solver.cpp:112] Iteration 36200, lr = 0.1
I0524 04:55:36.302470 11835 solver.cpp:239] Iteration 36210 (1.11292 iter/s, 8.98534s/10 iters), loss = 6.5373
I0524 04:55:36.302529 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5373 (* 1 = 6.5373 loss)
I0524 04:55:36.302789 11835 sgd_solver.cpp:112] Iteration 36210, lr = 0.1
I0524 04:55:43.534595 11835 solver.cpp:239] Iteration 36220 (1.38278 iter/s, 7.2318s/10 iters), loss = 6.89115
I0524 04:55:43.534644 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89115 (* 1 = 6.89115 loss)
I0524 04:55:43.534859 11835 sgd_solver.cpp:112] Iteration 36220, lr = 0.1
I0524 04:55:53.535190 11835 solver.cpp:239] Iteration 36230 (0.999983 iter/s, 10.0002s/10 iters), loss = 5.87778
I0524 04:55:53.535240 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87778 (* 1 = 5.87778 loss)
I0524 04:55:53.535472 11835 sgd_solver.cpp:112] Iteration 36230, lr = 0.1
I0524 04:55:59.956483 11835 solver.cpp:239] Iteration 36240 (1.55739 iter/s, 6.421s/10 iters), loss = 6.79882
I0524 04:55:59.956799 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79882 (* 1 = 6.79882 loss)
I0524 04:56:00.048928 11835 sgd_solver.cpp:112] Iteration 36240, lr = 0.1
I0524 04:56:06.623373 11835 solver.cpp:239] Iteration 36250 (1.50007 iter/s, 6.66634s/10 iters), loss = 6.08183
I0524 04:56:06.623427 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08183 (* 1 = 6.08183 loss)
I0524 04:56:06.623507 11835 sgd_solver.cpp:112] Iteration 36250, lr = 0.1
I0524 04:56:13.913813 11835 solver.cpp:239] Iteration 36260 (1.37173 iter/s, 7.29009s/10 iters), loss = 6.4997
I0524 04:56:13.913859 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4997 (* 1 = 6.4997 loss)
I0524 04:56:14.453671 11835 sgd_solver.cpp:112] Iteration 36260, lr = 0.1
I0524 04:56:21.933791 11835 solver.cpp:239] Iteration 36270 (1.24694 iter/s, 8.01964s/10 iters), loss = 6.48063
I0524 04:56:21.933831 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48063 (* 1 = 6.48063 loss)
I0524 04:56:21.934062 11835 sgd_solver.cpp:112] Iteration 36270, lr = 0.1
I0524 04:56:28.317783 11835 solver.cpp:239] Iteration 36280 (1.56649 iter/s, 6.38369s/10 iters), loss = 6.49817
I0524 04:56:28.317834 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49817 (* 1 = 6.49817 loss)
I0524 04:56:28.507740 11835 sgd_solver.cpp:112] Iteration 36280, lr = 0.1
I0524 04:56:35.844190 11835 solver.cpp:239] Iteration 36290 (1.32872 iter/s, 7.52606s/10 iters), loss = 6.38235
I0524 04:56:35.844425 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38235 (* 1 = 6.38235 loss)
I0524 04:56:35.844470 11835 sgd_solver.cpp:112] Iteration 36290, lr = 0.1
I0524 04:56:41.810746 11835 solver.cpp:239] Iteration 36300 (1.67613 iter/s, 5.96614s/10 iters), loss = 6.16738
I0524 04:56:41.810784 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16738 (* 1 = 6.16738 loss)
I0524 04:56:41.810803 11835 sgd_solver.cpp:112] Iteration 36300, lr = 0.1
I0524 04:56:48.008178 11835 solver.cpp:239] Iteration 36310 (1.61365 iter/s, 6.19714s/10 iters), loss = 5.19082
I0524 04:56:48.008234 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.19082 (* 1 = 5.19082 loss)
I0524 04:56:48.029925 11835 sgd_solver.cpp:112] Iteration 36310, lr = 0.1
I0524 04:56:55.183763 11835 solver.cpp:239] Iteration 36320 (1.39368 iter/s, 7.17525s/10 iters), loss = 6.81063
I0524 04:56:55.183822 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81063 (* 1 = 6.81063 loss)
I0524 04:56:55.184125 11835 sgd_solver.cpp:112] Iteration 36320, lr = 0.1
I0524 04:57:01.141261 11835 solver.cpp:239] Iteration 36330 (1.67864 iter/s, 5.95722s/10 iters), loss = 6.90634
I0524 04:57:01.141301 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90634 (* 1 = 6.90634 loss)
I0524 04:57:01.183903 11835 sgd_solver.cpp:112] Iteration 36330, lr = 0.1
I0524 04:57:06.999018 11835 solver.cpp:239] Iteration 36340 (1.70722 iter/s, 5.85747s/10 iters), loss = 5.85464
I0524 04:57:06.999168 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85464 (* 1 = 5.85464 loss)
I0524 04:57:06.999270 11835 sgd_solver.cpp:112] Iteration 36340, lr = 0.1
I0524 04:57:13.007974 11835 solver.cpp:239] Iteration 36350 (1.66428 iter/s, 6.00859s/10 iters), loss = 7.04653
I0524 04:57:13.008014 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04653 (* 1 = 7.04653 loss)
I0524 04:57:13.486399 11835 sgd_solver.cpp:112] Iteration 36350, lr = 0.1
I0524 04:57:21.586454 11835 solver.cpp:239] Iteration 36360 (1.16576 iter/s, 8.5781s/10 iters), loss = 5.54726
I0524 04:57:21.586516 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.54726 (* 1 = 5.54726 loss)
I0524 04:57:21.586720 11835 sgd_solver.cpp:112] Iteration 36360, lr = 0.1
I0524 04:57:29.264300 11835 solver.cpp:239] Iteration 36370 (1.30251 iter/s, 7.67749s/10 iters), loss = 6.71072
I0524 04:57:29.264358 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71072 (* 1 = 6.71072 loss)
I0524 04:57:29.264590 11835 sgd_solver.cpp:112] Iteration 36370, lr = 0.1
I0524 04:57:37.467293 11835 solver.cpp:239] Iteration 36380 (1.21912 iter/s, 8.20262s/10 iters), loss = 6.06509
I0524 04:57:37.467574 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06509 (* 1 = 6.06509 loss)
I0524 04:57:37.467625 11835 sgd_solver.cpp:112] Iteration 36380, lr = 0.1
I0524 04:57:43.891852 11835 solver.cpp:239] Iteration 36390 (1.55664 iter/s, 6.42409s/10 iters), loss = 6.07022
I0524 04:57:43.891908 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07022 (* 1 = 6.07022 loss)
I0524 04:57:43.892033 11835 sgd_solver.cpp:112] Iteration 36390, lr = 0.1
I0524 04:57:50.845504 11835 solver.cpp:239] Iteration 36400 (1.43816 iter/s, 6.95332s/10 iters), loss = 5.6545
I0524 04:57:50.845558 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.6545 (* 1 = 5.6545 loss)
I0524 04:57:50.845818 11835 sgd_solver.cpp:112] Iteration 36400, lr = 0.1
I0524 04:57:57.775673 11835 solver.cpp:239] Iteration 36410 (1.44303 iter/s, 6.92986s/10 iters), loss = 6.84202
I0524 04:57:57.775712 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84202 (* 1 = 6.84202 loss)
I0524 04:57:57.824564 11835 sgd_solver.cpp:112] Iteration 36410, lr = 0.1
I0524 04:58:05.477555 11835 solver.cpp:239] Iteration 36420 (1.29844 iter/s, 7.70154s/10 iters), loss = 6.24786
I0524 04:58:05.477605 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24786 (* 1 = 6.24786 loss)
I0524 04:58:05.477949 11835 sgd_solver.cpp:112] Iteration 36420, lr = 0.1
I0524 04:58:11.160684 11835 solver.cpp:239] Iteration 36430 (1.75967 iter/s, 5.68287s/10 iters), loss = 6.64465
I0524 04:58:11.160784 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64465 (* 1 = 6.64465 loss)
I0524 04:58:11.160800 11835 sgd_solver.cpp:112] Iteration 36430, lr = 0.1
I0524 04:58:18.558169 11835 solver.cpp:239] Iteration 36440 (1.35189 iter/s, 7.39706s/10 iters), loss = 5.82217
I0524 04:58:18.558223 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82217 (* 1 = 5.82217 loss)
I0524 04:58:18.585811 11835 sgd_solver.cpp:112] Iteration 36440, lr = 0.1
I0524 04:58:26.633941 11835 solver.cpp:239] Iteration 36450 (1.23833 iter/s, 8.0754s/10 iters), loss = 5.80655
I0524 04:58:26.633997 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80655 (* 1 = 5.80655 loss)
I0524 04:58:26.634234 11835 sgd_solver.cpp:112] Iteration 36450, lr = 0.1
I0524 04:58:32.735482 11835 solver.cpp:239] Iteration 36460 (1.63901 iter/s, 6.10126s/10 iters), loss = 5.99147
I0524 04:58:32.735525 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99147 (* 1 = 5.99147 loss)
I0524 04:58:32.735538 11835 sgd_solver.cpp:112] Iteration 36460, lr = 0.1
I0524 04:58:38.732983 11835 solver.cpp:239] Iteration 36470 (1.66805 iter/s, 5.99503s/10 iters), loss = 6.85793
I0524 04:58:38.733031 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.85793 (* 1 = 6.85793 loss)
I0524 04:58:38.901988 11835 sgd_solver.cpp:112] Iteration 36470, lr = 0.1
I0524 04:58:45.701223 11835 solver.cpp:239] Iteration 36480 (1.43515 iter/s, 6.96793s/10 iters), loss = 5.73873
I0524 04:58:45.701434 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73873 (* 1 = 5.73873 loss)
I0524 04:58:45.980829 11835 sgd_solver.cpp:112] Iteration 36480, lr = 0.1
I0524 04:58:53.698235 11835 solver.cpp:239] Iteration 36490 (1.25054 iter/s, 7.99652s/10 iters), loss = 6.16013
I0524 04:58:53.698283 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16013 (* 1 = 6.16013 loss)
I0524 04:58:55.138226 11835 sgd_solver.cpp:112] Iteration 36490, lr = 0.1
I0524 04:59:03.357345 11835 solver.cpp:239] Iteration 36500 (1.03534 iter/s, 9.65869s/10 iters), loss = 6.76947
I0524 04:59:03.357398 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76947 (* 1 = 6.76947 loss)
I0524 04:59:03.357589 11835 sgd_solver.cpp:112] Iteration 36500, lr = 0.1
I0524 04:59:09.586771 11835 solver.cpp:239] Iteration 36510 (1.60536 iter/s, 6.22914s/10 iters), loss = 5.78126
I0524 04:59:09.586810 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78126 (* 1 = 5.78126 loss)
I0524 04:59:09.586822 11835 sgd_solver.cpp:112] Iteration 36510, lr = 0.1
I0524 04:59:15.457780 11835 solver.cpp:239] Iteration 36520 (1.70369 iter/s, 5.86961s/10 iters), loss = 5.6567
I0524 04:59:15.457831 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.6567 (* 1 = 5.6567 loss)
I0524 04:59:15.457846 11835 sgd_solver.cpp:112] Iteration 36520, lr = 0.1
I0524 04:59:22.955668 11835 solver.cpp:239] Iteration 36530 (1.33378 iter/s, 7.49748s/10 iters), loss = 6.26425
I0524 04:59:22.955868 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26425 (* 1 = 6.26425 loss)
I0524 04:59:22.955940 11835 sgd_solver.cpp:112] Iteration 36530, lr = 0.1
I0524 04:59:30.393162 11835 solver.cpp:239] Iteration 36540 (1.34463 iter/s, 7.43701s/10 iters), loss = 5.67492
I0524 04:59:30.393213 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67492 (* 1 = 5.67492 loss)
I0524 04:59:31.732481 11835 sgd_solver.cpp:112] Iteration 36540, lr = 0.1
I0524 04:59:39.957765 11835 solver.cpp:239] Iteration 36550 (1.04557 iter/s, 9.56419s/10 iters), loss = 5.44945
I0524 04:59:39.957815 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.44945 (* 1 = 5.44945 loss)
I0524 04:59:39.958420 11835 sgd_solver.cpp:112] Iteration 36550, lr = 0.1
I0524 04:59:47.339296 11835 solver.cpp:239] Iteration 36560 (1.35479 iter/s, 7.38119s/10 iters), loss = 5.86216
I0524 04:59:47.339349 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86216 (* 1 = 5.86216 loss)
I0524 04:59:47.339588 11835 sgd_solver.cpp:112] Iteration 36560, lr = 0.1
I0524 04:59:53.123749 11835 solver.cpp:239] Iteration 36570 (1.72886 iter/s, 5.78417s/10 iters), loss = 6.15208
I0524 04:59:53.124029 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15208 (* 1 = 6.15208 loss)
I0524 04:59:53.124086 11835 sgd_solver.cpp:112] Iteration 36570, lr = 0.1
I0524 04:59:59.022002 11835 solver.cpp:239] Iteration 36580 (1.69581 iter/s, 5.89687s/10 iters), loss = 6.46753
I0524 04:59:59.022054 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46753 (* 1 = 6.46753 loss)
I0524 04:59:59.022281 11835 sgd_solver.cpp:112] Iteration 36580, lr = 0.1
I0524 05:00:04.565327 11835 solver.cpp:239] Iteration 36590 (1.80406 iter/s, 5.54307s/10 iters), loss = 5.73985
I0524 05:00:04.565372 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73985 (* 1 = 5.73985 loss)
I0524 05:00:04.565497 11835 sgd_solver.cpp:112] Iteration 36590, lr = 0.1
I0524 05:00:10.515379 11835 solver.cpp:239] Iteration 36600 (1.68074 iter/s, 5.94977s/10 iters), loss = 6.03286
I0524 05:00:10.515435 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03286 (* 1 = 6.03286 loss)
I0524 05:00:10.515592 11835 sgd_solver.cpp:112] Iteration 36600, lr = 0.1
I0524 05:00:16.236570 11835 solver.cpp:239] Iteration 36610 (1.74797 iter/s, 5.72092s/10 iters), loss = 5.85828
I0524 05:00:16.236610 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85828 (* 1 = 5.85828 loss)
I0524 05:00:16.236623 11835 sgd_solver.cpp:112] Iteration 36610, lr = 0.1
I0524 05:00:22.411768 11835 solver.cpp:239] Iteration 36620 (1.61946 iter/s, 6.17491s/10 iters), loss = 6.11486
I0524 05:00:22.411825 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11486 (* 1 = 6.11486 loss)
I0524 05:00:22.412024 11835 sgd_solver.cpp:112] Iteration 36620, lr = 0.1
I0524 05:00:29.914858 11835 solver.cpp:239] Iteration 36630 (1.33284 iter/s, 7.50275s/10 iters), loss = 5.79641
I0524 05:00:29.915063 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79641 (* 1 = 5.79641 loss)
I0524 05:00:30.660820 11835 sgd_solver.cpp:112] Iteration 36630, lr = 0.1
I0524 05:00:37.675719 11835 solver.cpp:239] Iteration 36640 (1.28859 iter/s, 7.76041s/10 iters), loss = 6.96392
I0524 05:00:37.675763 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96392 (* 1 = 6.96392 loss)
I0524 05:00:37.676028 11835 sgd_solver.cpp:112] Iteration 36640, lr = 0.1
I0524 05:00:44.409525 11835 solver.cpp:239] Iteration 36650 (1.48511 iter/s, 6.7335s/10 iters), loss = 7.07305
I0524 05:00:44.409574 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.07305 (* 1 = 7.07305 loss)
I0524 05:00:44.569895 11835 sgd_solver.cpp:112] Iteration 36650, lr = 0.1
I0524 05:00:50.185662 11835 solver.cpp:239] Iteration 36660 (1.73134 iter/s, 5.77588s/10 iters), loss = 6.21438
I0524 05:00:50.185703 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21438 (* 1 = 6.21438 loss)
I0524 05:00:50.199662 11835 sgd_solver.cpp:112] Iteration 36660, lr = 0.1
I0524 05:00:56.382458 11835 solver.cpp:239] Iteration 36670 (1.61381 iter/s, 6.19651s/10 iters), loss = 5.51523
I0524 05:00:56.382506 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.51523 (* 1 = 5.51523 loss)
I0524 05:00:56.382520 11835 sgd_solver.cpp:112] Iteration 36670, lr = 0.1
I0524 05:01:05.369391 11835 solver.cpp:239] Iteration 36680 (1.11278 iter/s, 8.98648s/10 iters), loss = 6.06403
I0524 05:01:05.369684 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06403 (* 1 = 6.06403 loss)
I0524 05:01:05.637512 11835 sgd_solver.cpp:112] Iteration 36680, lr = 0.1
I0524 05:01:12.920361 11835 solver.cpp:239] Iteration 36690 (1.32443 iter/s, 7.55041s/10 iters), loss = 7.30726
I0524 05:01:12.920414 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.30726 (* 1 = 7.30726 loss)
I0524 05:01:12.920662 11835 sgd_solver.cpp:112] Iteration 36690, lr = 0.1
I0524 05:01:18.896697 11835 solver.cpp:239] Iteration 36700 (1.67335 iter/s, 5.97604s/10 iters), loss = 6.3461
I0524 05:01:18.896752 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3461 (* 1 = 6.3461 loss)
I0524 05:01:18.896890 11835 sgd_solver.cpp:112] Iteration 36700, lr = 0.1
I0524 05:01:25.617362 11835 solver.cpp:239] Iteration 36710 (1.48802 iter/s, 6.72035s/10 iters), loss = 6.09708
I0524 05:01:25.617437 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09708 (* 1 = 6.09708 loss)
I0524 05:01:25.617460 11835 sgd_solver.cpp:112] Iteration 36710, lr = 0.1
I0524 05:01:31.339515 11835 solver.cpp:239] Iteration 36720 (1.74775 iter/s, 5.72166s/10 iters), loss = 5.74736
I0524 05:01:31.339550 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74736 (* 1 = 5.74736 loss)
I0524 05:01:31.339565 11835 sgd_solver.cpp:112] Iteration 36720, lr = 0.1
I0524 05:01:38.478685 11835 solver.cpp:239] Iteration 36730 (1.40079 iter/s, 7.13885s/10 iters), loss = 5.44925
I0524 05:01:38.478829 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.44925 (* 1 = 5.44925 loss)
I0524 05:01:38.478899 11835 sgd_solver.cpp:112] Iteration 36730, lr = 0.1
I0524 05:01:44.239697 11835 solver.cpp:239] Iteration 36740 (1.73592 iter/s, 5.76065s/10 iters), loss = 6.30792
I0524 05:01:44.239755 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30792 (* 1 = 6.30792 loss)
I0524 05:01:44.239961 11835 sgd_solver.cpp:112] Iteration 36740, lr = 0.1
I0524 05:01:50.456704 11835 solver.cpp:239] Iteration 36750 (1.60857 iter/s, 6.21672s/10 iters), loss = 7.08812
I0524 05:01:50.456743 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.08812 (* 1 = 7.08812 loss)
I0524 05:01:50.456874 11835 sgd_solver.cpp:112] Iteration 36750, lr = 0.1
I0524 05:01:59.000622 11835 solver.cpp:239] Iteration 36760 (1.17048 iter/s, 8.54354s/10 iters), loss = 5.20166
I0524 05:01:59.000690 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.20166 (* 1 = 5.20166 loss)
I0524 05:01:59.000897 11835 sgd_solver.cpp:112] Iteration 36760, lr = 0.1
I0524 05:02:04.919579 11835 solver.cpp:239] Iteration 36770 (1.68957 iter/s, 5.91865s/10 iters), loss = 6.89377
I0524 05:02:04.919639 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89377 (* 1 = 6.89377 loss)
I0524 05:02:04.919677 11835 sgd_solver.cpp:112] Iteration 36770, lr = 0.1
I0524 05:02:10.711472 11835 solver.cpp:239] Iteration 36780 (1.72663 iter/s, 5.79162s/10 iters), loss = 6.86764
I0524 05:02:10.711779 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86764 (* 1 = 6.86764 loss)
I0524 05:02:10.713482 11835 sgd_solver.cpp:112] Iteration 36780, lr = 0.1
I0524 05:02:17.048410 11835 solver.cpp:239] Iteration 36790 (1.57818 iter/s, 6.33642s/10 iters), loss = 5.99106
I0524 05:02:17.048461 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99106 (* 1 = 5.99106 loss)
I0524 05:02:17.048476 11835 sgd_solver.cpp:112] Iteration 36790, lr = 0.1
I0524 05:02:24.349479 11835 solver.cpp:239] Iteration 36800 (1.36972 iter/s, 7.30075s/10 iters), loss = 6.16541
I0524 05:02:24.349520 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16541 (* 1 = 6.16541 loss)
I0524 05:02:24.526659 11835 sgd_solver.cpp:112] Iteration 36800, lr = 0.1
I0524 05:02:30.486563 11835 solver.cpp:239] Iteration 36810 (1.62951 iter/s, 6.1368s/10 iters), loss = 6.40206
I0524 05:02:30.486608 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40206 (* 1 = 6.40206 loss)
I0524 05:02:31.030267 11835 sgd_solver.cpp:112] Iteration 36810, lr = 0.1
I0524 05:02:36.850545 11835 solver.cpp:239] Iteration 36820 (1.57141 iter/s, 6.3637s/10 iters), loss = 6.27422
I0524 05:02:36.850591 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27422 (* 1 = 6.27422 loss)
I0524 05:02:36.850672 11835 sgd_solver.cpp:112] Iteration 36820, lr = 0.1
I0524 05:02:42.770737 11835 solver.cpp:239] Iteration 36830 (1.68922 iter/s, 5.91991s/10 iters), loss = 5.5917
I0524 05:02:42.771011 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.5917 (* 1 = 5.5917 loss)
I0524 05:02:42.771067 11835 sgd_solver.cpp:112] Iteration 36830, lr = 0.1
I0524 05:02:51.692865 11835 solver.cpp:239] Iteration 36840 (1.121 iter/s, 8.92061s/10 iters), loss = 6.93053
I0524 05:02:51.692914 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93053 (* 1 = 6.93053 loss)
I0524 05:02:51.693023 11835 sgd_solver.cpp:112] Iteration 36840, lr = 0.1
I0524 05:02:59.705072 11835 solver.cpp:239] Iteration 36850 (1.24815 iter/s, 8.01186s/10 iters), loss = 5.56077
I0524 05:02:59.705122 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56077 (* 1 = 5.56077 loss)
I0524 05:02:59.760818 11835 sgd_solver.cpp:112] Iteration 36850, lr = 0.1
I0524 05:03:07.694730 11835 solver.cpp:239] Iteration 36860 (1.25167 iter/s, 7.9893s/10 iters), loss = 5.39987
I0524 05:03:07.694784 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.39987 (* 1 = 5.39987 loss)
I0524 05:03:07.755456 11835 sgd_solver.cpp:112] Iteration 36860, lr = 0.1
I0524 05:03:14.034202 11835 solver.cpp:239] Iteration 36870 (1.57749 iter/s, 6.33917s/10 iters), loss = 6.425
I0524 05:03:14.034481 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.425 (* 1 = 6.425 loss)
I0524 05:03:14.034543 11835 sgd_solver.cpp:112] Iteration 36870, lr = 0.1
I0524 05:03:21.635380 11835 solver.cpp:239] Iteration 36880 (1.31573 iter/s, 7.60033s/10 iters), loss = 6.30516
I0524 05:03:21.635422 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30516 (* 1 = 6.30516 loss)
I0524 05:03:22.147238 11835 sgd_solver.cpp:112] Iteration 36880, lr = 0.1
I0524 05:03:29.264634 11835 solver.cpp:239] Iteration 36890 (1.3108 iter/s, 7.62891s/10 iters), loss = 6.79546
I0524 05:03:29.264683 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79546 (* 1 = 6.79546 loss)
I0524 05:03:29.747817 11835 sgd_solver.cpp:112] Iteration 36890, lr = 0.1
I0524 05:03:36.285862 11835 solver.cpp:239] Iteration 36900 (1.42432 iter/s, 7.0209s/10 iters), loss = 6.51514
I0524 05:03:36.285919 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51514 (* 1 = 6.51514 loss)
I0524 05:03:36.286062 11835 sgd_solver.cpp:112] Iteration 36900, lr = 0.1
I0524 05:03:44.688055 11835 solver.cpp:239] Iteration 36910 (1.19022 iter/s, 8.40181s/10 iters), loss = 5.95772
I0524 05:03:44.688273 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95772 (* 1 = 5.95772 loss)
I0524 05:03:44.799952 11835 sgd_solver.cpp:112] Iteration 36910, lr = 0.1
I0524 05:03:51.357101 11835 solver.cpp:239] Iteration 36920 (1.49957 iter/s, 6.66859s/10 iters), loss = 7.6006
I0524 05:03:51.357157 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.6006 (* 1 = 7.6006 loss)
I0524 05:03:51.357388 11835 sgd_solver.cpp:112] Iteration 36920, lr = 0.1
I0524 05:03:58.948738 11835 solver.cpp:239] Iteration 36930 (1.3173 iter/s, 7.59129s/10 iters), loss = 5.92733
I0524 05:03:58.948789 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92733 (* 1 = 5.92733 loss)
I0524 05:03:58.949162 11835 sgd_solver.cpp:112] Iteration 36930, lr = 0.1
I0524 05:04:06.944000 11835 solver.cpp:239] Iteration 36940 (1.2508 iter/s, 7.99491s/10 iters), loss = 6.30184
I0524 05:04:06.944039 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30184 (* 1 = 6.30184 loss)
I0524 05:04:06.944185 11835 sgd_solver.cpp:112] Iteration 36940, lr = 0.1
I0524 05:04:14.043210 11835 solver.cpp:239] Iteration 36950 (1.40867 iter/s, 7.09888s/10 iters), loss = 6.70985
I0524 05:04:14.043264 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70985 (* 1 = 6.70985 loss)
I0524 05:04:14.444080 11835 sgd_solver.cpp:112] Iteration 36950, lr = 0.1
I0524 05:04:23.724520 11835 solver.cpp:239] Iteration 36960 (1.03296 iter/s, 9.6809s/10 iters), loss = 5.85199
I0524 05:04:23.724776 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85199 (* 1 = 5.85199 loss)
I0524 05:04:23.724825 11835 sgd_solver.cpp:112] Iteration 36960, lr = 0.1
I0524 05:04:29.261389 11835 solver.cpp:239] Iteration 36970 (1.80652 iter/s, 5.53549s/10 iters), loss = 6.89691
I0524 05:04:29.261435 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89691 (* 1 = 6.89691 loss)
I0524 05:04:29.261448 11835 sgd_solver.cpp:112] Iteration 36970, lr = 0.1
I0524 05:04:37.097632 11835 solver.cpp:239] Iteration 36980 (1.27618 iter/s, 7.8359s/10 iters), loss = 6.4824
I0524 05:04:37.097681 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4824 (* 1 = 6.4824 loss)
I0524 05:04:37.097867 11835 sgd_solver.cpp:112] Iteration 36980, lr = 0.1
I0524 05:04:43.638731 11835 solver.cpp:239] Iteration 36990 (1.52887 iter/s, 6.54079s/10 iters), loss = 6.13445
I0524 05:04:43.638790 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13445 (* 1 = 6.13445 loss)
I0524 05:04:43.638922 11835 sgd_solver.cpp:112] Iteration 36990, lr = 0.1
I0524 05:04:49.880385 11835 solver.cpp:239] Iteration 37000 (1.60259 iter/s, 6.23988s/10 iters), loss = 6.9255
I0524 05:04:49.880431 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9255 (* 1 = 6.9255 loss)
I0524 05:04:49.880446 11835 sgd_solver.cpp:112] Iteration 37000, lr = 0.1
I0524 05:04:57.465381 11835 solver.cpp:239] Iteration 37010 (1.31861 iter/s, 7.58372s/10 iters), loss = 6.19995
I0524 05:04:57.465665 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19995 (* 1 = 6.19995 loss)
I0524 05:04:58.137616 11835 sgd_solver.cpp:112] Iteration 37010, lr = 0.1
I0524 05:05:04.907104 11835 solver.cpp:239] Iteration 37020 (1.34387 iter/s, 7.44122s/10 iters), loss = 6.16039
I0524 05:05:04.907145 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16039 (* 1 = 6.16039 loss)
I0524 05:05:04.907157 11835 sgd_solver.cpp:112] Iteration 37020, lr = 0.1
I0524 05:05:13.478261 11835 solver.cpp:239] Iteration 37030 (1.16705 iter/s, 8.56862s/10 iters), loss = 6.19451
I0524 05:05:13.478309 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19451 (* 1 = 6.19451 loss)
I0524 05:05:14.263290 11835 sgd_solver.cpp:112] Iteration 37030, lr = 0.1
I0524 05:05:21.249475 11835 solver.cpp:239] Iteration 37040 (1.28686 iter/s, 7.77086s/10 iters), loss = 7.61831
I0524 05:05:21.249534 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.61831 (* 1 = 7.61831 loss)
I0524 05:05:21.249763 11835 sgd_solver.cpp:112] Iteration 37040, lr = 0.1
I0524 05:05:26.858363 11835 solver.cpp:239] Iteration 37050 (1.78297 iter/s, 5.60862s/10 iters), loss = 5.57389
I0524 05:05:26.858399 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.57389 (* 1 = 5.57389 loss)
I0524 05:05:26.858410 11835 sgd_solver.cpp:112] Iteration 37050, lr = 0.1
I0524 05:05:34.222455 11835 solver.cpp:239] Iteration 37060 (1.358 iter/s, 7.36377s/10 iters), loss = 5.24646
I0524 05:05:34.222772 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.24646 (* 1 = 5.24646 loss)
I0524 05:05:34.618962 11835 sgd_solver.cpp:112] Iteration 37060, lr = 0.1
I0524 05:05:40.034121 11835 solver.cpp:239] Iteration 37070 (1.72082 iter/s, 5.81119s/10 iters), loss = 6.2069
I0524 05:05:40.034168 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2069 (* 1 = 6.2069 loss)
I0524 05:05:40.034180 11835 sgd_solver.cpp:112] Iteration 37070, lr = 0.1
I0524 05:05:47.575544 11835 solver.cpp:239] Iteration 37080 (1.32608 iter/s, 7.54103s/10 iters), loss = 5.62944
I0524 05:05:47.575583 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.62944 (* 1 = 5.62944 loss)
I0524 05:05:47.575721 11835 sgd_solver.cpp:112] Iteration 37080, lr = 0.1
I0524 05:05:54.766062 11835 solver.cpp:239] Iteration 37090 (1.39078 iter/s, 7.19019s/10 iters), loss = 5.82423
I0524 05:05:54.766134 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82423 (* 1 = 5.82423 loss)
I0524 05:05:54.766211 11835 sgd_solver.cpp:112] Iteration 37090, lr = 0.1
I0524 05:06:00.729684 11835 solver.cpp:239] Iteration 37100 (1.67692 iter/s, 5.96333s/10 iters), loss = 5.90124
I0524 05:06:00.729741 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90124 (* 1 = 5.90124 loss)
I0524 05:06:00.729835 11835 sgd_solver.cpp:112] Iteration 37100, lr = 0.1
I0524 05:06:08.213274 11835 solver.cpp:239] Iteration 37110 (1.33632 iter/s, 7.48325s/10 iters), loss = 6.67437
I0524 05:06:08.213402 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67437 (* 1 = 6.67437 loss)
I0524 05:06:08.213573 11835 sgd_solver.cpp:112] Iteration 37110, lr = 0.1
I0524 05:06:13.979094 11835 solver.cpp:239] Iteration 37120 (1.73446 iter/s, 5.76547s/10 iters), loss = 6.29498
I0524 05:06:13.979137 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29498 (* 1 = 6.29498 loss)
I0524 05:06:13.979627 11835 sgd_solver.cpp:112] Iteration 37120, lr = 0.1
I0524 05:06:20.018138 11835 solver.cpp:239] Iteration 37130 (1.65597 iter/s, 6.03875s/10 iters), loss = 7.23146
I0524 05:06:20.018195 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.23146 (* 1 = 7.23146 loss)
I0524 05:06:20.018313 11835 sgd_solver.cpp:112] Iteration 37130, lr = 0.1
I0524 05:06:27.061893 11835 solver.cpp:239] Iteration 37140 (1.41976 iter/s, 7.04344s/10 iters), loss = 6.9654
I0524 05:06:27.061930 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9654 (* 1 = 6.9654 loss)
I0524 05:06:27.062166 11835 sgd_solver.cpp:112] Iteration 37140, lr = 0.1
I0524 05:06:33.071060 11835 solver.cpp:239] Iteration 37150 (1.6642 iter/s, 6.00889s/10 iters), loss = 6.05083
I0524 05:06:33.071110 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05083 (* 1 = 6.05083 loss)
I0524 05:06:33.240298 11835 sgd_solver.cpp:112] Iteration 37150, lr = 0.1
I0524 05:06:40.132648 11835 solver.cpp:239] Iteration 37160 (1.41618 iter/s, 7.06126s/10 iters), loss = 6.36849
I0524 05:06:40.132915 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36849 (* 1 = 6.36849 loss)
I0524 05:06:40.755256 11835 sgd_solver.cpp:112] Iteration 37160, lr = 0.1
I0524 05:06:47.077215 11835 solver.cpp:239] Iteration 37170 (1.44007 iter/s, 6.94409s/10 iters), loss = 6.56273
I0524 05:06:47.077255 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56273 (* 1 = 6.56273 loss)
I0524 05:06:47.451867 11835 sgd_solver.cpp:112] Iteration 37170, lr = 0.1
I0524 05:06:54.282459 11835 solver.cpp:239] Iteration 37180 (1.38794 iter/s, 7.20492s/10 iters), loss = 5.81172
I0524 05:06:54.282508 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81172 (* 1 = 5.81172 loss)
I0524 05:06:54.282522 11835 sgd_solver.cpp:112] Iteration 37180, lr = 0.1
I0524 05:07:00.733997 11835 solver.cpp:239] Iteration 37190 (1.55036 iter/s, 6.45014s/10 iters), loss = 6.58715
I0524 05:07:00.734050 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58715 (* 1 = 6.58715 loss)
I0524 05:07:00.734655 11835 sgd_solver.cpp:112] Iteration 37190, lr = 0.1
I0524 05:07:06.201759 11835 solver.cpp:239] Iteration 37200 (1.82899 iter/s, 5.4675s/10 iters), loss = 6.48236
I0524 05:07:06.201795 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48236 (* 1 = 6.48236 loss)
I0524 05:07:06.201823 11835 sgd_solver.cpp:112] Iteration 37200, lr = 0.1
I0524 05:07:12.783670 11835 solver.cpp:239] Iteration 37210 (1.51938 iter/s, 6.58161s/10 iters), loss = 6.43736
I0524 05:07:12.783852 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43736 (* 1 = 6.43736 loss)
I0524 05:07:12.814388 11835 sgd_solver.cpp:112] Iteration 37210, lr = 0.1
I0524 05:07:18.662992 11835 solver.cpp:239] Iteration 37220 (1.70099 iter/s, 5.87892s/10 iters), loss = 6.39391
I0524 05:07:18.663031 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39391 (* 1 = 6.39391 loss)
I0524 05:07:18.663075 11835 sgd_solver.cpp:112] Iteration 37220, lr = 0.1
I0524 05:07:26.504047 11835 solver.cpp:239] Iteration 37230 (1.27539 iter/s, 7.84071s/10 iters), loss = 6.38715
I0524 05:07:26.504097 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38715 (* 1 = 6.38715 loss)
I0524 05:07:26.504212 11835 sgd_solver.cpp:112] Iteration 37230, lr = 0.1
I0524 05:07:32.313794 11835 solver.cpp:239] Iteration 37240 (1.72133 iter/s, 5.80948s/10 iters), loss = 6.23097
I0524 05:07:32.313839 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23097 (* 1 = 6.23097 loss)
I0524 05:07:32.313853 11835 sgd_solver.cpp:112] Iteration 37240, lr = 0.1
I0524 05:07:38.929798 11835 solver.cpp:239] Iteration 37250 (1.51155 iter/s, 6.61571s/10 iters), loss = 7.40596
I0524 05:07:38.929841 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.40596 (* 1 = 7.40596 loss)
I0524 05:07:38.929945 11835 sgd_solver.cpp:112] Iteration 37250, lr = 0.1
I0524 05:07:45.817709 11835 solver.cpp:239] Iteration 37260 (1.45188 iter/s, 6.8876s/10 iters), loss = 6.16978
I0524 05:07:45.817852 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16978 (* 1 = 6.16978 loss)
I0524 05:07:45.817867 11835 sgd_solver.cpp:112] Iteration 37260, lr = 0.1
I0524 05:07:51.526815 11835 solver.cpp:239] Iteration 37270 (1.75171 iter/s, 5.70872s/10 iters), loss = 5.50206
I0524 05:07:51.526860 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.50206 (* 1 = 5.50206 loss)
I0524 05:07:51.527017 11835 sgd_solver.cpp:112] Iteration 37270, lr = 0.1
I0524 05:07:57.981591 11835 solver.cpp:239] Iteration 37280 (1.54931 iter/s, 6.45447s/10 iters), loss = 6.5655
I0524 05:07:57.981649 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5655 (* 1 = 6.5655 loss)
I0524 05:07:57.981837 11835 sgd_solver.cpp:112] Iteration 37280, lr = 0.1
I0524 05:08:03.965442 11835 solver.cpp:239] Iteration 37290 (1.67124 iter/s, 5.98357s/10 iters), loss = 5.75823
I0524 05:08:03.965481 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75823 (* 1 = 5.75823 loss)
I0524 05:08:03.965492 11835 sgd_solver.cpp:112] Iteration 37290, lr = 0.1
I0524 05:08:10.465261 11835 solver.cpp:239] Iteration 37300 (1.53859 iter/s, 6.49945s/10 iters), loss = 5.45138
I0524 05:08:10.465313 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.45138 (* 1 = 5.45138 loss)
I0524 05:08:10.465488 11835 sgd_solver.cpp:112] Iteration 37300, lr = 0.1
I0524 05:08:17.736827 11835 solver.cpp:239] Iteration 37310 (1.37528 iter/s, 7.27123s/10 iters), loss = 6.74498
I0524 05:08:17.737011 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74498 (* 1 = 6.74498 loss)
I0524 05:08:17.928829 11835 sgd_solver.cpp:112] Iteration 37310, lr = 0.1
I0524 05:08:25.121131 11835 solver.cpp:239] Iteration 37320 (1.35431 iter/s, 7.38385s/10 iters), loss = 5.43521
I0524 05:08:25.121181 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43521 (* 1 = 5.43521 loss)
I0524 05:08:25.121404 11835 sgd_solver.cpp:112] Iteration 37320, lr = 0.1
I0524 05:08:30.955909 11835 solver.cpp:239] Iteration 37330 (1.71394 iter/s, 5.8345s/10 iters), loss = 6.10727
I0524 05:08:30.955955 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10727 (* 1 = 6.10727 loss)
I0524 05:08:30.956248 11835 sgd_solver.cpp:112] Iteration 37330, lr = 0.1
I0524 05:08:36.686856 11835 solver.cpp:239] Iteration 37340 (1.74499 iter/s, 5.73068s/10 iters), loss = 5.98058
I0524 05:08:36.686893 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98058 (* 1 = 5.98058 loss)
I0524 05:08:36.686921 11835 sgd_solver.cpp:112] Iteration 37340, lr = 0.1
I0524 05:08:44.918241 11835 solver.cpp:239] Iteration 37350 (1.21492 iter/s, 8.23103s/10 iters), loss = 7.00966
I0524 05:08:44.918298 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00966 (* 1 = 7.00966 loss)
I0524 05:08:44.918409 11835 sgd_solver.cpp:112] Iteration 37350, lr = 0.1
I0524 05:08:50.802541 11835 solver.cpp:239] Iteration 37360 (1.69952 iter/s, 5.88403s/10 iters), loss = 5.96456
I0524 05:08:50.802745 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96456 (* 1 = 5.96456 loss)
I0524 05:08:50.802764 11835 sgd_solver.cpp:112] Iteration 37360, lr = 0.1
I0524 05:08:57.439160 11835 solver.cpp:239] Iteration 37370 (1.50689 iter/s, 6.63617s/10 iters), loss = 5.98407
I0524 05:08:57.439201 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98407 (* 1 = 5.98407 loss)
I0524 05:08:57.604115 11835 sgd_solver.cpp:112] Iteration 37370, lr = 0.1
I0524 05:09:05.645146 11835 solver.cpp:239] Iteration 37380 (1.21867 iter/s, 8.20564s/10 iters), loss = 5.62637
I0524 05:09:05.645185 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.62637 (* 1 = 5.62637 loss)
I0524 05:09:05.796749 11835 sgd_solver.cpp:112] Iteration 37380, lr = 0.1
I0524 05:09:11.605602 11835 solver.cpp:239] Iteration 37390 (1.6778 iter/s, 5.96019s/10 iters), loss = 5.86692
I0524 05:09:11.605646 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86692 (* 1 = 5.86692 loss)
I0524 05:09:11.605657 11835 sgd_solver.cpp:112] Iteration 37390, lr = 0.1
I0524 05:09:17.970719 11835 solver.cpp:239] Iteration 37400 (1.57122 iter/s, 6.36449s/10 iters), loss = 5.85826
I0524 05:09:17.970774 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85826 (* 1 = 5.85826 loss)
I0524 05:09:17.970988 11835 sgd_solver.cpp:112] Iteration 37400, lr = 0.1
I0524 05:09:24.787446 11835 solver.cpp:239] Iteration 37410 (1.46705 iter/s, 6.81642s/10 iters), loss = 6.0379
I0524 05:09:24.787699 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0379 (* 1 = 6.0379 loss)
I0524 05:09:24.787755 11835 sgd_solver.cpp:112] Iteration 37410, lr = 0.1
I0524 05:09:30.994292 11835 solver.cpp:239] Iteration 37420 (1.6113 iter/s, 6.20618s/10 iters), loss = 6.16241
I0524 05:09:30.994350 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16241 (* 1 = 6.16241 loss)
I0524 05:09:31.049974 11835 sgd_solver.cpp:112] Iteration 37420, lr = 0.1
I0524 05:09:36.757117 11835 solver.cpp:239] Iteration 37430 (1.73534 iter/s, 5.76255s/10 iters), loss = 5.4583
I0524 05:09:36.757158 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.4583 (* 1 = 5.4583 loss)
I0524 05:09:36.757169 11835 sgd_solver.cpp:112] Iteration 37430, lr = 0.1
I0524 05:09:43.410681 11835 solver.cpp:239] Iteration 37440 (1.50311 iter/s, 6.65287s/10 iters), loss = 5.50411
I0524 05:09:43.410754 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.50411 (* 1 = 5.50411 loss)
I0524 05:09:43.410966 11835 sgd_solver.cpp:112] Iteration 37440, lr = 0.1
I0524 05:09:52.147130 11835 solver.cpp:239] Iteration 37450 (1.14468 iter/s, 8.73604s/10 iters), loss = 5.67918
I0524 05:09:52.147182 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67918 (* 1 = 5.67918 loss)
I0524 05:09:52.916859 11835 sgd_solver.cpp:112] Iteration 37450, lr = 0.1
I0524 05:09:59.874536 11835 solver.cpp:239] Iteration 37460 (1.29415 iter/s, 7.72706s/10 iters), loss = 5.84372
I0524 05:09:59.874868 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84372 (* 1 = 5.84372 loss)
I0524 05:09:59.874923 11835 sgd_solver.cpp:112] Iteration 37460, lr = 0.1
I0524 05:10:06.745136 11835 solver.cpp:239] Iteration 37470 (1.45562 iter/s, 6.86992s/10 iters), loss = 6.5303
I0524 05:10:06.745185 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5303 (* 1 = 6.5303 loss)
I0524 05:10:07.386046 11835 sgd_solver.cpp:112] Iteration 37470, lr = 0.1
I0524 05:10:14.402979 11835 solver.cpp:239] Iteration 37480 (1.30591 iter/s, 7.6575s/10 iters), loss = 6.44199
I0524 05:10:14.403031 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44199 (* 1 = 6.44199 loss)
I0524 05:10:14.403139 11835 sgd_solver.cpp:112] Iteration 37480, lr = 0.1
I0524 05:10:21.208772 11835 solver.cpp:239] Iteration 37490 (1.4694 iter/s, 6.80549s/10 iters), loss = 7.25982
I0524 05:10:21.208822 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.25982 (* 1 = 7.25982 loss)
I0524 05:10:21.208986 11835 sgd_solver.cpp:112] Iteration 37490, lr = 0.1
I0524 05:10:28.755558 11835 solver.cpp:239] Iteration 37500 (1.32512 iter/s, 7.54646s/10 iters), loss = 6.32071
I0524 05:10:28.755599 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32071 (* 1 = 6.32071 loss)
I0524 05:10:28.755833 11835 sgd_solver.cpp:112] Iteration 37500, lr = 0.1
I0524 05:10:37.685485 11835 solver.cpp:239] Iteration 37510 (1.11988 iter/s, 8.92954s/10 iters), loss = 6.67531
I0524 05:10:37.685676 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67531 (* 1 = 6.67531 loss)
I0524 05:10:38.379374 11835 sgd_solver.cpp:112] Iteration 37510, lr = 0.1
I0524 05:10:45.432243 11835 solver.cpp:239] Iteration 37520 (1.29095 iter/s, 7.74626s/10 iters), loss = 7.00956
I0524 05:10:45.432297 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00956 (* 1 = 7.00956 loss)
I0524 05:10:45.432325 11835 sgd_solver.cpp:112] Iteration 37520, lr = 0.1
I0524 05:10:51.465040 11835 solver.cpp:239] Iteration 37530 (1.65769 iter/s, 6.0325s/10 iters), loss = 6.93284
I0524 05:10:51.465096 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93284 (* 1 = 6.93284 loss)
I0524 05:10:51.465129 11835 sgd_solver.cpp:112] Iteration 37530, lr = 0.1
I0524 05:11:00.022769 11835 solver.cpp:239] Iteration 37540 (1.16859 iter/s, 8.55734s/10 iters), loss = 6.89486
I0524 05:11:00.022819 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89486 (* 1 = 6.89486 loss)
I0524 05:11:00.023188 11835 sgd_solver.cpp:112] Iteration 37540, lr = 0.1
I0524 05:11:06.609369 11835 solver.cpp:239] Iteration 37550 (1.51831 iter/s, 6.58629s/10 iters), loss = 5.80598
I0524 05:11:06.609419 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80598 (* 1 = 5.80598 loss)
I0524 05:11:06.609772 11835 sgd_solver.cpp:112] Iteration 37550, lr = 0.1
I0524 05:11:12.210227 11835 solver.cpp:239] Iteration 37560 (1.78552 iter/s, 5.60059s/10 iters), loss = 6.88764
I0524 05:11:12.210327 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88764 (* 1 = 6.88764 loss)
I0524 05:11:12.210342 11835 sgd_solver.cpp:112] Iteration 37560, lr = 0.1
I0524 05:11:20.948786 11835 solver.cpp:239] Iteration 37570 (1.14469 iter/s, 8.736s/10 iters), loss = 5.75563
I0524 05:11:20.948837 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75563 (* 1 = 5.75563 loss)
I0524 05:11:21.785856 11835 sgd_solver.cpp:112] Iteration 37570, lr = 0.1
I0524 05:11:30.118266 11835 solver.cpp:239] Iteration 37580 (1.09062 iter/s, 9.16908s/10 iters), loss = 6.63787
I0524 05:11:30.118315 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63787 (* 1 = 6.63787 loss)
I0524 05:11:30.118489 11835 sgd_solver.cpp:112] Iteration 37580, lr = 0.1
I0524 05:11:35.867157 11835 solver.cpp:239] Iteration 37590 (1.73955 iter/s, 5.7486s/10 iters), loss = 5.49528
I0524 05:11:35.867210 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.49528 (* 1 = 5.49528 loss)
I0524 05:11:37.360167 11835 sgd_solver.cpp:112] Iteration 37590, lr = 0.1
I0524 05:11:43.191339 11835 solver.cpp:239] Iteration 37600 (1.3654 iter/s, 7.32384s/10 iters), loss = 6.28619
I0524 05:11:43.191604 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.28619 (* 1 = 6.28619 loss)
I0524 05:11:43.299044 11835 sgd_solver.cpp:112] Iteration 37600, lr = 0.1
I0524 05:11:49.249457 11835 solver.cpp:239] Iteration 37610 (1.6508 iter/s, 6.05767s/10 iters), loss = 6.17707
I0524 05:11:49.249505 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17707 (* 1 = 6.17707 loss)
I0524 05:11:50.025883 11835 sgd_solver.cpp:112] Iteration 37610, lr = 0.1
I0524 05:11:57.267195 11835 solver.cpp:239] Iteration 37620 (1.24729 iter/s, 8.01737s/10 iters), loss = 6.2092
I0524 05:11:57.267252 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2092 (* 1 = 6.2092 loss)
I0524 05:11:57.267283 11835 sgd_solver.cpp:112] Iteration 37620, lr = 0.1
I0524 05:12:03.395651 11835 solver.cpp:239] Iteration 37630 (1.63181 iter/s, 6.12817s/10 iters), loss = 6.31347
I0524 05:12:03.395697 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31347 (* 1 = 6.31347 loss)
I0524 05:12:03.395937 11835 sgd_solver.cpp:112] Iteration 37630, lr = 0.1
I0524 05:12:11.475986 11835 solver.cpp:239] Iteration 37640 (1.23763 iter/s, 8.07997s/10 iters), loss = 6.79207
I0524 05:12:11.476039 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79207 (* 1 = 6.79207 loss)
I0524 05:12:11.476209 11835 sgd_solver.cpp:112] Iteration 37640, lr = 0.1
I0524 05:12:17.803568 11835 solver.cpp:239] Iteration 37650 (1.58045 iter/s, 6.32729s/10 iters), loss = 5.61506
I0524 05:12:17.803865 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.61506 (* 1 = 5.61506 loss)
I0524 05:12:17.803901 11835 sgd_solver.cpp:112] Iteration 37650, lr = 0.1
I0524 05:12:24.114161 11835 solver.cpp:239] Iteration 37660 (1.58502 iter/s, 6.30906s/10 iters), loss = 5.53819
I0524 05:12:24.114209 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.53819 (* 1 = 5.53819 loss)
I0524 05:12:24.114306 11835 sgd_solver.cpp:112] Iteration 37660, lr = 0.1
I0524 05:12:31.761344 11835 solver.cpp:239] Iteration 37670 (1.30773 iter/s, 7.64683s/10 iters), loss = 5.64826
I0524 05:12:31.761400 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.64826 (* 1 = 5.64826 loss)
I0524 05:12:31.761623 11835 sgd_solver.cpp:112] Iteration 37670, lr = 0.1
I0524 05:12:38.314780 11835 solver.cpp:239] Iteration 37680 (1.52599 iter/s, 6.5531s/10 iters), loss = 5.64843
I0524 05:12:38.314836 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.64843 (* 1 = 5.64843 loss)
I0524 05:12:38.314973 11835 sgd_solver.cpp:112] Iteration 37680, lr = 0.1
I0524 05:12:43.862850 11835 solver.cpp:239] Iteration 37690 (1.80251 iter/s, 5.54781s/10 iters), loss = 5.62948
I0524 05:12:43.862892 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.62948 (* 1 = 5.62948 loss)
I0524 05:12:43.862905 11835 sgd_solver.cpp:112] Iteration 37690, lr = 0.1
I0524 05:12:52.454952 11835 solver.cpp:239] Iteration 37700 (1.16392 iter/s, 8.59165s/10 iters), loss = 6.43171
I0524 05:12:52.455196 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43171 (* 1 = 6.43171 loss)
I0524 05:12:52.455245 11835 sgd_solver.cpp:112] Iteration 37700, lr = 0.1
I0524 05:12:58.260134 11835 solver.cpp:239] Iteration 37710 (1.72273 iter/s, 5.80475s/10 iters), loss = 5.30149
I0524 05:12:58.260180 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.30149 (* 1 = 5.30149 loss)
I0524 05:12:59.087391 11835 sgd_solver.cpp:112] Iteration 37710, lr = 0.1
I0524 05:13:06.578402 11835 solver.cpp:239] Iteration 37720 (1.20223 iter/s, 8.3179s/10 iters), loss = 6.52375
I0524 05:13:06.578452 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52375 (* 1 = 6.52375 loss)
I0524 05:13:06.581836 11835 sgd_solver.cpp:112] Iteration 37720, lr = 0.1
I0524 05:13:12.917481 11835 solver.cpp:239] Iteration 37730 (1.57759 iter/s, 6.33879s/10 iters), loss = 6.06377
I0524 05:13:12.917522 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06377 (* 1 = 6.06377 loss)
I0524 05:13:12.991111 11835 sgd_solver.cpp:112] Iteration 37730, lr = 0.1
I0524 05:13:20.987996 11835 solver.cpp:239] Iteration 37740 (1.23913 iter/s, 8.07015s/10 iters), loss = 6.42782
I0524 05:13:20.988051 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42782 (* 1 = 6.42782 loss)
I0524 05:13:20.988236 11835 sgd_solver.cpp:112] Iteration 37740, lr = 0.1
I0524 05:13:27.656754 11835 solver.cpp:239] Iteration 37750 (1.4996 iter/s, 6.66844s/10 iters), loss = 6.90832
I0524 05:13:27.657060 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90832 (* 1 = 6.90832 loss)
I0524 05:13:27.859782 11835 sgd_solver.cpp:112] Iteration 37750, lr = 0.1
I0524 05:13:33.568224 11835 solver.cpp:239] Iteration 37760 (1.69177 iter/s, 5.91097s/10 iters), loss = 6.3084
I0524 05:13:33.568269 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3084 (* 1 = 6.3084 loss)
I0524 05:13:33.568285 11835 sgd_solver.cpp:112] Iteration 37760, lr = 0.1
I0524 05:13:40.168311 11835 solver.cpp:239] Iteration 37770 (1.51521 iter/s, 6.59973s/10 iters), loss = 6.60687
I0524 05:13:40.168359 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60687 (* 1 = 6.60687 loss)
I0524 05:13:40.168618 11835 sgd_solver.cpp:112] Iteration 37770, lr = 0.1
I0524 05:13:46.124349 11835 solver.cpp:239] Iteration 37780 (1.67905 iter/s, 5.95575s/10 iters), loss = 7.18634
I0524 05:13:46.124402 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.18634 (* 1 = 7.18634 loss)
I0524 05:13:46.124759 11835 sgd_solver.cpp:112] Iteration 37780, lr = 0.1
I0524 05:13:52.306336 11835 solver.cpp:239] Iteration 37790 (1.61768 iter/s, 6.1817s/10 iters), loss = 6.58112
I0524 05:13:52.306372 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58112 (* 1 = 6.58112 loss)
I0524 05:13:52.319186 11835 sgd_solver.cpp:112] Iteration 37790, lr = 0.1
I0524 05:13:57.955488 11835 solver.cpp:239] Iteration 37800 (1.77026 iter/s, 5.64888s/10 iters), loss = 6.19192
I0524 05:13:57.955746 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19192 (* 1 = 6.19192 loss)
I0524 05:13:57.955803 11835 sgd_solver.cpp:112] Iteration 37800, lr = 0.1
I0524 05:14:04.685093 11835 solver.cpp:239] Iteration 37810 (1.48611 iter/s, 6.72898s/10 iters), loss = 6.22559
I0524 05:14:04.685133 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22559 (* 1 = 6.22559 loss)
I0524 05:14:04.685500 11835 sgd_solver.cpp:112] Iteration 37810, lr = 0.1
I0524 05:14:11.555263 11835 solver.cpp:239] Iteration 37820 (1.45563 iter/s, 6.86986s/10 iters), loss = 6.30366
I0524 05:14:11.555315 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30366 (* 1 = 6.30366 loss)
I0524 05:14:12.106091 11835 sgd_solver.cpp:112] Iteration 37820, lr = 0.1
I0524 05:14:19.372187 11835 solver.cpp:239] Iteration 37830 (1.27933 iter/s, 7.81657s/10 iters), loss = 6.19491
I0524 05:14:19.372241 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19491 (* 1 = 6.19491 loss)
I0524 05:14:19.408263 11835 sgd_solver.cpp:112] Iteration 37830, lr = 0.1
I0524 05:14:27.051966 11835 solver.cpp:239] Iteration 37840 (1.30218 iter/s, 7.67944s/10 iters), loss = 6.14316
I0524 05:14:27.052002 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14316 (* 1 = 6.14316 loss)
I0524 05:14:27.657078 11835 sgd_solver.cpp:112] Iteration 37840, lr = 0.1
I0524 05:14:34.663810 11835 solver.cpp:239] Iteration 37850 (1.3138 iter/s, 7.6115s/10 iters), loss = 6.0328
I0524 05:14:34.663941 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0328 (* 1 = 6.0328 loss)
I0524 05:14:34.686838 11835 sgd_solver.cpp:112] Iteration 37850, lr = 0.1
I0524 05:14:41.388512 11835 solver.cpp:239] Iteration 37860 (1.48714 iter/s, 6.72431s/10 iters), loss = 6.07705
I0524 05:14:41.388564 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07705 (* 1 = 6.07705 loss)
I0524 05:14:41.388579 11835 sgd_solver.cpp:112] Iteration 37860, lr = 0.1
I0524 05:14:47.510352 11835 solver.cpp:239] Iteration 37870 (1.63387 iter/s, 6.12045s/10 iters), loss = 6.27295
I0524 05:14:47.510397 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27295 (* 1 = 6.27295 loss)
I0524 05:14:47.510592 11835 sgd_solver.cpp:112] Iteration 37870, lr = 0.1
I0524 05:14:53.611141 11835 solver.cpp:239] Iteration 37880 (1.63921 iter/s, 6.10052s/10 iters), loss = 5.96229
I0524 05:14:53.611181 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96229 (* 1 = 5.96229 loss)
I0524 05:14:53.611192 11835 sgd_solver.cpp:112] Iteration 37880, lr = 0.1
I0524 05:15:03.378181 11835 solver.cpp:239] Iteration 37890 (1.0239 iter/s, 9.76663s/10 iters), loss = 6.74861
I0524 05:15:03.378234 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74861 (* 1 = 6.74861 loss)
I0524 05:15:03.802402 11835 sgd_solver.cpp:112] Iteration 37890, lr = 0.1
I0524 05:15:10.051507 11835 solver.cpp:239] Iteration 37900 (1.49857 iter/s, 6.67302s/10 iters), loss = 6.96589
I0524 05:15:10.051673 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96589 (* 1 = 6.96589 loss)
I0524 05:15:10.051690 11835 sgd_solver.cpp:112] Iteration 37900, lr = 0.1
I0524 05:15:19.065492 11835 solver.cpp:239] Iteration 37910 (1.10948 iter/s, 9.01326s/10 iters), loss = 6.27811
I0524 05:15:19.065546 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27811 (* 1 = 6.27811 loss)
I0524 05:15:19.065969 11835 sgd_solver.cpp:112] Iteration 37910, lr = 0.1
I0524 05:15:26.085419 11835 solver.cpp:239] Iteration 37920 (1.42458 iter/s, 7.01961s/10 iters), loss = 6.75163
I0524 05:15:26.085461 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.75163 (* 1 = 6.75163 loss)
I0524 05:15:26.085757 11835 sgd_solver.cpp:112] Iteration 37920, lr = 0.1
I0524 05:15:31.685060 11835 solver.cpp:239] Iteration 37930 (1.78591 iter/s, 5.59937s/10 iters), loss = 5.74721
I0524 05:15:31.685104 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74721 (* 1 = 5.74721 loss)
I0524 05:15:31.685375 11835 sgd_solver.cpp:112] Iteration 37930, lr = 0.1
I0524 05:15:37.821221 11835 solver.cpp:239] Iteration 37940 (1.62976 iter/s, 6.13589s/10 iters), loss = 7.40021
I0524 05:15:37.821261 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.40021 (* 1 = 7.40021 loss)
I0524 05:15:37.881848 11835 sgd_solver.cpp:112] Iteration 37940, lr = 0.1
I0524 05:15:44.299003 11835 solver.cpp:239] Iteration 37950 (1.54381 iter/s, 6.47748s/10 iters), loss = 6.10858
I0524 05:15:44.299106 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10858 (* 1 = 6.10858 loss)
I0524 05:15:44.299123 11835 sgd_solver.cpp:112] Iteration 37950, lr = 0.1
I0524 05:15:50.376778 11835 solver.cpp:239] Iteration 37960 (1.64549 iter/s, 6.07721s/10 iters), loss = 6.17771
I0524 05:15:50.376816 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17771 (* 1 = 6.17771 loss)
I0524 05:15:50.376910 11835 sgd_solver.cpp:112] Iteration 37960, lr = 0.1
I0524 05:15:56.823108 11835 solver.cpp:239] Iteration 37970 (1.55134 iter/s, 6.44604s/10 iters), loss = 5.27904
I0524 05:15:56.823155 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.27904 (* 1 = 5.27904 loss)
I0524 05:15:56.823397 11835 sgd_solver.cpp:112] Iteration 37970, lr = 0.1
I0524 05:16:04.231406 11835 solver.cpp:239] Iteration 37980 (1.3499 iter/s, 7.40797s/10 iters), loss = 6.42853
I0524 05:16:04.231454 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42853 (* 1 = 6.42853 loss)
I0524 05:16:04.231580 11835 sgd_solver.cpp:112] Iteration 37980, lr = 0.1
I0524 05:16:09.883390 11835 solver.cpp:239] Iteration 37990 (1.76937 iter/s, 5.65172s/10 iters), loss = 6.05913
I0524 05:16:09.883436 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05913 (* 1 = 6.05913 loss)
I0524 05:16:09.884028 11835 sgd_solver.cpp:112] Iteration 37990, lr = 0.1
I0524 05:16:17.653112 11835 solver.cpp:239] Iteration 38000 (1.28711 iter/s, 7.76937s/10 iters), loss = 5.70357
I0524 05:16:17.653348 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.70357 (* 1 = 5.70357 loss)
I0524 05:16:17.653398 11835 sgd_solver.cpp:112] Iteration 38000, lr = 0.1
I0524 05:16:24.729769 11835 solver.cpp:239] Iteration 38010 (1.41323 iter/s, 7.07598s/10 iters), loss = 5.59106
I0524 05:16:24.729820 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.59106 (* 1 = 5.59106 loss)
I0524 05:16:24.730015 11835 sgd_solver.cpp:112] Iteration 38010, lr = 0.1
I0524 05:16:30.816275 11835 solver.cpp:239] Iteration 38020 (1.64306 iter/s, 6.0862s/10 iters), loss = 6.32431
I0524 05:16:30.816323 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32431 (* 1 = 6.32431 loss)
I0524 05:16:30.816349 11835 sgd_solver.cpp:112] Iteration 38020, lr = 0.1
I0524 05:16:39.224757 11835 solver.cpp:239] Iteration 38030 (1.18942 iter/s, 8.40749s/10 iters), loss = 6.57729
I0524 05:16:39.224817 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57729 (* 1 = 6.57729 loss)
I0524 05:16:39.225047 11835 sgd_solver.cpp:112] Iteration 38030, lr = 0.1
I0524 05:16:45.456583 11835 solver.cpp:239] Iteration 38040 (1.60474 iter/s, 6.23153s/10 iters), loss = 5.97407
I0524 05:16:45.456627 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97407 (* 1 = 5.97407 loss)
I0524 05:16:45.456871 11835 sgd_solver.cpp:112] Iteration 38040, lr = 0.1
I0524 05:16:52.682653 11835 solver.cpp:239] Iteration 38050 (1.38394 iter/s, 7.22573s/10 iters), loss = 5.68481
I0524 05:16:52.682847 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68481 (* 1 = 5.68481 loss)
I0524 05:16:52.682929 11835 sgd_solver.cpp:112] Iteration 38050, lr = 0.1
I0524 05:16:59.083482 11835 solver.cpp:239] Iteration 38060 (1.5624 iter/s, 6.40039s/10 iters), loss = 7.07229
I0524 05:16:59.083534 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.07229 (* 1 = 7.07229 loss)
I0524 05:16:59.087275 11835 sgd_solver.cpp:112] Iteration 38060, lr = 0.1
I0524 05:17:05.983453 11835 solver.cpp:239] Iteration 38070 (1.44935 iter/s, 6.89966s/10 iters), loss = 6.49684
I0524 05:17:05.983492 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49684 (* 1 = 6.49684 loss)
I0524 05:17:06.844800 11835 sgd_solver.cpp:112] Iteration 38070, lr = 0.1
I0524 05:17:14.177733 11835 solver.cpp:239] Iteration 38080 (1.22042 iter/s, 8.19392s/10 iters), loss = 6.77498
I0524 05:17:14.177783 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.77498 (* 1 = 6.77498 loss)
I0524 05:17:14.177896 11835 sgd_solver.cpp:112] Iteration 38080, lr = 0.1
I0524 05:17:20.776051 11835 solver.cpp:239] Iteration 38090 (1.51561 iter/s, 6.59802s/10 iters), loss = 6.25623
I0524 05:17:20.776103 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25623 (* 1 = 6.25623 loss)
I0524 05:17:20.776286 11835 sgd_solver.cpp:112] Iteration 38090, lr = 0.1
I0524 05:17:28.476614 11835 solver.cpp:239] Iteration 38100 (1.29866 iter/s, 7.70022s/10 iters), loss = 5.56295
I0524 05:17:28.476852 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56295 (* 1 = 5.56295 loss)
I0524 05:17:28.476899 11835 sgd_solver.cpp:112] Iteration 38100, lr = 0.1
I0524 05:17:35.367573 11835 solver.cpp:239] Iteration 38110 (1.45127 iter/s, 6.89049s/10 iters), loss = 6.71505
I0524 05:17:35.367626 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71505 (* 1 = 6.71505 loss)
I0524 05:17:35.378064 11835 sgd_solver.cpp:112] Iteration 38110, lr = 0.1
I0524 05:17:41.179062 11835 solver.cpp:239] Iteration 38120 (1.72082 iter/s, 5.81119s/10 iters), loss = 7.09192
I0524 05:17:41.179121 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09192 (* 1 = 7.09192 loss)
I0524 05:17:41.179164 11835 sgd_solver.cpp:112] Iteration 38120, lr = 0.1
I0524 05:17:47.513566 11835 solver.cpp:239] Iteration 38130 (1.57873 iter/s, 6.3342s/10 iters), loss = 6.5731
I0524 05:17:47.513619 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5731 (* 1 = 6.5731 loss)
I0524 05:17:47.513634 11835 sgd_solver.cpp:112] Iteration 38130, lr = 0.1
I0524 05:17:53.409065 11835 solver.cpp:239] Iteration 38140 (1.69646 iter/s, 5.89462s/10 iters), loss = 5.86505
I0524 05:17:53.409114 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86505 (* 1 = 5.86505 loss)
I0524 05:17:53.409130 11835 sgd_solver.cpp:112] Iteration 38140, lr = 0.1
I0524 05:18:02.078763 11835 solver.cpp:239] Iteration 38150 (1.15349 iter/s, 8.66932s/10 iters), loss = 5.82415
I0524 05:18:02.079068 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82415 (* 1 = 5.82415 loss)
I0524 05:18:02.079119 11835 sgd_solver.cpp:112] Iteration 38150, lr = 0.1
I0524 05:18:08.889609 11835 solver.cpp:239] Iteration 38160 (1.46838 iter/s, 6.81021s/10 iters), loss = 6.38653
I0524 05:18:08.889675 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38653 (* 1 = 6.38653 loss)
I0524 05:18:08.889693 11835 sgd_solver.cpp:112] Iteration 38160, lr = 0.1
I0524 05:18:15.533901 11835 solver.cpp:239] Iteration 38170 (1.50512 iter/s, 6.64398s/10 iters), loss = 6.58401
I0524 05:18:15.533946 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58401 (* 1 = 6.58401 loss)
I0524 05:18:15.534152 11835 sgd_solver.cpp:112] Iteration 38170, lr = 0.1
I0524 05:18:21.714510 11835 solver.cpp:239] Iteration 38180 (1.61804 iter/s, 6.18031s/10 iters), loss = 7.13208
I0524 05:18:21.714572 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13208 (* 1 = 7.13208 loss)
I0524 05:18:21.714691 11835 sgd_solver.cpp:112] Iteration 38180, lr = 0.1
I0524 05:18:30.649598 11835 solver.cpp:239] Iteration 38190 (1.11923 iter/s, 8.93469s/10 iters), loss = 6.14133
I0524 05:18:30.649662 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14133 (* 1 = 6.14133 loss)
I0524 05:18:31.443399 11835 sgd_solver.cpp:112] Iteration 38190, lr = 0.1
I0524 05:18:38.431104 11835 solver.cpp:239] Iteration 38200 (1.28516 iter/s, 7.78115s/10 iters), loss = 6.64257
I0524 05:18:38.431334 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64257 (* 1 = 6.64257 loss)
I0524 05:18:39.354919 11835 sgd_solver.cpp:112] Iteration 38200, lr = 0.1
I0524 05:18:45.469359 11835 solver.cpp:239] Iteration 38210 (1.42091 iter/s, 7.03775s/10 iters), loss = 5.84451
I0524 05:18:45.469432 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84451 (* 1 = 5.84451 loss)
I0524 05:18:45.469820 11835 sgd_solver.cpp:112] Iteration 38210, lr = 0.1
I0524 05:18:54.142796 11835 solver.cpp:239] Iteration 38220 (1.153 iter/s, 8.67301s/10 iters), loss = 6.5663
I0524 05:18:54.142885 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5663 (* 1 = 6.5663 loss)
I0524 05:18:54.142910 11835 sgd_solver.cpp:112] Iteration 38220, lr = 0.1
I0524 05:19:01.091976 11835 solver.cpp:239] Iteration 38230 (1.43909 iter/s, 6.94882s/10 iters), loss = 6.38996
I0524 05:19:01.092032 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38996 (* 1 = 6.38996 loss)
I0524 05:19:01.092080 11835 sgd_solver.cpp:112] Iteration 38230, lr = 0.1
I0524 05:19:06.995168 11835 solver.cpp:239] Iteration 38240 (1.69408 iter/s, 5.9029s/10 iters), loss = 5.98546
I0524 05:19:06.995237 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98546 (* 1 = 5.98546 loss)
I0524 05:19:07.567127 11835 sgd_solver.cpp:112] Iteration 38240, lr = 0.1
I0524 05:19:15.441607 11835 solver.cpp:239] Iteration 38250 (1.18398 iter/s, 8.44606s/10 iters), loss = 7.4117
I0524 05:19:15.441910 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.4117 (* 1 = 7.4117 loss)
I0524 05:19:16.674477 11835 sgd_solver.cpp:112] Iteration 38250, lr = 0.1
I0524 05:19:24.910761 11835 solver.cpp:239] Iteration 38260 (1.05612 iter/s, 9.4686s/10 iters), loss = 6.73836
I0524 05:19:24.910820 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73836 (* 1 = 6.73836 loss)
I0524 05:19:25.489645 11835 sgd_solver.cpp:112] Iteration 38260, lr = 0.1
I0524 05:19:31.286092 11835 solver.cpp:239] Iteration 38270 (1.56862 iter/s, 6.37503s/10 iters), loss = 6.37751
I0524 05:19:31.286149 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37751 (* 1 = 6.37751 loss)
I0524 05:19:31.286171 11835 sgd_solver.cpp:112] Iteration 38270, lr = 0.1
I0524 05:19:38.433490 11835 solver.cpp:239] Iteration 38280 (1.39918 iter/s, 7.14704s/10 iters), loss = 6.74481
I0524 05:19:38.433558 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74481 (* 1 = 6.74481 loss)
I0524 05:19:38.433795 11835 sgd_solver.cpp:112] Iteration 38280, lr = 0.1
I0524 05:19:45.812810 11835 solver.cpp:239] Iteration 38290 (1.35522 iter/s, 7.37889s/10 iters), loss = 7.06865
I0524 05:19:45.813110 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.06865 (* 1 = 7.06865 loss)
I0524 05:19:45.813254 11835 sgd_solver.cpp:112] Iteration 38290, lr = 0.1
I0524 05:19:53.777099 11835 solver.cpp:239] Iteration 38300 (1.2557 iter/s, 7.96371s/10 iters), loss = 6.10827
I0524 05:19:53.777165 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10827 (* 1 = 6.10827 loss)
I0524 05:19:53.777263 11835 sgd_solver.cpp:112] Iteration 38300, lr = 0.1
I0524 05:20:03.635624 11835 solver.cpp:239] Iteration 38310 (1.01439 iter/s, 9.85809s/10 iters), loss = 7.17844
I0524 05:20:03.635668 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.17844 (* 1 = 7.17844 loss)
I0524 05:20:03.726728 11835 sgd_solver.cpp:112] Iteration 38310, lr = 0.1
I0524 05:20:10.257447 11835 solver.cpp:239] Iteration 38320 (1.51023 iter/s, 6.62149s/10 iters), loss = 7.2883
I0524 05:20:10.257534 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2883 (* 1 = 7.2883 loss)
I0524 05:20:11.201520 11835 sgd_solver.cpp:112] Iteration 38320, lr = 0.1
I0524 05:20:18.370594 11835 solver.cpp:239] Iteration 38330 (1.23263 iter/s, 8.11274s/10 iters), loss = 6.80921
I0524 05:20:18.370878 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80921 (* 1 = 6.80921 loss)
I0524 05:20:19.068980 11835 sgd_solver.cpp:112] Iteration 38330, lr = 0.1
I0524 05:20:26.348984 11835 solver.cpp:239] Iteration 38340 (1.25348 iter/s, 7.97781s/10 iters), loss = 6.08823
I0524 05:20:26.349089 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08823 (* 1 = 6.08823 loss)
I0524 05:20:27.460674 11835 sgd_solver.cpp:112] Iteration 38340, lr = 0.1
I0524 05:20:34.221731 11835 solver.cpp:239] Iteration 38350 (1.27026 iter/s, 7.8724s/10 iters), loss = 5.39701
I0524 05:20:34.221786 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.39701 (* 1 = 5.39701 loss)
I0524 05:20:34.222105 11835 sgd_solver.cpp:112] Iteration 38350, lr = 0.1
I0524 05:20:42.051630 11835 solver.cpp:239] Iteration 38360 (1.27722 iter/s, 7.82952s/10 iters), loss = 5.40022
I0524 05:20:42.051699 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.40022 (* 1 = 5.40022 loss)
I0524 05:20:42.133955 11835 sgd_solver.cpp:112] Iteration 38360, lr = 0.1
I0524 05:20:49.641890 11835 solver.cpp:239] Iteration 38370 (1.31754 iter/s, 7.5899s/10 iters), loss = 6.06516
I0524 05:20:49.642123 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06516 (* 1 = 6.06516 loss)
I0524 05:20:49.642231 11835 sgd_solver.cpp:112] Iteration 38370, lr = 0.1
I0524 05:20:56.074297 11835 solver.cpp:239] Iteration 38380 (1.55473 iter/s, 6.43197s/10 iters), loss = 5.94744
I0524 05:20:56.074352 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94744 (* 1 = 5.94744 loss)
I0524 05:20:56.872289 11835 sgd_solver.cpp:112] Iteration 38380, lr = 0.1
I0524 05:21:04.020929 11835 solver.cpp:239] Iteration 38390 (1.25845 iter/s, 7.94627s/10 iters), loss = 6.22198
I0524 05:21:04.020998 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22198 (* 1 = 6.22198 loss)
I0524 05:21:04.025274 11835 sgd_solver.cpp:112] Iteration 38390, lr = 0.1
I0524 05:21:11.093081 11835 solver.cpp:239] Iteration 38400 (1.41406 iter/s, 7.07182s/10 iters), loss = 6.27088
I0524 05:21:11.093144 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27088 (* 1 = 6.27088 loss)
I0524 05:21:11.144419 11835 sgd_solver.cpp:112] Iteration 38400, lr = 0.1
I0524 05:21:20.393291 11835 solver.cpp:239] Iteration 38410 (1.07529 iter/s, 9.29979s/10 iters), loss = 6.84292
I0524 05:21:20.393457 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84292 (* 1 = 6.84292 loss)
I0524 05:21:20.393606 11835 sgd_solver.cpp:112] Iteration 38410, lr = 0.1
I0524 05:21:27.099272 11835 solver.cpp:239] Iteration 38420 (1.4913 iter/s, 6.70555s/10 iters), loss = 5.61695
I0524 05:21:27.099337 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.61695 (* 1 = 5.61695 loss)
I0524 05:21:27.102910 11835 sgd_solver.cpp:112] Iteration 38420, lr = 0.1
I0524 05:21:36.796106 11835 solver.cpp:239] Iteration 38430 (1.03131 iter/s, 9.69641s/10 iters), loss = 6.2244
I0524 05:21:36.796160 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2244 (* 1 = 6.2244 loss)
I0524 05:21:36.796363 11835 sgd_solver.cpp:112] Iteration 38430, lr = 0.1
I0524 05:21:48.707510 11835 solver.cpp:239] Iteration 38440 (0.839569 iter/s, 11.9109s/10 iters), loss = 5.43367
I0524 05:21:48.707612 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43367 (* 1 = 5.43367 loss)
I0524 05:21:49.383167 11835 sgd_solver.cpp:112] Iteration 38440, lr = 0.1
I0524 05:21:57.214578 11835 solver.cpp:239] Iteration 38450 (1.17555 iter/s, 8.50665s/10 iters), loss = 5.26959
I0524 05:21:57.214859 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.26959 (* 1 = 5.26959 loss)
I0524 05:21:57.304909 11835 sgd_solver.cpp:112] Iteration 38450, lr = 0.1
I0524 05:22:03.716437 11835 solver.cpp:239] Iteration 38460 (1.53814 iter/s, 6.50135s/10 iters), loss = 6.4335
I0524 05:22:03.716487 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4335 (* 1 = 6.4335 loss)
I0524 05:22:03.716573 11835 sgd_solver.cpp:112] Iteration 38460, lr = 0.1
I0524 05:22:11.951232 11835 solver.cpp:239] Iteration 38470 (1.21441 iter/s, 8.23443s/10 iters), loss = 6.1406
I0524 05:22:11.951285 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1406 (* 1 = 6.1406 loss)
I0524 05:22:12.152495 11835 sgd_solver.cpp:112] Iteration 38470, lr = 0.1
I0524 05:22:18.318177 11835 solver.cpp:239] Iteration 38480 (1.57068 iter/s, 6.36665s/10 iters), loss = 5.9565
I0524 05:22:18.318218 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9565 (* 1 = 5.9565 loss)
I0524 05:22:18.658305 11835 sgd_solver.cpp:112] Iteration 38480, lr = 0.1
I0524 05:22:24.861871 11835 solver.cpp:239] Iteration 38490 (1.52826 iter/s, 6.54339s/10 iters), loss = 5.91933
I0524 05:22:24.861927 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91933 (* 1 = 5.91933 loss)
I0524 05:22:24.862043 11835 sgd_solver.cpp:112] Iteration 38490, lr = 0.1
I0524 05:22:34.097288 11835 solver.cpp:239] Iteration 38500 (1.08284 iter/s, 9.235s/10 iters), loss = 6.46856
I0524 05:22:34.097507 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46856 (* 1 = 6.46856 loss)
I0524 05:22:34.107770 11835 sgd_solver.cpp:112] Iteration 38500, lr = 0.1
I0524 05:22:42.801321 11835 solver.cpp:239] Iteration 38510 (1.14896 iter/s, 8.70351s/10 iters), loss = 5.98433
I0524 05:22:42.801370 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98433 (* 1 = 5.98433 loss)
I0524 05:22:42.801553 11835 sgd_solver.cpp:112] Iteration 38510, lr = 0.1
I0524 05:22:49.506319 11835 solver.cpp:239] Iteration 38520 (1.49149 iter/s, 6.70468s/10 iters), loss = 5.83014
I0524 05:22:49.506363 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83014 (* 1 = 5.83014 loss)
I0524 05:22:49.506377 11835 sgd_solver.cpp:112] Iteration 38520, lr = 0.1
I0524 05:22:56.602567 11835 solver.cpp:239] Iteration 38530 (1.40927 iter/s, 7.09586s/10 iters), loss = 5.58895
I0524 05:22:56.602630 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.58895 (* 1 = 5.58895 loss)
I0524 05:22:56.602846 11835 sgd_solver.cpp:112] Iteration 38530, lr = 0.1
I0524 05:23:02.549343 11835 solver.cpp:239] Iteration 38540 (1.68166 iter/s, 5.9465s/10 iters), loss = 5.35963
I0524 05:23:02.549382 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.35963 (* 1 = 5.35963 loss)
I0524 05:23:02.549396 11835 sgd_solver.cpp:112] Iteration 38540, lr = 0.1
I0524 05:23:09.245966 11835 solver.cpp:239] Iteration 38550 (1.49338 iter/s, 6.69624s/10 iters), loss = 5.3762
I0524 05:23:09.246215 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.3762 (* 1 = 5.3762 loss)
I0524 05:23:09.246263 11835 sgd_solver.cpp:112] Iteration 38550, lr = 0.1
I0524 05:23:15.208952 11835 solver.cpp:239] Iteration 38560 (1.67713 iter/s, 5.96255s/10 iters), loss = 6.04094
I0524 05:23:15.209000 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04094 (* 1 = 6.04094 loss)
I0524 05:23:15.209015 11835 sgd_solver.cpp:112] Iteration 38560, lr = 0.1
I0524 05:23:22.752262 11835 solver.cpp:239] Iteration 38570 (1.32575 iter/s, 7.5429s/10 iters), loss = 6.92052
I0524 05:23:22.752305 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92052 (* 1 = 6.92052 loss)
I0524 05:23:22.752504 11835 sgd_solver.cpp:112] Iteration 38570, lr = 0.1
I0524 05:23:32.925719 11835 solver.cpp:239] Iteration 38580 (0.982993 iter/s, 10.173s/10 iters), loss = 6.4199
I0524 05:23:32.925777 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4199 (* 1 = 6.4199 loss)
I0524 05:23:32.970613 11835 sgd_solver.cpp:112] Iteration 38580, lr = 0.1
I0524 05:23:39.565759 11835 solver.cpp:239] Iteration 38590 (1.50609 iter/s, 6.63973s/10 iters), loss = 6.2678
I0524 05:23:39.566028 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2678 (* 1 = 6.2678 loss)
I0524 05:23:40.137198 11835 sgd_solver.cpp:112] Iteration 38590, lr = 0.1
I0524 05:23:48.102605 11835 solver.cpp:239] Iteration 38600 (1.17147 iter/s, 8.53628s/10 iters), loss = 6.25901
I0524 05:23:48.102663 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25901 (* 1 = 6.25901 loss)
I0524 05:23:48.429764 11835 sgd_solver.cpp:112] Iteration 38600, lr = 0.1
I0524 05:23:55.009989 11835 solver.cpp:239] Iteration 38610 (1.44779 iter/s, 6.90706s/10 iters), loss = 6.22814
I0524 05:23:55.010046 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22814 (* 1 = 6.22814 loss)
I0524 05:23:55.989399 11835 sgd_solver.cpp:112] Iteration 38610, lr = 0.1
I0524 05:24:05.589766 11835 solver.cpp:239] Iteration 38620 (0.945242 iter/s, 10.5793s/10 iters), loss = 6.61796
I0524 05:24:05.589851 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61796 (* 1 = 6.61796 loss)
I0524 05:24:06.094960 11835 sgd_solver.cpp:112] Iteration 38620, lr = 0.1
I0524 05:24:12.793092 11835 solver.cpp:239] Iteration 38630 (1.38831 iter/s, 7.20298s/10 iters), loss = 5.15475
I0524 05:24:12.793310 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.15475 (* 1 = 5.15475 loss)
I0524 05:24:12.793344 11835 sgd_solver.cpp:112] Iteration 38630, lr = 0.1
I0524 05:24:22.735682 11835 solver.cpp:239] Iteration 38640 (1.00584 iter/s, 9.94195s/10 iters), loss = 6.08285
I0524 05:24:22.735780 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08285 (* 1 = 6.08285 loss)
I0524 05:24:23.001374 11835 sgd_solver.cpp:112] Iteration 38640, lr = 0.1
I0524 05:24:29.256695 11835 solver.cpp:239] Iteration 38650 (1.53358 iter/s, 6.52067s/10 iters), loss = 5.8773
I0524 05:24:29.256750 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8773 (* 1 = 5.8773 loss)
I0524 05:24:29.256764 11835 sgd_solver.cpp:112] Iteration 38650, lr = 0.1
I0524 05:24:37.795735 11835 solver.cpp:239] Iteration 38660 (1.17114 iter/s, 8.53866s/10 iters), loss = 5.50443
I0524 05:24:37.795791 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.50443 (* 1 = 5.50443 loss)
I0524 05:24:37.795912 11835 sgd_solver.cpp:112] Iteration 38660, lr = 0.1
I0524 05:24:45.031116 11835 solver.cpp:239] Iteration 38670 (1.38216 iter/s, 7.23504s/10 iters), loss = 7.13537
I0524 05:24:45.031250 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13537 (* 1 = 7.13537 loss)
I0524 05:24:45.031316 11835 sgd_solver.cpp:112] Iteration 38670, lr = 0.1
I0524 05:24:53.008683 11835 solver.cpp:239] Iteration 38680 (1.25359 iter/s, 7.97709s/10 iters), loss = 6.14398
I0524 05:24:53.008747 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14398 (* 1 = 6.14398 loss)
I0524 05:24:53.149502 11835 sgd_solver.cpp:112] Iteration 38680, lr = 0.1
I0524 05:25:01.774025 11835 solver.cpp:239] Iteration 38690 (1.14091 iter/s, 8.76494s/10 iters), loss = 5.87736
I0524 05:25:01.774078 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87736 (* 1 = 5.87736 loss)
I0524 05:25:01.879422 11835 sgd_solver.cpp:112] Iteration 38690, lr = 0.1
I0524 05:25:08.462280 11835 solver.cpp:239] Iteration 38700 (1.49523 iter/s, 6.68795s/10 iters), loss = 5.70559
I0524 05:25:08.462321 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.70559 (* 1 = 5.70559 loss)
I0524 05:25:08.475292 11835 sgd_solver.cpp:112] Iteration 38700, lr = 0.1
I0524 05:25:14.556058 11835 solver.cpp:239] Iteration 38710 (1.6411 iter/s, 6.09349s/10 iters), loss = 6.05962
I0524 05:25:14.556120 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05962 (* 1 = 6.05962 loss)
I0524 05:25:14.556171 11835 sgd_solver.cpp:112] Iteration 38710, lr = 0.1
I0524 05:25:21.139653 11835 solver.cpp:239] Iteration 38720 (1.519 iter/s, 6.58328s/10 iters), loss = 6.2082
I0524 05:25:21.139863 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2082 (* 1 = 6.2082 loss)
I0524 05:25:21.140179 11835 sgd_solver.cpp:112] Iteration 38720, lr = 0.1
I0524 05:25:31.046447 11835 solver.cpp:239] Iteration 38730 (1.00947 iter/s, 9.9062s/10 iters), loss = 7.03811
I0524 05:25:31.046531 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03811 (* 1 = 7.03811 loss)
I0524 05:25:31.592238 11835 sgd_solver.cpp:112] Iteration 38730, lr = 0.1
I0524 05:25:40.143085 11835 solver.cpp:239] Iteration 38740 (1.09936 iter/s, 9.09621s/10 iters), loss = 5.61309
I0524 05:25:40.143143 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.61309 (* 1 = 5.61309 loss)
I0524 05:25:40.143350 11835 sgd_solver.cpp:112] Iteration 38740, lr = 0.1
I0524 05:25:47.110939 11835 solver.cpp:239] Iteration 38750 (1.43523 iter/s, 6.96753s/10 iters), loss = 5.29146
I0524 05:25:47.111029 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.29146 (* 1 = 5.29146 loss)
I0524 05:25:47.111261 11835 sgd_solver.cpp:112] Iteration 38750, lr = 0.1
I0524 05:25:55.083972 11835 solver.cpp:239] Iteration 38760 (1.25429 iter/s, 7.97263s/10 iters), loss = 5.91298
I0524 05:25:55.084264 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91298 (* 1 = 5.91298 loss)
I0524 05:25:55.814726 11835 sgd_solver.cpp:112] Iteration 38760, lr = 0.1
I0524 05:26:04.203630 11835 solver.cpp:239] Iteration 38770 (1.09661 iter/s, 9.11903s/10 iters), loss = 6.67679
I0524 05:26:04.203719 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67679 (* 1 = 6.67679 loss)
I0524 05:26:04.203773 11835 sgd_solver.cpp:112] Iteration 38770, lr = 0.1
I0524 05:26:13.712707 11835 solver.cpp:239] Iteration 38780 (1.05168 iter/s, 9.50863s/10 iters), loss = 6.87873
I0524 05:26:13.712762 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87873 (* 1 = 6.87873 loss)
I0524 05:26:13.712945 11835 sgd_solver.cpp:112] Iteration 38780, lr = 0.1
I0524 05:26:20.514912 11835 solver.cpp:239] Iteration 38790 (1.47018 iter/s, 6.80189s/10 iters), loss = 6.44128
I0524 05:26:20.515015 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44128 (* 1 = 6.44128 loss)
I0524 05:26:21.602946 11835 sgd_solver.cpp:112] Iteration 38790, lr = 0.1
I0524 05:26:28.783318 11835 solver.cpp:239] Iteration 38800 (1.20948 iter/s, 8.26802s/10 iters), loss = 6.34357
I0524 05:26:28.783566 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34357 (* 1 = 6.34357 loss)
I0524 05:26:29.707471 11835 sgd_solver.cpp:112] Iteration 38800, lr = 0.1
I0524 05:26:35.320966 11835 solver.cpp:239] Iteration 38810 (1.52971 iter/s, 6.53717s/10 iters), loss = 5.38377
I0524 05:26:35.321019 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.38377 (* 1 = 5.38377 loss)
I0524 05:26:35.321035 11835 sgd_solver.cpp:112] Iteration 38810, lr = 0.1
I0524 05:26:42.636346 11835 solver.cpp:239] Iteration 38820 (1.36706 iter/s, 7.31499s/10 iters), loss = 6.5741
I0524 05:26:42.636395 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5741 (* 1 = 6.5741 loss)
I0524 05:26:42.636528 11835 sgd_solver.cpp:112] Iteration 38820, lr = 0.1
I0524 05:26:50.708171 11835 solver.cpp:239] Iteration 38830 (1.23893 iter/s, 8.07146s/10 iters), loss = 6.13429
I0524 05:26:50.708227 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13429 (* 1 = 6.13429 loss)
I0524 05:26:50.708462 11835 sgd_solver.cpp:112] Iteration 38830, lr = 0.1
I0524 05:26:57.387518 11835 solver.cpp:239] Iteration 38840 (1.49722 iter/s, 6.67903s/10 iters), loss = 5.91379
I0524 05:26:57.387567 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91379 (* 1 = 5.91379 loss)
I0524 05:26:57.387583 11835 sgd_solver.cpp:112] Iteration 38840, lr = 0.1
I0524 05:27:05.280704 11835 solver.cpp:239] Iteration 38850 (1.26702 iter/s, 7.89252s/10 iters), loss = 5.60958
I0524 05:27:05.280982 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.60958 (* 1 = 5.60958 loss)
I0524 05:27:05.282012 11835 sgd_solver.cpp:112] Iteration 38850, lr = 0.1
I0524 05:27:13.022361 11835 solver.cpp:239] Iteration 38860 (1.2918 iter/s, 7.74112s/10 iters), loss = 6.23607
I0524 05:27:13.022398 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23607 (* 1 = 6.23607 loss)
I0524 05:27:13.022500 11835 sgd_solver.cpp:112] Iteration 38860, lr = 0.1
I0524 05:27:21.146893 11835 solver.cpp:239] Iteration 38870 (1.23089 iter/s, 8.12417s/10 iters), loss = 5.81574
I0524 05:27:21.146947 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81574 (* 1 = 5.81574 loss)
I0524 05:27:21.146966 11835 sgd_solver.cpp:112] Iteration 38870, lr = 0.1
I0524 05:27:29.832778 11835 solver.cpp:239] Iteration 38880 (1.15134 iter/s, 8.6855s/10 iters), loss = 5.88305
I0524 05:27:29.832826 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88305 (* 1 = 5.88305 loss)
I0524 05:27:29.833113 11835 sgd_solver.cpp:112] Iteration 38880, lr = 0.1
I0524 05:27:38.170068 11835 solver.cpp:239] Iteration 38890 (1.19948 iter/s, 8.33691s/10 iters), loss = 6.11289
I0524 05:27:38.170233 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11289 (* 1 = 6.11289 loss)
I0524 05:27:38.386842 11835 sgd_solver.cpp:112] Iteration 38890, lr = 0.1
I0524 05:27:44.890352 11835 solver.cpp:239] Iteration 38900 (1.48813 iter/s, 6.71986s/10 iters), loss = 6.25788
I0524 05:27:44.890403 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25788 (* 1 = 6.25788 loss)
I0524 05:27:44.890420 11835 sgd_solver.cpp:112] Iteration 38900, lr = 0.1
I0524 05:27:53.570607 11835 solver.cpp:239] Iteration 38910 (1.15209 iter/s, 8.67986s/10 iters), loss = 5.66516
I0524 05:27:53.570662 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.66516 (* 1 = 5.66516 loss)
I0524 05:27:53.584900 11835 sgd_solver.cpp:112] Iteration 38910, lr = 0.1
I0524 05:28:00.666589 11835 solver.cpp:239] Iteration 38920 (1.40931 iter/s, 7.09566s/10 iters), loss = 7.06277
I0524 05:28:00.666632 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.06277 (* 1 = 7.06277 loss)
I0524 05:28:00.666646 11835 sgd_solver.cpp:112] Iteration 38920, lr = 0.1
I0524 05:28:08.564074 11835 solver.cpp:239] Iteration 38930 (1.26632 iter/s, 7.89687s/10 iters), loss = 6.43024
I0524 05:28:08.564317 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43024 (* 1 = 6.43024 loss)
I0524 05:28:09.057282 11835 sgd_solver.cpp:112] Iteration 38930, lr = 0.1
I0524 05:28:18.202258 11835 solver.cpp:239] Iteration 38940 (1.0376 iter/s, 9.63761s/10 iters), loss = 6.35415
I0524 05:28:18.202316 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35415 (* 1 = 6.35415 loss)
I0524 05:28:18.202543 11835 sgd_solver.cpp:112] Iteration 38940, lr = 0.1
I0524 05:28:24.876771 11835 solver.cpp:239] Iteration 38950 (1.49831 iter/s, 6.6742s/10 iters), loss = 6.31777
I0524 05:28:24.876814 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31777 (* 1 = 6.31777 loss)
I0524 05:28:24.876992 11835 sgd_solver.cpp:112] Iteration 38950, lr = 0.1
I0524 05:28:31.139636 11835 solver.cpp:239] Iteration 38960 (1.59679 iter/s, 6.26257s/10 iters), loss = 7.15667
I0524 05:28:31.139683 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.15667 (* 1 = 7.15667 loss)
I0524 05:28:31.400132 11835 sgd_solver.cpp:112] Iteration 38960, lr = 0.1
I0524 05:28:40.099597 11835 solver.cpp:239] Iteration 38970 (1.11613 iter/s, 8.95957s/10 iters), loss = 7.08837
I0524 05:28:40.099839 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.08837 (* 1 = 7.08837 loss)
I0524 05:28:40.175530 11835 sgd_solver.cpp:112] Iteration 38970, lr = 0.1
I0524 05:28:46.847291 11835 solver.cpp:239] Iteration 38980 (1.48209 iter/s, 6.74721s/10 iters), loss = 6.90238
I0524 05:28:46.847347 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90238 (* 1 = 6.90238 loss)
I0524 05:28:46.847476 11835 sgd_solver.cpp:112] Iteration 38980, lr = 0.1
I0524 05:28:53.295985 11835 solver.cpp:239] Iteration 38990 (1.55078 iter/s, 6.44838s/10 iters), loss = 5.62488
I0524 05:28:53.296041 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.62488 (* 1 = 5.62488 loss)
I0524 05:28:53.296162 11835 sgd_solver.cpp:112] Iteration 38990, lr = 0.1
I0524 05:29:00.011797 11835 solver.cpp:239] Iteration 39000 (1.4891 iter/s, 6.71548s/10 iters), loss = 5.72334
I0524 05:29:00.011875 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72334 (* 1 = 5.72334 loss)
I0524 05:29:00.037854 11835 sgd_solver.cpp:112] Iteration 39000, lr = 0.1
I0524 05:29:06.550761 11835 solver.cpp:239] Iteration 39010 (1.52937 iter/s, 6.53863s/10 iters), loss = 5.45575
I0524 05:29:06.550822 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.45575 (* 1 = 5.45575 loss)
I0524 05:29:06.551257 11835 sgd_solver.cpp:112] Iteration 39010, lr = 0.1
I0524 05:29:13.797405 11835 solver.cpp:239] Iteration 39020 (1.38001 iter/s, 7.24631s/10 iters), loss = 6.19003
I0524 05:29:13.797655 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19003 (* 1 = 6.19003 loss)
I0524 05:29:14.831478 11835 sgd_solver.cpp:112] Iteration 39020, lr = 0.1
I0524 05:29:22.134155 11835 solver.cpp:239] Iteration 39030 (1.19959 iter/s, 8.33621s/10 iters), loss = 6.59373
I0524 05:29:22.134209 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59373 (* 1 = 6.59373 loss)
I0524 05:29:22.147137 11835 sgd_solver.cpp:112] Iteration 39030, lr = 0.1
I0524 05:29:29.544657 11835 solver.cpp:239] Iteration 39040 (1.3495 iter/s, 7.41014s/10 iters), loss = 6.7754
I0524 05:29:29.544730 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7754 (* 1 = 6.7754 loss)
I0524 05:29:29.544782 11835 sgd_solver.cpp:112] Iteration 39040, lr = 0.1
I0524 05:29:35.808174 11835 solver.cpp:239] Iteration 39050 (1.59662 iter/s, 6.26321s/10 iters), loss = 6.08487
I0524 05:29:35.808221 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08487 (* 1 = 6.08487 loss)
I0524 05:29:35.808472 11835 sgd_solver.cpp:112] Iteration 39050, lr = 0.1
I0524 05:29:42.914535 11835 solver.cpp:239] Iteration 39060 (1.40725 iter/s, 7.10603s/10 iters), loss = 6.28942
I0524 05:29:42.914583 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.28942 (* 1 = 6.28942 loss)
I0524 05:29:42.914649 11835 sgd_solver.cpp:112] Iteration 39060, lr = 0.1
I0524 05:29:49.821069 11835 solver.cpp:239] Iteration 39070 (1.44797 iter/s, 6.90621s/10 iters), loss = 6.04083
I0524 05:29:49.821271 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04083 (* 1 = 6.04083 loss)
I0524 05:29:49.821318 11835 sgd_solver.cpp:112] Iteration 39070, lr = 0.1
I0524 05:29:59.530347 11835 solver.cpp:239] Iteration 39080 (1.03001 iter/s, 9.70863s/10 iters), loss = 6.81023
I0524 05:29:59.530433 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81023 (* 1 = 6.81023 loss)
I0524 05:29:59.530732 11835 sgd_solver.cpp:112] Iteration 39080, lr = 0.1
I0524 05:30:07.233433 11835 solver.cpp:239] Iteration 39090 (1.29824 iter/s, 7.70271s/10 iters), loss = 6.88738
I0524 05:30:07.233494 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88738 (* 1 = 6.88738 loss)
I0524 05:30:07.233860 11835 sgd_solver.cpp:112] Iteration 39090, lr = 0.1
I0524 05:30:15.178675 11835 solver.cpp:239] Iteration 39100 (1.25867 iter/s, 7.94489s/10 iters), loss = 6.42133
I0524 05:30:15.178733 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42133 (* 1 = 6.42133 loss)
I0524 05:30:15.178779 11835 sgd_solver.cpp:112] Iteration 39100, lr = 0.1
I0524 05:30:23.561543 11835 solver.cpp:239] Iteration 39110 (1.19297 iter/s, 8.38247s/10 iters), loss = 5.77397
I0524 05:30:23.561797 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.77397 (* 1 = 5.77397 loss)
I0524 05:30:24.113656 11835 sgd_solver.cpp:112] Iteration 39110, lr = 0.1
I0524 05:30:30.252091 11835 solver.cpp:239] Iteration 39120 (1.49475 iter/s, 6.69009s/10 iters), loss = 6.42848
I0524 05:30:30.252131 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42848 (* 1 = 6.42848 loss)
I0524 05:30:30.253121 11835 sgd_solver.cpp:112] Iteration 39120, lr = 0.1
I0524 05:30:38.037060 11835 solver.cpp:239] Iteration 39130 (1.28458 iter/s, 7.78462s/10 iters), loss = 6.29156
I0524 05:30:38.037111 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29156 (* 1 = 6.29156 loss)
I0524 05:30:38.037125 11835 sgd_solver.cpp:112] Iteration 39130, lr = 0.1
I0524 05:30:44.191644 11835 solver.cpp:239] Iteration 39140 (1.62547 iter/s, 6.15207s/10 iters), loss = 6.98015
I0524 05:30:44.191697 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98015 (* 1 = 6.98015 loss)
I0524 05:30:44.191834 11835 sgd_solver.cpp:112] Iteration 39140, lr = 0.1
I0524 05:30:51.152803 11835 solver.cpp:239] Iteration 39150 (1.43661 iter/s, 6.96082s/10 iters), loss = 6.30492
I0524 05:30:51.152863 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30492 (* 1 = 6.30492 loss)
I0524 05:30:51.152956 11835 sgd_solver.cpp:112] Iteration 39150, lr = 0.1
I0524 05:30:58.006558 11835 solver.cpp:239] Iteration 39160 (1.45912 iter/s, 6.85344s/10 iters), loss = 5.96214
I0524 05:30:58.006790 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96214 (* 1 = 5.96214 loss)
I0524 05:30:58.018859 11835 sgd_solver.cpp:112] Iteration 39160, lr = 0.1
I0524 05:31:04.450886 11835 solver.cpp:239] Iteration 39170 (1.55187 iter/s, 6.44384s/10 iters), loss = 7.42724
I0524 05:31:04.450937 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.42724 (* 1 = 7.42724 loss)
I0524 05:31:04.450953 11835 sgd_solver.cpp:112] Iteration 39170, lr = 0.1
I0524 05:31:10.859992 11835 solver.cpp:239] Iteration 39180 (1.56038 iter/s, 6.40871s/10 iters), loss = 6.65418
I0524 05:31:10.860039 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65418 (* 1 = 6.65418 loss)
I0524 05:31:10.860054 11835 sgd_solver.cpp:112] Iteration 39180, lr = 0.1
I0524 05:31:19.937603 11835 solver.cpp:239] Iteration 39190 (1.10193 iter/s, 9.07499s/10 iters), loss = 6.4455
I0524 05:31:19.937661 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4455 (* 1 = 6.4455 loss)
I0524 05:31:20.663892 11835 sgd_solver.cpp:112] Iteration 39190, lr = 0.1
I0524 05:31:29.013265 11835 solver.cpp:239] Iteration 39200 (1.1019 iter/s, 9.07525s/10 iters), loss = 5.84158
I0524 05:31:29.013511 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84158 (* 1 = 5.84158 loss)
I0524 05:31:29.013556 11835 sgd_solver.cpp:112] Iteration 39200, lr = 0.1
I0524 05:31:34.925740 11835 solver.cpp:239] Iteration 39210 (1.69147 iter/s, 5.91201s/10 iters), loss = 6.76776
I0524 05:31:34.925840 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76776 (* 1 = 6.76776 loss)
I0524 05:31:34.925915 11835 sgd_solver.cpp:112] Iteration 39210, lr = 0.1
I0524 05:31:41.480610 11835 solver.cpp:239] Iteration 39220 (1.52566 iter/s, 6.55456s/10 iters), loss = 6.04074
I0524 05:31:41.480666 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04074 (* 1 = 6.04074 loss)
I0524 05:31:41.554306 11835 sgd_solver.cpp:112] Iteration 39220, lr = 0.1
I0524 05:31:48.310358 11835 solver.cpp:239] Iteration 39230 (1.46425 iter/s, 6.82942s/10 iters), loss = 6.25409
I0524 05:31:48.310410 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25409 (* 1 = 6.25409 loss)
I0524 05:31:48.381011 11835 sgd_solver.cpp:112] Iteration 39230, lr = 0.1
I0524 05:31:54.501116 11835 solver.cpp:239] Iteration 39240 (1.61538 iter/s, 6.19047s/10 iters), loss = 5.69751
I0524 05:31:54.501155 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.69751 (* 1 = 5.69751 loss)
I0524 05:31:54.995438 11835 sgd_solver.cpp:112] Iteration 39240, lr = 0.1
I0524 05:32:02.669872 11835 solver.cpp:239] Iteration 39250 (1.22423 iter/s, 8.16838s/10 iters), loss = 5.77596
I0524 05:32:02.670223 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.77596 (* 1 = 5.77596 loss)
I0524 05:32:02.687192 11835 sgd_solver.cpp:112] Iteration 39250, lr = 0.1
I0524 05:32:10.624960 11835 solver.cpp:239] Iteration 39260 (1.25716 iter/s, 7.95446s/10 iters), loss = 5.88633
I0524 05:32:10.625018 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88633 (* 1 = 5.88633 loss)
I0524 05:32:10.728404 11835 sgd_solver.cpp:112] Iteration 39260, lr = 0.1
I0524 05:32:18.821709 11835 solver.cpp:239] Iteration 39270 (1.22005 iter/s, 8.19637s/10 iters), loss = 7.18721
I0524 05:32:18.821774 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.18721 (* 1 = 7.18721 loss)
I0524 05:32:18.821794 11835 sgd_solver.cpp:112] Iteration 39270, lr = 0.1
I0524 05:32:25.448572 11835 solver.cpp:239] Iteration 39280 (1.50908 iter/s, 6.62654s/10 iters), loss = 6.46463
I0524 05:32:25.448639 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46463 (* 1 = 6.46463 loss)
I0524 05:32:25.459883 11835 sgd_solver.cpp:112] Iteration 39280, lr = 0.1
I0524 05:32:33.309034 11835 solver.cpp:239] Iteration 39290 (1.27225 iter/s, 7.86009s/10 iters), loss = 5.97502
I0524 05:32:33.309355 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97502 (* 1 = 5.97502 loss)
I0524 05:32:33.390712 11835 sgd_solver.cpp:112] Iteration 39290, lr = 0.1
I0524 05:32:39.277776 11835 solver.cpp:239] Iteration 39300 (1.67551 iter/s, 5.96833s/10 iters), loss = 6.89279
I0524 05:32:39.277817 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89279 (* 1 = 6.89279 loss)
I0524 05:32:39.277828 11835 sgd_solver.cpp:112] Iteration 39300, lr = 0.1
I0524 05:32:45.888722 11835 solver.cpp:239] Iteration 39310 (1.51271 iter/s, 6.61064s/10 iters), loss = 6.74282
I0524 05:32:45.888777 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74282 (* 1 = 6.74282 loss)
I0524 05:32:45.888794 11835 sgd_solver.cpp:112] Iteration 39310, lr = 0.1
I0524 05:32:53.322955 11835 solver.cpp:239] Iteration 39320 (1.3452 iter/s, 7.43383s/10 iters), loss = 6.79488
I0524 05:32:53.323068 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79488 (* 1 = 6.79488 loss)
I0524 05:32:53.478269 11835 sgd_solver.cpp:112] Iteration 39320, lr = 0.1
I0524 05:32:59.517490 11835 solver.cpp:239] Iteration 39330 (1.6144 iter/s, 6.19424s/10 iters), loss = 5.19162
I0524 05:32:59.517554 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.19162 (* 1 = 5.19162 loss)
I0524 05:32:59.528854 11835 sgd_solver.cpp:112] Iteration 39330, lr = 0.1
I0524 05:33:05.764292 11835 solver.cpp:239] Iteration 39340 (1.6009 iter/s, 6.24649s/10 iters), loss = 5.7052
I0524 05:33:05.764526 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7052 (* 1 = 5.7052 loss)
I0524 05:33:05.764560 11835 sgd_solver.cpp:112] Iteration 39340, lr = 0.1
I0524 05:33:11.465474 11835 solver.cpp:239] Iteration 39350 (1.7548 iter/s, 5.69864s/10 iters), loss = 5.50119
I0524 05:33:11.465515 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.50119 (* 1 = 5.50119 loss)
I0524 05:33:11.465528 11835 sgd_solver.cpp:112] Iteration 39350, lr = 0.1
I0524 05:33:17.218565 11835 solver.cpp:239] Iteration 39360 (1.7383 iter/s, 5.75275s/10 iters), loss = 6.05968
I0524 05:33:17.218629 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05968 (* 1 = 6.05968 loss)
I0524 05:33:17.218811 11835 sgd_solver.cpp:112] Iteration 39360, lr = 0.1
I0524 05:33:24.185487 11835 solver.cpp:239] Iteration 39370 (1.43542 iter/s, 6.96661s/10 iters), loss = 5.11037
I0524 05:33:24.185524 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.11037 (* 1 = 5.11037 loss)
I0524 05:33:24.235282 11835 sgd_solver.cpp:112] Iteration 39370, lr = 0.1
I0524 05:33:33.017107 11835 solver.cpp:239] Iteration 39380 (1.13234 iter/s, 8.83123s/10 iters), loss = 7.52181
I0524 05:33:33.017164 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.52181 (* 1 = 7.52181 loss)
I0524 05:33:34.154620 11835 sgd_solver.cpp:112] Iteration 39380, lr = 0.1
I0524 05:33:40.802611 11835 solver.cpp:239] Iteration 39390 (1.2845 iter/s, 7.78515s/10 iters), loss = 6.20299
I0524 05:33:40.802872 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20299 (* 1 = 6.20299 loss)
I0524 05:33:40.802906 11835 sgd_solver.cpp:112] Iteration 39390, lr = 0.1
I0524 05:33:48.666229 11835 solver.cpp:239] Iteration 39400 (1.27181 iter/s, 7.8628s/10 iters), loss = 5.732
I0524 05:33:48.666286 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.732 (* 1 = 5.732 loss)
I0524 05:33:48.666303 11835 sgd_solver.cpp:112] Iteration 39400, lr = 0.1
I0524 05:33:59.816540 11835 solver.cpp:239] Iteration 39410 (0.896924 iter/s, 11.1492s/10 iters), loss = 5.93985
I0524 05:33:59.816596 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93985 (* 1 = 5.93985 loss)
I0524 05:33:59.961683 11835 sgd_solver.cpp:112] Iteration 39410, lr = 0.1
I0524 05:34:06.043264 11835 solver.cpp:239] Iteration 39420 (1.60606 iter/s, 6.22643s/10 iters), loss = 5.25138
I0524 05:34:06.043314 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.25138 (* 1 = 5.25138 loss)
I0524 05:34:06.043550 11835 sgd_solver.cpp:112] Iteration 39420, lr = 0.1
I0524 05:34:11.876492 11835 solver.cpp:239] Iteration 39430 (1.7144 iter/s, 5.83294s/10 iters), loss = 6.29892
I0524 05:34:11.876782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29892 (* 1 = 6.29892 loss)
I0524 05:34:12.662639 11835 sgd_solver.cpp:112] Iteration 39430, lr = 0.1
I0524 05:34:19.240301 11835 solver.cpp:239] Iteration 39440 (1.35809 iter/s, 7.36326s/10 iters), loss = 5.51024
I0524 05:34:19.240356 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.51024 (* 1 = 5.51024 loss)
I0524 05:34:19.240568 11835 sgd_solver.cpp:112] Iteration 39440, lr = 0.1
I0524 05:34:28.916540 11835 solver.cpp:239] Iteration 39450 (1.0335 iter/s, 9.67582s/10 iters), loss = 7.01761
I0524 05:34:28.916594 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01761 (* 1 = 7.01761 loss)
I0524 05:34:28.916676 11835 sgd_solver.cpp:112] Iteration 39450, lr = 0.1
I0524 05:34:39.698115 11835 solver.cpp:239] Iteration 39460 (0.92755 iter/s, 10.7811s/10 iters), loss = 7.08792
I0524 05:34:39.698205 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.08792 (* 1 = 7.08792 loss)
I0524 05:34:39.698261 11835 sgd_solver.cpp:112] Iteration 39460, lr = 0.1
I0524 05:34:49.316026 11835 solver.cpp:239] Iteration 39470 (1.03978 iter/s, 9.61746s/10 iters), loss = 6.82465
I0524 05:34:49.316182 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.82465 (* 1 = 6.82465 loss)
I0524 05:34:49.468909 11835 sgd_solver.cpp:112] Iteration 39470, lr = 0.1
I0524 05:34:55.804049 11835 solver.cpp:239] Iteration 39480 (1.5414 iter/s, 6.48762s/10 iters), loss = 6.33254
I0524 05:34:55.804093 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33254 (* 1 = 6.33254 loss)
I0524 05:34:56.239840 11835 sgd_solver.cpp:112] Iteration 39480, lr = 0.1
I0524 05:35:04.276994 11835 solver.cpp:239] Iteration 39490 (1.18028 iter/s, 8.47257s/10 iters), loss = 5.65708
I0524 05:35:04.277045 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.65708 (* 1 = 5.65708 loss)
I0524 05:35:04.361315 11835 sgd_solver.cpp:112] Iteration 39490, lr = 0.1
I0524 05:35:10.256166 11835 solver.cpp:239] Iteration 39500 (1.67255 iter/s, 5.97889s/10 iters), loss = 5.96112
I0524 05:35:10.256217 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96112 (* 1 = 5.96112 loss)
I0524 05:35:10.256233 11835 sgd_solver.cpp:112] Iteration 39500, lr = 0.1
I0524 05:35:16.515838 11835 solver.cpp:239] Iteration 39510 (1.5976 iter/s, 6.25938s/10 iters), loss = 5.81927
I0524 05:35:16.515888 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81927 (* 1 = 5.81927 loss)
I0524 05:35:16.515950 11835 sgd_solver.cpp:112] Iteration 39510, lr = 0.1
I0524 05:35:22.494333 11835 solver.cpp:239] Iteration 39520 (1.67275 iter/s, 5.97817s/10 iters), loss = 7.245
I0524 05:35:22.494568 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.245 (* 1 = 7.245 loss)
I0524 05:35:22.495503 11835 sgd_solver.cpp:112] Iteration 39520, lr = 0.1
I0524 05:35:28.239526 11835 solver.cpp:239] Iteration 39530 (1.74071 iter/s, 5.74477s/10 iters), loss = 6.19323
I0524 05:35:28.239560 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19323 (* 1 = 6.19323 loss)
I0524 05:35:28.239574 11835 sgd_solver.cpp:112] Iteration 39530, lr = 0.1
I0524 05:35:35.603713 11835 solver.cpp:239] Iteration 39540 (1.35798 iter/s, 7.36386s/10 iters), loss = 6.29537
I0524 05:35:35.603761 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29537 (* 1 = 6.29537 loss)
I0524 05:35:35.603849 11835 sgd_solver.cpp:112] Iteration 39540, lr = 0.1
I0524 05:35:43.978803 11835 solver.cpp:239] Iteration 39550 (1.19407 iter/s, 8.37472s/10 iters), loss = 6.29517
I0524 05:35:43.978857 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29517 (* 1 = 6.29517 loss)
I0524 05:35:43.979058 11835 sgd_solver.cpp:112] Iteration 39550, lr = 0.1
I0524 05:35:53.916688 11835 solver.cpp:239] Iteration 39560 (1.00629 iter/s, 9.93746s/10 iters), loss = 5.78438
I0524 05:35:53.916962 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78438 (* 1 = 5.78438 loss)
I0524 05:35:53.917009 11835 sgd_solver.cpp:112] Iteration 39560, lr = 0.1
I0524 05:36:05.237934 11835 solver.cpp:239] Iteration 39570 (0.883382 iter/s, 11.3201s/10 iters), loss = 6.93203
I0524 05:36:05.237987 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93203 (* 1 = 6.93203 loss)
I0524 05:36:06.398242 11835 sgd_solver.cpp:112] Iteration 39570, lr = 0.1
I0524 05:36:13.084283 11835 solver.cpp:239] Iteration 39580 (1.27454 iter/s, 7.84598s/10 iters), loss = 6.63919
I0524 05:36:13.084343 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63919 (* 1 = 6.63919 loss)
I0524 05:36:13.084477 11835 sgd_solver.cpp:112] Iteration 39580, lr = 0.1
I0524 05:36:20.138972 11835 solver.cpp:239] Iteration 39590 (1.41756 iter/s, 7.05436s/10 iters), loss = 5.99546
I0524 05:36:20.139014 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99546 (* 1 = 5.99546 loss)
I0524 05:36:20.139155 11835 sgd_solver.cpp:112] Iteration 39590, lr = 0.1
I0524 05:36:25.843152 11835 solver.cpp:239] Iteration 39600 (1.75318 iter/s, 5.70391s/10 iters), loss = 5.84069
I0524 05:36:25.843351 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84069 (* 1 = 5.84069 loss)
I0524 05:36:25.843443 11835 sgd_solver.cpp:112] Iteration 39600, lr = 0.1
I0524 05:36:31.791348 11835 solver.cpp:239] Iteration 39610 (1.68129 iter/s, 5.9478s/10 iters), loss = 5.65993
I0524 05:36:31.791390 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.65993 (* 1 = 5.65993 loss)
I0524 05:36:32.477059 11835 sgd_solver.cpp:112] Iteration 39610, lr = 0.1
I0524 05:36:38.596657 11835 solver.cpp:239] Iteration 39620 (1.46951 iter/s, 6.80499s/10 iters), loss = 7.10443
I0524 05:36:38.596807 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10443 (* 1 = 7.10443 loss)
I0524 05:36:38.934911 11835 sgd_solver.cpp:112] Iteration 39620, lr = 0.1
I0524 05:36:47.679817 11835 solver.cpp:239] Iteration 39630 (1.10099 iter/s, 9.08275s/10 iters), loss = 5.73007
I0524 05:36:47.679864 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73007 (* 1 = 5.73007 loss)
I0524 05:36:47.680022 11835 sgd_solver.cpp:112] Iteration 39630, lr = 0.1
I0524 05:36:55.355026 11835 solver.cpp:239] Iteration 39640 (1.30296 iter/s, 7.67486s/10 iters), loss = 6.67179
I0524 05:36:55.355079 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67179 (* 1 = 6.67179 loss)
I0524 05:36:56.402421 11835 sgd_solver.cpp:112] Iteration 39640, lr = 0.1
I0524 05:37:03.625813 11835 solver.cpp:239] Iteration 39650 (1.20913 iter/s, 8.27041s/10 iters), loss = 4.88763
I0524 05:37:03.625948 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.88763 (* 1 = 4.88763 loss)
I0524 05:37:03.627811 11835 sgd_solver.cpp:112] Iteration 39650, lr = 0.1
I0524 05:37:10.372512 11835 solver.cpp:239] Iteration 39660 (1.48228 iter/s, 6.74636s/10 iters), loss = 6.25013
I0524 05:37:10.372571 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25013 (* 1 = 6.25013 loss)
I0524 05:37:11.223961 11835 sgd_solver.cpp:112] Iteration 39660, lr = 0.1
I0524 05:37:18.735432 11835 solver.cpp:239] Iteration 39670 (1.19581 iter/s, 8.36254s/10 iters), loss = 6.36996
I0524 05:37:18.735489 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36996 (* 1 = 6.36996 loss)
I0524 05:37:18.735592 11835 sgd_solver.cpp:112] Iteration 39670, lr = 0.1
I0524 05:37:26.695304 11835 solver.cpp:239] Iteration 39680 (1.25636 iter/s, 7.9595s/10 iters), loss = 6.34717
I0524 05:37:26.695540 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34717 (* 1 = 6.34717 loss)
I0524 05:37:26.695570 11835 sgd_solver.cpp:112] Iteration 39680, lr = 0.1
I0524 05:37:36.387617 11835 solver.cpp:239] Iteration 39690 (1.03181 iter/s, 9.69171s/10 iters), loss = 6.8139
I0524 05:37:36.387670 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8139 (* 1 = 6.8139 loss)
I0524 05:37:37.065474 11835 sgd_solver.cpp:112] Iteration 39690, lr = 0.1
I0524 05:37:44.744379 11835 solver.cpp:239] Iteration 39700 (1.19669 iter/s, 8.35639s/10 iters), loss = 6.13805
I0524 05:37:44.744426 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13805 (* 1 = 6.13805 loss)
I0524 05:37:44.810595 11835 sgd_solver.cpp:112] Iteration 39700, lr = 0.1
I0524 05:37:51.770402 11835 solver.cpp:239] Iteration 39710 (1.42335 iter/s, 7.02567s/10 iters), loss = 6.12193
I0524 05:37:51.770467 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12193 (* 1 = 6.12193 loss)
I0524 05:37:52.379789 11835 sgd_solver.cpp:112] Iteration 39710, lr = 0.1
I0524 05:38:00.663090 11835 solver.cpp:239] Iteration 39720 (1.12457 iter/s, 8.89229s/10 iters), loss = 6.70308
I0524 05:38:00.663378 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70308 (* 1 = 6.70308 loss)
I0524 05:38:00.767221 11835 sgd_solver.cpp:112] Iteration 39720, lr = 0.1
I0524 05:38:06.901962 11835 solver.cpp:239] Iteration 39730 (1.60296 iter/s, 6.23847s/10 iters), loss = 5.63459
I0524 05:38:06.902007 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63459 (* 1 = 5.63459 loss)
I0524 05:38:06.902025 11835 sgd_solver.cpp:112] Iteration 39730, lr = 0.1
I0524 05:38:14.898720 11835 solver.cpp:239] Iteration 39740 (1.25057 iter/s, 7.99636s/10 iters), loss = 5.69956
I0524 05:38:14.898787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.69956 (* 1 = 5.69956 loss)
I0524 05:38:14.898977 11835 sgd_solver.cpp:112] Iteration 39740, lr = 0.1
I0524 05:38:21.235430 11835 solver.cpp:239] Iteration 39750 (1.57818 iter/s, 6.33639s/10 iters), loss = 5.8852
I0524 05:38:21.235539 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8852 (* 1 = 5.8852 loss)
I0524 05:38:21.235594 11835 sgd_solver.cpp:112] Iteration 39750, lr = 0.1
I0524 05:38:27.703552 11835 solver.cpp:239] Iteration 39760 (1.54612 iter/s, 6.46782s/10 iters), loss = 6.47053
I0524 05:38:27.703598 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.47053 (* 1 = 6.47053 loss)
I0524 05:38:27.703860 11835 sgd_solver.cpp:112] Iteration 39760, lr = 0.1
I0524 05:38:33.715456 11835 solver.cpp:239] Iteration 39770 (1.66345 iter/s, 6.01161s/10 iters), loss = 6.38828
I0524 05:38:33.715710 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38828 (* 1 = 6.38828 loss)
I0524 05:38:34.031520 11835 sgd_solver.cpp:112] Iteration 39770, lr = 0.1
I0524 05:38:41.181264 11835 solver.cpp:239] Iteration 39780 (1.33953 iter/s, 7.46532s/10 iters), loss = 6.11808
I0524 05:38:41.181313 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11808 (* 1 = 6.11808 loss)
I0524 05:38:41.181352 11835 sgd_solver.cpp:112] Iteration 39780, lr = 0.1
I0524 05:38:49.163676 11835 solver.cpp:239] Iteration 39790 (1.25281 iter/s, 7.98205s/10 iters), loss = 5.18997
I0524 05:38:49.163741 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.18997 (* 1 = 5.18997 loss)
I0524 05:38:49.163841 11835 sgd_solver.cpp:112] Iteration 39790, lr = 0.1
I0524 05:38:55.641417 11835 solver.cpp:239] Iteration 39800 (1.54382 iter/s, 6.47743s/10 iters), loss = 5.7655
I0524 05:38:55.641487 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7655 (* 1 = 5.7655 loss)
I0524 05:38:55.641587 11835 sgd_solver.cpp:112] Iteration 39800, lr = 0.1
I0524 05:39:02.022583 11835 solver.cpp:239] Iteration 39810 (1.56718 iter/s, 6.38087s/10 iters), loss = 6.39853
I0524 05:39:02.022627 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39853 (* 1 = 6.39853 loss)
I0524 05:39:02.022640 11835 sgd_solver.cpp:112] Iteration 39810, lr = 0.1
I0524 05:39:08.180734 11835 solver.cpp:239] Iteration 39820 (1.62399 iter/s, 6.15767s/10 iters), loss = 5.75221
I0524 05:39:08.180891 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75221 (* 1 = 5.75221 loss)
I0524 05:39:08.180915 11835 sgd_solver.cpp:112] Iteration 39820, lr = 0.1
I0524 05:39:14.991837 11835 solver.cpp:239] Iteration 39830 (1.46872 iter/s, 6.80864s/10 iters), loss = 6.96625
I0524 05:39:14.991891 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96625 (* 1 = 6.96625 loss)
I0524 05:39:15.468892 11835 sgd_solver.cpp:112] Iteration 39830, lr = 0.1
I0524 05:39:22.961105 11835 solver.cpp:239] Iteration 39840 (1.25488 iter/s, 7.9689s/10 iters), loss = 7.10482
I0524 05:39:22.961164 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10482 (* 1 = 7.10482 loss)
I0524 05:39:22.961221 11835 sgd_solver.cpp:112] Iteration 39840, lr = 0.1
I0524 05:39:29.112864 11835 solver.cpp:239] Iteration 39850 (1.62563 iter/s, 6.15147s/10 iters), loss = 5.16754
I0524 05:39:29.112921 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.16754 (* 1 = 5.16754 loss)
I0524 05:39:29.112933 11835 sgd_solver.cpp:112] Iteration 39850, lr = 0.1
I0524 05:39:36.724644 11835 solver.cpp:239] Iteration 39860 (1.31381 iter/s, 7.61143s/10 iters), loss = 6.12721
I0524 05:39:36.724699 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12721 (* 1 = 6.12721 loss)
I0524 05:39:36.724930 11835 sgd_solver.cpp:112] Iteration 39860, lr = 0.1
I0524 05:39:44.097110 11835 solver.cpp:239] Iteration 39870 (1.35646 iter/s, 7.37212s/10 iters), loss = 6.36229
I0524 05:39:44.097322 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36229 (* 1 = 6.36229 loss)
I0524 05:39:44.097363 11835 sgd_solver.cpp:112] Iteration 39870, lr = 0.1
I0524 05:39:52.230229 11835 solver.cpp:239] Iteration 39880 (1.22966 iter/s, 8.13231s/10 iters), loss = 6.19425
I0524 05:39:52.230279 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19425 (* 1 = 6.19425 loss)
I0524 05:39:52.260493 11835 sgd_solver.cpp:112] Iteration 39880, lr = 0.1
I0524 05:39:58.527901 11835 solver.cpp:239] Iteration 39890 (1.58796 iter/s, 6.29738s/10 iters), loss = 5.93481
I0524 05:39:58.527945 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93481 (* 1 = 5.93481 loss)
I0524 05:39:59.293100 11835 sgd_solver.cpp:112] Iteration 39890, lr = 0.1
I0524 05:40:05.188313 11835 solver.cpp:239] Iteration 39900 (1.50148 iter/s, 6.66011s/10 iters), loss = 6.37355
I0524 05:40:05.188354 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37355 (* 1 = 6.37355 loss)
I0524 05:40:05.188367 11835 sgd_solver.cpp:112] Iteration 39900, lr = 0.1
I0524 05:40:12.186595 11835 solver.cpp:239] Iteration 39910 (1.42899 iter/s, 6.99797s/10 iters), loss = 7.22843
I0524 05:40:12.186646 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.22843 (* 1 = 7.22843 loss)
I0524 05:40:12.186663 11835 sgd_solver.cpp:112] Iteration 39910, lr = 0.1
I0524 05:40:21.241983 11835 solver.cpp:239] Iteration 39920 (1.10437 iter/s, 9.05491s/10 iters), loss = 5.95856
I0524 05:40:21.242233 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95856 (* 1 = 5.95856 loss)
I0524 05:40:21.242283 11835 sgd_solver.cpp:112] Iteration 39920, lr = 0.1
I0524 05:40:29.699836 11835 solver.cpp:239] Iteration 39930 (1.18241 iter/s, 8.45732s/10 iters), loss = 5.45751
I0524 05:40:29.699887 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.45751 (* 1 = 5.45751 loss)
I0524 05:40:29.699901 11835 sgd_solver.cpp:112] Iteration 39930, lr = 0.1
I0524 05:40:37.869089 11835 solver.cpp:239] Iteration 39940 (1.22416 iter/s, 8.16889s/10 iters), loss = 5.78123
I0524 05:40:37.869144 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78123 (* 1 = 5.78123 loss)
I0524 05:40:38.021842 11835 sgd_solver.cpp:112] Iteration 39940, lr = 0.1
I0524 05:40:44.581358 11835 solver.cpp:239] Iteration 39950 (1.48988 iter/s, 6.71194s/10 iters), loss = 6.8806
I0524 05:40:44.581425 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8806 (* 1 = 6.8806 loss)
I0524 05:40:44.581728 11835 sgd_solver.cpp:112] Iteration 39950, lr = 0.1
I0524 05:40:50.559993 11835 solver.cpp:239] Iteration 39960 (1.6727 iter/s, 5.97835s/10 iters), loss = 6.92051
I0524 05:40:50.560039 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92051 (* 1 = 6.92051 loss)
I0524 05:40:50.560458 11835 sgd_solver.cpp:112] Iteration 39960, lr = 0.1
I0524 05:40:57.196105 11835 solver.cpp:239] Iteration 39970 (1.50698 iter/s, 6.63578s/10 iters), loss = 6.23141
I0524 05:40:57.196310 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23141 (* 1 = 6.23141 loss)
I0524 05:40:57.738231 11835 sgd_solver.cpp:112] Iteration 39970, lr = 0.1
I0524 05:41:04.471567 11835 solver.cpp:239] Iteration 39980 (1.37457 iter/s, 7.275s/10 iters), loss = 6.71478
I0524 05:41:04.471635 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71478 (* 1 = 6.71478 loss)
I0524 05:41:04.471827 11835 sgd_solver.cpp:112] Iteration 39980, lr = 0.1
I0524 05:41:11.164021 11835 solver.cpp:239] Iteration 39990 (1.49429 iter/s, 6.69213s/10 iters), loss = 5.89621
I0524 05:41:11.164078 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89621 (* 1 = 5.89621 loss)
I0524 05:41:11.164197 11835 sgd_solver.cpp:112] Iteration 39990, lr = 0.1
I0524 05:41:16.892983 11835 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_40000.caffemodel
I0524 05:41:18.438012 11835 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_40000.solverstate
I0524 05:41:19.050545 11835 solver.cpp:239] Iteration 40000 (1.26804 iter/s, 7.88617s/10 iters), loss = 6.45134
I0524 05:41:19.050587 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45134 (* 1 = 6.45134 loss)
I0524 05:41:19.050729 11835 sgd_solver.cpp:112] Iteration 40000, lr = 0.1
I0524 05:41:26.851693 11835 solver.cpp:239] Iteration 40010 (1.28192 iter/s, 7.80078s/10 iters), loss = 6.33338
I0524 05:41:26.851778 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33338 (* 1 = 6.33338 loss)
I0524 05:41:26.860359 11835 sgd_solver.cpp:112] Iteration 40010, lr = 0.1
I0524 05:41:33.443354 11835 solver.cpp:239] Iteration 40020 (1.51714 iter/s, 6.59133s/10 iters), loss = 5.53891
I0524 05:41:33.443611 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.53891 (* 1 = 5.53891 loss)
I0524 05:41:33.466972 11835 sgd_solver.cpp:112] Iteration 40020, lr = 0.1
I0524 05:41:41.967725 11835 solver.cpp:239] Iteration 40030 (1.17317 iter/s, 8.52388s/10 iters), loss = 7.0781
I0524 05:41:41.967785 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.0781 (* 1 = 7.0781 loss)
I0524 05:41:42.060694 11835 sgd_solver.cpp:112] Iteration 40030, lr = 0.1
I0524 05:41:48.106855 11835 solver.cpp:239] Iteration 40040 (1.62897 iter/s, 6.13883s/10 iters), loss = 5.88254
I0524 05:41:48.106909 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88254 (* 1 = 5.88254 loss)
I0524 05:41:48.137161 11835 sgd_solver.cpp:112] Iteration 40040, lr = 0.1
I0524 05:41:55.545429 11835 solver.cpp:239] Iteration 40050 (1.34441 iter/s, 7.43823s/10 iters), loss = 6.2765
I0524 05:41:55.545545 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2765 (* 1 = 6.2765 loss)
I0524 05:41:55.545740 11835 sgd_solver.cpp:112] Iteration 40050, lr = 0.1
I0524 05:42:02.845343 11835 solver.cpp:239] Iteration 40060 (1.36994 iter/s, 7.29957s/10 iters), loss = 6.15226
I0524 05:42:02.845393 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15226 (* 1 = 6.15226 loss)
I0524 05:42:02.946375 11835 sgd_solver.cpp:112] Iteration 40060, lr = 0.1
I0524 05:42:09.034884 11835 solver.cpp:239] Iteration 40070 (1.6157 iter/s, 6.18926s/10 iters), loss = 7.39822
I0524 05:42:09.035188 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.39822 (* 1 = 7.39822 loss)
I0524 05:42:09.035235 11835 sgd_solver.cpp:112] Iteration 40070, lr = 0.1
I0524 05:42:17.282310 11835 solver.cpp:239] Iteration 40080 (1.21261 iter/s, 8.24665s/10 iters), loss = 7.2247
I0524 05:42:17.282364 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2247 (* 1 = 7.2247 loss)
I0524 05:42:17.282487 11835 sgd_solver.cpp:112] Iteration 40080, lr = 0.1
I0524 05:42:24.665863 11835 solver.cpp:239] Iteration 40090 (1.35442 iter/s, 7.38322s/10 iters), loss = 7.25402
I0524 05:42:24.665925 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.25402 (* 1 = 7.25402 loss)
I0524 05:42:25.489982 11835 sgd_solver.cpp:112] Iteration 40090, lr = 0.1
I0524 05:42:31.327630 11835 solver.cpp:239] Iteration 40100 (1.50117 iter/s, 6.66147s/10 iters), loss = 5.7719
I0524 05:42:31.327668 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7719 (* 1 = 5.7719 loss)
I0524 05:42:31.431778 11835 sgd_solver.cpp:112] Iteration 40100, lr = 0.1
I0524 05:42:40.221101 11835 solver.cpp:239] Iteration 40110 (1.12447 iter/s, 8.89309s/10 iters), loss = 6.40863
I0524 05:42:40.221246 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40863 (* 1 = 6.40863 loss)
I0524 05:42:40.221380 11835 sgd_solver.cpp:112] Iteration 40110, lr = 0.1
I0524 05:42:46.544174 11835 solver.cpp:239] Iteration 40120 (1.58161 iter/s, 6.32269s/10 iters), loss = 6.59871
I0524 05:42:46.544226 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59871 (* 1 = 6.59871 loss)
I0524 05:42:47.880738 11835 sgd_solver.cpp:112] Iteration 40120, lr = 0.1
I0524 05:42:58.777364 11835 solver.cpp:239] Iteration 40130 (0.817484 iter/s, 12.2327s/10 iters), loss = 6.98939
I0524 05:42:58.777441 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98939 (* 1 = 6.98939 loss)
I0524 05:42:58.792165 11835 sgd_solver.cpp:112] Iteration 40130, lr = 0.1
I0524 05:43:04.825665 11835 solver.cpp:239] Iteration 40140 (1.65345 iter/s, 6.04795s/10 iters), loss = 6.72404
I0524 05:43:04.825814 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72404 (* 1 = 6.72404 loss)
I0524 05:43:04.825844 11835 sgd_solver.cpp:112] Iteration 40140, lr = 0.1
I0524 05:43:10.803872 11835 solver.cpp:239] Iteration 40150 (1.67343 iter/s, 5.97576s/10 iters), loss = 6.16881
I0524 05:43:10.804093 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16881 (* 1 = 6.16881 loss)
I0524 05:43:11.791442 11835 sgd_solver.cpp:112] Iteration 40150, lr = 0.1
I0524 05:43:18.265010 11835 solver.cpp:239] Iteration 40160 (1.34037 iter/s, 7.46062s/10 iters), loss = 6.8121
I0524 05:43:18.265079 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8121 (* 1 = 6.8121 loss)
I0524 05:43:18.265092 11835 sgd_solver.cpp:112] Iteration 40160, lr = 0.1
I0524 05:43:25.337460 11835 solver.cpp:239] Iteration 40170 (1.41401 iter/s, 7.0721s/10 iters), loss = 5.34332
I0524 05:43:25.337571 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.34332 (* 1 = 5.34332 loss)
I0524 05:43:26.130704 11835 sgd_solver.cpp:112] Iteration 40170, lr = 0.1
I0524 05:43:32.865983 11835 solver.cpp:239] Iteration 40180 (1.32834 iter/s, 7.52817s/10 iters), loss = 7.0567
I0524 05:43:32.866040 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.0567 (* 1 = 7.0567 loss)
I0524 05:43:32.866339 11835 sgd_solver.cpp:112] Iteration 40180, lr = 0.1
I0524 05:43:41.182736 11835 solver.cpp:239] Iteration 40190 (1.20245 iter/s, 8.31638s/10 iters), loss = 5.72923
I0524 05:43:41.182977 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72923 (* 1 = 5.72923 loss)
I0524 05:43:42.169415 11835 sgd_solver.cpp:112] Iteration 40190, lr = 0.1
I0524 05:43:49.900570 11835 solver.cpp:239] Iteration 40200 (1.14715 iter/s, 8.71726s/10 iters), loss = 7.15097
I0524 05:43:49.900626 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.15097 (* 1 = 7.15097 loss)
I0524 05:43:49.900662 11835 sgd_solver.cpp:112] Iteration 40200, lr = 0.1
I0524 05:43:56.272724 11835 solver.cpp:239] Iteration 40210 (1.5694 iter/s, 6.37186s/10 iters), loss = 7.24215
I0524 05:43:56.272768 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.24215 (* 1 = 7.24215 loss)
I0524 05:43:56.273152 11835 sgd_solver.cpp:112] Iteration 40210, lr = 0.1
I0524 05:44:05.059098 11835 solver.cpp:239] Iteration 40220 (1.13818 iter/s, 8.78598s/10 iters), loss = 7.36135
I0524 05:44:05.059161 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.36135 (* 1 = 7.36135 loss)
I0524 05:44:05.366755 11835 sgd_solver.cpp:112] Iteration 40220, lr = 0.1
I0524 05:44:11.198114 11835 solver.cpp:239] Iteration 40230 (1.629 iter/s, 6.13873s/10 iters), loss = 6.79246
I0524 05:44:11.198284 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79246 (* 1 = 6.79246 loss)
I0524 05:44:11.198570 11835 sgd_solver.cpp:112] Iteration 40230, lr = 0.1
I0524 05:44:17.678272 11835 solver.cpp:239] Iteration 40240 (1.54327 iter/s, 6.47975s/10 iters), loss = 5.71542
I0524 05:44:17.678308 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71542 (* 1 = 5.71542 loss)
I0524 05:44:17.678493 11835 sgd_solver.cpp:112] Iteration 40240, lr = 0.1
I0524 05:44:23.632993 11835 solver.cpp:239] Iteration 40250 (1.67942 iter/s, 5.95444s/10 iters), loss = 6.1314
I0524 05:44:23.633049 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1314 (* 1 = 6.1314 loss)
I0524 05:44:23.633222 11835 sgd_solver.cpp:112] Iteration 40250, lr = 0.1
I0524 05:44:31.790421 11835 solver.cpp:239] Iteration 40260 (1.22593 iter/s, 8.15705s/10 iters), loss = 5.37046
I0524 05:44:31.790477 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.37046 (* 1 = 5.37046 loss)
I0524 05:44:31.790545 11835 sgd_solver.cpp:112] Iteration 40260, lr = 0.1
I0524 05:44:38.191586 11835 solver.cpp:239] Iteration 40270 (1.56229 iter/s, 6.40087s/10 iters), loss = 5.98118
I0524 05:44:38.191622 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98118 (* 1 = 5.98118 loss)
I0524 05:44:38.552616 11835 sgd_solver.cpp:112] Iteration 40270, lr = 0.1
I0524 05:44:46.139921 11835 solver.cpp:239] Iteration 40280 (1.25818 iter/s, 7.94798s/10 iters), loss = 6.77726
I0524 05:44:46.140061 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.77726 (* 1 = 6.77726 loss)
I0524 05:44:46.140384 11835 sgd_solver.cpp:112] Iteration 40280, lr = 0.1
I0524 05:44:52.619868 11835 solver.cpp:239] Iteration 40290 (1.54332 iter/s, 6.47955s/10 iters), loss = 6.7149
I0524 05:44:52.619928 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7149 (* 1 = 6.7149 loss)
I0524 05:44:52.620354 11835 sgd_solver.cpp:112] Iteration 40290, lr = 0.1
I0524 05:44:58.426033 11835 solver.cpp:239] Iteration 40300 (1.72239 iter/s, 5.80589s/10 iters), loss = 6.67854
I0524 05:44:58.426086 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67854 (* 1 = 6.67854 loss)
I0524 05:44:58.507580 11835 sgd_solver.cpp:112] Iteration 40300, lr = 0.1
I0524 05:45:04.595263 11835 solver.cpp:239] Iteration 40310 (1.62103 iter/s, 6.16893s/10 iters), loss = 5.21974
I0524 05:45:04.595311 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.21974 (* 1 = 5.21974 loss)
I0524 05:45:04.595327 11835 sgd_solver.cpp:112] Iteration 40310, lr = 0.1
I0524 05:45:13.639015 11835 solver.cpp:239] Iteration 40320 (1.1058 iter/s, 9.04326s/10 iters), loss = 6.25093
I0524 05:45:13.639097 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25093 (* 1 = 6.25093 loss)
I0524 05:45:13.639228 11835 sgd_solver.cpp:112] Iteration 40320, lr = 0.1
I0524 05:45:22.515445 11835 solver.cpp:239] Iteration 40330 (1.12663 iter/s, 8.87604s/10 iters), loss = 6.12274
I0524 05:45:22.515693 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12274 (* 1 = 6.12274 loss)
I0524 05:45:22.515714 11835 sgd_solver.cpp:112] Iteration 40330, lr = 0.1
I0524 05:45:29.434221 11835 solver.cpp:239] Iteration 40340 (1.44545 iter/s, 6.91825s/10 iters), loss = 6.67858
I0524 05:45:29.434275 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67858 (* 1 = 6.67858 loss)
I0524 05:45:29.436889 11835 sgd_solver.cpp:112] Iteration 40340, lr = 0.1
I0524 05:45:36.757908 11835 solver.cpp:239] Iteration 40350 (1.3655 iter/s, 7.32334s/10 iters), loss = 6.22449
I0524 05:45:36.757977 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22449 (* 1 = 6.22449 loss)
I0524 05:45:36.758111 11835 sgd_solver.cpp:112] Iteration 40350, lr = 0.1
I0524 05:45:46.114920 11835 solver.cpp:239] Iteration 40360 (1.06877 iter/s, 9.35659s/10 iters), loss = 5.23706
I0524 05:45:46.114979 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.23706 (* 1 = 5.23706 loss)
I0524 05:45:46.115303 11835 sgd_solver.cpp:112] Iteration 40360, lr = 0.1
I0524 05:45:54.338973 11835 solver.cpp:239] Iteration 40370 (1.216 iter/s, 8.22367s/10 iters), loss = 6.04604
I0524 05:45:54.339201 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04604 (* 1 = 6.04604 loss)
I0524 05:45:54.522399 11835 sgd_solver.cpp:112] Iteration 40370, lr = 0.1
I0524 05:46:00.485927 11835 solver.cpp:239] Iteration 40380 (1.62694 iter/s, 6.1465s/10 iters), loss = 6.15085
I0524 05:46:00.485985 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15085 (* 1 = 6.15085 loss)
I0524 05:46:01.308410 11835 sgd_solver.cpp:112] Iteration 40380, lr = 0.1
I0524 05:46:08.022316 11835 solver.cpp:239] Iteration 40390 (1.32695 iter/s, 7.53605s/10 iters), loss = 5.26839
I0524 05:46:08.022358 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.26839 (* 1 = 5.26839 loss)
I0524 05:46:08.022526 11835 sgd_solver.cpp:112] Iteration 40390, lr = 0.1
I0524 05:46:17.688030 11835 solver.cpp:239] Iteration 40400 (1.03463 iter/s, 9.6653s/10 iters), loss = 6.79633
I0524 05:46:17.688081 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79633 (* 1 = 6.79633 loss)
I0524 05:46:17.701191 11835 sgd_solver.cpp:112] Iteration 40400, lr = 0.1
I0524 05:46:27.767201 11835 solver.cpp:239] Iteration 40410 (0.992188 iter/s, 10.0787s/10 iters), loss = 6.84997
I0524 05:46:27.767365 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84997 (* 1 = 6.84997 loss)
I0524 05:46:27.771680 11835 sgd_solver.cpp:112] Iteration 40410, lr = 0.1
I0524 05:46:35.374455 11835 solver.cpp:239] Iteration 40420 (1.31462 iter/s, 7.60679s/10 iters), loss = 5.20122
I0524 05:46:35.374512 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.20122 (* 1 = 5.20122 loss)
I0524 05:46:35.374626 11835 sgd_solver.cpp:112] Iteration 40420, lr = 0.1
I0524 05:46:41.568871 11835 solver.cpp:239] Iteration 40430 (1.61443 iter/s, 6.19412s/10 iters), loss = 6.6169
I0524 05:46:41.568915 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6169 (* 1 = 6.6169 loss)
I0524 05:46:41.569044 11835 sgd_solver.cpp:112] Iteration 40430, lr = 0.1
I0524 05:46:48.715298 11835 solver.cpp:239] Iteration 40440 (1.39937 iter/s, 7.14608s/10 iters), loss = 5.33022
I0524 05:46:48.715368 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.33022 (* 1 = 5.33022 loss)
I0524 05:46:48.715589 11835 sgd_solver.cpp:112] Iteration 40440, lr = 0.1
I0524 05:46:56.156155 11835 solver.cpp:239] Iteration 40450 (1.34399 iter/s, 7.44051s/10 iters), loss = 6.14378
I0524 05:46:56.156206 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14378 (* 1 = 6.14378 loss)
I0524 05:46:56.156306 11835 sgd_solver.cpp:112] Iteration 40450, lr = 0.1
I0524 05:47:05.506067 11835 solver.cpp:239] Iteration 40460 (1.06958 iter/s, 9.34949s/10 iters), loss = 5.30009
I0524 05:47:05.506316 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.30009 (* 1 = 5.30009 loss)
I0524 05:47:05.506363 11835 sgd_solver.cpp:112] Iteration 40460, lr = 0.1
I0524 05:47:13.385509 11835 solver.cpp:239] Iteration 40470 (1.26926 iter/s, 7.87862s/10 iters), loss = 5.1679
I0524 05:47:13.385562 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.1679 (* 1 = 5.1679 loss)
I0524 05:47:14.113414 11835 sgd_solver.cpp:112] Iteration 40470, lr = 0.1
I0524 05:47:21.557358 11835 solver.cpp:239] Iteration 40480 (1.22377 iter/s, 8.17147s/10 iters), loss = 6.33578
I0524 05:47:21.557416 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33578 (* 1 = 6.33578 loss)
I0524 05:47:21.557615 11835 sgd_solver.cpp:112] Iteration 40480, lr = 0.1
I0524 05:47:27.565868 11835 solver.cpp:239] Iteration 40490 (1.66439 iter/s, 6.00822s/10 iters), loss = 5.56464
I0524 05:47:27.565917 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56464 (* 1 = 5.56464 loss)
I0524 05:47:27.566439 11835 sgd_solver.cpp:112] Iteration 40490, lr = 0.1
I0524 05:47:36.643564 11835 solver.cpp:239] Iteration 40500 (1.10165 iter/s, 9.07731s/10 iters), loss = 6.93409
I0524 05:47:36.643846 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93409 (* 1 = 6.93409 loss)
I0524 05:47:36.643892 11835 sgd_solver.cpp:112] Iteration 40500, lr = 0.1
I0524 05:47:44.593667 11835 solver.cpp:239] Iteration 40510 (1.2581 iter/s, 7.94849s/10 iters), loss = 6.17122
I0524 05:47:44.593731 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17122 (* 1 = 6.17122 loss)
I0524 05:47:44.593770 11835 sgd_solver.cpp:112] Iteration 40510, lr = 0.1
I0524 05:47:50.660161 11835 solver.cpp:239] Iteration 40520 (1.64848 iter/s, 6.06619s/10 iters), loss = 5.60209
I0524 05:47:50.660218 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.60209 (* 1 = 5.60209 loss)
I0524 05:47:50.660291 11835 sgd_solver.cpp:112] Iteration 40520, lr = 0.1
I0524 05:47:57.768873 11835 solver.cpp:239] Iteration 40530 (1.40679 iter/s, 7.10838s/10 iters), loss = 6.70716
I0524 05:47:57.768925 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70716 (* 1 = 6.70716 loss)
I0524 05:47:57.768987 11835 sgd_solver.cpp:112] Iteration 40530, lr = 0.1
I0524 05:48:04.753021 11835 solver.cpp:239] Iteration 40540 (1.43188 iter/s, 6.98382s/10 iters), loss = 7.06185
I0524 05:48:04.753077 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.06185 (* 1 = 7.06185 loss)
I0524 05:48:05.785172 11835 sgd_solver.cpp:112] Iteration 40540, lr = 0.1
I0524 05:48:12.617789 11835 solver.cpp:239] Iteration 40550 (1.27155 iter/s, 7.8644s/10 iters), loss = 5.79852
I0524 05:48:12.617949 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79852 (* 1 = 5.79852 loss)
I0524 05:48:12.618218 11835 sgd_solver.cpp:112] Iteration 40550, lr = 0.1
I0524 05:48:18.318509 11835 solver.cpp:239] Iteration 40560 (1.75428 iter/s, 5.70034s/10 iters), loss = 5.42964
I0524 05:48:18.318557 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.42964 (* 1 = 5.42964 loss)
I0524 05:48:18.367797 11835 sgd_solver.cpp:112] Iteration 40560, lr = 0.1
I0524 05:48:24.618455 11835 solver.cpp:239] Iteration 40570 (1.58739 iter/s, 6.29966s/10 iters), loss = 4.77857
I0524 05:48:24.618510 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.77857 (* 1 = 4.77857 loss)
I0524 05:48:25.001252 11835 sgd_solver.cpp:112] Iteration 40570, lr = 0.1
I0524 05:48:32.713232 11835 solver.cpp:239] Iteration 40580 (1.23542 iter/s, 8.0944s/10 iters), loss = 5.58857
I0524 05:48:32.713299 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.58857 (* 1 = 5.58857 loss)
I0524 05:48:32.713572 11835 sgd_solver.cpp:112] Iteration 40580, lr = 0.1
I0524 05:48:40.476619 11835 solver.cpp:239] Iteration 40590 (1.28816 iter/s, 7.76302s/10 iters), loss = 6.14799
I0524 05:48:40.476677 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14799 (* 1 = 6.14799 loss)
I0524 05:48:40.476776 11835 sgd_solver.cpp:112] Iteration 40590, lr = 0.1
I0524 05:48:46.108407 11835 solver.cpp:239] Iteration 40600 (1.77572 iter/s, 5.63151s/10 iters), loss = 6.98142
I0524 05:48:46.108568 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98142 (* 1 = 6.98142 loss)
I0524 05:48:46.108585 11835 sgd_solver.cpp:112] Iteration 40600, lr = 0.1
I0524 05:48:52.246620 11835 solver.cpp:239] Iteration 40610 (1.62932 iter/s, 6.13752s/10 iters), loss = 5.78965
I0524 05:48:52.246667 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78965 (* 1 = 5.78965 loss)
I0524 05:48:52.346211 11835 sgd_solver.cpp:112] Iteration 40610, lr = 0.1
I0524 05:48:59.080341 11835 solver.cpp:239] Iteration 40620 (1.4634 iter/s, 6.8334s/10 iters), loss = 6.5994
I0524 05:48:59.080397 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5994 (* 1 = 6.5994 loss)
I0524 05:48:59.080632 11835 sgd_solver.cpp:112] Iteration 40620, lr = 0.1
I0524 05:49:06.118321 11835 solver.cpp:239] Iteration 40630 (1.42093 iter/s, 7.03765s/10 iters), loss = 6.60208
I0524 05:49:06.118384 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60208 (* 1 = 6.60208 loss)
I0524 05:49:06.118594 11835 sgd_solver.cpp:112] Iteration 40630, lr = 0.1
I0524 05:49:14.947820 11835 solver.cpp:239] Iteration 40640 (1.13262 iter/s, 8.82909s/10 iters), loss = 5.74917
I0524 05:49:14.947880 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74917 (* 1 = 5.74917 loss)
I0524 05:49:14.947960 11835 sgd_solver.cpp:112] Iteration 40640, lr = 0.1
I0524 05:49:22.135063 11835 solver.cpp:239] Iteration 40650 (1.39142 iter/s, 7.18688s/10 iters), loss = 6.96438
I0524 05:49:22.135313 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96438 (* 1 = 6.96438 loss)
I0524 05:49:22.135339 11835 sgd_solver.cpp:112] Iteration 40650, lr = 0.1
I0524 05:49:28.909229 11835 solver.cpp:239] Iteration 40660 (1.47678 iter/s, 6.7715s/10 iters), loss = 7.30598
I0524 05:49:28.909282 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.30598 (* 1 = 7.30598 loss)
I0524 05:49:28.909325 11835 sgd_solver.cpp:112] Iteration 40660, lr = 0.1
I0524 05:49:36.033571 11835 solver.cpp:239] Iteration 40670 (1.40371 iter/s, 7.12399s/10 iters), loss = 5.45353
I0524 05:49:36.033641 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.45353 (* 1 = 5.45353 loss)
I0524 05:49:36.033720 11835 sgd_solver.cpp:112] Iteration 40670, lr = 0.1
I0524 05:49:44.047802 11835 solver.cpp:239] Iteration 40680 (1.24784 iter/s, 8.01386s/10 iters), loss = 6.96666
I0524 05:49:44.047853 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96666 (* 1 = 6.96666 loss)
I0524 05:49:44.047948 11835 sgd_solver.cpp:112] Iteration 40680, lr = 0.1
I0524 05:49:53.511592 11835 solver.cpp:239] Iteration 40690 (1.05671 iter/s, 9.46337s/10 iters), loss = 5.81979
I0524 05:49:53.511878 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81979 (* 1 = 5.81979 loss)
I0524 05:49:53.534204 11835 sgd_solver.cpp:112] Iteration 40690, lr = 0.1
I0524 05:49:59.321727 11835 solver.cpp:239] Iteration 40700 (1.72126 iter/s, 5.8097s/10 iters), loss = 6.87904
I0524 05:49:59.321776 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87904 (* 1 = 6.87904 loss)
I0524 05:49:59.321792 11835 sgd_solver.cpp:112] Iteration 40700, lr = 0.1
I0524 05:50:05.717605 11835 solver.cpp:239] Iteration 40710 (1.56358 iter/s, 6.39559s/10 iters), loss = 5.65617
I0524 05:50:05.717655 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.65617 (* 1 = 5.65617 loss)
I0524 05:50:05.717672 11835 sgd_solver.cpp:112] Iteration 40710, lr = 0.1
I0524 05:50:12.308789 11835 solver.cpp:239] Iteration 40720 (1.51725 iter/s, 6.59087s/10 iters), loss = 6.35203
I0524 05:50:12.308851 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35203 (* 1 = 6.35203 loss)
I0524 05:50:12.362179 11835 sgd_solver.cpp:112] Iteration 40720, lr = 0.1
I0524 05:50:19.587896 11835 solver.cpp:239] Iteration 40730 (1.37386 iter/s, 7.27877s/10 iters), loss = 6.03851
I0524 05:50:19.587946 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03851 (* 1 = 6.03851 loss)
I0524 05:50:19.618218 11835 sgd_solver.cpp:112] Iteration 40730, lr = 0.1
I0524 05:50:29.803475 11835 solver.cpp:239] Iteration 40740 (0.97894 iter/s, 10.2151s/10 iters), loss = 6.63273
I0524 05:50:29.803774 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63273 (* 1 = 6.63273 loss)
I0524 05:50:30.033301 11835 sgd_solver.cpp:112] Iteration 40740, lr = 0.1
I0524 05:50:39.263839 11835 solver.cpp:239] Iteration 40750 (1.05711 iter/s, 9.45973s/10 iters), loss = 7.30427
I0524 05:50:39.263905 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.30427 (* 1 = 7.30427 loss)
I0524 05:50:39.264288 11835 sgd_solver.cpp:112] Iteration 40750, lr = 0.1
I0524 05:50:45.017480 11835 solver.cpp:239] Iteration 40760 (1.73812 iter/s, 5.75336s/10 iters), loss = 5.86574
I0524 05:50:45.017535 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86574 (* 1 = 5.86574 loss)
I0524 05:50:45.017551 11835 sgd_solver.cpp:112] Iteration 40760, lr = 0.1
I0524 05:50:52.834861 11835 solver.cpp:239] Iteration 40770 (1.27926 iter/s, 7.81703s/10 iters), loss = 6.87113
I0524 05:50:52.834910 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87113 (* 1 = 6.87113 loss)
I0524 05:50:52.834924 11835 sgd_solver.cpp:112] Iteration 40770, lr = 0.1
I0524 05:51:01.833612 11835 solver.cpp:239] Iteration 40780 (1.11145 iter/s, 8.99725s/10 iters), loss = 6.33327
I0524 05:51:01.833801 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33327 (* 1 = 6.33327 loss)
I0524 05:51:01.833822 11835 sgd_solver.cpp:112] Iteration 40780, lr = 0.1
I0524 05:51:07.922499 11835 solver.cpp:239] Iteration 40790 (1.64247 iter/s, 6.08838s/10 iters), loss = 7.65681
I0524 05:51:07.922538 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.65681 (* 1 = 7.65681 loss)
I0524 05:51:07.922549 11835 sgd_solver.cpp:112] Iteration 40790, lr = 0.1
I0524 05:51:15.584897 11835 solver.cpp:239] Iteration 40800 (1.30551 iter/s, 7.65987s/10 iters), loss = 5.06033
I0524 05:51:15.584951 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.06033 (* 1 = 5.06033 loss)
I0524 05:51:15.879808 11835 sgd_solver.cpp:112] Iteration 40800, lr = 0.1
I0524 05:51:22.610899 11835 solver.cpp:239] Iteration 40810 (1.42335 iter/s, 7.02569s/10 iters), loss = 6.14199
I0524 05:51:22.610939 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14199 (* 1 = 6.14199 loss)
I0524 05:51:22.611047 11835 sgd_solver.cpp:112] Iteration 40810, lr = 0.1
I0524 05:51:29.244256 11835 solver.cpp:239] Iteration 40820 (1.5076 iter/s, 6.63305s/10 iters), loss = 6.49446
I0524 05:51:29.244303 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49446 (* 1 = 6.49446 loss)
I0524 05:51:29.804661 11835 sgd_solver.cpp:112] Iteration 40820, lr = 0.1
I0524 05:51:37.112342 11835 solver.cpp:239] Iteration 40830 (1.27102 iter/s, 7.86772s/10 iters), loss = 5.53887
I0524 05:51:37.112684 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.53887 (* 1 = 5.53887 loss)
I0524 05:51:37.253100 11835 sgd_solver.cpp:112] Iteration 40830, lr = 0.1
I0524 05:51:43.500823 11835 solver.cpp:239] Iteration 40840 (1.56545 iter/s, 6.38793s/10 iters), loss = 6.55214
I0524 05:51:43.500895 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55214 (* 1 = 6.55214 loss)
I0524 05:51:43.500922 11835 sgd_solver.cpp:112] Iteration 40840, lr = 0.1
I0524 05:51:50.439182 11835 solver.cpp:239] Iteration 40850 (1.44133 iter/s, 6.93804s/10 iters), loss = 7.62111
I0524 05:51:50.439226 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.62111 (* 1 = 7.62111 loss)
I0524 05:51:50.439239 11835 sgd_solver.cpp:112] Iteration 40850, lr = 0.1
I0524 05:51:56.855525 11835 solver.cpp:239] Iteration 40860 (1.55859 iter/s, 6.41604s/10 iters), loss = 6.61845
I0524 05:51:56.855574 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61845 (* 1 = 6.61845 loss)
I0524 05:51:57.718683 11835 sgd_solver.cpp:112] Iteration 40860, lr = 0.1
I0524 05:52:05.463402 11835 solver.cpp:239] Iteration 40870 (1.16178 iter/s, 8.60749s/10 iters), loss = 6.43612
I0524 05:52:05.463449 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43612 (* 1 = 6.43612 loss)
I0524 05:52:06.734690 11835 sgd_solver.cpp:112] Iteration 40870, lr = 0.1
I0524 05:52:14.426777 11835 solver.cpp:239] Iteration 40880 (1.1157 iter/s, 8.96299s/10 iters), loss = 6.01873
I0524 05:52:14.427002 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01873 (* 1 = 6.01873 loss)
I0524 05:52:14.452849 11835 sgd_solver.cpp:112] Iteration 40880, lr = 0.1
I0524 05:52:20.755789 11835 solver.cpp:239] Iteration 40890 (1.58014 iter/s, 6.32855s/10 iters), loss = 6.29854
I0524 05:52:20.755837 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29854 (* 1 = 6.29854 loss)
I0524 05:52:20.755851 11835 sgd_solver.cpp:112] Iteration 40890, lr = 0.1
I0524 05:52:26.860460 11835 solver.cpp:239] Iteration 40900 (1.63817 iter/s, 6.10439s/10 iters), loss = 5.83043
I0524 05:52:26.860512 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83043 (* 1 = 5.83043 loss)
I0524 05:52:26.860654 11835 sgd_solver.cpp:112] Iteration 40900, lr = 0.1
I0524 05:52:34.897580 11835 solver.cpp:239] Iteration 40910 (1.24428 iter/s, 8.03676s/10 iters), loss = 6.31824
I0524 05:52:34.897635 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31824 (* 1 = 6.31824 loss)
I0524 05:52:34.948722 11835 sgd_solver.cpp:112] Iteration 40910, lr = 0.1
I0524 05:52:42.893676 11835 solver.cpp:239] Iteration 40920 (1.25067 iter/s, 7.99574s/10 iters), loss = 5.80772
I0524 05:52:42.893723 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80772 (* 1 = 5.80772 loss)
I0524 05:52:42.893795 11835 sgd_solver.cpp:112] Iteration 40920, lr = 0.1
I0524 05:52:49.763100 11835 solver.cpp:239] Iteration 40930 (1.4558 iter/s, 6.8691s/10 iters), loss = 6.27965
I0524 05:52:49.763258 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27965 (* 1 = 6.27965 loss)
I0524 05:52:49.772967 11835 sgd_solver.cpp:112] Iteration 40930, lr = 0.1
I0524 05:52:55.856438 11835 solver.cpp:239] Iteration 40940 (1.64124 iter/s, 6.09295s/10 iters), loss = 5.48733
I0524 05:52:55.856477 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.48733 (* 1 = 5.48733 loss)
I0524 05:52:55.856489 11835 sgd_solver.cpp:112] Iteration 40940, lr = 0.1
I0524 05:53:01.715870 11835 solver.cpp:239] Iteration 40950 (1.70676 iter/s, 5.85907s/10 iters), loss = 5.30179
I0524 05:53:01.715931 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.30179 (* 1 = 5.30179 loss)
I0524 05:53:01.716341 11835 sgd_solver.cpp:112] Iteration 40950, lr = 0.1
I0524 05:53:08.372655 11835 solver.cpp:239] Iteration 40960 (1.5023 iter/s, 6.65646s/10 iters), loss = 5.91185
I0524 05:53:08.372711 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91185 (* 1 = 5.91185 loss)
I0524 05:53:08.372778 11835 sgd_solver.cpp:112] Iteration 40960, lr = 0.1
I0524 05:53:15.707146 11835 solver.cpp:239] Iteration 40970 (1.36348 iter/s, 7.33416s/10 iters), loss = 6.50964
I0524 05:53:15.707187 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50964 (* 1 = 6.50964 loss)
I0524 05:53:15.707201 11835 sgd_solver.cpp:112] Iteration 40970, lr = 0.1
I0524 05:53:22.909608 11835 solver.cpp:239] Iteration 40980 (1.38869 iter/s, 7.20102s/10 iters), loss = 6.17678
I0524 05:53:22.909782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17678 (* 1 = 6.17678 loss)
I0524 05:53:22.941107 11835 sgd_solver.cpp:112] Iteration 40980, lr = 0.1
I0524 05:53:29.892206 11835 solver.cpp:239] Iteration 40990 (1.43222 iter/s, 6.98216s/10 iters), loss = 6.53149
I0524 05:53:29.892253 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53149 (* 1 = 6.53149 loss)
I0524 05:53:29.892267 11835 sgd_solver.cpp:112] Iteration 40990, lr = 0.1
I0524 05:53:40.537992 11835 solver.cpp:239] Iteration 41000 (0.939478 iter/s, 10.6442s/10 iters), loss = 7.77764
I0524 05:53:40.538053 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.77764 (* 1 = 7.77764 loss)
I0524 05:53:40.538260 11835 sgd_solver.cpp:112] Iteration 41000, lr = 0.1
I0524 05:53:49.722126 11835 solver.cpp:239] Iteration 41010 (1.08888 iter/s, 9.18372s/10 iters), loss = 5.63469
I0524 05:53:49.722179 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63469 (* 1 = 5.63469 loss)
I0524 05:53:49.975864 11835 sgd_solver.cpp:112] Iteration 41010, lr = 0.1
I0524 05:53:55.990351 11835 solver.cpp:239] Iteration 41020 (1.59542 iter/s, 6.26793s/10 iters), loss = 6.23
I0524 05:53:55.990636 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23 (* 1 = 6.23 loss)
I0524 05:53:55.990813 11835 sgd_solver.cpp:112] Iteration 41020, lr = 0.1
I0524 05:54:04.003850 11835 solver.cpp:239] Iteration 41030 (1.24798 iter/s, 8.01295s/10 iters), loss = 6.04897
I0524 05:54:04.003904 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04897 (* 1 = 6.04897 loss)
I0524 05:54:04.025606 11835 sgd_solver.cpp:112] Iteration 41030, lr = 0.1
I0524 05:54:12.384402 11835 solver.cpp:239] Iteration 41040 (1.19329 iter/s, 8.38018s/10 iters), loss = 5.57753
I0524 05:54:12.384451 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.57753 (* 1 = 5.57753 loss)
I0524 05:54:12.384465 11835 sgd_solver.cpp:112] Iteration 41040, lr = 0.1
I0524 05:54:20.164999 11835 solver.cpp:239] Iteration 41050 (1.2853 iter/s, 7.78025s/10 iters), loss = 6.40666
I0524 05:54:20.165047 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40666 (* 1 = 6.40666 loss)
I0524 05:54:20.165323 11835 sgd_solver.cpp:112] Iteration 41050, lr = 0.1
I0524 05:54:26.124819 11835 solver.cpp:239] Iteration 41060 (1.67798 iter/s, 5.95954s/10 iters), loss = 6.2744
I0524 05:54:26.125068 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2744 (* 1 = 6.2744 loss)
I0524 05:54:26.125113 11835 sgd_solver.cpp:112] Iteration 41060, lr = 0.1
I0524 05:54:34.229923 11835 solver.cpp:239] Iteration 41070 (1.2339 iter/s, 8.10437s/10 iters), loss = 6.913
I0524 05:54:34.230011 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.913 (* 1 = 6.913 loss)
I0524 05:54:34.775655 11835 sgd_solver.cpp:112] Iteration 41070, lr = 0.1
I0524 05:54:41.464804 11835 solver.cpp:239] Iteration 41080 (1.38226 iter/s, 7.23452s/10 iters), loss = 6.08055
I0524 05:54:41.464862 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08055 (* 1 = 6.08055 loss)
I0524 05:54:41.464880 11835 sgd_solver.cpp:112] Iteration 41080, lr = 0.1
I0524 05:54:48.844971 11835 solver.cpp:239] Iteration 41090 (1.35505 iter/s, 7.37979s/10 iters), loss = 7.1718
I0524 05:54:48.845018 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.1718 (* 1 = 7.1718 loss)
I0524 05:54:49.031405 11835 sgd_solver.cpp:112] Iteration 41090, lr = 0.1
I0524 05:54:56.442781 11835 solver.cpp:239] Iteration 41100 (1.31623 iter/s, 7.59745s/10 iters), loss = 6.52313
I0524 05:54:56.443027 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52313 (* 1 = 6.52313 loss)
I0524 05:54:56.443073 11835 sgd_solver.cpp:112] Iteration 41100, lr = 0.1
I0524 05:55:04.042512 11835 solver.cpp:239] Iteration 41110 (1.31597 iter/s, 7.59898s/10 iters), loss = 7.14301
I0524 05:55:04.042562 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.14301 (* 1 = 7.14301 loss)
I0524 05:55:04.057813 11835 sgd_solver.cpp:112] Iteration 41110, lr = 0.1
I0524 05:55:10.089931 11835 solver.cpp:239] Iteration 41120 (1.65367 iter/s, 6.04714s/10 iters), loss = 5.24222
I0524 05:55:10.089965 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.24222 (* 1 = 5.24222 loss)
I0524 05:55:10.963937 11835 sgd_solver.cpp:112] Iteration 41120, lr = 0.1
I0524 05:55:17.264442 11835 solver.cpp:239] Iteration 41130 (1.39389 iter/s, 7.17419s/10 iters), loss = 5.45245
I0524 05:55:17.264492 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.45245 (* 1 = 5.45245 loss)
I0524 05:55:17.264714 11835 sgd_solver.cpp:112] Iteration 41130, lr = 0.1
I0524 05:55:24.218747 11835 solver.cpp:239] Iteration 41140 (1.43802 iter/s, 6.95399s/10 iters), loss = 5.90125
I0524 05:55:24.218788 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90125 (* 1 = 5.90125 loss)
I0524 05:55:24.218801 11835 sgd_solver.cpp:112] Iteration 41140, lr = 0.1
I0524 05:55:31.397115 11835 solver.cpp:239] Iteration 41150 (1.39335 iter/s, 7.17692s/10 iters), loss = 7.02632
I0524 05:55:31.397266 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.02632 (* 1 = 7.02632 loss)
I0524 05:55:31.529284 11835 sgd_solver.cpp:112] Iteration 41150, lr = 0.1
I0524 05:55:37.545599 11835 solver.cpp:239] Iteration 41160 (1.62652 iter/s, 6.14811s/10 iters), loss = 5.68258
I0524 05:55:37.545639 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68258 (* 1 = 5.68258 loss)
I0524 05:55:37.588364 11835 sgd_solver.cpp:112] Iteration 41160, lr = 0.1
I0524 05:55:44.313753 11835 solver.cpp:239] Iteration 41170 (1.47758 iter/s, 6.76782s/10 iters), loss = 6.30232
I0524 05:55:44.313820 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30232 (* 1 = 6.30232 loss)
I0524 05:55:44.313843 11835 sgd_solver.cpp:112] Iteration 41170, lr = 0.1
I0524 05:55:53.224261 11835 solver.cpp:239] Iteration 41180 (1.12236 iter/s, 8.9098s/10 iters), loss = 5.82696
I0524 05:55:53.224310 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82696 (* 1 = 5.82696 loss)
I0524 05:55:53.224499 11835 sgd_solver.cpp:112] Iteration 41180, lr = 0.1
I0524 05:55:59.446557 11835 solver.cpp:239] Iteration 41190 (1.6072 iter/s, 6.22201s/10 iters), loss = 6.40701
I0524 05:55:59.446597 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40701 (* 1 = 6.40701 loss)
I0524 05:55:59.446609 11835 sgd_solver.cpp:112] Iteration 41190, lr = 0.1
I0524 05:56:06.851568 11835 solver.cpp:239] Iteration 41200 (1.3505 iter/s, 7.40468s/10 iters), loss = 5.99969
I0524 05:56:06.851850 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99969 (* 1 = 5.99969 loss)
I0524 05:56:06.851892 11835 sgd_solver.cpp:112] Iteration 41200, lr = 0.1
I0524 05:56:13.941035 11835 solver.cpp:239] Iteration 41210 (1.41093 iter/s, 7.08754s/10 iters), loss = 7.01134
I0524 05:56:13.941084 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01134 (* 1 = 7.01134 loss)
I0524 05:56:13.941259 11835 sgd_solver.cpp:112] Iteration 41210, lr = 0.1
I0524 05:56:21.070045 11835 solver.cpp:239] Iteration 41220 (1.40278 iter/s, 7.12869s/10 iters), loss = 6.59419
I0524 05:56:21.070088 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59419 (* 1 = 6.59419 loss)
I0524 05:56:21.130853 11835 sgd_solver.cpp:112] Iteration 41220, lr = 0.1
I0524 05:56:28.211479 11835 solver.cpp:239] Iteration 41230 (1.40035 iter/s, 7.14109s/10 iters), loss = 6.72485
I0524 05:56:28.211570 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72485 (* 1 = 6.72485 loss)
I0524 05:56:28.211599 11835 sgd_solver.cpp:112] Iteration 41230, lr = 0.1
I0524 05:56:36.671078 11835 solver.cpp:239] Iteration 41240 (1.18223 iter/s, 8.4586s/10 iters), loss = 6.69375
I0524 05:56:36.671135 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69375 (* 1 = 6.69375 loss)
I0524 05:56:36.885339 11835 sgd_solver.cpp:112] Iteration 41240, lr = 0.1
I0524 05:56:44.241497 11835 solver.cpp:239] Iteration 41250 (1.321 iter/s, 7.57004s/10 iters), loss = 6.40029
I0524 05:56:44.241592 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40029 (* 1 = 6.40029 loss)
I0524 05:56:44.241833 11835 sgd_solver.cpp:112] Iteration 41250, lr = 0.1
I0524 05:56:51.564256 11835 solver.cpp:239] Iteration 41260 (1.36567 iter/s, 7.32239s/10 iters), loss = 6.21481
I0524 05:56:51.564313 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21481 (* 1 = 6.21481 loss)
I0524 05:56:51.564342 11835 sgd_solver.cpp:112] Iteration 41260, lr = 0.1
I0524 05:56:58.177072 11835 solver.cpp:239] Iteration 41270 (1.51229 iter/s, 6.61249s/10 iters), loss = 6.1783
I0524 05:56:58.177127 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1783 (* 1 = 6.1783 loss)
I0524 05:56:58.177212 11835 sgd_solver.cpp:112] Iteration 41270, lr = 0.1
I0524 05:57:04.602535 11835 solver.cpp:239] Iteration 41280 (1.55638 iter/s, 6.42516s/10 iters), loss = 6.17919
I0524 05:57:04.602586 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17919 (* 1 = 6.17919 loss)
I0524 05:57:05.126451 11835 sgd_solver.cpp:112] Iteration 41280, lr = 0.1
I0524 05:57:14.163296 11835 solver.cpp:239] Iteration 41290 (1.04599 iter/s, 9.56034s/10 iters), loss = 5.85501
I0524 05:57:14.163568 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85501 (* 1 = 5.85501 loss)
I0524 05:57:14.262284 11835 sgd_solver.cpp:112] Iteration 41290, lr = 0.1
I0524 05:57:20.547358 11835 solver.cpp:239] Iteration 41300 (1.56651 iter/s, 6.3836s/10 iters), loss = 5.98417
I0524 05:57:20.547408 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98417 (* 1 = 5.98417 loss)
I0524 05:57:20.547752 11835 sgd_solver.cpp:112] Iteration 41300, lr = 0.1
I0524 05:57:27.984679 11835 solver.cpp:239] Iteration 41310 (1.34463 iter/s, 7.43698s/10 iters), loss = 6.07602
I0524 05:57:27.984738 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07602 (* 1 = 6.07602 loss)
I0524 05:57:28.487541 11835 sgd_solver.cpp:112] Iteration 41310, lr = 0.1
I0524 05:57:35.742094 11835 solver.cpp:239] Iteration 41320 (1.28915 iter/s, 7.75706s/10 iters), loss = 6.4039
I0524 05:57:35.742147 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4039 (* 1 = 6.4039 loss)
I0524 05:57:35.746497 11835 sgd_solver.cpp:112] Iteration 41320, lr = 0.1
I0524 05:57:43.159065 11835 solver.cpp:239] Iteration 41330 (1.34832 iter/s, 7.41664s/10 iters), loss = 5.27761
I0524 05:57:43.159134 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.27761 (* 1 = 5.27761 loss)
I0524 05:57:43.159299 11835 sgd_solver.cpp:112] Iteration 41330, lr = 0.1
I0524 05:57:50.905278 11835 solver.cpp:239] Iteration 41340 (1.29101 iter/s, 7.74585s/10 iters), loss = 6.40381
I0524 05:57:50.905498 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40381 (* 1 = 6.40381 loss)
I0524 05:57:50.905547 11835 sgd_solver.cpp:112] Iteration 41340, lr = 0.1
I0524 05:57:56.905812 11835 solver.cpp:239] Iteration 41350 (1.66663 iter/s, 6.00012s/10 iters), loss = 6.79075
I0524 05:57:56.905863 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79075 (* 1 = 6.79075 loss)
I0524 05:57:56.905877 11835 sgd_solver.cpp:112] Iteration 41350, lr = 0.1
I0524 05:58:03.305399 11835 solver.cpp:239] Iteration 41360 (1.56269 iter/s, 6.39922s/10 iters), loss = 5.94161
I0524 05:58:03.305446 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94161 (* 1 = 5.94161 loss)
I0524 05:58:03.305461 11835 sgd_solver.cpp:112] Iteration 41360, lr = 0.1
I0524 05:58:12.013259 11835 solver.cpp:239] Iteration 41370 (1.14845 iter/s, 8.7074s/10 iters), loss = 6.74702
I0524 05:58:12.013314 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74702 (* 1 = 6.74702 loss)
I0524 05:58:12.013622 11835 sgd_solver.cpp:112] Iteration 41370, lr = 0.1
I0524 05:58:19.309219 11835 solver.cpp:239] Iteration 41380 (1.37069 iter/s, 7.29562s/10 iters), loss = 6.62084
I0524 05:58:19.309296 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62084 (* 1 = 6.62084 loss)
I0524 05:58:20.663924 11835 sgd_solver.cpp:112] Iteration 41380, lr = 0.1
I0524 05:58:27.542762 11835 solver.cpp:239] Iteration 41390 (1.2146 iter/s, 8.23317s/10 iters), loss = 6.26932
I0524 05:58:27.542994 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26932 (* 1 = 6.26932 loss)
I0524 05:58:27.600812 11835 sgd_solver.cpp:112] Iteration 41390, lr = 0.1
I0524 05:58:33.531102 11835 solver.cpp:239] Iteration 41400 (1.67003 iter/s, 5.98792s/10 iters), loss = 6.53262
I0524 05:58:33.531160 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53262 (* 1 = 6.53262 loss)
I0524 05:58:33.531179 11835 sgd_solver.cpp:112] Iteration 41400, lr = 0.1
I0524 05:58:41.176674 11835 solver.cpp:239] Iteration 41410 (1.30801 iter/s, 7.64522s/10 iters), loss = 5.1038
I0524 05:58:41.176734 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.1038 (* 1 = 5.1038 loss)
I0524 05:58:41.176846 11835 sgd_solver.cpp:112] Iteration 41410, lr = 0.1
I0524 05:58:47.462115 11835 solver.cpp:239] Iteration 41420 (1.59106 iter/s, 6.28513s/10 iters), loss = 6.36686
I0524 05:58:47.462185 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36686 (* 1 = 6.36686 loss)
I0524 05:58:47.462229 11835 sgd_solver.cpp:112] Iteration 41420, lr = 0.1
I0524 05:58:54.191617 11835 solver.cpp:239] Iteration 41430 (1.48607 iter/s, 6.72915s/10 iters), loss = 6.51659
I0524 05:58:54.191700 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51659 (* 1 = 6.51659 loss)
I0524 05:58:55.086143 11835 sgd_solver.cpp:112] Iteration 41430, lr = 0.1
I0524 05:59:01.965075 11835 solver.cpp:239] Iteration 41440 (1.28649 iter/s, 7.77309s/10 iters), loss = 6.7055
I0524 05:59:01.965335 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7055 (* 1 = 6.7055 loss)
I0524 05:59:01.965375 11835 sgd_solver.cpp:112] Iteration 41440, lr = 0.1
I0524 05:59:12.238821 11835 solver.cpp:239] Iteration 41450 (0.973463 iter/s, 10.2726s/10 iters), loss = 5.6215
I0524 05:59:12.238979 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.6215 (* 1 = 5.6215 loss)
I0524 05:59:13.307168 11835 sgd_solver.cpp:112] Iteration 41450, lr = 0.1
I0524 05:59:21.542408 11835 solver.cpp:239] Iteration 41460 (1.0749 iter/s, 9.30315s/10 iters), loss = 5.88667
I0524 05:59:21.542469 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88667 (* 1 = 5.88667 loss)
I0524 05:59:21.542681 11835 sgd_solver.cpp:112] Iteration 41460, lr = 0.1
I0524 05:59:28.682669 11835 solver.cpp:239] Iteration 41470 (1.40057 iter/s, 7.13993s/10 iters), loss = 6.38599
I0524 05:59:28.682729 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38599 (* 1 = 6.38599 loss)
I0524 05:59:28.682974 11835 sgd_solver.cpp:112] Iteration 41470, lr = 0.1
I0524 05:59:35.738554 11835 solver.cpp:239] Iteration 41480 (1.41733 iter/s, 7.05554s/10 iters), loss = 6.59255
I0524 05:59:35.738800 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59255 (* 1 = 6.59255 loss)
I0524 05:59:35.739169 11835 sgd_solver.cpp:112] Iteration 41480, lr = 0.1
I0524 05:59:42.278511 11835 solver.cpp:239] Iteration 41490 (1.52917 iter/s, 6.53948s/10 iters), loss = 6.97064
I0524 05:59:42.278565 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.97064 (* 1 = 6.97064 loss)
I0524 05:59:42.278592 11835 sgd_solver.cpp:112] Iteration 41490, lr = 0.1
I0524 05:59:50.585980 11835 solver.cpp:239] Iteration 41500 (1.20379 iter/s, 8.3071s/10 iters), loss = 6.61116
I0524 05:59:50.586027 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61116 (* 1 = 6.61116 loss)
I0524 05:59:50.603309 11835 sgd_solver.cpp:112] Iteration 41500, lr = 0.1
I0524 05:59:56.577981 11835 solver.cpp:239] Iteration 41510 (1.66897 iter/s, 5.99171s/10 iters), loss = 6.80296
I0524 05:59:56.578099 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80296 (* 1 = 6.80296 loss)
I0524 05:59:56.772053 11835 sgd_solver.cpp:112] Iteration 41510, lr = 0.1
I0524 06:00:04.361927 11835 solver.cpp:239] Iteration 41520 (1.28476 iter/s, 7.78358s/10 iters), loss = 5.73307
I0524 06:00:04.361989 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73307 (* 1 = 5.73307 loss)
I0524 06:00:05.026926 11835 sgd_solver.cpp:112] Iteration 41520, lr = 0.1
I0524 06:00:11.769085 11835 solver.cpp:239] Iteration 41530 (1.35011 iter/s, 7.40681s/10 iters), loss = 5.87357
I0524 06:00:11.769306 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87357 (* 1 = 5.87357 loss)
I0524 06:00:11.769351 11835 sgd_solver.cpp:112] Iteration 41530, lr = 0.1
I0524 06:00:19.225769 11835 solver.cpp:239] Iteration 41540 (1.34117 iter/s, 7.45619s/10 iters), loss = 5.7894
I0524 06:00:19.225927 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7894 (* 1 = 5.7894 loss)
I0524 06:00:19.275606 11835 sgd_solver.cpp:112] Iteration 41540, lr = 0.1
I0524 06:00:26.670070 11835 solver.cpp:239] Iteration 41550 (1.34337 iter/s, 7.44394s/10 iters), loss = 5.9814
I0524 06:00:26.670121 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9814 (* 1 = 5.9814 loss)
I0524 06:00:27.056071 11835 sgd_solver.cpp:112] Iteration 41550, lr = 0.1
I0524 06:00:33.101557 11835 solver.cpp:239] Iteration 41560 (1.55493 iter/s, 6.43116s/10 iters), loss = 6.41483
I0524 06:00:33.101689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41483 (* 1 = 6.41483 loss)
I0524 06:00:33.101723 11835 sgd_solver.cpp:112] Iteration 41560, lr = 0.1
I0524 06:00:41.661346 11835 solver.cpp:239] Iteration 41570 (1.16831 iter/s, 8.55939s/10 iters), loss = 5.69361
I0524 06:00:41.661403 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.69361 (* 1 = 5.69361 loss)
I0524 06:00:41.661505 11835 sgd_solver.cpp:112] Iteration 41570, lr = 0.1
I0524 06:00:49.799399 11835 solver.cpp:239] Iteration 41580 (1.22885 iter/s, 8.13769s/10 iters), loss = 5.82922
I0524 06:00:49.799705 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82922 (* 1 = 5.82922 loss)
I0524 06:00:50.124593 11835 sgd_solver.cpp:112] Iteration 41580, lr = 0.1
I0524 06:00:59.409773 11835 solver.cpp:239] Iteration 41590 (1.04061 iter/s, 9.60973s/10 iters), loss = 6.19728
I0524 06:00:59.409832 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19728 (* 1 = 6.19728 loss)
I0524 06:00:59.410017 11835 sgd_solver.cpp:112] Iteration 41590, lr = 0.1
I0524 06:01:10.288260 11835 solver.cpp:239] Iteration 41600 (0.919285 iter/s, 10.878s/10 iters), loss = 5.68832
I0524 06:01:10.288311 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68832 (* 1 = 5.68832 loss)
I0524 06:01:11.110026 11835 sgd_solver.cpp:112] Iteration 41600, lr = 0.1
I0524 06:01:17.156294 11835 solver.cpp:239] Iteration 41610 (1.45609 iter/s, 6.86771s/10 iters), loss = 7.57681
I0524 06:01:17.156370 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.57681 (* 1 = 7.57681 loss)
I0524 06:01:17.156464 11835 sgd_solver.cpp:112] Iteration 41610, lr = 0.1
I0524 06:01:25.450408 11835 solver.cpp:239] Iteration 41620 (1.20573 iter/s, 8.29374s/10 iters), loss = 5.89679
I0524 06:01:25.450592 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89679 (* 1 = 5.89679 loss)
I0524 06:01:25.450611 11835 sgd_solver.cpp:112] Iteration 41620, lr = 0.1
I0524 06:01:31.750218 11835 solver.cpp:239] Iteration 41630 (1.58745 iter/s, 6.29939s/10 iters), loss = 6.11116
I0524 06:01:31.750277 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11116 (* 1 = 6.11116 loss)
I0524 06:01:31.921703 11835 sgd_solver.cpp:112] Iteration 41630, lr = 0.1
I0524 06:01:41.423161 11835 solver.cpp:239] Iteration 41640 (1.03386 iter/s, 9.67248s/10 iters), loss = 6.70301
I0524 06:01:41.423264 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70301 (* 1 = 6.70301 loss)
I0524 06:01:41.614018 11835 sgd_solver.cpp:112] Iteration 41640, lr = 0.1
I0524 06:01:48.462550 11835 solver.cpp:239] Iteration 41650 (1.42065 iter/s, 7.03903s/10 iters), loss = 7.50315
I0524 06:01:48.462610 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.50315 (* 1 = 7.50315 loss)
I0524 06:01:49.386941 11835 sgd_solver.cpp:112] Iteration 41650, lr = 0.1
I0524 06:01:56.141896 11835 solver.cpp:239] Iteration 41660 (1.30226 iter/s, 7.67898s/10 iters), loss = 5.66981
I0524 06:01:56.142040 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.66981 (* 1 = 5.66981 loss)
I0524 06:01:56.142163 11835 sgd_solver.cpp:112] Iteration 41660, lr = 0.1
I0524 06:02:02.260759 11835 solver.cpp:239] Iteration 41670 (1.63439 iter/s, 6.11848s/10 iters), loss = 5.90877
I0524 06:02:02.260807 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90877 (* 1 = 5.90877 loss)
I0524 06:02:02.377868 11835 sgd_solver.cpp:112] Iteration 41670, lr = 0.1
I0524 06:02:08.270121 11835 solver.cpp:239] Iteration 41680 (1.66415 iter/s, 6.00908s/10 iters), loss = 5.70758
I0524 06:02:08.270169 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.70758 (* 1 = 5.70758 loss)
I0524 06:02:08.270476 11835 sgd_solver.cpp:112] Iteration 41680, lr = 0.1
I0524 06:02:15.733289 11835 solver.cpp:239] Iteration 41690 (1.33998 iter/s, 7.46282s/10 iters), loss = 6.30773
I0524 06:02:15.733350 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30773 (* 1 = 6.30773 loss)
I0524 06:02:15.733403 11835 sgd_solver.cpp:112] Iteration 41690, lr = 0.1
I0524 06:02:21.957515 11835 solver.cpp:239] Iteration 41700 (1.60671 iter/s, 6.22391s/10 iters), loss = 6.23605
I0524 06:02:21.957567 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23605 (* 1 = 6.23605 loss)
I0524 06:02:21.957581 11835 sgd_solver.cpp:112] Iteration 41700, lr = 0.1
I0524 06:02:29.438179 11835 solver.cpp:239] Iteration 41710 (1.33684 iter/s, 7.48032s/10 iters), loss = 6.97807
I0524 06:02:29.438431 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.97807 (* 1 = 6.97807 loss)
I0524 06:02:29.522593 11835 sgd_solver.cpp:112] Iteration 41710, lr = 0.1
I0524 06:02:36.867358 11835 solver.cpp:239] Iteration 41720 (1.34614 iter/s, 7.42867s/10 iters), loss = 5.0744
I0524 06:02:36.867410 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.0744 (* 1 = 5.0744 loss)
I0524 06:02:36.867540 11835 sgd_solver.cpp:112] Iteration 41720, lr = 0.1
I0524 06:02:43.412318 11835 solver.cpp:239] Iteration 41730 (1.52796 iter/s, 6.54465s/10 iters), loss = 6.84365
I0524 06:02:43.412376 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84365 (* 1 = 6.84365 loss)
I0524 06:02:43.619617 11835 sgd_solver.cpp:112] Iteration 41730, lr = 0.1
I0524 06:02:50.040343 11835 solver.cpp:239] Iteration 41740 (1.50882 iter/s, 6.62769s/10 iters), loss = 6.79401
I0524 06:02:50.040391 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79401 (* 1 = 6.79401 loss)
I0524 06:02:50.040408 11835 sgd_solver.cpp:112] Iteration 41740, lr = 0.1
I0524 06:02:56.972875 11835 solver.cpp:239] Iteration 41750 (1.44255 iter/s, 6.93217s/10 iters), loss = 5.90497
I0524 06:02:56.972932 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90497 (* 1 = 5.90497 loss)
I0524 06:02:56.973034 11835 sgd_solver.cpp:112] Iteration 41750, lr = 0.1
I0524 06:03:05.079051 11835 solver.cpp:239] Iteration 41760 (1.23368 iter/s, 8.10581s/10 iters), loss = 5.24383
I0524 06:03:05.079206 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.24383 (* 1 = 5.24383 loss)
I0524 06:03:05.079221 11835 sgd_solver.cpp:112] Iteration 41760, lr = 0.1
I0524 06:03:12.362735 11835 solver.cpp:239] Iteration 41770 (1.37322 iter/s, 7.28214s/10 iters), loss = 6.34369
I0524 06:03:12.362787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34369 (* 1 = 6.34369 loss)
I0524 06:03:12.362802 11835 sgd_solver.cpp:112] Iteration 41770, lr = 0.1
I0524 06:03:19.874804 11835 solver.cpp:239] Iteration 41780 (1.33126 iter/s, 7.51165s/10 iters), loss = 6.12876
I0524 06:03:19.874862 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12876 (* 1 = 6.12876 loss)
I0524 06:03:19.875056 11835 sgd_solver.cpp:112] Iteration 41780, lr = 0.1
I0524 06:03:27.798517 11835 solver.cpp:239] Iteration 41790 (1.2621 iter/s, 7.92332s/10 iters), loss = 6.64237
I0524 06:03:27.798606 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64237 (* 1 = 6.64237 loss)
I0524 06:03:27.875739 11835 sgd_solver.cpp:112] Iteration 41790, lr = 0.1
I0524 06:03:36.831496 11835 solver.cpp:239] Iteration 41800 (1.10711 iter/s, 9.03256s/10 iters), loss = 5.53866
I0524 06:03:36.831645 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.53866 (* 1 = 5.53866 loss)
I0524 06:03:36.831665 11835 sgd_solver.cpp:112] Iteration 41800, lr = 0.1
I0524 06:03:42.854116 11835 solver.cpp:239] Iteration 41810 (1.66051 iter/s, 6.02224s/10 iters), loss = 5.10782
I0524 06:03:42.854164 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.10782 (* 1 = 5.10782 loss)
I0524 06:03:42.854411 11835 sgd_solver.cpp:112] Iteration 41810, lr = 0.1
I0524 06:03:49.761528 11835 solver.cpp:239] Iteration 41820 (1.44779 iter/s, 6.90709s/10 iters), loss = 5.5781
I0524 06:03:49.761576 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.5781 (* 1 = 5.5781 loss)
I0524 06:03:49.784935 11835 sgd_solver.cpp:112] Iteration 41820, lr = 0.1
I0524 06:03:57.627224 11835 solver.cpp:239] Iteration 41830 (1.27141 iter/s, 7.86531s/10 iters), loss = 6.04035
I0524 06:03:57.627324 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04035 (* 1 = 6.04035 loss)
I0524 06:03:57.702958 11835 sgd_solver.cpp:112] Iteration 41830, lr = 0.1
I0524 06:04:04.443400 11835 solver.cpp:239] Iteration 41840 (1.46718 iter/s, 6.81582s/10 iters), loss = 6.33225
I0524 06:04:04.443452 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33225 (* 1 = 6.33225 loss)
I0524 06:04:05.006583 11835 sgd_solver.cpp:112] Iteration 41840, lr = 0.1
I0524 06:04:11.422751 11835 solver.cpp:239] Iteration 41850 (1.43286 iter/s, 6.97903s/10 iters), loss = 7.0921
I0524 06:04:11.423010 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.0921 (* 1 = 7.0921 loss)
I0524 06:04:11.423161 11835 sgd_solver.cpp:112] Iteration 41850, lr = 0.1
I0524 06:04:17.817215 11835 solver.cpp:239] Iteration 41860 (1.56397 iter/s, 6.39396s/10 iters), loss = 6.60093
I0524 06:04:17.817294 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60093 (* 1 = 6.60093 loss)
I0524 06:04:17.817409 11835 sgd_solver.cpp:112] Iteration 41860, lr = 0.1
I0524 06:04:25.166061 11835 solver.cpp:239] Iteration 41870 (1.36082 iter/s, 7.3485s/10 iters), loss = 6.2562
I0524 06:04:25.166110 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2562 (* 1 = 6.2562 loss)
I0524 06:04:25.166237 11835 sgd_solver.cpp:112] Iteration 41870, lr = 0.1
I0524 06:04:33.330137 11835 solver.cpp:239] Iteration 41880 (1.22493 iter/s, 8.16371s/10 iters), loss = 5.37404
I0524 06:04:33.330188 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.37404 (* 1 = 5.37404 loss)
I0524 06:04:33.973994 11835 sgd_solver.cpp:112] Iteration 41880, lr = 0.1
I0524 06:04:43.903192 11835 solver.cpp:239] Iteration 41890 (0.945844 iter/s, 10.5726s/10 iters), loss = 5.88748
I0524 06:04:43.903458 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88748 (* 1 = 5.88748 loss)
I0524 06:04:43.903513 11835 sgd_solver.cpp:112] Iteration 41890, lr = 0.1
I0524 06:04:50.580214 11835 solver.cpp:239] Iteration 41900 (1.49783 iter/s, 6.67631s/10 iters), loss = 6.05805
I0524 06:04:50.580263 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05805 (* 1 = 6.05805 loss)
I0524 06:04:50.580492 11835 sgd_solver.cpp:112] Iteration 41900, lr = 0.1
I0524 06:04:56.543407 11835 solver.cpp:239] Iteration 41910 (1.67704 iter/s, 5.96289s/10 iters), loss = 6.05626
I0524 06:04:56.543467 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05626 (* 1 = 6.05626 loss)
I0524 06:04:56.543678 11835 sgd_solver.cpp:112] Iteration 41910, lr = 0.1
I0524 06:05:02.779508 11835 solver.cpp:239] Iteration 41920 (1.60364 iter/s, 6.23581s/10 iters), loss = 5.68276
I0524 06:05:02.779559 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68276 (* 1 = 5.68276 loss)
I0524 06:05:02.978942 11835 sgd_solver.cpp:112] Iteration 41920, lr = 0.1
I0524 06:05:11.016394 11835 solver.cpp:239] Iteration 41930 (1.21411 iter/s, 8.23646s/10 iters), loss = 6.16727
I0524 06:05:11.016490 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16727 (* 1 = 6.16727 loss)
I0524 06:05:11.366313 11835 sgd_solver.cpp:112] Iteration 41930, lr = 0.1
I0524 06:05:18.056041 11835 solver.cpp:239] Iteration 41940 (1.4206 iter/s, 7.0393s/10 iters), loss = 6.09321
I0524 06:05:18.056279 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09321 (* 1 = 6.09321 loss)
I0524 06:05:19.030273 11835 sgd_solver.cpp:112] Iteration 41940, lr = 0.1
I0524 06:05:26.879387 11835 solver.cpp:239] Iteration 41950 (1.13343 iter/s, 8.82278s/10 iters), loss = 6.13424
I0524 06:05:26.879461 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13424 (* 1 = 6.13424 loss)
I0524 06:05:26.879626 11835 sgd_solver.cpp:112] Iteration 41950, lr = 0.1
I0524 06:05:36.432145 11835 solver.cpp:239] Iteration 41960 (1.04686 iter/s, 9.55234s/10 iters), loss = 6.205
I0524 06:05:36.432194 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.205 (* 1 = 6.205 loss)
I0524 06:05:36.432433 11835 sgd_solver.cpp:112] Iteration 41960, lr = 0.1
I0524 06:05:44.386440 11835 solver.cpp:239] Iteration 41970 (1.25724 iter/s, 7.95394s/10 iters), loss = 5.9589
I0524 06:05:44.386490 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9589 (* 1 = 5.9589 loss)
I0524 06:05:44.386504 11835 sgd_solver.cpp:112] Iteration 41970, lr = 0.1
I0524 06:05:53.974035 11835 solver.cpp:239] Iteration 41980 (1.04307 iter/s, 9.58706s/10 iters), loss = 5.2962
I0524 06:05:53.974309 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.2962 (* 1 = 5.2962 loss)
I0524 06:05:53.974465 11835 sgd_solver.cpp:112] Iteration 41980, lr = 0.1
I0524 06:06:01.002355 11835 solver.cpp:239] Iteration 41990 (1.42292 iter/s, 7.02781s/10 iters), loss = 6.91359
I0524 06:06:01.002408 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.91359 (* 1 = 6.91359 loss)
I0524 06:06:01.002617 11835 sgd_solver.cpp:112] Iteration 41990, lr = 0.1
I0524 06:06:08.998185 11835 solver.cpp:239] Iteration 42000 (1.25071 iter/s, 7.99548s/10 iters), loss = 6.34859
I0524 06:06:08.998224 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34859 (* 1 = 6.34859 loss)
I0524 06:06:09.789825 11835 sgd_solver.cpp:112] Iteration 42000, lr = 0.1
I0524 06:06:17.455912 11835 solver.cpp:239] Iteration 42010 (1.1824 iter/s, 8.45736s/10 iters), loss = 6.25994
I0524 06:06:17.455955 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25994 (* 1 = 6.25994 loss)
I0524 06:06:17.501056 11835 sgd_solver.cpp:112] Iteration 42010, lr = 0.1
I0524 06:06:25.479063 11835 solver.cpp:239] Iteration 42020 (1.24645 iter/s, 8.02278s/10 iters), loss = 5.94204
I0524 06:06:25.479347 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94204 (* 1 = 5.94204 loss)
I0524 06:06:25.479403 11835 sgd_solver.cpp:112] Iteration 42020, lr = 0.1
I0524 06:06:31.942781 11835 solver.cpp:239] Iteration 42030 (1.54721 iter/s, 6.46326s/10 iters), loss = 6.88193
I0524 06:06:31.942911 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88193 (* 1 = 6.88193 loss)
I0524 06:06:31.943055 11835 sgd_solver.cpp:112] Iteration 42030, lr = 0.1
I0524 06:06:38.674149 11835 solver.cpp:239] Iteration 42040 (1.48565 iter/s, 6.73105s/10 iters), loss = 6.34159
I0524 06:06:38.674196 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34159 (* 1 = 6.34159 loss)
I0524 06:06:38.674656 11835 sgd_solver.cpp:112] Iteration 42040, lr = 0.1
I0524 06:06:46.458919 11835 solver.cpp:239] Iteration 42050 (1.28462 iter/s, 7.78441s/10 iters), loss = 5.36827
I0524 06:06:46.458972 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.36827 (* 1 = 5.36827 loss)
I0524 06:06:46.473536 11835 sgd_solver.cpp:112] Iteration 42050, lr = 0.1
I0524 06:06:53.751363 11835 solver.cpp:239] Iteration 42060 (1.37135 iter/s, 7.2921s/10 iters), loss = 6.6734
I0524 06:06:53.751456 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6734 (* 1 = 6.6734 loss)
I0524 06:06:53.751610 11835 sgd_solver.cpp:112] Iteration 42060, lr = 0.1
I0524 06:06:59.809082 11835 solver.cpp:239] Iteration 42070 (1.65086 iter/s, 6.05743s/10 iters), loss = 7.22331
I0524 06:06:59.809222 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.22331 (* 1 = 7.22331 loss)
I0524 06:07:00.546370 11835 sgd_solver.cpp:112] Iteration 42070, lr = 0.1
I0524 06:07:08.353385 11835 solver.cpp:239] Iteration 42080 (1.17044 iter/s, 8.54383s/10 iters), loss = 6.12266
I0524 06:07:08.353457 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12266 (* 1 = 6.12266 loss)
I0524 06:07:08.353735 11835 sgd_solver.cpp:112] Iteration 42080, lr = 0.1
I0524 06:07:15.788290 11835 solver.cpp:239] Iteration 42090 (1.34507 iter/s, 7.43456s/10 iters), loss = 5.93583
I0524 06:07:15.788409 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93583 (* 1 = 5.93583 loss)
I0524 06:07:16.089655 11835 sgd_solver.cpp:112] Iteration 42090, lr = 0.1
I0524 06:07:22.859871 11835 solver.cpp:239] Iteration 42100 (1.41417 iter/s, 7.07127s/10 iters), loss = 5.92543
I0524 06:07:22.859913 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92543 (* 1 = 5.92543 loss)
I0524 06:07:22.896683 11835 sgd_solver.cpp:112] Iteration 42100, lr = 0.1
I0524 06:07:29.795469 11835 solver.cpp:239] Iteration 42110 (1.4419 iter/s, 6.93527s/10 iters), loss = 5.90408
I0524 06:07:29.795544 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90408 (* 1 = 5.90408 loss)
I0524 06:07:30.672634 11835 sgd_solver.cpp:112] Iteration 42110, lr = 0.1
I0524 06:07:37.260164 11835 solver.cpp:239] Iteration 42120 (1.3397 iter/s, 7.46434s/10 iters), loss = 6.58105
I0524 06:07:37.260229 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58105 (* 1 = 6.58105 loss)
I0524 06:07:37.260450 11835 sgd_solver.cpp:112] Iteration 42120, lr = 0.1
I0524 06:07:47.388684 11835 solver.cpp:239] Iteration 42130 (0.987354 iter/s, 10.1281s/10 iters), loss = 6.07853
I0524 06:07:47.388739 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07853 (* 1 = 6.07853 loss)
I0524 06:07:47.388770 11835 sgd_solver.cpp:112] Iteration 42130, lr = 0.1
I0524 06:07:54.170881 11835 solver.cpp:239] Iteration 42140 (1.47452 iter/s, 6.78187s/10 iters), loss = 6.63536
I0524 06:07:54.170948 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63536 (* 1 = 6.63536 loss)
I0524 06:07:54.313189 11835 sgd_solver.cpp:112] Iteration 42140, lr = 0.1
I0524 06:08:00.688777 11835 solver.cpp:239] Iteration 42150 (1.53431 iter/s, 6.51758s/10 iters), loss = 6.36655
I0524 06:08:00.689064 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36655 (* 1 = 6.36655 loss)
I0524 06:08:00.689117 11835 sgd_solver.cpp:112] Iteration 42150, lr = 0.1
I0524 06:08:07.536267 11835 solver.cpp:239] Iteration 42160 (1.46049 iter/s, 6.84702s/10 iters), loss = 5.99677
I0524 06:08:07.536307 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99677 (* 1 = 5.99677 loss)
I0524 06:08:07.536447 11835 sgd_solver.cpp:112] Iteration 42160, lr = 0.1
I0524 06:08:15.261040 11835 solver.cpp:239] Iteration 42170 (1.29459 iter/s, 7.72442s/10 iters), loss = 6.26001
I0524 06:08:15.261091 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26001 (* 1 = 6.26001 loss)
I0524 06:08:15.349054 11835 sgd_solver.cpp:112] Iteration 42170, lr = 0.1
I0524 06:08:22.699519 11835 solver.cpp:239] Iteration 42180 (1.34442 iter/s, 7.43814s/10 iters), loss = 6.08888
I0524 06:08:22.699643 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08888 (* 1 = 6.08888 loss)
I0524 06:08:22.699760 11835 sgd_solver.cpp:112] Iteration 42180, lr = 0.1
I0524 06:08:28.980114 11835 solver.cpp:239] Iteration 42190 (1.59228 iter/s, 6.2803s/10 iters), loss = 5.81234
I0524 06:08:28.980160 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81234 (* 1 = 5.81234 loss)
I0524 06:08:28.980403 11835 sgd_solver.cpp:112] Iteration 42190, lr = 0.1
I0524 06:08:40.178246 11835 solver.cpp:239] Iteration 42200 (0.893045 iter/s, 11.1976s/10 iters), loss = 6.89512
I0524 06:08:40.178474 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89512 (* 1 = 6.89512 loss)
I0524 06:08:40.255141 11835 sgd_solver.cpp:112] Iteration 42200, lr = 0.1
I0524 06:08:47.512554 11835 solver.cpp:239] Iteration 42210 (1.36355 iter/s, 7.33381s/10 iters), loss = 6.32124
I0524 06:08:47.512636 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32124 (* 1 = 6.32124 loss)
I0524 06:08:47.512825 11835 sgd_solver.cpp:112] Iteration 42210, lr = 0.1
I0524 06:08:53.639647 11835 solver.cpp:239] Iteration 42220 (1.63218 iter/s, 6.12677s/10 iters), loss = 5.5693
I0524 06:08:53.639730 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.5693 (* 1 = 5.5693 loss)
I0524 06:08:53.639756 11835 sgd_solver.cpp:112] Iteration 42220, lr = 0.1
I0524 06:09:00.801682 11835 solver.cpp:239] Iteration 42230 (1.39633 iter/s, 7.16165s/10 iters), loss = 5.61127
I0524 06:09:00.801723 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.61127 (* 1 = 5.61127 loss)
I0524 06:09:00.801733 11835 sgd_solver.cpp:112] Iteration 42230, lr = 0.1
I0524 06:09:06.582590 11835 solver.cpp:239] Iteration 42240 (1.72992 iter/s, 5.78062s/10 iters), loss = 7.64364
I0524 06:09:06.582645 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.64364 (* 1 = 7.64364 loss)
I0524 06:09:06.714665 11835 sgd_solver.cpp:112] Iteration 42240, lr = 0.1
I0524 06:09:13.207376 11835 solver.cpp:239] Iteration 42250 (1.50956 iter/s, 6.62445s/10 iters), loss = 5.93768
I0524 06:09:13.207648 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93768 (* 1 = 5.93768 loss)
I0524 06:09:13.907733 11835 sgd_solver.cpp:112] Iteration 42250, lr = 0.1
I0524 06:09:20.806191 11835 solver.cpp:239] Iteration 42260 (1.31609 iter/s, 7.59827s/10 iters), loss = 6.01748
I0524 06:09:20.806241 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01748 (* 1 = 6.01748 loss)
I0524 06:09:21.383157 11835 sgd_solver.cpp:112] Iteration 42260, lr = 0.1
I0524 06:09:28.125083 11835 solver.cpp:239] Iteration 42270 (1.36639 iter/s, 7.31855s/10 iters), loss = 6.41254
I0524 06:09:28.125140 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41254 (* 1 = 6.41254 loss)
I0524 06:09:28.246525 11835 sgd_solver.cpp:112] Iteration 42270, lr = 0.1
I0524 06:09:37.681644 11835 solver.cpp:239] Iteration 42280 (1.04645 iter/s, 9.55614s/10 iters), loss = 6.97303
I0524 06:09:37.681725 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.97303 (* 1 = 6.97303 loss)
I0524 06:09:37.682008 11835 sgd_solver.cpp:112] Iteration 42280, lr = 0.1
I0524 06:09:44.567404 11835 solver.cpp:239] Iteration 42290 (1.45234 iter/s, 6.88544s/10 iters), loss = 5.82946
I0524 06:09:44.567629 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82946 (* 1 = 5.82946 loss)
I0524 06:09:44.753113 11835 sgd_solver.cpp:112] Iteration 42290, lr = 0.1
I0524 06:09:52.160102 11835 solver.cpp:239] Iteration 42300 (1.31714 iter/s, 7.5922s/10 iters), loss = 6.415
I0524 06:09:52.160176 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.415 (* 1 = 6.415 loss)
I0524 06:09:52.160363 11835 sgd_solver.cpp:112] Iteration 42300, lr = 0.1
I0524 06:09:58.016394 11835 solver.cpp:239] Iteration 42310 (1.70765 iter/s, 5.85599s/10 iters), loss = 5.97567
I0524 06:09:58.016443 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97567 (* 1 = 5.97567 loss)
I0524 06:09:58.016456 11835 sgd_solver.cpp:112] Iteration 42310, lr = 0.1
I0524 06:10:05.578183 11835 solver.cpp:239] Iteration 42320 (1.3225 iter/s, 7.56145s/10 iters), loss = 6.77821
I0524 06:10:05.578238 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.77821 (* 1 = 6.77821 loss)
I0524 06:10:05.578366 11835 sgd_solver.cpp:112] Iteration 42320, lr = 0.1
I0524 06:10:12.016839 11835 solver.cpp:239] Iteration 42330 (1.55319 iter/s, 6.43836s/10 iters), loss = 5.62414
I0524 06:10:12.016888 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.62414 (* 1 = 5.62414 loss)
I0524 06:10:12.017118 11835 sgd_solver.cpp:112] Iteration 42330, lr = 0.1
I0524 06:10:18.916239 11835 solver.cpp:239] Iteration 42340 (1.44947 iter/s, 6.89908s/10 iters), loss = 6.09986
I0524 06:10:18.916478 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09986 (* 1 = 6.09986 loss)
I0524 06:10:19.115835 11835 sgd_solver.cpp:112] Iteration 42340, lr = 0.1
I0524 06:10:26.063329 11835 solver.cpp:239] Iteration 42350 (1.39927 iter/s, 7.14659s/10 iters), loss = 5.92869
I0524 06:10:26.063494 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92869 (* 1 = 5.92869 loss)
I0524 06:10:26.063650 11835 sgd_solver.cpp:112] Iteration 42350, lr = 0.1
I0524 06:10:34.935624 11835 solver.cpp:239] Iteration 42360 (1.12716 iter/s, 8.87189s/10 iters), loss = 5.49318
I0524 06:10:34.935665 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.49318 (* 1 = 5.49318 loss)
I0524 06:10:34.991979 11835 sgd_solver.cpp:112] Iteration 42360, lr = 0.1
I0524 06:10:41.628163 11835 solver.cpp:239] Iteration 42370 (1.49429 iter/s, 6.69214s/10 iters), loss = 5.6217
I0524 06:10:41.628291 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.6217 (* 1 = 5.6217 loss)
I0524 06:10:41.628335 11835 sgd_solver.cpp:112] Iteration 42370, lr = 0.1
I0524 06:10:48.027976 11835 solver.cpp:239] Iteration 42380 (1.56268 iter/s, 6.39928s/10 iters), loss = 5.51646
I0524 06:10:48.028026 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.51646 (* 1 = 5.51646 loss)
I0524 06:10:48.028043 11835 sgd_solver.cpp:112] Iteration 42380, lr = 0.1
I0524 06:10:54.964499 11835 solver.cpp:239] Iteration 42390 (1.44171 iter/s, 6.93621s/10 iters), loss = 6.35857
I0524 06:10:54.964731 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35857 (* 1 = 6.35857 loss)
I0524 06:10:55.751673 11835 sgd_solver.cpp:112] Iteration 42390, lr = 0.1
I0524 06:11:02.212474 11835 solver.cpp:239] Iteration 42400 (1.3798 iter/s, 7.24744s/10 iters), loss = 7.03699
I0524 06:11:02.212612 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03699 (* 1 = 7.03699 loss)
I0524 06:11:02.212716 11835 sgd_solver.cpp:112] Iteration 42400, lr = 0.1
I0524 06:11:12.737824 11835 solver.cpp:239] Iteration 42410 (0.950131 iter/s, 10.5249s/10 iters), loss = 5.91971
I0524 06:11:12.737879 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91971 (* 1 = 5.91971 loss)
I0524 06:11:13.976018 11835 sgd_solver.cpp:112] Iteration 42410, lr = 0.1
I0524 06:11:21.680361 11835 solver.cpp:239] Iteration 42420 (1.1183 iter/s, 8.94213s/10 iters), loss = 7.14869
I0524 06:11:21.680438 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.14869 (* 1 = 7.14869 loss)
I0524 06:11:21.680665 11835 sgd_solver.cpp:112] Iteration 42420, lr = 0.1
I0524 06:11:27.636662 11835 solver.cpp:239] Iteration 42430 (1.67897 iter/s, 5.95602s/10 iters), loss = 5.96858
I0524 06:11:27.636843 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96858 (* 1 = 5.96858 loss)
I0524 06:11:27.636857 11835 sgd_solver.cpp:112] Iteration 42430, lr = 0.1
I0524 06:11:34.094458 11835 solver.cpp:239] Iteration 42440 (1.54863 iter/s, 6.45732s/10 iters), loss = 6.17519
I0524 06:11:34.094533 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17519 (* 1 = 6.17519 loss)
I0524 06:11:34.094735 11835 sgd_solver.cpp:112] Iteration 42440, lr = 0.1
I0524 06:11:41.317688 11835 solver.cpp:239] Iteration 42450 (1.38448 iter/s, 7.2229s/10 iters), loss = 6.02572
I0524 06:11:41.317744 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02572 (* 1 = 6.02572 loss)
I0524 06:11:41.981354 11835 sgd_solver.cpp:112] Iteration 42450, lr = 0.1
I0524 06:11:49.098438 11835 solver.cpp:239] Iteration 42460 (1.28528 iter/s, 7.7804s/10 iters), loss = 6.86589
I0524 06:11:49.098489 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86589 (* 1 = 6.86589 loss)
I0524 06:11:49.098610 11835 sgd_solver.cpp:112] Iteration 42460, lr = 0.1
I0524 06:11:56.451056 11835 solver.cpp:239] Iteration 42470 (1.36012 iter/s, 7.35227s/10 iters), loss = 5.01536
I0524 06:11:56.451131 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.01536 (* 1 = 5.01536 loss)
I0524 06:11:57.018971 11835 sgd_solver.cpp:112] Iteration 42470, lr = 0.1
I0524 06:12:03.320520 11835 solver.cpp:239] Iteration 42480 (1.45579 iter/s, 6.86914s/10 iters), loss = 6.68908
I0524 06:12:03.320752 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.68908 (* 1 = 6.68908 loss)
I0524 06:12:03.320786 11835 sgd_solver.cpp:112] Iteration 42480, lr = 0.1
I0524 06:12:10.055989 11835 solver.cpp:239] Iteration 42490 (1.4848 iter/s, 6.73491s/10 iters), loss = 5.53845
I0524 06:12:10.056138 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.53845 (* 1 = 5.53845 loss)
I0524 06:12:10.056213 11835 sgd_solver.cpp:112] Iteration 42490, lr = 0.1
I0524 06:12:16.067288 11835 solver.cpp:239] Iteration 42500 (1.66362 iter/s, 6.01099s/10 iters), loss = 6.37542
I0524 06:12:16.067335 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37542 (* 1 = 6.37542 loss)
I0524 06:12:16.067759 11835 sgd_solver.cpp:112] Iteration 42500, lr = 0.1
I0524 06:12:22.797492 11835 solver.cpp:239] Iteration 42510 (1.48591 iter/s, 6.72989s/10 iters), loss = 6.3468
I0524 06:12:22.797540 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3468 (* 1 = 6.3468 loss)
I0524 06:12:23.267014 11835 sgd_solver.cpp:112] Iteration 42510, lr = 0.1
I0524 06:12:30.931429 11835 solver.cpp:239] Iteration 42520 (1.22948 iter/s, 8.13354s/10 iters), loss = 6.66344
I0524 06:12:30.931543 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66344 (* 1 = 6.66344 loss)
I0524 06:12:30.948006 11835 sgd_solver.cpp:112] Iteration 42520, lr = 0.1
I0524 06:12:37.239667 11835 solver.cpp:239] Iteration 42530 (1.58531 iter/s, 6.30792s/10 iters), loss = 7.07942
I0524 06:12:37.239866 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.07942 (* 1 = 7.07942 loss)
I0524 06:12:37.266852 11835 sgd_solver.cpp:112] Iteration 42530, lr = 0.1
I0524 06:12:44.140669 11835 solver.cpp:239] Iteration 42540 (1.44916 iter/s, 6.90055s/10 iters), loss = 5.82725
I0524 06:12:44.140728 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82725 (* 1 = 5.82725 loss)
I0524 06:12:44.140776 11835 sgd_solver.cpp:112] Iteration 42540, lr = 0.1
I0524 06:12:50.874397 11835 solver.cpp:239] Iteration 42550 (1.48513 iter/s, 6.7334s/10 iters), loss = 7.30189
I0524 06:12:50.874454 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.30189 (* 1 = 7.30189 loss)
I0524 06:12:50.880091 11835 sgd_solver.cpp:112] Iteration 42550, lr = 0.1
I0524 06:12:56.991227 11835 solver.cpp:239] Iteration 42560 (1.63491 iter/s, 6.11653s/10 iters), loss = 6.02485
I0524 06:12:56.991295 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02485 (* 1 = 6.02485 loss)
I0524 06:12:56.991323 11835 sgd_solver.cpp:112] Iteration 42560, lr = 0.1
I0524 06:13:05.130515 11835 solver.cpp:239] Iteration 42570 (1.22866 iter/s, 8.13893s/10 iters), loss = 7.30612
I0524 06:13:05.130554 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.30612 (* 1 = 7.30612 loss)
I0524 06:13:05.566151 11835 sgd_solver.cpp:112] Iteration 42570, lr = 0.1
I0524 06:13:12.758100 11835 solver.cpp:239] Iteration 42580 (1.31109 iter/s, 7.62724s/10 iters), loss = 6.54438
I0524 06:13:12.758273 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54438 (* 1 = 6.54438 loss)
I0524 06:13:12.805061 11835 sgd_solver.cpp:112] Iteration 42580, lr = 0.1
I0524 06:13:22.216071 11835 solver.cpp:239] Iteration 42590 (1.05737 iter/s, 9.45743s/10 iters), loss = 5.93477
I0524 06:13:22.216136 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93477 (* 1 = 5.93477 loss)
I0524 06:13:22.216351 11835 sgd_solver.cpp:112] Iteration 42590, lr = 0.1
I0524 06:13:28.069237 11835 solver.cpp:239] Iteration 42600 (1.70856 iter/s, 5.85288s/10 iters), loss = 6.01218
I0524 06:13:28.069284 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01218 (* 1 = 6.01218 loss)
I0524 06:13:28.069299 11835 sgd_solver.cpp:112] Iteration 42600, lr = 0.1
I0524 06:13:36.250373 11835 solver.cpp:239] Iteration 42610 (1.22239 iter/s, 8.1807s/10 iters), loss = 5.53412
I0524 06:13:36.250433 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.53412 (* 1 = 5.53412 loss)
I0524 06:13:36.250453 11835 sgd_solver.cpp:112] Iteration 42610, lr = 0.1
I0524 06:13:43.871830 11835 solver.cpp:239] Iteration 42620 (1.31215 iter/s, 7.6211s/10 iters), loss = 5.94695
I0524 06:13:43.872067 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94695 (* 1 = 5.94695 loss)
I0524 06:13:45.040720 11835 sgd_solver.cpp:112] Iteration 42620, lr = 0.1
I0524 06:13:50.852488 11835 solver.cpp:239] Iteration 42630 (1.43263 iter/s, 6.98019s/10 iters), loss = 6.63278
I0524 06:13:50.852538 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63278 (* 1 = 6.63278 loss)
I0524 06:13:51.244215 11835 sgd_solver.cpp:112] Iteration 42630, lr = 0.1
I0524 06:13:59.370198 11835 solver.cpp:239] Iteration 42640 (1.17408 iter/s, 8.51733s/10 iters), loss = 6.98867
I0524 06:13:59.370249 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98867 (* 1 = 6.98867 loss)
I0524 06:13:59.370302 11835 sgd_solver.cpp:112] Iteration 42640, lr = 0.1
I0524 06:14:08.198226 11835 solver.cpp:239] Iteration 42650 (1.13281 iter/s, 8.82763s/10 iters), loss = 6.31505
I0524 06:14:08.198281 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31505 (* 1 = 6.31505 loss)
I0524 06:14:08.198295 11835 sgd_solver.cpp:112] Iteration 42650, lr = 0.1
I0524 06:14:16.259680 11835 solver.cpp:239] Iteration 42660 (1.24054 iter/s, 8.06098s/10 iters), loss = 5.51662
I0524 06:14:16.259992 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.51662 (* 1 = 5.51662 loss)
I0524 06:14:16.260038 11835 sgd_solver.cpp:112] Iteration 42660, lr = 0.1
I0524 06:14:25.546916 11835 solver.cpp:239] Iteration 42670 (1.07683 iter/s, 9.28656s/10 iters), loss = 6.32385
I0524 06:14:25.547004 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32385 (* 1 = 6.32385 loss)
I0524 06:14:25.547021 11835 sgd_solver.cpp:112] Iteration 42670, lr = 0.1
I0524 06:14:32.187160 11835 solver.cpp:239] Iteration 42680 (1.50653 iter/s, 6.63776s/10 iters), loss = 6.15436
I0524 06:14:32.187218 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15436 (* 1 = 6.15436 loss)
I0524 06:14:32.187325 11835 sgd_solver.cpp:112] Iteration 42680, lr = 0.1
I0524 06:14:41.012665 11835 solver.cpp:239] Iteration 42690 (1.13313 iter/s, 8.82511s/10 iters), loss = 5.88617
I0524 06:14:41.012713 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88617 (* 1 = 5.88617 loss)
I0524 06:14:41.012739 11835 sgd_solver.cpp:112] Iteration 42690, lr = 0.1
I0524 06:14:48.583540 11835 solver.cpp:239] Iteration 42700 (1.3211 iter/s, 7.56943s/10 iters), loss = 6.3262
I0524 06:14:48.583745 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3262 (* 1 = 6.3262 loss)
I0524 06:14:49.112498 11835 sgd_solver.cpp:112] Iteration 42700, lr = 0.1
I0524 06:14:55.158498 11835 solver.cpp:239] Iteration 42710 (1.52102 iter/s, 6.57452s/10 iters), loss = 6.803
I0524 06:14:55.158538 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.803 (* 1 = 6.803 loss)
I0524 06:14:55.158934 11835 sgd_solver.cpp:112] Iteration 42710, lr = 0.1
I0524 06:15:02.563462 11835 solver.cpp:239] Iteration 42720 (1.35051 iter/s, 7.40463s/10 iters), loss = 5.93569
I0524 06:15:02.563513 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93569 (* 1 = 5.93569 loss)
I0524 06:15:03.645505 11835 sgd_solver.cpp:112] Iteration 42720, lr = 0.1
I0524 06:15:11.344439 11835 solver.cpp:239] Iteration 42730 (1.13888 iter/s, 8.78058s/10 iters), loss = 6.87255
I0524 06:15:11.344503 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87255 (* 1 = 6.87255 loss)
I0524 06:15:11.344650 11835 sgd_solver.cpp:112] Iteration 42730, lr = 0.1
I0524 06:15:18.026635 11835 solver.cpp:239] Iteration 42740 (1.49659 iter/s, 6.68186s/10 iters), loss = 5.79552
I0524 06:15:18.026736 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79552 (* 1 = 5.79552 loss)
I0524 06:15:18.026764 11835 sgd_solver.cpp:112] Iteration 42740, lr = 0.1
I0524 06:15:25.898952 11835 solver.cpp:239] Iteration 42750 (1.27034 iter/s, 7.87193s/10 iters), loss = 5.75303
I0524 06:15:25.899194 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75303 (* 1 = 5.75303 loss)
I0524 06:15:25.899209 11835 sgd_solver.cpp:112] Iteration 42750, lr = 0.1
I0524 06:15:33.078735 11835 solver.cpp:239] Iteration 42760 (1.39292 iter/s, 7.17917s/10 iters), loss = 6.14663
I0524 06:15:33.078786 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14663 (* 1 = 6.14663 loss)
I0524 06:15:33.078979 11835 sgd_solver.cpp:112] Iteration 42760, lr = 0.1
I0524 06:15:39.076995 11835 solver.cpp:239] Iteration 42770 (1.66733 iter/s, 5.99762s/10 iters), loss = 6.26059
I0524 06:15:39.077073 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26059 (* 1 = 6.26059 loss)
I0524 06:15:39.077105 11835 sgd_solver.cpp:112] Iteration 42770, lr = 0.1
I0524 06:15:46.941612 11835 solver.cpp:239] Iteration 42780 (1.27191 iter/s, 7.8622s/10 iters), loss = 5.71231
I0524 06:15:46.941659 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71231 (* 1 = 5.71231 loss)
I0524 06:15:47.683313 11835 sgd_solver.cpp:112] Iteration 42780, lr = 0.1
I0524 06:15:55.299273 11835 solver.cpp:239] Iteration 42790 (1.19656 iter/s, 8.35727s/10 iters), loss = 6.6832
I0524 06:15:55.299347 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6832 (* 1 = 6.6832 loss)
I0524 06:15:55.299371 11835 sgd_solver.cpp:112] Iteration 42790, lr = 0.1
I0524 06:16:01.405997 11835 solver.cpp:239] Iteration 42800 (1.6382 iter/s, 6.10426s/10 iters), loss = 5.98108
I0524 06:16:01.406297 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98108 (* 1 = 5.98108 loss)
I0524 06:16:01.406352 11835 sgd_solver.cpp:112] Iteration 42800, lr = 0.1
I0524 06:16:10.071857 11835 solver.cpp:239] Iteration 42810 (1.15403 iter/s, 8.66528s/10 iters), loss = 6.72072
I0524 06:16:10.071913 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72072 (* 1 = 6.72072 loss)
I0524 06:16:10.072140 11835 sgd_solver.cpp:112] Iteration 42810, lr = 0.1
I0524 06:16:20.947443 11835 solver.cpp:239] Iteration 42820 (0.91953 iter/s, 10.8751s/10 iters), loss = 5.74547
I0524 06:16:20.947505 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74547 (* 1 = 5.74547 loss)
I0524 06:16:20.947527 11835 sgd_solver.cpp:112] Iteration 42820, lr = 0.1
I0524 06:16:30.869628 11835 solver.cpp:239] Iteration 42830 (1.008 iter/s, 9.92064s/10 iters), loss = 5.0113
I0524 06:16:30.869688 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.0113 (* 1 = 5.0113 loss)
I0524 06:16:30.870024 11835 sgd_solver.cpp:112] Iteration 42830, lr = 0.1
I0524 06:16:37.499569 11835 solver.cpp:239] Iteration 42840 (1.50838 iter/s, 6.62963s/10 iters), loss = 6.54897
I0524 06:16:37.499802 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54897 (* 1 = 6.54897 loss)
I0524 06:16:37.499855 11835 sgd_solver.cpp:112] Iteration 42840, lr = 0.1
I0524 06:16:46.294490 11835 solver.cpp:239] Iteration 42850 (1.13709 iter/s, 8.79438s/10 iters), loss = 6.5707
I0524 06:16:46.294550 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5707 (* 1 = 6.5707 loss)
I0524 06:16:46.294863 11835 sgd_solver.cpp:112] Iteration 42850, lr = 0.1
I0524 06:16:52.726223 11835 solver.cpp:239] Iteration 42860 (1.55486 iter/s, 6.43144s/10 iters), loss = 6.46265
I0524 06:16:52.726260 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46265 (* 1 = 6.46265 loss)
I0524 06:16:52.726274 11835 sgd_solver.cpp:112] Iteration 42860, lr = 0.1
I0524 06:17:01.390779 11835 solver.cpp:239] Iteration 42870 (1.15419 iter/s, 8.6641s/10 iters), loss = 5.31102
I0524 06:17:01.390832 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.31102 (* 1 = 5.31102 loss)
I0524 06:17:01.518602 11835 sgd_solver.cpp:112] Iteration 42870, lr = 0.1
I0524 06:17:07.391587 11835 solver.cpp:239] Iteration 42880 (1.66652 iter/s, 6.00051s/10 iters), loss = 6.04352
I0524 06:17:07.391640 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04352 (* 1 = 6.04352 loss)
I0524 06:17:07.392009 11835 sgd_solver.cpp:112] Iteration 42880, lr = 0.1
I0524 06:17:13.734288 11835 solver.cpp:239] Iteration 42890 (1.57669 iter/s, 6.3424s/10 iters), loss = 6.47237
I0524 06:17:13.734482 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.47237 (* 1 = 6.47237 loss)
I0524 06:17:13.959803 11835 sgd_solver.cpp:112] Iteration 42890, lr = 0.1
I0524 06:17:20.313835 11835 solver.cpp:239] Iteration 42900 (1.51996 iter/s, 6.5791s/10 iters), loss = 6.00561
I0524 06:17:20.313905 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00561 (* 1 = 6.00561 loss)
I0524 06:17:21.263993 11835 sgd_solver.cpp:112] Iteration 42900, lr = 0.1
I0524 06:17:28.105599 11835 solver.cpp:239] Iteration 42910 (1.28347 iter/s, 7.7914s/10 iters), loss = 6.36419
I0524 06:17:28.105660 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36419 (* 1 = 6.36419 loss)
I0524 06:17:28.105675 11835 sgd_solver.cpp:112] Iteration 42910, lr = 0.1
I0524 06:17:35.752223 11835 solver.cpp:239] Iteration 42920 (1.30783 iter/s, 7.64624s/10 iters), loss = 6.36489
I0524 06:17:35.752290 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36489 (* 1 = 6.36489 loss)
I0524 06:17:35.752365 11835 sgd_solver.cpp:112] Iteration 42920, lr = 0.1
I0524 06:17:41.757416 11835 solver.cpp:239] Iteration 42930 (1.66531 iter/s, 6.00489s/10 iters), loss = 5.48158
I0524 06:17:41.757465 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.48158 (* 1 = 5.48158 loss)
I0524 06:17:41.837349 11835 sgd_solver.cpp:112] Iteration 42930, lr = 0.1
I0524 06:17:47.855087 11835 solver.cpp:239] Iteration 42940 (1.64005 iter/s, 6.09738s/10 iters), loss = 6.07761
I0524 06:17:47.855365 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07761 (* 1 = 6.07761 loss)
I0524 06:17:47.855423 11835 sgd_solver.cpp:112] Iteration 42940, lr = 0.1
I0524 06:17:55.187515 11835 solver.cpp:239] Iteration 42950 (1.36391 iter/s, 7.33186s/10 iters), loss = 6.43334
I0524 06:17:55.187569 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43334 (* 1 = 6.43334 loss)
I0524 06:17:55.187624 11835 sgd_solver.cpp:112] Iteration 42950, lr = 0.1
I0524 06:18:02.354503 11835 solver.cpp:239] Iteration 42960 (1.39535 iter/s, 7.16665s/10 iters), loss = 6.84773
I0524 06:18:02.354573 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84773 (* 1 = 6.84773 loss)
I0524 06:18:02.354965 11835 sgd_solver.cpp:112] Iteration 42960, lr = 0.1
I0524 06:18:07.995733 11835 solver.cpp:239] Iteration 42970 (1.77275 iter/s, 5.64095s/10 iters), loss = 6.36571
I0524 06:18:07.995785 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36571 (* 1 = 6.36571 loss)
I0524 06:18:08.378525 11835 sgd_solver.cpp:112] Iteration 42970, lr = 0.1
I0524 06:18:14.396764 11835 solver.cpp:239] Iteration 42980 (1.56233 iter/s, 6.40071s/10 iters), loss = 6.15821
I0524 06:18:14.396826 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15821 (* 1 = 6.15821 loss)
I0524 06:18:14.489851 11835 sgd_solver.cpp:112] Iteration 42980, lr = 0.1
I0524 06:18:21.922194 11835 solver.cpp:239] Iteration 42990 (1.32889 iter/s, 7.52507s/10 iters), loss = 6.55047
I0524 06:18:21.922340 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55047 (* 1 = 6.55047 loss)
I0524 06:18:21.922358 11835 sgd_solver.cpp:112] Iteration 42990, lr = 0.1
I0524 06:18:28.031026 11835 solver.cpp:239] Iteration 43000 (1.63738 iter/s, 6.10732s/10 iters), loss = 6.07479
I0524 06:18:28.031098 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07479 (* 1 = 6.07479 loss)
I0524 06:18:28.031271 11835 sgd_solver.cpp:112] Iteration 43000, lr = 0.1
I0524 06:18:34.269876 11835 solver.cpp:239] Iteration 43010 (1.60294 iter/s, 6.23854s/10 iters), loss = 5.74122
I0524 06:18:34.269932 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74122 (* 1 = 5.74122 loss)
I0524 06:18:34.274596 11835 sgd_solver.cpp:112] Iteration 43010, lr = 0.1
I0524 06:18:40.574329 11835 solver.cpp:239] Iteration 43020 (1.58625 iter/s, 6.30416s/10 iters), loss = 6.71907
I0524 06:18:40.574369 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71907 (* 1 = 6.71907 loss)
I0524 06:18:40.679240 11835 sgd_solver.cpp:112] Iteration 43020, lr = 0.1
I0524 06:18:48.384770 11835 solver.cpp:239] Iteration 43030 (1.2804 iter/s, 7.81008s/10 iters), loss = 6.17873
I0524 06:18:48.384832 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17873 (* 1 = 6.17873 loss)
I0524 06:18:48.385015 11835 sgd_solver.cpp:112] Iteration 43030, lr = 0.1
I0524 06:18:55.677740 11835 solver.cpp:239] Iteration 43040 (1.37125 iter/s, 7.29263s/10 iters), loss = 6.74592
I0524 06:18:55.677917 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74592 (* 1 = 6.74592 loss)
I0524 06:18:55.677937 11835 sgd_solver.cpp:112] Iteration 43040, lr = 0.1
I0524 06:19:02.242717 11835 solver.cpp:239] Iteration 43050 (1.52382 iter/s, 6.56245s/10 iters), loss = 6.64003
I0524 06:19:02.242765 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64003 (* 1 = 6.64003 loss)
I0524 06:19:02.741616 11835 sgd_solver.cpp:112] Iteration 43050, lr = 0.1
I0524 06:19:12.577437 11835 solver.cpp:239] Iteration 43060 (0.967654 iter/s, 10.3343s/10 iters), loss = 5.02498
I0524 06:19:12.577493 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.02498 (* 1 = 5.02498 loss)
I0524 06:19:12.577579 11835 sgd_solver.cpp:112] Iteration 43060, lr = 0.1
I0524 06:19:18.979779 11835 solver.cpp:239] Iteration 43070 (1.56201 iter/s, 6.40203s/10 iters), loss = 6.53008
I0524 06:19:18.979838 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53008 (* 1 = 6.53008 loss)
I0524 06:19:18.979854 11835 sgd_solver.cpp:112] Iteration 43070, lr = 0.1
I0524 06:19:27.652519 11835 solver.cpp:239] Iteration 43080 (1.15309 iter/s, 8.67236s/10 iters), loss = 5.13645
I0524 06:19:27.652818 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.13645 (* 1 = 5.13645 loss)
I0524 06:19:27.652853 11835 sgd_solver.cpp:112] Iteration 43080, lr = 0.1
I0524 06:19:33.904212 11835 solver.cpp:239] Iteration 43090 (1.60026 iter/s, 6.249s/10 iters), loss = 5.65947
I0524 06:19:33.904263 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.65947 (* 1 = 5.65947 loss)
I0524 06:19:33.964910 11835 sgd_solver.cpp:112] Iteration 43090, lr = 0.1
I0524 06:19:40.687057 11835 solver.cpp:239] Iteration 43100 (1.47438 iter/s, 6.78252s/10 iters), loss = 6.11788
I0524 06:19:40.687106 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11788 (* 1 = 6.11788 loss)
I0524 06:19:41.417001 11835 sgd_solver.cpp:112] Iteration 43100, lr = 0.1
I0524 06:19:50.256989 11835 solver.cpp:239] Iteration 43110 (1.04499 iter/s, 9.56949s/10 iters), loss = 7.2681
I0524 06:19:50.257045 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2681 (* 1 = 7.2681 loss)
I0524 06:19:51.038957 11835 sgd_solver.cpp:112] Iteration 43110, lr = 0.1
I0524 06:19:56.873320 11835 solver.cpp:239] Iteration 43120 (1.51148 iter/s, 6.61603s/10 iters), loss = 6.95097
I0524 06:19:56.873365 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.95097 (* 1 = 6.95097 loss)
I0524 06:19:56.873380 11835 sgd_solver.cpp:112] Iteration 43120, lr = 0.1
I0524 06:20:02.906476 11835 solver.cpp:239] Iteration 43130 (1.65761 iter/s, 6.03279s/10 iters), loss = 5.80744
I0524 06:20:02.906683 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80744 (* 1 = 5.80744 loss)
I0524 06:20:02.906749 11835 sgd_solver.cpp:112] Iteration 43130, lr = 0.1
I0524 06:20:10.442978 11835 solver.cpp:239] Iteration 43140 (1.32697 iter/s, 7.53596s/10 iters), loss = 5.51931
I0524 06:20:10.443038 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.51931 (* 1 = 5.51931 loss)
I0524 06:20:10.530843 11835 sgd_solver.cpp:112] Iteration 43140, lr = 0.1
I0524 06:20:20.185999 11835 solver.cpp:239] Iteration 43150 (1.02642 iter/s, 9.74258s/10 iters), loss = 6.08838
I0524 06:20:20.186053 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08838 (* 1 = 6.08838 loss)
I0524 06:20:20.266258 11835 sgd_solver.cpp:112] Iteration 43150, lr = 0.1
I0524 06:20:28.347290 11835 solver.cpp:239] Iteration 43160 (1.22535 iter/s, 8.16091s/10 iters), loss = 5.92294
I0524 06:20:28.347373 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92294 (* 1 = 5.92294 loss)
I0524 06:20:28.347631 11835 sgd_solver.cpp:112] Iteration 43160, lr = 0.1
I0524 06:20:34.810220 11835 solver.cpp:239] Iteration 43170 (1.54736 iter/s, 6.46261s/10 iters), loss = 6.5856
I0524 06:20:34.810497 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5856 (* 1 = 6.5856 loss)
I0524 06:20:34.810554 11835 sgd_solver.cpp:112] Iteration 43170, lr = 0.1
I0524 06:20:40.841826 11835 solver.cpp:239] Iteration 43180 (1.65819 iter/s, 6.03068s/10 iters), loss = 5.75622
I0524 06:20:40.841878 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75622 (* 1 = 5.75622 loss)
I0524 06:20:40.899222 11835 sgd_solver.cpp:112] Iteration 43180, lr = 0.1
I0524 06:20:48.684252 11835 solver.cpp:239] Iteration 43190 (1.27518 iter/s, 7.84205s/10 iters), loss = 5.09806
I0524 06:20:48.684386 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.09806 (* 1 = 5.09806 loss)
I0524 06:20:48.684525 11835 sgd_solver.cpp:112] Iteration 43190, lr = 0.1
I0524 06:20:56.197168 11835 solver.cpp:239] Iteration 43200 (1.3311 iter/s, 7.51257s/10 iters), loss = 7.34488
I0524 06:20:56.197227 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.34488 (* 1 = 7.34488 loss)
I0524 06:20:56.197397 11835 sgd_solver.cpp:112] Iteration 43200, lr = 0.1
I0524 06:21:03.683785 11835 solver.cpp:239] Iteration 43210 (1.33578 iter/s, 7.48627s/10 iters), loss = 6.53163
I0524 06:21:03.683850 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53163 (* 1 = 6.53163 loss)
I0524 06:21:04.568778 11835 sgd_solver.cpp:112] Iteration 43210, lr = 0.1
I0524 06:21:11.621919 11835 solver.cpp:239] Iteration 43220 (1.2598 iter/s, 7.93777s/10 iters), loss = 5.10314
I0524 06:21:11.622220 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.10314 (* 1 = 5.10314 loss)
I0524 06:21:12.141641 11835 sgd_solver.cpp:112] Iteration 43220, lr = 0.1
I0524 06:21:18.192927 11835 solver.cpp:239] Iteration 43230 (1.52194 iter/s, 6.57056s/10 iters), loss = 6.15467
I0524 06:21:18.192976 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15467 (* 1 = 6.15467 loss)
I0524 06:21:18.192989 11835 sgd_solver.cpp:112] Iteration 43230, lr = 0.1
I0524 06:21:24.218304 11835 solver.cpp:239] Iteration 43240 (1.65973 iter/s, 6.02509s/10 iters), loss = 5.70538
I0524 06:21:24.218375 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.70538 (* 1 = 5.70538 loss)
I0524 06:21:24.218459 11835 sgd_solver.cpp:112] Iteration 43240, lr = 0.1
I0524 06:21:31.503929 11835 solver.cpp:239] Iteration 43250 (1.37263 iter/s, 7.28528s/10 iters), loss = 3.96175
I0524 06:21:31.504052 11835 solver.cpp:258]     Train net output #0: softmax_loss = 3.96175 (* 1 = 3.96175 loss)
I0524 06:21:31.504148 11835 sgd_solver.cpp:112] Iteration 43250, lr = 0.1
I0524 06:21:39.565603 11835 solver.cpp:239] Iteration 43260 (1.24049 iter/s, 8.0613s/10 iters), loss = 5.9095
I0524 06:21:39.565668 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9095 (* 1 = 5.9095 loss)
I0524 06:21:39.565861 11835 sgd_solver.cpp:112] Iteration 43260, lr = 0.1
I0524 06:21:47.104866 11835 solver.cpp:239] Iteration 43270 (1.32645 iter/s, 7.5389s/10 iters), loss = 5.43056
I0524 06:21:47.105106 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43056 (* 1 = 5.43056 loss)
I0524 06:21:47.105165 11835 sgd_solver.cpp:112] Iteration 43270, lr = 0.1
I0524 06:21:53.780386 11835 solver.cpp:239] Iteration 43280 (1.49858 iter/s, 6.67298s/10 iters), loss = 5.09674
I0524 06:21:53.780457 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.09674 (* 1 = 5.09674 loss)
I0524 06:21:53.780484 11835 sgd_solver.cpp:112] Iteration 43280, lr = 0.1
I0524 06:22:02.352433 11835 solver.cpp:239] Iteration 43290 (1.16664 iter/s, 8.57165s/10 iters), loss = 6.61302
I0524 06:22:02.352504 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61302 (* 1 = 6.61302 loss)
I0524 06:22:02.352639 11835 sgd_solver.cpp:112] Iteration 43290, lr = 0.1
I0524 06:22:08.228829 11835 solver.cpp:239] Iteration 43300 (1.70181 iter/s, 5.87611s/10 iters), loss = 5.96196
I0524 06:22:08.228890 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96196 (* 1 = 5.96196 loss)
I0524 06:22:08.247385 11835 sgd_solver.cpp:112] Iteration 43300, lr = 0.1
I0524 06:22:16.223873 11835 solver.cpp:239] Iteration 43310 (1.25083 iter/s, 7.99467s/10 iters), loss = 7.86954
I0524 06:22:16.223929 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.86954 (* 1 = 7.86954 loss)
I0524 06:22:16.224196 11835 sgd_solver.cpp:112] Iteration 43310, lr = 0.1
I0524 06:22:22.484772 11835 solver.cpp:239] Iteration 43320 (1.59729 iter/s, 6.2606s/10 iters), loss = 6.62867
I0524 06:22:22.485007 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62867 (* 1 = 6.62867 loss)
I0524 06:22:22.485050 11835 sgd_solver.cpp:112] Iteration 43320, lr = 0.1
I0524 06:22:33.564093 11835 solver.cpp:239] Iteration 43330 (0.902803 iter/s, 11.0766s/10 iters), loss = 7.07502
I0524 06:22:33.564146 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.07502 (* 1 = 7.07502 loss)
I0524 06:22:33.577188 11835 sgd_solver.cpp:112] Iteration 43330, lr = 0.1
I0524 06:22:41.651990 11835 solver.cpp:239] Iteration 43340 (1.23647 iter/s, 8.08753s/10 iters), loss = 6.6077
I0524 06:22:41.652050 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6077 (* 1 = 6.6077 loss)
I0524 06:22:41.652107 11835 sgd_solver.cpp:112] Iteration 43340, lr = 0.1
I0524 06:22:49.961844 11835 solver.cpp:239] Iteration 43350 (1.20345 iter/s, 8.30948s/10 iters), loss = 5.95641
I0524 06:22:49.961896 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95641 (* 1 = 5.95641 loss)
I0524 06:22:49.961992 11835 sgd_solver.cpp:112] Iteration 43350, lr = 0.1
I0524 06:22:56.379969 11835 solver.cpp:239] Iteration 43360 (1.55816 iter/s, 6.41782s/10 iters), loss = 6.29145
I0524 06:22:56.380192 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29145 (* 1 = 6.29145 loss)
I0524 06:22:56.380218 11835 sgd_solver.cpp:112] Iteration 43360, lr = 0.1
I0524 06:23:03.085682 11835 solver.cpp:239] Iteration 43370 (1.49137 iter/s, 6.70523s/10 iters), loss = 5.76874
I0524 06:23:03.085737 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.76874 (* 1 = 5.76874 loss)
I0524 06:23:03.085999 11835 sgd_solver.cpp:112] Iteration 43370, lr = 0.1
I0524 06:23:10.223939 11835 solver.cpp:239] Iteration 43380 (1.40097 iter/s, 7.13792s/10 iters), loss = 7.52852
I0524 06:23:10.223997 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.52852 (* 1 = 7.52852 loss)
I0524 06:23:10.224133 11835 sgd_solver.cpp:112] Iteration 43380, lr = 0.1
I0524 06:23:16.814625 11835 solver.cpp:239] Iteration 43390 (1.51737 iter/s, 6.59037s/10 iters), loss = 6.56285
I0524 06:23:16.814685 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56285 (* 1 = 6.56285 loss)
I0524 06:23:16.814779 11835 sgd_solver.cpp:112] Iteration 43390, lr = 0.1
I0524 06:23:22.870045 11835 solver.cpp:239] Iteration 43400 (1.65149 iter/s, 6.05513s/10 iters), loss = 6.38802
I0524 06:23:22.870090 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38802 (* 1 = 6.38802 loss)
I0524 06:23:22.870318 11835 sgd_solver.cpp:112] Iteration 43400, lr = 0.1
I0524 06:23:29.325747 11835 solver.cpp:239] Iteration 43410 (1.54909 iter/s, 6.45539s/10 iters), loss = 6.66481
I0524 06:23:29.326050 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66481 (* 1 = 6.66481 loss)
I0524 06:23:29.326097 11835 sgd_solver.cpp:112] Iteration 43410, lr = 0.1
I0524 06:23:35.679417 11835 solver.cpp:239] Iteration 43420 (1.57406 iter/s, 6.35299s/10 iters), loss = 6.27227
I0524 06:23:35.679464 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27227 (* 1 = 6.27227 loss)
I0524 06:23:35.852802 11835 sgd_solver.cpp:112] Iteration 43420, lr = 0.1
I0524 06:23:42.109773 11835 solver.cpp:239] Iteration 43430 (1.5552 iter/s, 6.43004s/10 iters), loss = 6.75255
I0524 06:23:42.109832 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.75255 (* 1 = 6.75255 loss)
I0524 06:23:42.109848 11835 sgd_solver.cpp:112] Iteration 43430, lr = 0.1
I0524 06:23:48.171671 11835 solver.cpp:239] Iteration 43440 (1.64973 iter/s, 6.06161s/10 iters), loss = 7.60471
I0524 06:23:48.171723 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.60471 (* 1 = 7.60471 loss)
I0524 06:23:48.171813 11835 sgd_solver.cpp:112] Iteration 43440, lr = 0.1
I0524 06:23:55.074457 11835 solver.cpp:239] Iteration 43450 (1.44876 iter/s, 6.90248s/10 iters), loss = 6.39899
I0524 06:23:55.074498 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39899 (* 1 = 6.39899 loss)
I0524 06:23:55.323607 11835 sgd_solver.cpp:112] Iteration 43450, lr = 0.1
I0524 06:24:03.184273 11835 solver.cpp:239] Iteration 43460 (1.23313 iter/s, 8.10946s/10 iters), loss = 7.18358
I0524 06:24:03.184408 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.18358 (* 1 = 7.18358 loss)
I0524 06:24:03.849649 11835 sgd_solver.cpp:112] Iteration 43460, lr = 0.1
I0524 06:24:12.102090 11835 solver.cpp:239] Iteration 43470 (1.12141 iter/s, 8.91734s/10 iters), loss = 5.85132
I0524 06:24:12.102141 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85132 (* 1 = 5.85132 loss)
I0524 06:24:13.218680 11835 sgd_solver.cpp:112] Iteration 43470, lr = 0.1
I0524 06:24:19.610846 11835 solver.cpp:239] Iteration 43480 (1.33184 iter/s, 7.5084s/10 iters), loss = 5.90821
I0524 06:24:19.610931 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90821 (* 1 = 5.90821 loss)
I0524 06:24:19.610960 11835 sgd_solver.cpp:112] Iteration 43480, lr = 0.1
I0524 06:24:26.412756 11835 solver.cpp:239] Iteration 43490 (1.47072 iter/s, 6.79938s/10 iters), loss = 6.70644
I0524 06:24:26.412804 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70644 (* 1 = 6.70644 loss)
I0524 06:24:26.413017 11835 sgd_solver.cpp:112] Iteration 43490, lr = 0.1
I0524 06:24:32.707093 11835 solver.cpp:239] Iteration 43500 (1.58881 iter/s, 6.29404s/10 iters), loss = 5.38941
I0524 06:24:32.707165 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.38941 (* 1 = 5.38941 loss)
I0524 06:24:32.707190 11835 sgd_solver.cpp:112] Iteration 43500, lr = 0.1
I0524 06:24:38.784442 11835 solver.cpp:239] Iteration 43510 (1.64554 iter/s, 6.07702s/10 iters), loss = 5.69385
I0524 06:24:38.785024 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.69385 (* 1 = 5.69385 loss)
I0524 06:24:38.785058 11835 sgd_solver.cpp:112] Iteration 43510, lr = 0.1
I0524 06:24:44.896416 11835 solver.cpp:239] Iteration 43520 (1.63647 iter/s, 6.11071s/10 iters), loss = 6.48716
I0524 06:24:44.896487 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48716 (* 1 = 6.48716 loss)
I0524 06:24:44.896548 11835 sgd_solver.cpp:112] Iteration 43520, lr = 0.1
I0524 06:24:51.222134 11835 solver.cpp:239] Iteration 43530 (1.58092 iter/s, 6.32542s/10 iters), loss = 5.60942
I0524 06:24:51.222187 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.60942 (* 1 = 5.60942 loss)
I0524 06:24:51.239002 11835 sgd_solver.cpp:112] Iteration 43530, lr = 0.1
I0524 06:24:59.769631 11835 solver.cpp:239] Iteration 43540 (1.16998 iter/s, 8.54713s/10 iters), loss = 6.4703
I0524 06:24:59.769680 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4703 (* 1 = 6.4703 loss)
I0524 06:25:00.641579 11835 sgd_solver.cpp:112] Iteration 43540, lr = 0.1
I0524 06:25:07.769330 11835 solver.cpp:239] Iteration 43550 (1.2501 iter/s, 7.99934s/10 iters), loss = 6.42906
I0524 06:25:07.769379 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42906 (* 1 = 6.42906 loss)
I0524 06:25:07.769505 11835 sgd_solver.cpp:112] Iteration 43550, lr = 0.1
I0524 06:25:15.905714 11835 solver.cpp:239] Iteration 43560 (1.2291 iter/s, 8.13601s/10 iters), loss = 6.39807
I0524 06:25:15.905943 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39807 (* 1 = 6.39807 loss)
I0524 06:25:15.906039 11835 sgd_solver.cpp:112] Iteration 43560, lr = 0.1
I0524 06:25:23.159376 11835 solver.cpp:239] Iteration 43570 (1.3787 iter/s, 7.25319s/10 iters), loss = 6.47463
I0524 06:25:23.159433 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.47463 (* 1 = 6.47463 loss)
I0524 06:25:23.159523 11835 sgd_solver.cpp:112] Iteration 43570, lr = 0.1
I0524 06:25:30.026409 11835 solver.cpp:239] Iteration 43580 (1.4563 iter/s, 6.86672s/10 iters), loss = 6.43183
I0524 06:25:30.026445 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43183 (* 1 = 6.43183 loss)
I0524 06:25:30.026561 11835 sgd_solver.cpp:112] Iteration 43580, lr = 0.1
I0524 06:25:36.082636 11835 solver.cpp:239] Iteration 43590 (1.65127 iter/s, 6.05595s/10 iters), loss = 6.36269
I0524 06:25:36.082684 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36269 (* 1 = 6.36269 loss)
I0524 06:25:36.491168 11835 sgd_solver.cpp:112] Iteration 43590, lr = 0.1
I0524 06:25:42.830230 11835 solver.cpp:239] Iteration 43600 (1.48208 iter/s, 6.74729s/10 iters), loss = 6.30455
I0524 06:25:42.830286 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30455 (* 1 = 6.30455 loss)
I0524 06:25:43.031724 11835 sgd_solver.cpp:112] Iteration 43600, lr = 0.1
I0524 06:25:49.426633 11835 solver.cpp:239] Iteration 43610 (1.51605 iter/s, 6.59609s/10 iters), loss = 5.31677
I0524 06:25:49.426833 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.31677 (* 1 = 5.31677 loss)
I0524 06:25:49.444021 11835 sgd_solver.cpp:112] Iteration 43610, lr = 0.1
I0524 06:25:56.348490 11835 solver.cpp:239] Iteration 43620 (1.44479 iter/s, 6.92141s/10 iters), loss = 5.88085
I0524 06:25:56.348543 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88085 (* 1 = 5.88085 loss)
I0524 06:25:56.348558 11835 sgd_solver.cpp:112] Iteration 43620, lr = 0.1
I0524 06:26:02.564460 11835 solver.cpp:239] Iteration 43630 (1.60883 iter/s, 6.21569s/10 iters), loss = 6.50903
I0524 06:26:02.564507 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50903 (* 1 = 6.50903 loss)
I0524 06:26:02.564520 11835 sgd_solver.cpp:112] Iteration 43630, lr = 0.1
I0524 06:26:09.452970 11835 solver.cpp:239] Iteration 43640 (1.45176 iter/s, 6.88818s/10 iters), loss = 5.99104
I0524 06:26:09.453047 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99104 (* 1 = 5.99104 loss)
I0524 06:26:09.501996 11835 sgd_solver.cpp:112] Iteration 43640, lr = 0.1
I0524 06:26:17.259511 11835 solver.cpp:239] Iteration 43650 (1.28104 iter/s, 7.80616s/10 iters), loss = 6.09028
I0524 06:26:17.259568 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09028 (* 1 = 6.09028 loss)
I0524 06:26:17.259816 11835 sgd_solver.cpp:112] Iteration 43650, lr = 0.1
I0524 06:26:23.363065 11835 solver.cpp:239] Iteration 43660 (1.63847 iter/s, 6.10327s/10 iters), loss = 6.85238
I0524 06:26:23.363215 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.85238 (* 1 = 6.85238 loss)
I0524 06:26:23.363230 11835 sgd_solver.cpp:112] Iteration 43660, lr = 0.1
I0524 06:26:31.577049 11835 solver.cpp:239] Iteration 43670 (1.21751 iter/s, 8.21347s/10 iters), loss = 5.49596
I0524 06:26:31.577101 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.49596 (* 1 = 5.49596 loss)
I0524 06:26:31.649979 11835 sgd_solver.cpp:112] Iteration 43670, lr = 0.1
I0524 06:26:38.036403 11835 solver.cpp:239] Iteration 43680 (1.54822 iter/s, 6.45905s/10 iters), loss = 5.93737
I0524 06:26:38.036473 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93737 (* 1 = 5.93737 loss)
I0524 06:26:38.036677 11835 sgd_solver.cpp:112] Iteration 43680, lr = 0.1
I0524 06:26:44.888578 11835 solver.cpp:239] Iteration 43690 (1.45946 iter/s, 6.85187s/10 iters), loss = 6.02777
I0524 06:26:44.888622 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02777 (* 1 = 6.02777 loss)
I0524 06:26:44.888633 11835 sgd_solver.cpp:112] Iteration 43690, lr = 0.1
I0524 06:26:51.519852 11835 solver.cpp:239] Iteration 43700 (1.50811 iter/s, 6.63082s/10 iters), loss = 5.92248
I0524 06:26:51.519901 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92248 (* 1 = 5.92248 loss)
I0524 06:26:51.520028 11835 sgd_solver.cpp:112] Iteration 43700, lr = 0.1
I0524 06:26:57.756381 11835 solver.cpp:239] Iteration 43710 (1.60353 iter/s, 6.23624s/10 iters), loss = 5.96889
I0524 06:26:57.756593 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96889 (* 1 = 5.96889 loss)
I0524 06:26:57.756636 11835 sgd_solver.cpp:112] Iteration 43710, lr = 0.1
I0524 06:27:05.677853 11835 solver.cpp:239] Iteration 43720 (1.26247 iter/s, 7.92096s/10 iters), loss = 5.96765
I0524 06:27:05.677909 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96765 (* 1 = 5.96765 loss)
I0524 06:27:05.678066 11835 sgd_solver.cpp:112] Iteration 43720, lr = 0.1
I0524 06:27:16.923218 11835 solver.cpp:239] Iteration 43730 (0.889294 iter/s, 11.2449s/10 iters), loss = 5.27458
I0524 06:27:16.923283 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.27458 (* 1 = 5.27458 loss)
I0524 06:27:17.360714 11835 sgd_solver.cpp:112] Iteration 43730, lr = 0.1
I0524 06:27:23.213439 11835 solver.cpp:239] Iteration 43740 (1.58984 iter/s, 6.28993s/10 iters), loss = 5.95489
I0524 06:27:23.213492 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95489 (* 1 = 5.95489 loss)
I0524 06:27:23.213858 11835 sgd_solver.cpp:112] Iteration 43740, lr = 0.1
I0524 06:27:31.537245 11835 solver.cpp:239] Iteration 43750 (1.20143 iter/s, 8.32343s/10 iters), loss = 7.03017
I0524 06:27:31.537478 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03017 (* 1 = 7.03017 loss)
I0524 06:27:31.537521 11835 sgd_solver.cpp:112] Iteration 43750, lr = 0.1
I0524 06:27:37.503497 11835 solver.cpp:239] Iteration 43760 (1.6768 iter/s, 5.96375s/10 iters), loss = 6.02768
I0524 06:27:37.503554 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02768 (* 1 = 6.02768 loss)
I0524 06:27:37.503911 11835 sgd_solver.cpp:112] Iteration 43760, lr = 0.1
I0524 06:27:43.150250 11835 solver.cpp:239] Iteration 43770 (1.77101 iter/s, 5.64648s/10 iters), loss = 5.39654
I0524 06:27:43.150291 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.39654 (* 1 = 5.39654 loss)
I0524 06:27:43.150862 11835 sgd_solver.cpp:112] Iteration 43770, lr = 0.1
I0524 06:27:50.693159 11835 solver.cpp:239] Iteration 43780 (1.32581 iter/s, 7.54257s/10 iters), loss = 6.65677
I0524 06:27:50.693212 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65677 (* 1 = 6.65677 loss)
I0524 06:27:50.693310 11835 sgd_solver.cpp:112] Iteration 43780, lr = 0.1
I0524 06:27:56.682763 11835 solver.cpp:239] Iteration 43790 (1.66964 iter/s, 5.98932s/10 iters), loss = 6.70836
I0524 06:27:56.682822 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70836 (* 1 = 6.70836 loss)
I0524 06:27:56.738000 11835 sgd_solver.cpp:112] Iteration 43790, lr = 0.1
I0524 06:28:03.207142 11835 solver.cpp:239] Iteration 43800 (1.53279 iter/s, 6.52406s/10 iters), loss = 5.90091
I0524 06:28:03.207391 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90091 (* 1 = 5.90091 loss)
I0524 06:28:03.207440 11835 sgd_solver.cpp:112] Iteration 43800, lr = 0.1
I0524 06:28:09.236064 11835 solver.cpp:239] Iteration 43810 (1.6588 iter/s, 6.02846s/10 iters), loss = 6.49001
I0524 06:28:09.236125 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49001 (* 1 = 6.49001 loss)
I0524 06:28:09.236347 11835 sgd_solver.cpp:112] Iteration 43810, lr = 0.1
I0524 06:28:15.649089 11835 solver.cpp:239] Iteration 43820 (1.5594 iter/s, 6.41273s/10 iters), loss = 6.46741
I0524 06:28:15.649122 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46741 (* 1 = 6.46741 loss)
I0524 06:28:15.649139 11835 sgd_solver.cpp:112] Iteration 43820, lr = 0.1
I0524 06:28:24.362815 11835 solver.cpp:239] Iteration 43830 (1.14766 iter/s, 8.71335s/10 iters), loss = 6.42174
I0524 06:28:24.362864 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42174 (* 1 = 6.42174 loss)
I0524 06:28:25.035161 11835 sgd_solver.cpp:112] Iteration 43830, lr = 0.1
I0524 06:28:34.398943 11835 solver.cpp:239] Iteration 43840 (0.996444 iter/s, 10.0357s/10 iters), loss = 6.04409
I0524 06:28:34.399180 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04409 (* 1 = 6.04409 loss)
I0524 06:28:34.399246 11835 sgd_solver.cpp:112] Iteration 43840, lr = 0.1
I0524 06:28:41.796654 11835 solver.cpp:239] Iteration 43850 (1.35191 iter/s, 7.39696s/10 iters), loss = 6.72389
I0524 06:28:41.796705 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72389 (* 1 = 6.72389 loss)
I0524 06:28:42.622573 11835 sgd_solver.cpp:112] Iteration 43850, lr = 0.1
I0524 06:28:49.046003 11835 solver.cpp:239] Iteration 43860 (1.3795 iter/s, 7.24902s/10 iters), loss = 6.70501
I0524 06:28:49.046059 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70501 (* 1 = 6.70501 loss)
I0524 06:28:49.046072 11835 sgd_solver.cpp:112] Iteration 43860, lr = 0.1
I0524 06:28:55.978158 11835 solver.cpp:239] Iteration 43870 (1.44264 iter/s, 6.93174s/10 iters), loss = 5.451
I0524 06:28:55.978206 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.451 (* 1 = 5.451 loss)
I0524 06:28:55.982533 11835 sgd_solver.cpp:112] Iteration 43870, lr = 0.1
I0524 06:29:02.001246 11835 solver.cpp:239] Iteration 43880 (1.66035 iter/s, 6.02281s/10 iters), loss = 6.64875
I0524 06:29:02.001292 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64875 (* 1 = 6.64875 loss)
I0524 06:29:02.001307 11835 sgd_solver.cpp:112] Iteration 43880, lr = 0.1
I0524 06:29:07.599550 11835 solver.cpp:239] Iteration 43890 (1.78634 iter/s, 5.59803s/10 iters), loss = 6.27078
I0524 06:29:07.599745 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27078 (* 1 = 6.27078 loss)
I0524 06:29:07.599776 11835 sgd_solver.cpp:112] Iteration 43890, lr = 0.1
I0524 06:29:13.334481 11835 solver.cpp:239] Iteration 43900 (1.74384 iter/s, 5.73447s/10 iters), loss = 5.2255
I0524 06:29:13.334522 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.2255 (* 1 = 5.2255 loss)
I0524 06:29:13.501344 11835 sgd_solver.cpp:112] Iteration 43900, lr = 0.1
I0524 06:29:22.245379 11835 solver.cpp:239] Iteration 43910 (1.12227 iter/s, 8.91051s/10 iters), loss = 6.66285
I0524 06:29:22.245440 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66285 (* 1 = 6.66285 loss)
I0524 06:29:22.245514 11835 sgd_solver.cpp:112] Iteration 43910, lr = 0.1
I0524 06:29:28.699571 11835 solver.cpp:239] Iteration 43920 (1.54946 iter/s, 6.45388s/10 iters), loss = 6.46495
I0524 06:29:28.699630 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46495 (* 1 = 6.46495 loss)
I0524 06:29:28.770143 11835 sgd_solver.cpp:112] Iteration 43920, lr = 0.1
I0524 06:29:34.961136 11835 solver.cpp:239] Iteration 43930 (1.59712 iter/s, 6.26126s/10 iters), loss = 6.18986
I0524 06:29:34.961195 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18986 (* 1 = 6.18986 loss)
I0524 06:29:35.242035 11835 sgd_solver.cpp:112] Iteration 43930, lr = 0.1
I0524 06:29:43.285769 11835 solver.cpp:239] Iteration 43940 (1.20131 iter/s, 8.32425s/10 iters), loss = 7.18373
I0524 06:29:43.285933 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.18373 (* 1 = 7.18373 loss)
I0524 06:29:43.286429 11835 sgd_solver.cpp:112] Iteration 43940, lr = 0.1
I0524 06:29:49.872066 11835 solver.cpp:239] Iteration 43950 (1.5184 iter/s, 6.58588s/10 iters), loss = 6.32883
I0524 06:29:49.872115 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32883 (* 1 = 6.32883 loss)
I0524 06:29:51.160245 11835 sgd_solver.cpp:112] Iteration 43950, lr = 0.1
I0524 06:29:59.330991 11835 solver.cpp:239] Iteration 43960 (1.05725 iter/s, 9.45848s/10 iters), loss = 6.44708
I0524 06:29:59.331064 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44708 (* 1 = 6.44708 loss)
I0524 06:29:59.331084 11835 sgd_solver.cpp:112] Iteration 43960, lr = 0.1
I0524 06:30:06.659535 11835 solver.cpp:239] Iteration 43970 (1.36459 iter/s, 7.32819s/10 iters), loss = 6.64244
I0524 06:30:06.659590 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64244 (* 1 = 6.64244 loss)
I0524 06:30:06.659822 11835 sgd_solver.cpp:112] Iteration 43970, lr = 0.1
I0524 06:30:13.977178 11835 solver.cpp:239] Iteration 43980 (1.36662 iter/s, 7.3173s/10 iters), loss = 6.65107
I0524 06:30:13.977414 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65107 (* 1 = 6.65107 loss)
I0524 06:30:13.977454 11835 sgd_solver.cpp:112] Iteration 43980, lr = 0.1
I0524 06:30:21.043751 11835 solver.cpp:239] Iteration 43990 (1.41521 iter/s, 7.06606s/10 iters), loss = 7.53929
I0524 06:30:21.043802 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.53929 (* 1 = 7.53929 loss)
I0524 06:30:21.043895 11835 sgd_solver.cpp:112] Iteration 43990, lr = 0.1
I0524 06:30:27.143508 11835 solver.cpp:239] Iteration 44000 (1.63949 iter/s, 6.09948s/10 iters), loss = 5.69599
I0524 06:30:27.143548 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.69599 (* 1 = 5.69599 loss)
I0524 06:30:27.717995 11835 sgd_solver.cpp:112] Iteration 44000, lr = 0.1
I0524 06:30:33.627208 11835 solver.cpp:239] Iteration 44010 (1.5424 iter/s, 6.4834s/10 iters), loss = 6.32974
I0524 06:30:33.627260 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32974 (* 1 = 6.32974 loss)
I0524 06:30:33.627275 11835 sgd_solver.cpp:112] Iteration 44010, lr = 0.1
I0524 06:30:41.209903 11835 solver.cpp:239] Iteration 44020 (1.31886 iter/s, 7.58229s/10 iters), loss = 6.48678
I0524 06:30:41.209956 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48678 (* 1 = 6.48678 loss)
I0524 06:30:41.397524 11835 sgd_solver.cpp:112] Iteration 44020, lr = 0.1
I0524 06:30:47.931439 11835 solver.cpp:239] Iteration 44030 (1.48783 iter/s, 6.72121s/10 iters), loss = 6.07113
I0524 06:30:47.931613 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07113 (* 1 = 6.07113 loss)
I0524 06:30:48.005007 11835 sgd_solver.cpp:112] Iteration 44030, lr = 0.1
I0524 06:30:54.928758 11835 solver.cpp:239] Iteration 44040 (1.4292 iter/s, 6.9969s/10 iters), loss = 5.09855
I0524 06:30:54.928802 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.09855 (* 1 = 5.09855 loss)
I0524 06:30:54.928915 11835 sgd_solver.cpp:112] Iteration 44040, lr = 0.1
I0524 06:31:02.727967 11835 solver.cpp:239] Iteration 44050 (1.28224 iter/s, 7.79885s/10 iters), loss = 5.50715
I0524 06:31:02.728024 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.50715 (* 1 = 5.50715 loss)
I0524 06:31:02.728039 11835 sgd_solver.cpp:112] Iteration 44050, lr = 0.1
I0524 06:31:11.883791 11835 solver.cpp:239] Iteration 44060 (1.09228 iter/s, 9.15518s/10 iters), loss = 6.85212
I0524 06:31:11.883846 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.85212 (* 1 = 6.85212 loss)
I0524 06:31:11.958859 11835 sgd_solver.cpp:112] Iteration 44060, lr = 0.1
I0524 06:31:18.256741 11835 solver.cpp:239] Iteration 44070 (1.56921 iter/s, 6.37265s/10 iters), loss = 7.05646
I0524 06:31:18.257009 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.05646 (* 1 = 7.05646 loss)
I0524 06:31:18.257053 11835 sgd_solver.cpp:112] Iteration 44070, lr = 0.1
I0524 06:31:26.124891 11835 solver.cpp:239] Iteration 44080 (1.27104 iter/s, 7.86758s/10 iters), loss = 5.76903
I0524 06:31:26.125061 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.76903 (* 1 = 5.76903 loss)
I0524 06:31:26.125097 11835 sgd_solver.cpp:112] Iteration 44080, lr = 0.1
I0524 06:31:33.013356 11835 solver.cpp:239] Iteration 44090 (1.45179 iter/s, 6.88805s/10 iters), loss = 6.46453
I0524 06:31:33.013409 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46453 (* 1 = 6.46453 loss)
I0524 06:31:33.013427 11835 sgd_solver.cpp:112] Iteration 44090, lr = 0.1
I0524 06:31:41.062778 11835 solver.cpp:239] Iteration 44100 (1.24243 iter/s, 8.04875s/10 iters), loss = 6.34317
I0524 06:31:41.062834 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34317 (* 1 = 6.34317 loss)
I0524 06:31:41.062955 11835 sgd_solver.cpp:112] Iteration 44100, lr = 0.1
I0524 06:31:48.748739 11835 solver.cpp:239] Iteration 44110 (1.30113 iter/s, 7.68562s/10 iters), loss = 6.1892
I0524 06:31:48.748957 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1892 (* 1 = 6.1892 loss)
I0524 06:31:49.527333 11835 sgd_solver.cpp:112] Iteration 44110, lr = 0.1
I0524 06:31:58.190654 11835 solver.cpp:239] Iteration 44120 (1.05917 iter/s, 9.44136s/10 iters), loss = 6.60678
I0524 06:31:58.190755 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60678 (* 1 = 6.60678 loss)
I0524 06:31:58.916703 11835 sgd_solver.cpp:112] Iteration 44120, lr = 0.1
I0524 06:32:08.850075 11835 solver.cpp:239] Iteration 44130 (0.938179 iter/s, 10.6589s/10 iters), loss = 6.24256
I0524 06:32:08.850128 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24256 (* 1 = 6.24256 loss)
I0524 06:32:08.850239 11835 sgd_solver.cpp:112] Iteration 44130, lr = 0.1
I0524 06:32:14.784973 11835 solver.cpp:239] Iteration 44140 (1.68503 iter/s, 5.93463s/10 iters), loss = 5.41425
I0524 06:32:14.785009 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.41425 (* 1 = 5.41425 loss)
I0524 06:32:14.785022 11835 sgd_solver.cpp:112] Iteration 44140, lr = 0.1
I0524 06:32:20.423738 11835 solver.cpp:239] Iteration 44150 (1.77352 iter/s, 5.6385s/10 iters), loss = 5.38301
I0524 06:32:20.423847 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.38301 (* 1 = 5.38301 loss)
I0524 06:32:20.423864 11835 sgd_solver.cpp:112] Iteration 44150, lr = 0.1
I0524 06:32:26.264964 11835 solver.cpp:239] Iteration 44160 (1.71208 iter/s, 5.84086s/10 iters), loss = 5.80222
I0524 06:32:26.265048 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80222 (* 1 = 5.80222 loss)
I0524 06:32:26.265121 11835 sgd_solver.cpp:112] Iteration 44160, lr = 0.1
I0524 06:32:33.791396 11835 solver.cpp:239] Iteration 44170 (1.32871 iter/s, 7.52607s/10 iters), loss = 6.40303
I0524 06:32:33.791448 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40303 (* 1 = 6.40303 loss)
I0524 06:32:33.805706 11835 sgd_solver.cpp:112] Iteration 44170, lr = 0.1
I0524 06:32:40.128440 11835 solver.cpp:239] Iteration 44180 (1.5781 iter/s, 6.33674s/10 iters), loss = 6.0421
I0524 06:32:40.128499 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0421 (* 1 = 6.0421 loss)
I0524 06:32:40.128728 11835 sgd_solver.cpp:112] Iteration 44180, lr = 0.1
I0524 06:32:50.231581 11835 solver.cpp:239] Iteration 44190 (0.989835 iter/s, 10.1027s/10 iters), loss = 6.53224
I0524 06:32:50.231633 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53224 (* 1 = 6.53224 loss)
I0524 06:32:51.426453 11835 sgd_solver.cpp:112] Iteration 44190, lr = 0.1
I0524 06:32:59.232265 11835 solver.cpp:239] Iteration 44200 (1.11107 iter/s, 9.00029s/10 iters), loss = 6.01843
I0524 06:32:59.232309 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01843 (* 1 = 6.01843 loss)
I0524 06:32:59.232326 11835 sgd_solver.cpp:112] Iteration 44200, lr = 0.1
I0524 06:33:07.531592 11835 solver.cpp:239] Iteration 44210 (1.20498 iter/s, 8.29889s/10 iters), loss = 5.85168
I0524 06:33:07.531642 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85168 (* 1 = 5.85168 loss)
I0524 06:33:07.535945 11835 sgd_solver.cpp:112] Iteration 44210, lr = 0.1
I0524 06:33:17.402011 11835 solver.cpp:239] Iteration 44220 (1.01317 iter/s, 9.86999s/10 iters), loss = 5.71944
I0524 06:33:17.402058 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71944 (* 1 = 5.71944 loss)
I0524 06:33:17.402185 11835 sgd_solver.cpp:112] Iteration 44220, lr = 0.1
I0524 06:33:26.029381 11835 solver.cpp:239] Iteration 44230 (1.15915 iter/s, 8.62698s/10 iters), loss = 7.25963
I0524 06:33:26.030689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.25963 (* 1 = 7.25963 loss)
I0524 06:33:26.030763 11835 sgd_solver.cpp:112] Iteration 44230, lr = 0.1
I0524 06:33:34.107671 11835 solver.cpp:239] Iteration 44240 (1.23845 iter/s, 8.07458s/10 iters), loss = 7.05701
I0524 06:33:34.107738 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.05701 (* 1 = 7.05701 loss)
I0524 06:33:34.107817 11835 sgd_solver.cpp:112] Iteration 44240, lr = 0.1
I0524 06:33:43.210829 11835 solver.cpp:239] Iteration 44250 (1.09857 iter/s, 9.10273s/10 iters), loss = 6.04386
I0524 06:33:43.210886 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04386 (* 1 = 6.04386 loss)
I0524 06:33:43.394986 11835 sgd_solver.cpp:112] Iteration 44250, lr = 0.1
I0524 06:33:51.787586 11835 solver.cpp:239] Iteration 44260 (1.166 iter/s, 8.57634s/10 iters), loss = 5.86967
I0524 06:33:51.787673 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86967 (* 1 = 5.86967 loss)
I0524 06:33:51.787714 11835 sgd_solver.cpp:112] Iteration 44260, lr = 0.1
I0524 06:33:58.840745 11835 solver.cpp:239] Iteration 44270 (1.41787 iter/s, 7.05282s/10 iters), loss = 6.05221
I0524 06:33:58.840912 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05221 (* 1 = 6.05221 loss)
I0524 06:33:58.872385 11835 sgd_solver.cpp:112] Iteration 44270, lr = 0.1
I0524 06:34:05.238543 11835 solver.cpp:239] Iteration 44280 (1.56314 iter/s, 6.39738s/10 iters), loss = 6.93434
I0524 06:34:05.238597 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93434 (* 1 = 6.93434 loss)
I0524 06:34:05.244292 11835 sgd_solver.cpp:112] Iteration 44280, lr = 0.1
I0524 06:34:12.188251 11835 solver.cpp:239] Iteration 44290 (1.43897 iter/s, 6.9494s/10 iters), loss = 6.3432
I0524 06:34:12.188292 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3432 (* 1 = 6.3432 loss)
I0524 06:34:12.227259 11835 sgd_solver.cpp:112] Iteration 44290, lr = 0.1
I0524 06:34:18.968320 11835 solver.cpp:239] Iteration 44300 (1.47498 iter/s, 6.77975s/10 iters), loss = 6.62313
I0524 06:34:18.968374 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62313 (* 1 = 6.62313 loss)
I0524 06:34:18.968482 11835 sgd_solver.cpp:112] Iteration 44300, lr = 0.1
I0524 06:34:26.394660 11835 solver.cpp:239] Iteration 44310 (1.34662 iter/s, 7.42599s/10 iters), loss = 5.48062
I0524 06:34:26.394739 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.48062 (* 1 = 5.48062 loss)
I0524 06:34:26.394834 11835 sgd_solver.cpp:112] Iteration 44310, lr = 0.1
I0524 06:34:32.912878 11835 solver.cpp:239] Iteration 44320 (1.53424 iter/s, 6.5179s/10 iters), loss = 6.71437
I0524 06:34:32.913079 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71437 (* 1 = 6.71437 loss)
I0524 06:34:32.913094 11835 sgd_solver.cpp:112] Iteration 44320, lr = 0.1
I0524 06:34:38.830226 11835 solver.cpp:239] Iteration 44330 (1.69038 iter/s, 5.91583s/10 iters), loss = 5.37581
I0524 06:34:38.830276 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.37581 (* 1 = 5.37581 loss)
I0524 06:34:38.870712 11835 sgd_solver.cpp:112] Iteration 44330, lr = 0.1
I0524 06:34:46.026083 11835 solver.cpp:239] Iteration 44340 (1.38975 iter/s, 7.19552s/10 iters), loss = 6.53488
I0524 06:34:46.026167 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53488 (* 1 = 6.53488 loss)
I0524 06:34:46.028074 11835 sgd_solver.cpp:112] Iteration 44340, lr = 0.1
I0524 06:34:52.967500 11835 solver.cpp:239] Iteration 44350 (1.4407 iter/s, 6.94108s/10 iters), loss = 5.87383
I0524 06:34:52.967550 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87383 (* 1 = 5.87383 loss)
I0524 06:34:52.967734 11835 sgd_solver.cpp:112] Iteration 44350, lr = 0.1
I0524 06:35:01.207767 11835 solver.cpp:239] Iteration 44360 (1.21361 iter/s, 8.23989s/10 iters), loss = 5.26641
I0524 06:35:01.207825 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.26641 (* 1 = 5.26641 loss)
I0524 06:35:01.208007 11835 sgd_solver.cpp:112] Iteration 44360, lr = 0.1
I0524 06:35:07.225749 11835 solver.cpp:239] Iteration 44370 (1.66177 iter/s, 6.01768s/10 iters), loss = 5.90358
I0524 06:35:07.225903 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90358 (* 1 = 5.90358 loss)
I0524 06:35:07.246709 11835 sgd_solver.cpp:112] Iteration 44370, lr = 0.1
I0524 06:35:13.565001 11835 solver.cpp:239] Iteration 44380 (1.57757 iter/s, 6.33885s/10 iters), loss = 5.77782
I0524 06:35:13.565052 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.77782 (* 1 = 5.77782 loss)
I0524 06:35:14.338379 11835 sgd_solver.cpp:112] Iteration 44380, lr = 0.1
I0524 06:35:24.396462 11835 solver.cpp:239] Iteration 44390 (0.923276 iter/s, 10.831s/10 iters), loss = 6.20235
I0524 06:35:24.396520 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20235 (* 1 = 6.20235 loss)
I0524 06:35:24.618888 11835 sgd_solver.cpp:112] Iteration 44390, lr = 0.1
I0524 06:35:33.793999 11835 solver.cpp:239] Iteration 44400 (1.06416 iter/s, 9.39711s/10 iters), loss = 6.70636
I0524 06:35:33.794054 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70636 (* 1 = 6.70636 loss)
I0524 06:35:33.794102 11835 sgd_solver.cpp:112] Iteration 44400, lr = 0.1
I0524 06:35:41.767711 11835 solver.cpp:239] Iteration 44410 (1.25418 iter/s, 7.97334s/10 iters), loss = 6.61717
I0524 06:35:41.767964 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61717 (* 1 = 6.61717 loss)
I0524 06:35:41.768003 11835 sgd_solver.cpp:112] Iteration 44410, lr = 0.1
I0524 06:35:47.947922 11835 solver.cpp:239] Iteration 44420 (1.61818 iter/s, 6.17977s/10 iters), loss = 6.21398
I0524 06:35:47.947975 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21398 (* 1 = 6.21398 loss)
I0524 06:35:47.947990 11835 sgd_solver.cpp:112] Iteration 44420, lr = 0.1
I0524 06:35:53.788048 11835 solver.cpp:239] Iteration 44430 (1.71239 iter/s, 5.83977s/10 iters), loss = 6.76791
I0524 06:35:53.788092 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76791 (* 1 = 6.76791 loss)
I0524 06:35:53.788118 11835 sgd_solver.cpp:112] Iteration 44430, lr = 0.1
I0524 06:35:59.487761 11835 solver.cpp:239] Iteration 44440 (1.75457 iter/s, 5.6994s/10 iters), loss = 7.02394
I0524 06:35:59.487807 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.02394 (* 1 = 7.02394 loss)
I0524 06:35:59.516001 11835 sgd_solver.cpp:112] Iteration 44440, lr = 0.1
I0524 06:36:06.971346 11835 solver.cpp:239] Iteration 44450 (1.33632 iter/s, 7.48324s/10 iters), loss = 6.4022
I0524 06:36:06.971408 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4022 (* 1 = 6.4022 loss)
I0524 06:36:07.382556 11835 sgd_solver.cpp:112] Iteration 44450, lr = 0.1
I0524 06:36:14.190397 11835 solver.cpp:239] Iteration 44460 (1.38529 iter/s, 7.21872s/10 iters), loss = 6.24158
I0524 06:36:14.190665 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24158 (* 1 = 6.24158 loss)
I0524 06:36:14.190873 11835 sgd_solver.cpp:112] Iteration 44460, lr = 0.1
I0524 06:36:20.747618 11835 solver.cpp:239] Iteration 44470 (1.52515 iter/s, 6.55671s/10 iters), loss = 6.14837
I0524 06:36:20.747666 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14837 (* 1 = 6.14837 loss)
I0524 06:36:20.765071 11835 sgd_solver.cpp:112] Iteration 44470, lr = 0.1
I0524 06:36:26.409010 11835 solver.cpp:239] Iteration 44480 (1.76643 iter/s, 5.66112s/10 iters), loss = 7.13191
I0524 06:36:26.409059 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13191 (* 1 = 7.13191 loss)
I0524 06:36:26.685168 11835 sgd_solver.cpp:112] Iteration 44480, lr = 0.1
I0524 06:36:33.409430 11835 solver.cpp:239] Iteration 44490 (1.42855 iter/s, 7.0001s/10 iters), loss = 5.72408
I0524 06:36:33.409481 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72408 (* 1 = 5.72408 loss)
I0524 06:36:33.519896 11835 sgd_solver.cpp:112] Iteration 44490, lr = 0.1
I0524 06:36:39.869843 11835 solver.cpp:239] Iteration 44500 (1.54796 iter/s, 6.4601s/10 iters), loss = 6.34004
I0524 06:36:39.869905 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34004 (* 1 = 6.34004 loss)
I0524 06:36:39.870111 11835 sgd_solver.cpp:112] Iteration 44500, lr = 0.1
I0524 06:36:45.869410 11835 solver.cpp:239] Iteration 44510 (1.66687 iter/s, 5.99928s/10 iters), loss = 7.60377
I0524 06:36:45.869637 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.60377 (* 1 = 7.60377 loss)
I0524 06:36:45.869676 11835 sgd_solver.cpp:112] Iteration 44510, lr = 0.1
I0524 06:36:53.927887 11835 solver.cpp:239] Iteration 44520 (1.2411 iter/s, 8.05736s/10 iters), loss = 6.31003
I0524 06:36:53.927989 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31003 (* 1 = 6.31003 loss)
I0524 06:36:53.928062 11835 sgd_solver.cpp:112] Iteration 44520, lr = 0.1
I0524 06:37:00.694928 11835 solver.cpp:239] Iteration 44530 (1.47783 iter/s, 6.7667s/10 iters), loss = 6.09793
I0524 06:37:00.694988 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09793 (* 1 = 6.09793 loss)
I0524 06:37:00.695009 11835 sgd_solver.cpp:112] Iteration 44530, lr = 0.1
I0524 06:37:07.043192 11835 solver.cpp:239] Iteration 44540 (1.57531 iter/s, 6.34797s/10 iters), loss = 6.44053
I0524 06:37:07.043231 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44053 (* 1 = 6.44053 loss)
I0524 06:37:07.043340 11835 sgd_solver.cpp:112] Iteration 44540, lr = 0.1
I0524 06:37:13.127264 11835 solver.cpp:239] Iteration 44550 (1.64371 iter/s, 6.08378s/10 iters), loss = 6.69947
I0524 06:37:13.127321 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69947 (* 1 = 6.69947 loss)
I0524 06:37:13.127367 11835 sgd_solver.cpp:112] Iteration 44550, lr = 0.1
I0524 06:37:21.996937 11835 solver.cpp:239] Iteration 44560 (1.12749 iter/s, 8.86929s/10 iters), loss = 5.9924
I0524 06:37:21.997130 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9924 (* 1 = 5.9924 loss)
I0524 06:37:22.679901 11835 sgd_solver.cpp:112] Iteration 44560, lr = 0.1
I0524 06:37:28.903942 11835 solver.cpp:239] Iteration 44570 (1.4479 iter/s, 6.90655s/10 iters), loss = 6.10031
I0524 06:37:28.903997 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10031 (* 1 = 6.10031 loss)
I0524 06:37:28.944195 11835 sgd_solver.cpp:112] Iteration 44570, lr = 0.1
I0524 06:37:39.803200 11835 solver.cpp:239] Iteration 44580 (0.917533 iter/s, 10.8988s/10 iters), loss = 6.29672
I0524 06:37:39.803258 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29672 (* 1 = 6.29672 loss)
I0524 06:37:39.803395 11835 sgd_solver.cpp:112] Iteration 44580, lr = 0.1
I0524 06:37:45.728799 11835 solver.cpp:239] Iteration 44590 (1.68767 iter/s, 5.92531s/10 iters), loss = 5.92149
I0524 06:37:45.728849 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92149 (* 1 = 5.92149 loss)
I0524 06:37:45.728904 11835 sgd_solver.cpp:112] Iteration 44590, lr = 0.1
I0524 06:37:51.775023 11835 solver.cpp:239] Iteration 44600 (1.654 iter/s, 6.04593s/10 iters), loss = 6.67132
I0524 06:37:51.775076 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67132 (* 1 = 6.67132 loss)
I0524 06:37:51.775091 11835 sgd_solver.cpp:112] Iteration 44600, lr = 0.1
I0524 06:37:57.789481 11835 solver.cpp:239] Iteration 44610 (1.66276 iter/s, 6.01409s/10 iters), loss = 6.06147
I0524 06:37:57.789769 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06147 (* 1 = 6.06147 loss)
I0524 06:37:57.789805 11835 sgd_solver.cpp:112] Iteration 44610, lr = 0.1
I0524 06:38:04.579481 11835 solver.cpp:239] Iteration 44620 (1.47291 iter/s, 6.78929s/10 iters), loss = 6.06787
I0524 06:38:04.579533 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06787 (* 1 = 6.06787 loss)
I0524 06:38:04.579607 11835 sgd_solver.cpp:112] Iteration 44620, lr = 0.1
I0524 06:38:11.391777 11835 solver.cpp:239] Iteration 44630 (1.468 iter/s, 6.81199s/10 iters), loss = 5.9827
I0524 06:38:11.391815 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9827 (* 1 = 5.9827 loss)
I0524 06:38:11.392050 11835 sgd_solver.cpp:112] Iteration 44630, lr = 0.1
I0524 06:38:19.507932 11835 solver.cpp:239] Iteration 44640 (1.23217 iter/s, 8.11576s/10 iters), loss = 5.57076
I0524 06:38:19.507997 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.57076 (* 1 = 5.57076 loss)
I0524 06:38:19.508039 11835 sgd_solver.cpp:112] Iteration 44640, lr = 0.1
I0524 06:38:25.777777 11835 solver.cpp:239] Iteration 44650 (1.59501 iter/s, 6.26954s/10 iters), loss = 6.02468
I0524 06:38:25.777823 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02468 (* 1 = 6.02468 loss)
I0524 06:38:25.778141 11835 sgd_solver.cpp:112] Iteration 44650, lr = 0.1
I0524 06:38:33.227824 11835 solver.cpp:239] Iteration 44660 (1.34234 iter/s, 7.4497s/10 iters), loss = 6.3705
I0524 06:38:33.227995 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3705 (* 1 = 6.3705 loss)
I0524 06:38:33.510902 11835 sgd_solver.cpp:112] Iteration 44660, lr = 0.1
I0524 06:38:39.436569 11835 solver.cpp:239] Iteration 44670 (1.61074 iter/s, 6.20834s/10 iters), loss = 6.87176
I0524 06:38:39.436621 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87176 (* 1 = 6.87176 loss)
I0524 06:38:39.436638 11835 sgd_solver.cpp:112] Iteration 44670, lr = 0.1
I0524 06:38:46.871827 11835 solver.cpp:239] Iteration 44680 (1.34501 iter/s, 7.43492s/10 iters), loss = 6.36601
I0524 06:38:46.871883 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36601 (* 1 = 6.36601 loss)
I0524 06:38:47.034396 11835 sgd_solver.cpp:112] Iteration 44680, lr = 0.1
I0524 06:38:54.133630 11835 solver.cpp:239] Iteration 44690 (1.37713 iter/s, 7.26146s/10 iters), loss = 6.20335
I0524 06:38:54.133699 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20335 (* 1 = 6.20335 loss)
I0524 06:38:54.322808 11835 sgd_solver.cpp:112] Iteration 44690, lr = 0.1
I0524 06:39:00.695904 11835 solver.cpp:239] Iteration 44700 (1.52394 iter/s, 6.56195s/10 iters), loss = 6.43028
I0524 06:39:00.695955 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43028 (* 1 = 6.43028 loss)
I0524 06:39:01.151093 11835 sgd_solver.cpp:112] Iteration 44700, lr = 0.1
I0524 06:39:09.566690 11835 solver.cpp:239] Iteration 44710 (1.12735 iter/s, 8.87038s/10 iters), loss = 5.98777
I0524 06:39:09.566953 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98777 (* 1 = 5.98777 loss)
I0524 06:39:09.716964 11835 sgd_solver.cpp:112] Iteration 44710, lr = 0.1
I0524 06:39:17.769572 11835 solver.cpp:239] Iteration 44720 (1.21917 iter/s, 8.20228s/10 iters), loss = 6.48476
I0524 06:39:17.769670 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48476 (* 1 = 6.48476 loss)
I0524 06:39:18.385207 11835 sgd_solver.cpp:112] Iteration 44720, lr = 0.1
I0524 06:39:26.750216 11835 solver.cpp:239] Iteration 44730 (1.11356 iter/s, 8.9802s/10 iters), loss = 6.84251
I0524 06:39:26.750275 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84251 (* 1 = 6.84251 loss)
I0524 06:39:26.750470 11835 sgd_solver.cpp:112] Iteration 44730, lr = 0.1
I0524 06:39:33.404896 11835 solver.cpp:239] Iteration 44740 (1.50278 iter/s, 6.65433s/10 iters), loss = 6.62238
I0524 06:39:33.404960 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62238 (* 1 = 6.62238 loss)
I0524 06:39:33.405131 11835 sgd_solver.cpp:112] Iteration 44740, lr = 0.1
I0524 06:39:39.916167 11835 solver.cpp:239] Iteration 44750 (1.53587 iter/s, 6.51096s/10 iters), loss = 5.77051
I0524 06:39:39.916368 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.77051 (* 1 = 5.77051 loss)
I0524 06:39:40.048125 11835 sgd_solver.cpp:112] Iteration 44750, lr = 0.1
I0524 06:39:47.169718 11835 solver.cpp:239] Iteration 44760 (1.37873 iter/s, 7.25307s/10 iters), loss = 5.80745
I0524 06:39:47.169771 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80745 (* 1 = 5.80745 loss)
I0524 06:39:47.192530 11835 sgd_solver.cpp:112] Iteration 44760, lr = 0.1
I0524 06:39:54.633620 11835 solver.cpp:239] Iteration 44770 (1.33984 iter/s, 7.46356s/10 iters), loss = 6.29297
I0524 06:39:54.633668 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29297 (* 1 = 6.29297 loss)
I0524 06:39:55.290382 11835 sgd_solver.cpp:112] Iteration 44770, lr = 0.1
I0524 06:40:02.647517 11835 solver.cpp:239] Iteration 44780 (1.24789 iter/s, 8.01354s/10 iters), loss = 6.01516
I0524 06:40:02.647567 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01516 (* 1 = 6.01516 loss)
I0524 06:40:04.139655 11835 sgd_solver.cpp:112] Iteration 44780, lr = 0.1
I0524 06:40:10.400986 11835 solver.cpp:239] Iteration 44790 (1.2898 iter/s, 7.75312s/10 iters), loss = 5.89725
I0524 06:40:10.401149 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89725 (* 1 = 5.89725 loss)
I0524 06:40:10.401168 11835 sgd_solver.cpp:112] Iteration 44790, lr = 0.1
I0524 06:40:16.285106 11835 solver.cpp:239] Iteration 44800 (1.69975 iter/s, 5.88322s/10 iters), loss = 5.50784
I0524 06:40:16.285141 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.50784 (* 1 = 5.50784 loss)
I0524 06:40:16.285153 11835 sgd_solver.cpp:112] Iteration 44800, lr = 0.1
I0524 06:40:22.288101 11835 solver.cpp:239] Iteration 44810 (1.66591 iter/s, 6.00272s/10 iters), loss = 5.97609
I0524 06:40:22.288156 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97609 (* 1 = 5.97609 loss)
I0524 06:40:22.508146 11835 sgd_solver.cpp:112] Iteration 44810, lr = 0.1
I0524 06:40:31.011584 11835 solver.cpp:239] Iteration 44820 (1.14638 iter/s, 8.72309s/10 iters), loss = 5.89175
I0524 06:40:31.011638 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89175 (* 1 = 5.89175 loss)
I0524 06:40:31.011924 11835 sgd_solver.cpp:112] Iteration 44820, lr = 0.1
I0524 06:40:37.457638 11835 solver.cpp:239] Iteration 44830 (1.55141 iter/s, 6.44575s/10 iters), loss = 6.46102
I0524 06:40:37.457689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46102 (* 1 = 6.46102 loss)
I0524 06:40:37.457782 11835 sgd_solver.cpp:112] Iteration 44830, lr = 0.1
I0524 06:40:44.269856 11835 solver.cpp:239] Iteration 44840 (1.46802 iter/s, 6.8119s/10 iters), loss = 5.60277
I0524 06:40:44.270097 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.60277 (* 1 = 5.60277 loss)
I0524 06:40:44.328235 11835 sgd_solver.cpp:112] Iteration 44840, lr = 0.1
I0524 06:40:50.147478 11835 solver.cpp:239] Iteration 44850 (1.7015 iter/s, 5.87718s/10 iters), loss = 6.90927
I0524 06:40:50.147547 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90927 (* 1 = 6.90927 loss)
I0524 06:40:50.147604 11835 sgd_solver.cpp:112] Iteration 44850, lr = 0.1
I0524 06:40:56.430600 11835 solver.cpp:239] Iteration 44860 (1.59164 iter/s, 6.28282s/10 iters), loss = 6.28313
I0524 06:40:56.430654 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.28313 (* 1 = 6.28313 loss)
I0524 06:40:56.673508 11835 sgd_solver.cpp:112] Iteration 44860, lr = 0.1
I0524 06:41:04.643982 11835 solver.cpp:239] Iteration 44870 (1.21758 iter/s, 8.213s/10 iters), loss = 6.42285
I0524 06:41:04.644031 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42285 (* 1 = 6.42285 loss)
I0524 06:41:04.644371 11835 sgd_solver.cpp:112] Iteration 44870, lr = 0.1
I0524 06:41:10.681313 11835 solver.cpp:239] Iteration 44880 (1.65644 iter/s, 6.03703s/10 iters), loss = 5.33216
I0524 06:41:10.681380 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.33216 (* 1 = 5.33216 loss)
I0524 06:41:11.355015 11835 sgd_solver.cpp:112] Iteration 44880, lr = 0.1
I0524 06:41:17.469848 11835 solver.cpp:239] Iteration 44890 (1.47314 iter/s, 6.78821s/10 iters), loss = 5.19573
I0524 06:41:17.470165 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.19573 (* 1 = 5.19573 loss)
I0524 06:41:17.470221 11835 sgd_solver.cpp:112] Iteration 44890, lr = 0.1
I0524 06:41:25.078944 11835 solver.cpp:239] Iteration 44900 (1.31432 iter/s, 7.60848s/10 iters), loss = 6.60035
I0524 06:41:25.079097 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60035 (* 1 = 6.60035 loss)
I0524 06:41:25.079123 11835 sgd_solver.cpp:112] Iteration 44900, lr = 0.1
I0524 06:41:31.150447 11835 solver.cpp:239] Iteration 44910 (1.64714 iter/s, 6.07115s/10 iters), loss = 4.92931
I0524 06:41:31.150537 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.92931 (* 1 = 4.92931 loss)
I0524 06:41:31.464726 11835 sgd_solver.cpp:112] Iteration 44910, lr = 0.1
I0524 06:41:37.994844 11835 solver.cpp:239] Iteration 44920 (1.46113 iter/s, 6.84404s/10 iters), loss = 6.27292
I0524 06:41:37.994907 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27292 (* 1 = 6.27292 loss)
I0524 06:41:38.001441 11835 sgd_solver.cpp:112] Iteration 44920, lr = 0.1
I0524 06:41:44.573743 11835 solver.cpp:239] Iteration 44930 (1.52008 iter/s, 6.5786s/10 iters), loss = 6.10452
I0524 06:41:44.573782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10452 (* 1 = 6.10452 loss)
I0524 06:41:44.574012 11835 sgd_solver.cpp:112] Iteration 44930, lr = 0.1
I0524 06:41:52.371119 11835 solver.cpp:239] Iteration 44940 (1.28254 iter/s, 7.79702s/10 iters), loss = 5.95237
I0524 06:41:52.371265 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95237 (* 1 = 5.95237 loss)
I0524 06:41:52.480880 11835 sgd_solver.cpp:112] Iteration 44940, lr = 0.1
I0524 06:42:00.649332 11835 solver.cpp:239] Iteration 44950 (1.20806 iter/s, 8.27774s/10 iters), loss = 6.57684
I0524 06:42:00.649394 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57684 (* 1 = 6.57684 loss)
I0524 06:42:01.082406 11835 sgd_solver.cpp:112] Iteration 44950, lr = 0.1
I0524 06:42:10.174191 11835 solver.cpp:239] Iteration 44960 (1.04993 iter/s, 9.52443s/10 iters), loss = 6.25451
I0524 06:42:10.174276 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25451 (* 1 = 6.25451 loss)
I0524 06:42:10.202329 11835 sgd_solver.cpp:112] Iteration 44960, lr = 0.1
I0524 06:42:17.554867 11835 solver.cpp:239] Iteration 44970 (1.35495 iter/s, 7.38032s/10 iters), loss = 6.06432
I0524 06:42:17.554916 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06432 (* 1 = 6.06432 loss)
I0524 06:42:17.598225 11835 sgd_solver.cpp:112] Iteration 44970, lr = 0.1
I0524 06:42:24.369853 11835 solver.cpp:239] Iteration 44980 (1.46743 iter/s, 6.81463s/10 iters), loss = 6.00489
I0524 06:42:24.370123 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00489 (* 1 = 6.00489 loss)
I0524 06:42:24.737555 11835 sgd_solver.cpp:112] Iteration 44980, lr = 0.1
I0524 06:42:31.012028 11835 solver.cpp:239] Iteration 44990 (1.50565 iter/s, 6.64167s/10 iters), loss = 5.55554
I0524 06:42:31.012092 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.55554 (* 1 = 5.55554 loss)
I0524 06:42:31.012197 11835 sgd_solver.cpp:112] Iteration 44990, lr = 0.1
I0524 06:42:37.869566 11835 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_45000.caffemodel
I0524 06:42:38.032847 11835 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_45000.solverstate
I0524 06:42:38.588933 11835 solver.cpp:239] Iteration 45000 (1.31986 iter/s, 7.57656s/10 iters), loss = 5.92173
I0524 06:42:38.588989 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92173 (* 1 = 5.92173 loss)
I0524 06:42:38.589200 11835 sgd_solver.cpp:112] Iteration 45000, lr = 0.1
I0524 06:42:45.328625 11835 solver.cpp:239] Iteration 45010 (1.48381 iter/s, 6.73939s/10 iters), loss = 7.37563
I0524 06:42:45.328658 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.37563 (* 1 = 7.37563 loss)
I0524 06:42:45.328670 11835 sgd_solver.cpp:112] Iteration 45010, lr = 0.1
I0524 06:42:52.896138 11835 solver.cpp:239] Iteration 45020 (1.3215 iter/s, 7.56714s/10 iters), loss = 6.46381
I0524 06:42:52.896215 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46381 (* 1 = 6.46381 loss)
I0524 06:42:52.896235 11835 sgd_solver.cpp:112] Iteration 45020, lr = 0.1
I0524 06:42:59.397728 11835 solver.cpp:239] Iteration 45030 (1.53825 iter/s, 6.50089s/10 iters), loss = 5.948
I0524 06:42:59.398058 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.948 (* 1 = 5.948 loss)
I0524 06:42:59.415508 11835 sgd_solver.cpp:112] Iteration 45030, lr = 0.1
I0524 06:43:07.304854 11835 solver.cpp:239] Iteration 45040 (1.26477 iter/s, 7.90657s/10 iters), loss = 6.80182
I0524 06:43:07.304908 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80182 (* 1 = 6.80182 loss)
I0524 06:43:07.374155 11835 sgd_solver.cpp:112] Iteration 45040, lr = 0.1
I0524 06:43:14.227429 11835 solver.cpp:239] Iteration 45050 (1.44462 iter/s, 6.92225s/10 iters), loss = 6.04098
I0524 06:43:14.227531 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04098 (* 1 = 6.04098 loss)
I0524 06:43:14.227716 11835 sgd_solver.cpp:112] Iteration 45050, lr = 0.1
I0524 06:43:20.932441 11835 solver.cpp:239] Iteration 45060 (1.49149 iter/s, 6.70471s/10 iters), loss = 6.40151
I0524 06:43:20.932480 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40151 (* 1 = 6.40151 loss)
I0524 06:43:20.932492 11835 sgd_solver.cpp:112] Iteration 45060, lr = 0.1
I0524 06:43:27.821314 11835 solver.cpp:239] Iteration 45070 (1.45168 iter/s, 6.88855s/10 iters), loss = 6.45745
I0524 06:43:27.821373 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45745 (* 1 = 6.45745 loss)
I0524 06:43:27.821499 11835 sgd_solver.cpp:112] Iteration 45070, lr = 0.1
I0524 06:43:34.233862 11835 solver.cpp:239] Iteration 45080 (1.55952 iter/s, 6.41223s/10 iters), loss = 6.31895
I0524 06:43:34.234109 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31895 (* 1 = 6.31895 loss)
I0524 06:43:34.493449 11835 sgd_solver.cpp:112] Iteration 45080, lr = 0.1
I0524 06:43:41.974270 11835 solver.cpp:239] Iteration 45090 (1.292 iter/s, 7.73991s/10 iters), loss = 6.49006
I0524 06:43:41.974310 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49006 (* 1 = 6.49006 loss)
I0524 06:43:41.974323 11835 sgd_solver.cpp:112] Iteration 45090, lr = 0.1
I0524 06:43:48.484850 11835 solver.cpp:239] Iteration 45100 (1.53603 iter/s, 6.51028s/10 iters), loss = 6.54183
I0524 06:43:48.484917 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54183 (* 1 = 6.54183 loss)
I0524 06:43:48.484933 11835 sgd_solver.cpp:112] Iteration 45100, lr = 0.1
I0524 06:43:54.543742 11835 solver.cpp:239] Iteration 45110 (1.6506 iter/s, 6.05839s/10 iters), loss = 6.13821
I0524 06:43:54.543810 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13821 (* 1 = 6.13821 loss)
I0524 06:43:54.544057 11835 sgd_solver.cpp:112] Iteration 45110, lr = 0.1
I0524 06:44:01.178849 11835 solver.cpp:239] Iteration 45120 (1.50721 iter/s, 6.63477s/10 iters), loss = 6.24716
I0524 06:44:01.178915 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24716 (* 1 = 6.24716 loss)
I0524 06:44:01.280403 11835 sgd_solver.cpp:112] Iteration 45120, lr = 0.1
I0524 06:44:11.164471 11835 solver.cpp:239] Iteration 45130 (1.00148 iter/s, 9.98517s/10 iters), loss = 5.92206
I0524 06:44:11.164768 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92206 (* 1 = 5.92206 loss)
I0524 06:44:11.165057 11835 sgd_solver.cpp:112] Iteration 45130, lr = 0.1
I0524 06:44:17.211032 11835 solver.cpp:239] Iteration 45140 (1.65397 iter/s, 6.04607s/10 iters), loss = 6.53317
I0524 06:44:17.211091 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53317 (* 1 = 6.53317 loss)
I0524 06:44:17.803762 11835 sgd_solver.cpp:112] Iteration 45140, lr = 0.1
I0524 06:44:26.378502 11835 solver.cpp:239] Iteration 45150 (1.09086 iter/s, 9.16706s/10 iters), loss = 6.19683
I0524 06:44:26.378576 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19683 (* 1 = 6.19683 loss)
I0524 06:44:26.396898 11835 sgd_solver.cpp:112] Iteration 45150, lr = 0.1
I0524 06:44:32.145108 11835 solver.cpp:239] Iteration 45160 (1.73421 iter/s, 5.76632s/10 iters), loss = 6.7224
I0524 06:44:32.145192 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7224 (* 1 = 6.7224 loss)
I0524 06:44:32.145220 11835 sgd_solver.cpp:112] Iteration 45160, lr = 0.1
I0524 06:44:38.532907 11835 solver.cpp:239] Iteration 45170 (1.56557 iter/s, 6.38746s/10 iters), loss = 6.70402
I0524 06:44:38.532948 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70402 (* 1 = 6.70402 loss)
I0524 06:44:38.533087 11835 sgd_solver.cpp:112] Iteration 45170, lr = 0.1
I0524 06:44:45.950105 11835 solver.cpp:239] Iteration 45180 (1.34828 iter/s, 7.41687s/10 iters), loss = 6.34684
I0524 06:44:45.950343 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34684 (* 1 = 6.34684 loss)
I0524 06:44:46.505372 11835 sgd_solver.cpp:112] Iteration 45180, lr = 0.1
I0524 06:44:52.623050 11835 solver.cpp:239] Iteration 45190 (1.49869 iter/s, 6.67248s/10 iters), loss = 5.3154
I0524 06:44:52.623100 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.3154 (* 1 = 5.3154 loss)
I0524 06:44:53.397169 11835 sgd_solver.cpp:112] Iteration 45190, lr = 0.1
I0524 06:45:02.544445 11835 solver.cpp:239] Iteration 45200 (1.00797 iter/s, 9.92097s/10 iters), loss = 6.18765
I0524 06:45:02.544487 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18765 (* 1 = 6.18765 loss)
I0524 06:45:03.978096 11835 sgd_solver.cpp:112] Iteration 45200, lr = 0.1
I0524 06:45:11.428414 11835 solver.cpp:239] Iteration 45210 (1.12567 iter/s, 8.88358s/10 iters), loss = 6.13112
I0524 06:45:11.428468 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13112 (* 1 = 6.13112 loss)
I0524 06:45:11.428483 11835 sgd_solver.cpp:112] Iteration 45210, lr = 0.1
I0524 06:45:18.021308 11835 solver.cpp:239] Iteration 45220 (1.51685 iter/s, 6.59259s/10 iters), loss = 6.81788
I0524 06:45:18.021440 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81788 (* 1 = 6.81788 loss)
I0524 06:45:18.021543 11835 sgd_solver.cpp:112] Iteration 45220, lr = 0.1
I0524 06:45:23.763152 11835 solver.cpp:239] Iteration 45230 (1.74171 iter/s, 5.74149s/10 iters), loss = 6.71707
I0524 06:45:23.763200 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71707 (* 1 = 6.71707 loss)
I0524 06:45:23.763216 11835 sgd_solver.cpp:112] Iteration 45230, lr = 0.1
I0524 06:45:32.213984 11835 solver.cpp:239] Iteration 45240 (1.18338 iter/s, 8.45037s/10 iters), loss = 5.56561
I0524 06:45:32.214047 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56561 (* 1 = 5.56561 loss)
I0524 06:45:32.214205 11835 sgd_solver.cpp:112] Iteration 45240, lr = 0.1
I0524 06:45:40.076481 11835 solver.cpp:239] Iteration 45250 (1.27192 iter/s, 7.86214s/10 iters), loss = 6.15955
I0524 06:45:40.076534 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15955 (* 1 = 6.15955 loss)
I0524 06:45:40.076665 11835 sgd_solver.cpp:112] Iteration 45250, lr = 0.1
I0524 06:45:45.767766 11835 solver.cpp:239] Iteration 45260 (1.75716 iter/s, 5.691s/10 iters), loss = 5.20908
I0524 06:45:45.767820 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.20908 (* 1 = 5.20908 loss)
I0524 06:45:45.806171 11835 sgd_solver.cpp:112] Iteration 45260, lr = 0.1
I0524 06:45:54.922062 11835 solver.cpp:239] Iteration 45270 (1.09243 iter/s, 9.15388s/10 iters), loss = 7.04576
I0524 06:45:54.922271 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04576 (* 1 = 7.04576 loss)
I0524 06:45:54.922288 11835 sgd_solver.cpp:112] Iteration 45270, lr = 0.1
I0524 06:46:01.213055 11835 solver.cpp:239] Iteration 45280 (1.58987 iter/s, 6.28982s/10 iters), loss = 6.22607
I0524 06:46:01.213114 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22607 (* 1 = 6.22607 loss)
I0524 06:46:01.213330 11835 sgd_solver.cpp:112] Iteration 45280, lr = 0.1
I0524 06:46:08.471091 11835 solver.cpp:239] Iteration 45290 (1.37785 iter/s, 7.25769s/10 iters), loss = 6.5046
I0524 06:46:08.471159 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5046 (* 1 = 6.5046 loss)
I0524 06:46:08.520192 11835 sgd_solver.cpp:112] Iteration 45290, lr = 0.1
I0524 06:46:16.991816 11835 solver.cpp:239] Iteration 45300 (1.17366 iter/s, 8.52033s/10 iters), loss = 6.0476
I0524 06:46:16.991871 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0476 (* 1 = 6.0476 loss)
I0524 06:46:16.992082 11835 sgd_solver.cpp:112] Iteration 45300, lr = 0.1
I0524 06:46:23.920173 11835 solver.cpp:239] Iteration 45310 (1.44341 iter/s, 6.92804s/10 iters), loss = 6.25038
I0524 06:46:23.920210 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25038 (* 1 = 6.25038 loss)
I0524 06:46:23.920225 11835 sgd_solver.cpp:112] Iteration 45310, lr = 0.1
I0524 06:46:33.114671 11835 solver.cpp:239] Iteration 45320 (1.08765 iter/s, 9.19409s/10 iters), loss = 6.71259
I0524 06:46:33.115397 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71259 (* 1 = 6.71259 loss)
I0524 06:46:33.115470 11835 sgd_solver.cpp:112] Iteration 45320, lr = 0.1
I0524 06:46:39.941429 11835 solver.cpp:239] Iteration 45330 (1.46504 iter/s, 6.82577s/10 iters), loss = 5.35647
I0524 06:46:39.941471 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.35647 (* 1 = 5.35647 loss)
I0524 06:46:39.943425 11835 sgd_solver.cpp:112] Iteration 45330, lr = 0.1
I0524 06:46:48.271742 11835 solver.cpp:239] Iteration 45340 (1.20049 iter/s, 8.32994s/10 iters), loss = 6.17064
I0524 06:46:48.271803 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17064 (* 1 = 6.17064 loss)
I0524 06:46:48.272083 11835 sgd_solver.cpp:112] Iteration 45340, lr = 0.1
I0524 06:46:54.094732 11835 solver.cpp:239] Iteration 45350 (1.71742 iter/s, 5.82269s/10 iters), loss = 5.94966
I0524 06:46:54.094789 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94966 (* 1 = 5.94966 loss)
I0524 06:46:54.094956 11835 sgd_solver.cpp:112] Iteration 45350, lr = 0.1
I0524 06:47:02.657902 11835 solver.cpp:239] Iteration 45360 (1.16784 iter/s, 8.56279s/10 iters), loss = 6.57353
I0524 06:47:02.657950 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57353 (* 1 = 6.57353 loss)
I0524 06:47:02.658253 11835 sgd_solver.cpp:112] Iteration 45360, lr = 0.1
I0524 06:47:11.556352 11835 solver.cpp:239] Iteration 45370 (1.12384 iter/s, 8.89806s/10 iters), loss = 6.04668
I0524 06:47:11.556603 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04668 (* 1 = 6.04668 loss)
I0524 06:47:11.556650 11835 sgd_solver.cpp:112] Iteration 45370, lr = 0.1
I0524 06:47:18.469308 11835 solver.cpp:239] Iteration 45380 (1.44666 iter/s, 6.91248s/10 iters), loss = 6.51702
I0524 06:47:18.469358 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51702 (* 1 = 6.51702 loss)
I0524 06:47:18.469372 11835 sgd_solver.cpp:112] Iteration 45380, lr = 0.1
I0524 06:47:25.099593 11835 solver.cpp:239] Iteration 45390 (1.50844 iter/s, 6.62936s/10 iters), loss = 4.89269
I0524 06:47:25.099654 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.89269 (* 1 = 4.89269 loss)
I0524 06:47:25.494349 11835 sgd_solver.cpp:112] Iteration 45390, lr = 0.1
I0524 06:47:34.366883 11835 solver.cpp:239] Iteration 45400 (1.07911 iter/s, 9.26686s/10 iters), loss = 6.5948
I0524 06:47:34.366945 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5948 (* 1 = 6.5948 loss)
I0524 06:47:34.366964 11835 sgd_solver.cpp:112] Iteration 45400, lr = 0.1
I0524 06:47:41.252001 11835 solver.cpp:239] Iteration 45410 (1.45248 iter/s, 6.88479s/10 iters), loss = 6.18564
I0524 06:47:41.252048 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18564 (* 1 = 6.18564 loss)
I0524 06:47:42.297008 11835 sgd_solver.cpp:112] Iteration 45410, lr = 0.1
I0524 06:47:49.642644 11835 solver.cpp:239] Iteration 45420 (1.19186 iter/s, 8.39028s/10 iters), loss = 5.65851
I0524 06:47:49.642704 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.65851 (* 1 = 5.65851 loss)
I0524 06:47:49.642886 11835 sgd_solver.cpp:112] Iteration 45420, lr = 0.1
I0524 06:47:55.617012 11835 solver.cpp:239] Iteration 45430 (1.6739 iter/s, 5.97407s/10 iters), loss = 5.72606
I0524 06:47:55.617064 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72606 (* 1 = 5.72606 loss)
I0524 06:47:55.617079 11835 sgd_solver.cpp:112] Iteration 45430, lr = 0.1
I0524 06:48:01.844353 11835 solver.cpp:239] Iteration 45440 (1.6059 iter/s, 6.22705s/10 iters), loss = 6.11551
I0524 06:48:01.844404 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11551 (* 1 = 6.11551 loss)
I0524 06:48:01.844437 11835 sgd_solver.cpp:112] Iteration 45440, lr = 0.1
I0524 06:48:08.318840 11835 solver.cpp:239] Iteration 45450 (1.54459 iter/s, 6.47419s/10 iters), loss = 5.61475
I0524 06:48:08.318882 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.61475 (* 1 = 5.61475 loss)
I0524 06:48:09.057229 11835 sgd_solver.cpp:112] Iteration 45450, lr = 0.1
I0524 06:48:16.156565 11835 solver.cpp:239] Iteration 45460 (1.27594 iter/s, 7.83738s/10 iters), loss = 6.80524
I0524 06:48:16.156719 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80524 (* 1 = 6.80524 loss)
I0524 06:48:16.860980 11835 sgd_solver.cpp:112] Iteration 45460, lr = 0.1
I0524 06:48:26.305555 11835 solver.cpp:239] Iteration 45470 (0.985372 iter/s, 10.1484s/10 iters), loss = 5.89369
I0524 06:48:26.305605 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89369 (* 1 = 5.89369 loss)
I0524 06:48:26.426909 11835 sgd_solver.cpp:112] Iteration 45470, lr = 0.1
I0524 06:48:35.557163 11835 solver.cpp:239] Iteration 45480 (1.08094 iter/s, 9.2512s/10 iters), loss = 6.33312
I0524 06:48:35.557210 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33312 (* 1 = 6.33312 loss)
I0524 06:48:35.953904 11835 sgd_solver.cpp:112] Iteration 45480, lr = 0.1
I0524 06:48:44.227195 11835 solver.cpp:239] Iteration 45490 (1.15345 iter/s, 8.66963s/10 iters), loss = 5.29502
I0524 06:48:44.227272 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.29502 (* 1 = 5.29502 loss)
I0524 06:48:44.382725 11835 sgd_solver.cpp:112] Iteration 45490, lr = 0.1
I0524 06:48:50.526384 11835 solver.cpp:239] Iteration 45500 (1.58758 iter/s, 6.29888s/10 iters), loss = 5.22524
I0524 06:48:50.526521 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.22524 (* 1 = 5.22524 loss)
I0524 06:48:50.927320 11835 sgd_solver.cpp:112] Iteration 45500, lr = 0.1
I0524 06:48:56.679796 11835 solver.cpp:239] Iteration 45510 (1.62521 iter/s, 6.15304s/10 iters), loss = 5.43338
I0524 06:48:56.679847 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43338 (* 1 = 5.43338 loss)
I0524 06:48:56.679925 11835 sgd_solver.cpp:112] Iteration 45510, lr = 0.1
I0524 06:49:04.226869 11835 solver.cpp:239] Iteration 45520 (1.32508 iter/s, 7.54672s/10 iters), loss = 6.63158
I0524 06:49:04.226927 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63158 (* 1 = 6.63158 loss)
I0524 06:49:04.227139 11835 sgd_solver.cpp:112] Iteration 45520, lr = 0.1
I0524 06:49:12.349268 11835 solver.cpp:239] Iteration 45530 (1.23122 iter/s, 8.12202s/10 iters), loss = 5.96755
I0524 06:49:12.349323 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96755 (* 1 = 5.96755 loss)
I0524 06:49:13.118058 11835 sgd_solver.cpp:112] Iteration 45530, lr = 0.1
I0524 06:49:19.305351 11835 solver.cpp:239] Iteration 45540 (1.43766 iter/s, 6.95575s/10 iters), loss = 6.62536
I0524 06:49:19.305413 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62536 (* 1 = 6.62536 loss)
I0524 06:49:19.305429 11835 sgd_solver.cpp:112] Iteration 45540, lr = 0.1
I0524 06:49:27.333554 11835 solver.cpp:239] Iteration 45550 (1.24567 iter/s, 8.02783s/10 iters), loss = 5.3336
I0524 06:49:27.334033 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.3336 (* 1 = 5.3336 loss)
I0524 06:49:27.334053 11835 sgd_solver.cpp:112] Iteration 45550, lr = 0.1
I0524 06:49:34.126776 11835 solver.cpp:239] Iteration 45560 (1.47226 iter/s, 6.79229s/10 iters), loss = 6.71762
I0524 06:49:34.126844 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71762 (* 1 = 6.71762 loss)
I0524 06:49:34.128181 11835 sgd_solver.cpp:112] Iteration 45560, lr = 0.1
I0524 06:49:41.167596 11835 solver.cpp:239] Iteration 45570 (1.42036 iter/s, 7.04048s/10 iters), loss = 6.03949
I0524 06:49:41.167649 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03949 (* 1 = 6.03949 loss)
I0524 06:49:41.167863 11835 sgd_solver.cpp:112] Iteration 45570, lr = 0.1
I0524 06:49:48.618103 11835 solver.cpp:239] Iteration 45580 (1.34225 iter/s, 7.45016s/10 iters), loss = 6.26284
I0524 06:49:48.618160 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26284 (* 1 = 6.26284 loss)
I0524 06:49:48.618178 11835 sgd_solver.cpp:112] Iteration 45580, lr = 0.1
I0524 06:49:55.455775 11835 solver.cpp:239] Iteration 45590 (1.46257 iter/s, 6.83729s/10 iters), loss = 7.10911
I0524 06:49:55.455829 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10911 (* 1 = 7.10911 loss)
I0524 06:49:55.455844 11835 sgd_solver.cpp:112] Iteration 45590, lr = 0.1
I0524 06:50:01.505834 11835 solver.cpp:239] Iteration 45600 (1.65299 iter/s, 6.04964s/10 iters), loss = 6.16865
I0524 06:50:01.506076 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16865 (* 1 = 6.16865 loss)
I0524 06:50:01.506121 11835 sgd_solver.cpp:112] Iteration 45600, lr = 0.1
I0524 06:50:07.138825 11835 solver.cpp:239] Iteration 45610 (1.7754 iter/s, 5.63253s/10 iters), loss = 5.50832
I0524 06:50:07.138878 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.50832 (* 1 = 5.50832 loss)
I0524 06:50:07.138895 11835 sgd_solver.cpp:112] Iteration 45610, lr = 0.1
I0524 06:50:15.212604 11835 solver.cpp:239] Iteration 45620 (1.23897 iter/s, 8.07125s/10 iters), loss = 5.03337
I0524 06:50:15.212654 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.03337 (* 1 = 5.03337 loss)
I0524 06:50:15.212816 11835 sgd_solver.cpp:112] Iteration 45620, lr = 0.1
I0524 06:50:24.174924 11835 solver.cpp:239] Iteration 45630 (1.11584 iter/s, 8.96186s/10 iters), loss = 7.46307
I0524 06:50:24.174994 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.46307 (* 1 = 7.46307 loss)
I0524 06:50:24.175053 11835 sgd_solver.cpp:112] Iteration 45630, lr = 0.1
I0524 06:50:31.675038 11835 solver.cpp:239] Iteration 45640 (1.33338 iter/s, 7.49976s/10 iters), loss = 6.34691
I0524 06:50:31.675207 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34691 (* 1 = 6.34691 loss)
I0524 06:50:31.675460 11835 sgd_solver.cpp:112] Iteration 45640, lr = 0.1
I0524 06:50:38.353610 11835 solver.cpp:239] Iteration 45650 (1.49744 iter/s, 6.67808s/10 iters), loss = 5.45026
I0524 06:50:38.353746 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.45026 (* 1 = 5.45026 loss)
I0524 06:50:38.353780 11835 sgd_solver.cpp:112] Iteration 45650, lr = 0.1
I0524 06:50:45.591611 11835 solver.cpp:239] Iteration 45660 (1.38166 iter/s, 7.23765s/10 iters), loss = 5.96027
I0524 06:50:45.591662 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96027 (* 1 = 5.96027 loss)
I0524 06:50:45.662179 11835 sgd_solver.cpp:112] Iteration 45660, lr = 0.1
I0524 06:50:55.838538 11835 solver.cpp:239] Iteration 45670 (0.975948 iter/s, 10.2464s/10 iters), loss = 5.52984
I0524 06:50:55.838646 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.52984 (* 1 = 5.52984 loss)
I0524 06:50:55.838783 11835 sgd_solver.cpp:112] Iteration 45670, lr = 0.1
I0524 06:51:01.914103 11835 solver.cpp:239] Iteration 45680 (1.64602 iter/s, 6.07527s/10 iters), loss = 6.26267
I0524 06:51:01.914355 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26267 (* 1 = 6.26267 loss)
I0524 06:51:01.914396 11835 sgd_solver.cpp:112] Iteration 45680, lr = 0.1
I0524 06:51:09.288857 11835 solver.cpp:239] Iteration 45690 (1.35609 iter/s, 7.37412s/10 iters), loss = 6.81677
I0524 06:51:09.288921 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81677 (* 1 = 6.81677 loss)
I0524 06:51:09.312470 11835 sgd_solver.cpp:112] Iteration 45690, lr = 0.1
I0524 06:51:16.775156 11835 solver.cpp:239] Iteration 45700 (1.33583 iter/s, 7.48595s/10 iters), loss = 5.80465
I0524 06:51:16.775208 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80465 (* 1 = 5.80465 loss)
I0524 06:51:16.775305 11835 sgd_solver.cpp:112] Iteration 45700, lr = 0.1
I0524 06:51:24.580031 11835 solver.cpp:239] Iteration 45710 (1.28131 iter/s, 7.80453s/10 iters), loss = 6.40033
I0524 06:51:24.580081 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40033 (* 1 = 6.40033 loss)
I0524 06:51:24.580096 11835 sgd_solver.cpp:112] Iteration 45710, lr = 0.1
I0524 06:51:31.021173 11835 solver.cpp:239] Iteration 45720 (1.55259 iter/s, 6.44084s/10 iters), loss = 6.39409
I0524 06:51:31.021235 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39409 (* 1 = 6.39409 loss)
I0524 06:51:31.065780 11835 sgd_solver.cpp:112] Iteration 45720, lr = 0.1
I0524 06:51:37.039732 11835 solver.cpp:239] Iteration 45730 (1.66161 iter/s, 6.01827s/10 iters), loss = 6.33364
I0524 06:51:37.039898 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33364 (* 1 = 6.33364 loss)
I0524 06:51:37.040015 11835 sgd_solver.cpp:112] Iteration 45730, lr = 0.1
I0524 06:51:43.963356 11835 solver.cpp:239] Iteration 45740 (1.44442 iter/s, 6.9232s/10 iters), loss = 7.22198
I0524 06:51:43.963402 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.22198 (* 1 = 7.22198 loss)
I0524 06:51:43.963605 11835 sgd_solver.cpp:112] Iteration 45740, lr = 0.1
I0524 06:51:51.450950 11835 solver.cpp:239] Iteration 45750 (1.3356 iter/s, 7.48725s/10 iters), loss = 6.88892
I0524 06:51:51.451009 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88892 (* 1 = 6.88892 loss)
I0524 06:51:51.451076 11835 sgd_solver.cpp:112] Iteration 45750, lr = 0.1
I0524 06:51:59.406641 11835 solver.cpp:239] Iteration 45760 (1.25702 iter/s, 7.95533s/10 iters), loss = 6.38912
I0524 06:51:59.406705 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38912 (* 1 = 6.38912 loss)
I0524 06:51:59.406723 11835 sgd_solver.cpp:112] Iteration 45760, lr = 0.1
I0524 06:52:06.575497 11835 solver.cpp:239] Iteration 45770 (1.39499 iter/s, 7.16852s/10 iters), loss = 6.02405
I0524 06:52:06.575567 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02405 (* 1 = 6.02405 loss)
I0524 06:52:06.575765 11835 sgd_solver.cpp:112] Iteration 45770, lr = 0.1
I0524 06:52:13.364922 11835 solver.cpp:239] Iteration 45780 (1.47295 iter/s, 6.78912s/10 iters), loss = 6.61132
I0524 06:52:13.365124 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61132 (* 1 = 6.61132 loss)
I0524 06:52:13.365159 11835 sgd_solver.cpp:112] Iteration 45780, lr = 0.1
I0524 06:52:20.021754 11835 solver.cpp:239] Iteration 45790 (1.5028 iter/s, 6.65423s/10 iters), loss = 6.10543
I0524 06:52:20.021826 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10543 (* 1 = 6.10543 loss)
I0524 06:52:20.021852 11835 sgd_solver.cpp:112] Iteration 45790, lr = 0.1
I0524 06:52:26.096290 11835 solver.cpp:239] Iteration 45800 (1.6463 iter/s, 6.07421s/10 iters), loss = 6.9057
I0524 06:52:26.096333 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9057 (* 1 = 6.9057 loss)
I0524 06:52:26.096345 11835 sgd_solver.cpp:112] Iteration 45800, lr = 0.1
I0524 06:52:35.100087 11835 solver.cpp:239] Iteration 45810 (1.11076 iter/s, 9.00284s/10 iters), loss = 5.78327
I0524 06:52:35.100142 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78327 (* 1 = 5.78327 loss)
I0524 06:52:35.100231 11835 sgd_solver.cpp:112] Iteration 45810, lr = 0.1
I0524 06:52:41.573940 11835 solver.cpp:239] Iteration 45820 (1.54475 iter/s, 6.47354s/10 iters), loss = 5.7591
I0524 06:52:41.573997 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7591 (* 1 = 5.7591 loss)
I0524 06:52:41.574304 11835 sgd_solver.cpp:112] Iteration 45820, lr = 0.1
I0524 06:52:48.552162 11835 solver.cpp:239] Iteration 45830 (1.4331 iter/s, 6.97789s/10 iters), loss = 5.54433
I0524 06:52:48.552459 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.54433 (* 1 = 5.54433 loss)
I0524 06:52:48.599309 11835 sgd_solver.cpp:112] Iteration 45830, lr = 0.1
I0524 06:52:54.964903 11835 solver.cpp:239] Iteration 45840 (1.55952 iter/s, 6.41224s/10 iters), loss = 7.16879
I0524 06:52:54.964957 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.16879 (* 1 = 7.16879 loss)
I0524 06:52:54.965049 11835 sgd_solver.cpp:112] Iteration 45840, lr = 0.1
I0524 06:53:01.671583 11835 solver.cpp:239] Iteration 45850 (1.49112 iter/s, 6.70637s/10 iters), loss = 7.10306
I0524 06:53:01.671644 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10306 (* 1 = 7.10306 loss)
I0524 06:53:01.671752 11835 sgd_solver.cpp:112] Iteration 45850, lr = 0.1
I0524 06:53:09.886618 11835 solver.cpp:239] Iteration 45860 (1.21734 iter/s, 8.21465s/10 iters), loss = 5.53281
I0524 06:53:09.886679 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.53281 (* 1 = 5.53281 loss)
I0524 06:53:09.896769 11835 sgd_solver.cpp:112] Iteration 45860, lr = 0.1
I0524 06:53:15.825954 11835 solver.cpp:239] Iteration 45870 (1.68377 iter/s, 5.93905s/10 iters), loss = 6.2879
I0524 06:53:15.825997 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2879 (* 1 = 6.2879 loss)
I0524 06:53:15.826011 11835 sgd_solver.cpp:112] Iteration 45870, lr = 0.1
I0524 06:53:24.738675 11835 solver.cpp:239] Iteration 45880 (1.12204 iter/s, 8.91233s/10 iters), loss = 5.99553
I0524 06:53:24.738914 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99553 (* 1 = 5.99553 loss)
I0524 06:53:24.761505 11835 sgd_solver.cpp:112] Iteration 45880, lr = 0.1
I0524 06:53:31.268908 11835 solver.cpp:239] Iteration 45890 (1.53145 iter/s, 6.52974s/10 iters), loss = 6.50188
I0524 06:53:31.268965 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50188 (* 1 = 6.50188 loss)
I0524 06:53:31.554993 11835 sgd_solver.cpp:112] Iteration 45890, lr = 0.1
I0524 06:53:39.752225 11835 solver.cpp:239] Iteration 45900 (1.17884 iter/s, 8.48294s/10 iters), loss = 5.87319
I0524 06:53:39.752277 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87319 (* 1 = 5.87319 loss)
I0524 06:53:39.862664 11835 sgd_solver.cpp:112] Iteration 45900, lr = 0.1
I0524 06:53:45.666492 11835 solver.cpp:239] Iteration 45910 (1.6909 iter/s, 5.914s/10 iters), loss = 6.41653
I0524 06:53:45.666532 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41653 (* 1 = 6.41653 loss)
I0524 06:53:45.666544 11835 sgd_solver.cpp:112] Iteration 45910, lr = 0.1
I0524 06:53:52.218250 11835 solver.cpp:239] Iteration 45920 (1.5269 iter/s, 6.54923s/10 iters), loss = 5.74167
I0524 06:53:52.218299 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74167 (* 1 = 5.74167 loss)
I0524 06:53:52.218314 11835 sgd_solver.cpp:112] Iteration 45920, lr = 0.1
I0524 06:53:58.706285 11835 solver.cpp:239] Iteration 45930 (1.54143 iter/s, 6.48749s/10 iters), loss = 6.60202
I0524 06:53:58.706485 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60202 (* 1 = 6.60202 loss)
I0524 06:53:58.706519 11835 sgd_solver.cpp:112] Iteration 45930, lr = 0.1
I0524 06:54:05.924235 11835 solver.cpp:239] Iteration 45940 (1.38566 iter/s, 7.21675s/10 iters), loss = 5.65459
I0524 06:54:05.924281 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.65459 (* 1 = 5.65459 loss)
I0524 06:54:05.937124 11835 sgd_solver.cpp:112] Iteration 45940, lr = 0.1
I0524 06:54:12.687357 11835 solver.cpp:239] Iteration 45950 (1.47868 iter/s, 6.7628s/10 iters), loss = 5.83621
I0524 06:54:12.687424 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83621 (* 1 = 5.83621 loss)
I0524 06:54:12.687487 11835 sgd_solver.cpp:112] Iteration 45950, lr = 0.1
I0524 06:54:18.900880 11835 solver.cpp:239] Iteration 45960 (1.60947 iter/s, 6.21322s/10 iters), loss = 6.5311
I0524 06:54:18.900930 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5311 (* 1 = 6.5311 loss)
I0524 06:54:18.901170 11835 sgd_solver.cpp:112] Iteration 45960, lr = 0.1
I0524 06:54:25.575778 11835 solver.cpp:239] Iteration 45970 (1.49822 iter/s, 6.6746s/10 iters), loss = 5.399
I0524 06:54:25.575817 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.399 (* 1 = 5.399 loss)
I0524 06:54:25.697165 11835 sgd_solver.cpp:112] Iteration 45970, lr = 0.1
I0524 06:54:31.874719 11835 solver.cpp:239] Iteration 45980 (1.58765 iter/s, 6.29864s/10 iters), loss = 5.12476
I0524 06:54:31.875005 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.12476 (* 1 = 5.12476 loss)
I0524 06:54:31.875139 11835 sgd_solver.cpp:112] Iteration 45980, lr = 0.1
I0524 06:54:39.940904 11835 solver.cpp:239] Iteration 45990 (1.23983 iter/s, 8.06563s/10 iters), loss = 5.44798
I0524 06:54:39.940958 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.44798 (* 1 = 5.44798 loss)
I0524 06:54:39.941071 11835 sgd_solver.cpp:112] Iteration 45990, lr = 0.1
I0524 06:54:46.666018 11835 solver.cpp:239] Iteration 46000 (1.48703 iter/s, 6.72481s/10 iters), loss = 5.59158
I0524 06:54:46.666064 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.59158 (* 1 = 5.59158 loss)
I0524 06:54:46.666201 11835 sgd_solver.cpp:112] Iteration 46000, lr = 0.1
I0524 06:54:52.495534 11835 solver.cpp:239] Iteration 46010 (1.71549 iter/s, 5.82924s/10 iters), loss = 7.07307
I0524 06:54:52.495579 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.07307 (* 1 = 7.07307 loss)
I0524 06:54:52.495800 11835 sgd_solver.cpp:112] Iteration 46010, lr = 0.1
I0524 06:55:00.439404 11835 solver.cpp:239] Iteration 46020 (1.25889 iter/s, 7.9435s/10 iters), loss = 5.76939
I0524 06:55:00.439462 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.76939 (* 1 = 5.76939 loss)
I0524 06:55:00.439745 11835 sgd_solver.cpp:112] Iteration 46020, lr = 0.1
I0524 06:55:06.446312 11835 solver.cpp:239] Iteration 46030 (1.66483 iter/s, 6.00661s/10 iters), loss = 6.40765
I0524 06:55:06.446483 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40765 (* 1 = 6.40765 loss)
I0524 06:55:06.446503 11835 sgd_solver.cpp:112] Iteration 46030, lr = 0.1
I0524 06:55:15.428194 11835 solver.cpp:239] Iteration 46040 (1.11368 iter/s, 8.97923s/10 iters), loss = 5.32725
I0524 06:55:15.428249 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.32725 (* 1 = 5.32725 loss)
I0524 06:55:15.594573 11835 sgd_solver.cpp:112] Iteration 46040, lr = 0.1
I0524 06:55:25.012295 11835 solver.cpp:239] Iteration 46050 (1.04344 iter/s, 9.58367s/10 iters), loss = 6.2043
I0524 06:55:25.012351 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2043 (* 1 = 6.2043 loss)
I0524 06:55:25.012538 11835 sgd_solver.cpp:112] Iteration 46050, lr = 0.1
I0524 06:55:31.673382 11835 solver.cpp:239] Iteration 46060 (1.50133 iter/s, 6.66077s/10 iters), loss = 6.10716
I0524 06:55:31.673444 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10716 (* 1 = 6.10716 loss)
I0524 06:55:31.673686 11835 sgd_solver.cpp:112] Iteration 46060, lr = 0.1
I0524 06:55:38.334084 11835 solver.cpp:239] Iteration 46070 (1.50142 iter/s, 6.66038s/10 iters), loss = 5.73174
I0524 06:55:38.334260 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73174 (* 1 = 5.73174 loss)
I0524 06:55:38.662593 11835 sgd_solver.cpp:112] Iteration 46070, lr = 0.1
I0524 06:55:45.094086 11835 solver.cpp:239] Iteration 46080 (1.47938 iter/s, 6.75957s/10 iters), loss = 6.50751
I0524 06:55:45.094135 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50751 (* 1 = 6.50751 loss)
I0524 06:55:45.835841 11835 sgd_solver.cpp:112] Iteration 46080, lr = 0.1
I0524 06:55:53.213688 11835 solver.cpp:239] Iteration 46090 (1.23164 iter/s, 8.11924s/10 iters), loss = 6.14344
I0524 06:55:53.213747 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14344 (* 1 = 6.14344 loss)
I0524 06:55:53.213874 11835 sgd_solver.cpp:112] Iteration 46090, lr = 0.1
I0524 06:56:00.676702 11835 solver.cpp:239] Iteration 46100 (1.34 iter/s, 7.46267s/10 iters), loss = 6.04614
I0524 06:56:00.676749 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04614 (* 1 = 6.04614 loss)
I0524 06:56:00.676996 11835 sgd_solver.cpp:112] Iteration 46100, lr = 0.1
I0524 06:56:08.311651 11835 solver.cpp:239] Iteration 46110 (1.30983 iter/s, 7.6346s/10 iters), loss = 6.03534
I0524 06:56:08.311710 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03534 (* 1 = 6.03534 loss)
I0524 06:56:09.057938 11835 sgd_solver.cpp:112] Iteration 46110, lr = 0.1
I0524 06:56:15.524593 11835 solver.cpp:239] Iteration 46120 (1.38646 iter/s, 7.2126s/10 iters), loss = 5.52756
I0524 06:56:15.524648 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.52756 (* 1 = 5.52756 loss)
I0524 06:56:15.524710 11835 sgd_solver.cpp:112] Iteration 46120, lr = 0.1
I0524 06:56:22.739372 11835 solver.cpp:239] Iteration 46130 (1.38611 iter/s, 7.21445s/10 iters), loss = 5.70225
I0524 06:56:22.739415 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.70225 (* 1 = 5.70225 loss)
I0524 06:56:23.117738 11835 sgd_solver.cpp:112] Iteration 46130, lr = 0.1
I0524 06:56:31.079808 11835 solver.cpp:239] Iteration 46140 (1.19903 iter/s, 8.34005s/10 iters), loss = 5.71785
I0524 06:56:31.079869 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71785 (* 1 = 5.71785 loss)
I0524 06:56:31.790361 11835 sgd_solver.cpp:112] Iteration 46140, lr = 0.1
I0524 06:56:38.617960 11835 solver.cpp:239] Iteration 46150 (1.32665 iter/s, 7.53777s/10 iters), loss = 6.43758
I0524 06:56:38.618024 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43758 (* 1 = 6.43758 loss)
I0524 06:56:38.640605 11835 sgd_solver.cpp:112] Iteration 46150, lr = 0.1
I0524 06:56:46.216936 11835 solver.cpp:239] Iteration 46160 (1.31603 iter/s, 7.59862s/10 iters), loss = 6.92946
I0524 06:56:46.217290 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92946 (* 1 = 6.92946 loss)
I0524 06:56:46.217367 11835 sgd_solver.cpp:112] Iteration 46160, lr = 0.1
I0524 06:56:55.475755 11835 solver.cpp:239] Iteration 46170 (1.08012 iter/s, 9.25825s/10 iters), loss = 5.49587
I0524 06:56:55.475806 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.49587 (* 1 = 5.49587 loss)
I0524 06:56:55.475864 11835 sgd_solver.cpp:112] Iteration 46170, lr = 0.1
I0524 06:57:02.267351 11835 solver.cpp:239] Iteration 46180 (1.47248 iter/s, 6.79127s/10 iters), loss = 5.35398
I0524 06:57:02.267426 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.35398 (* 1 = 5.35398 loss)
I0524 06:57:02.268189 11835 sgd_solver.cpp:112] Iteration 46180, lr = 0.1
I0524 06:57:12.088719 11835 solver.cpp:239] Iteration 46190 (1.01824 iter/s, 9.8209s/10 iters), loss = 5.3452
I0524 06:57:12.088799 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.3452 (* 1 = 5.3452 loss)
I0524 06:57:12.751385 11835 sgd_solver.cpp:112] Iteration 46190, lr = 0.1
I0524 06:57:18.647915 11835 solver.cpp:239] Iteration 46200 (1.52465 iter/s, 6.55888s/10 iters), loss = 6.50833
I0524 06:57:18.648154 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50833 (* 1 = 6.50833 loss)
I0524 06:57:19.285663 11835 sgd_solver.cpp:112] Iteration 46200, lr = 0.1
I0524 06:57:26.779456 11835 solver.cpp:239] Iteration 46210 (1.22986 iter/s, 8.13102s/10 iters), loss = 5.43372
I0524 06:57:26.779546 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43372 (* 1 = 5.43372 loss)
I0524 06:57:26.779846 11835 sgd_solver.cpp:112] Iteration 46210, lr = 0.1
I0524 06:57:34.198736 11835 solver.cpp:239] Iteration 46220 (1.34791 iter/s, 7.41891s/10 iters), loss = 5.33796
I0524 06:57:34.198801 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.33796 (* 1 = 5.33796 loss)
I0524 06:57:34.198982 11835 sgd_solver.cpp:112] Iteration 46220, lr = 0.1
I0524 06:57:41.061576 11835 solver.cpp:239] Iteration 46230 (1.45719 iter/s, 6.86252s/10 iters), loss = 6.40136
I0524 06:57:41.061627 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40136 (* 1 = 6.40136 loss)
I0524 06:57:41.061849 11835 sgd_solver.cpp:112] Iteration 46230, lr = 0.1
I0524 06:57:47.915575 11835 solver.cpp:239] Iteration 46240 (1.45907 iter/s, 6.85366s/10 iters), loss = 6.82906
I0524 06:57:47.915678 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.82906 (* 1 = 6.82906 loss)
I0524 06:57:47.915710 11835 sgd_solver.cpp:112] Iteration 46240, lr = 0.1
I0524 06:57:54.716338 11835 solver.cpp:239] Iteration 46250 (1.47049 iter/s, 6.80044s/10 iters), loss = 5.82573
I0524 06:57:54.716601 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82573 (* 1 = 5.82573 loss)
I0524 06:57:54.716637 11835 sgd_solver.cpp:112] Iteration 46250, lr = 0.1
I0524 06:58:00.516768 11835 solver.cpp:239] Iteration 46260 (1.72416 iter/s, 5.79993s/10 iters), loss = 6.00675
I0524 06:58:00.516829 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00675 (* 1 = 6.00675 loss)
I0524 06:58:00.516898 11835 sgd_solver.cpp:112] Iteration 46260, lr = 0.1
I0524 06:58:09.374894 11835 solver.cpp:239] Iteration 46270 (1.12896 iter/s, 8.85773s/10 iters), loss = 6.60181
I0524 06:58:09.374956 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60181 (* 1 = 6.60181 loss)
I0524 06:58:09.375098 11835 sgd_solver.cpp:112] Iteration 46270, lr = 0.1
I0524 06:58:17.469627 11835 solver.cpp:239] Iteration 46280 (1.23543 iter/s, 8.09437s/10 iters), loss = 5.64524
I0524 06:58:17.469693 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.64524 (* 1 = 5.64524 loss)
I0524 06:58:17.514686 11835 sgd_solver.cpp:112] Iteration 46280, lr = 0.1
I0524 06:58:25.127121 11835 solver.cpp:239] Iteration 46290 (1.30597 iter/s, 7.65714s/10 iters), loss = 6.70212
I0524 06:58:25.127398 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70212 (* 1 = 6.70212 loss)
I0524 06:58:25.708323 11835 sgd_solver.cpp:112] Iteration 46290, lr = 0.1
I0524 06:58:31.729127 11835 solver.cpp:239] Iteration 46300 (1.51481 iter/s, 6.6015s/10 iters), loss = 6.76276
I0524 06:58:31.729187 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76276 (* 1 = 6.76276 loss)
I0524 06:58:31.729300 11835 sgd_solver.cpp:112] Iteration 46300, lr = 0.1
I0524 06:58:37.582315 11835 solver.cpp:239] Iteration 46310 (1.70855 iter/s, 5.8529s/10 iters), loss = 6.32456
I0524 06:58:37.582361 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32456 (* 1 = 6.32456 loss)
I0524 06:58:37.582731 11835 sgd_solver.cpp:112] Iteration 46310, lr = 0.1
I0524 06:58:46.752257 11835 solver.cpp:239] Iteration 46320 (1.09057 iter/s, 9.16954s/10 iters), loss = 6.67539
I0524 06:58:46.752310 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67539 (* 1 = 6.67539 loss)
I0524 06:58:48.036336 11835 sgd_solver.cpp:112] Iteration 46320, lr = 0.1
I0524 06:58:54.562737 11835 solver.cpp:239] Iteration 46330 (1.28039 iter/s, 7.81012s/10 iters), loss = 6.68098
I0524 06:58:54.562782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.68098 (* 1 = 6.68098 loss)
I0524 06:58:54.562887 11835 sgd_solver.cpp:112] Iteration 46330, lr = 0.1
I0524 06:59:02.764138 11835 solver.cpp:239] Iteration 46340 (1.21936 iter/s, 8.20103s/10 iters), loss = 5.83548
I0524 06:59:02.764322 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83548 (* 1 = 5.83548 loss)
I0524 06:59:02.985255 11835 sgd_solver.cpp:112] Iteration 46340, lr = 0.1
I0524 06:59:10.843778 11835 solver.cpp:239] Iteration 46350 (1.23775 iter/s, 8.07916s/10 iters), loss = 6.73505
I0524 06:59:10.843842 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73505 (* 1 = 6.73505 loss)
I0524 06:59:10.843895 11835 sgd_solver.cpp:112] Iteration 46350, lr = 0.1
I0524 06:59:18.224392 11835 solver.cpp:239] Iteration 46360 (1.35497 iter/s, 7.38026s/10 iters), loss = 6.41858
I0524 06:59:18.224457 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41858 (* 1 = 6.41858 loss)
I0524 06:59:18.224810 11835 sgd_solver.cpp:112] Iteration 46360, lr = 0.1
I0524 06:59:24.251905 11835 solver.cpp:239] Iteration 46370 (1.65914 iter/s, 6.02721s/10 iters), loss = 6.00263
I0524 06:59:24.251962 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00263 (* 1 = 6.00263 loss)
I0524 06:59:24.254712 11835 sgd_solver.cpp:112] Iteration 46370, lr = 0.1
I0524 06:59:31.628041 11835 solver.cpp:239] Iteration 46380 (1.35579 iter/s, 7.37579s/10 iters), loss = 5.8998
I0524 06:59:31.628089 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8998 (* 1 = 5.8998 loss)
I0524 06:59:31.628104 11835 sgd_solver.cpp:112] Iteration 46380, lr = 0.1
I0524 06:59:38.165320 11835 solver.cpp:239] Iteration 46390 (1.52977 iter/s, 6.53693s/10 iters), loss = 5.384
I0524 06:59:38.165604 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.384 (* 1 = 5.384 loss)
I0524 06:59:39.032153 11835 sgd_solver.cpp:112] Iteration 46390, lr = 0.1
I0524 06:59:48.769290 11835 solver.cpp:239] Iteration 46400 (0.943102 iter/s, 10.6033s/10 iters), loss = 5.83049
I0524 06:59:48.769346 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83049 (* 1 = 5.83049 loss)
I0524 06:59:48.769363 11835 sgd_solver.cpp:112] Iteration 46400, lr = 0.1
I0524 06:59:54.796005 11835 solver.cpp:239] Iteration 46410 (1.65966 iter/s, 6.02533s/10 iters), loss = 6.02698
I0524 06:59:54.796064 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02698 (* 1 = 6.02698 loss)
I0524 06:59:55.311074 11835 sgd_solver.cpp:112] Iteration 46410, lr = 0.1
I0524 07:00:04.171152 11835 solver.cpp:239] Iteration 46420 (1.0667 iter/s, 9.37473s/10 iters), loss = 6.43114
I0524 07:00:04.171201 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43114 (* 1 = 6.43114 loss)
I0524 07:00:04.470183 11835 sgd_solver.cpp:112] Iteration 46420, lr = 0.1
I0524 07:00:14.402679 11835 solver.cpp:239] Iteration 46430 (0.977413 iter/s, 10.2311s/10 iters), loss = 6.28866
I0524 07:00:14.402966 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.28866 (* 1 = 6.28866 loss)
I0524 07:00:15.117630 11835 sgd_solver.cpp:112] Iteration 46430, lr = 0.1
I0524 07:00:21.736665 11835 solver.cpp:239] Iteration 46440 (1.36361 iter/s, 7.33345s/10 iters), loss = 5.43093
I0524 07:00:21.736708 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43093 (* 1 = 5.43093 loss)
I0524 07:00:21.736721 11835 sgd_solver.cpp:112] Iteration 46440, lr = 0.1
I0524 07:00:29.254217 11835 solver.cpp:239] Iteration 46450 (1.33029 iter/s, 7.51713s/10 iters), loss = 5.63658
I0524 07:00:29.254274 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63658 (* 1 = 5.63658 loss)
I0524 07:00:29.254351 11835 sgd_solver.cpp:112] Iteration 46450, lr = 0.1
I0524 07:00:36.124923 11835 solver.cpp:239] Iteration 46460 (1.45552 iter/s, 6.87038s/10 iters), loss = 6.96814
I0524 07:00:36.124972 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96814 (* 1 = 6.96814 loss)
I0524 07:00:36.125227 11835 sgd_solver.cpp:112] Iteration 46460, lr = 0.1
I0524 07:00:42.439115 11835 solver.cpp:239] Iteration 46470 (1.58381 iter/s, 6.3139s/10 iters), loss = 5.00256
I0524 07:00:42.439168 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.00256 (* 1 = 5.00256 loss)
I0524 07:00:42.439183 11835 sgd_solver.cpp:112] Iteration 46470, lr = 0.1
I0524 07:00:48.902173 11835 solver.cpp:239] Iteration 46480 (1.54748 iter/s, 6.46213s/10 iters), loss = 6.06011
I0524 07:00:48.902395 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06011 (* 1 = 6.06011 loss)
I0524 07:00:48.902436 11835 sgd_solver.cpp:112] Iteration 46480, lr = 0.1
I0524 07:00:56.920008 11835 solver.cpp:239] Iteration 46490 (1.2473 iter/s, 8.01731s/10 iters), loss = 6.60417
I0524 07:00:56.920058 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60417 (* 1 = 6.60417 loss)
I0524 07:00:56.920106 11835 sgd_solver.cpp:112] Iteration 46490, lr = 0.1
I0524 07:01:04.752651 11835 solver.cpp:239] Iteration 46500 (1.27677 iter/s, 7.83229s/10 iters), loss = 5.25459
I0524 07:01:04.752708 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.25459 (* 1 = 5.25459 loss)
I0524 07:01:04.752928 11835 sgd_solver.cpp:112] Iteration 46500, lr = 0.1
I0524 07:01:11.236482 11835 solver.cpp:239] Iteration 46510 (1.54237 iter/s, 6.48351s/10 iters), loss = 6.76477
I0524 07:01:11.236541 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76477 (* 1 = 6.76477 loss)
I0524 07:01:11.784561 11835 sgd_solver.cpp:112] Iteration 46510, lr = 0.1
I0524 07:01:17.832798 11835 solver.cpp:239] Iteration 46520 (1.51607 iter/s, 6.59601s/10 iters), loss = 6.18343
I0524 07:01:17.832839 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18343 (* 1 = 6.18343 loss)
I0524 07:01:17.858898 11835 sgd_solver.cpp:112] Iteration 46520, lr = 0.1
I0524 07:01:24.769549 11835 solver.cpp:239] Iteration 46530 (1.44166 iter/s, 6.93643s/10 iters), loss = 6.10138
I0524 07:01:24.769771 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10138 (* 1 = 6.10138 loss)
I0524 07:01:25.389588 11835 sgd_solver.cpp:112] Iteration 46530, lr = 0.1
I0524 07:01:32.183225 11835 solver.cpp:239] Iteration 46540 (1.34895 iter/s, 7.41317s/10 iters), loss = 6.73563
I0524 07:01:32.183282 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73563 (* 1 = 6.73563 loss)
I0524 07:01:32.183382 11835 sgd_solver.cpp:112] Iteration 46540, lr = 0.1
I0524 07:01:39.034806 11835 solver.cpp:239] Iteration 46550 (1.45959 iter/s, 6.85125s/10 iters), loss = 6.58947
I0524 07:01:39.034860 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58947 (* 1 = 6.58947 loss)
I0524 07:01:39.035117 11835 sgd_solver.cpp:112] Iteration 46550, lr = 0.1
I0524 07:01:45.107179 11835 solver.cpp:239] Iteration 46560 (1.64688 iter/s, 6.07209s/10 iters), loss = 7.59153
I0524 07:01:45.107235 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.59153 (* 1 = 7.59153 loss)
I0524 07:01:45.107352 11835 sgd_solver.cpp:112] Iteration 46560, lr = 0.1
I0524 07:01:51.064967 11835 solver.cpp:239] Iteration 46570 (1.67856 iter/s, 5.95749s/10 iters), loss = 5.42356
I0524 07:01:51.065017 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.42356 (* 1 = 5.42356 loss)
I0524 07:01:51.065382 11835 sgd_solver.cpp:112] Iteration 46570, lr = 0.1
I0524 07:01:57.349472 11835 solver.cpp:239] Iteration 46580 (1.59129 iter/s, 6.28421s/10 iters), loss = 6.84844
I0524 07:01:57.349721 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84844 (* 1 = 6.84844 loss)
I0524 07:01:57.349774 11835 sgd_solver.cpp:112] Iteration 46580, lr = 0.1
I0524 07:02:03.903048 11835 solver.cpp:239] Iteration 46590 (1.526 iter/s, 6.55308s/10 iters), loss = 6.48888
I0524 07:02:03.903082 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48888 (* 1 = 6.48888 loss)
I0524 07:02:03.903167 11835 sgd_solver.cpp:112] Iteration 46590, lr = 0.1
I0524 07:02:12.487682 11835 solver.cpp:239] Iteration 46600 (1.16492 iter/s, 8.58425s/10 iters), loss = 6.26472
I0524 07:02:12.487741 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26472 (* 1 = 6.26472 loss)
I0524 07:02:12.487934 11835 sgd_solver.cpp:112] Iteration 46600, lr = 0.1
I0524 07:02:19.219612 11835 solver.cpp:239] Iteration 46610 (1.48553 iter/s, 6.7316s/10 iters), loss = 5.80382
I0524 07:02:19.219673 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80382 (* 1 = 5.80382 loss)
I0524 07:02:19.219820 11835 sgd_solver.cpp:112] Iteration 46610, lr = 0.1
I0524 07:02:25.018455 11835 solver.cpp:239] Iteration 46620 (1.72457 iter/s, 5.79856s/10 iters), loss = 6.12378
I0524 07:02:25.018509 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12378 (* 1 = 6.12378 loss)
I0524 07:02:25.018527 11835 sgd_solver.cpp:112] Iteration 46620, lr = 0.1
I0524 07:02:34.184530 11835 solver.cpp:239] Iteration 46630 (1.09103 iter/s, 9.16567s/10 iters), loss = 6.61605
I0524 07:02:34.190753 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61605 (* 1 = 6.61605 loss)
I0524 07:02:34.897126 11835 sgd_solver.cpp:112] Iteration 46630, lr = 0.1
I0524 07:02:43.928434 11835 solver.cpp:239] Iteration 46640 (1.02716 iter/s, 9.73559s/10 iters), loss = 5.28637
I0524 07:02:43.928488 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.28637 (* 1 = 5.28637 loss)
I0524 07:02:43.951298 11835 sgd_solver.cpp:112] Iteration 46640, lr = 0.1
I0524 07:02:51.600575 11835 solver.cpp:239] Iteration 46650 (1.30348 iter/s, 7.67178s/10 iters), loss = 5.81954
I0524 07:02:51.600633 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81954 (* 1 = 5.81954 loss)
I0524 07:02:51.600684 11835 sgd_solver.cpp:112] Iteration 46650, lr = 0.1
I0524 07:02:59.875409 11835 solver.cpp:239] Iteration 46660 (1.20854 iter/s, 8.27447s/10 iters), loss = 6.1385
I0524 07:02:59.875454 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1385 (* 1 = 6.1385 loss)
I0524 07:02:59.875783 11835 sgd_solver.cpp:112] Iteration 46660, lr = 0.1
I0524 07:03:07.840194 11835 solver.cpp:239] Iteration 46670 (1.25558 iter/s, 7.96443s/10 iters), loss = 6.28836
I0524 07:03:07.840427 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.28836 (* 1 = 6.28836 loss)
I0524 07:03:07.840468 11835 sgd_solver.cpp:112] Iteration 46670, lr = 0.1
I0524 07:03:14.855372 11835 solver.cpp:239] Iteration 46680 (1.42568 iter/s, 7.01422s/10 iters), loss = 6.45995
I0524 07:03:14.855428 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45995 (* 1 = 6.45995 loss)
I0524 07:03:15.401754 11835 sgd_solver.cpp:112] Iteration 46680, lr = 0.1
I0524 07:03:21.740890 11835 solver.cpp:239] Iteration 46690 (1.45239 iter/s, 6.8852s/10 iters), loss = 5.34405
I0524 07:03:21.740933 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.34405 (* 1 = 5.34405 loss)
I0524 07:03:21.740947 11835 sgd_solver.cpp:112] Iteration 46690, lr = 0.1
I0524 07:03:27.671115 11835 solver.cpp:239] Iteration 46700 (1.68695 iter/s, 5.92786s/10 iters), loss = 6.9166
I0524 07:03:27.671175 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9166 (* 1 = 6.9166 loss)
I0524 07:03:27.671700 11835 sgd_solver.cpp:112] Iteration 46700, lr = 0.1
I0524 07:03:35.946539 11835 solver.cpp:239] Iteration 46710 (1.20845 iter/s, 8.27505s/10 iters), loss = 6.15491
I0524 07:03:35.946589 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15491 (* 1 = 6.15491 loss)
I0524 07:03:35.946604 11835 sgd_solver.cpp:112] Iteration 46710, lr = 0.1
I0524 07:03:44.259598 11835 solver.cpp:239] Iteration 46720 (1.20298 iter/s, 8.31269s/10 iters), loss = 6.43907
I0524 07:03:44.259840 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43907 (* 1 = 6.43907 loss)
I0524 07:03:44.259887 11835 sgd_solver.cpp:112] Iteration 46720, lr = 0.1
I0524 07:03:50.001400 11835 solver.cpp:239] Iteration 46730 (1.74174 iter/s, 5.74138s/10 iters), loss = 6.42926
I0524 07:03:50.001440 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42926 (* 1 = 6.42926 loss)
I0524 07:03:50.001453 11835 sgd_solver.cpp:112] Iteration 46730, lr = 0.1
I0524 07:03:58.121958 11835 solver.cpp:239] Iteration 46740 (1.23151 iter/s, 8.12013s/10 iters), loss = 5.84479
I0524 07:03:58.122014 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84479 (* 1 = 5.84479 loss)
I0524 07:03:58.122098 11835 sgd_solver.cpp:112] Iteration 46740, lr = 0.1
I0524 07:04:06.032980 11835 solver.cpp:239] Iteration 46750 (1.26412 iter/s, 7.91066s/10 iters), loss = 6.01228
I0524 07:04:06.033041 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01228 (* 1 = 6.01228 loss)
I0524 07:04:06.242475 11835 sgd_solver.cpp:112] Iteration 46750, lr = 0.1
I0524 07:04:13.183526 11835 solver.cpp:239] Iteration 46760 (1.39856 iter/s, 7.15019s/10 iters), loss = 5.06761
I0524 07:04:13.183609 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.06761 (* 1 = 5.06761 loss)
I0524 07:04:14.421671 11835 sgd_solver.cpp:112] Iteration 46760, lr = 0.1
I0524 07:04:22.554462 11835 solver.cpp:239] Iteration 46770 (1.06718 iter/s, 9.3705s/10 iters), loss = 6.30246
I0524 07:04:22.554523 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30246 (* 1 = 6.30246 loss)
I0524 07:04:22.705487 11835 sgd_solver.cpp:112] Iteration 46770, lr = 0.1
I0524 07:04:30.215824 11835 solver.cpp:239] Iteration 46780 (1.30531 iter/s, 7.661s/10 iters), loss = 6.2264
I0524 07:04:30.215914 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2264 (* 1 = 6.2264 loss)
I0524 07:04:30.216163 11835 sgd_solver.cpp:112] Iteration 46780, lr = 0.1
I0524 07:04:37.017160 11835 solver.cpp:239] Iteration 46790 (1.47039 iter/s, 6.80094s/10 iters), loss = 6.48194
I0524 07:04:37.017305 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48194 (* 1 = 6.48194 loss)
I0524 07:04:37.017380 11835 sgd_solver.cpp:112] Iteration 46790, lr = 0.1
I0524 07:04:44.075029 11835 solver.cpp:239] Iteration 46800 (1.41693 iter/s, 7.05752s/10 iters), loss = 5.49802
I0524 07:04:44.075085 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.49802 (* 1 = 5.49802 loss)
I0524 07:04:44.075145 11835 sgd_solver.cpp:112] Iteration 46800, lr = 0.1
I0524 07:04:52.467428 11835 solver.cpp:239] Iteration 46810 (1.19161 iter/s, 8.39198s/10 iters), loss = 5.62435
I0524 07:04:52.467764 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.62435 (* 1 = 5.62435 loss)
I0524 07:04:52.515128 11835 sgd_solver.cpp:112] Iteration 46810, lr = 0.1
I0524 07:05:01.636838 11835 solver.cpp:239] Iteration 46820 (1.09065 iter/s, 9.1688s/10 iters), loss = 5.86177
I0524 07:05:01.636904 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86177 (* 1 = 5.86177 loss)
I0524 07:05:01.636965 11835 sgd_solver.cpp:112] Iteration 46820, lr = 0.1
I0524 07:05:10.347407 11835 solver.cpp:239] Iteration 46830 (1.14809 iter/s, 8.71012s/10 iters), loss = 6.73988
I0524 07:05:10.347537 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73988 (* 1 = 6.73988 loss)
I0524 07:05:10.537289 11835 sgd_solver.cpp:112] Iteration 46830, lr = 0.1
I0524 07:05:16.352174 11835 solver.cpp:239] Iteration 46840 (1.66543 iter/s, 6.00446s/10 iters), loss = 5.33778
I0524 07:05:16.352224 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.33778 (* 1 = 5.33778 loss)
I0524 07:05:16.460965 11835 sgd_solver.cpp:112] Iteration 46840, lr = 0.1
I0524 07:05:29.769587 11835 solver.cpp:239] Iteration 46850 (0.745334 iter/s, 13.4168s/10 iters), loss = 5.24843
I0524 07:05:29.769853 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.24843 (* 1 = 5.24843 loss)
I0524 07:05:29.770005 11835 sgd_solver.cpp:112] Iteration 46850, lr = 0.1
I0524 07:05:36.459708 11835 solver.cpp:239] Iteration 46860 (1.49485 iter/s, 6.68964s/10 iters), loss = 5.76018
I0524 07:05:36.459774 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.76018 (* 1 = 5.76018 loss)
I0524 07:05:36.459880 11835 sgd_solver.cpp:112] Iteration 46860, lr = 0.1
I0524 07:05:43.313207 11835 solver.cpp:239] Iteration 46870 (1.45918 iter/s, 6.85317s/10 iters), loss = 6.2572
I0524 07:05:43.313261 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2572 (* 1 = 6.2572 loss)
I0524 07:05:43.313374 11835 sgd_solver.cpp:112] Iteration 46870, lr = 0.1
I0524 07:05:51.318298 11835 solver.cpp:239] Iteration 46880 (1.24926 iter/s, 8.00472s/10 iters), loss = 6.56087
I0524 07:05:51.318393 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56087 (* 1 = 6.56087 loss)
I0524 07:05:51.318686 11835 sgd_solver.cpp:112] Iteration 46880, lr = 0.1
I0524 07:05:58.302942 11835 solver.cpp:239] Iteration 46890 (1.43178 iter/s, 6.9843s/10 iters), loss = 6.53047
I0524 07:05:58.302989 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53047 (* 1 = 6.53047 loss)
I0524 07:05:58.303045 11835 sgd_solver.cpp:112] Iteration 46890, lr = 0.1
I0524 07:06:06.631811 11835 solver.cpp:239] Iteration 46900 (1.2007 iter/s, 8.32848s/10 iters), loss = 7.58003
I0524 07:06:06.632113 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.58003 (* 1 = 7.58003 loss)
I0524 07:06:07.074127 11835 sgd_solver.cpp:112] Iteration 46900, lr = 0.1
I0524 07:06:15.210887 11835 solver.cpp:239] Iteration 46910 (1.16571 iter/s, 8.57848s/10 iters), loss = 6.69721
I0524 07:06:15.210937 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69721 (* 1 = 6.69721 loss)
I0524 07:06:15.928741 11835 sgd_solver.cpp:112] Iteration 46910, lr = 0.1
I0524 07:06:24.416972 11835 solver.cpp:239] Iteration 46920 (1.08629 iter/s, 9.20568s/10 iters), loss = 6.34243
I0524 07:06:24.417083 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34243 (* 1 = 6.34243 loss)
I0524 07:06:24.417116 11835 sgd_solver.cpp:112] Iteration 46920, lr = 0.1
I0524 07:06:31.749408 11835 solver.cpp:239] Iteration 46930 (1.36387 iter/s, 7.3321s/10 iters), loss = 6.06119
I0524 07:06:31.749474 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06119 (* 1 = 6.06119 loss)
I0524 07:06:31.749733 11835 sgd_solver.cpp:112] Iteration 46930, lr = 0.1
I0524 07:06:39.080914 11835 solver.cpp:239] Iteration 46940 (1.36404 iter/s, 7.33117s/10 iters), loss = 6.00361
I0524 07:06:39.081068 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00361 (* 1 = 6.00361 loss)
I0524 07:06:39.081086 11835 sgd_solver.cpp:112] Iteration 46940, lr = 0.1
I0524 07:06:45.551628 11835 solver.cpp:239] Iteration 46950 (1.54566 iter/s, 6.46973s/10 iters), loss = 6.64939
I0524 07:06:45.551681 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64939 (* 1 = 6.64939 loss)
I0524 07:06:45.551695 11835 sgd_solver.cpp:112] Iteration 46950, lr = 0.1
I0524 07:06:52.168859 11835 solver.cpp:239] Iteration 46960 (1.51128 iter/s, 6.61692s/10 iters), loss = 6.05779
I0524 07:06:52.168927 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05779 (* 1 = 6.05779 loss)
I0524 07:06:52.272224 11835 sgd_solver.cpp:112] Iteration 46960, lr = 0.1
I0524 07:06:58.546630 11835 solver.cpp:239] Iteration 46970 (1.56802 iter/s, 6.37747s/10 iters), loss = 6.3639
I0524 07:06:58.546681 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3639 (* 1 = 6.3639 loss)
I0524 07:06:58.546762 11835 sgd_solver.cpp:112] Iteration 46970, lr = 0.1
I0524 07:07:05.292619 11835 solver.cpp:239] Iteration 46980 (1.48243 iter/s, 6.74567s/10 iters), loss = 6.24779
I0524 07:07:05.292670 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24779 (* 1 = 6.24779 loss)
I0524 07:07:05.292685 11835 sgd_solver.cpp:112] Iteration 46980, lr = 0.1
I0524 07:07:13.884310 11835 solver.cpp:239] Iteration 46990 (1.16397 iter/s, 8.5913s/10 iters), loss = 6.26497
I0524 07:07:13.884559 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26497 (* 1 = 6.26497 loss)
I0524 07:07:13.884598 11835 sgd_solver.cpp:112] Iteration 46990, lr = 0.1
I0524 07:07:20.495596 11835 solver.cpp:239] Iteration 47000 (1.51267 iter/s, 6.61081s/10 iters), loss = 6.47003
I0524 07:07:20.495645 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.47003 (* 1 = 6.47003 loss)
I0524 07:07:20.514096 11835 sgd_solver.cpp:112] Iteration 47000, lr = 0.1
I0524 07:07:28.181313 11835 solver.cpp:239] Iteration 47010 (1.30117 iter/s, 7.68536s/10 iters), loss = 6.73875
I0524 07:07:28.181365 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73875 (* 1 = 6.73875 loss)
I0524 07:07:28.181710 11835 sgd_solver.cpp:112] Iteration 47010, lr = 0.1
I0524 07:07:34.015135 11835 solver.cpp:239] Iteration 47020 (1.71422 iter/s, 5.83354s/10 iters), loss = 7.36115
I0524 07:07:34.015179 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.36115 (* 1 = 7.36115 loss)
I0524 07:07:34.015282 11835 sgd_solver.cpp:112] Iteration 47020, lr = 0.1
I0524 07:07:40.463230 11835 solver.cpp:239] Iteration 47030 (1.55092 iter/s, 6.4478s/10 iters), loss = 6.52697
I0524 07:07:40.463279 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52697 (* 1 = 6.52697 loss)
I0524 07:07:40.515386 11835 sgd_solver.cpp:112] Iteration 47030, lr = 0.1
I0524 07:07:48.163909 11835 solver.cpp:239] Iteration 47040 (1.29865 iter/s, 7.70032s/10 iters), loss = 5.81829
I0524 07:07:48.164229 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81829 (* 1 = 5.81829 loss)
I0524 07:07:48.164283 11835 sgd_solver.cpp:112] Iteration 47040, lr = 0.1
I0524 07:07:54.173132 11835 solver.cpp:239] Iteration 47050 (1.66427 iter/s, 6.00864s/10 iters), loss = 6.60945
I0524 07:07:54.173233 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60945 (* 1 = 6.60945 loss)
I0524 07:07:54.195698 11835 sgd_solver.cpp:112] Iteration 47050, lr = 0.1
I0524 07:08:02.850339 11835 solver.cpp:239] Iteration 47060 (1.1525 iter/s, 8.67678s/10 iters), loss = 6.71982
I0524 07:08:02.850394 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71982 (* 1 = 6.71982 loss)
I0524 07:08:03.531468 11835 sgd_solver.cpp:112] Iteration 47060, lr = 0.1
I0524 07:08:11.150163 11835 solver.cpp:239] Iteration 47070 (1.2049 iter/s, 8.29944s/10 iters), loss = 6.27832
I0524 07:08:11.150214 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27832 (* 1 = 6.27832 loss)
I0524 07:08:11.150449 11835 sgd_solver.cpp:112] Iteration 47070, lr = 0.1
I0524 07:08:17.357364 11835 solver.cpp:239] Iteration 47080 (1.61111 iter/s, 6.20691s/10 iters), loss = 6.09961
I0524 07:08:17.357409 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09961 (* 1 = 6.09961 loss)
I0524 07:08:17.357424 11835 sgd_solver.cpp:112] Iteration 47080, lr = 0.1
I0524 07:08:24.422730 11835 solver.cpp:239] Iteration 47090 (1.41542 iter/s, 7.06502s/10 iters), loss = 7.18246
I0524 07:08:24.422998 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.18246 (* 1 = 7.18246 loss)
I0524 07:08:24.427063 11835 sgd_solver.cpp:112] Iteration 47090, lr = 0.1
I0524 07:08:30.025207 11835 solver.cpp:239] Iteration 47100 (1.78507 iter/s, 5.60202s/10 iters), loss = 6.38838
I0524 07:08:30.025260 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38838 (* 1 = 6.38838 loss)
I0524 07:08:30.025360 11835 sgd_solver.cpp:112] Iteration 47100, lr = 0.1
I0524 07:08:37.873582 11835 solver.cpp:239] Iteration 47110 (1.27421 iter/s, 7.84803s/10 iters), loss = 6.23236
I0524 07:08:37.873622 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23236 (* 1 = 6.23236 loss)
I0524 07:08:38.733742 11835 sgd_solver.cpp:112] Iteration 47110, lr = 0.1
I0524 07:08:44.823197 11835 solver.cpp:239] Iteration 47120 (1.43899 iter/s, 6.9493s/10 iters), loss = 6.12096
I0524 07:08:44.823251 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12096 (* 1 = 6.12096 loss)
I0524 07:08:44.823299 11835 sgd_solver.cpp:112] Iteration 47120, lr = 0.1
I0524 07:08:53.328630 11835 solver.cpp:239] Iteration 47130 (1.17577 iter/s, 8.50506s/10 iters), loss = 5.53706
I0524 07:08:53.328681 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.53706 (* 1 = 5.53706 loss)
I0524 07:08:53.328696 11835 sgd_solver.cpp:112] Iteration 47130, lr = 0.1
I0524 07:09:02.019260 11835 solver.cpp:239] Iteration 47140 (1.15086 iter/s, 8.68914s/10 iters), loss = 6.87062
I0524 07:09:02.019541 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87062 (* 1 = 6.87062 loss)
I0524 07:09:02.019594 11835 sgd_solver.cpp:112] Iteration 47140, lr = 0.1
I0524 07:09:09.601651 11835 solver.cpp:239] Iteration 47150 (1.31894 iter/s, 7.58187s/10 iters), loss = 6.20988
I0524 07:09:09.601701 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20988 (* 1 = 6.20988 loss)
I0524 07:09:09.602001 11835 sgd_solver.cpp:112] Iteration 47150, lr = 0.1
I0524 07:09:16.042795 11835 solver.cpp:239] Iteration 47160 (1.55259 iter/s, 6.44083s/10 iters), loss = 7.01845
I0524 07:09:16.042855 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01845 (* 1 = 7.01845 loss)
I0524 07:09:16.043046 11835 sgd_solver.cpp:112] Iteration 47160, lr = 0.1
I0524 07:09:21.954982 11835 solver.cpp:239] Iteration 47170 (1.6915 iter/s, 5.91191s/10 iters), loss = 5.31668
I0524 07:09:21.955024 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.31668 (* 1 = 5.31668 loss)
I0524 07:09:21.955035 11835 sgd_solver.cpp:112] Iteration 47170, lr = 0.1
I0524 07:09:29.919040 11835 solver.cpp:239] Iteration 47180 (1.25572 iter/s, 7.96357s/10 iters), loss = 5.16624
I0524 07:09:29.919087 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.16624 (* 1 = 5.16624 loss)
I0524 07:09:29.919219 11835 sgd_solver.cpp:112] Iteration 47180, lr = 0.1
I0524 07:09:35.735070 11835 solver.cpp:239] Iteration 47190 (1.71947 iter/s, 5.81576s/10 iters), loss = 6.78335
I0524 07:09:35.735357 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78335 (* 1 = 6.78335 loss)
I0524 07:09:36.568015 11835 sgd_solver.cpp:112] Iteration 47190, lr = 0.1
I0524 07:09:44.050834 11835 solver.cpp:239] Iteration 47200 (1.20262 iter/s, 8.31518s/10 iters), loss = 6.29636
I0524 07:09:44.050917 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29636 (* 1 = 6.29636 loss)
I0524 07:09:44.056212 11835 sgd_solver.cpp:112] Iteration 47200, lr = 0.1
I0524 07:09:52.230284 11835 solver.cpp:239] Iteration 47210 (1.22263 iter/s, 8.17907s/10 iters), loss = 5.92488
I0524 07:09:52.230337 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92488 (* 1 = 5.92488 loss)
I0524 07:09:52.230355 11835 sgd_solver.cpp:112] Iteration 47210, lr = 0.1
I0524 07:09:59.086309 11835 solver.cpp:239] Iteration 47220 (1.45875 iter/s, 6.85517s/10 iters), loss = 5.99689
I0524 07:09:59.086354 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99689 (* 1 = 5.99689 loss)
I0524 07:09:59.086575 11835 sgd_solver.cpp:112] Iteration 47220, lr = 0.1
I0524 07:10:06.373458 11835 solver.cpp:239] Iteration 47230 (1.37234 iter/s, 7.2868s/10 iters), loss = 5.85829
I0524 07:10:06.373721 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85829 (* 1 = 5.85829 loss)
I0524 07:10:07.357125 11835 sgd_solver.cpp:112] Iteration 47230, lr = 0.1
I0524 07:10:13.263520 11835 solver.cpp:239] Iteration 47240 (1.45147 iter/s, 6.88956s/10 iters), loss = 6.50406
I0524 07:10:13.263581 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50406 (* 1 = 6.50406 loss)
I0524 07:10:13.263599 11835 sgd_solver.cpp:112] Iteration 47240, lr = 0.1
I0524 07:10:20.050426 11835 solver.cpp:239] Iteration 47250 (1.4735 iter/s, 6.78658s/10 iters), loss = 5.28184
I0524 07:10:20.050489 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.28184 (* 1 = 5.28184 loss)
I0524 07:10:20.050562 11835 sgd_solver.cpp:112] Iteration 47250, lr = 0.1
I0524 07:10:29.166144 11835 solver.cpp:239] Iteration 47260 (1.09705 iter/s, 9.11531s/10 iters), loss = 6.42296
I0524 07:10:29.166193 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42296 (* 1 = 6.42296 loss)
I0524 07:10:29.166545 11835 sgd_solver.cpp:112] Iteration 47260, lr = 0.1
I0524 07:10:36.308748 11835 solver.cpp:239] Iteration 47270 (1.40012 iter/s, 7.14226s/10 iters), loss = 6.40036
I0524 07:10:36.308815 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40036 (* 1 = 6.40036 loss)
I0524 07:10:36.656780 11835 sgd_solver.cpp:112] Iteration 47270, lr = 0.1
I0524 07:10:42.921274 11835 solver.cpp:239] Iteration 47280 (1.51235 iter/s, 6.61221s/10 iters), loss = 6.80297
I0524 07:10:42.921340 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80297 (* 1 = 6.80297 loss)
I0524 07:10:42.921437 11835 sgd_solver.cpp:112] Iteration 47280, lr = 0.1
I0524 07:10:50.381477 11835 solver.cpp:239] Iteration 47290 (1.34051 iter/s, 7.45986s/10 iters), loss = 6.67413
I0524 07:10:50.381533 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67413 (* 1 = 6.67413 loss)
I0524 07:10:50.381600 11835 sgd_solver.cpp:112] Iteration 47290, lr = 0.1
I0524 07:10:59.426028 11835 solver.cpp:239] Iteration 47300 (1.10569 iter/s, 9.04414s/10 iters), loss = 6.27893
I0524 07:10:59.426084 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27893 (* 1 = 6.27893 loss)
I0524 07:10:59.426434 11835 sgd_solver.cpp:112] Iteration 47300, lr = 0.1
I0524 07:11:08.185428 11835 solver.cpp:239] Iteration 47310 (1.14168 iter/s, 8.759s/10 iters), loss = 4.38471
I0524 07:11:08.185614 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.38471 (* 1 = 4.38471 loss)
I0524 07:11:08.185652 11835 sgd_solver.cpp:112] Iteration 47310, lr = 0.1
I0524 07:11:14.243767 11835 solver.cpp:239] Iteration 47320 (1.65073 iter/s, 6.05794s/10 iters), loss = 6.34576
I0524 07:11:14.243816 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34576 (* 1 = 6.34576 loss)
I0524 07:11:15.329520 11835 sgd_solver.cpp:112] Iteration 47320, lr = 0.1
I0524 07:11:23.348316 11835 solver.cpp:239] Iteration 47330 (1.0984 iter/s, 9.10415s/10 iters), loss = 5.67394
I0524 07:11:23.348368 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67394 (* 1 = 5.67394 loss)
I0524 07:11:23.348497 11835 sgd_solver.cpp:112] Iteration 47330, lr = 0.1
I0524 07:11:31.899715 11835 solver.cpp:239] Iteration 47340 (1.16945 iter/s, 8.55102s/10 iters), loss = 5.92698
I0524 07:11:31.899771 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92698 (* 1 = 5.92698 loss)
I0524 07:11:31.899798 11835 sgd_solver.cpp:112] Iteration 47340, lr = 0.1
I0524 07:11:40.587930 11835 solver.cpp:239] Iteration 47350 (1.15104 iter/s, 8.68781s/10 iters), loss = 6.09398
I0524 07:11:40.588076 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09398 (* 1 = 6.09398 loss)
I0524 07:11:40.588100 11835 sgd_solver.cpp:112] Iteration 47350, lr = 0.1
I0524 07:11:49.280721 11835 solver.cpp:239] Iteration 47360 (1.15044 iter/s, 8.6923s/10 iters), loss = 6.44633
I0524 07:11:49.280797 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44633 (* 1 = 6.44633 loss)
I0524 07:11:49.281020 11835 sgd_solver.cpp:112] Iteration 47360, lr = 0.1
I0524 07:11:56.149147 11835 solver.cpp:239] Iteration 47370 (1.45601 iter/s, 6.86809s/10 iters), loss = 6.74765
I0524 07:11:56.149197 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74765 (* 1 = 6.74765 loss)
I0524 07:11:56.149211 11835 sgd_solver.cpp:112] Iteration 47370, lr = 0.1
I0524 07:12:03.279950 11835 solver.cpp:239] Iteration 47380 (1.40245 iter/s, 7.13038s/10 iters), loss = 5.50439
I0524 07:12:03.280009 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.50439 (* 1 = 5.50439 loss)
I0524 07:12:03.650614 11835 sgd_solver.cpp:112] Iteration 47380, lr = 0.1
I0524 07:12:10.729899 11835 solver.cpp:239] Iteration 47390 (1.34235 iter/s, 7.4496s/10 iters), loss = 5.48751
I0524 07:12:10.730154 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.48751 (* 1 = 5.48751 loss)
I0524 07:12:10.730204 11835 sgd_solver.cpp:112] Iteration 47390, lr = 0.1
I0524 07:12:18.269740 11835 solver.cpp:239] Iteration 47400 (1.32643 iter/s, 7.53905s/10 iters), loss = 6.71115
I0524 07:12:18.269891 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71115 (* 1 = 6.71115 loss)
I0524 07:12:18.269934 11835 sgd_solver.cpp:112] Iteration 47400, lr = 0.1
I0524 07:12:25.099300 11835 solver.cpp:239] Iteration 47410 (1.46429 iter/s, 6.82924s/10 iters), loss = 5.46657
I0524 07:12:25.099345 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.46657 (* 1 = 5.46657 loss)
I0524 07:12:25.699054 11835 sgd_solver.cpp:112] Iteration 47410, lr = 0.1
I0524 07:12:32.629859 11835 solver.cpp:239] Iteration 47420 (1.32799 iter/s, 7.53021s/10 iters), loss = 7.56447
I0524 07:12:32.629915 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.56447 (* 1 = 7.56447 loss)
I0524 07:12:32.635463 11835 sgd_solver.cpp:112] Iteration 47420, lr = 0.1
I0524 07:12:40.055441 11835 solver.cpp:239] Iteration 47430 (1.34676 iter/s, 7.42523s/10 iters), loss = 5.40006
I0524 07:12:40.055548 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.40006 (* 1 = 5.40006 loss)
I0524 07:12:40.055598 11835 sgd_solver.cpp:112] Iteration 47430, lr = 0.1
I0524 07:12:47.508435 11835 solver.cpp:239] Iteration 47440 (1.3418 iter/s, 7.45266s/10 iters), loss = 6.81149
I0524 07:12:47.508663 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81149 (* 1 = 6.81149 loss)
I0524 07:12:48.034214 11835 sgd_solver.cpp:112] Iteration 47440, lr = 0.1
I0524 07:12:53.862375 11835 solver.cpp:239] Iteration 47450 (1.57394 iter/s, 6.35347s/10 iters), loss = 5.677
I0524 07:12:53.862465 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.677 (* 1 = 5.677 loss)
I0524 07:12:54.417400 11835 sgd_solver.cpp:112] Iteration 47450, lr = 0.1
I0524 07:13:00.103729 11835 solver.cpp:239] Iteration 47460 (1.6023 iter/s, 6.24104s/10 iters), loss = 5.34356
I0524 07:13:00.103768 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.34356 (* 1 = 5.34356 loss)
I0524 07:13:00.330515 11835 sgd_solver.cpp:112] Iteration 47460, lr = 0.1
I0524 07:13:06.518172 11835 solver.cpp:239] Iteration 47470 (1.55906 iter/s, 6.41414s/10 iters), loss = 5.37776
I0524 07:13:06.518247 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.37776 (* 1 = 5.37776 loss)
I0524 07:13:06.518460 11835 sgd_solver.cpp:112] Iteration 47470, lr = 0.1
I0524 07:13:15.715673 11835 solver.cpp:239] Iteration 47480 (1.0873 iter/s, 9.1971s/10 iters), loss = 6.16121
I0524 07:13:15.715714 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16121 (* 1 = 6.16121 loss)
I0524 07:13:15.715940 11835 sgd_solver.cpp:112] Iteration 47480, lr = 0.1
I0524 07:13:24.205991 11835 solver.cpp:239] Iteration 47490 (1.17786 iter/s, 8.48994s/10 iters), loss = 6.08565
I0524 07:13:24.206187 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08565 (* 1 = 6.08565 loss)
I0524 07:13:24.206256 11835 sgd_solver.cpp:112] Iteration 47490, lr = 0.1
I0524 07:13:33.446230 11835 solver.cpp:239] Iteration 47500 (1.08229 iter/s, 9.23969s/10 iters), loss = 6.76493
I0524 07:13:33.446285 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76493 (* 1 = 6.76493 loss)
I0524 07:13:33.446543 11835 sgd_solver.cpp:112] Iteration 47500, lr = 0.1
I0524 07:13:39.817730 11835 solver.cpp:239] Iteration 47510 (1.56956 iter/s, 6.3712s/10 iters), loss = 6.83915
I0524 07:13:39.817785 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83915 (* 1 = 6.83915 loss)
I0524 07:13:39.818270 11835 sgd_solver.cpp:112] Iteration 47510, lr = 0.1
I0524 07:13:48.329784 11835 solver.cpp:239] Iteration 47520 (1.17486 iter/s, 8.51166s/10 iters), loss = 6.46461
I0524 07:13:48.329845 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46461 (* 1 = 6.46461 loss)
I0524 07:13:48.330206 11835 sgd_solver.cpp:112] Iteration 47520, lr = 0.1
I0524 07:13:56.377954 11835 solver.cpp:239] Iteration 47530 (1.24258 iter/s, 8.04779s/10 iters), loss = 6.04018
I0524 07:13:56.378114 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04018 (* 1 = 6.04018 loss)
I0524 07:13:56.378195 11835 sgd_solver.cpp:112] Iteration 47530, lr = 0.1
I0524 07:14:04.139796 11835 solver.cpp:239] Iteration 47540 (1.28843 iter/s, 7.7614s/10 iters), loss = 6.14604
I0524 07:14:04.139843 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14604 (* 1 = 6.14604 loss)
I0524 07:14:04.139858 11835 sgd_solver.cpp:112] Iteration 47540, lr = 0.1
I0524 07:14:10.009591 11835 solver.cpp:239] Iteration 47550 (1.70372 iter/s, 5.86952s/10 iters), loss = 5.63816
I0524 07:14:10.009630 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63816 (* 1 = 5.63816 loss)
I0524 07:14:10.047870 11835 sgd_solver.cpp:112] Iteration 47550, lr = 0.1
I0524 07:14:17.929846 11835 solver.cpp:239] Iteration 47560 (1.26264 iter/s, 7.91989s/10 iters), loss = 6.29017
I0524 07:14:17.929901 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29017 (* 1 = 6.29017 loss)
I0524 07:14:17.930063 11835 sgd_solver.cpp:112] Iteration 47560, lr = 0.1
I0524 07:14:26.592361 11835 solver.cpp:239] Iteration 47570 (1.15445 iter/s, 8.66213s/10 iters), loss = 7.28149
I0524 07:14:26.592571 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.28149 (* 1 = 7.28149 loss)
I0524 07:14:26.592654 11835 sgd_solver.cpp:112] Iteration 47570, lr = 0.1
I0524 07:14:34.627995 11835 solver.cpp:239] Iteration 47580 (1.24453 iter/s, 8.03513s/10 iters), loss = 5.50269
I0524 07:14:34.628041 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.50269 (* 1 = 5.50269 loss)
I0524 07:14:34.628056 11835 sgd_solver.cpp:112] Iteration 47580, lr = 0.1
I0524 07:14:43.602284 11835 solver.cpp:239] Iteration 47590 (1.11434 iter/s, 8.97389s/10 iters), loss = 5.59451
I0524 07:14:43.602340 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.59451 (* 1 = 5.59451 loss)
I0524 07:14:44.899158 11835 sgd_solver.cpp:112] Iteration 47590, lr = 0.1
I0524 07:14:52.873993 11835 solver.cpp:239] Iteration 47600 (1.0786 iter/s, 9.2713s/10 iters), loss = 5.53597
I0524 07:14:52.874052 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.53597 (* 1 = 5.53597 loss)
I0524 07:14:52.874151 11835 sgd_solver.cpp:112] Iteration 47600, lr = 0.1
I0524 07:15:00.746840 11835 solver.cpp:239] Iteration 47610 (1.27025 iter/s, 7.87249s/10 iters), loss = 6.32715
I0524 07:15:00.747016 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32715 (* 1 = 6.32715 loss)
I0524 07:15:01.204000 11835 sgd_solver.cpp:112] Iteration 47610, lr = 0.1
I0524 07:15:08.345376 11835 solver.cpp:239] Iteration 47620 (1.31613 iter/s, 7.59805s/10 iters), loss = 6.50819
I0524 07:15:08.345448 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50819 (* 1 = 6.50819 loss)
I0524 07:15:08.830971 11835 sgd_solver.cpp:112] Iteration 47620, lr = 0.1
I0524 07:15:15.358119 11835 solver.cpp:239] Iteration 47630 (1.42605 iter/s, 7.0124s/10 iters), loss = 5.68843
I0524 07:15:15.358170 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68843 (* 1 = 5.68843 loss)
I0524 07:15:15.484957 11835 sgd_solver.cpp:112] Iteration 47630, lr = 0.1
I0524 07:15:24.370594 11835 solver.cpp:239] Iteration 47640 (1.10962 iter/s, 9.01207s/10 iters), loss = 6.63659
I0524 07:15:24.370647 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63659 (* 1 = 6.63659 loss)
I0524 07:15:24.370781 11835 sgd_solver.cpp:112] Iteration 47640, lr = 0.1
I0524 07:15:30.445533 11835 solver.cpp:239] Iteration 47650 (1.64619 iter/s, 6.07464s/10 iters), loss = 5.83417
I0524 07:15:30.445598 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83417 (* 1 = 5.83417 loss)
I0524 07:15:30.451701 11835 sgd_solver.cpp:112] Iteration 47650, lr = 0.1
I0524 07:15:37.723381 11835 solver.cpp:239] Iteration 47660 (1.3741 iter/s, 7.27749s/10 iters), loss = 5.89873
I0524 07:15:37.723559 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89873 (* 1 = 5.89873 loss)
I0524 07:15:37.762660 11835 sgd_solver.cpp:112] Iteration 47660, lr = 0.1
I0524 07:15:44.004576 11835 solver.cpp:239] Iteration 47670 (1.59216 iter/s, 6.28077s/10 iters), loss = 6.42106
I0524 07:15:44.004633 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42106 (* 1 = 6.42106 loss)
I0524 07:15:44.004833 11835 sgd_solver.cpp:112] Iteration 47670, lr = 0.1
I0524 07:15:52.215868 11835 solver.cpp:239] Iteration 47680 (1.21789 iter/s, 8.2109s/10 iters), loss = 5.8638
I0524 07:15:52.215924 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8638 (* 1 = 5.8638 loss)
I0524 07:15:52.216038 11835 sgd_solver.cpp:112] Iteration 47680, lr = 0.1
I0524 07:16:00.430364 11835 solver.cpp:239] Iteration 47690 (1.21742 iter/s, 8.21412s/10 iters), loss = 5.74514
I0524 07:16:00.430418 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74514 (* 1 = 5.74514 loss)
I0524 07:16:00.430546 11835 sgd_solver.cpp:112] Iteration 47690, lr = 0.1
I0524 07:16:06.901504 11835 solver.cpp:239] Iteration 47700 (1.54539 iter/s, 6.47084s/10 iters), loss = 5.79108
I0524 07:16:06.901554 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79108 (* 1 = 5.79108 loss)
I0524 07:16:06.901657 11835 sgd_solver.cpp:112] Iteration 47700, lr = 0.1
I0524 07:16:13.181370 11835 solver.cpp:239] Iteration 47710 (1.59246 iter/s, 6.27958s/10 iters), loss = 5.78409
I0524 07:16:13.181605 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78409 (* 1 = 5.78409 loss)
I0524 07:16:13.181650 11835 sgd_solver.cpp:112] Iteration 47710, lr = 0.1
I0524 07:16:21.464197 11835 solver.cpp:239] Iteration 47720 (1.20741 iter/s, 8.2822s/10 iters), loss = 6.00656
I0524 07:16:21.464256 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00656 (* 1 = 6.00656 loss)
I0524 07:16:21.464366 11835 sgd_solver.cpp:112] Iteration 47720, lr = 0.1
I0524 07:16:30.223925 11835 solver.cpp:239] Iteration 47730 (1.14164 iter/s, 8.75932s/10 iters), loss = 5.41646
I0524 07:16:30.223973 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.41646 (* 1 = 5.41646 loss)
I0524 07:16:30.240530 11835 sgd_solver.cpp:112] Iteration 47730, lr = 0.1
I0524 07:16:37.302147 11835 solver.cpp:239] Iteration 47740 (1.41285 iter/s, 7.0779s/10 iters), loss = 6.48719
I0524 07:16:37.302201 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48719 (* 1 = 6.48719 loss)
I0524 07:16:37.614116 11835 sgd_solver.cpp:112] Iteration 47740, lr = 0.1
I0524 07:16:44.175606 11835 solver.cpp:239] Iteration 47750 (1.45494 iter/s, 6.87315s/10 iters), loss = 7.29423
I0524 07:16:44.175803 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.29423 (* 1 = 7.29423 loss)
I0524 07:16:44.175930 11835 sgd_solver.cpp:112] Iteration 47750, lr = 0.1
I0524 07:16:51.003074 11835 solver.cpp:239] Iteration 47760 (1.46478 iter/s, 6.82698s/10 iters), loss = 5.57513
I0524 07:16:51.003160 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.57513 (* 1 = 5.57513 loss)
I0524 07:16:51.003342 11835 sgd_solver.cpp:112] Iteration 47760, lr = 0.1
I0524 07:16:57.246948 11835 solver.cpp:239] Iteration 47770 (1.60165 iter/s, 6.24356s/10 iters), loss = 5.9041
I0524 07:16:57.246991 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9041 (* 1 = 5.9041 loss)
I0524 07:16:57.247006 11835 sgd_solver.cpp:112] Iteration 47770, lr = 0.1
I0524 07:17:06.737462 11835 solver.cpp:239] Iteration 47780 (1.05373 iter/s, 9.4901s/10 iters), loss = 6.34202
I0524 07:17:06.737517 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34202 (* 1 = 6.34202 loss)
I0524 07:17:06.737573 11835 sgd_solver.cpp:112] Iteration 47780, lr = 0.1
I0524 07:17:12.908706 11835 solver.cpp:239] Iteration 47790 (1.6205 iter/s, 6.17094s/10 iters), loss = 6.33275
I0524 07:17:12.908797 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33275 (* 1 = 6.33275 loss)
I0524 07:17:12.908828 11835 sgd_solver.cpp:112] Iteration 47790, lr = 0.1
I0524 07:17:19.596143 11835 solver.cpp:239] Iteration 47800 (1.4959 iter/s, 6.68495s/10 iters), loss = 6.90366
I0524 07:17:19.596390 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90366 (* 1 = 6.90366 loss)
I0524 07:17:19.686575 11835 sgd_solver.cpp:112] Iteration 47800, lr = 0.1
I0524 07:17:25.787382 11835 solver.cpp:239] Iteration 47810 (1.61531 iter/s, 6.19078s/10 iters), loss = 6.44719
I0524 07:17:25.787431 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44719 (* 1 = 6.44719 loss)
I0524 07:17:25.787559 11835 sgd_solver.cpp:112] Iteration 47810, lr = 0.1
I0524 07:17:31.411700 11835 solver.cpp:239] Iteration 47820 (1.77808 iter/s, 5.62404s/10 iters), loss = 6.94674
I0524 07:17:31.411751 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.94674 (* 1 = 6.94674 loss)
I0524 07:17:31.478898 11835 sgd_solver.cpp:112] Iteration 47820, lr = 0.1
I0524 07:17:37.929738 11835 solver.cpp:239] Iteration 47830 (1.53427 iter/s, 6.51774s/10 iters), loss = 6.37288
I0524 07:17:37.929785 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37288 (* 1 = 6.37288 loss)
I0524 07:17:38.316777 11835 sgd_solver.cpp:112] Iteration 47830, lr = 0.1
I0524 07:17:44.828523 11835 solver.cpp:239] Iteration 47840 (1.4496 iter/s, 6.89847s/10 iters), loss = 6.65021
I0524 07:17:44.828573 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65021 (* 1 = 6.65021 loss)
I0524 07:17:44.828619 11835 sgd_solver.cpp:112] Iteration 47840, lr = 0.1
I0524 07:17:52.030813 11835 solver.cpp:239] Iteration 47850 (1.38851 iter/s, 7.20195s/10 iters), loss = 5.7407
I0524 07:17:52.031059 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7407 (* 1 = 5.7407 loss)
I0524 07:17:52.031097 11835 sgd_solver.cpp:112] Iteration 47850, lr = 0.1
I0524 07:17:58.114076 11835 solver.cpp:239] Iteration 47860 (1.64398 iter/s, 6.0828s/10 iters), loss = 6.882
I0524 07:17:58.114122 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.882 (* 1 = 6.882 loss)
I0524 07:17:58.114137 11835 sgd_solver.cpp:112] Iteration 47860, lr = 0.1
I0524 07:18:06.773444 11835 solver.cpp:239] Iteration 47870 (1.15488 iter/s, 8.65891s/10 iters), loss = 4.41097
I0524 07:18:06.773496 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.41097 (* 1 = 4.41097 loss)
I0524 07:18:06.773527 11835 sgd_solver.cpp:112] Iteration 47870, lr = 0.1
I0524 07:18:13.417381 11835 solver.cpp:239] Iteration 47880 (1.5052 iter/s, 6.64362s/10 iters), loss = 5.5545
I0524 07:18:13.417425 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.5545 (* 1 = 5.5545 loss)
I0524 07:18:13.417649 11835 sgd_solver.cpp:112] Iteration 47880, lr = 0.1
I0524 07:18:21.843405 11835 solver.cpp:239] Iteration 47890 (1.18685 iter/s, 8.42565s/10 iters), loss = 6.91505
I0524 07:18:21.843461 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.91505 (* 1 = 6.91505 loss)
I0524 07:18:21.843705 11835 sgd_solver.cpp:112] Iteration 47890, lr = 0.1
I0524 07:18:28.777796 11835 solver.cpp:239] Iteration 47900 (1.44216 iter/s, 6.93405s/10 iters), loss = 6.69317
I0524 07:18:28.778070 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69317 (* 1 = 6.69317 loss)
I0524 07:18:28.778100 11835 sgd_solver.cpp:112] Iteration 47900, lr = 0.1
I0524 07:18:36.369865 11835 solver.cpp:239] Iteration 47910 (1.31732 iter/s, 7.59115s/10 iters), loss = 5.52048
I0524 07:18:36.369927 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.52048 (* 1 = 5.52048 loss)
I0524 07:18:36.945941 11835 sgd_solver.cpp:112] Iteration 47910, lr = 0.1
I0524 07:18:44.406321 11835 solver.cpp:239] Iteration 47920 (1.24439 iter/s, 8.03608s/10 iters), loss = 6.12181
I0524 07:18:44.406370 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12181 (* 1 = 6.12181 loss)
I0524 07:18:44.406610 11835 sgd_solver.cpp:112] Iteration 47920, lr = 0.1
I0524 07:18:53.266773 11835 solver.cpp:239] Iteration 47930 (1.12866 iter/s, 8.86005s/10 iters), loss = 5.54592
I0524 07:18:53.266854 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.54592 (* 1 = 5.54592 loss)
I0524 07:18:53.279100 11835 sgd_solver.cpp:112] Iteration 47930, lr = 0.1
I0524 07:19:01.534869 11835 solver.cpp:239] Iteration 47940 (1.20953 iter/s, 8.26769s/10 iters), loss = 6.26761
I0524 07:19:01.535111 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26761 (* 1 = 6.26761 loss)
I0524 07:19:01.535279 11835 sgd_solver.cpp:112] Iteration 47940, lr = 0.1
I0524 07:19:09.072331 11835 solver.cpp:239] Iteration 47950 (1.32679 iter/s, 7.53696s/10 iters), loss = 6.62635
I0524 07:19:09.072368 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62635 (* 1 = 6.62635 loss)
I0524 07:19:09.896216 11835 sgd_solver.cpp:112] Iteration 47950, lr = 0.1
I0524 07:19:16.072737 11835 solver.cpp:239] Iteration 47960 (1.42855 iter/s, 7.00009s/10 iters), loss = 6.58966
I0524 07:19:16.072788 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58966 (* 1 = 6.58966 loss)
I0524 07:19:16.072975 11835 sgd_solver.cpp:112] Iteration 47960, lr = 0.1
I0524 07:19:22.214735 11835 solver.cpp:239] Iteration 47970 (1.62821 iter/s, 6.1417s/10 iters), loss = 6.70663
I0524 07:19:22.214787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70663 (* 1 = 6.70663 loss)
I0524 07:19:22.214848 11835 sgd_solver.cpp:112] Iteration 47970, lr = 0.1
I0524 07:19:29.945521 11835 solver.cpp:239] Iteration 47980 (1.29359 iter/s, 7.73044s/10 iters), loss = 5.42248
I0524 07:19:29.945559 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.42248 (* 1 = 5.42248 loss)
I0524 07:19:30.838155 11835 sgd_solver.cpp:112] Iteration 47980, lr = 0.1
I0524 07:19:39.604171 11835 solver.cpp:239] Iteration 47990 (1.03539 iter/s, 9.65822s/10 iters), loss = 5.97291
I0524 07:19:39.604394 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97291 (* 1 = 5.97291 loss)
I0524 07:19:39.604527 11835 sgd_solver.cpp:112] Iteration 47990, lr = 0.1
I0524 07:19:46.191138 11835 solver.cpp:239] Iteration 48000 (1.51826 iter/s, 6.58649s/10 iters), loss = 6.0125
I0524 07:19:46.191190 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0125 (* 1 = 6.0125 loss)
I0524 07:19:46.191591 11835 sgd_solver.cpp:112] Iteration 48000, lr = 0.1
I0524 07:19:52.887753 11835 solver.cpp:239] Iteration 48010 (1.49336 iter/s, 6.69631s/10 iters), loss = 6.89127
I0524 07:19:52.887799 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89127 (* 1 = 6.89127 loss)
I0524 07:19:53.049511 11835 sgd_solver.cpp:112] Iteration 48010, lr = 0.1
I0524 07:19:58.942579 11835 solver.cpp:239] Iteration 48020 (1.65167 iter/s, 6.05448s/10 iters), loss = 5.81286
I0524 07:19:58.942678 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81286 (* 1 = 5.81286 loss)
I0524 07:19:58.942752 11835 sgd_solver.cpp:112] Iteration 48020, lr = 0.1
I0524 07:20:05.256134 11835 solver.cpp:239] Iteration 48030 (1.58402 iter/s, 6.31305s/10 iters), loss = 6.23527
I0524 07:20:05.256201 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23527 (* 1 = 6.23527 loss)
I0524 07:20:05.256420 11835 sgd_solver.cpp:112] Iteration 48030, lr = 0.1
I0524 07:20:11.292958 11835 solver.cpp:239] Iteration 48040 (1.65658 iter/s, 6.03654s/10 iters), loss = 6.44527
I0524 07:20:11.293141 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44527 (* 1 = 6.44527 loss)
I0524 07:20:11.293419 11835 sgd_solver.cpp:112] Iteration 48040, lr = 0.1
I0524 07:20:20.589985 11835 solver.cpp:239] Iteration 48050 (1.07568 iter/s, 9.29648s/10 iters), loss = 6.24666
I0524 07:20:20.590044 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24666 (* 1 = 6.24666 loss)
I0524 07:20:20.590062 11835 sgd_solver.cpp:112] Iteration 48050, lr = 0.1
I0524 07:20:28.294086 11835 solver.cpp:239] Iteration 48060 (1.29844 iter/s, 7.70155s/10 iters), loss = 5.99873
I0524 07:20:28.294145 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99873 (* 1 = 5.99873 loss)
I0524 07:20:29.191807 11835 sgd_solver.cpp:112] Iteration 48060, lr = 0.1
I0524 07:20:35.461882 11835 solver.cpp:239] Iteration 48070 (1.39519 iter/s, 7.16747s/10 iters), loss = 5.24869
I0524 07:20:35.461922 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.24869 (* 1 = 5.24869 loss)
I0524 07:20:35.461935 11835 sgd_solver.cpp:112] Iteration 48070, lr = 0.1
I0524 07:20:41.315614 11835 solver.cpp:239] Iteration 48080 (1.70847 iter/s, 5.85318s/10 iters), loss = 6.89231
I0524 07:20:41.315781 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89231 (* 1 = 6.89231 loss)
I0524 07:20:41.315802 11835 sgd_solver.cpp:112] Iteration 48080, lr = 0.1
I0524 07:20:48.999454 11835 solver.cpp:239] Iteration 48090 (1.30151 iter/s, 7.68337s/10 iters), loss = 6.33071
I0524 07:20:48.999502 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33071 (* 1 = 6.33071 loss)
I0524 07:20:49.247807 11835 sgd_solver.cpp:112] Iteration 48090, lr = 0.1
I0524 07:21:00.142856 11835 solver.cpp:239] Iteration 48100 (0.897431 iter/s, 11.1429s/10 iters), loss = 6.52805
I0524 07:21:00.142912 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52805 (* 1 = 6.52805 loss)
I0524 07:21:00.142928 11835 sgd_solver.cpp:112] Iteration 48100, lr = 0.1
I0524 07:21:06.069030 11835 solver.cpp:239] Iteration 48110 (1.68782 iter/s, 5.92479s/10 iters), loss = 6.98328
I0524 07:21:06.069095 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98328 (* 1 = 6.98328 loss)
I0524 07:21:06.069365 11835 sgd_solver.cpp:112] Iteration 48110, lr = 0.1
I0524 07:21:12.976178 11835 solver.cpp:239] Iteration 48120 (1.44785 iter/s, 6.90681s/10 iters), loss = 5.28281
I0524 07:21:12.976423 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.28281 (* 1 = 5.28281 loss)
I0524 07:21:13.218139 11835 sgd_solver.cpp:112] Iteration 48120, lr = 0.1
I0524 07:21:20.498324 11835 solver.cpp:239] Iteration 48130 (1.3295 iter/s, 7.52163s/10 iters), loss = 5.17555
I0524 07:21:20.498401 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.17555 (* 1 = 5.17555 loss)
I0524 07:21:20.498514 11835 sgd_solver.cpp:112] Iteration 48130, lr = 0.1
I0524 07:21:26.737145 11835 solver.cpp:239] Iteration 48140 (1.60295 iter/s, 6.23851s/10 iters), loss = 6.99828
I0524 07:21:26.737205 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.99828 (* 1 = 6.99828 loss)
I0524 07:21:26.737320 11835 sgd_solver.cpp:112] Iteration 48140, lr = 0.1
I0524 07:21:33.389108 11835 solver.cpp:239] Iteration 48150 (1.50338 iter/s, 6.65166s/10 iters), loss = 5.74082
I0524 07:21:33.389155 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74082 (* 1 = 5.74082 loss)
I0524 07:21:33.428184 11835 sgd_solver.cpp:112] Iteration 48150, lr = 0.1
I0524 07:21:39.821337 11835 solver.cpp:239] Iteration 48160 (1.55474 iter/s, 6.43192s/10 iters), loss = 5.45775
I0524 07:21:39.821385 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.45775 (* 1 = 5.45775 loss)
I0524 07:21:40.103164 11835 sgd_solver.cpp:112] Iteration 48160, lr = 0.1
I0524 07:21:46.135880 11835 solver.cpp:239] Iteration 48170 (1.58372 iter/s, 6.31424s/10 iters), loss = 6.25667
I0524 07:21:46.136152 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25667 (* 1 = 6.25667 loss)
I0524 07:21:46.136195 11835 sgd_solver.cpp:112] Iteration 48170, lr = 0.1
I0524 07:21:54.384449 11835 solver.cpp:239] Iteration 48180 (1.21242 iter/s, 8.24797s/10 iters), loss = 6.20865
I0524 07:21:54.384501 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20865 (* 1 = 6.20865 loss)
I0524 07:21:54.384595 11835 sgd_solver.cpp:112] Iteration 48180, lr = 0.1
I0524 07:22:01.483806 11835 solver.cpp:239] Iteration 48190 (1.40864 iter/s, 7.09902s/10 iters), loss = 6.10402
I0524 07:22:01.483875 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10402 (* 1 = 6.10402 loss)
I0524 07:22:01.483913 11835 sgd_solver.cpp:112] Iteration 48190, lr = 0.1
I0524 07:22:08.747109 11835 solver.cpp:239] Iteration 48200 (1.37685 iter/s, 7.26297s/10 iters), loss = 6.06305
I0524 07:22:08.747172 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06305 (* 1 = 6.06305 loss)
I0524 07:22:08.796190 11835 sgd_solver.cpp:112] Iteration 48200, lr = 0.1
I0524 07:22:15.647753 11835 solver.cpp:239] Iteration 48210 (1.44921 iter/s, 6.90031s/10 iters), loss = 5.91464
I0524 07:22:15.647805 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91464 (* 1 = 5.91464 loss)
I0524 07:22:15.688824 11835 sgd_solver.cpp:112] Iteration 48210, lr = 0.1
I0524 07:22:22.200721 11835 solver.cpp:239] Iteration 48220 (1.5261 iter/s, 6.55266s/10 iters), loss = 5.96928
I0524 07:22:22.200868 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96928 (* 1 = 5.96928 loss)
I0524 07:22:22.200884 11835 sgd_solver.cpp:112] Iteration 48220, lr = 0.1
I0524 07:22:27.976869 11835 solver.cpp:239] Iteration 48230 (1.73138 iter/s, 5.77575s/10 iters), loss = 6.50717
I0524 07:22:27.976912 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50717 (* 1 = 6.50717 loss)
I0524 07:22:27.976927 11835 sgd_solver.cpp:112] Iteration 48230, lr = 0.1
I0524 07:22:35.163802 11835 solver.cpp:239] Iteration 48240 (1.39149 iter/s, 7.18653s/10 iters), loss = 5.13409
I0524 07:22:35.163853 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.13409 (* 1 = 5.13409 loss)
I0524 07:22:35.336421 11835 sgd_solver.cpp:112] Iteration 48240, lr = 0.1
I0524 07:22:41.127346 11835 solver.cpp:239] Iteration 48250 (1.67693 iter/s, 5.96326s/10 iters), loss = 6.42629
I0524 07:22:41.127390 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42629 (* 1 = 6.42629 loss)
I0524 07:22:41.127404 11835 sgd_solver.cpp:112] Iteration 48250, lr = 0.1
I0524 07:22:48.136071 11835 solver.cpp:239] Iteration 48260 (1.42686 iter/s, 7.00841s/10 iters), loss = 5.60764
I0524 07:22:48.136121 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.60764 (* 1 = 5.60764 loss)
I0524 07:22:48.136461 11835 sgd_solver.cpp:112] Iteration 48260, lr = 0.1
I0524 07:22:55.762114 11835 solver.cpp:239] Iteration 48270 (1.31136 iter/s, 7.62568s/10 iters), loss = 6.39505
I0524 07:22:55.762315 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39505 (* 1 = 6.39505 loss)
I0524 07:22:55.762442 11835 sgd_solver.cpp:112] Iteration 48270, lr = 0.1
I0524 07:23:01.882278 11835 solver.cpp:239] Iteration 48280 (1.63406 iter/s, 6.11974s/10 iters), loss = 5.16745
I0524 07:23:01.882320 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.16745 (* 1 = 5.16745 loss)
I0524 07:23:02.463259 11835 sgd_solver.cpp:112] Iteration 48280, lr = 0.1
I0524 07:23:10.878017 11835 solver.cpp:239] Iteration 48290 (1.11169 iter/s, 8.99535s/10 iters), loss = 6.14384
I0524 07:23:10.878070 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14384 (* 1 = 6.14384 loss)
I0524 07:23:10.878242 11835 sgd_solver.cpp:112] Iteration 48290, lr = 0.1
I0524 07:23:17.589534 11835 solver.cpp:239] Iteration 48300 (1.49005 iter/s, 6.71117s/10 iters), loss = 5.2765
I0524 07:23:17.589599 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.2765 (* 1 = 5.2765 loss)
I0524 07:23:17.589622 11835 sgd_solver.cpp:112] Iteration 48300, lr = 0.1
I0524 07:23:24.886873 11835 solver.cpp:239] Iteration 48310 (1.37043 iter/s, 7.29699s/10 iters), loss = 6.74558
I0524 07:23:24.886935 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74558 (* 1 = 6.74558 loss)
I0524 07:23:24.933944 11835 sgd_solver.cpp:112] Iteration 48310, lr = 0.1
I0524 07:23:34.629928 11835 solver.cpp:239] Iteration 48320 (1.02642 iter/s, 9.74262s/10 iters), loss = 6.35556
I0524 07:23:34.630203 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35556 (* 1 = 6.35556 loss)
I0524 07:23:34.630254 11835 sgd_solver.cpp:112] Iteration 48320, lr = 0.1
I0524 07:23:40.640507 11835 solver.cpp:239] Iteration 48330 (1.66386 iter/s, 6.01012s/10 iters), loss = 5.79173
I0524 07:23:40.640566 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79173 (* 1 = 5.79173 loss)
I0524 07:23:40.640705 11835 sgd_solver.cpp:112] Iteration 48330, lr = 0.1
I0524 07:23:48.161227 11835 solver.cpp:239] Iteration 48340 (1.32972 iter/s, 7.52036s/10 iters), loss = 6.12557
I0524 07:23:48.161280 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12557 (* 1 = 6.12557 loss)
I0524 07:23:48.161370 11835 sgd_solver.cpp:112] Iteration 48340, lr = 0.1
I0524 07:23:54.214975 11835 solver.cpp:239] Iteration 48350 (1.65195 iter/s, 6.05347s/10 iters), loss = 6.64215
I0524 07:23:54.215018 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64215 (* 1 = 6.64215 loss)
I0524 07:23:54.215045 11835 sgd_solver.cpp:112] Iteration 48350, lr = 0.1
I0524 07:24:00.082710 11835 solver.cpp:239] Iteration 48360 (1.70432 iter/s, 5.86744s/10 iters), loss = 6.80759
I0524 07:24:00.082757 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80759 (* 1 = 6.80759 loss)
I0524 07:24:00.082772 11835 sgd_solver.cpp:112] Iteration 48360, lr = 0.1
I0524 07:24:08.145593 11835 solver.cpp:239] Iteration 48370 (1.24032 iter/s, 8.06246s/10 iters), loss = 5.81537
I0524 07:24:08.145840 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81537 (* 1 = 5.81537 loss)
I0524 07:24:08.145900 11835 sgd_solver.cpp:112] Iteration 48370, lr = 0.1
I0524 07:24:16.611006 11835 solver.cpp:239] Iteration 48380 (1.18135 iter/s, 8.46487s/10 iters), loss = 5.93913
I0524 07:24:16.611062 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93913 (* 1 = 5.93913 loss)
I0524 07:24:16.611603 11835 sgd_solver.cpp:112] Iteration 48380, lr = 0.1
I0524 07:24:26.092715 11835 solver.cpp:239] Iteration 48390 (1.05471 iter/s, 9.48129s/10 iters), loss = 5.37076
I0524 07:24:26.092782 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.37076 (* 1 = 5.37076 loss)
I0524 07:24:26.092988 11835 sgd_solver.cpp:112] Iteration 48390, lr = 0.1
I0524 07:24:34.103381 11835 solver.cpp:239] Iteration 48400 (1.24839 iter/s, 8.01031s/10 iters), loss = 6.56802
I0524 07:24:34.103431 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56802 (* 1 = 6.56802 loss)
I0524 07:24:34.103616 11835 sgd_solver.cpp:112] Iteration 48400, lr = 0.1
I0524 07:24:41.213083 11835 solver.cpp:239] Iteration 48410 (1.40659 iter/s, 7.10937s/10 iters), loss = 6.89585
I0524 07:24:41.213404 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89585 (* 1 = 6.89585 loss)
I0524 07:24:41.213449 11835 sgd_solver.cpp:112] Iteration 48410, lr = 0.1
I0524 07:24:48.087334 11835 solver.cpp:239] Iteration 48420 (1.45482 iter/s, 6.87371s/10 iters), loss = 5.66384
I0524 07:24:48.087389 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.66384 (* 1 = 5.66384 loss)
I0524 07:24:48.087512 11835 sgd_solver.cpp:112] Iteration 48420, lr = 0.1
I0524 07:24:53.783038 11835 solver.cpp:239] Iteration 48430 (1.7558 iter/s, 5.69542s/10 iters), loss = 6.38113
I0524 07:24:53.783100 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38113 (* 1 = 6.38113 loss)
I0524 07:24:53.783581 11835 sgd_solver.cpp:112] Iteration 48430, lr = 0.1
I0524 07:25:01.071540 11835 solver.cpp:239] Iteration 48440 (1.37209 iter/s, 7.28816s/10 iters), loss = 5.68845
I0524 07:25:01.071589 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68845 (* 1 = 5.68845 loss)
I0524 07:25:01.071799 11835 sgd_solver.cpp:112] Iteration 48440, lr = 0.1
I0524 07:25:08.253093 11835 solver.cpp:239] Iteration 48450 (1.39252 iter/s, 7.18121s/10 iters), loss = 6.15236
I0524 07:25:08.253155 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15236 (* 1 = 6.15236 loss)
I0524 07:25:08.253299 11835 sgd_solver.cpp:112] Iteration 48450, lr = 0.1
I0524 07:25:15.831116 11835 solver.cpp:239] Iteration 48460 (1.31967 iter/s, 7.57766s/10 iters), loss = 5.43212
I0524 07:25:15.831332 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43212 (* 1 = 5.43212 loss)
I0524 07:25:15.831369 11835 sgd_solver.cpp:112] Iteration 48460, lr = 0.1
I0524 07:25:21.774073 11835 solver.cpp:239] Iteration 48470 (1.68336 iter/s, 5.94051s/10 iters), loss = 6.7718
I0524 07:25:21.774127 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7718 (* 1 = 6.7718 loss)
I0524 07:25:21.774353 11835 sgd_solver.cpp:112] Iteration 48470, lr = 0.1
I0524 07:25:28.584738 11835 solver.cpp:239] Iteration 48480 (1.46835 iter/s, 6.81035s/10 iters), loss = 5.58239
I0524 07:25:28.584791 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.58239 (* 1 = 5.58239 loss)
I0524 07:25:28.584909 11835 sgd_solver.cpp:112] Iteration 48480, lr = 0.1
I0524 07:25:34.791802 11835 solver.cpp:239] Iteration 48490 (1.61114 iter/s, 6.20678s/10 iters), loss = 6.70549
I0524 07:25:34.791836 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70549 (* 1 = 6.70549 loss)
I0524 07:25:34.791848 11835 sgd_solver.cpp:112] Iteration 48490, lr = 0.1
I0524 07:25:43.423175 11835 solver.cpp:239] Iteration 48500 (1.15861 iter/s, 8.631s/10 iters), loss = 6.28381
I0524 07:25:43.423226 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.28381 (* 1 = 6.28381 loss)
I0524 07:25:44.234159 11835 sgd_solver.cpp:112] Iteration 48500, lr = 0.1
I0524 07:25:51.585949 11835 solver.cpp:239] Iteration 48510 (1.22513 iter/s, 8.1624s/10 iters), loss = 6.5503
I0524 07:25:51.586217 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5503 (* 1 = 6.5503 loss)
I0524 07:25:52.311661 11835 sgd_solver.cpp:112] Iteration 48510, lr = 0.1
I0524 07:25:59.385160 11835 solver.cpp:239] Iteration 48520 (1.28227 iter/s, 7.79868s/10 iters), loss = 5.69721
I0524 07:25:59.385213 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.69721 (* 1 = 5.69721 loss)
I0524 07:25:59.385334 11835 sgd_solver.cpp:112] Iteration 48520, lr = 0.1
I0524 07:26:05.240958 11835 solver.cpp:239] Iteration 48530 (1.70779 iter/s, 5.85552s/10 iters), loss = 6.70974
I0524 07:26:05.241005 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70974 (* 1 = 6.70974 loss)
I0524 07:26:05.241020 11835 sgd_solver.cpp:112] Iteration 48530, lr = 0.1
I0524 07:26:12.041358 11835 solver.cpp:239] Iteration 48540 (1.47057 iter/s, 6.80009s/10 iters), loss = 5.12237
I0524 07:26:12.041426 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.12237 (* 1 = 5.12237 loss)
I0524 07:26:12.041505 11835 sgd_solver.cpp:112] Iteration 48540, lr = 0.1
I0524 07:26:17.993013 11835 solver.cpp:239] Iteration 48550 (1.68029 iter/s, 5.95137s/10 iters), loss = 5.90408
I0524 07:26:17.993067 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90408 (* 1 = 5.90408 loss)
I0524 07:26:18.924808 11835 sgd_solver.cpp:112] Iteration 48550, lr = 0.1
I0524 07:26:26.485330 11835 solver.cpp:239] Iteration 48560 (1.17759 iter/s, 8.49192s/10 iters), loss = 5.84383
I0524 07:26:26.485633 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84383 (* 1 = 5.84383 loss)
I0524 07:26:26.485664 11835 sgd_solver.cpp:112] Iteration 48560, lr = 0.1
I0524 07:26:33.632191 11835 solver.cpp:239] Iteration 48570 (1.39934 iter/s, 7.14623s/10 iters), loss = 6.20592
I0524 07:26:33.632238 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20592 (* 1 = 6.20592 loss)
I0524 07:26:34.284862 11835 sgd_solver.cpp:112] Iteration 48570, lr = 0.1
I0524 07:26:41.131399 11835 solver.cpp:239] Iteration 48580 (1.33353 iter/s, 7.49888s/10 iters), loss = 6.24576
I0524 07:26:41.131446 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24576 (* 1 = 6.24576 loss)
I0524 07:26:41.185324 11835 sgd_solver.cpp:112] Iteration 48580, lr = 0.1
I0524 07:26:48.804721 11835 solver.cpp:239] Iteration 48590 (1.30328 iter/s, 7.67297s/10 iters), loss = 6.89261
I0524 07:26:48.804780 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89261 (* 1 = 6.89261 loss)
I0524 07:26:48.804878 11835 sgd_solver.cpp:112] Iteration 48590, lr = 0.1
I0524 07:26:55.352113 11835 solver.cpp:239] Iteration 48600 (1.5274 iter/s, 6.54706s/10 iters), loss = 6.21177
I0524 07:26:55.352208 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21177 (* 1 = 6.21177 loss)
I0524 07:26:56.646780 11835 sgd_solver.cpp:112] Iteration 48600, lr = 0.1
I0524 07:27:04.923434 11835 solver.cpp:239] Iteration 48610 (1.04484 iter/s, 9.57087s/10 iters), loss = 6.02542
I0524 07:27:04.923491 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02542 (* 1 = 6.02542 loss)
I0524 07:27:04.923694 11835 sgd_solver.cpp:112] Iteration 48610, lr = 0.1
I0524 07:27:13.063652 11835 solver.cpp:239] Iteration 48620 (1.22852 iter/s, 8.13985s/10 iters), loss = 5.99741
I0524 07:27:13.063720 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99741 (* 1 = 5.99741 loss)
I0524 07:27:13.063936 11835 sgd_solver.cpp:112] Iteration 48620, lr = 0.1
I0524 07:27:19.496548 11835 solver.cpp:239] Iteration 48630 (1.55458 iter/s, 6.43259s/10 iters), loss = 6.44164
I0524 07:27:19.496595 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44164 (* 1 = 6.44164 loss)
I0524 07:27:19.496673 11835 sgd_solver.cpp:112] Iteration 48630, lr = 0.1
I0524 07:27:25.570514 11835 solver.cpp:239] Iteration 48640 (1.64645 iter/s, 6.07368s/10 iters), loss = 5.6004
I0524 07:27:25.570561 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.6004 (* 1 = 5.6004 loss)
I0524 07:27:25.570575 11835 sgd_solver.cpp:112] Iteration 48640, lr = 0.1
I0524 07:27:36.369925 11835 solver.cpp:239] Iteration 48650 (0.926206 iter/s, 10.7967s/10 iters), loss = 6.67849
I0524 07:27:36.370095 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67849 (* 1 = 6.67849 loss)
I0524 07:27:36.370115 11835 sgd_solver.cpp:112] Iteration 48650, lr = 0.1
I0524 07:27:43.506482 11835 solver.cpp:239] Iteration 48660 (1.40142 iter/s, 7.13563s/10 iters), loss = 5.98984
I0524 07:27:43.506527 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98984 (* 1 = 5.98984 loss)
I0524 07:27:43.506685 11835 sgd_solver.cpp:112] Iteration 48660, lr = 0.1
I0524 07:27:51.121855 11835 solver.cpp:239] Iteration 48670 (1.31319 iter/s, 7.61503s/10 iters), loss = 5.63162
I0524 07:27:51.121901 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63162 (* 1 = 5.63162 loss)
I0524 07:27:52.492692 11835 sgd_solver.cpp:112] Iteration 48670, lr = 0.1
I0524 07:27:58.882736 11835 solver.cpp:239] Iteration 48680 (1.28858 iter/s, 7.7605s/10 iters), loss = 5.94859
I0524 07:27:58.882807 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94859 (* 1 = 5.94859 loss)
I0524 07:27:58.883023 11835 sgd_solver.cpp:112] Iteration 48680, lr = 0.1
I0524 07:28:06.554040 11835 solver.cpp:239] Iteration 48690 (1.30362 iter/s, 7.67093s/10 iters), loss = 6.26296
I0524 07:28:06.554424 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26296 (* 1 = 6.26296 loss)
I0524 07:28:06.554502 11835 sgd_solver.cpp:112] Iteration 48690, lr = 0.1
I0524 07:28:15.728368 11835 solver.cpp:239] Iteration 48700 (1.09007 iter/s, 9.17371s/10 iters), loss = 6.22829
I0524 07:28:15.728415 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22829 (* 1 = 6.22829 loss)
I0524 07:28:15.728476 11835 sgd_solver.cpp:112] Iteration 48700, lr = 0.1
I0524 07:28:23.193581 11835 solver.cpp:239] Iteration 48710 (1.33961 iter/s, 7.46486s/10 iters), loss = 5.42175
I0524 07:28:23.193642 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.42175 (* 1 = 5.42175 loss)
I0524 07:28:23.193833 11835 sgd_solver.cpp:112] Iteration 48710, lr = 0.1
I0524 07:28:30.306273 11835 solver.cpp:239] Iteration 48720 (1.406 iter/s, 7.11235s/10 iters), loss = 5.63997
I0524 07:28:30.306371 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63997 (* 1 = 5.63997 loss)
I0524 07:28:30.306581 11835 sgd_solver.cpp:112] Iteration 48720, lr = 0.1
I0524 07:28:39.302146 11835 solver.cpp:239] Iteration 48730 (1.11167 iter/s, 8.99547s/10 iters), loss = 6.58688
I0524 07:28:39.302290 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58688 (* 1 = 6.58688 loss)
I0524 07:28:39.401947 11835 sgd_solver.cpp:112] Iteration 48730, lr = 0.1
I0524 07:28:48.195788 11835 solver.cpp:239] Iteration 48740 (1.12446 iter/s, 8.89316s/10 iters), loss = 5.21246
I0524 07:28:48.195852 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.21246 (* 1 = 5.21246 loss)
I0524 07:28:48.333884 11835 sgd_solver.cpp:112] Iteration 48740, lr = 0.1
I0524 07:28:56.510762 11835 solver.cpp:239] Iteration 48750 (1.20271 iter/s, 8.31454s/10 iters), loss = 6.00122
I0524 07:28:56.510821 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00122 (* 1 = 6.00122 loss)
I0524 07:28:56.698372 11835 sgd_solver.cpp:112] Iteration 48750, lr = 0.1
I0524 07:29:04.143832 11835 solver.cpp:239] Iteration 48760 (1.31015 iter/s, 7.63273s/10 iters), loss = 6.28727
I0524 07:29:04.143872 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.28727 (* 1 = 6.28727 loss)
I0524 07:29:04.143985 11835 sgd_solver.cpp:112] Iteration 48760, lr = 0.1
I0524 07:29:12.217690 11835 solver.cpp:239] Iteration 48770 (1.23862 iter/s, 8.0735s/10 iters), loss = 5.94426
I0524 07:29:12.217918 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94426 (* 1 = 5.94426 loss)
I0524 07:29:12.217967 11835 sgd_solver.cpp:112] Iteration 48770, lr = 0.1
I0524 07:29:18.363607 11835 solver.cpp:239] Iteration 48780 (1.62748 iter/s, 6.14446s/10 iters), loss = 6.71659
I0524 07:29:18.363653 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71659 (* 1 = 6.71659 loss)
I0524 07:29:18.363852 11835 sgd_solver.cpp:112] Iteration 48780, lr = 0.1
I0524 07:29:26.575752 11835 solver.cpp:239] Iteration 48790 (1.21776 iter/s, 8.21179s/10 iters), loss = 6.04349
I0524 07:29:26.575794 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04349 (* 1 = 6.04349 loss)
I0524 07:29:26.576014 11835 sgd_solver.cpp:112] Iteration 48790, lr = 0.1
I0524 07:29:36.620981 11835 solver.cpp:239] Iteration 48800 (0.995541 iter/s, 10.0448s/10 iters), loss = 5.94525
I0524 07:29:36.621031 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94525 (* 1 = 5.94525 loss)
I0524 07:29:36.795724 11835 sgd_solver.cpp:112] Iteration 48800, lr = 0.1
I0524 07:29:42.426836 11835 solver.cpp:239] Iteration 48810 (1.72248 iter/s, 5.80558s/10 iters), loss = 5.30704
I0524 07:29:42.427018 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.30704 (* 1 = 5.30704 loss)
I0524 07:29:42.567976 11835 sgd_solver.cpp:112] Iteration 48810, lr = 0.1
I0524 07:29:48.804738 11835 solver.cpp:239] Iteration 48820 (1.56802 iter/s, 6.37747s/10 iters), loss = 6.6401
I0524 07:29:48.804791 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6401 (* 1 = 6.6401 loss)
I0524 07:29:48.804807 11835 sgd_solver.cpp:112] Iteration 48820, lr = 0.1
I0524 07:29:54.938835 11835 solver.cpp:239] Iteration 48830 (1.63036 iter/s, 6.13363s/10 iters), loss = 6.46864
I0524 07:29:54.938892 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46864 (* 1 = 6.46864 loss)
I0524 07:29:54.938943 11835 sgd_solver.cpp:112] Iteration 48830, lr = 0.1
I0524 07:30:01.314290 11835 solver.cpp:239] Iteration 48840 (1.56859 iter/s, 6.37514s/10 iters), loss = 5.458
I0524 07:30:01.314359 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.458 (* 1 = 5.458 loss)
I0524 07:30:01.314558 11835 sgd_solver.cpp:112] Iteration 48840, lr = 0.1
I0524 07:30:07.370654 11835 solver.cpp:239] Iteration 48850 (1.65123 iter/s, 6.05608s/10 iters), loss = 6.63987
I0524 07:30:07.370703 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63987 (* 1 = 6.63987 loss)
I0524 07:30:07.370801 11835 sgd_solver.cpp:112] Iteration 48850, lr = 0.1
I0524 07:30:14.840668 11835 solver.cpp:239] Iteration 48860 (1.33875 iter/s, 7.46967s/10 iters), loss = 5.20561
I0524 07:30:14.840970 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.20561 (* 1 = 5.20561 loss)
I0524 07:30:14.841022 11835 sgd_solver.cpp:112] Iteration 48860, lr = 0.1
I0524 07:30:20.721487 11835 solver.cpp:239] Iteration 48870 (1.70063 iter/s, 5.88016s/10 iters), loss = 6.10911
I0524 07:30:20.721525 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10911 (* 1 = 6.10911 loss)
I0524 07:30:20.883808 11835 sgd_solver.cpp:112] Iteration 48870, lr = 0.1
I0524 07:30:27.262197 11835 solver.cpp:239] Iteration 48880 (1.52896 iter/s, 6.54041s/10 iters), loss = 6.02647
I0524 07:30:27.262246 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02647 (* 1 = 6.02647 loss)
I0524 07:30:27.262264 11835 sgd_solver.cpp:112] Iteration 48880, lr = 0.1
I0524 07:30:34.563148 11835 solver.cpp:239] Iteration 48890 (1.36975 iter/s, 7.30062s/10 iters), loss = 6.73641
I0524 07:30:34.563205 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73641 (* 1 = 6.73641 loss)
I0524 07:30:34.563390 11835 sgd_solver.cpp:112] Iteration 48890, lr = 0.1
I0524 07:30:41.791357 11835 solver.cpp:239] Iteration 48900 (1.38353 iter/s, 7.22788s/10 iters), loss = 6.1676
I0524 07:30:41.791409 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1676 (* 1 = 6.1676 loss)
I0524 07:30:43.183878 11835 sgd_solver.cpp:112] Iteration 48900, lr = 0.1
I0524 07:30:51.354336 11835 solver.cpp:239] Iteration 48910 (1.04575 iter/s, 9.56255s/10 iters), loss = 6.17845
I0524 07:30:51.354565 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17845 (* 1 = 6.17845 loss)
I0524 07:30:51.354612 11835 sgd_solver.cpp:112] Iteration 48910, lr = 0.1
I0524 07:30:59.557755 11835 solver.cpp:239] Iteration 48920 (1.21909 iter/s, 8.20285s/10 iters), loss = 6.08428
I0524 07:30:59.557839 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08428 (* 1 = 6.08428 loss)
I0524 07:30:59.631554 11835 sgd_solver.cpp:112] Iteration 48920, lr = 0.1
I0524 07:31:07.137138 11835 solver.cpp:239] Iteration 48930 (1.31943 iter/s, 7.57901s/10 iters), loss = 5.06686
I0524 07:31:07.137195 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.06686 (* 1 = 5.06686 loss)
I0524 07:31:07.137270 11835 sgd_solver.cpp:112] Iteration 48930, lr = 0.1
I0524 07:31:13.278139 11835 solver.cpp:239] Iteration 48940 (1.62848 iter/s, 6.1407s/10 iters), loss = 5.59127
I0524 07:31:13.278204 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.59127 (* 1 = 5.59127 loss)
I0524 07:31:14.377729 11835 sgd_solver.cpp:112] Iteration 48940, lr = 0.1
I0524 07:31:21.826231 11835 solver.cpp:239] Iteration 48950 (1.1699 iter/s, 8.54771s/10 iters), loss = 5.54032
I0524 07:31:21.826370 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.54032 (* 1 = 5.54032 loss)
I0524 07:31:21.826386 11835 sgd_solver.cpp:112] Iteration 48950, lr = 0.1
I0524 07:31:28.353472 11835 solver.cpp:239] Iteration 48960 (1.53213 iter/s, 6.52685s/10 iters), loss = 5.97665
I0524 07:31:28.353534 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97665 (* 1 = 5.97665 loss)
I0524 07:31:28.353554 11835 sgd_solver.cpp:112] Iteration 48960, lr = 0.1
I0524 07:31:34.491981 11835 solver.cpp:239] Iteration 48970 (1.6292 iter/s, 6.138s/10 iters), loss = 6.80972
I0524 07:31:34.492035 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80972 (* 1 = 6.80972 loss)
I0524 07:31:34.513751 11835 sgd_solver.cpp:112] Iteration 48970, lr = 0.1
I0524 07:31:41.832474 11835 solver.cpp:239] Iteration 48980 (1.36237 iter/s, 7.34013s/10 iters), loss = 6.11635
I0524 07:31:41.832541 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11635 (* 1 = 6.11635 loss)
I0524 07:31:41.832561 11835 sgd_solver.cpp:112] Iteration 48980, lr = 0.1
I0524 07:31:47.897980 11835 solver.cpp:239] Iteration 48990 (1.64876 iter/s, 6.06517s/10 iters), loss = 6.38806
I0524 07:31:47.898054 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38806 (* 1 = 6.38806 loss)
I0524 07:31:48.045470 11835 sgd_solver.cpp:112] Iteration 48990, lr = 0.1
I0524 07:31:55.541378 11835 solver.cpp:239] Iteration 49000 (1.30838 iter/s, 7.64304s/10 iters), loss = 5.63828
I0524 07:31:55.541584 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63828 (* 1 = 5.63828 loss)
I0524 07:31:55.541678 11835 sgd_solver.cpp:112] Iteration 49000, lr = 0.1
I0524 07:32:02.337273 11835 solver.cpp:239] Iteration 49010 (1.47158 iter/s, 6.79541s/10 iters), loss = 5.44332
I0524 07:32:02.337330 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.44332 (* 1 = 5.44332 loss)
I0524 07:32:02.337420 11835 sgd_solver.cpp:112] Iteration 49010, lr = 0.1
I0524 07:32:08.312690 11835 solver.cpp:239] Iteration 49020 (1.6736 iter/s, 5.97513s/10 iters), loss = 5.81062
I0524 07:32:08.312727 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81062 (* 1 = 5.81062 loss)
I0524 07:32:08.312738 11835 sgd_solver.cpp:112] Iteration 49020, lr = 0.1
I0524 07:32:17.859707 11835 solver.cpp:239] Iteration 49030 (1.04749 iter/s, 9.54661s/10 iters), loss = 6.80252
I0524 07:32:17.859756 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80252 (* 1 = 6.80252 loss)
I0524 07:32:17.859889 11835 sgd_solver.cpp:112] Iteration 49030, lr = 0.1
I0524 07:32:23.983392 11835 solver.cpp:239] Iteration 49040 (1.63309 iter/s, 6.12335s/10 iters), loss = 5.92435
I0524 07:32:23.983458 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92435 (* 1 = 5.92435 loss)
I0524 07:32:23.983551 11835 sgd_solver.cpp:112] Iteration 49040, lr = 0.1
I0524 07:32:31.788750 11835 solver.cpp:239] Iteration 49050 (1.28123 iter/s, 7.80499s/10 iters), loss = 5.24366
I0524 07:32:31.788946 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.24366 (* 1 = 5.24366 loss)
I0524 07:32:31.788980 11835 sgd_solver.cpp:112] Iteration 49050, lr = 0.1
I0524 07:32:40.783495 11835 solver.cpp:239] Iteration 49060 (1.11209 iter/s, 8.99206s/10 iters), loss = 6.64369
I0524 07:32:40.783543 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64369 (* 1 = 6.64369 loss)
I0524 07:32:40.783896 11835 sgd_solver.cpp:112] Iteration 49060, lr = 0.1
I0524 07:32:47.596753 11835 solver.cpp:239] Iteration 49070 (1.4678 iter/s, 6.81293s/10 iters), loss = 5.9945
I0524 07:32:47.596812 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9945 (* 1 = 5.9945 loss)
I0524 07:32:47.596899 11835 sgd_solver.cpp:112] Iteration 49070, lr = 0.1
I0524 07:32:55.043195 11835 solver.cpp:239] Iteration 49080 (1.34299 iter/s, 7.44607s/10 iters), loss = 6.27012
I0524 07:32:55.043246 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27012 (* 1 = 6.27012 loss)
I0524 07:32:55.043264 11835 sgd_solver.cpp:112] Iteration 49080, lr = 0.1
I0524 07:33:02.341192 11835 solver.cpp:239] Iteration 49090 (1.3703 iter/s, 7.29765s/10 iters), loss = 5.61839
I0524 07:33:02.341452 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.61839 (* 1 = 5.61839 loss)
I0524 07:33:03.355281 11835 sgd_solver.cpp:112] Iteration 49090, lr = 0.1
I0524 07:33:09.184016 11835 solver.cpp:239] Iteration 49100 (1.46149 iter/s, 6.84235s/10 iters), loss = 5.24759
I0524 07:33:09.184084 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.24759 (* 1 = 5.24759 loss)
I0524 07:33:09.220432 11835 sgd_solver.cpp:112] Iteration 49100, lr = 0.1
I0524 07:33:17.024829 11835 solver.cpp:239] Iteration 49110 (1.27544 iter/s, 7.84043s/10 iters), loss = 5.65162
I0524 07:33:17.024886 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.65162 (* 1 = 5.65162 loss)
I0524 07:33:17.025125 11835 sgd_solver.cpp:112] Iteration 49110, lr = 0.1
I0524 07:33:24.237849 11835 solver.cpp:239] Iteration 49120 (1.38645 iter/s, 7.21267s/10 iters), loss = 5.98797
I0524 07:33:24.237917 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98797 (* 1 = 5.98797 loss)
I0524 07:33:24.295433 11835 sgd_solver.cpp:112] Iteration 49120, lr = 0.1
I0524 07:33:30.539607 11835 solver.cpp:239] Iteration 49130 (1.58694 iter/s, 6.30144s/10 iters), loss = 5.35148
I0524 07:33:30.539680 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.35148 (* 1 = 5.35148 loss)
I0524 07:33:30.539832 11835 sgd_solver.cpp:112] Iteration 49130, lr = 0.1
I0524 07:33:37.272791 11835 solver.cpp:239] Iteration 49140 (1.48525 iter/s, 6.73286s/10 iters), loss = 6.83669
I0524 07:33:37.273355 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83669 (* 1 = 6.83669 loss)
I0524 07:33:37.273382 11835 sgd_solver.cpp:112] Iteration 49140, lr = 0.1
I0524 07:33:43.298013 11835 solver.cpp:239] Iteration 49150 (1.65991 iter/s, 6.02443s/10 iters), loss = 6.18373
I0524 07:33:43.298084 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18373 (* 1 = 6.18373 loss)
I0524 07:33:43.298172 11835 sgd_solver.cpp:112] Iteration 49150, lr = 0.1
I0524 07:33:50.446116 11835 solver.cpp:239] Iteration 49160 (1.39904 iter/s, 7.14773s/10 iters), loss = 5.56056
I0524 07:33:50.446203 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56056 (* 1 = 5.56056 loss)
I0524 07:33:50.446246 11835 sgd_solver.cpp:112] Iteration 49160, lr = 0.1
I0524 07:33:57.598670 11835 solver.cpp:239] Iteration 49170 (1.39817 iter/s, 7.1522s/10 iters), loss = 6.72464
I0524 07:33:57.598742 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72464 (* 1 = 6.72464 loss)
I0524 07:33:57.598839 11835 sgd_solver.cpp:112] Iteration 49170, lr = 0.1
I0524 07:34:03.805734 11835 solver.cpp:239] Iteration 49180 (1.61115 iter/s, 6.20676s/10 iters), loss = 5.66054
I0524 07:34:03.805784 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.66054 (* 1 = 5.66054 loss)
I0524 07:34:03.805896 11835 sgd_solver.cpp:112] Iteration 49180, lr = 0.1
I0524 07:34:09.884780 11835 solver.cpp:239] Iteration 49190 (1.64507 iter/s, 6.07876s/10 iters), loss = 5.52504
I0524 07:34:09.884939 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.52504 (* 1 = 5.52504 loss)
I0524 07:34:10.068325 11835 sgd_solver.cpp:112] Iteration 49190, lr = 0.1
I0524 07:34:16.721823 11835 solver.cpp:239] Iteration 49200 (1.46271 iter/s, 6.83661s/10 iters), loss = 6.3766
I0524 07:34:16.721900 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3766 (* 1 = 6.3766 loss)
I0524 07:34:17.173032 11835 sgd_solver.cpp:112] Iteration 49200, lr = 0.1
I0524 07:34:25.049378 11835 solver.cpp:239] Iteration 49210 (1.20089 iter/s, 8.32716s/10 iters), loss = 6.08765
I0524 07:34:25.049432 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08765 (* 1 = 6.08765 loss)
I0524 07:34:25.049542 11835 sgd_solver.cpp:112] Iteration 49210, lr = 0.1
I0524 07:34:31.142889 11835 solver.cpp:239] Iteration 49220 (1.64117 iter/s, 6.0932s/10 iters), loss = 6.73539
I0524 07:34:31.142937 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73539 (* 1 = 6.73539 loss)
I0524 07:34:31.143182 11835 sgd_solver.cpp:112] Iteration 49220, lr = 0.1
I0524 07:34:37.615738 11835 solver.cpp:239] Iteration 49230 (1.54499 iter/s, 6.47255s/10 iters), loss = 6.15994
I0524 07:34:37.615787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15994 (* 1 = 6.15994 loss)
I0524 07:34:37.615959 11835 sgd_solver.cpp:112] Iteration 49230, lr = 0.1
I0524 07:34:44.310503 11835 solver.cpp:239] Iteration 49240 (1.49378 iter/s, 6.69444s/10 iters), loss = 6.13977
I0524 07:34:44.310824 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13977 (* 1 = 6.13977 loss)
I0524 07:34:44.311110 11835 sgd_solver.cpp:112] Iteration 49240, lr = 0.1
I0524 07:34:50.826210 11835 solver.cpp:239] Iteration 49250 (1.53488 iter/s, 6.51516s/10 iters), loss = 6.12043
I0524 07:34:50.826254 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12043 (* 1 = 6.12043 loss)
I0524 07:34:50.826390 11835 sgd_solver.cpp:112] Iteration 49250, lr = 0.1
I0524 07:34:58.414152 11835 solver.cpp:239] Iteration 49260 (1.31795 iter/s, 7.58755s/10 iters), loss = 5.92872
I0524 07:34:58.414247 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92872 (* 1 = 5.92872 loss)
I0524 07:34:58.435818 11835 sgd_solver.cpp:112] Iteration 49260, lr = 0.1
I0524 07:35:06.870474 11835 solver.cpp:239] Iteration 49270 (1.1826 iter/s, 8.45591s/10 iters), loss = 5.47649
I0524 07:35:06.870528 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.47649 (* 1 = 5.47649 loss)
I0524 07:35:07.370280 11835 sgd_solver.cpp:112] Iteration 49270, lr = 0.1
I0524 07:35:13.568363 11835 solver.cpp:239] Iteration 49280 (1.49308 iter/s, 6.69757s/10 iters), loss = 6.15986
I0524 07:35:13.568414 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15986 (* 1 = 6.15986 loss)
I0524 07:35:13.568429 11835 sgd_solver.cpp:112] Iteration 49280, lr = 0.1
I0524 07:35:21.972776 11835 solver.cpp:239] Iteration 49290 (1.18991 iter/s, 8.404s/10 iters), loss = 5.40129
I0524 07:35:21.973032 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.40129 (* 1 = 5.40129 loss)
I0524 07:35:21.973078 11835 sgd_solver.cpp:112] Iteration 49290, lr = 0.1
I0524 07:35:28.096297 11835 solver.cpp:239] Iteration 49300 (1.63318 iter/s, 6.12302s/10 iters), loss = 6.65679
I0524 07:35:28.096338 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65679 (* 1 = 6.65679 loss)
I0524 07:35:28.532382 11835 sgd_solver.cpp:112] Iteration 49300, lr = 0.1
I0524 07:35:35.487813 11835 solver.cpp:239] Iteration 49310 (1.35296 iter/s, 7.39118s/10 iters), loss = 5.92884
I0524 07:35:35.487884 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92884 (* 1 = 5.92884 loss)
I0524 07:35:35.487906 11835 sgd_solver.cpp:112] Iteration 49310, lr = 0.1
I0524 07:35:45.036206 11835 solver.cpp:239] Iteration 49320 (1.04759 iter/s, 9.54576s/10 iters), loss = 7.22677
I0524 07:35:45.036263 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.22677 (* 1 = 7.22677 loss)
I0524 07:35:45.036342 11835 sgd_solver.cpp:112] Iteration 49320, lr = 0.1
I0524 07:35:53.445551 11835 solver.cpp:239] Iteration 49330 (1.18921 iter/s, 8.40896s/10 iters), loss = 6.37806
I0524 07:35:53.445698 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37806 (* 1 = 6.37806 loss)
I0524 07:35:53.445763 11835 sgd_solver.cpp:112] Iteration 49330, lr = 0.1
I0524 07:35:59.959383 11835 solver.cpp:239] Iteration 49340 (1.53529 iter/s, 6.51343s/10 iters), loss = 6.25596
I0524 07:35:59.959439 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25596 (* 1 = 6.25596 loss)
I0524 07:35:59.959774 11835 sgd_solver.cpp:112] Iteration 49340, lr = 0.1
I0524 07:36:06.372496 11835 solver.cpp:239] Iteration 49350 (1.55938 iter/s, 6.4128s/10 iters), loss = 6.04016
I0524 07:36:06.372567 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04016 (* 1 = 6.04016 loss)
I0524 07:36:06.372807 11835 sgd_solver.cpp:112] Iteration 49350, lr = 0.1
I0524 07:36:12.674963 11835 solver.cpp:239] Iteration 49360 (1.58676 iter/s, 6.30216s/10 iters), loss = 6.39995
I0524 07:36:12.675019 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39995 (* 1 = 6.39995 loss)
I0524 07:36:12.675196 11835 sgd_solver.cpp:112] Iteration 49360, lr = 0.1
I0524 07:36:19.115283 11835 solver.cpp:239] Iteration 49370 (1.55279 iter/s, 6.44001s/10 iters), loss = 6.31726
I0524 07:36:19.115337 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31726 (* 1 = 6.31726 loss)
I0524 07:36:19.202137 11835 sgd_solver.cpp:112] Iteration 49370, lr = 0.1
I0524 07:36:25.216897 11835 solver.cpp:239] Iteration 49380 (1.63899 iter/s, 6.10133s/10 iters), loss = 5.9313
I0524 07:36:25.217134 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9313 (* 1 = 5.9313 loss)
I0524 07:36:25.232271 11835 sgd_solver.cpp:112] Iteration 49380, lr = 0.1
I0524 07:36:31.455972 11835 solver.cpp:239] Iteration 49390 (1.60292 iter/s, 6.23862s/10 iters), loss = 6.20579
I0524 07:36:31.456033 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20579 (* 1 = 6.20579 loss)
I0524 07:36:31.456048 11835 sgd_solver.cpp:112] Iteration 49390, lr = 0.1
I0524 07:36:37.744171 11835 solver.cpp:239] Iteration 49400 (1.59036 iter/s, 6.28787s/10 iters), loss = 5.41837
I0524 07:36:37.744240 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.41837 (* 1 = 5.41837 loss)
I0524 07:36:37.744262 11835 sgd_solver.cpp:112] Iteration 49400, lr = 0.1
I0524 07:36:44.489833 11835 solver.cpp:239] Iteration 49410 (1.48251 iter/s, 6.74532s/10 iters), loss = 5.88917
I0524 07:36:44.489897 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88917 (* 1 = 5.88917 loss)
I0524 07:36:44.490101 11835 sgd_solver.cpp:112] Iteration 49410, lr = 0.1
I0524 07:36:52.520474 11835 solver.cpp:239] Iteration 49420 (1.24529 iter/s, 8.03028s/10 iters), loss = 6.49105
I0524 07:36:52.520529 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49105 (* 1 = 6.49105 loss)
I0524 07:36:52.537782 11835 sgd_solver.cpp:112] Iteration 49420, lr = 0.1
I0524 07:36:59.623131 11835 solver.cpp:239] Iteration 49430 (1.40799 iter/s, 7.10233s/10 iters), loss = 5.83414
I0524 07:36:59.623343 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83414 (* 1 = 5.83414 loss)
I0524 07:36:59.753244 11835 sgd_solver.cpp:112] Iteration 49430, lr = 0.1
I0524 07:37:06.089690 11835 solver.cpp:239] Iteration 49440 (1.54652 iter/s, 6.46612s/10 iters), loss = 6.09833
I0524 07:37:06.089773 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09833 (* 1 = 6.09833 loss)
I0524 07:37:06.089797 11835 sgd_solver.cpp:112] Iteration 49440, lr = 0.1
I0524 07:37:13.057678 11835 solver.cpp:239] Iteration 49450 (1.43521 iter/s, 6.96762s/10 iters), loss = 5.68474
I0524 07:37:13.057729 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68474 (* 1 = 5.68474 loss)
I0524 07:37:13.057896 11835 sgd_solver.cpp:112] Iteration 49450, lr = 0.1
I0524 07:37:19.994331 11835 solver.cpp:239] Iteration 49460 (1.44169 iter/s, 6.93632s/10 iters), loss = 6.36408
I0524 07:37:19.994385 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36408 (* 1 = 6.36408 loss)
I0524 07:37:19.994624 11835 sgd_solver.cpp:112] Iteration 49460, lr = 0.1
I0524 07:37:30.497689 11835 solver.cpp:239] Iteration 49470 (0.952118 iter/s, 10.5029s/10 iters), loss = 6.17701
I0524 07:37:30.497925 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17701 (* 1 = 6.17701 loss)
I0524 07:37:30.498194 11835 sgd_solver.cpp:112] Iteration 49470, lr = 0.1
I0524 07:37:37.464972 11835 solver.cpp:239] Iteration 49480 (1.43538 iter/s, 6.9668s/10 iters), loss = 5.28135
I0524 07:37:37.465030 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.28135 (* 1 = 5.28135 loss)
I0524 07:37:37.465199 11835 sgd_solver.cpp:112] Iteration 49480, lr = 0.1
I0524 07:37:45.409096 11835 solver.cpp:239] Iteration 49490 (1.25885 iter/s, 7.94376s/10 iters), loss = 5.0803
I0524 07:37:45.409144 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.0803 (* 1 = 5.0803 loss)
I0524 07:37:46.609623 11835 sgd_solver.cpp:112] Iteration 49490, lr = 0.1
I0524 07:37:53.816956 11835 solver.cpp:239] Iteration 49500 (1.18942 iter/s, 8.40747s/10 iters), loss = 5.79897
I0524 07:37:53.817042 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79897 (* 1 = 5.79897 loss)
I0524 07:37:54.833786 11835 sgd_solver.cpp:112] Iteration 49500, lr = 0.1
I0524 07:38:01.607653 11835 solver.cpp:239] Iteration 49510 (1.28365 iter/s, 7.79026s/10 iters), loss = 6.60719
I0524 07:38:01.608033 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60719 (* 1 = 6.60719 loss)
I0524 07:38:01.608079 11835 sgd_solver.cpp:112] Iteration 49510, lr = 0.1
I0524 07:38:07.533144 11835 solver.cpp:239] Iteration 49520 (1.68793 iter/s, 5.92442s/10 iters), loss = 5.56046
I0524 07:38:07.533201 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56046 (* 1 = 5.56046 loss)
I0524 07:38:07.900269 11835 sgd_solver.cpp:112] Iteration 49520, lr = 0.1
I0524 07:38:13.818259 11835 solver.cpp:239] Iteration 49530 (1.59114 iter/s, 6.28481s/10 iters), loss = 5.66676
I0524 07:38:13.818315 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.66676 (* 1 = 5.66676 loss)
I0524 07:38:13.818369 11835 sgd_solver.cpp:112] Iteration 49530, lr = 0.1
I0524 07:38:23.052973 11835 solver.cpp:239] Iteration 49540 (1.08292 iter/s, 9.2343s/10 iters), loss = 6.93803
I0524 07:38:23.053025 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93803 (* 1 = 6.93803 loss)
I0524 07:38:23.053113 11835 sgd_solver.cpp:112] Iteration 49540, lr = 0.1
I0524 07:38:30.502162 11835 solver.cpp:239] Iteration 49550 (1.34249 iter/s, 7.44885s/10 iters), loss = 5.92766
I0524 07:38:30.502208 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92766 (* 1 = 5.92766 loss)
I0524 07:38:30.523983 11835 sgd_solver.cpp:112] Iteration 49550, lr = 0.1
I0524 07:38:39.965958 11835 solver.cpp:239] Iteration 49560 (1.0567 iter/s, 9.46338s/10 iters), loss = 6.14791
I0524 07:38:39.966189 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14791 (* 1 = 6.14791 loss)
I0524 07:38:39.966230 11835 sgd_solver.cpp:112] Iteration 49560, lr = 0.1
I0524 07:38:46.374804 11835 solver.cpp:239] Iteration 49570 (1.56072 iter/s, 6.4073s/10 iters), loss = 7.27838
I0524 07:38:46.374850 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.27838 (* 1 = 7.27838 loss)
I0524 07:38:46.375236 11835 sgd_solver.cpp:112] Iteration 49570, lr = 0.1
I0524 07:38:53.930668 11835 solver.cpp:239] Iteration 49580 (1.32354 iter/s, 7.55551s/10 iters), loss = 6.42369
I0524 07:38:53.930737 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42369 (* 1 = 6.42369 loss)
I0524 07:38:53.930794 11835 sgd_solver.cpp:112] Iteration 49580, lr = 0.1
I0524 07:39:02.480494 11835 solver.cpp:239] Iteration 49590 (1.16967 iter/s, 8.54943s/10 iters), loss = 6.19061
I0524 07:39:02.480571 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19061 (* 1 = 6.19061 loss)
I0524 07:39:02.480742 11835 sgd_solver.cpp:112] Iteration 49590, lr = 0.1
I0524 07:39:08.328770 11835 solver.cpp:239] Iteration 49600 (1.71 iter/s, 5.84796s/10 iters), loss = 4.88189
I0524 07:39:08.328847 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.88189 (* 1 = 4.88189 loss)
I0524 07:39:08.328867 11835 sgd_solver.cpp:112] Iteration 49600, lr = 0.1
I0524 07:39:16.827667 11835 solver.cpp:239] Iteration 49610 (1.17668 iter/s, 8.49851s/10 iters), loss = 5.48939
I0524 07:39:16.827877 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.48939 (* 1 = 5.48939 loss)
I0524 07:39:17.003041 11835 sgd_solver.cpp:112] Iteration 49610, lr = 0.1
I0524 07:39:24.965363 11835 solver.cpp:239] Iteration 49620 (1.22893 iter/s, 8.13718s/10 iters), loss = 5.6281
I0524 07:39:24.965423 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.6281 (* 1 = 5.6281 loss)
I0524 07:39:24.965677 11835 sgd_solver.cpp:112] Iteration 49620, lr = 0.1
I0524 07:39:31.095643 11835 solver.cpp:239] Iteration 49630 (1.63132 iter/s, 6.12999s/10 iters), loss = 5.52246
I0524 07:39:31.095675 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.52246 (* 1 = 5.52246 loss)
I0524 07:39:31.322582 11835 sgd_solver.cpp:112] Iteration 49630, lr = 0.1
I0524 07:39:38.508112 11835 solver.cpp:239] Iteration 49640 (1.34914 iter/s, 7.41213s/10 iters), loss = 5.2318
I0524 07:39:38.508167 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.2318 (* 1 = 5.2318 loss)
I0524 07:39:38.513622 11835 sgd_solver.cpp:112] Iteration 49640, lr = 0.1
I0524 07:39:44.755565 11835 solver.cpp:239] Iteration 49650 (1.60073 iter/s, 6.24715s/10 iters), loss = 6.42114
I0524 07:39:44.755632 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42114 (* 1 = 6.42114 loss)
I0524 07:39:44.755735 11835 sgd_solver.cpp:112] Iteration 49650, lr = 0.1
I0524 07:39:53.293561 11835 solver.cpp:239] Iteration 49660 (1.17129 iter/s, 8.53761s/10 iters), loss = 7.2762
I0524 07:39:53.293804 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2762 (* 1 = 7.2762 loss)
I0524 07:39:53.727084 11835 sgd_solver.cpp:112] Iteration 49660, lr = 0.1
I0524 07:40:00.404196 11835 solver.cpp:239] Iteration 49670 (1.40644 iter/s, 7.11014s/10 iters), loss = 6.07446
I0524 07:40:00.404244 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07446 (* 1 = 6.07446 loss)
I0524 07:40:00.404678 11835 sgd_solver.cpp:112] Iteration 49670, lr = 0.1
I0524 07:40:08.421587 11835 solver.cpp:239] Iteration 49680 (1.24735 iter/s, 8.01703s/10 iters), loss = 5.93773
I0524 07:40:08.421639 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93773 (* 1 = 5.93773 loss)
I0524 07:40:09.585315 11835 sgd_solver.cpp:112] Iteration 49680, lr = 0.1
I0524 07:40:16.463531 11835 solver.cpp:239] Iteration 49690 (1.24354 iter/s, 8.04157s/10 iters), loss = 5.6289
I0524 07:40:16.463593 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.6289 (* 1 = 5.6289 loss)
I0524 07:40:16.465524 11835 sgd_solver.cpp:112] Iteration 49690, lr = 0.1
I0524 07:40:24.754142 11835 solver.cpp:239] Iteration 49700 (1.20624 iter/s, 8.29024s/10 iters), loss = 6.09622
I0524 07:40:24.754371 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09622 (* 1 = 6.09622 loss)
I0524 07:40:24.754425 11835 sgd_solver.cpp:112] Iteration 49700, lr = 0.1
I0524 07:40:30.964310 11835 solver.cpp:239] Iteration 49710 (1.61091 iter/s, 6.20766s/10 iters), loss = 5.91322
I0524 07:40:30.964383 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91322 (* 1 = 5.91322 loss)
I0524 07:40:30.964432 11835 sgd_solver.cpp:112] Iteration 49710, lr = 0.1
I0524 07:40:38.276780 11835 solver.cpp:239] Iteration 49720 (1.36762 iter/s, 7.31197s/10 iters), loss = 6.33588
I0524 07:40:38.276823 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33588 (* 1 = 6.33588 loss)
I0524 07:40:38.276835 11835 sgd_solver.cpp:112] Iteration 49720, lr = 0.1
I0524 07:40:44.859120 11835 solver.cpp:239] Iteration 49730 (1.51931 iter/s, 6.58194s/10 iters), loss = 6.25436
I0524 07:40:44.859236 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25436 (* 1 = 6.25436 loss)
I0524 07:40:44.859359 11835 sgd_solver.cpp:112] Iteration 49730, lr = 0.1
I0524 07:40:50.657311 11835 solver.cpp:239] Iteration 49740 (1.72477 iter/s, 5.79788s/10 iters), loss = 5.88754
I0524 07:40:50.657358 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88754 (* 1 = 5.88754 loss)
I0524 07:40:50.657518 11835 sgd_solver.cpp:112] Iteration 49740, lr = 0.1
I0524 07:40:56.993492 11835 solver.cpp:239] Iteration 49750 (1.57831 iter/s, 6.33588s/10 iters), loss = 6.29588
I0524 07:40:56.993710 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29588 (* 1 = 6.29588 loss)
I0524 07:40:56.993806 11835 sgd_solver.cpp:112] Iteration 49750, lr = 0.1
I0524 07:41:03.804380 11835 solver.cpp:239] Iteration 49760 (1.46834 iter/s, 6.81042s/10 iters), loss = 5.75809
I0524 07:41:03.804446 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75809 (* 1 = 5.75809 loss)
I0524 07:41:03.804468 11835 sgd_solver.cpp:112] Iteration 49760, lr = 0.1
I0524 07:41:14.432621 11835 solver.cpp:239] Iteration 49770 (0.941027 iter/s, 10.6267s/10 iters), loss = 5.86666
I0524 07:41:14.432669 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86666 (* 1 = 5.86666 loss)
I0524 07:41:14.980137 11835 sgd_solver.cpp:112] Iteration 49770, lr = 0.1
I0524 07:41:22.669265 11835 solver.cpp:239] Iteration 49780 (1.21414 iter/s, 8.23627s/10 iters), loss = 5.4145
I0524 07:41:22.669327 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.4145 (* 1 = 5.4145 loss)
I0524 07:41:22.669497 11835 sgd_solver.cpp:112] Iteration 49780, lr = 0.1
I0524 07:41:28.810173 11835 solver.cpp:239] Iteration 49790 (1.62851 iter/s, 6.1406s/10 iters), loss = 6.37282
I0524 07:41:28.810467 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37282 (* 1 = 6.37282 loss)
I0524 07:41:29.295745 11835 sgd_solver.cpp:112] Iteration 49790, lr = 0.1
I0524 07:41:35.767935 11835 solver.cpp:239] Iteration 49800 (1.43735 iter/s, 6.95723s/10 iters), loss = 6.03595
I0524 07:41:35.768002 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03595 (* 1 = 6.03595 loss)
I0524 07:41:35.768105 11835 sgd_solver.cpp:112] Iteration 49800, lr = 0.1
I0524 07:41:42.105872 11835 solver.cpp:239] Iteration 49810 (1.57788 iter/s, 6.33763s/10 iters), loss = 5.27657
I0524 07:41:42.105955 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.27657 (* 1 = 5.27657 loss)
I0524 07:41:42.302942 11835 sgd_solver.cpp:112] Iteration 49810, lr = 0.1
I0524 07:41:48.417435 11835 solver.cpp:239] Iteration 49820 (1.58447 iter/s, 6.31126s/10 iters), loss = 5.46951
I0524 07:41:48.417484 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.46951 (* 1 = 5.46951 loss)
I0524 07:41:48.417510 11835 sgd_solver.cpp:112] Iteration 49820, lr = 0.1
I0524 07:41:55.283634 11835 solver.cpp:239] Iteration 49830 (1.45648 iter/s, 6.86587s/10 iters), loss = 6.06569
I0524 07:41:55.283689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06569 (* 1 = 6.06569 loss)
I0524 07:41:55.283843 11835 sgd_solver.cpp:112] Iteration 49830, lr = 0.1
I0524 07:42:02.294060 11835 solver.cpp:239] Iteration 49840 (1.42651 iter/s, 7.01011s/10 iters), loss = 5.78086
I0524 07:42:02.294209 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78086 (* 1 = 5.78086 loss)
I0524 07:42:02.388772 11835 sgd_solver.cpp:112] Iteration 49840, lr = 0.1
I0524 07:42:08.960319 11835 solver.cpp:239] Iteration 49850 (1.50019 iter/s, 6.66584s/10 iters), loss = 5.88123
I0524 07:42:08.960371 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88123 (* 1 = 5.88123 loss)
I0524 07:42:08.987541 11835 sgd_solver.cpp:112] Iteration 49850, lr = 0.1
I0524 07:42:15.236599 11835 solver.cpp:239] Iteration 49860 (1.59338 iter/s, 6.27598s/10 iters), loss = 5.31702
I0524 07:42:15.236657 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.31702 (* 1 = 5.31702 loss)
I0524 07:42:15.236826 11835 sgd_solver.cpp:112] Iteration 49860, lr = 0.1
I0524 07:42:24.572038 11835 solver.cpp:239] Iteration 49870 (1.07124 iter/s, 9.33495s/10 iters), loss = 6.76601
I0524 07:42:24.572161 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76601 (* 1 = 6.76601 loss)
I0524 07:42:25.874366 11835 sgd_solver.cpp:112] Iteration 49870, lr = 0.1
I0524 07:42:33.225226 11835 solver.cpp:239] Iteration 49880 (1.1557 iter/s, 8.65276s/10 iters), loss = 6.73772
I0524 07:42:33.225344 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73772 (* 1 = 6.73772 loss)
I0524 07:42:33.225589 11835 sgd_solver.cpp:112] Iteration 49880, lr = 0.1
I0524 07:42:41.108901 11835 solver.cpp:239] Iteration 49890 (1.26852 iter/s, 7.88323s/10 iters), loss = 6.04449
I0524 07:42:41.108980 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04449 (* 1 = 6.04449 loss)
I0524 07:42:41.108995 11835 sgd_solver.cpp:112] Iteration 49890, lr = 0.1
I0524 07:42:48.871592 11835 solver.cpp:239] Iteration 49900 (1.28827 iter/s, 7.76232s/10 iters), loss = 5.51442
I0524 07:42:48.871644 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.51442 (* 1 = 5.51442 loss)
I0524 07:42:48.871868 11835 sgd_solver.cpp:112] Iteration 49900, lr = 0.1
I0524 07:42:54.872872 11835 solver.cpp:239] Iteration 49910 (1.66639 iter/s, 6.001s/10 iters), loss = 5.14421
I0524 07:42:54.872911 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.14421 (* 1 = 5.14421 loss)
I0524 07:42:55.007215 11835 sgd_solver.cpp:112] Iteration 49910, lr = 0.1
I0524 07:43:01.818964 11835 solver.cpp:239] Iteration 49920 (1.43973 iter/s, 6.94576s/10 iters), loss = 6.32322
I0524 07:43:01.819020 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32322 (* 1 = 6.32322 loss)
I0524 07:43:01.819134 11835 sgd_solver.cpp:112] Iteration 49920, lr = 0.1
I0524 07:43:08.302714 11835 solver.cpp:239] Iteration 49930 (1.54239 iter/s, 6.48344s/10 iters), loss = 6.77477
I0524 07:43:08.302937 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.77477 (* 1 = 6.77477 loss)
I0524 07:43:08.302975 11835 sgd_solver.cpp:112] Iteration 49930, lr = 0.1
I0524 07:43:15.137960 11835 solver.cpp:239] Iteration 49940 (1.46334 iter/s, 6.83368s/10 iters), loss = 6.6353
I0524 07:43:15.138022 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6353 (* 1 = 6.6353 loss)
I0524 07:43:15.965564 11835 sgd_solver.cpp:112] Iteration 49940, lr = 0.1
I0524 07:43:22.859686 11835 solver.cpp:239] Iteration 49950 (1.29511 iter/s, 7.72134s/10 iters), loss = 6.15922
I0524 07:43:22.859756 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15922 (* 1 = 6.15922 loss)
I0524 07:43:22.870021 11835 sgd_solver.cpp:112] Iteration 49950, lr = 0.1
I0524 07:43:31.234202 11835 solver.cpp:239] Iteration 49960 (1.19415 iter/s, 8.37414s/10 iters), loss = 6.96954
I0524 07:43:31.234252 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.96954 (* 1 = 6.96954 loss)
I0524 07:43:31.347051 11835 sgd_solver.cpp:112] Iteration 49960, lr = 0.1
I0524 07:43:37.317036 11835 solver.cpp:239] Iteration 49970 (1.64405 iter/s, 6.08255s/10 iters), loss = 6.5517
I0524 07:43:37.317086 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5517 (* 1 = 6.5517 loss)
I0524 07:43:37.937039 11835 sgd_solver.cpp:112] Iteration 49970, lr = 0.1
I0524 07:43:45.231706 11835 solver.cpp:239] Iteration 49980 (1.26354 iter/s, 7.9143s/10 iters), loss = 6.3289
I0524 07:43:45.231942 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3289 (* 1 = 6.3289 loss)
I0524 07:43:45.701709 11835 sgd_solver.cpp:112] Iteration 49980, lr = 0.1
I0524 07:43:51.505268 11835 solver.cpp:239] Iteration 49990 (1.5941 iter/s, 6.27311s/10 iters), loss = 6.84739
I0524 07:43:51.505308 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84739 (* 1 = 6.84739 loss)
I0524 07:43:51.505321 11835 sgd_solver.cpp:112] Iteration 49990, lr = 0.1
I0524 07:43:56.722872 11835 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_50000.caffemodel
I0524 07:43:58.421625 11835 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_50000.solverstate
I0524 07:43:59.080425 11835 solver.cpp:239] Iteration 50000 (1.32017 iter/s, 7.57476s/10 iters), loss = 6.08426
I0524 07:43:59.080466 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08426 (* 1 = 6.08426 loss)
I0524 07:43:59.090324 11835 sgd_solver.cpp:112] Iteration 50000, lr = 0.1
I0524 07:44:05.757697 11835 solver.cpp:239] Iteration 50010 (1.49769 iter/s, 6.67696s/10 iters), loss = 6.91821
I0524 07:44:05.757751 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.91821 (* 1 = 6.91821 loss)
I0524 07:44:05.758067 11835 sgd_solver.cpp:112] Iteration 50010, lr = 0.1
I0524 07:44:13.070246 11835 solver.cpp:239] Iteration 50020 (1.36758 iter/s, 7.31221s/10 iters), loss = 7.55922
I0524 07:44:13.070303 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.55922 (* 1 = 7.55922 loss)
I0524 07:44:13.080835 11835 sgd_solver.cpp:112] Iteration 50020, lr = 0.1
I0524 07:44:19.194943 11835 solver.cpp:239] Iteration 50030 (1.63281 iter/s, 6.1244s/10 iters), loss = 6.41763
I0524 07:44:19.195200 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41763 (* 1 = 6.41763 loss)
I0524 07:44:19.199836 11835 sgd_solver.cpp:112] Iteration 50030, lr = 0.1
I0524 07:44:26.222018 11835 solver.cpp:239] Iteration 50040 (1.42317 iter/s, 7.02659s/10 iters), loss = 6.88048
I0524 07:44:26.222059 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88048 (* 1 = 6.88048 loss)
I0524 07:44:26.550662 11835 sgd_solver.cpp:112] Iteration 50040, lr = 0.1
I0524 07:44:33.588776 11835 solver.cpp:239] Iteration 50050 (1.35751 iter/s, 7.36641s/10 iters), loss = 5.89308
I0524 07:44:33.588948 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89308 (* 1 = 5.89308 loss)
I0524 07:44:34.037576 11835 sgd_solver.cpp:112] Iteration 50050, lr = 0.1
I0524 07:44:41.269071 11835 solver.cpp:239] Iteration 50060 (1.3021 iter/s, 7.6799s/10 iters), loss = 5.50253
I0524 07:44:41.269124 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.50253 (* 1 = 5.50253 loss)
I0524 07:44:41.279299 11835 sgd_solver.cpp:112] Iteration 50060, lr = 0.1
I0524 07:44:49.051632 11835 solver.cpp:239] Iteration 50070 (1.28498 iter/s, 7.78221s/10 iters), loss = 8.10797
I0524 07:44:49.051686 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.10797 (* 1 = 8.10797 loss)
I0524 07:44:49.052055 11835 sgd_solver.cpp:112] Iteration 50070, lr = 0.1
I0524 07:44:55.101882 11835 solver.cpp:239] Iteration 50080 (1.6529 iter/s, 6.04995s/10 iters), loss = 6.07625
I0524 07:44:55.102170 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07625 (* 1 = 6.07625 loss)
I0524 07:44:55.268443 11835 sgd_solver.cpp:112] Iteration 50080, lr = 0.1
I0524 07:45:01.459363 11835 solver.cpp:239] Iteration 50090 (1.57306 iter/s, 6.35704s/10 iters), loss = 5.69414
I0524 07:45:01.459398 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.69414 (* 1 = 5.69414 loss)
I0524 07:45:01.718430 11835 sgd_solver.cpp:112] Iteration 50090, lr = 0.1
I0524 07:45:09.018615 11835 solver.cpp:239] Iteration 50100 (1.32295 iter/s, 7.55889s/10 iters), loss = 6.64524
I0524 07:45:09.018673 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64524 (* 1 = 6.64524 loss)
I0524 07:45:09.039602 11835 sgd_solver.cpp:112] Iteration 50100, lr = 0.1
I0524 07:45:15.028720 11835 solver.cpp:239] Iteration 50110 (1.66394 iter/s, 6.00982s/10 iters), loss = 6.21311
I0524 07:45:15.028785 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21311 (* 1 = 6.21311 loss)
I0524 07:45:15.029027 11835 sgd_solver.cpp:112] Iteration 50110, lr = 0.1
I0524 07:45:21.194002 11835 solver.cpp:239] Iteration 50120 (1.62206 iter/s, 6.16499s/10 iters), loss = 6.45787
I0524 07:45:21.194063 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45787 (* 1 = 6.45787 loss)
I0524 07:45:21.194519 11835 sgd_solver.cpp:112] Iteration 50120, lr = 0.1
I0524 07:45:28.902663 11835 solver.cpp:239] Iteration 50130 (1.2973 iter/s, 7.70833s/10 iters), loss = 6.23889
I0524 07:45:28.902912 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23889 (* 1 = 6.23889 loss)
I0524 07:45:29.541247 11835 sgd_solver.cpp:112] Iteration 50130, lr = 0.1
I0524 07:45:35.536448 11835 solver.cpp:239] Iteration 50140 (1.50754 iter/s, 6.6333s/10 iters), loss = 5.87917
I0524 07:45:35.536520 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87917 (* 1 = 5.87917 loss)
I0524 07:45:36.104787 11835 sgd_solver.cpp:112] Iteration 50140, lr = 0.1
I0524 07:45:42.879859 11835 solver.cpp:239] Iteration 50150 (1.36183 iter/s, 7.34307s/10 iters), loss = 5.95187
I0524 07:45:42.879911 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95187 (* 1 = 5.95187 loss)
I0524 07:45:42.880038 11835 sgd_solver.cpp:112] Iteration 50150, lr = 0.1
I0524 07:45:52.746229 11835 solver.cpp:239] Iteration 50160 (1.01359 iter/s, 9.86595s/10 iters), loss = 5.98668
I0524 07:45:52.746281 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98668 (* 1 = 5.98668 loss)
I0524 07:45:53.121047 11835 sgd_solver.cpp:112] Iteration 50160, lr = 0.1
I0524 07:46:04.049621 11835 solver.cpp:239] Iteration 50170 (0.884729 iter/s, 11.3029s/10 iters), loss = 6.99859
I0524 07:46:04.049885 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.99859 (* 1 = 6.99859 loss)
I0524 07:46:04.049904 11835 sgd_solver.cpp:112] Iteration 50170, lr = 0.1
I0524 07:46:11.061805 11835 solver.cpp:239] Iteration 50180 (1.42621 iter/s, 7.0116s/10 iters), loss = 6.92181
I0524 07:46:11.061864 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92181 (* 1 = 6.92181 loss)
I0524 07:46:11.200022 11835 sgd_solver.cpp:112] Iteration 50180, lr = 0.1
I0524 07:46:18.416805 11835 solver.cpp:239] Iteration 50190 (1.35968 iter/s, 7.35465s/10 iters), loss = 5.18872
I0524 07:46:18.416862 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.18872 (* 1 = 5.18872 loss)
I0524 07:46:18.416880 11835 sgd_solver.cpp:112] Iteration 50190, lr = 0.1
I0524 07:46:25.689865 11835 solver.cpp:239] Iteration 50200 (1.375 iter/s, 7.27272s/10 iters), loss = 7.01083
I0524 07:46:25.689924 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01083 (* 1 = 7.01083 loss)
I0524 07:46:25.689947 11835 sgd_solver.cpp:112] Iteration 50200, lr = 0.1
I0524 07:46:33.004427 11835 solver.cpp:239] Iteration 50210 (1.3672 iter/s, 7.31423s/10 iters), loss = 5.65675
I0524 07:46:33.004472 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.65675 (* 1 = 5.65675 loss)
I0524 07:46:33.004698 11835 sgd_solver.cpp:112] Iteration 50210, lr = 0.1
I0524 07:46:41.001111 11835 solver.cpp:239] Iteration 50220 (1.25058 iter/s, 7.99632s/10 iters), loss = 5.62347
I0524 07:46:41.001292 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.62347 (* 1 = 5.62347 loss)
I0524 07:46:41.001562 11835 sgd_solver.cpp:112] Iteration 50220, lr = 0.1
I0524 07:46:47.899927 11835 solver.cpp:239] Iteration 50230 (1.44961 iter/s, 6.89841s/10 iters), loss = 5.81011
I0524 07:46:47.899968 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81011 (* 1 = 5.81011 loss)
I0524 07:46:48.279037 11835 sgd_solver.cpp:112] Iteration 50230, lr = 0.1
I0524 07:46:55.489629 11835 solver.cpp:239] Iteration 50240 (1.31764 iter/s, 7.58935s/10 iters), loss = 6.00154
I0524 07:46:55.489696 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00154 (* 1 = 6.00154 loss)
I0524 07:46:55.489907 11835 sgd_solver.cpp:112] Iteration 50240, lr = 0.1
I0524 07:47:01.586273 11835 solver.cpp:239] Iteration 50250 (1.64033 iter/s, 6.09634s/10 iters), loss = 5.83147
I0524 07:47:01.586324 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83147 (* 1 = 5.83147 loss)
I0524 07:47:01.790041 11835 sgd_solver.cpp:112] Iteration 50250, lr = 0.1
I0524 07:47:08.430193 11835 solver.cpp:239] Iteration 50260 (1.46122 iter/s, 6.84361s/10 iters), loss = 5.72236
I0524 07:47:08.430243 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72236 (* 1 = 5.72236 loss)
I0524 07:47:08.430371 11835 sgd_solver.cpp:112] Iteration 50260, lr = 0.1
I0524 07:47:14.515929 11835 solver.cpp:239] Iteration 50270 (1.64327 iter/s, 6.08542s/10 iters), loss = 6.93546
I0524 07:47:14.516175 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93546 (* 1 = 6.93546 loss)
I0524 07:47:14.604182 11835 sgd_solver.cpp:112] Iteration 50270, lr = 0.1
I0524 07:47:21.881045 11835 solver.cpp:239] Iteration 50280 (1.35784 iter/s, 7.36463s/10 iters), loss = 7.04976
I0524 07:47:21.881100 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04976 (* 1 = 7.04976 loss)
I0524 07:47:21.881296 11835 sgd_solver.cpp:112] Iteration 50280, lr = 0.1
I0524 07:47:27.771734 11835 solver.cpp:239] Iteration 50290 (1.69767 iter/s, 5.89041s/10 iters), loss = 6.4889
I0524 07:47:27.771775 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4889 (* 1 = 6.4889 loss)
I0524 07:47:28.153298 11835 sgd_solver.cpp:112] Iteration 50290, lr = 0.1
I0524 07:47:34.387822 11835 solver.cpp:239] Iteration 50300 (1.51154 iter/s, 6.61578s/10 iters), loss = 6.13256
I0524 07:47:34.387895 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13256 (* 1 = 6.13256 loss)
I0524 07:47:34.388239 11835 sgd_solver.cpp:112] Iteration 50300, lr = 0.1
I0524 07:47:43.915904 11835 solver.cpp:239] Iteration 50310 (1.04957 iter/s, 9.52767s/10 iters), loss = 5.75969
I0524 07:47:43.915941 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75969 (* 1 = 5.75969 loss)
I0524 07:47:44.590180 11835 sgd_solver.cpp:112] Iteration 50310, lr = 0.1
I0524 07:47:50.540112 11835 solver.cpp:239] Iteration 50320 (1.50969 iter/s, 6.6239s/10 iters), loss = 6.71968
I0524 07:47:50.540169 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71968 (* 1 = 6.71968 loss)
I0524 07:47:50.540206 11835 sgd_solver.cpp:112] Iteration 50320, lr = 0.1
I0524 07:47:56.493533 11835 solver.cpp:239] Iteration 50330 (1.67979 iter/s, 5.95314s/10 iters), loss = 6.13314
I0524 07:47:56.493580 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13314 (* 1 = 6.13314 loss)
I0524 07:47:56.807013 11835 sgd_solver.cpp:112] Iteration 50330, lr = 0.1
I0524 07:48:03.388445 11835 solver.cpp:239] Iteration 50340 (1.45041 iter/s, 6.89459s/10 iters), loss = 6.55355
I0524 07:48:03.388506 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55355 (* 1 = 6.55355 loss)
I0524 07:48:03.401561 11835 sgd_solver.cpp:112] Iteration 50340, lr = 0.1
I0524 07:48:09.434471 11835 solver.cpp:239] Iteration 50350 (1.65406 iter/s, 6.04574s/10 iters), loss = 5.58631
I0524 07:48:09.434525 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.58631 (* 1 = 5.58631 loss)
I0524 07:48:09.434538 11835 sgd_solver.cpp:112] Iteration 50350, lr = 0.1
I0524 07:48:16.965261 11835 solver.cpp:239] Iteration 50360 (1.32795 iter/s, 7.53039s/10 iters), loss = 6.19066
I0524 07:48:16.965509 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19066 (* 1 = 6.19066 loss)
I0524 07:48:16.965555 11835 sgd_solver.cpp:112] Iteration 50360, lr = 0.1
I0524 07:48:24.219367 11835 solver.cpp:239] Iteration 50370 (1.37863 iter/s, 7.25357s/10 iters), loss = 5.3871
I0524 07:48:24.219436 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.3871 (* 1 = 5.3871 loss)
I0524 07:48:24.219758 11835 sgd_solver.cpp:112] Iteration 50370, lr = 0.1
I0524 07:48:30.963812 11835 solver.cpp:239] Iteration 50380 (1.48277 iter/s, 6.74412s/10 iters), loss = 5.43034
I0524 07:48:30.963871 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43034 (* 1 = 5.43034 loss)
I0524 07:48:30.964136 11835 sgd_solver.cpp:112] Iteration 50380, lr = 0.1
I0524 07:48:37.335547 11835 solver.cpp:239] Iteration 50390 (1.5695 iter/s, 6.37144s/10 iters), loss = 6.3438
I0524 07:48:37.335598 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3438 (* 1 = 6.3438 loss)
I0524 07:48:37.335760 11835 sgd_solver.cpp:112] Iteration 50390, lr = 0.1
I0524 07:48:45.177567 11835 solver.cpp:239] Iteration 50400 (1.27524 iter/s, 7.84165s/10 iters), loss = 6.39441
I0524 07:48:45.177628 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39441 (* 1 = 6.39441 loss)
I0524 07:48:45.177825 11835 sgd_solver.cpp:112] Iteration 50400, lr = 0.1
I0524 07:48:54.469585 11835 solver.cpp:239] Iteration 50410 (1.07624 iter/s, 9.29161s/10 iters), loss = 5.32079
I0524 07:48:54.469759 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.32079 (* 1 = 5.32079 loss)
I0524 07:48:54.648633 11835 sgd_solver.cpp:112] Iteration 50410, lr = 0.1
I0524 07:49:01.911818 11835 solver.cpp:239] Iteration 50420 (1.34376 iter/s, 7.44178s/10 iters), loss = 6.4792
I0524 07:49:01.911864 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4792 (* 1 = 6.4792 loss)
I0524 07:49:01.932605 11835 sgd_solver.cpp:112] Iteration 50420, lr = 0.1
I0524 07:49:09.785116 11835 solver.cpp:239] Iteration 50430 (1.27017 iter/s, 7.87294s/10 iters), loss = 5.4845
I0524 07:49:09.785171 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.4845 (* 1 = 5.4845 loss)
I0524 07:49:09.799217 11835 sgd_solver.cpp:112] Iteration 50430, lr = 0.1
I0524 07:49:16.795608 11835 solver.cpp:239] Iteration 50440 (1.4265 iter/s, 7.01017s/10 iters), loss = 6.84647
I0524 07:49:16.795657 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84647 (* 1 = 6.84647 loss)
I0524 07:49:16.795899 11835 sgd_solver.cpp:112] Iteration 50440, lr = 0.1
I0524 07:49:22.340869 11835 solver.cpp:239] Iteration 50450 (1.80342 iter/s, 5.545s/10 iters), loss = 6.31533
I0524 07:49:22.340907 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31533 (* 1 = 6.31533 loss)
I0524 07:49:22.340920 11835 sgd_solver.cpp:112] Iteration 50450, lr = 0.1
I0524 07:49:29.456853 11835 solver.cpp:239] Iteration 50460 (1.40535 iter/s, 7.11566s/10 iters), loss = 6.07491
I0524 07:49:29.457037 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07491 (* 1 = 6.07491 loss)
I0524 07:49:29.457227 11835 sgd_solver.cpp:112] Iteration 50460, lr = 0.1
I0524 07:49:36.173316 11835 solver.cpp:239] Iteration 50470 (1.48898 iter/s, 6.71602s/10 iters), loss = 7.39087
I0524 07:49:36.173382 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.39087 (* 1 = 7.39087 loss)
I0524 07:49:36.919376 11835 sgd_solver.cpp:112] Iteration 50470, lr = 0.1
I0524 07:49:44.729594 11835 solver.cpp:239] Iteration 50480 (1.16878 iter/s, 8.5559s/10 iters), loss = 5.81298
I0524 07:49:44.729640 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81298 (* 1 = 5.81298 loss)
I0524 07:49:44.729655 11835 sgd_solver.cpp:112] Iteration 50480, lr = 0.1
I0524 07:49:52.022614 11835 solver.cpp:239] Iteration 50490 (1.37124 iter/s, 7.29269s/10 iters), loss = 5.48344
I0524 07:49:52.022656 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.48344 (* 1 = 5.48344 loss)
I0524 07:49:52.022716 11835 sgd_solver.cpp:112] Iteration 50490, lr = 0.1
I0524 07:49:59.267098 11835 solver.cpp:239] Iteration 50500 (1.38044 iter/s, 7.24407s/10 iters), loss = 6.59775
I0524 07:49:59.267172 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59775 (* 1 = 6.59775 loss)
I0524 07:49:59.470783 11835 sgd_solver.cpp:112] Iteration 50500, lr = 0.1
I0524 07:50:06.100069 11835 solver.cpp:239] Iteration 50510 (1.46356 iter/s, 6.83264s/10 iters), loss = 5.92592
I0524 07:50:06.100114 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92592 (* 1 = 5.92592 loss)
I0524 07:50:06.100368 11835 sgd_solver.cpp:112] Iteration 50510, lr = 0.1
I0524 07:50:12.370241 11835 solver.cpp:239] Iteration 50520 (1.59493 iter/s, 6.26988s/10 iters), loss = 6.17866
I0524 07:50:12.370291 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17866 (* 1 = 6.17866 loss)
I0524 07:50:12.373898 11835 sgd_solver.cpp:112] Iteration 50520, lr = 0.1
I0524 07:50:19.284057 11835 solver.cpp:239] Iteration 50530 (1.44645 iter/s, 6.9135s/10 iters), loss = 6.20842
I0524 07:50:19.284109 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20842 (* 1 = 6.20842 loss)
I0524 07:50:19.314446 11835 sgd_solver.cpp:112] Iteration 50530, lr = 0.1
I0524 07:50:25.074373 11835 solver.cpp:239] Iteration 50540 (1.7271 iter/s, 5.79004s/10 iters), loss = 4.46736
I0524 07:50:25.074415 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.46736 (* 1 = 4.46736 loss)
I0524 07:50:25.074451 11835 sgd_solver.cpp:112] Iteration 50540, lr = 0.1
I0524 07:50:31.631834 11835 solver.cpp:239] Iteration 50550 (1.52505 iter/s, 6.55715s/10 iters), loss = 6.0766
I0524 07:50:31.631999 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0766 (* 1 = 6.0766 loss)
I0524 07:50:31.632143 11835 sgd_solver.cpp:112] Iteration 50550, lr = 0.1
I0524 07:50:38.066634 11835 solver.cpp:239] Iteration 50560 (1.55415 iter/s, 6.43438s/10 iters), loss = 5.18856
I0524 07:50:38.066689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.18856 (* 1 = 5.18856 loss)
I0524 07:50:38.066788 11835 sgd_solver.cpp:112] Iteration 50560, lr = 0.1
I0524 07:50:46.616986 11835 solver.cpp:239] Iteration 50570 (1.16959 iter/s, 8.54997s/10 iters), loss = 5.64264
I0524 07:50:46.617036 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.64264 (* 1 = 5.64264 loss)
I0524 07:50:46.638720 11835 sgd_solver.cpp:112] Iteration 50570, lr = 0.1
I0524 07:50:52.686278 11835 solver.cpp:239] Iteration 50580 (1.64771 iter/s, 6.06901s/10 iters), loss = 6.67035
I0524 07:50:52.686336 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67035 (* 1 = 6.67035 loss)
I0524 07:50:52.686353 11835 sgd_solver.cpp:112] Iteration 50580, lr = 0.1
I0524 07:51:00.405787 11835 solver.cpp:239] Iteration 50590 (1.29549 iter/s, 7.71909s/10 iters), loss = 7.04695
I0524 07:51:00.405841 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04695 (* 1 = 7.04695 loss)
I0524 07:51:00.743815 11835 sgd_solver.cpp:112] Iteration 50590, lr = 0.1
I0524 07:51:06.367890 11835 solver.cpp:239] Iteration 50600 (1.67734 iter/s, 5.96181s/10 iters), loss = 6.98667
I0524 07:51:06.368183 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98667 (* 1 = 6.98667 loss)
I0524 07:51:06.368263 11835 sgd_solver.cpp:112] Iteration 50600, lr = 0.1
I0524 07:51:12.597314 11835 solver.cpp:239] Iteration 50610 (1.60567 iter/s, 6.22794s/10 iters), loss = 6.07867
I0524 07:51:12.597365 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07867 (* 1 = 6.07867 loss)
I0524 07:51:12.597543 11835 sgd_solver.cpp:112] Iteration 50610, lr = 0.1
I0524 07:51:19.000315 11835 solver.cpp:239] Iteration 50620 (1.56185 iter/s, 6.40268s/10 iters), loss = 6.45371
I0524 07:51:19.000458 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45371 (* 1 = 6.45371 loss)
I0524 07:51:19.000497 11835 sgd_solver.cpp:112] Iteration 50620, lr = 0.1
I0524 07:51:25.573757 11835 solver.cpp:239] Iteration 50630 (1.52135 iter/s, 6.57312s/10 iters), loss = 5.27635
I0524 07:51:25.573817 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.27635 (* 1 = 5.27635 loss)
I0524 07:51:26.097851 11835 sgd_solver.cpp:112] Iteration 50630, lr = 0.1
I0524 07:51:32.439924 11835 solver.cpp:239] Iteration 50640 (1.45648 iter/s, 6.86585s/10 iters), loss = 5.68729
I0524 07:51:32.440022 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68729 (* 1 = 5.68729 loss)
I0524 07:51:32.758617 11835 sgd_solver.cpp:112] Iteration 50640, lr = 0.1
I0524 07:51:40.542433 11835 solver.cpp:239] Iteration 50650 (1.23424 iter/s, 8.10215s/10 iters), loss = 6.43397
I0524 07:51:40.542585 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43397 (* 1 = 6.43397 loss)
I0524 07:51:41.346746 11835 sgd_solver.cpp:112] Iteration 50650, lr = 0.1
I0524 07:51:47.856899 11835 solver.cpp:239] Iteration 50660 (1.36724 iter/s, 7.31402s/10 iters), loss = 5.3548
I0524 07:51:47.856951 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.3548 (* 1 = 5.3548 loss)
I0524 07:51:47.857028 11835 sgd_solver.cpp:112] Iteration 50660, lr = 0.1
I0524 07:51:54.231784 11835 solver.cpp:239] Iteration 50670 (1.56873 iter/s, 6.37459s/10 iters), loss = 6.3332
I0524 07:51:54.231840 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3332 (* 1 = 6.3332 loss)
I0524 07:51:54.231854 11835 sgd_solver.cpp:112] Iteration 50670, lr = 0.1
I0524 07:52:01.527680 11835 solver.cpp:239] Iteration 50680 (1.3707 iter/s, 7.29556s/10 iters), loss = 5.38155
I0524 07:52:01.527750 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.38155 (* 1 = 5.38155 loss)
I0524 07:52:01.527772 11835 sgd_solver.cpp:112] Iteration 50680, lr = 0.1
I0524 07:52:10.817070 11835 solver.cpp:239] Iteration 50690 (1.07661 iter/s, 9.28842s/10 iters), loss = 5.06642
I0524 07:52:10.817294 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.06642 (* 1 = 5.06642 loss)
I0524 07:52:10.843986 11835 sgd_solver.cpp:112] Iteration 50690, lr = 0.1
I0524 07:52:17.318452 11835 solver.cpp:239] Iteration 50700 (1.53824 iter/s, 6.50093s/10 iters), loss = 6.175
I0524 07:52:17.318501 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.175 (* 1 = 6.175 loss)
I0524 07:52:17.318557 11835 sgd_solver.cpp:112] Iteration 50700, lr = 0.1
I0524 07:52:24.336452 11835 solver.cpp:239] Iteration 50710 (1.42497 iter/s, 7.01768s/10 iters), loss = 6.45206
I0524 07:52:24.336499 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45206 (* 1 = 6.45206 loss)
I0524 07:52:24.561888 11835 sgd_solver.cpp:112] Iteration 50710, lr = 0.1
I0524 07:52:32.154382 11835 solver.cpp:239] Iteration 50720 (1.27917 iter/s, 7.81759s/10 iters), loss = 6.59704
I0524 07:52:32.154423 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59704 (* 1 = 6.59704 loss)
I0524 07:52:32.154762 11835 sgd_solver.cpp:112] Iteration 50720, lr = 0.1
I0524 07:52:38.217550 11835 solver.cpp:239] Iteration 50730 (1.64938 iter/s, 6.06289s/10 iters), loss = 6.40208
I0524 07:52:38.217617 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40208 (* 1 = 6.40208 loss)
I0524 07:52:38.217635 11835 sgd_solver.cpp:112] Iteration 50730, lr = 0.1
I0524 07:52:44.019765 11835 solver.cpp:239] Iteration 50740 (1.72356 iter/s, 5.80193s/10 iters), loss = 6.39872
I0524 07:52:44.020181 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39872 (* 1 = 6.39872 loss)
I0524 07:52:44.020207 11835 sgd_solver.cpp:112] Iteration 50740, lr = 0.1
I0524 07:52:52.591838 11835 solver.cpp:239] Iteration 50750 (1.16697 iter/s, 8.56923s/10 iters), loss = 6.16534
I0524 07:52:52.591890 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16534 (* 1 = 6.16534 loss)
I0524 07:52:52.847520 11835 sgd_solver.cpp:112] Iteration 50750, lr = 0.1
I0524 07:52:59.039538 11835 solver.cpp:239] Iteration 50760 (1.55103 iter/s, 6.44734s/10 iters), loss = 6.78649
I0524 07:52:59.039662 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78649 (* 1 = 6.78649 loss)
I0524 07:53:00.567039 11835 sgd_solver.cpp:112] Iteration 50760, lr = 0.1
I0524 07:53:07.618613 11835 solver.cpp:239] Iteration 50770 (1.16568 iter/s, 8.57866s/10 iters), loss = 5.77402
I0524 07:53:07.618672 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.77402 (* 1 = 5.77402 loss)
I0524 07:53:07.642223 11835 sgd_solver.cpp:112] Iteration 50770, lr = 0.1
I0524 07:53:15.344728 11835 solver.cpp:239] Iteration 50780 (1.29437 iter/s, 7.72575s/10 iters), loss = 5.80526
I0524 07:53:15.344904 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80526 (* 1 = 5.80526 loss)
I0524 07:53:15.344925 11835 sgd_solver.cpp:112] Iteration 50780, lr = 0.1
I0524 07:53:23.324394 11835 solver.cpp:239] Iteration 50790 (1.25326 iter/s, 7.97918s/10 iters), loss = 6.91761
I0524 07:53:23.324453 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.91761 (* 1 = 6.91761 loss)
I0524 07:53:23.324529 11835 sgd_solver.cpp:112] Iteration 50790, lr = 0.1
I0524 07:53:29.532109 11835 solver.cpp:239] Iteration 50800 (1.61097 iter/s, 6.20743s/10 iters), loss = 5.97031
I0524 07:53:29.532155 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97031 (* 1 = 5.97031 loss)
I0524 07:53:29.542104 11835 sgd_solver.cpp:112] Iteration 50800, lr = 0.1
I0524 07:53:35.923025 11835 solver.cpp:239] Iteration 50810 (1.56479 iter/s, 6.39062s/10 iters), loss = 6.5929
I0524 07:53:35.923081 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5929 (* 1 = 6.5929 loss)
I0524 07:53:35.941712 11835 sgd_solver.cpp:112] Iteration 50810, lr = 0.1
I0524 07:53:42.584475 11835 solver.cpp:239] Iteration 50820 (1.50124 iter/s, 6.66114s/10 iters), loss = 6.03046
I0524 07:53:42.584517 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03046 (* 1 = 6.03046 loss)
I0524 07:53:42.584528 11835 sgd_solver.cpp:112] Iteration 50820, lr = 0.1
I0524 07:53:51.970633 11835 solver.cpp:239] Iteration 50830 (1.06545 iter/s, 9.38574s/10 iters), loss = 5.88773
I0524 07:53:51.970868 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88773 (* 1 = 5.88773 loss)
I0524 07:53:51.970950 11835 sgd_solver.cpp:112] Iteration 50830, lr = 0.1
I0524 07:53:58.035343 11835 solver.cpp:239] Iteration 50840 (1.64901 iter/s, 6.06423s/10 iters), loss = 7.47263
I0524 07:53:58.035403 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.47263 (* 1 = 7.47263 loss)
I0524 07:53:58.035665 11835 sgd_solver.cpp:112] Iteration 50840, lr = 0.1
I0524 07:54:08.068866 11835 solver.cpp:239] Iteration 50850 (0.996703 iter/s, 10.0331s/10 iters), loss = 5.51796
I0524 07:54:08.068928 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.51796 (* 1 = 5.51796 loss)
I0524 07:54:08.069233 11835 sgd_solver.cpp:112] Iteration 50850, lr = 0.1
I0524 07:54:16.075891 11835 solver.cpp:239] Iteration 50860 (1.24896 iter/s, 8.00666s/10 iters), loss = 6.45739
I0524 07:54:16.075939 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45739 (* 1 = 6.45739 loss)
I0524 07:54:16.076221 11835 sgd_solver.cpp:112] Iteration 50860, lr = 0.1
I0524 07:54:23.352039 11835 solver.cpp:239] Iteration 50870 (1.37442 iter/s, 7.2758s/10 iters), loss = 6.39199
I0524 07:54:23.352329 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39199 (* 1 = 6.39199 loss)
I0524 07:54:23.696481 11835 sgd_solver.cpp:112] Iteration 50870, lr = 0.1
I0524 07:54:31.853690 11835 solver.cpp:239] Iteration 50880 (1.17632 iter/s, 8.50106s/10 iters), loss = 6.22983
I0524 07:54:31.853741 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22983 (* 1 = 6.22983 loss)
I0524 07:54:31.857412 11835 sgd_solver.cpp:112] Iteration 50880, lr = 0.1
I0524 07:54:38.524238 11835 solver.cpp:239] Iteration 50890 (1.4992 iter/s, 6.67024s/10 iters), loss = 5.67114
I0524 07:54:38.524303 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67114 (* 1 = 5.67114 loss)
I0524 07:54:38.690969 11835 sgd_solver.cpp:112] Iteration 50890, lr = 0.1
I0524 07:54:46.184643 11835 solver.cpp:239] Iteration 50900 (1.30548 iter/s, 7.66004s/10 iters), loss = 6.5243
I0524 07:54:46.184700 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5243 (* 1 = 6.5243 loss)
I0524 07:54:46.184814 11835 sgd_solver.cpp:112] Iteration 50900, lr = 0.1
I0524 07:54:52.851575 11835 solver.cpp:239] Iteration 50910 (1.50001 iter/s, 6.66662s/10 iters), loss = 5.18886
I0524 07:54:52.851646 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.18886 (* 1 = 5.18886 loss)
I0524 07:54:53.081519 11835 sgd_solver.cpp:112] Iteration 50910, lr = 0.1
I0524 07:55:00.029966 11835 solver.cpp:239] Iteration 50920 (1.39314 iter/s, 7.17804s/10 iters), loss = 4.85856
I0524 07:55:00.030107 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.85856 (* 1 = 4.85856 loss)
I0524 07:55:00.052817 11835 sgd_solver.cpp:112] Iteration 50920, lr = 0.1
I0524 07:55:07.112977 11835 solver.cpp:239] Iteration 50930 (1.41191 iter/s, 7.0826s/10 iters), loss = 6.31501
I0524 07:55:07.113029 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31501 (* 1 = 6.31501 loss)
I0524 07:55:08.501166 11835 sgd_solver.cpp:112] Iteration 50930, lr = 0.1
I0524 07:55:17.210876 11835 solver.cpp:239] Iteration 50940 (0.990348 iter/s, 10.0975s/10 iters), loss = 6.49469
I0524 07:55:17.210937 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49469 (* 1 = 6.49469 loss)
I0524 07:55:17.210956 11835 sgd_solver.cpp:112] Iteration 50940, lr = 0.1
I0524 07:55:24.454244 11835 solver.cpp:239] Iteration 50950 (1.38067 iter/s, 7.24284s/10 iters), loss = 7.13808
I0524 07:55:24.454303 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.13808 (* 1 = 7.13808 loss)
I0524 07:55:25.839701 11835 sgd_solver.cpp:112] Iteration 50950, lr = 0.1
I0524 07:55:32.901536 11835 solver.cpp:239] Iteration 50960 (1.18387 iter/s, 8.44691s/10 iters), loss = 5.56361
I0524 07:55:32.901772 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56361 (* 1 = 5.56361 loss)
I0524 07:55:32.901805 11835 sgd_solver.cpp:112] Iteration 50960, lr = 0.1
I0524 07:55:39.547530 11835 solver.cpp:239] Iteration 50970 (1.50477 iter/s, 6.64555s/10 iters), loss = 6.40755
I0524 07:55:39.547569 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40755 (* 1 = 6.40755 loss)
I0524 07:55:39.547714 11835 sgd_solver.cpp:112] Iteration 50970, lr = 0.1
I0524 07:55:45.734822 11835 solver.cpp:239] Iteration 50980 (1.61629 iter/s, 6.187s/10 iters), loss = 5.06963
I0524 07:55:45.734876 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.06963 (* 1 = 5.06963 loss)
I0524 07:55:45.862432 11835 sgd_solver.cpp:112] Iteration 50980, lr = 0.1
I0524 07:55:53.343598 11835 solver.cpp:239] Iteration 50990 (1.31433 iter/s, 7.60843s/10 iters), loss = 6.26119
I0524 07:55:53.343646 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26119 (* 1 = 6.26119 loss)
I0524 07:55:53.343979 11835 sgd_solver.cpp:112] Iteration 50990, lr = 0.1
I0524 07:56:01.266183 11835 solver.cpp:239] Iteration 51000 (1.26227 iter/s, 7.92223s/10 iters), loss = 6.44622
I0524 07:56:01.266255 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44622 (* 1 = 6.44622 loss)
I0524 07:56:01.266443 11835 sgd_solver.cpp:112] Iteration 51000, lr = 0.1
I0524 07:56:09.395627 11835 solver.cpp:239] Iteration 51010 (1.23016 iter/s, 8.12906s/10 iters), loss = 6.8883
I0524 07:56:09.395884 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8883 (* 1 = 6.8883 loss)
I0524 07:56:09.395907 11835 sgd_solver.cpp:112] Iteration 51010, lr = 0.1
I0524 07:56:18.121701 11835 solver.cpp:239] Iteration 51020 (1.14608 iter/s, 8.72539s/10 iters), loss = 5.74957
I0524 07:56:18.121755 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74957 (* 1 = 5.74957 loss)
I0524 07:56:18.128082 11835 sgd_solver.cpp:112] Iteration 51020, lr = 0.1
I0524 07:56:24.147465 11835 solver.cpp:239] Iteration 51030 (1.65962 iter/s, 6.02547s/10 iters), loss = 6.47526
I0524 07:56:24.147532 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.47526 (* 1 = 6.47526 loss)
I0524 07:56:24.147657 11835 sgd_solver.cpp:112] Iteration 51030, lr = 0.1
I0524 07:56:32.090608 11835 solver.cpp:239] Iteration 51040 (1.259 iter/s, 7.94279s/10 iters), loss = 6.27708
I0524 07:56:32.090649 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27708 (* 1 = 6.27708 loss)
I0524 07:56:32.099303 11835 sgd_solver.cpp:112] Iteration 51040, lr = 0.1
I0524 07:56:39.348840 11835 solver.cpp:239] Iteration 51050 (1.37781 iter/s, 7.2579s/10 iters), loss = 6.08399
I0524 07:56:39.348892 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08399 (* 1 = 6.08399 loss)
I0524 07:56:40.261572 11835 sgd_solver.cpp:112] Iteration 51050, lr = 0.1
I0524 07:56:47.543613 11835 solver.cpp:239] Iteration 51060 (1.22034 iter/s, 8.19441s/10 iters), loss = 6.05078
I0524 07:56:47.543665 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05078 (* 1 = 6.05078 loss)
I0524 07:56:47.543885 11835 sgd_solver.cpp:112] Iteration 51060, lr = 0.1
I0524 07:56:53.998518 11835 solver.cpp:239] Iteration 51070 (1.54928 iter/s, 6.4546s/10 iters), loss = 5.86424
I0524 07:56:53.998569 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86424 (* 1 = 5.86424 loss)
I0524 07:56:55.373142 11835 sgd_solver.cpp:112] Iteration 51070, lr = 0.1
I0524 07:57:02.616853 11835 solver.cpp:239] Iteration 51080 (1.16037 iter/s, 8.61794s/10 iters), loss = 6.49477
I0524 07:57:02.616904 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49477 (* 1 = 6.49477 loss)
I0524 07:57:02.616940 11835 sgd_solver.cpp:112] Iteration 51080, lr = 0.1
I0524 07:57:12.766883 11835 solver.cpp:239] Iteration 51090 (0.985261 iter/s, 10.1496s/10 iters), loss = 5.5053
I0524 07:57:12.767128 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.5053 (* 1 = 5.5053 loss)
I0524 07:57:12.771349 11835 sgd_solver.cpp:112] Iteration 51090, lr = 0.1
I0524 07:57:20.112469 11835 solver.cpp:239] Iteration 51100 (1.36146 iter/s, 7.34508s/10 iters), loss = 6.12088
I0524 07:57:20.112525 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12088 (* 1 = 6.12088 loss)
I0524 07:57:20.112668 11835 sgd_solver.cpp:112] Iteration 51100, lr = 0.1
I0524 07:57:29.301189 11835 solver.cpp:239] Iteration 51110 (1.08834 iter/s, 9.18831s/10 iters), loss = 6.18292
I0524 07:57:29.301239 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18292 (* 1 = 6.18292 loss)
I0524 07:57:29.341630 11835 sgd_solver.cpp:112] Iteration 51110, lr = 0.1
I0524 07:57:36.692215 11835 solver.cpp:239] Iteration 51120 (1.35306 iter/s, 7.39068s/10 iters), loss = 6.04465
I0524 07:57:36.692286 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04465 (* 1 = 6.04465 loss)
I0524 07:57:36.692431 11835 sgd_solver.cpp:112] Iteration 51120, lr = 0.1
I0524 07:57:42.905436 11835 solver.cpp:239] Iteration 51130 (1.60955 iter/s, 6.21292s/10 iters), loss = 7.49181
I0524 07:57:42.905645 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.49181 (* 1 = 7.49181 loss)
I0524 07:57:42.980420 11835 sgd_solver.cpp:112] Iteration 51130, lr = 0.1
I0524 07:57:49.212544 11835 solver.cpp:239] Iteration 51140 (1.58563 iter/s, 6.30664s/10 iters), loss = 5.97487
I0524 07:57:49.212608 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97487 (* 1 = 5.97487 loss)
I0524 07:57:49.212630 11835 sgd_solver.cpp:112] Iteration 51140, lr = 0.1
I0524 07:57:56.277737 11835 solver.cpp:239] Iteration 51150 (1.41546 iter/s, 7.06482s/10 iters), loss = 6.4677
I0524 07:57:56.277776 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4677 (* 1 = 6.4677 loss)
I0524 07:57:56.277812 11835 sgd_solver.cpp:112] Iteration 51150, lr = 0.1
I0524 07:58:01.963135 11835 solver.cpp:239] Iteration 51160 (1.75897 iter/s, 5.68513s/10 iters), loss = 5.76045
I0524 07:58:01.963183 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.76045 (* 1 = 5.76045 loss)
I0524 07:58:01.963347 11835 sgd_solver.cpp:112] Iteration 51160, lr = 0.1
I0524 07:58:08.705624 11835 solver.cpp:239] Iteration 51170 (1.48321 iter/s, 6.74215s/10 iters), loss = 6.08669
I0524 07:58:08.705689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08669 (* 1 = 6.08669 loss)
I0524 07:58:09.694725 11835 sgd_solver.cpp:112] Iteration 51170, lr = 0.1
I0524 07:58:18.528204 11835 solver.cpp:239] Iteration 51180 (1.01811 iter/s, 9.82213s/10 iters), loss = 5.95088
I0524 07:58:18.528414 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95088 (* 1 = 5.95088 loss)
I0524 07:58:18.528447 11835 sgd_solver.cpp:112] Iteration 51180, lr = 0.1
I0524 07:58:24.328994 11835 solver.cpp:239] Iteration 51190 (1.72402 iter/s, 5.80038s/10 iters), loss = 6.63517
I0524 07:58:24.329035 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63517 (* 1 = 6.63517 loss)
I0524 07:58:24.805835 11835 sgd_solver.cpp:112] Iteration 51190, lr = 0.1
I0524 07:58:32.352300 11835 solver.cpp:239] Iteration 51200 (1.24643 iter/s, 8.02294s/10 iters), loss = 5.19053
I0524 07:58:32.352352 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.19053 (* 1 = 5.19053 loss)
I0524 07:58:32.352399 11835 sgd_solver.cpp:112] Iteration 51200, lr = 0.1
I0524 07:58:38.569730 11835 solver.cpp:239] Iteration 51210 (1.60846 iter/s, 6.21713s/10 iters), loss = 7.01205
I0524 07:58:38.569789 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01205 (* 1 = 7.01205 loss)
I0524 07:58:38.569829 11835 sgd_solver.cpp:112] Iteration 51210, lr = 0.1
I0524 07:58:47.556293 11835 solver.cpp:239] Iteration 51220 (1.11282 iter/s, 8.98616s/10 iters), loss = 5.43113
I0524 07:58:47.556340 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43113 (* 1 = 5.43113 loss)
I0524 07:58:48.546622 11835 sgd_solver.cpp:112] Iteration 51220, lr = 0.1
I0524 07:58:55.343415 11835 solver.cpp:239] Iteration 51230 (1.28423 iter/s, 7.78678s/10 iters), loss = 7.41591
I0524 07:58:55.343472 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.41591 (* 1 = 7.41591 loss)
I0524 07:58:55.343492 11835 sgd_solver.cpp:112] Iteration 51230, lr = 0.1
I0524 07:59:03.622016 11835 solver.cpp:239] Iteration 51240 (1.20799 iter/s, 8.27824s/10 iters), loss = 5.85596
I0524 07:59:03.622071 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85596 (* 1 = 5.85596 loss)
I0524 07:59:03.723856 11835 sgd_solver.cpp:112] Iteration 51240, lr = 0.1
I0524 07:59:12.125739 11835 solver.cpp:239] Iteration 51250 (1.17601 iter/s, 8.50334s/10 iters), loss = 5.32126
I0524 07:59:12.125799 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.32126 (* 1 = 5.32126 loss)
I0524 07:59:12.125986 11835 sgd_solver.cpp:112] Iteration 51250, lr = 0.1
I0524 07:59:20.895314 11835 solver.cpp:239] Iteration 51260 (1.14036 iter/s, 8.76917s/10 iters), loss = 6.65436
I0524 07:59:20.895521 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65436 (* 1 = 6.65436 loss)
I0524 07:59:20.895619 11835 sgd_solver.cpp:112] Iteration 51260, lr = 0.1
I0524 07:59:28.560330 11835 solver.cpp:239] Iteration 51270 (1.30471 iter/s, 7.66452s/10 iters), loss = 5.90488
I0524 07:59:28.560389 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90488 (* 1 = 5.90488 loss)
I0524 07:59:28.560403 11835 sgd_solver.cpp:112] Iteration 51270, lr = 0.1
I0524 07:59:35.861784 11835 solver.cpp:239] Iteration 51280 (1.36972 iter/s, 7.30075s/10 iters), loss = 6.69504
I0524 07:59:35.861827 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69504 (* 1 = 6.69504 loss)
I0524 07:59:36.512181 11835 sgd_solver.cpp:112] Iteration 51280, lr = 0.1
I0524 07:59:45.148206 11835 solver.cpp:239] Iteration 51290 (1.07689 iter/s, 9.28601s/10 iters), loss = 6.65117
I0524 07:59:45.148259 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65117 (* 1 = 6.65117 loss)
I0524 07:59:45.210796 11835 sgd_solver.cpp:112] Iteration 51290, lr = 0.1
I0524 07:59:52.626231 11835 solver.cpp:239] Iteration 51300 (1.33732 iter/s, 7.47766s/10 iters), loss = 5.40784
I0524 07:59:52.626482 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.40784 (* 1 = 5.40784 loss)
I0524 07:59:52.626523 11835 sgd_solver.cpp:112] Iteration 51300, lr = 0.1
I0524 08:00:00.201303 11835 solver.cpp:239] Iteration 51310 (1.32025 iter/s, 7.57435s/10 iters), loss = 7.41475
I0524 08:00:00.201357 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.41475 (* 1 = 7.41475 loss)
I0524 08:00:00.216217 11835 sgd_solver.cpp:112] Iteration 51310, lr = 0.1
I0524 08:00:06.447926 11835 solver.cpp:239] Iteration 51320 (1.60094 iter/s, 6.24633s/10 iters), loss = 5.84485
I0524 08:00:06.447983 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84485 (* 1 = 5.84485 loss)
I0524 08:00:06.448117 11835 sgd_solver.cpp:112] Iteration 51320, lr = 0.1
I0524 08:00:12.788628 11835 solver.cpp:239] Iteration 51330 (1.57719 iter/s, 6.34039s/10 iters), loss = 5.56192
I0524 08:00:12.788686 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56192 (* 1 = 5.56192 loss)
I0524 08:00:12.801640 11835 sgd_solver.cpp:112] Iteration 51330, lr = 0.1
I0524 08:00:21.527313 11835 solver.cpp:239] Iteration 51340 (1.14439 iter/s, 8.73829s/10 iters), loss = 6.79652
I0524 08:00:21.527377 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79652 (* 1 = 6.79652 loss)
I0524 08:00:21.527448 11835 sgd_solver.cpp:112] Iteration 51340, lr = 0.1
I0524 08:00:29.690589 11835 solver.cpp:239] Iteration 51350 (1.22506 iter/s, 8.16286s/10 iters), loss = 6.30767
I0524 08:00:29.690933 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30767 (* 1 = 6.30767 loss)
I0524 08:00:30.127776 11835 sgd_solver.cpp:112] Iteration 51350, lr = 0.1
I0524 08:00:36.716325 11835 solver.cpp:239] Iteration 51360 (1.42346 iter/s, 7.02515s/10 iters), loss = 5.22311
I0524 08:00:36.716387 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.22311 (* 1 = 5.22311 loss)
I0524 08:00:36.995519 11835 sgd_solver.cpp:112] Iteration 51360, lr = 0.1
I0524 08:00:44.067631 11835 solver.cpp:239] Iteration 51370 (1.36037 iter/s, 7.35096s/10 iters), loss = 7.12347
I0524 08:00:44.067695 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.12347 (* 1 = 7.12347 loss)
I0524 08:00:44.067808 11835 sgd_solver.cpp:112] Iteration 51370, lr = 0.1
I0524 08:00:50.359889 11835 solver.cpp:239] Iteration 51380 (1.58933 iter/s, 6.29195s/10 iters), loss = 5.5111
I0524 08:00:50.359936 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.5111 (* 1 = 5.5111 loss)
I0524 08:00:50.954071 11835 sgd_solver.cpp:112] Iteration 51380, lr = 0.1
I0524 08:01:00.965905 11835 solver.cpp:239] Iteration 51390 (0.942902 iter/s, 10.6056s/10 iters), loss = 6.4178
I0524 08:01:00.966075 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4178 (* 1 = 6.4178 loss)
I0524 08:01:00.966094 11835 sgd_solver.cpp:112] Iteration 51390, lr = 0.1
I0524 08:01:06.868331 11835 solver.cpp:239] Iteration 51400 (1.69493 iter/s, 5.89996s/10 iters), loss = 5.66809
I0524 08:01:06.868381 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.66809 (* 1 = 5.66809 loss)
I0524 08:01:06.868413 11835 sgd_solver.cpp:112] Iteration 51400, lr = 0.1
I0524 08:01:16.996018 11835 solver.cpp:239] Iteration 51410 (0.987435 iter/s, 10.1272s/10 iters), loss = 5.45027
I0524 08:01:16.996074 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.45027 (* 1 = 5.45027 loss)
I0524 08:01:17.602885 11835 sgd_solver.cpp:112] Iteration 51410, lr = 0.1
I0524 08:01:24.764217 11835 solver.cpp:239] Iteration 51420 (1.28736 iter/s, 7.76783s/10 iters), loss = 6.19905
I0524 08:01:24.764297 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19905 (* 1 = 6.19905 loss)
I0524 08:01:24.892143 11835 sgd_solver.cpp:112] Iteration 51420, lr = 0.1
I0524 08:01:33.839743 11835 solver.cpp:239] Iteration 51430 (1.10191 iter/s, 9.07511s/10 iters), loss = 6.15357
I0524 08:01:33.840061 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15357 (* 1 = 6.15357 loss)
I0524 08:01:33.840139 11835 sgd_solver.cpp:112] Iteration 51430, lr = 0.1
I0524 08:01:39.598757 11835 solver.cpp:239] Iteration 51440 (1.73656 iter/s, 5.75852s/10 iters), loss = 6.02192
I0524 08:01:39.598812 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02192 (* 1 = 6.02192 loss)
I0524 08:01:39.598829 11835 sgd_solver.cpp:112] Iteration 51440, lr = 0.1
I0524 08:01:46.575249 11835 solver.cpp:239] Iteration 51450 (1.43346 iter/s, 6.97611s/10 iters), loss = 6.09312
I0524 08:01:46.575296 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09312 (* 1 = 6.09312 loss)
I0524 08:01:46.575314 11835 sgd_solver.cpp:112] Iteration 51450, lr = 0.1
I0524 08:01:53.044601 11835 solver.cpp:239] Iteration 51460 (1.54582 iter/s, 6.46904s/10 iters), loss = 7.02042
I0524 08:01:53.044662 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.02042 (* 1 = 7.02042 loss)
I0524 08:01:53.044905 11835 sgd_solver.cpp:112] Iteration 51460, lr = 0.1
I0524 08:02:00.734077 11835 solver.cpp:239] Iteration 51470 (1.30054 iter/s, 7.6891s/10 iters), loss = 6.28409
I0524 08:02:00.734141 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.28409 (* 1 = 6.28409 loss)
I0524 08:02:00.741972 11835 sgd_solver.cpp:112] Iteration 51470, lr = 0.1
I0524 08:02:09.111718 11835 solver.cpp:239] Iteration 51480 (1.19371 iter/s, 8.37722s/10 iters), loss = 5.71448
I0524 08:02:09.111994 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71448 (* 1 = 5.71448 loss)
I0524 08:02:09.112048 11835 sgd_solver.cpp:112] Iteration 51480, lr = 0.1
I0524 08:02:16.753177 11835 solver.cpp:239] Iteration 51490 (1.3091 iter/s, 7.63883s/10 iters), loss = 4.92337
I0524 08:02:16.753242 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.92337 (* 1 = 4.92337 loss)
I0524 08:02:17.412482 11835 sgd_solver.cpp:112] Iteration 51490, lr = 0.1
I0524 08:02:24.667526 11835 solver.cpp:239] Iteration 51500 (1.26359 iter/s, 7.91395s/10 iters), loss = 5.63937
I0524 08:02:24.667603 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63937 (* 1 = 5.63937 loss)
I0524 08:02:24.667776 11835 sgd_solver.cpp:112] Iteration 51500, lr = 0.1
I0524 08:02:31.713284 11835 solver.cpp:239] Iteration 51510 (1.41936 iter/s, 7.04542s/10 iters), loss = 5.95159
I0524 08:02:31.713326 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95159 (* 1 = 5.95159 loss)
I0524 08:02:31.713338 11835 sgd_solver.cpp:112] Iteration 51510, lr = 0.1
I0524 08:02:38.234575 11835 solver.cpp:239] Iteration 51520 (1.53377 iter/s, 6.51987s/10 iters), loss = 6.63904
I0524 08:02:38.234640 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63904 (* 1 = 6.63904 loss)
I0524 08:02:38.292325 11835 sgd_solver.cpp:112] Iteration 51520, lr = 0.1
I0524 08:02:46.024871 11835 solver.cpp:239] Iteration 51530 (1.28371 iter/s, 7.78993s/10 iters), loss = 5.69769
I0524 08:02:46.025035 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.69769 (* 1 = 5.69769 loss)
I0524 08:02:46.025058 11835 sgd_solver.cpp:112] Iteration 51530, lr = 0.1
I0524 08:02:54.215977 11835 solver.cpp:239] Iteration 51540 (1.22092 iter/s, 8.19052s/10 iters), loss = 6.17901
I0524 08:02:54.216033 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17901 (* 1 = 6.17901 loss)
I0524 08:02:54.667124 11835 sgd_solver.cpp:112] Iteration 51540, lr = 0.1
I0524 08:03:02.129271 11835 solver.cpp:239] Iteration 51550 (1.26375 iter/s, 7.91293s/10 iters), loss = 6.2682
I0524 08:03:02.129319 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2682 (* 1 = 6.2682 loss)
I0524 08:03:02.382555 11835 sgd_solver.cpp:112] Iteration 51550, lr = 0.1
I0524 08:03:11.847846 11835 solver.cpp:239] Iteration 51560 (1.029 iter/s, 9.71815s/10 iters), loss = 5.54733
I0524 08:03:11.847913 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.54733 (* 1 = 5.54733 loss)
I0524 08:03:11.887960 11835 sgd_solver.cpp:112] Iteration 51560, lr = 0.1
I0524 08:03:18.197891 11835 solver.cpp:239] Iteration 51570 (1.57487 iter/s, 6.34974s/10 iters), loss = 5.73593
I0524 08:03:18.198093 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73593 (* 1 = 5.73593 loss)
I0524 08:03:18.198204 11835 sgd_solver.cpp:112] Iteration 51570, lr = 0.1
I0524 08:03:25.432353 11835 solver.cpp:239] Iteration 51580 (1.38236 iter/s, 7.23398s/10 iters), loss = 5.94422
I0524 08:03:25.432406 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94422 (* 1 = 5.94422 loss)
I0524 08:03:25.651145 11835 sgd_solver.cpp:112] Iteration 51580, lr = 0.1
I0524 08:03:33.337832 11835 solver.cpp:239] Iteration 51590 (1.265 iter/s, 7.90512s/10 iters), loss = 5.38125
I0524 08:03:33.337889 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.38125 (* 1 = 5.38125 loss)
I0524 08:03:33.391949 11835 sgd_solver.cpp:112] Iteration 51590, lr = 0.1
I0524 08:03:41.119925 11835 solver.cpp:239] Iteration 51600 (1.28506 iter/s, 7.78171s/10 iters), loss = 5.44572
I0524 08:03:41.119984 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.44572 (* 1 = 5.44572 loss)
I0524 08:03:41.120044 11835 sgd_solver.cpp:112] Iteration 51600, lr = 0.1
I0524 08:03:47.398916 11835 solver.cpp:239] Iteration 51610 (1.59269 iter/s, 6.27869s/10 iters), loss = 4.92225
I0524 08:03:47.398967 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.92225 (* 1 = 4.92225 loss)
I0524 08:03:47.398982 11835 sgd_solver.cpp:112] Iteration 51610, lr = 0.1
I0524 08:03:56.712545 11835 solver.cpp:239] Iteration 51620 (1.07375 iter/s, 9.31314s/10 iters), loss = 6.10325
I0524 08:03:56.712771 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10325 (* 1 = 6.10325 loss)
I0524 08:03:57.542266 11835 sgd_solver.cpp:112] Iteration 51620, lr = 0.1
I0524 08:04:05.023974 11835 solver.cpp:239] Iteration 51630 (1.20324 iter/s, 8.3109s/10 iters), loss = 5.87975
I0524 08:04:05.024035 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87975 (* 1 = 5.87975 loss)
I0524 08:04:05.024118 11835 sgd_solver.cpp:112] Iteration 51630, lr = 0.1
I0524 08:04:13.251396 11835 solver.cpp:239] Iteration 51640 (1.2155 iter/s, 8.22705s/10 iters), loss = 6.23118
I0524 08:04:13.251454 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23118 (* 1 = 6.23118 loss)
I0524 08:04:13.621865 11835 sgd_solver.cpp:112] Iteration 51640, lr = 0.1
I0524 08:04:22.242823 11835 solver.cpp:239] Iteration 51650 (1.11222 iter/s, 8.99103s/10 iters), loss = 6.22143
I0524 08:04:22.242871 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22143 (* 1 = 6.22143 loss)
I0524 08:04:22.243096 11835 sgd_solver.cpp:112] Iteration 51650, lr = 0.1
I0524 08:04:28.354724 11835 solver.cpp:239] Iteration 51660 (1.63623 iter/s, 6.11159s/10 iters), loss = 7.09801
I0524 08:04:28.355029 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.09801 (* 1 = 7.09801 loss)
I0524 08:04:28.355095 11835 sgd_solver.cpp:112] Iteration 51660, lr = 0.1
I0524 08:04:35.268736 11835 solver.cpp:239] Iteration 51670 (1.44646 iter/s, 6.91344s/10 iters), loss = 6.33785
I0524 08:04:35.268805 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33785 (* 1 = 6.33785 loss)
I0524 08:04:35.268919 11835 sgd_solver.cpp:112] Iteration 51670, lr = 0.1
I0524 08:04:43.306105 11835 solver.cpp:239] Iteration 51680 (1.24425 iter/s, 8.03699s/10 iters), loss = 5.65184
I0524 08:04:43.306167 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.65184 (* 1 = 5.65184 loss)
I0524 08:04:43.686949 11835 sgd_solver.cpp:112] Iteration 51680, lr = 0.1
I0524 08:04:49.903035 11835 solver.cpp:239] Iteration 51690 (1.51593 iter/s, 6.59661s/10 iters), loss = 5.43647
I0524 08:04:49.903101 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43647 (* 1 = 5.43647 loss)
I0524 08:04:49.903214 11835 sgd_solver.cpp:112] Iteration 51690, lr = 0.1
I0524 08:04:56.664036 11835 solver.cpp:239] Iteration 51700 (1.47915 iter/s, 6.76065s/10 iters), loss = 5.8426
I0524 08:04:56.664095 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8426 (* 1 = 5.8426 loss)
I0524 08:04:56.664305 11835 sgd_solver.cpp:112] Iteration 51700, lr = 0.1
I0524 08:05:03.357295 11835 solver.cpp:239] Iteration 51710 (1.49411 iter/s, 6.69294s/10 iters), loss = 5.61725
I0524 08:05:03.357549 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.61725 (* 1 = 5.61725 loss)
I0524 08:05:03.953385 11835 sgd_solver.cpp:112] Iteration 51710, lr = 0.1
I0524 08:05:13.866242 11835 solver.cpp:239] Iteration 51720 (0.951628 iter/s, 10.5083s/10 iters), loss = 4.78791
I0524 08:05:13.866286 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.78791 (* 1 = 4.78791 loss)
I0524 08:05:13.866525 11835 sgd_solver.cpp:112] Iteration 51720, lr = 0.1
I0524 08:05:19.853549 11835 solver.cpp:239] Iteration 51730 (1.67028 iter/s, 5.98704s/10 iters), loss = 5.82615
I0524 08:05:19.853588 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82615 (* 1 = 5.82615 loss)
I0524 08:05:19.853643 11835 sgd_solver.cpp:112] Iteration 51730, lr = 0.1
I0524 08:05:26.372064 11835 solver.cpp:239] Iteration 51740 (1.53416 iter/s, 6.51821s/10 iters), loss = 5.65465
I0524 08:05:26.372118 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.65465 (* 1 = 5.65465 loss)
I0524 08:05:26.372145 11835 sgd_solver.cpp:112] Iteration 51740, lr = 0.1
I0524 08:05:32.649677 11835 solver.cpp:239] Iteration 51750 (1.59304 iter/s, 6.27733s/10 iters), loss = 5.73334
I0524 08:05:32.649724 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73334 (* 1 = 5.73334 loss)
I0524 08:05:32.706068 11835 sgd_solver.cpp:112] Iteration 51750, lr = 0.1
I0524 08:05:39.292275 11835 solver.cpp:239] Iteration 51760 (1.50551 iter/s, 6.64229s/10 iters), loss = 5.56646
I0524 08:05:39.292552 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56646 (* 1 = 5.56646 loss)
I0524 08:05:39.790731 11835 sgd_solver.cpp:112] Iteration 51760, lr = 0.1
I0524 08:05:45.478345 11835 solver.cpp:239] Iteration 51770 (1.61666 iter/s, 6.18561s/10 iters), loss = 6.49946
I0524 08:05:45.478381 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49946 (* 1 = 6.49946 loss)
I0524 08:05:45.478394 11835 sgd_solver.cpp:112] Iteration 51770, lr = 0.1
I0524 08:05:52.266451 11835 solver.cpp:239] Iteration 51780 (1.47325 iter/s, 6.78771s/10 iters), loss = 6.01781
I0524 08:05:52.266571 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01781 (* 1 = 6.01781 loss)
I0524 08:05:52.266822 11835 sgd_solver.cpp:112] Iteration 51780, lr = 0.1
I0524 08:06:00.820006 11835 solver.cpp:239] Iteration 51790 (1.16916 iter/s, 8.55316s/10 iters), loss = 5.3952
I0524 08:06:00.820063 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.3952 (* 1 = 5.3952 loss)
I0524 08:06:00.820171 11835 sgd_solver.cpp:112] Iteration 51790, lr = 0.1
I0524 08:06:08.211230 11835 solver.cpp:239] Iteration 51800 (1.35302 iter/s, 7.39088s/10 iters), loss = 7.01445
I0524 08:06:08.211370 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01445 (* 1 = 7.01445 loss)
I0524 08:06:08.496666 11835 sgd_solver.cpp:112] Iteration 51800, lr = 0.1
I0524 08:06:15.339131 11835 solver.cpp:239] Iteration 51810 (1.403 iter/s, 7.12758s/10 iters), loss = 5.70584
I0524 08:06:15.339370 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.70584 (* 1 = 5.70584 loss)
I0524 08:06:15.339432 11835 sgd_solver.cpp:112] Iteration 51810, lr = 0.1
I0524 08:06:22.169168 11835 solver.cpp:239] Iteration 51820 (1.46423 iter/s, 6.82955s/10 iters), loss = 5.99815
I0524 08:06:22.169258 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99815 (* 1 = 5.99815 loss)
I0524 08:06:22.360255 11835 sgd_solver.cpp:112] Iteration 51820, lr = 0.1
I0524 08:06:28.360139 11835 solver.cpp:239] Iteration 51830 (1.61534 iter/s, 6.19066s/10 iters), loss = 5.81147
I0524 08:06:28.360193 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81147 (* 1 = 5.81147 loss)
I0524 08:06:28.360206 11835 sgd_solver.cpp:112] Iteration 51830, lr = 0.1
I0524 08:06:34.696250 11835 solver.cpp:239] Iteration 51840 (1.57835 iter/s, 6.33573s/10 iters), loss = 5.79781
I0524 08:06:34.696296 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79781 (* 1 = 5.79781 loss)
I0524 08:06:34.696512 11835 sgd_solver.cpp:112] Iteration 51840, lr = 0.1
I0524 08:06:42.327922 11835 solver.cpp:239] Iteration 51850 (1.31039 iter/s, 7.63132s/10 iters), loss = 6.18508
I0524 08:06:42.327999 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18508 (* 1 = 6.18508 loss)
I0524 08:06:42.328092 11835 sgd_solver.cpp:112] Iteration 51850, lr = 0.1
I0524 08:06:49.663173 11835 solver.cpp:239] Iteration 51860 (1.36335 iter/s, 7.33488s/10 iters), loss = 6.65198
I0524 08:06:49.663456 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65198 (* 1 = 6.65198 loss)
I0524 08:06:49.663501 11835 sgd_solver.cpp:112] Iteration 51860, lr = 0.1
I0524 08:06:57.019250 11835 solver.cpp:239] Iteration 51870 (1.35953 iter/s, 7.35549s/10 iters), loss = 5.77101
I0524 08:06:57.019309 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.77101 (* 1 = 5.77101 loss)
I0524 08:06:57.056011 11835 sgd_solver.cpp:112] Iteration 51870, lr = 0.1
I0524 08:07:02.892649 11835 solver.cpp:239] Iteration 51880 (1.70268 iter/s, 5.8731s/10 iters), loss = 6.58455
I0524 08:07:02.892722 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58455 (* 1 = 6.58455 loss)
I0524 08:07:02.892938 11835 sgd_solver.cpp:112] Iteration 51880, lr = 0.1
I0524 08:07:10.991972 11835 solver.cpp:239] Iteration 51890 (1.23473 iter/s, 8.09893s/10 iters), loss = 5.70522
I0524 08:07:10.992027 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.70522 (* 1 = 5.70522 loss)
I0524 08:07:10.992125 11835 sgd_solver.cpp:112] Iteration 51890, lr = 0.1
I0524 08:07:16.816747 11835 solver.cpp:239] Iteration 51900 (1.71689 iter/s, 5.8245s/10 iters), loss = 7.33861
I0524 08:07:16.816814 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.33861 (* 1 = 7.33861 loss)
I0524 08:07:16.816831 11835 sgd_solver.cpp:112] Iteration 51900, lr = 0.1
I0524 08:07:22.969532 11835 solver.cpp:239] Iteration 51910 (1.62535 iter/s, 6.15251s/10 iters), loss = 5.612
I0524 08:07:22.969594 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.612 (* 1 = 5.612 loss)
I0524 08:07:22.996647 11835 sgd_solver.cpp:112] Iteration 51910, lr = 0.1
I0524 08:07:30.823233 11835 solver.cpp:239] Iteration 51920 (1.27335 iter/s, 7.85332s/10 iters), loss = 5.36368
I0524 08:07:30.823284 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.36368 (* 1 = 5.36368 loss)
I0524 08:07:30.823520 11835 sgd_solver.cpp:112] Iteration 51920, lr = 0.1
I0524 08:07:36.742889 11835 solver.cpp:239] Iteration 51930 (1.69038 iter/s, 5.91583s/10 iters), loss = 4.92408
I0524 08:07:36.742949 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.92408 (* 1 = 4.92408 loss)
I0524 08:07:36.742966 11835 sgd_solver.cpp:112] Iteration 51930, lr = 0.1
I0524 08:07:45.928462 11835 solver.cpp:239] Iteration 51940 (1.08873 iter/s, 9.18501s/10 iters), loss = 5.52493
I0524 08:07:45.928514 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.52493 (* 1 = 5.52493 loss)
I0524 08:07:46.849258 11835 sgd_solver.cpp:112] Iteration 51940, lr = 0.1
I0524 08:07:52.409463 11835 solver.cpp:239] Iteration 51950 (1.54304 iter/s, 6.4807s/10 iters), loss = 6.09046
I0524 08:07:52.409512 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09046 (* 1 = 6.09046 loss)
I0524 08:07:52.409525 11835 sgd_solver.cpp:112] Iteration 51950, lr = 0.1
I0524 08:07:59.573983 11835 solver.cpp:239] Iteration 51960 (1.39583 iter/s, 7.1642s/10 iters), loss = 6.66017
I0524 08:07:59.574268 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66017 (* 1 = 6.66017 loss)
I0524 08:07:59.574323 11835 sgd_solver.cpp:112] Iteration 51960, lr = 0.1
I0524 08:08:05.990212 11835 solver.cpp:239] Iteration 51970 (1.55867 iter/s, 6.41571s/10 iters), loss = 5.6516
I0524 08:08:05.990267 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.6516 (* 1 = 5.6516 loss)
I0524 08:08:06.306910 11835 sgd_solver.cpp:112] Iteration 51970, lr = 0.1
I0524 08:08:12.327755 11835 solver.cpp:239] Iteration 51980 (1.57797 iter/s, 6.33724s/10 iters), loss = 5.41475
I0524 08:08:12.327812 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.41475 (* 1 = 5.41475 loss)
I0524 08:08:12.327831 11835 sgd_solver.cpp:112] Iteration 51980, lr = 0.1
I0524 08:08:20.294414 11835 solver.cpp:239] Iteration 51990 (1.2553 iter/s, 7.96624s/10 iters), loss = 6.98256
I0524 08:08:20.294476 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.98256 (* 1 = 6.98256 loss)
I0524 08:08:20.334761 11835 sgd_solver.cpp:112] Iteration 51990, lr = 0.1
I0524 08:08:27.815392 11835 solver.cpp:239] Iteration 52000 (1.32967 iter/s, 7.52064s/10 iters), loss = 6.41288
I0524 08:08:27.815433 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41288 (* 1 = 6.41288 loss)
I0524 08:08:28.831656 11835 sgd_solver.cpp:112] Iteration 52000, lr = 0.1
I0524 08:08:36.421521 11835 solver.cpp:239] Iteration 52010 (1.16201 iter/s, 8.60576s/10 iters), loss = 5.56181
I0524 08:08:36.421663 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56181 (* 1 = 5.56181 loss)
I0524 08:08:36.430191 11835 sgd_solver.cpp:112] Iteration 52010, lr = 0.1
I0524 08:08:43.922431 11835 solver.cpp:239] Iteration 52020 (1.33325 iter/s, 7.50048s/10 iters), loss = 6.39302
I0524 08:08:43.922487 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39302 (* 1 = 6.39302 loss)
I0524 08:08:43.922503 11835 sgd_solver.cpp:112] Iteration 52020, lr = 0.1
I0524 08:08:53.011451 11835 solver.cpp:239] Iteration 52030 (1.10029 iter/s, 9.0885s/10 iters), loss = 5.77526
I0524 08:08:53.011512 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.77526 (* 1 = 5.77526 loss)
I0524 08:08:53.191004 11835 sgd_solver.cpp:112] Iteration 52030, lr = 0.1
I0524 08:08:59.169152 11835 solver.cpp:239] Iteration 52040 (1.62406 iter/s, 6.1574s/10 iters), loss = 6.06516
I0524 08:08:59.169200 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06516 (* 1 = 6.06516 loss)
I0524 08:08:59.169450 11835 sgd_solver.cpp:112] Iteration 52040, lr = 0.1
I0524 08:09:06.286880 11835 solver.cpp:239] Iteration 52050 (1.40501 iter/s, 7.11738s/10 iters), loss = 5.25735
I0524 08:09:06.286975 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.25735 (* 1 = 5.25735 loss)
I0524 08:09:06.287132 11835 sgd_solver.cpp:112] Iteration 52050, lr = 0.1
I0524 08:09:12.688345 11835 solver.cpp:239] Iteration 52060 (1.56222 iter/s, 6.40114s/10 iters), loss = 6.32478
I0524 08:09:12.688491 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32478 (* 1 = 6.32478 loss)
I0524 08:09:12.712095 11835 sgd_solver.cpp:112] Iteration 52060, lr = 0.1
I0524 08:09:19.363170 11835 solver.cpp:239] Iteration 52070 (1.49826 iter/s, 6.67443s/10 iters), loss = 5.97271
I0524 08:09:19.363220 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97271 (* 1 = 5.97271 loss)
I0524 08:09:20.179497 11835 sgd_solver.cpp:112] Iteration 52070, lr = 0.1
I0524 08:09:26.648959 11835 solver.cpp:239] Iteration 52080 (1.3726 iter/s, 7.28544s/10 iters), loss = 5.18287
I0524 08:09:26.649021 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.18287 (* 1 = 5.18287 loss)
I0524 08:09:26.649117 11835 sgd_solver.cpp:112] Iteration 52080, lr = 0.1
I0524 08:09:33.172453 11835 solver.cpp:239] Iteration 52090 (1.533 iter/s, 6.52315s/10 iters), loss = 6.36733
I0524 08:09:33.172510 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36733 (* 1 = 6.36733 loss)
I0524 08:09:33.228752 11835 sgd_solver.cpp:112] Iteration 52090, lr = 0.1
I0524 08:09:41.638820 11835 solver.cpp:239] Iteration 52100 (1.1812 iter/s, 8.46597s/10 iters), loss = 4.87586
I0524 08:09:41.638903 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.87586 (* 1 = 4.87586 loss)
I0524 08:09:41.638978 11835 sgd_solver.cpp:112] Iteration 52100, lr = 0.1
I0524 08:09:48.351266 11835 solver.cpp:239] Iteration 52110 (1.48984 iter/s, 6.71212s/10 iters), loss = 5.30024
I0524 08:09:48.351552 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.30024 (* 1 = 5.30024 loss)
I0524 08:09:48.351593 11835 sgd_solver.cpp:112] Iteration 52110, lr = 0.1
I0524 08:09:54.384320 11835 solver.cpp:239] Iteration 52120 (1.65767 iter/s, 6.03256s/10 iters), loss = 6.88152
I0524 08:09:54.384377 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88152 (* 1 = 6.88152 loss)
I0524 08:09:54.384393 11835 sgd_solver.cpp:112] Iteration 52120, lr = 0.1
I0524 08:10:00.246974 11835 solver.cpp:239] Iteration 52130 (1.70579 iter/s, 5.86238s/10 iters), loss = 5.92598
I0524 08:10:00.247014 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92598 (* 1 = 5.92598 loss)
I0524 08:10:00.247046 11835 sgd_solver.cpp:112] Iteration 52130, lr = 0.1
I0524 08:10:07.848860 11835 solver.cpp:239] Iteration 52140 (1.31552 iter/s, 7.60155s/10 iters), loss = 6.22026
I0524 08:10:07.848917 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22026 (* 1 = 6.22026 loss)
I0524 08:10:08.805016 11835 sgd_solver.cpp:112] Iteration 52140, lr = 0.1
I0524 08:10:17.673801 11835 solver.cpp:239] Iteration 52150 (1.01786 iter/s, 9.8245s/10 iters), loss = 5.57053
I0524 08:10:17.673877 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.57053 (* 1 = 5.57053 loss)
I0524 08:10:17.673919 11835 sgd_solver.cpp:112] Iteration 52150, lr = 0.1
I0524 08:10:24.100875 11835 solver.cpp:239] Iteration 52160 (1.55619 iter/s, 6.42597s/10 iters), loss = 5.651
I0524 08:10:24.101228 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.651 (* 1 = 5.651 loss)
I0524 08:10:24.460341 11835 sgd_solver.cpp:112] Iteration 52160, lr = 0.1
I0524 08:10:31.685369 11835 solver.cpp:239] Iteration 52170 (1.31857 iter/s, 7.58397s/10 iters), loss = 6.38497
I0524 08:10:31.685427 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38497 (* 1 = 6.38497 loss)
I0524 08:10:32.230916 11835 sgd_solver.cpp:112] Iteration 52170, lr = 0.1
I0524 08:10:38.366803 11835 solver.cpp:239] Iteration 52180 (1.49675 iter/s, 6.68112s/10 iters), loss = 5.44991
I0524 08:10:38.366854 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.44991 (* 1 = 5.44991 loss)
I0524 08:10:39.723747 11835 sgd_solver.cpp:112] Iteration 52180, lr = 0.1
I0524 08:10:47.151329 11835 solver.cpp:239] Iteration 52190 (1.13842 iter/s, 8.78413s/10 iters), loss = 6.70284
I0524 08:10:47.151446 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70284 (* 1 = 6.70284 loss)
I0524 08:10:47.151463 11835 sgd_solver.cpp:112] Iteration 52190, lr = 0.1
I0524 08:10:54.660336 11835 solver.cpp:239] Iteration 52200 (1.33182 iter/s, 7.50853s/10 iters), loss = 6.78883
I0524 08:10:54.660568 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78883 (* 1 = 6.78883 loss)
I0524 08:10:54.660655 11835 sgd_solver.cpp:112] Iteration 52200, lr = 0.1
I0524 08:11:02.055714 11835 solver.cpp:239] Iteration 52210 (1.35228 iter/s, 7.39491s/10 iters), loss = 5.99139
I0524 08:11:02.055766 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99139 (* 1 = 5.99139 loss)
I0524 08:11:02.055780 11835 sgd_solver.cpp:112] Iteration 52210, lr = 0.1
I0524 08:11:09.466549 11835 solver.cpp:239] Iteration 52220 (1.34955 iter/s, 7.40989s/10 iters), loss = 5.76864
I0524 08:11:09.466593 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.76864 (* 1 = 5.76864 loss)
I0524 08:11:10.405617 11835 sgd_solver.cpp:112] Iteration 52220, lr = 0.1
I0524 08:11:20.623534 11835 solver.cpp:239] Iteration 52230 (0.896338 iter/s, 11.1565s/10 iters), loss = 6.65349
I0524 08:11:20.623612 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65349 (* 1 = 6.65349 loss)
I0524 08:11:20.623739 11835 sgd_solver.cpp:112] Iteration 52230, lr = 0.1
I0524 08:11:29.616104 11835 solver.cpp:239] Iteration 52240 (1.11208 iter/s, 8.99216s/10 iters), loss = 5.68129
I0524 08:11:29.616389 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68129 (* 1 = 5.68129 loss)
I0524 08:11:29.616547 11835 sgd_solver.cpp:112] Iteration 52240, lr = 0.1
I0524 08:11:36.020006 11835 solver.cpp:239] Iteration 52250 (1.56167 iter/s, 6.4034s/10 iters), loss = 6.2224
I0524 08:11:36.020068 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2224 (* 1 = 6.2224 loss)
I0524 08:11:36.020087 11835 sgd_solver.cpp:112] Iteration 52250, lr = 0.1
I0524 08:11:42.784540 11835 solver.cpp:239] Iteration 52260 (1.47837 iter/s, 6.76421s/10 iters), loss = 6.8473
I0524 08:11:42.784595 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8473 (* 1 = 6.8473 loss)
I0524 08:11:43.066500 11835 sgd_solver.cpp:112] Iteration 52260, lr = 0.1
I0524 08:11:50.953028 11835 solver.cpp:239] Iteration 52270 (1.22427 iter/s, 8.16812s/10 iters), loss = 6.62399
I0524 08:11:50.953094 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62399 (* 1 = 6.62399 loss)
I0524 08:11:51.792019 11835 sgd_solver.cpp:112] Iteration 52270, lr = 0.1
I0524 08:11:58.479239 11835 solver.cpp:239] Iteration 52280 (1.32875 iter/s, 7.52586s/10 iters), loss = 6.60009
I0524 08:11:58.479300 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60009 (* 1 = 6.60009 loss)
I0524 08:11:58.690904 11835 sgd_solver.cpp:112] Iteration 52280, lr = 0.1
I0524 08:12:07.689162 11835 solver.cpp:239] Iteration 52290 (1.08584 iter/s, 9.20949s/10 iters), loss = 6.6201
I0524 08:12:07.689432 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6201 (* 1 = 6.6201 loss)
I0524 08:12:08.452695 11835 sgd_solver.cpp:112] Iteration 52290, lr = 0.1
I0524 08:12:16.225414 11835 solver.cpp:239] Iteration 52300 (1.17155 iter/s, 8.53568s/10 iters), loss = 6.54065
I0524 08:12:16.225464 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54065 (* 1 = 6.54065 loss)
I0524 08:12:16.244752 11835 sgd_solver.cpp:112] Iteration 52300, lr = 0.1
I0524 08:12:22.959883 11835 solver.cpp:239] Iteration 52310 (1.48497 iter/s, 6.73415s/10 iters), loss = 6.57955
I0524 08:12:22.959975 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57955 (* 1 = 6.57955 loss)
I0524 08:12:23.433434 11835 sgd_solver.cpp:112] Iteration 52310, lr = 0.1
I0524 08:12:29.807749 11835 solver.cpp:239] Iteration 52320 (1.46038 iter/s, 6.84754s/10 iters), loss = 6.72337
I0524 08:12:29.807792 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72337 (* 1 = 6.72337 loss)
I0524 08:12:29.807806 11835 sgd_solver.cpp:112] Iteration 52320, lr = 0.1
I0524 08:12:36.523100 11835 solver.cpp:239] Iteration 52330 (1.48928 iter/s, 6.71463s/10 iters), loss = 5.56678
I0524 08:12:36.523185 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56678 (* 1 = 5.56678 loss)
I0524 08:12:36.562073 11835 sgd_solver.cpp:112] Iteration 52330, lr = 0.1
I0524 08:12:44.777029 11835 solver.cpp:239] Iteration 52340 (1.2116 iter/s, 8.25352s/10 iters), loss = 6.74362
I0524 08:12:44.777297 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74362 (* 1 = 6.74362 loss)
I0524 08:12:44.777330 11835 sgd_solver.cpp:112] Iteration 52340, lr = 0.1
I0524 08:12:51.085742 11835 solver.cpp:239] Iteration 52350 (1.58523 iter/s, 6.30823s/10 iters), loss = 5.58818
I0524 08:12:51.085785 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.58818 (* 1 = 5.58818 loss)
I0524 08:12:51.430358 11835 sgd_solver.cpp:112] Iteration 52350, lr = 0.1
I0524 08:12:57.306408 11835 solver.cpp:239] Iteration 52360 (1.60762 iter/s, 6.22037s/10 iters), loss = 5.85946
I0524 08:12:57.306458 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85946 (* 1 = 5.85946 loss)
I0524 08:12:57.533691 11835 sgd_solver.cpp:112] Iteration 52360, lr = 0.1
I0524 08:13:04.600448 11835 solver.cpp:239] Iteration 52370 (1.37105 iter/s, 7.29367s/10 iters), loss = 5.80259
I0524 08:13:04.600513 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80259 (* 1 = 5.80259 loss)
I0524 08:13:04.601055 11835 sgd_solver.cpp:112] Iteration 52370, lr = 0.1
I0524 08:13:11.085352 11835 solver.cpp:239] Iteration 52380 (1.54212 iter/s, 6.48459s/10 iters), loss = 5.47937
I0524 08:13:11.085402 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.47937 (* 1 = 5.47937 loss)
I0524 08:13:11.085453 11835 sgd_solver.cpp:112] Iteration 52380, lr = 0.1
I0524 08:13:19.559108 11835 solver.cpp:239] Iteration 52390 (1.18017 iter/s, 8.47335s/10 iters), loss = 6.7595
I0524 08:13:19.559347 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7595 (* 1 = 6.7595 loss)
I0524 08:13:19.559394 11835 sgd_solver.cpp:112] Iteration 52390, lr = 0.1
I0524 08:13:27.825866 11835 solver.cpp:239] Iteration 52400 (1.20974 iter/s, 8.26621s/10 iters), loss = 5.92755
I0524 08:13:27.825922 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92755 (* 1 = 5.92755 loss)
I0524 08:13:28.926748 11835 sgd_solver.cpp:112] Iteration 52400, lr = 0.1
I0524 08:13:35.840157 11835 solver.cpp:239] Iteration 52410 (1.24783 iter/s, 8.01391s/10 iters), loss = 5.46288
I0524 08:13:35.840214 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.46288 (* 1 = 5.46288 loss)
I0524 08:13:35.840425 11835 sgd_solver.cpp:112] Iteration 52410, lr = 0.1
I0524 08:13:42.526443 11835 solver.cpp:239] Iteration 52420 (1.49567 iter/s, 6.68598s/10 iters), loss = 7.11351
I0524 08:13:42.526482 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.11351 (* 1 = 7.11351 loss)
I0524 08:13:43.120863 11835 sgd_solver.cpp:112] Iteration 52420, lr = 0.1
I0524 08:13:50.709189 11835 solver.cpp:239] Iteration 52430 (1.22214 iter/s, 8.18239s/10 iters), loss = 5.67099
I0524 08:13:50.709380 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67099 (* 1 = 5.67099 loss)
I0524 08:13:50.709425 11835 sgd_solver.cpp:112] Iteration 52430, lr = 0.1
I0524 08:13:59.047806 11835 solver.cpp:239] Iteration 52440 (1.19936 iter/s, 8.33778s/10 iters), loss = 5.87973
I0524 08:13:59.047863 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87973 (* 1 = 5.87973 loss)
I0524 08:13:59.047960 11835 sgd_solver.cpp:112] Iteration 52440, lr = 0.1
I0524 08:14:06.300716 11835 solver.cpp:239] Iteration 52450 (1.37882 iter/s, 7.25257s/10 iters), loss = 5.91002
I0524 08:14:06.300773 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91002 (* 1 = 5.91002 loss)
I0524 08:14:06.300879 11835 sgd_solver.cpp:112] Iteration 52450, lr = 0.1
I0524 08:14:14.874243 11835 solver.cpp:239] Iteration 52460 (1.16643 iter/s, 8.57314s/10 iters), loss = 6.16224
I0524 08:14:14.874300 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16224 (* 1 = 6.16224 loss)
I0524 08:14:14.874541 11835 sgd_solver.cpp:112] Iteration 52460, lr = 0.1
I0524 08:14:21.744726 11835 solver.cpp:239] Iteration 52470 (1.45557 iter/s, 6.87017s/10 iters), loss = 5.75186
I0524 08:14:21.744871 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75186 (* 1 = 5.75186 loss)
I0524 08:14:21.744983 11835 sgd_solver.cpp:112] Iteration 52470, lr = 0.1
I0524 08:14:29.334530 11835 solver.cpp:239] Iteration 52480 (1.31763 iter/s, 7.58936s/10 iters), loss = 6.14478
I0524 08:14:29.334583 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14478 (* 1 = 6.14478 loss)
I0524 08:14:29.371171 11835 sgd_solver.cpp:112] Iteration 52480, lr = 0.1
I0524 08:14:35.766927 11835 solver.cpp:239] Iteration 52490 (1.5547 iter/s, 6.4321s/10 iters), loss = 6.47081
I0524 08:14:35.766975 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.47081 (* 1 = 6.47081 loss)
I0524 08:14:35.775614 11835 sgd_solver.cpp:112] Iteration 52490, lr = 0.1
I0524 08:14:45.736186 11835 solver.cpp:239] Iteration 52500 (1.00313 iter/s, 9.96883s/10 iters), loss = 6.99527
I0524 08:14:45.736238 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.99527 (* 1 = 6.99527 loss)
I0524 08:14:45.928992 11835 sgd_solver.cpp:112] Iteration 52500, lr = 0.1
I0524 08:14:53.031391 11835 solver.cpp:239] Iteration 52510 (1.37083 iter/s, 7.29486s/10 iters), loss = 6.07171
I0524 08:14:53.031707 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07171 (* 1 = 6.07171 loss)
I0524 08:14:53.031759 11835 sgd_solver.cpp:112] Iteration 52510, lr = 0.1
I0524 08:15:00.274950 11835 solver.cpp:239] Iteration 52520 (1.38069 iter/s, 7.24278s/10 iters), loss = 6.53956
I0524 08:15:00.275099 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53956 (* 1 = 6.53956 loss)
I0524 08:15:00.449546 11835 sgd_solver.cpp:112] Iteration 52520, lr = 0.1
I0524 08:15:07.127995 11835 solver.cpp:239] Iteration 52530 (1.45928 iter/s, 6.85267s/10 iters), loss = 5.97193
I0524 08:15:07.128062 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97193 (* 1 = 5.97193 loss)
I0524 08:15:07.128104 11835 sgd_solver.cpp:112] Iteration 52530, lr = 0.1
I0524 08:15:14.107272 11835 solver.cpp:239] Iteration 52540 (1.43288 iter/s, 6.97894s/10 iters), loss = 5.66987
I0524 08:15:14.107314 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.66987 (* 1 = 5.66987 loss)
I0524 08:15:14.107326 11835 sgd_solver.cpp:112] Iteration 52540, lr = 0.1
I0524 08:15:20.658030 11835 solver.cpp:239] Iteration 52550 (1.52664 iter/s, 6.55031s/10 iters), loss = 6.07356
I0524 08:15:20.658138 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07356 (* 1 = 6.07356 loss)
I0524 08:15:20.658282 11835 sgd_solver.cpp:112] Iteration 52550, lr = 0.1
I0524 08:15:27.770437 11835 solver.cpp:239] Iteration 52560 (1.40606 iter/s, 7.11205s/10 iters), loss = 6.29492
I0524 08:15:27.770625 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29492 (* 1 = 6.29492 loss)
I0524 08:15:27.770727 11835 sgd_solver.cpp:112] Iteration 52560, lr = 0.1
I0524 08:15:34.603703 11835 solver.cpp:239] Iteration 52570 (1.46352 iter/s, 6.83282s/10 iters), loss = 6.05145
I0524 08:15:34.603755 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05145 (* 1 = 6.05145 loss)
I0524 08:15:34.603770 11835 sgd_solver.cpp:112] Iteration 52570, lr = 0.1
I0524 08:15:41.988577 11835 solver.cpp:239] Iteration 52580 (1.35419 iter/s, 7.38448s/10 iters), loss = 6.23713
I0524 08:15:41.988620 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23713 (* 1 = 6.23713 loss)
I0524 08:15:41.988632 11835 sgd_solver.cpp:112] Iteration 52580, lr = 0.1
I0524 08:15:49.689245 11835 solver.cpp:239] Iteration 52590 (1.29902 iter/s, 7.69812s/10 iters), loss = 7.0537
I0524 08:15:49.689330 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.0537 (* 1 = 7.0537 loss)
I0524 08:15:49.689550 11835 sgd_solver.cpp:112] Iteration 52590, lr = 0.1
I0524 08:15:56.451123 11835 solver.cpp:239] Iteration 52600 (1.47895 iter/s, 6.76155s/10 iters), loss = 6.04208
I0524 08:15:56.451180 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04208 (* 1 = 6.04208 loss)
I0524 08:15:56.451380 11835 sgd_solver.cpp:112] Iteration 52600, lr = 0.1
I0524 08:16:05.172024 11835 solver.cpp:239] Iteration 52610 (1.14672 iter/s, 8.72052s/10 iters), loss = 5.5056
I0524 08:16:05.172286 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.5056 (* 1 = 5.5056 loss)
I0524 08:16:05.172336 11835 sgd_solver.cpp:112] Iteration 52610, lr = 0.1
I0524 08:16:11.269691 11835 solver.cpp:239] Iteration 52620 (1.64039 iter/s, 6.09611s/10 iters), loss = 5.33294
I0524 08:16:11.269745 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.33294 (* 1 = 5.33294 loss)
I0524 08:16:11.637504 11835 sgd_solver.cpp:112] Iteration 52620, lr = 0.1
I0524 08:16:19.888723 11835 solver.cpp:239] Iteration 52630 (1.16028 iter/s, 8.61864s/10 iters), loss = 7.08324
I0524 08:16:19.888784 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.08324 (* 1 = 7.08324 loss)
I0524 08:16:19.888804 11835 sgd_solver.cpp:112] Iteration 52630, lr = 0.1
I0524 08:16:28.182149 11835 solver.cpp:239] Iteration 52640 (1.20592 iter/s, 8.29245s/10 iters), loss = 6.09231
I0524 08:16:28.182207 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09231 (* 1 = 6.09231 loss)
I0524 08:16:28.182339 11835 sgd_solver.cpp:112] Iteration 52640, lr = 0.1
I0524 08:16:34.391685 11835 solver.cpp:239] Iteration 52650 (1.6105 iter/s, 6.20925s/10 iters), loss = 5.5056
I0524 08:16:34.391719 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.5056 (* 1 = 5.5056 loss)
I0524 08:16:34.391731 11835 sgd_solver.cpp:112] Iteration 52650, lr = 0.1
I0524 08:16:40.422947 11835 solver.cpp:239] Iteration 52660 (1.65811 iter/s, 6.03097s/10 iters), loss = 5.6258
I0524 08:16:40.423159 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.6258 (* 1 = 5.6258 loss)
I0524 08:16:40.424394 11835 sgd_solver.cpp:112] Iteration 52660, lr = 0.1
I0524 08:16:48.889621 11835 solver.cpp:239] Iteration 52670 (1.18117 iter/s, 8.46615s/10 iters), loss = 5.73551
I0524 08:16:48.889662 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73551 (* 1 = 5.73551 loss)
I0524 08:16:49.503221 11835 sgd_solver.cpp:112] Iteration 52670, lr = 0.1
I0524 08:16:58.177484 11835 solver.cpp:239] Iteration 52680 (1.07672 iter/s, 9.28746s/10 iters), loss = 5.3686
I0524 08:16:58.177541 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.3686 (* 1 = 5.3686 loss)
I0524 08:16:58.177851 11835 sgd_solver.cpp:112] Iteration 52680, lr = 0.1
I0524 08:17:07.137387 11835 solver.cpp:239] Iteration 52690 (1.11614 iter/s, 8.95949s/10 iters), loss = 6.92013
I0524 08:17:07.137483 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92013 (* 1 = 6.92013 loss)
I0524 08:17:07.137514 11835 sgd_solver.cpp:112] Iteration 52690, lr = 0.1
I0524 08:17:14.601882 11835 solver.cpp:239] Iteration 52700 (1.33974 iter/s, 7.46412s/10 iters), loss = 6.20319
I0524 08:17:14.602181 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20319 (* 1 = 6.20319 loss)
I0524 08:17:14.966421 11835 sgd_solver.cpp:112] Iteration 52700, lr = 0.1
I0524 08:17:22.435533 11835 solver.cpp:239] Iteration 52710 (1.27663 iter/s, 7.83313s/10 iters), loss = 4.81714
I0524 08:17:22.435586 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.81714 (* 1 = 4.81714 loss)
I0524 08:17:22.489769 11835 sgd_solver.cpp:112] Iteration 52710, lr = 0.1
I0524 08:17:29.787367 11835 solver.cpp:239] Iteration 52720 (1.36027 iter/s, 7.3515s/10 iters), loss = 5.96812
I0524 08:17:29.787478 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96812 (* 1 = 5.96812 loss)
I0524 08:17:29.866648 11835 sgd_solver.cpp:112] Iteration 52720, lr = 0.1
I0524 08:17:35.893326 11835 solver.cpp:239] Iteration 52730 (1.63782 iter/s, 6.10567s/10 iters), loss = 6.15205
I0524 08:17:35.893388 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15205 (* 1 = 6.15205 loss)
I0524 08:17:35.893519 11835 sgd_solver.cpp:112] Iteration 52730, lr = 0.1
I0524 08:17:41.466434 11835 solver.cpp:239] Iteration 52740 (1.79441 iter/s, 5.57285s/10 iters), loss = 6.75901
I0524 08:17:41.466475 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.75901 (* 1 = 6.75901 loss)
I0524 08:17:41.728289 11835 sgd_solver.cpp:112] Iteration 52740, lr = 0.1
I0524 08:17:47.551522 11835 solver.cpp:239] Iteration 52750 (1.64344 iter/s, 6.0848s/10 iters), loss = 6.26464
I0524 08:17:47.551661 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26464 (* 1 = 6.26464 loss)
I0524 08:17:48.379354 11835 sgd_solver.cpp:112] Iteration 52750, lr = 0.1
I0524 08:17:55.242374 11835 solver.cpp:239] Iteration 52760 (1.30032 iter/s, 7.69043s/10 iters), loss = 5.41867
I0524 08:17:55.242437 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.41867 (* 1 = 5.41867 loss)
I0524 08:17:55.242453 11835 sgd_solver.cpp:112] Iteration 52760, lr = 0.1
I0524 08:18:02.551291 11835 solver.cpp:239] Iteration 52770 (1.36833 iter/s, 7.3082s/10 iters), loss = 5.40316
I0524 08:18:02.551362 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.40316 (* 1 = 5.40316 loss)
I0524 08:18:02.551528 11835 sgd_solver.cpp:112] Iteration 52770, lr = 0.1
I0524 08:18:09.832237 11835 solver.cpp:239] Iteration 52780 (1.37351 iter/s, 7.28062s/10 iters), loss = 6.43364
I0524 08:18:09.832286 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43364 (* 1 = 6.43364 loss)
I0524 08:18:09.832433 11835 sgd_solver.cpp:112] Iteration 52780, lr = 0.1
I0524 08:18:16.291550 11835 solver.cpp:239] Iteration 52790 (1.54823 iter/s, 6.45901s/10 iters), loss = 6.10127
I0524 08:18:16.291609 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10127 (* 1 = 6.10127 loss)
I0524 08:18:16.291782 11835 sgd_solver.cpp:112] Iteration 52790, lr = 0.1
I0524 08:18:22.671321 11835 solver.cpp:239] Iteration 52800 (1.56753 iter/s, 6.37946s/10 iters), loss = 6.46298
I0524 08:18:22.671591 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46298 (* 1 = 6.46298 loss)
I0524 08:18:22.671614 11835 sgd_solver.cpp:112] Iteration 52800, lr = 0.1
I0524 08:18:28.527976 11835 solver.cpp:239] Iteration 52810 (1.70789 iter/s, 5.85517s/10 iters), loss = 5.17962
I0524 08:18:28.528013 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.17962 (* 1 = 5.17962 loss)
I0524 08:18:28.788199 11835 sgd_solver.cpp:112] Iteration 52810, lr = 0.1
I0524 08:18:35.454767 11835 solver.cpp:239] Iteration 52820 (1.44374 iter/s, 6.92648s/10 iters), loss = 5.67934
I0524 08:18:35.454818 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67934 (* 1 = 5.67934 loss)
I0524 08:18:35.454941 11835 sgd_solver.cpp:112] Iteration 52820, lr = 0.1
I0524 08:18:41.952188 11835 solver.cpp:239] Iteration 52830 (1.53914 iter/s, 6.49712s/10 iters), loss = 6.14558
I0524 08:18:41.952235 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14558 (* 1 = 6.14558 loss)
I0524 08:18:41.952810 11835 sgd_solver.cpp:112] Iteration 52830, lr = 0.1
I0524 08:18:49.679221 11835 solver.cpp:239] Iteration 52840 (1.29421 iter/s, 7.72669s/10 iters), loss = 6.6801
I0524 08:18:49.679275 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6801 (* 1 = 6.6801 loss)
I0524 08:18:49.679373 11835 sgd_solver.cpp:112] Iteration 52840, lr = 0.1
I0524 08:18:58.652743 11835 solver.cpp:239] Iteration 52850 (1.11444 iter/s, 8.97312s/10 iters), loss = 7.24817
I0524 08:18:58.653000 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.24817 (* 1 = 7.24817 loss)
I0524 08:18:58.653038 11835 sgd_solver.cpp:112] Iteration 52850, lr = 0.1
I0524 08:19:07.665458 11835 solver.cpp:239] Iteration 52860 (1.10974 iter/s, 9.01111s/10 iters), loss = 5.91444
I0524 08:19:07.665513 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91444 (* 1 = 5.91444 loss)
I0524 08:19:07.723721 11835 sgd_solver.cpp:112] Iteration 52860, lr = 0.1
I0524 08:19:15.483134 11835 solver.cpp:239] Iteration 52870 (1.27921 iter/s, 7.81732s/10 iters), loss = 6.86055
I0524 08:19:15.483180 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86055 (* 1 = 6.86055 loss)
I0524 08:19:15.483194 11835 sgd_solver.cpp:112] Iteration 52870, lr = 0.1
I0524 08:19:25.613178 11835 solver.cpp:239] Iteration 52880 (0.987264 iter/s, 10.129s/10 iters), loss = 5.23373
I0524 08:19:25.613227 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.23373 (* 1 = 5.23373 loss)
I0524 08:19:25.613386 11835 sgd_solver.cpp:112] Iteration 52880, lr = 0.1
I0524 08:19:32.263736 11835 solver.cpp:239] Iteration 52890 (1.5037 iter/s, 6.65024s/10 iters), loss = 6.82632
I0524 08:19:32.263908 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.82632 (* 1 = 6.82632 loss)
I0524 08:19:32.263957 11835 sgd_solver.cpp:112] Iteration 52890, lr = 0.1
I0524 08:19:40.740209 11835 solver.cpp:239] Iteration 52900 (1.1798 iter/s, 8.47598s/10 iters), loss = 5.31021
I0524 08:19:40.740268 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.31021 (* 1 = 5.31021 loss)
I0524 08:19:41.017207 11835 sgd_solver.cpp:112] Iteration 52900, lr = 0.1
I0524 08:19:48.017097 11835 solver.cpp:239] Iteration 52910 (1.37428 iter/s, 7.27655s/10 iters), loss = 5.82033
I0524 08:19:48.017158 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82033 (* 1 = 5.82033 loss)
I0524 08:19:48.017294 11835 sgd_solver.cpp:112] Iteration 52910, lr = 0.1
I0524 08:19:56.386653 11835 solver.cpp:239] Iteration 52920 (1.19486 iter/s, 8.36917s/10 iters), loss = 6.26938
I0524 08:19:56.386735 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26938 (* 1 = 6.26938 loss)
I0524 08:19:56.474493 11835 sgd_solver.cpp:112] Iteration 52920, lr = 0.1
I0524 08:20:04.600006 11835 solver.cpp:239] Iteration 52930 (1.21759 iter/s, 8.21295s/10 iters), loss = 6.41807
I0524 08:20:04.600344 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41807 (* 1 = 6.41807 loss)
I0524 08:20:04.600379 11835 sgd_solver.cpp:112] Iteration 52930, lr = 0.1
I0524 08:20:11.342046 11835 solver.cpp:239] Iteration 52940 (1.48335 iter/s, 6.7415s/10 iters), loss = 6.9216
I0524 08:20:11.342098 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9216 (* 1 = 6.9216 loss)
I0524 08:20:11.342393 11835 sgd_solver.cpp:112] Iteration 52940, lr = 0.1
I0524 08:20:17.780028 11835 solver.cpp:239] Iteration 52950 (1.55335 iter/s, 6.43768s/10 iters), loss = 5.72266
I0524 08:20:17.780074 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72266 (* 1 = 5.72266 loss)
I0524 08:20:17.780305 11835 sgd_solver.cpp:112] Iteration 52950, lr = 0.1
I0524 08:20:23.905462 11835 solver.cpp:239] Iteration 52960 (1.63261 iter/s, 6.12514s/10 iters), loss = 5.62368
I0524 08:20:23.905515 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.62368 (* 1 = 5.62368 loss)
I0524 08:20:24.002125 11835 sgd_solver.cpp:112] Iteration 52960, lr = 0.1
I0524 08:20:30.862344 11835 solver.cpp:239] Iteration 52970 (1.43749 iter/s, 6.95656s/10 iters), loss = 5.71582
I0524 08:20:30.862401 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71582 (* 1 = 5.71582 loss)
I0524 08:20:30.862475 11835 sgd_solver.cpp:112] Iteration 52970, lr = 0.1
I0524 08:20:37.604961 11835 solver.cpp:239] Iteration 52980 (1.48317 iter/s, 6.74231s/10 iters), loss = 4.98361
I0524 08:20:37.605104 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.98361 (* 1 = 4.98361 loss)
I0524 08:20:37.985728 11835 sgd_solver.cpp:112] Iteration 52980, lr = 0.1
I0524 08:20:44.914543 11835 solver.cpp:239] Iteration 52990 (1.36815 iter/s, 7.30914s/10 iters), loss = 5.88894
I0524 08:20:44.914603 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88894 (* 1 = 5.88894 loss)
I0524 08:20:44.914822 11835 sgd_solver.cpp:112] Iteration 52990, lr = 0.1
I0524 08:20:52.519522 11835 solver.cpp:239] Iteration 53000 (1.31499 iter/s, 7.60462s/10 iters), loss = 5.46708
I0524 08:20:52.519592 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.46708 (* 1 = 5.46708 loss)
I0524 08:20:52.677639 11835 sgd_solver.cpp:112] Iteration 53000, lr = 0.1
I0524 08:20:59.673334 11835 solver.cpp:239] Iteration 53010 (1.39792 iter/s, 7.15347s/10 iters), loss = 4.72413
I0524 08:20:59.673389 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.72413 (* 1 = 4.72413 loss)
I0524 08:20:59.673477 11835 sgd_solver.cpp:112] Iteration 53010, lr = 0.1
I0524 08:21:05.799178 11835 solver.cpp:239] Iteration 53020 (1.63251 iter/s, 6.12554s/10 iters), loss = 5.38369
I0524 08:21:05.799232 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.38369 (* 1 = 5.38369 loss)
I0524 08:21:06.648977 11835 sgd_solver.cpp:112] Iteration 53020, lr = 0.1
I0524 08:21:13.956413 11835 solver.cpp:239] Iteration 53030 (1.22596 iter/s, 8.15686s/10 iters), loss = 5.67369
I0524 08:21:13.956642 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67369 (* 1 = 5.67369 loss)
I0524 08:21:13.956692 11835 sgd_solver.cpp:112] Iteration 53030, lr = 0.1
I0524 08:21:20.270352 11835 solver.cpp:239] Iteration 53040 (1.58395 iter/s, 6.31334s/10 iters), loss = 5.7532
I0524 08:21:20.270392 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7532 (* 1 = 5.7532 loss)
I0524 08:21:20.270531 11835 sgd_solver.cpp:112] Iteration 53040, lr = 0.1
I0524 08:21:27.228451 11835 solver.cpp:239] Iteration 53050 (1.43724 iter/s, 6.95778s/10 iters), loss = 5.75188
I0524 08:21:27.228513 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75188 (* 1 = 5.75188 loss)
I0524 08:21:27.228755 11835 sgd_solver.cpp:112] Iteration 53050, lr = 0.1
I0524 08:21:33.128135 11835 solver.cpp:239] Iteration 53060 (1.69509 iter/s, 5.89941s/10 iters), loss = 6.338
I0524 08:21:33.128173 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.338 (* 1 = 6.338 loss)
I0524 08:21:33.128186 11835 sgd_solver.cpp:112] Iteration 53060, lr = 0.1
I0524 08:21:40.512298 11835 solver.cpp:239] Iteration 53070 (1.35431 iter/s, 7.38384s/10 iters), loss = 7.32637
I0524 08:21:40.512356 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.32637 (* 1 = 7.32637 loss)
I0524 08:21:40.512521 11835 sgd_solver.cpp:112] Iteration 53070, lr = 0.1
I0524 08:21:47.434089 11835 solver.cpp:239] Iteration 53080 (1.44478 iter/s, 6.92146s/10 iters), loss = 5.67512
I0524 08:21:47.434412 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67512 (* 1 = 5.67512 loss)
I0524 08:21:48.674101 11835 sgd_solver.cpp:112] Iteration 53080, lr = 0.1
I0524 08:21:54.459489 11835 solver.cpp:239] Iteration 53090 (1.42352 iter/s, 7.02485s/10 iters), loss = 5.94784
I0524 08:21:54.459535 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94784 (* 1 = 5.94784 loss)
I0524 08:21:55.367561 11835 sgd_solver.cpp:112] Iteration 53090, lr = 0.1
I0524 08:22:02.215039 11835 solver.cpp:239] Iteration 53100 (1.28946 iter/s, 7.75518s/10 iters), loss = 5.91449
I0524 08:22:02.215121 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91449 (* 1 = 5.91449 loss)
I0524 08:22:02.918778 11835 sgd_solver.cpp:112] Iteration 53100, lr = 0.1
I0524 08:22:09.451799 11835 solver.cpp:239] Iteration 53110 (1.3819 iter/s, 7.23642s/10 iters), loss = 5.95575
I0524 08:22:09.451854 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95575 (* 1 = 5.95575 loss)
I0524 08:22:09.451870 11835 sgd_solver.cpp:112] Iteration 53110, lr = 0.1
I0524 08:22:15.349997 11835 solver.cpp:239] Iteration 53120 (1.69552 iter/s, 5.89791s/10 iters), loss = 6.11437
I0524 08:22:15.350075 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11437 (* 1 = 6.11437 loss)
I0524 08:22:15.350162 11835 sgd_solver.cpp:112] Iteration 53120, lr = 0.1
I0524 08:22:21.191359 11835 solver.cpp:239] Iteration 53130 (1.71202 iter/s, 5.84106s/10 iters), loss = 7.2906
I0524 08:22:21.191579 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.2906 (* 1 = 7.2906 loss)
I0524 08:22:21.191615 11835 sgd_solver.cpp:112] Iteration 53130, lr = 0.1
I0524 08:22:32.299705 11835 solver.cpp:239] Iteration 53140 (0.900277 iter/s, 11.1077s/10 iters), loss = 6.72613
I0524 08:22:32.299763 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72613 (* 1 = 6.72613 loss)
I0524 08:22:32.891618 11835 sgd_solver.cpp:112] Iteration 53140, lr = 0.1
I0524 08:22:38.971886 11835 solver.cpp:239] Iteration 53150 (1.49883 iter/s, 6.67186s/10 iters), loss = 4.93086
I0524 08:22:38.971945 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.93086 (* 1 = 4.93086 loss)
I0524 08:22:39.374946 11835 sgd_solver.cpp:112] Iteration 53150, lr = 0.1
I0524 08:22:45.994324 11835 solver.cpp:239] Iteration 53160 (1.42407 iter/s, 7.02212s/10 iters), loss = 5.78686
I0524 08:22:45.994361 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78686 (* 1 = 5.78686 loss)
I0524 08:22:45.994374 11835 sgd_solver.cpp:112] Iteration 53160, lr = 0.1
I0524 08:22:53.793954 11835 solver.cpp:239] Iteration 53170 (1.28217 iter/s, 7.79928s/10 iters), loss = 6.06392
I0524 08:22:53.794123 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06392 (* 1 = 6.06392 loss)
I0524 08:22:54.665242 11835 sgd_solver.cpp:112] Iteration 53170, lr = 0.1
I0524 08:23:05.287586 11835 solver.cpp:239] Iteration 53180 (0.870092 iter/s, 11.493s/10 iters), loss = 7.03225
I0524 08:23:05.287673 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03225 (* 1 = 7.03225 loss)
I0524 08:23:05.974261 11835 sgd_solver.cpp:112] Iteration 53180, lr = 0.1
I0524 08:23:12.883780 11835 solver.cpp:239] Iteration 53190 (1.31651 iter/s, 7.59584s/10 iters), loss = 5.26601
I0524 08:23:12.883839 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.26601 (* 1 = 5.26601 loss)
I0524 08:23:13.069535 11835 sgd_solver.cpp:112] Iteration 53190, lr = 0.1
I0524 08:23:19.095116 11835 solver.cpp:239] Iteration 53200 (1.61004 iter/s, 6.21104s/10 iters), loss = 6.31549
I0524 08:23:19.095165 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31549 (* 1 = 6.31549 loss)
I0524 08:23:19.095582 11835 sgd_solver.cpp:112] Iteration 53200, lr = 0.1
I0524 08:23:27.349797 11835 solver.cpp:239] Iteration 53210 (1.21149 iter/s, 8.25431s/10 iters), loss = 4.74001
I0524 08:23:27.350008 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.74001 (* 1 = 4.74001 loss)
I0524 08:23:28.234733 11835 sgd_solver.cpp:112] Iteration 53210, lr = 0.1
I0524 08:23:36.725389 11835 solver.cpp:239] Iteration 53220 (1.06666 iter/s, 9.37503s/10 iters), loss = 6.49809
I0524 08:23:36.725442 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49809 (* 1 = 6.49809 loss)
I0524 08:23:36.725857 11835 sgd_solver.cpp:112] Iteration 53220, lr = 0.1
I0524 08:23:43.536554 11835 solver.cpp:239] Iteration 53230 (1.46825 iter/s, 6.81084s/10 iters), loss = 7.44511
I0524 08:23:43.536605 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.44511 (* 1 = 7.44511 loss)
I0524 08:23:43.599404 11835 sgd_solver.cpp:112] Iteration 53230, lr = 0.1
I0524 08:23:49.431412 11835 solver.cpp:239] Iteration 53240 (1.69647 iter/s, 5.89459s/10 iters), loss = 5.95162
I0524 08:23:49.431445 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95162 (* 1 = 5.95162 loss)
I0524 08:23:49.431468 11835 sgd_solver.cpp:112] Iteration 53240, lr = 0.1
I0524 08:23:55.690238 11835 solver.cpp:239] Iteration 53250 (1.59782 iter/s, 6.25854s/10 iters), loss = 5.6457
I0524 08:23:55.690289 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.6457 (* 1 = 5.6457 loss)
I0524 08:23:55.690304 11835 sgd_solver.cpp:112] Iteration 53250, lr = 0.1
I0524 08:24:04.631884 11835 solver.cpp:239] Iteration 53260 (1.11842 iter/s, 8.94118s/10 iters), loss = 5.33827
I0524 08:24:04.632146 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.33827 (* 1 = 5.33827 loss)
I0524 08:24:04.632189 11835 sgd_solver.cpp:112] Iteration 53260, lr = 0.1
I0524 08:24:11.061501 11835 solver.cpp:239] Iteration 53270 (1.55544 iter/s, 6.42905s/10 iters), loss = 5.37948
I0524 08:24:11.061550 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.37948 (* 1 = 5.37948 loss)
I0524 08:24:11.061689 11835 sgd_solver.cpp:112] Iteration 53270, lr = 0.1
I0524 08:24:17.230042 11835 solver.cpp:239] Iteration 53280 (1.62121 iter/s, 6.16824s/10 iters), loss = 5.75636
I0524 08:24:17.230098 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75636 (* 1 = 5.75636 loss)
I0524 08:24:17.230201 11835 sgd_solver.cpp:112] Iteration 53280, lr = 0.1
I0524 08:24:25.998961 11835 solver.cpp:239] Iteration 53290 (1.14044 iter/s, 8.76853s/10 iters), loss = 4.99069
I0524 08:24:25.999008 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.99069 (* 1 = 4.99069 loss)
I0524 08:24:25.999120 11835 sgd_solver.cpp:112] Iteration 53290, lr = 0.1
I0524 08:24:31.913697 11835 solver.cpp:239] Iteration 53300 (1.69077 iter/s, 5.91447s/10 iters), loss = 5.79367
I0524 08:24:31.913731 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79367 (* 1 = 5.79367 loss)
I0524 08:24:32.284155 11835 sgd_solver.cpp:112] Iteration 53300, lr = 0.1
I0524 08:24:40.065024 11835 solver.cpp:239] Iteration 53310 (1.22685 iter/s, 8.15097s/10 iters), loss = 6.43916
I0524 08:24:40.065165 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43916 (* 1 = 6.43916 loss)
I0524 08:24:40.217929 11835 sgd_solver.cpp:112] Iteration 53310, lr = 0.1
I0524 08:24:47.943958 11835 solver.cpp:239] Iteration 53320 (1.26928 iter/s, 7.87848s/10 iters), loss = 6.65447
I0524 08:24:47.944032 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65447 (* 1 = 6.65447 loss)
I0524 08:24:47.949636 11835 sgd_solver.cpp:112] Iteration 53320, lr = 0.1
I0524 08:24:55.061384 11835 solver.cpp:239] Iteration 53330 (1.40507 iter/s, 7.11706s/10 iters), loss = 6.57616
I0524 08:24:55.061539 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57616 (* 1 = 6.57616 loss)
I0524 08:24:55.333442 11835 sgd_solver.cpp:112] Iteration 53330, lr = 0.1
I0524 08:25:01.183820 11835 solver.cpp:239] Iteration 53340 (1.63343 iter/s, 6.1221s/10 iters), loss = 6.36936
I0524 08:25:01.183871 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36936 (* 1 = 6.36936 loss)
I0524 08:25:01.480525 11835 sgd_solver.cpp:112] Iteration 53340, lr = 0.1
I0524 08:25:07.992645 11835 solver.cpp:239] Iteration 53350 (1.46876 iter/s, 6.80848s/10 iters), loss = 5.80951
I0524 08:25:07.992777 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80951 (* 1 = 5.80951 loss)
I0524 08:25:07.992823 11835 sgd_solver.cpp:112] Iteration 53350, lr = 0.1
I0524 08:25:14.967404 11835 solver.cpp:239] Iteration 53360 (1.43381 iter/s, 6.97442s/10 iters), loss = 5.37645
I0524 08:25:14.967669 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.37645 (* 1 = 5.37645 loss)
I0524 08:25:15.013510 11835 sgd_solver.cpp:112] Iteration 53360, lr = 0.1
I0524 08:25:21.831758 11835 solver.cpp:239] Iteration 53370 (1.45691 iter/s, 6.86383s/10 iters), loss = 6.63066
I0524 08:25:21.831840 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63066 (* 1 = 6.63066 loss)
I0524 08:25:21.832082 11835 sgd_solver.cpp:112] Iteration 53370, lr = 0.1
I0524 08:25:30.163352 11835 solver.cpp:239] Iteration 53380 (1.20031 iter/s, 8.3312s/10 iters), loss = 5.697
I0524 08:25:30.163408 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.697 (* 1 = 5.697 loss)
I0524 08:25:30.163523 11835 sgd_solver.cpp:112] Iteration 53380, lr = 0.1
I0524 08:25:38.568230 11835 solver.cpp:239] Iteration 53390 (1.18984 iter/s, 8.4045s/10 iters), loss = 6.35464
I0524 08:25:38.568300 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35464 (* 1 = 6.35464 loss)
I0524 08:25:38.568388 11835 sgd_solver.cpp:112] Iteration 53390, lr = 0.1
I0524 08:25:44.853950 11835 solver.cpp:239] Iteration 53400 (1.59099 iter/s, 6.28541s/10 iters), loss = 6.16309
I0524 08:25:44.854019 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16309 (* 1 = 6.16309 loss)
I0524 08:25:44.854122 11835 sgd_solver.cpp:112] Iteration 53400, lr = 0.1
I0524 08:25:51.337440 11835 solver.cpp:239] Iteration 53410 (1.54245 iter/s, 6.48318s/10 iters), loss = 6.46892
I0524 08:25:51.337683 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46892 (* 1 = 6.46892 loss)
I0524 08:25:51.337721 11835 sgd_solver.cpp:112] Iteration 53410, lr = 0.1
I0524 08:25:59.410006 11835 solver.cpp:239] Iteration 53420 (1.23918 iter/s, 8.06983s/10 iters), loss = 6.61212
I0524 08:25:59.410074 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61212 (* 1 = 6.61212 loss)
I0524 08:25:59.410282 11835 sgd_solver.cpp:112] Iteration 53420, lr = 0.1
I0524 08:26:05.567416 11835 solver.cpp:239] Iteration 53430 (1.62416 iter/s, 6.15703s/10 iters), loss = 6.07297
I0524 08:26:05.567639 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07297 (* 1 = 6.07297 loss)
I0524 08:26:05.567698 11835 sgd_solver.cpp:112] Iteration 53430, lr = 0.1
I0524 08:26:13.976434 11835 solver.cpp:239] Iteration 53440 (1.18926 iter/s, 8.40859s/10 iters), loss = 6.03028
I0524 08:26:13.976496 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03028 (* 1 = 6.03028 loss)
I0524 08:26:13.976653 11835 sgd_solver.cpp:112] Iteration 53440, lr = 0.1
I0524 08:26:22.183966 11835 solver.cpp:239] Iteration 53450 (1.21845 iter/s, 8.20716s/10 iters), loss = 6.72963
I0524 08:26:22.184228 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72963 (* 1 = 6.72963 loss)
I0524 08:26:22.411507 11835 sgd_solver.cpp:112] Iteration 53450, lr = 0.1
I0524 08:26:30.356523 11835 solver.cpp:239] Iteration 53460 (1.22369 iter/s, 8.17201s/10 iters), loss = 5.43574
I0524 08:26:30.356606 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43574 (* 1 = 5.43574 loss)
I0524 08:26:30.356621 11835 sgd_solver.cpp:112] Iteration 53460, lr = 0.1
I0524 08:26:36.835355 11835 solver.cpp:239] Iteration 53470 (1.54357 iter/s, 6.47849s/10 iters), loss = 6.29115
I0524 08:26:36.835429 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29115 (* 1 = 6.29115 loss)
I0524 08:26:37.071552 11835 sgd_solver.cpp:112] Iteration 53470, lr = 0.1
I0524 08:26:44.410763 11835 solver.cpp:239] Iteration 53480 (1.32012 iter/s, 7.57506s/10 iters), loss = 5.89856
I0524 08:26:44.410816 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89856 (* 1 = 5.89856 loss)
I0524 08:26:44.432454 11835 sgd_solver.cpp:112] Iteration 53480, lr = 0.1
I0524 08:26:51.427513 11835 solver.cpp:239] Iteration 53490 (1.42523 iter/s, 7.01643s/10 iters), loss = 7.4604
I0524 08:26:51.427611 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.4604 (* 1 = 7.4604 loss)
I0524 08:26:51.427634 11835 sgd_solver.cpp:112] Iteration 53490, lr = 0.1
I0524 08:26:59.535509 11835 solver.cpp:239] Iteration 53500 (1.23345 iter/s, 8.10732s/10 iters), loss = 4.97775
I0524 08:26:59.535797 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.97775 (* 1 = 4.97775 loss)
I0524 08:26:59.545374 11835 sgd_solver.cpp:112] Iteration 53500, lr = 0.1
I0524 08:27:06.306368 11835 solver.cpp:239] Iteration 53510 (1.47703 iter/s, 6.77034s/10 iters), loss = 6.32632
I0524 08:27:06.306424 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32632 (* 1 = 6.32632 loss)
I0524 08:27:06.306462 11835 sgd_solver.cpp:112] Iteration 53510, lr = 0.1
I0524 08:27:13.643283 11835 solver.cpp:239] Iteration 53520 (1.36303 iter/s, 7.33658s/10 iters), loss = 6.35471
I0524 08:27:13.643345 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35471 (* 1 = 6.35471 loss)
I0524 08:27:13.643366 11835 sgd_solver.cpp:112] Iteration 53520, lr = 0.1
I0524 08:27:23.104321 11835 solver.cpp:239] Iteration 53530 (1.05714 iter/s, 9.4595s/10 iters), loss = 7.60399
I0524 08:27:23.104384 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.60399 (* 1 = 7.60399 loss)
I0524 08:27:23.908620 11835 sgd_solver.cpp:112] Iteration 53530, lr = 0.1
I0524 08:27:32.479429 11835 solver.cpp:239] Iteration 53540 (1.0667 iter/s, 9.37469s/10 iters), loss = 5.55636
I0524 08:27:32.479786 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.55636 (* 1 = 5.55636 loss)
I0524 08:27:32.479840 11835 sgd_solver.cpp:112] Iteration 53540, lr = 0.1
I0524 08:27:39.087805 11835 solver.cpp:239] Iteration 53550 (1.51345 iter/s, 6.60742s/10 iters), loss = 5.75111
I0524 08:27:39.087867 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75111 (* 1 = 5.75111 loss)
I0524 08:27:39.184371 11835 sgd_solver.cpp:112] Iteration 53550, lr = 0.1
I0524 08:27:45.954139 11835 solver.cpp:239] Iteration 53560 (1.45645 iter/s, 6.86599s/10 iters), loss = 6.21416
I0524 08:27:45.954226 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21416 (* 1 = 6.21416 loss)
I0524 08:27:45.954378 11835 sgd_solver.cpp:112] Iteration 53560, lr = 0.1
I0524 08:27:51.788946 11835 solver.cpp:239] Iteration 53570 (1.71394 iter/s, 5.83451s/10 iters), loss = 6.39955
I0524 08:27:51.788991 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39955 (* 1 = 6.39955 loss)
I0524 08:27:51.832444 11835 sgd_solver.cpp:112] Iteration 53570, lr = 0.1
I0524 08:27:59.028697 11835 solver.cpp:239] Iteration 53580 (1.38133 iter/s, 7.2394s/10 iters), loss = 6.18794
I0524 08:27:59.028793 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18794 (* 1 = 6.18794 loss)
I0524 08:27:59.028837 11835 sgd_solver.cpp:112] Iteration 53580, lr = 0.1
I0524 08:28:08.168699 11835 solver.cpp:239] Iteration 53590 (1.09414 iter/s, 9.13956s/10 iters), loss = 5.80994
I0524 08:28:08.168974 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80994 (* 1 = 5.80994 loss)
I0524 08:28:08.692443 11835 sgd_solver.cpp:112] Iteration 53590, lr = 0.1
I0524 08:28:16.422236 11835 solver.cpp:239] Iteration 53600 (1.21168 iter/s, 8.25301s/10 iters), loss = 6.48861
I0524 08:28:16.422292 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48861 (* 1 = 6.48861 loss)
I0524 08:28:16.422317 11835 sgd_solver.cpp:112] Iteration 53600, lr = 0.1
I0524 08:28:24.772635 11835 solver.cpp:239] Iteration 53610 (1.1976 iter/s, 8.35003s/10 iters), loss = 5.0196
I0524 08:28:24.772687 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.0196 (* 1 = 5.0196 loss)
I0524 08:28:25.076185 11835 sgd_solver.cpp:112] Iteration 53610, lr = 0.1
I0524 08:28:31.624203 11835 solver.cpp:239] Iteration 53620 (1.45959 iter/s, 6.85124s/10 iters), loss = 6.44372
I0524 08:28:31.624279 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44372 (* 1 = 6.44372 loss)
I0524 08:28:31.624323 11835 sgd_solver.cpp:112] Iteration 53620, lr = 0.1
I0524 08:28:38.872826 11835 solver.cpp:239] Iteration 53630 (1.37964 iter/s, 7.24829s/10 iters), loss = 5.55922
I0524 08:28:38.873031 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.55922 (* 1 = 5.55922 loss)
I0524 08:28:38.873049 11835 sgd_solver.cpp:112] Iteration 53630, lr = 0.1
I0524 08:28:45.391670 11835 solver.cpp:239] Iteration 53640 (1.53458 iter/s, 6.51646s/10 iters), loss = 6.59217
I0524 08:28:45.391722 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59217 (* 1 = 6.59217 loss)
I0524 08:28:45.803650 11835 sgd_solver.cpp:112] Iteration 53640, lr = 0.1
I0524 08:28:54.556797 11835 solver.cpp:239] Iteration 53650 (1.09114 iter/s, 9.16472s/10 iters), loss = 5.83576
I0524 08:28:54.556887 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83576 (* 1 = 5.83576 loss)
I0524 08:28:54.862030 11835 sgd_solver.cpp:112] Iteration 53650, lr = 0.1
I0524 08:29:01.821055 11835 solver.cpp:239] Iteration 53660 (1.37667 iter/s, 7.2639s/10 iters), loss = 6.2325
I0524 08:29:01.821157 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2325 (* 1 = 6.2325 loss)
I0524 08:29:01.821182 11835 sgd_solver.cpp:112] Iteration 53660, lr = 0.1
I0524 08:29:08.546017 11835 solver.cpp:239] Iteration 53670 (1.48708 iter/s, 6.72461s/10 iters), loss = 5.91429
I0524 08:29:08.546072 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91429 (* 1 = 5.91429 loss)
I0524 08:29:08.546458 11835 sgd_solver.cpp:112] Iteration 53670, lr = 0.1
I0524 08:29:14.884258 11835 solver.cpp:239] Iteration 53680 (1.5778 iter/s, 6.33793s/10 iters), loss = 5.63007
I0524 08:29:14.884513 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63007 (* 1 = 5.63007 loss)
I0524 08:29:14.884544 11835 sgd_solver.cpp:112] Iteration 53680, lr = 0.1
I0524 08:29:21.774610 11835 solver.cpp:239] Iteration 53690 (1.45146 iter/s, 6.8896s/10 iters), loss = 5.89275
I0524 08:29:21.774677 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89275 (* 1 = 5.89275 loss)
I0524 08:29:22.414383 11835 sgd_solver.cpp:112] Iteration 53690, lr = 0.1
I0524 08:29:29.253049 11835 solver.cpp:239] Iteration 53700 (1.33724 iter/s, 7.47809s/10 iters), loss = 5.87448
I0524 08:29:29.253104 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87448 (* 1 = 5.87448 loss)
I0524 08:29:29.253278 11835 sgd_solver.cpp:112] Iteration 53700, lr = 0.1
I0524 08:29:35.575947 11835 solver.cpp:239] Iteration 53710 (1.58163 iter/s, 6.3226s/10 iters), loss = 6.42802
I0524 08:29:35.575999 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42802 (* 1 = 6.42802 loss)
I0524 08:29:35.588929 11835 sgd_solver.cpp:112] Iteration 53710, lr = 0.1
I0524 08:29:44.121177 11835 solver.cpp:239] Iteration 53720 (1.17029 iter/s, 8.54486s/10 iters), loss = 5.9085
I0524 08:29:44.121213 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9085 (* 1 = 5.9085 loss)
I0524 08:29:44.729933 11835 sgd_solver.cpp:112] Iteration 53720, lr = 0.1
I0524 08:29:51.586905 11835 solver.cpp:239] Iteration 53730 (1.33952 iter/s, 7.46539s/10 iters), loss = 6.09042
I0524 08:29:51.587052 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09042 (* 1 = 6.09042 loss)
I0524 08:29:51.587097 11835 sgd_solver.cpp:112] Iteration 53730, lr = 0.1
I0524 08:29:58.503188 11835 solver.cpp:239] Iteration 53740 (1.44595 iter/s, 6.91588s/10 iters), loss = 6.28819
I0524 08:29:58.503242 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.28819 (* 1 = 6.28819 loss)
I0524 08:29:58.782876 11835 sgd_solver.cpp:112] Iteration 53740, lr = 0.1
I0524 08:30:06.117772 11835 solver.cpp:239] Iteration 53750 (1.31333 iter/s, 7.61423s/10 iters), loss = 5.89215
I0524 08:30:06.117844 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89215 (* 1 = 5.89215 loss)
I0524 08:30:06.154656 11835 sgd_solver.cpp:112] Iteration 53750, lr = 0.1
I0524 08:30:12.229585 11835 solver.cpp:239] Iteration 53760 (1.63626 iter/s, 6.11148s/10 iters), loss = 6.11473
I0524 08:30:12.229746 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11473 (* 1 = 6.11473 loss)
I0524 08:30:12.229768 11835 sgd_solver.cpp:112] Iteration 53760, lr = 0.1
I0524 08:30:18.991185 11835 solver.cpp:239] Iteration 53770 (1.47906 iter/s, 6.76104s/10 iters), loss = 6.14716
I0524 08:30:18.991227 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14716 (* 1 = 6.14716 loss)
I0524 08:30:18.991241 11835 sgd_solver.cpp:112] Iteration 53770, lr = 0.1
I0524 08:30:25.681252 11835 solver.cpp:239] Iteration 53780 (1.49508 iter/s, 6.68862s/10 iters), loss = 7.20354
I0524 08:30:25.681510 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.20354 (* 1 = 7.20354 loss)
I0524 08:30:25.681550 11835 sgd_solver.cpp:112] Iteration 53780, lr = 0.1
I0524 08:30:32.790617 11835 solver.cpp:239] Iteration 53790 (1.4069 iter/s, 7.10783s/10 iters), loss = 5.51369
I0524 08:30:32.790675 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.51369 (* 1 = 5.51369 loss)
I0524 08:30:32.790805 11835 sgd_solver.cpp:112] Iteration 53790, lr = 0.1
I0524 08:30:41.602784 11835 solver.cpp:239] Iteration 53800 (1.13484 iter/s, 8.81178s/10 iters), loss = 6.23972
I0524 08:30:41.602828 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23972 (* 1 = 6.23972 loss)
I0524 08:30:41.682741 11835 sgd_solver.cpp:112] Iteration 53800, lr = 0.1
I0524 08:30:50.676213 11835 solver.cpp:239] Iteration 53810 (1.10217 iter/s, 9.07303s/10 iters), loss = 6.50104
I0524 08:30:50.676273 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50104 (* 1 = 6.50104 loss)
I0524 08:30:51.497858 11835 sgd_solver.cpp:112] Iteration 53810, lr = 0.1
I0524 08:30:58.324609 11835 solver.cpp:239] Iteration 53820 (1.30753 iter/s, 7.64802s/10 iters), loss = 5.39168
I0524 08:30:58.324808 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.39168 (* 1 = 5.39168 loss)
I0524 08:30:58.324842 11835 sgd_solver.cpp:112] Iteration 53820, lr = 0.1
I0524 08:31:04.519767 11835 solver.cpp:239] Iteration 53830 (1.61428 iter/s, 6.1947s/10 iters), loss = 5.86629
I0524 08:31:04.519821 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86629 (* 1 = 5.86629 loss)
I0524 08:31:04.519839 11835 sgd_solver.cpp:112] Iteration 53830, lr = 0.1
I0524 08:31:11.595460 11835 solver.cpp:239] Iteration 53840 (1.41335 iter/s, 7.07538s/10 iters), loss = 5.64273
I0524 08:31:11.595502 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.64273 (* 1 = 5.64273 loss)
I0524 08:31:11.636284 11835 sgd_solver.cpp:112] Iteration 53840, lr = 0.1
I0524 08:31:18.123663 11835 solver.cpp:239] Iteration 53850 (1.53189 iter/s, 6.52788s/10 iters), loss = 5.72273
I0524 08:31:18.123785 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72273 (* 1 = 5.72273 loss)
I0524 08:31:18.489850 11835 sgd_solver.cpp:112] Iteration 53850, lr = 0.1
I0524 08:31:26.024518 11835 solver.cpp:239] Iteration 53860 (1.26575 iter/s, 7.90048s/10 iters), loss = 5.91579
I0524 08:31:26.024569 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91579 (* 1 = 5.91579 loss)
I0524 08:31:26.024755 11835 sgd_solver.cpp:112] Iteration 53860, lr = 0.1
I0524 08:31:34.118155 11835 solver.cpp:239] Iteration 53870 (1.23559 iter/s, 8.09327s/10 iters), loss = 7.01773
I0524 08:31:34.118317 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01773 (* 1 = 7.01773 loss)
I0524 08:31:34.118365 11835 sgd_solver.cpp:112] Iteration 53870, lr = 0.1
I0524 08:31:39.750167 11835 solver.cpp:239] Iteration 53880 (1.77568 iter/s, 5.63164s/10 iters), loss = 5.7361
I0524 08:31:39.750219 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7361 (* 1 = 5.7361 loss)
I0524 08:31:39.750236 11835 sgd_solver.cpp:112] Iteration 53880, lr = 0.1
I0524 08:31:47.201267 11835 solver.cpp:239] Iteration 53890 (1.34216 iter/s, 7.45069s/10 iters), loss = 6.30326
I0524 08:31:47.201331 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30326 (* 1 = 6.30326 loss)
I0524 08:31:47.244709 11835 sgd_solver.cpp:112] Iteration 53890, lr = 0.1
I0524 08:31:54.037433 11835 solver.cpp:239] Iteration 53900 (1.46288 iter/s, 6.83584s/10 iters), loss = 5.65377
I0524 08:31:54.037485 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.65377 (* 1 = 5.65377 loss)
I0524 08:31:54.284099 11835 sgd_solver.cpp:112] Iteration 53900, lr = 0.1
I0524 08:32:01.214257 11835 solver.cpp:239] Iteration 53910 (1.39344 iter/s, 7.1765s/10 iters), loss = 5.46571
I0524 08:32:01.214316 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.46571 (* 1 = 5.46571 loss)
I0524 08:32:01.214557 11835 sgd_solver.cpp:112] Iteration 53910, lr = 0.1
I0524 08:32:08.210624 11835 solver.cpp:239] Iteration 53920 (1.42938 iter/s, 6.99604s/10 iters), loss = 6.8539
I0524 08:32:08.210935 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8539 (* 1 = 6.8539 loss)
I0524 08:32:08.509727 11835 sgd_solver.cpp:112] Iteration 53920, lr = 0.1
I0524 08:32:16.714840 11835 solver.cpp:239] Iteration 53930 (1.17597 iter/s, 8.50361s/10 iters), loss = 6.09727
I0524 08:32:16.714900 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09727 (* 1 = 6.09727 loss)
I0524 08:32:16.715019 11835 sgd_solver.cpp:112] Iteration 53930, lr = 0.1
I0524 08:32:24.379129 11835 solver.cpp:239] Iteration 53940 (1.30481 iter/s, 7.66394s/10 iters), loss = 5.82197
I0524 08:32:24.379179 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82197 (* 1 = 5.82197 loss)
I0524 08:32:24.608834 11835 sgd_solver.cpp:112] Iteration 53940, lr = 0.1
I0524 08:32:30.837563 11835 solver.cpp:239] Iteration 53950 (1.54843 iter/s, 6.45814s/10 iters), loss = 5.26716
I0524 08:32:30.837604 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.26716 (* 1 = 5.26716 loss)
I0524 08:32:31.124456 11835 sgd_solver.cpp:112] Iteration 53950, lr = 0.1
I0524 08:32:37.155624 11835 solver.cpp:239] Iteration 53960 (1.58284 iter/s, 6.31777s/10 iters), loss = 6.21897
I0524 08:32:37.155676 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21897 (* 1 = 6.21897 loss)
I0524 08:32:37.723912 11835 sgd_solver.cpp:112] Iteration 53960, lr = 0.1
I0524 08:32:44.048012 11835 solver.cpp:239] Iteration 53970 (1.45094 iter/s, 6.89207s/10 iters), loss = 7.04959
I0524 08:32:44.048254 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04959 (* 1 = 7.04959 loss)
I0524 08:32:44.524992 11835 sgd_solver.cpp:112] Iteration 53970, lr = 0.1
I0524 08:32:52.382385 11835 solver.cpp:239] Iteration 53980 (1.19993 iter/s, 8.33383s/10 iters), loss = 6.6903
I0524 08:32:52.382429 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6903 (* 1 = 6.6903 loss)
I0524 08:32:52.696555 11835 sgd_solver.cpp:112] Iteration 53980, lr = 0.1
I0524 08:32:59.458636 11835 solver.cpp:239] Iteration 53990 (1.41324 iter/s, 7.07593s/10 iters), loss = 6.43894
I0524 08:32:59.458691 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43894 (* 1 = 6.43894 loss)
I0524 08:32:59.521401 11835 sgd_solver.cpp:112] Iteration 53990, lr = 0.1
I0524 08:33:07.340631 11835 solver.cpp:239] Iteration 54000 (1.26877 iter/s, 7.88163s/10 iters), loss = 5.67449
I0524 08:33:07.340688 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67449 (* 1 = 5.67449 loss)
I0524 08:33:07.381076 11835 sgd_solver.cpp:112] Iteration 54000, lr = 0.1
I0524 08:33:13.286264 11835 solver.cpp:239] Iteration 54010 (1.68199 iter/s, 5.94535s/10 iters), loss = 5.39596
I0524 08:33:13.286332 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.39596 (* 1 = 5.39596 loss)
I0524 08:33:13.542449 11835 sgd_solver.cpp:112] Iteration 54010, lr = 0.1
I0524 08:33:23.529906 11835 solver.cpp:239] Iteration 54020 (0.976258 iter/s, 10.2432s/10 iters), loss = 5.97967
I0524 08:33:23.530220 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97967 (* 1 = 5.97967 loss)
I0524 08:33:24.266373 11835 sgd_solver.cpp:112] Iteration 54020, lr = 0.1
I0524 08:33:32.698185 11835 solver.cpp:239] Iteration 54030 (1.09079 iter/s, 9.16766s/10 iters), loss = 5.74818
I0524 08:33:32.698257 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74818 (* 1 = 5.74818 loss)
I0524 08:33:32.821507 11835 sgd_solver.cpp:112] Iteration 54030, lr = 0.1
I0524 08:33:39.575978 11835 solver.cpp:239] Iteration 54040 (1.45403 iter/s, 6.87744s/10 iters), loss = 6.30936
I0524 08:33:39.576086 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30936 (* 1 = 6.30936 loss)
I0524 08:33:39.576122 11835 sgd_solver.cpp:112] Iteration 54040, lr = 0.1
I0524 08:33:46.864040 11835 solver.cpp:239] Iteration 54050 (1.37217 iter/s, 7.2877s/10 iters), loss = 5.93049
I0524 08:33:46.864095 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93049 (* 1 = 5.93049 loss)
I0524 08:33:46.864459 11835 sgd_solver.cpp:112] Iteration 54050, lr = 0.1
I0524 08:33:53.678432 11835 solver.cpp:239] Iteration 54060 (1.46756 iter/s, 6.81403s/10 iters), loss = 5.81502
I0524 08:33:53.678673 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81502 (* 1 = 5.81502 loss)
I0524 08:33:54.403576 11835 sgd_solver.cpp:112] Iteration 54060, lr = 0.1
I0524 08:34:02.969790 11835 solver.cpp:239] Iteration 54070 (1.07634 iter/s, 9.29079s/10 iters), loss = 4.82153
I0524 08:34:02.969840 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.82153 (* 1 = 4.82153 loss)
I0524 08:34:02.970022 11835 sgd_solver.cpp:112] Iteration 54070, lr = 0.1
I0524 08:34:10.008905 11835 solver.cpp:239] Iteration 54080 (1.4207 iter/s, 7.03878s/10 iters), loss = 6.24356
I0524 08:34:10.008963 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24356 (* 1 = 6.24356 loss)
I0524 08:34:10.009317 11835 sgd_solver.cpp:112] Iteration 54080, lr = 0.1
I0524 08:34:16.523710 11835 solver.cpp:239] Iteration 54090 (1.53504 iter/s, 6.51449s/10 iters), loss = 6.29848
I0524 08:34:16.523761 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29848 (* 1 = 6.29848 loss)
I0524 08:34:17.825935 11835 sgd_solver.cpp:112] Iteration 54090, lr = 0.1
I0524 08:34:25.735916 11835 solver.cpp:239] Iteration 54100 (1.08556 iter/s, 9.2118s/10 iters), loss = 5.87995
I0524 08:34:25.736208 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87995 (* 1 = 5.87995 loss)
I0524 08:34:25.736292 11835 sgd_solver.cpp:112] Iteration 54100, lr = 0.1
I0524 08:34:34.435377 11835 solver.cpp:239] Iteration 54110 (1.1496 iter/s, 8.69868s/10 iters), loss = 6.74682
I0524 08:34:34.435431 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74682 (* 1 = 6.74682 loss)
I0524 08:34:34.435672 11835 sgd_solver.cpp:112] Iteration 54110, lr = 0.1
I0524 08:34:44.172514 11835 solver.cpp:239] Iteration 54120 (1.02704 iter/s, 9.7367s/10 iters), loss = 4.9046
I0524 08:34:44.172575 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.9046 (* 1 = 4.9046 loss)
I0524 08:34:44.172735 11835 sgd_solver.cpp:112] Iteration 54120, lr = 0.1
I0524 08:34:50.638653 11835 solver.cpp:239] Iteration 54130 (1.54659 iter/s, 6.46583s/10 iters), loss = 5.84655
I0524 08:34:50.638734 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84655 (* 1 = 5.84655 loss)
I0524 08:34:50.661553 11835 sgd_solver.cpp:112] Iteration 54130, lr = 0.1
I0524 08:34:56.736949 11835 solver.cpp:239] Iteration 54140 (1.63989 iter/s, 6.09797s/10 iters), loss = 7.24889
I0524 08:34:56.737187 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.24889 (* 1 = 7.24889 loss)
I0524 08:34:56.737226 11835 sgd_solver.cpp:112] Iteration 54140, lr = 0.1
I0524 08:35:04.131333 11835 solver.cpp:239] Iteration 54150 (1.35247 iter/s, 7.39388s/10 iters), loss = 6.31901
I0524 08:35:04.131391 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31901 (* 1 = 6.31901 loss)
I0524 08:35:04.131639 11835 sgd_solver.cpp:112] Iteration 54150, lr = 0.1
I0524 08:35:11.256719 11835 solver.cpp:239] Iteration 54160 (1.4035 iter/s, 7.12506s/10 iters), loss = 7.29906
I0524 08:35:11.256783 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.29906 (* 1 = 7.29906 loss)
I0524 08:35:11.256857 11835 sgd_solver.cpp:112] Iteration 54160, lr = 0.1
I0524 08:35:19.019695 11835 solver.cpp:239] Iteration 54170 (1.28823 iter/s, 7.76261s/10 iters), loss = 6.19906
I0524 08:35:19.019754 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19906 (* 1 = 6.19906 loss)
I0524 08:35:19.025658 11835 sgd_solver.cpp:112] Iteration 54170, lr = 0.1
I0524 08:35:28.086262 11835 solver.cpp:239] Iteration 54180 (1.103 iter/s, 9.06616s/10 iters), loss = 6.70233
I0524 08:35:28.086485 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70233 (* 1 = 6.70233 loss)
I0524 08:35:28.086509 11835 sgd_solver.cpp:112] Iteration 54180, lr = 0.1
I0524 08:35:35.495606 11835 solver.cpp:239] Iteration 54190 (1.34984 iter/s, 7.40826s/10 iters), loss = 6.66115
I0524 08:35:35.495687 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66115 (* 1 = 6.66115 loss)
I0524 08:35:35.495776 11835 sgd_solver.cpp:112] Iteration 54190, lr = 0.1
I0524 08:35:42.630931 11835 solver.cpp:239] Iteration 54200 (1.40154 iter/s, 7.13498s/10 iters), loss = 6.09604
I0524 08:35:42.630978 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09604 (* 1 = 6.09604 loss)
I0524 08:35:42.631314 11835 sgd_solver.cpp:112] Iteration 54200, lr = 0.1
I0524 08:35:49.752938 11835 solver.cpp:239] Iteration 54210 (1.40416 iter/s, 7.1217s/10 iters), loss = 6.37932
I0524 08:35:49.752976 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37932 (* 1 = 6.37932 loss)
I0524 08:35:49.753298 11835 sgd_solver.cpp:112] Iteration 54210, lr = 0.1
I0524 08:35:57.664662 11835 solver.cpp:239] Iteration 54220 (1.264 iter/s, 7.91137s/10 iters), loss = 5.23523
I0524 08:35:57.664727 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.23523 (* 1 = 5.23523 loss)
I0524 08:35:58.591588 11835 sgd_solver.cpp:112] Iteration 54220, lr = 0.1
I0524 08:36:04.741714 11835 solver.cpp:239] Iteration 54230 (1.41309 iter/s, 7.07671s/10 iters), loss = 5.37531
I0524 08:36:04.741785 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.37531 (* 1 = 5.37531 loss)
I0524 08:36:04.769709 11835 sgd_solver.cpp:112] Iteration 54230, lr = 0.1
I0524 08:36:10.858482 11835 solver.cpp:239] Iteration 54240 (1.63493 iter/s, 6.11647s/10 iters), loss = 6.22877
I0524 08:36:10.858543 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22877 (* 1 = 6.22877 loss)
I0524 08:36:10.858777 11835 sgd_solver.cpp:112] Iteration 54240, lr = 0.1
I0524 08:36:18.754739 11835 solver.cpp:239] Iteration 54250 (1.26648 iter/s, 7.89588s/10 iters), loss = 5.74621
I0524 08:36:18.754794 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74621 (* 1 = 5.74621 loss)
I0524 08:36:18.754999 11835 sgd_solver.cpp:112] Iteration 54250, lr = 0.1
I0524 08:36:26.997735 11835 solver.cpp:239] Iteration 54260 (1.21321 iter/s, 8.24262s/10 iters), loss = 5.03071
I0524 08:36:26.997795 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.03071 (* 1 = 5.03071 loss)
I0524 08:36:26.997903 11835 sgd_solver.cpp:112] Iteration 54260, lr = 0.1
I0524 08:36:33.071660 11835 solver.cpp:239] Iteration 54270 (1.64646 iter/s, 6.07363s/10 iters), loss = 4.91664
I0524 08:36:33.071802 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.91664 (* 1 = 4.91664 loss)
I0524 08:36:33.071815 11835 sgd_solver.cpp:112] Iteration 54270, lr = 0.1
I0524 08:36:39.998423 11835 solver.cpp:239] Iteration 54280 (1.44377 iter/s, 6.92631s/10 iters), loss = 5.48035
I0524 08:36:39.998482 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.48035 (* 1 = 5.48035 loss)
I0524 08:36:39.998570 11835 sgd_solver.cpp:112] Iteration 54280, lr = 0.1
I0524 08:36:46.513118 11835 solver.cpp:239] Iteration 54290 (1.53507 iter/s, 6.51436s/10 iters), loss = 5.43482
I0524 08:36:46.513221 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43482 (* 1 = 5.43482 loss)
I0524 08:36:46.513309 11835 sgd_solver.cpp:112] Iteration 54290, lr = 0.1
I0524 08:36:52.737901 11835 solver.cpp:239] Iteration 54300 (1.60656 iter/s, 6.22448s/10 iters), loss = 6.59769
I0524 08:36:52.737948 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59769 (* 1 = 6.59769 loss)
I0524 08:36:52.737963 11835 sgd_solver.cpp:112] Iteration 54300, lr = 0.1
I0524 08:36:58.960738 11835 solver.cpp:239] Iteration 54310 (1.60708 iter/s, 6.22247s/10 iters), loss = 5.95835
I0524 08:36:58.960793 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95835 (* 1 = 5.95835 loss)
I0524 08:36:58.961251 11835 sgd_solver.cpp:112] Iteration 54310, lr = 0.1
I0524 08:37:04.730525 11835 solver.cpp:239] Iteration 54320 (1.73325 iter/s, 5.7695s/10 iters), loss = 5.62126
I0524 08:37:04.730785 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.62126 (* 1 = 5.62126 loss)
I0524 08:37:04.781038 11835 sgd_solver.cpp:112] Iteration 54320, lr = 0.1
I0524 08:37:13.049268 11835 solver.cpp:239] Iteration 54330 (1.20218 iter/s, 8.31819s/10 iters), loss = 5.94798
I0524 08:37:13.049325 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94798 (* 1 = 5.94798 loss)
I0524 08:37:13.941793 11835 sgd_solver.cpp:112] Iteration 54330, lr = 0.1
I0524 08:37:21.392657 11835 solver.cpp:239] Iteration 54340 (1.19861 iter/s, 8.34301s/10 iters), loss = 5.89864
I0524 08:37:21.392696 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89864 (* 1 = 5.89864 loss)
I0524 08:37:21.403082 11835 sgd_solver.cpp:112] Iteration 54340, lr = 0.1
I0524 08:37:28.461975 11835 solver.cpp:239] Iteration 54350 (1.41463 iter/s, 7.06899s/10 iters), loss = 6.34503
I0524 08:37:28.462044 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34503 (* 1 = 6.34503 loss)
I0524 08:37:28.861100 11835 sgd_solver.cpp:112] Iteration 54350, lr = 0.1
I0524 08:37:36.615257 11835 solver.cpp:239] Iteration 54360 (1.22656 iter/s, 8.15291s/10 iters), loss = 5.64686
I0524 08:37:36.615582 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.64686 (* 1 = 5.64686 loss)
I0524 08:37:36.666680 11835 sgd_solver.cpp:112] Iteration 54360, lr = 0.1
I0524 08:37:43.073328 11835 solver.cpp:239] Iteration 54370 (1.54856 iter/s, 6.45761s/10 iters), loss = 6.49133
I0524 08:37:43.073433 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49133 (* 1 = 6.49133 loss)
I0524 08:37:43.073457 11835 sgd_solver.cpp:112] Iteration 54370, lr = 0.1
I0524 08:37:53.093492 11835 solver.cpp:239] Iteration 54380 (0.998247 iter/s, 10.0176s/10 iters), loss = 5.7306
I0524 08:37:53.093542 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7306 (* 1 = 5.7306 loss)
I0524 08:37:53.763443 11835 sgd_solver.cpp:112] Iteration 54380, lr = 0.1
I0524 08:38:02.291085 11835 solver.cpp:239] Iteration 54390 (1.08729 iter/s, 9.19718s/10 iters), loss = 6.85119
I0524 08:38:02.291153 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.85119 (* 1 = 6.85119 loss)
I0524 08:38:03.685742 11835 sgd_solver.cpp:112] Iteration 54390, lr = 0.1
I0524 08:38:10.147248 11835 solver.cpp:239] Iteration 54400 (1.27295 iter/s, 7.85578s/10 iters), loss = 4.85099
I0524 08:38:10.147359 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.85099 (* 1 = 4.85099 loss)
I0524 08:38:10.554922 11835 sgd_solver.cpp:112] Iteration 54400, lr = 0.1
I0524 08:38:17.971963 11835 solver.cpp:239] Iteration 54410 (1.27807 iter/s, 7.82431s/10 iters), loss = 6.84818
I0524 08:38:17.972012 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84818 (* 1 = 6.84818 loss)
I0524 08:38:17.972100 11835 sgd_solver.cpp:112] Iteration 54410, lr = 0.1
I0524 08:38:24.310679 11835 solver.cpp:239] Iteration 54420 (1.57768 iter/s, 6.33841s/10 iters), loss = 5.73242
I0524 08:38:24.310823 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73242 (* 1 = 5.73242 loss)
I0524 08:38:24.584125 11835 sgd_solver.cpp:112] Iteration 54420, lr = 0.1
I0524 08:38:31.564102 11835 solver.cpp:239] Iteration 54430 (1.37873 iter/s, 7.25306s/10 iters), loss = 5.2736
I0524 08:38:31.564172 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.2736 (* 1 = 5.2736 loss)
I0524 08:38:31.564378 11835 sgd_solver.cpp:112] Iteration 54430, lr = 0.1
I0524 08:38:38.763412 11835 solver.cpp:239] Iteration 54440 (1.38909 iter/s, 7.19894s/10 iters), loss = 5.9338
I0524 08:38:38.763490 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9338 (* 1 = 5.9338 loss)
I0524 08:38:38.763535 11835 sgd_solver.cpp:112] Iteration 54440, lr = 0.1
I0524 08:38:45.875747 11835 solver.cpp:239] Iteration 54450 (1.40607 iter/s, 7.112s/10 iters), loss = 6.37949
I0524 08:38:45.875946 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37949 (* 1 = 6.37949 loss)
I0524 08:38:45.875967 11835 sgd_solver.cpp:112] Iteration 54450, lr = 0.1
I0524 08:38:52.913449 11835 solver.cpp:239] Iteration 54460 (1.42146 iter/s, 7.035s/10 iters), loss = 5.62518
I0524 08:38:52.913508 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.62518 (* 1 = 5.62518 loss)
I0524 08:38:53.227138 11835 sgd_solver.cpp:112] Iteration 54460, lr = 0.1
I0524 08:39:00.558303 11835 solver.cpp:239] Iteration 54470 (1.30813 iter/s, 7.64451s/10 iters), loss = 6.62196
I0524 08:39:00.558379 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62196 (* 1 = 6.62196 loss)
I0524 08:39:00.608085 11835 sgd_solver.cpp:112] Iteration 54470, lr = 0.1
I0524 08:39:09.323338 11835 solver.cpp:239] Iteration 54480 (1.14095 iter/s, 8.76464s/10 iters), loss = 6.78933
I0524 08:39:09.323397 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78933 (* 1 = 6.78933 loss)
I0524 08:39:09.323567 11835 sgd_solver.cpp:112] Iteration 54480, lr = 0.1
I0524 08:39:16.738931 11835 solver.cpp:239] Iteration 54490 (1.34857 iter/s, 7.41524s/10 iters), loss = 5.85973
I0524 08:39:16.739091 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85973 (* 1 = 5.85973 loss)
I0524 08:39:16.739334 11835 sgd_solver.cpp:112] Iteration 54490, lr = 0.1
I0524 08:39:24.957890 11835 solver.cpp:239] Iteration 54500 (1.21677 iter/s, 8.2185s/10 iters), loss = 6.71524
I0524 08:39:24.957937 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71524 (* 1 = 6.71524 loss)
I0524 08:39:25.167085 11835 sgd_solver.cpp:112] Iteration 54500, lr = 0.1
I0524 08:39:31.601523 11835 solver.cpp:239] Iteration 54510 (1.50527 iter/s, 6.64333s/10 iters), loss = 5.83043
I0524 08:39:31.601572 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83043 (* 1 = 5.83043 loss)
I0524 08:39:31.601722 11835 sgd_solver.cpp:112] Iteration 54510, lr = 0.1
I0524 08:39:38.343575 11835 solver.cpp:239] Iteration 54520 (1.4833 iter/s, 6.74172s/10 iters), loss = 6.42218
I0524 08:39:38.343633 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42218 (* 1 = 6.42218 loss)
I0524 08:39:38.343652 11835 sgd_solver.cpp:112] Iteration 54520, lr = 0.1
I0524 08:39:46.508900 11835 solver.cpp:239] Iteration 54530 (1.22474 iter/s, 8.16497s/10 iters), loss = 5.94775
I0524 08:39:46.508952 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94775 (* 1 = 5.94775 loss)
I0524 08:39:46.508981 11835 sgd_solver.cpp:112] Iteration 54530, lr = 0.1
I0524 08:39:54.151345 11835 solver.cpp:239] Iteration 54540 (1.30854 iter/s, 7.64208s/10 iters), loss = 6.17756
I0524 08:39:54.151641 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17756 (* 1 = 6.17756 loss)
I0524 08:39:54.526234 11835 sgd_solver.cpp:112] Iteration 54540, lr = 0.1
I0524 08:40:02.100147 11835 solver.cpp:239] Iteration 54550 (1.25814 iter/s, 7.94824s/10 iters), loss = 6.11509
I0524 08:40:02.100198 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11509 (* 1 = 6.11509 loss)
I0524 08:40:02.816450 11835 sgd_solver.cpp:112] Iteration 54550, lr = 0.1
I0524 08:40:10.207576 11835 solver.cpp:239] Iteration 54560 (1.23349 iter/s, 8.10707s/10 iters), loss = 6.42785
I0524 08:40:10.207626 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42785 (* 1 = 6.42785 loss)
I0524 08:40:10.207860 11835 sgd_solver.cpp:112] Iteration 54560, lr = 0.1
I0524 08:40:16.319911 11835 solver.cpp:239] Iteration 54570 (1.63612 iter/s, 6.11204s/10 iters), loss = 6.57815
I0524 08:40:16.319970 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57815 (* 1 = 6.57815 loss)
I0524 08:40:16.321918 11835 sgd_solver.cpp:112] Iteration 54570, lr = 0.1
I0524 08:40:25.496301 11835 solver.cpp:239] Iteration 54580 (1.0898 iter/s, 9.17598s/10 iters), loss = 6.73664
I0524 08:40:25.496518 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73664 (* 1 = 6.73664 loss)
I0524 08:40:26.091974 11835 sgd_solver.cpp:112] Iteration 54580, lr = 0.1
I0524 08:40:34.002683 11835 solver.cpp:239] Iteration 54590 (1.17566 iter/s, 8.50585s/10 iters), loss = 5.58134
I0524 08:40:34.002754 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.58134 (* 1 = 5.58134 loss)
I0524 08:40:34.002820 11835 sgd_solver.cpp:112] Iteration 54590, lr = 0.1
I0524 08:40:41.797111 11835 solver.cpp:239] Iteration 54600 (1.28303 iter/s, 7.79405s/10 iters), loss = 6.30664
I0524 08:40:41.797205 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30664 (* 1 = 6.30664 loss)
I0524 08:40:41.797390 11835 sgd_solver.cpp:112] Iteration 54600, lr = 0.1
I0524 08:40:48.816354 11835 solver.cpp:239] Iteration 54610 (1.42472 iter/s, 7.0189s/10 iters), loss = 6.24313
I0524 08:40:48.816406 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24313 (* 1 = 6.24313 loss)
I0524 08:40:48.816493 11835 sgd_solver.cpp:112] Iteration 54610, lr = 0.1
I0524 08:40:55.226265 11835 solver.cpp:239] Iteration 54620 (1.56016 iter/s, 6.40961s/10 iters), loss = 4.90514
I0524 08:40:55.226307 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.90514 (* 1 = 4.90514 loss)
I0524 08:40:55.236941 11835 sgd_solver.cpp:112] Iteration 54620, lr = 0.1
I0524 08:41:03.444633 11835 solver.cpp:239] Iteration 54630 (1.21684 iter/s, 8.21801s/10 iters), loss = 5.80608
I0524 08:41:03.444803 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80608 (* 1 = 5.80608 loss)
I0524 08:41:03.444833 11835 sgd_solver.cpp:112] Iteration 54630, lr = 0.1
I0524 08:41:10.411381 11835 solver.cpp:239] Iteration 54640 (1.43548 iter/s, 6.96632s/10 iters), loss = 5.4918
I0524 08:41:10.411419 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.4918 (* 1 = 5.4918 loss)
I0524 08:41:10.411566 11835 sgd_solver.cpp:112] Iteration 54640, lr = 0.1
I0524 08:41:16.280789 11835 solver.cpp:239] Iteration 54650 (1.70384 iter/s, 5.8691s/10 iters), loss = 6.30741
I0524 08:41:16.280879 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30741 (* 1 = 6.30741 loss)
I0524 08:41:16.281014 11835 sgd_solver.cpp:112] Iteration 54650, lr = 0.1
I0524 08:41:24.741040 11835 solver.cpp:239] Iteration 54660 (1.18205 iter/s, 8.45985s/10 iters), loss = 6.01875
I0524 08:41:24.741096 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01875 (* 1 = 6.01875 loss)
I0524 08:41:24.741114 11835 sgd_solver.cpp:112] Iteration 54660, lr = 0.1
I0524 08:41:33.497542 11835 solver.cpp:239] Iteration 54670 (1.14234 iter/s, 8.75394s/10 iters), loss = 5.8796
I0524 08:41:33.497787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8796 (* 1 = 5.8796 loss)
I0524 08:41:33.497820 11835 sgd_solver.cpp:112] Iteration 54670, lr = 0.1
I0524 08:41:40.594805 11835 solver.cpp:239] Iteration 54680 (1.40913 iter/s, 7.09657s/10 iters), loss = 7.03385
I0524 08:41:40.594873 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03385 (* 1 = 7.03385 loss)
I0524 08:41:40.594918 11835 sgd_solver.cpp:112] Iteration 54680, lr = 0.1
I0524 08:41:49.844861 11835 solver.cpp:239] Iteration 54690 (1.08112 iter/s, 9.24965s/10 iters), loss = 6.27492
I0524 08:41:49.844923 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27492 (* 1 = 6.27492 loss)
I0524 08:41:50.853003 11835 sgd_solver.cpp:112] Iteration 54690, lr = 0.1
I0524 08:41:59.875306 11835 solver.cpp:239] Iteration 54700 (0.997009 iter/s, 10.03s/10 iters), loss = 6.64135
I0524 08:41:59.875362 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64135 (* 1 = 6.64135 loss)
I0524 08:41:59.939685 11835 sgd_solver.cpp:112] Iteration 54700, lr = 0.1
I0524 08:42:07.393918 11835 solver.cpp:239] Iteration 54710 (1.3301 iter/s, 7.51825s/10 iters), loss = 6.21915
I0524 08:42:07.394127 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21915 (* 1 = 6.21915 loss)
I0524 08:42:07.900542 11835 sgd_solver.cpp:112] Iteration 54710, lr = 0.1
I0524 08:42:17.130026 11835 solver.cpp:239] Iteration 54720 (1.02716 iter/s, 9.73554s/10 iters), loss = 6.40539
I0524 08:42:17.130089 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40539 (* 1 = 6.40539 loss)
I0524 08:42:17.188354 11835 sgd_solver.cpp:112] Iteration 54720, lr = 0.1
I0524 08:42:25.762754 11835 solver.cpp:239] Iteration 54730 (1.15844 iter/s, 8.63233s/10 iters), loss = 6.39445
I0524 08:42:25.762801 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39445 (* 1 = 6.39445 loss)
I0524 08:42:26.599283 11835 sgd_solver.cpp:112] Iteration 54730, lr = 0.1
I0524 08:42:35.428025 11835 solver.cpp:239] Iteration 54740 (1.03468 iter/s, 9.66484s/10 iters), loss = 6.27474
I0524 08:42:35.428076 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27474 (* 1 = 6.27474 loss)
I0524 08:42:35.428092 11835 sgd_solver.cpp:112] Iteration 54740, lr = 0.1
I0524 08:42:42.969048 11835 solver.cpp:239] Iteration 54750 (1.32614 iter/s, 7.54067s/10 iters), loss = 5.67806
I0524 08:42:42.969276 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67806 (* 1 = 5.67806 loss)
I0524 08:42:42.969329 11835 sgd_solver.cpp:112] Iteration 54750, lr = 0.1
I0524 08:42:49.281002 11835 solver.cpp:239] Iteration 54760 (1.58441 iter/s, 6.31148s/10 iters), loss = 6.55471
I0524 08:42:49.281057 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55471 (* 1 = 6.55471 loss)
I0524 08:42:49.281253 11835 sgd_solver.cpp:112] Iteration 54760, lr = 0.1
I0524 08:42:56.776370 11835 solver.cpp:239] Iteration 54770 (1.33422 iter/s, 7.49501s/10 iters), loss = 5.41121
I0524 08:42:56.776424 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.41121 (* 1 = 5.41121 loss)
I0524 08:42:56.776716 11835 sgd_solver.cpp:112] Iteration 54770, lr = 0.1
I0524 08:43:04.090939 11835 solver.cpp:239] Iteration 54780 (1.3672 iter/s, 7.31421s/10 iters), loss = 6.33469
I0524 08:43:04.091027 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33469 (* 1 = 6.33469 loss)
I0524 08:43:04.091212 11835 sgd_solver.cpp:112] Iteration 54780, lr = 0.1
I0524 08:43:10.571346 11835 solver.cpp:239] Iteration 54790 (1.54319 iter/s, 6.48008s/10 iters), loss = 6.71886
I0524 08:43:10.571415 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71886 (* 1 = 6.71886 loss)
I0524 08:43:10.592092 11835 sgd_solver.cpp:112] Iteration 54790, lr = 0.1
I0524 08:43:16.718472 11835 solver.cpp:239] Iteration 54800 (1.62686 iter/s, 6.14683s/10 iters), loss = 6.24524
I0524 08:43:16.718725 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24524 (* 1 = 6.24524 loss)
I0524 08:43:16.718770 11835 sgd_solver.cpp:112] Iteration 54800, lr = 0.1
I0524 08:43:25.101945 11835 solver.cpp:239] Iteration 54810 (1.1929 iter/s, 8.38294s/10 iters), loss = 5.60158
I0524 08:43:25.102005 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.60158 (* 1 = 5.60158 loss)
I0524 08:43:25.102020 11835 sgd_solver.cpp:112] Iteration 54810, lr = 0.1
I0524 08:43:31.166221 11835 solver.cpp:239] Iteration 54820 (1.64908 iter/s, 6.06399s/10 iters), loss = 5.43884
I0524 08:43:31.166276 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43884 (* 1 = 5.43884 loss)
I0524 08:43:31.166558 11835 sgd_solver.cpp:112] Iteration 54820, lr = 0.1
I0524 08:43:38.271030 11835 solver.cpp:239] Iteration 54830 (1.40756 iter/s, 7.10448s/10 iters), loss = 5.74791
I0524 08:43:38.271088 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74791 (* 1 = 5.74791 loss)
I0524 08:43:38.271174 11835 sgd_solver.cpp:112] Iteration 54830, lr = 0.1
I0524 08:43:46.268846 11835 solver.cpp:239] Iteration 54840 (1.2504 iter/s, 7.99745s/10 iters), loss = 6.54019
I0524 08:43:46.268900 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54019 (* 1 = 6.54019 loss)
I0524 08:43:46.268916 11835 sgd_solver.cpp:112] Iteration 54840, lr = 0.1
I0524 08:43:52.886211 11835 solver.cpp:239] Iteration 54850 (1.51174 iter/s, 6.61489s/10 iters), loss = 6.09536
I0524 08:43:52.886514 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09536 (* 1 = 6.09536 loss)
I0524 08:43:54.150405 11835 sgd_solver.cpp:112] Iteration 54850, lr = 0.1
I0524 08:44:03.825619 11835 solver.cpp:239] Iteration 54860 (0.914182 iter/s, 10.9387s/10 iters), loss = 5.40684
I0524 08:44:03.825678 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.40684 (* 1 = 5.40684 loss)
I0524 08:44:03.831148 11835 sgd_solver.cpp:112] Iteration 54860, lr = 0.1
I0524 08:44:11.650226 11835 solver.cpp:239] Iteration 54870 (1.27808 iter/s, 7.82424s/10 iters), loss = 5.54557
I0524 08:44:11.650312 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.54557 (* 1 = 5.54557 loss)
I0524 08:44:12.135007 11835 sgd_solver.cpp:112] Iteration 54870, lr = 0.1
I0524 08:44:19.155578 11835 solver.cpp:239] Iteration 54880 (1.33244 iter/s, 7.50501s/10 iters), loss = 6.30046
I0524 08:44:19.155624 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30046 (* 1 = 6.30046 loss)
I0524 08:44:19.183581 11835 sgd_solver.cpp:112] Iteration 54880, lr = 0.1
I0524 08:44:25.495173 11835 solver.cpp:239] Iteration 54890 (1.57746 iter/s, 6.3393s/10 iters), loss = 5.8872
I0524 08:44:25.495437 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8872 (* 1 = 5.8872 loss)
I0524 08:44:25.598460 11835 sgd_solver.cpp:112] Iteration 54890, lr = 0.1
I0524 08:44:32.798292 11835 solver.cpp:239] Iteration 54900 (1.36937 iter/s, 7.30262s/10 iters), loss = 6.28363
I0524 08:44:32.798362 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.28363 (* 1 = 6.28363 loss)
I0524 08:44:32.952064 11835 sgd_solver.cpp:112] Iteration 54900, lr = 0.1
I0524 08:44:38.977443 11835 solver.cpp:239] Iteration 54910 (1.61843 iter/s, 6.17884s/10 iters), loss = 6.41281
I0524 08:44:38.977510 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41281 (* 1 = 6.41281 loss)
I0524 08:44:38.977581 11835 sgd_solver.cpp:112] Iteration 54910, lr = 0.1
I0524 08:44:46.869535 11835 solver.cpp:239] Iteration 54920 (1.26715 iter/s, 7.89173s/10 iters), loss = 5.63889
I0524 08:44:46.869586 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63889 (* 1 = 5.63889 loss)
I0524 08:44:46.904196 11835 sgd_solver.cpp:112] Iteration 54920, lr = 0.1
I0524 08:44:53.008857 11835 solver.cpp:239] Iteration 54930 (1.62892 iter/s, 6.13902s/10 iters), loss = 6.05711
I0524 08:44:53.008913 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05711 (* 1 = 6.05711 loss)
I0524 08:44:53.008977 11835 sgd_solver.cpp:112] Iteration 54930, lr = 0.1
I0524 08:45:00.784142 11835 solver.cpp:239] Iteration 54940 (1.28618 iter/s, 7.77493s/10 iters), loss = 6.07415
I0524 08:45:00.784301 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07415 (* 1 = 6.07415 loss)
I0524 08:45:00.784338 11835 sgd_solver.cpp:112] Iteration 54940, lr = 0.1
I0524 08:45:07.695577 11835 solver.cpp:239] Iteration 54950 (1.44696 iter/s, 6.91102s/10 iters), loss = 6.49094
I0524 08:45:07.695626 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49094 (* 1 = 6.49094 loss)
I0524 08:45:07.695943 11835 sgd_solver.cpp:112] Iteration 54950, lr = 0.1
I0524 08:45:13.694131 11835 solver.cpp:239] Iteration 54960 (1.66715 iter/s, 5.99827s/10 iters), loss = 6.22395
I0524 08:45:13.694186 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22395 (* 1 = 6.22395 loss)
I0524 08:45:13.694201 11835 sgd_solver.cpp:112] Iteration 54960, lr = 0.1
I0524 08:45:22.883072 11835 solver.cpp:239] Iteration 54970 (1.08832 iter/s, 9.18846s/10 iters), loss = 5.38129
I0524 08:45:22.883141 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.38129 (* 1 = 5.38129 loss)
I0524 08:45:22.917821 11835 sgd_solver.cpp:112] Iteration 54970, lr = 0.1
I0524 08:45:28.803442 11835 solver.cpp:239] Iteration 54980 (1.68917 iter/s, 5.92007s/10 iters), loss = 7.00916
I0524 08:45:28.803495 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00916 (* 1 = 7.00916 loss)
I0524 08:45:28.803524 11835 sgd_solver.cpp:112] Iteration 54980, lr = 0.1
I0524 08:45:37.701972 11835 solver.cpp:239] Iteration 54990 (1.12384 iter/s, 8.89807s/10 iters), loss = 5.02503
I0524 08:45:37.702261 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.02503 (* 1 = 5.02503 loss)
I0524 08:45:37.702303 11835 sgd_solver.cpp:112] Iteration 54990, lr = 0.1
I0524 08:45:43.016687 11835 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_55000.caffemodel
I0524 08:45:43.213623 11835 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_55000.solverstate
I0524 08:45:43.987978 11835 solver.cpp:239] Iteration 55000 (1.59096 iter/s, 6.2855s/10 iters), loss = 6.68534
I0524 08:45:43.988039 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.68534 (* 1 = 6.68534 loss)
I0524 08:45:44.261204 11835 sgd_solver.cpp:112] Iteration 55000, lr = 0.1
I0524 08:45:50.311194 11835 solver.cpp:239] Iteration 55010 (1.58155 iter/s, 6.32291s/10 iters), loss = 5.49547
I0524 08:45:50.311244 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.49547 (* 1 = 5.49547 loss)
I0524 08:45:50.311262 11835 sgd_solver.cpp:112] Iteration 55010, lr = 0.1
I0524 08:45:56.300774 11835 solver.cpp:239] Iteration 55020 (1.66975 iter/s, 5.98891s/10 iters), loss = 5.3042
I0524 08:45:56.300834 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.3042 (* 1 = 5.3042 loss)
I0524 08:45:56.300911 11835 sgd_solver.cpp:112] Iteration 55020, lr = 0.1
I0524 08:46:02.455248 11835 solver.cpp:239] Iteration 55030 (1.62491 iter/s, 6.15418s/10 iters), loss = 6.84258
I0524 08:46:02.455289 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84258 (* 1 = 6.84258 loss)
I0524 08:46:02.469231 11835 sgd_solver.cpp:112] Iteration 55030, lr = 0.1
I0524 08:46:09.316375 11835 solver.cpp:239] Iteration 55040 (1.45755 iter/s, 6.86081s/10 iters), loss = 6.88194
I0524 08:46:09.316625 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88194 (* 1 = 6.88194 loss)
I0524 08:46:09.525816 11835 sgd_solver.cpp:112] Iteration 55040, lr = 0.1
I0524 08:46:16.387266 11835 solver.cpp:239] Iteration 55050 (1.41435 iter/s, 7.07037s/10 iters), loss = 6.33081
I0524 08:46:16.387349 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33081 (* 1 = 6.33081 loss)
I0524 08:46:16.387434 11835 sgd_solver.cpp:112] Iteration 55050, lr = 0.1
I0524 08:46:22.448441 11835 solver.cpp:239] Iteration 55060 (1.64993 iter/s, 6.06086s/10 iters), loss = 5.69328
I0524 08:46:22.448488 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.69328 (* 1 = 5.69328 loss)
I0524 08:46:22.448758 11835 sgd_solver.cpp:112] Iteration 55060, lr = 0.1
I0524 08:46:29.921713 11835 solver.cpp:239] Iteration 55070 (1.33816 iter/s, 7.47292s/10 iters), loss = 5.68774
I0524 08:46:29.921787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68774 (* 1 = 5.68774 loss)
I0524 08:46:29.922016 11835 sgd_solver.cpp:112] Iteration 55070, lr = 0.1
I0524 08:46:37.191308 11835 solver.cpp:239] Iteration 55080 (1.37566 iter/s, 7.26923s/10 iters), loss = 5.58027
I0524 08:46:37.191367 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.58027 (* 1 = 5.58027 loss)
I0524 08:46:37.191423 11835 sgd_solver.cpp:112] Iteration 55080, lr = 0.1
I0524 08:46:44.627108 11835 solver.cpp:239] Iteration 55090 (1.34491 iter/s, 7.43544s/10 iters), loss = 6.24573
I0524 08:46:44.627406 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24573 (* 1 = 6.24573 loss)
I0524 08:46:45.237720 11835 sgd_solver.cpp:112] Iteration 55090, lr = 0.1
I0524 08:46:53.713160 11835 solver.cpp:239] Iteration 55100 (1.10066 iter/s, 9.08543s/10 iters), loss = 5.5391
I0524 08:46:53.713222 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.5391 (* 1 = 5.5391 loss)
I0524 08:46:53.713277 11835 sgd_solver.cpp:112] Iteration 55100, lr = 0.1
I0524 08:46:59.647789 11835 solver.cpp:239] Iteration 55110 (1.68511 iter/s, 5.93434s/10 iters), loss = 5.90927
I0524 08:46:59.647838 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90927 (* 1 = 5.90927 loss)
I0524 08:46:59.647853 11835 sgd_solver.cpp:112] Iteration 55110, lr = 0.1
I0524 08:47:06.668004 11835 solver.cpp:239] Iteration 55120 (1.42475 iter/s, 7.01879s/10 iters), loss = 6.24185
I0524 08:47:06.668046 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24185 (* 1 = 6.24185 loss)
I0524 08:47:06.668062 11835 sgd_solver.cpp:112] Iteration 55120, lr = 0.1
I0524 08:47:13.276489 11835 solver.cpp:239] Iteration 55130 (1.51378 iter/s, 6.60598s/10 iters), loss = 7.82795
I0524 08:47:13.276549 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.82795 (* 1 = 7.82795 loss)
I0524 08:47:13.276679 11835 sgd_solver.cpp:112] Iteration 55130, lr = 0.1
I0524 08:47:19.315645 11835 solver.cpp:239] Iteration 55140 (1.65594 iter/s, 6.03887s/10 iters), loss = 5.8647
I0524 08:47:19.315898 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8647 (* 1 = 5.8647 loss)
I0524 08:47:19.315945 11835 sgd_solver.cpp:112] Iteration 55140, lr = 0.1
I0524 08:47:25.973582 11835 solver.cpp:239] Iteration 55150 (1.50215 iter/s, 6.65713s/10 iters), loss = 6.25829
I0524 08:47:25.973639 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25829 (* 1 = 6.25829 loss)
I0524 08:47:25.973989 11835 sgd_solver.cpp:112] Iteration 55150, lr = 0.1
I0524 08:47:32.084045 11835 solver.cpp:239] Iteration 55160 (1.63662 iter/s, 6.11017s/10 iters), loss = 5.99026
I0524 08:47:32.084105 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99026 (* 1 = 5.99026 loss)
I0524 08:47:32.084275 11835 sgd_solver.cpp:112] Iteration 55160, lr = 0.1
I0524 08:47:38.189072 11835 solver.cpp:239] Iteration 55170 (1.63807 iter/s, 6.10474s/10 iters), loss = 5.49135
I0524 08:47:38.189134 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.49135 (* 1 = 5.49135 loss)
I0524 08:47:38.189302 11835 sgd_solver.cpp:112] Iteration 55170, lr = 0.1
I0524 08:47:46.430061 11835 solver.cpp:239] Iteration 55180 (1.2135 iter/s, 8.24061s/10 iters), loss = 5.15511
I0524 08:47:46.430110 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.15511 (* 1 = 5.15511 loss)
I0524 08:47:46.430194 11835 sgd_solver.cpp:112] Iteration 55180, lr = 0.1
I0524 08:47:52.324213 11835 solver.cpp:239] Iteration 55190 (1.69668 iter/s, 5.89388s/10 iters), loss = 5.43079
I0524 08:47:52.324357 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43079 (* 1 = 5.43079 loss)
I0524 08:47:52.342449 11835 sgd_solver.cpp:112] Iteration 55190, lr = 0.1
I0524 08:48:01.427825 11835 solver.cpp:239] Iteration 55200 (1.09852 iter/s, 9.10312s/10 iters), loss = 5.86323
I0524 08:48:01.427877 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86323 (* 1 = 5.86323 loss)
I0524 08:48:01.783481 11835 sgd_solver.cpp:112] Iteration 55200, lr = 0.1
I0524 08:48:09.839504 11835 solver.cpp:239] Iteration 55210 (1.18888 iter/s, 8.41131s/10 iters), loss = 5.98344
I0524 08:48:09.839553 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98344 (* 1 = 5.98344 loss)
I0524 08:48:09.839787 11835 sgd_solver.cpp:112] Iteration 55210, lr = 0.1
I0524 08:48:16.785289 11835 solver.cpp:239] Iteration 55220 (1.43979 iter/s, 6.94546s/10 iters), loss = 6.86608
I0524 08:48:16.785343 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86608 (* 1 = 6.86608 loss)
I0524 08:48:16.790942 11835 sgd_solver.cpp:112] Iteration 55220, lr = 0.1
I0524 08:48:24.153698 11835 solver.cpp:239] Iteration 55230 (1.35721 iter/s, 7.36806s/10 iters), loss = 5.38644
I0524 08:48:24.154013 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.38644 (* 1 = 5.38644 loss)
I0524 08:48:24.154060 11835 sgd_solver.cpp:112] Iteration 55230, lr = 0.1
I0524 08:48:30.138880 11835 solver.cpp:239] Iteration 55240 (1.67097 iter/s, 5.98453s/10 iters), loss = 6.35086
I0524 08:48:30.138921 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35086 (* 1 = 6.35086 loss)
I0524 08:48:30.602278 11835 sgd_solver.cpp:112] Iteration 55240, lr = 0.1
I0524 08:48:37.936002 11835 solver.cpp:239] Iteration 55250 (1.28258 iter/s, 7.79677s/10 iters), loss = 5.42782
I0524 08:48:37.936061 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.42782 (* 1 = 5.42782 loss)
I0524 08:48:37.936087 11835 sgd_solver.cpp:112] Iteration 55250, lr = 0.1
I0524 08:48:44.087451 11835 solver.cpp:239] Iteration 55260 (1.62571 iter/s, 6.15114s/10 iters), loss = 5.72768
I0524 08:48:44.087515 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72768 (* 1 = 5.72768 loss)
I0524 08:48:44.087589 11835 sgd_solver.cpp:112] Iteration 55260, lr = 0.1
I0524 08:48:50.539690 11835 solver.cpp:239] Iteration 55270 (1.54992 iter/s, 6.45194s/10 iters), loss = 7.3453
I0524 08:48:50.539739 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.3453 (* 1 = 7.3453 loss)
I0524 08:48:50.577981 11835 sgd_solver.cpp:112] Iteration 55270, lr = 0.1
I0524 08:48:58.375206 11835 solver.cpp:239] Iteration 55280 (1.2763 iter/s, 7.83517s/10 iters), loss = 5.95894
I0524 08:48:58.375457 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95894 (* 1 = 5.95894 loss)
I0524 08:48:59.101869 11835 sgd_solver.cpp:112] Iteration 55280, lr = 0.1
I0524 08:49:05.481281 11835 solver.cpp:239] Iteration 55290 (1.40735 iter/s, 7.10557s/10 iters), loss = 6.26078
I0524 08:49:05.481348 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26078 (* 1 = 6.26078 loss)
I0524 08:49:06.580420 11835 sgd_solver.cpp:112] Iteration 55290, lr = 0.1
I0524 08:49:12.699892 11835 solver.cpp:239] Iteration 55300 (1.38537 iter/s, 7.21828s/10 iters), loss = 6.13598
I0524 08:49:12.699939 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13598 (* 1 = 6.13598 loss)
I0524 08:49:12.700212 11835 sgd_solver.cpp:112] Iteration 55300, lr = 0.1
I0524 08:49:20.249964 11835 solver.cpp:239] Iteration 55310 (1.32455 iter/s, 7.54972s/10 iters), loss = 5.92129
I0524 08:49:20.250025 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92129 (* 1 = 5.92129 loss)
I0524 08:49:20.250208 11835 sgd_solver.cpp:112] Iteration 55310, lr = 0.1
I0524 08:49:26.835197 11835 solver.cpp:239] Iteration 55320 (1.51862 iter/s, 6.58492s/10 iters), loss = 5.00813
I0524 08:49:26.835247 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.00813 (* 1 = 5.00813 loss)
I0524 08:49:26.835484 11835 sgd_solver.cpp:112] Iteration 55320, lr = 0.1
I0524 08:49:33.050978 11835 solver.cpp:239] Iteration 55330 (1.60889 iter/s, 6.21548s/10 iters), loss = 6.70637
I0524 08:49:33.051208 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70637 (* 1 = 6.70637 loss)
I0524 08:49:33.051432 11835 sgd_solver.cpp:112] Iteration 55330, lr = 0.1
I0524 08:49:39.567710 11835 solver.cpp:239] Iteration 55340 (1.53462 iter/s, 6.51629s/10 iters), loss = 6.02226
I0524 08:49:39.567766 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02226 (* 1 = 6.02226 loss)
I0524 08:49:39.617368 11835 sgd_solver.cpp:112] Iteration 55340, lr = 0.1
I0524 08:49:45.887071 11835 solver.cpp:239] Iteration 55350 (1.58252 iter/s, 6.31906s/10 iters), loss = 6.01823
I0524 08:49:45.887135 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01823 (* 1 = 6.01823 loss)
I0524 08:49:45.887171 11835 sgd_solver.cpp:112] Iteration 55350, lr = 0.1
I0524 08:49:53.887591 11835 solver.cpp:239] Iteration 55360 (1.24998 iter/s, 8.0001s/10 iters), loss = 5.90876
I0524 08:49:53.887667 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90876 (* 1 = 5.90876 loss)
I0524 08:49:53.887825 11835 sgd_solver.cpp:112] Iteration 55360, lr = 0.1
I0524 08:50:00.392520 11835 solver.cpp:239] Iteration 55370 (1.53737 iter/s, 6.50461s/10 iters), loss = 5.99852
I0524 08:50:00.392573 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99852 (* 1 = 5.99852 loss)
I0524 08:50:00.392776 11835 sgd_solver.cpp:112] Iteration 55370, lr = 0.1
I0524 08:50:08.656924 11835 solver.cpp:239] Iteration 55380 (1.21006 iter/s, 8.26402s/10 iters), loss = 4.80837
I0524 08:50:08.657243 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.80837 (* 1 = 4.80837 loss)
I0524 08:50:08.657284 11835 sgd_solver.cpp:112] Iteration 55380, lr = 0.1
I0524 08:50:15.628048 11835 solver.cpp:239] Iteration 55390 (1.43459 iter/s, 6.97064s/10 iters), loss = 6.51326
I0524 08:50:15.628098 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51326 (* 1 = 6.51326 loss)
I0524 08:50:16.417412 11835 sgd_solver.cpp:112] Iteration 55390, lr = 0.1
I0524 08:50:22.849467 11835 solver.cpp:239] Iteration 55400 (1.38483 iter/s, 7.22109s/10 iters), loss = 6.53206
I0524 08:50:22.849581 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53206 (* 1 = 6.53206 loss)
I0524 08:50:22.849666 11835 sgd_solver.cpp:112] Iteration 55400, lr = 0.1
I0524 08:50:29.711345 11835 solver.cpp:239] Iteration 55410 (1.4574 iter/s, 6.86156s/10 iters), loss = 6.50374
I0524 08:50:29.711396 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50374 (* 1 = 6.50374 loss)
I0524 08:50:29.711544 11835 sgd_solver.cpp:112] Iteration 55410, lr = 0.1
I0524 08:50:35.881944 11835 solver.cpp:239] Iteration 55420 (1.62066 iter/s, 6.17031s/10 iters), loss = 7.10095
I0524 08:50:35.881997 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.10095 (* 1 = 7.10095 loss)
I0524 08:50:35.882014 11835 sgd_solver.cpp:112] Iteration 55420, lr = 0.1
I0524 08:50:43.161219 11835 solver.cpp:239] Iteration 55430 (1.37383 iter/s, 7.27893s/10 iters), loss = 6.54241
I0524 08:50:43.161437 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54241 (* 1 = 6.54241 loss)
I0524 08:50:43.227648 11835 sgd_solver.cpp:112] Iteration 55430, lr = 0.1
I0524 08:50:50.839294 11835 solver.cpp:239] Iteration 55440 (1.3025 iter/s, 7.67757s/10 iters), loss = 6.56826
I0524 08:50:50.839354 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.56826 (* 1 = 6.56826 loss)
I0524 08:50:50.849481 11835 sgd_solver.cpp:112] Iteration 55440, lr = 0.1
I0524 08:50:57.527760 11835 solver.cpp:239] Iteration 55450 (1.49518 iter/s, 6.68816s/10 iters), loss = 5.19091
I0524 08:50:57.527815 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.19091 (* 1 = 5.19091 loss)
I0524 08:50:57.571266 11835 sgd_solver.cpp:112] Iteration 55450, lr = 0.1
I0524 08:51:03.712586 11835 solver.cpp:239] Iteration 55460 (1.61694 iter/s, 6.18454s/10 iters), loss = 5.09448
I0524 08:51:03.712635 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.09448 (* 1 = 5.09448 loss)
I0524 08:51:03.712649 11835 sgd_solver.cpp:112] Iteration 55460, lr = 0.1
I0524 08:51:11.520807 11835 solver.cpp:239] Iteration 55470 (1.28079 iter/s, 7.8077s/10 iters), loss = 6.00816
I0524 08:51:11.520880 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00816 (* 1 = 6.00816 loss)
I0524 08:51:11.521230 11835 sgd_solver.cpp:112] Iteration 55470, lr = 0.1
I0524 08:51:20.709009 11835 solver.cpp:239] Iteration 55480 (1.0884 iter/s, 9.1878s/10 iters), loss = 6.7397
I0524 08:51:20.709239 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7397 (* 1 = 6.7397 loss)
I0524 08:51:20.709444 11835 sgd_solver.cpp:112] Iteration 55480, lr = 0.1
I0524 08:51:26.447414 11835 solver.cpp:239] Iteration 55490 (1.74277 iter/s, 5.73799s/10 iters), loss = 6.4196
I0524 08:51:26.447460 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4196 (* 1 = 6.4196 loss)
I0524 08:51:26.447474 11835 sgd_solver.cpp:112] Iteration 55490, lr = 0.1
I0524 08:51:32.282981 11835 solver.cpp:239] Iteration 55500 (1.71487 iter/s, 5.83136s/10 iters), loss = 7.00372
I0524 08:51:32.283056 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.00372 (* 1 = 7.00372 loss)
I0524 08:51:32.564249 11835 sgd_solver.cpp:112] Iteration 55500, lr = 0.1
I0524 08:51:38.148244 11835 solver.cpp:239] Iteration 55510 (1.70504 iter/s, 5.86497s/10 iters), loss = 5.68603
I0524 08:51:38.148290 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68603 (* 1 = 5.68603 loss)
I0524 08:51:38.148675 11835 sgd_solver.cpp:112] Iteration 55510, lr = 0.1
I0524 08:51:46.352562 11835 solver.cpp:239] Iteration 55520 (1.21893 iter/s, 8.20395s/10 iters), loss = 6.11877
I0524 08:51:46.352618 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11877 (* 1 = 6.11877 loss)
I0524 08:51:46.352648 11835 sgd_solver.cpp:112] Iteration 55520, lr = 0.1
I0524 08:51:52.282974 11835 solver.cpp:239] Iteration 55530 (1.6863 iter/s, 5.93013s/10 iters), loss = 5.52887
I0524 08:51:52.283174 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.52887 (* 1 = 5.52887 loss)
I0524 08:51:52.569273 11835 sgd_solver.cpp:112] Iteration 55530, lr = 0.1
I0524 08:52:00.971845 11835 solver.cpp:239] Iteration 55540 (1.15097 iter/s, 8.68833s/10 iters), loss = 7.38095
I0524 08:52:00.971909 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.38095 (* 1 = 7.38095 loss)
I0524 08:52:00.972098 11835 sgd_solver.cpp:112] Iteration 55540, lr = 0.1
I0524 08:52:07.185685 11835 solver.cpp:239] Iteration 55550 (1.60939 iter/s, 6.21354s/10 iters), loss = 6.62714
I0524 08:52:07.185727 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.62714 (* 1 = 6.62714 loss)
I0524 08:52:07.359417 11835 sgd_solver.cpp:112] Iteration 55550, lr = 0.1
I0524 08:52:13.816581 11835 solver.cpp:239] Iteration 55560 (1.50816 iter/s, 6.63058s/10 iters), loss = 5.26564
I0524 08:52:13.816640 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.26564 (* 1 = 5.26564 loss)
I0524 08:52:13.839512 11835 sgd_solver.cpp:112] Iteration 55560, lr = 0.1
I0524 08:52:21.179920 11835 solver.cpp:239] Iteration 55570 (1.35814 iter/s, 7.363s/10 iters), loss = 6.04327
I0524 08:52:21.179971 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04327 (* 1 = 6.04327 loss)
I0524 08:52:21.389443 11835 sgd_solver.cpp:112] Iteration 55570, lr = 0.1
I0524 08:52:28.829819 11835 solver.cpp:239] Iteration 55580 (1.30727 iter/s, 7.64956s/10 iters), loss = 6.6854
I0524 08:52:28.830065 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6854 (* 1 = 6.6854 loss)
I0524 08:52:28.830113 11835 sgd_solver.cpp:112] Iteration 55580, lr = 0.1
I0524 08:52:34.794234 11835 solver.cpp:239] Iteration 55590 (1.67673 iter/s, 5.96399s/10 iters), loss = 6.29443
I0524 08:52:34.794273 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29443 (* 1 = 6.29443 loss)
I0524 08:52:35.593652 11835 sgd_solver.cpp:112] Iteration 55590, lr = 0.1
I0524 08:52:41.919718 11835 solver.cpp:239] Iteration 55600 (1.40348 iter/s, 7.12515s/10 iters), loss = 5.79761
I0524 08:52:41.919790 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79761 (* 1 = 5.79761 loss)
I0524 08:52:41.919927 11835 sgd_solver.cpp:112] Iteration 55600, lr = 0.1
I0524 08:52:48.158840 11835 solver.cpp:239] Iteration 55610 (1.60287 iter/s, 6.23881s/10 iters), loss = 5.50747
I0524 08:52:48.158934 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.50747 (* 1 = 5.50747 loss)
I0524 08:52:48.158967 11835 sgd_solver.cpp:112] Iteration 55610, lr = 0.1
I0524 08:52:54.837697 11835 solver.cpp:239] Iteration 55620 (1.49735 iter/s, 6.67846s/10 iters), loss = 6.55907
I0524 08:52:54.837760 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55907 (* 1 = 6.55907 loss)
I0524 08:52:54.837854 11835 sgd_solver.cpp:112] Iteration 55620, lr = 0.1
I0524 08:53:02.769752 11835 solver.cpp:239] Iteration 55630 (1.26076 iter/s, 7.93169s/10 iters), loss = 6.6549
I0524 08:53:02.769989 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6549 (* 1 = 6.6549 loss)
I0524 08:53:03.411492 11835 sgd_solver.cpp:112] Iteration 55630, lr = 0.1
I0524 08:53:11.039142 11835 solver.cpp:239] Iteration 55640 (1.20936 iter/s, 8.26885s/10 iters), loss = 6.14796
I0524 08:53:11.039216 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14796 (* 1 = 6.14796 loss)
I0524 08:53:11.039432 11835 sgd_solver.cpp:112] Iteration 55640, lr = 0.1
I0524 08:53:17.105731 11835 solver.cpp:239] Iteration 55650 (1.64845 iter/s, 6.0663s/10 iters), loss = 8.13827
I0524 08:53:17.105772 11835 solver.cpp:258]     Train net output #0: softmax_loss = 8.13827 (* 1 = 8.13827 loss)
I0524 08:53:17.151072 11835 sgd_solver.cpp:112] Iteration 55650, lr = 0.1
I0524 08:53:23.241412 11835 solver.cpp:239] Iteration 55660 (1.62989 iter/s, 6.13539s/10 iters), loss = 5.71925
I0524 08:53:23.241473 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71925 (* 1 = 5.71925 loss)
I0524 08:53:23.241658 11835 sgd_solver.cpp:112] Iteration 55660, lr = 0.1
I0524 08:53:29.991981 11835 solver.cpp:239] Iteration 55670 (1.48143 iter/s, 6.75021s/10 iters), loss = 5.61588
I0524 08:53:29.992041 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.61588 (* 1 = 5.61588 loss)
I0524 08:53:29.992378 11835 sgd_solver.cpp:112] Iteration 55670, lr = 0.1
I0524 08:53:37.171954 11835 solver.cpp:239] Iteration 55680 (1.39283 iter/s, 7.17964s/10 iters), loss = 6.65813
I0524 08:53:37.172157 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65813 (* 1 = 6.65813 loss)
I0524 08:53:37.210964 11835 sgd_solver.cpp:112] Iteration 55680, lr = 0.1
I0524 08:53:45.497705 11835 solver.cpp:239] Iteration 55690 (1.20117 iter/s, 8.3252s/10 iters), loss = 5.20841
I0524 08:53:45.497786 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.20841 (* 1 = 5.20841 loss)
I0524 08:53:45.497864 11835 sgd_solver.cpp:112] Iteration 55690, lr = 0.1
I0524 08:53:52.519824 11835 solver.cpp:239] Iteration 55700 (1.42414 iter/s, 7.02177s/10 iters), loss = 5.01831
I0524 08:53:52.519881 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.01831 (* 1 = 5.01831 loss)
I0524 08:53:52.520148 11835 sgd_solver.cpp:112] Iteration 55700, lr = 0.1
I0524 08:53:59.145095 11835 solver.cpp:239] Iteration 55710 (1.50945 iter/s, 6.62494s/10 iters), loss = 6.51305
I0524 08:53:59.145160 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51305 (* 1 = 6.51305 loss)
I0524 08:53:59.145426 11835 sgd_solver.cpp:112] Iteration 55710, lr = 0.1
I0524 08:54:05.397567 11835 solver.cpp:239] Iteration 55720 (1.59944 iter/s, 6.25218s/10 iters), loss = 4.29238
I0524 08:54:05.397606 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.29238 (* 1 = 4.29238 loss)
I0524 08:54:05.397728 11835 sgd_solver.cpp:112] Iteration 55720, lr = 0.1
I0524 08:54:11.416713 11835 solver.cpp:239] Iteration 55730 (1.66144 iter/s, 6.01887s/10 iters), loss = 5.45602
I0524 08:54:11.416869 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.45602 (* 1 = 5.45602 loss)
I0524 08:54:11.416889 11835 sgd_solver.cpp:112] Iteration 55730, lr = 0.1
I0524 08:54:17.653671 11835 solver.cpp:239] Iteration 55740 (1.60399 iter/s, 6.23446s/10 iters), loss = 6.60612
I0524 08:54:17.653746 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60612 (* 1 = 6.60612 loss)
I0524 08:54:17.654083 11835 sgd_solver.cpp:112] Iteration 55740, lr = 0.1
I0524 08:54:25.442780 11835 solver.cpp:239] Iteration 55750 (1.2839 iter/s, 7.78876s/10 iters), loss = 5.28357
I0524 08:54:25.442847 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.28357 (* 1 = 5.28357 loss)
I0524 08:54:25.442986 11835 sgd_solver.cpp:112] Iteration 55750, lr = 0.1
I0524 08:54:36.208873 11835 solver.cpp:239] Iteration 55760 (0.928883 iter/s, 10.7656s/10 iters), loss = 6.07172
I0524 08:54:36.208930 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07172 (* 1 = 6.07172 loss)
I0524 08:54:36.209252 11835 sgd_solver.cpp:112] Iteration 55760, lr = 0.1
I0524 08:54:44.793195 11835 solver.cpp:239] Iteration 55770 (1.16497 iter/s, 8.58394s/10 iters), loss = 6.11311
I0524 08:54:44.793457 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11311 (* 1 = 6.11311 loss)
I0524 08:54:44.793514 11835 sgd_solver.cpp:112] Iteration 55770, lr = 0.1
I0524 08:54:51.401427 11835 solver.cpp:239] Iteration 55780 (1.51342 iter/s, 6.60754s/10 iters), loss = 5.58775
I0524 08:54:51.401468 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.58775 (* 1 = 5.58775 loss)
I0524 08:54:51.411954 11835 sgd_solver.cpp:112] Iteration 55780, lr = 0.1
I0524 08:55:00.188416 11835 solver.cpp:239] Iteration 55790 (1.1381 iter/s, 8.78661s/10 iters), loss = 7.16251
I0524 08:55:00.188467 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.16251 (* 1 = 7.16251 loss)
I0524 08:55:00.226758 11835 sgd_solver.cpp:112] Iteration 55790, lr = 0.1
I0524 08:55:06.625150 11835 solver.cpp:239] Iteration 55800 (1.55366 iter/s, 6.43643s/10 iters), loss = 6.04776
I0524 08:55:06.625211 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04776 (* 1 = 6.04776 loss)
I0524 08:55:06.934873 11835 sgd_solver.cpp:112] Iteration 55800, lr = 0.1
I0524 08:55:12.813984 11835 solver.cpp:239] Iteration 55810 (1.61589 iter/s, 6.18855s/10 iters), loss = 6.32982
I0524 08:55:12.814025 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32982 (* 1 = 6.32982 loss)
I0524 08:55:12.814038 11835 sgd_solver.cpp:112] Iteration 55810, lr = 0.1
I0524 08:55:19.824062 11835 solver.cpp:239] Iteration 55820 (1.42659 iter/s, 7.00973s/10 iters), loss = 5.2664
I0524 08:55:19.824388 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.2664 (* 1 = 5.2664 loss)
I0524 08:55:19.824417 11835 sgd_solver.cpp:112] Iteration 55820, lr = 0.1
I0524 08:55:27.935982 11835 solver.cpp:239] Iteration 55830 (1.23286 iter/s, 8.11125s/10 iters), loss = 5.94154
I0524 08:55:27.936043 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94154 (* 1 = 5.94154 loss)
I0524 08:55:27.941613 11835 sgd_solver.cpp:112] Iteration 55830, lr = 0.1
I0524 08:55:34.370105 11835 solver.cpp:239] Iteration 55840 (1.55429 iter/s, 6.43381s/10 iters), loss = 5.23773
I0524 08:55:34.370172 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.23773 (* 1 = 5.23773 loss)
I0524 08:55:34.370326 11835 sgd_solver.cpp:112] Iteration 55840, lr = 0.1
I0524 08:55:41.811659 11835 solver.cpp:239] Iteration 55850 (1.34387 iter/s, 7.4412s/10 iters), loss = 5.93884
I0524 08:55:41.811717 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93884 (* 1 = 5.93884 loss)
I0524 08:55:42.144847 11835 sgd_solver.cpp:112] Iteration 55850, lr = 0.1
I0524 08:55:48.729492 11835 solver.cpp:239] Iteration 55860 (1.44561 iter/s, 6.91752s/10 iters), loss = 6.61625
I0524 08:55:48.729532 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61625 (* 1 = 6.61625 loss)
I0524 08:55:48.729564 11835 sgd_solver.cpp:112] Iteration 55860, lr = 0.1
I0524 08:55:56.891530 11835 solver.cpp:239] Iteration 55870 (1.22524 iter/s, 8.16167s/10 iters), loss = 6.08458
I0524 08:55:56.891705 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08458 (* 1 = 6.08458 loss)
I0524 08:55:57.745091 11835 sgd_solver.cpp:112] Iteration 55870, lr = 0.1
I0524 08:56:04.596854 11835 solver.cpp:239] Iteration 55880 (1.29788 iter/s, 7.70485s/10 iters), loss = 6.34194
I0524 08:56:04.596909 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34194 (* 1 = 6.34194 loss)
I0524 08:56:04.597085 11835 sgd_solver.cpp:112] Iteration 55880, lr = 0.1
I0524 08:56:11.838048 11835 solver.cpp:239] Iteration 55890 (1.38105 iter/s, 7.24087s/10 iters), loss = 6.7847
I0524 08:56:11.838099 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7847 (* 1 = 6.7847 loss)
I0524 08:56:13.106647 11835 sgd_solver.cpp:112] Iteration 55890, lr = 0.1
I0524 08:56:20.580658 11835 solver.cpp:239] Iteration 55900 (1.14388 iter/s, 8.74218s/10 iters), loss = 5.87421
I0524 08:56:20.580715 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87421 (* 1 = 5.87421 loss)
I0524 08:56:20.580868 11835 sgd_solver.cpp:112] Iteration 55900, lr = 0.1
I0524 08:56:27.610038 11835 solver.cpp:239] Iteration 55910 (1.42267 iter/s, 7.02905s/10 iters), loss = 6.07221
I0524 08:56:27.610327 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.07221 (* 1 = 6.07221 loss)
I0524 08:56:27.610383 11835 sgd_solver.cpp:112] Iteration 55910, lr = 0.1
I0524 08:56:33.911307 11835 solver.cpp:239] Iteration 55920 (1.58717 iter/s, 6.30052s/10 iters), loss = 6.08401
I0524 08:56:33.911368 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08401 (* 1 = 6.08401 loss)
I0524 08:56:33.911476 11835 sgd_solver.cpp:112] Iteration 55920, lr = 0.1
I0524 08:56:40.924309 11835 solver.cpp:239] Iteration 55930 (1.42599 iter/s, 7.01268s/10 iters), loss = 6.54713
I0524 08:56:40.924353 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54713 (* 1 = 6.54713 loss)
I0524 08:56:40.924468 11835 sgd_solver.cpp:112] Iteration 55930, lr = 0.1
I0524 08:56:46.864989 11835 solver.cpp:239] Iteration 55940 (1.68339 iter/s, 5.94038s/10 iters), loss = 5.75477
I0524 08:56:46.865046 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75477 (* 1 = 5.75477 loss)
I0524 08:56:46.881695 11835 sgd_solver.cpp:112] Iteration 55940, lr = 0.1
I0524 08:56:53.798321 11835 solver.cpp:239] Iteration 55950 (1.44238 iter/s, 6.933s/10 iters), loss = 6.17527
I0524 08:56:53.798375 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17527 (* 1 = 6.17527 loss)
I0524 08:56:53.798390 11835 sgd_solver.cpp:112] Iteration 55950, lr = 0.1
I0524 08:57:03.640094 11835 solver.cpp:239] Iteration 55960 (1.01612 iter/s, 9.84135s/10 iters), loss = 6.18853
I0524 08:57:03.640348 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18853 (* 1 = 6.18853 loss)
I0524 08:57:03.640465 11835 sgd_solver.cpp:112] Iteration 55960, lr = 0.1
I0524 08:57:11.259137 11835 solver.cpp:239] Iteration 55970 (1.31259 iter/s, 7.61853s/10 iters), loss = 5.8197
I0524 08:57:11.259181 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8197 (* 1 = 5.8197 loss)
I0524 08:57:11.259297 11835 sgd_solver.cpp:112] Iteration 55970, lr = 0.1
I0524 08:57:17.231966 11835 solver.cpp:239] Iteration 55980 (1.67433 iter/s, 5.97254s/10 iters), loss = 5.94815
I0524 08:57:17.232019 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94815 (* 1 = 5.94815 loss)
I0524 08:57:17.237458 11835 sgd_solver.cpp:112] Iteration 55980, lr = 0.1
I0524 08:57:24.486521 11835 solver.cpp:239] Iteration 55990 (1.37851 iter/s, 7.25422s/10 iters), loss = 5.86256
I0524 08:57:24.486593 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86256 (* 1 = 5.86256 loss)
I0524 08:57:25.657632 11835 sgd_solver.cpp:112] Iteration 55990, lr = 0.1
I0524 08:57:32.521222 11835 solver.cpp:239] Iteration 56000 (1.24466 iter/s, 8.03433s/10 iters), loss = 5.96353
I0524 08:57:32.521292 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96353 (* 1 = 5.96353 loss)
I0524 08:57:33.171846 11835 sgd_solver.cpp:112] Iteration 56000, lr = 0.1
I0524 08:57:40.412092 11835 solver.cpp:239] Iteration 56010 (1.26735 iter/s, 7.89046s/10 iters), loss = 6.86875
I0524 08:57:40.412303 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86875 (* 1 = 6.86875 loss)
I0524 08:57:40.412328 11835 sgd_solver.cpp:112] Iteration 56010, lr = 0.1
I0524 08:57:47.248916 11835 solver.cpp:239] Iteration 56020 (1.46278 iter/s, 6.83631s/10 iters), loss = 5.82597
I0524 08:57:47.248977 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82597 (* 1 = 5.82597 loss)
I0524 08:57:47.249006 11835 sgd_solver.cpp:112] Iteration 56020, lr = 0.1
I0524 08:57:54.832335 11835 solver.cpp:239] Iteration 56030 (1.31873 iter/s, 7.58307s/10 iters), loss = 5.96115
I0524 08:57:54.832398 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96115 (* 1 = 5.96115 loss)
I0524 08:57:54.834234 11835 sgd_solver.cpp:112] Iteration 56030, lr = 0.1
I0524 08:58:02.157572 11835 solver.cpp:239] Iteration 56040 (1.36522 iter/s, 7.32485s/10 iters), loss = 7.08446
I0524 08:58:02.157660 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.08446 (* 1 = 7.08446 loss)
I0524 08:58:02.157678 11835 sgd_solver.cpp:112] Iteration 56040, lr = 0.1
I0524 08:58:09.737448 11835 solver.cpp:239] Iteration 56050 (1.31935 iter/s, 7.5795s/10 iters), loss = 5.99471
I0524 08:58:09.737509 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99471 (* 1 = 5.99471 loss)
I0524 08:58:10.363950 11835 sgd_solver.cpp:112] Iteration 56050, lr = 0.1
I0524 08:58:15.950279 11835 solver.cpp:239] Iteration 56060 (1.60965 iter/s, 6.21253s/10 iters), loss = 6.05106
I0524 08:58:15.950520 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05106 (* 1 = 6.05106 loss)
I0524 08:58:16.244247 11835 sgd_solver.cpp:112] Iteration 56060, lr = 0.1
I0524 08:58:22.524454 11835 solver.cpp:239] Iteration 56070 (1.52121 iter/s, 6.5737s/10 iters), loss = 7.02039
I0524 08:58:22.524502 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.02039 (* 1 = 7.02039 loss)
I0524 08:58:22.524519 11835 sgd_solver.cpp:112] Iteration 56070, lr = 0.1
I0524 08:58:30.529958 11835 solver.cpp:239] Iteration 56080 (1.2492 iter/s, 8.00513s/10 iters), loss = 4.81485
I0524 08:58:30.530014 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.81485 (* 1 = 4.81485 loss)
I0524 08:58:30.530190 11835 sgd_solver.cpp:112] Iteration 56080, lr = 0.1
I0524 08:58:36.659519 11835 solver.cpp:239] Iteration 56090 (1.63152 iter/s, 6.12927s/10 iters), loss = 6.65644
I0524 08:58:36.659567 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65644 (* 1 = 6.65644 loss)
I0524 08:58:36.659821 11835 sgd_solver.cpp:112] Iteration 56090, lr = 0.1
I0524 08:58:43.190963 11835 solver.cpp:239] Iteration 56100 (1.53113 iter/s, 6.53114s/10 iters), loss = 6.19057
I0524 08:58:43.191011 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19057 (* 1 = 6.19057 loss)
I0524 08:58:43.191262 11835 sgd_solver.cpp:112] Iteration 56100, lr = 0.1
I0524 08:58:49.154896 11835 solver.cpp:239] Iteration 56110 (1.67683 iter/s, 5.96365s/10 iters), loss = 6.03895
I0524 08:58:49.155012 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03895 (* 1 = 6.03895 loss)
I0524 08:58:50.297477 11835 sgd_solver.cpp:112] Iteration 56110, lr = 0.1
I0524 08:58:56.052669 11835 solver.cpp:239] Iteration 56120 (1.44982 iter/s, 6.8974s/10 iters), loss = 4.9206
I0524 08:58:56.052716 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.9206 (* 1 = 4.9206 loss)
I0524 08:58:56.052731 11835 sgd_solver.cpp:112] Iteration 56120, lr = 0.1
I0524 08:59:06.094806 11835 solver.cpp:239] Iteration 56130 (0.995853 iter/s, 10.0416s/10 iters), loss = 6.50241
I0524 08:59:06.094863 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50241 (* 1 = 6.50241 loss)
I0524 08:59:06.389530 11835 sgd_solver.cpp:112] Iteration 56130, lr = 0.1
I0524 08:59:13.614629 11835 solver.cpp:239] Iteration 56140 (1.32988 iter/s, 7.51948s/10 iters), loss = 6.04053
I0524 08:59:13.614683 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04053 (* 1 = 6.04053 loss)
I0524 08:59:13.614805 11835 sgd_solver.cpp:112] Iteration 56140, lr = 0.1
I0524 08:59:19.708269 11835 solver.cpp:239] Iteration 56150 (1.64113 iter/s, 6.09336s/10 iters), loss = 5.32783
I0524 08:59:19.708508 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.32783 (* 1 = 5.32783 loss)
I0524 08:59:19.708592 11835 sgd_solver.cpp:112] Iteration 56150, lr = 0.1
I0524 08:59:27.445053 11835 solver.cpp:239] Iteration 56160 (1.29261 iter/s, 7.73628s/10 iters), loss = 6.45968
I0524 08:59:27.445111 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45968 (* 1 = 6.45968 loss)
I0524 08:59:27.445163 11835 sgd_solver.cpp:112] Iteration 56160, lr = 0.1
I0524 08:59:34.462327 11835 solver.cpp:239] Iteration 56170 (1.42512 iter/s, 7.01694s/10 iters), loss = 6.14131
I0524 08:59:34.462379 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14131 (* 1 = 6.14131 loss)
I0524 08:59:34.462581 11835 sgd_solver.cpp:112] Iteration 56170, lr = 0.1
I0524 08:59:42.648440 11835 solver.cpp:239] Iteration 56180 (1.22164 iter/s, 8.18575s/10 iters), loss = 6.21232
I0524 08:59:42.648495 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21232 (* 1 = 6.21232 loss)
I0524 08:59:42.648723 11835 sgd_solver.cpp:112] Iteration 56180, lr = 0.1
I0524 08:59:48.568328 11835 solver.cpp:239] Iteration 56190 (1.68931 iter/s, 5.91959s/10 iters), loss = 5.09177
I0524 08:59:48.568379 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.09177 (* 1 = 5.09177 loss)
I0524 08:59:48.568393 11835 sgd_solver.cpp:112] Iteration 56190, lr = 0.1
I0524 08:59:54.285640 11835 solver.cpp:239] Iteration 56200 (1.74916 iter/s, 5.71704s/10 iters), loss = 5.79932
I0524 08:59:54.285766 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79932 (* 1 = 5.79932 loss)
I0524 08:59:54.285820 11835 sgd_solver.cpp:112] Iteration 56200, lr = 0.1
I0524 09:00:01.250918 11835 solver.cpp:239] Iteration 56210 (1.43578 iter/s, 6.96487s/10 iters), loss = 4.8648
I0524 09:00:01.250986 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.8648 (* 1 = 4.8648 loss)
I0524 09:00:01.251302 11835 sgd_solver.cpp:112] Iteration 56210, lr = 0.1
I0524 09:00:07.695729 11835 solver.cpp:239] Iteration 56220 (1.55171 iter/s, 6.4445s/10 iters), loss = 6.90652
I0524 09:00:07.695776 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90652 (* 1 = 6.90652 loss)
I0524 09:00:07.835777 11835 sgd_solver.cpp:112] Iteration 56220, lr = 0.1
I0524 09:00:14.295101 11835 solver.cpp:239] Iteration 56230 (1.51537 iter/s, 6.59906s/10 iters), loss = 4.91268
I0524 09:00:14.295150 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.91268 (* 1 = 4.91268 loss)
I0524 09:00:15.175586 11835 sgd_solver.cpp:112] Iteration 56230, lr = 0.1
I0524 09:00:26.920240 11835 solver.cpp:239] Iteration 56240 (0.792104 iter/s, 12.6246s/10 iters), loss = 6.60086
I0524 09:00:26.920398 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60086 (* 1 = 6.60086 loss)
I0524 09:00:27.342911 11835 sgd_solver.cpp:112] Iteration 56240, lr = 0.1
I0524 09:00:33.971199 11835 solver.cpp:239] Iteration 56250 (1.41833 iter/s, 7.05055s/10 iters), loss = 5.92825
I0524 09:00:33.971241 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92825 (* 1 = 5.92825 loss)
I0524 09:00:33.971374 11835 sgd_solver.cpp:112] Iteration 56250, lr = 0.1
I0524 09:00:41.009459 11835 solver.cpp:239] Iteration 56260 (1.42088 iter/s, 7.03787s/10 iters), loss = 5.72387
I0524 09:00:41.009647 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72387 (* 1 = 5.72387 loss)
I0524 09:00:41.009676 11835 sgd_solver.cpp:112] Iteration 56260, lr = 0.1
I0524 09:00:48.916656 11835 solver.cpp:239] Iteration 56270 (1.26474 iter/s, 7.90679s/10 iters), loss = 6.12723
I0524 09:00:48.916710 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12723 (* 1 = 6.12723 loss)
I0524 09:00:48.916833 11835 sgd_solver.cpp:112] Iteration 56270, lr = 0.1
I0524 09:00:56.034353 11835 solver.cpp:239] Iteration 56280 (1.40502 iter/s, 7.11733s/10 iters), loss = 6.11771
I0524 09:00:56.034469 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11771 (* 1 = 6.11771 loss)
I0524 09:00:56.034492 11835 sgd_solver.cpp:112] Iteration 56280, lr = 0.1
I0524 09:01:02.648391 11835 solver.cpp:239] Iteration 56290 (1.51206 iter/s, 6.61349s/10 iters), loss = 5.61518
I0524 09:01:02.648625 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.61518 (* 1 = 5.61518 loss)
I0524 09:01:02.648674 11835 sgd_solver.cpp:112] Iteration 56290, lr = 0.1
I0524 09:01:09.807104 11835 solver.cpp:239] Iteration 56300 (1.39708 iter/s, 7.15778s/10 iters), loss = 5.80549
I0524 09:01:09.807145 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80549 (* 1 = 5.80549 loss)
I0524 09:01:10.102061 11835 sgd_solver.cpp:112] Iteration 56300, lr = 0.1
I0524 09:01:19.811403 11835 solver.cpp:239] Iteration 56310 (0.999614 iter/s, 10.0039s/10 iters), loss = 6.45704
I0524 09:01:19.811457 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45704 (* 1 = 6.45704 loss)
I0524 09:01:19.811710 11835 sgd_solver.cpp:112] Iteration 56310, lr = 0.1
I0524 09:01:25.602280 11835 solver.cpp:239] Iteration 56320 (1.72694 iter/s, 5.7906s/10 iters), loss = 5.52477
I0524 09:01:25.602326 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.52477 (* 1 = 5.52477 loss)
I0524 09:01:25.602344 11835 sgd_solver.cpp:112] Iteration 56320, lr = 0.1
I0524 09:01:31.285095 11835 solver.cpp:239] Iteration 56330 (1.7598 iter/s, 5.68246s/10 iters), loss = 6.16564
I0524 09:01:31.285151 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16564 (* 1 = 6.16564 loss)
I0524 09:01:31.285213 11835 sgd_solver.cpp:112] Iteration 56330, lr = 0.1
I0524 09:01:38.818650 11835 solver.cpp:239] Iteration 56340 (1.32745 iter/s, 7.53322s/10 iters), loss = 6.54161
I0524 09:01:38.818863 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54161 (* 1 = 6.54161 loss)
I0524 09:01:38.818967 11835 sgd_solver.cpp:112] Iteration 56340, lr = 0.1
I0524 09:01:45.067236 11835 solver.cpp:239] Iteration 56350 (1.60048 iter/s, 6.24813s/10 iters), loss = 5.64179
I0524 09:01:45.067286 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.64179 (* 1 = 5.64179 loss)
I0524 09:01:45.068503 11835 sgd_solver.cpp:112] Iteration 56350, lr = 0.1
I0524 09:01:52.268517 11835 solver.cpp:239] Iteration 56360 (1.38872 iter/s, 7.20089s/10 iters), loss = 5.90186
I0524 09:01:52.268597 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90186 (* 1 = 5.90186 loss)
I0524 09:01:52.268621 11835 sgd_solver.cpp:112] Iteration 56360, lr = 0.1
I0524 09:01:59.546420 11835 solver.cpp:239] Iteration 56370 (1.37409 iter/s, 7.27755s/10 iters), loss = 5.91005
I0524 09:01:59.546473 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91005 (* 1 = 5.91005 loss)
I0524 09:01:59.858651 11835 sgd_solver.cpp:112] Iteration 56370, lr = 0.1
I0524 09:02:08.525171 11835 solver.cpp:239] Iteration 56380 (1.11379 iter/s, 8.97835s/10 iters), loss = 6.57137
I0524 09:02:08.525243 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57137 (* 1 = 6.57137 loss)
I0524 09:02:08.525365 11835 sgd_solver.cpp:112] Iteration 56380, lr = 0.1
I0524 09:02:15.517505 11835 solver.cpp:239] Iteration 56390 (1.43021 iter/s, 6.99198s/10 iters), loss = 6.02348
I0524 09:02:15.517685 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.02348 (* 1 = 6.02348 loss)
I0524 09:02:16.029623 11835 sgd_solver.cpp:112] Iteration 56390, lr = 0.1
I0524 09:02:22.402619 11835 solver.cpp:239] Iteration 56400 (1.4525 iter/s, 6.88468s/10 iters), loss = 7.24219
I0524 09:02:22.402665 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.24219 (* 1 = 7.24219 loss)
I0524 09:02:23.741690 11835 sgd_solver.cpp:112] Iteration 56400, lr = 0.1
I0524 09:02:30.079624 11835 solver.cpp:239] Iteration 56410 (1.30265 iter/s, 7.67666s/10 iters), loss = 5.8038
I0524 09:02:30.079676 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8038 (* 1 = 5.8038 loss)
I0524 09:02:30.118643 11835 sgd_solver.cpp:112] Iteration 56410, lr = 0.1
I0524 09:02:36.580301 11835 solver.cpp:239] Iteration 56420 (1.53837 iter/s, 6.50037s/10 iters), loss = 5.428
I0524 09:02:36.580356 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.428 (* 1 = 5.428 loss)
I0524 09:02:36.597618 11835 sgd_solver.cpp:112] Iteration 56420, lr = 0.1
I0524 09:02:42.715658 11835 solver.cpp:239] Iteration 56430 (1.62997 iter/s, 6.13506s/10 iters), loss = 5.89158
I0524 09:02:42.715710 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89158 (* 1 = 5.89158 loss)
I0524 09:02:42.715725 11835 sgd_solver.cpp:112] Iteration 56430, lr = 0.1
I0524 09:02:48.974520 11835 solver.cpp:239] Iteration 56440 (1.59783 iter/s, 6.25851s/10 iters), loss = 6.10876
I0524 09:02:48.974781 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10876 (* 1 = 6.10876 loss)
I0524 09:02:48.974834 11835 sgd_solver.cpp:112] Iteration 56440, lr = 0.1
I0524 09:02:55.665730 11835 solver.cpp:239] Iteration 56450 (1.49464 iter/s, 6.69059s/10 iters), loss = 5.44506
I0524 09:02:55.665777 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.44506 (* 1 = 5.44506 loss)
I0524 09:02:55.665995 11835 sgd_solver.cpp:112] Iteration 56450, lr = 0.1
I0524 09:03:04.973217 11835 solver.cpp:239] Iteration 56460 (1.07445 iter/s, 9.30708s/10 iters), loss = 5.24232
I0524 09:03:04.973270 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.24232 (* 1 = 5.24232 loss)
I0524 09:03:05.090252 11835 sgd_solver.cpp:112] Iteration 56460, lr = 0.1
I0524 09:03:11.097636 11835 solver.cpp:239] Iteration 56470 (1.63289 iter/s, 6.12413s/10 iters), loss = 5.67967
I0524 09:03:11.097687 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67967 (* 1 = 5.67967 loss)
I0524 09:03:11.097702 11835 sgd_solver.cpp:112] Iteration 56470, lr = 0.1
I0524 09:03:18.206188 11835 solver.cpp:239] Iteration 56480 (1.40682 iter/s, 7.10822s/10 iters), loss = 5.30442
I0524 09:03:18.206243 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.30442 (* 1 = 5.30442 loss)
I0524 09:03:18.206305 11835 sgd_solver.cpp:112] Iteration 56480, lr = 0.1
I0524 09:03:24.939275 11835 solver.cpp:239] Iteration 56490 (1.48527 iter/s, 6.73277s/10 iters), loss = 6.11923
I0524 09:03:24.939568 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11923 (* 1 = 6.11923 loss)
I0524 09:03:24.939615 11835 sgd_solver.cpp:112] Iteration 56490, lr = 0.1
I0524 09:03:31.430557 11835 solver.cpp:239] Iteration 56500 (1.54065 iter/s, 6.49075s/10 iters), loss = 6.2383
I0524 09:03:31.430615 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2383 (* 1 = 6.2383 loss)
I0524 09:03:31.430719 11835 sgd_solver.cpp:112] Iteration 56500, lr = 0.1
I0524 09:03:38.219043 11835 solver.cpp:239] Iteration 56510 (1.47315 iter/s, 6.78817s/10 iters), loss = 5.7936
I0524 09:03:38.219103 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7936 (* 1 = 5.7936 loss)
I0524 09:03:38.219205 11835 sgd_solver.cpp:112] Iteration 56510, lr = 0.1
I0524 09:03:46.766001 11835 solver.cpp:239] Iteration 56520 (1.17006 iter/s, 8.54657s/10 iters), loss = 5.92453
I0524 09:03:46.766055 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92453 (* 1 = 5.92453 loss)
I0524 09:03:46.766083 11835 sgd_solver.cpp:112] Iteration 56520, lr = 0.1
I0524 09:03:53.000638 11835 solver.cpp:239] Iteration 56530 (1.6043 iter/s, 6.23323s/10 iters), loss = 5.88359
I0524 09:03:53.000691 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88359 (* 1 = 5.88359 loss)
I0524 09:03:54.158650 11835 sgd_solver.cpp:112] Iteration 56530, lr = 0.1
I0524 09:04:04.202409 11835 solver.cpp:239] Iteration 56540 (0.892755 iter/s, 11.2013s/10 iters), loss = 6.03033
I0524 09:04:04.202646 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03033 (* 1 = 6.03033 loss)
I0524 09:04:04.202687 11835 sgd_solver.cpp:112] Iteration 56540, lr = 0.1
I0524 09:04:12.760239 11835 solver.cpp:239] Iteration 56550 (1.1686 iter/s, 8.55727s/10 iters), loss = 6.87877
I0524 09:04:12.760293 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.87877 (* 1 = 6.87877 loss)
I0524 09:04:12.760597 11835 sgd_solver.cpp:112] Iteration 56550, lr = 0.1
I0524 09:04:19.161500 11835 solver.cpp:239] Iteration 56560 (1.56227 iter/s, 6.40096s/10 iters), loss = 5.88333
I0524 09:04:19.161556 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88333 (* 1 = 5.88333 loss)
I0524 09:04:19.161670 11835 sgd_solver.cpp:112] Iteration 56560, lr = 0.1
I0524 09:04:24.848062 11835 solver.cpp:239] Iteration 56570 (1.75861 iter/s, 5.68629s/10 iters), loss = 5.84156
I0524 09:04:24.848104 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84156 (* 1 = 5.84156 loss)
I0524 09:04:24.848120 11835 sgd_solver.cpp:112] Iteration 56570, lr = 0.1
I0524 09:04:32.138053 11835 solver.cpp:239] Iteration 56580 (1.37182 iter/s, 7.2896s/10 iters), loss = 5.80687
I0524 09:04:32.138105 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80687 (* 1 = 5.80687 loss)
I0524 09:04:32.143030 11835 sgd_solver.cpp:112] Iteration 56580, lr = 0.1
I0524 09:04:39.412679 11835 solver.cpp:239] Iteration 56590 (1.37471 iter/s, 7.27428s/10 iters), loss = 6.12219
I0524 09:04:39.412968 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12219 (* 1 = 6.12219 loss)
I0524 09:04:39.422569 11835 sgd_solver.cpp:112] Iteration 56590, lr = 0.1
I0524 09:04:46.959350 11835 solver.cpp:239] Iteration 56600 (1.32519 iter/s, 7.54612s/10 iters), loss = 5.88236
I0524 09:04:46.959434 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88236 (* 1 = 5.88236 loss)
I0524 09:04:46.959681 11835 sgd_solver.cpp:112] Iteration 56600, lr = 0.1
I0524 09:04:53.685919 11835 solver.cpp:239] Iteration 56610 (1.48671 iter/s, 6.72624s/10 iters), loss = 5.71909
I0524 09:04:53.685969 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71909 (* 1 = 5.71909 loss)
I0524 09:04:54.018800 11835 sgd_solver.cpp:112] Iteration 56610, lr = 0.1
I0524 09:05:00.974330 11835 solver.cpp:239] Iteration 56620 (1.3721 iter/s, 7.28808s/10 iters), loss = 4.96607
I0524 09:05:00.974383 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.96607 (* 1 = 4.96607 loss)
I0524 09:05:00.974398 11835 sgd_solver.cpp:112] Iteration 56620, lr = 0.1
I0524 09:05:08.353627 11835 solver.cpp:239] Iteration 56630 (1.35536 iter/s, 7.3781s/10 iters), loss = 6.14435
I0524 09:05:08.353687 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14435 (* 1 = 6.14435 loss)
I0524 09:05:08.353904 11835 sgd_solver.cpp:112] Iteration 56630, lr = 0.1
I0524 09:05:16.392309 11835 solver.cpp:239] Iteration 56640 (1.24404 iter/s, 8.03832s/10 iters), loss = 5.84097
I0524 09:05:16.392608 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84097 (* 1 = 5.84097 loss)
I0524 09:05:16.392664 11835 sgd_solver.cpp:112] Iteration 56640, lr = 0.1
I0524 09:05:24.464552 11835 solver.cpp:239] Iteration 56650 (1.23895 iter/s, 8.07136s/10 iters), loss = 5.91522
I0524 09:05:24.464608 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91522 (* 1 = 5.91522 loss)
I0524 09:05:24.464764 11835 sgd_solver.cpp:112] Iteration 56650, lr = 0.1
I0524 09:05:32.088426 11835 solver.cpp:239] Iteration 56660 (1.31173 iter/s, 7.62352s/10 iters), loss = 6.52051
I0524 09:05:32.088476 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52051 (* 1 = 6.52051 loss)
I0524 09:05:32.575428 11835 sgd_solver.cpp:112] Iteration 56660, lr = 0.1
I0524 09:05:40.434218 11835 solver.cpp:239] Iteration 56670 (1.19826 iter/s, 8.34542s/10 iters), loss = 6.29591
I0524 09:05:40.434270 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29591 (* 1 = 6.29591 loss)
I0524 09:05:40.605173 11835 sgd_solver.cpp:112] Iteration 56670, lr = 0.1
I0524 09:05:46.450668 11835 solver.cpp:239] Iteration 56680 (1.66219 iter/s, 6.01617s/10 iters), loss = 6.52756
I0524 09:05:46.450862 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.52756 (* 1 = 6.52756 loss)
I0524 09:05:46.450889 11835 sgd_solver.cpp:112] Iteration 56680, lr = 0.1
I0524 09:05:54.075871 11835 solver.cpp:239] Iteration 56690 (1.31153 iter/s, 7.62468s/10 iters), loss = 5.14138
I0524 09:05:54.075930 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.14138 (* 1 = 5.14138 loss)
I0524 09:05:54.076038 11835 sgd_solver.cpp:112] Iteration 56690, lr = 0.1
I0524 09:06:00.372251 11835 solver.cpp:239] Iteration 56700 (1.58829 iter/s, 6.29608s/10 iters), loss = 4.75298
I0524 09:06:00.372299 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.75298 (* 1 = 4.75298 loss)
I0524 09:06:00.372407 11835 sgd_solver.cpp:112] Iteration 56700, lr = 0.1
I0524 09:06:06.965982 11835 solver.cpp:239] Iteration 56710 (1.51666 iter/s, 6.59342s/10 iters), loss = 6.45084
I0524 09:06:06.966033 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45084 (* 1 = 6.45084 loss)
I0524 09:06:06.966123 11835 sgd_solver.cpp:112] Iteration 56710, lr = 0.1
I0524 09:06:14.335254 11835 solver.cpp:239] Iteration 56720 (1.35705 iter/s, 7.36893s/10 iters), loss = 6.31769
I0524 09:06:14.335311 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31769 (* 1 = 6.31769 loss)
I0524 09:06:15.514200 11835 sgd_solver.cpp:112] Iteration 56720, lr = 0.1
I0524 09:06:24.990947 11835 solver.cpp:239] Iteration 56730 (0.938506 iter/s, 10.6552s/10 iters), loss = 6.43745
I0524 09:06:24.991209 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43745 (* 1 = 6.43745 loss)
I0524 09:06:24.991266 11835 sgd_solver.cpp:112] Iteration 56730, lr = 0.1
I0524 09:06:35.494596 11835 solver.cpp:239] Iteration 56740 (0.952106 iter/s, 10.503s/10 iters), loss = 5.72159
I0524 09:06:35.494743 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72159 (* 1 = 5.72159 loss)
I0524 09:06:36.318271 11835 sgd_solver.cpp:112] Iteration 56740, lr = 0.1
I0524 09:06:43.781885 11835 solver.cpp:239] Iteration 56750 (1.20672 iter/s, 8.28691s/10 iters), loss = 6.33134
I0524 09:06:43.781939 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33134 (* 1 = 6.33134 loss)
I0524 09:06:43.866364 11835 sgd_solver.cpp:112] Iteration 56750, lr = 0.1
I0524 09:06:52.480912 11835 solver.cpp:239] Iteration 56760 (1.1496 iter/s, 8.69864s/10 iters), loss = 5.89118
I0524 09:06:52.481024 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.89118 (* 1 = 5.89118 loss)
I0524 09:06:52.551815 11835 sgd_solver.cpp:112] Iteration 56760, lr = 0.1
I0524 09:07:00.687186 11835 solver.cpp:239] Iteration 56770 (1.21863 iter/s, 8.20591s/10 iters), loss = 4.67645
I0524 09:07:00.690742 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.67645 (* 1 = 4.67645 loss)
I0524 09:07:01.050530 11835 sgd_solver.cpp:112] Iteration 56770, lr = 0.1
I0524 09:07:08.797981 11835 solver.cpp:239] Iteration 56780 (1.23351 iter/s, 8.10692s/10 iters), loss = 5.187
I0524 09:07:08.798041 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.187 (* 1 = 5.187 loss)
I0524 09:07:08.893537 11835 sgd_solver.cpp:112] Iteration 56780, lr = 0.1
I0524 09:07:15.303267 11835 solver.cpp:239] Iteration 56790 (1.53729 iter/s, 6.50497s/10 iters), loss = 7.03616
I0524 09:07:15.303328 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03616 (* 1 = 7.03616 loss)
I0524 09:07:15.519299 11835 sgd_solver.cpp:112] Iteration 56790, lr = 0.1
I0524 09:07:23.554420 11835 solver.cpp:239] Iteration 56800 (1.21201 iter/s, 8.25077s/10 iters), loss = 5.71724
I0524 09:07:23.554474 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71724 (* 1 = 5.71724 loss)
I0524 09:07:23.554757 11835 sgd_solver.cpp:112] Iteration 56800, lr = 0.1
I0524 09:07:31.103698 11835 solver.cpp:239] Iteration 56810 (1.32469 iter/s, 7.54893s/10 iters), loss = 7.03254
I0524 09:07:31.104061 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.03254 (* 1 = 7.03254 loss)
I0524 09:07:31.104127 11835 sgd_solver.cpp:112] Iteration 56810, lr = 0.1
I0524 09:07:37.964351 11835 solver.cpp:239] Iteration 56820 (1.45779 iter/s, 6.8597s/10 iters), loss = 6.31479
I0524 09:07:37.964413 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31479 (* 1 = 6.31479 loss)
I0524 09:07:37.964594 11835 sgd_solver.cpp:112] Iteration 56820, lr = 0.1
I0524 09:07:44.425485 11835 solver.cpp:239] Iteration 56830 (1.54779 iter/s, 6.46082s/10 iters), loss = 6.9584
I0524 09:07:44.425555 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9584 (* 1 = 6.9584 loss)
I0524 09:07:44.425570 11835 sgd_solver.cpp:112] Iteration 56830, lr = 0.1
I0524 09:07:51.847535 11835 solver.cpp:239] Iteration 56840 (1.34741 iter/s, 7.42165s/10 iters), loss = 6.88984
I0524 09:07:51.847594 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88984 (* 1 = 6.88984 loss)
I0524 09:07:52.035292 11835 sgd_solver.cpp:112] Iteration 56840, lr = 0.1
I0524 09:07:59.353117 11835 solver.cpp:239] Iteration 56850 (1.3324 iter/s, 7.50523s/10 iters), loss = 5.95064
I0524 09:07:59.353169 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95064 (* 1 = 5.95064 loss)
I0524 09:07:59.353183 11835 sgd_solver.cpp:112] Iteration 56850, lr = 0.1
I0524 09:08:06.570765 11835 solver.cpp:239] Iteration 56860 (1.38556 iter/s, 7.21729s/10 iters), loss = 5.82336
I0524 09:08:06.570921 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82336 (* 1 = 5.82336 loss)
I0524 09:08:06.570986 11835 sgd_solver.cpp:112] Iteration 56860, lr = 0.1
I0524 09:08:12.405141 11835 solver.cpp:239] Iteration 56870 (1.71409 iter/s, 5.83401s/10 iters), loss = 5.73926
I0524 09:08:12.405179 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73926 (* 1 = 5.73926 loss)
I0524 09:08:13.419554 11835 sgd_solver.cpp:112] Iteration 56870, lr = 0.1
I0524 09:08:21.583410 11835 solver.cpp:239] Iteration 56880 (1.08958 iter/s, 9.17787s/10 iters), loss = 5.73733
I0524 09:08:21.583456 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.73733 (* 1 = 5.73733 loss)
I0524 09:08:21.583470 11835 sgd_solver.cpp:112] Iteration 56880, lr = 0.1
I0524 09:08:31.213451 11835 solver.cpp:239] Iteration 56890 (1.03846 iter/s, 9.62963s/10 iters), loss = 6.39661
I0524 09:08:31.213505 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39661 (* 1 = 6.39661 loss)
I0524 09:08:31.213522 11835 sgd_solver.cpp:112] Iteration 56890, lr = 0.1
I0524 09:08:39.449627 11835 solver.cpp:239] Iteration 56900 (1.21437 iter/s, 8.23471s/10 iters), loss = 5.25943
I0524 09:08:39.450381 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.25943 (* 1 = 5.25943 loss)
I0524 09:08:39.450419 11835 sgd_solver.cpp:112] Iteration 56900, lr = 0.1
I0524 09:08:45.761165 11835 solver.cpp:239] Iteration 56910 (1.58465 iter/s, 6.31052s/10 iters), loss = 6.43628
I0524 09:08:45.761234 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43628 (* 1 = 6.43628 loss)
I0524 09:08:45.761358 11835 sgd_solver.cpp:112] Iteration 56910, lr = 0.1
I0524 09:08:52.833107 11835 solver.cpp:239] Iteration 56920 (1.41411 iter/s, 7.0716s/10 iters), loss = 6.45412
I0524 09:08:52.833163 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45412 (* 1 = 6.45412 loss)
I0524 09:08:52.833295 11835 sgd_solver.cpp:112] Iteration 56920, lr = 0.1
I0524 09:08:58.924520 11835 solver.cpp:239] Iteration 56930 (1.64173 iter/s, 6.09112s/10 iters), loss = 6.01897
I0524 09:08:58.924571 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01897 (* 1 = 6.01897 loss)
I0524 09:08:58.941947 11835 sgd_solver.cpp:112] Iteration 56930, lr = 0.1
I0524 09:09:05.512676 11835 solver.cpp:239] Iteration 56940 (1.51795 iter/s, 6.58785s/10 iters), loss = 6.9869
I0524 09:09:05.512727 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9869 (* 1 = 6.9869 loss)
I0524 09:09:05.512888 11835 sgd_solver.cpp:112] Iteration 56940, lr = 0.1
I0524 09:09:12.864696 11835 solver.cpp:239] Iteration 56950 (1.36024 iter/s, 7.35167s/10 iters), loss = 5.76973
I0524 09:09:12.870749 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.76973 (* 1 = 5.76973 loss)
I0524 09:09:12.870779 11835 sgd_solver.cpp:112] Iteration 56950, lr = 0.1
I0524 09:09:19.919317 11835 solver.cpp:239] Iteration 56960 (1.4189 iter/s, 7.04774s/10 iters), loss = 5.95976
I0524 09:09:19.919375 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95976 (* 1 = 5.95976 loss)
I0524 09:09:20.909404 11835 sgd_solver.cpp:112] Iteration 56960, lr = 0.1
I0524 09:09:29.295672 11835 solver.cpp:239] Iteration 56970 (1.06656 iter/s, 9.37594s/10 iters), loss = 6.12349
I0524 09:09:29.295716 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12349 (* 1 = 6.12349 loss)
I0524 09:09:29.295850 11835 sgd_solver.cpp:112] Iteration 56970, lr = 0.1
I0524 09:09:35.899492 11835 solver.cpp:239] Iteration 56980 (1.51435 iter/s, 6.60351s/10 iters), loss = 6.30646
I0524 09:09:35.899549 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30646 (* 1 = 6.30646 loss)
I0524 09:09:35.899725 11835 sgd_solver.cpp:112] Iteration 56980, lr = 0.1
I0524 09:09:41.606210 11835 solver.cpp:239] Iteration 56990 (1.7524 iter/s, 5.70645s/10 iters), loss = 5.63627
I0524 09:09:41.606254 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63627 (* 1 = 5.63627 loss)
I0524 09:09:41.606497 11835 sgd_solver.cpp:112] Iteration 56990, lr = 0.1
I0524 09:09:48.187553 11835 solver.cpp:239] Iteration 57000 (1.51952 iter/s, 6.58103s/10 iters), loss = 6.51313
I0524 09:09:48.187706 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51313 (* 1 = 6.51313 loss)
I0524 09:09:48.187739 11835 sgd_solver.cpp:112] Iteration 57000, lr = 0.1
I0524 09:09:54.979478 11835 solver.cpp:239] Iteration 57010 (1.47243 iter/s, 6.79151s/10 iters), loss = 6.04181
I0524 09:09:54.979542 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04181 (* 1 = 6.04181 loss)
I0524 09:09:54.979851 11835 sgd_solver.cpp:112] Iteration 57010, lr = 0.1
I0524 09:10:02.669311 11835 solver.cpp:239] Iteration 57020 (1.30048 iter/s, 7.68947s/10 iters), loss = 5.99891
I0524 09:10:02.669360 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99891 (* 1 = 5.99891 loss)
I0524 09:10:02.687876 11835 sgd_solver.cpp:112] Iteration 57020, lr = 0.1
I0524 09:10:11.481171 11835 solver.cpp:239] Iteration 57030 (1.13488 iter/s, 8.81147s/10 iters), loss = 6.9742
I0524 09:10:11.481225 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9742 (* 1 = 6.9742 loss)
I0524 09:10:11.653492 11835 sgd_solver.cpp:112] Iteration 57030, lr = 0.1
I0524 09:10:18.886946 11835 solver.cpp:239] Iteration 57040 (1.35036 iter/s, 7.40543s/10 iters), loss = 5.59193
I0524 09:10:18.887243 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.59193 (* 1 = 5.59193 loss)
I0524 09:10:18.887467 11835 sgd_solver.cpp:112] Iteration 57040, lr = 0.1
I0524 09:10:26.488041 11835 solver.cpp:239] Iteration 57050 (1.31569 iter/s, 7.60055s/10 iters), loss = 5.82305
I0524 09:10:26.488095 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82305 (* 1 = 5.82305 loss)
I0524 09:10:26.507437 11835 sgd_solver.cpp:112] Iteration 57050, lr = 0.1
I0524 09:10:32.253404 11835 solver.cpp:239] Iteration 57060 (1.73458 iter/s, 5.76509s/10 iters), loss = 6.70468
I0524 09:10:32.253449 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70468 (* 1 = 6.70468 loss)
I0524 09:10:32.253512 11835 sgd_solver.cpp:112] Iteration 57060, lr = 0.1
I0524 09:10:39.862962 11835 solver.cpp:239] Iteration 57070 (1.3142 iter/s, 7.60921s/10 iters), loss = 6.39777
I0524 09:10:39.863019 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39777 (* 1 = 6.39777 loss)
I0524 09:10:39.863189 11835 sgd_solver.cpp:112] Iteration 57070, lr = 0.1
I0524 09:10:45.986670 11835 solver.cpp:239] Iteration 57080 (1.63308 iter/s, 6.12338s/10 iters), loss = 6.39253
I0524 09:10:45.986783 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.39253 (* 1 = 6.39253 loss)
I0524 09:10:45.986811 11835 sgd_solver.cpp:112] Iteration 57080, lr = 0.1
I0524 09:10:52.487669 11835 solver.cpp:239] Iteration 57090 (1.53846 iter/s, 6.50003s/10 iters), loss = 4.91307
I0524 09:10:52.487905 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.91307 (* 1 = 4.91307 loss)
I0524 09:10:52.487952 11835 sgd_solver.cpp:112] Iteration 57090, lr = 0.1
I0524 09:11:00.249877 11835 solver.cpp:239] Iteration 57100 (1.2884 iter/s, 7.76156s/10 iters), loss = 5.86865
I0524 09:11:00.249933 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86865 (* 1 = 5.86865 loss)
I0524 09:11:00.250308 11835 sgd_solver.cpp:112] Iteration 57100, lr = 0.1
I0524 09:11:07.257732 11835 solver.cpp:239] Iteration 57110 (1.42704 iter/s, 7.00753s/10 iters), loss = 6.60565
I0524 09:11:07.257786 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60565 (* 1 = 6.60565 loss)
I0524 09:11:08.624074 11835 sgd_solver.cpp:112] Iteration 57110, lr = 0.1
I0524 09:11:14.619951 11835 solver.cpp:239] Iteration 57120 (1.35835 iter/s, 7.36187s/10 iters), loss = 5.4484
I0524 09:11:14.620019 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.4484 (* 1 = 5.4484 loss)
I0524 09:11:14.620043 11835 sgd_solver.cpp:112] Iteration 57120, lr = 0.1
I0524 09:11:21.328135 11835 solver.cpp:239] Iteration 57130 (1.49126 iter/s, 6.70572s/10 iters), loss = 5.98448
I0524 09:11:21.328194 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98448 (* 1 = 5.98448 loss)
I0524 09:11:21.328449 11835 sgd_solver.cpp:112] Iteration 57130, lr = 0.1
I0524 09:11:27.923141 11835 solver.cpp:239] Iteration 57140 (1.51637 iter/s, 6.59469s/10 iters), loss = 5.95866
I0524 09:11:27.923409 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95866 (* 1 = 5.95866 loss)
I0524 09:11:27.941373 11835 sgd_solver.cpp:112] Iteration 57140, lr = 0.1
I0524 09:11:33.597084 11835 solver.cpp:239] Iteration 57150 (1.76257 iter/s, 5.67352s/10 iters), loss = 6.55067
I0524 09:11:33.597126 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.55067 (* 1 = 6.55067 loss)
I0524 09:11:33.953313 11835 sgd_solver.cpp:112] Iteration 57150, lr = 0.1
I0524 09:11:40.267668 11835 solver.cpp:239] Iteration 57160 (1.49919 iter/s, 6.67027s/10 iters), loss = 5.41352
I0524 09:11:40.267729 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.41352 (* 1 = 5.41352 loss)
I0524 09:11:40.267853 11835 sgd_solver.cpp:112] Iteration 57160, lr = 0.1
I0524 09:11:46.105295 11835 solver.cpp:239] Iteration 57170 (1.71311 iter/s, 5.83734s/10 iters), loss = 5.31982
I0524 09:11:46.105355 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.31982 (* 1 = 5.31982 loss)
I0524 09:11:46.105376 11835 sgd_solver.cpp:112] Iteration 57170, lr = 0.1
I0524 09:11:51.929127 11835 solver.cpp:239] Iteration 57180 (1.71719 iter/s, 5.82346s/10 iters), loss = 5.558
I0524 09:11:51.929229 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.558 (* 1 = 5.558 loss)
I0524 09:11:51.929322 11835 sgd_solver.cpp:112] Iteration 57180, lr = 0.1
I0524 09:11:59.314419 11835 solver.cpp:239] Iteration 57190 (1.35411 iter/s, 7.38492s/10 iters), loss = 5.61181
I0524 09:11:59.314731 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.61181 (* 1 = 5.61181 loss)
I0524 09:11:59.422789 11835 sgd_solver.cpp:112] Iteration 57190, lr = 0.1
I0524 09:12:08.736224 11835 solver.cpp:239] Iteration 57200 (1.06144 iter/s, 9.42119s/10 iters), loss = 5.78182
I0524 09:12:08.736281 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78182 (* 1 = 5.78182 loss)
I0524 09:12:08.736477 11835 sgd_solver.cpp:112] Iteration 57200, lr = 0.1
I0524 09:12:17.841401 11835 solver.cpp:239] Iteration 57210 (1.09832 iter/s, 9.10477s/10 iters), loss = 6.46571
I0524 09:12:17.841447 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46571 (* 1 = 6.46571 loss)
I0524 09:12:17.940950 11835 sgd_solver.cpp:112] Iteration 57210, lr = 0.1
I0524 09:12:25.820081 11835 solver.cpp:239] Iteration 57220 (1.25339 iter/s, 7.97833s/10 iters), loss = 5.53621
I0524 09:12:25.820133 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.53621 (* 1 = 5.53621 loss)
I0524 09:12:25.835053 11835 sgd_solver.cpp:112] Iteration 57220, lr = 0.1
I0524 09:12:32.655452 11835 solver.cpp:239] Iteration 57230 (1.46305 iter/s, 6.83505s/10 iters), loss = 5.8573
I0524 09:12:32.655589 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8573 (* 1 = 5.8573 loss)
I0524 09:12:32.655813 11835 sgd_solver.cpp:112] Iteration 57230, lr = 0.1
I0524 09:12:39.494810 11835 solver.cpp:239] Iteration 57240 (1.46221 iter/s, 6.83896s/10 iters), loss = 6.65711
I0524 09:12:39.494860 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65711 (* 1 = 6.65711 loss)
I0524 09:12:39.641307 11835 sgd_solver.cpp:112] Iteration 57240, lr = 0.1
I0524 09:12:47.137305 11835 solver.cpp:239] Iteration 57250 (1.30853 iter/s, 7.64216s/10 iters), loss = 6.47552
I0524 09:12:47.137344 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.47552 (* 1 = 6.47552 loss)
I0524 09:12:47.219120 11835 sgd_solver.cpp:112] Iteration 57250, lr = 0.1
I0524 09:12:53.036134 11835 solver.cpp:239] Iteration 57260 (1.69533 iter/s, 5.89855s/10 iters), loss = 5.79016
I0524 09:12:53.036186 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79016 (* 1 = 5.79016 loss)
I0524 09:12:53.699949 11835 sgd_solver.cpp:112] Iteration 57260, lr = 0.1
I0524 09:13:01.363204 11835 solver.cpp:239] Iteration 57270 (1.20096 iter/s, 8.3267s/10 iters), loss = 6.64433
I0524 09:13:01.363266 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64433 (* 1 = 6.64433 loss)
I0524 09:13:01.363363 11835 sgd_solver.cpp:112] Iteration 57270, lr = 0.1
I0524 09:13:08.985986 11835 solver.cpp:239] Iteration 57280 (1.31192 iter/s, 7.62243s/10 iters), loss = 5.99754
I0524 09:13:08.986146 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99754 (* 1 = 5.99754 loss)
I0524 09:13:08.986171 11835 sgd_solver.cpp:112] Iteration 57280, lr = 0.1
I0524 09:13:16.746217 11835 solver.cpp:239] Iteration 57290 (1.2887 iter/s, 7.75976s/10 iters), loss = 6.66216
I0524 09:13:16.746273 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66216 (* 1 = 6.66216 loss)
I0524 09:13:16.847789 11835 sgd_solver.cpp:112] Iteration 57290, lr = 0.1
I0524 09:13:24.676738 11835 solver.cpp:239] Iteration 57300 (1.26101 iter/s, 7.93015s/10 iters), loss = 6.30492
I0524 09:13:24.676795 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30492 (* 1 = 6.30492 loss)
I0524 09:13:24.677016 11835 sgd_solver.cpp:112] Iteration 57300, lr = 0.1
I0524 09:13:31.740384 11835 solver.cpp:239] Iteration 57310 (1.41576 iter/s, 7.06332s/10 iters), loss = 6.333
I0524 09:13:31.740422 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.333 (* 1 = 6.333 loss)
I0524 09:13:31.740453 11835 sgd_solver.cpp:112] Iteration 57310, lr = 0.1
I0524 09:13:37.818195 11835 solver.cpp:239] Iteration 57320 (1.64541 iter/s, 6.07752s/10 iters), loss = 6.30075
I0524 09:13:37.818251 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30075 (* 1 = 6.30075 loss)
I0524 09:13:37.818291 11835 sgd_solver.cpp:112] Iteration 57320, lr = 0.1
I0524 09:13:44.647181 11835 solver.cpp:239] Iteration 57330 (1.46442 iter/s, 6.82864s/10 iters), loss = 6.18753
I0524 09:13:44.647483 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18753 (* 1 = 6.18753 loss)
I0524 09:13:45.167815 11835 sgd_solver.cpp:112] Iteration 57330, lr = 0.1
I0524 09:13:52.876628 11835 solver.cpp:239] Iteration 57340 (1.21523 iter/s, 8.22888s/10 iters), loss = 7.0997
I0524 09:13:52.876675 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.0997 (* 1 = 7.0997 loss)
I0524 09:13:52.876813 11835 sgd_solver.cpp:112] Iteration 57340, lr = 0.1
I0524 09:14:00.585453 11835 solver.cpp:239] Iteration 57350 (1.29727 iter/s, 7.70847s/10 iters), loss = 6.73871
I0524 09:14:00.585505 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.73871 (* 1 = 6.73871 loss)
I0524 09:14:00.585623 11835 sgd_solver.cpp:112] Iteration 57350, lr = 0.1
I0524 09:14:07.944070 11835 solver.cpp:239] Iteration 57360 (1.35901 iter/s, 7.35827s/10 iters), loss = 6.26302
I0524 09:14:07.944119 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26302 (* 1 = 6.26302 loss)
I0524 09:14:08.258297 11835 sgd_solver.cpp:112] Iteration 57360, lr = 0.1
I0524 09:14:15.468886 11835 solver.cpp:239] Iteration 57370 (1.32899 iter/s, 7.52449s/10 iters), loss = 5.81328
I0524 09:14:15.469035 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81328 (* 1 = 5.81328 loss)
I0524 09:14:15.998185 11835 sgd_solver.cpp:112] Iteration 57370, lr = 0.1
I0524 09:14:23.312122 11835 solver.cpp:239] Iteration 57380 (1.27506 iter/s, 7.84277s/10 iters), loss = 6.72526
I0524 09:14:23.312186 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72526 (* 1 = 6.72526 loss)
I0524 09:14:23.312410 11835 sgd_solver.cpp:112] Iteration 57380, lr = 0.1
I0524 09:14:32.017972 11835 solver.cpp:239] Iteration 57390 (1.1487 iter/s, 8.70546s/10 iters), loss = 5.78839
I0524 09:14:32.018021 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78839 (* 1 = 5.78839 loss)
I0524 09:14:32.356016 11835 sgd_solver.cpp:112] Iteration 57390, lr = 0.1
I0524 09:14:39.981125 11835 solver.cpp:239] Iteration 57400 (1.25584 iter/s, 7.96279s/10 iters), loss = 6.14808
I0524 09:14:39.981174 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14808 (* 1 = 6.14808 loss)
I0524 09:14:39.981279 11835 sgd_solver.cpp:112] Iteration 57400, lr = 0.1
I0524 09:14:46.719539 11835 solver.cpp:239] Iteration 57410 (1.4841 iter/s, 6.73811s/10 iters), loss = 6.72234
I0524 09:14:46.719768 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72234 (* 1 = 6.72234 loss)
I0524 09:14:46.719810 11835 sgd_solver.cpp:112] Iteration 57410, lr = 0.1
I0524 09:14:53.430840 11835 solver.cpp:239] Iteration 57420 (1.49018 iter/s, 6.71061s/10 iters), loss = 6.86012
I0524 09:14:53.430891 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86012 (* 1 = 6.86012 loss)
I0524 09:14:53.430905 11835 sgd_solver.cpp:112] Iteration 57420, lr = 0.1
I0524 09:15:01.248615 11835 solver.cpp:239] Iteration 57430 (1.27923 iter/s, 7.81722s/10 iters), loss = 5.70962
I0524 09:15:01.248670 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.70962 (* 1 = 5.70962 loss)
I0524 09:15:01.248932 11835 sgd_solver.cpp:112] Iteration 57430, lr = 0.1
I0524 09:15:08.910456 11835 solver.cpp:239] Iteration 57440 (1.30523 iter/s, 7.6615s/10 iters), loss = 5.98512
I0524 09:15:08.910511 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98512 (* 1 = 5.98512 loss)
I0524 09:15:09.605512 11835 sgd_solver.cpp:112] Iteration 57440, lr = 0.1
I0524 09:15:16.213718 11835 solver.cpp:239] Iteration 57450 (1.36932 iter/s, 7.30291s/10 iters), loss = 6.03305
I0524 09:15:16.213773 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03305 (* 1 = 6.03305 loss)
I0524 09:15:16.213852 11835 sgd_solver.cpp:112] Iteration 57450, lr = 0.1
I0524 09:15:24.053812 11835 solver.cpp:239] Iteration 57460 (1.27555 iter/s, 7.83974s/10 iters), loss = 5.43509
I0524 09:15:24.054103 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43509 (* 1 = 5.43509 loss)
I0524 09:15:24.063804 11835 sgd_solver.cpp:112] Iteration 57460, lr = 0.1
I0524 09:15:29.950014 11835 solver.cpp:239] Iteration 57470 (1.69615 iter/s, 5.89571s/10 iters), loss = 5.95177
I0524 09:15:29.950062 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95177 (* 1 = 5.95177 loss)
I0524 09:15:29.950075 11835 sgd_solver.cpp:112] Iteration 57470, lr = 0.1
I0524 09:15:36.147617 11835 solver.cpp:239] Iteration 57480 (1.61361 iter/s, 6.19729s/10 iters), loss = 6.60928
I0524 09:15:36.147675 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60928 (* 1 = 6.60928 loss)
I0524 09:15:36.147883 11835 sgd_solver.cpp:112] Iteration 57480, lr = 0.1
I0524 09:15:46.187815 11835 solver.cpp:239] Iteration 57490 (0.996039 iter/s, 10.0398s/10 iters), loss = 6.09577
I0524 09:15:46.187860 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09577 (* 1 = 6.09577 loss)
I0524 09:15:46.187872 11835 sgd_solver.cpp:112] Iteration 57490, lr = 0.1
I0524 09:15:52.105921 11835 solver.cpp:239] Iteration 57500 (1.69013 iter/s, 5.91672s/10 iters), loss = 5.97056
I0524 09:15:52.105965 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97056 (* 1 = 5.97056 loss)
I0524 09:15:52.105978 11835 sgd_solver.cpp:112] Iteration 57500, lr = 0.1
I0524 09:16:01.106127 11835 solver.cpp:239] Iteration 57510 (1.11113 iter/s, 8.99981s/10 iters), loss = 6.29813
I0524 09:16:01.106333 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29813 (* 1 = 6.29813 loss)
I0524 09:16:01.106474 11835 sgd_solver.cpp:112] Iteration 57510, lr = 0.1
I0524 09:16:07.063303 11835 solver.cpp:239] Iteration 57520 (1.67876 iter/s, 5.95677s/10 iters), loss = 5.27363
I0524 09:16:07.063340 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.27363 (* 1 = 5.27363 loss)
I0524 09:16:07.098006 11835 sgd_solver.cpp:112] Iteration 57520, lr = 0.1
I0524 09:16:15.586772 11835 solver.cpp:239] Iteration 57530 (1.17328 iter/s, 8.52309s/10 iters), loss = 5.99433
I0524 09:16:15.586825 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99433 (* 1 = 5.99433 loss)
I0524 09:16:15.587028 11835 sgd_solver.cpp:112] Iteration 57530, lr = 0.1
I0524 09:16:24.010481 11835 solver.cpp:239] Iteration 57540 (1.18718 iter/s, 8.42333s/10 iters), loss = 5.92953
I0524 09:16:24.010535 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92953 (* 1 = 5.92953 loss)
I0524 09:16:24.010738 11835 sgd_solver.cpp:112] Iteration 57540, lr = 0.1
I0524 09:16:29.871008 11835 solver.cpp:239] Iteration 57550 (1.70641 iter/s, 5.86025s/10 iters), loss = 5.99793
I0524 09:16:29.871057 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99793 (* 1 = 5.99793 loss)
I0524 09:16:29.872503 11835 sgd_solver.cpp:112] Iteration 57550, lr = 0.1
I0524 09:16:36.390929 11835 solver.cpp:239] Iteration 57560 (1.53383 iter/s, 6.51961s/10 iters), loss = 6.72785
I0524 09:16:36.391095 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72785 (* 1 = 6.72785 loss)
I0524 09:16:36.391129 11835 sgd_solver.cpp:112] Iteration 57560, lr = 0.1
I0524 09:16:44.155036 11835 solver.cpp:239] Iteration 57570 (1.28805 iter/s, 7.76365s/10 iters), loss = 6.04971
I0524 09:16:44.155091 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04971 (* 1 = 6.04971 loss)
I0524 09:16:44.204536 11835 sgd_solver.cpp:112] Iteration 57570, lr = 0.1
I0524 09:16:50.233129 11835 solver.cpp:239] Iteration 57580 (1.64534 iter/s, 6.07778s/10 iters), loss = 5.13792
I0524 09:16:50.233175 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.13792 (* 1 = 5.13792 loss)
I0524 09:16:50.962811 11835 sgd_solver.cpp:112] Iteration 57580, lr = 0.1
I0524 09:16:56.406442 11835 solver.cpp:239] Iteration 57590 (1.61995 iter/s, 6.17303s/10 iters), loss = 7.23513
I0524 09:16:56.406512 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.23513 (* 1 = 7.23513 loss)
I0524 09:16:56.427276 11835 sgd_solver.cpp:112] Iteration 57590, lr = 0.1
I0524 09:17:04.815130 11835 solver.cpp:239] Iteration 57600 (1.1893 iter/s, 8.40828s/10 iters), loss = 5.19894
I0524 09:17:04.815208 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.19894 (* 1 = 5.19894 loss)
I0524 09:17:04.815414 11835 sgd_solver.cpp:112] Iteration 57600, lr = 0.1
I0524 09:17:10.913738 11835 solver.cpp:239] Iteration 57610 (1.6398 iter/s, 6.09832s/10 iters), loss = 5.00017
I0524 09:17:10.913925 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.00017 (* 1 = 5.00017 loss)
I0524 09:17:10.913966 11835 sgd_solver.cpp:112] Iteration 57610, lr = 0.1
I0524 09:17:19.216904 11835 solver.cpp:239] Iteration 57620 (1.20444 iter/s, 8.30263s/10 iters), loss = 6.37803
I0524 09:17:19.216954 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37803 (* 1 = 6.37803 loss)
I0524 09:17:19.217187 11835 sgd_solver.cpp:112] Iteration 57620, lr = 0.1
I0524 09:17:28.061213 11835 solver.cpp:239] Iteration 57630 (1.13072 iter/s, 8.84392s/10 iters), loss = 6.29929
I0524 09:17:28.061252 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29929 (* 1 = 6.29929 loss)
I0524 09:17:28.768142 11835 sgd_solver.cpp:112] Iteration 57630, lr = 0.1
I0524 09:17:36.253240 11835 solver.cpp:239] Iteration 57640 (1.22075 iter/s, 8.19166s/10 iters), loss = 6.37773
I0524 09:17:36.253298 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37773 (* 1 = 6.37773 loss)
I0524 09:17:36.253397 11835 sgd_solver.cpp:112] Iteration 57640, lr = 0.1
I0524 09:17:42.837481 11835 solver.cpp:239] Iteration 57650 (1.51885 iter/s, 6.58392s/10 iters), loss = 6.38069
I0524 09:17:42.837674 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.38069 (* 1 = 6.38069 loss)
I0524 09:17:42.837762 11835 sgd_solver.cpp:112] Iteration 57650, lr = 0.1
I0524 09:17:49.874794 11835 solver.cpp:239] Iteration 57660 (1.42109 iter/s, 7.03686s/10 iters), loss = 5.32951
I0524 09:17:49.874866 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.32951 (* 1 = 5.32951 loss)
I0524 09:17:50.305347 11835 sgd_solver.cpp:112] Iteration 57660, lr = 0.1
I0524 09:17:58.733037 11835 solver.cpp:239] Iteration 57670 (1.12895 iter/s, 8.85782s/10 iters), loss = 5.78899
I0524 09:17:58.733100 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78899 (* 1 = 5.78899 loss)
I0524 09:17:58.733290 11835 sgd_solver.cpp:112] Iteration 57670, lr = 0.1
I0524 09:18:07.052734 11835 solver.cpp:239] Iteration 57680 (1.20202 iter/s, 8.31932s/10 iters), loss = 6.17565
I0524 09:18:07.052803 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17565 (* 1 = 6.17565 loss)
I0524 09:18:07.053009 11835 sgd_solver.cpp:112] Iteration 57680, lr = 0.1
I0524 09:18:15.799127 11835 solver.cpp:239] Iteration 57690 (1.14338 iter/s, 8.746s/10 iters), loss = 5.99908
I0524 09:18:15.799453 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99908 (* 1 = 5.99908 loss)
I0524 09:18:16.227751 11835 sgd_solver.cpp:112] Iteration 57690, lr = 0.1
I0524 09:18:25.698549 11835 solver.cpp:239] Iteration 57700 (1.01023 iter/s, 9.89876s/10 iters), loss = 5.92536
I0524 09:18:25.698602 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92536 (* 1 = 5.92536 loss)
I0524 09:18:25.698796 11835 sgd_solver.cpp:112] Iteration 57700, lr = 0.1
I0524 09:18:31.361999 11835 solver.cpp:239] Iteration 57710 (1.76579 iter/s, 5.66319s/10 iters), loss = 6.5598
I0524 09:18:31.362038 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5598 (* 1 = 6.5598 loss)
I0524 09:18:31.362053 11835 sgd_solver.cpp:112] Iteration 57710, lr = 0.1
I0524 09:18:37.280809 11835 solver.cpp:239] Iteration 57720 (1.68961 iter/s, 5.91852s/10 iters), loss = 7.01864
I0524 09:18:37.280869 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01864 (* 1 = 7.01864 loss)
I0524 09:18:37.372159 11835 sgd_solver.cpp:112] Iteration 57720, lr = 0.1
I0524 09:18:43.144697 11835 solver.cpp:239] Iteration 57730 (1.70543 iter/s, 5.86361s/10 iters), loss = 5.93457
I0524 09:18:43.144742 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93457 (* 1 = 5.93457 loss)
I0524 09:18:43.144757 11835 sgd_solver.cpp:112] Iteration 57730, lr = 0.1
I0524 09:18:52.562461 11835 solver.cpp:239] Iteration 57740 (1.06211 iter/s, 9.41519s/10 iters), loss = 6.21651
I0524 09:18:52.562753 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21651 (* 1 = 6.21651 loss)
I0524 09:18:52.562782 11835 sgd_solver.cpp:112] Iteration 57740, lr = 0.1
I0524 09:18:58.464613 11835 solver.cpp:239] Iteration 57750 (1.69444 iter/s, 5.90166s/10 iters), loss = 5.8683
I0524 09:18:58.464666 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8683 (* 1 = 5.8683 loss)
I0524 09:18:58.464907 11835 sgd_solver.cpp:112] Iteration 57750, lr = 0.1
I0524 09:19:04.465703 11835 solver.cpp:239] Iteration 57760 (1.66644 iter/s, 6.0008s/10 iters), loss = 5.85171
I0524 09:19:04.465750 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85171 (* 1 = 5.85171 loss)
I0524 09:19:04.649345 11835 sgd_solver.cpp:112] Iteration 57760, lr = 0.1
I0524 09:19:12.332026 11835 solver.cpp:239] Iteration 57770 (1.2713 iter/s, 7.86597s/10 iters), loss = 5.34204
I0524 09:19:12.332077 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.34204 (* 1 = 5.34204 loss)
I0524 09:19:13.120751 11835 sgd_solver.cpp:112] Iteration 57770, lr = 0.1
I0524 09:19:24.091344 11835 solver.cpp:239] Iteration 57780 (0.850425 iter/s, 11.7588s/10 iters), loss = 5.82753
I0524 09:19:24.091565 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82753 (* 1 = 5.82753 loss)
I0524 09:19:24.635493 11835 sgd_solver.cpp:112] Iteration 57780, lr = 0.1
I0524 09:19:30.676340 11835 solver.cpp:239] Iteration 57790 (1.51871 iter/s, 6.58455s/10 iters), loss = 6.00517
I0524 09:19:30.676388 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00517 (* 1 = 6.00517 loss)
I0524 09:19:30.676620 11835 sgd_solver.cpp:112] Iteration 57790, lr = 0.1
I0524 09:19:36.200043 11835 solver.cpp:239] Iteration 57800 (1.81047 iter/s, 5.52343s/10 iters), loss = 5.67841
I0524 09:19:36.200100 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67841 (* 1 = 5.67841 loss)
I0524 09:19:36.200376 11835 sgd_solver.cpp:112] Iteration 57800, lr = 0.1
I0524 09:19:43.581689 11835 solver.cpp:239] Iteration 57810 (1.35478 iter/s, 7.38129s/10 iters), loss = 5.72873
I0524 09:19:43.581759 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72873 (* 1 = 5.72873 loss)
I0524 09:19:43.581828 11835 sgd_solver.cpp:112] Iteration 57810, lr = 0.1
I0524 09:19:52.041854 11835 solver.cpp:239] Iteration 57820 (1.18207 iter/s, 8.45976s/10 iters), loss = 5.60549
I0524 09:19:52.041934 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.60549 (* 1 = 5.60549 loss)
I0524 09:19:52.042021 11835 sgd_solver.cpp:112] Iteration 57820, lr = 0.1
I0524 09:19:58.991835 11835 solver.cpp:239] Iteration 57830 (1.43892 iter/s, 6.94965s/10 iters), loss = 5.61272
I0524 09:19:58.992065 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.61272 (* 1 = 5.61272 loss)
I0524 09:19:59.037173 11835 sgd_solver.cpp:112] Iteration 57830, lr = 0.1
I0524 09:20:04.870574 11835 solver.cpp:239] Iteration 57840 (1.70118 iter/s, 5.87828s/10 iters), loss = 6.51528
I0524 09:20:04.870626 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.51528 (* 1 = 6.51528 loss)
I0524 09:20:05.037264 11835 sgd_solver.cpp:112] Iteration 57840, lr = 0.1
I0524 09:20:11.896445 11835 solver.cpp:239] Iteration 57850 (1.42338 iter/s, 7.02555s/10 iters), loss = 6.81603
I0524 09:20:11.896490 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.81603 (* 1 = 6.81603 loss)
I0524 09:20:11.905122 11835 sgd_solver.cpp:112] Iteration 57850, lr = 0.1
I0524 09:20:21.031762 11835 solver.cpp:239] Iteration 57860 (1.0947 iter/s, 9.13492s/10 iters), loss = 6.65016
I0524 09:20:21.031814 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65016 (* 1 = 6.65016 loss)
I0524 09:20:21.684242 11835 sgd_solver.cpp:112] Iteration 57860, lr = 0.1
I0524 09:20:29.316895 11835 solver.cpp:239] Iteration 57870 (1.20704 iter/s, 8.28475s/10 iters), loss = 5.63177
I0524 09:20:29.317143 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63177 (* 1 = 5.63177 loss)
I0524 09:20:30.062135 11835 sgd_solver.cpp:112] Iteration 57870, lr = 0.1
I0524 09:20:39.274626 11835 solver.cpp:239] Iteration 57880 (1.00431 iter/s, 9.95707s/10 iters), loss = 7.21161
I0524 09:20:39.274797 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.21161 (* 1 = 7.21161 loss)
I0524 09:20:39.275046 11835 sgd_solver.cpp:112] Iteration 57880, lr = 0.1
I0524 09:20:46.723083 11835 solver.cpp:239] Iteration 57890 (1.34263 iter/s, 7.4481s/10 iters), loss = 4.68689
I0524 09:20:46.723124 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.68689 (* 1 = 4.68689 loss)
I0524 09:20:47.683121 11835 sgd_solver.cpp:112] Iteration 57890, lr = 0.1
I0524 09:20:53.573794 11835 solver.cpp:239] Iteration 57900 (1.45977 iter/s, 6.85039s/10 iters), loss = 6.11191
I0524 09:20:53.573856 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11191 (* 1 = 6.11191 loss)
I0524 09:20:53.574105 11835 sgd_solver.cpp:112] Iteration 57900, lr = 0.1
I0524 09:21:01.130556 11835 solver.cpp:239] Iteration 57910 (1.32338 iter/s, 7.55639s/10 iters), loss = 5.9925
I0524 09:21:01.130846 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9925 (* 1 = 5.9925 loss)
I0524 09:21:01.130869 11835 sgd_solver.cpp:112] Iteration 57910, lr = 0.1
I0524 09:21:09.488876 11835 solver.cpp:239] Iteration 57920 (1.19654 iter/s, 8.35743s/10 iters), loss = 6.2459
I0524 09:21:09.488920 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2459 (* 1 = 6.2459 loss)
I0524 09:21:09.499510 11835 sgd_solver.cpp:112] Iteration 57920, lr = 0.1
I0524 09:21:17.789538 11835 solver.cpp:239] Iteration 57930 (1.20478 iter/s, 8.30029s/10 iters), loss = 6.66386
I0524 09:21:17.789592 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66386 (* 1 = 6.66386 loss)
I0524 09:21:17.958828 11835 sgd_solver.cpp:112] Iteration 57930, lr = 0.1
I0524 09:21:24.868993 11835 solver.cpp:239] Iteration 57940 (1.41261 iter/s, 7.0791s/10 iters), loss = 6.63231
I0524 09:21:24.869081 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.63231 (* 1 = 6.63231 loss)
I0524 09:21:24.869120 11835 sgd_solver.cpp:112] Iteration 57940, lr = 0.1
I0524 09:21:31.022972 11835 solver.cpp:239] Iteration 57950 (1.62505 iter/s, 6.15367s/10 iters), loss = 6.24151
I0524 09:21:31.023017 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.24151 (* 1 = 6.24151 loss)
I0524 09:21:31.023032 11835 sgd_solver.cpp:112] Iteration 57950, lr = 0.1
I0524 09:21:38.464527 11835 solver.cpp:239] Iteration 57960 (1.34387 iter/s, 7.44122s/10 iters), loss = 5.92808
I0524 09:21:38.464813 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92808 (* 1 = 5.92808 loss)
I0524 09:21:38.464860 11835 sgd_solver.cpp:112] Iteration 57960, lr = 0.1
I0524 09:21:45.814889 11835 solver.cpp:239] Iteration 57970 (1.36078 iter/s, 7.34872s/10 iters), loss = 5.91811
I0524 09:21:45.814940 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91811 (* 1 = 5.91811 loss)
I0524 09:21:45.833166 11835 sgd_solver.cpp:112] Iteration 57970, lr = 0.1
I0524 09:21:52.077026 11835 solver.cpp:239] Iteration 57980 (1.59698 iter/s, 6.26182s/10 iters), loss = 7.27506
I0524 09:21:52.077088 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.27506 (* 1 = 7.27506 loss)
I0524 09:21:52.733105 11835 sgd_solver.cpp:112] Iteration 57980, lr = 0.1
I0524 09:22:00.055124 11835 solver.cpp:239] Iteration 57990 (1.25349 iter/s, 7.97773s/10 iters), loss = 5.39708
I0524 09:22:00.055178 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.39708 (* 1 = 5.39708 loss)
I0524 09:22:00.069618 11835 sgd_solver.cpp:112] Iteration 57990, lr = 0.1
I0524 09:22:06.870378 11835 solver.cpp:239] Iteration 58000 (1.46736 iter/s, 6.81495s/10 iters), loss = 6.16919
I0524 09:22:06.870414 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16919 (* 1 = 6.16919 loss)
I0524 09:22:06.870646 11835 sgd_solver.cpp:112] Iteration 58000, lr = 0.1
I0524 09:22:14.400842 11835 solver.cpp:239] Iteration 58010 (1.328 iter/s, 7.53012s/10 iters), loss = 5.88776
I0524 09:22:14.406807 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88776 (* 1 = 5.88776 loss)
I0524 09:22:15.059486 11835 sgd_solver.cpp:112] Iteration 58010, lr = 0.1
I0524 09:22:22.766891 11835 solver.cpp:239] Iteration 58020 (1.19648 iter/s, 8.35783s/10 iters), loss = 6.08976
I0524 09:22:22.766947 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.08976 (* 1 = 6.08976 loss)
I0524 09:22:22.767194 11835 sgd_solver.cpp:112] Iteration 58020, lr = 0.1
I0524 09:22:30.076387 11835 solver.cpp:239] Iteration 58030 (1.36815 iter/s, 7.30915s/10 iters), loss = 6.54946
I0524 09:22:30.076465 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54946 (* 1 = 6.54946 loss)
I0524 09:22:30.076687 11835 sgd_solver.cpp:112] Iteration 58030, lr = 0.1
I0524 09:22:39.180793 11835 solver.cpp:239] Iteration 58040 (1.09842 iter/s, 9.104s/10 iters), loss = 6.25278
I0524 09:22:39.180845 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25278 (* 1 = 6.25278 loss)
I0524 09:22:39.351160 11835 sgd_solver.cpp:112] Iteration 58040, lr = 0.1
I0524 09:22:46.328218 11835 solver.cpp:239] Iteration 58050 (1.39917 iter/s, 7.1471s/10 iters), loss = 5.94945
I0524 09:22:46.328424 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94945 (* 1 = 5.94945 loss)
I0524 09:22:46.722677 11835 sgd_solver.cpp:112] Iteration 58050, lr = 0.1
I0524 09:22:53.310884 11835 solver.cpp:239] Iteration 58060 (1.43222 iter/s, 6.98215s/10 iters), loss = 6.95594
I0524 09:22:53.310950 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.95594 (* 1 = 6.95594 loss)
I0524 09:22:53.311162 11835 sgd_solver.cpp:112] Iteration 58060, lr = 0.1
I0524 09:22:59.562957 11835 solver.cpp:239] Iteration 58070 (1.59955 iter/s, 6.25178s/10 iters), loss = 5.47158
I0524 09:22:59.562995 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.47158 (* 1 = 5.47158 loss)
I0524 09:22:59.563009 11835 sgd_solver.cpp:112] Iteration 58070, lr = 0.1
I0524 09:23:06.739236 11835 solver.cpp:239] Iteration 58080 (1.39356 iter/s, 7.17589s/10 iters), loss = 6.89429
I0524 09:23:06.739290 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89429 (* 1 = 6.89429 loss)
I0524 09:23:06.739307 11835 sgd_solver.cpp:112] Iteration 58080, lr = 0.1
I0524 09:23:13.613339 11835 solver.cpp:239] Iteration 58090 (1.4548 iter/s, 6.87378s/10 iters), loss = 6.3816
I0524 09:23:13.613394 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3816 (* 1 = 6.3816 loss)
I0524 09:23:13.613720 11835 sgd_solver.cpp:112] Iteration 58090, lr = 0.1
I0524 09:23:20.945104 11835 solver.cpp:239] Iteration 58100 (1.36399 iter/s, 7.33142s/10 iters), loss = 5.85165
I0524 09:23:20.945382 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85165 (* 1 = 5.85165 loss)
I0524 09:23:20.945425 11835 sgd_solver.cpp:112] Iteration 58100, lr = 0.1
I0524 09:23:27.895458 11835 solver.cpp:239] Iteration 58110 (1.43891 iter/s, 6.94972s/10 iters), loss = 6.11497
I0524 09:23:27.895506 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11497 (* 1 = 6.11497 loss)
I0524 09:23:28.469857 11835 sgd_solver.cpp:112] Iteration 58110, lr = 0.1
I0524 09:23:34.713011 11835 solver.cpp:239] Iteration 58120 (1.46687 iter/s, 6.81724s/10 iters), loss = 5.8644
I0524 09:23:34.713069 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8644 (* 1 = 5.8644 loss)
I0524 09:23:34.715025 11835 sgd_solver.cpp:112] Iteration 58120, lr = 0.1
I0524 09:23:41.179280 11835 solver.cpp:239] Iteration 58130 (1.54656 iter/s, 6.46597s/10 iters), loss = 5.71239
I0524 09:23:41.179322 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71239 (* 1 = 5.71239 loss)
I0524 09:23:41.179334 11835 sgd_solver.cpp:112] Iteration 58130, lr = 0.1
I0524 09:23:49.054554 11835 solver.cpp:239] Iteration 58140 (1.26991 iter/s, 7.8746s/10 iters), loss = 6.0553
I0524 09:23:49.054605 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0553 (* 1 = 6.0553 loss)
I0524 09:23:49.249810 11835 sgd_solver.cpp:112] Iteration 58140, lr = 0.1
I0524 09:23:56.118785 11835 solver.cpp:239] Iteration 58150 (1.41565 iter/s, 7.06392s/10 iters), loss = 6.69316
I0524 09:23:56.118984 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69316 (* 1 = 6.69316 loss)
I0524 09:23:56.119000 11835 sgd_solver.cpp:112] Iteration 58150, lr = 0.1
I0524 09:24:03.341807 11835 solver.cpp:239] Iteration 58160 (1.38474 iter/s, 7.22158s/10 iters), loss = 5.39599
I0524 09:24:03.341859 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.39599 (* 1 = 5.39599 loss)
I0524 09:24:03.341925 11835 sgd_solver.cpp:112] Iteration 58160, lr = 0.1
I0524 09:24:12.919783 11835 solver.cpp:239] Iteration 58170 (1.04411 iter/s, 9.57756s/10 iters), loss = 5.71579
I0524 09:24:12.919831 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.71579 (* 1 = 5.71579 loss)
I0524 09:24:12.919843 11835 sgd_solver.cpp:112] Iteration 58170, lr = 0.1
I0524 09:24:19.889487 11835 solver.cpp:239] Iteration 58180 (1.43508 iter/s, 6.96827s/10 iters), loss = 6.3436
I0524 09:24:19.889536 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3436 (* 1 = 6.3436 loss)
I0524 09:24:19.890684 11835 sgd_solver.cpp:112] Iteration 58180, lr = 0.1
I0524 09:24:27.882839 11835 solver.cpp:239] Iteration 58190 (1.2511 iter/s, 7.993s/10 iters), loss = 6.82116
I0524 09:24:27.883111 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.82116 (* 1 = 6.82116 loss)
I0524 09:24:27.883160 11835 sgd_solver.cpp:112] Iteration 58190, lr = 0.1
I0524 09:24:37.244045 11835 solver.cpp:239] Iteration 58200 (1.06833 iter/s, 9.36038s/10 iters), loss = 5.83176
I0524 09:24:37.244103 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83176 (* 1 = 5.83176 loss)
I0524 09:24:37.358294 11835 sgd_solver.cpp:112] Iteration 58200, lr = 0.1
I0524 09:24:44.027634 11835 solver.cpp:239] Iteration 58210 (1.47422 iter/s, 6.78326s/10 iters), loss = 5.75811
I0524 09:24:44.027686 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75811 (* 1 = 5.75811 loss)
I0524 09:24:44.028084 11835 sgd_solver.cpp:112] Iteration 58210, lr = 0.1
I0524 09:24:50.295424 11835 solver.cpp:239] Iteration 58220 (1.59553 iter/s, 6.2675s/10 iters), loss = 4.77201
I0524 09:24:50.295467 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.77201 (* 1 = 4.77201 loss)
I0524 09:24:50.295480 11835 sgd_solver.cpp:112] Iteration 58220, lr = 0.1
I0524 09:24:58.649153 11835 solver.cpp:239] Iteration 58230 (1.19716 iter/s, 8.35309s/10 iters), loss = 6.5954
I0524 09:24:58.649376 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5954 (* 1 = 6.5954 loss)
I0524 09:24:59.294224 11835 sgd_solver.cpp:112] Iteration 58230, lr = 0.1
I0524 09:25:06.795766 11835 solver.cpp:239] Iteration 58240 (1.22758 iter/s, 8.1461s/10 iters), loss = 5.72502
I0524 09:25:06.795814 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72502 (* 1 = 5.72502 loss)
I0524 09:25:06.836022 11835 sgd_solver.cpp:112] Iteration 58240, lr = 0.1
I0524 09:25:14.824079 11835 solver.cpp:239] Iteration 58250 (1.24565 iter/s, 8.02795s/10 iters), loss = 6.19593
I0524 09:25:14.824131 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19593 (* 1 = 6.19593 loss)
I0524 09:25:14.824223 11835 sgd_solver.cpp:112] Iteration 58250, lr = 0.1
I0524 09:25:23.102995 11835 solver.cpp:239] Iteration 58260 (1.20794 iter/s, 8.27854s/10 iters), loss = 5.88714
I0524 09:25:23.103051 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88714 (* 1 = 5.88714 loss)
I0524 09:25:23.103088 11835 sgd_solver.cpp:112] Iteration 58260, lr = 0.1
I0524 09:25:29.048558 11835 solver.cpp:239] Iteration 58270 (1.68201 iter/s, 5.94528s/10 iters), loss = 5.67424
I0524 09:25:29.048720 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67424 (* 1 = 5.67424 loss)
I0524 09:25:29.048756 11835 sgd_solver.cpp:112] Iteration 58270, lr = 0.1
I0524 09:25:35.504676 11835 solver.cpp:239] Iteration 58280 (1.54905 iter/s, 6.45557s/10 iters), loss = 6.85586
I0524 09:25:35.504745 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.85586 (* 1 = 6.85586 loss)
I0524 09:25:35.504957 11835 sgd_solver.cpp:112] Iteration 58280, lr = 0.1
I0524 09:25:42.075968 11835 solver.cpp:239] Iteration 58290 (1.52184 iter/s, 6.57098s/10 iters), loss = 6.09201
I0524 09:25:42.076016 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09201 (* 1 = 6.09201 loss)
I0524 09:25:42.076112 11835 sgd_solver.cpp:112] Iteration 58290, lr = 0.1
I0524 09:25:47.919667 11835 solver.cpp:239] Iteration 58300 (1.71133 iter/s, 5.84341s/10 iters), loss = 5.75061
I0524 09:25:47.919723 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.75061 (* 1 = 5.75061 loss)
I0524 09:25:47.919762 11835 sgd_solver.cpp:112] Iteration 58300, lr = 0.1
I0524 09:25:54.399648 11835 solver.cpp:239] Iteration 58310 (1.54329 iter/s, 6.47967s/10 iters), loss = 6.46658
I0524 09:25:54.399716 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46658 (* 1 = 6.46658 loss)
I0524 09:25:54.404554 11835 sgd_solver.cpp:112] Iteration 58310, lr = 0.1
I0524 09:26:02.550194 11835 solver.cpp:239] Iteration 58320 (1.22697 iter/s, 8.15016s/10 iters), loss = 5.67635
I0524 09:26:02.550449 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67635 (* 1 = 5.67635 loss)
I0524 09:26:02.550498 11835 sgd_solver.cpp:112] Iteration 58320, lr = 0.1
I0524 09:26:08.630566 11835 solver.cpp:239] Iteration 58330 (1.64502 iter/s, 6.07897s/10 iters), loss = 6.04228
I0524 09:26:08.630617 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04228 (* 1 = 6.04228 loss)
I0524 09:26:08.630633 11835 sgd_solver.cpp:112] Iteration 58330, lr = 0.1
I0524 09:26:16.220086 11835 solver.cpp:239] Iteration 58340 (1.31768 iter/s, 7.58912s/10 iters), loss = 6.14422
I0524 09:26:16.220130 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14422 (* 1 = 6.14422 loss)
I0524 09:26:16.220254 11835 sgd_solver.cpp:112] Iteration 58340, lr = 0.1
I0524 09:26:22.532040 11835 solver.cpp:239] Iteration 58350 (1.58437 iter/s, 6.31166s/10 iters), loss = 7.77551
I0524 09:26:22.532099 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.77551 (* 1 = 7.77551 loss)
I0524 09:26:23.623848 11835 sgd_solver.cpp:112] Iteration 58350, lr = 0.1
I0524 09:26:31.309234 11835 solver.cpp:239] Iteration 58360 (1.13937 iter/s, 8.77679s/10 iters), loss = 6.33935
I0524 09:26:31.309294 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33935 (* 1 = 6.33935 loss)
I0524 09:26:31.309310 11835 sgd_solver.cpp:112] Iteration 58360, lr = 0.1
I0524 09:26:37.454985 11835 solver.cpp:239] Iteration 58370 (1.62722 iter/s, 6.14543s/10 iters), loss = 6.61009
I0524 09:26:37.455361 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61009 (* 1 = 6.61009 loss)
I0524 09:26:37.455384 11835 sgd_solver.cpp:112] Iteration 58370, lr = 0.1
I0524 09:26:46.703056 11835 solver.cpp:239] Iteration 58380 (1.08139 iter/s, 9.24734s/10 iters), loss = 5.88153
I0524 09:26:46.703106 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.88153 (* 1 = 5.88153 loss)
I0524 09:26:47.021397 11835 sgd_solver.cpp:112] Iteration 58380, lr = 0.1
I0524 09:26:53.933236 11835 solver.cpp:239] Iteration 58390 (1.38316 iter/s, 7.22983s/10 iters), loss = 6.84016
I0524 09:26:53.933295 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84016 (* 1 = 6.84016 loss)
I0524 09:26:54.514003 11835 sgd_solver.cpp:112] Iteration 58390, lr = 0.1
I0524 09:27:01.553797 11835 solver.cpp:239] Iteration 58400 (1.3123 iter/s, 7.62021s/10 iters), loss = 5.58012
I0524 09:27:01.553850 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.58012 (* 1 = 5.58012 loss)
I0524 09:27:01.599225 11835 sgd_solver.cpp:112] Iteration 58400, lr = 0.1
I0524 09:27:07.493265 11835 solver.cpp:239] Iteration 58410 (1.68373 iter/s, 5.93918s/10 iters), loss = 5.23847
I0524 09:27:07.493544 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.23847 (* 1 = 5.23847 loss)
I0524 09:27:07.493590 11835 sgd_solver.cpp:112] Iteration 58410, lr = 0.1
I0524 09:27:14.138782 11835 solver.cpp:239] Iteration 58420 (1.50497 iter/s, 6.64466s/10 iters), loss = 7.04693
I0524 09:27:14.138828 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.04693 (* 1 = 7.04693 loss)
I0524 09:27:14.138840 11835 sgd_solver.cpp:112] Iteration 58420, lr = 0.1
I0524 09:27:23.780853 11835 solver.cpp:239] Iteration 58430 (1.03728 iter/s, 9.64055s/10 iters), loss = 5.63393
I0524 09:27:23.780908 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.63393 (* 1 = 5.63393 loss)
I0524 09:27:24.100920 11835 sgd_solver.cpp:112] Iteration 58430, lr = 0.1
I0524 09:27:30.954645 11835 solver.cpp:239] Iteration 58440 (1.39403 iter/s, 7.17346s/10 iters), loss = 6.2053
I0524 09:27:30.954730 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2053 (* 1 = 6.2053 loss)
I0524 09:27:30.977705 11835 sgd_solver.cpp:112] Iteration 58440, lr = 0.1
I0524 09:27:37.824036 11835 solver.cpp:239] Iteration 58450 (1.4558 iter/s, 6.86907s/10 iters), loss = 7.33103
I0524 09:27:37.824239 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.33103 (* 1 = 7.33103 loss)
I0524 09:27:39.185994 11835 sgd_solver.cpp:112] Iteration 58450, lr = 0.1
I0524 09:27:47.080904 11835 solver.cpp:239] Iteration 58460 (1.08035 iter/s, 9.25628s/10 iters), loss = 5.8364
I0524 09:27:47.080977 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8364 (* 1 = 5.8364 loss)
I0524 09:27:47.081095 11835 sgd_solver.cpp:112] Iteration 58460, lr = 0.1
I0524 09:27:54.357959 11835 solver.cpp:239] Iteration 58470 (1.37425 iter/s, 7.27671s/10 iters), loss = 6.34656
I0524 09:27:54.358005 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34656 (* 1 = 6.34656 loss)
I0524 09:27:54.358201 11835 sgd_solver.cpp:112] Iteration 58470, lr = 0.1
I0524 09:28:00.826766 11835 solver.cpp:239] Iteration 58480 (1.54595 iter/s, 6.4685s/10 iters), loss = 5.44048
I0524 09:28:00.826820 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.44048 (* 1 = 5.44048 loss)
I0524 09:28:01.303596 11835 sgd_solver.cpp:112] Iteration 58480, lr = 0.1
I0524 09:28:08.785706 11835 solver.cpp:239] Iteration 58490 (1.25651 iter/s, 7.95858s/10 iters), loss = 5.35823
I0524 09:28:08.785866 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.35823 (* 1 = 5.35823 loss)
I0524 09:28:09.247167 11835 sgd_solver.cpp:112] Iteration 58490, lr = 0.1
I0524 09:28:15.564013 11835 solver.cpp:239] Iteration 58500 (1.47539 iter/s, 6.77788s/10 iters), loss = 6.41241
I0524 09:28:15.564069 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41241 (* 1 = 6.41241 loss)
I0524 09:28:15.622118 11835 sgd_solver.cpp:112] Iteration 58500, lr = 0.1
I0524 09:28:23.439847 11835 solver.cpp:239] Iteration 58510 (1.26977 iter/s, 7.87547s/10 iters), loss = 6.75648
I0524 09:28:23.439898 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.75648 (* 1 = 6.75648 loss)
I0524 09:28:23.439913 11835 sgd_solver.cpp:112] Iteration 58510, lr = 0.1
I0524 09:28:30.864518 11835 solver.cpp:239] Iteration 58520 (1.34692 iter/s, 7.42434s/10 iters), loss = 5.28314
I0524 09:28:30.864570 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.28314 (* 1 = 5.28314 loss)
I0524 09:28:30.864624 11835 sgd_solver.cpp:112] Iteration 58520, lr = 0.1
I0524 09:28:37.171885 11835 solver.cpp:239] Iteration 58530 (1.58552 iter/s, 6.30707s/10 iters), loss = 6.23006
I0524 09:28:37.171947 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.23006 (* 1 = 6.23006 loss)
I0524 09:28:37.171960 11835 sgd_solver.cpp:112] Iteration 58530, lr = 0.1
I0524 09:28:44.165103 11835 solver.cpp:239] Iteration 58540 (1.43004 iter/s, 6.99282s/10 iters), loss = 5.68459
I0524 09:28:44.165426 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68459 (* 1 = 5.68459 loss)
I0524 09:28:44.290125 11835 sgd_solver.cpp:112] Iteration 58540, lr = 0.1
I0524 09:28:50.691331 11835 solver.cpp:239] Iteration 58550 (1.5324 iter/s, 6.52572s/10 iters), loss = 6.11819
I0524 09:28:50.691376 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11819 (* 1 = 6.11819 loss)
I0524 09:28:50.691390 11835 sgd_solver.cpp:112] Iteration 58550, lr = 0.1
I0524 09:28:57.817843 11835 solver.cpp:239] Iteration 58560 (1.40329 iter/s, 7.12611s/10 iters), loss = 5.90112
I0524 09:28:57.817896 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90112 (* 1 = 5.90112 loss)
I0524 09:28:57.818013 11835 sgd_solver.cpp:112] Iteration 58560, lr = 0.1
I0524 09:29:04.130729 11835 solver.cpp:239] Iteration 58570 (1.58414 iter/s, 6.31256s/10 iters), loss = 5.18776
I0524 09:29:04.130790 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.18776 (* 1 = 5.18776 loss)
I0524 09:29:04.130969 11835 sgd_solver.cpp:112] Iteration 58570, lr = 0.1
I0524 09:29:12.145980 11835 solver.cpp:239] Iteration 58580 (1.24768 iter/s, 8.01488s/10 iters), loss = 6.04305
I0524 09:29:12.146086 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04305 (* 1 = 6.04305 loss)
I0524 09:29:12.146340 11835 sgd_solver.cpp:112] Iteration 58580, lr = 0.1
I0524 09:29:18.977320 11835 solver.cpp:239] Iteration 58590 (1.46391 iter/s, 6.83101s/10 iters), loss = 5.65428
I0524 09:29:18.977561 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.65428 (* 1 = 5.65428 loss)
I0524 09:29:18.977608 11835 sgd_solver.cpp:112] Iteration 58590, lr = 0.1
I0524 09:29:28.115375 11835 solver.cpp:239] Iteration 58600 (1.09439 iter/s, 9.13748s/10 iters), loss = 6.60242
I0524 09:29:28.115429 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60242 (* 1 = 6.60242 loss)
I0524 09:29:28.145802 11835 sgd_solver.cpp:112] Iteration 58600, lr = 0.1
I0524 09:29:34.841125 11835 solver.cpp:239] Iteration 58610 (1.48689 iter/s, 6.72543s/10 iters), loss = 5.64675
I0524 09:29:34.841192 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.64675 (* 1 = 5.64675 loss)
I0524 09:29:34.841297 11835 sgd_solver.cpp:112] Iteration 58610, lr = 0.1
I0524 09:29:41.815591 11835 solver.cpp:239] Iteration 58620 (1.43387 iter/s, 6.97415s/10 iters), loss = 5.56193
I0524 09:29:41.815632 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56193 (* 1 = 5.56193 loss)
I0524 09:29:41.815644 11835 sgd_solver.cpp:112] Iteration 58620, lr = 0.1
I0524 09:29:50.402068 11835 solver.cpp:239] Iteration 58630 (1.16468 iter/s, 8.58609s/10 iters), loss = 6.25356
I0524 09:29:50.402329 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25356 (* 1 = 6.25356 loss)
I0524 09:29:50.402369 11835 sgd_solver.cpp:112] Iteration 58630, lr = 0.1
I0524 09:29:57.424255 11835 solver.cpp:239] Iteration 58640 (1.42416 iter/s, 7.02169s/10 iters), loss = 6.8724
I0524 09:29:57.424386 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8724 (* 1 = 6.8724 loss)
I0524 09:29:57.698819 11835 sgd_solver.cpp:112] Iteration 58640, lr = 0.1
I0524 09:30:04.011274 11835 solver.cpp:239] Iteration 58650 (1.51822 iter/s, 6.58667s/10 iters), loss = 5.39171
I0524 09:30:04.011342 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.39171 (* 1 = 5.39171 loss)
I0524 09:30:04.011616 11835 sgd_solver.cpp:112] Iteration 58650, lr = 0.1
I0524 09:30:09.929847 11835 solver.cpp:239] Iteration 58660 (1.68968 iter/s, 5.91829s/10 iters), loss = 5.09139
I0524 09:30:09.929903 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.09139 (* 1 = 5.09139 loss)
I0524 09:30:09.929917 11835 sgd_solver.cpp:112] Iteration 58660, lr = 0.1
I0524 09:30:16.660753 11835 solver.cpp:239] Iteration 58670 (1.48624 iter/s, 6.72837s/10 iters), loss = 6.21662
I0524 09:30:16.660861 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21662 (* 1 = 6.21662 loss)
I0524 09:30:16.875167 11835 sgd_solver.cpp:112] Iteration 58670, lr = 0.1
I0524 09:30:24.359668 11835 solver.cpp:239] Iteration 58680 (1.29894 iter/s, 7.69858s/10 iters), loss = 4.74439
I0524 09:30:24.359974 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.74439 (* 1 = 4.74439 loss)
I0524 09:30:24.360023 11835 sgd_solver.cpp:112] Iteration 58680, lr = 0.1
I0524 09:30:31.571979 11835 solver.cpp:239] Iteration 58690 (1.38664 iter/s, 7.21169s/10 iters), loss = 5.83598
I0524 09:30:31.572082 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.83598 (* 1 = 5.83598 loss)
I0524 09:30:31.572422 11835 sgd_solver.cpp:112] Iteration 58690, lr = 0.1
I0524 09:30:37.824353 11835 solver.cpp:239] Iteration 58700 (1.59947 iter/s, 6.25208s/10 iters), loss = 6.05697
I0524 09:30:37.824395 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05697 (* 1 = 6.05697 loss)
I0524 09:30:37.851660 11835 sgd_solver.cpp:112] Iteration 58700, lr = 0.1
I0524 09:30:44.353461 11835 solver.cpp:239] Iteration 58710 (1.53167 iter/s, 6.5288s/10 iters), loss = 6.75492
I0524 09:30:44.353529 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.75492 (* 1 = 6.75492 loss)
I0524 09:30:44.353587 11835 sgd_solver.cpp:112] Iteration 58710, lr = 0.1
I0524 09:30:50.802335 11835 solver.cpp:239] Iteration 58720 (1.55073 iter/s, 6.44856s/10 iters), loss = 6.44361
I0524 09:30:50.802392 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44361 (* 1 = 6.44361 loss)
I0524 09:30:50.802609 11835 sgd_solver.cpp:112] Iteration 58720, lr = 0.1
I0524 09:30:56.973341 11835 solver.cpp:239] Iteration 58730 (1.62056 iter/s, 6.17071s/10 iters), loss = 5.76255
I0524 09:30:56.973556 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.76255 (* 1 = 5.76255 loss)
I0524 09:30:56.973597 11835 sgd_solver.cpp:112] Iteration 58730, lr = 0.1
I0524 09:31:04.667162 11835 solver.cpp:239] Iteration 58740 (1.29983 iter/s, 7.69329s/10 iters), loss = 5.62485
I0524 09:31:04.667241 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.62485 (* 1 = 5.62485 loss)
I0524 09:31:04.667381 11835 sgd_solver.cpp:112] Iteration 58740, lr = 0.1
I0524 09:31:11.512563 11835 solver.cpp:239] Iteration 58750 (1.46091 iter/s, 6.84505s/10 iters), loss = 6.12515
I0524 09:31:11.512625 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12515 (* 1 = 6.12515 loss)
I0524 09:31:11.512655 11835 sgd_solver.cpp:112] Iteration 58750, lr = 0.1
I0524 09:31:17.831750 11835 solver.cpp:239] Iteration 58760 (1.58256 iter/s, 6.31889s/10 iters), loss = 6.22778
I0524 09:31:17.831804 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22778 (* 1 = 6.22778 loss)
I0524 09:31:17.846031 11835 sgd_solver.cpp:112] Iteration 58760, lr = 0.1
I0524 09:31:24.924440 11835 solver.cpp:239] Iteration 58770 (1.40997 iter/s, 7.09237s/10 iters), loss = 6.09092
I0524 09:31:24.924515 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09092 (* 1 = 6.09092 loss)
I0524 09:31:24.924533 11835 sgd_solver.cpp:112] Iteration 58770, lr = 0.1
I0524 09:31:31.814802 11835 solver.cpp:239] Iteration 58780 (1.45137 iter/s, 6.89002s/10 iters), loss = 6.01594
I0524 09:31:31.815219 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01594 (* 1 = 6.01594 loss)
I0524 09:31:31.815249 11835 sgd_solver.cpp:112] Iteration 58780, lr = 0.1
I0524 09:31:37.379695 11835 solver.cpp:239] Iteration 58790 (1.79719 iter/s, 5.56425s/10 iters), loss = 6.06301
I0524 09:31:37.379750 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06301 (* 1 = 6.06301 loss)
I0524 09:31:37.379796 11835 sgd_solver.cpp:112] Iteration 58790, lr = 0.1
I0524 09:31:45.165503 11835 solver.cpp:239] Iteration 58800 (1.28445 iter/s, 7.78545s/10 iters), loss = 5.48287
I0524 09:31:45.165560 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.48287 (* 1 = 5.48287 loss)
I0524 09:31:45.165712 11835 sgd_solver.cpp:112] Iteration 58800, lr = 0.1
I0524 09:31:52.558400 11835 solver.cpp:239] Iteration 58810 (1.35271 iter/s, 7.39256s/10 iters), loss = 5.76695
I0524 09:31:52.558455 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.76695 (* 1 = 5.76695 loss)
I0524 09:31:52.558637 11835 sgd_solver.cpp:112] Iteration 58810, lr = 0.1
I0524 09:31:59.128638 11835 solver.cpp:239] Iteration 58820 (1.52209 iter/s, 6.56992s/10 iters), loss = 5.91862
I0524 09:31:59.128695 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91862 (* 1 = 5.91862 loss)
I0524 09:31:59.128826 11835 sgd_solver.cpp:112] Iteration 58820, lr = 0.1
I0524 09:32:06.601101 11835 solver.cpp:239] Iteration 58830 (1.33831 iter/s, 7.47212s/10 iters), loss = 5.96125
I0524 09:32:06.601366 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96125 (* 1 = 5.96125 loss)
I0524 09:32:06.718168 11835 sgd_solver.cpp:112] Iteration 58830, lr = 0.1
I0524 09:32:15.564343 11835 solver.cpp:239] Iteration 58840 (1.11574 iter/s, 8.96266s/10 iters), loss = 6.12544
I0524 09:32:15.564393 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12544 (* 1 = 6.12544 loss)
I0524 09:32:16.014531 11835 sgd_solver.cpp:112] Iteration 58840, lr = 0.1
I0524 09:32:22.503710 11835 solver.cpp:239] Iteration 58850 (1.44112 iter/s, 6.93905s/10 iters), loss = 5.39913
I0524 09:32:22.503758 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.39913 (* 1 = 5.39913 loss)
I0524 09:32:22.503772 11835 sgd_solver.cpp:112] Iteration 58850, lr = 0.1
I0524 09:32:29.107975 11835 solver.cpp:239] Iteration 58860 (1.51424 iter/s, 6.60396s/10 iters), loss = 7.14502
I0524 09:32:29.108031 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.14502 (* 1 = 7.14502 loss)
I0524 09:32:30.001961 11835 sgd_solver.cpp:112] Iteration 58860, lr = 0.1
I0524 09:32:39.165451 11835 solver.cpp:239] Iteration 58870 (0.994332 iter/s, 10.057s/10 iters), loss = 5.81892
I0524 09:32:39.165688 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81892 (* 1 = 5.81892 loss)
I0524 09:32:39.499425 11835 sgd_solver.cpp:112] Iteration 58870, lr = 0.1
I0524 09:32:45.555418 11835 solver.cpp:239] Iteration 58880 (1.56507 iter/s, 6.3895s/10 iters), loss = 6.66563
I0524 09:32:45.555454 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.66563 (* 1 = 6.66563 loss)
I0524 09:32:46.541805 11835 sgd_solver.cpp:112] Iteration 58880, lr = 0.1
I0524 09:32:53.676230 11835 solver.cpp:239] Iteration 58890 (1.23146 iter/s, 8.12045s/10 iters), loss = 5.8217
I0524 09:32:53.676281 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8217 (* 1 = 5.8217 loss)
I0524 09:32:53.825716 11835 sgd_solver.cpp:112] Iteration 58890, lr = 0.1
I0524 09:33:00.207952 11835 solver.cpp:239] Iteration 58900 (1.53106 iter/s, 6.53143s/10 iters), loss = 5.78314
I0524 09:33:00.207993 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78314 (* 1 = 5.78314 loss)
I0524 09:33:00.416000 11835 sgd_solver.cpp:112] Iteration 58900, lr = 0.1
I0524 09:33:07.780812 11835 solver.cpp:239] Iteration 58910 (1.32056 iter/s, 7.57252s/10 iters), loss = 7.05414
I0524 09:33:07.780867 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.05414 (* 1 = 7.05414 loss)
I0524 09:33:07.969249 11835 sgd_solver.cpp:112] Iteration 58910, lr = 0.1
I0524 09:33:15.378192 11835 solver.cpp:239] Iteration 58920 (1.31631 iter/s, 7.59702s/10 iters), loss = 6.30075
I0524 09:33:15.378410 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30075 (* 1 = 6.30075 loss)
I0524 09:33:15.569020 11835 sgd_solver.cpp:112] Iteration 58920, lr = 0.1
I0524 09:33:23.338866 11835 solver.cpp:239] Iteration 58930 (1.25625 iter/s, 7.96018s/10 iters), loss = 5.41244
I0524 09:33:23.338912 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.41244 (* 1 = 5.41244 loss)
I0524 09:33:23.659046 11835 sgd_solver.cpp:112] Iteration 58930, lr = 0.1
I0524 09:33:29.878264 11835 solver.cpp:239] Iteration 58940 (1.52926 iter/s, 6.53909s/10 iters), loss = 6.42233
I0524 09:33:29.878319 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.42233 (* 1 = 6.42233 loss)
I0524 09:33:29.878451 11835 sgd_solver.cpp:112] Iteration 58940, lr = 0.1
I0524 09:33:36.276918 11835 solver.cpp:239] Iteration 58950 (1.5629 iter/s, 6.39835s/10 iters), loss = 5.97077
I0524 09:33:36.276973 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97077 (* 1 = 5.97077 loss)
I0524 09:33:36.277169 11835 sgd_solver.cpp:112] Iteration 58950, lr = 0.1
I0524 09:33:42.554733 11835 solver.cpp:239] Iteration 58960 (1.59299 iter/s, 6.2775s/10 iters), loss = 6.34742
I0524 09:33:42.554792 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34742 (* 1 = 6.34742 loss)
I0524 09:33:42.555034 11835 sgd_solver.cpp:112] Iteration 58960, lr = 0.1
I0524 09:33:49.588644 11835 solver.cpp:239] Iteration 58970 (1.42175 iter/s, 7.03359s/10 iters), loss = 5.59093
I0524 09:33:49.594810 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.59093 (* 1 = 5.59093 loss)
I0524 09:33:50.586194 11835 sgd_solver.cpp:112] Iteration 58970, lr = 0.1
I0524 09:33:58.658066 11835 solver.cpp:239] Iteration 58980 (1.1034 iter/s, 9.0629s/10 iters), loss = 5.26843
I0524 09:33:58.658198 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.26843 (* 1 = 5.26843 loss)
I0524 09:33:58.724498 11835 sgd_solver.cpp:112] Iteration 58980, lr = 0.1
I0524 09:34:06.152874 11835 solver.cpp:239] Iteration 58990 (1.33432 iter/s, 7.49444s/10 iters), loss = 6.15435
I0524 09:34:06.152912 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15435 (* 1 = 6.15435 loss)
I0524 09:34:06.293824 11835 sgd_solver.cpp:112] Iteration 58990, lr = 0.1
I0524 09:34:12.143838 11835 solver.cpp:239] Iteration 59000 (1.66926 iter/s, 5.99068s/10 iters), loss = 5.10548
I0524 09:34:12.143905 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.10548 (* 1 = 5.10548 loss)
I0524 09:34:12.167374 11835 sgd_solver.cpp:112] Iteration 59000, lr = 0.1
I0524 09:34:19.362126 11835 solver.cpp:239] Iteration 59010 (1.38544 iter/s, 7.21794s/10 iters), loss = 7.05603
I0524 09:34:19.362191 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.05603 (* 1 = 7.05603 loss)
I0524 09:34:19.362380 11835 sgd_solver.cpp:112] Iteration 59010, lr = 0.1
I0524 09:34:25.149462 11835 solver.cpp:239] Iteration 59020 (1.728 iter/s, 5.78705s/10 iters), loss = 5.96016
I0524 09:34:25.149703 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96016 (* 1 = 5.96016 loss)
I0524 09:34:25.149745 11835 sgd_solver.cpp:112] Iteration 59020, lr = 0.1
I0524 09:34:32.809973 11835 solver.cpp:239] Iteration 59030 (1.30549 iter/s, 7.65997s/10 iters), loss = 6.80725
I0524 09:34:32.810020 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.80725 (* 1 = 6.80725 loss)
I0524 09:34:32.810034 11835 sgd_solver.cpp:112] Iteration 59030, lr = 0.1
I0524 09:34:38.751684 11835 solver.cpp:239] Iteration 59040 (1.68311 iter/s, 5.94139s/10 iters), loss = 6.04173
I0524 09:34:38.751730 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04173 (* 1 = 6.04173 loss)
I0524 09:34:38.751859 11835 sgd_solver.cpp:112] Iteration 59040, lr = 0.1
I0524 09:34:44.832520 11835 solver.cpp:239] Iteration 59050 (1.64459 iter/s, 6.08054s/10 iters), loss = 6.65605
I0524 09:34:44.832572 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.65605 (* 1 = 6.65605 loss)
I0524 09:34:45.223942 11835 sgd_solver.cpp:112] Iteration 59050, lr = 0.1
I0524 09:34:54.329572 11835 solver.cpp:239] Iteration 59060 (1.05301 iter/s, 9.49661s/10 iters), loss = 6.58298
I0524 09:34:54.329658 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58298 (* 1 = 6.58298 loss)
I0524 09:34:54.329870 11835 sgd_solver.cpp:112] Iteration 59060, lr = 0.1
I0524 09:35:00.601768 11835 solver.cpp:239] Iteration 59070 (1.59442 iter/s, 6.27187s/10 iters), loss = 5.70982
I0524 09:35:00.602017 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.70982 (* 1 = 5.70982 loss)
I0524 09:35:00.602041 11835 sgd_solver.cpp:112] Iteration 59070, lr = 0.1
I0524 09:35:06.943572 11835 solver.cpp:239] Iteration 59080 (1.57699 iter/s, 6.34121s/10 iters), loss = 6.05759
I0524 09:35:06.943621 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05759 (* 1 = 6.05759 loss)
I0524 09:35:06.943636 11835 sgd_solver.cpp:112] Iteration 59080, lr = 0.1
I0524 09:35:14.931524 11835 solver.cpp:239] Iteration 59090 (1.25196 iter/s, 7.98747s/10 iters), loss = 5.9458
I0524 09:35:14.931610 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9458 (* 1 = 5.9458 loss)
I0524 09:35:15.552362 11835 sgd_solver.cpp:112] Iteration 59090, lr = 0.1
I0524 09:35:23.864346 11835 solver.cpp:239] Iteration 59100 (1.11952 iter/s, 8.9324s/10 iters), loss = 5.70988
I0524 09:35:23.864404 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.70988 (* 1 = 5.70988 loss)
I0524 09:35:23.864603 11835 sgd_solver.cpp:112] Iteration 59100, lr = 0.1
I0524 09:35:30.442865 11835 solver.cpp:239] Iteration 59110 (1.52018 iter/s, 6.57819s/10 iters), loss = 5.64619
I0524 09:35:30.442929 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.64619 (* 1 = 5.64619 loss)
I0524 09:35:31.227010 11835 sgd_solver.cpp:112] Iteration 59110, lr = 0.1
I0524 09:35:38.246549 11835 solver.cpp:239] Iteration 59120 (1.28151 iter/s, 7.80332s/10 iters), loss = 5.81448
I0524 09:35:38.246601 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81448 (* 1 = 5.81448 loss)
I0524 09:35:38.589113 11835 sgd_solver.cpp:112] Iteration 59120, lr = 0.1
I0524 09:35:49.358093 11835 solver.cpp:239] Iteration 59130 (0.900004 iter/s, 11.1111s/10 iters), loss = 5.53539
I0524 09:35:49.358150 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.53539 (* 1 = 5.53539 loss)
I0524 09:35:49.362442 11835 sgd_solver.cpp:112] Iteration 59130, lr = 0.1
I0524 09:35:57.477151 11835 solver.cpp:239] Iteration 59140 (1.23173 iter/s, 8.11869s/10 iters), loss = 5.79584
I0524 09:35:57.477231 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79584 (* 1 = 5.79584 loss)
I0524 09:35:58.843017 11835 sgd_solver.cpp:112] Iteration 59140, lr = 0.1
I0524 09:36:07.116680 11835 solver.cpp:239] Iteration 59150 (1.03744 iter/s, 9.6391s/10 iters), loss = 5.8464
I0524 09:36:07.116904 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8464 (* 1 = 5.8464 loss)
I0524 09:36:07.116997 11835 sgd_solver.cpp:112] Iteration 59150, lr = 0.1
I0524 09:36:15.183030 11835 solver.cpp:239] Iteration 59160 (1.2398 iter/s, 8.06585s/10 iters), loss = 5.24885
I0524 09:36:15.183084 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.24885 (* 1 = 5.24885 loss)
I0524 09:36:15.183109 11835 sgd_solver.cpp:112] Iteration 59160, lr = 0.1
I0524 09:36:22.269718 11835 solver.cpp:239] Iteration 59170 (1.41116 iter/s, 7.08638s/10 iters), loss = 5.97517
I0524 09:36:22.269757 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.97517 (* 1 = 5.97517 loss)
I0524 09:36:22.269769 11835 sgd_solver.cpp:112] Iteration 59170, lr = 0.1
I0524 09:36:29.222055 11835 solver.cpp:239] Iteration 59180 (1.43843 iter/s, 6.952s/10 iters), loss = 5.86935
I0524 09:36:29.222147 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86935 (* 1 = 5.86935 loss)
I0524 09:36:29.973527 11835 sgd_solver.cpp:112] Iteration 59180, lr = 0.1
I0524 09:36:37.687731 11835 solver.cpp:239] Iteration 59190 (1.1813 iter/s, 8.46526s/10 iters), loss = 6.6856
I0524 09:36:37.687948 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6856 (* 1 = 6.6856 loss)
I0524 09:36:37.687980 11835 sgd_solver.cpp:112] Iteration 59190, lr = 0.1
I0524 09:36:44.428341 11835 solver.cpp:239] Iteration 59200 (1.48367 iter/s, 6.74004s/10 iters), loss = 5.79204
I0524 09:36:44.428383 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79204 (* 1 = 5.79204 loss)
I0524 09:36:44.428400 11835 sgd_solver.cpp:112] Iteration 59200, lr = 0.1
I0524 09:36:50.442123 11835 solver.cpp:239] Iteration 59210 (1.66295 iter/s, 6.01341s/10 iters), loss = 6.09558
I0524 09:36:50.442229 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09558 (* 1 = 6.09558 loss)
I0524 09:36:50.442251 11835 sgd_solver.cpp:112] Iteration 59210, lr = 0.1
I0524 09:36:56.363370 11835 solver.cpp:239] Iteration 59220 (1.68893 iter/s, 5.92092s/10 iters), loss = 6.70154
I0524 09:36:56.363435 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.70154 (* 1 = 6.70154 loss)
I0524 09:36:56.363535 11835 sgd_solver.cpp:112] Iteration 59220, lr = 0.1
I0524 09:37:02.292084 11835 solver.cpp:239] Iteration 59230 (1.68679 iter/s, 5.92843s/10 iters), loss = 6.86303
I0524 09:37:02.292130 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.86303 (* 1 = 6.86303 loss)
I0524 09:37:02.292145 11835 sgd_solver.cpp:112] Iteration 59230, lr = 0.1
I0524 09:37:10.103065 11835 solver.cpp:239] Iteration 59240 (1.28032 iter/s, 7.81054s/10 iters), loss = 6.22972
I0524 09:37:10.103348 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22972 (* 1 = 6.22972 loss)
I0524 09:37:10.103389 11835 sgd_solver.cpp:112] Iteration 59240, lr = 0.1
I0524 09:37:17.992921 11835 solver.cpp:239] Iteration 59250 (1.26754 iter/s, 7.88929s/10 iters), loss = 5.67879
I0524 09:37:17.993104 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67879 (* 1 = 5.67879 loss)
I0524 09:37:17.993157 11835 sgd_solver.cpp:112] Iteration 59250, lr = 0.1
I0524 09:37:25.077524 11835 solver.cpp:239] Iteration 59260 (1.41158 iter/s, 7.08424s/10 iters), loss = 5.95503
I0524 09:37:25.077581 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.95503 (* 1 = 5.95503 loss)
I0524 09:37:25.077680 11835 sgd_solver.cpp:112] Iteration 59260, lr = 0.1
I0524 09:37:31.325486 11835 solver.cpp:239] Iteration 59270 (1.6006 iter/s, 6.24767s/10 iters), loss = 5.81676
I0524 09:37:31.325522 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81676 (* 1 = 5.81676 loss)
I0524 09:37:31.325534 11835 sgd_solver.cpp:112] Iteration 59270, lr = 0.1
I0524 09:37:38.061683 11835 solver.cpp:239] Iteration 59280 (1.48459 iter/s, 6.73588s/10 iters), loss = 6.36968
I0524 09:37:38.061805 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36968 (* 1 = 6.36968 loss)
I0524 09:37:38.061959 11835 sgd_solver.cpp:112] Iteration 59280, lr = 0.1
I0524 09:37:44.376564 11835 solver.cpp:239] Iteration 59290 (1.58364 iter/s, 6.31457s/10 iters), loss = 6.1805
I0524 09:37:44.376787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1805 (* 1 = 6.1805 loss)
I0524 09:37:44.714773 11835 sgd_solver.cpp:112] Iteration 59290, lr = 0.1
I0524 09:37:54.522418 11835 solver.cpp:239] Iteration 59300 (0.985682 iter/s, 10.1453s/10 iters), loss = 6.58536
I0524 09:37:54.522585 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58536 (* 1 = 6.58536 loss)
I0524 09:37:54.522733 11835 sgd_solver.cpp:112] Iteration 59300, lr = 0.1
I0524 09:38:01.777238 11835 solver.cpp:239] Iteration 59310 (1.37846 iter/s, 7.25446s/10 iters), loss = 6.37263
I0524 09:38:01.777292 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.37263 (* 1 = 6.37263 loss)
I0524 09:38:02.099865 11835 sgd_solver.cpp:112] Iteration 59310, lr = 0.1
I0524 09:38:10.861094 11835 solver.cpp:239] Iteration 59320 (1.1009 iter/s, 9.08346s/10 iters), loss = 5.82817
I0524 09:38:10.861207 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82817 (* 1 = 5.82817 loss)
I0524 09:38:10.875813 11835 sgd_solver.cpp:112] Iteration 59320, lr = 0.1
I0524 09:38:18.666592 11835 solver.cpp:239] Iteration 59330 (1.28121 iter/s, 7.80514s/10 iters), loss = 6.43
I0524 09:38:18.666859 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.43 (* 1 = 6.43 loss)
I0524 09:38:19.091612 11835 sgd_solver.cpp:112] Iteration 59330, lr = 0.1
I0524 09:38:25.236572 11835 solver.cpp:239] Iteration 59340 (1.52219 iter/s, 6.5695s/10 iters), loss = 5.76782
I0524 09:38:25.236624 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.76782 (* 1 = 5.76782 loss)
I0524 09:38:25.491817 11835 sgd_solver.cpp:112] Iteration 59340, lr = 0.1
I0524 09:38:33.033038 11835 solver.cpp:239] Iteration 59350 (1.28269 iter/s, 7.79611s/10 iters), loss = 5.90851
I0524 09:38:33.033118 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.90851 (* 1 = 5.90851 loss)
I0524 09:38:33.033263 11835 sgd_solver.cpp:112] Iteration 59350, lr = 0.1
I0524 09:38:42.104480 11835 solver.cpp:239] Iteration 59360 (1.10241 iter/s, 9.07103s/10 iters), loss = 5.68841
I0524 09:38:42.104554 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68841 (* 1 = 5.68841 loss)
I0524 09:38:42.104694 11835 sgd_solver.cpp:112] Iteration 59360, lr = 0.1
I0524 09:38:48.093883 11835 solver.cpp:239] Iteration 59370 (1.6697 iter/s, 5.98911s/10 iters), loss = 5.94236
I0524 09:38:48.093931 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94236 (* 1 = 5.94236 loss)
I0524 09:38:48.094043 11835 sgd_solver.cpp:112] Iteration 59370, lr = 0.1
I0524 09:38:54.202646 11835 solver.cpp:239] Iteration 59380 (1.63707 iter/s, 6.10846s/10 iters), loss = 5.15113
I0524 09:38:54.203013 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.15113 (* 1 = 5.15113 loss)
I0524 09:38:54.203070 11835 sgd_solver.cpp:112] Iteration 59380, lr = 0.1
I0524 09:39:01.484879 11835 solver.cpp:239] Iteration 59390 (1.37335 iter/s, 7.28148s/10 iters), loss = 6.35592
I0524 09:39:01.484938 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35592 (* 1 = 6.35592 loss)
I0524 09:39:01.485045 11835 sgd_solver.cpp:112] Iteration 59390, lr = 0.1
I0524 09:39:08.749907 11835 solver.cpp:239] Iteration 59400 (1.37652 iter/s, 7.26468s/10 iters), loss = 5.48784
I0524 09:39:08.750043 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.48784 (* 1 = 5.48784 loss)
I0524 09:39:08.750260 11835 sgd_solver.cpp:112] Iteration 59400, lr = 0.1
I0524 09:39:15.429420 11835 solver.cpp:239] Iteration 59410 (1.49719 iter/s, 6.6792s/10 iters), loss = 7.0983
I0524 09:39:15.429479 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.0983 (* 1 = 7.0983 loss)
I0524 09:39:15.429548 11835 sgd_solver.cpp:112] Iteration 59410, lr = 0.1
I0524 09:39:21.618767 11835 solver.cpp:239] Iteration 59420 (1.61576 iter/s, 6.18906s/10 iters), loss = 5.7891
I0524 09:39:21.618810 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7891 (* 1 = 5.7891 loss)
I0524 09:39:21.618839 11835 sgd_solver.cpp:112] Iteration 59420, lr = 0.1
I0524 09:39:28.947563 11835 solver.cpp:239] Iteration 59430 (1.36454 iter/s, 7.32846s/10 iters), loss = 5.94038
I0524 09:39:28.947842 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94038 (* 1 = 5.94038 loss)
I0524 09:39:28.947903 11835 sgd_solver.cpp:112] Iteration 59430, lr = 0.1
I0524 09:39:35.700126 11835 solver.cpp:239] Iteration 59440 (1.48103 iter/s, 6.75207s/10 iters), loss = 5.21667
I0524 09:39:35.700173 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.21667 (* 1 = 5.21667 loss)
I0524 09:39:35.700188 11835 sgd_solver.cpp:112] Iteration 59440, lr = 0.1
I0524 09:39:42.446947 11835 solver.cpp:239] Iteration 59450 (1.48226 iter/s, 6.74644s/10 iters), loss = 6.46044
I0524 09:39:42.447002 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46044 (* 1 = 6.46044 loss)
I0524 09:39:42.448115 11835 sgd_solver.cpp:112] Iteration 59450, lr = 0.1
I0524 09:39:49.001930 11835 solver.cpp:239] Iteration 59460 (1.52563 iter/s, 6.55467s/10 iters), loss = 6.12121
I0524 09:39:49.001993 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.12121 (* 1 = 6.12121 loss)
I0524 09:39:49.002013 11835 sgd_solver.cpp:112] Iteration 59460, lr = 0.1
I0524 09:39:55.659323 11835 solver.cpp:239] Iteration 59470 (1.50221 iter/s, 6.65685s/10 iters), loss = 6.47914
I0524 09:39:55.659380 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.47914 (* 1 = 6.47914 loss)
I0524 09:39:55.659431 11835 sgd_solver.cpp:112] Iteration 59470, lr = 0.1
I0524 09:40:01.413570 11835 solver.cpp:239] Iteration 59480 (1.73793 iter/s, 5.75397s/10 iters), loss = 5.67631
I0524 09:40:01.413736 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67631 (* 1 = 5.67631 loss)
I0524 09:40:01.413758 11835 sgd_solver.cpp:112] Iteration 59480, lr = 0.1
I0524 09:40:10.011338 11835 solver.cpp:239] Iteration 59490 (1.16317 iter/s, 8.59723s/10 iters), loss = 6.74685
I0524 09:40:10.011425 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.74685 (* 1 = 6.74685 loss)
I0524 09:40:10.011440 11835 sgd_solver.cpp:112] Iteration 59490, lr = 0.1
I0524 09:40:16.502027 11835 solver.cpp:239] Iteration 59500 (1.54101 iter/s, 6.48924s/10 iters), loss = 5.65387
I0524 09:40:16.502212 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.65387 (* 1 = 5.65387 loss)
I0524 09:40:16.502266 11835 sgd_solver.cpp:112] Iteration 59500, lr = 0.1
I0524 09:40:23.714673 11835 solver.cpp:239] Iteration 59510 (1.38653 iter/s, 7.21227s/10 iters), loss = 6.16561
I0524 09:40:23.714763 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16561 (* 1 = 6.16561 loss)
I0524 09:40:23.740846 11835 sgd_solver.cpp:112] Iteration 59510, lr = 0.1
I0524 09:40:31.214579 11835 solver.cpp:239] Iteration 59520 (1.33341 iter/s, 7.49955s/10 iters), loss = 5.85773
I0524 09:40:31.214638 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85773 (* 1 = 5.85773 loss)
I0524 09:40:31.214792 11835 sgd_solver.cpp:112] Iteration 59520, lr = 0.1
I0524 09:40:37.162739 11835 solver.cpp:239] Iteration 59530 (1.68129 iter/s, 5.94781s/10 iters), loss = 6.88817
I0524 09:40:37.163028 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.88817 (* 1 = 6.88817 loss)
I0524 09:40:37.558075 11835 sgd_solver.cpp:112] Iteration 59530, lr = 0.1
I0524 09:40:43.485853 11835 solver.cpp:239] Iteration 59540 (1.58163 iter/s, 6.3226s/10 iters), loss = 6.18893
I0524 09:40:43.485914 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.18893 (* 1 = 6.18893 loss)
I0524 09:40:43.486109 11835 sgd_solver.cpp:112] Iteration 59540, lr = 0.1
I0524 09:40:50.274894 11835 solver.cpp:239] Iteration 59550 (1.47303 iter/s, 6.78872s/10 iters), loss = 6.97032
I0524 09:40:50.274940 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.97032 (* 1 = 6.97032 loss)
I0524 09:40:50.275002 11835 sgd_solver.cpp:112] Iteration 59550, lr = 0.1
I0524 09:40:56.708391 11835 solver.cpp:239] Iteration 59560 (1.55444 iter/s, 6.43317s/10 iters), loss = 6.60639
I0524 09:40:56.708488 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60639 (* 1 = 6.60639 loss)
I0524 09:40:56.712759 11835 sgd_solver.cpp:112] Iteration 59560, lr = 0.1
I0524 09:41:03.741927 11835 solver.cpp:239] Iteration 59570 (1.42182 iter/s, 7.03322s/10 iters), loss = 5.7688
I0524 09:41:03.741986 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7688 (* 1 = 5.7688 loss)
I0524 09:41:03.742199 11835 sgd_solver.cpp:112] Iteration 59570, lr = 0.1
I0524 09:41:11.585525 11835 solver.cpp:239] Iteration 59580 (1.27498 iter/s, 7.84324s/10 iters), loss = 5.85202
I0524 09:41:11.585698 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85202 (* 1 = 5.85202 loss)
I0524 09:41:11.971601 11835 sgd_solver.cpp:112] Iteration 59580, lr = 0.1
I0524 09:41:19.657826 11835 solver.cpp:239] Iteration 59590 (1.23888 iter/s, 8.07182s/10 iters), loss = 6.30075
I0524 09:41:19.657881 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30075 (* 1 = 6.30075 loss)
I0524 09:41:19.657896 11835 sgd_solver.cpp:112] Iteration 59590, lr = 0.1
I0524 09:41:27.635296 11835 solver.cpp:239] Iteration 59600 (1.25361 iter/s, 7.97694s/10 iters), loss = 6.03015
I0524 09:41:27.635336 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.03015 (* 1 = 6.03015 loss)
I0524 09:41:27.635654 11835 sgd_solver.cpp:112] Iteration 59600, lr = 0.1
I0524 09:41:34.137382 11835 solver.cpp:239] Iteration 59610 (1.53805 iter/s, 6.50175s/10 iters), loss = 5.53784
I0524 09:41:34.137455 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.53784 (* 1 = 5.53784 loss)
I0524 09:41:34.137508 11835 sgd_solver.cpp:112] Iteration 59610, lr = 0.1
I0524 09:41:44.182601 11835 solver.cpp:239] Iteration 59620 (0.995581 iter/s, 10.0444s/10 iters), loss = 5.76978
I0524 09:41:44.182936 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.76978 (* 1 = 5.76978 loss)
I0524 09:41:44.182986 11835 sgd_solver.cpp:112] Iteration 59620, lr = 0.1
I0524 09:41:51.737465 11835 solver.cpp:239] Iteration 59630 (1.32385 iter/s, 7.55375s/10 iters), loss = 5.56918
I0524 09:41:51.737514 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56918 (* 1 = 5.56918 loss)
I0524 09:41:51.737860 11835 sgd_solver.cpp:112] Iteration 59630, lr = 0.1
I0524 09:42:00.971695 11835 solver.cpp:239] Iteration 59640 (1.08298 iter/s, 9.23382s/10 iters), loss = 7.34245
I0524 09:42:00.971750 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.34245 (* 1 = 7.34245 loss)
I0524 09:42:00.972038 11835 sgd_solver.cpp:112] Iteration 59640, lr = 0.1
I0524 09:42:06.934473 11835 solver.cpp:239] Iteration 59650 (1.67716 iter/s, 5.96247s/10 iters), loss = 6.59025
I0524 09:42:06.934562 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.59025 (* 1 = 6.59025 loss)
I0524 09:42:06.951068 11835 sgd_solver.cpp:112] Iteration 59650, lr = 0.1
I0524 09:42:13.604615 11835 solver.cpp:239] Iteration 59660 (1.4993 iter/s, 6.6698s/10 iters), loss = 4.89521
I0524 09:42:13.604672 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.89521 (* 1 = 4.89521 loss)
I0524 09:42:13.604897 11835 sgd_solver.cpp:112] Iteration 59660, lr = 0.1
I0524 09:42:19.946943 11835 solver.cpp:239] Iteration 59670 (1.57678 iter/s, 6.34204s/10 iters), loss = 6.71514
I0524 09:42:19.947150 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71514 (* 1 = 6.71514 loss)
I0524 09:42:19.947181 11835 sgd_solver.cpp:112] Iteration 59670, lr = 0.1
I0524 09:42:26.593763 11835 solver.cpp:239] Iteration 59680 (1.50459 iter/s, 6.64634s/10 iters), loss = 4.69397
I0524 09:42:26.593814 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.69397 (* 1 = 4.69397 loss)
I0524 09:42:26.593829 11835 sgd_solver.cpp:112] Iteration 59680, lr = 0.1
I0524 09:42:33.054613 11835 solver.cpp:239] Iteration 59690 (1.54785 iter/s, 6.46056s/10 iters), loss = 5.38148
I0524 09:42:33.054651 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.38148 (* 1 = 5.38148 loss)
I0524 09:42:33.054663 11835 sgd_solver.cpp:112] Iteration 59690, lr = 0.1
I0524 09:42:39.862092 11835 solver.cpp:239] Iteration 59700 (1.46904 iter/s, 6.80717s/10 iters), loss = 5.22662
I0524 09:42:39.862156 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.22662 (* 1 = 5.22662 loss)
I0524 09:42:39.862174 11835 sgd_solver.cpp:112] Iteration 59700, lr = 0.1
I0524 09:42:49.699880 11835 solver.cpp:239] Iteration 59710 (1.01653 iter/s, 9.83736s/10 iters), loss = 6.25201
I0524 09:42:49.699935 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25201 (* 1 = 6.25201 loss)
I0524 09:42:49.699951 11835 sgd_solver.cpp:112] Iteration 59710, lr = 0.1
I0524 09:42:56.487538 11835 solver.cpp:239] Iteration 59720 (1.47333 iter/s, 6.78735s/10 iters), loss = 6.6703
I0524 09:42:56.487680 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6703 (* 1 = 6.6703 loss)
I0524 09:42:56.924401 11835 sgd_solver.cpp:112] Iteration 59720, lr = 0.1
I0524 09:43:04.642383 11835 solver.cpp:239] Iteration 59730 (1.22633 iter/s, 8.15438s/10 iters), loss = 6.89575
I0524 09:43:04.642447 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.89575 (* 1 = 6.89575 loss)
I0524 09:43:04.717321 11835 sgd_solver.cpp:112] Iteration 59730, lr = 0.1
I0524 09:43:12.187607 11835 solver.cpp:239] Iteration 59740 (1.3254 iter/s, 7.54487s/10 iters), loss = 5.92346
I0524 09:43:12.187655 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92346 (* 1 = 5.92346 loss)
I0524 09:43:12.496369 11835 sgd_solver.cpp:112] Iteration 59740, lr = 0.1
I0524 09:43:18.841032 11835 solver.cpp:239] Iteration 59750 (1.50306 iter/s, 6.6531s/10 iters), loss = 7.82511
I0524 09:43:18.841080 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.82511 (* 1 = 7.82511 loss)
I0524 09:43:19.385301 11835 sgd_solver.cpp:112] Iteration 59750, lr = 0.1
I0524 09:43:25.625013 11835 solver.cpp:239] Iteration 59760 (1.47413 iter/s, 6.78368s/10 iters), loss = 6.31463
I0524 09:43:25.625051 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31463 (* 1 = 6.31463 loss)
I0524 09:43:25.625064 11835 sgd_solver.cpp:112] Iteration 59760, lr = 0.1
I0524 09:43:33.735462 11835 solver.cpp:239] Iteration 59770 (1.23303 iter/s, 8.11008s/10 iters), loss = 5.67754
I0524 09:43:33.735738 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67754 (* 1 = 5.67754 loss)
I0524 09:43:33.735805 11835 sgd_solver.cpp:112] Iteration 59770, lr = 0.1
I0524 09:43:39.894815 11835 solver.cpp:239] Iteration 59780 (1.62379 iter/s, 6.15845s/10 iters), loss = 4.86167
I0524 09:43:39.894876 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.86167 (* 1 = 4.86167 loss)
I0524 09:43:39.894919 11835 sgd_solver.cpp:112] Iteration 59780, lr = 0.1
I0524 09:43:46.865185 11835 solver.cpp:239] Iteration 59790 (1.43471 iter/s, 6.97005s/10 iters), loss = 5.74357
I0524 09:43:46.865224 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74357 (* 1 = 5.74357 loss)
I0524 09:43:46.865348 11835 sgd_solver.cpp:112] Iteration 59790, lr = 0.1
I0524 09:43:54.236397 11835 solver.cpp:239] Iteration 59800 (1.35669 iter/s, 7.37088s/10 iters), loss = 6.10076
I0524 09:43:54.236454 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10076 (* 1 = 6.10076 loss)
I0524 09:43:54.281646 11835 sgd_solver.cpp:112] Iteration 59800, lr = 0.1
I0524 09:44:01.153285 11835 solver.cpp:239] Iteration 59810 (1.4458 iter/s, 6.91656s/10 iters), loss = 6.06568
I0524 09:44:01.153348 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.06568 (* 1 = 6.06568 loss)
I0524 09:44:01.521167 11835 sgd_solver.cpp:112] Iteration 59810, lr = 0.1
I0524 09:44:08.456660 11835 solver.cpp:239] Iteration 59820 (1.36929 iter/s, 7.30304s/10 iters), loss = 6.00381
I0524 09:44:08.456807 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.00381 (* 1 = 6.00381 loss)
I0524 09:44:08.456979 11835 sgd_solver.cpp:112] Iteration 59820, lr = 0.1
I0524 09:44:14.588863 11835 solver.cpp:239] Iteration 59830 (1.63084 iter/s, 6.13181s/10 iters), loss = 5.49287
I0524 09:44:14.588918 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.49287 (* 1 = 5.49287 loss)
I0524 09:44:14.762536 11835 sgd_solver.cpp:112] Iteration 59830, lr = 0.1
I0524 09:44:21.091706 11835 solver.cpp:239] Iteration 59840 (1.53786 iter/s, 6.50253s/10 iters), loss = 6.83385
I0524 09:44:21.091755 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.83385 (* 1 = 6.83385 loss)
I0524 09:44:21.091773 11835 sgd_solver.cpp:112] Iteration 59840, lr = 0.1
I0524 09:44:28.467788 11835 solver.cpp:239] Iteration 59850 (1.35619 iter/s, 7.37358s/10 iters), loss = 5.98577
I0524 09:44:28.467824 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98577 (* 1 = 5.98577 loss)
I0524 09:44:28.467836 11835 sgd_solver.cpp:112] Iteration 59850, lr = 0.1
I0524 09:44:38.962919 11835 solver.cpp:239] Iteration 59860 (0.952864 iter/s, 10.4947s/10 iters), loss = 5.27632
I0524 09:44:38.963162 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.27632 (* 1 = 5.27632 loss)
I0524 09:44:39.112174 11835 sgd_solver.cpp:112] Iteration 59860, lr = 0.1
I0524 09:44:44.757915 11835 solver.cpp:239] Iteration 59870 (1.72576 iter/s, 5.79456s/10 iters), loss = 6.05787
I0524 09:44:44.757966 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05787 (* 1 = 6.05787 loss)
I0524 09:44:44.758186 11835 sgd_solver.cpp:112] Iteration 59870, lr = 0.1
I0524 09:44:52.452062 11835 solver.cpp:239] Iteration 59880 (1.29975 iter/s, 7.6938s/10 iters), loss = 5.84343
I0524 09:44:52.452114 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84343 (* 1 = 5.84343 loss)
I0524 09:44:53.269542 11835 sgd_solver.cpp:112] Iteration 59880, lr = 0.1
I0524 09:45:02.175129 11835 solver.cpp:239] Iteration 59890 (1.02853 iter/s, 9.72264s/10 iters), loss = 5.79874
I0524 09:45:02.175195 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79874 (* 1 = 5.79874 loss)
I0524 09:45:02.175426 11835 sgd_solver.cpp:112] Iteration 59890, lr = 0.1
I0524 09:45:08.017891 11835 solver.cpp:239] Iteration 59900 (1.7116 iter/s, 5.84248s/10 iters), loss = 6.32155
I0524 09:45:08.017942 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.32155 (* 1 = 6.32155 loss)
I0524 09:45:08.017956 11835 sgd_solver.cpp:112] Iteration 59900, lr = 0.1
I0524 09:45:13.995441 11835 solver.cpp:239] Iteration 59910 (1.673 iter/s, 5.97727s/10 iters), loss = 5.84878
I0524 09:45:13.995717 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84878 (* 1 = 5.84878 loss)
I0524 09:45:13.995760 11835 sgd_solver.cpp:112] Iteration 59910, lr = 0.1
I0524 09:45:19.995733 11835 solver.cpp:239] Iteration 59920 (1.66672 iter/s, 5.99979s/10 iters), loss = 5.52069
I0524 09:45:19.995766 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.52069 (* 1 = 5.52069 loss)
I0524 09:45:20.060951 11835 sgd_solver.cpp:112] Iteration 59920, lr = 0.1
I0524 09:45:29.608054 11835 solver.cpp:239] Iteration 59930 (1.04038 iter/s, 9.6119s/10 iters), loss = 6.01102
I0524 09:45:29.608112 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01102 (* 1 = 6.01102 loss)
I0524 09:45:29.752554 11835 sgd_solver.cpp:112] Iteration 59930, lr = 0.1
I0524 09:45:37.591482 11835 solver.cpp:239] Iteration 59940 (1.25265 iter/s, 7.98308s/10 iters), loss = 5.74394
I0524 09:45:37.591531 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74394 (* 1 = 5.74394 loss)
I0524 09:45:37.591542 11835 sgd_solver.cpp:112] Iteration 59940, lr = 0.1
I0524 09:45:46.110956 11835 solver.cpp:239] Iteration 59950 (1.17414 iter/s, 8.5169s/10 iters), loss = 5.48897
I0524 09:45:46.111104 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.48897 (* 1 = 5.48897 loss)
I0524 09:45:46.111227 11835 sgd_solver.cpp:112] Iteration 59950, lr = 0.1
I0524 09:45:51.782737 11835 solver.cpp:239] Iteration 59960 (1.76324 iter/s, 5.67139s/10 iters), loss = 5.82822
I0524 09:45:51.782799 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82822 (* 1 = 5.82822 loss)
I0524 09:45:51.783023 11835 sgd_solver.cpp:112] Iteration 59960, lr = 0.1
I0524 09:45:58.891367 11835 solver.cpp:239] Iteration 59970 (1.40681 iter/s, 7.10829s/10 iters), loss = 4.93268
I0524 09:45:58.891424 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.93268 (* 1 = 4.93268 loss)
I0524 09:45:58.891731 11835 sgd_solver.cpp:112] Iteration 59970, lr = 0.1
I0524 09:46:06.412405 11835 solver.cpp:239] Iteration 59980 (1.32966 iter/s, 7.5207s/10 iters), loss = 5.98591
I0524 09:46:06.412451 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98591 (* 1 = 5.98591 loss)
I0524 09:46:06.412601 11835 sgd_solver.cpp:112] Iteration 59980, lr = 0.1
I0524 09:46:12.721611 11835 solver.cpp:239] Iteration 59990 (1.58506 iter/s, 6.30891s/10 iters), loss = 6.20258
I0524 09:46:12.721662 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20258 (* 1 = 6.20258 loss)
I0524 09:46:12.721698 11835 sgd_solver.cpp:112] Iteration 59990, lr = 0.1
I0524 09:46:19.472879 11835 solver.cpp:468] Snapshotting to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_60000.caffemodel
I0524 09:46:19.640708 11835 sgd_solver.cpp:280] Snapshotting solver state to binary proto file ../asset/snapshot/AMImageMeanCdata/AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet/2018-05-23_AMImageMeanCdata-b0.3s30_fc_0.35_112x96_b+FaceAdd_MobileFaceNet_zkx_iter_60000.solverstate
I0524 09:46:20.207258 11835 solver.cpp:239] Iteration 60000 (1.33595 iter/s, 7.48531s/10 iters), loss = 7.23407
I0524 09:46:20.207303 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.23407 (* 1 = 7.23407 loss)
I0524 09:46:20.233263 11835 sgd_solver.cpp:112] Iteration 60000, lr = 0.1
I0524 09:46:27.355279 11835 solver.cpp:239] Iteration 60010 (1.39905 iter/s, 7.1477s/10 iters), loss = 7.993
I0524 09:46:27.355334 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.993 (* 1 = 7.993 loss)
I0524 09:46:27.355443 11835 sgd_solver.cpp:112] Iteration 60010, lr = 0.1
I0524 09:46:34.697717 11835 solver.cpp:239] Iteration 60020 (1.36201 iter/s, 7.34209s/10 iters), loss = 6.40292
I0524 09:46:34.697767 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.40292 (* 1 = 6.40292 loss)
I0524 09:46:34.892880 11835 sgd_solver.cpp:112] Iteration 60020, lr = 0.1
I0524 09:46:42.104570 11835 solver.cpp:239] Iteration 60030 (1.35016 iter/s, 7.40651s/10 iters), loss = 6.31509
I0524 09:46:42.104641 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31509 (* 1 = 6.31509 loss)
I0524 09:46:42.104852 11835 sgd_solver.cpp:112] Iteration 60030, lr = 0.1
I0524 09:46:50.147974 11835 solver.cpp:239] Iteration 60040 (1.24331 iter/s, 8.04303s/10 iters), loss = 6.0757
I0524 09:46:50.148178 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0757 (* 1 = 6.0757 loss)
I0524 09:46:50.537420 11835 sgd_solver.cpp:112] Iteration 60040, lr = 0.1
I0524 09:46:58.462070 11835 solver.cpp:239] Iteration 60050 (1.20285 iter/s, 8.31357s/10 iters), loss = 6.26991
I0524 09:46:58.462157 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26991 (* 1 = 6.26991 loss)
I0524 09:46:59.003276 11835 sgd_solver.cpp:112] Iteration 60050, lr = 0.1
I0524 09:47:05.458010 11835 solver.cpp:239] Iteration 60060 (1.42947 iter/s, 6.99561s/10 iters), loss = 5.78136
I0524 09:47:05.458060 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78136 (* 1 = 5.78136 loss)
I0524 09:47:06.143488 11835 sgd_solver.cpp:112] Iteration 60060, lr = 0.1
I0524 09:47:13.736208 11835 solver.cpp:239] Iteration 60070 (1.20805 iter/s, 8.27782s/10 iters), loss = 6.13552
I0524 09:47:13.736270 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13552 (* 1 = 6.13552 loss)
I0524 09:47:13.736445 11835 sgd_solver.cpp:112] Iteration 60070, lr = 0.1
I0524 09:47:19.616006 11835 solver.cpp:239] Iteration 60080 (1.70082 iter/s, 5.87952s/10 iters), loss = 6.6441
I0524 09:47:19.616044 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.6441 (* 1 = 6.6441 loss)
I0524 09:47:19.616055 11835 sgd_solver.cpp:112] Iteration 60080, lr = 0.1
I0524 09:47:27.951975 11835 solver.cpp:239] Iteration 60090 (1.19968 iter/s, 8.33553s/10 iters), loss = 6.76477
I0524 09:47:27.952222 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76477 (* 1 = 6.76477 loss)
I0524 09:47:27.952301 11835 sgd_solver.cpp:112] Iteration 60090, lr = 0.1
I0524 09:47:35.167076 11835 solver.cpp:239] Iteration 60100 (1.38607 iter/s, 7.21462s/10 iters), loss = 6.78984
I0524 09:47:35.167125 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.78984 (* 1 = 6.78984 loss)
I0524 09:47:35.624316 11835 sgd_solver.cpp:112] Iteration 60100, lr = 0.1
I0524 09:47:42.831713 11835 solver.cpp:239] Iteration 60110 (1.30475 iter/s, 7.66429s/10 iters), loss = 6.15939
I0524 09:47:42.831765 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15939 (* 1 = 6.15939 loss)
I0524 09:47:42.832015 11835 sgd_solver.cpp:112] Iteration 60110, lr = 0.1
I0524 09:47:50.460453 11835 solver.cpp:239] Iteration 60120 (1.31089 iter/s, 7.62838s/10 iters), loss = 6.3447
I0524 09:47:50.460510 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3447 (* 1 = 6.3447 loss)
I0524 09:47:50.460837 11835 sgd_solver.cpp:112] Iteration 60120, lr = 0.1
I0524 09:47:56.718662 11835 solver.cpp:239] Iteration 60130 (1.59798 iter/s, 6.25791s/10 iters), loss = 6.54523
I0524 09:47:56.718726 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54523 (* 1 = 6.54523 loss)
I0524 09:47:56.718896 11835 sgd_solver.cpp:112] Iteration 60130, lr = 0.1
I0524 09:48:02.838161 11835 solver.cpp:239] Iteration 60140 (1.6342 iter/s, 6.1192s/10 iters), loss = 6.17872
I0524 09:48:02.838347 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17872 (* 1 = 6.17872 loss)
I0524 09:48:02.838371 11835 sgd_solver.cpp:112] Iteration 60140, lr = 0.1
I0524 09:48:09.586735 11835 solver.cpp:239] Iteration 60150 (1.4819 iter/s, 6.74809s/10 iters), loss = 6.19337
I0524 09:48:09.586781 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19337 (* 1 = 6.19337 loss)
I0524 09:48:09.586794 11835 sgd_solver.cpp:112] Iteration 60150, lr = 0.1
I0524 09:48:17.899631 11835 solver.cpp:239] Iteration 60160 (1.203 iter/s, 8.31252s/10 iters), loss = 6.82387
I0524 09:48:17.899689 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.82387 (* 1 = 6.82387 loss)
I0524 09:48:18.463282 11835 sgd_solver.cpp:112] Iteration 60160, lr = 0.1
I0524 09:48:25.196805 11835 solver.cpp:239] Iteration 60170 (1.37046 iter/s, 7.29683s/10 iters), loss = 6.58092
I0524 09:48:25.196861 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.58092 (* 1 = 6.58092 loss)
I0524 09:48:25.196879 11835 sgd_solver.cpp:112] Iteration 60170, lr = 0.1
I0524 09:48:32.915320 11835 solver.cpp:239] Iteration 60180 (1.29566 iter/s, 7.7181s/10 iters), loss = 4.88697
I0524 09:48:32.915680 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.88697 (* 1 = 4.88697 loss)
I0524 09:48:32.915930 11835 sgd_solver.cpp:112] Iteration 60180, lr = 0.1
I0524 09:48:38.797693 11835 solver.cpp:239] Iteration 60190 (1.70013 iter/s, 5.88189s/10 iters), loss = 6.30155
I0524 09:48:38.797740 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.30155 (* 1 = 6.30155 loss)
I0524 09:48:38.797754 11835 sgd_solver.cpp:112] Iteration 60190, lr = 0.1
I0524 09:48:44.869086 11835 solver.cpp:239] Iteration 60200 (1.64716 iter/s, 6.07105s/10 iters), loss = 5.50802
I0524 09:48:44.869137 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.50802 (* 1 = 5.50802 loss)
I0524 09:48:44.869374 11835 sgd_solver.cpp:112] Iteration 60200, lr = 0.1
I0524 09:48:50.746243 11835 solver.cpp:239] Iteration 60210 (1.70158 iter/s, 5.87688s/10 iters), loss = 7.48341
I0524 09:48:50.746291 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.48341 (* 1 = 7.48341 loss)
I0524 09:48:51.536761 11835 sgd_solver.cpp:112] Iteration 60210, lr = 0.1
I0524 09:48:58.046219 11835 solver.cpp:239] Iteration 60220 (1.36993 iter/s, 7.29963s/10 iters), loss = 6.21532
I0524 09:48:58.046336 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.21532 (* 1 = 6.21532 loss)
I0524 09:48:58.303441 11835 sgd_solver.cpp:112] Iteration 60220, lr = 0.1
I0524 09:49:05.600915 11835 solver.cpp:239] Iteration 60230 (1.32374 iter/s, 7.55434s/10 iters), loss = 6.72583
I0524 09:49:05.601172 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.72583 (* 1 = 6.72583 loss)
I0524 09:49:05.601222 11835 sgd_solver.cpp:112] Iteration 60230, lr = 0.1
I0524 09:49:13.300828 11835 solver.cpp:239] Iteration 60240 (1.29882 iter/s, 7.69927s/10 iters), loss = 6.04385
I0524 09:49:13.300884 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.04385 (* 1 = 6.04385 loss)
I0524 09:49:14.165357 11835 sgd_solver.cpp:112] Iteration 60240, lr = 0.1
I0524 09:49:22.060236 11835 solver.cpp:239] Iteration 60250 (1.14168 iter/s, 8.75902s/10 iters), loss = 6.94466
I0524 09:49:22.060307 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.94466 (* 1 = 6.94466 loss)
I0524 09:49:22.060511 11835 sgd_solver.cpp:112] Iteration 60250, lr = 0.1
I0524 09:49:30.418128 11835 solver.cpp:239] Iteration 60260 (1.19653 iter/s, 8.3575s/10 iters), loss = 6.16871
I0524 09:49:30.418200 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16871 (* 1 = 6.16871 loss)
I0524 09:49:30.556792 11835 sgd_solver.cpp:112] Iteration 60260, lr = 0.1
I0524 09:49:37.494184 11835 solver.cpp:239] Iteration 60270 (1.41329 iter/s, 7.0757s/10 iters), loss = 5.52832
I0524 09:49:37.494361 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.52832 (* 1 = 5.52832 loss)
I0524 09:49:37.494410 11835 sgd_solver.cpp:112] Iteration 60270, lr = 0.1
I0524 09:49:43.699327 11835 solver.cpp:239] Iteration 60280 (1.61167 iter/s, 6.20473s/10 iters), loss = 6.93183
I0524 09:49:43.699373 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93183 (* 1 = 6.93183 loss)
I0524 09:49:43.699635 11835 sgd_solver.cpp:112] Iteration 60280, lr = 0.1
I0524 09:49:49.525326 11835 solver.cpp:239] Iteration 60290 (1.71653 iter/s, 5.82572s/10 iters), loss = 6.29887
I0524 09:49:49.525377 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.29887 (* 1 = 6.29887 loss)
I0524 09:49:49.894165 11835 sgd_solver.cpp:112] Iteration 60290, lr = 0.1
I0524 09:49:58.076196 11835 solver.cpp:239] Iteration 60300 (1.16952 iter/s, 8.55048s/10 iters), loss = 5.34324
I0524 09:49:58.076249 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.34324 (* 1 = 5.34324 loss)
I0524 09:49:58.143084 11835 sgd_solver.cpp:112] Iteration 60300, lr = 0.1
I0524 09:50:04.308848 11835 solver.cpp:239] Iteration 60310 (1.60453 iter/s, 6.23236s/10 iters), loss = 6.2817
I0524 09:50:04.308903 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2817 (* 1 = 6.2817 loss)
I0524 09:50:04.308920 11835 sgd_solver.cpp:112] Iteration 60310, lr = 0.1
I0524 09:50:10.815564 11835 solver.cpp:239] Iteration 60320 (1.53694 iter/s, 6.50642s/10 iters), loss = 5.8649
I0524 09:50:10.815780 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8649 (* 1 = 5.8649 loss)
I0524 09:50:10.815810 11835 sgd_solver.cpp:112] Iteration 60320, lr = 0.1
I0524 09:50:19.315706 11835 solver.cpp:239] Iteration 60330 (1.1766 iter/s, 8.49909s/10 iters), loss = 6.49283
I0524 09:50:19.315775 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49283 (* 1 = 6.49283 loss)
I0524 09:50:19.316031 11835 sgd_solver.cpp:112] Iteration 60330, lr = 0.1
I0524 09:50:25.699283 11835 solver.cpp:239] Iteration 60340 (1.5666 iter/s, 6.38326s/10 iters), loss = 5.54153
I0524 09:50:25.699343 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.54153 (* 1 = 5.54153 loss)
I0524 09:50:25.699439 11835 sgd_solver.cpp:112] Iteration 60340, lr = 0.1
I0524 09:50:32.911038 11835 solver.cpp:239] Iteration 60350 (1.38669 iter/s, 7.21141s/10 iters), loss = 5.22952
I0524 09:50:32.911094 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.22952 (* 1 = 5.22952 loss)
I0524 09:50:32.911111 11835 sgd_solver.cpp:112] Iteration 60350, lr = 0.1
I0524 09:50:39.260987 11835 solver.cpp:239] Iteration 60360 (1.57495 iter/s, 6.3494s/10 iters), loss = 5.67808
I0524 09:50:39.261040 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67808 (* 1 = 5.67808 loss)
I0524 09:50:39.677187 11835 sgd_solver.cpp:112] Iteration 60360, lr = 0.1
I0524 09:50:45.635979 11835 solver.cpp:239] Iteration 60370 (1.5687 iter/s, 6.37469s/10 iters), loss = 6.22766
I0524 09:50:45.636162 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.22766 (* 1 = 6.22766 loss)
I0524 09:50:45.725162 11835 sgd_solver.cpp:112] Iteration 60370, lr = 0.1
I0524 09:50:52.071902 11835 solver.cpp:239] Iteration 60380 (1.55388 iter/s, 6.4355s/10 iters), loss = 6.20849
I0524 09:50:52.071951 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20849 (* 1 = 6.20849 loss)
I0524 09:50:52.072002 11835 sgd_solver.cpp:112] Iteration 60380, lr = 0.1
I0524 09:50:58.633455 11835 solver.cpp:239] Iteration 60390 (1.5241 iter/s, 6.56125s/10 iters), loss = 6.46502
I0524 09:50:58.633504 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.46502 (* 1 = 6.46502 loss)
I0524 09:50:58.633518 11835 sgd_solver.cpp:112] Iteration 60390, lr = 0.1
I0524 09:51:05.599316 11835 solver.cpp:239] Iteration 60400 (1.43566 iter/s, 6.96544s/10 iters), loss = 5.194
I0524 09:51:05.599371 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.194 (* 1 = 5.194 loss)
I0524 09:51:05.599498 11835 sgd_solver.cpp:112] Iteration 60400, lr = 0.1
I0524 09:51:12.795429 11835 solver.cpp:239] Iteration 60410 (1.38971 iter/s, 7.19577s/10 iters), loss = 6.11015
I0524 09:51:12.795488 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11015 (* 1 = 6.11015 loss)
I0524 09:51:12.795585 11835 sgd_solver.cpp:112] Iteration 60410, lr = 0.1
I0524 09:51:20.628748 11835 solver.cpp:239] Iteration 60420 (1.27666 iter/s, 7.83296s/10 iters), loss = 6.84017
I0524 09:51:20.628937 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84017 (* 1 = 6.84017 loss)
I0524 09:51:20.628969 11835 sgd_solver.cpp:112] Iteration 60420, lr = 0.1
I0524 09:51:27.308339 11835 solver.cpp:239] Iteration 60430 (1.49744 iter/s, 6.67805s/10 iters), loss = 5.87929
I0524 09:51:27.308392 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87929 (* 1 = 5.87929 loss)
I0524 09:51:27.308420 11835 sgd_solver.cpp:112] Iteration 60430, lr = 0.1
I0524 09:51:33.547807 11835 solver.cpp:239] Iteration 60440 (1.60278 iter/s, 6.23918s/10 iters), loss = 5.46586
I0524 09:51:33.547857 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.46586 (* 1 = 5.46586 loss)
I0524 09:51:33.755270 11835 sgd_solver.cpp:112] Iteration 60440, lr = 0.1
I0524 09:51:40.278437 11835 solver.cpp:239] Iteration 60450 (1.48582 iter/s, 6.7303s/10 iters), loss = 6.71888
I0524 09:51:40.278496 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71888 (* 1 = 6.71888 loss)
I0524 09:51:40.278602 11835 sgd_solver.cpp:112] Iteration 60450, lr = 0.1
I0524 09:51:46.642644 11835 solver.cpp:239] Iteration 60460 (1.57136 iter/s, 6.3639s/10 iters), loss = 6.11707
I0524 09:51:46.642737 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11707 (* 1 = 6.11707 loss)
I0524 09:51:46.881496 11835 sgd_solver.cpp:112] Iteration 60460, lr = 0.1
I0524 09:51:56.431576 11835 solver.cpp:239] Iteration 60470 (1.02161 iter/s, 9.78849s/10 iters), loss = 6.35336
I0524 09:51:56.431795 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.35336 (* 1 = 6.35336 loss)
I0524 09:51:56.458824 11835 sgd_solver.cpp:112] Iteration 60470, lr = 0.1
I0524 09:52:03.055970 11835 solver.cpp:239] Iteration 60480 (1.50968 iter/s, 6.62394s/10 iters), loss = 6.53159
I0524 09:52:03.056013 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53159 (* 1 = 6.53159 loss)
I0524 09:52:03.056247 11835 sgd_solver.cpp:112] Iteration 60480, lr = 0.1
I0524 09:52:11.938853 11835 solver.cpp:239] Iteration 60490 (1.12581 iter/s, 8.88248s/10 iters), loss = 5.84536
I0524 09:52:11.938908 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84536 (* 1 = 5.84536 loss)
I0524 09:52:11.939111 11835 sgd_solver.cpp:112] Iteration 60490, lr = 0.1
I0524 09:52:18.470858 11835 solver.cpp:239] Iteration 60500 (1.531 iter/s, 6.53168s/10 iters), loss = 5.54172
I0524 09:52:18.470927 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.54172 (* 1 = 5.54172 loss)
I0524 09:52:18.524171 11835 sgd_solver.cpp:112] Iteration 60500, lr = 0.1
I0524 09:52:26.834781 11835 solver.cpp:239] Iteration 60510 (1.19567 iter/s, 8.36353s/10 iters), loss = 4.87385
I0524 09:52:26.835053 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.87385 (* 1 = 4.87385 loss)
I0524 09:52:26.835216 11835 sgd_solver.cpp:112] Iteration 60510, lr = 0.1
I0524 09:52:34.369578 11835 solver.cpp:239] Iteration 60520 (1.32727 iter/s, 7.53428s/10 iters), loss = 6.47932
I0524 09:52:34.369627 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.47932 (* 1 = 6.47932 loss)
I0524 09:52:34.369683 11835 sgd_solver.cpp:112] Iteration 60520, lr = 0.1
I0524 09:52:41.034684 11835 solver.cpp:239] Iteration 60530 (1.50042 iter/s, 6.66481s/10 iters), loss = 6.84332
I0524 09:52:41.034759 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.84332 (* 1 = 6.84332 loss)
I0524 09:52:41.034771 11835 sgd_solver.cpp:112] Iteration 60530, lr = 0.1
I0524 09:52:48.247642 11835 solver.cpp:239] Iteration 60540 (1.38648 iter/s, 7.2125s/10 iters), loss = 5.76311
I0524 09:52:48.247695 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.76311 (* 1 = 5.76311 loss)
I0524 09:52:48.247712 11835 sgd_solver.cpp:112] Iteration 60540, lr = 0.1
I0524 09:52:54.277362 11835 solver.cpp:239] Iteration 60550 (1.65883 iter/s, 6.02834s/10 iters), loss = 5.50804
I0524 09:52:54.277405 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.50804 (* 1 = 5.50804 loss)
I0524 09:52:54.277500 11835 sgd_solver.cpp:112] Iteration 60550, lr = 0.1
I0524 09:53:03.337945 11835 solver.cpp:239] Iteration 60560 (1.10373 iter/s, 9.06018s/10 iters), loss = 5.91861
I0524 09:53:03.338201 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91861 (* 1 = 5.91861 loss)
I0524 09:53:03.513427 11835 sgd_solver.cpp:112] Iteration 60560, lr = 0.1
I0524 09:53:09.741245 11835 solver.cpp:239] Iteration 60570 (1.56181 iter/s, 6.40282s/10 iters), loss = 6.25739
I0524 09:53:09.741294 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25739 (* 1 = 6.25739 loss)
I0524 09:53:09.741310 11835 sgd_solver.cpp:112] Iteration 60570, lr = 0.1
I0524 09:53:19.295647 11835 solver.cpp:239] Iteration 60580 (1.04669 iter/s, 9.55392s/10 iters), loss = 6.5098
I0524 09:53:19.295701 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5098 (* 1 = 6.5098 loss)
I0524 09:53:20.121409 11835 sgd_solver.cpp:112] Iteration 60580, lr = 0.1
I0524 09:53:29.410303 11835 solver.cpp:239] Iteration 60590 (0.988708 iter/s, 10.1142s/10 iters), loss = 5.80709
I0524 09:53:29.410367 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80709 (* 1 = 5.80709 loss)
I0524 09:53:29.454783 11835 sgd_solver.cpp:112] Iteration 60590, lr = 0.1
I0524 09:53:35.052361 11835 solver.cpp:239] Iteration 60600 (1.77249 iter/s, 5.64178s/10 iters), loss = 5.74787
I0524 09:53:35.052614 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.74787 (* 1 = 5.74787 loss)
I0524 09:53:35.052659 11835 sgd_solver.cpp:112] Iteration 60600, lr = 0.1
I0524 09:53:41.763523 11835 solver.cpp:239] Iteration 60610 (1.49017 iter/s, 6.71064s/10 iters), loss = 6.64908
I0524 09:53:41.763577 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.64908 (* 1 = 6.64908 loss)
I0524 09:53:41.763593 11835 sgd_solver.cpp:112] Iteration 60610, lr = 0.1
I0524 09:53:47.509312 11835 solver.cpp:239] Iteration 60620 (1.74049 iter/s, 5.74552s/10 iters), loss = 5.7643
I0524 09:53:47.509357 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.7643 (* 1 = 5.7643 loss)
I0524 09:53:47.903273 11835 sgd_solver.cpp:112] Iteration 60620, lr = 0.1
I0524 09:53:53.913704 11835 solver.cpp:239] Iteration 60630 (1.5615 iter/s, 6.4041s/10 iters), loss = 5.99739
I0524 09:53:53.913745 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.99739 (* 1 = 5.99739 loss)
I0524 09:53:54.705931 11835 sgd_solver.cpp:112] Iteration 60630, lr = 0.1
I0524 09:54:02.332815 11835 solver.cpp:239] Iteration 60640 (1.18783 iter/s, 8.41873s/10 iters), loss = 6.93743
I0524 09:54:02.332871 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.93743 (* 1 = 6.93743 loss)
I0524 09:54:02.364405 11835 sgd_solver.cpp:112] Iteration 60640, lr = 0.1
I0524 09:54:09.556126 11835 solver.cpp:239] Iteration 60650 (1.38447 iter/s, 7.22297s/10 iters), loss = 5.10698
I0524 09:54:09.556334 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.10698 (* 1 = 5.10698 loss)
I0524 09:54:09.556376 11835 sgd_solver.cpp:112] Iteration 60650, lr = 0.1
I0524 09:54:19.400182 11835 solver.cpp:239] Iteration 60660 (1.01591 iter/s, 9.84339s/10 iters), loss = 5.85886
I0524 09:54:19.400228 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85886 (* 1 = 5.85886 loss)
I0524 09:54:19.400341 11835 sgd_solver.cpp:112] Iteration 60660, lr = 0.1
I0524 09:54:25.817061 11835 solver.cpp:239] Iteration 60670 (1.55847 iter/s, 6.41656s/10 iters), loss = 6.10766
I0524 09:54:25.817179 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.10766 (* 1 = 6.10766 loss)
I0524 09:54:25.817327 11835 sgd_solver.cpp:112] Iteration 60670, lr = 0.1
I0524 09:54:32.018146 11835 solver.cpp:239] Iteration 60680 (1.6127 iter/s, 6.20079s/10 iters), loss = 5.26232
I0524 09:54:32.018189 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.26232 (* 1 = 5.26232 loss)
I0524 09:54:32.018203 11835 sgd_solver.cpp:112] Iteration 60680, lr = 0.1
I0524 09:54:40.319669 11835 solver.cpp:239] Iteration 60690 (1.20467 iter/s, 8.30101s/10 iters), loss = 6.28882
I0524 09:54:40.319898 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.28882 (* 1 = 6.28882 loss)
I0524 09:54:40.319926 11835 sgd_solver.cpp:112] Iteration 60690, lr = 0.1
I0524 09:54:46.941329 11835 solver.cpp:239] Iteration 60700 (1.5104 iter/s, 6.62076s/10 iters), loss = 6.14029
I0524 09:54:46.941435 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14029 (* 1 = 6.14029 loss)
I0524 09:54:46.941464 11835 sgd_solver.cpp:112] Iteration 60700, lr = 0.1
I0524 09:54:53.222782 11835 solver.cpp:239] Iteration 60710 (1.59206 iter/s, 6.28117s/10 iters), loss = 5.93138
I0524 09:54:53.222826 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93138 (* 1 = 5.93138 loss)
I0524 09:54:54.077229 11835 sgd_solver.cpp:112] Iteration 60710, lr = 0.1
I0524 09:55:00.888146 11835 solver.cpp:239] Iteration 60720 (1.30463 iter/s, 7.665s/10 iters), loss = 5.96549
I0524 09:55:00.888200 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96549 (* 1 = 5.96549 loss)
I0524 09:55:00.901325 11835 sgd_solver.cpp:112] Iteration 60720, lr = 0.1
I0524 09:55:08.966214 11835 solver.cpp:239] Iteration 60730 (1.23798 iter/s, 8.07768s/10 iters), loss = 5.85128
I0524 09:55:08.966286 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.85128 (* 1 = 5.85128 loss)
I0524 09:55:08.985323 11835 sgd_solver.cpp:112] Iteration 60730, lr = 0.1
I0524 09:55:16.028820 11835 solver.cpp:239] Iteration 60740 (1.41598 iter/s, 7.06226s/10 iters), loss = 6.71949
I0524 09:55:16.029108 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.71949 (* 1 = 6.71949 loss)
I0524 09:55:16.029161 11835 sgd_solver.cpp:112] Iteration 60740, lr = 0.1
I0524 09:55:23.537189 11835 solver.cpp:239] Iteration 60750 (1.33195 iter/s, 7.5078s/10 iters), loss = 6.2088
I0524 09:55:23.537250 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.2088 (* 1 = 6.2088 loss)
I0524 09:55:23.537511 11835 sgd_solver.cpp:112] Iteration 60750, lr = 0.1
I0524 09:55:30.840607 11835 solver.cpp:239] Iteration 60760 (1.36929 iter/s, 7.30308s/10 iters), loss = 6.599
I0524 09:55:30.840659 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.599 (* 1 = 6.599 loss)
I0524 09:55:30.840786 11835 sgd_solver.cpp:112] Iteration 60760, lr = 0.1
I0524 09:55:38.103876 11835 solver.cpp:239] Iteration 60770 (1.37685 iter/s, 7.26294s/10 iters), loss = 5.9363
I0524 09:55:38.103920 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9363 (* 1 = 5.9363 loss)
I0524 09:55:38.104094 11835 sgd_solver.cpp:112] Iteration 60770, lr = 0.1
I0524 09:55:44.961984 11835 solver.cpp:239] Iteration 60780 (1.45819 iter/s, 6.85779s/10 iters), loss = 6.26993
I0524 09:55:44.962039 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26993 (* 1 = 6.26993 loss)
I0524 09:55:44.962055 11835 sgd_solver.cpp:112] Iteration 60780, lr = 0.1
I0524 09:55:52.640135 11835 solver.cpp:239] Iteration 60790 (1.30283 iter/s, 7.67561s/10 iters), loss = 6.09519
I0524 09:55:52.640362 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09519 (* 1 = 6.09519 loss)
I0524 09:55:52.821552 11835 sgd_solver.cpp:112] Iteration 60790, lr = 0.1
I0524 09:56:00.297672 11835 solver.cpp:239] Iteration 60800 (1.30599 iter/s, 7.65705s/10 iters), loss = 6.54204
I0524 09:56:00.297722 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.54204 (* 1 = 6.54204 loss)
I0524 09:56:00.392313 11835 sgd_solver.cpp:112] Iteration 60800, lr = 0.1
I0524 09:56:08.390362 11835 solver.cpp:239] Iteration 60810 (1.23574 iter/s, 8.09229s/10 iters), loss = 5.87729
I0524 09:56:08.390439 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87729 (* 1 = 5.87729 loss)
I0524 09:56:08.390497 11835 sgd_solver.cpp:112] Iteration 60810, lr = 0.1
I0524 09:56:16.794291 11835 solver.cpp:239] Iteration 60820 (1.18997 iter/s, 8.40354s/10 iters), loss = 6.76062
I0524 09:56:16.794342 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.76062 (* 1 = 6.76062 loss)
I0524 09:56:17.630754 11835 sgd_solver.cpp:112] Iteration 60820, lr = 0.1
I0524 09:56:25.703032 11835 solver.cpp:239] Iteration 60830 (1.12254 iter/s, 8.90835s/10 iters), loss = 5.46688
I0524 09:56:25.703248 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.46688 (* 1 = 5.46688 loss)
I0524 09:56:25.971899 11835 sgd_solver.cpp:112] Iteration 60830, lr = 0.1
I0524 09:56:32.630276 11835 solver.cpp:239] Iteration 60840 (1.44367 iter/s, 6.9268s/10 iters), loss = 5.92045
I0524 09:56:32.630328 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92045 (* 1 = 5.92045 loss)
I0524 09:56:32.630514 11835 sgd_solver.cpp:112] Iteration 60840, lr = 0.1
I0524 09:56:38.699424 11835 solver.cpp:239] Iteration 60850 (1.64776 iter/s, 6.06884s/10 iters), loss = 5.94572
I0524 09:56:38.699550 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94572 (* 1 = 5.94572 loss)
I0524 09:56:39.308288 11835 sgd_solver.cpp:112] Iteration 60850, lr = 0.1
I0524 09:56:49.147320 11835 solver.cpp:239] Iteration 60860 (0.957173 iter/s, 10.4474s/10 iters), loss = 6.1851
I0524 09:56:49.147382 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1851 (* 1 = 6.1851 loss)
I0524 09:56:49.147399 11835 sgd_solver.cpp:112] Iteration 60860, lr = 0.1
I0524 09:56:55.022183 11835 solver.cpp:239] Iteration 60870 (1.7029 iter/s, 5.87233s/10 iters), loss = 5.82805
I0524 09:56:55.022291 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82805 (* 1 = 5.82805 loss)
I0524 09:56:55.994951 11835 sgd_solver.cpp:112] Iteration 60870, lr = 0.1
I0524 09:57:04.649152 11835 solver.cpp:239] Iteration 60880 (1.03879 iter/s, 9.62655s/10 iters), loss = 5.67422
I0524 09:57:04.649204 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.67422 (* 1 = 5.67422 loss)
I0524 09:57:04.865012 11835 sgd_solver.cpp:112] Iteration 60880, lr = 0.1
I0524 09:57:13.000721 11835 solver.cpp:239] Iteration 60890 (1.19743 iter/s, 8.3512s/10 iters), loss = 5.50152
I0524 09:57:13.000788 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.50152 (* 1 = 5.50152 loss)
I0524 09:57:13.000813 11835 sgd_solver.cpp:112] Iteration 60890, lr = 0.1
I0524 09:57:19.609959 11835 solver.cpp:239] Iteration 60900 (1.51311 iter/s, 6.60891s/10 iters), loss = 5.51109
I0524 09:57:19.610015 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.51109 (* 1 = 5.51109 loss)
I0524 09:57:19.610227 11835 sgd_solver.cpp:112] Iteration 60900, lr = 0.1
I0524 09:57:26.549733 11835 solver.cpp:239] Iteration 60910 (1.44104 iter/s, 6.93945s/10 iters), loss = 5.55726
I0524 09:57:26.549885 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.55726 (* 1 = 5.55726 loss)
I0524 09:57:27.438678 11835 sgd_solver.cpp:112] Iteration 60910, lr = 0.1
I0524 09:57:33.944571 11835 solver.cpp:239] Iteration 60920 (1.35237 iter/s, 7.3944s/10 iters), loss = 6.0829
I0524 09:57:33.944622 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.0829 (* 1 = 6.0829 loss)
I0524 09:57:33.944638 11835 sgd_solver.cpp:112] Iteration 60920, lr = 0.1
I0524 09:57:42.358847 11835 solver.cpp:239] Iteration 60930 (1.18852 iter/s, 8.41384s/10 iters), loss = 6.57092
I0524 09:57:42.358888 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57092 (* 1 = 6.57092 loss)
I0524 09:57:43.236639 11835 sgd_solver.cpp:112] Iteration 60930, lr = 0.1
I0524 09:57:49.730916 11835 solver.cpp:239] Iteration 60940 (1.35653 iter/s, 7.37173s/10 iters), loss = 6.26014
I0524 09:57:49.730960 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.26014 (* 1 = 6.26014 loss)
I0524 09:57:49.731057 11835 sgd_solver.cpp:112] Iteration 60940, lr = 0.1
I0524 09:57:55.319931 11835 solver.cpp:239] Iteration 60950 (1.78931 iter/s, 5.58875s/10 iters), loss = 5.60435
I0524 09:57:55.319980 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.60435 (* 1 = 5.60435 loss)
I0524 09:57:55.319998 11835 sgd_solver.cpp:112] Iteration 60950, lr = 0.1
I0524 09:58:01.836714 11835 solver.cpp:239] Iteration 60960 (1.53457 iter/s, 6.51648s/10 iters), loss = 5.91155
I0524 09:58:01.836920 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.91155 (* 1 = 5.91155 loss)
I0524 09:58:03.205584 11835 sgd_solver.cpp:112] Iteration 60960, lr = 0.1
I0524 09:58:11.904090 11835 solver.cpp:239] Iteration 60970 (0.993366 iter/s, 10.0668s/10 iters), loss = 6.67076
I0524 09:58:11.904151 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.67076 (* 1 = 6.67076 loss)
I0524 09:58:11.904269 11835 sgd_solver.cpp:112] Iteration 60970, lr = 0.1
I0524 09:58:18.688814 11835 solver.cpp:239] Iteration 60980 (1.47397 iter/s, 6.78439s/10 iters), loss = 5.21988
I0524 09:58:18.688874 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.21988 (* 1 = 5.21988 loss)
I0524 09:58:18.689095 11835 sgd_solver.cpp:112] Iteration 60980, lr = 0.1
I0524 09:58:26.590060 11835 solver.cpp:239] Iteration 60990 (1.26569 iter/s, 7.90083s/10 iters), loss = 6.31
I0524 09:58:26.590239 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.31 (* 1 = 6.31 loss)
I0524 09:58:26.590286 11835 sgd_solver.cpp:112] Iteration 60990, lr = 0.1
I0524 09:58:34.885044 11835 solver.cpp:239] Iteration 61000 (1.20577 iter/s, 8.29343s/10 iters), loss = 5.79088
I0524 09:58:34.885274 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.79088 (* 1 = 5.79088 loss)
I0524 09:58:34.885295 11835 sgd_solver.cpp:112] Iteration 61000, lr = 0.1
I0524 09:58:41.715457 11835 solver.cpp:239] Iteration 61010 (1.46428 iter/s, 6.82932s/10 iters), loss = 6.44663
I0524 09:58:41.715509 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44663 (* 1 = 6.44663 loss)
I0524 09:58:41.715564 11835 sgd_solver.cpp:112] Iteration 61010, lr = 0.1
I0524 09:58:48.417237 11835 solver.cpp:239] Iteration 61020 (1.49221 iter/s, 6.70145s/10 iters), loss = 6.34233
I0524 09:58:48.417294 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34233 (* 1 = 6.34233 loss)
I0524 09:58:48.787017 11835 sgd_solver.cpp:112] Iteration 61020, lr = 0.1
I0524 09:58:55.779762 11835 solver.cpp:239] Iteration 61030 (1.35829 iter/s, 7.36219s/10 iters), loss = 6.5651
I0524 09:58:55.779806 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.5651 (* 1 = 6.5651 loss)
I0524 09:58:55.779827 11835 sgd_solver.cpp:112] Iteration 61030, lr = 0.1
I0524 09:59:03.163764 11835 solver.cpp:239] Iteration 61040 (1.35435 iter/s, 7.38361s/10 iters), loss = 5.11364
I0524 09:59:03.163902 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.11364 (* 1 = 5.11364 loss)
I0524 09:59:03.163935 11835 sgd_solver.cpp:112] Iteration 61040, lr = 0.1
I0524 09:59:10.026615 11835 solver.cpp:239] Iteration 61050 (1.45721 iter/s, 6.86245s/10 iters), loss = 6.85494
I0524 09:59:10.026762 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.85494 (* 1 = 6.85494 loss)
I0524 09:59:10.026847 11835 sgd_solver.cpp:112] Iteration 61050, lr = 0.1
I0524 09:59:21.580484 11835 solver.cpp:239] Iteration 61060 (0.865554 iter/s, 11.5533s/10 iters), loss = 6.15029
I0524 09:59:21.580523 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15029 (* 1 = 6.15029 loss)
I0524 09:59:22.675665 11835 sgd_solver.cpp:112] Iteration 61060, lr = 0.1
I0524 09:59:29.648965 11835 solver.cpp:239] Iteration 61070 (1.23945 iter/s, 8.0681s/10 iters), loss = 6.48748
I0524 09:59:29.649027 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.48748 (* 1 = 6.48748 loss)
I0524 09:59:29.649245 11835 sgd_solver.cpp:112] Iteration 61070, lr = 0.1
I0524 09:59:35.906350 11835 solver.cpp:239] Iteration 61080 (1.59819 iter/s, 6.25707s/10 iters), loss = 4.93894
I0524 09:59:35.906409 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.93894 (* 1 = 4.93894 loss)
I0524 09:59:35.906651 11835 sgd_solver.cpp:112] Iteration 61080, lr = 0.1
I0524 09:59:42.825460 11835 solver.cpp:239] Iteration 61090 (1.44534 iter/s, 6.91879s/10 iters), loss = 5.98196
I0524 09:59:42.825654 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.98196 (* 1 = 5.98196 loss)
I0524 09:59:42.844794 11835 sgd_solver.cpp:112] Iteration 61090, lr = 0.1
I0524 09:59:48.759660 11835 solver.cpp:239] Iteration 61100 (1.68526 iter/s, 5.9338s/10 iters), loss = 6.15206
I0524 09:59:48.759696 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.15206 (* 1 = 6.15206 loss)
I0524 09:59:48.759709 11835 sgd_solver.cpp:112] Iteration 61100, lr = 0.1
I0524 09:59:54.807262 11835 solver.cpp:239] Iteration 61110 (1.65363 iter/s, 6.04732s/10 iters), loss = 6.17463
I0524 09:59:54.807320 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17463 (* 1 = 6.17463 loss)
I0524 09:59:54.807404 11835 sgd_solver.cpp:112] Iteration 61110, lr = 0.1
I0524 10:00:04.287292 11835 solver.cpp:239] Iteration 61120 (1.0549 iter/s, 9.4796s/10 iters), loss = 5.78977
I0524 10:00:04.287349 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.78977 (* 1 = 5.78977 loss)
I0524 10:00:04.287364 11835 sgd_solver.cpp:112] Iteration 61120, lr = 0.1
I0524 10:00:11.088627 11835 solver.cpp:239] Iteration 61130 (1.47041 iter/s, 6.80081s/10 iters), loss = 5.9912
I0524 10:00:11.088706 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.9912 (* 1 = 5.9912 loss)
I0524 10:00:11.088789 11835 sgd_solver.cpp:112] Iteration 61130, lr = 0.1
I0524 10:00:17.308785 11835 solver.cpp:239] Iteration 61140 (1.60776 iter/s, 6.21984s/10 iters), loss = 5.81031
I0524 10:00:17.308992 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81031 (* 1 = 5.81031 loss)
I0524 10:00:17.685834 11835 sgd_solver.cpp:112] Iteration 61140, lr = 0.1
I0524 10:00:26.525351 11835 solver.cpp:239] Iteration 61150 (1.08507 iter/s, 9.21601s/10 iters), loss = 6.28973
I0524 10:00:26.525405 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.28973 (* 1 = 6.28973 loss)
I0524 10:00:26.601291 11835 sgd_solver.cpp:112] Iteration 61150, lr = 0.1
I0524 10:00:34.474162 11835 solver.cpp:239] Iteration 61160 (1.25811 iter/s, 7.94845s/10 iters), loss = 6.01501
I0524 10:00:34.474215 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01501 (* 1 = 6.01501 loss)
I0524 10:00:34.503855 11835 sgd_solver.cpp:112] Iteration 61160, lr = 0.1
I0524 10:00:40.389451 11835 solver.cpp:239] Iteration 61170 (1.69062 iter/s, 5.91501s/10 iters), loss = 5.14516
I0524 10:00:40.389511 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.14516 (* 1 = 5.14516 loss)
I0524 10:00:40.389528 11835 sgd_solver.cpp:112] Iteration 61170, lr = 0.1
I0524 10:00:46.656050 11835 solver.cpp:239] Iteration 61180 (1.59586 iter/s, 6.26623s/10 iters), loss = 5.72219
I0524 10:00:46.656124 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72219 (* 1 = 5.72219 loss)
I0524 10:00:46.913676 11835 sgd_solver.cpp:112] Iteration 61180, lr = 0.1
I0524 10:00:53.345602 11835 solver.cpp:239] Iteration 61190 (1.49494 iter/s, 6.68922s/10 iters), loss = 5.20371
I0524 10:00:53.345728 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.20371 (* 1 = 5.20371 loss)
I0524 10:00:53.364094 11835 sgd_solver.cpp:112] Iteration 61190, lr = 0.1
I0524 10:00:59.772115 11835 solver.cpp:239] Iteration 61200 (1.55614 iter/s, 6.42614s/10 iters), loss = 6.3159
I0524 10:00:59.772171 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3159 (* 1 = 6.3159 loss)
I0524 10:00:59.800088 11835 sgd_solver.cpp:112] Iteration 61200, lr = 0.1
I0524 10:01:06.809356 11835 solver.cpp:239] Iteration 61210 (1.42108 iter/s, 7.03691s/10 iters), loss = 6.60684
I0524 10:01:06.809417 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.60684 (* 1 = 6.60684 loss)
I0524 10:01:06.809597 11835 sgd_solver.cpp:112] Iteration 61210, lr = 0.1
I0524 10:01:13.844441 11835 solver.cpp:239] Iteration 61220 (1.42151 iter/s, 7.03476s/10 iters), loss = 5.38678
I0524 10:01:13.844494 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.38678 (* 1 = 5.38678 loss)
I0524 10:01:13.844682 11835 sgd_solver.cpp:112] Iteration 61220, lr = 0.1
I0524 10:01:20.252848 11835 solver.cpp:239] Iteration 61230 (1.56053 iter/s, 6.40806s/10 iters), loss = 5.96114
I0524 10:01:20.252918 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.96114 (* 1 = 5.96114 loss)
I0524 10:01:20.253159 11835 sgd_solver.cpp:112] Iteration 61230, lr = 0.1
I0524 10:01:26.868840 11835 solver.cpp:239] Iteration 61240 (1.51156 iter/s, 6.61568s/10 iters), loss = 5.86154
I0524 10:01:26.869112 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.86154 (* 1 = 5.86154 loss)
I0524 10:01:26.926858 11835 sgd_solver.cpp:112] Iteration 61240, lr = 0.1
I0524 10:01:32.896903 11835 solver.cpp:239] Iteration 61250 (1.65904 iter/s, 6.02759s/10 iters), loss = 6.47991
I0524 10:01:32.896955 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.47991 (* 1 = 6.47991 loss)
I0524 10:01:32.896967 11835 sgd_solver.cpp:112] Iteration 61250, lr = 0.1
I0524 10:01:39.429147 11835 solver.cpp:239] Iteration 61260 (1.53094 iter/s, 6.53192s/10 iters), loss = 6.17243
I0524 10:01:39.429205 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17243 (* 1 = 6.17243 loss)
I0524 10:01:39.429421 11835 sgd_solver.cpp:112] Iteration 61260, lr = 0.1
I0524 10:01:45.517376 11835 solver.cpp:239] Iteration 61270 (1.64259 iter/s, 6.08794s/10 iters), loss = 6.79314
I0524 10:01:45.517421 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.79314 (* 1 = 6.79314 loss)
I0524 10:01:45.517434 11835 sgd_solver.cpp:112] Iteration 61270, lr = 0.1
I0524 10:01:55.981986 11835 solver.cpp:239] Iteration 61280 (0.955643 iter/s, 10.4642s/10 iters), loss = 5.72221
I0524 10:01:55.982036 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.72221 (* 1 = 5.72221 loss)
I0524 10:01:57.133550 11835 sgd_solver.cpp:112] Iteration 61280, lr = 0.1
I0524 10:02:03.803215 11835 solver.cpp:239] Iteration 61290 (1.27863 iter/s, 7.82085s/10 iters), loss = 6.05466
I0524 10:02:03.803279 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05466 (* 1 = 6.05466 loss)
I0524 10:02:03.850826 11835 sgd_solver.cpp:112] Iteration 61290, lr = 0.1
I0524 10:02:10.853433 11835 solver.cpp:239] Iteration 61300 (1.41847 iter/s, 7.04987s/10 iters), loss = 6.20116
I0524 10:02:10.853495 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20116 (* 1 = 6.20116 loss)
I0524 10:02:10.853623 11835 sgd_solver.cpp:112] Iteration 61300, lr = 0.1
I0524 10:02:18.249068 11835 solver.cpp:239] Iteration 61310 (1.35221 iter/s, 7.3953s/10 iters), loss = 6.1731
I0524 10:02:18.249125 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1731 (* 1 = 6.1731 loss)
I0524 10:02:18.249212 11835 sgd_solver.cpp:112] Iteration 61310, lr = 0.1
I0524 10:02:24.499461 11835 solver.cpp:239] Iteration 61320 (1.59997 iter/s, 6.2501s/10 iters), loss = 6.14522
I0524 10:02:24.499511 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.14522 (* 1 = 6.14522 loss)
I0524 10:02:24.499713 11835 sgd_solver.cpp:112] Iteration 61320, lr = 0.1
I0524 10:02:31.270859 11835 solver.cpp:239] Iteration 61330 (1.47687 iter/s, 6.77109s/10 iters), loss = 6.09255
I0524 10:02:31.271016 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.09255 (* 1 = 6.09255 loss)
I0524 10:02:31.294306 11835 sgd_solver.cpp:112] Iteration 61330, lr = 0.1
I0524 10:02:41.315013 11835 solver.cpp:239] Iteration 61340 (0.995657 iter/s, 10.0436s/10 iters), loss = 5.52558
I0524 10:02:41.315062 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.52558 (* 1 = 5.52558 loss)
I0524 10:02:42.197381 11835 sgd_solver.cpp:112] Iteration 61340, lr = 0.1
I0524 10:02:48.674615 11835 solver.cpp:239] Iteration 61350 (1.35883 iter/s, 7.35926s/10 iters), loss = 5.94207
I0524 10:02:48.674670 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.94207 (* 1 = 5.94207 loss)
I0524 10:02:48.674966 11835 sgd_solver.cpp:112] Iteration 61350, lr = 0.1
I0524 10:02:55.939220 11835 solver.cpp:239] Iteration 61360 (1.3766 iter/s, 7.26426s/10 iters), loss = 7.1054
I0524 10:02:55.939277 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.1054 (* 1 = 7.1054 loss)
I0524 10:02:56.516027 11835 sgd_solver.cpp:112] Iteration 61360, lr = 0.1
I0524 10:03:04.204769 11835 solver.cpp:239] Iteration 61370 (1.20989 iter/s, 8.26518s/10 iters), loss = 5.76301
I0524 10:03:04.205015 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.76301 (* 1 = 5.76301 loss)
I0524 10:03:04.232559 11835 sgd_solver.cpp:112] Iteration 61370, lr = 0.1
I0524 10:03:11.642056 11835 solver.cpp:239] Iteration 61380 (1.34467 iter/s, 7.43675s/10 iters), loss = 5.82657
I0524 10:03:11.642115 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82657 (* 1 = 5.82657 loss)
I0524 10:03:11.642232 11835 sgd_solver.cpp:112] Iteration 61380, lr = 0.1
I0524 10:03:19.402959 11835 solver.cpp:239] Iteration 61390 (1.28857 iter/s, 7.76054s/10 iters), loss = 5.92344
I0524 10:03:19.403014 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.92344 (* 1 = 5.92344 loss)
I0524 10:03:19.403164 11835 sgd_solver.cpp:112] Iteration 61390, lr = 0.1
I0524 10:03:27.187691 11835 solver.cpp:239] Iteration 61400 (1.28462 iter/s, 7.78438s/10 iters), loss = 6.61552
I0524 10:03:27.187733 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.61552 (* 1 = 6.61552 loss)
I0524 10:03:27.231010 11835 sgd_solver.cpp:112] Iteration 61400, lr = 0.1
I0524 10:03:33.153415 11835 solver.cpp:239] Iteration 61410 (1.67632 iter/s, 5.96545s/10 iters), loss = 7.01991
I0524 10:03:33.153457 11835 solver.cpp:258]     Train net output #0: softmax_loss = 7.01991 (* 1 = 7.01991 loss)
I0524 10:03:33.153472 11835 sgd_solver.cpp:112] Iteration 61410, lr = 0.1
I0524 10:03:42.621363 11835 solver.cpp:239] Iteration 61420 (1.05624 iter/s, 9.46753s/10 iters), loss = 6.16395
I0524 10:03:42.626804 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.16395 (* 1 = 6.16395 loss)
I0524 10:03:42.684096 11835 sgd_solver.cpp:112] Iteration 61420, lr = 0.1
I0524 10:03:49.461192 11835 solver.cpp:239] Iteration 61430 (1.46324 iter/s, 6.83415s/10 iters), loss = 5.56545
I0524 10:03:49.461235 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.56545 (* 1 = 5.56545 loss)
I0524 10:03:49.461346 11835 sgd_solver.cpp:112] Iteration 61430, lr = 0.1
I0524 10:03:55.924881 11835 solver.cpp:239] Iteration 61440 (1.54718 iter/s, 6.46339s/10 iters), loss = 6.8211
I0524 10:03:55.924932 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.8211 (* 1 = 6.8211 loss)
I0524 10:03:55.925009 11835 sgd_solver.cpp:112] Iteration 61440, lr = 0.1
I0524 10:04:03.557529 11835 solver.cpp:239] Iteration 61450 (1.31022 iter/s, 7.63229s/10 iters), loss = 6.27997
I0524 10:04:03.557600 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.27997 (* 1 = 6.27997 loss)
I0524 10:04:03.641638 11835 sgd_solver.cpp:112] Iteration 61450, lr = 0.1
I0524 10:04:11.961675 11835 solver.cpp:239] Iteration 61460 (1.18994 iter/s, 8.40376s/10 iters), loss = 5.8118
I0524 10:04:11.961721 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.8118 (* 1 = 5.8118 loss)
I0524 10:04:11.961966 11835 sgd_solver.cpp:112] Iteration 61460, lr = 0.1
I0524 10:04:18.765338 11835 solver.cpp:239] Iteration 61470 (1.46987 iter/s, 6.80334s/10 iters), loss = 6.53587
I0524 10:04:18.765604 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.53587 (* 1 = 6.53587 loss)
I0524 10:04:18.765682 11835 sgd_solver.cpp:112] Iteration 61470, lr = 0.1
I0524 10:04:27.164847 11835 solver.cpp:239] Iteration 61480 (1.19066 iter/s, 8.39868s/10 iters), loss = 5.40457
I0524 10:04:27.164906 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.40457 (* 1 = 5.40457 loss)
I0524 10:04:27.165076 11835 sgd_solver.cpp:112] Iteration 61480, lr = 0.1
I0524 10:04:34.803899 11835 solver.cpp:239] Iteration 61490 (1.30912 iter/s, 7.6387s/10 iters), loss = 5.58261
I0524 10:04:34.803948 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.58261 (* 1 = 5.58261 loss)
I0524 10:04:35.274183 11835 sgd_solver.cpp:112] Iteration 61490, lr = 0.1
I0524 10:04:42.473639 11835 solver.cpp:239] Iteration 61500 (1.30388 iter/s, 7.6694s/10 iters), loss = 5.87336
I0524 10:04:42.473678 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.87336 (* 1 = 5.87336 loss)
I0524 10:04:42.518790 11835 sgd_solver.cpp:112] Iteration 61500, lr = 0.1
I0524 10:04:49.540405 11835 solver.cpp:239] Iteration 61510 (1.41514 iter/s, 7.06644s/10 iters), loss = 5.84335
I0524 10:04:49.540570 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.84335 (* 1 = 5.84335 loss)
I0524 10:04:49.540588 11835 sgd_solver.cpp:112] Iteration 61510, lr = 0.1
I0524 10:04:59.492202 11835 solver.cpp:239] Iteration 61520 (1.00492 iter/s, 9.95105s/10 iters), loss = 6.50843
I0524 10:04:59.492256 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.50843 (* 1 = 6.50843 loss)
I0524 10:04:59.715276 11835 sgd_solver.cpp:112] Iteration 61520, lr = 0.1
I0524 10:05:06.357224 11835 solver.cpp:239] Iteration 61530 (1.45673 iter/s, 6.8647s/10 iters), loss = 6.19555
I0524 10:05:06.357277 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19555 (* 1 = 6.19555 loss)
I0524 10:05:06.357293 11835 sgd_solver.cpp:112] Iteration 61530, lr = 0.1
I0524 10:05:12.614143 11835 solver.cpp:239] Iteration 61540 (1.59859 iter/s, 6.25553s/10 iters), loss = 6.01583
I0524 10:05:12.614189 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.01583 (* 1 = 6.01583 loss)
I0524 10:05:12.614432 11835 sgd_solver.cpp:112] Iteration 61540, lr = 0.1
I0524 10:05:23.344933 11835 solver.cpp:239] Iteration 61550 (0.931938 iter/s, 10.7303s/10 iters), loss = 5.80185
I0524 10:05:23.345178 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.80185 (* 1 = 5.80185 loss)
I0524 10:05:23.345213 11835 sgd_solver.cpp:112] Iteration 61550, lr = 0.1
I0524 10:05:29.465956 11835 solver.cpp:239] Iteration 61560 (1.63413 iter/s, 6.11947s/10 iters), loss = 6.90782
I0524 10:05:29.466084 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.90782 (* 1 = 6.90782 loss)
I0524 10:05:29.466122 11835 sgd_solver.cpp:112] Iteration 61560, lr = 0.1
I0524 10:05:35.661170 11835 solver.cpp:239] Iteration 61570 (1.61429 iter/s, 6.19466s/10 iters), loss = 6.36388
I0524 10:05:35.661226 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.36388 (* 1 = 6.36388 loss)
I0524 10:05:35.661352 11835 sgd_solver.cpp:112] Iteration 61570, lr = 0.1
I0524 10:05:44.802120 11835 solver.cpp:239] Iteration 61580 (1.09403 iter/s, 9.14055s/10 iters), loss = 5.93747
I0524 10:05:44.802217 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.93747 (* 1 = 5.93747 loss)
I0524 10:05:44.802258 11835 sgd_solver.cpp:112] Iteration 61580, lr = 0.1
I0524 10:05:52.224279 11835 solver.cpp:239] Iteration 61590 (1.34749 iter/s, 7.42121s/10 iters), loss = 6.17593
I0524 10:05:52.224336 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.17593 (* 1 = 6.17593 loss)
I0524 10:05:52.224525 11835 sgd_solver.cpp:112] Iteration 61590, lr = 0.1
I0524 10:05:59.595183 11835 solver.cpp:239] Iteration 61600 (1.35675 iter/s, 7.37056s/10 iters), loss = 6.34305
I0524 10:05:59.595324 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.34305 (* 1 = 6.34305 loss)
I0524 10:05:59.595342 11835 sgd_solver.cpp:112] Iteration 61600, lr = 0.1
I0524 10:06:07.320199 11835 solver.cpp:239] Iteration 61610 (1.29459 iter/s, 7.72447s/10 iters), loss = 6.92939
I0524 10:06:07.320256 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.92939 (* 1 = 6.92939 loss)
I0524 10:06:07.397430 11835 sgd_solver.cpp:112] Iteration 61610, lr = 0.1
I0524 10:06:13.902981 11835 solver.cpp:239] Iteration 61620 (1.51919 iter/s, 6.58245s/10 iters), loss = 6.11612
I0524 10:06:13.903050 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.11612 (* 1 = 6.11612 loss)
I0524 10:06:14.260255 11835 sgd_solver.cpp:112] Iteration 61620, lr = 0.1
I0524 10:06:20.444756 11835 solver.cpp:239] Iteration 61630 (1.52871 iter/s, 6.54145s/10 iters), loss = 6.25051
I0524 10:06:20.444830 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25051 (* 1 = 6.25051 loss)
I0524 10:06:20.445047 11835 sgd_solver.cpp:112] Iteration 61630, lr = 0.1
I0524 10:06:27.961622 11835 solver.cpp:239] Iteration 61640 (1.3304 iter/s, 7.51652s/10 iters), loss = 6.9713
I0524 10:06:27.961671 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.9713 (* 1 = 6.9713 loss)
I0524 10:06:27.961786 11835 sgd_solver.cpp:112] Iteration 61640, lr = 0.1
I0524 10:06:34.703114 11835 solver.cpp:239] Iteration 61650 (1.48342 iter/s, 6.74117s/10 iters), loss = 5.62376
I0524 10:06:34.703260 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.62376 (* 1 = 5.62376 loss)
I0524 10:06:34.703317 11835 sgd_solver.cpp:112] Iteration 61650, lr = 0.1
I0524 10:06:41.605293 11835 solver.cpp:239] Iteration 61660 (1.4489 iter/s, 6.90179s/10 iters), loss = 6.45564
I0524 10:06:41.605342 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.45564 (* 1 = 6.45564 loss)
I0524 10:06:42.292170 11835 sgd_solver.cpp:112] Iteration 61660, lr = 0.1
I0524 10:06:50.223804 11835 solver.cpp:239] Iteration 61670 (1.16034 iter/s, 8.61813s/10 iters), loss = 6.20117
I0524 10:06:50.223865 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.20117 (* 1 = 6.20117 loss)
I0524 10:06:51.175930 11835 sgd_solver.cpp:112] Iteration 61670, lr = 0.1
I0524 10:06:59.078426 11835 solver.cpp:239] Iteration 61680 (1.12941 iter/s, 8.85419s/10 iters), loss = 4.82635
I0524 10:06:59.078532 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.82635 (* 1 = 4.82635 loss)
I0524 10:06:59.078656 11835 sgd_solver.cpp:112] Iteration 61680, lr = 0.1
I0524 10:07:05.649968 11835 solver.cpp:239] Iteration 61690 (1.5218 iter/s, 6.57115s/10 iters), loss = 5.41072
I0524 10:07:05.650225 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.41072 (* 1 = 5.41072 loss)
I0524 10:07:05.650255 11835 sgd_solver.cpp:112] Iteration 61690, lr = 0.1
I0524 10:07:14.982553 11835 solver.cpp:239] Iteration 61700 (1.07182 iter/s, 9.32989s/10 iters), loss = 6.3881
I0524 10:07:14.982607 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.3881 (* 1 = 6.3881 loss)
I0524 10:07:15.387384 11835 sgd_solver.cpp:112] Iteration 61700, lr = 0.1
I0524 10:07:22.842001 11835 solver.cpp:239] Iteration 61710 (1.27242 iter/s, 7.85907s/10 iters), loss = 5.68835
I0524 10:07:22.842089 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68835 (* 1 = 5.68835 loss)
I0524 10:07:22.842108 11835 sgd_solver.cpp:112] Iteration 61710, lr = 0.1
I0524 10:07:29.713110 11835 solver.cpp:239] Iteration 61720 (1.45548 iter/s, 6.87058s/10 iters), loss = 5.68405
I0524 10:07:29.713165 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.68405 (* 1 = 5.68405 loss)
I0524 10:07:30.541826 11835 sgd_solver.cpp:112] Iteration 61720, lr = 0.1
I0524 10:07:36.643353 11835 solver.cpp:239] Iteration 61730 (1.44302 iter/s, 6.92992s/10 iters), loss = 6.41283
I0524 10:07:36.643795 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41283 (* 1 = 6.41283 loss)
I0524 10:07:36.643829 11835 sgd_solver.cpp:112] Iteration 61730, lr = 0.1
I0524 10:07:44.215632 11835 solver.cpp:239] Iteration 61740 (1.32074 iter/s, 7.57153s/10 iters), loss = 6.7832
I0524 10:07:44.215677 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.7832 (* 1 = 6.7832 loss)
I0524 10:07:44.217638 11835 sgd_solver.cpp:112] Iteration 61740, lr = 0.1
I0524 10:07:52.983682 11835 solver.cpp:239] Iteration 61750 (1.14056 iter/s, 8.76765s/10 iters), loss = 6.19236
I0524 10:07:52.983731 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.19236 (* 1 = 6.19236 loss)
I0524 10:07:53.054266 11835 sgd_solver.cpp:112] Iteration 61750, lr = 0.1
I0524 10:08:00.662181 11835 solver.cpp:239] Iteration 61760 (1.3024 iter/s, 7.67815s/10 iters), loss = 6.05489
I0524 10:08:00.662256 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.05489 (* 1 = 6.05489 loss)
I0524 10:08:00.716296 11835 sgd_solver.cpp:112] Iteration 61760, lr = 0.1
I0524 10:08:06.483995 11835 solver.cpp:239] Iteration 61770 (1.71776 iter/s, 5.82154s/10 iters), loss = 6.49495
I0524 10:08:06.484045 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49495 (* 1 = 6.49495 loss)
I0524 10:08:06.484060 11835 sgd_solver.cpp:112] Iteration 61770, lr = 0.1
I0524 10:08:17.344067 11835 solver.cpp:239] Iteration 61780 (0.920849 iter/s, 10.8595s/10 iters), loss = 6.49068
I0524 10:08:17.344303 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.49068 (* 1 = 6.49068 loss)
I0524 10:08:17.344347 11835 sgd_solver.cpp:112] Iteration 61780, lr = 0.1
I0524 10:08:24.075326 11835 solver.cpp:239] Iteration 61790 (1.48599 iter/s, 6.7295s/10 iters), loss = 6.13443
I0524 10:08:24.075378 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.13443 (* 1 = 6.13443 loss)
I0524 10:08:24.075510 11835 sgd_solver.cpp:112] Iteration 61790, lr = 0.1
I0524 10:08:30.814734 11835 solver.cpp:239] Iteration 61800 (1.48388 iter/s, 6.73908s/10 iters), loss = 5.81653
I0524 10:08:30.814784 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.81653 (* 1 = 5.81653 loss)
I0524 10:08:31.040129 11835 sgd_solver.cpp:112] Iteration 61800, lr = 0.1
I0524 10:08:39.171794 11835 solver.cpp:239] Iteration 61810 (1.19665 iter/s, 8.35668s/10 iters), loss = 5.43232
I0524 10:08:39.171854 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.43232 (* 1 = 5.43232 loss)
I0524 10:08:40.015242 11835 sgd_solver.cpp:112] Iteration 61810, lr = 0.1
I0524 10:08:48.276680 11835 solver.cpp:239] Iteration 61820 (1.09836 iter/s, 9.10448s/10 iters), loss = 6.33956
I0524 10:08:48.276978 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.33956 (* 1 = 6.33956 loss)
I0524 10:08:48.277034 11835 sgd_solver.cpp:112] Iteration 61820, lr = 0.1
I0524 10:08:54.556620 11835 solver.cpp:239] Iteration 61830 (1.59251 iter/s, 6.2794s/10 iters), loss = 5.35981
I0524 10:08:54.556685 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.35981 (* 1 = 5.35981 loss)
I0524 10:08:55.504110 11835 sgd_solver.cpp:112] Iteration 61830, lr = 0.1
I0524 10:09:01.520910 11835 solver.cpp:239] Iteration 61840 (1.43596 iter/s, 6.96396s/10 iters), loss = 6.41668
I0524 10:09:01.520956 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.41668 (* 1 = 6.41668 loss)
I0524 10:09:01.521152 11835 sgd_solver.cpp:112] Iteration 61840, lr = 0.1
I0524 10:09:11.519654 11835 solver.cpp:239] Iteration 61850 (1.00017 iter/s, 9.9983s/10 iters), loss = 6.4538
I0524 10:09:11.519718 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.4538 (* 1 = 6.4538 loss)
I0524 10:09:12.136620 11835 sgd_solver.cpp:112] Iteration 61850, lr = 0.1
I0524 10:09:18.134730 11835 solver.cpp:239] Iteration 61860 (1.51177 iter/s, 6.61476s/10 iters), loss = 5.82038
I0524 10:09:18.134783 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.82038 (* 1 = 5.82038 loss)
I0524 10:09:18.134799 11835 sgd_solver.cpp:112] Iteration 61860, lr = 0.1
I0524 10:09:25.132884 11835 solver.cpp:239] Iteration 61870 (1.42904 iter/s, 6.99773s/10 iters), loss = 6.44935
I0524 10:09:25.133126 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.44935 (* 1 = 6.44935 loss)
I0524 10:09:25.308320 11835 sgd_solver.cpp:112] Iteration 61870, lr = 0.1
I0524 10:09:32.036128 11835 solver.cpp:239] Iteration 61880 (1.44869 iter/s, 6.90279s/10 iters), loss = 6.25299
I0524 10:09:32.036172 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.25299 (* 1 = 6.25299 loss)
I0524 10:09:32.139420 11835 sgd_solver.cpp:112] Iteration 61880, lr = 0.1
I0524 10:09:37.811205 11835 solver.cpp:239] Iteration 61890 (1.73166 iter/s, 5.7748s/10 iters), loss = 6.1326
I0524 10:09:37.811254 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.1326 (* 1 = 6.1326 loss)
I0524 10:09:37.811357 11835 sgd_solver.cpp:112] Iteration 61890, lr = 0.1
I0524 10:09:45.963881 11835 solver.cpp:239] Iteration 61900 (1.22665 iter/s, 8.15231s/10 iters), loss = 5.40857
I0524 10:09:45.963920 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.40857 (* 1 = 5.40857 loss)
I0524 10:09:46.052587 11835 sgd_solver.cpp:112] Iteration 61900, lr = 0.1
I0524 10:09:52.398738 11835 solver.cpp:239] Iteration 61910 (1.55412 iter/s, 6.43453s/10 iters), loss = 4.80049
I0524 10:09:52.398787 11835 solver.cpp:258]     Train net output #0: softmax_loss = 4.80049 (* 1 = 4.80049 loss)
I0524 10:09:53.640668 11835 sgd_solver.cpp:112] Iteration 61910, lr = 0.1
I0524 10:10:00.024729 11835 solver.cpp:239] Iteration 61920 (1.31136 iter/s, 7.62565s/10 iters), loss = 5.00932
I0524 10:10:00.024888 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.00932 (* 1 = 5.00932 loss)
I0524 10:10:00.809744 11835 sgd_solver.cpp:112] Iteration 61920, lr = 0.1
I0524 10:10:08.247102 11835 solver.cpp:239] Iteration 61930 (1.21627 iter/s, 8.22188s/10 iters), loss = 5.70395
I0524 10:10:08.247156 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.70395 (* 1 = 5.70395 loss)
I0524 10:10:08.247205 11835 sgd_solver.cpp:112] Iteration 61930, lr = 0.1
I0524 10:10:16.687093 11835 solver.cpp:239] Iteration 61940 (1.18489 iter/s, 8.43961s/10 iters), loss = 6.68633
I0524 10:10:16.687146 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.68633 (* 1 = 6.68633 loss)
I0524 10:10:16.730478 11835 sgd_solver.cpp:112] Iteration 61940, lr = 0.1
I0524 10:10:22.796218 11835 solver.cpp:239] Iteration 61950 (1.63698 iter/s, 6.10883s/10 iters), loss = 6.69887
I0524 10:10:22.796283 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.69887 (* 1 = 6.69887 loss)
I0524 10:10:23.628430 11835 sgd_solver.cpp:112] Iteration 61950, lr = 0.1
I0524 10:10:30.226178 11835 solver.cpp:239] Iteration 61960 (1.34596 iter/s, 7.42963s/10 iters), loss = 5.48497
I0524 10:10:30.226375 11835 solver.cpp:258]     Train net output #0: softmax_loss = 5.48497 (* 1 = 5.48497 loss)
I0524 10:10:30.226392 11835 sgd_solver.cpp:112] Iteration 61960, lr = 0.1
I0524 10:10:39.789963 11835 solver.cpp:239] Iteration 61970 (1.04568 iter/s, 9.56316s/10 iters), loss = 6.57347
I0524 10:10:39.790014 11835 solver.cpp:258]     Train net output #0: softmax_loss = 6.57347 (* 1 = 6.57347 loss)
I0524 10:10:39.828994 11835 sgd_solver.cpp:112] Iteration 61970, lr = 0.1
