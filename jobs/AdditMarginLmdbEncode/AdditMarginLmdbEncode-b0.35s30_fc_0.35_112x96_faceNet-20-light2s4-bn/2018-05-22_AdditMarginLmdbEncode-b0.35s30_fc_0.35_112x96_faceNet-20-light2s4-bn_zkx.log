./build/tools/caffe: /home/zkx/anaconda2/lib/libtiff.so.5: no version information available (required by /home/zkx/env/opencv/lib/libopencv_highgui.so.2.4)
I0522 21:41:41.337960 26620 upgrade_proto.cpp:1084] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': models/AdditMarginLmdbEncode/AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/solver.prototxt
I0522 21:41:41.338203 26620 upgrade_proto.cpp:1091] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W0522 21:41:41.338212 26620 upgrade_proto.cpp:1093] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I0522 21:41:41.338335 26620 caffe.cpp:204] Using GPUs 0, 1, 2, 3
I0522 21:41:41.359725 26620 caffe.cpp:209] GPU 0: TITAN Xp
I0522 21:41:41.360287 26620 caffe.cpp:209] GPU 1: TITAN Xp
I0522 21:41:41.360848 26620 caffe.cpp:209] GPU 2: TITAN Xp
I0522 21:41:41.361413 26620 caffe.cpp:209] GPU 3: TITAN Xp
I0522 21:41:42.491533 26620 solver.cpp:45] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 500000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "../asset/snapshot/AdditMarginLmdbEncode/AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx"
solver_mode: GPU
device_id: 0
net: "models/AdditMarginLmdbEncode/AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/train.prototxt"
train_state {
  level: 0
  stage: ""
}
snapshot_after_train: true
stepvalue: 150000
stepvalue: 300000
iter_size: 1
type: "SGD"
I0522 21:41:42.491711 26620 solver.cpp:102] Creating training net from net file: models/AdditMarginLmdbEncode/AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/train.prototxt
I0522 21:41:42.493268 26620 net.cpp:51] Initializing net from parameters: 
name: "2018-05-22_AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_train"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  data_param {
    source: "/home/zkx/Data_sdb/TrainData/Data_sdc/Patches/lmdb/fc_0.35_112x96-base_cvdecode-shuffle_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_1"
  type: "PReLU"
  bottom: "conv1_1"
  top: "conv1_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "conv1_3"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_3"
  type: "PReLU"
  bottom: "conv1_3"
  top: "conv1_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res1_3"
  type: "Eltwise"
  bottom: "conv1_1"
  bottom: "conv1_3"
  top: "res1_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res1_3_reduce"
  type: "Convolution"
  bottom: "res1_3"
  top: "res1_3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "res1_3_reduce"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "res1_3"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_1"
  type: "PReLU"
  bottom: "conv2_1"
  top: "conv2_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res1_3_p"
  type: "Eltwise"
  bottom: "pool1"
  bottom: "conv2_1"
  top: "res1_3_p"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_3"
  type: "Convolution"
  bottom: "res1_3_p"
  top: "conv2_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_3"
  type: "PReLU"
  bottom: "conv2_3"
  top: "conv2_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res2_3"
  type: "Eltwise"
  bottom: "res1_3_p"
  bottom: "conv2_3"
  top: "res2_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_5"
  type: "Convolution"
  bottom: "res2_3"
  top: "conv2_5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2_5"
  type: "PReLU"
  bottom: "conv2_5"
  top: "conv2_5"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res2_5"
  type: "Eltwise"
  bottom: "res2_3"
  bottom: "conv2_5"
  top: "res2_5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res2_5_reduce"
  type: "Convolution"
  bottom: "res2_5"
  top: "res2_5_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2_5_reduce"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "res2_5"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_1"
  type: "PReLU"
  bottom: "conv3_1"
  top: "conv3_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res2_5_p"
  type: "Eltwise"
  bottom: "pool2"
  bottom: "conv3_1"
  top: "res2_5_p"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "res2_5_p"
  top: "conv3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_3"
  type: "PReLU"
  bottom: "conv3_3"
  top: "conv3_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res3_3"
  type: "Eltwise"
  bottom: "res2_5_p"
  bottom: "conv3_3"
  top: "res3_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_5"
  type: "Convolution"
  bottom: "res3_3"
  top: "conv3_5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_5"
  type: "PReLU"
  bottom: "conv3_5"
  top: "conv3_5"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res3_5"
  type: "Eltwise"
  bottom: "res3_3"
  bottom: "conv3_5"
  top: "res3_5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res3_5_reduce"
  type: "Convolution"
  bottom: "res3_5"
  top: "res3_5_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 144
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3_5_reduce"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "res3_5"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 144
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_1"
  type: "PReLU"
  bottom: "conv4_1"
  top: "conv4_1"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res3_5_p"
  type: "Eltwise"
  bottom: "pool3"
  bottom: "conv4_1"
  top: "res3_5_p"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "res3_5_p"
  top: "conv4_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 144
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4_3"
  type: "PReLU"
  bottom: "conv4_3"
  top: "conv4_3"
  prelu_param {
    filler {
      type: "gaussian"
      std: 0.03
    }
  }
}
layer {
  name: "res4_3"
  type: "Eltwise"
  bottom: "res3_5_p"
  bottom: "conv4_3"
  top: "res4_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "fc5"
  type: "InnerProduct"
  bottom: "res4_3"
  top: "fc5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc5_bn"
  type: "BatchNorm"
  bottom: "fc5"
  top: "fc5"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "norm1"
  type: "Normalize"
  bottom: "fc5"
  top: "norm1"
}
layer {
  name: "fc-6_l2"
  type: "InnerProduct"
  bottom: "norm1"
  top: "fc-6_l2"
  param {
    lr_mult: 1
  }
  inner_product_param {
    num_output: 21331
    bias_term: false
    weight_filler {
      type: "xavier"
    }
    normalize: true
  }
}
layer {
  name: "fc-6_margin"
  type: "LabelSpecificAdd"
  bottom: "fc-6_l2"
  bottom: "label"
  top: "fc-6_margin"
  label_specific_add_param {
    bias: -0.35
  }
}
layer {
  name: "fc-6_margin_scale"
  type: "Scale"
  bottom: "fc-6_margin"
  top: "fc-6_margin_scale"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    filler {
      type: "constant"
      value: 30
    }
  }
}
layer {
  name: "softmax_loss"
  type: "SoftmaxWithLoss"
  bottom: "fc-6_margin_scale"
  bottom: "label"
  top: "softmax_loss"
}
I0522 21:41:42.493587 26620 layer_factory.hpp:77] Creating layer data
I0522 21:41:42.493743 26620 db_lmdb.cpp:35] Opened lmdb /home/zkx/Data_sdb/TrainData/Data_sdc/Patches/lmdb/fc_0.35_112x96-base_cvdecode-shuffle_lmdb
I0522 21:41:42.493785 26620 net.cpp:84] Creating Layer data
I0522 21:41:42.493798 26620 net.cpp:380] data -> data
I0522 21:41:42.493824 26620 net.cpp:380] data -> label
I0522 21:41:42.495829 26620 data_layer.cpp:45] output data size: 64,3,112,96
I0522 21:41:42.523066 26620 net.cpp:122] Setting up data
I0522 21:41:42.523102 26620 net.cpp:129] Top shape: 64 3 112 96 (2064384)
I0522 21:41:42.523108 26620 net.cpp:129] Top shape: 64 (64)
I0522 21:41:42.523139 26620 net.cpp:137] Memory required for data: 8257792
I0522 21:41:42.523154 26620 layer_factory.hpp:77] Creating layer label_data_1_split
I0522 21:41:42.523181 26620 net.cpp:84] Creating Layer label_data_1_split
I0522 21:41:42.523195 26620 net.cpp:406] label_data_1_split <- label
I0522 21:41:42.523210 26620 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0522 21:41:42.523223 26620 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0522 21:41:42.523413 26620 net.cpp:122] Setting up label_data_1_split
I0522 21:41:42.523466 26620 net.cpp:129] Top shape: 64 (64)
I0522 21:41:42.523488 26620 net.cpp:129] Top shape: 64 (64)
I0522 21:41:42.523495 26620 net.cpp:137] Memory required for data: 8258304
I0522 21:41:42.523507 26620 layer_factory.hpp:77] Creating layer conv1_1
I0522 21:41:42.523556 26620 net.cpp:84] Creating Layer conv1_1
I0522 21:41:42.523568 26620 net.cpp:406] conv1_1 <- data
I0522 21:41:42.523586 26620 net.cpp:380] conv1_1 -> conv1_1
I0522 21:41:43.813525 26620 net.cpp:122] Setting up conv1_1
I0522 21:41:43.813580 26620 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.813585 26620 net.cpp:137] Memory required for data: 24773376
I0522 21:41:43.813628 26620 layer_factory.hpp:77] Creating layer relu1_1
I0522 21:41:43.813683 26620 net.cpp:84] Creating Layer relu1_1
I0522 21:41:43.813690 26620 net.cpp:406] relu1_1 <- conv1_1
I0522 21:41:43.813697 26620 net.cpp:367] relu1_1 -> conv1_1 (in-place)
I0522 21:41:43.815163 26620 net.cpp:122] Setting up relu1_1
I0522 21:41:43.815181 26620 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.815186 26620 net.cpp:137] Memory required for data: 41288448
I0522 21:41:43.815194 26620 layer_factory.hpp:77] Creating layer conv1_1_relu1_1_0_split
I0522 21:41:43.815217 26620 net.cpp:84] Creating Layer conv1_1_relu1_1_0_split
I0522 21:41:43.815223 26620 net.cpp:406] conv1_1_relu1_1_0_split <- conv1_1
I0522 21:41:43.815229 26620 net.cpp:380] conv1_1_relu1_1_0_split -> conv1_1_relu1_1_0_split_0
I0522 21:41:43.815237 26620 net.cpp:380] conv1_1_relu1_1_0_split -> conv1_1_relu1_1_0_split_1
I0522 21:41:43.815275 26620 net.cpp:122] Setting up conv1_1_relu1_1_0_split
I0522 21:41:43.815282 26620 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.815287 26620 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.815290 26620 net.cpp:137] Memory required for data: 74318592
I0522 21:41:43.815294 26620 layer_factory.hpp:77] Creating layer conv1_3
I0522 21:41:43.815320 26620 net.cpp:84] Creating Layer conv1_3
I0522 21:41:43.815325 26620 net.cpp:406] conv1_3 <- conv1_1_relu1_1_0_split_0
I0522 21:41:43.815330 26620 net.cpp:380] conv1_3 -> conv1_3
I0522 21:41:43.818150 26620 net.cpp:122] Setting up conv1_3
I0522 21:41:43.818166 26620 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.818171 26620 net.cpp:137] Memory required for data: 90833664
I0522 21:41:43.818181 26620 layer_factory.hpp:77] Creating layer relu1_3
I0522 21:41:43.818193 26620 net.cpp:84] Creating Layer relu1_3
I0522 21:41:43.818198 26620 net.cpp:406] relu1_3 <- conv1_3
I0522 21:41:43.818207 26620 net.cpp:367] relu1_3 -> conv1_3 (in-place)
I0522 21:41:43.818351 26620 net.cpp:122] Setting up relu1_3
I0522 21:41:43.818359 26620 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.818364 26620 net.cpp:137] Memory required for data: 107348736
I0522 21:41:43.818369 26620 layer_factory.hpp:77] Creating layer res1_3
I0522 21:41:43.818377 26620 net.cpp:84] Creating Layer res1_3
I0522 21:41:43.818383 26620 net.cpp:406] res1_3 <- conv1_1_relu1_1_0_split_1
I0522 21:41:43.818388 26620 net.cpp:406] res1_3 <- conv1_3
I0522 21:41:43.818398 26620 net.cpp:380] res1_3 -> res1_3
I0522 21:41:43.818428 26620 net.cpp:122] Setting up res1_3
I0522 21:41:43.818436 26620 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.818440 26620 net.cpp:137] Memory required for data: 123863808
I0522 21:41:43.818444 26620 layer_factory.hpp:77] Creating layer res1_3_res1_3_0_split
I0522 21:41:43.818454 26620 net.cpp:84] Creating Layer res1_3_res1_3_0_split
I0522 21:41:43.818477 26620 net.cpp:406] res1_3_res1_3_0_split <- res1_3
I0522 21:41:43.818485 26620 net.cpp:380] res1_3_res1_3_0_split -> res1_3_res1_3_0_split_0
I0522 21:41:43.818492 26620 net.cpp:380] res1_3_res1_3_0_split -> res1_3_res1_3_0_split_1
I0522 21:41:43.818526 26620 net.cpp:122] Setting up res1_3_res1_3_0_split
I0522 21:41:43.818534 26620 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.818538 26620 net.cpp:129] Top shape: 64 24 56 48 (4128768)
I0522 21:41:43.818542 26620 net.cpp:137] Memory required for data: 156893952
I0522 21:41:43.818544 26620 layer_factory.hpp:77] Creating layer res1_3_reduce
I0522 21:41:43.818557 26620 net.cpp:84] Creating Layer res1_3_reduce
I0522 21:41:43.818562 26620 net.cpp:406] res1_3_reduce <- res1_3_res1_3_0_split_0
I0522 21:41:43.818569 26620 net.cpp:380] res1_3_reduce -> res1_3_reduce
I0522 21:41:43.821485 26620 net.cpp:122] Setting up res1_3_reduce
I0522 21:41:43.821499 26620 net.cpp:129] Top shape: 64 48 56 48 (8257536)
I0522 21:41:43.821503 26620 net.cpp:137] Memory required for data: 189924096
I0522 21:41:43.821511 26620 layer_factory.hpp:77] Creating layer pool1
I0522 21:41:43.821522 26620 net.cpp:84] Creating Layer pool1
I0522 21:41:43.821529 26620 net.cpp:406] pool1 <- res1_3_reduce
I0522 21:41:43.821535 26620 net.cpp:380] pool1 -> pool1
I0522 21:41:43.821600 26620 net.cpp:122] Setting up pool1
I0522 21:41:43.821609 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.821612 26620 net.cpp:137] Memory required for data: 198181632
I0522 21:41:43.821615 26620 layer_factory.hpp:77] Creating layer conv2_1
I0522 21:41:43.821627 26620 net.cpp:84] Creating Layer conv2_1
I0522 21:41:43.821632 26620 net.cpp:406] conv2_1 <- res1_3_res1_3_0_split_1
I0522 21:41:43.821640 26620 net.cpp:380] conv2_1 -> conv2_1
I0522 21:41:43.828945 26620 net.cpp:122] Setting up conv2_1
I0522 21:41:43.828963 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.828968 26620 net.cpp:137] Memory required for data: 206439168
I0522 21:41:43.828980 26620 layer_factory.hpp:77] Creating layer relu2_1
I0522 21:41:43.828994 26620 net.cpp:84] Creating Layer relu2_1
I0522 21:41:43.828999 26620 net.cpp:406] relu2_1 <- conv2_1
I0522 21:41:43.829006 26620 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I0522 21:41:43.829136 26620 net.cpp:122] Setting up relu2_1
I0522 21:41:43.829145 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.829149 26620 net.cpp:137] Memory required for data: 214696704
I0522 21:41:43.829154 26620 layer_factory.hpp:77] Creating layer res1_3_p
I0522 21:41:43.829162 26620 net.cpp:84] Creating Layer res1_3_p
I0522 21:41:43.829167 26620 net.cpp:406] res1_3_p <- pool1
I0522 21:41:43.829172 26620 net.cpp:406] res1_3_p <- conv2_1
I0522 21:41:43.829179 26620 net.cpp:380] res1_3_p -> res1_3_p
I0522 21:41:43.829202 26620 net.cpp:122] Setting up res1_3_p
I0522 21:41:43.829210 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.829213 26620 net.cpp:137] Memory required for data: 222954240
I0522 21:41:43.829216 26620 layer_factory.hpp:77] Creating layer res1_3_p_res1_3_p_0_split
I0522 21:41:43.829223 26620 net.cpp:84] Creating Layer res1_3_p_res1_3_p_0_split
I0522 21:41:43.829226 26620 net.cpp:406] res1_3_p_res1_3_p_0_split <- res1_3_p
I0522 21:41:43.829233 26620 net.cpp:380] res1_3_p_res1_3_p_0_split -> res1_3_p_res1_3_p_0_split_0
I0522 21:41:43.829239 26620 net.cpp:380] res1_3_p_res1_3_p_0_split -> res1_3_p_res1_3_p_0_split_1
I0522 21:41:43.829274 26620 net.cpp:122] Setting up res1_3_p_res1_3_p_0_split
I0522 21:41:43.829282 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.829286 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.829289 26620 net.cpp:137] Memory required for data: 239469312
I0522 21:41:43.829293 26620 layer_factory.hpp:77] Creating layer conv2_3
I0522 21:41:43.829305 26620 net.cpp:84] Creating Layer conv2_3
I0522 21:41:43.829309 26620 net.cpp:406] conv2_3 <- res1_3_p_res1_3_p_0_split_0
I0522 21:41:43.829318 26620 net.cpp:380] conv2_3 -> conv2_3
I0522 21:41:43.835815 26620 net.cpp:122] Setting up conv2_3
I0522 21:41:43.835853 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.835858 26620 net.cpp:137] Memory required for data: 247726848
I0522 21:41:43.835866 26620 layer_factory.hpp:77] Creating layer relu2_3
I0522 21:41:43.835882 26620 net.cpp:84] Creating Layer relu2_3
I0522 21:41:43.835888 26620 net.cpp:406] relu2_3 <- conv2_3
I0522 21:41:43.835896 26620 net.cpp:367] relu2_3 -> conv2_3 (in-place)
I0522 21:41:43.836032 26620 net.cpp:122] Setting up relu2_3
I0522 21:41:43.836041 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.836045 26620 net.cpp:137] Memory required for data: 255984384
I0522 21:41:43.836051 26620 layer_factory.hpp:77] Creating layer res2_3
I0522 21:41:43.836060 26620 net.cpp:84] Creating Layer res2_3
I0522 21:41:43.836064 26620 net.cpp:406] res2_3 <- res1_3_p_res1_3_p_0_split_1
I0522 21:41:43.836069 26620 net.cpp:406] res2_3 <- conv2_3
I0522 21:41:43.836076 26620 net.cpp:380] res2_3 -> res2_3
I0522 21:41:43.836102 26620 net.cpp:122] Setting up res2_3
I0522 21:41:43.836108 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.836112 26620 net.cpp:137] Memory required for data: 264241920
I0522 21:41:43.836115 26620 layer_factory.hpp:77] Creating layer res2_3_res2_3_0_split
I0522 21:41:43.836122 26620 net.cpp:84] Creating Layer res2_3_res2_3_0_split
I0522 21:41:43.836127 26620 net.cpp:406] res2_3_res2_3_0_split <- res2_3
I0522 21:41:43.836133 26620 net.cpp:380] res2_3_res2_3_0_split -> res2_3_res2_3_0_split_0
I0522 21:41:43.836138 26620 net.cpp:380] res2_3_res2_3_0_split -> res2_3_res2_3_0_split_1
I0522 21:41:43.836175 26620 net.cpp:122] Setting up res2_3_res2_3_0_split
I0522 21:41:43.836182 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.836186 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.836189 26620 net.cpp:137] Memory required for data: 280756992
I0522 21:41:43.836192 26620 layer_factory.hpp:77] Creating layer conv2_5
I0522 21:41:43.836205 26620 net.cpp:84] Creating Layer conv2_5
I0522 21:41:43.836210 26620 net.cpp:406] conv2_5 <- res2_3_res2_3_0_split_0
I0522 21:41:43.836218 26620 net.cpp:380] conv2_5 -> conv2_5
I0522 21:41:43.841886 26620 net.cpp:122] Setting up conv2_5
I0522 21:41:43.841903 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.841907 26620 net.cpp:137] Memory required for data: 289014528
I0522 21:41:43.841917 26620 layer_factory.hpp:77] Creating layer relu2_5
I0522 21:41:43.841926 26620 net.cpp:84] Creating Layer relu2_5
I0522 21:41:43.841931 26620 net.cpp:406] relu2_5 <- conv2_5
I0522 21:41:43.841938 26620 net.cpp:367] relu2_5 -> conv2_5 (in-place)
I0522 21:41:43.842067 26620 net.cpp:122] Setting up relu2_5
I0522 21:41:43.842075 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.842078 26620 net.cpp:137] Memory required for data: 297272064
I0522 21:41:43.842088 26620 layer_factory.hpp:77] Creating layer res2_5
I0522 21:41:43.842097 26620 net.cpp:84] Creating Layer res2_5
I0522 21:41:43.842101 26620 net.cpp:406] res2_5 <- res2_3_res2_3_0_split_1
I0522 21:41:43.842105 26620 net.cpp:406] res2_5 <- conv2_5
I0522 21:41:43.842111 26620 net.cpp:380] res2_5 -> res2_5
I0522 21:41:43.842136 26620 net.cpp:122] Setting up res2_5
I0522 21:41:43.842144 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.842146 26620 net.cpp:137] Memory required for data: 305529600
I0522 21:41:43.842150 26620 layer_factory.hpp:77] Creating layer res2_5_res2_5_0_split
I0522 21:41:43.842156 26620 net.cpp:84] Creating Layer res2_5_res2_5_0_split
I0522 21:41:43.842159 26620 net.cpp:406] res2_5_res2_5_0_split <- res2_5
I0522 21:41:43.842165 26620 net.cpp:380] res2_5_res2_5_0_split -> res2_5_res2_5_0_split_0
I0522 21:41:43.842171 26620 net.cpp:380] res2_5_res2_5_0_split -> res2_5_res2_5_0_split_1
I0522 21:41:43.842203 26620 net.cpp:122] Setting up res2_5_res2_5_0_split
I0522 21:41:43.842211 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.842214 26620 net.cpp:129] Top shape: 64 48 28 24 (2064384)
I0522 21:41:43.842217 26620 net.cpp:137] Memory required for data: 322044672
I0522 21:41:43.842238 26620 layer_factory.hpp:77] Creating layer res2_5_reduce
I0522 21:41:43.842250 26620 net.cpp:84] Creating Layer res2_5_reduce
I0522 21:41:43.842257 26620 net.cpp:406] res2_5_reduce <- res2_5_res2_5_0_split_0
I0522 21:41:43.842263 26620 net.cpp:380] res2_5_reduce -> res2_5_reduce
I0522 21:41:43.843760 26620 net.cpp:122] Setting up res2_5_reduce
I0522 21:41:43.843776 26620 net.cpp:129] Top shape: 64 72 28 24 (3096576)
I0522 21:41:43.843780 26620 net.cpp:137] Memory required for data: 334430976
I0522 21:41:43.843791 26620 layer_factory.hpp:77] Creating layer pool2
I0522 21:41:43.843801 26620 net.cpp:84] Creating Layer pool2
I0522 21:41:43.843806 26620 net.cpp:406] pool2 <- res2_5_reduce
I0522 21:41:43.843813 26620 net.cpp:380] pool2 -> pool2
I0522 21:41:43.843857 26620 net.cpp:122] Setting up pool2
I0522 21:41:43.843864 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.843868 26620 net.cpp:137] Memory required for data: 337527552
I0522 21:41:43.843870 26620 layer_factory.hpp:77] Creating layer conv3_1
I0522 21:41:43.843881 26620 net.cpp:84] Creating Layer conv3_1
I0522 21:41:43.843886 26620 net.cpp:406] conv3_1 <- res2_5_res2_5_0_split_1
I0522 21:41:43.843894 26620 net.cpp:380] conv3_1 -> conv3_1
I0522 21:41:43.849508 26620 net.cpp:122] Setting up conv3_1
I0522 21:41:43.849524 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.849529 26620 net.cpp:137] Memory required for data: 340624128
I0522 21:41:43.849536 26620 layer_factory.hpp:77] Creating layer relu3_1
I0522 21:41:43.849547 26620 net.cpp:84] Creating Layer relu3_1
I0522 21:41:43.849553 26620 net.cpp:406] relu3_1 <- conv3_1
I0522 21:41:43.849560 26620 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I0522 21:41:43.849668 26620 net.cpp:122] Setting up relu3_1
I0522 21:41:43.849678 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.849680 26620 net.cpp:137] Memory required for data: 343720704
I0522 21:41:43.849685 26620 layer_factory.hpp:77] Creating layer res2_5_p
I0522 21:41:43.849694 26620 net.cpp:84] Creating Layer res2_5_p
I0522 21:41:43.849699 26620 net.cpp:406] res2_5_p <- pool2
I0522 21:41:43.849704 26620 net.cpp:406] res2_5_p <- conv3_1
I0522 21:41:43.849710 26620 net.cpp:380] res2_5_p -> res2_5_p
I0522 21:41:43.849735 26620 net.cpp:122] Setting up res2_5_p
I0522 21:41:43.849742 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.849746 26620 net.cpp:137] Memory required for data: 346817280
I0522 21:41:43.849750 26620 layer_factory.hpp:77] Creating layer res2_5_p_res2_5_p_0_split
I0522 21:41:43.849756 26620 net.cpp:84] Creating Layer res2_5_p_res2_5_p_0_split
I0522 21:41:43.849759 26620 net.cpp:406] res2_5_p_res2_5_p_0_split <- res2_5_p
I0522 21:41:43.849766 26620 net.cpp:380] res2_5_p_res2_5_p_0_split -> res2_5_p_res2_5_p_0_split_0
I0522 21:41:43.849772 26620 net.cpp:380] res2_5_p_res2_5_p_0_split -> res2_5_p_res2_5_p_0_split_1
I0522 21:41:43.849805 26620 net.cpp:122] Setting up res2_5_p_res2_5_p_0_split
I0522 21:41:43.849812 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.849817 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.849819 26620 net.cpp:137] Memory required for data: 353010432
I0522 21:41:43.849823 26620 layer_factory.hpp:77] Creating layer conv3_3
I0522 21:41:43.849833 26620 net.cpp:84] Creating Layer conv3_3
I0522 21:41:43.849839 26620 net.cpp:406] conv3_3 <- res2_5_p_res2_5_p_0_split_0
I0522 21:41:43.849846 26620 net.cpp:380] conv3_3 -> conv3_3
I0522 21:41:43.853968 26620 net.cpp:122] Setting up conv3_3
I0522 21:41:43.853986 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.853989 26620 net.cpp:137] Memory required for data: 356107008
I0522 21:41:43.853997 26620 layer_factory.hpp:77] Creating layer relu3_3
I0522 21:41:43.854008 26620 net.cpp:84] Creating Layer relu3_3
I0522 21:41:43.854014 26620 net.cpp:406] relu3_3 <- conv3_3
I0522 21:41:43.854022 26620 net.cpp:367] relu3_3 -> conv3_3 (in-place)
I0522 21:41:43.854136 26620 net.cpp:122] Setting up relu3_3
I0522 21:41:43.854146 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.854162 26620 net.cpp:137] Memory required for data: 359203584
I0522 21:41:43.854171 26620 layer_factory.hpp:77] Creating layer res3_3
I0522 21:41:43.854179 26620 net.cpp:84] Creating Layer res3_3
I0522 21:41:43.854184 26620 net.cpp:406] res3_3 <- res2_5_p_res2_5_p_0_split_1
I0522 21:41:43.854189 26620 net.cpp:406] res3_3 <- conv3_3
I0522 21:41:43.854197 26620 net.cpp:380] res3_3 -> res3_3
I0522 21:41:43.854228 26620 net.cpp:122] Setting up res3_3
I0522 21:41:43.854238 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.854241 26620 net.cpp:137] Memory required for data: 362300160
I0522 21:41:43.854245 26620 layer_factory.hpp:77] Creating layer res3_3_res3_3_0_split
I0522 21:41:43.854255 26620 net.cpp:84] Creating Layer res3_3_res3_3_0_split
I0522 21:41:43.854260 26620 net.cpp:406] res3_3_res3_3_0_split <- res3_3
I0522 21:41:43.854266 26620 net.cpp:380] res3_3_res3_3_0_split -> res3_3_res3_3_0_split_0
I0522 21:41:43.854274 26620 net.cpp:380] res3_3_res3_3_0_split -> res3_3_res3_3_0_split_1
I0522 21:41:43.854310 26620 net.cpp:122] Setting up res3_3_res3_3_0_split
I0522 21:41:43.854316 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.854321 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.854326 26620 net.cpp:137] Memory required for data: 368493312
I0522 21:41:43.854328 26620 layer_factory.hpp:77] Creating layer conv3_5
I0522 21:41:43.854339 26620 net.cpp:84] Creating Layer conv3_5
I0522 21:41:43.854346 26620 net.cpp:406] conv3_5 <- res3_3_res3_3_0_split_0
I0522 21:41:43.854352 26620 net.cpp:380] conv3_5 -> conv3_5
I0522 21:41:43.856813 26620 net.cpp:122] Setting up conv3_5
I0522 21:41:43.856830 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.856835 26620 net.cpp:137] Memory required for data: 371589888
I0522 21:41:43.856844 26620 layer_factory.hpp:77] Creating layer relu3_5
I0522 21:41:43.856856 26620 net.cpp:84] Creating Layer relu3_5
I0522 21:41:43.856863 26620 net.cpp:406] relu3_5 <- conv3_5
I0522 21:41:43.856869 26620 net.cpp:367] relu3_5 -> conv3_5 (in-place)
I0522 21:41:43.856984 26620 net.cpp:122] Setting up relu3_5
I0522 21:41:43.856992 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.856997 26620 net.cpp:137] Memory required for data: 374686464
I0522 21:41:43.857002 26620 layer_factory.hpp:77] Creating layer res3_5
I0522 21:41:43.857010 26620 net.cpp:84] Creating Layer res3_5
I0522 21:41:43.857015 26620 net.cpp:406] res3_5 <- res3_3_res3_3_0_split_1
I0522 21:41:43.857020 26620 net.cpp:406] res3_5 <- conv3_5
I0522 21:41:43.857026 26620 net.cpp:380] res3_5 -> res3_5
I0522 21:41:43.857053 26620 net.cpp:122] Setting up res3_5
I0522 21:41:43.857060 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.857064 26620 net.cpp:137] Memory required for data: 377783040
I0522 21:41:43.857066 26620 layer_factory.hpp:77] Creating layer res3_5_res3_5_0_split
I0522 21:41:43.857074 26620 net.cpp:84] Creating Layer res3_5_res3_5_0_split
I0522 21:41:43.857079 26620 net.cpp:406] res3_5_res3_5_0_split <- res3_5
I0522 21:41:43.857084 26620 net.cpp:380] res3_5_res3_5_0_split -> res3_5_res3_5_0_split_0
I0522 21:41:43.857091 26620 net.cpp:380] res3_5_res3_5_0_split -> res3_5_res3_5_0_split_1
I0522 21:41:43.857125 26620 net.cpp:122] Setting up res3_5_res3_5_0_split
I0522 21:41:43.857132 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.857136 26620 net.cpp:129] Top shape: 64 72 14 12 (774144)
I0522 21:41:43.857139 26620 net.cpp:137] Memory required for data: 383976192
I0522 21:41:43.857142 26620 layer_factory.hpp:77] Creating layer res3_5_reduce
I0522 21:41:43.857153 26620 net.cpp:84] Creating Layer res3_5_reduce
I0522 21:41:43.857158 26620 net.cpp:406] res3_5_reduce <- res3_5_res3_5_0_split_0
I0522 21:41:43.857167 26620 net.cpp:380] res3_5_reduce -> res3_5_reduce
I0522 21:41:43.861305 26620 net.cpp:122] Setting up res3_5_reduce
I0522 21:41:43.861323 26620 net.cpp:129] Top shape: 64 144 14 12 (1548288)
I0522 21:41:43.861327 26620 net.cpp:137] Memory required for data: 390169344
I0522 21:41:43.861349 26620 layer_factory.hpp:77] Creating layer pool3
I0522 21:41:43.861361 26620 net.cpp:84] Creating Layer pool3
I0522 21:41:43.861367 26620 net.cpp:406] pool3 <- res3_5_reduce
I0522 21:41:43.861374 26620 net.cpp:380] pool3 -> pool3
I0522 21:41:43.861421 26620 net.cpp:122] Setting up pool3
I0522 21:41:43.861429 26620 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.861433 26620 net.cpp:137] Memory required for data: 391717632
I0522 21:41:43.861436 26620 layer_factory.hpp:77] Creating layer conv4_1
I0522 21:41:43.861446 26620 net.cpp:84] Creating Layer conv4_1
I0522 21:41:43.861451 26620 net.cpp:406] conv4_1 <- res3_5_res3_5_0_split_1
I0522 21:41:43.861459 26620 net.cpp:380] conv4_1 -> conv4_1
I0522 21:41:43.868043 26620 net.cpp:122] Setting up conv4_1
I0522 21:41:43.868063 26620 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.868067 26620 net.cpp:137] Memory required for data: 393265920
I0522 21:41:43.868075 26620 layer_factory.hpp:77] Creating layer relu4_1
I0522 21:41:43.868084 26620 net.cpp:84] Creating Layer relu4_1
I0522 21:41:43.868088 26620 net.cpp:406] relu4_1 <- conv4_1
I0522 21:41:43.868094 26620 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I0522 21:41:43.868198 26620 net.cpp:122] Setting up relu4_1
I0522 21:41:43.868207 26620 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.868212 26620 net.cpp:137] Memory required for data: 394814208
I0522 21:41:43.868227 26620 layer_factory.hpp:77] Creating layer res3_5_p
I0522 21:41:43.868237 26620 net.cpp:84] Creating Layer res3_5_p
I0522 21:41:43.868242 26620 net.cpp:406] res3_5_p <- pool3
I0522 21:41:43.868247 26620 net.cpp:406] res3_5_p <- conv4_1
I0522 21:41:43.868253 26620 net.cpp:380] res3_5_p -> res3_5_p
I0522 21:41:43.868283 26620 net.cpp:122] Setting up res3_5_p
I0522 21:41:43.868291 26620 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.868294 26620 net.cpp:137] Memory required for data: 396362496
I0522 21:41:43.868297 26620 layer_factory.hpp:77] Creating layer res3_5_p_res3_5_p_0_split
I0522 21:41:43.868304 26620 net.cpp:84] Creating Layer res3_5_p_res3_5_p_0_split
I0522 21:41:43.868307 26620 net.cpp:406] res3_5_p_res3_5_p_0_split <- res3_5_p
I0522 21:41:43.868314 26620 net.cpp:380] res3_5_p_res3_5_p_0_split -> res3_5_p_res3_5_p_0_split_0
I0522 21:41:43.868320 26620 net.cpp:380] res3_5_p_res3_5_p_0_split -> res3_5_p_res3_5_p_0_split_1
I0522 21:41:43.868357 26620 net.cpp:122] Setting up res3_5_p_res3_5_p_0_split
I0522 21:41:43.868363 26620 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.868368 26620 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.868371 26620 net.cpp:137] Memory required for data: 399459072
I0522 21:41:43.868374 26620 layer_factory.hpp:77] Creating layer conv4_3
I0522 21:41:43.868386 26620 net.cpp:84] Creating Layer conv4_3
I0522 21:41:43.868391 26620 net.cpp:406] conv4_3 <- res3_5_p_res3_5_p_0_split_0
I0522 21:41:43.868398 26620 net.cpp:380] conv4_3 -> conv4_3
I0522 21:41:43.875061 26620 net.cpp:122] Setting up conv4_3
I0522 21:41:43.875077 26620 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.875082 26620 net.cpp:137] Memory required for data: 401007360
I0522 21:41:43.875090 26620 layer_factory.hpp:77] Creating layer relu4_3
I0522 21:41:43.875102 26620 net.cpp:84] Creating Layer relu4_3
I0522 21:41:43.875108 26620 net.cpp:406] relu4_3 <- conv4_3
I0522 21:41:43.875114 26620 net.cpp:367] relu4_3 -> conv4_3 (in-place)
I0522 21:41:43.875233 26620 net.cpp:122] Setting up relu4_3
I0522 21:41:43.875241 26620 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.875246 26620 net.cpp:137] Memory required for data: 402555648
I0522 21:41:43.875250 26620 layer_factory.hpp:77] Creating layer res4_3
I0522 21:41:43.875257 26620 net.cpp:84] Creating Layer res4_3
I0522 21:41:43.875262 26620 net.cpp:406] res4_3 <- res3_5_p_res3_5_p_0_split_1
I0522 21:41:43.875267 26620 net.cpp:406] res4_3 <- conv4_3
I0522 21:41:43.875272 26620 net.cpp:380] res4_3 -> res4_3
I0522 21:41:43.875299 26620 net.cpp:122] Setting up res4_3
I0522 21:41:43.875306 26620 net.cpp:129] Top shape: 64 144 7 6 (387072)
I0522 21:41:43.875325 26620 net.cpp:137] Memory required for data: 404103936
I0522 21:41:43.875331 26620 layer_factory.hpp:77] Creating layer fc5
I0522 21:41:43.875339 26620 net.cpp:84] Creating Layer fc5
I0522 21:41:43.875344 26620 net.cpp:406] fc5 <- res4_3
I0522 21:41:43.875352 26620 net.cpp:380] fc5 -> fc5
I0522 21:41:43.888247 26620 net.cpp:122] Setting up fc5
I0522 21:41:43.888273 26620 net.cpp:129] Top shape: 64 256 (16384)
I0522 21:41:43.888278 26620 net.cpp:137] Memory required for data: 404169472
I0522 21:41:43.888288 26620 layer_factory.hpp:77] Creating layer fc5_bn
I0522 21:41:43.888298 26620 net.cpp:84] Creating Layer fc5_bn
I0522 21:41:43.888303 26620 net.cpp:406] fc5_bn <- fc5
I0522 21:41:43.888314 26620 net.cpp:367] fc5_bn -> fc5 (in-place)
I0522 21:41:43.888528 26620 net.cpp:122] Setting up fc5_bn
I0522 21:41:43.888537 26620 net.cpp:129] Top shape: 64 256 (16384)
I0522 21:41:43.888540 26620 net.cpp:137] Memory required for data: 404235008
I0522 21:41:43.888548 26620 layer_factory.hpp:77] Creating layer norm1
I0522 21:41:43.888566 26620 net.cpp:84] Creating Layer norm1
I0522 21:41:43.888571 26620 net.cpp:406] norm1 <- fc5
I0522 21:41:43.888578 26620 net.cpp:380] norm1 -> norm1
I0522 21:41:43.888636 26620 net.cpp:122] Setting up norm1
I0522 21:41:43.888643 26620 net.cpp:129] Top shape: 64 256 1 1 (16384)
I0522 21:41:43.888648 26620 net.cpp:137] Memory required for data: 404300544
I0522 21:41:43.888650 26620 layer_factory.hpp:77] Creating layer fc-6_l2
I0522 21:41:43.888664 26620 net.cpp:84] Creating Layer fc-6_l2
I0522 21:41:43.888669 26620 net.cpp:406] fc-6_l2 <- norm1
I0522 21:41:43.888674 26620 net.cpp:380] fc-6_l2 -> fc-6_l2
I0522 21:41:43.937248 26620 net.cpp:122] Setting up fc-6_l2
I0522 21:41:43.937297 26620 net.cpp:129] Top shape: 64 21331 (1365184)
I0522 21:41:43.937306 26620 net.cpp:137] Memory required for data: 409761280
I0522 21:41:43.937321 26620 layer_factory.hpp:77] Creating layer fc-6_margin
I0522 21:41:43.937351 26620 net.cpp:84] Creating Layer fc-6_margin
I0522 21:41:43.937362 26620 net.cpp:406] fc-6_margin <- fc-6_l2
I0522 21:41:43.937376 26620 net.cpp:406] fc-6_margin <- label_data_1_split_0
I0522 21:41:43.937388 26620 net.cpp:380] fc-6_margin -> fc-6_margin
I0522 21:41:43.937458 26620 net.cpp:122] Setting up fc-6_margin
I0522 21:41:43.937470 26620 net.cpp:129] Top shape: 64 21331 (1365184)
I0522 21:41:43.937477 26620 net.cpp:137] Memory required for data: 415222016
I0522 21:41:43.937484 26620 layer_factory.hpp:77] Creating layer fc-6_margin_scale
I0522 21:41:43.937582 26620 net.cpp:84] Creating Layer fc-6_margin_scale
I0522 21:41:43.937602 26620 net.cpp:406] fc-6_margin_scale <- fc-6_margin
I0522 21:41:43.937613 26620 net.cpp:380] fc-6_margin_scale -> fc-6_margin_scale
I0522 21:41:43.937858 26620 net.cpp:122] Setting up fc-6_margin_scale
I0522 21:41:43.937872 26620 net.cpp:129] Top shape: 64 21331 (1365184)
I0522 21:41:43.937877 26620 net.cpp:137] Memory required for data: 420682752
I0522 21:41:43.937885 26620 layer_factory.hpp:77] Creating layer softmax_loss
I0522 21:41:43.937894 26620 net.cpp:84] Creating Layer softmax_loss
I0522 21:41:43.937901 26620 net.cpp:406] softmax_loss <- fc-6_margin_scale
I0522 21:41:43.937906 26620 net.cpp:406] softmax_loss <- label_data_1_split_1
I0522 21:41:43.937913 26620 net.cpp:380] softmax_loss -> softmax_loss
I0522 21:41:43.937925 26620 layer_factory.hpp:77] Creating layer softmax_loss
I0522 21:41:43.942733 26620 net.cpp:122] Setting up softmax_loss
I0522 21:41:43.942752 26620 net.cpp:129] Top shape: (1)
I0522 21:41:43.942757 26620 net.cpp:132]     with loss weight 1
I0522 21:41:43.942778 26620 net.cpp:137] Memory required for data: 420682756
I0522 21:41:43.942783 26620 net.cpp:198] softmax_loss needs backward computation.
I0522 21:41:43.942787 26620 net.cpp:198] fc-6_margin_scale needs backward computation.
I0522 21:41:43.942791 26620 net.cpp:198] fc-6_margin needs backward computation.
I0522 21:41:43.942795 26620 net.cpp:198] fc-6_l2 needs backward computation.
I0522 21:41:43.942800 26620 net.cpp:198] norm1 needs backward computation.
I0522 21:41:43.942828 26620 net.cpp:198] fc5_bn needs backward computation.
I0522 21:41:43.942834 26620 net.cpp:198] fc5 needs backward computation.
I0522 21:41:43.942837 26620 net.cpp:198] res4_3 needs backward computation.
I0522 21:41:43.942842 26620 net.cpp:198] relu4_3 needs backward computation.
I0522 21:41:43.942845 26620 net.cpp:198] conv4_3 needs backward computation.
I0522 21:41:43.942850 26620 net.cpp:198] res3_5_p_res3_5_p_0_split needs backward computation.
I0522 21:41:43.942853 26620 net.cpp:198] res3_5_p needs backward computation.
I0522 21:41:43.942858 26620 net.cpp:198] relu4_1 needs backward computation.
I0522 21:41:43.942864 26620 net.cpp:198] conv4_1 needs backward computation.
I0522 21:41:43.942868 26620 net.cpp:198] pool3 needs backward computation.
I0522 21:41:43.942873 26620 net.cpp:198] res3_5_reduce needs backward computation.
I0522 21:41:43.942875 26620 net.cpp:198] res3_5_res3_5_0_split needs backward computation.
I0522 21:41:43.942881 26620 net.cpp:198] res3_5 needs backward computation.
I0522 21:41:43.942885 26620 net.cpp:198] relu3_5 needs backward computation.
I0522 21:41:43.942889 26620 net.cpp:198] conv3_5 needs backward computation.
I0522 21:41:43.942893 26620 net.cpp:198] res3_3_res3_3_0_split needs backward computation.
I0522 21:41:43.942899 26620 net.cpp:198] res3_3 needs backward computation.
I0522 21:41:43.942903 26620 net.cpp:198] relu3_3 needs backward computation.
I0522 21:41:43.942909 26620 net.cpp:198] conv3_3 needs backward computation.
I0522 21:41:43.942912 26620 net.cpp:198] res2_5_p_res2_5_p_0_split needs backward computation.
I0522 21:41:43.942916 26620 net.cpp:198] res2_5_p needs backward computation.
I0522 21:41:43.942920 26620 net.cpp:198] relu3_1 needs backward computation.
I0522 21:41:43.942925 26620 net.cpp:198] conv3_1 needs backward computation.
I0522 21:41:43.942929 26620 net.cpp:198] pool2 needs backward computation.
I0522 21:41:43.942934 26620 net.cpp:198] res2_5_reduce needs backward computation.
I0522 21:41:43.942937 26620 net.cpp:198] res2_5_res2_5_0_split needs backward computation.
I0522 21:41:43.942944 26620 net.cpp:198] res2_5 needs backward computation.
I0522 21:41:43.942947 26620 net.cpp:198] relu2_5 needs backward computation.
I0522 21:41:43.942951 26620 net.cpp:198] conv2_5 needs backward computation.
I0522 21:41:43.942955 26620 net.cpp:198] res2_3_res2_3_0_split needs backward computation.
I0522 21:41:43.942960 26620 net.cpp:198] res2_3 needs backward computation.
I0522 21:41:43.942965 26620 net.cpp:198] relu2_3 needs backward computation.
I0522 21:41:43.942968 26620 net.cpp:198] conv2_3 needs backward computation.
I0522 21:41:43.942973 26620 net.cpp:198] res1_3_p_res1_3_p_0_split needs backward computation.
I0522 21:41:43.942977 26620 net.cpp:198] res1_3_p needs backward computation.
I0522 21:41:43.942983 26620 net.cpp:198] relu2_1 needs backward computation.
I0522 21:41:43.942986 26620 net.cpp:198] conv2_1 needs backward computation.
I0522 21:41:43.942991 26620 net.cpp:198] pool1 needs backward computation.
I0522 21:41:43.942996 26620 net.cpp:198] res1_3_reduce needs backward computation.
I0522 21:41:43.943001 26620 net.cpp:198] res1_3_res1_3_0_split needs backward computation.
I0522 21:41:43.943004 26620 net.cpp:198] res1_3 needs backward computation.
I0522 21:41:43.943011 26620 net.cpp:198] relu1_3 needs backward computation.
I0522 21:41:43.943013 26620 net.cpp:198] conv1_3 needs backward computation.
I0522 21:41:43.943017 26620 net.cpp:198] conv1_1_relu1_1_0_split needs backward computation.
I0522 21:41:43.943022 26620 net.cpp:198] relu1_1 needs backward computation.
I0522 21:41:43.943027 26620 net.cpp:198] conv1_1 needs backward computation.
I0522 21:41:43.943030 26620 net.cpp:200] label_data_1_split does not need backward computation.
I0522 21:41:43.943037 26620 net.cpp:200] data does not need backward computation.
I0522 21:41:43.943040 26620 net.cpp:242] This network produces output softmax_loss
I0522 21:41:43.943078 26620 net.cpp:255] Network initialization done.
I0522 21:41:43.943260 26620 solver.cpp:57] Solver scaffolding done.
I0522 21:41:43.944841 26620 caffe.cpp:235] Resuming from ../asset/snapshot/AdditMarginLmdbEncode/AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_395000.solverstate
I0522 21:41:43.998359 26620 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AdditMarginLmdbEncode/AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_395000.caffemodel
I0522 21:41:43.998389 26620 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:41:44.005630 26620 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:41:44.032572 26620 caffe.cpp:239] Starting Optimization
I0522 21:41:53.534623 26707 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AdditMarginLmdbEncode/AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_395000.caffemodel
I0522 21:41:53.534678 26707 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:41:53.545728 26707 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:41:53.676347 26708 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AdditMarginLmdbEncode/AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_395000.caffemodel
I0522 21:41:53.676389 26708 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:41:53.688647 26708 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:41:53.688779 26706 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ../asset/snapshot/AdditMarginLmdbEncode/AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn/2018-05-22_AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_iter_395000.caffemodel
I0522 21:41:53.688802 26706 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0522 21:41:53.700294 26706 sgd_solver.cpp:325] SGDSolver: restoring history
I0522 21:41:53.968224 26620 solver.cpp:293] Solving 2018-05-22_AdditMarginLmdbEncode-b0.35s30_fc_0.35_112x96_faceNet-20-light2s4-bn_zkx_train
I0522 21:41:53.968266 26620 solver.cpp:294] Learning Rate Policy: multistep
I0522 21:41:54.282603 26620 solver.cpp:239] Iteration 395000 (-nan iter/s, 0.306491s/10 iters), loss = 7.05626
I0522 21:41:54.282644 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.05626 (* 1 = 7.05626 loss)
I0522 21:41:54.283674 26620 sgd_solver.cpp:112] Iteration 395000, lr = 0.0001
I0522 21:41:55.201862 26620 solver.cpp:239] Iteration 395010 (10.8794 iter/s, 0.919167s/10 iters), loss = 5.37828
I0522 21:41:55.201925 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.37828 (* 1 = 5.37828 loss)
I0522 21:41:55.203881 26620 sgd_solver.cpp:112] Iteration 395010, lr = 0.0001
I0522 21:41:57.318166 26620 solver.cpp:239] Iteration 395020 (4.72562 iter/s, 2.11612s/10 iters), loss = 6.71167
I0522 21:41:57.318262 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.71167 (* 1 = 6.71167 loss)
I0522 21:41:57.504721 26620 sgd_solver.cpp:112] Iteration 395020, lr = 0.0001
I0522 21:42:03.038372 26620 solver.cpp:239] Iteration 395030 (1.74829 iter/s, 5.71988s/10 iters), loss = 6.64172
I0522 21:42:03.038449 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.64172 (* 1 = 6.64172 loss)
I0522 21:42:03.959164 26620 sgd_solver.cpp:112] Iteration 395030, lr = 0.0001
I0522 21:42:12.075212 26620 solver.cpp:239] Iteration 395040 (1.10664 iter/s, 9.0364s/10 iters), loss = 5.80098
I0522 21:42:12.075510 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.80098 (* 1 = 5.80098 loss)
I0522 21:42:12.101140 26620 sgd_solver.cpp:112] Iteration 395040, lr = 0.0001
I0522 21:42:18.936466 26620 solver.cpp:239] Iteration 395050 (1.45758 iter/s, 6.86067s/10 iters), loss = 5.61104
I0522 21:42:18.936522 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.61104 (* 1 = 5.61104 loss)
I0522 21:42:19.873553 26620 sgd_solver.cpp:112] Iteration 395050, lr = 0.0001
I0522 21:42:28.311581 26620 solver.cpp:239] Iteration 395060 (1.0667 iter/s, 9.3747s/10 iters), loss = 5.40947
I0522 21:42:28.311628 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.40947 (* 1 = 5.40947 loss)
I0522 21:42:29.223371 26620 sgd_solver.cpp:112] Iteration 395060, lr = 0.0001
I0522 21:42:35.045413 26620 solver.cpp:239] Iteration 395070 (1.48511 iter/s, 6.73351s/10 iters), loss = 6.34631
I0522 21:42:35.045469 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.34631 (* 1 = 6.34631 loss)
I0522 21:42:35.063108 26620 sgd_solver.cpp:112] Iteration 395070, lr = 0.0001
I0522 21:42:41.148689 26620 solver.cpp:239] Iteration 395080 (1.63855 iter/s, 6.10296s/10 iters), loss = 5.70318
I0522 21:42:41.148772 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.70318 (* 1 = 5.70318 loss)
I0522 21:42:41.157285 26620 sgd_solver.cpp:112] Iteration 395080, lr = 0.0001
I0522 21:42:47.715270 26620 solver.cpp:239] Iteration 395090 (1.52294 iter/s, 6.56623s/10 iters), loss = 7.05193
I0522 21:42:47.715399 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.05193 (* 1 = 7.05193 loss)
I0522 21:42:48.030254 26620 sgd_solver.cpp:112] Iteration 395090, lr = 0.0001
I0522 21:42:55.780203 26620 solver.cpp:239] Iteration 395100 (1.24 iter/s, 8.06449s/10 iters), loss = 7.16427
I0522 21:42:55.780256 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.16427 (* 1 = 7.16427 loss)
I0522 21:42:56.462924 26620 sgd_solver.cpp:112] Iteration 395100, lr = 0.0001
I0522 21:43:05.340934 26620 solver.cpp:239] Iteration 395110 (1.046 iter/s, 9.56026s/10 iters), loss = 5.98779
I0522 21:43:05.341011 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.98779 (* 1 = 5.98779 loss)
I0522 21:43:05.871364 26620 sgd_solver.cpp:112] Iteration 395110, lr = 0.0001
I0522 21:43:13.900100 26620 solver.cpp:239] Iteration 395120 (1.16839 iter/s, 8.55876s/10 iters), loss = 5.43215
I0522 21:43:13.900153 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.43215 (* 1 = 5.43215 loss)
I0522 21:43:14.739953 26620 sgd_solver.cpp:112] Iteration 395120, lr = 0.0001
I0522 21:43:23.460808 26620 solver.cpp:239] Iteration 395130 (1.046 iter/s, 9.56026s/10 iters), loss = 6.9553
I0522 21:43:23.461012 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.9553 (* 1 = 6.9553 loss)
I0522 21:43:23.591483 26620 sgd_solver.cpp:112] Iteration 395130, lr = 0.0001
I0522 21:43:33.538027 26620 solver.cpp:239] Iteration 395140 (0.992396 iter/s, 10.0766s/10 iters), loss = 6.28841
I0522 21:43:33.538085 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.28841 (* 1 = 6.28841 loss)
I0522 21:43:34.446946 26620 sgd_solver.cpp:112] Iteration 395140, lr = 0.0001
I0522 21:43:40.600231 26620 solver.cpp:239] Iteration 395150 (1.41606 iter/s, 7.06185s/10 iters), loss = 6.45312
I0522 21:43:40.600294 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.45312 (* 1 = 6.45312 loss)
I0522 21:43:41.552745 26620 sgd_solver.cpp:112] Iteration 395150, lr = 0.0001
I0522 21:43:49.159319 26620 solver.cpp:239] Iteration 395160 (1.1684 iter/s, 8.55868s/10 iters), loss = 7.69208
I0522 21:43:49.159399 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.69208 (* 1 = 7.69208 loss)
I0522 21:43:50.089959 26620 sgd_solver.cpp:112] Iteration 395160, lr = 0.0001
I0522 21:43:55.027084 26620 solver.cpp:239] Iteration 395170 (1.70432 iter/s, 5.86746s/10 iters), loss = 5.25208
I0522 21:43:55.027303 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.25208 (* 1 = 5.25208 loss)
I0522 21:43:55.258667 26620 sgd_solver.cpp:112] Iteration 395170, lr = 0.0001
I0522 21:44:00.909867 26620 solver.cpp:239] Iteration 395180 (1.70001 iter/s, 5.88233s/10 iters), loss = 5.53563
I0522 21:44:00.909934 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.53563 (* 1 = 5.53563 loss)
I0522 21:44:00.927476 26620 sgd_solver.cpp:112] Iteration 395180, lr = 0.0001
I0522 21:44:08.937469 26620 solver.cpp:239] Iteration 395190 (1.24576 iter/s, 8.02721s/10 iters), loss = 7.02015
I0522 21:44:08.937541 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.02015 (* 1 = 7.02015 loss)
I0522 21:44:09.462218 26620 sgd_solver.cpp:112] Iteration 395190, lr = 0.0001
I0522 21:44:15.923156 26620 solver.cpp:239] Iteration 395200 (1.43157 iter/s, 6.98534s/10 iters), loss = 6.21094
I0522 21:44:15.923202 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.21094 (* 1 = 6.21094 loss)
I0522 21:44:15.940598 26620 sgd_solver.cpp:112] Iteration 395200, lr = 0.0001
I0522 21:44:23.302902 26620 solver.cpp:239] Iteration 395210 (1.35512 iter/s, 7.37939s/10 iters), loss = 7.1038
I0522 21:44:23.302975 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.1038 (* 1 = 7.1038 loss)
I0522 21:44:24.189693 26620 sgd_solver.cpp:112] Iteration 395210, lr = 0.0001
I0522 21:44:31.746382 26620 solver.cpp:239] Iteration 395220 (1.1844 iter/s, 8.44308s/10 iters), loss = 5.65301
I0522 21:44:31.746551 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.65301 (* 1 = 5.65301 loss)
I0522 21:44:31.764261 26620 sgd_solver.cpp:112] Iteration 395220, lr = 0.0001
I0522 21:44:36.448137 26620 solver.cpp:239] Iteration 395230 (2.12703 iter/s, 4.7014s/10 iters), loss = 6.63353
I0522 21:44:36.448181 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.63353 (* 1 = 6.63353 loss)
I0522 21:44:37.362885 26620 sgd_solver.cpp:112] Iteration 395230, lr = 0.0001
I0522 21:44:47.035504 26620 solver.cpp:239] Iteration 395240 (0.944564 iter/s, 10.5869s/10 iters), loss = 5.48527
I0522 21:44:47.035560 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.48527 (* 1 = 5.48527 loss)
I0522 21:44:47.053617 26620 sgd_solver.cpp:112] Iteration 395240, lr = 0.0001
I0522 21:44:54.503170 26620 solver.cpp:239] Iteration 395250 (1.33917 iter/s, 7.46731s/10 iters), loss = 6.67348
I0522 21:44:54.503234 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.67348 (* 1 = 6.67348 loss)
I0522 21:44:55.413894 26620 sgd_solver.cpp:112] Iteration 395250, lr = 0.0001
I0522 21:45:01.881510 26620 solver.cpp:239] Iteration 395260 (1.35538 iter/s, 7.37799s/10 iters), loss = 7.58219
I0522 21:45:01.881690 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.58219 (* 1 = 7.58219 loss)
I0522 21:45:02.836484 26620 sgd_solver.cpp:112] Iteration 395260, lr = 0.0001
I0522 21:45:10.441300 26620 solver.cpp:239] Iteration 395270 (1.16832 iter/s, 8.55928s/10 iters), loss = 6.5207
I0522 21:45:10.441352 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.5207 (* 1 = 6.5207 loss)
I0522 21:45:10.459059 26620 sgd_solver.cpp:112] Iteration 395270, lr = 0.0001
I0522 21:45:13.648304 26620 solver.cpp:239] Iteration 395280 (3.11836 iter/s, 3.20681s/10 iters), loss = 6.03803
I0522 21:45:13.648367 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.03803 (* 1 = 6.03803 loss)
I0522 21:45:13.665630 26620 sgd_solver.cpp:112] Iteration 395280, lr = 0.0001
I0522 21:45:23.397038 26620 solver.cpp:239] Iteration 395290 (1.02582 iter/s, 9.74828s/10 iters), loss = 5.99518
I0522 21:45:23.397104 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.99518 (* 1 = 5.99518 loss)
I0522 21:45:24.253612 26620 sgd_solver.cpp:112] Iteration 395290, lr = 0.0001
I0522 21:45:31.746029 26620 solver.cpp:239] Iteration 395300 (1.19781 iter/s, 8.34859s/10 iters), loss = 5.58264
I0522 21:45:31.746094 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.58264 (* 1 = 5.58264 loss)
I0522 21:45:32.025784 26620 sgd_solver.cpp:112] Iteration 395300, lr = 0.0001
I0522 21:45:41.513208 26620 solver.cpp:239] Iteration 395310 (1.02388 iter/s, 9.76673s/10 iters), loss = 6.76362
I0522 21:45:41.513260 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.76362 (* 1 = 6.76362 loss)
I0522 21:45:42.332360 26620 sgd_solver.cpp:112] Iteration 395310, lr = 0.0001
I0522 21:45:50.970959 26620 solver.cpp:239] Iteration 395320 (1.05738 iter/s, 9.45732s/10 iters), loss = 6.60137
I0522 21:45:50.971014 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.60137 (* 1 = 6.60137 loss)
I0522 21:45:51.103122 26620 sgd_solver.cpp:112] Iteration 395320, lr = 0.0001
I0522 21:45:59.789505 26620 solver.cpp:239] Iteration 395330 (1.13403 iter/s, 8.81813s/10 iters), loss = 7.32201
I0522 21:45:59.789577 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.32201 (* 1 = 7.32201 loss)
I0522 21:46:00.720891 26620 sgd_solver.cpp:112] Iteration 395330, lr = 0.0001
I0522 21:46:07.274654 26620 solver.cpp:239] Iteration 395340 (1.33604 iter/s, 7.48478s/10 iters), loss = 7.38812
I0522 21:46:07.274797 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.38812 (* 1 = 7.38812 loss)
I0522 21:46:08.242096 26620 sgd_solver.cpp:112] Iteration 395340, lr = 0.0001
I0522 21:46:17.371384 26620 solver.cpp:239] Iteration 395350 (0.990473 iter/s, 10.0962s/10 iters), loss = 6.35538
I0522 21:46:17.371462 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.35538 (* 1 = 6.35538 loss)
I0522 21:46:17.693614 26620 sgd_solver.cpp:112] Iteration 395350, lr = 0.0001
I0522 21:46:20.878142 26620 solver.cpp:239] Iteration 395360 (2.85181 iter/s, 3.50654s/10 iters), loss = 5.27226
I0522 21:46:20.878185 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.27226 (* 1 = 5.27226 loss)
I0522 21:46:21.573110 26620 sgd_solver.cpp:112] Iteration 395360, lr = 0.0001
I0522 21:46:29.109621 26620 solver.cpp:239] Iteration 395370 (1.2149 iter/s, 8.23111s/10 iters), loss = 5.6877
I0522 21:46:29.109671 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.6877 (* 1 = 5.6877 loss)
I0522 21:46:29.127334 26620 sgd_solver.cpp:112] Iteration 395370, lr = 0.0001
I0522 21:46:37.470646 26620 solver.cpp:239] Iteration 395380 (1.19608 iter/s, 8.36064s/10 iters), loss = 5.99464
I0522 21:46:37.470764 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.99464 (* 1 = 5.99464 loss)
I0522 21:46:37.488561 26620 sgd_solver.cpp:112] Iteration 395380, lr = 0.0001
I0522 21:46:45.953197 26620 solver.cpp:239] Iteration 395390 (1.17895 iter/s, 8.48209s/10 iters), loss = 6.00917
I0522 21:46:45.953258 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.00917 (* 1 = 6.00917 loss)
I0522 21:46:45.970166 26620 sgd_solver.cpp:112] Iteration 395390, lr = 0.0001
I0522 21:46:55.332051 26620 solver.cpp:239] Iteration 395400 (1.06628 iter/s, 9.37842s/10 iters), loss = 5.31644
I0522 21:46:55.332103 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.31644 (* 1 = 5.31644 loss)
I0522 21:46:55.858222 26620 sgd_solver.cpp:112] Iteration 395400, lr = 0.0001
I0522 21:47:03.524873 26620 solver.cpp:239] Iteration 395410 (1.22064 iter/s, 8.19244s/10 iters), loss = 6.76852
I0522 21:47:03.524930 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.76852 (* 1 = 6.76852 loss)
I0522 21:47:03.721179 26620 sgd_solver.cpp:112] Iteration 395410, lr = 0.0001
I0522 21:47:11.632109 26620 solver.cpp:239] Iteration 395420 (1.23352 iter/s, 8.10686s/10 iters), loss = 6.14213
I0522 21:47:11.632216 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.14213 (* 1 = 6.14213 loss)
I0522 21:47:12.435158 26620 sgd_solver.cpp:112] Iteration 395420, lr = 0.0001
I0522 21:47:20.753404 26620 solver.cpp:239] Iteration 395430 (1.09639 iter/s, 9.12082s/10 iters), loss = 6.42269
I0522 21:47:20.753461 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.42269 (* 1 = 6.42269 loss)
I0522 21:47:20.771092 26620 sgd_solver.cpp:112] Iteration 395430, lr = 0.0001
I0522 21:47:28.011618 26620 solver.cpp:239] Iteration 395440 (1.37782 iter/s, 7.25786s/10 iters), loss = 6.8749
I0522 21:47:28.011679 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.8749 (* 1 = 6.8749 loss)
I0522 21:47:28.810784 26620 sgd_solver.cpp:112] Iteration 395440, lr = 0.0001
I0522 21:47:36.337193 26620 solver.cpp:239] Iteration 395450 (1.20118 iter/s, 8.32518s/10 iters), loss = 5.87662
I0522 21:47:36.337246 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.87662 (* 1 = 5.87662 loss)
I0522 21:47:37.116813 26620 sgd_solver.cpp:112] Iteration 395450, lr = 0.0001
I0522 21:47:45.654880 26620 solver.cpp:239] Iteration 395460 (1.07328 iter/s, 9.31727s/10 iters), loss = 6.53758
I0522 21:47:45.655067 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.53758 (* 1 = 6.53758 loss)
I0522 21:47:46.575520 26620 sgd_solver.cpp:112] Iteration 395460, lr = 0.0001
I0522 21:47:53.797678 26620 solver.cpp:239] Iteration 395470 (1.22816 iter/s, 8.14228s/10 iters), loss = 6.9386
I0522 21:47:53.797751 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.9386 (* 1 = 6.9386 loss)
I0522 21:47:54.741639 26620 sgd_solver.cpp:112] Iteration 395470, lr = 0.0001
I0522 21:48:03.199970 26620 solver.cpp:239] Iteration 395480 (1.06362 iter/s, 9.40185s/10 iters), loss = 5.91765
I0522 21:48:03.200024 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.91765 (* 1 = 5.91765 loss)
I0522 21:48:04.083251 26620 sgd_solver.cpp:112] Iteration 395480, lr = 0.0001
I0522 21:48:12.848511 26620 solver.cpp:239] Iteration 395490 (1.03647 iter/s, 9.6481s/10 iters), loss = 6.27068
I0522 21:48:12.848598 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.27068 (* 1 = 6.27068 loss)
I0522 21:48:13.098667 26620 sgd_solver.cpp:112] Iteration 395490, lr = 0.0001
I0522 21:48:21.042232 26620 solver.cpp:239] Iteration 395500 (1.22051 iter/s, 8.19331s/10 iters), loss = 6.6735
I0522 21:48:21.042428 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.6735 (* 1 = 6.6735 loss)
I0522 21:48:21.358361 26620 sgd_solver.cpp:112] Iteration 395500, lr = 0.0001
I0522 21:48:28.558979 26620 solver.cpp:239] Iteration 395510 (1.33045 iter/s, 7.51625s/10 iters), loss = 6.9565
I0522 21:48:28.559043 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.9565 (* 1 = 6.9565 loss)
I0522 21:48:29.439756 26620 sgd_solver.cpp:112] Iteration 395510, lr = 0.0001
I0522 21:48:37.027803 26620 solver.cpp:239] Iteration 395520 (1.18086 iter/s, 8.46843s/10 iters), loss = 7.42338
I0522 21:48:37.027865 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.42338 (* 1 = 7.42338 loss)
I0522 21:48:37.935717 26620 sgd_solver.cpp:112] Iteration 395520, lr = 0.0001
I0522 21:48:43.268244 26620 solver.cpp:239] Iteration 395530 (1.60253 iter/s, 6.24013s/10 iters), loss = 5.68851
I0522 21:48:43.268311 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.68851 (* 1 = 5.68851 loss)
I0522 21:48:44.179139 26620 sgd_solver.cpp:112] Iteration 395530, lr = 0.0001
I0522 21:48:51.419473 26620 solver.cpp:239] Iteration 395540 (1.22687 iter/s, 8.15084s/10 iters), loss = 6.58427
I0522 21:48:51.419708 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.58427 (* 1 = 6.58427 loss)
I0522 21:48:51.986040 26620 sgd_solver.cpp:112] Iteration 395540, lr = 0.0001
I0522 21:49:01.530649 26620 solver.cpp:239] Iteration 395550 (0.989064 iter/s, 10.1106s/10 iters), loss = 7.46041
I0522 21:49:01.530719 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.46041 (* 1 = 7.46041 loss)
I0522 21:49:02.354655 26620 sgd_solver.cpp:112] Iteration 395550, lr = 0.0001
I0522 21:49:09.568181 26620 solver.cpp:239] Iteration 395560 (1.24422 iter/s, 8.03714s/10 iters), loss = 5.36941
I0522 21:49:09.568228 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.36941 (* 1 = 5.36941 loss)
I0522 21:49:09.581575 26620 sgd_solver.cpp:112] Iteration 395560, lr = 0.0001
I0522 21:49:12.551533 26620 solver.cpp:239] Iteration 395570 (3.35214 iter/s, 2.98317s/10 iters), loss = 5.40358
I0522 21:49:12.551585 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.40358 (* 1 = 5.40358 loss)
I0522 21:49:12.582617 26620 sgd_solver.cpp:112] Iteration 395570, lr = 0.0001
I0522 21:49:19.796439 26620 solver.cpp:239] Iteration 395580 (1.38035 iter/s, 7.24455s/10 iters), loss = 6.76149
I0522 21:49:19.796506 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.76149 (* 1 = 6.76149 loss)
I0522 21:49:20.444851 26620 sgd_solver.cpp:112] Iteration 395580, lr = 0.0001
I0522 21:49:28.737895 26620 solver.cpp:239] Iteration 395590 (1.11844 iter/s, 8.94103s/10 iters), loss = 5.73455
I0522 21:49:28.738119 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.73455 (* 1 = 5.73455 loss)
I0522 21:49:29.674666 26620 sgd_solver.cpp:112] Iteration 395590, lr = 0.0001
I0522 21:49:37.284004 26620 solver.cpp:239] Iteration 395600 (1.1702 iter/s, 8.54554s/10 iters), loss = 7.54053
I0522 21:49:37.284073 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.54053 (* 1 = 7.54053 loss)
I0522 21:49:37.716675 26620 sgd_solver.cpp:112] Iteration 395600, lr = 0.0001
I0522 21:49:45.404563 26620 solver.cpp:239] Iteration 395610 (1.2315 iter/s, 8.12017s/10 iters), loss = 7.42749
I0522 21:49:45.404618 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.42749 (* 1 = 7.42749 loss)
I0522 21:49:46.247061 26620 sgd_solver.cpp:112] Iteration 395610, lr = 0.0001
I0522 21:49:49.788455 26620 solver.cpp:239] Iteration 395620 (2.2812 iter/s, 4.38366s/10 iters), loss = 6.95582
I0522 21:49:49.788507 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.95582 (* 1 = 6.95582 loss)
I0522 21:49:49.806216 26620 sgd_solver.cpp:112] Iteration 395620, lr = 0.0001
I0522 21:49:56.114426 26620 solver.cpp:239] Iteration 395630 (1.58086 iter/s, 6.32566s/10 iters), loss = 6.46554
I0522 21:49:56.114475 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.46554 (* 1 = 6.46554 loss)
I0522 21:49:56.592025 26620 sgd_solver.cpp:112] Iteration 395630, lr = 0.0001
I0522 21:50:04.708683 26620 solver.cpp:239] Iteration 395640 (1.16362 iter/s, 8.59386s/10 iters), loss = 6.01016
I0522 21:50:04.708871 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.01016 (* 1 = 6.01016 loss)
I0522 21:50:05.632221 26620 sgd_solver.cpp:112] Iteration 395640, lr = 0.0001
I0522 21:50:14.682054 26620 solver.cpp:239] Iteration 395650 (1.00273 iter/s, 9.97279s/10 iters), loss = 5.78651
I0522 21:50:14.682112 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.78651 (* 1 = 5.78651 loss)
I0522 21:50:15.583834 26620 sgd_solver.cpp:112] Iteration 395650, lr = 0.0001
I0522 21:50:23.556272 26620 solver.cpp:239] Iteration 395660 (1.12691 iter/s, 8.87381s/10 iters), loss = 7.36931
I0522 21:50:23.556345 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.36931 (* 1 = 7.36931 loss)
I0522 21:50:23.573276 26620 sgd_solver.cpp:112] Iteration 395660, lr = 0.0001
I0522 21:50:27.528856 26620 solver.cpp:239] Iteration 395670 (2.51741 iter/s, 3.97234s/10 iters), loss = 7.30764
I0522 21:50:27.528910 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.30764 (* 1 = 7.30764 loss)
I0522 21:50:28.365911 26620 sgd_solver.cpp:112] Iteration 395670, lr = 0.0001
I0522 21:50:36.286479 26620 solver.cpp:239] Iteration 395680 (1.14192 iter/s, 8.75721s/10 iters), loss = 6.6981
I0522 21:50:36.286659 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.6981 (* 1 = 6.6981 loss)
I0522 21:50:37.225196 26620 sgd_solver.cpp:112] Iteration 395680, lr = 0.0001
I0522 21:50:42.392115 26620 solver.cpp:239] Iteration 395690 (1.63795 iter/s, 6.10521s/10 iters), loss = 6.84078
I0522 21:50:42.392168 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.84078 (* 1 = 6.84078 loss)
I0522 21:50:43.294944 26620 sgd_solver.cpp:112] Iteration 395690, lr = 0.0001
I0522 21:50:52.868381 26620 solver.cpp:239] Iteration 395700 (0.954581 iter/s, 10.4758s/10 iters), loss = 6.90715
I0522 21:50:52.868428 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.90715 (* 1 = 6.90715 loss)
I0522 21:50:53.811712 26620 sgd_solver.cpp:112] Iteration 395700, lr = 0.0001
I0522 21:50:59.695420 26620 solver.cpp:239] Iteration 395710 (1.46483 iter/s, 6.82671s/10 iters), loss = 7.00398
I0522 21:50:59.695488 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.00398 (* 1 = 7.00398 loss)
I0522 21:50:59.740504 26620 sgd_solver.cpp:112] Iteration 395710, lr = 0.0001
I0522 21:51:08.696940 26620 solver.cpp:239] Iteration 395720 (1.11098 iter/s, 9.00108s/10 iters), loss = 6.57032
I0522 21:51:08.697190 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.57032 (* 1 = 6.57032 loss)
I0522 21:51:09.070962 26620 sgd_solver.cpp:112] Iteration 395720, lr = 0.0001
I0522 21:51:17.078284 26620 solver.cpp:239] Iteration 395730 (1.19321 iter/s, 8.38076s/10 iters), loss = 6.38172
I0522 21:51:17.078346 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.38172 (* 1 = 6.38172 loss)
I0522 21:51:18.006165 26620 sgd_solver.cpp:112] Iteration 395730, lr = 0.0001
I0522 21:51:27.059087 26620 solver.cpp:239] Iteration 395740 (1.00197 iter/s, 9.98035s/10 iters), loss = 7.21846
I0522 21:51:27.059144 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.21846 (* 1 = 7.21846 loss)
I0522 21:51:27.839053 26620 sgd_solver.cpp:112] Iteration 395740, lr = 0.0001
I0522 21:51:36.046378 26620 solver.cpp:239] Iteration 395750 (1.11273 iter/s, 8.98688s/10 iters), loss = 7.25057
I0522 21:51:36.046428 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.25057 (* 1 = 7.25057 loss)
I0522 21:51:36.964079 26620 sgd_solver.cpp:112] Iteration 395750, lr = 0.0001
I0522 21:51:44.530845 26620 solver.cpp:239] Iteration 395760 (1.17868 iter/s, 8.48407s/10 iters), loss = 7.08476
I0522 21:51:44.531076 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.08476 (* 1 = 7.08476 loss)
I0522 21:51:44.735227 26620 sgd_solver.cpp:112] Iteration 395760, lr = 0.0001
I0522 21:51:51.202121 26620 solver.cpp:239] Iteration 395770 (1.49907 iter/s, 6.6708s/10 iters), loss = 7.45658
I0522 21:51:51.202172 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.45658 (* 1 = 7.45658 loss)
I0522 21:51:51.234932 26620 sgd_solver.cpp:112] Iteration 395770, lr = 0.0001
I0522 21:51:59.035848 26620 solver.cpp:239] Iteration 395780 (1.27659 iter/s, 7.83336s/10 iters), loss = 5.91318
I0522 21:51:59.035900 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.91318 (* 1 = 5.91318 loss)
I0522 21:51:59.942136 26620 sgd_solver.cpp:112] Iteration 395780, lr = 0.0001
I0522 21:52:06.713673 26620 solver.cpp:239] Iteration 395790 (1.30252 iter/s, 7.67745s/10 iters), loss = 8.0097
I0522 21:52:06.713742 26620 solver.cpp:258]     Train net output #0: softmax_loss = 8.0097 (* 1 = 8.0097 loss)
I0522 21:52:07.165853 26620 sgd_solver.cpp:112] Iteration 395790, lr = 0.0001
I0522 21:52:14.997411 26620 solver.cpp:239] Iteration 395800 (1.20724 iter/s, 8.28334s/10 iters), loss = 5.63285
I0522 21:52:14.997604 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.63285 (* 1 = 5.63285 loss)
I0522 21:52:15.879104 26620 sgd_solver.cpp:112] Iteration 395800, lr = 0.0001
I0522 21:52:22.163204 26620 solver.cpp:239] Iteration 395810 (1.39686 iter/s, 7.1589s/10 iters), loss = 7.24629
I0522 21:52:22.163256 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.24629 (* 1 = 7.24629 loss)
I0522 21:52:23.090593 26620 sgd_solver.cpp:112] Iteration 395810, lr = 0.0001
I0522 21:52:31.296494 26620 solver.cpp:239] Iteration 395820 (1.09495 iter/s, 9.13287s/10 iters), loss = 5.58111
I0522 21:52:31.296540 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.58111 (* 1 = 5.58111 loss)
I0522 21:52:31.844861 26620 sgd_solver.cpp:112] Iteration 395820, lr = 0.0001
I0522 21:52:41.045473 26620 solver.cpp:239] Iteration 395830 (1.02579 iter/s, 9.74854s/10 iters), loss = 6.3492
I0522 21:52:41.045536 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.3492 (* 1 = 6.3492 loss)
I0522 21:52:41.980394 26620 sgd_solver.cpp:112] Iteration 395830, lr = 0.0001
I0522 21:52:49.443405 26620 solver.cpp:239] Iteration 395840 (1.19083 iter/s, 8.39753s/10 iters), loss = 5.86139
I0522 21:52:49.443637 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.86139 (* 1 = 5.86139 loss)
I0522 21:52:50.645925 26620 sgd_solver.cpp:112] Iteration 395840, lr = 0.0001
I0522 21:53:00.995249 26620 solver.cpp:239] Iteration 395850 (0.865712 iter/s, 11.5512s/10 iters), loss = 6.35565
I0522 21:53:00.995298 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.35565 (* 1 = 6.35565 loss)
I0522 21:53:02.043467 26620 sgd_solver.cpp:112] Iteration 395850, lr = 0.0001
I0522 21:53:12.860710 26620 solver.cpp:239] Iteration 395860 (0.842819 iter/s, 11.8649s/10 iters), loss = 6.32559
I0522 21:53:12.860759 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.32559 (* 1 = 6.32559 loss)
I0522 21:53:13.998543 26620 sgd_solver.cpp:112] Iteration 395860, lr = 0.0001
I0522 21:53:24.042601 26620 solver.cpp:239] Iteration 395870 (0.894342 iter/s, 11.1814s/10 iters), loss = 7.30667
I0522 21:53:24.042860 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.30667 (* 1 = 7.30667 loss)
I0522 21:53:25.224182 26620 sgd_solver.cpp:112] Iteration 395870, lr = 0.0001
I0522 21:53:38.409864 26620 solver.cpp:239] Iteration 395880 (0.696065 iter/s, 14.3665s/10 iters), loss = 6.7656
I0522 21:53:38.409919 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.7656 (* 1 = 6.7656 loss)
I0522 21:53:39.561560 26620 sgd_solver.cpp:112] Iteration 395880, lr = 0.0001
I0522 21:53:52.678391 26620 solver.cpp:239] Iteration 395890 (0.700873 iter/s, 14.2679s/10 iters), loss = 7.03059
I0522 21:53:52.678463 26620 solver.cpp:258]     Train net output #0: softmax_loss = 7.03059 (* 1 = 7.03059 loss)
I0522 21:53:53.771905 26620 sgd_solver.cpp:112] Iteration 395890, lr = 0.0001
I0522 21:54:05.835283 26620 solver.cpp:239] Iteration 395900 (0.760091 iter/s, 13.1563s/10 iters), loss = 5.7065
I0522 21:54:05.835474 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.7065 (* 1 = 5.7065 loss)
I0522 21:54:06.907393 26620 sgd_solver.cpp:112] Iteration 395900, lr = 0.0001
I0522 21:54:17.597260 26620 solver.cpp:239] Iteration 395910 (0.850242 iter/s, 11.7614s/10 iters), loss = 6.75568
I0522 21:54:17.597312 26620 solver.cpp:258]     Train net output #0: softmax_loss = 6.75568 (* 1 = 6.75568 loss)
I0522 21:54:18.768124 26620 sgd_solver.cpp:112] Iteration 395910, lr = 0.0001
I0522 21:54:27.639286 26620 solver.cpp:239] Iteration 395920 (0.99586 iter/s, 10.0416s/10 iters), loss = 5.52954
I0522 21:54:27.639354 26620 solver.cpp:258]     Train net output #0: softmax_loss = 5.52954 (* 1 = 5.52954 loss)
I0522 21:54:28.798024 26620 sgd_solver.cpp:112] Iteration 395920, lr = 0.0001
